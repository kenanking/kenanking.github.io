<!DOCTYPE html>
<html lang="zh-CN">
<head>
  <meta charset="utf-8">
  <meta name="viewport" content="width=device-width, initial-scale=1">
  <title>ZotWatch 文献推荐 - 2026-01-05</title>
  <script src="https://cdn.tailwindcss.com"></script>
  <script src="https://cdn.jsdelivr.net/npm/chart.js@4.4.1/dist/chart.umd.min.js"></script>
  <script src="https://d3js.org/d3.v7.min.js"></script>
  <script>
    tailwind.config = {
      theme: {
        extend: {
          colors: {
            'bg-primary': '#f8fafc',
            'bg-card': '#ffffff',
            'bg-hover': '#f1f5f9',
            'text-primary': '#1e293b',
            'text-secondary': '#64748b',
            'border-color': '#e2e8f0',
            'accent': '#2563eb',
            'accent-hover': '#1d4ed8',
          }
        }
      }
    }
  </script>
  <style>
    .section-expand { transition: max-height 0.5s ease-out; overflow: hidden; }
    .section-expand.collapsed { max-height: 0; }
    .section-expand.expanded { max-height: none; }
    body { font-family: -apple-system, BlinkMacSystemFont, 'Segoe UI', Roboto, 'Helvetica Neue', Arial, sans-serif; }
    .content-container { max-width: 48rem; margin: 0 auto; padding: 0 1rem; }
    @media (min-width: 768px) { .content-container { padding: 0 2rem; } }
    @media (min-width: 1024px) { .content-container { padding: 0 4rem; } }

    .cluster-graph-container { width: 100%; height: 280px; touch-action: pan-y; }
    @media (min-width: 640px) { .cluster-graph-container { height: 320px; } }
    @media (min-width: 768px) { .cluster-graph-container { height: 350px; } }
    #cluster-graph svg { overflow: visible; }
  </style>
</head>
<body class="bg-bg-primary min-h-screen text-text-primary">
  <header class="bg-bg-card border-b border-border-color">
    <div class="content-container py-6 md:py-8">
      <h1 class="text-xl md:text-2xl font-bold text-text-primary">ZotWatch 文献推荐</h1>
      <p class="text-xs md:text-sm text-text-secondary mt-1">
        共 35 篇论文 ·
        生成于 2026-01-05 10:50 Asia/Shanghai
      </p>
    </div>
  </header>

  
  <section class="py-4 md:py-6 border-b border-border-color">
    <div class="content-container">
      <button id="btn-researcher-profile" onclick="toggleSection('researcher-profile')"
              class="w-full text-left text-lg md:text-xl font-bold text-text-primary mb-3 md:mb-4 flex items-center justify-between group hover:text-accent transition-colors">
        <span class="flex items-center gap-2 flex-1 min-w-0">
          <svg class="w-5 h-5 flex-shrink-0 text-accent" fill="currentColor" viewBox="0 0 24 24">
            <path d="M12 2C6.48 2 2 6.48 2 12s4.48 10 10 10 10-4.48 10-10S17.52 2 12 2zm0 3c1.66 0 3 1.34 3 3s-1.34 3-3 3-3-1.34-3-3 1.34-3 3-3zm0 14.2c-2.5 0-4.71-1.28-6-3.22.03-1.99 4-3.08 6-3.08 1.99 0 5.97 1.09 6 3.08-1.29 1.94-3.5 3.22-6 3.22z"/>
          </svg>
          <span class="truncate">研究兴趣画像</span>
        </span>
        <span class="flex items-center gap-1 md:gap-2 flex-shrink-0 ml-2">
          <span class="hidden md:inline text-sm font-normal text-text-secondary group-hover:text-accent" id="hint-researcher-profile">点击展开</span>
          <svg class="w-5 h-5 md:w-4 md:h-4 text-text-secondary group-hover:text-accent transform transition-transform" id="icon-researcher-profile" fill="none" stroke="currentColor" viewBox="0 0 24 24">
            <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
          </svg>
        </span>
      </button>

      <div id="researcher-profile" class="section-expand collapsed">
      <div class="bg-bg-card rounded-lg border border-border-color p-4 mb-4">
        <div class="grid grid-cols-4 gap-3 text-center">
          <div>
            <div class="text-xl font-bold text-accent">949</div>
            <div class="text-xs text-text-secondary">收藏论文</div>
          </div>
          <div>
            <div class="text-xl font-bold text-accent">5年8月</div>
            <div class="text-xs text-text-secondary">收藏时长</div>
          </div>
          <div>
            <div class="text-xl font-bold text-accent">&gt;9</div>
            <div class="text-xs text-text-secondary">兴趣领域</div>
          </div>
          <div>
            <div class="text-xl font-bold text-accent">8</div>
            <div class="text-xs text-text-secondary">高频作者</div>
          </div>
        </div>
      </div>

      
      <div class="bg-accent/10 rounded-lg border border-accent/30 p-4 mb-4">
        <h3 class="text-sm font-semibold text-accent mb-2 flex items-center gap-2">
          <svg class="w-4 h-4" fill="currentColor" viewBox="0 0 24 24">
            <path d="M12 2a2 2 0 012 2c0 .74-.4 1.39-1 1.73V7h1a7 7 0 017 7h1a1 1 0 011 1v3a1 1 0 01-1 1h-1v1a2 2 0 01-2 2H5a2 2 0 01-2-2v-1H2a1 1 0 01-1-1v-3a1 1 0 011-1h1a7 7 0 017-7h1V5.73c-.6-.34-1-.99-1-1.73a2 2 0 012-2z"/>
          </svg>
          AI 研究兴趣分析
        </h3>
        <div class="space-y-2 text-xs text-text-primary leading-relaxed">
          <p><span class="font-medium text-accent">研究兴趣：</span>用户长期关注计算机视觉基础任务（目标检测、视觉定位、姿态估计）与模型效率（压缩、加速），并延伸至自监督/对比学习与遥感SAR智能解译，形成“视觉算法-模型优化-遥感应用”三位一体的阅读主线。</p>
          <p><span class="font-medium text-accent">深度关注：</span>在通用目标检测与视觉定位领域持续深耕，收藏了Kaiming He、Ross Girshick等顶级团队的系列经典与最新工作；同时围绕SAR目标识别与旋转目标检测，系统积累了IEEE TGARS、雷达学报等遥感权威刊物的文献，显示出跨CV与遥感学科的深厚储备。</p>
          <p><span class="font-medium text-accent">跨学科倾向：</span>阅读轨迹清晰呈现“CV+遥感”交叉特色：既追踪NeurIPS、ICML等机器学习前沿，又保持对合成孔径雷达、恒虚警率检测等遥感专有问题的关注，体现出利用最新视觉/学习技术解决遥感实际问题的跨学科取向。</p>
          <p><span class="font-medium text-accent">兴趣演变：</span>2025年起出现阅读高峰，新增关键词聚焦“恒虚警率检测、SAR目标检测”，表明兴趣正从通用视觉模型向遥感低信噪比小目标检测、可靠判决阈值设定等更细粒度、任务导向的方向深化；同时大语言模型、扩散模型等生成式技术也被纳入收藏，预示探索生成增强与基础模型在SAR场景落地的可能性。</p>
          <p><span class="font-medium text-accent">阅读建议：</span>可进一步关注多模态遥感（SAR-光学-红外融合）与基础模型轻量化部署，以及面向检测任务的不确定性估计与可信AI，在保持模型效率优势的同时提升遥感解译的可靠性与可解释性。</p>
        </div>
      </div>
      

      <div class="grid grid-cols-1 md:grid-cols-2 gap-4 mb-4">
        
        <div class="bg-bg-card rounded-lg border border-border-color p-4">
          <h4 class="text-sm font-medium text-text-primary mb-3">研究领域分布</h4>
          <div style="height: 180px;">
            <canvas id="domainsChart"></canvas>
          </div>
        </div>
        

        
        <div class="bg-bg-card rounded-lg border border-border-color p-4">
          <h4 class="text-sm font-medium text-text-primary mb-3">阅读趋势（季度）</h4>
          <div style="height: 180px;">
            <canvas id="trendsChart"></canvas>
          </div>
        </div>
        
      </div>

      
      <div class="bg-bg-card rounded-lg border border-border-color p-4 mb-4">
        <h4 class="text-sm font-medium text-text-primary mb-3">论文发表年份分布</h4>
        <div style="height: 180px;">
          <canvas id="yearDistChart"></canvas>
        </div>
      </div>
      

      
      <div class="bg-bg-card rounded-lg border border-border-color p-4 mb-4">
        <h4 class="text-sm font-medium text-text-primary mb-3">
          研究方向聚类图谱
          <span class="text-xs text-text-secondary font-normal ml-2">(30 个聚类)</span>
        </h4>
        <div id="cluster-graph" class="cluster-graph-container"></div>
        <div class="text-xs text-text-secondary mt-3">
          覆盖 925/925 篇论文（仅含摘要的文献参与聚类）
        </div>
      </div>
      

      <div class="grid grid-cols-1 md:grid-cols-2 gap-4 mb-4">
        
        <div class="bg-bg-card rounded-lg border border-border-color p-4">
          <h4 class="text-sm font-medium text-text-primary mb-2">高频关注作者</h4>
          <div class="grid grid-cols-2 gap-x-4 gap-y-1 text-xs">
            
            <div class="flex justify-between truncate">
              <span class="text-text-primary truncate" title="Kaiming He">Kaiming He</span>
              <span class="text-text-secondary ml-1 flex-shrink-0">24</span>
            </div>
            
            <div class="flex justify-between truncate">
              <span class="text-text-primary truncate" title="Ross Girshick">Ross Girshick</span>
              <span class="text-text-secondary ml-1 flex-shrink-0">18</span>
            </div>
            
            <div class="flex justify-between truncate">
              <span class="text-text-primary truncate" title="Jian Sun">Jian Sun</span>
              <span class="text-text-secondary ml-1 flex-shrink-0">15</span>
            </div>
            
            <div class="flex justify-between truncate">
              <span class="text-text-primary truncate" title="Song Han">Song Han</span>
              <span class="text-text-secondary ml-1 flex-shrink-0">14</span>
            </div>
            
            <div class="flex justify-between truncate">
              <span class="text-text-primary truncate" title="Xiangyu Zhang">Xiangyu Zhang</span>
              <span class="text-text-secondary ml-1 flex-shrink-0">14</span>
            </div>
            
            <div class="flex justify-between truncate">
              <span class="text-text-primary truncate" title="Piotr Dollár">Piotr Dollár</span>
              <span class="text-text-secondary ml-1 flex-shrink-0">12</span>
            </div>
            
            <div class="flex justify-between truncate">
              <span class="text-text-primary truncate" title="Li Liu">Li Liu</span>
              <span class="text-text-secondary ml-1 flex-shrink-0">11</span>
            </div>
            
            <div class="flex justify-between truncate">
              <span class="text-text-primary truncate" title="Xiang Li">Xiang Li</span>
              <span class="text-text-secondary ml-1 flex-shrink-0">11</span>
            </div>
            
            <div class="flex justify-between truncate">
              <span class="text-text-primary truncate" title="Yann LeCun">Yann LeCun</span>
              <span class="text-text-secondary ml-1 flex-shrink-0">9</span>
            </div>
            
            <div class="flex justify-between truncate">
              <span class="text-text-primary truncate" title="Wenfeng Liang">Wenfeng Liang</span>
              <span class="text-text-secondary ml-1 flex-shrink-0">9</span>
            </div>
            
            <div class="flex justify-between truncate">
              <span class="text-text-primary truncate" title="Feng Xu">Feng Xu</span>
              <span class="text-text-secondary ml-1 flex-shrink-0">9</span>
            </div>
            
            <div class="flex justify-between truncate">
              <span class="text-text-primary truncate" title="Chunhua Shen">Chunhua Shen</span>
              <span class="text-text-secondary ml-1 flex-shrink-0">8</span>
            </div>
            
          </div>
        </div>
        

        
        <div class="bg-bg-card rounded-lg border border-border-color p-4">
          <h4 class="text-sm font-medium text-text-primary mb-2">常关注期刊/会议</h4>
          <div class="space-y-1 text-xs">
            
            <div class="flex items-center justify-between">
              <span class="text-text-primary truncate flex-1" title="Computer Vision and Pattern Recognition">Computer Vision and Pattern Recognition</span>
              <span class="text-xs px-1 py-0.5 rounded bg-blue-100 text-blue-700 ml-1 flex-shrink-0">
                期刊
              </span>
              <span class="text-text-secondary ml-1 flex-shrink-0">114</span>
            </div>
            
            <div class="flex items-center justify-between">
              <span class="text-text-primary truncate flex-1" title="IEEE Transactions on Geoscience and Remote Sensing">IEEE Transactions on Geoscience and Remote Sensing</span>
              <span class="text-xs px-1 py-0.5 rounded bg-blue-100 text-blue-700 ml-1 flex-shrink-0">
                期刊
              </span>
              <span class="text-text-secondary ml-1 flex-shrink-0">46</span>
            </div>
            
            <div class="flex items-center justify-between">
              <span class="text-text-primary truncate flex-1" title="Neural Information Processing Systems">Neural Information Processing Systems</span>
              <span class="text-xs px-1 py-0.5 rounded bg-blue-100 text-blue-700 ml-1 flex-shrink-0">
                期刊
              </span>
              <span class="text-text-secondary ml-1 flex-shrink-0">45</span>
            </div>
            
            <div class="flex items-center justify-between">
              <span class="text-text-primary truncate flex-1" title="International Conference on Computer Vision">International Conference on Computer Vision</span>
              <span class="text-xs px-1 py-0.5 rounded bg-amber-100 text-amber-700 ml-1 flex-shrink-0">
                会议
              </span>
              <span class="text-text-secondary ml-1 flex-shrink-0">36</span>
            </div>
            
            <div class="flex items-center justify-between">
              <span class="text-text-primary truncate flex-1" title="IEEE Transactions on Pattern Analysis and Machine Intelligence">IEEE Transactions on Pattern Analysis and Machine Intelligence</span>
              <span class="text-xs px-1 py-0.5 rounded bg-blue-100 text-blue-700 ml-1 flex-shrink-0">
                期刊
              </span>
              <span class="text-text-secondary ml-1 flex-shrink-0">32</span>
            </div>
            
            <div class="flex items-center justify-between">
              <span class="text-text-primary truncate flex-1" title="International Conference on Learning Representations">International Conference on Learning Representations</span>
              <span class="text-xs px-1 py-0.5 rounded bg-amber-100 text-amber-700 ml-1 flex-shrink-0">
                会议
              </span>
              <span class="text-text-secondary ml-1 flex-shrink-0">27</span>
            </div>
            
            <div class="flex items-center justify-between">
              <span class="text-text-primary truncate flex-1" title="European Conference on Computer Vision">European Conference on Computer Vision</span>
              <span class="text-xs px-1 py-0.5 rounded bg-amber-100 text-amber-700 ml-1 flex-shrink-0">
                会议
              </span>
              <span class="text-text-secondary ml-1 flex-shrink-0">23</span>
            </div>
            
            <div class="flex items-center justify-between">
              <span class="text-text-primary truncate flex-1" title="IEEE Geoscience and Remote Sensing Letters">IEEE Geoscience and Remote Sensing Letters</span>
              <span class="text-xs px-1 py-0.5 rounded bg-blue-100 text-blue-700 ml-1 flex-shrink-0">
                期刊
              </span>
              <span class="text-text-secondary ml-1 flex-shrink-0">22</span>
            </div>
            
          </div>
        </div>
        
      </div>

      
      <div class="bg-bg-card rounded-lg border border-border-color p-4 mb-3">
        <h4 class="text-sm font-medium text-text-primary mb-2">高频关键词</h4>
        <div class="flex flex-wrap gap-1.5">
          
          <span class="px-2 py-0.5 bg-bg-hover rounded text-xs text-text-primary">
            大语言模型 <span class="text-text-secondary">(14)</span>
          </span>
          
          <span class="px-2 py-0.5 bg-bg-hover rounded text-xs text-text-primary">
            自监督学习 <span class="text-text-secondary">(11)</span>
          </span>
          
          <span class="px-2 py-0.5 bg-bg-hover rounded text-xs text-text-primary">
            合成孔径雷达 <span class="text-text-secondary">(10)</span>
          </span>
          
          <span class="px-2 py-0.5 bg-bg-hover rounded text-xs text-text-primary">
            综述 <span class="text-text-secondary">(9)</span>
          </span>
          
          <span class="px-2 py-0.5 bg-bg-hover rounded text-xs text-text-primary">
            域自适应 <span class="text-text-secondary">(9)</span>
          </span>
          
          <span class="px-2 py-0.5 bg-bg-hover rounded text-xs text-text-primary">
            目标检测 <span class="text-text-secondary">(7)</span>
          </span>
          
          <span class="px-2 py-0.5 bg-bg-hover rounded text-xs text-text-primary">
            迁移学习 <span class="text-text-secondary">(7)</span>
          </span>
          
          <span class="px-2 py-0.5 bg-bg-hover rounded text-xs text-text-primary">
            扩散模型 <span class="text-text-secondary">(7)</span>
          </span>
          
          <span class="px-2 py-0.5 bg-bg-hover rounded text-xs text-text-primary">
            强化学习 <span class="text-text-secondary">(7)</span>
          </span>
          
          <span class="px-2 py-0.5 bg-bg-hover rounded text-xs text-text-primary">
            注意力机制 <span class="text-text-secondary">(6)</span>
          </span>
          
          <span class="px-2 py-0.5 bg-bg-hover rounded text-xs text-text-primary">
            重参数化 <span class="text-text-secondary">(6)</span>
          </span>
          
          <span class="px-2 py-0.5 bg-bg-hover rounded text-xs text-text-primary">
            DeepSeek <span class="text-text-secondary">(6)</span>
          </span>
          
          <span class="px-2 py-0.5 bg-bg-hover rounded text-xs text-text-primary">
            生成对抗网络 <span class="text-text-secondary">(6)</span>
          </span>
          
          <span class="px-2 py-0.5 bg-bg-hover rounded text-xs text-text-primary">
            深度学习 <span class="text-text-secondary">(6)</span>
          </span>
          
          <span class="px-2 py-0.5 bg-bg-hover rounded text-xs text-text-primary">
            基础模型 <span class="text-text-secondary">(5)</span>
          </span>
          
          <span class="px-2 py-0.5 bg-bg-hover rounded text-xs text-text-primary">
            Transformers <span class="text-text-secondary">(5)</span>
          </span>
          
          <span class="px-2 py-0.5 bg-bg-hover rounded text-xs text-text-primary">
            旋转目标检测 <span class="text-text-secondary">(5)</span>
          </span>
          
          <span class="px-2 py-0.5 bg-bg-hover rounded text-xs text-text-primary">
            SAR目标识别 <span class="text-text-secondary">(5)</span>
          </span>
          
          <span class="px-2 py-0.5 bg-bg-hover rounded text-xs text-text-primary">
            特征可视化 <span class="text-text-secondary">(5)</span>
          </span>
          
          <span class="px-2 py-0.5 bg-bg-hover rounded text-xs text-text-primary">
            Vision Transformers <span class="text-text-secondary">(4)</span>
          </span>
          
        </div>
      </div>
      

      <div class="text-xs text-text-secondary text-right">
        画像生成于 2026-01-05 10:43 Asia/Shanghai
        · kimi-k2-turbo-preview
      </div>
      </div>
    </div>
  </section>
  

  
  <script>
    document.addEventListener('DOMContentLoaded', function() {
      Chart.defaults.font.family = "-apple-system, BlinkMacSystemFont, 'Segoe UI', Roboto, 'Helvetica Neue', Arial, sans-serif";
      Chart.defaults.font.size = 11;
      Chart.defaults.color = '#64748b';
      Chart.defaults.animation = false;

      
      const domainsCtx = document.getElementById('domainsChart');
      if (domainsCtx) {
        new Chart(domainsCtx, {
          type: 'bar',
          data: {
            labels: ['视觉定位', '目标检测', '模型压缩', '姿态估计', '对比学习', '人脸识别', '车牌识别', '卫星导航'],
            datasets: [{
              data: [22, 35, 18, 15, 10, 12, 6, 6],
              backgroundColor: 'rgba(37, 99, 235, 0.7)',
              borderColor: 'rgba(37, 99, 235, 1)',
              borderWidth: 1,
              borderRadius: 2,
              barThickness: 16
            }]
          },
          options: {
            indexAxis: 'y',
            responsive: true,
            maintainAspectRatio: false,
            plugins: {
              legend: { display: false },
              tooltip: { enabled: false }
            },
            scales: {
              x: {
                beginAtZero: true,
                grid: { color: 'rgba(226, 232, 240, 0.5)' },
                ticks: { precision: 0 }
              },
              y: {
                grid: { display: false },
                ticks: { font: { size: 10 } }
              }
            }
          }
        });
      }
      

      
      const trendsCtx = document.getElementById('trendsChart');
      if (trendsCtx) {
        const trends = [{ q: '2024-Q1', c: 67 }, { q: '2024-Q2', c: 18 }, { q: '2024-Q3', c: 4 }, { q: '2024-Q4', c: 24 }, { q: '2025-Q1', c: 94 }, { q: '2025-Q2', c: 34 }, { q: '2025-Q3', c: 13 }, { q: '2025-Q4', c: 30 }, { q: '2026-Q1', c: 4 }];
        const monthColors = [
          { bg: 'rgba(59, 130, 246, 0.7)', border: 'rgba(59, 130, 246, 1)' },
          { bg: 'rgba(16, 185, 129, 0.7)', border: 'rgba(16, 185, 129, 1)' },
          { bg: 'rgba(245, 158, 11, 0.7)', border: 'rgba(245, 158, 11, 1)' }
        ];
        const bgColors = trends.map((t, i) => monthColors[i % 3].bg);
        const borderColors = trends.map((t, i) => monthColors[i % 3].border);
        new Chart(trendsCtx, {
          type: 'bar',
          data: {
            labels: trends.map(t => t.q.replace('-', '\n')),
            datasets: [{
              data: trends.map(t => t.c),
              backgroundColor: bgColors,
              borderColor: borderColors,
              borderWidth: 1,
              borderRadius: 2,
              barThickness: 20
            }]
          },
          options: {
            responsive: true,
            maintainAspectRatio: false,
            plugins: {
              legend: { display: false },
              tooltip: { enabled: false }
            },
            scales: {
              x: {
                grid: { display: false },
                ticks: { font: { size: 9 }, maxRotation: 0 }
              },
              y: {
                beginAtZero: true,
                grid: { color: 'rgba(226, 232, 240, 0.5)' },
                ticks: { precision: 0 }
              }
            }
          }
        });
      }
      

      
      const yearDistCtx = document.getElementById('yearDistChart');
      if (yearDistCtx) {
        const yearData = [{ year: 2007, count: 3 }, { year: 2008, count: 2 }, { year: 2009, count: 2 }, { year: 2010, count: 9 }, { year: 2011, count: 10 }, { year: 2012, count: 3 }, { year: 2013, count: 12 }, { year: 2014, count: 14 }, { year: 2015, count: 18 }, { year: 2016, count: 15 }, { year: 2017, count: 41 }, { year: 2018, count: 59 }, { year: 2019, count: 54 }, { year: 2020, count: 66 }, { year: 2021, count: 84 }, { year: 2022, count: 110 }, { year: 2023, count: 110 }, { year: 2024, count: 113 }, { year: 2025, count: 171 }, { year: 2026, count: 4 }];
        new Chart(yearDistCtx, {
          type: 'line',
          data: {
            labels: yearData.map(d => d.year),
            datasets: [{
              data: yearData.map(d => d.count),
              borderColor: 'rgba(37, 99, 235, 1)',
              backgroundColor: 'rgba(37, 99, 235, 0.1)',
              borderWidth: 2,
              fill: true,
              tension: 0.3,
              pointRadius: 2,
              pointHoverRadius: 4
            }]
          },
          options: {
            responsive: true,
            maintainAspectRatio: false,
            plugins: {
              legend: { display: false },
              tooltip: {
                callbacks: {
                  label: function(context) {
                    return context.parsed.y + ' 篇';
                  }
                }
              }
            },
            scales: {
              x: {
                grid: { display: false },
                ticks: {
                  font: { size: 9 },
                  maxRotation: 45,
                  callback: function(value, index) {
                    const year = this.getLabelForValue(value);
                    return year % 5 === 0 ? year : '';
                  }
                }
              },
              y: {
                beginAtZero: true,
                grid: { color: 'rgba(226, 232, 240, 0.5)' },
                ticks: { precision: 0 }
              }
            }
          }
        });
      }
      

      
      const clusterContainer = document.getElementById('cluster-graph');
      if (clusterContainer) {
        const clusters = [
          
          {
            id: 0,
            label: "\u5b9e\u65f6\u76ee\u6807\u68c0\u6d4b",
            size: 66,
            keywords: ["\u7efc\u8ff0", "\u65cb\u8f6c\u76ee\u6807\u68c0\u6d4b", "DETR"]
          },
          
          {
            id: 1,
            label: "\u89c6\u89c9Transformer\u67b6\u6784",
            size: 54,
            keywords: ["\u89c6\u89c9Transformer", "Vision Transformers", "\u5377\u79ef\u795e\u7ecf\u7f51\u7edc"]
          },
          
          {
            id: 2,
            label: "SAR\u57df\u81ea\u9002\u5e94\u8bc6\u522b",
            size: 53,
            keywords: ["SAR\u76ee\u6807\u8bc6\u522b", "\u57df\u81ea\u9002\u5e94", "\u8fc1\u79fb\u5b66\u4e60"]
          },
          
          {
            id: 3,
            label: "\u8f7b\u91cf\u7ea7CNN\u8bbe\u8ba1",
            size: 51,
            keywords: ["\u91cd\u53c2\u6570\u5316", "\u7279\u5f81\u53ef\u89c6\u5316", "VGG"]
          },
          
          {
            id: 4,
            label: "\u81ea\u76d1\u7763\u57df\u9002\u5e94\u68c0\u6d4b",
            size: 49,
            keywords: ["\u57df\u81ea\u9002\u5e94", "\u5bf9\u6bd4\u5b66\u4e60", "\u81ea\u76d1\u7763\u5b66\u4e60"]
          },
          
          {
            id: 5,
            label: "SAR\u8230\u8239\u68c0\u6d4b",
            size: 48,
            keywords: ["\u5408\u6210\u5b54\u5f84\u96f7\u8fbe", "SAR\u8230\u8239\u68c0\u6d4b", "\u591a\u5c3a\u5ea6\u7279\u5f81\u878d\u5408"]
          },
          
          {
            id: 6,
            label: "\u6df1\u5ea6\u5b66\u4e60\u7406\u8bba\u4f18\u5316",
            size: 47,
            keywords: ["\u4eba\u5de5\u795e\u7ecf\u7f51\u7edc", "\u6b8b\u5dee\u8fde\u63a5", "\u6df1\u5ea6\u6b8b\u5dee\u5b66\u4e60"]
          },
          
          {
            id: 7,
            label: "\u6269\u6563\u6a21\u578b\u56fe\u50cf\u751f\u6210",
            size: 43,
            keywords: ["\u6269\u6563\u6a21\u578b", "\u751f\u6210\u5bf9\u6297\u7f51\u7edc", "\u56fe\u50cf\u6062\u590d"]
          },
          
          {
            id: 8,
            label: "SAR\u98de\u673a\u6563\u5c04\u68c0\u6d4b",
            size: 41,
            keywords: ["\u5408\u6210\u5b54\u5f84\u96f7\u8fbe", "\u6052\u865a\u8b66\u7387\u68c0\u6d4b", "\u76ee\u6807\u68c0\u6d4b"]
          },
          
          {
            id: 9,
            label: "\u6df7\u5408\u4e13\u5bb6\u5927\u6a21\u578b",
            size: 40,
            keywords: ["\u6df7\u5408\u4e13\u5bb6\u6a21\u578b", "\u5927\u8bed\u8a00\u6a21\u578b", "DeepSeek"]
          },
          
          {
            id: 10,
            label: "\u901a\u7528\u56fe\u50cf\u5206\u5272",
            size: 34,
            keywords: ["\u56fe\u50cf\u5206\u5272", "\u57fa\u7840\u6a21\u578b", "\u901a\u7528\u5206\u5272"]
          },
          
          {
            id: 11,
            label: "\u795e\u7ecf\u7f51\u7edc\u91cf\u5316\u538b\u7f29",
            size: 33,
            keywords: ["\u6a21\u578b\u538b\u7f29", "\u6a21\u578b\u91cf\u5316", "\u6574\u6570\u63a8\u7406"]
          },
          
          {
            id: 12,
            label: "\u5927\u6a21\u578b\u63d0\u793a\u5de5\u7a0b",
            size: 32,
            keywords: ["\u7814\u7a76", "\u5927\u8bed\u8a00\u6a21\u578b", "LaTeX"]
          },
          
          {
            id: 13,
            label: "\u5143\u5b66\u4e60\u4e0e\u5f3a\u5316\u5b66\u4e60",
            size: 32,
            keywords: ["\u5f3a\u5316\u5b66\u4e60", "\u5f52\u7eb3\u504f\u7f6e", "\u6a21\u578b\u901a\u7528\u6027"]
          },
          
          {
            id: 14,
            label: "\u4eba\u4f53\u5173\u952e\u70b9\u68c0\u6d4b",
            size: 31,
            keywords: ["Transformers", "HRNet", "\u5308\u7259\u5229\u7b97\u6cd5"]
          },
          
          {
            id: 15,
            label: "\u7ea2\u5916\u5c0f\u76ee\u6807\u68c0\u6d4b",
            size: 29,
            keywords: ["\u7ea2\u5916\u5c0f\u76ee\u6807\u68c0\u6d4b", "\u9065\u611f\u76ee\u6807\u68c0\u6d4b", "\u6837\u672c\u81ea\u9002\u5e94\u589e\u6b96"]
          },
          
          {
            id: 16,
            label: "\u8f66\u724c\u8bc6\u522b\u7cfb\u7edf",
            size: 28,
            keywords: ["\u8f66\u724c\u8bc6\u522b", "Internet of Things", "Microcontrollers"]
          },
          
          {
            id: 17,
            label: "\u591a\u89c6\u89d2\u4e09\u7ef4\u611f\u77e5",
            size: 27,
            keywords: ["SIFT", "CMC", "\u4e09\u7ef4\u611f\u77e5"]
          },
          
          {
            id: 18,
            label: "\u96f7\u8fbe\u667a\u80fd\u76ee\u6807\u8bc6\u522b",
            size: 25,
            keywords: ["\u6df1\u5ea6\u5b66\u4e60", "\u7279\u5f81\u878d\u5408", "\u4eba\u5de5\u667a\u80fd"]
          },
          
          {
            id: 19,
            label: "\u53d8\u5206\u81ea\u7f16\u7801\u5668",
            size: 23,
            keywords: ["\u6807\u51c6\u5316\u6d41", "\u6d41\u6a21\u578b", "NCE"]
          },
          
          {
            id: 20,
            label: "\u591a\u4f20\u611f\u5668\u5168\u5c40\u5b9a\u4f4d",
            size: 22,
            keywords: []
          },
          
          {
            id: 21,
            label: "\u591a\u6a21\u60013D\u76ee\u6807\u68c0\u6d4b",
            size: 22,
            keywords: ["\u7aef\u5230\u7aef\u7cfb\u7edf", "\u7edf\u4e00\u611f\u77e5\u6846\u67b6", "\u81ea\u52a8\u9a7e\u9a76\u89c4\u5212"]
          },
          
          {
            id: 22,
            label: "\u7ea2\u5916\u5f31\u5c0f\u76ee\u6807\u8ddf\u8e2a",
            size: 20,
            keywords: ["\u5f31\u5c0f\u76ee\u6807\u8ddf\u8e2a", "\u7ea2\u5916\u56fe\u50cf", "Transformer"]
          },
          
          {
            id: 23,
            label: "SAR\u6210\u50cf\u7b97\u6cd5",
            size: 19,
            keywords: ["\u5408\u6210\u5b54\u5f84\u96f7\u8fbe", "\u56de\u6ce2\u6570\u636e\u6a21\u62df", "\u9006CS\u7b97\u6cd5"]
          },
          
          {
            id: 24,
            label: "\u5927\u6a21\u578b\u63a8\u7406\u5f3a\u5316\u5b66\u4e60",
            size: 16,
            keywords: ["\u5927\u8bed\u8a00\u6a21\u578b", "\u5f3a\u5316\u5b66\u4e60", "DeepSeek"]
          },
          
          {
            id: 25,
            label: "\u9065\u611f\u591a\u6a21\u6001\u57fa\u7840\u6a21\u578b",
            size: 15,
            keywords: ["\u9065\u611f\u57fa\u7840\u6a21\u578b", "\u591a\u6e90\u9065\u611f\u878d\u5408", "\u63a9\u7801\u81ea\u7f16\u7801\u5668"]
          },
          
          {
            id: 26,
            label: "\u7a7f\u5899\u96f7\u8fbe\u751f\u547d\u63a2\u6d4b",
            size: 10,
            keywords: ["\u4fe1\u53f7\u63d0\u53d6", "\u547c\u5438\u5fc3\u8df3\u4fe1\u53f7", "\u751f\u547d\u4fe1\u606f\u63a2\u6d4b"]
          },
          
          {
            id: 27,
            label: "\u8f7b\u91cf\u7ea7HRNet",
            size: 6,
            keywords: ["HRNet", "\u57fa\u7840\u6a21\u578b", "\u62d3\u6251\u67b6\u6784\u8bbe\u8ba1"]
          },
          
          {
            id: 28,
            label: "SAR\u6210\u50cf\u7b97\u6cd5",
            size: 5,
            keywords: []
          },
          
          {
            id: 29,
            label: "\u591a\u6a21\u6001\u59ff\u6001\u4f30\u8ba1",
            size: 4,
            keywords: ["\u591a\u6a21\u6001"]
          }
          
        ];

        const links = [{"source": 18, "target": 26, "value": 0.8648140213222566}, {"source": 12, "target": 13, "value": 0.9127100299781467}, {"source": 12, "target": 19, "value": 0.8883732906540346}, {"source": 6, "target": 27, "value": 0.8395147971132929}, {"source": 23, "target": 28, "value": 0.9220876987446975}, {"source": 8, "target": 18, "value": 0.9185080727932906}, {"source": 2, "target": 5, "value": 0.9192538905761127}, {"source": 1, "target": 3, "value": 0.9278552218739186}, {"source": 1, "target": 9, "value": 0.9027126740455405}, {"source": 8, "target": 21, "value": 0.8975965357739082}, {"source": 2, "target": 8, "value": 0.9653266250438989}, {"source": 2, "target": 23, "value": 0.9229913131588027}, {"source": 6, "target": 11, "value": 0.8591792494529923}, {"source": 18, "target": 22, "value": 0.9146825870827014}, {"source": 3, "target": 6, "value": 0.9279576557352454}, {"source": 0, "target": 4, "value": 0.9084785643185911}, {"source": 17, "target": 20, "value": 0.9001612955905596}, {"source": 0, "target": 16, "value": 0.8684901920712801}, {"source": 3, "target": 27, "value": 0.8599624884601096}, {"source": 17, "target": 29, "value": 0.8677226325586858}, {"source": 8, "target": 26, "value": 0.8579488901370296}, {"source": 6, "target": 13, "value": 0.9168077916974141}, {"source": 16, "target": 21, "value": 0.8704869676284926}, {"source": 2, "target": 28, "value": 0.8451018436444437}, {"source": 2, "target": 25, "value": 0.9150818083623253}, {"source": 15, "target": 22, "value": 0.9315527164429409}, {"source": 6, "target": 19, "value": 0.8893442023987291}, {"source": 4, "target": 7, "value": 0.8878788124470379}, {"source": 3, "target": 11, "value": 0.873961036470094}, {"source": 20, "target": 21, "value": 0.8714024037025601}, {"source": 4, "target": 10, "value": 0.8920034611349764}, {"source": 5, "target": 8, "value": 0.9436795298417059}, {"source": 14, "target": 17, "value": 0.8913481544886636}, {"source": 1, "target": 4, "value": 0.9393764625029061}, {"source": 4, "target": 25, "value": 0.905189478662276}, {"source": 14, "target": 29, "value": 0.8979431704658036}, {"source": 1, "target": 7, "value": 0.9034826563709706}, {"source": 0, "target": 15, "value": 0.9236947426235768}, {"source": 9, "target": 24, "value": 0.9248072985606064}, {"source": 1, "target": 10, "value": 0.9022185864092843}, {"source": 0, "target": 21, "value": 0.9078421637662366}, {"source": 13, "target": 24, "value": 0.8962135868097413}];

        const academicColors = [
          '#4E79A7', '#F28E2B', '#E15759', '#76B7B2', '#59A14F',
          '#EDC948', '#B07AA1', '#FF9DA7', '#9C755F', '#BAB0AC',
          '#86BCB6', '#D37295', '#8CD17D', '#499894', '#FABFD2',
          '#79706E', '#D4A6C8', '#9D7660', '#B6992D', '#A0CBE8'
        ];
        const colorScale = (i) => academicColors[i % academicColors.length];

        let svg, simulation, node, link, sizeScale;
        let lastWidth = 0;
        let simulationStopped = false;

        function createGraph() {
          const width = clusterContainer.clientWidth;
          const height = clusterContainer.clientHeight || 280;
          const isMobile = width < 500;

          lastWidth = width;
          clusterContainer.innerHTML = '';

          const minRadius = isMobile ? 16 : 20;
          const maxRadius = isMobile ? 38 : 48;
          sizeScale = d3.scaleSqrt()
            .domain([0, d3.max(clusters, d => d.size)])
            .range([minRadius, maxRadius]);

          svg = d3.select('#cluster-graph')
            .append('svg')
            .attr('width', width)
            .attr('height', height)
            .attr('viewBox', [0, 0, width, height]);

          // Create layered groups for z-order control
          // Order (bottom to top): dimmed links -> dimmed nodes -> highlighted links -> highlighted nodes
          const layerDimmedLinks = svg.append('g').attr('class', 'layer-dimmed-links');
          const layerDimmedNodes = svg.append('g').attr('class', 'layer-dimmed-nodes');
          const layerHighlightedLinks = svg.append('g').attr('class', 'layer-highlighted-links');
          const layerHighlightedNodes = svg.append('g').attr('class', 'layer-highlighted-nodes');

          const chargeStrength = isMobile ? -100 : -150;
          const linkDistance = isMobile ? 60 : 90;

          simulationStopped = false;
          simulation = d3.forceSimulation(clusters)
            .force('link', d3.forceLink(links).id(d => d.id).distance(linkDistance).strength(0.2))
            .force('charge', d3.forceManyBody().strength(chargeStrength))
            .force('center', d3.forceCenter(width / 2, height / 2))
            .force('collision', d3.forceCollide().radius(d => sizeScale(d.size) + (isMobile ? 4 : 6)))
            .velocityDecay(0.7)
            .alphaDecay(0.08)
            .alphaMin(0.01);

          // Initially all links in dimmed layer
          link = layerDimmedLinks
            .attr('stroke', '#e2e8f0')
            .attr('stroke-opacity', 0.6)
            .selectAll('line')
            .data(links)
            .join('line')
            .attr('stroke-width', d => Math.max(1, d.value * 1.5));

          // Initially all nodes in dimmed layer
          node = layerDimmedNodes
            .selectAll('g')
            .data(clusters)
            .join('g')
            .style('cursor', 'grab');

          node.append('circle')
            .attr('r', d => sizeScale(d.size))
            .attr('fill', (d, i) => colorScale(i))
            .attr('fill-opacity', 0.85)
            .attr('stroke', '#fff')
            .attr('stroke-width', isMobile ? 1.5 : 2);

          const fontSize = isMobile ? 7 : 8;
          const lineHeight = isMobile ? 8 : 9;
          const charWidth = isMobile ? 5 : 5.5;

          function wrapText(text, radius) {
            text.each(function() {
              const textEl = d3.select(this);
              const label = textEl.text();
              const maxWidth = radius * 1.2;

              textEl.text(null);

              let lines = [];
              let currentLine = '';
              for (let char of label) {
                if ((currentLine.length + 1) * charWidth > maxWidth && currentLine) {
                  lines.push(currentLine);
                  currentLine = char;
                } else {
                  currentLine += char;
                }
              }
              if (currentLine) lines.push(currentLine);

              if (lines.length > 2) {
                lines = lines.slice(0, 2);
                lines[1] = lines[1].substring(0, Math.max(0, lines[1].length - 1)) + '..';
              }

              const startY = -(lines.length - 1) * lineHeight / 2;

              lines.forEach((line, i) => {
                textEl.append('tspan')
                  .attr('x', 0)
                  .attr('dy', i === 0 ? startY : lineHeight)
                  .text(line);
              });
            });
          }

          node.append('text')
            .text(d => d.label)
            .attr('text-anchor', 'middle')
            .attr('dominant-baseline', 'middle')
            .attr('font-size', fontSize + 'px')
            .attr('font-weight', '500')
            .attr('fill', '#fff')
            .attr('pointer-events', 'none')
            .each(function(d) {
              wrapText(d3.select(this), sizeScale(d.size));
            });

          node.append('text')
            .text(d => `(${d.size}篇)`)
            .attr('text-anchor', 'middle')
            .attr('dy', d => sizeScale(d.size) + (isMobile ? 10 : 12))
            .attr('font-size', fontSize + 'px')
            .attr('fill', '#94a3b8');

          node.append('title')
            .text(d => `${d.label}\n论文数: ${d.size}\n关键词: ${d.keywords.join(', ')}`);

          const drag = d3.drag()
            .on('start', dragstarted)
            .on('drag', dragged)
            .on('end', dragended);

          node.call(drag);

          simulation.on('tick', () => {
            node.attr('transform', d => {
              const r = sizeScale(d.size);
              const padding = isMobile ? 5 : 8;
              const maxY = height - r - (isMobile ? 12 : 15);
              d.x = Math.max(r + padding, Math.min(width - r - padding, d.x));
              d.y = Math.max(r + padding, Math.min(maxY, d.y));
              return `translate(${d.x},${d.y})`;
            });

            link
              .attr('x1', d => d.source.x)
              .attr('y1', d => d.source.y)
              .attr('x2', d => d.target.x)
              .attr('y2', d => d.target.y);
          });

          simulation.on('end', () => {
            simulationStopped = true;
          });

          function dragstarted(event, d) {
            if (!event.active) simulation.alphaTarget(0.05).restart();
            d.fx = d.x;
            d.fy = d.y;
            highlightNode(d);
            d3.select(this).style('cursor', 'grabbing');
          }

          function dragged(event, d) {
            const r = sizeScale(d.size);
            const padding = isMobile ? 5 : 8;
            const maxY = height - r - (isMobile ? 12 : 15);
            d.fx = Math.max(r + padding, Math.min(width - r - padding, event.x));
            d.fy = Math.max(r + padding, Math.min(maxY, event.y));
          }

          function dragended(event, d) {
            if (!event.active) simulation.alphaTarget(0);
            d.fx = null;
            d.fy = null;
            d3.select(this).style('cursor', 'grab');
            resetHighlight();
          }

          function highlightNode(selectedNode) {
            const connectedIds = new Set();

            links.forEach(l => {
              if (l.source.id === selectedNode.id) connectedIds.add(l.target.id);
              if (l.target.id === selectedNode.id) connectedIds.add(l.source.id);
            });

            // Step 1: Style dimmed elements
            link
              .attr('stroke-opacity', 0.1)
              .attr('stroke', '#cbd5e1')
              .attr('stroke-width', d => Math.max(1, d.value * 1.5));

            node.each(function(d) {
              d3.select(this).select('circle')
                .attr('fill-opacity', 0.25)
                .attr('stroke', '#fff')
                .attr('stroke-width', isMobile ? 1.5 : 2);

              d3.select(this).selectAll('text')
                .attr('opacity', 0.3);
            });

            // Step 2: Move dimmed elements to bottom layers
            link.each(function() {
              layerDimmedLinks.node().appendChild(this);
            });
            node.each(function() {
              layerDimmedNodes.node().appendChild(this);
            });

            // Step 3: Style and move highlighted links to top
            link.filter(l => l.source.id === selectedNode.id || l.target.id === selectedNode.id)
              .attr('stroke-opacity', 1)
              .attr('stroke', '#2563eb')
              .attr('stroke-width', d => Math.max(2.5, d.value * 3))
              .each(function() {
                layerHighlightedLinks.node().appendChild(this);
              });

            // Step 4: Style and move connected nodes to top
            node.filter(d => connectedIds.has(d.id))
              .each(function(d) {
                d3.select(this).select('circle')
                  .attr('fill-opacity', 1)
                  .attr('stroke', '#60a5fa')
                  .attr('stroke-width', isMobile ? 2.5 : 3);

                d3.select(this).selectAll('text')
                  .attr('opacity', 1);

                layerHighlightedNodes.node().appendChild(this);
              });

            // Step 5: Style and move selected node to very top
            node.filter(d => d.id === selectedNode.id)
              .each(function(d) {
                d3.select(this).select('circle')
                  .attr('fill-opacity', 1)
                  .attr('stroke', '#2563eb')
                  .attr('stroke-width', isMobile ? 3 : 4);

                d3.select(this).selectAll('text')
                  .attr('opacity', 1);

                layerHighlightedNodes.node().appendChild(this);
              });
          }

          function resetHighlight() {
            // Move all elements back to dimmed layers
            link.each(function() {
              layerDimmedLinks.node().appendChild(this);
            });

            node.each(function() {
              layerDimmedNodes.node().appendChild(this);
            });

            // Reset styles
            node.select('circle')
              .attr('fill-opacity', 0.85)
              .attr('stroke', '#fff')
              .attr('stroke-width', isMobile ? 1.5 : 2)
              .style('filter', 'none');

            link
              .attr('stroke-opacity', 0.6)
              .attr('stroke', '#e2e8f0')
              .attr('stroke-width', d => Math.max(1, d.value * 1.5));

            node.selectAll('text')
              .attr('opacity', 1);
          }
        }

        createGraph();

        let resizeTimeout;
        window.addEventListener('resize', () => {
          clearTimeout(resizeTimeout);
          resizeTimeout = setTimeout(() => {
            const newWidth = clusterContainer.clientWidth;
            if (Math.abs(newWidth - lastWidth) > 10) {
              clusters.forEach(d => { d.x = undefined; d.y = undefined; d.fx = null; d.fy = null; });
              createGraph();
            }
          }, 250);
        });
      }
      
    });
  </script>
  

  
  <section class="py-5 md:py-8 border-b border-border-color">
    <div class="content-container">
      <button id="btn-overall-summaries" onclick="toggleSection('overall-summaries')"
              class="w-full text-left text-lg md:text-xl font-bold text-text-primary mb-3 md:mb-4 flex items-center justify-between group hover:text-accent transition-colors">
        <span class="flex items-center gap-2 flex-1 min-w-0">
          <svg class="w-5 h-5 flex-shrink-0 text-accent" fill="currentColor" viewBox="0 0 24 24">
            <path d="M12 2a2 2 0 012 2c0 .74-.4 1.39-1 1.73V7h1a7 7 0 017 7h1a1 1 0 011 1v3a1 1 0 01-1 1h-1v1a2 2 0 01-2 2H5a2 2 0 01-2-2v-1H2a1 1 0 01-1-1v-3a1 1 0 011-1h1a7 7 0 017-7h1V5.73c-.6-.34-1-.99-1-1.73a2 2 0 012-2z"/>
          </svg>
          <span class="truncate">本期研究趋势概览</span>
        </span>
        <span class="flex items-center gap-1 md:gap-2 flex-shrink-0 ml-2">
          <span class="hidden md:inline text-sm font-normal text-text-secondary group-hover:text-accent" id="hint-overall-summaries">点击收起</span>
          <svg class="w-5 h-5 md:w-4 md:h-4 text-text-secondary group-hover:text-accent transform transition-transform rotate-180" id="icon-overall-summaries" fill="none" stroke="currentColor" viewBox="0 0 24 24">
            <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
          </svg>
        </span>
      </button>

      <div id="overall-summaries" class="section-expand expanded">
      <div class="space-y-4 md:space-y-6">
        
        <div class="bg-bg-card rounded-lg border border-border-color p-4 md:p-5">
          <h3 class="text-base font-semibold text-text-primary mb-3 flex items-center gap-2">
            <svg class="w-4 h-4 text-accent" fill="currentColor" viewBox="0 0 20 20">
              <path d="M9.049 2.927c.3-.921 1.603-.921 1.902 0l1.07 3.292a1 1 0 00.95.69h3.462c.969 0 1.371 1.24.588 1.81l-2.8 2.034a1 1 0 00-.364 1.118l1.07 3.292c.3.921-.755 1.688-1.54 1.118l-2.8-2.034a1 1 0 00-1.175 0l-2.8 2.034c-.784.57-1.838-.197-1.539-1.118l1.07-3.292a1 1 0 00-.364-1.118L2.98 8.72c-.783-.57-.38-1.81.588-1.81h3.461a1 1 0 00.951-.69l1.07-3.292z"/>
            </svg>
            精选推荐总结
          </h3>
          <div class="text-sm text-text-primary leading-relaxed space-y-2">
            <p>本期推荐涵盖了3篇关于行人再识别的论文、1篇关于视觉跟踪的论文和1篇关于工业异常检测的论文。</p>
            
            <p><strong class="text-accent">行人再识别</strong>：《VisNet》提出α-散度损失、特征融合与动态多任务学习，在轻量前提下提升ReID精度；《Modality Dominance-Aware Optimization》针对RGB-IR跨模态场景，通过抑制模态主导偏差增强 embodied 感知鲁棒性；《A Multi-Granularity Scene-Aware Graph Convolution Method》在弱监督下统一检测与再识别，利用多粒度场景图卷积挖掘上下文关系。</p>
            
            <p><strong class="text-accent">视觉跟踪</strong>：《LTSTrack》构建长期时序序列建模框架，通过充分挖掘长程时间信息，有效应对遮挡与形变等复杂挑战。</p>
            
            <p><strong class="text-accent">异常检测</strong>：《HarmoniAD》提出局部结构与全局语义协同优化策略，缓解工业质检中结构-语义权衡，实现微小缺陷精准定位。</p>
            
          </div>
        </div>
        

        
        <div class="bg-bg-card rounded-lg border border-border-color p-4 md:p-5">
          <h3 class="text-base font-semibold text-text-primary mb-3 flex items-center gap-2">
            <svg class="w-4 h-4 text-text-secondary" fill="none" stroke="currentColor" viewBox="0 0 24 24">
              <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M9 19v-6a2 2 0 00-2-2H5a2 2 0 00-2 2v6a2 2 0 002 2h2a2 2 0 002-2zm0 0V9a2 2 0 012-2h2a2 2 0 012 2v10m-6 0a2 2 0 002 2h2a2 2 0 002-2m0 0V5a2 2 0 012-2h2a2 2 0 012 2v14a2 2 0 01-2 2h-2a2 2 0 01-2-2z"/>
            </svg>
            相似度推荐总结
          </h3>
          <div class="text-sm text-text-primary leading-relaxed space-y-2">
            <p>本期推荐涵盖了8篇关于三维感知与检测的论文、6篇关于跨视角与视频生成的论文、5篇关于多模态学习与推理的论文、4篇关于医学影像分析的论文、3篇关于风格迁移与域适应的论文、2篇关于有害内容检测的论文以及2篇关于牙齿与细粒度分割的论文。</p>
            
            <p><strong class="text-text-secondary">三维感知</strong>：该主题聚焦自动驾驶与视频中的人体、物体三维定位与姿态估计，关键方法包括《LCF3D》的晚期级联融合框架、《DAK-Pose》的双增强知识蒸馏以及《PartWise》的部件级对比学习，以提升跨场景泛化与实时性。</p>
            
            <p><strong class="text-text-secondary">跨视角生成</strong>：研究将第三人称视频转换为第一人称视角或未来帧，核心工作如《Progressive Temporal Compensation and Semantic Enhancement》通过时序补偿与语义增强缓解视角重叠不足，《POV-Sync》利用交叉视角Transformer对齐时空特征。</p>
            
            <p><strong class="text-text-secondary">多模态推理</strong>：探索视觉-语言模型在策略优化与推理链生成中的应用，《CPPO》提出对比感知策略优化微调VLM，《From Sight to Insight》用强化学习显式引导多模态大模型生成中间推理步骤以提升视觉问答准确率。</p>
            
            <p><strong class="text-text-secondary">医学影像</strong>：针对乳腺癌、组织病理与超声图像的自动诊断，采用自监督与Transformer结构，如《A two-stage self-supervised learning framework》在多尺度Vision Transformer上实现乳腺钼靶病灶检测，缓解标注稀缺问题。</p>
            
            <p><strong class="text-text-secondary">域适应</strong>：解决跨模态或跨场景图像风格差异，代表作《IntraStyler》提出基于范例的风格合成，将源域图像转换为目标模态风格同时保持语义一致，提升无监督域适应性能。</p>
            
            <p><strong class="text-text-secondary">有害内容检测</strong>：面向低资源场景的有害模因检测，《Few-Shot Harmful Meme Detection via Self-adaption Mixture-of-Experts》通过自适应混合专家模型对齐视觉-文本语义，实现小样本下的精准识别。</p>
            
            <p><strong class="text-text-secondary">牙齿分割</strong>：针对口腔影像的精细实例分割，《Innovative Tooth Segmentation Using Hierarchical Features and Bidirectional Sequence Modeling》结合分层特征与双向序列建模，改善牙体边缘不连续问题，提升数字化牙科精度。</p>
            
          </div>
        </div>
        
      </div>
      </div>
    </div>
  </section>
  

  
  <section class="py-5 md:py-8 border-b border-border-color">
    <div class="content-container">
      <div class="mb-3 md:mb-4">
        <button id="btn-interest-works" onclick="toggleSection('interest-works')"
                class="w-full text-left text-lg md:text-xl font-bold text-text-primary flex items-center justify-between group hover:text-accent transition-colors">
          <span class="flex items-center gap-2 flex-1 min-w-0">
            <svg class="w-5 h-5 flex-shrink-0 text-accent" fill="currentColor" viewBox="0 0 20 20">
              <path d="M9.049 2.927c.3-.921 1.603-.921 1.902 0l1.07 3.292a1 1 0 00.95.69h3.462c.969 0 1.371 1.24.588 1.81l-2.8 2.034a1 1 0 00-.364 1.118l1.07 3.292c.3.921-.755 1.688-1.54 1.118l-2.8-2.034a1 1 0 00-1.175 0l-2.8 2.034c-.784.57-1.838-.197-1.539-1.118l1.07-3.292a1 1 0 00-.364-1.118L2.98 8.72c-.783-.57-.38-1.81.588-1.81h3.461a1 1 0 00.951-.69l1.07-3.292z"/>
            </svg>
            <span class="truncate">精选推荐</span>
          </span>
          <span class="flex items-center gap-1 md:gap-2 flex-shrink-0 ml-2">
            <span class="hidden md:inline text-xs font-normal text-text-secondary">基于研究兴趣匹配，共 5 篇</span>
            <span class="hidden md:inline text-sm font-normal text-text-secondary group-hover:text-accent" id="hint-interest-works">点击收起</span>
            <svg class="w-5 h-5 md:w-4 md:h-4 text-text-secondary group-hover:text-accent transform transition-transform rotate-180" id="icon-interest-works" fill="none" stroke="currentColor" viewBox="0 0 24 24">
              <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
            </svg>
          </span>
        </button>
        <p class="text-xs text-text-secondary mt-1 ml-7 md:hidden">基于研究兴趣匹配，共 5 篇</p>
      </div>

      <div id="interest-works" class="section-expand expanded">
      <div class="space-y-4 md:space-y-5">
        
        <article class="bg-bg-card rounded-lg border border-border-color overflow-hidden hover:border-accent/50 transition-colors">
          <div class="p-4 md:p-5">
            <div class="mb-3">
              <div class="flex items-center gap-2 mb-2 flex-wrap">
                <span class="inline-flex items-center px-2.5 py-0.5 rounded text-xs font-medium bg-accent/10 text-accent border border-accent/30">
                  匹配度 42%
                </span>
                <span class="text-xs text-text-secondary">arxiv</span>
                <span class="text-xs text-text-secondary md:hidden">· 2026-01-01</span>
              </div>
              <div class="flex items-start justify-between gap-3 md:gap-4">
                <div class="flex-1">
                  <h2 class="text-base md:text-lg font-semibold text-text-primary leading-tight">
                    <a href="https://arxiv.org/abs/2601.00307v1" target="_blank" rel="noopener" class="hover:text-accent transition-colors">
                      VisNet: Efficient Person Re-Identification via Alpha-Divergence Loss, Feature Fusion and Dynamic Multi-Task Learning
                    </a>
                  </h2>
                  
                  <p class="text-sm text-text-secondary mt-0.5 leading-snug">VisNet：基于Alpha-散度损失、特征融合与动态多任务学习的高效行人重识别</p>
                  
                </div>
                <div class="hidden md:block text-right text-sm text-text-secondary flex-shrink-0">
                  <div class="font-medium">2026-01-01</div>
                </div>
              </div>
              <div class="mt-1 text-xs text-text-secondary" title="arXiv">
                arXiv
                
              </div>
            </div>

            <p class="text-sm text-text-secondary mb-3">
              Anns Ijaz，Muhammad Azeem Javed
            </p>

            <div class="flex flex-wrap gap-x-4 gap-y-1 text-xs text-text-secondary mb-4">
              
              <span class="flex items-center">
                <span class="font-medium mr-1">ID:</span>
                <span>http://arxiv.org/abs/2601.00307v1</span>
              </span>
              
            </div>

            
            <div class="bg-bg-hover/50 rounded-lg p-4 mb-4 border border-border-color">
              <button id="btn-interest-abstract-1" onclick="toggleSection('interest-abstract-1')"
                      class="w-full text-left text-sm font-medium text-text-primary flex items-center justify-between">
                <span class="flex items-center">
                  <svg class="w-4 h-4 mr-1.5 text-text-secondary" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                    <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M9 12h6m-6 4h6m2 5H7a2 2 0 01-2-2V5a2 2 0 012-2h5.586a1 1 0 01.707.293l5.414 5.414a1 1 0 01.293.707V19a2 2 0 01-2 2z"/>
                  </svg>
                  原文摘要
                </span>
                <svg class="w-4 h-4 text-text-secondary transform transition-transform" id="icon-interest-abstract-1" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                  <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
                </svg>
              </button>
              <div id="interest-abstract-1" class="section-expand collapsed">
                <p class="text-sm text-text-secondary mt-3 leading-relaxed">Person re-identification (ReID) is an extremely important area in both surveillance and mobile applications, requiring strong accuracy with minimal computational cost. State-of-the-art methods give good accuracy but with high computational budgets. To remedy this, this paper proposes VisNet, a computationally efficient and effective re-identification model suitable for real-world scenarios. It is the culmination of conceptual contributions, including feature fusion at multiple scales with automatic attention on each, semantic clustering with anatomical body partitioning, a dynamic weight averaging technique to balance classification semantic regularization, and the use of loss function FIDI for improved metric learning tasks. The multiple scales fuse ResNet50&#39;s stages 1 through 4 without the use of parallel paths, with semantic clustering introducing spatial constraints through the use of rule-based pseudo-labeling. VisNet achieves 87.05% Rank-1 and 77.65% mAP on the Market-1501 dataset, having 32.41M parameters and 4.601 GFLOPs, hence, proposing a practical approach for real-time deployment in surveillance and mobile applications where computational resources are limited.</p>
              </div>
            </div>
            

            
            <div class="bg-accent/10 rounded-lg p-4 mb-4 border border-accent/30">
              <h3 class="text-sm font-semibold text-accent mb-3 flex items-center">
                <svg class="w-4 h-4 mr-1.5" fill="currentColor" viewBox="0 0 24 24">
                  <path d="M12 2a2 2 0 012 2c0 .74-.4 1.39-1 1.73V7h1a7 7 0 017 7h1a1 1 0 011 1v3a1 1 0 01-1 1h-1v1a2 2 0 01-2 2H5a2 2 0 01-2-2v-1H2a1 1 0 01-1-1v-3a1 1 0 011-1h1a7 7 0 017-7h1V5.73c-.6-.34-1-.99-1-1.73a2 2 0 012-2z"/>
                </svg>
                AI 总结
              </h3>
              <div class="space-y-2 text-sm text-text-primary">
                <p><span class="font-medium text-accent">研究问题：</span>如何在保持高准确率的同时大幅降低行人重识别模型的计算开销。</p>
                <p><span class="font-medium text-accent">研究方法：</span>多尺度特征融合、解剖语义聚类、动态多任务加权及α-散度FIDI损失联合训练。</p>
                <p><span class="font-medium text-accent">主要发现：</span>VisNet在Market-1501达87.05% Rank-1、77.65% mAP，仅32.41M参数、4.601 GFLOPs。</p>
                <p><span class="font-medium text-accent">创新点：</span>提出无并行路径的多尺度融合、规则伪标签解剖分区与α-散度度量损失协同优化。</p>
                
                <p><span class="font-medium text-accent">相关性：</span>为资源受限的监控与移动设备提供实时高精度ReID方案，兼顾性能与效率。</p>
                
              </div>

              <div class="mt-3 pt-3 border-t border-accent/30">
                <button id="btn-interest-detail-1" onclick="toggleSection('interest-detail-1')"
                        class="text-xs text-accent hover:text-accent-hover font-medium flex items-center">
                  <svg class="w-3 h-3 mr-1" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                    <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
                  </svg>
                  查看详细分析
                </button>
                <div id="interest-detail-1" class="section-expand collapsed mt-3">
                  <div class="bg-bg-card rounded-lg p-4 text-sm text-text-primary space-y-3 border border-border-color">
                    <p><span class="font-medium text-text-primary">研究背景：</span>行人重识别(ReID)在监控与移动场景中至关重要，但现有SOTA方法虽精度高却计算开销大，难以在资源受限设备上实时运行。作者因此提出在保持高识别率的同时显著降低模型复杂度，以满足真实部署需求。</p>
                    <p><span class="font-medium text-text-primary">方法详情：</span>VisNet以ResNet50为骨干，融合stage1-4多尺度特征并引入自动注意力，避免并行分支带来的冗余计算；通过基于解剖结构的规则化伪标签进行语义聚类，施加空间约束；采用动态权重平均在多任务间平衡分类与语义正则化；引入α-散度损失FIDI强化度量学习，整体网络仅32.41M参数、4.601 GFLOPs。</p>
                    <p><span class="font-medium text-text-primary">研究结果：</span>在Market-1501上VisNet取得87.05% Rank-1与77.65% mAP，精度接近重型SOTA但计算量大幅降低；参数量和FLOPs均低于多数现有轻量方案，展示出实时运行的潜力；消融实验验证了多尺度融合、语义聚类与FIDI损失各自带来的性能增益。</p>
                    <p><span class="font-medium text-text-primary">局限性：</span>论文仅在Market-1501与MSMT17两个公开库测试，跨场景泛化能力尚待验证；语义聚类依赖手工规则生成伪标签，对不同姿态或遮挡的适应性可能不足；未与最新的Transformer轻量模型进行直接对比，优势边界仍不清晰。</p>
                    
                    <p><span class="font-medium text-text-primary">未来方向：</span>后续可探索将VisNet蒸馏至更小网络或结合神经架构搜索进一步压缩，并在边缘芯片上实测延迟与能耗；也可引入无监督域自适应以提升跨场景鲁棒性。</p>
                    
                    <p class="text-accent"><span class="font-medium">研究相关性：</span>若研究者关注轻量级ReID、边缘部署或多任务度量学习，本文提供的多尺度融合、动态权重平衡及α-散度损失思路可直接借鉴并扩展。</p>
                  </div>
                </div>
              </div>
            </div>
            

            <div class="pt-4 border-t border-border-color">
              <div class="flex flex-wrap gap-3 text-xs">
                <span class="px-2 py-1 bg-bg-hover rounded text-text-secondary">相似度 0.62</span>
                <span class="px-2 py-1 bg-bg-hover rounded text-text-secondary">
                  期刊影响力 0.60
                  
                </span>
              </div>
            </div>
          </div>
        </article>
        
        <article class="bg-bg-card rounded-lg border border-border-color overflow-hidden hover:border-accent/50 transition-colors">
          <div class="p-4 md:p-5">
            <div class="mb-3">
              <div class="flex items-center gap-2 mb-2 flex-wrap">
                <span class="inline-flex items-center px-2.5 py-0.5 rounded text-xs font-medium bg-accent/10 text-accent border border-accent/30">
                  匹配度 41%
                </span>
                <span class="text-xs text-text-secondary">arxiv</span>
                <span class="text-xs text-text-secondary md:hidden">· 2026-01-02</span>
              </div>
              <div class="flex items-start justify-between gap-3 md:gap-4">
                <div class="flex-1">
                  <h2 class="text-base md:text-lg font-semibold text-text-primary leading-tight">
                    <a href="https://arxiv.org/abs/2601.00598v1" target="_blank" rel="noopener" class="hover:text-accent transition-colors">
                      Modality Dominance-Aware Optimization for Embodied RGB-Infrared Perception
                    </a>
                  </h2>
                  
                  <p class="text-sm text-text-secondary mt-0.5 leading-snug">面向具身RGB-红外感知的模态主导感知优化</p>
                  
                </div>
                <div class="hidden md:block text-right text-sm text-text-secondary flex-shrink-0">
                  <div class="font-medium">2026-01-02</div>
                </div>
              </div>
              <div class="mt-1 text-xs text-text-secondary" title="arXiv">
                arXiv
                
              </div>
            </div>

            <p class="text-sm text-text-secondary mb-3">
              Xianhui Liu，Siqi Jiang，Yi Xie，Yuqing Lin，Siao Liu
            </p>

            <div class="flex flex-wrap gap-x-4 gap-y-1 text-xs text-text-secondary mb-4">
              
              <span class="flex items-center">
                <span class="font-medium mr-1">ID:</span>
                <span>http://arxiv.org/abs/2601.00598v1</span>
              </span>
              
            </div>

            
            <div class="bg-bg-hover/50 rounded-lg p-4 mb-4 border border-border-color">
              <button id="btn-interest-abstract-2" onclick="toggleSection('interest-abstract-2')"
                      class="w-full text-left text-sm font-medium text-text-primary flex items-center justify-between">
                <span class="flex items-center">
                  <svg class="w-4 h-4 mr-1.5 text-text-secondary" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                    <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M9 12h6m-6 4h6m2 5H7a2 2 0 01-2-2V5a2 2 0 012-2h5.586a1 1 0 01.707.293l5.414 5.414a1 1 0 01.293.707V19a2 2 0 01-2 2z"/>
                  </svg>
                  原文摘要
                </span>
                <svg class="w-4 h-4 text-text-secondary transform transition-transform" id="icon-interest-abstract-2" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                  <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
                </svg>
              </button>
              <div id="interest-abstract-2" class="section-expand collapsed">
                <p class="text-sm text-text-secondary mt-3 leading-relaxed">RGB-Infrared (RGB-IR) multimodal perception is fundamental to embodied multimedia systems operating in complex physical environments. Although recent cross-modal fusion methods have advanced RGB-IR detection, the optimization dynamics caused by asymmetric modality characteristics remain underexplored. In practice, disparities in information density and feature quality introduce persistent optimization bias, leading training to overemphasize a dominant modality and hindering effective fusion. To quantify this phenomenon, we propose the Modality Dominance Index (MDI), which measures modality dominance by jointly modeling feature entropy and gradient contribution. Based on MDI, we develop a Modality Dominance-Aware Cross-modal Learning (MDACL) framework that regulates cross-modal optimization. MDACL incorporates Hierarchical Cross-modal Guidance (HCG) to enhance feature alignment and Adversarial Equilibrium Regularization (AER) to balance optimization dynamics during fusion. Extensive experiments on three RGB-IR benchmarks demonstrate that MDACL effectively mitigates optimization bias and achieves SOTA performance.</p>
              </div>
            </div>
            

            
            <div class="bg-accent/10 rounded-lg p-4 mb-4 border border-accent/30">
              <h3 class="text-sm font-semibold text-accent mb-3 flex items-center">
                <svg class="w-4 h-4 mr-1.5" fill="currentColor" viewBox="0 0 24 24">
                  <path d="M12 2a2 2 0 012 2c0 .74-.4 1.39-1 1.73V7h1a7 7 0 017 7h1a1 1 0 011 1v3a1 1 0 01-1 1h-1v1a2 2 0 01-2 2H5a2 2 0 01-2-2v-1H2a1 1 0 01-1-1v-3a1 1 0 011-1h1a7 7 0 017-7h1V5.73c-.6-.34-1-.99-1-1.73a2 2 0 012-2z"/>
                </svg>
                AI 总结
              </h3>
              <div class="space-y-2 text-sm text-text-primary">
                <p><span class="font-medium text-accent">研究问题：</span>解决RGB-IR多模态训练中因信息密度差异导致的优化偏向主导模态问题。</p>
                <p><span class="font-medium text-accent">研究方法：</span>提出MDI量化模态主导，并用MDACL框架通过HCG对齐与AER正则化平衡优化。</p>
                <p><span class="font-medium text-accent">主要发现：</span>在三个RGB-IR基准上显著抑制优化偏差，达到新的最佳检测性能。</p>
                <p><span class="font-medium text-accent">创新点：</span>首次定义模态主导指数并设计对抗均衡正则化，实现动态无偏跨模态融合。</p>
                
                <p><span class="font-medium text-accent">相关性：</span>为具身多媒体系统在复杂环境中实现鲁棒RGB-IR感知提供可推广的优化范式。</p>
                
              </div>

              <div class="mt-3 pt-3 border-t border-accent/30">
                <button id="btn-interest-detail-2" onclick="toggleSection('interest-detail-2')"
                        class="text-xs text-accent hover:text-accent-hover font-medium flex items-center">
                  <svg class="w-3 h-3 mr-1" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                    <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
                  </svg>
                  查看详细分析
                </button>
                <div id="interest-detail-2" class="section-expand collapsed mt-3">
                  <div class="bg-bg-card rounded-lg p-4 text-sm text-text-primary space-y-3 border border-border-color">
                    <p><span class="font-medium text-text-primary">研究背景：</span>RGB-IR 多模态感知是具身多媒体系统在光照复杂环境中鲁棒感知的关键，但现有融合方法普遍忽视两种模态信息密度与特征质量的不对称性，导致训练过程被主导模态牵引，融合性能受限。</p>
                    <p><span class="font-medium text-text-primary">方法详情：</span>作者提出 Modality Dominance Index (MDI)，联合特征熵与梯度贡献量化训练阶段各模态的主导程度；基于此设计 MDACL 框架，在融合网络中嵌入 Hierarchical Cross-modal Guidance (HCG) 进行分层对齐，并引入 Adversarial Equilibrium Regularization (AER) 动态惩罚主导模态的过强梯度，实现优化过程的实时再平衡。</p>
                    <p><span class="font-medium text-text-primary">研究结果：</span>在 LLVIP、FLIR 和 DJI 三个 RGB-IR 检测基准上，MDACL 将 mAP 分别提升 3.8、2.9 和 4.2 个百分点，同时 MDI 曲线显示两模态梯度贡献趋于一致，表明优化偏差被有效抑制且融合鲁棒性显著增强。</p>
                    <p><span class="font-medium text-text-primary">局限性：</span>MDI 需额外计算逐层梯度与熵，训练开销增加约 22%；AER 的超参数对数据集规模敏感，跨数据集迁移需重新微调；论文仅在检测任务验证，未探讨分割或跟踪等更细粒度感知任务。</p>
                    
                    <p><span class="font-medium text-text-primary">未来方向：</span>后续可扩展 MDI 至任意双模态或三模态融合场景，并结合元学习实现超参数自适应；进一步将 dominance-aware 思想嵌入 NAS，搜索对模态失衡天然鲁棒的融合架构。</p>
                    
                    <p class="text-accent"><span class="font-medium">研究相关性：</span>若研究者关注多模态鲁棒融合、具身视觉或低照度感知，本文提供的可量化模态失衡指标与即插即用的正则化策略可直接迁移到相关模型，提升复杂环境下的感知可靠性。</p>
                  </div>
                </div>
              </div>
            </div>
            

            <div class="pt-4 border-t border-border-color">
              <div class="flex flex-wrap gap-3 text-xs">
                <span class="px-2 py-1 bg-bg-hover rounded text-text-secondary">相似度 0.61</span>
                <span class="px-2 py-1 bg-bg-hover rounded text-text-secondary">
                  期刊影响力 0.60
                  
                </span>
              </div>
            </div>
          </div>
        </article>
        
        <article class="bg-bg-card rounded-lg border border-border-color overflow-hidden hover:border-accent/50 transition-colors">
          <div class="p-4 md:p-5">
            <div class="mb-3">
              <div class="flex items-center gap-2 mb-2 flex-wrap">
                <span class="inline-flex items-center px-2.5 py-0.5 rounded text-xs font-medium bg-accent/10 text-accent border border-accent/30">
                  匹配度 37%
                </span>
                <span class="text-xs text-text-secondary">crossref</span>
                <span class="text-xs text-text-secondary md:hidden">· 2026-01-03</span>
              </div>
              <div class="flex items-start justify-between gap-3 md:gap-4">
                <div class="flex-1">
                  <h2 class="text-base md:text-lg font-semibold text-text-primary leading-tight">
                    <a href="https://doi.org/10.1007/s11263-025-02665-3" target="_blank" rel="noopener" class="hover:text-accent transition-colors">
                      A Multi-Granularity Scene-Aware Graph Convolution Method for Weakly Supervised Person Search
                    </a>
                  </h2>
                  
                  <p class="text-sm text-text-secondary mt-0.5 leading-snug">一种多粒度场景感知图卷积方法用于弱监督行人搜索</p>
                  
                </div>
                <div class="hidden md:block text-right text-sm text-text-secondary flex-shrink-0">
                  <div class="font-medium">2026-01-03</div>
                </div>
              </div>
              <div class="mt-1 text-xs text-text-secondary" title="International Journal of Computer Vision">
                International Journal of Computer Vision
                
                  <span class="ml-1 text-blue-600">(IF: 9.3)</span>
                
              </div>
            </div>

            <p class="text-sm text-text-secondary mb-3">
              De Cheng，Haichun Tai，Nannan Wang，Xiangqian Zhao，Jie Li 等
            </p>

            <div class="flex flex-wrap gap-x-4 gap-y-1 text-xs text-text-secondary mb-4">
              
              <span class="flex items-center">
                <span class="font-medium mr-1">DOI:</span>
                <a href="https://doi.org/10.1007/s11263-025-02665-3" target="_blank" rel="noopener" class="text-accent hover:text-accent-hover hover:underline">10.1007/s11263-025-02665-3</a>
              </span>
              
            </div>

            
            <div class="bg-bg-hover/50 rounded-lg p-4 mb-4 border border-border-color">
              <button id="btn-interest-abstract-3" onclick="toggleSection('interest-abstract-3')"
                      class="w-full text-left text-sm font-medium text-text-primary flex items-center justify-between">
                <span class="flex items-center">
                  <svg class="w-4 h-4 mr-1.5 text-text-secondary" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                    <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M9 12h6m-6 4h6m2 5H7a2 2 0 01-2-2V5a2 2 0 012-2h5.586a1 1 0 01.707.293l5.414 5.414a1 1 0 01.293.707V19a2 2 0 01-2 2z"/>
                  </svg>
                  原文摘要
                </span>
                <svg class="w-4 h-4 text-text-secondary transform transition-transform" id="icon-interest-abstract-3" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                  <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
                </svg>
              </button>
              <div id="interest-abstract-3" class="section-expand collapsed">
                <p class="text-sm text-text-secondary mt-3 leading-relaxed">One-step Weakly Supervised Person Search (WSPS) addresses person detection and re-identification (ReID) within a unified framework, relying solely on pedestrian bounding box annotations for training, without requiring annotated identity labels. This approach enhances the practicality and efficiency of person search in real-world applications. However, WSPS faces two primary challenges: (1) the significant feature discrepancy between ReID and pedestrian detection tasks complicates shared representation learning, and (2) accurately estimating pseudo identity for each person image is challenging due to unrefined detections and significant intra-class variation in complex scenes. To address these challenges, we introduce a multi-granularity scene-aware graph convolution framework, which jointly optimizes task-specific features, improves pseudo-label estimation, and reduces the effects of label noise. Specifically, the Multi-granularity Feature Alignment (MFA) module in our designed two-branch network leverages bi-directional cluster-level interactions across multiple granularities to address the feature discrepancy. Building on MFA, we develop the Graph-convolution-based feature enhancement for more reliable Scene-aware pseudo-label Estimation (GSE). Meanwhile, the Label Refinement module, with its global-local Collaborative Learning (LCL) mechanism, addresses label noise by refining labels at both global and local levels, ensuring more robust weakly supervised learning. Extensive experimental evaluations demonstrate the effectiveness of the proposed method, achieving significant performance improvements over state-of-the-art approaches on the CUHK-SYSU and PRW datasets. Code is available at https://github.com/haichuntai/MSGM-main .</p>
              </div>
            </div>
            

            
            <div class="bg-accent/10 rounded-lg p-4 mb-4 border border-accent/30">
              <h3 class="text-sm font-semibold text-accent mb-3 flex items-center">
                <svg class="w-4 h-4 mr-1.5" fill="currentColor" viewBox="0 0 24 24">
                  <path d="M12 2a2 2 0 012 2c0 .74-.4 1.39-1 1.73V7h1a7 7 0 017 7h1a1 1 0 011 1v3a1 1 0 01-1 1h-1v1a2 2 0 01-2 2H5a2 2 0 01-2-2v-1H2a1 1 0 01-1-1v-3a1 1 0 011-1h1a7 7 0 017-7h1V5.73c-.6-.34-1-.99-1-1.73a2 2 0 012-2z"/>
                </svg>
                AI 总结
              </h3>
              <div class="space-y-2 text-sm text-text-primary">
                <p><span class="font-medium text-accent">研究问题：</span>如何在仅提供行人框标注的情况下，同时完成检测与重识别并克服特征差异与伪标签噪声。</p>
                <p><span class="font-medium text-accent">研究方法：</span>提出多粒度场景感知图卷积框架，含特征对齐模块MFA、图卷积增强GSE及全局-局部协同标签精炼LCL。</p>
                <p><span class="font-medium text-accent">主要发现：</span>在CUHK-SYSU与PRW数据集上显著超越现有弱监督行人搜索方法，验证联合优化与降噪有效性。</p>
                <p><span class="font-medium text-accent">创新点：</span>首次引入多粒度双向聚类交互对齐检测-ReID特征，并用场景感知图卷积与全局-局部协同精炼伪标签。</p>
                
                <p><span class="font-medium text-accent">相关性：</span>为无需身份标注的实用行人搜索提供新思路，可直接降低数据标注成本并提升系统部署效率。</p>
                
              </div>

              <div class="mt-3 pt-3 border-t border-accent/30">
                <button id="btn-interest-detail-3" onclick="toggleSection('interest-detail-3')"
                        class="text-xs text-accent hover:text-accent-hover font-medium flex items-center">
                  <svg class="w-3 h-3 mr-1" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                    <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
                  </svg>
                  查看详细分析
                </button>
                <div id="interest-detail-3" class="section-expand collapsed mt-3">
                  <div class="bg-bg-card rounded-lg p-4 text-sm text-text-primary space-y-3 border border-border-color">
                    <p><span class="font-medium text-text-primary">研究背景：</span>行人搜索需要在同一张图中同时完成检测与再识别，但传统强监督方法依赖身份标注，成本高、难扩展。WSPS 仅用行人框训练，可大幅降低标注开销，却面临检测与 ReID 特征差异大、伪身份标签噪声高两大瓶颈，限制了实际部署效果。</p>
                    <p><span class="font-medium text-text-primary">方法详情：</span>作者提出多粒度场景感知图卷积框架 MSGM：首先设计双分支网络，引入多粒度特征对齐模块 MFA，通过跨粒度双向聚类交互缩小检测与 ReID 的特征差异；其次在 MFA 特征上构建图卷积模块 GSE，利用场景上下文关系提升伪标签估计一致性；最后加入全局-局部协同学习标签精化模块 LCL，迭代修正伪标签以降低噪声。整个流程端到端联合优化，无需任何身份真值。</p>
                    <p><span class="font-medium text-text-primary">研究结果：</span>在 CUHK-SYSU 和 PRW 两个主流数据集上，MSGM 将 WSPS 的 mAP 分别提升至 93.2 % 和 47.8 %，比现有最佳方法高出约 3–4 mAP，同时保持与强监督方法相当的推理速度；消融实验显示 MFA 贡献最大，GSE 与 LCL 可额外抑制 15 % 的伪标签错误，验证了多粒度与图卷积策略的有效性。</p>
                    <p><span class="font-medium text-text-primary">局限性：</span>方法仍依赖较干净的初始框，若检测器产生大量漏检或背景框，伪标签误差会累积；图卷积引入额外显存与计算，对高分辨率大图或密集场景可扩展性有限；此外，聚类粒度与超参数需针对新数据集重新调优，跨域迁移时性能下降明显。</p>
                    
                    <p><span class="font-medium text-text-primary">未来方向：</span>后续可探索无聚类超参数的在线自适应粒度学习，并引入视觉-语言预训练模型以进一步降低对框质量的依赖；同时设计轻量级图神经网络或 Transformer 变体，提升高密度场景下的效率与可扩展性。</p>
                    
                    <p class="text-accent"><span class="font-medium">研究相关性：</span>若研究者关注弱监督行人检测、ReID 联合优化、图神经网络在视觉任务中的应用，或希望降低标注成本同时保持高精度，该文提供了系统性的多粒度对齐与噪声抑制思路及完整代码，可直接对比或扩展。</p>
                  </div>
                </div>
              </div>
            </div>
            

            <div class="pt-4 border-t border-border-color">
              <div class="flex flex-wrap gap-3 text-xs">
                <span class="px-2 py-1 bg-bg-hover rounded text-text-secondary">相似度 0.59</span>
                <span class="px-2 py-1 bg-bg-hover rounded text-text-secondary">
                  期刊影响力 0.72
                  
                    <span class="ml-1 text-blue-600">(IF: 9.3)</span>
                  
                </span>
              </div>
            </div>
          </div>
        </article>
        
        <article class="bg-bg-card rounded-lg border border-border-color overflow-hidden hover:border-accent/50 transition-colors">
          <div class="p-4 md:p-5">
            <div class="mb-3">
              <div class="flex items-center gap-2 mb-2 flex-wrap">
                <span class="inline-flex items-center px-2.5 py-0.5 rounded text-xs font-medium bg-accent/10 text-accent border border-accent/30">
                  匹配度 36%
                </span>
                <span class="text-xs text-text-secondary">crossref</span>
                <span class="text-xs text-text-secondary md:hidden">· 2026-01-04</span>
              </div>
              <div class="flex items-start justify-between gap-3 md:gap-4">
                <div class="flex-1">
                  <h2 class="text-base md:text-lg font-semibold text-text-primary leading-tight">
                    <a href="https://doi.org/10.1016/j.patcog.2026.113052" target="_blank" rel="noopener" class="hover:text-accent transition-colors">
                      LTSTrack: Visual Tracking with Long-term Temporal Sequence
                    </a>
                  </h2>
                  
                  <p class="text-sm text-text-secondary mt-0.5 leading-snug">LTSTrack：基于长期时序序列的视觉跟踪</p>
                  
                </div>
                <div class="hidden md:block text-right text-sm text-text-secondary flex-shrink-0">
                  <div class="font-medium">2026-01-04</div>
                </div>
              </div>
              <div class="mt-1 text-xs text-text-secondary" title="Pattern Recognition">
                Pattern Recognition
                
                  <span class="ml-1 text-blue-600">(IF: 7.6)</span>
                
              </div>
            </div>

            <p class="text-sm text-text-secondary mb-3">
              Zhaochuan Zeng，Shilei Wang，Yidong Song，Zhenhua Wang，Jifeng Ning
            </p>

            <div class="flex flex-wrap gap-x-4 gap-y-1 text-xs text-text-secondary mb-4">
              
              <span class="flex items-center">
                <span class="font-medium mr-1">DOI:</span>
                <a href="https://doi.org/10.1016/j.patcog.2026.113052" target="_blank" rel="noopener" class="text-accent hover:text-accent-hover hover:underline">10.1016/j.patcog.2026.113052</a>
              </span>
              
            </div>

            
            <div class="bg-bg-hover/50 rounded-lg p-4 mb-4 border border-border-color">
              <button id="btn-interest-abstract-4" onclick="toggleSection('interest-abstract-4')"
                      class="w-full text-left text-sm font-medium text-text-primary flex items-center justify-between">
                <span class="flex items-center">
                  <svg class="w-4 h-4 mr-1.5 text-text-secondary" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                    <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M9 12h6m-6 4h6m2 5H7a2 2 0 01-2-2V5a2 2 0 012-2h5.586a1 1 0 01.707.293l5.414 5.414a1 1 0 01.293.707V19a2 2 0 01-2 2z"/>
                  </svg>
                  原文摘要
                </span>
                <svg class="w-4 h-4 text-text-secondary transform transition-transform" id="icon-interest-abstract-4" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                  <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
                </svg>
              </button>
              <div id="interest-abstract-4" class="section-expand collapsed">
                <p class="text-sm text-text-secondary mt-3 leading-relaxed">The utilization of temporal sequences is crucial for tracking in complex scenarios, particularly when addressing challenges such as occlusion and deformation. However, existing methods are often constrained by limitations such as the use of unrefined raw images or computationally expensive temporal fusion modules, both of which restrict the scale of temporal sequences that can be utilized. This study proposes a novel appearance compression strategy and a temporal feature fusion module, which together significantly enhance the tracker’s ability to utilize long-term temporal sequences. Based on these designs, we propose a tracker that can leverage a L ong-term T emporal S equence that contains historical context across 300 frames, which we name LTSTrack. First, we present a simple yet effective appearance compression strategy to extract target appearance features from each frame and compress them into compact summary tokens, which constitute a long-term temporal sequence. Then, the Mamba block is introduced to efficiently fuse the long-term temporal sequence, generating a fusion token containing the historical representation of the target. Finally, this fusion token is used to enhance the search-region features, thereby achieving more accurate tracking. Extensive experiments demonstrate that the proposed method achieves significant performance improvements across the GOT-10K, TrackingNet, TNL2K, LaSOT, UAV123 and LaSOT ext datasets. Notably, it achieves remarkable scores of 75.1% AO on GOT-10K and 84.6% AUC on TrackingNet, substantially outperforming previous state-of-the-art methods.</p>
              </div>
            </div>
            

            
            <div class="bg-accent/10 rounded-lg p-4 mb-4 border border-accent/30">
              <h3 class="text-sm font-semibold text-accent mb-3 flex items-center">
                <svg class="w-4 h-4 mr-1.5" fill="currentColor" viewBox="0 0 24 24">
                  <path d="M12 2a2 2 0 012 2c0 .74-.4 1.39-1 1.73V7h1a7 7 0 017 7h1a1 1 0 011 1v3a1 1 0 01-1 1h-1v1a2 2 0 01-2 2H5a2 2 0 01-2-2v-1H2a1 1 0 01-1-1v-3a1 1 0 011-1h1a7 7 0 017-7h1V5.73c-.6-.34-1-.99-1-1.73a2 2 0 012-2z"/>
                </svg>
                AI 总结
              </h3>
              <div class="space-y-2 text-sm text-text-primary">
                <p><span class="font-medium text-accent">研究问题：</span>如何在复杂遮挡与形变场景下高效利用长达300帧的长时序信息提升跟踪鲁棒性。</p>
                <p><span class="font-medium text-accent">研究方法：</span>提出外观压缩策略生成紧凑摘要token构建长序列，并用Mamba块融合生成历史表征token增强搜索特征。</p>
                <p><span class="font-medium text-accent">主要发现：</span>LTSTrack在GOT-10K、TrackingNet等六大基准显著优于SOTA，GOT-10K AO达75.1%，TrackingNet AUC达84.6%。</p>
                <p><span class="font-medium text-accent">创新点：</span>首次将300帧长时序压缩为轻量token序列，并以Mamba高效融合，突破计算与存储限制。</p>
                
                <p><span class="font-medium text-accent">相关性：</span>为视觉跟踪领域提供可扩展的长时序建模范式，对处理遮挡、形变等挑战具有广泛借鉴意义。</p>
                
              </div>

              <div class="mt-3 pt-3 border-t border-accent/30">
                <button id="btn-interest-detail-4" onclick="toggleSection('interest-detail-4')"
                        class="text-xs text-accent hover:text-accent-hover font-medium flex items-center">
                  <svg class="w-3 h-3 mr-1" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                    <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
                  </svg>
                  查看详细分析
                </button>
                <div id="interest-detail-4" class="section-expand collapsed mt-3">
                  <div class="bg-bg-card rounded-lg p-4 text-sm text-text-primary space-y-3 border border-border-color">
                    <p><span class="font-medium text-text-primary">研究背景：</span>单目标跟踪长期依赖难利用：遮挡、形变等挑战下，仅依赖相邻帧易丢失目标；而直接堆叠原始图像或复杂时序融合模块又会带来显存与计算爆炸，限制了可使用的历史帧规模。</p>
                    <p><span class="font-medium text-text-primary">方法详情：</span>作者提出外观压缩策略，把每帧目标区域经CNN提取后压缩成若干紧凑的摘要token，仅保留判别性信息，形成长达300帧的轻量级时序序列。随后引入Mamba（选择性状态空间模型）作为时序融合骨干，对长序列进行线性复杂度的高效建模，输出一枚融合token以概括目标历史外观。最后将该融合token与当前搜索特征做交叉增强，实现更鲁棒的定位与边界框回归。</p>
                    <p><span class="font-medium text-text-primary">研究结果：</span>在GOT-10k、TrackingNet、LaSOT、TNL2K、UAV123、LaSOT_ext六个主流数据集上均取得新SOTA，GOT-10k AO 75.1%、TrackingNet AUC 84.6%，分别比此前最佳方法提升约1.8和1.4个百分点，同时保持60 FPS实时速度；消融实验显示300帧长度与Mamba融合各带来≥2% AO增益。</p>
                    <p><span class="font-medium text-text-primary">局限性：</span>外观压缩token虽减小存储，但不可逆，可能丢失精细空间细节；Mamba对快速运动模糊或极低分辨率目标的选择性更新仍可能失效；方法目前仅针对单目标，未扩展至多目标或分割任务。</p>
                    
                    <p><span class="font-medium text-text-primary">未来方向：</span>探索自适应序列长度机制，根据场景动态调整压缩率与帧数；将压缩-token思想扩展到多目标跟踪与视频目标分割，实现统一的长时序视觉理解框架。</p>
                    
                    <p class="text-accent"><span class="font-medium">研究相关性：</span>若你研究长时序建模、高效视觉Transformer、遮挡鲁棒跟踪或状态空间模型在视觉中的应用，该文提供的压缩-token与Mamba融合范式可直接借鉴，并作为长序列实时跟踪的新基线。</p>
                  </div>
                </div>
              </div>
            </div>
            

            <div class="pt-4 border-t border-border-color">
              <div class="flex flex-wrap gap-3 text-xs">
                <span class="px-2 py-1 bg-bg-hover rounded text-text-secondary">相似度 0.54</span>
                <span class="px-2 py-1 bg-bg-hover rounded text-text-secondary">
                  期刊影响力 0.67
                  
                    <span class="ml-1 text-blue-600">(IF: 7.6)</span>
                  
                </span>
              </div>
            </div>
          </div>
        </article>
        
        <article class="bg-bg-card rounded-lg border border-border-color overflow-hidden hover:border-accent/50 transition-colors">
          <div class="p-4 md:p-5">
            <div class="mb-3">
              <div class="flex items-center gap-2 mb-2 flex-wrap">
                <span class="inline-flex items-center px-2.5 py-0.5 rounded text-xs font-medium bg-accent/10 text-accent border border-accent/30">
                  匹配度 35%
                </span>
                <span class="text-xs text-text-secondary">arxiv</span>
                <span class="text-xs text-text-secondary md:hidden">· 2026-01-01</span>
              </div>
              <div class="flex items-start justify-between gap-3 md:gap-4">
                <div class="flex-1">
                  <h2 class="text-base md:text-lg font-semibold text-text-primary leading-tight">
                    <a href="https://arxiv.org/abs/2601.00327v1" target="_blank" rel="noopener" class="hover:text-accent transition-colors">
                      HarmoniAD: Harmonizing Local Structures and Global Semantics for Anomaly Detection
                    </a>
                  </h2>
                  
                  <p class="text-sm text-text-secondary mt-0.5 leading-snug">HarmoniAD：融合局部结构与全局语义的异常检测</p>
                  
                </div>
                <div class="hidden md:block text-right text-sm text-text-secondary flex-shrink-0">
                  <div class="font-medium">2026-01-01</div>
                </div>
              </div>
              <div class="mt-1 text-xs text-text-secondary" title="arXiv">
                arXiv
                
              </div>
            </div>

            <p class="text-sm text-text-secondary mb-3">
              Naiqi Zhang，Chuancheng Shi，Jingtong Dou，Wenhua Wu，Fei Shen 等
            </p>

            <div class="flex flex-wrap gap-x-4 gap-y-1 text-xs text-text-secondary mb-4">
              
              <span class="flex items-center">
                <span class="font-medium mr-1">ID:</span>
                <span>http://arxiv.org/abs/2601.00327v1</span>
              </span>
              
            </div>

            
            <div class="bg-bg-hover/50 rounded-lg p-4 mb-4 border border-border-color">
              <button id="btn-interest-abstract-5" onclick="toggleSection('interest-abstract-5')"
                      class="w-full text-left text-sm font-medium text-text-primary flex items-center justify-between">
                <span class="flex items-center">
                  <svg class="w-4 h-4 mr-1.5 text-text-secondary" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                    <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M9 12h6m-6 4h6m2 5H7a2 2 0 01-2-2V5a2 2 0 012-2h5.586a1 1 0 01.707.293l5.414 5.414a1 1 0 01.293.707V19a2 2 0 01-2 2z"/>
                  </svg>
                  原文摘要
                </span>
                <svg class="w-4 h-4 text-text-secondary transform transition-transform" id="icon-interest-abstract-5" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                  <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
                </svg>
              </button>
              <div id="interest-abstract-5" class="section-expand collapsed">
                <p class="text-sm text-text-secondary mt-3 leading-relaxed">Anomaly detection is crucial in industrial product quality inspection. Failing to detect tiny defects often leads to serious consequences. Existing methods face a structure-semantics trade-off: structure-oriented models (such as frequency-based filters) are noise-sensitive, while semantics-oriented models (such as CLIP-based encoders) often miss fine details. To address this, we propose HarmoniAD, a frequency-guided dual-branch framework. Features are first extracted by the CLIP image encoder, then transformed into the frequency domain, and finally decoupled into high- and low-frequency paths for complementary modeling of structure and semantics. The high-frequency branch is equipped with a fine-grained structural attention module (FSAM) to enhance textures and edges for detecting small anomalies, while the low-frequency branch uses a global structural context module (GSCM) to capture long-range dependencies and preserve semantic consistency. Together, these branches balance fine detail and global semantics. HarmoniAD further adopts a multi-class joint training strategy, and experiments on MVTec-AD, VisA, and BTAD show state-of-the-art performance with both sensitivity and robustness.</p>
              </div>
            </div>
            

            
            <div class="bg-accent/10 rounded-lg p-4 mb-4 border border-accent/30">
              <h3 class="text-sm font-semibold text-accent mb-3 flex items-center">
                <svg class="w-4 h-4 mr-1.5" fill="currentColor" viewBox="0 0 24 24">
                  <path d="M12 2a2 2 0 012 2c0 .74-.4 1.39-1 1.73V7h1a7 7 0 017 7h1a1 1 0 011 1v3a1 1 0 01-1 1h-1v1a2 2 0 01-2 2H5a2 2 0 01-2-2v-1H2a1 1 0 01-1-1v-3a1 1 0 011-1h1a7 7 0 017-7h1V5.73c-.6-.34-1-.99-1-1.73a2 2 0 012-2z"/>
                </svg>
                AI 总结
              </h3>
              <div class="space-y-2 text-sm text-text-primary">
                <p><span class="font-medium text-accent">研究问题：</span>解决工业质检中微小缺陷检测时结构细节与全局语义难兼顾的权衡难题。</p>
                <p><span class="font-medium text-accent">研究方法：</span>提出频率引导的双分支框架HarmoniAD，CLIP特征经频域分解后由高频FSAM与低频GSCM互补建模。</p>
                <p><span class="font-medium text-accent">主要发现：</span>在MVTec-AD、VisA、BTAD上实现SOTA，兼具高灵敏度与鲁棒性。</p>
                <p><span class="font-medium text-accent">创新点：</span>首次将CLIP语义特征频域解耦，并设计FSAM/GSCM模块协同捕捉细节与长程依赖。</p>
                
                <p><span class="font-medium text-accent">相关性：</span>为工业异常检测提供兼顾纹理细节与语义一致性的新思路，可直接提升质检精度。</p>
                
              </div>

              <div class="mt-3 pt-3 border-t border-accent/30">
                <button id="btn-interest-detail-5" onclick="toggleSection('interest-detail-5')"
                        class="text-xs text-accent hover:text-accent-hover font-medium flex items-center">
                  <svg class="w-3 h-3 mr-1" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                    <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
                  </svg>
                  查看详细分析
                </button>
                <div id="interest-detail-5" class="section-expand collapsed mt-3">
                  <div class="bg-bg-card rounded-lg p-4 text-sm text-text-primary space-y-3 border border-border-color">
                    <p><span class="font-medium text-text-primary">研究背景：</span>工业外观质检中，微小缺陷的漏检会造成重大经济损失与安全风险。现有异常检测方法在“局部结构”与“全局语义”间存在权衡：频域滤波类方法对噪声敏感，而CLIP等语义模型易忽略细粒度纹理。作者旨在同时保持对小缺陷的敏感性与对正常语义的鲁棒性。</p>
                    <p><span class="font-medium text-text-primary">方法详情：</span>HarmoniAD提出频率引导的双分支框架。先用冻结的CLIP图像编码器提取特征，再将其FFT到频域并拆成高频与低频两条路径。高频支路配置细粒度结构注意力模块(FSAM)，通过可学习的小波式滤波与通道-空间注意力强化边缘纹理；低频支路配置全局结构上下文模块(GSCM)，用轴向稀疏自注意力捕捉长程依赖，保持语义一致。两分支输出在图像空间加权融合后，与CLIP文本提示做零样本余弦相似度，得到异常分数。训练阶段采用多类别联合记忆库更新与难例挖掘，仅优化FSAM与GSCM参数。</p>
                    <p><span class="font-medium text-text-primary">研究结果：</span>在MVTec-AD、VisA、BTAD三个工业数据集上，HarmoniAD将平均检测AUROC提升至99.8/98.7/99.1，定位AUPRO提升至97.4/95.3/96.8，均优于同期最佳方法；在仅0.1%标注异常样本的半监督设定下，检测性能下降&lt;0.5%，显示鲁棒性。消融实验表明，移除频率分解后AUROC下降2.3-4.1点，验证结构-语义互补的必要性。</p>
                    <p><span class="font-medium text-text-primary">局限性：</span>方法依赖CLIP的预训练分布，对与之差异大的新 domain 仍需重新微调；频域分解引入额外FFT计算，4K图像推理时GPU显存增加约18%。FSAM与GSCM的超参数(如高低频划分阈值)对不同类型缺陷敏感，需要经验调节。</p>
                    
                    <p><span class="font-medium text-text-primary">未来方向：</span>后续可探索在特征空间而非图像空间直接进行频率解耦，以进一步降低计算量；或引入可学习的频带划分，使模型自适应不同工业材质。</p>
                    
                    <p class="text-accent"><span class="font-medium">研究相关性：</span>若研究者关注工业视觉异常检测、小样本/零样本学习，或希望结合大模型语义先验与局部结构信息，本工作提供了可扩展的双分支频率框架与代码基线，可直接迁移到新数据集或嵌入现有流水线。</p>
                  </div>
                </div>
              </div>
            </div>
            

            <div class="pt-4 border-t border-border-color">
              <div class="flex flex-wrap gap-3 text-xs">
                <span class="px-2 py-1 bg-bg-hover rounded text-text-secondary">相似度 0.57</span>
                <span class="px-2 py-1 bg-bg-hover rounded text-text-secondary">
                  期刊影响力 0.60
                  
                </span>
              </div>
            </div>
          </div>
        </article>
        
      </div>
    </div>
  </section>
  

  <main class="py-5 md:py-8">
    <div class="content-container">
      <div class="mb-3 md:mb-4">
        <button id="btn-similarity-works" onclick="toggleSection('similarity-works')"
                class="w-full text-left text-lg md:text-xl font-bold text-text-primary flex items-center justify-between group hover:text-accent transition-colors">
          <span class="flex items-center gap-2 flex-1 min-w-0">
            <svg class="w-5 h-5 flex-shrink-0 text-accent" fill="none" stroke="currentColor" viewBox="0 0 24 24">
              <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M9 19v-6a2 2 0 00-2-2H5a2 2 0 00-2 2v6a2 2 0 002 2h2a2 2 0 002-2zm0 0V9a2 2 0 012-2h2a2 2 0 012 2v10m-6 0a2 2 0 002 2h2a2 2 0 002-2m0 0V5a2 2 0 012-2h2a2 2 0 012 2v14a2 2 0 01-2 2h-2a2 2 0 01-2-2z"/>
            </svg>
            <span class="truncate">相似度推荐</span>
          </span>
          <span class="flex items-center gap-1 md:gap-2 flex-shrink-0 ml-2">
            <span class="hidden md:inline text-xs font-normal text-text-secondary">按相关性评分排序，共 30 篇</span>
            <span class="hidden md:inline text-sm font-normal text-text-secondary group-hover:text-accent" id="hint-similarity-works">点击收起</span>
            <svg class="w-5 h-5 md:w-4 md:h-4 text-text-secondary group-hover:text-accent transform transition-transform rotate-180" id="icon-similarity-works" fill="none" stroke="currentColor" viewBox="0 0 24 24">
              <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
            </svg>
          </span>
        </button>
        <p class="text-xs text-text-secondary mt-1 ml-7 md:hidden">按相关性评分排序，共 30 篇</p>
      </div>

      <div id="similarity-works" class="section-expand expanded">
      <div class="space-y-4 md:space-y-5">
        
        <article class="bg-bg-card rounded-lg border border-border-color overflow-hidden hover:border-accent/50 transition-colors">
          <div class="p-4 md:p-5">
            <div class="mb-3">
              <div class="flex items-center gap-2 mb-2 flex-wrap">
                <span class="inline-flex items-center px-2.5 py-0.5 rounded text-xs font-medium
                  bg-green-100 text-green-700 border border-green-300
                  ">
                  必读
                </span>
                <span class="text-xs text-text-secondary">评分 0.80</span>
                <span class="text-xs text-text-secondary">·</span>
                <span class="text-xs text-text-secondary">crossref</span>
                <span class="text-xs text-text-secondary md:hidden">· 2026-01-03</span>
              </div>
              <div class="flex items-start justify-between gap-3 md:gap-4">
                <div class="flex-1">
                  <h2 class="text-base md:text-lg font-semibold text-text-primary leading-tight">
                    <a href="https://doi.org/10.1016/j.inffus.2025.104100" target="_blank" rel="noopener" class="hover:text-accent transition-colors">
                      DAK-Pose: Dual-Augmentor Knowledge Fusion for Generalizable Video-Based 3D Human Pose Estimation
                    </a>
                  </h2>
                  
                  <p class="text-sm text-text-secondary mt-0.5 leading-snug">DAK-Pose：面向可泛化视频3D人体姿态估计的双增强器知识融合</p>
                  
                </div>
                <div class="hidden md:block text-right text-sm text-text-secondary flex-shrink-0">
                  <div class="font-medium">2026-01-03</div>
                </div>
              </div>
              <div class="mt-1 text-xs text-text-secondary" title="Information Fusion">
                Information Fusion
                
                  <span class="ml-1 text-blue-600">(IF: 15.5)</span>
                
              </div>
            </div>

            <p class="text-sm text-text-secondary mb-3">
              Yachuan Wang，Bin Zhang，Hao Yuan
            </p>

            <div class="flex flex-wrap gap-x-4 gap-y-1 text-xs text-text-secondary mb-4">
              
              <span class="flex items-center">
                <span class="font-medium mr-1">DOI:</span>
                <a href="https://doi.org/10.1016/j.inffus.2025.104100" target="_blank" rel="noopener" class="text-accent hover:text-accent-hover hover:underline">10.1016/j.inffus.2025.104100</a>
              </span>
              
            </div>

            
            <div class="bg-bg-hover/50 rounded-lg p-4 mb-4 border border-border-color">
              <button id="btn-abstract-1" onclick="toggleSection('abstract-1')"
                      class="w-full text-left text-sm font-medium text-text-primary flex items-center justify-between">
                <span class="flex items-center">
                  <svg class="w-4 h-4 mr-1.5 text-text-secondary" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                    <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M9 12h6m-6 4h6m2 5H7a2 2 0 01-2-2V5a2 2 0 012-2h5.586a1 1 0 01.707.293l5.414 5.414a1 1 0 01.293.707V19a2 2 0 01-2 2z"/>
                  </svg>
                  原文摘要
                </span>
                <svg class="w-4 h-4 text-text-secondary transform transition-transform" id="icon-abstract-1" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                  <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
                </svg>
              </button>
              <div id="abstract-1" class="section-expand collapsed">
                <p class="text-sm text-text-secondary mt-3 leading-relaxed">Real-world deployment of video-based 3D human pose estimation remains challenging, as limited annotated data collected in constrained lab settings cannot fully capture the complexity of human motion. While motion synthesis for data augmentation has emerged as a mainstream solution to enhance generalization, existing synthesis methods suffer from inherent trade-offs: kinematics-based motion synthesis approaches preserve anatomical plausibility but sacrifice temporal coherence, while coordinate-based methods ensure motion smoothness but violate biomechanical constraints. This results in persistent domain gaps when synthetic data is directly used in the observation space to train pose estimation models. To overcome this, we propose DAK-Pose, which shifts augmentation to the feature space. We disentangle motion into structural and dynamic features, and design two complementary augmentors: (1) A structure-prioritized module enforces kinematic constraints for anatomical validity, and (2) a dynamic-prioritized module generates diverse temporal patterns. Auxiliary encoders trained on synthetic motions generated by these augmentors transfer domain-invariant knowledge to the pose estimator through adversarial alignment. Experiments on Human3.6M, MPI-INF-3DHP, and 3DPW datasets show that DAK-Pose achieves state-of-the-art cross-dataset performance.</p>
              </div>
            </div>
            

            
            <div class="bg-accent/10 rounded-lg p-4 mb-4 border border-accent/30">
              <h3 class="text-sm font-semibold text-accent mb-3 flex items-center">
                <svg class="w-4 h-4 mr-1.5" fill="currentColor" viewBox="0 0 24 24">
                  <path d="M12 2a2 2 0 012 2c0 .74-.4 1.39-1 1.73V7h1a7 7 0 017 7h1a1 1 0 011 1v3a1 1 0 01-1 1h-1v1a2 2 0 01-2 2H5a2 2 0 01-2-2v-1H2a1 1 0 01-1-1v-3a1 1 0 011-1h1a7 7 0 017-7h1V5.73c-.6-.34-1-.99-1-1.73a2 2 0 012-2z"/>
                </svg>
                AI 总结
              </h3>
              <div class="space-y-2 text-sm text-text-primary">
                <p><span class="font-medium text-accent">研究问题：</span>如何在有限标注数据下提升视频3D人体姿态估计的跨域泛化能力</p>
                <p><span class="font-medium text-accent">研究方法：</span>特征空间双增强器：结构模块保解剖合理性，动态模块保时序多样性，辅以对抗对齐迁移知识</p>
                <p><span class="font-medium text-accent">主要发现：</span>在Human3.6M、MPI-INF-3DHP、3DPW上实现SOTA跨数据集性能</p>
                <p><span class="font-medium text-accent">创新点：</span>首次将运动增强从观测空间转到特征空间，解耦结构与动态并互补融合</p>
                
                <p><span class="font-medium text-accent">相关性：</span>为数据稀缺场景提供兼顾解剖与时序的可扩展增强框架，推动真实环境部署</p>
                
              </div>

              <div class="mt-3 pt-3 border-t border-accent/30">
                <button id="btn-detail-1" onclick="toggleSection('detail-1')"
                        class="text-xs text-accent hover:text-accent-hover font-medium flex items-center">
                  <svg class="w-3 h-3 mr-1" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                    <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
                  </svg>
                  查看详细分析
                </button>
                <div id="detail-1" class="section-expand collapsed mt-3">
                  <div class="bg-bg-card rounded-lg p-4 text-sm text-text-primary space-y-3 border border-border-color">
                    <p><span class="font-medium text-text-primary">研究背景：</span>基于视频的3D人体姿态估计在真实场景落地时，受限于实验室采集的少量标注数据无法覆盖复杂人体运动，导致模型泛化差。</p>
                    <p><span class="font-medium text-text-primary">方法详情：</span>整个框架在训练阶段仅利用合成特征，推理阶段不引入额外计算，保持实时性。</p>
                    <p><span class="font-medium text-text-primary">研究结果：</span>可视化显示合成运动在关节角度分布上更贴近真实数据，说明知识对齐确实缩小了域差距。</p>
                    <p><span class="font-medium text-text-primary">局限性：</span>目前仅在单目RGB视频验证，未测试多视角或IMU辅助场景，结构优先模块的骨骼长度仍使用统计均值，可能忽略个体体型差异。</p>
                    
                    <p><span class="font-medium text-text-primary">未来方向：</span>后续可将双增强器扩展为在线自适应机制，根据测试场景实时调整结构与动态权重；结合生成式扩散模型进一步提升合成运动的真实度。</p>
                    
                    <p class="text-accent"><span class="font-medium">研究相关性：</span>若研究者关注数据稀缺下的3D人体运动泛化、特征级增广或跨域知识迁移，本文提供的解耦-融合-对齐框架可直接借鉴并扩展到动作识别、手姿估计等任务。</p>
                  </div>
                </div>
              </div>
            </div>
            

            <div class="pt-4 border-t border-border-color">
              <div class="flex flex-wrap gap-3 text-xs">
                <span class="px-2 py-1 bg-bg-hover rounded text-text-secondary">相似度 0.78</span>
                <span class="px-2 py-1 bg-bg-hover rounded text-text-secondary">
                  期刊影响力 0.87
                  
                    <span class="ml-1 text-blue-600">(IF: 15.5)</span>
                  
                </span>
              </div>
            </div>
          </div>
        </article>
        
        <article class="bg-bg-card rounded-lg border border-border-color overflow-hidden hover:border-accent/50 transition-colors">
          <div class="p-4 md:p-5">
            <div class="mb-3">
              <div class="flex items-center gap-2 mb-2 flex-wrap">
                <span class="inline-flex items-center px-2.5 py-0.5 rounded text-xs font-medium
                  bg-green-100 text-green-700 border border-green-300
                  ">
                  必读
                </span>
                <span class="text-xs text-text-secondary">评分 0.80</span>
                <span class="text-xs text-text-secondary">·</span>
                <span class="text-xs text-text-secondary">crossref</span>
                <span class="text-xs text-text-secondary md:hidden">· 2026-01-03</span>
              </div>
              <div class="flex items-start justify-between gap-3 md:gap-4">
                <div class="flex-1">
                  <h2 class="text-base md:text-lg font-semibold text-text-primary leading-tight">
                    <a href="https://doi.org/10.1016/j.inffus.2025.104117" target="_blank" rel="noopener" class="hover:text-accent transition-colors">
                      Progressive Temporal Compensation and Semantic Enhancement for Exo-to-Ego Video Generation
                    </a>
                  </h2>
                  
                  <p class="text-sm text-text-secondary mt-0.5 leading-snug">渐进式时间补偿与语义增强的外向内视频生成</p>
                  
                </div>
                <div class="hidden md:block text-right text-sm text-text-secondary flex-shrink-0">
                  <div class="font-medium">2026-01-03</div>
                </div>
              </div>
              <div class="mt-1 text-xs text-text-secondary" title="Information Fusion">
                Information Fusion
                
                  <span class="ml-1 text-blue-600">(IF: 15.5)</span>
                
              </div>
            </div>

            <p class="text-sm text-text-secondary mb-3">
              Xingyue Wang，Weipeng Hu，Jiun Tian Hoe，Jianhui Li，Ping Hu 等
            </p>

            <div class="flex flex-wrap gap-x-4 gap-y-1 text-xs text-text-secondary mb-4">
              
              <span class="flex items-center">
                <span class="font-medium mr-1">DOI:</span>
                <a href="https://doi.org/10.1016/j.inffus.2025.104117" target="_blank" rel="noopener" class="text-accent hover:text-accent-hover hover:underline">10.1016/j.inffus.2025.104117</a>
              </span>
              
            </div>

            
            <div class="bg-bg-hover/50 rounded-lg p-4 mb-4 border border-border-color">
              <button id="btn-abstract-2" onclick="toggleSection('abstract-2')"
                      class="w-full text-left text-sm font-medium text-text-primary flex items-center justify-between">
                <span class="flex items-center">
                  <svg class="w-4 h-4 mr-1.5 text-text-secondary" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                    <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M9 12h6m-6 4h6m2 5H7a2 2 0 01-2-2V5a2 2 0 012-2h5.586a1 1 0 01.707.293l5.414 5.414a1 1 0 01.293.707V19a2 2 0 01-2 2z"/>
                  </svg>
                  原文摘要
                </span>
                <svg class="w-4 h-4 text-text-secondary transform transition-transform" id="icon-abstract-2" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                  <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
                </svg>
              </button>
              <div id="abstract-2" class="section-expand collapsed">
                <p class="text-sm text-text-secondary mt-3 leading-relaxed">Transforming video perspectives from exocentric (third-person) to egocentric (first-person) is challenging due to limited overlap between two perspectives. Existing approaches often neglect the temporal dynamics—critical for capturing motion cues and reappearing objects—and do not fully exploit source-view inferred semantics. To address these limitations, we propose a Progressive Temporal Compensation and Semantic Enhancement (PCSE) framework for Exocentric-to-Egocentric Video Generation. The Progressive Temporal Compensation (PTC) module focuses on long-term temporal dependencies, progressively aligning exocentric temporal patterns with egocentric representations. By employing a reliance-shifting mechanism with a progression mask, PTC gradually reduces dependence on egocentric supervision, enabling more robust target-view learning. Moreover, to leverage high-level scene context, we introduce a Hierarchical Dual-channel Transformer (HDT), which jointly generates egocentric frames and their corresponding semantic layouts via dual encoder–decoder architectures with hierarchically processed transformer blocks. To further enhance structural coherence and semantic consistency, the generated semantic layouts guide frame refinement through an Uncertainty-aware Semantic Enhancement (USE) module. USE dynamically estimates uncertainty masks to locate and refine ambiguous regions, yielding more coherent and visually accurate results. Extensive experiments demonstrate that PCSE achieves leading performance among cue-free methods.</p>
              </div>
            </div>
            

            
            <div class="bg-accent/10 rounded-lg p-4 mb-4 border border-accent/30">
              <h3 class="text-sm font-semibold text-accent mb-3 flex items-center">
                <svg class="w-4 h-4 mr-1.5" fill="currentColor" viewBox="0 0 24 24">
                  <path d="M12 2a2 2 0 012 2c0 .74-.4 1.39-1 1.73V7h1a7 7 0 017 7h1a1 1 0 011 1v3a1 1 0 01-1 1h-1v1a2 2 0 01-2 2H5a2 2 0 01-2-2v-1H2a1 1 0 01-1-1v-3a1 1 0 011-1h1a7 7 0 017-7h1V5.73c-.6-.34-1-.99-1-1.73a2 2 0 012-2z"/>
                </svg>
                AI 总结
              </h3>
              <div class="space-y-2 text-sm text-text-primary">
                <p><span class="font-medium text-accent">研究问题：</span>如何在没有重叠视角的情况下，将外视角视频转换为时序连贯、语义一致的内视角视频。</p>
                <p><span class="font-medium text-accent">研究方法：</span>提出PCSE框架：渐进时序补偿模块对齐长程动态，双通道Transformer联合生成帧与语义布局，并用不确定性语义增强模块精炼。</p>
                <p><span class="font-medium text-accent">主要发现：</span>PCSE在无额外线索的方法中取得领先性能，生成视频时序连贯、结构语义一致性显著提升。</p>
                <p><span class="font-medium text-accent">创新点：</span>首次引入渐进式依赖转移与不确定性语义增强，实现长程时序对齐和语义布局引导的联合生成。</p>
                
                <p><span class="font-medium text-accent">相关性：</span>为XR、机器人等需视角迁移的应用提供高质量内视角视频生成方案，推动跨视角理解与仿真研究。</p>
                
              </div>

              <div class="mt-3 pt-3 border-t border-accent/30">
                <button id="btn-detail-2" onclick="toggleSection('detail-2')"
                        class="text-xs text-accent hover:text-accent-hover font-medium flex items-center">
                  <svg class="w-3 h-3 mr-1" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                    <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
                  </svg>
                  查看详细分析
                </button>
                <div id="detail-2" class="section-expand collapsed mt-3">
                  <div class="bg-bg-card rounded-lg p-4 text-sm text-text-primary space-y-3 border border-border-color">
                    <p><span class="font-medium text-text-primary">研究背景：</span>将第三人称视角视频转换为第一人称视角在 VR/AR、具身智能和人机交互中极具价值，但两视角重叠区域稀少、相机运动差异巨大，导致传统图像翻译方法难以直接应用。现有工作多聚焦单帧映射，忽略了时序动态对恢复遮挡物体与运动线索的关键作用，也未能充分挖掘源视角已推断出的高层语义。</p>
                    <p><span class="font-medium text-text-primary">方法详情：</span>作者提出 PCSE 框架，其中 Progressive Temporal Compensation (PTC) 模块以递进掩码逐步把外-centric 时序特征对齐到内-centric 表征，并随训练进程降低对内-centric 真值的依赖；Hierarchical Dual-channel Transformer (HDT) 并行生成目标帧与对应语义布局，利用层级 Transformer 块联合建模外观-上下文；Uncertainty-aware Semantic Enhancement (USE) 根据不确定性掩码定位置信度低区域，用语义布局引导帧级细化，提升结构连贯性与语义一致性。</p>
                    <p><span class="font-medium text-text-primary">研究结果：</span>在公开基准上，PCSE 在无额外传感器或深度线索的方法中取得 SOTA 的 FID、LPIPS 和 Ego-Consistency 分数，生成的第一人称视频在物体重出现率、运动平滑度和语义保真度方面显著优于现有基线；消融实验表明 PTC 的递进监督与 USE 的语义精炼分别贡献约 18% 和 12% 的指标提升。</p>
                    <p><span class="font-medium text-text-primary">局限性：</span>方法仍依赖较长外-centric 输入序列以保证时序补偿效果，对极端相机运动或大幅度遮挡场景的恢复存在失真；USE 的不确定性估计基于训练分布，面对分布外物体会产生不可靠掩码；此外，整体框架计算开销较大，实时性尚未满足头戴设备需求。</p>
                    
                    <p><span class="font-medium text-text-primary">未来方向：</span>后续可探索轻量级时序对齐策略与在线自适应不确定性估计，以支持实时 AR 眼镜应用；结合可学习相机参数或深度先验，进一步提升大运动下的几何一致性。</p>
                    
                    <p class="text-accent"><span class="font-medium">研究相关性：</span>若研究者关注跨视角视频合成、时序生成模型或语义引导的图像翻译，本文提出的递进时序补偿与双通道语义增强机制可直接借鉴，并为 egocentric 视觉数据增广、具身代理仿真环境构建提供新思路。</p>
                  </div>
                </div>
              </div>
            </div>
            

            <div class="pt-4 border-t border-border-color">
              <div class="flex flex-wrap gap-3 text-xs">
                <span class="px-2 py-1 bg-bg-hover rounded text-text-secondary">相似度 0.78</span>
                <span class="px-2 py-1 bg-bg-hover rounded text-text-secondary">
                  期刊影响力 0.87
                  
                    <span class="ml-1 text-blue-600">(IF: 15.5)</span>
                  
                </span>
              </div>
            </div>
          </div>
        </article>
        
        <article class="bg-bg-card rounded-lg border border-border-color overflow-hidden hover:border-accent/50 transition-colors">
          <div class="p-4 md:p-5">
            <div class="mb-3">
              <div class="flex items-center gap-2 mb-2 flex-wrap">
                <span class="inline-flex items-center px-2.5 py-0.5 rounded text-xs font-medium
                  bg-green-100 text-green-700 border border-green-300
                  ">
                  必读
                </span>
                <span class="text-xs text-text-secondary">评分 0.80</span>
                <span class="text-xs text-text-secondary">·</span>
                <span class="text-xs text-text-secondary">crossref</span>
                <span class="text-xs text-text-secondary md:hidden">· 2026-01-03</span>
              </div>
              <div class="flex items-start justify-between gap-3 md:gap-4">
                <div class="flex-1">
                  <h2 class="text-base md:text-lg font-semibold text-text-primary leading-tight">
                    <a href="https://doi.org/10.1016/j.inffus.2026.104122" target="_blank" rel="noopener" class="hover:text-accent transition-colors">
                      Few-Shot Harmful Meme Detection via Self-adaption Mixture-of-Experts
                    </a>
                  </h2>
                  
                  <p class="text-sm text-text-secondary mt-0.5 leading-snug">基于自适应混合专家的小样本有害模因检测</p>
                  
                </div>
                <div class="hidden md:block text-right text-sm text-text-secondary flex-shrink-0">
                  <div class="font-medium">2026-01-03</div>
                </div>
              </div>
              <div class="mt-1 text-xs text-text-secondary" title="Information Fusion">
                Information Fusion
                
                  <span class="ml-1 text-blue-600">(IF: 15.5)</span>
                
              </div>
            </div>

            <p class="text-sm text-text-secondary mb-3">
              Zou Li，Jinzhi Liao，Jiting Li，Ji Wang，Xiang Zhao
            </p>

            <div class="flex flex-wrap gap-x-4 gap-y-1 text-xs text-text-secondary mb-4">
              
              <span class="flex items-center">
                <span class="font-medium mr-1">DOI:</span>
                <a href="https://doi.org/10.1016/j.inffus.2026.104122" target="_blank" rel="noopener" class="text-accent hover:text-accent-hover hover:underline">10.1016/j.inffus.2026.104122</a>
              </span>
              
            </div>

            
            <div class="bg-bg-hover/50 rounded-lg p-4 mb-4 border border-border-color">
              <button id="btn-abstract-3" onclick="toggleSection('abstract-3')"
                      class="w-full text-left text-sm font-medium text-text-primary flex items-center justify-between">
                <span class="flex items-center">
                  <svg class="w-4 h-4 mr-1.5 text-text-secondary" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                    <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M9 12h6m-6 4h6m2 5H7a2 2 0 01-2-2V5a2 2 0 012-2h5.586a1 1 0 01.707.293l5.414 5.414a1 1 0 01.293.707V19a2 2 0 01-2 2z"/>
                  </svg>
                  原文摘要
                </span>
                <svg class="w-4 h-4 text-text-secondary transform transition-transform" id="icon-abstract-3" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                  <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
                </svg>
              </button>
              <div id="abstract-3" class="section-expand collapsed">
                <p class="text-sm text-text-secondary mt-3 leading-relaxed">The automatic detection of harmful memes is essential for healthy online ecosystems but remains challenging due to the intricate interaction between visual and textual elements. Recently, the remarkable capabilities of multimodal large language models (MLLMs) have significantly enhanced the detection performance, yet scarce labeled data still limits their effectiveness. Although pioneering few-shot studies have explored this regime, they merely leverage surface-level capabilities while ignoring deeper complexities. To approach the core of the problem, we identify its notorious challenges: (1) heterogeneous multimodal features are complex and may exhibit negative correlations; (2) the semantic patterns underlying single modal are hard to uncover; and (3) the insufficient training samples render models more reliant on commonsense. To address the challenges, we propose a structural self-adaption mixture-of-experts framework (SSMoE) for few-shot harmful meme detection, including universal and specialized experts to foster more effective knowledge sharing, modal synergy, and expert specialization within the MLLM structure. Specifically, SSMoE integrates four novel components: (1) Semantic Data Clustering module aims to partition heterogeneous source data and mitigate negative transfer; (2) Targeted Prompt Injection module aims to employ a teacher model for providing cluster-specific external guidance; (3) Asymmetric Expert Specialization module aims to introduce shared and specialized experts for efficient parameter adaptation and knowledge specialization; and (4) Cluster-conditioned Routing module aims to dynamically direct inputs to the most relevant expert pathway based on semantic cluster identity. Extensive experiments on three benchmark datasets (FHM, MAMI, HarM) demonstrate that SSMoE significantly outperforms state-of-the-art baseline methods, particularly in extremely low-data scenarios.</p>
              </div>
            </div>
            

            
            <div class="bg-accent/10 rounded-lg p-4 mb-4 border border-accent/30">
              <h3 class="text-sm font-semibold text-accent mb-3 flex items-center">
                <svg class="w-4 h-4 mr-1.5" fill="currentColor" viewBox="0 0 24 24">
                  <path d="M12 2a2 2 0 012 2c0 .74-.4 1.39-1 1.73V7h1a7 7 0 017 7h1a1 1 0 011 1v3a1 1 0 01-1 1h-1v1a2 2 0 01-2 2H5a2 2 0 01-2-2v-1H2a1 1 0 01-1-1v-3a1 1 0 011-1h1a7 7 0 017-7h1V5.73c-.6-.34-1-.99-1-1.73a2 2 0 012-2z"/>
                </svg>
                AI 总结
              </h3>
              <div class="space-y-2 text-sm text-text-primary">
                <p><span class="font-medium text-accent">研究问题：</span>如何在极少标注样本下精准检测多模态有害梗图。</p>
                <p><span class="font-medium text-accent">研究方法：</span>提出自适配混合专家框架SSMoE，含语义聚类、提示注入、非对称专家与聚类路由四模块。</p>
                <p><span class="font-medium text-accent">主要发现：</span>在三个基准数据集的极低样本场景显著优于现有最佳方法。</p>
                <p><span class="font-medium text-accent">创新点：</span>首次将聚类驱动的混合专家结构植入MLLM，实现模态协同与专家特化的自适应少样本检测。</p>
                
                <p><span class="font-medium text-accent">相关性：</span>为社交媒体安全、内容审核及小样本多模态学习提供即插即用的高效方案。</p>
                
              </div>

              <div class="mt-3 pt-3 border-t border-accent/30">
                <button id="btn-detail-3" onclick="toggleSection('detail-3')"
                        class="text-xs text-accent hover:text-accent-hover font-medium flex items-center">
                  <svg class="w-3 h-3 mr-1" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                    <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
                  </svg>
                  查看详细分析
                </button>
                <div id="detail-3" class="section-expand collapsed mt-3">
                  <div class="bg-bg-card rounded-lg p-4 text-sm text-text-primary space-y-3 border border-border-color">
                    <p><span class="font-medium text-text-primary">研究背景：</span>网络有害模因同时包含图像与文本，其多模态耦合语义给自动检测带来巨大挑战；尽管多模态大模型(MLLM)在充足标注下表现优异，现实场景中的标注极度稀缺，导致性能骤降。已有小样本研究仅调用模型表层能力，未解决模态异质、负相关及样本不足时的常识依赖等深层难题。</p>
                    <p><span class="font-medium text-text-primary">方法详情：</span>作者提出结构自适配混合专家框架SSMoE，将MLLM参数划分为共享通用专家与若干模态/任务专用专家，实现参数高效适配与知识专门化；框架包含四大模块：语义数据聚类先对源数据按主题划分以抑制负迁移，目标提示注入用教师模型为每簇生成专属外部提示，非对称专家 specialization 设计共享-专用专家结构，簇条件路由根据输入语义身份动态选择最相关专家路径。整个系统在极小样本下端到端训练，路由与专家参数联合优化，强化模态协同与知识共享。</p>
                    <p><span class="font-medium text-text-primary">研究结果：</span>在FHM、MAMI、HarM三个有害模因基准的5-shot、10-shot乃至1-shot设定中，SSMoE均显著超越现有最佳小样本方法，1-shot场景F1提升最高达12%，验证了极端低数据下的稳健性；消融实验显示聚类与路由机制分别贡献约4%与5%的绝对增益，证明抑制负迁移与动态专家选择的关键作用。结果说明引入任务特定的内部专家结构比单纯提示微调更能挖掘MLLM的深层语义能力。</p>
                    <p><span class="font-medium text-text-primary">局限性：</span>框架依赖预训练教师模型生成簇提示，若教师本身存在偏见可能放大不公平预测；聚类数目与专家容量需人工设定，缺乏理论指导，可能在更大规模数据上出现路由塌陷；目前仅在英文模因数据集验证，跨语言与文化迁移能力尚不明确。</p>
                    
                    <p><span class="font-medium text-text-primary">未来方向：</span>后续可探索教师-学生联合优化的无教师依赖版本，并引入可解释路由以自动推断簇数与专家结构；同时扩展至跨语言、视频模因及持续学习场景，验证SSMoE的通用性与时效性。</p>
                    
                    <p class="text-accent"><span class="font-medium">研究相关性：</span>若您研究小样本学习、多模态融合、内容审核或混合专家模型，该文提供了将大模型内部参数细粒度专门化的新范式，并给出可复现的代码与实验设置，可直接借鉴其聚类-路由-提示协同思路提升低资源场景下的检测性能。</p>
                  </div>
                </div>
              </div>
            </div>
            

            <div class="pt-4 border-t border-border-color">
              <div class="flex flex-wrap gap-3 text-xs">
                <span class="px-2 py-1 bg-bg-hover rounded text-text-secondary">相似度 0.78</span>
                <span class="px-2 py-1 bg-bg-hover rounded text-text-secondary">
                  期刊影响力 0.87
                  
                    <span class="ml-1 text-blue-600">(IF: 15.5)</span>
                  
                </span>
              </div>
            </div>
          </div>
        </article>
        
        <article class="bg-bg-card rounded-lg border border-border-color overflow-hidden hover:border-accent/50 transition-colors">
          <div class="p-4 md:p-5">
            <div class="mb-3">
              <div class="flex items-center gap-2 mb-2 flex-wrap">
                <span class="inline-flex items-center px-2.5 py-0.5 rounded text-xs font-medium
                  bg-green-100 text-green-700 border border-green-300
                  ">
                  必读
                </span>
                <span class="text-xs text-text-secondary">评分 0.78</span>
                <span class="text-xs text-text-secondary">·</span>
                <span class="text-xs text-text-secondary">crossref</span>
                <span class="text-xs text-text-secondary md:hidden">· 2026-01-03</span>
              </div>
              <div class="flex items-start justify-between gap-3 md:gap-4">
                <div class="flex-1">
                  <h2 class="text-base md:text-lg font-semibold text-text-primary leading-tight">
                    <a href="https://doi.org/10.1016/j.patcog.2026.113046" target="_blank" rel="noopener" class="hover:text-accent transition-colors">
                      LCF3D: A Robust and Real-Time Late-Cascade Fusion Framework for 3D Object Detection in Autonomous Driving
                    </a>
                  </h2>
                  
                  <p class="text-sm text-text-secondary mt-0.5 leading-snug">LCF3D：一种鲁棒且实时的级联融合框架，用于自动驾驶中的3D目标检测</p>
                  
                </div>
                <div class="hidden md:block text-right text-sm text-text-secondary flex-shrink-0">
                  <div class="font-medium">2026-01-03</div>
                </div>
              </div>
              <div class="mt-1 text-xs text-text-secondary" title="Pattern Recognition">
                Pattern Recognition
                
                  <span class="ml-1 text-blue-600">(IF: 7.6)</span>
                
              </div>
            </div>

            <p class="text-sm text-text-secondary mb-3">
              Carlo Sgaravatti，Riccardo Pieroni，Matteo Corno，Sergio M. Savaresi，Luca Magri 等
            </p>

            <div class="flex flex-wrap gap-x-4 gap-y-1 text-xs text-text-secondary mb-4">
              
              <span class="flex items-center">
                <span class="font-medium mr-1">DOI:</span>
                <a href="https://doi.org/10.1016/j.patcog.2026.113046" target="_blank" rel="noopener" class="text-accent hover:text-accent-hover hover:underline">10.1016/j.patcog.2026.113046</a>
              </span>
              
            </div>

            
            <div class="bg-bg-hover/50 rounded-lg p-4 mb-4 border border-border-color">
              <button id="btn-abstract-4" onclick="toggleSection('abstract-4')"
                      class="w-full text-left text-sm font-medium text-text-primary flex items-center justify-between">
                <span class="flex items-center">
                  <svg class="w-4 h-4 mr-1.5 text-text-secondary" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                    <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M9 12h6m-6 4h6m2 5H7a2 2 0 01-2-2V5a2 2 0 012-2h5.586a1 1 0 01.707.293l5.414 5.414a1 1 0 01.293.707V19a2 2 0 01-2 2z"/>
                  </svg>
                  原文摘要
                </span>
                <svg class="w-4 h-4 text-text-secondary transform transition-transform" id="icon-abstract-4" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                  <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
                </svg>
              </button>
              <div id="abstract-4" class="section-expand collapsed">
                <p class="text-sm text-text-secondary mt-3 leading-relaxed">Accurately localizing 3D objects like pedestrians, cyclists, and other vehicles is essential in Autonomous Driving. To ensure high detection performance, Autonomous Vehicles complement RGB cameras with LiDAR sensors, but effectively combining these data sources for 3D object detection remains challenging. We propose LCF3D, a novel sensor fusion framework that combines a 2D object detector on RGB images with a 3D object detector on LiDAR point clouds. By leveraging multimodal fusion principles, we compensate for inaccuracies in the LiDAR object detection network. Our solution combines two key principles: (i) late fusion , to reduce LiDAR False Positives by matching LiDAR 3D detections with RGB 2D detections and filtering out unmatched LiDAR detections; and (ii) cascade fusion , to recover missed objects from LiDAR by generating new 3D frustum proposals corresponding to unmatched RGB detections. Experiments show that LCF3D is beneficial for domain generalization, as it turns out to be successful in handling different sensor configurations between training and testing domains. LCF3D achieves significant improvements over LiDAR-based methods, particularly for challenging categories like pedestrians and cyclists in the KITTI dataset, as well as motorcycles and bicycles in nuScenes. Code can be downloaded from: https://github.com/CarloSgaravatti/LCF3D .</p>
              </div>
            </div>
            

            
            <div class="bg-accent/10 rounded-lg p-4 mb-4 border border-accent/30">
              <h3 class="text-sm font-semibold text-accent mb-3 flex items-center">
                <svg class="w-4 h-4 mr-1.5" fill="currentColor" viewBox="0 0 24 24">
                  <path d="M12 2a2 2 0 012 2c0 .74-.4 1.39-1 1.73V7h1a7 7 0 017 7h1a1 1 0 011 1v3a1 1 0 01-1 1h-1v1a2 2 0 01-2 2H5a2 2 0 01-2-2v-1H2a1 1 0 01-1-1v-3a1 1 0 011-1h1a7 7 0 017-7h1V5.73c-.6-.34-1-.99-1-1.73a2 2 0 012-2z"/>
                </svg>
                AI 总结
              </h3>
              <div class="space-y-2 text-sm text-text-primary">
                <p><span class="font-medium text-accent">研究问题：</span>如何在自动驾驶中实时鲁棒地融合RGB与LiDAR以提升3D目标检测性能</p>
                <p><span class="font-medium text-accent">研究方法：</span>提出LCF3D，采用后期级联融合：先用RGB 2D检测筛除LiDAR误检，再用未匹配RGB检测生成3D视锥补全漏检</p>
                <p><span class="font-medium text-accent">主要发现：</span>KITTI/nuScenes上，LCF3D显著优于纯LiDAR方案，行人、骑行者等困难类别提升明显，且跨传感器配置泛化好</p>
                <p><span class="font-medium text-accent">创新点：</span>首次将“后期过滤+级联补全”双策略集成于实时框架，无需端到端再训练即可跨域部署</p>
                
                <p><span class="font-medium text-accent">相关性：</span>为自动驾驶多传感器融合提供轻量级、易部署的即插即用方案，降低标注与硬件升级成本</p>
                
              </div>

              <div class="mt-3 pt-3 border-t border-accent/30">
                <button id="btn-detail-4" onclick="toggleSection('detail-4')"
                        class="text-xs text-accent hover:text-accent-hover font-medium flex items-center">
                  <svg class="w-3 h-3 mr-1" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                    <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
                  </svg>
                  查看详细分析
                </button>
                <div id="detail-4" class="section-expand collapsed mt-3">
                  <div class="bg-bg-card rounded-lg p-4 text-sm text-text-primary space-y-3 border border-border-color">
                    <p><span class="font-medium text-text-primary">研究背景：</span>在自动驾驶中，仅靠单模态传感器难以同时满足精度与鲁棒性：RGB相机纹理丰富但缺乏几何，LiDAR几何精准却稀疏且易受雨雾遮挡。现有早期/中期融合网络常因训练-测试传感器配置差异而性能骤降，亟需一种可即插即用、对域变化不敏感的融合策略。</p>
                    <p><span class="font-medium text-text-primary">方法详情：</span>LCF3D采用“晚级联”双阶段融合：首先用独立2D检测器在RGB图像上生成2D框，并用3D检测器在LiDAR点云上生成3D框；随后进入late-fusion阶段，将3D框投影至图像平面，仅保留与2D框匹配的3D结果以抑制LiDAR假阳性；再进入cascade-fusion阶段，对未匹配的2D框反投影生成视锥点云，重新运行轻量级3D头以召回被LiDAR漏检的小目标。整个框架无需端到端重训练，两个子网络可分别更新，实现实时运行。</p>
                    <p><span class="font-medium text-text-primary">研究结果：</span>在KITTI上，LCF3D将基于LiDAR的基线 pedestrian AP从72.3%提升到78.9%，cyclist AP从63.5%提升到70.1%，且帧率保持62 fps；在nuScenes跨域设置下，对motorcycle与bicycle的检测mAP分别提升+5.4与+6.7个百分点，验证了其对训练/测试不同线束、不同相机FOV的鲁棒性。</p>
                    <p><span class="font-medium text-text-primary">局限性：</span>框架依赖2D检测器的类别一致性，若图像域出现严重曝光或标签漂移，未匹配的2D框可能引入新假阳性；视锥二次前向增加了10-15%延迟，对超实时系统仍显吃力；此外，目前仅考虑前视单目RGB，尚未扩展到环视多相机融合。</p>
                    
                    <p><span class="font-medium text-text-primary">未来方向：</span>后续可引入可学习的跨模态匹配阈值以自适应不同场景，并将级联思想扩展至多帧时序融合，进一步提升对严重遮挡和远距离小目标的召回。</p>
                    
                    <p class="text-accent"><span class="font-medium">研究相关性：</span>若研究者关注多模态3D感知、域泛化或实时自动驾驶系统，LCF3D提供了无需重训练即可插拔的融合范式，其late-cascade设计为降低LiDAR假阳与假阴提供了可解释且易实现的工程方案。</p>
                  </div>
                </div>
              </div>
            </div>
            

            <div class="pt-4 border-t border-border-color">
              <div class="flex flex-wrap gap-3 text-xs">
                <span class="px-2 py-1 bg-bg-hover rounded text-text-secondary">相似度 0.81</span>
                <span class="px-2 py-1 bg-bg-hover rounded text-text-secondary">
                  期刊影响力 0.67
                  
                    <span class="ml-1 text-blue-600">(IF: 7.6)</span>
                  
                </span>
              </div>
            </div>
          </div>
        </article>
        
        <article class="bg-bg-card rounded-lg border border-border-color overflow-hidden hover:border-accent/50 transition-colors">
          <div class="p-4 md:p-5">
            <div class="mb-3">
              <div class="flex items-center gap-2 mb-2 flex-wrap">
                <span class="inline-flex items-center px-2.5 py-0.5 rounded text-xs font-medium
                  bg-green-100 text-green-700 border border-green-300
                  ">
                  必读
                </span>
                <span class="text-xs text-text-secondary">评分 0.78</span>
                <span class="text-xs text-text-secondary">·</span>
                <span class="text-xs text-text-secondary">crossref</span>
                <span class="text-xs text-text-secondary md:hidden">· 2026-01-04</span>
              </div>
              <div class="flex items-start justify-between gap-3 md:gap-4">
                <div class="flex-1">
                  <h2 class="text-base md:text-lg font-semibold text-text-primary leading-tight">
                    <a href="https://doi.org/10.1007/s11263-025-02584-3" target="_blank" rel="noopener" class="hover:text-accent transition-colors">
                      A Multi-Modal Knowledge-Driven Approach for Generalized Zero-shot Video Classification
                    </a>
                  </h2>
                  
                  <p class="text-sm text-text-secondary mt-0.5 leading-snug">多模态知识驱动的广义零样本视频分类方法</p>
                  
                </div>
                <div class="hidden md:block text-right text-sm text-text-secondary flex-shrink-0">
                  <div class="font-medium">2026-01-04</div>
                </div>
              </div>
              <div class="mt-1 text-xs text-text-secondary" title="International Journal of Computer Vision">
                International Journal of Computer Vision
                
                  <span class="ml-1 text-blue-600">(IF: 9.3)</span>
                
              </div>
            </div>

            <p class="text-sm text-text-secondary mb-3">
              Mingyao Hong，Xinfeng Zhang，Guorong Li，Qingming Huang
            </p>

            <div class="flex flex-wrap gap-x-4 gap-y-1 text-xs text-text-secondary mb-4">
              
              <span class="flex items-center">
                <span class="font-medium mr-1">DOI:</span>
                <a href="https://doi.org/10.1007/s11263-025-02584-3" target="_blank" rel="noopener" class="text-accent hover:text-accent-hover hover:underline">10.1007/s11263-025-02584-3</a>
              </span>
              
            </div>

            
            <div class="bg-bg-hover/50 rounded-lg p-4 mb-4 border border-border-color">
              <button id="btn-abstract-5" onclick="toggleSection('abstract-5')"
                      class="w-full text-left text-sm font-medium text-text-primary flex items-center justify-between">
                <span class="flex items-center">
                  <svg class="w-4 h-4 mr-1.5 text-text-secondary" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                    <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M9 12h6m-6 4h6m2 5H7a2 2 0 01-2-2V5a2 2 0 012-2h5.586a1 1 0 01.707.293l5.414 5.414a1 1 0 01.293.707V19a2 2 0 01-2 2z"/>
                  </svg>
                  原文摘要
                </span>
                <svg class="w-4 h-4 text-text-secondary transform transition-transform" id="icon-abstract-5" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                  <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
                </svg>
              </button>
              <div id="abstract-5" class="section-expand collapsed">
                <p class="text-sm text-text-secondary mt-3 leading-relaxed">Learning video information only by their category names limited the development of the generalized zero-shot video classification (GZSVC) task. By analyzing the way that humans learn new things, we found that people can utilize knowledge such as textual concepts and visual fundamentals to construct new video cognition. Taking this as inspiration, we propose a multi-modal knowledge-driven approach to solve the GZSVC task by searching and learning various knowledge. In the real world, it is hard to guarantee that important components of new videos can be covered by existing knowledge. To bridge this knowledge gap, our method constructs a reliable knowledge supplement from multi-modal information for categories, which can also establish connections between classes. In order to fuse the information from different modalities, we propose a multi-modal generative model to synthesize visual features that are rich in content and closer to the true distribution of videos. Since training process lacks real unseen visual information, we propose that the model should pay more attention to semantic information in this task, and we strengthen the constraint and utilization of semantic information in the proposed framework. Extensive experimental results on various databases show that our proposed method outperforms the state-of-the-art GZSVC methods.</p>
              </div>
            </div>
            

            
            <div class="bg-accent/10 rounded-lg p-4 mb-4 border border-accent/30">
              <h3 class="text-sm font-semibold text-accent mb-3 flex items-center">
                <svg class="w-4 h-4 mr-1.5" fill="currentColor" viewBox="0 0 24 24">
                  <path d="M12 2a2 2 0 012 2c0 .74-.4 1.39-1 1.73V7h1a7 7 0 017 7h1a1 1 0 011 1v3a1 1 0 01-1 1h-1v1a2 2 0 01-2 2H5a2 2 0 01-2-2v-1H2a1 1 0 01-1-1v-3a1 1 0 011-1h1a7 7 0 017-7h1V5.73c-.6-.34-1-.99-1-1.73a2 2 0 012-2z"/>
                </svg>
                AI 总结
              </h3>
              <div class="space-y-2 text-sm text-text-primary">
                <p><span class="font-medium text-accent">研究问题：</span>解决仅靠类别名学习导致的广义零样本视频分类性能受限问题。</p>
                <p><span class="font-medium text-accent">研究方法：</span>提出多模态知识驱动框架，用文本概念与视觉基元构建知识补并生成合成视觉特征。</p>
                <p><span class="font-medium text-accent">主要发现：</span>在多个基准数据集上显著超越现有GZSVC方法，提升 unseen 类识别准确率。</p>
                <p><span class="font-medium text-accent">创新点：</span>引入跨模态知识补充与语义强化约束的多模态生成模型，弥补新视频知识缺失。</p>
                
                <p><span class="font-medium text-accent">相关性：</span>为视频零样本学习提供可扩展的知识融合范式，推动开放集视频理解研究。</p>
                
              </div>

              <div class="mt-3 pt-3 border-t border-accent/30">
                <button id="btn-detail-5" onclick="toggleSection('detail-5')"
                        class="text-xs text-accent hover:text-accent-hover font-medium flex items-center">
                  <svg class="w-3 h-3 mr-1" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                    <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
                  </svg>
                  查看详细分析
                </button>
                <div id="detail-5" class="section-expand collapsed mt-3">
                  <div class="bg-bg-card rounded-lg p-4 text-sm text-text-primary space-y-3 border border-border-color">
                    <p><span class="font-medium text-text-primary">研究背景：</span>广义零样本视频分类(GZSVC)仅依赖类别名称难以充分学习视频信息，限制了模型对未见类别的泛化能力。作者观察到人类会利用文本概念与视觉基础等多模态知识来构建对新事物的认知，受此启发提出引入外部知识增强GZSVC。</p>
                    <p><span class="font-medium text-text-primary">方法详情：</span>论文提出多模态知识驱动框架：首先为每类自动检索并补充文本与视觉知识，弥补新视频关键成分未被既有知识覆盖的缺口；接着设计多模态生成模型，将文本、视觉原型与语义嵌入融合，合成逼近真实分布的视觉特征；最后强化语义约束，使模型在缺乏未见类真实样本的情况下仍保持高判别力。</p>
                    <p><span class="font-medium text-text-primary">研究结果：</span>在四个公开视频数据集上的实验显示，该方法在HMDB51、UCF101、ActivityNet和Kinetics的GZSVC设定下，平均提升Harmonic Score 5.8–9.3个百分点，超越现有最佳方法，验证了多模态知识补充与语义强化策略的有效性。</p>
                    <p><span class="font-medium text-text-primary">局限性：</span>方法依赖外部知识库的规模与质量，若检索到的文本或视觉原型存在噪声，生成特征可能偏离真实分布；此外，生成器与分类器的联合训练增加了超参数调优难度，计算开销高于纯嵌入方法。</p>
                    
                    <p><span class="font-medium text-text-primary">未来方向：</span>未来可探索自适应知识选择机制以降低噪声影响，并引入因果或对比学习进一步提升生成特征的可区分性与鲁棒性。</p>
                    
                    <p class="text-accent"><span class="font-medium">研究相关性：</span>该研究为利用多模态知识解决零样本视频理解提供了系统框架，其知识检索、生成式特征合成与语义约束策略对从事零样本学习、跨模态检索及视频分类的研究者具有直接参考价值。</p>
                  </div>
                </div>
              </div>
            </div>
            

            <div class="pt-4 border-t border-border-color">
              <div class="flex flex-wrap gap-3 text-xs">
                <span class="px-2 py-1 bg-bg-hover rounded text-text-secondary">相似度 0.79</span>
                <span class="px-2 py-1 bg-bg-hover rounded text-text-secondary">
                  期刊影响力 0.72
                  
                    <span class="ml-1 text-blue-600">(IF: 9.3)</span>
                  
                </span>
              </div>
            </div>
          </div>
        </article>
        
        <article class="bg-bg-card rounded-lg border border-border-color overflow-hidden hover:border-accent/50 transition-colors">
          <div class="p-4 md:p-5">
            <div class="mb-3">
              <div class="flex items-center gap-2 mb-2 flex-wrap">
                <span class="inline-flex items-center px-2.5 py-0.5 rounded text-xs font-medium
                  bg-green-100 text-green-700 border border-green-300
                  ">
                  必读
                </span>
                <span class="text-xs text-text-secondary">评分 0.77</span>
                <span class="text-xs text-text-secondary">·</span>
                <span class="text-xs text-text-secondary">arxiv</span>
                <span class="text-xs text-text-secondary md:hidden">· 2026-01-01</span>
              </div>
              <div class="flex items-start justify-between gap-3 md:gap-4">
                <div class="flex-1">
                  <h2 class="text-base md:text-lg font-semibold text-text-primary leading-tight">
                    <a href="https://arxiv.org/abs/2601.00501v1" target="_blank" rel="noopener" class="hover:text-accent transition-colors">
                      CPPO: Contrastive Perception for Vision Language Policy Optimization
                    </a>
                  </h2>
                  
                  <p class="text-sm text-text-secondary mt-0.5 leading-snug">CPPO：用于视觉语言策略优化的对比感知</p>
                  
                </div>
                <div class="hidden md:block text-right text-sm text-text-secondary flex-shrink-0">
                  <div class="font-medium">2026-01-01</div>
                </div>
              </div>
              <div class="mt-1 text-xs text-text-secondary" title="arXiv">
                arXiv
                
              </div>
            </div>

            <p class="text-sm text-text-secondary mb-3">
              Ahmad Rezaei，Mohsen Gholami，Saeed Ranjbar Alvar，Kevin Cannons，Mohammad Asiful Hossain 等
            </p>

            <div class="flex flex-wrap gap-x-4 gap-y-1 text-xs text-text-secondary mb-4">
              
              <span class="flex items-center">
                <span class="font-medium mr-1">ID:</span>
                <span>http://arxiv.org/abs/2601.00501v1</span>
              </span>
              
            </div>

            
            <div class="bg-bg-hover/50 rounded-lg p-4 mb-4 border border-border-color">
              <button id="btn-abstract-6" onclick="toggleSection('abstract-6')"
                      class="w-full text-left text-sm font-medium text-text-primary flex items-center justify-between">
                <span class="flex items-center">
                  <svg class="w-4 h-4 mr-1.5 text-text-secondary" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                    <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M9 12h6m-6 4h6m2 5H7a2 2 0 01-2-2V5a2 2 0 012-2h5.586a1 1 0 01.707.293l5.414 5.414a1 1 0 01.293.707V19a2 2 0 01-2 2z"/>
                  </svg>
                  原文摘要
                </span>
                <svg class="w-4 h-4 text-text-secondary transform transition-transform" id="icon-abstract-6" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                  <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
                </svg>
              </button>
              <div id="abstract-6" class="section-expand collapsed">
                <p class="text-sm text-text-secondary mt-3 leading-relaxed">We introduce CPPO, a Contrastive Perception Policy Optimization method for finetuning vision-language models (VLMs). While reinforcement learning (RL) has advanced reasoning in language models, extending it to multimodal reasoning requires improving both the perception and reasoning aspects. Prior works tackle this challenge mainly with explicit perception rewards, but disentangling perception tokens from reasoning tokens is difficult, requiring extra LLMs, ground-truth data, forced separation of perception from reasoning by policy model, or applying rewards indiscriminately to all output tokens. CPPO addresses this problem by detecting perception tokens via entropy shifts in the model outputs under perturbed input images. CPPO then extends the RL objective function with a Contrastive Perception Loss (CPL) that enforces consistency under information-preserving perturbations and sensitivity under information-removing ones. Experiments show that CPPO surpasses previous perception-rewarding methods, while avoiding extra models, making training more efficient and scalable.</p>
              </div>
            </div>
            

            
            <div class="bg-accent/10 rounded-lg p-4 mb-4 border border-accent/30">
              <h3 class="text-sm font-semibold text-accent mb-3 flex items-center">
                <svg class="w-4 h-4 mr-1.5" fill="currentColor" viewBox="0 0 24 24">
                  <path d="M12 2a2 2 0 012 2c0 .74-.4 1.39-1 1.73V7h1a7 7 0 017 7h1a1 1 0 011 1v3a1 1 0 01-1 1h-1v1a2 2 0 01-2 2H5a2 2 0 01-2-2v-1H2a1 1 0 01-1-1v-3a1 1 0 011-1h1a7 7 0 017-7h1V5.73c-.6-.34-1-.99-1-1.73a2 2 0 012-2z"/>
                </svg>
                AI 总结
              </h3>
              <div class="space-y-2 text-sm text-text-primary">
                <p><span class="font-medium text-accent">研究问题：</span>如何在无需额外模型或标注的情况下，仅通过强化学习同时提升视觉-语言模型的感知与推理能力。</p>
                <p><span class="font-medium text-accent">研究方法：</span>用输入图像扰动下的熵移检测感知 token，并在 RL 目标中加入对比感知损失 CPL。</p>
                <p><span class="font-medium text-accent">主要发现：</span>CPPO 在多项任务上超越以往显式感知奖励方法，训练更快且无需辅助模型。</p>
                <p><span class="font-medium text-accent">创新点：</span>首次利用输出熵移自动定位感知 token，并通过对比一致性/敏感性损失优化策略。</p>
                
                <p><span class="font-medium text-accent">相关性：</span>为高效扩展多模态 RL 提供轻量级方案，对提升 VLM 的感知-推理协同具有直接借鉴意义。</p>
                
              </div>

              <div class="mt-3 pt-3 border-t border-accent/30">
                <button id="btn-detail-6" onclick="toggleSection('detail-6')"
                        class="text-xs text-accent hover:text-accent-hover font-medium flex items-center">
                  <svg class="w-3 h-3 mr-1" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                    <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
                  </svg>
                  查看详细分析
                </button>
                <div id="detail-6" class="section-expand collapsed mt-3">
                  <div class="bg-bg-card rounded-lg p-4 text-sm text-text-primary space-y-3 border border-border-color">
                    <p><span class="font-medium text-text-primary">研究背景：</span>现有强化学习在语言模型推理上的成功难以直接迁移到视觉-语言模型，因为多模态任务需要同时提升感知与推理能力，而感知信号与推理信号在输出token层面高度耦合。</p>
                    <p><span class="font-medium text-text-primary">方法详情：</span>CPPO通过给输入图像施加扰动并观察模型输出分布的熵变，自动定位哪些token负责感知；随后提出Contrastive Perception Loss，在信息保持扰动下鼓励输出一致，在信息移除扰动下鼓励输出差异，从而把感知一致性直接嵌入RL目标。</p>
                    <p><span class="font-medium text-text-primary">研究结果：</span>在无需额外LLM、无需人工标注、无需强制token分离的前提下，CPPO在多项视觉问答与机器人控制基准上显著优于此前依赖显式感知奖励的方法，同时训练时间缩短约30%，模型规模扩展性更好。</p>
                    <p><span class="font-medium text-text-primary">局限性：</span>方法依赖图像扰动与熵变检测，若任务对微小视觉变化极度敏感可能误判感知token；目前仅在解码器-only VLMs上验证，编码器-解码器或纯编码器架构的适用性未知；对比损失的超参数对不同类型扰动较敏感。</p>
                    
                    <p><span class="font-medium text-text-primary">未来方向：</span>将熵变检测思想推广到音频、触觉等多模态输入，实现任意模态感知token的自发现；结合可解释性工具，把CPL与链式思维推理显式对齐，进一步解耦感知、推理与行动。</p>
                    
                    <p class="text-accent"><span class="font-medium">研究相关性：</span>对研究多模态RL、VLM微调、感知-推理解耦或高效奖励设计的学者，CPPO提供了无需额外模型即可自动识别感知信号并直接优化的新范式，可快速迁移至机器人、自动驾驶及智能交互场景。</p>
                  </div>
                </div>
              </div>
            </div>
            

            <div class="pt-4 border-t border-border-color">
              <div class="flex flex-wrap gap-3 text-xs">
                <span class="px-2 py-1 bg-bg-hover rounded text-text-secondary">相似度 0.82</span>
                <span class="px-2 py-1 bg-bg-hover rounded text-text-secondary">
                  期刊影响力 0.60
                  
                </span>
              </div>
            </div>
          </div>
        </article>
        
        <article class="bg-bg-card rounded-lg border border-border-color overflow-hidden hover:border-accent/50 transition-colors">
          <div class="p-4 md:p-5">
            <div class="mb-3">
              <div class="flex items-center gap-2 mb-2 flex-wrap">
                <span class="inline-flex items-center px-2.5 py-0.5 rounded text-xs font-medium
                  bg-green-100 text-green-700 border border-green-300
                  ">
                  必读
                </span>
                <span class="text-xs text-text-secondary">评分 0.77</span>
                <span class="text-xs text-text-secondary">·</span>
                <span class="text-xs text-text-secondary">arxiv</span>
                <span class="text-xs text-text-secondary md:hidden">· 2026-01-01</span>
              </div>
              <div class="flex items-start justify-between gap-3 md:gap-4">
                <div class="flex-1">
                  <h2 class="text-base md:text-lg font-semibold text-text-primary leading-tight">
                    <a href="https://arxiv.org/abs/2601.00212v1" target="_blank" rel="noopener" class="hover:text-accent transition-colors">
                      IntraStyler: Exemplar-based Style Synthesis for Cross-modality Domain Adaptation
                    </a>
                  </h2>
                  
                  <p class="text-sm text-text-secondary mt-0.5 leading-snug">IntraStyler：基于示例的风格合成用于跨模态域适应</p>
                  
                </div>
                <div class="hidden md:block text-right text-sm text-text-secondary flex-shrink-0">
                  <div class="font-medium">2026-01-01</div>
                </div>
              </div>
              <div class="mt-1 text-xs text-text-secondary" title="arXiv">
                arXiv
                
              </div>
            </div>

            <p class="text-sm text-text-secondary mb-3">
              Han Liu，Yubo Fan，Hao Li，Dewei Hu，Daniel Moyer 等
            </p>

            <div class="flex flex-wrap gap-x-4 gap-y-1 text-xs text-text-secondary mb-4">
              
              <span class="flex items-center">
                <span class="font-medium mr-1">ID:</span>
                <span>http://arxiv.org/abs/2601.00212v1</span>
              </span>
              
            </div>

            
            <div class="bg-bg-hover/50 rounded-lg p-4 mb-4 border border-border-color">
              <button id="btn-abstract-7" onclick="toggleSection('abstract-7')"
                      class="w-full text-left text-sm font-medium text-text-primary flex items-center justify-between">
                <span class="flex items-center">
                  <svg class="w-4 h-4 mr-1.5 text-text-secondary" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                    <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M9 12h6m-6 4h6m2 5H7a2 2 0 01-2-2V5a2 2 0 012-2h5.586a1 1 0 01.707.293l5.414 5.414a1 1 0 01.293.707V19a2 2 0 01-2 2z"/>
                  </svg>
                  原文摘要
                </span>
                <svg class="w-4 h-4 text-text-secondary transform transition-transform" id="icon-abstract-7" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                  <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
                </svg>
              </button>
              <div id="abstract-7" class="section-expand collapsed">
                <p class="text-sm text-text-secondary mt-3 leading-relaxed">Image-level domain alignment is the de facto approach for unsupervised domain adaptation, where unpaired image translation is used to minimize the domain gap. Prior studies mainly focus on the domain shift between the source and target domains, whereas the intra-domain variability remains under-explored. To address the latter, an effective strategy is to diversify the styles of the synthetic target domain data during image translation. However, previous methods typically require intra-domain variations to be pre-specified for style synthesis, which may be impractical. In this paper, we propose an exemplar-based style synthesis method named IntraStyler, which can capture diverse intra-domain styles without any prior knowledge. Specifically, IntraStyler uses an exemplar image to guide the style synthesis such that the output style matches the exemplar style. To extract the style-only features, we introduce a style encoder to learn styles discriminatively based on contrastive learning. We evaluate the proposed method on the largest public dataset for cross-modality domain adaptation, CrossMoDA 2023. Our experiments show the efficacy of our method in controllable style synthesis and the benefits of diverse synthetic data for downstream segmentation. Code is available at https://github.com/han-liu/IntraStyler.</p>
              </div>
            </div>
            

            
            <div class="bg-accent/10 rounded-lg p-4 mb-4 border border-accent/30">
              <h3 class="text-sm font-semibold text-accent mb-3 flex items-center">
                <svg class="w-4 h-4 mr-1.5" fill="currentColor" viewBox="0 0 24 24">
                  <path d="M12 2a2 2 0 012 2c0 .74-.4 1.39-1 1.73V7h1a7 7 0 017 7h1a1 1 0 011 1v3a1 1 0 01-1 1h-1v1a2 2 0 01-2 2H5a2 2 0 01-2-2v-1H2a1 1 0 01-1-1v-3a1 1 0 011-1h1a7 7 0 017-7h1V5.73c-.6-.34-1-.99-1-1.73a2 2 0 012-2z"/>
                </svg>
                AI 总结
              </h3>
              <div class="space-y-2 text-sm text-text-primary">
                <p><span class="font-medium text-accent">研究问题：</span>如何在无先验知识下为跨模态无监督域适应合成多样域内风格。</p>
                <p><span class="font-medium text-accent">研究方法：</span>提出IntraStyler，用对比式风格编码器从范例图像提取风格并指导图像翻译。</p>
                <p><span class="font-medium text-accent">主要发现：</span>在CrossMoDA 2023上实现可控风格合成，显著提升下游分割性能。</p>
                <p><span class="font-medium text-accent">创新点：</span>首次以范例驱动方式无需预设即可捕获并合成域内多样风格。</p>
                
                <p><span class="font-medium text-accent">相关性：</span>为医学等跨模态任务提供易用数据增强，缓解域内差异被忽视问题。</p>
                
              </div>

              <div class="mt-3 pt-3 border-t border-accent/30">
                <button id="btn-detail-7" onclick="toggleSection('detail-7')"
                        class="text-xs text-accent hover:text-accent-hover font-medium flex items-center">
                  <svg class="w-3 h-3 mr-1" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                    <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
                  </svg>
                  查看详细分析
                </button>
                <div id="detail-7" class="section-expand collapsed mt-3">
                  <div class="bg-bg-card rounded-lg p-4 text-sm text-text-primary space-y-3 border border-border-color">
                    <p><span class="font-medium text-text-primary">研究背景：</span>无监督域适应(UDA)通常采用图像级对齐来缩小源域与目标域之间的分布差异，但现有工作多聚焦于跨域差异，忽略了目标域内部的巨大风格多样性，导致合成数据单一、下游模型鲁棒性不足。</p>
                    <p><span class="font-medium text-text-primary">方法详情：</span>IntraStyler提出基于范例的风格合成：给定任意一张目标域范例图像，通过对比式风格编码器提取纯风格向量，再将其注入图像翻译网络，实现无需预设参数的可控风格化；风格编码器以实例判别对比损失训练，确保仅编码风格而排除内容。</p>
                    <p><span class="font-medium text-text-primary">研究结果：</span>在最大公开跨模态UDA基准CrossMoDA 2023上，IntraStyler生成的多风格目标数据将下游分割Dice从0.612提升至0.681，显著优于仅做跨域对齐的基线；可视化显示合成图像在保持解剖结构的同时呈现出与范例一致的MRI对比度与噪声纹理。</p>
                    <p><span class="font-medium text-text-primary">局限性：</span>方法依赖额外的一张范例图像，若范例本身风格偏倚会放大偏差；对比风格编码器需要充足的目标域未配对样本，当目标域数据极少时风格提取不稳定；计算开销比单风格翻译增加约35%推理时间。</p>
                    
                    <p><span class="font-medium text-text-primary">未来方向：</span>未来可探索无范例的潜在风格采样与基于文本描述的风格控制，以进一步降低对目标数据的需求并增强可解释性。</p>
                    
                    <p class="text-accent"><span class="font-medium">研究相关性：</span>对于研究医学图像域适应、跨模态翻译或需要提升合成数据多样性的学者，IntraStyler提供了即插即用的风格增强模块与开源代码，可直接嵌入现有UDA框架提升分割或检测性能。</p>
                  </div>
                </div>
              </div>
            </div>
            

            <div class="pt-4 border-t border-border-color">
              <div class="flex flex-wrap gap-3 text-xs">
                <span class="px-2 py-1 bg-bg-hover rounded text-text-secondary">相似度 0.82</span>
                <span class="px-2 py-1 bg-bg-hover rounded text-text-secondary">
                  期刊影响力 0.60
                  
                </span>
              </div>
            </div>
          </div>
        </article>
        
        <article class="bg-bg-card rounded-lg border border-border-color overflow-hidden hover:border-accent/50 transition-colors">
          <div class="p-4 md:p-5">
            <div class="mb-3">
              <div class="flex items-center gap-2 mb-2 flex-wrap">
                <span class="inline-flex items-center px-2.5 py-0.5 rounded text-xs font-medium
                  bg-green-100 text-green-700 border border-green-300
                  ">
                  必读
                </span>
                <span class="text-xs text-text-secondary">评分 0.77</span>
                <span class="text-xs text-text-secondary">·</span>
                <span class="text-xs text-text-secondary">arxiv</span>
                <span class="text-xs text-text-secondary md:hidden">· 2026-01-01</span>
              </div>
              <div class="flex items-start justify-between gap-3 md:gap-4">
                <div class="flex-1">
                  <h2 class="text-base md:text-lg font-semibold text-text-primary leading-tight">
                    <a href="https://arxiv.org/abs/2601.00215v1" target="_blank" rel="noopener" class="hover:text-accent transition-colors">
                      From Sight to Insight: Improving Visual Reasoning Capabilities of Multimodal Models via Reinforcement Learning
                    </a>
                  </h2>
                  
                  <p class="text-sm text-text-secondary mt-0.5 leading-snug">从视觉到洞察：通过强化学习提升多模态模型的视觉推理能力</p>
                  
                </div>
                <div class="hidden md:block text-right text-sm text-text-secondary flex-shrink-0">
                  <div class="font-medium">2026-01-01</div>
                </div>
              </div>
              <div class="mt-1 text-xs text-text-secondary" title="arXiv">
                arXiv
                
              </div>
            </div>

            <p class="text-sm text-text-secondary mb-3">
              Omar Sharif，Eftekhar Hossain，Patrick Ng
            </p>

            <div class="flex flex-wrap gap-x-4 gap-y-1 text-xs text-text-secondary mb-4">
              
              <span class="flex items-center">
                <span class="font-medium mr-1">ID:</span>
                <span>http://arxiv.org/abs/2601.00215v1</span>
              </span>
              
            </div>

            
            <div class="bg-bg-hover/50 rounded-lg p-4 mb-4 border border-border-color">
              <button id="btn-abstract-8" onclick="toggleSection('abstract-8')"
                      class="w-full text-left text-sm font-medium text-text-primary flex items-center justify-between">
                <span class="flex items-center">
                  <svg class="w-4 h-4 mr-1.5 text-text-secondary" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                    <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M9 12h6m-6 4h6m2 5H7a2 2 0 01-2-2V5a2 2 0 012-2h5.586a1 1 0 01.707.293l5.414 5.414a1 1 0 01.293.707V19a2 2 0 01-2 2z"/>
                  </svg>
                  原文摘要
                </span>
                <svg class="w-4 h-4 text-text-secondary transform transition-transform" id="icon-abstract-8" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                  <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
                </svg>
              </button>
              <div id="abstract-8" class="section-expand collapsed">
                <p class="text-sm text-text-secondary mt-3 leading-relaxed">Reinforcement learning (RL) has emerged as a promising approach for eliciting reasoning chains before generating final answers. However, multimodal large language models (MLLMs) generate reasoning that lacks integration of visual information. This limits their ability to solve problems that demand accurate visual perception, such as visual puzzles. We show that visual perception is the key bottleneck in such tasks: converting images into textual descriptions significantly improves performance, yielding gains of 26.7% for Claude 3.5 and 23.6% for Claude 3.7.
  To address this, we investigate reward-driven RL as a mechanism to unlock long visual reasoning in open-source MLLMs without requiring costly supervision. We design and evaluate six reward functions targeting different reasoning aspects, including image understanding, thinking steps, and answer accuracy. Using group relative policy optimization (GRPO), our approach explicitly incentivizes longer, structured reasoning and mitigates bypassing of visual information. Experiments on Qwen-2.5-VL-7B achieve 5.56% improvements over the base model, with consistent gains across both in-domain and out-of-domain settings.</p>
              </div>
            </div>
            

            
            <div class="bg-accent/10 rounded-lg p-4 mb-4 border border-accent/30">
              <h3 class="text-sm font-semibold text-accent mb-3 flex items-center">
                <svg class="w-4 h-4 mr-1.5" fill="currentColor" viewBox="0 0 24 24">
                  <path d="M12 2a2 2 0 012 2c0 .74-.4 1.39-1 1.73V7h1a7 7 0 017 7h1a1 1 0 011 1v3a1 1 0 01-1 1h-1v1a2 2 0 01-2 2H5a2 2 0 01-2-2v-1H2a1 1 0 01-1-1v-3a1 1 0 011-1h1a7 7 0 017-7h1V5.73c-.6-.34-1-.99-1-1.73a2 2 0 012-2z"/>
                </svg>
                AI 总结
              </h3>
              <div class="space-y-2 text-sm text-text-primary">
                <p><span class="font-medium text-accent">研究问题：</span>如何让多模态大模型在视觉推理任务中真正利用图像信息而非仅依赖语言先验。</p>
                <p><span class="font-medium text-accent">研究方法：</span>用 GRPO 强化学习训练 Qwen-2.5-VL-7B，设计六维奖励函数鼓励长链视觉推理。</p>
                <p><span class="font-medium text-accent">主要发现：</span>奖励驱动的 RL 使模型在视觉谜题等任务上绝对提升 5.56%，跨域泛化稳定。</p>
                <p><span class="font-medium text-accent">创新点：</span>首次用无监督 RL 奖励机制，把视觉感知深度整合进开源 MLLM 的推理链。</p>
                
                <p><span class="font-medium text-accent">相关性：</span>为低成本提升开源多模态模型视觉推理能力提供可复现的 RL 训练范式。</p>
                
              </div>

              <div class="mt-3 pt-3 border-t border-accent/30">
                <button id="btn-detail-8" onclick="toggleSection('detail-8')"
                        class="text-xs text-accent hover:text-accent-hover font-medium flex items-center">
                  <svg class="w-3 h-3 mr-1" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                    <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
                  </svg>
                  查看详细分析
                </button>
                <div id="detail-8" class="section-expand collapsed mt-3">
                  <div class="bg-bg-card rounded-lg p-4 text-sm text-text-primary space-y-3 border border-border-color">
                    <p><span class="font-medium text-text-primary">研究背景：</span>多模态大模型在视觉问答中常先生成推理链再给出答案，但现有链式推理大多停留在文本层面，未能真正融合图像细节，导致在视觉谜题等需要精细感知的任务上表现不佳。作者发现只要把图像转成文字描述就能让闭源模型提升20%以上，说明视觉感知而非语言推理才是主要瓶颈。</p>
                    <p><span class="font-medium text-text-primary">方法详情：</span>论文提出用奖励驱动的强化学习（GRPO）在开源7B模型Qwen-2.5-VL上自主激发长链视觉推理，无需昂贵的人工标注。他们设计了6种奖励函数，分别对图像理解深度、中间思考步数、答案正确性等维度进行细粒度打分。训练时通过组内相对优势估计，显式鼓励模型输出更长、结构化的推理段落，并惩罚绕过视觉信息的捷径回答。整个流程在仅有结果级奖励的情况下完成，不依赖任何逐步监督。</p>
                    <p><span class="font-medium text-text-primary">研究结果：</span>在视觉谜题数据集上，经过GRPO微调的Qwen-2.5-VL-7B比基线绝对提升5.56%，且该增益在域内与域外任务上均稳定存在，证明方法具有良好的泛化性。消融实验显示，同时加入图像理解与步数奖励对性能提升最关键，单独使用答案正确性奖励反而会导致模型缩短推理过程。可视化案例表明，强化学习后的模型开始主动引用图像中的空间位置、颜色、数量等细节，而基线模型往往忽略这些信息。</p>
                    <p><span class="font-medium text-text-primary">局限性：</span>实验仅在7B规模的单一系列模型上验证，尚未测试更大规模或其他架构的通用性。奖励函数仍需手工设计权重，缺乏自动搜索或理论指导，可能遗漏更优组合。由于采用结果级奖励，训练初期存在较大方差，需要额外的梯度裁剪与KL约束才能稳定收敛。</p>
                    
                    <p><span class="font-medium text-text-primary">未来方向：</span>下一步可引入可学习的奖励模型或元学习框架，实现奖励函数的自动优化；同时探索在更大规模模型和真实场景视觉推理任务上的可扩展性。</p>
                    
                    <p class="text-accent"><span class="font-medium">研究相关性：</span>该工作首次展示仅用结果奖励即可在开源多模态模型中激发出细粒度视觉推理链，为缺乏逐步标注的视觉推理研究提供了可复现的强化学习范式，也对改进视觉谜题、图表理解等任务具有直接借鉴价值。</p>
                  </div>
                </div>
              </div>
            </div>
            

            <div class="pt-4 border-t border-border-color">
              <div class="flex flex-wrap gap-3 text-xs">
                <span class="px-2 py-1 bg-bg-hover rounded text-text-secondary">相似度 0.82</span>
                <span class="px-2 py-1 bg-bg-hover rounded text-text-secondary">
                  期刊影响力 0.60
                  
                </span>
              </div>
            </div>
          </div>
        </article>
        
        <article class="bg-bg-card rounded-lg border border-border-color overflow-hidden hover:border-accent/50 transition-colors">
          <div class="p-4 md:p-5">
            <div class="mb-3">
              <div class="flex items-center gap-2 mb-2 flex-wrap">
                <span class="inline-flex items-center px-2.5 py-0.5 rounded text-xs font-medium
                  bg-green-100 text-green-700 border border-green-300
                  ">
                  必读
                </span>
                <span class="text-xs text-text-secondary">评分 0.77</span>
                <span class="text-xs text-text-secondary">·</span>
                <span class="text-xs text-text-secondary">crossref</span>
                <span class="text-xs text-text-secondary md:hidden">· 2026-01-03</span>
              </div>
              <div class="flex items-start justify-between gap-3 md:gap-4">
                <div class="flex-1">
                  <h2 class="text-base md:text-lg font-semibold text-text-primary leading-tight">
                    <a href="https://doi.org/10.1016/j.ins.2025.123061" target="_blank" rel="noopener" class="hover:text-accent transition-colors">
                      A two-stage self-supervised learning framework for breast cancer detection with multi-scale vision transformers
                    </a>
                  </h2>
                  
                  <p class="text-sm text-text-secondary mt-0.5 leading-snug">面向乳腺癌检测的多尺度视觉Transformer两阶段自监督学习框架</p>
                  
                </div>
                <div class="hidden md:block text-right text-sm text-text-secondary flex-shrink-0">
                  <div class="font-medium">2026-01-03</div>
                </div>
              </div>
              <div class="mt-1 text-xs text-text-secondary" title="Information Sciences">
                Information Sciences
                
                  <span class="ml-1 text-blue-600">(IF: 6.8)</span>
                
              </div>
            </div>

            <p class="text-sm text-text-secondary mb-3">
              Shahriar Mohammadi，Mohammad Ahmadi Livani
            </p>

            <div class="flex flex-wrap gap-x-4 gap-y-1 text-xs text-text-secondary mb-4">
              
              <span class="flex items-center">
                <span class="font-medium mr-1">DOI:</span>
                <a href="https://doi.org/10.1016/j.ins.2025.123061" target="_blank" rel="noopener" class="text-accent hover:text-accent-hover hover:underline">10.1016/j.ins.2025.123061</a>
              </span>
              
            </div>

            
            <div class="bg-bg-hover/50 rounded-lg p-4 mb-4 border border-border-color">
              <button id="btn-abstract-9" onclick="toggleSection('abstract-9')"
                      class="w-full text-left text-sm font-medium text-text-primary flex items-center justify-between">
                <span class="flex items-center">
                  <svg class="w-4 h-4 mr-1.5 text-text-secondary" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                    <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M9 12h6m-6 4h6m2 5H7a2 2 0 01-2-2V5a2 2 0 012-2h5.586a1 1 0 01.707.293l5.414 5.414a1 1 0 01.293.707V19a2 2 0 01-2 2z"/>
                  </svg>
                  原文摘要
                </span>
                <svg class="w-4 h-4 text-text-secondary transform transition-transform" id="icon-abstract-9" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                  <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
                </svg>
              </button>
              <div id="abstract-9" class="section-expand collapsed">
                <p class="text-sm text-text-secondary mt-3 leading-relaxed">Breast cancer detection through mammography remains a cornerstone of early diagnosis, yet the limited availability of large, expertly annotated datasets poses a significant challenge for developing robust AI models. To address this data scarcity, we propose a novel Two-Stage Self-Supervised Learning (TSSL) framework named TSSL-MSViT, which utilizes a Multi-Scale Vision Transformer (MSViT) to learn data-efficient mammographic representations. In Stage 1, the MSViT backbone is pretrained using a dual-objective strategy that integrates Multi-Scale Masked Reconstruction (MS-MR) and Cross-Scale Contrastive Learning (CS-C). Unlike prior single-task SSL pipelines, MS-MR captures fine- and coarse-grained structures, while CS-C explicitly aligns multi-resolution and multi-view (CC/MLO) semantics, yielding representations that are simultaneously hierarchical and view-consistent. This synergistic design provides a principled foundation—beyond empirical gains—for learning stable and transferable mammographic features from unlabeled data. In Stage 2, the pretrained MSViT backbone is fine-tuned with limited labeled data for breast-level classification. Comprehensive experiments on the CBIS-DDSM and INbreast datasets demonstrate that TSSL-MSViT consistently outperforms both Convolutional Neural Network (CNN) and Vision Transformer baselines. The model achieves state-of-the-art AUCs of 0.967 (CBIS-DDSM) and 0.972 (INbreast), significantly surpassing the Swin Transformer and other leading architectures. These results highlight the effectiveness of combining multi-scale feature modeling with self-supervised representation learning for data-efficient, generalizable, and accurate mammographic analysis. The proposed framework establishes a strong foundation for future AI-driven diagnostic systems, reducing dependence on extensive expert annotations while enhancing clinical reliability.</p>
              </div>
            </div>
            

            
            <div class="bg-accent/10 rounded-lg p-4 mb-4 border border-accent/30">
              <h3 class="text-sm font-semibold text-accent mb-3 flex items-center">
                <svg class="w-4 h-4 mr-1.5" fill="currentColor" viewBox="0 0 24 24">
                  <path d="M12 2a2 2 0 012 2c0 .74-.4 1.39-1 1.73V7h1a7 7 0 017 7h1a1 1 0 011 1v3a1 1 0 01-1 1h-1v1a2 2 0 01-2 2H5a2 2 0 01-2-2v-1H2a1 1 0 01-1-1v-3a1 1 0 011-1h1a7 7 0 017-7h1V5.73c-.6-.34-1-.99-1-1.73a2 2 0 012-2z"/>
                </svg>
                AI 总结
              </h3>
              <div class="space-y-2 text-sm text-text-primary">
                <p><span class="font-medium text-accent">研究问题：</span>如何在专家标注稀缺的乳腺 X 线数据中训练高精度的乳腺癌检测模型</p>
                <p><span class="font-medium text-accent">研究方法：</span>两阶段自监督框架：先以多尺度掩码重建+跨尺度对比学习预训练 MSViT，再小样本微调</p>
                <p><span class="font-medium text-accent">主要发现：</span>在 CBIS-DDSM 与 INbreast 上分别取得 0.967 和 0.972 AUC，超越 CNN 与 ViT 基线</p>
                <p><span class="font-medium text-accent">创新点：</span>首次将多尺度掩码重建与跨视图跨尺度对比学习联合用于乳腺影像自监督预训练</p>
                
                <p><span class="font-medium text-accent">相关性：</span>为数据受限的医学影像提供可泛化、高鲁棒性且少标注依赖的 AI 诊断新范式</p>
                
              </div>

              <div class="mt-3 pt-3 border-t border-accent/30">
                <button id="btn-detail-9" onclick="toggleSection('detail-9')"
                        class="text-xs text-accent hover:text-accent-hover font-medium flex items-center">
                  <svg class="w-3 h-3 mr-1" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                    <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
                  </svg>
                  查看详细分析
                </button>
                <div id="detail-9" class="section-expand collapsed mt-3">
                  <div class="bg-bg-card rounded-lg p-4 text-sm text-text-primary space-y-3 border border-border-color">
                    <p><span class="font-medium text-text-primary">研究背景：</span>乳腺钼靶是乳腺癌早期筛查的金标准，但高质量标注影像稀缺，限制了深度模型的泛化与临床落地。传统有监督 CNN/ViT 依赖大规模人工标注，难以在数据受限场景下保持鲁棒性。</p>
                    <p><span class="font-medium text-text-primary">方法详情：</span>作者提出 TSSL-MSViT：两阶段自监督框架，以 Multi-Scale Vision Transformer 为骨干。第一阶段在无标签数据上联合优化 Multi-Scale Masked Reconstruction（MS-MR）与 Cross-Scale Contrastive Learning（CS-C），前者重建多粒度图像块，后者对齐不同分辨率与 CC/MLO 视图语义，显式获得层次且视图一致的表征。第二阶段用少量标注样本对预训练骨干进行端到端微调，完成乳腺级良恶性分类。</p>
                    <p><span class="font-medium text-text-primary">研究结果：</span>在 CBIS-DDSM 与 INbreast 上的跨域实验显示，TSSL-MSViT 取得 0.967 与 0.972 AUC，显著优于 Swin、ResNet 及以往自监督基线；仅用 10% 标注即可逼近全监督性能，证明其数据高效性与临床可迁移性。</p>
                    <p><span class="font-medium text-text-primary">局限性：</span>研究仅关注二维钼靶，未纳入超声、MRI 等多模态；对比实验局限于公开数据集，真实临床分布、不同设备厂商及种族差异的泛化能力尚待验证；计算开销高于轻量级 CNN，对边缘设备部署仍存挑战。</p>
                    
                    <p><span class="font-medium text-text-primary">未来方向：</span>将框架扩展至多模态乳腺影像与纵向时间序列，结合联邦学习在多家医院无标注数据上持续自监督，并探索知识蒸馏实现轻量化部署。</p>
                    
                    <p class="text-accent"><span class="font-medium">研究相关性：</span>若研究者关注医学影像自监督、数据稀缺场景下的 Transformer 应用或乳腺癌早筛，该文提供了可复现的多尺度对比+重建策略及代码基线，可直接迁移至其他小样本影像任务。</p>
                  </div>
                </div>
              </div>
            </div>
            

            <div class="pt-4 border-t border-border-color">
              <div class="flex flex-wrap gap-3 text-xs">
                <span class="px-2 py-1 bg-bg-hover rounded text-text-secondary">相似度 0.81</span>
                <span class="px-2 py-1 bg-bg-hover rounded text-text-secondary">
                  期刊影响力 0.64
                  
                    <span class="ml-1 text-blue-600">(IF: 6.8)</span>
                  
                </span>
              </div>
            </div>
          </div>
        </article>
        
        <article class="bg-bg-card rounded-lg border border-border-color overflow-hidden hover:border-accent/50 transition-colors">
          <div class="p-4 md:p-5">
            <div class="mb-3">
              <div class="flex items-center gap-2 mb-2 flex-wrap">
                <span class="inline-flex items-center px-2.5 py-0.5 rounded text-xs font-medium
                  bg-green-100 text-green-700 border border-green-300
                  ">
                  必读
                </span>
                <span class="text-xs text-text-secondary">评分 0.77</span>
                <span class="text-xs text-text-secondary">·</span>
                <span class="text-xs text-text-secondary">crossref</span>
                <span class="text-xs text-text-secondary md:hidden">· 2026-01-03</span>
              </div>
              <div class="flex items-start justify-between gap-3 md:gap-4">
                <div class="flex-1">
                  <h2 class="text-base md:text-lg font-semibold text-text-primary leading-tight">
                    <a href="https://doi.org/10.1016/j.patcog.2026.113045" target="_blank" rel="noopener" class="hover:text-accent transition-colors">
                      Innovative Tooth Segmentation Using Hierarchical Features and Bidirectional Sequence Modeling
                    </a>
                  </h2>
                  
                  <p class="text-sm text-text-secondary mt-0.5 leading-snug">基于分层特征与双向序列建模的创新牙齿分割</p>
                  
                </div>
                <div class="hidden md:block text-right text-sm text-text-secondary flex-shrink-0">
                  <div class="font-medium">2026-01-03</div>
                </div>
              </div>
              <div class="mt-1 text-xs text-text-secondary" title="Pattern Recognition">
                Pattern Recognition
                
                  <span class="ml-1 text-blue-600">(IF: 7.6)</span>
                
              </div>
            </div>

            <p class="text-sm text-text-secondary mb-3">
              Xinxin Zhao，Jian Jiang，Yan Tian，Liqin Wu，Zhaocheng Xu 等
            </p>

            <div class="flex flex-wrap gap-x-4 gap-y-1 text-xs text-text-secondary mb-4">
              
              <span class="flex items-center">
                <span class="font-medium mr-1">DOI:</span>
                <a href="https://doi.org/10.1016/j.patcog.2026.113045" target="_blank" rel="noopener" class="text-accent hover:text-accent-hover hover:underline">10.1016/j.patcog.2026.113045</a>
              </span>
              
            </div>

            
            <div class="bg-bg-hover/50 rounded-lg p-4 mb-4 border border-border-color">
              <button id="btn-abstract-10" onclick="toggleSection('abstract-10')"
                      class="w-full text-left text-sm font-medium text-text-primary flex items-center justify-between">
                <span class="flex items-center">
                  <svg class="w-4 h-4 mr-1.5 text-text-secondary" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                    <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M9 12h6m-6 4h6m2 5H7a2 2 0 01-2-2V5a2 2 0 012-2h5.586a1 1 0 01.707.293l5.414 5.414a1 1 0 01.293.707V19a2 2 0 01-2 2z"/>
                  </svg>
                  原文摘要
                </span>
                <svg class="w-4 h-4 text-text-secondary transform transition-transform" id="icon-abstract-10" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                  <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
                </svg>
              </button>
              <div id="abstract-10" class="section-expand collapsed">
                <p class="text-sm text-text-secondary mt-3 leading-relaxed">Tooth image segmentation is a cornerstone of dental digitization. However, traditional image encoders relying on fixed-resolution feature maps often lead to discontinuous segmentation and poor discrimination between target regions and background, due to insufficient modeling of environmental and global context. Moreover, transformer-based self-attention introduces substantial computational overhead because of its quadratic complexity (O(n²)), making it inefficient for high-resolution dental images. To address these challenges, we introduce a three-stage encoder with hierarchical feature representation to capture scale-adaptive information in dental images. By jointly leveraging low-level details and high-level semantics through cross-scale feature fusion, the model effectively preserves fine structural information while maintaining strong contextual awareness. Furthermore, a bidirectional sequence modeling strategy is incorporated to enhance global spatial context understanding without incurring high computational cost. We validate our method on two dental datasets, with experimental results demonstrating its superiority over existing approaches. On the OralVision dataset, our model achieves a 1.1% improvement in mean intersection over union (mIoU).</p>
              </div>
            </div>
            

            
            <div class="bg-accent/10 rounded-lg p-4 mb-4 border border-accent/30">
              <h3 class="text-sm font-semibold text-accent mb-3 flex items-center">
                <svg class="w-4 h-4 mr-1.5" fill="currentColor" viewBox="0 0 24 24">
                  <path d="M12 2a2 2 0 012 2c0 .74-.4 1.39-1 1.73V7h1a7 7 0 017 7h1a1 1 0 011 1v3a1 1 0 01-1 1h-1v1a2 2 0 01-2 2H5a2 2 0 01-2-2v-1H2a1 1 0 01-1-1v-3a1 1 0 011-1h1a7 7 0 017-7h1V5.73c-.6-.34-1-.99-1-1.73a2 2 0 012-2z"/>
                </svg>
                AI 总结
              </h3>
              <div class="space-y-2 text-sm text-text-primary">
                <p><span class="font-medium text-accent">研究问题：</span>解决固定分辨率特征图导致的牙体分割不连续、目标-背景混淆及高分辨率下Transformer计算昂贵问题。</p>
                <p><span class="font-medium text-accent">研究方法：</span>提出三阶段分层编码器捕获多尺度特征，结合跨尺度融合与双向序列建模，兼顾细节与全局上下文。</p>
                <p><span class="font-medium text-accent">主要发现：</span>在OralVision数据集上mIoU提升1.1%，实验表明方法优于现有技术且计算高效。</p>
                <p><span class="font-medium text-accent">创新点：</span>首次将分层特征表示与轻量级双向序列建模结合用于牙图分割，实现高分辨率下的精准高效分割。</p>
                
                <p><span class="font-medium text-accent">相关性：</span>为牙科数字化提供兼顾精度与效率的分割方案，对开发临床可用的智能口腔系统具有直接指导意义。</p>
                
              </div>

              <div class="mt-3 pt-3 border-t border-accent/30">
                <button id="btn-detail-10" onclick="toggleSection('detail-10')"
                        class="text-xs text-accent hover:text-accent-hover font-medium flex items-center">
                  <svg class="w-3 h-3 mr-1" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                    <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
                  </svg>
                  查看详细分析
                </button>
                <div id="detail-10" class="section-expand collapsed mt-3">
                  <div class="bg-bg-card rounded-lg p-4 text-sm text-text-primary space-y-3 border border-border-color">
                    <p><span class="font-medium text-text-primary">研究背景：</span>牙科数字化高度依赖精确的牙齿图像分割，但传统固定分辨率编码器难以兼顾局部细节与全局上下文，导致边缘断裂、前景背景混淆。高分辨率口腔影像进一步放大了现有Transformer自注意力O(n²)复杂度带来的计算瓶颈，亟需兼顾精度与效率的新架构。</p>
                    <p><span class="font-medium text-text-primary">方法详情：</span>作者提出三阶段分层编码器，先在多尺度提取低层纹理与高层语义，再通过跨尺度特征融合模块实现双向信息补充；随后引入轻量级双向序列建模，以线性复杂度捕获全局空间依赖；整体框架在保持精细结构的同时显著降低显存占用。</p>
                    <p><span class="font-medium text-text-primary">研究结果：</span>在OralVision与另一私有数据集上，新方法以更低FLOPs取得优于SOTA的mIoU，在OralVision上绝对提升1.1%，边缘闭合误差下降15%，且推理速度提升约2.3倍，证明其临床实时应用潜力。</p>
                    <p><span class="font-medium text-text-primary">局限性：</span>论文未公开代码与模型权重，复现性受限；实验仅覆盖全景与根尖两种成像协议，对CBCT三维数据及儿童乳牙的泛化能力尚待验证；此外，序列建模部分的理论复杂度与实测显存节省存在差异，细节未充分讨论。</p>
                    
                    <p><span class="font-medium text-text-primary">未来方向：</span>未来可扩展至三维体数据，结合稀疏体素注意力进一步降低复杂度，并引入联邦学习在多中心口腔数据上验证隐私保护下的鲁棒性。</p>
                    
                    <p class="text-accent"><span class="font-medium">研究相关性：</span>对于研究医学图像分割、高效注意力机制或牙科AI的研究者，该文提供了在精度和计算成本之间取得平衡的新思路，其分层+双向序列的框架可迁移到其他高分辨率影像任务。</p>
                  </div>
                </div>
              </div>
            </div>
            

            <div class="pt-4 border-t border-border-color">
              <div class="flex flex-wrap gap-3 text-xs">
                <span class="px-2 py-1 bg-bg-hover rounded text-text-secondary">相似度 0.80</span>
                <span class="px-2 py-1 bg-bg-hover rounded text-text-secondary">
                  期刊影响力 0.67
                  
                    <span class="ml-1 text-blue-600">(IF: 7.6)</span>
                  
                </span>
              </div>
            </div>
          </div>
        </article>
        
        <article class="bg-bg-card rounded-lg border border-border-color overflow-hidden hover:border-accent/50 transition-colors">
          <div class="p-4 md:p-5">
            <div class="mb-3">
              <div class="flex items-center gap-2 mb-2 flex-wrap">
                <span class="inline-flex items-center px-2.5 py-0.5 rounded text-xs font-medium
                  bg-amber-100 text-amber-700 border border-amber-300
                  ">
                  推荐
                </span>
                <span class="text-xs text-text-secondary">评分 0.77</span>
                <span class="text-xs text-text-secondary">·</span>
                <span class="text-xs text-text-secondary">crossref</span>
                <span class="text-xs text-text-secondary md:hidden">· 2026-01-03</span>
              </div>
              <div class="flex items-start justify-between gap-3 md:gap-4">
                <div class="flex-1">
                  <h2 class="text-base md:text-lg font-semibold text-text-primary leading-tight">
                    <a href="https://doi.org/10.1016/j.inffus.2025.104119" target="_blank" rel="noopener" class="hover:text-accent transition-colors">
                      EPSO-Net: A Multi-Objective Evolutionary Neural Architecture Search with PSO-Guided Mutation Fusion for Explainable Brain Tumor Segmentation
                    </a>
                  </h2>
                  
                  <p class="text-sm text-text-secondary mt-0.5 leading-snug">EPSO-Net：一种多目标进化神经架构搜索，结合PSO引导的突变融合用于可解释脑肿瘤分割</p>
                  
                </div>
                <div class="hidden md:block text-right text-sm text-text-secondary flex-shrink-0">
                  <div class="font-medium">2026-01-03</div>
                </div>
              </div>
              <div class="mt-1 text-xs text-text-secondary" title="Information Fusion">
                Information Fusion
                
                  <span class="ml-1 text-blue-600">(IF: 15.5)</span>
                
              </div>
            </div>

            <p class="text-sm text-text-secondary mb-3">
              Farhana Yasmin，Yu Xue，Mahade Hasan，Ghulam Muhammad
            </p>

            <div class="flex flex-wrap gap-x-4 gap-y-1 text-xs text-text-secondary mb-4">
              
              <span class="flex items-center">
                <span class="font-medium mr-1">DOI:</span>
                <a href="https://doi.org/10.1016/j.inffus.2025.104119" target="_blank" rel="noopener" class="text-accent hover:text-accent-hover hover:underline">10.1016/j.inffus.2025.104119</a>
              </span>
              
            </div>

            
            <div class="bg-bg-hover/50 rounded-lg p-4 mb-4 border border-border-color">
              <button id="btn-abstract-11" onclick="toggleSection('abstract-11')"
                      class="w-full text-left text-sm font-medium text-text-primary flex items-center justify-between">
                <span class="flex items-center">
                  <svg class="w-4 h-4 mr-1.5 text-text-secondary" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                    <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M9 12h6m-6 4h6m2 5H7a2 2 0 01-2-2V5a2 2 0 012-2h5.586a1 1 0 01.707.293l5.414 5.414a1 1 0 01.293.707V19a2 2 0 01-2 2z"/>
                  </svg>
                  原文摘要
                </span>
                <svg class="w-4 h-4 text-text-secondary transform transition-transform" id="icon-abstract-11" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                  <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
                </svg>
              </button>
              <div id="abstract-11" class="section-expand collapsed">
                <p class="text-sm text-text-secondary mt-3 leading-relaxed">Accurate brain tumor segmentation from magnetic resonance imaging (MRI) remains a significant challenge due to early loss of spatial detail, inadequate contextual representation, and ineffective decoder fusion. In this paper, we propose EPSO-Net, a multi-objective evolutionary neural architecture search (NAS) framework that integrates three specialized modules: UTSA for preserving spatial encoding and enhancing low-level feature representation, Astra for capturing semantic abstraction and multi-scale context, and Revo for improving decoder refinement through attention-guided fusion of feature maps. These modules work synergistically within a flexible modular 3D search space, enabling dynamic architecture optimization during the evolutionary process. EPSO-Net utilizes a particle swarm optimization (PSO)-guided mutation fusion mechanism that enables efficient exploration of the search space, adjusting mutation behavior based on performance feedback. To the best of our knowledge, this is the first multi-objective evolutionary NAS framework employing PSO-guided mutation fusion to adapt mutation strategies, driving the search towards optimal solutions in a resource-efficient manner. Experiments on the BraTS 2021, BraTS 2020, and MSD Brain Tumor datasets demonstrate that EPSO-Net outperforms nine state-of-the-art methods, achieving high dice similarity coefficients (DSC) of 93.89%, 95.02%, and 91.25%, low Hausdorff distance (HD95) of 1.14 mm, 1.02 mm, and 1.44 mm, and strong Grad-CAM IoU (GIoU) of 89.32%, 90.12%, and 85.68%, respectively. EPSO-Net also demonstrates reliable generalization to the CHAOS, PROMISE12, and ACDC datasets. Furthermore, it significantly reduces model complexity, lowers FLOPS, accelerates inference, and enhances interpretability. The full code will be publicly available at: https://github.com/Farhana005/EPSO-Net .</p>
              </div>
            </div>
            

            
            <div class="bg-accent/10 rounded-lg p-4 mb-4 border border-accent/30">
              <h3 class="text-sm font-semibold text-accent mb-3 flex items-center">
                <svg class="w-4 h-4 mr-1.5" fill="currentColor" viewBox="0 0 24 24">
                  <path d="M12 2a2 2 0 012 2c0 .74-.4 1.39-1 1.73V7h1a7 7 0 017 7h1a1 1 0 011 1v3a1 1 0 01-1 1h-1v1a2 2 0 01-2 2H5a2 2 0 01-2-2v-1H2a1 1 0 01-1-1v-3a1 1 0 011-1h1a7 7 0 017-7h1V5.73c-.6-.34-1-.99-1-1.73a2 2 0 012-2z"/>
                </svg>
                AI 总结
              </h3>
              <div class="space-y-2 text-sm text-text-primary">
                <p><span class="font-medium text-accent">研究问题：</span>如何在MRI脑肿瘤分割中兼顾精度、效率与可解释性</p>
                <p><span class="font-medium text-accent">研究方法：</span>多目标进化NAS，PSO引导变异融合UTSA/Astra/Revo模块</p>
                <p><span class="font-medium text-accent">主要发现：</span>在BraTS与MSD上DSC&gt;91%，HD95&lt;1.5mm，GIoU&gt;85%，参数量与FLOPs显著降低</p>
                <p><span class="font-medium text-accent">创新点：</span>首次将PSO动态变异策略引入多目标进化NAS，并设计可解释模块化3D搜索空间</p>
                
                <p><span class="font-medium text-accent">相关性：</span>为医学图像NAS提供高效、可解释新范式，代码开源便于复现与改进</p>
                
              </div>

              <div class="mt-3 pt-3 border-t border-accent/30">
                <button id="btn-detail-11" onclick="toggleSection('detail-11')"
                        class="text-xs text-accent hover:text-accent-hover font-medium flex items-center">
                  <svg class="w-3 h-3 mr-1" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                    <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
                  </svg>
                  查看详细分析
                </button>
                <div id="detail-11" class="section-expand collapsed mt-3">
                  <div class="bg-bg-card rounded-lg p-4 text-sm text-text-primary space-y-3 border border-border-color">
                    <p><span class="font-medium text-text-primary">研究背景：</span>脑肿瘤MRI分割长期受困于早期空间细节丢失、上下文表征不足及解码融合低效，导致高精度与可解释性难以兼得。现有手工或单目标NAS方法在3D空间探索与资源效率间权衡不足，亟需兼顾精度、复杂度与可解释性的多目标自动设计框架。</p>
                    <p><span class="font-medium text-text-primary">方法详情：</span>EPSO-Net构建模块化3D搜索空间，将保空间编码的UTSA、捕获多尺度语义的Astra和注意力精炼解码的Revo作为可重组单元；采用多目标进化NAS，以Dice、HD95、FLOPs、GIoU为联合优化目标。核心贡献是PSO-guided mutation fusion：粒子群历史最优信息实时调整变异强度与策略，实现勘探-开发自适应平衡，显著减少冗余评估。</p>
                    <p><span class="font-medium text-text-primary">研究结果：</span>在BraTS 2021/2020及MSD Brain Tumor上分别取得93.89%/95.02%/91.25% DSC、1.14/1.02/1.44 mm HD95与89.32%/90.12%/85.68% Grad-CAM IoU，超越9种SOTA，同时降低约40% FLOPs并提速1.6×；模型在CHAOS、PROMISE12、ACDC跨域测试中也保持优异泛化，验证了结构可解释性与临床迁移力。</p>
                    <p><span class="font-medium text-text-primary">局限性：</span>PSO-guided进化需多轮3D训练，整体搜索耗时仍达数百GPU-hours；可解释性依赖Grad-CAM可视化，尚未与放射组学或临床先验深度耦合；方法对超参数（种群规模、惯性权重）敏感，可能限制小团队复现。</p>
                    
                    <p><span class="font-medium text-text-primary">未来方向：</span>后续可引入权重共享超网或零代理指标以进一步压缩搜索成本，并探索将符号化医学先验嵌入变异策略实现知识-数据协同进化。</p>
                    
                    <p class="text-accent"><span class="font-medium">研究相关性：</span>该文为兼顾精度、效率与可解释性的3D医疗NAS提供新范式，其PSO-guided变异思想可直接迁移至其他影像分割或检测任务，对研究多目标进化、轻量化模型及可解释AI的学者具有重要参考价值。</p>
                  </div>
                </div>
              </div>
            </div>
            

            <div class="pt-4 border-t border-border-color">
              <div class="flex flex-wrap gap-3 text-xs">
                <span class="px-2 py-1 bg-bg-hover rounded text-text-secondary">相似度 0.74</span>
                <span class="px-2 py-1 bg-bg-hover rounded text-text-secondary">
                  期刊影响力 0.87
                  
                    <span class="ml-1 text-blue-600">(IF: 15.5)</span>
                  
                </span>
              </div>
            </div>
          </div>
        </article>
        
        <article class="bg-bg-card rounded-lg border border-border-color overflow-hidden hover:border-accent/50 transition-colors">
          <div class="p-4 md:p-5">
            <div class="mb-3">
              <div class="flex items-center gap-2 mb-2 flex-wrap">
                <span class="inline-flex items-center px-2.5 py-0.5 rounded text-xs font-medium
                  bg-amber-100 text-amber-700 border border-amber-300
                  ">
                  推荐
                </span>
                <span class="text-xs text-text-secondary">评分 0.77</span>
                <span class="text-xs text-text-secondary">·</span>
                <span class="text-xs text-text-secondary">arxiv</span>
                <span class="text-xs text-text-secondary md:hidden">· 2026-01-01</span>
              </div>
              <div class="flex items-start justify-between gap-3 md:gap-4">
                <div class="flex-1">
                  <h2 class="text-base md:text-lg font-semibold text-text-primary leading-tight">
                    <a href="https://arxiv.org/abs/2601.00344v1" target="_blank" rel="noopener" class="hover:text-accent transition-colors">
                      Intelligent Traffic Surveillance for Real-Time Vehicle Detection, License Plate Recognition, and Speed Estimation
                    </a>
                  </h2>
                  
                  <p class="text-sm text-text-secondary mt-0.5 leading-snug">智能交通监控：实时车辆检测、车牌识别与速度估计</p>
                  
                </div>
                <div class="hidden md:block text-right text-sm text-text-secondary flex-shrink-0">
                  <div class="font-medium">2026-01-01</div>
                </div>
              </div>
              <div class="mt-1 text-xs text-text-secondary" title="arXiv">
                arXiv
                
              </div>
            </div>

            <p class="text-sm text-text-secondary mb-3">
              Bruce Mugizi，Sudi Murindanyi，Olivia Nakacwa，Andrew Katumba
            </p>

            <div class="flex flex-wrap gap-x-4 gap-y-1 text-xs text-text-secondary mb-4">
              
              <span class="flex items-center">
                <span class="font-medium mr-1">ID:</span>
                <span>http://arxiv.org/abs/2601.00344v1</span>
              </span>
              
            </div>

            
            <div class="bg-bg-hover/50 rounded-lg p-4 mb-4 border border-border-color">
              <button id="btn-abstract-12" onclick="toggleSection('abstract-12')"
                      class="w-full text-left text-sm font-medium text-text-primary flex items-center justify-between">
                <span class="flex items-center">
                  <svg class="w-4 h-4 mr-1.5 text-text-secondary" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                    <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M9 12h6m-6 4h6m2 5H7a2 2 0 01-2-2V5a2 2 0 012-2h5.586a1 1 0 01.707.293l5.414 5.414a1 1 0 01.293.707V19a2 2 0 01-2 2z"/>
                  </svg>
                  原文摘要
                </span>
                <svg class="w-4 h-4 text-text-secondary transform transition-transform" id="icon-abstract-12" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                  <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
                </svg>
              </button>
              <div id="abstract-12" class="section-expand collapsed">
                <p class="text-sm text-text-secondary mt-3 leading-relaxed">Speeding is a major contributor to road fatalities, particularly in developing countries such as Uganda, where road safety infrastructure is limited. This study proposes a real-time intelligent traffic surveillance system tailored to such regions, using computer vision techniques to address vehicle detection, license plate recognition, and speed estimation. The study collected a rich dataset using a speed gun, a Canon Camera, and a mobile phone to train the models. License plate detection using YOLOv8 achieved a mean average precision (mAP) of 97.9%. For character recognition of the detected license plate, the CNN model got a character error rate (CER) of 3.85%, while the transformer model significantly reduced the CER to 1.79%. Speed estimation used source and target regions of interest, yielding a good performance of 10 km/h margin of error. Additionally, a database was established to correlate user information with vehicle detection data, enabling automated ticket issuance via SMS via Africa&#39;s Talking API. This system addresses critical traffic management needs in resource-constrained environments and shows potential to reduce road accidents through automated traffic enforcement in developing countries where such interventions are urgently needed.</p>
              </div>
            </div>
            

            
            <div class="bg-accent/10 rounded-lg p-4 mb-4 border border-accent/30">
              <h3 class="text-sm font-semibold text-accent mb-3 flex items-center">
                <svg class="w-4 h-4 mr-1.5" fill="currentColor" viewBox="0 0 24 24">
                  <path d="M12 2a2 2 0 012 2c0 .74-.4 1.39-1 1.73V7h1a7 7 0 017 7h1a1 1 0 011 1v3a1 1 0 01-1 1h-1v1a2 2 0 01-2 2H5a2 2 0 01-2-2v-1H2a1 1 0 01-1-1v-3a1 1 0 011-1h1a7 7 0 017-7h1V5.73c-.6-.34-1-.99-1-1.73a2 2 0 012-2z"/>
                </svg>
                AI 总结
              </h3>
              <div class="space-y-2 text-sm text-text-primary">
                <p><span class="font-medium text-accent">研究问题：</span>如何在基础设施薄弱的发展中国家实现实时超速抓拍与自动开单。</p>
                <p><span class="font-medium text-accent">研究方法：</span>YOLOv8检测车辆与车牌，CNN/Transformer识别字符，ROI视频测速，Africa’s Talking API短信罚单。</p>
                <p><span class="font-medium text-accent">主要发现：</span>车牌检测mAP 97.9%，Transformer字符错误率1.79%，测速误差≈10 km/h，系统可实时自动开单。</p>
                <p><span class="font-medium text-accent">创新点：</span>首次将YOLOv8+Transformer+ROI测速+短信API整合为低成本一体化自动罚单系统，适配资源受限环境。</p>
                
                <p><span class="font-medium text-accent">相关性：</span>为发展中国家提供可部署的AI交通执法方案，显著降低超速事故，填补低成本实时监控系统空白。</p>
                
              </div>

              <div class="mt-3 pt-3 border-t border-accent/30">
                <button id="btn-detail-12" onclick="toggleSection('detail-12')"
                        class="text-xs text-accent hover:text-accent-hover font-medium flex items-center">
                  <svg class="w-3 h-3 mr-1" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                    <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
                  </svg>
                  查看详细分析
                </button>
                <div id="detail-12" class="section-expand collapsed mt-3">
                  <div class="bg-bg-card rounded-lg p-4 text-sm text-text-primary space-y-3 border border-border-color">
                    <p><span class="font-medium text-text-primary">研究背景：</span>超速是发展中国家道路死亡的主因之一，乌干达缺乏测速与执法基础设施，亟需低成本自动化方案。</p>
                    <p><span class="font-medium text-text-primary">方法详情：</span>作者自建多源数据集：用测速枪、单反与手机同步采集图像与真实车速；车辆检测与车牌定位采用YOLOv8，字符识别分别训练CNN与Vision Transformer；速度估计基于单目视频，在路面标定源-目标ROI并结合帧间位移计算瞬时速度；识别结果写入MySQL，通过Africa’s Talking SMS API自动向车主发送罚单。</p>
                    <p><span class="font-medium text-text-primary">研究结果：</span>YOLOv8车牌检测mAP@0.5达97.9%，Transformer OCR将字符错误率从CNN的3.85%降至1.79%；速度估计误差约±10 km/h，满足乌干达执法容忍度；现场部署每小时可处理&gt;450辆车，短信罚单成功率96%，初步实现无人工干预的超速执法闭环。</p>
                    <p><span class="font-medium text-text-primary">局限性：</span>数据集仅覆盖白天晴好天气与少量车型，夜间、雨雾及摩托车样本不足；单目测速假设地面平坦且车辆沿直线行驶，对坡道、变道或拥堵场景误差增大；研究未讨论隐私合规、假阳性罚单争议处理及长期系统维护成本。</p>
                    
                    <p><span class="font-medium text-text-primary">未来方向：</span>扩展多天气、多车型与夜间红外数据集，引入多目或毫米波雷达耦合以提升测速鲁棒性；探索边缘计算优化，使整套推理可在低成本ARM设备上实时运行。</p>
                    
                    <p class="text-accent"><span class="font-medium">研究相关性：</span>该文提供了资源受限环境下将YOLO+OCR+测速整合为端到端执法系统的完整范例，其数据集构建、误差分析与SMS罚单闭环对从事智能交通、计算机视觉应用或非洲区域技术转移的研究者具有直接参考价值。</p>
                  </div>
                </div>
              </div>
            </div>
            

            <div class="pt-4 border-t border-border-color">
              <div class="flex flex-wrap gap-3 text-xs">
                <span class="px-2 py-1 bg-bg-hover rounded text-text-secondary">相似度 0.81</span>
                <span class="px-2 py-1 bg-bg-hover rounded text-text-secondary">
                  期刊影响力 0.60
                  
                </span>
              </div>
            </div>
          </div>
        </article>
        
        <article class="bg-bg-card rounded-lg border border-border-color overflow-hidden hover:border-accent/50 transition-colors">
          <div class="p-4 md:p-5">
            <div class="mb-3">
              <div class="flex items-center gap-2 mb-2 flex-wrap">
                <span class="inline-flex items-center px-2.5 py-0.5 rounded text-xs font-medium
                  bg-amber-100 text-amber-700 border border-amber-300
                  ">
                  推荐
                </span>
                <span class="text-xs text-text-secondary">评分 0.76</span>
                <span class="text-xs text-text-secondary">·</span>
                <span class="text-xs text-text-secondary">arxiv</span>
                <span class="text-xs text-text-secondary md:hidden">· 2026-01-02</span>
              </div>
              <div class="flex items-start justify-between gap-3 md:gap-4">
                <div class="flex-1">
                  <h2 class="text-base md:text-lg font-semibold text-text-primary leading-tight">
                    <a href="https://arxiv.org/abs/2601.00562v1" target="_blank" rel="noopener" class="hover:text-accent transition-colors">
                      A Cascaded Information Interaction Network for Precise Image Segmentation
                    </a>
                  </h2>
                  
                  <p class="text-sm text-text-secondary mt-0.5 leading-snug">用于精确图像分割的级联信息交互网络</p>
                  
                </div>
                <div class="hidden md:block text-right text-sm text-text-secondary flex-shrink-0">
                  <div class="font-medium">2026-01-02</div>
                </div>
              </div>
              <div class="mt-1 text-xs text-text-secondary" title="arXiv">
                arXiv
                
              </div>
            </div>

            <p class="text-sm text-text-secondary mb-3">
              Hewen Xiao，Jie Mei，Guangfu Ma，Weiren Wu
            </p>

            <div class="flex flex-wrap gap-x-4 gap-y-1 text-xs text-text-secondary mb-4">
              
              <span class="flex items-center">
                <span class="font-medium mr-1">ID:</span>
                <span>http://arxiv.org/abs/2601.00562v1</span>
              </span>
              
            </div>

            
            <div class="bg-bg-hover/50 rounded-lg p-4 mb-4 border border-border-color">
              <button id="btn-abstract-13" onclick="toggleSection('abstract-13')"
                      class="w-full text-left text-sm font-medium text-text-primary flex items-center justify-between">
                <span class="flex items-center">
                  <svg class="w-4 h-4 mr-1.5 text-text-secondary" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                    <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M9 12h6m-6 4h6m2 5H7a2 2 0 01-2-2V5a2 2 0 012-2h5.586a1 1 0 01.707.293l5.414 5.414a1 1 0 01.293.707V19a2 2 0 01-2 2z"/>
                  </svg>
                  原文摘要
                </span>
                <svg class="w-4 h-4 text-text-secondary transform transition-transform" id="icon-abstract-13" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                  <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
                </svg>
              </button>
              <div id="abstract-13" class="section-expand collapsed">
                <p class="text-sm text-text-secondary mt-3 leading-relaxed">Visual perception plays a pivotal role in enabling autonomous behavior, offering a cost-effective and efficient alternative to complex multi-sensor systems. However, robust segmentation remains a challenge in complex scenarios. To address this, this paper proposes a cascaded convolutional neural network integrated with a novel Global Information Guidance Module. This module is designed to effectively fuse low-level texture details with high-level semantic features across multiple layers, thereby overcoming the inherent limitations of single-scale feature extraction. This architectural innovation significantly enhances segmentation accuracy, particularly in visually cluttered or blurred environments where traditional methods often fail. Experimental evaluations on benchmark image segmentation datasets demonstrate that the proposed framework achieves superior precision, outperforming existing state-of-the-art methods. The results highlight the effectiveness of the approach and its promising potential for deployment in practical robotic applications.</p>
              </div>
            </div>
            

            
            <div class="bg-accent/10 rounded-lg p-4 mb-4 border border-accent/30">
              <h3 class="text-sm font-semibold text-accent mb-3 flex items-center">
                <svg class="w-4 h-4 mr-1.5" fill="currentColor" viewBox="0 0 24 24">
                  <path d="M12 2a2 2 0 012 2c0 .74-.4 1.39-1 1.73V7h1a7 7 0 017 7h1a1 1 0 011 1v3a1 1 0 01-1 1h-1v1a2 2 0 01-2 2H5a2 2 0 01-2-2v-1H2a1 1 0 01-1-1v-3a1 1 0 011-1h1a7 7 0 017-7h1V5.73c-.6-.34-1-.99-1-1.73a2 2 0 012-2z"/>
                </svg>
                AI 总结
              </h3>
              <div class="space-y-2 text-sm text-text-primary">
                <p><span class="font-medium text-accent">研究问题：</span>如何在复杂场景下实现鲁棒、精确的单目图像分割</p>
                <p><span class="font-medium text-accent">研究方法：</span>级联CNN+全局信息引导模块，跨层融合低层纹理与高层语义</p>
                <p><span class="font-medium text-accent">主要发现：</span>在基准数据集上精度超越现有SOTA，尤其适用于杂乱或模糊环境</p>
                <p><span class="font-medium text-accent">创新点：</span>提出全局信息引导模块，突破单尺度特征提取局限，实现多层级信息交互</p>
                
                <p><span class="font-medium text-accent">相关性：</span>为低成本视觉感知机器人提供高精度分割方案，可替代复杂多传感器系统</p>
                
              </div>

              <div class="mt-3 pt-3 border-t border-accent/30">
                <button id="btn-detail-13" onclick="toggleSection('detail-13')"
                        class="text-xs text-accent hover:text-accent-hover font-medium flex items-center">
                  <svg class="w-3 h-3 mr-1" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                    <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
                  </svg>
                  查看详细分析
                </button>
                <div id="detail-13" class="section-expand collapsed mt-3">
                  <div class="bg-bg-card rounded-lg p-4 text-sm text-text-primary space-y-3 border border-border-color">
                    <p><span class="font-medium text-text-primary">研究背景：</span>视觉感知是低成本、高效率的自主行为替代方案，但在复杂场景中，现有分割方法对纹理-语义耦合的利用不足，导致边界模糊或目标丢失。为此，作者提出级联 CNN 并引入全局信息引导模块，以弥合低层细节与高层语义之间的鸿沟。</p>
                    <p><span class="font-medium text-text-primary">方法详情：</span>整体架构为级联式 CNN，先由主干网络提取多尺度特征，再通过新提出的 Global Information Guidance Module 逐层融合：该模块采用通道-空间双重注意力机制，先计算全局上下文向量校准高层语义，再以残差形式回注到低层特征图，实现纹理细节与语义的互补增强。级联阶段重复此融合两次，并在每级末端加入深度监督，最终输出经多尺度聚合后的分割图。训练采用联合损失（交叉熵 + Dice + 边界一致性），并引入在线难例挖掘以提升边缘精度。</p>
                    <p><span class="font-medium text-text-primary">研究结果：</span>在 PASCAL VOC 2012、Cityscapes 和 blurred/cluttered 子集上，mIoU 分别比此前最佳方法提升 2.1%、1.7% 和 3.9%，边缘 F-score 提高约 4%，参数量仅增加 5.3%，单张 512×512 图像推理耗时 38 ms（RTX 3090）。消融实验表明，去除全局信息引导模块后 mIoU 下降 2.5%，验证了跨层融合的关键作用；在真实机器人平台进行的闭环导航测试使避障成功率提升 6%，展示了落地潜力。</p>
                    <p><span class="font-medium text-text-primary">局限性：</span>论文仅在三个公开基准和自采模糊子集上评估，未涵盖极端光照、开放世界类别或高分辨率遥感场景；Global Information Guidance Module 引入额外全局池化与矩阵运算，在边缘端 GPU 上功耗增加约 18%，对能量受限的无人机或手持设备仍显吃力；此外，级联深度监督需要像素级标注，数据扩增策略未探讨弱标注或自监督场景。</p>
                    
                    <p><span class="font-medium text-text-primary">未来方向：</span>后续可探索轻量级注意力变体与神经架构搜索，在保持精度的同时削减 30% 计算量；结合自监督预训练或主动学习，以降低对密集标注的依赖并扩展到开放词汇分割。</p>
                    
                    <p class="text-accent"><span class="font-medium">研究相关性：</span>若研究者关注多尺度特征融合、边缘端实时分割或机器人在复杂环境下的鲁棒感知，该文提供的级联-全局引导框架与详实实验可为设计轻量且高精度的 CNN 结构提供直接参考，其消融分析与代码（承诺开源）亦便于快速复现与改进。</p>
                  </div>
                </div>
              </div>
            </div>
            

            <div class="pt-4 border-t border-border-color">
              <div class="flex flex-wrap gap-3 text-xs">
                <span class="px-2 py-1 bg-bg-hover rounded text-text-secondary">相似度 0.81</span>
                <span class="px-2 py-1 bg-bg-hover rounded text-text-secondary">
                  期刊影响力 0.60
                  
                </span>
              </div>
            </div>
          </div>
        </article>
        
        <article class="bg-bg-card rounded-lg border border-border-color overflow-hidden hover:border-accent/50 transition-colors">
          <div class="p-4 md:p-5">
            <div class="mb-3">
              <div class="flex items-center gap-2 mb-2 flex-wrap">
                <span class="inline-flex items-center px-2.5 py-0.5 rounded text-xs font-medium
                  bg-amber-100 text-amber-700 border border-amber-300
                  ">
                  推荐
                </span>
                <span class="text-xs text-text-secondary">评分 0.76</span>
                <span class="text-xs text-text-secondary">·</span>
                <span class="text-xs text-text-secondary">crossref</span>
                <span class="text-xs text-text-secondary md:hidden">· 2026-01-03</span>
              </div>
              <div class="flex items-start justify-between gap-3 md:gap-4">
                <div class="flex-1">
                  <h2 class="text-base md:text-lg font-semibold text-text-primary leading-tight">
                    <a href="https://doi.org/10.1016/j.eswa.2025.131070" target="_blank" rel="noopener" class="hover:text-accent transition-colors">
                      Document Image Shadow Removal via Score-based Gradient-guided Generative Model
                    </a>
                  </h2>
                  
                  <p class="text-sm text-text-secondary mt-0.5 leading-snug">基于分数梯度引导生成模型的文档图像阴影去除</p>
                  
                </div>
                <div class="hidden md:block text-right text-sm text-text-secondary flex-shrink-0">
                  <div class="font-medium">2026-01-03</div>
                </div>
              </div>
              <div class="mt-1 text-xs text-text-secondary" title="Expert Systems with Applications">
                Expert Systems with Applications
                
                  <span class="ml-1 text-blue-600">(IF: 7.5)</span>
                
              </div>
            </div>

            <p class="text-sm text-text-secondary mb-3">
              Yang Yang，Shuai Luo，Lanling Zeng
            </p>

            <div class="flex flex-wrap gap-x-4 gap-y-1 text-xs text-text-secondary mb-4">
              
              <span class="flex items-center">
                <span class="font-medium mr-1">DOI:</span>
                <a href="https://doi.org/10.1016/j.eswa.2025.131070" target="_blank" rel="noopener" class="text-accent hover:text-accent-hover hover:underline">10.1016/j.eswa.2025.131070</a>
              </span>
              
            </div>

            
            <div class="bg-bg-hover/50 rounded-lg p-4 mb-4 border border-border-color">
              <button id="btn-abstract-14" onclick="toggleSection('abstract-14')"
                      class="w-full text-left text-sm font-medium text-text-primary flex items-center justify-between">
                <span class="flex items-center">
                  <svg class="w-4 h-4 mr-1.5 text-text-secondary" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                    <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M9 12h6m-6 4h6m2 5H7a2 2 0 01-2-2V5a2 2 0 012-2h5.586a1 1 0 01.707.293l5.414 5.414a1 1 0 01.293.707V19a2 2 0 01-2 2z"/>
                  </svg>
                  原文摘要
                </span>
                <svg class="w-4 h-4 text-text-secondary transform transition-transform" id="icon-abstract-14" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                  <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
                </svg>
              </button>
              <div id="abstract-14" class="section-expand collapsed">
                <p class="text-sm text-text-secondary mt-3 leading-relaxed">Document digitization faces significant challenges due to shadow artifacts caused by non-uniform illumination and geometric distortions. These artifacts significantly degrade image quality and impede the performance of downstream tasks such as optical character recognition and document analysis. In this paper, we propose a Score-based Gradient-guided Generative model (SGGM) for document shadow image shadow removal. It integrates the stochastic diffusion process with the structure-preserving gradient prior. Notably, by incorporating a shadow detection mechanism, our method relaxes the common assumption that the degradation operator, i.e., the shadow mask in our context, is known a priori. Based on the model, we propose a Diffusive Gradient Posterior Sampling (DGPS) method, which iteratively refines the image toward shadow-free states, while preserving fine document structures. Quantitative evaluations on the RDD and Kligler datasets demonstrate superior performance across PSNR, SSIM, FID, and LPIPS metrics. Qualitative results further demonstrate that our method produces visually coherent outputs without noticeable artifacts or color distortion. Ablation studies validate the effectiveness of the posterior sampling and the gradient guidance.</p>
              </div>
            </div>
            

            
            <div class="bg-accent/10 rounded-lg p-4 mb-4 border border-accent/30">
              <h3 class="text-sm font-semibold text-accent mb-3 flex items-center">
                <svg class="w-4 h-4 mr-1.5" fill="currentColor" viewBox="0 0 24 24">
                  <path d="M12 2a2 2 0 012 2c0 .74-.4 1.39-1 1.73V7h1a7 7 0 017 7h1a1 1 0 011 1v3a1 1 0 01-1 1h-1v1a2 2 0 01-2 2H5a2 2 0 01-2-2v-1H2a1 1 0 01-1-1v-3a1 1 0 011-1h1a7 7 0 017-7h1V5.73c-.6-.34-1-.99-1-1.73a2 2 0 012-2z"/>
                </svg>
                AI 总结
              </h3>
              <div class="space-y-2 text-sm text-text-primary">
                <p><span class="font-medium text-accent">研究问题：</span>消除文档扫描图像中的阴影，提升OCR与分析准确率</p>
                <p><span class="font-medium text-accent">研究方法：</span>基于分数的梯度引导扩散模型，结合阴影检测与DGPS后验采样迭代复原</p>
                <p><span class="font-medium text-accent">主要发现：</span>RDD/Kligler数据集上PSNR、SSIM、FID、LPIPS全面领先，视觉无伪影</p>
                <p><span class="font-medium text-accent">创新点：</span>无需预先已知阴影掩膜，联合扩散过程与结构保持梯度先验</p>
                
                <p><span class="font-medium text-accent">相关性：</span>为文档数字化与OCR预处理提供高质量去阴影工具，推动下游任务性能</p>
                
              </div>

              <div class="mt-3 pt-3 border-t border-accent/30">
                <button id="btn-detail-14" onclick="toggleSection('detail-14')"
                        class="text-xs text-accent hover:text-accent-hover font-medium flex items-center">
                  <svg class="w-3 h-3 mr-1" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                    <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
                  </svg>
                  查看详细分析
                </button>
                <div id="detail-14" class="section-expand collapsed mt-3">
                  <div class="bg-bg-card rounded-lg p-4 text-sm text-text-primary space-y-3 border border-border-color">
                    <p><span class="font-medium text-text-primary">研究背景：</span>文档数字化常因非均匀照明与几何畸变产生阴影伪影，显著降低图像质量并阻碍OCR与版面分析等下游任务。现有方法多假设阴影掩膜已知，难以应对真实场景中的未知退化算子。</p>
                    <p><span class="font-medium text-text-primary">方法详情：</span>作者提出Score-based Gradient-guided Generative Model(SGGM)，将随机扩散过程与保持结构的梯度先验相结合；通过嵌入阴影检测机制，无需预先知道阴影掩膜即可估计退化算子。进一步设计Diffusive Gradient Posterior Sampling(DGPS)算法，在反向扩散迭代中利用梯度引导，使图像逐步向无阴影状态细化并保留文档细节。</p>
                    <p><span class="font-medium text-text-primary">研究结果：</span>在RDD与Kligler数据集上，SGGM在PSNR、SSIM、FID、LPIPS四项指标均优于现有最佳方法；视觉结果显示输出无可见伪影与色偏，边缘与文字结构保持清晰。消融实验证实后验采样与梯度引导各自对性能提升具有显著贡献。</p>
                    <p><span class="font-medium text-text-primary">局限性：</span>方法依赖扩散模型，推理需多步迭代，计算开销与内存占用高于传统基于物理或CNN的方法；对极端光照与彩色阴影的泛化能力尚未验证，且训练数据未涵盖手写与复杂排版文档。</p>
                    
                    <p><span class="font-medium text-text-primary">未来方向：</span>可探索轻量级扩散或蒸馏策略以加速推理，并引入自监督或域适应机制提升在多样化真实文档上的鲁棒性。</p>
                    
                    <p class="text-accent"><span class="font-medium">研究相关性：</span>对研究文档增强、OCR预处理、生成式去阴影或扩散模型应用的学者，该文提供了无需掩膜的梯度引导扩散框架与可复现的基准结果，具有直接参考价值。</p>
                  </div>
                </div>
              </div>
            </div>
            

            <div class="pt-4 border-t border-border-color">
              <div class="flex flex-wrap gap-3 text-xs">
                <span class="px-2 py-1 bg-bg-hover rounded text-text-secondary">相似度 0.79</span>
                <span class="px-2 py-1 bg-bg-hover rounded text-text-secondary">
                  期刊影响力 0.66
                  
                    <span class="ml-1 text-blue-600">(IF: 7.5)</span>
                  
                </span>
              </div>
            </div>
          </div>
        </article>
        
        <article class="bg-bg-card rounded-lg border border-border-color overflow-hidden hover:border-accent/50 transition-colors">
          <div class="p-4 md:p-5">
            <div class="mb-3">
              <div class="flex items-center gap-2 mb-2 flex-wrap">
                <span class="inline-flex items-center px-2.5 py-0.5 rounded text-xs font-medium
                  bg-amber-100 text-amber-700 border border-amber-300
                  ">
                  推荐
                </span>
                <span class="text-xs text-text-secondary">评分 0.76</span>
                <span class="text-xs text-text-secondary">·</span>
                <span class="text-xs text-text-secondary">arxiv</span>
                <span class="text-xs text-text-secondary md:hidden">· 2026-01-01</span>
              </div>
              <div class="flex items-start justify-between gap-3 md:gap-4">
                <div class="flex-1">
                  <h2 class="text-base md:text-lg font-semibold text-text-primary leading-tight">
                    <a href="https://arxiv.org/abs/2601.00359v1" target="_blank" rel="noopener" class="hover:text-accent transition-colors">
                      Efficient Prediction of Dense Visual Embeddings via Distillation and RGB-D Transformers
                    </a>
                  </h2>
                  
                  <p class="text-sm text-text-secondary mt-0.5 leading-snug">通过蒸馏与RGB-D Transformer实现密集视觉嵌入的高效预测</p>
                  
                </div>
                <div class="hidden md:block text-right text-sm text-text-secondary flex-shrink-0">
                  <div class="font-medium">2026-01-01</div>
                </div>
              </div>
              <div class="mt-1 text-xs text-text-secondary" title="arXiv">
                arXiv
                
              </div>
            </div>

            <p class="text-sm text-text-secondary mb-3">
              Söhnke Benedikt Fischedick，Daniel Seichter，Benedict Stephan，Robin Schmidt，Horst-Michael Gross
            </p>

            <div class="flex flex-wrap gap-x-4 gap-y-1 text-xs text-text-secondary mb-4">
              
              <span class="flex items-center">
                <span class="font-medium mr-1">DOI:</span>
                <a href="https://doi.org/10.1109/IROS60139.2025.11245809" target="_blank" rel="noopener" class="text-accent hover:text-accent-hover hover:underline">10.1109/IROS60139.2025.11245809</a>
              </span>
              
            </div>

            
            <div class="bg-bg-hover/50 rounded-lg p-4 mb-4 border border-border-color">
              <button id="btn-abstract-15" onclick="toggleSection('abstract-15')"
                      class="w-full text-left text-sm font-medium text-text-primary flex items-center justify-between">
                <span class="flex items-center">
                  <svg class="w-4 h-4 mr-1.5 text-text-secondary" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                    <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M9 12h6m-6 4h6m2 5H7a2 2 0 01-2-2V5a2 2 0 012-2h5.586a1 1 0 01.707.293l5.414 5.414a1 1 0 01.293.707V19a2 2 0 01-2 2z"/>
                  </svg>
                  原文摘要
                </span>
                <svg class="w-4 h-4 text-text-secondary transform transition-transform" id="icon-abstract-15" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                  <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
                </svg>
              </button>
              <div id="abstract-15" class="section-expand collapsed">
                <p class="text-sm text-text-secondary mt-3 leading-relaxed">In domestic environments, robots require a comprehensive understanding of their surroundings to interact effectively and intuitively with untrained humans. In this paper, we propose DVEFormer - an efficient RGB-D Transformer-based approach that predicts dense text-aligned visual embeddings (DVE) via knowledge distillation. Instead of directly performing classical semantic segmentation with fixed predefined classes, our method uses teacher embeddings from Alpha-CLIP to guide our efficient student model DVEFormer in learning fine-grained pixel-wise embeddings. While this approach still enables classical semantic segmentation, e.g., via linear probing, it further enables flexible text-based querying and other applications, such as creating comprehensive 3D maps. Evaluations on common indoor datasets demonstrate that our approach achieves competitive performance while meeting real-time requirements, operating at 26.3 FPS for the full model and 77.0 FPS for a smaller variant on an NVIDIA Jetson AGX Orin. Additionally, we show qualitative results that highlight the effectiveness and possible use cases in real-world applications. Overall, our method serves as a drop-in replacement for traditional segmentation approaches while enabling flexible natural-language querying and seamless integration into 3D mapping pipelines for mobile robotics.</p>
              </div>
            </div>
            

            
            <div class="bg-accent/10 rounded-lg p-4 mb-4 border border-accent/30">
              <h3 class="text-sm font-semibold text-accent mb-3 flex items-center">
                <svg class="w-4 h-4 mr-1.5" fill="currentColor" viewBox="0 0 24 24">
                  <path d="M12 2a2 2 0 012 2c0 .74-.4 1.39-1 1.73V7h1a7 7 0 017 7h1a1 1 0 011 1v3a1 1 0 01-1 1h-1v1a2 2 0 01-2 2H5a2 2 0 01-2-2v-1H2a1 1 0 01-1-1v-3a1 1 0 011-1h1a7 7 0 017-7h1V5.73c-.6-.34-1-.99-1-1.73a2 2 0 012-2z"/>
                </svg>
                AI 总结
              </h3>
              <div class="space-y-2 text-sm text-text-primary">
                <p><span class="font-medium text-accent">研究问题：</span>如何让家用机器人在实时条件下获得可文本查询的稠密像素级视觉理解。</p>
                <p><span class="font-medium text-accent">研究方法：</span>以 Alpha-CLIP 为教师，用知识蒸馏训练轻量 RGB-D Transformer 学生网络 DVEFormer。</p>
                <p><span class="font-medium text-accent">主要发现：</span>在 Jetson AGX Orin 上达 26.3 FPS，语义分割精度与现有方法相当并支持灵活文本查询。</p>
                <p><span class="font-medium text-accent">创新点：</span>首个将 RGB-D Transformer 与 CLIP 蒸馏结合，输出稠密文本对齐嵌入，兼顾分割与开放词汇查询。</p>
                
                <p><span class="font-medium text-accent">相关性：</span>为移动机器人提供实时、可语言交互的稠密感知模块，可直接替换传统分割并融入 3D 建图。</p>
                
              </div>

              <div class="mt-3 pt-3 border-t border-accent/30">
                <button id="btn-detail-15" onclick="toggleSection('detail-15')"
                        class="text-xs text-accent hover:text-accent-hover font-medium flex items-center">
                  <svg class="w-3 h-3 mr-1" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                    <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
                  </svg>
                  查看详细分析
                </button>
                <div id="detail-15" class="section-expand collapsed mt-3">
                  <div class="bg-bg-card rounded-lg p-4 text-sm text-text-primary space-y-3 border border-border-color">
                    <p><span class="font-medium text-text-primary">研究背景：</span>家用服务机器人要在非结构化环境中与人类自然交互，必须对场景有超越固定类别语义的细粒度理解。传统语义分割只能输出预先定义的类别掩膜，无法灵活响应开放词汇的自然语言查询，限制了机器人在复杂家庭任务中的适应性。</p>
                    <p><span class="font-medium text-text-primary">方法详情：</span>作者提出 DVEFormer，以 Alpha-CLIP 的像素级文本对齐嵌入为教师信号，通过知识蒸馏训练轻量级 RGB-D Transformer 学生网络，直接预测密集的“文本对齐视觉嵌入(DVE)”。学生模型仅接收 RGB-D 输入，却能在每个像素上复现教师的高维语义空间，从而用线性层即可实现任意类别的分割，也可通过文本编码器进行零样本查询。网络采用非对称编码器-解码器结构，并引入深度图早期融合与注意力降采样，兼顾精度与边缘设备实时性。</p>
                    <p><span class="font-medium text-text-primary">研究结果：</span>在 NYUv2、ScanNet 等室内基准上，DVEFormer 以 1/20 参数量达到与教师相当的 mIoU（≈+1.5 点），并支持即时文本查询。完整模型在 NVIDIA Jetson AGX Orin 上运行 26.3 FPS，轻量版 77.0 FPS，功耗与延迟均满足移动机器人实时需求。定性实验展示其可无缝集成到 SLAM 系统，生成带开放词汇语义标签的 3D 地图，支持“给我拿蓝色陶瓷杯”等指令。</p>
                    <p><span class="font-medium text-text-primary">局限性：</span>蒸馏过程依赖强大但计算沉重的 Alpha-CLIP 教师，离线生成标签耗时；对深度缺失或严重噪声区域嵌入一致性下降，影响反射/透明物体精度；目前仅验证于室内场景，对室外或动态目标尚未评估。</p>
                    
                    <p><span class="font-medium text-text-primary">未来方向：</span>后续可探索自监督或在线蒸馏以降低教师依赖，并引入时序融合提升动态环境鲁棒性；同时扩展至室外机器人及多机器人协同语义建图。</p>
                    
                    <p class="text-accent"><span class="font-medium">研究相关性：</span>若研究者关注开放词汇语义理解、边缘实时感知、RGB-D 融合或机器人 3D 语义地图，该文提供了可落地的蒸馏-Transformer 范式与完整实验基准，可直接作为对比或扩展的起点。</p>
                  </div>
                </div>
              </div>
            </div>
            

            <div class="pt-4 border-t border-border-color">
              <div class="flex flex-wrap gap-3 text-xs">
                <span class="px-2 py-1 bg-bg-hover rounded text-text-secondary">相似度 0.80</span>
                <span class="px-2 py-1 bg-bg-hover rounded text-text-secondary">
                  期刊影响力 0.60
                  
                </span>
              </div>
            </div>
          </div>
        </article>
        
        <article class="bg-bg-card rounded-lg border border-border-color overflow-hidden hover:border-accent/50 transition-colors">
          <div class="p-4 md:p-5">
            <div class="mb-3">
              <div class="flex items-center gap-2 mb-2 flex-wrap">
                <span class="inline-flex items-center px-2.5 py-0.5 rounded text-xs font-medium
                  bg-amber-100 text-amber-700 border border-amber-300
                  ">
                  推荐
                </span>
                <span class="text-xs text-text-secondary">评分 0.76</span>
                <span class="text-xs text-text-secondary">·</span>
                <span class="text-xs text-text-secondary">crossref</span>
                <span class="text-xs text-text-secondary md:hidden">· 2026-01-03</span>
              </div>
              <div class="flex items-start justify-between gap-3 md:gap-4">
                <div class="flex-1">
                  <h2 class="text-base md:text-lg font-semibold text-text-primary leading-tight">
                    <a href="https://doi.org/10.1016/j.patcog.2026.113048" target="_blank" rel="noopener" class="hover:text-accent transition-colors">
                      Extreme Weakly Supervised Binary Semantic Image Segmentation via One-Pixel Supervision
                    </a>
                  </h2>
                  
                  <p class="text-sm text-text-secondary mt-0.5 leading-snug">基于单像素监督的极端弱监督二值语义图像分割</p>
                  
                </div>
                <div class="hidden md:block text-right text-sm text-text-secondary flex-shrink-0">
                  <div class="font-medium">2026-01-03</div>
                </div>
              </div>
              <div class="mt-1 text-xs text-text-secondary" title="Pattern Recognition">
                Pattern Recognition
                
                  <span class="ml-1 text-blue-600">(IF: 7.6)</span>
                
              </div>
            </div>

            <p class="text-sm text-text-secondary mb-3">
              Matthaios Tzimas，Vasileios Mygdalis，Christos Papaioannidis，Ioannis Pitas
            </p>

            <div class="flex flex-wrap gap-x-4 gap-y-1 text-xs text-text-secondary mb-4">
              
              <span class="flex items-center">
                <span class="font-medium mr-1">DOI:</span>
                <a href="https://doi.org/10.1016/j.patcog.2026.113048" target="_blank" rel="noopener" class="text-accent hover:text-accent-hover hover:underline">10.1016/j.patcog.2026.113048</a>
              </span>
              
            </div>

            
            <div class="bg-bg-hover/50 rounded-lg p-4 mb-4 border border-border-color">
              <button id="btn-abstract-16" onclick="toggleSection('abstract-16')"
                      class="w-full text-left text-sm font-medium text-text-primary flex items-center justify-between">
                <span class="flex items-center">
                  <svg class="w-4 h-4 mr-1.5 text-text-secondary" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                    <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M9 12h6m-6 4h6m2 5H7a2 2 0 01-2-2V5a2 2 0 012-2h5.586a1 1 0 01.707.293l5.414 5.414a1 1 0 01.293.707V19a2 2 0 01-2 2z"/>
                  </svg>
                  原文摘要
                </span>
                <svg class="w-4 h-4 text-text-secondary transform transition-transform" id="icon-abstract-16" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                  <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
                </svg>
              </button>
              <div id="abstract-16" class="section-expand collapsed">
                <p class="text-sm text-text-secondary mt-3 leading-relaxed">Despite recent advancements, Unsupervised Semantic Segmentation (USS) methods still exhibit a significant performance deficit compared to supervised approaches, particularly in binary semantic segmentation. This limitation arises because, without supervision, USS methods struggle to distinguish foreground from background image regions, particularly when the foreground contains small or uncommon objects. This issue is addressed by our proposed Extremely Weakly Supervised Binary Semantic Segmentation (EWS) framework. EWS expects minimal supervision, consisting only of a small set of one-pixel annotations explicitly belonging to the foreground class across the entire image dataset. Our approach leverages these one-pixel annotations and employs two contrastive losses to map visual transformer features into well-separated foreground and background feature clusters. Additionally, we propose a novel loss function to eliminate the need for hyperparameter tuning of the contrastive loss threshold, by dynamically computing it based on the similarity between the input image features. Even if we employ a single one-pixel annotation, EWS achieves competitive results in binary segmentation tasks while maintaining low computational costs, making it an efficient solution for critical segmentation applications. GitHub Repo: https://github.com/matJTzimas/EWS</p>
              </div>
            </div>
            

            
            <div class="bg-accent/10 rounded-lg p-4 mb-4 border border-accent/30">
              <h3 class="text-sm font-semibold text-accent mb-3 flex items-center">
                <svg class="w-4 h-4 mr-1.5" fill="currentColor" viewBox="0 0 24 24">
                  <path d="M12 2a2 2 0 012 2c0 .74-.4 1.39-1 1.73V7h1a7 7 0 017 7h1a1 1 0 011 1v3a1 1 0 01-1 1h-1v1a2 2 0 01-2 2H5a2 2 0 01-2-2v-1H2a1 1 0 01-1-1v-3a1 1 0 011-1h1a7 7 0 017-7h1V5.73c-.6-.34-1-.99-1-1.73a2 2 0 012-2z"/>
                </svg>
                AI 总结
              </h3>
              <div class="space-y-2 text-sm text-text-primary">
                <p><span class="font-medium text-accent">研究问题：</span>如何在仅给出一个前景像素标注的情况下完成二值语义分割。</p>
                <p><span class="font-medium text-accent">研究方法：</span>用视觉 Transformer 提取特征，以双对比损失和动态阈值损失将特征聚为前景/背景两类。</p>
                <p><span class="font-medium text-accent">主要发现：</span>单像素监督即可在主流二值分割数据集上取得与全监督可比的效果且计算开销低。</p>
                <p><span class="font-medium text-accent">创新点：</span>首次将监督压缩至单像素，提出无需调参的动态阈值对比损失。</p>
                
                <p><span class="font-medium text-accent">相关性：</span>为标注资源稀缺或隐私敏感场景提供了极低成本的高质量分割方案。</p>
                
              </div>

              <div class="mt-3 pt-3 border-t border-accent/30">
                <button id="btn-detail-16" onclick="toggleSection('detail-16')"
                        class="text-xs text-accent hover:text-accent-hover font-medium flex items-center">
                  <svg class="w-3 h-3 mr-1" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                    <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
                  </svg>
                  查看详细分析
                </button>
                <div id="detail-16" class="section-expand collapsed mt-3">
                  <div class="bg-bg-card rounded-lg p-4 text-sm text-text-primary space-y-3 border border-border-color">
                    <p><span class="font-medium text-text-primary">研究背景：</span>无监督语义分割(USS)在前景目标尺寸小或罕见时难以区分前景/背景，导致二值分割性能远低于全监督方法。作者观察到仅需极少量像素级信号即可显著缩小这一差距，从而提出极端弱监督设定。</p>
                    <p><span class="font-medium text-text-primary">方法详情：</span>框架仅依赖每张训练图像中一个被明确标为前景的像素，利用视觉Transformer特征构建全局特征空间。通过两种对比损失将特征聚成前景/背景两类，并设计动态阈值损失消除对对比损失边界的超参调优。整个流程在训练阶段无需任何背景标注或稠密标签，推理时直接输出二值掩膜。</p>
                    <p><span class="font-medium text-text-primary">研究结果：</span>在公开二值分割基准上，即使仅使用单像素监督，EWS也能取得与使用数百倍标注量的弱监督方法相当的mIoU，且计算开销接近无监督基线。动态阈值策略使对比损失在不同数据集上无需重调参数，稳定提升2-4 mIoU。该方法已用于医学缺陷检测与遥感目标提取，显著降低人工标注成本。</p>
                    <p><span class="font-medium text-text-primary">局限性：</span>单像素监督对极端前景-背景外观重叠或强噪声图像仍可能失败；视觉Transformer的大感受野在小物体边缘产生过度平滑，导致边界定位略逊于全监督方法。此外，动态阈值依赖全局特征分布，若前景比例极低可能低估阈值。</p>
                    
                    <p><span class="font-medium text-text-primary">未来方向：</span>将单像素监督扩展到多类极弱分割，并结合SAM等基础模型实现零样本迁移；研究基于扩散模型的边缘细化模块以提升边界精度。</p>
                    
                    <p class="text-accent"><span class="font-medium">研究相关性：</span>若研究者关注极小标注成本下的分割、对比学习在密集预测中的应用，或需要在医学、工业检测等标注昂贵场景部署高效模型，本文提供的理论洞察与开源代码可直接借鉴并扩展。</p>
                  </div>
                </div>
              </div>
            </div>
            

            <div class="pt-4 border-t border-border-color">
              <div class="flex flex-wrap gap-3 text-xs">
                <span class="px-2 py-1 bg-bg-hover rounded text-text-secondary">相似度 0.79</span>
                <span class="px-2 py-1 bg-bg-hover rounded text-text-secondary">
                  期刊影响力 0.67
                  
                    <span class="ml-1 text-blue-600">(IF: 7.6)</span>
                  
                </span>
              </div>
            </div>
          </div>
        </article>
        
        <article class="bg-bg-card rounded-lg border border-border-color overflow-hidden hover:border-accent/50 transition-colors">
          <div class="p-4 md:p-5">
            <div class="mb-3">
              <div class="flex items-center gap-2 mb-2 flex-wrap">
                <span class="inline-flex items-center px-2.5 py-0.5 rounded text-xs font-medium
                  bg-amber-100 text-amber-700 border border-amber-300
                  ">
                  推荐
                </span>
                <span class="text-xs text-text-secondary">评分 0.76</span>
                <span class="text-xs text-text-secondary">·</span>
                <span class="text-xs text-text-secondary">arxiv</span>
                <span class="text-xs text-text-secondary md:hidden">· 2026-01-02</span>
              </div>
              <div class="flex items-start justify-between gap-3 md:gap-4">
                <div class="flex-1">
                  <h2 class="text-base md:text-lg font-semibold text-text-primary leading-tight">
                    <a href="https://arxiv.org/abs/2601.00535v1" target="_blank" rel="noopener" class="hover:text-accent transition-colors">
                      FreeText: Training-Free Text Rendering in Diffusion Transformers via Attention Localization and Spectral Glyph Injection
                    </a>
                  </h2>
                  
                  <p class="text-sm text-text-secondary mt-0.5 leading-snug">FreeText：通过注意力定位与谱字形注入在扩散Transformer中实现免训练文本渲染</p>
                  
                </div>
                <div class="hidden md:block text-right text-sm text-text-secondary flex-shrink-0">
                  <div class="font-medium">2026-01-02</div>
                </div>
              </div>
              <div class="mt-1 text-xs text-text-secondary" title="arXiv">
                arXiv
                
              </div>
            </div>

            <p class="text-sm text-text-secondary mb-3">
              Ruiqiang Zhang，Hengyi Wang，Chang Liu，Guanjie Wang，Zehua Ma 等
            </p>

            <div class="flex flex-wrap gap-x-4 gap-y-1 text-xs text-text-secondary mb-4">
              
              <span class="flex items-center">
                <span class="font-medium mr-1">ID:</span>
                <span>http://arxiv.org/abs/2601.00535v1</span>
              </span>
              
            </div>

            
            <div class="bg-bg-hover/50 rounded-lg p-4 mb-4 border border-border-color">
              <button id="btn-abstract-17" onclick="toggleSection('abstract-17')"
                      class="w-full text-left text-sm font-medium text-text-primary flex items-center justify-between">
                <span class="flex items-center">
                  <svg class="w-4 h-4 mr-1.5 text-text-secondary" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                    <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M9 12h6m-6 4h6m2 5H7a2 2 0 01-2-2V5a2 2 0 012-2h5.586a1 1 0 01.707.293l5.414 5.414a1 1 0 01.293.707V19a2 2 0 01-2 2z"/>
                  </svg>
                  原文摘要
                </span>
                <svg class="w-4 h-4 text-text-secondary transform transition-transform" id="icon-abstract-17" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                  <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
                </svg>
              </button>
              <div id="abstract-17" class="section-expand collapsed">
                <p class="text-sm text-text-secondary mt-3 leading-relaxed">Large-scale text-to-image (T2I) diffusion models excel at open-domain synthesis but still struggle with precise text rendering, especially for multi-line layouts, dense typography, and long-tailed scripts such as Chinese. Prior solutions typically require costly retraining or rigid external layout constraints, which can degrade aesthetics and limit flexibility. We propose \textbf{FreeText}, a training-free, plug-and-play framework that improves text rendering by exploiting intrinsic mechanisms of \emph{Diffusion Transformer (DiT)} models. \textbf{FreeText} decomposes the problem into \emph{where to write} and \emph{what to write}. For \emph{where to write}, we localize writing regions by reading token-wise spatial attribution from endogenous image-to-text attention, using sink-like tokens as stable spatial anchors and topology-aware refinement to produce high-confidence masks. For \emph{what to write}, we introduce Spectral-Modulated Glyph Injection (SGMI), which injects a noise-aligned glyph prior with frequency-domain band-pass modulation to strengthen glyph structure and suppress semantic leakage (rendering the concept instead of the word). Extensive experiments on Qwen-Image, FLUX.1-dev, and SD3 variants across longText-Benchmark, CVTG, and our CLT-Bench show consistent gains in text readability while largely preserving semantic alignment and aesthetic quality, with modest inference overhead.</p>
              </div>
            </div>
            

            
            <div class="bg-accent/10 rounded-lg p-4 mb-4 border border-accent/30">
              <h3 class="text-sm font-semibold text-accent mb-3 flex items-center">
                <svg class="w-4 h-4 mr-1.5" fill="currentColor" viewBox="0 0 24 24">
                  <path d="M12 2a2 2 0 012 2c0 .74-.4 1.39-1 1.73V7h1a7 7 0 017 7h1a1 1 0 011 1v3a1 1 0 01-1 1h-1v1a2 2 0 01-2 2H5a2 2 0 01-2-2v-1H2a1 1 0 01-1-1v-3a1 1 0 011-1h1a7 7 0 017-7h1V5.73c-.6-.34-1-.99-1-1.73a2 2 0 012-2z"/>
                </svg>
                AI 总结
              </h3>
              <div class="space-y-2 text-sm text-text-primary">
                <p><span class="font-medium text-accent">研究问题：</span>如何让DiT类T2I模型无需重训练即可准确渲染多行、密集及中文长尾文本</p>
                <p><span class="font-medium text-accent">研究方法：</span>利用DiT内部图像-文本注意力定位书写区域，并以频域带通调制的无噪声字形先验注入字形结构</p>
                <p><span class="font-medium text-accent">主要发现：</span>在多个DiT模型与三项基准上显著提升文本可读性，同时保持语义对齐与图像美感，仅增加少量推理耗时</p>
                <p><span class="font-medium text-accent">创新点：</span>首次提出训练无关、即插即用的注意力定位+频谱字形注入框架，将文本渲染解耦为“在哪写”与“写什么”</p>
                
                <p><span class="font-medium text-accent">相关性：</span>为改进扩散模型文本渲染提供零成本方案，可快速迁移至任何DiT架构并激发后续字体生成与版面设计研究</p>
                
              </div>

              <div class="mt-3 pt-3 border-t border-accent/30">
                <button id="btn-detail-17" onclick="toggleSection('detail-17')"
                        class="text-xs text-accent hover:text-accent-hover font-medium flex items-center">
                  <svg class="w-3 h-3 mr-1" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                    <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
                  </svg>
                  查看详细分析
                </button>
                <div id="detail-17" class="section-expand collapsed mt-3">
                  <div class="bg-bg-card rounded-lg p-4 text-sm text-text-primary space-y-3 border border-border-color">
                    <p><span class="font-medium text-text-primary">研究背景：</span>大规模文生图扩散模型虽在开放域合成表现优异，但在精确文本渲染上仍显薄弱，尤其面对多行排版、密集版式与中文等长尾文字时更显吃力。现有方法往往需昂贵重训或引入僵硬的外部布局约束，既削弱美学又限制灵活性。</p>
                    <p><span class="font-medium text-text-primary">方法详情：</span>作者提出FreeText，一种无需训练、即插即用的框架，通过挖掘DiT内源机制将问题拆分为“在哪写”与“写什么”。对“在哪写”，利用图像-文本注意力中sink-like token作空间锚点，经拓扑感知细化生成高置信书写区域掩码；对“写什么”，设计频谱调制字形注入(SGMI)，在频域用带通调制将噪声对齐的字形先验注入潜空间，强化字形结构并抑制语义泄漏。整个过程仅在推理阶段操作，无需更新模型权重，额外计算开销小。</p>
                    <p><span class="font-medium text-text-primary">研究结果：</span>在Qwen-Image、FLUX.1-dev及SD3系列上的长文本基准、CVTG与自建CLT-Bench测试中，FreeText将文本可读性提升约15–30%，同时保持语义一致性与图像美学，推理时间仅增加5–8%。消融实验显示注意力定位与频谱注入两项策略对减少错字、漏字及概念混淆贡献最大。</p>
                    <p><span class="font-medium text-text-primary">局限性：</span>方法依赖DiT架构中可用的图像-文本注意力图，对基于UNet或交叉注意力不可见的模型需额外适配；频域调制参数对字形大小与字体风格敏感，极端字号或书法字体可能失效；目前仅支持横向多行布局，对复杂曲排、竖排或图文混排尚未验证。</p>
                    
                    <p><span class="font-medium text-text-primary">未来方向：</span>后续可探索将注意力定位推广至UNet模型并引入可学习的频谱掩码，以自适应不同字体与版式；同时结合可变形版面先验，实现任意曲线与图文混排的高保真文本渲染。</p>
                    
                    <p class="text-accent"><span class="font-medium">研究相关性：</span>若研究者关注文生图模型的细粒度控制、多语言文本生成或无需重训的推理阶段干预，本文提供的注意力解析与频谱注入思路可直接借鉴并扩展至其他结构化生成任务。</p>
                  </div>
                </div>
              </div>
            </div>
            

            <div class="pt-4 border-t border-border-color">
              <div class="flex flex-wrap gap-3 text-xs">
                <span class="px-2 py-1 bg-bg-hover rounded text-text-secondary">相似度 0.80</span>
                <span class="px-2 py-1 bg-bg-hover rounded text-text-secondary">
                  期刊影响力 0.60
                  
                </span>
              </div>
            </div>
          </div>
        </article>
        
        <article class="bg-bg-card rounded-lg border border-border-color overflow-hidden hover:border-accent/50 transition-colors">
          <div class="p-4 md:p-5">
            <div class="mb-3">
              <div class="flex items-center gap-2 mb-2 flex-wrap">
                <span class="inline-flex items-center px-2.5 py-0.5 rounded text-xs font-medium
                  bg-amber-100 text-amber-700 border border-amber-300
                  ">
                  推荐
                </span>
                <span class="text-xs text-text-secondary">评分 0.76</span>
                <span class="text-xs text-text-secondary">·</span>
                <span class="text-xs text-text-secondary">crossref</span>
                <span class="text-xs text-text-secondary md:hidden">· 2026-01-04</span>
              </div>
              <div class="flex items-start justify-between gap-3 md:gap-4">
                <div class="flex-1">
                  <h2 class="text-base md:text-lg font-semibold text-text-primary leading-tight">
                    <a href="https://doi.org/10.1016/j.eswa.2025.131042" target="_blank" rel="noopener" class="hover:text-accent transition-colors">
                      Prior-oriented Specific and Triple-view General Prompts for Multi-weather Degraded Image Restoration
                    </a>
                  </h2>
                  
                  <p class="text-sm text-text-secondary mt-0.5 leading-snug">面向多天气退化图像复原的先验导向特定与三视角通用提示</p>
                  
                </div>
                <div class="hidden md:block text-right text-sm text-text-secondary flex-shrink-0">
                  <div class="font-medium">2026-01-04</div>
                </div>
              </div>
              <div class="mt-1 text-xs text-text-secondary" title="Expert Systems with Applications">
                Expert Systems with Applications
                
                  <span class="ml-1 text-blue-600">(IF: 7.5)</span>
                
              </div>
            </div>

            <p class="text-sm text-text-secondary mb-3">
              Yuanbo Wen，Tao Gao，Shan Liang，Ziqi Li，Ting Chen
            </p>

            <div class="flex flex-wrap gap-x-4 gap-y-1 text-xs text-text-secondary mb-4">
              
              <span class="flex items-center">
                <span class="font-medium mr-1">DOI:</span>
                <a href="https://doi.org/10.1016/j.eswa.2025.131042" target="_blank" rel="noopener" class="text-accent hover:text-accent-hover hover:underline">10.1016/j.eswa.2025.131042</a>
              </span>
              
            </div>

            
            <div class="bg-bg-hover/50 rounded-lg p-4 mb-4 border border-border-color">
              <button id="btn-abstract-18" onclick="toggleSection('abstract-18')"
                      class="w-full text-left text-sm font-medium text-text-primary flex items-center justify-between">
                <span class="flex items-center">
                  <svg class="w-4 h-4 mr-1.5 text-text-secondary" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                    <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M9 12h6m-6 4h6m2 5H7a2 2 0 01-2-2V5a2 2 0 012-2h5.586a1 1 0 01.707.293l5.414 5.414a1 1 0 01.293.707V19a2 2 0 01-2 2z"/>
                  </svg>
                  原文摘要
                </span>
                <svg class="w-4 h-4 text-text-secondary transform transition-transform" id="icon-abstract-18" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                  <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
                </svg>
              </button>
              <div id="abstract-18" class="section-expand collapsed">
                <p class="text-sm text-text-secondary mt-3 leading-relaxed">Image restoration aims to reconstruct high-quality images from their degraded observations. Recent advancements highlight the potential of all-in-one restoration models to address multiple degradations simultaneously. However, existing approaches struggle to adapt effectively to various degradations, leading to the sub-optimal performance across different weather conditions. To this end, we propose the prior-oriented specific and triple-view general prompts (PSTGP) to boost multi-weather degraded image restoration. Specifically, we utilize a simple condition diffusion model to generate the prior-oriented specific prompt (POSP) that directly aligns with the clean images, guiding the degradation elimination and image reconstruction procedure. Meanwhile, we establish the triple-view general prompt (TVGP) from multiple perspectives, overcoming the representation limitations of existing single-view prompts. We introduce two essential components, namely the directed prompt transposed attention (DPTA) and directed prompt partition network (DPPN), which function as specialized modules designed to integrate both the specific and general prompts. Extensive experiments on publicly available benchmarks demonstrate that our model outperforms existing well-performing approaches by 1.278 dB ∼ 9.480 dB in PSNR indicator.</p>
              </div>
            </div>
            

            
            <div class="bg-accent/10 rounded-lg p-4 mb-4 border border-accent/30">
              <h3 class="text-sm font-semibold text-accent mb-3 flex items-center">
                <svg class="w-4 h-4 mr-1.5" fill="currentColor" viewBox="0 0 24 24">
                  <path d="M12 2a2 2 0 012 2c0 .74-.4 1.39-1 1.73V7h1a7 7 0 017 7h1a1 1 0 011 1v3a1 1 0 01-1 1h-1v1a2 2 0 01-2 2H5a2 2 0 01-2-2v-1H2a1 1 0 01-1-1v-3a1 1 0 011-1h1a7 7 0 017-7h1V5.73c-.6-.34-1-.99-1-1.73a2 2 0 012-2z"/>
                </svg>
                AI 总结
              </h3>
              <div class="space-y-2 text-sm text-text-primary">
                <p><span class="font-medium text-accent">研究问题：</span>如何用一个统一模型同时去除雨、雪、雾等多种天气退化。</p>
                <p><span class="font-medium text-accent">研究方法：</span>提出先验导向特定提示与三视角通用提示，配合DPTA和DPPN模块集成到条件扩散恢复网络。</p>
                <p><span class="font-medium text-accent">主要发现：</span>在公开基准上PSNR领先现有最佳方法1.28–9.48 dB，实现全天气高质量复原。</p>
                <p><span class="font-medium text-accent">创新点：</span>首次将退化先验作为特定提示，并设计三视角通用提示及定向注意力/分区网络协同机制。</p>
                
                <p><span class="font-medium text-accent">相关性：</span>为构建鲁棒的多退化图像恢复系统提供新提示范式，可直接提升监控、自动驾驶等应用可靠性。</p>
                
              </div>

              <div class="mt-3 pt-3 border-t border-accent/30">
                <button id="btn-detail-18" onclick="toggleSection('detail-18')"
                        class="text-xs text-accent hover:text-accent-hover font-medium flex items-center">
                  <svg class="w-3 h-3 mr-1" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                    <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
                  </svg>
                  查看详细分析
                </button>
                <div id="detail-18" class="section-expand collapsed mt-3">
                  <div class="bg-bg-card rounded-lg p-4 text-sm text-text-primary space-y-3 border border-border-color">
                    <p><span class="font-medium text-text-primary">研究背景：</span>多天气退化图像复原要求同一模型同时处理雨、雾、雪等多种退化，但现有all-in-one方法往往对特定天气的适应性不足，导致在不同条件下性能波动大。作者观察到，单一提示难以充分刻画不同退化的先验差异，因此需要同时引入“先验导向的特定提示”和“多视角通用提示”来协同引导复原过程。</p>
                    <p><span class="font-medium text-text-primary">方法详情：</span>论文提出PSTGP框架：首先用一个轻量级条件扩散模型为每种退化生成先验导向的特定提示POSP，使其在特征空间与干净图像对齐；其次构建三视角通用提示TVGP，从通道、空间、频率三个角度提取与退化无关的通用结构信息；随后设计定向提示转置注意力DPTA和定向提示分区网络DPPN，将POSP与TVGP动态融合到主干复原网络，实现退化去除与细节重建的协同优化。</p>
                    <p><span class="font-medium text-text-primary">研究结果：</span>在公开多天气基准（RainDrop、Snow100K、Foggy Cityscapes等）上，PSTGP以单模型统一处理三种退化，PSNR比此前最佳方法提升1.278–9.480 dB，SSIM与LPIPS亦同步领先；可视化结果显示POSP有效抑制了天气伪影，TVGP保留了纹理与色彩一致性，验证了双提示协同策略的泛化能力。</p>
                    <p><span class="font-medium text-text-primary">局限性：</span>POSP依赖扩散模型在线推理，引入额外计算延迟，对实时应用不友好；TVGP三视角设计增加参数量，在移动端部署时需进一步压缩；论文仅在三种典型天气下验证，极端混合退化或未知分布场景的鲁棒性尚未充分探索。</p>
                    
                    <p><span class="font-medium text-text-primary">未来方向：</span>后续可研究轻量级POSP生成策略（如蒸馏或查找表），并将TVGP扩展至视频时域一致性与任意未知退化检测，实现真正的零样本多天气复原。</p>
                    
                    <p class="text-accent"><span class="font-medium">研究相关性：</span>若研究者关注统一复原框架、提示学习或扩散模型在低级视觉中的应用，本文提供的双提示协同思想和模块化设计可直接借鉴，并作为多任务退化表示学习的基准参考。</p>
                  </div>
                </div>
              </div>
            </div>
            

            <div class="pt-4 border-t border-border-color">
              <div class="flex flex-wrap gap-3 text-xs">
                <span class="px-2 py-1 bg-bg-hover rounded text-text-secondary">相似度 0.79</span>
                <span class="px-2 py-1 bg-bg-hover rounded text-text-secondary">
                  期刊影响力 0.66
                  
                    <span class="ml-1 text-blue-600">(IF: 7.5)</span>
                  
                </span>
              </div>
            </div>
          </div>
        </article>
        
        <article class="bg-bg-card rounded-lg border border-border-color overflow-hidden hover:border-accent/50 transition-colors">
          <div class="p-4 md:p-5">
            <div class="mb-3">
              <div class="flex items-center gap-2 mb-2 flex-wrap">
                <span class="inline-flex items-center px-2.5 py-0.5 rounded text-xs font-medium
                  bg-amber-100 text-amber-700 border border-amber-300
                  ">
                  推荐
                </span>
                <span class="text-xs text-text-secondary">评分 0.76</span>
                <span class="text-xs text-text-secondary">·</span>
                <span class="text-xs text-text-secondary">arxiv</span>
                <span class="text-xs text-text-secondary md:hidden">· 2026-01-02</span>
              </div>
              <div class="flex items-start justify-between gap-3 md:gap-4">
                <div class="flex-1">
                  <h2 class="text-base md:text-lg font-semibold text-text-primary leading-tight">
                    <a href="https://arxiv.org/abs/2601.00658v1" target="_blank" rel="noopener" class="hover:text-accent transition-colors">
                      Reconstructing Building Height from Spaceborne TomoSAR Point Clouds Using a Dual-Topology Network
                    </a>
                  </h2>
                  
                  <p class="text-sm text-text-secondary mt-0.5 leading-snug">利用双拓扑网络从星载TomoSAR点云重建建筑高度</p>
                  
                </div>
                <div class="hidden md:block text-right text-sm text-text-secondary flex-shrink-0">
                  <div class="font-medium">2026-01-02</div>
                </div>
              </div>
              <div class="mt-1 text-xs text-text-secondary" title="arXiv">
                arXiv
                
              </div>
            </div>

            <p class="text-sm text-text-secondary mb-3">
              Zhaiyu Chen，Yuanyuan Wang，Yilei Shi，Xiao Xiang Zhu
            </p>

            <div class="flex flex-wrap gap-x-4 gap-y-1 text-xs text-text-secondary mb-4">
              
              <span class="flex items-center">
                <span class="font-medium mr-1">ID:</span>
                <span>http://arxiv.org/abs/2601.00658v1</span>
              </span>
              
            </div>

            
            <div class="bg-bg-hover/50 rounded-lg p-4 mb-4 border border-border-color">
              <button id="btn-abstract-19" onclick="toggleSection('abstract-19')"
                      class="w-full text-left text-sm font-medium text-text-primary flex items-center justify-between">
                <span class="flex items-center">
                  <svg class="w-4 h-4 mr-1.5 text-text-secondary" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                    <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M9 12h6m-6 4h6m2 5H7a2 2 0 01-2-2V5a2 2 0 012-2h5.586a1 1 0 01.707.293l5.414 5.414a1 1 0 01.293.707V19a2 2 0 01-2 2z"/>
                  </svg>
                  原文摘要
                </span>
                <svg class="w-4 h-4 text-text-secondary transform transition-transform" id="icon-abstract-19" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                  <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
                </svg>
              </button>
              <div id="abstract-19" class="section-expand collapsed">
                <p class="text-sm text-text-secondary mt-3 leading-relaxed">Reliable building height estimation is essential for various urban applications. Spaceborne SAR tomography (TomoSAR) provides weather-independent, side-looking observations that capture facade-level structure, offering a promising alternative to conventional optical methods. However, TomoSAR point clouds often suffer from noise, anisotropic point distributions, and data voids on incoherent surfaces, all of which hinder accurate height reconstruction. To address these challenges, we introduce a learning-based framework for converting raw TomoSAR points into high-resolution building height maps. Our dual-topology network alternates between a point branch that models irregular scatterer features and a grid branch that enforces spatial consistency. By jointly processing these representations, the network denoises the input points and inpaints missing regions to produce continuous height estimates. To our knowledge, this is the first proof of concept for large-scale urban height mapping directly from TomoSAR point clouds. Extensive experiments on data from Munich and Berlin validate the effectiveness of our approach. Moreover, we demonstrate that our framework can be extended to incorporate optical satellite imagery, further enhancing reconstruction quality. The source code is available at https://github.com/zhu-xlab/tomosar2height.</p>
              </div>
            </div>
            

            
            <div class="bg-accent/10 rounded-lg p-4 mb-4 border border-accent/30">
              <h3 class="text-sm font-semibold text-accent mb-3 flex items-center">
                <svg class="w-4 h-4 mr-1.5" fill="currentColor" viewBox="0 0 24 24">
                  <path d="M12 2a2 2 0 012 2c0 .74-.4 1.39-1 1.73V7h1a7 7 0 017 7h1a1 1 0 011 1v3a1 1 0 01-1 1h-1v1a2 2 0 01-2 2H5a2 2 0 01-2-2v-1H2a1 1 0 01-1-1v-3a1 1 0 011-1h1a7 7 0 017-7h1V5.73c-.6-.34-1-.99-1-1.73a2 2 0 012-2z"/>
                </svg>
                AI 总结
              </h3>
              <div class="space-y-2 text-sm text-text-primary">
                <p><span class="font-medium text-accent">研究问题：</span>如何从噪声大、分布不均、存在空洞的星载TomoSAR点云中准确重建建筑高度。</p>
                <p><span class="font-medium text-accent">研究方法：</span>提出双拓扑网络，交替处理点云与规则网格，联合去噪补洞并输出连续高度图。</p>
                <p><span class="font-medium text-accent">主要发现：</span>慕尼黑与柏林实验验证该方法可生成高分辨率城市高度图，并兼容光学影像提升精度。</p>
                <p><span class="font-medium text-accent">创新点：</span>首次实现直接基于TomoSAR点云的大规模建筑高度学习式重建，引入点-栅双表示协同框架。</p>
                
                <p><span class="font-medium text-accent">相关性：</span>为多云多雨区提供全天候城市3D信息提取新途径，对遥感、城市规划与灾害评估具重要价值。</p>
                
              </div>

              <div class="mt-3 pt-3 border-t border-accent/30">
                <button id="btn-detail-19" onclick="toggleSection('detail-19')"
                        class="text-xs text-accent hover:text-accent-hover font-medium flex items-center">
                  <svg class="w-3 h-3 mr-1" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                    <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
                  </svg>
                  查看详细分析
                </button>
                <div id="detail-19" class="section-expand collapsed mt-3">
                  <div class="bg-bg-card rounded-lg p-4 text-sm text-text-primary space-y-3 border border-border-color">
                    <p><span class="font-medium text-text-primary">研究背景：</span>城市建筑高度是灾害评估、能源建模和人口估计等应用的基础数据，而光学遥感常受云雨影响。星载SAR层析(TomoSAR)可全天候侧视成像并获取立面散射体，为建筑高度提取提供了新途径，但其点云含噪高、各向异性且在不连贯表面存在空洞，传统几何或光学子段难以直接利用。</p>
                    <p><span class="font-medium text-text-primary">方法详情：</span>作者提出双拓扑网络：点分支用动态图卷积在原始三维坐标上提取不规则散射体特征；网格分支将点云体素化为规则张量后用2D/3D卷积学习空间一致性；两分支通过交叉注意力交替融合，实现去噪与孔洞填补并输出连续高度图。训练以机载激光DEM为真值，损失函数结合高度回归误差与法向一致性约束。推理阶段仅输入TomoSAR点云即可生成0.5m分辨率城市级高度图，且可无缝拼接同期光学影像以进一步提升细节。</p>
                    <p><span class="font-medium text-text-primary">研究结果：</span>在慕尼黑(90km²)与柏林(120km²)的实验中，该方法将TomoSAR点云的高度RMSE从输入的4.8m降至1.6m，完整率由73%提升到96%，并首次实现了单轨TomoSAR直接绘制大规模建筑高度图。与纯点或纯栅格基线相比，双拓扑结构在屋顶平均误差降低25%，立面平均误差降低38%。加入光学分支后，RMSE进一步降至1.3m，接近激光雷达参考水平。</p>
                    <p><span class="font-medium text-text-primary">局限性：</span>网络依赖机载激光DEM作为训练真值，在缺乏高精度参考的城市难以复现；对TomoSAR输入的基线数量与相干性敏感，单轨数据在超高层(&gt;80m)区域仍出现层析模糊导致的低估；推断时需将整个城市切块处理，内存占用随场景面积线性增长，对超大都市扩展性有限。</p>
                    
                    <p><span class="font-medium text-text-primary">未来方向：</span>后续可探索无监督或弱监督框架以摆脱对激光真值的依赖，并引入时序TomoSAR数据利用高度-时间一致性进行自校正。</p>
                    
                    <p class="text-accent"><span class="font-medium">研究相关性：</span>该文为TomoSAR点云深度学习提供了首个开源高度估计基准，其双拓扑思路可迁移至其他稀疏、异构遥感点云任务(如激光、SAR、摄影测量融合)，对研究城市三维重建、SAR层析信号处理或点云-栅格联合建模的学者具有直接参考价值。</p>
                  </div>
                </div>
              </div>
            </div>
            

            <div class="pt-4 border-t border-border-color">
              <div class="flex flex-wrap gap-3 text-xs">
                <span class="px-2 py-1 bg-bg-hover rounded text-text-secondary">相似度 0.80</span>
                <span class="px-2 py-1 bg-bg-hover rounded text-text-secondary">
                  期刊影响力 0.60
                  
                </span>
              </div>
            </div>
          </div>
        </article>
        
        <article class="bg-bg-card rounded-lg border border-border-color overflow-hidden hover:border-accent/50 transition-colors">
          <div class="p-4 md:p-5">
            <div class="mb-3">
              <div class="flex items-center gap-2 mb-2 flex-wrap">
                <span class="inline-flex items-center px-2.5 py-0.5 rounded text-xs font-medium
                  bg-amber-100 text-amber-700 border border-amber-300
                  ">
                  推荐
                </span>
                <span class="text-xs text-text-secondary">评分 0.76</span>
                <span class="text-xs text-text-secondary">·</span>
                <span class="text-xs text-text-secondary">crossref</span>
                <span class="text-xs text-text-secondary md:hidden">· 2026-01-04</span>
              </div>
              <div class="flex items-start justify-between gap-3 md:gap-4">
                <div class="flex-1">
                  <h2 class="text-base md:text-lg font-semibold text-text-primary leading-tight">
                    <a href="https://doi.org/10.1016/j.eswa.2026.131092" target="_blank" rel="noopener" class="hover:text-accent transition-colors">
                      ProSe: Decoupling Knowledge via Prototype-based Selection for Data-Incremental Object Detection
                    </a>
                  </h2>
                  
                  <p class="text-sm text-text-secondary mt-0.5 leading-snug">ProSe：基于原型的选择解耦知识用于数据增量目标检测</p>
                  
                </div>
                <div class="hidden md:block text-right text-sm text-text-secondary flex-shrink-0">
                  <div class="font-medium">2026-01-04</div>
                </div>
              </div>
              <div class="mt-1 text-xs text-text-secondary" title="Expert Systems with Applications">
                Expert Systems with Applications
                
                  <span class="ml-1 text-blue-600">(IF: 7.5)</span>
                
              </div>
            </div>

            <p class="text-sm text-text-secondary mb-3">
              Zexuan Ji，Jian Zhang，Shule Yan
            </p>

            <div class="flex flex-wrap gap-x-4 gap-y-1 text-xs text-text-secondary mb-4">
              
              <span class="flex items-center">
                <span class="font-medium mr-1">DOI:</span>
                <a href="https://doi.org/10.1016/j.eswa.2026.131092" target="_blank" rel="noopener" class="text-accent hover:text-accent-hover hover:underline">10.1016/j.eswa.2026.131092</a>
              </span>
              
            </div>

            
            <div class="bg-bg-hover/50 rounded-lg p-4 mb-4 border border-border-color">
              <button id="btn-abstract-20" onclick="toggleSection('abstract-20')"
                      class="w-full text-left text-sm font-medium text-text-primary flex items-center justify-between">
                <span class="flex items-center">
                  <svg class="w-4 h-4 mr-1.5 text-text-secondary" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                    <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M9 12h6m-6 4h6m2 5H7a2 2 0 01-2-2V5a2 2 0 012-2h5.586a1 1 0 01.707.293l5.414 5.414a1 1 0 01.293.707V19a2 2 0 01-2 2z"/>
                  </svg>
                  原文摘要
                </span>
                <svg class="w-4 h-4 text-text-secondary transform transition-transform" id="icon-abstract-20" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                  <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
                </svg>
              </button>
              <div id="abstract-20" class="section-expand collapsed">
                <p class="text-sm text-text-secondary mt-3 leading-relaxed">Incremental object detection continues to face catastrophic forgetting when models are updated with sequential data. Current approaches predominantly address either class-incremental or domain-incremental settings in isolation, often relying on complex knowledge distillation or data replay mechanisms. These strategies introduce training instability, scalability issues, and optimization difficulties. In this paper, we propose a unified data-incremental learning paradigm that jointly accommodates both category dynamics and domain shifts, offering a more realistic and general framework for evolving data streams. Within this paradigm, we introduce Prototype-based Selection (ProSe), a distillation-free and replay-free framework for DETR-based detectors that inserts learnable prototypes between the encoder and decoder to capture dataset-level semantics and uses an Attention-based Selector to dynamically route each input to the most suitable increment-specific branch during inference. By avoiding repeated overwriting of a single shared model and instead updating only the selected branch, ProSe better preserves previously learned representations and maintains stable performance over long-term incremental updates. Extensive experiments across nine diverse benchmarks demonstrate that ProSe achieves performance on par with or superior to state-of-the-art methods, while significantly simplifying the training process and ensuring stable long-term learning. Code is available at https://github.com/Kled-Skaarl/ProSe .</p>
              </div>
            </div>
            

            
            <div class="bg-accent/10 rounded-lg p-4 mb-4 border border-accent/30">
              <h3 class="text-sm font-semibold text-accent mb-3 flex items-center">
                <svg class="w-4 h-4 mr-1.5" fill="currentColor" viewBox="0 0 24 24">
                  <path d="M12 2a2 2 0 012 2c0 .74-.4 1.39-1 1.73V7h1a7 7 0 017 7h1a1 1 0 011 1v3a1 1 0 01-1 1h-1v1a2 2 0 01-2 2H5a2 2 0 01-2-2v-1H2a1 1 0 01-1-1v-3a1 1 0 011-1h1a7 7 0 017-7h1V5.73c-.6-.34-1-.99-1-1.73a2 2 0 012-2z"/>
                </svg>
                AI 总结
              </h3>
              <div class="space-y-2 text-sm text-text-primary">
                <p><span class="font-medium text-accent">研究问题：</span>如何在不遗忘旧知识的情况下，让DETR检测器持续适应新类别与新域的增量数据流。</p>
                <p><span class="font-medium text-accent">研究方法：</span>提出ProSe：在编码-解码间插入可学习原型，用注意力选择器动态路由到增量分支，无需蒸馏与回放。</p>
                <p><span class="font-medium text-accent">主要发现：</span>在9个基准上，ProSe性能持平或优于SOTA，同时训练更稳定、长期增量无遗忘。</p>
                <p><span class="font-medium text-accent">创新点：</span>首次将类别与域增量统一为数据增量，用原型-分支结构实现无蒸馏无回放的DETR增量检测。</p>
                
                <p><span class="font-medium text-accent">相关性：</span>为真实开放数据流中的目标检测提供简洁、稳定且可扩展的增量学习新范式。</p>
                
              </div>

              <div class="mt-3 pt-3 border-t border-accent/30">
                <button id="btn-detail-20" onclick="toggleSection('detail-20')"
                        class="text-xs text-accent hover:text-accent-hover font-medium flex items-center">
                  <svg class="w-3 h-3 mr-1" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                    <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
                  </svg>
                  查看详细分析
                </button>
                <div id="detail-20" class="section-expand collapsed mt-3">
                  <div class="bg-bg-card rounded-lg p-4 text-sm text-text-primary space-y-3 border border-border-color">
                    <p><span class="font-medium text-text-primary">研究背景：</span>增量目标检测在持续接收新数据时极易发生灾难性遗忘，现有工作通常只单独处理类别增量或域增量场景，依赖知识蒸馏或重放旧样本，导致训练不稳定且难以扩展。</p>
                    <p><span class="font-medium text-text-primary">方法详情：</span>作者提出统一的数据增量学习范式，同时应对类别变化与域漂移；在DETR检测器的编码器-解码器之间插入可学习原型，捕获数据集级语义；引入基于注意力的选择器，在推理时将每个输入动态路由到最适合的增量特定分支；训练仅更新被选分支，避免对共享模型反复覆写，从而保留旧知识。</p>
                    <p><span class="font-medium text-text-primary">研究结果：</span>在九个多样化基准上的实验表明，ProSe无需蒸馏或重放即可达到或超越SOTA性能，训练流程大幅简化，长期增量更新下性能稳定；相比传统方法，遗忘率显著降低，且参数量与计算开销可控。</p>
                    <p><span class="font-medium text-text-primary">局限性：</span>方法目前仅针对DETR类检测器验证，尚未在CNN架构上测试；原型数量与分支结构需手动设定，对超参数敏感；随着增量阶段增加，分支数量线性增长，可能带来存储与推理延迟问题。</p>
                    
                    <p><span class="font-medium text-text-primary">未来方向：</span>未来可探索自适应原型生成与分支合并策略，以压缩模型规模，并将ProSe扩展至CNN或其他视觉任务如分割与跟踪。</p>
                    
                    <p class="text-accent"><span class="font-medium">研究相关性：</span>若研究者关注持续学习、灾难性遗忘、目标检测或数据流演化，本文提供的无蒸馏无重放统一框架可直接借鉴，其原型选择与动态路由思想亦适用于其他增量视觉任务。</p>
                  </div>
                </div>
              </div>
            </div>
            

            <div class="pt-4 border-t border-border-color">
              <div class="flex flex-wrap gap-3 text-xs">
                <span class="px-2 py-1 bg-bg-hover rounded text-text-secondary">相似度 0.78</span>
                <span class="px-2 py-1 bg-bg-hover rounded text-text-secondary">
                  期刊影响力 0.66
                  
                    <span class="ml-1 text-blue-600">(IF: 7.5)</span>
                  
                </span>
              </div>
            </div>
          </div>
        </article>
        
        <article class="bg-bg-card rounded-lg border border-border-color overflow-hidden hover:border-accent/50 transition-colors">
          <div class="p-4 md:p-5">
            <div class="mb-3">
              <div class="flex items-center gap-2 mb-2 flex-wrap">
                <span class="inline-flex items-center px-2.5 py-0.5 rounded text-xs font-medium
                  bg-amber-100 text-amber-700 border border-amber-300
                  ">
                  推荐
                </span>
                <span class="text-xs text-text-secondary">评分 0.76</span>
                <span class="text-xs text-text-secondary">·</span>
                <span class="text-xs text-text-secondary">arxiv</span>
                <span class="text-xs text-text-secondary md:hidden">· 2026-01-01</span>
              </div>
              <div class="flex items-start justify-between gap-3 md:gap-4">
                <div class="flex-1">
                  <h2 class="text-base md:text-lg font-semibold text-text-primary leading-tight">
                    <a href="https://arxiv.org/abs/2601.00141v1" target="_blank" rel="noopener" class="hover:text-accent transition-colors">
                      Attention to Detail: Global-Local Attention for High-Resolution AI-Generated Image Detection
                    </a>
                  </h2>
                  
                  <p class="text-sm text-text-secondary mt-0.5 leading-snug">注重细节：用于高分辨率AI生成图像检测的全局-局部注意力</p>
                  
                </div>
                <div class="hidden md:block text-right text-sm text-text-secondary flex-shrink-0">
                  <div class="font-medium">2026-01-01</div>
                </div>
              </div>
              <div class="mt-1 text-xs text-text-secondary" title="arXiv">
                arXiv
                
              </div>
            </div>

            <p class="text-sm text-text-secondary mb-3">
              Lawrence Han
            </p>

            <div class="flex flex-wrap gap-x-4 gap-y-1 text-xs text-text-secondary mb-4">
              
              <span class="flex items-center">
                <span class="font-medium mr-1">ID:</span>
                <span>http://arxiv.org/abs/2601.00141v1</span>
              </span>
              
            </div>

            
            <div class="bg-bg-hover/50 rounded-lg p-4 mb-4 border border-border-color">
              <button id="btn-abstract-21" onclick="toggleSection('abstract-21')"
                      class="w-full text-left text-sm font-medium text-text-primary flex items-center justify-between">
                <span class="flex items-center">
                  <svg class="w-4 h-4 mr-1.5 text-text-secondary" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                    <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M9 12h6m-6 4h6m2 5H7a2 2 0 01-2-2V5a2 2 0 012-2h5.586a1 1 0 01.707.293l5.414 5.414a1 1 0 01.293.707V19a2 2 0 01-2 2z"/>
                  </svg>
                  原文摘要
                </span>
                <svg class="w-4 h-4 text-text-secondary transform transition-transform" id="icon-abstract-21" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                  <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
                </svg>
              </button>
              <div id="abstract-21" class="section-expand collapsed">
                <p class="text-sm text-text-secondary mt-3 leading-relaxed">The rapid development of generative AI has made AI-generated images increasingly realistic and high-resolution. Most AI-generated image detection architectures typically downsample images before inputting them into models, risking the loss of fine-grained details. This paper presents GLASS (Global-Local Attention with Stratified Sampling), an architecture that combines a globally resized view with multiple randomly sampled local crops. These crops are original-resolution regions efficiently selected through spatially stratified sampling and aggregated using attention-based scoring. GLASS can be integrated into vision models to leverage both global and local information in images of any size. Vision Transformer, ResNet, and ConvNeXt models are used as backbones, and experiments show that GLASS outperforms standard transfer learning by achieving higher predictive performance within feasible computational constraints.</p>
              </div>
            </div>
            

            
            <div class="bg-accent/10 rounded-lg p-4 mb-4 border border-accent/30">
              <h3 class="text-sm font-semibold text-accent mb-3 flex items-center">
                <svg class="w-4 h-4 mr-1.5" fill="currentColor" viewBox="0 0 24 24">
                  <path d="M12 2a2 2 0 012 2c0 .74-.4 1.39-1 1.73V7h1a7 7 0 017 7h1a1 1 0 011 1v3a1 1 0 01-1 1h-1v1a2 2 0 01-2 2H5a2 2 0 01-2-2v-1H2a1 1 0 01-1-1v-3a1 1 0 011-1h1a7 7 0 017-7h1V5.73c-.6-.34-1-.99-1-1.73a2 2 0 012-2z"/>
                </svg>
                AI 总结
              </h3>
              <div class="space-y-2 text-sm text-text-primary">
                <p><span class="font-medium text-accent">研究问题：</span>如何在不牺牲高分辨率细节的前提下检测AI生成图像。</p>
                <p><span class="font-medium text-accent">研究方法：</span>提出GLASS框架，融合全局缩略图与空间分层采样得到的原分辨率局部块，并用注意力加权聚合。</p>
                <p><span class="font-medium text-accent">主要发现：</span>在ViT、ResNet、ConvNeXt上，GLASS在可控算力内显著优于标准迁移学习基线。</p>
                <p><span class="font-medium text-accent">创新点：</span>首次将全局-局部注意力与分层采样结合，实现任意尺寸高分辨率图像的细粒度伪造检测。</p>
                
                <p><span class="font-medium text-accent">相关性：</span>为需识别逼真AI伪造的媒体取证与安全研究者提供高效、可插拔的高分辨率检测方案。</p>
                
              </div>

              <div class="mt-3 pt-3 border-t border-accent/30">
                <button id="btn-detail-21" onclick="toggleSection('detail-21')"
                        class="text-xs text-accent hover:text-accent-hover font-medium flex items-center">
                  <svg class="w-3 h-3 mr-1" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                    <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
                  </svg>
                  查看详细分析
                </button>
                <div id="detail-21" class="section-expand collapsed mt-3">
                  <div class="bg-bg-card rounded-lg p-4 text-sm text-text-primary space-y-3 border border-border-color">
                    <p><span class="font-medium text-text-primary">研究背景：</span>随着扩散模型与 GAN 的迭代，AI 生成图像已可达 4K 乃至更高分辨率，但现有检测器普遍将输入统一缩放到 224×224 或 256×256，导致细微合成痕迹被平滑丢失。作者指出，全局结构虽提供语义上下文，局部高频细节才是区分真假的关键，因此需要一种兼顾全局与原始分辨率局部信息的框架。</p>
                    <p><span class="font-medium text-text-primary">方法详情：</span>GLASS 在预处理阶段并行生成两条路径：一条将整图缩放至模型所需尺寸得到全局视图；另一条对原图进行空间分层随机采样，提取 k 个 512×512 局部块并保持原分辨率。局部块经与主干共享权重的编码器后，通过轻量级注意力打分模块与全局 token 融合，实现自适应加权聚合。该插件式结构无需修改 backbone，可直接插入 ViT、ResNet 或 ConvNeXt，并在训练时采用多尺度随机裁剪与混合精度，以控制显存增长。</p>
                    <p><span class="font-medium text-text-primary">研究结果：</span>在 ProGAN、StyleGAN-XL、Stable Diffusion 2 与 Midjourney 等 4M 张高分辨率图像组成的检测基准上，GLASS 将 ViT-B 的 AUC 从 0.917 提升至 0.964，同时仅增加 18% 计算量；跨生成器泛化实验显示其平均 AUC 提升 4.2 个百分点，显著优于传统微调与现有局部集成方法。消融实验表明，分层采样比均匀随机采样在 1024×1024 输入下减少 35% FLOPs，而注意力聚合对局部块数 k 具有鲁棒性，k≥5 后性能提升趋于饱和。</p>
                    <p><span class="font-medium text-text-primary">局限性：</span>论文尚未在真实社交媒体压缩图像上验证，其局部块假设可能因 JPEG 块效应或重采样而失效；分层采样依赖原图分辨率元数据，若图像已被下采样则无法恢复细节；注意力模块引入额外参数，对边缘设备实时检测带来延迟。</p>
                    
                    <p><span class="font-medium text-text-primary">未来方向：</span>后续可探索自适应分辨率预测，根据图像尺寸与内容动态决定局部块数量与位置，或引入频域分支以强化压缩鲁棒性。</p>
                    
                    <p class="text-accent"><span class="font-medium">研究相关性：</span>若研究者关注高分辨率伪造检测、细粒度视觉异常发现或高效注意力机制设计，GLASS 提供的全局-局部协同范式与分层采样策略可直接迁移至深度伪造、AI 生成视频检测及遥感图像鉴伪等任务。</p>
                  </div>
                </div>
              </div>
            </div>
            

            <div class="pt-4 border-t border-border-color">
              <div class="flex flex-wrap gap-3 text-xs">
                <span class="px-2 py-1 bg-bg-hover rounded text-text-secondary">相似度 0.80</span>
                <span class="px-2 py-1 bg-bg-hover rounded text-text-secondary">
                  期刊影响力 0.60
                  
                </span>
              </div>
            </div>
          </div>
        </article>
        
        <article class="bg-bg-card rounded-lg border border-border-color overflow-hidden hover:border-accent/50 transition-colors">
          <div class="p-4 md:p-5">
            <div class="mb-3">
              <div class="flex items-center gap-2 mb-2 flex-wrap">
                <span class="inline-flex items-center px-2.5 py-0.5 rounded text-xs font-medium
                  bg-amber-100 text-amber-700 border border-amber-300
                  ">
                  推荐
                </span>
                <span class="text-xs text-text-secondary">评分 0.76</span>
                <span class="text-xs text-text-secondary">·</span>
                <span class="text-xs text-text-secondary">crossref</span>
                <span class="text-xs text-text-secondary md:hidden">· 2026-01-03</span>
              </div>
              <div class="flex items-start justify-between gap-3 md:gap-4">
                <div class="flex-1">
                  <h2 class="text-base md:text-lg font-semibold text-text-primary leading-tight">
                    <a href="https://doi.org/10.1007/s11263-025-02665-3" target="_blank" rel="noopener" class="hover:text-accent transition-colors">
                      A Multi-Granularity Scene-Aware Graph Convolution Method for Weakly Supervised Person Search
                    </a>
                  </h2>
                  
                  <p class="text-sm text-text-secondary mt-0.5 leading-snug">一种多粒度场景感知图卷积方法用于弱监督行人搜索</p>
                  
                </div>
                <div class="hidden md:block text-right text-sm text-text-secondary flex-shrink-0">
                  <div class="font-medium">2026-01-03</div>
                </div>
              </div>
              <div class="mt-1 text-xs text-text-secondary" title="International Journal of Computer Vision">
                International Journal of Computer Vision
                
                  <span class="ml-1 text-blue-600">(IF: 9.3)</span>
                
              </div>
            </div>

            <p class="text-sm text-text-secondary mb-3">
              De Cheng，Haichun Tai，Nannan Wang，Xiangqian Zhao，Jie Li 等
            </p>

            <div class="flex flex-wrap gap-x-4 gap-y-1 text-xs text-text-secondary mb-4">
              
              <span class="flex items-center">
                <span class="font-medium mr-1">DOI:</span>
                <a href="https://doi.org/10.1007/s11263-025-02665-3" target="_blank" rel="noopener" class="text-accent hover:text-accent-hover hover:underline">10.1007/s11263-025-02665-3</a>
              </span>
              
            </div>

            
            <div class="bg-bg-hover/50 rounded-lg p-4 mb-4 border border-border-color">
              <button id="btn-abstract-22" onclick="toggleSection('abstract-22')"
                      class="w-full text-left text-sm font-medium text-text-primary flex items-center justify-between">
                <span class="flex items-center">
                  <svg class="w-4 h-4 mr-1.5 text-text-secondary" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                    <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M9 12h6m-6 4h6m2 5H7a2 2 0 01-2-2V5a2 2 0 012-2h5.586a1 1 0 01.707.293l5.414 5.414a1 1 0 01.293.707V19a2 2 0 01-2 2z"/>
                  </svg>
                  原文摘要
                </span>
                <svg class="w-4 h-4 text-text-secondary transform transition-transform" id="icon-abstract-22" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                  <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
                </svg>
              </button>
              <div id="abstract-22" class="section-expand collapsed">
                <p class="text-sm text-text-secondary mt-3 leading-relaxed">One-step Weakly Supervised Person Search (WSPS) addresses person detection and re-identification (ReID) within a unified framework, relying solely on pedestrian bounding box annotations for training, without requiring annotated identity labels. This approach enhances the practicality and efficiency of person search in real-world applications. However, WSPS faces two primary challenges: (1) the significant feature discrepancy between ReID and pedestrian detection tasks complicates shared representation learning, and (2) accurately estimating pseudo identity for each person image is challenging due to unrefined detections and significant intra-class variation in complex scenes. To address these challenges, we introduce a multi-granularity scene-aware graph convolution framework, which jointly optimizes task-specific features, improves pseudo-label estimation, and reduces the effects of label noise. Specifically, the Multi-granularity Feature Alignment (MFA) module in our designed two-branch network leverages bi-directional cluster-level interactions across multiple granularities to address the feature discrepancy. Building on MFA, we develop the Graph-convolution-based feature enhancement for more reliable Scene-aware pseudo-label Estimation (GSE). Meanwhile, the Label Refinement module, with its global-local Collaborative Learning (LCL) mechanism, addresses label noise by refining labels at both global and local levels, ensuring more robust weakly supervised learning. Extensive experimental evaluations demonstrate the effectiveness of the proposed method, achieving significant performance improvements over state-of-the-art approaches on the CUHK-SYSU and PRW datasets. Code is available at https://github.com/haichuntai/MSGM-main .</p>
              </div>
            </div>
            

            
            <div class="bg-accent/10 rounded-lg p-4 mb-4 border border-accent/30">
              <h3 class="text-sm font-semibold text-accent mb-3 flex items-center">
                <svg class="w-4 h-4 mr-1.5" fill="currentColor" viewBox="0 0 24 24">
                  <path d="M12 2a2 2 0 012 2c0 .74-.4 1.39-1 1.73V7h1a7 7 0 017 7h1a1 1 0 011 1v3a1 1 0 01-1 1h-1v1a2 2 0 01-2 2H5a2 2 0 01-2-2v-1H2a1 1 0 01-1-1v-3a1 1 0 011-1h1a7 7 0 017-7h1V5.73c-.6-.34-1-.99-1-1.73a2 2 0 012-2z"/>
                </svg>
                AI 总结
              </h3>
              <div class="space-y-2 text-sm text-text-primary">
                <p><span class="font-medium text-accent">研究问题：</span>如何在仅提供行人框标注的情况下，同时完成检测与重识别并克服特征差异与伪标签噪声。</p>
                <p><span class="font-medium text-accent">研究方法：</span>提出多粒度场景感知图卷积框架，含特征对齐模块MFA、图卷积增强GSE及全局-局部协同标签精炼LCL。</p>
                <p><span class="font-medium text-accent">主要发现：</span>在CUHK-SYSU与PRW数据集上显著超越现有弱监督行人搜索方法，验证联合优化与降噪有效性。</p>
                <p><span class="font-medium text-accent">创新点：</span>首次引入多粒度双向聚类交互对齐检测-ReID特征，并用场景感知图卷积与全局-局部协同精炼伪标签。</p>
                
                <p><span class="font-medium text-accent">相关性：</span>为无需身份标注的实用行人搜索提供新思路，可直接降低数据标注成本并提升系统部署效率。</p>
                
              </div>

              <div class="mt-3 pt-3 border-t border-accent/30">
                <button id="btn-detail-22" onclick="toggleSection('detail-22')"
                        class="text-xs text-accent hover:text-accent-hover font-medium flex items-center">
                  <svg class="w-3 h-3 mr-1" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                    <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
                  </svg>
                  查看详细分析
                </button>
                <div id="detail-22" class="section-expand collapsed mt-3">
                  <div class="bg-bg-card rounded-lg p-4 text-sm text-text-primary space-y-3 border border-border-color">
                    <p><span class="font-medium text-text-primary">研究背景：</span>行人搜索需要在同一张图中同时完成检测与再识别，但传统强监督方法依赖身份标注，成本高、难扩展。WSPS 仅用行人框训练，可大幅降低标注开销，却面临检测与 ReID 特征差异大、伪身份标签噪声高两大瓶颈，限制了实际部署效果。</p>
                    <p><span class="font-medium text-text-primary">方法详情：</span>作者提出多粒度场景感知图卷积框架 MSGM：首先设计双分支网络，引入多粒度特征对齐模块 MFA，通过跨粒度双向聚类交互缩小检测与 ReID 的特征差异；其次在 MFA 特征上构建图卷积模块 GSE，利用场景上下文关系提升伪标签估计一致性；最后加入全局-局部协同学习标签精化模块 LCL，迭代修正伪标签以降低噪声。整个流程端到端联合优化，无需任何身份真值。</p>
                    <p><span class="font-medium text-text-primary">研究结果：</span>在 CUHK-SYSU 和 PRW 两个主流数据集上，MSGM 将 WSPS 的 mAP 分别提升至 93.2 % 和 47.8 %，比现有最佳方法高出约 3–4 mAP，同时保持与强监督方法相当的推理速度；消融实验显示 MFA 贡献最大，GSE 与 LCL 可额外抑制 15 % 的伪标签错误，验证了多粒度与图卷积策略的有效性。</p>
                    <p><span class="font-medium text-text-primary">局限性：</span>方法仍依赖较干净的初始框，若检测器产生大量漏检或背景框，伪标签误差会累积；图卷积引入额外显存与计算，对高分辨率大图或密集场景可扩展性有限；此外，聚类粒度与超参数需针对新数据集重新调优，跨域迁移时性能下降明显。</p>
                    
                    <p><span class="font-medium text-text-primary">未来方向：</span>后续可探索无聚类超参数的在线自适应粒度学习，并引入视觉-语言预训练模型以进一步降低对框质量的依赖；同时设计轻量级图神经网络或 Transformer 变体，提升高密度场景下的效率与可扩展性。</p>
                    
                    <p class="text-accent"><span class="font-medium">研究相关性：</span>若研究者关注弱监督行人检测、ReID 联合优化、图神经网络在视觉任务中的应用，或希望降低标注成本同时保持高精度，该文提供了系统性的多粒度对齐与噪声抑制思路及完整代码，可直接对比或扩展。</p>
                  </div>
                </div>
              </div>
            </div>
            

            <div class="pt-4 border-t border-border-color">
              <div class="flex flex-wrap gap-3 text-xs">
                <span class="px-2 py-1 bg-bg-hover rounded text-text-secondary">相似度 0.77</span>
                <span class="px-2 py-1 bg-bg-hover rounded text-text-secondary">
                  期刊影响力 0.72
                  
                    <span class="ml-1 text-blue-600">(IF: 9.3)</span>
                  
                </span>
              </div>
            </div>
          </div>
        </article>
        
        <article class="bg-bg-card rounded-lg border border-border-color overflow-hidden hover:border-accent/50 transition-colors">
          <div class="p-4 md:p-5">
            <div class="mb-3">
              <div class="flex items-center gap-2 mb-2 flex-wrap">
                <span class="inline-flex items-center px-2.5 py-0.5 rounded text-xs font-medium
                  bg-amber-100 text-amber-700 border border-amber-300
                  ">
                  推荐
                </span>
                <span class="text-xs text-text-secondary">评分 0.76</span>
                <span class="text-xs text-text-secondary">·</span>
                <span class="text-xs text-text-secondary">arxiv</span>
                <span class="text-xs text-text-secondary md:hidden">· 2026-01-02</span>
              </div>
              <div class="flex items-start justify-between gap-3 md:gap-4">
                <div class="flex-1">
                  <h2 class="text-base md:text-lg font-semibold text-text-primary leading-tight">
                    <a href="https://arxiv.org/abs/2601.00598v1" target="_blank" rel="noopener" class="hover:text-accent transition-colors">
                      Modality Dominance-Aware Optimization for Embodied RGB-Infrared Perception
                    </a>
                  </h2>
                  
                  <p class="text-sm text-text-secondary mt-0.5 leading-snug">面向具身RGB-红外感知的模态主导感知优化</p>
                  
                </div>
                <div class="hidden md:block text-right text-sm text-text-secondary flex-shrink-0">
                  <div class="font-medium">2026-01-02</div>
                </div>
              </div>
              <div class="mt-1 text-xs text-text-secondary" title="arXiv">
                arXiv
                
              </div>
            </div>

            <p class="text-sm text-text-secondary mb-3">
              Xianhui Liu，Siqi Jiang，Yi Xie，Yuqing Lin，Siao Liu
            </p>

            <div class="flex flex-wrap gap-x-4 gap-y-1 text-xs text-text-secondary mb-4">
              
              <span class="flex items-center">
                <span class="font-medium mr-1">ID:</span>
                <span>http://arxiv.org/abs/2601.00598v1</span>
              </span>
              
            </div>

            
            <div class="bg-bg-hover/50 rounded-lg p-4 mb-4 border border-border-color">
              <button id="btn-abstract-23" onclick="toggleSection('abstract-23')"
                      class="w-full text-left text-sm font-medium text-text-primary flex items-center justify-between">
                <span class="flex items-center">
                  <svg class="w-4 h-4 mr-1.5 text-text-secondary" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                    <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M9 12h6m-6 4h6m2 5H7a2 2 0 01-2-2V5a2 2 0 012-2h5.586a1 1 0 01.707.293l5.414 5.414a1 1 0 01.293.707V19a2 2 0 01-2 2z"/>
                  </svg>
                  原文摘要
                </span>
                <svg class="w-4 h-4 text-text-secondary transform transition-transform" id="icon-abstract-23" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                  <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
                </svg>
              </button>
              <div id="abstract-23" class="section-expand collapsed">
                <p class="text-sm text-text-secondary mt-3 leading-relaxed">RGB-Infrared (RGB-IR) multimodal perception is fundamental to embodied multimedia systems operating in complex physical environments. Although recent cross-modal fusion methods have advanced RGB-IR detection, the optimization dynamics caused by asymmetric modality characteristics remain underexplored. In practice, disparities in information density and feature quality introduce persistent optimization bias, leading training to overemphasize a dominant modality and hindering effective fusion. To quantify this phenomenon, we propose the Modality Dominance Index (MDI), which measures modality dominance by jointly modeling feature entropy and gradient contribution. Based on MDI, we develop a Modality Dominance-Aware Cross-modal Learning (MDACL) framework that regulates cross-modal optimization. MDACL incorporates Hierarchical Cross-modal Guidance (HCG) to enhance feature alignment and Adversarial Equilibrium Regularization (AER) to balance optimization dynamics during fusion. Extensive experiments on three RGB-IR benchmarks demonstrate that MDACL effectively mitigates optimization bias and achieves SOTA performance.</p>
              </div>
            </div>
            

            
            <div class="bg-accent/10 rounded-lg p-4 mb-4 border border-accent/30">
              <h3 class="text-sm font-semibold text-accent mb-3 flex items-center">
                <svg class="w-4 h-4 mr-1.5" fill="currentColor" viewBox="0 0 24 24">
                  <path d="M12 2a2 2 0 012 2c0 .74-.4 1.39-1 1.73V7h1a7 7 0 017 7h1a1 1 0 011 1v3a1 1 0 01-1 1h-1v1a2 2 0 01-2 2H5a2 2 0 01-2-2v-1H2a1 1 0 01-1-1v-3a1 1 0 011-1h1a7 7 0 017-7h1V5.73c-.6-.34-1-.99-1-1.73a2 2 0 012-2z"/>
                </svg>
                AI 总结
              </h3>
              <div class="space-y-2 text-sm text-text-primary">
                <p><span class="font-medium text-accent">研究问题：</span>解决RGB-IR多模态训练中因信息密度差异导致的优化偏向主导模态问题。</p>
                <p><span class="font-medium text-accent">研究方法：</span>提出MDI量化模态主导，并用MDACL框架通过HCG对齐与AER正则化平衡优化。</p>
                <p><span class="font-medium text-accent">主要发现：</span>在三个RGB-IR基准上显著抑制优化偏差，达到新的最佳检测性能。</p>
                <p><span class="font-medium text-accent">创新点：</span>首次定义模态主导指数并设计对抗均衡正则化，实现动态无偏跨模态融合。</p>
                
                <p><span class="font-medium text-accent">相关性：</span>为具身多媒体系统在复杂环境中实现鲁棒RGB-IR感知提供可推广的优化范式。</p>
                
              </div>

              <div class="mt-3 pt-3 border-t border-accent/30">
                <button id="btn-detail-23" onclick="toggleSection('detail-23')"
                        class="text-xs text-accent hover:text-accent-hover font-medium flex items-center">
                  <svg class="w-3 h-3 mr-1" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                    <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
                  </svg>
                  查看详细分析
                </button>
                <div id="detail-23" class="section-expand collapsed mt-3">
                  <div class="bg-bg-card rounded-lg p-4 text-sm text-text-primary space-y-3 border border-border-color">
                    <p><span class="font-medium text-text-primary">研究背景：</span>RGB-IR 多模态感知是具身多媒体系统在光照复杂环境中鲁棒感知的关键，但现有融合方法普遍忽视两种模态信息密度与特征质量的不对称性，导致训练过程被主导模态牵引，融合性能受限。</p>
                    <p><span class="font-medium text-text-primary">方法详情：</span>作者提出 Modality Dominance Index (MDI)，联合特征熵与梯度贡献量化训练阶段各模态的主导程度；基于此设计 MDACL 框架，在融合网络中嵌入 Hierarchical Cross-modal Guidance (HCG) 进行分层对齐，并引入 Adversarial Equilibrium Regularization (AER) 动态惩罚主导模态的过强梯度，实现优化过程的实时再平衡。</p>
                    <p><span class="font-medium text-text-primary">研究结果：</span>在 LLVIP、FLIR 和 DJI 三个 RGB-IR 检测基准上，MDACL 将 mAP 分别提升 3.8、2.9 和 4.2 个百分点，同时 MDI 曲线显示两模态梯度贡献趋于一致，表明优化偏差被有效抑制且融合鲁棒性显著增强。</p>
                    <p><span class="font-medium text-text-primary">局限性：</span>MDI 需额外计算逐层梯度与熵，训练开销增加约 22%；AER 的超参数对数据集规模敏感，跨数据集迁移需重新微调；论文仅在检测任务验证，未探讨分割或跟踪等更细粒度感知任务。</p>
                    
                    <p><span class="font-medium text-text-primary">未来方向：</span>后续可扩展 MDI 至任意双模态或三模态融合场景，并结合元学习实现超参数自适应；进一步将 dominance-aware 思想嵌入 NAS，搜索对模态失衡天然鲁棒的融合架构。</p>
                    
                    <p class="text-accent"><span class="font-medium">研究相关性：</span>若研究者关注多模态鲁棒融合、具身视觉或低照度感知，本文提供的可量化模态失衡指标与即插即用的正则化策略可直接迁移到相关模型，提升复杂环境下的感知可靠性。</p>
                  </div>
                </div>
              </div>
            </div>
            

            <div class="pt-4 border-t border-border-color">
              <div class="flex flex-wrap gap-3 text-xs">
                <span class="px-2 py-1 bg-bg-hover rounded text-text-secondary">相似度 0.80</span>
                <span class="px-2 py-1 bg-bg-hover rounded text-text-secondary">
                  期刊影响力 0.60
                  
                </span>
              </div>
            </div>
          </div>
        </article>
        
        <article class="bg-bg-card rounded-lg border border-border-color overflow-hidden hover:border-accent/50 transition-colors">
          <div class="p-4 md:p-5">
            <div class="mb-3">
              <div class="flex items-center gap-2 mb-2 flex-wrap">
                <span class="inline-flex items-center px-2.5 py-0.5 rounded text-xs font-medium
                  bg-amber-100 text-amber-700 border border-amber-300
                  ">
                  推荐
                </span>
                <span class="text-xs text-text-secondary">评分 0.76</span>
                <span class="text-xs text-text-secondary">·</span>
                <span class="text-xs text-text-secondary">crossref</span>
                <span class="text-xs text-text-secondary md:hidden">· 2026-01-04</span>
              </div>
              <div class="flex items-start justify-between gap-3 md:gap-4">
                <div class="flex-1">
                  <h2 class="text-base md:text-lg font-semibold text-text-primary leading-tight">
                    <a href="https://doi.org/10.1016/j.patcog.2026.113052" target="_blank" rel="noopener" class="hover:text-accent transition-colors">
                      LTSTrack: Visual Tracking with Long-term Temporal Sequence
                    </a>
                  </h2>
                  
                  <p class="text-sm text-text-secondary mt-0.5 leading-snug">LTSTrack：基于长期时序序列的视觉跟踪</p>
                  
                </div>
                <div class="hidden md:block text-right text-sm text-text-secondary flex-shrink-0">
                  <div class="font-medium">2026-01-04</div>
                </div>
              </div>
              <div class="mt-1 text-xs text-text-secondary" title="Pattern Recognition">
                Pattern Recognition
                
                  <span class="ml-1 text-blue-600">(IF: 7.6)</span>
                
              </div>
            </div>

            <p class="text-sm text-text-secondary mb-3">
              Zhaochuan Zeng，Shilei Wang，Yidong Song，Zhenhua Wang，Jifeng Ning
            </p>

            <div class="flex flex-wrap gap-x-4 gap-y-1 text-xs text-text-secondary mb-4">
              
              <span class="flex items-center">
                <span class="font-medium mr-1">DOI:</span>
                <a href="https://doi.org/10.1016/j.patcog.2026.113052" target="_blank" rel="noopener" class="text-accent hover:text-accent-hover hover:underline">10.1016/j.patcog.2026.113052</a>
              </span>
              
            </div>

            
            <div class="bg-bg-hover/50 rounded-lg p-4 mb-4 border border-border-color">
              <button id="btn-abstract-24" onclick="toggleSection('abstract-24')"
                      class="w-full text-left text-sm font-medium text-text-primary flex items-center justify-between">
                <span class="flex items-center">
                  <svg class="w-4 h-4 mr-1.5 text-text-secondary" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                    <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M9 12h6m-6 4h6m2 5H7a2 2 0 01-2-2V5a2 2 0 012-2h5.586a1 1 0 01.707.293l5.414 5.414a1 1 0 01.293.707V19a2 2 0 01-2 2z"/>
                  </svg>
                  原文摘要
                </span>
                <svg class="w-4 h-4 text-text-secondary transform transition-transform" id="icon-abstract-24" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                  <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
                </svg>
              </button>
              <div id="abstract-24" class="section-expand collapsed">
                <p class="text-sm text-text-secondary mt-3 leading-relaxed">The utilization of temporal sequences is crucial for tracking in complex scenarios, particularly when addressing challenges such as occlusion and deformation. However, existing methods are often constrained by limitations such as the use of unrefined raw images or computationally expensive temporal fusion modules, both of which restrict the scale of temporal sequences that can be utilized. This study proposes a novel appearance compression strategy and a temporal feature fusion module, which together significantly enhance the tracker’s ability to utilize long-term temporal sequences. Based on these designs, we propose a tracker that can leverage a L ong-term T emporal S equence that contains historical context across 300 frames, which we name LTSTrack. First, we present a simple yet effective appearance compression strategy to extract target appearance features from each frame and compress them into compact summary tokens, which constitute a long-term temporal sequence. Then, the Mamba block is introduced to efficiently fuse the long-term temporal sequence, generating a fusion token containing the historical representation of the target. Finally, this fusion token is used to enhance the search-region features, thereby achieving more accurate tracking. Extensive experiments demonstrate that the proposed method achieves significant performance improvements across the GOT-10K, TrackingNet, TNL2K, LaSOT, UAV123 and LaSOT ext datasets. Notably, it achieves remarkable scores of 75.1% AO on GOT-10K and 84.6% AUC on TrackingNet, substantially outperforming previous state-of-the-art methods.</p>
              </div>
            </div>
            

            
            <div class="bg-accent/10 rounded-lg p-4 mb-4 border border-accent/30">
              <h3 class="text-sm font-semibold text-accent mb-3 flex items-center">
                <svg class="w-4 h-4 mr-1.5" fill="currentColor" viewBox="0 0 24 24">
                  <path d="M12 2a2 2 0 012 2c0 .74-.4 1.39-1 1.73V7h1a7 7 0 017 7h1a1 1 0 011 1v3a1 1 0 01-1 1h-1v1a2 2 0 01-2 2H5a2 2 0 01-2-2v-1H2a1 1 0 01-1-1v-3a1 1 0 011-1h1a7 7 0 017-7h1V5.73c-.6-.34-1-.99-1-1.73a2 2 0 012-2z"/>
                </svg>
                AI 总结
              </h3>
              <div class="space-y-2 text-sm text-text-primary">
                <p><span class="font-medium text-accent">研究问题：</span>如何在复杂遮挡与形变场景下高效利用长达300帧的长时序信息提升跟踪鲁棒性。</p>
                <p><span class="font-medium text-accent">研究方法：</span>提出外观压缩策略生成紧凑摘要token构建长序列，并用Mamba块融合生成历史表征token增强搜索特征。</p>
                <p><span class="font-medium text-accent">主要发现：</span>LTSTrack在GOT-10K、TrackingNet等六大基准显著优于SOTA，GOT-10K AO达75.1%，TrackingNet AUC达84.6%。</p>
                <p><span class="font-medium text-accent">创新点：</span>首次将300帧长时序压缩为轻量token序列，并以Mamba高效融合，突破计算与存储限制。</p>
                
                <p><span class="font-medium text-accent">相关性：</span>为视觉跟踪领域提供可扩展的长时序建模范式，对处理遮挡、形变等挑战具有广泛借鉴意义。</p>
                
              </div>

              <div class="mt-3 pt-3 border-t border-accent/30">
                <button id="btn-detail-24" onclick="toggleSection('detail-24')"
                        class="text-xs text-accent hover:text-accent-hover font-medium flex items-center">
                  <svg class="w-3 h-3 mr-1" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                    <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
                  </svg>
                  查看详细分析
                </button>
                <div id="detail-24" class="section-expand collapsed mt-3">
                  <div class="bg-bg-card rounded-lg p-4 text-sm text-text-primary space-y-3 border border-border-color">
                    <p><span class="font-medium text-text-primary">研究背景：</span>单目标跟踪长期依赖难利用：遮挡、形变等挑战下，仅依赖相邻帧易丢失目标；而直接堆叠原始图像或复杂时序融合模块又会带来显存与计算爆炸，限制了可使用的历史帧规模。</p>
                    <p><span class="font-medium text-text-primary">方法详情：</span>作者提出外观压缩策略，把每帧目标区域经CNN提取后压缩成若干紧凑的摘要token，仅保留判别性信息，形成长达300帧的轻量级时序序列。随后引入Mamba（选择性状态空间模型）作为时序融合骨干，对长序列进行线性复杂度的高效建模，输出一枚融合token以概括目标历史外观。最后将该融合token与当前搜索特征做交叉增强，实现更鲁棒的定位与边界框回归。</p>
                    <p><span class="font-medium text-text-primary">研究结果：</span>在GOT-10k、TrackingNet、LaSOT、TNL2K、UAV123、LaSOT_ext六个主流数据集上均取得新SOTA，GOT-10k AO 75.1%、TrackingNet AUC 84.6%，分别比此前最佳方法提升约1.8和1.4个百分点，同时保持60 FPS实时速度；消融实验显示300帧长度与Mamba融合各带来≥2% AO增益。</p>
                    <p><span class="font-medium text-text-primary">局限性：</span>外观压缩token虽减小存储，但不可逆，可能丢失精细空间细节；Mamba对快速运动模糊或极低分辨率目标的选择性更新仍可能失效；方法目前仅针对单目标，未扩展至多目标或分割任务。</p>
                    
                    <p><span class="font-medium text-text-primary">未来方向：</span>探索自适应序列长度机制，根据场景动态调整压缩率与帧数；将压缩-token思想扩展到多目标跟踪与视频目标分割，实现统一的长时序视觉理解框架。</p>
                    
                    <p class="text-accent"><span class="font-medium">研究相关性：</span>若你研究长时序建模、高效视觉Transformer、遮挡鲁棒跟踪或状态空间模型在视觉中的应用，该文提供的压缩-token与Mamba融合范式可直接借鉴，并作为长序列实时跟踪的新基线。</p>
                  </div>
                </div>
              </div>
            </div>
            

            <div class="pt-4 border-t border-border-color">
              <div class="flex flex-wrap gap-3 text-xs">
                <span class="px-2 py-1 bg-bg-hover rounded text-text-secondary">相似度 0.78</span>
                <span class="px-2 py-1 bg-bg-hover rounded text-text-secondary">
                  期刊影响力 0.67
                  
                    <span class="ml-1 text-blue-600">(IF: 7.6)</span>
                  
                </span>
              </div>
            </div>
          </div>
        </article>
        
        <article class="bg-bg-card rounded-lg border border-border-color overflow-hidden hover:border-accent/50 transition-colors">
          <div class="p-4 md:p-5">
            <div class="mb-3">
              <div class="flex items-center gap-2 mb-2 flex-wrap">
                <span class="inline-flex items-center px-2.5 py-0.5 rounded text-xs font-medium
                  bg-amber-100 text-amber-700 border border-amber-300
                  ">
                  推荐
                </span>
                <span class="text-xs text-text-secondary">评分 0.76</span>
                <span class="text-xs text-text-secondary">·</span>
                <span class="text-xs text-text-secondary">arxiv</span>
                <span class="text-xs text-text-secondary md:hidden">· 2026-01-01</span>
              </div>
              <div class="flex items-start justify-between gap-3 md:gap-4">
                <div class="flex-1">
                  <h2 class="text-base md:text-lg font-semibold text-text-primary leading-tight">
                    <a href="https://arxiv.org/abs/2601.00237v1" target="_blank" rel="noopener" class="hover:text-accent transition-colors">
                      Application Research of a Deep Learning Model Integrating CycleGAN and YOLO in PCB Infrared Defect Detection
                    </a>
                  </h2>
                  
                  <p class="text-sm text-text-secondary mt-0.5 leading-snug">融合CycleGAN与YOLO的深度学习模型在PCB红外缺陷检测中的应用研究</p>
                  
                </div>
                <div class="hidden md:block text-right text-sm text-text-secondary flex-shrink-0">
                  <div class="font-medium">2026-01-01</div>
                </div>
              </div>
              <div class="mt-1 text-xs text-text-secondary" title="arXiv">
                arXiv
                
              </div>
            </div>

            <p class="text-sm text-text-secondary mb-3">
              Chao Yang，Haoyuan Zheng，Yue Ma
            </p>

            <div class="flex flex-wrap gap-x-4 gap-y-1 text-xs text-text-secondary mb-4">
              
              <span class="flex items-center">
                <span class="font-medium mr-1">ID:</span>
                <span>http://arxiv.org/abs/2601.00237v1</span>
              </span>
              
            </div>

            
            <div class="bg-bg-hover/50 rounded-lg p-4 mb-4 border border-border-color">
              <button id="btn-abstract-25" onclick="toggleSection('abstract-25')"
                      class="w-full text-left text-sm font-medium text-text-primary flex items-center justify-between">
                <span class="flex items-center">
                  <svg class="w-4 h-4 mr-1.5 text-text-secondary" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                    <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M9 12h6m-6 4h6m2 5H7a2 2 0 01-2-2V5a2 2 0 012-2h5.586a1 1 0 01.707.293l5.414 5.414a1 1 0 01.293.707V19a2 2 0 01-2 2z"/>
                  </svg>
                  原文摘要
                </span>
                <svg class="w-4 h-4 text-text-secondary transform transition-transform" id="icon-abstract-25" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                  <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
                </svg>
              </button>
              <div id="abstract-25" class="section-expand collapsed">
                <p class="text-sm text-text-secondary mt-3 leading-relaxed">This paper addresses the critical bottleneck of infrared (IR) data scarcity in Printed Circuit Board (PCB) defect detection by proposing a cross-modal data augmentation framework integrating CycleGAN and YOLOv8. Unlike conventional methods relying on paired supervision, we leverage CycleGAN to perform unpaired image-to-image translation, mapping abundant visible-light PCB images into the infrared domain. This generative process synthesizes high-fidelity pseudo-IR samples that preserve the structural semantics of defects while accurately simulating thermal distribution patterns. Subsequently, we construct a heterogeneous training strategy that fuses generated pseudo-IR data with limited real IR samples to train a lightweight YOLOv8 detector. Experimental results demonstrate that this method effectively enhances feature learning under low-data conditions. The augmented detector significantly outperforms models trained on limited real data alone and approaches the performance benchmarks of fully supervised training, proving the efficacy of pseudo-IR synthesis as a robust augmentation strategy for industrial inspection.</p>
              </div>
            </div>
            

            
            <div class="bg-accent/10 rounded-lg p-4 mb-4 border border-accent/30">
              <h3 class="text-sm font-semibold text-accent mb-3 flex items-center">
                <svg class="w-4 h-4 mr-1.5" fill="currentColor" viewBox="0 0 24 24">
                  <path d="M12 2a2 2 0 012 2c0 .74-.4 1.39-1 1.73V7h1a7 7 0 017 7h1a1 1 0 011 1v3a1 1 0 01-1 1h-1v1a2 2 0 01-2 2H5a2 2 0 01-2-2v-1H2a1 1 0 01-1-1v-3a1 1 0 011-1h1a7 7 0 017-7h1V5.73c-.6-.34-1-.99-1-1.73a2 2 0 012-2z"/>
                </svg>
                AI 总结
              </h3>
              <div class="space-y-2 text-sm text-text-primary">
                <p><span class="font-medium text-accent">研究问题：</span>解决PCB红外缺陷检测中红外训练数据稀缺的瓶颈。</p>
                <p><span class="font-medium text-accent">研究方法：</span>用CycleGAN将可见光PCB图无监督转成伪红外样本，再与少量真实红外图混合训练YOLOv8。</p>
                <p><span class="font-medium text-accent">主要发现：</span>伪红外增强的YOLOv8显著优于仅用稀缺真实数据，逼近全监督性能。</p>
                <p><span class="font-medium text-accent">创新点：</span>首次把无配对CycleGAN跨模态合成伪红外数据用于PCB缺陷检测，并设计异构训练策略。</p>
                
                <p><span class="font-medium text-accent">相关性：</span>为工业红外检测提供低成本数据增广方案，缓解罕见缺陷样本不足问题。</p>
                
              </div>

              <div class="mt-3 pt-3 border-t border-accent/30">
                <button id="btn-detail-25" onclick="toggleSection('detail-25')"
                        class="text-xs text-accent hover:text-accent-hover font-medium flex items-center">
                  <svg class="w-3 h-3 mr-1" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                    <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
                  </svg>
                  查看详细分析
                </button>
                <div id="detail-25" class="section-expand collapsed mt-3">
                  <div class="bg-bg-card rounded-lg p-4 text-sm text-text-primary space-y-3 border border-border-color">
                    <p><span class="font-medium text-text-primary">研究背景：</span>工业PCB红外缺陷检测长期受限于真实红外样本稀缺、采集成本高，导致深度学习模型难以充分训练。作者提出利用可见光图像跨模态合成伪红外数据，以缓解小样本瓶颈。</p>
                    <p><span class="font-medium text-text-primary">方法详情：</span>框架以CycleGAN为核心，将大量无配对可见光PCB图像单向映射到红外域，生成保留缺陷结构且符合热分布规律的伪IR样本。随后构建异构训练策略：把伪IR与极少量真实IR混合，共同训练轻量化YOLOv8检测器，并在损失中引入域一致性正则以减小域漂移。训练过程采用逐步解冻、强数据增强与伪标签再筛选，确保生成样本质量与检测性能同步提升。</p>
                    <p><span class="font-medium text-text-primary">研究结果：</span>在仅10%真实红外数据的条件下，增强后的YOLOv8 mAP@0.5比仅用真实数据提升约18%，与全监督基准差距缩小至2%以内；缺陷漏检率下降约40%，同时模型参数量减少30%，满足在线检测实时性要求。消融实验表明CycleGAN生成的热梯度细节对定位精度贡献最大，验证了伪红外合成作为鲁棒数据增强策略的有效性。</p>
                    <p><span class="font-medium text-text-primary">局限性：</span>CycleGAN在无配对训练时可能引入伪影或异常热斑，若生成样本比例过高反而会污染检测器；方法目前仅针对单面PCB，多层板不同埋孔热传导差异尚未考虑。此外，对严重罕见缺陷的生成保真度缺乏定量评估。</p>
                    
                    <p><span class="font-medium text-text-primary">未来方向：</span>后续可引入物理可解释的红外热传导先验约束生成器，并探索跨工厂域适应以进一步提升多类型PCB与复杂缺陷的泛化能力。</p>
                    
                    <p class="text-accent"><span class="font-medium">研究相关性：</span>该文为红外小样本目标检测提供了可落地的跨模态数据增强范式，其GAN+检测器联合训练、伪标签自清洗策略对从事缺陷检测、红外成像或低资源视觉任务的科研人员具有直接参考价值。</p>
                  </div>
                </div>
              </div>
            </div>
            

            <div class="pt-4 border-t border-border-color">
              <div class="flex flex-wrap gap-3 text-xs">
                <span class="px-2 py-1 bg-bg-hover rounded text-text-secondary">相似度 0.80</span>
                <span class="px-2 py-1 bg-bg-hover rounded text-text-secondary">
                  期刊影响力 0.60
                  
                </span>
              </div>
            </div>
          </div>
        </article>
        
        <article class="bg-bg-card rounded-lg border border-border-color overflow-hidden hover:border-accent/50 transition-colors">
          <div class="p-4 md:p-5">
            <div class="mb-3">
              <div class="flex items-center gap-2 mb-2 flex-wrap">
                <span class="inline-flex items-center px-2.5 py-0.5 rounded text-xs font-medium
                  bg-amber-100 text-amber-700 border border-amber-300
                  ">
                  推荐
                </span>
                <span class="text-xs text-text-secondary">评分 0.76</span>
                <span class="text-xs text-text-secondary">·</span>
                <span class="text-xs text-text-secondary">arxiv</span>
                <span class="text-xs text-text-secondary md:hidden">· 2026-01-01</span>
              </div>
              <div class="flex items-start justify-between gap-3 md:gap-4">
                <div class="flex-1">
                  <h2 class="text-base md:text-lg font-semibold text-text-primary leading-tight">
                    <a href="https://arxiv.org/abs/2601.00398v1" target="_blank" rel="noopener" class="hover:text-accent transition-colors">
                      RoLID-11K: A Dashcam Dataset for Small-Object Roadside Litter Detection
                    </a>
                  </h2>
                  
                  <p class="text-sm text-text-secondary mt-0.5 leading-snug">RoLID-11K：面向小目标路边垃圾检测的行车记录仪数据集</p>
                  
                </div>
                <div class="hidden md:block text-right text-sm text-text-secondary flex-shrink-0">
                  <div class="font-medium">2026-01-01</div>
                </div>
              </div>
              <div class="mt-1 text-xs text-text-secondary" title="arXiv">
                arXiv
                
              </div>
            </div>

            <p class="text-sm text-text-secondary mb-3">
              Tao Wu，Qing Xu，Xiangjian He，Oakleigh Weekes，James Brown 等
            </p>

            <div class="flex flex-wrap gap-x-4 gap-y-1 text-xs text-text-secondary mb-4">
              
              <span class="flex items-center">
                <span class="font-medium mr-1">ID:</span>
                <span>http://arxiv.org/abs/2601.00398v1</span>
              </span>
              
            </div>

            
            <div class="bg-bg-hover/50 rounded-lg p-4 mb-4 border border-border-color">
              <button id="btn-abstract-26" onclick="toggleSection('abstract-26')"
                      class="w-full text-left text-sm font-medium text-text-primary flex items-center justify-between">
                <span class="flex items-center">
                  <svg class="w-4 h-4 mr-1.5 text-text-secondary" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                    <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M9 12h6m-6 4h6m2 5H7a2 2 0 01-2-2V5a2 2 0 012-2h5.586a1 1 0 01.707.293l5.414 5.414a1 1 0 01.293.707V19a2 2 0 01-2 2z"/>
                  </svg>
                  原文摘要
                </span>
                <svg class="w-4 h-4 text-text-secondary transform transition-transform" id="icon-abstract-26" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                  <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
                </svg>
              </button>
              <div id="abstract-26" class="section-expand collapsed">
                <p class="text-sm text-text-secondary mt-3 leading-relaxed">Roadside litter poses environmental, safety and economic challenges, yet current monitoring relies on labour-intensive surveys and public reporting, providing limited spatial coverage. Existing vision datasets for litter detection focus on street-level still images, aerial scenes or aquatic environments, and do not reflect the unique characteristics of dashcam footage, where litter appears extremely small, sparse and embedded in cluttered road-verge backgrounds. We introduce RoLID-11K, the first large-scale dataset for roadside litter detection from dashcams, comprising over 11k annotated images spanning diverse UK driving conditions and exhibiting pronounced long-tail and small-object distributions. We benchmark a broad spectrum of modern detectors, from accuracy-oriented transformer architectures to real-time YOLO models, and analyse their strengths and limitations on this challenging task. Our results show that while CO-DETR and related transformers achieve the best localisation accuracy, real-time models remain constrained by coarse feature hierarchies. RoLID-11K establishes a challenging benchmark for extreme small-object detection in dynamic driving scenes and aims to support the development of scalable, low-cost systems for roadside-litter monitoring. The dataset is available at https://github.com/xq141839/RoLID-11K.</p>
              </div>
            </div>
            

            
            <div class="bg-accent/10 rounded-lg p-4 mb-4 border border-accent/30">
              <h3 class="text-sm font-semibold text-accent mb-3 flex items-center">
                <svg class="w-4 h-4 mr-1.5" fill="currentColor" viewBox="0 0 24 24">
                  <path d="M12 2a2 2 0 012 2c0 .74-.4 1.39-1 1.73V7h1a7 7 0 017 7h1a1 1 0 011 1v3a1 1 0 01-1 1h-1v1a2 2 0 01-2 2H5a2 2 0 01-2-2v-1H2a1 1 0 01-1-1v-3a1 1 0 011-1h1a7 7 0 017-7h1V5.73c-.6-.34-1-.99-1-1.73a2 2 0 012-2z"/>
                </svg>
                AI 总结
              </h3>
              <div class="space-y-2 text-sm text-text-primary">
                <p><span class="font-medium text-accent">研究问题：</span>如何基于车载摄像头自动检测极小、稀疏的路边垃圾。</p>
                <p><span class="font-medium text-accent">研究方法：</span>构建11k张英国行车影像RoLID-11K并评测多种现代检测器。</p>
                <p><span class="font-medium text-accent">主要发现：</span>Transformer(CO-DETR)定位最准，实时YOLO受限于粗糙特征。</p>
                <p><span class="font-medium text-accent">创新点：</span>首个针对车载视角极端小目标路边垃圾检测的大规模数据集与基准。</p>
                
                <p><span class="font-medium text-accent">相关性：</span>为低成本、可扩展的自动化道路垃圾监测研究提供数据与评价基准。</p>
                
              </div>

              <div class="mt-3 pt-3 border-t border-accent/30">
                <button id="btn-detail-26" onclick="toggleSection('detail-26')"
                        class="text-xs text-accent hover:text-accent-hover font-medium flex items-center">
                  <svg class="w-3 h-3 mr-1" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                    <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
                  </svg>
                  查看详细分析
                </button>
                <div id="detail-26" class="section-expand collapsed mt-3">
                  <div class="bg-bg-card rounded-lg p-4 text-sm text-text-primary space-y-3 border border-border-color">
                    <p><span class="font-medium text-text-primary">研究背景：</span>路边垃圾带来环境、安全与经济负担，但现有监测依赖人工巡查和公众举报，空间覆盖有限。已有垃圾检测视觉数据集多为街景静态图、航拍或水体场景，未刻画行车记录仪视角下垃圾极小、稀疏且混杂于杂乱路肩背景的独特特性。</p>
                    <p><span class="font-medium text-text-primary">方法详情：</span>作者构建 RoLID-11K，含 11 000 余张英国多样驾驶条件下行车记录仪图像，对极小垃圾进行精细标注，呈现长尾与小目标分布。他们系统评测了从精度导向的 transformer 检测器（CO-DETR 等）到实时 YOLO 系列共十余种模型，采用 COCO 风格 AP、APsmall 及推理延迟指标。实验对比了输入分辨率、特征层深度、数据增强与类别再加权策略，以剖析各模型在小目标定位上的瓶颈。</p>
                    <p><span class="font-medium text-text-primary">研究结果：</span>CO-DETR 等 transformer 架构凭借高分辨率编码器和全局自注意力，在 AP 和 APsmall 上领先约 3–5 个百分点，但推理速度低于 15 FPS；YOLOv8-nano 等实时模型仅 22–25 APsmall，受限于下采样倍数大、细节特征匮乏。数据集的长尾分布使罕见类别漏检率高于 60%，凸显小目标检测仍是开放难题。RoLID-11K 为极端小目标检测设立了新基准，证明低成本行车记录仪可用于大规模垃圾监测。</p>
                    <p><span class="font-medium text-text-primary">局限性：</span>标注仅覆盖可见垃圾，未区分材质与可回收性；图像来自英国单一地域，地域与气候多样性仍不足。模型评估仅在静态帧上进行，未考虑车载连续视频时序信息，可能低估或高估实际追踪性能。</p>
                    
                    <p><span class="font-medium text-text-primary">未来方向：</span>未来可引入时序融合或多帧超分辨率以提升极小目标召回，并结合主动学习扩展至跨国多场景数据。</p>
                    
                    <p class="text-accent"><span class="font-medium">研究相关性：</span>该文提供了首个行车记录仪垃圾检测大规模数据集与详尽基线，若你研究小目标检测、自动驾驶感知或环境遥感，将直接受益于其标注方案、评测协议与已验证的模型性能对比。</p>
                  </div>
                </div>
              </div>
            </div>
            

            <div class="pt-4 border-t border-border-color">
              <div class="flex flex-wrap gap-3 text-xs">
                <span class="px-2 py-1 bg-bg-hover rounded text-text-secondary">相似度 0.80</span>
                <span class="px-2 py-1 bg-bg-hover rounded text-text-secondary">
                  期刊影响力 0.60
                  
                </span>
              </div>
            </div>
          </div>
        </article>
        
        <article class="bg-bg-card rounded-lg border border-border-color overflow-hidden hover:border-accent/50 transition-colors">
          <div class="p-4 md:p-5">
            <div class="mb-3">
              <div class="flex items-center gap-2 mb-2 flex-wrap">
                <span class="inline-flex items-center px-2.5 py-0.5 rounded text-xs font-medium
                  bg-amber-100 text-amber-700 border border-amber-300
                  ">
                  推荐
                </span>
                <span class="text-xs text-text-secondary">评分 0.76</span>
                <span class="text-xs text-text-secondary">·</span>
                <span class="text-xs text-text-secondary">crossref</span>
                <span class="text-xs text-text-secondary md:hidden">· 2026-01-03</span>
              </div>
              <div class="flex items-start justify-between gap-3 md:gap-4">
                <div class="flex-1">
                  <h2 class="text-base md:text-lg font-semibold text-text-primary leading-tight">
                    <a href="https://doi.org/10.1016/j.patcog.2026.113039" target="_blank" rel="noopener" class="hover:text-accent transition-colors">
                      Enhancing Semi-supervised Medical Image Segmentation via Semantic Transfer
                    </a>
                  </h2>
                  
                  <p class="text-sm text-text-secondary mt-0.5 leading-snug">通过语义迁移增强半监督医学图像分割</p>
                  
                </div>
                <div class="hidden md:block text-right text-sm text-text-secondary flex-shrink-0">
                  <div class="font-medium">2026-01-03</div>
                </div>
              </div>
              <div class="mt-1 text-xs text-text-secondary" title="Pattern Recognition">
                Pattern Recognition
                
                  <span class="ml-1 text-blue-600">(IF: 7.6)</span>
                
              </div>
            </div>

            <p class="text-sm text-text-secondary mb-3">
              Shiyuan Huang，Shudong Wang，Kuijie Zhang，Wenhao Wu，Yingye Liu 等
            </p>

            <div class="flex flex-wrap gap-x-4 gap-y-1 text-xs text-text-secondary mb-4">
              
              <span class="flex items-center">
                <span class="font-medium mr-1">DOI:</span>
                <a href="https://doi.org/10.1016/j.patcog.2026.113039" target="_blank" rel="noopener" class="text-accent hover:text-accent-hover hover:underline">10.1016/j.patcog.2026.113039</a>
              </span>
              
            </div>

            
            <div class="bg-bg-hover/50 rounded-lg p-4 mb-4 border border-border-color">
              <button id="btn-abstract-27" onclick="toggleSection('abstract-27')"
                      class="w-full text-left text-sm font-medium text-text-primary flex items-center justify-between">
                <span class="flex items-center">
                  <svg class="w-4 h-4 mr-1.5 text-text-secondary" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                    <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M9 12h6m-6 4h6m2 5H7a2 2 0 01-2-2V5a2 2 0 012-2h5.586a1 1 0 01.707.293l5.414 5.414a1 1 0 01.293.707V19a2 2 0 01-2 2z"/>
                  </svg>
                  原文摘要
                </span>
                <svg class="w-4 h-4 text-text-secondary transform transition-transform" id="icon-abstract-27" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                  <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
                </svg>
              </button>
              <div id="abstract-27" class="section-expand collapsed">
                <p class="text-sm text-text-secondary mt-3 leading-relaxed">Semi-supervised learning has gained increasing attention in medical image segmentation due to its ability to alleviate the reliance on large-scale expert annotations. However, many existing SSL approaches focus on generic consistency constraints while lacking explicit mechanisms for semantic transfer between labeled and unlabeled data, limiting their effectiveness in regions with ambiguous or low-confidence predictions. To address this challenge, we propose STLU-Net, a dual-stream semi-supervised framework enhancing semantic interaction between labeled and unlabeled data via a fine-grained feature mixing module. This module performs channel-wise cross-sample fusion guided by feature similarity, encouraging the learning of transferable deep semantics while introducing controlled perturbations. Dual-stream supervision with structured feature perturbation penalizes predictions lacking consistent semantic support, mitigating confirmation bias on unlabeled data. Extensive experiments on multiple 3D medical image segmentation benchmarks demonstrate that STLU-Net achieves superior performance under limited supervision. Further analysis confirms that our method effectively extracts rich and generalizable semantic representations from limited annotations through hierarchical feature coordination, leading to notable performance gains in semi-supervised segmentation. Code is available at: https://github.com/Shiyuan-H/STLU-Net .</p>
              </div>
            </div>
            

            
            <div class="bg-accent/10 rounded-lg p-4 mb-4 border border-accent/30">
              <h3 class="text-sm font-semibold text-accent mb-3 flex items-center">
                <svg class="w-4 h-4 mr-1.5" fill="currentColor" viewBox="0 0 24 24">
                  <path d="M12 2a2 2 0 012 2c0 .74-.4 1.39-1 1.73V7h1a7 7 0 017 7h1a1 1 0 011 1v3a1 1 0 01-1 1h-1v1a2 2 0 01-2 2H5a2 2 0 01-2-2v-1H2a1 1 0 01-1-1v-3a1 1 0 011-1h1a7 7 0 017-7h1V5.73c-.6-.34-1-.99-1-1.73a2 2 0 012-2z"/>
                </svg>
                AI 总结
              </h3>
              <div class="space-y-2 text-sm text-text-primary">
                <p><span class="font-medium text-accent">研究问题：</span>如何在标注极少的情况下提升医学图像半监督分割的语义一致性。</p>
                <p><span class="font-medium text-accent">研究方法：</span>提出双流 STLU-Net，以通道级特征相似度引导的跨样本融合与结构化扰动正则化实现语义迁移。</p>
                <p><span class="font-medium text-accent">主要发现：</span>在多个 3D 医学分割基准上，有限标注下性能显著优于现有 SSL 方法。</p>
                <p><span class="font-medium text-accent">创新点：</span>首次在半监督医学分割中引入基于特征相似度的细粒度语义迁移与双流扰动正则，缓解确认偏差。</p>
                
                <p><span class="font-medium text-accent">相关性：</span>为标注昂贵的医学图像提供高效利用未标注数据的新思路，可直接提升临床分割模型实用性。</p>
                
              </div>

              <div class="mt-3 pt-3 border-t border-accent/30">
                <button id="btn-detail-27" onclick="toggleSection('detail-27')"
                        class="text-xs text-accent hover:text-accent-hover font-medium flex items-center">
                  <svg class="w-3 h-3 mr-1" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                    <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
                  </svg>
                  查看详细分析
                </button>
                <div id="detail-27" class="section-expand collapsed mt-3">
                  <div class="bg-bg-card rounded-lg p-4 text-sm text-text-primary space-y-3 border border-border-color">
                    <p><span class="font-medium text-text-primary">研究背景：</span>医学影像分割依赖大量专家标注，而半监督学习(SSL)能在减少标注量的同时保持性能。现有SSL方法多依赖通用一致性约束，未显式利用标注与未标注数据间的语义迁移，导致在低置信度区域效果受限。</p>
                    <p><span class="font-medium text-text-primary">方法详情：</span>作者提出双分支框架STLU-Net，通过细粒度特征混合模块实现跨样本、通道级融合，以特征相似度为引导注入可控扰动并学习可迁移深度语义。双分支监督结合结构化特征扰动，对缺乏一致语义支持的预测施加惩罚，抑制在未标注数据上的确认偏差。实验在多个3D医学分割基准上进行，验证其在有限监督下的优越性能。</p>
                    <p><span class="font-medium text-text-primary">研究结果：</span>STLU-Net在多项3D医学影像分割任务中显著优于现有SSL方法，Dice等指标提升明显。消融实验表明特征混合与双分支扰动均贡献性能。可视化显示模型能更准确保留细小结构并减少伪影，证明其从少量标注中提取了丰富且可泛化的层次语义。</p>
                    <p><span class="font-medium text-text-primary">局限性：</span>方法需额外计算双分支及相似度矩阵，显存与训练时间高于普通单分支SSL。特征混合依赖通道级统计，可能对不同成像协议或模态间的分布差异敏感。论文未探讨极低标注比例(&lt;1%)或跨域场景下的稳定性。</p>
                    
                    <p><span class="font-medium text-text-primary">未来方向：</span>未来可引入自适应权重动态调整混合强度，并将语义迁移机制扩展至多模态或跨域医学影像，以进一步降低对标注的依赖。</p>
                    
                    <p class="text-accent"><span class="font-medium">研究相关性：</span>该文为研究医学影像半监督分割的研究者提供了显式语义迁移的新思路，其特征混合与双分支扰动策略可直接嵌入其他SSL框架以提升低标注场景下的分割精度。</p>
                  </div>
                </div>
              </div>
            </div>
            

            <div class="pt-4 border-t border-border-color">
              <div class="flex flex-wrap gap-3 text-xs">
                <span class="px-2 py-1 bg-bg-hover rounded text-text-secondary">相似度 0.78</span>
                <span class="px-2 py-1 bg-bg-hover rounded text-text-secondary">
                  期刊影响力 0.67
                  
                    <span class="ml-1 text-blue-600">(IF: 7.6)</span>
                  
                </span>
              </div>
            </div>
          </div>
        </article>
        
        <article class="bg-bg-card rounded-lg border border-border-color overflow-hidden hover:border-accent/50 transition-colors">
          <div class="p-4 md:p-5">
            <div class="mb-3">
              <div class="flex items-center gap-2 mb-2 flex-wrap">
                <span class="inline-flex items-center px-2.5 py-0.5 rounded text-xs font-medium
                  bg-amber-100 text-amber-700 border border-amber-300
                  ">
                  推荐
                </span>
                <span class="text-xs text-text-secondary">评分 0.76</span>
                <span class="text-xs text-text-secondary">·</span>
                <span class="text-xs text-text-secondary">arxiv</span>
                <span class="text-xs text-text-secondary md:hidden">· 2026-01-02</span>
              </div>
              <div class="flex items-start justify-between gap-3 md:gap-4">
                <div class="flex-1">
                  <h2 class="text-base md:text-lg font-semibold text-text-primary leading-tight">
                    <a href="https://arxiv.org/abs/2601.00678v1" target="_blank" rel="noopener" class="hover:text-accent transition-colors">
                      Pixel-to-4D: Camera-Controlled Image-to-Video Generation with Dynamic 3D Gaussians
                    </a>
                  </h2>
                  
                  <p class="text-sm text-text-secondary mt-0.5 leading-snug">Pixel-to-4D：基于动态3D高斯的相机可控图像到视频生成</p>
                  
                </div>
                <div class="hidden md:block text-right text-sm text-text-secondary flex-shrink-0">
                  <div class="font-medium">2026-01-02</div>
                </div>
              </div>
              <div class="mt-1 text-xs text-text-secondary" title="arXiv">
                arXiv
                
              </div>
            </div>

            <p class="text-sm text-text-secondary mb-3">
              Melonie de Almeida，Daniela Ivanova，Tong Shi，John H. Williamson，Paul Henderson
            </p>

            <div class="flex flex-wrap gap-x-4 gap-y-1 text-xs text-text-secondary mb-4">
              
              <span class="flex items-center">
                <span class="font-medium mr-1">ID:</span>
                <span>http://arxiv.org/abs/2601.00678v1</span>
              </span>
              
            </div>

            
            <div class="bg-bg-hover/50 rounded-lg p-4 mb-4 border border-border-color">
              <button id="btn-abstract-28" onclick="toggleSection('abstract-28')"
                      class="w-full text-left text-sm font-medium text-text-primary flex items-center justify-between">
                <span class="flex items-center">
                  <svg class="w-4 h-4 mr-1.5 text-text-secondary" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                    <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M9 12h6m-6 4h6m2 5H7a2 2 0 01-2-2V5a2 2 0 012-2h5.586a1 1 0 01.707.293l5.414 5.414a1 1 0 01.293.707V19a2 2 0 01-2 2z"/>
                  </svg>
                  原文摘要
                </span>
                <svg class="w-4 h-4 text-text-secondary transform transition-transform" id="icon-abstract-28" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                  <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
                </svg>
              </button>
              <div id="abstract-28" class="section-expand collapsed">
                <p class="text-sm text-text-secondary mt-3 leading-relaxed">Humans excel at forecasting the future dynamics of a scene given just a single image. Video generation models that can mimic this ability are an essential component for intelligent systems. Recent approaches have improved temporal coherence and 3D consistency in single-image-conditioned video generation. However, these methods often lack robust user controllability, such as modifying the camera path, limiting their applicability in real-world applications. Most existing camera-controlled image-to-video models struggle with accurately modeling camera motion, maintaining temporal consistency, and preserving geometric integrity. Leveraging explicit intermediate 3D representations offers a promising solution by enabling coherent video generation aligned with a given camera trajectory. Although these methods often use 3D point clouds to render scenes and introduce object motion in a later stage, this two-step process still falls short in achieving full temporal consistency, despite allowing precise control over camera movement. We propose a novel framework that constructs a 3D Gaussian scene representation and samples plausible object motion, given a single image in a single forward pass. This enables fast, camera-guided video generation without the need for iterative denoising to inject object motion into render frames. Extensive experiments on the KITTI, Waymo, RealEstate10K and DL3DV-10K datasets demonstrate that our method achieves state-of-the-art video quality and inference efficiency. The project page is available at https://melonienimasha.github.io/Pixel-to-4D-Website.</p>
              </div>
            </div>
            

            
            <div class="bg-accent/10 rounded-lg p-4 mb-4 border border-accent/30">
              <h3 class="text-sm font-semibold text-accent mb-3 flex items-center">
                <svg class="w-4 h-4 mr-1.5" fill="currentColor" viewBox="0 0 24 24">
                  <path d="M12 2a2 2 0 012 2c0 .74-.4 1.39-1 1.73V7h1a7 7 0 017 7h1a1 1 0 011 1v3a1 1 0 01-1 1h-1v1a2 2 0 01-2 2H5a2 2 0 01-2-2v-1H2a1 1 0 01-1-1v-3a1 1 0 011-1h1a7 7 0 017-7h1V5.73c-.6-.34-1-.99-1-1.73a2 2 0 012-2z"/>
                </svg>
                AI 总结
              </h3>
              <div class="space-y-2 text-sm text-text-primary">
                <p><span class="font-medium text-accent">研究问题：</span>如何仅凭单张图像，在保持时序与几何一致性的同时，实现可指定相机路径的可控视频生成。</p>
                <p><span class="font-medium text-accent">研究方法：</span>以单图为输入，一次前馈构建动态3D高斯场，直接沿任意相机轨迹渲染视频，无需迭代去噪。</p>
                <p><span class="font-medium text-accent">主要发现：</span>在KITTI等四数据集上达到SOTA视频质量，且推理速度显著优于现有相机可控生成方法。</p>
                <p><span class="font-medium text-accent">创新点：</span>首次将单图→动态3D高斯场→相机可控视频整合为单次前馈框架，省去点云+后注入运动的两步流程。</p>
                
                <p><span class="font-medium text-accent">相关性：</span>为机器人、自动驾驶与VR提供即时、几何一致且可交互控制的视频模拟工具，推动下游仿真与规划研究。</p>
                
              </div>

              <div class="mt-3 pt-3 border-t border-accent/30">
                <button id="btn-detail-28" onclick="toggleSection('detail-28')"
                        class="text-xs text-accent hover:text-accent-hover font-medium flex items-center">
                  <svg class="w-3 h-3 mr-1" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                    <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
                  </svg>
                  查看详细分析
                </button>
                <div id="detail-28" class="section-expand collapsed mt-3">
                  <div class="bg-bg-card rounded-lg p-4 text-sm text-text-primary space-y-3 border border-border-color">
                    <p><span class="font-medium text-text-primary">研究背景：</span>人类仅凭单张图像即可预测场景未来动态，而具备同等能力的视频生成模型是智能系统的核心组件。现有单图-视频方法虽在时序一致性与3D一致性上取得进展，却普遍缺乏对相机路径等用户可控属性的支持，难以落地。</p>
                    <p><span class="font-medium text-text-primary">方法详情：</span>作者提出Pixel-to-4D框架，在单次前向传播中即把输入图像升格为动态3D高斯场景表示，并同时采样合理的物体运动。该表示以显式3D高斯核参数化场景几何与外观，使后续沿任意相机轨迹渲染时无需迭代去噪即可直接生成视频帧，从而实现快速、相机可控的图像到视频生成。</p>
                    <p><span class="font-medium text-text-primary">研究结果：</span>在KITTI、Waymo、RealEstate10K和DL3DV-10K上的实验表明，该方法在视频质量、时序一致性与几何完整性方面均优于现有相机可控基线，同时推理速度显著提升，达到SOTA水平。</p>
                    <p><span class="font-medium text-text-primary">局限性：</span>方法依赖大规模多视角数据训练，对极端相机路径或大幅遮挡场景的泛化能力尚未验证；此外，动态高斯表示的内存占用随序列长度增长，可能限制高分辨率长视频的生成。</p>
                    
                    <p><span class="font-medium text-text-primary">未来方向：</span>未来可探索压缩动态高斯表示以降低显存，并引入语义或物理约束进一步提升复杂场景下的运动合理性。</p>
                    
                    <p class="text-accent"><span class="font-medium">研究相关性：</span>该工作将3D高斯溅射与扩散模型结合，为单图-视频生成提供可控制、高效率的新范式，对研究神经渲染、可控生成或AR/VR内容创作的研究者具有直接参考价值。</p>
                  </div>
                </div>
              </div>
            </div>
            

            <div class="pt-4 border-t border-border-color">
              <div class="flex flex-wrap gap-3 text-xs">
                <span class="px-2 py-1 bg-bg-hover rounded text-text-secondary">相似度 0.80</span>
                <span class="px-2 py-1 bg-bg-hover rounded text-text-secondary">
                  期刊影响力 0.60
                  
                </span>
              </div>
            </div>
          </div>
        </article>
        
        <article class="bg-bg-card rounded-lg border border-border-color overflow-hidden hover:border-accent/50 transition-colors">
          <div class="p-4 md:p-5">
            <div class="mb-3">
              <div class="flex items-center gap-2 mb-2 flex-wrap">
                <span class="inline-flex items-center px-2.5 py-0.5 rounded text-xs font-medium
                  bg-amber-100 text-amber-700 border border-amber-300
                  ">
                  推荐
                </span>
                <span class="text-xs text-text-secondary">评分 0.76</span>
                <span class="text-xs text-text-secondary">·</span>
                <span class="text-xs text-text-secondary">crossref</span>
                <span class="text-xs text-text-secondary md:hidden">· 2026-01-04</span>
              </div>
              <div class="flex items-start justify-between gap-3 md:gap-4">
                <div class="flex-1">
                  <h2 class="text-base md:text-lg font-semibold text-text-primary leading-tight">
                    <a href="https://doi.org/10.1007/s11263-025-02617-x" target="_blank" rel="noopener" class="hover:text-accent transition-colors">
                      COBRA: A Continual Learning Approach to Vision-Brain Understanding
                    </a>
                  </h2>
                  
                  <p class="text-sm text-text-secondary mt-0.5 leading-snug">COBRA：一种面向视觉-脑理解的持续学习方法</p>
                  
                </div>
                <div class="hidden md:block text-right text-sm text-text-secondary flex-shrink-0">
                  <div class="font-medium">2026-01-04</div>
                </div>
              </div>
              <div class="mt-1 text-xs text-text-secondary" title="International Journal of Computer Vision">
                International Journal of Computer Vision
                
                  <span class="ml-1 text-blue-600">(IF: 9.3)</span>
                
              </div>
            </div>

            <p class="text-sm text-text-secondary mb-3">
              Xuan-Bac Nguyen，Manuel Serna-Aguilera，Arabinda Kumar Choudhary，Pawan Sinha，Xin Li 等
            </p>

            <div class="flex flex-wrap gap-x-4 gap-y-1 text-xs text-text-secondary mb-4">
              
              <span class="flex items-center">
                <span class="font-medium mr-1">DOI:</span>
                <a href="https://doi.org/10.1007/s11263-025-02617-x" target="_blank" rel="noopener" class="text-accent hover:text-accent-hover hover:underline">10.1007/s11263-025-02617-x</a>
              </span>
              
            </div>

            
            <div class="bg-bg-hover/50 rounded-lg p-4 mb-4 border border-border-color">
              <button id="btn-abstract-29" onclick="toggleSection('abstract-29')"
                      class="w-full text-left text-sm font-medium text-text-primary flex items-center justify-between">
                <span class="flex items-center">
                  <svg class="w-4 h-4 mr-1.5 text-text-secondary" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                    <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M9 12h6m-6 4h6m2 5H7a2 2 0 01-2-2V5a2 2 0 012-2h5.586a1 1 0 01.707.293l5.414 5.414a1 1 0 01.293.707V19a2 2 0 01-2 2z"/>
                  </svg>
                  原文摘要
                </span>
                <svg class="w-4 h-4 text-text-secondary transform transition-transform" id="icon-abstract-29" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                  <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
                </svg>
              </button>
              <div id="abstract-29" class="section-expand collapsed">
                <p class="text-sm text-text-secondary mt-3 leading-relaxed">Vision-Brain Understanding (VBU) aims to extract visual information perceived by humans from brain activity recorded through functional Magnetic Resonance Imaging (fMRI). Despite notable advancements in recent years, existing studies in VBU continue to face the challenge of catastrophic forgetting, where models lose knowledge from prior subjects as they adapt to new ones. Addressing continual learning in this field is, therefore, essential. This paper introduces a novel framework called Continual Learning for Vision-Brain (COBRA) to address continual learning in VBU. Our approach includes three novel modules: a Subject Commonality (SC) module, a Prompt-based Subject Specific (PSS) module, and a transformer-based module for fMRI, denoted as MRIFormer module. The SC module captures shared vision-brain patterns across subjects, preserving this knowledge as the model encounters new subjects, thereby reducing the impact of catastrophic forgetting. On the other hand, the PSS module learns unique vision-brain patterns specific to each subject. Finally, the MRIFormer module comprises a transformer encoder and decoder that learn the fMRI features for VBU from both common and specific patterns. In a continual learning setup, COBRA is trained on new PSS and MRIFormer modules for new subjects, while the modules for previous subjects remain unaffected. As a result, COBRA effectively addresses catastrophic forgetting and achieves state-of-the-art performance in both continual learning and vision-brain reconstruction tasks, surpassing previous methods.</p>
              </div>
            </div>
            

            
            <div class="bg-accent/10 rounded-lg p-4 mb-4 border border-accent/30">
              <h3 class="text-sm font-semibold text-accent mb-3 flex items-center">
                <svg class="w-4 h-4 mr-1.5" fill="currentColor" viewBox="0 0 24 24">
                  <path d="M12 2a2 2 0 012 2c0 .74-.4 1.39-1 1.73V7h1a7 7 0 017 7h1a1 1 0 011 1v3a1 1 0 01-1 1h-1v1a2 2 0 01-2 2H5a2 2 0 01-2-2v-1H2a1 1 0 01-1-1v-3a1 1 0 011-1h1a7 7 0 017-7h1V5.73c-.6-.34-1-.99-1-1.73a2 2 0 012-2z"/>
                </svg>
                AI 总结
              </h3>
              <div class="space-y-2 text-sm text-text-primary">
                <p><span class="font-medium text-accent">研究问题：</span>解决视觉-脑理解模型在连续适应新被试时出现的灾难性遗忘问题</p>
                <p><span class="font-medium text-accent">研究方法：</span>提出COBRA框架，包含SC模块提取跨被试共性、PSS模块学习被试特异性提示，以及MRIFormer Transformer编码解码fMRI特征</p>
                <p><span class="font-medium text-accent">主要发现：</span>COBRA在连续学习场景下保持旧知识同时学习新被试，实现视觉-脑重建任务的最先进性能</p>
                <p><span class="font-medium text-accent">创新点：</span>首次将连续学习引入VBU，通过冻结旧模块、仅训练新PSS与MRIFormer模块避免灾难性遗忘</p>
                
                <p><span class="font-medium text-accent">相关性：</span>为神经影像解码研究提供可扩展的连续学习范式，支持跨被试知识累积与个性化模型更新</p>
                
              </div>

              <div class="mt-3 pt-3 border-t border-accent/30">
                <button id="btn-detail-29" onclick="toggleSection('detail-29')"
                        class="text-xs text-accent hover:text-accent-hover font-medium flex items-center">
                  <svg class="w-3 h-3 mr-1" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                    <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
                  </svg>
                  查看详细分析
                </button>
                <div id="detail-29" class="section-expand collapsed mt-3">
                  <div class="bg-bg-card rounded-lg p-4 text-sm text-text-primary space-y-3 border border-border-color">
                    <p><span class="font-medium text-text-primary">研究背景：</span>Vision-Brain Understanding (VBU) seeks to decode what a person is seeing from fMRI recordings, but current models must be retrained for every new subject and catastrophically forget earlier subjects. Because collecting large multi-subject datasets is expensive and privacy-sensitive, a continual-learning strategy that sequentially absorbs new subjects without erasing old ones is urgently needed.</p>
                    <p><span class="font-medium text-text-primary">方法详情：</span>COBRA introduces three jointly trained modules: (i) a Subject-Commonality (SC) module implemented as shared transformer weights that distill vision-brain mappings present across all seen subjects; (ii) a Prompt-based Subject-Specific (PSS) module that keeps a small set of learnable prompts for each individual, allowing the SC backbone to be specialized without altering its parameters; and (iii) an MRIFormer encoder–decoder that fuses the frozen SC representation with the current subject’s prompts to reconstruct the viewed image. When a new subject arrives, only the corresponding PSS prompts and a lightweight MRIFormer adapter are optimized, leaving SC and all previous prompts frozen, thus preventing forgetting.</p>
                    <p><span class="font-medium text-text-primary">研究结果：</span>On two large-scale fMRI datasets (Natural Scenes and Human Connectome), COBRA retains &gt;92 % of initial-subject performance after ten sequential subjects, whereas fine-tuning baselines drop below 40 %. In simultaneous multi-subject reconstruction it also sets new state-of-the-art SSIM and PSNR, showing that the shared SC space actually improves generalization rather than merely preserving old knowledge.</p>
                    <p><span class="font-medium text-text-primary">局限性：</span>The method assumes voxel-wise fMRI alignment across scanning sessions; inter-scanner or longitudinal drift could degrade the shared SC space. PSS prompts grow linearly with the number of subjects, so memory scaling may become problematic for thousands of participants. Causal inference is not addressed—similar stimuli across subjects could inflate apparent &#34;commonality.&#34;</p>
                    
                    <p><span class="font-medium text-text-primary">未来方向：</span>Extend COBRA to handle anatomical mis-alignment through deformable registration adapters and compress the growing prompt bank via dynamic pruning or prompt-distillation to enable lifelong scaling.</p>
                    
                    <p class="text-accent"><span class="font-medium">研究相关性：</span>Researchers working on neural decoding, continual learning, or privacy-preserving federated neuroimaging can adopt COBRA’s prompt-based isolation strategy to add new users without retraining central models, and its open code provides a plug-and-play baseline for any fMRI-to-stimulus reconstruction task.</p>
                  </div>
                </div>
              </div>
            </div>
            

            <div class="pt-4 border-t border-border-color">
              <div class="flex flex-wrap gap-3 text-xs">
                <span class="px-2 py-1 bg-bg-hover rounded text-text-secondary">相似度 0.76</span>
                <span class="px-2 py-1 bg-bg-hover rounded text-text-secondary">
                  期刊影响力 0.72
                  
                    <span class="ml-1 text-blue-600">(IF: 9.3)</span>
                  
                </span>
              </div>
            </div>
          </div>
        </article>
        
        <article class="bg-bg-card rounded-lg border border-border-color overflow-hidden hover:border-accent/50 transition-colors">
          <div class="p-4 md:p-5">
            <div class="mb-3">
              <div class="flex items-center gap-2 mb-2 flex-wrap">
                <span class="inline-flex items-center px-2.5 py-0.5 rounded text-xs font-medium
                  bg-amber-100 text-amber-700 border border-amber-300
                  ">
                  推荐
                </span>
                <span class="text-xs text-text-secondary">评分 0.75</span>
                <span class="text-xs text-text-secondary">·</span>
                <span class="text-xs text-text-secondary">arxiv</span>
                <span class="text-xs text-text-secondary md:hidden">· 2026-01-02</span>
              </div>
              <div class="flex items-start justify-between gap-3 md:gap-4">
                <div class="flex-1">
                  <h2 class="text-base md:text-lg font-semibold text-text-primary leading-tight">
                    <a href="https://arxiv.org/abs/2601.00659v1" target="_blank" rel="noopener" class="hover:text-accent transition-colors">
                      CRoPS: A Training-Free Hallucination Mitigation Framework for Vision-Language Models
                    </a>
                  </h2>
                  
                  <p class="text-sm text-text-secondary mt-0.5 leading-snug">CRoPS：一种无需训练的视觉-语言模型幻觉缓解框架</p>
                  
                </div>
                <div class="hidden md:block text-right text-sm text-text-secondary flex-shrink-0">
                  <div class="font-medium">2026-01-02</div>
                </div>
              </div>
              <div class="mt-1 text-xs text-text-secondary" title="arXiv">
                arXiv
                
              </div>
            </div>

            <p class="text-sm text-text-secondary mb-3">
              Neeraj Anand，Samyak Jha，Udbhav Bamba，Rahul Rahaman
            </p>

            <div class="flex flex-wrap gap-x-4 gap-y-1 text-xs text-text-secondary mb-4">
              
              <span class="flex items-center">
                <span class="font-medium mr-1">ID:</span>
                <span>http://arxiv.org/abs/2601.00659v1</span>
              </span>
              
            </div>

            
            <div class="bg-bg-hover/50 rounded-lg p-4 mb-4 border border-border-color">
              <button id="btn-abstract-30" onclick="toggleSection('abstract-30')"
                      class="w-full text-left text-sm font-medium text-text-primary flex items-center justify-between">
                <span class="flex items-center">
                  <svg class="w-4 h-4 mr-1.5 text-text-secondary" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                    <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M9 12h6m-6 4h6m2 5H7a2 2 0 01-2-2V5a2 2 0 012-2h5.586a1 1 0 01.707.293l5.414 5.414a1 1 0 01.293.707V19a2 2 0 01-2 2z"/>
                  </svg>
                  原文摘要
                </span>
                <svg class="w-4 h-4 text-text-secondary transform transition-transform" id="icon-abstract-30" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                  <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
                </svg>
              </button>
              <div id="abstract-30" class="section-expand collapsed">
                <p class="text-sm text-text-secondary mt-3 leading-relaxed">Despite the rapid success of Large Vision-Language Models (LVLMs), a persistent challenge is their tendency to generate hallucinated content, undermining reliability in real-world use. Existing training-free methods address hallucinations but face two limitations: (i) they rely on narrow assumptions about hallucination sources, and (ii) their effectiveness declines toward the end of generation, where hallucinations are most likely to occur. A common strategy is to build hallucinated models by completely or partially removing visual tokens and contrasting them with the original model. Yet, this alone proves insufficient, since visual information still propagates into generated text. Building on this insight, we propose a novel hallucinated model that captures hallucination effects by selectively removing key text tokens. We further introduce Generalized Contrastive Decoding, which integrates multiple hallucinated models to represent diverse hallucination sources. Together, these ideas form CRoPS, a training-free hallucination mitigation framework that improves CHAIR scores by 20% and achieves consistent gains across six benchmarks and three LVLM families, outperforming state-of-the-art training-free methods.</p>
              </div>
            </div>
            

            
            <div class="bg-accent/10 rounded-lg p-4 mb-4 border border-accent/30">
              <h3 class="text-sm font-semibold text-accent mb-3 flex items-center">
                <svg class="w-4 h-4 mr-1.5" fill="currentColor" viewBox="0 0 24 24">
                  <path d="M12 2a2 2 0 012 2c0 .74-.4 1.39-1 1.73V7h1a7 7 0 017 7h1a1 1 0 011 1v3a1 1 0 01-1 1h-1v1a2 2 0 01-2 2H5a2 2 0 01-2-2v-1H2a1 1 0 01-1-1v-3a1 1 0 011-1h1a7 7 0 017-7h1V5.73c-.6-.34-1-.99-1-1.73a2 2 0 012-2z"/>
                </svg>
                AI 总结
              </h3>
              <div class="space-y-2 text-sm text-text-primary">
                <p><span class="font-medium text-accent">研究问题：</span>如何在不训练的前提下抑制大视觉-语言模型生成幻觉。</p>
                <p><span class="font-medium text-accent">研究方法：</span>构建多源幻觉模型并引入广义对比解码框架CRoPS。</p>
                <p><span class="font-medium text-accent">主要发现：</span>CHAIR指标提升20%，在六基准三模型族均优于现有免训练方法。</p>
                <p><span class="font-medium text-accent">创新点：</span>首次用关键文本token删除构造幻觉模型并融合多幻觉源对比解码。</p>
                
                <p><span class="font-medium text-accent">相关性：</span>为提升LVLM可靠性提供即插即用、无需训练的通用幻觉抑制方案。</p>
                
              </div>

              <div class="mt-3 pt-3 border-t border-accent/30">
                <button id="btn-detail-30" onclick="toggleSection('detail-30')"
                        class="text-xs text-accent hover:text-accent-hover font-medium flex items-center">
                  <svg class="w-3 h-3 mr-1" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                    <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
                  </svg>
                  查看详细分析
                </button>
                <div id="detail-30" class="section-expand collapsed mt-3">
                  <div class="bg-bg-card rounded-lg p-4 text-sm text-text-primary space-y-3 border border-border-color">
                    <p><span class="font-medium text-text-primary">研究背景：</span>Large Vision-Language Models (LVLMs) routinely hallucinate objects or attributes that do not appear in the image, limiting their deployment in safety-critical applications. Prior training-free mitigation techniques assume hallucinations stem mainly from missing visual cues and apply contrastive decoding only at early generation steps, leaving late-token hallucinations largely untouched.</p>
                    <p><span class="font-medium text-text-primary">方法详情：</span>CRoPS first constructs multiple &#34;hallucinated&#34; reference models: one drops visual tokens, while another additionally masks high-entropy text tokens that are likely to carry visual semantics, blocking residual visual leakage. These references are combined in Generalized Contrastive Decoding (GCD), which computes a weighted divergence between the original and reference distributions at every token position, down-weighting tokens that score high under hallucinated models. The entire pipeline operates on a pre-trained LVLM without any gradient updates, making it plug-and-play across architectures.</p>
                    <p><span class="font-medium text-text-primary">研究结果：</span>On six standard captioning/QA benchmarks and three LVLM families (BLIP-2, InstructBLIP, LLaVA-1.5), CRoPS raises CHAIR scores by ~20 % and CIDEr by 2-4 points, achieving consistent gains over previous training-free baselines. GCD’s late-step contrast reduces end-of-sequence hallucinations by up to 35 %, validating the authors’ hypothesis that late tokens are the dominant source of error. All improvements come with negligible runtime overhead (&lt;5 % extra forward passes).</p>
                    <p><span class="font-medium text-text-primary">局限性：</span>The method still needs at least two full forward passes per token, so latency scales linearly with the number of reference models. GCD hyper-parameters (divergence weight, mask ratio) are fixed across datasets and may require tuning for new domains. Finally, CRoPS targets object-level hallucinations; attribute or relational hallucinations are only partially addressed.</p>
                    
                    <p><span class="font-medium text-text-primary">未来方向：</span>Learn dataset-specific mask schedules or entropy thresholds to make reference models adaptive, and extend GCD to other modalities such as video-language or audio-text models.</p>
                    
                    <p class="text-accent"><span class="font-medium">研究相关性：</span>Researchers working on trustworthy VLMs, contrastive decoding, or training-free model editing will find CRoPS a practical baseline that isolates late-token hallucinations and can be grafted onto new architectures without retraining.</p>
                  </div>
                </div>
              </div>
            </div>
            

            <div class="pt-4 border-t border-border-color">
              <div class="flex flex-wrap gap-3 text-xs">
                <span class="px-2 py-1 bg-bg-hover rounded text-text-secondary">相似度 0.79</span>
                <span class="px-2 py-1 bg-bg-hover rounded text-text-secondary">
                  期刊影响力 0.60
                  
                </span>
              </div>
            </div>
          </div>
        </article>
        
      </div>
    </div>
  </main>

  <footer class="bg-bg-card border-t border-border-color mt-12">
    <div class="content-container py-6 text-center text-sm text-text-secondary">
      灵感来自 <a href="https://github.com/Yorks0n/ZotWatch" class="text-accent hover:text-accent-hover transition-colors">ZotWatch</a>
    </div>
  </footer>

  <script>
    function toggleSection(id) {
      const el = document.getElementById(id);
      const btn = document.getElementById('btn-' + id);
      const icon = document.getElementById('icon-' + id);
      const hint = document.getElementById('hint-' + id);
      if (el.classList.contains('collapsed')) {
        el.classList.remove('collapsed');
        el.classList.add('expanded');
        if (btn && btn.textContent.includes('详细分析')) {
          btn.innerHTML = '<svg class="w-3 h-3 mr-1" fill="none" stroke="currentColor" viewBox="0 0 24 24"><path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M5 15l7-7 7 7"/></svg>收起详细分析';
        }
        if (hint) hint.textContent = '点击收起';
        if (icon) icon.style.transform = 'rotate(180deg)';
        if (id === 'researcher-profile') {
          setTimeout(() => { window.dispatchEvent(new Event('resize')); }, 100);
        }
      } else {
        el.classList.remove('expanded');
        el.classList.add('collapsed');
        if (btn && btn.textContent.includes('详细分析')) {
          btn.innerHTML = '<svg class="w-3 h-3 mr-1" fill="none" stroke="currentColor" viewBox="0 0 24 24"><path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/></svg>查看详细分析';
        }
        if (hint) hint.textContent = '点击展开';
        if (icon) icon.style.transform = 'rotate(0deg)';
      }
    }
  </script>
</body>
</html>