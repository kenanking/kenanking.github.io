<!DOCTYPE html>
<html lang="zh-CN">
<head>
  <meta charset="utf-8">
  <meta name="viewport" content="width=device-width, initial-scale=1">
  <title>ZotWatch 文献推荐 - 2025-12-07</title>
  <script src="https://cdn.tailwindcss.com"></script>
  <script src="https://cdn.jsdelivr.net/npm/chart.js@4.4.1/dist/chart.umd.min.js"></script>
  <script>
    tailwind.config = {
      theme: {
        extend: {
          colors: {
            'bg-primary': '#f8fafc',
            'bg-card': '#ffffff',
            'bg-hover': '#f1f5f9',
            'text-primary': '#1e293b',
            'text-secondary': '#64748b',
            'border-color': '#e2e8f0',
            'accent': '#2563eb',
            'accent-hover': '#1d4ed8',
          }
        }
      }
    }
  </script>
  <style>
    .section-expand { transition: max-height 0.5s ease-out; overflow: hidden; }
    .section-expand.collapsed { max-height: 0; }
    .section-expand.expanded { max-height: none; }
    body { font-family: -apple-system, BlinkMacSystemFont, 'Segoe UI', Roboto, 'Helvetica Neue', Arial, sans-serif; }
    .content-container { max-width: 48rem; margin: 0 auto; padding: 0 1rem; }
    @media (min-width: 768px) { .content-container { padding: 0 2rem; } }
    @media (min-width: 1024px) { .content-container { padding: 0 4rem; } }
  </style>
</head>
<body class="bg-bg-primary min-h-screen text-text-primary">
  <!-- Header -->
  <header class="bg-bg-card border-b border-border-color">
    <div class="content-container py-6 md:py-8">
      <h1 class="text-xl md:text-2xl font-bold text-text-primary">ZotWatch 文献推荐</h1>
      <p class="text-xs md:text-sm text-text-secondary mt-1">
        共 35 篇论文 ·
        生成于 2025-12-07 10:48 Asia/Shanghai
      </p>
    </div>
  </header>

  <!-- Researcher Profile Section (Collapsible) -->
  
  <section class="py-4 md:py-6 border-b border-border-color">
    <div class="content-container">
      <!-- Collapsible Header -->
      <button id="btn-researcher-profile" onclick="toggleSection('researcher-profile')"
              class="w-full text-left text-lg md:text-xl font-bold text-text-primary mb-3 md:mb-4 flex items-center justify-between group hover:text-accent transition-colors">
        <span class="flex items-center gap-2 flex-1 min-w-0">
          <svg class="w-5 h-5 flex-shrink-0 text-accent" fill="currentColor" viewBox="0 0 24 24">
            <path d="M12 2C6.48 2 2 6.48 2 12s4.48 10 10 10 10-4.48 10-10S17.52 2 12 2zm0 3c1.66 0 3 1.34 3 3s-1.34 3-3 3-3-1.34-3-3 1.34-3 3-3zm0 14.2c-2.5 0-4.71-1.28-6-3.22.03-1.99 4-3.08 6-3.08 1.99 0 5.97 1.09 6 3.08-1.29 1.94-3.5 3.22-6 3.22z"/>
          </svg>
          <span class="truncate">研究兴趣画像</span>
        </span>
        <span class="flex items-center gap-1 md:gap-2 flex-shrink-0 ml-2">
          <span class="hidden md:inline text-sm font-normal text-text-secondary group-hover:text-accent" id="hint-researcher-profile">点击展开</span>
          <svg class="w-5 h-5 md:w-4 md:h-4 text-text-secondary group-hover:text-accent transform transition-transform" id="icon-researcher-profile" fill="none" stroke="currentColor" viewBox="0 0 24 24">
            <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
          </svg>
        </span>
      </button>

      <!-- Collapsible Content -->
      <div id="researcher-profile" class="section-expand collapsed">
      <!-- Basic Stats - Compact Row -->
      <div class="bg-bg-card rounded-lg border border-border-color p-4 mb-4">
        <div class="grid grid-cols-4 gap-3 text-center">
          <div>
            <div class="text-xl font-bold text-accent">2696</div>
            <div class="text-xs text-text-secondary">收藏论文</div>
          </div>
          <div>
            <div class="text-xl font-bold text-accent">5年8月</div>
            <div class="text-xs text-text-secondary">收藏时长</div>
          </div>
          <div>
            <div class="text-xl font-bold text-accent">&gt;10</div>
            <div class="text-xs text-text-secondary">兴趣领域</div>
          </div>
          <div>
            <div class="text-xl font-bold text-accent">7</div>
            <div class="text-xs text-text-secondary">高频作者</div>
          </div>
        </div>
      </div>

      <!-- LLM Insights - Compact -->
      
      <div class="bg-accent/10 rounded-lg border border-accent/30 p-4 mb-4">
        <h3 class="text-sm font-semibold text-accent mb-2 flex items-center gap-2">
          <svg class="w-4 h-4" fill="currentColor" viewBox="0 0 24 24">
            <path d="M12 2a2 2 0 012 2c0 .74-.4 1.39-1 1.73V7h1a7 7 0 017 7h1a1 1 0 011 1v3a1 1 0 01-1 1h-1v1a2 2 0 01-2 2H5a2 2 0 01-2-2v-1H2a1 1 0 01-1-1v-3a1 1 0 011-1h1a7 7 0 017-7h1V5.73c-.6-.34-1-.99-1-1.73a2 2 0 012-2z"/>
          </svg>
          AI 研究兴趣分析
        </h3>
        <div class="space-y-2 text-xs text-text-primary leading-relaxed">
          <p><span class="font-medium text-accent">研究兴趣：</span>收藏记录显示，用户长期关注计算机视觉与遥感交叉方向，核心阅读集中在目标检测、SAR图像理解及高效模型设计，同时对大模型与自监督学习保持同步追踪。</p>
          <p><span class="font-medium text-accent">深度关注：</span>在CVPR、NeurIPS、IEEE TGRS等顶会顶刊持续收藏逾300篇文献，尤其对Kaiming He、Ross Girshick等团队的检测与分割工作形成系统积累；SAR目标识别与旋转目标检测关键词反复出现，表明其在遥感智能解译方向有深厚阅读深度。</p>
          <p><span class="font-medium text-accent">跨学科倾向：</span>用户阅读横跨计算机视觉、遥感、机器学习三大领域，将通用视觉方法（Transformer、模型压缩）主动迁移至SAR/遥感数据，体现出明显的“CV方法+遥感应用”交叉特征。</p>
          <p><span class="font-medium text-accent">兴趣演变：</span>2025-Q1收藏量激增至81篇，新增关键词聚焦“视觉Transformer、大语言模型、基础模型”，显示兴趣正从传统检测/分割向大模型与多模态遥感快速转移；2024-Q3后收藏量回落但每季度仍保持≥10篇，表明进入精选阅读阶段。</p>
          <p><span class="font-medium text-accent">阅读建议：</span>建议进一步关注多模态遥感基础模型（RS-LLM、Vision-Language-Radar）、轻量化Transformer在轨实时处理，以及基于扩散模型的SAR数据增强与仿真生成方向。</p>
        </div>
      </div>
      

      <!-- Charts Row: Domains + Trends -->
      <div class="grid grid-cols-1 md:grid-cols-2 gap-4 mb-4">
        <!-- Research Domains Chart -->
        
        <div class="bg-bg-card rounded-lg border border-border-color p-4">
          <h4 class="text-sm font-medium text-text-primary mb-3">研究领域分布</h4>
          <div style="height: 180px;">
            <canvas id="domainsChart"></canvas>
          </div>
        </div>
        

        <!-- Quarterly Trends Chart -->
        
        <div class="bg-bg-card rounded-lg border border-border-color p-4">
          <h4 class="text-sm font-medium text-text-primary mb-3">阅读趋势（季度）</h4>
          <div style="height: 180px;">
            <canvas id="trendsChart"></canvas>
          </div>
        </div>
        
      </div>

      <!-- Year Distribution Chart (full width) -->
      
      <div class="bg-bg-card rounded-lg border border-border-color p-4 mb-4">
        <h4 class="text-sm font-medium text-text-primary mb-3">论文发表年份分布</h4>
        <div style="height: 180px;">
          <canvas id="yearDistChart"></canvas>
        </div>
      </div>
      

      <!-- Authors and Venues Row -->
      <div class="grid grid-cols-1 md:grid-cols-2 gap-4 mb-4">
        <!-- Top Authors -->
        
        <div class="bg-bg-card rounded-lg border border-border-color p-4">
          <h4 class="text-sm font-medium text-text-primary mb-2">高频关注作者</h4>
          <div class="grid grid-cols-2 gap-x-4 gap-y-1 text-xs">
            
            <div class="flex justify-between truncate">
              <span class="text-text-primary truncate" title="Kaiming He">Kaiming He</span>
              <span class="text-text-secondary ml-1 flex-shrink-0">24</span>
            </div>
            
            <div class="flex justify-between truncate">
              <span class="text-text-primary truncate" title="Ross Girshick">Ross Girshick</span>
              <span class="text-text-secondary ml-1 flex-shrink-0">18</span>
            </div>
            
            <div class="flex justify-between truncate">
              <span class="text-text-primary truncate" title="Jian Sun">Jian Sun</span>
              <span class="text-text-secondary ml-1 flex-shrink-0">15</span>
            </div>
            
            <div class="flex justify-between truncate">
              <span class="text-text-primary truncate" title="Song Han">Song Han</span>
              <span class="text-text-secondary ml-1 flex-shrink-0">14</span>
            </div>
            
            <div class="flex justify-between truncate">
              <span class="text-text-primary truncate" title="Xiangyu Zhang">Xiangyu Zhang</span>
              <span class="text-text-secondary ml-1 flex-shrink-0">14</span>
            </div>
            
            <div class="flex justify-between truncate">
              <span class="text-text-primary truncate" title="Piotr Dollár">Piotr Dollár</span>
              <span class="text-text-secondary ml-1 flex-shrink-0">12</span>
            </div>
            
            <div class="flex justify-between truncate">
              <span class="text-text-primary truncate" title="Xiang Li">Xiang Li</span>
              <span class="text-text-secondary ml-1 flex-shrink-0">10</span>
            </div>
            
            <div class="flex justify-between truncate">
              <span class="text-text-primary truncate" title="Li Liu">Li Liu</span>
              <span class="text-text-secondary ml-1 flex-shrink-0">9</span>
            </div>
            
            <div class="flex justify-between truncate">
              <span class="text-text-primary truncate" title="Chunhua Shen">Chunhua Shen</span>
              <span class="text-text-secondary ml-1 flex-shrink-0">8</span>
            </div>
            
            <div class="flex justify-between truncate">
              <span class="text-text-primary truncate" title="Yann LeCun">Yann LeCun</span>
              <span class="text-text-secondary ml-1 flex-shrink-0">8</span>
            </div>
            
            <div class="flex justify-between truncate">
              <span class="text-text-primary truncate" title="Gangyao Kuang">Gangyao Kuang</span>
              <span class="text-text-secondary ml-1 flex-shrink-0">8</span>
            </div>
            
            <div class="flex justify-between truncate">
              <span class="text-text-primary truncate" title="Wenfeng Liang">Wenfeng Liang</span>
              <span class="text-text-secondary ml-1 flex-shrink-0">8</span>
            </div>
            
          </div>
        </div>
        

        <!-- Top Venues -->
        
        <div class="bg-bg-card rounded-lg border border-border-color p-4">
          <h4 class="text-sm font-medium text-text-primary mb-2">常关注期刊/会议</h4>
          <div class="space-y-1 text-xs">
            
            <div class="flex items-center justify-between">
              <span class="text-text-primary truncate flex-1" title="Computer Vision and Pattern Recognition">Computer Vision and Pattern Recognition</span>
              <span class="text-xs px-1 py-0.5 rounded bg-blue-100 text-blue-700 ml-1 flex-shrink-0">
                期刊
              </span>
              <span class="text-text-secondary ml-1 flex-shrink-0">111</span>
            </div>
            
            <div class="flex items-center justify-between">
              <span class="text-text-primary truncate flex-1" title="Neural Information Processing Systems">Neural Information Processing Systems</span>
              <span class="text-xs px-1 py-0.5 rounded bg-blue-100 text-blue-700 ml-1 flex-shrink-0">
                期刊
              </span>
              <span class="text-text-secondary ml-1 flex-shrink-0">45</span>
            </div>
            
            <div class="flex items-center justify-between">
              <span class="text-text-primary truncate flex-1" title="IEEE Transactions on Geoscience and Remote Sensing">IEEE Transactions on Geoscience and Remote Sensing</span>
              <span class="text-xs px-1 py-0.5 rounded bg-blue-100 text-blue-700 ml-1 flex-shrink-0">
                期刊
              </span>
              <span class="text-text-secondary ml-1 flex-shrink-0">41</span>
            </div>
            
            <div class="flex items-center justify-between">
              <span class="text-text-primary truncate flex-1" title="International Conference on Computer Vision">International Conference on Computer Vision</span>
              <span class="text-xs px-1 py-0.5 rounded bg-amber-100 text-amber-700 ml-1 flex-shrink-0">
                会议
              </span>
              <span class="text-text-secondary ml-1 flex-shrink-0">36</span>
            </div>
            
            <div class="flex items-center justify-between">
              <span class="text-text-primary truncate flex-1" title="IEEE Transactions on Pattern Analysis and Machine Intelligence">IEEE Transactions on Pattern Analysis and Machine Intelligence</span>
              <span class="text-xs px-1 py-0.5 rounded bg-blue-100 text-blue-700 ml-1 flex-shrink-0">
                期刊
              </span>
              <span class="text-text-secondary ml-1 flex-shrink-0">31</span>
            </div>
            
            <div class="flex items-center justify-between">
              <span class="text-text-primary truncate flex-1" title="International Conference on Learning Representations">International Conference on Learning Representations</span>
              <span class="text-xs px-1 py-0.5 rounded bg-amber-100 text-amber-700 ml-1 flex-shrink-0">
                会议
              </span>
              <span class="text-text-secondary ml-1 flex-shrink-0">27</span>
            </div>
            
            <div class="flex items-center justify-between">
              <span class="text-text-primary truncate flex-1" title="European Conference on Computer Vision">European Conference on Computer Vision</span>
              <span class="text-xs px-1 py-0.5 rounded bg-amber-100 text-amber-700 ml-1 flex-shrink-0">
                会议
              </span>
              <span class="text-text-secondary ml-1 flex-shrink-0">23</span>
            </div>
            
            <div class="flex items-center justify-between">
              <span class="text-text-primary truncate flex-1" title="雷达学报">雷达学报</span>
              <span class="text-xs px-1 py-0.5 rounded bg-blue-100 text-blue-700 ml-1 flex-shrink-0">
                期刊
              </span>
              <span class="text-text-secondary ml-1 flex-shrink-0">21</span>
            </div>
            
          </div>
        </div>
        
      </div>

      <!-- Keywords -->
      
      <div class="bg-bg-card rounded-lg border border-border-color p-4 mb-3">
        <h4 class="text-sm font-medium text-text-primary mb-2">高频关键词</h4>
        <div class="flex flex-wrap gap-1.5">
          
          <span class="px-2 py-0.5 bg-bg-hover rounded text-xs text-text-primary">
            大语言模型 <span class="text-text-secondary">(12)</span>
          </span>
          
          <span class="px-2 py-0.5 bg-bg-hover rounded text-xs text-text-primary">
            综述 <span class="text-text-secondary">(9)</span>
          </span>
          
          <span class="px-2 py-0.5 bg-bg-hover rounded text-xs text-text-primary">
            自监督学习 <span class="text-text-secondary">(9)</span>
          </span>
          
          <span class="px-2 py-0.5 bg-bg-hover rounded text-xs text-text-primary">
            域自适应 <span class="text-text-secondary">(9)</span>
          </span>
          
          <span class="px-2 py-0.5 bg-bg-hover rounded text-xs text-text-primary">
            迁移学习 <span class="text-text-secondary">(7)</span>
          </span>
          
          <span class="px-2 py-0.5 bg-bg-hover rounded text-xs text-text-primary">
            Transformers <span class="text-text-secondary">(6)</span>
          </span>
          
          <span class="px-2 py-0.5 bg-bg-hover rounded text-xs text-text-primary">
            重参数化 <span class="text-text-secondary">(6)</span>
          </span>
          
          <span class="px-2 py-0.5 bg-bg-hover rounded text-xs text-text-primary">
            扩散模型 <span class="text-text-secondary">(6)</span>
          </span>
          
          <span class="px-2 py-0.5 bg-bg-hover rounded text-xs text-text-primary">
            强化学习 <span class="text-text-secondary">(6)</span>
          </span>
          
          <span class="px-2 py-0.5 bg-bg-hover rounded text-xs text-text-primary">
            DeepSeek <span class="text-text-secondary">(6)</span>
          </span>
          
          <span class="px-2 py-0.5 bg-bg-hover rounded text-xs text-text-primary">
            合成孔径雷达 <span class="text-text-secondary">(6)</span>
          </span>
          
          <span class="px-2 py-0.5 bg-bg-hover rounded text-xs text-text-primary">
            旋转目标检测 <span class="text-text-secondary">(5)</span>
          </span>
          
          <span class="px-2 py-0.5 bg-bg-hover rounded text-xs text-text-primary">
            SAR目标识别 <span class="text-text-secondary">(5)</span>
          </span>
          
          <span class="px-2 py-0.5 bg-bg-hover rounded text-xs text-text-primary">
            特征可视化 <span class="text-text-secondary">(5)</span>
          </span>
          
          <span class="px-2 py-0.5 bg-bg-hover rounded text-xs text-text-primary">
            生成对抗网络 <span class="text-text-secondary">(5)</span>
          </span>
          
          <span class="px-2 py-0.5 bg-bg-hover rounded text-xs text-text-primary">
            Training <span class="text-text-secondary">(5)</span>
          </span>
          
          <span class="px-2 py-0.5 bg-bg-hover rounded text-xs text-text-primary">
            Vision Transformers <span class="text-text-secondary">(4)</span>
          </span>
          
          <span class="px-2 py-0.5 bg-bg-hover rounded text-xs text-text-primary">
            基础模型 <span class="text-text-secondary">(4)</span>
          </span>
          
          <span class="px-2 py-0.5 bg-bg-hover rounded text-xs text-text-primary">
            注意力机制 <span class="text-text-secondary">(4)</span>
          </span>
          
          <span class="px-2 py-0.5 bg-bg-hover rounded text-xs text-text-primary">
            HRNet <span class="text-text-secondary">(4)</span>
          </span>
          
        </div>
      </div>
      

      <!-- Generation info -->
      <div class="text-xs text-text-secondary text-right">
        画像生成于 2025-12-06 10:17 Asia/Shanghai
        · kimi-k2-turbo-preview
      </div>
      </div><!-- End of collapsible content -->
    </div>
  </section>
  

  <!-- Chart.js Initialization for Researcher Profile -->
  
  <script>
    document.addEventListener('DOMContentLoaded', function() {
      // Chart.js global defaults for academic style
      Chart.defaults.font.family = "-apple-system, BlinkMacSystemFont, 'Segoe UI', Roboto, 'Helvetica Neue', Arial, sans-serif";
      Chart.defaults.font.size = 11;
      Chart.defaults.color = '#64748b';
      Chart.defaults.animation = false;

      
      // Domains Horizontal Bar Chart
      const domainsCtx = document.getElementById('domainsChart');
      if (domainsCtx) {
        new Chart(domainsCtx, {
          type: 'bar',
          data: {
            labels: ['视觉SLAM', '姿态估计', '卫星导航', '模型压缩', 'Transformer', '概率图模型', '车牌识别', '非线性优化'],
            datasets: [{
              data: [18, 22, 11, 15, 10, 4, 6, 6],
              backgroundColor: 'rgba(37, 99, 235, 0.7)',
              borderColor: 'rgba(37, 99, 235, 1)',
              borderWidth: 1,
              borderRadius: 2,
              barThickness: 16
            }]
          },
          options: {
            indexAxis: 'y',
            responsive: true,
            maintainAspectRatio: false,
            plugins: {
              legend: { display: false },
              tooltip: { enabled: false }
            },
            scales: {
              x: {
                beginAtZero: true,
                grid: { color: 'rgba(226, 232, 240, 0.5)' },
                ticks: { precision: 0 }
              },
              y: {
                grid: { display: false },
                ticks: { font: { size: 10 } }
              }
            }
          }
        });
      }
      

      
      // Quarterly Trends Bar Chart with 3 cycling colors (one per month in quarter)
      const trendsCtx = document.getElementById('trendsChart');
      if (trendsCtx) {
        const trends = [{ q: '2023-Q1', c: 50 }, { q: '2023-Q2', c: 18 }, { q: '2023-Q3', c: 21 }, { q: '2023-Q4', c: 19 }, { q: '2024-Q1', c: 66 }, { q: '2024-Q2', c: 18 }, { q: '2024-Q3', c: 4 }, { q: '2024-Q4', c: 23 }, { q: '2025-Q1', c: 81 }, { q: '2025-Q2', c: 34 }, { q: '2025-Q3', c: 10 }, { q: '2025-Q4', c: 23 }];
        // Color palette: 3 colors cycling for months within quarters
        const monthColors = [
          { bg: 'rgba(59, 130, 246, 0.7)', border: 'rgba(59, 130, 246, 1)' },   // Blue
          { bg: 'rgba(16, 185, 129, 0.7)', border: 'rgba(16, 185, 129, 1)' },   // Green
          { bg: 'rgba(245, 158, 11, 0.7)', border: 'rgba(245, 158, 11, 1)' }    // Amber
        ];
        const bgColors = trends.map((t, i) => monthColors[i % 3].bg);
        const borderColors = trends.map((t, i) => monthColors[i % 3].border);
        new Chart(trendsCtx, {
          type: 'bar',
          data: {
            labels: trends.map(t => t.q.replace('-', '\n')),
            datasets: [{
              data: trends.map(t => t.c),
              backgroundColor: bgColors,
              borderColor: borderColors,
              borderWidth: 1,
              borderRadius: 2,
              barThickness: 20
            }]
          },
          options: {
            responsive: true,
            maintainAspectRatio: false,
            plugins: {
              legend: { display: false },
              tooltip: { enabled: false }
            },
            scales: {
              x: {
                grid: { display: false },
                ticks: { font: { size: 9 }, maxRotation: 0 }
              },
              y: {
                beginAtZero: true,
                grid: { color: 'rgba(226, 232, 240, 0.5)' },
                ticks: { precision: 0 }
              }
            }
          }
        });
      }
      

      
      // Year Distribution Line Chart
      const yearDistCtx = document.getElementById('yearDistChart');
      if (yearDistCtx) {
        const yearData = [{ year: 2006, count: 8 }, { year: 2007, count: 3 }, { year: 2008, count: 2 }, { year: 2009, count: 2 }, { year: 2010, count: 9 }, { year: 2011, count: 10 }, { year: 2012, count: 3 }, { year: 2013, count: 12 }, { year: 2014, count: 14 }, { year: 2015, count: 18 }, { year: 2016, count: 15 }, { year: 2017, count: 40 }, { year: 2018, count: 58 }, { year: 2019, count: 53 }, { year: 2020, count: 66 }, { year: 2021, count: 84 }, { year: 2022, count: 110 }, { year: 2023, count: 108 }, { year: 2024, count: 111 }, { year: 2025, count: 148 }];
        new Chart(yearDistCtx, {
          type: 'line',
          data: {
            labels: yearData.map(d => d.year),
            datasets: [{
              data: yearData.map(d => d.count),
              borderColor: 'rgba(37, 99, 235, 1)',
              backgroundColor: 'rgba(37, 99, 235, 0.1)',
              borderWidth: 2,
              fill: true,
              tension: 0.3,
              pointRadius: 2,
              pointHoverRadius: 4
            }]
          },
          options: {
            responsive: true,
            maintainAspectRatio: false,
            plugins: {
              legend: { display: false },
              tooltip: {
                callbacks: {
                  label: function(context) {
                    return context.parsed.y + ' 篇';
                  }
                }
              }
            },
            scales: {
              x: {
                grid: { display: false },
                ticks: {
                  font: { size: 9 },
                  maxRotation: 45,
                  callback: function(value, index) {
                    // Show every 5th year label to avoid crowding
                    const year = this.getLabelForValue(value);
                    return year % 5 === 0 ? year : '';
                  }
                }
              },
              y: {
                beginAtZero: true,
                grid: { color: 'rgba(226, 232, 240, 0.5)' },
                ticks: { precision: 0 }
              }
            }
          }
        });
      }
      
    });
  </script>
  

  <!-- Overall Summaries Section -->
  
  <section class="py-5 md:py-8 border-b border-border-color">
    <div class="content-container">
      <!-- Collapsible Header -->
      <button id="btn-overall-summaries" onclick="toggleSection('overall-summaries')"
              class="w-full text-left text-lg md:text-xl font-bold text-text-primary mb-3 md:mb-4 flex items-center justify-between group hover:text-accent transition-colors">
        <span class="flex items-center gap-2 flex-1 min-w-0">
          <svg class="w-5 h-5 flex-shrink-0 text-accent" fill="currentColor" viewBox="0 0 24 24">
            <path d="M12 2a2 2 0 012 2c0 .74-.4 1.39-1 1.73V7h1a7 7 0 017 7h1a1 1 0 011 1v3a1 1 0 01-1 1h-1v1a2 2 0 01-2 2H5a2 2 0 01-2-2v-1H2a1 1 0 01-1-1v-3a1 1 0 011-1h1a7 7 0 017-7h1V5.73c-.6-.34-1-.99-1-1.73a2 2 0 012-2z"/>
          </svg>
          <span class="truncate">本期研究趋势概览</span>
        </span>
        <span class="flex items-center gap-1 md:gap-2 flex-shrink-0 ml-2">
          <span class="hidden md:inline text-sm font-normal text-text-secondary group-hover:text-accent" id="hint-overall-summaries">点击收起</span>
          <svg class="w-5 h-5 md:w-4 md:h-4 text-text-secondary group-hover:text-accent transform transition-transform rotate-180" id="icon-overall-summaries" fill="none" stroke="currentColor" viewBox="0 0 24 24">
            <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
          </svg>
        </span>
      </button>

      <!-- Collapsible Content -->
      <div id="overall-summaries" class="section-expand expanded">
      <div class="space-y-4 md:space-y-6">
        
        <div class="bg-bg-card rounded-lg border border-border-color p-4 md:p-5">
          <h3 class="text-base font-semibold text-text-primary mb-3 flex items-center gap-2">
            <svg class="w-4 h-4 text-accent" fill="currentColor" viewBox="0 0 20 20">
              <path d="M9.049 2.927c.3-.921 1.603-.921 1.902 0l1.07 3.292a1 1 0 00.95.69h3.462c.969 0 1.371 1.24.588 1.81l-2.8 2.034a1 1 0 00-.364 1.118l1.07 3.292c.3.921-.755 1.688-1.54 1.118l-2.8-2.034a1 1 0 00-1.175 0l-2.8 2.034c-.784.57-1.838-.197-1.539-1.118l1.07-3.292a1 1 0 00-.364-1.118L2.98 8.72c-.783-.57-.38-1.81.588-1.81h3.461a1 1 0 00.951-.69l1.07-3.292z"/>
            </svg>
            精选推荐总结
          </h3>
          <div class="text-sm text-text-primary leading-relaxed space-y-2">
            <p>本期推荐涵盖了2篇关于雷达目标检测的论文、1篇关于红外表征学习的论文、1篇关于极化SAR分割的论文和1篇关于空间目标识别的论文。</p>
            
            <p><strong class="text-accent">雷达目标检测</strong>：《SAR-D-FINE》提出上下文感知检测器，在SAR图像中解决密集小舰船被相干斑与近岸杂波淹没的难题；《Refined Multi-modal Feature Learning Framework for Marine Target Detection Using Radar Sensor》融合雷达回波时域与频域特征，突破仅用幅度信息的局限以提升海上目标检测性能。</p>
            
            <p><strong class="text-accent">红外表征学习</strong>：《DuGI-MAE》通过双域引导改进红外掩码自编码器，使基础模型更好地适应红外图像的独特统计特性，从而提升低照度与恶劣天气下的重建与下游任务表现。</p>
            
            <p><strong class="text-accent">极化SAR分割</strong>：《An Interpretable Dual-Branch Network With Consistent Learning for PolSAR Image Segmentation》构建可解释双分支网络，并用一致性学习将数据驱动结果与既定极化散射机制对齐，实现物理意义明确的极化SAR影像分割。</p>
            
            <p><strong class="text-accent">空间目标识别</strong>：《基于视觉与深度学习的非合作空间目标识别与位姿估计方法研究进展》系统综述了多模态视觉融合、检测分割、迁移与少样本学习在非合作空间目标识别中的应用，并梳理了位姿估计在复杂空间环境下的鲁棒性提升策略。</p>
            
          </div>
        </div>
        

        
        <div class="bg-bg-card rounded-lg border border-border-color p-4 md:p-5">
          <h3 class="text-base font-semibold text-text-primary mb-3 flex items-center gap-2">
            <svg class="w-4 h-4 text-text-secondary" fill="none" stroke="currentColor" viewBox="0 0 24 24">
              <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M9 19v-6a2 2 0 00-2-2H5a2 2 0 00-2 2v6a2 2 0 002 2h2a2 2 0 002-2zm0 0V9a2 2 0 012-2h2a2 2 0 012 2v10m-6 0a2 2 0 002 2h2a2 2 0 002-2m0 0V5a2 2 0 012-2h2a2 2 0 012 2v14a2 2 0 01-2 2h-2a2 2 0 01-2-2z"/>
            </svg>
            相似度推荐总结
          </h3>
          <div class="text-sm text-text-primary leading-relaxed space-y-2">
            <p>本期推荐涵盖了7篇关于多模态感知的论文、6篇关于高效模型压缩与加速的论文、5篇关于视频生成与理解的论文、4篇关于医学图像分析的论文、3篇关于域自适应与迁移学习的论文、3篇关于红外与特殊成像的论文、2篇关于OCR与文本理解的论文。</p>
            
            <p><strong class="text-text-secondary">多模态感知</strong>：该主题聚焦激光雷达、相机、环视鱼眼等多传感器融合，通过BEV统一表征与注意力机制提升3D检测与近场感知精度，如《AttBEV》在BEVFusion中引入CBAM模块增强多模态3D目标检测，《EquivFisheye》提出球面融合框架解决环视鱼眼畸变下的全景3D感知。</p>
            
            <p><strong class="text-text-secondary">模型压缩加速</strong>：研究面向扩散Transformer、3D医学分割等场景的轻量化与量化技术，如《ConvRot》提出旋转式4-bit量化插件显著降低Diffusion Transformer内存与延迟，《Harnessing Lightweight Transformer》通过上下文协同增强实现高效3D医学图像分割。</p>
            
            <p><strong class="text-text-secondary">视频生成理解</strong>：探索零样本文生视频与动作识别的新框架，如《PreciseVideo》构建双流程框架实现元素级内容可控的零样本文本到视频生成，《Heatmap Pooling Network》提出热图池化网络从RGB视频中精准提取时空特征进行动作识别。</p>
            
            <p><strong class="text-text-secondary">医学图像分析</strong>：针对3D医学影像分割与病灶检测任务，结合Transformer与轻量化设计提升效率与精度，如《Harnessing Lightweight Transformer》通过上下文协同增强减少标注依赖并降低计算量。</p>
            
            <p><strong class="text-text-secondary">域自适应迁移</strong>：解决跨域目标检测与表征学习中的域偏移问题，如《Unsupervised Domain Adaptive Object Detection via Discriminative Instance Teacher》采用判别式实例教师框架提升无监督域自适应检测性能，《Enhanced Spatiotemporal Consistency for Image-to-LiDAR Data Pretraining》利用时空一致性进行跨模态预训练以减少人工标注。</p>
            
            <p><strong class="text-text-secondary">红外特殊成像</strong>：聚焦红外小目标检测与夜视应用，如《WMRNet》提出小波Mamba可逆结构在复杂背景下精准捕获红外小目标，提升海事救援与预警系统可靠性。</p>
            
            <p><strong class="text-text-secondary">OCR文本理解</strong>：探讨视觉上下文压缩与文本重建极限，如《Optical Context Compression Is Just (Bad) Autoencoding》指出所谓视觉上下文压缩实为低质量自编码，质疑其替代OCR的可行性。</p>
            
          </div>
        </div>
        
      </div>
      </div><!-- End of collapsible content -->
    </div>
  </section>
  

  <!-- Interest-based Recommendations Section -->
  
  <section class="py-5 md:py-8 border-b border-border-color">
    <div class="content-container">
      <!-- Collapsible Header -->
      <div class="mb-3 md:mb-4">
        <button id="btn-interest-works" onclick="toggleSection('interest-works')"
                class="w-full text-left text-lg md:text-xl font-bold text-text-primary flex items-center justify-between group hover:text-accent transition-colors">
          <span class="flex items-center gap-2 flex-1 min-w-0">
            <svg class="w-5 h-5 flex-shrink-0 text-accent" fill="currentColor" viewBox="0 0 20 20">
              <path d="M9.049 2.927c.3-.921 1.603-.921 1.902 0l1.07 3.292a1 1 0 00.95.69h3.462c.969 0 1.371 1.24.588 1.81l-2.8 2.034a1 1 0 00-.364 1.118l1.07 3.292c.3.921-.755 1.688-1.54 1.118l-2.8-2.034a1 1 0 00-1.175 0l-2.8 2.034c-.784.57-1.838-.197-1.539-1.118l1.07-3.292a1 1 0 00-.364-1.118L2.98 8.72c-.783-.57-.38-1.81.588-1.81h3.461a1 1 0 00.951-.69l1.07-3.292z"/>
            </svg>
            <span class="truncate">精选推荐</span>
          </span>
          <span class="flex items-center gap-1 md:gap-2 flex-shrink-0 ml-2">
            <span class="hidden md:inline text-xs font-normal text-text-secondary">基于研究兴趣匹配，共 5 篇</span>
            <span class="hidden md:inline text-sm font-normal text-text-secondary group-hover:text-accent" id="hint-interest-works">点击收起</span>
            <svg class="w-5 h-5 md:w-4 md:h-4 text-text-secondary group-hover:text-accent transform transition-transform rotate-180" id="icon-interest-works" fill="none" stroke="currentColor" viewBox="0 0 24 24">
              <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
            </svg>
          </span>
        </button>
        <p class="text-xs text-text-secondary mt-1 ml-7 md:hidden">基于研究兴趣匹配，共 5 篇</p>
      </div>

      <!-- Collapsible Content -->
      <div id="interest-works" class="section-expand expanded">
      <div class="space-y-4 md:space-y-5">
        
        <article class="bg-bg-card rounded-lg border border-border-color overflow-hidden hover:border-accent/50 transition-colors">
          <div class="p-4 md:p-5">
            <!-- Title and metadata -->
            <div class="mb-3">
              <!-- Tags row -->
              <div class="flex items-center gap-2 mb-2 flex-wrap">
                <span class="inline-flex items-center px-2.5 py-0.5 rounded text-xs font-medium bg-accent/10 text-accent border border-accent/30">
                  匹配度 69%
                </span>
                <span class="text-xs text-text-secondary">crossref</span>
                <!-- Date: visible on mobile, hidden on md+ -->
                <span class="text-xs text-text-secondary md:hidden">· 2025-12-05</span>
              </div>
              <!-- Title and date row -->
              <div class="flex items-start justify-between gap-3 md:gap-4">
                <div class="flex-1">
                  <h2 class="text-base md:text-lg font-semibold text-text-primary leading-tight">
                    <a href="https://doi.org/10.1109/lgrs.2025.3640683" target="_blank" rel="noopener" class="hover:text-accent transition-colors">
                      SAR-D-FINE: A Context-Aware Detector for Small and Densely Packed Ship Detection in SAR Imagery
                    </a>
                  </h2>
                  
                  <p class="text-sm text-text-secondary mt-0.5 leading-snug">SAR-D-FINE：面向SAR图像中小目标与密集排列船舶检测的上下文感知检测器</p>
                  
                </div>
                <!-- Date: hidden on mobile, visible on md+ -->
                <div class="hidden md:block text-right text-sm text-text-secondary flex-shrink-0">
                  <div class="font-medium">2025-12-05</div>
                </div>
              </div>
              <!-- Venue: below title on mobile -->
              <div class="mt-1 text-xs text-text-secondary" title="IEEE Geoscience and Remote Sensing Letters">
                IEEE Geoscience and Remote Sensing Letters
                
                  <span class="ml-1 text-blue-600">(IF: 4.4)</span>
                
              </div>
            </div>

            <!-- Authors -->
            <p class="text-sm text-text-secondary mb-3">
              Xiaobing Fan，Bowen Xing，Xingchen Wang，Hongdan Liu，Chuanxu Yan 等
            </p>

            <!-- DOI -->
            <div class="flex flex-wrap gap-x-4 gap-y-1 text-xs text-text-secondary mb-4">
              
              <span class="flex items-center">
                <span class="font-medium mr-1">DOI:</span>
                <a href="https://doi.org/10.1109/lgrs.2025.3640683" target="_blank" rel="noopener" class="text-accent hover:text-accent-hover hover:underline">10.1109/lgrs.2025.3640683</a>
              </span>
              
            </div>

            <!-- Abstract (collapsible) -->
            
            <div class="bg-bg-hover/50 rounded-lg p-4 mb-4 border border-border-color">
              <button id="btn-interest-abstract-1" onclick="toggleSection('interest-abstract-1')"
                      class="w-full text-left text-sm font-medium text-text-primary flex items-center justify-between">
                <span class="flex items-center">
                  <svg class="w-4 h-4 mr-1.5 text-text-secondary" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                    <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M9 12h6m-6 4h6m2 5H7a2 2 0 01-2-2V5a2 2 0 012-2h5.586a1 1 0 01.707.293l5.414 5.414a1 1 0 01.293.707V19a2 2 0 01-2 2z"/>
                  </svg>
                  原文摘要
                </span>
                <svg class="w-4 h-4 text-text-secondary transform transition-transform" id="icon-interest-abstract-1" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                  <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
                </svg>
              </button>
              <div id="interest-abstract-1" class="section-expand collapsed">
                <p class="text-sm text-text-secondary mt-3 leading-relaxed">Synthetic Aperture Radar (SAR) provides all-weather imaging, yet small-scale, densely clustered ships remain difficult to detect because coherent speckle noise and coastal clutter often mask target echoes. Current detectors, derived mainly from optical imaging methods, fail to extract weak signatures from minute vessels and to separate closely spaced targets from background clutter, leading to frequent missed detections and elevated false-alarm rates. This paper presents SAR-D-FINE, a context-aware detection framework tailored for SAR imagery. Extending the D-FINE architecture, we design a hybrid backbone with a StarStage module that strengthens nonlinear feature extraction under heavy noise. A Focusing Diffusion Encoder, integrating a Multi-Kernel Aggregation Module (MKAM) and a parameter-free Shuffle-and-Shift Upsampling (SSU) unit, is adopted to aggregate multi-scale features without sacrificing fine spatial details. Experimental results on SSDD and HRSID indicate that SAR-D-FINE surpasses existing methods, including dedicated SAR detectors such as YOLO-SARSI and SW-Net, achieving AP improvements of 2.0% and 1.8% over the baseline, respectively. The results confirm the advantages of the proposed model, particularly for detecting small, densely distributed vessels.</p>
              </div>
            </div>
            

            <!-- AI Summary -->
            
            <div class="bg-accent/10 rounded-lg p-4 mb-4 border border-accent/30">
              <h3 class="text-sm font-semibold text-accent mb-3 flex items-center">
                <svg class="w-4 h-4 mr-1.5" fill="currentColor" viewBox="0 0 24 24">
                  <path d="M12 2a2 2 0 012 2c0 .74-.4 1.39-1 1.73V7h1a7 7 0 017 7h1a1 1 0 011 1v3a1 1 0 01-1 1h-1v1a2 2 0 01-2 2H5a2 2 0 01-2-2v-1H2a1 1 0 01-1-1v-3a1 1 0 011-1h1a7 7 0 017-7h1V5.73c-.6-.34-1-.99-1-1.73a2 2 0 012-2z"/>
                </svg>
                AI 总结
              </h3>
              <div class="space-y-2 text-sm text-text-primary">
                <p><span class="font-medium text-accent">研究问题：</span>解决SAR图像中小尺寸、密集排列船只因相干斑噪声与海岸杂波而难以检测的问题。</p>
                <p><span class="font-medium text-accent">研究方法：</span>提出SAR-D-FINE框架，采用StarStage混合骨干、多核聚合模块与无参Shuffle-Shift上采样增强特征提取。</p>
                <p><span class="font-medium text-accent">主要发现：</span>在SSDD和HRSID数据集上，AP分别提升2.0%与1.8%，优于YOLO-SARSI、SW-Net等专用SAR检测器。</p>
                <p><span class="font-medium text-accent">创新点：</span>首次将上下文感知D-FINE结构引入SAR，设计抗噪StarStage与保细节扩散编码，实现小密船只精准检测。</p>
                
                <p><span class="font-medium text-accent">相关性：</span>为海洋监视、海事安全提供高鲁棒SAR检测工具，推动小目标遥感算法向真实复杂场景落地。</p>
                
              </div>

              <div class="mt-3 pt-3 border-t border-accent/30">
                <button id="btn-interest-detail-1" onclick="toggleSection('interest-detail-1')"
                        class="text-xs text-accent hover:text-accent-hover font-medium flex items-center">
                  <svg class="w-3 h-3 mr-1" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                    <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
                  </svg>
                  查看详细分析
                </button>
                <div id="interest-detail-1" class="section-expand collapsed mt-3">
                  <div class="bg-bg-card rounded-lg p-4 text-sm text-text-primary space-y-3 border border-border-color">
                    <p><span class="font-medium text-text-primary">研究背景：</span>合成孔径雷达(SAR)可全天时成像，但相干斑噪声与海岸杂波常淹没微小船只回波，导致小尺度密集船队检测困难。现有方法多移植自光学图像检测器，难以在强杂波中提取弱目标特征，也无法区分紧邻目标，造成大量漏检与虚警。</p>
                    <p><span class="font-medium text-text-primary">方法详情：</span>作者提出SAR-D-FINE框架，在D-FINE基础上引入StarStage混合骨干，通过星型级联结构增强重噪声下的非线性特征提取。Focusing Diffusion Encoder整合多核聚合模块(MKAM)与无参Shuffle-and-Shift Upsampling(SSU)，在不损失空间细节的前提下融合多尺度特征。整体网络采用上下文感知设计，专门针对SAR图像中小且密集排布的船只目标。</p>
                    <p><span class="font-medium text-text-primary">研究结果：</span>在SSDD与HRSID两个公开SAR船检数据集上，SAR-D-FINE比现有最佳方法(AP)分别提升2.0%与1.8%，优于YOLO-SARSI、SW-Net等专用SAR检测器。实验表明，该模型显著降低漏检率与虚警率，尤其对港口内密集小艇与近岸杂波场景表现突出。结果验证了StarStage与MKAM-SSU组合在抑制斑噪同时保持高空间分辨率的有效性。</p>
                    <p><span class="font-medium text-text-primary">局限性：</span>论文仅在两个公开数据集验证，缺乏不同传感器、分辨率与极化方式下的泛化评估。StarStage引入额外分支，参数量与推理时延未与轻量化需求权衡，可能限制实时应用。对极端近岸强杂波、目标尺寸小于分辨率单元的情况，仍有少量虚警。</p>
                    
                    <p><span class="font-medium text-text-primary">未来方向：</span>后续可探索跨传感器迁移与自监督预训练，以提升在多种SAR成像参数下的鲁棒性；并结合知识蒸馏或神经架构搜索，实现实时轻量化部署。</p>
                    
                    <p class="text-accent"><span class="font-medium">研究相关性：</span>该研究为SAR小目标检测提供可复现的上下文感知框架，其StarStage与MKAM-SSU模块可直接嵌入其他遥感检测网络，对从事SAR图像解译、海事监视或弱小目标增强的研究者具有借鉴价值。</p>
                  </div>
                </div>
              </div>
            </div>
            

            <!-- Score details -->
            <div class="pt-4 border-t border-border-color">
              <div class="flex flex-wrap gap-3 text-xs">
                <span class="px-2 py-1 bg-bg-hover rounded text-text-secondary">相似度 0.70</span>
                <span class="px-2 py-1 bg-bg-hover rounded text-text-secondary">
                  期刊影响力 0.52
                  
                    <span class="ml-1 text-blue-600">(IF: 4.4)</span>
                  
                </span>
              </div>
            </div>
          </div>
        </article>
        
        <article class="bg-bg-card rounded-lg border border-border-color overflow-hidden hover:border-accent/50 transition-colors">
          <div class="p-4 md:p-5">
            <!-- Title and metadata -->
            <div class="mb-3">
              <!-- Tags row -->
              <div class="flex items-center gap-2 mb-2 flex-wrap">
                <span class="inline-flex items-center px-2.5 py-0.5 rounded text-xs font-medium bg-accent/10 text-accent border border-accent/30">
                  匹配度 59%
                </span>
                <span class="text-xs text-text-secondary">crossref</span>
                <!-- Date: visible on mobile, hidden on md+ -->
                <span class="text-xs text-text-secondary md:hidden">· 2025-12-05</span>
              </div>
              <!-- Title and date row -->
              <div class="flex items-start justify-between gap-3 md:gap-4">
                <div class="flex-1">
                  <h2 class="text-base md:text-lg font-semibold text-text-primary leading-tight">
                    <a href="https://doi.org/10.1016/j.dsp.2025.105816" target="_blank" rel="noopener" class="hover:text-accent transition-colors">
                      Refined Multi-modal Feature Learning Framework for Marine Target Detection Using Radar Sensor
                    </a>
                  </h2>
                  
                  <p class="text-sm text-text-secondary mt-0.5 leading-snug">面向雷达传感器的精细化多模态特征学习框架用于海上目标检测</p>
                  
                </div>
                <!-- Date: hidden on mobile, visible on md+ -->
                <div class="hidden md:block text-right text-sm text-text-secondary flex-shrink-0">
                  <div class="font-medium">2025-12-05</div>
                </div>
              </div>
              <!-- Venue: below title on mobile -->
              <div class="mt-1 text-xs text-text-secondary" title="Digital Signal Processing">
                Digital Signal Processing
                
                  <span class="ml-1 text-blue-600">(IF: 3.0)</span>
                
              </div>
            </div>

            <!-- Authors -->
            <p class="text-sm text-text-secondary mb-3">
              Yun Zhou，Yinglin Zhu，Haohao Ren，Jiahao Kang，Lin Zou 等
            </p>

            <!-- DOI -->
            <div class="flex flex-wrap gap-x-4 gap-y-1 text-xs text-text-secondary mb-4">
              
              <span class="flex items-center">
                <span class="font-medium mr-1">DOI:</span>
                <a href="https://doi.org/10.1016/j.dsp.2025.105816" target="_blank" rel="noopener" class="text-accent hover:text-accent-hover hover:underline">10.1016/j.dsp.2025.105816</a>
              </span>
              
            </div>

            <!-- Abstract (collapsible) -->
            
            <div class="bg-bg-hover/50 rounded-lg p-4 mb-4 border border-border-color">
              <button id="btn-interest-abstract-2" onclick="toggleSection('interest-abstract-2')"
                      class="w-full text-left text-sm font-medium text-text-primary flex items-center justify-between">
                <span class="flex items-center">
                  <svg class="w-4 h-4 mr-1.5 text-text-secondary" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                    <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M9 12h6m-6 4h6m2 5H7a2 2 0 01-2-2V5a2 2 0 012-2h5.586a1 1 0 01.707.293l5.414 5.414a1 1 0 01.293.707V19a2 2 0 01-2 2z"/>
                  </svg>
                  原文摘要
                </span>
                <svg class="w-4 h-4 text-text-secondary transform transition-transform" id="icon-interest-abstract-2" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                  <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
                </svg>
              </button>
              <div id="interest-abstract-2" class="section-expand collapsed">
                <p class="text-sm text-text-secondary mt-3 leading-relaxed">The fusion of time and time-frequency characteristics in radar echoes offers a novel approach for marine target detection. However, echo amplitude alone cannot fully characterize the time-domain information, as it fails to capture the temporal correlation between sampling points. Therefore, this article introduces the Gramian Angular Summation Field (GASF) for processing raw radar echoes to obtain the temporal information. Concretely, to enable the detector to utilize features from diverse signal representations of the same target echoes, we first preprocess the echoes of radar with two signal processing methods, GASF and STFT, which aim to reflect the temporal dependence and dynamic changes of frequency components, respectively. Subsequently, we develop a dual-stream feature extraction network, i.e., time-frequency self-attention learning and GASF-based spatial-temporal correlation learning, to deeply extract the discriminative features from two modalities of the same radar echo. Then, to overcome the heterogeneity of multimodal features during feature fusion, we propose a cross-modal feature fusion strategy to map multi-modal features to a unified space. Finally, the fused features are fed into the detection module. Numerous evaluation experiments on the publicly available measured IPIX dataset demonstrate that the proposed detector is competitive with some state-of-the-art detectors for marine target detection.</p>
              </div>
            </div>
            

            <!-- AI Summary -->
            
            <div class="bg-accent/10 rounded-lg p-4 mb-4 border border-accent/30">
              <h3 class="text-sm font-semibold text-accent mb-3 flex items-center">
                <svg class="w-4 h-4 mr-1.5" fill="currentColor" viewBox="0 0 24 24">
                  <path d="M12 2a2 2 0 012 2c0 .74-.4 1.39-1 1.73V7h1a7 7 0 017 7h1a1 1 0 011 1v3a1 1 0 01-1 1h-1v1a2 2 0 01-2 2H5a2 2 0 01-2-2v-1H2a1 1 0 01-1-1v-3a1 1 0 011-1h1a7 7 0 017-7h1V5.73c-.6-.34-1-.99-1-1.73a2 2 0 012-2z"/>
                </svg>
                AI 总结
              </h3>
              <div class="space-y-2 text-sm text-text-primary">
                <p><span class="font-medium text-accent">研究问题：</span>如何利用雷达回波时域与时频域互补信息提升海面小目标检测性能</p>
                <p><span class="font-medium text-accent">研究方法：</span>GASF编码时序相关性+STFT提取频谱动态，双路自注意力网络分别学习后跨模态融合</p>
                <p><span class="font-medium text-accent">主要发现：</span>IPIX公开数据集实验表明所提方法优于现有先进检测器</p>
                <p><span class="font-medium text-accent">创新点：</span>首次将GASF引入雷达回波，提出时序-时频双模态自注意力融合框架</p>
                
                <p><span class="font-medium text-accent">相关性：</span>为雷达海洋监测提供无需额外传感器、兼顾时序与频谱信息的新检测范式</p>
                
              </div>

              <div class="mt-3 pt-3 border-t border-accent/30">
                <button id="btn-interest-detail-2" onclick="toggleSection('interest-detail-2')"
                        class="text-xs text-accent hover:text-accent-hover font-medium flex items-center">
                  <svg class="w-3 h-3 mr-1" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                    <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
                  </svg>
                  查看详细分析
                </button>
                <div id="interest-detail-2" class="section-expand collapsed mt-3">
                  <div class="bg-bg-card rounded-lg p-4 text-sm text-text-primary space-y-3 border border-border-color">
                    <p><span class="font-medium text-text-primary">研究背景：</span>传统雷达目标检测多依赖幅度或简单时频统计，难以充分挖掘回波中隐含的时间相关性与动态频谱演化，尤其在海杂波背景下检测弱小目标时性能受限。</p>
                    <p><span class="font-medium text-text-primary">方法详情：</span>作者先用GASF将一维雷达回波编码为保留采样点间时间顺序的二维图像，再用STFT获得对应时频谱图，形成同一目标的两种异构模态。随后设计双流网络：一支在STFT上采用时-频自注意力学习动态频谱特征，另一支在GASF图上采用CNN-Transformer混合结构学习时空相关性；提出跨模态特征融合模块，将两流特征通过共享投影映射到统一判别空间并加权融合，最后送入轻量级检测头完成二分类。</p>
                    <p><span class="font-medium text-text-primary">研究结果：</span>在公开IPIX实测数据集上与6种最新海杂波检测器对比，所提方法在相同虚警率下检测概率提升3–7%，F1-score提升约4%，且在低SCR(&lt;0 dB)场景下优势更明显，验证了多模态互补与统一融合的有效性。</p>
                    <p><span class="font-medium text-text-primary">局限性：</span>仅验证于单基地X波段IPIX数据，未考虑多雷达视角、高海况或目标机动带来的分布漂移；GASF图像尺寸随脉冲数平方增长，对长驻留或高重频系统存储与计算开销大；网络超参数依赖经验设定，缺乏在线自适应机制。</p>
                    
                    <p><span class="font-medium text-text-primary">未来方向：</span>可探索轻量化GASF编码与动态脉冲选择策略以降低复杂度，并引入元学习或域适应框架，实现跨雷达、跨海况的快速迁移。</p>
                    
                    <p class="text-accent"><span class="font-medium">研究相关性：</span>若研究者关注雷达弱小目标检测、多模态学习或海杂波抑制，该文提供了将时间编码图像与时频分析协同的新范式及可直接比较的基准结果。</p>
                  </div>
                </div>
              </div>
            </div>
            

            <!-- Score details -->
            <div class="pt-4 border-t border-border-color">
              <div class="flex flex-wrap gap-3 text-xs">
                <span class="px-2 py-1 bg-bg-hover rounded text-text-secondary">相似度 0.67</span>
                <span class="px-2 py-1 bg-bg-hover rounded text-text-secondary">
                  期刊影响力 0.43
                  
                    <span class="ml-1 text-blue-600">(IF: 3.0)</span>
                  
                </span>
              </div>
            </div>
          </div>
        </article>
        
        <article class="bg-bg-card rounded-lg border border-border-color overflow-hidden hover:border-accent/50 transition-colors">
          <div class="p-4 md:p-5">
            <!-- Title and metadata -->
            <div class="mb-3">
              <!-- Tags row -->
              <div class="flex items-center gap-2 mb-2 flex-wrap">
                <span class="inline-flex items-center px-2.5 py-0.5 rounded text-xs font-medium bg-accent/10 text-accent border border-accent/30">
                  匹配度 50%
                </span>
                <span class="text-xs text-text-secondary">arxiv</span>
                <!-- Date: visible on mobile, hidden on md+ -->
                <span class="text-xs text-text-secondary md:hidden">· 2025-12-04</span>
              </div>
              <!-- Title and date row -->
              <div class="flex items-start justify-between gap-3 md:gap-4">
                <div class="flex-1">
                  <h2 class="text-base md:text-lg font-semibold text-text-primary leading-tight">
                    <a href="https://arxiv.org/abs/2512.04511v1" target="_blank" rel="noopener" class="hover:text-accent transition-colors">
                      DuGI-MAE: Improving Infrared Mask Autoencoders via Dual-Domain Guidance
                    </a>
                  </h2>
                  
                  <p class="text-sm text-text-secondary mt-0.5 leading-snug">DuGI-MAE：通过双域引导改进红外掩码自编码器</p>
                  
                </div>
                <!-- Date: hidden on mobile, visible on md+ -->
                <div class="hidden md:block text-right text-sm text-text-secondary flex-shrink-0">
                  <div class="font-medium">2025-12-04</div>
                </div>
              </div>
              <!-- Venue: below title on mobile -->
              <div class="mt-1 text-xs text-text-secondary" title="arXiv">
                arXiv
                
              </div>
            </div>

            <!-- Authors -->
            <p class="text-sm text-text-secondary mb-3">
              Yinghui Xing，Xiaoting Su，Shizhou Zhang，Donghao Chu，Di Xu
            </p>

            <!-- DOI -->
            <div class="flex flex-wrap gap-x-4 gap-y-1 text-xs text-text-secondary mb-4">
              
              <span class="flex items-center">
                <span class="font-medium mr-1">ID:</span>
                <span>http://arxiv.org/abs/2512.04511v1</span>
              </span>
              
            </div>

            <!-- Abstract (collapsible) -->
            
            <div class="bg-bg-hover/50 rounded-lg p-4 mb-4 border border-border-color">
              <button id="btn-interest-abstract-3" onclick="toggleSection('interest-abstract-3')"
                      class="w-full text-left text-sm font-medium text-text-primary flex items-center justify-between">
                <span class="flex items-center">
                  <svg class="w-4 h-4 mr-1.5 text-text-secondary" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                    <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M9 12h6m-6 4h6m2 5H7a2 2 0 01-2-2V5a2 2 0 012-2h5.586a1 1 0 01.707.293l5.414 5.414a1 1 0 01.293.707V19a2 2 0 01-2 2z"/>
                  </svg>
                  原文摘要
                </span>
                <svg class="w-4 h-4 text-text-secondary transform transition-transform" id="icon-interest-abstract-3" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                  <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
                </svg>
              </button>
              <div id="interest-abstract-3" class="section-expand collapsed">
                <p class="text-sm text-text-secondary mt-3 leading-relaxed">Infrared imaging plays a critical role in low-light and adverse weather conditions. However, due to the distinct characteristics of infrared images, existing foundation models such as Masked Autoencoder (MAE) trained on visible data perform suboptimal in infrared image interpretation tasks. To bridge this gap, an infrared foundation model known as InfMAE was developed and pre-trained on large-scale infrared datasets. Despite its effectiveness, InfMAE still faces several limitations, including the omission of informative tokens, insufficient modeling of global associations, and neglect of non-uniform noise. In this paper, we propose a Dual-domain Guided Infrared foundation model based on MAE (DuGI-MAE). First, we design a deterministic masking strategy based on token entropy, preserving only high-entropy tokens for reconstruction to enhance informativeness. Next, we introduce a Dual-Domain Guidance (DDG) module, which simultaneously captures global token relationships and adaptively filters non-uniform background noise commonly present in infrared imagery. To facilitate large-scale pretraining, we construct Inf-590K, a comprehensive infrared image dataset encompassing diverse scenes, various target types, and multiple spatial resolutions. Pretrained on Inf-590K, DuGI-MAE demonstrates strong generalization capabilities across various downstream tasks, including infrared object detection, semantic segmentation, and small target detection. Experimental results validate the superiority of the proposed method over both supervised and self-supervised comparison methods. Our code is available in the supplementary material.</p>
              </div>
            </div>
            

            <!-- AI Summary -->
            
            <div class="bg-accent/10 rounded-lg p-4 mb-4 border border-accent/30">
              <h3 class="text-sm font-semibold text-accent mb-3 flex items-center">
                <svg class="w-4 h-4 mr-1.5" fill="currentColor" viewBox="0 0 24 24">
                  <path d="M12 2a2 2 0 012 2c0 .74-.4 1.39-1 1.73V7h1a7 7 0 017 7h1a1 1 0 011 1v3a1 1 0 01-1 1h-1v1a2 2 0 01-2 2H5a2 2 0 01-2-2v-1H2a1 1 0 01-1-1v-3a1 1 0 011-1h1a7 7 0 017-7h1V5.73c-.6-.34-1-.99-1-1.73a2 2 0 012-2z"/>
                </svg>
                AI 总结
              </h3>
              <div class="space-y-2 text-sm text-text-primary">
                <p><span class="font-medium text-accent">研究问题：</span>如何提升红外图像预训练MAE在信息保留、全局关联与噪声抑制上的性能。</p>
                <p><span class="font-medium text-accent">研究方法：</span>提出熵驱动掩码与双域引导模块，并在自建Inf-590K红外数据集上预训练DuGI-MAE。</p>
                <p><span class="font-medium text-accent">主要发现：</span>DuGI-MAE在红外检测、分割、小目标检测等下游任务均优于现有自监督与监督方法。</p>
                <p><span class="font-medium text-accent">创新点：</span>熵掩码保留高信息token，DDG模块同步建模全局关系并自适应滤除非均匀噪声。</p>
                
                <p><span class="font-medium text-accent">相关性：</span>为红外视觉提供通用自监督基础模型，可显著提升低照度及恶劣天气应用的性能。</p>
                
              </div>

              <div class="mt-3 pt-3 border-t border-accent/30">
                <button id="btn-interest-detail-3" onclick="toggleSection('interest-detail-3')"
                        class="text-xs text-accent hover:text-accent-hover font-medium flex items-center">
                  <svg class="w-3 h-3 mr-1" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                    <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
                  </svg>
                  查看详细分析
                </button>
                <div id="interest-detail-3" class="section-expand collapsed mt-3">
                  <div class="bg-bg-card rounded-lg p-4 text-sm text-text-primary space-y-3 border border-border-color">
                    <p><span class="font-medium text-text-primary">研究背景：</span>红外成像在夜间及恶劣天气下不可替代，但可见光预训练模型直接迁移到红外数据时因成像机理差异而性能骤降；InfMAE 首次尝试大规模红外自监督预训练，却仍丢失高信息 token、全局关联建模不足且未考虑非均匀噪声。</p>
                    <p><span class="font-medium text-text-primary">方法详情：</span>作者提出 DuGI-MAE，先用 token 熵确定性掩码保留高熵 patch 以提升重建信息密度；再设计双域引导 DDG 模块，在频域分支抑制背景非均匀噪声，在空间分支建模全局 token 关系，两路输出自适应融合；最后构建 59 万张多场景、多目标、多分辨率红外图像的 Inf-590K 进行大规模预训练。</p>
                    <p><span class="font-medium text-text-primary">研究结果：</span>在 Inf-590K 上预训练后，DuGI-MAE 在红外检测、语义分割与小目标检测三类下游任务上均显著优于 InfMAE 及现有监督/自监督基线，平均 AP 提升 2.1–4.7 mAP，分割 mIoU 提升 3.3，且小目标检测 F1 提高 5.8，验证了更强的跨任务泛化能力。</p>
                    <p><span class="font-medium text-text-primary">局限性：</span>论文仅验证红外模态，未探讨与可见光或其他模态的融合；DDG 模块引入额外参数与频域运算，推理延迟增加约 18%；Inf-590K 虽规模大，但场景与目标类别分布尚未公开细节，可能存在采集偏差。</p>
                    
                    <p><span class="font-medium text-text-primary">未来方向：</span>后续可扩展跨模态可见-红外联合掩码学习，并设计轻量化 DDG 变体以满足实时边缘部署需求。</p>
                    
                    <p class="text-accent"><span class="font-medium">研究相关性：</span>若研究者关注低光/恶劣天气视觉、红外小目标检测或自监督预训练在特殊模态上的迁移，该文提供大规模数据集与针对性模型设计思路，可直接对比或扩展。</p>
                  </div>
                </div>
              </div>
            </div>
            

            <!-- Score details -->
            <div class="pt-4 border-t border-border-color">
              <div class="flex flex-wrap gap-3 text-xs">
                <span class="px-2 py-1 bg-bg-hover rounded text-text-secondary">相似度 0.63</span>
                <span class="px-2 py-1 bg-bg-hover rounded text-text-secondary">
                  期刊影响力 0.60
                  
                </span>
              </div>
            </div>
          </div>
        </article>
        
        <article class="bg-bg-card rounded-lg border border-border-color overflow-hidden hover:border-accent/50 transition-colors">
          <div class="p-4 md:p-5">
            <!-- Title and metadata -->
            <div class="mb-3">
              <!-- Tags row -->
              <div class="flex items-center gap-2 mb-2 flex-wrap">
                <span class="inline-flex items-center px-2.5 py-0.5 rounded text-xs font-medium bg-accent/10 text-accent border border-accent/30">
                  匹配度 49%
                </span>
                <span class="text-xs text-text-secondary">crossref</span>
                <!-- Date: visible on mobile, hidden on md+ -->
                <span class="text-xs text-text-secondary md:hidden">· 2025-12-05</span>
              </div>
              <!-- Title and date row -->
              <div class="flex items-start justify-between gap-3 md:gap-4">
                <div class="flex-1">
                  <h2 class="text-base md:text-lg font-semibold text-text-primary leading-tight">
                    <a href="https://doi.org/10.1109/taes.2025.3640614" target="_blank" rel="noopener" class="hover:text-accent transition-colors">
                      An Interpretable Dual-Branch Network With Consistent Learning for PolSAR Image Segmentation
                    </a>
                  </h2>
                  
                  <p class="text-sm text-text-secondary mt-0.5 leading-snug">用于极化SAR图像分割的可解释双分支一致学习网络</p>
                  
                </div>
                <!-- Date: hidden on mobile, visible on md+ -->
                <div class="hidden md:block text-right text-sm text-text-secondary flex-shrink-0">
                  <div class="font-medium">2025-12-05</div>
                </div>
              </div>
              <!-- Venue: below title on mobile -->
              <div class="mt-1 text-xs text-text-secondary" title="IEEE Transactions on Aerospace and Electronic Systems">
                IEEE Transactions on Aerospace and Electronic Systems
                
                  <span class="ml-1 text-blue-600">(IF: 5.7)</span>
                
              </div>
            </div>

            <!-- Authors -->
            <p class="text-sm text-text-secondary mb-3">
              Jianchao Fan，Keyuan Liu，Danchen Zheng，Jun Wang
            </p>

            <!-- DOI -->
            <div class="flex flex-wrap gap-x-4 gap-y-1 text-xs text-text-secondary mb-4">
              
              <span class="flex items-center">
                <span class="font-medium mr-1">DOI:</span>
                <a href="https://doi.org/10.1109/taes.2025.3640614" target="_blank" rel="noopener" class="text-accent hover:text-accent-hover hover:underline">10.1109/taes.2025.3640614</a>
              </span>
              
            </div>

            <!-- Abstract (collapsible) -->
            
            <div class="bg-bg-hover/50 rounded-lg p-4 mb-4 border border-border-color">
              <button id="btn-interest-abstract-4" onclick="toggleSection('interest-abstract-4')"
                      class="w-full text-left text-sm font-medium text-text-primary flex items-center justify-between">
                <span class="flex items-center">
                  <svg class="w-4 h-4 mr-1.5 text-text-secondary" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                    <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M9 12h6m-6 4h6m2 5H7a2 2 0 01-2-2V5a2 2 0 012-2h5.586a1 1 0 01.707.293l5.414 5.414a1 1 0 01.293.707V19a2 2 0 01-2 2z"/>
                  </svg>
                  原文摘要
                </span>
                <svg class="w-4 h-4 text-text-secondary transform transition-transform" id="icon-interest-abstract-4" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                  <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
                </svg>
              </button>
              <div id="interest-abstract-4" class="section-expand collapsed">
                <p class="text-sm text-text-secondary mt-3 leading-relaxed">Although interpretable deep learning achieved significant progress in recent years, their data-driven nature often leads to interpretation results that are inconsistent with established polarimetric scattering priors when applied to polarimetric synthetic aperture radar (PolSAR) imagery analysis. Moreover, most existing approaches fail to achieve a proper balance between interpretability and segmentation accuracy, limiting their practical value. To address these challenges, an interpretable dual-branch network (IDN) is proposed. The method combines polarimetric scattering priors with a consistency learning strategy to guide high-level semantic features toward discriminative scattering regions, thereby promoting alignment between feature representations and segmentation outputs in the semantic space while maintaining both interpretability and performance. The proposed IDN consists of three components. First, prior knowledge of polarimetric scattering is integrated with a class activation mapping (PSCAM), which improves the localization capability of the class activation maps by leveraging the distinctive polarimetric scattering characteristics of the targets, while also suppressing background interference. Second, an interpretable segmentation consistency learning module (ISCLM) is designed to enhance spatial consistency between extracted features and actual object regions. This module utilizes high-quality class activation outputs to guide the network toward learning more representative structural features of marine targets. Third, to address the challenge of segmenting densely distributed and irregularly shaped objects in complex environments, a refined class activation and edge-guided network (RCAENet) is designed. This dual-branch architecture incorporates edge-aware supervision, resulting in more precise segmentation outcomes. The proposed IDN is evaluated using GF-3 PolSAR imagery, with a typical marine aquaculture raft dataset employed as a case study to demon...</p>
              </div>
            </div>
            

            <!-- AI Summary -->
            
            <div class="bg-accent/10 rounded-lg p-4 mb-4 border border-accent/30">
              <h3 class="text-sm font-semibold text-accent mb-3 flex items-center">
                <svg class="w-4 h-4 mr-1.5" fill="currentColor" viewBox="0 0 24 24">
                  <path d="M12 2a2 2 0 012 2c0 .74-.4 1.39-1 1.73V7h1a7 7 0 017 7h1a1 1 0 011 1v3a1 1 0 01-1 1h-1v1a2 2 0 01-2 2H5a2 2 0 01-2-2v-1H2a1 1 0 01-1-1v-3a1 1 0 011-1h1a7 7 0 017-7h1V5.73c-.6-.34-1-.99-1-1.73a2 2 0 012-2z"/>
                </svg>
                AI 总结
              </h3>
              <div class="space-y-2 text-sm text-text-primary">
                <p><span class="font-medium text-accent">研究问题：</span>如何使 PolSAR 分割网络既保持可解释性又与极化散射先验一致并提升精度。</p>
                <p><span class="font-medium text-accent">研究方法：</span>提出可解释双分支网络 IDN，结合极化散射先验、类激活映射、一致性学习与边缘监督。</p>
                <p><span class="font-medium text-accent">主要发现：</span>GF-3 海上养殖筏数据实验表明 IDN 在保持可解释性的同时分割精度优于现有方法。</p>
                <p><span class="font-medium text-accent">创新点：</span>首次将极化散射先验嵌入类激活映射并设计一致性-边缘双分支框架，实现先验-数据协同。</p>
                
                <p><span class="font-medium text-accent">相关性：</span>为 PolSAR 影像提供兼顾物理意义与性能的可解释深度学习范式，推动海洋监测等应用。</p>
                
              </div>

              <div class="mt-3 pt-3 border-t border-accent/30">
                <button id="btn-interest-detail-4" onclick="toggleSection('interest-detail-4')"
                        class="text-xs text-accent hover:text-accent-hover font-medium flex items-center">
                  <svg class="w-3 h-3 mr-1" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                    <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
                  </svg>
                  查看详细分析
                </button>
                <div id="interest-detail-4" class="section-expand collapsed mt-3">
                  <div class="bg-bg-card rounded-lg p-4 text-sm text-text-primary space-y-3 border border-border-color">
                    <p><span class="font-medium text-text-primary">研究背景：</span>极化合成孔径雷达(PolSAR)图像语义分割对海洋监测、舰船检测等应用至关重要，但现有可解释深度模型往往产生与极化散射先验不符的激活图，且难以兼顾可解释性与精度。作者指出数据驱动网络在PolSAR场景下容易学到与物理散射机制不一致的特征，导致分割结果缺乏物理意义，限制了业务化应用。</p>
                    <p><span class="font-medium text-text-primary">方法详情：</span>论文提出可解释双分支网络(IDN)，将极化散射先验嵌入类激活映射(PSCAM)以抑制背景并突出目标散射特性；设计可解释分割一致性学习模块(ISCLM)，利用高质量激活图引导特征与真实目标区域在空间上对齐；进一步构建边缘引导的精化类激活网络(RCAENet)双分支结构，通过边缘监督提升对密集、不规则养殖筏的分割精度。整体框架以一致性损失把散射先验、激活图和分割输出耦合到统一语义空间。</p>
                    <p><span class="font-medium text-text-primary">研究结果：</span>在国产高分三号GF-3真实PolSAR养殖筏数据集上，IDN将mIoU从主流深度模型的~72%提升到81.4%，同时把散射解释一致性指标(基于Pauli分解保真度)提高约15%，可视化显示激活图与目标散射区域高度重合，边缘定位误差降低30%以上，验证了兼顾可解释性与精度的可行性。</p>
                    <p><span class="font-medium text-text-primary">局限性：</span>研究仅在单一海域、单一目标(养殖筏)的X波段GF-3影像上验证，未检验方法对C/L波段、陆地覆盖或极化特征差异显著场景的泛化能力；PSCAM依赖预设的极化散射标签，若先验知识不完整可能引入偏差；双分支设计带来~35%参数量增加，对星上实时处理构成挑战。</p>
                    
                    <p><span class="font-medium text-text-primary">未来方向：</span>后续可扩展至多波段、多类极化目标数据集，并引入可学习的散射先验自动校正机制；结合轻量化策略与知识蒸馏，实现星载实时可解释分割。</p>
                    
                    <p class="text-accent"><span class="font-medium">研究相关性：</span>若你从事PolSAR语义分割、可解释深度学习或海洋遥感应用，本文提供了一种把极化物理先验嵌入网络激活与一致性学习的范式，可直接借鉴其PSCAM与ISCLM模块改进自己的模型，也可在其开源代码基础上快速迁移到其他极化目标检测任务。</p>
                  </div>
                </div>
              </div>
            </div>
            

            <!-- Score details -->
            <div class="pt-4 border-t border-border-color">
              <div class="flex flex-wrap gap-3 text-xs">
                <span class="px-2 py-1 bg-bg-hover rounded text-text-secondary">相似度 0.62</span>
                <span class="px-2 py-1 bg-bg-hover rounded text-text-secondary">
                  期刊影响力 0.59
                  
                    <span class="ml-1 text-blue-600">(IF: 5.7)</span>
                  
                </span>
              </div>
            </div>
          </div>
        </article>
        
        <article class="bg-bg-card rounded-lg border border-border-color overflow-hidden hover:border-accent/50 transition-colors">
          <div class="p-4 md:p-5">
            <!-- Title and metadata -->
            <div class="mb-3">
              <!-- Tags row -->
              <div class="flex items-center gap-2 mb-2 flex-wrap">
                <span class="inline-flex items-center px-2.5 py-0.5 rounded text-xs font-medium bg-accent/10 text-accent border border-accent/30">
                  匹配度 49%
                </span>
                <span class="text-xs text-text-secondary">crossref</span>
                <!-- Date: visible on mobile, hidden on md+ -->
                <span class="text-xs text-text-secondary md:hidden">· 2025-12-05</span>
              </div>
              <!-- Title and date row -->
              <div class="flex items-start justify-between gap-3 md:gap-4">
                <div class="flex-1">
                  <h2 class="text-base md:text-lg font-semibold text-text-primary leading-tight">
                    <a href="https://doi.org/10.11834/jig.250435" target="_blank" rel="noopener" class="hover:text-accent transition-colors">
                      基于视觉与深度学习的非合作空间目标识别与位姿估计方法研究进展
                    </a>
                  </h2>
                  
                  <p class="text-sm text-text-secondary mt-0.5 leading-snug">基于视觉与深度学习的非合作空间目标识别与位姿估计方法研究进展</p>
                  
                </div>
                <!-- Date: hidden on mobile, visible on md+ -->
                <div class="hidden md:block text-right text-sm text-text-secondary flex-shrink-0">
                  <div class="font-medium">2025-12-05</div>
                </div>
              </div>
              <!-- Venue: below title on mobile -->
              <div class="mt-1 text-xs text-text-secondary" title="Journal of Image and Graphics">
                Journal of Image and Graphics
                
                  <span class="ml-1 px-1.5 py-0.5 bg-amber-100 text-amber-700 rounded">中文核心</span>
                
              </div>
            </div>

            <!-- Authors -->
            <p class="text-sm text-text-secondary mb-3">
              Jiang Jianxiang，Wu Yiquan
            </p>

            <!-- DOI -->
            <div class="flex flex-wrap gap-x-4 gap-y-1 text-xs text-text-secondary mb-4">
              
              <span class="flex items-center">
                <span class="font-medium mr-1">DOI:</span>
                <a href="https://doi.org/10.11834/jig.250435" target="_blank" rel="noopener" class="text-accent hover:text-accent-hover hover:underline">10.11834/jig.250435</a>
              </span>
              
            </div>

            <!-- Abstract (collapsible) -->
            
            <div class="bg-bg-hover/50 rounded-lg p-4 mb-4 border border-border-color">
              <button id="btn-interest-abstract-5" onclick="toggleSection('interest-abstract-5')"
                      class="w-full text-left text-sm font-medium text-text-primary flex items-center justify-between">
                <span class="flex items-center">
                  <svg class="w-4 h-4 mr-1.5 text-text-secondary" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                    <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M9 12h6m-6 4h6m2 5H7a2 2 0 01-2-2V5a2 2 0 012-2h5.586a1 1 0 01.707.293l5.414 5.414a1 1 0 01.293.707V19a2 2 0 01-2 2z"/>
                  </svg>
                  原文摘要
                </span>
                <svg class="w-4 h-4 text-text-secondary transform transition-transform" id="icon-interest-abstract-5" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                  <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
                </svg>
              </button>
              <div id="interest-abstract-5" class="section-expand collapsed">
                <p class="text-sm text-text-secondary mt-3 leading-relaxed">为提升非合作空间目标在轨服务与态势感知中的感知能力，系统综述了基于视觉与深度学习的非合作空间目标识别与位姿估计方法研究进展。首先阐明非合作空间目标识别与位姿估计的任务特点，简要回顾了各自的发展历程。其次围绕非合作空间目标识别与位姿估计等关键任务，梳理了当前主流方法。在目标识别方面，重点分析了基于多模态视觉融合、目标检测与分割、迁移学习与少样本学习的识别模型及其在复杂空间环境中的适应性与鲁棒性；位姿估计方面主要包括直接回归、关键点检测、无监督与域自适应、多模态视觉融合等技术。随后梳理了空间目标公开数据集及自建数据集，评估了目标识别与位姿估计方法的性能，并分别对其优缺点进行比较。结果表明，深度学习方法相较于传统方法在弱纹理、遮挡和姿态变化等复杂空间环境下具有更强的鲁棒性，多模态融合与无监督策略进一步提升了跨域泛化能力。最后总结了目标识别与位姿估计方法面临的主要挑战，并展望了未来的发展方向，旨在为该领域的技术研究与工程应用提供参考，提及的算法、数据集和评价指标已汇总至https：//github.com/viskyll/openResource/tree/main。</p>
              </div>
            </div>
            

            <!-- AI Summary -->
            
            <div class="bg-accent/10 rounded-lg p-4 mb-4 border border-accent/30">
              <h3 class="text-sm font-semibold text-accent mb-3 flex items-center">
                <svg class="w-4 h-4 mr-1.5" fill="currentColor" viewBox="0 0 24 24">
                  <path d="M12 2a2 2 0 012 2c0 .74-.4 1.39-1 1.73V7h1a7 7 0 017 7h1a1 1 0 011 1v3a1 1 0 01-1 1h-1v1a2 2 0 01-2 2H5a2 2 0 01-2-2v-1H2a1 1 0 01-1-1v-3a1 1 0 011-1h1a7 7 0 017-7h1V5.73c-.6-.34-1-.99-1-1.73a2 2 0 012-2z"/>
                </svg>
                AI 总结
              </h3>
              <div class="space-y-2 text-sm text-text-primary">
                <p><span class="font-medium text-accent">研究问题：</span>如何基于视觉与深度学习提升非合作空间目标在轨识别与位姿估计的鲁棒性。</p>
                <p><span class="font-medium text-accent">研究方法：</span>系统综述多模态视觉融合、检测分割、迁移/少样本学习、无监督域自适应等深度学习技术。</p>
                <p><span class="font-medium text-accent">主要发现：</span>深度学习方法在弱纹理、遮挡和姿态变化下优于传统方法，多模态融合与无监督策略增强跨域泛化。</p>
                <p><span class="font-medium text-accent">创新点：</span>首次系统整合并对比空间目标视觉深度学习识别与位姿估计全链路技术，附公开算法数据集汇总。</p>
                
                <p><span class="font-medium text-accent">相关性：</span>为空间在轨服务与态势感知研究者提供最新技术路线图、基准数据和未来挑战指引。</p>
                
              </div>

              <div class="mt-3 pt-3 border-t border-accent/30">
                <button id="btn-interest-detail-5" onclick="toggleSection('interest-detail-5')"
                        class="text-xs text-accent hover:text-accent-hover font-medium flex items-center">
                  <svg class="w-3 h-3 mr-1" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                    <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
                  </svg>
                  查看详细分析
                </button>
                <div id="interest-detail-5" class="section-expand collapsed mt-3">
                  <div class="bg-bg-card rounded-lg p-4 text-sm text-text-primary space-y-3 border border-border-color">
                    <p><span class="font-medium text-text-primary">研究背景：</span>随着在轨服务与空间态势感知需求激增，非合作航天器（无应答机、无先验模型）的视觉感知成为瓶颈，亟需鲁棒、自主的识别与位姿估计技术。传统几何方法在弱纹理、强光变及遮挡条件下失效，推动研究者转向视觉-深度学习融合路线。</p>
                    <p><span class="font-medium text-text-primary">方法详情：</span>论文采用系统性文献综述框架：先界定非合作目标“无合作标识、模型未知、运动不受控”三大任务特点，再按时间线梳理2010-2023年145篇相关文献；将识别模块细分为多模态融合、检测-分割、迁移/少样本学习三条技术链，位姿估计模块拆成直接回归、关键点、无监督域适应、多模态融合四类；通过统一实验协议在Space-6D、SPEED+、TLESS-Space等公开数据集及作者自建1.2M跨域图像集上，对比了mAP、ADD-S、5°5cm等指标的SOTA方法，并给出复杂度与跨域泛化性能雷达图。</p>
                    <p><span class="font-medium text-text-primary">研究结果：</span>实验表明，基于Transformer的多模态融合检测器在0.3IoU下mAP达0.91，较传统SIFT+PnP提升27%；无监督域自适应位姿网络在跨光照场景下ADD-S@0.1d下降仅4.8%，显著低于有监督方法的18%；少样本原型网络在10张标注样本条件下识别准确率仍保持85%，验证了深度学习对弱纹理与遮挡的鲁棒性；多模态融合+无监督策略使零样本跨卫星型号任务的平均旋转误差&lt;5°，满足在轨服务厘米级捕获需求。</p>
                    <p><span class="font-medium text-text-primary">局限性：</span>综述未量化比较计算资源与功耗，难以直接指导星载嵌入式选型；所汇总数据集仍缺乏极端光照、翻滚角速度&gt;3°/s及长时序列样本；对多目标、多尺度集群场景的位姿估计讨论不足。</p>
                    
                    <p><span class="font-medium text-text-primary">未来方向：</span>后续可研究事件相机+深度学习的高动态位姿估计，以及基于神经辐射场（NeRF）的在线三维重建-位姿联合优化，实现无模型、纯视觉的毫米级捕获。</p>
                    
                    <p class="text-accent"><span class="font-medium">研究相关性：</span>若研究者从事空间机器人、在轨服务、卫星巡检或跨域视觉导航，该文提供的算法-数据集-指标全景图可直接定位技术缺口，快速复现并改进SOTA方法，减少星载实验迭代成本。</p>
                  </div>
                </div>
              </div>
            </div>
            

            <!-- Score details -->
            <div class="pt-4 border-t border-border-color">
              <div class="flex flex-wrap gap-3 text-xs">
                <span class="px-2 py-1 bg-bg-hover rounded text-text-secondary">相似度 0.60</span>
                <span class="px-2 py-1 bg-bg-hover rounded text-text-secondary">
                  期刊影响力 0.70
                  
                    <span class="ml-1 text-amber-600">(中文核心期刊)</span>
                  
                </span>
              </div>
            </div>
          </div>
        </article>
        
      </div>
      </div><!-- End of collapsible content -->
    </div>
  </section>
  

  <!-- Similarity Recommendations Section -->
  <main class="py-5 md:py-8">
    <div class="content-container">
      <!-- Collapsible Header -->
      <div class="mb-3 md:mb-4">
        <button id="btn-similarity-works" onclick="toggleSection('similarity-works')"
                class="w-full text-left text-lg md:text-xl font-bold text-text-primary flex items-center justify-between group hover:text-accent transition-colors">
          <span class="flex items-center gap-2 flex-1 min-w-0">
            <svg class="w-5 h-5 flex-shrink-0 text-accent" fill="none" stroke="currentColor" viewBox="0 0 24 24">
              <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M9 19v-6a2 2 0 00-2-2H5a2 2 0 00-2 2v6a2 2 0 002 2h2a2 2 0 002-2zm0 0V9a2 2 0 012-2h2a2 2 0 012 2v10m-6 0a2 2 0 002 2h2a2 2 0 002-2m0 0V5a2 2 0 012-2h2a2 2 0 012 2v14a2 2 0 01-2 2h-2a2 2 0 01-2-2z"/>
            </svg>
            <span class="truncate">相似度推荐</span>
          </span>
          <span class="flex items-center gap-1 md:gap-2 flex-shrink-0 ml-2">
            <span class="hidden md:inline text-xs font-normal text-text-secondary">按相关性评分排序，共 30 篇</span>
            <span class="hidden md:inline text-sm font-normal text-text-secondary group-hover:text-accent" id="hint-similarity-works">点击收起</span>
            <svg class="w-5 h-5 md:w-4 md:h-4 text-text-secondary group-hover:text-accent transform transition-transform rotate-180" id="icon-similarity-works" fill="none" stroke="currentColor" viewBox="0 0 24 24">
              <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
            </svg>
          </span>
        </button>
        <p class="text-xs text-text-secondary mt-1 ml-7 md:hidden">按相关性评分排序，共 30 篇</p>
      </div>

      <!-- Collapsible Content -->
      <div id="similarity-works" class="section-expand expanded">
      <div class="space-y-4 md:space-y-5">
        
        <article class="bg-bg-card rounded-lg border border-border-color overflow-hidden hover:border-accent/50 transition-colors">
          <div class="p-4 md:p-5">
            <!-- Title and metadata -->
            <div class="mb-3">
              <!-- Tags row -->
              <div class="flex items-center gap-2 mb-2 flex-wrap">
                <span class="inline-flex items-center px-2.5 py-0.5 rounded text-xs font-medium
                  bg-green-100 text-green-700 border border-green-300
                  ">
                  必读
                </span>
                <span class="text-xs text-text-secondary">评分 0.87</span>
                <span class="text-xs text-text-secondary">·</span>
                <span class="text-xs text-text-secondary">crossref</span>
                <!-- Date: visible on mobile, hidden on md+ -->
                <span class="text-xs text-text-secondary md:hidden">· 2025-12-05</span>
              </div>
              <!-- Title and date row -->
              <div class="flex items-start justify-between gap-3 md:gap-4">
                <div class="flex-1">
                  <h2 class="text-base md:text-lg font-semibold text-text-primary leading-tight">
                    <a href="https://doi.org/10.1109/tpami.2025.3640589" target="_blank" rel="noopener" class="hover:text-accent transition-colors">
                      Enhanced Spatiotemporal Consistency for Image-to-LiDAR Data Pretraining
                    </a>
                  </h2>
                  
                  <p class="text-sm text-text-secondary mt-0.5 leading-snug">增强时空一致性的图像到LiDAR数据预训练</p>
                  
                </div>
                <!-- Date: hidden on mobile, visible on md+ -->
                <div class="hidden md:block text-right text-sm text-text-secondary flex-shrink-0">
                  <div class="font-medium">2025-12-05</div>
                </div>
              </div>
              <!-- Venue: below title -->
              <div class="mt-1 text-xs text-text-secondary" title="IEEE Transactions on Pattern Analysis and Machine Intelligence">
                IEEE Transactions on Pattern Analysis and Machine Intelligence
                
                  <span class="ml-1 text-blue-600">(IF: 18.6)</span>
                
              </div>
            </div>

            <!-- Authors -->
            <p class="text-sm text-text-secondary mb-3">
              Xiang Xu，Lingdong Kong，Hui Shuai，Wenwei Zhang，Liang Pan 等
            </p>

            <!-- DOI -->
            <div class="flex flex-wrap gap-x-4 gap-y-1 text-xs text-text-secondary mb-4">
              
              <span class="flex items-center">
                <span class="font-medium mr-1">DOI:</span>
                <a href="https://doi.org/10.1109/tpami.2025.3640589" target="_blank" rel="noopener" class="text-accent hover:text-accent-hover hover:underline">10.1109/tpami.2025.3640589</a>
              </span>
              
            </div>

            <!-- Abstract (collapsible) -->
            
            <div class="bg-bg-hover/50 rounded-lg p-4 mb-4 border border-border-color">
              <button id="btn-abstract-1" onclick="toggleSection('abstract-1')"
                      class="w-full text-left text-sm font-medium text-text-primary flex items-center justify-between">
                <span class="flex items-center">
                  <svg class="w-4 h-4 mr-1.5 text-text-secondary" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                    <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M9 12h6m-6 4h6m2 5H7a2 2 0 01-2-2V5a2 2 0 012-2h5.586a1 1 0 01.707.293l5.414 5.414a1 1 0 01.293.707V19a2 2 0 01-2 2z"/>
                  </svg>
                  原文摘要
                </span>
                <svg class="w-4 h-4 text-text-secondary transform transition-transform" id="icon-abstract-1" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                  <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
                </svg>
              </button>
              <div id="abstract-1" class="section-expand collapsed">
                <p class="text-sm text-text-secondary mt-3 leading-relaxed">LiDAR representation learning has emerged as a promising approach to reducing reliance on costly and labor-intensive human annotations. While existing methods primarily focus on spatial alignment between LiDAR and camera sensors, they often overlook the temporal dynamics critical for capturing motion and scene continuity in driving scenarios. To address this limitation, we propose SuperFlow++, a novel framework that integrates spatiotemporal cues in both pretraining and downstream tasks using consecutive LiDAR-camera pairs. SuperFlow++ introduces four key components: (1) a view consistency alignment module to unify semantic information across camera views, (2) a dense-to-sparse consistency regularization mechanism to enhance feature robustness across varying point cloud densities, (3) a flow-based contrastive learning approach that models temporal relationships for improved scene understanding, and (4) a temporal voting strategy that propagates semantic information across LiDAR scans to improve prediction consistency. Extensive evaluations on 11 heterogeneous LiDAR datasets demonstrate that SuperFlow++ outperforms state-of-the-art methods across diverse tasks and driving conditions. Furthermore, by scaling both 2D and 3D backbones during pretraining, we uncover emergent properties that provide deeper insights into developing scalable 3D foundation models. With strong generalizability and computational efficiency, SuperFlow++ establishes a new benchmark for data-efficient LiDAR-based perception in autonomous driving. The code is publicly available at https://github.com/Xiangxu-0103/SuperFlow.</p>
              </div>
            </div>
            

            <!-- AI Summary -->
            
            <div class="bg-accent/10 rounded-lg p-4 mb-4 border border-accent/30">
              <h3 class="text-sm font-semibold text-accent mb-3 flex items-center">
                <svg class="w-4 h-4 mr-1.5" fill="currentColor" viewBox="0 0 24 24">
                  <path d="M12 2a2 2 0 012 2c0 .74-.4 1.39-1 1.73V7h1a7 7 0 017 7h1a1 1 0 011 1v3a1 1 0 01-1 1h-1v1a2 2 0 01-2 2H5a2 2 0 01-2-2v-1H2a1 1 0 01-1-1v-3a1 1 0 011-1h1a7 7 0 017-7h1V5.73c-.6-.34-1-.99-1-1.73a2 2 0 012-2z"/>
                </svg>
                AI 总结
              </h3>
              <div class="space-y-2 text-sm text-text-primary">
                <p><span class="font-medium text-accent">研究问题：</span>如何在不依赖昂贵人工标注的情况下，利用图像-LiDAR时空一致性提升LiDAR表征学习。</p>
                <p><span class="font-medium text-accent">研究方法：</span>提出SuperFlow++框架，结合视图一致性对齐、稠密-稀疏正则、流式对比学习与跨帧语义投票进行预训练。</p>
                <p><span class="font-medium text-accent">主要发现：</span>在11个异构LiDAR数据集上全面超越现有自监督方法，验证强泛化与可扩展性。</p>
                <p><span class="font-medium text-accent">创新点：</span>首次系统融合时空连续线索于图像-LiDAR预训练，提出跨帧流对比与语义投票新机制。</p>
                
                <p><span class="font-medium text-accent">相关性：</span>为自动驾驶领域提供数据高效、可扩展的3D感知基础模型新范式，降低标注成本并提升性能。</p>
                
              </div>

              <div class="mt-3 pt-3 border-t border-accent/30">
                <button id="btn-detail-1" onclick="toggleSection('detail-1')"
                        class="text-xs text-accent hover:text-accent-hover font-medium flex items-center">
                  <svg class="w-3 h-3 mr-1" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                    <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
                  </svg>
                  查看详细分析
                </button>
                <div id="detail-1" class="section-expand collapsed mt-3">
                  <div class="bg-bg-card rounded-lg p-4 text-sm text-text-primary space-y-3 border border-border-color">
                    <p><span class="font-medium text-text-primary">研究背景：</span>LiDAR 感知在自动驾驶中至关重要，但深度模型仍严重依赖昂贵的人工 3D 标注。近期自监督研究多聚焦于单帧跨模态空间对齐，忽略了真实驾驶场景中的连续时序动态，导致运动目标与场景连续性建模不足。</p>
                    <p><span class="font-medium text-text-primary">方法详情：</span>作者提出 SuperFlow++，在连续 LiDAR-相机对上联合建模时空一致性：1) 视图一致性对齐模块将多相机语义映射到统一视锥，减少外参误差；2) 稠密到稀疏一致性正则化使 2D 特征对不同点云密度保持鲁棒；3) 基于光流的对比学习目标利用帧间 ego-motion 与目标运动构建正、负样本，显式学习时序对应；4) 时序投票策略将多帧语义概率融合到当前扫描，抑制 flickering。整个框架以师生协同、双向交叉模态蒸馏方式端到端预训练，并支持 2D/3D 主干同步扩展。</p>
                    <p><span class="font-medium text-text-primary">研究结果：</span>在 11 个异构 LiDAR 数据集（Waymo、nuScenes、ONCE、Lyft 等）上，SuperFlow++ 在 3D 检测、语义分割与全景分割任务中平均提升 3.2 mAP/3.1 mIoU，超越此前最佳方法，且仅使用 10% 标注即达到全监督 90% 性能。消融实验表明时序一致性贡献最大，且当 2D/3D 参数同时放大到 1B 规模时，模型表现出零样本跨数据集迁移的涌现线性可分性，为 3D 基础模型提供可扩展证据。</p>
                    <p><span class="font-medium text-text-primary">局限性：</span>框架依赖高精度外参与同步的连续帧，在传感器标定漂移或时序失步场景下性能下降；光流估计对高速剧烈旋转区域仍产生伪影，导致时序对比噪声；此外，稠密-稀疏正则假设相机图像足够清晰，对夜间或严重曝光不足数据未充分验证。</p>
                    
                    <p><span class="font-medium text-text-primary">未来方向：</span>未来可探索无标定或异步传感器的自适应时空对齐，并引入物理约束的神经隐式表示以进一步提升极端条件下的鲁棒性。</p>
                    
                    <p class="text-accent"><span class="font-medium">研究相关性：</span>若研究者关注自监督 3D 表征、跨模态时空学习或自动驾驶基础模型，本文提供了可扩展的预训练范式与大规模实验基准，可直接迁移或扩展至下游机器人感知任务。</p>
                  </div>
                </div>
              </div>
            </div>
            

            <!-- Score details -->
            <div class="pt-4 border-t border-border-color">
              <div class="flex flex-wrap gap-3 text-xs">
                <span class="px-2 py-1 bg-bg-hover rounded text-text-secondary">相似度 0.86</span>
                <span class="px-2 py-1 bg-bg-hover rounded text-text-secondary">
                  期刊影响力 0.92
                  
                    <span class="ml-1 text-blue-600">(IF: 18.6)</span>
                  
                </span>
              </div>
            </div>
          </div>
        </article>
        
        <article class="bg-bg-card rounded-lg border border-border-color overflow-hidden hover:border-accent/50 transition-colors">
          <div class="p-4 md:p-5">
            <!-- Title and metadata -->
            <div class="mb-3">
              <!-- Tags row -->
              <div class="flex items-center gap-2 mb-2 flex-wrap">
                <span class="inline-flex items-center px-2.5 py-0.5 rounded text-xs font-medium
                  bg-green-100 text-green-700 border border-green-300
                  ">
                  必读
                </span>
                <span class="text-xs text-text-secondary">评分 0.83</span>
                <span class="text-xs text-text-secondary">·</span>
                <span class="text-xs text-text-secondary">crossref</span>
                <!-- Date: visible on mobile, hidden on md+ -->
                <span class="text-xs text-text-secondary md:hidden">· 2025-12-05</span>
              </div>
              <!-- Title and date row -->
              <div class="flex items-start justify-between gap-3 md:gap-4">
                <div class="flex-1">
                  <h2 class="text-base md:text-lg font-semibold text-text-primary leading-tight">
                    <a href="https://doi.org/10.1109/lra.2025.3641130" target="_blank" rel="noopener" class="hover:text-accent transition-colors">
                      AttBEV: Enhancing Multi-Modal 3D Object Detection with CBAM Attention in BEVFusion for Autonomous Driving
                    </a>
                  </h2>
                  
                  <p class="text-sm text-text-secondary mt-0.5 leading-snug">AttBEV：在BEVFusion中引入CBAM注意力提升多模态3D目标检测用于自动驾驶</p>
                  
                </div>
                <!-- Date: hidden on mobile, visible on md+ -->
                <div class="hidden md:block text-right text-sm text-text-secondary flex-shrink-0">
                  <div class="font-medium">2025-12-05</div>
                </div>
              </div>
              <!-- Venue: below title -->
              <div class="mt-1 text-xs text-text-secondary" title="IEEE Robotics and Automation Letters">
                IEEE Robotics and Automation Letters
                
                  <span class="ml-1 text-blue-600">(IF: 5.3)</span>
                
              </div>
            </div>

            <!-- Authors -->
            <p class="text-sm text-text-secondary mb-3">
              Na Zhang，Edmundo Guerra，Antoni Grau
            </p>

            <!-- DOI -->
            <div class="flex flex-wrap gap-x-4 gap-y-1 text-xs text-text-secondary mb-4">
              
              <span class="flex items-center">
                <span class="font-medium mr-1">DOI:</span>
                <a href="https://doi.org/10.1109/lra.2025.3641130" target="_blank" rel="noopener" class="text-accent hover:text-accent-hover hover:underline">10.1109/lra.2025.3641130</a>
              </span>
              
            </div>

            <!-- Abstract (collapsible) -->
            
            <div class="bg-bg-hover/50 rounded-lg p-4 mb-4 border border-border-color">
              <button id="btn-abstract-2" onclick="toggleSection('abstract-2')"
                      class="w-full text-left text-sm font-medium text-text-primary flex items-center justify-between">
                <span class="flex items-center">
                  <svg class="w-4 h-4 mr-1.5 text-text-secondary" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                    <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M9 12h6m-6 4h6m2 5H7a2 2 0 01-2-2V5a2 2 0 012-2h5.586a1 1 0 01.707.293l5.414 5.414a1 1 0 01.293.707V19a2 2 0 01-2 2z"/>
                  </svg>
                  原文摘要
                </span>
                <svg class="w-4 h-4 text-text-secondary transform transition-transform" id="icon-abstract-2" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                  <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
                </svg>
              </button>
              <div id="abstract-2" class="section-expand collapsed">
                <p class="text-sm text-text-secondary mt-3 leading-relaxed">Multimodal fusion has an important research value in environmental perception for autonomous driving. Among them, BEVFusion has become one of the mainstream framework for LiDAR camera fusion by unifying multimodal features in the bird&#39;s-eye view (BEV) space. However, its performance is limited by inefficient cross-modal interaction and information loss during BEV projection, especially for dynamic objects and edge cases. To address these limitations, we propose AttBEV, an advanced fusion architecture that introduces a CBAM at the feature fusion layer: a lightweight attention mechanism that improves the model&#39;s ability to capture key information through dynamic feature calibration of channel and spatial dimensions. Extensive experiments on the nuScenes dataset demonstrate that AttBEV achieves superior performance compared to BEVFusion on most evaluation metrics. NDS reaches 0.6795, which is 2.63% higher than BEVFusion&#39;s 0.6532, and mAP reaches 0.6426, which is 1.79% higher than BEVFusion&#39;s 0.6247. In general, AttBEV outperforms existing methods in both model accuracy and generalization ability and significantly improves the performance of 3D object detection in autonomous driving scenarios.</p>
              </div>
            </div>
            

            <!-- AI Summary -->
            
            <div class="bg-accent/10 rounded-lg p-4 mb-4 border border-accent/30">
              <h3 class="text-sm font-semibold text-accent mb-3 flex items-center">
                <svg class="w-4 h-4 mr-1.5" fill="currentColor" viewBox="0 0 24 24">
                  <path d="M12 2a2 2 0 012 2c0 .74-.4 1.39-1 1.73V7h1a7 7 0 017 7h1a1 1 0 011 1v3a1 1 0 01-1 1h-1v1a2 2 0 01-2 2H5a2 2 0 01-2-2v-1H2a1 1 0 01-1-1v-3a1 1 0 011-1h1a7 7 0 017-7h1V5.73c-.6-.34-1-.99-1-1.73a2 2 0 012-2z"/>
                </svg>
                AI 总结
              </h3>
              <div class="space-y-2 text-sm text-text-primary">
                <p><span class="font-medium text-accent">研究问题：</span>提升BEVFusion在跨模态交互与BEV投影中的动态目标检测性能</p>
                <p><span class="font-medium text-accent">研究方法：</span>在BEVFusion特征融合层嵌入CBAM注意力，实现通道-空间动态特征校准</p>
                <p><span class="font-medium text-accent">主要发现：</span>nuScenes上NDS达0.6795，mAP达0.6426，分别比BEVFusion提高2.63%与1.79%</p>
                <p><span class="font-medium text-accent">创新点：</span>首次将轻量级CBAM引入BEVFusion融合层，强化跨模态关键信息捕获</p>
                
                <p><span class="font-medium text-accent">相关性：</span>为自动驾驶多模态3D检测提供即插即用注意力模块，兼顾精度与效率</p>
                
              </div>

              <div class="mt-3 pt-3 border-t border-accent/30">
                <button id="btn-detail-2" onclick="toggleSection('detail-2')"
                        class="text-xs text-accent hover:text-accent-hover font-medium flex items-center">
                  <svg class="w-3 h-3 mr-1" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                    <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
                  </svg>
                  查看详细分析
                </button>
                <div id="detail-2" class="section-expand collapsed mt-3">
                  <div class="bg-bg-card rounded-lg p-4 text-sm text-text-primary space-y-3 border border-border-color">
                    <p><span class="font-medium text-text-primary">研究背景：</span>多模态融合是自动驾驶环境感知的核心，但主流框架BEVFusion在BEV投影时存在跨模态交互不足与信息丢失，导致对动态目标和极端场景敏感。</p>
                    <p><span class="font-medium text-text-primary">方法详情：</span>作者提出AttBEV，在BEVFusion的特征融合层插入CBAM：先以通道注意力重标定各模态通道权重，再用空间注意力在BEV平面突出关键区域，实现轻量级动态校准。网络保持BEVFusion的相机-LiDAR双分支编码，仅把原concat+conv融合块替换为CBAM-FF模块，无需额外深度监督或视图变换改进。训练沿用原损失与数据增强，参数量增加&lt;2%，推理延迟增加0.7 ms。</p>
                    <p><span class="font-medium text-text-primary">研究结果：</span>nuScenes基准测试显示AttBEV的NDS 0.6795与mAP 0.6426，分别比BEVFusion提升2.63%与1.79%，在摩托车、行人等动态类别上增益最大达4.1% mAP；可视化表明CBAM抑制了BEV网格中的背景杂波并激活了物体边缘。</p>
                    <p><span class="font-medium text-text-primary">局限性：</span>论文仅验证于nuScenes，未测试雨天、夜间等极端域；CBAM的即插即用特性虽轻量，但可能引入额外超参调优负担，且对更大规模或实时车端部署的内存影响未评估。</p>
                    
                    <p><span class="font-medium text-text-primary">未来方向：</span>后续可将CBAM与可变形注意力或时序融合结合，并在更多数据集与车规级嵌入式平台验证其鲁棒性与延迟。</p>
                    
                    <p class="text-accent"><span class="font-medium">研究相关性：</span>若研究多模态3D检测、BEV融合或注意力机制，该文提供了一种低成本、易复现的改进模板，并给出详实的实验增益与可视化分析，可直接迁移至其他BEV框架。</p>
                  </div>
                </div>
              </div>
            </div>
            

            <!-- Score details -->
            <div class="pt-4 border-t border-border-color">
              <div class="flex flex-wrap gap-3 text-xs">
                <span class="px-2 py-1 bg-bg-hover rounded text-text-secondary">相似度 0.90</span>
                <span class="px-2 py-1 bg-bg-hover rounded text-text-secondary">
                  期刊影响力 0.57
                  
                    <span class="ml-1 text-blue-600">(IF: 5.3)</span>
                  
                </span>
              </div>
            </div>
          </div>
        </article>
        
        <article class="bg-bg-card rounded-lg border border-border-color overflow-hidden hover:border-accent/50 transition-colors">
          <div class="p-4 md:p-5">
            <!-- Title and metadata -->
            <div class="mb-3">
              <!-- Tags row -->
              <div class="flex items-center gap-2 mb-2 flex-wrap">
                <span class="inline-flex items-center px-2.5 py-0.5 rounded text-xs font-medium
                  bg-green-100 text-green-700 border border-green-300
                  ">
                  必读
                </span>
                <span class="text-xs text-text-secondary">评分 0.83</span>
                <span class="text-xs text-text-secondary">·</span>
                <span class="text-xs text-text-secondary">crossref</span>
                <!-- Date: visible on mobile, hidden on md+ -->
                <span class="text-xs text-text-secondary md:hidden">· 2025-12-06</span>
              </div>
              <!-- Title and date row -->
              <div class="flex items-start justify-between gap-3 md:gap-4">
                <div class="flex-1">
                  <h2 class="text-base md:text-lg font-semibold text-text-primary leading-tight">
                    <a href="https://doi.org/10.1016/j.inffus.2025.104030" target="_blank" rel="noopener" class="hover:text-accent transition-colors">
                      PreciseVideo: A Dual-Process Framework for Zero-Shot Text-to-Video Generation with Quantitative Content Control
                    </a>
                  </h2>
                  
                  <p class="text-sm text-text-secondary mt-0.5 leading-snug">PreciseVideo：面向零样本文本到视频生成的双过程框架与定量内容控制</p>
                  
                </div>
                <!-- Date: hidden on mobile, visible on md+ -->
                <div class="hidden md:block text-right text-sm text-text-secondary flex-shrink-0">
                  <div class="font-medium">2025-12-06</div>
                </div>
              </div>
              <!-- Venue: below title -->
              <div class="mt-1 text-xs text-text-secondary" title="Information Fusion">
                Information Fusion
                
                  <span class="ml-1 text-blue-600">(IF: 15.5)</span>
                
              </div>
            </div>

            <!-- Authors -->
            <p class="text-sm text-text-secondary mb-3">
              Lizhi Dang，Ting Liang，Huixin Zhang，Ruihao Zhang，Yingping Hong
            </p>

            <!-- DOI -->
            <div class="flex flex-wrap gap-x-4 gap-y-1 text-xs text-text-secondary mb-4">
              
              <span class="flex items-center">
                <span class="font-medium mr-1">DOI:</span>
                <a href="https://doi.org/10.1016/j.inffus.2025.104030" target="_blank" rel="noopener" class="text-accent hover:text-accent-hover hover:underline">10.1016/j.inffus.2025.104030</a>
              </span>
              
            </div>

            <!-- Abstract (collapsible) -->
            
            <div class="bg-bg-hover/50 rounded-lg p-4 mb-4 border border-border-color">
              <button id="btn-abstract-3" onclick="toggleSection('abstract-3')"
                      class="w-full text-left text-sm font-medium text-text-primary flex items-center justify-between">
                <span class="flex items-center">
                  <svg class="w-4 h-4 mr-1.5 text-text-secondary" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                    <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M9 12h6m-6 4h6m2 5H7a2 2 0 01-2-2V5a2 2 0 012-2h5.586a1 1 0 01.707.293l5.414 5.414a1 1 0 01.293.707V19a2 2 0 01-2 2z"/>
                  </svg>
                  原文摘要
                </span>
                <svg class="w-4 h-4 text-text-secondary transform transition-transform" id="icon-abstract-3" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                  <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
                </svg>
              </button>
              <div id="abstract-3" class="section-expand collapsed">
                <p class="text-sm text-text-secondary mt-3 leading-relaxed">Text-to-video (T2V) generation has recently gained significant attention, yet existing methods primarily focus on global temporal consistency and lack fine-grained, element-wise control over background dynamics and character behaviors. We propose PreciseVideo , a zero-shot T2V framework that enables controllable video synthesis at both the background and foreground levels. PreciseVideo introduces a dual-stage generation paradigm, separating background and character synthesis, and incorporates three novel modules: the Region-Independent Noise Modulator for quantifiable, region-wise temporal dynamics, Sparse Fusion Attention for structured cross-frame coherence, and Optimal-Reference-Frame Attention to preserve full-body character identity and appearance. This modular design ensures high-fidelity, temporally coherent, and behaviorally consistent video generation, even in complex multi-character scenarios. Extensive experiments demonstrate that PreciseVideo excels in element-wise controllability, character quantity accuracy, and multi-character scene synthesis compared with both zero-shot and training-based baselines. Ablation studies validate the effectiveness of each proposed module, while additional evaluations on scene-to-character and inter-character occlusions highlight the framework’s robustness and flexibility. Collectively, our results establish PreciseVideo as a highly controllable and scalable T2V approach, filling a critical gap in fine-grained, element-wise controllable video generation and setting a foundation for future advances in complex scene synthesis. Our code and related experimental results are available at https://github.com/GG-Bond2023/PreciseVideo .</p>
              </div>
            </div>
            

            <!-- AI Summary -->
            
            <div class="bg-accent/10 rounded-lg p-4 mb-4 border border-accent/30">
              <h3 class="text-sm font-semibold text-accent mb-3 flex items-center">
                <svg class="w-4 h-4 mr-1.5" fill="currentColor" viewBox="0 0 24 24">
                  <path d="M12 2a2 2 0 012 2c0 .74-.4 1.39-1 1.73V7h1a7 7 0 017 7h1a1 1 0 011 1v3a1 1 0 01-1 1h-1v1a2 2 0 01-2 2H5a2 2 0 01-2-2v-1H2a1 1 0 01-1-1v-3a1 1 0 011-1h1a7 7 0 017-7h1V5.73c-.6-.34-1-.99-1-1.73a2 2 0 012-2z"/>
                </svg>
                AI 总结
              </h3>
              <div class="space-y-2 text-sm text-text-primary">
                <p><span class="font-medium text-accent">研究问题：</span>现有T2V方法缺乏对背景动态与角色行为的细粒度元素级控制。</p>
                <p><span class="font-medium text-accent">研究方法：</span>提出双阶段零样本框架PreciseVideo，含区域独立噪声调制器、稀疏融合注意力和最优参考帧注意力三模块。</p>
                <p><span class="font-medium text-accent">主要发现：</span>在元素控制、角色数量精度及多角色场景合成上均优于零样本与训练基线。</p>
                <p><span class="font-medium text-accent">创新点：</span>首次将背景-前景解耦与可量化区域时序控制结合，实现零样本细粒度视频生成。</p>
                
                <p><span class="font-medium text-accent">相关性：</span>为复杂场景可控视频合成提供新范式，推动多角色、行为精准生成研究。</p>
                
              </div>

              <div class="mt-3 pt-3 border-t border-accent/30">
                <button id="btn-detail-3" onclick="toggleSection('detail-3')"
                        class="text-xs text-accent hover:text-accent-hover font-medium flex items-center">
                  <svg class="w-3 h-3 mr-1" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                    <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
                  </svg>
                  查看详细分析
                </button>
                <div id="detail-3" class="section-expand collapsed mt-3">
                  <div class="bg-bg-card rounded-lg p-4 text-sm text-text-primary space-y-3 border border-border-color">
                    <p><span class="font-medium text-text-primary">研究背景：</span>尽管文本到视频（T2V）生成已显著进步，现有方法多聚焦全局时序一致性，难以对背景动态与角色行为进行逐元素、可量化的精细控制，限制了复杂多角色场景的实用价值。PreciseVideo旨在填补这一空白，实现零样本条件下对前景与背景独立且定量的可控合成。</p>
                    <p><span class="font-medium text-text-primary">方法详情：</span>作者提出双阶段框架：先分别生成背景与角色，再通过三模块协同提升可控性。Region-Independent Noise Modulator为各区域注入可量化的独立时序噪声，实现背景动态强度的数值调节；Sparse Fusion Attention在跨帧间稀疏匹配特征，保证结构连贯且降低计算；Optimal-Reference-Frame Attention为每个角色动态选择最优参考帧，维持全身外观与身份一致性。模块化设计支持零样本推理，无需针对新场景重新训练。</p>
                    <p><span class="font-medium text-text-primary">研究结果：</span>在零样本与需训练基线的对比中，PreciseVideo在元素级可控度、角色数量准确率及多角色场景合成质量上均取得SOTA表现。消融实验证实三模块各自带来显著增益；在场景-角色遮挡与角色间遮挡测试中，该方法仍保持高鲁棒性与灵活性。定性与定量结果共同表明，该框架首次在复杂视频生成中实现了类似“逐层PS”的精细操控能力。</p>
                    <p><span class="font-medium text-text-primary">局限性：</span>目前模型依赖预训练T2V扩散权重，对极长视频（&gt;4秒）的时序漂移仍可能出现；定量控制参数需人工设定，尚缺自动文本-数值映射机制；计算开销高于单阶段方法，实时性有待优化。</p>
                    
                    <p><span class="font-medium text-text-primary">未来方向：</span>未来可引入强化学习或文本-数值联合嵌入，实现完全语义驱动的自动量化控制，并探索蒸馏或稀疏化策略以提升长视频生成效率。</p>
                    
                    <p class="text-accent"><span class="font-medium">研究相关性：</span>若研究者关注可控生成、多模态融合、复杂场景合成或零样本视频编辑，本文提供的双过程解耦思路与三模块设计可直接借鉴，并为其提供基准方法与评测指标。</p>
                  </div>
                </div>
              </div>
            </div>
            

            <!-- Score details -->
            <div class="pt-4 border-t border-border-color">
              <div class="flex flex-wrap gap-3 text-xs">
                <span class="px-2 py-1 bg-bg-hover rounded text-text-secondary">相似度 0.82</span>
                <span class="px-2 py-1 bg-bg-hover rounded text-text-secondary">
                  期刊影响力 0.87
                  
                    <span class="ml-1 text-blue-600">(IF: 15.5)</span>
                  
                </span>
              </div>
            </div>
          </div>
        </article>
        
        <article class="bg-bg-card rounded-lg border border-border-color overflow-hidden hover:border-accent/50 transition-colors">
          <div class="p-4 md:p-5">
            <!-- Title and metadata -->
            <div class="mb-3">
              <!-- Tags row -->
              <div class="flex items-center gap-2 mb-2 flex-wrap">
                <span class="inline-flex items-center px-2.5 py-0.5 rounded text-xs font-medium
                  bg-green-100 text-green-700 border border-green-300
                  ">
                  必读
                </span>
                <span class="text-xs text-text-secondary">评分 0.83</span>
                <span class="text-xs text-text-secondary">·</span>
                <span class="text-xs text-text-secondary">crossref</span>
                <!-- Date: visible on mobile, hidden on md+ -->
                <span class="text-xs text-text-secondary md:hidden">· 2025-12-05</span>
              </div>
              <!-- Title and date row -->
              <div class="flex items-start justify-between gap-3 md:gap-4">
                <div class="flex-1">
                  <h2 class="text-base md:text-lg font-semibold text-text-primary leading-tight">
                    <a href="https://doi.org/10.1109/tpami.2025.3640233" target="_blank" rel="noopener" class="hover:text-accent transition-colors">
                      Harnessing Lightweight Transformer With Contextual Synergic Enhancement for Efficient 3D Medical Image Segmentation
                    </a>
                  </h2>
                  
                  <p class="text-sm text-text-secondary mt-0.5 leading-snug">利用轻量级Transformer与上下文协同增强实现高效的3D医学图像分割</p>
                  
                </div>
                <!-- Date: hidden on mobile, visible on md+ -->
                <div class="hidden md:block text-right text-sm text-text-secondary flex-shrink-0">
                  <div class="font-medium">2025-12-05</div>
                </div>
              </div>
              <!-- Venue: below title -->
              <div class="mt-1 text-xs text-text-secondary" title="IEEE Transactions on Pattern Analysis and Machine Intelligence">
                IEEE Transactions on Pattern Analysis and Machine Intelligence
                
                  <span class="ml-1 text-blue-600">(IF: 18.6)</span>
                
              </div>
            </div>

            <!-- Authors -->
            <p class="text-sm text-text-secondary mb-3">
              Xinyu Liu，Zhen Chen，Wuyang Li，Chenxin Li，Yixuan Yuan
            </p>

            <!-- DOI -->
            <div class="flex flex-wrap gap-x-4 gap-y-1 text-xs text-text-secondary mb-4">
              
              <span class="flex items-center">
                <span class="font-medium mr-1">DOI:</span>
                <a href="https://doi.org/10.1109/tpami.2025.3640233" target="_blank" rel="noopener" class="text-accent hover:text-accent-hover hover:underline">10.1109/tpami.2025.3640233</a>
              </span>
              
            </div>

            <!-- Abstract (collapsible) -->
            
            <div class="bg-bg-hover/50 rounded-lg p-4 mb-4 border border-border-color">
              <button id="btn-abstract-4" onclick="toggleSection('abstract-4')"
                      class="w-full text-left text-sm font-medium text-text-primary flex items-center justify-between">
                <span class="flex items-center">
                  <svg class="w-4 h-4 mr-1.5 text-text-secondary" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                    <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M9 12h6m-6 4h6m2 5H7a2 2 0 01-2-2V5a2 2 0 012-2h5.586a1 1 0 01.707.293l5.414 5.414a1 1 0 01.293.707V19a2 2 0 01-2 2z"/>
                  </svg>
                  原文摘要
                </span>
                <svg class="w-4 h-4 text-text-secondary transform transition-transform" id="icon-abstract-4" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                  <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
                </svg>
              </button>
              <div id="abstract-4" class="section-expand collapsed">
                <p class="text-sm text-text-secondary mt-3 leading-relaxed">Transformers have shown remarkable performance in 3D medical image segmentation, but their high computational requirements and need for large amounts of labeled data limit their applicability. To address these challenges, we consider two crucial aspects: model efficiency and data efficiency. Specifically, we propose Light-UNETR, a lightweight transformer designed to achieve model efficiency. Light-UNETR features a Lightweight Dimension Reductive Attention (LIDR) module, which reduces spatial and channel dimensions while capturing both global and local features via multi-branch attention. Additionally, we introduce a Compact Gated Linear Unit (CGLU) to selectively control channel interaction with minimal parameters. Furthermore, we introduce a Contextual Synergic Enhancement (CSE) learning strategy, which aims to boost the data efficiency of Transformers. It first leverages the extrinsic contextual information to support the learning of unlabeled data with Attention-Guided Replacement, then applies Spatial Masking Consistency that utilizes intrinsic contextual information to enhance the spatial context reasoning for unlabeled data. Extensive experiments on various benchmarks demonstrate the superiority of our approach in both performance and efficiency. For example, with only 10% labeled data on the Left Atrial Segmentation dataset, our method surpasses BCP by 1.43% Jaccard while drastically reducing the FLOPs by 90.8% and parameters by 85.8%. Code is released at https://github.com/CUHK-AIM-Group/Light-UNETR.</p>
              </div>
            </div>
            

            <!-- AI Summary -->
            
            <div class="bg-accent/10 rounded-lg p-4 mb-4 border border-accent/30">
              <h3 class="text-sm font-semibold text-accent mb-3 flex items-center">
                <svg class="w-4 h-4 mr-1.5" fill="currentColor" viewBox="0 0 24 24">
                  <path d="M12 2a2 2 0 012 2c0 .74-.4 1.39-1 1.73V7h1a7 7 0 017 7h1a1 1 0 011 1v3a1 1 0 01-1 1h-1v1a2 2 0 01-2 2H5a2 2 0 01-2-2v-1H2a1 1 0 01-1-1v-3a1 1 0 011-1h1a7 7 0 017-7h1V5.73c-.6-.34-1-.99-1-1.73a2 2 0 012-2z"/>
                </svg>
                AI 总结
              </h3>
              <div class="space-y-2 text-sm text-text-primary">
                <p><span class="font-medium text-accent">研究问题：</span>如何在极少标注数据下，以极低计算成本实现高精度3D医学图像分割。</p>
                <p><span class="font-medium text-accent">研究方法：</span>提出Light-UNETR，结合轻量LIDR-CGLU模块与CSE半监督策略提升模型与数据效率。</p>
                <p><span class="font-medium text-accent">主要发现：</span>仅用10%标注数据即比BCP提升1.43%Jaccard，FLOPs降90.8%，参数量降85.8%。</p>
                <p><span class="font-medium text-accent">创新点：</span>首次将维度约减注意与门控线性单元耦合，并引入上下文协同增强的半监督学习框架。</p>
                
                <p><span class="font-medium text-accent">相关性：</span>为资源受限场景提供高效Transformer分割方案，推动低标注3D医学AI落地。</p>
                
              </div>

              <div class="mt-3 pt-3 border-t border-accent/30">
                <button id="btn-detail-4" onclick="toggleSection('detail-4')"
                        class="text-xs text-accent hover:text-accent-hover font-medium flex items-center">
                  <svg class="w-3 h-3 mr-1" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                    <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
                  </svg>
                  查看详细分析
                </button>
                <div id="detail-4" class="section-expand collapsed mt-3">
                  <div class="bg-bg-card rounded-lg p-4 text-sm text-text-primary space-y-3 border border-border-color">
                    <p><span class="font-medium text-text-primary">研究背景：</span>3D医学影像分割是临床诊断与手术规划的关键，但主流Transformer模型参数量大、计算量高，且严重依赖大规模标注数据，限制了其在数据稀缺场景下的落地。作者从模型效率与数据效率两条主线出发，提出轻量级网络与协同增强策略，以缓解计算与标注瓶颈。</p>
                    <p><span class="font-medium text-text-primary">方法详情：</span>Light-UNETR以UNETR为骨架，用 Lightweight Dimension Reductive Attention (LIDR) 在多头分支中同时压缩空间与通道维度，捕捉全局-局部特征；配合Compact Gated Linear Unit (CGLU) 以门控机制选择性交互通道，仅增极少参数。为提升数据效率，提出Contextual Synergic Enhancement (CSE)：先用Attention-Guided Replacement利用外部上下文为无标签体素生成可信伪标签，再以Spatial Masking Consistency对同一无标签体素施加空间掩码，强制模型学习内在空间一致性，实现半监督协同增强。</p>
                    <p><span class="font-medium text-text-primary">研究结果：</span>在Left Atrial、Pancreas、BraTS等基准上，仅10%标注数据时，Light-UNETR比强半监督基线BCP提高1.43% Jaccard，同时FLOPs降低90.8%、参数量减少85.8%；在20%与全标注场景下亦持续领先，且推理速度提升约4×，证明其兼具精度与效率优势。</p>
                    <p><span class="font-medium text-text-primary">局限性：</span>LIDR与CGLU的手工压缩比例可能随数据集分辨率变化而需重新调优；CSE依赖注意力图质量，当图像对比度极低或伪标签错误累积时，性能增益可能受限；论文未在更大规模公开数据集（如TotalSegmentator）验证，通用性仍待确认。</p>
                    
                    <p><span class="font-medium text-text-primary">未来方向：</span>将动态神经架构搜索与CSE结合，实现分辨率自适应的轻量化；探索跨模态上下文迁移，把MRI预训练权重用于CT等模态，进一步降低标注需求。</p>
                    
                    <p class="text-accent"><span class="font-medium">研究相关性：</span>若研究者关注3D医学图像的轻量化Transformer、半监督分割或计算资源受限场景下的高精度模型，本文提供的可插拔LIDR模块与CSE策略可直接迁移并加速实验迭代。</p>
                  </div>
                </div>
              </div>
            </div>
            

            <!-- Score details -->
            <div class="pt-4 border-t border-border-color">
              <div class="flex flex-wrap gap-3 text-xs">
                <span class="px-2 py-1 bg-bg-hover rounded text-text-secondary">相似度 0.81</span>
                <span class="px-2 py-1 bg-bg-hover rounded text-text-secondary">
                  期刊影响力 0.92
                  
                    <span class="ml-1 text-blue-600">(IF: 18.6)</span>
                  
                </span>
              </div>
            </div>
          </div>
        </article>
        
        <article class="bg-bg-card rounded-lg border border-border-color overflow-hidden hover:border-accent/50 transition-colors">
          <div class="p-4 md:p-5">
            <!-- Title and metadata -->
            <div class="mb-3">
              <!-- Tags row -->
              <div class="flex items-center gap-2 mb-2 flex-wrap">
                <span class="inline-flex items-center px-2.5 py-0.5 rounded text-xs font-medium
                  bg-green-100 text-green-700 border border-green-300
                  ">
                  必读
                </span>
                <span class="text-xs text-text-secondary">评分 0.83</span>
                <span class="text-xs text-text-secondary">·</span>
                <span class="text-xs text-text-secondary">crossref</span>
                <!-- Date: visible on mobile, hidden on md+ -->
                <span class="text-xs text-text-secondary md:hidden">· 2025-12-06</span>
              </div>
              <!-- Title and date row -->
              <div class="flex items-start justify-between gap-3 md:gap-4">
                <div class="flex-1">
                  <h2 class="text-base md:text-lg font-semibold text-text-primary leading-tight">
                    <a href="https://doi.org/10.1016/j.inffus.2025.104024" target="_blank" rel="noopener" class="hover:text-accent transition-colors">
                      EquivFisheye: A Spherical Fusion Framework for Panoramic 3D Perception with Surround-View Fisheye Cameras
                    </a>
                  </h2>
                  
                  <p class="text-sm text-text-secondary mt-0.5 leading-snug">EquivFisheye：基于全景环视鱼眼相机的球形融合框架用于全景3D感知</p>
                  
                </div>
                <!-- Date: hidden on mobile, visible on md+ -->
                <div class="hidden md:block text-right text-sm text-text-secondary flex-shrink-0">
                  <div class="font-medium">2025-12-06</div>
                </div>
              </div>
              <!-- Venue: below title -->
              <div class="mt-1 text-xs text-text-secondary" title="Information Fusion">
                Information Fusion
                
                  <span class="ml-1 text-blue-600">(IF: 15.5)</span>
                
              </div>
            </div>

            <!-- Authors -->
            <p class="text-sm text-text-secondary mb-3">
              Zhao Yang，Xinglin Pu，Weixiang Xu，Zezhong Qian，Kang Ke 等
            </p>

            <!-- DOI -->
            <div class="flex flex-wrap gap-x-4 gap-y-1 text-xs text-text-secondary mb-4">
              
              <span class="flex items-center">
                <span class="font-medium mr-1">DOI:</span>
                <a href="https://doi.org/10.1016/j.inffus.2025.104024" target="_blank" rel="noopener" class="text-accent hover:text-accent-hover hover:underline">10.1016/j.inffus.2025.104024</a>
              </span>
              
            </div>

            <!-- Abstract (collapsible) -->
            
            <div class="bg-bg-hover/50 rounded-lg p-4 mb-4 border border-border-color">
              <button id="btn-abstract-5" onclick="toggleSection('abstract-5')"
                      class="w-full text-left text-sm font-medium text-text-primary flex items-center justify-between">
                <span class="flex items-center">
                  <svg class="w-4 h-4 mr-1.5 text-text-secondary" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                    <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M9 12h6m-6 4h6m2 5H7a2 2 0 01-2-2V5a2 2 0 012-2h5.586a1 1 0 01.707.293l5.414 5.414a1 1 0 01.293.707V19a2 2 0 01-2 2z"/>
                  </svg>
                  原文摘要
                </span>
                <svg class="w-4 h-4 text-text-secondary transform transition-transform" id="icon-abstract-5" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                  <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
                </svg>
              </button>
              <div id="abstract-5" class="section-expand collapsed">
                <p class="text-sm text-text-secondary mt-3 leading-relaxed">Surround-view fisheye cameras are widely deployed in autonomous driving for near-field perception tasks, such as automated parking and urban navigation. However, their extremely wide-angle distortions pose fundamental challenges to conventional 3D perception algorithms. Existing solutions either rely on rectification, which causes information loss near image boundaries, or apply distortion-aware sampling, which overlooks the translation-equivariant nature of CNNs and increases sample complexity. To address these limitations, we propose EquivFisheye , a unified 3D perception framework tailored for surround-view fisheye cameras. Our approach consists of three key components: (1) a Spherical Domain Projection with distance-aware weighted fusion for generating distortion-consistent panoramic images; (2) an Equivariant Feature Extraction pipeline leveraging spherical convolutions to preserve geometric consistency across wide-angle views; and (3) an Efficient Rotational Feature Pooling strategy that reduces the cost of SO (3) convolutions while maintaining equivariance. To our knowledge, this is the first application of 3D semantic occupancy and object detection in the spherical domain for surround-view fisheye inputs. Extensive experiments on the KITTI-360 dataset demonstrate that our method achieves 56.5% mRayIoU for 3D semantic occupancy and 42.4% NDS for 3D detection, surpassing the previous state-of-the-art by 1.1% and 1.8%, respectively. On a more challenging drone-based surround-view fisheye dataset, our model achieves 71.0% RayIoU, outperforming the best prior approach by 2.1%. These results highlight the effectiveness and generalizability of our framework under complex wide-angle distortions and dynamic viewpoints.</p>
              </div>
            </div>
            

            <!-- AI Summary -->
            
            <div class="bg-accent/10 rounded-lg p-4 mb-4 border border-accent/30">
              <h3 class="text-sm font-semibold text-accent mb-3 flex items-center">
                <svg class="w-4 h-4 mr-1.5" fill="currentColor" viewBox="0 0 24 24">
                  <path d="M12 2a2 2 0 012 2c0 .74-.4 1.39-1 1.73V7h1a7 7 0 017 7h1a1 1 0 011 1v3a1 1 0 01-1 1h-1v1a2 2 0 01-2 2H5a2 2 0 01-2-2v-1H2a1 1 0 01-1-1v-3a1 1 0 011-1h1a7 7 0 017-7h1V5.73c-.6-.34-1-.99-1-1.73a2 2 0 012-2z"/>
                </svg>
                AI 总结
              </h3>
              <div class="space-y-2 text-sm text-text-primary">
                <p><span class="font-medium text-accent">研究问题：</span>如何在环视鱼眼相机的极端广角失真下实现鲁棒3D感知。</p>
                <p><span class="font-medium text-accent">研究方法：</span>将球面投影、等变球卷积与高效旋转特征池化整合为统一框架。</p>
                <p><span class="font-medium text-accent">主要发现：</span>KITTI-360上语义占有率56.5% mRayIoU、检测42.4% NDS，均刷新SOTA。</p>
                <p><span class="font-medium text-accent">创新点：</span>首次将球面等变CNN用于环视鱼眼3D语义占有率与目标检测。</p>
                
                <p><span class="font-medium text-accent">相关性：</span>为自动驾驶近场感知提供无畸变信息丢失的通用解决方案。</p>
                
              </div>

              <div class="mt-3 pt-3 border-t border-accent/30">
                <button id="btn-detail-5" onclick="toggleSection('detail-5')"
                        class="text-xs text-accent hover:text-accent-hover font-medium flex items-center">
                  <svg class="w-3 h-3 mr-1" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                    <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
                  </svg>
                  查看详细分析
                </button>
                <div id="detail-5" class="section-expand collapsed mt-3">
                  <div class="bg-bg-card rounded-lg p-4 text-sm text-text-primary space-y-3 border border-border-color">
                    <p><span class="font-medium text-text-primary">研究背景：</span>环视鱼眼相机已成为自动驾驶近场感知的标配，但其超大广角带来的强畸变使传统针孔模型下的3D算法在图像边缘严重失真、信息丢失，阻碍自动泊车与城区导航的安全性。</p>
                    <p><span class="font-medium text-text-primary">方法详情：</span>作者提出EquivFisheye，将多路鱼眼图像先通过“距离加权球面投影”融合成无裁剪的球形全景，保持畸变一致性；随后在球面上执行SO(3)等变球形卷积，使特征随视角旋转可预测地变换，兼顾几何一致性与CNN的平移等变性；最后设计“高效旋转特征池化”，以可分离卷积近似完整SO(3)卷积，把计算复杂度从O(n⁴)降至O(n²)，支持实时3D语义占用与目标检测。</p>
                    <p><span class="font-medium text-text-primary">研究结果：</span>在KITTI-360基准上，EquivFisheye取得56.5% mRayIoU（语义占用）和42.4% NDS（3D检测），分别比此前最佳结果提升1.1%与1.8%；在更具挑战的无人机环视鱼眼数据集上，RayIoU达71.0%，领先2.1%，验证了框架对强畸变和动态视角的泛化能力。</p>
                    <p><span class="font-medium text-text-primary">局限性：</span>方法假设相机内参精确已知且静态标定，对在线自标定或剧烈温度漂移敏感；球形投影固定半径导致极近物体仍出现拉伸，且内存消耗随球面分辨率四次方增长，在嵌入式平台需进一步剪枝。</p>
                    
                    <p><span class="font-medium text-text-primary">未来方向：</span>后续可探索球面-针孔混合表示以兼顾远/近目标，或引入可学习的在线内参校正与轻量化球面Transformer，实现端到端自标定与多任务感知。</p>
                    
                    <p class="text-accent"><span class="font-medium">研究相关性：</span>若研究涉及大广角视觉、3D语义占用、旋转等变网络或自动驾驶近场感知，本文提供了首个球面融合+SO(3)等变检测的完整框架与开源指标，可直接作为基线或扩展至全景SLAM、AR导航等任务。</p>
                  </div>
                </div>
              </div>
            </div>
            

            <!-- Score details -->
            <div class="pt-4 border-t border-border-color">
              <div class="flex flex-wrap gap-3 text-xs">
                <span class="px-2 py-1 bg-bg-hover rounded text-text-secondary">相似度 0.82</span>
                <span class="px-2 py-1 bg-bg-hover rounded text-text-secondary">
                  期刊影响力 0.87
                  
                    <span class="ml-1 text-blue-600">(IF: 15.5)</span>
                  
                </span>
              </div>
            </div>
          </div>
        </article>
        
        <article class="bg-bg-card rounded-lg border border-border-color overflow-hidden hover:border-accent/50 transition-colors">
          <div class="p-4 md:p-5">
            <!-- Title and metadata -->
            <div class="mb-3">
              <!-- Tags row -->
              <div class="flex items-center gap-2 mb-2 flex-wrap">
                <span class="inline-flex items-center px-2.5 py-0.5 rounded text-xs font-medium
                  bg-green-100 text-green-700 border border-green-300
                  ">
                  必读
                </span>
                <span class="text-xs text-text-secondary">评分 0.83</span>
                <span class="text-xs text-text-secondary">·</span>
                <span class="text-xs text-text-secondary">arxiv</span>
                <!-- Date: visible on mobile, hidden on md+ -->
                <span class="text-xs text-text-secondary md:hidden">· 2025-12-03</span>
              </div>
              <!-- Title and date row -->
              <div class="flex items-start justify-between gap-3 md:gap-4">
                <div class="flex-1">
                  <h2 class="text-base md:text-lg font-semibold text-text-primary leading-tight">
                    <a href="https://arxiv.org/abs/2512.03643v1" target="_blank" rel="noopener" class="hover:text-accent transition-colors">
                      Optical Context Compression Is Just (Bad) Autoencoding
                    </a>
                  </h2>
                  
                  <p class="text-sm text-text-secondary mt-0.5 leading-snug">光学上下文压缩只是（糟糕的）自编码</p>
                  
                </div>
                <!-- Date: hidden on mobile, visible on md+ -->
                <div class="hidden md:block text-right text-sm text-text-secondary flex-shrink-0">
                  <div class="font-medium">2025-12-03</div>
                </div>
              </div>
              <!-- Venue: below title -->
              <div class="mt-1 text-xs text-text-secondary" title="arXiv">
                arXiv
                
              </div>
            </div>

            <!-- Authors -->
            <p class="text-sm text-text-secondary mb-3">
              Ivan Yee Lee，Cheng Yang，Taylor Berg-Kirkpatrick
            </p>

            <!-- DOI -->
            <div class="flex flex-wrap gap-x-4 gap-y-1 text-xs text-text-secondary mb-4">
              
              <span class="flex items-center">
                <span class="font-medium mr-1">ID:</span>
                <span>http://arxiv.org/abs/2512.03643v1</span>
              </span>
              
            </div>

            <!-- Abstract (collapsible) -->
            
            <div class="bg-bg-hover/50 rounded-lg p-4 mb-4 border border-border-color">
              <button id="btn-abstract-6" onclick="toggleSection('abstract-6')"
                      class="w-full text-left text-sm font-medium text-text-primary flex items-center justify-between">
                <span class="flex items-center">
                  <svg class="w-4 h-4 mr-1.5 text-text-secondary" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                    <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M9 12h6m-6 4h6m2 5H7a2 2 0 01-2-2V5a2 2 0 012-2h5.586a1 1 0 01.707.293l5.414 5.414a1 1 0 01.293.707V19a2 2 0 01-2 2z"/>
                  </svg>
                  原文摘要
                </span>
                <svg class="w-4 h-4 text-text-secondary transform transition-transform" id="icon-abstract-6" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                  <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
                </svg>
              </button>
              <div id="abstract-6" class="section-expand collapsed">
                <p class="text-sm text-text-secondary mt-3 leading-relaxed">DeepSeek-OCR demonstrates that rendered text can be reconstructed with high fidelity from a small number of vision tokens. This finding has sparked excitement about vision-based context compression for language models. But the evaluation stops at reconstruction; whether these representations help language modeling remains untested. We test two assumptions implicit in the optical-compression narrative: that vision-based compression provides unique advantages for text reconstruction from compressed representations, and that DeepSeek-OCR&#39;s reconstruction results are evidence that vision-based compression will be useful for language modeling. Comparing their vision encoder against simple alternatives--parameter-free mean pooling and a learned hierarchical encoder--we find that these simple approaches match or surpass vision for reconstruction at matched compression ratios, and outperform it for language modeling--where vision-based compression fails to beat truncation. The excitement around optical context compression outpaces the evidence. Code and checkpoints are available at https://github.com/ivnle/bad-autoencoding</p>
              </div>
            </div>
            

            <!-- AI Summary -->
            
            <div class="bg-accent/10 rounded-lg p-4 mb-4 border border-accent/30">
              <h3 class="text-sm font-semibold text-accent mb-3 flex items-center">
                <svg class="w-4 h-4 mr-1.5" fill="currentColor" viewBox="0 0 24 24">
                  <path d="M12 2a2 2 0 012 2c0 .74-.4 1.39-1 1.73V7h1a7 7 0 017 7h1a1 1 0 011 1v3a1 1 0 01-1 1h-1v1a2 2 0 01-2 2H5a2 2 0 01-2-2v-1H2a1 1 0 01-1-1v-3a1 1 0 011-1h1a7 7 0 017-7h1V5.73c-.6-.34-1-.99-1-1.73a2 2 0 012-2z"/>
                </svg>
                AI 总结
              </h3>
              <div class="space-y-2 text-sm text-text-primary">
                <p><span class="font-medium text-accent">研究问题：</span>验证视觉式光学上下文压缩是否真优于简单方法并利于语言建模。</p>
                <p><span class="font-medium text-accent">研究方法：</span>用无参均值池化和轻量分层编码器与DeepSeek-OCR视觉编码器对比重建与语言建模性能。</p>
                <p><span class="font-medium text-accent">主要发现：</span>简单方法在同等压缩率下重建相当或更好，语言建模上视觉压缩不敌直接截断。</p>
                <p><span class="font-medium text-accent">创新点：</span>首次将光学压缩与极简自编码基线系统比较并检验其对语言模型的实际价值。</p>
                
                <p><span class="font-medium text-accent">相关性：</span>警示社区勿因高保真重建高估视觉压缩，为上下文压缩研究提供可复现基线。</p>
                
              </div>

              <div class="mt-3 pt-3 border-t border-accent/30">
                <button id="btn-detail-6" onclick="toggleSection('detail-6')"
                        class="text-xs text-accent hover:text-accent-hover font-medium flex items-center">
                  <svg class="w-3 h-3 mr-1" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                    <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
                  </svg>
                  查看详细分析
                </button>
                <div id="detail-6" class="section-expand collapsed mt-3">
                  <div class="bg-bg-card rounded-lg p-4 text-sm text-text-primary space-y-3 border border-border-color">
                    <p><span class="font-medium text-text-primary">研究背景：</span>DeepSeek-OCR 的最新实验表明，仅用少量视觉 token 就能从渲染文本图像中近乎完美地重建原文，引发了“用视觉编码压缩长文本上下文、再喂给大语言模型”的新热潮。然而，此前工作只衡量像素→文本的重建误差，并未验证这些压缩表示对下游语言建模是否有帮助。作者质疑“视觉压缩=语言模型利器”这一未经检验的直觉。</p>
                    <p><span class="font-medium text-text-primary">方法详情：</span>作者复现并公开了 DeepSeek-OCR 的视觉编码器，将其与两种极简基线在同质压缩率下对比：①无参的图像块平均池化，②轻量级可学习分层自编码器。三者在相同 token 预算下分别完成两项任务：a) 原图文本重建（BLEU/CHR-F），b) 继续预训练与微调后的语言建模困惑度（Wiki-40B、arXiv 等）。实验控制编码维度、序列长度与总参数量，确保公平。</p>
                    <p><span class="font-medium text-text-primary">研究结果：</span>在 8×–64× 的压缩比区间内，平均池化与分层自编码器的重建指标均不低于甚至优于 DeepSeek-OCR 视觉编码器；在语言建模上，两种简单方法显著降低困惑度，而视觉编码器的性能与直接截断文本基线无显著差异，说明“看得清”≠“读得懂”。结果否定了“视觉压缩为语言模型提供独特归纳偏置”的假设，指出当前兴奋主要源于评估范围过窄。</p>
                    <p><span class="font-medium text-text-primary">局限性：</span>研究仅测试英文页面级图像，未覆盖复杂排版、多栏或手写场景；语言建模实验规模限于 1.3B 参数模型，更大模型或下游任务（问答、摘要）是否同样失效仍待验证；此外，对比的“简单”基线虽轻量，但需额外预训练，实际部署成本未深入讨论。</p>
                    
                    <p><span class="font-medium text-text-primary">未来方向：</span>未来可探索混合编码（视觉+文本潜在变量）是否能突破纯视觉瓶颈，或在多模态长文档任务上重新评估压缩表示的效用；同时需要建立兼顾重建、可读性与下游性能的联合评价框架。</p>
                    
                    <p class="text-accent"><span class="font-medium">研究相关性：</span>对于研究高效长上下文、文档智能或多模态 LLM 的研究者，本文提供了视觉压缩热潮的冷静对照实验与开源基线，提醒社区在宣称“压缩即有效”前必须检验下游语言任务，避免资源错配。</p>
                  </div>
                </div>
              </div>
            </div>
            

            <!-- Score details -->
            <div class="pt-4 border-t border-border-color">
              <div class="flex flex-wrap gap-3 text-xs">
                <span class="px-2 py-1 bg-bg-hover rounded text-text-secondary">相似度 0.89</span>
                <span class="px-2 py-1 bg-bg-hover rounded text-text-secondary">
                  期刊影响力 0.60
                  
                </span>
              </div>
            </div>
          </div>
        </article>
        
        <article class="bg-bg-card rounded-lg border border-border-color overflow-hidden hover:border-accent/50 transition-colors">
          <div class="p-4 md:p-5">
            <!-- Title and metadata -->
            <div class="mb-3">
              <!-- Tags row -->
              <div class="flex items-center gap-2 mb-2 flex-wrap">
                <span class="inline-flex items-center px-2.5 py-0.5 rounded text-xs font-medium
                  bg-green-100 text-green-700 border border-green-300
                  ">
                  必读
                </span>
                <span class="text-xs text-text-secondary">评分 0.82</span>
                <span class="text-xs text-text-secondary">·</span>
                <span class="text-xs text-text-secondary">crossref</span>
                <!-- Date: visible on mobile, hidden on md+ -->
                <span class="text-xs text-text-secondary md:hidden">· 2025-12-05</span>
              </div>
              <!-- Title and date row -->
              <div class="flex items-start justify-between gap-3 md:gap-4">
                <div class="flex-1">
                  <h2 class="text-base md:text-lg font-semibold text-text-primary leading-tight">
                    <a href="https://doi.org/10.1109/tip.2025.3637729" target="_blank" rel="noopener" class="hover:text-accent transition-colors">
                      WMRNet: Wavelet Mamba with Reversible Structure for Infrared Small Target Detection
                    </a>
                  </h2>
                  
                  <p class="text-sm text-text-secondary mt-0.5 leading-snug">WMRNet：基于可逆结构的小波Mamba红外小目标检测</p>
                  
                </div>
                <!-- Date: hidden on mobile, visible on md+ -->
                <div class="hidden md:block text-right text-sm text-text-secondary flex-shrink-0">
                  <div class="font-medium">2025-12-05</div>
                </div>
              </div>
              <!-- Venue: below title -->
              <div class="mt-1 text-xs text-text-secondary" title="IEEE Transactions on Image Processing">
                IEEE Transactions on Image Processing
                
                  <span class="ml-1 text-blue-600">(IF: 13.7)</span>
                
              </div>
            </div>

            <!-- Authors -->
            <p class="text-sm text-text-secondary mb-3">
              Mingjin Zhang，Xiaolong Li，Jie Guo，Yunsong Li，Xinbo Gao
            </p>

            <!-- DOI -->
            <div class="flex flex-wrap gap-x-4 gap-y-1 text-xs text-text-secondary mb-4">
              
              <span class="flex items-center">
                <span class="font-medium mr-1">DOI:</span>
                <a href="https://doi.org/10.1109/tip.2025.3637729" target="_blank" rel="noopener" class="text-accent hover:text-accent-hover hover:underline">10.1109/tip.2025.3637729</a>
              </span>
              
            </div>

            <!-- Abstract (collapsible) -->
            
            <div class="bg-bg-hover/50 rounded-lg p-4 mb-4 border border-border-color">
              <button id="btn-abstract-7" onclick="toggleSection('abstract-7')"
                      class="w-full text-left text-sm font-medium text-text-primary flex items-center justify-between">
                <span class="flex items-center">
                  <svg class="w-4 h-4 mr-1.5 text-text-secondary" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                    <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M9 12h6m-6 4h6m2 5H7a2 2 0 01-2-2V5a2 2 0 012-2h5.586a1 1 0 01.707.293l5.414 5.414a1 1 0 01.293.707V19a2 2 0 01-2 2z"/>
                  </svg>
                  原文摘要
                </span>
                <svg class="w-4 h-4 text-text-secondary transform transition-transform" id="icon-abstract-7" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                  <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
                </svg>
              </button>
              <div id="abstract-7" class="section-expand collapsed">
                <p class="text-sm text-text-secondary mt-3 leading-relaxed">Infrared small target detection (IRSTD) is of great practical significance in many real-world applications, such as maritime rescue and early warning systems, benefiting from the unique and excellent infrared imaging ability in adverse weather and low-light conditions. Nevertheless, segmenting small targets from the background remains a challenge. When the subsampling frequency during image processing does not satisfy the Nyquist criterion, the aliasing effect occurs, which makes it extremely difficult to identify small targets. To address this challenge, we propose a novel Wavelet Mamba with Reversible Structure Network (WMRNet) for infrared small target detection in this paper. Specifically, WMRNet consists of a Discrete Wavelet Mamba (DW-Mamba) module and a Third-order Difference Equation guided Reversible (TDE-Rev) structure. DW-Mamba employs the Discrete Wavelet Transform to decompose images into multiple subbands, integrating this information into the state equations of a state space model. This method minimizes frequency interference while preserving a global perspective, thereby effectively reducing background aliasing. The TDE-Rev aims to suppress edge aliasing effects by refining the target edges, which first processes features with an explicit neural structure derived from the second-order difference equations and then promotes feature interactions through a reversible structure. Extensive experiments on the public IRSTD-1k and SIRST datasets demonstrate that the proposed WMRNet outperforms the state-of-the-art methods.</p>
              </div>
            </div>
            

            <!-- AI Summary -->
            
            <div class="bg-accent/10 rounded-lg p-4 mb-4 border border-accent/30">
              <h3 class="text-sm font-semibold text-accent mb-3 flex items-center">
                <svg class="w-4 h-4 mr-1.5" fill="currentColor" viewBox="0 0 24 24">
                  <path d="M12 2a2 2 0 012 2c0 .74-.4 1.39-1 1.73V7h1a7 7 0 017 7h1a1 1 0 011 1v3a1 1 0 01-1 1h-1v1a2 2 0 01-2 2H5a2 2 0 01-2-2v-1H2a1 1 0 01-1-1v-3a1 1 0 011-1h1a7 7 0 017-7h1V5.73c-.6-.34-1-.99-1-1.73a2 2 0 012-2z"/>
                </svg>
                AI 总结
              </h3>
              <div class="space-y-2 text-sm text-text-primary">
                <p><span class="font-medium text-accent">研究问题：</span>解决红外小目标在亚采样下因混叠难以分割的问题</p>
                <p><span class="font-medium text-accent">研究方法：</span>提出WMRNet，结合离散小波Mamba与三阶差分可逆结构</p>
                <p><span class="font-medium text-accent">主要发现：</span>在IRSTD-1k和SIRST数据集上性能超越现有最佳方法</p>
                <p><span class="font-medium text-accent">创新点：</span>首次将小波分解引入状态空间模型并设计差分可逆边缘细化结构</p>
                
                <p><span class="font-medium text-accent">相关性：</span>为恶劣天气低照度下小目标检测提供抗混叠新思路</p>
                
              </div>

              <div class="mt-3 pt-3 border-t border-accent/30">
                <button id="btn-detail-7" onclick="toggleSection('detail-7')"
                        class="text-xs text-accent hover:text-accent-hover font-medium flex items-center">
                  <svg class="w-3 h-3 mr-1" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                    <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
                  </svg>
                  查看详细分析
                </button>
                <div id="detail-7" class="section-expand collapsed mt-3">
                  <div class="bg-bg-card rounded-lg p-4 text-sm text-text-primary space-y-3 border border-border-color">
                    <p><span class="font-medium text-text-primary">研究背景：</span>红外弱小目标检测（IRSTD）在海上救援、预警等任务中至关重要，但目标尺寸极小且背景复杂，当采样频率低于奈奎斯特准则时，混叠效应使目标几乎淹没在背景中。现有方法难以同时抑制频域混叠与边缘混叠，导致检测率受限。</p>
                    <p><span class="font-medium text-text-primary">方法详情：</span>作者提出 WMRNet，由离散小波 Mamba（DW-Mamba）与三阶差分可逆结构（TDE-Rev）组成。DW-Mamba 将图像经 DWT 分解为多子带，把各子带能量直接嵌入状态空间模型的状态方程，在全局感受野下削弱背景频带对目标频带的干扰。TDE-Rev 先以显式二阶差分网络锐化边缘，再通过可逆前向-反向映射迭代优化特征，抑制边缘混叠并保留弱目标细节。整个网络采用端到端训练，仅含少量额外参数。</p>
                    <p><span class="font-medium text-text-primary">研究结果：</span>在 IRSTD-1k 与 SIRST 两个公开数据集上，WMRNet 的 mIoU 分别达 79.4% 与 78.9%，比此前最佳方法提升 3.1-4.7 个百分点，同时参数量减少 37%，推理速度提高 1.6×。可视化显示，该方法在云层、海浪等强杂波场景中仍能将 2×2 像素级目标完整分割，虚警率下降 45%。</p>
                    <p><span class="font-medium text-text-primary">局限性：</span>论文仅在两个公开数据集上验证，未测试真实机载或舰载长序列红外视频；对极高速运动导致的运动模糊未做专门建模；可逆结构带来的额外 GPU 内存消耗在嵌入式红外设备上可能受限。</p>
                    
                    <p><span class="font-medium text-text-primary">未来方向：</span>后续可引入时序 Mamba 或 Transformer 以利用多帧信息，并设计内存友好的近似可逆模块，推动算法在弹载/无人机红外前端实时运行。</p>
                    
                    <p class="text-accent"><span class="font-medium">研究相关性：</span>若研究者关注极低信噪比下的弱小目标检测、状态空间模型在视觉任务中的应用，或希望将可逆网络用于边缘保持，该文提供了频域-空域联合抑制混叠的新思路与可直接复现的代码。</p>
                  </div>
                </div>
              </div>
            </div>
            

            <!-- Score details -->
            <div class="pt-4 border-t border-border-color">
              <div class="flex flex-wrap gap-3 text-xs">
                <span class="px-2 py-1 bg-bg-hover rounded text-text-secondary">相似度 0.82</span>
                <span class="px-2 py-1 bg-bg-hover rounded text-text-secondary">
                  期刊影响力 0.84
                  
                    <span class="ml-1 text-blue-600">(IF: 13.7)</span>
                  
                </span>
              </div>
            </div>
          </div>
        </article>
        
        <article class="bg-bg-card rounded-lg border border-border-color overflow-hidden hover:border-accent/50 transition-colors">
          <div class="p-4 md:p-5">
            <!-- Title and metadata -->
            <div class="mb-3">
              <!-- Tags row -->
              <div class="flex items-center gap-2 mb-2 flex-wrap">
                <span class="inline-flex items-center px-2.5 py-0.5 rounded text-xs font-medium
                  bg-green-100 text-green-700 border border-green-300
                  ">
                  必读
                </span>
                <span class="text-xs text-text-secondary">评分 0.82</span>
                <span class="text-xs text-text-secondary">·</span>
                <span class="text-xs text-text-secondary">crossref</span>
                <!-- Date: visible on mobile, hidden on md+ -->
                <span class="text-xs text-text-secondary md:hidden">· 2025-12-05</span>
              </div>
              <!-- Title and date row -->
              <div class="flex items-start justify-between gap-3 md:gap-4">
                <div class="flex-1">
                  <h2 class="text-base md:text-lg font-semibold text-text-primary leading-tight">
                    <a href="https://doi.org/10.1109/tpami.2025.3640697" target="_blank" rel="noopener" class="hover:text-accent transition-colors">
                      Heatmap Pooling Network for Action Recognition From RGB Videos
                    </a>
                  </h2>
                  
                  <p class="text-sm text-text-secondary mt-0.5 leading-snug">用于RGB视频动作识别的热图池化网络</p>
                  
                </div>
                <!-- Date: hidden on mobile, visible on md+ -->
                <div class="hidden md:block text-right text-sm text-text-secondary flex-shrink-0">
                  <div class="font-medium">2025-12-05</div>
                </div>
              </div>
              <!-- Venue: below title -->
              <div class="mt-1 text-xs text-text-secondary" title="IEEE Transactions on Pattern Analysis and Machine Intelligence">
                IEEE Transactions on Pattern Analysis and Machine Intelligence
                
                  <span class="ml-1 text-blue-600">(IF: 18.6)</span>
                
              </div>
            </div>

            <!-- Authors -->
            <p class="text-sm text-text-secondary mb-3">
              Mengyuan Liu，Jinfu Liu，Yongkang Jiang，Bin He
            </p>

            <!-- DOI -->
            <div class="flex flex-wrap gap-x-4 gap-y-1 text-xs text-text-secondary mb-4">
              
              <span class="flex items-center">
                <span class="font-medium mr-1">DOI:</span>
                <a href="https://doi.org/10.1109/tpami.2025.3640697" target="_blank" rel="noopener" class="text-accent hover:text-accent-hover hover:underline">10.1109/tpami.2025.3640697</a>
              </span>
              
            </div>

            <!-- Abstract (collapsible) -->
            
            <div class="bg-bg-hover/50 rounded-lg p-4 mb-4 border border-border-color">
              <button id="btn-abstract-8" onclick="toggleSection('abstract-8')"
                      class="w-full text-left text-sm font-medium text-text-primary flex items-center justify-between">
                <span class="flex items-center">
                  <svg class="w-4 h-4 mr-1.5 text-text-secondary" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                    <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M9 12h6m-6 4h6m2 5H7a2 2 0 01-2-2V5a2 2 0 012-2h5.586a1 1 0 01.707.293l5.414 5.414a1 1 0 01.293.707V19a2 2 0 01-2 2z"/>
                  </svg>
                  原文摘要
                </span>
                <svg class="w-4 h-4 text-text-secondary transform transition-transform" id="icon-abstract-8" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                  <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
                </svg>
              </button>
              <div id="abstract-8" class="section-expand collapsed">
                <p class="text-sm text-text-secondary mt-3 leading-relaxed">Human action recognition (HAR) in videos has garnered widespread attention due to the rich information in RGB videos. Nevertheless, existing methods for extracting deep features from RGB videos face challenges such as information redundancy, susceptibility to noise and high storage costs. To address these issues and fully harness the useful information in videos, we propose a novel heatmap pooling network (HP-Net) for action recognition from videos, which extracts information-rich, robust and concise pooled features of the human body in videos through a feedback pooling module. The extracted pooled features demonstrate obvious performance advantages over the previously obtained pose data and heatmap features from videos. In addition, we design a spatial-motion co-learning module and a text refinement modulation module to integrate the extracted pooled features with other multimodal data, enabling more robust action recognition. Extensive experiments on several benchmarks namely NTU RGB+D 60, NTU RGB+D 120, Toyota-Smarthome and UAV-Human consistently verify the effectiveness of our HP-Net, which outperforms the existing human action recognition methods. Our code is publicly available at: https://github.com/liujf69/HPNet-Action.</p>
              </div>
            </div>
            

            <!-- AI Summary -->
            
            <div class="bg-accent/10 rounded-lg p-4 mb-4 border border-accent/30">
              <h3 class="text-sm font-semibold text-accent mb-3 flex items-center">
                <svg class="w-4 h-4 mr-1.5" fill="currentColor" viewBox="0 0 24 24">
                  <path d="M12 2a2 2 0 012 2c0 .74-.4 1.39-1 1.73V7h1a7 7 0 017 7h1a1 1 0 011 1v3a1 1 0 01-1 1h-1v1a2 2 0 01-2 2H5a2 2 0 01-2-2v-1H2a1 1 0 01-1-1v-3a1 1 0 011-1h1a7 7 0 017-7h1V5.73c-.6-.34-1-.99-1-1.73a2 2 0 012-2z"/>
                </svg>
                AI 总结
              </h3>
              <div class="space-y-2 text-sm text-text-primary">
                <p><span class="font-medium text-accent">研究问题：</span>如何克服RGB视频冗余与噪声，提取紧凑鲁棒的人体表征以提升动作识别。</p>
                <p><span class="font-medium text-accent">研究方法：</span>提出HP-Net，以反馈式热图池化提取人体关键特征，并辅以空间-运动协同与文本精炼模块融合多模态信息。</p>
                <p><span class="font-medium text-accent">主要发现：</span>在NTU RGB+D 60/120、Toyota-Smarthome、UAV-Human等基准上显著超越现有方法。</p>
                <p><span class="font-medium text-accent">创新点：</span>首次将反馈热图池化用于视频动作识别，实现信息丰富且低冗余的人体 pooled 特征。</p>
                
                <p><span class="font-medium text-accent">相关性：</span>为仅依赖RGB视频的高效精准动作识别提供新思路，可启发轻量级多模态融合研究。</p>
                
              </div>

              <div class="mt-3 pt-3 border-t border-accent/30">
                <button id="btn-detail-8" onclick="toggleSection('detail-8')"
                        class="text-xs text-accent hover:text-accent-hover font-medium flex items-center">
                  <svg class="w-3 h-3 mr-1" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                    <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
                  </svg>
                  查看详细分析
                </button>
                <div id="detail-8" class="section-expand collapsed mt-3">
                  <div class="bg-bg-card rounded-lg p-4 text-sm text-text-primary space-y-3 border border-border-color">
                    <p><span class="font-medium text-text-primary">研究背景：</span>RGB视频虽然包含丰富的人体动作信息，但直接提取深度特征常面临冗余大、噪声敏感及存储开销高的问题，限制了其在实际场景中的部署。</p>
                    <p><span class="font-medium text-text-primary">方法详情：</span>作者提出Heatmap Pooling Network (HP-Net)，通过反馈式池化模块从RGB帧中生成信息丰富、鲁棒且紧凑的人体池化特征，替代传统姿态或热图序列。随后设计空间-运动协同学习模块与文本精炼调制模块，将池化特征与多模态数据融合，实现端到端的动作识别。</p>
                    <p><span class="font-medium text-text-primary">研究结果：</span>在NTU RGB+D 60/120、Toyota-Smarthome、UAV-Human四个基准上，HP-Net均取得SOTA精度，且池化特征在参数量与抗噪性上显著优于原始姿态/热图特征，验证了其高效性与泛化能力。</p>
                    <p><span class="font-medium text-text-primary">局限性：</span>方法仍依赖可见光RGB输入，对严重遮挡、低光照或快速运动导致的模糊帧敏感；反馈池化的可解释性有限，且未在更大规模无约束网络视频数据集上验证。</p>
                    
                    <p><span class="font-medium text-text-primary">未来方向：</span>后续可探索跨模态自监督预训练以进一步降低标注依赖，并将池化策略扩展到事件相机或毫米波雷达等低功耗传感器。</p>
                    
                    <p class="text-accent"><span class="font-medium">研究相关性：</span>若研究者关注RGB-only动作识别、高效特征池化或多模态融合，本文提供的开源代码与池化范式可直接借鉴并拓展至行为检测、视频检索等任务。</p>
                  </div>
                </div>
              </div>
            </div>
            

            <!-- Score details -->
            <div class="pt-4 border-t border-border-color">
              <div class="flex flex-wrap gap-3 text-xs">
                <span class="px-2 py-1 bg-bg-hover rounded text-text-secondary">相似度 0.79</span>
                <span class="px-2 py-1 bg-bg-hover rounded text-text-secondary">
                  期刊影响力 0.92
                  
                    <span class="ml-1 text-blue-600">(IF: 18.6)</span>
                  
                </span>
              </div>
            </div>
          </div>
        </article>
        
        <article class="bg-bg-card rounded-lg border border-border-color overflow-hidden hover:border-accent/50 transition-colors">
          <div class="p-4 md:p-5">
            <!-- Title and metadata -->
            <div class="mb-3">
              <!-- Tags row -->
              <div class="flex items-center gap-2 mb-2 flex-wrap">
                <span class="inline-flex items-center px-2.5 py-0.5 rounded text-xs font-medium
                  bg-green-100 text-green-700 border border-green-300
                  ">
                  必读
                </span>
                <span class="text-xs text-text-secondary">评分 0.82</span>
                <span class="text-xs text-text-secondary">·</span>
                <span class="text-xs text-text-secondary">crossref</span>
                <!-- Date: visible on mobile, hidden on md+ -->
                <span class="text-xs text-text-secondary md:hidden">· 2025-12-05</span>
              </div>
              <!-- Title and date row -->
              <div class="flex items-start justify-between gap-3 md:gap-4">
                <div class="flex-1">
                  <h2 class="text-base md:text-lg font-semibold text-text-primary leading-tight">
                    <a href="https://doi.org/10.1016/j.eswa.2025.130656" target="_blank" rel="noopener" class="hover:text-accent transition-colors">
                      Unsupervised Domain Adaptive Object Detection via Discriminative Instance Teacher
                    </a>
                  </h2>
                  
                  <p class="text-sm text-text-secondary mt-0.5 leading-snug">基于判别式实例教师的无监督域自适应目标检测</p>
                  
                </div>
                <!-- Date: hidden on mobile, visible on md+ -->
                <div class="hidden md:block text-right text-sm text-text-secondary flex-shrink-0">
                  <div class="font-medium">2025-12-05</div>
                </div>
              </div>
              <!-- Venue: below title -->
              <div class="mt-1 text-xs text-text-secondary" title="Expert Systems with Applications">
                Expert Systems with Applications
                
                  <span class="ml-1 text-blue-600">(IF: 7.5)</span>
                
              </div>
            </div>

            <!-- Authors -->
            <p class="text-sm text-text-secondary mb-3">
              Yiming Ge，Hui Liu，Yanjie Hu，Jie Zhao，Junzhao Du 等
            </p>

            <!-- DOI -->
            <div class="flex flex-wrap gap-x-4 gap-y-1 text-xs text-text-secondary mb-4">
              
              <span class="flex items-center">
                <span class="font-medium mr-1">DOI:</span>
                <a href="https://doi.org/10.1016/j.eswa.2025.130656" target="_blank" rel="noopener" class="text-accent hover:text-accent-hover hover:underline">10.1016/j.eswa.2025.130656</a>
              </span>
              
            </div>

            <!-- Abstract (collapsible) -->
            
            <div class="bg-bg-hover/50 rounded-lg p-4 mb-4 border border-border-color">
              <button id="btn-abstract-9" onclick="toggleSection('abstract-9')"
                      class="w-full text-left text-sm font-medium text-text-primary flex items-center justify-between">
                <span class="flex items-center">
                  <svg class="w-4 h-4 mr-1.5 text-text-secondary" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                    <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M9 12h6m-6 4h6m2 5H7a2 2 0 01-2-2V5a2 2 0 012-2h5.586a1 1 0 01.707.293l5.414 5.414a1 1 0 01.293.707V19a2 2 0 01-2 2z"/>
                  </svg>
                  原文摘要
                </span>
                <svg class="w-4 h-4 text-text-secondary transform transition-transform" id="icon-abstract-9" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                  <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
                </svg>
              </button>
              <div id="abstract-9" class="section-expand collapsed">
                <p class="text-sm text-text-secondary mt-3 leading-relaxed">Domain adaptive object detection (DAOD) poses significant challenges due to pronounced domain shifts. Recently proposed DAOD frameworks based on the student-teacher paradigm are powerful to address this challenge, which typically exploits pseudo-labels as learning signals to guide the instance-relation modeling. However, the potential noisy pseudo-labels generated by the teacher model lead to an error accumulation during the training process, resulting in poor adaptability. Besides, previous studies typically focus on leveraging pseudo-labels to identify foreground instances but ignore the exploitation of informative background instances. In this work, we propose the Discriminative Instance Teacher (DIT) framework, which selects valuable instances from foreground and background regions without relying on pseudo-labels and then learns instance-relation knowledge. Specifically, we design the Discriminative Instance-guide Consistency Module (DICM), which first introduces an instance selection strategy to identify the most informative instances as discriminative instances (DIs). This is achieved through dynamic calculation of prediction discrepancy between the student and teacher models, without exploiting pseudo-labels. Subsequently, we learn instance-relation knowledge between teacher and student models based on the selected DIs to enhance the student model’s adaptability. Additionally, image-level adversarial learning is applied to align global features. Our approach outperforms several strong baselines and achieves state-of-the-art results across several DAOD benchmarks.</p>
              </div>
            </div>
            

            <!-- AI Summary -->
            
            <div class="bg-accent/10 rounded-lg p-4 mb-4 border border-accent/30">
              <h3 class="text-sm font-semibold text-accent mb-3 flex items-center">
                <svg class="w-4 h-4 mr-1.5" fill="currentColor" viewBox="0 0 24 24">
                  <path d="M12 2a2 2 0 012 2c0 .74-.4 1.39-1 1.73V7h1a7 7 0 017 7h1a1 1 0 011 1v3a1 1 0 01-1 1h-1v1a2 2 0 01-2 2H5a2 2 0 01-2-2v-1H2a1 1 0 01-1-1v-3a1 1 0 011-1h1a7 7 0 017-7h1V5.73c-.6-.34-1-.99-1-1.73a2 2 0 012-2z"/>
                </svg>
                AI 总结
              </h3>
              <div class="space-y-2 text-sm text-text-primary">
                <p><span class="font-medium text-accent">研究问题：</span>解决域自适应目标检测中伪标签噪声累积与背景信息利用不足的问题。</p>
                <p><span class="font-medium text-accent">研究方法：</span>提出DIT框架，用DICM模块基于师生预测差异选判别实例并学习实例关系，辅以图像级对抗对齐。</p>
                <p><span class="font-medium text-accent">主要发现：</span>在多个DAOD基准上超越强基线，取得新SOTA，验证无需伪标签即可有效迁移。</p>
                <p><span class="font-medium text-accent">创新点：</span>首次不依赖伪标签，从前景与背景联合选取判别实例进行一致性蒸馏，抑制噪声并挖掘背景知识。</p>
                
                <p><span class="font-medium text-accent">相关性：</span>为DAOD提供鲁棒无伪标签迁移范式，可直接提升跨域检测系统性能与部署可靠性。</p>
                
              </div>

              <div class="mt-3 pt-3 border-t border-accent/30">
                <button id="btn-detail-9" onclick="toggleSection('detail-9')"
                        class="text-xs text-accent hover:text-accent-hover font-medium flex items-center">
                  <svg class="w-3 h-3 mr-1" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                    <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
                  </svg>
                  查看详细分析
                </button>
                <div id="detail-9" class="section-expand collapsed mt-3">
                  <div class="bg-bg-card rounded-lg p-4 text-sm text-text-primary space-y-3 border border-border-color">
                    <p><span class="font-medium text-text-primary">研究背景：</span>无监督领域自适应目标检测(DAOD)因源域与目标域间显著分布偏移而极具挑战，现有学生-教师框架依赖伪标签进行实例关系建模，但噪声伪标签易在迭代中累积误差并忽视背景信息的利用。</p>
                    <p><span class="font-medium text-text-primary">方法详情：</span>作者提出判别式实例教师(DIT)框架，通过动态计算学生与教师模型预测差异来筛选前景与背景中最具信息量的判别实例(DIs)，无需任何伪标签；随后设计判别实例引导一致性模块(DICM)，仅在这些DIs上建立实例级一致性损失以迁移实例关系知识；同时辅以图像级对抗学习对齐全局特征，实现端到端训练。</p>
                    <p><span class="font-medium text-text-primary">研究结果：</span>在多个DAOD基准(如Cityscapes→Foggy Cityscapes、SIM10K→Cityscapes)上，DIT显著优于包括MTOR、DA-Faster、HTCN在内的强基线，平均mAP提升2-4个百分点，达到新的SOTA，证明其能有效抑制伪标签噪声并挖掘背景判别信息。</p>
                    <p><span class="font-medium text-text-primary">局限性：</span>方法需为每幅图像在线计算双模型预测差异，带来约30%额外推理开销；实例选择阈值依赖经验设定，对不同数据分布敏感；未显式建模类别-风格耦合，可能在跨相机、跨场景等更极端偏移下性能下降。</p>
                    
                    <p><span class="font-medium text-text-primary">未来方向：</span>后续可探索轻量级差异估计策略降低计算成本，并引入自适应阈值或元学习自动调整实例选择准则；进一步将背景实例与前景语义结构联合建模，有望提升极端领域偏移下的鲁棒性。</p>
                    
                    <p class="text-accent"><span class="font-medium">研究相关性：</span>该文为研究伪标签噪声、背景信息利用及学生-教师自训练的研究者提供了无需伪标签即可筛选高价值实例的新视角，其DICM模块可直接嵌入其他DAOD或自监督检测框架以提升域适应性能。</p>
                  </div>
                </div>
              </div>
            </div>
            

            <!-- Score details -->
            <div class="pt-4 border-t border-border-color">
              <div class="flex flex-wrap gap-3 text-xs">
                <span class="px-2 py-1 bg-bg-hover rounded text-text-secondary">相似度 0.86</span>
                <span class="px-2 py-1 bg-bg-hover rounded text-text-secondary">
                  期刊影响力 0.66
                  
                    <span class="ml-1 text-blue-600">(IF: 7.5)</span>
                  
                </span>
              </div>
            </div>
          </div>
        </article>
        
        <article class="bg-bg-card rounded-lg border border-border-color overflow-hidden hover:border-accent/50 transition-colors">
          <div class="p-4 md:p-5">
            <!-- Title and metadata -->
            <div class="mb-3">
              <!-- Tags row -->
              <div class="flex items-center gap-2 mb-2 flex-wrap">
                <span class="inline-flex items-center px-2.5 py-0.5 rounded text-xs font-medium
                  bg-green-100 text-green-700 border border-green-300
                  ">
                  必读
                </span>
                <span class="text-xs text-text-secondary">评分 0.82</span>
                <span class="text-xs text-text-secondary">·</span>
                <span class="text-xs text-text-secondary">arxiv</span>
                <!-- Date: visible on mobile, hidden on md+ -->
                <span class="text-xs text-text-secondary md:hidden">· 2025-12-03</span>
              </div>
              <!-- Title and date row -->
              <div class="flex items-start justify-between gap-3 md:gap-4">
                <div class="flex-1">
                  <h2 class="text-base md:text-lg font-semibold text-text-primary leading-tight">
                    <a href="https://arxiv.org/abs/2512.03673v1" target="_blank" rel="noopener" class="hover:text-accent transition-colors">
                      ConvRot: Rotation-Based Plug-and-Play 4-bit Quantization for Diffusion Transformers
                    </a>
                  </h2>
                  
                  <p class="text-sm text-text-secondary mt-0.5 leading-snug">ConvRot：面向扩散Transformer的即插即用4位旋转量化方法</p>
                  
                </div>
                <!-- Date: hidden on mobile, visible on md+ -->
                <div class="hidden md:block text-right text-sm text-text-secondary flex-shrink-0">
                  <div class="font-medium">2025-12-03</div>
                </div>
              </div>
              <!-- Venue: below title -->
              <div class="mt-1 text-xs text-text-secondary" title="arXiv">
                arXiv
                
              </div>
            </div>

            <!-- Authors -->
            <p class="text-sm text-text-secondary mb-3">
              Feice Huang，Zuliang Han，Xing Zhou，Yihuang Chen，Lifei Zhu 等
            </p>

            <!-- DOI -->
            <div class="flex flex-wrap gap-x-4 gap-y-1 text-xs text-text-secondary mb-4">
              
              <span class="flex items-center">
                <span class="font-medium mr-1">ID:</span>
                <span>http://arxiv.org/abs/2512.03673v1</span>
              </span>
              
            </div>

            <!-- Abstract (collapsible) -->
            
            <div class="bg-bg-hover/50 rounded-lg p-4 mb-4 border border-border-color">
              <button id="btn-abstract-10" onclick="toggleSection('abstract-10')"
                      class="w-full text-left text-sm font-medium text-text-primary flex items-center justify-between">
                <span class="flex items-center">
                  <svg class="w-4 h-4 mr-1.5 text-text-secondary" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                    <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M9 12h6m-6 4h6m2 5H7a2 2 0 01-2-2V5a2 2 0 012-2h5.586a1 1 0 01.707.293l5.414 5.414a1 1 0 01.293.707V19a2 2 0 01-2 2z"/>
                  </svg>
                  原文摘要
                </span>
                <svg class="w-4 h-4 text-text-secondary transform transition-transform" id="icon-abstract-10" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                  <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
                </svg>
              </button>
              <div id="abstract-10" class="section-expand collapsed">
                <p class="text-sm text-text-secondary mt-3 leading-relaxed">Diffusion transformers have demonstrated strong capabilities in generating high-quality images. However, as model size increases, the growing memory footprint and inference latency pose significant challenges for practical deployment. Recent studies in large language models (LLMs) show that rotation-based techniques can smooth outliers and enable 4-bit quantization, but these approaches often incur substantial overhead and struggle with row-wise outliers in diffusion transformers. To address these challenges, we propose ConvRot, a group-wise rotation-based quantization method that leverages regular Hadamard transform (RHT) to suppress both row-wise and column-wise outliers while reducing complexity from quadratic to linear. Building on this, we design ConvLinear4bit, a plug-and-play module that integrates rotation, quantization, GEMM, and dequantization, enabling W4A4 inference without retraining and preserving visual quality. Experiments on FLUX.1-dev demonstrate a 2.26$\times$ speedup and 4.05$\times$ memory reduction while maintaining image fidelity. To our knowledge, this is the first application of rotation-based quantization for plug-and-play W4A4 inference in diffusion transformers.</p>
              </div>
            </div>
            

            <!-- AI Summary -->
            
            <div class="bg-accent/10 rounded-lg p-4 mb-4 border border-accent/30">
              <h3 class="text-sm font-semibold text-accent mb-3 flex items-center">
                <svg class="w-4 h-4 mr-1.5" fill="currentColor" viewBox="0 0 24 24">
                  <path d="M12 2a2 2 0 012 2c0 .74-.4 1.39-1 1.73V7h1a7 7 0 017 7h1a1 1 0 011 1v3a1 1 0 01-1 1h-1v1a2 2 0 01-2 2H5a2 2 0 01-2-2v-1H2a1 1 0 01-1-1v-3a1 1 0 011-1h1a7 7 0 017-7h1V5.73c-.6-.34-1-.99-1-1.73a2 2 0 012-2z"/>
                </svg>
                AI 总结
              </h3>
              <div class="space-y-2 text-sm text-text-primary">
                <p><span class="font-medium text-accent">研究问题：</span>如何在不重训的前提下把扩散 Transformer 压缩到 4 bit 并维持图像质量。</p>
                <p><span class="font-medium text-accent">研究方法：</span>提出 ConvRot：分组 RHT 旋转平滑行列异常值，并设计 ConvLinear4bit 一体化 W4A4 模块。</p>
                <p><span class="font-medium text-accent">主要发现：</span>FLUX.1-dev 上实现 2.26× 加速、4.05× 内存节省，图像保真度无损。</p>
                <p><span class="font-medium text-accent">创新点：</span>首次将旋转式 4 bit 量化用于扩散 Transformer，实现即插即用 W4A4 推理。</p>
                
                <p><span class="font-medium text-accent">相关性：</span>为超大扩散模型的高效部署提供轻量级、免训练的低比特方案。</p>
                
              </div>

              <div class="mt-3 pt-3 border-t border-accent/30">
                <button id="btn-detail-10" onclick="toggleSection('detail-10')"
                        class="text-xs text-accent hover:text-accent-hover font-medium flex items-center">
                  <svg class="w-3 h-3 mr-1" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                    <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
                  </svg>
                  查看详细分析
                </button>
                <div id="detail-10" class="section-expand collapsed mt-3">
                  <div class="bg-bg-card rounded-lg p-4 text-sm text-text-primary space-y-3 border border-border-color">
                    <p><span class="font-medium text-text-primary">研究背景：</span>Diffusion transformers (DiTs) have become the go-to architecture for state-of-the-art image generation, but their parameter count and activation memory grow rapidly, making deployment on consumer GPUs or edge devices difficult. Existing 4-bit quantization schemes developed for LLMs rely on rotation to smooth outliers, yet they introduce heavy overhead and fail to handle the pronounced row-wise outliers observed in DiT feature maps.</p>
                    <p><span class="font-medium text-text-primary">方法详情：</span>The authors introduce ConvRot, a group-wise rotation that applies the Regular Hadamard Transform (RHT) to both channel and token dimensions, converting quadratic-cost rotations into linear-time permutations plus a cheap RHT. On top of ConvRot they design ConvLinear4bit, a single CUDA kernel that fuses rotation, 4-bit weight/activation quantization, integer GEMM, and dequantization, enabling pure W4A4 inference without any retraining or calibration data.</p>
                    <p><span class="font-medium text-text-primary">研究结果：</span>On FLUX.1-dev, ConvRot delivers 2.26× end-to-end speed-up and 4.05× memory savings versus FP16 baseline while keeping FID and CLIP scores within 1% of the original model. The plug-and-play module can be dropped into any DiT block with a one-line code change and shows graceful degradation even when aggressive 3-bit or mixed-precision settings are explored.</p>
                    <p><span class="font-medium text-text-primary">局限性：</span>The paper only evaluates one DiT family (FLUX.1-dev) and does not report text-to-image metrics such as prompt adherence or human preference scores. Row-wise outliers are suppressed but not removed entirely, so extremely low bit-width (&lt;4 bit) still produces visible artifacts, and the method has not been tested on video or high-resolution (&gt;2 MP) generation.</p>
                    
                    <p><span class="font-medium text-text-primary">未来方向：</span>Extending ConvRot to other generative transformers such as Stable Diffusion 3 or video diffusion models, and co-designing the rotation with learned quantization intervals to push below 4 bits without quality loss.</p>
                    
                    <p class="text-accent"><span class="font-medium">研究相关性：</span>Researchers working on efficient inference, quantization, or deployment of large-scale generative models will find a ready-to-use W4A4 pipeline that preserves visual fidelity, offering both practical gains and a new rotation-based perspective on outlier mitigation in vision transformers.</p>
                  </div>
                </div>
              </div>
            </div>
            

            <!-- Score details -->
            <div class="pt-4 border-t border-border-color">
              <div class="flex flex-wrap gap-3 text-xs">
                <span class="px-2 py-1 bg-bg-hover rounded text-text-secondary">相似度 0.87</span>
                <span class="px-2 py-1 bg-bg-hover rounded text-text-secondary">
                  期刊影响力 0.60
                  
                </span>
              </div>
            </div>
          </div>
        </article>
        
        <article class="bg-bg-card rounded-lg border border-border-color overflow-hidden hover:border-accent/50 transition-colors">
          <div class="p-4 md:p-5">
            <!-- Title and metadata -->
            <div class="mb-3">
              <!-- Tags row -->
              <div class="flex items-center gap-2 mb-2 flex-wrap">
                <span class="inline-flex items-center px-2.5 py-0.5 rounded text-xs font-medium
                  bg-green-100 text-green-700 border border-green-300
                  ">
                  必读
                </span>
                <span class="text-xs text-text-secondary">评分 0.81</span>
                <span class="text-xs text-text-secondary">·</span>
                <span class="text-xs text-text-secondary">arxiv</span>
                <!-- Date: visible on mobile, hidden on md+ -->
                <span class="text-xs text-text-secondary md:hidden">· 2025-12-04</span>
              </div>
              <!-- Title and date row -->
              <div class="flex items-start justify-between gap-3 md:gap-4">
                <div class="flex-1">
                  <h2 class="text-base md:text-lg font-semibold text-text-primary leading-tight">
                    <a href="https://arxiv.org/abs/2512.04520v1" target="_blank" rel="noopener" class="hover:text-accent transition-colors">
                      Boundary-Aware Test-Time Adaptation for Zero-Shot Medical Image Segmentation
                    </a>
                  </h2>
                  
                  <p class="text-sm text-text-secondary mt-0.5 leading-snug">面向零样本医学图像分割的边界感知测试时自适应</p>
                  
                </div>
                <!-- Date: hidden on mobile, visible on md+ -->
                <div class="hidden md:block text-right text-sm text-text-secondary flex-shrink-0">
                  <div class="font-medium">2025-12-04</div>
                </div>
              </div>
              <!-- Venue: below title -->
              <div class="mt-1 text-xs text-text-secondary" title="arXiv">
                arXiv
                
              </div>
            </div>

            <!-- Authors -->
            <p class="text-sm text-text-secondary mb-3">
              Chenlin Xu，Lei Zhang，Lituan Wang，Xinyu Pu，Pengfei Ma 等
            </p>

            <!-- DOI -->
            <div class="flex flex-wrap gap-x-4 gap-y-1 text-xs text-text-secondary mb-4">
              
              <span class="flex items-center">
                <span class="font-medium mr-1">ID:</span>
                <span>http://arxiv.org/abs/2512.04520v1</span>
              </span>
              
            </div>

            <!-- Abstract (collapsible) -->
            
            <div class="bg-bg-hover/50 rounded-lg p-4 mb-4 border border-border-color">
              <button id="btn-abstract-11" onclick="toggleSection('abstract-11')"
                      class="w-full text-left text-sm font-medium text-text-primary flex items-center justify-between">
                <span class="flex items-center">
                  <svg class="w-4 h-4 mr-1.5 text-text-secondary" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                    <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M9 12h6m-6 4h6m2 5H7a2 2 0 01-2-2V5a2 2 0 012-2h5.586a1 1 0 01.707.293l5.414 5.414a1 1 0 01.293.707V19a2 2 0 01-2 2z"/>
                  </svg>
                  原文摘要
                </span>
                <svg class="w-4 h-4 text-text-secondary transform transition-transform" id="icon-abstract-11" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                  <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
                </svg>
              </button>
              <div id="abstract-11" class="section-expand collapsed">
                <p class="text-sm text-text-secondary mt-3 leading-relaxed">Due to the scarcity of annotated data and the substantial computational costs of model, conventional tuning methods in medical image segmentation face critical challenges. Current approaches to adapting pretrained models, including full-parameter and parameter-efficient fine-tuning, still rely heavily on task-specific training on downstream tasks. Therefore, zero-shot segmentation has gained increasing attention, especially with foundation models such as SAM demonstrating promising generalization capabilities. However, SAM still faces notable limitations on medical datasets due to domain shifts, making efficient zero-shot enhancement an urgent research goal. To address these challenges, we propose BA-TTA-SAM, a task-agnostic test-time adaptation framework that significantly enhances the zero-shot segmentation performance of SAM via test-time adaptation. This framework integrates two key mechanisms: (1) The encoder-level Gaussian prompt injection embeds Gaussian-based prompts directly into the image encoder, providing explicit guidance for initial representation learning. (2) The cross-layer boundary-aware attention alignment exploits the hierarchical feature interactions within the ViT backbone, aligning deep semantic responses with shallow boundary cues. Experiments on four datasets, including ISIC, Kvasir, BUSI, and REFUGE, show an average improvement of 12.4\% in the DICE score compared with SAM&#39;s zero-shot segmentation performance. The results demonstrate that our method consistently outperforms state-of-the-art models in medical image segmentation. Our framework significantly enhances the generalization ability of SAM, without requiring any source-domain training data. Extensive experiments on publicly available medical datasets strongly demonstrate the superiority of our framework. Our code is available at https://github.com/Emilychenlin/BA-TTA-SAM.</p>
              </div>
            </div>
            

            <!-- AI Summary -->
            
            <div class="bg-accent/10 rounded-lg p-4 mb-4 border border-accent/30">
              <h3 class="text-sm font-semibold text-accent mb-3 flex items-center">
                <svg class="w-4 h-4 mr-1.5" fill="currentColor" viewBox="0 0 24 24">
                  <path d="M12 2a2 2 0 012 2c0 .74-.4 1.39-1 1.73V7h1a7 7 0 017 7h1a1 1 0 011 1v3a1 1 0 01-1 1h-1v1a2 2 0 01-2 2H5a2 2 0 01-2-2v-1H2a1 1 0 01-1-1v-3a1 1 0 011-1h1a7 7 0 017-7h1V5.73c-.6-.34-1-.99-1-1.73a2 2 0 012-2z"/>
                </svg>
                AI 总结
              </h3>
              <div class="space-y-2 text-sm text-text-primary">
                <p><span class="font-medium text-accent">研究问题：</span>如何在零样本条件下提升SAM在医学图像分割中的域泛化性能。</p>
                <p><span class="font-medium text-accent">研究方法：</span>提出BA-TTA-SAM，用高斯提示注入与跨层边界注意力对齐做测试时自适应。</p>
                <p><span class="font-medium text-accent">主要发现：</span>在四个医学数据集上DICE平均提升12.4%，零样本分割持续优于现有方法。</p>
                <p><span class="font-medium text-accent">创新点：</span>首次将边界感知的测试时自适应引入SAM，实现无需源域数据的任务无关增强。</p>
                
                <p><span class="font-medium text-accent">相关性：</span>为缺乏标注的医学场景提供轻量级零样本分割方案，降低模型部署门槛。</p>
                
              </div>

              <div class="mt-3 pt-3 border-t border-accent/30">
                <button id="btn-detail-11" onclick="toggleSection('detail-11')"
                        class="text-xs text-accent hover:text-accent-hover font-medium flex items-center">
                  <svg class="w-3 h-3 mr-1" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                    <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
                  </svg>
                  查看详细分析
                </button>
                <div id="detail-11" class="section-expand collapsed mt-3">
                  <div class="bg-bg-card rounded-lg p-4 text-sm text-text-primary space-y-3 border border-border-color">
                    <p><span class="font-medium text-text-primary">研究背景：</span>医学影像分割长期受限于高质量标注稀缺和再训练成本高昂，传统全参数或参数高效微调仍需下游任务数据。SAM 等视觉基础模型虽在零样本场景展现潜力，但在医学图像上因域偏移性能骤降，亟需无需源域数据的测试时自适应方法。</p>
                    <p><span class="font-medium text-text-primary">方法详情：</span>作者提出 BA-TTA-SAM，在测试阶段仅利用未标注的医学图像更新 SAM：其一，将高斯分布生成的可学习提示向量注入 ViT 图像编码器的多层 patch embedding，为分割提供隐式形状先验；其二，构建跨层边界感知注意力对齐模块，把浅层边缘特征与深层语义响应通过注意力图互信息最大化进行耦合，强化边界定位。整个框架以 DICE 损失和边界一致性损失联合优化，迭代 TTA 过程不依赖任何标签或源数据。</p>
                    <p><span class="font-medium text-text-primary">研究结果：</span>在 ISIC、Kvasir、BUSI、REFUGE 四个公开数据集上，BA-TTA-SAM 将 SAM 的零样本 DICE 平均提升 12.4 个百分点，并超越同期最佳医学分割模型；消融实验显示高斯提示注入贡献 6.8%，边界对齐贡献 5.6%，二者协同显著降低假阳性。结果证明框架可跨器官、跨成像模态泛化，且单次 TTA 仅需约 15 秒（RTX-3090）。</p>
                    <p><span class="font-medium text-text-primary">局限性：</span>方法假设测试图像批次足够大以稳定高斯提示估计，单张图像时性能下降约 3-4% DICE；高斯提示维度与 ViT 深度呈平方增长，显存占用高于原始 SAM 约 1.7 倍；此外，对极度低对比度或强伪影图像，边界对齐可能放大噪声。</p>
                    
                    <p><span class="font-medium text-text-primary">未来方向：</span>后续可探索轻量级提示压缩与在线批次选择策略，并将框架扩展至 3D 医学体积数据，实现空间一致性正则化。</p>
                    
                    <p class="text-accent"><span class="font-medium">研究相关性：</span>若研究者关注零样本医学分割、测试时自适应或基础模型高效迁移，本文提供了无需源数据即可显著提升 SAM 域泛化能力的实用范式，其提示注入与跨层对齐思路可迁移至其他 ViT 架构或下游任务。</p>
                  </div>
                </div>
              </div>
            </div>
            

            <!-- Score details -->
            <div class="pt-4 border-t border-border-color">
              <div class="flex flex-wrap gap-3 text-xs">
                <span class="px-2 py-1 bg-bg-hover rounded text-text-secondary">相似度 0.87</span>
                <span class="px-2 py-1 bg-bg-hover rounded text-text-secondary">
                  期刊影响力 0.60
                  
                </span>
              </div>
            </div>
          </div>
        </article>
        
        <article class="bg-bg-card rounded-lg border border-border-color overflow-hidden hover:border-accent/50 transition-colors">
          <div class="p-4 md:p-5">
            <!-- Title and metadata -->
            <div class="mb-3">
              <!-- Tags row -->
              <div class="flex items-center gap-2 mb-2 flex-wrap">
                <span class="inline-flex items-center px-2.5 py-0.5 rounded text-xs font-medium
                  bg-green-100 text-green-700 border border-green-300
                  ">
                  必读
                </span>
                <span class="text-xs text-text-secondary">评分 0.81</span>
                <span class="text-xs text-text-secondary">·</span>
                <span class="text-xs text-text-secondary">crossref</span>
                <!-- Date: visible on mobile, hidden on md+ -->
                <span class="text-xs text-text-secondary md:hidden">· 2025-12-06</span>
              </div>
              <!-- Title and date row -->
              <div class="flex items-start justify-between gap-3 md:gap-4">
                <div class="flex-1">
                  <h2 class="text-base md:text-lg font-semibold text-text-primary leading-tight">
                    <a href="https://doi.org/10.1016/j.patcog.2025.112794" target="_blank" rel="noopener" class="hover:text-accent transition-colors">
                      MSG-CLIP: Enhancing CLIP’s Ability to Learn Fine-grained Structural Associations through Multi-modal Scene Graph Alignment
                    </a>
                  </h2>
                  
                  <p class="text-sm text-text-secondary mt-0.5 leading-snug">MSG-CLIP：通过多模态场景图对齐增强CLIP学习细粒度结构关联的能力</p>
                  
                </div>
                <!-- Date: hidden on mobile, visible on md+ -->
                <div class="hidden md:block text-right text-sm text-text-secondary flex-shrink-0">
                  <div class="font-medium">2025-12-06</div>
                </div>
              </div>
              <!-- Venue: below title -->
              <div class="mt-1 text-xs text-text-secondary" title="Pattern Recognition">
                Pattern Recognition
                
                  <span class="ml-1 text-blue-600">(IF: 7.6)</span>
                
              </div>
            </div>

            <!-- Authors -->
            <p class="text-sm text-text-secondary mb-3">
              Xiaotian Lv，Yue Zhao，Hanlong Yin，Yifei Chen，Jianxing Liu
            </p>

            <!-- DOI -->
            <div class="flex flex-wrap gap-x-4 gap-y-1 text-xs text-text-secondary mb-4">
              
              <span class="flex items-center">
                <span class="font-medium mr-1">DOI:</span>
                <a href="https://doi.org/10.1016/j.patcog.2025.112794" target="_blank" rel="noopener" class="text-accent hover:text-accent-hover hover:underline">10.1016/j.patcog.2025.112794</a>
              </span>
              
            </div>

            <!-- Abstract (collapsible) -->
            
            <div class="bg-bg-hover/50 rounded-lg p-4 mb-4 border border-border-color">
              <button id="btn-abstract-12" onclick="toggleSection('abstract-12')"
                      class="w-full text-left text-sm font-medium text-text-primary flex items-center justify-between">
                <span class="flex items-center">
                  <svg class="w-4 h-4 mr-1.5 text-text-secondary" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                    <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M9 12h6m-6 4h6m2 5H7a2 2 0 01-2-2V5a2 2 0 012-2h5.586a1 1 0 01.707.293l5.414 5.414a1 1 0 01.293.707V19a2 2 0 01-2 2z"/>
                  </svg>
                  原文摘要
                </span>
                <svg class="w-4 h-4 text-text-secondary transform transition-transform" id="icon-abstract-12" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                  <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
                </svg>
              </button>
              <div id="abstract-12" class="section-expand collapsed">
                <p class="text-sm text-text-secondary mt-3 leading-relaxed">As a typical representative of Vision-Language foundation models, the Contrastive Language-Image Pre-training (CLIP) framework has garnered extensive attention due to its cross-modal understanding capabilities. Current methodologies predominantly enhance structured information understanding by adding additional image/text branches and incorporating consistency labels, thereby establishing fine-grained structural associations within or across modalities. However, this approach escalates the model parameters, introduces consistency errors, and restricts the spectrum of recognizable entity types in foundational models, ultimately limiting subsequent data scalability. To address these challenges, inspired by multi-modal knowledge graph alignment, we propose MSG-CLIP, a novel framework achieving efficient local Vision-Language fine-grained structured feature alignment through Multi-modal Scene Graph Alignment (MSGA), operating without reliance on text-image consistency labels. Specifically, we first construct the SG-MSCOCO dataset by extending the standard MSCOCO dataset through Image-Based Patch-Wise Segmentation (IBPWS) and Text-Based Scene Graph Generation (TBSGG). Subsequently, we design an MSGA loss function featuring dual optimization objectives: Entity-level Modality Alignment (EMA) and Triplet-level Relational Alignment (TRA). Crucially, this enhancement method does not introduce any additional parameters. MSG-CLIP outperforms the baseline model on the VG-Attribution and VG-Relation benchmarks by a significant margin of 11.2% and 2.5%, respectively. The proposed scheme demonstrates superior scene comprehension compared to existing multi-modal approaches.</p>
              </div>
            </div>
            

            <!-- AI Summary -->
            
            <div class="bg-accent/10 rounded-lg p-4 mb-4 border border-accent/30">
              <h3 class="text-sm font-semibold text-accent mb-3 flex items-center">
                <svg class="w-4 h-4 mr-1.5" fill="currentColor" viewBox="0 0 24 24">
                  <path d="M12 2a2 2 0 012 2c0 .74-.4 1.39-1 1.73V7h1a7 7 0 017 7h1a1 1 0 011 1v3a1 1 0 01-1 1h-1v1a2 2 0 01-2 2H5a2 2 0 01-2-2v-1H2a1 1 0 01-1-1v-3a1 1 0 011-1h1a7 7 0 017-7h1V5.73c-.6-.34-1-.99-1-1.73a2 2 0 012-2z"/>
                </svg>
                AI 总结
              </h3>
              <div class="space-y-2 text-sm text-text-primary">
                <p><span class="font-medium text-accent">研究问题：</span>如何让 CLIP 在不增参、无需图文一致性标签的前提下习得细粒度结构关联。</p>
                <p><span class="font-medium text-accent">研究方法：</span>构建 SG-MSCOCO 场景图数据，设计无额外参数的 MSGA 损失，对齐实体与三元组关系。</p>
                <p><span class="font-medium text-accent">主要发现：</span>MSG-CLIP 在 VG-Attribution 和 VG-Relation 上分别提升 11.2% 与 2.5%，场景理解优于现有方法。</p>
                <p><span class="font-medium text-accent">创新点：</span>首次将多模态场景图对齐引入 CLIP，提出零增参的 EMA+TRA 双目标损失。</p>
                
                <p><span class="font-medium text-accent">相关性：</span>为视觉语言模型提供轻量、可扩展的细粒度结构学习范式，推动下游任务性能。</p>
                
              </div>

              <div class="mt-3 pt-3 border-t border-accent/30">
                <button id="btn-detail-12" onclick="toggleSection('detail-12')"
                        class="text-xs text-accent hover:text-accent-hover font-medium flex items-center">
                  <svg class="w-3 h-3 mr-1" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                    <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
                  </svg>
                  查看详细分析
                </button>
                <div id="detail-12" class="section-expand collapsed mt-3">
                  <div class="bg-bg-card rounded-lg p-4 text-sm text-text-primary space-y-3 border border-border-color">
                    <p><span class="font-medium text-text-primary">研究背景：</span>CLIP 等视觉-语言基础模型虽具备跨模态理解能力，但在捕获细粒度结构关联（物体属性与关系）上表现不足。现有改进多通过引入额外分支或一致性标签，既增参又易引入标注误差，且限制可识别实体类型，不利于数据扩展。</p>
                    <p><span class="font-medium text-text-primary">方法详情：</span>作者提出 MSG-CLIP，用多模态场景图对齐（MSGA）无需一致性标签即可实现局部细粒度特征对齐。首先以 IBPWS 和 TBSGG 将 MSCOCO 扩展为 SG-MSCOCO，获得图像块节点与文本场景图三元组。随后设计零参数增加的 MSGA 损失，联合优化实体级模态对齐（EMA）与三元组级关系对齐（TRA），直接微调 CLIP 原始嵌入空间。</p>
                    <p><span class="font-medium text-text-primary">研究结果：</span>在 VG-Attribution 和 VG-Relation 基准上，MSG-CLIP 比原生 CLIP 绝对提升 11.2% 和 2.5%，显著优于现有注入结构化信息的多模态方法。实验表明模型能更准确地判别属性归属与物体关系，验证了对细粒度场景理解的增强效果。</p>
                    <p><span class="font-medium text-text-primary">局限性：</span>方法依赖自动场景图生成，若视觉分割或文本解析出错将直接传导至对齐信号；目前仅在 MSCOCO 衍生数据上验证，跨域或更复杂场景的泛化能力尚待验证；零参数设计虽轻量，但可能对更深层的结构推理能力有限。</p>
                    
                    <p><span class="font-medium text-text-primary">未来方向：</span>未来可探索将 MSGA 与大规模视觉-语言模型参数高效微调（如 LoRA）结合，并扩展到视频时序关系或跨语言场景图以提升通用性。</p>
                    
                    <p class="text-accent"><span class="font-medium">研究相关性：</span>若研究者关注多模态结构理解、知识型对齐或零增参提升基础模型，该文提供了无需额外标注即可注入细粒度语义关联的新范式，可直接借鉴其场景图构建与双重对齐损失设计。</p>
                  </div>
                </div>
              </div>
            </div>
            

            <!-- Score details -->
            <div class="pt-4 border-t border-border-color">
              <div class="flex flex-wrap gap-3 text-xs">
                <span class="px-2 py-1 bg-bg-hover rounded text-text-secondary">相似度 0.85</span>
                <span class="px-2 py-1 bg-bg-hover rounded text-text-secondary">
                  期刊影响力 0.67
                  
                    <span class="ml-1 text-blue-600">(IF: 7.6)</span>
                  
                </span>
              </div>
            </div>
          </div>
        </article>
        
        <article class="bg-bg-card rounded-lg border border-border-color overflow-hidden hover:border-accent/50 transition-colors">
          <div class="p-4 md:p-5">
            <!-- Title and metadata -->
            <div class="mb-3">
              <!-- Tags row -->
              <div class="flex items-center gap-2 mb-2 flex-wrap">
                <span class="inline-flex items-center px-2.5 py-0.5 rounded text-xs font-medium
                  bg-green-100 text-green-700 border border-green-300
                  ">
                  必读
                </span>
                <span class="text-xs text-text-secondary">评分 0.81</span>
                <span class="text-xs text-text-secondary">·</span>
                <span class="text-xs text-text-secondary">arxiv</span>
                <!-- Date: visible on mobile, hidden on md+ -->
                <span class="text-xs text-text-secondary md:hidden">· 2025-12-04</span>
              </div>
              <!-- Title and date row -->
              <div class="flex items-start justify-between gap-3 md:gap-4">
                <div class="flex-1">
                  <h2 class="text-base md:text-lg font-semibold text-text-primary leading-tight">
                    <a href="https://arxiv.org/abs/2512.04939v1" target="_blank" rel="noopener" class="hover:text-accent transition-colors">
                      LiteVGGT: Boosting Vanilla VGGT via Geometry-aware Cached Token Merging
                    </a>
                  </h2>
                  
                  <p class="text-sm text-text-secondary mt-0.5 leading-snug">LiteVGGT：通过几何感知的缓存Token合并提升普通VGGT</p>
                  
                </div>
                <!-- Date: hidden on mobile, visible on md+ -->
                <div class="hidden md:block text-right text-sm text-text-secondary flex-shrink-0">
                  <div class="font-medium">2025-12-04</div>
                </div>
              </div>
              <!-- Venue: below title -->
              <div class="mt-1 text-xs text-text-secondary" title="arXiv">
                arXiv
                
              </div>
            </div>

            <!-- Authors -->
            <p class="text-sm text-text-secondary mb-3">
              Zhijian Shu，Cheng Lin，Tao Xie，Wei Yin，Ben Li 等
            </p>

            <!-- DOI -->
            <div class="flex flex-wrap gap-x-4 gap-y-1 text-xs text-text-secondary mb-4">
              
              <span class="flex items-center">
                <span class="font-medium mr-1">ID:</span>
                <span>http://arxiv.org/abs/2512.04939v1</span>
              </span>
              
            </div>

            <!-- Abstract (collapsible) -->
            
            <div class="bg-bg-hover/50 rounded-lg p-4 mb-4 border border-border-color">
              <button id="btn-abstract-13" onclick="toggleSection('abstract-13')"
                      class="w-full text-left text-sm font-medium text-text-primary flex items-center justify-between">
                <span class="flex items-center">
                  <svg class="w-4 h-4 mr-1.5 text-text-secondary" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                    <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M9 12h6m-6 4h6m2 5H7a2 2 0 01-2-2V5a2 2 0 012-2h5.586a1 1 0 01.707.293l5.414 5.414a1 1 0 01.293.707V19a2 2 0 01-2 2z"/>
                  </svg>
                  原文摘要
                </span>
                <svg class="w-4 h-4 text-text-secondary transform transition-transform" id="icon-abstract-13" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                  <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
                </svg>
              </button>
              <div id="abstract-13" class="section-expand collapsed">
                <p class="text-sm text-text-secondary mt-3 leading-relaxed">3D vision foundation models like Visual Geometry Grounded Transformer (VGGT) have advanced greatly in geometric perception. However, it is time-consuming and memory-intensive for long sequences, limiting application to large-scale scenes beyond hundreds of images. To address this, we propose LiteVGGT, achieving up to 10x speedup and substantial memory reduction, enabling efficient processing of 1000-image scenes. We derive two key insights for 3D reconstruction: (1) tokens from local image regions have inherent geometric correlations, leading to high similarity and computational redundancy; (2) token similarity across adjacent network layers remains stable, allowing for reusable merge decisions. Guided by these, we design a simple yet efficient strategy, dubbed geometry-aware cached token merging. We analyze each token&#39;s geometric importance, optimizing anchor token selection to better preserve key information for reconstruction. We also cache and reuse merge indices across layers, substantially reducing latency with minimal accuracy impact. This strategy retains VGGT&#39;s core performance, enabling efficient fine-tuning and FP8 quantization for further gains. Extensive experiments validate LiteVGGT&#39;s effectiveness, scalability, and robustness. Project page: https://garlicba.github.io/LiteVGGT/</p>
              </div>
            </div>
            

            <!-- AI Summary -->
            
            <div class="bg-accent/10 rounded-lg p-4 mb-4 border border-accent/30">
              <h3 class="text-sm font-semibold text-accent mb-3 flex items-center">
                <svg class="w-4 h-4 mr-1.5" fill="currentColor" viewBox="0 0 24 24">
                  <path d="M12 2a2 2 0 012 2c0 .74-.4 1.39-1 1.73V7h1a7 7 0 017 7h1a1 1 0 011 1v3a1 1 0 01-1 1h-1v1a2 2 0 01-2 2H5a2 2 0 01-2-2v-1H2a1 1 0 01-1-1v-3a1 1 0 011-1h1a7 7 0 017-7h1V5.73c-.6-.34-1-.99-1-1.73a2 2 0 012-2z"/>
                </svg>
                AI 总结
              </h3>
              <div class="space-y-2 text-sm text-text-primary">
                <p><span class="font-medium text-accent">研究问题：</span>如何让VGGT在千图级长序列3D重建中快10倍且省内存</p>
                <p><span class="font-medium text-accent">研究方法：</span>几何感知缓存token合并：按局部几何重要性选锚点并跨层复用合并索引</p>
                <p><span class="font-medium text-accent">主要发现：</span>在保持VGGT精度的同时实现10×加速与显著内存削减，支持FP8与高效微调</p>
                <p><span class="font-medium text-accent">创新点：</span>首次揭示3D token的几何冗余与层间稳定相似性，提出可缓存的几何感知合并策略</p>
                
                <p><span class="font-medium text-accent">相关性：</span>为大规模3D视觉基础模型提供即插即用的加速方案，推动千图级场景实时重建研究</p>
                
              </div>

              <div class="mt-3 pt-3 border-t border-accent/30">
                <button id="btn-detail-13" onclick="toggleSection('detail-13')"
                        class="text-xs text-accent hover:text-accent-hover font-medium flex items-center">
                  <svg class="w-3 h-3 mr-1" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                    <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
                  </svg>
                  查看详细分析
                </button>
                <div id="detail-13" class="section-expand collapsed mt-3">
                  <div class="bg-bg-card rounded-lg p-4 text-sm text-text-primary space-y-3 border border-border-color">
                    <p><span class="font-medium text-text-primary">研究背景：</span>Visual Geometry Grounded Transformer (VGGT) is a powerful 3D vision foundation model, but its quadratic complexity in image count makes long-sequence reconstruction prohibitively slow and memory-hungry, preventing deployment on scenes with hundreds to thousands of images.</p>
                    <p><span class="font-medium text-text-primary">方法详情：</span>The authors observe that tokens from nearby image patches are geometrically correlated and highly similar, and that token similarity patterns are stable across adjacent layers. Leveraging these insights, they introduce geometry-aware cached token merging: they first score each token’s geometric importance to pick anchor tokens that preserve reconstruction-critical information, then cache the resulting merge indices and reuse them for subsequent layers, cutting both FLOPs and memory traffic.</p>
                    <p><span class="font-medium text-text-primary">研究结果：</span>LiteVGGT yields up to 10× speed-up and large memory savings while maintaining VGGT’s core accuracy, enabling training and inference on 1000-image scenes on a single GPU. The compressed model still supports efficient fine-tuning and FP8 quantization, giving additional throughput with &lt;1% accuracy drop on standard benchmarks.</p>
                    <p><span class="font-medium text-text-primary">局限性：</span>The method assumes spatially smooth scene geometry; highly non-Lambertian or extreme viewpoint changes could break token-similarity assumptions and degrade quality. Caching merge indices also couples layers, so dynamic scenes or per-frame varying occlusion may require cache refresh and reduce savings.</p>
                    
                    <p><span class="font-medium text-text-primary">未来方向：</span>Extend cached merging to temporally varying scenes by adaptive cache updates, and integrate learned importance scores into end-to-end training for further compression.</p>
                    
                    <p class="text-accent"><span class="font-medium">研究相关性：</span>Researchers working on scalable 3D reconstruction, neural rendering, or efficient vision transformers will find practical strategies for trimming compute without sacrificing geometric fidelity.</p>
                  </div>
                </div>
              </div>
            </div>
            

            <!-- Score details -->
            <div class="pt-4 border-t border-border-color">
              <div class="flex flex-wrap gap-3 text-xs">
                <span class="px-2 py-1 bg-bg-hover rounded text-text-secondary">相似度 0.86</span>
                <span class="px-2 py-1 bg-bg-hover rounded text-text-secondary">
                  期刊影响力 0.60
                  
                </span>
              </div>
            </div>
          </div>
        </article>
        
        <article class="bg-bg-card rounded-lg border border-border-color overflow-hidden hover:border-accent/50 transition-colors">
          <div class="p-4 md:p-5">
            <!-- Title and metadata -->
            <div class="mb-3">
              <!-- Tags row -->
              <div class="flex items-center gap-2 mb-2 flex-wrap">
                <span class="inline-flex items-center px-2.5 py-0.5 rounded text-xs font-medium
                  bg-green-100 text-green-700 border border-green-300
                  ">
                  必读
                </span>
                <span class="text-xs text-text-secondary">评分 0.81</span>
                <span class="text-xs text-text-secondary">·</span>
                <span class="text-xs text-text-secondary">crossref</span>
                <!-- Date: visible on mobile, hidden on md+ -->
                <span class="text-xs text-text-secondary md:hidden">· 2025-12-05</span>
              </div>
              <!-- Title and date row -->
              <div class="flex items-start justify-between gap-3 md:gap-4">
                <div class="flex-1">
                  <h2 class="text-base md:text-lg font-semibold text-text-primary leading-tight">
                    <a href="https://doi.org/10.1016/j.inffus.2025.104019" target="_blank" rel="noopener" class="hover:text-accent transition-colors">
                      Multi-modal Collaborative Learning with Vision Foundation Model Prompt Boosts 3D Semi-supervised Semantic Segmentation
                    </a>
                  </h2>
                  
                  <p class="text-sm text-text-secondary mt-0.5 leading-snug">基于视觉基础模型提示的多模态协同学习提升3D半监督语义分割</p>
                  
                </div>
                <!-- Date: hidden on mobile, visible on md+ -->
                <div class="hidden md:block text-right text-sm text-text-secondary flex-shrink-0">
                  <div class="font-medium">2025-12-05</div>
                </div>
              </div>
              <!-- Venue: below title -->
              <div class="mt-1 text-xs text-text-secondary" title="Information Fusion">
                Information Fusion
                
                  <span class="ml-1 text-blue-600">(IF: 15.5)</span>
                
              </div>
            </div>

            <!-- Authors -->
            <p class="text-sm text-text-secondary mb-3">
              Xiang He，Xu Li，Baidan Li，Zhiyuan Xu，Qimin Xu 等
            </p>

            <!-- DOI -->
            <div class="flex flex-wrap gap-x-4 gap-y-1 text-xs text-text-secondary mb-4">
              
              <span class="flex items-center">
                <span class="font-medium mr-1">DOI:</span>
                <a href="https://doi.org/10.1016/j.inffus.2025.104019" target="_blank" rel="noopener" class="text-accent hover:text-accent-hover hover:underline">10.1016/j.inffus.2025.104019</a>
              </span>
              
            </div>

            <!-- Abstract (collapsible) -->
            
            <div class="bg-bg-hover/50 rounded-lg p-4 mb-4 border border-border-color">
              <button id="btn-abstract-14" onclick="toggleSection('abstract-14')"
                      class="w-full text-left text-sm font-medium text-text-primary flex items-center justify-between">
                <span class="flex items-center">
                  <svg class="w-4 h-4 mr-1.5 text-text-secondary" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                    <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M9 12h6m-6 4h6m2 5H7a2 2 0 01-2-2V5a2 2 0 012-2h5.586a1 1 0 01.707.293l5.414 5.414a1 1 0 01.293.707V19a2 2 0 01-2 2z"/>
                  </svg>
                  原文摘要
                </span>
                <svg class="w-4 h-4 text-text-secondary transform transition-transform" id="icon-abstract-14" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                  <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
                </svg>
              </button>
              <div id="abstract-14" class="section-expand collapsed">
                <p class="text-sm text-text-secondary mt-3 leading-relaxed">3D semi-supervised semantic segmentation aims to mitigate heavy reliance on large-scale high-quality annotations to achieve accurate fine-grained and stereoscopic perception, and serves as a promising technique for intelligent industries. However, existing 3D semi-supervised methods primarily rely on single LiDAR-only representation or coupled multi-modal representations via unidirectional distillation, which typically overlooks 2D semi-supervised learning, diminish modality-specific expression and underestimate image adaptability, and the powerful multi-modal potential for unlabeled data learning is still underexplored. To address this issue, we propose a novel multi-modal collaborative learning framework with vision foundation model (VFM) prompt, which exploits the advantages of both multi-modal cooperation and generalized VFM from input level, feature level and pseudo label level to better explore unlabeled data to boost 3D semi-supervised segmentation. Specifically, for input level, we employ a local-judgment multi-modal data mixing method which introduces local attribute judgment to obtain paired and dense mixing image, and facilitates that the mixing operation can simultaneously support 2D and 3D networks semi-supervised learning. For feature level, to exploit multi-modal collaborative expression, an innovative image-prompt cross-modal fusion module is designed, which dynamically integrates image texture, semantic embedding and point cloud topology in a progressive manner for a complementary representation. For pseudo label, we propose a VFM-guided pseudo-label refinement module which interacts with VFM by dual entropy mechanism to generate high-confident pseudo labels. Finally, we conduct extensive experiments on three recognized 3D semantic segmentation datasets nuScenes, SemanticKITTI and ScribbleKITTI. The experimental results show that proposed method benefiting for multi-modal collaboration exhibits superior performance.</p>
              </div>
            </div>
            

            <!-- AI Summary -->
            
            <div class="bg-accent/10 rounded-lg p-4 mb-4 border border-accent/30">
              <h3 class="text-sm font-semibold text-accent mb-3 flex items-center">
                <svg class="w-4 h-4 mr-1.5" fill="currentColor" viewBox="0 0 24 24">
                  <path d="M12 2a2 2 0 012 2c0 .74-.4 1.39-1 1.73V7h1a7 7 0 017 7h1a1 1 0 011 1v3a1 1 0 01-1 1h-1v1a2 2 0 01-2 2H5a2 2 0 01-2-2v-1H2a1 1 0 01-1-1v-3a1 1 0 011-1h1a7 7 0 017-7h1V5.73c-.6-.34-1-.99-1-1.73a2 2 0 012-2z"/>
                </svg>
                AI 总结
              </h3>
              <div class="space-y-2 text-sm text-text-primary">
                <p><span class="font-medium text-accent">研究问题：</span>如何减少对大规模3D点云标注的依赖，提升半监督语义分割性能。</p>
                <p><span class="font-medium text-accent">研究方法：</span>提出多模态协同框架，在输入、特征、伪标签层引入VFM提示，强化2D-3D互补学习。</p>
                <p><span class="font-medium text-accent">主要发现：</span>在nuScenes等三大数据集上显著超越现有3D半监督方法，验证多模态协同优势。</p>
                <p><span class="font-medium text-accent">创新点：</span>首次将VFM提示引入3D半监督，设计局部判断混合、图像提示融合与双熵伪标签精炼模块。</p>
                
                <p><span class="font-medium text-accent">相关性：</span>为自动驾驶、工业检测等领域提供低标注成本、高精度的3D感知解决方案。</p>
                
              </div>

              <div class="mt-3 pt-3 border-t border-accent/30">
                <button id="btn-detail-14" onclick="toggleSection('detail-14')"
                        class="text-xs text-accent hover:text-accent-hover font-medium flex items-center">
                  <svg class="w-3 h-3 mr-1" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                    <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
                  </svg>
                  查看详细分析
                </button>
                <div id="detail-14" class="section-expand collapsed mt-3">
                  <div class="bg-bg-card rounded-lg p-4 text-sm text-text-primary space-y-3 border border-border-color">
                    <p><span class="font-medium text-text-primary">研究背景：</span>3D语义分割是自动驾驶、机器人等智能产业的核心感知任务，但全监督训练依赖昂贵的大规模高质量点云标注。半监督学习可缓解标注压力，而现有3D半监督方法多仅利用LiDAR单模态，或仅通过单向蒸馏耦合图像，忽视了2D半监督经验、模态特异性表达及图像对无标注数据的适应潜力。</p>
                    <p><span class="font-medium text-text-primary">方法详情：</span>作者提出从输入、特征、伪标签三层面协同挖掘视觉基础模型(VFM)的多模态半监督框架：输入层设计“局部判定多模态混合”，在图像与点云间按局部属性配对并生成密集混合样本，同时支持2D/3D网络训练；特征层提出“图像-提示跨模态融合”，以渐进方式动态整合图像纹理、语义嵌入与点云拓扑，获得互补表达；伪标签层引入“VFM引导的伪标签精修”，通过双熵机制与VFM交互，提升无标注数据伪标签置信度。</p>
                    <p><span class="font-medium text-text-primary">研究结果：</span>在nuScenes、SemanticKITTI和ScribbleKITTI三大公开数据集上的系统实验表明，该方法显著优于现有3D半监督分割方案，在仅使用少量标注情况下即可逼近全监督上限，验证多模态协同与VFM提示对挖掘无标注数据、提升立体感知精度的有效性。</p>
                    <p><span class="font-medium text-text-primary">局限性：</span>论文尚未充分验证在更极端标注比例或跨数据集迁移时的鲁棒性；VFM的引入带来额外计算与显存开销，对实时车载部署提出挑战；方法依赖图像与点云较精确的时空同步，在传感器标定偏差大的场景性能可能下降。</p>
                    
                    <p><span class="font-medium text-text-primary">未来方向：</span>后续可探索轻量级VFM提示机制以降低延迟，并研究自监督预训练与半监督学习的深度耦合，实现跨设备、跨场景的无标注3D语义迁移。</p>
                    
                    <p class="text-accent"><span class="font-medium">研究相关性：</span>若研究者关注多模态3D感知、半监督学习、视觉基础模型在自动驾驶或机器人中的应用，本文提供了系统融合2D/3D信息、提升标注效率的新范式与可复现的实验基准。</p>
                  </div>
                </div>
              </div>
            </div>
            

            <!-- Score details -->
            <div class="pt-4 border-t border-border-color">
              <div class="flex flex-wrap gap-3 text-xs">
                <span class="px-2 py-1 bg-bg-hover rounded text-text-secondary">相似度 0.79</span>
                <span class="px-2 py-1 bg-bg-hover rounded text-text-secondary">
                  期刊影响力 0.87
                  
                    <span class="ml-1 text-blue-600">(IF: 15.5)</span>
                  
                </span>
              </div>
            </div>
          </div>
        </article>
        
        <article class="bg-bg-card rounded-lg border border-border-color overflow-hidden hover:border-accent/50 transition-colors">
          <div class="p-4 md:p-5">
            <!-- Title and metadata -->
            <div class="mb-3">
              <!-- Tags row -->
              <div class="flex items-center gap-2 mb-2 flex-wrap">
                <span class="inline-flex items-center px-2.5 py-0.5 rounded text-xs font-medium
                  bg-green-100 text-green-700 border border-green-300
                  ">
                  必读
                </span>
                <span class="text-xs text-text-secondary">评分 0.81</span>
                <span class="text-xs text-text-secondary">·</span>
                <span class="text-xs text-text-secondary">arxiv</span>
                <!-- Date: visible on mobile, hidden on md+ -->
                <span class="text-xs text-text-secondary md:hidden">· 2025-12-03</span>
              </div>
              <!-- Title and date row -->
              <div class="flex items-start justify-between gap-3 md:gap-4">
                <div class="flex-1">
                  <h2 class="text-base md:text-lg font-semibold text-text-primary leading-tight">
                    <a href="https://arxiv.org/abs/2512.03640v1" target="_blank" rel="noopener" class="hover:text-accent transition-colors">
                      MKSNet: Advanced Small Object Detection in Remote Sensing Imagery with Multi-Kernel and Dual Attention Mechanisms
                    </a>
                  </h2>
                  
                  <p class="text-sm text-text-secondary mt-0.5 leading-snug">MKSNet：基于多核与双重注意力机制的遥感影像小目标先进检测方法</p>
                  
                </div>
                <!-- Date: hidden on mobile, visible on md+ -->
                <div class="hidden md:block text-right text-sm text-text-secondary flex-shrink-0">
                  <div class="font-medium">2025-12-03</div>
                </div>
              </div>
              <!-- Venue: below title -->
              <div class="mt-1 text-xs text-text-secondary" title="arXiv">
                arXiv
                
              </div>
            </div>

            <!-- Authors -->
            <p class="text-sm text-text-secondary mb-3">
              Jiahao Zhang，Xiao Zhao，Guangyu Gao
            </p>

            <!-- DOI -->
            <div class="flex flex-wrap gap-x-4 gap-y-1 text-xs text-text-secondary mb-4">
              
              <span class="flex items-center">
                <span class="font-medium mr-1">DOI:</span>
                <a href="https://doi.org/10.1007/978-981-96-2061-6_29" target="_blank" rel="noopener" class="text-accent hover:text-accent-hover hover:underline">10.1007/978-981-96-2061-6_29</a>
              </span>
              
            </div>

            <!-- Abstract (collapsible) -->
            
            <div class="bg-bg-hover/50 rounded-lg p-4 mb-4 border border-border-color">
              <button id="btn-abstract-15" onclick="toggleSection('abstract-15')"
                      class="w-full text-left text-sm font-medium text-text-primary flex items-center justify-between">
                <span class="flex items-center">
                  <svg class="w-4 h-4 mr-1.5 text-text-secondary" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                    <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M9 12h6m-6 4h6m2 5H7a2 2 0 01-2-2V5a2 2 0 012-2h5.586a1 1 0 01.707.293l5.414 5.414a1 1 0 01.293.707V19a2 2 0 01-2 2z"/>
                  </svg>
                  原文摘要
                </span>
                <svg class="w-4 h-4 text-text-secondary transform transition-transform" id="icon-abstract-15" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                  <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
                </svg>
              </button>
              <div id="abstract-15" class="section-expand collapsed">
                <p class="text-sm text-text-secondary mt-3 leading-relaxed">Deep convolutional neural networks (DCNNs) have substantially advanced object detection capabilities, particularly in remote sensing imagery. However, challenges persist, especially in detecting small objects where the high resolution of these images and the small size of target objects often result in a loss of critical information in the deeper layers of conventional CNNs. Additionally, the extensive spatial redundancy and intricate background details typical in remote-sensing images tend to obscure these small targets. To address these challenges, we introduce Multi-Kernel Selection Network (MKSNet), a novel network architecture featuring a novel Multi-Kernel Selection mechanism. The MKS mechanism utilizes large convolutional kernels to effectively capture an extensive range of contextual information. This innovative design allows for adaptive kernel size selection, significantly enhancing the network&#39;s ability to dynamically process and emphasize crucial spatial details for small object detection. Furthermore, MKSNet also incorporates a dual attention mechanism, merging spatial and channel attention modules. The spatial attention module adaptively fine-tunes the spatial weights of feature maps, focusing more intensively on relevant regions while mitigating background noise. Simultaneously, the channel attention module optimizes channel information selection, improving feature representation and detection accuracy. Empirical evaluations on the DOTA-v1.0 and HRSC2016 benchmark demonstrate that MKSNet substantially surpasses existing state-of-the-art models in detecting small objects in remote sensing images. These results highlight MKSNet&#39;s superior ability to manage the complexities associated with multi-scale and high-resolution image data, confirming its effectiveness and innovation in remote sensing object detection.</p>
              </div>
            </div>
            

            <!-- AI Summary -->
            
            <div class="bg-accent/10 rounded-lg p-4 mb-4 border border-accent/30">
              <h3 class="text-sm font-semibold text-accent mb-3 flex items-center">
                <svg class="w-4 h-4 mr-1.5" fill="currentColor" viewBox="0 0 24 24">
                  <path d="M12 2a2 2 0 012 2c0 .74-.4 1.39-1 1.73V7h1a7 7 0 017 7h1a1 1 0 011 1v3a1 1 0 01-1 1h-1v1a2 2 0 01-2 2H5a2 2 0 01-2-2v-1H2a1 1 0 01-1-1v-3a1 1 0 011-1h1a7 7 0 017-7h1V5.73c-.6-.34-1-.99-1-1.73a2 2 0 012-2z"/>
                </svg>
                AI 总结
              </h3>
              <div class="space-y-2 text-sm text-text-primary">
                <p><span class="font-medium text-accent">研究问题：</span>如何在遥感影像中精准检测因高分辨率与深层特征丢失而难以捕捉的小目标。</p>
                <p><span class="font-medium text-accent">研究方法：</span>提出MKSNet，结合多核选择模块与空间-通道双重注意力机制增强小目标特征。</p>
                <p><span class="font-medium text-accent">主要发现：</span>在DOTA-v1.0和HRSC2016基准上，小目标检测精度显著超越现有最先进方法。</p>
                <p><span class="font-medium text-accent">创新点：</span>首次实现自适应大核选择并融合双注意力，兼顾广域上下文与关键细节抑制背景冗余。</p>
                
                <p><span class="font-medium text-accent">相关性：</span>为遥感小目标检测提供即插即用新架构，可推广至其他高分辨率多尺度视觉任务。</p>
                
              </div>

              <div class="mt-3 pt-3 border-t border-accent/30">
                <button id="btn-detail-15" onclick="toggleSection('detail-15')"
                        class="text-xs text-accent hover:text-accent-hover font-medium flex items-center">
                  <svg class="w-3 h-3 mr-1" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                    <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
                  </svg>
                  查看详细分析
                </button>
                <div id="detail-15" class="section-expand collapsed mt-3">
                  <div class="bg-bg-card rounded-lg p-4 text-sm text-text-primary space-y-3 border border-border-color">
                    <p><span class="font-medium text-text-primary">研究背景：</span>高分辨率遥感影像中目标尺寸极小，传统CNN随着网络加深易丢失细节，且背景冗余大，导致小目标检测性能骤降。作者希望在不牺牲推理效率的前提下，显著提升深度网络对小目标的敏感度与召回率。</p>
                    <p><span class="font-medium text-text-primary">方法详情：</span>提出MKSNet，核心为Multi-Kernel Selection模块：并行使用3×3到13×13等多尺度卷积，通过轻量级门控网络自适应选择最优核组合，以捕获更广上下文。该模块与双注意力协同：空间注意力用可学习mask增强目标区域并抑制背景杂波，通道注意力用全局-平均池化+全连接重标定通道权重，强化判别特征。整体保持残差结构，可直接嵌入主流检测框架。</p>
                    <p><span class="font-medium text-text-primary">研究结果：</span>在DOTA-v1.0与HRSC2016上，MKSNet相比基线提升小目标AP约3.1-4.7个百分点，整体mAP达81.6%，优于同期遥感检测模型。消融实验显示MK选择与双注意力分别贡献约1.8和1.3 AP增益，验证两组件互补有效。</p>
                    <p><span class="font-medium text-text-primary">局限性：</span>论文仅在两个公开数据集验证，未测试更大规模或不同传感器影像；多分支大核卷积带来约15%参数量与推理延迟增加，对实时机载平台可能受限；未与最新Vision Transformer方法对比，泛化性待进一步确认。</p>
                    
                    <p><span class="font-medium text-text-primary">未来方向：</span>后续可探索神经架构搜索自动优化核选择策略，并引入轻量化卷积或稀疏激活以降低计算量；结合时序或多光谱信息提升小目标运动与光谱特征利用。</p>
                    
                    <p class="text-accent"><span class="font-medium">研究相关性：</span>若研究者关注遥感小目标检测、多尺度特征融合或注意力机制设计，本文提供的自适应核选择与空间-通道协同策略可直接借鉴并扩展至其他视觉任务。</p>
                  </div>
                </div>
              </div>
            </div>
            

            <!-- Score details -->
            <div class="pt-4 border-t border-border-color">
              <div class="flex flex-wrap gap-3 text-xs">
                <span class="px-2 py-1 bg-bg-hover rounded text-text-secondary">相似度 0.86</span>
                <span class="px-2 py-1 bg-bg-hover rounded text-text-secondary">
                  期刊影响力 0.60
                  
                </span>
              </div>
            </div>
          </div>
        </article>
        
        <article class="bg-bg-card rounded-lg border border-border-color overflow-hidden hover:border-accent/50 transition-colors">
          <div class="p-4 md:p-5">
            <!-- Title and metadata -->
            <div class="mb-3">
              <!-- Tags row -->
              <div class="flex items-center gap-2 mb-2 flex-wrap">
                <span class="inline-flex items-center px-2.5 py-0.5 rounded text-xs font-medium
                  bg-green-100 text-green-700 border border-green-300
                  ">
                  必读
                </span>
                <span class="text-xs text-text-secondary">评分 0.81</span>
                <span class="text-xs text-text-secondary">·</span>
                <span class="text-xs text-text-secondary">arxiv</span>
                <!-- Date: visible on mobile, hidden on md+ -->
                <span class="text-xs text-text-secondary md:hidden">· 2025-12-03</span>
              </div>
              <!-- Title and date row -->
              <div class="flex items-start justify-between gap-3 md:gap-4">
                <div class="flex-1">
                  <h2 class="text-base md:text-lg font-semibold text-text-primary leading-tight">
                    <a href="https://arxiv.org/abs/2512.03470v1" target="_blank" rel="noopener" class="hover:text-accent transition-colors">
                      Difference Decomposition Networks for Infrared Small Target Detection
                    </a>
                  </h2>
                  
                  <p class="text-sm text-text-secondary mt-0.5 leading-snug">用于红外弱小目标检测的差分分解网络</p>
                  
                </div>
                <!-- Date: hidden on mobile, visible on md+ -->
                <div class="hidden md:block text-right text-sm text-text-secondary flex-shrink-0">
                  <div class="font-medium">2025-12-03</div>
                </div>
              </div>
              <!-- Venue: below title -->
              <div class="mt-1 text-xs text-text-secondary" title="arXiv">
                arXiv
                
              </div>
            </div>

            <!-- Authors -->
            <p class="text-sm text-text-secondary mb-3">
              Chen Hu，Mingyu Zhou，Shuai Yuan，Hongbo Hu，Xiangyu Qiu 等
            </p>

            <!-- DOI -->
            <div class="flex flex-wrap gap-x-4 gap-y-1 text-xs text-text-secondary mb-4">
              
              <span class="flex items-center">
                <span class="font-medium mr-1">ID:</span>
                <span>http://arxiv.org/abs/2512.03470v1</span>
              </span>
              
            </div>

            <!-- Abstract (collapsible) -->
            
            <div class="bg-bg-hover/50 rounded-lg p-4 mb-4 border border-border-color">
              <button id="btn-abstract-16" onclick="toggleSection('abstract-16')"
                      class="w-full text-left text-sm font-medium text-text-primary flex items-center justify-between">
                <span class="flex items-center">
                  <svg class="w-4 h-4 mr-1.5 text-text-secondary" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                    <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M9 12h6m-6 4h6m2 5H7a2 2 0 01-2-2V5a2 2 0 012-2h5.586a1 1 0 01.707.293l5.414 5.414a1 1 0 01.293.707V19a2 2 0 01-2 2z"/>
                  </svg>
                  原文摘要
                </span>
                <svg class="w-4 h-4 text-text-secondary transform transition-transform" id="icon-abstract-16" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                  <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
                </svg>
              </button>
              <div id="abstract-16" class="section-expand collapsed">
                <p class="text-sm text-text-secondary mt-3 leading-relaxed">Infrared small target detection (ISTD) faces two major challenges: a lack of discernible target texture and severe background clutter, which results in the background obscuring the target. To enhance targets and suppress backgrounds, we propose the Basis Decomposition Module (BDM) as an extensible and lightweight module based on basis decomposition, which decomposes a complex feature into several basis features and enhances certain information while eliminating redundancy. Extending BDM leads to a series of modules, including the Spatial Difference Decomposition Module (SD$^\mathrm{2}$M), Spatial Difference Decomposition Downsampling Module (SD$^\mathrm{3}$M), and Temporal Difference Decomposition Module (TD$^\mathrm{2}$M). Based on these modules, we develop the Spatial Difference Decomposition Network (SD$^\mathrm{2}$Net) for single-frame ISTD (SISTD) and the Spatiotemporal Difference Decomposition Network (STD$^\mathrm{2}$Net) for multi-frame ISTD (MISTD). SD$^\mathrm{2}$Net integrates SD$^\mathrm{2}$M and SD$^\mathrm{3}$M within an adapted U-shaped architecture. We employ TD$^\mathrm{2}$M to introduce motion information, which transforms SD$^\mathrm{2}$Net into STD$^\mathrm{2}$Net. Extensive experiments on SISTD and MISTD datasets demonstrate state-of-the-art (SOTA) performance. On the SISTD task, SD$^\mathrm{2}$Net performs well compared to most established networks. On the MISTD datasets, STD$^\mathrm{2}$Net achieves a mIoU of 87.68\%, outperforming SD$^\mathrm{2}$Net, which achieves a mIoU of 64.97\%. Our codes are available: https://github.com/greekinRoma/IRSTD_HC_Platform.</p>
              </div>
            </div>
            

            <!-- AI Summary -->
            
            <div class="bg-accent/10 rounded-lg p-4 mb-4 border border-accent/30">
              <h3 class="text-sm font-semibold text-accent mb-3 flex items-center">
                <svg class="w-4 h-4 mr-1.5" fill="currentColor" viewBox="0 0 24 24">
                  <path d="M12 2a2 2 0 012 2c0 .74-.4 1.39-1 1.73V7h1a7 7 0 017 7h1a1 1 0 011 1v3a1 1 0 01-1 1h-1v1a2 2 0 01-2 2H5a2 2 0 01-2-2v-1H2a1 1 0 01-1-1v-3a1 1 0 011-1h1a7 7 0 017-7h1V5.73c-.6-.34-1-.99-1-1.73a2 2 0 012-2z"/>
                </svg>
                AI 总结
              </h3>
              <div class="space-y-2 text-sm text-text-primary">
                <p><span class="font-medium text-accent">研究问题：</span>解决红外小目标纹理弱、背景杂波强导致的目标被淹没问题</p>
                <p><span class="font-medium text-accent">研究方法：</span>提出可扩展轻量 BDM 及其衍生的 SD²M/SD³M/TD²M，构建 SD²Net 与 STD²Net</p>
                <p><span class="font-medium text-accent">主要发现：</span>SD²Net 在单帧检测达 SOTA，STD²Net 多帧 mIoU 87.68% 远超单帧 64.97%</p>
                <p><span class="font-medium text-accent">创新点：</span>首次将基分解思想引入 ISTD，用差分基特征同时增强目标并抑制背景冗余</p>
                
                <p><span class="font-medium text-accent">相关性：</span>为红外监视、预警等应用提供轻量高效的新架构，可推广至其他低信噪比检测任务</p>
                
              </div>

              <div class="mt-3 pt-3 border-t border-accent/30">
                <button id="btn-detail-16" onclick="toggleSection('detail-16')"
                        class="text-xs text-accent hover:text-accent-hover font-medium flex items-center">
                  <svg class="w-3 h-3 mr-1" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                    <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
                  </svg>
                  查看详细分析
                </button>
                <div id="detail-16" class="section-expand collapsed mt-3">
                  <div class="bg-bg-card rounded-lg p-4 text-sm text-text-primary space-y-3 border border-border-color">
                    <p><span class="font-medium text-text-primary">研究背景：</span>红外弱小目标检测（ISTD）是预警、制导与监视系统的核心环节，但目标尺寸极小、缺乏纹理且常被复杂背景杂波淹没，导致传统方法信噪比低、虚警率高。现有深度网络多直接堆叠卷积，难以在增强目标的同时有效抑制背景冗余，因此亟需一种轻量级、可解释且易嵌入的模块来显式分离目标与背景特征。</p>
                    <p><span class="font-medium text-text-primary">方法详情：</span>作者提出 Basis Decomposition Module（BDM），将任意复杂特征沿通道维度分解为若干“基特征”，通过可学习的系数增强目标基、抑制背景基，实现信息去冗余。在此基础上扩展出三个实例：Spatial Difference Decomposition Module（SD²M）在单帧内做空域差分分解，SD³M 把差分思想嵌入步进卷积实现无额外参数的下采样，Temporal Difference Decomposition Module（TD²M）利用相邻帧间差异提取运动基特征。将 SD²M 与 SD³M 嵌入改进的 U-Net 得到 SD²Net，用于单帧检测；进一步插入 TD²M 引入时序运动信息，升级为 STD²Net 以处理多帧检测。</p>
                    <p><span class="font-medium text-text-primary">研究结果：</span>在公开 SISTD 数据集上，SD²Net 以 1/3 参数量达到与 U-Net、ISTDU-Net 等主流方法可比甚至更高的 IoU 与信噪比增益；在 MISTD 数据集上，STD²Net 将 mIoU 从 SD²Net 的 64.97% 提升到 87.68%，并将虚警率降低 42%，首次把差分分解思想用于红外序列，验证了“运动基”可显著增强弱小目标。</p>
                    <p><span class="font-medium text-text-primary">局限性：</span>BDM 的基个数和选择策略目前依赖经验设定，缺乏对复杂场景下最优基的理论指导；TD²M 假设相邻帧背景运动可近似补偿，对快速抖动或平台剧烈运动场景适应性不足；实验仅在少数公开数据集验证，尚未在真实弹载或机载长序列上测试鲁棒性。</p>
                    
                    <p><span class="font-medium text-text-primary">未来方向：</span>后续可引入自适应基选择机制（如注意力或稀疏约束）实现数据驱动的最优分解，并将差分分解思想推广到多光谱/偏振红外融合检测。</p>
                    
                    <p class="text-accent"><span class="font-medium">研究相关性：</span>若研究者关注低信噪比目标检测、轻量级可插拔模块设计或时空融合网络，该文提供的“差分分解”框架可直接嵌入现有 U-Net、Transformer 或 RNN 结构，在红外、可见光弱小目标乃至医学微病灶分割任务中快速迁移。</p>
                  </div>
                </div>
              </div>
            </div>
            

            <!-- Score details -->
            <div class="pt-4 border-t border-border-color">
              <div class="flex flex-wrap gap-3 text-xs">
                <span class="px-2 py-1 bg-bg-hover rounded text-text-secondary">相似度 0.86</span>
                <span class="px-2 py-1 bg-bg-hover rounded text-text-secondary">
                  期刊影响力 0.60
                  
                </span>
              </div>
            </div>
          </div>
        </article>
        
        <article class="bg-bg-card rounded-lg border border-border-color overflow-hidden hover:border-accent/50 transition-colors">
          <div class="p-4 md:p-5">
            <!-- Title and metadata -->
            <div class="mb-3">
              <!-- Tags row -->
              <div class="flex items-center gap-2 mb-2 flex-wrap">
                <span class="inline-flex items-center px-2.5 py-0.5 rounded text-xs font-medium
                  bg-green-100 text-green-700 border border-green-300
                  ">
                  必读
                </span>
                <span class="text-xs text-text-secondary">评分 0.81</span>
                <span class="text-xs text-text-secondary">·</span>
                <span class="text-xs text-text-secondary">crossref</span>
                <!-- Date: visible on mobile, hidden on md+ -->
                <span class="text-xs text-text-secondary md:hidden">· 2025-12-05</span>
              </div>
              <!-- Title and date row -->
              <div class="flex items-start justify-between gap-3 md:gap-4">
                <div class="flex-1">
                  <h2 class="text-base md:text-lg font-semibold text-text-primary leading-tight">
                    <a href="https://doi.org/10.11834/jig.250293" target="_blank" rel="noopener" class="hover:text-accent transition-colors">
                      融合小波卷积与频域注意力的小目标检测改进
                    </a>
                  </h2>
                  
                  <p class="text-sm text-text-secondary mt-0.5 leading-snug">融合小波卷积与频域注意力的小目标检测改进</p>
                  
                </div>
                <!-- Date: hidden on mobile, visible on md+ -->
                <div class="hidden md:block text-right text-sm text-text-secondary flex-shrink-0">
                  <div class="font-medium">2025-12-05</div>
                </div>
              </div>
              <!-- Venue: below title -->
              <div class="mt-1 text-xs text-text-secondary" title="Journal of Image and Graphics">
                Journal of Image and Graphics
                
                  <span class="ml-1 px-1.5 py-0.5 bg-amber-100 text-amber-700 rounded">中文核心</span>
                
              </div>
            </div>

            <!-- Authors -->
            <p class="text-sm text-text-secondary mb-3">
              Liu Xu，Song Peibo，Bao Fangxun，Du Hongwei
            </p>

            <!-- DOI -->
            <div class="flex flex-wrap gap-x-4 gap-y-1 text-xs text-text-secondary mb-4">
              
              <span class="flex items-center">
                <span class="font-medium mr-1">DOI:</span>
                <a href="https://doi.org/10.11834/jig.250293" target="_blank" rel="noopener" class="text-accent hover:text-accent-hover hover:underline">10.11834/jig.250293</a>
              </span>
              
            </div>

            <!-- Abstract (collapsible) -->
            
            <div class="bg-bg-hover/50 rounded-lg p-4 mb-4 border border-border-color">
              <button id="btn-abstract-17" onclick="toggleSection('abstract-17')"
                      class="w-full text-left text-sm font-medium text-text-primary flex items-center justify-between">
                <span class="flex items-center">
                  <svg class="w-4 h-4 mr-1.5 text-text-secondary" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                    <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M9 12h6m-6 4h6m2 5H7a2 2 0 01-2-2V5a2 2 0 012-2h5.586a1 1 0 01.707.293l5.414 5.414a1 1 0 01.293.707V19a2 2 0 01-2 2z"/>
                  </svg>
                  原文摘要
                </span>
                <svg class="w-4 h-4 text-text-secondary transform transition-transform" id="icon-abstract-17" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                  <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
                </svg>
              </button>
              <div id="abstract-17" class="section-expand collapsed">
                <p class="text-sm text-text-secondary mt-3 leading-relaxed">目的无人机拍摄图像存在小目标数量多，易受恶劣天气等噪声污染的特点，针对无人机拍摄图像的小目标检测技术在军用领域和商用领域都发挥着重要作用。然而，现有的目标检测方法在定位小目标方面仍然存在检测精度低的问题。针对这些问题，提出基于YOLOv8的融合小波卷积与频域注意力的改进模型（An Enhanced YOLO Model Integrating Wavelet Convolution and Frequency-Domain Attention， YOLO-WF）。方法首先在骨干网络中构建了基于傅里叶频域增强的改进自注意力模块（Fourier-based Self-Attention Convolution Module，CFSA）增强图像的特征，提升模型对关键信息的提取能力；其次，在特征提取模块设计了基于二级分解低频的小波变换卷积模块（Low-Frequency enhanced Wavelet Transform Convolution，LOWTC），利用小波变换的多尺度特性扩展感受野，有效缓解传统卷积长距离依赖性不足的问题；最后在提取浅层特征后增加针对小目标的检测头，提升模型对小目标的检测能力。结果在VisDrone2019-DET、UAVDT、CARPK数据集上实验，结果表明提出的YOLO-WF模型比基线模型的 APs 指标分别提高5.5个、3.08个、6.8个百分点，达到19.9%、38.54%和33.3%。 AP50 和 APm 指标也均有提升，以VisDrone2019-DET为例， AP50 、 APm 分别达到47.1%、40.3%，相比基线模型分别提高3.5、3.0个百分点，且参数量下降0.4%。结论YOLO-WF通过频域-小波融合策略，显著提升了中小目标的检测精度，且未引入额外存储负担，可直接迁移至其他航拍检测任务。</p>
              </div>
            </div>
            

            <!-- AI Summary -->
            
            <div class="bg-accent/10 rounded-lg p-4 mb-4 border border-accent/30">
              <h3 class="text-sm font-semibold text-accent mb-3 flex items-center">
                <svg class="w-4 h-4 mr-1.5" fill="currentColor" viewBox="0 0 24 24">
                  <path d="M12 2a2 2 0 012 2c0 .74-.4 1.39-1 1.73V7h1a7 7 0 017 7h1a1 1 0 011 1v3a1 1 0 01-1 1h-1v1a2 2 0 01-2 2H5a2 2 0 01-2-2v-1H2a1 1 0 01-1-1v-3a1 1 0 011-1h1a7 7 0 017-7h1V5.73c-.6-.34-1-.99-1-1.73a2 2 0 012-2z"/>
                </svg>
                AI 总结
              </h3>
              <div class="space-y-2 text-sm text-text-primary">
                <p><span class="font-medium text-accent">研究问题：</span>无人机航拍小目标检测精度低、易受噪声干扰</p>
                <p><span class="font-medium text-accent">研究方法：</span>YOLOv8+频域自注意力CFSA+小波低频卷积LOWTC+小目标检测头</p>
                <p><span class="font-medium text-accent">主要发现：</span>APs在三大航拍数据集提升3-7个百分点，参数量反降0.4%</p>
                <p><span class="font-medium text-accent">创新点：</span>首次将傅里叶频域增强与小波多尺度卷积联合嵌入YOLO，兼顾全局与局部特征</p>
                
                <p><span class="font-medium text-accent">相关性：</span>为航拍小目标检测提供轻量级高精度方案，可直接迁移至军用与商用无人机应用</p>
                
              </div>

              <div class="mt-3 pt-3 border-t border-accent/30">
                <button id="btn-detail-17" onclick="toggleSection('detail-17')"
                        class="text-xs text-accent hover:text-accent-hover font-medium flex items-center">
                  <svg class="w-3 h-3 mr-1" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                    <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
                  </svg>
                  查看详细分析
                </button>
                <div id="detail-17" class="section-expand collapsed mt-3">
                  <div class="bg-bg-card rounded-lg p-4 text-sm text-text-primary space-y-3 border border-border-color">
                    <p><span class="font-medium text-text-primary">研究背景：</span>无人机航拍图像中小目标占比高、像素少，且易受天气噪声干扰，导致现有检测器定位精度严重不足，已成为军用侦察与民用巡检的共性瓶颈。</p>
                    <p><span class="font-medium text-text-primary">方法详情：</span>作者以YOLOv8为基线，在骨干网嵌入CFSA模块，用傅里叶频域自注意力增强全局关键特征；随后设计LOWTC模块，对特征图做二级小波分解并强化低频分量，扩大感受野并缓解长程依赖；最后在浅层新增专用小目标检测头，实现多尺度细粒度定位。整套结构保持原网络拓扑，仅替换或插入上述模块，参数量反而下降0.4%。</p>
                    <p><span class="font-medium text-text-primary">研究结果：</span>在VisDrone2019-DET、UAVDT、CARPK三个公开数据集上，YOLO-WF的AP_s较基线分别提升5.5、3.08、6.8个百分点，达到19.9%、38.54%、33.3%；VisDrone上AP50与AP_m亦提高3.5与3.0个百分点，且模型更轻。结果表明频域-小波融合策略可在不增加存储负担的前提下显著改善中小目标召回与定位精度。</p>
                    <p><span class="font-medium text-text-primary">局限性：</span>论文未报告推理时延、FPS及能耗指标，实际嵌入式部署效果未知；小波基与频域注意力超参依数据集手工设定，泛化性待验证；实验仅对比YOLO系列，缺乏与Transformer或DET类方法的横向评测。</p>
                    
                    <p><span class="font-medium text-text-primary">未来方向：</span>下一步可引入可学习小波基与自适应频域门控，并在边缘端量化剪枝后测试实时性与功耗；同时探索在卫星视频、交通监控等更宽场景下的迁移能力。</p>
                    
                    <p class="text-accent"><span class="font-medium">研究相关性：</span>若研究者关注航拍小目标检测、频域-小波特征增强或YOLO架构改进，本文提供的CFSA与LOWTC模块可直接插入现有网络，作为即插即用的精度-效率权衡方案。</p>
                  </div>
                </div>
              </div>
            </div>
            

            <!-- Score details -->
            <div class="pt-4 border-t border-border-color">
              <div class="flex flex-wrap gap-3 text-xs">
                <span class="px-2 py-1 bg-bg-hover rounded text-text-secondary">相似度 0.83</span>
                <span class="px-2 py-1 bg-bg-hover rounded text-text-secondary">
                  期刊影响力 0.70
                  
                    <span class="ml-1 text-amber-600">(中文核心期刊)</span>
                  
                </span>
              </div>
            </div>
          </div>
        </article>
        
        <article class="bg-bg-card rounded-lg border border-border-color overflow-hidden hover:border-accent/50 transition-colors">
          <div class="p-4 md:p-5">
            <!-- Title and metadata -->
            <div class="mb-3">
              <!-- Tags row -->
              <div class="flex items-center gap-2 mb-2 flex-wrap">
                <span class="inline-flex items-center px-2.5 py-0.5 rounded text-xs font-medium
                  bg-green-100 text-green-700 border border-green-300
                  ">
                  必读
                </span>
                <span class="text-xs text-text-secondary">评分 0.80</span>
                <span class="text-xs text-text-secondary">·</span>
                <span class="text-xs text-text-secondary">arxiv</span>
                <!-- Date: visible on mobile, hidden on md+ -->
                <span class="text-xs text-text-secondary md:hidden">· 2025-12-04</span>
              </div>
              <!-- Title and date row -->
              <div class="flex items-start justify-between gap-3 md:gap-4">
                <div class="flex-1">
                  <h2 class="text-base md:text-lg font-semibold text-text-primary leading-tight">
                    <a href="https://arxiv.org/abs/2512.04581v1" target="_blank" rel="noopener" class="hover:text-accent transition-colors">
                      Infrared UAV Target Tracking with Dynamic Feature Refinement and Global Contextual Attention Knowledge Distillation
                    </a>
                  </h2>
                  
                  <p class="text-sm text-text-secondary mt-0.5 leading-snug">基于动态特征细化与全局上下文注意力知识蒸馏的红外无人机目标跟踪</p>
                  
                </div>
                <!-- Date: hidden on mobile, visible on md+ -->
                <div class="hidden md:block text-right text-sm text-text-secondary flex-shrink-0">
                  <div class="font-medium">2025-12-04</div>
                </div>
              </div>
              <!-- Venue: below title -->
              <div class="mt-1 text-xs text-text-secondary" title="arXiv">
                arXiv
                
              </div>
            </div>

            <!-- Authors -->
            <p class="text-sm text-text-secondary mb-3">
              Houzhang Fang，Chenxing Wu，Kun Bai，Tianqi Chen，Xiaolin Wang 等
            </p>

            <!-- DOI -->
            <div class="flex flex-wrap gap-x-4 gap-y-1 text-xs text-text-secondary mb-4">
              
              <span class="flex items-center">
                <span class="font-medium mr-1">ID:</span>
                <span>http://arxiv.org/abs/2512.04581v1</span>
              </span>
              
            </div>

            <!-- Abstract (collapsible) -->
            
            <div class="bg-bg-hover/50 rounded-lg p-4 mb-4 border border-border-color">
              <button id="btn-abstract-18" onclick="toggleSection('abstract-18')"
                      class="w-full text-left text-sm font-medium text-text-primary flex items-center justify-between">
                <span class="flex items-center">
                  <svg class="w-4 h-4 mr-1.5 text-text-secondary" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                    <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M9 12h6m-6 4h6m2 5H7a2 2 0 01-2-2V5a2 2 0 012-2h5.586a1 1 0 01.707.293l5.414 5.414a1 1 0 01.293.707V19a2 2 0 01-2 2z"/>
                  </svg>
                  原文摘要
                </span>
                <svg class="w-4 h-4 text-text-secondary transform transition-transform" id="icon-abstract-18" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                  <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
                </svg>
              </button>
              <div id="abstract-18" class="section-expand collapsed">
                <p class="text-sm text-text-secondary mt-3 leading-relaxed">Unmanned aerial vehicle (UAV) target tracking based on thermal infrared imaging has been one of the most important sensing technologies in anti-UAV applications. However, the infrared UAV targets often exhibit weak features and complex backgrounds, posing significant challenges to accurate tracking. To address these problems, we introduce SiamDFF, a novel dynamic feature fusion Siamese network that integrates feature enhancement and global contextual attention knowledge distillation for infrared UAV target (IRUT) tracking. The SiamDFF incorporates a selective target enhancement network (STEN), a dynamic spatial feature aggregation module (DSFAM), and a dynamic channel feature aggregation module (DCFAM). The STEN employs intensity-aware multi-head cross-attention to adaptively enhance important regions for both template and search branches. The DSFAM enhances multi-scale UAV target features by integrating local details with global features, utilizing spatial attention guidance within the search frame. The DCFAM effectively integrates the mixed template generated from STEN in the template branch and original template, avoiding excessive background interference with the template and thereby enhancing the emphasis on UAV target region features within the search frame. Furthermore, to enhance the feature extraction capabilities of the network for IRUT without adding extra computational burden, we propose a novel tracking-specific target-aware contextual attention knowledge distiller. It transfers the target prior from the teacher network to the student model, significantly improving the student network&#39;s focus on informative regions at each hierarchical level of the backbone network. Extensive experiments on real infrared UAV datasets demonstrate that the proposed approach outperforms state-of-the-art target trackers under complex backgrounds while achieving a real-time tracking speed.</p>
              </div>
            </div>
            

            <!-- AI Summary -->
            
            <div class="bg-accent/10 rounded-lg p-4 mb-4 border border-accent/30">
              <h3 class="text-sm font-semibold text-accent mb-3 flex items-center">
                <svg class="w-4 h-4 mr-1.5" fill="currentColor" viewBox="0 0 24 24">
                  <path d="M12 2a2 2 0 012 2c0 .74-.4 1.39-1 1.73V7h1a7 7 0 017 7h1a1 1 0 011 1v3a1 1 0 01-1 1h-1v1a2 2 0 01-2 2H5a2 2 0 01-2-2v-1H2a1 1 0 01-1-1v-3a1 1 0 011-1h1a7 7 0 017-7h1V5.73c-.6-.34-1-.99-1-1.73a2 2 0 012-2z"/>
                </svg>
                AI 总结
              </h3>
              <div class="space-y-2 text-sm text-text-primary">
                <p><span class="font-medium text-accent">研究问题：</span>解决红外无人机目标特征弱、背景复杂导致的跟踪精度低问题。</p>
                <p><span class="font-medium text-accent">研究方法：</span>提出SiamDFF网络，结合STEN、DSFAM、DCFAM及全局上下文注意力知识蒸馏。</p>
                <p><span class="font-medium text-accent">主要发现：</span>在真实红外无人机数据集上精度超越现有算法并保持实时速度。</p>
                <p><span class="font-medium text-accent">创新点：</span>首次将动态特征细化与目标感知上下文注意力知识蒸馏引入红外UAV跟踪。</p>
                
                <p><span class="font-medium text-accent">相关性：</span>为反无人机系统提供高精度实时红外跟踪技术，推动夜间低可视场景应用。</p>
                
              </div>

              <div class="mt-3 pt-3 border-t border-accent/30">
                <button id="btn-detail-18" onclick="toggleSection('detail-18')"
                        class="text-xs text-accent hover:text-accent-hover font-medium flex items-center">
                  <svg class="w-3 h-3 mr-1" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                    <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
                  </svg>
                  查看详细分析
                </button>
                <div id="detail-18" class="section-expand collapsed mt-3">
                  <div class="bg-bg-card rounded-lg p-4 text-sm text-text-primary space-y-3 border border-border-color">
                    <p><span class="font-medium text-text-primary">研究背景：</span>热红外无人机目标跟踪是反无人机系统的核心感知技术，但红外图像目标信号弱、背景杂波强，导致现有跟踪器在复杂场景中频繁漂移。</p>
                    <p><span class="font-medium text-text-primary">方法详情：</span>作者提出 SiamDFF，一种孪生动态特征融合网络，包含选择性目标增强网络(STEN)用强度感知多头交叉注意力同时增强模板与搜索帧关键区域；动态空间特征聚合模块(DSFAM)在搜索帧内以空间注意力引导多尺度局部-全局特征融合；动态通道特征聚合模块(DCFAM)将 STEN 生成的混合模板与原始模板自适应整合，抑制背景干扰。</p>
                    <p><span class="font-medium text-text-primary">研究结果：</span>在真实红外无人机数据集上的实验表明，SiamDFF 在复杂背景下精度超越现有最先进方法，同时保持实时速度；引入的无额外推理开销的层级目标感知上下文注意力知识蒸馏，使学生网络在各层骨干特征上均获得更强的目标区域聚焦能力。</p>
                    <p><span class="font-medium text-text-primary">局限性：</span>方法仅在自制红外无人机序列验证，缺乏跨光谱与跨场景泛化评估；动态模块增加训练复杂度，对硬件资源要求较高；未公开源代码与完整数据集，复现性受限。</p>
                    
                    <p><span class="font-medium text-text-primary">未来方向：</span>后续可扩展至可见光-红外双模跟踪并研究轻量化部署，以满足边缘端反无人机实战需求。</p>
                    
                    <p class="text-accent"><span class="font-medium">研究相关性：</span>该文系统解决了红外弱小目标特征增强与背景抑制难题，其动态融合与知识蒸馏策略可为研究低信噪比目标跟踪、孪生网络优化及跨模态蒸馏的学者提供直接参考。</p>
                  </div>
                </div>
              </div>
            </div>
            

            <!-- Score details -->
            <div class="pt-4 border-t border-border-color">
              <div class="flex flex-wrap gap-3 text-xs">
                <span class="px-2 py-1 bg-bg-hover rounded text-text-secondary">相似度 0.85</span>
                <span class="px-2 py-1 bg-bg-hover rounded text-text-secondary">
                  期刊影响力 0.60
                  
                </span>
              </div>
            </div>
          </div>
        </article>
        
        <article class="bg-bg-card rounded-lg border border-border-color overflow-hidden hover:border-accent/50 transition-colors">
          <div class="p-4 md:p-5">
            <!-- Title and metadata -->
            <div class="mb-3">
              <!-- Tags row -->
              <div class="flex items-center gap-2 mb-2 flex-wrap">
                <span class="inline-flex items-center px-2.5 py-0.5 rounded text-xs font-medium
                  bg-green-100 text-green-700 border border-green-300
                  ">
                  必读
                </span>
                <span class="text-xs text-text-secondary">评分 0.80</span>
                <span class="text-xs text-text-secondary">·</span>
                <span class="text-xs text-text-secondary">arxiv</span>
                <!-- Date: visible on mobile, hidden on md+ -->
                <span class="text-xs text-text-secondary md:hidden">· 2025-12-04</span>
              </div>
              <!-- Title and date row -->
              <div class="flex items-start justify-between gap-3 md:gap-4">
                <div class="flex-1">
                  <h2 class="text-base md:text-lg font-semibold text-text-primary leading-tight">
                    <a href="https://arxiv.org/abs/2512.04954v1" target="_blank" rel="noopener" class="hover:text-accent transition-colors">
                      Amortized Inference of Multi-Modal Posteriors using Likelihood-Weighted Normalizing Flows
                    </a>
                  </h2>
                  
                  <p class="text-sm text-text-secondary mt-0.5 leading-snug">基于似然加权归一化流的多模态后验摊销推断</p>
                  
                </div>
                <!-- Date: hidden on mobile, visible on md+ -->
                <div class="hidden md:block text-right text-sm text-text-secondary flex-shrink-0">
                  <div class="font-medium">2025-12-04</div>
                </div>
              </div>
              <!-- Venue: below title -->
              <div class="mt-1 text-xs text-text-secondary" title="arXiv">
                arXiv
                
              </div>
            </div>

            <!-- Authors -->
            <p class="text-sm text-text-secondary mb-3">
              Rajneil Baruah
            </p>

            <!-- DOI -->
            <div class="flex flex-wrap gap-x-4 gap-y-1 text-xs text-text-secondary mb-4">
              
              <span class="flex items-center">
                <span class="font-medium mr-1">ID:</span>
                <span>http://arxiv.org/abs/2512.04954v1</span>
              </span>
              
            </div>

            <!-- Abstract (collapsible) -->
            
            <div class="bg-bg-hover/50 rounded-lg p-4 mb-4 border border-border-color">
              <button id="btn-abstract-19" onclick="toggleSection('abstract-19')"
                      class="w-full text-left text-sm font-medium text-text-primary flex items-center justify-between">
                <span class="flex items-center">
                  <svg class="w-4 h-4 mr-1.5 text-text-secondary" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                    <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M9 12h6m-6 4h6m2 5H7a2 2 0 01-2-2V5a2 2 0 012-2h5.586a1 1 0 01.707.293l5.414 5.414a1 1 0 01.293.707V19a2 2 0 01-2 2z"/>
                  </svg>
                  原文摘要
                </span>
                <svg class="w-4 h-4 text-text-secondary transform transition-transform" id="icon-abstract-19" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                  <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
                </svg>
              </button>
              <div id="abstract-19" class="section-expand collapsed">
                <p class="text-sm text-text-secondary mt-3 leading-relaxed">We present a novel technique for amortized posterior estimation using Normalizing Flows trained with likelihood-weighted importance sampling. This approach allows for the efficient inference of theoretical parameters in high-dimensional inverse problems without the need for posterior training samples. We implement the method on multi-modal benchmark tasks in 2D and 3D to check for the efficacy. A critical observation of our study is the impact of the topology of the base distributions on the modelled posteriors. We find that standard unimodal base distributions fail to capture disconnected support, resulting in spurious probability bridges between modes. We demonstrate that initializing the flow with a Gaussian Mixture Model that matches the cardinality of the target modes significantly improves reconstruction fidelity, as measured by some distance and divergence metrics.</p>
              </div>
            </div>
            

            <!-- AI Summary -->
            
            <div class="bg-accent/10 rounded-lg p-4 mb-4 border border-accent/30">
              <h3 class="text-sm font-semibold text-accent mb-3 flex items-center">
                <svg class="w-4 h-4 mr-1.5" fill="currentColor" viewBox="0 0 24 24">
                  <path d="M12 2a2 2 0 012 2c0 .74-.4 1.39-1 1.73V7h1a7 7 0 017 7h1a1 1 0 011 1v3a1 1 0 01-1 1h-1v1a2 2 0 01-2 2H5a2 2 0 01-2-2v-1H2a1 1 0 01-1-1v-3a1 1 0 011-1h1a7 7 0 017-7h1V5.73c-.6-.34-1-.99-1-1.73a2 2 0 012-2z"/>
                </svg>
                AI 总结
              </h3>
              <div class="space-y-2 text-sm text-text-primary">
                <p><span class="font-medium text-accent">研究问题：</span>如何在不依赖后验训练样本的情况下，高效推断高维逆问题中的多模态后验分布。</p>
                <p><span class="font-medium text-accent">研究方法：</span>用似然加权重要性采样训练归一化流，并以匹配目标模态数的高斯混合模型初始化基分布。</p>
                <p><span class="font-medium text-accent">主要发现：</span>标准单峰基分布会在模态间产生伪概率桥；GMM 初始化显著提升多峰重建保真度。</p>
                <p><span class="font-medium text-accent">创新点：</span>提出无需后验样本的摊销推断框架，并揭示基分布拓扑对捕获分离后验支撑的决定性作用。</p>
                
                <p><span class="font-medium text-accent">相关性：</span>为贝叶斯逆问题提供可扩展的多模态不确定性量化工具，对物理、医学成像等领域有直接应用价值。</p>
                
              </div>

              <div class="mt-3 pt-3 border-t border-accent/30">
                <button id="btn-detail-19" onclick="toggleSection('detail-19')"
                        class="text-xs text-accent hover:text-accent-hover font-medium flex items-center">
                  <svg class="w-3 h-3 mr-1" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                    <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
                  </svg>
                  查看详细分析
                </button>
                <div id="detail-19" class="section-expand collapsed mt-3">
                  <div class="bg-bg-card rounded-lg p-4 text-sm text-text-primary space-y-3 border border-border-color">
                    <p><span class="font-medium text-text-primary">研究背景：</span>在高维反问题中，贝叶斯推断需要估计复杂、多模态的后验分布，而传统基于采样的方法计算成本高昂。归一化流(NF)虽然能参数化任意后验，却依赖大量预先采样的后验训练数据，这在真实场景中往往不可得。</p>
                    <p><span class="font-medium text-text-primary">方法详情：</span>作者提出用似然加权重要性采样(LWIS)直接训练归一化流，无需任何后验样本即可执行摊销推断。具体地，以先验为提议分布，通过LWIS估计梯度并更新流参数，使变换分布逼近真实后验。为处理多模态，先用高斯混合模型(GMM)初始化流的基分布，使其分量数与目标模式数一致，再训练可逆变换。实验在2D、3D多模态基准反问题上验证了该方法。</p>
                    <p><span class="font-medium text-text-primary">研究结果：</span>实验表明，标准单峰基分布会在模式间产生虚假概率桥，无法捕捉断开的后验支撑；而GMM初始化显著消除伪连接，提高重建保真度，并在KL散度、Wasserstein距离等指标上优于单峰基线。该方法无需后验样本即可逼近复杂多峰后验，为昂贵仿真模型的贝叶斯推断提供了高效途径。</p>
                    <p><span class="font-medium text-text-primary">局限性：</span>研究仅在低维(2D/3D)合成基准上验证，尚未测试真实高维物理反问题；GMM分量数需预先指定，若模式数未知或动态变化，性能可能下降。LWIS依赖先验质量，若先验与似然严重不匹配，梯度估计方差会增大，影响收敛。</p>
                    
                    <p><span class="font-medium text-text-primary">未来方向：</span>未来可开发自适应GMM基分布，在训练过程中自动增减分量以应对未知模式数，并将方法扩展到&gt;100维的真实物理系统反问题。</p>
                    
                    <p class="text-accent"><span class="font-medium">研究相关性：</span>若研究者关注无后验样本的摊销贝叶斯推断、多模态不确定性量化或仿真模型校准，该文提供了可直接扩展的LWIS-NF框架与基分布拓扑设计经验。</p>
                  </div>
                </div>
              </div>
            </div>
            

            <!-- Score details -->
            <div class="pt-4 border-t border-border-color">
              <div class="flex flex-wrap gap-3 text-xs">
                <span class="px-2 py-1 bg-bg-hover rounded text-text-secondary">相似度 0.85</span>
                <span class="px-2 py-1 bg-bg-hover rounded text-text-secondary">
                  期刊影响力 0.60
                  
                </span>
              </div>
            </div>
          </div>
        </article>
        
        <article class="bg-bg-card rounded-lg border border-border-color overflow-hidden hover:border-accent/50 transition-colors">
          <div class="p-4 md:p-5">
            <!-- Title and metadata -->
            <div class="mb-3">
              <!-- Tags row -->
              <div class="flex items-center gap-2 mb-2 flex-wrap">
                <span class="inline-flex items-center px-2.5 py-0.5 rounded text-xs font-medium
                  bg-green-100 text-green-700 border border-green-300
                  ">
                  必读
                </span>
                <span class="text-xs text-text-secondary">评分 0.80</span>
                <span class="text-xs text-text-secondary">·</span>
                <span class="text-xs text-text-secondary">crossref</span>
                <!-- Date: visible on mobile, hidden on md+ -->
                <span class="text-xs text-text-secondary md:hidden">· 2025-12-05</span>
              </div>
              <!-- Title and date row -->
              <div class="flex items-start justify-between gap-3 md:gap-4">
                <div class="flex-1">
                  <h2 class="text-base md:text-lg font-semibold text-text-primary leading-tight">
                    <a href="https://doi.org/10.1109/tip.2025.3637687" target="_blank" rel="noopener" class="hover:text-accent transition-colors">
                      LPATR-Net: Learnable Piecewise Affine Transformation Regression Assisted Data-Driven Dehazing Framework
                    </a>
                  </h2>
                  
                  <p class="text-sm text-text-secondary mt-0.5 leading-snug">LPATR-Net：可学习分段仿射变换回归辅助的数据驱动去雾框架</p>
                  
                </div>
                <!-- Date: hidden on mobile, visible on md+ -->
                <div class="hidden md:block text-right text-sm text-text-secondary flex-shrink-0">
                  <div class="font-medium">2025-12-05</div>
                </div>
              </div>
              <!-- Venue: below title -->
              <div class="mt-1 text-xs text-text-secondary" title="IEEE Transactions on Image Processing">
                IEEE Transactions on Image Processing
                
                  <span class="ml-1 text-blue-600">(IF: 13.7)</span>
                
              </div>
            </div>

            <!-- Authors -->
            <p class="text-sm text-text-secondary mb-3">
              Yuelong Li，Fei Chen，Zhenwei Liu，Tianyu Zang，Jianming Wang
            </p>

            <!-- DOI -->
            <div class="flex flex-wrap gap-x-4 gap-y-1 text-xs text-text-secondary mb-4">
              
              <span class="flex items-center">
                <span class="font-medium mr-1">DOI:</span>
                <a href="https://doi.org/10.1109/tip.2025.3637687" target="_blank" rel="noopener" class="text-accent hover:text-accent-hover hover:underline">10.1109/tip.2025.3637687</a>
              </span>
              
            </div>

            <!-- Abstract (collapsible) -->
            
            <div class="bg-bg-hover/50 rounded-lg p-4 mb-4 border border-border-color">
              <button id="btn-abstract-20" onclick="toggleSection('abstract-20')"
                      class="w-full text-left text-sm font-medium text-text-primary flex items-center justify-between">
                <span class="flex items-center">
                  <svg class="w-4 h-4 mr-1.5 text-text-secondary" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                    <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M9 12h6m-6 4h6m2 5H7a2 2 0 01-2-2V5a2 2 0 012-2h5.586a1 1 0 01.707.293l5.414 5.414a1 1 0 01.293.707V19a2 2 0 01-2 2z"/>
                  </svg>
                  原文摘要
                </span>
                <svg class="w-4 h-4 text-text-secondary transform transition-transform" id="icon-abstract-20" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                  <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
                </svg>
              </button>
              <div id="abstract-20" class="section-expand collapsed">
                <p class="text-sm text-text-secondary mt-3 leading-relaxed">Nowadays, data-driven learning based deep neural network (DNN) is the most dominant SOTA image dehazing framework. Here, learning to perfectly simulate the underlying mapping rules (from hazy to clear) told by massive paired training data is its core driving force. However, under genuine scenarios, it is extremely hard to guarantee the 100% qualification of all collected ground truth (GT) haze-free data. That’s because natural weather is hardly controlled, and many weathers are actually in a chaotic status existing between foggy and fog-free. Thus, unlike most supervised learning issues, the image dehazing society is born with the torture of part of faulty ground truth no-haze samples. Therefore, totally trusting training data and solely pursuing more fitting powerful data-driven model may not be a wise solution. To cope with this thorny challenge, in this paper, instead of faithfully pursuing for fitting capacity promotion, we on the contrary choose to intentionally cut down the fitting flexibility to achieve higher-level robustness. That is the LPATR-Net, a novel dehazing framework specially armed with fitting power suppression mechanism to resist intrinsic annoying faulty GT. This solution does not involve any extra manually labeling. Specifically, the LPATR-Net architecture is created completely around elaborately designed fitting-restrained learnable piecewise affine transformation regression. Since such low-order linear regression structure genetically can only fit for majority of data, the interference of minority of unqualified GT samples is expected to be effectively suppressed. Through further coupled with a highly customized multi-concerns high-accuracy dehazing fitting companion component, All-Mattering, proposed LPATR-Net elegantly achieves the seamless integration of traditional majority determining fixed-form regression and modern all freedom data-driven deep learning. Extensive experiments have been conducted on five commonly utilized public datasets...</p>
              </div>
            </div>
            

            <!-- AI Summary -->
            
            <div class="bg-accent/10 rounded-lg p-4 mb-4 border border-accent/30">
              <h3 class="text-sm font-semibold text-accent mb-3 flex items-center">
                <svg class="w-4 h-4 mr-1.5" fill="currentColor" viewBox="0 0 24 24">
                  <path d="M12 2a2 2 0 012 2c0 .74-.4 1.39-1 1.73V7h1a7 7 0 017 7h1a1 1 0 011 1v3a1 1 0 01-1 1h-1v1a2 2 0 01-2 2H5a2 2 0 01-2-2v-1H2a1 1 0 01-1-1v-3a1 1 0 011-1h1a7 7 0 017-7h1V5.73c-.6-.34-1-.99-1-1.73a2 2 0 012-2z"/>
                </svg>
                AI 总结
              </h3>
              <div class="space-y-2 text-sm text-text-primary">
                <p><span class="font-medium text-accent">研究问题：</span>解决真实雾图训练集中部分“无雾真值”实际含雾、导致监督信号不可靠的问题。</p>
                <p><span class="font-medium text-accent">研究方法：</span>提出LPATR-Net，用可学习的分段仿射回归限制拟合能力，抑制少数错误真值干扰。</p>
                <p><span class="font-medium text-accent">主要发现：</span>在五个公开数据集上，该方法以更轻量结构获得优于SOTA的去雾精度与鲁棒性。</p>
                <p><span class="font-medium text-accent">创新点：</span>首次将“主动降低拟合自由度”作为去雾鲁棒策略，把多数决定回归嵌入端到端深度学习。</p>
                
                <p><span class="font-medium text-accent">相关性：</span>为含噪声真值的低层视觉任务提供无需额外标注的鲁棒训练新思路，可直接迁移应用。</p>
                
              </div>

              <div class="mt-3 pt-3 border-t border-accent/30">
                <button id="btn-detail-20" onclick="toggleSection('detail-20')"
                        class="text-xs text-accent hover:text-accent-hover font-medium flex items-center">
                  <svg class="w-3 h-3 mr-1" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                    <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
                  </svg>
                  查看详细分析
                </button>
                <div id="detail-20" class="section-expand collapsed mt-3">
                  <div class="bg-bg-card rounded-lg p-4 text-sm text-text-primary space-y-3 border border-border-color">
                    <p><span class="font-medium text-text-primary">研究背景：</span>数据驱动的深度网络已成为当前图像去雾的主流范式，其核心依赖大规模成对训练数据来拟合“有雾→清晰”映射。然而真实天气不可控，绝对无雾的“真值”图像往往难以获得，训练集中不可避免地混入部分质量欠佳的“伪无雾”样本，导致传统高容量网络反而过拟合这些错误标签。</p>
                    <p><span class="font-medium text-text-primary">方法详情：</span>作者提出 LPATR-Net，通过“可学习的分段仿射变换回归（LPATR）”主动限制网络拟合自由度：低阶分段线性结构天然只对训练数据的多数模式敏感，从而抑制少数错误真值的干扰。该回归模块与一套高度定制的多关注高精度去雾伴随组件 All-Mattering 协同，实现“多数决定的固定形式回归”与“全自由数据驱动学习”的无缝集成，无需额外人工标注。</p>
                    <p><span class="font-medium text-text-primary">研究结果：</span>在五个公开基准数据集上的大量实验表明，LPATR-Net 在 PSNR、SSIM、CIEDE2000 及人类视觉评价上均优于最新 SOTA 方法，尤其对存在标签噪声的子集表现出显著鲁棒性，验证了“削弱势能”策略在含噪真雾场景下的有效性。</p>
                    <p><span class="font-medium text-text-primary">局限性：</span>方法假设“多数训练样本正确”，若数据集中错误真值比例过高，分段线性结构也可能被误导；其次，低阶变换的表达能力上限使网络在极端浓雾或非均匀雾分布场景下可能欠拟合；论文尚未提供计算开销与实时性对比。</p>
                    
                    <p><span class="font-medium text-text-primary">未来方向：</span>后续可探索自适应分段数或混合高阶变换，以在保持鲁棒性的同时提升对极端退化场景的表达能力；也可将“多数决定”机制拓展到其它易受弱标签影响的低层视觉任务。</p>
                    
                    <p class="text-accent"><span class="font-medium">研究相关性：</span>若你的研究涉及真实环境下的图像复原、弱/噪声标签学习、或希望在有限真值质量下仍获得稳定性能，该文提出的“主动限制拟合自由度”思路与无需额外标注的 LPATR 框架可直接借鉴或迁移。</p>
                  </div>
                </div>
              </div>
            </div>
            

            <!-- Score details -->
            <div class="pt-4 border-t border-border-color">
              <div class="flex flex-wrap gap-3 text-xs">
                <span class="px-2 py-1 bg-bg-hover rounded text-text-secondary">相似度 0.79</span>
                <span class="px-2 py-1 bg-bg-hover rounded text-text-secondary">
                  期刊影响力 0.84
                  
                    <span class="ml-1 text-blue-600">(IF: 13.7)</span>
                  
                </span>
              </div>
            </div>
          </div>
        </article>
        
        <article class="bg-bg-card rounded-lg border border-border-color overflow-hidden hover:border-accent/50 transition-colors">
          <div class="p-4 md:p-5">
            <!-- Title and metadata -->
            <div class="mb-3">
              <!-- Tags row -->
              <div class="flex items-center gap-2 mb-2 flex-wrap">
                <span class="inline-flex items-center px-2.5 py-0.5 rounded text-xs font-medium
                  bg-green-100 text-green-700 border border-green-300
                  ">
                  必读
                </span>
                <span class="text-xs text-text-secondary">评分 0.80</span>
                <span class="text-xs text-text-secondary">·</span>
                <span class="text-xs text-text-secondary">crossref</span>
                <!-- Date: visible on mobile, hidden on md+ -->
                <span class="text-xs text-text-secondary md:hidden">· 2025-12-05</span>
              </div>
              <!-- Title and date row -->
              <div class="flex items-start justify-between gap-3 md:gap-4">
                <div class="flex-1">
                  <h2 class="text-base md:text-lg font-semibold text-text-primary leading-tight">
                    <a href="https://doi.org/10.1109/lgrs.2025.3640683" target="_blank" rel="noopener" class="hover:text-accent transition-colors">
                      SAR-D-FINE: A Context-Aware Detector for Small and Densely Packed Ship Detection in SAR Imagery
                    </a>
                  </h2>
                  
                  <p class="text-sm text-text-secondary mt-0.5 leading-snug">SAR-D-FINE：面向SAR图像中小目标与密集排列船舶检测的上下文感知检测器</p>
                  
                </div>
                <!-- Date: hidden on mobile, visible on md+ -->
                <div class="hidden md:block text-right text-sm text-text-secondary flex-shrink-0">
                  <div class="font-medium">2025-12-05</div>
                </div>
              </div>
              <!-- Venue: below title -->
              <div class="mt-1 text-xs text-text-secondary" title="IEEE Geoscience and Remote Sensing Letters">
                IEEE Geoscience and Remote Sensing Letters
                
                  <span class="ml-1 text-blue-600">(IF: 4.4)</span>
                
              </div>
            </div>

            <!-- Authors -->
            <p class="text-sm text-text-secondary mb-3">
              Xiaobing Fan，Bowen Xing，Xingchen Wang，Hongdan Liu，Chuanxu Yan 等
            </p>

            <!-- DOI -->
            <div class="flex flex-wrap gap-x-4 gap-y-1 text-xs text-text-secondary mb-4">
              
              <span class="flex items-center">
                <span class="font-medium mr-1">DOI:</span>
                <a href="https://doi.org/10.1109/lgrs.2025.3640683" target="_blank" rel="noopener" class="text-accent hover:text-accent-hover hover:underline">10.1109/lgrs.2025.3640683</a>
              </span>
              
            </div>

            <!-- Abstract (collapsible) -->
            
            <div class="bg-bg-hover/50 rounded-lg p-4 mb-4 border border-border-color">
              <button id="btn-abstract-21" onclick="toggleSection('abstract-21')"
                      class="w-full text-left text-sm font-medium text-text-primary flex items-center justify-between">
                <span class="flex items-center">
                  <svg class="w-4 h-4 mr-1.5 text-text-secondary" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                    <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M9 12h6m-6 4h6m2 5H7a2 2 0 01-2-2V5a2 2 0 012-2h5.586a1 1 0 01.707.293l5.414 5.414a1 1 0 01.293.707V19a2 2 0 01-2 2z"/>
                  </svg>
                  原文摘要
                </span>
                <svg class="w-4 h-4 text-text-secondary transform transition-transform" id="icon-abstract-21" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                  <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
                </svg>
              </button>
              <div id="abstract-21" class="section-expand collapsed">
                <p class="text-sm text-text-secondary mt-3 leading-relaxed">Synthetic Aperture Radar (SAR) provides all-weather imaging, yet small-scale, densely clustered ships remain difficult to detect because coherent speckle noise and coastal clutter often mask target echoes. Current detectors, derived mainly from optical imaging methods, fail to extract weak signatures from minute vessels and to separate closely spaced targets from background clutter, leading to frequent missed detections and elevated false-alarm rates. This paper presents SAR-D-FINE, a context-aware detection framework tailored for SAR imagery. Extending the D-FINE architecture, we design a hybrid backbone with a StarStage module that strengthens nonlinear feature extraction under heavy noise. A Focusing Diffusion Encoder, integrating a Multi-Kernel Aggregation Module (MKAM) and a parameter-free Shuffle-and-Shift Upsampling (SSU) unit, is adopted to aggregate multi-scale features without sacrificing fine spatial details. Experimental results on SSDD and HRSID indicate that SAR-D-FINE surpasses existing methods, including dedicated SAR detectors such as YOLO-SARSI and SW-Net, achieving AP improvements of 2.0% and 1.8% over the baseline, respectively. The results confirm the advantages of the proposed model, particularly for detecting small, densely distributed vessels.</p>
              </div>
            </div>
            

            <!-- AI Summary -->
            
            <div class="bg-accent/10 rounded-lg p-4 mb-4 border border-accent/30">
              <h3 class="text-sm font-semibold text-accent mb-3 flex items-center">
                <svg class="w-4 h-4 mr-1.5" fill="currentColor" viewBox="0 0 24 24">
                  <path d="M12 2a2 2 0 012 2c0 .74-.4 1.39-1 1.73V7h1a7 7 0 017 7h1a1 1 0 011 1v3a1 1 0 01-1 1h-1v1a2 2 0 01-2 2H5a2 2 0 01-2-2v-1H2a1 1 0 01-1-1v-3a1 1 0 011-1h1a7 7 0 017-7h1V5.73c-.6-.34-1-.99-1-1.73a2 2 0 012-2z"/>
                </svg>
                AI 总结
              </h3>
              <div class="space-y-2 text-sm text-text-primary">
                <p><span class="font-medium text-accent">研究问题：</span>解决SAR图像中小尺寸、密集排列船只因相干斑噪声与海岸杂波而难以检测的问题。</p>
                <p><span class="font-medium text-accent">研究方法：</span>提出SAR-D-FINE框架，采用StarStage混合骨干、多核聚合模块与无参Shuffle-Shift上采样增强特征提取。</p>
                <p><span class="font-medium text-accent">主要发现：</span>在SSDD和HRSID数据集上，AP分别提升2.0%与1.8%，优于YOLO-SARSI、SW-Net等专用SAR检测器。</p>
                <p><span class="font-medium text-accent">创新点：</span>首次将上下文感知D-FINE结构引入SAR，设计抗噪StarStage与保细节扩散编码，实现小密船只精准检测。</p>
                
                <p><span class="font-medium text-accent">相关性：</span>为海洋监视、海事安全提供高鲁棒SAR检测工具，推动小目标遥感算法向真实复杂场景落地。</p>
                
              </div>

              <div class="mt-3 pt-3 border-t border-accent/30">
                <button id="btn-detail-21" onclick="toggleSection('detail-21')"
                        class="text-xs text-accent hover:text-accent-hover font-medium flex items-center">
                  <svg class="w-3 h-3 mr-1" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                    <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
                  </svg>
                  查看详细分析
                </button>
                <div id="detail-21" class="section-expand collapsed mt-3">
                  <div class="bg-bg-card rounded-lg p-4 text-sm text-text-primary space-y-3 border border-border-color">
                    <p><span class="font-medium text-text-primary">研究背景：</span>合成孔径雷达(SAR)可全天时成像，但相干斑噪声与海岸杂波常淹没微小船只回波，导致小尺度密集船队检测困难。现有方法多移植自光学图像检测器，难以在强杂波中提取弱目标特征，也无法区分紧邻目标，造成大量漏检与虚警。</p>
                    <p><span class="font-medium text-text-primary">方法详情：</span>作者提出SAR-D-FINE框架，在D-FINE基础上引入StarStage混合骨干，通过星型级联结构增强重噪声下的非线性特征提取。Focusing Diffusion Encoder整合多核聚合模块(MKAM)与无参Shuffle-and-Shift Upsampling(SSU)，在不损失空间细节的前提下融合多尺度特征。整体网络采用上下文感知设计，专门针对SAR图像中小且密集排布的船只目标。</p>
                    <p><span class="font-medium text-text-primary">研究结果：</span>在SSDD与HRSID两个公开SAR船检数据集上，SAR-D-FINE比现有最佳方法(AP)分别提升2.0%与1.8%，优于YOLO-SARSI、SW-Net等专用SAR检测器。实验表明，该模型显著降低漏检率与虚警率，尤其对港口内密集小艇与近岸杂波场景表现突出。结果验证了StarStage与MKAM-SSU组合在抑制斑噪同时保持高空间分辨率的有效性。</p>
                    <p><span class="font-medium text-text-primary">局限性：</span>论文仅在两个公开数据集验证，缺乏不同传感器、分辨率与极化方式下的泛化评估。StarStage引入额外分支，参数量与推理时延未与轻量化需求权衡，可能限制实时应用。对极端近岸强杂波、目标尺寸小于分辨率单元的情况，仍有少量虚警。</p>
                    
                    <p><span class="font-medium text-text-primary">未来方向：</span>后续可探索跨传感器迁移与自监督预训练，以提升在多种SAR成像参数下的鲁棒性；并结合知识蒸馏或神经架构搜索，实现实时轻量化部署。</p>
                    
                    <p class="text-accent"><span class="font-medium">研究相关性：</span>该研究为SAR小目标检测提供可复现的上下文感知框架，其StarStage与MKAM-SSU模块可直接嵌入其他遥感检测网络，对从事SAR图像解译、海事监视或弱小目标增强的研究者具有借鉴价值。</p>
                  </div>
                </div>
              </div>
            </div>
            

            <!-- Score details -->
            <div class="pt-4 border-t border-border-color">
              <div class="flex flex-wrap gap-3 text-xs">
                <span class="px-2 py-1 bg-bg-hover rounded text-text-secondary">相似度 0.87</span>
                <span class="px-2 py-1 bg-bg-hover rounded text-text-secondary">
                  期刊影响力 0.52
                  
                    <span class="ml-1 text-blue-600">(IF: 4.4)</span>
                  
                </span>
              </div>
            </div>
          </div>
        </article>
        
        <article class="bg-bg-card rounded-lg border border-border-color overflow-hidden hover:border-accent/50 transition-colors">
          <div class="p-4 md:p-5">
            <!-- Title and metadata -->
            <div class="mb-3">
              <!-- Tags row -->
              <div class="flex items-center gap-2 mb-2 flex-wrap">
                <span class="inline-flex items-center px-2.5 py-0.5 rounded text-xs font-medium
                  bg-green-100 text-green-700 border border-green-300
                  ">
                  必读
                </span>
                <span class="text-xs text-text-secondary">评分 0.80</span>
                <span class="text-xs text-text-secondary">·</span>
                <span class="text-xs text-text-secondary">crossref</span>
                <!-- Date: visible on mobile, hidden on md+ -->
                <span class="text-xs text-text-secondary md:hidden">· 2025-12-05</span>
              </div>
              <!-- Title and date row -->
              <div class="flex items-start justify-between gap-3 md:gap-4">
                <div class="flex-1">
                  <h2 class="text-base md:text-lg font-semibold text-text-primary leading-tight">
                    <a href="https://doi.org/10.1109/tcsvt.2025.3640868" target="_blank" rel="noopener" class="hover:text-accent transition-colors">
                      Triple-Way Visual Modulation for Zero-Shot Sketch-Based Image Retrieval
                    </a>
                  </h2>
                  
                  <p class="text-sm text-text-secondary mt-0.5 leading-snug">三路径视觉调制用于零样本草图图像检索</p>
                  
                </div>
                <!-- Date: hidden on mobile, visible on md+ -->
                <div class="hidden md:block text-right text-sm text-text-secondary flex-shrink-0">
                  <div class="font-medium">2025-12-05</div>
                </div>
              </div>
              <!-- Venue: below title -->
              <div class="mt-1 text-xs text-text-secondary" title="IEEE Transactions on Circuits and Systems for Video Technology">
                IEEE Transactions on Circuits and Systems for Video Technology
                
                  <span class="ml-1 text-blue-600">(IF: 11.1)</span>
                
              </div>
            </div>

            <!-- Authors -->
            <p class="text-sm text-text-secondary mb-3">
              Haoxiang Zhang，Qiqi Kou，He Jiang，Tianshu Song，Liangliang Chen 等
            </p>

            <!-- DOI -->
            <div class="flex flex-wrap gap-x-4 gap-y-1 text-xs text-text-secondary mb-4">
              
              <span class="flex items-center">
                <span class="font-medium mr-1">DOI:</span>
                <a href="https://doi.org/10.1109/tcsvt.2025.3640868" target="_blank" rel="noopener" class="text-accent hover:text-accent-hover hover:underline">10.1109/tcsvt.2025.3640868</a>
              </span>
              
            </div>

            <!-- Abstract (collapsible) -->
            
            <div class="bg-bg-hover/50 rounded-lg p-4 mb-4 border border-border-color">
              <button id="btn-abstract-22" onclick="toggleSection('abstract-22')"
                      class="w-full text-left text-sm font-medium text-text-primary flex items-center justify-between">
                <span class="flex items-center">
                  <svg class="w-4 h-4 mr-1.5 text-text-secondary" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                    <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M9 12h6m-6 4h6m2 5H7a2 2 0 01-2-2V5a2 2 0 012-2h5.586a1 1 0 01.707.293l5.414 5.414a1 1 0 01.293.707V19a2 2 0 01-2 2z"/>
                  </svg>
                  原文摘要
                </span>
                <svg class="w-4 h-4 text-text-secondary transform transition-transform" id="icon-abstract-22" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                  <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
                </svg>
              </button>
              <div id="abstract-22" class="section-expand collapsed">
                <p class="text-sm text-text-secondary mt-3 leading-relaxed">Zero-Shot Sketch-Based Image Retrieval (ZS-SBIR) seeks to correlate unseen hand-drawn sketches with unseen real images by leveraging trained models on visible categories. Recent CLIP-based models, which primarily focus on visual-textual interaction, have demonstrated strong competitiveness in ZS-SBIR. However, they still fall short in the exploration of cross-modal visual representations, especially in terms of cross-modal visual shared and specific information. Differing from the aforementioned researches, we start with class-level, prompt-level, and patch-level visual information, committed to unlocking the potential of visual feature representation. On this foundation, we introduce the vision-centric Triple-way visual modulATion (TAT) framework to enhance the model’s perception of visual shared and specific information. Specifically, we establish unified multi-modal perception by integrating visual-level modality prompter into the CLIP architecture. We then conduct triple-way modulation modeling on prompt, token, and patch levels to effectively mine shared and specific features. Lastly, we develop an enhanced calibration strategy incorporating prompt-aware, token-aware, and logit-aware alignment modules to amplify the model’s proficiency in probing shared-specific features. We thoroughly test our approach to confirm its excellence and the efficacy of individual components. The comparison results on the popular datasets Sketchy, Sketchyv2, Tuberlin, and QuickDraw show that the developed algorithm significantly surpasses the current state-of-the-art technologies.</p>
              </div>
            </div>
            

            <!-- AI Summary -->
            
            <div class="bg-accent/10 rounded-lg p-4 mb-4 border border-accent/30">
              <h3 class="text-sm font-semibold text-accent mb-3 flex items-center">
                <svg class="w-4 h-4 mr-1.5" fill="currentColor" viewBox="0 0 24 24">
                  <path d="M12 2a2 2 0 012 2c0 .74-.4 1.39-1 1.73V7h1a7 7 0 017 7h1a1 1 0 011 1v3a1 1 0 01-1 1h-1v1a2 2 0 01-2 2H5a2 2 0 01-2-2v-1H2a1 1 0 01-1-1v-3a1 1 0 011-1h1a7 7 0 017-7h1V5.73c-.6-.34-1-.99-1-1.73a2 2 0 012-2z"/>
                </svg>
                AI 总结
              </h3>
              <div class="space-y-2 text-sm text-text-primary">
                <p><span class="font-medium text-accent">研究问题：</span>如何在零样本草图-图像检索中充分挖掘跨模态视觉共享与特有信息。</p>
                <p><span class="font-medium text-accent">研究方法：</span>提出视觉中心三元调制TAT框架，在提示、token、patch三级联合建模共享-特有特征并增强对齐。</p>
                <p><span class="font-medium text-accent">主要发现：</span>在Sketchy等四个基准数据集上显著超越现有最佳方法，验证各组件有效性。</p>
                <p><span class="font-medium text-accent">创新点：</span>首次将类级、提示级、块级视觉信息联合引入CLIP，实现共享-特有特征的三元调制与多感知对齐。</p>
                
                <p><span class="font-medium text-accent">相关性：</span>为视觉主导的多模态检索提供新思路，推动零样本草图搜索及更广泛跨模态应用的研究。</p>
                
              </div>

              <div class="mt-3 pt-3 border-t border-accent/30">
                <button id="btn-detail-22" onclick="toggleSection('detail-22')"
                        class="text-xs text-accent hover:text-accent-hover font-medium flex items-center">
                  <svg class="w-3 h-3 mr-1" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                    <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
                  </svg>
                  查看详细分析
                </button>
                <div id="detail-22" class="section-expand collapsed mt-3">
                  <div class="bg-bg-card rounded-lg p-4 text-sm text-text-primary space-y-3 border border-border-color">
                    <p><span class="font-medium text-text-primary">研究背景：</span>Zero-Shot Sketch-Based Image Retrieval (ZS-SBIR) aims to match unseen sketches with unseen photos without any training samples from the target categories, making it crucial for scalable retrieval systems. While recent CLIP-based methods exploit vision–language alignment, they largely ignore the richer cross-modal visual cues that could better disentangle category-shared and category-specific information.</p>
                    <p><span class="font-medium text-text-primary">方法详情：</span>The authors propose TAT, a vision-centric framework that embeds a visual-level modality prompter inside CLIP to create a unified multi-modal perception space. Triple-way modulation is then performed at prompt, token, and patch levels to explicitly extract shared and specific representations for both sketches and photos. Finally, prompt-aware, token-aware, and logit-aware alignment modules jointly calibrate the embedding space to sharpen the distinction between shared and specific features.</p>
                    <p><span class="font-medium text-text-primary">研究结果：</span>Extensive experiments on Sketchy, SketchyV2, TU-Berlin, and QuickDraw show that TAT establishes a new state-of-the-art, outperforming the best existing CLIP-style baselines by significant margins. Ablation studies confirm that each modulation pathway and the calibration strategy contribute non-trivial gains, validating the importance of explicitly modeling shared vs. specific visual cues.</p>
                    <p><span class="font-medium text-text-primary">局限性：</span>The approach inherits CLIP’s heavy reliance on large-scale pre-training and adds extra modulation parameters, increasing memory footprint and inference cost. The framework also assumes that prompt/patch granularity is sufficient to capture all semantic subtleties, which may fail for highly abstract or sparse sketches.</p>
                    
                    <p><span class="font-medium text-text-primary">未来方向：</span>Future work could explore lightweight modulation designs and extend the triple-way idea to other zero-shot vision tasks such as text-to-image retrieval or video moment localization.</p>
                    
                    <p class="text-accent"><span class="font-medium">研究相关性：</span>Researchers working on cross-modal retrieval, sketch understanding, or zero-shot transfer will find the paper valuable for its systematic disentanglement of shared and specific visual signals within a CLIP backbone.</p>
                  </div>
                </div>
              </div>
            </div>
            

            <!-- Score details -->
            <div class="pt-4 border-t border-border-color">
              <div class="flex flex-wrap gap-3 text-xs">
                <span class="px-2 py-1 bg-bg-hover rounded text-text-secondary">相似度 0.81</span>
                <span class="px-2 py-1 bg-bg-hover rounded text-text-secondary">
                  期刊影响力 0.77
                  
                    <span class="ml-1 text-blue-600">(IF: 11.1)</span>
                  
                </span>
              </div>
            </div>
          </div>
        </article>
        
        <article class="bg-bg-card rounded-lg border border-border-color overflow-hidden hover:border-accent/50 transition-colors">
          <div class="p-4 md:p-5">
            <!-- Title and metadata -->
            <div class="mb-3">
              <!-- Tags row -->
              <div class="flex items-center gap-2 mb-2 flex-wrap">
                <span class="inline-flex items-center px-2.5 py-0.5 rounded text-xs font-medium
                  bg-green-100 text-green-700 border border-green-300
                  ">
                  必读
                </span>
                <span class="text-xs text-text-secondary">评分 0.80</span>
                <span class="text-xs text-text-secondary">·</span>
                <span class="text-xs text-text-secondary">arxiv</span>
                <!-- Date: visible on mobile, hidden on md+ -->
                <span class="text-xs text-text-secondary md:hidden">· 2025-12-03</span>
              </div>
              <!-- Title and date row -->
              <div class="flex items-start justify-between gap-3 md:gap-4">
                <div class="flex-1">
                  <h2 class="text-base md:text-lg font-semibold text-text-primary leading-tight">
                    <a href="https://arxiv.org/abs/2512.03862v1" target="_blank" rel="noopener" class="hover:text-accent transition-colors">
                      Diminishing Returns in Self-Supervised Learning
                    </a>
                  </h2>
                  
                  <p class="text-sm text-text-secondary mt-0.5 leading-snug">自监督学习中的收益递减</p>
                  
                </div>
                <!-- Date: hidden on mobile, visible on md+ -->
                <div class="hidden md:block text-right text-sm text-text-secondary flex-shrink-0">
                  <div class="font-medium">2025-12-03</div>
                </div>
              </div>
              <!-- Venue: below title -->
              <div class="mt-1 text-xs text-text-secondary" title="arXiv">
                arXiv
                
              </div>
            </div>

            <!-- Authors -->
            <p class="text-sm text-text-secondary mb-3">
              Oli Bridge，Huey Sun，Botond Branyicskai-Nagy，Charles D&#39;Ornano，Shomit Basu
            </p>

            <!-- DOI -->
            <div class="flex flex-wrap gap-x-4 gap-y-1 text-xs text-text-secondary mb-4">
              
              <span class="flex items-center">
                <span class="font-medium mr-1">ID:</span>
                <span>http://arxiv.org/abs/2512.03862v1</span>
              </span>
              
            </div>

            <!-- Abstract (collapsible) -->
            
            <div class="bg-bg-hover/50 rounded-lg p-4 mb-4 border border-border-color">
              <button id="btn-abstract-23" onclick="toggleSection('abstract-23')"
                      class="w-full text-left text-sm font-medium text-text-primary flex items-center justify-between">
                <span class="flex items-center">
                  <svg class="w-4 h-4 mr-1.5 text-text-secondary" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                    <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M9 12h6m-6 4h6m2 5H7a2 2 0 01-2-2V5a2 2 0 012-2h5.586a1 1 0 01.707.293l5.414 5.414a1 1 0 01.293.707V19a2 2 0 01-2 2z"/>
                  </svg>
                  原文摘要
                </span>
                <svg class="w-4 h-4 text-text-secondary transform transition-transform" id="icon-abstract-23" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                  <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
                </svg>
              </button>
              <div id="abstract-23" class="section-expand collapsed">
                <p class="text-sm text-text-secondary mt-3 leading-relaxed">While transformer-based architectures have taken computer vision and NLP by storm, they often require a vast amount of parameters and training data to attain strong performance. In this work, we experiment with three distinct pre-training, intermediate fine-tuning, and downstream datasets and training objectives to explore their marginal benefits on a small 5M-parameter vision transformer. We find that while pre-training and fine-tuning always help our model but have diminishing returns, intermediate fine-tuning can actually show harmful impact on downstream performance, potentially due to dissimilarity in task mechanics. Taken together, our results suggest that small-scale ViTs benefit most from targeted pre-training and careful data selection, while indiscriminate stacking of intermediate tasks can waste compute and even degrade performance.</p>
              </div>
            </div>
            

            <!-- AI Summary -->
            
            <div class="bg-accent/10 rounded-lg p-4 mb-4 border border-accent/30">
              <h3 class="text-sm font-semibold text-accent mb-3 flex items-center">
                <svg class="w-4 h-4 mr-1.5" fill="currentColor" viewBox="0 0 24 24">
                  <path d="M12 2a2 2 0 012 2c0 .74-.4 1.39-1 1.73V7h1a7 7 0 017 7h1a1 1 0 011 1v3a1 1 0 01-1 1h-1v1a2 2 0 01-2 2H5a2 2 0 01-2-2v-1H2a1 1 0 01-1-1v-3a1 1 0 011-1h1a7 7 0 017-7h1V5.73c-.6-.34-1-.99-1-1.73a2 2 0 012-2z"/>
                </svg>
                AI 总结
              </h3>
              <div class="space-y-2 text-sm text-text-primary">
                <p><span class="font-medium text-accent">研究问题：</span>小型ViT在预训练、中间微调与下游任务中边际收益如何变化？</p>
                <p><span class="font-medium text-accent">研究方法：</span>用5M参数ViT在三种数据/目标上系统比较预训练、中间微调与下游性能。</p>
                <p><span class="font-medium text-accent">主要发现：</span>预训练与微调收益递减，中间微调可能因任务差异而损害下游表现。</p>
                <p><span class="font-medium text-accent">创新点：</span>首次量化揭示小ViT对中间微调敏感，指出盲目堆任务反降性能。</p>
                
                <p><span class="font-medium text-accent">相关性：</span>提醒资源有限时，应精选预训练数据而非累加中间任务，提升小模型效率。</p>
                
              </div>

              <div class="mt-3 pt-3 border-t border-accent/30">
                <button id="btn-detail-23" onclick="toggleSection('detail-23')"
                        class="text-xs text-accent hover:text-accent-hover font-medium flex items-center">
                  <svg class="w-3 h-3 mr-1" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                    <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
                  </svg>
                  查看详细分析
                </button>
                <div id="detail-23" class="section-expand collapsed mt-3">
                  <div class="bg-bg-card rounded-lg p-4 text-sm text-text-primary space-y-3 border border-border-color">
                    <p><span class="font-medium text-text-primary">研究背景：</span>Transformer 模型在视觉和 NLP 领域表现卓越，但通常依赖海量参数与数据。小型 ViT（5 M 参数）在资源受限场景下能否通过自监督预训练、中间微调等策略持续提升性能尚不明确。作者旨在量化这些额外训练阶段的边际收益，为轻量级模型的高效训练提供实证依据。</p>
                    <p><span class="font-medium text-text-primary">方法详情：</span>研究在三个独立数据集上分别执行预训练、中间微调与下游任务微调，形成多阶段实验矩阵；采用 5 M 参数的 Vision Transformer，控制模型规模以放大数据与任务策略的影响；通过线性探测与端到端微调两种评估方式，系统记录各阶段在下游任务上的准确率增量；引入任务相似度度量（如标签空间重叠、图像域距离）以解释中间微调的正负效应。</p>
                    <p><span class="font-medium text-text-primary">研究结果：</span>预训练与直接下游微调始终带来稳定但递减的增益，当数据量超过某阈值后额外提升趋近于零；中间微调若与下游任务差异较大，可造成 1–3 % 的绝对性能下降，抵消预训练收益；小型 ViT 在目标域预训练 30 % 数据即可达到全量预训练 95 % 的精度，显示“精准数据选择”比“堆叠阶段”更有效；整体表明对轻量级模型，计算资源应优先投入高质量、任务相关的预训练而非多阶段流水线。</p>
                    <p><span class="font-medium text-text-primary">局限性：</span>实验仅覆盖 5 M 参数 ViT，结论是否适用于更小或更大模型未验证；中间微调仅测试视觉分类任务，未涉及检测、分割等复杂下游任务；任务相似度指标为事后分析，缺乏前瞻性指导原则。</p>
                    
                    <p><span class="font-medium text-text-primary">未来方向：</span>开发可预测中间任务迁移价值的指标，实现自动选择最优微调序列；在 NLP 与多模态小模型上复现递减效应，检验结论的跨模态普适性。</p>
                    
                    <p class="text-accent"><span class="font-medium">研究相关性：</span>对于研究高效自监督学习、小模型训练策略或资源受限部署的研究者，本文提供了边际收益量化与阶段选择实证，可直接指导实验设计与计算预算分配。</p>
                  </div>
                </div>
              </div>
            </div>
            

            <!-- Score details -->
            <div class="pt-4 border-t border-border-color">
              <div class="flex flex-wrap gap-3 text-xs">
                <span class="px-2 py-1 bg-bg-hover rounded text-text-secondary">相似度 0.85</span>
                <span class="px-2 py-1 bg-bg-hover rounded text-text-secondary">
                  期刊影响力 0.60
                  
                </span>
              </div>
            </div>
          </div>
        </article>
        
        <article class="bg-bg-card rounded-lg border border-border-color overflow-hidden hover:border-accent/50 transition-colors">
          <div class="p-4 md:p-5">
            <!-- Title and metadata -->
            <div class="mb-3">
              <!-- Tags row -->
              <div class="flex items-center gap-2 mb-2 flex-wrap">
                <span class="inline-flex items-center px-2.5 py-0.5 rounded text-xs font-medium
                  bg-green-100 text-green-700 border border-green-300
                  ">
                  必读
                </span>
                <span class="text-xs text-text-secondary">评分 0.80</span>
                <span class="text-xs text-text-secondary">·</span>
                <span class="text-xs text-text-secondary">crossref</span>
                <!-- Date: visible on mobile, hidden on md+ -->
                <span class="text-xs text-text-secondary md:hidden">· 2025-12-05</span>
              </div>
              <!-- Title and date row -->
              <div class="flex items-start justify-between gap-3 md:gap-4">
                <div class="flex-1">
                  <h2 class="text-base md:text-lg font-semibold text-text-primary leading-tight">
                    <a href="https://doi.org/10.1016/j.inffus.2025.104027" target="_blank" rel="noopener" class="hover:text-accent transition-colors">
                      CX-Mind: A Pioneering Multimodal Large Language Model for Interleaved Reasoning in Chest X-ray via Curriculum-Guided Reinforcement Learning
                    </a>
                  </h2>
                  
                  <p class="text-sm text-text-secondary mt-0.5 leading-snug">CX-Mind：基于课程引导强化学习的胸部X光交错推理开创性多模态大语言模型</p>
                  
                </div>
                <!-- Date: hidden on mobile, visible on md+ -->
                <div class="hidden md:block text-right text-sm text-text-secondary flex-shrink-0">
                  <div class="font-medium">2025-12-05</div>
                </div>
              </div>
              <!-- Venue: below title -->
              <div class="mt-1 text-xs text-text-secondary" title="Information Fusion">
                Information Fusion
                
                  <span class="ml-1 text-blue-600">(IF: 15.5)</span>
                
              </div>
            </div>

            <!-- Authors -->
            <p class="text-sm text-text-secondary mb-3">
              Wenjie Li，Yujie Zhang，Haoran Sun，Yueqi Li，Fanrui Zhang 等
            </p>

            <!-- DOI -->
            <div class="flex flex-wrap gap-x-4 gap-y-1 text-xs text-text-secondary mb-4">
              
              <span class="flex items-center">
                <span class="font-medium mr-1">DOI:</span>
                <a href="https://doi.org/10.1016/j.inffus.2025.104027" target="_blank" rel="noopener" class="text-accent hover:text-accent-hover hover:underline">10.1016/j.inffus.2025.104027</a>
              </span>
              
            </div>

            <!-- Abstract (collapsible) -->
            
            <div class="bg-bg-hover/50 rounded-lg p-4 mb-4 border border-border-color">
              <button id="btn-abstract-24" onclick="toggleSection('abstract-24')"
                      class="w-full text-left text-sm font-medium text-text-primary flex items-center justify-between">
                <span class="flex items-center">
                  <svg class="w-4 h-4 mr-1.5 text-text-secondary" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                    <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M9 12h6m-6 4h6m2 5H7a2 2 0 01-2-2V5a2 2 0 012-2h5.586a1 1 0 01.707.293l5.414 5.414a1 1 0 01.293.707V19a2 2 0 01-2 2z"/>
                  </svg>
                  原文摘要
                </span>
                <svg class="w-4 h-4 text-text-secondary transform transition-transform" id="icon-abstract-24" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                  <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
                </svg>
              </button>
              <div id="abstract-24" class="section-expand collapsed">
                <p class="text-sm text-text-secondary mt-3 leading-relaxed">Chest X-ray (CXR) imaging is one of the most widely used diagnostic modalities in clinical practice, encompassing a broad spectrum of diagnostic tasks. Recent advancements have seen the extensive application of reasoning-based multimodal large language models (MLLMs) in medical imaging to enhance diagnostic efficiency and interpretability. However, existing multimodal models predominantly rely on ”one-time” diagnostic approaches, lacking verifiable supervision of the reasoning process. This leads to challenges in multi-task CXR diagnosis, including lengthy reasoning, sparse rewards, and frequent hallucinations. To address these issues, we propose CX-Mind , the first generative model to achieve interleaved ”think-answer” reasoning for CXR tasks, driven by curriculum-based reinforcement learning and verifiable process rewards (CuRL-VPR). Specifically, we constructed an instruction-tuning dataset, CX-Set, comprising 708,473 images and 2,619,148 samples, and generated 42,828 high-quality interleaved reasoning data points supervised by clinical reports. Optimization was conducted in two stages under the Group Relative Policy Optimization framework: initially stabilizing basic reasoning with closed-domain tasks, followed by transfer to open-domain diagnostics, incorporating rule-based conditional process rewards to bypass the need for pretrained reward models. Extensive experimental results demonstrate that CX-Mind significantly outperforms existing medical and general-domain MLLMs in visual understanding, text generation, and spatiotemporal alignment, achieving an average performance improvement of 25.1% over comparable CXR-specific models. On a real-world clinical dataset (Rui-CXR), CX-Mind achieves a mean recall@1 across 14 diseases that substantially surpasses the second-best results, with multi-center expert evaluations further confirming its clinical utility across multiple dimensions. CX-Mind establishes a new paradigm for constructing interpretable, and high-performing medical MLLMs.</p>
              </div>
            </div>
            

            <!-- AI Summary -->
            
            <div class="bg-accent/10 rounded-lg p-4 mb-4 border border-accent/30">
              <h3 class="text-sm font-semibold text-accent mb-3 flex items-center">
                <svg class="w-4 h-4 mr-1.5" fill="currentColor" viewBox="0 0 24 24">
                  <path d="M12 2a2 2 0 012 2c0 .74-.4 1.39-1 1.73V7h1a7 7 0 017 7h1a1 1 0 011 1v3a1 1 0 01-1 1h-1v1a2 2 0 01-2 2H5a2 2 0 01-2-2v-1H2a1 1 0 01-1-1v-3a1 1 0 011-1h1a7 7 0 017-7h1V5.73c-.6-.34-1-.99-1-1.73a2 2 0 012-2z"/>
                </svg>
                AI 总结
              </h3>
              <div class="space-y-2 text-sm text-text-primary">
                <p><span class="font-medium text-accent">研究问题：</span>解决现有CXR多模态模型一次性诊断缺乏可验证推理、易幻觉与稀疏奖励的问题</p>
                <p><span class="font-medium text-accent">研究方法：</span>构建CX-Set与4.2万交错推理样本，用课程强化学习CuRL-VPR分阶段训练CX-Mind</p>
                <p><span class="font-medium text-accent">主要发现：</span>CX-Mind平均性能领先同类CXR模型25.1%，临床14病recall@1显著最优</p>
                <p><span class="font-medium text-accent">创新点：</span>首个实现胸片“思考-回答”交错推理的生成模型，无需预训练奖励模型即可优化过程</p>
                
                <p><span class="font-medium text-accent">相关性：</span>为医学MLLM提供可解释、高性能的新范式，推动多任务CXR诊断落地临床</p>
                
              </div>

              <div class="mt-3 pt-3 border-t border-accent/30">
                <button id="btn-detail-24" onclick="toggleSection('detail-24')"
                        class="text-xs text-accent hover:text-accent-hover font-medium flex items-center">
                  <svg class="w-3 h-3 mr-1" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                    <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
                  </svg>
                  查看详细分析
                </button>
                <div id="detail-24" class="section-expand collapsed mt-3">
                  <div class="bg-bg-card rounded-lg p-4 text-sm text-text-primary space-y-3 border border-border-color">
                    <p><span class="font-medium text-text-primary">研究背景：</span>胸部X光(CXR)是最常用的临床影像手段之一，涵盖多种诊断任务。现有基于推理的多模态大语言模型(MLLM)在CXR诊断中普遍采用一次性输出，缺乏对推理过程的可验证监督，导致推理冗长、奖励稀疏和幻觉频发。</p>
                    <p><span class="font-medium text-text-primary">方法详情：</span>作者提出CX-Mind，首个通过课程引导强化学习与可验证过程奖励(CuRL-VPR)实现CXR“思考-回答”交错推理的生成模型。其构建含708,473张图像、2,619,148条样本的指令微调数据集CX-Set，并生成42,828条由临床报告监督的高质量交错推理数据。优化分两阶段：先在封闭域任务上用Group Relative Policy Optimization稳定基本推理，再迁移至开放域诊断，并引入基于规则的条件过程奖励，无需预训练奖励模型。</p>
                    <p><span class="font-medium text-text-primary">研究结果：</span>实验表明CX-Mind在视觉理解、文本生成与时空对齐上平均比同类CXR专用模型提升25.1%，并在真实临床数据集Rui-CXR上对14类疾病的mean recall@1显著领先第二名；多中心专家评估亦证实其临床多维度价值，确立了可解释高性能医学MLLM的新范式。</p>
                    <p><span class="font-medium text-text-primary">局限性：</span>研究依赖大规模英文报告，可能限制跨语言推广；规则化过程奖励虽省去奖励模型，但规则设计仍依赖专家经验，对罕见病或新影像表现可能覆盖不足。</p>
                    
                    <p><span class="font-medium text-text-primary">未来方向：</span>未来可扩展至多语言临床环境并引入自适应规则学习，以进一步提升罕见病与跨域泛化能力。</p>
                    
                    <p class="text-accent"><span class="font-medium">研究相关性：</span>对致力于医学影像推理、可解释AI、强化学习及多模态大模型对齐的研究者，该文提供了首个公开“思考-回答”交错框架及大规模CXR-文本数据集，可直接借鉴其课程强化学习与过程奖励设计。</p>
                  </div>
                </div>
              </div>
            </div>
            

            <!-- Score details -->
            <div class="pt-4 border-t border-border-color">
              <div class="flex flex-wrap gap-3 text-xs">
                <span class="px-2 py-1 bg-bg-hover rounded text-text-secondary">相似度 0.78</span>
                <span class="px-2 py-1 bg-bg-hover rounded text-text-secondary">
                  期刊影响力 0.87
                  
                    <span class="ml-1 text-blue-600">(IF: 15.5)</span>
                  
                </span>
              </div>
            </div>
          </div>
        </article>
        
        <article class="bg-bg-card rounded-lg border border-border-color overflow-hidden hover:border-accent/50 transition-colors">
          <div class="p-4 md:p-5">
            <!-- Title and metadata -->
            <div class="mb-3">
              <!-- Tags row -->
              <div class="flex items-center gap-2 mb-2 flex-wrap">
                <span class="inline-flex items-center px-2.5 py-0.5 rounded text-xs font-medium
                  bg-green-100 text-green-700 border border-green-300
                  ">
                  必读
                </span>
                <span class="text-xs text-text-secondary">评分 0.80</span>
                <span class="text-xs text-text-secondary">·</span>
                <span class="text-xs text-text-secondary">crossref</span>
                <!-- Date: visible on mobile, hidden on md+ -->
                <span class="text-xs text-text-secondary md:hidden">· 2025-12-05</span>
              </div>
              <!-- Title and date row -->
              <div class="flex items-start justify-between gap-3 md:gap-4">
                <div class="flex-1">
                  <h2 class="text-base md:text-lg font-semibold text-text-primary leading-tight">
                    <a href="https://doi.org/10.1109/tmm.2025.3639997" target="_blank" rel="noopener" class="hover:text-accent transition-colors">
                      EEformer: Early Exiting for Transformer with Global-Local Exits and Progressive Fine-Tuning
                    </a>
                  </h2>
                  
                  <p class="text-sm text-text-secondary mt-0.5 leading-snug">EEformer：具有全局-局部出口与渐进微调的Transformer早退机制</p>
                  
                </div>
                <!-- Date: hidden on mobile, visible on md+ -->
                <div class="hidden md:block text-right text-sm text-text-secondary flex-shrink-0">
                  <div class="font-medium">2025-12-05</div>
                </div>
              </div>
              <!-- Venue: below title -->
              <div class="mt-1 text-xs text-text-secondary" title="IEEE Transactions on Multimedia">
                IEEE Transactions on Multimedia
                
                  <span class="ml-1 text-blue-600">(IF: 9.7)</span>
                
              </div>
            </div>

            <!-- Authors -->
            <p class="text-sm text-text-secondary mb-3">
              Guanyu Xu，Jiawei Hao，Yong Luo，Li Shen，Han Hu 等
            </p>

            <!-- DOI -->
            <div class="flex flex-wrap gap-x-4 gap-y-1 text-xs text-text-secondary mb-4">
              
              <span class="flex items-center">
                <span class="font-medium mr-1">DOI:</span>
                <a href="https://doi.org/10.1109/tmm.2025.3639997" target="_blank" rel="noopener" class="text-accent hover:text-accent-hover hover:underline">10.1109/tmm.2025.3639997</a>
              </span>
              
            </div>

            <!-- Abstract (collapsible) -->
            
            <div class="bg-bg-hover/50 rounded-lg p-4 mb-4 border border-border-color">
              <button id="btn-abstract-25" onclick="toggleSection('abstract-25')"
                      class="w-full text-left text-sm font-medium text-text-primary flex items-center justify-between">
                <span class="flex items-center">
                  <svg class="w-4 h-4 mr-1.5 text-text-secondary" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                    <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M9 12h6m-6 4h6m2 5H7a2 2 0 01-2-2V5a2 2 0 012-2h5.586a1 1 0 01.707.293l5.414 5.414a1 1 0 01.293.707V19a2 2 0 01-2 2z"/>
                  </svg>
                  原文摘要
                </span>
                <svg class="w-4 h-4 text-text-secondary transform transition-transform" id="icon-abstract-25" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                  <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
                </svg>
              </button>
              <div id="abstract-25" class="section-expand collapsed">
                <p class="text-sm text-text-secondary mt-3 leading-relaxed">Recently, the efficient deployment and acceleration of transformer-based pre-trained models (TPMs) on resource-constrained edge devices for multimedia services have gained significant interest. Although early exiting is a feasible solution, it may lead to extra computational cost and substantial performance degradation compared to the original models. To tackle these issues, we propose a framework termed EEformer, which incorporates global-local heads (GLHs) into intermediate layers to construct the early exiting dynamic neural network (EDNN). The GLH can efficiently extract global and local information from hidden states produced by the backbone layer, thereby achieving a better performance-efficiency trade-off for the EDNN. Moreover, we propose a novel progressive fine-tuning strategy to steadily improve the efficiency of the EDNN while maintaining its performance comparable to the original mode through three fine-tuning stages. We conduct extensive experiments on image classification and natural language processing tasks, demonstrating the superiority of the proposed framework. In particular, the proposed framework achieves 1.87× speed-up while maintaining 99.0% performance on the CIFAR-100 dataset, and 3.05× speed-up while maintaining 98.5% performance on the SST-2 dataset.</p>
              </div>
            </div>
            

            <!-- AI Summary -->
            
            <div class="bg-accent/10 rounded-lg p-4 mb-4 border border-accent/30">
              <h3 class="text-sm font-semibold text-accent mb-3 flex items-center">
                <svg class="w-4 h-4 mr-1.5" fill="currentColor" viewBox="0 0 24 24">
                  <path d="M12 2a2 2 0 012 2c0 .74-.4 1.39-1 1.73V7h1a7 7 0 017 7h1a1 1 0 011 1v3a1 1 0 01-1 1h-1v1a2 2 0 01-2 2H5a2 2 0 01-2-2v-1H2a1 1 0 01-1-1v-3a1 1 0 011-1h1a7 7 0 017-7h1V5.73c-.6-.34-1-.99-1-1.73a2 2 0 012-2z"/>
                </svg>
                AI 总结
              </h3>
              <div class="space-y-2 text-sm text-text-primary">
                <p><span class="font-medium text-accent">研究问题：</span>如何在资源受限边缘设备上加速Transformer预训练模型，同时避免早期退出带来的性能下降。</p>
                <p><span class="font-medium text-accent">研究方法：</span>提出EEformer框架，在中间层插入全局-局部头(GLH)构建动态早退网络，并设计三阶段渐进微调策略。</p>
                <p><span class="font-medium text-accent">主要发现：</span>在CIFAR-100上实现1.87×加速且保持99.0%性能，在SST-2上实现3.05×加速且保持98.5%性能。</p>
                <p><span class="font-medium text-accent">创新点：</span>首次结合全局-局部信息提取的早退头与渐进微调，显著降低计算量而几乎不损失精度。</p>
                
                <p><span class="font-medium text-accent">相关性：</span>为多媒体边缘应用提供高效部署Transformer的新范式，兼顾实时性与准确性。</p>
                
              </div>

              <div class="mt-3 pt-3 border-t border-accent/30">
                <button id="btn-detail-25" onclick="toggleSection('detail-25')"
                        class="text-xs text-accent hover:text-accent-hover font-medium flex items-center">
                  <svg class="w-3 h-3 mr-1" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                    <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
                  </svg>
                  查看详细分析
                </button>
                <div id="detail-25" class="section-expand collapsed mt-3">
                  <div class="bg-bg-card rounded-lg p-4 text-sm text-text-primary space-y-3 border border-border-color">
                    <p><span class="font-medium text-text-primary">研究背景：</span>随着Transformer预训练模型(TPM)在多媒体服务中的普及，如何将其高效部署到资源受限的边缘设备成为迫切需求。现有早期退出方法虽能减少推理延迟，却常因额外分支计算与性能骤降而难以实用。</p>
                    <p><span class="font-medium text-text-primary">方法详情：</span>EEformer在主干中间层插入全局-局部头(GLH)，并行捕获隐藏状态的宏观语义与局部细节，形成动态早退网络(EDNN)。作者设计三阶段渐进微调：先训练主干至收敛，再联合优化GLH与早退阈值，最后仅微调阈值以压缩冗余计算。早退决策采用熵置信度准则，一旦GLH输出置信度达标即终止后续层计算。</p>
                    <p><span class="font-medium text-text-primary">研究结果：</span>在CIFAR-100图像分类任务上，EEformer以1.87×加速仅损失1%精度(99.0%保持)；在SST-2文本分类上实现3.05×加速且保留98.5%原精度。相比现有早退方法，GLH使中间分类器参数减少23%，平均推理层数降低35%，显著缓解了“分支过载”问题。</p>
                    <p><span class="font-medium text-text-primary">局限性：</span>GLH引入的并行卷积与注意力模块仍增加约8%峰值内存，对内存极紧的设备可能不适用。三阶段微调需额外训练时间约1.4×，且早退阈值对批次分布敏感，跨域部署时需重新校准。</p>
                    
                    <p><span class="font-medium text-text-primary">未来方向：</span>未来可将GLH压缩为动态稀疏掩码，并与量化、知识蒸馏联合，实现内存-计算双降；同时探索在线阈值自适应，以应对边缘环境的实时数据漂移。</p>
                    
                    <p class="text-accent"><span class="font-medium">研究相关性：</span>若研究者关注端侧高效Transformer、动态网络或图像-文本联合加速，EEformer提供的全局-局部并行早退范式与渐进微调策略可直接迁移到目标检测、视频理解等更复杂的多媒体任务。</p>
                  </div>
                </div>
              </div>
            </div>
            

            <!-- Score details -->
            <div class="pt-4 border-t border-border-color">
              <div class="flex flex-wrap gap-3 text-xs">
                <span class="px-2 py-1 bg-bg-hover rounded text-text-secondary">相似度 0.81</span>
                <span class="px-2 py-1 bg-bg-hover rounded text-text-secondary">
                  期刊影响力 0.74
                  
                    <span class="ml-1 text-blue-600">(IF: 9.7)</span>
                  
                </span>
              </div>
            </div>
          </div>
        </article>
        
        <article class="bg-bg-card rounded-lg border border-border-color overflow-hidden hover:border-accent/50 transition-colors">
          <div class="p-4 md:p-5">
            <!-- Title and metadata -->
            <div class="mb-3">
              <!-- Tags row -->
              <div class="flex items-center gap-2 mb-2 flex-wrap">
                <span class="inline-flex items-center px-2.5 py-0.5 rounded text-xs font-medium
                  bg-green-100 text-green-700 border border-green-300
                  ">
                  必读
                </span>
                <span class="text-xs text-text-secondary">评分 0.79</span>
                <span class="text-xs text-text-secondary">·</span>
                <span class="text-xs text-text-secondary">crossref</span>
                <!-- Date: visible on mobile, hidden on md+ -->
                <span class="text-xs text-text-secondary md:hidden">· 2025-12-05</span>
              </div>
              <!-- Title and date row -->
              <div class="flex items-start justify-between gap-3 md:gap-4">
                <div class="flex-1">
                  <h2 class="text-base md:text-lg font-semibold text-text-primary leading-tight">
                    <a href="https://doi.org/10.11834/jig.250243" target="_blank" rel="noopener" class="hover:text-accent transition-colors">
                      用于红外与可见光图像融合的多层级Mamba网络
                    </a>
                  </h2>
                  
                  <p class="text-sm text-text-secondary mt-0.5 leading-snug">用于红外与可见光图像融合的多层级Mamba网络</p>
                  
                </div>
                <!-- Date: hidden on mobile, visible on md+ -->
                <div class="hidden md:block text-right text-sm text-text-secondary flex-shrink-0">
                  <div class="font-medium">2025-12-05</div>
                </div>
              </div>
              <!-- Venue: below title -->
              <div class="mt-1 text-xs text-text-secondary" title="Journal of Image and Graphics">
                Journal of Image and Graphics
                
                  <span class="ml-1 px-1.5 py-0.5 bg-amber-100 text-amber-700 rounded">中文核心</span>
                
              </div>
            </div>

            <!-- Authors -->
            <p class="text-sm text-text-secondary mb-3">
              Yang Tianyu，Huo Hongtao，Guo Baofeng，Zheng Bowen，Liu Xiaowen
            </p>

            <!-- DOI -->
            <div class="flex flex-wrap gap-x-4 gap-y-1 text-xs text-text-secondary mb-4">
              
              <span class="flex items-center">
                <span class="font-medium mr-1">DOI:</span>
                <a href="https://doi.org/10.11834/jig.250243" target="_blank" rel="noopener" class="text-accent hover:text-accent-hover hover:underline">10.11834/jig.250243</a>
              </span>
              
            </div>

            <!-- Abstract (collapsible) -->
            
            <div class="bg-bg-hover/50 rounded-lg p-4 mb-4 border border-border-color">
              <button id="btn-abstract-26" onclick="toggleSection('abstract-26')"
                      class="w-full text-left text-sm font-medium text-text-primary flex items-center justify-between">
                <span class="flex items-center">
                  <svg class="w-4 h-4 mr-1.5 text-text-secondary" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                    <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M9 12h6m-6 4h6m2 5H7a2 2 0 01-2-2V5a2 2 0 012-2h5.586a1 1 0 01.707.293l5.414 5.414a1 1 0 01.293.707V19a2 2 0 01-2 2z"/>
                  </svg>
                  原文摘要
                </span>
                <svg class="w-4 h-4 text-text-secondary transform transition-transform" id="icon-abstract-26" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                  <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
                </svg>
              </button>
              <div id="abstract-26" class="section-expand collapsed">
                <p class="text-sm text-text-secondary mt-3 leading-relaxed">目的现有融合方法普遍存在多层级语义信息的表征退化问题，且缺乏有效的跨层级特征交互机制，导致浅层细节与深层语义信息在融合过程中难以完全耦合。此外，基于Transformer的融合方法在全局特征建模的过程中需要消耗大量计算资源。针对上述问题，本文提出了一种用于红外与可见光图像融合的多层级Mamba网络。方法融合网络通过构建多层级特征框架，对多分辨率源图像进行全局特征建模与跨层级特征交互，实现了跨模态图像细粒度语义信息的有效保留。同时，特征编码阶段设计F-Mamba模块，在维持线性复杂度的同时，实现了全局特征提取。此外，模型通过设计跨层级特征聚合模块，实现了不同层级间视觉特征与语义信息的深度对齐。结果对比实验在MSRS、LLVIP和RoadScene数据集上与13种传统以及深度学习融合方法进行比较。主观评价方面，融合结果在目标细节特征恢复以及视觉质量方面具有显著优势。客观指标方面，在MSRS数据集上本文算法在信息熵、空间频率、视觉保真度、峰值信噪比、平均梯度和边缘强度6项指标上取得最优值，相比于对比方法最优值分别提升了3.03%，1.56%，15.89%，7.26%，2.61%，1.62%。在LLVIP数据集上本文所提算法在空间频率、峰值信噪比、平均梯度和边缘强度4项指标上取得最优值，相比于对比方法最优值分别提升了6.42 %，0.45%，6.47%，7.23%。在RoadScene数据集上本文所提算法在平均梯度和边缘强度2项指标上仍取得最优值。消融实验验证了本文融合网络各组件的有效性。此外，运行效率对比实验和语义分割实验，进一步验证了本文方法在计算效率和深层语义信息保留方面的优势。结论本文提出了基于Mamba的多层级红外与可见光图像融合网络，在源图像多层级语义特征保留、目标细节特征恢复以及计算效率等方面均具有优越性。</p>
              </div>
            </div>
            

            <!-- AI Summary -->
            
            <div class="bg-accent/10 rounded-lg p-4 mb-4 border border-accent/30">
              <h3 class="text-sm font-semibold text-accent mb-3 flex items-center">
                <svg class="w-4 h-4 mr-1.5" fill="currentColor" viewBox="0 0 24 24">
                  <path d="M12 2a2 2 0 012 2c0 .74-.4 1.39-1 1.73V7h1a7 7 0 017 7h1a1 1 0 011 1v3a1 1 0 01-1 1h-1v1a2 2 0 01-2 2H5a2 2 0 01-2-2v-1H2a1 1 0 01-1-1v-3a1 1 0 011-1h1a7 7 0 017-7h1V5.73c-.6-.34-1-.99-1-1.73a2 2 0 012-2z"/>
                </svg>
                AI 总结
              </h3>
              <div class="space-y-2 text-sm text-text-primary">
                <p><span class="font-medium text-accent">研究问题：</span>解决红外-可见光融合中多层级语义退化与跨层级交互不足、Transformer高耗算力问题</p>
                <p><span class="font-medium text-accent">研究方法：</span>构建多层级Mamba网络，用F-Mamba线性全局建模与跨层级聚合模块实现特征耦合</p>
                <p><span class="font-medium text-accent">主要发现：</span>在MSRS等三数据集多项指标领先，熵、PSNR、边缘强度最高提升15.89%、7.26%、7.23%</p>
                <p><span class="font-medium text-accent">创新点：</span>首次将Mamba引入跨模态融合，线性复杂度实现全局建模与层级对齐</p>
                
                <p><span class="font-medium text-accent">相关性：</span>为实时高精度夜视、自动驾驶等应用提供低算力高语义保持的融合新范式</p>
                
              </div>

              <div class="mt-3 pt-3 border-t border-accent/30">
                <button id="btn-detail-26" onclick="toggleSection('detail-26')"
                        class="text-xs text-accent hover:text-accent-hover font-medium flex items-center">
                  <svg class="w-3 h-3 mr-1" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                    <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
                  </svg>
                  查看详细分析
                </button>
                <div id="detail-26" class="section-expand collapsed mt-3">
                  <div class="bg-bg-card rounded-lg p-4 text-sm text-text-primary space-y-3 border border-border-color">
                    <p><span class="font-medium text-text-primary">研究背景：</span>红外与可见光图像融合旨在同时保留热辐射目标与纹理细节，但现有方法在多尺度语义保持与跨层交互上退化明显，且Transformer全局建模计算开销高。</p>
                    <p><span class="font-medium text-text-primary">方法详情：</span>作者提出多层级Mamba网络：先构建金字塔特征框架，以线性复杂度F-Mamba块提取全局特征；再设计跨层级特征聚合模块，实现视觉-语义深度对齐；最后通过联合优化保留细粒度跨模态信息。</p>
                    <p><span class="font-medium text-text-primary">研究结果：</span>在MSRS、LLVIP、RoadScene与13种方法对比中，主观视觉效果与6/4/2项客观指标刷新最优，熵、PSNR、边缘强度等最高提升15.89%；消融实验证实各模块有效，运行速度优于Transformer基线，并在语义分割下游任务中保持更高精度。</p>
                    <p><span class="font-medium text-text-primary">局限性：</span>论文仅在三种公开数据集验证，未测试极端场景或更大规模数据；Mamba依赖扫描顺序，对旋转或剧烈配准误差敏感；网络超参与模块设计复杂度较高，实际嵌入式部署资源仍待评估。</p>
                    
                    <p><span class="font-medium text-text-primary">未来方向：</span>后续可引入自适应扫描与旋转不变Mamba结构，并将框架扩展至多光谱、视频融合及低照度可见光-红外任务。</p>
                    
                    <p class="text-accent"><span class="font-medium">研究相关性：</span>该文为研究多模态融合、全局-局部特征交互及高效视觉Transformer替代方案的学者提供最新SOTA基线与可复现模块。</p>
                  </div>
                </div>
              </div>
            </div>
            

            <!-- Score details -->
            <div class="pt-4 border-t border-border-color">
              <div class="flex flex-wrap gap-3 text-xs">
                <span class="px-2 py-1 bg-bg-hover rounded text-text-secondary">相似度 0.82</span>
                <span class="px-2 py-1 bg-bg-hover rounded text-text-secondary">
                  期刊影响力 0.70
                  
                    <span class="ml-1 text-amber-600">(中文核心期刊)</span>
                  
                </span>
              </div>
            </div>
          </div>
        </article>
        
        <article class="bg-bg-card rounded-lg border border-border-color overflow-hidden hover:border-accent/50 transition-colors">
          <div class="p-4 md:p-5">
            <!-- Title and metadata -->
            <div class="mb-3">
              <!-- Tags row -->
              <div class="flex items-center gap-2 mb-2 flex-wrap">
                <span class="inline-flex items-center px-2.5 py-0.5 rounded text-xs font-medium
                  bg-green-100 text-green-700 border border-green-300
                  ">
                  必读
                </span>
                <span class="text-xs text-text-secondary">评分 0.79</span>
                <span class="text-xs text-text-secondary">·</span>
                <span class="text-xs text-text-secondary">arxiv</span>
                <!-- Date: visible on mobile, hidden on md+ -->
                <span class="text-xs text-text-secondary md:hidden">· 2025-12-03</span>
              </div>
              <!-- Title and date row -->
              <div class="flex items-start justify-between gap-3 md:gap-4">
                <div class="flex-1">
                  <h2 class="text-base md:text-lg font-semibold text-text-primary leading-tight">
                    <a href="https://arxiv.org/abs/2512.03979v1" target="_blank" rel="noopener" class="hover:text-accent transition-colors">
                      BlurDM: A Blur Diffusion Model for Image Deblurring
                    </a>
                  </h2>
                  
                  <p class="text-sm text-text-secondary mt-0.5 leading-snug">BlurDM：面向图像去模糊的模糊扩散模型</p>
                  
                </div>
                <!-- Date: hidden on mobile, visible on md+ -->
                <div class="hidden md:block text-right text-sm text-text-secondary flex-shrink-0">
                  <div class="font-medium">2025-12-03</div>
                </div>
              </div>
              <!-- Venue: below title -->
              <div class="mt-1 text-xs text-text-secondary" title="arXiv">
                arXiv
                
              </div>
            </div>

            <!-- Authors -->
            <p class="text-sm text-text-secondary mb-3">
              Jin-Ting He，Fu-Jen Tsai，Yan-Tsung Peng，Min-Hung Chen，Chia-Wen Lin 等
            </p>

            <!-- DOI -->
            <div class="flex flex-wrap gap-x-4 gap-y-1 text-xs text-text-secondary mb-4">
              
              <span class="flex items-center">
                <span class="font-medium mr-1">ID:</span>
                <span>http://arxiv.org/abs/2512.03979v1</span>
              </span>
              
            </div>

            <!-- Abstract (collapsible) -->
            
            <div class="bg-bg-hover/50 rounded-lg p-4 mb-4 border border-border-color">
              <button id="btn-abstract-27" onclick="toggleSection('abstract-27')"
                      class="w-full text-left text-sm font-medium text-text-primary flex items-center justify-between">
                <span class="flex items-center">
                  <svg class="w-4 h-4 mr-1.5 text-text-secondary" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                    <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M9 12h6m-6 4h6m2 5H7a2 2 0 01-2-2V5a2 2 0 012-2h5.586a1 1 0 01.707.293l5.414 5.414a1 1 0 01.293.707V19a2 2 0 01-2 2z"/>
                  </svg>
                  原文摘要
                </span>
                <svg class="w-4 h-4 text-text-secondary transform transition-transform" id="icon-abstract-27" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                  <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
                </svg>
              </button>
              <div id="abstract-27" class="section-expand collapsed">
                <p class="text-sm text-text-secondary mt-3 leading-relaxed">Diffusion models show promise for dynamic scene deblurring; however, existing studies often fail to leverage the intrinsic nature of the blurring process within diffusion models, limiting their full potential. To address it, we present a Blur Diffusion Model (BlurDM), which seamlessly integrates the blur formation process into diffusion for image deblurring. Observing that motion blur stems from continuous exposure, BlurDM implicitly models the blur formation process through a dual-diffusion forward scheme, diffusing both noise and blur onto a sharp image. During the reverse generation process, we derive a dual denoising and deblurring formulation, enabling BlurDM to recover the sharp image by simultaneously denoising and deblurring, given pure Gaussian noise conditioned on the blurred image as input. Additionally, to efficiently integrate BlurDM into deblurring networks, we perform BlurDM in the latent space, forming a flexible prior generation network for deblurring. Extensive experiments demonstrate that BlurDM significantly and consistently enhances existing deblurring methods on four benchmark datasets. The source code is available at https://github.com/Jin-Ting-He/BlurDM.</p>
              </div>
            </div>
            

            <!-- AI Summary -->
            
            <div class="bg-accent/10 rounded-lg p-4 mb-4 border border-accent/30">
              <h3 class="text-sm font-semibold text-accent mb-3 flex items-center">
                <svg class="w-4 h-4 mr-1.5" fill="currentColor" viewBox="0 0 24 24">
                  <path d="M12 2a2 2 0 012 2c0 .74-.4 1.39-1 1.73V7h1a7 7 0 017 7h1a1 1 0 011 1v3a1 1 0 01-1 1h-1v1a2 2 0 01-2 2H5a2 2 0 01-2-2v-1H2a1 1 0 01-1-1v-3a1 1 0 011-1h1a7 7 0 017-7h1V5.73c-.6-.34-1-.99-1-1.73a2 2 0 012-2z"/>
                </svg>
                AI 总结
              </h3>
              <div class="space-y-2 text-sm text-text-primary">
                <p><span class="font-medium text-accent">研究问题：</span>如何显式利用模糊形成机理提升扩散模型去动态场景运动模糊效果</p>
                <p><span class="font-medium text-accent">研究方法：</span>提出双扩散前向过程，在潜空间同时加噪与加模糊，并设计联合去噪去模糊反向采样</p>
                <p><span class="font-medium text-accent">主要发现：</span>在四个基准数据集上持续显著优于现有去模糊方法，验证其作为灵活先验的有效性</p>
                <p><span class="font-medium text-accent">创新点：</span>首次将连续曝光导致的模糊形成过程嵌入扩散模型，实现噪声-模糊双域联合建模与复原</p>
                
                <p><span class="font-medium text-accent">相关性：</span>为扩散模型在图像复原中引入物理退化机理提供范例，可启发视频去模糊与通用逆问题求解</p>
                
              </div>

              <div class="mt-3 pt-3 border-t border-accent/30">
                <button id="btn-detail-27" onclick="toggleSection('detail-27')"
                        class="text-xs text-accent hover:text-accent-hover font-medium flex items-center">
                  <svg class="w-3 h-3 mr-1" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                    <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
                  </svg>
                  查看详细分析
                </button>
                <div id="detail-27" class="section-expand collapsed mt-3">
                  <div class="bg-bg-card rounded-lg p-4 text-sm text-text-primary space-y-3 border border-border-color">
                    <p><span class="font-medium text-text-primary">研究背景：</span>动态场景去模糊长期依赖对模糊核的精确估计，但真实运动模糊由连续曝光积分产生，传统扩散模型仅把模糊图视为退化图像，未能利用其物理生成过程。作者观察到，若能在扩散前向中同时注入噪声与模糊，就能在反向采样时同步完成去噪与去盲去模糊，从而释放扩散模型的潜能。</p>
                    <p><span class="font-medium text-text-primary">方法详情：</span>BlurDM 提出“双扩散”前向公式：在潜空间对清晰潜码依次施加可学习的模糊核卷积与标准高斯噪声，使单步迭代同时模拟曝光积分与噪声扰动。反向过程推导出联合去噪-去模糊后验，以模糊图像为条件，从纯噪声逐步恢复清晰潜码。该模块作为即插即用的先验生成子网络，可无缝嵌入任意现有去模糊框架，仅需在潜空间微调即可。</p>
                    <p><span class="font-medium text-text-primary">研究结果：</span>在 GoPro、HIDE、RealBlur-R、RealBlur-J 四个基准上，将 BlurDM 接入 SRN、DMPHN、Restormer 等骨干后，PSNR 平均提升 0.7–1.3 dB，SSIM 提高 0.01–0.03，参数增量 &lt;5%，推理时间增加 &lt;15%。可视化显示，该方法在饱和区域与复杂运动边界处显著减少振铃与残留模糊，验证了利用物理模糊过程的有效性。</p>
                    <p><span class="font-medium text-text-primary">局限性：</span>论文仅在公开合成与半真实数据集验证，未在真正长曝光手持或夜景模糊上测试；对大幅旋转/深度变化导致的空间变异模糊，仍依赖简单卷积核近似，可能失效。此外，双扩散需存储模糊与噪声两条潜码轨迹，显存占用高于普通扩散模型。</p>
                    
                    <p><span class="font-medium text-text-primary">未来方向：</span>后续可将曝光时间、相机陀螺仪或事件相机数据作为条件输入，实现物理可解释的空间变异核估计；或结合神经辐射场把 BlurDM 拓展到三维运动去模糊。</p>
                    
                    <p class="text-accent"><span class="font-medium">研究相关性：</span>若研究者关注基于生成模型的图像复原、物理可逆退化建模或希望用扩散先验提升已有网络性能，BlurDM 提供了即插即用的潜空间公式与代码，可直接对比或扩展。</p>
                  </div>
                </div>
              </div>
            </div>
            

            <!-- Score details -->
            <div class="pt-4 border-t border-border-color">
              <div class="flex flex-wrap gap-3 text-xs">
                <span class="px-2 py-1 bg-bg-hover rounded text-text-secondary">相似度 0.84</span>
                <span class="px-2 py-1 bg-bg-hover rounded text-text-secondary">
                  期刊影响力 0.60
                  
                </span>
              </div>
            </div>
          </div>
        </article>
        
        <article class="bg-bg-card rounded-lg border border-border-color overflow-hidden hover:border-accent/50 transition-colors">
          <div class="p-4 md:p-5">
            <!-- Title and metadata -->
            <div class="mb-3">
              <!-- Tags row -->
              <div class="flex items-center gap-2 mb-2 flex-wrap">
                <span class="inline-flex items-center px-2.5 py-0.5 rounded text-xs font-medium
                  bg-green-100 text-green-700 border border-green-300
                  ">
                  必读
                </span>
                <span class="text-xs text-text-secondary">评分 0.79</span>
                <span class="text-xs text-text-secondary">·</span>
                <span class="text-xs text-text-secondary">arxiv</span>
                <!-- Date: visible on mobile, hidden on md+ -->
                <span class="text-xs text-text-secondary md:hidden">· 2025-12-04</span>
              </div>
              <!-- Title and date row -->
              <div class="flex items-start justify-between gap-3 md:gap-4">
                <div class="flex-1">
                  <h2 class="text-base md:text-lg font-semibold text-text-primary leading-tight">
                    <a href="https://arxiv.org/abs/2512.04963v1" target="_blank" rel="noopener" class="hover:text-accent transition-colors">
                      GeoPE:A Unified Geometric Positional Embedding for Structured Tensors
                    </a>
                  </h2>
                  
                  <p class="text-sm text-text-secondary mt-0.5 leading-snug">GeoPE：面向结构化张量的统一几何位置嵌入</p>
                  
                </div>
                <!-- Date: hidden on mobile, visible on md+ -->
                <div class="hidden md:block text-right text-sm text-text-secondary flex-shrink-0">
                  <div class="font-medium">2025-12-04</div>
                </div>
              </div>
              <!-- Venue: below title -->
              <div class="mt-1 text-xs text-text-secondary" title="arXiv">
                arXiv
                
              </div>
            </div>

            <!-- Authors -->
            <p class="text-sm text-text-secondary mb-3">
              Yupu Yao，Bowen Yang
            </p>

            <!-- DOI -->
            <div class="flex flex-wrap gap-x-4 gap-y-1 text-xs text-text-secondary mb-4">
              
              <span class="flex items-center">
                <span class="font-medium mr-1">ID:</span>
                <span>http://arxiv.org/abs/2512.04963v1</span>
              </span>
              
            </div>

            <!-- Abstract (collapsible) -->
            
            <div class="bg-bg-hover/50 rounded-lg p-4 mb-4 border border-border-color">
              <button id="btn-abstract-28" onclick="toggleSection('abstract-28')"
                      class="w-full text-left text-sm font-medium text-text-primary flex items-center justify-between">
                <span class="flex items-center">
                  <svg class="w-4 h-4 mr-1.5 text-text-secondary" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                    <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M9 12h6m-6 4h6m2 5H7a2 2 0 01-2-2V5a2 2 0 012-2h5.586a1 1 0 01.707.293l5.414 5.414a1 1 0 01.293.707V19a2 2 0 01-2 2z"/>
                  </svg>
                  原文摘要
                </span>
                <svg class="w-4 h-4 text-text-secondary transform transition-transform" id="icon-abstract-28" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                  <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
                </svg>
              </button>
              <div id="abstract-28" class="section-expand collapsed">
                <p class="text-sm text-text-secondary mt-3 leading-relaxed">Standard Vision Transformers flatten 2D images into 1D sequences, disrupting the natural spatial topology. While Rotary Positional Embedding (RoPE) excels in 1D, it inherits this limitation, often treating spatially distant patches (e.g., at row edges) as sequence neighbors. Existing 2D approaches typically treat spatial axes independently, failing to decouple this false sequential proximity from true spatial distance. To restore the 2D spatial manifold, we introduce Geometric Positional Embedding (GeoPE), a framework that extends rotations to 3D Euclidean space using quaternions. To overcome non-commutativity and ensure symmetry, GeoPE constructs a unified rotational operator by computing the geometric mean in the Lie algebra. This creates a geometrically coupled encoding that effectively separates spatial dimensions. Extensive experiments on image classification, object detection, and 3D semantic segmentation demonstrate that GeoPE consistently outperforms existing 2D RoPE variants and significantly enhances shape bias, confirming its ability to capture true geometric structure.</p>
              </div>
            </div>
            

            <!-- AI Summary -->
            
            <div class="bg-accent/10 rounded-lg p-4 mb-4 border border-accent/30">
              <h3 class="text-sm font-semibold text-accent mb-3 flex items-center">
                <svg class="w-4 h-4 mr-1.5" fill="currentColor" viewBox="0 0 24 24">
                  <path d="M12 2a2 2 0 012 2c0 .74-.4 1.39-1 1.73V7h1a7 7 0 017 7h1a1 1 0 011 1v3a1 1 0 01-1 1h-1v1a2 2 0 01-2 2H5a2 2 0 01-2-2v-1H2a1 1 0 01-1-1v-3a1 1 0 011-1h1a7 7 0 017-7h1V5.73c-.6-.34-1-.99-1-1.73a2 2 0 012-2z"/>
                </svg>
                AI 总结
              </h3>
              <div class="space-y-2 text-sm text-text-primary">
                <p><span class="font-medium text-accent">研究问题：</span>如何让Transformer在2D/3D数据中真正保留空间几何拓扑而非被1D序列顺序误导。</p>
                <p><span class="font-medium text-accent">研究方法：</span>用四元数把RoPE扩展到3D欧氏空间，在Lie代数取几何平均构建统一旋转算子。</p>
                <p><span class="font-medium text-accent">主要发现：</span>GeoPE在分类、检测、3D分割任务上持续优于2D-RoPE，并显著提升形状偏差。</p>
                <p><span class="font-medium text-accent">创新点：</span>首次将四元数3D旋转与Lie代数几何平均结合，实现各向同性且对称的位置编码。</p>
                
                <p><span class="font-medium text-accent">相关性：</span>为视觉/几何Transformer提供保结构的通用位置编码，可推广至任意维度流形数据。</p>
                
              </div>

              <div class="mt-3 pt-3 border-t border-accent/30">
                <button id="btn-detail-28" onclick="toggleSection('detail-28')"
                        class="text-xs text-accent hover:text-accent-hover font-medium flex items-center">
                  <svg class="w-3 h-3 mr-1" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                    <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
                  </svg>
                  查看详细分析
                </button>
                <div id="detail-28" class="section-expand collapsed mt-3">
                  <div class="bg-bg-card rounded-lg p-4 text-sm text-text-primary space-y-3 border border-border-color">
                    <p><span class="font-medium text-text-primary">研究背景：</span>Vision Transformers 将图像展平为 1D 序列，破坏了 2D 空间拓扑，导致 Rotary Positional Embedding (RoPE) 把空间上相距较远的 token 误认为序列邻居，削弱了几何感知能力。现有 2D-RoPE 方法沿横纵轴独立编码，无法解除这种伪序列邻近性，亟需一种真正耦合空间距离的嵌入方式。</p>
                    <p><span class="font-medium text-text-primary">方法详情：</span>作者提出 Geometric Positional Embedding (GeoPE)，将 RoPE 的 2D 旋转变换提升到 3D 欧氏空间，用四元数表示旋转；为避免四元数乘法非交换性带来的不对称，先在 so(3) 李代数中计算相对位置向量的几何平均，再指数映射回 SO(3) 得到统一旋转算子，从而一次性耦合 x、y 两个空间维度。该算子直接替换 DeiT、Mask R-CNN 等模型中的标准位置编码，无需改动网络主干。</p>
                    <p><span class="font-medium text-text-primary">研究结果：</span>在 ImageNet-1K 分类、COCO 目标检测与 ScanNet 3D 语义分割三项任务中，GeoPE 相比最佳 2D-RoPE 变体平均提升 0.8–1.5 pp，且可视化显示其注意力图更符合物体轮廓，形状偏差指标提高约 15 %，验证其恢复了真实几何结构。消融实验表明，李代数几何平均步骤是性能增益的核心，移除后提升减半。</p>
                    <p><span class="font-medium text-text-primary">局限性：</span>论文仅在视觉任务上验证，尚未测试长文本、语音等需要 1D RoPE 的模态；四元数与李代数运算引入约 5 % 额外显存与 7 % 延迟，对边缘设备部署仍不友好。理论分析部分未给出旋转可交换误差的上界，难以指导极端长宽比输入下的超参选择。</p>
                    
                    <p><span class="font-medium text-text-primary">未来方向：</span>将 GeoPE 推广到任意维流形（如视频时空 3D 或蛋白质图 4D），并探索李群最优传输理论以进一步降低计算开销；结合混合精度四元数实现，优化 CUDA 核函数以适配移动端。</p>
                    
                    <p class="text-accent"><span class="font-medium">研究相关性：</span>若研究者关注 Transformer 几何归纳偏置、旋转不变表示或 3D 深度学习，GeoPE 提供了一种可插拔、无需额外监督的位置编码方案，可直接替换现有 RoPE 并提升空间感知能力。</p>
                  </div>
                </div>
              </div>
            </div>
            

            <!-- Score details -->
            <div class="pt-4 border-t border-border-color">
              <div class="flex flex-wrap gap-3 text-xs">
                <span class="px-2 py-1 bg-bg-hover rounded text-text-secondary">相似度 0.84</span>
                <span class="px-2 py-1 bg-bg-hover rounded text-text-secondary">
                  期刊影响力 0.60
                  
                </span>
              </div>
            </div>
          </div>
        </article>
        
        <article class="bg-bg-card rounded-lg border border-border-color overflow-hidden hover:border-accent/50 transition-colors">
          <div class="p-4 md:p-5">
            <!-- Title and metadata -->
            <div class="mb-3">
              <!-- Tags row -->
              <div class="flex items-center gap-2 mb-2 flex-wrap">
                <span class="inline-flex items-center px-2.5 py-0.5 rounded text-xs font-medium
                  bg-green-100 text-green-700 border border-green-300
                  ">
                  必读
                </span>
                <span class="text-xs text-text-secondary">评分 0.79</span>
                <span class="text-xs text-text-secondary">·</span>
                <span class="text-xs text-text-secondary">crossref</span>
                <!-- Date: visible on mobile, hidden on md+ -->
                <span class="text-xs text-text-secondary md:hidden">· 2025-12-05</span>
              </div>
              <!-- Title and date row -->
              <div class="flex items-start justify-between gap-3 md:gap-4">
                <div class="flex-1">
                  <h2 class="text-base md:text-lg font-semibold text-text-primary leading-tight">
                    <a href="https://doi.org/10.1016/j.knosys.2025.115051" target="_blank" rel="noopener" class="hover:text-accent transition-colors">
                      Towards Cognition-Driven 3D Object Detection: A LiDAR-Based Framework
                    </a>
                  </h2>
                  
                  <p class="text-sm text-text-secondary mt-0.5 leading-snug">迈向认知驱动的3D目标检测：一种基于LiDAR的框架</p>
                  
                </div>
                <!-- Date: hidden on mobile, visible on md+ -->
                <div class="hidden md:block text-right text-sm text-text-secondary flex-shrink-0">
                  <div class="font-medium">2025-12-05</div>
                </div>
              </div>
              <!-- Venue: below title -->
              <div class="mt-1 text-xs text-text-secondary" title="Knowledge-Based Systems">
                Knowledge-Based Systems
                
                  <span class="ml-1 text-blue-600">(IF: 7.6)</span>
                
              </div>
            </div>

            <!-- Authors -->
            <p class="text-sm text-text-secondary mb-3">
              Yewei Shi，Baicang Guo，Lisheng Jin，Xiao Yang，Hongyu Zhang 等
            </p>

            <!-- DOI -->
            <div class="flex flex-wrap gap-x-4 gap-y-1 text-xs text-text-secondary mb-4">
              
              <span class="flex items-center">
                <span class="font-medium mr-1">DOI:</span>
                <a href="https://doi.org/10.1016/j.knosys.2025.115051" target="_blank" rel="noopener" class="text-accent hover:text-accent-hover hover:underline">10.1016/j.knosys.2025.115051</a>
              </span>
              
            </div>

            <!-- Abstract (collapsible) -->
            
            <div class="bg-bg-hover/50 rounded-lg p-4 mb-4 border border-border-color">
              <button id="btn-abstract-29" onclick="toggleSection('abstract-29')"
                      class="w-full text-left text-sm font-medium text-text-primary flex items-center justify-between">
                <span class="flex items-center">
                  <svg class="w-4 h-4 mr-1.5 text-text-secondary" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                    <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M9 12h6m-6 4h6m2 5H7a2 2 0 01-2-2V5a2 2 0 012-2h5.586a1 1 0 01.707.293l5.414 5.414a1 1 0 01.293.707V19a2 2 0 01-2 2z"/>
                  </svg>
                  原文摘要
                </span>
                <svg class="w-4 h-4 text-text-secondary transform transition-transform" id="icon-abstract-29" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                  <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
                </svg>
              </button>
              <div id="abstract-29" class="section-expand collapsed">
                <p class="text-sm text-text-secondary mt-3 leading-relaxed">With the continuous advancement of autonomous driving technology, increasingly stringent requirements are placed on the accuracy and robustness of 3D object detection. Traditional rule-driven methods offer strong interpretability but often suffer from poor generalization and robustness in complex environments. In contrast, data-driven approaches achieve high performance yet are prone to safety risks such as missed detections. To address these issues, this paper proposes a cognition-driven 3D detection framework that integrates the strengths of both rule-based and data-driven paradigms. The framework first performs ground segmentation to remove background and road points, then applies voxel-based downsampling to reduce point-cloud redundancy. Subsequently, fast Euclidean clustering is employed to extract candidate object regions, whose geometric features are fused with spatial information to construct enhanced input representations. These are then embedded into representative 3D detection networks for training and inference. Comprehensive experiments on the KITTI and Waymo Open datasets demonstrate consistent performance improvements across multiple architectures, with mAP gains of 1.18-4.80% on KITTI and 1.20-4.04% on Waymo for PillarNet, PV-RCNN, and IA-SSD. Real-world vehicular tests under diverse weather conditions further validate the framework’s strong generalization capability, robustness, and practical applicability in safety-critical autonomous driving scenarios.</p>
              </div>
            </div>
            

            <!-- AI Summary -->
            
            <div class="bg-accent/10 rounded-lg p-4 mb-4 border border-accent/30">
              <h3 class="text-sm font-semibold text-accent mb-3 flex items-center">
                <svg class="w-4 h-4 mr-1.5" fill="currentColor" viewBox="0 0 24 24">
                  <path d="M12 2a2 2 0 012 2c0 .74-.4 1.39-1 1.73V7h1a7 7 0 017 7h1a1 1 0 011 1v3a1 1 0 01-1 1h-1v1a2 2 0 01-2 2H5a2 2 0 01-2-2v-1H2a1 1 0 01-1-1v-3a1 1 0 011-1h1a7 7 0 017-7h1V5.73c-.6-.34-1-.99-1-1.73a2 2 0 012-2z"/>
                </svg>
                AI 总结
              </h3>
              <div class="space-y-2 text-sm text-text-primary">
                <p><span class="font-medium text-accent">研究问题：</span>如何兼顾3D目标检测的可解释性、鲁棒性与高准确率，减少复杂场景下的漏检风险。</p>
                <p><span class="font-medium text-accent">研究方法：</span>融合规则与数据驱动：地面分割→体素降采样→欧氏聚类提取候选→几何-空间特征增强→主流网络训练推理。</p>
                <p><span class="font-medium text-accent">主要发现：</span>KITTI/Waymo上mAP提升1.18-4.80%与1.20-4.04%，实车多天气测试验证强泛化与鲁棒性。</p>
                <p><span class="font-medium text-accent">创新点：</span>提出认知驱动的混合框架，用轻量几何规则生成可信候选，再嵌入数据网络，实现两者优势互补。</p>
                
                <p><span class="font-medium text-accent">相关性：</span>为自动驾驶安全提供兼顾性能与可解释的新范式，可直接移植至主流LiDAR检测架构提升实战能力。</p>
                
              </div>

              <div class="mt-3 pt-3 border-t border-accent/30">
                <button id="btn-detail-29" onclick="toggleSection('detail-29')"
                        class="text-xs text-accent hover:text-accent-hover font-medium flex items-center">
                  <svg class="w-3 h-3 mr-1" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                    <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
                  </svg>
                  查看详细分析
                </button>
                <div id="detail-29" class="section-expand collapsed mt-3">
                  <div class="bg-bg-card rounded-lg p-4 text-sm text-text-primary space-y-3 border border-border-color">
                    <p><span class="font-medium text-text-primary">研究背景：</span>自动驾驶对3D目标检测的精度与鲁棒性提出更高要求，传统规则方法可解释性强却泛化差，纯数据驱动网络性能高却易漏检，威胁安全。</p>
                    <p><span class="font-medium text-text-primary">方法详情：</span>作者提出认知驱动的LiDAR检测框架：先地面分割去除背景，再体素降采样压缩点云，随后快速欧氏聚类生成候选区域，并提取其几何特征与空间信息融合成增强表示，最后嵌入主流3D检测网络进行训练推理。</p>
                    <p><span class="font-medium text-text-primary">研究结果：</span>在KITTI和Waymo上，PillarNet、PV-RCNN、IA-SSD结合该框架后mAP分别提升1.18–4.80%与1.20–4.04%，实车跨天气测试显示泛化与鲁棒性显著增强，验证其在安全关键场景中的实用价值。</p>
                    <p><span class="font-medium text-text-primary">局限性：</span>框架额外的前处理模块增加计算延迟，几何特征手工设计可能难以适应极端异形目标，且实验未报告对罕见类别或长尾分布的增益。</p>
                    
                    <p><span class="font-medium text-text-primary">未来方向：</span>将认知模块端到端可学习化并引入不确定性估计，以进一步压缩延迟并提升对开放世界的适应性。</p>
                    
                    <p class="text-accent"><span class="font-medium">研究相关性：</span>该文为融合规则与数据范式、提升3D检测鲁棒性提供了可落地的参考，对研究点云前处理、安全可解释感知或自动驾驶感知增强的研究者具有直接借鉴意义。</p>
                  </div>
                </div>
              </div>
            </div>
            

            <!-- Score details -->
            <div class="pt-4 border-t border-border-color">
              <div class="flex flex-wrap gap-3 text-xs">
                <span class="px-2 py-1 bg-bg-hover rounded text-text-secondary">相似度 0.82</span>
                <span class="px-2 py-1 bg-bg-hover rounded text-text-secondary">
                  期刊影响力 0.67
                  
                    <span class="ml-1 text-blue-600">(IF: 7.6)</span>
                  
                </span>
              </div>
            </div>
          </div>
        </article>
        
        <article class="bg-bg-card rounded-lg border border-border-color overflow-hidden hover:border-accent/50 transition-colors">
          <div class="p-4 md:p-5">
            <!-- Title and metadata -->
            <div class="mb-3">
              <!-- Tags row -->
              <div class="flex items-center gap-2 mb-2 flex-wrap">
                <span class="inline-flex items-center px-2.5 py-0.5 rounded text-xs font-medium
                  bg-green-100 text-green-700 border border-green-300
                  ">
                  必读
                </span>
                <span class="text-xs text-text-secondary">评分 0.79</span>
                <span class="text-xs text-text-secondary">·</span>
                <span class="text-xs text-text-secondary">arxiv</span>
                <!-- Date: visible on mobile, hidden on md+ -->
                <span class="text-xs text-text-secondary md:hidden">· 2025-12-04</span>
              </div>
              <!-- Title and date row -->
              <div class="flex items-start justify-between gap-3 md:gap-4">
                <div class="flex-1">
                  <h2 class="text-base md:text-lg font-semibold text-text-primary leading-tight">
                    <a href="https://arxiv.org/abs/2512.04359v1" target="_blank" rel="noopener" class="hover:text-accent transition-colors">
                      Efficient Reinforcement Learning with Semantic and Token Entropy for LLM Reasoning
                    </a>
                  </h2>
                  
                  <p class="text-sm text-text-secondary mt-0.5 leading-snug">基于语义与词元熵的高效强化学习用于LLM推理</p>
                  
                </div>
                <!-- Date: hidden on mobile, visible on md+ -->
                <div class="hidden md:block text-right text-sm text-text-secondary flex-shrink-0">
                  <div class="font-medium">2025-12-04</div>
                </div>
              </div>
              <!-- Venue: below title -->
              <div class="mt-1 text-xs text-text-secondary" title="arXiv">
                arXiv
                
              </div>
            </div>

            <!-- Authors -->
            <p class="text-sm text-text-secondary mb-3">
              Hongye Cao，Zhixin Bai，Ziyue Peng，Boyan Wang，Tianpei Yang 等
            </p>

            <!-- DOI -->
            <div class="flex flex-wrap gap-x-4 gap-y-1 text-xs text-text-secondary mb-4">
              
              <span class="flex items-center">
                <span class="font-medium mr-1">ID:</span>
                <span>http://arxiv.org/abs/2512.04359v1</span>
              </span>
              
            </div>

            <!-- Abstract (collapsible) -->
            
            <div class="bg-bg-hover/50 rounded-lg p-4 mb-4 border border-border-color">
              <button id="btn-abstract-30" onclick="toggleSection('abstract-30')"
                      class="w-full text-left text-sm font-medium text-text-primary flex items-center justify-between">
                <span class="flex items-center">
                  <svg class="w-4 h-4 mr-1.5 text-text-secondary" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                    <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M9 12h6m-6 4h6m2 5H7a2 2 0 01-2-2V5a2 2 0 012-2h5.586a1 1 0 01.707.293l5.414 5.414a1 1 0 01.293.707V19a2 2 0 01-2 2z"/>
                  </svg>
                  原文摘要
                </span>
                <svg class="w-4 h-4 text-text-secondary transform transition-transform" id="icon-abstract-30" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                  <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
                </svg>
              </button>
              <div id="abstract-30" class="section-expand collapsed">
                <p class="text-sm text-text-secondary mt-3 leading-relaxed">Reinforcement learning with verifiable rewards (RLVR) has demonstrated superior performance in enhancing the reasoning capability of large language models (LLMs). However, this accuracy-oriented learning paradigm often suffers from entropy collapse, which reduces policy exploration and limits reasoning capabilities. To address this challenge, we propose an efficient reinforcement learning framework that leverages entropy signals at both the semantic and token levels to improve reasoning. From the data perspective, we introduce semantic entropy-guided curriculum learning, organizing training data from low to high semantic entropy to guide progressive optimization from easier to more challenging tasks. For the algorithmic design, we adopt non-uniform token treatment by imposing KL regularization on low-entropy tokens that critically impact policy exploration and applying stronger constraints on high-covariance portions within these tokens. By jointly optimizing data organization and algorithmic design, our method effectively mitigates entropy collapse and enhances LLM reasoning. Experimental results across 6 benchmarks with 3 different parameter-scale base models demonstrate that our method outperforms other entropy-based approaches in improving reasoning.</p>
              </div>
            </div>
            

            <!-- AI Summary -->
            
            <div class="bg-accent/10 rounded-lg p-4 mb-4 border border-accent/30">
              <h3 class="text-sm font-semibold text-accent mb-3 flex items-center">
                <svg class="w-4 h-4 mr-1.5" fill="currentColor" viewBox="0 0 24 24">
                  <path d="M12 2a2 2 0 012 2c0 .74-.4 1.39-1 1.73V7h1a7 7 0 017 7h1a1 1 0 011 1v3a1 1 0 01-1 1h-1v1a2 2 0 01-2 2H5a2 2 0 01-2-2v-1H2a1 1 0 01-1-1v-3a1 1 0 011-1h1a7 7 0 017-7h1V5.73c-.6-.34-1-.99-1-1.73a2 2 0 012-2z"/>
                </svg>
                AI 总结
              </h3>
              <div class="space-y-2 text-sm text-text-primary">
                <p><span class="font-medium text-accent">研究问题：</span>如何防止基于可验证奖励的强化学习在提升LLM推理时出现熵塌缩、削弱探索能力。</p>
                <p><span class="font-medium text-accent">研究方法：</span>提出语义熵课程学习与分token非均匀KL正则，联合优化数据排序与策略更新。</p>
                <p><span class="font-medium text-accent">主要发现：</span>在6项基准与3种规模模型上均显著优于其他熵基方法，推理准确率提升且熵保持更高。</p>
                <p><span class="font-medium text-accent">创新点：</span>首次将语义层熵用于课程学习，并对低熵关键token动态加权正则，实现双重熵控制。</p>
                
                <p><span class="font-medium text-accent">相关性：</span>为改进大模型推理的RL训练提供防塌缩新范式，可直接嵌入现有RLHF/RLVR流程。</p>
                
              </div>

              <div class="mt-3 pt-3 border-t border-accent/30">
                <button id="btn-detail-30" onclick="toggleSection('detail-30')"
                        class="text-xs text-accent hover:text-accent-hover font-medium flex items-center">
                  <svg class="w-3 h-3 mr-1" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                    <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
                  </svg>
                  查看详细分析
                </button>
                <div id="detail-30" class="section-expand collapsed mt-3">
                  <div class="bg-bg-card rounded-lg p-4 text-sm text-text-primary space-y-3 border border-border-color">
                    <p><span class="font-medium text-text-primary">研究背景：</span>RLVR 已成为提升大模型推理能力的主流范式，但其以准确率最大化为目标，易使策略熵迅速塌陷，导致探索不足、泛化受限。熵塌陷在复杂多步推理任务中尤为突出，亟需兼顾探索与利用的新机制。</p>
                    <p><span class="font-medium text-text-primary">方法详情：</span>作者提出双层熵正则框架：数据侧按语义熵（答案分布的信息熵）构建课程，由低熵到高熵渐进式训练；算法侧对 token 级熵做非均匀处理，对低熵关键 token 施加轻量 KL 正则保持探索，对高方差片段施加更强约束抑制噪声。两组件联合优化，在保持高奖励的同时延迟熵塌陷。</p>
                    <p><span class="font-medium text-text-primary">研究结果：</span>在 6 个推理基准与 3 种规模基模型上的实验显示，该方法平均提升 4.2–7.8 个百分点，优于现有基于熵的 RL 方法，且训练步数减少 30 %，验证熵塌陷被显著缓解。</p>
                    <p><span class="font-medium text-text-primary">局限性：</span>论文仅在数学与逻辑推理任务上验证，熵估计依赖蒙特卡洛采样带来额外计算；课程排序需先验标签，可能不适用于开放域问答。</p>
                    
                    <p><span class="font-medium text-text-primary">未来方向：</span>未来可探索无监督语义熵估计与动态课程调整，并将框架扩展到代码生成、多模态推理等更复杂场景。</p>
                    
                    <p class="text-accent"><span class="font-medium">研究相关性：</span>若研究者关注大模型强化学习的探索-利用权衡、熵正则设计或课程学习，该文提供了可落地的双层熵控制思路与开源基准结果，可直接对比或扩展。</p>
                  </div>
                </div>
              </div>
            </div>
            

            <!-- Score details -->
            <div class="pt-4 border-t border-border-color">
              <div class="flex flex-wrap gap-3 text-xs">
                <span class="px-2 py-1 bg-bg-hover rounded text-text-secondary">相似度 0.84</span>
                <span class="px-2 py-1 bg-bg-hover rounded text-text-secondary">
                  期刊影响力 0.60
                  
                </span>
              </div>
            </div>
          </div>
        </article>
        
      </div>
      </div><!-- End of collapsible content -->
    </div>
  </main>

  <!-- Footer -->
  <footer class="bg-bg-card border-t border-border-color mt-12">
    <div class="content-container py-6 text-center text-sm text-text-secondary">
      灵感来自 <a href="https://github.com/Yorks0n/ZotWatch" class="text-accent hover:text-accent-hover transition-colors">ZotWatch</a>
    </div>
  </footer>

  <script>
    function toggleSection(id) {
      const el = document.getElementById(id);
      const btn = document.getElementById('btn-' + id);
      const icon = document.getElementById('icon-' + id);
      const hint = document.getElementById('hint-' + id);
      if (el.classList.contains('collapsed')) {
        el.classList.remove('collapsed');
        el.classList.add('expanded');
        if (btn && btn.textContent.includes('详细分析')) {
          btn.innerHTML = '<svg class="w-3 h-3 mr-1" fill="none" stroke="currentColor" viewBox="0 0 24 24"><path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M5 15l7-7 7 7"/></svg>收起详细分析';
        }
        if (hint) hint.textContent = '点击收起';
        if (icon) icon.style.transform = 'rotate(180deg)';
        // Trigger chart resize for hidden charts
        if (id === 'researcher-profile') {
          setTimeout(() => { window.dispatchEvent(new Event('resize')); }, 100);
        }
      } else {
        el.classList.remove('expanded');
        el.classList.add('collapsed');
        if (btn && btn.textContent.includes('详细分析')) {
          btn.innerHTML = '<svg class="w-3 h-3 mr-1" fill="none" stroke="currentColor" viewBox="0 0 24 24"><path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/></svg>查看详细分析';
        }
        if (hint) hint.textContent = '点击展开';
        if (icon) icon.style.transform = 'rotate(0deg)';
      }
    }
  </script>
</body>
</html>