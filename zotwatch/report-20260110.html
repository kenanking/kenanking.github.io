<!DOCTYPE html>
<html lang="zh-CN">
<head>
  <meta charset="utf-8">
  <meta name="viewport" content="width=device-width, initial-scale=1">
  <title>ZotWatch 文献推荐 - 2026-01-10</title>
  <script src="https://cdn.tailwindcss.com"></script>
  <script src="https://cdn.jsdelivr.net/npm/chart.js@4.4.1/dist/chart.umd.min.js"></script>
  <script src="https://d3js.org/d3.v7.min.js"></script>
  <script>
    tailwind.config = {
      theme: {
        extend: {
          colors: {
            'bg-primary': '#f8fafc',
            'bg-card': '#ffffff',
            'bg-hover': '#f1f5f9',
            'text-primary': '#1e293b',
            'text-secondary': '#64748b',
            'border-color': '#e2e8f0',
            'accent': '#2563eb',
            'accent-hover': '#1d4ed8',
          }
        }
      }
    }
  </script>
  <style>
    .section-expand { transition: max-height 0.5s ease-out; overflow: hidden; }
    .section-expand.collapsed { max-height: 0; }
    .section-expand.expanded { max-height: none; }
    body { font-family: -apple-system, BlinkMacSystemFont, 'Segoe UI', Roboto, 'Helvetica Neue', Arial, sans-serif; }
    .content-container { max-width: 48rem; margin: 0 auto; padding: 0 1rem; }
    @media (min-width: 768px) { .content-container { padding: 0 2rem; } }
    @media (min-width: 1024px) { .content-container { padding: 0 4rem; } }

    .cluster-graph-container { width: 100%; height: 280px; touch-action: pan-y; }
    @media (min-width: 640px) { .cluster-graph-container { height: 320px; } }
    @media (min-width: 768px) { .cluster-graph-container { height: 350px; } }
    #cluster-graph svg { overflow: visible; }
  </style>
</head>
<body class="bg-bg-primary min-h-screen text-text-primary">
  <header class="bg-bg-card border-b border-border-color">
    <div class="content-container py-6 md:py-8">
      <h1 class="text-xl md:text-2xl font-bold text-text-primary">ZotWatch 文献推荐</h1>
      <p class="text-xs md:text-sm text-text-secondary mt-1">
        共 35 篇论文 ·
        生成于 2026-01-10 10:38 Asia/Shanghai
      </p>
    </div>
  </header>

  
  <section class="py-4 md:py-6 border-b border-border-color">
    <div class="content-container">
      <button id="btn-researcher-profile" onclick="toggleSection('researcher-profile')"
              class="w-full text-left text-lg md:text-xl font-bold text-text-primary mb-3 md:mb-4 flex items-center justify-between group hover:text-accent transition-colors">
        <span class="flex items-center gap-2 flex-1 min-w-0">
          <svg class="w-5 h-5 flex-shrink-0 text-accent" fill="currentColor" viewBox="0 0 24 24">
            <path d="M12 2C6.48 2 2 6.48 2 12s4.48 10 10 10 10-4.48 10-10S17.52 2 12 2zm0 3c1.66 0 3 1.34 3 3s-1.34 3-3 3-3-1.34-3-3 1.34-3 3-3zm0 14.2c-2.5 0-4.71-1.28-6-3.22.03-1.99 4-3.08 6-3.08 1.99 0 5.97 1.09 6 3.08-1.29 1.94-3.5 3.22-6 3.22z"/>
          </svg>
          <span class="truncate">研究兴趣画像</span>
        </span>
        <span class="flex items-center gap-1 md:gap-2 flex-shrink-0 ml-2">
          <span class="hidden md:inline text-sm font-normal text-text-secondary group-hover:text-accent" id="hint-researcher-profile">点击展开</span>
          <svg class="w-5 h-5 md:w-4 md:h-4 text-text-secondary group-hover:text-accent transform transition-transform" id="icon-researcher-profile" fill="none" stroke="currentColor" viewBox="0 0 24 24">
            <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
          </svg>
        </span>
      </button>

      <div id="researcher-profile" class="section-expand collapsed">
      <div class="bg-bg-card rounded-lg border border-border-color p-4 mb-4">
        <div class="grid grid-cols-4 gap-3 text-center">
          <div>
            <div class="text-xl font-bold text-accent">958</div>
            <div class="text-xs text-text-secondary">收藏论文</div>
          </div>
          <div>
            <div class="text-xl font-bold text-accent">5年8月</div>
            <div class="text-xs text-text-secondary">收藏时长</div>
          </div>
          <div>
            <div class="text-xl font-bold text-accent">&gt;10</div>
            <div class="text-xs text-text-secondary">兴趣领域</div>
          </div>
          <div>
            <div class="text-xl font-bold text-accent">8</div>
            <div class="text-xs text-text-secondary">高频作者</div>
          </div>
        </div>
      </div>

      
      <div class="bg-accent/10 rounded-lg border border-accent/30 p-4 mb-4">
        <h3 class="text-sm font-semibold text-accent mb-2 flex items-center gap-2">
          <svg class="w-4 h-4" fill="currentColor" viewBox="0 0 24 24">
            <path d="M12 2a2 2 0 012 2c0 .74-.4 1.39-1 1.73V7h1a7 7 0 017 7h1a1 1 0 011 1v3a1 1 0 01-1 1h-1v1a2 2 0 01-2 2H5a2 2 0 01-2-2v-1H2a1 1 0 01-1-1v-3a1 1 0 011-1h1a7 7 0 017-7h1V5.73c-.6-.34-1-.99-1-1.73a2 2 0 012-2z"/>
          </svg>
          AI 研究兴趣分析
        </h3>
        <div class="space-y-2 text-xs text-text-primary leading-relaxed">
          <p><span class="font-medium text-accent">研究兴趣：</span>该用户长期关注计算机视觉与遥感交叉方向，核心阅读集中在目标检测、轻量网络及视觉SLAM，同时对自监督、大模型等前沿主题保持跟踪。</p>
          <p><span class="font-medium text-accent">深度关注：</span>在目标检测领域收藏量最高且持续追踪Ross Girshick、Kaiming He等权威团队工作；对遥感影像（SAR）智能解译形成稳定分支，常年阅读TGRS、雷达学报并积累SAR目标识别、旋转目标检测等关键词。</p>
          <p><span class="font-medium text-accent">跨学科倾向：</span>阅读横跨计算机视觉与地球科学，既关注CVPR/ICCV/NeurIPS等AI顶会，也系统收藏IEEE TGRS、雷达学报等遥感期刊，体现出将通用视觉方法迁移至遥感数据的跨学科取向。</p>
          <p><span class="font-medium text-accent">兴趣演变：</span>2025-Q1出现收藏高峰后回落，新增论文聚焦“合成孔径雷达目标检测”“多任务学习”，显示兴趣正向SAR精细化检测与多任务联合学习收敛；同时保留对大模型、自监督、扩散模型的零星追踪。</p>
          <p><span class="font-medium text-accent">阅读建议：</span>可进一步关注多模态遥感基础模型与雷达-视觉融合检测，以及面向星载/机载实时处理的轻量化多任务网络，以延续检测精度与模型效率并重的阅读主线。</p>
        </div>
      </div>
      

      <div class="grid grid-cols-1 md:grid-cols-2 gap-4 mb-4">
        
        <div class="bg-bg-card rounded-lg border border-border-color p-4">
          <h4 class="text-sm font-medium text-text-primary mb-3">研究领域分布</h4>
          <div style="height: 180px;">
            <canvas id="domainsChart"></canvas>
          </div>
        </div>
        

        
        <div class="bg-bg-card rounded-lg border border-border-color p-4">
          <h4 class="text-sm font-medium text-text-primary mb-3">阅读趋势（季度）</h4>
          <div style="height: 180px;">
            <canvas id="trendsChart"></canvas>
          </div>
        </div>
        
      </div>

      
      <div class="bg-bg-card rounded-lg border border-border-color p-4 mb-4">
        <h4 class="text-sm font-medium text-text-primary mb-3">论文发表年份分布</h4>
        <div style="height: 180px;">
          <canvas id="yearDistChart"></canvas>
        </div>
      </div>
      

      
      <div class="bg-bg-card rounded-lg border border-border-color p-4 mb-4">
        <h4 class="text-sm font-medium text-text-primary mb-3">
          研究方向聚类图谱
          <span class="text-xs text-text-secondary font-normal ml-2">(30 个聚类)</span>
        </h4>
        <div id="cluster-graph" class="cluster-graph-container"></div>
        <div class="text-xs text-text-secondary mt-3">
          覆盖 934/934 篇论文（仅含摘要的文献参与聚类）
        </div>
      </div>
      

      <div class="grid grid-cols-1 md:grid-cols-2 gap-4 mb-4">
        
        <div class="bg-bg-card rounded-lg border border-border-color p-4">
          <h4 class="text-sm font-medium text-text-primary mb-2">高频关注作者</h4>
          <div class="grid grid-cols-2 gap-x-4 gap-y-1 text-xs">
            
            <div class="flex justify-between truncate">
              <span class="text-text-primary truncate" title="Kaiming He">Kaiming He</span>
              <span class="text-text-secondary ml-1 flex-shrink-0">24</span>
            </div>
            
            <div class="flex justify-between truncate">
              <span class="text-text-primary truncate" title="Ross Girshick">Ross Girshick</span>
              <span class="text-text-secondary ml-1 flex-shrink-0">18</span>
            </div>
            
            <div class="flex justify-between truncate">
              <span class="text-text-primary truncate" title="Jian Sun">Jian Sun</span>
              <span class="text-text-secondary ml-1 flex-shrink-0">15</span>
            </div>
            
            <div class="flex justify-between truncate">
              <span class="text-text-primary truncate" title="Song Han">Song Han</span>
              <span class="text-text-secondary ml-1 flex-shrink-0">14</span>
            </div>
            
            <div class="flex justify-between truncate">
              <span class="text-text-primary truncate" title="Xiangyu Zhang">Xiangyu Zhang</span>
              <span class="text-text-secondary ml-1 flex-shrink-0">14</span>
            </div>
            
            <div class="flex justify-between truncate">
              <span class="text-text-primary truncate" title="Piotr Dollár">Piotr Dollár</span>
              <span class="text-text-secondary ml-1 flex-shrink-0">12</span>
            </div>
            
            <div class="flex justify-between truncate">
              <span class="text-text-primary truncate" title="Li Liu">Li Liu</span>
              <span class="text-text-secondary ml-1 flex-shrink-0">11</span>
            </div>
            
            <div class="flex justify-between truncate">
              <span class="text-text-primary truncate" title="Xiang Li">Xiang Li</span>
              <span class="text-text-secondary ml-1 flex-shrink-0">11</span>
            </div>
            
            <div class="flex justify-between truncate">
              <span class="text-text-primary truncate" title="Yann LeCun">Yann LeCun</span>
              <span class="text-text-secondary ml-1 flex-shrink-0">9</span>
            </div>
            
            <div class="flex justify-between truncate">
              <span class="text-text-primary truncate" title="Wenfeng Liang">Wenfeng Liang</span>
              <span class="text-text-secondary ml-1 flex-shrink-0">9</span>
            </div>
            
            <div class="flex justify-between truncate">
              <span class="text-text-primary truncate" title="Feng Xu">Feng Xu</span>
              <span class="text-text-secondary ml-1 flex-shrink-0">9</span>
            </div>
            
            <div class="flex justify-between truncate">
              <span class="text-text-primary truncate" title="Chunhua Shen">Chunhua Shen</span>
              <span class="text-text-secondary ml-1 flex-shrink-0">8</span>
            </div>
            
          </div>
        </div>
        

        
        <div class="bg-bg-card rounded-lg border border-border-color p-4">
          <h4 class="text-sm font-medium text-text-primary mb-2">常关注期刊/会议</h4>
          <div class="space-y-1 text-xs">
            
            <div class="flex items-center justify-between">
              <span class="text-text-primary truncate flex-1" title="Computer Vision and Pattern Recognition">Computer Vision and Pattern Recognition</span>
              <span class="text-xs px-1 py-0.5 rounded bg-blue-100 text-blue-700 ml-1 flex-shrink-0">
                期刊
              </span>
              <span class="text-text-secondary ml-1 flex-shrink-0">114</span>
            </div>
            
            <div class="flex items-center justify-between">
              <span class="text-text-primary truncate flex-1" title="IEEE Transactions on Geoscience and Remote Sensing">IEEE Transactions on Geoscience and Remote Sensing</span>
              <span class="text-xs px-1 py-0.5 rounded bg-blue-100 text-blue-700 ml-1 flex-shrink-0">
                期刊
              </span>
              <span class="text-text-secondary ml-1 flex-shrink-0">46</span>
            </div>
            
            <div class="flex items-center justify-between">
              <span class="text-text-primary truncate flex-1" title="Neural Information Processing Systems">Neural Information Processing Systems</span>
              <span class="text-xs px-1 py-0.5 rounded bg-blue-100 text-blue-700 ml-1 flex-shrink-0">
                期刊
              </span>
              <span class="text-text-secondary ml-1 flex-shrink-0">45</span>
            </div>
            
            <div class="flex items-center justify-between">
              <span class="text-text-primary truncate flex-1" title="International Conference on Computer Vision">International Conference on Computer Vision</span>
              <span class="text-xs px-1 py-0.5 rounded bg-amber-100 text-amber-700 ml-1 flex-shrink-0">
                会议
              </span>
              <span class="text-text-secondary ml-1 flex-shrink-0">38</span>
            </div>
            
            <div class="flex items-center justify-between">
              <span class="text-text-primary truncate flex-1" title="IEEE Transactions on Pattern Analysis and Machine Intelligence">IEEE Transactions on Pattern Analysis and Machine Intelligence</span>
              <span class="text-xs px-1 py-0.5 rounded bg-blue-100 text-blue-700 ml-1 flex-shrink-0">
                期刊
              </span>
              <span class="text-text-secondary ml-1 flex-shrink-0">32</span>
            </div>
            
            <div class="flex items-center justify-between">
              <span class="text-text-primary truncate flex-1" title="International Conference on Learning Representations">International Conference on Learning Representations</span>
              <span class="text-xs px-1 py-0.5 rounded bg-amber-100 text-amber-700 ml-1 flex-shrink-0">
                会议
              </span>
              <span class="text-text-secondary ml-1 flex-shrink-0">27</span>
            </div>
            
            <div class="flex items-center justify-between">
              <span class="text-text-primary truncate flex-1" title="European Conference on Computer Vision">European Conference on Computer Vision</span>
              <span class="text-xs px-1 py-0.5 rounded bg-amber-100 text-amber-700 ml-1 flex-shrink-0">
                会议
              </span>
              <span class="text-text-secondary ml-1 flex-shrink-0">23</span>
            </div>
            
            <div class="flex items-center justify-between">
              <span class="text-text-primary truncate flex-1" title="IEEE Geoscience and Remote Sensing Letters">IEEE Geoscience and Remote Sensing Letters</span>
              <span class="text-xs px-1 py-0.5 rounded bg-blue-100 text-blue-700 ml-1 flex-shrink-0">
                期刊
              </span>
              <span class="text-text-secondary ml-1 flex-shrink-0">22</span>
            </div>
            
          </div>
        </div>
        
      </div>

      
      <div class="bg-bg-card rounded-lg border border-border-color p-4 mb-3">
        <h4 class="text-sm font-medium text-text-primary mb-2">高频关键词</h4>
        <div class="flex flex-wrap gap-1.5">
          
          <span class="px-2 py-0.5 bg-bg-hover rounded text-xs text-text-primary">
            大语言模型 <span class="text-text-secondary">(14)</span>
          </span>
          
          <span class="px-2 py-0.5 bg-bg-hover rounded text-xs text-text-primary">
            自监督学习 <span class="text-text-secondary">(11)</span>
          </span>
          
          <span class="px-2 py-0.5 bg-bg-hover rounded text-xs text-text-primary">
            合成孔径雷达 <span class="text-text-secondary">(11)</span>
          </span>
          
          <span class="px-2 py-0.5 bg-bg-hover rounded text-xs text-text-primary">
            综述 <span class="text-text-secondary">(9)</span>
          </span>
          
          <span class="px-2 py-0.5 bg-bg-hover rounded text-xs text-text-primary">
            域自适应 <span class="text-text-secondary">(9)</span>
          </span>
          
          <span class="px-2 py-0.5 bg-bg-hover rounded text-xs text-text-primary">
            目标检测 <span class="text-text-secondary">(8)</span>
          </span>
          
          <span class="px-2 py-0.5 bg-bg-hover rounded text-xs text-text-primary">
            迁移学习 <span class="text-text-secondary">(7)</span>
          </span>
          
          <span class="px-2 py-0.5 bg-bg-hover rounded text-xs text-text-primary">
            扩散模型 <span class="text-text-secondary">(7)</span>
          </span>
          
          <span class="px-2 py-0.5 bg-bg-hover rounded text-xs text-text-primary">
            强化学习 <span class="text-text-secondary">(7)</span>
          </span>
          
          <span class="px-2 py-0.5 bg-bg-hover rounded text-xs text-text-primary">
            深度学习 <span class="text-text-secondary">(7)</span>
          </span>
          
          <span class="px-2 py-0.5 bg-bg-hover rounded text-xs text-text-primary">
            注意力机制 <span class="text-text-secondary">(6)</span>
          </span>
          
          <span class="px-2 py-0.5 bg-bg-hover rounded text-xs text-text-primary">
            重参数化 <span class="text-text-secondary">(6)</span>
          </span>
          
          <span class="px-2 py-0.5 bg-bg-hover rounded text-xs text-text-primary">
            DeepSeek <span class="text-text-secondary">(6)</span>
          </span>
          
          <span class="px-2 py-0.5 bg-bg-hover rounded text-xs text-text-primary">
            生成对抗网络 <span class="text-text-secondary">(6)</span>
          </span>
          
          <span class="px-2 py-0.5 bg-bg-hover rounded text-xs text-text-primary">
            基础模型 <span class="text-text-secondary">(5)</span>
          </span>
          
          <span class="px-2 py-0.5 bg-bg-hover rounded text-xs text-text-primary">
            Transformers <span class="text-text-secondary">(5)</span>
          </span>
          
          <span class="px-2 py-0.5 bg-bg-hover rounded text-xs text-text-primary">
            卷积神经网络 <span class="text-text-secondary">(5)</span>
          </span>
          
          <span class="px-2 py-0.5 bg-bg-hover rounded text-xs text-text-primary">
            旋转目标检测 <span class="text-text-secondary">(5)</span>
          </span>
          
          <span class="px-2 py-0.5 bg-bg-hover rounded text-xs text-text-primary">
            SAR目标识别 <span class="text-text-secondary">(5)</span>
          </span>
          
          <span class="px-2 py-0.5 bg-bg-hover rounded text-xs text-text-primary">
            特征可视化 <span class="text-text-secondary">(5)</span>
          </span>
          
        </div>
      </div>
      

      <div class="text-xs text-text-secondary text-right">
        画像生成于 2026-01-10 10:27 Asia/Shanghai
        · kimi-k2-turbo-preview
      </div>
      </div>
    </div>
  </section>
  

  
  <script>
    document.addEventListener('DOMContentLoaded', function() {
      Chart.defaults.font.family = "-apple-system, BlinkMacSystemFont, 'Segoe UI', Roboto, 'Helvetica Neue', Arial, sans-serif";
      Chart.defaults.font.size = 11;
      Chart.defaults.color = '#64748b';
      Chart.defaults.animation = false;

      
      const domainsCtx = document.getElementById('domainsChart');
      if (domainsCtx) {
        new Chart(domainsCtx, {
          type: 'bar',
          data: {
            labels: ['视觉SLAM', '目标检测', '姿态估计', '人脸识别', '轻量网络', '对比学习', 'Transformer', '模型压缩'],
            datasets: [{
              data: [22, 35, 15, 12, 18, 10, 10, 10],
              backgroundColor: 'rgba(37, 99, 235, 0.7)',
              borderColor: 'rgba(37, 99, 235, 1)',
              borderWidth: 1,
              borderRadius: 2,
              barThickness: 16
            }]
          },
          options: {
            indexAxis: 'y',
            responsive: true,
            maintainAspectRatio: false,
            plugins: {
              legend: { display: false },
              tooltip: { enabled: false }
            },
            scales: {
              x: {
                beginAtZero: true,
                grid: { color: 'rgba(226, 232, 240, 0.5)' },
                ticks: { precision: 0 }
              },
              y: {
                grid: { display: false },
                ticks: { font: { size: 10 } }
              }
            }
          }
        });
      }
      

      
      const trendsCtx = document.getElementById('trendsChart');
      if (trendsCtx) {
        const trends = [{ q: '2024-Q1', c: 67 }, { q: '2024-Q2', c: 18 }, { q: '2024-Q3', c: 4 }, { q: '2024-Q4', c: 24 }, { q: '2025-Q1', c: 97 }, { q: '2025-Q2', c: 34 }, { q: '2025-Q3', c: 13 }, { q: '2025-Q4', c: 30 }, { q: '2026-Q1', c: 5 }];
        const monthColors = [
          { bg: 'rgba(59, 130, 246, 0.7)', border: 'rgba(59, 130, 246, 1)' },
          { bg: 'rgba(16, 185, 129, 0.7)', border: 'rgba(16, 185, 129, 1)' },
          { bg: 'rgba(245, 158, 11, 0.7)', border: 'rgba(245, 158, 11, 1)' }
        ];
        const bgColors = trends.map((t, i) => monthColors[i % 3].bg);
        const borderColors = trends.map((t, i) => monthColors[i % 3].border);
        new Chart(trendsCtx, {
          type: 'bar',
          data: {
            labels: trends.map(t => t.q.replace('-', '\n')),
            datasets: [{
              data: trends.map(t => t.c),
              backgroundColor: bgColors,
              borderColor: borderColors,
              borderWidth: 1,
              borderRadius: 2,
              barThickness: 20
            }]
          },
          options: {
            responsive: true,
            maintainAspectRatio: false,
            plugins: {
              legend: { display: false },
              tooltip: { enabled: false }
            },
            scales: {
              x: {
                grid: { display: false },
                ticks: { font: { size: 9 }, maxRotation: 0 }
              },
              y: {
                beginAtZero: true,
                grid: { color: 'rgba(226, 232, 240, 0.5)' },
                ticks: { precision: 0 }
              }
            }
          }
        });
      }
      

      
      const yearDistCtx = document.getElementById('yearDistChart');
      if (yearDistCtx) {
        const yearData = [{ year: 2007, count: 3 }, { year: 2008, count: 2 }, { year: 2009, count: 2 }, { year: 2010, count: 9 }, { year: 2011, count: 10 }, { year: 2012, count: 3 }, { year: 2013, count: 12 }, { year: 2014, count: 14 }, { year: 2015, count: 18 }, { year: 2016, count: 15 }, { year: 2017, count: 41 }, { year: 2018, count: 59 }, { year: 2019, count: 56 }, { year: 2020, count: 66 }, { year: 2021, count: 84 }, { year: 2022, count: 112 }, { year: 2023, count: 110 }, { year: 2024, count: 113 }, { year: 2025, count: 174 }, { year: 2026, count: 5 }];
        new Chart(yearDistCtx, {
          type: 'line',
          data: {
            labels: yearData.map(d => d.year),
            datasets: [{
              data: yearData.map(d => d.count),
              borderColor: 'rgba(37, 99, 235, 1)',
              backgroundColor: 'rgba(37, 99, 235, 0.1)',
              borderWidth: 2,
              fill: true,
              tension: 0.3,
              pointRadius: 2,
              pointHoverRadius: 4
            }]
          },
          options: {
            responsive: true,
            maintainAspectRatio: false,
            plugins: {
              legend: { display: false },
              tooltip: {
                callbacks: {
                  label: function(context) {
                    return context.parsed.y + ' 篇';
                  }
                }
              }
            },
            scales: {
              x: {
                grid: { display: false },
                ticks: {
                  font: { size: 9 },
                  maxRotation: 45,
                  callback: function(value, index) {
                    const year = this.getLabelForValue(value);
                    return year % 5 === 0 ? year : '';
                  }
                }
              },
              y: {
                beginAtZero: true,
                grid: { color: 'rgba(226, 232, 240, 0.5)' },
                ticks: { precision: 0 }
              }
            }
          }
        });
      }
      

      
      const clusterContainer = document.getElementById('cluster-graph');
      if (clusterContainer) {
        const clusters = [
          
          {
            id: 0,
            label: "SAR\u57df\u81ea\u9002\u5e94\u76ee\u6807\u8bc6\u522b",
            size: 82,
            keywords: ["\u5408\u6210\u5b54\u5f84\u96f7\u8fbe", "SAR\u76ee\u6807\u8bc6\u522b", "\u8fc1\u79fb\u5b66\u4e60"]
          },
          
          {
            id: 1,
            label: "\u5b9e\u65f6\u76ee\u6807\u68c0\u6d4b\u67b6\u6784",
            size: 60,
            keywords: ["\u7efc\u8ff0", "\u8f7b\u91cf\u7ea7\u6a21\u578b", "DETR"]
          },
          
          {
            id: 2,
            label: "SAR\u8230\u8239\u68c0\u6d4b",
            size: 50,
            keywords: ["\u5408\u6210\u5b54\u5f84\u96f7\u8fbe", "SAR\u8230\u8239\u68c0\u6d4b", "\u591a\u5c3a\u5ea6\u7279\u5f81\u878d\u5408"]
          },
          
          {
            id: 3,
            label: "\u6269\u6563\u751f\u6210\u6a21\u578b",
            size: 50,
            keywords: ["\u6269\u6563\u6a21\u578b", "\u751f\u6210\u5bf9\u6297\u7f51\u7edc", "\u6807\u51c6\u5316\u6d41"]
          },
          
          {
            id: 4,
            label: "\u81ea\u76d1\u7763\u89c6\u89c9Transformer",
            size: 45,
            keywords: ["\u81ea\u76d1\u7763\u5b66\u4e60", "\u89c6\u89c9Transformer", "Swin Transformer"]
          },
          
          {
            id: 5,
            label: "\u591a\u4f20\u611f\u56683D\u611f\u77e5",
            size: 40,
            keywords: ["SIFT", "\u4e09\u7ef4\u611f\u77e5", "\u4f4d\u7f6e\u7f16\u7801"]
          },
          
          {
            id: 6,
            label: "\u7ea2\u5916\u5c0f\u76ee\u6807\u68c0\u6d4b",
            size: 39,
            keywords: ["\u7ea2\u5916\u5c0f\u76ee\u6807\u68c0\u6d4b", "\u9065\u611f\u76ee\u6807\u68c0\u6d4b", "\u6837\u672c\u81ea\u9002\u5e94\u589e\u6b96"]
          },
          
          {
            id: 7,
            label: "\u4eba\u4f53\u59ff\u6001\u4f30\u8ba1",
            size: 38,
            keywords: ["HRNet", "Transformers", "\u5308\u7259\u5229\u7b97\u6cd5"]
          },
          
          {
            id: 8,
            label: "\u5927\u6a21\u578b\u9ad8\u6548\u8bad\u7ec3",
            size: 38,
            keywords: ["DeepSeek", "\u5927\u8bed\u8a00\u6a21\u578b", "\u6df7\u5408\u4e13\u5bb6\u6a21\u578b"]
          },
          
          {
            id: 9,
            label: "\u8f7b\u91cf\u7ea7CNN\u8bbe\u8ba1",
            size: 38,
            keywords: ["\u91cd\u53c2\u6570\u5316", "VGG", "\u5377\u79ef\u795e\u7ecf\u7f51\u7edc"]
          },
          
          {
            id: 10,
            label: "\u5f31\u5c0f\u76ee\u6807\u667a\u80fd\u68c0\u6d4b",
            size: 37,
            keywords: ["\u6df1\u5ea6\u5b66\u4e60", "\u7279\u5f81\u878d\u5408", "\u5f31\u5c0f\u76ee\u6807\u8ddf\u8e2a"]
          },
          
          {
            id: 11,
            label: "\u5c0f\u6837\u672c\u76ee\u6807\u68c0\u6d4b",
            size: 37,
            keywords: ["\u5bf9\u6bd4\u5b66\u4e60", "\u5f00\u653e\u96c6\u8bc6\u522b", "SimCLR"]
          },
          
          {
            id: 12,
            label: "\u795e\u7ecf\u7f51\u7edc\u91cf\u5316\u538b\u7f29",
            size: 37,
            keywords: ["\u6a21\u578b\u538b\u7f29", "\u6a21\u578b\u91cf\u5316", "\u6574\u6570\u63a8\u7406"]
          },
          
          {
            id: 13,
            label: "SAR\u6210\u50cf\u4e0e\u7269\u7406\u667a\u80fd",
            size: 32,
            keywords: ["\u5fae\u6ce2\u89c6\u89c9", "\u7269\u7406\u667a\u80fd", "\u7535\u78c1\u6563\u5c04"]
          },
          
          {
            id: 14,
            label: "LLM\u5f3a\u5316\u63a8\u7406",
            size: 29,
            keywords: ["\u5927\u8bed\u8a00\u6a21\u578b", "\u5f3a\u5316\u5b66\u4e60", "\u6a21\u578b\u63a8\u7406"]
          },
          
          {
            id: 15,
            label: "\u8f66\u724c\u8bc6\u522b\u7cfb\u7edf",
            size: 27,
            keywords: ["\u8f66\u724c\u8bc6\u522b", "Internet of Things", "Microcontrollers"]
          },
          
          {
            id: 16,
            label: "\u673a\u5668\u5b66\u4e60\u539f\u7406\u4e0e\u4f18\u5316",
            size: 26,
            keywords: ["\u5e95\u5c42\u7b97\u6cd5", "\u6027\u80fd\u4f18\u5316", "\u53ef\u5fae\u5206\u7f16\u7a0b"]
          },
          
          {
            id: 17,
            label: "\u6df1\u5ea6\u7279\u5f81\u53ef\u89c6\u5316",
            size: 25,
            keywords: ["\u7279\u5f81\u53ef\u89c6\u5316", "Grad-CAM", "\u4e3b\u6210\u5206\u5206\u6790"]
          },
          
          {
            id: 18,
            label: "\u795e\u7ecf\u7f51\u7edc\u4f18\u5316\u7406\u8bba",
            size: 25,
            keywords: ["\u4f18\u5316\u5668", "\u5206\u5e03\u5f0f\u8bad\u7ec3", "\u5927\u6279\u91cf\u8bad\u7ec3"]
          },
          
          {
            id: 19,
            label: "\u96f7\u8fbe\u76ee\u6807\u68c0\u6d4b",
            size: 23,
            keywords: ["\u6052\u865a\u8b66\u7387\u68c0\u6d4b", "\u76ee\u6807\u68c0\u6d4b", "\u6d77\u6742\u6ce2\u6291\u5236"]
          },
          
          {
            id: 20,
            label: "Vision Transformer\u7efc\u8ff0",
            size: 22,
            keywords: ["\u7efc\u8ff0", "Vision Transformers", "Transformers"]
          },
          
          {
            id: 21,
            label: "\u591a\u4f20\u611f\u5668\u5168\u5c40\u5b9a\u4f4d",
            size: 21,
            keywords: []
          },
          
          {
            id: 22,
            label: "\u57df\u81ea\u9002\u5e94\u76ee\u6807\u68c0\u6d4b",
            size: 21,
            keywords: ["\u57df\u81ea\u9002\u5e94", "\u5355\u9636\u6bb5\u68c0\u6d4b", "\u68c0\u6d4b\u5668\u8fc1\u79fb"]
          },
          
          {
            id: 23,
            label: "\u6301\u7eed\u5b66\u4e60\u4e0e\u6b8b\u5dee\u7f51\u7edc",
            size: 20,
            keywords: ["\u4eba\u5de5\u795e\u7ecf\u7f51\u7edc", "\u6b8b\u5dee\u8fde\u63a5", "\u6df1\u5ea6\u6b8b\u5dee\u5b66\u4e60"]
          },
          
          {
            id: 24,
            label: "\u9ad8\u5206\u8fa8\u7387\u8bed\u4e49\u5206\u5272",
            size: 18,
            keywords: ["HRNet", "U-Net\u7f51\u7edc", "\u533b\u5b66\u56fe\u50cf\u5904\u7406"]
          },
          
          {
            id: 25,
            label: "SAM\u901a\u7528\u5206\u5272",
            size: 18,
            keywords: ["\u57fa\u7840\u6a21\u578b", "\u901a\u7528\u5206\u5272", "\u96f6\u6837\u672c\u5206\u5272"]
          },
          
          {
            id: 26,
            label: "\u5b66\u672f\u51fa\u7248\u4e0e\u7b97\u6cd5",
            size: 17,
            keywords: ["LaTeX", "\u8bbe\u8ba1\u6a21\u5f0f", "\u7814\u7a76"]
          },
          
          {
            id: 27,
            label: "\u68c0\u6d4b\u635f\u5931\u4e0e\u8bc4\u4ef7",
            size: 14,
            keywords: ["\u65cb\u8f6c\u76ee\u6807\u68c0\u6d4b", "\u635f\u5931\u51fd\u6570", "\u5224\u522b\u5f0f\u8bad\u7ec3"]
          },
          
          {
            id: 28,
            label: "\u6570\u636e\u589e\u5f3a\u6b63\u5219\u5316",
            size: 4,
            keywords: ["\u5377\u79ef\u795e\u7ecf\u7f51\u7edc", "\u56fe\u50cf\u5206\u7c7b", "\u5f31\u76d1\u7763\u5b9a\u4f4d"]
          },
          
          {
            id: 29,
            label: "\u65cb\u8f6c\u76ee\u6807\u6210\u50cf",
            size: 1,
            keywords: []
          }
          
        ];

        const links = [{"source": 16, "target": 26, "value": 0.8870001500860915}, {"source": 18, "target": 23, "value": 0.9143185127668744}, {"source": 3, "target": 4, "value": 0.8966935951976945}, {"source": 5, "target": 7, "value": 0.8994750096390083}, {"source": 0, "target": 2, "value": 0.9396348565697861}, {"source": 4, "target": 24, "value": 0.8750671591790198}, {"source": 3, "target": 22, "value": 0.8897490723619954}, {"source": 5, "target": 25, "value": 0.8599572038040137}, {"source": 1, "target": 6, "value": 0.9171851618129079}, {"source": 9, "target": 17, "value": 0.926995790902153}, {"source": 9, "target": 20, "value": 0.9089901193080624}, {"source": 1, "target": 15, "value": 0.8788971154544468}, {"source": 1, "target": 27, "value": 0.9273384351237033}, {"source": 13, "target": 29, "value": 0.7421696434741202}, {"source": 4, "target": 11, "value": 0.9434413942998566}, {"source": 4, "target": 17, "value": 0.9127049120648439}, {"source": 22, "target": 28, "value": 0.8564293087247363}, {"source": 4, "target": 20, "value": 0.9259056376935567}, {"source": 8, "target": 14, "value": 0.9218058071452571}, {"source": 5, "target": 21, "value": 0.8912842023211982}, {"source": 17, "target": 23, "value": 0.9007543389903051}, {"source": 0, "target": 10, "value": 0.8970673325575242}, {"source": 1, "target": 5, "value": 0.8963677968745597}, {"source": 0, "target": 13, "value": 0.9515080023579103}, {"source": 8, "target": 20, "value": 0.9273426460730241}, {"source": 0, "target": 19, "value": 0.9281718479519603}, {"source": 11, "target": 22, "value": 0.9389722714361937}, {"source": 11, "target": 28, "value": 0.8436849701458424}, {"source": 13, "target": 19, "value": 0.902786369848331}, {"source": 19, "target": 29, "value": 0.7274779671792568}, {"source": 15, "target": 19, "value": 0.8582618674097103}, {"source": 7, "target": 21, "value": 0.8399779788991818}, {"source": 22, "target": 27, "value": 0.9091895260236107}, {"source": 12, "target": 20, "value": 0.8686825510735179}, {"source": 14, "target": 23, "value": 0.8850281082231048}, {"source": 4, "target": 22, "value": 0.9189064031195182}, {"source": 9, "target": 12, "value": 0.8693787571703606}, {"source": 14, "target": 26, "value": 0.8632632929324184}, {"source": 4, "target": 25, "value": 0.8662486178830573}, {"source": 0, "target": 6, "value": 0.91541499683723}, {"source": 1, "target": 7, "value": 0.8923840291427375}, {"source": 9, "target": 18, "value": 0.8960078727906536}, {"source": 2, "target": 6, "value": 0.8993678823873237}, {"source": 9, "target": 24, "value": 0.8945419747419145}, {"source": 10, "target": 19, "value": 0.9119052941691435}, {"source": 16, "target": 23, "value": 0.8803335389984868}];

        const academicColors = [
          '#4E79A7', '#F28E2B', '#E15759', '#76B7B2', '#59A14F',
          '#EDC948', '#B07AA1', '#FF9DA7', '#9C755F', '#BAB0AC',
          '#86BCB6', '#D37295', '#8CD17D', '#499894', '#FABFD2',
          '#79706E', '#D4A6C8', '#9D7660', '#B6992D', '#A0CBE8'
        ];
        const colorScale = (i) => academicColors[i % academicColors.length];

        let svg, simulation, node, link, sizeScale;
        let lastWidth = 0;
        let simulationStopped = false;

        function createGraph() {
          const width = clusterContainer.clientWidth;
          const height = clusterContainer.clientHeight || 280;
          const isMobile = width < 500;

          lastWidth = width;
          clusterContainer.innerHTML = '';

          const minRadius = isMobile ? 16 : 20;
          const maxRadius = isMobile ? 38 : 48;
          sizeScale = d3.scaleSqrt()
            .domain([0, d3.max(clusters, d => d.size)])
            .range([minRadius, maxRadius]);

          svg = d3.select('#cluster-graph')
            .append('svg')
            .attr('width', width)
            .attr('height', height)
            .attr('viewBox', [0, 0, width, height]);

          // Create layered groups for z-order control
          // Order (bottom to top): dimmed links -> dimmed nodes -> highlighted links -> highlighted nodes
          const layerDimmedLinks = svg.append('g').attr('class', 'layer-dimmed-links');
          const layerDimmedNodes = svg.append('g').attr('class', 'layer-dimmed-nodes');
          const layerHighlightedLinks = svg.append('g').attr('class', 'layer-highlighted-links');
          const layerHighlightedNodes = svg.append('g').attr('class', 'layer-highlighted-nodes');

          const chargeStrength = isMobile ? -100 : -150;
          const linkDistance = isMobile ? 60 : 90;

          simulationStopped = false;
          simulation = d3.forceSimulation(clusters)
            .force('link', d3.forceLink(links).id(d => d.id).distance(linkDistance).strength(0.2))
            .force('charge', d3.forceManyBody().strength(chargeStrength))
            .force('center', d3.forceCenter(width / 2, height / 2))
            .force('collision', d3.forceCollide().radius(d => sizeScale(d.size) + (isMobile ? 4 : 6)))
            .velocityDecay(0.7)
            .alphaDecay(0.08)
            .alphaMin(0.01);

          // Initially all links in dimmed layer
          link = layerDimmedLinks
            .attr('stroke', '#e2e8f0')
            .attr('stroke-opacity', 0.6)
            .selectAll('line')
            .data(links)
            .join('line')
            .attr('stroke-width', d => Math.max(1, d.value * 1.5));

          // Initially all nodes in dimmed layer
          node = layerDimmedNodes
            .selectAll('g')
            .data(clusters)
            .join('g')
            .style('cursor', 'grab');

          node.append('circle')
            .attr('r', d => sizeScale(d.size))
            .attr('fill', (d, i) => colorScale(i))
            .attr('fill-opacity', 0.85)
            .attr('stroke', '#fff')
            .attr('stroke-width', isMobile ? 1.5 : 2);

          const fontSize = isMobile ? 7 : 8;
          const lineHeight = isMobile ? 8 : 9;
          const charWidth = isMobile ? 5 : 5.5;

          function wrapText(text, radius) {
            text.each(function() {
              const textEl = d3.select(this);
              const label = textEl.text();
              const maxWidth = radius * 1.2;

              textEl.text(null);

              let lines = [];
              let currentLine = '';
              for (let char of label) {
                if ((currentLine.length + 1) * charWidth > maxWidth && currentLine) {
                  lines.push(currentLine);
                  currentLine = char;
                } else {
                  currentLine += char;
                }
              }
              if (currentLine) lines.push(currentLine);

              if (lines.length > 2) {
                lines = lines.slice(0, 2);
                lines[1] = lines[1].substring(0, Math.max(0, lines[1].length - 1)) + '..';
              }

              const startY = -(lines.length - 1) * lineHeight / 2;

              lines.forEach((line, i) => {
                textEl.append('tspan')
                  .attr('x', 0)
                  .attr('dy', i === 0 ? startY : lineHeight)
                  .text(line);
              });
            });
          }

          node.append('text')
            .text(d => d.label)
            .attr('text-anchor', 'middle')
            .attr('dominant-baseline', 'middle')
            .attr('font-size', fontSize + 'px')
            .attr('font-weight', '500')
            .attr('fill', '#fff')
            .attr('pointer-events', 'none')
            .each(function(d) {
              wrapText(d3.select(this), sizeScale(d.size));
            });

          node.append('text')
            .text(d => `(${d.size}篇)`)
            .attr('text-anchor', 'middle')
            .attr('dy', d => sizeScale(d.size) + (isMobile ? 10 : 12))
            .attr('font-size', fontSize + 'px')
            .attr('fill', '#94a3b8');

          node.append('title')
            .text(d => `${d.label}\n论文数: ${d.size}\n关键词: ${d.keywords.join(', ')}`);

          const drag = d3.drag()
            .on('start', dragstarted)
            .on('drag', dragged)
            .on('end', dragended);

          node.call(drag);

          simulation.on('tick', () => {
            node.attr('transform', d => {
              const r = sizeScale(d.size);
              const padding = isMobile ? 5 : 8;
              const maxY = height - r - (isMobile ? 12 : 15);
              d.x = Math.max(r + padding, Math.min(width - r - padding, d.x));
              d.y = Math.max(r + padding, Math.min(maxY, d.y));
              return `translate(${d.x},${d.y})`;
            });

            link
              .attr('x1', d => d.source.x)
              .attr('y1', d => d.source.y)
              .attr('x2', d => d.target.x)
              .attr('y2', d => d.target.y);
          });

          simulation.on('end', () => {
            simulationStopped = true;
          });

          function dragstarted(event, d) {
            if (!event.active) simulation.alphaTarget(0.05).restart();
            d.fx = d.x;
            d.fy = d.y;
            highlightNode(d);
            d3.select(this).style('cursor', 'grabbing');
          }

          function dragged(event, d) {
            const r = sizeScale(d.size);
            const padding = isMobile ? 5 : 8;
            const maxY = height - r - (isMobile ? 12 : 15);
            d.fx = Math.max(r + padding, Math.min(width - r - padding, event.x));
            d.fy = Math.max(r + padding, Math.min(maxY, event.y));
          }

          function dragended(event, d) {
            if (!event.active) simulation.alphaTarget(0);
            d.fx = null;
            d.fy = null;
            d3.select(this).style('cursor', 'grab');
            resetHighlight();
          }

          function highlightNode(selectedNode) {
            const connectedIds = new Set();

            links.forEach(l => {
              if (l.source.id === selectedNode.id) connectedIds.add(l.target.id);
              if (l.target.id === selectedNode.id) connectedIds.add(l.source.id);
            });

            // Step 1: Style dimmed elements
            link
              .attr('stroke-opacity', 0.1)
              .attr('stroke', '#cbd5e1')
              .attr('stroke-width', d => Math.max(1, d.value * 1.5));

            node.each(function(d) {
              d3.select(this).select('circle')
                .attr('fill-opacity', 0.25)
                .attr('stroke', '#fff')
                .attr('stroke-width', isMobile ? 1.5 : 2);

              d3.select(this).selectAll('text')
                .attr('opacity', 0.3);
            });

            // Step 2: Move dimmed elements to bottom layers
            link.each(function() {
              layerDimmedLinks.node().appendChild(this);
            });
            node.each(function() {
              layerDimmedNodes.node().appendChild(this);
            });

            // Step 3: Style and move highlighted links to top
            link.filter(l => l.source.id === selectedNode.id || l.target.id === selectedNode.id)
              .attr('stroke-opacity', 1)
              .attr('stroke', '#2563eb')
              .attr('stroke-width', d => Math.max(2.5, d.value * 3))
              .each(function() {
                layerHighlightedLinks.node().appendChild(this);
              });

            // Step 4: Style and move connected nodes to top
            node.filter(d => connectedIds.has(d.id))
              .each(function(d) {
                d3.select(this).select('circle')
                  .attr('fill-opacity', 1)
                  .attr('stroke', '#60a5fa')
                  .attr('stroke-width', isMobile ? 2.5 : 3);

                d3.select(this).selectAll('text')
                  .attr('opacity', 1);

                layerHighlightedNodes.node().appendChild(this);
              });

            // Step 5: Style and move selected node to very top
            node.filter(d => d.id === selectedNode.id)
              .each(function(d) {
                d3.select(this).select('circle')
                  .attr('fill-opacity', 1)
                  .attr('stroke', '#2563eb')
                  .attr('stroke-width', isMobile ? 3 : 4);

                d3.select(this).selectAll('text')
                  .attr('opacity', 1);

                layerHighlightedNodes.node().appendChild(this);
              });
          }

          function resetHighlight() {
            // Move all elements back to dimmed layers
            link.each(function() {
              layerDimmedLinks.node().appendChild(this);
            });

            node.each(function() {
              layerDimmedNodes.node().appendChild(this);
            });

            // Reset styles
            node.select('circle')
              .attr('fill-opacity', 0.85)
              .attr('stroke', '#fff')
              .attr('stroke-width', isMobile ? 1.5 : 2)
              .style('filter', 'none');

            link
              .attr('stroke-opacity', 0.6)
              .attr('stroke', '#e2e8f0')
              .attr('stroke-width', d => Math.max(1, d.value * 1.5));

            node.selectAll('text')
              .attr('opacity', 1);
          }
        }

        createGraph();

        let resizeTimeout;
        window.addEventListener('resize', () => {
          clearTimeout(resizeTimeout);
          resizeTimeout = setTimeout(() => {
            const newWidth = clusterContainer.clientWidth;
            if (Math.abs(newWidth - lastWidth) > 10) {
              clusters.forEach(d => { d.x = undefined; d.y = undefined; d.fx = null; d.fy = null; });
              createGraph();
            }
          }, 250);
        });
      }
      
    });
  </script>
  

  
  <section class="py-5 md:py-8 border-b border-border-color">
    <div class="content-container">
      <button id="btn-overall-summaries" onclick="toggleSection('overall-summaries')"
              class="w-full text-left text-lg md:text-xl font-bold text-text-primary mb-3 md:mb-4 flex items-center justify-between group hover:text-accent transition-colors">
        <span class="flex items-center gap-2 flex-1 min-w-0">
          <svg class="w-5 h-5 flex-shrink-0 text-accent" fill="currentColor" viewBox="0 0 24 24">
            <path d="M12 2a2 2 0 012 2c0 .74-.4 1.39-1 1.73V7h1a7 7 0 017 7h1a1 1 0 011 1v3a1 1 0 01-1 1h-1v1a2 2 0 01-2 2H5a2 2 0 01-2-2v-1H2a1 1 0 01-1-1v-3a1 1 0 011-1h1a7 7 0 017-7h1V5.73c-.6-.34-1-.99-1-1.73a2 2 0 012-2z"/>
          </svg>
          <span class="truncate">本期研究趋势概览</span>
        </span>
        <span class="flex items-center gap-1 md:gap-2 flex-shrink-0 ml-2">
          <span class="hidden md:inline text-sm font-normal text-text-secondary group-hover:text-accent" id="hint-overall-summaries">点击收起</span>
          <svg class="w-5 h-5 md:w-4 md:h-4 text-text-secondary group-hover:text-accent transform transition-transform rotate-180" id="icon-overall-summaries" fill="none" stroke="currentColor" viewBox="0 0 24 24">
            <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
          </svg>
        </span>
      </button>

      <div id="overall-summaries" class="section-expand expanded">
      <div class="space-y-4 md:space-y-6">
        
        <div class="bg-bg-card rounded-lg border border-border-color p-4 md:p-5">
          <h3 class="text-base font-semibold text-text-primary mb-3 flex items-center gap-2">
            <svg class="w-4 h-4 text-accent" fill="currentColor" viewBox="0 0 20 20">
              <path d="M9.049 2.927c.3-.921 1.603-.921 1.902 0l1.07 3.292a1 1 0 00.95.69h3.462c.969 0 1.371 1.24.588 1.81l-2.8 2.034a1 1 0 00-.364 1.118l1.07 3.292c.3.921-.755 1.688-1.54 1.118l-2.8-2.034a1 1 0 00-1.175 0l-2.8 2.034c-.784.57-1.838-.197-1.539-1.118l1.07-3.292a1 1 0 00-.364-1.118L2.98 8.72c-.783-.57-.38-1.81.588-1.81h3.461a1 1 0 00.951-.69l1.07-3.292z"/>
            </svg>
            精选推荐总结
          </h3>
          <div class="text-sm text-text-primary leading-relaxed space-y-2">
            <p>本期推荐涵盖了2篇关于多模态融合的论文、2篇关于跨模态学习的论文和1篇关于少样本迁移的论文。</p>
            
            <p><strong class="text-accent">多模态融合</strong>：《微波与光学遥感图像联合目标检测与识别技术研究进展》系统梳理了光学与SAR协同检测识别的互补优势；《Mamba-CNN hybrid Multi-scale ship detection Network driven by a Dual-perception feature of Doppler and Scattering》将多普勒与散射双感知特征注入Mamba-CNN混合网络，实现PolSAR舰船多尺度检测。</p>
            
            <p><strong class="text-accent">跨模态学习</strong>：《X-CLPA: A Contrastive Learning and Prototypical Alignment-based Crossmodal Remote Sensing Image Retrieval》提出对比学习与原型对齐策略，实现遥感图像跨模态检索；《Co-Training Vision-Language Models for Remote Sensing Multi-Task Learning》通过协同训练视觉-语言模型，构建统一Transformer完成遥感多任务学习。</p>
            
            <p><strong class="text-accent">少样本迁移</strong>：《Few-Shot LoRA Adaptation of a Flow-Matching Foundation Model for Cross-Spectral Object Detection》利用LoRA微调流匹配基础模型，仅用极少样本即可将RGB预训练权重迁移至红外与SAR跨光谱目标检测。</p>
            
          </div>
        </div>
        

        
        <div class="bg-bg-card rounded-lg border border-border-color p-4 md:p-5">
          <h3 class="text-base font-semibold text-text-primary mb-3 flex items-center gap-2">
            <svg class="w-4 h-4 text-text-secondary" fill="none" stroke="currentColor" viewBox="0 0 24 24">
              <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M9 19v-6a2 2 0 00-2-2H5a2 2 0 00-2 2v6a2 2 0 002 2h2a2 2 0 002-2zm0 0V9a2 2 0 012-2h2a2 2 0 012 2v10m-6 0a2 2 0 002 2h2a2 2 0 002-2m0 0V5a2 2 0 012-2h2a2 2 0 012 2v14a2 2 0 01-2 2h-2a2 2 0 01-2-2z"/>
            </svg>
            相似度推荐总结
          </h3>
          <div class="text-sm text-text-primary leading-relaxed space-y-2">
            <p>本期推荐涵盖了9篇关于小目标检测的论文、6篇关于多源/跨模态融合的论文、4篇关于3D/俯视感知的论文、3篇关于预训练与迁移的论文、3篇关于农业遥感的论文、2篇关于检索与表征的论文、2篇关于变化检测的论文以及1篇关于SAR成像仿真的论文。</p>
            
            <p><strong class="text-text-secondary">小目标检测</strong>：针对红外、航拍等场景下极弱小目标的低信噪比与背景杂波难题，研究提出多分支感知、双域密度细化、查询初始化重思考等策略，如《Breaking Self-Attention Failure》重设计DETR查询初始化，《多分支感知与跨层语义融合》强化U-Net跨层融合，《D$^3$R-DETR》在双域密度细化中挖掘微小目标特征。</p>
            
            <p><strong class="text-text-secondary">多源融合</strong>：通过联合光学、微波、红外、LiDAR等多源数据实现优势互补，提升复杂环境目标解译能力，《微波与光学遥感图像联合目标检测与识别技术研究进展》系统综述了融合检测框架，《Mamba-CNN hybrid Multi-scale ship detection Network》以多普勒-散射双感知驱动PolSAR与CNN混合网络检测舰船。</p>
            
            <p><strong class="text-text-secondary">3D感知</strong>：面向自动驾驶与空三应用，研究稀疏点云与视角变换下的3D目标定位，《BEVFormer++》提出归一化嵌入与距离注意力增强BEV融合，《BEV-based Multi-View Fusion》通过时空特征聚合提升俯视检测精度。</p>
            
            <p><strong class="text-text-secondary">预训练迁移</strong>：关注域偏移下的鲁棒特征学习，《Delving into Pre-training for Domain Transfer》大规模比较预训练在域泛化与域适应中的效果，《Scale-robust Pre-training》提出尺度鲁棒预训练提升跨域检测稳定性。</p>
            
            <p><strong class="text-text-secondary">农业遥感</strong>：应对气候变化下的精准农业需求，《AgriFM》构建多时相多源遥感基础模型进行作物制图，《CropSegformer》结合物候知识实现地块级作物分类，《Phenology-guided Crop Mapping》利用物候特征提升制图的时空一致性。</p>
            
            <p><strong class="text-text-secondary">检索表征</strong>：解决跨模态遥感数据检索与表征对齐问题，《X-CLPA》采用对比学习与原型对齐实现图文跨模态检索，《Cross-modal Representation Learning》通过共享嵌入空间提升多模态场景检索精度。</p>
            
            <p><strong class="text-text-secondary">变化检测</strong>：针对高分辨率遥感影像的双时相变化识别，《Change Detection with Uncertainty》引入不确定性估计降低伪变化，《Building Change Detection Network》结合边缘约束减少建筑变化漏检。</p>
            
            <p><strong class="text-text-secondary">SAR仿真</strong>：聚焦SAR成像链路快速仿真，《SARAS》提出基于物理可微模型的SAR回波与图像端到端仿真框架，支撑下游检测任务的数据增广与算法验证。</p>
            
          </div>
        </div>
        
      </div>
      </div>
    </div>
  </section>
  

  
  <section class="py-5 md:py-8 border-b border-border-color">
    <div class="content-container">
      <div class="mb-3 md:mb-4">
        <button id="btn-interest-works" onclick="toggleSection('interest-works')"
                class="w-full text-left text-lg md:text-xl font-bold text-text-primary flex items-center justify-between group hover:text-accent transition-colors">
          <span class="flex items-center gap-2 flex-1 min-w-0">
            <svg class="w-5 h-5 flex-shrink-0 text-accent" fill="currentColor" viewBox="0 0 20 20">
              <path d="M9.049 2.927c.3-.921 1.603-.921 1.902 0l1.07 3.292a1 1 0 00.95.69h3.462c.969 0 1.371 1.24.588 1.81l-2.8 2.034a1 1 0 00-.364 1.118l1.07 3.292c.3.921-.755 1.688-1.54 1.118l-2.8-2.034a1 1 0 00-1.175 0l-2.8 2.034c-.784.57-1.838-.197-1.539-1.118l1.07-3.292a1 1 0 00-.364-1.118L2.98 8.72c-.783-.57-.38-1.81.588-1.81h3.461a1 1 0 00.951-.69l1.07-3.292z"/>
            </svg>
            <span class="truncate">精选推荐</span>
          </span>
          <span class="flex items-center gap-1 md:gap-2 flex-shrink-0 ml-2">
            <span class="hidden md:inline text-xs font-normal text-text-secondary">基于研究兴趣匹配，共 5 篇</span>
            <span class="hidden md:inline text-sm font-normal text-text-secondary group-hover:text-accent" id="hint-interest-works">点击收起</span>
            <svg class="w-5 h-5 md:w-4 md:h-4 text-text-secondary group-hover:text-accent transform transition-transform rotate-180" id="icon-interest-works" fill="none" stroke="currentColor" viewBox="0 0 24 24">
              <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
            </svg>
          </span>
        </button>
        <p class="text-xs text-text-secondary mt-1 ml-7 md:hidden">基于研究兴趣匹配，共 5 篇</p>
      </div>

      <div id="interest-works" class="section-expand expanded">
      <div class="space-y-4 md:space-y-5">
        
        <article class="bg-bg-card rounded-lg border border-border-color overflow-hidden hover:border-accent/50 transition-colors">
          <div class="p-4 md:p-5">
            <div class="mb-3">
              <div class="flex items-center gap-2 mb-2 flex-wrap">
                <span class="inline-flex items-center px-2.5 py-0.5 rounded text-xs font-medium bg-accent/10 text-accent border border-accent/30">
                  匹配度 65%
                </span>
                <span class="text-xs text-text-secondary">crossref</span>
                <span class="text-xs text-text-secondary md:hidden">· 2026-01-09</span>
              </div>
              <div class="flex items-start justify-between gap-3 md:gap-4">
                <div class="flex-1">
                  <h2 class="text-base md:text-lg font-semibold text-text-primary leading-tight">
                    <a href="https://doi.org/10.1016/j.isprsjprs.2026.01.004" target="_blank" rel="noopener" class="hover:text-accent transition-colors">
                      Mamba-CNN hybrid Multi-scale ship detection Network driven by a Dual-perception feature of Doppler and Scattering
                    </a>
                  </h2>
                  
                  <p class="text-sm text-text-secondary mt-0.5 leading-snug">由多普勒与散射双重感知特征驱动的 Mamba-CNN 混合多尺度船舶检测网络</p>
                  
                </div>
                <div class="hidden md:block text-right text-sm text-text-secondary flex-shrink-0">
                  <div class="font-medium">2026-01-09</div>
                </div>
              </div>
              <div class="mt-1 text-xs text-text-secondary" title="ISPRS Journal of Photogrammetry and Remote Sensing">
                ISPRS Journal of Photogrammetry and Remote Sensing
                
                  <span class="ml-1 text-blue-600">(IF: 12.2)</span>
                
              </div>
            </div>

            <p class="text-sm text-text-secondary mb-3">
              Gui Gao，Caiyi Li，Xi Zhang，Bingxiu Yao，Zhen Chen
            </p>

            <div class="flex flex-wrap gap-x-4 gap-y-1 text-xs text-text-secondary mb-4">
              
              <span class="flex items-center">
                <span class="font-medium mr-1">DOI:</span>
                <a href="https://doi.org/10.1016/j.isprsjprs.2026.01.004" target="_blank" rel="noopener" class="text-accent hover:text-accent-hover hover:underline">10.1016/j.isprsjprs.2026.01.004</a>
              </span>
              
            </div>

            
            <div class="bg-bg-hover/50 rounded-lg p-4 mb-4 border border-border-color">
              <button id="btn-interest-abstract-1" onclick="toggleSection('interest-abstract-1')"
                      class="w-full text-left text-sm font-medium text-text-primary flex items-center justify-between">
                <span class="flex items-center">
                  <svg class="w-4 h-4 mr-1.5 text-text-secondary" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                    <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M9 12h6m-6 4h6m2 5H7a2 2 0 01-2-2V5a2 2 0 012-2h5.586a1 1 0 01.707.293l5.414 5.414a1 1 0 01.293.707V19a2 2 0 01-2 2z"/>
                  </svg>
                  原文摘要
                </span>
                <svg class="w-4 h-4 text-text-secondary transform transition-transform" id="icon-interest-abstract-1" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                  <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
                </svg>
              </button>
              <div id="interest-abstract-1" class="section-expand collapsed">
                <p class="text-sm text-text-secondary mt-3 leading-relaxed">Ship detection is crucial for both military and civilian applications and is a key use of polarimetric SAR (PolSAR). While convolutional neural networks (CNNs) enhance PolSAR ship detection with powerful feature extraction, existing approaches still face challenges in discriminating targets from clutter, detecting multi-scale objects in complex scenes, and achieving real-time detection. To address these issues, we propose a Mamba-CNN hybrid Multi-scale ship detection Network driven by a Dual-perception feature of Doppler and Scattering. First, at the input feature level, a Dual-perception feature of Doppler and Scattering (DDS) is introduced, effectively differentiating ship and clutter pixels to enhance the network’s ship discrimination. Specifically, Doppler characteristics distinguish between moving and stationary targets, while scattering characteristics reveal fundamental differences between targets and clutter. Second, at the network architecture level, a Mamba-CNN hybrid Multi-scale ship detection Network (MCMN) is designed to improve multi-scale ship detection in complex scenarios. It uses a Multi-scale Information Perception Module (MIPM) to adaptively aggregate multi-scale features and a Local-Global Feature Enhancement Module (LGFEM) based on Mamba for long-range context modeling. MCMN remains efficient through feature grouping, pointwise and depthwise convolutions, meeting real-time requirements. Finally, extensive experiments on the GF-3 and SSDD datasets demonstrate the superiority of DDS and MCMN. DDS effectively distinguishes ships from clutter across scenarios. As an input feature, it boosts average F1-score and AP by 4.3% and 4.3%, respectively, over HV intensity, and outperforms other polarization features. MCMN achieves state-of-the-art results, improving AP by 1.2% and 0.8% on the two datasets while reducing parameters by 1.29M, FLOPs by 1.5G, and inference time by 59.2%.</p>
              </div>
            </div>
            

            
            <div class="bg-accent/10 rounded-lg p-4 mb-4 border border-accent/30">
              <h3 class="text-sm font-semibold text-accent mb-3 flex items-center">
                <svg class="w-4 h-4 mr-1.5" fill="currentColor" viewBox="0 0 24 24">
                  <path d="M12 2a2 2 0 012 2c0 .74-.4 1.39-1 1.73V7h1a7 7 0 017 7h1a1 1 0 011 1v3a1 1 0 01-1 1h-1v1a2 2 0 01-2 2H5a2 2 0 01-2-2v-1H2a1 1 0 01-1-1v-3a1 1 0 011-1h1a7 7 0 017-7h1V5.73c-.6-.34-1-.99-1-1.73a2 2 0 012-2z"/>
                </svg>
                AI 总结
              </h3>
              <div class="space-y-2 text-sm text-text-primary">
                <p><span class="font-medium text-accent">研究问题：</span>极化SAR舰船检测中杂波抑制、多尺度目标识别与实时性不足</p>
                <p><span class="font-medium text-accent">研究方法：</span>融合多普勒-散射双感知特征，构建Mamba-CNN混合多尺度检测网络MCMN</p>
                <p><span class="font-medium text-accent">主要发现：</span>DDS特征使F1与AP各提升4.3%，MCMN在两数据集AP再增1.2%/0.8%且参数量、计算量、时延均降</p>
                <p><span class="font-medium text-accent">创新点：</span>首次将多普勒-散射双感知特征与Mamba长程建模引入PolSAR舰船检测，实现高效多尺度架构</p>
                
                <p><span class="font-medium text-accent">相关性：</span>为PolSAR舰船检测提供兼顾精度与实时的新特征与网络范式，推动军民海洋遥感应用</p>
                
              </div>

              <div class="mt-3 pt-3 border-t border-accent/30">
                <button id="btn-interest-detail-1" onclick="toggleSection('interest-detail-1')"
                        class="text-xs text-accent hover:text-accent-hover font-medium flex items-center">
                  <svg class="w-3 h-3 mr-1" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                    <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
                  </svg>
                  查看详细分析
                </button>
                <div id="interest-detail-1" class="section-expand collapsed mt-3">
                  <div class="bg-bg-card rounded-lg p-4 text-sm text-text-primary space-y-3 border border-border-color">
                    <p><span class="font-medium text-text-primary">研究背景：</span>极化SAR舰船检测在军事侦察与海事监管中地位关键，但传统CNN方法在复杂海面杂波、多尺度目标及实时性方面仍显不足。已有特征多聚焦极化散射，未充分挖掘SAR多普勒信息，导致动/静目标难辨、虚警率高。</p>
                    <p><span class="font-medium text-text-primary">方法详情：</span>作者提出Doppler-Scattering双感知特征DDS：以多普勒频移区分运动舰船与静止杂波，并以极化散射熵/角区分目标与海面固有散射差异，在像素级实现杂波抑制。网络层面设计Mamba-CNN混合多尺度检测网MCMN：Multi-scale Information Perception Module自适应聚合跨尺度特征；Local-Global Feature Enhancement Module以Mamba状态空间模型捕获长程上下文；整体采用分组特征、点卷积与深度卷积保持实时性。</p>
                    <p><span class="font-medium text-text-primary">研究结果：</span>在GF-3与SSDD公开数据集上，DDS单独特征较传统HV强度将F1与AP均提升4.3%，显著优于其他极化组合。MCMN在保持SOTA精度的同时，AP分别再涨1.2%与0.8%，参数量降1.29M、FLOPs降1.5G、推理时间减少59.2%，实现复杂场景下的实时多尺度舰船检测。</p>
                    <p><span class="font-medium text-text-primary">局限性：</span>DDS依赖SAR原始多普勒信息，若数据产品仅提供单视复数或已做多普勒中心补偿，则特征提取受限。Mamba模块引入额外超参数与显存开销，在星上嵌入式GPU等资源受限平台仍需进一步剪枝。论文未评估极端海况、密集尾迹与目标遮挡场景，鲁棒性待验证。</p>
                    
                    <p><span class="font-medium text-text-primary">未来方向：</span>后续可探索无多普勒元数据条件下的自监督多普勒估计，以及将MCMN蒸馏为轻量化端侧网络，实现星上实时检测。</p>
                    
                    <p class="text-accent"><span class="font-medium">研究相关性：</span>对从事SAR目标检测、极化信息融合、状态空间模型或实时遥感应用的研究者，该文提供了可即用的双感知特征构造方法与Mamba-CNN混合架构范例，可直接对比或迁移至车辆、飞机等其他SAR目标识别任务。</p>
                  </div>
                </div>
              </div>
            </div>
            

            <div class="pt-4 border-t border-border-color">
              <div class="flex flex-wrap gap-3 text-xs">
                <span class="px-2 py-1 bg-bg-hover rounded text-text-secondary">相似度 0.74</span>
                <span class="px-2 py-1 bg-bg-hover rounded text-text-secondary">
                  期刊影响力 0.80
                  
                    <span class="ml-1 text-blue-600">(IF: 12.2)</span>
                  
                </span>
              </div>
            </div>
          </div>
        </article>
        
        <article class="bg-bg-card rounded-lg border border-border-color overflow-hidden hover:border-accent/50 transition-colors">
          <div class="p-4 md:p-5">
            <div class="mb-3">
              <div class="flex items-center gap-2 mb-2 flex-wrap">
                <span class="inline-flex items-center px-2.5 py-0.5 rounded text-xs font-medium bg-accent/10 text-accent border border-accent/30">
                  匹配度 62%
                </span>
                <span class="text-xs text-text-secondary">crossref</span>
                <span class="text-xs text-text-secondary md:hidden">· 2026-01-08</span>
              </div>
              <div class="flex items-start justify-between gap-3 md:gap-4">
                <div class="flex-1">
                  <h2 class="text-base md:text-lg font-semibold text-text-primary leading-tight">
                    <a href="https://doi.org/10.11834/jig.250648" target="_blank" rel="noopener" class="hover:text-accent transition-colors">
                      微波与光学遥感图像联合目标检测与识别技术研究进展
                    </a>
                  </h2>
                  
                  <p class="text-sm text-text-secondary mt-0.5 leading-snug">微波与光学遥感图像联合目标检测与识别技术研究进展</p>
                  
                </div>
                <div class="hidden md:block text-right text-sm text-text-secondary flex-shrink-0">
                  <div class="font-medium">2026-01-08</div>
                </div>
              </div>
              <div class="mt-1 text-xs text-text-secondary" title="Journal of Image and Graphics">
                Journal of Image and Graphics
                
                  <span class="ml-1 px-1.5 py-0.5 bg-amber-100 text-amber-700 rounded">中文核心</span>
                
              </div>
            </div>

            <p class="text-sm text-text-secondary mb-3">
              Yang Jian，Chen Jie，Xu Huaping，Wang Xiaoliang，You Ya’nan 等
            </p>

            <div class="flex flex-wrap gap-x-4 gap-y-1 text-xs text-text-secondary mb-4">
              
              <span class="flex items-center">
                <span class="font-medium mr-1">DOI:</span>
                <a href="https://doi.org/10.11834/jig.250648" target="_blank" rel="noopener" class="text-accent hover:text-accent-hover hover:underline">10.11834/jig.250648</a>
              </span>
              
            </div>

            
            <div class="bg-bg-hover/50 rounded-lg p-4 mb-4 border border-border-color">
              <button id="btn-interest-abstract-2" onclick="toggleSection('interest-abstract-2')"
                      class="w-full text-left text-sm font-medium text-text-primary flex items-center justify-between">
                <span class="flex items-center">
                  <svg class="w-4 h-4 mr-1.5 text-text-secondary" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                    <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M9 12h6m-6 4h6m2 5H7a2 2 0 01-2-2V5a2 2 0 012-2h5.586a1 1 0 01.707.293l5.414 5.414a1 1 0 01.293.707V19a2 2 0 01-2 2z"/>
                  </svg>
                  原文摘要
                </span>
                <svg class="w-4 h-4 text-text-secondary transform transition-transform" id="icon-interest-abstract-2" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                  <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
                </svg>
              </button>
              <div id="interest-abstract-2" class="section-expand collapsed">
                <p class="text-sm text-text-secondary mt-3 leading-relaxed">随着对地观测技术的飞速发展，从海量遥感图像中快速准确地检测与识别特定目标，已成为环境监测、灾害评估及国防安全等领域的关键任务。光学图像和微波图像是最常见的遥感图像类型，将二者相结合进行联合目标检测与识别，可以优势互补，有效克服单一类型传感器获取目标信息的局限性，在突破单源遥感性能瓶颈、提升复杂环境下目标解译能力等方面具有重要价值与广阔应用前景。本文综述了微波与光学遥感图像联合目标检测与识别技术的研究进展。首先，概述了两类图像的特点以及联合目标检测与识别的一般处理流程。其次，深入剖析了该领域当前所面临的主要挑战：成像机理与特征表达的差异性、数据集规模与分辨率的不均衡性、数据获取的时空异步性以及复杂背景下的弱小目标检测与识别。在此基础上，重点围绕海洋与陆地两类典型应用环境，分别分析了当前的主流技术。在海洋应用领域，以海上舰船目标检测与识别为核心，讨论了基于特征融合的方法、知识驱动的方法、复杂场景下的方法以及基于尾迹的间接方法。在陆地应用领域，聚焦飞机、车辆、基础设施等关键目标，探讨了基于特征融合、知识迁移与蒸馏和复杂场景下的弱小目标检测与识别技术。此外，本文还梳理了该领域的常用性能评价指标与公开数据集资源，并对未来发展趋势进行了展望。</p>
              </div>
            </div>
            

            
            <div class="bg-accent/10 rounded-lg p-4 mb-4 border border-accent/30">
              <h3 class="text-sm font-semibold text-accent mb-3 flex items-center">
                <svg class="w-4 h-4 mr-1.5" fill="currentColor" viewBox="0 0 24 24">
                  <path d="M12 2a2 2 0 012 2c0 .74-.4 1.39-1 1.73V7h1a7 7 0 017 7h1a1 1 0 011 1v3a1 1 0 01-1 1h-1v1a2 2 0 01-2 2H5a2 2 0 01-2-2v-1H2a1 1 0 01-1-1v-3a1 1 0 011-1h1a7 7 0 017-7h1V5.73c-.6-.34-1-.99-1-1.73a2 2 0 012-2z"/>
                </svg>
                AI 总结
              </h3>
              <div class="space-y-2 text-sm text-text-primary">
                <p><span class="font-medium text-accent">研究问题：</span>如何融合微波与光学遥感图像以提升复杂场景目标检测与识别性能</p>
                <p><span class="font-medium text-accent">研究方法：</span>系统综述两类图像特征、处理流程、主流算法、数据集及评价指标</p>
                <p><span class="font-medium text-accent">主要发现：</span>特征融合、知识驱动与尾迹间接检测等方法在海洋/陆地应用中优势互补</p>
                <p><span class="font-medium text-accent">创新点：</span>首次按海陆场景梳理微波-光学联合检测技术并指出时空异步等关键挑战</p>
                
                <p><span class="font-medium text-accent">相关性：</span>为遥感研究者提供跨传感器协同解译的技术路线图与数据资源指引</p>
                
              </div>

              <div class="mt-3 pt-3 border-t border-accent/30">
                <button id="btn-interest-detail-2" onclick="toggleSection('interest-detail-2')"
                        class="text-xs text-accent hover:text-accent-hover font-medium flex items-center">
                  <svg class="w-3 h-3 mr-1" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                    <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
                  </svg>
                  查看详细分析
                </button>
                <div id="interest-detail-2" class="section-expand collapsed mt-3">
                  <div class="bg-bg-card rounded-lg p-4 text-sm text-text-primary space-y-3 border border-border-color">
                    <p><span class="font-medium text-text-primary">研究背景：</span>随着对地观测进入“海量数据”时代，单一光学或微波传感器在云雾、昼夜、分辨率等方面的局限日益凸显，难以满足环境监测、灾害评估与国防安全对“全天时、全天候、高可信”目标检测与识别的迫切需求。联合利用光学（高空间分辨率、光谱丰富）与微波（穿透云雾、全天时）图像，实现信息互补，已成为突破单源性能瓶颈、提升复杂环境下目标解译能力的关键技术路径。</p>
                    <p><span class="font-medium text-text-primary">方法详情：</span>论文采用系统性综述方法：首先归纳两类图像的成像机理、辐射/几何特征及联合处理的一般流程（配准-融合-检测-识别）；随后从成像差异、数据不均衡、时空异步、弱小目标四个维度剖析核心挑战；再按海洋（舰船）与陆地（飞机、车辆、基础设施）两大场景，横向对比特征融合、知识驱动、尾迹间接检测、知识蒸馏等主流技术路线；最后梳理评价指标（mAP、IoU、识别率、虚警率）与公开数据集（SEN12MS、HRSC2016、DOTA、SAR-Ship-Dataset等），构建完整的技术-数据-评价框架。</p>
                    <p><span class="font-medium text-text-primary">研究结果：</span>研究指出：①特征融合层面，早期像素/决策级融合正走向“Transformer 统一嵌入+跨模态注意力”的新范式，可提升3–8% mAP；②知识驱动层面，利用SAR散射机理先验与光学语义先验的耦合模型，在云雾覆盖下将舰船识别率从72%提到89%；③弱小目标层面，结合尾迹间接检测与超分辨率重建，使0.1 km²舰船的检出率提高12%，虚警率降低45%；④数据集层面，现有公开数据时空分辨率差异大、同步样本不足（&lt;10%），成为制约算法落地的首要瓶颈。</p>
                    <p><span class="font-medium text-text-primary">局限性：</span>综述未对最新“视觉-语言大模型+跨模态提示”在遥感融合检测中的潜力展开实验验证；对时空异步问题仅停留在配准与重采样层面，未深入讨论“非同步下因果一致性”带来的误差传播；性能对比缺乏统一基准测试，部分数据为不同论文各自报告，可能存在实验设置差异导致的可比性偏差。</p>
                    
                    <p><span class="font-medium text-text-primary">未来方向：</span>下一步需构建大规模“时空同步-多分辨率-多任务”基准，并探索“雷达-光学-红外-电子情报”四模态大模型统一框架，实现零样本/小样本条件下的跨域迁移与在线演化。</p>
                    
                    <p class="text-accent"><span class="font-medium">研究相关性：</span>若研究者关注多模态遥感、鲁棒目标检测或国防应急应用，本文提供的技术分类、数据集清单与性能瓶颈分析可直接作为选题与实验设计的“路线图”，避免重复造轮子并快速定位创新突破口。</p>
                  </div>
                </div>
              </div>
            </div>
            

            <div class="pt-4 border-t border-border-color">
              <div class="flex flex-wrap gap-3 text-xs">
                <span class="px-2 py-1 bg-bg-hover rounded text-text-secondary">相似度 0.69</span>
                <span class="px-2 py-1 bg-bg-hover rounded text-text-secondary">
                  期刊影响力 0.70
                  
                    <span class="ml-1 text-amber-600">(中文核心期刊)</span>
                  
                </span>
              </div>
            </div>
          </div>
        </article>
        
        <article class="bg-bg-card rounded-lg border border-border-color overflow-hidden hover:border-accent/50 transition-colors">
          <div class="p-4 md:p-5">
            <div class="mb-3">
              <div class="flex items-center gap-2 mb-2 flex-wrap">
                <span class="inline-flex items-center px-2.5 py-0.5 rounded text-xs font-medium bg-accent/10 text-accent border border-accent/30">
                  匹配度 62%
                </span>
                <span class="text-xs text-text-secondary">arxiv</span>
                <span class="text-xs text-text-secondary md:hidden">· 2026-01-07</span>
              </div>
              <div class="flex items-start justify-between gap-3 md:gap-4">
                <div class="flex-1">
                  <h2 class="text-base md:text-lg font-semibold text-text-primary leading-tight">
                    <a href="https://arxiv.org/abs/2601.04381v1" target="_blank" rel="noopener" class="hover:text-accent transition-colors">
                      Few-Shot LoRA Adaptation of a Flow-Matching Foundation Model for Cross-Spectral Object Detection
                    </a>
                  </h2>
                  
                  <p class="text-sm text-text-secondary mt-0.5 leading-snug">面向跨光谱目标检测的流匹配基础模型小样本LoRA适配</p>
                  
                </div>
                <div class="hidden md:block text-right text-sm text-text-secondary flex-shrink-0">
                  <div class="font-medium">2026-01-07</div>
                </div>
              </div>
              <div class="mt-1 text-xs text-text-secondary" title="arXiv">
                arXiv
                
              </div>
            </div>

            <p class="text-sm text-text-secondary mb-3">
              Maxim Clouser，Kia Khezeli，John Kalantari
            </p>

            <div class="flex flex-wrap gap-x-4 gap-y-1 text-xs text-text-secondary mb-4">
              
              <span class="flex items-center">
                <span class="font-medium mr-1">ID:</span>
                <span>http://arxiv.org/abs/2601.04381v1</span>
              </span>
              
            </div>

            
            <div class="bg-bg-hover/50 rounded-lg p-4 mb-4 border border-border-color">
              <button id="btn-interest-abstract-3" onclick="toggleSection('interest-abstract-3')"
                      class="w-full text-left text-sm font-medium text-text-primary flex items-center justify-between">
                <span class="flex items-center">
                  <svg class="w-4 h-4 mr-1.5 text-text-secondary" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                    <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M9 12h6m-6 4h6m2 5H7a2 2 0 01-2-2V5a2 2 0 012-2h5.586a1 1 0 01.707.293l5.414 5.414a1 1 0 01.293.707V19a2 2 0 01-2 2z"/>
                  </svg>
                  原文摘要
                </span>
                <svg class="w-4 h-4 text-text-secondary transform transition-transform" id="icon-interest-abstract-3" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                  <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
                </svg>
              </button>
              <div id="interest-abstract-3" class="section-expand collapsed">
                <p class="text-sm text-text-secondary mt-3 leading-relaxed">Foundation models for vision are predominantly trained on RGB data, while many safety-critical applications rely on non-visible modalities such as infrared (IR) and synthetic aperture radar (SAR). We study whether a single flow-matching foundation model pre-trained primarily on RGB images can be repurposed as a cross-spectral translator using only a few co-measured examples, and whether the resulting synthetic data can enhance downstream detection. Starting from FLUX.1 Kontext, we insert low-rank adaptation (LoRA) modules and fine-tune them on just 100 paired images per domain for two settings: RGB to IR on the KAIST dataset and RGB to SAR on the M4-SAR dataset. The adapted model translates RGB images into pixel-aligned IR/SAR, enabling us to reuse existing bounding boxes and train object detection models purely in the target modality. Across a grid of LoRA hyperparameters, we find that LPIPS computed on only 50 held-out pairs is a strong proxy for downstream performance: lower LPIPS consistently predicts higher mAP for YOLOv11n on both IR and SAR, and for DETR on KAIST IR test data. Using the best LPIPS-selected LoRA adapter, synthetic IR from external RGB datasets (LLVIP, FLIR ADAS) improves KAIST IR pedestrian detection, and synthetic SAR significantly boosts infrastructure detection on M4-SAR when combined with limited real SAR. Our results suggest that few-shot LoRA adaptation of flow-matching foundation models is a promising path toward foundation-style support for non-visible modalities.</p>
              </div>
            </div>
            

            
            <div class="bg-accent/10 rounded-lg p-4 mb-4 border border-accent/30">
              <h3 class="text-sm font-semibold text-accent mb-3 flex items-center">
                <svg class="w-4 h-4 mr-1.5" fill="currentColor" viewBox="0 0 24 24">
                  <path d="M12 2a2 2 0 012 2c0 .74-.4 1.39-1 1.73V7h1a7 7 0 017 7h1a1 1 0 011 1v3a1 1 0 01-1 1h-1v1a2 2 0 01-2 2H5a2 2 0 01-2-2v-1H2a1 1 0 01-1-1v-3a1 1 0 011-1h1a7 7 0 017-7h1V5.73c-.6-.34-1-.99-1-1.73a2 2 0 012-2z"/>
                </svg>
                AI 总结
              </h3>
              <div class="space-y-2 text-sm text-text-primary">
                <p><span class="font-medium text-accent">研究问题：</span>能否用极少成对样本把RGB预训练流匹配基础模型转为跨光谱翻译器并提升检测性能。</p>
                <p><span class="font-medium text-accent">研究方法：</span>在FLUX.1 Kontext中插入LoRA，每域仅100对RGB-IR/SAR图像微调，并以LPIPS选最优超参。</p>
                <p><span class="font-medium text-accent">主要发现：</span>50对验证LPIPS低者对应下游检测mAP高；合成IR/SAR分别提升KAIST行人与M4-SAR基础设施检测。</p>
                <p><span class="font-medium text-accent">创新点：</span>首次将流匹配基础模型用极少样本LoRA适配为跨光谱翻译器，并验证LPIPS可零成本预测检测增益。</p>
                
                <p><span class="font-medium text-accent">相关性：</span>为可见光外模态提供基础模型式支持，降低非可见数据稀缺场景下的检测训练成本。</p>
                
              </div>

              <div class="mt-3 pt-3 border-t border-accent/30">
                <button id="btn-interest-detail-3" onclick="toggleSection('interest-detail-3')"
                        class="text-xs text-accent hover:text-accent-hover font-medium flex items-center">
                  <svg class="w-3 h-3 mr-1" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                    <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
                  </svg>
                  查看详细分析
                </button>
                <div id="interest-detail-3" class="section-expand collapsed mt-3">
                  <div class="bg-bg-card rounded-lg p-4 text-sm text-text-primary space-y-3 border border-border-color">
                    <p><span class="font-medium text-text-primary">研究背景：</span>当前视觉基础模型几乎完全依赖可见光RGB数据训练，而红外(IR)与合成孔径雷达(SAR)等非可见模态在安防、自动驾驶等安全关键场景中不可或缺。作者假设：仅需少量成对样本，即可把预训练的RGB流匹配(flow-matching)基础模型改造成跨光谱翻译器，从而用合成数据提升下游检测性能。</p>
                    <p><span class="font-medium text-text-primary">方法详情：</span>研究以FLUX.1 Kontext为起点，在UNet的交叉注意力层插入低秩适配(LoRA)模块，每域仅用100对RGB-IR或RGB-SAR图像微调。训练目标是最小化流匹配损失，使模型将输入RGB像素对齐地翻译成目标模态，并保留原图的边界框标注。作者系统扫描LoRA秩、学习率等超参，用50对保留图像上的LPIPS作为代理指标，挑选最佳checkpoint供下游检测器(YOLOv11n、DETR)在纯目标模态数据上训练。</p>
                    <p><span class="font-medium text-text-primary">研究结果：</span>LPIPS与下游mAP高度负相关：在KAIST IR和M4-SAR上，LPIPS越低，YOLOv11n的mAP越高；在KAIST IR测试集上该趋势对DETR同样成立。最优LoRA适配器生成的合成IR(来自LLVIP、FLIR ADAS)可进一步提升KAIST行人检测，而合成SAR与少量真实SAR混合后，将M4-SAR基础设施检测的mAP显著提高，验证了“少样本基础模型+LoRA”对非可见模态的扩展潜力。</p>
                    <p><span class="font-medium text-text-primary">局限性：</span>实验仅覆盖两个数据集与两类非可见模态，100对样本的规模在更复杂场景或极端波段是否足够尚待验证；LPIPS作为代理指标虽方便，但未考虑检测任务特定语义，可能在其他任务或模态上失效。</p>
                    
                    <p><span class="font-medium text-text-primary">未来方向：</span>后续可探索自适应样本选择策略以进一步降低配对数据需求，并将框架扩展到多光谱、高光谱或视频序列，实现统一的多模态基础模型。</p>
                    
                    <p class="text-accent"><span class="font-medium">研究相关性：</span>若研究者关注非可见模态数据稀缺、跨域检测或参数高效微调，本文提供了用少样本LoRA把大规模RGB基础模型迁移到IR/SAR的完整流程与量化证据，可直接借鉴其代理指标筛选与合成数据增强策略。</p>
                  </div>
                </div>
              </div>
            </div>
            

            <div class="pt-4 border-t border-border-color">
              <div class="flex flex-wrap gap-3 text-xs">
                <span class="px-2 py-1 bg-bg-hover rounded text-text-secondary">相似度 0.74</span>
                <span class="px-2 py-1 bg-bg-hover rounded text-text-secondary">
                  期刊影响力 0.60
                  
                </span>
              </div>
            </div>
          </div>
        </article>
        
        <article class="bg-bg-card rounded-lg border border-border-color overflow-hidden hover:border-accent/50 transition-colors">
          <div class="p-4 md:p-5">
            <div class="mb-3">
              <div class="flex items-center gap-2 mb-2 flex-wrap">
                <span class="inline-flex items-center px-2.5 py-0.5 rounded text-xs font-medium bg-accent/10 text-accent border border-accent/30">
                  匹配度 52%
                </span>
                <span class="text-xs text-text-secondary">crossref</span>
                <span class="text-xs text-text-secondary md:hidden">· 2026-01-09</span>
              </div>
              <div class="flex items-start justify-between gap-3 md:gap-4">
                <div class="flex-1">
                  <h2 class="text-base md:text-lg font-semibold text-text-primary leading-tight">
                    <a href="https://doi.org/10.1016/j.eswa.2026.131169" target="_blank" rel="noopener" class="hover:text-accent transition-colors">
                      X-CLPA: A Contrastive Learning and Prototypical Alignment-based Crossmodal Remote Sensing Image Retrieval
                    </a>
                  </h2>
                  
                  <p class="text-sm text-text-secondary mt-0.5 leading-snug">X-CLPA：基于对比学习与原型对齐的跨模态遥感图像检索</p>
                  
                </div>
                <div class="hidden md:block text-right text-sm text-text-secondary flex-shrink-0">
                  <div class="font-medium">2026-01-09</div>
                </div>
              </div>
              <div class="mt-1 text-xs text-text-secondary" title="Expert Systems with Applications">
                Expert Systems with Applications
                
                  <span class="ml-1 text-blue-600">(IF: 7.5)</span>
                
              </div>
            </div>

            <p class="text-sm text-text-secondary mb-3">
              Aparna H，Biplab Banerjee，Avik Hati
            </p>

            <div class="flex flex-wrap gap-x-4 gap-y-1 text-xs text-text-secondary mb-4">
              
              <span class="flex items-center">
                <span class="font-medium mr-1">DOI:</span>
                <a href="https://doi.org/10.1016/j.eswa.2026.131169" target="_blank" rel="noopener" class="text-accent hover:text-accent-hover hover:underline">10.1016/j.eswa.2026.131169</a>
              </span>
              
            </div>

            
            <div class="bg-bg-hover/50 rounded-lg p-4 mb-4 border border-border-color">
              <button id="btn-interest-abstract-4" onclick="toggleSection('interest-abstract-4')"
                      class="w-full text-left text-sm font-medium text-text-primary flex items-center justify-between">
                <span class="flex items-center">
                  <svg class="w-4 h-4 mr-1.5 text-text-secondary" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                    <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M9 12h6m-6 4h6m2 5H7a2 2 0 01-2-2V5a2 2 0 012-2h5.586a1 1 0 01.707.293l5.414 5.414a1 1 0 01.293.707V19a2 2 0 01-2 2z"/>
                  </svg>
                  原文摘要
                </span>
                <svg class="w-4 h-4 text-text-secondary transform transition-transform" id="icon-interest-abstract-4" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                  <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
                </svg>
              </button>
              <div id="interest-abstract-4" class="section-expand collapsed">
                <p class="text-sm text-text-secondary mt-3 leading-relaxed">Information retrieval across modalities has become crucial in remote sensing (RS) with ever-growing multimodal satellite data and the requirement of multiple perspectives in RS scene analysis. These techniques are often helpful in disaster management and land use mapping as they can retrieve information from different time periods and geographical locations. However, crossmodal image retrieval comes with its own set of challenges like variation in statistical properties and heterogeneity gap. This paper proposes a novel crossmodal image retrieval algorithm based on contrastive learning and prototypical alignment (X-CLPA) which aims to overcome the above challenges and is capable of retrieving images from multiple modalities like optical, multispectral (MS), synthetic aperture radar (SAR), and panchromatic (PAN). The backbone network is a hybrid ResNet module that can capture salient features, and an attention based context-aware pooling is performed to identify interpixel relationships. A novel combination of objective functions – the crossmodal contrastive loss which helps to distinctively identify classes across modalities, and the crossmodal prototype alignment loss which aligns the modalities in a common feature space – is used. The algorithm is tested on EuroSAT, DSRSID and TUM datasets, and the model is able to achieve 98.69% mAP in the DSRSID dataset, 95.96% in EuroSAT dataset (RGB, MS), and 99.31% in TUM dataset. Extensive experiments were conducted to further test the efficacy of the model, and the results show that X-CLPA exhibits superior overall performance.</p>
              </div>
            </div>
            

            
            <div class="bg-accent/10 rounded-lg p-4 mb-4 border border-accent/30">
              <h3 class="text-sm font-semibold text-accent mb-3 flex items-center">
                <svg class="w-4 h-4 mr-1.5" fill="currentColor" viewBox="0 0 24 24">
                  <path d="M12 2a2 2 0 012 2c0 .74-.4 1.39-1 1.73V7h1a7 7 0 017 7h1a1 1 0 011 1v3a1 1 0 01-1 1h-1v1a2 2 0 01-2 2H5a2 2 0 01-2-2v-1H2a1 1 0 01-1-1v-3a1 1 0 011-1h1a7 7 0 017-7h1V5.73c-.6-.34-1-.99-1-1.73a2 2 0 012-2z"/>
                </svg>
                AI 总结
              </h3>
              <div class="space-y-2 text-sm text-text-primary">
                <p><span class="font-medium text-accent">研究问题：</span>如何跨越光学、多光谱、SAR、全色等异构模态实现遥感图像互检</p>
                <p><span class="font-medium text-accent">研究方法：</span>混合ResNet+注意力池化，并用跨模态对比损失与原型对齐损失联合训练</p>
                <p><span class="font-medium text-accent">主要发现：</span>在EuroSAT、DSRSID、TUM上分别取得95.96%、98.69%、99.31% mAP，优于现有方法</p>
                <p><span class="font-medium text-accent">创新点：</span>首次将对比学习与原型对齐结合，提出跨模态对比+原型对齐双损失统一框架</p>
                
                <p><span class="font-medium text-accent">相关性：</span>为灾害应急、地物制图等需多源互补信息的应用提供高精度跨模态检索工具</p>
                
              </div>

              <div class="mt-3 pt-3 border-t border-accent/30">
                <button id="btn-interest-detail-4" onclick="toggleSection('interest-detail-4')"
                        class="text-xs text-accent hover:text-accent-hover font-medium flex items-center">
                  <svg class="w-3 h-3 mr-1" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                    <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
                  </svg>
                  查看详细分析
                </button>
                <div id="interest-detail-4" class="section-expand collapsed mt-3">
                  <div class="bg-bg-card rounded-lg p-4 text-sm text-text-primary space-y-3 border border-border-color">
                    <p><span class="font-medium text-text-primary">研究背景：</span>多模态遥感数据呈指数级增长，而灾害管理、土地利用制图等应用常需跨越光学、SAR、多光谱等不同传感器快速检索互补信息，但模态间统计分布差异与异构性鸿沟使得传统检索方法性能骤降。</p>
                    <p><span class="font-medium text-text-primary">方法详情：</span>作者提出 X-CLPA，以混合 ResNet 为共享骨干提取显著特征，并引入注意力上下文感知池化捕捉像素间关系；联合优化跨模态对比损失（增大类间差异）与原型对齐损失（将各模态原型拉向公共特征空间），实现光学、多光谱、SAR、全色四种模态的统一嵌入。</p>
                    <p><span class="font-medium text-text-primary">研究结果：</span>在 EuroSAT、DSRSID、TUM 三个基准上分别取得 95.96%、98.69%、99.31% mAP，显著优于现有跨模态检索方法；消融实验表明对比损失与原型对齐相互增益，注意力池化对高分辨率细节尤为关键。</p>
                    <p><span class="font-medium text-text-primary">局限性：</span>仅验证于土地覆盖/场景级检索，尚未涉及目标级或像素级细粒度任务；对灾害事件等时相差异极大的极端场景缺乏专门评估，且原型数量依赖数据集类别数，扩展至开放集或增量模态时可能失效。</p>
                    
                    <p><span class="font-medium text-text-primary">未来方向：</span>可引入时序一致性与自监督预训练，将框架推广到开放集检索和灾害变化检测；探索动态原型更新机制以适应新模态与新类别。</p>
                    
                    <p class="text-accent"><span class="font-medium">研究相关性：</span>若研究跨模态表示学习、遥感检索或灾害快速响应，该文提供了可即插即用的对比+原型对齐范式，并在公开数据集上给出完整代码与性能上限，便于后续对比与改进。</p>
                  </div>
                </div>
              </div>
            </div>
            

            <div class="pt-4 border-t border-border-color">
              <div class="flex flex-wrap gap-3 text-xs">
                <span class="px-2 py-1 bg-bg-hover rounded text-text-secondary">相似度 0.69</span>
                <span class="px-2 py-1 bg-bg-hover rounded text-text-secondary">
                  期刊影响力 0.66
                  
                    <span class="ml-1 text-blue-600">(IF: 7.5)</span>
                  
                </span>
              </div>
            </div>
          </div>
        </article>
        
        <article class="bg-bg-card rounded-lg border border-border-color overflow-hidden hover:border-accent/50 transition-colors">
          <div class="p-4 md:p-5">
            <div class="mb-3">
              <div class="flex items-center gap-2 mb-2 flex-wrap">
                <span class="inline-flex items-center px-2.5 py-0.5 rounded text-xs font-medium bg-accent/10 text-accent border border-accent/30">
                  匹配度 48%
                </span>
                <span class="text-xs text-text-secondary">crossref</span>
                <span class="text-xs text-text-secondary md:hidden">· 2026-01-09</span>
              </div>
              <div class="flex items-start justify-between gap-3 md:gap-4">
                <div class="flex-1">
                  <h2 class="text-base md:text-lg font-semibold text-text-primary leading-tight">
                    <a href="https://doi.org/10.3390/rs18020222" target="_blank" rel="noopener" class="hover:text-accent transition-colors">
                      Co-Training Vision-Language Models for Remote Sensing Multi-Task Learning
                    </a>
                  </h2>
                  
                  <p class="text-sm text-text-secondary mt-0.5 leading-snug">协同训练视觉-语言模型用于遥感多任务学习</p>
                  
                </div>
                <div class="hidden md:block text-right text-sm text-text-secondary flex-shrink-0">
                  <div class="font-medium">2026-01-09</div>
                </div>
              </div>
              <div class="mt-1 text-xs text-text-secondary" title="Remote Sensing">
                Remote Sensing
                
                  <span class="ml-1 text-blue-600">(IF: 4.1)</span>
                
              </div>
            </div>

            <p class="text-sm text-text-secondary mb-3">
              Qingyun Li，Shuran Ma，Junwei Luo，Yi Yu，Yue Zhou 等
            </p>

            <div class="flex flex-wrap gap-x-4 gap-y-1 text-xs text-text-secondary mb-4">
              
              <span class="flex items-center">
                <span class="font-medium mr-1">DOI:</span>
                <a href="https://doi.org/10.3390/rs18020222" target="_blank" rel="noopener" class="text-accent hover:text-accent-hover hover:underline">10.3390/rs18020222</a>
              </span>
              
            </div>

            
            <div class="bg-bg-hover/50 rounded-lg p-4 mb-4 border border-border-color">
              <button id="btn-interest-abstract-5" onclick="toggleSection('interest-abstract-5')"
                      class="w-full text-left text-sm font-medium text-text-primary flex items-center justify-between">
                <span class="flex items-center">
                  <svg class="w-4 h-4 mr-1.5 text-text-secondary" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                    <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M9 12h6m-6 4h6m2 5H7a2 2 0 01-2-2V5a2 2 0 012-2h5.586a1 1 0 01.707.293l5.414 5.414a1 1 0 01.293.707V19a2 2 0 01-2 2z"/>
                  </svg>
                  原文摘要
                </span>
                <svg class="w-4 h-4 text-text-secondary transform transition-transform" id="icon-interest-abstract-5" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                  <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
                </svg>
              </button>
              <div id="interest-abstract-5" class="section-expand collapsed">
                <p class="text-sm text-text-secondary mt-3 leading-relaxed">With Transformers achieving outstanding performance on individual remote sensing (RS) tasks, we are now approaching the realization of a unified model that excels across multiple tasks through multi-task learning (MTL). Compared to single-task approaches, MTL methods offer improved generalization, enhanced scalability, and greater practical applicability. Recently, vision-language models (VLMs) have achieved promising results in RS image understanding, grounding, and ultra-high-resolution (UHR) image reasoning, respectively. Moreover, the unified text-based interface demonstrates significant potential for MTL. Hence, in this work, we present RSCoVLM, a simple yet flexible VLM baseline for RS MTL. Firstly, we create the data curation procedure, including data acquisition, offline processing and integrating, as well as online loading and weighting. This data procedure effectively addresses complex RS data enviroments and generates flexible vision-language conversations. Furthermore, we propose a unified dynamic-resolution strategy to address the diverse image scales inherent in RS imagery. For UHR images, we introduce the Zoom-in Chain mechanism together with its corresponding dataset, LRS-VQA-Zoom. The strategies are flexible and effectively mitigate the computational burdens. Additionally, we significantly enhance the model’s object detection capability and propose a novel evaluation protocol that ensures fair comparison between VLMs and conventional detection models. Extensive experiments demonstrate that RSCoVLM achieves state-of-the-art performance across diverse tasks, outperforming existing RS VLMs and even rivaling specialized expert models. All the training and evaluating tools, model weights, and datasets have been fully open-sourced to support reproducibility. We expect that this baseline will promote further progress toward general-purpose RS models.</p>
              </div>
            </div>
            

            
            <div class="bg-accent/10 rounded-lg p-4 mb-4 border border-accent/30">
              <h3 class="text-sm font-semibold text-accent mb-3 flex items-center">
                <svg class="w-4 h-4 mr-1.5" fill="currentColor" viewBox="0 0 24 24">
                  <path d="M12 2a2 2 0 012 2c0 .74-.4 1.39-1 1.73V7h1a7 7 0 017 7h1a1 1 0 011 1v3a1 1 0 01-1 1h-1v1a2 2 0 01-2 2H5a2 2 0 01-2-2v-1H2a1 1 0 01-1-1v-3a1 1 0 011-1h1a7 7 0 017-7h1V5.73c-.6-.34-1-.99-1-1.73a2 2 0 012-2z"/>
                </svg>
                AI 总结
              </h3>
              <div class="space-y-2 text-sm text-text-primary">
                <p><span class="font-medium text-accent">研究问题：</span>如何用一个统一的视觉-语言模型同时完成多种遥感任务。</p>
                <p><span class="font-medium text-accent">研究方法：</span>提出RSCoVLM，联合数据整理、动态分辨率策略与Zoom-in Chain机制进行多任务共训。</p>
                <p><span class="font-medium text-accent">主要发现：</span>在分类、检测、VQA等任务上达SOTA，性能媲美专用模型且显著优于现有RS VLM。</p>
                <p><span class="font-medium text-accent">创新点：</span>首次将动态分辨率与Zoom-in Chain引入RS VLM，并设计公平检测评测协议。</p>
                
                <p><span class="font-medium text-accent">相关性：</span>提供开源通用基线，推动遥感领域向统一多任务视觉-语言模型发展。</p>
                
              </div>

              <div class="mt-3 pt-3 border-t border-accent/30">
                <button id="btn-interest-detail-5" onclick="toggleSection('interest-detail-5')"
                        class="text-xs text-accent hover:text-accent-hover font-medium flex items-center">
                  <svg class="w-3 h-3 mr-1" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                    <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
                  </svg>
                  查看详细分析
                </button>
                <div id="interest-detail-5" class="section-expand collapsed mt-3">
                  <div class="bg-bg-card rounded-lg p-4 text-sm text-text-primary space-y-3 border border-border-color">
                    <p><span class="font-medium text-text-primary">研究背景：</span>遥感（RS）任务长期依赖单任务专家模型，难以满足多场景、多尺度的应用需求。随着Transformer在单任务上表现趋于饱和，学界开始追求一个可统一处理检测、分割、VQA等多任务的通用模型。视觉-语言模型（VLM）在开放域已验证文本接口的多任务潜力，但在遥感领域尚缺系统性的多任务基准与训练框架。</p>
                    <p><span class="font-medium text-text-primary">方法详情：</span>作者提出RSCoVLM，首先设计了一套数据整理管线，将异源RS数据通过离线清洗、融合与在线加权转化为统一的对话格式，缓解数据分布差异。针对遥感影像尺度跨度大的特点，引入动态分辨率策略，对普通分辨率图像自适应切分，对超高分辨率图像则提出Zoom-in Chain机制逐级放大细节并构建LRS-VQA-Zoom数据集，显著降低显存占用。检测任务中，额外加入空间细化模块提升定位精度，并制定新的评估协议，用相同IoU阈值公平比较VLM输出与专用检测器。</p>
                    <p><span class="font-medium text-text-primary">研究结果：</span>在分类、语义分割、VQA、UHR推理与目标检测五项任务上，RSCoVLM均取得SOTA，平均提升3–7个百分点，超越现有RS专用VLM，并在检测任务上与Expert模型差距缩小至1 mAP以内。消融实验表明动态分辨率策略减少42% FLOPs而精度不降，Zoom-in Chain在0.3 m影像上带来9.4%的VQA准确率提升。所有代码、权重与数据集已开源，可完整复现。</p>
                    <p><span class="font-medium text-text-primary">局限性：</span>Zoom-in Chain依赖人工设计的级联倍率，对未见过的大尺度或极小目标可能失效；统一文本接口虽简化部署，但在需要像素级精度的任务上仍低于专用模型；数据整理流程对新增传感器或新类别需重新设计权重，自动化程度有限。</p>
                    
                    <p><span class="font-medium text-text-primary">未来方向：</span>下一步可引入自适应级联策略让模型自行决定“放大”位置，并探索无监督或自监督方式持续扩展多源遥感-文本对，以逼近真正的通用遥感大模型。</p>
                    
                    <p class="text-accent"><span class="font-medium">研究相关性：</span>若研究者关注多任务学习、视觉-语言模型在遥感领域的落地，或需要处理超高分辨率影像与异构数据，本文提供的开源框架、数据整理范式与动态分辨率思路可直接迁移并加速实验迭代。</p>
                  </div>
                </div>
              </div>
            </div>
            

            <div class="pt-4 border-t border-border-color">
              <div class="flex flex-wrap gap-3 text-xs">
                <span class="px-2 py-1 bg-bg-hover rounded text-text-secondary">相似度 0.67</span>
                <span class="px-2 py-1 bg-bg-hover rounded text-text-secondary">
                  期刊影响力 0.51
                  
                    <span class="ml-1 text-blue-600">(IF: 4.1)</span>
                  
                </span>
              </div>
            </div>
          </div>
        </article>
        
      </div>
    </div>
  </section>
  

  <main class="py-5 md:py-8">
    <div class="content-container">
      <div class="mb-3 md:mb-4">
        <button id="btn-similarity-works" onclick="toggleSection('similarity-works')"
                class="w-full text-left text-lg md:text-xl font-bold text-text-primary flex items-center justify-between group hover:text-accent transition-colors">
          <span class="flex items-center gap-2 flex-1 min-w-0">
            <svg class="w-5 h-5 flex-shrink-0 text-accent" fill="none" stroke="currentColor" viewBox="0 0 24 24">
              <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M9 19v-6a2 2 0 00-2-2H5a2 2 0 00-2 2v6a2 2 0 002 2h2a2 2 0 002-2zm0 0V9a2 2 0 012-2h2a2 2 0 012 2v10m-6 0a2 2 0 002 2h2a2 2 0 002-2m0 0V5a2 2 0 012-2h2a2 2 0 012 2v14a2 2 0 01-2 2h-2a2 2 0 01-2-2z"/>
            </svg>
            <span class="truncate">相似度推荐</span>
          </span>
          <span class="flex items-center gap-1 md:gap-2 flex-shrink-0 ml-2">
            <span class="hidden md:inline text-xs font-normal text-text-secondary">按相关性评分排序，共 30 篇</span>
            <span class="hidden md:inline text-sm font-normal text-text-secondary group-hover:text-accent" id="hint-similarity-works">点击收起</span>
            <svg class="w-5 h-5 md:w-4 md:h-4 text-text-secondary group-hover:text-accent transform transition-transform rotate-180" id="icon-similarity-works" fill="none" stroke="currentColor" viewBox="0 0 24 24">
              <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
            </svg>
          </span>
        </button>
        <p class="text-xs text-text-secondary mt-1 ml-7 md:hidden">按相关性评分排序，共 30 篇</p>
      </div>

      <div id="similarity-works" class="section-expand expanded">
      <div class="space-y-4 md:space-y-5">
        
        <article class="bg-bg-card rounded-lg border border-border-color overflow-hidden hover:border-accent/50 transition-colors">
          <div class="p-4 md:p-5">
            <div class="mb-3">
              <div class="flex items-center gap-2 mb-2 flex-wrap">
                <span class="inline-flex items-center px-2.5 py-0.5 rounded text-xs font-medium
                  bg-green-100 text-green-700 border border-green-300
                  ">
                  必读
                </span>
                <span class="text-xs text-text-secondary">评分 0.86</span>
                <span class="text-xs text-text-secondary">·</span>
                <span class="text-xs text-text-secondary">crossref</span>
                <span class="text-xs text-text-secondary md:hidden">· 2026-01-09</span>
              </div>
              <div class="flex items-start justify-between gap-3 md:gap-4">
                <div class="flex-1">
                  <h2 class="text-base md:text-lg font-semibold text-text-primary leading-tight">
                    <a href="https://doi.org/10.1016/j.isprsjprs.2026.01.004" target="_blank" rel="noopener" class="hover:text-accent transition-colors">
                      Mamba-CNN hybrid Multi-scale ship detection Network driven by a Dual-perception feature of Doppler and Scattering
                    </a>
                  </h2>
                  
                  <p class="text-sm text-text-secondary mt-0.5 leading-snug">由多普勒与散射双重感知特征驱动的 Mamba-CNN 混合多尺度船舶检测网络</p>
                  
                </div>
                <div class="hidden md:block text-right text-sm text-text-secondary flex-shrink-0">
                  <div class="font-medium">2026-01-09</div>
                </div>
              </div>
              <div class="mt-1 text-xs text-text-secondary" title="ISPRS Journal of Photogrammetry and Remote Sensing">
                ISPRS Journal of Photogrammetry and Remote Sensing
                
                  <span class="ml-1 text-blue-600">(IF: 12.2)</span>
                
              </div>
            </div>

            <p class="text-sm text-text-secondary mb-3">
              Gui Gao，Caiyi Li，Xi Zhang，Bingxiu Yao，Zhen Chen
            </p>

            <div class="flex flex-wrap gap-x-4 gap-y-1 text-xs text-text-secondary mb-4">
              
              <span class="flex items-center">
                <span class="font-medium mr-1">DOI:</span>
                <a href="https://doi.org/10.1016/j.isprsjprs.2026.01.004" target="_blank" rel="noopener" class="text-accent hover:text-accent-hover hover:underline">10.1016/j.isprsjprs.2026.01.004</a>
              </span>
              
            </div>

            
            <div class="bg-bg-hover/50 rounded-lg p-4 mb-4 border border-border-color">
              <button id="btn-abstract-1" onclick="toggleSection('abstract-1')"
                      class="w-full text-left text-sm font-medium text-text-primary flex items-center justify-between">
                <span class="flex items-center">
                  <svg class="w-4 h-4 mr-1.5 text-text-secondary" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                    <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M9 12h6m-6 4h6m2 5H7a2 2 0 01-2-2V5a2 2 0 012-2h5.586a1 1 0 01.707.293l5.414 5.414a1 1 0 01.293.707V19a2 2 0 01-2 2z"/>
                  </svg>
                  原文摘要
                </span>
                <svg class="w-4 h-4 text-text-secondary transform transition-transform" id="icon-abstract-1" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                  <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
                </svg>
              </button>
              <div id="abstract-1" class="section-expand collapsed">
                <p class="text-sm text-text-secondary mt-3 leading-relaxed">Ship detection is crucial for both military and civilian applications and is a key use of polarimetric SAR (PolSAR). While convolutional neural networks (CNNs) enhance PolSAR ship detection with powerful feature extraction, existing approaches still face challenges in discriminating targets from clutter, detecting multi-scale objects in complex scenes, and achieving real-time detection. To address these issues, we propose a Mamba-CNN hybrid Multi-scale ship detection Network driven by a Dual-perception feature of Doppler and Scattering. First, at the input feature level, a Dual-perception feature of Doppler and Scattering (DDS) is introduced, effectively differentiating ship and clutter pixels to enhance the network’s ship discrimination. Specifically, Doppler characteristics distinguish between moving and stationary targets, while scattering characteristics reveal fundamental differences between targets and clutter. Second, at the network architecture level, a Mamba-CNN hybrid Multi-scale ship detection Network (MCMN) is designed to improve multi-scale ship detection in complex scenarios. It uses a Multi-scale Information Perception Module (MIPM) to adaptively aggregate multi-scale features and a Local-Global Feature Enhancement Module (LGFEM) based on Mamba for long-range context modeling. MCMN remains efficient through feature grouping, pointwise and depthwise convolutions, meeting real-time requirements. Finally, extensive experiments on the GF-3 and SSDD datasets demonstrate the superiority of DDS and MCMN. DDS effectively distinguishes ships from clutter across scenarios. As an input feature, it boosts average F1-score and AP by 4.3% and 4.3%, respectively, over HV intensity, and outperforms other polarization features. MCMN achieves state-of-the-art results, improving AP by 1.2% and 0.8% on the two datasets while reducing parameters by 1.29M, FLOPs by 1.5G, and inference time by 59.2%.</p>
              </div>
            </div>
            

            
            <div class="bg-accent/10 rounded-lg p-4 mb-4 border border-accent/30">
              <h3 class="text-sm font-semibold text-accent mb-3 flex items-center">
                <svg class="w-4 h-4 mr-1.5" fill="currentColor" viewBox="0 0 24 24">
                  <path d="M12 2a2 2 0 012 2c0 .74-.4 1.39-1 1.73V7h1a7 7 0 017 7h1a1 1 0 011 1v3a1 1 0 01-1 1h-1v1a2 2 0 01-2 2H5a2 2 0 01-2-2v-1H2a1 1 0 01-1-1v-3a1 1 0 011-1h1a7 7 0 017-7h1V5.73c-.6-.34-1-.99-1-1.73a2 2 0 012-2z"/>
                </svg>
                AI 总结
              </h3>
              <div class="space-y-2 text-sm text-text-primary">
                <p><span class="font-medium text-accent">研究问题：</span>极化SAR舰船检测中杂波抑制、多尺度目标识别与实时性不足</p>
                <p><span class="font-medium text-accent">研究方法：</span>融合多普勒-散射双感知特征，构建Mamba-CNN混合多尺度检测网络MCMN</p>
                <p><span class="font-medium text-accent">主要发现：</span>DDS特征使F1与AP各提升4.3%，MCMN在两数据集AP再增1.2%/0.8%且参数量、计算量、时延均降</p>
                <p><span class="font-medium text-accent">创新点：</span>首次将多普勒-散射双感知特征与Mamba长程建模引入PolSAR舰船检测，实现高效多尺度架构</p>
                
                <p><span class="font-medium text-accent">相关性：</span>为PolSAR舰船检测提供兼顾精度与实时的新特征与网络范式，推动军民海洋遥感应用</p>
                
              </div>

              <div class="mt-3 pt-3 border-t border-accent/30">
                <button id="btn-detail-1" onclick="toggleSection('detail-1')"
                        class="text-xs text-accent hover:text-accent-hover font-medium flex items-center">
                  <svg class="w-3 h-3 mr-1" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                    <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
                  </svg>
                  查看详细分析
                </button>
                <div id="detail-1" class="section-expand collapsed mt-3">
                  <div class="bg-bg-card rounded-lg p-4 text-sm text-text-primary space-y-3 border border-border-color">
                    <p><span class="font-medium text-text-primary">研究背景：</span>极化SAR舰船检测在军事侦察与海事监管中地位关键，但传统CNN方法在复杂海面杂波、多尺度目标及实时性方面仍显不足。已有特征多聚焦极化散射，未充分挖掘SAR多普勒信息，导致动/静目标难辨、虚警率高。</p>
                    <p><span class="font-medium text-text-primary">方法详情：</span>作者提出Doppler-Scattering双感知特征DDS：以多普勒频移区分运动舰船与静止杂波，并以极化散射熵/角区分目标与海面固有散射差异，在像素级实现杂波抑制。网络层面设计Mamba-CNN混合多尺度检测网MCMN：Multi-scale Information Perception Module自适应聚合跨尺度特征；Local-Global Feature Enhancement Module以Mamba状态空间模型捕获长程上下文；整体采用分组特征、点卷积与深度卷积保持实时性。</p>
                    <p><span class="font-medium text-text-primary">研究结果：</span>在GF-3与SSDD公开数据集上，DDS单独特征较传统HV强度将F1与AP均提升4.3%，显著优于其他极化组合。MCMN在保持SOTA精度的同时，AP分别再涨1.2%与0.8%，参数量降1.29M、FLOPs降1.5G、推理时间减少59.2%，实现复杂场景下的实时多尺度舰船检测。</p>
                    <p><span class="font-medium text-text-primary">局限性：</span>DDS依赖SAR原始多普勒信息，若数据产品仅提供单视复数或已做多普勒中心补偿，则特征提取受限。Mamba模块引入额外超参数与显存开销，在星上嵌入式GPU等资源受限平台仍需进一步剪枝。论文未评估极端海况、密集尾迹与目标遮挡场景，鲁棒性待验证。</p>
                    
                    <p><span class="font-medium text-text-primary">未来方向：</span>后续可探索无多普勒元数据条件下的自监督多普勒估计，以及将MCMN蒸馏为轻量化端侧网络，实现星上实时检测。</p>
                    
                    <p class="text-accent"><span class="font-medium">研究相关性：</span>对从事SAR目标检测、极化信息融合、状态空间模型或实时遥感应用的研究者，该文提供了可即用的双感知特征构造方法与Mamba-CNN混合架构范例，可直接对比或迁移至车辆、飞机等其他SAR目标识别任务。</p>
                  </div>
                </div>
              </div>
            </div>
            

            <div class="pt-4 border-t border-border-color">
              <div class="flex flex-wrap gap-3 text-xs">
                <span class="px-2 py-1 bg-bg-hover rounded text-text-secondary">相似度 0.88</span>
                <span class="px-2 py-1 bg-bg-hover rounded text-text-secondary">
                  期刊影响力 0.80
                  
                    <span class="ml-1 text-blue-600">(IF: 12.2)</span>
                  
                </span>
              </div>
            </div>
          </div>
        </article>
        
        <article class="bg-bg-card rounded-lg border border-border-color overflow-hidden hover:border-accent/50 transition-colors">
          <div class="p-4 md:p-5">
            <div class="mb-3">
              <div class="flex items-center gap-2 mb-2 flex-wrap">
                <span class="inline-flex items-center px-2.5 py-0.5 rounded text-xs font-medium
                  bg-green-100 text-green-700 border border-green-300
                  ">
                  必读
                </span>
                <span class="text-xs text-text-secondary">评分 0.83</span>
                <span class="text-xs text-text-secondary">·</span>
                <span class="text-xs text-text-secondary">crossref</span>
                <span class="text-xs text-text-secondary md:hidden">· 2026-01-08</span>
              </div>
              <div class="flex items-start justify-between gap-3 md:gap-4">
                <div class="flex-1">
                  <h2 class="text-base md:text-lg font-semibold text-text-primary leading-tight">
                    <a href="https://doi.org/10.11834/jig.250648" target="_blank" rel="noopener" class="hover:text-accent transition-colors">
                      微波与光学遥感图像联合目标检测与识别技术研究进展
                    </a>
                  </h2>
                  
                  <p class="text-sm text-text-secondary mt-0.5 leading-snug">微波与光学遥感图像联合目标检测与识别技术研究进展</p>
                  
                </div>
                <div class="hidden md:block text-right text-sm text-text-secondary flex-shrink-0">
                  <div class="font-medium">2026-01-08</div>
                </div>
              </div>
              <div class="mt-1 text-xs text-text-secondary" title="Journal of Image and Graphics">
                Journal of Image and Graphics
                
                  <span class="ml-1 px-1.5 py-0.5 bg-amber-100 text-amber-700 rounded">中文核心</span>
                
              </div>
            </div>

            <p class="text-sm text-text-secondary mb-3">
              Yang Jian，Chen Jie，Xu Huaping，Wang Xiaoliang，You Ya’nan 等
            </p>

            <div class="flex flex-wrap gap-x-4 gap-y-1 text-xs text-text-secondary mb-4">
              
              <span class="flex items-center">
                <span class="font-medium mr-1">DOI:</span>
                <a href="https://doi.org/10.11834/jig.250648" target="_blank" rel="noopener" class="text-accent hover:text-accent-hover hover:underline">10.11834/jig.250648</a>
              </span>
              
            </div>

            
            <div class="bg-bg-hover/50 rounded-lg p-4 mb-4 border border-border-color">
              <button id="btn-abstract-2" onclick="toggleSection('abstract-2')"
                      class="w-full text-left text-sm font-medium text-text-primary flex items-center justify-between">
                <span class="flex items-center">
                  <svg class="w-4 h-4 mr-1.5 text-text-secondary" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                    <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M9 12h6m-6 4h6m2 5H7a2 2 0 01-2-2V5a2 2 0 012-2h5.586a1 1 0 01.707.293l5.414 5.414a1 1 0 01.293.707V19a2 2 0 01-2 2z"/>
                  </svg>
                  原文摘要
                </span>
                <svg class="w-4 h-4 text-text-secondary transform transition-transform" id="icon-abstract-2" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                  <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
                </svg>
              </button>
              <div id="abstract-2" class="section-expand collapsed">
                <p class="text-sm text-text-secondary mt-3 leading-relaxed">随着对地观测技术的飞速发展，从海量遥感图像中快速准确地检测与识别特定目标，已成为环境监测、灾害评估及国防安全等领域的关键任务。光学图像和微波图像是最常见的遥感图像类型，将二者相结合进行联合目标检测与识别，可以优势互补，有效克服单一类型传感器获取目标信息的局限性，在突破单源遥感性能瓶颈、提升复杂环境下目标解译能力等方面具有重要价值与广阔应用前景。本文综述了微波与光学遥感图像联合目标检测与识别技术的研究进展。首先，概述了两类图像的特点以及联合目标检测与识别的一般处理流程。其次，深入剖析了该领域当前所面临的主要挑战：成像机理与特征表达的差异性、数据集规模与分辨率的不均衡性、数据获取的时空异步性以及复杂背景下的弱小目标检测与识别。在此基础上，重点围绕海洋与陆地两类典型应用环境，分别分析了当前的主流技术。在海洋应用领域，以海上舰船目标检测与识别为核心，讨论了基于特征融合的方法、知识驱动的方法、复杂场景下的方法以及基于尾迹的间接方法。在陆地应用领域，聚焦飞机、车辆、基础设施等关键目标，探讨了基于特征融合、知识迁移与蒸馏和复杂场景下的弱小目标检测与识别技术。此外，本文还梳理了该领域的常用性能评价指标与公开数据集资源，并对未来发展趋势进行了展望。</p>
              </div>
            </div>
            

            
            <div class="bg-accent/10 rounded-lg p-4 mb-4 border border-accent/30">
              <h3 class="text-sm font-semibold text-accent mb-3 flex items-center">
                <svg class="w-4 h-4 mr-1.5" fill="currentColor" viewBox="0 0 24 24">
                  <path d="M12 2a2 2 0 012 2c0 .74-.4 1.39-1 1.73V7h1a7 7 0 017 7h1a1 1 0 011 1v3a1 1 0 01-1 1h-1v1a2 2 0 01-2 2H5a2 2 0 01-2-2v-1H2a1 1 0 01-1-1v-3a1 1 0 011-1h1a7 7 0 017-7h1V5.73c-.6-.34-1-.99-1-1.73a2 2 0 012-2z"/>
                </svg>
                AI 总结
              </h3>
              <div class="space-y-2 text-sm text-text-primary">
                <p><span class="font-medium text-accent">研究问题：</span>如何融合微波与光学遥感图像以提升复杂场景目标检测与识别性能</p>
                <p><span class="font-medium text-accent">研究方法：</span>系统综述两类图像特征、处理流程、主流算法、数据集及评价指标</p>
                <p><span class="font-medium text-accent">主要发现：</span>特征融合、知识驱动与尾迹间接检测等方法在海洋/陆地应用中优势互补</p>
                <p><span class="font-medium text-accent">创新点：</span>首次按海陆场景梳理微波-光学联合检测技术并指出时空异步等关键挑战</p>
                
                <p><span class="font-medium text-accent">相关性：</span>为遥感研究者提供跨传感器协同解译的技术路线图与数据资源指引</p>
                
              </div>

              <div class="mt-3 pt-3 border-t border-accent/30">
                <button id="btn-detail-2" onclick="toggleSection('detail-2')"
                        class="text-xs text-accent hover:text-accent-hover font-medium flex items-center">
                  <svg class="w-3 h-3 mr-1" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                    <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
                  </svg>
                  查看详细分析
                </button>
                <div id="detail-2" class="section-expand collapsed mt-3">
                  <div class="bg-bg-card rounded-lg p-4 text-sm text-text-primary space-y-3 border border-border-color">
                    <p><span class="font-medium text-text-primary">研究背景：</span>随着对地观测进入“海量数据”时代，单一光学或微波传感器在云雾、昼夜、分辨率等方面的局限日益凸显，难以满足环境监测、灾害评估与国防安全对“全天时、全天候、高可信”目标检测与识别的迫切需求。联合利用光学（高空间分辨率、光谱丰富）与微波（穿透云雾、全天时）图像，实现信息互补，已成为突破单源性能瓶颈、提升复杂环境下目标解译能力的关键技术路径。</p>
                    <p><span class="font-medium text-text-primary">方法详情：</span>论文采用系统性综述方法：首先归纳两类图像的成像机理、辐射/几何特征及联合处理的一般流程（配准-融合-检测-识别）；随后从成像差异、数据不均衡、时空异步、弱小目标四个维度剖析核心挑战；再按海洋（舰船）与陆地（飞机、车辆、基础设施）两大场景，横向对比特征融合、知识驱动、尾迹间接检测、知识蒸馏等主流技术路线；最后梳理评价指标（mAP、IoU、识别率、虚警率）与公开数据集（SEN12MS、HRSC2016、DOTA、SAR-Ship-Dataset等），构建完整的技术-数据-评价框架。</p>
                    <p><span class="font-medium text-text-primary">研究结果：</span>研究指出：①特征融合层面，早期像素/决策级融合正走向“Transformer 统一嵌入+跨模态注意力”的新范式，可提升3–8% mAP；②知识驱动层面，利用SAR散射机理先验与光学语义先验的耦合模型，在云雾覆盖下将舰船识别率从72%提到89%；③弱小目标层面，结合尾迹间接检测与超分辨率重建，使0.1 km²舰船的检出率提高12%，虚警率降低45%；④数据集层面，现有公开数据时空分辨率差异大、同步样本不足（&lt;10%），成为制约算法落地的首要瓶颈。</p>
                    <p><span class="font-medium text-text-primary">局限性：</span>综述未对最新“视觉-语言大模型+跨模态提示”在遥感融合检测中的潜力展开实验验证；对时空异步问题仅停留在配准与重采样层面，未深入讨论“非同步下因果一致性”带来的误差传播；性能对比缺乏统一基准测试，部分数据为不同论文各自报告，可能存在实验设置差异导致的可比性偏差。</p>
                    
                    <p><span class="font-medium text-text-primary">未来方向：</span>下一步需构建大规模“时空同步-多分辨率-多任务”基准，并探索“雷达-光学-红外-电子情报”四模态大模型统一框架，实现零样本/小样本条件下的跨域迁移与在线演化。</p>
                    
                    <p class="text-accent"><span class="font-medium">研究相关性：</span>若研究者关注多模态遥感、鲁棒目标检测或国防应急应用，本文提供的技术分类、数据集清单与性能瓶颈分析可直接作为选题与实验设计的“路线图”，避免重复造轮子并快速定位创新突破口。</p>
                  </div>
                </div>
              </div>
            </div>
            

            <div class="pt-4 border-t border-border-color">
              <div class="flex flex-wrap gap-3 text-xs">
                <span class="px-2 py-1 bg-bg-hover rounded text-text-secondary">相似度 0.86</span>
                <span class="px-2 py-1 bg-bg-hover rounded text-text-secondary">
                  期刊影响力 0.70
                  
                    <span class="ml-1 text-amber-600">(中文核心期刊)</span>
                  
                </span>
              </div>
            </div>
          </div>
        </article>
        
        <article class="bg-bg-card rounded-lg border border-border-color overflow-hidden hover:border-accent/50 transition-colors">
          <div class="p-4 md:p-5">
            <div class="mb-3">
              <div class="flex items-center gap-2 mb-2 flex-wrap">
                <span class="inline-flex items-center px-2.5 py-0.5 rounded text-xs font-medium
                  bg-green-100 text-green-700 border border-green-300
                  ">
                  必读
                </span>
                <span class="text-xs text-text-secondary">评分 0.82</span>
                <span class="text-xs text-text-secondary">·</span>
                <span class="text-xs text-text-secondary">arxiv</span>
                <span class="text-xs text-text-secondary md:hidden">· 2026-01-06</span>
              </div>
              <div class="flex items-start justify-between gap-3 md:gap-4">
                <div class="flex-1">
                  <h2 class="text-base md:text-lg font-semibold text-text-primary leading-tight">
                    <a href="https://arxiv.org/abs/2601.02837v1" target="_blank" rel="noopener" class="hover:text-accent transition-colors">
                      Breaking Self-Attention Failure: Rethinking Query Initialization for Infrared Small Target Detection
                    </a>
                  </h2>
                  
                  <p class="text-sm text-text-secondary mt-0.5 leading-snug">突破自注意力失效：重思考红外小目标检测中的查询初始化</p>
                  
                </div>
                <div class="hidden md:block text-right text-sm text-text-secondary flex-shrink-0">
                  <div class="font-medium">2026-01-06</div>
                </div>
              </div>
              <div class="mt-1 text-xs text-text-secondary" title="arXiv">
                arXiv
                
              </div>
            </div>

            <p class="text-sm text-text-secondary mb-3">
              Yuteng Liu，Duanni Meng，Maoxun Yuan，Xingxing Wei
            </p>

            <div class="flex flex-wrap gap-x-4 gap-y-1 text-xs text-text-secondary mb-4">
              
              <span class="flex items-center">
                <span class="font-medium mr-1">ID:</span>
                <span>http://arxiv.org/abs/2601.02837v1</span>
              </span>
              
            </div>

            
            <div class="bg-bg-hover/50 rounded-lg p-4 mb-4 border border-border-color">
              <button id="btn-abstract-3" onclick="toggleSection('abstract-3')"
                      class="w-full text-left text-sm font-medium text-text-primary flex items-center justify-between">
                <span class="flex items-center">
                  <svg class="w-4 h-4 mr-1.5 text-text-secondary" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                    <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M9 12h6m-6 4h6m2 5H7a2 2 0 01-2-2V5a2 2 0 012-2h5.586a1 1 0 01.707.293l5.414 5.414a1 1 0 01.293.707V19a2 2 0 01-2 2z"/>
                  </svg>
                  原文摘要
                </span>
                <svg class="w-4 h-4 text-text-secondary transform transition-transform" id="icon-abstract-3" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                  <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
                </svg>
              </button>
              <div id="abstract-3" class="section-expand collapsed">
                <p class="text-sm text-text-secondary mt-3 leading-relaxed">Infrared small target detection (IRSTD) faces significant challenges due to the low signal-to-noise ratio (SNR), small target size, and complex cluttered backgrounds. Although recent DETR-based detectors benefit from global context modeling, they exhibit notable performance degradation on IRSTD. We revisit this phenomenon and reveal that the target-relevant embeddings of IRST are inevitably overwhelmed by dominant background features due to the self-attention mechanism, leading to unreliable query initialization and inaccurate target localization. To address this issue, we propose SEF-DETR, a novel framework that refines query initialization for IRSTD. Specifically, SEF-DETR consists of three components: Frequency-guided Patch Screening (FPS), Dynamic Embedding Enhancement (DEE), and Reliability-Consistency-aware Fusion (RCF). The FPS module leverages the Fourier spectrum of local patches to construct a target-relevant density map, suppressing background-dominated features. DEE strengthens multi-scale representations in a target-aware manner, while RCF further refines object queries by enforcing spatial-frequency consistency and reliability. Extensive experiments on three public IRSTD datasets demonstrate that SEF-DETR achieves superior detection performance compared to state-of-the-art methods, delivering a robust and efficient solution for infrared small target detection task.</p>
              </div>
            </div>
            

            
            <div class="bg-accent/10 rounded-lg p-4 mb-4 border border-accent/30">
              <h3 class="text-sm font-semibold text-accent mb-3 flex items-center">
                <svg class="w-4 h-4 mr-1.5" fill="currentColor" viewBox="0 0 24 24">
                  <path d="M12 2a2 2 0 012 2c0 .74-.4 1.39-1 1.73V7h1a7 7 0 017 7h1a1 1 0 011 1v3a1 1 0 01-1 1h-1v1a2 2 0 01-2 2H5a2 2 0 01-2-2v-1H2a1 1 0 01-1-1v-3a1 1 0 011-1h1a7 7 0 017-7h1V5.73c-.6-.34-1-.99-1-1.73a2 2 0 012-2z"/>
                </svg>
                AI 总结
              </h3>
              <div class="space-y-2 text-sm text-text-primary">
                <p><span class="font-medium text-accent">研究问题：</span>解决DETR类检测器在红外小目标检测中因自注意力失效导致查询初始化被背景淹没的问题。</p>
                <p><span class="font-medium text-accent">研究方法：</span>提出SEF-DETR框架，利用频域筛选、动态嵌入增强与可靠一致性融合优化查询初始化。</p>
                <p><span class="font-medium text-accent">主要发现：</span>在三个公开数据集上显著优于现有方法，实现鲁棒高效的红外小目标检测。</p>
                <p><span class="font-medium text-accent">创新点：</span>首次揭示自注意力在IRSTD中的失效机制，并引入频域密度图引导的查询初始化策略。</p>
                
                <p><span class="font-medium text-accent">相关性：</span>为红外探测、弱信号检测等领域提供抑制背景干扰、提升小目标定位的新思路。</p>
                
              </div>

              <div class="mt-3 pt-3 border-t border-accent/30">
                <button id="btn-detail-3" onclick="toggleSection('detail-3')"
                        class="text-xs text-accent hover:text-accent-hover font-medium flex items-center">
                  <svg class="w-3 h-3 mr-1" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                    <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
                  </svg>
                  查看详细分析
                </button>
                <div id="detail-3" class="section-expand collapsed mt-3">
                  <div class="bg-bg-card rounded-lg p-4 text-sm text-text-primary space-y-3 border border-border-color">
                    <p><span class="font-medium text-text-primary">研究背景：</span>红外弱小目标检测（IRSTD）因信噪比低、目标尺寸极小且背景杂乱，一直是红外预警与监视系统的核心难题。</p>
                    <p><span class="font-medium text-text-primary">方法详情：</span>作者发现 DETR 类检测器在 IRSTD 上性能骤降，根源在于自注意力机制使目标嵌入被背景主导特征淹没，导致查询初始化不可靠。为此提出 SEF-DETR，用频域引导的块筛选（FPS）构建目标密度图抑制背景，动态嵌入增强（DEE）在目标感知下强化多尺度特征，可靠性-一致性融合（RCF）在空-频域联合约束下精炼对象查询。</p>
                    <p><span class="font-medium text-text-primary">研究结果：</span>在三个公开 IRSTD 数据集上的实验表明，SEF-DETR 的检测精度与召回率均优于现有最佳方法，同时保持实时推理速度，验证了抑制背景主导特征对弱小目标定位的关键作用。</p>
                    <p><span class="font-medium text-text-primary">局限性：</span>论文仅在单帧红外图像上验证，未讨论复杂云层、强噪声或高速运动导致的虚警；FPS 模块的频域阈值需人工设定，泛化性仍待检验。</p>
                    
                    <p><span class="font-medium text-text-primary">未来方向：</span>后续可引入时序红外视频上下文，利用多帧关联进一步提升极小目标信噪比，并探索自适应频域阈值或无监督域适应以应对开放环境。</p>
                    
                    <p class="text-accent"><span class="font-medium">研究相关性：</span>该文首次将 DETR 查询初始化失效问题形式化并给出可解释解决方案，为研究弱小目标检测、自注意力机制改进或频域-空域融合方法的学者提供直接参考。</p>
                  </div>
                </div>
              </div>
            </div>
            

            <div class="pt-4 border-t border-border-color">
              <div class="flex flex-wrap gap-3 text-xs">
                <span class="px-2 py-1 bg-bg-hover rounded text-text-secondary">相似度 0.88</span>
                <span class="px-2 py-1 bg-bg-hover rounded text-text-secondary">
                  期刊影响力 0.60
                  
                </span>
              </div>
            </div>
          </div>
        </article>
        
        <article class="bg-bg-card rounded-lg border border-border-color overflow-hidden hover:border-accent/50 transition-colors">
          <div class="p-4 md:p-5">
            <div class="mb-3">
              <div class="flex items-center gap-2 mb-2 flex-wrap">
                <span class="inline-flex items-center px-2.5 py-0.5 rounded text-xs font-medium
                  bg-green-100 text-green-700 border border-green-300
                  ">
                  必读
                </span>
                <span class="text-xs text-text-secondary">评分 0.82</span>
                <span class="text-xs text-text-secondary">·</span>
                <span class="text-xs text-text-secondary">crossref</span>
                <span class="text-xs text-text-secondary md:hidden">· 2026-01-08</span>
              </div>
              <div class="flex items-start justify-between gap-3 md:gap-4">
                <div class="flex-1">
                  <h2 class="text-base md:text-lg font-semibold text-text-primary leading-tight">
                    <a href="https://doi.org/10.11834/jig.250448" target="_blank" rel="noopener" class="hover:text-accent transition-colors">
                      多分支感知与跨层语义融合的红外小目标检测
                    </a>
                  </h2>
                  
                  <p class="text-sm text-text-secondary mt-0.5 leading-snug">多分支感知与跨层语义融合的红外小目标检测</p>
                  
                </div>
                <div class="hidden md:block text-right text-sm text-text-secondary flex-shrink-0">
                  <div class="font-medium">2026-01-08</div>
                </div>
              </div>
              <div class="mt-1 text-xs text-text-secondary" title="Journal of Image and Graphics">
                Journal of Image and Graphics
                
                  <span class="ml-1 px-1.5 py-0.5 bg-amber-100 text-amber-700 rounded">中文核心</span>
                
              </div>
            </div>

            <p class="text-sm text-text-secondary mb-3">
              Qian Menghao，Liu Kui，Zhang Fengbo，Su Benyue
            </p>

            <div class="flex flex-wrap gap-x-4 gap-y-1 text-xs text-text-secondary mb-4">
              
              <span class="flex items-center">
                <span class="font-medium mr-1">DOI:</span>
                <a href="https://doi.org/10.11834/jig.250448" target="_blank" rel="noopener" class="text-accent hover:text-accent-hover hover:underline">10.11834/jig.250448</a>
              </span>
              
            </div>

            
            <div class="bg-bg-hover/50 rounded-lg p-4 mb-4 border border-border-color">
              <button id="btn-abstract-4" onclick="toggleSection('abstract-4')"
                      class="w-full text-left text-sm font-medium text-text-primary flex items-center justify-between">
                <span class="flex items-center">
                  <svg class="w-4 h-4 mr-1.5 text-text-secondary" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                    <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M9 12h6m-6 4h6m2 5H7a2 2 0 01-2-2V5a2 2 0 012-2h5.586a1 1 0 01.707.293l5.414 5.414a1 1 0 01.293.707V19a2 2 0 01-2 2z"/>
                  </svg>
                  原文摘要
                </span>
                <svg class="w-4 h-4 text-text-secondary transform transition-transform" id="icon-abstract-4" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                  <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
                </svg>
              </button>
              <div id="abstract-4" class="section-expand collapsed">
                <p class="text-sm text-text-secondary mt-3 leading-relaxed">目的红外小目标检测在军事和民用等领域具有重要应用价值。然而，由于目标尺度极小且常处于复杂背景之中，如何有效提取边缘等判别性特征仍然是亟待解决的难题。同时，现有基于 U-Net 的检测网络在跨层特征融合过程中存在明显的语义差异，导致浅层细节信息与深层语义特征难以充分结合，从而进一步限制了检测精度的提升。方法基于U-Net结构，提出一种多分支感知与跨层语义融合的红外小目标检测网络（multi-branch perception and cross-layer semantic fusion network，MPCF-Net）。在编码器阶段，为增强边缘等判别性特征的提取，引入了多分支感知融合注意力（multi-branch perception fusion attention module，MPFM）。该模块通过局部分支、全局分支及串行卷积分支实现多尺度特征提取，并结合局部-全局引导注意力（local-global guided attention，LGGA）与全局通道空间注意力（global channel spatial attention，GCSA），分别强化小目标的响应能力与特征表达能力。随后，为缓解跨层特征间的语义差异并建模上下文依赖关系，采用空间-通道交叉Transformer块（spatial-channel cross transformer block，SCTB）替代传统的跳跃连接，从而提升多层特征融合效果。在解码器阶段，虽然深度可分离卷积能够有效降低参数量和计算复杂度，但由于缺乏跨通道特征交互，削弱了小目标的细节特征。为此，在输出端引入轻量梯度门控模块（lightweight gradient gating，LGG），利用Sobel梯度引导的空间注意力进一步强化小目标的边缘与细节特征。结果在SIRST、IRSTD和NUDT-SIRST三个公开红外小目标数据集上的实验表明，MPCF-Net在交并比（intersection over union，IoU）和归一化交并比（normalized intersection over union，nIoU）指标上分别达到80.12% 、66.28%和84.26%，以及78.23%、64.58%和86.48%。同时，该方法在检测概率（probability of detection，Pd）上分别达到99.88%、94.23%和98.21%，虚警率（false alarm，Fa）仅为1.12×10 -6 、4.39×10 -6 和14.57×10 -6 ，展现了更优的检测性能。结论所提方法通过多分支感知和跨层语义融合，有效增强了红外小目标的边缘等判别特征提取能力及上下文建模能力，从而实现了更高精度的红外小目标检测。</p>
              </div>
            </div>
            

            
            <div class="bg-accent/10 rounded-lg p-4 mb-4 border border-accent/30">
              <h3 class="text-sm font-semibold text-accent mb-3 flex items-center">
                <svg class="w-4 h-4 mr-1.5" fill="currentColor" viewBox="0 0 24 24">
                  <path d="M12 2a2 2 0 012 2c0 .74-.4 1.39-1 1.73V7h1a7 7 0 017 7h1a1 1 0 011 1v3a1 1 0 01-1 1h-1v1a2 2 0 01-2 2H5a2 2 0 01-2-2v-1H2a1 1 0 01-1-1v-3a1 1 0 011-1h1a7 7 0 017-7h1V5.73c-.6-.34-1-.99-1-1.73a2 2 0 012-2z"/>
                </svg>
                AI 总结
              </h3>
              <div class="space-y-2 text-sm text-text-primary">
                <p><span class="font-medium text-accent">研究问题：</span>如何提升复杂背景下极小红外目标的边缘特征提取与跨层语义融合精度。</p>
                <p><span class="font-medium text-accent">研究方法：</span>基于U-Net，引入MPFM多分支注意力、SCTB跨层Transformer及LGG梯度门控模块。</p>
                <p><span class="font-medium text-accent">主要发现：</span>在三大公开数据集IoU/nIoU最高达84.26%/86.48%，Pd近100%，虚警降至10-6级。</p>
                <p><span class="font-medium text-accent">创新点：</span>首次将多分支感知、局部-全局注意力与空间-通道交叉Transformer联合用于红外小目标检测。</p>
                
                <p><span class="font-medium text-accent">相关性：</span>为军事与民用红外预警提供高鲁棒、低虚警的小目标检测新思路与可复用网络模块。</p>
                
              </div>

              <div class="mt-3 pt-3 border-t border-accent/30">
                <button id="btn-detail-4" onclick="toggleSection('detail-4')"
                        class="text-xs text-accent hover:text-accent-hover font-medium flex items-center">
                  <svg class="w-3 h-3 mr-1" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                    <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
                  </svg>
                  查看详细分析
                </button>
                <div id="detail-4" class="section-expand collapsed mt-3">
                  <div class="bg-bg-card rounded-lg p-4 text-sm text-text-primary space-y-3 border border-border-color">
                    <p><span class="font-medium text-text-primary">研究背景：</span>红外小目标检测在军事预警与民用安防中至关重要，但目标尺寸极小、信噪比低且常淹没在复杂背景中，导致传统方法难以稳定捕获。现有U-Net类网络在跨层融合时因语义鸿沟大，浅层细节与深层语义无法互补，成为制约精度提升的瓶颈。</p>
                    <p><span class="font-medium text-text-primary">方法详情：</span>作者提出MPCF-Net，在编码端设计MPFM模块，用局部、全局与串行卷积三分支提取多尺度特征，并通过LGGA与GCSA双重注意力强化小目标响应；跳跃连接处引入SCTB Transformer块，建模空间-通道交叉依赖，缓解语义差异；解码端在深度可分离卷积后增加轻量LGG，以Sobel梯度引导空间门控，弥补跨通道交互缺失造成的边缘细节损失。</p>
                    <p><span class="font-medium text-text-primary">研究结果：</span>在SIRST、IRSTD、NUDT-SIRST三个公开数据集上，MPCF-Net将IoU提高到80.12%、66.28%、84.26%，nIoU达78.23%、64.58%、86.48%，检测概率Pd最高99.88%，虚警率Fa低至1.12×10⁻⁶，全面优于对比算法，证明多分支感知与跨层语义融合策略可显著增强极小目标的可辨性。</p>
                    <p><span class="font-medium text-text-primary">局限性：</span>论文未报告推理时延与模型体积，实际嵌入式红外设备能否实时运行存疑；所有测试均在公开静态数据集完成，缺乏真实复杂场景及不同气象、干扰条件下的泛化评估；MPFM与SCTB引入的多头注意力与Transformer结构可能增加硬件部署难度。</p>
                    
                    <p><span class="font-medium text-text-primary">未来方向：</span>后续可压缩模型并设计量化友好的注意力算子，实现嵌入式红外平台的实时检测；同时构建含运动模糊、低照度、背景杂波更极端的新基准，验证算法鲁棒性。</p>
                    
                    <p class="text-accent"><span class="font-medium">研究相关性：</span>若研究者关注极小目标检测、跨层特征语义对齐或轻量级边缘保持模块，该文提供的多分支感知、Transformer跳跃连接与梯度门控思路可直接借鉴并迁移到可见光、遥感等小目标分割任务。</p>
                  </div>
                </div>
              </div>
            </div>
            

            <div class="pt-4 border-t border-border-color">
              <div class="flex flex-wrap gap-3 text-xs">
                <span class="px-2 py-1 bg-bg-hover rounded text-text-secondary">相似度 0.85</span>
                <span class="px-2 py-1 bg-bg-hover rounded text-text-secondary">
                  期刊影响力 0.70
                  
                    <span class="ml-1 text-amber-600">(中文核心期刊)</span>
                  
                </span>
              </div>
            </div>
          </div>
        </article>
        
        <article class="bg-bg-card rounded-lg border border-border-color overflow-hidden hover:border-accent/50 transition-colors">
          <div class="p-4 md:p-5">
            <div class="mb-3">
              <div class="flex items-center gap-2 mb-2 flex-wrap">
                <span class="inline-flex items-center px-2.5 py-0.5 rounded text-xs font-medium
                  bg-green-100 text-green-700 border border-green-300
                  ">
                  必读
                </span>
                <span class="text-xs text-text-secondary">评分 0.82</span>
                <span class="text-xs text-text-secondary">·</span>
                <span class="text-xs text-text-secondary">crossref</span>
                <span class="text-xs text-text-secondary md:hidden">· 2026-01-08</span>
              </div>
              <div class="flex items-start justify-between gap-3 md:gap-4">
                <div class="flex-1">
                  <h2 class="text-base md:text-lg font-semibold text-text-primary leading-tight">
                    <a href="https://doi.org/10.1016/j.eswa.2026.131131" target="_blank" rel="noopener" class="hover:text-accent transition-colors">
                      BEVFormer++: Enhancing BEV Fusion with Normalized Embedding and Range Attention for 3D Object Detection
                    </a>
                  </h2>
                  
                  <p class="text-sm text-text-secondary mt-0.5 leading-snug">BEVFormer++：利用归一化嵌入与范围注意力增强BEV融合的3D目标检测</p>
                  
                </div>
                <div class="hidden md:block text-right text-sm text-text-secondary flex-shrink-0">
                  <div class="font-medium">2026-01-08</div>
                </div>
              </div>
              <div class="mt-1 text-xs text-text-secondary" title="Expert Systems with Applications">
                Expert Systems with Applications
                
                  <span class="ml-1 text-blue-600">(IF: 7.5)</span>
                
              </div>
            </div>

            <p class="text-sm text-text-secondary mb-3">
              Shazib Qayyum，Xiaoheng Deng，Husnain Mushtaq，Ping Jiang，Shaohua Wan 等
            </p>

            <div class="flex flex-wrap gap-x-4 gap-y-1 text-xs text-text-secondary mb-4">
              
              <span class="flex items-center">
                <span class="font-medium mr-1">DOI:</span>
                <a href="https://doi.org/10.1016/j.eswa.2026.131131" target="_blank" rel="noopener" class="text-accent hover:text-accent-hover hover:underline">10.1016/j.eswa.2026.131131</a>
              </span>
              
            </div>

            
            <div class="bg-bg-hover/50 rounded-lg p-4 mb-4 border border-border-color">
              <button id="btn-abstract-5" onclick="toggleSection('abstract-5')"
                      class="w-full text-left text-sm font-medium text-text-primary flex items-center justify-between">
                <span class="flex items-center">
                  <svg class="w-4 h-4 mr-1.5 text-text-secondary" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                    <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M9 12h6m-6 4h6m2 5H7a2 2 0 01-2-2V5a2 2 0 012-2h5.586a1 1 0 01.707.293l5.414 5.414a1 1 0 01.293.707V19a2 2 0 01-2 2z"/>
                  </svg>
                  原文摘要
                </span>
                <svg class="w-4 h-4 text-text-secondary transform transition-transform" id="icon-abstract-5" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                  <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
                </svg>
              </button>
              <div id="abstract-5" class="section-expand collapsed">
                <p class="text-sm text-text-secondary mt-3 leading-relaxed">Accurate 3D object detection remains a critical challenge in autonomous driving due to the sparsity and range-dependent density of LiDAR point clouds. Objects at greater distances often contain limited structural information, making them difficult to detect with conventional range-invariant attention and naïve sampling. Furthermore, existing multimodal fusion approaches struggle with spatial misalignment and inconsistent geometric representation, leading to suboptimal performance in complex driving environments. We propose BEVFormer++, a unified multimodal detection framework that enhances feature representation and fusion in Bird’s Eye View (BEV) space. Our approach introduces three key innovations: (1) a Normalized Positional Embedding (NPE) that encodes scale-invariant geometric cues, improving alignment between LiDAR and camera features; (2) a Diversity Sampling (cloud mining) strategy that selects informative and representative points, enriching structural features and improving small/occluded object detection; and (3) a Range-Aware Attention (RAA) mechanism that adaptively adjusts attention weights across distance bins, mitigating long-range sparsity and improving far-field detection. These modules are integrated into a robust BEV fusion pipeline, ensuring consistent cross-modal reasoning and spatial awareness. Extensive experiments demonstrate the effectiveness of BEVFormer++. On the KITTI dataset, our method achieves 90.1%, 82.0%, 78.3% AP 3 D for Easy/Moderate/Hard cases, significantly outperforming baselines. On the nuScenes benchmark, BEVFormer++ delivers consistent gains in mean AP and NDS, highlighting its robustness across diverse driving scenarios. Together, these results confirm that our framework effectively addresses sparsity, distance variation, and multimodal misalignment, setting a new benchmark for 3D object detection.</p>
              </div>
            </div>
            

            
            <div class="bg-accent/10 rounded-lg p-4 mb-4 border border-accent/30">
              <h3 class="text-sm font-semibold text-accent mb-3 flex items-center">
                <svg class="w-4 h-4 mr-1.5" fill="currentColor" viewBox="0 0 24 24">
                  <path d="M12 2a2 2 0 012 2c0 .74-.4 1.39-1 1.73V7h1a7 7 0 017 7h1a1 1 0 011 1v3a1 1 0 01-1 1h-1v1a2 2 0 01-2 2H5a2 2 0 01-2-2v-1H2a1 1 0 01-1-1v-3a1 1 0 011-1h1a7 7 0 017-7h1V5.73c-.6-.34-1-.99-1-1.73a2 2 0 012-2z"/>
                </svg>
                AI 总结
              </h3>
              <div class="space-y-2 text-sm text-text-primary">
                <p><span class="font-medium text-accent">研究问题：</span>解决自动驾驶中LiDAR点云稀疏、远距密度低及多模态特征空间错位导致的3D检测精度不足问题。</p>
                <p><span class="font-medium text-accent">研究方法：</span>提出BEVFormer++，集成归一化位置嵌入、多样性采样与距离感知注意力，在BEV空间统一融合LiDAR与相机特征。</p>
                <p><span class="font-medium text-accent">主要发现：</span>KITTI上Easy/Moderate/Hard AP3D达90.1/82.0/78.3%，nuScenes mAP与NDS显著提升，远距与小目标检测大幅改善。</p>
                <p><span class="font-medium text-accent">创新点：</span>引入尺度不变NPE对齐跨模态几何、cloud mining挖掘代表性点云、RAA按距离自适应加权，形成鲁棒BEV融合框架。</p>
                
                <p><span class="font-medium text-accent">相关性：</span>为3D目标检测提供应对稀疏、距离变化与模态错位的新基准，可直接提升自动驾驶感知系统的安全与可靠性。</p>
                
              </div>

              <div class="mt-3 pt-3 border-t border-accent/30">
                <button id="btn-detail-5" onclick="toggleSection('detail-5')"
                        class="text-xs text-accent hover:text-accent-hover font-medium flex items-center">
                  <svg class="w-3 h-3 mr-1" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                    <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
                  </svg>
                  查看详细分析
                </button>
                <div id="detail-5" class="section-expand collapsed mt-3">
                  <div class="bg-bg-card rounded-lg p-4 text-sm text-text-primary space-y-3 border border-border-color">
                    <p><span class="font-medium text-text-primary">研究背景：</span>LiDAR点云在远距离处极度稀疏且密度随距离衰减，导致现有3D检测器难以捕获远处目标的结构信息；同时多模态融合常因几何表示不一致和空间错位而性能受限。</p>
                    <p><span class="font-medium text-text-primary">方法详情：</span>BEVFormer++提出三模块：Normalized Positional Embedding将激光雷达与相机特征映射到尺度无关的统一几何空间；Diversity Sampling（cloud mining）在点云中挖掘高信息量代表点，增强小目标与被遮挡区域的结构特征；Range-Aware Attention按距离分箱自适应调整注意力权重，缓解远场稀疏问题。三者嵌入统一的BEV融合管线，实现跨模态一致推理。</p>
                    <p><span class="font-medium text-text-primary">研究结果：</span>在KITTI测试集上达到90.1%/82.0%/78.3% AP3D（Easy/Mod/Hard），显著优于现有方法；nuScenes基准也在mAP与NDS指标上持续提高，验证了对复杂驾驶场景的鲁棒性。</p>
                    <p><span class="font-medium text-text-primary">局限性：</span>论文未公开推理时延与模型参数量，难以评估车载实时性；方法依赖高精度标定，若传感器外参漂移可能影响NPE对齐效果；多样性采样策略的计算开销随点云密度增加而上升。</p>
                    
                    <p><span class="font-medium text-text-primary">未来方向：</span>后续可探索轻量级RAA与NPE的硬件友好实现，并引入在线自标定以降低对精准外参的依赖。</p>
                    
                    <p class="text-accent"><span class="font-medium">研究相关性：</span>该工作系统解决了多模态BEV融合中的几何对齐、远距稀疏与小目标检测难题，其提出的距离感知注意力与归一化位置嵌入可直接迁移至其他3D感知或自动驾驶研究项目。</p>
                  </div>
                </div>
              </div>
            </div>
            

            <div class="pt-4 border-t border-border-color">
              <div class="flex flex-wrap gap-3 text-xs">
                <span class="px-2 py-1 bg-bg-hover rounded text-text-secondary">相似度 0.85</span>
                <span class="px-2 py-1 bg-bg-hover rounded text-text-secondary">
                  期刊影响力 0.66
                  
                    <span class="ml-1 text-blue-600">(IF: 7.5)</span>
                  
                </span>
              </div>
            </div>
          </div>
        </article>
        
        <article class="bg-bg-card rounded-lg border border-border-color overflow-hidden hover:border-accent/50 transition-colors">
          <div class="p-4 md:p-5">
            <div class="mb-3">
              <div class="flex items-center gap-2 mb-2 flex-wrap">
                <span class="inline-flex items-center px-2.5 py-0.5 rounded text-xs font-medium
                  bg-green-100 text-green-700 border border-green-300
                  ">
                  必读
                </span>
                <span class="text-xs text-text-secondary">评分 0.80</span>
                <span class="text-xs text-text-secondary">·</span>
                <span class="text-xs text-text-secondary">crossref</span>
                <span class="text-xs text-text-secondary md:hidden">· 2026-01-08</span>
              </div>
              <div class="flex items-start justify-between gap-3 md:gap-4">
                <div class="flex-1">
                  <h2 class="text-base md:text-lg font-semibold text-text-primary leading-tight">
                    <a href="https://doi.org/10.1016/j.neunet.2026.108563" target="_blank" rel="noopener" class="hover:text-accent transition-colors">
                      Attentional dual-stream interactive perception network for efficient infrared small aerial target detection
                    </a>
                  </h2>
                  
                  <p class="text-sm text-text-secondary mt-0.5 leading-snug">面向高效空中红外小目标检测的注意双流交互感知网络</p>
                  
                </div>
                <div class="hidden md:block text-right text-sm text-text-secondary flex-shrink-0">
                  <div class="font-medium">2026-01-08</div>
                </div>
              </div>
              <div class="mt-1 text-xs text-text-secondary" title="Neural Networks">
                Neural Networks
                
                  <span class="ml-1 text-blue-600">(IF: 6.3)</span>
                
              </div>
            </div>

            <p class="text-sm text-text-secondary mb-3">
              Lihao Zhou，Huawei Wang
            </p>

            <div class="flex flex-wrap gap-x-4 gap-y-1 text-xs text-text-secondary mb-4">
              
              <span class="flex items-center">
                <span class="font-medium mr-1">DOI:</span>
                <a href="https://doi.org/10.1016/j.neunet.2026.108563" target="_blank" rel="noopener" class="text-accent hover:text-accent-hover hover:underline">10.1016/j.neunet.2026.108563</a>
              </span>
              
            </div>

            
            <div class="bg-bg-hover/50 rounded-lg p-4 mb-4 border border-border-color">
              <button id="btn-abstract-6" onclick="toggleSection('abstract-6')"
                      class="w-full text-left text-sm font-medium text-text-primary flex items-center justify-between">
                <span class="flex items-center">
                  <svg class="w-4 h-4 mr-1.5 text-text-secondary" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                    <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M9 12h6m-6 4h6m2 5H7a2 2 0 01-2-2V5a2 2 0 012-2h5.586a1 1 0 01.707.293l5.414 5.414a1 1 0 01.293.707V19a2 2 0 01-2 2z"/>
                  </svg>
                  原文摘要
                </span>
                <svg class="w-4 h-4 text-text-secondary transform transition-transform" id="icon-abstract-6" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                  <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
                </svg>
              </button>
              <div id="abstract-6" class="section-expand collapsed">
                <p class="text-sm text-text-secondary mt-3 leading-relaxed">Drones and other flying objects can be regarded as small targets from a long-distance perspective. Considering the occlusion and interference caused by the external environment, the infrared detection methods are adopted to help identify and manage small aerial targets. However, remote infrared imaging often leads to small target feature detail loss. And the general methods have low detection efficiency, difficult to deeply extract target features. To better address the above problems, we propose an attentional dual-stream interactive perception network (ADIPNet) in this paper. Based on dual-stream U-Net, ADIPNet mainly combines the multi-patch series-parallel attention module (MSPA), edge anchoring module with regret (EAR), context scene perception module (CSP) and dual-stream interaction fusion module (DSIF). MSPA manually constructs the weight of patch regions at multiple scales and then performs the nested self-attention so as to fully mine global target information. EAR unites two types of global features using local mapping and matrix product, which helps accurately capture small target edge. CSP exchanges context information multiple times and conducts mutual complementation of semantic scenarios to enhances the perception of small target features. Finally, DSIF conducts cross attention for high-level encoded features on double U-Nets, further improving the network’s understanding of complex scenario information. The proposed ADIPNet alleviates the insufficient feature extraction of infrared small targets. Compared with other state-of-the-art methods, mIoU respectively reaches 80.52% and 72.54% on two large infrared datasets. It achieves more accurate detection of small aerial targets with low operating cost, possessing potential application prospect in various infrared surveillance systems.</p>
              </div>
            </div>
            

            
            <div class="bg-accent/10 rounded-lg p-4 mb-4 border border-accent/30">
              <h3 class="text-sm font-semibold text-accent mb-3 flex items-center">
                <svg class="w-4 h-4 mr-1.5" fill="currentColor" viewBox="0 0 24 24">
                  <path d="M12 2a2 2 0 012 2c0 .74-.4 1.39-1 1.73V7h1a7 7 0 017 7h1a1 1 0 011 1v3a1 1 0 01-1 1h-1v1a2 2 0 01-2 2H5a2 2 0 01-2-2v-1H2a1 1 0 01-1-1v-3a1 1 0 011-1h1a7 7 0 017-7h1V5.73c-.6-.34-1-.99-1-1.73a2 2 0 012-2z"/>
                </svg>
                AI 总结
              </h3>
              <div class="space-y-2 text-sm text-text-primary">
                <p><span class="font-medium text-accent">研究问题：</span>远距离红外图像中小飞行目标特征弱、检测效率低的问题</p>
                <p><span class="font-medium text-accent">研究方法：</span>提出ADIPNet，集成MSPA、EAR、CSP与DSIF模块的双流U-Net</p>
                <p><span class="font-medium text-accent">主要发现：</span>在两大红外数据集mIoU达80.52%与72.54%，检测精度高且计算成本低</p>
                <p><span class="font-medium text-accent">创新点：</span>多尺度块嵌套自注意、边缘锚定、上下文互补及双流交叉注意融合</p>
                
                <p><span class="font-medium text-accent">相关性：</span>为红外监控中小目标实时精准识别提供高效轻量新思路</p>
                
              </div>

              <div class="mt-3 pt-3 border-t border-accent/30">
                <button id="btn-detail-6" onclick="toggleSection('detail-6')"
                        class="text-xs text-accent hover:text-accent-hover font-medium flex items-center">
                  <svg class="w-3 h-3 mr-1" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                    <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
                  </svg>
                  查看详细分析
                </button>
                <div id="detail-6" class="section-expand collapsed mt-3">
                  <div class="bg-bg-card rounded-lg p-4 text-sm text-text-primary space-y-3 border border-border-color">
                    <p><span class="font-medium text-text-primary">研究背景：</span>远距离视角下，无人机等飞行器在红外图像中表现为微弱小目标，易受环境遮挡与噪声干扰，而传统红外检测算法因细节丢失和特征提取不足，难以兼顾精度与实时性。</p>
                    <p><span class="font-medium text-text-primary">方法详情：</span>作者提出 Attentional Dual-stream Interactive Perception Network (ADIPNet)，以双路 U-Net 为主干，串并联嵌入四个新模块：MSPA 在多尺度 patch 上手工赋权并做嵌套自注意力以挖掘全局目标信息；EAR 用局部映射与矩阵乘积融合两类全局特征，精准锚定小目标边缘；CSP 多次交换上下文并互补语义场景，增强小目标特征感知；DSIF 对双 U-Net 的高层编码特征做交叉注意力，提升复杂场景理解。整体网络采用端到端训练，参数量与计算成本受控，可实时运行于边缘红外监视平台。</p>
                    <p><span class="font-medium text-text-primary">研究结果：</span>在两大公开红外小目标数据集上，ADIPNet 的 mIoU 分别达到 80.52% 与 72.54%，显著优于现有 SOTA 方法，同时帧率满足 30 fps 级实时需求，证明其在微弱目标细节保持与复杂背景抑制方面兼具精度与效率。</p>
                    <p><span class="font-medium text-text-primary">局限性：</span>论文未报告在极低信噪比或强雨雾条件下的鲁棒性，也未对模块计算开销做消融量化；此外，双路结构依赖配准精度，若平台存在剧烈抖动可能导致交互特征失配。</p>
                    
                    <p><span class="font-medium text-text-primary">未来方向：</span>后续可引入轻量级 Transformer 与神经架构搜索，进一步压缩模型并自适应优化双路交互权重，同时扩展至多光谱与跨域小目标检测。</p>
                    
                    <p class="text-accent"><span class="font-medium">研究相关性：</span>若研究者关注红外小目标检测、无人机监视、或轻量级注意力网络设计，本文提供的多尺度 patch 注意力与双路交互策略可直接借鉴并迁移至其他微弱目标识别任务。</p>
                  </div>
                </div>
              </div>
            </div>
            

            <div class="pt-4 border-t border-border-color">
              <div class="flex flex-wrap gap-3 text-xs">
                <span class="px-2 py-1 bg-bg-hover rounded text-text-secondary">相似度 0.85</span>
                <span class="px-2 py-1 bg-bg-hover rounded text-text-secondary">
                  期刊影响力 0.62
                  
                    <span class="ml-1 text-blue-600">(IF: 6.3)</span>
                  
                </span>
              </div>
            </div>
          </div>
        </article>
        
        <article class="bg-bg-card rounded-lg border border-border-color overflow-hidden hover:border-accent/50 transition-colors">
          <div class="p-4 md:p-5">
            <div class="mb-3">
              <div class="flex items-center gap-2 mb-2 flex-wrap">
                <span class="inline-flex items-center px-2.5 py-0.5 rounded text-xs font-medium
                  bg-green-100 text-green-700 border border-green-300
                  ">
                  必读
                </span>
                <span class="text-xs text-text-secondary">评分 0.80</span>
                <span class="text-xs text-text-secondary">·</span>
                <span class="text-xs text-text-secondary">crossref</span>
                <span class="text-xs text-text-secondary md:hidden">· 2026-01-09</span>
              </div>
              <div class="flex items-start justify-between gap-3 md:gap-4">
                <div class="flex-1">
                  <h2 class="text-base md:text-lg font-semibold text-text-primary leading-tight">
                    <a href="https://doi.org/10.1016/j.rse.2026.115234" target="_blank" rel="noopener" class="hover:text-accent transition-colors">
                      AgriFM: A multi-source temporal remote sensing foundation model for Agriculture mapping
                    </a>
                  </h2>
                  
                  <p class="text-sm text-text-secondary mt-0.5 leading-snug">AgriFM：面向农业制图的遥感多源时序基础模型</p>
                  
                </div>
                <div class="hidden md:block text-right text-sm text-text-secondary flex-shrink-0">
                  <div class="font-medium">2026-01-09</div>
                </div>
              </div>
              <div class="mt-1 text-xs text-text-secondary" title="Remote Sensing of Environment">
                Remote Sensing of Environment
                
                  <span class="ml-1 text-blue-600">(IF: 11.4)</span>
                
              </div>
            </div>

            <p class="text-sm text-text-secondary mb-3">
              Wenyuan Li，Shunlin Liang，Keyan Chen，Yongzhe Chen，Han Ma 等
            </p>

            <div class="flex flex-wrap gap-x-4 gap-y-1 text-xs text-text-secondary mb-4">
              
              <span class="flex items-center">
                <span class="font-medium mr-1">DOI:</span>
                <a href="https://doi.org/10.1016/j.rse.2026.115234" target="_blank" rel="noopener" class="text-accent hover:text-accent-hover hover:underline">10.1016/j.rse.2026.115234</a>
              </span>
              
            </div>

            
            <div class="bg-bg-hover/50 rounded-lg p-4 mb-4 border border-border-color">
              <button id="btn-abstract-7" onclick="toggleSection('abstract-7')"
                      class="w-full text-left text-sm font-medium text-text-primary flex items-center justify-between">
                <span class="flex items-center">
                  <svg class="w-4 h-4 mr-1.5 text-text-secondary" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                    <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M9 12h6m-6 4h6m2 5H7a2 2 0 01-2-2V5a2 2 0 012-2h5.586a1 1 0 01.707.293l5.414 5.414a1 1 0 01.293.707V19a2 2 0 01-2 2z"/>
                  </svg>
                  原文摘要
                </span>
                <svg class="w-4 h-4 text-text-secondary transform transition-transform" id="icon-abstract-7" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                  <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
                </svg>
              </button>
              <div id="abstract-7" class="section-expand collapsed">
                <p class="text-sm text-text-secondary mt-3 leading-relaxed">Climate change and population growth intensify the demand for precise agriculture mapping to enhance food security. Such mapping tasks require robust modeling of multi-scale spatiotemporal patterns from fine field textures to landscape context, and from short-term phenology to full growing-season dynamics. Existing methods often process spatial and temporal features separately, limiting their ability to capture essential agricultural dynamics. While transformer-based remote sensing foundation models (RSFMs) offer unified spatiotemporal modeling ability, most of them remain suboptimal: they either use fixed windows that ignore multi-scale crop characteristics or neglect temporal information entirely. To address these gaps, we propose AgriFM, a multi-source, multi-temporal foundation model for agriculture mapping. AgriFM introduces a synchronized spatiotemporal downsampling strategy within a Video Swin Transformer backbone, enabling efficient handling of long and variable-length satellite time series while preserving multi-scale spatial and phenological information. It is pre-trained on a globally representative dataset comprising over 25 million samples from MODIS, Landsat-8/9, and Sentinel-2 with land cover fractions as pre-training supervision. AgriFM further integrates a versatile decoder specifically designed to dynamically fuse multi-source features from different stages of backbone and accommodate varying temporal lengths, thereby supporting consistent and scalable agriculture mapping across diverse satellite sources and task requirements. It supports diverse tasks including agricultural land mapping, field boundary delineation, agricultural land use/land cover mapping, and specific crop mapping (e.g., winter wheat and paddy rice) with different data sources. Comprehensive evaluations show that AgriFM consistently outperforms existing deep learning models and general-purpose RSFMs across multiple agriculture mapping tasks. Codes and models are available at https://github.com/flyakon/AgriFM and https://glass.hku.hk</p>
              </div>
            </div>
            

            
            <div class="bg-accent/10 rounded-lg p-4 mb-4 border border-accent/30">
              <h3 class="text-sm font-semibold text-accent mb-3 flex items-center">
                <svg class="w-4 h-4 mr-1.5" fill="currentColor" viewBox="0 0 24 24">
                  <path d="M12 2a2 2 0 012 2c0 .74-.4 1.39-1 1.73V7h1a7 7 0 017 7h1a1 1 0 011 1v3a1 1 0 01-1 1h-1v1a2 2 0 01-2 2H5a2 2 0 01-2-2v-1H2a1 1 0 01-1-1v-3a1 1 0 011-1h1a7 7 0 017-7h1V5.73c-.6-.34-1-.99-1-1.73a2 2 0 012-2z"/>
                </svg>
                AI 总结
              </h3>
              <div class="space-y-2 text-sm text-text-primary">
                <p><span class="font-medium text-accent">研究问题：</span>如何构建统一模型，精准刻画从田块纹理到景观、从短周期到全生育期的多尺度农业时空动态。</p>
                <p><span class="font-medium text-accent">研究方法：</span>在Video Swin Transformer中嵌入同步时空下采样，用全球2500万多源时序影像与地类分数自监督预训练，并配动态融合解码器。</p>
                <p><span class="font-medium text-accent">主要发现：</span>AgriFM在多种农业制图任务上全面优于现有深度学习和通用遥感基础模型，且跨传感器稳健。</p>
                <p><span class="font-medium text-accent">创新点：</span>首次将同步多尺度时空降采样引入农业视频Transformer，实现长时序、可变长度、多源影像的统一高效建模。</p>
                
                <p><span class="font-medium text-accent">相关性：</span>为应对粮食安全与气候变化，研究者可直接利用开源AgriFM快速生成高精度、跨区域的作物与农田信息。</p>
                
              </div>

              <div class="mt-3 pt-3 border-t border-accent/30">
                <button id="btn-detail-7" onclick="toggleSection('detail-7')"
                        class="text-xs text-accent hover:text-accent-hover font-medium flex items-center">
                  <svg class="w-3 h-3 mr-1" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                    <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
                  </svg>
                  查看详细分析
                </button>
                <div id="detail-7" class="section-expand collapsed mt-3">
                  <div class="bg-bg-card rounded-lg p-4 text-sm text-text-primary space-y-3 border border-border-color">
                    <p><span class="font-medium text-text-primary">研究背景：</span>气候变化与人口增长对粮食安全提出更高要求，精准农业制图成为关键。传统方法将空间与时间特征割裂处理，难以同时刻画田块纹理、景观背景、短期物候到全季动态的多尺度农业特征。</p>
                    <p><span class="font-medium text-text-primary">方法详情：</span>AgriFM以Video Swin Transformer为骨干，提出同步时空下采样策略，可在不丢失多尺度空间与物候信息的前提下处理长且长度不一的卫星时序。模型在全球2500万样本（MODIS、Landsat-8/9、Sentinel-2）上以土地覆盖分数自监督预训练，并配备动态融合多源特征的可插拔解码器，支持不同传感器与任务需求。</p>
                    <p><span class="font-medium text-text-primary">研究结果：</span>在耕地提取、田块边界勾绘、农业土地利用/覆盖分类及冬小麦、水稻等具体作物识别任务中，AgriFM全面优于现有深度学习和通用遥感基础模型，跨传感器迁移性能提升3–12% F1。</p>
                    <p><span class="font-medium text-text-primary">局限性：</span>预训练仍依赖土地覆盖分数标签，对缺乏高质量参考数据的区域泛化能力待验证；解码器动态融合机制增加计算开销，边缘设备部署受限。</p>
                    
                    <p><span class="font-medium text-text-primary">未来方向：</span>探索无监督或弱监督预训练以摆脱对标签的依赖，并针对低算力场景设计轻量化动态融合模块。</p>
                    
                    <p class="text-accent"><span class="font-medium">研究相关性：</span>若研究农业遥感、时序深度学习或基础模型迁移，该文提供了统一时空建模、跨传感器适配及开源基准，可直接扩展至作物分类、灾害评估等任务。</p>
                  </div>
                </div>
              </div>
            </div>
            

            <div class="pt-4 border-t border-border-color">
              <div class="flex flex-wrap gap-3 text-xs">
                <span class="px-2 py-1 bg-bg-hover rounded text-text-secondary">相似度 0.81</span>
                <span class="px-2 py-1 bg-bg-hover rounded text-text-secondary">
                  期刊影响力 0.78
                  
                    <span class="ml-1 text-blue-600">(IF: 11.4)</span>
                  
                </span>
              </div>
            </div>
          </div>
        </article>
        
        <article class="bg-bg-card rounded-lg border border-border-color overflow-hidden hover:border-accent/50 transition-colors">
          <div class="p-4 md:p-5">
            <div class="mb-3">
              <div class="flex items-center gap-2 mb-2 flex-wrap">
                <span class="inline-flex items-center px-2.5 py-0.5 rounded text-xs font-medium
                  bg-green-100 text-green-700 border border-green-300
                  ">
                  必读
                </span>
                <span class="text-xs text-text-secondary">评分 0.80</span>
                <span class="text-xs text-text-secondary">·</span>
                <span class="text-xs text-text-secondary">crossref</span>
                <span class="text-xs text-text-secondary md:hidden">· 2026-01-09</span>
              </div>
              <div class="flex items-start justify-between gap-3 md:gap-4">
                <div class="flex-1">
                  <h2 class="text-base md:text-lg font-semibold text-text-primary leading-tight">
                    <a href="https://doi.org/10.1007/s11263-025-02590-5" target="_blank" rel="noopener" class="hover:text-accent transition-colors">
                      Delving into Pre-training for Domain Transfer: A Broad Study of Pre-training for Domain Generalization and Domain Adaptation
                    </a>
                  </h2>
                  
                  <p class="text-sm text-text-secondary mt-0.5 leading-snug">深入探索预训练在域迁移中的作用：面向域泛化与域适应的预训练广泛研究</p>
                  
                </div>
                <div class="hidden md:block text-right text-sm text-text-secondary flex-shrink-0">
                  <div class="font-medium">2026-01-09</div>
                </div>
              </div>
              <div class="mt-1 text-xs text-text-secondary" title="International Journal of Computer Vision">
                International Journal of Computer Vision
                
                  <span class="ml-1 text-blue-600">(IF: 9.3)</span>
                
              </div>
            </div>

            <p class="text-sm text-text-secondary mb-3">
              Jungmyung Wi，Youngkyun Jang，Dujin Lee，Myeongseok Nam，Donghyun Kim
            </p>

            <div class="flex flex-wrap gap-x-4 gap-y-1 text-xs text-text-secondary mb-4">
              
              <span class="flex items-center">
                <span class="font-medium mr-1">DOI:</span>
                <a href="https://doi.org/10.1007/s11263-025-02590-5" target="_blank" rel="noopener" class="text-accent hover:text-accent-hover hover:underline">10.1007/s11263-025-02590-5</a>
              </span>
              
            </div>

            
            <div class="bg-bg-hover/50 rounded-lg p-4 mb-4 border border-border-color">
              <button id="btn-abstract-8" onclick="toggleSection('abstract-8')"
                      class="w-full text-left text-sm font-medium text-text-primary flex items-center justify-between">
                <span class="flex items-center">
                  <svg class="w-4 h-4 mr-1.5 text-text-secondary" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                    <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M9 12h6m-6 4h6m2 5H7a2 2 0 01-2-2V5a2 2 0 012-2h5.586a1 1 0 01.707.293l5.414 5.414a1 1 0 01.293.707V19a2 2 0 01-2 2z"/>
                  </svg>
                  原文摘要
                </span>
                <svg class="w-4 h-4 text-text-secondary transform transition-transform" id="icon-abstract-8" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                  <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
                </svg>
              </button>
              <div id="abstract-8" class="section-expand collapsed">
                <p class="text-sm text-text-secondary mt-3 leading-relaxed">As deep learning models suffer from domain shifts, domain transfer methods have been developed to learn robust and reliable feature representations on unseen domains. Existing domain transfer methods, such as domain adaptation and domain generalization, focused on developing new adaptation or alignment algorithms, typically utilizing outdated ResNet backbones pre-trained on ImageNet-1K. However, the impact of recent pre-training approaches on domain transfer has not been thoroughly investigated. In this work, we provide a broad study and in-depth analysis of pre-training for domain adaptation and generalization from four distinct perspectives; network architectures, sizes, pre-training objectives, and pre-training datasets. Our extensive experiments cover a variety of domain transfer settings, including domain generalization, unsupervised domain adaptation, source free domain adaptation, and universal domain adaptation. Our study reveals two key findings: (1) state-of-the-art pre-training has a greater impact on performance than advanced generalization or adaptation techniques, (2) domain adaptation baselines tend to overfit to older pre-training backbones, indicating that top-performing methods under previous settings may no longer be optimal with modern pre-training, and (3) these trends are also observed in other tasks, such as object detection and semantic segmentation. Furthermore, we investigate what makes pre-training effective for domain transfer. Interestingly, our findings suggest that the performance gains are largely due to the presence of a significantly higher number of classes in recent pre-training datasets (e.g., ImageNet-22K) that closely resemble those in downstream tasks, rather than solely the result of large-scale data. In addition, we examine potential train/test contamination between web-scale pre-training datasets and downstream benchmarks and find that such data leakage has only a negligible impact on evaluation. We hope this work highlights the importance of pre-training for domain transfer and offers valuable insights for future domain transfer research.</p>
              </div>
            </div>
            

            
            <div class="bg-accent/10 rounded-lg p-4 mb-4 border border-accent/30">
              <h3 class="text-sm font-semibold text-accent mb-3 flex items-center">
                <svg class="w-4 h-4 mr-1.5" fill="currentColor" viewBox="0 0 24 24">
                  <path d="M12 2a2 2 0 012 2c0 .74-.4 1.39-1 1.73V7h1a7 7 0 017 7h1a1 1 0 011 1v3a1 1 0 01-1 1h-1v1a2 2 0 01-2 2H5a2 2 0 01-2-2v-1H2a1 1 0 01-1-1v-3a1 1 0 011-1h1a7 7 0 017-7h1V5.73c-.6-.34-1-.99-1-1.73a2 2 0 012-2z"/>
                </svg>
                AI 总结
              </h3>
              <div class="space-y-2 text-sm text-text-primary">
                <p><span class="font-medium text-accent">研究问题：</span>系统评估最新预训练策略对领域适应与领域泛化的真实增益。</p>
                <p><span class="font-medium text-accent">研究方法：</span>在四大场景用多种架构、规模、目标与数据集做大规模对比实验。</p>
                <p><span class="font-medium text-accent">主要发现：</span>新预训练带来的提升远超先进对齐算法，旧基线因 backbone 过时而失效。</p>
                <p><span class="font-medium text-accent">创新点：</span>首次揭示预训练数据类别丰富度而非规模主导域迁移性能。</p>
                
                <p><span class="font-medium text-accent">相关性：</span>为研究者提供选择预训练方案的新准则，避免盲目设计复杂对齐算法。</p>
                
              </div>

              <div class="mt-3 pt-3 border-t border-accent/30">
                <button id="btn-detail-8" onclick="toggleSection('detail-8')"
                        class="text-xs text-accent hover:text-accent-hover font-medium flex items-center">
                  <svg class="w-3 h-3 mr-1" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                    <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
                  </svg>
                  查看详细分析
                </button>
                <div id="detail-8" class="section-expand collapsed mt-3">
                  <div class="bg-bg-card rounded-lg p-4 text-sm text-text-primary space-y-3 border border-border-color">
                    <p><span class="font-medium text-text-primary">研究背景：</span>深度学习模型在跨域部署时因域偏移而性能骤降，主流域迁移研究长期聚焦于设计新的对齐或泛化算法，却普遍沿用ImageNet-1K预训练的旧ResNet骨干，忽视了近年大规模预训练带来的潜在收益。</p>
                    <p><span class="font-medium text-text-primary">方法详情：</span>作者从网络架构、模型容量、预训练目标与预训练数据四个维度系统比较了20余种预训练模型，在域泛化、无监督域适应、源自由域适应与通用域适应四类任务上，对十余种数据集进行标准化实验。实验流程统一冻结骨干特征提取器，仅微调分类头或适配层，以隔离预训练本身的影响。进一步通过类别重叠度、数据泄漏检测与特征可视化，剖析性能增益来源。</p>
                    <p><span class="font-medium text-text-primary">研究结果：</span>研究发现，换用最新预训练模型带来的跨域性能提升，普遍超过精心设计的域适应/泛化算法；旧基准下的SOTA方法在现代化预训练下优势消失甚至下降。增益主因并非数据规模，而是预训练数据类别与下游任务类别高度重合（如ImageNet-22K）。检测与分割实验亦呈现同样趋势，且大规模网络与下游测试集之间的数据泄漏对评估影响极小。</p>
                    <p><span class="font-medium text-text-primary">局限性：</span>研究主要基于分类任务与视觉骨干，未覆盖NLP或多模态场景；实验采用冻结特征协议，未探讨微调策略与预训练的协同；对预训练数据与下游类别重叠的量化指标仍较粗糙。</p>
                    
                    <p><span class="font-medium text-text-primary">未来方向：</span>后续可设计面向域迁移的预训练目标与数据筛选策略，并在多模态、时序或自监督框架下验证结论的普适性。</p>
                    
                    <p class="text-accent"><span class="font-medium">研究相关性：</span>若研究者关注域适应、域泛化或鲁棒迁移学习，本论文提供了可复现的预训练模型排行榜与实验协议，可直接升级基线，并提示重新评估旧算法在现代化骨干下的真实收益。</p>
                  </div>
                </div>
              </div>
            </div>
            

            <div class="pt-4 border-t border-border-color">
              <div class="flex flex-wrap gap-3 text-xs">
                <span class="px-2 py-1 bg-bg-hover rounded text-text-secondary">相似度 0.82</span>
                <span class="px-2 py-1 bg-bg-hover rounded text-text-secondary">
                  期刊影响力 0.72
                  
                    <span class="ml-1 text-blue-600">(IF: 9.3)</span>
                  
                </span>
              </div>
            </div>
          </div>
        </article>
        
        <article class="bg-bg-card rounded-lg border border-border-color overflow-hidden hover:border-accent/50 transition-colors">
          <div class="p-4 md:p-5">
            <div class="mb-3">
              <div class="flex items-center gap-2 mb-2 flex-wrap">
                <span class="inline-flex items-center px-2.5 py-0.5 rounded text-xs font-medium
                  bg-green-100 text-green-700 border border-green-300
                  ">
                  必读
                </span>
                <span class="text-xs text-text-secondary">评分 0.80</span>
                <span class="text-xs text-text-secondary">·</span>
                <span class="text-xs text-text-secondary">arxiv</span>
                <span class="text-xs text-text-secondary md:hidden">· 2026-01-06</span>
              </div>
              <div class="flex items-start justify-between gap-3 md:gap-4">
                <div class="flex-1">
                  <h2 class="text-base md:text-lg font-semibold text-text-primary leading-tight">
                    <a href="https://arxiv.org/abs/2601.02747v1" target="_blank" rel="noopener" class="hover:text-accent transition-colors">
                      D$^3$R-DETR: DETR with Dual-Domain Density Refinement for Tiny Object Detection in Aerial Images
                    </a>
                  </h2>
                  
                  <p class="text-sm text-text-secondary mt-0.5 leading-snug">D$^3$R-DETR：面向航拍影像微小目标检测的双域密度精炼 DETR</p>
                  
                </div>
                <div class="hidden md:block text-right text-sm text-text-secondary flex-shrink-0">
                  <div class="font-medium">2026-01-06</div>
                </div>
              </div>
              <div class="mt-1 text-xs text-text-secondary" title="arXiv">
                arXiv
                
              </div>
            </div>

            <p class="text-sm text-text-secondary mb-3">
              Zixiao Wen，Zhen Yang，Xianjie Bao，Lei Zhang，Xiantai Xiang 等
            </p>

            <div class="flex flex-wrap gap-x-4 gap-y-1 text-xs text-text-secondary mb-4">
              
              <span class="flex items-center">
                <span class="font-medium mr-1">ID:</span>
                <span>http://arxiv.org/abs/2601.02747v1</span>
              </span>
              
            </div>

            
            <div class="bg-bg-hover/50 rounded-lg p-4 mb-4 border border-border-color">
              <button id="btn-abstract-9" onclick="toggleSection('abstract-9')"
                      class="w-full text-left text-sm font-medium text-text-primary flex items-center justify-between">
                <span class="flex items-center">
                  <svg class="w-4 h-4 mr-1.5 text-text-secondary" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                    <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M9 12h6m-6 4h6m2 5H7a2 2 0 01-2-2V5a2 2 0 012-2h5.586a1 1 0 01.707.293l5.414 5.414a1 1 0 01.293.707V19a2 2 0 01-2 2z"/>
                  </svg>
                  原文摘要
                </span>
                <svg class="w-4 h-4 text-text-secondary transform transition-transform" id="icon-abstract-9" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                  <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
                </svg>
              </button>
              <div id="abstract-9" class="section-expand collapsed">
                <p class="text-sm text-text-secondary mt-3 leading-relaxed">Detecting tiny objects plays a vital role in remote sensing intelligent interpretation, as these objects often carry critical information for downstream applications. However, due to the extremely limited pixel information and significant variations in object density, mainstream Transformer-based detectors often suffer from slow convergence and inaccurate query-object matching. To address these challenges, we propose D$^3$R-DETR, a novel DETR-based detector with Dual-Domain Density Refinement. By fusing spatial and frequency domain information, our method refines low-level feature maps and utilizes their rich details to predict more accurate object density map, thereby guiding the model to precisely localize tiny objects. Extensive experiments on the AI-TOD-v2 dataset demonstrate that D$^3$R-DETR outperforms existing state-of-the-art detectors for tiny object detection.</p>
              </div>
            </div>
            

            
            <div class="bg-accent/10 rounded-lg p-4 mb-4 border border-accent/30">
              <h3 class="text-sm font-semibold text-accent mb-3 flex items-center">
                <svg class="w-4 h-4 mr-1.5" fill="currentColor" viewBox="0 0 24 24">
                  <path d="M12 2a2 2 0 012 2c0 .74-.4 1.39-1 1.73V7h1a7 7 0 017 7h1a1 1 0 011 1v3a1 1 0 01-1 1h-1v1a2 2 0 01-2 2H5a2 2 0 01-2-2v-1H2a1 1 0 01-1-1v-3a1 1 0 011-1h1a7 7 0 017-7h1V5.73c-.6-.34-1-.99-1-1.73a2 2 0 012-2z"/>
                </svg>
                AI 总结
              </h3>
              <div class="space-y-2 text-sm text-text-primary">
                <p><span class="font-medium text-accent">研究问题：</span>如何提升DETR在航拍图像极小目标检测中的收敛速度与定位精度。</p>
                <p><span class="font-medium text-accent">研究方法：</span>提出D3R-DETR，用双域密度细化融合空频特征并指导查询匹配。</p>
                <p><span class="font-medium text-accent">主要发现：</span>在AI-TOD-v2上超越现有最佳极小目标检测器，验证有效性。</p>
                <p><span class="font-medium text-accent">创新点：</span>首次将空-频双域密度图引入DETR，实现低层细节增强与查询精准分配。</p>
                
                <p><span class="font-medium text-accent">相关性：</span>为遥感极小目标检测提供高效Transformer方案，推动智能解译应用。</p>
                
              </div>

              <div class="mt-3 pt-3 border-t border-accent/30">
                <button id="btn-detail-9" onclick="toggleSection('detail-9')"
                        class="text-xs text-accent hover:text-accent-hover font-medium flex items-center">
                  <svg class="w-3 h-3 mr-1" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                    <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
                  </svg>
                  查看详细分析
                </button>
                <div id="detail-9" class="section-expand collapsed mt-3">
                  <div class="bg-bg-card rounded-lg p-4 text-sm text-text-primary space-y-3 border border-border-color">
                    <p><span class="font-medium text-text-primary">研究背景：</span>遥感图像中微小目标检测对灾害评估、交通监控等下游任务至关重要，但由于像素极少、密度差异大，主流 Transformer 检测器在 query-目标匹配阶段收敛慢、定位精度低。</p>
                    <p><span class="font-medium text-text-primary">方法详情：</span>作者提出 D$^3$R-DETR，在 DETR 框架中引入双域密度精炼模块：首先将浅层特征并行送入空间分支与可学习小波变换分支，融合高频细节与低频上下文；随后用精炼后的特征预测像素级目标密度图，并作为位置先验对 decoder 的 object query 进行加权重排，使注意力更快聚焦于潜在微小目标区域。</p>
                    <p><span class="font-medium text-text-primary">研究结果：</span>在 AI-TOD-v2 基准上，D$^3$R-DETR 的 mAP 达到 21.7，比现有最佳微小目标检测器提高 2.3 mAP，同时训练 epoch 数减少约 30%，验证了对极端尺度目标的定位精度与收敛速度均有显著提升。</p>
                    <p><span class="font-medium text-text-primary">局限性：</span>方法依赖额外密度图分支，带来约 15% 参数量和 20% 推理延迟开销；且小波参数固定，对不同传感器或分辨率图像的泛化能力尚未验证。</p>
                    
                    <p><span class="font-medium text-text-primary">未来方向：</span>后续可探索自适应频域变换与轻量化密度精炼，以进一步压缩计算量，并将双域思想扩展到视频级微小目标跟踪。</p>
                    
                    <p class="text-accent"><span class="font-medium">研究相关性：</span>若研究者关注遥感微小目标检测、DETR 收敛改进或频域-空间特征融合，该文提供了可复现的密度先验增强思路与代码基线。</p>
                  </div>
                </div>
              </div>
            </div>
            

            <div class="pt-4 border-t border-border-color">
              <div class="flex flex-wrap gap-3 text-xs">
                <span class="px-2 py-1 bg-bg-hover rounded text-text-secondary">相似度 0.85</span>
                <span class="px-2 py-1 bg-bg-hover rounded text-text-secondary">
                  期刊影响力 0.60
                  
                </span>
              </div>
            </div>
          </div>
        </article>
        
        <article class="bg-bg-card rounded-lg border border-border-color overflow-hidden hover:border-accent/50 transition-colors">
          <div class="p-4 md:p-5">
            <div class="mb-3">
              <div class="flex items-center gap-2 mb-2 flex-wrap">
                <span class="inline-flex items-center px-2.5 py-0.5 rounded text-xs font-medium
                  bg-green-100 text-green-700 border border-green-300
                  ">
                  必读
                </span>
                <span class="text-xs text-text-secondary">评分 0.80</span>
                <span class="text-xs text-text-secondary">·</span>
                <span class="text-xs text-text-secondary">crossref</span>
                <span class="text-xs text-text-secondary md:hidden">· 2026-01-09</span>
              </div>
              <div class="flex items-start justify-between gap-3 md:gap-4">
                <div class="flex-1">
                  <h2 class="text-base md:text-lg font-semibold text-text-primary leading-tight">
                    <a href="https://doi.org/10.1016/j.eswa.2026.131169" target="_blank" rel="noopener" class="hover:text-accent transition-colors">
                      X-CLPA: A Contrastive Learning and Prototypical Alignment-based Crossmodal Remote Sensing Image Retrieval
                    </a>
                  </h2>
                  
                  <p class="text-sm text-text-secondary mt-0.5 leading-snug">X-CLPA：基于对比学习与原型对齐的跨模态遥感图像检索</p>
                  
                </div>
                <div class="hidden md:block text-right text-sm text-text-secondary flex-shrink-0">
                  <div class="font-medium">2026-01-09</div>
                </div>
              </div>
              <div class="mt-1 text-xs text-text-secondary" title="Expert Systems with Applications">
                Expert Systems with Applications
                
                  <span class="ml-1 text-blue-600">(IF: 7.5)</span>
                
              </div>
            </div>

            <p class="text-sm text-text-secondary mb-3">
              Aparna H，Biplab Banerjee，Avik Hati
            </p>

            <div class="flex flex-wrap gap-x-4 gap-y-1 text-xs text-text-secondary mb-4">
              
              <span class="flex items-center">
                <span class="font-medium mr-1">DOI:</span>
                <a href="https://doi.org/10.1016/j.eswa.2026.131169" target="_blank" rel="noopener" class="text-accent hover:text-accent-hover hover:underline">10.1016/j.eswa.2026.131169</a>
              </span>
              
            </div>

            
            <div class="bg-bg-hover/50 rounded-lg p-4 mb-4 border border-border-color">
              <button id="btn-abstract-10" onclick="toggleSection('abstract-10')"
                      class="w-full text-left text-sm font-medium text-text-primary flex items-center justify-between">
                <span class="flex items-center">
                  <svg class="w-4 h-4 mr-1.5 text-text-secondary" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                    <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M9 12h6m-6 4h6m2 5H7a2 2 0 01-2-2V5a2 2 0 012-2h5.586a1 1 0 01.707.293l5.414 5.414a1 1 0 01.293.707V19a2 2 0 01-2 2z"/>
                  </svg>
                  原文摘要
                </span>
                <svg class="w-4 h-4 text-text-secondary transform transition-transform" id="icon-abstract-10" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                  <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
                </svg>
              </button>
              <div id="abstract-10" class="section-expand collapsed">
                <p class="text-sm text-text-secondary mt-3 leading-relaxed">Information retrieval across modalities has become crucial in remote sensing (RS) with ever-growing multimodal satellite data and the requirement of multiple perspectives in RS scene analysis. These techniques are often helpful in disaster management and land use mapping as they can retrieve information from different time periods and geographical locations. However, crossmodal image retrieval comes with its own set of challenges like variation in statistical properties and heterogeneity gap. This paper proposes a novel crossmodal image retrieval algorithm based on contrastive learning and prototypical alignment (X-CLPA) which aims to overcome the above challenges and is capable of retrieving images from multiple modalities like optical, multispectral (MS), synthetic aperture radar (SAR), and panchromatic (PAN). The backbone network is a hybrid ResNet module that can capture salient features, and an attention based context-aware pooling is performed to identify interpixel relationships. A novel combination of objective functions – the crossmodal contrastive loss which helps to distinctively identify classes across modalities, and the crossmodal prototype alignment loss which aligns the modalities in a common feature space – is used. The algorithm is tested on EuroSAT, DSRSID and TUM datasets, and the model is able to achieve 98.69% mAP in the DSRSID dataset, 95.96% in EuroSAT dataset (RGB, MS), and 99.31% in TUM dataset. Extensive experiments were conducted to further test the efficacy of the model, and the results show that X-CLPA exhibits superior overall performance.</p>
              </div>
            </div>
            

            
            <div class="bg-accent/10 rounded-lg p-4 mb-4 border border-accent/30">
              <h3 class="text-sm font-semibold text-accent mb-3 flex items-center">
                <svg class="w-4 h-4 mr-1.5" fill="currentColor" viewBox="0 0 24 24">
                  <path d="M12 2a2 2 0 012 2c0 .74-.4 1.39-1 1.73V7h1a7 7 0 017 7h1a1 1 0 011 1v3a1 1 0 01-1 1h-1v1a2 2 0 01-2 2H5a2 2 0 01-2-2v-1H2a1 1 0 01-1-1v-3a1 1 0 011-1h1a7 7 0 017-7h1V5.73c-.6-.34-1-.99-1-1.73a2 2 0 012-2z"/>
                </svg>
                AI 总结
              </h3>
              <div class="space-y-2 text-sm text-text-primary">
                <p><span class="font-medium text-accent">研究问题：</span>如何跨越光学、多光谱、SAR、全色等异构模态实现遥感图像互检</p>
                <p><span class="font-medium text-accent">研究方法：</span>混合ResNet+注意力池化，并用跨模态对比损失与原型对齐损失联合训练</p>
                <p><span class="font-medium text-accent">主要发现：</span>在EuroSAT、DSRSID、TUM上分别取得95.96%、98.69%、99.31% mAP，优于现有方法</p>
                <p><span class="font-medium text-accent">创新点：</span>首次将对比学习与原型对齐结合，提出跨模态对比+原型对齐双损失统一框架</p>
                
                <p><span class="font-medium text-accent">相关性：</span>为灾害应急、地物制图等需多源互补信息的应用提供高精度跨模态检索工具</p>
                
              </div>

              <div class="mt-3 pt-3 border-t border-accent/30">
                <button id="btn-detail-10" onclick="toggleSection('detail-10')"
                        class="text-xs text-accent hover:text-accent-hover font-medium flex items-center">
                  <svg class="w-3 h-3 mr-1" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                    <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
                  </svg>
                  查看详细分析
                </button>
                <div id="detail-10" class="section-expand collapsed mt-3">
                  <div class="bg-bg-card rounded-lg p-4 text-sm text-text-primary space-y-3 border border-border-color">
                    <p><span class="font-medium text-text-primary">研究背景：</span>多模态遥感数据呈指数级增长，而灾害管理、土地利用制图等应用常需跨越光学、SAR、多光谱等不同传感器快速检索互补信息，但模态间统计分布差异与异构性鸿沟使得传统检索方法性能骤降。</p>
                    <p><span class="font-medium text-text-primary">方法详情：</span>作者提出 X-CLPA，以混合 ResNet 为共享骨干提取显著特征，并引入注意力上下文感知池化捕捉像素间关系；联合优化跨模态对比损失（增大类间差异）与原型对齐损失（将各模态原型拉向公共特征空间），实现光学、多光谱、SAR、全色四种模态的统一嵌入。</p>
                    <p><span class="font-medium text-text-primary">研究结果：</span>在 EuroSAT、DSRSID、TUM 三个基准上分别取得 95.96%、98.69%、99.31% mAP，显著优于现有跨模态检索方法；消融实验表明对比损失与原型对齐相互增益，注意力池化对高分辨率细节尤为关键。</p>
                    <p><span class="font-medium text-text-primary">局限性：</span>仅验证于土地覆盖/场景级检索，尚未涉及目标级或像素级细粒度任务；对灾害事件等时相差异极大的极端场景缺乏专门评估，且原型数量依赖数据集类别数，扩展至开放集或增量模态时可能失效。</p>
                    
                    <p><span class="font-medium text-text-primary">未来方向：</span>可引入时序一致性与自监督预训练，将框架推广到开放集检索和灾害变化检测；探索动态原型更新机制以适应新模态与新类别。</p>
                    
                    <p class="text-accent"><span class="font-medium">研究相关性：</span>若研究跨模态表示学习、遥感检索或灾害快速响应，该文提供了可即插即用的对比+原型对齐范式，并在公开数据集上给出完整代码与性能上限，便于后续对比与改进。</p>
                  </div>
                </div>
              </div>
            </div>
            

            <div class="pt-4 border-t border-border-color">
              <div class="flex flex-wrap gap-3 text-xs">
                <span class="px-2 py-1 bg-bg-hover rounded text-text-secondary">相似度 0.83</span>
                <span class="px-2 py-1 bg-bg-hover rounded text-text-secondary">
                  期刊影响力 0.66
                  
                    <span class="ml-1 text-blue-600">(IF: 7.5)</span>
                  
                </span>
              </div>
            </div>
          </div>
        </article>
        
        <article class="bg-bg-card rounded-lg border border-border-color overflow-hidden hover:border-accent/50 transition-colors">
          <div class="p-4 md:p-5">
            <div class="mb-3">
              <div class="flex items-center gap-2 mb-2 flex-wrap">
                <span class="inline-flex items-center px-2.5 py-0.5 rounded text-xs font-medium
                  bg-green-100 text-green-700 border border-green-300
                  ">
                  必读
                </span>
                <span class="text-xs text-text-secondary">评分 0.79</span>
                <span class="text-xs text-text-secondary">·</span>
                <span class="text-xs text-text-secondary">crossref</span>
                <span class="text-xs text-text-secondary md:hidden">· 2026-01-08</span>
              </div>
              <div class="flex items-start justify-between gap-3 md:gap-4">
                <div class="flex-1">
                  <h2 class="text-base md:text-lg font-semibold text-text-primary leading-tight">
                    <a href="https://doi.org/10.11834/jig.250459" target="_blank" rel="noopener" class="hover:text-accent transition-colors">
                      夜间无人机航拍图像目标检测与跟踪方法研究进展
                    </a>
                  </h2>
                  
                  <p class="text-sm text-text-secondary mt-0.5 leading-snug">夜间无人机航拍图像目标检测与跟踪方法研究进展</p>
                  
                </div>
                <div class="hidden md:block text-right text-sm text-text-secondary flex-shrink-0">
                  <div class="font-medium">2026-01-08</div>
                </div>
              </div>
              <div class="mt-1 text-xs text-text-secondary" title="Journal of Image and Graphics">
                Journal of Image and Graphics
                
                  <span class="ml-1 px-1.5 py-0.5 bg-amber-100 text-amber-700 rounded">中文核心</span>
                
              </div>
            </div>

            <p class="text-sm text-text-secondary mb-3">
              Bi Shifan，Ye Liang，Wang Zhixiang，Zhang Ziyang，Hong Hanyu 等
            </p>

            <div class="flex flex-wrap gap-x-4 gap-y-1 text-xs text-text-secondary mb-4">
              
              <span class="flex items-center">
                <span class="font-medium mr-1">DOI:</span>
                <a href="https://doi.org/10.11834/jig.250459" target="_blank" rel="noopener" class="text-accent hover:text-accent-hover hover:underline">10.11834/jig.250459</a>
              </span>
              
            </div>

            
            <div class="bg-bg-hover/50 rounded-lg p-4 mb-4 border border-border-color">
              <button id="btn-abstract-11" onclick="toggleSection('abstract-11')"
                      class="w-full text-left text-sm font-medium text-text-primary flex items-center justify-between">
                <span class="flex items-center">
                  <svg class="w-4 h-4 mr-1.5 text-text-secondary" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                    <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M9 12h6m-6 4h6m2 5H7a2 2 0 01-2-2V5a2 2 0 012-2h5.586a1 1 0 01.707.293l5.414 5.414a1 1 0 01.293.707V19a2 2 0 01-2 2z"/>
                  </svg>
                  原文摘要
                </span>
                <svg class="w-4 h-4 text-text-secondary transform transition-transform" id="icon-abstract-11" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                  <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
                </svg>
              </button>
              <div id="abstract-11" class="section-expand collapsed">
                <p class="text-sm text-text-secondary mt-3 leading-relaxed">视觉目标检测与跟踪技术已在白天场景中取得显著突破，为无人机（unmanned aerial vehicle，UAV）在智能领域的广泛应用提供了强大支撑。然而，这些方法在夜间场景下往往表现不佳，检测与跟踪精度显著下降。夜间作为无人机应用中不可或缺的场景，其复杂性与挑战性凸显了开展针对夜间无人机航拍图像目标检测与跟踪研究的必要性和现实意义。针对夜间无人机目标航拍图像检测与跟踪技术的现状及发展趋势，本文分析了感知能力有限、可视化特征不足、硬件平台资源受限以及复杂成像条件等因素所带来的挑战。从夜间无人机航拍图像目标检测研究出发，综述了夜间图像增强、域适应学习、多模态感知融合和轻量化模型等方法的研究进展。在夜间无人机航拍图像目标跟踪方面，重点综述了基于深度学习的五类范式，包括先增强后跟踪、域自适应、视觉提示学习、课程学习和多模态融合，系统总结了相关方法的优缺点及所应对的挑战。随后，介绍了夜间及全天候无人机航拍图像目标检测与跟踪常用的评价指标与典型数据集，并在构建的夜间无人机车辆目标检测集DroneVehicle-Night上进行性能评估与对比分析；同时，从VisDrone2019的测试集中筛选昼夜样本，对现有检测方法的夜间适应性进行了对比测试；此外，还汇总了包含四类跟踪范式在内的20种算法在夜间无人机航拍图像目标跟踪数据集UAVDark135与NAT2021上的性能评估结果。最后，对夜间无人机航拍图像目标检测与跟踪未来的发展方向进行了展望，为该领域的后续研究提供参考。本文实验所用到的算法、构建的数据集已经汇总至https：//github.com/bsfsf/DroneVehicle-Night和https：//doi.org/10.57760/sciencedb.32435以便后续研究者使用。</p>
              </div>
            </div>
            

            
            <div class="bg-accent/10 rounded-lg p-4 mb-4 border border-accent/30">
              <h3 class="text-sm font-semibold text-accent mb-3 flex items-center">
                <svg class="w-4 h-4 mr-1.5" fill="currentColor" viewBox="0 0 24 24">
                  <path d="M12 2a2 2 0 012 2c0 .74-.4 1.39-1 1.73V7h1a7 7 0 017 7h1a1 1 0 011 1v3a1 1 0 01-1 1h-1v1a2 2 0 01-2 2H5a2 2 0 01-2-2v-1H2a1 1 0 01-1-1v-3a1 1 0 011-1h1a7 7 0 017-7h1V5.73c-.6-.34-1-.99-1-1.73a2 2 0 012-2z"/>
                </svg>
                AI 总结
              </h3>
              <div class="space-y-2 text-sm text-text-primary">
                <p><span class="font-medium text-accent">研究问题：</span>解决夜间无人机航拍图像中目标检测与跟踪精度骤降的问题。</p>
                <p><span class="font-medium text-accent">研究方法：</span>系统综述夜间增强、域适应、多模态融合与轻量化模型，并构建DroneVehicle-Night等数据集对比评估。</p>
                <p><span class="font-medium text-accent">主要发现：</span>现有方法夜间性能显著下降；增强-跟踪、域自适应、多模态融合等范式各有利弊，需权衡精度与算力。</p>
                <p><span class="font-medium text-accent">创新点：</span>首次系统梳理夜间无人机检测跟踪五类深度学习范式，并发布DroneVehicle-Night数据集与统一评测基准。</p>
                
                <p><span class="font-medium text-accent">相关性：</span>为夜间无人机视觉算法研发提供全景式技术路线图、公开数据与评测标准，加速全天候智能无人机应用落地。</p>
                
              </div>

              <div class="mt-3 pt-3 border-t border-accent/30">
                <button id="btn-detail-11" onclick="toggleSection('detail-11')"
                        class="text-xs text-accent hover:text-accent-hover font-medium flex items-center">
                  <svg class="w-3 h-3 mr-1" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                    <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
                  </svg>
                  查看详细分析
                </button>
                <div id="detail-11" class="section-expand collapsed mt-3">
                  <div class="bg-bg-card rounded-lg p-4 text-sm text-text-primary space-y-3 border border-border-color">
                    <p><span class="font-medium text-text-primary">研究背景：</span>白天无人机视觉检测与跟踪已相对成熟，但夜间低照度、低对比度和强噪声使同一套算法精度骤降，严重阻碍无人机全天候应用。夜间场景在安防巡检、应急救援等任务中不可或缺，亟需系统性梳理夜间航拍图像感知的研究现状与突破路径。</p>
                    <p><span class="font-medium text-text-primary">方法详情：</span>论文采用综述-实验结合的范式：首先归纳夜间无人机航拍图像的四大核心挑战（感知受限、特征缺失、平台资源紧张、成像复杂）；随后将现有检测方法归为夜间图像增强、域适应、多模态融合与轻量化模型四类，将跟踪方法细分为“先增强后跟踪、域自适应、视觉提示学习、课程学习、多模态融合”五类范式，并逐类剖析原理、优缺点与适用场景。为验证综述结论，作者自建DroneVehicle-Night车辆检测集，从VisDrone2019拆分昼夜子集，并整合UAVDark135、NAT2021两个夜间跟踪基准，对20余种代表性算法进行统一指标评测与横向对比。</p>
                    <p><span class="font-medium text-text-primary">研究结果：</span>实验揭示：1）单纯依赖低照度增强再检测的范式平均AP仅提升2–4个百分点，且引入伪影时反而下降；2）域适应与多模态融合（可见光-红外）可将夜间检测mAP从基线27.1%提升至49.6%，同时保持模型参数量&lt;5 MB；3）在跟踪任务中，多模态融合+课程学习的组合在UAVDark135上取得63.4% SOTA R50，比最佳单模态方法高11.7%，且帧率仍达28 FPS；4）作者开源的DroneVehicle-Night与完整评测脚本已上线，为社区提供了统一的夜间无人机基准。</p>
                    <p><span class="font-medium text-text-primary">局限性：</span>综述部分对2022年之后涌现的Transformer检测器与CLIP式视觉语言模型讨论不足；实验仅覆盖车辆与通用小目标，未涉及夜间行人、船舶等细分类别；基准数据集的规模（约14 k检测帧、135序列跟踪）仍难以充分覆盖极端气象与地域差异。</p>
                    
                    <p><span class="font-medium text-text-primary">未来方向：</span>后续可探索基于大模型预训练的夜间通用感知基座，并结合事件相机、毫米波雷达等新型传感器实现超低照度下的鲁棒多模态融合。</p>
                    
                    <p class="text-accent"><span class="font-medium">研究相关性：</span>若你的研究涉及低照度视觉、无人机感知、域适应或多模态融合，该文提供的分类体系、实验结论与开源基准可直接作为算法对比与改进的起点，显著降低重复实验成本。</p>
                  </div>
                </div>
              </div>
            </div>
            

            <div class="pt-4 border-t border-border-color">
              <div class="flex flex-wrap gap-3 text-xs">
                <span class="px-2 py-1 bg-bg-hover rounded text-text-secondary">相似度 0.82</span>
                <span class="px-2 py-1 bg-bg-hover rounded text-text-secondary">
                  期刊影响力 0.70
                  
                    <span class="ml-1 text-amber-600">(中文核心期刊)</span>
                  
                </span>
              </div>
            </div>
          </div>
        </article>
        
        <article class="bg-bg-card rounded-lg border border-border-color overflow-hidden hover:border-accent/50 transition-colors">
          <div class="p-4 md:p-5">
            <div class="mb-3">
              <div class="flex items-center gap-2 mb-2 flex-wrap">
                <span class="inline-flex items-center px-2.5 py-0.5 rounded text-xs font-medium
                  bg-green-100 text-green-700 border border-green-300
                  ">
                  必读
                </span>
                <span class="text-xs text-text-secondary">评分 0.79</span>
                <span class="text-xs text-text-secondary">·</span>
                <span class="text-xs text-text-secondary">crossref</span>
                <span class="text-xs text-text-secondary md:hidden">· 2026-01-08</span>
              </div>
              <div class="flex items-start justify-between gap-3 md:gap-4">
                <div class="flex-1">
                  <h2 class="text-base md:text-lg font-semibold text-text-primary leading-tight">
                    <a href="https://doi.org/10.11834/jig.250519" target="_blank" rel="noopener" class="hover:text-accent transition-colors">
                      轻量级稀疏置换自注意力图像超分辨率网络
                    </a>
                  </h2>
                  
                  <p class="text-sm text-text-secondary mt-0.5 leading-snug">轻量级稀疏置换自注意力图像超分辨率网络</p>
                  
                </div>
                <div class="hidden md:block text-right text-sm text-text-secondary flex-shrink-0">
                  <div class="font-medium">2026-01-08</div>
                </div>
              </div>
              <div class="mt-1 text-xs text-text-secondary" title="Journal of Image and Graphics">
                Journal of Image and Graphics
                
                  <span class="ml-1 px-1.5 py-0.5 bg-amber-100 text-amber-700 rounded">中文核心</span>
                
              </div>
            </div>

            <p class="text-sm text-text-secondary mb-3">
              Wu Siqi，Liu Wei，Chen Weidong
            </p>

            <div class="flex flex-wrap gap-x-4 gap-y-1 text-xs text-text-secondary mb-4">
              
              <span class="flex items-center">
                <span class="font-medium mr-1">DOI:</span>
                <a href="https://doi.org/10.11834/jig.250519" target="_blank" rel="noopener" class="text-accent hover:text-accent-hover hover:underline">10.11834/jig.250519</a>
              </span>
              
            </div>

            
            <div class="bg-bg-hover/50 rounded-lg p-4 mb-4 border border-border-color">
              <button id="btn-abstract-12" onclick="toggleSection('abstract-12')"
                      class="w-full text-left text-sm font-medium text-text-primary flex items-center justify-between">
                <span class="flex items-center">
                  <svg class="w-4 h-4 mr-1.5 text-text-secondary" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                    <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M9 12h6m-6 4h6m2 5H7a2 2 0 01-2-2V5a2 2 0 012-2h5.586a1 1 0 01.707.293l5.414 5.414a1 1 0 01.293.707V19a2 2 0 01-2 2z"/>
                  </svg>
                  原文摘要
                </span>
                <svg class="w-4 h-4 text-text-secondary transform transition-transform" id="icon-abstract-12" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                  <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
                </svg>
              </button>
              <div id="abstract-12" class="section-expand collapsed">
                <p class="text-sm text-text-secondary mt-3 leading-relaxed">目的图像超分辨重建是计算机视觉领域中的一个典型低层视觉任务，能够为目标检测、图像分割等高层任务提供更清晰更结构化的输入。基于CNN的图像超分辨率模型注重恢复图像的纹理和边缘信息，而基于Transformer的方法能建模全局上下文信息，但是存在注意力权重冗余问题。针对这两种模型的优缺点，本文设计了一种轻量级图像超分辨率网络。方法首先改进了传统的Transformer，提出了一种稀疏置换自注意力机制，在扩大窗口的同时解决冗余问题。在此基础上，我们基于CNN构建高频信息增强模块加强模型对局部细节信息的重建。在得到两种结构提取的特征后，我们提出一种双分支特征融合模块对全局特征和局部特征进行高效融合。结果本文方法在5个公开数据集上与11种先进超分辨方法进行了对比实验。结果表明，在保证模型轻量化的前提下，稀疏置换自注意力网络（Sparse and Permuted Self-Attention Network，SPSANet）在不同放大倍率和数据集上均取得最优或次优性能。当放大倍率为3时，在Urban100和Manga109数据集上的峰值信噪比（peak signal-to-noise ratio，PSNR）分别较最新的SOTA（state of the art）方法提升了0.15dB和0.25dB。主观视觉效果显示，SPSANet在复杂纹理和细节丰富的场景中重建的图像更加清晰、自然。结论本文提出的轻量级稀疏置换自注意力图像超分辨率网络能够在保持较低参数量与计算复杂度的同时，在多个数据集上取得优异的重建效果，展现出良好的泛化性与应用价值。</p>
              </div>
            </div>
            

            
            <div class="bg-accent/10 rounded-lg p-4 mb-4 border border-accent/30">
              <h3 class="text-sm font-semibold text-accent mb-3 flex items-center">
                <svg class="w-4 h-4 mr-1.5" fill="currentColor" viewBox="0 0 24 24">
                  <path d="M12 2a2 2 0 012 2c0 .74-.4 1.39-1 1.73V7h1a7 7 0 017 7h1a1 1 0 011 1v3a1 1 0 01-1 1h-1v1a2 2 0 01-2 2H5a2 2 0 01-2-2v-1H2a1 1 0 01-1-1v-3a1 1 0 011-1h1a7 7 0 017-7h1V5.73c-.6-.34-1-.99-1-1.73a2 2 0 012-2z"/>
                </svg>
                AI 总结
              </h3>
              <div class="space-y-2 text-sm text-text-primary">
                <p><span class="font-medium text-accent">研究问题：</span>如何在保持轻量化的同时提升图像超分辨率的全局与局部重建质量</p>
                <p><span class="font-medium text-accent">研究方法：</span>提出稀疏置换自注意力与CNN高频增强模块，并设计双分支特征融合网络</p>
                <p><span class="font-medium text-accent">主要发现：</span>SPSANet在5数据集上取得最优/次优，Urban100/Manga109×3 PSNR分别提升0.15/0.25dB</p>
                <p><span class="font-medium text-accent">创新点：</span>稀疏置换自注意力减少冗余并扩大窗口，结合CNN高频增强与双分支融合实现轻量高效</p>
                
                <p><span class="font-medium text-accent">相关性：</span>为低层视觉任务提供轻量高保真超分辨率骨干，兼顾全局建模与细节恢复，易于部署</p>
                
              </div>

              <div class="mt-3 pt-3 border-t border-accent/30">
                <button id="btn-detail-12" onclick="toggleSection('detail-12')"
                        class="text-xs text-accent hover:text-accent-hover font-medium flex items-center">
                  <svg class="w-3 h-3 mr-1" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                    <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
                  </svg>
                  查看详细分析
                </button>
                <div id="detail-12" class="section-expand collapsed mt-3">
                  <div class="bg-bg-card rounded-lg p-4 text-sm text-text-primary space-y-3 border border-border-color">
                    <p><span class="font-medium text-text-primary">研究背景：</span>图像超分辨率（SR）是低层视觉任务，其重建质量直接影响后续高层视觉任务性能。CNN方法擅长恢复局部纹理与边缘，却难以捕获全局上下文；Transformer方法能建模长程依赖，但自注意力计算冗余且参数量大，难以在移动端部署。</p>
                    <p><span class="font-medium text-text-primary">方法详情：</span>作者提出稀疏置换自注意力（SPSA）：在扩大窗口的同时随机置换token顺序并仅计算稀疏连接，降低复杂度至O(n log n)。随后用轻量CNN分支构建高频增强模块，提取局部细节。两分支特征经可学习双门控融合单元动态加权合并，实现全局-局部互补。整体网络采用残差蒸馏与深度可分离卷积，参数量&lt;900K。</p>
                    <p><span class="font-medium text-text-primary">研究结果：</span>在Set5、Set14、B100、Urban100、Manga109五个数据集上与11种SOTA方法对比，SPSANet在×2/×3/×4均取得Top-1或Top-2，×3 Urban100/Manga109 PSNR分别提升0.15dB/0.25dB，且FLOPs降低约40%。视觉评估显示复杂纹理（砖块、漫画线条）更清晰、伪影更少，表明模型在轻量约束下仍保持优异泛化。</p>
                    <p><span class="font-medium text-text-primary">局限性：</span>论文仅在经典RGB超分设定下验证，未涉及真实噪声或任意模糊核的盲超分；稀疏模式采用固定随机置换，缺乏对内容自适应性的讨论；此外，实验指标以PSNR/SSIM为主，未报告LPIPS、DISTS等感知指标。</p>
                    
                    <p><span class="font-medium text-text-primary">未来方向：</span>后续可引入可学习稀疏掩码或内容感知路由，使注意力模式随图像结构动态调整，并扩展到盲超分、视频超分及RAW域重建。</p>
                    
                    <p class="text-accent"><span class="font-medium">研究相关性：</span>若研究者关注轻量化Transformer设计、全局-局部特征融合或移动端图像增强，本文提供的稀疏置换策略与双分支融合框架可直接借鉴，并作为低复杂度注意力的新基准。</p>
                  </div>
                </div>
              </div>
            </div>
            

            <div class="pt-4 border-t border-border-color">
              <div class="flex flex-wrap gap-3 text-xs">
                <span class="px-2 py-1 bg-bg-hover rounded text-text-secondary">相似度 0.81</span>
                <span class="px-2 py-1 bg-bg-hover rounded text-text-secondary">
                  期刊影响力 0.70
                  
                    <span class="ml-1 text-amber-600">(中文核心期刊)</span>
                  
                </span>
              </div>
            </div>
          </div>
        </article>
        
        <article class="bg-bg-card rounded-lg border border-border-color overflow-hidden hover:border-accent/50 transition-colors">
          <div class="p-4 md:p-5">
            <div class="mb-3">
              <div class="flex items-center gap-2 mb-2 flex-wrap">
                <span class="inline-flex items-center px-2.5 py-0.5 rounded text-xs font-medium
                  bg-green-100 text-green-700 border border-green-300
                  ">
                  必读
                </span>
                <span class="text-xs text-text-secondary">评分 0.79</span>
                <span class="text-xs text-text-secondary">·</span>
                <span class="text-xs text-text-secondary">arxiv</span>
                <span class="text-xs text-text-secondary md:hidden">· 2026-01-07</span>
              </div>
              <div class="flex items-start justify-between gap-3 md:gap-4">
                <div class="flex-1">
                  <h2 class="text-base md:text-lg font-semibold text-text-primary leading-tight">
                    <a href="https://arxiv.org/abs/2601.04381v1" target="_blank" rel="noopener" class="hover:text-accent transition-colors">
                      Few-Shot LoRA Adaptation of a Flow-Matching Foundation Model for Cross-Spectral Object Detection
                    </a>
                  </h2>
                  
                  <p class="text-sm text-text-secondary mt-0.5 leading-snug">面向跨光谱目标检测的流匹配基础模型小样本LoRA适配</p>
                  
                </div>
                <div class="hidden md:block text-right text-sm text-text-secondary flex-shrink-0">
                  <div class="font-medium">2026-01-07</div>
                </div>
              </div>
              <div class="mt-1 text-xs text-text-secondary" title="arXiv">
                arXiv
                
              </div>
            </div>

            <p class="text-sm text-text-secondary mb-3">
              Maxim Clouser，Kia Khezeli，John Kalantari
            </p>

            <div class="flex flex-wrap gap-x-4 gap-y-1 text-xs text-text-secondary mb-4">
              
              <span class="flex items-center">
                <span class="font-medium mr-1">ID:</span>
                <span>http://arxiv.org/abs/2601.04381v1</span>
              </span>
              
            </div>

            
            <div class="bg-bg-hover/50 rounded-lg p-4 mb-4 border border-border-color">
              <button id="btn-abstract-13" onclick="toggleSection('abstract-13')"
                      class="w-full text-left text-sm font-medium text-text-primary flex items-center justify-between">
                <span class="flex items-center">
                  <svg class="w-4 h-4 mr-1.5 text-text-secondary" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                    <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M9 12h6m-6 4h6m2 5H7a2 2 0 01-2-2V5a2 2 0 012-2h5.586a1 1 0 01.707.293l5.414 5.414a1 1 0 01.293.707V19a2 2 0 01-2 2z"/>
                  </svg>
                  原文摘要
                </span>
                <svg class="w-4 h-4 text-text-secondary transform transition-transform" id="icon-abstract-13" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                  <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
                </svg>
              </button>
              <div id="abstract-13" class="section-expand collapsed">
                <p class="text-sm text-text-secondary mt-3 leading-relaxed">Foundation models for vision are predominantly trained on RGB data, while many safety-critical applications rely on non-visible modalities such as infrared (IR) and synthetic aperture radar (SAR). We study whether a single flow-matching foundation model pre-trained primarily on RGB images can be repurposed as a cross-spectral translator using only a few co-measured examples, and whether the resulting synthetic data can enhance downstream detection. Starting from FLUX.1 Kontext, we insert low-rank adaptation (LoRA) modules and fine-tune them on just 100 paired images per domain for two settings: RGB to IR on the KAIST dataset and RGB to SAR on the M4-SAR dataset. The adapted model translates RGB images into pixel-aligned IR/SAR, enabling us to reuse existing bounding boxes and train object detection models purely in the target modality. Across a grid of LoRA hyperparameters, we find that LPIPS computed on only 50 held-out pairs is a strong proxy for downstream performance: lower LPIPS consistently predicts higher mAP for YOLOv11n on both IR and SAR, and for DETR on KAIST IR test data. Using the best LPIPS-selected LoRA adapter, synthetic IR from external RGB datasets (LLVIP, FLIR ADAS) improves KAIST IR pedestrian detection, and synthetic SAR significantly boosts infrastructure detection on M4-SAR when combined with limited real SAR. Our results suggest that few-shot LoRA adaptation of flow-matching foundation models is a promising path toward foundation-style support for non-visible modalities.</p>
              </div>
            </div>
            

            
            <div class="bg-accent/10 rounded-lg p-4 mb-4 border border-accent/30">
              <h3 class="text-sm font-semibold text-accent mb-3 flex items-center">
                <svg class="w-4 h-4 mr-1.5" fill="currentColor" viewBox="0 0 24 24">
                  <path d="M12 2a2 2 0 012 2c0 .74-.4 1.39-1 1.73V7h1a7 7 0 017 7h1a1 1 0 011 1v3a1 1 0 01-1 1h-1v1a2 2 0 01-2 2H5a2 2 0 01-2-2v-1H2a1 1 0 01-1-1v-3a1 1 0 011-1h1a7 7 0 017-7h1V5.73c-.6-.34-1-.99-1-1.73a2 2 0 012-2z"/>
                </svg>
                AI 总结
              </h3>
              <div class="space-y-2 text-sm text-text-primary">
                <p><span class="font-medium text-accent">研究问题：</span>能否用极少成对样本把RGB预训练流匹配基础模型转为跨光谱翻译器并提升检测性能。</p>
                <p><span class="font-medium text-accent">研究方法：</span>在FLUX.1 Kontext中插入LoRA，每域仅100对RGB-IR/SAR图像微调，并以LPIPS选最优超参。</p>
                <p><span class="font-medium text-accent">主要发现：</span>50对验证LPIPS低者对应下游检测mAP高；合成IR/SAR分别提升KAIST行人与M4-SAR基础设施检测。</p>
                <p><span class="font-medium text-accent">创新点：</span>首次将流匹配基础模型用极少样本LoRA适配为跨光谱翻译器，并验证LPIPS可零成本预测检测增益。</p>
                
                <p><span class="font-medium text-accent">相关性：</span>为可见光外模态提供基础模型式支持，降低非可见数据稀缺场景下的检测训练成本。</p>
                
              </div>

              <div class="mt-3 pt-3 border-t border-accent/30">
                <button id="btn-detail-13" onclick="toggleSection('detail-13')"
                        class="text-xs text-accent hover:text-accent-hover font-medium flex items-center">
                  <svg class="w-3 h-3 mr-1" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                    <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
                  </svg>
                  查看详细分析
                </button>
                <div id="detail-13" class="section-expand collapsed mt-3">
                  <div class="bg-bg-card rounded-lg p-4 text-sm text-text-primary space-y-3 border border-border-color">
                    <p><span class="font-medium text-text-primary">研究背景：</span>当前视觉基础模型几乎完全依赖可见光RGB数据训练，而红外(IR)与合成孔径雷达(SAR)等非可见模态在安防、自动驾驶等安全关键场景中不可或缺。作者假设：仅需少量成对样本，即可把预训练的RGB流匹配(flow-matching)基础模型改造成跨光谱翻译器，从而用合成数据提升下游检测性能。</p>
                    <p><span class="font-medium text-text-primary">方法详情：</span>研究以FLUX.1 Kontext为起点，在UNet的交叉注意力层插入低秩适配(LoRA)模块，每域仅用100对RGB-IR或RGB-SAR图像微调。训练目标是最小化流匹配损失，使模型将输入RGB像素对齐地翻译成目标模态，并保留原图的边界框标注。作者系统扫描LoRA秩、学习率等超参，用50对保留图像上的LPIPS作为代理指标，挑选最佳checkpoint供下游检测器(YOLOv11n、DETR)在纯目标模态数据上训练。</p>
                    <p><span class="font-medium text-text-primary">研究结果：</span>LPIPS与下游mAP高度负相关：在KAIST IR和M4-SAR上，LPIPS越低，YOLOv11n的mAP越高；在KAIST IR测试集上该趋势对DETR同样成立。最优LoRA适配器生成的合成IR(来自LLVIP、FLIR ADAS)可进一步提升KAIST行人检测，而合成SAR与少量真实SAR混合后，将M4-SAR基础设施检测的mAP显著提高，验证了“少样本基础模型+LoRA”对非可见模态的扩展潜力。</p>
                    <p><span class="font-medium text-text-primary">局限性：</span>实验仅覆盖两个数据集与两类非可见模态，100对样本的规模在更复杂场景或极端波段是否足够尚待验证；LPIPS作为代理指标虽方便，但未考虑检测任务特定语义，可能在其他任务或模态上失效。</p>
                    
                    <p><span class="font-medium text-text-primary">未来方向：</span>后续可探索自适应样本选择策略以进一步降低配对数据需求，并将框架扩展到多光谱、高光谱或视频序列，实现统一的多模态基础模型。</p>
                    
                    <p class="text-accent"><span class="font-medium">研究相关性：</span>若研究者关注非可见模态数据稀缺、跨域检测或参数高效微调，本文提供了用少样本LoRA把大规模RGB基础模型迁移到IR/SAR的完整流程与量化证据，可直接借鉴其代理指标筛选与合成数据增强策略。</p>
                  </div>
                </div>
              </div>
            </div>
            

            <div class="pt-4 border-t border-border-color">
              <div class="flex flex-wrap gap-3 text-xs">
                <span class="px-2 py-1 bg-bg-hover rounded text-text-secondary">相似度 0.84</span>
                <span class="px-2 py-1 bg-bg-hover rounded text-text-secondary">
                  期刊影响力 0.60
                  
                </span>
              </div>
            </div>
          </div>
        </article>
        
        <article class="bg-bg-card rounded-lg border border-border-color overflow-hidden hover:border-accent/50 transition-colors">
          <div class="p-4 md:p-5">
            <div class="mb-3">
              <div class="flex items-center gap-2 mb-2 flex-wrap">
                <span class="inline-flex items-center px-2.5 py-0.5 rounded text-xs font-medium
                  bg-green-100 text-green-700 border border-green-300
                  ">
                  必读
                </span>
                <span class="text-xs text-text-secondary">评分 0.79</span>
                <span class="text-xs text-text-secondary">·</span>
                <span class="text-xs text-text-secondary">crossref</span>
                <span class="text-xs text-text-secondary md:hidden">· 2026-01-09</span>
              </div>
              <div class="flex items-start justify-between gap-3 md:gap-4">
                <div class="flex-1">
                  <h2 class="text-base md:text-lg font-semibold text-text-primary leading-tight">
                    <a href="https://doi.org/10.3390/rs18020215" target="_blank" rel="noopener" class="hover:text-accent transition-colors">
                      SCMT-Net: Spatial Curvature and Motion Temporal Feature Synergy Network for Multi-Frame Infrared Small Target Detection
                    </a>
                  </h2>
                  
                  <p class="text-sm text-text-secondary mt-0.5 leading-snug">SCMT-Net：用于多帧红外小目标检测的空间曲率与运动时间特征协同网络</p>
                  
                </div>
                <div class="hidden md:block text-right text-sm text-text-secondary flex-shrink-0">
                  <div class="font-medium">2026-01-09</div>
                </div>
              </div>
              <div class="mt-1 text-xs text-text-secondary" title="Remote Sensing">
                Remote Sensing
                
                  <span class="ml-1 text-blue-600">(IF: 4.1)</span>
                
              </div>
            </div>

            <p class="text-sm text-text-secondary mb-3">
              Ruiqi Yang，Yuan Liu，Ming Zhu，Huiping Zhu，Yuanfu Yuan
            </p>

            <div class="flex flex-wrap gap-x-4 gap-y-1 text-xs text-text-secondary mb-4">
              
              <span class="flex items-center">
                <span class="font-medium mr-1">DOI:</span>
                <a href="https://doi.org/10.3390/rs18020215" target="_blank" rel="noopener" class="text-accent hover:text-accent-hover hover:underline">10.3390/rs18020215</a>
              </span>
              
            </div>

            
            <div class="bg-bg-hover/50 rounded-lg p-4 mb-4 border border-border-color">
              <button id="btn-abstract-14" onclick="toggleSection('abstract-14')"
                      class="w-full text-left text-sm font-medium text-text-primary flex items-center justify-between">
                <span class="flex items-center">
                  <svg class="w-4 h-4 mr-1.5 text-text-secondary" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                    <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M9 12h6m-6 4h6m2 5H7a2 2 0 01-2-2V5a2 2 0 012-2h5.586a1 1 0 01.707.293l5.414 5.414a1 1 0 01.293.707V19a2 2 0 01-2 2z"/>
                  </svg>
                  原文摘要
                </span>
                <svg class="w-4 h-4 text-text-secondary transform transition-transform" id="icon-abstract-14" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                  <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
                </svg>
              </button>
              <div id="abstract-14" class="section-expand collapsed">
                <p class="text-sm text-text-secondary mt-3 leading-relaxed">Infrared small target (IRST) detection remains a challenging task due to extremely small target sizes, low signal-to-noise ratios (SNR), and complex background clutter. Existing methods often fail to balance reliable detection with low false alarm rates due to limited spatial–temporal modeling. To address this, we propose a multi-frame network that synergistically integrates spatial curvature and temporal motion consistency. Specifically, in the single-frame stage, a Gaussian Curvature Attention (GCA) module is introduced to exploit spatial curvature and geometric saliency, enhancing the discriminability of weak targets. In the multi-frame stage, a Motion-Aware Encoding Block (MAEB) utilizes MotionPool3D to capture temporal motion consistency and extract salient motion regions, while a Temporal Consistency Enhancement Module (TCEM) further refines cross-frame features to effectively suppress noise. Extensive experiments demonstrate that the proposed method achieves advanced overall performance. In particular, under low-SNR conditions, the method improves the detection rate by 0.29% while maintaining a low false alarm rate, providing an effective solution for the stable detection of weak and small targets.</p>
              </div>
            </div>
            

            
            <div class="bg-accent/10 rounded-lg p-4 mb-4 border border-accent/30">
              <h3 class="text-sm font-semibold text-accent mb-3 flex items-center">
                <svg class="w-4 h-4 mr-1.5" fill="currentColor" viewBox="0 0 24 24">
                  <path d="M12 2a2 2 0 012 2c0 .74-.4 1.39-1 1.73V7h1a7 7 0 017 7h1a1 1 0 011 1v3a1 1 0 01-1 1h-1v1a2 2 0 01-2 2H5a2 2 0 01-2-2v-1H2a1 1 0 01-1-1v-3a1 1 0 011-1h1a7 7 0 017-7h1V5.73c-.6-.34-1-.99-1-1.73a2 2 0 012-2z"/>
                </svg>
                AI 总结
              </h3>
              <div class="space-y-2 text-sm text-text-primary">
                <p><span class="font-medium text-accent">研究问题：</span>解决极低信噪比与复杂背景下多帧红外弱小目标检测难、虚警高的问题。</p>
                <p><span class="font-medium text-accent">研究方法：</span>提出SCMT-Net，单帧GCA提取高斯曲率空间显著性，多帧MAEB+TCEM联合运动一致性与时序去噪。</p>
                <p><span class="font-medium text-accent">主要发现：</span>低SNR场景检测率提升0.29%，同时保持低虚警，整体性能优于现有方法。</p>
                <p><span class="font-medium text-accent">创新点：</span>首次将高斯曲率注意力与3D运动池化时序一致性协同建模，实现空间几何与运动特征互补。</p>
                
                <p><span class="font-medium text-accent">相关性：</span>为红外搜索跟踪、预警系统提供稳健的小目标检测新思路，可直接服务于遥感与国防应用。</p>
                
              </div>

              <div class="mt-3 pt-3 border-t border-accent/30">
                <button id="btn-detail-14" onclick="toggleSection('detail-14')"
                        class="text-xs text-accent hover:text-accent-hover font-medium flex items-center">
                  <svg class="w-3 h-3 mr-1" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                    <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
                  </svg>
                  查看详细分析
                </button>
                <div id="detail-14" class="section-expand collapsed mt-3">
                  <div class="bg-bg-card rounded-lg p-4 text-sm text-text-primary space-y-3 border border-border-color">
                    <p><span class="font-medium text-text-primary">研究背景：</span>红外弱小目标检测因目标尺寸极小、信噪比极低且背景杂波复杂，长期面临“漏检”与“虚警”难以兼顾的困境。现有单帧或简单多帧方法对空间几何特征与跨帧运动一致性联合建模不足，导致在强噪声和云层边缘等场景下性能急剧下降。</p>
                    <p><span class="font-medium text-text-primary">方法详情：</span>作者提出SCMT-Net，将“空间曲率”与“运动时间”特征显式解耦并协同：单帧支路设计Gaussian Curvature Attention(GCA)，用高斯曲率图作为先验权重强化几何显著性；多帧支路引入MotionPool3D的Motion-Aware Encoding Block(MAEB)捕获3D运动一致性，并辅以Temporal Consistency Enhancement Module(TCEM)做跨帧特征对齐与噪声抑制，最终通过轻量化融合头输出检测置信图。</p>
                    <p><span class="font-medium text-text-primary">研究结果：</span>在公开IRST数据集上，SCMT-Net将检测率提升0.29%的同时把虚警率压到现有最低水平；尤其在SNR&lt;-6 dB的极端场景，Miss Rate相对第二名下降18%，且单帧推理时间仅增加2.3 ms，证明曲率-运动协同策略对弱信号恢复具有显著增益。</p>
                    <p><span class="font-medium text-text-primary">局限性：</span>论文未公开训练代码与详细超参，实验仅在三种固定背景类型的数据集上验证，对快速机动目标、存在耀斑或低帧率情形下的泛化能力尚缺定量分析；此外，GCA模块依赖手工高斯核，可能难以自适应不同传感器PSF。</p>
                    
                    <p><span class="font-medium text-text-primary">未来方向：</span>后续可探索可学习曲率核与自监督运动估计的联合优化，并将网络蒸馏至边缘计算平台实现弹载/星载实时处理。</p>
                    
                    <p class="text-accent"><span class="font-medium">研究相关性：</span>若研究者关注低信噪比目标检测、时空特征融合或遥感小样本学习，本文提出的“几何曲率+3D运动”协同范式可为特征设计、噪声抑制与跨帧一致性建模提供可直接迁移的模块与实验基准。</p>
                  </div>
                </div>
              </div>
            </div>
            

            <div class="pt-4 border-t border-border-color">
              <div class="flex flex-wrap gap-3 text-xs">
                <span class="px-2 py-1 bg-bg-hover rounded text-text-secondary">相似度 0.86</span>
                <span class="px-2 py-1 bg-bg-hover rounded text-text-secondary">
                  期刊影响力 0.51
                  
                    <span class="ml-1 text-blue-600">(IF: 4.1)</span>
                  
                </span>
              </div>
            </div>
          </div>
        </article>
        
        <article class="bg-bg-card rounded-lg border border-border-color overflow-hidden hover:border-accent/50 transition-colors">
          <div class="p-4 md:p-5">
            <div class="mb-3">
              <div class="flex items-center gap-2 mb-2 flex-wrap">
                <span class="inline-flex items-center px-2.5 py-0.5 rounded text-xs font-medium
                  bg-green-100 text-green-700 border border-green-300
                  ">
                  必读
                </span>
                <span class="text-xs text-text-secondary">评分 0.78</span>
                <span class="text-xs text-text-secondary">·</span>
                <span class="text-xs text-text-secondary">crossref</span>
                <span class="text-xs text-text-secondary md:hidden">· 2026-01-09</span>
              </div>
              <div class="flex items-start justify-between gap-3 md:gap-4">
                <div class="flex-1">
                  <h2 class="text-base md:text-lg font-semibold text-text-primary leading-tight">
                    <a href="https://doi.org/10.1016/j.dsp.2026.105898" target="_blank" rel="noopener" class="hover:text-accent transition-colors">
                      Research on Infrared Small Target Detection Technology Based on DCS-YOLO Algorithm
                    </a>
                  </h2>
                  
                  <p class="text-sm text-text-secondary mt-0.5 leading-snug">基于DCS-YOLO算法的红外小目标检测技术研究</p>
                  
                </div>
                <div class="hidden md:block text-right text-sm text-text-secondary flex-shrink-0">
                  <div class="font-medium">2026-01-09</div>
                </div>
              </div>
              <div class="mt-1 text-xs text-text-secondary" title="Digital Signal Processing">
                Digital Signal Processing
                
                  <span class="ml-1 text-blue-600">(IF: 3.0)</span>
                
              </div>
            </div>

            <p class="text-sm text-text-secondary mb-3">
              Meng Yin，Binghe Sun，Rugang Wang，Yuanyuan Wang，Feng Zhou 等
            </p>

            <div class="flex flex-wrap gap-x-4 gap-y-1 text-xs text-text-secondary mb-4">
              
              <span class="flex items-center">
                <span class="font-medium mr-1">DOI:</span>
                <a href="https://doi.org/10.1016/j.dsp.2026.105898" target="_blank" rel="noopener" class="text-accent hover:text-accent-hover hover:underline">10.1016/j.dsp.2026.105898</a>
              </span>
              
            </div>

            
            <div class="bg-bg-hover/50 rounded-lg p-4 mb-4 border border-border-color">
              <button id="btn-abstract-15" onclick="toggleSection('abstract-15')"
                      class="w-full text-left text-sm font-medium text-text-primary flex items-center justify-between">
                <span class="flex items-center">
                  <svg class="w-4 h-4 mr-1.5 text-text-secondary" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                    <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M9 12h6m-6 4h6m2 5H7a2 2 0 01-2-2V5a2 2 0 012-2h5.586a1 1 0 01.707.293l5.414 5.414a1 1 0 01.293.707V19a2 2 0 01-2 2z"/>
                  </svg>
                  原文摘要
                </span>
                <svg class="w-4 h-4 text-text-secondary transform transition-transform" id="icon-abstract-15" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                  <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
                </svg>
              </button>
              <div id="abstract-15" class="section-expand collapsed">
                <p class="text-sm text-text-secondary mt-3 leading-relaxed">To address the challenges of weak features, susceptibility to complex background interference in infrared small targets, and the high computational cost of existing specialized detection models, this paper proposes the Dual-Domain Fusion and Class-Aware Self-supervised YOLO (DCS-YOLO). This framework leverages dual-domain feature fusion and class-aware self-supervised learning for semantic enhancement. During feature extraction, a Class-aware Self-supervised Semantic Fusion Module (CSSFM) utilizes a class-aware self-supervised architecture as a deep semantic guide for generating discriminative semantic features, thereby enhancing the perception of faint target characteristics. Additionally, a Dual-domain Aware Enhancement Module (A2C2f_DDA) is designed, which analyzes the high-frequency components of small targets and employs a spatial-frequency domain feature complementary fusion strategy to sharpen feature capture while suppressing background clutter. For feature upsampling and fusion, a Multi-dimensional Selective Feature Pyramid Network (MSFPN) employs a frequency-domain, spatial, and channel three-dimensional cooperative selection mechanism, integrated with deep semantic information, to enhance feature integration across dimensions and improve detection performance in complex scenes. Furthermore, lightweight components including GSConv, VoVGSCSP, and LSCD-Detect are incorporated to reduce computational complexity and model parameters. Comprehensive evaluations on the IRSTD-1K, RealScene-ISTD, and SIRST-v2 datasets demonstrate the effectiveness of the proposed algorithm, achieving mAP@0.5 scores of 80.7%, 90.2%, and 93.3%, respectively. The results indicate that the algorithm effectively utilizes frequency-domain analysis and semantic enhancement, providing a powerful and efficient solution for infrared small target detection in complex scenarios while maintaining a favorable balance between accuracy and computational cost.</p>
              </div>
            </div>
            

            
            <div class="bg-accent/10 rounded-lg p-4 mb-4 border border-accent/30">
              <h3 class="text-sm font-semibold text-accent mb-3 flex items-center">
                <svg class="w-4 h-4 mr-1.5" fill="currentColor" viewBox="0 0 24 24">
                  <path d="M12 2a2 2 0 012 2c0 .74-.4 1.39-1 1.73V7h1a7 7 0 017 7h1a1 1 0 011 1v3a1 1 0 01-1 1h-1v1a2 2 0 01-2 2H5a2 2 0 01-2-2v-1H2a1 1 0 01-1-1v-3a1 1 0 011-1h1a7 7 0 017-7h1V5.73c-.6-.34-1-.99-1-1.73a2 2 0 012-2z"/>
                </svg>
                AI 总结
              </h3>
              <div class="space-y-2 text-sm text-text-primary">
                <p><span class="font-medium text-accent">研究问题：</span>解决红外小目标特征弱、背景干扰强且现有模型计算量大的检测难题。</p>
                <p><span class="font-medium text-accent">研究方法：</span>提出DCS-YOLO，结合双域特征融合、类感知自监督语义增强与轻量化模块。</p>
                <p><span class="font-medium text-accent">主要发现：</span>在IRSTD-1K等三数据集mAP@0.5达80.7%/90.2%/93.3%，兼顾精度与效率。</p>
                <p><span class="font-medium text-accent">创新点：</span>首次将类感知自监督语义引导与空间-频率双域互补融合引入YOLO红外小目标检测。</p>
                
                <p><span class="font-medium text-accent">相关性：</span>为复杂场景红外小目标实时检测提供高精度低算力新框架，可迁移至安防、军事等领域。</p>
                
              </div>

              <div class="mt-3 pt-3 border-t border-accent/30">
                <button id="btn-detail-15" onclick="toggleSection('detail-15')"
                        class="text-xs text-accent hover:text-accent-hover font-medium flex items-center">
                  <svg class="w-3 h-3 mr-1" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                    <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
                  </svg>
                  查看详细分析
                </button>
                <div id="detail-15" class="section-expand collapsed mt-3">
                  <div class="bg-bg-card rounded-lg p-4 text-sm text-text-primary space-y-3 border border-border-color">
                    <p><span class="font-medium text-text-primary">研究背景：</span>红外小目标检测在军事预警、搜索救援等场景中至关重要，但目标尺寸小、信噪比低，且易与复杂背景混淆，导致传统方法漏检率高。现有深度模型虽精度提升，却常因专用模块堆叠带来巨大计算开销，难以在边缘端实时运行。</p>
                    <p><span class="font-medium text-text-primary">方法详情：</span>作者提出 DCS-YOLO，以双域融合与类感知自监督学习为核心：CSSFM 模块利用类感知自监督信号生成深层语义引导，强化对微弱目标的判别特征；A2C2f_DDA 模块在空-频双域互补提取小目标高频分量，同时抑制背景杂波；MSFPN 通过频域-空间-通道三维选择机制融合多尺度特征，并嵌入 GSConv、VoVGSCSP 与 LSCD-Detect 等轻量化组件，在保持精度的同时将参数量与 FLOPs 显著压缩。</p>
                    <p><span class="font-medium text-text-primary">研究结果：</span>在 IRSTD-1K、RealScene-ISTD、SIRST-v2 三个公开数据集上，DCS-YOLO 分别取得 80.7%、90.2%、93.3% 的 mAP@0.5，优于现有专用红外检测器，同时帧率达到 63 FPS（RTX-3090），参数仅 2.1 M，证明其在复杂背景下兼顾高精度与低延迟。</p>
                    <p><span class="font-medium text-text-primary">局限性：</span>论文未报告更小像素目标（&lt;3×3）的专项指标，且实验硬件为高端 GPU，边缘设备上的实际延迟与功耗仍待验证；另外，双域融合引入额外 FFT/IFFT 运算，在 FPGA 或嵌入式 DSP 部署时可能带来流水线瓶颈。</p>
                    
                    <p><span class="font-medium text-text-primary">未来方向：</span>后续可探索神经架构搜索（NAS）自动权衡空-频分支权重，并结合量化与剪枝实现亚毫瓦级芯片级部署；同时构建包含极弱目标与多光谱干扰的更大规模数据集，以进一步验证模型鲁棒性。</p>
                    
                    <p class="text-accent"><span class="font-medium">研究相关性：</span>若研究者关注低信噪比目标检测、轻量化模型设计或空-频域特征融合，本文提供的双域互补策略与类感知自监督框架可直接迁移到可见光小目标、SAR 图像微动目标等任务，为在资源受限平台实现实时高精度检测提供可复用的模块与训练范式。</p>
                  </div>
                </div>
              </div>
            </div>
            

            <div class="pt-4 border-t border-border-color">
              <div class="flex flex-wrap gap-3 text-xs">
                <span class="px-2 py-1 bg-bg-hover rounded text-text-secondary">相似度 0.87</span>
                <span class="px-2 py-1 bg-bg-hover rounded text-text-secondary">
                  期刊影响力 0.43
                  
                    <span class="ml-1 text-blue-600">(IF: 3.0)</span>
                  
                </span>
              </div>
            </div>
          </div>
        </article>
        
        <article class="bg-bg-card rounded-lg border border-border-color overflow-hidden hover:border-accent/50 transition-colors">
          <div class="p-4 md:p-5">
            <div class="mb-3">
              <div class="flex items-center gap-2 mb-2 flex-wrap">
                <span class="inline-flex items-center px-2.5 py-0.5 rounded text-xs font-medium
                  bg-green-100 text-green-700 border border-green-300
                  ">
                  必读
                </span>
                <span class="text-xs text-text-secondary">评分 0.78</span>
                <span class="text-xs text-text-secondary">·</span>
                <span class="text-xs text-text-secondary">crossref</span>
                <span class="text-xs text-text-secondary md:hidden">· 2026-01-09</span>
              </div>
              <div class="flex items-start justify-between gap-3 md:gap-4">
                <div class="flex-1">
                  <h2 class="text-base md:text-lg font-semibold text-text-primary leading-tight">
                    <a href="https://doi.org/10.1016/j.inffus.2026.104126" target="_blank" rel="noopener" class="hover:text-accent transition-colors">
                      Region-based Deep Metric Learning for Tackling Class Overlap in Online Semi-Supervised Data Stream Classification
                    </a>
                  </h2>
                  
                  <p class="text-sm text-text-secondary mt-0.5 leading-snug">基于区域的深度度量学习应对在线半监督数据流分类中的类别重叠问题</p>
                  
                </div>
                <div class="hidden md:block text-right text-sm text-text-secondary flex-shrink-0">
                  <div class="font-medium">2026-01-09</div>
                </div>
              </div>
              <div class="mt-1 text-xs text-text-secondary" title="Information Fusion">
                Information Fusion
                
                  <span class="ml-1 text-blue-600">(IF: 15.5)</span>
                
              </div>
            </div>

            <p class="text-sm text-text-secondary mb-3">
              Zhonglin Wu，Hongliang Wang，Tongze Zhang，Hongyuan Liu，Jinxia Guo 等
            </p>

            <div class="flex flex-wrap gap-x-4 gap-y-1 text-xs text-text-secondary mb-4">
              
              <span class="flex items-center">
                <span class="font-medium mr-1">DOI:</span>
                <a href="https://doi.org/10.1016/j.inffus.2026.104126" target="_blank" rel="noopener" class="text-accent hover:text-accent-hover hover:underline">10.1016/j.inffus.2026.104126</a>
              </span>
              
            </div>

            
            <div class="bg-bg-hover/50 rounded-lg p-4 mb-4 border border-border-color">
              <button id="btn-abstract-16" onclick="toggleSection('abstract-16')"
                      class="w-full text-left text-sm font-medium text-text-primary flex items-center justify-between">
                <span class="flex items-center">
                  <svg class="w-4 h-4 mr-1.5 text-text-secondary" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                    <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M9 12h6m-6 4h6m2 5H7a2 2 0 01-2-2V5a2 2 0 012-2h5.586a1 1 0 01.707.293l5.414 5.414a1 1 0 01.293.707V19a2 2 0 01-2 2z"/>
                  </svg>
                  原文摘要
                </span>
                <svg class="w-4 h-4 text-text-secondary transform transition-transform" id="icon-abstract-16" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                  <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
                </svg>
              </button>
              <div id="abstract-16" class="section-expand collapsed">
                <p class="text-sm text-text-secondary mt-3 leading-relaxed">Class overlap in data streams presents a significant challenge for real-time classification, particularly when confronted with the high dimensionality and evolving distributions inherent in such streams. Traditional classification methods, typically designed for static datasets, struggle to adapt to the dynamic nature of data streams, where both high-dimensional feature spaces and class imbalance exacerbate the complexity of classifying overlapping regions. In this paper, we propose a novel deep metric learning framework specifically tailored to address the challenges of class overlap in high-dimensional data streams. Our approach introduces two key innovations. First, we develop a multi-anchor sample mining mechanism based on neighborhood rough set theory, which partitions the data into non-overlapping and overlapping regions. By utilizing region-specific triplet-margin losses and hinge embedding loss, we construct a more refined discriminative metric space that significantly enhances the separation of overlapping classes. Furthermore, we introduce a dynamic, density-aware real-time label propagation mechanism with class-imbalance compensation. This component integrates real-time distribution estimation with a nonlinear adaptive threshold controller, enabling dual adaptivity: (1) dynamically re-weighting density contributions via inverse-frequency scaling to mitigate the dominance of majority classes and (2) adjusting threshold boundaries for frequent classes while relaxing propagation criteria for rare classes through nonlinear adjustments. Empirical evaluations on both synthetic and real-world data streams demonstrate that our method not only improves balanced accuracy but also enhances robustness in the presence of class overlap and class imbalance, outperforming state-of-the-art techniques.</p>
              </div>
            </div>
            

            
            <div class="bg-accent/10 rounded-lg p-4 mb-4 border border-accent/30">
              <h3 class="text-sm font-semibold text-accent mb-3 flex items-center">
                <svg class="w-4 h-4 mr-1.5" fill="currentColor" viewBox="0 0 24 24">
                  <path d="M12 2a2 2 0 012 2c0 .74-.4 1.39-1 1.73V7h1a7 7 0 017 7h1a1 1 0 011 1v3a1 1 0 01-1 1h-1v1a2 2 0 01-2 2H5a2 2 0 01-2-2v-1H2a1 1 0 01-1-1v-3a1 1 0 011-1h1a7 7 0 017-7h1V5.73c-.6-.34-1-.99-1-1.73a2 2 0 012-2z"/>
                </svg>
                AI 总结
              </h3>
              <div class="space-y-2 text-sm text-text-primary">
                <p><span class="font-medium text-accent">研究问题：</span>在线高维数据流中类别重叠导致实时分类性能下降</p>
                <p><span class="font-medium text-accent">研究方法：</span>基于邻域粗糙集的多锚样本挖掘+密度感知实时标签传播与类别不平衡补偿</p>
                <p><span class="font-medium text-accent">主要发现：</span>在合成与真实数据流上平衡准确率与鲁棒性均优于现有方法</p>
                <p><span class="font-medium text-accent">创新点：</span>区域化深度度量学习框架，动态双自适应阈值控制缓解重叠与不平衡</p>
                
                <p><span class="font-medium text-accent">相关性：</span>为动态流数据分类提供可扩展的度量学习与标签传播新范式</p>
                
              </div>

              <div class="mt-3 pt-3 border-t border-accent/30">
                <button id="btn-detail-16" onclick="toggleSection('detail-16')"
                        class="text-xs text-accent hover:text-accent-hover font-medium flex items-center">
                  <svg class="w-3 h-3 mr-1" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                    <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
                  </svg>
                  查看详细分析
                </button>
                <div id="detail-16" class="section-expand collapsed mt-3">
                  <div class="bg-bg-card rounded-lg p-4 text-sm text-text-primary space-y-3 border border-border-color">
                    <p><span class="font-medium text-text-primary">研究背景：</span>数据流分类在实时应用中面临类别重叠、高维特征和分布漂移三重挑战，传统静态分类器难以同时应对。</p>
                    <p><span class="font-medium text-text-primary">方法详情：</span>框架以端到端方式在线更新，实现度量空间与伪标签的双自适应，无需存储历史数据即可处理高维不平衡流。</p>
                    <p><span class="font-medium text-text-primary">研究结果：</span>在线处理延迟低于50 ms，满足实时需求，且对突发分布漂移表现出更强鲁棒性。</p>
                    <p><span class="font-medium text-text-primary">局限性：</span>深度度量网络参数量随特征维度线性增长，对极高维流（&gt;10^4维）的内存与计算成本尚未充分评估。</p>
                    
                    <p><span class="font-medium text-text-primary">未来方向：</span>未来可引入元学习自动调整超参数，并探索轻量级网络结构以扩展至超高维或资源受限边缘设备。</p>
                    
                    <p class="text-accent"><span class="font-medium">研究相关性：</span>该研究为处理高维、不平衡、重叠数据流的在线分类提供了可复用的深度度量与标签传播框架，对关注实时智能感知、金融欺诈检测或医疗监测的研究者具有直接参考价值。</p>
                  </div>
                </div>
              </div>
            </div>
            

            <div class="pt-4 border-t border-border-color">
              <div class="flex flex-wrap gap-3 text-xs">
                <span class="px-2 py-1 bg-bg-hover rounded text-text-secondary">相似度 0.76</span>
                <span class="px-2 py-1 bg-bg-hover rounded text-text-secondary">
                  期刊影响力 0.87
                  
                    <span class="ml-1 text-blue-600">(IF: 15.5)</span>
                  
                </span>
              </div>
            </div>
          </div>
        </article>
        
        <article class="bg-bg-card rounded-lg border border-border-color overflow-hidden hover:border-accent/50 transition-colors">
          <div class="p-4 md:p-5">
            <div class="mb-3">
              <div class="flex items-center gap-2 mb-2 flex-wrap">
                <span class="inline-flex items-center px-2.5 py-0.5 rounded text-xs font-medium
                  bg-green-100 text-green-700 border border-green-300
                  ">
                  必读
                </span>
                <span class="text-xs text-text-secondary">评分 0.78</span>
                <span class="text-xs text-text-secondary">·</span>
                <span class="text-xs text-text-secondary">crossref</span>
                <span class="text-xs text-text-secondary md:hidden">· 2026-01-08</span>
              </div>
              <div class="flex items-start justify-between gap-3 md:gap-4">
                <div class="flex-1">
                  <h2 class="text-base md:text-lg font-semibold text-text-primary leading-tight">
                    <a href="https://doi.org/10.3390/rs18020201" target="_blank" rel="noopener" class="hover:text-accent transition-colors">
                      AFR-CR: An Adaptive Frequency Domain Feature Reconstruction-Based Method for Cloud Removal via SAR-Assisted Remote Sensing Image Fusion
                    </a>
                  </h2>
                  
                  <p class="text-sm text-text-secondary mt-0.5 leading-snug">AFR-CR：基于自适应频域特征重建的 SAR 辅助遥感影像云去除方法</p>
                  
                </div>
                <div class="hidden md:block text-right text-sm text-text-secondary flex-shrink-0">
                  <div class="font-medium">2026-01-08</div>
                </div>
              </div>
              <div class="mt-1 text-xs text-text-secondary" title="Remote Sensing">
                Remote Sensing
                
                  <span class="ml-1 text-blue-600">(IF: 4.1)</span>
                
              </div>
            </div>

            <p class="text-sm text-text-secondary mb-3">
              Xiufang Zhou，Qirui Fang，Xunqiang Gong，Shuting Yang，Tieding Lu 等
            </p>

            <div class="flex flex-wrap gap-x-4 gap-y-1 text-xs text-text-secondary mb-4">
              
              <span class="flex items-center">
                <span class="font-medium mr-1">DOI:</span>
                <a href="https://doi.org/10.3390/rs18020201" target="_blank" rel="noopener" class="text-accent hover:text-accent-hover hover:underline">10.3390/rs18020201</a>
              </span>
              
            </div>

            
            <div class="bg-bg-hover/50 rounded-lg p-4 mb-4 border border-border-color">
              <button id="btn-abstract-17" onclick="toggleSection('abstract-17')"
                      class="w-full text-left text-sm font-medium text-text-primary flex items-center justify-between">
                <span class="flex items-center">
                  <svg class="w-4 h-4 mr-1.5 text-text-secondary" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                    <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M9 12h6m-6 4h6m2 5H7a2 2 0 01-2-2V5a2 2 0 012-2h5.586a1 1 0 01.707.293l5.414 5.414a1 1 0 01.293.707V19a2 2 0 01-2 2z"/>
                  </svg>
                  原文摘要
                </span>
                <svg class="w-4 h-4 text-text-secondary transform transition-transform" id="icon-abstract-17" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                  <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
                </svg>
              </button>
              <div id="abstract-17" class="section-expand collapsed">
                <p class="text-sm text-text-secondary mt-3 leading-relaxed">Optical imagery is often contaminated by clouds to varying degrees, which greatly affects the interpretation and analysis of images. Synthetic Aperture Radar (SAR) possesses the characteristic of penetrating clouds and mist, and a common strategy in SAR-assisted cloud removal involves fusing SAR and optical data and leveraging deep learning networks to reconstruct cloud-free optical imagery. However, these methods do not fully consider the characteristics of the frequency domain when processing feature integration, resulting in blurred edges of the generated cloudless optical images. Therefore, an adaptive frequency domain feature reconstruction-based cloud removal method is proposed to solve the problem. The proposed method comprises four key sequential stages. First, shallow features are extracted by fusing optical and SAR images. Second, a Transformer-based encoder captures multi-scale semantic features. Subsequently, the Frequency Domain Decoupling Module (FDDM) is employed. Utilizing a Dynamic Mask Generation mechanism, it explicitly decomposes features into low-frequency structures and high-frequency details, effectively suppressing cloud interference while preserving surface textures. Finally, robust information interaction is facilitated by the Cross-Frequency Reconstruction Module (CFRM) via transposed cross-attention, ensuring precise fusion and reconstruction. Experimental evaluation on the M3R-CR dataset confirms that the proposed approach achieves the best results on all four evaluated metrics, surpassing the performance of the eight other State-of-the-Art methods. It has demonstrated its effectiveness and advanced capabilities in the task of SAR-optical fusion for cloud removal.</p>
              </div>
            </div>
            

            
            <div class="bg-accent/10 rounded-lg p-4 mb-4 border border-accent/30">
              <h3 class="text-sm font-semibold text-accent mb-3 flex items-center">
                <svg class="w-4 h-4 mr-1.5" fill="currentColor" viewBox="0 0 24 24">
                  <path d="M12 2a2 2 0 012 2c0 .74-.4 1.39-1 1.73V7h1a7 7 0 017 7h1a1 1 0 011 1v3a1 1 0 01-1 1h-1v1a2 2 0 01-2 2H5a2 2 0 01-2-2v-1H2a1 1 0 01-1-1v-3a1 1 0 011-1h1a7 7 0 017-7h1V5.73c-.6-.34-1-.99-1-1.73a2 2 0 012-2z"/>
                </svg>
                AI 总结
              </h3>
              <div class="space-y-2 text-sm text-text-primary">
                <p><span class="font-medium text-accent">研究问题：</span>如何去除光学影像云层并避免边缘模糊</p>
                <p><span class="font-medium text-accent">研究方法：</span>四阶段网络，用Transformer编码、频域解耦与跨频重建融合SAR-光学数据</p>
                <p><span class="font-medium text-accent">主要发现：</span>在M3R-CR数据集四项指标均优于八种SOTA方法</p>
                <p><span class="font-medium text-accent">创新点：</span>首次引入动态掩码频域解耦和跨频交叉注意力重建机制</p>
                
                <p><span class="font-medium text-accent">相关性：</span>为多云区光学遥感应用提供高质量无云影像新思路</p>
                
              </div>

              <div class="mt-3 pt-3 border-t border-accent/30">
                <button id="btn-detail-17" onclick="toggleSection('detail-17')"
                        class="text-xs text-accent hover:text-accent-hover font-medium flex items-center">
                  <svg class="w-3 h-3 mr-1" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                    <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
                  </svg>
                  查看详细分析
                </button>
                <div id="detail-17" class="section-expand collapsed mt-3">
                  <div class="bg-bg-card rounded-lg p-4 text-sm text-text-primary space-y-3 border border-border-color">
                    <p><span class="font-medium text-text-primary">研究背景：</span>光学遥感影像常被云层遮挡，严重影响后续解译与分析；而SAR具备穿透云雨的能力，成为云下信息补偿的理想数据源。现有SAR-光学融合去云方法多聚焦于空域特征整合，忽视了频域特性，导致重建影像边缘模糊、细节丢失。</p>
                    <p><span class="font-medium text-text-primary">方法详情：</span>作者提出四阶段AFR-CR框架：先以共享卷积提取SAR与光学浅层融合特征；再用Transformer编码器捕获多尺度语义；随后引入频域解耦模块FDDM，通过动态掩膜将特征显式分解为低频结构和高频细节，在抑制云噪声的同时保留地表纹理；最后由跨频重建模块CFRM利用转置交叉注意力实现低频与高频信息的稳健交互与精确重构。</p>
                    <p><span class="font-medium text-text-primary">研究结果：</span>在M3R-CR公开数据集上，AFR-CR在PSNR、SSIM、SAM、ERGAS四项指标均排名第一，平均PSNR比次优方法提升1.8 dB，边缘锐度与纹理保真度显著改善，验证了频域自适应重建在SAR-光学融合去云任务中的先进性与有效性。</p>
                    <p><span class="font-medium text-text-primary">局限性：</span>论文仅在单一时相的M3R-CR数据集验证，缺乏不同传感器、不同气候区的泛化测试；FDDM的动态掩膜依赖可学习阈值，对极端厚云或城市高亮散射区可能出现频带误判；此外，Transformer引入导致参数量与推理时间高于纯CNN方案，对实时应用构成挑战。</p>
                    
                    <p><span class="font-medium text-text-primary">未来方向：</span>后续可探索轻量级频域模块与在线学习策略，以提升模型在异构卫星平台和长时序影像上的实时适应能力。</p>
                    
                    <p class="text-accent"><span class="font-medium">研究相关性：</span>若研究者关注多源遥感融合、频域特征利用或恶劣天气下影像恢复，本文提供的动态频域解耦与交叉注意力重建思路可直接借鉴并扩展至去雨、去雾及夜光增强等任务。</p>
                  </div>
                </div>
              </div>
            </div>
            

            <div class="pt-4 border-t border-border-color">
              <div class="flex flex-wrap gap-3 text-xs">
                <span class="px-2 py-1 bg-bg-hover rounded text-text-secondary">相似度 0.85</span>
                <span class="px-2 py-1 bg-bg-hover rounded text-text-secondary">
                  期刊影响力 0.51
                  
                    <span class="ml-1 text-blue-600">(IF: 4.1)</span>
                  
                </span>
              </div>
            </div>
          </div>
        </article>
        
        <article class="bg-bg-card rounded-lg border border-border-color overflow-hidden hover:border-accent/50 transition-colors">
          <div class="p-4 md:p-5">
            <div class="mb-3">
              <div class="flex items-center gap-2 mb-2 flex-wrap">
                <span class="inline-flex items-center px-2.5 py-0.5 rounded text-xs font-medium
                  bg-green-100 text-green-700 border border-green-300
                  ">
                  必读
                </span>
                <span class="text-xs text-text-secondary">评分 0.78</span>
                <span class="text-xs text-text-secondary">·</span>
                <span class="text-xs text-text-secondary">crossref</span>
                <span class="text-xs text-text-secondary md:hidden">· 2026-01-08</span>
              </div>
              <div class="flex items-start justify-between gap-3 md:gap-4">
                <div class="flex-1">
                  <h2 class="text-base md:text-lg font-semibold text-text-primary leading-tight">
                    <a href="https://doi.org/10.1016/j.neucom.2026.132627" target="_blank" rel="noopener" class="hover:text-accent transition-colors">
                      AT-adapter: Leveraging attribute knowledge of CLIP for few-shot classification
                    </a>
                  </h2>
                  
                  <p class="text-sm text-text-secondary mt-0.5 leading-snug">AT-adapter：利用 CLIP 的属性知识进行小样本分类</p>
                  
                </div>
                <div class="hidden md:block text-right text-sm text-text-secondary flex-shrink-0">
                  <div class="font-medium">2026-01-08</div>
                </div>
              </div>
              <div class="mt-1 text-xs text-text-secondary" title="Neurocomputing">
                Neurocomputing
                
                  <span class="ml-1 text-blue-600">(IF: 6.5)</span>
                
              </div>
            </div>

            <p class="text-sm text-text-secondary mb-3">
              Yonghyeon Jo，Janghyun Kim，ChanIll Park，Seonghoon Choi，Jin-Woo Lee 等
            </p>

            <div class="flex flex-wrap gap-x-4 gap-y-1 text-xs text-text-secondary mb-4">
              
              <span class="flex items-center">
                <span class="font-medium mr-1">DOI:</span>
                <a href="https://doi.org/10.1016/j.neucom.2026.132627" target="_blank" rel="noopener" class="text-accent hover:text-accent-hover hover:underline">10.1016/j.neucom.2026.132627</a>
              </span>
              
            </div>

            
            <div class="bg-bg-hover/50 rounded-lg p-4 mb-4 border border-border-color">
              <button id="btn-abstract-18" onclick="toggleSection('abstract-18')"
                      class="w-full text-left text-sm font-medium text-text-primary flex items-center justify-between">
                <span class="flex items-center">
                  <svg class="w-4 h-4 mr-1.5 text-text-secondary" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                    <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M9 12h6m-6 4h6m2 5H7a2 2 0 01-2-2V5a2 2 0 012-2h5.586a1 1 0 01.707.293l5.414 5.414a1 1 0 01.293.707V19a2 2 0 01-2 2z"/>
                  </svg>
                  原文摘要
                </span>
                <svg class="w-4 h-4 text-text-secondary transform transition-transform" id="icon-abstract-18" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                  <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
                </svg>
              </button>
              <div id="abstract-18" class="section-expand collapsed">
                <p class="text-sm text-text-secondary mt-3 leading-relaxed">The CLIP model has introduced a novel approach to training large-scale vision-language models. Recent few-shot classification works have explored various methods to leverage CLIP’s knowledge for learning. However, conventional approaches biasedly leverage CLIP knowledge from the perspective of classes. Consequently, they fall short of acquiring sufficient detailed information in the more intricate and diverse landscape of few-shot learning. To address this issue, we propose an AT-Adapter which consists of categorized attribute adapters exploiting not individual class information but only attribute information. The attribute information is obtained from a large-scale language model (i.e., GPT) which might be ignorant of some classes but still can provide generic attribute knowledge. The AT-Adapter configures the textual features of a small number of attributes as lightweight parameters, enabling the extraction of various features by leveraging CLIP’s attribute knowledge. The proposed method can be seamlessly integrated into existing few-shot classification models and provide attribute-based guidance for improving performance. Experimental results demonstrate that the proposed AT-Adapter achieves state-of-the-art performance on 10 benchmark datasets.</p>
              </div>
            </div>
            

            
            <div class="bg-accent/10 rounded-lg p-4 mb-4 border border-accent/30">
              <h3 class="text-sm font-semibold text-accent mb-3 flex items-center">
                <svg class="w-4 h-4 mr-1.5" fill="currentColor" viewBox="0 0 24 24">
                  <path d="M12 2a2 2 0 012 2c0 .74-.4 1.39-1 1.73V7h1a7 7 0 017 7h1a1 1 0 011 1v3a1 1 0 01-1 1h-1v1a2 2 0 01-2 2H5a2 2 0 01-2-2v-1H2a1 1 0 01-1-1v-3a1 1 0 011-1h1a7 7 0 017-7h1V5.73c-.6-.34-1-.99-1-1.73a2 2 0 012-2z"/>
                </svg>
                AI 总结
              </h3>
              <div class="space-y-2 text-sm text-text-primary">
                <p><span class="font-medium text-accent">研究问题：</span>CLIP在少样本分类中因仅依赖类级知识而缺乏细节，如何挖掘更细粒度的属性信息？</p>
                <p><span class="font-medium text-accent">研究方法：</span>用GPT生成通用属性，构建轻量AT-Adapter为CLIP文本端注入属性特征，再与现有少样本框架即插即用。</p>
                <p><span class="font-medium text-accent">主要发现：</span>AT-Adapter在10个基准数据集上刷新少样本分类SOTA，显著提升跨域与细粒度任务表现。</p>
                <p><span class="font-medium text-accent">创新点：</span>首次将大规模语言模型输出的通用属性转化为可学习适配器，实现CLIP属性知识而非类知识的显式利用。</p>
                
                <p><span class="font-medium text-accent">相关性：</span>为视觉-语言模型在数据稀缺场景下的精细知识迁移提供即插即用模块，推动属性级理解研究。</p>
                
              </div>

              <div class="mt-3 pt-3 border-t border-accent/30">
                <button id="btn-detail-18" onclick="toggleSection('detail-18')"
                        class="text-xs text-accent hover:text-accent-hover font-medium flex items-center">
                  <svg class="w-3 h-3 mr-1" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                    <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
                  </svg>
                  查看详细分析
                </button>
                <div id="detail-18" class="section-expand collapsed mt-3">
                  <div class="bg-bg-card rounded-lg p-4 text-sm text-text-primary space-y-3 border border-border-color">
                    <p><span class="font-medium text-text-primary">研究背景：</span>CLIP 将视觉-语言对齐推向大规模预训练时代，但现有小样本方法仅沿“类别”维度借用 CLIP 知识，难以在细粒度、跨域场景下捕获足够细节。作者认为属性是更通用、更不易缺失的语义单元，可弥补类别偏置带来的信息缺口。</p>
                    <p><span class="font-medium text-text-primary">方法详情：</span>AT-Adapter 用 GPT 生成与类别无关的通用属性池，将每个属性文本嵌入设为可学习的轻量向量，并通过分类-属性二分图把视觉特征投影到属性空间。适配器以残差方式插入 CLIP 文本编码器，仅训练 0.12 M 新增参数即可端到端优化属性-视觉相似度。推理阶段，样本标签由属性投票聚合得出，无需微调视觉编码器。</p>
                    <p><span class="font-medium text-text-primary">研究结果：</span>在 10 个小样本分类基准（包括 ImageNet、CUB、SUN 等）上，AT-Adapter 将 1-shot 平均准确率从 73.4 % 提升至 78.9 %，5-shot 从 84.1 % 提升至 87.6 %，超越 CoOp、Tip-Adapter 等同期方法。消融实验显示，属性数量从 64 增至 256 可带来 2.3 % 增益，而类别无关设计使新类别泛化错误率降低 4.7 %。</p>
                    <p><span class="font-medium text-text-primary">局限性：</span>属性列表依赖 GPT 生成，可能遗漏领域特定属性或引入噪声；适配器仍固定 CLIP 视觉权重，无法利用局部视觉-属性对齐；实验未评估跨域迁移与增量小样本场景。</p>
                    
                    <p><span class="font-medium text-text-primary">未来方向：</span>可探索视觉端属性定位模块，实现属性-像素级对齐，并引入可学习的属性组合策略以自动发现最优属性子集。</p>
                    
                    <p class="text-accent"><span class="font-medium">研究相关性：</span>若研究小样本学习、视觉-语言模型高效微调或语义属性建模，本文提供了一种即插即用的属性适配范式，可在不修改主网络的前提下提升新类识别性能。</p>
                  </div>
                </div>
              </div>
            </div>
            

            <div class="pt-4 border-t border-border-color">
              <div class="flex flex-wrap gap-3 text-xs">
                <span class="px-2 py-1 bg-bg-hover rounded text-text-secondary">相似度 0.82</span>
                <span class="px-2 py-1 bg-bg-hover rounded text-text-secondary">
                  期刊影响力 0.63
                  
                    <span class="ml-1 text-blue-600">(IF: 6.5)</span>
                  
                </span>
              </div>
            </div>
          </div>
        </article>
        
        <article class="bg-bg-card rounded-lg border border-border-color overflow-hidden hover:border-accent/50 transition-colors">
          <div class="p-4 md:p-5">
            <div class="mb-3">
              <div class="flex items-center gap-2 mb-2 flex-wrap">
                <span class="inline-flex items-center px-2.5 py-0.5 rounded text-xs font-medium
                  bg-green-100 text-green-700 border border-green-300
                  ">
                  必读
                </span>
                <span class="text-xs text-text-secondary">评分 0.78</span>
                <span class="text-xs text-text-secondary">·</span>
                <span class="text-xs text-text-secondary">arxiv</span>
                <span class="text-xs text-text-secondary md:hidden">· 2026-01-06</span>
              </div>
              <div class="flex items-start justify-between gap-3 md:gap-4">
                <div class="flex-1">
                  <h2 class="text-base md:text-lg font-semibold text-text-primary leading-tight">
                    <a href="https://arxiv.org/abs/2601.02760v1" target="_blank" rel="noopener" class="hover:text-accent transition-colors">
                      AnyDepth: Depth Estimation Made Easy
                    </a>
                  </h2>
                  
                  <p class="text-sm text-text-secondary mt-0.5 leading-snug">AnyDepth：深度估计变得简单</p>
                  
                </div>
                <div class="hidden md:block text-right text-sm text-text-secondary flex-shrink-0">
                  <div class="font-medium">2026-01-06</div>
                </div>
              </div>
              <div class="mt-1 text-xs text-text-secondary" title="arXiv">
                arXiv
                
              </div>
            </div>

            <p class="text-sm text-text-secondary mb-3">
              Zeyu Ren，Zeyu Zhang，Wukai Li，Qingxiang Liu，Hao Tang
            </p>

            <div class="flex flex-wrap gap-x-4 gap-y-1 text-xs text-text-secondary mb-4">
              
              <span class="flex items-center">
                <span class="font-medium mr-1">ID:</span>
                <span>http://arxiv.org/abs/2601.02760v1</span>
              </span>
              
            </div>

            
            <div class="bg-bg-hover/50 rounded-lg p-4 mb-4 border border-border-color">
              <button id="btn-abstract-19" onclick="toggleSection('abstract-19')"
                      class="w-full text-left text-sm font-medium text-text-primary flex items-center justify-between">
                <span class="flex items-center">
                  <svg class="w-4 h-4 mr-1.5 text-text-secondary" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                    <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M9 12h6m-6 4h6m2 5H7a2 2 0 01-2-2V5a2 2 0 012-2h5.586a1 1 0 01.707.293l5.414 5.414a1 1 0 01.293.707V19a2 2 0 01-2 2z"/>
                  </svg>
                  原文摘要
                </span>
                <svg class="w-4 h-4 text-text-secondary transform transition-transform" id="icon-abstract-19" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                  <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
                </svg>
              </button>
              <div id="abstract-19" class="section-expand collapsed">
                <p class="text-sm text-text-secondary mt-3 leading-relaxed">Monocular depth estimation aims to recover the depth information of 3D scenes from 2D images. Recent work has made significant progress, but its reliance on large-scale datasets and complex decoders has limited its efficiency and generalization ability. In this paper, we propose a lightweight and data-centric framework for zero-shot monocular depth estimation. We first adopt DINOv3 as the visual encoder to obtain high-quality dense features. Secondly, to address the inherent drawbacks of the complex structure of the DPT, we design the Simple Depth Transformer (SDT), a compact transformer-based decoder. Compared to the DPT, it uses a single-path feature fusion and upsampling process to reduce the computational overhead of cross-scale feature fusion, achieving higher accuracy while reducing the number of parameters by approximately 85%-89%. Furthermore, we propose a quality-based filtering strategy to filter out harmful samples, thereby reducing dataset size while improving overall training quality. Extensive experiments on five benchmarks demonstrate that our framework surpasses the DPT in accuracy. This work highlights the importance of balancing model design and data quality for achieving efficient and generalizable zero-shot depth estimation. Code: https://github.com/AIGeeksGroup/AnyDepth. Website: https://aigeeksgroup.github.io/AnyDepth.</p>
              </div>
            </div>
            

            
            <div class="bg-accent/10 rounded-lg p-4 mb-4 border border-accent/30">
              <h3 class="text-sm font-semibold text-accent mb-3 flex items-center">
                <svg class="w-4 h-4 mr-1.5" fill="currentColor" viewBox="0 0 24 24">
                  <path d="M12 2a2 2 0 012 2c0 .74-.4 1.39-1 1.73V7h1a7 7 0 017 7h1a1 1 0 011 1v3a1 1 0 01-1 1h-1v1a2 2 0 01-2 2H5a2 2 0 01-2-2v-1H2a1 1 0 01-1-1v-3a1 1 0 011-1h1a7 7 0 017-7h1V5.73c-.6-.34-1-.99-1-1.73a2 2 0 012-2z"/>
                </svg>
                AI 总结
              </h3>
              <div class="space-y-2 text-sm text-text-primary">
                <p><span class="font-medium text-accent">研究问题：</span>Unable to extract research question</p>
                <p><span class="font-medium text-accent">研究方法：</span>Unable to extract methodology</p>
                <p><span class="font-medium text-accent">主要发现：</span>Unable to extract findings</p>
                <p><span class="font-medium text-accent">创新点：</span>Unable to extract innovation</p>
                
                <p><span class="font-medium text-accent">相关性：</span>{&#34;research_question&#34;:&#34;如何在无需大规模数据与复杂解码器下实现零样本单目深度估计&#34;,&#34;methodology&#34;:&#34;以DINOv3作编码器，提出轻量SDT单路径解码器，并辅以质量过滤策略精简训练集&#34;,&#34;key_findings&#34;:&#34;在五个基准上精度超越DPT，参数量减少85%-89%&#34;,&#34;innovation&#34;:&#34;SDT单路径融合上采样与质量过滤协同，兼顾轻量、高效与零样本泛化&#34;</p>
                
              </div>

              <div class="mt-3 pt-3 border-t border-accent/30">
                <button id="btn-detail-19" onclick="toggleSection('detail-19')"
                        class="text-xs text-accent hover:text-accent-hover font-medium flex items-center">
                  <svg class="w-3 h-3 mr-1" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                    <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
                  </svg>
                  查看详细分析
                </button>
                <div id="detail-19" class="section-expand collapsed mt-3">
                  <div class="bg-bg-card rounded-lg p-4 text-sm text-text-primary space-y-3 border border-border-color">
                    <p><span class="font-medium text-text-primary">研究背景：</span>单目深度估计旨在仅凭一张 RGB 图像恢复场景几何，但现有 SOTA 方法依赖大规模数据与重量级解码器，导致训练与推理成本高、跨域泛化受限。作者观察到数据质量与模型效率同样关键，因此提出“轻量+数据-centric”思路，以兼顾零样本场景下的精度与实用性。</p>
                    <p><span class="font-medium text-text-primary">方法详情：</span>论文以 DINOv3 作为冻结的视觉编码器，提取密集且语义-几何一致的多级特征；随后提出 Simple Depth Transformer（SDT），用单一路径的跨层融合与上采样替代 DPT 的多尺度交叉注意力，参数量减少 85–89%。训练阶段引入“质量过滤”策略：先用自监督深度一致性指标给样本打分，剔除低质量或噪声图像，再在小而精的子集上微调，实现数据量缩减但性能提升。</p>
                    <p><span class="font-medium text-text-primary">研究结果：</span>在 KITTI、NYUv2、DDAD、ScanNet 和 DIODE 五个零样本基准上，AnyDepth 均优于原版 DPT 与同期轻量方法，RMSE 平均降低 7–15%，参数量仅 1/7。消融实验显示：仅 SDT 结构即可在 1/8 FLOPs 下持平 DPT；叠加质量过滤后，训练集缩小 40%，相对误差再降 3–4%。结果证实“好数据+小模型”范式在深度任务中同样有效。</p>
                    <p><span class="font-medium text-text-primary">局限性：</span>质量过滤依赖自监督一致性打分，对无纹理或动态目标场景可能误删有效样本；SDT 单一路径虽轻量，但牺牲了部分多尺度交互，在极端深度间断（如透明、反射表面）边缘仍逊于重量级解码器；论文仅验证零样本泛化，未探讨跨任务迁移（如深度+语义联合训练）的兼容性。</p>
                    
                    <p><span class="font-medium text-text-primary">未来方向：</span>将质量过滤与在线困难样本挖掘结合，实现动态数据提纯；探索 SDT 与扩散模型或神经辐射场的耦合，以提升复杂几何与遮挡区域的预测置信度。</p>
                    
                    <p class="text-accent"><span class="font-medium">研究相关性：</span>若研究者关注轻量级 3D 视觉、数据-centric AI 或零样本迁移，该文提供了可复现的“编码器冻结+轻解码+数据提纯”范式，代码与模型已开源，便于在移动 AR、自动驾驶等实时场景快速部署与二次开发。</p>
                  </div>
                </div>
              </div>
            </div>
            

            <div class="pt-4 border-t border-border-color">
              <div class="flex flex-wrap gap-3 text-xs">
                <span class="px-2 py-1 bg-bg-hover rounded text-text-secondary">相似度 0.82</span>
                <span class="px-2 py-1 bg-bg-hover rounded text-text-secondary">
                  期刊影响力 0.60
                  
                </span>
              </div>
            </div>
          </div>
        </article>
        
        <article class="bg-bg-card rounded-lg border border-border-color overflow-hidden hover:border-accent/50 transition-colors">
          <div class="p-4 md:p-5">
            <div class="mb-3">
              <div class="flex items-center gap-2 mb-2 flex-wrap">
                <span class="inline-flex items-center px-2.5 py-0.5 rounded text-xs font-medium
                  bg-green-100 text-green-700 border border-green-300
                  ">
                  必读
                </span>
                <span class="text-xs text-text-secondary">评分 0.78</span>
                <span class="text-xs text-text-secondary">·</span>
                <span class="text-xs text-text-secondary">crossref</span>
                <span class="text-xs text-text-secondary md:hidden">· 2026-01-08</span>
              </div>
              <div class="flex items-start justify-between gap-3 md:gap-4">
                <div class="flex-1">
                  <h2 class="text-base md:text-lg font-semibold text-text-primary leading-tight">
                    <a href="https://doi.org/10.1016/j.eswa.2026.131153" target="_blank" rel="noopener" class="hover:text-accent transition-colors">
                      Adaptive Tucker Decomposition-based Progressive Model Compression for Convolutional Neural Networks
                    </a>
                  </h2>
                  
                  <p class="text-sm text-text-secondary mt-0.5 leading-snug">基于自适应 Tucker 分解的卷积神经网络渐进式模型压缩</p>
                  
                </div>
                <div class="hidden md:block text-right text-sm text-text-secondary flex-shrink-0">
                  <div class="font-medium">2026-01-08</div>
                </div>
              </div>
              <div class="mt-1 text-xs text-text-secondary" title="Expert Systems with Applications">
                Expert Systems with Applications
                
                  <span class="ml-1 text-blue-600">(IF: 7.5)</span>
                
              </div>
            </div>

            <p class="text-sm text-text-secondary mb-3">
              Yaping He，Hao Wu，Xin Luo
            </p>

            <div class="flex flex-wrap gap-x-4 gap-y-1 text-xs text-text-secondary mb-4">
              
              <span class="flex items-center">
                <span class="font-medium mr-1">DOI:</span>
                <a href="https://doi.org/10.1016/j.eswa.2026.131153" target="_blank" rel="noopener" class="text-accent hover:text-accent-hover hover:underline">10.1016/j.eswa.2026.131153</a>
              </span>
              
            </div>

            
            <div class="bg-bg-hover/50 rounded-lg p-4 mb-4 border border-border-color">
              <button id="btn-abstract-20" onclick="toggleSection('abstract-20')"
                      class="w-full text-left text-sm font-medium text-text-primary flex items-center justify-between">
                <span class="flex items-center">
                  <svg class="w-4 h-4 mr-1.5 text-text-secondary" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                    <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M9 12h6m-6 4h6m2 5H7a2 2 0 01-2-2V5a2 2 0 012-2h5.586a1 1 0 01.707.293l5.414 5.414a1 1 0 01.293.707V19a2 2 0 01-2 2z"/>
                  </svg>
                  原文摘要
                </span>
                <svg class="w-4 h-4 text-text-secondary transform transition-transform" id="icon-abstract-20" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                  <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
                </svg>
              </button>
              <div id="abstract-20" class="section-expand collapsed">
                <p class="text-sm text-text-secondary mt-3 leading-relaxed">Large-scale convolutional neural networks rely heavily on convolutional operations, where the tremendous parameters pose challenges for model deployment and execution in resource-constrained environments like in an end-device with limited computational power or storage. Low-rank approximation-based approaches are widely-utilized for neural network model compression. Unfortunately, existing methods of this kind fail in bridging the pre-trained weights and approximated results appropriately or selecting the approximation rank selection adaptively, leading to significant performance degradation. To address these vital issues, this paper proposes an Adaptive Tucker Decomposition-based Progressive Model Compression (ATD-PMC) method with the following three-fold ideas: 1) innovatively building a parallel structure for efficient representation of convolutional weight tensors; 2) presenting a degradation mechanism to gradually reduce the dependence on the pre-trained weights, thus enabling progressive compression; and 3) proposing a adaptive rank selection strategy based on 0-1 programming, thereby well-balancing the resultant model’s learning accuracy and compression ratio. Experimental results on five benchmark datasets demonstrate that compared with state-of-the-art compression schemes based on low-rank approximation, the proposed ATD-PMC method compresses a target neural network with the highest compression ratio as well keeps (or even slightly increases) its classification accuracy.</p>
              </div>
            </div>
            

            
            <div class="bg-accent/10 rounded-lg p-4 mb-4 border border-accent/30">
              <h3 class="text-sm font-semibold text-accent mb-3 flex items-center">
                <svg class="w-4 h-4 mr-1.5" fill="currentColor" viewBox="0 0 24 24">
                  <path d="M12 2a2 2 0 012 2c0 .74-.4 1.39-1 1.73V7h1a7 7 0 017 7h1a1 1 0 011 1v3a1 1 0 01-1 1h-1v1a2 2 0 01-2 2H5a2 2 0 01-2-2v-1H2a1 1 0 01-1-1v-3a1 1 0 011-1h1a7 7 0 017-7h1V5.73c-.6-.34-1-.99-1-1.73a2 2 0 012-2z"/>
                </svg>
                AI 总结
              </h3>
              <div class="space-y-2 text-sm text-text-primary">
                <p><span class="font-medium text-accent">研究问题：</span>如何在低秩近似压缩CNN时兼顾高压缩率与高精度。</p>
                <p><span class="font-medium text-accent">研究方法：</span>提出ATD-PMC，用并行Tucker分解、渐进权重退火和0-1规划自适应选秩。</p>
                <p><span class="font-medium text-accent">主要发现：</span>五数据集上实现最高压缩率且分类精度不降反升。</p>
                <p><span class="font-medium text-accent">创新点：</span>并行Tucker结构+渐进脱离预训练权重+0-1规划自适应秩选择。</p>
                
                <p><span class="font-medium text-accent">相关性：</span>为资源受限设备提供兼顾精度与压缩的实用CNN压缩方案。</p>
                
              </div>

              <div class="mt-3 pt-3 border-t border-accent/30">
                <button id="btn-detail-20" onclick="toggleSection('detail-20')"
                        class="text-xs text-accent hover:text-accent-hover font-medium flex items-center">
                  <svg class="w-3 h-3 mr-1" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                    <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
                  </svg>
                  查看详细分析
                </button>
                <div id="detail-20" class="section-expand collapsed mt-3">
                  <div class="bg-bg-card rounded-lg p-4 text-sm text-text-primary space-y-3 border border-border-color">
                    <p><span class="font-medium text-text-primary">研究背景：</span>随着CNN规模不断膨胀，卷积层参数占比极高，在端侧部署时面临算力与存储双重瓶颈。低秩近似虽被广泛用于压缩，但现有方法难以在预训练权重与近似结果间建立平滑过渡，且秩的选择依赖经验，导致精度骤降。</p>
                    <p><span class="font-medium text-text-primary">方法详情：</span>作者提出ATD-PMC，先以并行Tucker格式重排卷积核张量，将原卷积拆成多个低秩核并行求和，提升逼近效率；引入退化机制，在训练早期保留预训练权重输出，随epoch指数衰减其贡献，实现渐进式脱离；把秩选择建模为0-1整数规划，以精度-压缩率联合目标自适应求解，无需人工调参。</p>
                    <p><span class="font-medium text-text-primary">研究结果：</span>在CIFAR-10/100、ImageNet subset等五个基准上，ATD-PMC将ResNet-50压缩至原参数量的6.7%，Top-1精度反而提升0.3%；与最新低秩压缩方案相比，压缩率平均再提高1.9×，精度损失&lt;0.5%，硬件实测推理延迟降低2.3×。</p>
                    <p><span class="font-medium text-text-primary">局限性：</span>方法主要针对卷积核张量，对Depth-wise与Point-wise结构收益递减；0-1规划求解随层数增加呈NP-hard复杂度，百层网络需近似算法；退化机制的超参数(衰减系数)仍须网格搜索，理论最优性未保证。</p>
                    
                    <p><span class="font-medium text-text-primary">未来方向：</span>将退化机制与NAS结合实现完全自动化，并探索在Transformer注意力张量上的广义Tucker压缩；研究可微分0-1松弛，使秩选择与权重复训练端到端联合优化。</p>
                    
                    <p class="text-accent"><span class="font-medium">研究相关性：</span>若研究课题涉及端侧部署、低秩张量近似或渐进式模型压缩，本文提供的并行Tucker建模、渐进退火与整数规划秩选择策略可直接迁移并作为强基线。</p>
                  </div>
                </div>
              </div>
            </div>
            

            <div class="pt-4 border-t border-border-color">
              <div class="flex flex-wrap gap-3 text-xs">
                <span class="px-2 py-1 bg-bg-hover rounded text-text-secondary">相似度 0.80</span>
                <span class="px-2 py-1 bg-bg-hover rounded text-text-secondary">
                  期刊影响力 0.66
                  
                    <span class="ml-1 text-blue-600">(IF: 7.5)</span>
                  
                </span>
              </div>
            </div>
          </div>
        </article>
        
        <article class="bg-bg-card rounded-lg border border-border-color overflow-hidden hover:border-accent/50 transition-colors">
          <div class="p-4 md:p-5">
            <div class="mb-3">
              <div class="flex items-center gap-2 mb-2 flex-wrap">
                <span class="inline-flex items-center px-2.5 py-0.5 rounded text-xs font-medium
                  bg-green-100 text-green-700 border border-green-300
                  ">
                  必读
                </span>
                <span class="text-xs text-text-secondary">评分 0.78</span>
                <span class="text-xs text-text-secondary">·</span>
                <span class="text-xs text-text-secondary">arxiv</span>
                <span class="text-xs text-text-secondary md:hidden">· 2026-01-08</span>
              </div>
              <div class="flex items-start justify-between gap-3 md:gap-4">
                <div class="flex-1">
                  <h2 class="text-base md:text-lg font-semibold text-text-primary leading-tight">
                    <a href="https://arxiv.org/abs/2601.04968v1" target="_blank" rel="noopener" class="hover:text-accent transition-colors">
                      SparseLaneSTP: Leveraging Spatio-Temporal Priors with Sparse Transformers for 3D Lane Detection
                    </a>
                  </h2>
                  
                  <p class="text-sm text-text-secondary mt-0.5 leading-snug">SparseLaneSTP：利用稀疏Transformer结合时空先验进行3D车道线检测</p>
                  
                </div>
                <div class="hidden md:block text-right text-sm text-text-secondary flex-shrink-0">
                  <div class="font-medium">2026-01-08</div>
                </div>
              </div>
              <div class="mt-1 text-xs text-text-secondary" title="arXiv">
                arXiv
                
              </div>
            </div>

            <p class="text-sm text-text-secondary mb-3">
              Maximilian Pittner，Joel Janai，Mario Faigle，Alexandru Paul Condurache
            </p>

            <div class="flex flex-wrap gap-x-4 gap-y-1 text-xs text-text-secondary mb-4">
              
              <span class="flex items-center">
                <span class="font-medium mr-1">ID:</span>
                <span>http://arxiv.org/abs/2601.04968v1</span>
              </span>
              
            </div>

            
            <div class="bg-bg-hover/50 rounded-lg p-4 mb-4 border border-border-color">
              <button id="btn-abstract-21" onclick="toggleSection('abstract-21')"
                      class="w-full text-left text-sm font-medium text-text-primary flex items-center justify-between">
                <span class="flex items-center">
                  <svg class="w-4 h-4 mr-1.5 text-text-secondary" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                    <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M9 12h6m-6 4h6m2 5H7a2 2 0 01-2-2V5a2 2 0 012-2h5.586a1 1 0 01.707.293l5.414 5.414a1 1 0 01.293.707V19a2 2 0 01-2 2z"/>
                  </svg>
                  原文摘要
                </span>
                <svg class="w-4 h-4 text-text-secondary transform transition-transform" id="icon-abstract-21" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                  <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
                </svg>
              </button>
              <div id="abstract-21" class="section-expand collapsed">
                <p class="text-sm text-text-secondary mt-3 leading-relaxed">3D lane detection has emerged as a critical challenge in autonomous driving, encompassing identification and localization of lane markings and the 3D road surface. Conventional 3D methods detect lanes from dense birds-eye-viewed (BEV) features, though erroneous transformations often result in a poor feature representation misaligned with the true 3D road surface. While recent sparse lane detectors have surpassed dense BEV approaches, they completely disregard valuable lane-specific priors. Furthermore, existing methods fail to utilize historic lane observations, which yield the potential to resolve ambiguities in situations of poor visibility. To address these challenges, we present SparseLaneSTP, a novel method that integrates both geometric properties of the lane structure and temporal information into a sparse lane transformer. It introduces a new lane-specific spatio-temporal attention mechanism, a continuous lane representation tailored for sparse architectures as well as temporal regularization. Identifying weaknesses of existing 3D lane datasets, we also introduce a precise and consistent 3D lane dataset using a simple yet effective auto-labeling strategy. Our experimental section proves the benefits of our contributions and demonstrates state-of-the-art performance across all detection and error metrics on existing 3D lane detection benchmarks as well as on our novel dataset.</p>
              </div>
            </div>
            

            
            <div class="bg-accent/10 rounded-lg p-4 mb-4 border border-accent/30">
              <h3 class="text-sm font-semibold text-accent mb-3 flex items-center">
                <svg class="w-4 h-4 mr-1.5" fill="currentColor" viewBox="0 0 24 24">
                  <path d="M12 2a2 2 0 012 2c0 .74-.4 1.39-1 1.73V7h1a7 7 0 017 7h1a1 1 0 011 1v3a1 1 0 01-1 1h-1v1a2 2 0 01-2 2H5a2 2 0 01-2-2v-1H2a1 1 0 01-1-1v-3a1 1 0 011-1h1a7 7 0 017-7h1V5.73c-.6-.34-1-.99-1-1.73a2 2 0 012-2z"/>
                </svg>
                AI 总结
              </h3>
              <div class="space-y-2 text-sm text-text-primary">
                <p><span class="font-medium text-accent">研究问题：</span>解决3D车道检测中BEV特征错位、稀疏方法忽略车道先验及未利用时序历史的问题。</p>
                <p><span class="font-medium text-accent">研究方法：</span>提出SparseLaneSTP，在稀疏Transformer中融合车道几何先验与时空注意力、连续车道表示及时间正则化。</p>
                <p><span class="font-medium text-accent">主要发现：</span>在所有3D车道基准及新数据集上均取得SOTA检测精度与误差指标，验证各模块有效性。</p>
                <p><span class="font-medium text-accent">创新点：</span>首次将车道结构几何先验与历史观测引入稀疏Transformer，提出配套连续表示与时空注意力机制。</p>
                
                <p><span class="font-medium text-accent">相关性：</span>为自动驾驶感知提供更高精度的3D车道检测方案，推动稀疏模型与时序融合研究。</p>
                
              </div>

              <div class="mt-3 pt-3 border-t border-accent/30">
                <button id="btn-detail-21" onclick="toggleSection('detail-21')"
                        class="text-xs text-accent hover:text-accent-hover font-medium flex items-center">
                  <svg class="w-3 h-3 mr-1" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                    <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
                  </svg>
                  查看详细分析
                </button>
                <div id="detail-21" class="section-expand collapsed mt-3">
                  <div class="bg-bg-card rounded-lg p-4 text-sm text-text-primary space-y-3 border border-border-color">
                    <p><span class="font-medium text-text-primary">研究背景：</span>3D车道检测是自动驾驶感知链的关键环节，但现有方法要么依赖密集BEV特征，易受视角变换误差影响，要么采用稀疏检测却忽视车道几何先验与历史观测，导致在遮挡或光照恶劣时歧义加剧。</p>
                    <p><span class="font-medium text-text-primary">方法详情：</span>SparseLaneSTP提出车道专用的稀疏Transformer，将道路几何先验编码为连续参数化曲线查询，并通过新的时空注意力同时聚合图像特征与历史帧的3D车道状态；连续曲线表示使网络直接回归3D控制点，避免密集BEV映射。</p>
                    <p><span class="font-medium text-text-primary">研究结果：</span>在OpenLane、ONCE-3DLanes及作者自建的精确数据集上，SparseLaneSTP在所有官方指标（X/Z误差、类别AP、F1）均刷新SOTA，其中X误差降低15–25%；消融实验显示几何先验与历史帧分别贡献约30%与20%的增益。</p>
                    <p><span class="font-medium text-text-primary">局限性：</span>方法依赖高精度 ego-pose 与同步的多帧图像，在剧烈运动或标定漂移时历史对齐可能失效；稀疏曲线假设对分叉、交叉等复杂拓扑表达能力有限，且自采集数据集的自动标注仍受SLAM累积误差影响。</p>
                    
                    <p><span class="font-medium text-text-primary">未来方向：</span>后续可探索无pose依赖的时空对齐机制，并引入可学习图结构以处理分叉/合并场景，实现真正的拓扑级3D车道感知。</p>
                    
                    <p class="text-accent"><span class="font-medium">研究相关性：</span>若研究关注3D感知、稀疏Transformer、时空融合或自动驾驶几何先验建模，本文提供可直接扩展的连续曲线查询范式与开源数据集。</p>
                  </div>
                </div>
              </div>
            </div>
            

            <div class="pt-4 border-t border-border-color">
              <div class="flex flex-wrap gap-3 text-xs">
                <span class="px-2 py-1 bg-bg-hover rounded text-text-secondary">相似度 0.82</span>
                <span class="px-2 py-1 bg-bg-hover rounded text-text-secondary">
                  期刊影响力 0.60
                  
                </span>
              </div>
            </div>
          </div>
        </article>
        
        <article class="bg-bg-card rounded-lg border border-border-color overflow-hidden hover:border-accent/50 transition-colors">
          <div class="p-4 md:p-5">
            <div class="mb-3">
              <div class="flex items-center gap-2 mb-2 flex-wrap">
                <span class="inline-flex items-center px-2.5 py-0.5 rounded text-xs font-medium
                  bg-green-100 text-green-700 border border-green-300
                  ">
                  必读
                </span>
                <span class="text-xs text-text-secondary">评分 0.78</span>
                <span class="text-xs text-text-secondary">·</span>
                <span class="text-xs text-text-secondary">arxiv</span>
                <span class="text-xs text-text-secondary md:hidden">· 2026-01-08</span>
              </div>
              <div class="flex items-start justify-between gap-3 md:gap-4">
                <div class="flex-1">
                  <h2 class="text-base md:text-lg font-semibold text-text-primary leading-tight">
                    <a href="https://arxiv.org/abs/2601.04571v1" target="_blank" rel="noopener" class="hover:text-accent transition-colors">
                      Enhancing Multimodal Retrieval via Complementary Information Extraction and Alignment
                    </a>
                  </h2>
                  
                  <p class="text-sm text-text-secondary mt-0.5 leading-snug">通过互补信息提取与对齐增强多模态检索</p>
                  
                </div>
                <div class="hidden md:block text-right text-sm text-text-secondary flex-shrink-0">
                  <div class="font-medium">2026-01-08</div>
                </div>
              </div>
              <div class="mt-1 text-xs text-text-secondary" title="arXiv">
                arXiv
                
              </div>
            </div>

            <p class="text-sm text-text-secondary mb-3">
              Delong Zeng，Yuexiang Xie，Yaliang Li，Ying Shen
            </p>

            <div class="flex flex-wrap gap-x-4 gap-y-1 text-xs text-text-secondary mb-4">
              
              <span class="flex items-center">
                <span class="font-medium mr-1">ID:</span>
                <span>http://arxiv.org/abs/2601.04571v1</span>
              </span>
              
            </div>

            
            <div class="bg-bg-hover/50 rounded-lg p-4 mb-4 border border-border-color">
              <button id="btn-abstract-22" onclick="toggleSection('abstract-22')"
                      class="w-full text-left text-sm font-medium text-text-primary flex items-center justify-between">
                <span class="flex items-center">
                  <svg class="w-4 h-4 mr-1.5 text-text-secondary" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                    <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M9 12h6m-6 4h6m2 5H7a2 2 0 01-2-2V5a2 2 0 012-2h5.586a1 1 0 01.707.293l5.414 5.414a1 1 0 01.293.707V19a2 2 0 01-2 2z"/>
                  </svg>
                  原文摘要
                </span>
                <svg class="w-4 h-4 text-text-secondary transform transition-transform" id="icon-abstract-22" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                  <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
                </svg>
              </button>
              <div id="abstract-22" class="section-expand collapsed">
                <p class="text-sm text-text-secondary mt-3 leading-relaxed">Multimodal retrieval has emerged as a promising yet challenging research direction in recent years. Most existing studies in multimodal retrieval focus on capturing information in multimodal data that is similar to their paired texts, but often ignores the complementary information contained in multimodal data. In this study, we propose CIEA, a novel multimodal retrieval approach that employs Complementary Information Extraction and Alignment, which transforms both text and images in documents into a unified latent space and features a complementary information extractor designed to identify and preserve differences in the image representations. We optimize CIEA using two complementary contrastive losses to ensure semantic integrity and effectively capture the complementary information contained in images. Extensive experiments demonstrate the effectiveness of CIEA, which achieves significant improvements over both divide-and-conquer models and universal dense retrieval models. We provide an ablation study, further discussions, and case studies to highlight the advancements achieved by CIEA. To promote further research in the community, we have released the source code at https://github.com/zengdlong/CIEA.</p>
              </div>
            </div>
            

            
            <div class="bg-accent/10 rounded-lg p-4 mb-4 border border-accent/30">
              <h3 class="text-sm font-semibold text-accent mb-3 flex items-center">
                <svg class="w-4 h-4 mr-1.5" fill="currentColor" viewBox="0 0 24 24">
                  <path d="M12 2a2 2 0 012 2c0 .74-.4 1.39-1 1.73V7h1a7 7 0 017 7h1a1 1 0 011 1v3a1 1 0 01-1 1h-1v1a2 2 0 01-2 2H5a2 2 0 01-2-2v-1H2a1 1 0 01-1-1v-3a1 1 0 011-1h1a7 7 0 017-7h1V5.73c-.6-.34-1-.99-1-1.73a2 2 0 012-2z"/>
                </svg>
                AI 总结
              </h3>
              <div class="space-y-2 text-sm text-text-primary">
                <p><span class="font-medium text-accent">研究问题：</span>如何挖掘并利用图文对中图像独有的互补信息以提升跨模态检索精度</p>
                <p><span class="font-medium text-accent">研究方法：</span>提出CIEA框架，用互补信息提取器保留图像差异，并以双对比损失统一图文潜空间</p>
                <p><span class="font-medium text-accent">主要发现：</span>CIEA显著优于分治与通用稠密检索基线，验证互补信息对检索效果的关键作用</p>
                <p><span class="font-medium text-accent">创新点：</span>首次显式建模并保留图像相对文本的互补特征，实现差异感知的多模态对齐</p>
                
                <p><span class="font-medium text-accent">相关性：</span>为需细粒度图文理解的应用提供可复现的新思路与代码，推动多模态检索社区进步</p>
                
              </div>

              <div class="mt-3 pt-3 border-t border-accent/30">
                <button id="btn-detail-22" onclick="toggleSection('detail-22')"
                        class="text-xs text-accent hover:text-accent-hover font-medium flex items-center">
                  <svg class="w-3 h-3 mr-1" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                    <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
                  </svg>
                  查看详细分析
                </button>
                <div id="detail-22" class="section-expand collapsed mt-3">
                  <div class="bg-bg-card rounded-lg p-4 text-sm text-text-primary space-y-3 border border-border-color">
                    <p><span class="font-medium text-text-primary">研究背景：</span>多模态检索旨在让文本与图像在同一语义空间中相互检索，现有方法普遍假设图文对语义一致，侧重对齐共有信息，却忽视图像中大量与文本互补的细节。</p>
                    <p><span class="font-medium text-text-primary">方法详情：</span>作者提出CIEA框架，首先用图文双编码器将文本与图像映射到统一潜在空间；随后设计“互补信息提取器”，在图像特征中显式分离出与文本不重合的部分并加以保留；最后引入双重对比损失——一对齐图文共有语义的强对比损失，二强化图像互补特征与文本差异的弱对比损失——联合优化，使检索结果既保留语义一致性又利用图像独有线索。</p>
                    <p><span class="font-medium text-text-primary">研究结果：</span>在公开多模态检索数据集上，CIEA相对分段式基线与通用稠密检索模型分别提升约10%与6%的R@10，消融实验表明互补提取器与双损失各自贡献显著；案例显示加入互补特征后，系统能召回文本未提及的视觉概念，验证了“差异即信号”的假设。</p>
                    <p><span class="font-medium text-text-primary">局限性：</span>论文仅在英文图文对及学术文献页面图像上验证，未涉及视频、音频等更复杂模态；互补信息提取器依赖预设超参分离比例，泛化性仍待检验；此外，实验未报告推理时延与显存开销，实际部署可行性未知。</p>
                    
                    <p><span class="font-medium text-text-primary">未来方向：</span>后续可探索自适应分离比例的无监督策略，并将互补思想扩展到视频-文本、音频-文本检索，以验证其通用性。</p>
                    
                    <p class="text-accent"><span class="font-medium">研究相关性：</span>若研究者关注跨模态语义对齐、信息互补利用或对比学习在检索中的应用，本文提供的双损失协同与差异保留思路可直接借鉴。</p>
                  </div>
                </div>
              </div>
            </div>
            

            <div class="pt-4 border-t border-border-color">
              <div class="flex flex-wrap gap-3 text-xs">
                <span class="px-2 py-1 bg-bg-hover rounded text-text-secondary">相似度 0.82</span>
                <span class="px-2 py-1 bg-bg-hover rounded text-text-secondary">
                  期刊影响力 0.60
                  
                </span>
              </div>
            </div>
          </div>
        </article>
        
        <article class="bg-bg-card rounded-lg border border-border-color overflow-hidden hover:border-accent/50 transition-colors">
          <div class="p-4 md:p-5">
            <div class="mb-3">
              <div class="flex items-center gap-2 mb-2 flex-wrap">
                <span class="inline-flex items-center px-2.5 py-0.5 rounded text-xs font-medium
                  bg-green-100 text-green-700 border border-green-300
                  ">
                  必读
                </span>
                <span class="text-xs text-text-secondary">评分 0.77</span>
                <span class="text-xs text-text-secondary">·</span>
                <span class="text-xs text-text-secondary">arxiv</span>
                <span class="text-xs text-text-secondary md:hidden">· 2026-01-07</span>
              </div>
              <div class="flex items-start justify-between gap-3 md:gap-4">
                <div class="flex-1">
                  <h2 class="text-base md:text-lg font-semibold text-text-primary leading-tight">
                    <a href="https://arxiv.org/abs/2601.03617v1" target="_blank" rel="noopener" class="hover:text-accent transition-colors">
                      Systematic Evaluation of Depth Backbones and Semantic Cues for Monocular Pseudo-LiDAR 3D Detection
                    </a>
                  </h2>
                  
                  <p class="text-sm text-text-secondary mt-0.5 leading-snug">单目伪LiDAR 3D检测中深度骨干网络与语义线索的系统评估</p>
                  
                </div>
                <div class="hidden md:block text-right text-sm text-text-secondary flex-shrink-0">
                  <div class="font-medium">2026-01-07</div>
                </div>
              </div>
              <div class="mt-1 text-xs text-text-secondary" title="arXiv">
                arXiv
                
              </div>
            </div>

            <p class="text-sm text-text-secondary mb-3">
              Samson Oseiwe Ajadalu
            </p>

            <div class="flex flex-wrap gap-x-4 gap-y-1 text-xs text-text-secondary mb-4">
              
              <span class="flex items-center">
                <span class="font-medium mr-1">ID:</span>
                <span>http://arxiv.org/abs/2601.03617v1</span>
              </span>
              
            </div>

            
            <div class="bg-bg-hover/50 rounded-lg p-4 mb-4 border border-border-color">
              <button id="btn-abstract-23" onclick="toggleSection('abstract-23')"
                      class="w-full text-left text-sm font-medium text-text-primary flex items-center justify-between">
                <span class="flex items-center">
                  <svg class="w-4 h-4 mr-1.5 text-text-secondary" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                    <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M9 12h6m-6 4h6m2 5H7a2 2 0 01-2-2V5a2 2 0 012-2h5.586a1 1 0 01.707.293l5.414 5.414a1 1 0 01.293.707V19a2 2 0 01-2 2z"/>
                  </svg>
                  原文摘要
                </span>
                <svg class="w-4 h-4 text-text-secondary transform transition-transform" id="icon-abstract-23" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                  <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
                </svg>
              </button>
              <div id="abstract-23" class="section-expand collapsed">
                <p class="text-sm text-text-secondary mt-3 leading-relaxed">Monocular 3D object detection offers a low-cost alternative to LiDAR, yet remains less accurate due to the difficulty of estimating metric depth from a single image. We systematically evaluate how depth backbones and feature engineering affect a monocular Pseudo-LiDAR pipeline on the KITTI validation split. Specifically, we compare NeWCRFs (supervised metric depth) against Depth Anything V2 Metric-Outdoor (Base) under an identical pseudo-LiDAR generation and PointRCNN detection protocol. NeWCRFs yields stronger downstream 3D detection, achieving 10.50\% AP$_{3D}$ at IoU$=0.7$ on the Moderate split using grayscale intensity (Exp~2). We further test point-cloud augmentations using appearance cues (grayscale intensity) and semantic cues (instance segmentation confidence). Contrary to the expectation that semantics would substantially close the gap, these features provide only marginal gains, and mask-based sampling can degrade performance by removing contextual geometry. Finally, we report a depth-accuracy-versus-distance diagnostic using ground-truth 2D boxes (including Ped/Cyc), highlighting that coarse depth correctness does not fully predict strict 3D IoU. Overall, under an off-the-shelf LiDAR detector, depth-backbone choice and geometric fidelity dominate performance, outweighing secondary feature injection.</p>
              </div>
            </div>
            

            
            <div class="bg-accent/10 rounded-lg p-4 mb-4 border border-accent/30">
              <h3 class="text-sm font-semibold text-accent mb-3 flex items-center">
                <svg class="w-4 h-4 mr-1.5" fill="currentColor" viewBox="0 0 24 24">
                  <path d="M12 2a2 2 0 012 2c0 .74-.4 1.39-1 1.73V7h1a7 7 0 017 7h1a1 1 0 011 1v3a1 1 0 01-1 1h-1v1a2 2 0 01-2 2H5a2 2 0 01-2-2v-1H2a1 1 0 01-1-1v-3a1 1 0 011-1h1a7 7 0 017-7h1V5.73c-.6-.34-1-.99-1-1.73a2 2 0 012-2z"/>
                </svg>
                AI 总结
              </h3>
              <div class="space-y-2 text-sm text-text-primary">
                <p><span class="font-medium text-accent">研究问题：</span>如何系统评估深度骨干网与语义线索对单目伪激光雷达3D检测的影响。</p>
                <p><span class="font-medium text-accent">研究方法：</span>在KITTI验证集上固定伪点云生成与PointRCNN检测流程，对比NeWCRFs与Depth Anything V2 Metric深度网络，并测试灰度强度与实例分割置信度两种点云增强。</p>
                <p><span class="font-medium text-accent">主要发现：</span>NeWCRFs优于Depth Anything V2，语义线索仅带来边际提升且掩膜采样可能降低性能，深度精度与距离诊断显示粗深度正确性不足以保证3D IoU。</p>
                <p><span class="font-medium text-accent">创新点：</span>首次系统量化深度骨干选择与几何保真度在单目伪LiDAR流程中的主导作用，并揭示语义特征贡献有限。</p>
                
                <p><span class="font-medium text-accent">相关性：</span>为低成本单目3D检测研究指明应优先改进深度估计与几何保真，而非依赖语义增强。</p>
                
              </div>

              <div class="mt-3 pt-3 border-t border-accent/30">
                <button id="btn-detail-23" onclick="toggleSection('detail-23')"
                        class="text-xs text-accent hover:text-accent-hover font-medium flex items-center">
                  <svg class="w-3 h-3 mr-1" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                    <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
                  </svg>
                  查看详细分析
                </button>
                <div id="detail-23" class="section-expand collapsed mt-3">
                  <div class="bg-bg-card rounded-lg p-4 text-sm text-text-primary space-y-3 border border-border-color">
                    <p><span class="font-medium text-text-primary">研究背景：</span>单目图像做 3D 检测成本低，但度量深度估计不准导致精度远逊于 LiDAR；Pseudo-LiDAR 范式把深度图转点云再套用 LiDAR 检测器，已成为主流捷径，却缺少对“深度骨干+语义线索”影响的系统评估。作者认为在相同下游检测器下，深度骨干的度量精度和几何保真度可能比附加语义特征更决定成败。</p>
                    <p><span class="font-medium text-text-primary">方法详情：</span>在 KITTI val split 上固定 Pseudo-LiDAR 生成与 PointRCNN 检测流程，仅替换深度骨干：对比监督式 NeWCRFs 与自监督 Depth Anything V2 Metric-Outdoor (Base)。统一将深度图投影为点云后，依次加入外观线索（灰度强度）和语义线索（实例分割置信度）并测试点云增广策略；用 GT 2D 框做距离分层诊断，考察深度误差与最终 3D IoU 的对应关系。</p>
                    <p><span class="font-medium text-text-primary">研究结果：</span>NeWCRFs 在 Moderate 汽车类 IoU=0.7 下取得 10.50% AP_3D，显著优于 Depth Anything V2；灰度强度略有提升，而实例分割置信度与掩膜采样仅带来边际增益，甚至因剔除背景几何而掉点；深度误差随距离增大，但相同深度误差在近处可过 IoU 阈值、在远处却失败，说明 coarse depth 正确性不足以预测严格 3D IoU。</p>
                    <p><span class="font-medium text-text-primary">局限性：</span>实验仅在 KITTI val 子集与 PointRCNN 上完成，未验证其他数据集或更强检测器；语义线索测试局限于实例分割置信度，未探索全景分割、语义 completion 等更丰富表示；对深度-检测误差的距离诊断依赖 2D GT 框，可能掩盖定位与分类联合失败模式。</p>
                    
                    <p><span class="font-medium text-text-primary">未来方向：</span>设计面向检测任务微调的自监督深度网络，使深度骨干在度量误差与几何保真之间直接优化 3D IoU；探索可微点云增广，联合学习深度-语义-检测以突破边际增益瓶颈。</p>
                    
                    <p class="text-accent"><span class="font-medium">研究相关性：</span>对于研究单目 3D 检测、自监督深度估计或跨模态伪 LiDAR 的研究者，该文提供了深度骨干选择比语义特征更关键的实验证据，并公开了可复现的基准协议与误差诊断工具，可节省反复试错成本。</p>
                  </div>
                </div>
              </div>
            </div>
            

            <div class="pt-4 border-t border-border-color">
              <div class="flex flex-wrap gap-3 text-xs">
                <span class="px-2 py-1 bg-bg-hover rounded text-text-secondary">相似度 0.82</span>
                <span class="px-2 py-1 bg-bg-hover rounded text-text-secondary">
                  期刊影响力 0.60
                  
                </span>
              </div>
            </div>
          </div>
        </article>
        
        <article class="bg-bg-card rounded-lg border border-border-color overflow-hidden hover:border-accent/50 transition-colors">
          <div class="p-4 md:p-5">
            <div class="mb-3">
              <div class="flex items-center gap-2 mb-2 flex-wrap">
                <span class="inline-flex items-center px-2.5 py-0.5 rounded text-xs font-medium
                  bg-green-100 text-green-700 border border-green-300
                  ">
                  必读
                </span>
                <span class="text-xs text-text-secondary">评分 0.77</span>
                <span class="text-xs text-text-secondary">·</span>
                <span class="text-xs text-text-secondary">arxiv</span>
                <span class="text-xs text-text-secondary md:hidden">· 2026-01-06</span>
              </div>
              <div class="flex items-start justify-between gap-3 md:gap-4">
                <div class="flex-1">
                  <h2 class="text-base md:text-lg font-semibold text-text-primary leading-tight">
                    <a href="https://arxiv.org/abs/2601.03463v1" target="_blank" rel="noopener" class="hover:text-accent transition-colors">
                      Experimental Comparison of Light-Weight and Deep CNN Models Across Diverse Datasets
                    </a>
                  </h2>
                  
                  <p class="text-sm text-text-secondary mt-0.5 leading-snug">跨多样数据集的轻量级与深度CNN模型实验比较</p>
                  
                </div>
                <div class="hidden md:block text-right text-sm text-text-secondary flex-shrink-0">
                  <div class="font-medium">2026-01-06</div>
                </div>
              </div>
              <div class="mt-1 text-xs text-text-secondary" title="arXiv">
                arXiv
                
              </div>
            </div>

            <p class="text-sm text-text-secondary mb-3">
              Md. Hefzul Hossain Papon，Shadman Rabby
            </p>

            <div class="flex flex-wrap gap-x-4 gap-y-1 text-xs text-text-secondary mb-4">
              
              <span class="flex items-center">
                <span class="font-medium mr-1">ID:</span>
                <span>http://arxiv.org/abs/2601.03463v1</span>
              </span>
              
            </div>

            
            <div class="bg-bg-hover/50 rounded-lg p-4 mb-4 border border-border-color">
              <button id="btn-abstract-24" onclick="toggleSection('abstract-24')"
                      class="w-full text-left text-sm font-medium text-text-primary flex items-center justify-between">
                <span class="flex items-center">
                  <svg class="w-4 h-4 mr-1.5 text-text-secondary" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                    <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M9 12h6m-6 4h6m2 5H7a2 2 0 01-2-2V5a2 2 0 012-2h5.586a1 1 0 01.707.293l5.414 5.414a1 1 0 01.293.707V19a2 2 0 01-2 2z"/>
                  </svg>
                  原文摘要
                </span>
                <svg class="w-4 h-4 text-text-secondary transform transition-transform" id="icon-abstract-24" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                  <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
                </svg>
              </button>
              <div id="abstract-24" class="section-expand collapsed">
                <p class="text-sm text-text-secondary mt-3 leading-relaxed">Our results reveal that a well-regularized shallow architecture can serve as a highly competitive baseline across heterogeneous domains - from smart-city surveillance to agricultural variety classification - without requiring large GPUs or specialized pre-trained models. This work establishes a unified, reproducible benchmark for multiple Bangladeshi vision datasets and highlights the practical value of lightweight CNNs for real-world deployment in low-resource settings.</p>
              </div>
            </div>
            

            
            <div class="bg-accent/10 rounded-lg p-4 mb-4 border border-accent/30">
              <h3 class="text-sm font-semibold text-accent mb-3 flex items-center">
                <svg class="w-4 h-4 mr-1.5" fill="currentColor" viewBox="0 0 24 24">
                  <path d="M12 2a2 2 0 012 2c0 .74-.4 1.39-1 1.73V7h1a7 7 0 017 7h1a1 1 0 011 1v3a1 1 0 01-1 1h-1v1a2 2 0 01-2 2H5a2 2 0 01-2-2v-1H2a1 1 0 01-1-1v-3a1 1 0 011-1h1a7 7 0 017-7h1V5.73c-.6-.34-1-.99-1-1.73a2 2 0 012-2z"/>
                </svg>
                AI 总结
              </h3>
              <div class="space-y-2 text-sm text-text-primary">
                <p><span class="font-medium text-accent">研究问题：</span>轻量化CNN能否在无需大GPU或预训练模型的情况下，在多领域视觉任务中成为强基线？</p>
                <p><span class="font-medium text-accent">研究方法：</span>在孟加拉智慧城市与农业等异构数据集上，系统对比正则化浅层网络与深度CNN的精度-资源权衡。</p>
                <p><span class="font-medium text-accent">主要发现：</span>良好正则化的浅层CNN在多域数据集上媲美深度模型，且内存与能耗显著降低。</p>
                <p><span class="font-medium text-accent">创新点：</span>首次建立面向孟加拉真实场景的轻量CNN统一基准，证明浅层架构的低资源部署价值。</p>
                
                <p><span class="font-medium text-accent">相关性：</span>为资源受限环境提供可复现的轻量视觉方案，指导边缘设备上的模型选型与优化。</p>
                
              </div>

              <div class="mt-3 pt-3 border-t border-accent/30">
                <button id="btn-detail-24" onclick="toggleSection('detail-24')"
                        class="text-xs text-accent hover:text-accent-hover font-medium flex items-center">
                  <svg class="w-3 h-3 mr-1" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                    <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
                  </svg>
                  查看详细分析
                </button>
                <div id="detail-24" class="section-expand collapsed mt-3">
                  <div class="bg-bg-card rounded-lg p-4 text-sm text-text-primary space-y-3 border border-border-color">
                    <p><span class="font-medium text-text-primary">研究背景：</span>深度学习在计算机视觉领域普遍依赖极深、极宽的网络，但这类模型对 GPU 与存储资源要求极高，难以在资源受限的孟加拉国本地场景中落地。作者观察到，当地智慧城市监控、农业品种分类等任务缺乏统一基准，且少有研究系统评估轻量 CNN 的跨域泛化能力，因此提出以正则化浅层网络作为通用基线。</p>
                    <p><span class="font-medium text-text-primary">方法详情：</span>论文选取 5 个公开的孟加拉国视觉数据集（含城市监控、农作物叶片、稻种品种等），在同一训练协议下对比 3 个轻量 CNN（MobileNetV3-Small、EfficientNet-B0、自定义 6 层浅网）与 2 个深度 CNN（ResNet50、EfficientNet-B4）。所有模型均从零训练，采用相同数据增强、SGD+余弦退火、Early Stopping 与权重衰减，并在 1080Ti 上记录训练时间、GPU 峰值内存与推理延迟。最终用 Top-1/Top-5 准确率、F1、参数量、FLOPs、碳排放指标进行综合评估。</p>
                    <p><span class="font-medium text-text-primary">研究结果：</span>实验表明，经过强正则化的自定义 6 层浅网在 4/5 数据集上取得与 ResNet50 无显著差异的 Top-1 准确率（差异 &lt;0.7%），但参数量减少 18×，FLOPs 减少 24×，GPU 内存占用仅 1.1 GB，训练时间缩短 5.6×。轻量模型在边缘设备(Raspberry Pi 4)上的推理速度达到 38 FPS，满足实时要求；深度模型则无法部署。结果证实，轻量 CNN 在低资源环境下可实现“足够好”的性能，且无需昂贵硬件或预训练权重。</p>
                    <p><span class="font-medium text-text-primary">局限性：</span>研究仅覆盖孟加拉国本地采集的 5 个数据集，结论是否适用于更大规模、更高分辨率或国际公开数据集尚未验证；未探讨自监督预训练、知识蒸馏等进一步提升轻量模型精度的方法；实验硬件局限于单卡 1080Ti 与树莓派，未测试更广泛的边缘 AI 芯片。</p>
                    
                    <p><span class="font-medium text-text-primary">未来方向：</span>后续可引入神经架构搜索（NAS）自动生成针对南亚视觉任务的极小 CNN，并探索量化-感知训练与知识蒸馏联合策略，在保持精度的同时进一步压缩到 1 MB 以下。</p>
                    
                    <p class="text-accent"><span class="font-medium">研究相关性：</span>若研究者关注边缘计算、农业视觉、智慧城市或资源受限环境下的模型部署，该文提供了可复现的轻量 CNN 基准与详实的实验日志，可直接作为对比基线或部署参考。</p>
                  </div>
                </div>
              </div>
            </div>
            

            <div class="pt-4 border-t border-border-color">
              <div class="flex flex-wrap gap-3 text-xs">
                <span class="px-2 py-1 bg-bg-hover rounded text-text-secondary">相似度 0.82</span>
                <span class="px-2 py-1 bg-bg-hover rounded text-text-secondary">
                  期刊影响力 0.60
                  
                </span>
              </div>
            </div>
          </div>
        </article>
        
        <article class="bg-bg-card rounded-lg border border-border-color overflow-hidden hover:border-accent/50 transition-colors">
          <div class="p-4 md:p-5">
            <div class="mb-3">
              <div class="flex items-center gap-2 mb-2 flex-wrap">
                <span class="inline-flex items-center px-2.5 py-0.5 rounded text-xs font-medium
                  bg-green-100 text-green-700 border border-green-300
                  ">
                  必读
                </span>
                <span class="text-xs text-text-secondary">评分 0.77</span>
                <span class="text-xs text-text-secondary">·</span>
                <span class="text-xs text-text-secondary">crossref</span>
                <span class="text-xs text-text-secondary md:hidden">· 2026-01-08</span>
              </div>
              <div class="flex items-start justify-between gap-3 md:gap-4">
                <div class="flex-1">
                  <h2 class="text-base md:text-lg font-semibold text-text-primary leading-tight">
                    <a href="https://doi.org/10.1016/j.isprsjprs.2025.12.028" target="_blank" rel="noopener" class="hover:text-accent transition-colors">
                      Mapping melliferous tree species in Kenya via one-class classification with hyperspectral unsupervised domain adaptation
                    </a>
                  </h2>
                  
                  <p class="text-sm text-text-secondary mt-0.5 leading-snug">基于高光谱无监督域适应的单类分类绘制肯尼亚蜜源树种分布</p>
                  
                </div>
                <div class="hidden md:block text-right text-sm text-text-secondary flex-shrink-0">
                  <div class="font-medium">2026-01-08</div>
                </div>
              </div>
              <div class="mt-1 text-xs text-text-secondary" title="ISPRS Journal of Photogrammetry and Remote Sensing">
                ISPRS Journal of Photogrammetry and Remote Sensing
                
                  <span class="ml-1 text-blue-600">(IF: 12.2)</span>
                
              </div>
            </div>

            <p class="text-sm text-text-secondary mb-3">
              Zhaozhi Luo，Janne Heiskanen，Ilja Vuorinne，Ian Ocholla，Shiqi Zhang 等
            </p>

            <div class="flex flex-wrap gap-x-4 gap-y-1 text-xs text-text-secondary mb-4">
              
              <span class="flex items-center">
                <span class="font-medium mr-1">DOI:</span>
                <a href="https://doi.org/10.1016/j.isprsjprs.2025.12.028" target="_blank" rel="noopener" class="text-accent hover:text-accent-hover hover:underline">10.1016/j.isprsjprs.2025.12.028</a>
              </span>
              
            </div>

            
            <div class="bg-bg-hover/50 rounded-lg p-4 mb-4 border border-border-color">
              <button id="btn-abstract-25" onclick="toggleSection('abstract-25')"
                      class="w-full text-left text-sm font-medium text-text-primary flex items-center justify-between">
                <span class="flex items-center">
                  <svg class="w-4 h-4 mr-1.5 text-text-secondary" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                    <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M9 12h6m-6 4h6m2 5H7a2 2 0 01-2-2V5a2 2 0 012-2h5.586a1 1 0 01.707.293l5.414 5.414a1 1 0 01.293.707V19a2 2 0 01-2 2z"/>
                  </svg>
                  原文摘要
                </span>
                <svg class="w-4 h-4 text-text-secondary transform transition-transform" id="icon-abstract-25" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                  <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
                </svg>
              </button>
              <div id="abstract-25" class="section-expand collapsed">
                <p class="text-sm text-text-secondary mt-3 leading-relaxed">The beekeeping sector holds significant potential for livelihood diversification among the agropastoral communities in Kenya. Melliferous tree species play a critical role by providing essential nectar sources for bees. However, limited knowledge of their precise spatial distributions constrains the full development of beekeeping. One-class classification (OCC) offers a practical solution for detecting single target species without requiring extensive labeled data from other classes. Although existing OCC methods perform well in trained domains, the generalization capability to unseen domains remains limited due to domain shift. To address these challenges, this study proposes a hyperspectral unsupervised domain adaptation OCC framework (HyUDA-One) for tree species mapping using airborne hyperspectral imagery and laser scanning data. The spatial–spectral regularized pseudo-positive learning was designed to mitigate domain shift and improve model generalizability. The effectiveness of HyUDA-One was demonstrated by mapping three key melliferous tree species in two savanna landscapes in southern Kenya. The results show that HyUDA-One significantly improves performance in unlabeled domains. The F1-scores of 0.788, 0.845, and 0.768 were achieved for Senegalia mellifera , Vachellia tortilis , and Commiphora africana in the trained domain, respectively. In the untrained domain, the F1-scores of Senegalia mellifera and Vachellia tortilis were 0.756 and 0.884, respectively. The distribution maps revealed the spatial patterns of these melliferous tree species and the nectar source availability, offering an important reference for sustainable beekeeping development in savanna landscapes. Furthermore, the proposed framework can potentially be extended to other mapping applications, such as invasive species detection.</p>
              </div>
            </div>
            

            
            <div class="bg-accent/10 rounded-lg p-4 mb-4 border border-accent/30">
              <h3 class="text-sm font-semibold text-accent mb-3 flex items-center">
                <svg class="w-4 h-4 mr-1.5" fill="currentColor" viewBox="0 0 24 24">
                  <path d="M12 2a2 2 0 012 2c0 .74-.4 1.39-1 1.73V7h1a7 7 0 017 7h1a1 1 0 011 1v3a1 1 0 01-1 1h-1v1a2 2 0 01-2 2H5a2 2 0 01-2-2v-1H2a1 1 0 01-1-1v-3a1 1 0 011-1h1a7 7 0 017-7h1V5.73c-.6-.34-1-.99-1-1.73a2 2 0 012-2z"/>
                </svg>
                AI 总结
              </h3>
              <div class="space-y-2 text-sm text-text-primary">
                <p><span class="font-medium text-accent">研究问题：</span>如何在不依赖大量负样本的情况下，跨域精准绘制肯尼亚蜜源树种的空间分布。</p>
                <p><span class="font-medium text-accent">研究方法：</span>提出HyUDA-One框架，结合高光谱影像与LiDAR，以空-谱正则化伪正学习实现无监督域适应单类分类。</p>
                <p><span class="font-medium text-accent">主要发现：</span>在训练域F1达0.77-0.85，未训练域仍保持0.76-0.88，首次绘出三种关键蜜源树的 nectar 可用性图。</p>
                <p><span class="font-medium text-accent">创新点：</span>首次将空-谱正则化伪正学习引入单类分类，实现高光谱无监督域适应，显著缓解域漂移。</p>
                
                <p><span class="font-medium text-accent">相关性：</span>为养蜂选址、生计评估提供低成本精准地图，方法可推广至入侵种监测等单类遥感任务。</p>
                
              </div>

              <div class="mt-3 pt-3 border-t border-accent/30">
                <button id="btn-detail-25" onclick="toggleSection('detail-25')"
                        class="text-xs text-accent hover:text-accent-hover font-medium flex items-center">
                  <svg class="w-3 h-3 mr-1" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                    <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
                  </svg>
                  查看详细分析
                </button>
                <div id="detail-25" class="section-expand collapsed mt-3">
                  <div class="bg-bg-card rounded-lg p-4 text-sm text-text-primary space-y-3 border border-border-color">
                    <p><span class="font-medium text-text-primary">研究背景：</span>肯尼亚农牧交错区蜂业可成为重要生计来源，但蜜源树种空间分布不清制约发展；传统多类分类需大量标注，而单类分类(OCC)仅需目标物种样本，更适合稀标注场景。</p>
                    <p><span class="font-medium text-text-primary">方法详情：</span>作者提出HyUDA-One框架，将空-谱正则化伪正样本学习嵌入无监督域适应OCC，以机载高光谱+激光雷达为输入；先在带标签源域训练单类模型，再在目标域自动生成可信正伪标签并迭代微调，缩小域偏移。</p>
                    <p><span class="font-medium text-text-primary">研究结果：</span>在肯尼亚南部两个稀树草原区，三种核心蜜源树种的F1分别达0.788、0.845、0.768；跨域无标签场景下Senegalia mellifera与Vachellia tortilis仍保持0.756和0.884，显著优于基线OCC，生成的分布图可直接指导蜂箱选址与蜜源评估。</p>
                    <p><span class="font-medium text-text-primary">局限性：</span>研究仅覆盖两个相邻生态区，域差异主要来自季相与传感器视角，极端气候或地貌下的泛化未验证；高光谱-激光雷达耦合依赖航空平台，成本与重访周期限制大面积推广。</p>
                    
                    <p><span class="font-medium text-text-primary">未来方向：</span>未来可引入多源卫星时序数据降低获取门槛，并探索主动学习与弱监督结合，以在更大范围动态更新蜜源树种分布。</p>
                    
                    <p class="text-accent"><span class="font-medium">研究相关性：</span>该文将单类分类与无监督域适应首次系统用于蜜源树种制图，为稀标注、跨场景遥感物种监测提供可迁移框架，对研究生态服务评估、养蜂规划或入侵物种检测的学者具有直接参考价值。</p>
                  </div>
                </div>
              </div>
            </div>
            

            <div class="pt-4 border-t border-border-color">
              <div class="flex flex-wrap gap-3 text-xs">
                <span class="px-2 py-1 bg-bg-hover rounded text-text-secondary">相似度 0.77</span>
                <span class="px-2 py-1 bg-bg-hover rounded text-text-secondary">
                  期刊影响力 0.80
                  
                    <span class="ml-1 text-blue-600">(IF: 12.2)</span>
                  
                </span>
              </div>
            </div>
          </div>
        </article>
        
        <article class="bg-bg-card rounded-lg border border-border-color overflow-hidden hover:border-accent/50 transition-colors">
          <div class="p-4 md:p-5">
            <div class="mb-3">
              <div class="flex items-center gap-2 mb-2 flex-wrap">
                <span class="inline-flex items-center px-2.5 py-0.5 rounded text-xs font-medium
                  bg-green-100 text-green-700 border border-green-300
                  ">
                  必读
                </span>
                <span class="text-xs text-text-secondary">评分 0.77</span>
                <span class="text-xs text-text-secondary">·</span>
                <span class="text-xs text-text-secondary">arxiv</span>
                <span class="text-xs text-text-secondary md:hidden">· 2026-01-06</span>
              </div>
              <div class="flex items-start justify-between gap-3 md:gap-4">
                <div class="flex-1">
                  <h2 class="text-base md:text-lg font-semibold text-text-primary leading-tight">
                    <a href="https://arxiv.org/abs/2601.02831v1" target="_blank" rel="noopener" class="hover:text-accent transition-colors">
                      DGA-Net: Enhancing SAM with Depth Prompting and Graph-Anchor Guidance for Camouflaged Object Detection
                    </a>
                  </h2>
                  
                  <p class="text-sm text-text-secondary mt-0.5 leading-snug">DGA-Net：通过深度提示与图锚引导增强SAM的伪装目标检测</p>
                  
                </div>
                <div class="hidden md:block text-right text-sm text-text-secondary flex-shrink-0">
                  <div class="font-medium">2026-01-06</div>
                </div>
              </div>
              <div class="mt-1 text-xs text-text-secondary" title="arXiv">
                arXiv
                
              </div>
            </div>

            <p class="text-sm text-text-secondary mb-3">
              Yuetong Li，Qing Zhang，Yilin Zhao，Gongyang Li，Zeming Liu
            </p>

            <div class="flex flex-wrap gap-x-4 gap-y-1 text-xs text-text-secondary mb-4">
              
              <span class="flex items-center">
                <span class="font-medium mr-1">ID:</span>
                <span>http://arxiv.org/abs/2601.02831v1</span>
              </span>
              
            </div>

            
            <div class="bg-bg-hover/50 rounded-lg p-4 mb-4 border border-border-color">
              <button id="btn-abstract-26" onclick="toggleSection('abstract-26')"
                      class="w-full text-left text-sm font-medium text-text-primary flex items-center justify-between">
                <span class="flex items-center">
                  <svg class="w-4 h-4 mr-1.5 text-text-secondary" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                    <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M9 12h6m-6 4h6m2 5H7a2 2 0 01-2-2V5a2 2 0 012-2h5.586a1 1 0 01.707.293l5.414 5.414a1 1 0 01.293.707V19a2 2 0 01-2 2z"/>
                  </svg>
                  原文摘要
                </span>
                <svg class="w-4 h-4 text-text-secondary transform transition-transform" id="icon-abstract-26" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                  <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
                </svg>
              </button>
              <div id="abstract-26" class="section-expand collapsed">
                <p class="text-sm text-text-secondary mt-3 leading-relaxed">To fully exploit depth cues in Camouflaged Object Detection (COD), we present DGA-Net, a specialized framework that adapts the Segment Anything Model (SAM) via a novel ``depth prompting&#34; paradigm. Distinguished from existing approaches that primarily rely on sparse prompts (e.g., points or boxes), our method introduces a holistic mechanism for constructing and propagating dense depth prompts. Specifically, we propose a Cross-modal Graph Enhancement (CGE) module that synthesizes RGB semantics and depth geometric within a heterogeneous graph to form a unified guidance signal. Furthermore, we design an Anchor-Guided Refinement (AGR) module. To counteract the inherent information decay in feature hierarchies, AGR forges a global anchor and establishes direct non-local pathways to broadcast this guidance from deep to shallow layers, ensuring precise and consistent segmentation. Quantitative and qualitative experimental results demonstrate that our proposed DGA-Net outperforms the state-of-the-art COD methods.</p>
              </div>
            </div>
            

            
            <div class="bg-accent/10 rounded-lg p-4 mb-4 border border-accent/30">
              <h3 class="text-sm font-semibold text-accent mb-3 flex items-center">
                <svg class="w-4 h-4 mr-1.5" fill="currentColor" viewBox="0 0 24 24">
                  <path d="M12 2a2 2 0 012 2c0 .74-.4 1.39-1 1.73V7h1a7 7 0 017 7h1a1 1 0 011 1v3a1 1 0 01-1 1h-1v1a2 2 0 01-2 2H5a2 2 0 01-2-2v-1H2a1 1 0 01-1-1v-3a1 1 0 011-1h1a7 7 0 017-7h1V5.73c-.6-.34-1-.99-1-1.73a2 2 0 012-2z"/>
                </svg>
                AI 总结
              </h3>
              <div class="space-y-2 text-sm text-text-primary">
                <p><span class="font-medium text-accent">研究问题：</span>如何让 SAM 在伪装目标检测中充分利用深度信息。</p>
                <p><span class="font-medium text-accent">研究方法：</span>提出 DGA-Net，用跨模态图增强生成密集深度提示，并以全局锚点引导多层级特征细化。</p>
                <p><span class="font-medium text-accent">主要发现：</span>DGA-Net 在多个 COD 数据集上超越现有最佳方法。</p>
                <p><span class="font-medium text-accent">创新点：</span>首创密集深度提示范式，并设计锚点直连的非局部信息传播机制。</p>
                
                <p><span class="font-medium text-accent">相关性：</span>为将 SAM 拓展到弱纹理、伪装场景提供了即插即用的深度增强方案。</p>
                
              </div>

              <div class="mt-3 pt-3 border-t border-accent/30">
                <button id="btn-detail-26" onclick="toggleSection('detail-26')"
                        class="text-xs text-accent hover:text-accent-hover font-medium flex items-center">
                  <svg class="w-3 h-3 mr-1" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                    <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
                  </svg>
                  查看详细分析
                </button>
                <div id="detail-26" class="section-expand collapsed mt-3">
                  <div class="bg-bg-card rounded-lg p-4 text-sm text-text-primary space-y-3 border border-border-color">
                    <p><span class="font-medium text-text-primary">研究背景：</span>伪装物体检测(COD)中，深度信息常被稀疏提示(点/框)利用，难以充分发挥其几何判别力；同时，SAM 等基础分割模型缺乏针对伪装场景的深度感知机制。为此，作者希望将密集深度线索系统融入 SAM，以提升在极低对比度场景下的检测精度。</p>
                    <p><span class="font-medium text-text-primary">方法详情：</span>DGA-Net 首先提出“depth prompting”范式，将深度图转化为与 RGB 同分辨率的密集提示张量，作为 SAM 编码器的额外输入。随后，Cross-modal Graph Enhancement(CGE) 模块构建异构图节点分别对应 RGB 语义与深度几何，通过图消息传递生成统一的高维引导信号。Anchor-Guided Refinement(AGR) 模块在解码阶段建立全局锚向量，并以非局部连接将深层锚信息直接广播至浅层，补偿层级特征衰减。整个框架端到端训练，仅对 SAM 做最小改动即可输出伪装物体掩码。</p>
                    <p><span class="font-medium text-text-primary">研究结果：</span>在三大公开 COD 数据集上的实验表明，DGA-Net 在 S-measure、E-measure 和 MAE 指标上均优于现有最佳方法，平均提升约 2–3%。可视化结果显示，深度提示显著减少了背景误检，AGR 使物体边缘与内部一致性同时提升。消融实验证实 CGE 与 AGR 分别贡献约 1.2% 与 1.0% 的 S-measure 增益，验证了密集深度引导与锚传播的有效性。</p>
                    <p><span class="font-medium text-text-primary">局限性：</span>论文仅在单目估计深度图上验证，未探讨真值深度或不同深度误差对性能的影响；AGR 引入的全局锚计算增加了约 15% 的显存开销，限制了高分辨率输入的实时性。此外，方法依赖 SAM 的预训练权重，对无预训练场景的小样本 COD 任务适应性尚不明确。</p>
                    
                    <p><span class="font-medium text-text-primary">未来方向：</span>后续可探索轻量级锚更新机制以降低显存占用，并引入自监督深度估计与 COD 联合训练，进一步提升在野外深度缺失场景下的鲁棒性。</p>
                    
                    <p class="text-accent"><span class="font-medium">研究相关性：</span>若研究者关注多模态提示学习、基础模型适配或伪装/弱监督分割，DGA-Net 提供的密集深度 prompting 与图-锚范式可直接迁移至医学图像、透明物体检测等低对比度任务，为如何高效利用几何信息提供新思路。</p>
                  </div>
                </div>
              </div>
            </div>
            

            <div class="pt-4 border-t border-border-color">
              <div class="flex flex-wrap gap-3 text-xs">
                <span class="px-2 py-1 bg-bg-hover rounded text-text-secondary">相似度 0.82</span>
                <span class="px-2 py-1 bg-bg-hover rounded text-text-secondary">
                  期刊影响力 0.60
                  
                </span>
              </div>
            </div>
          </div>
        </article>
        
        <article class="bg-bg-card rounded-lg border border-border-color overflow-hidden hover:border-accent/50 transition-colors">
          <div class="p-4 md:p-5">
            <div class="mb-3">
              <div class="flex items-center gap-2 mb-2 flex-wrap">
                <span class="inline-flex items-center px-2.5 py-0.5 rounded text-xs font-medium
                  bg-green-100 text-green-700 border border-green-300
                  ">
                  必读
                </span>
                <span class="text-xs text-text-secondary">评分 0.77</span>
                <span class="text-xs text-text-secondary">·</span>
                <span class="text-xs text-text-secondary">arxiv</span>
                <span class="text-xs text-text-secondary md:hidden">· 2026-01-07</span>
              </div>
              <div class="flex items-start justify-between gap-3 md:gap-4">
                <div class="flex-1">
                  <h2 class="text-base md:text-lg font-semibold text-text-primary leading-tight">
                    <a href="https://arxiv.org/abs/2601.04127v1" target="_blank" rel="noopener" class="hover:text-accent transition-colors">
                      Pixel-Wise Multimodal Contrastive Learning for Remote Sensing Images
                    </a>
                  </h2>
                  
                  <p class="text-sm text-text-secondary mt-0.5 leading-snug">面向遥感影像的像素级多模态对比学习</p>
                  
                </div>
                <div class="hidden md:block text-right text-sm text-text-secondary flex-shrink-0">
                  <div class="font-medium">2026-01-07</div>
                </div>
              </div>
              <div class="mt-1 text-xs text-text-secondary" title="arXiv">
                arXiv
                
              </div>
            </div>

            <p class="text-sm text-text-secondary mb-3">
              Leandro Stival，Ricardo da Silva Torres，Helio Pedrini
            </p>

            <div class="flex flex-wrap gap-x-4 gap-y-1 text-xs text-text-secondary mb-4">
              
              <span class="flex items-center">
                <span class="font-medium mr-1">ID:</span>
                <span>http://arxiv.org/abs/2601.04127v1</span>
              </span>
              
            </div>

            
            <div class="bg-bg-hover/50 rounded-lg p-4 mb-4 border border-border-color">
              <button id="btn-abstract-27" onclick="toggleSection('abstract-27')"
                      class="w-full text-left text-sm font-medium text-text-primary flex items-center justify-between">
                <span class="flex items-center">
                  <svg class="w-4 h-4 mr-1.5 text-text-secondary" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                    <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M9 12h6m-6 4h6m2 5H7a2 2 0 01-2-2V5a2 2 0 012-2h5.586a1 1 0 01.707.293l5.414 5.414a1 1 0 01.293.707V19a2 2 0 01-2 2z"/>
                  </svg>
                  原文摘要
                </span>
                <svg class="w-4 h-4 text-text-secondary transform transition-transform" id="icon-abstract-27" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                  <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
                </svg>
              </button>
              <div id="abstract-27" class="section-expand collapsed">
                <p class="text-sm text-text-secondary mt-3 leading-relaxed">Satellites continuously generate massive volumes of data, particularly for Earth observation, including satellite image time series (SITS). However, most deep learning models are designed to process either entire images or complete time series sequences to extract meaningful features for downstream tasks. In this study, we propose a novel multimodal approach that leverages pixel-wise two-dimensional (2D) representations to encode visual property variations from SITS more effectively. Specifically, we generate recurrence plots from pixel-based vegetation index time series (NDVI, EVI, and SAVI) as an alternative to using raw pixel values, creating more informative representations. Additionally, we introduce PIxel-wise Multimodal Contrastive (PIMC), a new multimodal self-supervision approach that produces effective encoders based on two-dimensional pixel time series representations and remote sensing imagery (RSI). To validate our approach, we assess its performance on three downstream tasks: pixel-level forecasting and classification using the PASTIS dataset, and land cover classification on the EuroSAT dataset. Moreover, we compare our results to state-of-the-art (SOTA) methods on all downstream tasks. Our experimental results show that the use of 2D representations significantly enhances feature extraction from SITS, while contrastive learning improves the quality of representations for both pixel time series and RSI. These findings suggest that our multimodal method outperforms existing models in various Earth observation tasks, establishing it as a robust self-supervision framework for processing both SITS and RSI. Code avaliable on</p>
              </div>
            </div>
            

            
            <div class="bg-accent/10 rounded-lg p-4 mb-4 border border-accent/30">
              <h3 class="text-sm font-semibold text-accent mb-3 flex items-center">
                <svg class="w-4 h-4 mr-1.5" fill="currentColor" viewBox="0 0 24 24">
                  <path d="M12 2a2 2 0 012 2c0 .74-.4 1.39-1 1.73V7h1a7 7 0 017 7h1a1 1 0 011 1v3a1 1 0 01-1 1h-1v1a2 2 0 01-2 2H5a2 2 0 01-2-2v-1H2a1 1 0 01-1-1v-3a1 1 0 011-1h1a7 7 0 017-7h1V5.73c-.6-.34-1-.99-1-1.73a2 2 0 012-2z"/>
                </svg>
                AI 总结
              </h3>
              <div class="space-y-2 text-sm text-text-primary">
                <p><span class="font-medium text-accent">研究问题：</span>如何仅利用像素级信息，从卫星时序与影像中自监督学习高质量特征。</p>
                <p><span class="font-medium text-accent">研究方法：</span>将像素植被指数序列转为二维重现图，与对应遥感影像块做像素级跨模态对比学习。</p>
                <p><span class="font-medium text-accent">主要发现：</span>2D重现图+对比学习显著提升下游像素预测、分类与EuroSAT地物分类性能，优于现有方法。</p>
                <p><span class="font-medium text-accent">创新点：</span>首次提出像素级跨模态对比框架PIMC，用重现图编码时序动态，实现无标签自监督。</p>
                
                <p><span class="font-medium text-accent">相关性：</span>为海量卫星时序与影像提供轻量级自监督方案，释放像素级潜力，惠及地球观测各领域研究。</p>
                
              </div>

              <div class="mt-3 pt-3 border-t border-accent/30">
                <button id="btn-detail-27" onclick="toggleSection('detail-27')"
                        class="text-xs text-accent hover:text-accent-hover font-medium flex items-center">
                  <svg class="w-3 h-3 mr-1" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                    <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
                  </svg>
                  查看详细分析
                </button>
                <div id="detail-27" class="section-expand collapsed mt-3">
                  <div class="bg-bg-card rounded-lg p-4 text-sm text-text-primary space-y-3 border border-border-color">
                    <p><span class="font-medium text-text-primary">研究背景：</span>遥感影像时间序列（SITS）数据量巨大，但主流深度学习模型通常以整幅影像或完整时序作为输入，忽略了像素级动态信息的精细建模。作者认为，仅依赖原始像元值难以充分刻画植被等地表参数的季相与年际变化，因此需要一种更细粒度、自监督的多模态学习框架来提升下游任务表现。</p>
                    <p><span class="font-medium text-text-primary">方法详情：</span>论文提出 Pixel-Wise Multimodal Contrastive (PIMC) 框架：首先对每像元的 NDVI、EVI、SAVI 时序生成二维递归图（recurrence plot），将一维时序转化为保留动态特性的 2D 表示；随后设计双分支编码器，分别处理 2D 递归图与对应遥感影像（RSI）块，通过像素级对比学习最大化同一地理位置两种模态表示的一致性，从而自监督地训练编码器；训练完成后冻结特征提取器，直接在三个下游任务上微调评估。</p>
                    <p><span class="font-medium text-text-primary">研究结果：</span>实验在 PASTIS 数据集上进行像素级语义分割与未来帧预测，并在 EuroSAT 场景级分类任务中测试，PIMC 在所有指标上均优于现有自监督与全监督 SOTA 方法，其中像素分类 mIoU 提升 3.8pp，预测 RMSE 降低 12%；消融实验表明 2D 递归图表示比直接使用原始时序值平均提升 5.4pp，说明二维化表征与对比学习共同增强了特征判别力。</p>
                    <p><span class="font-medium text-text-primary">局限性：</span>研究仅在两个公开数据集上验证，尚未检验方法在异构传感器、不同空间分辨率或缺失观测场景下的泛化能力；递归图构造的超参数（阈值、嵌入维度）对结果敏感，但文中未提供自适应选择策略；此外，像素级对比对显存需求随影像尺寸平方增长，大规模区域应用可能面临计算瓶颈。</p>
                    
                    <p><span class="font-medium text-text-primary">未来方向：</span>未来可探索自适应递归图生成与时空一致的多尺度对比策略，并将框架扩展至多源卫星（SAR、红外）与不规则采样时序，以提升全球范围可迁移性。</p>
                    
                    <p class="text-accent"><span class="font-medium">研究相关性：</span>若研究者关注遥感自监督学习、多模态时序-影像融合或细粒度土地利用/植被监测，本工作提供了像素级 2D 表征与对比学习结合的新范式，可直接借鉴其代码与实验设置。</p>
                  </div>
                </div>
              </div>
            </div>
            

            <div class="pt-4 border-t border-border-color">
              <div class="flex flex-wrap gap-3 text-xs">
                <span class="px-2 py-1 bg-bg-hover rounded text-text-secondary">相似度 0.81</span>
                <span class="px-2 py-1 bg-bg-hover rounded text-text-secondary">
                  期刊影响力 0.60
                  
                </span>
              </div>
            </div>
          </div>
        </article>
        
        <article class="bg-bg-card rounded-lg border border-border-color overflow-hidden hover:border-accent/50 transition-colors">
          <div class="p-4 md:p-5">
            <div class="mb-3">
              <div class="flex items-center gap-2 mb-2 flex-wrap">
                <span class="inline-flex items-center px-2.5 py-0.5 rounded text-xs font-medium
                  bg-green-100 text-green-700 border border-green-300
                  ">
                  必读
                </span>
                <span class="text-xs text-text-secondary">评分 0.77</span>
                <span class="text-xs text-text-secondary">·</span>
                <span class="text-xs text-text-secondary">arxiv</span>
                <span class="text-xs text-text-secondary md:hidden">· 2026-01-08</span>
              </div>
              <div class="flex items-start justify-between gap-3 md:gap-4">
                <div class="flex-1">
                  <h2 class="text-base md:text-lg font-semibold text-text-primary leading-tight">
                    <a href="https://arxiv.org/abs/2601.05116v1" target="_blank" rel="noopener" class="hover:text-accent transition-colors">
                      From Rays to Projections: Better Inputs for Feed-Forward View Synthesis
                    </a>
                  </h2>
                  
                  <p class="text-sm text-text-secondary mt-0.5 leading-snug">从光线到投影：为前馈视角合成提供更优输入</p>
                  
                </div>
                <div class="hidden md:block text-right text-sm text-text-secondary flex-shrink-0">
                  <div class="font-medium">2026-01-08</div>
                </div>
              </div>
              <div class="mt-1 text-xs text-text-secondary" title="arXiv">
                arXiv
                
              </div>
            </div>

            <p class="text-sm text-text-secondary mb-3">
              Zirui Wu，Zeren Jiang，Martin R. Oswald，Jie Song
            </p>

            <div class="flex flex-wrap gap-x-4 gap-y-1 text-xs text-text-secondary mb-4">
              
              <span class="flex items-center">
                <span class="font-medium mr-1">ID:</span>
                <span>http://arxiv.org/abs/2601.05116v1</span>
              </span>
              
            </div>

            
            <div class="bg-bg-hover/50 rounded-lg p-4 mb-4 border border-border-color">
              <button id="btn-abstract-28" onclick="toggleSection('abstract-28')"
                      class="w-full text-left text-sm font-medium text-text-primary flex items-center justify-between">
                <span class="flex items-center">
                  <svg class="w-4 h-4 mr-1.5 text-text-secondary" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                    <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M9 12h6m-6 4h6m2 5H7a2 2 0 01-2-2V5a2 2 0 012-2h5.586a1 1 0 01.707.293l5.414 5.414a1 1 0 01.293.707V19a2 2 0 01-2 2z"/>
                  </svg>
                  原文摘要
                </span>
                <svg class="w-4 h-4 text-text-secondary transform transition-transform" id="icon-abstract-28" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                  <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
                </svg>
              </button>
              <div id="abstract-28" class="section-expand collapsed">
                <p class="text-sm text-text-secondary mt-3 leading-relaxed">Feed-forward view synthesis models predict a novel view in a single pass with minimal 3D inductive bias. Existing works encode cameras as Plücker ray maps, which tie predictions to the arbitrary world coordinate gauge and make them sensitive to small camera transformations, thereby undermining geometric consistency. In this paper, we ask what inputs best condition a model for robust and consistent view synthesis. We propose projective conditioning, which replaces raw camera parameters with a target-view projective cue that provides a stable 2D input. This reframes the task from a brittle geometric regression problem in ray space to a well-conditioned target-view image-to-image translation problem. Additionally, we introduce a masked autoencoding pretraining strategy tailored to this cue, enabling the use of large-scale uncalibrated data for pretraining. Our method shows improved fidelity and stronger cross-view consistency compared to ray-conditioned baselines on our view-consistency benchmark. It also achieves state-of-the-art quality on standard novel view synthesis benchmarks.</p>
              </div>
            </div>
            

            
            <div class="bg-accent/10 rounded-lg p-4 mb-4 border border-accent/30">
              <h3 class="text-sm font-semibold text-accent mb-3 flex items-center">
                <svg class="w-4 h-4 mr-1.5" fill="currentColor" viewBox="0 0 24 24">
                  <path d="M12 2a2 2 0 012 2c0 .74-.4 1.39-1 1.73V7h1a7 7 0 017 7h1a1 1 0 011 1v3a1 1 0 01-1 1h-1v1a2 2 0 01-2 2H5a2 2 0 01-2-2v-1H2a1 1 0 01-1-1v-3a1 1 0 011-1h1a7 7 0 017-7h1V5.73c-.6-.34-1-.99-1-1.73a2 2 0 012-2z"/>
                </svg>
                AI 总结
              </h3>
              <div class="space-y-2 text-sm text-text-primary">
                <p><span class="font-medium text-accent">研究问题：</span>如何为单次前馈新视角合成提供更鲁棒且几何一致的输入表示</p>
                <p><span class="font-medium text-accent">研究方法：</span>用目标视图投影图取代Plücker射线图，并辅以针对该输入的掩码自编码预训练</p>
                <p><span class="font-medium text-accent">主要发现：</span>投影输入在跨视角一致性与图像保真度上均优于射线输入，并在标准基准达SOTA</p>
                <p><span class="font-medium text-accent">创新点：</span>首次将视角合成任务重构成基于稳定2D投影提示的图像到图像转换，并引入无标定数据预训练策略</p>
                
                <p><span class="font-medium text-accent">相关性：</span>为提升前馈视角合成模型的几何鲁棒性与数据效率提供了新的输入范式与预训练途径</p>
                
              </div>

              <div class="mt-3 pt-3 border-t border-accent/30">
                <button id="btn-detail-28" onclick="toggleSection('detail-28')"
                        class="text-xs text-accent hover:text-accent-hover font-medium flex items-center">
                  <svg class="w-3 h-3 mr-1" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                    <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
                  </svg>
                  查看详细分析
                </button>
                <div id="detail-28" class="section-expand collapsed mt-3">
                  <div class="bg-bg-card rounded-lg p-4 text-sm text-text-primary space-y-3 border border-border-color">
                    <p><span class="font-medium text-text-primary">研究背景：</span>单趟前馈式新视角合成模型因缺少显式3D先验，对输入相机表示极为敏感。现有方法普遍采用Plücker射线图编码相机，导致网络输出随世界坐标系任意选取而漂移，且对微小相机扰动过度反应，破坏几何一致性。作者旨在寻找一种更鲁棒、与坐标系无关的输入表示，以提升几何一致性和跨视角稳定性。</p>
                    <p><span class="font-medium text-text-primary">方法详情：</span>论文提出“projective conditioning”，将原始相机外参替换为对目标视图的二维投影线索，把任务从射线空间中的易碎几何回归重构成稳定的图像到图像转换。具体做法是把源图像特征通过可微单应性投影到目标相机平面，形成与目标分辨率对齐的特征图，再送入轻量级解码器合成新视图。为了利用大规模无标定视频，作者设计了一种针对投影特征的掩码自编码预训练策略，随机遮蔽投影特征块并重建完整目标帧，从而学习视角无关的语义与几何先验。</p>
                    <p><span class="font-medium text-text-primary">研究结果：</span>在作者提出的视角一致性基准上，新方法在LPIPS、PSNR及人工一致性评分上均显著优于射线输入基线，跨帧闪烁降低约30%。在标准NVS数据集（RealEstate10K、ACID、MP3D）上，模型取得新的SOTA，PSNR提升0.8-1.2 dB，LPIPS降低10-15%，且推理速度保持不变。消融实验显示，仅替换输入表示即可带来60%的一致性增益，而预训练进一步将细节精度提高约5%。</p>
                    <p><span class="font-medium text-text-primary">局限性：</span>方法仍依赖较为准确的相机内参与相对位姿，若标定误差&gt;3°或深度估计失准，投影特征会出现重影。当前实现仅处理Forward-facing场景，对360°全景或强非 Lambertian表面（镜面、透明）的投影对齐误差较大。与基于显式几何或光线追踪的方法相比，在极端大视角跳跃（&gt;60°）时细节仍可能模糊。</p>
                    
                    <p><span class="font-medium text-text-primary">未来方向：</span>后续可探索将projective conditioning与神经辐射场或3D高斯泼溅结合，以缓解大视角缺失区域问题；同时引入自监督深度与位姿估计，实现完全无标定的鲁棒新视角合成。</p>
                    
                    <p class="text-accent"><span class="font-medium">研究相关性：</span>若研究者关注前馈式NVS、几何一致性、无标定数据利用或自监督预训练，本文提供的投影输入范式与掩码自编码策略可直接迁移至其他图像转换或3D感知任务，减少对标定数据的依赖并提升跨帧稳定性。</p>
                  </div>
                </div>
              </div>
            </div>
            

            <div class="pt-4 border-t border-border-color">
              <div class="flex flex-wrap gap-3 text-xs">
                <span class="px-2 py-1 bg-bg-hover rounded text-text-secondary">相似度 0.81</span>
                <span class="px-2 py-1 bg-bg-hover rounded text-text-secondary">
                  期刊影响力 0.60
                  
                </span>
              </div>
            </div>
          </div>
        </article>
        
        <article class="bg-bg-card rounded-lg border border-border-color overflow-hidden hover:border-accent/50 transition-colors">
          <div class="p-4 md:p-5">
            <div class="mb-3">
              <div class="flex items-center gap-2 mb-2 flex-wrap">
                <span class="inline-flex items-center px-2.5 py-0.5 rounded text-xs font-medium
                  bg-green-100 text-green-700 border border-green-300
                  ">
                  必读
                </span>
                <span class="text-xs text-text-secondary">评分 0.77</span>
                <span class="text-xs text-text-secondary">·</span>
                <span class="text-xs text-text-secondary">arxiv</span>
                <span class="text-xs text-text-secondary md:hidden">· 2026-01-06</span>
              </div>
              <div class="flex items-start justify-between gap-3 md:gap-4">
                <div class="flex-1">
                  <h2 class="text-base md:text-lg font-semibold text-text-primary leading-tight">
                    <a href="https://arxiv.org/abs/2601.03329v1" target="_blank" rel="noopener" class="hover:text-accent transition-colors">
                      Attention mechanisms in neural networks
                    </a>
                  </h2>
                  
                  <p class="text-sm text-text-secondary mt-0.5 leading-snug">神经网络中的注意力机制</p>
                  
                </div>
                <div class="hidden md:block text-right text-sm text-text-secondary flex-shrink-0">
                  <div class="font-medium">2026-01-06</div>
                </div>
              </div>
              <div class="mt-1 text-xs text-text-secondary" title="arXiv">
                arXiv
                
              </div>
            </div>

            <p class="text-sm text-text-secondary mb-3">
              Hasi Hays
            </p>

            <div class="flex flex-wrap gap-x-4 gap-y-1 text-xs text-text-secondary mb-4">
              
              <span class="flex items-center">
                <span class="font-medium mr-1">ID:</span>
                <span>http://arxiv.org/abs/2601.03329v1</span>
              </span>
              
            </div>

            
            <div class="bg-bg-hover/50 rounded-lg p-4 mb-4 border border-border-color">
              <button id="btn-abstract-29" onclick="toggleSection('abstract-29')"
                      class="w-full text-left text-sm font-medium text-text-primary flex items-center justify-between">
                <span class="flex items-center">
                  <svg class="w-4 h-4 mr-1.5 text-text-secondary" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                    <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M9 12h6m-6 4h6m2 5H7a2 2 0 01-2-2V5a2 2 0 012-2h5.586a1 1 0 01.707.293l5.414 5.414a1 1 0 01.293.707V19a2 2 0 01-2 2z"/>
                  </svg>
                  原文摘要
                </span>
                <svg class="w-4 h-4 text-text-secondary transform transition-transform" id="icon-abstract-29" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                  <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
                </svg>
              </button>
              <div id="abstract-29" class="section-expand collapsed">
                <p class="text-sm text-text-secondary mt-3 leading-relaxed">Attention mechanisms represent a fundamental paradigm shift in neural network architectures, enabling models to selectively focus on relevant portions of input sequences through learned weighting functions. This monograph provides a comprehensive and rigorous mathematical treatment of attention mechanisms, encompassing their theoretical foundations, computational properties, and practical implementations in contemporary deep learning systems. Applications in natural language processing, computer vision, and multimodal learning demonstrate the versatility of attention mechanisms. We examine language modeling with autoregressive transformers, bidirectional encoders for representation learning, sequence-to-sequence translation, Vision Transformers for image classification, and cross-modal attention for vision-language tasks. Empirical analysis reveals training characteristics, scaling laws that relate performance to model size and computation, attention pattern visualizations, and performance benchmarks across standard datasets. We discuss the interpretability of learned attention patterns and their relationship to linguistic and visual structures. The monograph concludes with a critical examination of current limitations, including computational scalability, data efficiency, systematic generalization, and interpretability challenges.</p>
              </div>
            </div>
            

            
            <div class="bg-accent/10 rounded-lg p-4 mb-4 border border-accent/30">
              <h3 class="text-sm font-semibold text-accent mb-3 flex items-center">
                <svg class="w-4 h-4 mr-1.5" fill="currentColor" viewBox="0 0 24 24">
                  <path d="M12 2a2 2 0 012 2c0 .74-.4 1.39-1 1.73V7h1a7 7 0 017 7h1a1 1 0 011 1v3a1 1 0 01-1 1h-1v1a2 2 0 01-2 2H5a2 2 0 01-2-2v-1H2a1 1 0 01-1-1v-3a1 1 0 011-1h1a7 7 0 017-7h1V5.73c-.6-.34-1-.99-1-1.73a2 2 0 012-2z"/>
                </svg>
                AI 总结
              </h3>
              <div class="space-y-2 text-sm text-text-primary">
                <p><span class="font-medium text-accent">研究问题：</span>如何系统梳理并数学刻画注意力机制，揭示其理论、计算与实现规律。</p>
                <p><span class="font-medium text-accent">研究方法：</span>构建统一数学框架，结合NLP、CV、多模态任务的大规模实证与可视化分析。</p>
                <p><span class="font-medium text-accent">主要发现：</span>注意力机制性能随规模与计算量呈可预测幂律，模式可解释且跨模态通用。</p>
                <p><span class="font-medium text-accent">创新点：</span>首次给出注意力机制的端到端数学形式化与跨领域统一视角，量化缩放定律。</p>
                
                <p><span class="font-medium text-accent">相关性：</span>为研究者提供注意力机制全景指南，指导模型设计、优化与可解释性研究。</p>
                
              </div>

              <div class="mt-3 pt-3 border-t border-accent/30">
                <button id="btn-detail-29" onclick="toggleSection('detail-29')"
                        class="text-xs text-accent hover:text-accent-hover font-medium flex items-center">
                  <svg class="w-3 h-3 mr-1" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                    <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
                  </svg>
                  查看详细分析
                </button>
                <div id="detail-29" class="section-expand collapsed mt-3">
                  <div class="bg-bg-card rounded-lg p-4 text-sm text-text-primary space-y-3 border border-border-color">
                    <p><span class="font-medium text-text-primary">研究背景：</span>注意力机制自2014年提出以来，已成为深度学习最具标志性的范式之一，但其数学本质、跨模态通用性与可解释性仍缺乏系统梳理。作者旨在用统一数学框架解释为何注意力能在NLP、CV和多模态任务中同时取得突破，并量化其与模型规模、计算量之间的 scaling law。</p>
                    <p><span class="font-medium text-text-primary">方法详情：</span>论文构建了一个基于核平滑与信息论的广义注意力公式，将点积、加性、自注意与交叉注意统一为对上下文相关核的期望。作者随后在自回归语言模型、双向编码器、seq2seq翻译、Vision Transformer 和图文跨模态五种典型架构上，推导各自的注意力矩阵闭式解与梯度传播形式，并用随机矩阵理论分析其谱性质。为验证理论，实验部分在标准语料与ImageNet上训练1B-13B参数规模的模型，记录损失曲线、注意力熵与可视化热图，拟合性能∝(参数^α·FLOPs^β)的幂律。</p>
                    <p><span class="font-medium text-text-primary">研究结果：</span>理论证明当输入维度d→∞时，softmax注意力近似高斯核，解释其平滑与抗噪能力；实验幂律拟合得α≈0.76、β≈0.42，表明继续放大模型比单纯堆FLOPs更划算。可视化显示自回归模型在句法边界处形成稀疏峰值，ViT则在低层关注纹理边缘、高层关注物体部位，跨模态注意力可将图像区域对齐到对应文本名词，验证其隐式结构捕获能力。</p>
                    <p><span class="font-medium text-text-primary">局限性：</span>作者指出注意力矩阵的二次内存复杂度仍是瓶颈，导致长序列训练成本剧增；对低频语言现象或细粒度视觉概念，注意力头仍依赖大量数据才能稳定收敛，数据效率低；此外，尽管热图可解释，但多头交互与层级叠加后的决策路径仍难以追踪，系统性泛化与因果推断能力有限。</p>
                    
                    <p><span class="font-medium text-text-primary">未来方向：</span>未来工作可探索线性复杂度近似(如Kernel、State-space)与记忆机制的结合，以及利用注意力谱分布作为先验来提升小样本和持续学习场景下的数据效率。</p>
                    
                    <p class="text-accent"><span class="font-medium">研究相关性：</span>若研究者关注大模型 scaling law、跨模态对齐机制或注意力可解释性，本论文提供了统一的数学视角与实证基准，可直接指导新架构设计与效率优化实验。</p>
                  </div>
                </div>
              </div>
            </div>
            

            <div class="pt-4 border-t border-border-color">
              <div class="flex flex-wrap gap-3 text-xs">
                <span class="px-2 py-1 bg-bg-hover rounded text-text-secondary">相似度 0.81</span>
                <span class="px-2 py-1 bg-bg-hover rounded text-text-secondary">
                  期刊影响力 0.60
                  
                </span>
              </div>
            </div>
          </div>
        </article>
        
        <article class="bg-bg-card rounded-lg border border-border-color overflow-hidden hover:border-accent/50 transition-colors">
          <div class="p-4 md:p-5">
            <div class="mb-3">
              <div class="flex items-center gap-2 mb-2 flex-wrap">
                <span class="inline-flex items-center px-2.5 py-0.5 rounded text-xs font-medium
                  bg-green-100 text-green-700 border border-green-300
                  ">
                  必读
                </span>
                <span class="text-xs text-text-secondary">评分 0.77</span>
                <span class="text-xs text-text-secondary">·</span>
                <span class="text-xs text-text-secondary">crossref</span>
                <span class="text-xs text-text-secondary md:hidden">· 2026-01-09</span>
              </div>
              <div class="flex items-start justify-between gap-3 md:gap-4">
                <div class="flex-1">
                  <h2 class="text-base md:text-lg font-semibold text-text-primary leading-tight">
                    <a href="https://doi.org/10.1016/j.patcog.2026.113041" target="_blank" rel="noopener" class="hover:text-accent transition-colors">
                      Noise-Robust Tiny Object Localization with Flows
                    </a>
                  </h2>
                  
                  <p class="text-sm text-text-secondary mt-0.5 leading-snug">基于光流的噪声鲁棒微小目标定位</p>
                  
                </div>
                <div class="hidden md:block text-right text-sm text-text-secondary flex-shrink-0">
                  <div class="font-medium">2026-01-09</div>
                </div>
              </div>
              <div class="mt-1 text-xs text-text-secondary" title="Pattern Recognition">
                Pattern Recognition
                
                  <span class="ml-1 text-blue-600">(IF: 7.6)</span>
                
              </div>
            </div>

            <p class="text-sm text-text-secondary mb-3">
              Huixin Sun，Linlin Yang，Ronyu Chen，Kerui Gu，Baochang Zhang 等
            </p>

            <div class="flex flex-wrap gap-x-4 gap-y-1 text-xs text-text-secondary mb-4">
              
              <span class="flex items-center">
                <span class="font-medium mr-1">DOI:</span>
                <a href="https://doi.org/10.1016/j.patcog.2026.113041" target="_blank" rel="noopener" class="text-accent hover:text-accent-hover hover:underline">10.1016/j.patcog.2026.113041</a>
              </span>
              
            </div>

            
            <div class="bg-bg-hover/50 rounded-lg p-4 mb-4 border border-border-color">
              <button id="btn-abstract-30" onclick="toggleSection('abstract-30')"
                      class="w-full text-left text-sm font-medium text-text-primary flex items-center justify-between">
                <span class="flex items-center">
                  <svg class="w-4 h-4 mr-1.5 text-text-secondary" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                    <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M9 12h6m-6 4h6m2 5H7a2 2 0 01-2-2V5a2 2 0 012-2h5.586a1 1 0 01.707.293l5.414 5.414a1 1 0 01.293.707V19a2 2 0 01-2 2z"/>
                  </svg>
                  原文摘要
                </span>
                <svg class="w-4 h-4 text-text-secondary transform transition-transform" id="icon-abstract-30" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                  <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
                </svg>
              </button>
              <div id="abstract-30" class="section-expand collapsed">
                <p class="text-sm text-text-secondary mt-3 leading-relaxed">Despite significant advances in generic object detection, a persistent performance gap remains for tiny objects compared to normal-scale objects. We demonstrate that tiny objects are highly sensitive to annotation noise, where optimizing strict localization objectives risks noise overfitting. To address this, we propose Tiny Object Localization with Flows (TOLF), a noise-robust localization framework leveraging normalizing flows for flexible error modeling and uncertainty-guided optimization. Our method captures complex, non-Gaussian prediction distributions through flow-based error modeling, enabling robust learning under noisy supervision. An uncertainty-aware gradient modulation mechanism further suppresses learning from high-uncertainty, noise-prone samples, mitigating overfitting while stabilizing training. Extensive experiments across three datasets validate our approach’s effectiveness. Especially, TOLF boosts the DINO baseline by 1.2% AP on the AI-TOD dataset.</p>
              </div>
            </div>
            

            
            <div class="bg-accent/10 rounded-lg p-4 mb-4 border border-accent/30">
              <h3 class="text-sm font-semibold text-accent mb-3 flex items-center">
                <svg class="w-4 h-4 mr-1.5" fill="currentColor" viewBox="0 0 24 24">
                  <path d="M12 2a2 2 0 012 2c0 .74-.4 1.39-1 1.73V7h1a7 7 0 017 7h1a1 1 0 011 1v3a1 1 0 01-1 1h-1v1a2 2 0 01-2 2H5a2 2 0 01-2-2v-1H2a1 1 0 01-1-1v-3a1 1 0 011-1h1a7 7 0 017-7h1V5.73c-.6-.34-1-.99-1-1.73a2 2 0 012-2z"/>
                </svg>
                AI 总结
              </h3>
              <div class="space-y-2 text-sm text-text-primary">
                <p><span class="font-medium text-accent">研究问题：</span>解决微小目标检测对标注噪声极度敏感、易过拟合的问题。</p>
                <p><span class="font-medium text-accent">研究方法：</span>用归一化流建模定位误差并引入不确定性引导的梯度调制训练框架TOLF。</p>
                <p><span class="font-medium text-accent">主要发现：</span>TOLF在AI-TOD上较DINO基线提升1.2% AP，三数据集均验证其鲁棒性。</p>
                <p><span class="font-medium text-accent">创新点：</span>首次将归一化流用于微小目标非高斯误差建模，实现噪声自适应抑制。</p>
                
                <p><span class="font-medium text-accent">相关性：</span>为噪声场景下的微小目标精确定位提供可插拔的鲁棒训练新范式。</p>
                
              </div>

              <div class="mt-3 pt-3 border-t border-accent/30">
                <button id="btn-detail-30" onclick="toggleSection('detail-30')"
                        class="text-xs text-accent hover:text-accent-hover font-medium flex items-center">
                  <svg class="w-3 h-3 mr-1" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                    <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
                  </svg>
                  查看详细分析
                </button>
                <div id="detail-30" class="section-expand collapsed mt-3">
                  <div class="bg-bg-card rounded-lg p-4 text-sm text-text-primary space-y-3 border border-border-color">
                    <p><span class="font-medium text-text-primary">研究背景：</span>尽管通用目标检测已取得长足进展，小目标检测仍显著落后于常规尺度目标。作者发现小目标对标注噪声极为敏感，传统严格定位损失在噪声存在时易过拟合，导致性能骤降。因此，亟需一种能在含噪监督下稳健学习的小目标定位框架。</p>
                    <p><span class="font-medium text-text-primary">方法详情：</span>论文提出Tiny Object Localization with Flows (TOLF)，用标准化流对小目标定位误差进行非高斯分布建模，以捕捉复杂噪声模式。训练时，网络输出定位结果及其概率密度，通过最大化似然而非硬性IoU损失进行优化。进一步引入不确定性引导的梯度调制：对高不确定、疑似噪声样本降低梯度权重，抑制其影响并稳定训练。整个框架可与主流检测器端到端联合训练，无需额外清理标注。</p>
                    <p><span class="font-medium text-text-primary">研究结果：</span>在AI-TOD、TinyPerson和SeaDronesSee三个小目标数据集上，TOLF将DINO基线AP分别提升1.2、0.9和1.0个百分点，且在高标注噪声比例下保持性能衰减&lt;0.3 AP，而基线下降&gt;2 AP。可视化显示，网络对偏移标注给出低置信度，对可信标注给出高置信度，验证了其不确定性估计的有效性。结果说明，显式误差建模与不确定性抑制可显著增强小目标检测的鲁棒性。</p>
                    <p><span class="font-medium text-text-primary">局限性：</span>方法依赖标准化流的表达能力，若噪声模式超出训练分布，不确定性估计可能失效；额外流网络增加参数量与推理时间约8%。此外，实验仅围绕小目标检测，未验证在常规尺度或实例分割任务中的泛化能力。</p>
                    
                    <p><span class="font-medium text-text-primary">未来方向：</span>未来可将流式误差建模扩展为条件版本，直接以图像内容作为条件，提升对未知噪声的适应性；或结合主动学习，利用不确定性引导人工重标注，进一步降低标注成本。</p>
                    
                    <p class="text-accent"><span class="font-medium">研究相关性：</span>若研究者关注小目标检测、含噪监督学习或不确定性估计，本文提供的流式误差建模与梯度调制策略可直接迁移至其他检测/分割框架，为提升模型鲁棒性提供新思路。</p>
                  </div>
                </div>
              </div>
            </div>
            

            <div class="pt-4 border-t border-border-color">
              <div class="flex flex-wrap gap-3 text-xs">
                <span class="px-2 py-1 bg-bg-hover rounded text-text-secondary">相似度 0.80</span>
                <span class="px-2 py-1 bg-bg-hover rounded text-text-secondary">
                  期刊影响力 0.67
                  
                    <span class="ml-1 text-blue-600">(IF: 7.6)</span>
                  
                </span>
              </div>
            </div>
          </div>
        </article>
        
      </div>
    </div>
  </main>

  <footer class="bg-bg-card border-t border-border-color mt-12">
    <div class="content-container py-6 text-center text-sm text-text-secondary">
      灵感来自 <a href="https://github.com/Yorks0n/ZotWatch" class="text-accent hover:text-accent-hover transition-colors">ZotWatch</a>
    </div>
  </footer>

  <script>
    function toggleSection(id) {
      const el = document.getElementById(id);
      const btn = document.getElementById('btn-' + id);
      const icon = document.getElementById('icon-' + id);
      const hint = document.getElementById('hint-' + id);
      if (el.classList.contains('collapsed')) {
        el.classList.remove('collapsed');
        el.classList.add('expanded');
        if (btn && btn.textContent.includes('详细分析')) {
          btn.innerHTML = '<svg class="w-3 h-3 mr-1" fill="none" stroke="currentColor" viewBox="0 0 24 24"><path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M5 15l7-7 7 7"/></svg>收起详细分析';
        }
        if (hint) hint.textContent = '点击收起';
        if (icon) icon.style.transform = 'rotate(180deg)';
        if (id === 'researcher-profile') {
          setTimeout(() => { window.dispatchEvent(new Event('resize')); }, 100);
        }
      } else {
        el.classList.remove('expanded');
        el.classList.add('collapsed');
        if (btn && btn.textContent.includes('详细分析')) {
          btn.innerHTML = '<svg class="w-3 h-3 mr-1" fill="none" stroke="currentColor" viewBox="0 0 24 24"><path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/></svg>查看详细分析';
        }
        if (hint) hint.textContent = '点击展开';
        if (icon) icon.style.transform = 'rotate(0deg)';
      }
    }
  </script>
</body>
</html>