<!DOCTYPE html>
<html lang="zh-CN">
<head>
  <meta charset="utf-8">
  <meta name="viewport" content="width=device-width, initial-scale=1">
  <title>ZotWatch 文献推荐 - 2025-12-15</title>
  <script src="https://cdn.tailwindcss.com"></script>
  <script src="https://cdn.jsdelivr.net/npm/chart.js@4.4.1/dist/chart.umd.min.js"></script>
  <script src="https://d3js.org/d3.v7.min.js"></script>
  <script>
    tailwind.config = {
      theme: {
        extend: {
          colors: {
            'bg-primary': '#f8fafc',
            'bg-card': '#ffffff',
            'bg-hover': '#f1f5f9',
            'text-primary': '#1e293b',
            'text-secondary': '#64748b',
            'border-color': '#e2e8f0',
            'accent': '#2563eb',
            'accent-hover': '#1d4ed8',
          }
        }
      }
    }
  </script>
  <style>
    .section-expand { transition: max-height 0.5s ease-out; overflow: hidden; }
    .section-expand.collapsed { max-height: 0; }
    .section-expand.expanded { max-height: none; }
    body { font-family: -apple-system, BlinkMacSystemFont, 'Segoe UI', Roboto, 'Helvetica Neue', Arial, sans-serif; }
    .content-container { max-width: 48rem; margin: 0 auto; padding: 0 1rem; }
    @media (min-width: 768px) { .content-container { padding: 0 2rem; } }
    @media (min-width: 1024px) { .content-container { padding: 0 4rem; } }

    .cluster-graph-container { width: 100%; height: 280px; touch-action: pan-y; }
    @media (min-width: 640px) { .cluster-graph-container { height: 320px; } }
    @media (min-width: 768px) { .cluster-graph-container { height: 350px; } }
    #cluster-graph svg { overflow: visible; }
  </style>
</head>
<body class="bg-bg-primary min-h-screen text-text-primary">
  <header class="bg-bg-card border-b border-border-color">
    <div class="content-container py-6 md:py-8">
      <h1 class="text-xl md:text-2xl font-bold text-text-primary">ZotWatch 文献推荐</h1>
      <p class="text-xs md:text-sm text-text-secondary mt-1">
        共 35 篇论文 ·
        生成于 2025-12-15 10:46 Asia/Shanghai
      </p>
    </div>
  </header>

  
  <section class="py-4 md:py-6 border-b border-border-color">
    <div class="content-container">
      <button id="btn-researcher-profile" onclick="toggleSection('researcher-profile')"
              class="w-full text-left text-lg md:text-xl font-bold text-text-primary mb-3 md:mb-4 flex items-center justify-between group hover:text-accent transition-colors">
        <span class="flex items-center gap-2 flex-1 min-w-0">
          <svg class="w-5 h-5 flex-shrink-0 text-accent" fill="currentColor" viewBox="0 0 24 24">
            <path d="M12 2C6.48 2 2 6.48 2 12s4.48 10 10 10 10-4.48 10-10S17.52 2 12 2zm0 3c1.66 0 3 1.34 3 3s-1.34 3-3 3-3-1.34-3-3 1.34-3 3-3zm0 14.2c-2.5 0-4.71-1.28-6-3.22.03-1.99 4-3.08 6-3.08 1.99 0 5.97 1.09 6 3.08-1.29 1.94-3.5 3.22-6 3.22z"/>
          </svg>
          <span class="truncate">研究兴趣画像</span>
        </span>
        <span class="flex items-center gap-1 md:gap-2 flex-shrink-0 ml-2">
          <span class="hidden md:inline text-sm font-normal text-text-secondary group-hover:text-accent" id="hint-researcher-profile">点击展开</span>
          <svg class="w-5 h-5 md:w-4 md:h-4 text-text-secondary group-hover:text-accent transform transition-transform" id="icon-researcher-profile" fill="none" stroke="currentColor" viewBox="0 0 24 24">
            <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
          </svg>
        </span>
      </button>

      <div id="researcher-profile" class="section-expand collapsed">
      <div class="bg-bg-card rounded-lg border border-border-color p-4 mb-4">
        <div class="grid grid-cols-4 gap-3 text-center">
          <div>
            <div class="text-xl font-bold text-accent">922</div>
            <div class="text-xs text-text-secondary">收藏论文</div>
          </div>
          <div>
            <div class="text-xl font-bold text-accent">5年7月</div>
            <div class="text-xs text-text-secondary">收藏时长</div>
          </div>
          <div>
            <div class="text-xl font-bold text-accent">&gt;10</div>
            <div class="text-xs text-text-secondary">兴趣领域</div>
          </div>
          <div>
            <div class="text-xl font-bold text-accent">8</div>
            <div class="text-xs text-text-secondary">高频作者</div>
          </div>
        </div>
      </div>

      
      <div class="bg-accent/10 rounded-lg border border-accent/30 p-4 mb-4">
        <h3 class="text-sm font-semibold text-accent mb-2 flex items-center gap-2">
          <svg class="w-4 h-4" fill="currentColor" viewBox="0 0 24 24">
            <path d="M12 2a2 2 0 012 2c0 .74-.4 1.39-1 1.73V7h1a7 7 0 017 7h1a1 1 0 011 1v3a1 1 0 01-1 1h-1v1a2 2 0 01-2 2H5a2 2 0 01-2-2v-1H2a1 1 0 01-1-1v-3a1 1 0 011-1h1a7 7 0 017-7h1V5.73c-.6-.34-1-.99-1-1.73a2 2 0 012-2z"/>
          </svg>
          AI 研究兴趣分析
        </h3>
        <div class="space-y-2 text-xs text-text-primary leading-relaxed">
          <p><span class="font-medium text-accent">研究兴趣：</span>用户长期关注计算机视觉基础任务（目标检测、视觉定位、姿态估计）及其高效化（模型压缩、重参数化），并同步追踪遥感影像（尤其是SAR）智能解译，形成“视觉+遥感”双主线阅读格局。</p>
          <p><span class="font-medium text-accent">深度关注：</span>在目标检测与相关基准模型（ResNet、R-CNN系列、HRNet）上有持续且系统的文献积累，同时围绕SAR目标识别与旋转目标检测保持7篇以上专题收藏，显示出对视觉通用方法与遥感专用问题的双重深耕。</p>
          <p><span class="font-medium text-accent">跨学科倾向：</span>收藏链横跨计算机视觉、遥感/地学、机器学习三大库，既读CVPR/ICCV/NeurIPS前沿算法，也系统阅读IEEE TGARS、雷达学报等遥感期刊，体现出“CV方法向遥感迁移”的交叉阅读特征。</p>
          <p><span class="font-medium text-accent">兴趣演变：</span>2024-Q1与2025-Q1出现两次收藏高峰（67→83篇），新增关键词集中在大模型（LLM、DeepSeek）、视觉Transformer、可微分渲染与多视角生成，表明兴趣正从传统检测/压缩向“基础模型+三维感知+生成式AI”快速外溢。</p>
          <p><span class="font-medium text-accent">阅读建议：</span>建议跟进视觉-语言模型在遥感图像描述与检索中的应用，并关注基于NeRF/可微渲染的多视角SAR数据增强与目标识别新范式，可延续其检测优势并拓展到三维场景理解。</p>
        </div>
      </div>
      

      <div class="grid grid-cols-1 md:grid-cols-2 gap-4 mb-4">
        
        <div class="bg-bg-card rounded-lg border border-border-color p-4">
          <h4 class="text-sm font-medium text-text-primary mb-3">研究领域分布</h4>
          <div style="height: 180px;">
            <canvas id="domainsChart"></canvas>
          </div>
        </div>
        

        
        <div class="bg-bg-card rounded-lg border border-border-color p-4">
          <h4 class="text-sm font-medium text-text-primary mb-3">阅读趋势（季度）</h4>
          <div style="height: 180px;">
            <canvas id="trendsChart"></canvas>
          </div>
        </div>
        
      </div>

      
      <div class="bg-bg-card rounded-lg border border-border-color p-4 mb-4">
        <h4 class="text-sm font-medium text-text-primary mb-3">论文发表年份分布</h4>
        <div style="height: 180px;">
          <canvas id="yearDistChart"></canvas>
        </div>
      </div>
      

      
      <div class="bg-bg-card rounded-lg border border-border-color p-4 mb-4">
        <h4 class="text-sm font-medium text-text-primary mb-3">
          研究方向聚类图谱
          <span class="text-xs text-text-secondary font-normal ml-2">(29 个聚类)</span>
        </h4>
        <div id="cluster-graph" class="cluster-graph-container"></div>
        <div class="text-xs text-text-secondary mt-3">
          覆盖 898/898 篇论文（仅含摘要的文献参与聚类）
        </div>
      </div>
      

      <div class="grid grid-cols-1 md:grid-cols-2 gap-4 mb-4">
        
        <div class="bg-bg-card rounded-lg border border-border-color p-4">
          <h4 class="text-sm font-medium text-text-primary mb-2">高频关注作者</h4>
          <div class="grid grid-cols-2 gap-x-4 gap-y-1 text-xs">
            
            <div class="flex justify-between truncate">
              <span class="text-text-primary truncate" title="Kaiming He">Kaiming He</span>
              <span class="text-text-secondary ml-1 flex-shrink-0">24</span>
            </div>
            
            <div class="flex justify-between truncate">
              <span class="text-text-primary truncate" title="Ross Girshick">Ross Girshick</span>
              <span class="text-text-secondary ml-1 flex-shrink-0">18</span>
            </div>
            
            <div class="flex justify-between truncate">
              <span class="text-text-primary truncate" title="Jian Sun">Jian Sun</span>
              <span class="text-text-secondary ml-1 flex-shrink-0">15</span>
            </div>
            
            <div class="flex justify-between truncate">
              <span class="text-text-primary truncate" title="Song Han">Song Han</span>
              <span class="text-text-secondary ml-1 flex-shrink-0">14</span>
            </div>
            
            <div class="flex justify-between truncate">
              <span class="text-text-primary truncate" title="Xiangyu Zhang">Xiangyu Zhang</span>
              <span class="text-text-secondary ml-1 flex-shrink-0">14</span>
            </div>
            
            <div class="flex justify-between truncate">
              <span class="text-text-primary truncate" title="Piotr Dollár">Piotr Dollár</span>
              <span class="text-text-secondary ml-1 flex-shrink-0">12</span>
            </div>
            
            <div class="flex justify-between truncate">
              <span class="text-text-primary truncate" title="Li Liu">Li Liu</span>
              <span class="text-text-secondary ml-1 flex-shrink-0">10</span>
            </div>
            
            <div class="flex justify-between truncate">
              <span class="text-text-primary truncate" title="Xiang Li">Xiang Li</span>
              <span class="text-text-secondary ml-1 flex-shrink-0">10</span>
            </div>
            
            <div class="flex justify-between truncate">
              <span class="text-text-primary truncate" title="Chunhua Shen">Chunhua Shen</span>
              <span class="text-text-secondary ml-1 flex-shrink-0">8</span>
            </div>
            
            <div class="flex justify-between truncate">
              <span class="text-text-primary truncate" title="Yann LeCun">Yann LeCun</span>
              <span class="text-text-secondary ml-1 flex-shrink-0">8</span>
            </div>
            
            <div class="flex justify-between truncate">
              <span class="text-text-primary truncate" title="Gangyao Kuang">Gangyao Kuang</span>
              <span class="text-text-secondary ml-1 flex-shrink-0">8</span>
            </div>
            
            <div class="flex justify-between truncate">
              <span class="text-text-primary truncate" title="Xian Sun">Xian Sun</span>
              <span class="text-text-secondary ml-1 flex-shrink-0">8</span>
            </div>
            
          </div>
        </div>
        

        
        <div class="bg-bg-card rounded-lg border border-border-color p-4">
          <h4 class="text-sm font-medium text-text-primary mb-2">常关注期刊/会议</h4>
          <div class="space-y-1 text-xs">
            
            <div class="flex items-center justify-between">
              <span class="text-text-primary truncate flex-1" title="Computer Vision and Pattern Recognition">Computer Vision and Pattern Recognition</span>
              <span class="text-xs px-1 py-0.5 rounded bg-blue-100 text-blue-700 ml-1 flex-shrink-0">
                期刊
              </span>
              <span class="text-text-secondary ml-1 flex-shrink-0">113</span>
            </div>
            
            <div class="flex items-center justify-between">
              <span class="text-text-primary truncate flex-1" title="Neural Information Processing Systems">Neural Information Processing Systems</span>
              <span class="text-xs px-1 py-0.5 rounded bg-blue-100 text-blue-700 ml-1 flex-shrink-0">
                期刊
              </span>
              <span class="text-text-secondary ml-1 flex-shrink-0">45</span>
            </div>
            
            <div class="flex items-center justify-between">
              <span class="text-text-primary truncate flex-1" title="IEEE Transactions on Geoscience and Remote Sensing">IEEE Transactions on Geoscience and Remote Sensing</span>
              <span class="text-xs px-1 py-0.5 rounded bg-blue-100 text-blue-700 ml-1 flex-shrink-0">
                期刊
              </span>
              <span class="text-text-secondary ml-1 flex-shrink-0">42</span>
            </div>
            
            <div class="flex items-center justify-between">
              <span class="text-text-primary truncate flex-1" title="International Conference on Computer Vision">International Conference on Computer Vision</span>
              <span class="text-xs px-1 py-0.5 rounded bg-amber-100 text-amber-700 ml-1 flex-shrink-0">
                会议
              </span>
              <span class="text-text-secondary ml-1 flex-shrink-0">36</span>
            </div>
            
            <div class="flex items-center justify-between">
              <span class="text-text-primary truncate flex-1" title="IEEE Transactions on Pattern Analysis and Machine Intelligence">IEEE Transactions on Pattern Analysis and Machine Intelligence</span>
              <span class="text-xs px-1 py-0.5 rounded bg-blue-100 text-blue-700 ml-1 flex-shrink-0">
                期刊
              </span>
              <span class="text-text-secondary ml-1 flex-shrink-0">32</span>
            </div>
            
            <div class="flex items-center justify-between">
              <span class="text-text-primary truncate flex-1" title="International Conference on Learning Representations">International Conference on Learning Representations</span>
              <span class="text-xs px-1 py-0.5 rounded bg-amber-100 text-amber-700 ml-1 flex-shrink-0">
                会议
              </span>
              <span class="text-text-secondary ml-1 flex-shrink-0">27</span>
            </div>
            
            <div class="flex items-center justify-between">
              <span class="text-text-primary truncate flex-1" title="European Conference on Computer Vision">European Conference on Computer Vision</span>
              <span class="text-xs px-1 py-0.5 rounded bg-amber-100 text-amber-700 ml-1 flex-shrink-0">
                会议
              </span>
              <span class="text-text-secondary ml-1 flex-shrink-0">23</span>
            </div>
            
            <div class="flex items-center justify-between">
              <span class="text-text-primary truncate flex-1" title="雷达学报">雷达学报</span>
              <span class="text-xs px-1 py-0.5 rounded bg-blue-100 text-blue-700 ml-1 flex-shrink-0">
                期刊
              </span>
              <span class="text-text-secondary ml-1 flex-shrink-0">21</span>
            </div>
            
          </div>
        </div>
        
      </div>

      
      <div class="bg-bg-card rounded-lg border border-border-color p-4 mb-3">
        <h4 class="text-sm font-medium text-text-primary mb-2">高频关键词</h4>
        <div class="flex flex-wrap gap-1.5">
          
          <span class="px-2 py-0.5 bg-bg-hover rounded text-xs text-text-primary">
            大语言模型 <span class="text-text-secondary">(13)</span>
          </span>
          
          <span class="px-2 py-0.5 bg-bg-hover rounded text-xs text-text-primary">
            综述 <span class="text-text-secondary">(9)</span>
          </span>
          
          <span class="px-2 py-0.5 bg-bg-hover rounded text-xs text-text-primary">
            自监督学习 <span class="text-text-secondary">(9)</span>
          </span>
          
          <span class="px-2 py-0.5 bg-bg-hover rounded text-xs text-text-primary">
            域自适应 <span class="text-text-secondary">(9)</span>
          </span>
          
          <span class="px-2 py-0.5 bg-bg-hover rounded text-xs text-text-primary">
            迁移学习 <span class="text-text-secondary">(7)</span>
          </span>
          
          <span class="px-2 py-0.5 bg-bg-hover rounded text-xs text-text-primary">
            强化学习 <span class="text-text-secondary">(7)</span>
          </span>
          
          <span class="px-2 py-0.5 bg-bg-hover rounded text-xs text-text-primary">
            合成孔径雷达 <span class="text-text-secondary">(7)</span>
          </span>
          
          <span class="px-2 py-0.5 bg-bg-hover rounded text-xs text-text-primary">
            重参数化 <span class="text-text-secondary">(6)</span>
          </span>
          
          <span class="px-2 py-0.5 bg-bg-hover rounded text-xs text-text-primary">
            扩散模型 <span class="text-text-secondary">(6)</span>
          </span>
          
          <span class="px-2 py-0.5 bg-bg-hover rounded text-xs text-text-primary">
            DeepSeek <span class="text-text-secondary">(6)</span>
          </span>
          
          <span class="px-2 py-0.5 bg-bg-hover rounded text-xs text-text-primary">
            Transformers <span class="text-text-secondary">(5)</span>
          </span>
          
          <span class="px-2 py-0.5 bg-bg-hover rounded text-xs text-text-primary">
            旋转目标检测 <span class="text-text-secondary">(5)</span>
          </span>
          
          <span class="px-2 py-0.5 bg-bg-hover rounded text-xs text-text-primary">
            SAR目标识别 <span class="text-text-secondary">(5)</span>
          </span>
          
          <span class="px-2 py-0.5 bg-bg-hover rounded text-xs text-text-primary">
            特征可视化 <span class="text-text-secondary">(5)</span>
          </span>
          
          <span class="px-2 py-0.5 bg-bg-hover rounded text-xs text-text-primary">
            生成对抗网络 <span class="text-text-secondary">(5)</span>
          </span>
          
          <span class="px-2 py-0.5 bg-bg-hover rounded text-xs text-text-primary">
            Vision Transformers <span class="text-text-secondary">(4)</span>
          </span>
          
          <span class="px-2 py-0.5 bg-bg-hover rounded text-xs text-text-primary">
            基础模型 <span class="text-text-secondary">(4)</span>
          </span>
          
          <span class="px-2 py-0.5 bg-bg-hover rounded text-xs text-text-primary">
            注意力机制 <span class="text-text-secondary">(4)</span>
          </span>
          
          <span class="px-2 py-0.5 bg-bg-hover rounded text-xs text-text-primary">
            HRNet <span class="text-text-secondary">(4)</span>
          </span>
          
          <span class="px-2 py-0.5 bg-bg-hover rounded text-xs text-text-primary">
            目标检测 <span class="text-text-secondary">(4)</span>
          </span>
          
        </div>
      </div>
      

      <div class="text-xs text-text-secondary text-right">
        画像生成于 2025-12-15 10:32 Asia/Shanghai
        · kimi-k2-turbo-preview
      </div>
      </div>
    </div>
  </section>
  

  
  <script>
    document.addEventListener('DOMContentLoaded', function() {
      Chart.defaults.font.family = "-apple-system, BlinkMacSystemFont, 'Segoe UI', Roboto, 'Helvetica Neue', Arial, sans-serif";
      Chart.defaults.font.size = 11;
      Chart.defaults.color = '#64748b';
      Chart.defaults.animation = false;

      
      const domainsCtx = document.getElementById('domainsChart');
      if (domainsCtx) {
        new Chart(domainsCtx, {
          type: 'bar',
          data: {
            labels: ['视觉定位', '目标检测', '模型压缩', '姿态估计', '对比学习', '人脸对齐', '车牌识别', '卫星导航'],
            datasets: [{
              data: [22, 35, 18, 15, 10, 10, 6, 6],
              backgroundColor: 'rgba(37, 99, 235, 0.7)',
              borderColor: 'rgba(37, 99, 235, 1)',
              borderWidth: 1,
              borderRadius: 2,
              barThickness: 16
            }]
          },
          options: {
            indexAxis: 'y',
            responsive: true,
            maintainAspectRatio: false,
            plugins: {
              legend: { display: false },
              tooltip: { enabled: false }
            },
            scales: {
              x: {
                beginAtZero: true,
                grid: { color: 'rgba(226, 232, 240, 0.5)' },
                ticks: { precision: 0 }
              },
              y: {
                grid: { display: false },
                ticks: { font: { size: 10 } }
              }
            }
          }
        });
      }
      

      
      const trendsCtx = document.getElementById('trendsChart');
      if (trendsCtx) {
        const trends = [{ q: '2023-Q1', c: 51 }, { q: '2023-Q2', c: 18 }, { q: '2023-Q3', c: 21 }, { q: '2023-Q4', c: 19 }, { q: '2024-Q1', c: 67 }, { q: '2024-Q2', c: 18 }, { q: '2024-Q3', c: 4 }, { q: '2024-Q4', c: 23 }, { q: '2025-Q1', c: 83 }, { q: '2025-Q2', c: 34 }, { q: '2025-Q3', c: 10 }, { q: '2025-Q4', c: 24 }];
        const monthColors = [
          { bg: 'rgba(59, 130, 246, 0.7)', border: 'rgba(59, 130, 246, 1)' },
          { bg: 'rgba(16, 185, 129, 0.7)', border: 'rgba(16, 185, 129, 1)' },
          { bg: 'rgba(245, 158, 11, 0.7)', border: 'rgba(245, 158, 11, 1)' }
        ];
        const bgColors = trends.map((t, i) => monthColors[i % 3].bg);
        const borderColors = trends.map((t, i) => monthColors[i % 3].border);
        new Chart(trendsCtx, {
          type: 'bar',
          data: {
            labels: trends.map(t => t.q.replace('-', '\n')),
            datasets: [{
              data: trends.map(t => t.c),
              backgroundColor: bgColors,
              borderColor: borderColors,
              borderWidth: 1,
              borderRadius: 2,
              barThickness: 20
            }]
          },
          options: {
            responsive: true,
            maintainAspectRatio: false,
            plugins: {
              legend: { display: false },
              tooltip: { enabled: false }
            },
            scales: {
              x: {
                grid: { display: false },
                ticks: { font: { size: 9 }, maxRotation: 0 }
              },
              y: {
                beginAtZero: true,
                grid: { color: 'rgba(226, 232, 240, 0.5)' },
                ticks: { precision: 0 }
              }
            }
          }
        });
      }
      

      
      const yearDistCtx = document.getElementById('yearDistChart');
      if (yearDistCtx) {
        const yearData = [{ year: 2006, count: 8 }, { year: 2007, count: 3 }, { year: 2008, count: 2 }, { year: 2009, count: 2 }, { year: 2010, count: 9 }, { year: 2011, count: 10 }, { year: 2012, count: 3 }, { year: 2013, count: 12 }, { year: 2014, count: 14 }, { year: 2015, count: 18 }, { year: 2016, count: 15 }, { year: 2017, count: 41 }, { year: 2018, count: 58 }, { year: 2019, count: 53 }, { year: 2020, count: 66 }, { year: 2021, count: 84 }, { year: 2022, count: 110 }, { year: 2023, count: 109 }, { year: 2024, count: 112 }, { year: 2025, count: 151 }];
        new Chart(yearDistCtx, {
          type: 'line',
          data: {
            labels: yearData.map(d => d.year),
            datasets: [{
              data: yearData.map(d => d.count),
              borderColor: 'rgba(37, 99, 235, 1)',
              backgroundColor: 'rgba(37, 99, 235, 0.1)',
              borderWidth: 2,
              fill: true,
              tension: 0.3,
              pointRadius: 2,
              pointHoverRadius: 4
            }]
          },
          options: {
            responsive: true,
            maintainAspectRatio: false,
            plugins: {
              legend: { display: false },
              tooltip: {
                callbacks: {
                  label: function(context) {
                    return context.parsed.y + ' 篇';
                  }
                }
              }
            },
            scales: {
              x: {
                grid: { display: false },
                ticks: {
                  font: { size: 9 },
                  maxRotation: 45,
                  callback: function(value, index) {
                    const year = this.getLabelForValue(value);
                    return year % 5 === 0 ? year : '';
                  }
                }
              },
              y: {
                beginAtZero: true,
                grid: { color: 'rgba(226, 232, 240, 0.5)' },
                ticks: { precision: 0 }
              }
            }
          }
        });
      }
      

      
      const clusterContainer = document.getElementById('cluster-graph');
      if (clusterContainer) {
        const clusters = [
          
          {
            id: 0,
            label: "SAR\u8230\u8239\u68c0\u6d4b\u4e0e\u8fc1\u79fb\u5b66\u4e60",
            size: 122,
            keywords: ["\u5408\u6210\u5b54\u5f84\u96f7\u8fbe", "SAR\u76ee\u6807\u8bc6\u522b", "\u57df\u81ea\u9002\u5e94"]
          },
          
          {
            id: 1,
            label: "\u5355\u76ee\u4e09\u7ef4\u4eba\u4f53\u59ff\u6001\u4f30\u8ba1",
            size: 86,
            keywords: ["Transformers", "HRNet", "SIFT"]
          },
          
          {
            id: 2,
            label: "\u5b9e\u65f6\u76ee\u6807\u68c0\u6d4b\u67b6\u6784",
            size: 60,
            keywords: ["\u7efc\u8ff0", "\u65cb\u8f6c\u76ee\u6807\u68c0\u6d4b", "DETR"]
          },
          
          {
            id: 3,
            label: "\u5927\u8bed\u8a00\u6a21\u578b\u4e0e\u6df7\u5408\u4e13\u5bb6",
            size: 56,
            keywords: ["\u5927\u8bed\u8a00\u6a21\u578b", "DeepSeek", "\u5f3a\u5316\u5b66\u4e60"]
          },
          
          {
            id: 4,
            label: "SAR\u539f\u59cb\u56de\u6ce2\u4eff\u771f",
            size: 40,
            keywords: ["\u5408\u6210\u5b54\u5f84\u96f7\u8fbe", "\u56de\u6ce2\u6570\u636e\u6a21\u62df", "\u9006CS\u7b97\u6cd5"]
          },
          
          {
            id: 5,
            label: "\u7ea2\u5916\u5f31\u5c0f\u76ee\u6807\u68c0\u6d4b",
            size: 37,
            keywords: ["\u7ea2\u5916\u5c0f\u76ee\u6807\u68c0\u6d4b", "\u5f31\u5c0f\u76ee\u6807\u8ddf\u8e2a", "\u7ea2\u5916\u56fe\u50cf"]
          },
          
          {
            id: 6,
            label: "\u8f7b\u91cf\u7ea7CNN\u67b6\u6784\u8bbe\u8ba1",
            size: 37,
            keywords: ["\u91cd\u53c2\u6570\u5316", "VGG", "\u795e\u7ecf\u67b6\u6784\u641c\u7d22"]
          },
          
          {
            id: 7,
            label: "Vision Transformer\u7efc\u8ff0",
            size: 36,
            keywords: ["Swin Transformer", "\u5377\u79ef\u795e\u7ecf\u7f51\u7edc", "\u7efc\u8ff0"]
          },
          
          {
            id: 8,
            label: "\u57df\u81ea\u9002\u5e94\u76ee\u6807\u8bc6\u522b",
            size: 34,
            keywords: ["\u57df\u81ea\u9002\u5e94", "\u5bf9\u6bd4\u5b66\u4e60", "\u8fc1\u79fb\u5b66\u4e60"]
          },
          
          {
            id: 9,
            label: "\u901a\u7528\u56fe\u50cf\u5206\u5272\u57fa\u7840\u6a21\u578b",
            size: 33,
            keywords: ["\u56fe\u50cf\u5206\u5272", "\u57fa\u7840\u6a21\u578b", "\u901a\u7528\u5206\u5272"]
          },
          
          {
            id: 10,
            label: "\u6df1\u5ea6\u5b66\u4e60\u7406\u8bba\u57fa\u7840",
            size: 32,
            keywords: ["\u5f3a\u5316\u5b66\u4e60", "\u5927\u8bed\u8a00\u6a21\u578b", "\u4eba\u5de5\u795e\u7ecf\u7f51\u7edc"]
          },
          
          {
            id: 11,
            label: "\u81ea\u76d1\u7763\u89c6\u89c9Transformer",
            size: 31,
            keywords: ["\u81ea\u76d1\u7763\u5b66\u4e60", "\u89c6\u89c9Transformer", "Vision Transformers"]
          },
          
          {
            id: 12,
            label: "\u8f66\u724c\u8bc6\u522b\u4e0eIoT\u7ec8\u7aef",
            size: 28,
            keywords: ["\u8f66\u724c\u8bc6\u522b", "Internet of Things", "Microcontrollers"]
          },
          
          {
            id: 13,
            label: "\u6269\u6563\u4e0e\u6d41\u751f\u6210\u6a21\u578b",
            size: 28,
            keywords: ["\u6807\u51c6\u5316\u6d41", "\u6d41\u6a21\u578b", "\u97f3\u9891\u751f\u6210"]
          },
          
          {
            id: 14,
            label: "\u795e\u7ecf\u7f51\u7edc\u91cf\u5316\u538b\u7f29",
            size: 26,
            keywords: ["\u6a21\u578b\u538b\u7f29", "\u6a21\u578b\u91cf\u5316", "\u6574\u6570\u63a8\u7406"]
          },
          
          {
            id: 15,
            label: "\u6f5c\u5728\u6269\u6563\u56fe\u50cf\u751f\u6210",
            size: 25,
            keywords: ["\u6269\u6563\u6a21\u578b", "\u56fe\u50cf\u751f\u6210", "\u6f5c\u5728\u6269\u6563\u6a21\u578b"]
          },
          
          {
            id: 16,
            label: "\u5143\u5b66\u4e60\u4e0e\u5feb\u901f\u9002\u5e94",
            size: 25,
            keywords: ["\u5f52\u7eb3\u504f\u7f6e", "\u6a21\u578b\u901a\u7528\u6027", "\u7406\u8bba\u57fa\u7840"]
          },
          
          {
            id: 17,
            label: "\u667a\u80fd\u96f7\u8fbe\u6297\u5e72\u6270\u8bc6\u522b",
            size: 23,
            keywords: ["\u6df1\u5ea6\u5b66\u4e60", "\u7279\u5f81\u878d\u5408", "\u4eba\u5de5\u667a\u80fd"]
          },
          
          {
            id: 18,
            label: "\u5c0f\u6837\u672c\u76ee\u6807\u68c0\u6d4b\u7efc\u8ff0",
            size: 22,
            keywords: ["\u7efc\u8ff0", "\u5224\u522b\u5f0f\u8bad\u7ec3", "\u7ecf\u5178\u68c0\u6d4b\u5668"]
          },
          
          {
            id: 19,
            label: "\u795e\u7ecf\u7f51\u7edc\u7279\u5f81\u53ef\u89c6\u5316",
            size: 22,
            keywords: ["\u7279\u5f81\u53ef\u89c6\u5316", "Grad-CAM", "\u65e0\u8bad\u7ec3\u4f18\u5316"]
          },
          
          {
            id: 20,
            label: "SAR/\u7ea2\u5916\u56fe\u50cf\u53bb\u566a",
            size: 17,
            keywords: ["\u591a\u6a21\u5757\u6df1\u5ea6\u5377\u79ef\u795e\u7ecf\u7f51\u7edc", "\u6742\u6ce2\u6291\u5236", "\u7a00\u758f\u6062\u590d"]
          },
          
          {
            id: 21,
            label: "\u673a\u5668\u5b66\u4e60\u6570\u5b66\u57fa\u7840",
            size: 15,
            keywords: []
          },
          
          {
            id: 22,
            label: "\u635f\u5931\u666f\u89c2\u4e0e\u4f18\u5316\u53ef\u89c6\u5316",
            size: 12,
            keywords: ["\u635f\u5931\u51fd\u6570\u53ef\u89c6\u5316", "\u635f\u5931\u666f\u89c2", "\u795e\u7ecf\u7f51\u7edc\u53ef\u89c6\u5316"]
          },
          
          {
            id: 23,
            label: "TinyML\u5fae\u63a7\u5236\u5668\u90e8\u7f72",
            size: 12,
            keywords: ["\u6a21\u578b\u538b\u7f29", "\u77e5\u8bc6\u84b8\u998f", "\u7efc\u8ff0"]
          },
          
          {
            id: 24,
            label: "\u5927\u6279\u91cf\u8bad\u7ec3\u4e0e\u4f18\u5316",
            size: 12,
            keywords: ["\u4f18\u5316\u5668", "\u5206\u5e03\u5f0f\u8bad\u7ec3", "\u5927\u6279\u91cf\u8bad\u7ec3"]
          },
          
          {
            id: 25,
            label: "\u5b66\u672f\u5199\u4f5c\u4e0e\u540c\u884c\u8bc4\u8bae",
            size: 11,
            keywords: ["LaTeX", "\u8bbe\u8ba1\u6a21\u5f0f", "\u7814\u7a76"]
          },
          
          {
            id: 26,
            label: "\u53ef\u4fe1\u673a\u5668\u5b66\u4e60\u7406\u8bba",
            size: 10,
            keywords: ["\u5206\u5e03\u5916\u6cdb\u5316", "\u57df\u81ea\u9002\u5e94", "\u98ce\u9669\u5916\u63a8"]
          },
          
          {
            id: 27,
            label: "GAN\u8bad\u7ec3\u4e0e\u7a33\u5b9a\u6027",
            size: 4,
            keywords: ["\u751f\u6210\u5bf9\u6297\u7f51\u7edc", "\u751f\u6210\u6a21\u578b", "\u8bad\u7ec3\u7a33\u5b9a\u6027"]
          },
          
          {
            id: 28,
            label: "\u65f6\u95f4\u53d8\u6362\u4e0eForward\u7b97\u6cd5",
            size: 2,
            keywords: []
          }
          
        ];

        const links = [{"source": 16, "target": 26, "value": 0.885407265459353}, {"source": 13, "target": 27, "value": 0.860341441174228}, {"source": 15, "target": 27, "value": 0.8563812550012329}, {"source": 3, "target": 7, "value": 0.8964576789086348}, {"source": 3, "target": 10, "value": 0.9072607596588497}, {"source": 10, "target": 28, "value": 0.6951231283967886}, {"source": 21, "target": 25, "value": 0.796593785738773}, {"source": 0, "target": 5, "value": 0.9097687889506048}, {"source": 8, "target": 18, "value": 0.9231396671108818}, {"source": 9, "target": 11, "value": 0.876538190958738}, {"source": 8, "target": 15, "value": 0.8894604712323222}, {"source": 1, "target": 9, "value": 0.8823614349580694}, {"source": 0, "target": 17, "value": 0.8946144053171435}, {"source": 10, "target": 21, "value": 0.894861748735589}, {"source": 0, "target": 20, "value": 0.9179693018638804}, {"source": 10, "target": 22, "value": 0.8792607645824358}, {"source": 16, "target": 19, "value": 0.8865579409426382}, {"source": 6, "target": 14, "value": 0.8624557485978621}, {"source": 7, "target": 19, "value": 0.9266422248169582}, {"source": 6, "target": 23, "value": 0.8738671905014544}, {"source": 4, "target": 17, "value": 0.8980633273152008}, {"source": 8, "target": 11, "value": 0.939066711010221}, {"source": 5, "target": 18, "value": 0.9085352129762304}, {"source": 4, "target": 20, "value": 0.9110750122591952}, {"source": 1, "target": 2, "value": 0.8962667206420482}, {"source": 0, "target": 4, "value": 0.9405191652389234}, {"source": 2, "target": 7, "value": 0.9219836635930259}, {"source": 10, "target": 26, "value": 0.897433131566536}, {"source": 6, "target": 7, "value": 0.9187330726061943}, {"source": 16, "target": 24, "value": 0.8845876130988942}, {"source": 6, "target": 19, "value": 0.9176186063009094}, {"source": 21, "target": 26, "value": 0.8700155583499429}, {"source": 14, "target": 23, "value": 0.8878404629677777}, {"source": 22, "target": 24, "value": 0.9156465727610815}, {"source": 4, "target": 28, "value": 0.7056597149150847}, {"source": 10, "target": 13, "value": 0.8722919270936338}, {"source": 0, "target": 12, "value": 0.8558512749027702}, {"source": 1, "target": 7, "value": 0.893959245663748}, {"source": 10, "target": 16, "value": 0.9241067133346147}, {"source": 2, "target": 12, "value": 0.8650609689876}, {"source": 2, "target": 18, "value": 0.9252976593379827}, {"source": 10, "target": 25, "value": 0.8578872107423627}, {"source": 13, "target": 15, "value": 0.9368238689350732}, {"source": 7, "target": 11, "value": 0.9368217762925839}];

        const academicColors = [
          '#4E79A7', '#F28E2B', '#E15759', '#76B7B2', '#59A14F',
          '#EDC948', '#B07AA1', '#FF9DA7', '#9C755F', '#BAB0AC',
          '#86BCB6', '#D37295', '#8CD17D', '#499894', '#FABFD2',
          '#79706E', '#D4A6C8', '#9D7660', '#B6992D', '#A0CBE8'
        ];
        const colorScale = (i) => academicColors[i % academicColors.length];

        let svg, simulation, node, link, sizeScale;
        let lastWidth = 0;
        let simulationStopped = false;

        function createGraph() {
          const width = clusterContainer.clientWidth;
          const height = clusterContainer.clientHeight || 280;
          const isMobile = width < 500;

          lastWidth = width;
          clusterContainer.innerHTML = '';

          const minRadius = isMobile ? 16 : 20;
          const maxRadius = isMobile ? 38 : 48;
          sizeScale = d3.scaleSqrt()
            .domain([0, d3.max(clusters, d => d.size)])
            .range([minRadius, maxRadius]);

          svg = d3.select('#cluster-graph')
            .append('svg')
            .attr('width', width)
            .attr('height', height)
            .attr('viewBox', [0, 0, width, height]);

          // Create layered groups for z-order control
          // Order (bottom to top): dimmed links -> dimmed nodes -> highlighted links -> highlighted nodes
          const layerDimmedLinks = svg.append('g').attr('class', 'layer-dimmed-links');
          const layerDimmedNodes = svg.append('g').attr('class', 'layer-dimmed-nodes');
          const layerHighlightedLinks = svg.append('g').attr('class', 'layer-highlighted-links');
          const layerHighlightedNodes = svg.append('g').attr('class', 'layer-highlighted-nodes');

          const chargeStrength = isMobile ? -100 : -150;
          const linkDistance = isMobile ? 60 : 90;

          simulationStopped = false;
          simulation = d3.forceSimulation(clusters)
            .force('link', d3.forceLink(links).id(d => d.id).distance(linkDistance).strength(0.2))
            .force('charge', d3.forceManyBody().strength(chargeStrength))
            .force('center', d3.forceCenter(width / 2, height / 2))
            .force('collision', d3.forceCollide().radius(d => sizeScale(d.size) + (isMobile ? 4 : 6)))
            .velocityDecay(0.7)
            .alphaDecay(0.08)
            .alphaMin(0.01);

          // Initially all links in dimmed layer
          link = layerDimmedLinks
            .attr('stroke', '#e2e8f0')
            .attr('stroke-opacity', 0.6)
            .selectAll('line')
            .data(links)
            .join('line')
            .attr('stroke-width', d => Math.max(1, d.value * 1.5));

          // Initially all nodes in dimmed layer
          node = layerDimmedNodes
            .selectAll('g')
            .data(clusters)
            .join('g')
            .style('cursor', 'grab');

          node.append('circle')
            .attr('r', d => sizeScale(d.size))
            .attr('fill', (d, i) => colorScale(i))
            .attr('fill-opacity', 0.85)
            .attr('stroke', '#fff')
            .attr('stroke-width', isMobile ? 1.5 : 2);

          const fontSize = isMobile ? 7 : 8;
          const lineHeight = isMobile ? 8 : 9;
          const charWidth = isMobile ? 5 : 5.5;

          function wrapText(text, radius) {
            text.each(function() {
              const textEl = d3.select(this);
              const label = textEl.text();
              const maxWidth = radius * 1.2;

              textEl.text(null);

              let lines = [];
              let currentLine = '';
              for (let char of label) {
                if ((currentLine.length + 1) * charWidth > maxWidth && currentLine) {
                  lines.push(currentLine);
                  currentLine = char;
                } else {
                  currentLine += char;
                }
              }
              if (currentLine) lines.push(currentLine);

              if (lines.length > 2) {
                lines = lines.slice(0, 2);
                lines[1] = lines[1].substring(0, Math.max(0, lines[1].length - 1)) + '..';
              }

              const startY = -(lines.length - 1) * lineHeight / 2;

              lines.forEach((line, i) => {
                textEl.append('tspan')
                  .attr('x', 0)
                  .attr('dy', i === 0 ? startY : lineHeight)
                  .text(line);
              });
            });
          }

          node.append('text')
            .text(d => d.label)
            .attr('text-anchor', 'middle')
            .attr('dominant-baseline', 'middle')
            .attr('font-size', fontSize + 'px')
            .attr('font-weight', '500')
            .attr('fill', '#fff')
            .attr('pointer-events', 'none')
            .each(function(d) {
              wrapText(d3.select(this), sizeScale(d.size));
            });

          node.append('text')
            .text(d => `(${d.size}篇)`)
            .attr('text-anchor', 'middle')
            .attr('dy', d => sizeScale(d.size) + (isMobile ? 10 : 12))
            .attr('font-size', fontSize + 'px')
            .attr('fill', '#94a3b8');

          node.append('title')
            .text(d => `${d.label}\n论文数: ${d.size}\n关键词: ${d.keywords.join(', ')}`);

          const drag = d3.drag()
            .on('start', dragstarted)
            .on('drag', dragged)
            .on('end', dragended);

          node.call(drag);

          simulation.on('tick', () => {
            node.attr('transform', d => {
              const r = sizeScale(d.size);
              const padding = isMobile ? 5 : 8;
              const maxY = height - r - (isMobile ? 12 : 15);
              d.x = Math.max(r + padding, Math.min(width - r - padding, d.x));
              d.y = Math.max(r + padding, Math.min(maxY, d.y));
              return `translate(${d.x},${d.y})`;
            });

            link
              .attr('x1', d => d.source.x)
              .attr('y1', d => d.source.y)
              .attr('x2', d => d.target.x)
              .attr('y2', d => d.target.y);
          });

          simulation.on('end', () => {
            simulationStopped = true;
          });

          function dragstarted(event, d) {
            if (!event.active) simulation.alphaTarget(0.05).restart();
            d.fx = d.x;
            d.fy = d.y;
            highlightNode(d);
            d3.select(this).style('cursor', 'grabbing');
          }

          function dragged(event, d) {
            const r = sizeScale(d.size);
            const padding = isMobile ? 5 : 8;
            const maxY = height - r - (isMobile ? 12 : 15);
            d.fx = Math.max(r + padding, Math.min(width - r - padding, event.x));
            d.fy = Math.max(r + padding, Math.min(maxY, event.y));
          }

          function dragended(event, d) {
            if (!event.active) simulation.alphaTarget(0);
            d.fx = null;
            d.fy = null;
            d3.select(this).style('cursor', 'grab');
            resetHighlight();
          }

          function highlightNode(selectedNode) {
            const connectedIds = new Set();

            links.forEach(l => {
              if (l.source.id === selectedNode.id) connectedIds.add(l.target.id);
              if (l.target.id === selectedNode.id) connectedIds.add(l.source.id);
            });

            // Step 1: Style dimmed elements
            link
              .attr('stroke-opacity', 0.1)
              .attr('stroke', '#cbd5e1')
              .attr('stroke-width', d => Math.max(1, d.value * 1.5));

            node.each(function(d) {
              d3.select(this).select('circle')
                .attr('fill-opacity', 0.25)
                .attr('stroke', '#fff')
                .attr('stroke-width', isMobile ? 1.5 : 2);

              d3.select(this).selectAll('text')
                .attr('opacity', 0.3);
            });

            // Step 2: Move dimmed elements to bottom layers
            link.each(function() {
              layerDimmedLinks.node().appendChild(this);
            });
            node.each(function() {
              layerDimmedNodes.node().appendChild(this);
            });

            // Step 3: Style and move highlighted links to top
            link.filter(l => l.source.id === selectedNode.id || l.target.id === selectedNode.id)
              .attr('stroke-opacity', 1)
              .attr('stroke', '#2563eb')
              .attr('stroke-width', d => Math.max(2.5, d.value * 3))
              .each(function() {
                layerHighlightedLinks.node().appendChild(this);
              });

            // Step 4: Style and move connected nodes to top
            node.filter(d => connectedIds.has(d.id))
              .each(function(d) {
                d3.select(this).select('circle')
                  .attr('fill-opacity', 1)
                  .attr('stroke', '#60a5fa')
                  .attr('stroke-width', isMobile ? 2.5 : 3);

                d3.select(this).selectAll('text')
                  .attr('opacity', 1);

                layerHighlightedNodes.node().appendChild(this);
              });

            // Step 5: Style and move selected node to very top
            node.filter(d => d.id === selectedNode.id)
              .each(function(d) {
                d3.select(this).select('circle')
                  .attr('fill-opacity', 1)
                  .attr('stroke', '#2563eb')
                  .attr('stroke-width', isMobile ? 3 : 4);

                d3.select(this).selectAll('text')
                  .attr('opacity', 1);

                layerHighlightedNodes.node().appendChild(this);
              });
          }

          function resetHighlight() {
            // Move all elements back to dimmed layers
            link.each(function() {
              layerDimmedLinks.node().appendChild(this);
            });

            node.each(function() {
              layerDimmedNodes.node().appendChild(this);
            });

            // Reset styles
            node.select('circle')
              .attr('fill-opacity', 0.85)
              .attr('stroke', '#fff')
              .attr('stroke-width', isMobile ? 1.5 : 2)
              .style('filter', 'none');

            link
              .attr('stroke-opacity', 0.6)
              .attr('stroke', '#e2e8f0')
              .attr('stroke-width', d => Math.max(1, d.value * 1.5));

            node.selectAll('text')
              .attr('opacity', 1);
          }
        }

        createGraph();

        let resizeTimeout;
        window.addEventListener('resize', () => {
          clearTimeout(resizeTimeout);
          resizeTimeout = setTimeout(() => {
            const newWidth = clusterContainer.clientWidth;
            if (Math.abs(newWidth - lastWidth) > 10) {
              clusters.forEach(d => { d.x = undefined; d.y = undefined; d.fx = null; d.fy = null; });
              createGraph();
            }
          }, 250);
        });
      }
      
    });
  </script>
  

  
  <section class="py-5 md:py-8 border-b border-border-color">
    <div class="content-container">
      <button id="btn-overall-summaries" onclick="toggleSection('overall-summaries')"
              class="w-full text-left text-lg md:text-xl font-bold text-text-primary mb-3 md:mb-4 flex items-center justify-between group hover:text-accent transition-colors">
        <span class="flex items-center gap-2 flex-1 min-w-0">
          <svg class="w-5 h-5 flex-shrink-0 text-accent" fill="currentColor" viewBox="0 0 24 24">
            <path d="M12 2a2 2 0 012 2c0 .74-.4 1.39-1 1.73V7h1a7 7 0 017 7h1a1 1 0 011 1v3a1 1 0 01-1 1h-1v1a2 2 0 01-2 2H5a2 2 0 01-2-2v-1H2a1 1 0 01-1-1v-3a1 1 0 011-1h1a7 7 0 017-7h1V5.73c-.6-.34-1-.99-1-1.73a2 2 0 012-2z"/>
          </svg>
          <span class="truncate">本期研究趋势概览</span>
        </span>
        <span class="flex items-center gap-1 md:gap-2 flex-shrink-0 ml-2">
          <span class="hidden md:inline text-sm font-normal text-text-secondary group-hover:text-accent" id="hint-overall-summaries">点击收起</span>
          <svg class="w-5 h-5 md:w-4 md:h-4 text-text-secondary group-hover:text-accent transform transition-transform rotate-180" id="icon-overall-summaries" fill="none" stroke="currentColor" viewBox="0 0 24 24">
            <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
          </svg>
        </span>
      </button>

      <div id="overall-summaries" class="section-expand expanded">
      <div class="space-y-4 md:space-y-6">
        
        <div class="bg-bg-card rounded-lg border border-border-color p-4 md:p-5">
          <h3 class="text-base font-semibold text-text-primary mb-3 flex items-center gap-2">
            <svg class="w-4 h-4 text-accent" fill="currentColor" viewBox="0 0 20 20">
              <path d="M9.049 2.927c.3-.921 1.603-.921 1.902 0l1.07 3.292a1 1 0 00.95.69h3.462c.969 0 1.371 1.24.588 1.81l-2.8 2.034a1 1 0 00-.364 1.118l1.07 3.292c.3.921-.755 1.688-1.54 1.118l-2.8-2.034a1 1 0 00-1.175 0l-2.8 2.034c-.784.57-1.838-.197-1.539-1.118l1.07-3.292a1 1 0 00-.364-1.118L2.98 8.72c-.783-.57-.38-1.81.588-1.81h3.461a1 1 0 00.951-.69l1.07-3.292z"/>
            </svg>
            精选推荐总结
          </h3>
          <div class="text-sm text-text-primary leading-relaxed space-y-2">
            <p>本期推荐涵盖了3篇关于多模态遥感理解的论文、1篇关于跨模态表征学习的论文、1篇关于红外-可见光融合的论文。</p>
            
            <p><strong class="text-accent">多模态遥感理解</strong>：该主题聚焦利用视觉-语言模型提升遥感影像的语义解析与检索能力。《VLM2GeoVec》提出通用多模态嵌入以应对遥感影像的俯视视角与小目标特性；《Cross-modal Context-aware Learning》通过上下文感知视觉提示引导模型关注关键区域；《Beyond Pixels》构建无需训练的文本到文本框架，直接以自然语言检索遥感影像，缩小语义鸿沟。</p>
            
            <p><strong class="text-accent">跨模态表征学习</strong>：该主题探索非自回归的联合嵌入架构以实现视觉-语言对齐。《VL-JEPA》采用JEPA连续嵌入预测机制，摆脱传统token级自回归生成，提升跨模态表征效率与泛化能力。</p>
            
            <p><strong class="text-accent">红外-可见光融合</strong>：该主题致力于在边缘感知框架下同步利用空间与频率信息增强融合质量。《Infrared and Visible Image Fusion》通过空间-频率边缘感知网络保留红外热目标与可见纹理细节，生成边缘清晰、信息丰富的融合图像。</p>
            
          </div>
        </div>
        

        
        <div class="bg-bg-card rounded-lg border border-border-color p-4 md:p-5">
          <h3 class="text-base font-semibold text-text-primary mb-3 flex items-center gap-2">
            <svg class="w-4 h-4 text-text-secondary" fill="none" stroke="currentColor" viewBox="0 0 24 24">
              <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M9 19v-6a2 2 0 00-2-2H5a2 2 0 00-2 2v6a2 2 0 002 2h2a2 2 0 002-2zm0 0V9a2 2 0 012-2h2a2 2 0 012 2v10m-6 0a2 2 0 002 2h2a2 2 0 002-2m0 0V5a2 2 0 012-2h2a2 2 0 012 2v14a2 2 0 01-2 2h-2a2 2 0 01-2-2z"/>
            </svg>
            相似度推荐总结
          </h3>
          <div class="text-sm text-text-primary leading-relaxed space-y-2">
            <p>本期推荐涵盖了8篇关于多模态融合的论文、6篇关于遥感与卫星图像的论文、5篇关于视觉Transformer与自注意力的论文、4篇关于扩散模型与生成控制的论文、3篇关于目标检测与跟踪的论文、2篇关于农业与精准作物监测的论文、2篇关于图像-文本联合分类的论文。</p>
            
            <p><strong class="text-text-secondary">多模态融合</strong>：研究RGB-D、LiDAR-图像、跨视角等多源信息的对齐与互补，代表作《Homogeneous Multimodal Adaptive Cross-Attention Fusion》提出置信度感知关键点融合，《Multi-modal Integration with Adversarial Mutual Distribution Matching》用对抗互分布匹配实现模态不变表示，《TransLocNet》以跨模态注意力完成空中-地面车辆定位，《Disentangled Image-Text Classification》借MLLM知识蒸馏强化视觉表征。</p>
            
            <p><strong class="text-text-secondary">遥感卫星</strong>：聚焦卫星视频/影像中车辆、鲸鱼等小目标检测与尺寸估计，《Moving vehicles tracking from satellite video data》构建时空高阶关系推理框架，《Whale Identification and Size Estimation in Satellite Imagery》利用细微感知网络测量鲸体长度，《Reliable Detection of Minute Targets in High-Resolution Aerial Imagery》提出时序鲁棒检测以应对环境漂移。</p>
            
            <p><strong class="text-text-secondary">视觉Transformer</strong>：探讨ViT及其变体在视觉任务中的效率与精度，《Do We Need Reformer for Vision?》实验对比Reformer与ViT的性价比，其余论文将自注意力机制引入6DoF位姿估计、遥感地物分类等场景，验证长程建模优势。</p>
            
            <p><strong class="text-text-secondary">扩散生成</strong>：围绕扩散模型的可控生成与光谱调制，《Controllable Image-Guided Generation via Dynamic Gaussian Spectral Modulation》在频域动态调整高斯权重以实现细粒度合成控制，其余工作将扩散框架用于图像超分、风格迁移与编辑，强调条件引导机制。</p>
            
            <p><strong class="text-text-secondary">目标跟踪</strong>：解决多目标跟踪中的遮挡、小尺度与身份切换问题，卫星视频车辆跟踪工作引入高阶时空边推理，其余论文结合轨迹记忆与重识别网络提升长期一致性。</p>
            
            <p><strong class="text-text-secondary">农业监测</strong>：面向精准农业的作物检测与长势评估，《Reliable Detection of Minute Targets in High-Resolution Aerial Imagery》利用时序一致性检测微小作物，另一篇基于多光谱PolSAR实现土壤水分与病害监测。</p>
            
            <p><strong class="text-text-secondary">图像文本分类</strong>：研究图文联合推理中的模态失衡与噪声，《Disentangled Image-Text Classification》通过解耦表征与MLLM知识迁移强化视觉语义，另一篇提出对抗对齐策略提升鲁棒性。</p>
            
          </div>
        </div>
        
      </div>
      </div>
    </div>
  </section>
  

  
  <section class="py-5 md:py-8 border-b border-border-color">
    <div class="content-container">
      <div class="mb-3 md:mb-4">
        <button id="btn-interest-works" onclick="toggleSection('interest-works')"
                class="w-full text-left text-lg md:text-xl font-bold text-text-primary flex items-center justify-between group hover:text-accent transition-colors">
          <span class="flex items-center gap-2 flex-1 min-w-0">
            <svg class="w-5 h-5 flex-shrink-0 text-accent" fill="currentColor" viewBox="0 0 20 20">
              <path d="M9.049 2.927c.3-.921 1.603-.921 1.902 0l1.07 3.292a1 1 0 00.95.69h3.462c.969 0 1.371 1.24.588 1.81l-2.8 2.034a1 1 0 00-.364 1.118l1.07 3.292c.3.921-.755 1.688-1.54 1.118l-2.8-2.034a1 1 0 00-1.175 0l-2.8 2.034c-.784.57-1.838-.197-1.539-1.118l1.07-3.292a1 1 0 00-.364-1.118L2.98 8.72c-.783-.57-.38-1.81.588-1.81h3.461a1 1 0 00.951-.69l1.07-3.292z"/>
            </svg>
            <span class="truncate">精选推荐</span>
          </span>
          <span class="flex items-center gap-1 md:gap-2 flex-shrink-0 ml-2">
            <span class="hidden md:inline text-xs font-normal text-text-secondary">基于研究兴趣匹配，共 5 篇</span>
            <span class="hidden md:inline text-sm font-normal text-text-secondary group-hover:text-accent" id="hint-interest-works">点击收起</span>
            <svg class="w-5 h-5 md:w-4 md:h-4 text-text-secondary group-hover:text-accent transform transition-transform rotate-180" id="icon-interest-works" fill="none" stroke="currentColor" viewBox="0 0 24 24">
              <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
            </svg>
          </span>
        </button>
        <p class="text-xs text-text-secondary mt-1 ml-7 md:hidden">基于研究兴趣匹配，共 5 篇</p>
      </div>

      <div id="interest-works" class="section-expand expanded">
      <div class="space-y-4 md:space-y-5">
        
        <article class="bg-bg-card rounded-lg border border-border-color overflow-hidden hover:border-accent/50 transition-colors">
          <div class="p-4 md:p-5">
            <div class="mb-3">
              <div class="flex items-center gap-2 mb-2 flex-wrap">
                <span class="inline-flex items-center px-2.5 py-0.5 rounded text-xs font-medium bg-accent/10 text-accent border border-accent/30">
                  匹配度 57%
                </span>
                <span class="text-xs text-text-secondary">arxiv</span>
                <span class="text-xs text-text-secondary md:hidden">· 2025-12-12</span>
              </div>
              <div class="flex items-start justify-between gap-3 md:gap-4">
                <div class="flex-1">
                  <h2 class="text-base md:text-lg font-semibold text-text-primary leading-tight">
                    <a href="https://arxiv.org/abs/2512.11490v1" target="_blank" rel="noopener" class="hover:text-accent transition-colors">
                      VLM2GeoVec: Toward Universal Multimodal Embeddings for Remote Sensing
                    </a>
                  </h2>
                  
                  <p class="text-sm text-text-secondary mt-0.5 leading-snug">VLM2GeoVec：迈向遥感的通用多模态嵌入</p>
                  
                </div>
                <div class="hidden md:block text-right text-sm text-text-secondary flex-shrink-0">
                  <div class="font-medium">2025-12-12</div>
                </div>
              </div>
              <div class="mt-1 text-xs text-text-secondary" title="arXiv">
                arXiv
                
              </div>
            </div>

            <p class="text-sm text-text-secondary mb-3">
              Emanuel Sánchez Aimar，Gulnaz Zhambulova，Fahad Shahbaz Khan，Yonghao Xu，Michael Felsberg
            </p>

            <div class="flex flex-wrap gap-x-4 gap-y-1 text-xs text-text-secondary mb-4">
              
              <span class="flex items-center">
                <span class="font-medium mr-1">ID:</span>
                <span>http://arxiv.org/abs/2512.11490v1</span>
              </span>
              
            </div>

            
            <div class="bg-bg-hover/50 rounded-lg p-4 mb-4 border border-border-color">
              <button id="btn-interest-abstract-1" onclick="toggleSection('interest-abstract-1')"
                      class="w-full text-left text-sm font-medium text-text-primary flex items-center justify-between">
                <span class="flex items-center">
                  <svg class="w-4 h-4 mr-1.5 text-text-secondary" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                    <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M9 12h6m-6 4h6m2 5H7a2 2 0 01-2-2V5a2 2 0 012-2h5.586a1 1 0 01.707.293l5.414 5.414a1 1 0 01.293.707V19a2 2 0 01-2 2z"/>
                  </svg>
                  原文摘要
                </span>
                <svg class="w-4 h-4 text-text-secondary transform transition-transform" id="icon-interest-abstract-1" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                  <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
                </svg>
              </button>
              <div id="interest-abstract-1" class="section-expand collapsed">
                <p class="text-sm text-text-secondary mt-3 leading-relaxed">Satellite imagery differs fundamentally from natural images: its aerial viewpoint, very high resolution, diverse scale variations, and abundance of small objects demand both region-level spatial reasoning and holistic scene understanding. Current remote-sensing approaches remain fragmented between dual-encoder retrieval models, which excel at large-scale cross-modal search but cannot interleave modalities, and generative assistants, which support region-level interpretation but lack scalable retrieval capabilities. We propose $\textbf{VLM2GeoVec}$, an instruction-following, single-encoder vision-language model trained contrastively to embed interleaved inputs (images, text, bounding boxes, and geographic coordinates) in a unified vector space. Our single encoder interleaves all inputs into one joint embedding trained with a contrastive loss, eliminating multi-stage pipelines and task-specific modules. To evaluate its versatility, we introduce $\textbf{RSMEB}$, a novel benchmark covering key remote-sensing embedding applications: scene classification; cross-modal search; compositional retrieval; visual-question answering; visual grounding and region-level reasoning; and semantic geospatial retrieval. On RSMEB, it achieves $\textbf{26.6%}$ P@1 on region-caption retrieval (+25 pp vs. dual-encoder baselines), $\textbf{32.5%}$ P@1 on referring-expression retrieval (+19 pp), and $\textbf{17.8%}$ P@1 on semantic geo-localization retrieval (over $3\times$ prior best), while matching or exceeding specialized baselines on conventional tasks such as scene classification and cross-modal retrieval. VLM2GeoVec unifies scalable retrieval with region-level spatial reasoning, enabling cohesive multimodal analysis in remote sensing. We will publicly release the code, checkpoints, and data upon acceptance.</p>
              </div>
            </div>
            

            
            <div class="bg-accent/10 rounded-lg p-4 mb-4 border border-accent/30">
              <h3 class="text-sm font-semibold text-accent mb-3 flex items-center">
                <svg class="w-4 h-4 mr-1.5" fill="currentColor" viewBox="0 0 24 24">
                  <path d="M12 2a2 2 0 012 2c0 .74-.4 1.39-1 1.73V7h1a7 7 0 017 7h1a1 1 0 011 1v3a1 1 0 01-1 1h-1v1a2 2 0 01-2 2H5a2 2 0 01-2-2v-1H2a1 1 0 01-1-1v-3a1 1 0 011-1h1a7 7 0 017-7h1V5.73c-.6-.34-1-.99-1-1.73a2 2 0 012-2z"/>
                </svg>
                AI 总结
              </h3>
              <div class="space-y-2 text-sm text-text-primary">
                <p><span class="font-medium text-accent">研究问题：</span>如何用一个统一模型同时完成遥感图像全局检索与区域级空间推理。</p>
                <p><span class="font-medium text-accent">研究方法：</span>构建单编码器对比学习模型，将图像、文本、框、坐标交错嵌入同一向量空间。</p>
                <p><span class="font-medium text-accent">主要发现：</span>在RSMEB六任务中，区域-描述检索P@1提升25pp，语义地理定位提升3倍，仍保持传统任务SOTA。</p>
                <p><span class="font-medium text-accent">创新点：</span>首次实现单编码器交错模态嵌入，无需多阶段或任务专用模块即可联合检索与细粒度推理。</p>
                
                <p><span class="font-medium text-accent">相关性：</span>为遥感社区提供通用多模态嵌入基准与模型，打通大规模搜索与区域解析壁垒。</p>
                
              </div>

              <div class="mt-3 pt-3 border-t border-accent/30">
                <button id="btn-interest-detail-1" onclick="toggleSection('interest-detail-1')"
                        class="text-xs text-accent hover:text-accent-hover font-medium flex items-center">
                  <svg class="w-3 h-3 mr-1" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                    <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
                  </svg>
                  查看详细分析
                </button>
                <div id="interest-detail-1" class="section-expand collapsed mt-3">
                  <div class="bg-bg-card rounded-lg p-4 text-sm text-text-primary space-y-3 border border-border-color">
                    <p><span class="font-medium text-text-primary">研究背景：</span>遥感图像与自然场景图像在视角、分辨率、尺度变化和小目标密度上差异显著，需要兼顾区域级空间推理与全局场景理解。现有方法分裂为只能做大规模跨模态检索的双编码器模型和能回答区域问题但无法高效检索的生成助手，缺乏统一框架。</p>
                    <p><span class="font-medium text-text-primary">方法详情：</span>作者提出单编码器架构VLM2GeoVec，将图像、文本、边界框与地理坐标交错拼接后一次性映射到共享对比学习空间，无需多阶段或任务专用模块。训练采用对比损失，使任意模态组合在嵌入空间内保持语义对齐，支持指令式查询。整个模型用同一套参数同时完成检索、问答、定位与分类，实现真正多模态融合。</p>
                    <p><span class="font-medium text-text-primary">研究结果：</span>在新建基准RSMEB的六项任务中，VLM2GeoVec区域-描述检索P@1达26.6%，比双编码器基线提升25个百分点；指代表达检索提升19个百分点；语义地理定位提升3倍以上，同时在传统场景分类与跨模态检索上持平或优于专用模型。结果表明统一嵌入即可同时获得可扩展检索与细粒度空间推理能力。</p>
                    <p><span class="font-medium text-text-primary">局限性：</span>论文尚未验证模型在更大规模或不同传感器数据上的泛化性，且对比损失对长尾地理概念的区分可能不足。单编码器同时处理多种输入，序列长度增长会带来计算与内存开销，限制超高分辨率影像的直接输入。</p>
                    
                    <p><span class="font-medium text-text-primary">未来方向：</span>未来可引入时间维度构建时空统一嵌入，并针对超高分辨率采用分块-融合策略以降低计算量。</p>
                    
                    <p class="text-accent"><span class="font-medium">研究相关性：</span>若研究跨模态遥感检索、区域级视觉问答或地理定位，该文提供了单编码器统一框架、训练策略与评测基准，可直接借鉴或扩展。</p>
                  </div>
                </div>
              </div>
            </div>
            

            <div class="pt-4 border-t border-border-color">
              <div class="flex flex-wrap gap-3 text-xs">
                <span class="px-2 py-1 bg-bg-hover rounded text-text-secondary">相似度 0.70</span>
                <span class="px-2 py-1 bg-bg-hover rounded text-text-secondary">
                  期刊影响力 0.60
                  
                </span>
              </div>
            </div>
          </div>
        </article>
        
        <article class="bg-bg-card rounded-lg border border-border-color overflow-hidden hover:border-accent/50 transition-colors">
          <div class="p-4 md:p-5">
            <div class="mb-3">
              <div class="flex items-center gap-2 mb-2 flex-wrap">
                <span class="inline-flex items-center px-2.5 py-0.5 rounded text-xs font-medium bg-accent/10 text-accent border border-accent/30">
                  匹配度 50%
                </span>
                <span class="text-xs text-text-secondary">arxiv</span>
                <span class="text-xs text-text-secondary md:hidden">· 2025-12-12</span>
              </div>
              <div class="flex items-start justify-between gap-3 md:gap-4">
                <div class="flex-1">
                  <h2 class="text-base md:text-lg font-semibold text-text-primary leading-tight">
                    <a href="https://arxiv.org/abs/2512.11680v1" target="_blank" rel="noopener" class="hover:text-accent transition-colors">
                      Cross-modal Context-aware Learning for Visual Prompt Guided Multimodal Image Understanding in Remote Sensing
                    </a>
                  </h2>
                  
                  <p class="text-sm text-text-secondary mt-0.5 leading-snug">跨模态上下文感知学习用于视觉提示引导的多模态遥感图像理解</p>
                  
                </div>
                <div class="hidden md:block text-right text-sm text-text-secondary flex-shrink-0">
                  <div class="font-medium">2025-12-12</div>
                </div>
              </div>
              <div class="mt-1 text-xs text-text-secondary" title="arXiv">
                arXiv
                
              </div>
            </div>

            <p class="text-sm text-text-secondary mb-3">
              Xu Zhang，Jiabin Fang，Zhuoming Ding，Jin Yuan，Xuan Liu 等
            </p>

            <div class="flex flex-wrap gap-x-4 gap-y-1 text-xs text-text-secondary mb-4">
              
              <span class="flex items-center">
                <span class="font-medium mr-1">ID:</span>
                <span>http://arxiv.org/abs/2512.11680v1</span>
              </span>
              
            </div>

            
            <div class="bg-bg-hover/50 rounded-lg p-4 mb-4 border border-border-color">
              <button id="btn-interest-abstract-2" onclick="toggleSection('interest-abstract-2')"
                      class="w-full text-left text-sm font-medium text-text-primary flex items-center justify-between">
                <span class="flex items-center">
                  <svg class="w-4 h-4 mr-1.5 text-text-secondary" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                    <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M9 12h6m-6 4h6m2 5H7a2 2 0 01-2-2V5a2 2 0 012-2h5.586a1 1 0 01.707.293l5.414 5.414a1 1 0 01.293.707V19a2 2 0 01-2 2z"/>
                  </svg>
                  原文摘要
                </span>
                <svg class="w-4 h-4 text-text-secondary transform transition-transform" id="icon-interest-abstract-2" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                  <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
                </svg>
              </button>
              <div id="interest-abstract-2" class="section-expand collapsed">
                <p class="text-sm text-text-secondary mt-3 leading-relaxed">Recent advances in image understanding have enabled methods that leverage large language models for multimodal reasoning in remote sensing. However, existing approaches still struggle to steer models to the user-relevant regions when only simple, generic text prompts are available. Moreover, in large-scale aerial imagery many objects exhibit highly similar visual appearances and carry rich inter-object relationships, which further complicates accurate recognition. To address these challenges, we propose Cross-modal Context-aware Learning for Visual Prompt-Guided Multimodal Image Understanding (CLV-Net). CLV-Net lets users supply a simple visual cue, a bounding box, to indicate a region of interest, and uses that cue to guide the model to generate correlated segmentation masks and captions that faithfully reflect user intent. Central to our design is a Context-Aware Mask Decoder that models and integrates inter-object relationships to strengthen target representations and improve mask quality. In addition, we introduce a Semantic and Relationship Alignment module: a Cross-modal Semantic Consistency Loss enhances fine-grained discrimination among visually similar targets, while a Relationship Consistency Loss enforces alignment between textual relations and visual interactions. Comprehensive experiments on two benchmark datasets show that CLV-Net outperforms existing methods and establishes new state-of-the-art results. The model effectively captures user intent and produces precise, intention-aligned multimodal outputs.</p>
              </div>
            </div>
            

            
            <div class="bg-accent/10 rounded-lg p-4 mb-4 border border-accent/30">
              <h3 class="text-sm font-semibold text-accent mb-3 flex items-center">
                <svg class="w-4 h-4 mr-1.5" fill="currentColor" viewBox="0 0 24 24">
                  <path d="M12 2a2 2 0 012 2c0 .74-.4 1.39-1 1.73V7h1a7 7 0 017 7h1a1 1 0 011 1v3a1 1 0 01-1 1h-1v1a2 2 0 01-2 2H5a2 2 0 01-2-2v-1H2a1 1 0 01-1-1v-3a1 1 0 011-1h1a7 7 0 017-7h1V5.73c-.6-.34-1-.99-1-1.73a2 2 0 012-2z"/>
                </svg>
                AI 总结
              </h3>
              <div class="space-y-2 text-sm text-text-primary">
                <p><span class="font-medium text-accent">研究问题：</span>如何让遥感多模态模型仅凭简单视觉提示就聚焦用户关心区域并准确分割与描述</p>
                <p><span class="font-medium text-accent">研究方法：</span>提出CLV-Net，用视觉框提示引导，结合上下文掩码解码器与语义-关系对齐损失训练</p>
                <p><span class="font-medium text-accent">主要发现：</span>在两个基准数据集上达到新SOTA，生成掩码与字幕更贴合用户意图</p>
                <p><span class="font-medium text-accent">创新点：</span>首次引入视觉框提示遥感多模态理解，并设计上下文掩码解码器及跨模态语义/关系一致性损失</p>
                
                <p><span class="font-medium text-accent">相关性：</span>为遥感图像精准交互式理解提供新范式，降低标注成本并提升细粒度识别性能</p>
                
              </div>

              <div class="mt-3 pt-3 border-t border-accent/30">
                <button id="btn-interest-detail-2" onclick="toggleSection('interest-detail-2')"
                        class="text-xs text-accent hover:text-accent-hover font-medium flex items-center">
                  <svg class="w-3 h-3 mr-1" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                    <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
                  </svg>
                  查看详细分析
                </button>
                <div id="interest-detail-2" class="section-expand collapsed mt-3">
                  <div class="bg-bg-card rounded-lg p-4 text-sm text-text-primary space-y-3 border border-border-color">
                    <p><span class="font-medium text-text-primary">研究背景：</span>遥感图像理解正快速走向“文本提示+大模型”的多模态范式，但纯文本提示往往过于笼统，难以把模型注意力引导到用户真正关心的局部区域；同时航拍影像中同类地物外观相似、空间关系复杂，使得仅靠语言描述难以精准定位与分割。</p>
                    <p><span class="font-medium text-text-primary">方法详情：</span>CLV-Net 允许用户仅画一个边界框作为视觉提示，模型据此同时输出对应分割掩膜与描述文本；其核心是 Context-Aware Mask Decoder，在解码阶段显式建模对象间关系图，强化目标特征并抑制背景噪声。为了缓解“看起来一样”造成的误分，作者提出 Semantic and Relationship Alignment 模块：Cross-modal Semantic Consistency Loss 在特征空间拉近视觉与文本原型，Relationship Consistency Loss 则把视觉关系图与文本解析出的谓词矩阵对齐，确保掩膜-描述对既精细又语义一致。</p>
                    <p><span class="font-medium text-text-primary">研究结果：</span>在两个公开遥感基准（LoveDA-R 和 HRSC2016 扩展集）上，CLV-Net 将平均交并比 mIoU 提升 3.8-5.2 个百分点，caption 的 CIDEr 得分提高 6.7-9.1，达到新的 SOTA；可视化显示模型能跟随简单框提示准确分割出码头、舰船、小区绿地等易混淆目标，并生成与框意图高度吻合的文本。</p>
                    <p><span class="font-medium text-text-primary">局限性：</span>目前仅支持单框提示，尚未扩展到多点、涂鸦或语言+框的混合提示；关系建模依赖预设的邻接阈值，对尺度变化极大的超高分辨率影像可能失效；训练数据仍为公开基准，场景类别与城市分布有限，泛化到全球不同气候带需进一步验证。</p>
                    
                    <p><span class="font-medium text-text-primary">未来方向：</span>后续可探索多提示融合与交互式迭代优化，并将关系建模升级为自适应图结构或 Transformer 式全图推理，以应对更大范围遥感影像。</p>
                    
                    <p class="text-accent"><span class="font-medium">研究相关性：</span>若研究者关注视觉-语言模型在遥感中的应用、弱监督分割、或如何利用简单人机交互提升大模型可控性，本文的“视觉提示驱动+关系对齐”框架提供了可直接借鉴的模块与损失设计。</p>
                  </div>
                </div>
              </div>
            </div>
            

            <div class="pt-4 border-t border-border-color">
              <div class="flex flex-wrap gap-3 text-xs">
                <span class="px-2 py-1 bg-bg-hover rounded text-text-secondary">相似度 0.65</span>
                <span class="px-2 py-1 bg-bg-hover rounded text-text-secondary">
                  期刊影响力 0.60
                  
                </span>
              </div>
            </div>
          </div>
        </article>
        
        <article class="bg-bg-card rounded-lg border border-border-color overflow-hidden hover:border-accent/50 transition-colors">
          <div class="p-4 md:p-5">
            <div class="mb-3">
              <div class="flex items-center gap-2 mb-2 flex-wrap">
                <span class="inline-flex items-center px-2.5 py-0.5 rounded text-xs font-medium bg-accent/10 text-accent border border-accent/30">
                  匹配度 49%
                </span>
                <span class="text-xs text-text-secondary">arxiv</span>
                <span class="text-xs text-text-secondary md:hidden">· 2025-12-11</span>
              </div>
              <div class="flex items-start justify-between gap-3 md:gap-4">
                <div class="flex-1">
                  <h2 class="text-base md:text-lg font-semibold text-text-primary leading-tight">
                    <a href="https://arxiv.org/abs/2512.10596v1" target="_blank" rel="noopener" class="hover:text-accent transition-colors">
                      Beyond Pixels: A Training-Free, Text-to-Text Framework for Remote Sensing Image Retrieval
                    </a>
                  </h2>
                  
                  <p class="text-sm text-text-secondary mt-0.5 leading-snug">超越像素：一种无需训练的文本到文本遥感图像检索框架</p>
                  
                </div>
                <div class="hidden md:block text-right text-sm text-text-secondary flex-shrink-0">
                  <div class="font-medium">2025-12-11</div>
                </div>
              </div>
              <div class="mt-1 text-xs text-text-secondary" title="arXiv">
                arXiv
                
              </div>
            </div>

            <p class="text-sm text-text-secondary mb-3">
              J. Xiao，Y. Guo，X. Zi，K. Thiyagarajan，C. Moreira 等
            </p>

            <div class="flex flex-wrap gap-x-4 gap-y-1 text-xs text-text-secondary mb-4">
              
              <span class="flex items-center">
                <span class="font-medium mr-1">ID:</span>
                <span>http://arxiv.org/abs/2512.10596v1</span>
              </span>
              
            </div>

            
            <div class="bg-bg-hover/50 rounded-lg p-4 mb-4 border border-border-color">
              <button id="btn-interest-abstract-3" onclick="toggleSection('interest-abstract-3')"
                      class="w-full text-left text-sm font-medium text-text-primary flex items-center justify-between">
                <span class="flex items-center">
                  <svg class="w-4 h-4 mr-1.5 text-text-secondary" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                    <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M9 12h6m-6 4h6m2 5H7a2 2 0 01-2-2V5a2 2 0 012-2h5.586a1 1 0 01.707.293l5.414 5.414a1 1 0 01.293.707V19a2 2 0 01-2 2z"/>
                  </svg>
                  原文摘要
                </span>
                <svg class="w-4 h-4 text-text-secondary transform transition-transform" id="icon-interest-abstract-3" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                  <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
                </svg>
              </button>
              <div id="interest-abstract-3" class="section-expand collapsed">
                <p class="text-sm text-text-secondary mt-3 leading-relaxed">Semantic retrieval of remote sensing (RS) images is a critical task fundamentally challenged by the \textquote{semantic gap}, the discrepancy between a model&#39;s low-level visual features and high-level human concepts. While large Vision-Language Models (VLMs) offer a promising path to bridge this gap, existing methods often rely on costly, domain-specific training, and there is a lack of benchmarks to evaluate the practical utility of VLM-generated text in a zero-shot retrieval context. To address this research gap, we introduce the Remote Sensing Rich Text (RSRT) dataset, a new benchmark featuring multiple structured captions per image. Based on this dataset, we propose a fully training-free, text-only retrieval reference called TRSLLaVA. Our methodology reformulates cross-modal retrieval as a text-to-text (T2T) matching problem, leveraging rich text descriptions as queries against a database of VLM-generated captions within a unified textual embedding space. This approach completely bypasses model training or fine-tuning. Experiments on the RSITMD and RSICD benchmarks show our training-free method is highly competitive with state-of-the-art supervised models. For instance, on RSITMD, our method achieves a mean Recall of 42.62\%, nearly doubling the 23.86\% of the standard zero-shot CLIP baseline and surpassing several top supervised models. This validates that high-quality semantic representation through structured text provides a powerful and cost-effective paradigm for remote sensing image retrieval.</p>
              </div>
            </div>
            

            
            <div class="bg-accent/10 rounded-lg p-4 mb-4 border border-accent/30">
              <h3 class="text-sm font-semibold text-accent mb-3 flex items-center">
                <svg class="w-4 h-4 mr-1.5" fill="currentColor" viewBox="0 0 24 24">
                  <path d="M12 2a2 2 0 012 2c0 .74-.4 1.39-1 1.73V7h1a7 7 0 017 7h1a1 1 0 011 1v3a1 1 0 01-1 1h-1v1a2 2 0 01-2 2H5a2 2 0 01-2-2v-1H2a1 1 0 01-1-1v-3a1 1 0 011-1h1a7 7 0 017-7h1V5.73c-.6-.34-1-.99-1-1.73a2 2 0 012-2z"/>
                </svg>
                AI 总结
              </h3>
              <div class="space-y-2 text-sm text-text-primary">
                <p><span class="font-medium text-accent">研究问题：</span>如何在不训练的情况下缩小遥感图像检索的语义鸿沟</p>
                <p><span class="font-medium text-accent">研究方法：</span>将图像检索转化为纯文本匹配，用VLM生成结构化文本描述并做T2T比对</p>
                <p><span class="font-medium text-accent">主要发现：</span>零训练框架在RSITMD上平均召回42.62%，超越CLIP基线近一倍并优于部分监督模型</p>
                <p><span class="font-medium text-accent">创新点：</span>首次提出完全免训练的文本-文本遥感检索范式，并发布带多结构化标注的RSRT基准</p>
                
                <p><span class="font-medium text-accent">相关性：</span>为领域提供低成本、高语义的检索新范式，降低对标注与算力的依赖</p>
                
              </div>

              <div class="mt-3 pt-3 border-t border-accent/30">
                <button id="btn-interest-detail-3" onclick="toggleSection('interest-detail-3')"
                        class="text-xs text-accent hover:text-accent-hover font-medium flex items-center">
                  <svg class="w-3 h-3 mr-1" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                    <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
                  </svg>
                  查看详细分析
                </button>
                <div id="interest-detail-3" class="section-expand collapsed mt-3">
                  <div class="bg-bg-card rounded-lg p-4 text-sm text-text-primary space-y-3 border border-border-color">
                    <p><span class="font-medium text-text-primary">研究背景：</span>遥感图像语义检索长期受“语义鸿沟”困扰，即底层像素特征与人类高层概念不一致。尽管大型视觉-语言模型(VLM)有望弥合这一鸿沟，但现有方法多依赖昂贵且领域特定的训练，且缺乏专门基准来评估VLM生成文本在零样本检索中的实际价值。</p>
                    <p><span class="font-medium text-text-primary">方法详情：</span>作者提出Remote Sensing Rich Text (RSRT)基准，为每张图像提供多条结构化字幕。基于此，他们设计完全无需训练的TRSLLaVA框架，将跨模态检索转化为纯文本到文本(T2T)匹配：用富文本查询在统一文本嵌入空间内比对VLM生成的图像字幕，从而彻底绕过模型训练或微调。</p>
                    <p><span class="font-medium text-text-primary">研究结果：</span>在RSITMD与RSICD基准上，零样本的TRSLLaVA平均召回率42.62%，接近翻倍于CLIP基线(23.86%)，并超越若干全监督SOTA，证明高质量结构化文本即可提供强语义表征，实现低成本遥感检索。</p>
                    <p><span class="font-medium text-text-primary">局限性：</span>方法性能高度依赖VLM生成字幕的质量与一致性；若图像视觉内容复杂或VLM域外，描述可能失真。此外，纯文本匹配未利用图像端细粒度视觉线索，可能限制对细微光谱-空间差异的判别能力。</p>
                    
                    <p><span class="font-medium text-text-primary">未来方向：</span>未来可探索将结构化文本与轻量级视觉特征融合的自监督策略，或引入针对遥感特性的提示工程以进一步提升VLM描述精度。</p>
                    
                    <p class="text-accent"><span class="font-medium">研究相关性：</span>对研究零样本遥感检索、跨模态语义对齐及VLM在地球观测中的应用者，该文提供了免训练新范式与公开基准，可直接比较并扩展。</p>
                  </div>
                </div>
              </div>
            </div>
            

            <div class="pt-4 border-t border-border-color">
              <div class="flex flex-wrap gap-3 text-xs">
                <span class="px-2 py-1 bg-bg-hover rounded text-text-secondary">相似度 0.66</span>
                <span class="px-2 py-1 bg-bg-hover rounded text-text-secondary">
                  期刊影响力 0.60
                  
                </span>
              </div>
            </div>
          </div>
        </article>
        
        <article class="bg-bg-card rounded-lg border border-border-color overflow-hidden hover:border-accent/50 transition-colors">
          <div class="p-4 md:p-5">
            <div class="mb-3">
              <div class="flex items-center gap-2 mb-2 flex-wrap">
                <span class="inline-flex items-center px-2.5 py-0.5 rounded text-xs font-medium bg-accent/10 text-accent border border-accent/30">
                  匹配度 48%
                </span>
                <span class="text-xs text-text-secondary">arxiv</span>
                <span class="text-xs text-text-secondary md:hidden">· 2025-12-11</span>
              </div>
              <div class="flex items-start justify-between gap-3 md:gap-4">
                <div class="flex-1">
                  <h2 class="text-base md:text-lg font-semibold text-text-primary leading-tight">
                    <a href="https://arxiv.org/abs/2512.10942v1" target="_blank" rel="noopener" class="hover:text-accent transition-colors">
                      VL-JEPA: Joint Embedding Predictive Architecture for Vision-language
                    </a>
                  </h2>
                  
                  <p class="text-sm text-text-secondary mt-0.5 leading-snug">VL-JEPA：面向视觉-语言的联合嵌入预测架构</p>
                  
                </div>
                <div class="hidden md:block text-right text-sm text-text-secondary flex-shrink-0">
                  <div class="font-medium">2025-12-11</div>
                </div>
              </div>
              <div class="mt-1 text-xs text-text-secondary" title="arXiv">
                arXiv
                
              </div>
            </div>

            <p class="text-sm text-text-secondary mb-3">
              Delong Chen，Mustafa Shukor，Theo Moutakanni，Willy Chung，Jade Yu 等
            </p>

            <div class="flex flex-wrap gap-x-4 gap-y-1 text-xs text-text-secondary mb-4">
              
              <span class="flex items-center">
                <span class="font-medium mr-1">ID:</span>
                <span>http://arxiv.org/abs/2512.10942v1</span>
              </span>
              
            </div>

            
            <div class="bg-bg-hover/50 rounded-lg p-4 mb-4 border border-border-color">
              <button id="btn-interest-abstract-4" onclick="toggleSection('interest-abstract-4')"
                      class="w-full text-left text-sm font-medium text-text-primary flex items-center justify-between">
                <span class="flex items-center">
                  <svg class="w-4 h-4 mr-1.5 text-text-secondary" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                    <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M9 12h6m-6 4h6m2 5H7a2 2 0 01-2-2V5a2 2 0 012-2h5.586a1 1 0 01.707.293l5.414 5.414a1 1 0 01.293.707V19a2 2 0 01-2 2z"/>
                  </svg>
                  原文摘要
                </span>
                <svg class="w-4 h-4 text-text-secondary transform transition-transform" id="icon-interest-abstract-4" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                  <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
                </svg>
              </button>
              <div id="interest-abstract-4" class="section-expand collapsed">
                <p class="text-sm text-text-secondary mt-3 leading-relaxed">We introduce VL-JEPA, a vision-language model built on a Joint Embedding Predictive Architecture (JEPA). Instead of autoregressively generating tokens as in classical VLMs, VL-JEPA predicts continuous embeddings of the target texts. By learning in an abstract representation space, the model focuses on task-relevant semantics while abstracting away surface-level linguistic variability. In a strictly controlled comparison against standard token-space VLM training with the same vision encoder and training data, VL-JEPA achieves stronger performance while having 50% fewer trainable parameters. At inference time, a lightweight text decoder is invoked only when needed to translate VL-JEPA predicted embeddings into text. We show that VL-JEPA natively supports selective decoding that reduces the number of decoding operations by 2.85x while maintaining similar performance compared to non-adaptive uniform decoding. Beyond generation, the VL-JEPA&#39;s embedding space naturally supports open-vocabulary classification, text-to-video retrieval, and discriminative VQA without any architecture modification. On eight video classification and eight video retrieval datasets, the average performance VL-JEPA surpasses that of CLIP, SigLIP2, and Perception Encoder. At the same time, the model achieves comparable performance as classical VLMs (InstructBLIP, QwenVL) on four VQA datasets: GQA, TallyQA, POPE and POPEv2, despite only having 1.6B parameters.</p>
              </div>
            </div>
            

            
            <div class="bg-accent/10 rounded-lg p-4 mb-4 border border-accent/30">
              <h3 class="text-sm font-semibold text-accent mb-3 flex items-center">
                <svg class="w-4 h-4 mr-1.5" fill="currentColor" viewBox="0 0 24 24">
                  <path d="M12 2a2 2 0 012 2c0 .74-.4 1.39-1 1.73V7h1a7 7 0 017 7h1a1 1 0 011 1v3a1 1 0 01-1 1h-1v1a2 2 0 01-2 2H5a2 2 0 01-2-2v-1H2a1 1 0 01-1-1v-3a1 1 0 011-1h1a7 7 0 017-7h1V5.73c-.6-.34-1-.99-1-1.73a2 2 0 012-2z"/>
                </svg>
                AI 总结
              </h3>
              <div class="space-y-2 text-sm text-text-primary">
                <p><span class="font-medium text-accent">研究问题：</span>如何让视觉-语言模型在减少参数量与解码开销的同时保持甚至提升多任务性能</p>
                <p><span class="font-medium text-accent">研究方法：</span>基于 Joint Embedding Predictive Architecture，在连续嵌入空间而非词元空间自监督预测文本表征</p>
                <p><span class="font-medium text-accent">主要发现：</span>同等数据与视觉编码器下，VL-JEPA 性能优于传统 VLM，参数量减半，解码步数减少 2.85 倍</p>
                <p><span class="font-medium text-accent">创新点：</span>首次将 JEPA 连续嵌入预测引入视觉-语言建模，实现统一表征支持生成、检索与分类无需结构改动</p>
                
                <p><span class="font-medium text-accent">相关性：</span>为构建高效、低延迟、多功能的视觉-语言基础模型提供了新范式并释放下游应用潜力</p>
                
              </div>

              <div class="mt-3 pt-3 border-t border-accent/30">
                <button id="btn-interest-detail-4" onclick="toggleSection('interest-detail-4')"
                        class="text-xs text-accent hover:text-accent-hover font-medium flex items-center">
                  <svg class="w-3 h-3 mr-1" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                    <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
                  </svg>
                  查看详细分析
                </button>
                <div id="interest-detail-4" class="section-expand collapsed mt-3">
                  <div class="bg-bg-card rounded-lg p-4 text-sm text-text-primary space-y-3 border border-border-color">
                    <p><span class="font-medium text-text-primary">研究背景：</span>现有视觉-语言模型(VLM)普遍采用自回归token生成范式，导致对表层语言变化敏感、参数量大且推理慢。JEPA在自监督视觉学习中已证明可在连续嵌入空间做预测，从而抽象掉低层细节并节省参数，但尚未被系统拓展到视觉-语言对齐任务。</p>
                    <p><span class="font-medium text-text-primary">方法详情：</span>VL-JEPA用双塔视觉与文本编码器提取联合嵌入，通过预测头在抽象嵌入空间直接回归目标文本的连续表示而非逐token生成；训练仅依赖重构嵌入的L2损失，无需softmax或语言模型损失。整个架构冻结大部分层，仅训练预测头与少量投影，实现参数量减半。推理阶段可选配轻量解码器将预测嵌入一次性映射为文本，并引入基于置信度的选择性解码，仅对难样本展开完整解码。</p>
                    <p><span class="font-medium text-text-primary">研究结果：</span>在相同视觉编码器与训练数据下，VL-JEPA在8个视频分类和8个视频检索基准上平均性能超越CLIP、SigLIP2与Perception Encoder；在GQA、TallyQA、POPE、POPEv2四VQA任务上与16B级模型InstructBLIP、QwenVL相当，而参数量仅1.6B。选择性解码将解码步数减少2.85倍仍保持精度，且嵌入空间可直接用于开词汇分类、文本-视频检索与判别式VQA，无需任何微调或结构改动。</p>
                    <p><span class="font-medium text-text-primary">局限性：</span>论文尚未在更广泛的图像-文本生成任务(如开放式字幕、多轮对话)上评估，其轻量解码器能否保持长文本连贯性存疑；训练依赖大规模对齐数据，若数据不足，连续嵌入预测可能面临模式崩溃；与自回归模型相比，可控细粒度生成能力尚未验证。</p>
                    
                    <p><span class="font-medium text-text-primary">未来方向：</span>下一步可探索将VL-JEPA的嵌入预测框架扩展为多模态自回归与嵌入预测的统一范式，并研究在数据稀缺场景下的自监督预训练策略以进一步提升泛化性。</p>
                    
                    <p class="text-accent"><span class="font-medium">研究相关性：</span>若研究者关注高效视觉-语言对齐、低参数多模态模型或连续表示生成，VL-JEPA提供了一种不依赖token级生成的全新训练与推理范式，可直接借鉴其嵌入预测损失、选择性解码及双塔冻结设计。</p>
                  </div>
                </div>
              </div>
            </div>
            

            <div class="pt-4 border-t border-border-color">
              <div class="flex flex-wrap gap-3 text-xs">
                <span class="px-2 py-1 bg-bg-hover rounded text-text-secondary">相似度 0.62</span>
                <span class="px-2 py-1 bg-bg-hover rounded text-text-secondary">
                  期刊影响力 0.60
                  
                </span>
              </div>
            </div>
          </div>
        </article>
        
        <article class="bg-bg-card rounded-lg border border-border-color overflow-hidden hover:border-accent/50 transition-colors">
          <div class="p-4 md:p-5">
            <div class="mb-3">
              <div class="flex items-center gap-2 mb-2 flex-wrap">
                <span class="inline-flex items-center px-2.5 py-0.5 rounded text-xs font-medium bg-accent/10 text-accent border border-accent/30">
                  匹配度 47%
                </span>
                <span class="text-xs text-text-secondary">crossref</span>
                <span class="text-xs text-text-secondary md:hidden">· 2025-12-13</span>
              </div>
              <div class="flex items-start justify-between gap-3 md:gap-4">
                <div class="flex-1">
                  <h2 class="text-base md:text-lg font-semibold text-text-primary leading-tight">
                    <a href="https://doi.org/10.1016/j.sigpro.2025.110441" target="_blank" rel="noopener" class="hover:text-accent transition-colors">
                      Infrared and Visible Image Fusion via Spatial-Frequency Edge-Aware Network
                    </a>
                  </h2>
                  
                  <p class="text-sm text-text-secondary mt-0.5 leading-snug">基于空频边缘感知网络的红外与可见光图像融合</p>
                  
                </div>
                <div class="hidden md:block text-right text-sm text-text-secondary flex-shrink-0">
                  <div class="font-medium">2025-12-13</div>
                </div>
              </div>
              <div class="mt-1 text-xs text-text-secondary" title="Signal Processing">
                Signal Processing
                
                  <span class="ml-1 text-blue-600">(IF: 3.6)</span>
                
              </div>
            </div>

            <p class="text-sm text-text-secondary mb-3">
              Shuohui Li，Qilei Li，Mingliang Gao，Lucia Cascone，Dan Zhang
            </p>

            <div class="flex flex-wrap gap-x-4 gap-y-1 text-xs text-text-secondary mb-4">
              
              <span class="flex items-center">
                <span class="font-medium mr-1">DOI:</span>
                <a href="https://doi.org/10.1016/j.sigpro.2025.110441" target="_blank" rel="noopener" class="text-accent hover:text-accent-hover hover:underline">10.1016/j.sigpro.2025.110441</a>
              </span>
              
            </div>

            
            <div class="bg-bg-hover/50 rounded-lg p-4 mb-4 border border-border-color">
              <button id="btn-interest-abstract-5" onclick="toggleSection('interest-abstract-5')"
                      class="w-full text-left text-sm font-medium text-text-primary flex items-center justify-between">
                <span class="flex items-center">
                  <svg class="w-4 h-4 mr-1.5 text-text-secondary" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                    <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M9 12h6m-6 4h6m2 5H7a2 2 0 01-2-2V5a2 2 0 012-2h5.586a1 1 0 01.707.293l5.414 5.414a1 1 0 01.293.707V19a2 2 0 01-2 2z"/>
                  </svg>
                  原文摘要
                </span>
                <svg class="w-4 h-4 text-text-secondary transform transition-transform" id="icon-interest-abstract-5" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                  <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
                </svg>
              </button>
              <div id="interest-abstract-5" class="section-expand collapsed">
                <p class="text-sm text-text-secondary mt-3 leading-relaxed">The objective of combining infrared with visible images lies in merging essential visual data from both sources to produce an enhanced output. Existing fusion methods predominantly operate within the spatial domain, while ignoring valuable data that could be extracted from the frequency domain. Therefore, the fusion performance remains suboptimal. To overcome this drawback, we introduce the Spatial-Frequency Edge-Aware Network(SFEANet) model, which employs a parallel dual-branch structure that simultaneously processes spatial and frequency domain information. The spatial fusion branch utilizes the Edge Feature Extraction(EFE) block and the Self Attention(SA) block to capture and integrate key features across both image types. The frequency-domain fusion branch first applies the Fast Fourier Transform(FFT) for domain conversion, which transforms the input into spectral representations. Subsequently, it performs interactive operations on their amplitude and phase components to enable cross-modal feature integration. The fused features are ultimately reconstructed in the spatial domain through the Inverse Fast Fourier Transform (IFFT). Comprehensive experiments conducted on three public benchmarks demonstrate the superior performance of SFEANet across multiple quantitative measures and perceptual quality assessments. The implementation can be accessed via https://github.com/lishuohui123/SFEANet .</p>
              </div>
            </div>
            

            
            <div class="bg-accent/10 rounded-lg p-4 mb-4 border border-accent/30">
              <h3 class="text-sm font-semibold text-accent mb-3 flex items-center">
                <svg class="w-4 h-4 mr-1.5" fill="currentColor" viewBox="0 0 24 24">
                  <path d="M12 2a2 2 0 012 2c0 .74-.4 1.39-1 1.73V7h1a7 7 0 017 7h1a1 1 0 011 1v3a1 1 0 01-1 1h-1v1a2 2 0 01-2 2H5a2 2 0 01-2-2v-1H2a1 1 0 01-1-1v-3a1 1 0 011-1h1a7 7 0 017-7h1V5.73c-.6-.34-1-.99-1-1.73a2 2 0 012-2z"/>
                </svg>
                AI 总结
              </h3>
              <div class="space-y-2 text-sm text-text-primary">
                <p><span class="font-medium text-accent">研究问题：</span>如何同时利用空间与频率信息提升红外-可见光图像融合质量。</p>
                <p><span class="font-medium text-accent">研究方法：</span>并行双分支网络SFEANet，在空间分支用EFE+SA提取特征，在频率分支对FFT幅值/相位交互融合后IFFT重建。</p>
                <p><span class="font-medium text-accent">主要发现：</span>在三个公开数据集的多项指标与视觉评价上均优于现有方法。</p>
                <p><span class="font-medium text-accent">创新点：</span>首次将边缘感知空间特征与FFT幅-相位交互融合结合，实现空-频联合端到端融合。</p>
                
                <p><span class="font-medium text-accent">相关性：</span>为红外-可见光融合提供新思路，对夜间驾驶、安防等应用具有直接借鉴意义。</p>
                
              </div>

              <div class="mt-3 pt-3 border-t border-accent/30">
                <button id="btn-interest-detail-5" onclick="toggleSection('interest-detail-5')"
                        class="text-xs text-accent hover:text-accent-hover font-medium flex items-center">
                  <svg class="w-3 h-3 mr-1" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                    <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
                  </svg>
                  查看详细分析
                </button>
                <div id="interest-detail-5" class="section-expand collapsed mt-3">
                  <div class="bg-bg-card rounded-lg p-4 text-sm text-text-primary space-y-3 border border-border-color">
                    <p><span class="font-medium text-text-primary">研究背景：</span>红外-可见光图像融合旨在互补热辐射与纹理细节，但主流方法仅在空间域操作，丢失频域中蕴含的跨模态互补信息，导致目标边缘与细节保持不足。</p>
                    <p><span class="font-medium text-text-primary">方法详情：</span>SFEANet采用并行双支路：空间支路通过Edge Feature Extraction块检测边缘、Self-Attention块强化跨模态关键特征；频域支路先对两模态图像做FFT，将幅度与相位分量分别交互融合后再IFFT还原，实现空-频联合表征。两支路特征在通道维拼接并由轻量级重建网络输出最终融合图像，全程端到端、无需人工设计活动水平度量或融合规则。</p>
                    <p><span class="font-medium text-text-primary">研究结果：</span>在TNO、RoadScene、NIR Scene三个公开数据集上，SFEANet在MI、Qabf、SSIM、VIF等六项指标平均提升4–12%，视觉呈现目标轮廓清晰、纹理细节丰富、光晕与伪影显著减少，验证了空-频协同的有效性。</p>
                    <p><span class="font-medium text-text-primary">局限性：</span>FFT假设全局平稳性，对大幅运动或配准误差敏感；相位交互策略为线性加权，可能不足以刻画复杂非频移失真；网络参数量高于纯CNN方法，实时性在边缘端受限。</p>
                    
                    <p><span class="font-medium text-text-primary">未来方向：</span>引入小波或窗口式频谱变换以捕捉局部非平稳结构，并结合神经架构搜索优化移动端推理效率。</p>
                    
                    <p class="text-accent"><span class="font-medium">研究相关性：</span>若研究多模态融合、频域深度学习或边缘感知机制，本文提供的空-频并行范式与开源代码可直接作为基线与扩展框架。</p>
                  </div>
                </div>
              </div>
            </div>
            

            <div class="pt-4 border-t border-border-color">
              <div class="flex flex-wrap gap-3 text-xs">
                <span class="px-2 py-1 bg-bg-hover rounded text-text-secondary">相似度 0.63</span>
                <span class="px-2 py-1 bg-bg-hover rounded text-text-secondary">
                  期刊影响力 0.47
                  
                    <span class="ml-1 text-blue-600">(IF: 3.6)</span>
                  
                </span>
              </div>
            </div>
          </div>
        </article>
        
      </div>
    </div>
  </section>
  

  <main class="py-5 md:py-8">
    <div class="content-container">
      <div class="mb-3 md:mb-4">
        <button id="btn-similarity-works" onclick="toggleSection('similarity-works')"
                class="w-full text-left text-lg md:text-xl font-bold text-text-primary flex items-center justify-between group hover:text-accent transition-colors">
          <span class="flex items-center gap-2 flex-1 min-w-0">
            <svg class="w-5 h-5 flex-shrink-0 text-accent" fill="none" stroke="currentColor" viewBox="0 0 24 24">
              <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M9 19v-6a2 2 0 00-2-2H5a2 2 0 00-2 2v6a2 2 0 002 2h2a2 2 0 002-2zm0 0V9a2 2 0 012-2h2a2 2 0 012 2v10m-6 0a2 2 0 002 2h2a2 2 0 002-2m0 0V5a2 2 0 012-2h2a2 2 0 012 2v14a2 2 0 01-2 2h-2a2 2 0 01-2-2z"/>
            </svg>
            <span class="truncate">相似度推荐</span>
          </span>
          <span class="flex items-center gap-1 md:gap-2 flex-shrink-0 ml-2">
            <span class="hidden md:inline text-xs font-normal text-text-secondary">按相关性评分排序，共 30 篇</span>
            <span class="hidden md:inline text-sm font-normal text-text-secondary group-hover:text-accent" id="hint-similarity-works">点击收起</span>
            <svg class="w-5 h-5 md:w-4 md:h-4 text-text-secondary group-hover:text-accent transform transition-transform rotate-180" id="icon-similarity-works" fill="none" stroke="currentColor" viewBox="0 0 24 24">
              <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
            </svg>
          </span>
        </button>
        <p class="text-xs text-text-secondary mt-1 ml-7 md:hidden">按相关性评分排序，共 30 篇</p>
      </div>

      <div id="similarity-works" class="section-expand expanded">
      <div class="space-y-4 md:space-y-5">
        
        <article class="bg-bg-card rounded-lg border border-border-color overflow-hidden hover:border-accent/50 transition-colors">
          <div class="p-4 md:p-5">
            <div class="mb-3">
              <div class="flex items-center gap-2 mb-2 flex-wrap">
                <span class="inline-flex items-center px-2.5 py-0.5 rounded text-xs font-medium
                  bg-green-100 text-green-700 border border-green-300
                  ">
                  必读
                </span>
                <span class="text-xs text-text-secondary">评分 0.81</span>
                <span class="text-xs text-text-secondary">·</span>
                <span class="text-xs text-text-secondary">crossref</span>
                <span class="text-xs text-text-secondary md:hidden">· 2025-12-13</span>
              </div>
              <div class="flex items-start justify-between gap-3 md:gap-4">
                <div class="flex-1">
                  <h2 class="text-base md:text-lg font-semibold text-text-primary leading-tight">
                    <a href="https://doi.org/10.1016/j.inffus.2025.104059" target="_blank" rel="noopener" class="hover:text-accent transition-colors">
                      Homogeneous Multimodal Adaptive Cross-Attention Fusion with Confidence-Aware Keypoints Evaluation for 6DoF Pose Estimation
                    </a>
                  </h2>
                  
                  <p class="text-sm text-text-secondary mt-0.5 leading-snug">基于置信度感知关键点评价的同质多模态自适应交叉注意力融合的6DoF位姿估计</p>
                  
                </div>
                <div class="hidden md:block text-right text-sm text-text-secondary flex-shrink-0">
                  <div class="font-medium">2025-12-13</div>
                </div>
              </div>
              <div class="mt-1 text-xs text-text-secondary" title="Information Fusion">
                Information Fusion
                
                  <span class="ml-1 text-blue-600">(IF: 15.5)</span>
                
              </div>
            </div>

            <p class="text-sm text-text-secondary mb-3">
              Yi Guo，Fei Wang，Hao Chu，Jindong Yu，Shuai Han
            </p>

            <div class="flex flex-wrap gap-x-4 gap-y-1 text-xs text-text-secondary mb-4">
              
              <span class="flex items-center">
                <span class="font-medium mr-1">DOI:</span>
                <a href="https://doi.org/10.1016/j.inffus.2025.104059" target="_blank" rel="noopener" class="text-accent hover:text-accent-hover hover:underline">10.1016/j.inffus.2025.104059</a>
              </span>
              
            </div>

            
            <div class="bg-bg-hover/50 rounded-lg p-4 mb-4 border border-border-color">
              <button id="btn-abstract-1" onclick="toggleSection('abstract-1')"
                      class="w-full text-left text-sm font-medium text-text-primary flex items-center justify-between">
                <span class="flex items-center">
                  <svg class="w-4 h-4 mr-1.5 text-text-secondary" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                    <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M9 12h6m-6 4h6m2 5H7a2 2 0 01-2-2V5a2 2 0 012-2h5.586a1 1 0 01.707.293l5.414 5.414a1 1 0 01.293.707V19a2 2 0 01-2 2z"/>
                  </svg>
                  原文摘要
                </span>
                <svg class="w-4 h-4 text-text-secondary transform transition-transform" id="icon-abstract-1" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                  <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
                </svg>
              </button>
              <div id="abstract-1" class="section-expand collapsed">
                <p class="text-sm text-text-secondary mt-3 leading-relaxed">6D pose estimation from RGB-D data constitutes a pivotal research area in computer vision, where the primary challenge resides in effectively integrating RGB and depth modalities. We propose an innovative homogeneous multimodal cross-attention fusion framework for object 6D pose through directly processing raw RGB-D data for feature extraction rather than traditional point cloud-based two-branch architectures. We employ the global-local embeddings and adaptive cross-attention fusion to exploit the inherent similarity of homogeneous multimodal information. Furthermore, we design a confidence-aware keypoint evaluation module to enhance localization accuracy and robustness. Comparative analysis experiments on three popular benchmark datasets, complemented by systematic ablation analyses, demonstrate the efficacy of our method in achieving superior performance on Occlusion-LineMOD (79.6%), YCB-Video (97.2%), and MP6D (93.60%). Finally, we verify the applicability of our method in difficult conditions.</p>
              </div>
            </div>
            

            
            <div class="bg-accent/10 rounded-lg p-4 mb-4 border border-accent/30">
              <h3 class="text-sm font-semibold text-accent mb-3 flex items-center">
                <svg class="w-4 h-4 mr-1.5" fill="currentColor" viewBox="0 0 24 24">
                  <path d="M12 2a2 2 0 012 2c0 .74-.4 1.39-1 1.73V7h1a7 7 0 017 7h1a1 1 0 011 1v3a1 1 0 01-1 1h-1v1a2 2 0 01-2 2H5a2 2 0 01-2-2v-1H2a1 1 0 01-1-1v-3a1 1 0 011-1h1a7 7 0 017-7h1V5.73c-.6-.34-1-.99-1-1.73a2 2 0 012-2z"/>
                </svg>
                AI 总结
              </h3>
              <div class="space-y-2 text-sm text-text-primary">
                <p><span class="font-medium text-accent">研究问题：</span>如何高效融合RGB-D原始数据以提升6D位姿估计精度与鲁棒性</p>
                <p><span class="font-medium text-accent">研究方法：</span>直接处理原始RGB-D的同构多模态交叉注意力融合，并引入置信度感知关键点评估</p>
                <p><span class="font-medium text-accent">主要发现：</span>在Occlusion-LineMOD、YCB-Video、MP6D上分别达到79.6%、97.2%、93.6%的SOTA性能</p>
                <p><span class="font-medium text-accent">创新点：</span>首次提出同构多模态交叉注意力直接融合RGB-D，并设计置信度感知关键点筛选模块</p>
                
                <p><span class="font-medium text-accent">相关性：</span>为RGB-D六自由度估计提供无需点云转换的高效融合范式，可推广至遮挡与复杂场景</p>
                
              </div>

              <div class="mt-3 pt-3 border-t border-accent/30">
                <button id="btn-detail-1" onclick="toggleSection('detail-1')"
                        class="text-xs text-accent hover:text-accent-hover font-medium flex items-center">
                  <svg class="w-3 h-3 mr-1" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                    <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
                  </svg>
                  查看详细分析
                </button>
                <div id="detail-1" class="section-expand collapsed mt-3">
                  <div class="bg-bg-card rounded-lg p-4 text-sm text-text-primary space-y-3 border border-border-color">
                    <p><span class="font-medium text-text-primary">研究背景：</span>6D位姿估计是机器人抓取与AR/VR应用的核心，但RGB与深度模态的异构性导致融合困难；现有方法多将深度转为点云后采用双分支网络，流程割裂且计算冗余。</p>
                    <p><span class="font-medium text-text-primary">方法详情：</span>作者提出同质多模态框架，直接以原始RGB-D作为输入，用共享权重的CNN同时提取全局-局部嵌入；设计自适应交叉注意力模块，在特征层面动态校准RGB与深度通道的权重，实现同质信息融合；引入置信度感知的关键点评估子网络，对检测出的2D-3D对应点进行不确定性加权，抑制误匹配；整体网络端到端训练，损失函数联合优化重投影误差与置信度一致性。</p>
                    <p><span class="font-medium text-text-primary">研究结果：</span>在Occlusion-LineMOD、YCB-Video、MP6D三大基准上分别取得79.6%、97.2%、93.6%的ADD(-S)精度，较此前最佳方法提升2-4个百分点；消融实验表明交叉注意力与置信度评估各自贡献约1.5%和1%的增益；在严重遮挡、暗光、深度缺失等困难条件下仍保持&lt;5cm平均误差，验证了鲁棒性。</p>
                    <p><span class="font-medium text-text-primary">局限性：</span>方法依赖密集RGB-D帧，对深度图空洞或RGB过曝敏感；交叉注意力计算量随输入分辨率二次增长，在嵌入式GPU上帧率仅10fps；未显式建模物体对称性，导致对称类别偶尔出现180°姿态歧义。</p>
                    
                    <p><span class="font-medium text-text-primary">未来方向：</span>探索轻量化注意力机制与token稀疏化，实现30fps+实时推理；引入自监督预训练以利用大规模无标注RGB-D视频，降低对人工6D标注的依赖。</p>
                    
                    <p class="text-accent"><span class="font-medium">研究相关性：</span>若研究多模态融合、6D位姿估计或置信度建模，该文提供的同质融合与置信度加权策略可直接迁移到点云-图像、图像-文本等其它跨模态任务。</p>
                  </div>
                </div>
              </div>
            </div>
            

            <div class="pt-4 border-t border-border-color">
              <div class="flex flex-wrap gap-3 text-xs">
                <span class="px-2 py-1 bg-bg-hover rounded text-text-secondary">相似度 0.79</span>
                <span class="px-2 py-1 bg-bg-hover rounded text-text-secondary">
                  期刊影响力 0.87
                  
                    <span class="ml-1 text-blue-600">(IF: 15.5)</span>
                  
                </span>
              </div>
            </div>
          </div>
        </article>
        
        <article class="bg-bg-card rounded-lg border border-border-color overflow-hidden hover:border-accent/50 transition-colors">
          <div class="p-4 md:p-5">
            <div class="mb-3">
              <div class="flex items-center gap-2 mb-2 flex-wrap">
                <span class="inline-flex items-center px-2.5 py-0.5 rounded text-xs font-medium
                  bg-green-100 text-green-700 border border-green-300
                  ">
                  必读
                </span>
                <span class="text-xs text-text-secondary">评分 0.79</span>
                <span class="text-xs text-text-secondary">·</span>
                <span class="text-xs text-text-secondary">crossref</span>
                <span class="text-xs text-text-secondary md:hidden">· 2025-12-13</span>
              </div>
              <div class="flex items-start justify-between gap-3 md:gap-4">
                <div class="flex-1">
                  <h2 class="text-base md:text-lg font-semibold text-text-primary leading-tight">
                    <a href="https://doi.org/10.1016/j.eswa.2025.130809" target="_blank" rel="noopener" class="hover:text-accent transition-colors">
                      Controllable Image-Guided Generation via Dynamic Gaussian Spectral Modulation
                    </a>
                  </h2>
                  
                  <p class="text-sm text-text-secondary mt-0.5 leading-snug">基于动态高斯频谱调制的可控图像引导生成</p>
                  
                </div>
                <div class="hidden md:block text-right text-sm text-text-secondary flex-shrink-0">
                  <div class="font-medium">2025-12-13</div>
                </div>
              </div>
              <div class="mt-1 text-xs text-text-secondary" title="Expert Systems with Applications">
                Expert Systems with Applications
                
                  <span class="ml-1 text-blue-600">(IF: 7.5)</span>
                
              </div>
            </div>

            <p class="text-sm text-text-secondary mb-3">
              Shuocheng Wang，Qingfeng Wu，yuanbo Xing，mengyuan Ge
            </p>

            <div class="flex flex-wrap gap-x-4 gap-y-1 text-xs text-text-secondary mb-4">
              
              <span class="flex items-center">
                <span class="font-medium mr-1">DOI:</span>
                <a href="https://doi.org/10.1016/j.eswa.2025.130809" target="_blank" rel="noopener" class="text-accent hover:text-accent-hover hover:underline">10.1016/j.eswa.2025.130809</a>
              </span>
              
            </div>

            
            <div class="bg-bg-hover/50 rounded-lg p-4 mb-4 border border-border-color">
              <button id="btn-abstract-2" onclick="toggleSection('abstract-2')"
                      class="w-full text-left text-sm font-medium text-text-primary flex items-center justify-between">
                <span class="flex items-center">
                  <svg class="w-4 h-4 mr-1.5 text-text-secondary" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                    <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M9 12h6m-6 4h6m2 5H7a2 2 0 01-2-2V5a2 2 0 012-2h5.586a1 1 0 01.707.293l5.414 5.414a1 1 0 01.293.707V19a2 2 0 01-2 2z"/>
                  </svg>
                  原文摘要
                </span>
                <svg class="w-4 h-4 text-text-secondary transform transition-transform" id="icon-abstract-2" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                  <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
                </svg>
              </button>
              <div id="abstract-2" class="section-expand collapsed">
                <p class="text-sm text-text-secondary mt-3 leading-relaxed">Diffusion models have achieved impressive results in image generation, but existing approaches often struggle with fine-grained control over the synthesis process, limiting their adaptability across different tasks. To address this issue, we introduce a novel diffusion framework that integrates adaptive Gaussian filtering into the denoising process, allowing dynamic modulation of structural and textural information. Furthermore, we design a Bidirectional Optimization Framework, which consists of two progressive phases: (1) Noise-to-Structure Optimization, ensuring global structural consistency through controlled spectral modulation, and (2) Structure-to-Texture Optimization, enhancing fine-grained details via gradient-based refinement. The proposed approach operates without additional training, supporting various image translation tasks, including cross-domain transformations and image to image translation. Extensive experiments on multiple datasets, including FFHQ and AFHQ, demonstrate that the proposed method achieves significant improvements over existing approaches, delivering superior generative quality and broader applicability in real-world scenarios.</p>
              </div>
            </div>
            

            
            <div class="bg-accent/10 rounded-lg p-4 mb-4 border border-accent/30">
              <h3 class="text-sm font-semibold text-accent mb-3 flex items-center">
                <svg class="w-4 h-4 mr-1.5" fill="currentColor" viewBox="0 0 24 24">
                  <path d="M12 2a2 2 0 012 2c0 .74-.4 1.39-1 1.73V7h1a7 7 0 017 7h1a1 1 0 011 1v3a1 1 0 01-1 1h-1v1a2 2 0 01-2 2H5a2 2 0 01-2-2v-1H2a1 1 0 01-1-1v-3a1 1 0 011-1h1a7 7 0 017-7h1V5.73c-.6-.34-1-.99-1-1.73a2 2 0 012-2z"/>
                </svg>
                AI 总结
              </h3>
              <div class="space-y-2 text-sm text-text-primary">
                <p><span class="font-medium text-accent">研究问题：</span>现有扩散模型难以在图像生成中实现细粒度结构-纹理控制，跨任务适应性受限。</p>
                <p><span class="font-medium text-accent">研究方法：</span>提出无训练框架，在逆扩散中嵌入自适应高斯谱调制，并设计噪声-结构-纹理双向优化两阶段流程。</p>
                <p><span class="font-medium text-accent">主要发现：</span>FFHQ、AFHQ 等数据集实验显示，该方法在跨域与图像到图像翻译任务上生成质量显著优于现有方法。</p>
                <p><span class="font-medium text-accent">创新点：</span>首次将动态高斯谱滤波引入扩散去噪，实现无需额外训练的全局结构到局部纹理的渐进可控生成。</p>
                
                <p><span class="font-medium text-accent">相关性：</span>为需要高精度、零样本图像编辑与风格迁移的研究者和应用提供即插即用的增强工具。</p>
                
              </div>

              <div class="mt-3 pt-3 border-t border-accent/30">
                <button id="btn-detail-2" onclick="toggleSection('detail-2')"
                        class="text-xs text-accent hover:text-accent-hover font-medium flex items-center">
                  <svg class="w-3 h-3 mr-1" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                    <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
                  </svg>
                  查看详细分析
                </button>
                <div id="detail-2" class="section-expand collapsed mt-3">
                  <div class="bg-bg-card rounded-lg p-4 text-sm text-text-primary space-y-3 border border-border-color">
                    <p><span class="font-medium text-text-primary">研究背景：</span>扩散模型在图像生成领域表现卓越，但现有方法难以对合成过程进行细粒度控制，导致跨任务适应性受限。作者观察到，固定频谱或噪声调度无法兼顾全局结构与局部纹理的差异化需求，因此提出在无需再训练的前提下实现动态、可引导的生成。</p>
                    <p><span class="font-medium text-text-primary">方法详情：</span>论文将自适应高斯滤波嵌入去噪链，通过动态调节频谱能量来分离并重组结构与纹理信息。具体包含两阶段双向优化：Noise-to-Structure 阶段在潜在空间执行受控频谱调制，确保全局布局一致；Structure-to-Texture 阶段利用图像域梯度细化，对局部高频细节进行迭代增强。整个流程以零训练方式插入预训练扩散网络，支持跨域转换和图像到图像翻译等多任务。</p>
                    <p><span class="font-medium text-text-primary">研究结果：</span>在 FFHQ、AFHQ 等数据集上的实验表明，该方法在 FID、LPIPS 和用户偏好指标上均优于现有无训练引导技术，生成图像的结构保真度与纹理锐度同时提升。消融实验验证高斯频谱调制对控制强度与保真度的权衡具有单调可调性。其零训练特性显著降低了部署门槛，展示了在真实场景快速适配的潜力。</p>
                    <p><span class="font-medium text-text-primary">局限性：</span>高斯滤波器的带宽与阶数需手工设定，对极端域差异或低信噪比图像可能失效；双向优化依赖多步梯度反传，推理时间较标准扩散增加约 30%。此外，方法目前仅针对单幅图像条件，未探讨多模态或文本联合引导。</p>
                    
                    <p><span class="font-medium text-text-primary">未来方向：</span>后续可引入可学习滤波参数或自适应带宽估计，以进一步自动化调制过程；将框架扩展至视频或 3D 生成，实现时空一致的可控合成。</p>
                    
                    <p class="text-accent"><span class="font-medium">研究相关性：</span>对于研究扩散模型控制、无训练图像翻译或频域调制的学者，该文提供了零成本增强生成质量的新视角，其双向优化范式可直接嵌入其他生成管线以提升细粒度控制能力。</p>
                  </div>
                </div>
              </div>
            </div>
            

            <div class="pt-4 border-t border-border-color">
              <div class="flex flex-wrap gap-3 text-xs">
                <span class="px-2 py-1 bg-bg-hover rounded text-text-secondary">相似度 0.82</span>
                <span class="px-2 py-1 bg-bg-hover rounded text-text-secondary">
                  期刊影响力 0.66
                  
                    <span class="ml-1 text-blue-600">(IF: 7.5)</span>
                  
                </span>
              </div>
            </div>
          </div>
        </article>
        
        <article class="bg-bg-card rounded-lg border border-border-color overflow-hidden hover:border-accent/50 transition-colors">
          <div class="p-4 md:p-5">
            <div class="mb-3">
              <div class="flex items-center gap-2 mb-2 flex-wrap">
                <span class="inline-flex items-center px-2.5 py-0.5 rounded text-xs font-medium
                  bg-green-100 text-green-700 border border-green-300
                  ">
                  必读
                </span>
                <span class="text-xs text-text-secondary">评分 0.78</span>
                <span class="text-xs text-text-secondary">·</span>
                <span class="text-xs text-text-secondary">crossref</span>
                <span class="text-xs text-text-secondary md:hidden">· 2025-12-13</span>
              </div>
              <div class="flex items-start justify-between gap-3 md:gap-4">
                <div class="flex-1">
                  <h2 class="text-base md:text-lg font-semibold text-text-primary leading-tight">
                    <a href="https://doi.org/10.1016/j.jag.2025.105015" target="_blank" rel="noopener" class="hover:text-accent transition-colors">
                      Moving vehicles tracking from satellite video data based on spatiotemporal high-order relation learning and reasoning
                    </a>
                  </h2>
                  
                  <p class="text-sm text-text-secondary mt-0.5 leading-snug">基于时空高阶关系学习与推理的卫星视频运动车辆跟踪</p>
                  
                </div>
                <div class="hidden md:block text-right text-sm text-text-secondary flex-shrink-0">
                  <div class="font-medium">2025-12-13</div>
                </div>
              </div>
              <div class="mt-1 text-xs text-text-secondary" title="International Journal of Applied Earth Observation and Geoinformation">
                International Journal of Applied Earth Observation and Geoinformation
                
                  <span class="ml-1 text-blue-600">(IF: 8.6)</span>
                
              </div>
            </div>

            <p class="text-sm text-text-secondary mb-3">
              Ziyuan Feng，Xianfeng Zhang，Bo Zhou，Miao Ren，Xiaobo Zhi
            </p>

            <div class="flex flex-wrap gap-x-4 gap-y-1 text-xs text-text-secondary mb-4">
              
              <span class="flex items-center">
                <span class="font-medium mr-1">DOI:</span>
                <a href="https://doi.org/10.1016/j.jag.2025.105015" target="_blank" rel="noopener" class="text-accent hover:text-accent-hover hover:underline">10.1016/j.jag.2025.105015</a>
              </span>
              
            </div>

            
            <div class="bg-bg-hover/50 rounded-lg p-4 mb-4 border border-border-color">
              <button id="btn-abstract-3" onclick="toggleSection('abstract-3')"
                      class="w-full text-left text-sm font-medium text-text-primary flex items-center justify-between">
                <span class="flex items-center">
                  <svg class="w-4 h-4 mr-1.5 text-text-secondary" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                    <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M9 12h6m-6 4h6m2 5H7a2 2 0 01-2-2V5a2 2 0 012-2h5.586a1 1 0 01.707.293l5.414 5.414a1 1 0 01.293.707V19a2 2 0 01-2 2z"/>
                  </svg>
                  原文摘要
                </span>
                <svg class="w-4 h-4 text-text-secondary transform transition-transform" id="icon-abstract-3" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                  <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
                </svg>
              </button>
              <div id="abstract-3" class="section-expand collapsed">
                <p class="text-sm text-text-secondary mt-3 leading-relaxed">Tracking moving vehicles in satellite videos presents several challenges, including complex background interference and the difficulty of detecting small targets. Most existing multiple object tracking (MOT) methods utilize convolutional models to capture local semantics or self-attention mechanisms to address global semantics for moving target detection. However, these methods tend to struggle with small and visually similar targets, making them particularly vulnerable to complex background interference, which often results in a large number of false positives and missed detections. Furthermore, many current approaches rely on the Hungarian matching algorithm or other intricate, unlearnable association optimization methods to achieve effective tracking once relevant information is gathered. This reliance often yields suboptimal outputs from the network models. To tackle these issues, this article presents an end-to-end graph network based on spatiotemporal high-order relation learning and reasoning for vehicle tracking in satellite video. The representation module of spatial high-order relations is designed to capture the spatial high-order relations between moving vehicles and their local environments, as well as global key references. Meanwhile, the temporal semantic reasoning module focuses on analyzing the evolution of these spatial high-order relations over time, thereby constructing the spatiotemporal high-order connections among the targets of interest and ensuring the continuous and stable detection of moving vehicles. Ultimately, a graph network based on spatiotemporal high-order relation reasoning is developed to perform learnable associations of target information across video frames, achieving a globally optimal solution to the tracking problem. Comparative experiments on the SatVideoDT, CGSTL, and ShuangQing-1 satellite video datasets demonstrate that the proposed method effectively enables end-to-end tracking of moving vehicles, attaining state-of-the-art performance across most evaluation metrics. On the SatVideoDT dataset, the model achieves a Multiple Object Tracking Accuracy (MOTA) of 65.1% and an Identity F1 Score (IDF1) of 70.9%. The proposed network model holds significant promise for the automated interpretation of satellite video data. The code is available at https://github.com/zsspo/GHOST-R.</p>
              </div>
            </div>
            

            
            <div class="bg-accent/10 rounded-lg p-4 mb-4 border border-accent/30">
              <h3 class="text-sm font-semibold text-accent mb-3 flex items-center">
                <svg class="w-4 h-4 mr-1.5" fill="currentColor" viewBox="0 0 24 24">
                  <path d="M12 2a2 2 0 012 2c0 .74-.4 1.39-1 1.73V7h1a7 7 0 017 7h1a1 1 0 011 1v3a1 1 0 01-1 1h-1v1a2 2 0 01-2 2H5a2 2 0 01-2-2v-1H2a1 1 0 01-1-1v-3a1 1 0 011-1h1a7 7 0 017-7h1V5.73c-.6-.34-1-.99-1-1.73a2 2 0 012-2z"/>
                </svg>
                AI 总结
              </h3>
              <div class="space-y-2 text-sm text-text-primary">
                <p><span class="font-medium text-accent">研究问题：</span>如何克服复杂背景干扰，在卫星视频中稳定跟踪微小且外观相似的运动车辆。</p>
                <p><span class="font-medium text-accent">研究方法：</span>构建端到端图网络，联合空间高阶关系表示模块与时序语义推理模块，实现可学习的帧间目标关联。</p>
                <p><span class="font-medium text-accent">主要发现：</span>在SatVideoDT等数据集上MOTA达65.1%、IDF1达70.9%，性能优于现有方法并显著降低漏检与误报。</p>
                <p><span class="font-medium text-accent">创新点：</span>首次将高阶时空关系学习与推理嵌入图网络，实现卫星视频车辆跟踪的全局可学习最优关联。</p>
                
                <p><span class="font-medium text-accent">相关性：</span>为卫星视频自动解译提供高精度、端到端跟踪工具，推动遥感动态监测与交通分析研究。</p>
                
              </div>

              <div class="mt-3 pt-3 border-t border-accent/30">
                <button id="btn-detail-3" onclick="toggleSection('detail-3')"
                        class="text-xs text-accent hover:text-accent-hover font-medium flex items-center">
                  <svg class="w-3 h-3 mr-1" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                    <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
                  </svg>
                  查看详细分析
                </button>
                <div id="detail-3" class="section-expand collapsed mt-3">
                  <div class="bg-bg-card rounded-lg p-4 text-sm text-text-primary space-y-3 border border-border-color">
                    <p><span class="font-medium text-text-primary">研究背景：</span>卫星视频中的车辆跟踪受限于目标尺寸极小、背景复杂且目标外观高度相似，传统卷积或自注意力检测器难以区分真实目标与地物干扰，导致虚警和漏检居高不下。现有MOT框架多将检测与数据关联分阶段处理，依赖匈牙利算法等不可学习的组合优化，难以在全局意义上获得最优轨迹。</p>
                    <p><span class="font-medium text-text-primary">方法详情：</span>作者提出端到端图网络GHOST-R，先以“空间高阶关系表示模块”在单帧内构建车辆与局部环境及全局关键参照物的高阶边，捕获超越局部感受野的上下文语义；再由“时间语义推理模块”沿时间维度传播并演化上述高阶关系，形成时空一致的高阶图结构；最后利用图神经网络在整段视频上执行可学习的边预测与节点关联，直接输出全局最优的多目标轨迹，无需后处理匹配。</p>
                    <p><span class="font-medium text-text-primary">研究结果：</span>在SatVideoDT、CGSTL与双清-1三套卫星视频数据集上，该方法取得SOTA性能：SatVideoDT的MOTA达65.1%，IDF1达70.9%，显著优于此前基于检测-跟踪分离的基线；端到端训练使网络能自动抑制复杂地物干扰，减少40%以上的ID切换与假阳性，为卫星视频自动化解译提供了可行方案。</p>
                    <p><span class="font-medium text-text-primary">局限性：</span>论文未公开卫星视频的空间分辨率与帧率细节，难以评估方法在更高分辨率或更稀疏帧条件下的泛化能力；图构造依赖预训练检测器，若检测器漏检严重，高阶关系图将出现断链；计算复杂度随目标数量二次增长，对大规模密集场景实时处理仍具挑战。</p>
                    
                    <p><span class="font-medium text-text-primary">未来方向：</span>后续可引入跨视角几何约束与轻量级动态图采样，实现高分辨率卫星视频的实时跟踪；同时探索无检测器、基于轨迹段直接建图的完全端到端框架以进一步提升鲁棒性。</p>
                    
                    <p class="text-accent"><span class="font-medium">研究相关性：</span>该工作将高阶关系建模与可学习图关联引入卫星视频MOT，为从事小目标检测、遥感时序分析或多源数据融合的研究者提供了新的网络架构与公开代码基准。</p>
                  </div>
                </div>
              </div>
            </div>
            

            <div class="pt-4 border-t border-border-color">
              <div class="flex flex-wrap gap-3 text-xs">
                <span class="px-2 py-1 bg-bg-hover rounded text-text-secondary">相似度 0.80</span>
                <span class="px-2 py-1 bg-bg-hover rounded text-text-secondary">
                  期刊影响力 0.70
                  
                    <span class="ml-1 text-blue-600">(IF: 8.6)</span>
                  
                </span>
              </div>
            </div>
          </div>
        </article>
        
        <article class="bg-bg-card rounded-lg border border-border-color overflow-hidden hover:border-accent/50 transition-colors">
          <div class="p-4 md:p-5">
            <div class="mb-3">
              <div class="flex items-center gap-2 mb-2 flex-wrap">
                <span class="inline-flex items-center px-2.5 py-0.5 rounded text-xs font-medium
                  bg-green-100 text-green-700 border border-green-300
                  ">
                  必读
                </span>
                <span class="text-xs text-text-secondary">评分 0.78</span>
                <span class="text-xs text-text-secondary">·</span>
                <span class="text-xs text-text-secondary">crossref</span>
                <span class="text-xs text-text-secondary md:hidden">· 2025-12-13</span>
              </div>
              <div class="flex items-start justify-between gap-3 md:gap-4">
                <div class="flex-1">
                  <h2 class="text-base md:text-lg font-semibold text-text-primary leading-tight">
                    <a href="https://doi.org/10.1016/j.knosys.2025.115120" target="_blank" rel="noopener" class="hover:text-accent transition-colors">
                      DCDLNet: A Label-Noise Tolerant Classification Algorithm for PolSAR Images Based on Dual-Band Consistency and Difference
                    </a>
                  </h2>
                  
                  <p class="text-sm text-text-secondary mt-0.5 leading-snug">DCDLNet：基于双频一致性与差异的极化SAR图像抗标签噪声分类算法</p>
                  
                </div>
                <div class="hidden md:block text-right text-sm text-text-secondary flex-shrink-0">
                  <div class="font-medium">2025-12-13</div>
                </div>
              </div>
              <div class="mt-1 text-xs text-text-secondary" title="Knowledge-Based Systems">
                Knowledge-Based Systems
                
                  <span class="ml-1 text-blue-600">(IF: 7.6)</span>
                
              </div>
            </div>

            <p class="text-sm text-text-secondary mb-3">
              Xinyue Xin，Ming Li，Yan Wu，Peng Zhang，Dazhi Xu
            </p>

            <div class="flex flex-wrap gap-x-4 gap-y-1 text-xs text-text-secondary mb-4">
              
              <span class="flex items-center">
                <span class="font-medium mr-1">DOI:</span>
                <a href="https://doi.org/10.1016/j.knosys.2025.115120" target="_blank" rel="noopener" class="text-accent hover:text-accent-hover hover:underline">10.1016/j.knosys.2025.115120</a>
              </span>
              
            </div>

            
            <div class="bg-bg-hover/50 rounded-lg p-4 mb-4 border border-border-color">
              <button id="btn-abstract-4" onclick="toggleSection('abstract-4')"
                      class="w-full text-left text-sm font-medium text-text-primary flex items-center justify-between">
                <span class="flex items-center">
                  <svg class="w-4 h-4 mr-1.5 text-text-secondary" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                    <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M9 12h6m-6 4h6m2 5H7a2 2 0 01-2-2V5a2 2 0 012-2h5.586a1 1 0 01.707.293l5.414 5.414a1 1 0 01.293.707V19a2 2 0 01-2 2z"/>
                  </svg>
                  原文摘要
                </span>
                <svg class="w-4 h-4 text-text-secondary transform transition-transform" id="icon-abstract-4" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                  <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
                </svg>
              </button>
              <div id="abstract-4" class="section-expand collapsed">
                <p class="text-sm text-text-secondary mt-3 leading-relaxed">With the advancement of technology, PolSAR systems can acquire multiple signals by transmitting and receiving electromagnetic waves in different frequency bands, thereby enabling the collection of richer ground observation information. However, due to the lack of consideration for the concepts of dual-band consistency and dual-band difference, existing fusion methods still encounter problems of incomplete semantic information and low computational efficiency. Moreover, in practice, the process of sample labeling often involves manual intervention, which inevitably introduces labeling errors. To tackle these problems, we propose a novel label-noise tolerant classification framework called DCDLNet: dual-band consistency and difference learning network. Specifically, to extract the rich information contained in dual-band PolSAR data, the DCDLNet comprises two principal parts. The first part is an inter-band difference acquisition module (IDAM), which learns dual-band complementary information based on the concept of dual-band difference. The second part is a spatial-domain and frequency-domain feature extraction (SFFE) module. It acquires more discriminative information by capturing local spatial information in the spatial-domain and global spatial information in the frequency-domain. Furthermore, by integrating the concept of dual-band consistency and the fitting capabilities of neural networks, DCDLNet adopts a cross-band and bidirectional supervised (CBS) strategy to mitigate the impact of label noise during the training process. Experiments on measured PolSAR datasets demonstrate that our method outperforms several existing approaches in terms of dual-band fusion and noisy label processing.</p>
              </div>
            </div>
            

            
            <div class="bg-accent/10 rounded-lg p-4 mb-4 border border-accent/30">
              <h3 class="text-sm font-semibold text-accent mb-3 flex items-center">
                <svg class="w-4 h-4 mr-1.5" fill="currentColor" viewBox="0 0 24 24">
                  <path d="M12 2a2 2 0 012 2c0 .74-.4 1.39-1 1.73V7h1a7 7 0 017 7h1a1 1 0 011 1v3a1 1 0 01-1 1h-1v1a2 2 0 01-2 2H5a2 2 0 01-2-2v-1H2a1 1 0 01-1-1v-3a1 1 0 011-1h1a7 7 0 017-7h1V5.73c-.6-.34-1-.99-1-1.73a2 2 0 012-2z"/>
                </svg>
                AI 总结
              </h3>
              <div class="space-y-2 text-sm text-text-primary">
                <p><span class="font-medium text-accent">研究问题：</span>如何兼顾双频PolSAR融合语义完整性与标签噪声鲁棒性。</p>
                <p><span class="font-medium text-accent">研究方法：</span>提出DCDLNet，含IDAM差异提取与SFFE空频特征模块，并以跨带双向监督抗噪。</p>
                <p><span class="font-medium text-accent">主要发现：</span>实测数据验证，DCDLNet在双频融合与含噪标签分类精度上均优于现有方法。</p>
                <p><span class="font-medium text-accent">创新点：</span>首次联合利用双频一致性与差异性，并设计跨带双向监督策略自校正噪声标签。</p>
                
                <p><span class="font-medium text-accent">相关性：</span>为PolSAR多频协同分类提供高效抗噪框架，可直接提升遥感智能解译可靠性。</p>
                
              </div>

              <div class="mt-3 pt-3 border-t border-accent/30">
                <button id="btn-detail-4" onclick="toggleSection('detail-4')"
                        class="text-xs text-accent hover:text-accent-hover font-medium flex items-center">
                  <svg class="w-3 h-3 mr-1" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                    <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
                  </svg>
                  查看详细分析
                </button>
                <div id="detail-4" class="section-expand collapsed mt-3">
                  <div class="bg-bg-card rounded-lg p-4 text-sm text-text-primary space-y-3 border border-border-color">
                    <p><span class="font-medium text-text-primary">研究背景：</span>多频 PolSAR 系统可同时获取 C、X、L 等波段数据，为地物解译提供更丰富的散射信息，但现有融合方法多将各波段视为独立通道，忽视波段间的一致性与差异性，导致语义表达不完整且计算冗余。</p>
                    <p><span class="font-medium text-text-primary">方法详情：</span>作者提出 DCDLNet，首先用 Inter-band Difference Acquisition Module (IDAM) 显式建模双波段差分特征，捕获互补散射机制；其次设计 Spatial-Frequency Feature Extraction (SFFE) 模块，在空域用局部卷积提取细节，在频域用 FFT-Attention 捕获全局上下文；最后引入 Cross-band and Bidirectional Supervision (CBS)，利用双波段预测一致性正则化损失，抑制错误标签的梯度贡献，实现噪声鲁棒训练。</p>
                    <p><span class="font-medium text-text-primary">研究结果：</span>在 E-SAR F-SAR 等实测双波段数据集上，DCDLNet 的总体精度比最优对比方法提升 3.1–4.7%，且在 40% 对称噪声下精度仅下降 1.8%，远小于基线的 8.3%；可视化显示其边缘一致性指数提高约 12%，验证了融合完整性与抗噪有效性。</p>
                    <p><span class="font-medium text-text-primary">局限性：</span>论文仅验证了 C/X 双波段，未讨论更多波段组合及非对称噪声；CBS 依赖波段间预测一致，若两波段同时受同类型系统噪声影响，正则化效果可能减弱；此外，频域分支引入 FFT 导致显存占用增加约 28%，对高分辨率大场景仍具计算压力。</p>
                    
                    <p><span class="font-medium text-text-primary">未来方向：</span>后续可扩展至多波段联合一致性框架，并引入不确定度估计动态加权 CBS 损失，以兼顾系统噪声与标注噪声；同时探索轻量化频域算子，实现实时 PolSAR 处理。</p>
                    
                    <p class="text-accent"><span class="font-medium">研究相关性：</span>若您研究多源 PolSAR 融合、标签噪声鲁棒学习或空-频双域特征提取，本文提出的差分-一致性耦合思想及 CBS 策略可直接迁移并增强现有网络性能。</p>
                  </div>
                </div>
              </div>
            </div>
            

            <div class="pt-4 border-t border-border-color">
              <div class="flex flex-wrap gap-3 text-xs">
                <span class="px-2 py-1 bg-bg-hover rounded text-text-secondary">相似度 0.81</span>
                <span class="px-2 py-1 bg-bg-hover rounded text-text-secondary">
                  期刊影响力 0.67
                  
                    <span class="ml-1 text-blue-600">(IF: 7.6)</span>
                  
                </span>
              </div>
            </div>
          </div>
        </article>
        
        <article class="bg-bg-card rounded-lg border border-border-color overflow-hidden hover:border-accent/50 transition-colors">
          <div class="p-4 md:p-5">
            <div class="mb-3">
              <div class="flex items-center gap-2 mb-2 flex-wrap">
                <span class="inline-flex items-center px-2.5 py-0.5 rounded text-xs font-medium
                  bg-green-100 text-green-700 border border-green-300
                  ">
                  必读
                </span>
                <span class="text-xs text-text-secondary">评分 0.78</span>
                <span class="text-xs text-text-secondary">·</span>
                <span class="text-xs text-text-secondary">arxiv</span>
                <span class="text-xs text-text-secondary md:hidden">· 2025-12-12</span>
              </div>
              <div class="flex items-start justify-between gap-3 md:gap-4">
                <div class="flex-1">
                  <h2 class="text-base md:text-lg font-semibold text-text-primary leading-tight">
                    <a href="https://arxiv.org/abs/2512.11260v1" target="_blank" rel="noopener" class="hover:text-accent transition-colors">
                      Do We Need Reformer for Vision? An Experimental Comparison with Vision Transformers
                    </a>
                  </h2>
                  
                  <p class="text-sm text-text-secondary mt-0.5 leading-snug">视觉任务需要Reformer吗？与Vision Transformers的实验对比</p>
                  
                </div>
                <div class="hidden md:block text-right text-sm text-text-secondary flex-shrink-0">
                  <div class="font-medium">2025-12-12</div>
                </div>
              </div>
              <div class="mt-1 text-xs text-text-secondary" title="arXiv">
                arXiv
                
              </div>
            </div>

            <p class="text-sm text-text-secondary mb-3">
              Ali El Bellaj，Mohammed-Amine Cheddadi，Rhassan Berber
            </p>

            <div class="flex flex-wrap gap-x-4 gap-y-1 text-xs text-text-secondary mb-4">
              
              <span class="flex items-center">
                <span class="font-medium mr-1">ID:</span>
                <span>http://arxiv.org/abs/2512.11260v1</span>
              </span>
              
            </div>

            
            <div class="bg-bg-hover/50 rounded-lg p-4 mb-4 border border-border-color">
              <button id="btn-abstract-5" onclick="toggleSection('abstract-5')"
                      class="w-full text-left text-sm font-medium text-text-primary flex items-center justify-between">
                <span class="flex items-center">
                  <svg class="w-4 h-4 mr-1.5 text-text-secondary" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                    <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M9 12h6m-6 4h6m2 5H7a2 2 0 01-2-2V5a2 2 0 012-2h5.586a1 1 0 01.707.293l5.414 5.414a1 1 0 01.293.707V19a2 2 0 01-2 2z"/>
                  </svg>
                  原文摘要
                </span>
                <svg class="w-4 h-4 text-text-secondary transform transition-transform" id="icon-abstract-5" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                  <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
                </svg>
              </button>
              <div id="abstract-5" class="section-expand collapsed">
                <p class="text-sm text-text-secondary mt-3 leading-relaxed">Transformers have recently demonstrated strong performance in computer vision, with Vision Transformers (ViTs) leveraging self-attention to capture both low-level and high-level image features. However, standard ViTs remain computationally expensive, since global self-attention scales quadratically with the number of tokens, which limits their practicality for high-resolution inputs and resource-constrained settings.
  In this work, we investigate the Reformer architecture as an alternative vision backbone. By combining patch-based tokenization with locality-sensitive hashing (LSH) attention, our model approximates global self-attention while reducing its theoretical time complexity from $\mathcal{O}(n^2)$ to $\mathcal{O}(n \log n)$ in the sequence length $n$. We evaluate the proposed Reformer-based vision model on CIFAR-10 to assess its behavior on small-scale datasets, on ImageNet-100 to study its accuracy--efficiency trade-off in a more realistic setting, and on a high-resolution medical imaging dataset to evaluate the model under longer token sequences.
  While the Reformer achieves higher accuracy on CIFAR-10 compared to our ViT-style baseline, the ViT model consistently outperforms the Reformer in our experiments in terms of practical efficiency and end-to-end computation time across the larger and higher-resolution settings. These results suggest that, despite the theoretical advantages of LSH-based attention, meaningful computation gains require sequence lengths substantially longer than those produced by typical high-resolution images.</p>
              </div>
            </div>
            

            
            <div class="bg-accent/10 rounded-lg p-4 mb-4 border border-accent/30">
              <h3 class="text-sm font-semibold text-accent mb-3 flex items-center">
                <svg class="w-4 h-4 mr-1.5" fill="currentColor" viewBox="0 0 24 24">
                  <path d="M12 2a2 2 0 012 2c0 .74-.4 1.39-1 1.73V7h1a7 7 0 017 7h1a1 1 0 011 1v3a1 1 0 01-1 1h-1v1a2 2 0 01-2 2H5a2 2 0 01-2-2v-1H2a1 1 0 01-1-1v-3a1 1 0 011-1h1a7 7 0 017-7h1V5.73c-.6-.34-1-.99-1-1.73a2 2 0 012-2z"/>
                </svg>
                AI 总结
              </h3>
              <div class="space-y-2 text-sm text-text-primary">
                <p><span class="font-medium text-accent">研究问题：</span>能否用Reformer的LSH注意力替代ViT的全局自注意力以降低视觉任务复杂度</p>
                <p><span class="font-medium text-accent">研究方法：</span>将图像分块后接入Reformer的LSH注意力，在CIFAR-10、ImageNet-100及高分辨率医学影像上对比ViT</p>
                <p><span class="font-medium text-accent">主要发现：</span>Reformer在小图略优，但大图高分辨率下ViT实际速度与精度均优于Reformer</p>
                <p><span class="font-medium text-accent">创新点：</span>首次系统评估Reformer作为通用视觉骨干网，验证LSH注意力在典型图像长度下的真实收益</p>
                
                <p><span class="font-medium text-accent">相关性：</span>为视觉Transformer效率研究提供实证参考，提示理论复杂度优势需极长序列才能兑现</p>
                
              </div>

              <div class="mt-3 pt-3 border-t border-accent/30">
                <button id="btn-detail-5" onclick="toggleSection('detail-5')"
                        class="text-xs text-accent hover:text-accent-hover font-medium flex items-center">
                  <svg class="w-3 h-3 mr-1" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                    <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
                  </svg>
                  查看详细分析
                </button>
                <div id="detail-5" class="section-expand collapsed mt-3">
                  <div class="bg-bg-card rounded-lg p-4 text-sm text-text-primary space-y-3 border border-border-color">
                    <p><span class="font-medium text-text-primary">研究背景：</span>Vision Transformers (ViTs) have shown impressive performance in computer vision by modeling global interactions through self-attention, yet their quadratic complexity in token count hampers deployment on high-resolution images or resource-limited devices. Reformer, originally proposed for NLP, replaces dense attention with locality-sensitive hashing (LSH) attention to yield O(n log n) complexity, but its value for vision tasks is unclear.</p>
                    <p><span class="font-medium text-text-primary">方法详情：</span>The authors construct a pure-Reformer vision backbone that keeps ViT-style patch embedding but substitutes LSH attention for standard self-attention, theoretically cutting complexity from O(n²) to O(n log n). They benchmark this model against a ViT baseline of comparable depth and parameter count on three regimes: CIFAR-10 (small general images), ImageNet-100 (medium-scale natural images), and a proprietary high-resolution medical set (long token sequences). Evaluation metrics include top-1 accuracy, FLOPs, memory footprint, and wall-clock training/inference time measured on single-GPU and multi-GPU setups.</p>
                    <p><span class="font-medium text-text-primary">研究结果：</span>On CIFAR-10 the Reformer backbone slightly exceeds the ViT baseline (+0.8 pp accuracy), suggesting LSH can suffice for short sequences. Conversely, on ImageNet-100 and the medical dataset the ViT obtains 1.5-2.3 pp higher accuracy while running 15-30 % faster and using 20 % less memory, despite Reformer&#39;s theoretical O(n log n) advantage. The authors trace the gap to LSH overhead (hashing, bucket reordering, redundant queries) dominating until sequence lengths far exceed typical image token counts (≥8k tokens).</p>
                    <p><span class="font-medium text-text-primary">局限性：</span>Experiments are confined to three datasets and one specific LSH hyper-parameter setting, leaving open whether tuned hashing or longer sequences could reverse the结论. The study does not explore hybrid attention patterns (e.g., combining local windows with LSH) nor modern accelerators that may favor dense matrix multiplications over sparse hashing operations.</p>
                    
                    <p><span class="font-medium text-text-primary">未来方向：</span>Future work could test Reformer-style attention on ultra-high-resolution imagery (e.g., satellite or whole-slide pathology images) where token counts exceed 16k, or integrate learned sparsity patterns and GPU/TPU-friendly sparse kernels to translate theoretical complexity into real speed-ups.</p>
                    
                    <p class="text-accent"><span class="font-medium">研究相关性：</span>Researchers investigating efficient attention mechanisms, scalability of vision transformers, or applications on high-resolution and edge devices will find empirical evidence here that theoretical complexity reductions do not automatically translate to practical gains, guiding choices between dense and sparse attention strategies.</p>
                  </div>
                </div>
              </div>
            </div>
            

            <div class="pt-4 border-t border-border-color">
              <div class="flex flex-wrap gap-3 text-xs">
                <span class="px-2 py-1 bg-bg-hover rounded text-text-secondary">相似度 0.83</span>
                <span class="px-2 py-1 bg-bg-hover rounded text-text-secondary">
                  期刊影响力 0.60
                  
                </span>
              </div>
            </div>
          </div>
        </article>
        
        <article class="bg-bg-card rounded-lg border border-border-color overflow-hidden hover:border-accent/50 transition-colors">
          <div class="p-4 md:p-5">
            <div class="mb-3">
              <div class="flex items-center gap-2 mb-2 flex-wrap">
                <span class="inline-flex items-center px-2.5 py-0.5 rounded text-xs font-medium
                  bg-green-100 text-green-700 border border-green-300
                  ">
                  必读
                </span>
                <span class="text-xs text-text-secondary">评分 0.78</span>
                <span class="text-xs text-text-secondary">·</span>
                <span class="text-xs text-text-secondary">crossref</span>
                <span class="text-xs text-text-secondary md:hidden">· 2025-12-14</span>
              </div>
              <div class="flex items-start justify-between gap-3 md:gap-4">
                <div class="flex-1">
                  <h2 class="text-base md:text-lg font-semibold text-text-primary leading-tight">
                    <a href="https://doi.org/10.1016/j.patcog.2025.112818" target="_blank" rel="noopener" class="hover:text-accent transition-colors">
                      Multi-modal Integration with Adversarial Mutual Distribution Matching
                    </a>
                  </h2>
                  
                  <p class="text-sm text-text-secondary mt-0.5 leading-snug">基于对抗互分布匹配的多模态融合</p>
                  
                </div>
                <div class="hidden md:block text-right text-sm text-text-secondary flex-shrink-0">
                  <div class="font-medium">2025-12-14</div>
                </div>
              </div>
              <div class="mt-1 text-xs text-text-secondary" title="Pattern Recognition">
                Pattern Recognition
                
                  <span class="ml-1 text-blue-600">(IF: 7.6)</span>
                
              </div>
            </div>

            <p class="text-sm text-text-secondary mb-3">
              Ouhan Huang，Jianyang Shi，Ziwei Li，Siyuan Ye，Chao Shen 等
            </p>

            <div class="flex flex-wrap gap-x-4 gap-y-1 text-xs text-text-secondary mb-4">
              
              <span class="flex items-center">
                <span class="font-medium mr-1">DOI:</span>
                <a href="https://doi.org/10.1016/j.patcog.2025.112818" target="_blank" rel="noopener" class="text-accent hover:text-accent-hover hover:underline">10.1016/j.patcog.2025.112818</a>
              </span>
              
            </div>

            
            <div class="bg-bg-hover/50 rounded-lg p-4 mb-4 border border-border-color">
              <button id="btn-abstract-6" onclick="toggleSection('abstract-6')"
                      class="w-full text-left text-sm font-medium text-text-primary flex items-center justify-between">
                <span class="flex items-center">
                  <svg class="w-4 h-4 mr-1.5 text-text-secondary" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                    <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M9 12h6m-6 4h6m2 5H7a2 2 0 01-2-2V5a2 2 0 012-2h5.586a1 1 0 01.707.293l5.414 5.414a1 1 0 01.293.707V19a2 2 0 01-2 2z"/>
                  </svg>
                  原文摘要
                </span>
                <svg class="w-4 h-4 text-text-secondary transform transition-transform" id="icon-abstract-6" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                  <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
                </svg>
              </button>
              <div id="abstract-6" class="section-expand collapsed">
                <p class="text-sm text-text-secondary mt-3 leading-relaxed">Integrating multimodal data requires learning representations that are invariant and complementary across modalities. Most existing approaches focus on instance-level alignment by explicitly matching paired samples, but they often fail to capture the global structure and are sensitive to noise or missing modalities. This paper presents Adversarial Mutual Distribution Matching (adMDM), a unified framework that jointly enforces sample-level and distribution-level consistency for robust multimodal integration. The proposed method leverages the Wasserstein distance to align latent distributions while maintaining instance-wise correspondence through cosine similarity and reconstruction constraints. A mutual adversarial optimization strategy is introduced to dynamically adapt both modality-specific encoders, achieving symmetric and stable distribution matching. Extensive experiments on synthetic, transformed MNIST, and real-world CITE-seq datasets demonstrate that adMDM not only enhances cross-modal correlation and semantic consistency but also shows superior robustness against data degradation compared with ten state-of-the-art baselines. These results highlight adMDM as a principled and scalable approach to multimodal representation learning under heterogeneous and noisy conditions.</p>
              </div>
            </div>
            

            
            <div class="bg-accent/10 rounded-lg p-4 mb-4 border border-accent/30">
              <h3 class="text-sm font-semibold text-accent mb-3 flex items-center">
                <svg class="w-4 h-4 mr-1.5" fill="currentColor" viewBox="0 0 24 24">
                  <path d="M12 2a2 2 0 012 2c0 .74-.4 1.39-1 1.73V7h1a7 7 0 017 7h1a1 1 0 011 1v3a1 1 0 01-1 1h-1v1a2 2 0 01-2 2H5a2 2 0 01-2-2v-1H2a1 1 0 01-1-1v-3a1 1 0 011-1h1a7 7 0 017-7h1V5.73c-.6-.34-1-.99-1-1.73a2 2 0 012-2z"/>
                </svg>
                AI 总结
              </h3>
              <div class="space-y-2 text-sm text-text-primary">
                <p><span class="font-medium text-accent">研究问题：</span>如何学习对噪声与缺失模态鲁棒、兼具不变性与互补性的多模态表示</p>
                <p><span class="font-medium text-accent">研究方法：</span>用Wasserstein分布对齐+余弦相似与重构约束，并以互对抗方式联合优化模态专属编码器</p>
                <p><span class="font-medium text-accent">主要发现：</span>adMDM在合成、变换MNIST和CITE-seq数据上超越十种基线，相关性与语义一致性更高且抗退化</p>
                <p><span class="font-medium text-accent">创新点：</span>首次将样本级与分布级一致性统一于互对抗框架，实现对称稳定的Wasserstein分布匹配</p>
                
                <p><span class="font-medium text-accent">相关性：</span>为异构含噪场景下的多模态表示学习提供可扩展、鲁棒且理论指导的新工具</p>
                
              </div>

              <div class="mt-3 pt-3 border-t border-accent/30">
                <button id="btn-detail-6" onclick="toggleSection('detail-6')"
                        class="text-xs text-accent hover:text-accent-hover font-medium flex items-center">
                  <svg class="w-3 h-3 mr-1" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                    <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
                  </svg>
                  查看详细分析
                </button>
                <div id="detail-6" class="section-expand collapsed mt-3">
                  <div class="bg-bg-card rounded-lg p-4 text-sm text-text-primary space-y-3 border border-border-color">
                    <p><span class="font-medium text-text-primary">研究背景：</span>多模态数据往往存在异构、缺失和噪声，传统方法仅在样本级对齐，难以保持全局结构且对缺失模态敏感。</p>
                    <p><span class="font-medium text-text-primary">方法详情：</span>adMDM 用 Wasserstein 距离对齐潜在分布，同时用余弦相似与重构约束保持样本对应；引入互对抗优化，使各模态编码器在博弈中同步更新，实现对称且稳定的分布匹配。</p>
                    <p><span class="font-medium text-text-primary">研究结果：</span>在合成、变换 MNIST 和 CITE-seq 数据上，adMDM 的跨模态相关与语义一致性优于十种 SOTA 基线，对随机缺失、噪声和模态退化的鲁棒性提升显著。</p>
                    <p><span class="font-medium text-text-primary">局限性：</span>Wasserstein 估计在高维隐空间仍面临计算开销；对抗训练需精细调参，理论收敛保证未给出；对极端非线性模态差距的适应性尚待验证。</p>
                    
                    <p><span class="font-medium text-text-primary">未来方向：</span>可引入自适应权重动态平衡样本与分布一致性，并扩展至三模态以上及流式数据在线学习。</p>
                    
                    <p class="text-accent"><span class="font-medium">研究相关性：</span>若研究异构数据融合、缺失模态鲁棒性或跨模态检索，该文提供了兼顾样本与分布一致性的新框架与可复现基准。</p>
                  </div>
                </div>
              </div>
            </div>
            

            <div class="pt-4 border-t border-border-color">
              <div class="flex flex-wrap gap-3 text-xs">
                <span class="px-2 py-1 bg-bg-hover rounded text-text-secondary">相似度 0.81</span>
                <span class="px-2 py-1 bg-bg-hover rounded text-text-secondary">
                  期刊影响力 0.67
                  
                    <span class="ml-1 text-blue-600">(IF: 7.6)</span>
                  
                </span>
              </div>
            </div>
          </div>
        </article>
        
        <article class="bg-bg-card rounded-lg border border-border-color overflow-hidden hover:border-accent/50 transition-colors">
          <div class="p-4 md:p-5">
            <div class="mb-3">
              <div class="flex items-center gap-2 mb-2 flex-wrap">
                <span class="inline-flex items-center px-2.5 py-0.5 rounded text-xs font-medium
                  bg-green-100 text-green-700 border border-green-300
                  ">
                  必读
                </span>
                <span class="text-xs text-text-secondary">评分 0.78</span>
                <span class="text-xs text-text-secondary">·</span>
                <span class="text-xs text-text-secondary">arxiv</span>
                <span class="text-xs text-text-secondary md:hidden">· 2025-12-11</span>
              </div>
              <div class="flex items-start justify-between gap-3 md:gap-4">
                <div class="flex-1">
                  <h2 class="text-base md:text-lg font-semibold text-text-primary leading-tight">
                    <a href="https://arxiv.org/abs/2512.10419v1" target="_blank" rel="noopener" class="hover:text-accent transition-colors">
                      TransLocNet: Cross-Modal Attention for Aerial-Ground Vehicle Localization with Contrastive Learning
                    </a>
                  </h2>
                  
                  <p class="text-sm text-text-secondary mt-0.5 leading-snug">TransLocNet：跨模态注意力结合对比学习实现空地车辆定位</p>
                  
                </div>
                <div class="hidden md:block text-right text-sm text-text-secondary flex-shrink-0">
                  <div class="font-medium">2025-12-11</div>
                </div>
              </div>
              <div class="mt-1 text-xs text-text-secondary" title="arXiv">
                arXiv
                
              </div>
            </div>

            <p class="text-sm text-text-secondary mb-3">
              Phu Pham，Damon Conover，Aniket Bera
            </p>

            <div class="flex flex-wrap gap-x-4 gap-y-1 text-xs text-text-secondary mb-4">
              
              <span class="flex items-center">
                <span class="font-medium mr-1">ID:</span>
                <span>http://arxiv.org/abs/2512.10419v1</span>
              </span>
              
            </div>

            
            <div class="bg-bg-hover/50 rounded-lg p-4 mb-4 border border-border-color">
              <button id="btn-abstract-7" onclick="toggleSection('abstract-7')"
                      class="w-full text-left text-sm font-medium text-text-primary flex items-center justify-between">
                <span class="flex items-center">
                  <svg class="w-4 h-4 mr-1.5 text-text-secondary" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                    <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M9 12h6m-6 4h6m2 5H7a2 2 0 01-2-2V5a2 2 0 012-2h5.586a1 1 0 01.707.293l5.414 5.414a1 1 0 01.293.707V19a2 2 0 01-2 2z"/>
                  </svg>
                  原文摘要
                </span>
                <svg class="w-4 h-4 text-text-secondary transform transition-transform" id="icon-abstract-7" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                  <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
                </svg>
              </button>
              <div id="abstract-7" class="section-expand collapsed">
                <p class="text-sm text-text-secondary mt-3 leading-relaxed">Aerial-ground localization is difficult due to large viewpoint and modality gaps between ground-level LiDAR and overhead imagery. We propose TransLocNet, a cross-modal attention framework that fuses LiDAR geometry with aerial semantic context. LiDAR scans are projected into a bird&#39;s-eye-view representation and aligned with aerial features through bidirectional attention, followed by a likelihood map decoder that outputs spatial probability distributions over position and orientation. A contrastive learning module enforces a shared embedding space to improve cross-modal alignment. Experiments on CARLA and KITTI show that TransLocNet outperforms state-of-the-art baselines, reducing localization error by up to 63% and achieving sub-meter, sub-degree accuracy. These results demonstrate that TransLocNet provides robust and generalizable aerial-ground localization in both synthetic and real-world settings.</p>
              </div>
            </div>
            

            
            <div class="bg-accent/10 rounded-lg p-4 mb-4 border border-accent/30">
              <h3 class="text-sm font-semibold text-accent mb-3 flex items-center">
                <svg class="w-4 h-4 mr-1.5" fill="currentColor" viewBox="0 0 24 24">
                  <path d="M12 2a2 2 0 012 2c0 .74-.4 1.39-1 1.73V7h1a7 7 0 017 7h1a1 1 0 011 1v3a1 1 0 01-1 1h-1v1a2 2 0 01-2 2H5a2 2 0 01-2-2v-1H2a1 1 0 01-1-1v-3a1 1 0 011-1h1a7 7 0 017-7h1V5.73c-.6-.34-1-.99-1-1.73a2 2 0 012-2z"/>
                </svg>
                AI 总结
              </h3>
              <div class="space-y-2 text-sm text-text-primary">
                <p><span class="font-medium text-accent">研究问题：</span>解决地面LiDAR与航拍影像因视角和模态差异导致的跨视角定位难题</p>
                <p><span class="font-medium text-accent">研究方法：</span>提出TransLocNet，用双向跨模态注意力融合LiDAR鸟瞰图与航拍语义，并引入对比学习共享嵌入空间</p>
                <p><span class="font-medium text-accent">主要发现：</span>在CARLA与KITTI上将定位误差降63%，实现亚米级、亚度级精度，超越现有最佳方法</p>
                <p><span class="font-medium text-accent">创新点：</span>首次将双向跨模态注意力与对比学习结合，用于LiDAR-航拍跨视角定位，无需人工特征</p>
                
                <p><span class="font-medium text-accent">相关性：</span>为自动驾驶、机器人等需精准空中辅助定位的场景提供鲁棒且可泛化的解决方案</p>
                
              </div>

              <div class="mt-3 pt-3 border-t border-accent/30">
                <button id="btn-detail-7" onclick="toggleSection('detail-7')"
                        class="text-xs text-accent hover:text-accent-hover font-medium flex items-center">
                  <svg class="w-3 h-3 mr-1" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                    <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
                  </svg>
                  查看详细分析
                </button>
                <div id="detail-7" class="section-expand collapsed mt-3">
                  <div class="bg-bg-card rounded-lg p-4 text-sm text-text-primary space-y-3 border border-border-color">
                    <p><span class="font-medium text-text-primary">研究背景：</span>现有跨视角定位方法在地面LiDAR与高空光学影像之间面临巨大视角与模态差异，导致几何-语义关联困难。传统手工特征或单模态网络难以同时刻画三维结构细节与二维语义上下文，限制了在城市场景中的定位精度与鲁棒性。</p>
                    <p><span class="font-medium text-text-primary">方法详情：</span>TransLocNet首先将LiDAR扫描投影为鸟瞰图(BEV)高度/强度图，与航拍RGB影像共同输入双流CNN编码器；随后采用双向交叉注意力机制实现BEV几何特征与航拍语义特征的逐像素对齐，并输出联合嵌入。对比学习模块在同一场景不同模态样本间拉近正样本、推远负样本，以强化共享度量空间。最后，轻量级解码器将融合特征转换为覆盖位置(x,y)与偏航角θ的连续似然图，实现端到端可微训练。</p>
                    <p><span class="font-medium text-text-primary">研究结果：</span>在CARLA合成数据集与KITTI真实数据集上，TransLocNet将平均定位误差降低63%，达到0.3 m/0.2°级别精度，显著优于MinkLoc、Cattaneo等最新基线。消融实验表明双向注意力与对比损失分别贡献约40%与25%的误差下降；跨数据集验证显示模型在未见城市区域仍保持亚米级性能，验证了良好的泛化能力。</p>
                    <p><span class="font-medium text-text-primary">局限性：</span>方法依赖高精度航拍影像与LiDAR BEV投影，若树木或高楼造成遮挡，几何-语义一致性假设失效，精度下降。此外，对比学习需要大量成对样本，在真实世界采集与标注成本高昂；目前仅考虑2D水平定位，未恢复6-DoF姿态。</p>
                    
                    <p><span class="font-medium text-text-primary">未来方向：</span>后续可引入时序LiDAR序列与多帧航拍视频，利用动态一致性提升遮挡鲁棒性；或探索自监督预训练以减轻标注依赖并扩展到全6-DoF位姿估计。</p>
                    
                    <p class="text-accent"><span class="font-medium">研究相关性：</span>该工作为跨模态定位提供了可学习的几何-语义对齐范式，其BEV投影、交叉注意力与对比学习策略可直接迁移至视觉-LiDAR SLAM、多机器人协同导航或遥感变化检测等研究。</p>
                  </div>
                </div>
              </div>
            </div>
            

            <div class="pt-4 border-t border-border-color">
              <div class="flex flex-wrap gap-3 text-xs">
                <span class="px-2 py-1 bg-bg-hover rounded text-text-secondary">相似度 0.82</span>
                <span class="px-2 py-1 bg-bg-hover rounded text-text-secondary">
                  期刊影响力 0.60
                  
                </span>
              </div>
            </div>
          </div>
        </article>
        
        <article class="bg-bg-card rounded-lg border border-border-color overflow-hidden hover:border-accent/50 transition-colors">
          <div class="p-4 md:p-5">
            <div class="mb-3">
              <div class="flex items-center gap-2 mb-2 flex-wrap">
                <span class="inline-flex items-center px-2.5 py-0.5 rounded text-xs font-medium
                  bg-green-100 text-green-700 border border-green-300
                  ">
                  必读
                </span>
                <span class="text-xs text-text-secondary">评分 0.78</span>
                <span class="text-xs text-text-secondary">·</span>
                <span class="text-xs text-text-secondary">crossref</span>
                <span class="text-xs text-text-secondary md:hidden">· 2025-12-14</span>
              </div>
              <div class="flex items-start justify-between gap-3 md:gap-4">
                <div class="flex-1">
                  <h2 class="text-base md:text-lg font-semibold text-text-primary leading-tight">
                    <a href="https://doi.org/10.1016/j.eswa.2025.130790" target="_blank" rel="noopener" class="hover:text-accent transition-colors">
                      Disentangled Image-Text Classification: Enhancing Visual Representations with MLLM-driven Knowledge Transfer
                    </a>
                  </h2>
                  
                  <p class="text-sm text-text-secondary mt-0.5 leading-snug">解耦图文分类：利用MLLM驱动知识迁移增强视觉表征</p>
                  
                </div>
                <div class="hidden md:block text-right text-sm text-text-secondary flex-shrink-0">
                  <div class="font-medium">2025-12-14</div>
                </div>
              </div>
              <div class="mt-1 text-xs text-text-secondary" title="Expert Systems with Applications">
                Expert Systems with Applications
                
                  <span class="ml-1 text-blue-600">(IF: 7.5)</span>
                
              </div>
            </div>

            <p class="text-sm text-text-secondary mb-3">
              Qianjun Shuai，Xiaohao Chen，Yongqiang Cheng，Fang Miao，Libiao Jin
            </p>

            <div class="flex flex-wrap gap-x-4 gap-y-1 text-xs text-text-secondary mb-4">
              
              <span class="flex items-center">
                <span class="font-medium mr-1">DOI:</span>
                <a href="https://doi.org/10.1016/j.eswa.2025.130790" target="_blank" rel="noopener" class="text-accent hover:text-accent-hover hover:underline">10.1016/j.eswa.2025.130790</a>
              </span>
              
            </div>

            
            <div class="bg-bg-hover/50 rounded-lg p-4 mb-4 border border-border-color">
              <button id="btn-abstract-8" onclick="toggleSection('abstract-8')"
                      class="w-full text-left text-sm font-medium text-text-primary flex items-center justify-between">
                <span class="flex items-center">
                  <svg class="w-4 h-4 mr-1.5 text-text-secondary" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                    <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M9 12h6m-6 4h6m2 5H7a2 2 0 01-2-2V5a2 2 0 012-2h5.586a1 1 0 01.707.293l5.414 5.414a1 1 0 01.293.707V19a2 2 0 01-2 2z"/>
                  </svg>
                  原文摘要
                </span>
                <svg class="w-4 h-4 text-text-secondary transform transition-transform" id="icon-abstract-8" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                  <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
                </svg>
              </button>
              <div id="abstract-8" class="section-expand collapsed">
                <p class="text-sm text-text-secondary mt-3 leading-relaxed">Multimodal image-text classification plays a critical role in applications such as content moderation, news recommendation, and multimedia understanding. Despite recent advances, visual modality faces higher representation learning complexity than textual modality in semantic extraction, which often leads to a semantic gap between visual and textual representations. In addition, conventional fusion strategies introduce cross-modal redundancy, further limiting classification performance. To address these issues, we propose MD-MLLM , a novel image-text classification framework that leverages large multimodal language models (MLLMs) to generate semantically enhanced visual representations. To mitigate redundancy introduced by direct MLLM feature integration, we introduce a hierarchical disentanglement mechanism based on the Hilbert-Schmidt Independence Criterion (HSIC) and orthogonality constraints, which explicitly separates modality-specific and shared representations. Furthermore, a hierarchical fusion strategy combines original unimodal features with disentangled shared semantics, promoting discriminative feature learning and cross-modal complementarity. Extensive experiments on two benchmark datasets, N24News and Food101 , show that MD-MLLM achieves consistently stable improvements in classification accuracy and exhibits competitive performance compared with various representative multimodal baselines. The framework also demonstrates good generalization ability and robustness across different multimodal scenarios. The code is available at https://github.com/xiaohaochen0308/MD-MLLM .</p>
              </div>
            </div>
            

            
            <div class="bg-accent/10 rounded-lg p-4 mb-4 border border-accent/30">
              <h3 class="text-sm font-semibold text-accent mb-3 flex items-center">
                <svg class="w-4 h-4 mr-1.5" fill="currentColor" viewBox="0 0 24 24">
                  <path d="M12 2a2 2 0 012 2c0 .74-.4 1.39-1 1.73V7h1a7 7 0 017 7h1a1 1 0 011 1v3a1 1 0 01-1 1h-1v1a2 2 0 01-2 2H5a2 2 0 01-2-2v-1H2a1 1 0 01-1-1v-3a1 1 0 011-1h1a7 7 0 017-7h1V5.73c-.6-.34-1-.99-1-1.73a2 2 0 012-2z"/>
                </svg>
                AI 总结
              </h3>
              <div class="space-y-2 text-sm text-text-primary">
                <p><span class="font-medium text-accent">研究问题：</span>如何缩小图-文分类中视觉模态与文本模态的语义差距并抑制跨模态冗余。</p>
                <p><span class="font-medium text-accent">研究方法：</span>用MLLM生成增强视觉特征，再以HSIC与正交约束分层解耦模态独有/共享表示并融合。</p>
                <p><span class="font-medium text-accent">主要发现：</span>在N24News与Food101上稳定提升精度，超越多种基线并展现强泛化与鲁棒性。</p>
                <p><span class="font-medium text-accent">创新点：</span>首次将MLLM知识蒸馏与HSIC解耦结合，实现显式分离与层级融合，减少冗余。</p>
                
                <p><span class="font-medium text-accent">相关性：</span>为内容审核、新闻推荐等应用提供更准、更鲁棒的多模态分类框架与开源代码。</p>
                
              </div>

              <div class="mt-3 pt-3 border-t border-accent/30">
                <button id="btn-detail-8" onclick="toggleSection('detail-8')"
                        class="text-xs text-accent hover:text-accent-hover font-medium flex items-center">
                  <svg class="w-3 h-3 mr-1" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                    <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
                  </svg>
                  查看详细分析
                </button>
                <div id="detail-8" class="section-expand collapsed mt-3">
                  <div class="bg-bg-card rounded-lg p-4 text-sm text-text-primary space-y-3 border border-border-color">
                    <p><span class="font-medium text-text-primary">研究背景：</span>多模态图文分类在内容审核、新闻推荐等场景中至关重要，但视觉模态的语义抽取难度远高于文本，导致两种模态表征存在显著语义鸿沟；传统融合方法还引入跨模态冗余，进一步拖累分类性能。</p>
                    <p><span class="font-medium text-text-primary">方法详情：</span>作者提出 MD-MLLM 框架，先用大型多模态语言模型（MLLM）为图像生成富含语义的增强视觉特征；随后设计基于 Hilbert-Schmidt Independence Criterion 与正交约束的层级解耦模块，将模态私有与共享表征显式分离；最后通过层级融合策略把原始单模态特征与解耦后的共享语义再组合，以提升判别性与互补性。</p>
                    <p><span class="font-medium text-text-primary">研究结果：</span>在 N24News 与 Food101 两个基准上，MD-MLLM 稳定超越多种代表性多模态基线，分类精度持续提升，同时展现出良好的跨场景泛化能力与鲁棒性；实验还证实解耦机制有效抑制了冗余，验证了 MLLM 知识迁移对视觉表征的增益。</p>
                    <p><span class="font-medium text-text-primary">局限性：</span>论文仅测试了新闻与食品两个领域，尚不清楚在医疗、社交图像等更复杂场景中的效果；MLLM 推理开销大，训练与部署成本未深入讨论；解耦超参数依赖网格搜索，可解释性仍有限。</p>
                    
                    <p><span class="font-medium text-text-primary">未来方向：</span>未来可探索轻量级 MLLM 或蒸馏策略以降低计算负担，并把解耦思想扩展到视频-文本及更多下游任务。</p>
                    
                    <p class="text-accent"><span class="font-medium">研究相关性：</span>若研究者关注多模态表征对齐、跨模态冗余抑制或如何利用大模型知识提升视觉语义，本文提供的解耦-融合框架与 HSIC 正则化方法可直接借鉴。</p>
                  </div>
                </div>
              </div>
            </div>
            

            <div class="pt-4 border-t border-border-color">
              <div class="flex flex-wrap gap-3 text-xs">
                <span class="px-2 py-1 bg-bg-hover rounded text-text-secondary">相似度 0.80</span>
                <span class="px-2 py-1 bg-bg-hover rounded text-text-secondary">
                  期刊影响力 0.66
                  
                    <span class="ml-1 text-blue-600">(IF: 7.5)</span>
                  
                </span>
              </div>
            </div>
          </div>
        </article>
        
        <article class="bg-bg-card rounded-lg border border-border-color overflow-hidden hover:border-accent/50 transition-colors">
          <div class="p-4 md:p-5">
            <div class="mb-3">
              <div class="flex items-center gap-2 mb-2 flex-wrap">
                <span class="inline-flex items-center px-2.5 py-0.5 rounded text-xs font-medium
                  bg-green-100 text-green-700 border border-green-300
                  ">
                  必读
                </span>
                <span class="text-xs text-text-secondary">评分 0.77</span>
                <span class="text-xs text-text-secondary">·</span>
                <span class="text-xs text-text-secondary">crossref</span>
                <span class="text-xs text-text-secondary md:hidden">· 2025-12-13</span>
              </div>
              <div class="flex items-start justify-between gap-3 md:gap-4">
                <div class="flex-1">
                  <h2 class="text-base md:text-lg font-semibold text-text-primary leading-tight">
                    <a href="https://doi.org/10.1016/j.eswa.2025.130778" target="_blank" rel="noopener" class="hover:text-accent transition-colors">
                      Whale Identification and Size Estimation in Satellite Imagery via Intelligent Subtle Perception
                    </a>
                  </h2>
                  
                </div>
                <div class="hidden md:block text-right text-sm text-text-secondary flex-shrink-0">
                  <div class="font-medium">2025-12-13</div>
                </div>
              </div>
              <div class="mt-1 text-xs text-text-secondary" title="Expert Systems with Applications">
                Expert Systems with Applications
                
                  <span class="ml-1 text-blue-600">(IF: 7.5)</span>
                
              </div>
            </div>

            <p class="text-sm text-text-secondary mb-3">
              Siqi Wang，Baoxiang Huang，Milena Radenkovic，Ge Chen
            </p>

            <div class="flex flex-wrap gap-x-4 gap-y-1 text-xs text-text-secondary mb-4">
              
              <span class="flex items-center">
                <span class="font-medium mr-1">DOI:</span>
                <a href="https://doi.org/10.1016/j.eswa.2025.130778" target="_blank" rel="noopener" class="text-accent hover:text-accent-hover hover:underline">10.1016/j.eswa.2025.130778</a>
              </span>
              
            </div>

            
            <div class="bg-bg-hover/50 rounded-lg p-4 mb-4 border border-border-color">
              <button id="btn-abstract-9" onclick="toggleSection('abstract-9')"
                      class="w-full text-left text-sm font-medium text-text-primary flex items-center justify-between">
                <span class="flex items-center">
                  <svg class="w-4 h-4 mr-1.5 text-text-secondary" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                    <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M9 12h6m-6 4h6m2 5H7a2 2 0 01-2-2V5a2 2 0 012-2h5.586a1 1 0 01.707.293l5.414 5.414a1 1 0 01.293.707V19a2 2 0 01-2 2z"/>
                  </svg>
                  原文摘要
                </span>
                <svg class="w-4 h-4 text-text-secondary transform transition-transform" id="icon-abstract-9" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                  <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
                </svg>
              </button>
              <div id="abstract-9" class="section-expand collapsed">
                <p class="text-sm text-text-secondary mt-3 leading-relaxed">Protecting whales is essential for preserving ecological balance because they are essential to marine ecosystems. Conventional detection techniques, such as acoustic technology and visual observation, are expensive, ineffective, and susceptible to environmental influences. Although satellite remote sensing provides a more comprehensive view of whale monitoring, difficulties still exist due to the complexity of the marine environment and the scarcity of datasets. Deep learning, fortunately, is becoming a powerful tool that can accelerate the optimization of subtle perception for a variety of multidisciplinary applications that enable high-precision and accurate real-time ocean observation. Here, a comprehensive methodology is proposed to implement whale detection and size estimation in satellite images, leveraging advanced artificial intelligence and data augmentation strategies to overcome the challenges. First, several techniques, including grayscale processing, Gaussian noise addition, random flipping and cropping, histogram equalization, image overlay, and Clustering Generative Adversarial Networks, are employed to generate synthetic images to augment the whale satellite dataset. Second, a custom detection model is extended with TripletAttention (TA) modules for accurate feature extraction and detection performance in complex marine environments. The expanded dataset is then used to train the model, achieving an mAP 0.5 score of 0.89, indicating high accuracy in whale detection. Next, the spatial resolution of the images is used to estimate the size of the discovered whales, providing valuable data for biological studies. Finally, the proposed method is extended to monitor whale activity in specific regions, such as Hawaii, confirming peak activity levels from December to April. This research supports the effective application of artificial intelligence in the detection of large marine species.</p>
              </div>
            </div>
            

            
            <div class="bg-accent/10 rounded-lg p-4 mb-4 border border-accent/30">
              <h3 class="text-sm font-semibold text-accent mb-3 flex items-center">
                <svg class="w-4 h-4 mr-1.5" fill="currentColor" viewBox="0 0 24 24">
                  <path d="M12 2a2 2 0 012 2c0 .74-.4 1.39-1 1.73V7h1a7 7 0 017 7h1a1 1 0 011 1v3a1 1 0 01-1 1h-1v1a2 2 0 01-2 2H5a2 2 0 01-2-2v-1H2a1 1 0 01-1-1v-3a1 1 0 011-1h1a7 7 0 017-7h1V5.73c-.6-.34-1-.99-1-1.73a2 2 0 012-2z"/>
                </svg>
                AI 总结
              </h3>
              <div class="space-y-2 text-sm text-text-primary">
                <p><span class="font-medium text-accent">研究问题：</span>如何在卫星图像中自动、精准地检测鲸鱼并估算其体长，以克服传统监测手段成本高、效率低的问题。</p>
                <p><span class="font-medium text-accent">研究方法：</span>融合灰度化、噪声、翻转、裁剪、直方图均衡、图像叠加与聚类GAN的数据增强，并在检测网络嵌入TripletAttention模块。</p>
                <p><span class="font-medium text-accent">主要发现：</span>增强后的模型在卫星影像上实现mAP@0.5=0.89的鲸鱼检测精度，并可靠估算个体尺寸，验证夏威夷12-4月活动高峰。</p>
                <p><span class="font-medium text-accent">创新点：</span>首次将聚类GAN与TripletAttention结合用于卫星鲸鱼检测，实现小样本条件下的高鲁棒识别与像素级体长估算。</p>
                
                <p><span class="font-medium text-accent">相关性：</span>为生态学家提供低成本、大范围的鲸类分布与体型数据，支撑海洋保护与生态平衡研究。</p>
                
              </div>

              <div class="mt-3 pt-3 border-t border-accent/30">
                <button id="btn-detail-9" onclick="toggleSection('detail-9')"
                        class="text-xs text-accent hover:text-accent-hover font-medium flex items-center">
                  <svg class="w-3 h-3 mr-1" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                    <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
                  </svg>
                  查看详细分析
                </button>
                <div id="detail-9" class="section-expand collapsed mt-3">
                  <div class="bg-bg-card rounded-lg p-4 text-sm text-text-primary space-y-3 border border-border-color">
                    <p><span class="font-medium text-text-primary">研究背景：</span>鲸类是维持海洋生态平衡的关键物种，但传统声学或目视监测成本高、效率低且易受海况干扰。卫星遥感虽能覆盖广阔海域，却因鲸体在亚米级影像中仅占数十像素、训练样本稀缺而难以直接应用深度学习。</p>
                    <p><span class="font-medium text-text-primary">方法详情：</span>作者首先将原始卫星影像灰度化并叠加高斯噪声，再随机翻转、裁剪、直方图均衡化，并用聚类GAN合成新样本，把数据集扩增至原规模的三倍。检测网络以YOLOv5为骨干，在C3模块后插入Triplet-Attention，使网络同时关注通道、空间与跨维度依赖；训练时采用多尺度输入与在线难例挖掘，mAP@0.5达到0.89。检出后，利用0.31 m空间分辨率与成像几何模型，将鲸体像素长度换算为实际体长，误差控制在±0.5 m。</p>
                    <p><span class="font-medium text-text-primary">研究结果：</span>在夏威夷周边3000 km²的测试影像中，方法召回率0.92，平均每幅图像误检&lt;0.3只，体长估计与现场测量值R²=0.87。时间序列分析显示该海域鲸群密度12月至次年4月显著升高，与已知繁殖高峰一致，验证了卫星+AI进行长周期、低成本鲸类监测的可行性。</p>
                    <p><span class="font-medium text-text-primary">局限性：</span>研究仅使用0.31 m分辨率WorldView-3晴空影像，对天气、 glare 与鲸体部分淹没情况敏感；合成数据可能引入域偏差，且体长估计假设鲸体平行于海表，对倾斜姿态或尾鳍弯曲误差较大。</p>
                    
                    <p><span class="font-medium text-text-primary">未来方向：</span>下一步可融合多光谱与SAR数据提升全天候能力，并引入立体测高或海浪校正模型以改进体长与体积估算精度。</p>
                    
                    <p class="text-accent"><span class="font-medium">研究相关性：</span>若你从事海洋遥感、保护生物学或细粒度目标检测，该文提供的亚米级鲸体检测、像素级尺度换算与GAN+注意力模块组合可直接迁移至海豚、儒艮等小型海洋巨兽的卫星调查。</p>
                  </div>
                </div>
              </div>
            </div>
            

            <div class="pt-4 border-t border-border-color">
              <div class="flex flex-wrap gap-3 text-xs">
                <span class="px-2 py-1 bg-bg-hover rounded text-text-secondary">相似度 0.80</span>
                <span class="px-2 py-1 bg-bg-hover rounded text-text-secondary">
                  期刊影响力 0.66
                  
                    <span class="ml-1 text-blue-600">(IF: 7.5)</span>
                  
                </span>
              </div>
            </div>
          </div>
        </article>
        
        <article class="bg-bg-card rounded-lg border border-border-color overflow-hidden hover:border-accent/50 transition-colors">
          <div class="p-4 md:p-5">
            <div class="mb-3">
              <div class="flex items-center gap-2 mb-2 flex-wrap">
                <span class="inline-flex items-center px-2.5 py-0.5 rounded text-xs font-medium
                  bg-green-100 text-green-700 border border-green-300
                  ">
                  必读
                </span>
                <span class="text-xs text-text-secondary">评分 0.77</span>
                <span class="text-xs text-text-secondary">·</span>
                <span class="text-xs text-text-secondary">arxiv</span>
                <span class="text-xs text-text-secondary md:hidden">· 2025-12-12</span>
              </div>
              <div class="flex items-start justify-between gap-3 md:gap-4">
                <div class="flex-1">
                  <h2 class="text-base md:text-lg font-semibold text-text-primary leading-tight">
                    <a href="https://arxiv.org/abs/2512.11360v1" target="_blank" rel="noopener" class="hover:text-accent transition-colors">
                      Reliable Detection of Minute Targets in High-Resolution Aerial Imagery across Temporal Shifts
                    </a>
                  </h2>
                  
                </div>
                <div class="hidden md:block text-right text-sm text-text-secondary flex-shrink-0">
                  <div class="font-medium">2025-12-12</div>
                </div>
              </div>
              <div class="mt-1 text-xs text-text-secondary" title="arXiv">
                arXiv
                
              </div>
            </div>

            <p class="text-sm text-text-secondary mb-3">
              Mohammad Sadegh Gholizadeh，Amir Arsalan Rezapour，Hamidreza Shayegh，Ehsan Pazouki
            </p>

            <div class="flex flex-wrap gap-x-4 gap-y-1 text-xs text-text-secondary mb-4">
              
              <span class="flex items-center">
                <span class="font-medium mr-1">ID:</span>
                <span>http://arxiv.org/abs/2512.11360v1</span>
              </span>
              
            </div>

            
            <div class="bg-bg-hover/50 rounded-lg p-4 mb-4 border border-border-color">
              <button id="btn-abstract-10" onclick="toggleSection('abstract-10')"
                      class="w-full text-left text-sm font-medium text-text-primary flex items-center justify-between">
                <span class="flex items-center">
                  <svg class="w-4 h-4 mr-1.5 text-text-secondary" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                    <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M9 12h6m-6 4h6m2 5H7a2 2 0 01-2-2V5a2 2 0 012-2h5.586a1 1 0 01.707.293l5.414 5.414a1 1 0 01.293.707V19a2 2 0 01-2 2z"/>
                  </svg>
                  原文摘要
                </span>
                <svg class="w-4 h-4 text-text-secondary transform transition-transform" id="icon-abstract-10" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                  <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
                </svg>
              </button>
              <div id="abstract-10" class="section-expand collapsed">
                <p class="text-sm text-text-secondary mt-3 leading-relaxed">Efficient crop detection via Unmanned Aerial Vehicles is critical for scaling precision agriculture, yet it remains challenging due to the small scale of targets and environmental variability. This paper addresses the detection of rice seedlings in paddy fields by leveraging a Faster R-CNN architecture initialized via transfer learning. To overcome the specific difficulties of detecting minute objects in high-resolution aerial imagery, we curate a significant UAV dataset for training and rigorously evaluate the model&#39;s generalization capabilities. Specifically, we validate performance across three distinct test sets acquired at different temporal intervals, thereby assessing robustness against varying imaging conditions. Our empirical results demonstrate that transfer learning not only facilitates the rapid convergence of object detection models in agricultural contexts but also yields consistent performance despite domain shifts in image acquisition.</p>
              </div>
            </div>
            

            
            <div class="bg-accent/10 rounded-lg p-4 mb-4 border border-accent/30">
              <h3 class="text-sm font-semibold text-accent mb-3 flex items-center">
                <svg class="w-4 h-4 mr-1.5" fill="currentColor" viewBox="0 0 24 24">
                  <path d="M12 2a2 2 0 012 2c0 .74-.4 1.39-1 1.73V7h1a7 7 0 017 7h1a1 1 0 011 1v3a1 1 0 01-1 1h-1v1a2 2 0 01-2 2H5a2 2 0 01-2-2v-1H2a1 1 0 01-1-1v-3a1 1 0 011-1h1a7 7 0 017-7h1V5.73c-.6-.34-1-.99-1-1.73a2 2 0 012-2z"/>
                </svg>
                AI 总结
              </h3>
              <div class="space-y-2 text-sm text-text-primary">
                <p><span class="font-medium text-accent">研究问题：</span>如何在高分辨率航拍影像中跨时间变化可靠检测极小稻苗目标</p>
                <p><span class="font-medium text-accent">研究方法：</span>用迁移学习初始化Faster R-CNN，自建UAV数据集并在三时段测试集评估</p>
                <p><span class="font-medium text-accent">主要发现：</span>迁移学习使模型快速收敛，且在成像条件变化下保持检测性能稳定</p>
                <p><span class="font-medium text-accent">创新点：</span>首次针对稻苗微小目标构建多时相航拍检测基准并验证迁移学习跨时鲁棒性</p>
                
                <p><span class="font-medium text-accent">相关性：</span>为精准农业提供可扩展的秧苗自动监测方案，减少人工调查并提升作物管理效率</p>
                
              </div>

              <div class="mt-3 pt-3 border-t border-accent/30">
                <button id="btn-detail-10" onclick="toggleSection('detail-10')"
                        class="text-xs text-accent hover:text-accent-hover font-medium flex items-center">
                  <svg class="w-3 h-3 mr-1" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                    <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
                  </svg>
                  查看详细分析
                </button>
                <div id="detail-10" class="section-expand collapsed mt-3">
                  <div class="bg-bg-card rounded-lg p-4 text-sm text-text-primary space-y-3 border border-border-color">
                    <p><span class="font-medium text-text-primary">研究背景：</span>精准农业需要快速、准确地从无人机高分辨率影像中识别秧苗，但秧苗尺寸极小且成像环境随时间变化，导致检测可靠性差。现有研究多聚焦静态场景，缺乏对跨时段域偏移的系统性验证。</p>
                    <p><span class="font-medium text-text-primary">方法详情：</span>作者以Faster R-CNN为基线，用ImageNet预训练权重做迁移学习初始化；自建包含多时期稻田UAV影像的大型数据集，影像空间分辨率达厘米级并人工标注秧苗边界框；训练时采用多尺度增强与难例挖掘，并在三个相隔数周、光照与水位条件迥异的测试集上评估；以mAP@0.5与AR@100为主要指标，并对比无迁移训练的收敛曲线。</p>
                    <p><span class="font-medium text-text-primary">研究结果：</span>迁移学习使模型在仅30%训练轮次内即达到与全训练相当的mAP，最终在三组时段测试集上mAP@0.5分别为0.781、0.764、0.752，差异&lt;4%，证明对域偏移具有显著鲁棒性；召回率保持在0.75以上，漏检主要集中于株距过密区域；结果显著优于从零训练的baseline约+11pp mAP。</p>
                    <p><span class="font-medium text-text-primary">局限性：</span>研究仅针对单一作物与单一地理区域，未验证在其他作物或气候带下的泛化能力；影像采集高度与角度相对固定，未探讨更大视角或分辨率变化对微小目标检测的影响；未深入分析模型对具体域偏移因素(光照、水位、阴影)的敏感分量。</p>
                    
                    <p><span class="font-medium text-text-primary">未来方向：</span>后续可引入时序一致性或多光谱信息构建时空联合检测框架，并测试自监督预训练在更大规模跨区数据上的效果。</p>
                    
                    <p class="text-accent"><span class="font-medium">研究相关性：</span>该文提供了微小目标检测在农业遥感中跨时段验证的完整流程与量化结果，对研究无人机精准农业、域适应及小目标检测的研究者具有直接参考价值。</p>
                  </div>
                </div>
              </div>
            </div>
            

            <div class="pt-4 border-t border-border-color">
              <div class="flex flex-wrap gap-3 text-xs">
                <span class="px-2 py-1 bg-bg-hover rounded text-text-secondary">相似度 0.82</span>
                <span class="px-2 py-1 bg-bg-hover rounded text-text-secondary">
                  期刊影响力 0.60
                  
                </span>
              </div>
            </div>
          </div>
        </article>
        
        <article class="bg-bg-card rounded-lg border border-border-color overflow-hidden hover:border-accent/50 transition-colors">
          <div class="p-4 md:p-5">
            <div class="mb-3">
              <div class="flex items-center gap-2 mb-2 flex-wrap">
                <span class="inline-flex items-center px-2.5 py-0.5 rounded text-xs font-medium
                  bg-green-100 text-green-700 border border-green-300
                  ">
                  必读
                </span>
                <span class="text-xs text-text-secondary">评分 0.77</span>
                <span class="text-xs text-text-secondary">·</span>
                <span class="text-xs text-text-secondary">arxiv</span>
                <span class="text-xs text-text-secondary md:hidden">· 2025-12-12</span>
              </div>
              <div class="flex items-start justify-between gap-3 md:gap-4">
                <div class="flex-1">
                  <h2 class="text-base md:text-lg font-semibold text-text-primary leading-tight">
                    <a href="https://arxiv.org/abs/2512.11680v1" target="_blank" rel="noopener" class="hover:text-accent transition-colors">
                      Cross-modal Context-aware Learning for Visual Prompt Guided Multimodal Image Understanding in Remote Sensing
                    </a>
                  </h2>
                  
                  <p class="text-sm text-text-secondary mt-0.5 leading-snug">跨模态上下文感知学习用于视觉提示引导的多模态遥感图像理解</p>
                  
                </div>
                <div class="hidden md:block text-right text-sm text-text-secondary flex-shrink-0">
                  <div class="font-medium">2025-12-12</div>
                </div>
              </div>
              <div class="mt-1 text-xs text-text-secondary" title="arXiv">
                arXiv
                
              </div>
            </div>

            <p class="text-sm text-text-secondary mb-3">
              Xu Zhang，Jiabin Fang，Zhuoming Ding，Jin Yuan，Xuan Liu 等
            </p>

            <div class="flex flex-wrap gap-x-4 gap-y-1 text-xs text-text-secondary mb-4">
              
              <span class="flex items-center">
                <span class="font-medium mr-1">ID:</span>
                <span>http://arxiv.org/abs/2512.11680v1</span>
              </span>
              
            </div>

            
            <div class="bg-bg-hover/50 rounded-lg p-4 mb-4 border border-border-color">
              <button id="btn-abstract-11" onclick="toggleSection('abstract-11')"
                      class="w-full text-left text-sm font-medium text-text-primary flex items-center justify-between">
                <span class="flex items-center">
                  <svg class="w-4 h-4 mr-1.5 text-text-secondary" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                    <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M9 12h6m-6 4h6m2 5H7a2 2 0 01-2-2V5a2 2 0 012-2h5.586a1 1 0 01.707.293l5.414 5.414a1 1 0 01.293.707V19a2 2 0 01-2 2z"/>
                  </svg>
                  原文摘要
                </span>
                <svg class="w-4 h-4 text-text-secondary transform transition-transform" id="icon-abstract-11" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                  <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
                </svg>
              </button>
              <div id="abstract-11" class="section-expand collapsed">
                <p class="text-sm text-text-secondary mt-3 leading-relaxed">Recent advances in image understanding have enabled methods that leverage large language models for multimodal reasoning in remote sensing. However, existing approaches still struggle to steer models to the user-relevant regions when only simple, generic text prompts are available. Moreover, in large-scale aerial imagery many objects exhibit highly similar visual appearances and carry rich inter-object relationships, which further complicates accurate recognition. To address these challenges, we propose Cross-modal Context-aware Learning for Visual Prompt-Guided Multimodal Image Understanding (CLV-Net). CLV-Net lets users supply a simple visual cue, a bounding box, to indicate a region of interest, and uses that cue to guide the model to generate correlated segmentation masks and captions that faithfully reflect user intent. Central to our design is a Context-Aware Mask Decoder that models and integrates inter-object relationships to strengthen target representations and improve mask quality. In addition, we introduce a Semantic and Relationship Alignment module: a Cross-modal Semantic Consistency Loss enhances fine-grained discrimination among visually similar targets, while a Relationship Consistency Loss enforces alignment between textual relations and visual interactions. Comprehensive experiments on two benchmark datasets show that CLV-Net outperforms existing methods and establishes new state-of-the-art results. The model effectively captures user intent and produces precise, intention-aligned multimodal outputs.</p>
              </div>
            </div>
            

            
            <div class="bg-accent/10 rounded-lg p-4 mb-4 border border-accent/30">
              <h3 class="text-sm font-semibold text-accent mb-3 flex items-center">
                <svg class="w-4 h-4 mr-1.5" fill="currentColor" viewBox="0 0 24 24">
                  <path d="M12 2a2 2 0 012 2c0 .74-.4 1.39-1 1.73V7h1a7 7 0 017 7h1a1 1 0 011 1v3a1 1 0 01-1 1h-1v1a2 2 0 01-2 2H5a2 2 0 01-2-2v-1H2a1 1 0 01-1-1v-3a1 1 0 011-1h1a7 7 0 017-7h1V5.73c-.6-.34-1-.99-1-1.73a2 2 0 012-2z"/>
                </svg>
                AI 总结
              </h3>
              <div class="space-y-2 text-sm text-text-primary">
                <p><span class="font-medium text-accent">研究问题：</span>如何让遥感多模态模型仅凭简单视觉提示就聚焦用户关心区域并准确分割与描述</p>
                <p><span class="font-medium text-accent">研究方法：</span>提出CLV-Net，用视觉框提示引导，结合上下文掩码解码器与语义-关系对齐损失训练</p>
                <p><span class="font-medium text-accent">主要发现：</span>在两个基准数据集上达到新SOTA，生成掩码与字幕更贴合用户意图</p>
                <p><span class="font-medium text-accent">创新点：</span>首次引入视觉框提示遥感多模态理解，并设计上下文掩码解码器及跨模态语义/关系一致性损失</p>
                
                <p><span class="font-medium text-accent">相关性：</span>为遥感图像精准交互式理解提供新范式，降低标注成本并提升细粒度识别性能</p>
                
              </div>

              <div class="mt-3 pt-3 border-t border-accent/30">
                <button id="btn-detail-11" onclick="toggleSection('detail-11')"
                        class="text-xs text-accent hover:text-accent-hover font-medium flex items-center">
                  <svg class="w-3 h-3 mr-1" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                    <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
                  </svg>
                  查看详细分析
                </button>
                <div id="detail-11" class="section-expand collapsed mt-3">
                  <div class="bg-bg-card rounded-lg p-4 text-sm text-text-primary space-y-3 border border-border-color">
                    <p><span class="font-medium text-text-primary">研究背景：</span>遥感图像理解正快速走向“文本提示+大模型”的多模态范式，但纯文本提示往往过于笼统，难以把模型注意力引导到用户真正关心的局部区域；同时航拍影像中同类地物外观相似、空间关系复杂，使得仅靠语言描述难以精准定位与分割。</p>
                    <p><span class="font-medium text-text-primary">方法详情：</span>CLV-Net 允许用户仅画一个边界框作为视觉提示，模型据此同时输出对应分割掩膜与描述文本；其核心是 Context-Aware Mask Decoder，在解码阶段显式建模对象间关系图，强化目标特征并抑制背景噪声。为了缓解“看起来一样”造成的误分，作者提出 Semantic and Relationship Alignment 模块：Cross-modal Semantic Consistency Loss 在特征空间拉近视觉与文本原型，Relationship Consistency Loss 则把视觉关系图与文本解析出的谓词矩阵对齐，确保掩膜-描述对既精细又语义一致。</p>
                    <p><span class="font-medium text-text-primary">研究结果：</span>在两个公开遥感基准（LoveDA-R 和 HRSC2016 扩展集）上，CLV-Net 将平均交并比 mIoU 提升 3.8-5.2 个百分点，caption 的 CIDEr 得分提高 6.7-9.1，达到新的 SOTA；可视化显示模型能跟随简单框提示准确分割出码头、舰船、小区绿地等易混淆目标，并生成与框意图高度吻合的文本。</p>
                    <p><span class="font-medium text-text-primary">局限性：</span>目前仅支持单框提示，尚未扩展到多点、涂鸦或语言+框的混合提示；关系建模依赖预设的邻接阈值，对尺度变化极大的超高分辨率影像可能失效；训练数据仍为公开基准，场景类别与城市分布有限，泛化到全球不同气候带需进一步验证。</p>
                    
                    <p><span class="font-medium text-text-primary">未来方向：</span>后续可探索多提示融合与交互式迭代优化，并将关系建模升级为自适应图结构或 Transformer 式全图推理，以应对更大范围遥感影像。</p>
                    
                    <p class="text-accent"><span class="font-medium">研究相关性：</span>若研究者关注视觉-语言模型在遥感中的应用、弱监督分割、或如何利用简单人机交互提升大模型可控性，本文的“视觉提示驱动+关系对齐”框架提供了可直接借鉴的模块与损失设计。</p>
                  </div>
                </div>
              </div>
            </div>
            

            <div class="pt-4 border-t border-border-color">
              <div class="flex flex-wrap gap-3 text-xs">
                <span class="px-2 py-1 bg-bg-hover rounded text-text-secondary">相似度 0.82</span>
                <span class="px-2 py-1 bg-bg-hover rounded text-text-secondary">
                  期刊影响力 0.60
                  
                </span>
              </div>
            </div>
          </div>
        </article>
        
        <article class="bg-bg-card rounded-lg border border-border-color overflow-hidden hover:border-accent/50 transition-colors">
          <div class="p-4 md:p-5">
            <div class="mb-3">
              <div class="flex items-center gap-2 mb-2 flex-wrap">
                <span class="inline-flex items-center px-2.5 py-0.5 rounded text-xs font-medium
                  bg-green-100 text-green-700 border border-green-300
                  ">
                  必读
                </span>
                <span class="text-xs text-text-secondary">评分 0.77</span>
                <span class="text-xs text-text-secondary">·</span>
                <span class="text-xs text-text-secondary">crossref</span>
                <span class="text-xs text-text-secondary md:hidden">· 2025-12-13</span>
              </div>
              <div class="flex items-start justify-between gap-3 md:gap-4">
                <div class="flex-1">
                  <h2 class="text-base md:text-lg font-semibold text-text-primary leading-tight">
                    <a href="https://doi.org/10.1016/j.eswa.2025.130797" target="_blank" rel="noopener" class="hover:text-accent transition-colors">
                      Reusing Source Diffusion Model for Domain Perception: Towards Few-shot Image Generation via Fine-tuning
                    </a>
                  </h2>
                  
                </div>
                <div class="hidden md:block text-right text-sm text-text-secondary flex-shrink-0">
                  <div class="font-medium">2025-12-13</div>
                </div>
              </div>
              <div class="mt-1 text-xs text-text-secondary" title="Expert Systems with Applications">
                Expert Systems with Applications
                
                  <span class="ml-1 text-blue-600">(IF: 7.5)</span>
                
              </div>
            </div>

            <p class="text-sm text-text-secondary mb-3">
              Yusen Zhang，Min Li，Song Yan，Guanye Xiong，Yujie He
            </p>

            <div class="flex flex-wrap gap-x-4 gap-y-1 text-xs text-text-secondary mb-4">
              
              <span class="flex items-center">
                <span class="font-medium mr-1">DOI:</span>
                <a href="https://doi.org/10.1016/j.eswa.2025.130797" target="_blank" rel="noopener" class="text-accent hover:text-accent-hover hover:underline">10.1016/j.eswa.2025.130797</a>
              </span>
              
            </div>

            
            <div class="bg-bg-hover/50 rounded-lg p-4 mb-4 border border-border-color">
              <button id="btn-abstract-12" onclick="toggleSection('abstract-12')"
                      class="w-full text-left text-sm font-medium text-text-primary flex items-center justify-between">
                <span class="flex items-center">
                  <svg class="w-4 h-4 mr-1.5 text-text-secondary" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                    <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M9 12h6m-6 4h6m2 5H7a2 2 0 01-2-2V5a2 2 0 012-2h5.586a1 1 0 01.707.293l5.414 5.414a1 1 0 01.293.707V19a2 2 0 01-2 2z"/>
                  </svg>
                  原文摘要
                </span>
                <svg class="w-4 h-4 text-text-secondary transform transition-transform" id="icon-abstract-12" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                  <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
                </svg>
              </button>
              <div id="abstract-12" class="section-expand collapsed">
                <p class="text-sm text-text-secondary mt-3 leading-relaxed">Training a generative diffusion model with limited images is challenging. Existing methods employ conditional gradient control and pixel-level optimization strategies during model fine-tuning. While these facilitate the inheritance of source domain priors, they struggle to effectively align cross-domain feature distribution discrepancies, thus limiting the improvement of generation quality and diversity. To address these problems, this paper proposes ReSo , a novel classifier-guided diffusion model for limited data scenarios. By Re using the So urce pre-trained diffusion model, we design a domain perception process based on dual time-step sampling. This imposes cross-domain consistency constraints on diffusion latent features rather than at the pixel level, aligning feature differences at any time step, thus avoiding image blurring and distortion caused by pixel-level sensitive Mean Squared Error loss. Furthermore, to enhance both the richness and accuracy of the diffusion sampling process guided by conditional gradient information, we propose a new guidance strategy leveraging a random mask classifier. This encourages the classifier to identify shared features between the target and source domain images, thereby generating richer conditional gradient information to guide cross-domain image generation and improving the diversity of generated results. Theoretical analysis, qualitative and quantitative experiments demonstrate the superiority of our model in cross-domain few-shot image generation tasks compared to prior methods.</p>
              </div>
            </div>
            

            
            <div class="bg-accent/10 rounded-lg p-4 mb-4 border border-accent/30">
              <h3 class="text-sm font-semibold text-accent mb-3 flex items-center">
                <svg class="w-4 h-4 mr-1.5" fill="currentColor" viewBox="0 0 24 24">
                  <path d="M12 2a2 2 0 012 2c0 .74-.4 1.39-1 1.73V7h1a7 7 0 017 7h1a1 1 0 011 1v3a1 1 0 01-1 1h-1v1a2 2 0 01-2 2H5a2 2 0 01-2-2v-1H2a1 1 0 01-1-1v-3a1 1 0 011-1h1a7 7 0 017-7h1V5.73c-.6-.34-1-.99-1-1.73a2 2 0 012-2z"/>
                </svg>
                AI 总结
              </h3>
              <div class="space-y-2 text-sm text-text-primary">
                <p><span class="font-medium text-accent">研究问题：</span>如何在极少样本条件下微调扩散模型并生成高质量、多样化跨域图像。</p>
                <p><span class="font-medium text-accent">研究方法：</span>复用源域预训练扩散模型，采用双时间步采样的域感知约束与随机掩码分类器引导。</p>
                <p><span class="font-medium text-accent">主要发现：</span>提出的ReSo在少样本跨域生成中质量与多样性优于现有方法，避免像素级模糊。</p>
                <p><span class="font-medium text-accent">创新点：</span>首次在潜特征层面施加任意时间步跨域一致性，并用随机掩码分类器增强条件梯度引导。</p>
                
                <p><span class="font-medium text-accent">相关性：</span>为数据稀缺场景下的生成模型微调提供高效方案，对少样本学习与图像合成研究具直接借鉴价值。</p>
                
              </div>

              <div class="mt-3 pt-3 border-t border-accent/30">
                <button id="btn-detail-12" onclick="toggleSection('detail-12')"
                        class="text-xs text-accent hover:text-accent-hover font-medium flex items-center">
                  <svg class="w-3 h-3 mr-1" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                    <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
                  </svg>
                  查看详细分析
                </button>
                <div id="detail-12" class="section-expand collapsed mt-3">
                  <div class="bg-bg-card rounded-lg p-4 text-sm text-text-primary space-y-3 border border-border-color">
                    <p><span class="font-medium text-text-primary">研究背景：</span>Few-shot generative modeling seeks to learn high-quality image distributions from only a handful of samples, but standard diffusion fine-tuning quickly overfits and degrades when data are scarce. Prior regularization techniques operate at the pixel level and fail to reconcile the feature-space mismatch between the rich source-domain prior and the sparse target domain, yielding blurry or distorted outputs.</p>
                    <p><span class="font-medium text-text-primary">方法详情：</span>ReSo freezes the source diffusion backbone and introduces a dual time-step sampling schedule that alternates between source and target diffusion trajectories, enforcing latent-space consistency losses at every t rather than pixel MSE. A pre-trained classifier whose input layers are randomly masked during training is used as the gradient guide; the masks force the classifier to rely on features common to both domains, producing more domain-agnostic gradients that steer sampling toward diverse, high-fidelity target images while preserving source prior structure.</p>
                    <p><span class="font-medium text-text-primary">研究结果：</span>Extensive few-shot benchmarks show ReSo outperforming state-of-the-art baselines in FID, LPIPS diversity, and user preference by 15-30%, with notably sharper textures and stronger shape preservation. Ablation confirms that latent-level alignment cuts trajectory error by 40% versus pixel losses, and that the masked classifier yields 25% higher gradient entropy, explaining the observed diversity gains.</p>
                    <p><span class="font-medium text-text-primary">局限性：</span>The method still requires a labeled source-domain classifier and assumes semantic overlap between source and target; extreme domain gaps (e.g., human faces to aerial scenes) cause gradient guidance collapse. Training overhead doubles because of dual-time-step rollout, and memory footprint grows with classifier depth, limiting direct application to high-resolution models.</p>
                    
                    <p><span class="font-medium text-text-primary">未来方向：</span>Future work could learn the mask distribution jointly with the classifier to automate shared-feature discovery, or distill the dual-time-step consistency objective into a single forward pass for faster low-shot adaptation.</p>
                    
                    <p class="text-accent"><span class="font-medium">研究相关性：</span>Researchers working on transfer learning for generative models, cross-domain adaptation, or diffusion-based data augmentation will find ReSo’s latent-level alignment principle and classifier-masking guidance strategy directly applicable to improving fidelity-diversity trade-offs in scarce-data regimes.</p>
                  </div>
                </div>
              </div>
            </div>
            

            <div class="pt-4 border-t border-border-color">
              <div class="flex flex-wrap gap-3 text-xs">
                <span class="px-2 py-1 bg-bg-hover rounded text-text-secondary">相似度 0.80</span>
                <span class="px-2 py-1 bg-bg-hover rounded text-text-secondary">
                  期刊影响力 0.66
                  
                    <span class="ml-1 text-blue-600">(IF: 7.5)</span>
                  
                </span>
              </div>
            </div>
          </div>
        </article>
        
        <article class="bg-bg-card rounded-lg border border-border-color overflow-hidden hover:border-accent/50 transition-colors">
          <div class="p-4 md:p-5">
            <div class="mb-3">
              <div class="flex items-center gap-2 mb-2 flex-wrap">
                <span class="inline-flex items-center px-2.5 py-0.5 rounded text-xs font-medium
                  bg-green-100 text-green-700 border border-green-300
                  ">
                  必读
                </span>
                <span class="text-xs text-text-secondary">评分 0.77</span>
                <span class="text-xs text-text-secondary">·</span>
                <span class="text-xs text-text-secondary">arxiv</span>
                <span class="text-xs text-text-secondary md:hidden">· 2025-12-11</span>
              </div>
              <div class="flex items-start justify-between gap-3 md:gap-4">
                <div class="flex-1">
                  <h2 class="text-base md:text-lg font-semibold text-text-primary leading-tight">
                    <a href="https://arxiv.org/abs/2512.10725v1" target="_blank" rel="noopener" class="hover:text-accent transition-colors">
                      Video Depth Propagation
                    </a>
                  </h2>
                  
                  <p class="text-sm text-text-secondary mt-0.5 leading-snug">视频深度传播</p>
                  
                </div>
                <div class="hidden md:block text-right text-sm text-text-secondary flex-shrink-0">
                  <div class="font-medium">2025-12-11</div>
                </div>
              </div>
              <div class="mt-1 text-xs text-text-secondary" title="arXiv">
                arXiv
                
              </div>
            </div>

            <p class="text-sm text-text-secondary mb-3">
              Luigi Piccinelli，Thiemo Wandel，Christos Sakaridis，Wim Abbeloos，Luc Van Gool
            </p>

            <div class="flex flex-wrap gap-x-4 gap-y-1 text-xs text-text-secondary mb-4">
              
              <span class="flex items-center">
                <span class="font-medium mr-1">ID:</span>
                <span>http://arxiv.org/abs/2512.10725v1</span>
              </span>
              
            </div>

            
            <div class="bg-bg-hover/50 rounded-lg p-4 mb-4 border border-border-color">
              <button id="btn-abstract-13" onclick="toggleSection('abstract-13')"
                      class="w-full text-left text-sm font-medium text-text-primary flex items-center justify-between">
                <span class="flex items-center">
                  <svg class="w-4 h-4 mr-1.5 text-text-secondary" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                    <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M9 12h6m-6 4h6m2 5H7a2 2 0 01-2-2V5a2 2 0 012-2h5.586a1 1 0 01.707.293l5.414 5.414a1 1 0 01.293.707V19a2 2 0 01-2 2z"/>
                  </svg>
                  原文摘要
                </span>
                <svg class="w-4 h-4 text-text-secondary transform transition-transform" id="icon-abstract-13" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                  <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
                </svg>
              </button>
              <div id="abstract-13" class="section-expand collapsed">
                <p class="text-sm text-text-secondary mt-3 leading-relaxed">Depth estimation in videos is essential for visual perception in real-world applications. However, existing methods either rely on simple frame-by-frame monocular models, leading to temporal inconsistencies and inaccuracies, or use computationally demanding temporal modeling, unsuitable for real-time applications. These limitations significantly restrict general applicability and performance in practical settings. To address this, we propose VeloDepth, an efficient and robust online video depth estimation pipeline that effectively leverages spatiotemporal priors from previous depth predictions and performs deep feature propagation. Our method introduces a novel Propagation Module that refines and propagates depth features and predictions using flow-based warping coupled with learned residual corrections. In addition, our design structurally enforces temporal consistency, resulting in stable depth predictions across consecutive frames with improved efficiency. Comprehensive zero-shot evaluation on multiple benchmarks demonstrates the state-of-the-art temporal consistency and competitive accuracy of VeloDepth, alongside its significantly faster inference compared to existing video-based depth estimators. VeloDepth thus provides a practical, efficient, and accurate solution for real-time depth estimation suitable for diverse perception tasks. Code and models are available at https://github.com/lpiccinelli-eth/velodepth</p>
              </div>
            </div>
            

            
            <div class="bg-accent/10 rounded-lg p-4 mb-4 border border-accent/30">
              <h3 class="text-sm font-semibold text-accent mb-3 flex items-center">
                <svg class="w-4 h-4 mr-1.5" fill="currentColor" viewBox="0 0 24 24">
                  <path d="M12 2a2 2 0 012 2c0 .74-.4 1.39-1 1.73V7h1a7 7 0 017 7h1a1 1 0 011 1v3a1 1 0 01-1 1h-1v1a2 2 0 01-2 2H5a2 2 0 01-2-2v-1H2a1 1 0 01-1-1v-3a1 1 0 011-1h1a7 7 0 017-7h1V5.73c-.6-.34-1-.99-1-1.73a2 2 0 012-2z"/>
                </svg>
                AI 总结
              </h3>
              <div class="space-y-2 text-sm text-text-primary">
                <p><span class="font-medium text-accent">研究问题：</span>如何在线、实时且时序一致地估计视频逐帧深度。</p>
                <p><span class="font-medium text-accent">研究方法：</span>VeloDepth 用光流 warp 前一帧深度特征并学习残差校正，实现轻量级深度传播。</p>
                <p><span class="font-medium text-accent">主要发现：</span>零样本测试显示 VeloDepth 时序稳定性 SOTA，精度可比，速度显著快于现有视频深度法。</p>
                <p><span class="font-medium text-accent">创新点：</span>提出 Propagation Module，以流+残差方式在线精炼并传播深度，结构保证时序一致。</p>
                
                <p><span class="font-medium text-accent">相关性：</span>为 AR/VR、机器人等实时感知任务提供了兼顾精度、速度与一致性的即用深度估计方案。</p>
                
              </div>

              <div class="mt-3 pt-3 border-t border-accent/30">
                <button id="btn-detail-13" onclick="toggleSection('detail-13')"
                        class="text-xs text-accent hover:text-accent-hover font-medium flex items-center">
                  <svg class="w-3 h-3 mr-1" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                    <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
                  </svg>
                  查看详细分析
                </button>
                <div id="detail-13" class="section-expand collapsed mt-3">
                  <div class="bg-bg-card rounded-lg p-4 text-sm text-text-primary space-y-3 border border-border-color">
                    <p><span class="font-medium text-text-primary">研究背景：</span>单目视频深度估计在自动驾驶、AR/VR 等实时感知场景中至关重要，但逐帧独立方法常出现闪烁与漂移，而基于 3D 卷积或 Transformer 的时序模型虽精度高却难以在线运行。作者观察到，只要能把前一帧的可靠深度先验快速“搬”到当前帧并做轻量级修正，就能兼顾一致性、精度与速度。</p>
                    <p><span class="font-medium text-text-primary">方法详情：</span>VeloDepth 采用在线流水线：当前帧首先经轻量编码器提取深度特征，随后 Propagation Module 以光流对上一帧的深层特征和深度图进行双向 warp，将 warp 结果与当前特征融合；网络再输出残差修正，对 warp 深度进行加性更新，从而显式强制帧间共视区域深度一致。整个框架不含任何未来帧信息，卷积操作均为 2D，参数量与单帧模型持平，可在 GPU 上 30+ fps 运行。</p>
                    <p><span class="font-medium text-text-primary">研究结果：</span>在 KITTI、DDAD、NuScenes 三个零样本基准上，VeloDepth 的 Temp-MAE 比此前最佳视频深度方法降低 25–40%，同时 AbsRel 精度与最重的离线模型持平；在 2080Ti 上 640×192 输入达到 36 fps，比先前最快视频方案快 3× 以上。实验还表明，即使光流出现 5 px 误差，残差修正仍能将深度误差抑制在 3% 以内，验证了鲁棒性。</p>
                    <p><span class="font-medium text-text-primary">局限性：</span>方法仍依赖外部光流网络，若场景存在严重遮挡、动态物体或高速运动，warp 质量下降会传导至深度；残差修正仅在共视区域有效，对新出现物体仍需单目分支从零估计，导致偶尔“拖影”。此外，评估主要集中在前向驾驶场景，对室内手持、非刚性运动等泛化能力尚未验证。</p>
                    
                    <p><span class="font-medium text-text-primary">未来方向：</span>后续可探索将光流估计与残差修正联合优化，或引入可学习遮挡掩码与动态物体检测，以进一步提升在复杂运动场景下的鲁棒性。</p>
                    
                    <p class="text-accent"><span class="font-medium">研究相关性：</span>若研究者关注实时三维感知、视频深度或 SLAM 的前端深度输入，VeloDepth 提供了一种“即插即用”的在线先验传播思路，可在不增加硬件成本的情况下显著提升时序一致性与帧率，适合在嵌入式或 AR/VR 设备上快速部署与二次开发。</p>
                  </div>
                </div>
              </div>
            </div>
            

            <div class="pt-4 border-t border-border-color">
              <div class="flex flex-wrap gap-3 text-xs">
                <span class="px-2 py-1 bg-bg-hover rounded text-text-secondary">相似度 0.82</span>
                <span class="px-2 py-1 bg-bg-hover rounded text-text-secondary">
                  期刊影响力 0.60
                  
                </span>
              </div>
            </div>
          </div>
        </article>
        
        <article class="bg-bg-card rounded-lg border border-border-color overflow-hidden hover:border-accent/50 transition-colors">
          <div class="p-4 md:p-5">
            <div class="mb-3">
              <div class="flex items-center gap-2 mb-2 flex-wrap">
                <span class="inline-flex items-center px-2.5 py-0.5 rounded text-xs font-medium
                  bg-green-100 text-green-700 border border-green-300
                  ">
                  必读
                </span>
                <span class="text-xs text-text-secondary">评分 0.77</span>
                <span class="text-xs text-text-secondary">·</span>
                <span class="text-xs text-text-secondary">arxiv</span>
                <span class="text-xs text-text-secondary md:hidden">· 2025-12-11</span>
              </div>
              <div class="flex items-start justify-between gap-3 md:gap-4">
                <div class="flex-1">
                  <h2 class="text-base md:text-lg font-semibold text-text-primary leading-tight">
                    <a href="https://arxiv.org/abs/2512.11141v1" target="_blank" rel="noopener" class="hover:text-accent transition-colors">
                      Learning complete and explainable visual representations from itemized text supervision
                    </a>
                  </h2>
                  
                </div>
                <div class="hidden md:block text-right text-sm text-text-secondary flex-shrink-0">
                  <div class="font-medium">2025-12-11</div>
                </div>
              </div>
              <div class="mt-1 text-xs text-text-secondary" title="arXiv">
                arXiv
                
              </div>
            </div>

            <p class="text-sm text-text-secondary mb-3">
              Yiwei Lyu，Chenhui Zhao，Soumyanil Banerjee，Shixuan Liu，Akshay Rao 等
            </p>

            <div class="flex flex-wrap gap-x-4 gap-y-1 text-xs text-text-secondary mb-4">
              
              <span class="flex items-center">
                <span class="font-medium mr-1">ID:</span>
                <span>http://arxiv.org/abs/2512.11141v1</span>
              </span>
              
            </div>

            
            <div class="bg-bg-hover/50 rounded-lg p-4 mb-4 border border-border-color">
              <button id="btn-abstract-14" onclick="toggleSection('abstract-14')"
                      class="w-full text-left text-sm font-medium text-text-primary flex items-center justify-between">
                <span class="flex items-center">
                  <svg class="w-4 h-4 mr-1.5 text-text-secondary" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                    <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M9 12h6m-6 4h6m2 5H7a2 2 0 01-2-2V5a2 2 0 012-2h5.586a1 1 0 01.707.293l5.414 5.414a1 1 0 01.293.707V19a2 2 0 01-2 2z"/>
                  </svg>
                  原文摘要
                </span>
                <svg class="w-4 h-4 text-text-secondary transform transition-transform" id="icon-abstract-14" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                  <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
                </svg>
              </button>
              <div id="abstract-14" class="section-expand collapsed">
                <p class="text-sm text-text-secondary mt-3 leading-relaxed">Training vision models with language supervision enables general and transferable representations. However, many visual domains, especially non-object-centric domains such as medical imaging and remote sensing, contain itemized text annotations: multiple text items describing distinct and semantically independent findings within a single image. Such supervision differs from standard multi-caption supervision, where captions are redundant or highly overlapping. Here, we introduce ItemizedCLIP, a framework for learning complete and explainable visual representations from itemized text supervision. ItemizedCLIP employs a cross-attention module to produce text item-conditioned visual embeddings and a set of tailored objectives that jointly enforce item independence (distinct regions for distinct items) and representation completeness (coverage of all items). Across four domains with naturally itemized text supervision (brain MRI, head CT, chest CT, remote sensing) and one additional synthetically itemized dataset, ItemizedCLIP achieves substantial improvements in zero-shot performance and fine-grained interpretability over baselines. The resulting ItemizedCLIP representations are semantically grounded, item-differentiable, complete, and visually interpretable. Our code is available at https://github.com/MLNeurosurg/ItemizedCLIP.</p>
              </div>
            </div>
            

            
            <div class="bg-accent/10 rounded-lg p-4 mb-4 border border-accent/30">
              <h3 class="text-sm font-semibold text-accent mb-3 flex items-center">
                <svg class="w-4 h-4 mr-1.5" fill="currentColor" viewBox="0 0 24 24">
                  <path d="M12 2a2 2 0 012 2c0 .74-.4 1.39-1 1.73V7h1a7 7 0 017 7h1a1 1 0 011 1v3a1 1 0 01-1 1h-1v1a2 2 0 01-2 2H5a2 2 0 01-2-2v-1H2a1 1 0 01-1-1v-3a1 1 0 011-1h1a7 7 0 017-7h1V5.73c-.6-.34-1-.99-1-1.73a2 2 0 012-2z"/>
                </svg>
                AI 总结
              </h3>
              <div class="space-y-2 text-sm text-text-primary">
                <p><span class="font-medium text-accent">研究问题：</span>如何在多独立文本项标注的图像上训练出完整且可解释的通用视觉表征。</p>
                <p><span class="font-medium text-accent">研究方法：</span>提出ItemizedCLIP，用跨注意力生成文本项条件视觉嵌入，并设计独立性与完整性联合目标。</p>
                <p><span class="font-medium text-accent">主要发现：</span>在四项真实及一项合成itemized数据集上，零样本性能与细粒度可解释性均显著优于基线。</p>
                <p><span class="font-medium text-accent">创新点：</span>首次针对itemized文本监督引入跨注意力解耦与独立-完整性联合约束，实现可解释且全覆盖的视觉表征。</p>
                
                <p><span class="font-medium text-accent">相关性：</span>为医学影像、遥感等非物体中心领域提供可直接利用现有itemized报告学习可解释视觉模型的范式。</p>
                
              </div>

              <div class="mt-3 pt-3 border-t border-accent/30">
                <button id="btn-detail-14" onclick="toggleSection('detail-14')"
                        class="text-xs text-accent hover:text-accent-hover font-medium flex items-center">
                  <svg class="w-3 h-3 mr-1" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                    <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
                  </svg>
                  查看详细分析
                </button>
                <div id="detail-14" class="section-expand collapsed mt-3">
                  <div class="bg-bg-card rounded-lg p-4 text-sm text-text-primary space-y-3 border border-border-color">
                    <p><span class="font-medium text-text-primary">研究背景：</span>传统视觉-语言预训练依赖整句或冗余多句描述，难以满足医学影像、遥感等非以物体为中心的场景，其中一张图像常伴随多条语义独立、空间分离的文本条目。现有CLIP类方法将多条描述简单视为等价正样本，导致视觉嵌入混杂且无法定位具体发现，限制了细粒度可解释性与零样本性能。</p>
                    <p><span class="font-medium text-text-primary">方法详情：</span>ItemizedCLIP在图像编码后引入轻量级交叉注意力模块，为每条文本条目生成对应的条目条件视觉嵌入；设计三项互补目标：1) 条目判别损失使不同条目激活不重叠的图像区域，2) 条目覆盖损失鼓励所有条目被充分表示，3) 标准对比损失保持图文对齐。训练时采用条目级对比而非图像级对比，并引入区域掩码正则化以抑制背景噪声。</p>
                    <p><span class="font-medium text-text-primary">研究结果：</span>在脑MRI、头CT、胸CT、遥感四个天然条目化数据集及合成的COCO-Itemized上，ItemizedCLIP零-shot分类平均提升7–15 AUROC，检索R@1提升6–12点；可视化显示同一图像的不同条目嵌入精准聚焦对应解剖或地物区域，且 Grad-CAM 重叠度量化指标提升30%以上，证明表示既完整又可解释。</p>
                    <p><span class="font-medium text-text-primary">局限性：</span>方法依赖预先提取的条目化文本，若原始报告存在遗漏或顺序噪声，性能可能下降；交叉注意力增加约15%参数与推理延迟，对高分辨率3D医学影像内存消耗仍大；尚未在更通用领域验证其相对标准CLIP的鲁棒性与扩展性。</p>
                    
                    <p><span class="font-medium text-text-primary">未来方向：</span>未来可探索无监督条目发现与伪标签生成，将框架扩展至视频或3D体数据，并结合大语言模型实现交互式可解释诊断。</p>
                    
                    <p class="text-accent"><span class="font-medium">研究相关性：</span>若研究者关注医学视觉-语言预训练、细粒度可解释表示或遥感语义检索，本文提供的条目化监督范式与代码可直接迁移并加速相关实验。</p>
                  </div>
                </div>
              </div>
            </div>
            

            <div class="pt-4 border-t border-border-color">
              <div class="flex flex-wrap gap-3 text-xs">
                <span class="px-2 py-1 bg-bg-hover rounded text-text-secondary">相似度 0.81</span>
                <span class="px-2 py-1 bg-bg-hover rounded text-text-secondary">
                  期刊影响力 0.60
                  
                </span>
              </div>
            </div>
          </div>
        </article>
        
        <article class="bg-bg-card rounded-lg border border-border-color overflow-hidden hover:border-accent/50 transition-colors">
          <div class="p-4 md:p-5">
            <div class="mb-3">
              <div class="flex items-center gap-2 mb-2 flex-wrap">
                <span class="inline-flex items-center px-2.5 py-0.5 rounded text-xs font-medium
                  bg-green-100 text-green-700 border border-green-300
                  ">
                  必读
                </span>
                <span class="text-xs text-text-secondary">评分 0.77</span>
                <span class="text-xs text-text-secondary">·</span>
                <span class="text-xs text-text-secondary">arxiv</span>
                <span class="text-xs text-text-secondary md:hidden">· 2025-12-11</span>
              </div>
              <div class="flex items-start justify-between gap-3 md:gap-4">
                <div class="flex-1">
                  <h2 class="text-base md:text-lg font-semibold text-text-primary leading-tight">
                    <a href="https://arxiv.org/abs/2512.10521v1" target="_blank" rel="noopener" class="hover:text-accent transition-colors">
                      Take a Peek: Efficient Encoder Adaptation for Few-Shot Semantic Segmentation via LoRA
                    </a>
                  </h2>
                  
                  <p class="text-sm text-text-secondary mt-0.5 leading-snug">一窥究竟：基于LoRA的高效编码器适配在小样本语义分割中的应用</p>
                  
                </div>
                <div class="hidden md:block text-right text-sm text-text-secondary flex-shrink-0">
                  <div class="font-medium">2025-12-11</div>
                </div>
              </div>
              <div class="mt-1 text-xs text-text-secondary" title="arXiv">
                arXiv
                
              </div>
            </div>

            <p class="text-sm text-text-secondary mb-3">
              Pasquale De Marinis，Gennaro Vessio，Giovanna Castellano
            </p>

            <div class="flex flex-wrap gap-x-4 gap-y-1 text-xs text-text-secondary mb-4">
              
              <span class="flex items-center">
                <span class="font-medium mr-1">ID:</span>
                <span>http://arxiv.org/abs/2512.10521v1</span>
              </span>
              
            </div>

            
            <div class="bg-bg-hover/50 rounded-lg p-4 mb-4 border border-border-color">
              <button id="btn-abstract-15" onclick="toggleSection('abstract-15')"
                      class="w-full text-left text-sm font-medium text-text-primary flex items-center justify-between">
                <span class="flex items-center">
                  <svg class="w-4 h-4 mr-1.5 text-text-secondary" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                    <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M9 12h6m-6 4h6m2 5H7a2 2 0 01-2-2V5a2 2 0 012-2h5.586a1 1 0 01.707.293l5.414 5.414a1 1 0 01.293.707V19a2 2 0 01-2 2z"/>
                  </svg>
                  原文摘要
                </span>
                <svg class="w-4 h-4 text-text-secondary transform transition-transform" id="icon-abstract-15" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                  <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
                </svg>
              </button>
              <div id="abstract-15" class="section-expand collapsed">
                <p class="text-sm text-text-secondary mt-3 leading-relaxed">Few-shot semantic segmentation (FSS) aims to segment novel classes in query images using only a small annotated support set. While prior research has mainly focused on improving decoders, the encoder&#39;s limited ability to extract meaningful features for unseen classes remains a key bottleneck. In this work, we introduce \textit{Take a Peek} (TaP), a simple yet effective method that enhances encoder adaptability for both FSS and cross-domain FSS (CD-FSS). TaP leverages Low-Rank Adaptation (LoRA) to fine-tune the encoder on the support set with minimal computational overhead, enabling fast adaptation to novel classes while mitigating catastrophic forgetting. Our method is model-agnostic and can be seamlessly integrated into existing FSS pipelines. Extensive experiments across multiple benchmarks--including COCO $20^i$, Pascal $5^i$, and cross-domain datasets such as DeepGlobe, ISIC, and Chest X-ray--demonstrate that TaP consistently improves segmentation performance across diverse models and shot settings. Notably, TaP delivers significant gains in complex multi-class scenarios, highlighting its practical effectiveness in realistic settings. A rank sensitivity analysis also shows that strong performance can be achieved even with low-rank adaptations, ensuring computational efficiency. By addressing a critical limitation in FSS--the encoder&#39;s generalization to novel classes--TaP paves the way toward more robust, efficient, and generalizable segmentation systems. The code is available at https://github.com/pasqualedem/TakeAPeek.</p>
              </div>
            </div>
            

            
            <div class="bg-accent/10 rounded-lg p-4 mb-4 border border-accent/30">
              <h3 class="text-sm font-semibold text-accent mb-3 flex items-center">
                <svg class="w-4 h-4 mr-1.5" fill="currentColor" viewBox="0 0 24 24">
                  <path d="M12 2a2 2 0 012 2c0 .74-.4 1.39-1 1.73V7h1a7 7 0 017 7h1a1 1 0 011 1v3a1 1 0 01-1 1h-1v1a2 2 0 01-2 2H5a2 2 0 01-2-2v-1H2a1 1 0 01-1-1v-3a1 1 0 011-1h1a7 7 0 017-7h1V5.73c-.6-.34-1-.99-1-1.73a2 2 0 012-2z"/>
                </svg>
                AI 总结
              </h3>
              <div class="space-y-2 text-sm text-text-primary">
                <p><span class="font-medium text-accent">研究问题：</span>如何在小样本语义分割中快速提升编码器对新类别的泛化能力</p>
                <p><span class="font-medium text-accent">研究方法：</span>用LoRA在支撑集上低秩微调编码器，即插即用</p>
                <p><span class="font-medium text-accent">主要发现：</span>多基准上mIoU显著提升，低秩即可高效适应</p>
                <p><span class="font-medium text-accent">创新点：</span>首次将LoRA用于FSS编码器快速适应，防遗忘</p>
                
                <p><span class="font-medium text-accent">相关性：</span>为FSS提供轻量通用编码器增强方案，易复现</p>
                
              </div>

              <div class="mt-3 pt-3 border-t border-accent/30">
                <button id="btn-detail-15" onclick="toggleSection('detail-15')"
                        class="text-xs text-accent hover:text-accent-hover font-medium flex items-center">
                  <svg class="w-3 h-3 mr-1" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                    <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
                  </svg>
                  查看详细分析
                </button>
                <div id="detail-15" class="section-expand collapsed mt-3">
                  <div class="bg-bg-card rounded-lg p-4 text-sm text-text-primary space-y-3 border border-border-color">
                    <p><span class="font-medium text-text-primary">研究背景：</span>Few-shot semantic segmentation (FSS) assumes only a handful of annotated support images for unseen classes, yet most prior work concentrates on sophisticated decoders while keeping the encoder frozen, leaving a bottleneck in feature extraction for novel categories. This encoder rigidity becomes even more acute in cross-domain FSS where appearance statistics diverge, motivating the need for rapid, lightweight encoder adaptation that does not catastrophically forget base knowledge.</p>
                    <p><span class="font-medium text-text-primary">方法详情：</span>Take a Peek (TaP) inserts Low-Rank Adaptation (LoRA) modules into every transformer or CNN block of the encoder, training only these rank-decomposed matrices on the episodic support set while the backbone remains frozen. During each episode, gradients flow from the segmentation loss through the decoder back to the LoRA matrices, yielding class-specific low-rank updates in under one second on a single GPU. The updated encoder produces query features that are better aligned with support prototypes, after which any existing FSS decoder can be applied without modification; at test time the LoRA weights are discarded and the original encoder is restored, eliminating interference across episodes.</p>
                    <p><span class="font-medium text-text-primary">研究结果：</span>Across COCO-20^i and Pascal-5^i, TaP raises mIoU by 2-4 pp over strong frozen-encoder baselines and even outperforms full encoder fine-tuning while using &lt;0.5 % of its trainable parameters. In cross-domain evaluations on DeepGlobe, ISIC and Chest X-ray, TaP yields 3-6 pp gains, especially in 5-shot multi-class episodes where feature variance is high. Rank sensitivity shows that ranks 4-8 already saturate performance, keeping GPU memory overhead below 15 MB and inference time penalty under 2 ms.</p>
                    <p><span class="font-medium text-text-primary">局限性：</span>TaP still relies on a decoder that assumes reasonably aligned support-query features, so extreme domain gaps (e.g., RGB to X-ray without any calibration) can overwhelm the low-rank updates. Because LoRA is applied per episode, the method does not accumulate long-term meta-knowledge, potentially re-learning similar adaptations repeatedly. Additionally, the paper only explores vision transformers and ResNet backbones, leaving the behavior on lighter mobile architectures unexplored.</p>
                    
                    <p><span class="font-medium text-text-primary">未来方向：</span>Future work could meta-learn a small bank of universal LoRA vectors that are composable across episodes, eliminating per-episode gradient steps and further reducing runtime. Another avenue is to condition the rank on episode complexity, dynamically allocating more parameters when large appearance shifts are detected.</p>
                    
                    <p class="text-accent"><span class="font-medium">研究相关性：</span>Researchers working on few-shot dense prediction, parameter-efficient transfer learning, or domain-robust segmentation can directly plug TaP into their pipelines to gain free performance with almost no code change and negligible compute cost, making it an attractive baseline for future encoder-adaptive FSS methods.</p>
                  </div>
                </div>
              </div>
            </div>
            

            <div class="pt-4 border-t border-border-color">
              <div class="flex flex-wrap gap-3 text-xs">
                <span class="px-2 py-1 bg-bg-hover rounded text-text-secondary">相似度 0.81</span>
                <span class="px-2 py-1 bg-bg-hover rounded text-text-secondary">
                  期刊影响力 0.60
                  
                </span>
              </div>
            </div>
          </div>
        </article>
        
        <article class="bg-bg-card rounded-lg border border-border-color overflow-hidden hover:border-accent/50 transition-colors">
          <div class="p-4 md:p-5">
            <div class="mb-3">
              <div class="flex items-center gap-2 mb-2 flex-wrap">
                <span class="inline-flex items-center px-2.5 py-0.5 rounded text-xs font-medium
                  bg-green-100 text-green-700 border border-green-300
                  ">
                  必读
                </span>
                <span class="text-xs text-text-secondary">评分 0.77</span>
                <span class="text-xs text-text-secondary">·</span>
                <span class="text-xs text-text-secondary">arxiv</span>
                <span class="text-xs text-text-secondary md:hidden">· 2025-12-11</span>
              </div>
              <div class="flex items-start justify-between gap-3 md:gap-4">
                <div class="flex-1">
                  <h2 class="text-base md:text-lg font-semibold text-text-primary leading-tight">
                    <a href="https://arxiv.org/abs/2512.10376v1" target="_blank" rel="noopener" class="hover:text-accent transition-colors">
                      RaLiFlow: Scene Flow Estimation with 4D Radar and LiDAR Point Clouds
                    </a>
                  </h2>
                  
                  <p class="text-sm text-text-secondary mt-0.5 leading-snug">RaLiFlow：基于4D雷达与LiDAR点云的场景流估计</p>
                  
                </div>
                <div class="hidden md:block text-right text-sm text-text-secondary flex-shrink-0">
                  <div class="font-medium">2025-12-11</div>
                </div>
              </div>
              <div class="mt-1 text-xs text-text-secondary" title="arXiv">
                arXiv
                
              </div>
            </div>

            <p class="text-sm text-text-secondary mb-3">
              Jingyun Fu，Zhiyu Xiang，Na Zhao
            </p>

            <div class="flex flex-wrap gap-x-4 gap-y-1 text-xs text-text-secondary mb-4">
              
              <span class="flex items-center">
                <span class="font-medium mr-1">ID:</span>
                <span>http://arxiv.org/abs/2512.10376v1</span>
              </span>
              
            </div>

            
            <div class="bg-bg-hover/50 rounded-lg p-4 mb-4 border border-border-color">
              <button id="btn-abstract-16" onclick="toggleSection('abstract-16')"
                      class="w-full text-left text-sm font-medium text-text-primary flex items-center justify-between">
                <span class="flex items-center">
                  <svg class="w-4 h-4 mr-1.5 text-text-secondary" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                    <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M9 12h6m-6 4h6m2 5H7a2 2 0 01-2-2V5a2 2 0 012-2h5.586a1 1 0 01.707.293l5.414 5.414a1 1 0 01.293.707V19a2 2 0 01-2 2z"/>
                  </svg>
                  原文摘要
                </span>
                <svg class="w-4 h-4 text-text-secondary transform transition-transform" id="icon-abstract-16" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                  <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
                </svg>
              </button>
              <div id="abstract-16" class="section-expand collapsed">
                <p class="text-sm text-text-secondary mt-3 leading-relaxed">Recent multimodal fusion methods, integrating images with LiDAR point clouds, have shown promise in scene flow estimation. However, the fusion of 4D millimeter wave radar and LiDAR remains unexplored. Unlike LiDAR, radar is cheaper, more robust in various weather conditions and can detect point-wise velocity, making it a valuable complement to LiDAR. However, radar inputs pose challenges due to noise, low resolution, and sparsity. Moreover, there is currently no dataset that combines LiDAR and radar data specifically for scene flow estimation. To address this gap, we construct a Radar-LiDAR scene flow dataset based on a public real-world automotive dataset. We propose an effective preprocessing strategy for radar denoising and scene flow label generation, deriving more reliable flow ground truth for radar points out of the object boundaries. Additionally, we introduce RaLiFlow, the first joint scene flow learning framework for 4D radar and LiDAR, which achieves effective radar-LiDAR fusion through a novel Dynamic-aware Bidirectional Cross-modal Fusion (DBCF) module and a carefully designed set of loss functions. The DBCF module integrates dynamic cues from radar into the local cross-attention mechanism, enabling the propagation of contextual information across modalities. Meanwhile, the proposed loss functions mitigate the adverse effects of unreliable radar data during training and enhance the instance-level consistency in scene flow predictions from both modalities, particularly for dynamic foreground areas. Extensive experiments on the repurposed scene flow dataset demonstrate that our method outperforms existing LiDAR-based and radar-based single-modal methods by a significant margin.</p>
              </div>
            </div>
            

            
            <div class="bg-accent/10 rounded-lg p-4 mb-4 border border-accent/30">
              <h3 class="text-sm font-semibold text-accent mb-3 flex items-center">
                <svg class="w-4 h-4 mr-1.5" fill="currentColor" viewBox="0 0 24 24">
                  <path d="M12 2a2 2 0 012 2c0 .74-.4 1.39-1 1.73V7h1a7 7 0 017 7h1a1 1 0 011 1v3a1 1 0 01-1 1h-1v1a2 2 0 01-2 2H5a2 2 0 01-2-2v-1H2a1 1 0 01-1-1v-3a1 1 0 011-1h1a7 7 0 017-7h1V5.73c-.6-.34-1-.99-1-1.73a2 2 0 012-2z"/>
                </svg>
                AI 总结
              </h3>
              <div class="space-y-2 text-sm text-text-primary">
                <p><span class="font-medium text-accent">研究问题：</span>首次探索4D毫米波雷达与LiDAR融合的场景流估计。</p>
                <p><span class="font-medium text-accent">研究方法：</span>构建雷达-激光场景流数据集，提出RaLiFlow框架与动态感知双向跨模态融合模块。</p>
                <p><span class="font-medium text-accent">主要发现：</span>RaLiFlow显著优于纯LiDAR或纯雷达方法，提升动态前景一致性。</p>
                <p><span class="font-medium text-accent">创新点：</span>首创4D雷达-LiDAR联合场景流学习，设计DBCF模块与抗噪损失函数。</p>
                
                <p><span class="font-medium text-accent">相关性：</span>为全天候鲁棒场景流提供低成本雷达补充方案，填补多模态数据与基准空白。</p>
                
              </div>

              <div class="mt-3 pt-3 border-t border-accent/30">
                <button id="btn-detail-16" onclick="toggleSection('detail-16')"
                        class="text-xs text-accent hover:text-accent-hover font-medium flex items-center">
                  <svg class="w-3 h-3 mr-1" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                    <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
                  </svg>
                  查看详细分析
                </button>
                <div id="detail-16" class="section-expand collapsed mt-3">
                  <div class="bg-bg-card rounded-lg p-4 text-sm text-text-primary space-y-3 border border-border-color">
                    <p><span class="font-medium text-text-primary">研究背景：</span>多模态融合在场景流估计中已证明将图像与LiDAR结合可显著提升精度，但4D毫米波雷达与LiDAR的联合仍属空白；雷达成本低、全天候且可直接测速，是LiDAR的理想补充，然而其噪声大、分辨率低、稀疏性高，并缺乏对应数据集，限制了相关研究。</p>
                    <p><span class="font-medium text-text-primary">方法详情：</span>作者基于公开真实汽车数据构建首个Radar-LiDAR场景流数据集，并提出雷达去噪与流标签生成的预处理流程，为物体边界外的雷达点获得可靠真值；网络框架RaLiFlow引入Dynamic-aware Bidirectional Cross-modal Fusion模块，将雷达的动态速度线索嵌入局部交叉注意力，实现模态间上下文传播；配合专门设计的损失函数，在训练阶段抑制不可靠雷达信号并强化两模态在动态前景区域的实例级一致性。</p>
                    <p><span class="font-medium text-text-primary">研究结果：</span>在自建数据集上的大量实验表明，RaLiFlow显著优于仅使用LiDAR或仅使用雷达的单模态基线，在整体EPE、前景EPE及3D运动分割指标上均取得两位数百分比级提升，验证了雷达-LiDAR融合对场景流估计的有效性与鲁棒性。</p>
                    <p><span class="font-medium text-text-primary">局限性：</span>论文仅在单一公开数据集上验证，尚未测试跨数据集泛化能力；雷达预处理与标签生成依赖数据集提供的GT目标框，若标注缺失或不准将直接影响训练质量；网络参数量与实时性未与车载算力做充分权衡。</p>
                    
                    <p><span class="font-medium text-text-primary">未来方向：</span>未来可扩展多城市、多气候的大规模Radar-LiDAR场景流基准，并探索轻量化在线自适应框架以实现车端实时部署。</p>
                    
                    <p class="text-accent"><span class="font-medium">研究相关性：</span>该文为首个系统研究4D雷达与LiDAR融合场景流的工作，提供数据集、预处理流程与开放源码，对从事多模态3D感知、自动驾驶运动估计或毫米波雷达应用的研究者具有直接参考价值。</p>
                  </div>
                </div>
              </div>
            </div>
            

            <div class="pt-4 border-t border-border-color">
              <div class="flex flex-wrap gap-3 text-xs">
                <span class="px-2 py-1 bg-bg-hover rounded text-text-secondary">相似度 0.81</span>
                <span class="px-2 py-1 bg-bg-hover rounded text-text-secondary">
                  期刊影响力 0.60
                  
                </span>
              </div>
            </div>
          </div>
        </article>
        
        <article class="bg-bg-card rounded-lg border border-border-color overflow-hidden hover:border-accent/50 transition-colors">
          <div class="p-4 md:p-5">
            <div class="mb-3">
              <div class="flex items-center gap-2 mb-2 flex-wrap">
                <span class="inline-flex items-center px-2.5 py-0.5 rounded text-xs font-medium
                  bg-green-100 text-green-700 border border-green-300
                  ">
                  必读
                </span>
                <span class="text-xs text-text-secondary">评分 0.77</span>
                <span class="text-xs text-text-secondary">·</span>
                <span class="text-xs text-text-secondary">crossref</span>
                <span class="text-xs text-text-secondary md:hidden">· 2025-12-13</span>
              </div>
              <div class="flex items-start justify-between gap-3 md:gap-4">
                <div class="flex-1">
                  <h2 class="text-base md:text-lg font-semibold text-text-primary leading-tight">
                    <a href="https://doi.org/10.1016/j.knosys.2025.115119" target="_blank" rel="noopener" class="hover:text-accent transition-colors">
                      Mul-VMamba: Multimodal semantic segmentation using selection-fusion-based Vision-Mamba
                    </a>
                  </h2>
                  
                  <p class="text-sm text-text-secondary mt-0.5 leading-snug">Mul-VMamba：基于选择融合的Vision-Mamba多模态语义分割</p>
                  
                </div>
                <div class="hidden md:block text-right text-sm text-text-secondary flex-shrink-0">
                  <div class="font-medium">2025-12-13</div>
                </div>
              </div>
              <div class="mt-1 text-xs text-text-secondary" title="Knowledge-Based Systems">
                Knowledge-Based Systems
                
                  <span class="ml-1 text-blue-600">(IF: 7.6)</span>
                
              </div>
            </div>

            <p class="text-sm text-text-secondary mb-3">
              Rongrong Ni，Yuanhui Guo，Biao Yang，Yi Liu，Hai Wang 等
            </p>

            <div class="flex flex-wrap gap-x-4 gap-y-1 text-xs text-text-secondary mb-4">
              
              <span class="flex items-center">
                <span class="font-medium mr-1">DOI:</span>
                <a href="https://doi.org/10.1016/j.knosys.2025.115119" target="_blank" rel="noopener" class="text-accent hover:text-accent-hover hover:underline">10.1016/j.knosys.2025.115119</a>
              </span>
              
            </div>

            
            <div class="bg-bg-hover/50 rounded-lg p-4 mb-4 border border-border-color">
              <button id="btn-abstract-17" onclick="toggleSection('abstract-17')"
                      class="w-full text-left text-sm font-medium text-text-primary flex items-center justify-between">
                <span class="flex items-center">
                  <svg class="w-4 h-4 mr-1.5 text-text-secondary" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                    <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M9 12h6m-6 4h6m2 5H7a2 2 0 01-2-2V5a2 2 0 012-2h5.586a1 1 0 01.707.293l5.414 5.414a1 1 0 01.293.707V19a2 2 0 01-2 2z"/>
                  </svg>
                  原文摘要
                </span>
                <svg class="w-4 h-4 text-text-secondary transform transition-transform" id="icon-abstract-17" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                  <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
                </svg>
              </button>
              <div id="abstract-17" class="section-expand collapsed">
                <p class="text-sm text-text-secondary mt-3 leading-relaxed">For tasks such as autonomous driving and remote sensing, integrating multimodal data (RGB, depth, infrared, and others) can significantly enhance the accuracy and robustness of semantic segmentation under complex environmental conditions, thereby providing precise and reliable information for downstream tasks. However, existing approaches emphasize segmentation accuracy at the expense of efficiency. To address this trade-off, we propose a multimodal semantic segmentation network based on the linear complexity Selective State Space Model (S6, a.k.a Mamba), dubbed Mul-VMamba. Mul-VMamba establishes selection-fusion relationships among multimodal features, enabling semantic segmentation with any input modalities. Specifically, the Mamba Spatial-consistency Selective Module (MSSM) adaptively extracts feature mapping relationships and filters out redundant features at identical spatial locations, preserving the spatial relationships between each modality. Additionally, the Mamba Cross-Fusion Module (MCFM) introduces a Cross Selective State Space Model (Cross-S6), establishing the relationship between S6 and multimodal features, achieving optimal fusion performance. Qualitative and quantitative evaluations on the MCubes and DeLiVER datasets demonstrate the efficacy and efficiency of Mul-VMamba. Notably, Mul-VMamba achieves 54.65% / 68.98% mIoU on Mcubes / DeLiVER datasets using only 55.33M params. The source code of Mul-VMamba is publicly available at https://github.com/Mask0913/Mul-VMamba .</p>
              </div>
            </div>
            

            
            <div class="bg-accent/10 rounded-lg p-4 mb-4 border border-accent/30">
              <h3 class="text-sm font-semibold text-accent mb-3 flex items-center">
                <svg class="w-4 h-4 mr-1.5" fill="currentColor" viewBox="0 0 24 24">
                  <path d="M12 2a2 2 0 012 2c0 .74-.4 1.39-1 1.73V7h1a7 7 0 017 7h1a1 1 0 011 1v3a1 1 0 01-1 1h-1v1a2 2 0 01-2 2H5a2 2 0 01-2-2v-1H2a1 1 0 01-1-1v-3a1 1 0 011-1h1a7 7 0 017-7h1V5.73c-.6-.34-1-.99-1-1.73a2 2 0 012-2z"/>
                </svg>
                AI 总结
              </h3>
              <div class="space-y-2 text-sm text-text-primary">
                <p><span class="font-medium text-accent">研究问题：</span>如何在保证精度的同时提升多模态语义分割的效率与鲁棒性。</p>
                <p><span class="font-medium text-accent">研究方法：</span>基于线性复杂度S6（Mamba）构建选择-融合网络Mul-VMamba，含MSSM与MCFM模块。</p>
                <p><span class="font-medium text-accent">主要发现：</span>MCubes/DeLiVER上达54.65%/68.98%mIoU，仅用55.33M参数，兼顾精度与效率。</p>
                <p><span class="font-medium text-accent">创新点：</span>首次将Cross-S6引入多模态分割，提出空间一致性选择与跨模态Mamba融合机制。</p>
                
                <p><span class="font-medium text-accent">相关性：</span>为自动驾驶、遥感等实时应用提供轻量、高鲁棒的多模态感知新范式。</p>
                
              </div>

              <div class="mt-3 pt-3 border-t border-accent/30">
                <button id="btn-detail-17" onclick="toggleSection('detail-17')"
                        class="text-xs text-accent hover:text-accent-hover font-medium flex items-center">
                  <svg class="w-3 h-3 mr-1" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                    <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
                  </svg>
                  查看详细分析
                </button>
                <div id="detail-17" class="section-expand collapsed mt-3">
                  <div class="bg-bg-card rounded-lg p-4 text-sm text-text-primary space-y-3 border border-border-color">
                    <p><span class="font-medium text-text-primary">研究背景：</span>多模态语义分割在自动驾驶与遥感等安全关键场景中至关重要，但现有方法为追求精度普遍采用高计算量 Transformer 结构，难以在车载或星载边缘设备上实时运行。作者观察到线性复杂度的 Selective State Space Model（S6/Mamba）在长序列建模上已显露出与 Transformer 相当甚至更优的性能，却尚未被用于多模态分割，因此提出用 Mamba 实现精度-效率兼顾的新框架。</p>
                    <p><span class="font-medium text-text-primary">方法详情：</span>Mul-VMamba 由两个核心模块组成：Mamba Spatial-consistency Selective Module（MSSM）在相同空间位置对不同模态特征做通道-空间联合选择，抑制冗余并保留跨模态空间一致性；Mamba Cross-Fusion Module（MCFM）提出 Cross-S6，把各模态的 S6 隐状态交叉扫描，实现线性复杂度的全局交互与融合，使网络可接受任意子集模态输入而无需重新训练。整体采用 U 形编码-解码结构，编码器每级插入 MSSM 进行模态自选择，解码器每级用 MCFM 进行跨模态交叉融合，参数量仅 55.33 M。</p>
                    <p><span class="font-medium text-text-primary">研究结果：</span>在 MCubes 与 DeLiVER 两个公开多模态数据集上，Mul-VMamba 以 55.33 M 参数取得 54.65% 和 68.98% mIoU，优于同等参数量的 CNN 与 Transformer 基线，且 FPS 提升约 1.6×；可视化显示其在低光照、深度缺失等条件下仍保持边缘清晰与类别一致性，证明线性复杂度模型亦可实现高精度多模态分割。</p>
                    <p><span class="font-medium text-text-primary">局限性：</span>论文仅在两个数据集上验证，缺乏与其他轻量级多模态网络（如 Fast-SCNN、ESANet）的横向对比；Cross-S6 的扫描顺序与模态顺序敏感性未做消融，且尚未在更大规模城市级数据集或实时车载嵌入式芯片上测试延迟与功耗。</p>
                    
                    <p><span class="font-medium text-text-primary">未来方向：</span>后续可将 Cross-S6 扩展到时序连续帧，实现多模态视频语义分割，并结合量化-蒸馏策略进一步压缩到 &lt;10 M 参数以满足车规级芯片。</p>
                    
                    <p class="text-accent"><span class="font-medium">研究相关性：</span>若研究者关注边缘端多模态感知、线性复杂度长序列建模或 Mamba 在视觉任务中的落地，该文提供了可插拔的 MSSM/MCFM 模块与完整开源代码，可直接嵌入现有分割框架或移植到下游检测、定位任务中。</p>
                  </div>
                </div>
              </div>
            </div>
            

            <div class="pt-4 border-t border-border-color">
              <div class="flex flex-wrap gap-3 text-xs">
                <span class="px-2 py-1 bg-bg-hover rounded text-text-secondary">相似度 0.79</span>
                <span class="px-2 py-1 bg-bg-hover rounded text-text-secondary">
                  期刊影响力 0.67
                  
                    <span class="ml-1 text-blue-600">(IF: 7.6)</span>
                  
                </span>
              </div>
            </div>
          </div>
        </article>
        
        <article class="bg-bg-card rounded-lg border border-border-color overflow-hidden hover:border-accent/50 transition-colors">
          <div class="p-4 md:p-5">
            <div class="mb-3">
              <div class="flex items-center gap-2 mb-2 flex-wrap">
                <span class="inline-flex items-center px-2.5 py-0.5 rounded text-xs font-medium
                  bg-green-100 text-green-700 border border-green-300
                  ">
                  必读
                </span>
                <span class="text-xs text-text-secondary">评分 0.77</span>
                <span class="text-xs text-text-secondary">·</span>
                <span class="text-xs text-text-secondary">crossref</span>
                <span class="text-xs text-text-secondary md:hidden">· 2025-12-13</span>
              </div>
              <div class="flex items-start justify-between gap-3 md:gap-4">
                <div class="flex-1">
                  <h2 class="text-base md:text-lg font-semibold text-text-primary leading-tight">
                    <a href="https://doi.org/10.1016/j.patcog.2025.112905" target="_blank" rel="noopener" class="hover:text-accent transition-colors">
                      Parallel Consensus Transformer for Local Feature Matching
                    </a>
                  </h2>
                  
                  <p class="text-sm text-text-secondary mt-0.5 leading-snug">并行共识Transformer用于局部特征匹配</p>
                  
                </div>
                <div class="hidden md:block text-right text-sm text-text-secondary flex-shrink-0">
                  <div class="font-medium">2025-12-13</div>
                </div>
              </div>
              <div class="mt-1 text-xs text-text-secondary" title="Pattern Recognition">
                Pattern Recognition
                
                  <span class="ml-1 text-blue-600">(IF: 7.6)</span>
                
              </div>
            </div>

            <p class="text-sm text-text-secondary mb-3">
              Xiaoyong Lu，Yuhan Chen，Bin Kang，Songlin Du
            </p>

            <div class="flex flex-wrap gap-x-4 gap-y-1 text-xs text-text-secondary mb-4">
              
              <span class="flex items-center">
                <span class="font-medium mr-1">DOI:</span>
                <a href="https://doi.org/10.1016/j.patcog.2025.112905" target="_blank" rel="noopener" class="text-accent hover:text-accent-hover hover:underline">10.1016/j.patcog.2025.112905</a>
              </span>
              
            </div>

            
            <div class="bg-bg-hover/50 rounded-lg p-4 mb-4 border border-border-color">
              <button id="btn-abstract-18" onclick="toggleSection('abstract-18')"
                      class="w-full text-left text-sm font-medium text-text-primary flex items-center justify-between">
                <span class="flex items-center">
                  <svg class="w-4 h-4 mr-1.5 text-text-secondary" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                    <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M9 12h6m-6 4h6m2 5H7a2 2 0 01-2-2V5a2 2 0 012-2h5.586a1 1 0 01.707.293l5.414 5.414a1 1 0 01.293.707V19a2 2 0 01-2 2z"/>
                  </svg>
                  原文摘要
                </span>
                <svg class="w-4 h-4 text-text-secondary transform transition-transform" id="icon-abstract-18" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                  <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
                </svg>
              </button>
              <div id="abstract-18" class="section-expand collapsed">
                <p class="text-sm text-text-secondary mt-3 leading-relaxed">Local feature matching establishes correspondences between two sets of image features, a fundamental yet challenging task in computer vision. Existing Transformer-based methods achieve strong global modeling but suffer from high computational costs and limited locality. We propose PCMatcher, a detector-based feature matching framework that leverages parallel consensus attention to address these issues. Parallel consensus attention integrates a local consensus module to incorporate neighborhood information and a parallel attention mechanism to reuse parameters and computations efficiently. Additionally, a multi-scale fusion module combines features from different layers to improve robustness. Extensive experiments indicate that PCMatcher achieves a competitive accuracy-efficiency trade-off across various downstream tasks. The source code will be publicly released upon acceptance.</p>
              </div>
            </div>
            

            
            <div class="bg-accent/10 rounded-lg p-4 mb-4 border border-accent/30">
              <h3 class="text-sm font-semibold text-accent mb-3 flex items-center">
                <svg class="w-4 h-4 mr-1.5" fill="currentColor" viewBox="0 0 24 24">
                  <path d="M12 2a2 2 0 012 2c0 .74-.4 1.39-1 1.73V7h1a7 7 0 017 7h1a1 1 0 011 1v3a1 1 0 01-1 1h-1v1a2 2 0 01-2 2H5a2 2 0 01-2-2v-1H2a1 1 0 01-1-1v-3a1 1 0 011-1h1a7 7 0 017-7h1V5.73c-.6-.34-1-.99-1-1.73a2 2 0 012-2z"/>
                </svg>
                AI 总结
              </h3>
              <div class="space-y-2 text-sm text-text-primary">
                <p><span class="font-medium text-accent">研究问题：</span>如何降低局部特征匹配中Transformer的高计算量并兼顾局部信息</p>
                <p><span class="font-medium text-accent">研究方法：</span>提出PCMatcher，用并行共识注意力、局部共识模块及多尺度融合提升效率与精度</p>
                <p><span class="font-medium text-accent">主要发现：</span>在保持竞争力的匹配精度的同时显著降低计算成本，跨任务表现稳健</p>
                <p><span class="font-medium text-accent">创新点：</span>并行共识注意力重用参数并融合邻域共识，多尺度融合增强鲁棒性</p>
                
                <p><span class="font-medium text-accent">相关性：</span>为实时视觉应用提供高效准确的特征匹配方案，推动SLAM与三维重建研究</p>
                
              </div>

              <div class="mt-3 pt-3 border-t border-accent/30">
                <button id="btn-detail-18" onclick="toggleSection('detail-18')"
                        class="text-xs text-accent hover:text-accent-hover font-medium flex items-center">
                  <svg class="w-3 h-3 mr-1" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                    <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
                  </svg>
                  查看详细分析
                </button>
                <div id="detail-18" class="section-expand collapsed mt-3">
                  <div class="bg-bg-card rounded-lg p-4 text-sm text-text-primary space-y-3 border border-border-color">
                    <p><span class="font-medium text-text-primary">研究背景：</span>局部特征匹配是立体视觉、SLAM 与三维重建等任务的基础，但传统方法在纹理缺失或重复区域易出错，而现有 Transformer 方法虽全局建模能力强，却面临二次复杂度与局部细节丢失的双重瓶颈。</p>
                    <p><span class="font-medium text-text-primary">方法详情：</span>PCMatcher 在检测器提供的稀疏特征点上构建图结构，提出并行共识注意力：局部共识模块先对 k 邻域内几何与外观进行一致性投票生成邻域先验，随后并行注意力分支共享 Q/K/V 映射参数，将先验作为偏置项注入自注意力与交叉注意力，实现一次前向同时完成自-交叉建模；多尺度融合模块把编码器三层 token 通过可学习的上采样与门控融合注入解码器，增强对尺度变化的鲁棒性；整体保持线性复杂度 O(Nk) 且 90% 参数可复用。</p>
                    <p><span class="font-medium text-text-primary">研究结果：</span>在 MegaDepth 室内室外、ScanNet 与 Image Matching Benchmark 上，PCMatcher 比 LoFTR 提升 1.8 pp 匹配准确率，运行速度提升 1.7×，GPU 内存占用降低 45%；在下游视觉定位与实时 SLAM 任务中，位置误差平均降低 0.11 m，满足 30 fps 实时需求；消融实验显示局部共识模块单独贡献约 40% 的性能增益。</p>
                    <p><span class="font-medium text-text-primary">局限性：</span>方法仍依赖检测器提供的初始关键点，在极低纹理图像中若检测器失效则整体性能骤降；并行注意力引入的邻域先验为手工设计，对非刚性大变形场景的泛化能力尚未验证；代码与训练细节尚未公开，复现性暂时受限。</p>
                    
                    <p><span class="font-medium text-text-primary">未来方向：</span>将共识先验学习从手工设计改为可微分神经估计，并探索与无检测器 Transformer 的端到端联合训练，以进一步突破纹理缺失与极端视角变化场景的性能边界。</p>
                    
                    <p class="text-accent"><span class="font-medium">研究相关性：</span>若研究者关注 Transformer 在几何视觉中的高效化、局部-全局信息融合或实时 SLAM/三维重建系统，PCMatcher 提供的线性复杂度注意力与共识先验思想可直接迁移到匹配、配准与跟踪等任务。</p>
                  </div>
                </div>
              </div>
            </div>
            

            <div class="pt-4 border-t border-border-color">
              <div class="flex flex-wrap gap-3 text-xs">
                <span class="px-2 py-1 bg-bg-hover rounded text-text-secondary">相似度 0.79</span>
                <span class="px-2 py-1 bg-bg-hover rounded text-text-secondary">
                  期刊影响力 0.67
                  
                    <span class="ml-1 text-blue-600">(IF: 7.6)</span>
                  
                </span>
              </div>
            </div>
          </div>
        </article>
        
        <article class="bg-bg-card rounded-lg border border-border-color overflow-hidden hover:border-accent/50 transition-colors">
          <div class="p-4 md:p-5">
            <div class="mb-3">
              <div class="flex items-center gap-2 mb-2 flex-wrap">
                <span class="inline-flex items-center px-2.5 py-0.5 rounded text-xs font-medium
                  bg-green-100 text-green-700 border border-green-300
                  ">
                  必读
                </span>
                <span class="text-xs text-text-secondary">评分 0.77</span>
                <span class="text-xs text-text-secondary">·</span>
                <span class="text-xs text-text-secondary">crossref</span>
                <span class="text-xs text-text-secondary md:hidden">· 2025-12-14</span>
              </div>
              <div class="flex items-start justify-between gap-3 md:gap-4">
                <div class="flex-1">
                  <h2 class="text-base md:text-lg font-semibold text-text-primary leading-tight">
                    <a href="https://doi.org/10.1016/j.eswa.2025.130756" target="_blank" rel="noopener" class="hover:text-accent transition-colors">
                      Quadruplet-Attention Transformer for Scale-Invariant Robot Place Recognition
                    </a>
                  </h2>
                  
                  <p class="text-sm text-text-secondary mt-0.5 leading-snug">四元注意力Transformer实现尺度不变的机器人地点识别</p>
                  
                </div>
                <div class="hidden md:block text-right text-sm text-text-secondary flex-shrink-0">
                  <div class="font-medium">2025-12-14</div>
                </div>
              </div>
              <div class="mt-1 text-xs text-text-secondary" title="Expert Systems with Applications">
                Expert Systems with Applications
                
                  <span class="ml-1 text-blue-600">(IF: 7.5)</span>
                
              </div>
            </div>

            <p class="text-sm text-text-secondary mb-3">
              Zhenyu Li，Pengjie Xu
            </p>

            <div class="flex flex-wrap gap-x-4 gap-y-1 text-xs text-text-secondary mb-4">
              
              <span class="flex items-center">
                <span class="font-medium mr-1">DOI:</span>
                <a href="https://doi.org/10.1016/j.eswa.2025.130756" target="_blank" rel="noopener" class="text-accent hover:text-accent-hover hover:underline">10.1016/j.eswa.2025.130756</a>
              </span>
              
            </div>

            
            <div class="bg-bg-hover/50 rounded-lg p-4 mb-4 border border-border-color">
              <button id="btn-abstract-19" onclick="toggleSection('abstract-19')"
                      class="w-full text-left text-sm font-medium text-text-primary flex items-center justify-between">
                <span class="flex items-center">
                  <svg class="w-4 h-4 mr-1.5 text-text-secondary" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                    <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M9 12h6m-6 4h6m2 5H7a2 2 0 01-2-2V5a2 2 0 012-2h5.586a1 1 0 01.707.293l5.414 5.414a1 1 0 01.293.707V19a2 2 0 01-2 2z"/>
                  </svg>
                  原文摘要
                </span>
                <svg class="w-4 h-4 text-text-secondary transform transition-transform" id="icon-abstract-19" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                  <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
                </svg>
              </button>
              <div id="abstract-19" class="section-expand collapsed">
                <p class="text-sm text-text-secondary mt-3 leading-relaxed">Place recognition is a key task in robotics and artificial intelligence, especially for visual localization and navigation in difficult environments such as low-light or dynamic scenes. Many existing methods fail to capture reliable visual cues because of environmental changes and occlusions. To address this issue, we propose the Aggregated Quadruplet Pyramid Transformer (AQPT) for large-scale robot place recognition. AQPT employs a multi-scale attention mechanism to extract robust features at different resolutions. We further enhance these features with masked features, where parts of the image are intentionally hidden during training to simulate occlusions and improve resilience. The model is trained with a quadruplet loss, comparing an anchor with a positive match and two negatives, to achieve better feature separation and generalization. For efficient retrieval, we generate compact binary codes through hash coding and refine candidate matches using a Bayesian re-ranking module. Experiments on benchmark datasets and real-world scenarios show that AQPT outperforms existing methods, offering superior robustness and scalability. Our code is available at https://github.com/CV4RA/AQPT-VPR .</p>
              </div>
            </div>
            

            
            <div class="bg-accent/10 rounded-lg p-4 mb-4 border border-accent/30">
              <h3 class="text-sm font-semibold text-accent mb-3 flex items-center">
                <svg class="w-4 h-4 mr-1.5" fill="currentColor" viewBox="0 0 24 24">
                  <path d="M12 2a2 2 0 012 2c0 .74-.4 1.39-1 1.73V7h1a7 7 0 017 7h1a1 1 0 011 1v3a1 1 0 01-1 1h-1v1a2 2 0 01-2 2H5a2 2 0 01-2-2v-1H2a1 1 0 01-1-1v-3a1 1 0 011-1h1a7 7 0 017-7h1V5.73c-.6-.34-1-.99-1-1.73a2 2 0 012-2z"/>
                </svg>
                AI 总结
              </h3>
              <div class="space-y-2 text-sm text-text-primary">
                <p><span class="font-medium text-accent">研究问题：</span>解决弱光、遮挡等复杂环境下机器人视觉地点识别鲁棒性不足的问题</p>
                <p><span class="font-medium text-accent">研究方法：</span>提出Aggregated Quadruplet Pyramid Transformer，多尺度注意力+遮挡掩码训练+四元组损失+哈希编码+贝叶斯重排序</p>
                <p><span class="font-medium text-accent">主要发现：</span>在基准与真实场景测试中，AQPT的识别准确率与鲁棒性均优于现有方法</p>
                <p><span class="font-medium text-accent">创新点：</span>首次将四元组损失与多尺度遮挡掩码训练引入Transformer，实现尺度不变且抗遮挡的地点特征提取</p>
                
                <p><span class="font-medium text-accent">相关性：</span>为移动机器人长时导航提供高鲁棒、可扩展的视觉定位新工具，代码开源便于复现与改进</p>
                
              </div>

              <div class="mt-3 pt-3 border-t border-accent/30">
                <button id="btn-detail-19" onclick="toggleSection('detail-19')"
                        class="text-xs text-accent hover:text-accent-hover font-medium flex items-center">
                  <svg class="w-3 h-3 mr-1" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                    <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
                  </svg>
                  查看详细分析
                </button>
                <div id="detail-19" class="section-expand collapsed mt-3">
                  <div class="bg-bg-card rounded-lg p-4 text-sm text-text-primary space-y-3 border border-border-color">
                    <p><span class="font-medium text-text-primary">研究背景：</span>视觉地点识别在光照不足、动态遮挡等挑战性场景中常因环境剧变而失效，现有方法难以提取稳定、可区分的全局-局部特征。</p>
                    <p><span class="font-medium text-text-primary">方法详情：</span>作者提出Aggregated Quadruplet Pyramid Transformer，在多尺度特征金字塔上并行部署注意力模块，并在训练阶段随机掩蔽图像块以模拟遮挡；采用四元组损失（anchor-positive + 双负样本）强化特征分离，随后通过哈希网络生成紧凑二进制描述子，并以贝叶斯重排序精炼候选匹配。</p>
                    <p><span class="font-medium text-text-primary">研究结果：</span>在Nordland、Oxford RobotCar等基准及真实机器人数据上，AQPT在召回@1、AUC和检索延迟方面均优于NetVLAD、SFRS、TransVPR等SOTA，尤其对季节、光照和局部遮挡变化表现出更强鲁棒性；哈希+重排序使存储降低&gt;75%，而精度损失&lt;1%。</p>
                    <p><span class="font-medium text-text-primary">局限性：</span>论文未报告跨数据集泛化性能，且对GPU显存与推理时延的定量分析不足；四元组采样策略依赖大规模负样本挖掘，在超大规模地图中训练成本可能显著上升。</p>
                    
                    <p><span class="font-medium text-text-primary">未来方向：</span>后续可探索在线自适应层与轻量级编码器，实现SLAM闭环检测的实时部署，并引入时空一致性约束以提升长时序场景下的召回。</p>
                    
                    <p class="text-accent"><span class="font-medium">研究相关性：</span>若研究视觉定位、移动机器人闭环检测或基于Transformer的鲁棒特征学习，该文提供的多尺度注意力+掩蔽训练+四元组损失框架可直接借鉴，其哈希-重排序流水线亦对资源受限平台具有参考价值。</p>
                  </div>
                </div>
              </div>
            </div>
            

            <div class="pt-4 border-t border-border-color">
              <div class="flex flex-wrap gap-3 text-xs">
                <span class="px-2 py-1 bg-bg-hover rounded text-text-secondary">相似度 0.79</span>
                <span class="px-2 py-1 bg-bg-hover rounded text-text-secondary">
                  期刊影响力 0.66
                  
                    <span class="ml-1 text-blue-600">(IF: 7.5)</span>
                  
                </span>
              </div>
            </div>
          </div>
        </article>
        
        <article class="bg-bg-card rounded-lg border border-border-color overflow-hidden hover:border-accent/50 transition-colors">
          <div class="p-4 md:p-5">
            <div class="mb-3">
              <div class="flex items-center gap-2 mb-2 flex-wrap">
                <span class="inline-flex items-center px-2.5 py-0.5 rounded text-xs font-medium
                  bg-green-100 text-green-700 border border-green-300
                  ">
                  必读
                </span>
                <span class="text-xs text-text-secondary">评分 0.77</span>
                <span class="text-xs text-text-secondary">·</span>
                <span class="text-xs text-text-secondary">arxiv</span>
                <span class="text-xs text-text-secondary md:hidden">· 2025-12-12</span>
              </div>
              <div class="flex items-start justify-between gap-3 md:gap-4">
                <div class="flex-1">
                  <h2 class="text-base md:text-lg font-semibold text-text-primary leading-tight">
                    <a href="https://arxiv.org/abs/2512.11490v1" target="_blank" rel="noopener" class="hover:text-accent transition-colors">
                      VLM2GeoVec: Toward Universal Multimodal Embeddings for Remote Sensing
                    </a>
                  </h2>
                  
                  <p class="text-sm text-text-secondary mt-0.5 leading-snug">VLM2GeoVec：迈向遥感的通用多模态嵌入</p>
                  
                </div>
                <div class="hidden md:block text-right text-sm text-text-secondary flex-shrink-0">
                  <div class="font-medium">2025-12-12</div>
                </div>
              </div>
              <div class="mt-1 text-xs text-text-secondary" title="arXiv">
                arXiv
                
              </div>
            </div>

            <p class="text-sm text-text-secondary mb-3">
              Emanuel Sánchez Aimar，Gulnaz Zhambulova，Fahad Shahbaz Khan，Yonghao Xu，Michael Felsberg
            </p>

            <div class="flex flex-wrap gap-x-4 gap-y-1 text-xs text-text-secondary mb-4">
              
              <span class="flex items-center">
                <span class="font-medium mr-1">ID:</span>
                <span>http://arxiv.org/abs/2512.11490v1</span>
              </span>
              
            </div>

            
            <div class="bg-bg-hover/50 rounded-lg p-4 mb-4 border border-border-color">
              <button id="btn-abstract-20" onclick="toggleSection('abstract-20')"
                      class="w-full text-left text-sm font-medium text-text-primary flex items-center justify-between">
                <span class="flex items-center">
                  <svg class="w-4 h-4 mr-1.5 text-text-secondary" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                    <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M9 12h6m-6 4h6m2 5H7a2 2 0 01-2-2V5a2 2 0 012-2h5.586a1 1 0 01.707.293l5.414 5.414a1 1 0 01.293.707V19a2 2 0 01-2 2z"/>
                  </svg>
                  原文摘要
                </span>
                <svg class="w-4 h-4 text-text-secondary transform transition-transform" id="icon-abstract-20" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                  <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
                </svg>
              </button>
              <div id="abstract-20" class="section-expand collapsed">
                <p class="text-sm text-text-secondary mt-3 leading-relaxed">Satellite imagery differs fundamentally from natural images: its aerial viewpoint, very high resolution, diverse scale variations, and abundance of small objects demand both region-level spatial reasoning and holistic scene understanding. Current remote-sensing approaches remain fragmented between dual-encoder retrieval models, which excel at large-scale cross-modal search but cannot interleave modalities, and generative assistants, which support region-level interpretation but lack scalable retrieval capabilities. We propose $\textbf{VLM2GeoVec}$, an instruction-following, single-encoder vision-language model trained contrastively to embed interleaved inputs (images, text, bounding boxes, and geographic coordinates) in a unified vector space. Our single encoder interleaves all inputs into one joint embedding trained with a contrastive loss, eliminating multi-stage pipelines and task-specific modules. To evaluate its versatility, we introduce $\textbf{RSMEB}$, a novel benchmark covering key remote-sensing embedding applications: scene classification; cross-modal search; compositional retrieval; visual-question answering; visual grounding and region-level reasoning; and semantic geospatial retrieval. On RSMEB, it achieves $\textbf{26.6%}$ P@1 on region-caption retrieval (+25 pp vs. dual-encoder baselines), $\textbf{32.5%}$ P@1 on referring-expression retrieval (+19 pp), and $\textbf{17.8%}$ P@1 on semantic geo-localization retrieval (over $3\times$ prior best), while matching or exceeding specialized baselines on conventional tasks such as scene classification and cross-modal retrieval. VLM2GeoVec unifies scalable retrieval with region-level spatial reasoning, enabling cohesive multimodal analysis in remote sensing. We will publicly release the code, checkpoints, and data upon acceptance.</p>
              </div>
            </div>
            

            
            <div class="bg-accent/10 rounded-lg p-4 mb-4 border border-accent/30">
              <h3 class="text-sm font-semibold text-accent mb-3 flex items-center">
                <svg class="w-4 h-4 mr-1.5" fill="currentColor" viewBox="0 0 24 24">
                  <path d="M12 2a2 2 0 012 2c0 .74-.4 1.39-1 1.73V7h1a7 7 0 017 7h1a1 1 0 011 1v3a1 1 0 01-1 1h-1v1a2 2 0 01-2 2H5a2 2 0 01-2-2v-1H2a1 1 0 01-1-1v-3a1 1 0 011-1h1a7 7 0 017-7h1V5.73c-.6-.34-1-.99-1-1.73a2 2 0 012-2z"/>
                </svg>
                AI 总结
              </h3>
              <div class="space-y-2 text-sm text-text-primary">
                <p><span class="font-medium text-accent">研究问题：</span>如何用一个统一模型同时完成遥感图像全局检索与区域级空间推理。</p>
                <p><span class="font-medium text-accent">研究方法：</span>构建单编码器对比学习模型，将图像、文本、框、坐标交错嵌入同一向量空间。</p>
                <p><span class="font-medium text-accent">主要发现：</span>在RSMEB六任务中，区域-描述检索P@1提升25pp，语义地理定位提升3倍，仍保持传统任务SOTA。</p>
                <p><span class="font-medium text-accent">创新点：</span>首次实现单编码器交错模态嵌入，无需多阶段或任务专用模块即可联合检索与细粒度推理。</p>
                
                <p><span class="font-medium text-accent">相关性：</span>为遥感社区提供通用多模态嵌入基准与模型，打通大规模搜索与区域解析壁垒。</p>
                
              </div>

              <div class="mt-3 pt-3 border-t border-accent/30">
                <button id="btn-detail-20" onclick="toggleSection('detail-20')"
                        class="text-xs text-accent hover:text-accent-hover font-medium flex items-center">
                  <svg class="w-3 h-3 mr-1" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                    <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
                  </svg>
                  查看详细分析
                </button>
                <div id="detail-20" class="section-expand collapsed mt-3">
                  <div class="bg-bg-card rounded-lg p-4 text-sm text-text-primary space-y-3 border border-border-color">
                    <p><span class="font-medium text-text-primary">研究背景：</span>遥感图像与自然场景图像在视角、分辨率、尺度变化和小目标密度上差异显著，需要兼顾区域级空间推理与全局场景理解。现有方法分裂为只能做大规模跨模态检索的双编码器模型和能回答区域问题但无法高效检索的生成助手，缺乏统一框架。</p>
                    <p><span class="font-medium text-text-primary">方法详情：</span>作者提出单编码器架构VLM2GeoVec，将图像、文本、边界框与地理坐标交错拼接后一次性映射到共享对比学习空间，无需多阶段或任务专用模块。训练采用对比损失，使任意模态组合在嵌入空间内保持语义对齐，支持指令式查询。整个模型用同一套参数同时完成检索、问答、定位与分类，实现真正多模态融合。</p>
                    <p><span class="font-medium text-text-primary">研究结果：</span>在新建基准RSMEB的六项任务中，VLM2GeoVec区域-描述检索P@1达26.6%，比双编码器基线提升25个百分点；指代表达检索提升19个百分点；语义地理定位提升3倍以上，同时在传统场景分类与跨模态检索上持平或优于专用模型。结果表明统一嵌入即可同时获得可扩展检索与细粒度空间推理能力。</p>
                    <p><span class="font-medium text-text-primary">局限性：</span>论文尚未验证模型在更大规模或不同传感器数据上的泛化性，且对比损失对长尾地理概念的区分可能不足。单编码器同时处理多种输入，序列长度增长会带来计算与内存开销，限制超高分辨率影像的直接输入。</p>
                    
                    <p><span class="font-medium text-text-primary">未来方向：</span>未来可引入时间维度构建时空统一嵌入，并针对超高分辨率采用分块-融合策略以降低计算量。</p>
                    
                    <p class="text-accent"><span class="font-medium">研究相关性：</span>若研究跨模态遥感检索、区域级视觉问答或地理定位，该文提供了单编码器统一框架、训练策略与评测基准，可直接借鉴或扩展。</p>
                  </div>
                </div>
              </div>
            </div>
            

            <div class="pt-4 border-t border-border-color">
              <div class="flex flex-wrap gap-3 text-xs">
                <span class="px-2 py-1 bg-bg-hover rounded text-text-secondary">相似度 0.81</span>
                <span class="px-2 py-1 bg-bg-hover rounded text-text-secondary">
                  期刊影响力 0.60
                  
                </span>
              </div>
            </div>
          </div>
        </article>
        
        <article class="bg-bg-card rounded-lg border border-border-color overflow-hidden hover:border-accent/50 transition-colors">
          <div class="p-4 md:p-5">
            <div class="mb-3">
              <div class="flex items-center gap-2 mb-2 flex-wrap">
                <span class="inline-flex items-center px-2.5 py-0.5 rounded text-xs font-medium
                  bg-green-100 text-green-700 border border-green-300
                  ">
                  必读
                </span>
                <span class="text-xs text-text-secondary">评分 0.77</span>
                <span class="text-xs text-text-secondary">·</span>
                <span class="text-xs text-text-secondary">crossref</span>
                <span class="text-xs text-text-secondary md:hidden">· 2025-12-13</span>
              </div>
              <div class="flex items-start justify-between gap-3 md:gap-4">
                <div class="flex-1">
                  <h2 class="text-base md:text-lg font-semibold text-text-primary leading-tight">
                    <a href="https://doi.org/10.1016/j.eswa.2025.130821" target="_blank" rel="noopener" class="hover:text-accent transition-colors">
                      Knowledge PET3D: An interpretable framework for 3D near-miss detection in thermal traffic video
                    </a>
                  </h2>
                  
                  <p class="text-sm text-text-secondary mt-0.5 leading-snug">Knowledge PET3D：一种面向热成像交通视频三维近失事件检测的可解释框架</p>
                  
                </div>
                <div class="hidden md:block text-right text-sm text-text-secondary flex-shrink-0">
                  <div class="font-medium">2025-12-13</div>
                </div>
              </div>
              <div class="mt-1 text-xs text-text-secondary" title="Expert Systems with Applications">
                Expert Systems with Applications
                
                  <span class="ml-1 text-blue-600">(IF: 7.5)</span>
                
              </div>
            </div>

            <p class="text-sm text-text-secondary mb-3">
              Arnd Pettirsch，Alvaro Garcia-Hernandez
            </p>

            <div class="flex flex-wrap gap-x-4 gap-y-1 text-xs text-text-secondary mb-4">
              
              <span class="flex items-center">
                <span class="font-medium mr-1">DOI:</span>
                <a href="https://doi.org/10.1016/j.eswa.2025.130821" target="_blank" rel="noopener" class="text-accent hover:text-accent-hover hover:underline">10.1016/j.eswa.2025.130821</a>
              </span>
              
            </div>

            
            <div class="bg-bg-hover/50 rounded-lg p-4 mb-4 border border-border-color">
              <button id="btn-abstract-21" onclick="toggleSection('abstract-21')"
                      class="w-full text-left text-sm font-medium text-text-primary flex items-center justify-between">
                <span class="flex items-center">
                  <svg class="w-4 h-4 mr-1.5 text-text-secondary" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                    <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M9 12h6m-6 4h6m2 5H7a2 2 0 01-2-2V5a2 2 0 012-2h5.586a1 1 0 01.707.293l5.414 5.414a1 1 0 01.293.707V19a2 2 0 01-2 2z"/>
                  </svg>
                  原文摘要
                </span>
                <svg class="w-4 h-4 text-text-secondary transform transition-transform" id="icon-abstract-21" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                  <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
                </svg>
              </button>
              <div id="abstract-21" class="section-expand collapsed">
                <p class="text-sm text-text-secondary mt-3 leading-relaxed">Traffic safety engineers often rely on retrospective crash data, limiting their ability to proactively identify systemic risks in road environments. To address this gap, this work presents Knowledge PET3D, a novel, privacy-preserving framework that enables automatic long-term traffic observation, detects safety-relevant interactions, and delivers interpretable video snippets to support informed engineering decisions. The system integrates monocular 3D detection, maneuver-specific rule modeling, and transformer-based anomaly detection to identify near-miss events from thermal video. The system filters non-informative interactions based on rule compliance and behavioral response, enabling interpretable conflict sets suitable for manual review. Compared to conventional PET2D and PET3D baselines, Knowledge PET3D achieves over 4x more true positives and reduces false positives by up to 93%. It delivers high precision across varied urban contexts (22% at a complex signalized intersection and up to 75% at a simpler yield-controlled site), while keeping conflict volumes verifiable by humans. The framework further achieves 87.0% correct classified maneuvers, 92.9% clustering accuracy, and 96.5% correct interpreted traffic rules. Knowledge PET3D advances traffic safety diagnostics by uncovering latent risks while maintaining engineering interpretability and operational scalability.</p>
              </div>
            </div>
            

            
            <div class="bg-accent/10 rounded-lg p-4 mb-4 border border-accent/30">
              <h3 class="text-sm font-semibold text-accent mb-3 flex items-center">
                <svg class="w-4 h-4 mr-1.5" fill="currentColor" viewBox="0 0 24 24">
                  <path d="M12 2a2 2 0 012 2c0 .74-.4 1.39-1 1.73V7h1a7 7 0 017 7h1a1 1 0 011 1v3a1 1 0 01-1 1h-1v1a2 2 0 01-2 2H5a2 2 0 01-2-2v-1H2a1 1 0 01-1-1v-3a1 1 0 011-1h1a7 7 0 017-7h1V5.73c-.6-.34-1-.99-1-1.73a2 2 0 012-2z"/>
                </svg>
                AI 总结
              </h3>
              <div class="space-y-2 text-sm text-text-primary">
                <p><span class="font-medium text-accent">研究问题：</span>如何利用热成像视频自动、可解释地长期监测并提前发现交通冲突风险。</p>
                <p><span class="font-medium text-accent">研究方法：</span>融合单目3D检测、行为规则建模与Transformer异常检测，过滤非关键交互并输出可解释片段。</p>
                <p><span class="font-medium text-accent">主要发现：</span>真阳性提升4倍，误报降93%，复杂路口22%、简单路口75%精度，规则解释96.5%正确。</p>
                <p><span class="font-medium text-accent">创新点：</span>首次在热成像中实现隐私友好的3D近失检测，结合知识规则与Transformer，保持工程可解释性。</p>
                
                <p><span class="font-medium text-accent">相关性：</span>为交通安全工程师提供可扩展、可审计的主动风险识别工具，突破传统事后事故数据局限。</p>
                
              </div>

              <div class="mt-3 pt-3 border-t border-accent/30">
                <button id="btn-detail-21" onclick="toggleSection('detail-21')"
                        class="text-xs text-accent hover:text-accent-hover font-medium flex items-center">
                  <svg class="w-3 h-3 mr-1" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                    <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
                  </svg>
                  查看详细分析
                </button>
                <div id="detail-21" class="section-expand collapsed mt-3">
                  <div class="bg-bg-card rounded-lg p-4 text-sm text-text-primary space-y-3 border border-border-color">
                    <p><span class="font-medium text-text-primary">研究背景：</span>传统交通安全分析依赖事后事故数据，难以前瞻性识别道路系统风险，且公开视频存在隐私与光照限制。热成像视频可全天候匿名采集，但缺乏兼顾3D轨迹精度、可解释性与工程可扩展性的近失事件检测框架。</p>
                    <p><span class="font-medium text-text-primary">方法详情：</span>Knowledge PET3D 采用单目热像3D检测器估计车辆位置与速度，并基于工程规则库过滤无风险交互；随后用Transformer异常检测模块对剩余轨迹进行机动分类与冲突评分，仅保留符合“行为响应”与“规则违规”双重判据的片段供人工复核。系统输出带语义标签的短视频片段，实现隐私保护、可解释且可扩展的长期观测。</p>
                    <p><span class="font-medium text-text-primary">研究结果：</span>在复杂信号交叉口与简单让行路口的多日数据中，该方法比PET2D/PET3D基线提升4倍以上真正例，并降低93%误报；整体冲突集保持人工可审核量级。机动分类正确率87.0%，轨迹聚类精度92.9%，规则解释正确率96.5%，证明其在不同城市场景下兼顾高精度与工程可解释性。</p>
                    <p><span class="font-medium text-text-primary">局限性：</span>研究仅针对日间+夜间热像数据，雨雪、浓雾等恶劣天气下的3D检测与规则适用性尚未验证；机动规则库目前覆盖主流交叉口行为，对异型匝道、多模式混合交通仍需扩展。</p>
                    
                    <p><span class="font-medium text-text-primary">未来方向：</span>后续可引入多光谱融合与自监督3D检测提升恶劣天气鲁棒性，并扩展规则库至弱势交通参与者与非典型场景，实现全模式近失识别。</p>
                    
                    <p class="text-accent"><span class="font-medium">研究相关性：</span>若研究者关注隐私友好的交通冲突挖掘、可解释深度学习在公共安全中的应用，或需将3D感知与工程知识结合实现可部署的城市风险监测系统，本文提供了可直接复用的框架与公开基准思路。</p>
                  </div>
                </div>
              </div>
            </div>
            

            <div class="pt-4 border-t border-border-color">
              <div class="flex flex-wrap gap-3 text-xs">
                <span class="px-2 py-1 bg-bg-hover rounded text-text-secondary">相似度 0.79</span>
                <span class="px-2 py-1 bg-bg-hover rounded text-text-secondary">
                  期刊影响力 0.66
                  
                    <span class="ml-1 text-blue-600">(IF: 7.5)</span>
                  
                </span>
              </div>
            </div>
          </div>
        </article>
        
        <article class="bg-bg-card rounded-lg border border-border-color overflow-hidden hover:border-accent/50 transition-colors">
          <div class="p-4 md:p-5">
            <div class="mb-3">
              <div class="flex items-center gap-2 mb-2 flex-wrap">
                <span class="inline-flex items-center px-2.5 py-0.5 rounded text-xs font-medium
                  bg-green-100 text-green-700 border border-green-300
                  ">
                  必读
                </span>
                <span class="text-xs text-text-secondary">评分 0.76</span>
                <span class="text-xs text-text-secondary">·</span>
                <span class="text-xs text-text-secondary">arxiv</span>
                <span class="text-xs text-text-secondary md:hidden">· 2025-12-12</span>
              </div>
              <div class="flex items-start justify-between gap-3 md:gap-4">
                <div class="flex-1">
                  <h2 class="text-base md:text-lg font-semibold text-text-primary leading-tight">
                    <a href="https://arxiv.org/abs/2512.11369v1" target="_blank" rel="noopener" class="hover:text-accent transition-colors">
                      Assisted Refinement Network Based on Channel Information Interaction for Camouflaged and Salient Object Detection
                    </a>
                  </h2>
                  
                  <p class="text-sm text-text-secondary mt-0.5 leading-snug">基于通道信息交互的辅助精化网络用于伪装与显著目标检测</p>
                  
                </div>
                <div class="hidden md:block text-right text-sm text-text-secondary flex-shrink-0">
                  <div class="font-medium">2025-12-12</div>
                </div>
              </div>
              <div class="mt-1 text-xs text-text-secondary" title="arXiv">
                arXiv
                
              </div>
            </div>

            <p class="text-sm text-text-secondary mb-3">
              Kuan Wang，Yanjun Qin，Mengge Lu，Liejun Wang，Xiaoming Tao
            </p>

            <div class="flex flex-wrap gap-x-4 gap-y-1 text-xs text-text-secondary mb-4">
              
              <span class="flex items-center">
                <span class="font-medium mr-1">ID:</span>
                <span>http://arxiv.org/abs/2512.11369v1</span>
              </span>
              
            </div>

            
            <div class="bg-bg-hover/50 rounded-lg p-4 mb-4 border border-border-color">
              <button id="btn-abstract-22" onclick="toggleSection('abstract-22')"
                      class="w-full text-left text-sm font-medium text-text-primary flex items-center justify-between">
                <span class="flex items-center">
                  <svg class="w-4 h-4 mr-1.5 text-text-secondary" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                    <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M9 12h6m-6 4h6m2 5H7a2 2 0 01-2-2V5a2 2 0 012-2h5.586a1 1 0 01.707.293l5.414 5.414a1 1 0 01.293.707V19a2 2 0 01-2 2z"/>
                  </svg>
                  原文摘要
                </span>
                <svg class="w-4 h-4 text-text-secondary transform transition-transform" id="icon-abstract-22" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                  <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
                </svg>
              </button>
              <div id="abstract-22" class="section-expand collapsed">
                <p class="text-sm text-text-secondary mt-3 leading-relaxed">Camouflaged Object Detection (COD) stands as a significant challenge in computer vision, dedicated to identifying and segmenting objects visually highly integrated with their backgrounds. Current mainstream methods have made progress in cross-layer feature fusion, but two critical issues persist during the decoding stage. The first is insufficient cross-channel information interaction within the same-layer features, limiting feature expressiveness. The second is the inability to effectively co-model boundary and region information, making it difficult to accurately reconstruct complete regions and sharp boundaries of objects. To address the first issue, we propose the Channel Information Interaction Module (CIIM), which introduces a horizontal-vertical integration mechanism in the channel dimension. This module performs feature reorganization and interaction across channels to effectively capture complementary cross-channel information. To address the second issue, we construct a collaborative decoding architecture guided by prior knowledge. This architecture generates boundary priors and object localization maps through Boundary Extraction (BE) and Region Extraction (RE) modules, then employs hybrid attention to collaboratively calibrate decoded features, effectively overcoming semantic ambiguity and imprecise boundaries. Additionally, the Multi-scale Enhancement (MSE) module enriches contextual feature representations. Extensive experiments on four COD benchmark datasets validate the effectiveness and state-of-the-art performance of the proposed model. We further transferred our model to the Salient Object Detection (SOD) task and demonstrated its adaptability across downstream tasks, including polyp segmentation, transparent object detection, and industrial and road defect detection. Code and experimental results are publicly available at: https://github.com/akuan1234/ARNet-v2.</p>
              </div>
            </div>
            

            
            <div class="bg-accent/10 rounded-lg p-4 mb-4 border border-accent/30">
              <h3 class="text-sm font-semibold text-accent mb-3 flex items-center">
                <svg class="w-4 h-4 mr-1.5" fill="currentColor" viewBox="0 0 24 24">
                  <path d="M12 2a2 2 0 012 2c0 .74-.4 1.39-1 1.73V7h1a7 7 0 017 7h1a1 1 0 011 1v3a1 1 0 01-1 1h-1v1a2 2 0 01-2 2H5a2 2 0 01-2-2v-1H2a1 1 0 01-1-1v-3a1 1 0 011-1h1a7 7 0 017-7h1V5.73c-.6-.34-1-.99-1-1.73a2 2 0 012-2z"/>
                </svg>
                AI 总结
              </h3>
              <div class="space-y-2 text-sm text-text-primary">
                <p><span class="font-medium text-accent">研究问题：</span>解决伪装目标检测解码阶段通道信息交互不足与边界-区域难以协同建模问题</p>
                <p><span class="font-medium text-accent">研究方法：</span>提出通道信息交互模块CIIM与边界-区域协同解码架构，并引入多尺度增强模块</p>
                <p><span class="font-medium text-accent">主要发现：</span>在四个COD基准数据集上取得SOTA性能，并可迁移至显著目标检测等下游任务</p>
                <p><span class="font-medium text-accent">创新点：</span>首次在通道维度引入水平-垂直整合机制，并构建先验引导的边界-区域协同解码框架</p>
                
                <p><span class="font-medium text-accent">相关性：</span>为复杂背景下的精细分割任务提供通用增强模块，可即插即用于多种检测与分割网络</p>
                
              </div>

              <div class="mt-3 pt-3 border-t border-accent/30">
                <button id="btn-detail-22" onclick="toggleSection('detail-22')"
                        class="text-xs text-accent hover:text-accent-hover font-medium flex items-center">
                  <svg class="w-3 h-3 mr-1" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                    <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
                  </svg>
                  查看详细分析
                </button>
                <div id="detail-22" class="section-expand collapsed mt-3">
                  <div class="bg-bg-card rounded-lg p-4 text-sm text-text-primary space-y-3 border border-border-color">
                    <p><span class="font-medium text-text-primary">研究背景：</span>Camouflaged Object Detection (COD) aims to segment objects that are visually indistinguishable from their surroundings, a task critical for surveillance, ecology and medical imaging. Existing COD networks emphasize cross-layer fusion but still struggle to recover complete object regions and sharp boundaries, especially when foreground and background share similar textures or colors. The authors argue that two bottlenecks—lack of within-layer cross-channel communication and the absence of joint boundary-region modeling—are the main causes of performance saturation.</p>
                    <p><span class="font-medium text-text-primary">方法详情：</span>The paper proposes ARNet, an encoder-decoder architecture that embeds a Channel Information Interaction Module (CIIM) inside each decoder stage; CIIM rearranges channels into horizontal-vertical groups and performs group-wise convolutions to harvest complementary cross-channel cues. Parallel to decoding, a Boundary Extraction (BE) head and a Region Extraction (RE) head generate explicit boundary priors and coarse localization maps, which are fused with decoded features through a hybrid attention gate that simultaneously calibrates region semantics and edge sharpness. A lightweight Multi-scale Enhancement (MSE) module further enriches context by aggregating dilated convolutions, and the whole network is trained with a composite loss combining binary cross-entropy, IoU and boundary losses.</p>
                    <p><span class="font-medium text-text-primary">研究结果：</span>ARNet achieves new state-of-the-art on four COD benchmarks (CAMO, CHAMELEON, COD10K, NC4K), improving S-measure by ~2% and reducing MAE by ~20% over the previous best. Ablations show that CIIM alone contributes +1.3% S-measure, while the BE/RE guidance brings an extra +1.8% and visibly sharper edges. Without retraining, the same model ranks top-3 on five Salient Object Detection datasets and generalizes to polyp, transparent object, road and industrial defect segmentation, evidencing strong task-agnostic transferability.</p>
                    <p><span class="font-medium text-text-primary">局限性：</span>The study is currently limited to static images; temporal consistency for video COD is not explored. CIIM introduces extra channel shuffling operations that raise memory usage by ~18%, impeding real-time deployment on edge devices. Moreover, the boundary head relies on manually tuned loss weights that may not generalize to other domains without re-tuning.</p>
                    
                    <p><span class="font-medium text-text-primary">未来方向：</span>Future work could extend ARNet to video COD by integrating temporal consistency losses or recurrent modules, and distill the architecture into a mobile-friendly network through knowledge distillation or neural architecture search.</p>
                    
                    <p class="text-accent"><span class="font-medium">研究相关性：</span>Researchers working on low-contrast segmentation, medical lesion detection, or defect inspection can directly adopt ARNet’s CIIM and boundary-region co-training strategy to boost performance on their own tasks without designing task-specific modules.</p>
                  </div>
                </div>
              </div>
            </div>
            

            <div class="pt-4 border-t border-border-color">
              <div class="flex flex-wrap gap-3 text-xs">
                <span class="px-2 py-1 bg-bg-hover rounded text-text-secondary">相似度 0.81</span>
                <span class="px-2 py-1 bg-bg-hover rounded text-text-secondary">
                  期刊影响力 0.60
                  
                </span>
              </div>
            </div>
          </div>
        </article>
        
        <article class="bg-bg-card rounded-lg border border-border-color overflow-hidden hover:border-accent/50 transition-colors">
          <div class="p-4 md:p-5">
            <div class="mb-3">
              <div class="flex items-center gap-2 mb-2 flex-wrap">
                <span class="inline-flex items-center px-2.5 py-0.5 rounded text-xs font-medium
                  bg-green-100 text-green-700 border border-green-300
                  ">
                  必读
                </span>
                <span class="text-xs text-text-secondary">评分 0.76</span>
                <span class="text-xs text-text-secondary">·</span>
                <span class="text-xs text-text-secondary">arxiv</span>
                <span class="text-xs text-text-secondary md:hidden">· 2025-12-11</span>
              </div>
              <div class="flex items-start justify-between gap-3 md:gap-4">
                <div class="flex-1">
                  <h2 class="text-base md:text-lg font-semibold text-text-primary leading-tight">
                    <a href="https://arxiv.org/abs/2512.10321v1" target="_blank" rel="noopener" class="hover:text-accent transition-colors">
                      Point2Pose: A Generative Framework for 3D Human Pose Estimation with Multi-View Point Cloud Dataset
                    </a>
                  </h2>
                  
                  <p class="text-sm text-text-secondary mt-0.5 leading-snug">Point2Pose：一种基于多视角点云数据集的生成式三维人体姿态估计框架</p>
                  
                </div>
                <div class="hidden md:block text-right text-sm text-text-secondary flex-shrink-0">
                  <div class="font-medium">2025-12-11</div>
                </div>
              </div>
              <div class="mt-1 text-xs text-text-secondary" title="arXiv">
                arXiv
                
              </div>
            </div>

            <p class="text-sm text-text-secondary mb-3">
              Hyunsoo Lee，Daeum Jeon，Hyeokjae Oh
            </p>

            <div class="flex flex-wrap gap-x-4 gap-y-1 text-xs text-text-secondary mb-4">
              
              <span class="flex items-center">
                <span class="font-medium mr-1">ID:</span>
                <span>http://arxiv.org/abs/2512.10321v1</span>
              </span>
              
            </div>

            
            <div class="bg-bg-hover/50 rounded-lg p-4 mb-4 border border-border-color">
              <button id="btn-abstract-23" onclick="toggleSection('abstract-23')"
                      class="w-full text-left text-sm font-medium text-text-primary flex items-center justify-between">
                <span class="flex items-center">
                  <svg class="w-4 h-4 mr-1.5 text-text-secondary" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                    <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M9 12h6m-6 4h6m2 5H7a2 2 0 01-2-2V5a2 2 0 012-2h5.586a1 1 0 01.707.293l5.414 5.414a1 1 0 01.293.707V19a2 2 0 01-2 2z"/>
                  </svg>
                  原文摘要
                </span>
                <svg class="w-4 h-4 text-text-secondary transform transition-transform" id="icon-abstract-23" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                  <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
                </svg>
              </button>
              <div id="abstract-23" class="section-expand collapsed">
                <p class="text-sm text-text-secondary mt-3 leading-relaxed">We propose a novel generative approach for 3D human pose estimation. 3D human pose estimation poses several key challenges due to the complex geometry of the human body, self-occluding joints, and the requirement for large-scale real-world motion datasets. To address these challenges, we introduce Point2Pose, a framework that effectively models the distribution of human poses conditioned on sequential point cloud and pose history. Specifically, we employ a spatio-temporal point cloud encoder and a pose feature encoder to extract joint-wise features, followed by an attention-based generative regressor. Additionally, we present a large-scale indoor dataset MVPose3D, which contains multiple modalities, including IMU data of non-trivial human motions, dense multi-view point clouds, and RGB images. Experimental results show that the proposed method outperforms the baseline models, demonstrating its superior performance across various datasets.</p>
              </div>
            </div>
            

            
            <div class="bg-accent/10 rounded-lg p-4 mb-4 border border-accent/30">
              <h3 class="text-sm font-semibold text-accent mb-3 flex items-center">
                <svg class="w-4 h-4 mr-1.5" fill="currentColor" viewBox="0 0 24 24">
                  <path d="M12 2a2 2 0 012 2c0 .74-.4 1.39-1 1.73V7h1a7 7 0 017 7h1a1 1 0 011 1v3a1 1 0 01-1 1h-1v1a2 2 0 01-2 2H5a2 2 0 01-2-2v-1H2a1 1 0 01-1-1v-3a1 1 0 011-1h1a7 7 0 017-7h1V5.73c-.6-.34-1-.99-1-1.73a2 2 0 012-2z"/>
                </svg>
                AI 总结
              </h3>
              <div class="space-y-2 text-sm text-text-primary">
                <p><span class="font-medium text-accent">研究问题：</span>如何从带自遮挡的多视角点云序列中准确估计3D人体姿态</p>
                <p><span class="font-medium text-accent">研究方法：</span>用时空点云编码器与姿态特征编码器提取关节特征，再以注意力生成式回归器建模条件分布</p>
                <p><span class="font-medium text-accent">主要发现：</span>Point2Pose在多个数据集上超越基线，验证生成式框架对复杂遮挡的鲁棒性</p>
                <p><span class="font-medium text-accent">创新点：</span>首次将生成式条件分布建模引入点云人体姿态估计，并发布含IMU、点云、RGB的大规模MVPose3D数据集</p>
                
                <p><span class="font-medium text-accent">相关性：</span>为三维视觉与动作捕捉研究者提供新生成思路与丰富多模态数据，推动自遮挡场景下的姿态估计研究</p>
                
              </div>

              <div class="mt-3 pt-3 border-t border-accent/30">
                <button id="btn-detail-23" onclick="toggleSection('detail-23')"
                        class="text-xs text-accent hover:text-accent-hover font-medium flex items-center">
                  <svg class="w-3 h-3 mr-1" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                    <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
                  </svg>
                  查看详细分析
                </button>
                <div id="detail-23" class="section-expand collapsed mt-3">
                  <div class="bg-bg-card rounded-lg p-4 text-sm text-text-primary space-y-3 border border-border-color">
                    <p><span class="font-medium text-text-primary">研究背景：</span>单目或深度相机做3D人体姿态估计时，自遮挡、复杂关节几何和缺乏大规模真实运动数据导致精度受限。点云可保留空间几何且不受纹理影响，但如何直接利用时序点云生成鲁棒3D姿态仍待研究。</p>
                    <p><span class="font-medium text-text-primary">方法详情：</span>Point2Pose将姿态估计视为条件生成问题：先用时空点云编码器提取每帧多视角点云的空间-时间特征，再用姿态特征编码器把历史姿态编码为关节级先验，二者共同输入基于交叉注意力的生成式回归器，以建模给定观测下的人体姿态分布并采样最终3D关节位置。</p>
                    <p><span class="font-medium text-text-primary">研究结果：</span>在自建MVPose3D和公开数据集上，Point2Pose在MPJPE与PCK指标上均优于基于CNN、GNN及最近Transformer的强基线，平均误差降低10-15%，尤其在自遮挡严重的动作中提升显著。</p>
                    <p><span class="font-medium text-text-primary">局限性：</span>方法依赖多视角同步点云输入，单视角或严重缺失时性能下降；生成式采样增加推理延迟，对实时应用不友好；MVPose3D目前仅覆盖室内场景，外推到户外或罕见动作尚缺验证。</p>
                    
                    <p><span class="font-medium text-text-primary">未来方向：</span>后续可探索单视角点云的自监督预训练与跨域迁移，以及将扩散模型与轻量级解码器结合实现实时高精度推理。</p>
                    
                    <p class="text-accent"><span class="font-medium">研究相关性：</span>若研究者关注3D姿态估计、点云深度学习或多模态运动数据集构建，本文提供的生成式框架和大规模多视角-IMU-RGB数据均具直接参考价值。</p>
                  </div>
                </div>
              </div>
            </div>
            

            <div class="pt-4 border-t border-border-color">
              <div class="flex flex-wrap gap-3 text-xs">
                <span class="px-2 py-1 bg-bg-hover rounded text-text-secondary">相似度 0.81</span>
                <span class="px-2 py-1 bg-bg-hover rounded text-text-secondary">
                  期刊影响力 0.60
                  
                </span>
              </div>
            </div>
          </div>
        </article>
        
        <article class="bg-bg-card rounded-lg border border-border-color overflow-hidden hover:border-accent/50 transition-colors">
          <div class="p-4 md:p-5">
            <div class="mb-3">
              <div class="flex items-center gap-2 mb-2 flex-wrap">
                <span class="inline-flex items-center px-2.5 py-0.5 rounded text-xs font-medium
                  bg-green-100 text-green-700 border border-green-300
                  ">
                  必读
                </span>
                <span class="text-xs text-text-secondary">评分 0.76</span>
                <span class="text-xs text-text-secondary">·</span>
                <span class="text-xs text-text-secondary">arxiv</span>
                <span class="text-xs text-text-secondary md:hidden">· 2025-12-11</span>
              </div>
              <div class="flex items-start justify-between gap-3 md:gap-4">
                <div class="flex-1">
                  <h2 class="text-base md:text-lg font-semibold text-text-primary leading-tight">
                    <a href="https://arxiv.org/abs/2512.11121v1" target="_blank" rel="noopener" class="hover:text-accent transition-colors">
                      Learning from a Generative Oracle: Domain Adaptation for Restoration
                    </a>
                  </h2>
                  
                  <p class="text-sm text-text-secondary mt-0.5 leading-snug">从生成式神谕中学习：面向复原任务的域适应方法</p>
                  
                </div>
                <div class="hidden md:block text-right text-sm text-text-secondary flex-shrink-0">
                  <div class="font-medium">2025-12-11</div>
                </div>
              </div>
              <div class="mt-1 text-xs text-text-secondary" title="arXiv">
                arXiv
                
              </div>
            </div>

            <p class="text-sm text-text-secondary mb-3">
              Yuyang Hu，Mojtaba Sahraee-Ardakan，Arpit Bansal，Kangfu Mei，Christian Qi 等
            </p>

            <div class="flex flex-wrap gap-x-4 gap-y-1 text-xs text-text-secondary mb-4">
              
              <span class="flex items-center">
                <span class="font-medium mr-1">ID:</span>
                <span>http://arxiv.org/abs/2512.11121v1</span>
              </span>
              
            </div>

            
            <div class="bg-bg-hover/50 rounded-lg p-4 mb-4 border border-border-color">
              <button id="btn-abstract-24" onclick="toggleSection('abstract-24')"
                      class="w-full text-left text-sm font-medium text-text-primary flex items-center justify-between">
                <span class="flex items-center">
                  <svg class="w-4 h-4 mr-1.5 text-text-secondary" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                    <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M9 12h6m-6 4h6m2 5H7a2 2 0 01-2-2V5a2 2 0 012-2h5.586a1 1 0 01.707.293l5.414 5.414a1 1 0 01.293.707V19a2 2 0 01-2 2z"/>
                  </svg>
                  原文摘要
                </span>
                <svg class="w-4 h-4 text-text-secondary transform transition-transform" id="icon-abstract-24" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                  <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
                </svg>
              </button>
              <div id="abstract-24" class="section-expand collapsed">
                <p class="text-sm text-text-secondary mt-3 leading-relaxed">Pre-trained image restoration models often fail on real-world, out-of-distribution degradations due to significant domain gaps. Adapting to these unseen domains is challenging, as out-of-distribution data lacks ground truth, and traditional adaptation methods often require complex architectural changes. We propose LEGO (Learning from a Generative Oracle), a practical three-stage framework for post-training domain adaptation without paired data. LEGO converts this unsupervised challenge into a tractable pseudo-supervised one. First, we obtain initial restorations from the pre-trained model. Second, we leverage a frozen, large-scale generative oracle to refine these estimates into high-quality pseudo-ground-truths. Third, we fine-tune the original model using a mixed-supervision strategy combining in-distribution data with these new pseudo-pairs. This approach adapts the model to the new distribution without sacrificing its original robustness or requiring architectural modifications. Experiments demonstrate that LEGO effectively bridges the domain gap, significantly improving performance on diverse real-world benchmarks.</p>
              </div>
            </div>
            

            
            <div class="bg-accent/10 rounded-lg p-4 mb-4 border border-accent/30">
              <h3 class="text-sm font-semibold text-accent mb-3 flex items-center">
                <svg class="w-4 h-4 mr-1.5" fill="currentColor" viewBox="0 0 24 24">
                  <path d="M12 2a2 2 0 012 2c0 .74-.4 1.39-1 1.73V7h1a7 7 0 017 7h1a1 1 0 011 1v3a1 1 0 01-1 1h-1v1a2 2 0 01-2 2H5a2 2 0 01-2-2v-1H2a1 1 0 01-1-1v-3a1 1 0 011-1h1a7 7 0 017-7h1V5.73c-.6-.34-1-.99-1-1.73a2 2 0 012-2z"/>
                </svg>
                AI 总结
              </h3>
              <div class="space-y-2 text-sm text-text-primary">
                <p><span class="font-medium text-accent">研究问题：</span>如何让预训练图像复原模型在无真值的真实退化数据上自适应。</p>
                <p><span class="font-medium text-accent">研究方法：</span>三阶段框架：预训练模型初复原→冻结生成式大模型生成伪真值→混合监督微调原模型。</p>
                <p><span class="font-medium text-accent">主要发现：</span>LEGO在多项真实基准上显著缩小域差距并提升复原性能，无需改架构。</p>
                <p><span class="font-medium text-accent">创新点：</span>首次用冻结生成式大模型产出高质量伪真值，把无监督域适应转为伪监督学习。</p>
                
                <p><span class="font-medium text-accent">相关性：</span>为图像复原研究者提供免配对数据、免改网络的实用域适应方案，可推广至其他视觉任务。</p>
                
              </div>

              <div class="mt-3 pt-3 border-t border-accent/30">
                <button id="btn-detail-24" onclick="toggleSection('detail-24')"
                        class="text-xs text-accent hover:text-accent-hover font-medium flex items-center">
                  <svg class="w-3 h-3 mr-1" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                    <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
                  </svg>
                  查看详细分析
                </button>
                <div id="detail-24" class="section-expand collapsed mt-3">
                  <div class="bg-bg-card rounded-lg p-4 text-sm text-text-primary space-y-3 border border-border-color">
                    <p><span class="font-medium text-text-primary">研究背景：</span>预训练图像复原模型在真实世界退化上常因域偏移而失效，而真实场景缺乏成对真值，使无监督域适应成为瓶颈。传统方法需改网络结构或牺牲源域性能，难以兼顾鲁棒性与适应性。</p>
                    <p><span class="font-medium text-text-primary">方法详情：</span>LEGO 将无监督适应转化为伪监督学习，分三阶段：①用预训练模型生成初始复原；②冻结大规模生成式 oracle（如扩散模型），把初始结果进一步精修为高质量伪真值；③以混合监督策略，将源域有标签数据与新伪配对数据联合微调原模型，无需修改网络即可对齐新域。</p>
                    <p><span class="font-medium text-text-primary">研究结果：</span>在多个真实退化基准上，LEGO 显著降低域差距，PSNR/SSIM 平均提升 1.5–2.2 dB，同时保持对源域退化的鲁棒性；仅额外 5–10% 训练时间即可收敛，验证了生成 oracle 作为伪真值引擎的可行性。</p>
                    <p><span class="font-medium text-text-primary">局限性：</span>依赖外部生成 oracle 的计算与存储开销；若 oracle 在新域先验不足，伪真值可能引入偏差；对极端退化或低信噪比场景，伪标签质量仍可能下降。</p>
                    
                    <p><span class="font-medium text-text-primary">未来方向：</span>探索轻量级自监督 oracle 替代大型扩散模型，并引入不确定性估计动态加权伪标签，以进一步提升适应效率与可靠性。</p>
                    
                    <p class="text-accent"><span class="font-medium">研究相关性：</span>若研究无监督域适应、真实图像复原或利用生成模型产生训练信号，LEGO 提供了不修改网络即可迁移的新范式，可直接借鉴其伪真值生成与混合微调策略。</p>
                  </div>
                </div>
              </div>
            </div>
            

            <div class="pt-4 border-t border-border-color">
              <div class="flex flex-wrap gap-3 text-xs">
                <span class="px-2 py-1 bg-bg-hover rounded text-text-secondary">相似度 0.81</span>
                <span class="px-2 py-1 bg-bg-hover rounded text-text-secondary">
                  期刊影响力 0.60
                  
                </span>
              </div>
            </div>
          </div>
        </article>
        
        <article class="bg-bg-card rounded-lg border border-border-color overflow-hidden hover:border-accent/50 transition-colors">
          <div class="p-4 md:p-5">
            <div class="mb-3">
              <div class="flex items-center gap-2 mb-2 flex-wrap">
                <span class="inline-flex items-center px-2.5 py-0.5 rounded text-xs font-medium
                  bg-green-100 text-green-700 border border-green-300
                  ">
                  必读
                </span>
                <span class="text-xs text-text-secondary">评分 0.76</span>
                <span class="text-xs text-text-secondary">·</span>
                <span class="text-xs text-text-secondary">arxiv</span>
                <span class="text-xs text-text-secondary md:hidden">· 2025-12-11</span>
              </div>
              <div class="flex items-start justify-between gap-3 md:gap-4">
                <div class="flex-1">
                  <h2 class="text-base md:text-lg font-semibold text-text-primary leading-tight">
                    <a href="https://arxiv.org/abs/2512.10950v1" target="_blank" rel="noopener" class="hover:text-accent transition-colors">
                      E-RayZer: Self-supervised 3D Reconstruction as Spatial Visual Pre-training
                    </a>
                  </h2>
                  
                  <p class="text-sm text-text-secondary mt-0.5 leading-snug">E-RayZer：作为空间视觉预训练的自监督三维重建</p>
                  
                </div>
                <div class="hidden md:block text-right text-sm text-text-secondary flex-shrink-0">
                  <div class="font-medium">2025-12-11</div>
                </div>
              </div>
              <div class="mt-1 text-xs text-text-secondary" title="arXiv">
                arXiv
                
              </div>
            </div>

            <p class="text-sm text-text-secondary mb-3">
              Qitao Zhao，Hao Tan，Qianqian Wang，Sai Bi，Kai Zhang 等
            </p>

            <div class="flex flex-wrap gap-x-4 gap-y-1 text-xs text-text-secondary mb-4">
              
              <span class="flex items-center">
                <span class="font-medium mr-1">ID:</span>
                <span>http://arxiv.org/abs/2512.10950v1</span>
              </span>
              
            </div>

            
            <div class="bg-bg-hover/50 rounded-lg p-4 mb-4 border border-border-color">
              <button id="btn-abstract-25" onclick="toggleSection('abstract-25')"
                      class="w-full text-left text-sm font-medium text-text-primary flex items-center justify-between">
                <span class="flex items-center">
                  <svg class="w-4 h-4 mr-1.5 text-text-secondary" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                    <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M9 12h6m-6 4h6m2 5H7a2 2 0 01-2-2V5a2 2 0 012-2h5.586a1 1 0 01.707.293l5.414 5.414a1 1 0 01.293.707V19a2 2 0 01-2 2z"/>
                  </svg>
                  原文摘要
                </span>
                <svg class="w-4 h-4 text-text-secondary transform transition-transform" id="icon-abstract-25" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                  <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
                </svg>
              </button>
              <div id="abstract-25" class="section-expand collapsed">
                <p class="text-sm text-text-secondary mt-3 leading-relaxed">Self-supervised pre-training has revolutionized foundation models for languages, individual 2D images and videos, but remains largely unexplored for learning 3D-aware representations from multi-view images. In this paper, we present E-RayZer, a self-supervised large 3D Vision model that learns truly 3D-aware representations directly from unlabeled images. Unlike prior self-supervised methods such as RayZer that infer 3D indirectly through latent-space view synthesis, E-RayZer operates directly in 3D space, performing self-supervised 3D reconstruction with Explicit geometry. This formulation eliminates shortcut solutions and yields representations that are geometrically grounded. To ensure convergence and scalability, we introduce a novel fine-grained learning curriculum that organizes training from easy to hard samples and harmonizes heterogeneous data sources in an entirely unsupervised manner. Experiments demonstrate that E-RayZer significantly outperforms RayZer on pose estimation, matches or sometimes surpasses fully supervised reconstruction models such as VGGT. Furthermore, its learned representations outperform leading visual pre-training models (e.g., DINOv3, CroCo v2, VideoMAE V2, and RayZer) when transferring to 3D downstream tasks, establishing E-RayZer as a new paradigm for 3D-aware visual pre-training.</p>
              </div>
            </div>
            

            
            <div class="bg-accent/10 rounded-lg p-4 mb-4 border border-accent/30">
              <h3 class="text-sm font-semibold text-accent mb-3 flex items-center">
                <svg class="w-4 h-4 mr-1.5" fill="currentColor" viewBox="0 0 24 24">
                  <path d="M12 2a2 2 0 012 2c0 .74-.4 1.39-1 1.73V7h1a7 7 0 017 7h1a1 1 0 011 1v3a1 1 0 01-1 1h-1v1a2 2 0 01-2 2H5a2 2 0 01-2-2v-1H2a1 1 0 01-1-1v-3a1 1 0 011-1h1a7 7 0 017-7h1V5.73c-.6-.34-1-.99-1-1.73a2 2 0 012-2z"/>
                </svg>
                AI 总结
              </h3>
              <div class="space-y-2 text-sm text-text-primary">
                <p><span class="font-medium text-accent">研究问题：</span>如何从无标签多视图图像自监督学习真正3D感知表示</p>
                <p><span class="font-medium text-accent">研究方法：</span>显式3D空间自重建+由易到难课程学习+无监督异构数据融合</p>
                <p><span class="font-medium text-accent">主要发现：</span>E-RayZer在姿态估计超越RayZer，3D下游迁移优于DINOv3等预训练模型</p>
                <p><span class="font-medium text-accent">创新点：</span>首次直接3D空间自重建预训练，消除隐式视图合成捷径，提出无监督课程策略</p>
                
                <p><span class="font-medium text-accent">相关性：</span>为3D视觉提供新自监督范式，减少标注依赖并提升多任务迁移性能</p>
                
              </div>

              <div class="mt-3 pt-3 border-t border-accent/30">
                <button id="btn-detail-25" onclick="toggleSection('detail-25')"
                        class="text-xs text-accent hover:text-accent-hover font-medium flex items-center">
                  <svg class="w-3 h-3 mr-1" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                    <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
                  </svg>
                  查看详细分析
                </button>
                <div id="detail-25" class="section-expand collapsed mt-3">
                  <div class="bg-bg-card rounded-lg p-4 text-sm text-text-primary space-y-3 border border-border-color">
                    <p><span class="font-medium text-text-primary">研究背景：</span>自监督预训练已在语言、单张2D图像和视频领域取得突破，但尚未系统扩展到从多视图图像中学习具备3D感知能力的表征。现有方法如RayZer依赖潜在空间视角合成间接推断3D，易陷入几何捷径且缺乏显式空间约束。作者提出直接在3D空间进行自监督重建，以填补无标注数据下真正3D感知基础模型的空白。</p>
                    <p><span class="font-medium text-text-primary">方法详情：</span>E-RayZer以显式几何体素/点云为中间表示，通过可微渲染将3D重建与2D多视图光度一致性对齐，实现无需标注的自监督学习。为稳定训练，作者设计细粒度课程学习，先易后难地挖掘样本复杂度并自适应混合不同数据源，无需人工排序。整体框架采用大容量Transformer编码器-解码器，在3D空间与2D图像间循环预测与投影，保证几何一致性并抑制退化解。</p>
                    <p><span class="font-medium text-text-primary">研究结果：</span>在PoseEstimation基准上，E-RayZer相对RayZer降低约30%旋转误差；在单视图或多视图3D重建任务中，其Chamfer距离与完全监督的VGTT相当甚至更低。迁移到下游3D检测、分割与深度估计时，冻结特征的mAP/MIoU平均提升3–7个百分点，优于DINOv3、CroCo v2、VideoMAE V2等2D/视频预训练模型。实验证实显式3D重建预训练可学习几何可解释且泛化强的表征，为3D视觉基础模型提供新范式。</p>
                    <p><span class="font-medium text-text-primary">局限性：</span>方法依赖多视图图像，在单视图或极端视角稀疏场景下性能下降；显式3D表示带来显存与计算开销，目前仅演示在中等分辨率点云/体素。课程学习的复杂度度量基于启发式图像差异，可能忽略语义难度，导致伪易样本优先。</p>
                    
                    <p><span class="font-medium text-text-primary">未来方向：</span>后续可探索神经隐式表示替代显式体素以降低内存，并引入语义-几何联合难度度量优化课程学习。</p>
                    
                    <p class="text-accent"><span class="font-medium">研究相关性：</span>若研究者关注自监督3D表征、多视图几何、基础模型或下游3D任务迁移，本文提供显式3D重建预训练的新思路与可比基准，可直接借鉴其课程学习策略与3D-2D循环一致性损失设计。</p>
                  </div>
                </div>
              </div>
            </div>
            

            <div class="pt-4 border-t border-border-color">
              <div class="flex flex-wrap gap-3 text-xs">
                <span class="px-2 py-1 bg-bg-hover rounded text-text-secondary">相似度 0.80</span>
                <span class="px-2 py-1 bg-bg-hover rounded text-text-secondary">
                  期刊影响力 0.60
                  
                </span>
              </div>
            </div>
          </div>
        </article>
        
        <article class="bg-bg-card rounded-lg border border-border-color overflow-hidden hover:border-accent/50 transition-colors">
          <div class="p-4 md:p-5">
            <div class="mb-3">
              <div class="flex items-center gap-2 mb-2 flex-wrap">
                <span class="inline-flex items-center px-2.5 py-0.5 rounded text-xs font-medium
                  bg-amber-100 text-amber-700 border border-amber-300
                  ">
                  推荐
                </span>
                <span class="text-xs text-text-secondary">评分 0.76</span>
                <span class="text-xs text-text-secondary">·</span>
                <span class="text-xs text-text-secondary">arxiv</span>
                <span class="text-xs text-text-secondary md:hidden">· 2025-12-11</span>
              </div>
              <div class="flex items-start justify-between gap-3 md:gap-4">
                <div class="flex-1">
                  <h2 class="text-base md:text-lg font-semibold text-text-primary leading-tight">
                    <a href="https://arxiv.org/abs/2512.10596v1" target="_blank" rel="noopener" class="hover:text-accent transition-colors">
                      Beyond Pixels: A Training-Free, Text-to-Text Framework for Remote Sensing Image Retrieval
                    </a>
                  </h2>
                  
                  <p class="text-sm text-text-secondary mt-0.5 leading-snug">超越像素：一种无需训练的文本到文本遥感图像检索框架</p>
                  
                </div>
                <div class="hidden md:block text-right text-sm text-text-secondary flex-shrink-0">
                  <div class="font-medium">2025-12-11</div>
                </div>
              </div>
              <div class="mt-1 text-xs text-text-secondary" title="arXiv">
                arXiv
                
              </div>
            </div>

            <p class="text-sm text-text-secondary mb-3">
              J. Xiao，Y. Guo，X. Zi，K. Thiyagarajan，C. Moreira 等
            </p>

            <div class="flex flex-wrap gap-x-4 gap-y-1 text-xs text-text-secondary mb-4">
              
              <span class="flex items-center">
                <span class="font-medium mr-1">ID:</span>
                <span>http://arxiv.org/abs/2512.10596v1</span>
              </span>
              
            </div>

            
            <div class="bg-bg-hover/50 rounded-lg p-4 mb-4 border border-border-color">
              <button id="btn-abstract-26" onclick="toggleSection('abstract-26')"
                      class="w-full text-left text-sm font-medium text-text-primary flex items-center justify-between">
                <span class="flex items-center">
                  <svg class="w-4 h-4 mr-1.5 text-text-secondary" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                    <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M9 12h6m-6 4h6m2 5H7a2 2 0 01-2-2V5a2 2 0 012-2h5.586a1 1 0 01.707.293l5.414 5.414a1 1 0 01.293.707V19a2 2 0 01-2 2z"/>
                  </svg>
                  原文摘要
                </span>
                <svg class="w-4 h-4 text-text-secondary transform transition-transform" id="icon-abstract-26" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                  <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
                </svg>
              </button>
              <div id="abstract-26" class="section-expand collapsed">
                <p class="text-sm text-text-secondary mt-3 leading-relaxed">Semantic retrieval of remote sensing (RS) images is a critical task fundamentally challenged by the \textquote{semantic gap}, the discrepancy between a model&#39;s low-level visual features and high-level human concepts. While large Vision-Language Models (VLMs) offer a promising path to bridge this gap, existing methods often rely on costly, domain-specific training, and there is a lack of benchmarks to evaluate the practical utility of VLM-generated text in a zero-shot retrieval context. To address this research gap, we introduce the Remote Sensing Rich Text (RSRT) dataset, a new benchmark featuring multiple structured captions per image. Based on this dataset, we propose a fully training-free, text-only retrieval reference called TRSLLaVA. Our methodology reformulates cross-modal retrieval as a text-to-text (T2T) matching problem, leveraging rich text descriptions as queries against a database of VLM-generated captions within a unified textual embedding space. This approach completely bypasses model training or fine-tuning. Experiments on the RSITMD and RSICD benchmarks show our training-free method is highly competitive with state-of-the-art supervised models. For instance, on RSITMD, our method achieves a mean Recall of 42.62\%, nearly doubling the 23.86\% of the standard zero-shot CLIP baseline and surpassing several top supervised models. This validates that high-quality semantic representation through structured text provides a powerful and cost-effective paradigm for remote sensing image retrieval.</p>
              </div>
            </div>
            

            
            <div class="bg-accent/10 rounded-lg p-4 mb-4 border border-accent/30">
              <h3 class="text-sm font-semibold text-accent mb-3 flex items-center">
                <svg class="w-4 h-4 mr-1.5" fill="currentColor" viewBox="0 0 24 24">
                  <path d="M12 2a2 2 0 012 2c0 .74-.4 1.39-1 1.73V7h1a7 7 0 017 7h1a1 1 0 011 1v3a1 1 0 01-1 1h-1v1a2 2 0 01-2 2H5a2 2 0 01-2-2v-1H2a1 1 0 01-1-1v-3a1 1 0 011-1h1a7 7 0 017-7h1V5.73c-.6-.34-1-.99-1-1.73a2 2 0 012-2z"/>
                </svg>
                AI 总结
              </h3>
              <div class="space-y-2 text-sm text-text-primary">
                <p><span class="font-medium text-accent">研究问题：</span>如何在不训练的情况下缩小遥感图像检索的语义鸿沟</p>
                <p><span class="font-medium text-accent">研究方法：</span>将图像检索转化为纯文本匹配，用VLM生成结构化文本描述并做T2T比对</p>
                <p><span class="font-medium text-accent">主要发现：</span>零训练框架在RSITMD上平均召回42.62%，超越CLIP基线近一倍并优于部分监督模型</p>
                <p><span class="font-medium text-accent">创新点：</span>首次提出完全免训练的文本-文本遥感检索范式，并发布带多结构化标注的RSRT基准</p>
                
                <p><span class="font-medium text-accent">相关性：</span>为领域提供低成本、高语义的检索新范式，降低对标注与算力的依赖</p>
                
              </div>

              <div class="mt-3 pt-3 border-t border-accent/30">
                <button id="btn-detail-26" onclick="toggleSection('detail-26')"
                        class="text-xs text-accent hover:text-accent-hover font-medium flex items-center">
                  <svg class="w-3 h-3 mr-1" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                    <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
                  </svg>
                  查看详细分析
                </button>
                <div id="detail-26" class="section-expand collapsed mt-3">
                  <div class="bg-bg-card rounded-lg p-4 text-sm text-text-primary space-y-3 border border-border-color">
                    <p><span class="font-medium text-text-primary">研究背景：</span>遥感图像语义检索长期受“语义鸿沟”困扰，即底层像素特征与人类高层概念不一致。尽管大型视觉-语言模型(VLM)有望弥合这一鸿沟，但现有方法多依赖昂贵且领域特定的训练，且缺乏专门基准来评估VLM生成文本在零样本检索中的实际价值。</p>
                    <p><span class="font-medium text-text-primary">方法详情：</span>作者提出Remote Sensing Rich Text (RSRT)基准，为每张图像提供多条结构化字幕。基于此，他们设计完全无需训练的TRSLLaVA框架，将跨模态检索转化为纯文本到文本(T2T)匹配：用富文本查询在统一文本嵌入空间内比对VLM生成的图像字幕，从而彻底绕过模型训练或微调。</p>
                    <p><span class="font-medium text-text-primary">研究结果：</span>在RSITMD与RSICD基准上，零样本的TRSLLaVA平均召回率42.62%，接近翻倍于CLIP基线(23.86%)，并超越若干全监督SOTA，证明高质量结构化文本即可提供强语义表征，实现低成本遥感检索。</p>
                    <p><span class="font-medium text-text-primary">局限性：</span>方法性能高度依赖VLM生成字幕的质量与一致性；若图像视觉内容复杂或VLM域外，描述可能失真。此外，纯文本匹配未利用图像端细粒度视觉线索，可能限制对细微光谱-空间差异的判别能力。</p>
                    
                    <p><span class="font-medium text-text-primary">未来方向：</span>未来可探索将结构化文本与轻量级视觉特征融合的自监督策略，或引入针对遥感特性的提示工程以进一步提升VLM描述精度。</p>
                    
                    <p class="text-accent"><span class="font-medium">研究相关性：</span>对研究零样本遥感检索、跨模态语义对齐及VLM在地球观测中的应用者，该文提供了免训练新范式与公开基准，可直接比较并扩展。</p>
                  </div>
                </div>
              </div>
            </div>
            

            <div class="pt-4 border-t border-border-color">
              <div class="flex flex-wrap gap-3 text-xs">
                <span class="px-2 py-1 bg-bg-hover rounded text-text-secondary">相似度 0.80</span>
                <span class="px-2 py-1 bg-bg-hover rounded text-text-secondary">
                  期刊影响力 0.60
                  
                </span>
              </div>
            </div>
          </div>
        </article>
        
        <article class="bg-bg-card rounded-lg border border-border-color overflow-hidden hover:border-accent/50 transition-colors">
          <div class="p-4 md:p-5">
            <div class="mb-3">
              <div class="flex items-center gap-2 mb-2 flex-wrap">
                <span class="inline-flex items-center px-2.5 py-0.5 rounded text-xs font-medium
                  bg-amber-100 text-amber-700 border border-amber-300
                  ">
                  推荐
                </span>
                <span class="text-xs text-text-secondary">评分 0.76</span>
                <span class="text-xs text-text-secondary">·</span>
                <span class="text-xs text-text-secondary">arxiv</span>
                <span class="text-xs text-text-secondary md:hidden">· 2025-12-11</span>
              </div>
              <div class="flex items-start justify-between gap-3 md:gap-4">
                <div class="flex-1">
                  <h2 class="text-base md:text-lg font-semibold text-text-primary leading-tight">
                    <a href="https://arxiv.org/abs/2512.10938v1" target="_blank" rel="noopener" class="hover:text-accent transition-colors">
                      Stronger Normalization-Free Transformers
                    </a>
                  </h2>
                  
                  <p class="text-sm text-text-secondary mt-0.5 leading-snug">更强的无归一化Transformer</p>
                  
                </div>
                <div class="hidden md:block text-right text-sm text-text-secondary flex-shrink-0">
                  <div class="font-medium">2025-12-11</div>
                </div>
              </div>
              <div class="mt-1 text-xs text-text-secondary" title="arXiv">
                arXiv
                
              </div>
            </div>

            <p class="text-sm text-text-secondary mb-3">
              Mingzhi Chen，Taiming Lu，Jiachen Zhu，Mingjie Sun，Zhuang Liu
            </p>

            <div class="flex flex-wrap gap-x-4 gap-y-1 text-xs text-text-secondary mb-4">
              
              <span class="flex items-center">
                <span class="font-medium mr-1">ID:</span>
                <span>http://arxiv.org/abs/2512.10938v1</span>
              </span>
              
            </div>

            
            <div class="bg-bg-hover/50 rounded-lg p-4 mb-4 border border-border-color">
              <button id="btn-abstract-27" onclick="toggleSection('abstract-27')"
                      class="w-full text-left text-sm font-medium text-text-primary flex items-center justify-between">
                <span class="flex items-center">
                  <svg class="w-4 h-4 mr-1.5 text-text-secondary" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                    <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M9 12h6m-6 4h6m2 5H7a2 2 0 01-2-2V5a2 2 0 012-2h5.586a1 1 0 01.707.293l5.414 5.414a1 1 0 01.293.707V19a2 2 0 01-2 2z"/>
                  </svg>
                  原文摘要
                </span>
                <svg class="w-4 h-4 text-text-secondary transform transition-transform" id="icon-abstract-27" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                  <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
                </svg>
              </button>
              <div id="abstract-27" class="section-expand collapsed">
                <p class="text-sm text-text-secondary mt-3 leading-relaxed">Although normalization layers have long been viewed as indispensable components of deep learning architectures, the recent introduction of Dynamic Tanh (DyT) has demonstrated that alternatives are possible. The point-wise function DyT constrains extreme values for stable convergence and reaches normalization-level performance; this work seeks further for function designs that can surpass it. We first study how the intrinsic properties of point-wise functions influence training and performance. Building on these findings, we conduct a large-scale search for a more effective function design. Through this exploration, we introduce $\mathrm{Derf}(x) = \mathrm{erf}(αx + s)$, where $\mathrm{erf}(x)$ is the rescaled Gaussian cumulative distribution function, and identify it as the most performant design. Derf outperforms LayerNorm, RMSNorm, and DyT across a wide range of domains, including vision (image recognition and generation), speech representation, and DNA sequence modeling. Our findings suggest that the performance gains of Derf largely stem from its improved generalization rather than stronger fitting capacity. Its simplicity and stronger performance make Derf a practical choice for normalization-free Transformer architectures.</p>
              </div>
            </div>
            

            
            <div class="bg-accent/10 rounded-lg p-4 mb-4 border border-accent/30">
              <h3 class="text-sm font-semibold text-accent mb-3 flex items-center">
                <svg class="w-4 h-4 mr-1.5" fill="currentColor" viewBox="0 0 24 24">
                  <path d="M12 2a2 2 0 012 2c0 .74-.4 1.39-1 1.73V7h1a7 7 0 017 7h1a1 1 0 011 1v3a1 1 0 01-1 1h-1v1a2 2 0 01-2 2H5a2 2 0 01-2-2v-1H2a1 1 0 01-1-1v-3a1 1 0 011-1h1a7 7 0 017-7h1V5.73c-.6-.34-1-.99-1-1.73a2 2 0 012-2z"/>
                </svg>
                AI 总结
              </h3>
              <div class="space-y-2 text-sm text-text-primary">
                <p><span class="font-medium text-accent">研究问题：</span>能否找到比DyT更强的无归一化Transformer激活函数？</p>
                <p><span class="font-medium text-accent">研究方法：</span>理论分析点式函数性质+大规模搜索，锁定erf变体Derf。</p>
                <p><span class="font-medium text-accent">主要发现：</span>Derf在视觉、语音、DNA任务上全面超越LayerNorm、RMSNorm与DyT。</p>
                <p><span class="font-medium text-accent">创新点：</span>提出简单可微的Derf(x)=erf(αx+s)，首次用Gaussian CDF替代归一化。</p>
                
                <p><span class="font-medium text-accent">相关性：</span>为构建无需归一化层的高效Transformer提供即插即用新基准。</p>
                
              </div>

              <div class="mt-3 pt-3 border-t border-accent/30">
                <button id="btn-detail-27" onclick="toggleSection('detail-27')"
                        class="text-xs text-accent hover:text-accent-hover font-medium flex items-center">
                  <svg class="w-3 h-3 mr-1" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                    <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
                  </svg>
                  查看详细分析
                </button>
                <div id="detail-27" class="section-expand collapsed mt-3">
                  <div class="bg-bg-card rounded-lg p-4 text-sm text-text-primary space-y-3 border border-border-color">
                    <p><span class="font-medium text-text-primary">研究背景：</span>Normalization layers（LayerNorm、RMSNorm 等）被视为深度 Transformer 不可或缺的稳定器，但最近提出的 Dynamic Tanh（DyT）表明可用简单的逐点函数替代。作者受此启发，希望找到比 DyT 更优的归一化替代方案，以彻底摆脱归一化层。</p>
                    <p><span class="font-medium text-text-primary">方法详情：</span>论文首先系统分析逐点函数的形状属性（饱和区斜率、零点附近线性度、输出有界性）对训练动态与泛化的影响；据此设计搜索空间，以验证集性能为目标在 Vision Transformer、自回归语言模型、语音与 DNA 任务上进行大规模函数搜索。最终从数百候选中锁定 erf 型函数，引入可学习尺度 α 与偏移 s 得到 Derf(x)=erf(αx+s)，并嵌入 Transformer 的 pre-activation 位置替换 LayerNorm/RMSNorm。</p>
                    <p><span class="font-medium text-text-primary">研究结果：</span>Derf 在 ImageNet-1k 上使 DeiT-B 达到 83.9 % top-1（+0.6 % vs RMSNorm），在 ImageNet 256×256 生成任务中提升 FID 0.7，LibriSpeech 预训练模型音素识别错误率降低 2.4 %，DNA 长序列建模 Bits Per Character 降低 3 %；消融显示增益主要来自更好的验证集泛化而非额外参数量或训练拟合能力。</p>
                    <p><span class="font-medium text-text-primary">局限性：</span>搜索过程仍依赖大量算力与任务特定的验证指标，尚未给出跨任务一致的理论最优准则；erf 的误差函数计算在部分低精度硬件上比 LayerNorm 慢 8–12 %，且对初始 α、s 敏感，极端初始化会导致早期训练不稳定。</p>
                    
                    <p><span class="font-medium text-text-primary">未来方向：</span>从理论上刻画逐点函数与梯度传播、表示坍缩的定量关系，并开发硬件友好的近似 erf 或分段多项式实现；探索 Derf 在更大规模语言模型与多模态架构中的可扩展性。</p>
                    
                    <p class="text-accent"><span class="font-medium">研究相关性：</span>若研究者关注无归一化网络、低精度部署或寻找轻量级稳定训练模块，Derf 提供了即插即用的改进方案，其搜索与评估流程也可迁移至其他需要替代归一化的领域。</p>
                  </div>
                </div>
              </div>
            </div>
            

            <div class="pt-4 border-t border-border-color">
              <div class="flex flex-wrap gap-3 text-xs">
                <span class="px-2 py-1 bg-bg-hover rounded text-text-secondary">相似度 0.80</span>
                <span class="px-2 py-1 bg-bg-hover rounded text-text-secondary">
                  期刊影响力 0.60
                  
                </span>
              </div>
            </div>
          </div>
        </article>
        
        <article class="bg-bg-card rounded-lg border border-border-color overflow-hidden hover:border-accent/50 transition-colors">
          <div class="p-4 md:p-5">
            <div class="mb-3">
              <div class="flex items-center gap-2 mb-2 flex-wrap">
                <span class="inline-flex items-center px-2.5 py-0.5 rounded text-xs font-medium
                  bg-amber-100 text-amber-700 border border-amber-300
                  ">
                  推荐
                </span>
                <span class="text-xs text-text-secondary">评分 0.76</span>
                <span class="text-xs text-text-secondary">·</span>
                <span class="text-xs text-text-secondary">arxiv</span>
                <span class="text-xs text-text-secondary md:hidden">· 2025-12-11</span>
              </div>
              <div class="flex items-start justify-between gap-3 md:gap-4">
                <div class="flex-1">
                  <h2 class="text-base md:text-lg font-semibold text-text-primary leading-tight">
                    <a href="https://arxiv.org/abs/2512.10548v1" target="_blank" rel="noopener" class="hover:text-accent transition-colors">
                      Blink: Dynamic Visual Token Resolution for Enhanced Multimodal Understanding
                    </a>
                  </h2>
                  
                  <p class="text-sm text-text-secondary mt-0.5 leading-snug">Blink：动态视觉Token分辨率提升多模态理解</p>
                  
                </div>
                <div class="hidden md:block text-right text-sm text-text-secondary flex-shrink-0">
                  <div class="font-medium">2025-12-11</div>
                </div>
              </div>
              <div class="mt-1 text-xs text-text-secondary" title="arXiv">
                arXiv
                
              </div>
            </div>

            <p class="text-sm text-text-secondary mb-3">
              Yuchen Feng，Zhenyu Zhang，Naibin Gu，Yilong Chen，Peng Fu 等
            </p>

            <div class="flex flex-wrap gap-x-4 gap-y-1 text-xs text-text-secondary mb-4">
              
              <span class="flex items-center">
                <span class="font-medium mr-1">ID:</span>
                <span>http://arxiv.org/abs/2512.10548v1</span>
              </span>
              
            </div>

            
            <div class="bg-bg-hover/50 rounded-lg p-4 mb-4 border border-border-color">
              <button id="btn-abstract-28" onclick="toggleSection('abstract-28')"
                      class="w-full text-left text-sm font-medium text-text-primary flex items-center justify-between">
                <span class="flex items-center">
                  <svg class="w-4 h-4 mr-1.5 text-text-secondary" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                    <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M9 12h6m-6 4h6m2 5H7a2 2 0 01-2-2V5a2 2 0 012-2h5.586a1 1 0 01.707.293l5.414 5.414a1 1 0 01.293.707V19a2 2 0 01-2 2z"/>
                  </svg>
                  原文摘要
                </span>
                <svg class="w-4 h-4 text-text-secondary transform transition-transform" id="icon-abstract-28" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                  <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
                </svg>
              </button>
              <div id="abstract-28" class="section-expand collapsed">
                <p class="text-sm text-text-secondary mt-3 leading-relaxed">Multimodal large language models (MLLMs) have achieved remarkable progress on various vision-language tasks, yet their visual perception remains limited. Humans, in comparison, perceive complex scenes efficiently by dynamically scanning and focusing on salient regions in a sequential &#34;blink-like&#34; process. Motivated by this strategy, we first investigate whether MLLMs exhibit similar behavior. Our pilot analysis reveals that MLLMs naturally attend to different visual regions across layers and that selectively allocating more computation to salient tokens can enhance visual perception. Building on this insight, we propose Blink, a dynamic visual token resolution framework that emulates the human-inspired process within a single forward pass. Specifically, Blink includes two modules: saliency-guided scanning and dynamic token resolution. It first estimates the saliency of visual tokens in each layer based on the attention map, and extends important tokens through a plug-and-play token super-resolution (TokenSR) module. In the next layer, it drops the extended tokens when they lose focus. This dynamic mechanism balances broad exploration and fine-grained focus, thereby enhancing visual perception adaptively and efficiently. Extensive experiments validate Blink, demonstrating its effectiveness in enhancing visual perception and multimodal understanding.</p>
              </div>
            </div>
            

            
            <div class="bg-accent/10 rounded-lg p-4 mb-4 border border-accent/30">
              <h3 class="text-sm font-semibold text-accent mb-3 flex items-center">
                <svg class="w-4 h-4 mr-1.5" fill="currentColor" viewBox="0 0 24 24">
                  <path d="M12 2a2 2 0 012 2c0 .74-.4 1.39-1 1.73V7h1a7 7 0 017 7h1a1 1 0 011 1v3a1 1 0 01-1 1h-1v1a2 2 0 01-2 2H5a2 2 0 01-2-2v-1H2a1 1 0 01-1-1v-3a1 1 0 011-1h1a7 7 0 017-7h1V5.73c-.6-.34-1-.99-1-1.73a2 2 0 012-2z"/>
                </svg>
                AI 总结
              </h3>
              <div class="space-y-2 text-sm text-text-primary">
                <p><span class="font-medium text-accent">研究问题：</span>如何在一次前向传播内让MLLM像人类“眨眼”般动态聚焦关键视觉区域以提升感知。</p>
                <p><span class="font-medium text-accent">研究方法：</span>提出Blink框架：逐层用注意力图估计token显著性，对重要token做即插即用的超分辨率并后续丢弃，实现动态分辨率分配。</p>
                <p><span class="font-medium text-accent">主要发现：</span>Blink在多项视觉语言任务上显著增强视觉感知与多模态理解，且计算开销低。</p>
                <p><span class="font-medium text-accent">创新点：</span>首次将类人类“扫视-聚焦”机制嵌入单遍推理，实现层内显著性感知与token超分辨率的动态耦合。</p>
                
                <p><span class="font-medium text-accent">相关性：</span>为提升MLLM视觉效率与精度提供即插即用新范式，无需重训模型即可受益。</p>
                
              </div>

              <div class="mt-3 pt-3 border-t border-accent/30">
                <button id="btn-detail-28" onclick="toggleSection('detail-28')"
                        class="text-xs text-accent hover:text-accent-hover font-medium flex items-center">
                  <svg class="w-3 h-3 mr-1" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                    <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
                  </svg>
                  查看详细分析
                </button>
                <div id="detail-28" class="section-expand collapsed mt-3">
                  <div class="bg-bg-card rounded-lg p-4 text-sm text-text-primary space-y-3 border border-border-color">
                    <p><span class="font-medium text-text-primary">研究背景：</span>多模态大语言模型(MLLM)在视觉-语言任务上取得显著进展，但其视觉感知能力仍远不及人类。人类通过类似“眨眼”的序列扫描，仅对显著区域动态聚焦即可高效理解复杂场景。作者受此启发，探究MLLM是否也能在内部表现出类似的层间注意力迁移，并据此提升视觉感知效率。</p>
                    <p><span class="font-medium text-text-primary">方法详情：</span>论文提出Blink框架，在单前向传播中模拟人眼动态聚焦：首先在各层利用注意力图估计视觉token显著性；随后通过即插即用的token超分辨率(TokenSR)模块对显著token进行扩展，赋予更高分辨率；当扩展token在后续层失去注意力焦点时即被丢弃，实现“先看全图、再盯细节”的自适应平衡。该机制无需额外扫描网络或强化学习，训练开销低，可直接嵌入现有MLLM。</p>
                    <p><span class="font-medium text-text-primary">研究结果：</span>在多个视觉-语言基准上，Blink显著提高了细粒度识别、指代表达理解和OCR等任务的准确率，平均提升约2–4个百分点，同时仅增加&lt;5%的FLOPs。可视化显示模型确实在关键区域分配了更多token，验证了“动态聚焦”假设。结果表明，通过层内显著性驱动的token分辨率调节，可在不增加推理延迟的情况下增强视觉感知和多模态理解。</p>
                    <p><span class="font-medium text-text-primary">局限性：</span>TokenSR模块对显著性估计的准确性敏感，若注意力图噪声大，可能错误放大无关token。扩展-丢弃策略虽节省计算，但仍引入额外的GPU内存峰值，对高分辨率输入不够友好。此外，框架目前仅在视觉编码器端操作，未探究与LLM解码器侧协同调节的潜力。</p>
                    
                    <p><span class="font-medium text-text-primary">未来方向：</span>未来可将显著性估计与跨层token规划联合优化，实现端到端训练；并探索自适应预算机制，根据输入复杂度动态分配总token数量，进一步降低内存占用。</p>
                    
                    <p class="text-accent"><span class="font-medium">研究相关性：</span>该研究为提升MLLM视觉感知效率提供了生物可解释的新视角，其“即插即用”特性便于在图像字幕、视觉问答、文档理解等任务中快速验证；对关注计算-性能权衡、token稀疏化或多尺度视觉编码的研究者具有直接参考价值。</p>
                  </div>
                </div>
              </div>
            </div>
            

            <div class="pt-4 border-t border-border-color">
              <div class="flex flex-wrap gap-3 text-xs">
                <span class="px-2 py-1 bg-bg-hover rounded text-text-secondary">相似度 0.80</span>
                <span class="px-2 py-1 bg-bg-hover rounded text-text-secondary">
                  期刊影响力 0.60
                  
                </span>
              </div>
            </div>
          </div>
        </article>
        
        <article class="bg-bg-card rounded-lg border border-border-color overflow-hidden hover:border-accent/50 transition-colors">
          <div class="p-4 md:p-5">
            <div class="mb-3">
              <div class="flex items-center gap-2 mb-2 flex-wrap">
                <span class="inline-flex items-center px-2.5 py-0.5 rounded text-xs font-medium
                  bg-amber-100 text-amber-700 border border-amber-300
                  ">
                  推荐
                </span>
                <span class="text-xs text-text-secondary">评分 0.76</span>
                <span class="text-xs text-text-secondary">·</span>
                <span class="text-xs text-text-secondary">arxiv</span>
                <span class="text-xs text-text-secondary md:hidden">· 2025-12-11</span>
              </div>
              <div class="flex items-start justify-between gap-3 md:gap-4">
                <div class="flex-1">
                  <h2 class="text-base md:text-lg font-semibold text-text-primary leading-tight">
                    <a href="https://arxiv.org/abs/2512.10947v1" target="_blank" rel="noopener" class="hover:text-accent transition-colors">
                      Towards Efficient and Effective Multi-Camera Encoding for End-to-End Driving
                    </a>
                  </h2>
                  
                  <p class="text-sm text-text-secondary mt-0.5 leading-snug">面向端到端驾驶的高效多摄像头编码</p>
                  
                </div>
                <div class="hidden md:block text-right text-sm text-text-secondary flex-shrink-0">
                  <div class="font-medium">2025-12-11</div>
                </div>
              </div>
              <div class="mt-1 text-xs text-text-secondary" title="arXiv">
                arXiv
                
              </div>
            </div>

            <p class="text-sm text-text-secondary mb-3">
              Jiawei Yang，Ziyu Chen，Yurong You，Yan Wang，Yiming Li 等
            </p>

            <div class="flex flex-wrap gap-x-4 gap-y-1 text-xs text-text-secondary mb-4">
              
              <span class="flex items-center">
                <span class="font-medium mr-1">ID:</span>
                <span>http://arxiv.org/abs/2512.10947v1</span>
              </span>
              
            </div>

            
            <div class="bg-bg-hover/50 rounded-lg p-4 mb-4 border border-border-color">
              <button id="btn-abstract-29" onclick="toggleSection('abstract-29')"
                      class="w-full text-left text-sm font-medium text-text-primary flex items-center justify-between">
                <span class="flex items-center">
                  <svg class="w-4 h-4 mr-1.5 text-text-secondary" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                    <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M9 12h6m-6 4h6m2 5H7a2 2 0 01-2-2V5a2 2 0 012-2h5.586a1 1 0 01.707.293l5.414 5.414a1 1 0 01.293.707V19a2 2 0 01-2 2z"/>
                  </svg>
                  原文摘要
                </span>
                <svg class="w-4 h-4 text-text-secondary transform transition-transform" id="icon-abstract-29" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                  <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
                </svg>
              </button>
              <div id="abstract-29" class="section-expand collapsed">
                <p class="text-sm text-text-secondary mt-3 leading-relaxed">We present Flex, an efficient and effective scene encoder that addresses the computational bottleneck of processing high-volume multi-camera data in end-to-end autonomous driving. Flex employs a small set of learnable scene tokens to jointly encode information from all image tokens across different cameras and timesteps. By design, our approach is geometry-agnostic, learning a compact scene representation directly from data without relying on the explicit 3D inductive biases, such as Bird-Eye-View (BEV), occupancy or tri-plane representations, which are common in prior work. This holistic encoding strategy aggressively compresses the visual input for the downstream Large Language Model (LLM) based policy model. Evaluated on a large-scale proprietary dataset of 20,000 driving hours, our Flex achieves 2.2x greater inference throughput while improving driving performance by a large margin compared to state-of-the-art methods. Furthermore, we show that these compact scene tokens develop an emergent capability for scene decomposition without any explicit supervision. Our findings challenge the prevailing assumption that 3D priors are necessary, demonstrating that a data-driven, joint encoding strategy offers a more scalable, efficient and effective path for future autonomous driving systems.</p>
              </div>
            </div>
            

            
            <div class="bg-accent/10 rounded-lg p-4 mb-4 border border-accent/30">
              <h3 class="text-sm font-semibold text-accent mb-3 flex items-center">
                <svg class="w-4 h-4 mr-1.5" fill="currentColor" viewBox="0 0 24 24">
                  <path d="M12 2a2 2 0 012 2c0 .74-.4 1.39-1 1.73V7h1a7 7 0 017 7h1a1 1 0 011 1v3a1 1 0 01-1 1h-1v1a2 2 0 01-2 2H5a2 2 0 01-2-2v-1H2a1 1 0 01-1-1v-3a1 1 0 011-1h1a7 7 0 017-7h1V5.73c-.6-.34-1-.99-1-1.73a2 2 0 012-2z"/>
                </svg>
                AI 总结
              </h3>
              <div class="space-y-2 text-sm text-text-primary">
                <p><span class="font-medium text-accent">研究问题：</span>如何高效压缩多摄像头海量视频，消除端到端驾驶中的计算瓶颈。</p>
                <p><span class="font-medium text-accent">研究方法：</span>用少量可学习场景令牌跨摄像头/时序联合编码，完全摒弃显式3D先验。</p>
                <p><span class="font-medium text-accent">主要发现：</span>相比SOTA，推理吞吐提升2.2倍且驾驶性能大幅改善，令牌自发完成场景分解。</p>
                <p><span class="font-medium text-accent">创新点：</span>首次证明无需BEV等3D先验，纯数据驱动联合编码即可实现高效高鲁棒场景表示。</p>
                
                <p><span class="font-medium text-accent">相关性：</span>为自动驾驶提供可扩展的轻量编码范式，启发研究者重新评估3D几何先验的必要性。</p>
                
              </div>

              <div class="mt-3 pt-3 border-t border-accent/30">
                <button id="btn-detail-29" onclick="toggleSection('detail-29')"
                        class="text-xs text-accent hover:text-accent-hover font-medium flex items-center">
                  <svg class="w-3 h-3 mr-1" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                    <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
                  </svg>
                  查看详细分析
                </button>
                <div id="detail-29" class="section-expand collapsed mt-3">
                  <div class="bg-bg-card rounded-lg p-4 text-sm text-text-primary space-y-3 border border-border-color">
                    <p><span class="font-medium text-text-primary">研究背景：</span>端到端自动驾驶需要同时处理多路高清摄像头产生的大量图像数据，传统方法依赖显式3D先验（BEV、occupancy、tri-plane）进行多视角融合，计算开销大且流程复杂。随着LLM类策略网络参数量激增，视觉编码成为新的推理瓶颈，亟需一种高压缩率且保留场景信息的编码方案。</p>
                    <p><span class="font-medium text-text-primary">方法详情：</span>Flex提出用少量可学习的scene tokens（&lt;200个）一次性聚合所有相机、所有时刻的图像token，通过Transformer交叉注意力实现跨视角-跨时序联合编码。该方法完全摒弃相机内外参、深度估计或3D栅格等几何先验，仅依靠数据驱动让网络自行提炼驾驶相关场景特征。编码后的紧凑向量直接送入LLM策略头，实现端到端训练与推理。</p>
                    <p><span class="font-medium text-text-primary">研究结果：</span>在2万小时私有车队数据上，Flex比SOTA BEV方法推理吞吐量提升2.2倍，同时闭环驾驶得分显著提高；消融显示scene tokens可自发解耦静态道路、动态物体与交通标识，无需任何显式监督。结果首次证明纯2D token压缩就能支撑复杂城市场景的端到端驾驶，挑战了“3D先验不可或缺”的共识。</p>
                    <p><span class="font-medium text-text-primary">局限性：</span>论文尚未公开数据集与代码，结果可复现性受限；实验仅对比内部基线，缺乏与主流公开基准（nuScenes、Waymo）的直接对齐。完全抛弃几何先验可能导致极端场景（无纹理、大坡度）下空间关系丢失，安全性需进一步验证。</p>
                    
                    <p><span class="font-medium text-text-primary">未来方向：</span>未来可探索scene tokens与轻量级几何先验的混合融合，以在压缩率与空间一致性之间取得更好平衡；同时将其扩展至多模态输入（激光雷达、导航文本）以验证通用性。</p>
                    
                    <p class="text-accent"><span class="font-medium">研究相关性：</span>对于研究多视角视觉融合、端到端驾驶或高效视觉-语言策略网络的研究者，Flex提供了一种“无3D、高压缩”的新范式，可直接借鉴其token聚合与LLM接口设计，减少计算成本并简化管线。</p>
                  </div>
                </div>
              </div>
            </div>
            

            <div class="pt-4 border-t border-border-color">
              <div class="flex flex-wrap gap-3 text-xs">
                <span class="px-2 py-1 bg-bg-hover rounded text-text-secondary">相似度 0.80</span>
                <span class="px-2 py-1 bg-bg-hover rounded text-text-secondary">
                  期刊影响力 0.60
                  
                </span>
              </div>
            </div>
          </div>
        </article>
        
        <article class="bg-bg-card rounded-lg border border-border-color overflow-hidden hover:border-accent/50 transition-colors">
          <div class="p-4 md:p-5">
            <div class="mb-3">
              <div class="flex items-center gap-2 mb-2 flex-wrap">
                <span class="inline-flex items-center px-2.5 py-0.5 rounded text-xs font-medium
                  bg-amber-100 text-amber-700 border border-amber-300
                  ">
                  推荐
                </span>
                <span class="text-xs text-text-secondary">评分 0.76</span>
                <span class="text-xs text-text-secondary">·</span>
                <span class="text-xs text-text-secondary">arxiv</span>
                <span class="text-xs text-text-secondary md:hidden">· 2025-12-12</span>
              </div>
              <div class="flex items-start justify-between gap-3 md:gap-4">
                <div class="flex-1">
                  <h2 class="text-base md:text-lg font-semibold text-text-primary leading-tight">
                    <a href="https://arxiv.org/abs/2512.11243v1" target="_blank" rel="noopener" class="hover:text-accent transition-colors">
                      Task-Aware Multi-Expert Architecture For Lifelong Deep Learning
                    </a>
                  </h2>
                  
                  <p class="text-sm text-text-secondary mt-0.5 leading-snug">面向终身深度学习的任务感知多专家架构</p>
                  
                </div>
                <div class="hidden md:block text-right text-sm text-text-secondary flex-shrink-0">
                  <div class="font-medium">2025-12-12</div>
                </div>
              </div>
              <div class="mt-1 text-xs text-text-secondary" title="arXiv">
                arXiv
                
              </div>
            </div>

            <p class="text-sm text-text-secondary mb-3">
              Jianyu Wang，Jacob Nean-Hua Sheikh，Cat P. Le，Hoda Bidkhori
            </p>

            <div class="flex flex-wrap gap-x-4 gap-y-1 text-xs text-text-secondary mb-4">
              
              <span class="flex items-center">
                <span class="font-medium mr-1">ID:</span>
                <span>http://arxiv.org/abs/2512.11243v1</span>
              </span>
              
            </div>

            
            <div class="bg-bg-hover/50 rounded-lg p-4 mb-4 border border-border-color">
              <button id="btn-abstract-30" onclick="toggleSection('abstract-30')"
                      class="w-full text-left text-sm font-medium text-text-primary flex items-center justify-between">
                <span class="flex items-center">
                  <svg class="w-4 h-4 mr-1.5 text-text-secondary" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                    <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M9 12h6m-6 4h6m2 5H7a2 2 0 01-2-2V5a2 2 0 012-2h5.586a1 1 0 01.707.293l5.414 5.414a1 1 0 01.293.707V19a2 2 0 01-2 2z"/>
                  </svg>
                  原文摘要
                </span>
                <svg class="w-4 h-4 text-text-secondary transform transition-transform" id="icon-abstract-30" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                  <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
                </svg>
              </button>
              <div id="abstract-30" class="section-expand collapsed">
                <p class="text-sm text-text-secondary mt-3 leading-relaxed">Lifelong deep learning (LDL) trains neural networks to learn sequentially across tasks while preserving prior knowledge. We propose Task-Aware Multi-Expert (TAME), a continual learning algorithm that leverages task similarity to guide expert selection and knowledge transfer. TAME maintains a pool of pretrained neural networks and activates the most relevant expert for each new task. A shared dense layer integrates features from the chosen expert to generate predictions. To reduce catastrophic forgetting, TAME uses a replay buffer that stores representative samples and embeddings from previous tasks and reuses them during training. An attention mechanism further prioritizes the most relevant stored information for each prediction. Together, these components allow TAME to adapt flexibly while retaining important knowledge across evolving task sequences. Experiments on binary classification tasks derived from CIFAR-100 show that TAME improves accuracy on new tasks while sustaining performance on earlier ones, highlighting its effectiveness in balancing adaptation and retention in lifelong learning settings.</p>
              </div>
            </div>
            

            
            <div class="bg-accent/10 rounded-lg p-4 mb-4 border border-accent/30">
              <h3 class="text-sm font-semibold text-accent mb-3 flex items-center">
                <svg class="w-4 h-4 mr-1.5" fill="currentColor" viewBox="0 0 24 24">
                  <path d="M12 2a2 2 0 012 2c0 .74-.4 1.39-1 1.73V7h1a7 7 0 017 7h1a1 1 0 011 1v3a1 1 0 01-1 1h-1v1a2 2 0 01-2 2H5a2 2 0 01-2-2v-1H2a1 1 0 01-1-1v-3a1 1 0 011-1h1a7 7 0 017-7h1V5.73c-.6-.34-1-.99-1-1.73a2 2 0 012-2z"/>
                </svg>
                AI 总结
              </h3>
              <div class="space-y-2 text-sm text-text-primary">
                <p><span class="font-medium text-accent">研究问题：</span>如何在持续任务序列中避免灾难性遗忘并高效迁移知识</p>
                <p><span class="font-medium text-accent">研究方法：</span>任务感知多专家池+共享稠密层+回放缓冲+注意力加权重用</p>
                <p><span class="font-medium text-accent">主要发现：</span>TAME在CIFAR-100二分类序列上同时提升新任务准确率并保持旧任务性能</p>
                <p><span class="font-medium text-accent">创新点：</span>引入任务相似度驱动的专家选择与注意力回放机制，实现自适应知识整合</p>
                
                <p><span class="font-medium text-accent">相关性：</span>为终身深度学习提供可扩展架构，平衡适应与保留，对持续学习研究具直接借鉴意义</p>
                
              </div>

              <div class="mt-3 pt-3 border-t border-accent/30">
                <button id="btn-detail-30" onclick="toggleSection('detail-30')"
                        class="text-xs text-accent hover:text-accent-hover font-medium flex items-center">
                  <svg class="w-3 h-3 mr-1" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                    <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
                  </svg>
                  查看详细分析
                </button>
                <div id="detail-30" class="section-expand collapsed mt-3">
                  <div class="bg-bg-card rounded-lg p-4 text-sm text-text-primary space-y-3 border border-border-color">
                    <p><span class="font-medium text-text-primary">研究背景：</span>持续学习（lifelong/continual learning）要求深度网络在顺序到来的任务上不断适应新知识，同时避免灾难性遗忘。现有方法往往把网络参数视为单一整体，难以兼顾“学得新”与“不忘旧”的矛盾。作者观察到不同任务间存在可度量的相似性，提出用“多专家+任务感知”思路将知识模块化，从而更灵活地实现知识保留与迁移。</p>
                    <p><span class="font-medium text-text-primary">方法详情：</span>TAME维护一个预训练专家池，每个专家对应历史任务子集；当新任务到达时，先用任务相似度度量挑选最相关的一个或少数专家作为主干。固定专家特征后，仅训练一个共享的稠密分类层，把选中专家的特征整合成最终预测，显著降低可训练参数量。为抑制遗忘，系统保留一个回放缓冲区，存储先前任务的典型样本及其嵌入；训练时联合优化新任务损失与回放样本损失。进一步引入轻量级注意力模块，让模型在每次预测时动态加权回放嵌入，使最相关的历史信息对当前决策贡献最大。</p>
                    <p><span class="font-medium text-text-primary">研究结果：</span>在由CIFAR-100派生的20组二分类任务序列上，TAME相比EWC、MAS、DER++等强基线平均提升新任务准确率2.1–4.3个百分点，同时旧任务遗忘率降低18–25%。消融实验显示，专家选择策略与注意力回放各自贡献约40%和35%的性能增益。可视化表明，注意力权重确实聚焦于语义相近的旧任务嵌入，验证了“任务感知”机制的有效性。</p>
                    <p><span class="font-medium text-text-primary">局限性：</span>论文仅在CIFAR-100二分类协议上评估，尚未验证在更复杂、更高分辨率或增量类别/检测/分割任务上的泛化能力。专家池大小随任务数线性增长，存储与推理开销可能在大规模场景下成为瓶颈；相似度度量依赖手工设计的特征，可能无法捕捉深层语义相似性。</p>
                    
                    <p><span class="font-medium text-text-primary">未来方向：</span>后续可探索基于子网络掩码或超网络生成的“虚拟专家”，以固定参数预算支持无限任务扩展；同时引入可学习的任务相似度度量，实现端到端优化。</p>
                    
                    <p class="text-accent"><span class="font-medium">研究相关性：</span>若研究者关注持续学习、灾难性遗忘、模块化神经网络或多任务知识迁移，TAME提供了一种可扩展的“专家池+任务感知”范式，其代码与实验协议可直接作为基准或对比方法。</p>
                  </div>
                </div>
              </div>
            </div>
            

            <div class="pt-4 border-t border-border-color">
              <div class="flex flex-wrap gap-3 text-xs">
                <span class="px-2 py-1 bg-bg-hover rounded text-text-secondary">相似度 0.80</span>
                <span class="px-2 py-1 bg-bg-hover rounded text-text-secondary">
                  期刊影响力 0.60
                  
                </span>
              </div>
            </div>
          </div>
        </article>
        
      </div>
    </div>
  </main>

  <footer class="bg-bg-card border-t border-border-color mt-12">
    <div class="content-container py-6 text-center text-sm text-text-secondary">
      灵感来自 <a href="https://github.com/Yorks0n/ZotWatch" class="text-accent hover:text-accent-hover transition-colors">ZotWatch</a>
    </div>
  </footer>

  <script>
    function toggleSection(id) {
      const el = document.getElementById(id);
      const btn = document.getElementById('btn-' + id);
      const icon = document.getElementById('icon-' + id);
      const hint = document.getElementById('hint-' + id);
      if (el.classList.contains('collapsed')) {
        el.classList.remove('collapsed');
        el.classList.add('expanded');
        if (btn && btn.textContent.includes('详细分析')) {
          btn.innerHTML = '<svg class="w-3 h-3 mr-1" fill="none" stroke="currentColor" viewBox="0 0 24 24"><path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M5 15l7-7 7 7"/></svg>收起详细分析';
        }
        if (hint) hint.textContent = '点击收起';
        if (icon) icon.style.transform = 'rotate(180deg)';
        if (id === 'researcher-profile') {
          setTimeout(() => { window.dispatchEvent(new Event('resize')); }, 100);
        }
      } else {
        el.classList.remove('expanded');
        el.classList.add('collapsed');
        if (btn && btn.textContent.includes('详细分析')) {
          btn.innerHTML = '<svg class="w-3 h-3 mr-1" fill="none" stroke="currentColor" viewBox="0 0 24 24"><path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/></svg>查看详细分析';
        }
        if (hint) hint.textContent = '点击展开';
        if (icon) icon.style.transform = 'rotate(0deg)';
      }
    }
  </script>
</body>
</html>