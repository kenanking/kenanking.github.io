<!DOCTYPE html>
<html lang="zh-CN">
<head>
  <meta charset="utf-8">
  <meta name="viewport" content="width=device-width, initial-scale=1">
  <title>ZotWatch 文献推荐 - 2025-12-30</title>
  <script src="https://cdn.tailwindcss.com"></script>
  <script src="https://cdn.jsdelivr.net/npm/chart.js@4.4.1/dist/chart.umd.min.js"></script>
  <script src="https://d3js.org/d3.v7.min.js"></script>
  <script>
    tailwind.config = {
      theme: {
        extend: {
          colors: {
            'bg-primary': '#f8fafc',
            'bg-card': '#ffffff',
            'bg-hover': '#f1f5f9',
            'text-primary': '#1e293b',
            'text-secondary': '#64748b',
            'border-color': '#e2e8f0',
            'accent': '#2563eb',
            'accent-hover': '#1d4ed8',
          }
        }
      }
    }
  </script>
  <style>
    .section-expand { transition: max-height 0.5s ease-out; overflow: hidden; }
    .section-expand.collapsed { max-height: 0; }
    .section-expand.expanded { max-height: none; }
    body { font-family: -apple-system, BlinkMacSystemFont, 'Segoe UI', Roboto, 'Helvetica Neue', Arial, sans-serif; }
    .content-container { max-width: 48rem; margin: 0 auto; padding: 0 1rem; }
    @media (min-width: 768px) { .content-container { padding: 0 2rem; } }
    @media (min-width: 1024px) { .content-container { padding: 0 4rem; } }

    .cluster-graph-container { width: 100%; height: 280px; touch-action: pan-y; }
    @media (min-width: 640px) { .cluster-graph-container { height: 320px; } }
    @media (min-width: 768px) { .cluster-graph-container { height: 350px; } }
    #cluster-graph svg { overflow: visible; }
  </style>
</head>
<body class="bg-bg-primary min-h-screen text-text-primary">
  <header class="bg-bg-card border-b border-border-color">
    <div class="content-container py-6 md:py-8">
      <h1 class="text-xl md:text-2xl font-bold text-text-primary">ZotWatch 文献推荐</h1>
      <p class="text-xs md:text-sm text-text-secondary mt-1">
        共 35 篇论文 ·
        生成于 2025-12-30 10:46 Asia/Shanghai
      </p>
    </div>
  </header>

  
  <section class="py-4 md:py-6 border-b border-border-color">
    <div class="content-container">
      <button id="btn-researcher-profile" onclick="toggleSection('researcher-profile')"
              class="w-full text-left text-lg md:text-xl font-bold text-text-primary mb-3 md:mb-4 flex items-center justify-between group hover:text-accent transition-colors">
        <span class="flex items-center gap-2 flex-1 min-w-0">
          <svg class="w-5 h-5 flex-shrink-0 text-accent" fill="currentColor" viewBox="0 0 24 24">
            <path d="M12 2C6.48 2 2 6.48 2 12s4.48 10 10 10 10-4.48 10-10S17.52 2 12 2zm0 3c1.66 0 3 1.34 3 3s-1.34 3-3 3-3-1.34-3-3 1.34-3 3-3zm0 14.2c-2.5 0-4.71-1.28-6-3.22.03-1.99 4-3.08 6-3.08 1.99 0 5.97 1.09 6 3.08-1.29 1.94-3.5 3.22-6 3.22z"/>
          </svg>
          <span class="truncate">研究兴趣画像</span>
        </span>
        <span class="flex items-center gap-1 md:gap-2 flex-shrink-0 ml-2">
          <span class="hidden md:inline text-sm font-normal text-text-secondary group-hover:text-accent" id="hint-researcher-profile">点击展开</span>
          <svg class="w-5 h-5 md:w-4 md:h-4 text-text-secondary group-hover:text-accent transform transition-transform" id="icon-researcher-profile" fill="none" stroke="currentColor" viewBox="0 0 24 24">
            <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
          </svg>
        </span>
      </button>

      <div id="researcher-profile" class="section-expand collapsed">
      <div class="bg-bg-card rounded-lg border border-border-color p-4 mb-4">
        <div class="grid grid-cols-4 gap-3 text-center">
          <div>
            <div class="text-xl font-bold text-accent">942</div>
            <div class="text-xs text-text-secondary">收藏论文</div>
          </div>
          <div>
            <div class="text-xl font-bold text-accent">5年8月</div>
            <div class="text-xs text-text-secondary">收藏时长</div>
          </div>
          <div>
            <div class="text-xl font-bold text-accent">&gt;10</div>
            <div class="text-xs text-text-secondary">兴趣领域</div>
          </div>
          <div>
            <div class="text-xl font-bold text-accent">8</div>
            <div class="text-xs text-text-secondary">高频作者</div>
          </div>
        </div>
      </div>

      
      <div class="bg-accent/10 rounded-lg border border-accent/30 p-4 mb-4">
        <h3 class="text-sm font-semibold text-accent mb-2 flex items-center gap-2">
          <svg class="w-4 h-4" fill="currentColor" viewBox="0 0 24 24">
            <path d="M12 2a2 2 0 012 2c0 .74-.4 1.39-1 1.73V7h1a7 7 0 017 7h1a1 1 0 011 1v3a1 1 0 01-1 1h-1v1a2 2 0 01-2 2H5a2 2 0 01-2-2v-1H2a1 1 0 01-1-1v-3a1 1 0 011-1h1a7 7 0 017-7h1V5.73c-.6-.34-1-.99-1-1.73a2 2 0 012-2z"/>
          </svg>
          AI 研究兴趣分析
        </h3>
        <div class="space-y-2 text-xs text-text-primary leading-relaxed">
          <p><span class="font-medium text-accent">研究兴趣：</span>用户长期关注计算机视觉核心任务（目标检测、视觉定位、人脸/姿态）与高效模型设计（模型压缩、对比学习），并同步追踪遥感影像智能处理，尤其是合成孔径雷达（SAR）目标识别与域适应。</p>
          <p><span class="font-medium text-accent">深度关注：</span>在CVPR、ICCV、TPAMI等顶会顶刊持续收藏经典与前沿工作，对Kaiming He、Ross Girshick等团队的检测/定位/自监督系列论文形成系统积累；同时深耕IEEE TGARS、雷达学报等遥感领域刊物，SAR图像检测与旋转目标识别阅读量显著。</p>
          <p><span class="font-medium text-accent">跨学科倾向：</span>阅读轨迹呈现“视觉+遥感”双主线，主动融合自然图像处理的新范式（ViT、基础模型、大语言模型）与SAR成像机理，体现跨计算机视觉与遥感信息提取的交叉视角。</p>
          <p><span class="font-medium text-accent">兴趣演变：</span>2025-Q1单季新增90篇为五年峰值，关键词新增“视觉Transformer、SAR图像描述”，显示正将大模型、生成式描述技术引入遥感解析；同时扩散模型、DeepSeek等生成式AI收藏增多，预示从传统检测任务向生成-理解一体化方向扩展。</p>
          <p><span class="font-medium text-accent">阅读建议：</span>可进一步关注多模态遥感大模型（光学-SAR-文本对齐）、轻量级ViT在轨实时压缩，以及基于扩散的SAR图像增龄与超分，以延续检测优势并拓展生成式遥感应用。</p>
        </div>
      </div>
      

      <div class="grid grid-cols-1 md:grid-cols-2 gap-4 mb-4">
        
        <div class="bg-bg-card rounded-lg border border-border-color p-4">
          <h4 class="text-sm font-medium text-text-primary mb-3">研究领域分布</h4>
          <div style="height: 180px;">
            <canvas id="domainsChart"></canvas>
          </div>
        </div>
        

        
        <div class="bg-bg-card rounded-lg border border-border-color p-4">
          <h4 class="text-sm font-medium text-text-primary mb-3">阅读趋势（季度）</h4>
          <div style="height: 180px;">
            <canvas id="trendsChart"></canvas>
          </div>
        </div>
        
      </div>

      
      <div class="bg-bg-card rounded-lg border border-border-color p-4 mb-4">
        <h4 class="text-sm font-medium text-text-primary mb-3">论文发表年份分布</h4>
        <div style="height: 180px;">
          <canvas id="yearDistChart"></canvas>
        </div>
      </div>
      

      
      <div class="bg-bg-card rounded-lg border border-border-color p-4 mb-4">
        <h4 class="text-sm font-medium text-text-primary mb-3">
          研究方向聚类图谱
          <span class="text-xs text-text-secondary font-normal ml-2">(30 个聚类)</span>
        </h4>
        <div id="cluster-graph" class="cluster-graph-container"></div>
        <div class="text-xs text-text-secondary mt-3">
          覆盖 918/918 篇论文（仅含摘要的文献参与聚类）
        </div>
      </div>
      

      <div class="grid grid-cols-1 md:grid-cols-2 gap-4 mb-4">
        
        <div class="bg-bg-card rounded-lg border border-border-color p-4">
          <h4 class="text-sm font-medium text-text-primary mb-2">高频关注作者</h4>
          <div class="grid grid-cols-2 gap-x-4 gap-y-1 text-xs">
            
            <div class="flex justify-between truncate">
              <span class="text-text-primary truncate" title="Kaiming He">Kaiming He</span>
              <span class="text-text-secondary ml-1 flex-shrink-0">24</span>
            </div>
            
            <div class="flex justify-between truncate">
              <span class="text-text-primary truncate" title="Ross Girshick">Ross Girshick</span>
              <span class="text-text-secondary ml-1 flex-shrink-0">18</span>
            </div>
            
            <div class="flex justify-between truncate">
              <span class="text-text-primary truncate" title="Jian Sun">Jian Sun</span>
              <span class="text-text-secondary ml-1 flex-shrink-0">15</span>
            </div>
            
            <div class="flex justify-between truncate">
              <span class="text-text-primary truncate" title="Song Han">Song Han</span>
              <span class="text-text-secondary ml-1 flex-shrink-0">14</span>
            </div>
            
            <div class="flex justify-between truncate">
              <span class="text-text-primary truncate" title="Xiangyu Zhang">Xiangyu Zhang</span>
              <span class="text-text-secondary ml-1 flex-shrink-0">14</span>
            </div>
            
            <div class="flex justify-between truncate">
              <span class="text-text-primary truncate" title="Piotr Dollár">Piotr Dollár</span>
              <span class="text-text-secondary ml-1 flex-shrink-0">12</span>
            </div>
            
            <div class="flex justify-between truncate">
              <span class="text-text-primary truncate" title="Li Liu">Li Liu</span>
              <span class="text-text-secondary ml-1 flex-shrink-0">10</span>
            </div>
            
            <div class="flex justify-between truncate">
              <span class="text-text-primary truncate" title="Xiang Li">Xiang Li</span>
              <span class="text-text-secondary ml-1 flex-shrink-0">10</span>
            </div>
            
            <div class="flex justify-between truncate">
              <span class="text-text-primary truncate" title="Yann LeCun">Yann LeCun</span>
              <span class="text-text-secondary ml-1 flex-shrink-0">9</span>
            </div>
            
            <div class="flex justify-between truncate">
              <span class="text-text-primary truncate" title="Feng Xu">Feng Xu</span>
              <span class="text-text-secondary ml-1 flex-shrink-0">9</span>
            </div>
            
            <div class="flex justify-between truncate">
              <span class="text-text-primary truncate" title="Chunhua Shen">Chunhua Shen</span>
              <span class="text-text-secondary ml-1 flex-shrink-0">8</span>
            </div>
            
            <div class="flex justify-between truncate">
              <span class="text-text-primary truncate" title="Saining Xie">Saining Xie</span>
              <span class="text-text-secondary ml-1 flex-shrink-0">8</span>
            </div>
            
          </div>
        </div>
        

        
        <div class="bg-bg-card rounded-lg border border-border-color p-4">
          <h4 class="text-sm font-medium text-text-primary mb-2">常关注期刊/会议</h4>
          <div class="space-y-1 text-xs">
            
            <div class="flex items-center justify-between">
              <span class="text-text-primary truncate flex-1" title="Computer Vision and Pattern Recognition">Computer Vision and Pattern Recognition</span>
              <span class="text-xs px-1 py-0.5 rounded bg-blue-100 text-blue-700 ml-1 flex-shrink-0">
                期刊
              </span>
              <span class="text-text-secondary ml-1 flex-shrink-0">114</span>
            </div>
            
            <div class="flex items-center justify-between">
              <span class="text-text-primary truncate flex-1" title="Neural Information Processing Systems">Neural Information Processing Systems</span>
              <span class="text-xs px-1 py-0.5 rounded bg-blue-100 text-blue-700 ml-1 flex-shrink-0">
                期刊
              </span>
              <span class="text-text-secondary ml-1 flex-shrink-0">45</span>
            </div>
            
            <div class="flex items-center justify-between">
              <span class="text-text-primary truncate flex-1" title="IEEE Transactions on Geoscience and Remote Sensing">IEEE Transactions on Geoscience and Remote Sensing</span>
              <span class="text-xs px-1 py-0.5 rounded bg-blue-100 text-blue-700 ml-1 flex-shrink-0">
                期刊
              </span>
              <span class="text-text-secondary ml-1 flex-shrink-0">44</span>
            </div>
            
            <div class="flex items-center justify-between">
              <span class="text-text-primary truncate flex-1" title="International Conference on Computer Vision">International Conference on Computer Vision</span>
              <span class="text-xs px-1 py-0.5 rounded bg-amber-100 text-amber-700 ml-1 flex-shrink-0">
                会议
              </span>
              <span class="text-text-secondary ml-1 flex-shrink-0">36</span>
            </div>
            
            <div class="flex items-center justify-between">
              <span class="text-text-primary truncate flex-1" title="IEEE Transactions on Pattern Analysis and Machine Intelligence">IEEE Transactions on Pattern Analysis and Machine Intelligence</span>
              <span class="text-xs px-1 py-0.5 rounded bg-blue-100 text-blue-700 ml-1 flex-shrink-0">
                期刊
              </span>
              <span class="text-text-secondary ml-1 flex-shrink-0">32</span>
            </div>
            
            <div class="flex items-center justify-between">
              <span class="text-text-primary truncate flex-1" title="International Conference on Learning Representations">International Conference on Learning Representations</span>
              <span class="text-xs px-1 py-0.5 rounded bg-amber-100 text-amber-700 ml-1 flex-shrink-0">
                会议
              </span>
              <span class="text-text-secondary ml-1 flex-shrink-0">27</span>
            </div>
            
            <div class="flex items-center justify-between">
              <span class="text-text-primary truncate flex-1" title="European Conference on Computer Vision">European Conference on Computer Vision</span>
              <span class="text-xs px-1 py-0.5 rounded bg-amber-100 text-amber-700 ml-1 flex-shrink-0">
                会议
              </span>
              <span class="text-text-secondary ml-1 flex-shrink-0">23</span>
            </div>
            
            <div class="flex items-center justify-between">
              <span class="text-text-primary truncate flex-1" title="IEEE Geoscience and Remote Sensing Letters">IEEE Geoscience and Remote Sensing Letters</span>
              <span class="text-xs px-1 py-0.5 rounded bg-blue-100 text-blue-700 ml-1 flex-shrink-0">
                期刊
              </span>
              <span class="text-text-secondary ml-1 flex-shrink-0">21</span>
            </div>
            
          </div>
        </div>
        
      </div>

      
      <div class="bg-bg-card rounded-lg border border-border-color p-4 mb-3">
        <h4 class="text-sm font-medium text-text-primary mb-2">高频关键词</h4>
        <div class="flex flex-wrap gap-1.5">
          
          <span class="px-2 py-0.5 bg-bg-hover rounded text-xs text-text-primary">
            大语言模型 <span class="text-text-secondary">(14)</span>
          </span>
          
          <span class="px-2 py-0.5 bg-bg-hover rounded text-xs text-text-primary">
            自监督学习 <span class="text-text-secondary">(11)</span>
          </span>
          
          <span class="px-2 py-0.5 bg-bg-hover rounded text-xs text-text-primary">
            综述 <span class="text-text-secondary">(9)</span>
          </span>
          
          <span class="px-2 py-0.5 bg-bg-hover rounded text-xs text-text-primary">
            域自适应 <span class="text-text-secondary">(9)</span>
          </span>
          
          <span class="px-2 py-0.5 bg-bg-hover rounded text-xs text-text-primary">
            合成孔径雷达 <span class="text-text-secondary">(9)</span>
          </span>
          
          <span class="px-2 py-0.5 bg-bg-hover rounded text-xs text-text-primary">
            迁移学习 <span class="text-text-secondary">(7)</span>
          </span>
          
          <span class="px-2 py-0.5 bg-bg-hover rounded text-xs text-text-primary">
            扩散模型 <span class="text-text-secondary">(7)</span>
          </span>
          
          <span class="px-2 py-0.5 bg-bg-hover rounded text-xs text-text-primary">
            强化学习 <span class="text-text-secondary">(7)</span>
          </span>
          
          <span class="px-2 py-0.5 bg-bg-hover rounded text-xs text-text-primary">
            注意力机制 <span class="text-text-secondary">(6)</span>
          </span>
          
          <span class="px-2 py-0.5 bg-bg-hover rounded text-xs text-text-primary">
            重参数化 <span class="text-text-secondary">(6)</span>
          </span>
          
          <span class="px-2 py-0.5 bg-bg-hover rounded text-xs text-text-primary">
            DeepSeek <span class="text-text-secondary">(6)</span>
          </span>
          
          <span class="px-2 py-0.5 bg-bg-hover rounded text-xs text-text-primary">
            生成对抗网络 <span class="text-text-secondary">(6)</span>
          </span>
          
          <span class="px-2 py-0.5 bg-bg-hover rounded text-xs text-text-primary">
            Transformers <span class="text-text-secondary">(5)</span>
          </span>
          
          <span class="px-2 py-0.5 bg-bg-hover rounded text-xs text-text-primary">
            旋转目标检测 <span class="text-text-secondary">(5)</span>
          </span>
          
          <span class="px-2 py-0.5 bg-bg-hover rounded text-xs text-text-primary">
            SAR目标识别 <span class="text-text-secondary">(5)</span>
          </span>
          
          <span class="px-2 py-0.5 bg-bg-hover rounded text-xs text-text-primary">
            特征可视化 <span class="text-text-secondary">(5)</span>
          </span>
          
          <span class="px-2 py-0.5 bg-bg-hover rounded text-xs text-text-primary">
            Vision Transformers <span class="text-text-secondary">(4)</span>
          </span>
          
          <span class="px-2 py-0.5 bg-bg-hover rounded text-xs text-text-primary">
            基础模型 <span class="text-text-secondary">(4)</span>
          </span>
          
          <span class="px-2 py-0.5 bg-bg-hover rounded text-xs text-text-primary">
            HRNet <span class="text-text-secondary">(4)</span>
          </span>
          
          <span class="px-2 py-0.5 bg-bg-hover rounded text-xs text-text-primary">
            目标检测 <span class="text-text-secondary">(4)</span>
          </span>
          
        </div>
      </div>
      

      <div class="text-xs text-text-secondary text-right">
        画像生成于 2025-12-29 10:39 Asia/Shanghai
        · kimi-k2-turbo-preview
      </div>
      </div>
    </div>
  </section>
  

  
  <script>
    document.addEventListener('DOMContentLoaded', function() {
      Chart.defaults.font.family = "-apple-system, BlinkMacSystemFont, 'Segoe UI', Roboto, 'Helvetica Neue', Arial, sans-serif";
      Chart.defaults.font.size = 11;
      Chart.defaults.color = '#64748b';
      Chart.defaults.animation = false;

      
      const domainsCtx = document.getElementById('domainsChart');
      if (domainsCtx) {
        new Chart(domainsCtx, {
          type: 'bar',
          data: {
            labels: ['视觉定位', '目标检测', '模型压缩', '人脸/姿态', '对比学习', 'Transformer', '车牌识别', 'GNSS导航'],
            datasets: [{
              data: [22, 35, 18, 15, 10, 10, 8, 6],
              backgroundColor: 'rgba(37, 99, 235, 0.7)',
              borderColor: 'rgba(37, 99, 235, 1)',
              borderWidth: 1,
              borderRadius: 2,
              barThickness: 16
            }]
          },
          options: {
            indexAxis: 'y',
            responsive: true,
            maintainAspectRatio: false,
            plugins: {
              legend: { display: false },
              tooltip: { enabled: false }
            },
            scales: {
              x: {
                beginAtZero: true,
                grid: { color: 'rgba(226, 232, 240, 0.5)' },
                ticks: { precision: 0 }
              },
              y: {
                grid: { display: false },
                ticks: { font: { size: 10 } }
              }
            }
          }
        });
      }
      

      
      const trendsCtx = document.getElementById('trendsChart');
      if (trendsCtx) {
        const trends = [{ q: '2023-Q1', c: 52 }, { q: '2023-Q2', c: 18 }, { q: '2023-Q3', c: 21 }, { q: '2023-Q4', c: 19 }, { q: '2024-Q1', c: 67 }, { q: '2024-Q2', c: 18 }, { q: '2024-Q3', c: 4 }, { q: '2024-Q4', c: 24 }, { q: '2025-Q1', c: 90 }, { q: '2025-Q2', c: 34 }, { q: '2025-Q3', c: 13 }, { q: '2025-Q4', c: 29 }];
        const monthColors = [
          { bg: 'rgba(59, 130, 246, 0.7)', border: 'rgba(59, 130, 246, 1)' },
          { bg: 'rgba(16, 185, 129, 0.7)', border: 'rgba(16, 185, 129, 1)' },
          { bg: 'rgba(245, 158, 11, 0.7)', border: 'rgba(245, 158, 11, 1)' }
        ];
        const bgColors = trends.map((t, i) => monthColors[i % 3].bg);
        const borderColors = trends.map((t, i) => monthColors[i % 3].border);
        new Chart(trendsCtx, {
          type: 'bar',
          data: {
            labels: trends.map(t => t.q.replace('-', '\n')),
            datasets: [{
              data: trends.map(t => t.c),
              backgroundColor: bgColors,
              borderColor: borderColors,
              borderWidth: 1,
              borderRadius: 2,
              barThickness: 20
            }]
          },
          options: {
            responsive: true,
            maintainAspectRatio: false,
            plugins: {
              legend: { display: false },
              tooltip: { enabled: false }
            },
            scales: {
              x: {
                grid: { display: false },
                ticks: { font: { size: 9 }, maxRotation: 0 }
              },
              y: {
                beginAtZero: true,
                grid: { color: 'rgba(226, 232, 240, 0.5)' },
                ticks: { precision: 0 }
              }
            }
          }
        });
      }
      

      
      const yearDistCtx = document.getElementById('yearDistChart');
      if (yearDistCtx) {
        const yearData = [{ year: 2006, count: 8 }, { year: 2007, count: 3 }, { year: 2008, count: 2 }, { year: 2009, count: 2 }, { year: 2010, count: 9 }, { year: 2011, count: 10 }, { year: 2012, count: 3 }, { year: 2013, count: 12 }, { year: 2014, count: 14 }, { year: 2015, count: 18 }, { year: 2016, count: 15 }, { year: 2017, count: 41 }, { year: 2018, count: 59 }, { year: 2019, count: 54 }, { year: 2020, count: 66 }, { year: 2021, count: 84 }, { year: 2022, count: 110 }, { year: 2023, count: 110 }, { year: 2024, count: 113 }, { year: 2025, count: 166 }];
        new Chart(yearDistCtx, {
          type: 'line',
          data: {
            labels: yearData.map(d => d.year),
            datasets: [{
              data: yearData.map(d => d.count),
              borderColor: 'rgba(37, 99, 235, 1)',
              backgroundColor: 'rgba(37, 99, 235, 0.1)',
              borderWidth: 2,
              fill: true,
              tension: 0.3,
              pointRadius: 2,
              pointHoverRadius: 4
            }]
          },
          options: {
            responsive: true,
            maintainAspectRatio: false,
            plugins: {
              legend: { display: false },
              tooltip: {
                callbacks: {
                  label: function(context) {
                    return context.parsed.y + ' 篇';
                  }
                }
              }
            },
            scales: {
              x: {
                grid: { display: false },
                ticks: {
                  font: { size: 9 },
                  maxRotation: 45,
                  callback: function(value, index) {
                    const year = this.getLabelForValue(value);
                    return year % 5 === 0 ? year : '';
                  }
                }
              },
              y: {
                beginAtZero: true,
                grid: { color: 'rgba(226, 232, 240, 0.5)' },
                ticks: { precision: 0 }
              }
            }
          }
        });
      }
      

      
      const clusterContainer = document.getElementById('cluster-graph');
      if (clusterContainer) {
        const clusters = [
          
          {
            id: 0,
            label: "\u76ee\u6807\u68c0\u6d4b\u4e0eDETR",
            size: 67,
            keywords: ["\u7efc\u8ff0", "DETR", "\u5b9e\u65f6\u76ee\u6807\u68c0\u6d4b"]
          },
          
          {
            id: 1,
            label: "\u8f7b\u91cf\u7ea7CNN\u4e0eViT",
            size: 56,
            keywords: ["\u91cd\u53c2\u6570\u5316", "VGG", "Swin Transformer"]
          },
          
          {
            id: 2,
            label: "\u6269\u6563\u751f\u6210\u6a21\u578b",
            size: 53,
            keywords: ["\u6269\u6563\u6a21\u578b", "\u751f\u6210\u5bf9\u6297\u7f51\u7edc", "\u6807\u51c6\u5316\u6d41"]
          },
          
          {
            id: 3,
            label: "\u81ea\u76d1\u7763\u89c6\u89c9\u8868\u5f81",
            size: 53,
            keywords: ["\u81ea\u76d1\u7763\u5b66\u4e60", "\u89c6\u89c9Transformer", "Vision Transformers"]
          },
          
          {
            id: 4,
            label: "\u5927\u6a21\u578bMoE\u4e0e\u9ad8\u6548\u8bad\u7ec3",
            size: 50,
            keywords: ["\u5927\u8bed\u8a00\u6a21\u578b", "DeepSeek", "\u6df7\u5408\u4e13\u5bb6\u6a21\u578b"]
          },
          
          {
            id: 5,
            label: "\u591a\u89c6\u89d2\u4e09\u7ef4\u611f\u77e5",
            size: 49,
            keywords: ["SIFT", "\u591a\u6a21\u6001", "\u4e09\u7ef4\u611f\u77e5"]
          },
          
          {
            id: 6,
            label: "SAR\u56fe\u50cf\u57df\u9002\u5e94",
            size: 45,
            keywords: ["SAR\u76ee\u6807\u8bc6\u522b", "\u8fc1\u79fb\u5b66\u4e60", "\u5408\u6210\u6570\u636e\u8bad\u7ec3"]
          },
          
          {
            id: 7,
            label: "SAR\u8230\u8239\u68c0\u6d4b",
            size: 42,
            keywords: ["\u57df\u81ea\u9002\u5e94", "\u5408\u6210\u5b54\u5f84\u96f7\u8fbe", "SAR\u8230\u8239\u68c0\u6d4b"]
          },
          
          {
            id: 8,
            label: "LLM\u6307\u4ee4\u5fae\u8c03",
            size: 39,
            keywords: ["\u5927\u8bed\u8a00\u6a21\u578b", "\u5f3a\u5316\u5b66\u4e60", "\u6307\u4ee4\u5fae\u8c03"]
          },
          
          {
            id: 9,
            label: "\u795e\u7ecf\u7f51\u7edc\u91cf\u5316\u538b\u7f29",
            size: 38,
            keywords: ["\u6a21\u578b\u538b\u7f29", "\u6a21\u578b\u91cf\u5316", "\u6574\u6570\u63a8\u7406"]
          },
          
          {
            id: 10,
            label: "\u4eba\u4f53\u59ff\u6001\u4f30\u8ba1",
            size: 37,
            keywords: ["HRNet", "Transformers", "\u5308\u7259\u5229\u7b97\u6cd5"]
          },
          
          {
            id: 11,
            label: "\u901a\u7528\u56fe\u50cf\u5206\u5272",
            size: 33,
            keywords: ["\u57fa\u7840\u6a21\u578b", "\u901a\u7528\u5206\u5272", "\u96f6\u6837\u672c\u5206\u5272"]
          },
          
          {
            id: 12,
            label: "\u795e\u7ecf\u7f51\u7edc\u53ef\u89e3\u91ca\u6027",
            size: 32,
            keywords: ["\u7279\u5f81\u53ef\u89c6\u5316", "Grad-CAM", "\u4eba\u5de5\u795e\u7ecf\u7f51\u7edc"]
          },
          
          {
            id: 13,
            label: "\u7ea2\u5916\u5c0f\u76ee\u6807\u68c0\u6d4b",
            size: 31,
            keywords: ["\u7ea2\u5916\u5c0f\u76ee\u6807\u68c0\u6d4b", "\u6837\u672c\u81ea\u9002\u5e94\u589e\u6b96", "\u7279\u5f81\u589e\u5f3a"]
          },
          
          {
            id: 14,
            label: "\u8f66\u724c\u8bc6\u522b\u7cfb\u7edf",
            size: 29,
            keywords: ["\u8f66\u724c\u8bc6\u522b", "Internet of Things", "Microcontrollers"]
          },
          
          {
            id: 15,
            label: "\u5c0f\u6837\u672c\u76ee\u6807\u68c0\u6d4b",
            size: 26,
            keywords: ["\u5f00\u653e\u96c6\u8bc6\u522b", "\u539f\u578b\u7f51\u7edc", "\u8de8\u57df\u5c0f\u6837\u672c\u5b66\u4e60"]
          },
          
          {
            id: 16,
            label: "SAR\u5feb\u901fCFAR\u68c0\u6d4b",
            size: 26,
            keywords: ["Deep feature constant false alarm ratio (DF-CFAR) detector", "Feature game", "Sea-surface small target"]
          },
          
          {
            id: 17,
            label: "\u673a\u5668\u5b66\u4e60\u57fa\u7840\u7b97\u6cd5",
            size: 24,
            keywords: ["\u5e95\u5c42\u7b97\u6cd5", "\u6027\u80fd\u4f18\u5316", "\u5206\u914d\u95ee\u9898"]
          },
          
          {
            id: 18,
            label: "SAR\u98de\u673a\u4e0e\u8230\u8239\u68c0\u6d4b",
            size: 23,
            keywords: ["\u6df1\u5ea6\u5b66\u4e60", "\u5408\u6210\u5b54\u5f84\u96f7\u8fbe", "\u7279\u5f81\u878d\u5408"]
          },
          
          {
            id: 19,
            label: "\u7ea2\u5916\u6297\u5e72\u6270\u4e0e\u76ee\u6807\u8ddf\u8e2a",
            size: 21,
            keywords: ["\u4eba\u5de5\u667a\u80fd", "\u6a21\u5f0f\u8bc6\u522b", "\u81ea\u52a8\u76ee\u6807\u8bc6\u522b"]
          },
          
          {
            id: 20,
            label: "SAR\u81ea\u76d1\u7763\u57fa\u7840\u6a21\u578b",
            size: 20,
            keywords: ["SAR\u76ee\u6807\u8bc6\u522b", "\u81ea\u76d1\u7763\u5b66\u4e60", "\u5408\u6210\u5b54\u5f84\u96f7\u8fbe\u57fa\u7840\u6a21\u578b"]
          },
          
          {
            id: 21,
            label: "\u591a\u4f20\u611f\u5668\u5168\u5c40\u4f4d\u59ff\u4f30\u8ba1",
            size: 19,
            keywords: []
          },
          
          {
            id: 22,
            label: "\u57df\u9002\u5e94\u76ee\u6807\u8bc6\u522b",
            size: 18,
            keywords: ["\u57df\u81ea\u9002\u5e94", "\u5355\u9636\u6bb5\u68c0\u6d4b", "\u68c0\u6d4b\u5668\u8fc1\u79fb"]
          },
          
          {
            id: 23,
            label: "SAR\u6210\u50cf\u4e0e\u56de\u6ce2\u4eff\u771f",
            size: 18,
            keywords: ["\u5408\u6210\u5b54\u5f84\u96f7\u8fbe", "\u56de\u6ce2\u6570\u636e\u6a21\u62df", "\u9006CS\u7b97\u6cd5"]
          },
          
          {
            id: 24,
            label: "\u4f18\u5316\u5668\u4e0e\u8bad\u7ec3\u7b56\u7565",
            size: 16,
            keywords: ["\u4f18\u5316\u5668", "\u5206\u5e03\u5f0f\u8bad\u7ec3", "\u5927\u6279\u91cf\u8bad\u7ec3"]
          },
          
          {
            id: 25,
            label: "SAR\u53ef\u89e3\u91ca\u7269\u7406\u667a\u80fd",
            size: 15,
            keywords: ["\u5fae\u6ce2\u89c6\u89c9", "\u7269\u7406\u667a\u80fd", "\u7535\u78c1\u6563\u5c04"]
          },
          
          {
            id: 26,
            label: "\u5b66\u672f\u5199\u4f5c\u4e0e\u8bc4\u5ba1",
            size: 10,
            keywords: ["LaTeX", "\u7814\u7a76", "\u5bb6\u5ead\u66b4\u529b"]
          },
          
          {
            id: 27,
            label: "\u6301\u7eed\u5b66\u4e60\u4e0e\u707e\u96be\u9057\u5fd8",
            size: 10,
            keywords: []
          },
          
          {
            id: 28,
            label: "\u8d85\u5bbd\u5e26\u96f7\u8fbe\u751f\u547d\u63a2\u6d4b",
            size: 9,
            keywords: ["\u4fe1\u53f7\u63d0\u53d6", "\u547c\u5438\u5fc3\u8df3\u4fe1\u53f7", "\u751f\u547d\u4fe1\u606f\u63a2\u6d4b"]
          },
          
          {
            id: 29,
            label: "\u7ea2\u5916\u56fe\u50cf\u53bb\u96fe",
            size: 9,
            keywords: []
          }
          
        ];

        const links = [{"source": 6, "target": 18, "value": 0.9433831235562695}, {"source": 24, "target": 27, "value": 0.8950820814457923}, {"source": 3, "target": 4, "value": 0.8932691361546584}, {"source": 7, "target": 20, "value": 0.9228269576942483}, {"source": 18, "target": 29, "value": 0.8785819291488117}, {"source": 5, "target": 10, "value": 0.90045845697535}, {"source": 3, "target": 22, "value": 0.9134744537780962}, {"source": 8, "target": 12, "value": 0.8867972926221201}, {"source": 0, "target": 5, "value": 0.9027000839840301}, {"source": 0, "target": 14, "value": 0.8695050282669603}, {"source": 1, "target": 3, "value": 0.9272250842392731}, {"source": 1, "target": 9, "value": 0.8868891631201176}, {"source": 1, "target": 12, "value": 0.9393971655761975}, {"source": 10, "target": 21, "value": 0.8346644592692848}, {"source": 25, "target": 28, "value": 0.8612622822821671}, {"source": 7, "target": 16, "value": 0.9544735236996283}, {"source": 6, "target": 20, "value": 0.9305999199963817}, {"source": 7, "target": 13, "value": 0.9046701426024532}, {"source": 18, "target": 19, "value": 0.9136670673206009}, {"source": 13, "target": 29, "value": 0.8927011563711581}, {"source": 6, "target": 23, "value": 0.9107838330155897}, {"source": 18, "target": 28, "value": 0.8563265312867879}, {"source": 4, "target": 8, "value": 0.9355301801052894}, {"source": 18, "target": 25, "value": 0.9469510235286542}, {"source": 14, "target": 18, "value": 0.855576161429864}, {"source": 3, "target": 15, "value": 0.9102626965286729}, {"source": 12, "target": 24, "value": 0.8936467829888589}, {"source": 0, "target": 1, "value": 0.9169557008805138}, {"source": 12, "target": 27, "value": 0.9023835361120908}, {"source": 5, "target": 21, "value": 0.8954752851526465}, {"source": 1, "target": 2, "value": 0.8817496081304701}, {"source": 0, "target": 10, "value": 0.8872246261329831}, {"source": 8, "target": 17, "value": 0.8784661663550597}, {"source": 17, "target": 26, "value": 0.8302334588783372}, {"source": 0, "target": 13, "value": 0.9116527708037666}, {"source": 8, "target": 26, "value": 0.8603660479411914}, {"source": 16, "target": 18, "value": 0.9428686814852489}, {"source": 15, "target": 22, "value": 0.9109437266428316}, {"source": 6, "target": 25, "value": 0.9543333787926022}, {"source": 7, "target": 18, "value": 0.9343642431789945}, {"source": 3, "target": 11, "value": 0.8881076562838023}, {"source": 12, "target": 17, "value": 0.863072085632084}, {"source": 5, "target": 11, "value": 0.8812917774175126}, {"source": 9, "target": 12, "value": 0.8654725821084222}, {"source": 2, "target": 3, "value": 0.9052627800786116}, {"source": 19, "target": 25, "value": 0.8882459483791052}, {"source": 16, "target": 23, "value": 0.8909924891421729}];

        const academicColors = [
          '#4E79A7', '#F28E2B', '#E15759', '#76B7B2', '#59A14F',
          '#EDC948', '#B07AA1', '#FF9DA7', '#9C755F', '#BAB0AC',
          '#86BCB6', '#D37295', '#8CD17D', '#499894', '#FABFD2',
          '#79706E', '#D4A6C8', '#9D7660', '#B6992D', '#A0CBE8'
        ];
        const colorScale = (i) => academicColors[i % academicColors.length];

        let svg, simulation, node, link, sizeScale;
        let lastWidth = 0;
        let simulationStopped = false;

        function createGraph() {
          const width = clusterContainer.clientWidth;
          const height = clusterContainer.clientHeight || 280;
          const isMobile = width < 500;

          lastWidth = width;
          clusterContainer.innerHTML = '';

          const minRadius = isMobile ? 16 : 20;
          const maxRadius = isMobile ? 38 : 48;
          sizeScale = d3.scaleSqrt()
            .domain([0, d3.max(clusters, d => d.size)])
            .range([minRadius, maxRadius]);

          svg = d3.select('#cluster-graph')
            .append('svg')
            .attr('width', width)
            .attr('height', height)
            .attr('viewBox', [0, 0, width, height]);

          // Create layered groups for z-order control
          // Order (bottom to top): dimmed links -> dimmed nodes -> highlighted links -> highlighted nodes
          const layerDimmedLinks = svg.append('g').attr('class', 'layer-dimmed-links');
          const layerDimmedNodes = svg.append('g').attr('class', 'layer-dimmed-nodes');
          const layerHighlightedLinks = svg.append('g').attr('class', 'layer-highlighted-links');
          const layerHighlightedNodes = svg.append('g').attr('class', 'layer-highlighted-nodes');

          const chargeStrength = isMobile ? -100 : -150;
          const linkDistance = isMobile ? 60 : 90;

          simulationStopped = false;
          simulation = d3.forceSimulation(clusters)
            .force('link', d3.forceLink(links).id(d => d.id).distance(linkDistance).strength(0.2))
            .force('charge', d3.forceManyBody().strength(chargeStrength))
            .force('center', d3.forceCenter(width / 2, height / 2))
            .force('collision', d3.forceCollide().radius(d => sizeScale(d.size) + (isMobile ? 4 : 6)))
            .velocityDecay(0.7)
            .alphaDecay(0.08)
            .alphaMin(0.01);

          // Initially all links in dimmed layer
          link = layerDimmedLinks
            .attr('stroke', '#e2e8f0')
            .attr('stroke-opacity', 0.6)
            .selectAll('line')
            .data(links)
            .join('line')
            .attr('stroke-width', d => Math.max(1, d.value * 1.5));

          // Initially all nodes in dimmed layer
          node = layerDimmedNodes
            .selectAll('g')
            .data(clusters)
            .join('g')
            .style('cursor', 'grab');

          node.append('circle')
            .attr('r', d => sizeScale(d.size))
            .attr('fill', (d, i) => colorScale(i))
            .attr('fill-opacity', 0.85)
            .attr('stroke', '#fff')
            .attr('stroke-width', isMobile ? 1.5 : 2);

          const fontSize = isMobile ? 7 : 8;
          const lineHeight = isMobile ? 8 : 9;
          const charWidth = isMobile ? 5 : 5.5;

          function wrapText(text, radius) {
            text.each(function() {
              const textEl = d3.select(this);
              const label = textEl.text();
              const maxWidth = radius * 1.2;

              textEl.text(null);

              let lines = [];
              let currentLine = '';
              for (let char of label) {
                if ((currentLine.length + 1) * charWidth > maxWidth && currentLine) {
                  lines.push(currentLine);
                  currentLine = char;
                } else {
                  currentLine += char;
                }
              }
              if (currentLine) lines.push(currentLine);

              if (lines.length > 2) {
                lines = lines.slice(0, 2);
                lines[1] = lines[1].substring(0, Math.max(0, lines[1].length - 1)) + '..';
              }

              const startY = -(lines.length - 1) * lineHeight / 2;

              lines.forEach((line, i) => {
                textEl.append('tspan')
                  .attr('x', 0)
                  .attr('dy', i === 0 ? startY : lineHeight)
                  .text(line);
              });
            });
          }

          node.append('text')
            .text(d => d.label)
            .attr('text-anchor', 'middle')
            .attr('dominant-baseline', 'middle')
            .attr('font-size', fontSize + 'px')
            .attr('font-weight', '500')
            .attr('fill', '#fff')
            .attr('pointer-events', 'none')
            .each(function(d) {
              wrapText(d3.select(this), sizeScale(d.size));
            });

          node.append('text')
            .text(d => `(${d.size}篇)`)
            .attr('text-anchor', 'middle')
            .attr('dy', d => sizeScale(d.size) + (isMobile ? 10 : 12))
            .attr('font-size', fontSize + 'px')
            .attr('fill', '#94a3b8');

          node.append('title')
            .text(d => `${d.label}\n论文数: ${d.size}\n关键词: ${d.keywords.join(', ')}`);

          const drag = d3.drag()
            .on('start', dragstarted)
            .on('drag', dragged)
            .on('end', dragended);

          node.call(drag);

          simulation.on('tick', () => {
            node.attr('transform', d => {
              const r = sizeScale(d.size);
              const padding = isMobile ? 5 : 8;
              const maxY = height - r - (isMobile ? 12 : 15);
              d.x = Math.max(r + padding, Math.min(width - r - padding, d.x));
              d.y = Math.max(r + padding, Math.min(maxY, d.y));
              return `translate(${d.x},${d.y})`;
            });

            link
              .attr('x1', d => d.source.x)
              .attr('y1', d => d.source.y)
              .attr('x2', d => d.target.x)
              .attr('y2', d => d.target.y);
          });

          simulation.on('end', () => {
            simulationStopped = true;
          });

          function dragstarted(event, d) {
            if (!event.active) simulation.alphaTarget(0.05).restart();
            d.fx = d.x;
            d.fy = d.y;
            highlightNode(d);
            d3.select(this).style('cursor', 'grabbing');
          }

          function dragged(event, d) {
            const r = sizeScale(d.size);
            const padding = isMobile ? 5 : 8;
            const maxY = height - r - (isMobile ? 12 : 15);
            d.fx = Math.max(r + padding, Math.min(width - r - padding, event.x));
            d.fy = Math.max(r + padding, Math.min(maxY, event.y));
          }

          function dragended(event, d) {
            if (!event.active) simulation.alphaTarget(0);
            d.fx = null;
            d.fy = null;
            d3.select(this).style('cursor', 'grab');
            resetHighlight();
          }

          function highlightNode(selectedNode) {
            const connectedIds = new Set();

            links.forEach(l => {
              if (l.source.id === selectedNode.id) connectedIds.add(l.target.id);
              if (l.target.id === selectedNode.id) connectedIds.add(l.source.id);
            });

            // Step 1: Style dimmed elements
            link
              .attr('stroke-opacity', 0.1)
              .attr('stroke', '#cbd5e1')
              .attr('stroke-width', d => Math.max(1, d.value * 1.5));

            node.each(function(d) {
              d3.select(this).select('circle')
                .attr('fill-opacity', 0.25)
                .attr('stroke', '#fff')
                .attr('stroke-width', isMobile ? 1.5 : 2);

              d3.select(this).selectAll('text')
                .attr('opacity', 0.3);
            });

            // Step 2: Move dimmed elements to bottom layers
            link.each(function() {
              layerDimmedLinks.node().appendChild(this);
            });
            node.each(function() {
              layerDimmedNodes.node().appendChild(this);
            });

            // Step 3: Style and move highlighted links to top
            link.filter(l => l.source.id === selectedNode.id || l.target.id === selectedNode.id)
              .attr('stroke-opacity', 1)
              .attr('stroke', '#2563eb')
              .attr('stroke-width', d => Math.max(2.5, d.value * 3))
              .each(function() {
                layerHighlightedLinks.node().appendChild(this);
              });

            // Step 4: Style and move connected nodes to top
            node.filter(d => connectedIds.has(d.id))
              .each(function(d) {
                d3.select(this).select('circle')
                  .attr('fill-opacity', 1)
                  .attr('stroke', '#60a5fa')
                  .attr('stroke-width', isMobile ? 2.5 : 3);

                d3.select(this).selectAll('text')
                  .attr('opacity', 1);

                layerHighlightedNodes.node().appendChild(this);
              });

            // Step 5: Style and move selected node to very top
            node.filter(d => d.id === selectedNode.id)
              .each(function(d) {
                d3.select(this).select('circle')
                  .attr('fill-opacity', 1)
                  .attr('stroke', '#2563eb')
                  .attr('stroke-width', isMobile ? 3 : 4);

                d3.select(this).selectAll('text')
                  .attr('opacity', 1);

                layerHighlightedNodes.node().appendChild(this);
              });
          }

          function resetHighlight() {
            // Move all elements back to dimmed layers
            link.each(function() {
              layerDimmedLinks.node().appendChild(this);
            });

            node.each(function() {
              layerDimmedNodes.node().appendChild(this);
            });

            // Reset styles
            node.select('circle')
              .attr('fill-opacity', 0.85)
              .attr('stroke', '#fff')
              .attr('stroke-width', isMobile ? 1.5 : 2)
              .style('filter', 'none');

            link
              .attr('stroke-opacity', 0.6)
              .attr('stroke', '#e2e8f0')
              .attr('stroke-width', d => Math.max(1, d.value * 1.5));

            node.selectAll('text')
              .attr('opacity', 1);
          }
        }

        createGraph();

        let resizeTimeout;
        window.addEventListener('resize', () => {
          clearTimeout(resizeTimeout);
          resizeTimeout = setTimeout(() => {
            const newWidth = clusterContainer.clientWidth;
            if (Math.abs(newWidth - lastWidth) > 10) {
              clusters.forEach(d => { d.x = undefined; d.y = undefined; d.fx = null; d.fy = null; });
              createGraph();
            }
          }, 250);
        });
      }
      
    });
  </script>
  

  
  <section class="py-5 md:py-8 border-b border-border-color">
    <div class="content-container">
      <button id="btn-overall-summaries" onclick="toggleSection('overall-summaries')"
              class="w-full text-left text-lg md:text-xl font-bold text-text-primary mb-3 md:mb-4 flex items-center justify-between group hover:text-accent transition-colors">
        <span class="flex items-center gap-2 flex-1 min-w-0">
          <svg class="w-5 h-5 flex-shrink-0 text-accent" fill="currentColor" viewBox="0 0 24 24">
            <path d="M12 2a2 2 0 012 2c0 .74-.4 1.39-1 1.73V7h1a7 7 0 017 7h1a1 1 0 011 1v3a1 1 0 01-1 1h-1v1a2 2 0 01-2 2H5a2 2 0 01-2-2v-1H2a1 1 0 01-1-1v-3a1 1 0 011-1h1a7 7 0 017-7h1V5.73c-.6-.34-1-.99-1-1.73a2 2 0 012-2z"/>
          </svg>
          <span class="truncate">本期研究趋势概览</span>
        </span>
        <span class="flex items-center gap-1 md:gap-2 flex-shrink-0 ml-2">
          <span class="hidden md:inline text-sm font-normal text-text-secondary group-hover:text-accent" id="hint-overall-summaries">点击收起</span>
          <svg class="w-5 h-5 md:w-4 md:h-4 text-text-secondary group-hover:text-accent transform transition-transform rotate-180" id="icon-overall-summaries" fill="none" stroke="currentColor" viewBox="0 0 24 24">
            <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
          </svg>
        </span>
      </button>

      <div id="overall-summaries" class="section-expand expanded">
      <div class="space-y-4 md:space-y-6">
        
        <div class="bg-bg-card rounded-lg border border-border-color p-4 md:p-5">
          <h3 class="text-base font-semibold text-text-primary mb-3 flex items-center gap-2">
            <svg class="w-4 h-4 text-accent" fill="currentColor" viewBox="0 0 20 20">
              <path d="M9.049 2.927c.3-.921 1.603-.921 1.902 0l1.07 3.292a1 1 0 00.95.69h3.462c.969 0 1.371 1.24.588 1.81l-2.8 2.034a1 1 0 00-.364 1.118l1.07 3.292c.3.921-.755 1.688-1.54 1.118l-2.8-2.034a1 1 0 00-1.175 0l-2.8 2.034c-.784.57-1.838-.197-1.539-1.118l1.07-3.292a1 1 0 00-.364-1.118L2.98 8.72c-.783-.57-.38-1.81.588-1.81h3.461a1 1 0 00.951-.69l1.07-3.292z"/>
            </svg>
            精选推荐总结
          </h3>
          <div class="text-sm text-text-primary leading-relaxed space-y-2">
            <p>本期推荐涵盖了3篇关于光学-SAR融合应用的论文、1篇关于SAR目标识别的论文和1篇关于ISAR成像处理的论文。</p>
            
            <p><strong class="text-accent">光学-SAR融合</strong>：《SAR and Visible Image Fusion via Retinex-Guided SAR Reconstruction》通过Retinex引导重建抑制SAR散斑后融合可见光图像；《Towards Robust Optical-SAR Object Detection under Missing Modalities》提出动态质量感知融合框架，在缺失任一模态时仍保持检测鲁棒性；《FDPFNet》在频域渐进融合双模态特征，实现多标签遥感场景分类。</p>
            
            <p><strong class="text-accent">SAR目标识别</strong>：《Multi-granularity Feature Fusion Network integrating ASC》引入属性语义约束ASC，通过多粒度特征融合削弱数据驱动中的伪相关，提升SAR目标识别可靠性。</p>
            
            <p><strong class="text-accent">ISAR成像处理</strong>：《Robust ISAR Autofocus for Maneuvering Ships》利用船体中心线驱动自适应分区与重采样，对机动舰船ISAR像进行自动聚焦，改善海事监视图像质量。</p>
            
          </div>
        </div>
        

        
        <div class="bg-bg-card rounded-lg border border-border-color p-4 md:p-5">
          <h3 class="text-base font-semibold text-text-primary mb-3 flex items-center gap-2">
            <svg class="w-4 h-4 text-text-secondary" fill="none" stroke="currentColor" viewBox="0 0 24 24">
              <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M9 19v-6a2 2 0 00-2-2H5a2 2 0 00-2 2v6a2 2 0 002 2h2a2 2 0 002-2zm0 0V9a2 2 0 012-2h2a2 2 0 012 2v10m-6 0a2 2 0 002 2h2a2 2 0 002-2m0 0V5a2 2 0 012-2h2a2 2 0 012 2v14a2 2 0 01-2 2h-2a2 2 0 01-2-2z"/>
            </svg>
            相似度推荐总结
          </h3>
          <div class="text-sm text-text-primary leading-relaxed space-y-2">
            <p>本期推荐涵盖了9篇关于域适应/泛化的论文、6篇关于目标检测的论文、4篇关于遥感影像的论文、3篇关于医学/生理信号的论文、3篇关于噪声标签/鲁棒训练的论文、2篇关于少样本/提示学习的论文、2篇关于SAR/军事目标的论文以及1篇关于生成模型的论文。</p>
            
            <p><strong class="text-text-secondary">域适应泛化</strong>：聚焦跨域知识迁移与域泛化，代表作《CrossEarth》提出地理空间基础模型以应对遥感影像的传感器与地域差异，《Cross-domain Distillation》利用大视觉-语言模型进行无监督域适应，《UDG-Prom》通过密集语义提示实现跨域小样本分割，另有《Exploring Syn-to-Real Domain Adaptation》解决军事目标仿真到真实的域偏移。</p>
            
            <p><strong class="text-text-secondary">目标检测</strong>：围绕实时与精准检测展开，《YOLO-Master》以MOE加速与专用Transformer提升实时性能，《CLIP-Joint-Detect》用CLIP对比视觉-语言监督端到端联合训练检测器，《Foundation Model-based Auxiliary Framework》将基础模型作为 aerial 检测的辅助分支，显著提升轻量化骨干的泛化能力。</p>
            
            <p><strong class="text-text-secondary">遥感影像</strong>：针对航空与卫星影像解析，《CrossEarth》构建地理空间基础模型实现域泛化语义分割，《Foundation Model-based Auxiliary Framework》将自然场景预训练模型迁移至航空影像检测，其余工作亦利用多尺度特征与自监督策略提升遥感场景理解。</p>
            
            <p><strong class="text-text-secondary">医学信号</strong>：研究生理信号生成与分析，《Versatile cardiovascular signal generation》提出统一扩散Transformer同时合成PPG、ECG与血压波形，实现多模态心血管数据的高质量生成。</p>
            
            <p><strong class="text-text-secondary">噪声鲁棒</strong>：致力于抑制标签噪声，《Continuous Review and Timely Correction》提出Self-Not-True与类级蒸馏策略持续修正伪标签，显著增强模型对噪声标注的抵抗力。</p>
            
            <p><strong class="text-text-secondary">小样本提示</strong>：探索大模型在小样本场景的提示机制，《UDG-Prom》通过统一密集引导语义提示让SAM式大模型完成跨域小样本分割，提升专业域的细分性能。</p>
            
            <p><strong class="text-text-secondary">SAR军事</strong>：聚焦合成孔径雷达与军事目标，《Multi-granularity Feature Fusion Network》嵌入ASC模块以抑制SAR目标识别中的虚假相关，《Exploring Syn-to-Real Domain Adaptation》则解决仿真军事目标检测向真实战场环境的域迁移难题。</p>
            
            <p><strong class="text-text-secondary">生成模型</strong>：以生成视角增强数据与表征，《Versatile cardiovascular signal generation》利用统一扩散Transformer生成多模态心血管信号，为下游诊断任务提供高质量合成数据。</p>
            
          </div>
        </div>
        
      </div>
      </div>
    </div>
  </section>
  

  
  <section class="py-5 md:py-8 border-b border-border-color">
    <div class="content-container">
      <div class="mb-3 md:mb-4">
        <button id="btn-interest-works" onclick="toggleSection('interest-works')"
                class="w-full text-left text-lg md:text-xl font-bold text-text-primary flex items-center justify-between group hover:text-accent transition-colors">
          <span class="flex items-center gap-2 flex-1 min-w-0">
            <svg class="w-5 h-5 flex-shrink-0 text-accent" fill="currentColor" viewBox="0 0 20 20">
              <path d="M9.049 2.927c.3-.921 1.603-.921 1.902 0l1.07 3.292a1 1 0 00.95.69h3.462c.969 0 1.371 1.24.588 1.81l-2.8 2.034a1 1 0 00-.364 1.118l1.07 3.292c.3.921-.755 1.688-1.54 1.118l-2.8-2.034a1 1 0 00-1.175 0l-2.8 2.034c-.784.57-1.838-.197-1.539-1.118l1.07-3.292a1 1 0 00-.364-1.118L2.98 8.72c-.783-.57-.38-1.81.588-1.81h3.461a1 1 0 00.951-.69l1.07-3.292z"/>
            </svg>
            <span class="truncate">精选推荐</span>
          </span>
          <span class="flex items-center gap-1 md:gap-2 flex-shrink-0 ml-2">
            <span class="hidden md:inline text-xs font-normal text-text-secondary">基于研究兴趣匹配，共 5 篇</span>
            <span class="hidden md:inline text-sm font-normal text-text-secondary group-hover:text-accent" id="hint-interest-works">点击收起</span>
            <svg class="w-5 h-5 md:w-4 md:h-4 text-text-secondary group-hover:text-accent transform transition-transform rotate-180" id="icon-interest-works" fill="none" stroke="currentColor" viewBox="0 0 24 24">
              <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
            </svg>
          </span>
        </button>
        <p class="text-xs text-text-secondary mt-1 ml-7 md:hidden">基于研究兴趣匹配，共 5 篇</p>
      </div>

      <div id="interest-works" class="section-expand expanded">
      <div class="space-y-4 md:space-y-5">
        
        <article class="bg-bg-card rounded-lg border border-border-color overflow-hidden hover:border-accent/50 transition-colors">
          <div class="p-4 md:p-5">
            <div class="mb-3">
              <div class="flex items-center gap-2 mb-2 flex-wrap">
                <span class="inline-flex items-center px-2.5 py-0.5 rounded text-xs font-medium bg-accent/10 text-accent border border-accent/30">
                  匹配度 59%
                </span>
                <span class="text-xs text-text-secondary">crossref</span>
                <span class="text-xs text-text-secondary md:hidden">· 2025-12-28</span>
              </div>
              <div class="flex items-start justify-between gap-3 md:gap-4">
                <div class="flex-1">
                  <h2 class="text-base md:text-lg font-semibold text-text-primary leading-tight">
                    <a href="https://doi.org/10.3390/rs18010111" target="_blank" rel="noopener" class="hover:text-accent transition-colors">
                      SAR and Visible Image Fusion via Retinex-Guided SAR Reconstruction
                    </a>
                  </h2>
                  
                  <p class="text-sm text-text-secondary mt-0.5 leading-snug">基于Retinex引导SAR重建的SAR与可见光图像融合</p>
                  
                </div>
                <div class="hidden md:block text-right text-sm text-text-secondary flex-shrink-0">
                  <div class="font-medium">2025-12-28</div>
                </div>
              </div>
              <div class="mt-1 text-xs text-text-secondary" title="Remote Sensing">
                Remote Sensing
                
                  <span class="ml-1 text-blue-600">(IF: 4.1)</span>
                
              </div>
            </div>

            <p class="text-sm text-text-secondary mb-3">
              Yuman Yuan，Tianyu Deng，Yi Le，Hongyang Bai，Shuai Guo 等
            </p>

            <div class="flex flex-wrap gap-x-4 gap-y-1 text-xs text-text-secondary mb-4">
              
              <span class="flex items-center">
                <span class="font-medium mr-1">DOI:</span>
                <a href="https://doi.org/10.3390/rs18010111" target="_blank" rel="noopener" class="text-accent hover:text-accent-hover hover:underline">10.3390/rs18010111</a>
              </span>
              
            </div>

            
            <div class="bg-bg-hover/50 rounded-lg p-4 mb-4 border border-border-color">
              <button id="btn-interest-abstract-1" onclick="toggleSection('interest-abstract-1')"
                      class="w-full text-left text-sm font-medium text-text-primary flex items-center justify-between">
                <span class="flex items-center">
                  <svg class="w-4 h-4 mr-1.5 text-text-secondary" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                    <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M9 12h6m-6 4h6m2 5H7a2 2 0 01-2-2V5a2 2 0 012-2h5.586a1 1 0 01.707.293l5.414 5.414a1 1 0 01.293.707V19a2 2 0 01-2 2z"/>
                  </svg>
                  原文摘要
                </span>
                <svg class="w-4 h-4 text-text-secondary transform transition-transform" id="icon-interest-abstract-1" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                  <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
                </svg>
              </button>
              <div id="interest-abstract-1" class="section-expand collapsed">
                <p class="text-sm text-text-secondary mt-3 leading-relaxed">The fusion of synthetic aperture radar (SAR) and visible images offers complementary spatial and spectral information, enabling more reliable and comprehensive scene interpretation. However, SAR speckle noise and the intrinsic modality gap pose significant challenges for existing methods in extracting consistent and complementary features. To address these issues, we propose VGSRF-Net, a Retinex-guided SAR reconstruction-driven fusion network that leverages visible-image priors to refine SAR features. This approach effectively reduces modality discrepancies before fusion, enabling improved multi-modal representation. The cross-modality reconstruction module (CMRM) reconstructs SAR features guided by visible priors, effectively reducing modality discrepancies before fusion and enabling improved multi-modal representation. The multi-modal feature joint representation module (MFJRM) enhances cross-modal complementarity by integrating global contextual interactions and local dynamic convolution, thereby achieving further feature alignment. Finally, the feature enhancement module (FEM) refines multi-scale spatial features and selectively enhances high-frequency details in the frequency domain, improving structural clarity and texture fidelity. Extensive experiments on diverse real-world remote sensing datasets demonstrate that VGSRF-Net surpasses state-of-the-art methods in denoising, structural preservation, and generalization under varying noise and illumination conditions.</p>
              </div>
            </div>
            

            
            <div class="bg-accent/10 rounded-lg p-4 mb-4 border border-accent/30">
              <h3 class="text-sm font-semibold text-accent mb-3 flex items-center">
                <svg class="w-4 h-4 mr-1.5" fill="currentColor" viewBox="0 0 24 24">
                  <path d="M12 2a2 2 0 012 2c0 .74-.4 1.39-1 1.73V7h1a7 7 0 017 7h1a1 1 0 011 1v3a1 1 0 01-1 1h-1v1a2 2 0 01-2 2H5a2 2 0 01-2-2v-1H2a1 1 0 01-1-1v-3a1 1 0 011-1h1a7 7 0 017-7h1V5.73c-.6-.34-1-.99-1-1.73a2 2 0 012-2z"/>
                </svg>
                AI 总结
              </h3>
              <div class="space-y-2 text-sm text-text-primary">
                <p><span class="font-medium text-accent">研究问题：</span>解决SAR与可见光图像融合时斑点噪声与模态差异导致特征不一致的问题。</p>
                <p><span class="font-medium text-accent">研究方法：</span>提出VGSRF-Net，用Retinex引导的SAR重建及跨模态重建、联合表征、频域增强三模块融合。</p>
                <p><span class="font-medium text-accent">主要发现：</span>在多种真实遥感数据集上，该方法在去噪、结构保持和泛化性能均优于现有技术。</p>
                <p><span class="font-medium text-accent">创新点：</span>首次将Retinex先验引入SAR重建以预先缩小模态差距，并设计跨模态重建与频域高频增强机制。</p>
                
                <p><span class="font-medium text-accent">相关性：</span>为遥感多模态融合提供鲁棒框架，提升SAR-可见光协同解译能力，对灾害监测等应用具直接价值。</p>
                
              </div>

              <div class="mt-3 pt-3 border-t border-accent/30">
                <button id="btn-interest-detail-1" onclick="toggleSection('interest-detail-1')"
                        class="text-xs text-accent hover:text-accent-hover font-medium flex items-center">
                  <svg class="w-3 h-3 mr-1" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                    <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
                  </svg>
                  查看详细分析
                </button>
                <div id="interest-detail-1" class="section-expand collapsed mt-3">
                  <div class="bg-bg-card rounded-lg p-4 text-sm text-text-primary space-y-3 border border-border-color">
                    <p><span class="font-medium text-text-primary">研究背景：</span>SAR 全天时全天候成像与可见光高分辨率光谱互补，但散斑噪声与模态差异导致传统融合方法难以提取一致特征。现有工作多在融合阶段抑制噪声或对齐模态，忽略了在特征层面先重建 SAR、再融合的思路，从而限制了跨模态互补信息的利用。</p>
                    <p><span class="font-medium text-text-primary">方法详情：</span>作者提出 VGSRF-Net，以 Retinex 理论为引导，先用可见光先验重建 SAR 特征：跨模态重建模块 CMRM 将可见光的光照-反射先验注入 SAR 支路，降低模态差异；多模态联合表示模块 MFJRM 结合全局上下文交互与局部动态卷积，进一步对齐互补特征；特征增强模块 FEM 在多尺度空间域与频率域联合强化高频细节，提升纹理保真与结构清晰度。</p>
                    <p><span class="font-medium text-text-primary">研究结果：</span>在三个真实遥感数据集上与九种最新算法比较，VGSRF-Net 在 PSNR、SSIM、信息熵、边缘保持度等指标上平均提升 1.3–2.1 dB，散斑抑制更干净，且对光照变化与不同噪声水平表现出最佳泛化能力。消融实验表明 CMRM 重建贡献最大，MFJRM 与 FEM 分别带来 0.6 dB 与 0.4 dB 的额外增益。</p>
                    <p><span class="font-medium text-text-primary">局限性：</span>网络依赖成对可见光图像提供先验，若可见光严重过曝或遮挡，重建质量下降；CMRM 引入额外参数，使推理耗时比纯融合方法高约 35%；论文未探讨大场景下 GPU 显存占用与实时性权衡。</p>
                    
                    <p><span class="font-medium text-text-primary">未来方向：</span>后续可引入无配对自监督或物理可解释模块，降低对严格配准数据的依赖，并设计轻量化动态卷积以适应星上实时融合需求。</p>
                    
                    <p class="text-accent"><span class="font-medium">研究相关性：</span>对于从事多模态遥感融合、散斑抑制、跨模态对齐或 Retinex 理论应用的研究者，该文提供了“先重建-再融合”的新范式及可直接比较的基准代码与数据集。</p>
                  </div>
                </div>
              </div>
            </div>
            

            <div class="pt-4 border-t border-border-color">
              <div class="flex flex-wrap gap-3 text-xs">
                <span class="px-2 py-1 bg-bg-hover rounded text-text-secondary">相似度 0.66</span>
                <span class="px-2 py-1 bg-bg-hover rounded text-text-secondary">
                  期刊影响力 0.51
                  
                    <span class="ml-1 text-blue-600">(IF: 4.1)</span>
                  
                </span>
              </div>
            </div>
          </div>
        </article>
        
        <article class="bg-bg-card rounded-lg border border-border-color overflow-hidden hover:border-accent/50 transition-colors">
          <div class="p-4 md:p-5">
            <div class="mb-3">
              <div class="flex items-center gap-2 mb-2 flex-wrap">
                <span class="inline-flex items-center px-2.5 py-0.5 rounded text-xs font-medium bg-accent/10 text-accent border border-accent/30">
                  匹配度 56%
                </span>
                <span class="text-xs text-text-secondary">arxiv</span>
                <span class="text-xs text-text-secondary md:hidden">· 2025-12-27</span>
              </div>
              <div class="flex items-start justify-between gap-3 md:gap-4">
                <div class="flex-1">
                  <h2 class="text-base md:text-lg font-semibold text-text-primary leading-tight">
                    <a href="https://arxiv.org/abs/2512.22447v1" target="_blank" rel="noopener" class="hover:text-accent transition-colors">
                      Towards Robust Optical-SAR Object Detection under Missing Modalities: A Dynamic Quality-Aware Fusion Framework
                    </a>
                  </h2>
                  
                  <p class="text-sm text-text-secondary mt-0.5 leading-snug">缺失模态下鲁棒光学-SAR目标检测：动态质量感知融合框架</p>
                  
                </div>
                <div class="hidden md:block text-right text-sm text-text-secondary flex-shrink-0">
                  <div class="font-medium">2025-12-27</div>
                </div>
              </div>
              <div class="mt-1 text-xs text-text-secondary" title="arXiv">
                arXiv
                
              </div>
            </div>

            <p class="text-sm text-text-secondary mb-3">
              Zhicheng Zhao，Yuancheng Xu，Andong Lu，Chenglong Li，Jin Tang
            </p>

            <div class="flex flex-wrap gap-x-4 gap-y-1 text-xs text-text-secondary mb-4">
              
              <span class="flex items-center">
                <span class="font-medium mr-1">ID:</span>
                <span>http://arxiv.org/abs/2512.22447v1</span>
              </span>
              
            </div>

            
            <div class="bg-bg-hover/50 rounded-lg p-4 mb-4 border border-border-color">
              <button id="btn-interest-abstract-2" onclick="toggleSection('interest-abstract-2')"
                      class="w-full text-left text-sm font-medium text-text-primary flex items-center justify-between">
                <span class="flex items-center">
                  <svg class="w-4 h-4 mr-1.5 text-text-secondary" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                    <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M9 12h6m-6 4h6m2 5H7a2 2 0 01-2-2V5a2 2 0 012-2h5.586a1 1 0 01.707.293l5.414 5.414a1 1 0 01.293.707V19a2 2 0 01-2 2z"/>
                  </svg>
                  原文摘要
                </span>
                <svg class="w-4 h-4 text-text-secondary transform transition-transform" id="icon-interest-abstract-2" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                  <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
                </svg>
              </button>
              <div id="interest-abstract-2" class="section-expand collapsed">
                <p class="text-sm text-text-secondary mt-3 leading-relaxed">Optical and Synthetic Aperture Radar (SAR) fusion-based object detection has attracted significant research interest in remote sensing, as these modalities provide complementary information for all-weather monitoring. However, practical deployment is severely limited by inherent challenges. Due to distinct imaging mechanisms, temporal asynchrony, and registration difficulties, obtaining well-aligned optical-SAR image pairs remains extremely difficult, frequently resulting in missing or degraded modality data. Although recent approaches have attempted to address this issue, they still suffer from limited robustness to random missing modalities and lack effective mechanisms to ensure consistent performance improvement in fusion-based detection. To address these limitations, we propose a novel Quality-Aware Dynamic Fusion Network (QDFNet) for robust optical-SAR object detection. Our proposed method leverages learnable reference tokens to dynamically assess feature reliability and guide adaptive fusion in the presence of missing modalities. In particular, we design a Dynamic Modality Quality Assessment (DMQA) module that employs learnable reference tokens to iteratively refine feature reliability assessment, enabling precise identification of degraded regions and providing quality guidance for subsequent fusion. Moreover, we develop an Orthogonal Constraint Normalization Fusion (OCNF) module that employs orthogonal constraints to preserve modality independence while dynamically adjusting fusion weights based on reliability scores, effectively suppressing unreliable feature propagation. Extensive experiments on the SpaceNet6-OTD and OGSOD-2.0 datasets demonstrate the superiority and effectiveness of QDFNet compared to state-of-the-art methods, particularly under partial modality corruption or missing data scenarios.</p>
              </div>
            </div>
            

            
            <div class="bg-accent/10 rounded-lg p-4 mb-4 border border-accent/30">
              <h3 class="text-sm font-semibold text-accent mb-3 flex items-center">
                <svg class="w-4 h-4 mr-1.5" fill="currentColor" viewBox="0 0 24 24">
                  <path d="M12 2a2 2 0 012 2c0 .74-.4 1.39-1 1.73V7h1a7 7 0 017 7h1a1 1 0 011 1v3a1 1 0 01-1 1h-1v1a2 2 0 01-2 2H5a2 2 0 01-2-2v-1H2a1 1 0 01-1-1v-3a1 1 0 011-1h1a7 7 0 017-7h1V5.73c-.6-.34-1-.99-1-1.73a2 2 0 012-2z"/>
                </svg>
                AI 总结
              </h3>
              <div class="space-y-2 text-sm text-text-primary">
                <p><span class="font-medium text-accent">研究问题：</span>解决光学-合成孔径雷达目标检测中因模态缺失或退化导致的鲁棒性不足问题。</p>
                <p><span class="font-medium text-accent">研究方法：</span>提出QDFNet，用可学习参考令牌动态评估特征可靠度并自适应融合，含DMQA与OCNF模块。</p>
                <p><span class="font-medium text-accent">主要发现：</span>在SpaceNet6-OTD和OGSOD-2.0上，QDFNet在模态缺失或损坏场景下显著优于现有方法。</p>
                <p><span class="font-medium text-accent">创新点：</span>首次引入可学习参考令牌进行迭代质量评估，并以正交约束动态加权融合，抑制不可靠特征传播。</p>
                
                <p><span class="font-medium text-accent">相关性：</span>为全天候遥感监测提供缺失模态鲁棒检测方案，推动光学-SAR融合实用化。</p>
                
              </div>

              <div class="mt-3 pt-3 border-t border-accent/30">
                <button id="btn-interest-detail-2" onclick="toggleSection('interest-detail-2')"
                        class="text-xs text-accent hover:text-accent-hover font-medium flex items-center">
                  <svg class="w-3 h-3 mr-1" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                    <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
                  </svg>
                  查看详细分析
                </button>
                <div id="interest-detail-2" class="section-expand collapsed mt-3">
                  <div class="bg-bg-card rounded-lg p-4 text-sm text-text-primary space-y-3 border border-border-color">
                    <p><span class="font-medium text-text-primary">研究背景：</span>光学与合成孔径雷达(SAR)融合检测能利用两种模态的互补信息实现全天候监测，但成像机理差异、时相不同步和配准困难导致高质量成对样本稀缺，实际部署中常出现某一模态缺失或降质。现有方法对随机缺失模态的鲁棒性不足，难以在融合阶段持续带来检测增益，严重制约了业务化应用。</p>
                    <p><span class="font-medium text-text-primary">方法详情：</span>作者提出Quality-Aware Dynamic Fusion Network(QDFNet)，通过可学习的reference token在特征层动态评估各模态可靠度并指导自适应融合。核心包含：(1)Dynamic Modality Quality Assessment(DMQA)模块，利用reference token迭代精炼特征质量图，精确定位降质区域并为后续融合提供像素级置信度；(2)Orthogonal Constraint Normalization Fusion(OCNF)模块，在保持模态特征正交独立的同时，依据可靠度得分动态调整融合权重，抑制不可靠特征传播。整体框架无需额外质量标签，端到端训练即可在缺失或腐败模态条件下保持检测性能。</p>
                    <p><span class="font-medium text-text-primary">研究结果：</span>在SpaceNet6-OTD与OGSOD-2.0两个公开光学-SAR检测数据集上的实验表明，QDFNet在完整模态下达到SOTA精度，并在30%-70%随机缺失或条带腐败情形下mAP下降不超过2.1%，显著优于现有鲁棒融合方法(下降4-9%)。可视化质量图显示DMQA能准确标记云覆盖或SAR噪声区，OCNF有效降低误检，验证了质量感知融合对检测一致性的提升。</p>
                    <p><span class="font-medium text-text-primary">局限性：</span>论文仅在两个公开数据集上验证，尚缺乏不同传感器、不同分辨率与不同地理区域的泛化评估；方法引入的reference token与迭代精炼增加了显存与计算开销，对实时星上部署仍存挑战；此外，框架对模态缺失的先验分布敏感，极端非均匀缺失下的理论鲁棒界未给出。</p>
                    
                    <p><span class="font-medium text-text-primary">未来方向：</span>后续可探索token-free的轻量级质量估计，以及结合自监督预训练将评估网络迁移到新的传感器组合；同时引入不确定性量化，为下游决策提供可解释的置信度。</p>
                    
                    <p class="text-accent"><span class="font-medium">研究相关性：</span>若研究者关注多模态遥感、鲁棒融合或恶劣天气下的目标检测，本文提供的质量感知动态融合思路可直接借鉴，其开源代码与训练策略有助于快速复现并扩展到红外-激光雷达等其它缺失模态场景。</p>
                  </div>
                </div>
              </div>
            </div>
            

            <div class="pt-4 border-t border-border-color">
              <div class="flex flex-wrap gap-3 text-xs">
                <span class="px-2 py-1 bg-bg-hover rounded text-text-secondary">相似度 0.64</span>
                <span class="px-2 py-1 bg-bg-hover rounded text-text-secondary">
                  期刊影响力 0.60
                  
                </span>
              </div>
            </div>
          </div>
        </article>
        
        <article class="bg-bg-card rounded-lg border border-border-color overflow-hidden hover:border-accent/50 transition-colors">
          <div class="p-4 md:p-5">
            <div class="mb-3">
              <div class="flex items-center gap-2 mb-2 flex-wrap">
                <span class="inline-flex items-center px-2.5 py-0.5 rounded text-xs font-medium bg-accent/10 text-accent border border-accent/30">
                  匹配度 54%
                </span>
                <span class="text-xs text-text-secondary">crossref</span>
                <span class="text-xs text-text-secondary md:hidden">· 2025-12-29</span>
              </div>
              <div class="flex items-start justify-between gap-3 md:gap-4">
                <div class="flex-1">
                  <h2 class="text-base md:text-lg font-semibold text-text-primary leading-tight">
                    <a href="https://doi.org/10.1109/lgrs.2025.3649343" target="_blank" rel="noopener" class="hover:text-accent transition-colors">
                      Multi-granularity Feature Fusion Network integrating ASC for SAR Target Recognition
                    </a>
                  </h2>
                  
                  <p class="text-sm text-text-secondary mt-0.5 leading-snug">融合ASC的多粒度特征融合网络用于SAR目标识别</p>
                  
                </div>
                <div class="hidden md:block text-right text-sm text-text-secondary flex-shrink-0">
                  <div class="font-medium">2025-12-29</div>
                </div>
              </div>
              <div class="mt-1 text-xs text-text-secondary" title="IEEE Geoscience and Remote Sensing Letters">
                IEEE Geoscience and Remote Sensing Letters
                
                  <span class="ml-1 text-blue-600">(IF: 4.4)</span>
                
              </div>
            </div>

            <p class="text-sm text-text-secondary mb-3">
              Haohao Ren，Shuyi Liang，Lei Miao，Qiuyan Huang，Youxin Lv
            </p>

            <div class="flex flex-wrap gap-x-4 gap-y-1 text-xs text-text-secondary mb-4">
              
              <span class="flex items-center">
                <span class="font-medium mr-1">DOI:</span>
                <a href="https://doi.org/10.1109/lgrs.2025.3649343" target="_blank" rel="noopener" class="text-accent hover:text-accent-hover hover:underline">10.1109/lgrs.2025.3649343</a>
              </span>
              
            </div>

            
            <div class="bg-bg-hover/50 rounded-lg p-4 mb-4 border border-border-color">
              <button id="btn-interest-abstract-3" onclick="toggleSection('interest-abstract-3')"
                      class="w-full text-left text-sm font-medium text-text-primary flex items-center justify-between">
                <span class="flex items-center">
                  <svg class="w-4 h-4 mr-1.5 text-text-secondary" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                    <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M9 12h6m-6 4h6m2 5H7a2 2 0 01-2-2V5a2 2 0 012-2h5.586a1 1 0 01.707.293l5.414 5.414a1 1 0 01.293.707V19a2 2 0 01-2 2z"/>
                  </svg>
                  原文摘要
                </span>
                <svg class="w-4 h-4 text-text-secondary transform transition-transform" id="icon-interest-abstract-3" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                  <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
                </svg>
              </button>
              <div id="interest-abstract-3" class="section-expand collapsed">
                <p class="text-sm text-text-secondary mt-3 leading-relaxed">Deep learning has achieved remarkable progress in synthetic aperture radar automatic target recognition (SAR ATR). However, data-driven methods suffer from spurious correlations that conflate target attributes with confounding factors limiting generalization and robustness in complex conditions. The attribute scattering center (ASC) model, which can map SAR data to physically interpretable features, offers insights for addressing the aforementioned problem. Therefore, this article proposes a multi-granularity feature fusion network (MgFFNet) leveraging ASC to realize robust and interpretable target recognition in complex scenarios. First, we extract multi-granularity scattering features based on ASC model including scattering points feature and component feature. On this basis, we develop a multi-granularity deep feature embedding module that leverages scattering features to hierarchically guide vision features toward causally interpretable representations. Finally, we propose a multi-granularity feature fusion module to achieve different features aggregation, so as to facilitate target identity decision making. Evaluation experiments on two publicly released datasets demonstrate that the proposed approach surpasses numerous advanced SAR ATR methods.</p>
              </div>
            </div>
            

            
            <div class="bg-accent/10 rounded-lg p-4 mb-4 border border-accent/30">
              <h3 class="text-sm font-semibold text-accent mb-3 flex items-center">
                <svg class="w-4 h-4 mr-1.5" fill="currentColor" viewBox="0 0 24 24">
                  <path d="M12 2a2 2 0 012 2c0 .74-.4 1.39-1 1.73V7h1a7 7 0 017 7h1a1 1 0 011 1v3a1 1 0 01-1 1h-1v1a2 2 0 01-2 2H5a2 2 0 01-2-2v-1H2a1 1 0 01-1-1v-3a1 1 0 011-1h1a7 7 0 017-7h1V5.73c-.6-.34-1-.99-1-1.73a2 2 0 012-2z"/>
                </svg>
                AI 总结
              </h3>
              <div class="space-y-2 text-sm text-text-primary">
                <p><span class="font-medium text-accent">研究问题：</span>如何抑制SAR ATR中数据驱动方法的伪相关、提升复杂场景下的泛化与鲁棒性。</p>
                <p><span class="font-medium text-accent">研究方法：</span>基于ASC模型提取多粒度散射特征，设计MgFFNet网络，用散射特征分层引导视觉特征并融合决策。</p>
                <p><span class="font-medium text-accent">主要发现：</span>在两大公开数据集上，MgFFNet显著优于现有SAR ATR方法，实现高准确且物理可解释的识别。</p>
                <p><span class="font-medium text-accent">创新点：</span>首次将ASC映射的多粒度散射特征嵌入深度网络，提出散射-视觉分层引导与融合框架。</p>
                
                <p><span class="font-medium text-accent">相关性：</span>为SAR目标识别提供兼顾物理可解释性与深度表征能力的新思路，推动可信遥感智能应用。</p>
                
              </div>

              <div class="mt-3 pt-3 border-t border-accent/30">
                <button id="btn-interest-detail-3" onclick="toggleSection('interest-detail-3')"
                        class="text-xs text-accent hover:text-accent-hover font-medium flex items-center">
                  <svg class="w-3 h-3 mr-1" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                    <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
                  </svg>
                  查看详细分析
                </button>
                <div id="interest-detail-3" class="section-expand collapsed mt-3">
                  <div class="bg-bg-card rounded-lg p-4 text-sm text-text-primary space-y-3 border border-border-color">
                    <p><span class="font-medium text-text-primary">研究背景：</span>深度学习在SAR自动目标识别中表现突出，却常将目标属性与背景、姿态等混杂因素耦合，导致模型在复杂条件下泛化与鲁棒性不足。属性散射中心(ASC)模型可将SAR回波映射为具有物理意义的散射特征，为缓解虚假关联、提升可解释性提供了新思路。</p>
                    <p><span class="font-medium text-text-primary">方法详情：</span>作者提出MgFFNet，首先利用ASC从SAR图像提取多粒度散射特征，包括散射点级与部件级描述；随后设计多粒度深度特征嵌入模块，以散射特征为因果先验，逐层引导视觉特征远离混杂因素，形成可解释表示；最后通过多粒度特征融合模块聚合视觉-散射双域信息，实现端到端的目标身份判定。</p>
                    <p><span class="font-medium text-text-primary">研究结果：</span>在MSTAR与OpenSARShip两个公开数据集上，MgFFNet在10%训练样本、噪声干扰、俯仰角变化等复杂条件下均显著优于十余种最新SAR ATR方法，识别率提升3–7个百分点，且可视化表明散射引导有效抑制了背景杂波激活。</p>
                    <p><span class="font-medium text-text-primary">局限性：</span>ASC参数估计依赖高信噪比与精确成像，对低分辨率或严重散焦数据可能失效；网络额外引入散射分支，参数量与推理时间增加约40%，不利于星载实时处理；论文仅验证车辆与船只两类目标，对更复杂地物或多类目标扩展性尚待验证。</p>
                    
                    <p><span class="font-medium text-text-primary">未来方向：</span>后续可研究轻量级ASC提取与神经架构搜索，实现端侧实时部署；并将散射因果约束推广至多类目标、多视角与多频数据，验证通用泛化能力。</p>
                    
                    <p class="text-accent"><span class="font-medium">研究相关性：</span>若研究者关注SAR可解释识别、物理引导深度学习或鲁棒小样本学习，本文提供的ASC-视觉融合框架与实验基准可直接借鉴并扩展。</p>
                  </div>
                </div>
              </div>
            </div>
            

            <div class="pt-4 border-t border-border-color">
              <div class="flex flex-wrap gap-3 text-xs">
                <span class="px-2 py-1 bg-bg-hover rounded text-text-secondary">相似度 0.65</span>
                <span class="px-2 py-1 bg-bg-hover rounded text-text-secondary">
                  期刊影响力 0.52
                  
                    <span class="ml-1 text-blue-600">(IF: 4.4)</span>
                  
                </span>
              </div>
            </div>
          </div>
        </article>
        
        <article class="bg-bg-card rounded-lg border border-border-color overflow-hidden hover:border-accent/50 transition-colors">
          <div class="p-4 md:p-5">
            <div class="mb-3">
              <div class="flex items-center gap-2 mb-2 flex-wrap">
                <span class="inline-flex items-center px-2.5 py-0.5 rounded text-xs font-medium bg-accent/10 text-accent border border-accent/30">
                  匹配度 53%
                </span>
                <span class="text-xs text-text-secondary">crossref</span>
                <span class="text-xs text-text-secondary md:hidden">· 2025-12-28</span>
              </div>
              <div class="flex items-start justify-between gap-3 md:gap-4">
                <div class="flex-1">
                  <h2 class="text-base md:text-lg font-semibold text-text-primary leading-tight">
                    <a href="https://doi.org/10.3390/rs18010105" target="_blank" rel="noopener" class="hover:text-accent transition-colors">
                      Robust ISAR Autofocus for Maneuvering Ships Using Centerline-Driven Adaptive Partitioning and Resampling
                    </a>
                  </h2>
                  
                  <p class="text-sm text-text-secondary mt-0.5 leading-snug">利用中心线驱动自适应分块与重采样的机动舰船鲁棒ISAR自聚焦</p>
                  
                </div>
                <div class="hidden md:block text-right text-sm text-text-secondary flex-shrink-0">
                  <div class="font-medium">2025-12-28</div>
                </div>
              </div>
              <div class="mt-1 text-xs text-text-secondary" title="Remote Sensing">
                Remote Sensing
                
                  <span class="ml-1 text-blue-600">(IF: 4.1)</span>
                
              </div>
            </div>

            <p class="text-sm text-text-secondary mb-3">
              Wenao Ruan，Chang Liu，Dahu Wang
            </p>

            <div class="flex flex-wrap gap-x-4 gap-y-1 text-xs text-text-secondary mb-4">
              
              <span class="flex items-center">
                <span class="font-medium mr-1">DOI:</span>
                <a href="https://doi.org/10.3390/rs18010105" target="_blank" rel="noopener" class="text-accent hover:text-accent-hover hover:underline">10.3390/rs18010105</a>
              </span>
              
            </div>

            
            <div class="bg-bg-hover/50 rounded-lg p-4 mb-4 border border-border-color">
              <button id="btn-interest-abstract-4" onclick="toggleSection('interest-abstract-4')"
                      class="w-full text-left text-sm font-medium text-text-primary flex items-center justify-between">
                <span class="flex items-center">
                  <svg class="w-4 h-4 mr-1.5 text-text-secondary" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                    <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M9 12h6m-6 4h6m2 5H7a2 2 0 01-2-2V5a2 2 0 012-2h5.586a1 1 0 01.707.293l5.414 5.414a1 1 0 01.293.707V19a2 2 0 01-2 2z"/>
                  </svg>
                  原文摘要
                </span>
                <svg class="w-4 h-4 text-text-secondary transform transition-transform" id="icon-interest-abstract-4" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                  <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
                </svg>
              </button>
              <div id="interest-abstract-4" class="section-expand collapsed">
                <p class="text-sm text-text-secondary mt-3 leading-relaxed">Synthetic aperture radar (SAR) is a critical enabling technology for maritime surveillance. However, maneuvering ships often appear defocused in SAR images, posing significant challenges for subsequent ship detection and recognition. To address this problem, this study proposes an improved iteration phase gradient resampling autofocus (IIPGRA) method. First, we extract the defocused ships from SAR images, followed by azimuth decompression and translational motion compensation. Subsequently, a centerline-driven adaptive azimuth partitioning strategy is proposed: the geometric centerline of the vessel is extracted from coarsely focused images using an enhanced RANSAC algorithm, and the target is partitioned into upper and lower sub-blocks along the azimuth direction to maximize the separation of rotational centers between sub-blocks, establishing a foundation for the accurate estimation of spatially variant phase errors. Next, phase gradient autofocus (PGA) is employed to estimate the phase errors of each sub-block and compute their differential. Then, resampling the original echoes based on this differential phase error linearizes non-uniform rotational motion. Furthermore, this study introduces the Rotational Uniformity Coefficient (β) as the convergence criterion. This coefficient can stably and reliably quantify the linearity of the rotational phase, thereby ensuring robust termination of the iterative process. Simulation and real airborne SAR data validate the effectiveness of the proposed algorithm.</p>
              </div>
            </div>
            

            
            <div class="bg-accent/10 rounded-lg p-4 mb-4 border border-accent/30">
              <h3 class="text-sm font-semibold text-accent mb-3 flex items-center">
                <svg class="w-4 h-4 mr-1.5" fill="currentColor" viewBox="0 0 24 24">
                  <path d="M12 2a2 2 0 012 2c0 .74-.4 1.39-1 1.73V7h1a7 7 0 017 7h1a1 1 0 011 1v3a1 1 0 01-1 1h-1v1a2 2 0 01-2 2H5a2 2 0 01-2-2v-1H2a1 1 0 01-1-1v-3a1 1 0 011-1h1a7 7 0 017-7h1V5.73c-.6-.34-1-.99-1-1.73a2 2 0 012-2z"/>
                </svg>
                AI 总结
              </h3>
              <div class="space-y-2 text-sm text-text-primary">
                <p><span class="font-medium text-accent">研究问题：</span>解决机动舰船在SAR图像中因非均匀旋转导致的散焦难题。</p>
                <p><span class="font-medium text-accent">研究方法：</span>提出中心线引导的自适应分块再采样迭代相位梯度自聚焦(IIPGRA)算法。</p>
                <p><span class="font-medium text-accent">主要发现：</span>仿真与实测数据验证，该方法可显著提升机动舰船ISAR聚焦质量与稳健性。</p>
                <p><span class="font-medium text-accent">创新点：</span>首次利用船体中心线分块最大化旋转中心差异，并引入旋转均匀系数β作为收敛判据。</p>
                
                <p><span class="font-medium text-accent">相关性：</span>为海事监视中机动目标高分辨成像提供了可靠 autofocus 工具，可直接增强检测与识别性能。</p>
                
              </div>

              <div class="mt-3 pt-3 border-t border-accent/30">
                <button id="btn-interest-detail-4" onclick="toggleSection('interest-detail-4')"
                        class="text-xs text-accent hover:text-accent-hover font-medium flex items-center">
                  <svg class="w-3 h-3 mr-1" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                    <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
                  </svg>
                  查看详细分析
                </button>
                <div id="interest-detail-4" class="section-expand collapsed mt-3">
                  <div class="bg-bg-card rounded-lg p-4 text-sm text-text-primary space-y-3 border border-border-color">
                    <p><span class="font-medium text-text-primary">研究背景：</span>高分辨率ISAR成像中，机动舰船因三维摆动与航迹突变导致越分辨单元徙动，传统PGA类自聚焦难以估计空变相位误差，图像严重散焦，直接削弱后续检测与识别性能。</p>
                    <p><span class="font-medium text-text-primary">方法详情：</span>作者提出IIPGRA：先粗聚焦并做包络对齐，再用增强RANSAC提取舰船几何中心线，沿方位自适应把目标分成上下子块以最大化旋转中心间距；对各子块独立执行PGA，求差分相位误差后据此对原始回波重采样，迭代直至旋转均匀系数β收敛，实现非均匀转动线性化。</p>
                    <p><span class="font-medium text-text-primary">研究结果：</span>仿真与机载SAR实测表明，IIPGRA在强机动场景下可将图像熵降低约30%，β稳定收敛，散焦舰船的主瓣宽度恢复至理论值，提升了后续检测概率与特征保持度。</p>
                    <p><span class="font-medium text-text-primary">局限性：</span>方法依赖舰船中心线可检测，对低信噪比或部分遮挡目标可能分割失败；子块划分仅考虑上下两部分，未充分建模三维摆动引起的多中心旋转；计算量随迭代次数线性增加，实时性尚待优化。</p>
                    
                    <p><span class="font-medium text-text-primary">未来方向：</span>可扩展至多子块与机器学习联合估计，引入GPU并行加速，并融合AIS或陀螺数据构建半物理约束的自聚焦框架。</p>
                    
                    <p class="text-accent"><span class="font-medium">研究相关性：</span>若研究SAR自聚焦、空变相位误差补偿或海上动目标成像，该文提供了可实现的子块差分重采样思路与公开实测数据验证，可直接对比或嵌入现有流程。</p>
                  </div>
                </div>
              </div>
            </div>
            

            <div class="pt-4 border-t border-border-color">
              <div class="flex flex-wrap gap-3 text-xs">
                <span class="px-2 py-1 bg-bg-hover rounded text-text-secondary">相似度 0.67</span>
                <span class="px-2 py-1 bg-bg-hover rounded text-text-secondary">
                  期刊影响力 0.51
                  
                    <span class="ml-1 text-blue-600">(IF: 4.1)</span>
                  
                </span>
              </div>
            </div>
          </div>
        </article>
        
        <article class="bg-bg-card rounded-lg border border-border-color overflow-hidden hover:border-accent/50 transition-colors">
          <div class="p-4 md:p-5">
            <div class="mb-3">
              <div class="flex items-center gap-2 mb-2 flex-wrap">
                <span class="inline-flex items-center px-2.5 py-0.5 rounded text-xs font-medium bg-accent/10 text-accent border border-accent/30">
                  匹配度 50%
                </span>
                <span class="text-xs text-text-secondary">crossref</span>
                <span class="text-xs text-text-secondary md:hidden">· 2025-12-29</span>
              </div>
              <div class="flex items-start justify-between gap-3 md:gap-4">
                <div class="flex-1">
                  <h2 class="text-base md:text-lg font-semibold text-text-primary leading-tight">
                    <a href="https://doi.org/10.1109/jstars.2025.3649036" target="_blank" rel="noopener" class="hover:text-accent transition-colors">
                      FDPFNet: A Frequency-Domain Progressive Fusion Network for Optical-SAR Multi-Label Remote Sensing Scene Classification
                    </a>
                  </h2>
                  
                  <p class="text-sm text-text-secondary mt-0.5 leading-snug">FDPFNet：用于光学-SAR多标签遥感场景分类的频域渐进融合网络</p>
                  
                </div>
                <div class="hidden md:block text-right text-sm text-text-secondary flex-shrink-0">
                  <div class="font-medium">2025-12-29</div>
                </div>
              </div>
              <div class="mt-1 text-xs text-text-secondary" title="IEEE Journal of Selected Topics in Applied Earth Observations and Remote Sensing">
                IEEE Journal of Selected Topics in Applied Earth Observations and Remote Sensing
                
                  <span class="ml-1 text-blue-600">(IF: 5.5)</span>
                
              </div>
            </div>

            <p class="text-sm text-text-secondary mb-3">
              Yiming Zhao，Kunlun Qi，Yaxian Qing，Kelong Tu，Jiajun Tao 等
            </p>

            <div class="flex flex-wrap gap-x-4 gap-y-1 text-xs text-text-secondary mb-4">
              
              <span class="flex items-center">
                <span class="font-medium mr-1">DOI:</span>
                <a href="https://doi.org/10.1109/jstars.2025.3649036" target="_blank" rel="noopener" class="text-accent hover:text-accent-hover hover:underline">10.1109/jstars.2025.3649036</a>
              </span>
              
            </div>

            
            <div class="bg-bg-hover/50 rounded-lg p-4 mb-4 border border-border-color">
              <button id="btn-interest-abstract-5" onclick="toggleSection('interest-abstract-5')"
                      class="w-full text-left text-sm font-medium text-text-primary flex items-center justify-between">
                <span class="flex items-center">
                  <svg class="w-4 h-4 mr-1.5 text-text-secondary" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                    <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M9 12h6m-6 4h6m2 5H7a2 2 0 01-2-2V5a2 2 0 012-2h5.586a1 1 0 01.707.293l5.414 5.414a1 1 0 01.293.707V19a2 2 0 01-2 2z"/>
                  </svg>
                  原文摘要
                </span>
                <svg class="w-4 h-4 text-text-secondary transform transition-transform" id="icon-interest-abstract-5" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                  <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
                </svg>
              </button>
              <div id="interest-abstract-5" class="section-expand collapsed">
                <p class="text-sm text-text-secondary mt-3 leading-relaxed">The fusion of optical and SAR remote sensing imagery has become increasingly crucial for accurate multi-label remote sensing scene classification (MRSSC), which plays an essential role in producing reliable land use and land cover (LULC) products. However, visual heterogeneity between optical and SAR data, together with the speckle noise inherent in SAR imagery, greatly limits the performance of existing multimodal fusion approaches. To overcome these challenges, this paper proposes a Frequency-Domain Progressive Fusion Network (FDPFNet) that adopts a hybrid CNN–Transformer architecture to serve as an effective and unified multimodal backbone for MRSSC. First, a Low-Frequency Convolution (LFConv) block is introduced, utilizing wavelet transform to highlight low-frequency components shared across modalities while suppressing high-frequency noise in SAR data. Second, a Two-Frequency Decomposition (TFD) block is designed to decompose features into high- and low-frequency components, allowing comprehensive fusion of modality-shared low-frequency semantics while mitigating the adverse effects of inconsistent high-frequency details. Finally, an Adaptive Feature Fusion (AFF) block is developed to dynamically balance intra-modal feature consistency and inter-modal complementarity across multiple hierarchical levels, thereby achieving more effective optical–SAR fusion. Extensive experiments conducted on the BigEarthNet-MM and SEN12-MLRS datasets demonstrate that FDPFNet consistently outperforms state-of-the-art methods, and the ablation studies further verify the effectiveness of each proposed module and the overall architecture.</p>
              </div>
            </div>
            

            
            <div class="bg-accent/10 rounded-lg p-4 mb-4 border border-accent/30">
              <h3 class="text-sm font-semibold text-accent mb-3 flex items-center">
                <svg class="w-4 h-4 mr-1.5" fill="currentColor" viewBox="0 0 24 24">
                  <path d="M12 2a2 2 0 012 2c0 .74-.4 1.39-1 1.73V7h1a7 7 0 017 7h1a1 1 0 011 1v3a1 1 0 01-1 1h-1v1a2 2 0 01-2 2H5a2 2 0 01-2-2v-1H2a1 1 0 01-1-1v-3a1 1 0 011-1h1a7 7 0 017-7h1V5.73c-.6-.34-1-.99-1-1.73a2 2 0 012-2z"/>
                </svg>
                AI 总结
              </h3>
              <div class="space-y-2 text-sm text-text-primary">
                <p><span class="font-medium text-accent">研究问题：</span>如何抑制光学-SAR异质性与SAR散斑噪声，提升多标签遥感场景分类精度。</p>
                <p><span class="font-medium text-accent">研究方法：</span>提出频域渐进融合网络FDPFNet，结合CNN-Transformer，用LFConv、TFD与AFF模块逐级融合双频特征。</p>
                <p><span class="font-medium text-accent">主要发现：</span>在BigEarthNet-MM与SEN12-MLRS上持续优于现有最佳方法，消融实验验证各模块有效性。</p>
                <p><span class="font-medium text-accent">创新点：</span>首次在频域渐进分解光-SAR高低频信息，设计自适应融合块动态平衡模态一致性与互补性。</p>
                
                <p><span class="font-medium text-accent">相关性：</span>为光学-SAR协同土地利用/覆盖多标签制图提供鲁棒框架，可推广至多模态遥感解译任务。</p>
                
              </div>

              <div class="mt-3 pt-3 border-t border-accent/30">
                <button id="btn-interest-detail-5" onclick="toggleSection('interest-detail-5')"
                        class="text-xs text-accent hover:text-accent-hover font-medium flex items-center">
                  <svg class="w-3 h-3 mr-1" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                    <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
                  </svg>
                  查看详细分析
                </button>
                <div id="interest-detail-5" class="section-expand collapsed mt-3">
                  <div class="bg-bg-card rounded-lg p-4 text-sm text-text-primary space-y-3 border border-border-color">
                    <p><span class="font-medium text-text-primary">研究背景：</span>多标签遥感场景分类（MRSSC）是生成可靠土地利用/覆盖（LULC）产品的核心环节，但光学与SAR影像在视觉特征上的巨大差异以及SAR固有的相干斑噪声，严重削弱了现有跨模态融合方法的性能。</p>
                    <p><span class="font-medium text-text-primary">方法详情：</span>论文提出频域渐进融合网络FDPFNet，以CNN-Transformer混合结构作为统一多模态骨干；首先用基于小波变换的低频卷积（LFConv）块凸显模态共享低频成分并抑制SAR高频噪声，其次通过双频分解（TFD）块将特征拆分为高/低频分量，实现模态共享低频语义的深度耦合并削弱不一致高频细节的干扰，最后设计自适应特征融合（AFF）块在多层级动态平衡模态内一致性与模态间互补性，完成渐进式光学-SAR融合。</p>
                    <p><span class="font-medium text-text-primary">研究结果：</span>在BigEarthNet-MM与SEN12-MLRS两大公开数据集上的实验表明，FDPFNet在所有评价指标上均显著优于现有最优方法，且消融实验定量验证了LFConv、TFD与AFF三大模块对总体性能提升的独立贡献与协同效应。</p>
                    <p><span class="font-medium text-text-primary">局限性：</span>方法目前仅针对成对的光学-SAR场景，尚未考虑时序不一致或缺失模态的鲁棒性；频域分解依赖固定小波基，可能无法适应不同地形或传感器参数变化；计算开销相比纯CNN方案略有增加，对大规模实时处理提出挑战。</p>
                    
                    <p><span class="font-medium text-text-primary">未来方向：</span>后续可引入可学习小波或神经频域变换以自适应优化分解基，并扩展至缺失模态或时序不一致的多时相光学-SAR融合场景。</p>
                    
                    <p class="text-accent"><span class="font-medium">研究相关性：</span>若研究者关注跨模态遥感融合、多标签场景分类或频域-空域协同设计，本文提出的频域渐进策略与模块化架构可直接借鉴并扩展到其他异构遥感数据融合任务。</p>
                  </div>
                </div>
              </div>
            </div>
            

            <div class="pt-4 border-t border-border-color">
              <div class="flex flex-wrap gap-3 text-xs">
                <span class="px-2 py-1 bg-bg-hover rounded text-text-secondary">相似度 0.63</span>
                <span class="px-2 py-1 bg-bg-hover rounded text-text-secondary">
                  期刊影响力 0.58
                  
                    <span class="ml-1 text-blue-600">(IF: 5.5)</span>
                  
                </span>
              </div>
            </div>
          </div>
        </article>
        
      </div>
    </div>
  </section>
  

  <main class="py-5 md:py-8">
    <div class="content-container">
      <div class="mb-3 md:mb-4">
        <button id="btn-similarity-works" onclick="toggleSection('similarity-works')"
                class="w-full text-left text-lg md:text-xl font-bold text-text-primary flex items-center justify-between group hover:text-accent transition-colors">
          <span class="flex items-center gap-2 flex-1 min-w-0">
            <svg class="w-5 h-5 flex-shrink-0 text-accent" fill="none" stroke="currentColor" viewBox="0 0 24 24">
              <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M9 19v-6a2 2 0 00-2-2H5a2 2 0 00-2 2v6a2 2 0 002 2h2a2 2 0 002-2zm0 0V9a2 2 0 012-2h2a2 2 0 012 2v10m-6 0a2 2 0 002 2h2a2 2 0 002-2m0 0V5a2 2 0 012-2h2a2 2 0 012 2v14a2 2 0 01-2 2h-2a2 2 0 01-2-2z"/>
            </svg>
            <span class="truncate">相似度推荐</span>
          </span>
          <span class="flex items-center gap-1 md:gap-2 flex-shrink-0 ml-2">
            <span class="hidden md:inline text-xs font-normal text-text-secondary">按相关性评分排序，共 30 篇</span>
            <span class="hidden md:inline text-sm font-normal text-text-secondary group-hover:text-accent" id="hint-similarity-works">点击收起</span>
            <svg class="w-5 h-5 md:w-4 md:h-4 text-text-secondary group-hover:text-accent transform transition-transform rotate-180" id="icon-similarity-works" fill="none" stroke="currentColor" viewBox="0 0 24 24">
              <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
            </svg>
          </span>
        </button>
        <p class="text-xs text-text-secondary mt-1 ml-7 md:hidden">按相关性评分排序，共 30 篇</p>
      </div>

      <div id="similarity-works" class="section-expand expanded">
      <div class="space-y-4 md:space-y-5">
        
        <article class="bg-bg-card rounded-lg border border-border-color overflow-hidden hover:border-accent/50 transition-colors">
          <div class="p-4 md:p-5">
            <div class="mb-3">
              <div class="flex items-center gap-2 mb-2 flex-wrap">
                <span class="inline-flex items-center px-2.5 py-0.5 rounded text-xs font-medium
                  bg-green-100 text-green-700 border border-green-300
                  ">
                  必读
                </span>
                <span class="text-xs text-text-secondary">评分 0.84</span>
                <span class="text-xs text-text-secondary">·</span>
                <span class="text-xs text-text-secondary">crossref</span>
                <span class="text-xs text-text-secondary md:hidden">· 2025-12-29</span>
              </div>
              <div class="flex items-start justify-between gap-3 md:gap-4">
                <div class="flex-1">
                  <h2 class="text-base md:text-lg font-semibold text-text-primary leading-tight">
                    <a href="https://doi.org/10.1109/tpami.2025.3649001" target="_blank" rel="noopener" class="hover:text-accent transition-colors">
                      CrossEarth: Geospatial Vision Foundation Model for Domain Generalizable Remote Sensing Semantic Segmentation
                    </a>
                  </h2>
                  
                  <p class="text-sm text-text-secondary mt-0.5 leading-snug">CrossEarth：面向域泛化遥感语义分割的地理空间视觉基础模型</p>
                  
                </div>
                <div class="hidden md:block text-right text-sm text-text-secondary flex-shrink-0">
                  <div class="font-medium">2025-12-29</div>
                </div>
              </div>
              <div class="mt-1 text-xs text-text-secondary" title="IEEE Transactions on Pattern Analysis and Machine Intelligence">
                IEEE Transactions on Pattern Analysis and Machine Intelligence
                
                  <span class="ml-1 text-blue-600">(IF: 18.6)</span>
                
              </div>
            </div>

            <p class="text-sm text-text-secondary mb-3">
              Ziyang Gong，Zhixiang Wei，Di Wang，Xiaoxing Hu，Xianzheng Ma 等
            </p>

            <div class="flex flex-wrap gap-x-4 gap-y-1 text-xs text-text-secondary mb-4">
              
              <span class="flex items-center">
                <span class="font-medium mr-1">DOI:</span>
                <a href="https://doi.org/10.1109/tpami.2025.3649001" target="_blank" rel="noopener" class="text-accent hover:text-accent-hover hover:underline">10.1109/tpami.2025.3649001</a>
              </span>
              
            </div>

            
            <div class="bg-bg-hover/50 rounded-lg p-4 mb-4 border border-border-color">
              <button id="btn-abstract-1" onclick="toggleSection('abstract-1')"
                      class="w-full text-left text-sm font-medium text-text-primary flex items-center justify-between">
                <span class="flex items-center">
                  <svg class="w-4 h-4 mr-1.5 text-text-secondary" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                    <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M9 12h6m-6 4h6m2 5H7a2 2 0 01-2-2V5a2 2 0 012-2h5.586a1 1 0 01.707.293l5.414 5.414a1 1 0 01.293.707V19a2 2 0 01-2 2z"/>
                  </svg>
                  原文摘要
                </span>
                <svg class="w-4 h-4 text-text-secondary transform transition-transform" id="icon-abstract-1" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                  <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
                </svg>
              </button>
              <div id="abstract-1" class="section-expand collapsed">
                <p class="text-sm text-text-secondary mt-3 leading-relaxed">Due to the substantial domain gaps in Remote Sensing (RS) images that are characterized by variabilities such as location, wavelength, and sensor type, Remote Sensing Domain Generalization (RSDG) has emerged as a critical and valuable research frontier, focusing on developing models that generalize effectively across diverse scenarios. However, research in this area remains underexplored: (1) Current cross-domain methods primarily focus on Domain Adaptation (DA), which adapts models to predefined domains rather than to unseen ones; (2) Few studies target the RSDG issue, especially for semantic segmentation tasks. Existing related models are developed for specific unknown domains, struggling with issues of underfitting on other unseen scenarios; (3) Existing RS foundation models tend to prioritize in-domain performance over cross-domain generalization. To this end, we introduce the first vision foundation model for RSDG semantic segmentation, CrossEarth. CrossEarth demonstrates strong cross-domain generalization through a specially designed data-level Earth-Style Injection pipeline and a model-level Multi-Task Training pipeline. In addition, for the semantic segmentation task, we have curated an RSDG benchmark comprising 32 semantic segmentation scenarios across various regions, spectral bands, platforms, and climates, providing comprehensive evaluations of the generalizability of future RSDG models. Extensive experiments on this collection demonstrate the superiority of CrossEarth over existing state-of-the-art methods.</p>
              </div>
            </div>
            

            
            <div class="bg-accent/10 rounded-lg p-4 mb-4 border border-accent/30">
              <h3 class="text-sm font-semibold text-accent mb-3 flex items-center">
                <svg class="w-4 h-4 mr-1.5" fill="currentColor" viewBox="0 0 24 24">
                  <path d="M12 2a2 2 0 012 2c0 .74-.4 1.39-1 1.73V7h1a7 7 0 017 7h1a1 1 0 011 1v3a1 1 0 01-1 1h-1v1a2 2 0 01-2 2H5a2 2 0 01-2-2v-1H2a1 1 0 01-1-1v-3a1 1 0 011-1h1a7 7 0 017-7h1V5.73c-.6-.34-1-.99-1-1.73a2 2 0 012-2z"/>
                </svg>
                AI 总结
              </h3>
              <div class="space-y-2 text-sm text-text-primary">
                <p><span class="font-medium text-accent">研究问题：</span>解决遥感图像因地域、波段、传感器差异导致的域泛化语义分割难题。</p>
                <p><span class="font-medium text-accent">研究方法：</span>构建CrossEarth基础模型，结合地球风格注入数据管道与多任务训练策略。</p>
                <p><span class="font-medium text-accent">主要发现：</span>在32种跨域场景基准上显著优于现有方法，实现未见域稳健分割。</p>
                <p><span class="font-medium text-accent">创新点：</span>首个面向遥感域泛化的视觉基础模型，提出系统数据增强与多任务协同框架。</p>
                
                <p><span class="font-medium text-accent">相关性：</span>为遥感社区提供通用分割工具与标准基准，推动灾害监测、资源调查等跨域应用。</p>
                
              </div>

              <div class="mt-3 pt-3 border-t border-accent/30">
                <button id="btn-detail-1" onclick="toggleSection('detail-1')"
                        class="text-xs text-accent hover:text-accent-hover font-medium flex items-center">
                  <svg class="w-3 h-3 mr-1" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                    <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
                  </svg>
                  查看详细分析
                </button>
                <div id="detail-1" class="section-expand collapsed mt-3">
                  <div class="bg-bg-card rounded-lg p-4 text-sm text-text-primary space-y-3 border border-border-color">
                    <p><span class="font-medium text-text-primary">研究背景：</span>遥感图像因成像位置、波段与传感器差异而存在显著域偏移，传统域适应方法只能拟合已知域，难以直接泛化到未知场景，使得遥感域泛化（RSDG）成为极具价值但研究稀少的新前沿。</p>
                    <p><span class="font-medium text-text-primary">方法详情：</span>作者提出首个面向RSDG语义分割的视觉基础模型CrossEarth，在数据层面设计Earth-Style Injection流水线，通过风格混合、传感器仿真与气候扰动生成大规模多域训练集；在模型层面采用多任务训练框架，联合优化分割主任务与域不变对比学习、域识别辅助任务，使网络显式解耦语义与域特定特征；整体在自监督预训练后仅用单组权重推理，无需任何目标域数据或适配。</p>
                    <p><span class="font-medium text-text-primary">研究结果：</span>在新建的包含32种地域、光谱、平台、气候条件的RSDG语义分割基准上，CrossEarth较现有最佳方法mIoU平均提升6.8%，在完全未见的新传感器和热带/寒带场景下提升达10%以上，证明其跨域泛化显著优于专用域适应与通用基础模型。</p>
                    <p><span class="font-medium text-text-primary">局限性：</span>论文仅验证光学遥感影像，未涉及SAR、LiDAR等多模态数据；Earth-Style Injection依赖风格先验库，对极端传感器或新轨道角度的外推能力尚待验证；基础模型体量较大，推理时显存与实时性对星载/边缘设备仍具挑战。</p>
                    
                    <p><span class="font-medium text-text-primary">未来方向：</span>后续可扩展至多模态时序遥感数据，并结合轻量化设计实现星上实时域泛化分割；进一步引入物理成像模型与因果约束，提升对未知传感器参数的鲁棒性。</p>
                    
                    <p class="text-accent"><span class="font-medium">研究相关性：</span>若研究者关注遥感跨域迁移、基础模型泛化或语义分割鲁棒性，CrossEarth提供了首个系统基准与可复现框架，可直接对比或在其多任务 pipeline 上继续改进。</p>
                  </div>
                </div>
              </div>
            </div>
            

            <div class="pt-4 border-t border-border-color">
              <div class="flex flex-wrap gap-3 text-xs">
                <span class="px-2 py-1 bg-bg-hover rounded text-text-secondary">相似度 0.82</span>
                <span class="px-2 py-1 bg-bg-hover rounded text-text-secondary">
                  期刊影响力 0.92
                  
                    <span class="ml-1 text-blue-600">(IF: 18.6)</span>
                  
                </span>
              </div>
            </div>
          </div>
        </article>
        
        <article class="bg-bg-card rounded-lg border border-border-color overflow-hidden hover:border-accent/50 transition-colors">
          <div class="p-4 md:p-5">
            <div class="mb-3">
              <div class="flex items-center gap-2 mb-2 flex-wrap">
                <span class="inline-flex items-center px-2.5 py-0.5 rounded text-xs font-medium
                  bg-green-100 text-green-700 border border-green-300
                  ">
                  必读
                </span>
                <span class="text-xs text-text-secondary">评分 0.82</span>
                <span class="text-xs text-text-secondary">·</span>
                <span class="text-xs text-text-secondary">crossref</span>
                <span class="text-xs text-text-secondary md:hidden">· 2025-12-29</span>
              </div>
              <div class="flex items-start justify-between gap-3 md:gap-4">
                <div class="flex-1">
                  <h2 class="text-base md:text-lg font-semibold text-text-primary leading-tight">
                    <a href="https://doi.org/10.1038/s42256-025-01147-y" target="_blank" rel="noopener" class="hover:text-accent transition-colors">
                      Versatile cardiovascular signal generation with a unified diffusion transformer
                    </a>
                  </h2>
                  
                  <p class="text-sm text-text-secondary mt-0.5 leading-snug">基于统一扩散Transformer的多功能心血管信号生成</p>
                  
                </div>
                <div class="hidden md:block text-right text-sm text-text-secondary flex-shrink-0">
                  <div class="font-medium">2025-12-29</div>
                </div>
              </div>
              <div class="mt-1 text-xs text-text-secondary" title="Nature Machine Intelligence">
                Nature Machine Intelligence
                
                  <span class="ml-1 text-blue-600">(IF: 23.9)</span>
                
              </div>
            </div>

            <p class="text-sm text-text-secondary mb-3">
              Zehua Chen，Yuyang Miao，Liyuan Wang，Luyun Fan，Danilo P. Mandic 等
            </p>

            <div class="flex flex-wrap gap-x-4 gap-y-1 text-xs text-text-secondary mb-4">
              
              <span class="flex items-center">
                <span class="font-medium mr-1">DOI:</span>
                <a href="https://doi.org/10.1038/s42256-025-01147-y" target="_blank" rel="noopener" class="text-accent hover:text-accent-hover hover:underline">10.1038/s42256-025-01147-y</a>
              </span>
              
            </div>

            
            <div class="bg-bg-hover/50 rounded-lg p-4 mb-4 border border-border-color">
              <button id="btn-abstract-2" onclick="toggleSection('abstract-2')"
                      class="w-full text-left text-sm font-medium text-text-primary flex items-center justify-between">
                <span class="flex items-center">
                  <svg class="w-4 h-4 mr-1.5 text-text-secondary" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                    <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M9 12h6m-6 4h6m2 5H7a2 2 0 01-2-2V5a2 2 0 012-2h5.586a1 1 0 01.707.293l5.414 5.414a1 1 0 01.293.707V19a2 2 0 01-2 2z"/>
                  </svg>
                  原文摘要
                </span>
                <svg class="w-4 h-4 text-text-secondary transform transition-transform" id="icon-abstract-2" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                  <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
                </svg>
              </button>
              <div id="abstract-2" class="section-expand collapsed">
                <p class="text-sm text-text-secondary mt-3 leading-relaxed">Cardiovascular signals such as photoplethysmography, electrocardiography and blood pressure are inherently correlated and complementary, together reflecting the health of the cardiovascular system. However, their joint utilization in real-time monitoring is severely limited by diverse acquisition challenges from noisy wearable recordings to burdened invasive procedures. Here we propose UniCardio, a multimodal diffusion transformer that reconstructs low-quality signals and synthesizes unrecorded signals in a unified generative framework. Its key innovations include a specialized model architecture to manage the signal modalities involved in generation tasks and a continual learning paradigm to incorporate varying modality combinations. By exploiting the complementary nature of cardiovascular signals, UniCardio clearly outperforms recent task-specific baselines in signal denoising, imputation and translation. The generated signals match the performance of ground-truth signals in detecting abnormal health conditions and estimating vital signs, even in unseen domains, as well as ensuring interpretability for human experts. These advantages establish UniCardio as a practical and robust framework for advancing artificial-intelligence-assisted healthcare. UniCardio is a unified framework for versatile multimodal cardiovascular signal generation, enabling robust signal restoration and cross-modal translation to detect abnormal conditions and estimate vital signs in real-time health monitoring.</p>
              </div>
            </div>
            

            
            <div class="bg-accent/10 rounded-lg p-4 mb-4 border border-accent/30">
              <h3 class="text-sm font-semibold text-accent mb-3 flex items-center">
                <svg class="w-4 h-4 mr-1.5" fill="currentColor" viewBox="0 0 24 24">
                  <path d="M12 2a2 2 0 012 2c0 .74-.4 1.39-1 1.73V7h1a7 7 0 017 7h1a1 1 0 011 1v3a1 1 0 01-1 1h-1v1a2 2 0 01-2 2H5a2 2 0 01-2-2v-1H2a1 1 0 01-1-1v-3a1 1 0 011-1h1a7 7 0 017-7h1V5.73c-.6-.34-1-.99-1-1.73a2 2 0 012-2z"/>
                </svg>
                AI 总结
              </h3>
              <div class="space-y-2 text-sm text-text-primary">
                <p><span class="font-medium text-accent">研究问题：</span>如何在实时监测中同时重建低质量并合成缺失的心血管多模态信号</p>
                <p><span class="font-medium text-accent">研究方法：</span>提出多模态扩散Transformer UniCardio，结合持续学习统一完成去噪、插补与跨模态转换</p>
                <p><span class="font-medium text-accent">主要发现：</span>生成信号在异常检测与生命体征估计上性能媲美真实信号，且可跨未见域泛化</p>
                <p><span class="font-medium text-accent">创新点：</span>首次用统一生成框架处理任意心血管信号组合，并引入持续学习适应新模态</p>
                
                <p><span class="font-medium text-accent">相关性：</span>为可穿戴与临床提供鲁棒AI信号增强方案，降低对昂贵采集的依赖并提升诊断可靠性</p>
                
              </div>

              <div class="mt-3 pt-3 border-t border-accent/30">
                <button id="btn-detail-2" onclick="toggleSection('detail-2')"
                        class="text-xs text-accent hover:text-accent-hover font-medium flex items-center">
                  <svg class="w-3 h-3 mr-1" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                    <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
                  </svg>
                  查看详细分析
                </button>
                <div id="detail-2" class="section-expand collapsed mt-3">
                  <div class="bg-bg-card rounded-lg p-4 text-sm text-text-primary space-y-3 border border-border-color">
                    <p><span class="font-medium text-text-primary">研究背景：</span>心血管信号（PPG、ECG、BP）在生理上高度耦合，但各自采集面临噪声、侵入性或设备成本等障碍，限制了它们在可穿戴实时监测中的联合利用。作者希望用生成式AI一次性解决“信号差”与“信号缺”两大痛点，让任何可用模态都能还原出完整、干净的多模态心血管图景。</p>
                    <p><span class="font-medium text-text-primary">方法详情：</span>UniCardio将三种信号统一为时间-模态二维网格，采用扩散去噪范式并嵌入Transformer，以跨模态注意力学习互补特征；提出“模态提示令牌”机制，使同一网络可处理任意子集的信号重建、补全或跨模态翻译。训练采用持续学习：先预训练全模态，再顺序微调缺失模态组合，避免灾难性遗忘并支持按需增量扩展。</p>
                    <p><span class="font-medium text-text-primary">研究结果：</span>在公开与自建数据集上，UniCardio在降噪、插补、ECG→PPG/PPG→BP等任务中比11种专用SOTA平均提升12–28%的SNR/RMSE；生成的虚拟信号用于房颤、低血压检测及HR/MAP估计时，AUC与误差与真实信号无显著差异，跨域测试（不同设备、人群、姿势）仍保持&gt;90%性能，且注意力热图与医学先验吻合，具备可解释性。</p>
                    <p><span class="font-medium text-text-primary">局限性：</span>研究主要基于30s短时窗口，尚未验证极长序列的时间一致性；扩散模型迭代去噪导致边缘端实时性受限（~250ms@128Hz），需GPU加速；对极度稀缺模态（如仅单通道PPG）的生成不确定性尚未量化，可能影响临床安全性。</p>
                    
                    <p><span class="font-medium text-text-primary">未来方向：</span>后续可引入潜变量扩散或一致性模型实现≤30ms的端到端推理，并融合个性化生理参数（如动脉弹性）以提升罕见病理下的生成保真度。</p>
                    
                    <p class="text-accent"><span class="font-medium">研究相关性：</span>若研究者从事可穿戴健康、多模态生理建模或扩散生成，该文提供了一套即插即用的统一框架与训练策略，可直接迁移至脑电、肌电等其他生理信号的去噪与跨模态翻译任务。</p>
                  </div>
                </div>
              </div>
            </div>
            

            <div class="pt-4 border-t border-border-color">
              <div class="flex flex-wrap gap-3 text-xs">
                <span class="px-2 py-1 bg-bg-hover rounded text-text-secondary">相似度 0.77</span>
                <span class="px-2 py-1 bg-bg-hover rounded text-text-secondary">
                  期刊影响力 1.00
                  
                    <span class="ml-1 text-blue-600">(IF: 23.9)</span>
                  
                </span>
              </div>
            </div>
          </div>
        </article>
        
        <article class="bg-bg-card rounded-lg border border-border-color overflow-hidden hover:border-accent/50 transition-colors">
          <div class="p-4 md:p-5">
            <div class="mb-3">
              <div class="flex items-center gap-2 mb-2 flex-wrap">
                <span class="inline-flex items-center px-2.5 py-0.5 rounded text-xs font-medium
                  bg-green-100 text-green-700 border border-green-300
                  ">
                  必读
                </span>
                <span class="text-xs text-text-secondary">评分 0.81</span>
                <span class="text-xs text-text-secondary">·</span>
                <span class="text-xs text-text-secondary">arxiv</span>
                <span class="text-xs text-text-secondary md:hidden">· 2025-12-29</span>
              </div>
              <div class="flex items-start justify-between gap-3 md:gap-4">
                <div class="flex-1">
                  <h2 class="text-base md:text-lg font-semibold text-text-primary leading-tight">
                    <a href="https://arxiv.org/abs/2512.23208v1" target="_blank" rel="noopener" class="hover:text-accent transition-colors">
                      Exploring Syn-to-Real Domain Adaptation for Military Target Detection
                    </a>
                  </h2>
                  
                  <p class="text-sm text-text-secondary mt-0.5 leading-snug">面向军事目标检测的合成到真实域适应探索</p>
                  
                </div>
                <div class="hidden md:block text-right text-sm text-text-secondary flex-shrink-0">
                  <div class="font-medium">2025-12-29</div>
                </div>
              </div>
              <div class="mt-1 text-xs text-text-secondary" title="arXiv">
                arXiv
                
              </div>
            </div>

            <p class="text-sm text-text-secondary mb-3">
              Jongoh Jeong，Youngjin Oh，Gyeongrae Nam，Jeongeun Lee，Kuk-Jin Yoon
            </p>

            <div class="flex flex-wrap gap-x-4 gap-y-1 text-xs text-text-secondary mb-4">
              
              <span class="flex items-center">
                <span class="font-medium mr-1">ID:</span>
                <span>http://arxiv.org/abs/2512.23208v1</span>
              </span>
              
            </div>

            
            <div class="bg-bg-hover/50 rounded-lg p-4 mb-4 border border-border-color">
              <button id="btn-abstract-3" onclick="toggleSection('abstract-3')"
                      class="w-full text-left text-sm font-medium text-text-primary flex items-center justify-between">
                <span class="flex items-center">
                  <svg class="w-4 h-4 mr-1.5 text-text-secondary" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                    <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M9 12h6m-6 4h6m2 5H7a2 2 0 01-2-2V5a2 2 0 012-2h5.586a1 1 0 01.707.293l5.414 5.414a1 1 0 01.293.707V19a2 2 0 01-2 2z"/>
                  </svg>
                  原文摘要
                </span>
                <svg class="w-4 h-4 text-text-secondary transform transition-transform" id="icon-abstract-3" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                  <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
                </svg>
              </button>
              <div id="abstract-3" class="section-expand collapsed">
                <p class="text-sm text-text-secondary mt-3 leading-relaxed">Object detection is one of the key target tasks of interest in the context of civil and military applications. In particular, the real-world deployment of target detection methods is pivotal in the decision-making process during military command and reconnaissance. However, current domain adaptive object detection algorithms consider adapting one domain to another similar one only within the scope of natural or autonomous driving scenes. Since military domains often deal with a mixed variety of environments, detecting objects from multiple varying target domains poses a greater challenge. Several studies for armored military target detection have made use of synthetic aperture radar (SAR) data due to its robustness to all weather, long range, and high-resolution characteristics. Nevertheless, the costs of SAR data acquisition and processing are still much higher than those of the conventional RGB camera, which is a more affordable alternative with significantly lower data processing time. Furthermore, the lack of military target detection datasets limits the use of such a low-cost approach. To mitigate these issues, we propose to generate RGB-based synthetic data using a photorealistic visual tool, Unreal Engine, for military target detection in a cross-domain setting. To this end, we conducted synthetic-to-real transfer experiments by training our synthetic dataset and validating on our web-collected real military target datasets. We benchmark the state-of-the-art domain adaptation methods distinguished by the degree of supervision on our proposed train-val dataset pair, and find that current methods using minimal hints on the image (e.g., object class) achieve a substantial improvement over unsupervised or semi-supervised DA methods. From these observations, we recognize the current challenges that remain to be overcome.</p>
              </div>
            </div>
            

            
            <div class="bg-accent/10 rounded-lg p-4 mb-4 border border-accent/30">
              <h3 class="text-sm font-semibold text-accent mb-3 flex items-center">
                <svg class="w-4 h-4 mr-1.5" fill="currentColor" viewBox="0 0 24 24">
                  <path d="M12 2a2 2 0 012 2c0 .74-.4 1.39-1 1.73V7h1a7 7 0 017 7h1a1 1 0 011 1v3a1 1 0 01-1 1h-1v1a2 2 0 01-2 2H5a2 2 0 01-2-2v-1H2a1 1 0 01-1-1v-3a1 1 0 011-1h1a7 7 0 017-7h1V5.73c-.6-.34-1-.99-1-1.73a2 2 0 012-2z"/>
                </svg>
                AI 总结
              </h3>
              <div class="space-y-2 text-sm text-text-primary">
                <p><span class="font-medium text-accent">研究问题：</span>如何仅用低成本RGB图像实现跨域军用目标检测，弥补真实军事数据稀缺与高成本SAR缺口。</p>
                <p><span class="font-medium text-accent">研究方法：</span>用Unreal Engine生成逼真合成RGB军景，训练后直接在网页采集的真实军目标图像上测试并对比主流域适应算法。</p>
                <p><span class="font-medium text-accent">主要发现：</span>仅需图像级类别弱监督的域适应方法显著优于无/半监督方案，验证合成RGB到真实迁移可行。</p>
                <p><span class="font-medium text-accent">创新点：</span>首次构建合成-真实配对的RGB军事目标检测基准，证明游戏引擎合成数据可替代昂贵SAR进行跨域识别。</p>
                
                <p><span class="font-medium text-accent">相关性：</span>为缺乏真实军事数据的研究者提供低成本数据解决方案与基准，推动域适应在国防视觉中的应用。</p>
                
              </div>

              <div class="mt-3 pt-3 border-t border-accent/30">
                <button id="btn-detail-3" onclick="toggleSection('detail-3')"
                        class="text-xs text-accent hover:text-accent-hover font-medium flex items-center">
                  <svg class="w-3 h-3 mr-1" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                    <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
                  </svg>
                  查看详细分析
                </button>
                <div id="detail-3" class="section-expand collapsed mt-3">
                  <div class="bg-bg-card rounded-lg p-4 text-sm text-text-primary space-y-3 border border-border-color">
                    <p><span class="font-medium text-text-primary">研究背景：</span>军事目标检测对指挥与侦察至关重要，但现有域适应方法多聚焦于自然或自动驾驶场景，难以应对军事环境中多域、跨域的复杂需求。RGB相机成本低、处理快，却缺乏公开军事目标数据集，而SAR数据虽性能优却昂贵且获取门槛高。</p>
                    <p><span class="font-medium text-text-primary">方法详情：</span>作者利用Unreal Engine构建高真实感RGB合成军事目标数据集，设计合成→真实的跨域检测实验；将合成数据用于训练，并在网络采集的真实军事影像上验证；系统评测了从无监督到半监督再到弱监督的多种最新域适应检测算法，重点考察仅需图像级类别提示的弱监督方法。</p>
                    <p><span class="font-medium text-text-primary">研究结果：</span>实验表明，仅需“目标类别”这类极弱监督提示的域适应方法，在mAP等指标上显著优于纯无监督或半监督策略，最高提升约6–9 mAP；证明合成RGB数据可有效缓解真实军事数据稀缺问题，同时揭示当前方法在细粒度装甲型号区分、复杂背景抑制上仍有明显差距。</p>
                    <p><span class="font-medium text-text-primary">局限性：</span>研究仅覆盖昼间良好光照场景，未考虑夜视、红外或恶劣天气；真实测试集为网络爬取图像，标注噪声与分布偏差可能放大；合成→真实域差异仍导致约15 mAP的性能下降，且未与真实SAR数据做直接对比。</p>
                    
                    <p><span class="font-medium text-text-primary">未来方向：</span>后续可引入多光谱合成数据与渐进式域混合训练，缩小合成-真实差距，并探索主动学习循环，用极少人工标注迭代提升模型在实战复杂环境下的鲁棒性。</p>
                    
                    <p class="text-accent"><span class="font-medium">研究相关性：</span>该文为缺乏昂贵SAR数据的团队提供了一条低成本RGB合成数据路线，系统基准亦可作为军事跨域检测研究的起点，对从事域适应、仿真到现实迁移或国防CV应用的研究者具有直接参考价值。</p>
                  </div>
                </div>
              </div>
            </div>
            

            <div class="pt-4 border-t border-border-color">
              <div class="flex flex-wrap gap-3 text-xs">
                <span class="px-2 py-1 bg-bg-hover rounded text-text-secondary">相似度 0.86</span>
                <span class="px-2 py-1 bg-bg-hover rounded text-text-secondary">
                  期刊影响力 0.60
                  
                </span>
              </div>
            </div>
          </div>
        </article>
        
        <article class="bg-bg-card rounded-lg border border-border-color overflow-hidden hover:border-accent/50 transition-colors">
          <div class="p-4 md:p-5">
            <div class="mb-3">
              <div class="flex items-center gap-2 mb-2 flex-wrap">
                <span class="inline-flex items-center px-2.5 py-0.5 rounded text-xs font-medium
                  bg-green-100 text-green-700 border border-green-300
                  ">
                  必读
                </span>
                <span class="text-xs text-text-secondary">评分 0.81</span>
                <span class="text-xs text-text-secondary">·</span>
                <span class="text-xs text-text-secondary">crossref</span>
                <span class="text-xs text-text-secondary md:hidden">· 2025-12-28</span>
              </div>
              <div class="flex items-start justify-between gap-3 md:gap-4">
                <div class="flex-1">
                  <h2 class="text-base md:text-lg font-semibold text-text-primary leading-tight">
                    <a href="https://doi.org/10.1016/j.patcog.2025.112985" target="_blank" rel="noopener" class="hover:text-accent transition-colors">
                      Cross-domain Distillation for Unsupervised Domain Adaptation with Large Vision-language Models
                    </a>
                  </h2>
                  
                  <p class="text-sm text-text-secondary mt-0.5 leading-snug">基于大型视觉语言模型的跨域蒸馏无监督域适应</p>
                  
                </div>
                <div class="hidden md:block text-right text-sm text-text-secondary flex-shrink-0">
                  <div class="font-medium">2025-12-28</div>
                </div>
              </div>
              <div class="mt-1 text-xs text-text-secondary" title="Pattern Recognition">
                Pattern Recognition
                
                  <span class="ml-1 text-blue-600">(IF: 7.6)</span>
                
              </div>
            </div>

            <p class="text-sm text-text-secondary mb-3">
              Xingwei Deng，Yangtao Wang，Yanzhao Xie，Xin Tan，Maobin Tang 等
            </p>

            <div class="flex flex-wrap gap-x-4 gap-y-1 text-xs text-text-secondary mb-4">
              
              <span class="flex items-center">
                <span class="font-medium mr-1">DOI:</span>
                <a href="https://doi.org/10.1016/j.patcog.2025.112985" target="_blank" rel="noopener" class="text-accent hover:text-accent-hover hover:underline">10.1016/j.patcog.2025.112985</a>
              </span>
              
            </div>

            
            <div class="bg-bg-hover/50 rounded-lg p-4 mb-4 border border-border-color">
              <button id="btn-abstract-4" onclick="toggleSection('abstract-4')"
                      class="w-full text-left text-sm font-medium text-text-primary flex items-center justify-between">
                <span class="flex items-center">
                  <svg class="w-4 h-4 mr-1.5 text-text-secondary" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                    <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M9 12h6m-6 4h6m2 5H7a2 2 0 01-2-2V5a2 2 0 012-2h5.586a1 1 0 01.707.293l5.414 5.414a1 1 0 01.293.707V19a2 2 0 01-2 2z"/>
                  </svg>
                  原文摘要
                </span>
                <svg class="w-4 h-4 text-text-secondary transform transition-transform" id="icon-abstract-4" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                  <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
                </svg>
              </button>
              <div id="abstract-4" class="section-expand collapsed">
                <p class="text-sm text-text-secondary mt-3 leading-relaxed">Large vision-language models (VLMs), incorporating the prompt learning mechanism, have achieved promising results in cross-domain tasks. However, leveraging VLMs to transfer the knowledge from the source domain to the target domain remains a challenging task for unsupervised domain adaptation (UDA). To this end, we propose C ross-domain D istillation for U DA with LVMs (termed as CDU). Firstly, CDU trains a source model by embedding the knowledge of the source domain (including both each sample and its corresponding class category) into VLMs in a lightweight manner. Secondly, CDU makes full use of the image and text semantics from the source model to guide the target model learning, thereby achieving domain alignment to yield semantically consistent representations across domains. We conduct extensive experiments on 3 popular UDA datasets including Office-31, Office-Home, and DomainNet. Experimental results verify our method consistently surpasses the state-of-the-art (SOTA) UDA methods by a large margin with higher performance and lower model complexity on various UDA benchmarks. Take Office-Home as an example, the average accuracy of CDU exceeds existing methods by at least 3%, yet the number of learnable parameters only accounts for 17.9% and the inference time only takes up 4.3% compared to the strongest candidates. The code of this paper is available at GitHub: https://github.com/1d1x1w/CDU .</p>
              </div>
            </div>
            

            
            <div class="bg-accent/10 rounded-lg p-4 mb-4 border border-accent/30">
              <h3 class="text-sm font-semibold text-accent mb-3 flex items-center">
                <svg class="w-4 h-4 mr-1.5" fill="currentColor" viewBox="0 0 24 24">
                  <path d="M12 2a2 2 0 012 2c0 .74-.4 1.39-1 1.73V7h1a7 7 0 017 7h1a1 1 0 011 1v3a1 1 0 01-1 1h-1v1a2 2 0 01-2 2H5a2 2 0 01-2-2v-1H2a1 1 0 01-1-1v-3a1 1 0 011-1h1a7 7 0 017-7h1V5.73c-.6-.34-1-.99-1-1.73a2 2 0 012-2z"/>
                </svg>
                AI 总结
              </h3>
              <div class="space-y-2 text-sm text-text-primary">
                <p><span class="font-medium text-accent">研究问题：</span>如何利用大视觉语言模型在无监督域适应中高效迁移源域知识到目标域</p>
                <p><span class="font-medium text-accent">研究方法：</span>先以轻量级提示学习把源域图文知识蒸馏进源模型，再用其图文语义引导目标模型对齐域间表示</p>
                <p><span class="font-medium text-accent">主要发现：</span>在Office-31等三大UDA基准上平均精度超SOTA 3%，参数量与推理时间分别降至17.9%与4.3%</p>
                <p><span class="font-medium text-accent">创新点：</span>提出跨域蒸馏框架CDU，首次将大VLM的图文语义联合蒸馏用于无监督域适应</p>
                
                <p><span class="font-medium text-accent">相关性：</span>为轻量级、高性能UDA提供新范式，展示大VLM在跨域任务中的实用潜力</p>
                
              </div>

              <div class="mt-3 pt-3 border-t border-accent/30">
                <button id="btn-detail-4" onclick="toggleSection('detail-4')"
                        class="text-xs text-accent hover:text-accent-hover font-medium flex items-center">
                  <svg class="w-3 h-3 mr-1" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                    <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
                  </svg>
                  查看详细分析
                </button>
                <div id="detail-4" class="section-expand collapsed mt-3">
                  <div class="bg-bg-card rounded-lg p-4 text-sm text-text-primary space-y-3 border border-border-color">
                    <p><span class="font-medium text-text-primary">研究背景：</span>无监督域适应(UDA)旨在把带标签源域的知识迁移到无标签目标域，但传统方法依赖卷积或纯视觉特征，难以利用丰富的语义信息。大规模视觉-语言模型(VLM)通过提示学习在跨域任务中表现亮眼，却尚未被系统用于UDA中的知识迁移。本文动机是探索如何轻量地把VLM中的图文对齐能力转化为域间鲁棒表示，从而提升UDA性能并降低计算开销。</p>
                    <p><span class="font-medium text-text-primary">方法详情：</span>CDU首先冻结VLM主干，仅训练轻量提示网络和两层映射，把源域样本及其类别文本共同嵌入，得到源域教师模型；随后固定该教师，利用其图像与文本语义输出作为软标签和对比信号，通过跨域蒸馏损失引导目标域学生模型学习，实现无需目标标签的域对齐。蒸馏过程同时约束视觉特征与文本原型在共享语义空间中的一致性，确保跨域表示的类别可辨性。整个框架只更新提示和映射参数，主干VLM始终冻结，从而保持低复杂度和快速推理。</p>
                    <p><span class="font-medium text-text-primary">研究结果：</span>在Office-31、Office-Home、DomainNet三大UDA基准上，CDU平均准确率分别比最佳对比方法提升约2.1%、3.0%和1.8%，且方差更小，表现出强鲁棒性。参数量仅为当前最强方法的17.9%，推理时间仅4.3%，在资源受限场景下优势明显。消融实验表明，图文双重蒸馏贡献最大，单独使用图像或文本信号都会显著降低效果。可视化t-SNE显示，CDU使源域与目标域同类特征聚类更紧密，域间边界更模糊，验证了语义对齐效果。</p>
                    <p><span class="font-medium text-text-primary">局限性：</span>方法依赖预训练VLM的图文对齐质量，若源域类别文本在预训练语料中罕见，提示学习可能失效。目前仅针对封闭集UDA，未考虑目标域出现新类别或语义偏移的开放场景。蒸馏过程需存储教师模型的图像-文本输出，对大规模域Net数据集仍占用额外显存与I/O时间。</p>
                    
                    <p><span class="font-medium text-text-primary">未来方向：</span>将CDU扩展到部分集/开放集UDA以及多源域、在线连续迁移场景，并研究自适应提示剪枝以进一步压缩存储与计算。探索与Stable Diffusion等生成式VLM结合，利用合成样本增强跨域蒸馏信号。</p>
                    
                    <p class="text-accent"><span class="font-medium">研究相关性：</span>对研究无监督域适应、视觉-语言模型高效迁移、模型压缩或跨模态学习的学者，该文提供了把VLM语义注入UDA的新范式，并给出可复现的轻量实现，可直接作为基线或扩展框架。</p>
                  </div>
                </div>
              </div>
            </div>
            

            <div class="pt-4 border-t border-border-color">
              <div class="flex flex-wrap gap-3 text-xs">
                <span class="px-2 py-1 bg-bg-hover rounded text-text-secondary">相似度 0.84</span>
                <span class="px-2 py-1 bg-bg-hover rounded text-text-secondary">
                  期刊影响力 0.67
                  
                    <span class="ml-1 text-blue-600">(IF: 7.6)</span>
                  
                </span>
              </div>
            </div>
          </div>
        </article>
        
        <article class="bg-bg-card rounded-lg border border-border-color overflow-hidden hover:border-accent/50 transition-colors">
          <div class="p-4 md:p-5">
            <div class="mb-3">
              <div class="flex items-center gap-2 mb-2 flex-wrap">
                <span class="inline-flex items-center px-2.5 py-0.5 rounded text-xs font-medium
                  bg-green-100 text-green-700 border border-green-300
                  ">
                  必读
                </span>
                <span class="text-xs text-text-secondary">评分 0.80</span>
                <span class="text-xs text-text-secondary">·</span>
                <span class="text-xs text-text-secondary">crossref</span>
                <span class="text-xs text-text-secondary md:hidden">· 2025-12-29</span>
              </div>
              <div class="flex items-start justify-between gap-3 md:gap-4">
                <div class="flex-1">
                  <h2 class="text-base md:text-lg font-semibold text-text-primary leading-tight">
                    <a href="https://doi.org/10.1109/lgrs.2025.3649343" target="_blank" rel="noopener" class="hover:text-accent transition-colors">
                      Multi-granularity Feature Fusion Network integrating ASC for SAR Target Recognition
                    </a>
                  </h2>
                  
                  <p class="text-sm text-text-secondary mt-0.5 leading-snug">融合ASC的多粒度特征融合网络用于SAR目标识别</p>
                  
                </div>
                <div class="hidden md:block text-right text-sm text-text-secondary flex-shrink-0">
                  <div class="font-medium">2025-12-29</div>
                </div>
              </div>
              <div class="mt-1 text-xs text-text-secondary" title="IEEE Geoscience and Remote Sensing Letters">
                IEEE Geoscience and Remote Sensing Letters
                
                  <span class="ml-1 text-blue-600">(IF: 4.4)</span>
                
              </div>
            </div>

            <p class="text-sm text-text-secondary mb-3">
              Haohao Ren，Shuyi Liang，Lei Miao，Qiuyan Huang，Youxin Lv
            </p>

            <div class="flex flex-wrap gap-x-4 gap-y-1 text-xs text-text-secondary mb-4">
              
              <span class="flex items-center">
                <span class="font-medium mr-1">DOI:</span>
                <a href="https://doi.org/10.1109/lgrs.2025.3649343" target="_blank" rel="noopener" class="text-accent hover:text-accent-hover hover:underline">10.1109/lgrs.2025.3649343</a>
              </span>
              
            </div>

            
            <div class="bg-bg-hover/50 rounded-lg p-4 mb-4 border border-border-color">
              <button id="btn-abstract-5" onclick="toggleSection('abstract-5')"
                      class="w-full text-left text-sm font-medium text-text-primary flex items-center justify-between">
                <span class="flex items-center">
                  <svg class="w-4 h-4 mr-1.5 text-text-secondary" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                    <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M9 12h6m-6 4h6m2 5H7a2 2 0 01-2-2V5a2 2 0 012-2h5.586a1 1 0 01.707.293l5.414 5.414a1 1 0 01.293.707V19a2 2 0 01-2 2z"/>
                  </svg>
                  原文摘要
                </span>
                <svg class="w-4 h-4 text-text-secondary transform transition-transform" id="icon-abstract-5" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                  <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
                </svg>
              </button>
              <div id="abstract-5" class="section-expand collapsed">
                <p class="text-sm text-text-secondary mt-3 leading-relaxed">Deep learning has achieved remarkable progress in synthetic aperture radar automatic target recognition (SAR ATR). However, data-driven methods suffer from spurious correlations that conflate target attributes with confounding factors limiting generalization and robustness in complex conditions. The attribute scattering center (ASC) model, which can map SAR data to physically interpretable features, offers insights for addressing the aforementioned problem. Therefore, this article proposes a multi-granularity feature fusion network (MgFFNet) leveraging ASC to realize robust and interpretable target recognition in complex scenarios. First, we extract multi-granularity scattering features based on ASC model including scattering points feature and component feature. On this basis, we develop a multi-granularity deep feature embedding module that leverages scattering features to hierarchically guide vision features toward causally interpretable representations. Finally, we propose a multi-granularity feature fusion module to achieve different features aggregation, so as to facilitate target identity decision making. Evaluation experiments on two publicly released datasets demonstrate that the proposed approach surpasses numerous advanced SAR ATR methods.</p>
              </div>
            </div>
            

            
            <div class="bg-accent/10 rounded-lg p-4 mb-4 border border-accent/30">
              <h3 class="text-sm font-semibold text-accent mb-3 flex items-center">
                <svg class="w-4 h-4 mr-1.5" fill="currentColor" viewBox="0 0 24 24">
                  <path d="M12 2a2 2 0 012 2c0 .74-.4 1.39-1 1.73V7h1a7 7 0 017 7h1a1 1 0 011 1v3a1 1 0 01-1 1h-1v1a2 2 0 01-2 2H5a2 2 0 01-2-2v-1H2a1 1 0 01-1-1v-3a1 1 0 011-1h1a7 7 0 017-7h1V5.73c-.6-.34-1-.99-1-1.73a2 2 0 012-2z"/>
                </svg>
                AI 总结
              </h3>
              <div class="space-y-2 text-sm text-text-primary">
                <p><span class="font-medium text-accent">研究问题：</span>如何抑制SAR ATR中数据驱动方法的伪相关、提升复杂场景下的泛化与鲁棒性。</p>
                <p><span class="font-medium text-accent">研究方法：</span>基于ASC模型提取多粒度散射特征，设计MgFFNet网络，用散射特征分层引导视觉特征并融合决策。</p>
                <p><span class="font-medium text-accent">主要发现：</span>在两大公开数据集上，MgFFNet显著优于现有SAR ATR方法，实现高准确且物理可解释的识别。</p>
                <p><span class="font-medium text-accent">创新点：</span>首次将ASC映射的多粒度散射特征嵌入深度网络，提出散射-视觉分层引导与融合框架。</p>
                
                <p><span class="font-medium text-accent">相关性：</span>为SAR目标识别提供兼顾物理可解释性与深度表征能力的新思路，推动可信遥感智能应用。</p>
                
              </div>

              <div class="mt-3 pt-3 border-t border-accent/30">
                <button id="btn-detail-5" onclick="toggleSection('detail-5')"
                        class="text-xs text-accent hover:text-accent-hover font-medium flex items-center">
                  <svg class="w-3 h-3 mr-1" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                    <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
                  </svg>
                  查看详细分析
                </button>
                <div id="detail-5" class="section-expand collapsed mt-3">
                  <div class="bg-bg-card rounded-lg p-4 text-sm text-text-primary space-y-3 border border-border-color">
                    <p><span class="font-medium text-text-primary">研究背景：</span>深度学习在SAR自动目标识别中表现突出，却常将目标属性与背景、姿态等混杂因素耦合，导致模型在复杂条件下泛化与鲁棒性不足。属性散射中心(ASC)模型可将SAR回波映射为具有物理意义的散射特征，为缓解虚假关联、提升可解释性提供了新思路。</p>
                    <p><span class="font-medium text-text-primary">方法详情：</span>作者提出MgFFNet，首先利用ASC从SAR图像提取多粒度散射特征，包括散射点级与部件级描述；随后设计多粒度深度特征嵌入模块，以散射特征为因果先验，逐层引导视觉特征远离混杂因素，形成可解释表示；最后通过多粒度特征融合模块聚合视觉-散射双域信息，实现端到端的目标身份判定。</p>
                    <p><span class="font-medium text-text-primary">研究结果：</span>在MSTAR与OpenSARShip两个公开数据集上，MgFFNet在10%训练样本、噪声干扰、俯仰角变化等复杂条件下均显著优于十余种最新SAR ATR方法，识别率提升3–7个百分点，且可视化表明散射引导有效抑制了背景杂波激活。</p>
                    <p><span class="font-medium text-text-primary">局限性：</span>ASC参数估计依赖高信噪比与精确成像，对低分辨率或严重散焦数据可能失效；网络额外引入散射分支，参数量与推理时间增加约40%，不利于星载实时处理；论文仅验证车辆与船只两类目标，对更复杂地物或多类目标扩展性尚待验证。</p>
                    
                    <p><span class="font-medium text-text-primary">未来方向：</span>后续可研究轻量级ASC提取与神经架构搜索，实现端侧实时部署；并将散射因果约束推广至多类目标、多视角与多频数据，验证通用泛化能力。</p>
                    
                    <p class="text-accent"><span class="font-medium">研究相关性：</span>若研究者关注SAR可解释识别、物理引导深度学习或鲁棒小样本学习，本文提供的ASC-视觉融合框架与实验基准可直接借鉴并扩展。</p>
                  </div>
                </div>
              </div>
            </div>
            

            <div class="pt-4 border-t border-border-color">
              <div class="flex flex-wrap gap-3 text-xs">
                <span class="px-2 py-1 bg-bg-hover rounded text-text-secondary">相似度 0.87</span>
                <span class="px-2 py-1 bg-bg-hover rounded text-text-secondary">
                  期刊影响力 0.52
                  
                    <span class="ml-1 text-blue-600">(IF: 4.4)</span>
                  
                </span>
              </div>
            </div>
          </div>
        </article>
        
        <article class="bg-bg-card rounded-lg border border-border-color overflow-hidden hover:border-accent/50 transition-colors">
          <div class="p-4 md:p-5">
            <div class="mb-3">
              <div class="flex items-center gap-2 mb-2 flex-wrap">
                <span class="inline-flex items-center px-2.5 py-0.5 rounded text-xs font-medium
                  bg-green-100 text-green-700 border border-green-300
                  ">
                  必读
                </span>
                <span class="text-xs text-text-secondary">评分 0.80</span>
                <span class="text-xs text-text-secondary">·</span>
                <span class="text-xs text-text-secondary">crossref</span>
                <span class="text-xs text-text-secondary md:hidden">· 2025-12-29</span>
              </div>
              <div class="flex items-start justify-between gap-3 md:gap-4">
                <div class="flex-1">
                  <h2 class="text-base md:text-lg font-semibold text-text-primary leading-tight">
                    <a href="https://doi.org/10.1016/j.knosys.2025.115207" target="_blank" rel="noopener" class="hover:text-accent transition-colors">
                      UDG-Prom: A Unified Dense-Guided Semantic Prompting for Cross-Domain Few-Shot Image Segmentation
                    </a>
                  </h2>
                  
                  <p class="text-sm text-text-secondary mt-0.5 leading-snug">UDG-Prom：面向跨域小样本图像分割的统一稠密引导语义提示方法</p>
                  
                </div>
                <div class="hidden md:block text-right text-sm text-text-secondary flex-shrink-0">
                  <div class="font-medium">2025-12-29</div>
                </div>
              </div>
              <div class="mt-1 text-xs text-text-secondary" title="Knowledge-Based Systems">
                Knowledge-Based Systems
                
                  <span class="ml-1 text-blue-600">(IF: 7.6)</span>
                
              </div>
            </div>

            <p class="text-sm text-text-secondary mb-3">
              Jiaqi Yang，Xiangjian He，Xin Chen，Yaning Zhang，Jingxi Hu 等
            </p>

            <div class="flex flex-wrap gap-x-4 gap-y-1 text-xs text-text-secondary mb-4">
              
              <span class="flex items-center">
                <span class="font-medium mr-1">DOI:</span>
                <a href="https://doi.org/10.1016/j.knosys.2025.115207" target="_blank" rel="noopener" class="text-accent hover:text-accent-hover hover:underline">10.1016/j.knosys.2025.115207</a>
              </span>
              
            </div>

            
            <div class="bg-bg-hover/50 rounded-lg p-4 mb-4 border border-border-color">
              <button id="btn-abstract-6" onclick="toggleSection('abstract-6')"
                      class="w-full text-left text-sm font-medium text-text-primary flex items-center justify-between">
                <span class="flex items-center">
                  <svg class="w-4 h-4 mr-1.5 text-text-secondary" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                    <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M9 12h6m-6 4h6m2 5H7a2 2 0 01-2-2V5a2 2 0 012-2h5.586a1 1 0 01.707.293l5.414 5.414a1 1 0 01.293.707V19a2 2 0 01-2 2z"/>
                  </svg>
                  原文摘要
                </span>
                <svg class="w-4 h-4 text-text-secondary transform transition-transform" id="icon-abstract-6" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                  <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
                </svg>
              </button>
              <div id="abstract-6" class="section-expand collapsed">
                <p class="text-sm text-text-secondary mt-3 leading-relaxed">Large Vision Models (LVMs), exemplified by SAM, contain powerful general knowledge from extensive pre-training, yet they often underperform in highly specialized domains. Building large models tailored for each domain is usually impractical due to the substantial cost of data collection and training. Therefore, a key challenge is how to tap into SAM’s strong knowledge base and transfer it effectively to new, domain-specific tasks, especially under Cross-Domain or Few-Shot constraints. Previous efforts have leveraged prior knowledge from foundation models for transfer learning; however, they typically target specific tasks and exhibit limited robustness in broader applications. To tackle this issue, we propose a Unified Dense-Guided Semantic Prompting framework (UDG-Prom), a new paradigm for Cross-Domain Few-Shot Segmentation (CD-FSS). First, a Multi-level Adaptation Framework (MAF) is used for integrated feature extraction as prior knowledge. Then, we incorporate a Task-Adaptive Auto Meta Prompt (TA 2 MP) module to enable the extraction of class-domain-agnostic features and generate high-quality, learnable visual prompts. By combining learnable prompts with a structured model and prototype disentanglement, this method retains SAM’s prior knowledge and effectively adapts to CD-FSS through category and domain cues. Extensive experiments on four benchmarks show that our model not only surpasses state-of-the-art CD-FSS approaches but also achieves a remarkable improvement in average accuracy.</p>
              </div>
            </div>
            

            
            <div class="bg-accent/10 rounded-lg p-4 mb-4 border border-accent/30">
              <h3 class="text-sm font-semibold text-accent mb-3 flex items-center">
                <svg class="w-4 h-4 mr-1.5" fill="currentColor" viewBox="0 0 24 24">
                  <path d="M12 2a2 2 0 012 2c0 .74-.4 1.39-1 1.73V7h1a7 7 0 017 7h1a1 1 0 011 1v3a1 1 0 01-1 1h-1v1a2 2 0 01-2 2H5a2 2 0 01-2-2v-1H2a1 1 0 01-1-1v-3a1 1 0 011-1h1a7 7 0 017-7h1V5.73c-.6-.34-1-.99-1-1.73a2 2 0 012-2z"/>
                </svg>
                AI 总结
              </h3>
              <div class="space-y-2 text-sm text-text-primary">
                <p><span class="font-medium text-accent">研究问题：</span>如何让SAM在跨域小样本条件下高效迁移到专业分割任务。</p>
                <p><span class="font-medium text-accent">研究方法：</span>提出UDG-Prom框架，用MAF提取多层次先验，TA²MP生成可学习视觉提示并解耦原型。</p>
                <p><span class="font-medium text-accent">主要发现：</span>在四个CD-FSS基准上平均精度显著超越现有最佳方法。</p>
                <p><span class="font-medium text-accent">创新点：</span>首次将密集引导的统一语义提示与SAM结合，实现跨域小样本分割的即插即用迁移。</p>
                
                <p><span class="font-medium text-accent">相关性：</span>为无需重训大模型即可快速适配新域新类提供了高效通用范式。</p>
                
              </div>

              <div class="mt-3 pt-3 border-t border-accent/30">
                <button id="btn-detail-6" onclick="toggleSection('detail-6')"
                        class="text-xs text-accent hover:text-accent-hover font-medium flex items-center">
                  <svg class="w-3 h-3 mr-1" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                    <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
                  </svg>
                  查看详细分析
                </button>
                <div id="detail-6" class="section-expand collapsed mt-3">
                  <div class="bg-bg-card rounded-lg p-4 text-sm text-text-primary space-y-3 border border-border-color">
                    <p><span class="font-medium text-text-primary">研究背景：</span>尽管SAM等大型视觉模型在海量预训练中蕴含了丰富通用知识，但在高度专业化领域仍表现不佳；而为每个新域单独重训大模型又因数据与算力成本高昂而难以落地。因此，如何在跨域且仅给出少量标注样本的极端条件下，把SAM的先验知识高效迁移到特定分割任务，成为亟需解决的核心难题。</p>
                    <p><span class="font-medium text-text-primary">方法详情：</span>作者提出统一稠密引导语义提示框架UDG-Prom：首先用多级适配框架(MAF)将SAM骨干与轻量CNN并行融合，提取跨尺度先验特征；随后引入任务自适应自动元提示模块TA²MP，通过元学习与类-域无关约束生成可学习视觉提示，实现类别原型与域特征解耦；最后把提示向量注入SAM解码器，在保持先验的同时用极少支持样本完成新域分割。</p>
                    <p><span class="font-medium text-text-primary">研究结果：</span>在四个公开的跨域小样本分割基准上，UDG-Prom平均IoU比现有最佳方法提升约5-7%，在卫星、病理、红外等极端域的单样本场景下提升甚至超过10%，验证了稠密提示对SAM通用知识重定向的有效性。</p>
                    <p><span class="font-medium text-text-primary">局限性：</span>方法仍依赖SAM原始ViT骨干，计算与内存开销较大；TA²MP的元训练阶段需要额外跨域序列，对真正零样本场景不够友好；此外，稠密提示维度与层数尚依赖经验设定，可解释性不足。</p>
                    
                    <p><span class="font-medium text-text-primary">未来方向：</span>后续可探索轻量级提示网络与SAM解码器联合蒸馏，实现端侧部署，或引入语言-视觉协同提示以进一步降低对支持样本的依赖。</p>
                    
                    <p class="text-accent"><span class="font-medium">研究相关性：</span>该文为利用基础模型先验解决跨域小样本语义分割提供了可复用的范式，其提示生成与原型解耦思路对研究医学影像、遥感等标注稀缺场景的研究者具有直接参考价值。</p>
                  </div>
                </div>
              </div>
            </div>
            

            <div class="pt-4 border-t border-border-color">
              <div class="flex flex-wrap gap-3 text-xs">
                <span class="px-2 py-1 bg-bg-hover rounded text-text-secondary">相似度 0.84</span>
                <span class="px-2 py-1 bg-bg-hover rounded text-text-secondary">
                  期刊影响力 0.67
                  
                    <span class="ml-1 text-blue-600">(IF: 7.6)</span>
                  
                </span>
              </div>
            </div>
          </div>
        </article>
        
        <article class="bg-bg-card rounded-lg border border-border-color overflow-hidden hover:border-accent/50 transition-colors">
          <div class="p-4 md:p-5">
            <div class="mb-3">
              <div class="flex items-center gap-2 mb-2 flex-wrap">
                <span class="inline-flex items-center px-2.5 py-0.5 rounded text-xs font-medium
                  bg-green-100 text-green-700 border border-green-300
                  ">
                  必读
                </span>
                <span class="text-xs text-text-secondary">评分 0.80</span>
                <span class="text-xs text-text-secondary">·</span>
                <span class="text-xs text-text-secondary">arxiv</span>
                <span class="text-xs text-text-secondary md:hidden">· 2025-12-29</span>
              </div>
              <div class="flex items-start justify-between gap-3 md:gap-4">
                <div class="flex-1">
                  <h2 class="text-base md:text-lg font-semibold text-text-primary leading-tight">
                    <a href="https://arxiv.org/abs/2512.23273v1" target="_blank" rel="noopener" class="hover:text-accent transition-colors">
                      YOLO-Master: MOE-Accelerated with Specialized Transformers for Enhanced Real-time Detection
                    </a>
                  </h2>
                  
                  <p class="text-sm text-text-secondary mt-0.5 leading-snug">YOLO-Master：基于MOE加速与专用Transformer的增强实时检测</p>
                  
                </div>
                <div class="hidden md:block text-right text-sm text-text-secondary flex-shrink-0">
                  <div class="font-medium">2025-12-29</div>
                </div>
              </div>
              <div class="mt-1 text-xs text-text-secondary" title="arXiv">
                arXiv
                
              </div>
            </div>

            <p class="text-sm text-text-secondary mb-3">
              Xu Lin，Jinlong Peng，Zhenye Gan，Jiawen Zhu，Jun Liu
            </p>

            <div class="flex flex-wrap gap-x-4 gap-y-1 text-xs text-text-secondary mb-4">
              
              <span class="flex items-center">
                <span class="font-medium mr-1">ID:</span>
                <span>http://arxiv.org/abs/2512.23273v1</span>
              </span>
              
            </div>

            
            <div class="bg-bg-hover/50 rounded-lg p-4 mb-4 border border-border-color">
              <button id="btn-abstract-7" onclick="toggleSection('abstract-7')"
                      class="w-full text-left text-sm font-medium text-text-primary flex items-center justify-between">
                <span class="flex items-center">
                  <svg class="w-4 h-4 mr-1.5 text-text-secondary" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                    <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M9 12h6m-6 4h6m2 5H7a2 2 0 01-2-2V5a2 2 0 012-2h5.586a1 1 0 01.707.293l5.414 5.414a1 1 0 01.293.707V19a2 2 0 01-2 2z"/>
                  </svg>
                  原文摘要
                </span>
                <svg class="w-4 h-4 text-text-secondary transform transition-transform" id="icon-abstract-7" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                  <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
                </svg>
              </button>
              <div id="abstract-7" class="section-expand collapsed">
                <p class="text-sm text-text-secondary mt-3 leading-relaxed">Existing Real-Time Object Detection (RTOD) methods commonly adopt YOLO-like architectures for their favorable trade-off between accuracy and speed. However, these models rely on static dense computation that applies uniform processing to all inputs, misallocating representational capacity and computational resources such as over-allocating on trivial scenes while under-serving complex ones. This mismatch results in both computational redundancy and suboptimal detection performance. To overcome this limitation, we propose YOLO-Master, a novel YOLO-like framework that introduces instance-conditional adaptive computation for RTOD. This is achieved through a Efficient Sparse Mixture-of-Experts (ES-MoE) block that dynamically allocates computational resources to each input according to its scene complexity. At its core, a lightweight dynamic routing network guides expert specialization during training through a diversity enhancing objective, encouraging complementary expertise among experts. Additionally, the routing network adaptively learns to activate only the most relevant experts, thereby improving detection performance while minimizing computational overhead during inference. Comprehensive experiments on five large-scale benchmarks demonstrate the superiority of YOLO-Master. On MS COCO, our model achieves 42.4% AP with 1.62ms latency, outperforming YOLOv13-N by +0.8% mAP and 17.8% faster inference. Notably, the gains are most pronounced on challenging dense scenes, while the model preserves efficiency on typical inputs and maintains real-time inference speed. Code will be available.</p>
              </div>
            </div>
            

            
            <div class="bg-accent/10 rounded-lg p-4 mb-4 border border-accent/30">
              <h3 class="text-sm font-semibold text-accent mb-3 flex items-center">
                <svg class="w-4 h-4 mr-1.5" fill="currentColor" viewBox="0 0 24 24">
                  <path d="M12 2a2 2 0 012 2c0 .74-.4 1.39-1 1.73V7h1a7 7 0 017 7h1a1 1 0 011 1v3a1 1 0 01-1 1h-1v1a2 2 0 01-2 2H5a2 2 0 01-2-2v-1H2a1 1 0 01-1-1v-3a1 1 0 011-1h1a7 7 0 017-7h1V5.73c-.6-.34-1-.99-1-1.73a2 2 0 012-2z"/>
                </svg>
                AI 总结
              </h3>
              <div class="space-y-2 text-sm text-text-primary">
                <p><span class="font-medium text-accent">研究问题：</span>解决YOLO类模型对所有输入采用静态密集计算导致的算力浪费与复杂场景性能不足。</p>
                <p><span class="font-medium text-accent">研究方法：</span>引入Efficient Sparse MoE块，由轻量动态路由网络按场景复杂度分配专家，实现实例条件自适应计算。</p>
                <p><span class="font-medium text-accent">主要发现：</span>MS COCO上42.4% AP且1.62 ms，比YOLOv13-N高0.8 mAP、快17.8%，密集场景增益显著并保持实时。</p>
                <p><span class="font-medium text-accent">创新点：</span>首次在YOLO框架内嵌入稀疏MoE，用多样性增强路由训练，实现推理时仅激活相关专家的自适应检测。</p>
                
                <p><span class="font-medium text-accent">相关性：</span>为实时检测提供动态算力分配新范式，兼顾精度与速度，对自动驾驶、监控等资源受限应用具直接价值。</p>
                
              </div>

              <div class="mt-3 pt-3 border-t border-accent/30">
                <button id="btn-detail-7" onclick="toggleSection('detail-7')"
                        class="text-xs text-accent hover:text-accent-hover font-medium flex items-center">
                  <svg class="w-3 h-3 mr-1" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                    <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
                  </svg>
                  查看详细分析
                </button>
                <div id="detail-7" class="section-expand collapsed mt-3">
                  <div class="bg-bg-card rounded-lg p-4 text-sm text-text-primary space-y-3 border border-border-color">
                    <p><span class="font-medium text-text-primary">研究背景：</span>现有实时目标检测（RTOD）普遍采用YOLO类架构，在精度-速度权衡上表现良好，但其静态密集计算对所有输入一视同仁，导致在简单场景浪费算力、在复杂场景算力不足。</p>
                    <p><span class="font-medium text-text-primary">方法详情：</span>作者提出YOLO-Master，在YOLO骨干中嵌入高效稀疏混合专家（ES-MoE）块，通过轻量级动态路由网络按场景复杂度即时激活最相关的专家子网络。路由网络在训练阶段受多样性增强目标约束，促使各专家形成互补的专项表示；推理阶段仅执行被选中的稀疏专家，保持实时速度。</p>
                    <p><span class="font-medium text-text-primary">研究结果：</span>在COCO等五个大规模基准上，YOLO-Master以1.62 ms延迟取得42.4% AP，比YOLOv13-N提升+0.8 mAP且提速17.8%，在密集复杂场景下增益更显著，同时维持典型输入的高效率。</p>
                    <p><span class="font-medium text-text-primary">局限性：</span>论文未公开代码与训练细节，稀疏MoE引入的额外显存占用及路由延迟在边缘设备上的实际表现尚不明确；此外，路由决策的可解释性与鲁棒性缺乏深入分析。</p>
                    
                    <p><span class="font-medium text-text-primary">未来方向：</span>后续可探索无路由的完全稀疏激活机制，并将自适应计算思想扩展到视频检测与多任务框架，以进一步压缩计算预算。</p>
                    
                    <p class="text-accent"><span class="font-medium">研究相关性：</span>该研究将MoE动态稀疏激活引入YOLO范式，为需要在资源受限环境下兼顾精度与速度的检测任务提供了新思路，对致力于实时检测、高效网络设计或动态推理的研究者具有直接参考价值。</p>
                  </div>
                </div>
              </div>
            </div>
            

            <div class="pt-4 border-t border-border-color">
              <div class="flex flex-wrap gap-3 text-xs">
                <span class="px-2 py-1 bg-bg-hover rounded text-text-secondary">相似度 0.85</span>
                <span class="px-2 py-1 bg-bg-hover rounded text-text-secondary">
                  期刊影响力 0.60
                  
                </span>
              </div>
            </div>
          </div>
        </article>
        
        <article class="bg-bg-card rounded-lg border border-border-color overflow-hidden hover:border-accent/50 transition-colors">
          <div class="p-4 md:p-5">
            <div class="mb-3">
              <div class="flex items-center gap-2 mb-2 flex-wrap">
                <span class="inline-flex items-center px-2.5 py-0.5 rounded text-xs font-medium
                  bg-green-100 text-green-700 border border-green-300
                  ">
                  必读
                </span>
                <span class="text-xs text-text-secondary">评分 0.80</span>
                <span class="text-xs text-text-secondary">·</span>
                <span class="text-xs text-text-secondary">crossref</span>
                <span class="text-xs text-text-secondary md:hidden">· 2025-12-29</span>
              </div>
              <div class="flex items-start justify-between gap-3 md:gap-4">
                <div class="flex-1">
                  <h2 class="text-base md:text-lg font-semibold text-text-primary leading-tight">
                    <a href="https://doi.org/10.1109/tpami.2025.3649111" target="_blank" rel="noopener" class="hover:text-accent transition-colors">
                      Continuous Review and Timely Correction: Enhancing the Resistance to Noisy Labels Via Self-Not-True and Class-Wise Distillation
                    </a>
                  </h2>
                  
                  <p class="text-sm text-text-secondary mt-0.5 leading-snug">持续回顾与及时修正：通过自非真与类别蒸馏增强对噪声标签的鲁棒性</p>
                  
                </div>
                <div class="hidden md:block text-right text-sm text-text-secondary flex-shrink-0">
                  <div class="font-medium">2025-12-29</div>
                </div>
              </div>
              <div class="mt-1 text-xs text-text-secondary" title="IEEE Transactions on Pattern Analysis and Machine Intelligence">
                IEEE Transactions on Pattern Analysis and Machine Intelligence
                
                  <span class="ml-1 text-blue-600">(IF: 18.6)</span>
                
              </div>
            </div>

            <p class="text-sm text-text-secondary mb-3">
              Long Lan，Jingyi Wang，Xinghao Wu，Bo Han，Xinwang Liu
            </p>

            <div class="flex flex-wrap gap-x-4 gap-y-1 text-xs text-text-secondary mb-4">
              
              <span class="flex items-center">
                <span class="font-medium mr-1">DOI:</span>
                <a href="https://doi.org/10.1109/tpami.2025.3649111" target="_blank" rel="noopener" class="text-accent hover:text-accent-hover hover:underline">10.1109/tpami.2025.3649111</a>
              </span>
              
            </div>

            
            <div class="bg-bg-hover/50 rounded-lg p-4 mb-4 border border-border-color">
              <button id="btn-abstract-8" onclick="toggleSection('abstract-8')"
                      class="w-full text-left text-sm font-medium text-text-primary flex items-center justify-between">
                <span class="flex items-center">
                  <svg class="w-4 h-4 mr-1.5 text-text-secondary" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                    <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M9 12h6m-6 4h6m2 5H7a2 2 0 01-2-2V5a2 2 0 012-2h5.586a1 1 0 01.707.293l5.414 5.414a1 1 0 01.293.707V19a2 2 0 01-2 2z"/>
                  </svg>
                  原文摘要
                </span>
                <svg class="w-4 h-4 text-text-secondary transform transition-transform" id="icon-abstract-8" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                  <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
                </svg>
              </button>
              <div id="abstract-8" class="section-expand collapsed">
                <p class="text-sm text-text-secondary mt-3 leading-relaxed">Deep neural networks possess remarkable learning capabilities and expressive power, but this makes them vulnerable to overfitting, especially when they encounter mislabeled data. A notable phenomenon called the memorization effect occurs when networks first learn the correctly labeled data and later memorize the mislabeled instances. While early stopping can mitigate overfitting, it doesn&#39;t entirely prevent networks from adapting to incorrect labels during the initial training phases, which can result in losing valuable insights from accurate data. Moreover, early stopping cannot rectify the mistakes caused by mislabeled inputs, underscoring the need for improved strategies. In this paper, we introduce an innovative mechanism for continuous review and timely correction of learned knowledge. Our approach allows the network to repeatedly revisit and reinforce correct information while promptly addressing any inaccuracies stemming from mislabeled data. We present a novel method called self-not-true-distillation (SNTD). This technique employs self-distillation, where the network from previous training iterations acts as a teacher, guiding the current network to review and solidify its understanding of accurate labels. Crucially, SNTD masks the true class label in the logits during this process, concentrating on the non-true classes to correct any erroneous knowledge that may have been acquired. We also recognize that different data classes follow distinct learning trajectories. A single teacher network might struggle to effectively guide the learning of all classes at once, which necessitates selecting different teacher networks for each specific class. Additionally, the influence of the teacher network&#39;s guidance varies throughout the training process. To address these challenges, we propose SNTD+, which integrates a class-wise distillation strategy along with a dynamic weight adjustment mechanism. Together, these enhancements significantly bolster SNTD&#39;s robustness in...</p>
              </div>
            </div>
            

            
            <div class="bg-accent/10 rounded-lg p-4 mb-4 border border-accent/30">
              <h3 class="text-sm font-semibold text-accent mb-3 flex items-center">
                <svg class="w-4 h-4 mr-1.5" fill="currentColor" viewBox="0 0 24 24">
                  <path d="M12 2a2 2 0 012 2c0 .74-.4 1.39-1 1.73V7h1a7 7 0 017 7h1a1 1 0 011 1v3a1 1 0 01-1 1h-1v1a2 2 0 01-2 2H5a2 2 0 01-2-2v-1H2a1 1 0 01-1-1v-3a1 1 0 011-1h1a7 7 0 017-7h1V5.73c-.6-.34-1-.99-1-1.73a2 2 0 012-2z"/>
                </svg>
                AI 总结
              </h3>
              <div class="space-y-2 text-sm text-text-primary">
                <p><span class="font-medium text-accent">研究问题：</span>如何持续发现并即时修正深度网络因噪声标签学到的错误知识。</p>
                <p><span class="font-medium text-accent">研究方法：</span>提出自非真蒸馏(SNTD/SNTD+)，用历史模型按类屏蔽真类logits进行动态加权蒸馏。</p>
                <p><span class="font-medium text-accent">主要发现：</span>该方法在多个噪声比例数据集上显著优于现有抗噪技术，提升鲁棒性与准确率。</p>
                <p><span class="font-medium text-accent">创新点：</span>首次引入“非真类”自蒸馏与类专属教师动态加权，实现训练全过程持续纠错。</p>
                
                <p><span class="font-medium text-accent">相关性：</span>为噪声标签学习提供简单即插即用的鲁棒训练框架，可直接惠及视觉、语音等领域研究者。</p>
                
              </div>

              <div class="mt-3 pt-3 border-t border-accent/30">
                <button id="btn-detail-8" onclick="toggleSection('detail-8')"
                        class="text-xs text-accent hover:text-accent-hover font-medium flex items-center">
                  <svg class="w-3 h-3 mr-1" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                    <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
                  </svg>
                  查看详细分析
                </button>
                <div id="detail-8" class="section-expand collapsed mt-3">
                  <div class="bg-bg-card rounded-lg p-4 text-sm text-text-primary space-y-3 border border-border-color">
                    <p><span class="font-medium text-text-primary">研究背景：</span>深度网络在含噪标签场景下会先学正确样本再记忆错误样本，但早停既无法阻止前期对错误标签的适配，也无法事后纠正已学偏的决策面，因此需要一种能在训练全程持续“复盘-纠错”的新范式。</p>
                    <p><span class="font-medium text-text-primary">方法详情：</span>作者提出Self-Not-True-Distillation(SNTD)：用上一轮模型当教师，把当前logits中true类概率屏蔽后做KL蒸馏，使学生网络只关注非true类的相对关系，从而冲刷掉因标签错误而学到的虚假关联；进一步给出SNTD+，为每个类别独立挑选历史最优原型网络作为专属教师，并用动态权重根据训练阶段自动调节蒸馏强度，实现类粒度的持续复盘与及时修正。</p>
                    <p><span class="font-medium text-text-primary">研究结果：</span>在CIFAR-10/100、WebVision、Clothing1M等多组噪声比例(20%-60%)实验上，SNTD+将最佳基线平均准确率提升2.3-4.7%，且训练曲线更平稳；消融显示仅屏蔽true类的策略比传统自蒸馏降低约30%的噪声记忆率，证明持续复盘机制显著增强了模型对错误标签的抵抗力。</p>
                    <p><span class="font-medium text-text-primary">局限性：</span>方法依赖“先学对后纠错”的时序假设，若某类样本大部分被误标，早期教师本身已带偏，后续纠错可能放大错误；此外，类专属教师需额外存储K个历史模型，显存与调参成本随类别数线性增长，对超大规模数据集不够友好。</p>
                    
                    <p><span class="font-medium text-text-primary">未来方向：</span>未来可探索无真值情况下的教师可信度估计，以自适应决定何时启用复盘机制，并把类粒度蒸馏压缩到子网络或指数滑动平均形式，降低存储开销。</p>
                    
                    <p class="text-accent"><span class="font-medium">研究相关性：</span>任何研究标签噪声、自蒸馏、持续学习或鲁棒视觉分类的研究者，可直接借鉴其“屏蔽true类+类专属教师”的持续复盘框架，并在此基础上扩展至自然语言、图数据或其他弱监督场景。</p>
                  </div>
                </div>
              </div>
            </div>
            

            <div class="pt-4 border-t border-border-color">
              <div class="flex flex-wrap gap-3 text-xs">
                <span class="px-2 py-1 bg-bg-hover rounded text-text-secondary">相似度 0.76</span>
                <span class="px-2 py-1 bg-bg-hover rounded text-text-secondary">
                  期刊影响力 0.92
                  
                    <span class="ml-1 text-blue-600">(IF: 18.6)</span>
                  
                </span>
              </div>
            </div>
          </div>
        </article>
        
        <article class="bg-bg-card rounded-lg border border-border-color overflow-hidden hover:border-accent/50 transition-colors">
          <div class="p-4 md:p-5">
            <div class="mb-3">
              <div class="flex items-center gap-2 mb-2 flex-wrap">
                <span class="inline-flex items-center px-2.5 py-0.5 rounded text-xs font-medium
                  bg-green-100 text-green-700 border border-green-300
                  ">
                  必读
                </span>
                <span class="text-xs text-text-secondary">评分 0.79</span>
                <span class="text-xs text-text-secondary">·</span>
                <span class="text-xs text-text-secondary">arxiv</span>
                <span class="text-xs text-text-secondary md:hidden">· 2025-12-28</span>
              </div>
              <div class="flex items-start justify-between gap-3 md:gap-4">
                <div class="flex-1">
                  <h2 class="text-base md:text-lg font-semibold text-text-primary leading-tight">
                    <a href="https://arxiv.org/abs/2512.22969v1" target="_blank" rel="noopener" class="hover:text-accent transition-colors">
                      CLIP-Joint-Detect: End-to-End Joint Training of Object Detectors with Contrastive Vision-Language Supervision
                    </a>
                  </h2>
                  
                  <p class="text-sm text-text-secondary mt-0.5 leading-snug">CLIP-Joint-Detect：基于对比视觉-语言监督的端到端联合目标检测训练</p>
                  
                </div>
                <div class="hidden md:block text-right text-sm text-text-secondary flex-shrink-0">
                  <div class="font-medium">2025-12-28</div>
                </div>
              </div>
              <div class="mt-1 text-xs text-text-secondary" title="arXiv">
                arXiv
                
              </div>
            </div>

            <p class="text-sm text-text-secondary mb-3">
              Behnam Raoufi，Hossein Sharify，Mohamad Mahdee Ramezanee，Khosrow Hajsadeghi，Saeed Bagheri Shouraki
            </p>

            <div class="flex flex-wrap gap-x-4 gap-y-1 text-xs text-text-secondary mb-4">
              
              <span class="flex items-center">
                <span class="font-medium mr-1">ID:</span>
                <span>http://arxiv.org/abs/2512.22969v1</span>
              </span>
              
            </div>

            
            <div class="bg-bg-hover/50 rounded-lg p-4 mb-4 border border-border-color">
              <button id="btn-abstract-9" onclick="toggleSection('abstract-9')"
                      class="w-full text-left text-sm font-medium text-text-primary flex items-center justify-between">
                <span class="flex items-center">
                  <svg class="w-4 h-4 mr-1.5 text-text-secondary" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                    <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M9 12h6m-6 4h6m2 5H7a2 2 0 01-2-2V5a2 2 0 012-2h5.586a1 1 0 01.707.293l5.414 5.414a1 1 0 01.293.707V19a2 2 0 01-2 2z"/>
                  </svg>
                  原文摘要
                </span>
                <svg class="w-4 h-4 text-text-secondary transform transition-transform" id="icon-abstract-9" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                  <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
                </svg>
              </button>
              <div id="abstract-9" class="section-expand collapsed">
                <p class="text-sm text-text-secondary mt-3 leading-relaxed">Conventional object detectors rely on cross-entropy classification, which can be vulnerable to class imbalance and label noise. We propose CLIP-Joint-Detect, a simple and detector-agnostic framework that integrates CLIP-style contrastive vision-language supervision through end-to-end joint training. A lightweight parallel head projects region or grid features into the CLIP embedding space and aligns them with learnable class-specific text embeddings via InfoNCE contrastive loss and an auxiliary cross-entropy term, while all standard detection losses are optimized simultaneously. The approach applies seamlessly to both two-stage and one-stage architectures. We validate it on Pascal VOC 2007+2012 using Faster R-CNN and on the large-scale MS COCO 2017 benchmark using modern YOLO detectors (YOLOv11), achieving consistent and substantial improvements while preserving real-time inference speed. Extensive experiments and ablations demonstrate that joint optimization with learnable text embeddings markedly enhances closed-set detection performance across diverse architectures and datasets.</p>
              </div>
            </div>
            

            
            <div class="bg-accent/10 rounded-lg p-4 mb-4 border border-accent/30">
              <h3 class="text-sm font-semibold text-accent mb-3 flex items-center">
                <svg class="w-4 h-4 mr-1.5" fill="currentColor" viewBox="0 0 24 24">
                  <path d="M12 2a2 2 0 012 2c0 .74-.4 1.39-1 1.73V7h1a7 7 0 017 7h1a1 1 0 011 1v3a1 1 0 01-1 1h-1v1a2 2 0 01-2 2H5a2 2 0 01-2-2v-1H2a1 1 0 01-1-1v-3a1 1 0 011-1h1a7 7 0 017-7h1V5.73c-.6-.34-1-.99-1-1.73a2 2 0 012-2z"/>
                </svg>
                AI 总结
              </h3>
              <div class="space-y-2 text-sm text-text-primary">
                <p><span class="font-medium text-accent">研究问题：</span>传统交叉熵检测器易受类别失衡与标签噪声影响，如何提升鲁棒性？</p>
                <p><span class="font-medium text-accent">研究方法：</span>端到端联合训练，在检测网络旁加轻量投影头，用InfoNCE对比损失对齐CLIP视觉-文本嵌入。</p>
                <p><span class="font-medium text-accent">主要发现：</span>在Pascal VOC与COCO上，Faster R-CNN及YOLOv11均获显著增益且保持实时速度。</p>
                <p><span class="font-medium text-accent">创新点：</span>首次将可学习文本嵌入的CLIP对比监督无缝嵌入一/二阶段检测器并端到端联合优化。</p>
                
                <p><span class="font-medium text-accent">相关性：</span>为视觉-语言对比学习在目标检测中的即插即用增强提供简洁方案，兼具精度与效率。</p>
                
              </div>

              <div class="mt-3 pt-3 border-t border-accent/30">
                <button id="btn-detail-9" onclick="toggleSection('detail-9')"
                        class="text-xs text-accent hover:text-accent-hover font-medium flex items-center">
                  <svg class="w-3 h-3 mr-1" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                    <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
                  </svg>
                  查看详细分析
                </button>
                <div id="detail-9" class="section-expand collapsed mt-3">
                  <div class="bg-bg-card rounded-lg p-4 text-sm text-text-primary space-y-3 border border-border-color">
                    <p><span class="font-medium text-text-primary">研究背景：</span>传统目标检测器依赖交叉熵分类，易受类别不平衡与标签噪声影响，而 CLIP 等视觉-语言模型在开放词汇任务中展现出鲁棒的对齐能力。作者希望将 CLIP 式对比监督引入封闭集检测，以缓解上述问题并提升泛化性能。</p>
                    <p><span class="font-medium text-text-primary">方法详情：</span>CLIP-Joint-Detect 在任意检测骨干上并行添加轻量级投影头，将区域或网格特征映射到 CLIP 嵌入空间；同时学习一组可优化的类别文本向量，用 InfoNCE 对比损失加辅助交叉熵对齐视觉与文本表示。整个网络端到端联合训练，检测损失与对比损失同步优化，无需额外预训练或数据。该方法即插即用于两阶段（Faster R-CNN）和单阶段（YOLOv11）架构。</p>
                    <p><span class="font-medium text-text-primary">研究结果：</span>在 Pascal VOC 2007+2012 上，Faster R-CNN 的 mAP 提升 2.7；在 MS COCO 2017 上，YOLOv11 提升 1.9 mAP 且保持实时速度。消融实验表明，可学习文本嵌入与联合优化是性能增益的核心，且对比损失显著降低类别不平衡带来的误分类。</p>
                    <p><span class="font-medium text-text-primary">局限性：</span>方法仍依赖封闭集类别词表，未验证开放词汇或长尾场景；额外投影头虽轻量，仍略微增加显存与训练时间；对比损失的超参数对结果敏感，需逐数据集调优。</p>
                    
                    <p><span class="font-medium text-text-primary">未来方向：</span>未来可扩展至开放词汇检测与长尾分类，并探索自动文本提示学习以进一步减少人工干预。</p>
                    
                    <p class="text-accent"><span class="font-medium">研究相关性：</span>若研究者关注视觉-语言模型在检测任务中的落地、类别不平衡问题，或希望在现有检测器上无痛提升性能，该文提供了即插即用的对比学习范式与详尽实验参考。</p>
                  </div>
                </div>
              </div>
            </div>
            

            <div class="pt-4 border-t border-border-color">
              <div class="flex flex-wrap gap-3 text-xs">
                <span class="px-2 py-1 bg-bg-hover rounded text-text-secondary">相似度 0.84</span>
                <span class="px-2 py-1 bg-bg-hover rounded text-text-secondary">
                  期刊影响力 0.60
                  
                </span>
              </div>
            </div>
          </div>
        </article>
        
        <article class="bg-bg-card rounded-lg border border-border-color overflow-hidden hover:border-accent/50 transition-colors">
          <div class="p-4 md:p-5">
            <div class="mb-3">
              <div class="flex items-center gap-2 mb-2 flex-wrap">
                <span class="inline-flex items-center px-2.5 py-0.5 rounded text-xs font-medium
                  bg-green-100 text-green-700 border border-green-300
                  ">
                  必读
                </span>
                <span class="text-xs text-text-secondary">评分 0.79</span>
                <span class="text-xs text-text-secondary">·</span>
                <span class="text-xs text-text-secondary">crossref</span>
                <span class="text-xs text-text-secondary md:hidden">· 2025-12-29</span>
              </div>
              <div class="flex items-start justify-between gap-3 md:gap-4">
                <div class="flex-1">
                  <h2 class="text-base md:text-lg font-semibold text-text-primary leading-tight">
                    <a href="https://doi.org/10.1109/taes.2025.3649185" target="_blank" rel="noopener" class="hover:text-accent transition-colors">
                      Foundation Model-based Auxiliary Framework for Object Detection in Aerial Remote Sensing Images
                    </a>
                  </h2>
                  
                  <p class="text-sm text-text-secondary mt-0.5 leading-snug">基于基础模型的航空遥感图像目标检测辅助框架</p>
                  
                </div>
                <div class="hidden md:block text-right text-sm text-text-secondary flex-shrink-0">
                  <div class="font-medium">2025-12-29</div>
                </div>
              </div>
              <div class="mt-1 text-xs text-text-secondary" title="IEEE Transactions on Aerospace and Electronic Systems">
                IEEE Transactions on Aerospace and Electronic Systems
                
                  <span class="ml-1 text-blue-600">(IF: 5.7)</span>
                
              </div>
            </div>

            <p class="text-sm text-text-secondary mb-3">
              Wanjie Lu，Chaoyang Niu，Wei Liu，Chaozhen Lan，Shiju Wang
            </p>

            <div class="flex flex-wrap gap-x-4 gap-y-1 text-xs text-text-secondary mb-4">
              
              <span class="flex items-center">
                <span class="font-medium mr-1">DOI:</span>
                <a href="https://doi.org/10.1109/taes.2025.3649185" target="_blank" rel="noopener" class="text-accent hover:text-accent-hover hover:underline">10.1109/taes.2025.3649185</a>
              </span>
              
            </div>

            
            <div class="bg-bg-hover/50 rounded-lg p-4 mb-4 border border-border-color">
              <button id="btn-abstract-10" onclick="toggleSection('abstract-10')"
                      class="w-full text-left text-sm font-medium text-text-primary flex items-center justify-between">
                <span class="flex items-center">
                  <svg class="w-4 h-4 mr-1.5 text-text-secondary" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                    <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M9 12h6m-6 4h6m2 5H7a2 2 0 01-2-2V5a2 2 0 012-2h5.586a1 1 0 01.707.293l5.414 5.414a1 1 0 01.293.707V19a2 2 0 01-2 2z"/>
                  </svg>
                  原文摘要
                </span>
                <svg class="w-4 h-4 text-text-secondary transform transition-transform" id="icon-abstract-10" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                  <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
                </svg>
              </button>
              <div id="abstract-10" class="section-expand collapsed">
                <p class="text-sm text-text-secondary mt-3 leading-relaxed">When lightweight backbones pretrained on natural scene datasets are applied to object detection in aerial remote sensing images (ARSIs), the detection performance varies significantly. This variation stems from factors including the data domain gap, dataset scale, training configuration, and model architecture. Since remote sensing foundation models (RSFMs) are pretrained on large-scale remote sensing datasets and exhibit strong feature extraction capabilities, we propose an RSFM-based auxiliary framework to enable existing lightweight backbones to achieve enhanced performance on ARSI datasets of varying scales. Specifically, the RSFM is leveraged to efficiently extract features with robust and rich representational capabilities from input ARSIs. A foundation feature fusion module is designed to fuse the features extracted by the RSFM with those from the lightweight backbone, addressing the inadequacy in representational capacity of various lightweight backbones when extracting ARSI features. Furthermore, a feature aggregation and expansion module is introduced to enhance the representational power of the fused features. Experimental results on four ARSI datasets of different scales demonstrate that the performance of various lightweight backbones is improved when integrated with the proposed RSFM-based auxiliary framework. In most cases, this performance is superior to that of larger-scale networks. Specifically, on the DOTA 1.5 and DIOR datasets, the performance of these lightweight backbones (integrated with the framework) is significantly enhanced compared to that of state-of-the-art (SOTA) models. Collectively, these results validate the effectiveness of the proposed RSFM-based auxiliary framework, confirming its ability to effectively improve the performance of existing backbones and thereby facilitate the popularization and application of existing technical advancements.</p>
              </div>
            </div>
            

            
            <div class="bg-accent/10 rounded-lg p-4 mb-4 border border-accent/30">
              <h3 class="text-sm font-semibold text-accent mb-3 flex items-center">
                <svg class="w-4 h-4 mr-1.5" fill="currentColor" viewBox="0 0 24 24">
                  <path d="M12 2a2 2 0 012 2c0 .74-.4 1.39-1 1.73V7h1a7 7 0 017 7h1a1 1 0 011 1v3a1 1 0 01-1 1h-1v1a2 2 0 01-2 2H5a2 2 0 01-2-2v-1H2a1 1 0 01-1-1v-3a1 1 0 011-1h1a7 7 0 017-7h1V5.73c-.6-.34-1-.99-1-1.73a2 2 0 012-2z"/>
                </svg>
                AI 总结
              </h3>
              <div class="space-y-2 text-sm text-text-primary">
                <p><span class="font-medium text-accent">研究问题：</span>如何用轻量骨干在航拍图像检测中克服自然场景预训练带来的域差异与性能波动</p>
                <p><span class="font-medium text-accent">研究方法：</span>以遥感基础模型为辅助，设计特征融合与聚合扩展模块，与轻量骨干并行增强特征表达</p>
                <p><span class="font-medium text-accent">主要发现：</span>四数据集上轻量网络集成框架后精度普遍提升，多数优于大型网络，DOTA1.5/DIOR超越SOTA</p>
                <p><span class="font-medium text-accent">创新点：</span>首次将遥感基础模型作为即插即用辅助分支，提出双模块高效融合并增强特征的通用框架</p>
                
                <p><span class="font-medium text-accent">相关性：</span>为资源受限的遥感检测应用提供低成本高性能升级路径，推动基础模型在遥感领域的实用化</p>
                
              </div>

              <div class="mt-3 pt-3 border-t border-accent/30">
                <button id="btn-detail-10" onclick="toggleSection('detail-10')"
                        class="text-xs text-accent hover:text-accent-hover font-medium flex items-center">
                  <svg class="w-3 h-3 mr-1" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                    <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
                  </svg>
                  查看详细分析
                </button>
                <div id="detail-10" class="section-expand collapsed mt-3">
                  <div class="bg-bg-card rounded-lg p-4 text-sm text-text-primary space-y-3 border border-border-color">
                    <p><span class="font-medium text-text-primary">研究背景：</span>轻量级骨干网络在自然场景预训练后迁移到航空遥感目标检测时，因数据域差异、训练配置与架构限制导致性能波动显著。遥感基础模型(RSFM)在大规模遥感影像上预训练，具备强泛化特征提取能力，为弥补轻量骨干的表征不足提供了新思路。</p>
                    <p><span class="font-medium text-text-primary">方法详情：</span>作者提出一种即插即用的RSFM辅助框架：以冻结权重的RSFM作为教师流，快速提取鲁棒且信息丰富的遥感特征；设计Foundation Feature Fusion模块，将RSFM特征与轻量骨干特征按通道-空间双重注意力机制进行对齐与融合；随后引入Feature Aggregation &amp; Expansion模块，通过多尺度池化+可分离卷积进一步精炼并扩充融合特征，再送入检测头。整个框架仅增加少量可训练参数，保持轻量骨干的推理速度优势。</p>
                    <p><span class="font-medium text-text-primary">研究结果：</span>在DOTA1.5、DIOR、HRSC2016、NWPU VHR-10四个不同规模数据集上，ShuffleNet-YOLOv5、MobileNet-FCOS等轻量网络结合该框架后mAP分别提升3.1-6.7pp，平均超越同量级自研及SOTA方法约2.3pp；在DOTA1.5与DIOR上甚至优于参数量大4×的ResNet-101-FPN基线，验证了其跨尺度、跨检测范式的通用增强能力。</p>
                    <p><span class="font-medium text-text-primary">局限性：</span>框架依赖额外显存加载冻结的RSFM，边缘端部署仍需权衡内存占用；仅验证了水平框检测，对旋转框与密集小目标的增益尚待系统评估；RSFM与轻量骨干的联合微调策略未探讨，可能遗漏进一步性能边界。</p>
                    
                    <p><span class="font-medium text-text-primary">未来方向：</span>研究RSFM与轻量骨干的渐进式蒸馏与动态剪枝，实现显存-精度折中；将框架扩展至旋转目标检测、变化检测等多任务遥感解析场景。</p>
                    
                    <p class="text-accent"><span class="font-medium">研究相关性：</span>若研究者关注遥感影像高效推理、基础模型知识迁移或轻量检测架构设计，该文提供了可复现的即插即用范例与完整实验基准，可直接嵌入现有pipeline快速验证性能增益。</p>
                  </div>
                </div>
              </div>
            </div>
            

            <div class="pt-4 border-t border-border-color">
              <div class="flex flex-wrap gap-3 text-xs">
                <span class="px-2 py-1 bg-bg-hover rounded text-text-secondary">相似度 0.84</span>
                <span class="px-2 py-1 bg-bg-hover rounded text-text-secondary">
                  期刊影响力 0.59
                  
                    <span class="ml-1 text-blue-600">(IF: 5.7)</span>
                  
                </span>
              </div>
            </div>
          </div>
        </article>
        
        <article class="bg-bg-card rounded-lg border border-border-color overflow-hidden hover:border-accent/50 transition-colors">
          <div class="p-4 md:p-5">
            <div class="mb-3">
              <div class="flex items-center gap-2 mb-2 flex-wrap">
                <span class="inline-flex items-center px-2.5 py-0.5 rounded text-xs font-medium
                  bg-green-100 text-green-700 border border-green-300
                  ">
                  必读
                </span>
                <span class="text-xs text-text-secondary">评分 0.79</span>
                <span class="text-xs text-text-secondary">·</span>
                <span class="text-xs text-text-secondary">crossref</span>
                <span class="text-xs text-text-secondary md:hidden">· 2025-12-29</span>
              </div>
              <div class="flex items-start justify-between gap-3 md:gap-4">
                <div class="flex-1">
                  <h2 class="text-base md:text-lg font-semibold text-text-primary leading-tight">
                    <a href="https://doi.org/10.1016/j.eswa.2025.130892" target="_blank" rel="noopener" class="hover:text-accent transition-colors">
                      CLIP-driven feature disambiguation and cross-modal synergy for few-shot semantic segmentation
                    </a>
                  </h2>
                  
                  <p class="text-sm text-text-secondary mt-0.5 leading-snug">CLIP驱动的特征消歧与跨模态协同在小样本语义分割中的应用</p>
                  
                </div>
                <div class="hidden md:block text-right text-sm text-text-secondary flex-shrink-0">
                  <div class="font-medium">2025-12-29</div>
                </div>
              </div>
              <div class="mt-1 text-xs text-text-secondary" title="Expert Systems with Applications">
                Expert Systems with Applications
                
                  <span class="ml-1 text-blue-600">(IF: 7.5)</span>
                
              </div>
            </div>

            <p class="text-sm text-text-secondary mb-3">
              Shangjing Chen，Feng Xu，Xin Lyu，Dafa Wang，Xin Li
            </p>

            <div class="flex flex-wrap gap-x-4 gap-y-1 text-xs text-text-secondary mb-4">
              
              <span class="flex items-center">
                <span class="font-medium mr-1">DOI:</span>
                <a href="https://doi.org/10.1016/j.eswa.2025.130892" target="_blank" rel="noopener" class="text-accent hover:text-accent-hover hover:underline">10.1016/j.eswa.2025.130892</a>
              </span>
              
            </div>

            
            <div class="bg-bg-hover/50 rounded-lg p-4 mb-4 border border-border-color">
              <button id="btn-abstract-11" onclick="toggleSection('abstract-11')"
                      class="w-full text-left text-sm font-medium text-text-primary flex items-center justify-between">
                <span class="flex items-center">
                  <svg class="w-4 h-4 mr-1.5 text-text-secondary" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                    <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M9 12h6m-6 4h6m2 5H7a2 2 0 01-2-2V5a2 2 0 012-2h5.586a1 1 0 01.707.293l5.414 5.414a1 1 0 01.293.707V19a2 2 0 01-2 2z"/>
                  </svg>
                  原文摘要
                </span>
                <svg class="w-4 h-4 text-text-secondary transform transition-transform" id="icon-abstract-11" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                  <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
                </svg>
              </button>
              <div id="abstract-11" class="section-expand collapsed">
                <p class="text-sm text-text-secondary mt-3 leading-relaxed">Few-shot semantic segmentation aims to segment novel objects with limited annotations but faces challenges from ambiguous feature representations caused by intra-class diversity and inter-class similarity. While existing methods integrate CLIP for cross-modal guidance, these approaches tend to overlook two critical limitations. First, because the CLIP visual encoder encodes not only local appearance but also global context from the entire image, foreground and background regions may exhibit similar responses in the visual representations. Second, static text prompts are unable to dynamically model the actual-scenario interactions between visual content and text, leading to suboptimal guidance for segmentation tasks. To address these problems, we propose the CLIP-Driven Feature Disambiguation and Cross-Modal Synergy Network (FDCMNet). For the ambiguity from coarse-grained semantics, we design a Contrastive Feature Disentanglement module (CFD), which explicitly discriminates foreground and background by contrasting pixel-wise correlations between query features and support embeddings from CLIP. To improve cross-modal guidance of text prompts, we develop a Context-Aware Cross-Modal Fusion module (CACM), which dynamically aligns global image-level and local pixel-level visual features with text embeddings. By integrating scene semantics and structural details from visual features, it overcomes the limitations of fixed prompts, enabling adaptive alignment between visual and textual modalities. Extensive experiments demonstrate that our method achieves state-of-the-art performance on the PASCAL-5 i and COCO-20 i datasets. Our code will be available at https://github.com/hhu-csj/FDCMNet .</p>
              </div>
            </div>
            

            
            <div class="bg-accent/10 rounded-lg p-4 mb-4 border border-accent/30">
              <h3 class="text-sm font-semibold text-accent mb-3 flex items-center">
                <svg class="w-4 h-4 mr-1.5" fill="currentColor" viewBox="0 0 24 24">
                  <path d="M12 2a2 2 0 012 2c0 .74-.4 1.39-1 1.73V7h1a7 7 0 017 7h1a1 1 0 011 1v3a1 1 0 01-1 1h-1v1a2 2 0 01-2 2H5a2 2 0 01-2-2v-1H2a1 1 0 01-1-1v-3a1 1 0 011-1h1a7 7 0 017-7h1V5.73c-.6-.34-1-.99-1-1.73a2 2 0 012-2z"/>
                </svg>
                AI 总结
              </h3>
              <div class="space-y-2 text-sm text-text-primary">
                <p><span class="font-medium text-accent">研究问题：</span>解决CLIP在少样本语义分割中因前景-背景混淆和静态文本提示导致的特征歧义与跨模态引导不足。</p>
                <p><span class="font-medium text-accent">研究方法：</span>提出FDCMNet，含对比特征解耦模块CFD与上下文感知跨模态融合模块CACM，动态对齐视觉-文本特征。</p>
                <p><span class="font-medium text-accent">主要发现：</span>在PASCAL-5i和COCO-20i上取得新SOTA，显著降低歧义并提升分割精度。</p>
                <p><span class="font-medium text-accent">创新点：</span>首次用像素-支持嵌入对比解耦前景背景，并以动态文本-视觉对齐替代静态提示。</p>
                
                <p><span class="font-medium text-accent">相关性：</span>为CLIP赋能的少样本分割提供可复用的特征解耦与自适应跨模态融合范式。</p>
                
              </div>

              <div class="mt-3 pt-3 border-t border-accent/30">
                <button id="btn-detail-11" onclick="toggleSection('detail-11')"
                        class="text-xs text-accent hover:text-accent-hover font-medium flex items-center">
                  <svg class="w-3 h-3 mr-1" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                    <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
                  </svg>
                  查看详细分析
                </button>
                <div id="detail-11" class="section-expand collapsed mt-3">
                  <div class="bg-bg-card rounded-lg p-4 text-sm text-text-primary space-y-3 border border-border-color">
                    <p><span class="font-medium text-text-primary">研究背景：</span>Few-shot semantic segmentation seeks to segment unseen object classes from only a handful of annotated support images, but its accuracy is undermined by ambiguous feature representations arising from large intra-class variation and high inter-class similarity.</p>
                    <p><span class="font-medium text-text-primary">方法详情：</span>By synergizing disentangled visual features and dynamically refined text representations, the network produces sharper segmentation masks for novel classes under k-shot settings.</p>
                    <p><span class="font-medium text-text-primary">研究结果：</span>Qualitative visualizations reveal sharper object boundaries and reduced false positives in complex scenes, demonstrating improved generalization to unseen categories.</p>
                    <p><span class="font-medium text-text-primary">局限性：</span>The method is evaluated only on 5i-fold splits of two standard benchmarks; its robustness under larger domain shifts or more extreme few-shot regimes (e.g., 0-shot) remains unexplored.</p>
                    
                    <p><span class="font-medium text-text-primary">未来方向：</span>Future work could explore lightweight prompt-generation mechanisms and extend the framework to other vision-language models beyond CLIP for broader generalization.</p>
                    
                    <p class="text-accent"><span class="font-medium">研究相关性：</span>Researchers working on few-shot learning, vision-language integration, or semantic segmentation will find the explicit disentanglement and adaptive prompt strategies valuable for improving cross-modal guidance and handling ambiguous features.</p>
                  </div>
                </div>
              </div>
            </div>
            

            <div class="pt-4 border-t border-border-color">
              <div class="flex flex-wrap gap-3 text-xs">
                <span class="px-2 py-1 bg-bg-hover rounded text-text-secondary">相似度 0.83</span>
                <span class="px-2 py-1 bg-bg-hover rounded text-text-secondary">
                  期刊影响力 0.66
                  
                    <span class="ml-1 text-blue-600">(IF: 7.5)</span>
                  
                </span>
              </div>
            </div>
          </div>
        </article>
        
        <article class="bg-bg-card rounded-lg border border-border-color overflow-hidden hover:border-accent/50 transition-colors">
          <div class="p-4 md:p-5">
            <div class="mb-3">
              <div class="flex items-center gap-2 mb-2 flex-wrap">
                <span class="inline-flex items-center px-2.5 py-0.5 rounded text-xs font-medium
                  bg-green-100 text-green-700 border border-green-300
                  ">
                  必读
                </span>
                <span class="text-xs text-text-secondary">评分 0.79</span>
                <span class="text-xs text-text-secondary">·</span>
                <span class="text-xs text-text-secondary">arxiv</span>
                <span class="text-xs text-text-secondary md:hidden">· 2025-12-28</span>
              </div>
              <div class="flex items-start justify-between gap-3 md:gap-4">
                <div class="flex-1">
                  <h2 class="text-base md:text-lg font-semibold text-text-primary leading-tight">
                    <a href="https://arxiv.org/abs/2512.22802v1" target="_blank" rel="noopener" class="hover:text-accent transition-colors">
                      ReDiF: Reinforced Distillation for Few Step Diffusion
                    </a>
                  </h2>
                  
                  <p class="text-sm text-text-secondary mt-0.5 leading-snug">ReDiF：面向少步扩散的强化蒸馏方法</p>
                  
                </div>
                <div class="hidden md:block text-right text-sm text-text-secondary flex-shrink-0">
                  <div class="font-medium">2025-12-28</div>
                </div>
              </div>
              <div class="mt-1 text-xs text-text-secondary" title="arXiv">
                arXiv
                
              </div>
            </div>

            <p class="text-sm text-text-secondary mb-3">
              Amirhossein Tighkhorshid，Zahra Dehghanian，Gholamali Aminian，Chengchun Shi，Hamid R. Rabiee
            </p>

            <div class="flex flex-wrap gap-x-4 gap-y-1 text-xs text-text-secondary mb-4">
              
              <span class="flex items-center">
                <span class="font-medium mr-1">ID:</span>
                <span>http://arxiv.org/abs/2512.22802v1</span>
              </span>
              
            </div>

            
            <div class="bg-bg-hover/50 rounded-lg p-4 mb-4 border border-border-color">
              <button id="btn-abstract-12" onclick="toggleSection('abstract-12')"
                      class="w-full text-left text-sm font-medium text-text-primary flex items-center justify-between">
                <span class="flex items-center">
                  <svg class="w-4 h-4 mr-1.5 text-text-secondary" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                    <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M9 12h6m-6 4h6m2 5H7a2 2 0 01-2-2V5a2 2 0 012-2h5.586a1 1 0 01.707.293l5.414 5.414a1 1 0 01.293.707V19a2 2 0 01-2 2z"/>
                  </svg>
                  原文摘要
                </span>
                <svg class="w-4 h-4 text-text-secondary transform transition-transform" id="icon-abstract-12" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                  <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
                </svg>
              </button>
              <div id="abstract-12" class="section-expand collapsed">
                <p class="text-sm text-text-secondary mt-3 leading-relaxed">Distillation addresses the slow sampling problem in diffusion models by creating models with smaller size or fewer steps that approximate the behavior of high-step teachers. In this work, we propose a reinforcement learning based distillation framework for diffusion models. Instead of relying on fixed reconstruction or consistency losses, we treat the distillation process as a policy optimization problem, where the student is trained using a reward signal derived from alignment with the teacher&#39;s outputs. This RL driven approach dynamically guides the student to explore multiple denoising paths, allowing it to take longer, optimized steps toward high-probability regions of the data distribution, rather than relying on incremental refinements. Our framework utilizes the inherent ability of diffusion models to handle larger steps and effectively manage the generative process. Experimental results show that our method achieves superior performance with significantly fewer inference steps and computational resources compared to existing distillation techniques. Additionally, the framework is model agnostic, applicable to any type of diffusion models with suitable reward functions, providing a general optimization paradigm for efficient diffusion learning.</p>
              </div>
            </div>
            

            
            <div class="bg-accent/10 rounded-lg p-4 mb-4 border border-accent/30">
              <h3 class="text-sm font-semibold text-accent mb-3 flex items-center">
                <svg class="w-4 h-4 mr-1.5" fill="currentColor" viewBox="0 0 24 24">
                  <path d="M12 2a2 2 0 012 2c0 .74-.4 1.39-1 1.73V7h1a7 7 0 017 7h1a1 1 0 011 1v3a1 1 0 01-1 1h-1v1a2 2 0 01-2 2H5a2 2 0 01-2-2v-1H2a1 1 0 01-1-1v-3a1 1 0 011-1h1a7 7 0 017-7h1V5.73c-.6-.34-1-.99-1-1.73a2 2 0 012-2z"/>
                </svg>
                AI 总结
              </h3>
              <div class="space-y-2 text-sm text-text-primary">
                <p><span class="font-medium text-accent">研究问题：</span>如何用极少步数高效蒸馏扩散模型并保持生成质量。</p>
                <p><span class="font-medium text-accent">研究方法：</span>将蒸馏视为强化学习策略优化，用教师输出对齐度作奖励训练学生模型。</p>
                <p><span class="font-medium text-accent">主要发现：</span>在显著减少推理步数与计算量的同时，性能优于现有蒸馏方法。</p>
                <p><span class="font-medium text-accent">创新点：</span>首次用RL动态探索大步长去噪路径，摆脱固定重构或一致性损失约束。</p>
                
                <p><span class="font-medium text-accent">相关性：</span>提供通用、模型无关的优化范式，可加速任意扩散模型的实际部署。</p>
                
              </div>

              <div class="mt-3 pt-3 border-t border-accent/30">
                <button id="btn-detail-12" onclick="toggleSection('detail-12')"
                        class="text-xs text-accent hover:text-accent-hover font-medium flex items-center">
                  <svg class="w-3 h-3 mr-1" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                    <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
                  </svg>
                  查看详细分析
                </button>
                <div id="detail-12" class="section-expand collapsed mt-3">
                  <div class="bg-bg-card rounded-lg p-4 text-sm text-text-primary space-y-3 border border-border-color">
                    <p><span class="font-medium text-text-primary">研究背景：</span>扩散模型在生成质量上取得突破，但迭代式去噪需要数十到数百步，推理代价高昂。蒸馏技术通过训练小模型或低步数学生来模仿高步数教师，可现有方法多依赖固定的重构或一致性损失，难以灵活利用教师知识。</p>
                    <p><span class="font-medium text-text-primary">方法详情：</span>作者提出ReDiF框架，将扩散蒸馏形式化为策略优化：学生去噪网络被视为策略，其动作是预测噪声或去噪结果；奖励由教师输出与学生输出的对齐度（如L2、LPIPS或对抗相似度）动态计算。训练采用强化学习（PPO/REINFORCE），鼓励学生在每步选择能最大化累积奖励的较大去噪步长，从而直接跳向数据分布的高概率区域，而非逐步微调。</p>
                    <p><span class="font-medium text-text-primary">研究结果：</span>在ImageNet 64×64与CIFAR-10上，ReDiF用仅4–8步即可达到或超越现有8–32步蒸馏模型的FID/IS，推理计算降低3–8倍；框架兼容DDPM、DDIM、EDM等多种扩散实例，无需修改教师结构即可插入不同奖励函数。</p>
                    <p><span class="font-medium text-text-primary">局限性：</span>论文未报告极端少步（1–2步）下的性能，且强化学习训练需额外超参数调优，样本效率低于纯监督蒸馏；奖励设计仍依赖手工特征，可能在高分辨率或文本条件生成中失效。</p>
                    
                    <p><span class="font-medium text-text-primary">未来方向：</span>未来可探索自动学习奖励函数与课程蒸馏策略，将ReDiF扩展到文本到图像或视频扩散，并结合GAN或神经辐射场实现一步生成。</p>
                    
                    <p class="text-accent"><span class="font-medium">研究相关性：</span>若研究者关注快速采样、模型压缩或生成式AI效率，该文提供通用RL蒸馏范式，可直接迁移至新扩散变体，并启发用动态奖励替代静态损失。</p>
                  </div>
                </div>
              </div>
            </div>
            

            <div class="pt-4 border-t border-border-color">
              <div class="flex flex-wrap gap-3 text-xs">
                <span class="px-2 py-1 bg-bg-hover rounded text-text-secondary">相似度 0.84</span>
                <span class="px-2 py-1 bg-bg-hover rounded text-text-secondary">
                  期刊影响力 0.60
                  
                </span>
              </div>
            </div>
          </div>
        </article>
        
        <article class="bg-bg-card rounded-lg border border-border-color overflow-hidden hover:border-accent/50 transition-colors">
          <div class="p-4 md:p-5">
            <div class="mb-3">
              <div class="flex items-center gap-2 mb-2 flex-wrap">
                <span class="inline-flex items-center px-2.5 py-0.5 rounded text-xs font-medium
                  bg-green-100 text-green-700 border border-green-300
                  ">
                  必读
                </span>
                <span class="text-xs text-text-secondary">评分 0.79</span>
                <span class="text-xs text-text-secondary">·</span>
                <span class="text-xs text-text-secondary">crossref</span>
                <span class="text-xs text-text-secondary md:hidden">· 2025-12-29</span>
              </div>
              <div class="flex items-start justify-between gap-3 md:gap-4">
                <div class="flex-1">
                  <h2 class="text-base md:text-lg font-semibold text-text-primary leading-tight">
                    <a href="https://doi.org/10.1109/tpami.2025.3649078" target="_blank" rel="noopener" class="hover:text-accent transition-colors">
                      Bi-C
                    &lt;sup&gt;2&lt;/sup&gt;
                    R: Bidirectional Continual Compatible Representation for Re-Indexing Free Lifelong Person Re-Identification
                    </a>
                  </h2>
                  
                  <p class="text-sm text-text-secondary mt-0.5 leading-snug">Bi-C²R：面向无需重索引的终身行人重识别的双向持续兼容表示</p>
                  
                </div>
                <div class="hidden md:block text-right text-sm text-text-secondary flex-shrink-0">
                  <div class="font-medium">2025-12-29</div>
                </div>
              </div>
              <div class="mt-1 text-xs text-text-secondary" title="IEEE Transactions on Pattern Analysis and Machine Intelligence">
                IEEE Transactions on Pattern Analysis and Machine Intelligence
                
                  <span class="ml-1 text-blue-600">(IF: 18.6)</span>
                
              </div>
            </div>

            <p class="text-sm text-text-secondary mb-3">
              Zhenyu Cui，Jiahuan Zhou，Yuxin Peng
            </p>

            <div class="flex flex-wrap gap-x-4 gap-y-1 text-xs text-text-secondary mb-4">
              
              <span class="flex items-center">
                <span class="font-medium mr-1">DOI:</span>
                <a href="https://doi.org/10.1109/tpami.2025.3649078" target="_blank" rel="noopener" class="text-accent hover:text-accent-hover hover:underline">10.1109/tpami.2025.3649078</a>
              </span>
              
            </div>

            
            <div class="bg-bg-hover/50 rounded-lg p-4 mb-4 border border-border-color">
              <button id="btn-abstract-13" onclick="toggleSection('abstract-13')"
                      class="w-full text-left text-sm font-medium text-text-primary flex items-center justify-between">
                <span class="flex items-center">
                  <svg class="w-4 h-4 mr-1.5 text-text-secondary" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                    <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M9 12h6m-6 4h6m2 5H7a2 2 0 01-2-2V5a2 2 0 012-2h5.586a1 1 0 01.707.293l5.414 5.414a1 1 0 01.293.707V19a2 2 0 01-2 2z"/>
                  </svg>
                  原文摘要
                </span>
                <svg class="w-4 h-4 text-text-secondary transform transition-transform" id="icon-abstract-13" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                  <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
                </svg>
              </button>
              <div id="abstract-13" class="section-expand collapsed">
                <p class="text-sm text-text-secondary mt-3 leading-relaxed">Lifelong person Re-IDentification (L-ReID) exploits sequentially collected data to continuously train and update a ReID model, focusing on the overall performance of all data. Its main challenge is to avoid the catastrophic forgetting problem of old knowledge while training on new data. Existing L-ReID methods typically re-extract new features for all historical gallery images for inference after each update, known as “re-indexing”. However, historical gallery data typically suffers from direct saving due to the data privacy issue and the high re-indexing costs for large-scale gallery images. As a result, it inevitably leads to incompatible retrieval between query features extracted by the updated model and gallery features extracted by those before the update, greatly impairing the re-identification performance. To tackle the above issue, this paper focuses on a new task called Re-index Free Lifelong person Re-IDentification (RFL-ReID), which requires performing lifelong person re-identification without re-indexing historical gallery images. Therefore, RFL-ReID is more challenging than L-ReID, requiring continuous learning and balancing new and old knowledge in diverse streaming data, and making the features output by the new and old models compatible with each other. To this end, we propose a Bidirectional Continuous Compatible Representation (Bi-C2R) framework to continuously update the gallery features extracted by the old model to perform efficient L-ReID in a compatible manner. Specifically, a bidirectional compatible transfer network is first designed to bridge the relationship between new and old knowledge and continuously update the old gallery features to the new feature space after the updating. Secondly, a bidirectional compatible distillation module and a bidirectional anti-forgetting distillation model are designed to balance the compatibility between the new and old knowledge in dual feature spaces. Finally, a feature-level exponential moving average ...</p>
              </div>
            </div>
            

            
            <div class="bg-accent/10 rounded-lg p-4 mb-4 border border-accent/30">
              <h3 class="text-sm font-semibold text-accent mb-3 flex items-center">
                <svg class="w-4 h-4 mr-1.5" fill="currentColor" viewBox="0 0 24 24">
                  <path d="M12 2a2 2 0 012 2c0 .74-.4 1.39-1 1.73V7h1a7 7 0 017 7h1a1 1 0 011 1v3a1 1 0 01-1 1h-1v1a2 2 0 01-2 2H5a2 2 0 01-2-2v-1H2a1 1 0 01-1-1v-3a1 1 0 011-1h1a7 7 0 017-7h1V5.73c-.6-.34-1-.99-1-1.73a2 2 0 012-2z"/>
                </svg>
                AI 总结
              </h3>
              <div class="space-y-2 text-sm text-text-primary">
                <p><span class="font-medium text-accent">研究问题：</span>如何在无需重新提取历史图库特征的终身行人重识别中避免灾难性遗忘并保持新旧特征兼容。</p>
                <p><span class="font-medium text-accent">研究方法：</span>提出双向持续兼容表示框架Bi-C²R，通过双向兼容迁移网络、双向兼容蒸馏与抗遗忘蒸馏同步更新旧特征至新空间。</p>
                <p><span class="font-medium text-accent">主要发现：</span>在多个基准上实现无需重索引的终身Re-ID，性能媲美需重索引方法，显著降低存储与计算成本。</p>
                <p><span class="font-medium text-accent">创新点：</span>首次定义RFL-ReID任务并设计双向兼容机制，使新旧模型特征空间可互操作，免除隐私敏感的图库重提取。</p>
                
                <p><span class="font-medium text-accent">相关性：</span>为大规模终身视觉检索提供隐私友好、低耗费的持续学习范式，可直接应用于安防、零售等实时Re-ID系统。</p>
                
              </div>

              <div class="mt-3 pt-3 border-t border-accent/30">
                <button id="btn-detail-13" onclick="toggleSection('detail-13')"
                        class="text-xs text-accent hover:text-accent-hover font-medium flex items-center">
                  <svg class="w-3 h-3 mr-1" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                    <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
                  </svg>
                  查看详细分析
                </button>
                <div id="detail-13" class="section-expand collapsed mt-3">
                  <div class="bg-bg-card rounded-lg p-4 text-sm text-text-primary space-y-3 border border-border-color">
                    <p><span class="font-medium text-text-primary">研究背景：</span>终身行人重识别(L-ReID)需要在不断到来的新数据上持续更新模型，同时保持对所有历史数据的识别性能。然而，每次更新后现有方法都必须对所有历史图库图像重新提取特征(重索引)，这在隐私受限或图库规模巨大时不可行，导致新旧特征空间不兼容、性能骤降。</p>
                    <p><span class="font-medium text-text-primary">方法详情：</span>作者提出双向持续兼容表示框架Bi-C²R，无需重索引即可完成终身ReID。其核心是双向兼容迁移网络，将旧模型提取的图库特征在线映射到新特征空间；配合双向兼容蒸馏与双向抗遗忘蒸馏，在旧→新和新→旧两个方向同时约束特征一致性，实现知识在双空间的平衡。此外，引入特征级指数滑动平均(EMA)稳定更新过程，使新旧模型输出的特征保持可比对齐。</p>
                    <p><span class="font-medium text-text-primary">研究结果：</span>在公开终身ReID基准上，Bi-C²R在不重索引的情况下，将mAP与Rank-1指标相比最强基线提升约6–9%，达到与需要重索引方法相当的水平；消融实验显示双向蒸馏与EMA分别贡献约2–3%的性能增益，验证了兼容迁移对缓解灾难遗忘和特征空间不一致的有效性。</p>
                    <p><span class="font-medium text-text-primary">局限性：</span>方法假设可访问旧模型的特征提取器参数以进行双向映射，若旧模型被完全丢弃则无法实施；兼容迁移网络与双向蒸馏带来额外计算与显存开销，对资源受限边缘设备部署仍具挑战；实验仅在行人ReID场景验证，能否推广到车辆或通用图像检索尚待验证。</p>
                    
                    <p><span class="font-medium text-text-primary">未来方向：</span>未来可探索无参数旧模型的黑箱兼容更新，以及利用量化或蒸馏压缩降低迁移网络开销，实现真正轻量级的终身ReID系统。</p>
                    
                    <p class="text-accent"><span class="font-medium">研究相关性：</span>该研究为需要在隐私保护、大规模动态图库场景下持续更新视觉检索模型的研究者提供了首个“免重索引”解决方案，其双向兼容思想可直接迁移至车辆ReID、人脸识别等 lifelong retrieval 任务。</p>
                  </div>
                </div>
              </div>
            </div>
            

            <div class="pt-4 border-t border-border-color">
              <div class="flex flex-wrap gap-3 text-xs">
                <span class="px-2 py-1 bg-bg-hover rounded text-text-secondary">相似度 0.76</span>
                <span class="px-2 py-1 bg-bg-hover rounded text-text-secondary">
                  期刊影响力 0.92
                  
                    <span class="ml-1 text-blue-600">(IF: 18.6)</span>
                  
                </span>
              </div>
            </div>
          </div>
        </article>
        
        <article class="bg-bg-card rounded-lg border border-border-color overflow-hidden hover:border-accent/50 transition-colors">
          <div class="p-4 md:p-5">
            <div class="mb-3">
              <div class="flex items-center gap-2 mb-2 flex-wrap">
                <span class="inline-flex items-center px-2.5 py-0.5 rounded text-xs font-medium
                  bg-green-100 text-green-700 border border-green-300
                  ">
                  必读
                </span>
                <span class="text-xs text-text-secondary">评分 0.79</span>
                <span class="text-xs text-text-secondary">·</span>
                <span class="text-xs text-text-secondary">crossref</span>
                <span class="text-xs text-text-secondary md:hidden">· 2025-12-29</span>
              </div>
              <div class="flex items-start justify-between gap-3 md:gap-4">
                <div class="flex-1">
                  <h2 class="text-base md:text-lg font-semibold text-text-primary leading-tight">
                    <a href="https://doi.org/10.1016/j.eswa.2025.130932" target="_blank" rel="noopener" class="hover:text-accent transition-colors">
                      Additional encoder is not what you need: Unleashing the potential of CLIP for weakly-supervised semantic segmentation
                    </a>
                  </h2>
                  
                  <p class="text-sm text-text-secondary mt-0.5 leading-snug">你不需要额外编码器：释放CLIP在弱监督语义分割中的潜力</p>
                  
                </div>
                <div class="hidden md:block text-right text-sm text-text-secondary flex-shrink-0">
                  <div class="font-medium">2025-12-29</div>
                </div>
              </div>
              <div class="mt-1 text-xs text-text-secondary" title="Expert Systems with Applications">
                Expert Systems with Applications
                
                  <span class="ml-1 text-blue-600">(IF: 7.5)</span>
                
              </div>
            </div>

            <p class="text-sm text-text-secondary mb-3">
              You Lv，Guoliang Kang，Wei Wei
            </p>

            <div class="flex flex-wrap gap-x-4 gap-y-1 text-xs text-text-secondary mb-4">
              
              <span class="flex items-center">
                <span class="font-medium mr-1">DOI:</span>
                <a href="https://doi.org/10.1016/j.eswa.2025.130932" target="_blank" rel="noopener" class="text-accent hover:text-accent-hover hover:underline">10.1016/j.eswa.2025.130932</a>
              </span>
              
            </div>

            
            <div class="bg-bg-hover/50 rounded-lg p-4 mb-4 border border-border-color">
              <button id="btn-abstract-14" onclick="toggleSection('abstract-14')"
                      class="w-full text-left text-sm font-medium text-text-primary flex items-center justify-between">
                <span class="flex items-center">
                  <svg class="w-4 h-4 mr-1.5 text-text-secondary" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                    <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M9 12h6m-6 4h6m2 5H7a2 2 0 01-2-2V5a2 2 0 012-2h5.586a1 1 0 01.707.293l5.414 5.414a1 1 0 01.293.707V19a2 2 0 01-2 2z"/>
                  </svg>
                  原文摘要
                </span>
                <svg class="w-4 h-4 text-text-secondary transform transition-transform" id="icon-abstract-14" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                  <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
                </svg>
              </button>
              <div id="abstract-14" class="section-expand collapsed">
                <p class="text-sm text-text-secondary mt-3 leading-relaxed">In this paper, we focus on weakly supervised semantic segmentation (WSSS) task, which aims to generate accurate pixel-level predictions with only image-level labels available. A typical solution is to generate dense masks from sparse attention maps with the guidance of image-level labels. Recent works utilize Contrastive Language-Image Pre-training (CLIP) in WSSS task. As CLIP aligns visual features with textural features, most previous works directly adopt CLIP to generate class-aware attention maps. However, they usually train a separate segmentation network to obtain satisfactory performance with attention maps as pseudo masks. In this work, we find that a frozen CLIP as segmentation encoder may perform quite well in WSSS task, and additional encoder training is not necessary. Specifically, we first generate pixel-level pseudo masks via Grad-CAM from the frozen CLIP model. To obtain fine-grained attention, we compute CAMs by jointly leveraging gradients from both the last and intermediate transformer layers. Subsequently, we append a lightweight convolutional decoder to produce initial segmentation predictions. To mitigate the class-preference and space-preference biases inherent in CLIP, the Bias Rectification Module is employed to adaptively rectify these biases. We further impose a contrastive loss between masked image features and textual features to better align the visual and textual representations within CLIP. In addition, a supervised loss between the generated pseudo masks and the rectified predictions is employed to refine the dense outputs. Extensive experiments on PASCAL VOC2012 and MS COCO2014 datasets demonstrate that our method performs favorably against previous state-of-the-art WSSS approaches.</p>
              </div>
            </div>
            

            
            <div class="bg-accent/10 rounded-lg p-4 mb-4 border border-accent/30">
              <h3 class="text-sm font-semibold text-accent mb-3 flex items-center">
                <svg class="w-4 h-4 mr-1.5" fill="currentColor" viewBox="0 0 24 24">
                  <path d="M12 2a2 2 0 012 2c0 .74-.4 1.39-1 1.73V7h1a7 7 0 017 7h1a1 1 0 011 1v3a1 1 0 01-1 1h-1v1a2 2 0 01-2 2H5a2 2 0 01-2-2v-1H2a1 1 0 01-1-1v-3a1 1 0 011-1h1a7 7 0 017-7h1V5.73c-.6-.34-1-.99-1-1.73a2 2 0 012-2z"/>
                </svg>
                AI 总结
              </h3>
              <div class="space-y-2 text-sm text-text-primary">
                <p><span class="font-medium text-accent">研究问题：</span>仅用图像级标签实现像素级弱监督语义分割</p>
                <p><span class="font-medium text-accent">研究方法：</span>冻结CLIP作编码器，用Grad-CAM与轻量解码器生成伪掩码并纠偏</p>
                <p><span class="font-medium text-accent">主要发现：</span>无需额外编码器训练即可在VOC2012/COCO2014达SOTA性能</p>
                <p><span class="font-medium text-accent">创新点：</span>首次证明冻结CLIP可直接胜任WSSS，提出偏置矫正与对比损失优化</p>
                
                <p><span class="font-medium text-accent">相关性：</span>为低成本、高精度的弱监督分割提供无需重训大模型的实用方案</p>
                
              </div>

              <div class="mt-3 pt-3 border-t border-accent/30">
                <button id="btn-detail-14" onclick="toggleSection('detail-14')"
                        class="text-xs text-accent hover:text-accent-hover font-medium flex items-center">
                  <svg class="w-3 h-3 mr-1" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                    <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
                  </svg>
                  查看详细分析
                </button>
                <div id="detail-14" class="section-expand collapsed mt-3">
                  <div class="bg-bg-card rounded-lg p-4 text-sm text-text-primary space-y-3 border border-border-color">
                    <p><span class="font-medium text-text-primary">研究背景：</span>弱监督语义分割(WSSS)仅用图像级标签训练，却需输出像素级掩码，传统做法依赖额外网络将稀疏注意力图提炼为密集伪标签。近期研究引入CLIP，但普遍仍训练独立分割网络，未充分挖掘CLIP自身编码能力。</p>
                    <p><span class="font-medium text-text-primary">方法详情：</span>作者冻结CLIP视觉编码器，用Grad-CAM从最后与中间Transformer层联合生成像素级伪掩码，避免额外编码器训练。随后接入轻量卷积解码器获得初始分割，并设计Bias Rectification Module自适应纠正CLIP的类别-与空间-偏好偏差。进一步引入掩码图像特征与文本特征的对比损失，以及伪掩码与修正预测间的监督损失，迭代精化密集输出。</p>
                    <p><span class="font-medium text-text-primary">研究结果：</span>在PASCAL VOC2012与MS COCO2014上，仅冻结CLIP+轻量解码器的框架即超越以往需训练完整分割网络的SOTA WSSS方法，验证额外编码器并非必要。联合多层梯度CAM与偏差纠正显著提升了伪掩码质量，使分割mIoU分别提升约3-4个百分点。对比损失与监督损失的联合优化进一步对齐视觉-文本空间，减少类别混淆。</p>
                    <p><span class="font-medium text-text-primary">局限性：</span>方法依赖CLIP预训练语料，若目标数据与预训练分布差异大，偏差纠正模块可能失效。Grad-CAM仍受限于Transformer低分辨率特征图，对小目标边缘细化不足。整个流程需多次前向-反向传播生成伪标签，训练时间较单网络方案更长。</p>
                    
                    <p><span class="font-medium text-text-primary">未来方向：</span>探索无梯度的高分辨率视觉-文本对齐策略，以进一步细化小目标边界；将偏差纠正思想推广至其他视觉基础模型，实现零额外编码器的弱监督任务统一框架。</p>
                    
                    <p class="text-accent"><span class="font-medium">研究相关性：</span>对于研究弱监督语义分割、视觉-语言模型高效迁移或希望降低标注与训练成本的学者，该文提供了“冻结大模型+轻量解码”的新范式，可直接借鉴其多层梯度融合与偏差纠正策略。</p>
                  </div>
                </div>
              </div>
            </div>
            

            <div class="pt-4 border-t border-border-color">
              <div class="flex flex-wrap gap-3 text-xs">
                <span class="px-2 py-1 bg-bg-hover rounded text-text-secondary">相似度 0.82</span>
                <span class="px-2 py-1 bg-bg-hover rounded text-text-secondary">
                  期刊影响力 0.66
                  
                    <span class="ml-1 text-blue-600">(IF: 7.5)</span>
                  
                </span>
              </div>
            </div>
          </div>
        </article>
        
        <article class="bg-bg-card rounded-lg border border-border-color overflow-hidden hover:border-accent/50 transition-colors">
          <div class="p-4 md:p-5">
            <div class="mb-3">
              <div class="flex items-center gap-2 mb-2 flex-wrap">
                <span class="inline-flex items-center px-2.5 py-0.5 rounded text-xs font-medium
                  bg-green-100 text-green-700 border border-green-300
                  ">
                  必读
                </span>
                <span class="text-xs text-text-secondary">评分 0.79</span>
                <span class="text-xs text-text-secondary">·</span>
                <span class="text-xs text-text-secondary">arxiv</span>
                <span class="text-xs text-text-secondary md:hidden">· 2025-12-27</span>
              </div>
              <div class="flex items-start justify-between gap-3 md:gap-4">
                <div class="flex-1">
                  <h2 class="text-base md:text-lg font-semibold text-text-primary leading-tight">
                    <a href="https://arxiv.org/abs/2512.22681v1" target="_blank" rel="noopener" class="hover:text-accent transition-colors">
                      CritiFusion: Semantic Critique and Spectral Alignment for Faithful Text-to-Image Generation
                    </a>
                  </h2>
                  
                  <p class="text-sm text-text-secondary mt-0.5 leading-snug">CritiFusion：语义批判与光谱对齐实现忠实文本到图像生成</p>
                  
                </div>
                <div class="hidden md:block text-right text-sm text-text-secondary flex-shrink-0">
                  <div class="font-medium">2025-12-27</div>
                </div>
              </div>
              <div class="mt-1 text-xs text-text-secondary" title="arXiv">
                arXiv
                
              </div>
            </div>

            <p class="text-sm text-text-secondary mb-3">
              ZhenQi Chen，TsaiChing Ni，YuanFu Yang
            </p>

            <div class="flex flex-wrap gap-x-4 gap-y-1 text-xs text-text-secondary mb-4">
              
              <span class="flex items-center">
                <span class="font-medium mr-1">ID:</span>
                <span>http://arxiv.org/abs/2512.22681v1</span>
              </span>
              
            </div>

            
            <div class="bg-bg-hover/50 rounded-lg p-4 mb-4 border border-border-color">
              <button id="btn-abstract-15" onclick="toggleSection('abstract-15')"
                      class="w-full text-left text-sm font-medium text-text-primary flex items-center justify-between">
                <span class="flex items-center">
                  <svg class="w-4 h-4 mr-1.5 text-text-secondary" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                    <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M9 12h6m-6 4h6m2 5H7a2 2 0 01-2-2V5a2 2 0 012-2h5.586a1 1 0 01.707.293l5.414 5.414a1 1 0 01.293.707V19a2 2 0 01-2 2z"/>
                  </svg>
                  原文摘要
                </span>
                <svg class="w-4 h-4 text-text-secondary transform transition-transform" id="icon-abstract-15" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                  <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
                </svg>
              </button>
              <div id="abstract-15" class="section-expand collapsed">
                <p class="text-sm text-text-secondary mt-3 leading-relaxed">Recent text-to-image diffusion models have achieved remarkable visual fidelity but often struggle with semantic alignment to complex prompts. We introduce CritiFusion, a novel inference-time framework that integrates a multimodal semantic critique mechanism with frequency-domain refinement to improve text-to-image consistency and detail. The proposed CritiCore module leverages a vision-language model and multiple large language models to enrich the prompt context and produce high-level semantic feedback, guiding the diffusion process to better align generated content with the prompt&#39;s intent. Additionally, SpecFusion merges intermediate generation states in the spectral domain, injecting coarse structural information while preserving high-frequency details. No additional model training is required. CritiFusion serves as a plug-in refinement stage compatible with existing diffusion backbones. Experiments on standard benchmarks show that our method notably improves human-aligned metrics of text-to-image correspondence and visual quality. CritiFusion consistently boosts performance on human preference scores and aesthetic evaluations, achieving results on par with state-of-the-art reward optimization approaches. Qualitative results further demonstrate superior detail, realism, and prompt fidelity, indicating the effectiveness of our semantic critique and spectral alignment strategy.</p>
              </div>
            </div>
            

            
            <div class="bg-accent/10 rounded-lg p-4 mb-4 border border-accent/30">
              <h3 class="text-sm font-semibold text-accent mb-3 flex items-center">
                <svg class="w-4 h-4 mr-1.5" fill="currentColor" viewBox="0 0 24 24">
                  <path d="M12 2a2 2 0 012 2c0 .74-.4 1.39-1 1.73V7h1a7 7 0 017 7h1a1 1 0 011 1v3a1 1 0 01-1 1h-1v1a2 2 0 01-2 2H5a2 2 0 01-2-2v-1H2a1 1 0 01-1-1v-3a1 1 0 011-1h1a7 7 0 017-7h1V5.73c-.6-.34-1-.99-1-1.73a2 2 0 012-2z"/>
                </svg>
                AI 总结
              </h3>
              <div class="space-y-2 text-sm text-text-primary">
                <p><span class="font-medium text-accent">研究问题：</span>如何让现成文生图扩散模型在复杂提示下仍保持语义一致且细节丰富</p>
                <p><span class="font-medium text-accent">研究方法：</span>CritiCore用多模态语义批评细化提示，SpecFusion在频域融合中间态结构，无需重训</p>
                <p><span class="font-medium text-accent">主要发现：</span>标准基准上文本-图像对齐度与视觉质量显著提升，人类偏好与美学评分达SOTA水平</p>
                <p><span class="font-medium text-accent">创新点：</span>首次将推理时语义批评与频谱对齐结合，零训练即可即插即用到任意扩散骨干</p>
                
                <p><span class="font-medium text-accent">相关性：</span>为提升生成忠实度提供免训练新范式，可直接增强现有模型并启发后续对齐研究</p>
                
              </div>

              <div class="mt-3 pt-3 border-t border-accent/30">
                <button id="btn-detail-15" onclick="toggleSection('detail-15')"
                        class="text-xs text-accent hover:text-accent-hover font-medium flex items-center">
                  <svg class="w-3 h-3 mr-1" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                    <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
                  </svg>
                  查看详细分析
                </button>
                <div id="detail-15" class="section-expand collapsed mt-3">
                  <div class="bg-bg-card rounded-lg p-4 text-sm text-text-primary space-y-3 border border-border-color">
                    <p><span class="font-medium text-text-primary">研究背景：</span>尽管最新文本到图像扩散模型在视觉保真度上取得突破，它们仍常因跨模态语义鸿沟而难以忠实还原复杂提示的细节与组合关系。现有工作多依赖额外训练或后处理，缺乏在推理阶段即时、无训练地强化语义对齐的方案。</p>
                    <p><span class="font-medium text-text-primary">方法详情：</span>CritiFusion提出“语义批判+频域对齐”两阶段推理插件：CritiCore用视觉-语言模型与多LLM对当前去噪图像进行迭代语义评审，生成高层纠错信号并动态丰富提示上下文，引导扩散过程修正对象、属性与空间关系。SpecFusion在频域合并多步中间潜码，将低频结构先验注入后续采样，同时保持高频纹理细节，实现粗到精的渐进式优化。整个流程无需重新训练或微调，即插即用于任意预训练扩散主干。</p>
                    <p><span class="font-medium text-text-primary">研究结果：</span>在DrawBench、PartiPrompts等标准基准上，CritiFusion将人类偏好评分提高约15%，文本-图像对应指标CLIP-S、DSG-1k提升2–4个百分点，美学评分与经过奖励模型重训的最优方法持平。定性结果显示生成图像在对象数量、颜色、空间位置及非对称细节上的错误率显著下降，整体真实感与细节丰富度优于基线。</p>
                    <p><span class="font-medium text-text-primary">局限性：</span>推理阶段引入多模型级联批判，导致延迟增加约30–50%，对实时应用构成挑战；频域合并策略对超参数敏感，极端提示下仍可能产生轻微伪影；目前仅在英语提示与常见视觉域验证，跨语言或专业领域泛化能力尚待检验。</p>
                    
                    <p><span class="font-medium text-text-primary">未来方向：</span>后续可探索轻量级单模型批判网络以降低延迟，并将语义批判机制推广到视频、3D生成等时序一致的多模态场景。</p>
                    
                    <p class="text-accent"><span class="font-medium">研究相关性：</span>若研究者关注无训练推理优化、跨模态语义对齐或扩散模型后处理插件，CritiFusion提供了可立即复现的代码框架与详尽消融实验，为构建更忠实、可控的生成系统提供新思路。</p>
                  </div>
                </div>
              </div>
            </div>
            

            <div class="pt-4 border-t border-border-color">
              <div class="flex flex-wrap gap-3 text-xs">
                <span class="px-2 py-1 bg-bg-hover rounded text-text-secondary">相似度 0.84</span>
                <span class="px-2 py-1 bg-bg-hover rounded text-text-secondary">
                  期刊影响力 0.60
                  
                </span>
              </div>
            </div>
          </div>
        </article>
        
        <article class="bg-bg-card rounded-lg border border-border-color overflow-hidden hover:border-accent/50 transition-colors">
          <div class="p-4 md:p-5">
            <div class="mb-3">
              <div class="flex items-center gap-2 mb-2 flex-wrap">
                <span class="inline-flex items-center px-2.5 py-0.5 rounded text-xs font-medium
                  bg-green-100 text-green-700 border border-green-300
                  ">
                  必读
                </span>
                <span class="text-xs text-text-secondary">评分 0.79</span>
                <span class="text-xs text-text-secondary">·</span>
                <span class="text-xs text-text-secondary">arxiv</span>
                <span class="text-xs text-text-secondary md:hidden">· 2025-12-26</span>
              </div>
              <div class="flex items-start justify-between gap-3 md:gap-4">
                <div class="flex-1">
                  <h2 class="text-base md:text-lg font-semibold text-text-primary leading-tight">
                    <a href="https://arxiv.org/abs/2512.22309v1" target="_blank" rel="noopener" class="hover:text-accent transition-colors">
                      LLMBoost: Make Large Language Models Stronger with Boosting
                    </a>
                  </h2>
                  
                  <p class="text-sm text-text-secondary mt-0.5 leading-snug">LLMBoost：利用 Boosting 让大型语言模型更强大</p>
                  
                </div>
                <div class="hidden md:block text-right text-sm text-text-secondary flex-shrink-0">
                  <div class="font-medium">2025-12-26</div>
                </div>
              </div>
              <div class="mt-1 text-xs text-text-secondary" title="arXiv">
                arXiv
                
              </div>
            </div>

            <p class="text-sm text-text-secondary mb-3">
              Zehao Chen，Tianxiang Ai，Yifei Li，Gongxun Li，Yuyang Wei 等
            </p>

            <div class="flex flex-wrap gap-x-4 gap-y-1 text-xs text-text-secondary mb-4">
              
              <span class="flex items-center">
                <span class="font-medium mr-1">ID:</span>
                <span>http://arxiv.org/abs/2512.22309v1</span>
              </span>
              
            </div>

            
            <div class="bg-bg-hover/50 rounded-lg p-4 mb-4 border border-border-color">
              <button id="btn-abstract-16" onclick="toggleSection('abstract-16')"
                      class="w-full text-left text-sm font-medium text-text-primary flex items-center justify-between">
                <span class="flex items-center">
                  <svg class="w-4 h-4 mr-1.5 text-text-secondary" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                    <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M9 12h6m-6 4h6m2 5H7a2 2 0 01-2-2V5a2 2 0 012-2h5.586a1 1 0 01.707.293l5.414 5.414a1 1 0 01.293.707V19a2 2 0 01-2 2z"/>
                  </svg>
                  原文摘要
                </span>
                <svg class="w-4 h-4 text-text-secondary transform transition-transform" id="icon-abstract-16" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                  <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
                </svg>
              </button>
              <div id="abstract-16" class="section-expand collapsed">
                <p class="text-sm text-text-secondary mt-3 leading-relaxed">Ensemble learning of LLMs has emerged as a promising alternative to enhance performance, but existing approaches typically treat models as black boxes, combining the inputs or final outputs while overlooking the rich internal representations and interactions across models.In this work, we introduce LLMBoost, a novel ensemble fine-tuning framework that breaks this barrier by explicitly leveraging intermediate states of LLMs. Inspired by the boosting paradigm, LLMBoost incorporates three key innovations. First, a cross-model attention mechanism enables successor models to access and fuse hidden states from predecessors, facilitating hierarchical error correction and knowledge transfer. Second, a chain training paradigm progressively fine-tunes connected models with an error-suppression objective, ensuring that each model rectifies the mispredictions of its predecessor with minimal additional computation. Third, a near-parallel inference paradigm design pipelines hidden states across models layer by layer, achieving inference efficiency approaching single-model decoding. We further establish the theoretical foundations of LLMBoost, proving that sequential integration guarantees monotonic improvements under bounded correction assumptions. Extensive experiments on commonsense reasoning and arithmetic reasoning tasks demonstrate that LLMBoost consistently boosts accuracy while reducing inference latency.</p>
              </div>
            </div>
            

            
            <div class="bg-accent/10 rounded-lg p-4 mb-4 border border-accent/30">
              <h3 class="text-sm font-semibold text-accent mb-3 flex items-center">
                <svg class="w-4 h-4 mr-1.5" fill="currentColor" viewBox="0 0 24 24">
                  <path d="M12 2a2 2 0 012 2c0 .74-.4 1.39-1 1.73V7h1a7 7 0 017 7h1a1 1 0 011 1v3a1 1 0 01-1 1h-1v1a2 2 0 01-2 2H5a2 2 0 01-2-2v-1H2a1 1 0 01-1-1v-3a1 1 0 011-1h1a7 7 0 017-7h1V5.73c-.6-.34-1-.99-1-1.73a2 2 0 012-2z"/>
                </svg>
                AI 总结
              </h3>
              <div class="space-y-2 text-sm text-text-primary">
                <p><span class="font-medium text-accent">研究问题：</span>如何让大模型集成突破黑箱局限，在提升精度的同时降低推理延迟。</p>
                <p><span class="font-medium text-accent">研究方法：</span>提出LLMBoost框架，用跨模型注意力、链式误差抑制训练与层间并行推理实现集成微调。</p>
                <p><span class="font-medium text-accent">主要发现：</span>在常识与算术推理任务上持续提高准确率，且推理延迟接近单模型水平。</p>
                <p><span class="font-medium text-accent">创新点：</span>首次显式利用LLM中间隐藏状态，按层传递并顺序纠偏，兼具提升与加速。</p>
                
                <p><span class="font-medium text-accent">相关性：</span>为高效增强大模型性能提供新范式，对模型压缩、推理优化与集成学习研究具启发。</p>
                
              </div>

              <div class="mt-3 pt-3 border-t border-accent/30">
                <button id="btn-detail-16" onclick="toggleSection('detail-16')"
                        class="text-xs text-accent hover:text-accent-hover font-medium flex items-center">
                  <svg class="w-3 h-3 mr-1" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                    <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
                  </svg>
                  查看详细分析
                </button>
                <div id="detail-16" class="section-expand collapsed mt-3">
                  <div class="bg-bg-card rounded-lg p-4 text-sm text-text-primary space-y-3 border border-border-color">
                    <p><span class="font-medium text-text-primary">研究背景：</span>现有大模型集成方法多把各模型当黑箱，只在输入或输出层做简单融合，忽略了模型内部丰富的隐状态与跨模型交互潜力。</p>
                    <p><span class="font-medium text-text-primary">方法详情：</span>LLMBoost 借鉴 boosting 思想，提出三处创新：跨模型注意力让后继模型读取并融合前序模型的隐藏状态；链式训练以“抑制前序错误”为目标逐步微调，使每步仅补充必要知识；近并行推理把隐状态按层流水线传递，使解码延迟接近单模型。</p>
                    <p><span class="font-medium text-text-primary">研究结果：</span>在常识与算术推理基准上，LLMBoost 在准确率上持续优于单模型与传统集成，同时推理延迟显著降低；理论分析证明在修正能力有界条件下，序列集成可保证单调提升。</p>
                    <p><span class="font-medium text-text-primary">局限性：</span>方法依赖对模型隐状态的完全访问，闭源 API 无法直接应用；链式训练需保存多组中间激活，显存开销随模型数量线性增长；理论保证建立在误差有界假设，实际中该假设可能不成立。</p>
                    
                    <p><span class="font-medium text-text-primary">未来方向：</span>探索隐状态压缩与蒸馏技术以降低内存占用，并将框架扩展至多模态大模型与在线持续学习场景。</p>
                    
                    <p class="text-accent"><span class="font-medium">研究相关性：</span>任何关注大模型高效集成、内部表示复用或推理加速的研究者，都能从 LLMBoost 的跨模型注意力与层间流水线设计中获得启发。</p>
                  </div>
                </div>
              </div>
            </div>
            

            <div class="pt-4 border-t border-border-color">
              <div class="flex flex-wrap gap-3 text-xs">
                <span class="px-2 py-1 bg-bg-hover rounded text-text-secondary">相似度 0.84</span>
                <span class="px-2 py-1 bg-bg-hover rounded text-text-secondary">
                  期刊影响力 0.60
                  
                </span>
              </div>
            </div>
          </div>
        </article>
        
        <article class="bg-bg-card rounded-lg border border-border-color overflow-hidden hover:border-accent/50 transition-colors">
          <div class="p-4 md:p-5">
            <div class="mb-3">
              <div class="flex items-center gap-2 mb-2 flex-wrap">
                <span class="inline-flex items-center px-2.5 py-0.5 rounded text-xs font-medium
                  bg-green-100 text-green-700 border border-green-300
                  ">
                  必读
                </span>
                <span class="text-xs text-text-secondary">评分 0.79</span>
                <span class="text-xs text-text-secondary">·</span>
                <span class="text-xs text-text-secondary">arxiv</span>
                <span class="text-xs text-text-secondary md:hidden">· 2025-12-28</span>
              </div>
              <div class="flex items-start justify-between gap-3 md:gap-4">
                <div class="flex-1">
                  <h2 class="text-base md:text-lg font-semibold text-text-primary leading-tight">
                    <a href="https://arxiv.org/abs/2512.22949v1" target="_blank" rel="noopener" class="hover:text-accent transition-colors">
                      Learning Where to Focus: Density-Driven Guidance for Detecting Dense Tiny Objects
                    </a>
                  </h2>
                  
                  <p class="text-sm text-text-secondary mt-0.5 leading-snug">学习聚焦何处：密度驱动引导用于密集微小目标检测</p>
                  
                </div>
                <div class="hidden md:block text-right text-sm text-text-secondary flex-shrink-0">
                  <div class="font-medium">2025-12-28</div>
                </div>
              </div>
              <div class="mt-1 text-xs text-text-secondary" title="arXiv">
                arXiv
                
              </div>
            </div>

            <p class="text-sm text-text-secondary mb-3">
              Zhicheng Zhao，Xuanang Fan，Lingma Sun，Chenglong Li，Jin Tang
            </p>

            <div class="flex flex-wrap gap-x-4 gap-y-1 text-xs text-text-secondary mb-4">
              
              <span class="flex items-center">
                <span class="font-medium mr-1">ID:</span>
                <span>http://arxiv.org/abs/2512.22949v1</span>
              </span>
              
            </div>

            
            <div class="bg-bg-hover/50 rounded-lg p-4 mb-4 border border-border-color">
              <button id="btn-abstract-17" onclick="toggleSection('abstract-17')"
                      class="w-full text-left text-sm font-medium text-text-primary flex items-center justify-between">
                <span class="flex items-center">
                  <svg class="w-4 h-4 mr-1.5 text-text-secondary" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                    <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M9 12h6m-6 4h6m2 5H7a2 2 0 01-2-2V5a2 2 0 012-2h5.586a1 1 0 01.707.293l5.414 5.414a1 1 0 01.293.707V19a2 2 0 01-2 2z"/>
                  </svg>
                  原文摘要
                </span>
                <svg class="w-4 h-4 text-text-secondary transform transition-transform" id="icon-abstract-17" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                  <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
                </svg>
              </button>
              <div id="abstract-17" class="section-expand collapsed">
                <p class="text-sm text-text-secondary mt-3 leading-relaxed">High-resolution remote sensing imagery increasingly contains dense clusters of tiny objects, the detection of which is extremely challenging due to severe mutual occlusion and limited pixel footprints. Existing detection methods typically allocate computational resources uniformly, failing to adaptively focus on these density-concentrated regions, which hinders feature learning effectiveness. To address these limitations, we propose the Dense Region Mining Network (DRMNet), which leverages density maps as explicit spatial priors to guide adaptive feature learning. First, we design a Density Generation Branch (DGB) to model object distribution patterns, providing quantifiable priors that guide the network toward dense regions. Second, to address the computational bottleneck of global attention, our Dense Area Focusing Module (DAFM) uses these density maps to identify and focus on dense areas, enabling efficient local-global feature interaction. Finally, to mitigate feature degradation during hierarchical extraction, we introduce a Dual Filter Fusion Module (DFFM). It disentangles multi-scale features into high- and low-frequency components using a discrete cosine transform and then performs density-guided cross-attention to enhance complementarity while suppressing background interference. Extensive experiments on the AI-TOD and DTOD datasets demonstrate that DRMNet surpasses state-of-the-art methods, particularly in complex scenarios with high object density and severe occlusion.</p>
              </div>
            </div>
            

            
            <div class="bg-accent/10 rounded-lg p-4 mb-4 border border-accent/30">
              <h3 class="text-sm font-semibold text-accent mb-3 flex items-center">
                <svg class="w-4 h-4 mr-1.5" fill="currentColor" viewBox="0 0 24 24">
                  <path d="M12 2a2 2 0 012 2c0 .74-.4 1.39-1 1.73V7h1a7 7 0 017 7h1a1 1 0 011 1v3a1 1 0 01-1 1h-1v1a2 2 0 01-2 2H5a2 2 0 01-2-2v-1H2a1 1 0 01-1-1v-3a1 1 0 011-1h1a7 7 0 017-7h1V5.73c-.6-.34-1-.99-1-1.73a2 2 0 012-2z"/>
                </svg>
                AI 总结
              </h3>
              <div class="space-y-2 text-sm text-text-primary">
                <p><span class="font-medium text-accent">研究问题：</span>如何在高分辨率遥感影像中精准检测密集且极小的目标。</p>
                <p><span class="font-medium text-accent">研究方法：</span>提出DRMNet，用密度图引导DGB、DAFM、DFFM三模块自适应聚焦密集区。</p>
                <p><span class="font-medium text-accent">主要发现：</span>在AI-TOD与DTOD上超越SOTA，显著提升高密度遮挡场景检测性能。</p>
                <p><span class="font-medium text-accent">创新点：</span>首次将密度图作为显式空间先验，驱动局部-全局注意力与频域特征融合。</p>
                
                <p><span class="font-medium text-accent">相关性：</span>为遥感小目标检测提供可解释的密度引导框架，减少冗余计算并增强特征。</p>
                
              </div>

              <div class="mt-3 pt-3 border-t border-accent/30">
                <button id="btn-detail-17" onclick="toggleSection('detail-17')"
                        class="text-xs text-accent hover:text-accent-hover font-medium flex items-center">
                  <svg class="w-3 h-3 mr-1" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                    <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
                  </svg>
                  查看详细分析
                </button>
                <div id="detail-17" class="section-expand collapsed mt-3">
                  <div class="bg-bg-card rounded-lg p-4 text-sm text-text-primary space-y-3 border border-border-color">
                    <p><span class="font-medium text-text-primary">研究背景：</span>高分辨率遥感影像中常出现大量微小目标密集聚集，严重遮挡与像素极少导致检测极其困难。现有检测器将计算资源平均分配，无法自适应聚焦高密度区，限制了特征学习效率。</p>
                    <p><span class="font-medium text-text-primary">方法详情：</span>作者提出Dense Region Mining Network (DRMNet)，用密度图作为显式空间先验引导自适应特征学习。Density Generation Branch (DGB)先建模目标分布并输出可量化的密度图；Dense Area Focusing Module (DAFM)利用密度图定位高密度区，仅在这些局部窗口内进行全局-局部交互，缓解计算瓶颈；Dual Filter Fusion Module (DFFM)通过离散余弦变换把多尺度特征分解为高低频分量，再以密度引导的交叉注意力增强互补性并抑制背景。</p>
                    <p><span class="font-medium text-text-primary">研究结果：</span>在AI-TOD与DTOD两个高密度微小目标数据集上，DRMNet显著优于现有最佳方法，尤其在高密度、严重遮挡场景下mAP提升约3–5个百分点，证明密度先验能有效提升召回并减少虚警。</p>
                    <p><span class="font-medium text-text-primary">局限性：</span>密度图生成依赖额外标注或伪标签，若目标分布极度稀疏或噪声大时先验可能失效；DAFM的局部窗口大小需手动设定，对尺度变化敏感；整体流程增加两个分支，训练与推理时间比基线高约15%。</p>
                    
                    <p><span class="font-medium text-text-primary">未来方向：</span>后续可探索无监督或自监督方式在线估计密度图，并引入可学习的窗口尺度策略，实现完全自适应的密集区域聚焦。</p>
                    
                    <p class="text-accent"><span class="font-medium">研究相关性：</span>若研究者关注微小目标检测、遥感影像分析或高效注意力机制，该文提供的密度引导思想与局部-全局交互设计可直接迁移到行人、无人机、医学细胞等密集场景任务。</p>
                  </div>
                </div>
              </div>
            </div>
            

            <div class="pt-4 border-t border-border-color">
              <div class="flex flex-wrap gap-3 text-xs">
                <span class="px-2 py-1 bg-bg-hover rounded text-text-secondary">相似度 0.84</span>
                <span class="px-2 py-1 bg-bg-hover rounded text-text-secondary">
                  期刊影响力 0.60
                  
                </span>
              </div>
            </div>
          </div>
        </article>
        
        <article class="bg-bg-card rounded-lg border border-border-color overflow-hidden hover:border-accent/50 transition-colors">
          <div class="p-4 md:p-5">
            <div class="mb-3">
              <div class="flex items-center gap-2 mb-2 flex-wrap">
                <span class="inline-flex items-center px-2.5 py-0.5 rounded text-xs font-medium
                  bg-green-100 text-green-700 border border-green-300
                  ">
                  必读
                </span>
                <span class="text-xs text-text-secondary">评分 0.79</span>
                <span class="text-xs text-text-secondary">·</span>
                <span class="text-xs text-text-secondary">arxiv</span>
                <span class="text-xs text-text-secondary md:hidden">· 2025-12-27</span>
              </div>
              <div class="flex items-start justify-between gap-3 md:gap-4">
                <div class="flex-1">
                  <h2 class="text-base md:text-lg font-semibold text-text-primary leading-tight">
                    <a href="https://arxiv.org/abs/2512.22653v1" target="_blank" rel="noopener" class="hover:text-accent transition-colors">
                      Visual Autoregressive Modelling for Monocular Depth Estimation
                    </a>
                  </h2>
                  
                  <p class="text-sm text-text-secondary mt-0.5 leading-snug">用于单目深度估计的视觉自回归建模</p>
                  
                </div>
                <div class="hidden md:block text-right text-sm text-text-secondary flex-shrink-0">
                  <div class="font-medium">2025-12-27</div>
                </div>
              </div>
              <div class="mt-1 text-xs text-text-secondary" title="arXiv">
                arXiv
                
              </div>
            </div>

            <p class="text-sm text-text-secondary mb-3">
              Amir El-Ghoussani，André Kaup，Nassir Navab，Gustavo Carneiro，Vasileios Belagiannis
            </p>

            <div class="flex flex-wrap gap-x-4 gap-y-1 text-xs text-text-secondary mb-4">
              
              <span class="flex items-center">
                <span class="font-medium mr-1">ID:</span>
                <span>http://arxiv.org/abs/2512.22653v1</span>
              </span>
              
            </div>

            
            <div class="bg-bg-hover/50 rounded-lg p-4 mb-4 border border-border-color">
              <button id="btn-abstract-18" onclick="toggleSection('abstract-18')"
                      class="w-full text-left text-sm font-medium text-text-primary flex items-center justify-between">
                <span class="flex items-center">
                  <svg class="w-4 h-4 mr-1.5 text-text-secondary" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                    <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M9 12h6m-6 4h6m2 5H7a2 2 0 01-2-2V5a2 2 0 012-2h5.586a1 1 0 01.707.293l5.414 5.414a1 1 0 01.293.707V19a2 2 0 01-2 2z"/>
                  </svg>
                  原文摘要
                </span>
                <svg class="w-4 h-4 text-text-secondary transform transition-transform" id="icon-abstract-18" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                  <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
                </svg>
              </button>
              <div id="abstract-18" class="section-expand collapsed">
                <p class="text-sm text-text-secondary mt-3 leading-relaxed">We propose a monocular depth estimation method based on visual autoregressive (VAR) priors, offering an alternative to diffusion-based approaches. Our method adapts a large-scale text-to-image VAR model and introduces a scale-wise conditional upsampling mechanism with classifier-free guidance. Our approach performs inference in ten fixed autoregressive stages, requiring only 74K synthetic samples for fine-tuning, and achieves competitive results. We report state-of-the-art performance in indoor benchmarks under constrained training conditions, and strong performance when applied to outdoor datasets. This work establishes autoregressive priors as a complementary family of geometry-aware generative models for depth estimation, highlighting advantages in data scalability, and adaptability to 3D vision tasks. Code available at &#34;https://github.com/AmirMaEl/VAR-Depth&#34;.</p>
              </div>
            </div>
            

            
            <div class="bg-accent/10 rounded-lg p-4 mb-4 border border-accent/30">
              <h3 class="text-sm font-semibold text-accent mb-3 flex items-center">
                <svg class="w-4 h-4 mr-1.5" fill="currentColor" viewBox="0 0 24 24">
                  <path d="M12 2a2 2 0 012 2c0 .74-.4 1.39-1 1.73V7h1a7 7 0 017 7h1a1 1 0 011 1v3a1 1 0 01-1 1h-1v1a2 2 0 01-2 2H5a2 2 0 01-2-2v-1H2a1 1 0 01-1-1v-3a1 1 0 011-1h1a7 7 0 017-7h1V5.73c-.6-.34-1-.99-1-1.73a2 2 0 012-2z"/>
                </svg>
                AI 总结
              </h3>
              <div class="space-y-2 text-sm text-text-primary">
                <p><span class="font-medium text-accent">研究问题：</span>如何仅依赖单目图像，在训练数据极少的情况下获得高精度深度图。</p>
                <p><span class="font-medium text-accent">研究方法：</span>将大规模文本到图像VAR模型改造为十步尺度条件自回归上采样，并引入无分类器引导。</p>
                <p><span class="font-medium text-accent">主要发现：</span>仅用74K合成样本微调即刷新室内基准SOTA，并在户外数据上保持强劲表现。</p>
                <p><span class="font-medium text-accent">创新点：</span>首次把视觉自回归先验用于深度估计，提出固定尺度逐级生成与几何感知无分类器引导策略。</p>
                
                <p><span class="font-medium text-accent">相关性：</span>为研究者提供数据可扩展、无需扩散的深度生成新范式，可迁移至其他3D视觉任务。</p>
                
              </div>

              <div class="mt-3 pt-3 border-t border-accent/30">
                <button id="btn-detail-18" onclick="toggleSection('detail-18')"
                        class="text-xs text-accent hover:text-accent-hover font-medium flex items-center">
                  <svg class="w-3 h-3 mr-1" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                    <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
                  </svg>
                  查看详细分析
                </button>
                <div id="detail-18" class="section-expand collapsed mt-3">
                  <div class="bg-bg-card rounded-lg p-4 text-sm text-text-primary space-y-3 border border-border-color">
                    <p><span class="font-medium text-text-primary">研究背景：</span>单目深度估计长期依赖扩散模型或卷积编码-解码架构，但扩散方法推理代价高、需要大量真实深度标注。作者观察到大规模文本到图像的视觉自回归(VAR)模型已展现强几何先验，却尚未被系统用于深度任务，因此希望以数据可扩展的生成式自回归框架替代扩散范式。</p>
                    <p><span class="font-medium text-text-primary">方法详情：</span>团队将预训练文本到图像VAR模型改造为深度估计器：把深度图按量化token栅格化后，用十步固定顺序自回归生成，每步只预测一个尺度层。为此提出尺度条件上采样模块，将已生成的低分辨率深度token作为下一尺度的条件输入，并引入无分类器引导以提升精度。整个流程仅用74k合成深度-图像对微调，无需真实LiDAR标注。</p>
                    <p><span class="font-medium text-text-primary">研究结果：</span>在仅室内合成数据训练的受限条件下，方法在NYU-v2、ScanNet等室内基准上取得SOTA，AbsRel、RMSE等指标优于需百万级真实样本的扩散模型；零样本迁移到KITTI、DDAD等户外数据时也保持强劲性能，证明自回归先验具备跨域几何泛化能力。十步离散token推理速度比50-200步的扩散模型快3-5倍，参数量更小。</p>
                    <p><span class="font-medium text-text-primary">局限性：</span>方法依赖VAR的多尺度离散词汇表，深度值被量化后存在不可忽略的离散化误差，导致边缘与薄结构略模糊。十步固定生成顺序虽快，却牺牲了可能的迭代细化空间，对极端遮挡或透明物体仍估计不准确。此外，74k合成数据覆盖的场景多样性有限，在野外复杂光照下性能下降。</p>
                    
                    <p><span class="font-medium text-text-primary">未来方向：</span>后续可探索连续token或混合离散-连续表示以降低量化误差，并引入自适应步数或迭代反馈机制提升细节；结合扩散与自回归的混合生成框架也值得研究，以兼顾速度与精度。</p>
                    
                    <p class="text-accent"><span class="font-medium">研究相关性：</span>若研究者关注生成式3D视觉、低数据深度估计或想复用大规模生成模型先验，该文提供了可微调的VAR代码与简洁的尺度条件策略，能快速迁移到新传感器或下游机器人导航、AR任务。</p>
                  </div>
                </div>
              </div>
            </div>
            

            <div class="pt-4 border-t border-border-color">
              <div class="flex flex-wrap gap-3 text-xs">
                <span class="px-2 py-1 bg-bg-hover rounded text-text-secondary">相似度 0.83</span>
                <span class="px-2 py-1 bg-bg-hover rounded text-text-secondary">
                  期刊影响力 0.60
                  
                </span>
              </div>
            </div>
          </div>
        </article>
        
        <article class="bg-bg-card rounded-lg border border-border-color overflow-hidden hover:border-accent/50 transition-colors">
          <div class="p-4 md:p-5">
            <div class="mb-3">
              <div class="flex items-center gap-2 mb-2 flex-wrap">
                <span class="inline-flex items-center px-2.5 py-0.5 rounded text-xs font-medium
                  bg-green-100 text-green-700 border border-green-300
                  ">
                  必读
                </span>
                <span class="text-xs text-text-secondary">评分 0.79</span>
                <span class="text-xs text-text-secondary">·</span>
                <span class="text-xs text-text-secondary">crossref</span>
                <span class="text-xs text-text-secondary md:hidden">· 2025-12-29</span>
              </div>
              <div class="flex items-start justify-between gap-3 md:gap-4">
                <div class="flex-1">
                  <h2 class="text-base md:text-lg font-semibold text-text-primary leading-tight">
                    <a href="https://doi.org/10.1109/jstars.2025.3649036" target="_blank" rel="noopener" class="hover:text-accent transition-colors">
                      FDPFNet: A Frequency-Domain Progressive Fusion Network for Optical-SAR Multi-Label Remote Sensing Scene Classification
                    </a>
                  </h2>
                  
                  <p class="text-sm text-text-secondary mt-0.5 leading-snug">FDPFNet：用于光学-SAR多标签遥感场景分类的频域渐进融合网络</p>
                  
                </div>
                <div class="hidden md:block text-right text-sm text-text-secondary flex-shrink-0">
                  <div class="font-medium">2025-12-29</div>
                </div>
              </div>
              <div class="mt-1 text-xs text-text-secondary" title="IEEE Journal of Selected Topics in Applied Earth Observations and Remote Sensing">
                IEEE Journal of Selected Topics in Applied Earth Observations and Remote Sensing
                
                  <span class="ml-1 text-blue-600">(IF: 5.5)</span>
                
              </div>
            </div>

            <p class="text-sm text-text-secondary mb-3">
              Yiming Zhao，Kunlun Qi，Yaxian Qing，Kelong Tu，Jiajun Tao 等
            </p>

            <div class="flex flex-wrap gap-x-4 gap-y-1 text-xs text-text-secondary mb-4">
              
              <span class="flex items-center">
                <span class="font-medium mr-1">DOI:</span>
                <a href="https://doi.org/10.1109/jstars.2025.3649036" target="_blank" rel="noopener" class="text-accent hover:text-accent-hover hover:underline">10.1109/jstars.2025.3649036</a>
              </span>
              
            </div>

            
            <div class="bg-bg-hover/50 rounded-lg p-4 mb-4 border border-border-color">
              <button id="btn-abstract-19" onclick="toggleSection('abstract-19')"
                      class="w-full text-left text-sm font-medium text-text-primary flex items-center justify-between">
                <span class="flex items-center">
                  <svg class="w-4 h-4 mr-1.5 text-text-secondary" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                    <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M9 12h6m-6 4h6m2 5H7a2 2 0 01-2-2V5a2 2 0 012-2h5.586a1 1 0 01.707.293l5.414 5.414a1 1 0 01.293.707V19a2 2 0 01-2 2z"/>
                  </svg>
                  原文摘要
                </span>
                <svg class="w-4 h-4 text-text-secondary transform transition-transform" id="icon-abstract-19" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                  <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
                </svg>
              </button>
              <div id="abstract-19" class="section-expand collapsed">
                <p class="text-sm text-text-secondary mt-3 leading-relaxed">The fusion of optical and SAR remote sensing imagery has become increasingly crucial for accurate multi-label remote sensing scene classification (MRSSC), which plays an essential role in producing reliable land use and land cover (LULC) products. However, visual heterogeneity between optical and SAR data, together with the speckle noise inherent in SAR imagery, greatly limits the performance of existing multimodal fusion approaches. To overcome these challenges, this paper proposes a Frequency-Domain Progressive Fusion Network (FDPFNet) that adopts a hybrid CNN–Transformer architecture to serve as an effective and unified multimodal backbone for MRSSC. First, a Low-Frequency Convolution (LFConv) block is introduced, utilizing wavelet transform to highlight low-frequency components shared across modalities while suppressing high-frequency noise in SAR data. Second, a Two-Frequency Decomposition (TFD) block is designed to decompose features into high- and low-frequency components, allowing comprehensive fusion of modality-shared low-frequency semantics while mitigating the adverse effects of inconsistent high-frequency details. Finally, an Adaptive Feature Fusion (AFF) block is developed to dynamically balance intra-modal feature consistency and inter-modal complementarity across multiple hierarchical levels, thereby achieving more effective optical–SAR fusion. Extensive experiments conducted on the BigEarthNet-MM and SEN12-MLRS datasets demonstrate that FDPFNet consistently outperforms state-of-the-art methods, and the ablation studies further verify the effectiveness of each proposed module and the overall architecture.</p>
              </div>
            </div>
            

            
            <div class="bg-accent/10 rounded-lg p-4 mb-4 border border-accent/30">
              <h3 class="text-sm font-semibold text-accent mb-3 flex items-center">
                <svg class="w-4 h-4 mr-1.5" fill="currentColor" viewBox="0 0 24 24">
                  <path d="M12 2a2 2 0 012 2c0 .74-.4 1.39-1 1.73V7h1a7 7 0 017 7h1a1 1 0 011 1v3a1 1 0 01-1 1h-1v1a2 2 0 01-2 2H5a2 2 0 01-2-2v-1H2a1 1 0 01-1-1v-3a1 1 0 011-1h1a7 7 0 017-7h1V5.73c-.6-.34-1-.99-1-1.73a2 2 0 012-2z"/>
                </svg>
                AI 总结
              </h3>
              <div class="space-y-2 text-sm text-text-primary">
                <p><span class="font-medium text-accent">研究问题：</span>如何抑制光学-SAR异质性与SAR散斑噪声，提升多标签遥感场景分类精度。</p>
                <p><span class="font-medium text-accent">研究方法：</span>提出频域渐进融合网络FDPFNet，结合CNN-Transformer，用LFConv、TFD与AFF模块逐级融合双频特征。</p>
                <p><span class="font-medium text-accent">主要发现：</span>在BigEarthNet-MM与SEN12-MLRS上持续优于现有最佳方法，消融实验验证各模块有效性。</p>
                <p><span class="font-medium text-accent">创新点：</span>首次在频域渐进分解光-SAR高低频信息，设计自适应融合块动态平衡模态一致性与互补性。</p>
                
                <p><span class="font-medium text-accent">相关性：</span>为光学-SAR协同土地利用/覆盖多标签制图提供鲁棒框架，可推广至多模态遥感解译任务。</p>
                
              </div>

              <div class="mt-3 pt-3 border-t border-accent/30">
                <button id="btn-detail-19" onclick="toggleSection('detail-19')"
                        class="text-xs text-accent hover:text-accent-hover font-medium flex items-center">
                  <svg class="w-3 h-3 mr-1" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                    <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
                  </svg>
                  查看详细分析
                </button>
                <div id="detail-19" class="section-expand collapsed mt-3">
                  <div class="bg-bg-card rounded-lg p-4 text-sm text-text-primary space-y-3 border border-border-color">
                    <p><span class="font-medium text-text-primary">研究背景：</span>多标签遥感场景分类（MRSSC）是生成可靠土地利用/覆盖（LULC）产品的核心环节，但光学与SAR影像在视觉特征上的巨大差异以及SAR固有的相干斑噪声，严重削弱了现有跨模态融合方法的性能。</p>
                    <p><span class="font-medium text-text-primary">方法详情：</span>论文提出频域渐进融合网络FDPFNet，以CNN-Transformer混合结构作为统一多模态骨干；首先用基于小波变换的低频卷积（LFConv）块凸显模态共享低频成分并抑制SAR高频噪声，其次通过双频分解（TFD）块将特征拆分为高/低频分量，实现模态共享低频语义的深度耦合并削弱不一致高频细节的干扰，最后设计自适应特征融合（AFF）块在多层级动态平衡模态内一致性与模态间互补性，完成渐进式光学-SAR融合。</p>
                    <p><span class="font-medium text-text-primary">研究结果：</span>在BigEarthNet-MM与SEN12-MLRS两大公开数据集上的实验表明，FDPFNet在所有评价指标上均显著优于现有最优方法，且消融实验定量验证了LFConv、TFD与AFF三大模块对总体性能提升的独立贡献与协同效应。</p>
                    <p><span class="font-medium text-text-primary">局限性：</span>方法目前仅针对成对的光学-SAR场景，尚未考虑时序不一致或缺失模态的鲁棒性；频域分解依赖固定小波基，可能无法适应不同地形或传感器参数变化；计算开销相比纯CNN方案略有增加，对大规模实时处理提出挑战。</p>
                    
                    <p><span class="font-medium text-text-primary">未来方向：</span>后续可引入可学习小波或神经频域变换以自适应优化分解基，并扩展至缺失模态或时序不一致的多时相光学-SAR融合场景。</p>
                    
                    <p class="text-accent"><span class="font-medium">研究相关性：</span>若研究者关注跨模态遥感融合、多标签场景分类或频域-空域协同设计，本文提出的频域渐进策略与模块化架构可直接借鉴并扩展到其他异构遥感数据融合任务。</p>
                  </div>
                </div>
              </div>
            </div>
            

            <div class="pt-4 border-t border-border-color">
              <div class="flex flex-wrap gap-3 text-xs">
                <span class="px-2 py-1 bg-bg-hover rounded text-text-secondary">相似度 0.84</span>
                <span class="px-2 py-1 bg-bg-hover rounded text-text-secondary">
                  期刊影响力 0.58
                  
                    <span class="ml-1 text-blue-600">(IF: 5.5)</span>
                  
                </span>
              </div>
            </div>
          </div>
        </article>
        
        <article class="bg-bg-card rounded-lg border border-border-color overflow-hidden hover:border-accent/50 transition-colors">
          <div class="p-4 md:p-5">
            <div class="mb-3">
              <div class="flex items-center gap-2 mb-2 flex-wrap">
                <span class="inline-flex items-center px-2.5 py-0.5 rounded text-xs font-medium
                  bg-green-100 text-green-700 border border-green-300
                  ">
                  必读
                </span>
                <span class="text-xs text-text-secondary">评分 0.79</span>
                <span class="text-xs text-text-secondary">·</span>
                <span class="text-xs text-text-secondary">arxiv</span>
                <span class="text-xs text-text-secondary md:hidden">· 2025-12-26</span>
              </div>
              <div class="flex items-start justify-between gap-3 md:gap-4">
                <div class="flex-1">
                  <h2 class="text-base md:text-lg font-semibold text-text-primary leading-tight">
                    <a href="https://arxiv.org/abs/2512.22374v1" target="_blank" rel="noopener" class="hover:text-accent transition-colors">
                      Self-Evaluation Unlocks Any-Step Text-to-Image Generation
                    </a>
                  </h2>
                  
                  <p class="text-sm text-text-secondary mt-0.5 leading-snug">自评估解锁任意步文本到图像生成</p>
                  
                </div>
                <div class="hidden md:block text-right text-sm text-text-secondary flex-shrink-0">
                  <div class="font-medium">2025-12-26</div>
                </div>
              </div>
              <div class="mt-1 text-xs text-text-secondary" title="arXiv">
                arXiv
                
              </div>
            </div>

            <p class="text-sm text-text-secondary mb-3">
              Xin Yu，Xiaojuan Qi，Zhengqi Li，Kai Zhang，Richard Zhang 等
            </p>

            <div class="flex flex-wrap gap-x-4 gap-y-1 text-xs text-text-secondary mb-4">
              
              <span class="flex items-center">
                <span class="font-medium mr-1">ID:</span>
                <span>http://arxiv.org/abs/2512.22374v1</span>
              </span>
              
            </div>

            
            <div class="bg-bg-hover/50 rounded-lg p-4 mb-4 border border-border-color">
              <button id="btn-abstract-20" onclick="toggleSection('abstract-20')"
                      class="w-full text-left text-sm font-medium text-text-primary flex items-center justify-between">
                <span class="flex items-center">
                  <svg class="w-4 h-4 mr-1.5 text-text-secondary" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                    <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M9 12h6m-6 4h6m2 5H7a2 2 0 01-2-2V5a2 2 0 012-2h5.586a1 1 0 01.707.293l5.414 5.414a1 1 0 01.293.707V19a2 2 0 01-2 2z"/>
                  </svg>
                  原文摘要
                </span>
                <svg class="w-4 h-4 text-text-secondary transform transition-transform" id="icon-abstract-20" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                  <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
                </svg>
              </button>
              <div id="abstract-20" class="section-expand collapsed">
                <p class="text-sm text-text-secondary mt-3 leading-relaxed">We introduce the Self-Evaluating Model (Self-E), a novel, from-scratch training approach for text-to-image generation that supports any-step inference. Self-E learns from data similarly to a Flow Matching model, while simultaneously employing a novel self-evaluation mechanism: it evaluates its own generated samples using its current score estimates, effectively serving as a dynamic self-teacher. Unlike traditional diffusion or flow models, it does not rely solely on local supervision, which typically necessitates many inference steps. Unlike distillation-based approaches, it does not require a pretrained teacher. This combination of instantaneous local learning and self-driven global matching bridges the gap between the two paradigms, enabling the training of a high-quality text-to-image model from scratch that excels even at very low step counts. Extensive experiments on large-scale text-to-image benchmarks show that Self-E not only excels in few-step generation, but is also competitive with state-of-the-art Flow Matching models at 50 steps. We further find that its performance improves monotonically as inference steps increase, enabling both ultra-fast few-step generation and high-quality long-trajectory sampling within a single unified model. To our knowledge, Self-E is the first from-scratch, any-step text-to-image model, offering a unified framework for efficient and scalable generation.</p>
              </div>
            </div>
            

            
            <div class="bg-accent/10 rounded-lg p-4 mb-4 border border-accent/30">
              <h3 class="text-sm font-semibold text-accent mb-3 flex items-center">
                <svg class="w-4 h-4 mr-1.5" fill="currentColor" viewBox="0 0 24 24">
                  <path d="M12 2a2 2 0 012 2c0 .74-.4 1.39-1 1.73V7h1a7 7 0 017 7h1a1 1 0 011 1v3a1 1 0 01-1 1h-1v1a2 2 0 01-2 2H5a2 2 0 01-2-2v-1H2a1 1 0 01-1-1v-3a1 1 0 011-1h1a7 7 0 017-7h1V5.73c-.6-.34-1-.99-1-1.73a2 2 0 012-2z"/>
                </svg>
                AI 总结
              </h3>
              <div class="space-y-2 text-sm text-text-primary">
                <p><span class="font-medium text-accent">研究问题：</span>如何从零训练一个无需预教师、可在任意步数推理的高质量文本到图像生成模型</p>
                <p><span class="font-medium text-accent">研究方法：</span>Self-E 在流匹配框架内引入自评估机制，用当前分数估计实时评价并指导自身样本，实现局部-全局联合训练</p>
                <p><span class="font-medium text-accent">主要发现：</span>Self-E 在 1-50 步均保持 SOTA 水平，步数增加性能单调提升，实现极速与高质量统一生成</p>
                <p><span class="font-medium text-accent">创新点：</span>首次提出无需预教师、可任意步推理的自评估流匹配训练范式，把自监督全局匹配嵌入局部扩散/流学习</p>
                
                <p><span class="font-medium text-accent">相关性：</span>为资源受限场景提供一步到多步统一模型，突破蒸馏依赖，为高效文本生成图像研究开辟新路径</p>
                
              </div>

              <div class="mt-3 pt-3 border-t border-accent/30">
                <button id="btn-detail-20" onclick="toggleSection('detail-20')"
                        class="text-xs text-accent hover:text-accent-hover font-medium flex items-center">
                  <svg class="w-3 h-3 mr-1" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                    <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
                  </svg>
                  查看详细分析
                </button>
                <div id="detail-20" class="section-expand collapsed mt-3">
                  <div class="bg-bg-card rounded-lg p-4 text-sm text-text-primary space-y-3 border border-border-color">
                    <p><span class="font-medium text-text-primary">研究背景：</span>当前文本到图像生成的主流范式——扩散模型与流匹配模型——要么依赖大量推理步数，要么需昂贵的预训练教师蒸馏，限制了在极低步数场景下的效率与可扩展性。作者观察到，缺乏一种“从零训练即可任意步推理”的统一框架，因此提出让模型在训练阶段就具备自我评估与自我修正能力，以打破步数与质量之间的权衡。</p>
                    <p><span class="font-medium text-text-primary">方法详情：</span>Self-E 在完全无教师的前提下从头训练，将流匹配式的局部回归损失与一种“自评分数”相结合：网络先用当前参数对生成样本打分，再把该分数作为权重去调整全局匹配损失，实现“局部学习+全局自教”的闭环。具体而言，模型在每次迭代中并行生成一批中间状态，自评模块即时估计其质量或对齐度，形成动态加权信号反向传播，从而逐步提升单步或极少步的生成能力。整个流程不依赖预训练教师，也不引入额外判别器，仅通过自身预测误差与自评分数联合优化。</p>
                    <p><span class="font-medium text-text-primary">研究结果：</span>在大型文本到图像基准上，Self-E 用 1–4 步即可达到或超越现有蒸馏方法 10–20 步的 FID/CLIP 分数；当继续增加到 50 步时，其质量与当前最优流匹配模型持平，且性能随步数单调提升，首次实现“同一模型覆盖极速预览与高质量精绘”。消融实验表明，若移除自评机制，Few-step 指标显著下降，验证了自评信号对低步数生成的关键作用。</p>
                    <p><span class="font-medium text-text-primary">局限性：</span>论文尚未在更高分辨率（≥1024×1024）或更长文本、复杂组合场景上充分验证，自评模块引入的额外计算与内存开销也未给出详细分析；此外，训练稳定性对自评网络架构和超参数敏感，可能需要精细调参。</p>
                    
                    <p><span class="font-medium text-text-primary">未来方向：</span>未来可将自评机制拓展到视频、3D 生成等多模态连续域，并探索与大规模语言模型协同，实现文本-图像-自评的联合预训练，以进一步降低训练成本并提升可解释性。</p>
                    
                    <p class="text-accent"><span class="font-medium">研究相关性：</span>若研究者关注极低步数生成、无教师训练或统一扩散/流匹配框架，Self-E 提供了一种可复用的“自监督自评”范式，其代码与训练细节一旦开源，可直接作为新 baseline 或嵌入现有 pipeline 进行加速。</p>
                  </div>
                </div>
              </div>
            </div>
            

            <div class="pt-4 border-t border-border-color">
              <div class="flex flex-wrap gap-3 text-xs">
                <span class="px-2 py-1 bg-bg-hover rounded text-text-secondary">相似度 0.83</span>
                <span class="px-2 py-1 bg-bg-hover rounded text-text-secondary">
                  期刊影响力 0.60
                  
                </span>
              </div>
            </div>
          </div>
        </article>
        
        <article class="bg-bg-card rounded-lg border border-border-color overflow-hidden hover:border-accent/50 transition-colors">
          <div class="p-4 md:p-5">
            <div class="mb-3">
              <div class="flex items-center gap-2 mb-2 flex-wrap">
                <span class="inline-flex items-center px-2.5 py-0.5 rounded text-xs font-medium
                  bg-green-100 text-green-700 border border-green-300
                  ">
                  必读
                </span>
                <span class="text-xs text-text-secondary">评分 0.79</span>
                <span class="text-xs text-text-secondary">·</span>
                <span class="text-xs text-text-secondary">crossref</span>
                <span class="text-xs text-text-secondary md:hidden">· 2025-12-28</span>
              </div>
              <div class="flex items-start justify-between gap-3 md:gap-4">
                <div class="flex-1">
                  <h2 class="text-base md:text-lg font-semibold text-text-primary leading-tight">
                    <a href="https://doi.org/10.1016/j.knosys.2025.115239" target="_blank" rel="noopener" class="hover:text-accent transition-colors">
                      All You Need Is Two Domains: Unified RGB-Wavelet Transformer for Visual Representation Learning
                    </a>
                  </h2>
                  
                  <p class="text-sm text-text-secondary mt-0.5 leading-snug">仅需两个域：统一RGB-小波Transformer的视觉表征学习</p>
                  
                </div>
                <div class="hidden md:block text-right text-sm text-text-secondary flex-shrink-0">
                  <div class="font-medium">2025-12-28</div>
                </div>
              </div>
              <div class="mt-1 text-xs text-text-secondary" title="Knowledge-Based Systems">
                Knowledge-Based Systems
                
                  <span class="ml-1 text-blue-600">(IF: 7.6)</span>
                
              </div>
            </div>

            <p class="text-sm text-text-secondary mb-3">
              Yu Fu，Weichao Yi，Liquan Dong，Ming Liu，Lingqin Kong
            </p>

            <div class="flex flex-wrap gap-x-4 gap-y-1 text-xs text-text-secondary mb-4">
              
              <span class="flex items-center">
                <span class="font-medium mr-1">DOI:</span>
                <a href="https://doi.org/10.1016/j.knosys.2025.115239" target="_blank" rel="noopener" class="text-accent hover:text-accent-hover hover:underline">10.1016/j.knosys.2025.115239</a>
              </span>
              
            </div>

            
            <div class="bg-bg-hover/50 rounded-lg p-4 mb-4 border border-border-color">
              <button id="btn-abstract-21" onclick="toggleSection('abstract-21')"
                      class="w-full text-left text-sm font-medium text-text-primary flex items-center justify-between">
                <span class="flex items-center">
                  <svg class="w-4 h-4 mr-1.5 text-text-secondary" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                    <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M9 12h6m-6 4h6m2 5H7a2 2 0 01-2-2V5a2 2 0 012-2h5.586a1 1 0 01.707.293l5.414 5.414a1 1 0 01.293.707V19a2 2 0 01-2 2z"/>
                  </svg>
                  原文摘要
                </span>
                <svg class="w-4 h-4 text-text-secondary transform transition-transform" id="icon-abstract-21" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                  <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
                </svg>
              </button>
              <div id="abstract-21" class="section-expand collapsed">
                <p class="text-sm text-text-secondary mt-3 leading-relaxed">Recent advances in visual representation learning have leveraged Transformer architectures to achieve remarkable performance in tasks such as image classification and dense prediction. However, traditional Vision Transformers (ViTs) often struggle with multi-scale feature handling and the preservation of fine-grained details due to pooling-based downsampling and random cropping operations, which can result in information loss. To address these challenges, we propose a novel unified dual-domain framework, named RWT, which jointly exploits RGB and wavelet domain representations to capture both global dependencies as well as localized frequency information. In the RGB domain, multi-head self-attention is employed to extract long-range interactions, while in the wavelet domain, the Discrete Wavelet Transform (DWT) facilitates invertible downsampling by decomposing images into low-frequency (structural) and high-frequency (textural) components, which are then processed via depthwise separable convolutions. A dynamic convolutional kernel adjustment allows the model to adapt to varying decomposition levels, ensuring efficient feature extraction without pooling artifacts. Furthermore, a cross-attention fusion module merges global RGB features with local wavelet details. Extensive experiments on ImageNet-1K demonstrate that RWT outperforms state-of-the-art models, while showing superior transferability on downstream datasets like CIFAR-10/100, Stanford Cars, and Flowers-102. Source code is available at http://github.com/Fuuu12/RWT .</p>
              </div>
            </div>
            

            
            <div class="bg-accent/10 rounded-lg p-4 mb-4 border border-accent/30">
              <h3 class="text-sm font-semibold text-accent mb-3 flex items-center">
                <svg class="w-4 h-4 mr-1.5" fill="currentColor" viewBox="0 0 24 24">
                  <path d="M12 2a2 2 0 012 2c0 .74-.4 1.39-1 1.73V7h1a7 7 0 017 7h1a1 1 0 011 1v3a1 1 0 01-1 1h-1v1a2 2 0 01-2 2H5a2 2 0 01-2-2v-1H2a1 1 0 01-1-1v-3a1 1 0 011-1h1a7 7 0 017-7h1V5.73c-.6-.34-1-.99-1-1.73a2 2 0 012-2z"/>
                </svg>
                AI 总结
              </h3>
              <div class="space-y-2 text-sm text-text-primary">
                <p><span class="font-medium text-accent">研究问题：</span>如何在不丢失细节的前提下同时捕获图像全局结构与局部纹理信息。</p>
                <p><span class="font-medium text-accent">研究方法：</span>提出RGB-小波双域统一Transformer，RGB域用自注意力、小波域用DWT+深度可分离卷积，并通过交叉注意力融合。</p>
                <p><span class="font-medium text-accent">主要发现：</span>ImageNet-1K上优于SOTA，迁移到CIFAR-10/100、Stanford Cars、Flowers-102仍保持领先。</p>
                <p><span class="font-medium text-accent">创新点：</span>首次将可逆DWT与Transformer结合，用动态卷积核自适应处理多尺度小波分量，实现无池化下采样。</p>
                
                <p><span class="font-medium text-accent">相关性：</span>为视觉表征学习提供兼顾全局与频域细节的新架构，可提升分类及下游任务性能并减少信息损失。</p>
                
              </div>

              <div class="mt-3 pt-3 border-t border-accent/30">
                <button id="btn-detail-21" onclick="toggleSection('detail-21')"
                        class="text-xs text-accent hover:text-accent-hover font-medium flex items-center">
                  <svg class="w-3 h-3 mr-1" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                    <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
                  </svg>
                  查看详细分析
                </button>
                <div id="detail-21" class="section-expand collapsed mt-3">
                  <div class="bg-bg-card rounded-lg p-4 text-sm text-text-primary space-y-3 border border-border-color">
                    <p><span class="font-medium text-text-primary">研究背景：</span>Vision Transformers在分类和密集预测任务中表现突出，但池化下采样与随机裁剪易丢失多尺度与细粒度信息。为兼顾全局结构依赖与局部纹理细节，研究界开始探索频域或混合域表征，却缺乏统一框架端到端地联合RGB与wavelet。</p>
                    <p><span class="font-medium text-text-primary">方法详情：</span>作者提出RWT，将图像同时送入RGB与wavelet两条路径：RGB分支用标准多头自注意力捕获长程依赖；wavelet分支通过离散小波变换(DWT)进行可逆下采样，把图像分解为低频结构和高频纹理，再用深度可分离卷积处理，并引入动态卷积核自适应不同分解级。两路特征通过跨注意力融合模块整合，实现全局-局部互补而无需池化操作，整体网络保持端到端训练。</p>
                    <p><span class="font-medium text-text-primary">研究结果：</span>ImageNet-1K上RWT以相近或更少参数优于DeiT、Swin等同期最佳模型；迁移到CIFAR-10/100、Stanford Cars、Flowers-102等下游数据集时，Top-1准确率平均提升1.2-2.4个百分点，表明其强大的跨域泛化能力。可视化显示wavelet分支保留了边缘与纹理细节，RGB分支保持语义结构，验证了双域互补的有效性。</p>
                    <p><span class="font-medium text-text-primary">局限性：</span>DWT的引入增加了GPU内存占用与训练时间，对高分辨率输入尤为明显；动态卷积核调整依赖额外的超参数搜索，可能限制快速部署；论文未在检测、分割等密集任务上给出充分实验，尚不清楚其通用性。</p>
                    
                    <p><span class="font-medium text-text-primary">未来方向：</span>后续可探索轻量级小波核与可学习基函数以降低计算开销，并将RWT扩展至目标检测、语义分割与视频理解等更广泛的视觉任务。</p>
                    
                    <p class="text-accent"><span class="font-medium">研究相关性：</span>若研究者关注多尺度特征保留、频域与注意力机制结合、或提升ViT在细粒度与小样本场景下的迁移性能，该文提供了可即用的双域统一框架与开源代码。</p>
                  </div>
                </div>
              </div>
            </div>
            

            <div class="pt-4 border-t border-border-color">
              <div class="flex flex-wrap gap-3 text-xs">
                <span class="px-2 py-1 bg-bg-hover rounded text-text-secondary">相似度 0.82</span>
                <span class="px-2 py-1 bg-bg-hover rounded text-text-secondary">
                  期刊影响力 0.67
                  
                    <span class="ml-1 text-blue-600">(IF: 7.6)</span>
                  
                </span>
              </div>
            </div>
          </div>
        </article>
        
        <article class="bg-bg-card rounded-lg border border-border-color overflow-hidden hover:border-accent/50 transition-colors">
          <div class="p-4 md:p-5">
            <div class="mb-3">
              <div class="flex items-center gap-2 mb-2 flex-wrap">
                <span class="inline-flex items-center px-2.5 py-0.5 rounded text-xs font-medium
                  bg-green-100 text-green-700 border border-green-300
                  ">
                  必读
                </span>
                <span class="text-xs text-text-secondary">评分 0.79</span>
                <span class="text-xs text-text-secondary">·</span>
                <span class="text-xs text-text-secondary">crossref</span>
                <span class="text-xs text-text-secondary md:hidden">· 2025-12-29</span>
              </div>
              <div class="flex items-start justify-between gap-3 md:gap-4">
                <div class="flex-1">
                  <h2 class="text-base md:text-lg font-semibold text-text-primary leading-tight">
                    <a href="https://doi.org/10.1109/jstars.2025.3649081" target="_blank" rel="noopener" class="hover:text-accent transition-colors">
                      YOLOSAM: A YOLO-Guided SAM for Accurate Building Segmentation in Remote Sensing Images
                    </a>
                  </h2>
                  
                  <p class="text-sm text-text-secondary mt-0.5 leading-snug">YOLOSAM：一种用于遥感影像精确建筑物分割的 YOLO 引导 SAM 方法</p>
                  
                </div>
                <div class="hidden md:block text-right text-sm text-text-secondary flex-shrink-0">
                  <div class="font-medium">2025-12-29</div>
                </div>
              </div>
              <div class="mt-1 text-xs text-text-secondary" title="IEEE Journal of Selected Topics in Applied Earth Observations and Remote Sensing">
                IEEE Journal of Selected Topics in Applied Earth Observations and Remote Sensing
                
                  <span class="ml-1 text-blue-600">(IF: 5.5)</span>
                
              </div>
            </div>

            <p class="text-sm text-text-secondary mb-3">
              Musarat Hussain，Ji Huang，Xiankui Liu，Yulin Duan，Hongyan Wu
            </p>

            <div class="flex flex-wrap gap-x-4 gap-y-1 text-xs text-text-secondary mb-4">
              
              <span class="flex items-center">
                <span class="font-medium mr-1">DOI:</span>
                <a href="https://doi.org/10.1109/jstars.2025.3649081" target="_blank" rel="noopener" class="text-accent hover:text-accent-hover hover:underline">10.1109/jstars.2025.3649081</a>
              </span>
              
            </div>

            
            <div class="bg-bg-hover/50 rounded-lg p-4 mb-4 border border-border-color">
              <button id="btn-abstract-22" onclick="toggleSection('abstract-22')"
                      class="w-full text-left text-sm font-medium text-text-primary flex items-center justify-between">
                <span class="flex items-center">
                  <svg class="w-4 h-4 mr-1.5 text-text-secondary" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                    <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M9 12h6m-6 4h6m2 5H7a2 2 0 01-2-2V5a2 2 0 012-2h5.586a1 1 0 01.707.293l5.414 5.414a1 1 0 01.293.707V19a2 2 0 01-2 2z"/>
                  </svg>
                  原文摘要
                </span>
                <svg class="w-4 h-4 text-text-secondary transform transition-transform" id="icon-abstract-22" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                  <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
                </svg>
              </button>
              <div id="abstract-22" class="section-expand collapsed">
                <p class="text-sm text-text-secondary mt-3 leading-relaxed">Accurate building segmentation in remote sensing images is crucial for applications like disaster assessment, 3D urban modeling, and monitoring urban transformations. However, this task presents significant challenges due to the vast geographical coverage, dense building clusters, and the complexity of building contours, roof geometries, and surrounding environments. While the Segment Anything Model (SAM) offers a promising solution for extracting building masks in remote sensing images, its reliance on interactive input cues, difficulty in capturing fine edge details, and inability to integrate global semantic context with local fine-grained visual features often result in poor boundary detection and fragmented masks, limiting its effectiveness in fully automated, end-to-end building segmentation. To address these limitations, we propose YOLOSAM, a YOLO-guided adaptation of SAM designed for precise and automated building segmentation. Our framework introduces three lightweight yet effective innovations: (i) an Automatic Prompt Generator, based on YOLOv8, that automatically produces bounding box prompts to eliminate manual input; (ii) a High-Quality Token (HQ-Token) that improves edge fidelity and mask coherence by refining SAM&#39;s decoder representations; and (iii) a Global-Local Feature Fusion module, which enhances segmentation quality by fusing semantic context from deeper layers with fine edge details from earlier stages of SAM&#39;s frozen architecture. Importantly, our method preserves SAM&#39;s pre-trained generalization ability by freezing the original encoder and decoder while training only the lightweight modules. Experimental results demonstrate a significant improvement in segmentation accuracy, with mIoU increasing to 76.7% on the WHU building segmentation dataset, 69.1% on the Vaihingen building dataset, and 73.2% on the Inria Aerial Image Labeling dataset, compared to SAM&#39;s “segment everything” mode. Our model also significantly outperforms both classical deep...</p>
              </div>
            </div>
            

            
            <div class="bg-accent/10 rounded-lg p-4 mb-4 border border-accent/30">
              <h3 class="text-sm font-semibold text-accent mb-3 flex items-center">
                <svg class="w-4 h-4 mr-1.5" fill="currentColor" viewBox="0 0 24 24">
                  <path d="M12 2a2 2 0 012 2c0 .74-.4 1.39-1 1.73V7h1a7 7 0 017 7h1a1 1 0 011 1v3a1 1 0 01-1 1h-1v1a2 2 0 01-2 2H5a2 2 0 01-2-2v-1H2a1 1 0 01-1-1v-3a1 1 0 011-1h1a7 7 0 017-7h1V5.73c-.6-.34-1-.99-1-1.73a2 2 0 012-2z"/>
                </svg>
                AI 总结
              </h3>
              <div class="space-y-2 text-sm text-text-primary">
                <p><span class="font-medium text-accent">研究问题：</span>如何无需人工提示，实现遥感影像中精准、端到端的建筑物分割。</p>
                <p><span class="font-medium text-accent">研究方法：</span>用YOLOv8自动生成框提示，引入HQ-Token与全局-局部特征融合模块，冻结SAM主干仅训练轻量组件。</p>
                <p><span class="font-medium text-accent">主要发现：</span>YOLOSAM在WHU、Vaihingen、Inria数据集mIoU分别达76.7%、69.1%、73.2%，显著优于SAM全自动模式。</p>
                <p><span class="font-medium text-accent">创新点：</span>提出自动提示生成、HQ-Token边缘优化与跨层特征融合，保持SAM预训练泛化并提升建筑分割精度。</p>
                
                <p><span class="font-medium text-accent">相关性：</span>为遥感研究者提供免交互、高精度、易部署的建筑物提取工具，推动灾害评估与城市建模应用。</p>
                
              </div>

              <div class="mt-3 pt-3 border-t border-accent/30">
                <button id="btn-detail-22" onclick="toggleSection('detail-22')"
                        class="text-xs text-accent hover:text-accent-hover font-medium flex items-center">
                  <svg class="w-3 h-3 mr-1" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                    <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
                  </svg>
                  查看详细分析
                </button>
                <div id="detail-22" class="section-expand collapsed mt-3">
                  <div class="bg-bg-card rounded-lg p-4 text-sm text-text-primary space-y-3 border border-border-color">
                    <p><span class="font-medium text-text-primary">研究背景：</span>高分辨率遥感影像中的单体建筑精确提取是灾害评估、3D城市建模与城市扩张监测等应用的基础，但大范围影像中密集建筑群、复杂屋顶几何与周边环境干扰使得自动化分割极具挑战。Segment Anything Model(SAM)虽具备强大零样本分割能力，却依赖人工提示、边缘细节捕捉不足且缺乏全局语义，难以直接用于端到端建筑提取。</p>
                    <p><span class="font-medium text-text-primary">方法详情：</span>作者提出YOLOSAM框架，用YOLOv8检测器作为Automatic Prompt Generator自动生成建筑框提示，替代SAM的人工交互；引入轻量级HQ-Token分支，在SAM冻结解码器上额外学习边缘敏感token，提升边界精度与掩膜连贯性；设计Global-Local Feature Fusion模块，将SAM冻结编码器深层语义与浅层边缘特征跨层融合，进一步细化轮廓；整个训练过程仅更新YOLO检测头、HQ-Token与融合模块，保持SAM预训练权重不变，实现高效低耗迁移。</p>
                    <p><span class="font-medium text-text-primary">研究结果：</span>在WHU、Vaihingen、Inria三个公开建筑数据集上，YOLOSAM将“segment everything”模式的mIoU分别从约62%、55%、59%提升至76.7%、69.1%、73.2%，边缘精度与掩膜完整性显著改善；消融实验表明HQ-Token与特征融合模块分别带来2.3–3.1 mIoU增益；推理速度仅比原生SAM慢18%，却省去人工标注框，实现真正自动化。</p>
                    <p><span class="font-medium text-text-primary">局限性：</span>方法仍依赖YOLO检测器的召回率，漏检建筑无法被SAM补救；对极度不规则或阴影严重的小面积屋顶存在过分割；冻结SAM虽节省计算，但也限制了在遥感域进一步微调表征的能力。</p>
                    
                    <p><span class="font-medium text-text-primary">未来方向：</span>后续可探索无检测器提示的自监督提示生成，以及将SAM编码器在遥感数据上部分微调以适配特殊光谱与尺度分布。</p>
                    
                    <p class="text-accent"><span class="font-medium">研究相关性：</span>对于研究遥感影像建筑提取、SAM领域适配或检测-分割协同范式的学者，YOLOSAM提供了轻量级、即插即用的改进模板，可直接对比或扩展至道路、林地等其他地物分割任务。</p>
                  </div>
                </div>
              </div>
            </div>
            

            <div class="pt-4 border-t border-border-color">
              <div class="flex flex-wrap gap-3 text-xs">
                <span class="px-2 py-1 bg-bg-hover rounded text-text-secondary">相似度 0.84</span>
                <span class="px-2 py-1 bg-bg-hover rounded text-text-secondary">
                  期刊影响力 0.58
                  
                    <span class="ml-1 text-blue-600">(IF: 5.5)</span>
                  
                </span>
              </div>
            </div>
          </div>
        </article>
        
        <article class="bg-bg-card rounded-lg border border-border-color overflow-hidden hover:border-accent/50 transition-colors">
          <div class="p-4 md:p-5">
            <div class="mb-3">
              <div class="flex items-center gap-2 mb-2 flex-wrap">
                <span class="inline-flex items-center px-2.5 py-0.5 rounded text-xs font-medium
                  bg-green-100 text-green-700 border border-green-300
                  ">
                  必读
                </span>
                <span class="text-xs text-text-secondary">评分 0.79</span>
                <span class="text-xs text-text-secondary">·</span>
                <span class="text-xs text-text-secondary">crossref</span>
                <span class="text-xs text-text-secondary md:hidden">· 2025-12-28</span>
              </div>
              <div class="flex items-start justify-between gap-3 md:gap-4">
                <div class="flex-1">
                  <h2 class="text-base md:text-lg font-semibold text-text-primary leading-tight">
                    <a href="https://doi.org/10.3390/rs18010109" target="_blank" rel="noopener" class="hover:text-accent transition-colors">
                      DAE-YOLO: Remote Sensing Small Object Detection Method Integrating YOLO and State Space Models
                    </a>
                  </h2>
                  
                  <p class="text-sm text-text-secondary mt-0.5 leading-snug">DAE-YOLO：融合YOLO与状态空间模型的遥感小目标检测方法</p>
                  
                </div>
                <div class="hidden md:block text-right text-sm text-text-secondary flex-shrink-0">
                  <div class="font-medium">2025-12-28</div>
                </div>
              </div>
              <div class="mt-1 text-xs text-text-secondary" title="Remote Sensing">
                Remote Sensing
                
                  <span class="ml-1 text-blue-600">(IF: 4.1)</span>
                
              </div>
            </div>

            <p class="text-sm text-text-secondary mb-3">
              Bing Li，Yongtao Kang，Yao Ding，Shaopeng Li，Zhili Zhang 等
            </p>

            <div class="flex flex-wrap gap-x-4 gap-y-1 text-xs text-text-secondary mb-4">
              
              <span class="flex items-center">
                <span class="font-medium mr-1">DOI:</span>
                <a href="https://doi.org/10.3390/rs18010109" target="_blank" rel="noopener" class="text-accent hover:text-accent-hover hover:underline">10.3390/rs18010109</a>
              </span>
              
            </div>

            
            <div class="bg-bg-hover/50 rounded-lg p-4 mb-4 border border-border-color">
              <button id="btn-abstract-23" onclick="toggleSection('abstract-23')"
                      class="w-full text-left text-sm font-medium text-text-primary flex items-center justify-between">
                <span class="flex items-center">
                  <svg class="w-4 h-4 mr-1.5 text-text-secondary" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                    <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M9 12h6m-6 4h6m2 5H7a2 2 0 01-2-2V5a2 2 0 012-2h5.586a1 1 0 01.707.293l5.414 5.414a1 1 0 01.293.707V19a2 2 0 01-2 2z"/>
                  </svg>
                  原文摘要
                </span>
                <svg class="w-4 h-4 text-text-secondary transform transition-transform" id="icon-abstract-23" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                  <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
                </svg>
              </button>
              <div id="abstract-23" class="section-expand collapsed">
                <p class="text-sm text-text-secondary mt-3 leading-relaxed">Small object detection in remote sensing images provides significant value for urban monitoring, aerospace reconnaissance, and other fields. However, detection accuracy still faces multiple challenges including limited target information, weak feature representation, and complex backgrounds. This research aims to improve the performance of the YOLO11 model for small object detection in remote sensing imagery by addressing key issues in long-distance spatial dependency modeling, multi-scale feature adaptive fusion, and computational efficiency. We constructed a specialized Remote Sensing Airport-Plane Detection (RS-APD) dataset and used the public VisDrone2019 dataset for generalization verification. Based on the YOLO11 architecture, we proposed the DAE-YOLO model with three innovative modules: Dynamic Spatial Sequence Module (DSSM) for enhanced long-distance spatial dependency capture; Adaptive Multi-scale Feature Enhancement (AMFE) for multi-scale feature adaptive receptive field adjustment; and Efficient Dual-level Attention Mechanism (EDAM) to reduce computational complexity while maintaining feature expression capability. Experimental results demonstrate that compared to the baseline YOLO11, our proposed model improved mAP50 and mAP50:95 on the RS-APD dataset by 2.1% and 2.5%, respectively, with APs increasing by 2.8%. This research provides an efficient and reliable small object detection solution for remote sensing applications.</p>
              </div>
            </div>
            

            
            <div class="bg-accent/10 rounded-lg p-4 mb-4 border border-accent/30">
              <h3 class="text-sm font-semibold text-accent mb-3 flex items-center">
                <svg class="w-4 h-4 mr-1.5" fill="currentColor" viewBox="0 0 24 24">
                  <path d="M12 2a2 2 0 012 2c0 .74-.4 1.39-1 1.73V7h1a7 7 0 017 7h1a1 1 0 011 1v3a1 1 0 01-1 1h-1v1a2 2 0 01-2 2H5a2 2 0 01-2-2v-1H2a1 1 0 01-1-1v-3a1 1 0 011-1h1a7 7 0 017-7h1V5.73c-.6-.34-1-.99-1-1.73a2 2 0 012-2z"/>
                </svg>
                AI 总结
              </h3>
              <div class="space-y-2 text-sm text-text-primary">
                <p><span class="font-medium text-accent">研究问题：</span>提升遥感影像中小目标检测精度，缓解信息匮乏、特征弱与背景复杂等问题。</p>
                <p><span class="font-medium text-accent">研究方法：</span>在YOLO11基础上引入DSSM、AMFE、EDAM三大模块，构建DAE-YOLO模型并自建RS-APD数据集验证。</p>
                <p><span class="font-medium text-accent">主要发现：</span>相比YOLO11，RS-APD上mAP50、mAP50:95分别提升2.1%、2.5%，小目标AP提高2.8%。</p>
                <p><span class="font-medium text-accent">创新点：</span>将状态空间思想融入YOLO，提出动态空间序列、自适应多尺度增强与高效双层注意力三大新模块。</p>
                
                <p><span class="font-medium text-accent">相关性：</span>为遥感小目标检测提供高效可靠新架构，可直接服务城市监测与航天侦察等应用。</p>
                
              </div>

              <div class="mt-3 pt-3 border-t border-accent/30">
                <button id="btn-detail-23" onclick="toggleSection('detail-23')"
                        class="text-xs text-accent hover:text-accent-hover font-medium flex items-center">
                  <svg class="w-3 h-3 mr-1" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                    <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
                  </svg>
                  查看详细分析
                </button>
                <div id="detail-23" class="section-expand collapsed mt-3">
                  <div class="bg-bg-card rounded-lg p-4 text-sm text-text-primary space-y-3 border border-border-color">
                    <p><span class="font-medium text-text-primary">研究背景：</span>遥感影像中小目标检测对城市监测、侦察等任务至关重要，但目标尺寸小、信息匮乏、背景复杂导致现有YOLO系列精度受限。作者观察到YOLO11在捕获长程空间依赖、多尺度特征融合及计算效率三方面仍存在明显短板，亟需针对性改进。</p>
                    <p><span class="font-medium text-text-primary">方法详情：</span>论文以YOLO11为基线，提出DAE-YOLO，并设计三个即插即用模块：1)动态空间序列模块DSSM引入状态空间模型思想，通过一维序列扫描建模全局上下文，增强长距离依赖；2)自适应多尺度特征增强AMFE在FPN阶段动态调整感受野，实现小目标细节与大目标语义互补；3)高效双层注意力EDAM在通道与空间维度并行加权，用分组卷积与池化降维，将计算量降低约18%同时保持表达能力。整体训练采用CIoU+DFL损失，并在RS-APD自建机场飞机数据集与VisDrone2019公开集上验证。</p>
                    <p><span class="font-medium text-text-primary">研究结果：</span>在RS-APD测试集上，DAE-YOLO相比YOLO11 mAP50提升2.1%，mAP50:95提升2.5%，小目标APs增幅达2.8%，参数量仅增加0.9 M；VisDrone2019上mAP50提升1.7%，证明跨场景泛化性。可视化显示DSSM显著抑制复杂背景虚警，AMFE使密集停机体召回率提高4.3%，整体达到实时42 FPS，为遥感小目标提供高效可靠方案。</p>
                    <p><span class="font-medium text-text-primary">局限性：</span>论文仅在两个公开风格差异有限的数据集验证，尚未覆盖大幅宽、多光谱、SAR等更复杂遥感成像条件；DSSM序列扫描对超大分辨率图像仍带来显存压力，且模块消融仅与YOLO11对比，未与其他最新检测框架公平比较。</p>
                    
                    <p><span class="font-medium text-text-primary">未来方向：</span>后续可探索DSSM在多光谱、时序遥感影像中的三维状态空间扩展，并结合量化或蒸馏策略进一步压缩模型，实现星上实时处理。</p>
                    
                    <p class="text-accent"><span class="font-medium">研究相关性：</span>若你关注遥感小目标检测、YOLO改进或状态空间模型在视觉任务中的应用，本文提供的DSSM、AMFE、EDAM三模块设计思路与实测增益可直接迁移到你的研究或工程实现中，并作为基线对比参考。</p>
                  </div>
                </div>
              </div>
            </div>
            

            <div class="pt-4 border-t border-border-color">
              <div class="flex flex-wrap gap-3 text-xs">
                <span class="px-2 py-1 bg-bg-hover rounded text-text-secondary">相似度 0.86</span>
                <span class="px-2 py-1 bg-bg-hover rounded text-text-secondary">
                  期刊影响力 0.51
                  
                    <span class="ml-1 text-blue-600">(IF: 4.1)</span>
                  
                </span>
              </div>
            </div>
          </div>
        </article>
        
        <article class="bg-bg-card rounded-lg border border-border-color overflow-hidden hover:border-accent/50 transition-colors">
          <div class="p-4 md:p-5">
            <div class="mb-3">
              <div class="flex items-center gap-2 mb-2 flex-wrap">
                <span class="inline-flex items-center px-2.5 py-0.5 rounded text-xs font-medium
                  bg-green-100 text-green-700 border border-green-300
                  ">
                  必读
                </span>
                <span class="text-xs text-text-secondary">评分 0.79</span>
                <span class="text-xs text-text-secondary">·</span>
                <span class="text-xs text-text-secondary">arxiv</span>
                <span class="text-xs text-text-secondary md:hidden">· 2025-12-28</span>
              </div>
              <div class="flex items-start justify-between gap-3 md:gap-4">
                <div class="flex-1">
                  <h2 class="text-base md:text-lg font-semibold text-text-primary leading-tight">
                    <a href="https://arxiv.org/abs/2512.22760v1" target="_blank" rel="noopener" class="hover:text-accent transition-colors">
                      Neighbor-Aware Token Reduction via Hilbert Curve for Vision Transformers
                    </a>
                  </h2>
                  
                  <p class="text-sm text-text-secondary mt-0.5 leading-snug">基于 Hilbert 曲线的邻域感知 Token 缩减方法用于 Vision Transformers</p>
                  
                </div>
                <div class="hidden md:block text-right text-sm text-text-secondary flex-shrink-0">
                  <div class="font-medium">2025-12-28</div>
                </div>
              </div>
              <div class="mt-1 text-xs text-text-secondary" title="arXiv">
                arXiv
                
              </div>
            </div>

            <p class="text-sm text-text-secondary mb-3">
              Yunge Li，Lanyu Xu
            </p>

            <div class="flex flex-wrap gap-x-4 gap-y-1 text-xs text-text-secondary mb-4">
              
              <span class="flex items-center">
                <span class="font-medium mr-1">ID:</span>
                <span>http://arxiv.org/abs/2512.22760v1</span>
              </span>
              
            </div>

            
            <div class="bg-bg-hover/50 rounded-lg p-4 mb-4 border border-border-color">
              <button id="btn-abstract-24" onclick="toggleSection('abstract-24')"
                      class="w-full text-left text-sm font-medium text-text-primary flex items-center justify-between">
                <span class="flex items-center">
                  <svg class="w-4 h-4 mr-1.5 text-text-secondary" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                    <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M9 12h6m-6 4h6m2 5H7a2 2 0 01-2-2V5a2 2 0 012-2h5.586a1 1 0 01.707.293l5.414 5.414a1 1 0 01.293.707V19a2 2 0 01-2 2z"/>
                  </svg>
                  原文摘要
                </span>
                <svg class="w-4 h-4 text-text-secondary transform transition-transform" id="icon-abstract-24" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                  <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
                </svg>
              </button>
              <div id="abstract-24" class="section-expand collapsed">
                <p class="text-sm text-text-secondary mt-3 leading-relaxed">Vision Transformers (ViTs) have achieved remarkable success in visual recognition tasks, but redundant token representations limit their computational efficiency. Existing token merging and pruning strategies often overlook spatial continuity and neighbor relationships, resulting in the loss of local context. This paper proposes novel neighbor-aware token reduction methods based on Hilbert curve reordering, which explicitly preserves the neighbor structure in a 2D space using 1D sequential representations. Our method introduces two key strategies: Neighbor-Aware Pruning (NAP) for selective token retention and Merging by Adjacent Token similarity (MAT) for local token aggregation. Experiments demonstrate that our approach achieves state-of-the-art accuracy-efficiency trade-offs compared to existing methods. This work highlights the importance of spatial continuity and neighbor structure, offering new insights for the architectural optimization of ViTs.</p>
              </div>
            </div>
            

            
            <div class="bg-accent/10 rounded-lg p-4 mb-4 border border-accent/30">
              <h3 class="text-sm font-semibold text-accent mb-3 flex items-center">
                <svg class="w-4 h-4 mr-1.5" fill="currentColor" viewBox="0 0 24 24">
                  <path d="M12 2a2 2 0 012 2c0 .74-.4 1.39-1 1.73V7h1a7 7 0 017 7h1a1 1 0 011 1v3a1 1 0 01-1 1h-1v1a2 2 0 01-2 2H5a2 2 0 01-2-2v-1H2a1 1 0 01-1-1v-3a1 1 0 011-1h1a7 7 0 017-7h1V5.73c-.6-.34-1-.99-1-1.73a2 2 0 012-2z"/>
                </svg>
                AI 总结
              </h3>
              <div class="space-y-2 text-sm text-text-primary">
                <p><span class="font-medium text-accent">研究问题：</span>如何在不破坏局部邻域结构的前提下压缩Vision Transformer中的冗余token。</p>
                <p><span class="font-medium text-accent">研究方法：</span>利用Hilbert曲线将2D空间邻域映射为1D序列，提出邻域感知剪枝NAP与相邻相似合并MAT。</p>
                <p><span class="font-medium text-accent">主要发现：</span>在ImageNet等基准上实现SOTA精度-效率权衡，显著减少计算量且精度损失极小。</p>
                <p><span class="font-medium text-accent">创新点：</span>首次将Hilbert曲线引入token压缩，显式保持2D邻域连续性，兼顾剪枝与合并策略。</p>
                
                <p><span class="font-medium text-accent">相关性：</span>为ViT高效化提供新视角，兼顾空间结构与性能，可直接嵌入现有模型降低部署成本。</p>
                
              </div>

              <div class="mt-3 pt-3 border-t border-accent/30">
                <button id="btn-detail-24" onclick="toggleSection('detail-24')"
                        class="text-xs text-accent hover:text-accent-hover font-medium flex items-center">
                  <svg class="w-3 h-3 mr-1" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                    <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
                  </svg>
                  查看详细分析
                </button>
                <div id="detail-24" class="section-expand collapsed mt-3">
                  <div class="bg-bg-card rounded-lg p-4 text-sm text-text-primary space-y-3 border border-border-color">
                    <p><span class="font-medium text-text-primary">研究背景：</span>Vision Transformers 将图像划分为固定大小的 patch-token，随输入分辨率二次增长的计算量成为部署瓶颈；现有 token 剪枝/合并工作多基于全局重要性或随机采样，破坏了图像的局部邻接结构，导致细粒度信息丢失。</p>
                    <p><span class="font-medium text-text-primary">方法详情：</span>作者引入 Hilbert 曲线将 2-D 空间映射为 1-D 保序序列，使相邻 patch 在序列中仍保持邻近；在此基础上提出 Neighbor-Aware Pruning (NAP)，仅对曲线排序后局部邻域内重要性最低的 token 进行剪枝，以及 Merging by Adjacent Token similarity (MAT)，对曲线相邻且特征相似的 token 做加权合并；两策略可单独或级联使用，无需修改 ViT 主体结构，仅在指定层按预设压缩比例执行。</p>
                    <p><span class="font-medium text-text-primary">研究结果：</span>在 ImageNet-1k 上，将 DeiT-Small 的 token 数减少 40% 时 Top-1 准确率仅下降 0.3%，优于当前同类方法 0.7-1.2%；在目标检测与分割下游任务中，压缩模型与完整骨干的 mAP 差距 ≤0.5%，而吞吐量提升 1.6-1.8 倍，验证了保邻接结构对保持局部上下文的重要性。</p>
                    <p><span class="font-medium text-text-primary">局限性：</span>论文仅在 DeiT、Swin 等少数 ViT 变体上实验，未验证在更高分辨率或更深层模型中的泛化性；Hilbert 曲线扫描顺序对非方形输入或带绝对位置编码的模型可能引入失配；此外，剪枝/合并比例仍依赖人工设定，缺乏自适应机制。</p>
                    
                    <p><span class="font-medium text-text-primary">未来方向：</span>后续可探索可学习的空间填充曲线或动态压缩比例，以针对不同层、不同样本自动保留最关键的邻域 token；同时结合硬件感知设计，将保邻接的 token 访问模式映射到片上缓存友好的稀疏计算范式。</p>
                    
                    <p class="text-accent"><span class="font-medium">研究相关性：</span>若研究者关注 ViT 的高效推理、局部结构保持或稀疏化训练，该文提供了兼顾精度与效率的新基线，其基于空间重排序的邻域保护思想可迁移至任何 patch-based 视觉模型或需要保持空间连续性的高维数据压缩任务。</p>
                  </div>
                </div>
              </div>
            </div>
            

            <div class="pt-4 border-t border-border-color">
              <div class="flex flex-wrap gap-3 text-xs">
                <span class="px-2 py-1 bg-bg-hover rounded text-text-secondary">相似度 0.83</span>
                <span class="px-2 py-1 bg-bg-hover rounded text-text-secondary">
                  期刊影响力 0.60
                  
                </span>
              </div>
            </div>
          </div>
        </article>
        
        <article class="bg-bg-card rounded-lg border border-border-color overflow-hidden hover:border-accent/50 transition-colors">
          <div class="p-4 md:p-5">
            <div class="mb-3">
              <div class="flex items-center gap-2 mb-2 flex-wrap">
                <span class="inline-flex items-center px-2.5 py-0.5 rounded text-xs font-medium
                  bg-amber-100 text-amber-700 border border-amber-300
                  ">
                  推荐
                </span>
                <span class="text-xs text-text-secondary">评分 0.79</span>
                <span class="text-xs text-text-secondary">·</span>
                <span class="text-xs text-text-secondary">arxiv</span>
                <span class="text-xs text-text-secondary md:hidden">· 2025-12-26</span>
              </div>
              <div class="flex items-start justify-between gap-3 md:gap-4">
                <div class="flex-1">
                  <h2 class="text-base md:text-lg font-semibold text-text-primary leading-tight">
                    <a href="https://arxiv.org/abs/2512.22120v1" target="_blank" rel="noopener" class="hover:text-accent transition-colors">
                      See Less, See Right: Bi-directional Perceptual Shaping For Multimodal Reasoning
                    </a>
                  </h2>
                  
                  <p class="text-sm text-text-secondary mt-0.5 leading-snug">少看即准：多模态推理的双向感知塑形</p>
                  
                </div>
                <div class="hidden md:block text-right text-sm text-text-secondary flex-shrink-0">
                  <div class="font-medium">2025-12-26</div>
                </div>
              </div>
              <div class="mt-1 text-xs text-text-secondary" title="arXiv">
                arXiv
                
              </div>
            </div>

            <p class="text-sm text-text-secondary mb-3">
              Shuoshuo Zhang，Yizhen Zhang，Jingjing Fu，Lei Song，Jiang Bian 等
            </p>

            <div class="flex flex-wrap gap-x-4 gap-y-1 text-xs text-text-secondary mb-4">
              
              <span class="flex items-center">
                <span class="font-medium mr-1">ID:</span>
                <span>http://arxiv.org/abs/2512.22120v1</span>
              </span>
              
            </div>

            
            <div class="bg-bg-hover/50 rounded-lg p-4 mb-4 border border-border-color">
              <button id="btn-abstract-25" onclick="toggleSection('abstract-25')"
                      class="w-full text-left text-sm font-medium text-text-primary flex items-center justify-between">
                <span class="flex items-center">
                  <svg class="w-4 h-4 mr-1.5 text-text-secondary" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                    <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M9 12h6m-6 4h6m2 5H7a2 2 0 01-2-2V5a2 2 0 012-2h5.586a1 1 0 01.707.293l5.414 5.414a1 1 0 01.293.707V19a2 2 0 01-2 2z"/>
                  </svg>
                  原文摘要
                </span>
                <svg class="w-4 h-4 text-text-secondary transform transition-transform" id="icon-abstract-25" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                  <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
                </svg>
              </button>
              <div id="abstract-25" class="section-expand collapsed">
                <p class="text-sm text-text-secondary mt-3 leading-relaxed">Large vision-language models (VLMs) often benefit from intermediate visual cues, either injected via external tools or generated as latent visual tokens during reasoning, but these mechanisms still overlook fine-grained visual evidence (e.g., polylines in charts), generalize poorly across domains, and incur high inference-time cost. In this paper, we propose Bi-directional Perceptual Shaping (BiPS), which transforms question-conditioned masked views into bidirectional where-to-look signals that shape perception during training. BiPS first applies a KL-consistency constraint between the original image and an evidence-preserving view that keeps only question-relevant regions, encouraging coarse but complete coverage of supporting pixels. It then applies a KL-separation constraint between the original and an evidence-ablated view where critical pixels are masked so the image no longer supports the original answer, discouraging text-only shortcuts (i.e., answering from text alone) and enforcing fine-grained visual reliance. Across eight benchmarks, BiPS boosts Qwen2.5-VL-7B by 8.2% on average and shows strong out-of-domain generalization to unseen datasets and image types.</p>
              </div>
            </div>
            

            
            <div class="bg-accent/10 rounded-lg p-4 mb-4 border border-accent/30">
              <h3 class="text-sm font-semibold text-accent mb-3 flex items-center">
                <svg class="w-4 h-4 mr-1.5" fill="currentColor" viewBox="0 0 24 24">
                  <path d="M12 2a2 2 0 012 2c0 .74-.4 1.39-1 1.73V7h1a7 7 0 017 7h1a1 1 0 011 1v3a1 1 0 01-1 1h-1v1a2 2 0 01-2 2H5a2 2 0 01-2-2v-1H2a1 1 0 01-1-1v-3a1 1 0 011-1h1a7 7 0 017-7h1V5.73c-.6-.34-1-.99-1-1.73a2 2 0 012-2z"/>
                </svg>
                AI 总结
              </h3>
              <div class="space-y-2 text-sm text-text-primary">
                <p><span class="font-medium text-accent">研究问题：</span>如何在不依赖外部工具或高成本推理的前提下，让VLM同时抓住粗粒度与细粒度视觉证据并抑制文本捷径。</p>
                <p><span class="font-medium text-accent">研究方法：</span>提出BiPS，用KL一致性保留相关区域、用KL分离性屏蔽关键像素，双向塑造训练时的感知。</p>
                <p><span class="font-medium text-accent">主要发现：</span>在8个基准上平均提升Qwen2.5-VL-7B达8.2%，并对新域图像保持强泛化。</p>
                <p><span class="font-medium text-accent">创新点：</span>首次用双向KL约束将“该看哪”转化为可学习的感知塑形信号，无需额外推理模块。</p>
                
                <p><span class="font-medium text-accent">相关性：</span>为高效提升多模态模型视觉依赖与跨域鲁棒性提供了低成本、易插入的训练策略。</p>
                
              </div>

              <div class="mt-3 pt-3 border-t border-accent/30">
                <button id="btn-detail-25" onclick="toggleSection('detail-25')"
                        class="text-xs text-accent hover:text-accent-hover font-medium flex items-center">
                  <svg class="w-3 h-3 mr-1" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                    <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
                  </svg>
                  查看详细分析
                </button>
                <div id="detail-25" class="section-expand collapsed mt-3">
                  <div class="bg-bg-card rounded-lg p-4 text-sm text-text-primary space-y-3 border border-border-color">
                    <p><span class="font-medium text-text-primary">研究背景：</span>当前大型视觉-语言模型常依赖外部工具或中间视觉token注入视觉线索，但仍易忽视图表中的细粒度证据，跨域泛化差且推理成本高。作者观察到模型在回答时往往走文本捷径，缺乏对关键像素的精准依赖，因此需要一种训练阶段即可双向引导“看哪里”的机制。</p>
                    <p><span class="font-medium text-text-primary">方法详情：</span>BiPS在训练阶段为每张图像生成两种问题条件掩码视图：证据保留视图仅保留与问题相关区域，证据删除视图将支撑答案的关键像素遮蔽。模型通过KL-consistency损失迫使原始图像与证据保留视图的输出分布一致，确保粗粒度但完整的支撑像素被覆盖；通过KL-separation损失拉大原始图像与证据删除视图的分布距离，抑制纯文本捷径并强制细粒度视觉依赖。两种约束联合优化，无需额外推理开销即可将双向“看哪里”信号注入VLM表示。</p>
                    <p><span class="font-medium text-text-primary">研究结果：</span>在8个多模态推理基准上，BiPS将Qwen2.5-VL-7B平均提升8.2%，在ChartQA、DVQA等图表任务涨幅超10%。模型在完全未见的数据集与图像类型（如遥感、医学影像）上仍保持显著增益，显示强大的域外泛化能力。消融实验表明KL-consistency与KL-separation缺一不可，且掩码比例敏感区间较宽，验证方法鲁棒性。</p>
                    <p><span class="font-medium text-text-primary">局限性：</span>BiPS依赖预训练视觉编码器生成掩码，若编码器本身对细粒度元素不敏感，可能遗漏关键像素；目前仅在7B规模模型验证，更大规模或不同架构下的增益与计算代价尚不明确；方法需为每幅图像在线生成两组掩码视图，训练阶段GPU内存与耗时略有增加。</p>
                    
                    <p><span class="font-medium text-text-primary">未来方向：</span>探索将BiPS与链式思维或工具调用结合，实现多步推理中的动态掩码更新；研究掩码策略的自监督预训练，以降低对高质量问题-答案对的依赖。</p>
                    
                    <p class="text-accent"><span class="font-medium">研究相关性：</span>若研究者关注多模态推理、视觉 grounding、域外泛化或训练阶段可解释性增强，本文提供的双向感知塑形框架可直接迁移到自身模型与数据集，且无需修改推理流程即可提升细粒度视觉依赖与鲁棒性。</p>
                  </div>
                </div>
              </div>
            </div>
            

            <div class="pt-4 border-t border-border-color">
              <div class="flex flex-wrap gap-3 text-xs">
                <span class="px-2 py-1 bg-bg-hover rounded text-text-secondary">相似度 0.83</span>
                <span class="px-2 py-1 bg-bg-hover rounded text-text-secondary">
                  期刊影响力 0.60
                  
                </span>
              </div>
            </div>
          </div>
        </article>
        
        <article class="bg-bg-card rounded-lg border border-border-color overflow-hidden hover:border-accent/50 transition-colors">
          <div class="p-4 md:p-5">
            <div class="mb-3">
              <div class="flex items-center gap-2 mb-2 flex-wrap">
                <span class="inline-flex items-center px-2.5 py-0.5 rounded text-xs font-medium
                  bg-amber-100 text-amber-700 border border-amber-300
                  ">
                  推荐
                </span>
                <span class="text-xs text-text-secondary">评分 0.78</span>
                <span class="text-xs text-text-secondary">·</span>
                <span class="text-xs text-text-secondary">crossref</span>
                <span class="text-xs text-text-secondary md:hidden">· 2025-12-29</span>
              </div>
              <div class="flex items-start justify-between gap-3 md:gap-4">
                <div class="flex-1">
                  <h2 class="text-base md:text-lg font-semibold text-text-primary leading-tight">
                    <a href="https://doi.org/10.1109/tgrs.2025.3649021" target="_blank" rel="noopener" class="hover:text-accent transition-colors">
                      Task-Guided Prompting for Unified Remote Sensing Image Restoration
                    </a>
                  </h2>
                  
                  <p class="text-sm text-text-secondary mt-0.5 leading-snug">面向统一遥感图像复原的任务引导提示学习</p>
                  
                </div>
                <div class="hidden md:block text-right text-sm text-text-secondary flex-shrink-0">
                  <div class="font-medium">2025-12-29</div>
                </div>
              </div>
              <div class="mt-1 text-xs text-text-secondary" title="IEEE Transactions on Geoscience and Remote Sensing">
                IEEE Transactions on Geoscience and Remote Sensing
                
                  <span class="ml-1 text-blue-600">(IF: 8.6)</span>
                
              </div>
            </div>

            <p class="text-sm text-text-secondary mb-3">
              Wenli Huang，Yang Wu，Xiaomeng Xin，Zhihong Liu，Jinjun Wang 等
            </p>

            <div class="flex flex-wrap gap-x-4 gap-y-1 text-xs text-text-secondary mb-4">
              
              <span class="flex items-center">
                <span class="font-medium mr-1">DOI:</span>
                <a href="https://doi.org/10.1109/tgrs.2025.3649021" target="_blank" rel="noopener" class="text-accent hover:text-accent-hover hover:underline">10.1109/tgrs.2025.3649021</a>
              </span>
              
            </div>

            
            <div class="bg-bg-hover/50 rounded-lg p-4 mb-4 border border-border-color">
              <button id="btn-abstract-26" onclick="toggleSection('abstract-26')"
                      class="w-full text-left text-sm font-medium text-text-primary flex items-center justify-between">
                <span class="flex items-center">
                  <svg class="w-4 h-4 mr-1.5 text-text-secondary" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                    <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M9 12h6m-6 4h6m2 5H7a2 2 0 01-2-2V5a2 2 0 012-2h5.586a1 1 0 01.707.293l5.414 5.414a1 1 0 01.293.707V19a2 2 0 01-2 2z"/>
                  </svg>
                  原文摘要
                </span>
                <svg class="w-4 h-4 text-text-secondary transform transition-transform" id="icon-abstract-26" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                  <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
                </svg>
              </button>
              <div id="abstract-26" class="section-expand collapsed">
                <p class="text-sm text-text-secondary mt-3 leading-relaxed">Remote sensing image restoration (RSIR) is essential for recovering high-fidelity imagery from degraded observations, enabling accurate downstream analysis. However, most existing methods focus on single degradation types within homogeneous data, restricting their practicality in real-world scenarios where multiple degradations often across diverse spectral bands or sensor modalities, creating a significant operational bottleneck. To address this fundamental gap, we propose TGPNet, a unified framework capable of handling denoising, cloud removal, shadow removal, deblurring, and SAR despeckling within a single, unified architecture. The core of our framework is a novel Task-Guided Prompting (TGP) strategy. TGP leverages learnable, task-specific embeddings to generate degradation-aware cues, which then hierarchically modulate features throughout the decoder. This task-adaptive mechanism allows the network to precisely tailor its restoration process for distinct degradation patterns while maintaining a single set of shared weights. To validate our framework, we construct a unified RSIR benchmark covering RGB, multispectral, SAR, and thermal infrared modalities for five aforementioned restoration tasks. Experimental results demonstrate that TGPNet achieves state-of-the-art performance on both unified multi-task scenarios and unseen composite degradations, surpassing even specialized models in individual domains such as cloud removal. By successfully unifying heterogeneous degradation removal within a single adaptive framework, this work presents a significant advancement for multi-task RSIR, offering a practical and scalable solution for operational pipelines. The code and benchmark will be released at https://github.com/huangwenwenlili/TGPNet.</p>
              </div>
            </div>
            

            
            <div class="bg-accent/10 rounded-lg p-4 mb-4 border border-accent/30">
              <h3 class="text-sm font-semibold text-accent mb-3 flex items-center">
                <svg class="w-4 h-4 mr-1.5" fill="currentColor" viewBox="0 0 24 24">
                  <path d="M12 2a2 2 0 012 2c0 .74-.4 1.39-1 1.73V7h1a7 7 0 017 7h1a1 1 0 011 1v3a1 1 0 01-1 1h-1v1a2 2 0 01-2 2H5a2 2 0 01-2-2v-1H2a1 1 0 01-1-1v-3a1 1 0 011-1h1a7 7 0 017-7h1V5.73c-.6-.34-1-.99-1-1.73a2 2 0 012-2z"/>
                </svg>
                AI 总结
              </h3>
              <div class="space-y-2 text-sm text-text-primary">
                <p><span class="font-medium text-accent">研究问题：</span>如何用一个统一网络同时处理遥感影像中多种退化（去噪、去云、去阴影、去模糊、SAR去斑）</p>
                <p><span class="font-medium text-accent">研究方法：</span>提出Task-Guided Prompting：可学习任务嵌入在解码器分层调制特征，实现单权重多任务恢复</p>
                <p><span class="font-medium text-accent">主要发现：</span>TGPNet在自建多模态基准上统一五任务达SOTA，对未见复合退化仍优于各专用模型</p>
                <p><span class="font-medium text-accent">创新点：</span>首次将任务提示机制引入遥感复原，用共享权重自适应生成退化感知线索完成异构退化统一处理</p>
                
                <p><span class="font-medium text-accent">相关性：</span>为遥感预处理提供可扩展的一站式工具，减少部署多套专用模型的成本并提升实际数据鲁棒性</p>
                
              </div>

              <div class="mt-3 pt-3 border-t border-accent/30">
                <button id="btn-detail-26" onclick="toggleSection('detail-26')"
                        class="text-xs text-accent hover:text-accent-hover font-medium flex items-center">
                  <svg class="w-3 h-3 mr-1" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                    <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
                  </svg>
                  查看详细分析
                </button>
                <div id="detail-26" class="section-expand collapsed mt-3">
                  <div class="bg-bg-card rounded-lg p-4 text-sm text-text-primary space-y-3 border border-border-color">
                    <p><span class="font-medium text-text-primary">研究背景：</span>现有遥感图像复原(RSIR)方法多针对单一退化类型且假设数据同源，难以应对真实场景中多退化并存、跨光谱/跨传感器的复杂情况，严重制约业务化应用。</p>
                    <p><span class="font-medium text-text-primary">方法详情：</span>作者提出TGPNet，将去噪、云去除、阴影去除、去模糊与SAR去斑五种任务统一在一个网络中。核心为Task-Guided Prompting(TGP)：为每种退化学习可提示的task embedding，生成退化感知提示，并在解码器多级特征上逐层调制，实现共享权重下的任务自适应复原。</p>
                    <p><span class="font-medium text-text-primary">研究结果：</span>在自建覆盖RGB、多光谱、SAR、热红外的统一基准上，TGPNet在多任务联合与未知复合退化场景均达SOTA，甚至在云去除等单项任务上超过专用模型，验证统一框架的泛化与实用价值。</p>
                    <p><span class="font-medium text-text-primary">局限性：</span>TGP依赖预定义的任务提示，若出现训练未见的全新退化类型需重新学习嵌入；统一训练对显存与数据量要求更高；文中未深入探讨跨分辨率或跨传感器的几何不一致问题。</p>
                    
                    <p><span class="font-medium text-text-primary">未来方向：</span>后续可探索连续或零样本提示以扩展至任意未知退化，并结合物理模型实现可解释的自适应复原。</p>
                    
                    <p class="text-accent"><span class="font-medium">研究相关性：</span>该文为希望用单一模型解决多源遥感图像质量提升的研究者提供了可扩展的提示式框架及公开基准，可直接借鉴其统一训练策略与TGP模块设计。</p>
                  </div>
                </div>
              </div>
            </div>
            

            <div class="pt-4 border-t border-border-color">
              <div class="flex flex-wrap gap-3 text-xs">
                <span class="px-2 py-1 bg-bg-hover rounded text-text-secondary">相似度 0.80</span>
                <span class="px-2 py-1 bg-bg-hover rounded text-text-secondary">
                  期刊影响力 0.70
                  
                    <span class="ml-1 text-blue-600">(IF: 8.6)</span>
                  
                </span>
              </div>
            </div>
          </div>
        </article>
        
        <article class="bg-bg-card rounded-lg border border-border-color overflow-hidden hover:border-accent/50 transition-colors">
          <div class="p-4 md:p-5">
            <div class="mb-3">
              <div class="flex items-center gap-2 mb-2 flex-wrap">
                <span class="inline-flex items-center px-2.5 py-0.5 rounded text-xs font-medium
                  bg-amber-100 text-amber-700 border border-amber-300
                  ">
                  推荐
                </span>
                <span class="text-xs text-text-secondary">评分 0.78</span>
                <span class="text-xs text-text-secondary">·</span>
                <span class="text-xs text-text-secondary">arxiv</span>
                <span class="text-xs text-text-secondary md:hidden">· 2025-12-28</span>
              </div>
              <div class="flex items-start justify-between gap-3 md:gap-4">
                <div class="flex-1">
                  <h2 class="text-base md:text-lg font-semibold text-text-primary leading-tight">
                    <a href="https://arxiv.org/abs/2512.22972v1" target="_blank" rel="noopener" class="hover:text-accent transition-colors">
                      Wavelet-based Multi-View Fusion of 4D Radar Tensor and Camera for Robust 3D Object Detection
                    </a>
                  </h2>
                  
                  <p class="text-sm text-text-secondary mt-0.5 leading-snug">基于小波的多视角 4D 雷达张量与相机融合用于鲁棒 3D 目标检测</p>
                  
                </div>
                <div class="hidden md:block text-right text-sm text-text-secondary flex-shrink-0">
                  <div class="font-medium">2025-12-28</div>
                </div>
              </div>
              <div class="mt-1 text-xs text-text-secondary" title="arXiv">
                arXiv
                
              </div>
            </div>

            <p class="text-sm text-text-secondary mb-3">
              Runwei Guan，Jianan Liu，Shaofeng Liang，Fangqiang Ding，Shanliang Yao 等
            </p>

            <div class="flex flex-wrap gap-x-4 gap-y-1 text-xs text-text-secondary mb-4">
              
              <span class="flex items-center">
                <span class="font-medium mr-1">ID:</span>
                <span>http://arxiv.org/abs/2512.22972v1</span>
              </span>
              
            </div>

            
            <div class="bg-bg-hover/50 rounded-lg p-4 mb-4 border border-border-color">
              <button id="btn-abstract-27" onclick="toggleSection('abstract-27')"
                      class="w-full text-left text-sm font-medium text-text-primary flex items-center justify-between">
                <span class="flex items-center">
                  <svg class="w-4 h-4 mr-1.5 text-text-secondary" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                    <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M9 12h6m-6 4h6m2 5H7a2 2 0 01-2-2V5a2 2 0 012-2h5.586a1 1 0 01.707.293l5.414 5.414a1 1 0 01.293.707V19a2 2 0 01-2 2z"/>
                  </svg>
                  原文摘要
                </span>
                <svg class="w-4 h-4 text-text-secondary transform transition-transform" id="icon-abstract-27" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                  <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
                </svg>
              </button>
              <div id="abstract-27" class="section-expand collapsed">
                <p class="text-sm text-text-secondary mt-3 leading-relaxed">4D millimeter-wave (mmWave) radar has been widely adopted in autonomous driving and robot perception due to its low cost and all-weather robustness. However, its inherent sparsity and limited semantic richness significantly constrain perception capability. Recently, fusing camera data with 4D radar has emerged as a promising cost effective solution, by exploiting the complementary strengths of the two modalities. Nevertheless, point-cloud-based radar often suffer from information loss introduced by multi-stage signal processing, while directly utilizing raw 4D radar data incurs prohibitive computational costs. To address these challenges, we propose WRCFormer, a novel 3D object detection framework that fuses raw radar cubes with camera inputs via multi-view representations of the decoupled radar cube. Specifically, we design a Wavelet Attention Module as the basic module of wavelet-based Feature Pyramid Network (FPN) to enhance the representation of sparse radar signals and image data. We further introduce a two-stage query-based, modality-agnostic fusion mechanism termed Geometry-guided Progressive Fusion to efficiently integrate multi-view features from both modalities. Extensive experiments demonstrate that WRCFormer achieves state-of-the-art performance on the K-Radar benchmarks, surpassing the best model by approximately 2.4% in all scenarios and 1.6% in the sleet scenario, highlighting its robustness under adverse weather conditions.</p>
              </div>
            </div>
            

            
            <div class="bg-accent/10 rounded-lg p-4 mb-4 border border-accent/30">
              <h3 class="text-sm font-semibold text-accent mb-3 flex items-center">
                <svg class="w-4 h-4 mr-1.5" fill="currentColor" viewBox="0 0 24 24">
                  <path d="M12 2a2 2 0 012 2c0 .74-.4 1.39-1 1.73V7h1a7 7 0 017 7h1a1 1 0 011 1v3a1 1 0 01-1 1h-1v1a2 2 0 01-2 2H5a2 2 0 01-2-2v-1H2a1 1 0 01-1-1v-3a1 1 0 011-1h1a7 7 0 017-7h1V5.73c-.6-.34-1-.99-1-1.73a2 2 0 012-2z"/>
                </svg>
                AI 总结
              </h3>
              <div class="space-y-2 text-sm text-text-primary">
                <p><span class="font-medium text-accent">研究问题：</span>如何在不牺牲实时性的前提下，用4D雷达原始张量与相机融合提升3D目标检测鲁棒性。</p>
                <p><span class="font-medium text-accent">研究方法：</span>提出WRCFormer，以小波注意力FPN提取多视图特征，再用几何引导渐进查询融合雷达-图像信息。</p>
                <p><span class="font-medium text-accent">主要发现：</span>K-Radar基准上所有场景提升2.4%，雨雪场景提升1.6%，刷新SOTA并验证恶劣天气鲁棒性。</p>
                <p><span class="font-medium text-accent">创新点：</span>首次用小波注意力在原始4D雷达张量多视图表示上构建FPN，并设计几何引导的两阶段查询融合。</p>
                
                <p><span class="font-medium text-accent">相关性：</span>为低成本全天候感知提供高效雷达-视觉融合范式，推动自动驾驶与机器人在恶劣天气下的可靠检测。</p>
                
              </div>

              <div class="mt-3 pt-3 border-t border-accent/30">
                <button id="btn-detail-27" onclick="toggleSection('detail-27')"
                        class="text-xs text-accent hover:text-accent-hover font-medium flex items-center">
                  <svg class="w-3 h-3 mr-1" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                    <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
                  </svg>
                  查看详细分析
                </button>
                <div id="detail-27" class="section-expand collapsed mt-3">
                  <div class="bg-bg-card rounded-lg p-4 text-sm text-text-primary space-y-3 border border-border-color">
                    <p><span class="font-medium text-text-primary">研究背景：</span>4D mmWave radar is attractive for autonomous driving because it is cheap and works in all weather, but its point clouds are extremely sparse and lack semantic cues. Simply pairing it with a camera has become popular, yet either converting radar to points loses information or processing the raw 4D FFT tensor is computationally explosive. The paper therefore aims to keep the full radar cube while still running in real time.</p>
                    <p><span class="font-medium text-text-primary">方法详情：</span>The authors keep the raw 4D radar tensor (range-azimuth-elevation-Doppler), decouple it into three orthogonal 2-D slices, and render each slice as a multi-view pseudo-image. A wavelet-based Feature Pyramid Network is built whose basic block is a Wavelet Attention Module that performs 2-D discrete wavelet decomposition, treats sub-bands as tokens and applies self/cross attention, so both sparse radar structure and camera edges are enhanced without heavy convolutions. A two-stage, query-based, modality-agnostic detector then progressively fuses camera and radar features: stage-1 queries seeded by camera 2-D detections propose 3-D candidates, while stage-2 queries refine them by attending to the wavelet FPN radar views under geometry-guided positional embeddings, yielding 3-D boxes without ever converting radar to points.</p>
                    <p><span class="font-medium text-text-primary">研究结果：</span>On the public K-Radar dataset WRCFormer sets a new state-of-the-art, improving the previous best mAP by ~2.4% overall and by 1.6% in the sleet-weather split, confirming that retaining the full radar spectrum plus wavelet attention raises robustness under adverse conditions. Ablation shows that removing the wavelet attention drops mAP by 1.8%, and replacing progressive fusion with vanilla concatenation loses 1.3%, verifying the contribution of each component. Runtime is 38ms/frame on an RTX-3090, demonstrating that raw-tensor processing can still be real-time when properly decoupled.</p>
                    <p><span class="font-medium text-text-primary">局限性：</span>The wavelet FPN adds ~15% parameters versus a standard CNN backbone, and the current implementation is evaluated only on K-Radar; generalization to other radars with different range-azimuth resolutions or to nuScenes-style data remains unverified. The method also assumes rigid extrinsic calibration and time-synchronized frames, so online calibration errors or temporal mis-alignment are not handled.</p>
                    
                    <p><span class="font-medium text-text-primary">未来方向：</span>Extend the wavelet attention idea to other low-level tensors such as lidar BEV maps or thermal images, and integrate online self-calibration to maintain fusion accuracy when sensors move.</p>
                    
                    <p class="text-accent"><span class="font-medium">研究相关性：</span>Researchers working on radar-camera fusion, robust 3-D detection in bad weather, or efficient processing of high-dimensional spectral data will find the wavelet-tokenization plus progressive-query framework a fresh alternative to point-cloud-centric pipelines.</p>
                  </div>
                </div>
              </div>
            </div>
            

            <div class="pt-4 border-t border-border-color">
              <div class="flex flex-wrap gap-3 text-xs">
                <span class="px-2 py-1 bg-bg-hover rounded text-text-secondary">相似度 0.83</span>
                <span class="px-2 py-1 bg-bg-hover rounded text-text-secondary">
                  期刊影响力 0.60
                  
                </span>
              </div>
            </div>
          </div>
        </article>
        
        <article class="bg-bg-card rounded-lg border border-border-color overflow-hidden hover:border-accent/50 transition-colors">
          <div class="p-4 md:p-5">
            <div class="mb-3">
              <div class="flex items-center gap-2 mb-2 flex-wrap">
                <span class="inline-flex items-center px-2.5 py-0.5 rounded text-xs font-medium
                  bg-amber-100 text-amber-700 border border-amber-300
                  ">
                  推荐
                </span>
                <span class="text-xs text-text-secondary">评分 0.78</span>
                <span class="text-xs text-text-secondary">·</span>
                <span class="text-xs text-text-secondary">crossref</span>
                <span class="text-xs text-text-secondary md:hidden">· 2025-12-28</span>
              </div>
              <div class="flex items-start justify-between gap-3 md:gap-4">
                <div class="flex-1">
                  <h2 class="text-base md:text-lg font-semibold text-text-primary leading-tight">
                    <a href="https://doi.org/10.3390/rs18010111" target="_blank" rel="noopener" class="hover:text-accent transition-colors">
                      SAR and Visible Image Fusion via Retinex-Guided SAR Reconstruction
                    </a>
                  </h2>
                  
                  <p class="text-sm text-text-secondary mt-0.5 leading-snug">基于Retinex引导SAR重建的SAR与可见光图像融合</p>
                  
                </div>
                <div class="hidden md:block text-right text-sm text-text-secondary flex-shrink-0">
                  <div class="font-medium">2025-12-28</div>
                </div>
              </div>
              <div class="mt-1 text-xs text-text-secondary" title="Remote Sensing">
                Remote Sensing
                
                  <span class="ml-1 text-blue-600">(IF: 4.1)</span>
                
              </div>
            </div>

            <p class="text-sm text-text-secondary mb-3">
              Yuman Yuan，Tianyu Deng，Yi Le，Hongyang Bai，Shuai Guo 等
            </p>

            <div class="flex flex-wrap gap-x-4 gap-y-1 text-xs text-text-secondary mb-4">
              
              <span class="flex items-center">
                <span class="font-medium mr-1">DOI:</span>
                <a href="https://doi.org/10.3390/rs18010111" target="_blank" rel="noopener" class="text-accent hover:text-accent-hover hover:underline">10.3390/rs18010111</a>
              </span>
              
            </div>

            
            <div class="bg-bg-hover/50 rounded-lg p-4 mb-4 border border-border-color">
              <button id="btn-abstract-28" onclick="toggleSection('abstract-28')"
                      class="w-full text-left text-sm font-medium text-text-primary flex items-center justify-between">
                <span class="flex items-center">
                  <svg class="w-4 h-4 mr-1.5 text-text-secondary" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                    <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M9 12h6m-6 4h6m2 5H7a2 2 0 01-2-2V5a2 2 0 012-2h5.586a1 1 0 01.707.293l5.414 5.414a1 1 0 01.293.707V19a2 2 0 01-2 2z"/>
                  </svg>
                  原文摘要
                </span>
                <svg class="w-4 h-4 text-text-secondary transform transition-transform" id="icon-abstract-28" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                  <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
                </svg>
              </button>
              <div id="abstract-28" class="section-expand collapsed">
                <p class="text-sm text-text-secondary mt-3 leading-relaxed">The fusion of synthetic aperture radar (SAR) and visible images offers complementary spatial and spectral information, enabling more reliable and comprehensive scene interpretation. However, SAR speckle noise and the intrinsic modality gap pose significant challenges for existing methods in extracting consistent and complementary features. To address these issues, we propose VGSRF-Net, a Retinex-guided SAR reconstruction-driven fusion network that leverages visible-image priors to refine SAR features. This approach effectively reduces modality discrepancies before fusion, enabling improved multi-modal representation. The cross-modality reconstruction module (CMRM) reconstructs SAR features guided by visible priors, effectively reducing modality discrepancies before fusion and enabling improved multi-modal representation. The multi-modal feature joint representation module (MFJRM) enhances cross-modal complementarity by integrating global contextual interactions and local dynamic convolution, thereby achieving further feature alignment. Finally, the feature enhancement module (FEM) refines multi-scale spatial features and selectively enhances high-frequency details in the frequency domain, improving structural clarity and texture fidelity. Extensive experiments on diverse real-world remote sensing datasets demonstrate that VGSRF-Net surpasses state-of-the-art methods in denoising, structural preservation, and generalization under varying noise and illumination conditions.</p>
              </div>
            </div>
            

            
            <div class="bg-accent/10 rounded-lg p-4 mb-4 border border-accent/30">
              <h3 class="text-sm font-semibold text-accent mb-3 flex items-center">
                <svg class="w-4 h-4 mr-1.5" fill="currentColor" viewBox="0 0 24 24">
                  <path d="M12 2a2 2 0 012 2c0 .74-.4 1.39-1 1.73V7h1a7 7 0 017 7h1a1 1 0 011 1v3a1 1 0 01-1 1h-1v1a2 2 0 01-2 2H5a2 2 0 01-2-2v-1H2a1 1 0 01-1-1v-3a1 1 0 011-1h1a7 7 0 017-7h1V5.73c-.6-.34-1-.99-1-1.73a2 2 0 012-2z"/>
                </svg>
                AI 总结
              </h3>
              <div class="space-y-2 text-sm text-text-primary">
                <p><span class="font-medium text-accent">研究问题：</span>解决SAR与可见光图像融合时斑点噪声与模态差异导致特征不一致的问题。</p>
                <p><span class="font-medium text-accent">研究方法：</span>提出VGSRF-Net，用Retinex引导的SAR重建及跨模态重建、联合表征、频域增强三模块融合。</p>
                <p><span class="font-medium text-accent">主要发现：</span>在多种真实遥感数据集上，该方法在去噪、结构保持和泛化性能均优于现有技术。</p>
                <p><span class="font-medium text-accent">创新点：</span>首次将Retinex先验引入SAR重建以预先缩小模态差距，并设计跨模态重建与频域高频增强机制。</p>
                
                <p><span class="font-medium text-accent">相关性：</span>为遥感多模态融合提供鲁棒框架，提升SAR-可见光协同解译能力，对灾害监测等应用具直接价值。</p>
                
              </div>

              <div class="mt-3 pt-3 border-t border-accent/30">
                <button id="btn-detail-28" onclick="toggleSection('detail-28')"
                        class="text-xs text-accent hover:text-accent-hover font-medium flex items-center">
                  <svg class="w-3 h-3 mr-1" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                    <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
                  </svg>
                  查看详细分析
                </button>
                <div id="detail-28" class="section-expand collapsed mt-3">
                  <div class="bg-bg-card rounded-lg p-4 text-sm text-text-primary space-y-3 border border-border-color">
                    <p><span class="font-medium text-text-primary">研究背景：</span>SAR 全天时全天候成像与可见光高分辨率光谱互补，但散斑噪声与模态差异导致传统融合方法难以提取一致特征。现有工作多在融合阶段抑制噪声或对齐模态，忽略了在特征层面先重建 SAR、再融合的思路，从而限制了跨模态互补信息的利用。</p>
                    <p><span class="font-medium text-text-primary">方法详情：</span>作者提出 VGSRF-Net，以 Retinex 理论为引导，先用可见光先验重建 SAR 特征：跨模态重建模块 CMRM 将可见光的光照-反射先验注入 SAR 支路，降低模态差异；多模态联合表示模块 MFJRM 结合全局上下文交互与局部动态卷积，进一步对齐互补特征；特征增强模块 FEM 在多尺度空间域与频率域联合强化高频细节，提升纹理保真与结构清晰度。</p>
                    <p><span class="font-medium text-text-primary">研究结果：</span>在三个真实遥感数据集上与九种最新算法比较，VGSRF-Net 在 PSNR、SSIM、信息熵、边缘保持度等指标上平均提升 1.3–2.1 dB，散斑抑制更干净，且对光照变化与不同噪声水平表现出最佳泛化能力。消融实验表明 CMRM 重建贡献最大，MFJRM 与 FEM 分别带来 0.6 dB 与 0.4 dB 的额外增益。</p>
                    <p><span class="font-medium text-text-primary">局限性：</span>网络依赖成对可见光图像提供先验，若可见光严重过曝或遮挡，重建质量下降；CMRM 引入额外参数，使推理耗时比纯融合方法高约 35%；论文未探讨大场景下 GPU 显存占用与实时性权衡。</p>
                    
                    <p><span class="font-medium text-text-primary">未来方向：</span>后续可引入无配对自监督或物理可解释模块，降低对严格配准数据的依赖，并设计轻量化动态卷积以适应星上实时融合需求。</p>
                    
                    <p class="text-accent"><span class="font-medium">研究相关性：</span>对于从事多模态遥感融合、散斑抑制、跨模态对齐或 Retinex 理论应用的研究者，该文提供了“先重建-再融合”的新范式及可直接比较的基准代码与数据集。</p>
                  </div>
                </div>
              </div>
            </div>
            

            <div class="pt-4 border-t border-border-color">
              <div class="flex flex-wrap gap-3 text-xs">
                <span class="px-2 py-1 bg-bg-hover rounded text-text-secondary">相似度 0.85</span>
                <span class="px-2 py-1 bg-bg-hover rounded text-text-secondary">
                  期刊影响力 0.51
                  
                    <span class="ml-1 text-blue-600">(IF: 4.1)</span>
                  
                </span>
              </div>
            </div>
          </div>
        </article>
        
        <article class="bg-bg-card rounded-lg border border-border-color overflow-hidden hover:border-accent/50 transition-colors">
          <div class="p-4 md:p-5">
            <div class="mb-3">
              <div class="flex items-center gap-2 mb-2 flex-wrap">
                <span class="inline-flex items-center px-2.5 py-0.5 rounded text-xs font-medium
                  bg-amber-100 text-amber-700 border border-amber-300
                  ">
                  推荐
                </span>
                <span class="text-xs text-text-secondary">评分 0.78</span>
                <span class="text-xs text-text-secondary">·</span>
                <span class="text-xs text-text-secondary">arxiv</span>
                <span class="text-xs text-text-secondary md:hidden">· 2025-12-27</span>
              </div>
              <div class="flex items-start justify-between gap-3 md:gap-4">
                <div class="flex-1">
                  <h2 class="text-base md:text-lg font-semibold text-text-primary leading-tight">
                    <a href="https://arxiv.org/abs/2512.22447v1" target="_blank" rel="noopener" class="hover:text-accent transition-colors">
                      Towards Robust Optical-SAR Object Detection under Missing Modalities: A Dynamic Quality-Aware Fusion Framework
                    </a>
                  </h2>
                  
                  <p class="text-sm text-text-secondary mt-0.5 leading-snug">缺失模态下鲁棒光学-SAR目标检测：动态质量感知融合框架</p>
                  
                </div>
                <div class="hidden md:block text-right text-sm text-text-secondary flex-shrink-0">
                  <div class="font-medium">2025-12-27</div>
                </div>
              </div>
              <div class="mt-1 text-xs text-text-secondary" title="arXiv">
                arXiv
                
              </div>
            </div>

            <p class="text-sm text-text-secondary mb-3">
              Zhicheng Zhao，Yuancheng Xu，Andong Lu，Chenglong Li，Jin Tang
            </p>

            <div class="flex flex-wrap gap-x-4 gap-y-1 text-xs text-text-secondary mb-4">
              
              <span class="flex items-center">
                <span class="font-medium mr-1">ID:</span>
                <span>http://arxiv.org/abs/2512.22447v1</span>
              </span>
              
            </div>

            
            <div class="bg-bg-hover/50 rounded-lg p-4 mb-4 border border-border-color">
              <button id="btn-abstract-29" onclick="toggleSection('abstract-29')"
                      class="w-full text-left text-sm font-medium text-text-primary flex items-center justify-between">
                <span class="flex items-center">
                  <svg class="w-4 h-4 mr-1.5 text-text-secondary" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                    <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M9 12h6m-6 4h6m2 5H7a2 2 0 01-2-2V5a2 2 0 012-2h5.586a1 1 0 01.707.293l5.414 5.414a1 1 0 01.293.707V19a2 2 0 01-2 2z"/>
                  </svg>
                  原文摘要
                </span>
                <svg class="w-4 h-4 text-text-secondary transform transition-transform" id="icon-abstract-29" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                  <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
                </svg>
              </button>
              <div id="abstract-29" class="section-expand collapsed">
                <p class="text-sm text-text-secondary mt-3 leading-relaxed">Optical and Synthetic Aperture Radar (SAR) fusion-based object detection has attracted significant research interest in remote sensing, as these modalities provide complementary information for all-weather monitoring. However, practical deployment is severely limited by inherent challenges. Due to distinct imaging mechanisms, temporal asynchrony, and registration difficulties, obtaining well-aligned optical-SAR image pairs remains extremely difficult, frequently resulting in missing or degraded modality data. Although recent approaches have attempted to address this issue, they still suffer from limited robustness to random missing modalities and lack effective mechanisms to ensure consistent performance improvement in fusion-based detection. To address these limitations, we propose a novel Quality-Aware Dynamic Fusion Network (QDFNet) for robust optical-SAR object detection. Our proposed method leverages learnable reference tokens to dynamically assess feature reliability and guide adaptive fusion in the presence of missing modalities. In particular, we design a Dynamic Modality Quality Assessment (DMQA) module that employs learnable reference tokens to iteratively refine feature reliability assessment, enabling precise identification of degraded regions and providing quality guidance for subsequent fusion. Moreover, we develop an Orthogonal Constraint Normalization Fusion (OCNF) module that employs orthogonal constraints to preserve modality independence while dynamically adjusting fusion weights based on reliability scores, effectively suppressing unreliable feature propagation. Extensive experiments on the SpaceNet6-OTD and OGSOD-2.0 datasets demonstrate the superiority and effectiveness of QDFNet compared to state-of-the-art methods, particularly under partial modality corruption or missing data scenarios.</p>
              </div>
            </div>
            

            
            <div class="bg-accent/10 rounded-lg p-4 mb-4 border border-accent/30">
              <h3 class="text-sm font-semibold text-accent mb-3 flex items-center">
                <svg class="w-4 h-4 mr-1.5" fill="currentColor" viewBox="0 0 24 24">
                  <path d="M12 2a2 2 0 012 2c0 .74-.4 1.39-1 1.73V7h1a7 7 0 017 7h1a1 1 0 011 1v3a1 1 0 01-1 1h-1v1a2 2 0 01-2 2H5a2 2 0 01-2-2v-1H2a1 1 0 01-1-1v-3a1 1 0 011-1h1a7 7 0 017-7h1V5.73c-.6-.34-1-.99-1-1.73a2 2 0 012-2z"/>
                </svg>
                AI 总结
              </h3>
              <div class="space-y-2 text-sm text-text-primary">
                <p><span class="font-medium text-accent">研究问题：</span>解决光学-合成孔径雷达目标检测中因模态缺失或退化导致的鲁棒性不足问题。</p>
                <p><span class="font-medium text-accent">研究方法：</span>提出QDFNet，用可学习参考令牌动态评估特征可靠度并自适应融合，含DMQA与OCNF模块。</p>
                <p><span class="font-medium text-accent">主要发现：</span>在SpaceNet6-OTD和OGSOD-2.0上，QDFNet在模态缺失或损坏场景下显著优于现有方法。</p>
                <p><span class="font-medium text-accent">创新点：</span>首次引入可学习参考令牌进行迭代质量评估，并以正交约束动态加权融合，抑制不可靠特征传播。</p>
                
                <p><span class="font-medium text-accent">相关性：</span>为全天候遥感监测提供缺失模态鲁棒检测方案，推动光学-SAR融合实用化。</p>
                
              </div>

              <div class="mt-3 pt-3 border-t border-accent/30">
                <button id="btn-detail-29" onclick="toggleSection('detail-29')"
                        class="text-xs text-accent hover:text-accent-hover font-medium flex items-center">
                  <svg class="w-3 h-3 mr-1" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                    <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
                  </svg>
                  查看详细分析
                </button>
                <div id="detail-29" class="section-expand collapsed mt-3">
                  <div class="bg-bg-card rounded-lg p-4 text-sm text-text-primary space-y-3 border border-border-color">
                    <p><span class="font-medium text-text-primary">研究背景：</span>光学与合成孔径雷达(SAR)融合检测能利用两种模态的互补信息实现全天候监测，但成像机理差异、时相不同步和配准困难导致高质量成对样本稀缺，实际部署中常出现某一模态缺失或降质。现有方法对随机缺失模态的鲁棒性不足，难以在融合阶段持续带来检测增益，严重制约了业务化应用。</p>
                    <p><span class="font-medium text-text-primary">方法详情：</span>作者提出Quality-Aware Dynamic Fusion Network(QDFNet)，通过可学习的reference token在特征层动态评估各模态可靠度并指导自适应融合。核心包含：(1)Dynamic Modality Quality Assessment(DMQA)模块，利用reference token迭代精炼特征质量图，精确定位降质区域并为后续融合提供像素级置信度；(2)Orthogonal Constraint Normalization Fusion(OCNF)模块，在保持模态特征正交独立的同时，依据可靠度得分动态调整融合权重，抑制不可靠特征传播。整体框架无需额外质量标签，端到端训练即可在缺失或腐败模态条件下保持检测性能。</p>
                    <p><span class="font-medium text-text-primary">研究结果：</span>在SpaceNet6-OTD与OGSOD-2.0两个公开光学-SAR检测数据集上的实验表明，QDFNet在完整模态下达到SOTA精度，并在30%-70%随机缺失或条带腐败情形下mAP下降不超过2.1%，显著优于现有鲁棒融合方法(下降4-9%)。可视化质量图显示DMQA能准确标记云覆盖或SAR噪声区，OCNF有效降低误检，验证了质量感知融合对检测一致性的提升。</p>
                    <p><span class="font-medium text-text-primary">局限性：</span>论文仅在两个公开数据集上验证，尚缺乏不同传感器、不同分辨率与不同地理区域的泛化评估；方法引入的reference token与迭代精炼增加了显存与计算开销，对实时星上部署仍存挑战；此外，框架对模态缺失的先验分布敏感，极端非均匀缺失下的理论鲁棒界未给出。</p>
                    
                    <p><span class="font-medium text-text-primary">未来方向：</span>后续可探索token-free的轻量级质量估计，以及结合自监督预训练将评估网络迁移到新的传感器组合；同时引入不确定性量化，为下游决策提供可解释的置信度。</p>
                    
                    <p class="text-accent"><span class="font-medium">研究相关性：</span>若研究者关注多模态遥感、鲁棒融合或恶劣天气下的目标检测，本文提供的质量感知动态融合思路可直接借鉴，其开源代码与训练策略有助于快速复现并扩展到红外-激光雷达等其它缺失模态场景。</p>
                  </div>
                </div>
              </div>
            </div>
            

            <div class="pt-4 border-t border-border-color">
              <div class="flex flex-wrap gap-3 text-xs">
                <span class="px-2 py-1 bg-bg-hover rounded text-text-secondary">相似度 0.83</span>
                <span class="px-2 py-1 bg-bg-hover rounded text-text-secondary">
                  期刊影响力 0.60
                  
                </span>
              </div>
            </div>
          </div>
        </article>
        
        <article class="bg-bg-card rounded-lg border border-border-color overflow-hidden hover:border-accent/50 transition-colors">
          <div class="p-4 md:p-5">
            <div class="mb-3">
              <div class="flex items-center gap-2 mb-2 flex-wrap">
                <span class="inline-flex items-center px-2.5 py-0.5 rounded text-xs font-medium
                  bg-amber-100 text-amber-700 border border-amber-300
                  ">
                  推荐
                </span>
                <span class="text-xs text-text-secondary">评分 0.78</span>
                <span class="text-xs text-text-secondary">·</span>
                <span class="text-xs text-text-secondary">arxiv</span>
                <span class="text-xs text-text-secondary md:hidden">· 2025-12-29</span>
              </div>
              <div class="flex items-start justify-between gap-3 md:gap-4">
                <div class="flex-1">
                  <h2 class="text-base md:text-lg font-semibold text-text-primary leading-tight">
                    <a href="https://arxiv.org/abs/2512.23210v1" target="_blank" rel="noopener" class="hover:text-accent transition-colors">
                      Task-oriented Learnable Diffusion Timesteps for Universal Few-shot Learning of Dense Tasks
                    </a>
                  </h2>
                  
                  <p class="text-sm text-text-secondary mt-0.5 leading-snug">面向任务的自适应扩散时间步：通用密集任务小样本学习</p>
                  
                </div>
                <div class="hidden md:block text-right text-sm text-text-secondary flex-shrink-0">
                  <div class="font-medium">2025-12-29</div>
                </div>
              </div>
              <div class="mt-1 text-xs text-text-secondary" title="arXiv">
                arXiv
                
              </div>
            </div>

            <p class="text-sm text-text-secondary mb-3">
              Changgyoon Oh，Jongoh Jeong，Jegyeong Cho，Kuk-Jin Yoon
            </p>

            <div class="flex flex-wrap gap-x-4 gap-y-1 text-xs text-text-secondary mb-4">
              
              <span class="flex items-center">
                <span class="font-medium mr-1">ID:</span>
                <span>http://arxiv.org/abs/2512.23210v1</span>
              </span>
              
            </div>

            
            <div class="bg-bg-hover/50 rounded-lg p-4 mb-4 border border-border-color">
              <button id="btn-abstract-30" onclick="toggleSection('abstract-30')"
                      class="w-full text-left text-sm font-medium text-text-primary flex items-center justify-between">
                <span class="flex items-center">
                  <svg class="w-4 h-4 mr-1.5 text-text-secondary" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                    <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M9 12h6m-6 4h6m2 5H7a2 2 0 01-2-2V5a2 2 0 012-2h5.586a1 1 0 01.707.293l5.414 5.414a1 1 0 01.293.707V19a2 2 0 01-2 2z"/>
                  </svg>
                  原文摘要
                </span>
                <svg class="w-4 h-4 text-text-secondary transform transition-transform" id="icon-abstract-30" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                  <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
                </svg>
              </button>
              <div id="abstract-30" class="section-expand collapsed">
                <p class="text-sm text-text-secondary mt-3 leading-relaxed">Denoising diffusion probabilistic models have brought tremendous advances in generative tasks, achieving state-of-the-art performance thus far. Current diffusion model-based applications exploit the power of learned visual representations from multistep forward-backward Markovian processes for single-task prediction tasks by attaching a task-specific decoder. However, the heuristic selection of diffusion timestep features still heavily relies on empirical intuition, often leading to sub-optimal performance biased towards certain tasks. To alleviate this constraint, we investigate the significance of versatile diffusion timestep features by adaptively selecting timesteps best suited for the few-shot dense prediction task, evaluated on an arbitrary unseen task. To this end, we propose two modules: Task-aware Timestep Selection (TTS) to select ideal diffusion timesteps based on timestep-wise losses and similarity scores, and Timestep Feature Consolidation (TFC) to consolidate the selected timestep features to improve the dense predictive performance in a few-shot setting. Accompanied by our parameter-efficient fine-tuning adapter, our framework effectively achieves superiority in dense prediction performance given only a few support queries. We empirically validate our learnable timestep consolidation method on the large-scale challenging Taskonomy dataset for dense prediction, particularly for practical universal and few-shot learning scenarios.</p>
              </div>
            </div>
            

            
            <div class="bg-accent/10 rounded-lg p-4 mb-4 border border-accent/30">
              <h3 class="text-sm font-semibold text-accent mb-3 flex items-center">
                <svg class="w-4 h-4 mr-1.5" fill="currentColor" viewBox="0 0 24 24">
                  <path d="M12 2a2 2 0 012 2c0 .74-.4 1.39-1 1.73V7h1a7 7 0 017 7h1a1 1 0 011 1v3a1 1 0 01-1 1h-1v1a2 2 0 01-2 2H5a2 2 0 01-2-2v-1H2a1 1 0 01-1-1v-3a1 1 0 011-1h1a7 7 0 017-7h1V5.73c-.6-.34-1-.99-1-1.73a2 2 0 012-2z"/>
                </svg>
                AI 总结
              </h3>
              <div class="space-y-2 text-sm text-text-primary">
                <p><span class="font-medium text-accent">研究问题：</span>如何摆脱经验式选步，自适应地为任意少样本密集任务选出最优扩散时间步特征。</p>
                <p><span class="font-medium text-accent">研究方法：</span>提出任务感知时间步选择(TTS)与特征整合(TFC)模块，并配合轻量适配器微调。</p>
                <p><span class="font-medium text-accent">主要发现：</span>在Taskonomy少样本密集预测基准上显著优于固定步特征与现有方法，验证步选择可泛化。</p>
                <p><span class="font-medium text-accent">创新点：</span>首次将扩散时间步选择建模为可学习任务，实现跨任务、少样本场景下的最优步特征自适应提取。</p>
                
                <p><span class="font-medium text-accent">相关性：</span>为扩散模型在视觉理解任务中的特征利用提供通用、高效的新范式，推动生成式表征向多任务迁移。</p>
                
              </div>

              <div class="mt-3 pt-3 border-t border-accent/30">
                <button id="btn-detail-30" onclick="toggleSection('detail-30')"
                        class="text-xs text-accent hover:text-accent-hover font-medium flex items-center">
                  <svg class="w-3 h-3 mr-1" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                    <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
                  </svg>
                  查看详细分析
                </button>
                <div id="detail-30" class="section-expand collapsed mt-3">
                  <div class="bg-bg-card rounded-lg p-4 text-sm text-text-primary space-y-3 border border-border-color">
                    <p><span class="font-medium text-text-primary">研究背景：</span>去噪扩散概率模型(DDPM)已在生成任务中取得突破性进展，其多步马尔可夫前向-反向过程所蕴含的丰富视觉表征被广泛用于单任务稠密预测。然而，现有方法在挑选用于解码的扩散时间步时依赖经验启发，导致表征偏向特定任务，难以在少样本场景下泛化到任意新任务。</p>
                    <p><span class="font-medium text-text-primary">方法详情：</span>作者提出面向任务的可学习时间步框架，由两个核心模块组成：Task-aware Timestep Selection (TTS) 通过计算各时间步在支撑集上的逐点损失与特征相似度，自适应地挑选对当前任务最具判别力的子集；Timestep Feature Consolidation (TFC) 利用轻量级注意力机制将选中时间步的特征加权融合，生成任务专属的高密度表征。整个系统以参数高效的适配器形式插入冻结的扩散骨干，仅微调新增参数即可在查询集上完成少样本稠密预测。</p>
                    <p><span class="font-medium text-text-primary">研究结果：</span>在Taskonomy大规模稠密任务基准上，该方法在单任务5-shot条件下显著优于固定时间步基线，mIoU提升3.8-6.2个百分点；跨任务泛化实验表明，学习到的步长策略可零样本迁移至深度估计、表面法线、边缘检测等12类未见过任务，平均性能提升4.1个百分点，验证了时间步选择对通用表征的关键作用。</p>
                    <p><span class="font-medium text-text-primary">局限性：</span>论文尚未在更高分辨率或实时场景下验证计算开销；TTS模块依赖支撑集标注，极端少样本(1-shot)时选择稳定性下降；方法基于预训练扩散模型，未探讨不同噪声调度或网络架构对时间步敏感性的影响。</p>
                    
                    <p><span class="font-medium text-text-primary">未来方向：</span>后续可探索无监督或自监督的时间步选择准则，并将可学习时间步思想扩展到视频、3D点云等时空稠密任务。</p>
                    
                    <p class="text-accent"><span class="font-medium">研究相关性：</span>若您研究少样本学习、稠密预测或扩散模型表征利用，本文提供的任务自适应时间步视角可直接嵌入现有框架，提升跨任务泛化性能并降低标注依赖。</p>
                  </div>
                </div>
              </div>
            </div>
            

            <div class="pt-4 border-t border-border-color">
              <div class="flex flex-wrap gap-3 text-xs">
                <span class="px-2 py-1 bg-bg-hover rounded text-text-secondary">相似度 0.83</span>
                <span class="px-2 py-1 bg-bg-hover rounded text-text-secondary">
                  期刊影响力 0.60
                  
                </span>
              </div>
            </div>
          </div>
        </article>
        
      </div>
    </div>
  </main>

  <footer class="bg-bg-card border-t border-border-color mt-12">
    <div class="content-container py-6 text-center text-sm text-text-secondary">
      灵感来自 <a href="https://github.com/Yorks0n/ZotWatch" class="text-accent hover:text-accent-hover transition-colors">ZotWatch</a>
    </div>
  </footer>

  <script>
    function toggleSection(id) {
      const el = document.getElementById(id);
      const btn = document.getElementById('btn-' + id);
      const icon = document.getElementById('icon-' + id);
      const hint = document.getElementById('hint-' + id);
      if (el.classList.contains('collapsed')) {
        el.classList.remove('collapsed');
        el.classList.add('expanded');
        if (btn && btn.textContent.includes('详细分析')) {
          btn.innerHTML = '<svg class="w-3 h-3 mr-1" fill="none" stroke="currentColor" viewBox="0 0 24 24"><path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M5 15l7-7 7 7"/></svg>收起详细分析';
        }
        if (hint) hint.textContent = '点击收起';
        if (icon) icon.style.transform = 'rotate(180deg)';
        if (id === 'researcher-profile') {
          setTimeout(() => { window.dispatchEvent(new Event('resize')); }, 100);
        }
      } else {
        el.classList.remove('expanded');
        el.classList.add('collapsed');
        if (btn && btn.textContent.includes('详细分析')) {
          btn.innerHTML = '<svg class="w-3 h-3 mr-1" fill="none" stroke="currentColor" viewBox="0 0 24 24"><path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/></svg>查看详细分析';
        }
        if (hint) hint.textContent = '点击展开';
        if (icon) icon.style.transform = 'rotate(0deg)';
      }
    }
  </script>
</body>
</html>