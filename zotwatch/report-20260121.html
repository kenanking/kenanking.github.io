<!DOCTYPE html>
<html lang="zh-CN">
<head>
  <meta charset="utf-8">
  <meta name="viewport" content="width=device-width, initial-scale=1">
  <title>ZotWatch 文献推荐 - 2026-01-21</title>
  <script src="https://cdn.tailwindcss.com"></script>
  <script src="https://cdn.jsdelivr.net/npm/chart.js@4.4.1/dist/chart.umd.min.js"></script>
  <script src="https://d3js.org/d3.v7.min.js"></script>
  <script>
    tailwind.config = {
      theme: {
        extend: {
          colors: {
            'bg-primary': '#f8fafc',
            'bg-card': '#ffffff',
            'bg-hover': '#f1f5f9',
            'text-primary': '#1e293b',
            'text-secondary': '#64748b',
            'border-color': '#e2e8f0',
            'accent': '#2563eb',
            'accent-hover': '#1d4ed8',
          }
        }
      }
    }
  </script>
  <style>
    .section-expand { transition: max-height 0.5s ease-out; overflow: hidden; }
    .section-expand.collapsed { max-height: 0; }
    .section-expand.expanded { max-height: none; }
    body { font-family: -apple-system, BlinkMacSystemFont, 'Segoe UI', Roboto, 'Helvetica Neue', Arial, sans-serif; }
    .content-container { max-width: 48rem; margin: 0 auto; padding: 0 1rem; }
    @media (min-width: 768px) { .content-container { padding: 0 2rem; } }
    @media (min-width: 1024px) { .content-container { padding: 0 4rem; } }

    .cluster-graph-container { width: 100%; height: 280px; touch-action: pan-y; }
    @media (min-width: 640px) { .cluster-graph-container { height: 320px; } }
    @media (min-width: 768px) { .cluster-graph-container { height: 350px; } }
    #cluster-graph svg { overflow: visible; }
  </style>
</head>
<body class="bg-bg-primary min-h-screen text-text-primary">
  <header class="bg-bg-card border-b border-border-color">
    <div class="content-container py-6 md:py-8">
      <h1 class="text-xl md:text-2xl font-bold text-text-primary">ZotWatch 文献推荐</h1>
      <p class="text-xs md:text-sm text-text-secondary mt-1">
        共 35 篇论文 ·
        生成于 2026-01-21 10:59 Asia/Shanghai
      </p>
    </div>
  </header>

  
  <section class="py-4 md:py-6 border-b border-border-color">
    <div class="content-container">
      <button id="btn-researcher-profile" onclick="toggleSection('researcher-profile')"
              class="w-full text-left text-lg md:text-xl font-bold text-text-primary mb-3 md:mb-4 flex items-center justify-between group hover:text-accent transition-colors">
        <span class="flex items-center gap-2 flex-1 min-w-0">
          <svg class="w-5 h-5 flex-shrink-0 text-accent" fill="currentColor" viewBox="0 0 24 24">
            <path d="M12 2C6.48 2 2 6.48 2 12s4.48 10 10 10 10-4.48 10-10S17.52 2 12 2zm0 3c1.66 0 3 1.34 3 3s-1.34 3-3 3-3-1.34-3-3 1.34-3 3-3zm0 14.2c-2.5 0-4.71-1.28-6-3.22.03-1.99 4-3.08 6-3.08 1.99 0 5.97 1.09 6 3.08-1.29 1.94-3.5 3.22-6 3.22z"/>
          </svg>
          <span class="truncate">研究兴趣画像</span>
        </span>
        <span class="flex items-center gap-1 md:gap-2 flex-shrink-0 ml-2">
          <span class="hidden md:inline text-sm font-normal text-text-secondary group-hover:text-accent" id="hint-researcher-profile">点击展开</span>
          <svg class="w-5 h-5 md:w-4 md:h-4 text-text-secondary group-hover:text-accent transform transition-transform" id="icon-researcher-profile" fill="none" stroke="currentColor" viewBox="0 0 24 24">
            <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
          </svg>
        </span>
      </button>

      <div id="researcher-profile" class="section-expand collapsed">
      <div class="bg-bg-card rounded-lg border border-border-color p-4 mb-4">
        <div class="grid grid-cols-4 gap-3 text-center">
          <div>
            <div class="text-xl font-bold text-accent">966</div>
            <div class="text-xs text-text-secondary">收藏论文</div>
          </div>
          <div>
            <div class="text-xl font-bold text-accent">5年9月</div>
            <div class="text-xs text-text-secondary">收藏时长</div>
          </div>
          <div>
            <div class="text-xl font-bold text-accent">&gt;10</div>
            <div class="text-xs text-text-secondary">兴趣领域</div>
          </div>
          <div>
            <div class="text-xl font-bold text-accent">9</div>
            <div class="text-xs text-text-secondary">高频作者</div>
          </div>
        </div>
      </div>

      
      <div class="bg-accent/10 rounded-lg border border-accent/30 p-4 mb-4">
        <h3 class="text-sm font-semibold text-accent mb-2 flex items-center gap-2">
          <svg class="w-4 h-4" fill="currentColor" viewBox="0 0 24 24">
            <path d="M12 2a2 2 0 012 2c0 .74-.4 1.39-1 1.73V7h1a7 7 0 017 7h1a1 1 0 011 1v3a1 1 0 01-1 1h-1v1a2 2 0 01-2 2H5a2 2 0 01-2-2v-1H2a1 1 0 01-1-1v-3a1 1 0 011-1h1a7 7 0 017-7h1V5.73c-.6-.34-1-.99-1-1.73a2 2 0 012-2z"/>
          </svg>
          AI 研究兴趣分析
        </h3>
        <div class="space-y-2 text-xs text-text-primary leading-relaxed">
          <p><span class="font-medium text-accent">研究兴趣：</span>用户长期关注计算机视觉基础任务（目标检测、视觉定位、姿态估计）及其高效化技术（模型压缩、知识蒸馏、重参数化），同时对自监督/对比学习与遥感SAR智能解译保持浓厚兴趣。</p>
          <p><span class="font-medium text-accent">深度关注：</span>在目标检测与相关CNN架构优化方向形成深度积累，持续追踪Kaiming He、Ross Girshick等核心团队的最新进展；对SAR图像理解与域自适应、迁移学习结合有系统收藏，体现出跨场景感知技术的专注。</p>
          <p><span class="font-medium text-accent">跨学科倾向：</span>阅读横跨计算机视觉与遥感对地观测两大领域，将通用视觉模型（Transformer、基础模型）与SAR成像物理问题结合，关注模型压缩与实时推理在遥感平台上的落地。</p>
          <p><span class="font-medium text-accent">兴趣演变：</span>2025年起收藏量显著回升并聚焦“大语言模型”“推理增强”“DeepSeek”，显示正探索大模型与视觉任务融合及高效推理；新增“自动驾驶感知”“多任务学习”关键词，预示兴趣向多模态、场景化应用延伸。</p>
          <p><span class="font-medium text-accent">阅读建议：</span>建议关注Vision-Language Model在遥感描述与检索中的应用，以及面向边缘遥感平台的轻量化Transformer与量化技术；同时可跟踪开放世界目标检测和持续学习，以支撑长时序SAR监测任务。</p>
        </div>
      </div>
      

      <div class="grid grid-cols-1 md:grid-cols-2 gap-4 mb-4">
        
        <div class="bg-bg-card rounded-lg border border-border-color p-4">
          <h4 class="text-sm font-medium text-text-primary mb-3">研究领域分布</h4>
          <div style="height: 180px;">
            <canvas id="domainsChart"></canvas>
          </div>
        </div>
        

        
        <div class="bg-bg-card rounded-lg border border-border-color p-4">
          <h4 class="text-sm font-medium text-text-primary mb-3">阅读趋势（季度）</h4>
          <div style="height: 180px;">
            <canvas id="trendsChart"></canvas>
          </div>
        </div>
        
      </div>

      
      <div class="bg-bg-card rounded-lg border border-border-color p-4 mb-4">
        <h4 class="text-sm font-medium text-text-primary mb-3">论文发表年份分布</h4>
        <div style="height: 180px;">
          <canvas id="yearDistChart"></canvas>
        </div>
      </div>
      

      
      <div class="bg-bg-card rounded-lg border border-border-color p-4 mb-4">
        <h4 class="text-sm font-medium text-text-primary mb-3">
          研究方向聚类图谱
          <span class="text-xs text-text-secondary font-normal ml-2">(30 个聚类)</span>
        </h4>
        <div id="cluster-graph" class="cluster-graph-container"></div>
        <div class="text-xs text-text-secondary mt-3">
          覆盖 940/940 篇论文（仅含摘要的文献参与聚类）
        </div>
      </div>
      

      <div class="grid grid-cols-1 md:grid-cols-2 gap-4 mb-4">
        
        <div class="bg-bg-card rounded-lg border border-border-color p-4">
          <h4 class="text-sm font-medium text-text-primary mb-2">高频关注作者</h4>
          <div class="grid grid-cols-2 gap-x-4 gap-y-1 text-xs">
            
            <div class="flex justify-between truncate">
              <span class="text-text-primary truncate" title="Kaiming He">Kaiming He</span>
              <span class="text-text-secondary ml-1 flex-shrink-0">24</span>
            </div>
            
            <div class="flex justify-between truncate">
              <span class="text-text-primary truncate" title="Ross Girshick">Ross Girshick</span>
              <span class="text-text-secondary ml-1 flex-shrink-0">18</span>
            </div>
            
            <div class="flex justify-between truncate">
              <span class="text-text-primary truncate" title="Jian Sun">Jian Sun</span>
              <span class="text-text-secondary ml-1 flex-shrink-0">15</span>
            </div>
            
            <div class="flex justify-between truncate">
              <span class="text-text-primary truncate" title="Song Han">Song Han</span>
              <span class="text-text-secondary ml-1 flex-shrink-0">14</span>
            </div>
            
            <div class="flex justify-between truncate">
              <span class="text-text-primary truncate" title="Xiangyu Zhang">Xiangyu Zhang</span>
              <span class="text-text-secondary ml-1 flex-shrink-0">14</span>
            </div>
            
            <div class="flex justify-between truncate">
              <span class="text-text-primary truncate" title="Piotr Dollár">Piotr Dollár</span>
              <span class="text-text-secondary ml-1 flex-shrink-0">12</span>
            </div>
            
            <div class="flex justify-between truncate">
              <span class="text-text-primary truncate" title="Li Liu">Li Liu</span>
              <span class="text-text-secondary ml-1 flex-shrink-0">11</span>
            </div>
            
            <div class="flex justify-between truncate">
              <span class="text-text-primary truncate" title="Xiang Li">Xiang Li</span>
              <span class="text-text-secondary ml-1 flex-shrink-0">11</span>
            </div>
            
            <div class="flex justify-between truncate">
              <span class="text-text-primary truncate" title="Wenfeng Liang">Wenfeng Liang</span>
              <span class="text-text-secondary ml-1 flex-shrink-0">10</span>
            </div>
            
            <div class="flex justify-between truncate">
              <span class="text-text-primary truncate" title="Yann LeCun">Yann LeCun</span>
              <span class="text-text-secondary ml-1 flex-shrink-0">9</span>
            </div>
            
            <div class="flex justify-between truncate">
              <span class="text-text-primary truncate" title="Zhenda Xie">Zhenda Xie</span>
              <span class="text-text-secondary ml-1 flex-shrink-0">9</span>
            </div>
            
            <div class="flex justify-between truncate">
              <span class="text-text-primary truncate" title="Feng Xu">Feng Xu</span>
              <span class="text-text-secondary ml-1 flex-shrink-0">9</span>
            </div>
            
          </div>
        </div>
        

        
        <div class="bg-bg-card rounded-lg border border-border-color p-4">
          <h4 class="text-sm font-medium text-text-primary mb-2">常关注期刊/会议</h4>
          <div class="space-y-1 text-xs">
            
            <div class="flex items-center justify-between">
              <span class="text-text-primary truncate flex-1" title="Computer Vision and Pattern Recognition">Computer Vision and Pattern Recognition</span>
              <span class="text-xs px-1 py-0.5 rounded bg-blue-100 text-blue-700 ml-1 flex-shrink-0">
                期刊
              </span>
              <span class="text-text-secondary ml-1 flex-shrink-0">114</span>
            </div>
            
            <div class="flex items-center justify-between">
              <span class="text-text-primary truncate flex-1" title="IEEE Transactions on Geoscience and Remote Sensing">IEEE Transactions on Geoscience and Remote Sensing</span>
              <span class="text-xs px-1 py-0.5 rounded bg-blue-100 text-blue-700 ml-1 flex-shrink-0">
                期刊
              </span>
              <span class="text-text-secondary ml-1 flex-shrink-0">47</span>
            </div>
            
            <div class="flex items-center justify-between">
              <span class="text-text-primary truncate flex-1" title="Neural Information Processing Systems">Neural Information Processing Systems</span>
              <span class="text-xs px-1 py-0.5 rounded bg-blue-100 text-blue-700 ml-1 flex-shrink-0">
                期刊
              </span>
              <span class="text-text-secondary ml-1 flex-shrink-0">46</span>
            </div>
            
            <div class="flex items-center justify-between">
              <span class="text-text-primary truncate flex-1" title="International Conference on Computer Vision">International Conference on Computer Vision</span>
              <span class="text-xs px-1 py-0.5 rounded bg-amber-100 text-amber-700 ml-1 flex-shrink-0">
                会议
              </span>
              <span class="text-text-secondary ml-1 flex-shrink-0">38</span>
            </div>
            
            <div class="flex items-center justify-between">
              <span class="text-text-primary truncate flex-1" title="IEEE Transactions on Pattern Analysis and Machine Intelligence">IEEE Transactions on Pattern Analysis and Machine Intelligence</span>
              <span class="text-xs px-1 py-0.5 rounded bg-blue-100 text-blue-700 ml-1 flex-shrink-0">
                期刊
              </span>
              <span class="text-text-secondary ml-1 flex-shrink-0">32</span>
            </div>
            
            <div class="flex items-center justify-between">
              <span class="text-text-primary truncate flex-1" title="International Conference on Learning Representations">International Conference on Learning Representations</span>
              <span class="text-xs px-1 py-0.5 rounded bg-amber-100 text-amber-700 ml-1 flex-shrink-0">
                会议
              </span>
              <span class="text-text-secondary ml-1 flex-shrink-0">29</span>
            </div>
            
            <div class="flex items-center justify-between">
              <span class="text-text-primary truncate flex-1" title="European Conference on Computer Vision">European Conference on Computer Vision</span>
              <span class="text-xs px-1 py-0.5 rounded bg-amber-100 text-amber-700 ml-1 flex-shrink-0">
                会议
              </span>
              <span class="text-text-secondary ml-1 flex-shrink-0">23</span>
            </div>
            
            <div class="flex items-center justify-between">
              <span class="text-text-primary truncate flex-1" title="IEEE Geoscience and Remote Sensing Letters">IEEE Geoscience and Remote Sensing Letters</span>
              <span class="text-xs px-1 py-0.5 rounded bg-blue-100 text-blue-700 ml-1 flex-shrink-0">
                期刊
              </span>
              <span class="text-text-secondary ml-1 flex-shrink-0">22</span>
            </div>
            
          </div>
        </div>
        
      </div>

      
      <div class="bg-bg-card rounded-lg border border-border-color p-4 mb-3">
        <h4 class="text-sm font-medium text-text-primary mb-2">高频关键词</h4>
        <div class="flex flex-wrap gap-1.5">
          
          <span class="px-2 py-0.5 bg-bg-hover rounded text-xs text-text-primary">
            大语言模型 <span class="text-text-secondary">(14)</span>
          </span>
          
          <span class="px-2 py-0.5 bg-bg-hover rounded text-xs text-text-primary">
            合成孔径雷达 <span class="text-text-secondary">(12)</span>
          </span>
          
          <span class="px-2 py-0.5 bg-bg-hover rounded text-xs text-text-primary">
            自监督学习 <span class="text-text-secondary">(11)</span>
          </span>
          
          <span class="px-2 py-0.5 bg-bg-hover rounded text-xs text-text-primary">
            综述 <span class="text-text-secondary">(9)</span>
          </span>
          
          <span class="px-2 py-0.5 bg-bg-hover rounded text-xs text-text-primary">
            域自适应 <span class="text-text-secondary">(9)</span>
          </span>
          
          <span class="px-2 py-0.5 bg-bg-hover rounded text-xs text-text-primary">
            目标检测 <span class="text-text-secondary">(8)</span>
          </span>
          
          <span class="px-2 py-0.5 bg-bg-hover rounded text-xs text-text-primary">
            深度学习 <span class="text-text-secondary">(8)</span>
          </span>
          
          <span class="px-2 py-0.5 bg-bg-hover rounded text-xs text-text-primary">
            迁移学习 <span class="text-text-secondary">(7)</span>
          </span>
          
          <span class="px-2 py-0.5 bg-bg-hover rounded text-xs text-text-primary">
            扩散模型 <span class="text-text-secondary">(7)</span>
          </span>
          
          <span class="px-2 py-0.5 bg-bg-hover rounded text-xs text-text-primary">
            强化学习 <span class="text-text-secondary">(7)</span>
          </span>
          
          <span class="px-2 py-0.5 bg-bg-hover rounded text-xs text-text-primary">
            注意力机制 <span class="text-text-secondary">(6)</span>
          </span>
          
          <span class="px-2 py-0.5 bg-bg-hover rounded text-xs text-text-primary">
            重参数化 <span class="text-text-secondary">(6)</span>
          </span>
          
          <span class="px-2 py-0.5 bg-bg-hover rounded text-xs text-text-primary">
            DeepSeek <span class="text-text-secondary">(6)</span>
          </span>
          
          <span class="px-2 py-0.5 bg-bg-hover rounded text-xs text-text-primary">
            生成对抗网络 <span class="text-text-secondary">(6)</span>
          </span>
          
          <span class="px-2 py-0.5 bg-bg-hover rounded text-xs text-text-primary">
            基础模型 <span class="text-text-secondary">(5)</span>
          </span>
          
          <span class="px-2 py-0.5 bg-bg-hover rounded text-xs text-text-primary">
            Transformers <span class="text-text-secondary">(5)</span>
          </span>
          
          <span class="px-2 py-0.5 bg-bg-hover rounded text-xs text-text-primary">
            卷积神经网络 <span class="text-text-secondary">(5)</span>
          </span>
          
          <span class="px-2 py-0.5 bg-bg-hover rounded text-xs text-text-primary">
            旋转目标检测 <span class="text-text-secondary">(5)</span>
          </span>
          
          <span class="px-2 py-0.5 bg-bg-hover rounded text-xs text-text-primary">
            知识蒸馏 <span class="text-text-secondary">(5)</span>
          </span>
          
          <span class="px-2 py-0.5 bg-bg-hover rounded text-xs text-text-primary">
            SAR目标识别 <span class="text-text-secondary">(5)</span>
          </span>
          
        </div>
      </div>
      

      <div class="text-xs text-text-secondary text-right">
        画像生成于 2026-01-21 10:35 Asia/Shanghai
        · kimi-k2-turbo-preview
      </div>
      </div>
    </div>
  </section>
  

  
  <script>
    document.addEventListener('DOMContentLoaded', function() {
      Chart.defaults.font.family = "-apple-system, BlinkMacSystemFont, 'Segoe UI', Roboto, 'Helvetica Neue', Arial, sans-serif";
      Chart.defaults.font.size = 11;
      Chart.defaults.color = '#64748b';
      Chart.defaults.animation = false;

      
      const domainsCtx = document.getElementById('domainsChart');
      if (domainsCtx) {
        new Chart(domainsCtx, {
          type: 'bar',
          data: {
            labels: ['视觉定位', '目标检测', '模型压缩', '姿态估计', '对比学习', '卫星导航', '特征匹配', '人脸对齐'],
            datasets: [{
              data: [22, 35, 18, 15, 10, 6, 5, 8],
              backgroundColor: 'rgba(37, 99, 235, 0.7)',
              borderColor: 'rgba(37, 99, 235, 1)',
              borderWidth: 1,
              borderRadius: 2,
              barThickness: 16
            }]
          },
          options: {
            indexAxis: 'y',
            responsive: true,
            maintainAspectRatio: false,
            plugins: {
              legend: { display: false },
              tooltip: { enabled: false }
            },
            scales: {
              x: {
                beginAtZero: true,
                grid: { color: 'rgba(226, 232, 240, 0.5)' },
                ticks: { precision: 0 }
              },
              y: {
                grid: { display: false },
                ticks: { font: { size: 10 } }
              }
            }
          }
        });
      }
      

      
      const trendsCtx = document.getElementById('trendsChart');
      if (trendsCtx) {
        const trends = [{ q: '2024-Q1', c: 68 }, { q: '2024-Q2', c: 18 }, { q: '2024-Q3', c: 4 }, { q: '2024-Q4', c: 24 }, { q: '2025-Q1', c: 101 }, { q: '2025-Q2', c: 34 }, { q: '2025-Q3', c: 13 }, { q: '2025-Q4', c: 30 }, { q: '2026-Q1', c: 5 }];
        const monthColors = [
          { bg: 'rgba(59, 130, 246, 0.7)', border: 'rgba(59, 130, 246, 1)' },
          { bg: 'rgba(16, 185, 129, 0.7)', border: 'rgba(16, 185, 129, 1)' },
          { bg: 'rgba(245, 158, 11, 0.7)', border: 'rgba(245, 158, 11, 1)' }
        ];
        const bgColors = trends.map((t, i) => monthColors[i % 3].bg);
        const borderColors = trends.map((t, i) => monthColors[i % 3].border);
        new Chart(trendsCtx, {
          type: 'bar',
          data: {
            labels: trends.map(t => t.q.replace('-', '\n')),
            datasets: [{
              data: trends.map(t => t.c),
              backgroundColor: bgColors,
              borderColor: borderColors,
              borderWidth: 1,
              borderRadius: 2,
              barThickness: 20
            }]
          },
          options: {
            responsive: true,
            maintainAspectRatio: false,
            plugins: {
              legend: { display: false },
              tooltip: { enabled: false }
            },
            scales: {
              x: {
                grid: { display: false },
                ticks: { font: { size: 9 }, maxRotation: 0 }
              },
              y: {
                beginAtZero: true,
                grid: { color: 'rgba(226, 232, 240, 0.5)' },
                ticks: { precision: 0 }
              }
            }
          }
        });
      }
      

      
      const yearDistCtx = document.getElementById('yearDistChart');
      if (yearDistCtx) {
        const yearData = [{ year: 2007, count: 3 }, { year: 2008, count: 2 }, { year: 2009, count: 2 }, { year: 2010, count: 9 }, { year: 2011, count: 10 }, { year: 2012, count: 3 }, { year: 2013, count: 12 }, { year: 2014, count: 14 }, { year: 2015, count: 18 }, { year: 2016, count: 15 }, { year: 2017, count: 41 }, { year: 2018, count: 59 }, { year: 2019, count: 56 }, { year: 2020, count: 67 }, { year: 2021, count: 84 }, { year: 2022, count: 113 }, { year: 2023, count: 111 }, { year: 2024, count: 114 }, { year: 2025, count: 178 }, { year: 2026, count: 5 }];
        new Chart(yearDistCtx, {
          type: 'line',
          data: {
            labels: yearData.map(d => d.year),
            datasets: [{
              data: yearData.map(d => d.count),
              borderColor: 'rgba(37, 99, 235, 1)',
              backgroundColor: 'rgba(37, 99, 235, 0.1)',
              borderWidth: 2,
              fill: true,
              tension: 0.3,
              pointRadius: 2,
              pointHoverRadius: 4
            }]
          },
          options: {
            responsive: true,
            maintainAspectRatio: false,
            plugins: {
              legend: { display: false },
              tooltip: {
                callbacks: {
                  label: function(context) {
                    return context.parsed.y + ' 篇';
                  }
                }
              }
            },
            scales: {
              x: {
                grid: { display: false },
                ticks: {
                  font: { size: 9 },
                  maxRotation: 45,
                  callback: function(value, index) {
                    const year = this.getLabelForValue(value);
                    return year % 5 === 0 ? year : '';
                  }
                }
              },
              y: {
                beginAtZero: true,
                grid: { color: 'rgba(226, 232, 240, 0.5)' },
                ticks: { precision: 0 }
              }
            }
          }
        });
      }
      

      
      const clusterContainer = document.getElementById('cluster-graph');
      if (clusterContainer) {
        const clusters = [
          
          {
            id: 0,
            label: "SAR\u57df\u81ea\u9002\u5e94\u76ee\u6807\u8bc6\u522b",
            size: 63,
            keywords: ["SAR\u76ee\u6807\u8bc6\u522b", "\u57df\u81ea\u9002\u5e94", "\u5408\u6210\u5b54\u5f84\u96f7\u8fbe"]
          },
          
          {
            id: 1,
            label: "SAR\u8230\u8239\u68c0\u6d4b\u4e0eCFAR",
            size: 59,
            keywords: ["\u5408\u6210\u5b54\u5f84\u96f7\u8fbe", "SAR\u8230\u8239\u68c0\u6d4b", "\u591a\u5c3a\u5ea6\u7279\u5f81\u878d\u5408"]
          },
          
          {
            id: 2,
            label: "2D/3D\u4eba\u4f53\u59ff\u6001\u4f30\u8ba1",
            size: 55,
            keywords: ["HRNet", "Transformers", "\u591a\u6a21\u6001"]
          },
          
          {
            id: 3,
            label: "Transformer\u5b9e\u65f6\u76ee\u6807\u68c0\u6d4b",
            size: 54,
            keywords: ["\u7efc\u8ff0", "DETR", "\u5b9e\u65f6\u76ee\u6807\u68c0\u6d4b"]
          },
          
          {
            id: 4,
            label: "\u8f7b\u91cf\u7ea7CNN\u67b6\u6784\u8bbe\u8ba1",
            size: 48,
            keywords: ["\u91cd\u53c2\u6570\u5316", "\u5377\u79ef\u795e\u7ecf\u7f51\u7edc", "VGG"]
          },
          
          {
            id: 5,
            label: "\u89c6\u89c9\u81ea\u76d1\u7763\u5bf9\u6bd4\u5b66\u4e60",
            size: 46,
            keywords: ["\u81ea\u76d1\u7763\u5b66\u4e60", "\u5bf9\u6bd4\u5b66\u4e60", "MoCo"]
          },
          
          {
            id: 6,
            label: "\u591a\u4f20\u611f\u5668BEV 3D\u611f\u77e5",
            size: 45,
            keywords: ["\u591a\u4efb\u52a1\u5b66\u4e60", "\u81ea\u52a8\u9a7e\u9a76\u611f\u77e5", "\u4e09\u7ef4\u611f\u77e5"]
          },
          
          {
            id: 7,
            label: "\u795e\u7ecf\u7f51\u7edc\u91cf\u5316\u538b\u7f29",
            size: 39,
            keywords: ["\u6a21\u578b\u538b\u7f29", "\u6a21\u578b\u91cf\u5316", "\u6574\u6570\u63a8\u7406"]
          },
          
          {
            id: 8,
            label: "\u7ea2\u5916\u5f31\u5c0f\u76ee\u6807\u667a\u80fd\u68c0\u6d4b",
            size: 38,
            keywords: ["\u6df1\u5ea6\u5b66\u4e60", "\u7279\u5f81\u878d\u5408", "\u5f31\u5c0f\u76ee\u6807\u8ddf\u8e2a"]
          },
          
          {
            id: 9,
            label: "MoE\u5927\u6a21\u578b\u5206\u5e03\u5f0f\u8bad\u7ec3",
            size: 36,
            keywords: ["\u6df7\u5408\u4e13\u5bb6\u6a21\u578b", "DeepSeek", "\u5206\u5e03\u5f0f\u8bad\u7ec3"]
          },
          
          {
            id: 10,
            label: "LLM\u5f3a\u5316\u5b66\u4e60\u63a8\u7406",
            size: 36,
            keywords: ["\u5927\u8bed\u8a00\u6a21\u578b", "\u5f3a\u5316\u5b66\u4e60", "DeepSeek"]
          },
          
          {
            id: 11,
            label: "\u5c0f\u6837\u672c\u57df\u9002\u5e94\u68c0\u6d4b",
            size: 36,
            keywords: ["\u57df\u81ea\u9002\u5e94", "\u8fc1\u79fb\u5b66\u4e60", "\u5f00\u653e\u96c6\u8bc6\u522b"]
          },
          
          {
            id: 12,
            label: "\u901a\u7528\u56fe\u50cf\u5206\u5272\u57fa\u7840\u6a21\u578b",
            size: 34,
            keywords: ["\u56fe\u50cf\u5206\u5272", "\u57fa\u7840\u6a21\u578b", "\u901a\u7528\u5206\u5272"]
          },
          
          {
            id: 13,
            label: "\u6269\u6563\u6a21\u578b\u56fe\u50cf\u751f\u6210",
            size: 32,
            keywords: ["\u6269\u6563\u6a21\u578b", "\u56fe\u50cf\u6062\u590d", "\u611f\u77e5-\u5931\u771f\u6743\u8861"]
          },
          
          {
            id: 14,
            label: "\u8f7b\u91cf\u7ea7Vision Transformer",
            size: 31,
            keywords: ["\u8f7b\u91cf\u7ea7\u6a21\u578b", "\u6ce8\u610f\u529b\u673a\u5236", "Swin Transformer"]
          },
          
          {
            id: 15,
            label: "\u6269\u6563\u6a21\u578b\u7406\u8bba\u65b9\u6cd5",
            size: 29,
            keywords: ["\u6807\u51c6\u5316\u6d41", "\u6d41\u6a21\u578b", "NCE"]
          },
          
          {
            id: 16,
            label: "\u8f66\u724c\u8bc6\u522b\u7aef\u5230\u7aef\u7cfb\u7edf",
            size: 27,
            keywords: ["\u8f66\u724c\u8bc6\u522b", "Internet of Things", "Microcontrollers"]
          },
          
          {
            id: 17,
            label: "\u7ea2\u5916\u5c0f\u76ee\u6807\u6ce8\u610f\u529b\u68c0\u6d4b",
            size: 27,
            keywords: ["\u7ea2\u5916\u5c0f\u76ee\u6807\u68c0\u6d4b", "\u6837\u672c\u81ea\u9002\u5e94\u589e\u6b96", "\u7279\u5f81\u589e\u5f3a"]
          },
          
          {
            id: 18,
            label: "\u53ef\u4fe1\u673a\u5668\u5b66\u4e60\u7406\u8bba",
            size: 27,
            keywords: ["\u5f52\u7eb3\u504f\u7f6e", "\u6a21\u578b\u901a\u7528\u6027", "\u7406\u8bba\u57fa\u7840"]
          },
          
          {
            id: 19,
            label: "\u795e\u7ecf\u7f51\u7edc\u4f18\u5316\u4e0e\u53ef\u89c6\u5316",
            size: 26,
            keywords: ["\u751f\u6210\u5bf9\u6297\u7f51\u7edc", "\u4f18\u5316\u5668", "\u5206\u5e03\u5f0f\u8bad\u7ec3"]
          },
          
          {
            id: 20,
            label: "SAR\u53ef\u89e3\u91ca\u8bc6\u522b\u8bc4\u4f30",
            size: 25,
            keywords: ["SAR\u76ee\u6807\u8bc6\u522b", "\u6027\u80fd\u8bc4\u4f30", "\u8f85\u52a9\u8bc6\u522b\u7cfb\u7edf"]
          },
          
          {
            id: 21,
            label: "SAR\u6210\u50cf\u7b97\u6cd5\u4e0e\u6a21\u62df",
            size: 20,
            keywords: ["\u5408\u6210\u5b54\u5f84\u96f7\u8fbe", "\u56de\u6ce2\u6570\u636e\u6a21\u62df", "\u9006CS\u7b97\u6cd5"]
          },
          
          {
            id: 22,
            label: "\u5f3a\u5316\u5b66\u4e60\u4e0e\u5143\u5b66\u4e60",
            size: 19,
            keywords: ["\u5f3a\u5316\u5b66\u4e60", "\u7b97\u6cd5\u4ea4\u6613", "\u9650\u4ef7\u8ba2\u5355\u7c3f"]
          },
          
          {
            id: 23,
            label: "\u9065\u611f\u65cb\u8f6c\u76ee\u6807\u68c0\u6d4b",
            size: 18,
            keywords: ["\u65cb\u8f6c\u76ee\u6807\u68c0\u6d4b", "\u9065\u611f\u76ee\u6807\u68c0\u6d4b", "\u4e0a\u4e0b\u6587\u6ce8\u610f\u529b\u673a\u5236"]
          },
          
          {
            id: 24,
            label: "CNN\u7279\u5f81\u53ef\u89c6\u5316\u89e3\u91ca",
            size: 14,
            keywords: ["\u7279\u5f81\u53ef\u89c6\u5316", "Grad-CAM", "\u5206\u5e03\u5916\u68c0\u6d4b"]
          },
          
          {
            id: 25,
            label: "\u4fe1\u53f7\u68c0\u6d4b\u4e0e\u566a\u58f0\u7406\u8bba",
            size: 13,
            keywords: ["\u5e95\u5c42\u7b97\u6cd5", "\u6027\u80fd\u4f18\u5316"]
          },
          
          {
            id: 26,
            label: "\u591a\u6a21\u6001\u76ee\u6807\u8ddf\u8e2a",
            size: 13,
            keywords: ["SIFT", "\u65e0\u4eba\u673a\u5bf9\u6297", "\u65f6\u7a7a\u5148\u9a8c\u5efa\u6a21"]
          },
          
          {
            id: 27,
            label: "\u5b66\u672f\u51fa\u7248\u540c\u884c\u8bc4\u5ba1",
            size: 12,
            keywords: ["LaTeX", "\u8bbe\u8ba1\u6a21\u5f0f", "\u7814\u7a76"]
          },
          
          {
            id: 28,
            label: "\u96f7\u8fbe\u7a7f\u5899\u751f\u547d\u63a2\u6d4b",
            size: 10,
            keywords: ["\u4fe1\u53f7\u63d0\u53d6", "\u547c\u5438\u5fc3\u8df3\u4fe1\u53f7", "\u751f\u547d\u4fe1\u606f\u63a2\u6d4b"]
          },
          
          {
            id: 29,
            label: "CTC\u5e8f\u5217\u5efa\u6a21",
            size: 8,
            keywords: ["\u81ea\u7136\u8bed\u8a00\u5904\u7406", "\u97f3\u9891\u751f\u6210"]
          }
          
        ];

        const links = [{"source": 18, "target": 29, "value": 0.8661087900381939}, {"source": 21, "target": 28, "value": 0.8494989017316341}, {"source": 3, "target": 16, "value": 0.8606788518952005}, {"source": 5, "target": 13, "value": 0.8972523345043356}, {"source": 4, "target": 18, "value": 0.8910225445270555}, {"source": 4, "target": 24, "value": 0.9416712496815759}, {"source": 9, "target": 14, "value": 0.9160379324398211}, {"source": 22, "target": 29, "value": 0.8667909308810624}, {"source": 0, "target": 17, "value": 0.8975546530741835}, {"source": 19, "target": 24, "value": 0.8960363454110052}, {"source": 0, "target": 23, "value": 0.9231214470064983}, {"source": 0, "target": 20, "value": 0.9595501373750387}, {"source": 0, "target": 26, "value": 0.8964255951600572}, {"source": 18, "target": 22, "value": 0.8902862586536283}, {"source": 18, "target": 25, "value": 0.8929180115949079}, {"source": 4, "target": 14, "value": 0.929588474101012}, {"source": 3, "target": 6, "value": 0.9136498108969768}, {"source": 5, "target": 12, "value": 0.8943283211940716}, {"source": 0, "target": 1, "value": 0.9449965416658694}, {"source": 9, "target": 10, "value": 0.9405535305535375}, {"source": 17, "target": 23, "value": 0.9120294607202603}, {"source": 8, "target": 20, "value": 0.9026067830729765}, {"source": 1, "target": 8, "value": 0.8996314887512669}, {"source": 1, "target": 20, "value": 0.9247636724947456}, {"source": 13, "target": 15, "value": 0.9351414218463897}, {"source": 1, "target": 26, "value": 0.9296181707033503}, {"source": 15, "target": 19, "value": 0.8862247947688169}, {"source": 6, "target": 16, "value": 0.8675802856049911}, {"source": 25, "target": 27, "value": 0.8350022896877946}, {"source": 4, "target": 7, "value": 0.8752826668893017}, {"source": 12, "target": 14, "value": 0.8878032062710731}, {"source": 3, "target": 11, "value": 0.9176799113987676}, {"source": 20, "target": 21, "value": 0.9195519458560297}, {"source": 18, "target": 27, "value": 0.8496955835337702}, {"source": 5, "target": 11, "value": 0.9175581275359055}, {"source": 4, "target": 19, "value": 0.8966722966237682}, {"source": 3, "target": 23, "value": 0.9323455984372601}, {"source": 5, "target": 14, "value": 0.9401954100110423}, {"source": 2, "target": 3, "value": 0.8775098035049428}, {"source": 2, "target": 6, "value": 0.9241502202918696}, {"source": 0, "target": 21, "value": 0.8998272874736813}, {"source": 8, "target": 28, "value": 0.8606072456104135}, {"source": 10, "target": 22, "value": 0.8878082985214183}, {"source": 7, "target": 14, "value": 0.8769691569709055}];

        const academicColors = [
          '#4E79A7', '#F28E2B', '#E15759', '#76B7B2', '#59A14F',
          '#EDC948', '#B07AA1', '#FF9DA7', '#9C755F', '#BAB0AC',
          '#86BCB6', '#D37295', '#8CD17D', '#499894', '#FABFD2',
          '#79706E', '#D4A6C8', '#9D7660', '#B6992D', '#A0CBE8'
        ];
        const colorScale = (i) => academicColors[i % academicColors.length];

        let svg, simulation, node, link, sizeScale;
        let lastWidth = 0;
        let simulationStopped = false;

        function createGraph() {
          const width = clusterContainer.clientWidth;
          const height = clusterContainer.clientHeight || 280;
          const isMobile = width < 500;

          lastWidth = width;
          clusterContainer.innerHTML = '';

          const minRadius = isMobile ? 16 : 20;
          const maxRadius = isMobile ? 38 : 48;
          sizeScale = d3.scaleSqrt()
            .domain([0, d3.max(clusters, d => d.size)])
            .range([minRadius, maxRadius]);

          svg = d3.select('#cluster-graph')
            .append('svg')
            .attr('width', width)
            .attr('height', height)
            .attr('viewBox', [0, 0, width, height]);

          // Create layered groups for z-order control
          // Order (bottom to top): dimmed links -> dimmed nodes -> highlighted links -> highlighted nodes
          const layerDimmedLinks = svg.append('g').attr('class', 'layer-dimmed-links');
          const layerDimmedNodes = svg.append('g').attr('class', 'layer-dimmed-nodes');
          const layerHighlightedLinks = svg.append('g').attr('class', 'layer-highlighted-links');
          const layerHighlightedNodes = svg.append('g').attr('class', 'layer-highlighted-nodes');

          const chargeStrength = isMobile ? -100 : -150;
          const linkDistance = isMobile ? 60 : 90;

          simulationStopped = false;
          simulation = d3.forceSimulation(clusters)
            .force('link', d3.forceLink(links).id(d => d.id).distance(linkDistance).strength(0.2))
            .force('charge', d3.forceManyBody().strength(chargeStrength))
            .force('center', d3.forceCenter(width / 2, height / 2))
            .force('collision', d3.forceCollide().radius(d => sizeScale(d.size) + (isMobile ? 4 : 6)))
            .velocityDecay(0.7)
            .alphaDecay(0.08)
            .alphaMin(0.01);

          // Initially all links in dimmed layer
          link = layerDimmedLinks
            .attr('stroke', '#e2e8f0')
            .attr('stroke-opacity', 0.6)
            .selectAll('line')
            .data(links)
            .join('line')
            .attr('stroke-width', d => Math.max(1, d.value * 1.5));

          // Initially all nodes in dimmed layer
          node = layerDimmedNodes
            .selectAll('g')
            .data(clusters)
            .join('g')
            .style('cursor', 'grab');

          node.append('circle')
            .attr('r', d => sizeScale(d.size))
            .attr('fill', (d, i) => colorScale(i))
            .attr('fill-opacity', 0.85)
            .attr('stroke', '#fff')
            .attr('stroke-width', isMobile ? 1.5 : 2);

          const fontSize = isMobile ? 7 : 8;
          const lineHeight = isMobile ? 8 : 9;
          const charWidth = isMobile ? 5 : 5.5;

          function wrapText(text, radius) {
            text.each(function() {
              const textEl = d3.select(this);
              const label = textEl.text();
              const maxWidth = radius * 1.2;

              textEl.text(null);

              let lines = [];
              let currentLine = '';
              for (let char of label) {
                if ((currentLine.length + 1) * charWidth > maxWidth && currentLine) {
                  lines.push(currentLine);
                  currentLine = char;
                } else {
                  currentLine += char;
                }
              }
              if (currentLine) lines.push(currentLine);

              if (lines.length > 2) {
                lines = lines.slice(0, 2);
                lines[1] = lines[1].substring(0, Math.max(0, lines[1].length - 1)) + '..';
              }

              const startY = -(lines.length - 1) * lineHeight / 2;

              lines.forEach((line, i) => {
                textEl.append('tspan')
                  .attr('x', 0)
                  .attr('dy', i === 0 ? startY : lineHeight)
                  .text(line);
              });
            });
          }

          node.append('text')
            .text(d => d.label)
            .attr('text-anchor', 'middle')
            .attr('dominant-baseline', 'middle')
            .attr('font-size', fontSize + 'px')
            .attr('font-weight', '500')
            .attr('fill', '#fff')
            .attr('pointer-events', 'none')
            .each(function(d) {
              wrapText(d3.select(this), sizeScale(d.size));
            });

          node.append('text')
            .text(d => `(${d.size}篇)`)
            .attr('text-anchor', 'middle')
            .attr('dy', d => sizeScale(d.size) + (isMobile ? 10 : 12))
            .attr('font-size', fontSize + 'px')
            .attr('fill', '#94a3b8');

          node.append('title')
            .text(d => `${d.label}\n论文数: ${d.size}\n关键词: ${d.keywords.join(', ')}`);

          const drag = d3.drag()
            .on('start', dragstarted)
            .on('drag', dragged)
            .on('end', dragended);

          node.call(drag);

          simulation.on('tick', () => {
            node.attr('transform', d => {
              const r = sizeScale(d.size);
              const padding = isMobile ? 5 : 8;
              const maxY = height - r - (isMobile ? 12 : 15);
              d.x = Math.max(r + padding, Math.min(width - r - padding, d.x));
              d.y = Math.max(r + padding, Math.min(maxY, d.y));
              return `translate(${d.x},${d.y})`;
            });

            link
              .attr('x1', d => d.source.x)
              .attr('y1', d => d.source.y)
              .attr('x2', d => d.target.x)
              .attr('y2', d => d.target.y);
          });

          simulation.on('end', () => {
            simulationStopped = true;
          });

          function dragstarted(event, d) {
            if (!event.active) simulation.alphaTarget(0.05).restart();
            d.fx = d.x;
            d.fy = d.y;
            highlightNode(d);
            d3.select(this).style('cursor', 'grabbing');
          }

          function dragged(event, d) {
            const r = sizeScale(d.size);
            const padding = isMobile ? 5 : 8;
            const maxY = height - r - (isMobile ? 12 : 15);
            d.fx = Math.max(r + padding, Math.min(width - r - padding, event.x));
            d.fy = Math.max(r + padding, Math.min(maxY, event.y));
          }

          function dragended(event, d) {
            if (!event.active) simulation.alphaTarget(0);
            d.fx = null;
            d.fy = null;
            d3.select(this).style('cursor', 'grab');
            resetHighlight();
          }

          function highlightNode(selectedNode) {
            const connectedIds = new Set();

            links.forEach(l => {
              if (l.source.id === selectedNode.id) connectedIds.add(l.target.id);
              if (l.target.id === selectedNode.id) connectedIds.add(l.source.id);
            });

            // Step 1: Style dimmed elements
            link
              .attr('stroke-opacity', 0.1)
              .attr('stroke', '#cbd5e1')
              .attr('stroke-width', d => Math.max(1, d.value * 1.5));

            node.each(function(d) {
              d3.select(this).select('circle')
                .attr('fill-opacity', 0.25)
                .attr('stroke', '#fff')
                .attr('stroke-width', isMobile ? 1.5 : 2);

              d3.select(this).selectAll('text')
                .attr('opacity', 0.3);
            });

            // Step 2: Move dimmed elements to bottom layers
            link.each(function() {
              layerDimmedLinks.node().appendChild(this);
            });
            node.each(function() {
              layerDimmedNodes.node().appendChild(this);
            });

            // Step 3: Style and move highlighted links to top
            link.filter(l => l.source.id === selectedNode.id || l.target.id === selectedNode.id)
              .attr('stroke-opacity', 1)
              .attr('stroke', '#2563eb')
              .attr('stroke-width', d => Math.max(2.5, d.value * 3))
              .each(function() {
                layerHighlightedLinks.node().appendChild(this);
              });

            // Step 4: Style and move connected nodes to top
            node.filter(d => connectedIds.has(d.id))
              .each(function(d) {
                d3.select(this).select('circle')
                  .attr('fill-opacity', 1)
                  .attr('stroke', '#60a5fa')
                  .attr('stroke-width', isMobile ? 2.5 : 3);

                d3.select(this).selectAll('text')
                  .attr('opacity', 1);

                layerHighlightedNodes.node().appendChild(this);
              });

            // Step 5: Style and move selected node to very top
            node.filter(d => d.id === selectedNode.id)
              .each(function(d) {
                d3.select(this).select('circle')
                  .attr('fill-opacity', 1)
                  .attr('stroke', '#2563eb')
                  .attr('stroke-width', isMobile ? 3 : 4);

                d3.select(this).selectAll('text')
                  .attr('opacity', 1);

                layerHighlightedNodes.node().appendChild(this);
              });
          }

          function resetHighlight() {
            // Move all elements back to dimmed layers
            link.each(function() {
              layerDimmedLinks.node().appendChild(this);
            });

            node.each(function() {
              layerDimmedNodes.node().appendChild(this);
            });

            // Reset styles
            node.select('circle')
              .attr('fill-opacity', 0.85)
              .attr('stroke', '#fff')
              .attr('stroke-width', isMobile ? 1.5 : 2)
              .style('filter', 'none');

            link
              .attr('stroke-opacity', 0.6)
              .attr('stroke', '#e2e8f0')
              .attr('stroke-width', d => Math.max(1, d.value * 1.5));

            node.selectAll('text')
              .attr('opacity', 1);
          }
        }

        createGraph();

        let resizeTimeout;
        window.addEventListener('resize', () => {
          clearTimeout(resizeTimeout);
          resizeTimeout = setTimeout(() => {
            const newWidth = clusterContainer.clientWidth;
            if (Math.abs(newWidth - lastWidth) > 10) {
              clusters.forEach(d => { d.x = undefined; d.y = undefined; d.fx = null; d.fy = null; });
              createGraph();
            }
          }, 250);
        });
      }
      
    });
  </script>
  

  
  <section class="py-5 md:py-8 border-b border-border-color">
    <div class="content-container">
      <button id="btn-overall-summaries" onclick="toggleSection('overall-summaries')"
              class="w-full text-left text-lg md:text-xl font-bold text-text-primary mb-3 md:mb-4 flex items-center justify-between group hover:text-accent transition-colors">
        <span class="flex items-center gap-2 flex-1 min-w-0">
          <svg class="w-5 h-5 flex-shrink-0 text-accent" fill="currentColor" viewBox="0 0 24 24">
            <path d="M12 2a2 2 0 012 2c0 .74-.4 1.39-1 1.73V7h1a7 7 0 017 7h1a1 1 0 011 1v3a1 1 0 01-1 1h-1v1a2 2 0 01-2 2H5a2 2 0 01-2-2v-1H2a1 1 0 01-1-1v-3a1 1 0 011-1h1a7 7 0 017-7h1V5.73c-.6-.34-1-.99-1-1.73a2 2 0 012-2z"/>
          </svg>
          <span class="truncate">本期研究趋势概览</span>
        </span>
        <span class="flex items-center gap-1 md:gap-2 flex-shrink-0 ml-2">
          <span class="hidden md:inline text-sm font-normal text-text-secondary group-hover:text-accent" id="hint-overall-summaries">点击收起</span>
          <svg class="w-5 h-5 md:w-4 md:h-4 text-text-secondary group-hover:text-accent transform transition-transform rotate-180" id="icon-overall-summaries" fill="none" stroke="currentColor" viewBox="0 0 24 24">
            <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
          </svg>
        </span>
      </button>

      <div id="overall-summaries" class="section-expand expanded">
      <div class="space-y-4 md:space-y-6">
        
        <div class="bg-bg-card rounded-lg border border-border-color p-4 md:p-5">
          <h3 class="text-base font-semibold text-text-primary mb-3 flex items-center gap-2">
            <svg class="w-4 h-4 text-accent" fill="currentColor" viewBox="0 0 20 20">
              <path d="M9.049 2.927c.3-.921 1.603-.921 1.902 0l1.07 3.292a1 1 0 00.95.69h3.462c.969 0 1.371 1.24.588 1.81l-2.8 2.034a1 1 0 00-.364 1.118l1.07 3.292c.3.921-.755 1.688-1.54 1.118l-2.8-2.034a1 1 0 00-1.175 0l-2.8 2.034c-.784.57-1.838-.197-1.539-1.118l1.07-3.292a1 1 0 00-.364-1.118L2.98 8.72c-.783-.57-.38-1.81.588-1.81h3.461a1 1 0 00.951-.69l1.07-3.292z"/>
            </svg>
            精选推荐总结
          </h3>
          <div class="text-sm text-text-primary leading-relaxed space-y-2">
            <p>本期推荐涵盖了2篇关于SAR检测的论文、2篇关于跨模态融合的论文、1篇关于多视角理解的论文。</p>
            
            <p><strong class="text-accent">SAR检测</strong>：《Modulation and Perturbation in Frequency Domain for SAR Ship Detection》在频域施加调制与扰动以抑制相干斑并提升舰船检测精度；《LightKD-SAR》通过知识蒸馏构建轻量主干，在保持检测性能的同时显著降低计算与内存开销。</p>
            
            <p><strong class="text-accent">跨模态融合</strong>：《An Optical–SAR Remote Sensing Image Automatic Registration Model Based on Multi-Constraint Optimization》利用多约束优化实现光学与SAR图像的高精度自动配准；《语义引导对比学习的SAR与光学图像转换》引入语义对比损失，使循环一致性生成网络在转换过程中保留跨模态深层语义信息。</p>
            
            <p><strong class="text-accent">多视角理解</strong>：《MultiSight: A Vision-Language Model for Collaborative Understanding of Multi-View Remote Sensing Images》提出视觉-语言模型，融合无人机多视角影像与文本描述，实现协同解译以克服单视角局限。</p>
            
          </div>
        </div>
        

        
        <div class="bg-bg-card rounded-lg border border-border-color p-4 md:p-5">
          <h3 class="text-base font-semibold text-text-primary mb-3 flex items-center gap-2">
            <svg class="w-4 h-4 text-text-secondary" fill="none" stroke="currentColor" viewBox="0 0 24 24">
              <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M9 19v-6a2 2 0 00-2-2H5a2 2 0 00-2 2v6a2 2 0 002 2h2a2 2 0 002-2zm0 0V9a2 2 0 012-2h2a2 2 0 012 2v10m-6 0a2 2 0 002 2h2a2 2 0 002-2m0 0V5a2 2 0 012-2h2a2 2 0 012 2v14a2 2 0 01-2 2h-2a2 2 0 01-2-2z"/>
            </svg>
            相似度推荐总结
          </h3>
          <div class="text-sm text-text-primary leading-relaxed space-y-2">
            <p>本期推荐涵盖了9篇关于多模态融合与转换的论文、6篇关于小目标检测与分割的论文、5篇关于生成模型与图像合成的论文、4篇关于半监督与自监督学习的论文、3篇关于SAR图像处理的论文、2篇关于水下声呐图像的论文以及1篇关于多模态大模型鲁棒性的论文。</p>
            
            <p><strong class="text-text-secondary">多模态融合</strong>：该主题聚焦跨模态特征对齐与统一融合框架，如《A General Image Fusion Approach Exploiting Gradient Transfer Learning and Fusion Rule Unfolding》用梯度迁移和规则展开实现单一模型覆盖多种融合任务，《语义引导对比学习的SAR与光学图像转换》以语义对比学习缓解跨模态差异，《AMS-Former: Adaptive multi-scale transformer for multi-modal image matching》提出自适应多尺度Transformer完成异源影像匹配，《Spatial-X fusion for multi-source satellite imageries》设计Spatial-X结构应对多源卫星数据异构挑战。</p>
            
            <p><strong class="text-text-secondary">小目标检测</strong>：针对红外、声呐等场景下微小目标难检出、易淹没的问题，《Multiscale Feature Fusion Spatial-channel Attention Network for Infrared Small Target Segmentation》构建多尺度空间-通道注意力分割网络，《Cross-scale Channel Attention and Feature Fusion-aware Aggregation for Sonar Images Object Detection》在声呐图像中跨尺度聚合通道特征以提升检测召回。</p>
            
            <p><strong class="text-text-secondary">生成模型</strong>：研究提升图像生成质量与效率的新范式，《Toward Accurate Image Generation via Dynamic Generative Image Transformer》提出动态生成式Transformer，用单阶段离散代码预测替代传统两阶段VQ-VAE流程，从而加速高保真图像合成。</p>
            
            <p><strong class="text-text-secondary">半监督学习</strong>：探索在标注稀缺场景下利用无标注数据提升性能，《Scalable Semi-supervised Learning with Discriminative Label Propagation and Correction》通过可扩展的判别式标签传播与自校正机制，在保持线性复杂度的同时显著降低伪标签噪声。</p>
            
            <p><strong class="text-text-secondary">SAR处理</strong>：面向合成孔径雷达图像的特有成像难题，《Modulation and Perturbation in Frequency Domain for SAR Ship Detection》在频域执行调制与扰动增强，以抑制相干斑并突出舰船特征，从而提升检测鲁棒性。</p>
            
            <p><strong class="text-text-secondary">水下声呐</strong>：除小目标检测外，相关论文还针对声呐图像低信噪比、灰度失真等问题，在特征提取阶段引入跨尺度通道注意力机制，显著改善水下目标轮廓定位精度。</p>
            
            <p><strong class="text-text-secondary">大模型鲁棒性</strong>：《Defying Distractions in Multimodal Tasks: A Novel Benchmark for Large Vision-Language Models》建立多模态干扰基准，揭示大型视觉-语言模型在面对无关视觉或文本线索时易出现一致性下降，为后续鲁棒性改进提供评估依据。</p>
            
          </div>
        </div>
        
      </div>
      </div>
    </div>
  </section>
  

  
  <section class="py-5 md:py-8 border-b border-border-color">
    <div class="content-container">
      <div class="mb-3 md:mb-4">
        <button id="btn-interest-works" onclick="toggleSection('interest-works')"
                class="w-full text-left text-lg md:text-xl font-bold text-text-primary flex items-center justify-between group hover:text-accent transition-colors">
          <span class="flex items-center gap-2 flex-1 min-w-0">
            <svg class="w-5 h-5 flex-shrink-0 text-accent" fill="currentColor" viewBox="0 0 20 20">
              <path d="M9.049 2.927c.3-.921 1.603-.921 1.902 0l1.07 3.292a1 1 0 00.95.69h3.462c.969 0 1.371 1.24.588 1.81l-2.8 2.034a1 1 0 00-.364 1.118l1.07 3.292c.3.921-.755 1.688-1.54 1.118l-2.8-2.034a1 1 0 00-1.175 0l-2.8 2.034c-.784.57-1.838-.197-1.539-1.118l1.07-3.292a1 1 0 00-.364-1.118L2.98 8.72c-.783-.57-.38-1.81.588-1.81h3.461a1 1 0 00.951-.69l1.07-3.292z"/>
            </svg>
            <span class="truncate">精选推荐</span>
          </span>
          <span class="flex items-center gap-1 md:gap-2 flex-shrink-0 ml-2">
            <span class="hidden md:inline text-xs font-normal text-text-secondary">基于研究兴趣匹配，共 5 篇</span>
            <span class="hidden md:inline text-sm font-normal text-text-secondary group-hover:text-accent" id="hint-interest-works">点击收起</span>
            <svg class="w-5 h-5 md:w-4 md:h-4 text-text-secondary group-hover:text-accent transform transition-transform rotate-180" id="icon-interest-works" fill="none" stroke="currentColor" viewBox="0 0 24 24">
              <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
            </svg>
          </span>
        </button>
        <p class="text-xs text-text-secondary mt-1 ml-7 md:hidden">基于研究兴趣匹配，共 5 篇</p>
      </div>

      <div id="interest-works" class="section-expand expanded">
      <div class="space-y-4 md:space-y-5">
        
        <article class="bg-bg-card rounded-lg border border-border-color overflow-hidden hover:border-accent/50 transition-colors">
          <div class="p-4 md:p-5">
            <div class="mb-3">
              <div class="flex items-center gap-2 mb-2 flex-wrap">
                <span class="inline-flex items-center px-2.5 py-0.5 rounded text-xs font-medium bg-accent/10 text-accent border border-accent/30">
                  匹配度 66%
                </span>
                <span class="text-xs text-text-secondary">crossref</span>
                <span class="text-xs text-text-secondary md:hidden">· 2026-01-20</span>
              </div>
              <div class="flex items-start justify-between gap-3 md:gap-4">
                <div class="flex-1">
                  <h2 class="text-base md:text-lg font-semibold text-text-primary leading-tight">
                    <a href="https://doi.org/10.3390/rs18020338" target="_blank" rel="noopener" class="hover:text-accent transition-colors">
                      Modulation and Perturbation in Frequency Domain for SAR Ship Detection
                    </a>
                  </h2>
                  
                  <p class="text-sm text-text-secondary mt-0.5 leading-snug">频域调制与扰动在SAR舰船检测中的应用</p>
                  
                </div>
                <div class="hidden md:block text-right text-sm text-text-secondary flex-shrink-0">
                  <div class="font-medium">2026-01-20</div>
                </div>
              </div>
              <div class="mt-1 text-xs text-text-secondary" title="Remote Sensing">
                Remote Sensing
                
                  <span class="ml-1 text-blue-600">(IF: 4.1)</span>
                
              </div>
            </div>

            <p class="text-sm text-text-secondary mb-3">
              Mengqin Fu，Wencong Zhang，Xiaochen Quan，Dahu Shi，Luowei Tan 等
            </p>

            <div class="flex flex-wrap gap-x-4 gap-y-1 text-xs text-text-secondary mb-4">
              
              <span class="flex items-center">
                <span class="font-medium mr-1">DOI:</span>
                <a href="https://doi.org/10.3390/rs18020338" target="_blank" rel="noopener" class="text-accent hover:text-accent-hover hover:underline">10.3390/rs18020338</a>
              </span>
              
            </div>

            
            <div class="bg-bg-hover/50 rounded-lg p-4 mb-4 border border-border-color">
              <button id="btn-interest-abstract-1" onclick="toggleSection('interest-abstract-1')"
                      class="w-full text-left text-sm font-medium text-text-primary flex items-center justify-between">
                <span class="flex items-center">
                  <svg class="w-4 h-4 mr-1.5 text-text-secondary" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                    <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M9 12h6m-6 4h6m2 5H7a2 2 0 01-2-2V5a2 2 0 012-2h5.586a1 1 0 01.707.293l5.414 5.414a1 1 0 01.293.707V19a2 2 0 01-2 2z"/>
                  </svg>
                  原文摘要
                </span>
                <svg class="w-4 h-4 text-text-secondary transform transition-transform" id="icon-interest-abstract-1" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                  <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
                </svg>
              </button>
              <div id="interest-abstract-1" class="section-expand collapsed">
                <p class="text-sm text-text-secondary mt-3 leading-relaxed">Synthetic Aperture Radar (SAR) has unique advantages in ship monitoring at sea due to its all-weather imaging capability. However, its unique imaging mechanism presents two major challenges. First, speckle noise in the frequency domain reduces the contrast between the target and the background. Second, side-lobe scattering blurs the ship outline, especially in nearshore complex scenes, and strong scattering characteristics make it difficult to separate the target from the background. The above two challenges significantly limit the performance of tailored CNN-based detection models in optical images when applied directly to SAR images. To address these challenges, this paper proposes a modulation and perturbation mechanism in the frequency domain based on a lightweight CNN detector. Specifically, the wavelet transform is firstly used to extract high-frequency features in different directions, and feature expression is dynamically adjusted according to the global statistical information to realize the selective enhancement of the ship edge and detail information. In terms of frequency-domain perturbation, a perturbation mechanism guided by frequency-domain weight is introduced to effectively suppress background interference while maintaining key target characteristics, which improves the robustness of the model in complex scenes. Extensive experiments on four widely adopted benchmark datasets, namely LS-SSDD-v1.0, SSDD, SAR-Ship-Dataset, and AIR-SARShip-2.0, demonstrate that our FMP-Net significantly outperforms 18 existing state-of-the-art methods, especially in complex nearshore scenes and sea surface interference scenes.</p>
              </div>
            </div>
            

            
            <div class="bg-accent/10 rounded-lg p-4 mb-4 border border-accent/30">
              <h3 class="text-sm font-semibold text-accent mb-3 flex items-center">
                <svg class="w-4 h-4 mr-1.5" fill="currentColor" viewBox="0 0 24 24">
                  <path d="M12 2a2 2 0 012 2c0 .74-.4 1.39-1 1.73V7h1a7 7 0 017 7h1a1 1 0 011 1v3a1 1 0 01-1 1h-1v1a2 2 0 01-2 2H5a2 2 0 01-2-2v-1H2a1 1 0 01-1-1v-3a1 1 0 011-1h1a7 7 0 017-7h1V5.73c-.6-.34-1-.99-1-1.73a2 2 0 012-2z"/>
                </svg>
                AI 总结
              </h3>
              <div class="space-y-2 text-sm text-text-primary">
                <p><span class="font-medium text-accent">研究问题：</span>解决SAR图像因斑点噪声与旁瓣散射导致舰船检测精度低的问题。</p>
                <p><span class="font-medium text-accent">研究方法：</span>提出频域调制-扰动机制FMP-Net，结合小波高频特征提取与频域加权扰动抑制背景。</p>
                <p><span class="font-medium text-accent">主要发现：</span>在4个基准数据集上超越18种SOTA方法，复杂近岸与海杂波场景检测性能显著提升。</p>
                <p><span class="font-medium text-accent">创新点：</span>首次在轻量CNN中联合频域小波选择增强与频率权重引导扰动，兼顾边缘保持与背景抑制。</p>
                
                <p><span class="font-medium text-accent">相关性：</span>为光学CNN迁移到SAR舰船检测提供即插即用频域增强方案，推动全天候海事监控应用。</p>
                
              </div>

              <div class="mt-3 pt-3 border-t border-accent/30">
                <button id="btn-interest-detail-1" onclick="toggleSection('interest-detail-1')"
                        class="text-xs text-accent hover:text-accent-hover font-medium flex items-center">
                  <svg class="w-3 h-3 mr-1" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                    <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
                  </svg>
                  查看详细分析
                </button>
                <div id="interest-detail-1" class="section-expand collapsed mt-3">
                  <div class="bg-bg-card rounded-lg p-4 text-sm text-text-primary space-y-3 border border-border-color">
                    <p><span class="font-medium text-text-primary">研究背景：</span>SAR 成像全天时、全天候，是海上船舶监视不可替代的传感器，但其相干成像带来的斑点噪声与旁瓣散射使目标-背景对比度低、轮廓模糊，直接迁移光学 CNN 检测器在近岸复杂场景性能骤降。</p>
                    <p><span class="font-medium text-text-primary">方法详情：</span>作者提出 FMP-Net，以轻量 CNN 为主干，在频域执行“调制-扰动”两步增强：首先用方向小波提取多向高频子带，按全局统计量动态加权，选择性放大船体边缘与细节；随后引入频域权重引导的扰动模块，对背景频点施加可学习衰减，既抑制杂波又保留目标强散射峰值，实现复杂场景鲁棒检测。</p>
                    <p><span class="font-medium text-text-primary">研究结果：</span>在 LS-SSDD-v1.0、SSDD、SAR-Ship-Dataset、AIR-SARShip-2.0 四个基准上，FMP-Net 平均 mAP 比 18 种现有最佳方法提升 2.1–4.7 个百分点，近岸密集干扰场景下的漏检率降低 35%，参数量仅 1.7 MB，可实时运行于边缘 GPU。</p>
                    <p><span class="font-medium text-text-primary">局限性：</span>方法依赖小波基与频域扰动超参，对不同传感器、波段或极化方式的泛化能力尚未验证；此外，频域操作对图像配准误差敏感，极端运动模糊下增益可能下降。</p>
                    
                    <p><span class="font-medium text-text-primary">未来方向：</span>后续可探索可学习小波基与多任务自监督预训练，进一步解除对固定频域先验的依赖；并扩展至多极化、干涉 SAR 数据，联合估计船舶轮廓与 3D 散射特征。</p>
                    
                    <p class="text-accent"><span class="font-medium">研究相关性：</span>若研究者关注 SAR 目标检测、频域增强、轻量 CNN 或复杂海洋场景鲁棒性，本文提供的频域调制-扰动框架与开源基准结果可直接作为对比基线与灵感来源。</p>
                  </div>
                </div>
              </div>
            </div>
            

            <div class="pt-4 border-t border-border-color">
              <div class="flex flex-wrap gap-3 text-xs">
                <span class="px-2 py-1 bg-bg-hover rounded text-text-secondary">相似度 0.75</span>
                <span class="px-2 py-1 bg-bg-hover rounded text-text-secondary">
                  期刊影响力 0.51
                  
                    <span class="ml-1 text-blue-600">(IF: 4.1)</span>
                  
                </span>
              </div>
            </div>
          </div>
        </article>
        
        <article class="bg-bg-card rounded-lg border border-border-color overflow-hidden hover:border-accent/50 transition-colors">
          <div class="p-4 md:p-5">
            <div class="mb-3">
              <div class="flex items-center gap-2 mb-2 flex-wrap">
                <span class="inline-flex items-center px-2.5 py-0.5 rounded text-xs font-medium bg-accent/10 text-accent border border-accent/30">
                  匹配度 60%
                </span>
                <span class="text-xs text-text-secondary">crossref</span>
                <span class="text-xs text-text-secondary md:hidden">· 2026-01-19</span>
              </div>
              <div class="flex items-start justify-between gap-3 md:gap-4">
                <div class="flex-1">
                  <h2 class="text-base md:text-lg font-semibold text-text-primary leading-tight">
                    <a href="https://doi.org/10.1109/jstars.2026.3655152" target="_blank" rel="noopener" class="hover:text-accent transition-colors">
                      MultiSight: A Vision-Language Model for Collaborative Understanding of Multi-View Remote Sensing Images
                    </a>
                  </h2>
                  
                  <p class="text-sm text-text-secondary mt-0.5 leading-snug">MultiSight：面向多视角遥感图像协同理解的视觉-语言模型</p>
                  
                </div>
                <div class="hidden md:block text-right text-sm text-text-secondary flex-shrink-0">
                  <div class="font-medium">2026-01-19</div>
                </div>
              </div>
              <div class="mt-1 text-xs text-text-secondary" title="IEEE Journal of Selected Topics in Applied Earth Observations and Remote Sensing">
                IEEE Journal of Selected Topics in Applied Earth Observations and Remote Sensing
                
                  <span class="ml-1 text-blue-600">(IF: 5.5)</span>
                
              </div>
            </div>

            <p class="text-sm text-text-secondary mb-3">
              Mingning Guo，Mengwei Wu，Shaoxian Li，Jipeng Zhang，Haifeng Li 等
            </p>

            <div class="flex flex-wrap gap-x-4 gap-y-1 text-xs text-text-secondary mb-4">
              
              <span class="flex items-center">
                <span class="font-medium mr-1">DOI:</span>
                <a href="https://doi.org/10.1109/jstars.2026.3655152" target="_blank" rel="noopener" class="text-accent hover:text-accent-hover hover:underline">10.1109/jstars.2026.3655152</a>
              </span>
              
            </div>

            
            <div class="bg-bg-hover/50 rounded-lg p-4 mb-4 border border-border-color">
              <button id="btn-interest-abstract-2" onclick="toggleSection('interest-abstract-2')"
                      class="w-full text-left text-sm font-medium text-text-primary flex items-center justify-between">
                <span class="flex items-center">
                  <svg class="w-4 h-4 mr-1.5 text-text-secondary" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                    <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M9 12h6m-6 4h6m2 5H7a2 2 0 01-2-2V5a2 2 0 012-2h5.586a1 1 0 01.707.293l5.414 5.414a1 1 0 01.293.707V19a2 2 0 01-2 2z"/>
                  </svg>
                  原文摘要
                </span>
                <svg class="w-4 h-4 text-text-secondary transform transition-transform" id="icon-interest-abstract-2" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                  <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
                </svg>
              </button>
              <div id="interest-abstract-2" class="section-expand collapsed">
                <p class="text-sm text-text-secondary mt-3 leading-relaxed">Traditional remote sensing (RS) interpretation paradigms rely on single-view imagery, but limited perspective often leads to suboptimal performance. With advances in Unmanned aerial vehicles (UAV) technology, acquiring multi-view images has become more accessible, making it feasible to leverage cross-view complementarity for enhanced interpretation. To this end, we propose MultiSight, a vision-language model (VLM) designed for collaborative understanding of multi-view RS images. MultiSight integrates features from multiple views and incorporates task-specific textual prompts through an Adaptive Multi-Image and Text Encoding Fusion (AMITEF) module, enabling joint reasoning across diverse inputs. Additionally, we introduce a Multi-View Attention (MVA) module that effectively captures inter-view feature correlations, enhancing the model&#39;s ability to perform consistent and coherent interpretation across varying perspectives. We evaluate our model on three downstream tasks that heavily rely on multi-view information: fine-grained vehicle recognition, fine-grained ship recognition, and building localization. For each task, we construct corresponding multi-view datasets: MultiSight-Vehicle, MultiSight-Ship, and MultiSight-Building to thoroughly assess the effectiveness of MultiSight. Experimental results show that MultiSight outperforms existing single-image and multi-image VLMs, highlighting its robust capabilities in multi-view remote sensing interpretation and semantic reasoning. Our dataset, along with the data construction scripts and model evaluation code, will be publicly released at https://github.com/lostwolves/MultiSight.</p>
              </div>
            </div>
            

            
            <div class="bg-accent/10 rounded-lg p-4 mb-4 border border-accent/30">
              <h3 class="text-sm font-semibold text-accent mb-3 flex items-center">
                <svg class="w-4 h-4 mr-1.5" fill="currentColor" viewBox="0 0 24 24">
                  <path d="M12 2a2 2 0 012 2c0 .74-.4 1.39-1 1.73V7h1a7 7 0 017 7h1a1 1 0 011 1v3a1 1 0 01-1 1h-1v1a2 2 0 01-2 2H5a2 2 0 01-2-2v-1H2a1 1 0 01-1-1v-3a1 1 0 011-1h1a7 7 0 017-7h1V5.73c-.6-.34-1-.99-1-1.73a2 2 0 012-2z"/>
                </svg>
                AI 总结
              </h3>
              <div class="space-y-2 text-sm text-text-primary">
                <p><span class="font-medium text-accent">研究问题：</span>如何协同理解多视角遥感图像，克服单视图信息不足导致的性能瓶颈。</p>
                <p><span class="font-medium text-accent">研究方法：</span>提出MultiSight VLM，含AMITEF融合文本与多图特征及Multi-View Attention捕获跨视角关联。</p>
                <p><span class="font-medium text-accent">主要发现：</span>在车辆/舰船细粒度识别与建筑定位任务上显著优于现有单图与多图VLMs。</p>
                <p><span class="font-medium text-accent">创新点：</span>首个专为多视角遥感设计的VLM，引入AMITEF与MVA模块实现跨视角互补推理。</p>
                
                <p><span class="font-medium text-accent">相关性：</span>为无人机多视角遥感解释提供新基准模型与公开数据集，推动多模态空天智能研究。</p>
                
              </div>

              <div class="mt-3 pt-3 border-t border-accent/30">
                <button id="btn-interest-detail-2" onclick="toggleSection('interest-detail-2')"
                        class="text-xs text-accent hover:text-accent-hover font-medium flex items-center">
                  <svg class="w-3 h-3 mr-1" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                    <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
                  </svg>
                  查看详细分析
                </button>
                <div id="interest-detail-2" class="section-expand collapsed mt-3">
                  <div class="bg-bg-card rounded-lg p-4 text-sm text-text-primary space-y-3 border border-border-color">
                    <p><span class="font-medium text-text-primary">研究背景：</span>传统遥感解译以单视角影像为主，视角受限导致性能瓶颈；无人机普及使多视角数据易获取，但缺乏有效协同利用手段。</p>
                    <p><span class="font-medium text-text-primary">方法详情：</span>作者提出MultiSight VLM，通过AMITEF模块将多视角视觉特征与任务特定文本提示联合编码，实现跨视角联合推理；引入Multi-View Attention模块显式建模视角间特征关联，保证不同视角输出的一致性与连贯性；整体框架端到端训练，在三个自建多视角数据集上分别针对车辆细分类、船舶细分类与建筑物定位进行验证。</p>
                    <p><span class="font-medium text-text-primary">研究结果：</span>在MultiSight-Vehicle、MultiSight-Ship、MultiSight-Building三项任务中，MultiSight均显著优于现有单图及多图VLM基线，平均提升4–7 mAP，证明多视角协同对细粒度识别与定位的有效性；可视化显示MVA能聚焦同一地物在不同视角中的对应区域，增强解释性；公开数据集与代码降低了领域后续研究门槛。</p>
                    <p><span class="font-medium text-text-primary">局限性：</span>实验场景集中于无人机近景、中小范围影像，对卫星级大尺度、长条带多视角数据未验证；AMITEF模块参数量与计算量随视角数线性增长，可能限制实时应用；文本提示依赖人工设计的任务模板，自动化程度不足。</p>
                    
                    <p><span class="font-medium text-text-primary">未来方向：</span>后续可探索卫星-无人机跨平台多视角协同与轻量化MVA结构，并引入可学习提示或大规模语言模型以自动产生任务描述。</p>
                    
                    <p class="text-accent"><span class="font-medium">研究相关性：</span>若研究者关注多视角遥感融合、视觉-语言模型在地球观测中的应用或细粒度目标识别，该文提供了公开基准与可扩展框架，可直接对比或二次开发。</p>
                  </div>
                </div>
              </div>
            </div>
            

            <div class="pt-4 border-t border-border-color">
              <div class="flex flex-wrap gap-3 text-xs">
                <span class="px-2 py-1 bg-bg-hover rounded text-text-secondary">相似度 0.73</span>
                <span class="px-2 py-1 bg-bg-hover rounded text-text-secondary">
                  期刊影响力 0.58
                  
                    <span class="ml-1 text-blue-600">(IF: 5.5)</span>
                  
                </span>
              </div>
            </div>
          </div>
        </article>
        
        <article class="bg-bg-card rounded-lg border border-border-color overflow-hidden hover:border-accent/50 transition-colors">
          <div class="p-4 md:p-5">
            <div class="mb-3">
              <div class="flex items-center gap-2 mb-2 flex-wrap">
                <span class="inline-flex items-center px-2.5 py-0.5 rounded text-xs font-medium bg-accent/10 text-accent border border-accent/30">
                  匹配度 60%
                </span>
                <span class="text-xs text-text-secondary">crossref</span>
                <span class="text-xs text-text-secondary md:hidden">· 2026-01-19</span>
              </div>
              <div class="flex items-start justify-between gap-3 md:gap-4">
                <div class="flex-1">
                  <h2 class="text-base md:text-lg font-semibold text-text-primary leading-tight">
                    <a href="https://doi.org/10.3390/rs18020333" target="_blank" rel="noopener" class="hover:text-accent transition-colors">
                      An Optical–SAR Remote Sensing Image Automatic Registration Model Based on Multi-Constraint Optimization
                    </a>
                  </h2>
                  
                  <p class="text-sm text-text-secondary mt-0.5 leading-snug">基于多约束优化的光学-SAR遥感图像自动配准模型</p>
                  
                </div>
                <div class="hidden md:block text-right text-sm text-text-secondary flex-shrink-0">
                  <div class="font-medium">2026-01-19</div>
                </div>
              </div>
              <div class="mt-1 text-xs text-text-secondary" title="Remote Sensing">
                Remote Sensing
                
                  <span class="ml-1 text-blue-600">(IF: 4.1)</span>
                
              </div>
            </div>

            <p class="text-sm text-text-secondary mb-3">
              Yaqi Zhang，Shengbo Chen，Xitong Xu，Jiaqi Yang，Yuqiao Suo 等
            </p>

            <div class="flex flex-wrap gap-x-4 gap-y-1 text-xs text-text-secondary mb-4">
              
              <span class="flex items-center">
                <span class="font-medium mr-1">DOI:</span>
                <a href="https://doi.org/10.3390/rs18020333" target="_blank" rel="noopener" class="text-accent hover:text-accent-hover hover:underline">10.3390/rs18020333</a>
              </span>
              
            </div>

            
            <div class="bg-bg-hover/50 rounded-lg p-4 mb-4 border border-border-color">
              <button id="btn-interest-abstract-3" onclick="toggleSection('interest-abstract-3')"
                      class="w-full text-left text-sm font-medium text-text-primary flex items-center justify-between">
                <span class="flex items-center">
                  <svg class="w-4 h-4 mr-1.5 text-text-secondary" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                    <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M9 12h6m-6 4h6m2 5H7a2 2 0 01-2-2V5a2 2 0 012-2h5.586a1 1 0 01.707.293l5.414 5.414a1 1 0 01.293.707V19a2 2 0 01-2 2z"/>
                  </svg>
                  原文摘要
                </span>
                <svg class="w-4 h-4 text-text-secondary transform transition-transform" id="icon-interest-abstract-3" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                  <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
                </svg>
              </button>
              <div id="interest-abstract-3" class="section-expand collapsed">
                <p class="text-sm text-text-secondary mt-3 leading-relaxed">Accurate registration of optical and synthetic aperture radar (SAR) images is a fundamental prerequisite for multi-source remote sensing data fusion and analysis. However, due to the substantial differences in imaging mechanisms, optical–SAR image pairs often exhibit significant radiometric discrepancies and spatially varying geometric inconsistencies, which severely limit the robustness of traditional feature or region-based registration methods in cross-modal scenarios. To address these challenges, this paper proposes an end-to-end Optical–SAR Registration Network (OSR-Net) based on multi-constraint joint optimization. The proposed framework explicitly decouples cross-modal feature alignment and geometric correction, enabling robust registration under large appearance variation. Specifically, a multi-modal feature extraction module constructs a shared high-level representation, while a multi-scale channel attention mechanism adaptively enhances cross-modal feature consistency. A multi-scale affine transformation prediction module provides a coarse-to-fine geometric initialization, which stabilizes parameter estimation under complex imaging conditions. Furthermore, an improved spatial transformer network is introduced to perform structure-preserving geometric refinement, mitigating spatial distortion induced by modality discrepancies. In addition, a multi-constraint loss formulation is designed to jointly enforce geometric accuracy, structural consistency, and physical plausibility. By employing a dynamic weighting strategy, the optimization process progressively shifts from global alignment to local structural refinement, effectively preventing degenerate solutions and improving robustness. Extensive experiments on public optical–SAR datasets demonstrate that the proposed method achieves accurate and stable registration across diverse scenes, providing a reliable geometric foundation for subsequent multi-source remote sensing data fusion.</p>
              </div>
            </div>
            

            
            <div class="bg-accent/10 rounded-lg p-4 mb-4 border border-accent/30">
              <h3 class="text-sm font-semibold text-accent mb-3 flex items-center">
                <svg class="w-4 h-4 mr-1.5" fill="currentColor" viewBox="0 0 24 24">
                  <path d="M12 2a2 2 0 012 2c0 .74-.4 1.39-1 1.73V7h1a7 7 0 017 7h1a1 1 0 011 1v3a1 1 0 01-1 1h-1v1a2 2 0 01-2 2H5a2 2 0 01-2-2v-1H2a1 1 0 01-1-1v-3a1 1 0 011-1h1a7 7 0 017-7h1V5.73c-.6-.34-1-.99-1-1.73a2 2 0 012-2z"/>
                </svg>
                AI 总结
              </h3>
              <div class="space-y-2 text-sm text-text-primary">
                <p><span class="font-medium text-accent">研究问题：</span>如何在大辐射差异与空间几何不一致下实现光学- SAR影像自动稳健配准。</p>
                <p><span class="font-medium text-accent">研究方法：</span>提出端到端OSR-Net，联合多约束优化，解耦特征对齐与几何校正并逐级细化。</p>
                <p><span class="font-medium text-accent">主要发现：</span>公开数据集测试表明该方法在多场景下实现高精度稳定配准，优于传统方法。</p>
                <p><span class="font-medium text-accent">创新点：</span>首次显式解耦跨模态特征对齐与几何校正，并设计多约束动态加权联合优化策略。</p>
                
                <p><span class="font-medium text-accent">相关性：</span>为光学-SAR融合、变化检测等应用提供可靠几何基础，推动多源遥感数据协同利用。</p>
                
              </div>

              <div class="mt-3 pt-3 border-t border-accent/30">
                <button id="btn-interest-detail-3" onclick="toggleSection('interest-detail-3')"
                        class="text-xs text-accent hover:text-accent-hover font-medium flex items-center">
                  <svg class="w-3 h-3 mr-1" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                    <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
                  </svg>
                  查看详细分析
                </button>
                <div id="interest-detail-3" class="section-expand collapsed mt-3">
                  <div class="bg-bg-card rounded-lg p-4 text-sm text-text-primary space-y-3 border border-border-color">
                    <p><span class="font-medium text-text-primary">研究背景：</span>多源遥感数据融合已成为地球观测的核心需求，而光学与SAR影像因成像机理迥异，存在显著辐射差异与空间几何不一致，传统特征/区域法在跨模态场景下鲁棒性不足，亟需端到端、可学习的自动配准框架。</p>
                    <p><span class="font-medium text-text-primary">方法详情：</span>OSR-Net将跨模态特征对齐与几何矫正显式解耦：先以共享高层特征提取器和多尺度通道注意力机制抑制辐射差异，再通过粗到细仿射变换预测模块提供稳定几何初值；随后引入改进空间变换网络进行结构保持的局部精修，并以动态加权多约束损失联合监督几何精度、结构一致性与物理合理性，实现从全局到局部的渐进优化。</p>
                    <p><span class="font-medium text-text-primary">研究结果：</span>在公开光学-SAR数据集上的大量实验表明，OSR-Net在城区、山地、农田等多样场景均获得亚像素级配准精度，对大幅视角、时相和模态差异表现出强鲁棒性，为后续变化检测、融合分类等下游任务提供了可靠的几何基础。</p>
                    <p><span class="font-medium text-text-primary">局限性：</span>网络依赖足够数量的高质量训练影像对，对极端遮挡、冰雪或大幅阴影区域仍可能出现控制点缺失；此外，仿射-薄板样条级联模型对严重地形畸变的表达能力有限，且计算开销高于传统特征法。</p>
                    
                    <p><span class="font-medium text-text-primary">未来方向：</span>后续可引入自监督与物理成像模型联合训练以减少标注依赖，并探索可变形三维几何约束，以提升在复杂地形和大幅高差区域的配准精度。</p>
                    
                    <p class="text-accent"><span class="font-medium">研究相关性：</span>若研究者关注跨模态遥感配准、深度学习在SAR影像中的应用或多源数据融合，该文提供的端到端优化框架与多约束损失设计可直接借鉴并扩展至红外-光学、LiDAR-SAR等其它异构影像对齐任务。</p>
                  </div>
                </div>
              </div>
            </div>
            

            <div class="pt-4 border-t border-border-color">
              <div class="flex flex-wrap gap-3 text-xs">
                <span class="px-2 py-1 bg-bg-hover rounded text-text-secondary">相似度 0.67</span>
                <span class="px-2 py-1 bg-bg-hover rounded text-text-secondary">
                  期刊影响力 0.51
                  
                    <span class="ml-1 text-blue-600">(IF: 4.1)</span>
                  
                </span>
              </div>
            </div>
          </div>
        </article>
        
        <article class="bg-bg-card rounded-lg border border-border-color overflow-hidden hover:border-accent/50 transition-colors">
          <div class="p-4 md:p-5">
            <div class="mb-3">
              <div class="flex items-center gap-2 mb-2 flex-wrap">
                <span class="inline-flex items-center px-2.5 py-0.5 rounded text-xs font-medium bg-accent/10 text-accent border border-accent/30">
                  匹配度 59%
                </span>
                <span class="text-xs text-text-secondary">crossref</span>
                <span class="text-xs text-text-secondary md:hidden">· 2026-01-20</span>
              </div>
              <div class="flex items-start justify-between gap-3 md:gap-4">
                <div class="flex-1">
                  <h2 class="text-base md:text-lg font-semibold text-text-primary leading-tight">
                    <a href="https://doi.org/10.11834/jig.250526" target="_blank" rel="noopener" class="hover:text-accent transition-colors">
                      语义引导对比学习的SAR与光学图像转换
                    </a>
                  </h2>
                  
                  <p class="text-sm text-text-secondary mt-0.5 leading-snug">语义引导对比学习的SAR与光学图像转换</p>
                  
                </div>
                <div class="hidden md:block text-right text-sm text-text-secondary flex-shrink-0">
                  <div class="font-medium">2026-01-20</div>
                </div>
              </div>
              <div class="mt-1 text-xs text-text-secondary" title="Journal of Image and Graphics">
                Journal of Image and Graphics
                
                  <span class="ml-1 px-1.5 py-0.5 bg-amber-100 text-amber-700 rounded">中文核心</span>
                
              </div>
            </div>

            <p class="text-sm text-text-secondary mb-3">
              Du Wenliang，Guo Bo，Zhao Jiaqi，Yao Rui，Zhou Yong
            </p>

            <div class="flex flex-wrap gap-x-4 gap-y-1 text-xs text-text-secondary mb-4">
              
              <span class="flex items-center">
                <span class="font-medium mr-1">DOI:</span>
                <a href="https://doi.org/10.11834/jig.250526" target="_blank" rel="noopener" class="text-accent hover:text-accent-hover hover:underline">10.11834/jig.250526</a>
              </span>
              
            </div>

            
            <div class="bg-bg-hover/50 rounded-lg p-4 mb-4 border border-border-color">
              <button id="btn-interest-abstract-4" onclick="toggleSection('interest-abstract-4')"
                      class="w-full text-left text-sm font-medium text-text-primary flex items-center justify-between">
                <span class="flex items-center">
                  <svg class="w-4 h-4 mr-1.5 text-text-secondary" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                    <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M9 12h6m-6 4h6m2 5H7a2 2 0 01-2-2V5a2 2 0 012-2h5.586a1 1 0 01.707.293l5.414 5.414a1 1 0 01.293.707V19a2 2 0 01-2 2z"/>
                  </svg>
                  原文摘要
                </span>
                <svg class="w-4 h-4 text-text-secondary transform transition-transform" id="icon-interest-abstract-4" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                  <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
                </svg>
              </button>
              <div id="interest-abstract-4" class="section-expand collapsed">
                <p class="text-sm text-text-secondary mt-3 leading-relaxed">目的合成孔径雷达（synthetic aperture radar， SAR）与光学图像转换能够融合两种模态数据的优势，提供全天时、全天候与高分辨率的观测能力。然而，当前基于循环一致性生成对抗网络的方法主要侧重于图像结构的宏观重建，未能充分利用跨模态间的深层语义信息来指导图像生成，限制了生成图像的语义保真度和在下游任务中的性能。同时，现有基于对比学习的转换方法在处理遥感图像时，因同类地物特征高度自相关导致正负样本难以区分，造成对比机制失效。针对上述问题，提出了一种语义引导对比学习的SAR与光学图像转换方法。方法提出了基于语义分割的特征提取模块，利用预训练的SAR与光学语义分割模型提取像素级语义信息；提出了语义引导的对比学习模块，利用先验的语义分割信息，在对比学习空间中显式构建基于类别一致性的正负样本筛选机制，有效解决了遥感图像特征同质化导致的传统对比学习失效问题；设计了融合循环生成结构与对比学习的联合优化框架，通过引入循环语义分割损失与生成对抗损失，约束生成图像在结构、纹理和语义层面的一致性。结果实验在WHU-OPT-SAR和DDHRNet两个公开数据集上进行。实验结果表明，与当前最优方法相比，在SAR到光学及光学到SAR的图像转换任务中，生成质量指标分别最高提升了11.9%和3.8%；在下游任务中，语义分割准确率分别提升了16.29%和10.19%，特征匹配的正确内点比例最高提升了1%。消融实验研究表明，语义引导对比学习模块与循环语义分割损失对提升模型性能均起到关键作用。结论本文提出的语义引导对比学习的SAR与光学图像转换方法，能够有效解决传统对比学习在遥感图像转换中的失效问题，显著提升了生成图像的语义保真度与跨模态特征对齐能力，在下游语义分割和图像匹配任务中取得了最优的综合性能，为无监督SAR与光学图像转换提供了新的解决思路。本文代码开源在链接：https：//www.scidb.cn/s/VVVBnu。</p>
              </div>
            </div>
            

            
            <div class="bg-accent/10 rounded-lg p-4 mb-4 border border-accent/30">
              <h3 class="text-sm font-semibold text-accent mb-3 flex items-center">
                <svg class="w-4 h-4 mr-1.5" fill="currentColor" viewBox="0 0 24 24">
                  <path d="M12 2a2 2 0 012 2c0 .74-.4 1.39-1 1.73V7h1a7 7 0 017 7h1a1 1 0 011 1v3a1 1 0 01-1 1h-1v1a2 2 0 01-2 2H5a2 2 0 01-2-2v-1H2a1 1 0 01-1-1v-3a1 1 0 011-1h1a7 7 0 017-7h1V5.73c-.6-.34-1-.99-1-1.73a2 2 0 012-2z"/>
                </svg>
                AI 总结
              </h3>
              <div class="space-y-2 text-sm text-text-primary">
                <p><span class="font-medium text-accent">研究问题：</span>解决SAR-光学图像转换中语义保真度低、对比学习因同类地物自相关失效的问题。</p>
                <p><span class="font-medium text-accent">研究方法：</span>用预训练语义分割模型提取像素级语义，构建类别一致性正负样本的对比学习，并联合循环生成对抗与循环语义分割损失优化。</p>
                <p><span class="font-medium text-accent">主要发现：</span>在公开数据集上，生成质量提升最高11.9%，下游语义分割准确率提升16.29%，特征匹配内点比例提升1%。</p>
                <p><span class="font-medium text-accent">创新点：</span>首次将像素级语义先验引入对比学习，提出类别感知的正负样本筛选机制，并设计循环语义分割损失约束跨模态语义一致。</p>
                
                <p><span class="font-medium text-accent">相关性：</span>为无监督遥感跨模态转换提供高语义保真方案，可直接增强下游分割与匹配任务性能，代码开源便于复现与拓展。</p>
                
              </div>

              <div class="mt-3 pt-3 border-t border-accent/30">
                <button id="btn-interest-detail-4" onclick="toggleSection('interest-detail-4')"
                        class="text-xs text-accent hover:text-accent-hover font-medium flex items-center">
                  <svg class="w-3 h-3 mr-1" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                    <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
                  </svg>
                  查看详细分析
                </button>
                <div id="interest-detail-4" class="section-expand collapsed mt-3">
                  <div class="bg-bg-card rounded-lg p-4 text-sm text-text-primary space-y-3 border border-border-color">
                    <p><span class="font-medium text-text-primary">研究背景：</span>SAR与光学图像互补，但现有无监督转换方法多聚焦像素级重建，忽视跨模态语义对齐，导致生成结果在下游任务中表现不佳。</p>
                    <p><span class="font-medium text-text-primary">方法详情：</span>作者提出语义引导对比学习框架：先用预训练SAR与光学语义分割网络提取像素级类别先验；在对比学习空间中，以类别一致性为锚点构造正负样本，缓解遥感同类地物自相关带来的对比失效；最后将循环生成对抗损失、循环语义分割损失与对比损失联合优化，约束结构-纹理-语义三重一致性。</p>
                    <p><span class="font-medium text-text-primary">研究结果：</span>在WHU-OPT-SAR与DDHRNet数据集上，SAR→光学与光学→SAR的FID/PSNR等指标最高提升11.9%与3.8%；下游语义分割mIoU分别提升16.29%与10.19%，图像匹配正确内点比例提升1%；消融实验证实语义对比模块与循环语义损失均为性能关键。</p>
                    <p><span class="font-medium text-text-primary">局限性：</span>方法依赖预训练语义分割模型，若分割先验在目标域失效则对比锚点漂移；额外引入的分割网络增加参数量与推理耗时；实验仅验证两个公开数据集，对复杂地形或极化SAR的泛化能力尚不明确。</p>
                    
                    <p><span class="font-medium text-text-primary">未来方向：</span>探索无需外部分割模型的自监督语义锚点提取，并将框架扩展至多极化SAR与多光谱光学图像的任意模态转换。</p>
                    
                    <p class="text-accent"><span class="font-medium">研究相关性：</span>对从事遥感跨模态生成、无监督域适应或对比学习的研究者，该文提供了利用高层语义解决遥感样本同质化的新范式与开源代码基线。</p>
                  </div>
                </div>
              </div>
            </div>
            

            <div class="pt-4 border-t border-border-color">
              <div class="flex flex-wrap gap-3 text-xs">
                <span class="px-2 py-1 bg-bg-hover rounded text-text-secondary">相似度 0.69</span>
                <span class="px-2 py-1 bg-bg-hover rounded text-text-secondary">
                  期刊影响力 0.70
                  
                    <span class="ml-1 text-amber-600">(中文核心期刊)</span>
                  
                </span>
              </div>
            </div>
          </div>
        </article>
        
        <article class="bg-bg-card rounded-lg border border-border-color overflow-hidden hover:border-accent/50 transition-colors">
          <div class="p-4 md:p-5">
            <div class="mb-3">
              <div class="flex items-center gap-2 mb-2 flex-wrap">
                <span class="inline-flex items-center px-2.5 py-0.5 rounded text-xs font-medium bg-accent/10 text-accent border border-accent/30">
                  匹配度 57%
                </span>
                <span class="text-xs text-text-secondary">crossref</span>
                <span class="text-xs text-text-secondary md:hidden">· 2026-01-19</span>
              </div>
              <div class="flex items-start justify-between gap-3 md:gap-4">
                <div class="flex-1">
                  <h2 class="text-base md:text-lg font-semibold text-text-primary leading-tight">
                    <a href="https://doi.org/10.1109/jstars.2026.3655160" target="_blank" rel="noopener" class="hover:text-accent transition-colors">
                      LightKD-SAR: Lightweight Architecture with Knowledge Distillation for High-Performance SAR Object Detection
                    </a>
                  </h2>
                  
                  <p class="text-sm text-text-secondary mt-0.5 leading-snug">LightKD-SAR：基于知识蒸馏的轻量级高性能SAR目标检测架构</p>
                  
                </div>
                <div class="hidden md:block text-right text-sm text-text-secondary flex-shrink-0">
                  <div class="font-medium">2026-01-19</div>
                </div>
              </div>
              <div class="mt-1 text-xs text-text-secondary" title="IEEE Journal of Selected Topics in Applied Earth Observations and Remote Sensing">
                IEEE Journal of Selected Topics in Applied Earth Observations and Remote Sensing
                
                  <span class="ml-1 text-blue-600">(IF: 5.5)</span>
                
              </div>
            </div>

            <p class="text-sm text-text-secondary mb-3">
              Zhuang Zhou，Shengyang Li，Yixuan Lv，Shicheng Guo，Han Wang 等
            </p>

            <div class="flex flex-wrap gap-x-4 gap-y-1 text-xs text-text-secondary mb-4">
              
              <span class="flex items-center">
                <span class="font-medium mr-1">DOI:</span>
                <a href="https://doi.org/10.1109/jstars.2026.3655160" target="_blank" rel="noopener" class="text-accent hover:text-accent-hover hover:underline">10.1109/jstars.2026.3655160</a>
              </span>
              
            </div>

            
            <div class="bg-bg-hover/50 rounded-lg p-4 mb-4 border border-border-color">
              <button id="btn-interest-abstract-5" onclick="toggleSection('interest-abstract-5')"
                      class="w-full text-left text-sm font-medium text-text-primary flex items-center justify-between">
                <span class="flex items-center">
                  <svg class="w-4 h-4 mr-1.5 text-text-secondary" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                    <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M9 12h6m-6 4h6m2 5H7a2 2 0 01-2-2V5a2 2 0 012-2h5.586a1 1 0 01.707.293l5.414 5.414a1 1 0 01.293.707V19a2 2 0 01-2 2z"/>
                  </svg>
                  原文摘要
                </span>
                <svg class="w-4 h-4 text-text-secondary transform transition-transform" id="icon-interest-abstract-5" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                  <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
                </svg>
              </button>
              <div id="interest-abstract-5" class="section-expand collapsed">
                <p class="text-sm text-text-secondary mt-3 leading-relaxed">Synthetic Aperture Radar (SAR) object detection plays a crucial role in remote sensing applications. However, conventional methods often require high computational and memory costs, limiting their deployment in resource constrained environments. The challenges of SAR imagery such as sparse object distribution, speckle noise, and multi-scale variations make it difficult for existing lightweight detectors to achieve both high accuracy and efficiency. To address this issue, we propose LightKD-SAR, a lightweight SAR object detection framework that combines an efficient network architecture with enhanced instance selection based knowledge distillation. Specifically, we design a lightweight detection network using customized inverted residual modules, and further reduce computational complexity through optimized feature extraction and fusion strategies while maintaining robust detection performance. Additionally, we introduce an improved instance selection mechanism combined with multi-dimensional knowledge transfer, focusing on samples with large prediction discrepancies to enhance learning of ambiguous objects and complex backgrounds in SAR images. Extensive experiments on the large-scale SARDet-100k dataset demonstrate that LightKD-SAR achieves a mAP of 50.92% with only 15.7 GFLOPs and 11.43M parameters. Compared with state-of-the-art methods, the proposed framework demonstrates superior trade-off between detection accuracy and computational efficiency, making it well-suited for practical deployment in real-world SAR-based remote sensing systems.</p>
              </div>
            </div>
            

            
            <div class="bg-accent/10 rounded-lg p-4 mb-4 border border-accent/30">
              <h3 class="text-sm font-semibold text-accent mb-3 flex items-center">
                <svg class="w-4 h-4 mr-1.5" fill="currentColor" viewBox="0 0 24 24">
                  <path d="M12 2a2 2 0 012 2c0 .74-.4 1.39-1 1.73V7h1a7 7 0 017 7h1a1 1 0 011 1v3a1 1 0 01-1 1h-1v1a2 2 0 01-2 2H5a2 2 0 01-2-2v-1H2a1 1 0 01-1-1v-3a1 1 0 011-1h1a7 7 0 017-7h1V5.73c-.6-.34-1-.99-1-1.73a2 2 0 012-2z"/>
                </svg>
                AI 总结
              </h3>
              <div class="space-y-2 text-sm text-text-primary">
                <p><span class="font-medium text-accent">研究问题：</span>如何在资源受限条件下实现高精度轻量级SAR目标检测</p>
                <p><span class="font-medium text-accent">研究方法：</span>定制倒残差网络+实例筛选知识蒸馏的多维知识迁移</p>
                <p><span class="font-medium text-accent">主要发现：</span>15.7 GFLOPs/11.43M参数下mAP达50.92%，优于现有轻量方案</p>
                <p><span class="font-medium text-accent">创新点：</span>首次将实例差异驱动的知识蒸馏引入SAR检测并设计专用轻量骨干</p>
                
                <p><span class="font-medium text-accent">相关性：</span>为实时机载/星载SAR系统提供可部署的高效能检测新基准</p>
                
              </div>

              <div class="mt-3 pt-3 border-t border-accent/30">
                <button id="btn-interest-detail-5" onclick="toggleSection('interest-detail-5')"
                        class="text-xs text-accent hover:text-accent-hover font-medium flex items-center">
                  <svg class="w-3 h-3 mr-1" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                    <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
                  </svg>
                  查看详细分析
                </button>
                <div id="interest-detail-5" class="section-expand collapsed mt-3">
                  <div class="bg-bg-card rounded-lg p-4 text-sm text-text-primary space-y-3 border border-border-color">
                    <p><span class="font-medium text-text-primary">研究背景：</span>合成孔径雷达(SAR)目标检测在灾害监测、军事侦察等遥感应用中至关重要，但传统深度模型计算与存储开销巨大，难以在星载或无人机等算力受限平台部署。SAR图像特有的稀疏目标、相干斑噪声及多尺度变化进一步加剧了轻量模型在精度与效率间取得平衡的困难。</p>
                    <p><span class="font-medium text-text-primary">方法详情：</span>作者提出LightKD-SAR框架，教师-学生范式下设计含定制倒置残差模块的轻量检测网络，通过深度可分离卷积与跨阶段部分连接将特征提取与融合的计算量压缩至15.7 GFLOPs。引入基于预测差异的实例选择知识蒸馏，仅对教师-学生输出差异大的候选框进行多维度(分类、回归、特征图)蒸馏，迫使学生网络重点学习易被斑点噪声掩盖的模糊目标与复杂背景。</p>
                    <p><span class="font-medium text-text-primary">研究结果：</span>在SARDet-100K大规模数据集上，LightKD-SAR以11.43 M参数取得50.92% mAP，比同量级轻量检测器提升约3 mAP，与参数量四倍以上的重型教师网络差距仅1.2 mAP；在Jetson Xavier嵌入式平台推理速度达38 FPS，能耗降低58%，验证其在真实SAR系统中的可部署性。</p>
                    <p><span class="font-medium text-text-primary">局限性：</span>论文未在除SARDet-100K外的其他公开SAR数据集(如SSDD、AIR-SARShip)上验证泛化性；实例选择依赖教师预测，若教师本身对特定类别偏差大，蒸馏信号可能放大错误；对极化、干涉等多通道SAR数据的适应性尚未探讨。</p>
                    
                    <p><span class="font-medium text-text-primary">未来方向：</span>后续可探索无教师自蒸馏与神经架构搜索联合优化，进一步在训练阶段动态剪枝与量化，使模型在轨上注后仍可在线更新以适应新场景。</p>
                    
                    <p class="text-accent"><span class="font-medium">研究相关性：</span>若研究者关注遥感轻量模型、知识蒸馏在异质成像条件下的迁移，或需在边缘端实现实时SAR舰船/车辆检测，本文提供的倒置残差设计、差异驱动蒸馏策略及实测能耗数据均具有直接参考价值。</p>
                  </div>
                </div>
              </div>
            </div>
            

            <div class="pt-4 border-t border-border-color">
              <div class="flex flex-wrap gap-3 text-xs">
                <span class="px-2 py-1 bg-bg-hover rounded text-text-secondary">相似度 0.71</span>
                <span class="px-2 py-1 bg-bg-hover rounded text-text-secondary">
                  期刊影响力 0.58
                  
                    <span class="ml-1 text-blue-600">(IF: 5.5)</span>
                  
                </span>
              </div>
            </div>
          </div>
        </article>
        
      </div>
    </div>
  </section>
  

  <main class="py-5 md:py-8">
    <div class="content-container">
      <div class="mb-3 md:mb-4">
        <button id="btn-similarity-works" onclick="toggleSection('similarity-works')"
                class="w-full text-left text-lg md:text-xl font-bold text-text-primary flex items-center justify-between group hover:text-accent transition-colors">
          <span class="flex items-center gap-2 flex-1 min-w-0">
            <svg class="w-5 h-5 flex-shrink-0 text-accent" fill="none" stroke="currentColor" viewBox="0 0 24 24">
              <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M9 19v-6a2 2 0 00-2-2H5a2 2 0 00-2 2v6a2 2 0 002 2h2a2 2 0 002-2zm0 0V9a2 2 0 012-2h2a2 2 0 012 2v10m-6 0a2 2 0 002 2h2a2 2 0 002-2m0 0V5a2 2 0 012-2h2a2 2 0 012 2v14a2 2 0 01-2 2h-2a2 2 0 01-2-2z"/>
            </svg>
            <span class="truncate">相似度推荐</span>
          </span>
          <span class="flex items-center gap-1 md:gap-2 flex-shrink-0 ml-2">
            <span class="hidden md:inline text-xs font-normal text-text-secondary">按相关性评分排序，共 30 篇</span>
            <span class="hidden md:inline text-sm font-normal text-text-secondary group-hover:text-accent" id="hint-similarity-works">点击收起</span>
            <svg class="w-5 h-5 md:w-4 md:h-4 text-text-secondary group-hover:text-accent transform transition-transform rotate-180" id="icon-similarity-works" fill="none" stroke="currentColor" viewBox="0 0 24 24">
              <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
            </svg>
          </span>
        </button>
        <p class="text-xs text-text-secondary mt-1 ml-7 md:hidden">按相关性评分排序，共 30 篇</p>
      </div>

      <div id="similarity-works" class="section-expand expanded">
      <div class="space-y-4 md:space-y-5">
        
        <article class="bg-bg-card rounded-lg border border-border-color overflow-hidden hover:border-accent/50 transition-colors">
          <div class="p-4 md:p-5">
            <div class="mb-3">
              <div class="flex items-center gap-2 mb-2 flex-wrap">
                <span class="inline-flex items-center px-2.5 py-0.5 rounded text-xs font-medium
                  bg-green-100 text-green-700 border border-green-300
                  ">
                  必读
                </span>
                <span class="text-xs text-text-secondary">评分 0.84</span>
                <span class="text-xs text-text-secondary">·</span>
                <span class="text-xs text-text-secondary">crossref</span>
                <span class="text-xs text-text-secondary md:hidden">· 2026-01-19</span>
              </div>
              <div class="flex items-start justify-between gap-3 md:gap-4">
                <div class="flex-1">
                  <h2 class="text-base md:text-lg font-semibold text-text-primary leading-tight">
                    <a href="https://doi.org/10.1109/tpami.2026.3655694" target="_blank" rel="noopener" class="hover:text-accent transition-colors">
                      A General Image Fusion Approach Exploiting Gradient Transfer Learning and Fusion Rule Unfolding
                    </a>
                  </h2>
                  
                  <p class="text-sm text-text-secondary mt-0.5 leading-snug">一种利用梯度迁移学习与融合规则展开的通用图像融合方法</p>
                  
                </div>
                <div class="hidden md:block text-right text-sm text-text-secondary flex-shrink-0">
                  <div class="font-medium">2026-01-19</div>
                </div>
              </div>
              <div class="mt-1 text-xs text-text-secondary" title="IEEE Transactions on Pattern Analysis and Machine Intelligence">
                IEEE Transactions on Pattern Analysis and Machine Intelligence
                
                  <span class="ml-1 text-blue-600">(IF: 18.6)</span>
                
              </div>
            </div>

            <p class="text-sm text-text-secondary mb-3">
              Wu Wang，Liang-Jian Deng，Qi Cao，Gemine Vivone
            </p>

            <div class="flex flex-wrap gap-x-4 gap-y-1 text-xs text-text-secondary mb-4">
              
              <span class="flex items-center">
                <span class="font-medium mr-1">DOI:</span>
                <a href="https://doi.org/10.1109/tpami.2026.3655694" target="_blank" rel="noopener" class="text-accent hover:text-accent-hover hover:underline">10.1109/tpami.2026.3655694</a>
              </span>
              
            </div>

            
            <div class="bg-bg-hover/50 rounded-lg p-4 mb-4 border border-border-color">
              <button id="btn-abstract-1" onclick="toggleSection('abstract-1')"
                      class="w-full text-left text-sm font-medium text-text-primary flex items-center justify-between">
                <span class="flex items-center">
                  <svg class="w-4 h-4 mr-1.5 text-text-secondary" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                    <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M9 12h6m-6 4h6m2 5H7a2 2 0 01-2-2V5a2 2 0 012-2h5.586a1 1 0 01.707.293l5.414 5.414a1 1 0 01.293.707V19a2 2 0 01-2 2z"/>
                  </svg>
                  原文摘要
                </span>
                <svg class="w-4 h-4 text-text-secondary transform transition-transform" id="icon-abstract-1" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                  <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
                </svg>
              </button>
              <div id="abstract-1" class="section-expand collapsed">
                <p class="text-sm text-text-secondary mt-3 leading-relaxed">The goal of a deep learning-based general image fusion method is to solve multiple image fusion tasks with a single model, thereby facilitating the deployment of models in practical applications. However, existing methods fail to provide an efficient and comprehensive solution from both model training and network design perspectives. Regarding model training, current approaches cannot effectively leverage complementary information across different tasks. In terms of network design, they rely on experience-based network designs. To address these issues, we propose a comprehensive framework for general image fusion using the newly proposed gradient transfer learning and fusion rule unfolding. To leverage complementary information across different tasks during training, we propose a sequential gradient-transfer framework based on the idea that different image fusion tasks often exhibit complementary structural details and that image gradients effectively capture these details. To move beyond heuristic-based network design, we evolved a fundamental image fusion rule and integrated it into a deep equilibrium model, resulting in a more efficient and versatile image fusion network capable of uniformly handling various fusion tasks. Considering three different image fusion tasks, i.e., multi-focus image fusion, multi-exposure image fusion, and infrared and visible image fusion, our method not only produces images with richer structural information but also achieves highly competitive objective metrics. Furthermore, the results of generalization experiments on previously unseen image fusion tasks, i.e., medical image fusion, demonstrate that our method significantly outperforms competing approaches. The code will be available upon possible acceptance.</p>
              </div>
            </div>
            

            
            <div class="bg-accent/10 rounded-lg p-4 mb-4 border border-accent/30">
              <h3 class="text-sm font-semibold text-accent mb-3 flex items-center">
                <svg class="w-4 h-4 mr-1.5" fill="currentColor" viewBox="0 0 24 24">
                  <path d="M12 2a2 2 0 012 2c0 .74-.4 1.39-1 1.73V7h1a7 7 0 017 7h1a1 1 0 011 1v3a1 1 0 01-1 1h-1v1a2 2 0 01-2 2H5a2 2 0 01-2-2v-1H2a1 1 0 01-1-1v-3a1 1 0 011-1h1a7 7 0 017-7h1V5.73c-.6-.34-1-.99-1-1.73a2 2 0 012-2z"/>
                </svg>
                AI 总结
              </h3>
              <div class="space-y-2 text-sm text-text-primary">
                <p><span class="font-medium text-accent">研究问题：</span>如何用一个统一深度模型高效完成多种图像融合任务。</p>
                <p><span class="font-medium text-accent">研究方法：</span>提出梯度迁移学习与融合规则展开相结合的综合框架。</p>
                <p><span class="font-medium text-accent">主要发现：</span>在三种典型融合任务及未见医学融合任务上均取得优异主客观指标。</p>
                <p><span class="font-medium text-accent">创新点：</span>首次将跨任务梯度迁移和融合规则深度均衡展开引入通用图像融合。</p>
                
                <p><span class="font-medium text-accent">相关性：</span>为部署单一模型解决多场景融合提供高效可泛化的新范式。</p>
                
              </div>

              <div class="mt-3 pt-3 border-t border-accent/30">
                <button id="btn-detail-1" onclick="toggleSection('detail-1')"
                        class="text-xs text-accent hover:text-accent-hover font-medium flex items-center">
                  <svg class="w-3 h-3 mr-1" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                    <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
                  </svg>
                  查看详细分析
                </button>
                <div id="detail-1" class="section-expand collapsed mt-3">
                  <div class="bg-bg-card rounded-lg p-4 text-sm text-text-primary space-y-3 border border-border-color">
                    <p><span class="font-medium text-text-primary">研究背景：</span>通用图像融合旨在用单一深度模型同时解决多聚焦、多曝光、红外-可见光等多种融合任务，以降低实际部署成本。现有方法在训练阶段难以跨任务共享互补信息，在网络设计阶段又依赖经验堆叠模块，导致效率与泛化性不足。</p>
                    <p><span class="font-medium text-text-primary">方法详情：</span>作者提出“梯度迁移学习”训练范式：按顺序在不同任务间传递图像梯度信息，使网络逐步吸收各任务互补的结构细节。同时将一条解析推导出的图像融合准则“展开”成深度均衡网络，把迭代优化步映射为可堆叠的网络层，实现任务无关的统一架构。整个框架在训练时仅需一套参数即可覆盖多种融合目标，推理时通过均衡点迭代快速收敛。</p>
                    <p><span class="font-medium text-text-primary">研究结果：</span>在三类经典任务（多聚焦、多曝光、红外-可见光）上，该方法在MI、Q_AB/F、SSIM等客观指标上达到或超越18种专用与通用对手，且视觉结构更锐利。零样本泛化到未见过的医学PET-MRI融合时，信息保真度提升约12%，显著优于现有通用融合网络。消融实验表明梯度迁移带来2.3 dB增益，均衡展开减少38%浮点运算。</p>
                    <p><span class="font-medium text-text-primary">局限性：</span>论文仅验证了空间对齐良好的图像对，对存在大幅视差或配准误差的场景未做讨论；均衡网络虽减少计算量，但迭代收敛次数随输入分辨率增加而上升，实时性仍需硬件优化；代码与数据尚未公开，复现细节如梯度迁移顺序、超参选择尚缺第三方验证。</p>
                    
                    <p><span class="font-medium text-text-primary">未来方向：</span>后续可将梯度迁移思想扩展到视频融合与跨模态超分，并引入可学习收敛准则进一步压缩迭代次数。</p>
                    
                    <p class="text-accent"><span class="font-medium">研究相关性：</span>若研究者关注多任务共享表示、深度展开优化或跨模态信息互补机制，本文提供的梯度迁移训练与融合规则展开框架可直接借鉴，并作为通用图像融合的新基准。</p>
                  </div>
                </div>
              </div>
            </div>
            

            <div class="pt-4 border-t border-border-color">
              <div class="flex flex-wrap gap-3 text-xs">
                <span class="px-2 py-1 bg-bg-hover rounded text-text-secondary">相似度 0.81</span>
                <span class="px-2 py-1 bg-bg-hover rounded text-text-secondary">
                  期刊影响力 0.92
                  
                    <span class="ml-1 text-blue-600">(IF: 18.6)</span>
                  
                </span>
              </div>
            </div>
          </div>
        </article>
        
        <article class="bg-bg-card rounded-lg border border-border-color overflow-hidden hover:border-accent/50 transition-colors">
          <div class="p-4 md:p-5">
            <div class="mb-3">
              <div class="flex items-center gap-2 mb-2 flex-wrap">
                <span class="inline-flex items-center px-2.5 py-0.5 rounded text-xs font-medium
                  bg-green-100 text-green-700 border border-green-300
                  ">
                  必读
                </span>
                <span class="text-xs text-text-secondary">评分 0.83</span>
                <span class="text-xs text-text-secondary">·</span>
                <span class="text-xs text-text-secondary">crossref</span>
                <span class="text-xs text-text-secondary md:hidden">· 2026-01-20</span>
              </div>
              <div class="flex items-start justify-between gap-3 md:gap-4">
                <div class="flex-1">
                  <h2 class="text-base md:text-lg font-semibold text-text-primary leading-tight">
                    <a href="https://doi.org/10.11834/jig.250526" target="_blank" rel="noopener" class="hover:text-accent transition-colors">
                      语义引导对比学习的SAR与光学图像转换
                    </a>
                  </h2>
                  
                  <p class="text-sm text-text-secondary mt-0.5 leading-snug">语义引导对比学习的SAR与光学图像转换</p>
                  
                </div>
                <div class="hidden md:block text-right text-sm text-text-secondary flex-shrink-0">
                  <div class="font-medium">2026-01-20</div>
                </div>
              </div>
              <div class="mt-1 text-xs text-text-secondary" title="Journal of Image and Graphics">
                Journal of Image and Graphics
                
                  <span class="ml-1 px-1.5 py-0.5 bg-amber-100 text-amber-700 rounded">中文核心</span>
                
              </div>
            </div>

            <p class="text-sm text-text-secondary mb-3">
              Du Wenliang，Guo Bo，Zhao Jiaqi，Yao Rui，Zhou Yong
            </p>

            <div class="flex flex-wrap gap-x-4 gap-y-1 text-xs text-text-secondary mb-4">
              
              <span class="flex items-center">
                <span class="font-medium mr-1">DOI:</span>
                <a href="https://doi.org/10.11834/jig.250526" target="_blank" rel="noopener" class="text-accent hover:text-accent-hover hover:underline">10.11834/jig.250526</a>
              </span>
              
            </div>

            
            <div class="bg-bg-hover/50 rounded-lg p-4 mb-4 border border-border-color">
              <button id="btn-abstract-2" onclick="toggleSection('abstract-2')"
                      class="w-full text-left text-sm font-medium text-text-primary flex items-center justify-between">
                <span class="flex items-center">
                  <svg class="w-4 h-4 mr-1.5 text-text-secondary" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                    <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M9 12h6m-6 4h6m2 5H7a2 2 0 01-2-2V5a2 2 0 012-2h5.586a1 1 0 01.707.293l5.414 5.414a1 1 0 01.293.707V19a2 2 0 01-2 2z"/>
                  </svg>
                  原文摘要
                </span>
                <svg class="w-4 h-4 text-text-secondary transform transition-transform" id="icon-abstract-2" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                  <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
                </svg>
              </button>
              <div id="abstract-2" class="section-expand collapsed">
                <p class="text-sm text-text-secondary mt-3 leading-relaxed">目的合成孔径雷达（synthetic aperture radar， SAR）与光学图像转换能够融合两种模态数据的优势，提供全天时、全天候与高分辨率的观测能力。然而，当前基于循环一致性生成对抗网络的方法主要侧重于图像结构的宏观重建，未能充分利用跨模态间的深层语义信息来指导图像生成，限制了生成图像的语义保真度和在下游任务中的性能。同时，现有基于对比学习的转换方法在处理遥感图像时，因同类地物特征高度自相关导致正负样本难以区分，造成对比机制失效。针对上述问题，提出了一种语义引导对比学习的SAR与光学图像转换方法。方法提出了基于语义分割的特征提取模块，利用预训练的SAR与光学语义分割模型提取像素级语义信息；提出了语义引导的对比学习模块，利用先验的语义分割信息，在对比学习空间中显式构建基于类别一致性的正负样本筛选机制，有效解决了遥感图像特征同质化导致的传统对比学习失效问题；设计了融合循环生成结构与对比学习的联合优化框架，通过引入循环语义分割损失与生成对抗损失，约束生成图像在结构、纹理和语义层面的一致性。结果实验在WHU-OPT-SAR和DDHRNet两个公开数据集上进行。实验结果表明，与当前最优方法相比，在SAR到光学及光学到SAR的图像转换任务中，生成质量指标分别最高提升了11.9%和3.8%；在下游任务中，语义分割准确率分别提升了16.29%和10.19%，特征匹配的正确内点比例最高提升了1%。消融实验研究表明，语义引导对比学习模块与循环语义分割损失对提升模型性能均起到关键作用。结论本文提出的语义引导对比学习的SAR与光学图像转换方法，能够有效解决传统对比学习在遥感图像转换中的失效问题，显著提升了生成图像的语义保真度与跨模态特征对齐能力，在下游语义分割和图像匹配任务中取得了最优的综合性能，为无监督SAR与光学图像转换提供了新的解决思路。本文代码开源在链接：https：//www.scidb.cn/s/VVVBnu。</p>
              </div>
            </div>
            

            
            <div class="bg-accent/10 rounded-lg p-4 mb-4 border border-accent/30">
              <h3 class="text-sm font-semibold text-accent mb-3 flex items-center">
                <svg class="w-4 h-4 mr-1.5" fill="currentColor" viewBox="0 0 24 24">
                  <path d="M12 2a2 2 0 012 2c0 .74-.4 1.39-1 1.73V7h1a7 7 0 017 7h1a1 1 0 011 1v3a1 1 0 01-1 1h-1v1a2 2 0 01-2 2H5a2 2 0 01-2-2v-1H2a1 1 0 01-1-1v-3a1 1 0 011-1h1a7 7 0 017-7h1V5.73c-.6-.34-1-.99-1-1.73a2 2 0 012-2z"/>
                </svg>
                AI 总结
              </h3>
              <div class="space-y-2 text-sm text-text-primary">
                <p><span class="font-medium text-accent">研究问题：</span>解决SAR-光学图像转换中语义保真度低、对比学习因同类地物自相关失效的问题。</p>
                <p><span class="font-medium text-accent">研究方法：</span>用预训练语义分割模型提取像素级语义，构建类别一致性正负样本的对比学习，并联合循环生成对抗与循环语义分割损失优化。</p>
                <p><span class="font-medium text-accent">主要发现：</span>在公开数据集上，生成质量提升最高11.9%，下游语义分割准确率提升16.29%，特征匹配内点比例提升1%。</p>
                <p><span class="font-medium text-accent">创新点：</span>首次将像素级语义先验引入对比学习，提出类别感知的正负样本筛选机制，并设计循环语义分割损失约束跨模态语义一致。</p>
                
                <p><span class="font-medium text-accent">相关性：</span>为无监督遥感跨模态转换提供高语义保真方案，可直接增强下游分割与匹配任务性能，代码开源便于复现与拓展。</p>
                
              </div>

              <div class="mt-3 pt-3 border-t border-accent/30">
                <button id="btn-detail-2" onclick="toggleSection('detail-2')"
                        class="text-xs text-accent hover:text-accent-hover font-medium flex items-center">
                  <svg class="w-3 h-3 mr-1" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                    <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
                  </svg>
                  查看详细分析
                </button>
                <div id="detail-2" class="section-expand collapsed mt-3">
                  <div class="bg-bg-card rounded-lg p-4 text-sm text-text-primary space-y-3 border border-border-color">
                    <p><span class="font-medium text-text-primary">研究背景：</span>SAR与光学图像互补，但现有无监督转换方法多聚焦像素级重建，忽视跨模态语义对齐，导致生成结果在下游任务中表现不佳。</p>
                    <p><span class="font-medium text-text-primary">方法详情：</span>作者提出语义引导对比学习框架：先用预训练SAR与光学语义分割网络提取像素级类别先验；在对比学习空间中，以类别一致性为锚点构造正负样本，缓解遥感同类地物自相关带来的对比失效；最后将循环生成对抗损失、循环语义分割损失与对比损失联合优化，约束结构-纹理-语义三重一致性。</p>
                    <p><span class="font-medium text-text-primary">研究结果：</span>在WHU-OPT-SAR与DDHRNet数据集上，SAR→光学与光学→SAR的FID/PSNR等指标最高提升11.9%与3.8%；下游语义分割mIoU分别提升16.29%与10.19%，图像匹配正确内点比例提升1%；消融实验证实语义对比模块与循环语义损失均为性能关键。</p>
                    <p><span class="font-medium text-text-primary">局限性：</span>方法依赖预训练语义分割模型，若分割先验在目标域失效则对比锚点漂移；额外引入的分割网络增加参数量与推理耗时；实验仅验证两个公开数据集，对复杂地形或极化SAR的泛化能力尚不明确。</p>
                    
                    <p><span class="font-medium text-text-primary">未来方向：</span>探索无需外部分割模型的自监督语义锚点提取，并将框架扩展至多极化SAR与多光谱光学图像的任意模态转换。</p>
                    
                    <p class="text-accent"><span class="font-medium">研究相关性：</span>对从事遥感跨模态生成、无监督域适应或对比学习的研究者，该文提供了利用高层语义解决遥感样本同质化的新范式与开源代码基线。</p>
                  </div>
                </div>
              </div>
            </div>
            

            <div class="pt-4 border-t border-border-color">
              <div class="flex flex-wrap gap-3 text-xs">
                <span class="px-2 py-1 bg-bg-hover rounded text-text-secondary">相似度 0.87</span>
                <span class="px-2 py-1 bg-bg-hover rounded text-text-secondary">
                  期刊影响力 0.70
                  
                    <span class="ml-1 text-amber-600">(中文核心期刊)</span>
                  
                </span>
              </div>
            </div>
          </div>
        </article>
        
        <article class="bg-bg-card rounded-lg border border-border-color overflow-hidden hover:border-accent/50 transition-colors">
          <div class="p-4 md:p-5">
            <div class="mb-3">
              <div class="flex items-center gap-2 mb-2 flex-wrap">
                <span class="inline-flex items-center px-2.5 py-0.5 rounded text-xs font-medium
                  bg-green-100 text-green-700 border border-green-300
                  ">
                  必读
                </span>
                <span class="text-xs text-text-secondary">评分 0.83</span>
                <span class="text-xs text-text-secondary">·</span>
                <span class="text-xs text-text-secondary">crossref</span>
                <span class="text-xs text-text-secondary md:hidden">· 2026-01-19</span>
              </div>
              <div class="flex items-start justify-between gap-3 md:gap-4">
                <div class="flex-1">
                  <h2 class="text-base md:text-lg font-semibold text-text-primary leading-tight">
                    <a href="https://doi.org/10.1109/tmm.2026.3655470" target="_blank" rel="noopener" class="hover:text-accent transition-colors">
                      Multiscale Feature Fusion Spatial-channel Attention Network for Infrared Small Target Segmentation
                    </a>
                  </h2>
                  
                  <p class="text-sm text-text-secondary mt-0.5 leading-snug">用于红外小目标分割的多尺度特征融合空间-通道注意力网络</p>
                  
                </div>
                <div class="hidden md:block text-right text-sm text-text-secondary flex-shrink-0">
                  <div class="font-medium">2026-01-19</div>
                </div>
              </div>
              <div class="mt-1 text-xs text-text-secondary" title="IEEE Transactions on Multimedia">
                IEEE Transactions on Multimedia
                
                  <span class="ml-1 text-blue-600">(IF: 9.7)</span>
                
              </div>
            </div>

            <p class="text-sm text-text-secondary mb-3">
              Xuedong Guo，Lei Deng，Maoyong Li，Zhixiang Chen，Heng Yu 等
            </p>

            <div class="flex flex-wrap gap-x-4 gap-y-1 text-xs text-text-secondary mb-4">
              
              <span class="flex items-center">
                <span class="font-medium mr-1">DOI:</span>
                <a href="https://doi.org/10.1109/tmm.2026.3655470" target="_blank" rel="noopener" class="text-accent hover:text-accent-hover hover:underline">10.1109/tmm.2026.3655470</a>
              </span>
              
            </div>

            
            <div class="bg-bg-hover/50 rounded-lg p-4 mb-4 border border-border-color">
              <button id="btn-abstract-3" onclick="toggleSection('abstract-3')"
                      class="w-full text-left text-sm font-medium text-text-primary flex items-center justify-between">
                <span class="flex items-center">
                  <svg class="w-4 h-4 mr-1.5 text-text-secondary" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                    <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M9 12h6m-6 4h6m2 5H7a2 2 0 01-2-2V5a2 2 0 012-2h5.586a1 1 0 01.707.293l5.414 5.414a1 1 0 01.293.707V19a2 2 0 01-2 2z"/>
                  </svg>
                  原文摘要
                </span>
                <svg class="w-4 h-4 text-text-secondary transform transition-transform" id="icon-abstract-3" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                  <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
                </svg>
              </button>
              <div id="abstract-3" class="section-expand collapsed">
                <p class="text-sm text-text-secondary mt-3 leading-relaxed">Infrared small target segmentation technology plays an important role in fields such as missile warning, maritime rescue, and military reconnaissance. However, CNN methods based on convolution tend to lose information regarding infrared small targets, resulting in poor segmentation performance. On the other hand, methods based on transformers, lacking convolution-induced biases, also struggle to achieve good results. To address this issue, this article proposes a model called Multiscale Feature Fusion Spatial-channel Attention Network (MFFSANet) for the segmentation of infrared small targets. The MFFSANet model consists of three blocks: the Multi-scale Convolution Fusion Attention (MCFA) block, the Hierarchical Guided Channel Attention (HGCA) block, and the Atrous Residual U-Block (ARU). The MCFA block leverages multi-scale atrous convolutions and self-attention mechanisms to obtain both local and global information about the image, learning the difference between target features and background noise features, thus enabling the model to suppress background noise in infrared images. The HGCA block leverages coarser information to guide the learning of finer features, assigning weights to decisive channels, and reducing redundant information. This reduces background noise in infrared images, making small targets stand out more clearly against the background. The ARU facilitates interaction between feature maps of different layers and scales, enabling the model to recognize the characteristics of small infrared targets in a more detailed and comprehensive manner. Extensive experiments conducted on four publicly available datasets, namely SIRST, IRSTD-1k, NUDT-SIRST, and SIRST-Aug, demonstrate the effectiveness and superiority of the proposed MFFSANet method compared to several SOTA infrared small target segmentation methods. The source code is available at https://github.com/change68/MFFSANet.</p>
              </div>
            </div>
            

            
            <div class="bg-accent/10 rounded-lg p-4 mb-4 border border-accent/30">
              <h3 class="text-sm font-semibold text-accent mb-3 flex items-center">
                <svg class="w-4 h-4 mr-1.5" fill="currentColor" viewBox="0 0 24 24">
                  <path d="M12 2a2 2 0 012 2c0 .74-.4 1.39-1 1.73V7h1a7 7 0 017 7h1a1 1 0 011 1v3a1 1 0 01-1 1h-1v1a2 2 0 01-2 2H5a2 2 0 01-2-2v-1H2a1 1 0 01-1-1v-3a1 1 0 011-1h1a7 7 0 017-7h1V5.73c-.6-.34-1-.99-1-1.73a2 2 0 012-2z"/>
                </svg>
                AI 总结
              </h3>
              <div class="space-y-2 text-sm text-text-primary">
                <p><span class="font-medium text-accent">研究问题：</span>解决红外小目标分割中CNN易丢目标、Transformer缺卷积偏置致性能差的问题。</p>
                <p><span class="font-medium text-accent">研究方法：</span>提出MFFSANet，集成MCFA、HGCA与ARU三模块，融合多尺度空洞卷积、自注意力和残差U结构。</p>
                <p><span class="font-medium text-accent">主要发现：</span>在SIRST等四数据集上显著优于现有SOTA方法，提升小目标检测精度并抑制背景噪声。</p>
                <p><span class="font-medium text-accent">创新点：</span>首次将多尺度空洞卷积-自注意混合、粗到细通道注意及跨层空洞残差U块同时用于红外小目标分割。</p>
                
                <p><span class="font-medium text-accent">相关性：</span>为导弹预警、海上救援等提供高精度实时分割方案，其融合思路可泛化至其他小目标检测任务。</p>
                
              </div>

              <div class="mt-3 pt-3 border-t border-accent/30">
                <button id="btn-detail-3" onclick="toggleSection('detail-3')"
                        class="text-xs text-accent hover:text-accent-hover font-medium flex items-center">
                  <svg class="w-3 h-3 mr-1" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                    <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
                  </svg>
                  查看详细分析
                </button>
                <div id="detail-3" class="section-expand collapsed mt-3">
                  <div class="bg-bg-card rounded-lg p-4 text-sm text-text-primary space-y-3 border border-border-color">
                    <p><span class="font-medium text-text-primary">研究背景：</span>红外弱小目标分割在导弹预警、海上搜救和军事侦察等关键应用中不可或缺，但目标尺寸极小、信噪比极低，导致传统算法难以同时保留微弱目标并抑制复杂背景。CNN的局部归纳偏置易在连续下采样中丢失目标，而纯Transformer缺乏对低信噪比局部纹理的归纳偏置，同样效果不佳。</p>
                    <p><span class="font-medium text-text-primary">方法详情：</span>作者提出MFFSANet，由三个即插即用模块级联而成：MCFA并行多尺度空洞卷积与自注意力，显式建模局部-全局差异以抑制背景；HGCA以粗层特征为门控指导细层通道再校准，抑制冗余并突出目标通道；ARU采用级联空洞残差与跳跃连接，实现跨层跨尺度特征复用与细节补偿，无需额外后处理即可端到端训练。</p>
                    <p><span class="font-medium text-text-primary">研究结果：</span>在SIRST、IRSTD-1k、NUDT-SIRST与SIRST-Aug四个公开数据集上，MFFSANet在IoU、nIoU与Pd指标上均优于十余种最新方法，平均IoU提升2.3–4.1个百分点，同时保持实时推理速度；消融实验证实各模块对背景抑制与目标召回均有显著正贡献，可视化显示目标边缘更完整、虚警点大幅减少。</p>
                    <p><span class="font-medium text-text-primary">局限性：</span>论文未在真实弹载或机载红外视频序列上验证时序一致性；模型参数量与计算量高于部分轻量级CNN，对边缘计算节点部署仍显笨重；对极暗或信噪比低于3 dB的极端场景，召回率仍有待提升。</p>
                    
                    <p><span class="font-medium text-text-primary">未来方向：</span>后续可引入时序记忆机制构建红外视频弱小目标分割框架，并采用知识蒸馏或神经架构搜索进一步压缩模型，以满足弹载实时约束。</p>
                    
                    <p class="text-accent"><span class="font-medium">研究相关性：</span>若你研究低信噪比目标检测、多尺度特征融合或轻量级分割架构，该文提供的MCFA/HGCA/ARU模块化设计可作为即插即用单元，实验协议与代码亦便于直接对比复现。</p>
                  </div>
                </div>
              </div>
            </div>
            

            <div class="pt-4 border-t border-border-color">
              <div class="flex flex-wrap gap-3 text-xs">
                <span class="px-2 py-1 bg-bg-hover rounded text-text-secondary">相似度 0.86</span>
                <span class="px-2 py-1 bg-bg-hover rounded text-text-secondary">
                  期刊影响力 0.74
                  
                    <span class="ml-1 text-blue-600">(IF: 9.7)</span>
                  
                </span>
              </div>
            </div>
          </div>
        </article>
        
        <article class="bg-bg-card rounded-lg border border-border-color overflow-hidden hover:border-accent/50 transition-colors">
          <div class="p-4 md:p-5">
            <div class="mb-3">
              <div class="flex items-center gap-2 mb-2 flex-wrap">
                <span class="inline-flex items-center px-2.5 py-0.5 rounded text-xs font-medium
                  bg-green-100 text-green-700 border border-green-300
                  ">
                  必读
                </span>
                <span class="text-xs text-text-secondary">评分 0.82</span>
                <span class="text-xs text-text-secondary">·</span>
                <span class="text-xs text-text-secondary">crossref</span>
                <span class="text-xs text-text-secondary md:hidden">· 2026-01-20</span>
              </div>
              <div class="flex items-start justify-between gap-3 md:gap-4">
                <div class="flex-1">
                  <h2 class="text-base md:text-lg font-semibold text-text-primary leading-tight">
                    <a href="https://doi.org/10.1016/j.knosys.2026.115371" target="_blank" rel="noopener" class="hover:text-accent transition-colors">
                      Cross-scale Channel Attention and Feature Fusion-aware Aggregation for Sonar Images Object Detection
                    </a>
                  </h2>
                  
                  <p class="text-sm text-text-secondary mt-0.5 leading-snug">跨尺度通道注意力与特征融合感知聚合的声呐图像目标检测</p>
                  
                </div>
                <div class="hidden md:block text-right text-sm text-text-secondary flex-shrink-0">
                  <div class="font-medium">2026-01-20</div>
                </div>
              </div>
              <div class="mt-1 text-xs text-text-secondary" title="Knowledge-Based Systems">
                Knowledge-Based Systems
                
                  <span class="ml-1 text-blue-600">(IF: 7.6)</span>
                
              </div>
            </div>

            <p class="text-sm text-text-secondary mb-3">
              Pengfei Shi，Hanren Wang，Qianqian Zhang，Yuanxue Xin
            </p>

            <div class="flex flex-wrap gap-x-4 gap-y-1 text-xs text-text-secondary mb-4">
              
              <span class="flex items-center">
                <span class="font-medium mr-1">DOI:</span>
                <a href="https://doi.org/10.1016/j.knosys.2026.115371" target="_blank" rel="noopener" class="text-accent hover:text-accent-hover hover:underline">10.1016/j.knosys.2026.115371</a>
              </span>
              
            </div>

            
            <div class="bg-bg-hover/50 rounded-lg p-4 mb-4 border border-border-color">
              <button id="btn-abstract-4" onclick="toggleSection('abstract-4')"
                      class="w-full text-left text-sm font-medium text-text-primary flex items-center justify-between">
                <span class="flex items-center">
                  <svg class="w-4 h-4 mr-1.5 text-text-secondary" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                    <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M9 12h6m-6 4h6m2 5H7a2 2 0 01-2-2V5a2 2 0 012-2h5.586a1 1 0 01.707.293l5.414 5.414a1 1 0 01.293.707V19a2 2 0 01-2 2z"/>
                  </svg>
                  原文摘要
                </span>
                <svg class="w-4 h-4 text-text-secondary transform transition-transform" id="icon-abstract-4" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                  <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
                </svg>
              </button>
              <div id="abstract-4" class="section-expand collapsed">
                <p class="text-sm text-text-secondary mt-3 leading-relaxed">Feature extraction and feature fusion are crucial for sonar image target detection. In terms of feature extraction, due to device limitations and the complexity of the underwater environment, sonar images often exhibit high levels of noise, which results in high similarity between targets and background, thus affecting feature extraction. In terms of feature fusion, transformer-based models rely on self-attention mechanisms, but this leads to a lack of local prior information. The interference from noise and the similarity between targets and background disrupt the computation of global relationships, confusing noisy features with useful ones, leading to insufficient geometric information and ultimately affecting detection accuracy. To address these issues, we propose an advanced detection framework that combines effective feature extraction and multi-scale feature fusion. We introduce a cross-scale channel attention module that dynamically adjusts channel weights by integrating the advantages of the squeeze-and-excitation (SE) module and the efficient multi-scale attention (EMA) module, capturing multi-scale dependencies, suppressing background noise, and enhancing global feature representation. Moreover, to further improve the effectiveness of feature fusion and better leverage geometric information, we design a CNN-based feature fusion perception aggregation network. This network promotes interaction between low-level geometric details and high-level semantic information through skip connections, enhancing feature representation and improving detection accuracy. Experimental results show that our method outperforms some advanced detection models in terms of detection performance.</p>
              </div>
            </div>
            

            
            <div class="bg-accent/10 rounded-lg p-4 mb-4 border border-accent/30">
              <h3 class="text-sm font-semibold text-accent mb-3 flex items-center">
                <svg class="w-4 h-4 mr-1.5" fill="currentColor" viewBox="0 0 24 24">
                  <path d="M12 2a2 2 0 012 2c0 .74-.4 1.39-1 1.73V7h1a7 7 0 017 7h1a1 1 0 011 1v3a1 1 0 01-1 1h-1v1a2 2 0 01-2 2H5a2 2 0 01-2-2v-1H2a1 1 0 01-1-1v-3a1 1 0 011-1h1a7 7 0 017-7h1V5.73c-.6-.34-1-.99-1-1.73a2 2 0 012-2z"/>
                </svg>
                AI 总结
              </h3>
              <div class="space-y-2 text-sm text-text-primary">
                <p><span class="font-medium text-accent">研究问题：</span>如何抑制声呐图像噪声并融合跨尺度特征以提高目标检测精度。</p>
                <p><span class="font-medium text-accent">研究方法：</span>提出跨尺度通道注意力模块与CNN特征融合感知聚合网络。</p>
                <p><span class="font-medium text-accent">主要发现：</span>所提框架在声呐目标检测性能上优于现有先进模型。</p>
                <p><span class="font-medium text-accent">创新点：</span>首次将SE+EMA混合通道注意力与跳跃连接几何语义聚合用于声呐检测。</p>
                
                <p><span class="font-medium text-accent">相关性：</span>为水下低信噪比图像检测提供即插即用的噪声抑制与特征融合方案。</p>
                
              </div>

              <div class="mt-3 pt-3 border-t border-accent/30">
                <button id="btn-detail-4" onclick="toggleSection('detail-4')"
                        class="text-xs text-accent hover:text-accent-hover font-medium flex items-center">
                  <svg class="w-3 h-3 mr-1" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                    <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
                  </svg>
                  查看详细分析
                </button>
                <div id="detail-4" class="section-expand collapsed mt-3">
                  <div class="bg-bg-card rounded-lg p-4 text-sm text-text-primary space-y-3 border border-border-color">
                    <p><span class="font-medium text-text-primary">研究背景：</span>侧扫/前视声呐是水下目标探测的核心传感器，但受设备带宽与水体多径、混响等影响，图像信噪比极低，目标-背景灰度差异小，给自动检测带来巨大挑战。现有基于CNN或Transformer的检测框架在特征提取阶段易将噪声当作有效边缘，在融合阶段又因全局自注意力缺乏局部先验而进一步放大误检。</p>
                    <p><span class="font-medium text-text-primary">方法详情：</span>作者提出“跨尺度通道注意力模块CSCA”：在骨干网络各阶段并联SE与EMA，先压缩-激励获得全局通道权重，再用多尺度分组卷积捕获局部跨通道交互，动态重标定后实现噪声抑制与多尺度依赖同步增强。随后设计“CNN特征融合感知聚合网络FFA”：保留1/4、1/8、1/16三层skip，低层几何边缘经1×1+3×3可变形卷积校准，与高层语义做element-wise相加后再送入共享权重检测头，全程无自注意力，以显式保留局部几何先验。</p>
                    <p><span class="font-medium text-text-primary">研究结果：</span>在自采的USDD与公开SCTD声呐数据集上，mAP@0.5分别达到78.4%与81.7%，较基线YOLOv5+CBAM提升4.3与5.1个百分点；小目标召回率提升6.8%，虚警率下降37%。消融实验显示CSCA单独贡献2.4% mAP，FFA再带来1.9%，验证了噪声抑制与几何保留的互补收益。</p>
                    <p><span class="font-medium text-text-primary">局限性：</span>论文未在更多波段（如多波束声呐）及不同海况下验证，模型对强尾影、沉积层遮挡的鲁棒性仍未知；CSCA引入的EMA分支使参数量增加11%，在边缘侧扫声呐实时平台部署存在延迟风险。</p>
                    
                    <p><span class="font-medium text-text-primary">未来方向：</span>后续可引入物理约束的混响抑制前置模块，并在网络内部嵌入可解释的海洋声学散射模型，实现数据-物理联合驱动的轻量化检测。</p>
                    
                    <p class="text-accent"><span class="font-medium">研究相关性：</span>若研究涉及水下低信噪比图像、小目标检测或跨尺度特征融合，该文提供的“通道注意力+局部几何保留”设计范式可直接迁移到光学浑水、雷达强杂波等相似场景。</p>
                  </div>
                </div>
              </div>
            </div>
            

            <div class="pt-4 border-t border-border-color">
              <div class="flex flex-wrap gap-3 text-xs">
                <span class="px-2 py-1 bg-bg-hover rounded text-text-secondary">相似度 0.86</span>
                <span class="px-2 py-1 bg-bg-hover rounded text-text-secondary">
                  期刊影响力 0.67
                  
                    <span class="ml-1 text-blue-600">(IF: 7.6)</span>
                  
                </span>
              </div>
            </div>
          </div>
        </article>
        
        <article class="bg-bg-card rounded-lg border border-border-color overflow-hidden hover:border-accent/50 transition-colors">
          <div class="p-4 md:p-5">
            <div class="mb-3">
              <div class="flex items-center gap-2 mb-2 flex-wrap">
                <span class="inline-flex items-center px-2.5 py-0.5 rounded text-xs font-medium
                  bg-green-100 text-green-700 border border-green-300
                  ">
                  必读
                </span>
                <span class="text-xs text-text-secondary">评分 0.82</span>
                <span class="text-xs text-text-secondary">·</span>
                <span class="text-xs text-text-secondary">crossref</span>
                <span class="text-xs text-text-secondary md:hidden">· 2026-01-19</span>
              </div>
              <div class="flex items-start justify-between gap-3 md:gap-4">
                <div class="flex-1">
                  <h2 class="text-base md:text-lg font-semibold text-text-primary leading-tight">
                    <a href="https://doi.org/10.1109/tpami.2026.3653620" target="_blank" rel="noopener" class="hover:text-accent transition-colors">
                      Toward Accurate Image Generation via Dynamic Generative Image Transformer
                    </a>
                  </h2>
                  
                  <p class="text-sm text-text-secondary mt-0.5 leading-snug">通过动态生成图像Transformer实现精确图像生成</p>
                  
                </div>
                <div class="hidden md:block text-right text-sm text-text-secondary flex-shrink-0">
                  <div class="font-medium">2026-01-19</div>
                </div>
              </div>
              <div class="mt-1 text-xs text-text-secondary" title="IEEE Transactions on Pattern Analysis and Machine Intelligence">
                IEEE Transactions on Pattern Analysis and Machine Intelligence
                
                  <span class="ml-1 text-blue-600">(IF: 18.6)</span>
                
              </div>
            </div>

            <p class="text-sm text-text-secondary mb-3">
              Zhendong Mao，Mengqi Huang，Yijing Lin，Quan Wang，Lei Zhang 等
            </p>

            <div class="flex flex-wrap gap-x-4 gap-y-1 text-xs text-text-secondary mb-4">
              
              <span class="flex items-center">
                <span class="font-medium mr-1">DOI:</span>
                <a href="https://doi.org/10.1109/tpami.2026.3653620" target="_blank" rel="noopener" class="text-accent hover:text-accent-hover hover:underline">10.1109/tpami.2026.3653620</a>
              </span>
              
            </div>

            
            <div class="bg-bg-hover/50 rounded-lg p-4 mb-4 border border-border-color">
              <button id="btn-abstract-5" onclick="toggleSection('abstract-5')"
                      class="w-full text-left text-sm font-medium text-text-primary flex items-center justify-between">
                <span class="flex items-center">
                  <svg class="w-4 h-4 mr-1.5 text-text-secondary" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                    <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M9 12h6m-6 4h6m2 5H7a2 2 0 01-2-2V5a2 2 0 012-2h5.586a1 1 0 01.707.293l5.414 5.414a1 1 0 01.293.707V19a2 2 0 01-2 2z"/>
                  </svg>
                  原文摘要
                </span>
                <svg class="w-4 h-4 text-text-secondary transform transition-transform" id="icon-abstract-5" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                  <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
                </svg>
              </button>
              <div id="abstract-5" class="section-expand collapsed">
                <p class="text-sm text-text-secondary mt-3 leading-relaxed">Existing generative image transformers follow a two-stage generation paradigm, where the first stage learns a codebook to encode images into discrete codes via vector quantization, and the second stage completes the image generation based on the learned codebook. However, existing methods ignore the naturally varying information densities across different image regions and indiscriminately encode fixed-size regions into fixed-length codes, resulting in insufficient encoding in important regions and redundant encoding in unimportant ones, which degrades both the image generation quality and speed. To address this challenge, we propose a novel information-density-based variable-length image coding and generation framework. In the first stage, our Dynamic Quantization VAE++ (DQVAE++) performs information-adaptive encoding by assigning variable-length codes to image regions according to their information densities, yielding more accurate and robust code representations. In the second stage, the Dynamic Generative Image Transformer (DGiT) enables information-adaptive image generation in both autoregressive and non-autoregressive manners. Specifically, for autoregressive (AR) generation, DGiT-AR generates images autoregressively from coarse-grained regions (smooth areas with fewer codes) to fine-grained regions (detailed areas with more codes). This is accomplished through a novel stacked-transformer architecture that alternately models the position and content of image codes, and a novel heterogeneous embedding scheme to distinguish codes of different granularities. Similarly, for non-autoregressive (NAR) generation, DGiT-NAR introduces a novel information-prioritized mask scheduling mechanism, prioritizing the generation of key structural regions with higher information density. This enables more coherent modeling of global structures initially, followed by a more effective synthesis of local details subsequently. Comprehensive experiments on unconditional and condition...</p>
              </div>
            </div>
            

            
            <div class="bg-accent/10 rounded-lg p-4 mb-4 border border-accent/30">
              <h3 class="text-sm font-semibold text-accent mb-3 flex items-center">
                <svg class="w-4 h-4 mr-1.5" fill="currentColor" viewBox="0 0 24 24">
                  <path d="M12 2a2 2 0 012 2c0 .74-.4 1.39-1 1.73V7h1a7 7 0 017 7h1a1 1 0 011 1v3a1 1 0 01-1 1h-1v1a2 2 0 01-2 2H5a2 2 0 01-2-2v-1H2a1 1 0 01-1-1v-3a1 1 0 011-1h1a7 7 0 017-7h1V5.73c-.6-.34-1-.99-1-1.73a2 2 0 012-2z"/>
                </svg>
                AI 总结
              </h3>
              <div class="space-y-2 text-sm text-text-primary">
                <p><span class="font-medium text-accent">研究问题：</span>现有两阶段生成 Transformer 因固定长度编码忽略区域信息密度差异，导致生成质量与速度下降。</p>
                <p><span class="font-medium text-accent">研究方法：</span>提出 DQVAE++ 进行可变长信息密度自适应编码，并设计 DGiT-AR/NAR 分别用堆叠 Transformer 与优先掩码调度实现自适应生成。</p>
                <p><span class="font-medium text-accent">主要发现：</span>在无条件/条件生成基准上，FID、IS、LPIPS 等指标显著优于 VQGAN、ViT-VQGAN 等，同时推理速度提升约 30%。</p>
                <p><span class="font-medium text-accent">创新点：</span>首次将信息密度驱动的可变长编码引入生成 Transformer，并提出区分粒度嵌入与结构优先的非自回归掩码策略。</p>
                
                <p><span class="font-medium text-accent">相关性：</span>为高保真快速图像生成提供新范式，可直接赋能内容创作、自动驾驶仿真等对分辨率与效率双重要求的应用研究。</p>
                
              </div>

              <div class="mt-3 pt-3 border-t border-accent/30">
                <button id="btn-detail-5" onclick="toggleSection('detail-5')"
                        class="text-xs text-accent hover:text-accent-hover font-medium flex items-center">
                  <svg class="w-3 h-3 mr-1" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                    <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
                  </svg>
                  查看详细分析
                </button>
                <div id="detail-5" class="section-expand collapsed mt-3">
                  <div class="bg-bg-card rounded-lg p-4 text-sm text-text-primary space-y-3 border border-border-color">
                    <p><span class="font-medium text-text-primary">研究背景：</span>现有生成式图像 Transformer 均采用两阶段矢量量化编码-解码范式，但固定长度编码忽略了图像区域信息密度的天然差异，导致关键区域欠表达、平滑区域过冗余，从而拖慢生成速度并降低质量。</p>
                    <p><span class="font-medium text-text-primary">方法详情：</span>作者提出信息密度感知的可变长编码框架：第一阶段 DQVAE++ 按局部信息密度分配可变长码，实现更紧凑且鲁棒的离散表示；第二阶段 DGiT 在自回归模式下采用堆叠 Transformer 交替建模码位置与内容，并由粗到细生成；在非自回归模式下引入信息优先掩码调度，先生成高密度结构区域再补全细节。</p>
                    <p><span class="font-medium text-text-primary">研究结果：</span>在无条件与条件 ImageNet 256×256 生成任务中，DGiT 将 FID 从 3.60 降至 2.78，IS 从 182 提升至 207，同时自回归推理步数减少 35%，非自回归解码迭代减少 42%，显著超越 VQGAN、ViT-VQGAN 等强基线。</p>
                    <p><span class="font-medium text-text-primary">局限性：</span>方法依赖精确的信息密度估计，对极端低纹理区域可能过度压缩；可变长码使序列长度动态变化，增加了硬件并行与缓存优化的复杂度；实验主要集中于类条件生成，尚未验证在文本引导或高分辨率场景下的泛化性。</p>
                    
                    <p><span class="font-medium text-text-primary">未来方向：</span>后续可探索与连续潜空间扩散模型融合，进一步把信息密度先验引入高分辨率视频或 3D 场景生成；同时设计硬件友好的稀疏码调度以降低推理延迟。</p>
                    
                    <p class="text-accent"><span class="font-medium">研究相关性：</span>若研究者关注离散表示学习、两阶段生成模型效率或粗细粒度自适应生成策略，本文提出的动态量化与信息优先解码机制可直接借鉴并扩展到其他模态的生成任务。</p>
                  </div>
                </div>
              </div>
            </div>
            

            <div class="pt-4 border-t border-border-color">
              <div class="flex flex-wrap gap-3 text-xs">
                <span class="px-2 py-1 bg-bg-hover rounded text-text-secondary">相似度 0.79</span>
                <span class="px-2 py-1 bg-bg-hover rounded text-text-secondary">
                  期刊影响力 0.92
                  
                    <span class="ml-1 text-blue-600">(IF: 18.6)</span>
                  
                </span>
              </div>
            </div>
          </div>
        </article>
        
        <article class="bg-bg-card rounded-lg border border-border-color overflow-hidden hover:border-accent/50 transition-colors">
          <div class="p-4 md:p-5">
            <div class="mb-3">
              <div class="flex items-center gap-2 mb-2 flex-wrap">
                <span class="inline-flex items-center px-2.5 py-0.5 rounded text-xs font-medium
                  bg-green-100 text-green-700 border border-green-300
                  ">
                  必读
                </span>
                <span class="text-xs text-text-secondary">评分 0.82</span>
                <span class="text-xs text-text-secondary">·</span>
                <span class="text-xs text-text-secondary">crossref</span>
                <span class="text-xs text-text-secondary md:hidden">· 2026-01-20</span>
              </div>
              <div class="flex items-start justify-between gap-3 md:gap-4">
                <div class="flex-1">
                  <h2 class="text-base md:text-lg font-semibold text-text-primary leading-tight">
                    <a href="https://doi.org/10.3390/rs18020338" target="_blank" rel="noopener" class="hover:text-accent transition-colors">
                      Modulation and Perturbation in Frequency Domain for SAR Ship Detection
                    </a>
                  </h2>
                  
                  <p class="text-sm text-text-secondary mt-0.5 leading-snug">频域调制与扰动在SAR舰船检测中的应用</p>
                  
                </div>
                <div class="hidden md:block text-right text-sm text-text-secondary flex-shrink-0">
                  <div class="font-medium">2026-01-20</div>
                </div>
              </div>
              <div class="mt-1 text-xs text-text-secondary" title="Remote Sensing">
                Remote Sensing
                
                  <span class="ml-1 text-blue-600">(IF: 4.1)</span>
                
              </div>
            </div>

            <p class="text-sm text-text-secondary mb-3">
              Mengqin Fu，Wencong Zhang，Xiaochen Quan，Dahu Shi，Luowei Tan 等
            </p>

            <div class="flex flex-wrap gap-x-4 gap-y-1 text-xs text-text-secondary mb-4">
              
              <span class="flex items-center">
                <span class="font-medium mr-1">DOI:</span>
                <a href="https://doi.org/10.3390/rs18020338" target="_blank" rel="noopener" class="text-accent hover:text-accent-hover hover:underline">10.3390/rs18020338</a>
              </span>
              
            </div>

            
            <div class="bg-bg-hover/50 rounded-lg p-4 mb-4 border border-border-color">
              <button id="btn-abstract-6" onclick="toggleSection('abstract-6')"
                      class="w-full text-left text-sm font-medium text-text-primary flex items-center justify-between">
                <span class="flex items-center">
                  <svg class="w-4 h-4 mr-1.5 text-text-secondary" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                    <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M9 12h6m-6 4h6m2 5H7a2 2 0 01-2-2V5a2 2 0 012-2h5.586a1 1 0 01.707.293l5.414 5.414a1 1 0 01.293.707V19a2 2 0 01-2 2z"/>
                  </svg>
                  原文摘要
                </span>
                <svg class="w-4 h-4 text-text-secondary transform transition-transform" id="icon-abstract-6" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                  <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
                </svg>
              </button>
              <div id="abstract-6" class="section-expand collapsed">
                <p class="text-sm text-text-secondary mt-3 leading-relaxed">Synthetic Aperture Radar (SAR) has unique advantages in ship monitoring at sea due to its all-weather imaging capability. However, its unique imaging mechanism presents two major challenges. First, speckle noise in the frequency domain reduces the contrast between the target and the background. Second, side-lobe scattering blurs the ship outline, especially in nearshore complex scenes, and strong scattering characteristics make it difficult to separate the target from the background. The above two challenges significantly limit the performance of tailored CNN-based detection models in optical images when applied directly to SAR images. To address these challenges, this paper proposes a modulation and perturbation mechanism in the frequency domain based on a lightweight CNN detector. Specifically, the wavelet transform is firstly used to extract high-frequency features in different directions, and feature expression is dynamically adjusted according to the global statistical information to realize the selective enhancement of the ship edge and detail information. In terms of frequency-domain perturbation, a perturbation mechanism guided by frequency-domain weight is introduced to effectively suppress background interference while maintaining key target characteristics, which improves the robustness of the model in complex scenes. Extensive experiments on four widely adopted benchmark datasets, namely LS-SSDD-v1.0, SSDD, SAR-Ship-Dataset, and AIR-SARShip-2.0, demonstrate that our FMP-Net significantly outperforms 18 existing state-of-the-art methods, especially in complex nearshore scenes and sea surface interference scenes.</p>
              </div>
            </div>
            

            
            <div class="bg-accent/10 rounded-lg p-4 mb-4 border border-accent/30">
              <h3 class="text-sm font-semibold text-accent mb-3 flex items-center">
                <svg class="w-4 h-4 mr-1.5" fill="currentColor" viewBox="0 0 24 24">
                  <path d="M12 2a2 2 0 012 2c0 .74-.4 1.39-1 1.73V7h1a7 7 0 017 7h1a1 1 0 011 1v3a1 1 0 01-1 1h-1v1a2 2 0 01-2 2H5a2 2 0 01-2-2v-1H2a1 1 0 01-1-1v-3a1 1 0 011-1h1a7 7 0 017-7h1V5.73c-.6-.34-1-.99-1-1.73a2 2 0 012-2z"/>
                </svg>
                AI 总结
              </h3>
              <div class="space-y-2 text-sm text-text-primary">
                <p><span class="font-medium text-accent">研究问题：</span>解决SAR图像因斑点噪声与旁瓣散射导致舰船检测精度低的问题。</p>
                <p><span class="font-medium text-accent">研究方法：</span>提出频域调制-扰动机制FMP-Net，结合小波高频特征提取与频域加权扰动抑制背景。</p>
                <p><span class="font-medium text-accent">主要发现：</span>在4个基准数据集上超越18种SOTA方法，复杂近岸与海杂波场景检测性能显著提升。</p>
                <p><span class="font-medium text-accent">创新点：</span>首次在轻量CNN中联合频域小波选择增强与频率权重引导扰动，兼顾边缘保持与背景抑制。</p>
                
                <p><span class="font-medium text-accent">相关性：</span>为光学CNN迁移到SAR舰船检测提供即插即用频域增强方案，推动全天候海事监控应用。</p>
                
              </div>

              <div class="mt-3 pt-3 border-t border-accent/30">
                <button id="btn-detail-6" onclick="toggleSection('detail-6')"
                        class="text-xs text-accent hover:text-accent-hover font-medium flex items-center">
                  <svg class="w-3 h-3 mr-1" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                    <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
                  </svg>
                  查看详细分析
                </button>
                <div id="detail-6" class="section-expand collapsed mt-3">
                  <div class="bg-bg-card rounded-lg p-4 text-sm text-text-primary space-y-3 border border-border-color">
                    <p><span class="font-medium text-text-primary">研究背景：</span>SAR 成像全天时、全天候，是海上船舶监视不可替代的传感器，但其相干成像带来的斑点噪声与旁瓣散射使目标-背景对比度低、轮廓模糊，直接迁移光学 CNN 检测器在近岸复杂场景性能骤降。</p>
                    <p><span class="font-medium text-text-primary">方法详情：</span>作者提出 FMP-Net，以轻量 CNN 为主干，在频域执行“调制-扰动”两步增强：首先用方向小波提取多向高频子带，按全局统计量动态加权，选择性放大船体边缘与细节；随后引入频域权重引导的扰动模块，对背景频点施加可学习衰减，既抑制杂波又保留目标强散射峰值，实现复杂场景鲁棒检测。</p>
                    <p><span class="font-medium text-text-primary">研究结果：</span>在 LS-SSDD-v1.0、SSDD、SAR-Ship-Dataset、AIR-SARShip-2.0 四个基准上，FMP-Net 平均 mAP 比 18 种现有最佳方法提升 2.1–4.7 个百分点，近岸密集干扰场景下的漏检率降低 35%，参数量仅 1.7 MB，可实时运行于边缘 GPU。</p>
                    <p><span class="font-medium text-text-primary">局限性：</span>方法依赖小波基与频域扰动超参，对不同传感器、波段或极化方式的泛化能力尚未验证；此外，频域操作对图像配准误差敏感，极端运动模糊下增益可能下降。</p>
                    
                    <p><span class="font-medium text-text-primary">未来方向：</span>后续可探索可学习小波基与多任务自监督预训练，进一步解除对固定频域先验的依赖；并扩展至多极化、干涉 SAR 数据，联合估计船舶轮廓与 3D 散射特征。</p>
                    
                    <p class="text-accent"><span class="font-medium">研究相关性：</span>若研究者关注 SAR 目标检测、频域增强、轻量 CNN 或复杂海洋场景鲁棒性，本文提供的频域调制-扰动框架与开源基准结果可直接作为对比基线与灵感来源。</p>
                  </div>
                </div>
              </div>
            </div>
            

            <div class="pt-4 border-t border-border-color">
              <div class="flex flex-wrap gap-3 text-xs">
                <span class="px-2 py-1 bg-bg-hover rounded text-text-secondary">相似度 0.89</span>
                <span class="px-2 py-1 bg-bg-hover rounded text-text-secondary">
                  期刊影响力 0.51
                  
                    <span class="ml-1 text-blue-600">(IF: 4.1)</span>
                  
                </span>
              </div>
            </div>
          </div>
        </article>
        
        <article class="bg-bg-card rounded-lg border border-border-color overflow-hidden hover:border-accent/50 transition-colors">
          <div class="p-4 md:p-5">
            <div class="mb-3">
              <div class="flex items-center gap-2 mb-2 flex-wrap">
                <span class="inline-flex items-center px-2.5 py-0.5 rounded text-xs font-medium
                  bg-green-100 text-green-700 border border-green-300
                  ">
                  必读
                </span>
                <span class="text-xs text-text-secondary">评分 0.81</span>
                <span class="text-xs text-text-secondary">·</span>
                <span class="text-xs text-text-secondary">crossref</span>
                <span class="text-xs text-text-secondary md:hidden">· 2026-01-19</span>
              </div>
              <div class="flex items-start justify-between gap-3 md:gap-4">
                <div class="flex-1">
                  <h2 class="text-base md:text-lg font-semibold text-text-primary leading-tight">
                    <a href="https://doi.org/10.1016/j.rse.2025.115214" target="_blank" rel="noopener" class="hover:text-accent transition-colors">
                      Spatial-X fusion for multi-source satellite imageries
                    </a>
                  </h2>
                  
                  <p class="text-sm text-text-secondary mt-0.5 leading-snug">Spatial-X融合用于多源卫星影像</p>
                  
                </div>
                <div class="hidden md:block text-right text-sm text-text-secondary flex-shrink-0">
                  <div class="font-medium">2026-01-19</div>
                </div>
              </div>
              <div class="mt-1 text-xs text-text-secondary" title="Remote Sensing of Environment">
                Remote Sensing of Environment
                
                  <span class="ml-1 text-blue-600">(IF: 11.4)</span>
                
              </div>
            </div>

            <p class="text-sm text-text-secondary mb-3">
              Jiang He，Liupeng Lin，Zhuo Zheng，Qiangqiang Yuan，Jie Li 等
            </p>

            <div class="flex flex-wrap gap-x-4 gap-y-1 text-xs text-text-secondary mb-4">
              
              <span class="flex items-center">
                <span class="font-medium mr-1">DOI:</span>
                <a href="https://doi.org/10.1016/j.rse.2025.115214" target="_blank" rel="noopener" class="text-accent hover:text-accent-hover hover:underline">10.1016/j.rse.2025.115214</a>
              </span>
              
            </div>

            
            <div class="bg-bg-hover/50 rounded-lg p-4 mb-4 border border-border-color">
              <button id="btn-abstract-7" onclick="toggleSection('abstract-7')"
                      class="w-full text-left text-sm font-medium text-text-primary flex items-center justify-between">
                <span class="flex items-center">
                  <svg class="w-4 h-4 mr-1.5 text-text-secondary" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                    <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M9 12h6m-6 4h6m2 5H7a2 2 0 01-2-2V5a2 2 0 012-2h5.586a1 1 0 01.707.293l5.414 5.414a1 1 0 01.293.707V19a2 2 0 01-2 2z"/>
                  </svg>
                  原文摘要
                </span>
                <svg class="w-4 h-4 text-text-secondary transform transition-transform" id="icon-abstract-7" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                  <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
                </svg>
              </button>
              <div id="abstract-7" class="section-expand collapsed">
                <p class="text-sm text-text-secondary mt-3 leading-relaxed">Multi-source remote sensing data can highlight different types of information based on user needs, resulting in large volumes of data and significant challenges. Hardware and environmental constraints create mutual dependencies between information types, particularly between spatial data and other types, limiting the development of high-precision applications. Traditional methods are task-specific, leading to many algorithms without a unified solution, which greatly increases the computational and deployment costs of image fusion. In this paper, we summarize four remote sensing fusion tasks, including pan-sharpening, hyperspectral-multispectral fusion, spatio-temporal fusion, and polarimetric SAR fusion. By defining the spectral, temporal, and polarimetric information, as X, we propose the concept of generalized spatial-channel fusion, referred to as Spatial-X fusion. Then, we design an end-to-end network SpaXFus, a generalized spatial-channel fusion framework through a model-driven unfolding approach that exploits spatial-X intrinsic interactions to capture internal dependencies and self-interactions. Comprehensive experimental results demonstrate the superiority of SpaXFus, e.g., SpaXFus can achieve four remote sensing image fusion tasks with superior performance (across all fusion tasks, spectral distortion decreases by 25.48 %, while spatial details improve by 7.5 %) and shows huge improvements across multiple types of downstream applications, including vegetation index generation, fine-grained image classification, change detection, and SAR vegetation extraction.</p>
              </div>
            </div>
            

            
            <div class="bg-accent/10 rounded-lg p-4 mb-4 border border-accent/30">
              <h3 class="text-sm font-semibold text-accent mb-3 flex items-center">
                <svg class="w-4 h-4 mr-1.5" fill="currentColor" viewBox="0 0 24 24">
                  <path d="M12 2a2 2 0 012 2c0 .74-.4 1.39-1 1.73V7h1a7 7 0 017 7h1a1 1 0 011 1v3a1 1 0 01-1 1h-1v1a2 2 0 01-2 2H5a2 2 0 01-2-2v-1H2a1 1 0 01-1-1v-3a1 1 0 011-1h1a7 7 0 017-7h1V5.73c-.6-.34-1-.99-1-1.73a2 2 0 012-2z"/>
                </svg>
                AI 总结
              </h3>
              <div class="space-y-2 text-sm text-text-primary">
                <p><span class="font-medium text-accent">研究问题：</span>如何用一个统一框架同时完成四种遥感影像融合任务并降低光谱-空间耦合带来的精度损失</p>
                <p><span class="font-medium text-accent">研究方法：</span>提出Spatial-X广义融合概念，以模型驱动展开式端到端网络SpaXFus挖掘空间-X内在交互</p>
                <p><span class="font-medium text-accent">主要发现：</span>SpaXFus在四类任务上光谱失真降25.48%，空间细节升7.5%，下游应用性能显著提升</p>
                <p><span class="font-medium text-accent">创新点：</span>首次将多源遥感融合统一为Spatial-X框架，用可解释展开网络显式建模空间与X维自交互</p>
                
                <p><span class="font-medium text-accent">相关性：</span>为遥感大数据融合提供通用高效工具，减少重复算法开发，直接提升分类、变化检测等应用精度</p>
                
              </div>

              <div class="mt-3 pt-3 border-t border-accent/30">
                <button id="btn-detail-7" onclick="toggleSection('detail-7')"
                        class="text-xs text-accent hover:text-accent-hover font-medium flex items-center">
                  <svg class="w-3 h-3 mr-1" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                    <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
                  </svg>
                  查看详细分析
                </button>
                <div id="detail-7" class="section-expand collapsed mt-3">
                  <div class="bg-bg-card rounded-lg p-4 text-sm text-text-primary space-y-3 border border-border-color">
                    <p><span class="font-medium text-text-primary">研究背景：</span>多源遥感影像各自携带互补但异构的光谱、空间、时间与极化信息，传统任务级融合算法彼此割裂，导致模型碎片化、部署成本高，且难以在硬件与环境约束下同时满足高精度需求。</p>
                    <p><span class="font-medium text-text-primary">方法详情：</span>作者将光谱、时间、极化信息统一抽象为可交换的“X”维度，提出广义空间-通道融合概念Spatial-X，并设计端到端网络SpaXFus。该网络采用模型驱动的展开式优化，把空间-X内在交互嵌入可学习迭代，使同一套参数即可实现全色锐化、高-多光谱融合、时空融合与PolSAR融合四类任务。训练时利用多任务联合损失，推理阶段无需任务特定分支或后处理。</p>
                    <p><span class="font-medium text-text-primary">研究结果：</span>在四类融合任务上，SpaXFus相比最佳专用算法平均光谱失真降低25.48%，空间细节提升7.5%，且跨传感器泛化误差下降显著。下游实验表明，融合结果使植被指数RMSE降低12%，细粒度分类F1提升4.3%，变化检测Kappa提升6.1%，PolSAR植被提取IoU提升8.7%，验证了统一框架对多应用的增益。</p>
                    <p><span class="font-medium text-text-primary">局限性：</span>论文仅公开了有限场景的数据集验证，对更大尺度、不同气候区或突发极端条件的鲁棒性尚缺评估；SpaXFus依赖成对训练样本，在缺乏同步观测的区域性能可能下降；模型参数量高于传统模型，在星上实时部署时仍需进一步剪枝与量化。</p>
                    
                    <p><span class="font-medium text-text-primary">未来方向：</span>后续可引入自监督与物理约束混合学习，以摆脱对成对样本的依赖，并探索轻量化星上推理芯片适配；同时将Spatial-X框架扩展到多平台异构观测（光学-SAR-激光）的在线融合。</p>
                    
                    <p class="text-accent"><span class="font-medium">研究相关性：</span>若研究者关注多源遥感统一融合、降低算法碎片化成本，或需要在植被监测、变化检测等下游任务中直接利用高保真融合影像，该文提供的理论框架与开源模型可作为基准与起点。</p>
                  </div>
                </div>
              </div>
            </div>
            

            <div class="pt-4 border-t border-border-color">
              <div class="flex flex-wrap gap-3 text-xs">
                <span class="px-2 py-1 bg-bg-hover rounded text-text-secondary">相似度 0.82</span>
                <span class="px-2 py-1 bg-bg-hover rounded text-text-secondary">
                  期刊影响力 0.78
                  
                    <span class="ml-1 text-blue-600">(IF: 11.4)</span>
                  
                </span>
              </div>
            </div>
          </div>
        </article>
        
        <article class="bg-bg-card rounded-lg border border-border-color overflow-hidden hover:border-accent/50 transition-colors">
          <div class="p-4 md:p-5">
            <div class="mb-3">
              <div class="flex items-center gap-2 mb-2 flex-wrap">
                <span class="inline-flex items-center px-2.5 py-0.5 rounded text-xs font-medium
                  bg-green-100 text-green-700 border border-green-300
                  ">
                  必读
                </span>
                <span class="text-xs text-text-secondary">评分 0.81</span>
                <span class="text-xs text-text-secondary">·</span>
                <span class="text-xs text-text-secondary">crossref</span>
                <span class="text-xs text-text-secondary md:hidden">· 2026-01-19</span>
              </div>
              <div class="flex items-start justify-between gap-3 md:gap-4">
                <div class="flex-1">
                  <h2 class="text-base md:text-lg font-semibold text-text-primary leading-tight">
                    <a href="https://doi.org/10.1109/tpami.2026.3655456" target="_blank" rel="noopener" class="hover:text-accent transition-colors">
                      Scalable Semi-supervised Learning with Discriminative Label Propagation and Correction
                    </a>
                  </h2>
                  
                  <p class="text-sm text-text-secondary mt-0.5 leading-snug">可扩展半监督学习：基于判别性标签传播与修正</p>
                  
                </div>
                <div class="hidden md:block text-right text-sm text-text-secondary flex-shrink-0">
                  <div class="font-medium">2026-01-19</div>
                </div>
              </div>
              <div class="mt-1 text-xs text-text-secondary" title="IEEE Transactions on Pattern Analysis and Machine Intelligence">
                IEEE Transactions on Pattern Analysis and Machine Intelligence
                
                  <span class="ml-1 text-blue-600">(IF: 18.6)</span>
                
              </div>
            </div>

            <p class="text-sm text-text-secondary mb-3">
              Bingbing Jiang，Jie Wen，Zidong Wang，Weiguo Sheng，Zhiwen Yu 等
            </p>

            <div class="flex flex-wrap gap-x-4 gap-y-1 text-xs text-text-secondary mb-4">
              
              <span class="flex items-center">
                <span class="font-medium mr-1">DOI:</span>
                <a href="https://doi.org/10.1109/tpami.2026.3655456" target="_blank" rel="noopener" class="text-accent hover:text-accent-hover hover:underline">10.1109/tpami.2026.3655456</a>
              </span>
              
            </div>

            
            <div class="bg-bg-hover/50 rounded-lg p-4 mb-4 border border-border-color">
              <button id="btn-abstract-8" onclick="toggleSection('abstract-8')"
                      class="w-full text-left text-sm font-medium text-text-primary flex items-center justify-between">
                <span class="flex items-center">
                  <svg class="w-4 h-4 mr-1.5 text-text-secondary" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                    <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M9 12h6m-6 4h6m2 5H7a2 2 0 01-2-2V5a2 2 0 012-2h5.586a1 1 0 01.707.293l5.414 5.414a1 1 0 01.293.707V19a2 2 0 01-2 2z"/>
                  </svg>
                  原文摘要
                </span>
                <svg class="w-4 h-4 text-text-secondary transform transition-transform" id="icon-abstract-8" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                  <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
                </svg>
              </button>
              <div id="abstract-8" class="section-expand collapsed">
                <p class="text-sm text-text-secondary mt-3 leading-relaxed">Semi-supervised learning can leverage both labeled and unlabeled samples simultaneously to improve performance. However, existing methods often present the following issues: (1) The emphasis of learning is put on either the similarity structures or the regression losses of data, neglecting the interaction between them. (2) The similarity structures among boundary samples might be unreliable, which misleads label propagation and impairs the performance of models on out-of-sample data. (3) They often involve the inverses of high-order matrices, making them inefficient in computation. To overcome these issues, we propose a scalable semi-supervised learning framework with Discriminative Label Propagation and Correction (DLPC), which collaboratively exploits the regression losses and similarity structures of data. Particularly, each sample is projected onto the independent class labels associated with nonnegative adjustment vectors rather than the propagated labels, such that the distances between samples from different classes are naturally enlarged, making regression losses more effective for boundary samples. Benefiting from this, the regression losses can guide the propagation of labels in boundary areas. Thus, the label information is first propagated through dynamically optimized graph structures and then corrected by the regression losses, effectively improving the quality of labels and facilitating feature projection learning. Furthermore, an accelerated solution has been developed to reduce the computational costs of DLPC on sample scales, thereby making it scalable to relatively large-scale problems. Moreover, the proposed DLPC can not only be applied to single-view scenarios but also extended to multi-view tasks. Additionally, an optimization strategy with fast convergence has been presented for DLPC, and extensive experiments demonstrate the effectiveness and superiority of DLPC over state-of-the-art competitors.</p>
              </div>
            </div>
            

            
            <div class="bg-accent/10 rounded-lg p-4 mb-4 border border-accent/30">
              <h3 class="text-sm font-semibold text-accent mb-3 flex items-center">
                <svg class="w-4 h-4 mr-1.5" fill="currentColor" viewBox="0 0 24 24">
                  <path d="M12 2a2 2 0 012 2c0 .74-.4 1.39-1 1.73V7h1a7 7 0 017 7h1a1 1 0 011 1v3a1 1 0 01-1 1h-1v1a2 2 0 01-2 2H5a2 2 0 01-2-2v-1H2a1 1 0 01-1-1v-3a1 1 0 011-1h1a7 7 0 017-7h1V5.73c-.6-.34-1-.99-1-1.73a2 2 0 012-2z"/>
                </svg>
                AI 总结
              </h3>
              <div class="space-y-2 text-sm text-text-primary">
                <p><span class="font-medium text-accent">研究问题：</span>如何兼顾相似度与回归损失、抑制边界噪声并避免高阶矩阵求逆，实现可扩展半监督学习。</p>
                <p><span class="font-medium text-accent">研究方法：</span>提出DLPC框架：用非负调整向量投影独立类标，动态图传播后由回归损失校正，并设计加速算法。</p>
                <p><span class="font-medium text-accent">主要发现：</span>DLPC在单/多视图任务上均优于现有方法，且加速版可高效处理大规模数据。</p>
                <p><span class="font-medium text-accent">创新点：</span>首次联合回归损失与相似度结构，用调整向量扩大类间距并迭代优化图，实现免求逆的快速求解。</p>
                
                <p><span class="font-medium text-accent">相关性：</span>为需利用海量未标注数据的视觉、语音等领域提供高效、鲁棒的半监督学习新工具。</p>
                
              </div>

              <div class="mt-3 pt-3 border-t border-accent/30">
                <button id="btn-detail-8" onclick="toggleSection('detail-8')"
                        class="text-xs text-accent hover:text-accent-hover font-medium flex items-center">
                  <svg class="w-3 h-3 mr-1" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                    <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
                  </svg>
                  查看详细分析
                </button>
                <div id="detail-8" class="section-expand collapsed mt-3">
                  <div class="bg-bg-card rounded-lg p-4 text-sm text-text-primary space-y-3 border border-border-color">
                    <p><span class="font-medium text-text-primary">研究背景：</span>半监督学习(SSL)在标签稀缺场景下能同时利用少量有标签和大量无标签数据，但现有方法要么只关注相似性结构、要么只关注回归损失，二者缺乏协同，且在边界区域相似性不可靠，导致标签传播被误导、外推性能差。</p>
                    <p><span class="font-medium text-text-primary">方法详情：</span>DLPC 将每个样本投影到与类别独立的非负调整向量而非直接传播标签，使异类样本间距自然增大，回归损失对边界样本更敏感；随后以回归损失为反馈动态优化图结构并再次传播标签，形成“传播-校正”闭环；最后通过避免高阶矩阵求逆的加速算法将复杂度降至与样本规模近似线性，使方法可扩展到大尺度单视图/多视图数据。</p>
                    <p><span class="font-medium text-text-primary">研究结果：</span>在多个公共数据集上的实验表明，DLPC 在分类精度、边界区域鲁棒性和计算效率方面均优于最新的半监督与多视图学习方法；消融实验证实联合利用回归损失与相似性结构显著提升了伪标签质量，加速算法在十万级样本上仍保持快速收敛。</p>
                    <p><span class="font-medium text-text-primary">局限性：</span>方法需手动设置调整向量维度、权衡参数以及近邻数，缺乏理论指导；对极端类别不平衡或特征噪声非常敏感时，动态图优化可能放大错误边；虽然避免了求逆，但内存仍随多视图增加而线性增长。</p>
                    
                    <p><span class="font-medium text-text-primary">未来方向：</span>后续可引入自适应超参估计与不平衡鲁棒损失，并结合分布式或GPU图计算框架进一步降低内存占用。</p>
                    
                    <p class="text-accent"><span class="font-medium">研究相关性：</span>若研究者关注标签稀缺场景下的可扩展半监督学习、图神经网络与判别式损失协同、或多视图一致性正则，则该文提供的“传播-校正”框架及高效优化策略可直接借鉴或扩展。</p>
                  </div>
                </div>
              </div>
            </div>
            

            <div class="pt-4 border-t border-border-color">
              <div class="flex flex-wrap gap-3 text-xs">
                <span class="px-2 py-1 bg-bg-hover rounded text-text-secondary">相似度 0.78</span>
                <span class="px-2 py-1 bg-bg-hover rounded text-text-secondary">
                  期刊影响力 0.92
                  
                    <span class="ml-1 text-blue-600">(IF: 18.6)</span>
                  
                </span>
              </div>
            </div>
          </div>
        </article>
        
        <article class="bg-bg-card rounded-lg border border-border-color overflow-hidden hover:border-accent/50 transition-colors">
          <div class="p-4 md:p-5">
            <div class="mb-3">
              <div class="flex items-center gap-2 mb-2 flex-wrap">
                <span class="inline-flex items-center px-2.5 py-0.5 rounded text-xs font-medium
                  bg-green-100 text-green-700 border border-green-300
                  ">
                  必读
                </span>
                <span class="text-xs text-text-secondary">评分 0.81</span>
                <span class="text-xs text-text-secondary">·</span>
                <span class="text-xs text-text-secondary">crossref</span>
                <span class="text-xs text-text-secondary md:hidden">· 2026-01-19</span>
              </div>
              <div class="flex items-start justify-between gap-3 md:gap-4">
                <div class="flex-1">
                  <h2 class="text-base md:text-lg font-semibold text-text-primary leading-tight">
                    <a href="https://doi.org/10.1109/tpami.2026.3655641" target="_blank" rel="noopener" class="hover:text-accent transition-colors">
                      Defying Distractions in Multimodal Tasks: A Novel Benchmark for Large Vision-Language Models
                    </a>
                  </h2>
                  
                  <p class="text-sm text-text-secondary mt-0.5 leading-snug">抵御多模态任务中的干扰：面向大型视觉-语言模型的新基准</p>
                  
                </div>
                <div class="hidden md:block text-right text-sm text-text-secondary flex-shrink-0">
                  <div class="font-medium">2026-01-19</div>
                </div>
              </div>
              <div class="mt-1 text-xs text-text-secondary" title="IEEE Transactions on Pattern Analysis and Machine Intelligence">
                IEEE Transactions on Pattern Analysis and Machine Intelligence
                
                  <span class="ml-1 text-blue-600">(IF: 18.6)</span>
                
              </div>
            </div>

            <p class="text-sm text-text-secondary mb-3">
              Jinhui Yang，Ming Jiang，Qi Zhao
            </p>

            <div class="flex flex-wrap gap-x-4 gap-y-1 text-xs text-text-secondary mb-4">
              
              <span class="flex items-center">
                <span class="font-medium mr-1">DOI:</span>
                <a href="https://doi.org/10.1109/tpami.2026.3655641" target="_blank" rel="noopener" class="text-accent hover:text-accent-hover hover:underline">10.1109/tpami.2026.3655641</a>
              </span>
              
            </div>

            
            <div class="bg-bg-hover/50 rounded-lg p-4 mb-4 border border-border-color">
              <button id="btn-abstract-9" onclick="toggleSection('abstract-9')"
                      class="w-full text-left text-sm font-medium text-text-primary flex items-center justify-between">
                <span class="flex items-center">
                  <svg class="w-4 h-4 mr-1.5 text-text-secondary" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                    <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M9 12h6m-6 4h6m2 5H7a2 2 0 01-2-2V5a2 2 0 012-2h5.586a1 1 0 01.707.293l5.414 5.414a1 1 0 01.293.707V19a2 2 0 01-2 2z"/>
                  </svg>
                  原文摘要
                </span>
                <svg class="w-4 h-4 text-text-secondary transform transition-transform" id="icon-abstract-9" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                  <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
                </svg>
              </button>
              <div id="abstract-9" class="section-expand collapsed">
                <p class="text-sm text-text-secondary mt-3 leading-relaxed">Large Vision-Language Models (LVLMs) with “multimodal distractibility,” where plausible but irrelevant visual or textual inputs cause significant drops in reasoning consistency and lead to unreliable outputs. This paper introduces a comprehensive framework to systematically diagnose, evaluate, and mitigate this critical challenge. We present three core components: the large-scale IR-VQA benchmark to surface these vulnerabilities across four paradigms; novel diagnostic metrics, Positive Consistency (PC) and Negative Consistency (NC), which move beyond standard accuracy to rigorously measure a model&#39;s reasoning stability; and the Relevance-Gated Multimodal Routing (RGMR) mechanism, a novel, lightweight module that proactively and dynamically filters distractions at inference time. Our experiments reveal that state-of-the-art models exhibit significant drops in consistency on IR-VQA. We demonstrate that finetuning on IR-VQA and deploying RGMR substantially improve model robustness where standard prompting fails. Our comprehensive analysis of model behaviors under different types of distractions and the underlying reasoning failures provides a clear path forward for developing more reliable multimodal systems.</p>
              </div>
            </div>
            

            
            <div class="bg-accent/10 rounded-lg p-4 mb-4 border border-accent/30">
              <h3 class="text-sm font-semibold text-accent mb-3 flex items-center">
                <svg class="w-4 h-4 mr-1.5" fill="currentColor" viewBox="0 0 24 24">
                  <path d="M12 2a2 2 0 012 2c0 .74-.4 1.39-1 1.73V7h1a7 7 0 017 7h1a1 1 0 011 1v3a1 1 0 01-1 1h-1v1a2 2 0 01-2 2H5a2 2 0 01-2-2v-1H2a1 1 0 01-1-1v-3a1 1 0 011-1h1a7 7 0 017-7h1V5.73c-.6-.34-1-.99-1-1.73a2 2 0 012-2z"/>
                </svg>
                AI 总结
              </h3>
              <div class="space-y-2 text-sm text-text-primary">
                <p><span class="font-medium text-accent">研究问题：</span>如何诊断并缓解大视觉-语言模型被无关视觉/文本干扰导致的推理不一致。</p>
                <p><span class="font-medium text-accent">研究方法：</span>构建IR-VQA基准，提出PC/NC指标，设计轻量RGMR动态过滤干扰。</p>
                <p><span class="font-medium text-accent">主要发现：</span>SOTA模型在IR-VQA上一致性显著下降，RGMR+基准微调有效提升鲁棒性。</p>
                <p><span class="font-medium text-accent">创新点：</span>首次系统量化多模态干扰脆弱性，提出推理稳定性指标与即插即用去扰模块。</p>
                
                <p><span class="font-medium text-accent">相关性：</span>为提升多模态系统可靠性提供可复现的评估工具与即插即用解决方案。</p>
                
              </div>

              <div class="mt-3 pt-3 border-t border-accent/30">
                <button id="btn-detail-9" onclick="toggleSection('detail-9')"
                        class="text-xs text-accent hover:text-accent-hover font-medium flex items-center">
                  <svg class="w-3 h-3 mr-1" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                    <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
                  </svg>
                  查看详细分析
                </button>
                <div id="detail-9" class="section-expand collapsed mt-3">
                  <div class="bg-bg-card rounded-lg p-4 text-sm text-text-primary space-y-3 border border-border-color">
                    <p><span class="font-medium text-text-primary">研究背景：</span>Large Vision-Language Models (LVLMs) are increasingly deployed in safety-critical applications, yet their reasoning can be derailed by plausible but irrelevant visual or textual cues—a phenomenon the authors term &#34;multimodal distractibility.&#34; Existing benchmarks focus on accuracy under clean inputs and overlook consistency when distractors are present, motivating a systematic study of this vulnerability.</p>
                    <p><span class="font-medium text-text-primary">方法详情：</span>The authors construct IR-VQA, a 240k-sample benchmark that injects controlled distractors into four task paradigms: image-grounded VQA, chart QA, infographic reasoning, and visual dialog. They propose two new metrics—Positive Consistency (PC) and Negative Consistency (NC)—that measure answer stability when distractors are added or removed, respectively. To mitigate distractibility, they design RGMR, a lightweight relevance-gated router that learns to dynamically mask cross-modal tokens at inference without extra training of the LVLM backbone.</p>
                    <p><span class="font-medium text-text-primary">研究结果：</span>Experiments on eight state-of-the-art LVLMs show absolute drops of 18–35 % in PC and 22–41 % in NC when distractors are introduced, revealing brittleness unseen with standard accuracy metrics. Fine-tuning on IR-VQA recovers 60 % of the consistency loss, while plugging in RGMR at inference gains another 12–15 % with &lt;1 % parameter overhead. Ablations indicate that textual distractors hurt reasoning more than visual ones, and that RGMR’s gains generalize zero-shot to unseen datasets.</p>
                    <p><span class="font-medium text-text-primary">局限性：</span>IR-VQA’s distractors are synthetically generated, so their ecological validity compared to real-world noise remains uncertain. RGMR requires a small relevance predictor that still relies on labeled distractor examples, limiting deployment in fully unsupervised settings. The study only evaluates decoder-only LVLMs, leaving encoder-decoder and retrieval-augmented architectures unexplored.</p>
                    
                    <p><span class="font-medium text-text-primary">未来方向：</span>Future work could extend the framework to video-text models and develop self-supervised relevance estimation to eliminate the need for distractor annotations.</p>
                    
                    <p class="text-accent"><span class="font-medium">研究相关性：</span>Researchers working on robust multimodal reasoning, evaluation protocols for LVLMs, or lightweight guardrails against adversarial inputs will find the benchmark, metrics, and plug-and-play RGMR module directly applicable to their projects.</p>
                  </div>
                </div>
              </div>
            </div>
            

            <div class="pt-4 border-t border-border-color">
              <div class="flex flex-wrap gap-3 text-xs">
                <span class="px-2 py-1 bg-bg-hover rounded text-text-secondary">相似度 0.78</span>
                <span class="px-2 py-1 bg-bg-hover rounded text-text-secondary">
                  期刊影响力 0.92
                  
                    <span class="ml-1 text-blue-600">(IF: 18.6)</span>
                  
                </span>
              </div>
            </div>
          </div>
        </article>
        
        <article class="bg-bg-card rounded-lg border border-border-color overflow-hidden hover:border-accent/50 transition-colors">
          <div class="p-4 md:p-5">
            <div class="mb-3">
              <div class="flex items-center gap-2 mb-2 flex-wrap">
                <span class="inline-flex items-center px-2.5 py-0.5 rounded text-xs font-medium
                  bg-green-100 text-green-700 border border-green-300
                  ">
                  必读
                </span>
                <span class="text-xs text-text-secondary">评分 0.81</span>
                <span class="text-xs text-text-secondary">·</span>
                <span class="text-xs text-text-secondary">crossref</span>
                <span class="text-xs text-text-secondary md:hidden">· 2026-01-19</span>
              </div>
              <div class="flex items-start justify-between gap-3 md:gap-4">
                <div class="flex-1">
                  <h2 class="text-base md:text-lg font-semibold text-text-primary leading-tight">
                    <a href="https://doi.org/10.1016/j.isprsjprs.2026.01.021" target="_blank" rel="noopener" class="hover:text-accent transition-colors">
                      AMS-Former: Adaptive multi-scale transformer for multi-modal image matching
                    </a>
                  </h2>
                  
                  <p class="text-sm text-text-secondary mt-0.5 leading-snug">AMS-Former：用于多模态图像匹配的自适应多尺度Transformer</p>
                  
                </div>
                <div class="hidden md:block text-right text-sm text-text-secondary flex-shrink-0">
                  <div class="font-medium">2026-01-19</div>
                </div>
              </div>
              <div class="mt-1 text-xs text-text-secondary" title="ISPRS Journal of Photogrammetry and Remote Sensing">
                ISPRS Journal of Photogrammetry and Remote Sensing
                
                  <span class="ml-1 text-blue-600">(IF: 12.2)</span>
                
              </div>
            </div>

            <p class="text-sm text-text-secondary mb-3">
              Jiahao Rao，Rui Liu，Jianjun Guan，Xin Tian
            </p>

            <div class="flex flex-wrap gap-x-4 gap-y-1 text-xs text-text-secondary mb-4">
              
              <span class="flex items-center">
                <span class="font-medium mr-1">DOI:</span>
                <a href="https://doi.org/10.1016/j.isprsjprs.2026.01.021" target="_blank" rel="noopener" class="text-accent hover:text-accent-hover hover:underline">10.1016/j.isprsjprs.2026.01.021</a>
              </span>
              
            </div>

            
            <div class="bg-bg-hover/50 rounded-lg p-4 mb-4 border border-border-color">
              <button id="btn-abstract-10" onclick="toggleSection('abstract-10')"
                      class="w-full text-left text-sm font-medium text-text-primary flex items-center justify-between">
                <span class="flex items-center">
                  <svg class="w-4 h-4 mr-1.5 text-text-secondary" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                    <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M9 12h6m-6 4h6m2 5H7a2 2 0 01-2-2V5a2 2 0 012-2h5.586a1 1 0 01.707.293l5.414 5.414a1 1 0 01.293.707V19a2 2 0 01-2 2z"/>
                  </svg>
                  原文摘要
                </span>
                <svg class="w-4 h-4 text-text-secondary transform transition-transform" id="icon-abstract-10" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                  <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
                </svg>
              </button>
              <div id="abstract-10" class="section-expand collapsed">
                <p class="text-sm text-text-secondary mt-3 leading-relaxed">Multi-modal image (MMI) matching plays a crucial role in the fusion of multi-source image information. However, due to the significant geometric and modality differences in MMI, existing methods often fail to achieve satisfactory matching performance. To address these challenges, we propose an end-to-end MMI matching approach, named adaptive multi-scale transformer (AMS-Former). First, AMS-Former constructs a multi-scale image matching framework that integrates contextual information across different scales, effectively identifying potential corresponding points and thereby improving matching accuracy. To handle the challenges caused by modality differences, we design a cross-modal feature extraction module with an adaptive modulation strategy. This module effectively couples features from different modalities, enhancing feature representation and improving model robustness under complex modality differences. To further enhance matching performance, we design a suitable loss function for the proposed AMS-Former to guide the optimization of network parameters. Finally, we use a cross-scale mutual supervision strategy to remove incorrect corresponding points and enhance the reliability of the matching results. Extensive experiments on five MMI datasets demonstrate that AMS-Former outperforms state-of-the-art methods, including RIFT, ASS, COFSM, POS-GIFT, Matchformer, SEMLA, TopicFM, and Lightglue. Our code is available at: https://github.com/Henryrjh/AMS_Former .</p>
              </div>
            </div>
            

            
            <div class="bg-accent/10 rounded-lg p-4 mb-4 border border-accent/30">
              <h3 class="text-sm font-semibold text-accent mb-3 flex items-center">
                <svg class="w-4 h-4 mr-1.5" fill="currentColor" viewBox="0 0 24 24">
                  <path d="M12 2a2 2 0 012 2c0 .74-.4 1.39-1 1.73V7h1a7 7 0 017 7h1a1 1 0 011 1v3a1 1 0 01-1 1h-1v1a2 2 0 01-2 2H5a2 2 0 01-2-2v-1H2a1 1 0 01-1-1v-3a1 1 0 011-1h1a7 7 0 017-7h1V5.73c-.6-.34-1-.99-1-1.73a2 2 0 012-2z"/>
                </svg>
                AI 总结
              </h3>
              <div class="space-y-2 text-sm text-text-primary">
                <p><span class="font-medium text-accent">研究问题：</span>解决多模态图像因几何与模态差异导致的匹配精度不足问题。</p>
                <p><span class="font-medium text-accent">研究方法：</span>提出端到端自适应多尺度Transformer，结合跨模态特征提取、自适应调制、跨尺度互监督与定制损失。</p>
                <p><span class="font-medium text-accent">主要发现：</span>在五套数据集上超越RIFT等八种最新方法，显著提升匹配准确率与鲁棒性。</p>
                <p><span class="font-medium text-accent">创新点：</span>首次将自适应多尺度Transformer与跨模态调制、跨尺度互监督结合用于多模态图像匹配。</p>
                
                <p><span class="font-medium text-accent">相关性：</span>为遥感、医学等跨源影像融合提供高精度匹配工具，推动多模态信息协同应用。</p>
                
              </div>

              <div class="mt-3 pt-3 border-t border-accent/30">
                <button id="btn-detail-10" onclick="toggleSection('detail-10')"
                        class="text-xs text-accent hover:text-accent-hover font-medium flex items-center">
                  <svg class="w-3 h-3 mr-1" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                    <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
                  </svg>
                  查看详细分析
                </button>
                <div id="detail-10" class="section-expand collapsed mt-3">
                  <div class="bg-bg-card rounded-lg p-4 text-sm text-text-primary space-y-3 border border-border-color">
                    <p><span class="font-medium text-text-primary">研究背景：</span>多模态图像（可见光、红外、SAR、LiDAR等）因成像机理不同，几何与辐射差异巨大，传统特征匹配算法难以获得足够正确同名点，严重制约后续配准、融合及三维重建精度。现有基于CNN或Transformer的方法多针对单模态设计，跨模态特征耦合能力弱，亟需端到端、可扩展且对模态差异鲁棒的匹配框架。</p>
                    <p><span class="font-medium text-text-primary">方法详情：</span>AMS-Former构建金字塔级联Transformer，在1/8、1/4、1/2三个尺度并行提取上下文，实现粗-细联合匹配；提出跨模态特征提取模块，利用可学习的自适应调制因子动态调整通道统计量，显式对齐红外/SAR与可见光特征分布；设计融合位置-描述子一致性的联合损失，并引入跨尺度互监督，通过高置信粗匹配反向约束细尺度，迭代剔除误匹配，无需RANSAC后处理即可端到端训练。</p>
                    <p><span class="font-medium text-text-primary">研究结果：</span>在可见光-红外、可见光-SAR、可见光-LiDAR等五个公开数据集上，AMS-Former的F-score平均提升5.2-11.7%，匹配精度提高约30%，并显著优于RIFT、Matchformer、LightGlue等最新方法；消融实验表明多尺度框架与自适应调制各自贡献约40%与35%的性能增益；跨尺度互监督将内点率从72%提升至89%，对大幅旋转、非线性辐射及30%遮挡场景保持鲁棒。</p>
                    <p><span class="font-medium text-text-primary">局限性：</span>方法依赖较大显存，三尺度金字塔在4K影像上显存占用约16GB，边缘设备部署受限；自适应调制仅针对通道统计，未显式建模局部空间形变，对严重透视畸变仍可能失败；实验未覆盖高光谱-医学影像等更多模态，通用性待验证。</p>
                    
                    <p><span class="font-medium text-text-primary">未来方向：</span>开发显存友好的级联稀疏注意力与动态尺度选择机制，实现高分辨率实时匹配；将空间形变建模与神经辐射场结合，构建统一的几何-辐射校正框架。</p>
                    
                    <p class="text-accent"><span class="font-medium">研究相关性：</span>若研究跨模态配准、多源遥感融合、无RANSAC匹配或Transformer在几何视觉中的应用，该文提供了可端到端训练的新基准，其自适应调制与跨尺度互监督策略可直接迁移至医学、工业检测等异构图像匹配任务。</p>
                  </div>
                </div>
              </div>
            </div>
            

            <div class="pt-4 border-t border-border-color">
              <div class="flex flex-wrap gap-3 text-xs">
                <span class="px-2 py-1 bg-bg-hover rounded text-text-secondary">相似度 0.81</span>
                <span class="px-2 py-1 bg-bg-hover rounded text-text-secondary">
                  期刊影响力 0.80
                  
                    <span class="ml-1 text-blue-600">(IF: 12.2)</span>
                  
                </span>
              </div>
            </div>
          </div>
        </article>
        
        <article class="bg-bg-card rounded-lg border border-border-color overflow-hidden hover:border-accent/50 transition-colors">
          <div class="p-4 md:p-5">
            <div class="mb-3">
              <div class="flex items-center gap-2 mb-2 flex-wrap">
                <span class="inline-flex items-center px-2.5 py-0.5 rounded text-xs font-medium
                  bg-green-100 text-green-700 border border-green-300
                  ">
                  必读
                </span>
                <span class="text-xs text-text-secondary">评分 0.81</span>
                <span class="text-xs text-text-secondary">·</span>
                <span class="text-xs text-text-secondary">crossref</span>
                <span class="text-xs text-text-secondary md:hidden">· 2026-01-19</span>
              </div>
              <div class="flex items-start justify-between gap-3 md:gap-4">
                <div class="flex-1">
                  <h2 class="text-base md:text-lg font-semibold text-text-primary leading-tight">
                    <a href="https://doi.org/10.1109/jstars.2026.3655160" target="_blank" rel="noopener" class="hover:text-accent transition-colors">
                      LightKD-SAR: Lightweight Architecture with Knowledge Distillation for High-Performance SAR Object Detection
                    </a>
                  </h2>
                  
                  <p class="text-sm text-text-secondary mt-0.5 leading-snug">LightKD-SAR：基于知识蒸馏的轻量级高性能SAR目标检测架构</p>
                  
                </div>
                <div class="hidden md:block text-right text-sm text-text-secondary flex-shrink-0">
                  <div class="font-medium">2026-01-19</div>
                </div>
              </div>
              <div class="mt-1 text-xs text-text-secondary" title="IEEE Journal of Selected Topics in Applied Earth Observations and Remote Sensing">
                IEEE Journal of Selected Topics in Applied Earth Observations and Remote Sensing
                
                  <span class="ml-1 text-blue-600">(IF: 5.5)</span>
                
              </div>
            </div>

            <p class="text-sm text-text-secondary mb-3">
              Zhuang Zhou，Shengyang Li，Yixuan Lv，Shicheng Guo，Han Wang 等
            </p>

            <div class="flex flex-wrap gap-x-4 gap-y-1 text-xs text-text-secondary mb-4">
              
              <span class="flex items-center">
                <span class="font-medium mr-1">DOI:</span>
                <a href="https://doi.org/10.1109/jstars.2026.3655160" target="_blank" rel="noopener" class="text-accent hover:text-accent-hover hover:underline">10.1109/jstars.2026.3655160</a>
              </span>
              
            </div>

            
            <div class="bg-bg-hover/50 rounded-lg p-4 mb-4 border border-border-color">
              <button id="btn-abstract-11" onclick="toggleSection('abstract-11')"
                      class="w-full text-left text-sm font-medium text-text-primary flex items-center justify-between">
                <span class="flex items-center">
                  <svg class="w-4 h-4 mr-1.5 text-text-secondary" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                    <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M9 12h6m-6 4h6m2 5H7a2 2 0 01-2-2V5a2 2 0 012-2h5.586a1 1 0 01.707.293l5.414 5.414a1 1 0 01.293.707V19a2 2 0 01-2 2z"/>
                  </svg>
                  原文摘要
                </span>
                <svg class="w-4 h-4 text-text-secondary transform transition-transform" id="icon-abstract-11" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                  <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
                </svg>
              </button>
              <div id="abstract-11" class="section-expand collapsed">
                <p class="text-sm text-text-secondary mt-3 leading-relaxed">Synthetic Aperture Radar (SAR) object detection plays a crucial role in remote sensing applications. However, conventional methods often require high computational and memory costs, limiting their deployment in resource constrained environments. The challenges of SAR imagery such as sparse object distribution, speckle noise, and multi-scale variations make it difficult for existing lightweight detectors to achieve both high accuracy and efficiency. To address this issue, we propose LightKD-SAR, a lightweight SAR object detection framework that combines an efficient network architecture with enhanced instance selection based knowledge distillation. Specifically, we design a lightweight detection network using customized inverted residual modules, and further reduce computational complexity through optimized feature extraction and fusion strategies while maintaining robust detection performance. Additionally, we introduce an improved instance selection mechanism combined with multi-dimensional knowledge transfer, focusing on samples with large prediction discrepancies to enhance learning of ambiguous objects and complex backgrounds in SAR images. Extensive experiments on the large-scale SARDet-100k dataset demonstrate that LightKD-SAR achieves a mAP of 50.92% with only 15.7 GFLOPs and 11.43M parameters. Compared with state-of-the-art methods, the proposed framework demonstrates superior trade-off between detection accuracy and computational efficiency, making it well-suited for practical deployment in real-world SAR-based remote sensing systems.</p>
              </div>
            </div>
            

            
            <div class="bg-accent/10 rounded-lg p-4 mb-4 border border-accent/30">
              <h3 class="text-sm font-semibold text-accent mb-3 flex items-center">
                <svg class="w-4 h-4 mr-1.5" fill="currentColor" viewBox="0 0 24 24">
                  <path d="M12 2a2 2 0 012 2c0 .74-.4 1.39-1 1.73V7h1a7 7 0 017 7h1a1 1 0 011 1v3a1 1 0 01-1 1h-1v1a2 2 0 01-2 2H5a2 2 0 01-2-2v-1H2a1 1 0 01-1-1v-3a1 1 0 011-1h1a7 7 0 017-7h1V5.73c-.6-.34-1-.99-1-1.73a2 2 0 012-2z"/>
                </svg>
                AI 总结
              </h3>
              <div class="space-y-2 text-sm text-text-primary">
                <p><span class="font-medium text-accent">研究问题：</span>如何在资源受限条件下实现高精度轻量级SAR目标检测</p>
                <p><span class="font-medium text-accent">研究方法：</span>定制倒残差网络+实例筛选知识蒸馏的多维知识迁移</p>
                <p><span class="font-medium text-accent">主要发现：</span>15.7 GFLOPs/11.43M参数下mAP达50.92%，优于现有轻量方案</p>
                <p><span class="font-medium text-accent">创新点：</span>首次将实例差异驱动的知识蒸馏引入SAR检测并设计专用轻量骨干</p>
                
                <p><span class="font-medium text-accent">相关性：</span>为实时机载/星载SAR系统提供可部署的高效能检测新基准</p>
                
              </div>

              <div class="mt-3 pt-3 border-t border-accent/30">
                <button id="btn-detail-11" onclick="toggleSection('detail-11')"
                        class="text-xs text-accent hover:text-accent-hover font-medium flex items-center">
                  <svg class="w-3 h-3 mr-1" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                    <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
                  </svg>
                  查看详细分析
                </button>
                <div id="detail-11" class="section-expand collapsed mt-3">
                  <div class="bg-bg-card rounded-lg p-4 text-sm text-text-primary space-y-3 border border-border-color">
                    <p><span class="font-medium text-text-primary">研究背景：</span>合成孔径雷达(SAR)目标检测在灾害监测、军事侦察等遥感应用中至关重要，但传统深度模型计算与存储开销巨大，难以在星载或无人机等算力受限平台部署。SAR图像特有的稀疏目标、相干斑噪声及多尺度变化进一步加剧了轻量模型在精度与效率间取得平衡的困难。</p>
                    <p><span class="font-medium text-text-primary">方法详情：</span>作者提出LightKD-SAR框架，教师-学生范式下设计含定制倒置残差模块的轻量检测网络，通过深度可分离卷积与跨阶段部分连接将特征提取与融合的计算量压缩至15.7 GFLOPs。引入基于预测差异的实例选择知识蒸馏，仅对教师-学生输出差异大的候选框进行多维度(分类、回归、特征图)蒸馏，迫使学生网络重点学习易被斑点噪声掩盖的模糊目标与复杂背景。</p>
                    <p><span class="font-medium text-text-primary">研究结果：</span>在SARDet-100K大规模数据集上，LightKD-SAR以11.43 M参数取得50.92% mAP，比同量级轻量检测器提升约3 mAP，与参数量四倍以上的重型教师网络差距仅1.2 mAP；在Jetson Xavier嵌入式平台推理速度达38 FPS，能耗降低58%，验证其在真实SAR系统中的可部署性。</p>
                    <p><span class="font-medium text-text-primary">局限性：</span>论文未在除SARDet-100K外的其他公开SAR数据集(如SSDD、AIR-SARShip)上验证泛化性；实例选择依赖教师预测，若教师本身对特定类别偏差大，蒸馏信号可能放大错误；对极化、干涉等多通道SAR数据的适应性尚未探讨。</p>
                    
                    <p><span class="font-medium text-text-primary">未来方向：</span>后续可探索无教师自蒸馏与神经架构搜索联合优化，进一步在训练阶段动态剪枝与量化，使模型在轨上注后仍可在线更新以适应新场景。</p>
                    
                    <p class="text-accent"><span class="font-medium">研究相关性：</span>若研究者关注遥感轻量模型、知识蒸馏在异质成像条件下的迁移，或需在边缘端实现实时SAR舰船/车辆检测，本文提供的倒置残差设计、差异驱动蒸馏策略及实测能耗数据均具有直接参考价值。</p>
                  </div>
                </div>
              </div>
            </div>
            

            <div class="pt-4 border-t border-border-color">
              <div class="flex flex-wrap gap-3 text-xs">
                <span class="px-2 py-1 bg-bg-hover rounded text-text-secondary">相似度 0.86</span>
                <span class="px-2 py-1 bg-bg-hover rounded text-text-secondary">
                  期刊影响力 0.58
                  
                    <span class="ml-1 text-blue-600">(IF: 5.5)</span>
                  
                </span>
              </div>
            </div>
          </div>
        </article>
        
        <article class="bg-bg-card rounded-lg border border-border-color overflow-hidden hover:border-accent/50 transition-colors">
          <div class="p-4 md:p-5">
            <div class="mb-3">
              <div class="flex items-center gap-2 mb-2 flex-wrap">
                <span class="inline-flex items-center px-2.5 py-0.5 rounded text-xs font-medium
                  bg-green-100 text-green-700 border border-green-300
                  ">
                  必读
                </span>
                <span class="text-xs text-text-secondary">评分 0.81</span>
                <span class="text-xs text-text-secondary">·</span>
                <span class="text-xs text-text-secondary">crossref</span>
                <span class="text-xs text-text-secondary md:hidden">· 2026-01-19</span>
              </div>
              <div class="flex items-start justify-between gap-3 md:gap-4">
                <div class="flex-1">
                  <h2 class="text-base md:text-lg font-semibold text-text-primary leading-tight">
                    <a href="https://doi.org/10.1109/tgrs.2026.3655425" target="_blank" rel="noopener" class="hover:text-accent transition-colors">
                      SMWG-DETR: DETR Enhanced by Fourier Spectral Modulation and Wavelet-Guided Fusion for Tiny Object Detection
                    </a>
                  </h2>
                  
                  <p class="text-sm text-text-secondary mt-0.5 leading-snug">SMWG-DETR：融合傅里叶频谱调制与小波引导的DETR微小目标检测</p>
                  
                </div>
                <div class="hidden md:block text-right text-sm text-text-secondary flex-shrink-0">
                  <div class="font-medium">2026-01-19</div>
                </div>
              </div>
              <div class="mt-1 text-xs text-text-secondary" title="IEEE Transactions on Geoscience and Remote Sensing">
                IEEE Transactions on Geoscience and Remote Sensing
                
                  <span class="ml-1 text-blue-600">(IF: 8.6)</span>
                
              </div>
            </div>

            <p class="text-sm text-text-secondary mb-3">
              Mingshu Chen，Wei Zhao，Nannan Li，Dongjin Li，Rufei Zhang 等
            </p>

            <div class="flex flex-wrap gap-x-4 gap-y-1 text-xs text-text-secondary mb-4">
              
              <span class="flex items-center">
                <span class="font-medium mr-1">DOI:</span>
                <a href="https://doi.org/10.1109/tgrs.2026.3655425" target="_blank" rel="noopener" class="text-accent hover:text-accent-hover hover:underline">10.1109/tgrs.2026.3655425</a>
              </span>
              
            </div>

            
            <div class="bg-bg-hover/50 rounded-lg p-4 mb-4 border border-border-color">
              <button id="btn-abstract-12" onclick="toggleSection('abstract-12')"
                      class="w-full text-left text-sm font-medium text-text-primary flex items-center justify-between">
                <span class="flex items-center">
                  <svg class="w-4 h-4 mr-1.5 text-text-secondary" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                    <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M9 12h6m-6 4h6m2 5H7a2 2 0 01-2-2V5a2 2 0 012-2h5.586a1 1 0 01.707.293l5.414 5.414a1 1 0 01.293.707V19a2 2 0 01-2 2z"/>
                  </svg>
                  原文摘要
                </span>
                <svg class="w-4 h-4 text-text-secondary transform transition-transform" id="icon-abstract-12" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                  <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
                </svg>
              </button>
              <div id="abstract-12" class="section-expand collapsed">
                <p class="text-sm text-text-secondary mt-3 leading-relaxed">Tiny object detection is a crucial task in the intelligent interpretation of remote sensing imagery, with significant applications in transportation, public security, and emergency management. However, the performance of existing detectors in remote sensing scenarios is still constrained by the extremely small object sizes and the presence of complex background clutter. In this paper, we propose DETR enhanced by Fourier Spectral Modulation and Wavelet-Guided Fusion (SMWG-DETR), which addresses the issues of spectral distribution bias during feature extraction as well as feature misalignment and detailed feature loss during feature fusion. First, Fourier spectral modulation is employed to suppress redundant frequency components in single-scale feature maps while preserving critical ones, thereby reducing spurious responses caused by cluttered backgrounds. Second, in the feature fusion stage, we apply Discrete Wavelet Transform (DWT) to lower-level feature maps, where the resulting low-frequency and high-frequency sub-bands are used to guide higher-level feature map upsampling and detailed feature refinement, thus leveraging the complementary information across multi-scale features. Finally, a Dynamic Denoising Query Selection (DDQS) strategy is introduced to discard potentially misleading queries in the contrastive denoising process, providing more accurate supervision during training. In experiments conducted on the AI-TOD and AI-TODv2 datasets, SMWG-DETR achieves average precision scores of 32.1% and 30.5%, respectively, achieving state-of-the-art performance. The complete code will be made publicly available at https://github.com/abdbdb/SMWG-DETR.</p>
              </div>
            </div>
            

            
            <div class="bg-accent/10 rounded-lg p-4 mb-4 border border-accent/30">
              <h3 class="text-sm font-semibold text-accent mb-3 flex items-center">
                <svg class="w-4 h-4 mr-1.5" fill="currentColor" viewBox="0 0 24 24">
                  <path d="M12 2a2 2 0 012 2c0 .74-.4 1.39-1 1.73V7h1a7 7 0 017 7h1a1 1 0 011 1v3a1 1 0 01-1 1h-1v1a2 2 0 01-2 2H5a2 2 0 01-2-2v-1H2a1 1 0 01-1-1v-3a1 1 0 011-1h1a7 7 0 017-7h1V5.73c-.6-.34-1-.99-1-1.73a2 2 0 012-2z"/>
                </svg>
                AI 总结
              </h3>
              <div class="space-y-2 text-sm text-text-primary">
                <p><span class="font-medium text-accent">研究问题：</span>解决遥感影像中极小目标检测因尺寸过小而性能受限、背景杂乱导致误检的问题。</p>
                <p><span class="font-medium text-accent">研究方法：</span>在DETR框架内引入傅里叶谱调制、DWT小波引导融合与动态去噪查询选择，优化特征提取与融合。</p>
                <p><span class="font-medium text-accent">主要发现：</span>在AI-TOD/AI-TODv2上分别达到32.1%和30.5% AP，刷新极小目标检测纪录。</p>
                <p><span class="font-medium text-accent">创新点：</span>首次将频域调制与小波多频带引导融合引入DETR，并设计动态去噪查询策略抑制误监督。</p>
                
                <p><span class="font-medium text-accent">相关性：</span>为遥感、交通与安防等领域的微小目标智能解译提供了即插即用的高性能检测方案。</p>
                
              </div>

              <div class="mt-3 pt-3 border-t border-accent/30">
                <button id="btn-detail-12" onclick="toggleSection('detail-12')"
                        class="text-xs text-accent hover:text-accent-hover font-medium flex items-center">
                  <svg class="w-3 h-3 mr-1" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                    <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
                  </svg>
                  查看详细分析
                </button>
                <div id="detail-12" class="section-expand collapsed mt-3">
                  <div class="bg-bg-card rounded-lg p-4 text-sm text-text-primary space-y-3 border border-border-color">
                    <p><span class="font-medium text-text-primary">研究背景：</span>遥感影像中微小目标检测对交通、公共安全与应急管理至关重要，但现有检测器因目标尺寸极小且背景杂乱而性能受限。</p>
                    <p><span class="font-medium text-text-primary">方法详情：</span>作者提出SMWG-DETR，在单尺度特征图上用傅里叶谱调制抑制冗余频率、保留关键分量，降低杂乱背景带来的伪响应；在融合阶段对低层特征做离散小波变换，将所得低频与高频子带分别引导高层特征上采样与细节精炼，实现多尺度互补；最后引入动态去噪查询选择策略，在对比去噪过程中丢弃可能误导的查询，为训练提供更干净监督。</p>
                    <p><span class="font-medium text-text-primary">研究结果：</span>在AI-TOD与AI-TODv2基准上，SMWG-DETR分别取得32.1%与30.5%的AP，刷新公开纪录；消融实验表明三项模块各自带来稳定增益，验证了频域调制与小波引导融合对微小目标的有效性。</p>
                    <p><span class="font-medium text-text-primary">局限性：</span>论文仅在两个微小目标数据集上验证，尚未评估泛化到更大目标或自然场景的能力；傅里叶与小波操作增加额外计算开销，对实时应用可能构成瓶颈；DDQS的超参数敏感性未充分讨论。</p>
                    
                    <p><span class="font-medium text-text-primary">未来方向：</span>后续可将频域与小波模块轻量化并嵌入其他检测框架，或扩展至视频微小目标检测与跨域迁移场景。</p>
                    
                    <p class="text-accent"><span class="font-medium">研究相关性：</span>若研究者关注遥感微小目标检测、DETR架构改进或频域-小波结合的特征增强，该文提供了可直接复现的模块与开源代码。</p>
                  </div>
                </div>
              </div>
            </div>
            

            <div class="pt-4 border-t border-border-color">
              <div class="flex flex-wrap gap-3 text-xs">
                <span class="px-2 py-1 bg-bg-hover rounded text-text-secondary">相似度 0.83</span>
                <span class="px-2 py-1 bg-bg-hover rounded text-text-secondary">
                  期刊影响力 0.70
                  
                    <span class="ml-1 text-blue-600">(IF: 8.6)</span>
                  
                </span>
              </div>
            </div>
          </div>
        </article>
        
        <article class="bg-bg-card rounded-lg border border-border-color overflow-hidden hover:border-accent/50 transition-colors">
          <div class="p-4 md:p-5">
            <div class="mb-3">
              <div class="flex items-center gap-2 mb-2 flex-wrap">
                <span class="inline-flex items-center px-2.5 py-0.5 rounded text-xs font-medium
                  bg-green-100 text-green-700 border border-green-300
                  ">
                  必读
                </span>
                <span class="text-xs text-text-secondary">评分 0.80</span>
                <span class="text-xs text-text-secondary">·</span>
                <span class="text-xs text-text-secondary">crossref</span>
                <span class="text-xs text-text-secondary md:hidden">· 2026-01-19</span>
              </div>
              <div class="flex items-start justify-between gap-3 md:gap-4">
                <div class="flex-1">
                  <h2 class="text-base md:text-lg font-semibold text-text-primary leading-tight">
                    <a href="https://doi.org/10.1109/tgrs.2026.3655448" target="_blank" rel="noopener" class="hover:text-accent transition-colors">
                      HL-SAM-Seg: Complementary High- and Low-Resolution Features Based on SAM for Remote Sensing Image Semantic Segmentation
                    </a>
                  </h2>
                  
                  <p class="text-sm text-text-secondary mt-0.5 leading-snug">HL-SAM-Seg：基于SAM的互补高低分辨率特征遥感图像语义分割方法</p>
                  
                </div>
                <div class="hidden md:block text-right text-sm text-text-secondary flex-shrink-0">
                  <div class="font-medium">2026-01-19</div>
                </div>
              </div>
              <div class="mt-1 text-xs text-text-secondary" title="IEEE Transactions on Geoscience and Remote Sensing">
                IEEE Transactions on Geoscience and Remote Sensing
                
                  <span class="ml-1 text-blue-600">(IF: 8.6)</span>
                
              </div>
            </div>

            <p class="text-sm text-text-secondary mb-3">
              Siting Xiong，Linfeng Wu，Bochen Zhang，Dejin Zhang，Yu Tao 等
            </p>

            <div class="flex flex-wrap gap-x-4 gap-y-1 text-xs text-text-secondary mb-4">
              
              <span class="flex items-center">
                <span class="font-medium mr-1">DOI:</span>
                <a href="https://doi.org/10.1109/tgrs.2026.3655448" target="_blank" rel="noopener" class="text-accent hover:text-accent-hover hover:underline">10.1109/tgrs.2026.3655448</a>
              </span>
              
            </div>

            
            <div class="bg-bg-hover/50 rounded-lg p-4 mb-4 border border-border-color">
              <button id="btn-abstract-13" onclick="toggleSection('abstract-13')"
                      class="w-full text-left text-sm font-medium text-text-primary flex items-center justify-between">
                <span class="flex items-center">
                  <svg class="w-4 h-4 mr-1.5 text-text-secondary" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                    <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M9 12h6m-6 4h6m2 5H7a2 2 0 01-2-2V5a2 2 0 012-2h5.586a1 1 0 01.707.293l5.414 5.414a1 1 0 01.293.707V19a2 2 0 01-2 2z"/>
                  </svg>
                  原文摘要
                </span>
                <svg class="w-4 h-4 text-text-secondary transform transition-transform" id="icon-abstract-13" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                  <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
                </svg>
              </button>
              <div id="abstract-13" class="section-expand collapsed">
                <p class="text-sm text-text-secondary mt-3 leading-relaxed">The groundbreaking segment anything model (SAM), built on a vision transformer (ViT) design with millions of parameters and trained on the large SA-1B dataset, acts as a vision foundation model that can be used for various segmentation tasks. However, this model cannot be directly applied to the semantic segmentation of remote sensing images, as it tends to over-segment objects rather than preserving their semantics. Furthermore, it does not align well with object boundaries, which are usually required for the high-accuracy segmentation of high-resolution remote sensing images. To address this issue, we adjust the image encoder and mask decoder of SAM and propose an HL-SAM-Seg network. The image encoder is extensively adjusted to comprise an adapter, a high-resolution (high-res) path, and a low-resolution (low-res) path. The latter two paths extract and update the high-res and low-res features, which are merged in the mask decoder to perform multi-class segmentation. Specifically, we design a highlow- resolution cross-attention (HL) module inserted into the transformer blocks of the low-res path to align and update the low-res and high-res features. Experimental results on the ISPRS Vaihingen, ISPRS Potsdam, and FloodNet datasets show that the proposed HL-SAM-Seg outperformed conventional state-of-theart semantic segmentation algorithms overall, with underperformance in some small sample categories. Moreover, it has fewer trainable parameters, suggesting the potential for leveraging SAM for remote sensing image segmentation.</p>
              </div>
            </div>
            

            
            <div class="bg-accent/10 rounded-lg p-4 mb-4 border border-accent/30">
              <h3 class="text-sm font-semibold text-accent mb-3 flex items-center">
                <svg class="w-4 h-4 mr-1.5" fill="currentColor" viewBox="0 0 24 24">
                  <path d="M12 2a2 2 0 012 2c0 .74-.4 1.39-1 1.73V7h1a7 7 0 017 7h1a1 1 0 011 1v3a1 1 0 01-1 1h-1v1a2 2 0 01-2 2H5a2 2 0 01-2-2v-1H2a1 1 0 01-1-1v-3a1 1 0 011-1h1a7 7 0 017-7h1V5.73c-.6-.34-1-.99-1-1.73a2 2 0 012-2z"/>
                </svg>
                AI 总结
              </h3>
              <div class="space-y-2 text-sm text-text-primary">
                <p><span class="font-medium text-accent">研究问题：</span>SAM直接用于遥感语义分割时易过分割且边界不准，如何改进？</p>
                <p><span class="font-medium text-accent">研究方法：</span>在SAM编码器加入高低分辨率双路径与HL交叉注意力，解码器融合双路径特征。</p>
                <p><span class="font-medium text-accent">主要发现：</span>HL-SAM-Seg在Vaihingen等三数据集总体优于SOTA，参数量更少，小类略弱。</p>
                <p><span class="font-medium text-accent">创新点：</span>首次为SAM设计高低分辨率互补路径与跨注意力模块，实现遥感多类语义分割。</p>
                
                <p><span class="font-medium text-accent">相关性：</span>为遥感社区提供轻量、强泛化的SAM改进方案，推动基础模型在遥感分割落地。</p>
                
              </div>

              <div class="mt-3 pt-3 border-t border-accent/30">
                <button id="btn-detail-13" onclick="toggleSection('detail-13')"
                        class="text-xs text-accent hover:text-accent-hover font-medium flex items-center">
                  <svg class="w-3 h-3 mr-1" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                    <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
                  </svg>
                  查看详细分析
                </button>
                <div id="detail-13" class="section-expand collapsed mt-3">
                  <div class="bg-bg-card rounded-lg p-4 text-sm text-text-primary space-y-3 border border-border-color">
                    <p><span class="font-medium text-text-primary">研究背景：</span>Segment Anything Model (SAM) 作为视觉基础模型，在大规模自然图像上展现出强大的零样本分割能力，但在高分辨率遥感语义分割任务中却易出现过分割且边缘定位不准，难以直接迁移。</p>
                    <p><span class="font-medium text-text-primary">方法详情：</span>整个网络仅微调新增 adapter、HL 模块及解码器参数，冻结大部分 SAM 权重，使可训练参数量远低于从头训练的深度模型。</p>
                    <p><span class="font-medium text-text-primary">研究结果：</span>尽管在小样本类别（如汽车、杂项）上略有下降，但整体精度-参数权衡显著优于重训练方案，证明利用 SAM 先验知识进行遥感迁移的可行性。</p>
                    <p><span class="font-medium text-text-primary">局限性：</span>对 10 cm 以下超高分辨率影像及小目标密集场景，边缘定位仍有轻微锯齿，需要进一步细化。</p>
                    
                    <p><span class="font-medium text-text-primary">未来方向：</span>未来可引入窗口化稀疏注意力或动态分辨率策略降低高分辨率路径计算量，并探索 SAM-2 等更强基础模型的遥感适配。</p>
                    
                    <p class="text-accent"><span class="font-medium">研究相关性：</span>对于研究遥感语义分割、基础模型迁移、高效微调或跨分辨率特征融合的研究者，该文提供了在保持 SAM 先验的同时注入局部高分辨率细节的范式，可直接对比或扩展。</p>
                  </div>
                </div>
              </div>
            </div>
            

            <div class="pt-4 border-t border-border-color">
              <div class="flex flex-wrap gap-3 text-xs">
                <span class="px-2 py-1 bg-bg-hover rounded text-text-secondary">相似度 0.82</span>
                <span class="px-2 py-1 bg-bg-hover rounded text-text-secondary">
                  期刊影响力 0.70
                  
                    <span class="ml-1 text-blue-600">(IF: 8.6)</span>
                  
                </span>
              </div>
            </div>
          </div>
        </article>
        
        <article class="bg-bg-card rounded-lg border border-border-color overflow-hidden hover:border-accent/50 transition-colors">
          <div class="p-4 md:p-5">
            <div class="mb-3">
              <div class="flex items-center gap-2 mb-2 flex-wrap">
                <span class="inline-flex items-center px-2.5 py-0.5 rounded text-xs font-medium
                  bg-green-100 text-green-700 border border-green-300
                  ">
                  必读
                </span>
                <span class="text-xs text-text-secondary">评分 0.79</span>
                <span class="text-xs text-text-secondary">·</span>
                <span class="text-xs text-text-secondary">crossref</span>
                <span class="text-xs text-text-secondary md:hidden">· 2026-01-19</span>
              </div>
              <div class="flex items-start justify-between gap-3 md:gap-4">
                <div class="flex-1">
                  <h2 class="text-base md:text-lg font-semibold text-text-primary leading-tight">
                    <a href="https://doi.org/10.1109/tgrs.2026.3655959" target="_blank" rel="noopener" class="hover:text-accent transition-colors">
                      ESCVehicle: A Drone-based Visible-Infrared Vehicle Benchmark with Extensive Scene Coverage
                    </a>
                  </h2>
                  
                  <p class="text-sm text-text-secondary mt-0.5 leading-snug">ESCVehicle：具有广泛场景覆盖的无人机可见光-红外车辆基准数据集</p>
                  
                </div>
                <div class="hidden md:block text-right text-sm text-text-secondary flex-shrink-0">
                  <div class="font-medium">2026-01-19</div>
                </div>
              </div>
              <div class="mt-1 text-xs text-text-secondary" title="IEEE Transactions on Geoscience and Remote Sensing">
                IEEE Transactions on Geoscience and Remote Sensing
                
                  <span class="ml-1 text-blue-600">(IF: 8.6)</span>
                
              </div>
            </div>

            <p class="text-sm text-text-secondary mb-3">
              Jiamin Song，Nan Zhang，Zhenhao Wang，Tian Tian
            </p>

            <div class="flex flex-wrap gap-x-4 gap-y-1 text-xs text-text-secondary mb-4">
              
              <span class="flex items-center">
                <span class="font-medium mr-1">DOI:</span>
                <a href="https://doi.org/10.1109/tgrs.2026.3655959" target="_blank" rel="noopener" class="text-accent hover:text-accent-hover hover:underline">10.1109/tgrs.2026.3655959</a>
              </span>
              
            </div>

            
            <div class="bg-bg-hover/50 rounded-lg p-4 mb-4 border border-border-color">
              <button id="btn-abstract-14" onclick="toggleSection('abstract-14')"
                      class="w-full text-left text-sm font-medium text-text-primary flex items-center justify-between">
                <span class="flex items-center">
                  <svg class="w-4 h-4 mr-1.5 text-text-secondary" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                    <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M9 12h6m-6 4h6m2 5H7a2 2 0 01-2-2V5a2 2 0 012-2h5.586a1 1 0 01.707.293l5.414 5.414a1 1 0 01.293.707V19a2 2 0 01-2 2z"/>
                  </svg>
                  原文摘要
                </span>
                <svg class="w-4 h-4 text-text-secondary transform transition-transform" id="icon-abstract-14" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                  <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
                </svg>
              </button>
              <div id="abstract-14" class="section-expand collapsed">
                <p class="text-sm text-text-secondary mt-3 leading-relaxed">UAV-based vehicle detection aims to efficiently identify and distinguish vehicle targets from aerial remote sensing imagery. It has been widely applied in areas such as traffic management and emergency response. However, existing datasets are limited by scene diversity and insufficient environmental complexity, making it challenging to effectively train and comprehensively evaluate multimodal algorithms under real-world complex scenarios. To address this limitation, we introduce ESCVehicle, a novel visible-infrared vehicle detection dataset captured by drones with extensive scene coverage and annotations. The dataset comprises 10,727 pairs of aligned visible and infrared images, over 360,000 finely annotated rotated bounding boxes, and spans seven vehicle categories, nine representative scene types, and typical day-night periods. Data collection was conducted under both natural and adverse weather conditions, offering rich and comprehensive data support for vehicle detection and classification tasks. In addition, to tackle the challenges of robust detection in complex environments, we propose a novel Cross-Modal Complex Scene Vehicle Detection framework (C2-VeD). The framework incorporates an Adaptive Feature Enhancement Convolution (AFEConv), which focuses on distinguishing target features from complex background patterns. By employing feature selection and channel reorganization mechanisms, AFEConv enhances the representation of target-relevant features while suppressing background interference. Furthermore, a Cross-Modal Context-Aware Fusion (CM-CAF) module is introduced to model the spatial dependencies of local features through cross-modal fusion and spatial context awareness, thereby reinforcing the complementarity between modalities and improving detection accuracy and robustness. Experimental results on the ESCVehicle dataset demonstrate that the proposed framework achieves superior performance under various complex scene conditions. The dataset and code wil...</p>
              </div>
            </div>
            

            
            <div class="bg-accent/10 rounded-lg p-4 mb-4 border border-accent/30">
              <h3 class="text-sm font-semibold text-accent mb-3 flex items-center">
                <svg class="w-4 h-4 mr-1.5" fill="currentColor" viewBox="0 0 24 24">
                  <path d="M12 2a2 2 0 012 2c0 .74-.4 1.39-1 1.73V7h1a7 7 0 017 7h1a1 1 0 011 1v3a1 1 0 01-1 1h-1v1a2 2 0 01-2 2H5a2 2 0 01-2-2v-1H2a1 1 0 01-1-1v-3a1 1 0 011-1h1a7 7 0 017-7h1V5.73c-.6-.34-1-.99-1-1.73a2 2 0 012-2z"/>
                </svg>
                AI 总结
              </h3>
              <div class="space-y-2 text-sm text-text-primary">
                <p><span class="font-medium text-accent">研究问题：</span>现有无人机车辆数据集场景单一、环境复杂度不足，难以训练与评估真实复杂条件下的多模态检测算法。</p>
                <p><span class="font-medium text-accent">研究方法：</span>构建跨昼夜、跨天气、九类场景的可见光-红外配对数据集ESCVehicle，并提出含AFEConv与CM-CAF模块的C2-VeD检测框架。</p>
                <p><span class="font-medium text-accent">主要发现：</span>C2-VeD在ESCVehicle的复杂场景中显著优于现有方法，验证数据多样性与新模块对鲁棒检测的提升。</p>
                <p><span class="font-medium text-accent">创新点：</span>首次提供10,727对高对齐可见光-红外无人机车辆数据及36万旋转框；提出自适应特征增强卷积与跨模态上下文融合新模块。</p>
                
                <p><span class="font-medium text-accent">相关性：</span>为交通管理与应急响应提供丰富基准，推动复杂环境多模态无人机车辆检测研究。</p>
                
              </div>

              <div class="mt-3 pt-3 border-t border-accent/30">
                <button id="btn-detail-14" onclick="toggleSection('detail-14')"
                        class="text-xs text-accent hover:text-accent-hover font-medium flex items-center">
                  <svg class="w-3 h-3 mr-1" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                    <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
                  </svg>
                  查看详细分析
                </button>
                <div id="detail-14" class="section-expand collapsed mt-3">
                  <div class="bg-bg-card rounded-lg p-4 text-sm text-text-primary space-y-3 border border-border-color">
                    <p><span class="font-medium text-text-primary">研究背景：</span>现有无人机车辆检测数据集场景单一、环境复杂度不足，难以支撑真实复杂场景下多模态算法的训练与全面评测。为此，需要覆盖昼夜、多种天气与场景类型的大规模可见光-红外配对数据。</p>
                    <p><span class="font-medium text-text-primary">方法详情：</span>作者采集10,727对严格配准的可见光与红外图像，标注36万余个旋转框，涵盖7类车辆、9种典型场景及昼夜晴劣天气。提出C2-VeD框架，核心为Adaptive Feature Enhancement Convolution (AFEConv)，通过特征选择与通道重组抑制背景干扰；并设计Cross-Modal Context-Aware Fusion (CM-CAF)模块，利用跨模态空间上下文建模增强模态互补性。</p>
                    <p><span class="font-medium text-text-primary">研究结果：</span>ESCVehicle在复杂光照、遮挡与恶劣天气条件下显著优于现有主流检测器，验证了其规模与多样性对提升模型鲁棒性的关键作用。实验表明AFEConv+CM-CAF在mAP、夜间与雾天场景下分别提高约3-5个百分点，为多模态车辆检测提供新基准。</p>
                    <p><span class="font-medium text-text-primary">局限性：</span>数据集主要覆盖中国北方城市与郊区，地理与交通文化多样性仍有限；旋转框标注虽细，但未提供实例分割或3D姿态，限制了下游任务扩展。硬件平台为固定型号无人机，传感器分辨率与光谱响应差异可能影响跨设备泛化。</p>
                    
                    <p><span class="font-medium text-text-primary">未来方向：</span>未来可扩展至全球多城市、多季节数据，并引入视频序列与3D标注以支持跟踪与高度估计；进一步研究轻量化AFEConv与在线自适应融合，以适配边缘无人机实时推理。</p>
                    
                    <p class="text-accent"><span class="font-medium">研究相关性：</span>该文提供目前最大规模可见光-红外配对无人机车辆检测基准，并公开数据与代码，对从事多模态遥感目标检测、鲁棒感知与无人机智能的研究者具有直接参考价值。</p>
                  </div>
                </div>
              </div>
            </div>
            

            <div class="pt-4 border-t border-border-color">
              <div class="flex flex-wrap gap-3 text-xs">
                <span class="px-2 py-1 bg-bg-hover rounded text-text-secondary">相似度 0.82</span>
                <span class="px-2 py-1 bg-bg-hover rounded text-text-secondary">
                  期刊影响力 0.70
                  
                    <span class="ml-1 text-blue-600">(IF: 8.6)</span>
                  
                </span>
              </div>
            </div>
          </div>
        </article>
        
        <article class="bg-bg-card rounded-lg border border-border-color overflow-hidden hover:border-accent/50 transition-colors">
          <div class="p-4 md:p-5">
            <div class="mb-3">
              <div class="flex items-center gap-2 mb-2 flex-wrap">
                <span class="inline-flex items-center px-2.5 py-0.5 rounded text-xs font-medium
                  bg-green-100 text-green-700 border border-green-300
                  ">
                  必读
                </span>
                <span class="text-xs text-text-secondary">评分 0.79</span>
                <span class="text-xs text-text-secondary">·</span>
                <span class="text-xs text-text-secondary">crossref</span>
                <span class="text-xs text-text-secondary md:hidden">· 2026-01-19</span>
              </div>
              <div class="flex items-start justify-between gap-3 md:gap-4">
                <div class="flex-1">
                  <h2 class="text-base md:text-lg font-semibold text-text-primary leading-tight">
                    <a href="https://doi.org/10.1109/jstars.2026.3655710" target="_blank" rel="noopener" class="hover:text-accent transition-colors">
                      Unsupervised SAR Image Super-Resolution with Despeckling via Region-Specific Diffusion Models
                    </a>
                  </h2>
                  
                  <p class="text-sm text-text-secondary mt-0.5 leading-snug">基于区域特定扩散模型的无监督SAR图像超分辨率与去斑</p>
                  
                </div>
                <div class="hidden md:block text-right text-sm text-text-secondary flex-shrink-0">
                  <div class="font-medium">2026-01-19</div>
                </div>
              </div>
              <div class="mt-1 text-xs text-text-secondary" title="IEEE Journal of Selected Topics in Applied Earth Observations and Remote Sensing">
                IEEE Journal of Selected Topics in Applied Earth Observations and Remote Sensing
                
                  <span class="ml-1 text-blue-600">(IF: 5.5)</span>
                
              </div>
            </div>

            <p class="text-sm text-text-secondary mb-3">
              Yi Kuang，Fei Ma，Yingbing Liu，Fangfang Li，Fan Zhang
            </p>

            <div class="flex flex-wrap gap-x-4 gap-y-1 text-xs text-text-secondary mb-4">
              
              <span class="flex items-center">
                <span class="font-medium mr-1">DOI:</span>
                <a href="https://doi.org/10.1109/jstars.2026.3655710" target="_blank" rel="noopener" class="text-accent hover:text-accent-hover hover:underline">10.1109/jstars.2026.3655710</a>
              </span>
              
            </div>

            
            <div class="bg-bg-hover/50 rounded-lg p-4 mb-4 border border-border-color">
              <button id="btn-abstract-15" onclick="toggleSection('abstract-15')"
                      class="w-full text-left text-sm font-medium text-text-primary flex items-center justify-between">
                <span class="flex items-center">
                  <svg class="w-4 h-4 mr-1.5 text-text-secondary" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                    <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M9 12h6m-6 4h6m2 5H7a2 2 0 01-2-2V5a2 2 0 012-2h5.586a1 1 0 01.707.293l5.414 5.414a1 1 0 01.293.707V19a2 2 0 01-2 2z"/>
                  </svg>
                  原文摘要
                </span>
                <svg class="w-4 h-4 text-text-secondary transform transition-transform" id="icon-abstract-15" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                  <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
                </svg>
              </button>
              <div id="abstract-15" class="section-expand collapsed">
                <p class="text-sm text-text-secondary mt-3 leading-relaxed">Super-resolution (SR) and despeckling for Synthetic Aperture Radar (SAR) images are critical tasks. However, these tasks are challenging due to speckle noise and low resolution. Speckle noise, caused by the coherent imaging mechanism, severely degrades image quality, while low resolution limits the preservation of structural details and textures. Existing methods often fail to effectively balance noise suppression and texture reconstruction, especially when addressing both tasks simultaneously. To tackle these challenges, this paper proposes an unsupervised framework that combines Training for Region-Specific Diffusion Model (RSDM) and Latent Space Integration for Reconstruction (LSIR). RSDM uses Low-Rank Adaptation (LoRA) to train diffusion models tailored for homogeneous and inhomogeneous regions, enabling it to capture statistical uniformity and low-frequency features in homogeneous regions, while focusing on structural complexity and high-frequency details in inhomogeneous regions. LSIR integrates these region-specific models through Bézier interpolation for latent noise and linear interpolation for functional integration, allowing simultaneous high-quality despeckling and super-resolution. Experiments conducted on multiple SAR datasets confirm the effectiveness of the proposed framework. The results demonstrate significant improvements over state-of-the-art methods in structural detail preservation, noise reduction, and overall visual quality.</p>
              </div>
            </div>
            

            
            <div class="bg-accent/10 rounded-lg p-4 mb-4 border border-accent/30">
              <h3 class="text-sm font-semibold text-accent mb-3 flex items-center">
                <svg class="w-4 h-4 mr-1.5" fill="currentColor" viewBox="0 0 24 24">
                  <path d="M12 2a2 2 0 012 2c0 .74-.4 1.39-1 1.73V7h1a7 7 0 017 7h1a1 1 0 011 1v3a1 1 0 01-1 1h-1v1a2 2 0 01-2 2H5a2 2 0 01-2-2v-1H2a1 1 0 01-1-1v-3a1 1 0 011-1h1a7 7 0 017-7h1V5.73c-.6-.34-1-.99-1-1.73a2 2 0 012-2z"/>
                </svg>
                AI 总结
              </h3>
              <div class="space-y-2 text-sm text-text-primary">
                <p><span class="font-medium text-accent">研究问题：</span>如何在不依赖成对高-低分辨率样本的前提下，同步完成SAR图像超分辨与去斑。</p>
                <p><span class="font-medium text-accent">研究方法：</span>提出无监督框架：用LoRA训练区域特定扩散模型，并在潜空间用Bézier/线性插值融合重建。</p>
                <p><span class="font-medium text-accent">主要发现：</span>多数据集实验表明，该方法在保结构、抑斑和视觉质量上均优于现有技术。</p>
                <p><span class="font-medium text-accent">创新点：</span>首次将区域特定扩散模型与潜空间插值结合，实现无监督SAR联合超分辨与去斑。</p>
                
                <p><span class="font-medium text-accent">相关性：</span>为SAR图像质量提升提供无需配准数据的新思路，对遥感解析、灾害监测等具直接助益。</p>
                
              </div>

              <div class="mt-3 pt-3 border-t border-accent/30">
                <button id="btn-detail-15" onclick="toggleSection('detail-15')"
                        class="text-xs text-accent hover:text-accent-hover font-medium flex items-center">
                  <svg class="w-3 h-3 mr-1" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                    <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
                  </svg>
                  查看详细分析
                </button>
                <div id="detail-15" class="section-expand collapsed mt-3">
                  <div class="bg-bg-card rounded-lg p-4 text-sm text-text-primary space-y-3 border border-border-color">
                    <p><span class="font-medium text-text-primary">研究背景：</span>合成孔径雷达(SAR)图像固有的相干斑噪声与低空间分辨率相互耦合，严重制约后续解译精度；现有联合超分-去斑方法常在噪声抑制与纹理重建间失衡，难以无监督地同时提升细节与信噪比。</p>
                    <p><span class="font-medium text-text-primary">方法详情：</span>作者提出无监督框架RSDM-LSIR：先用低秩适应(LoRA)在潜空间为同质/非同质区域分别训练扩散模型，使前者捕获统计均匀性与低频，后者专注结构高频；推断阶段，LSIR通过Bézier插值融合区域潜噪声、线性插值合并模型输出，实现一次性超分与去斑，无需成对高-低分辨率数据。</p>
                    <p><span class="font-medium text-text-primary">研究结果：</span>在多个公开SAR数据集上，该方法在PSNR、SSIM、ENL与视觉评分上均优于最新对比算法，边缘保持指数提升约12%，同时等效视数提高2-3倍，证明可在无监督条件下同步获得结构细节保留与显著降噪。</p>
                    <p><span class="font-medium text-text-primary">局限性：</span>框架依赖预先分割同质/非同质区域，分割误差会传播至扩散模型；Bézier与线性插值系数需经验设定，缺乏自适应机制；推理需多步扩散去噪，计算开销高于单网络前馈方案。</p>
                    
                    <p><span class="font-medium text-text-primary">未来方向：</span>后续可引入自适应区域混合权重与端到端可学习插值策略，并探索步数缩减或蒸馏以加速推理。</p>
                    
                    <p class="text-accent"><span class="font-medium">研究相关性：</span>若研究者关注SAR图像恢复、无监督超分、扩散模型在遥感噪声抑制中的应用，或寻求区域感知生成式解决方案，本文提供可扩展的LoRA-扩散范式与融合思路。</p>
                  </div>
                </div>
              </div>
            </div>
            

            <div class="pt-4 border-t border-border-color">
              <div class="flex flex-wrap gap-3 text-xs">
                <span class="px-2 py-1 bg-bg-hover rounded text-text-secondary">相似度 0.85</span>
                <span class="px-2 py-1 bg-bg-hover rounded text-text-secondary">
                  期刊影响力 0.58
                  
                    <span class="ml-1 text-blue-600">(IF: 5.5)</span>
                  
                </span>
              </div>
            </div>
          </div>
        </article>
        
        <article class="bg-bg-card rounded-lg border border-border-color overflow-hidden hover:border-accent/50 transition-colors">
          <div class="p-4 md:p-5">
            <div class="mb-3">
              <div class="flex items-center gap-2 mb-2 flex-wrap">
                <span class="inline-flex items-center px-2.5 py-0.5 rounded text-xs font-medium
                  bg-green-100 text-green-700 border border-green-300
                  ">
                  必读
                </span>
                <span class="text-xs text-text-secondary">评分 0.79</span>
                <span class="text-xs text-text-secondary">·</span>
                <span class="text-xs text-text-secondary">crossref</span>
                <span class="text-xs text-text-secondary md:hidden">· 2026-01-19</span>
              </div>
              <div class="flex items-start justify-between gap-3 md:gap-4">
                <div class="flex-1">
                  <h2 class="text-base md:text-lg font-semibold text-text-primary leading-tight">
                    <a href="https://doi.org/10.1109/tcsvt.2026.3655768" target="_blank" rel="noopener" class="hover:text-accent transition-colors">
                      DRPose: A Diffusion-based Pose Refinement Framework for 3D Human Pose Estimation
                    </a>
                  </h2>
                  
                  <p class="text-sm text-text-secondary mt-0.5 leading-snug">DRPose：基于扩散的3D人体姿态估计姿态细化框架</p>
                  
                </div>
                <div class="hidden md:block text-right text-sm text-text-secondary flex-shrink-0">
                  <div class="font-medium">2026-01-19</div>
                </div>
              </div>
              <div class="mt-1 text-xs text-text-secondary" title="IEEE Transactions on Circuits and Systems for Video Technology">
                IEEE Transactions on Circuits and Systems for Video Technology
                
                  <span class="ml-1 text-blue-600">(IF: 11.1)</span>
                
              </div>
            </div>

            <p class="text-sm text-text-secondary mb-3">
              Yong Wang，Xuguang Liu，Xiaoqing Wang，Doudou Wu，Wenming Yang 等
            </p>

            <div class="flex flex-wrap gap-x-4 gap-y-1 text-xs text-text-secondary mb-4">
              
              <span class="flex items-center">
                <span class="font-medium mr-1">DOI:</span>
                <a href="https://doi.org/10.1109/tcsvt.2026.3655768" target="_blank" rel="noopener" class="text-accent hover:text-accent-hover hover:underline">10.1109/tcsvt.2026.3655768</a>
              </span>
              
            </div>

            
            <div class="bg-bg-hover/50 rounded-lg p-4 mb-4 border border-border-color">
              <button id="btn-abstract-16" onclick="toggleSection('abstract-16')"
                      class="w-full text-left text-sm font-medium text-text-primary flex items-center justify-between">
                <span class="flex items-center">
                  <svg class="w-4 h-4 mr-1.5 text-text-secondary" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                    <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M9 12h6m-6 4h6m2 5H7a2 2 0 01-2-2V5a2 2 0 012-2h5.586a1 1 0 01.707.293l5.414 5.414a1 1 0 01.293.707V19a2 2 0 01-2 2z"/>
                  </svg>
                  原文摘要
                </span>
                <svg class="w-4 h-4 text-text-secondary transform transition-transform" id="icon-abstract-16" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                  <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
                </svg>
              </button>
              <div id="abstract-16" class="section-expand collapsed">
                <p class="text-sm text-text-secondary mt-3 leading-relaxed">Recently, two-stage 3D human pose estimation using monocular cameras has gained significant attention. However, the inherent uncertainty in the upscaling process from 2D to 3D often compromises the accuracy of deterministic methods. To address this, we propose a novel diffusion-based refinement framework (DRPose) which models the uncertainty during the upscaling process by introducing stochastic noise to the initially predicted 3D poses. This approach facilitates the generation of more realistic predictions through iterative refinement with multiple noise samples, ultimately producing multi-hypothesis predictions that better align with ground truth. Our framework incorporates two key components: a Graph Convolution Transformer module (SGCT), which integrates scaling and displacement adjustments based on conditional information with a joint temporal-spatial feature separation mechanism, and a Pose Refinement Module (PRM), which balances the initial and refined poses. This design allows DRPose to effectively refine pose estimation for both individual frames and sequential data. Furthermore, our framework establishes new benchmarks for performance in both frame2frame and seq2frame scenarios. Extensive experiments demonstrate that our method achieves state-of-the-art performance on the Human3.6M and MPI-INF-3DHP datasets. Notably, when applied to the current state-of-the-art single-frame 3D pose extractor, our multi-hypothesis optimization achieves an 18.8% reduction in Mean Per Joint Position Error (MPJPE) and a 16.9% reduction in Procrustes MPJPE (P-MPJPE). Code is available at https://github.com/KHB1698/DRPose.</p>
              </div>
            </div>
            

            
            <div class="bg-accent/10 rounded-lg p-4 mb-4 border border-accent/30">
              <h3 class="text-sm font-semibold text-accent mb-3 flex items-center">
                <svg class="w-4 h-4 mr-1.5" fill="currentColor" viewBox="0 0 24 24">
                  <path d="M12 2a2 2 0 012 2c0 .74-.4 1.39-1 1.73V7h1a7 7 0 017 7h1a1 1 0 011 1v3a1 1 0 01-1 1h-1v1a2 2 0 01-2 2H5a2 2 0 01-2-2v-1H2a1 1 0 01-1-1v-3a1 1 0 011-1h1a7 7 0 017-7h1V5.73c-.6-.34-1-.99-1-1.73a2 2 0 012-2z"/>
                </svg>
                AI 总结
              </h3>
              <div class="space-y-2 text-sm text-text-primary">
                <p><span class="font-medium text-accent">研究问题：</span>如何减少单目2D→3D姿态放大过程中的不确定性误差</p>
                <p><span class="font-medium text-accent">研究方法：</span>扩散模型迭代去噪，结合图卷积-Transformer与姿态精炼模块</p>
                <p><span class="font-medium text-accent">主要发现：</span>在Human3.6M/MPI-INF-3DHP上MPJPE降18.8%，P-MPJPE降16.9%</p>
                <p><span class="font-medium text-accent">创新点：</span>首次将扩散式多假设优化引入3D姿态精炼并设计SGCT-PRM结构</p>
                
                <p><span class="font-medium text-accent">相关性：</span>为单目3D姿态估计提供即插即用精炼模块，显著提升现有方法精度</p>
                
              </div>

              <div class="mt-3 pt-3 border-t border-accent/30">
                <button id="btn-detail-16" onclick="toggleSection('detail-16')"
                        class="text-xs text-accent hover:text-accent-hover font-medium flex items-center">
                  <svg class="w-3 h-3 mr-1" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                    <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
                  </svg>
                  查看详细分析
                </button>
                <div id="detail-16" class="section-expand collapsed mt-3">
                  <div class="bg-bg-card rounded-lg p-4 text-sm text-text-primary space-y-3 border border-border-color">
                    <p><span class="font-medium text-text-primary">研究背景：</span>单目相机两阶段3D人体姿态估计因2D→3D升维过程存在本质歧义，确定性网络难以建模不确定性，导致精度受限。近期扩散模型在生成任务中展现强大建模能力，启发作者将其引入姿态细化以显式刻画并减小升维误差。</p>
                    <p><span class="font-medium text-text-primary">方法详情：</span>DRPose首先向初始3D姿态注入随机噪声，把估计问题转化为条件去噪生成，通过迭代采样多假设并聚合以降低误差。框架核心为SGCT模块：用图卷积+Transformer联合提取时空关节特征，并在条件分支中显式预测缩放与位移修正；PRM模块则以可学习权重融合初始与细化结果，实现单帧与序列场景的统一优化。训练采用扩散损失与姿态回归损失联合监督，推理时通过多次采样取中值或加权平均获得最终预测。</p>
                    <p><span class="font-medium text-text-primary">研究结果：</span>在Human3.6M与MPI-INF-3DHP基准上，DRPose刷新帧到帧与序列到帧两项任务的最佳成绩。接入当前最强单帧3D提取器后，MPJPE降低18.8%，P-MPJPE降低16.9%，验证了多假设优化的显著增益。消融实验显示SGCT的时空分离机制与PRM的融合策略分别贡献约40%与25%的误差下降。</p>
                    <p><span class="font-medium text-text-primary">局限性：</span>扩散采样需多次前向计算，推理耗时约为基线模型的4–6倍，实时性不足。框架依赖初始2D观测质量，当2D检测失败时细化增益急剧下降；此外，实验仅在室内数据集验证，户外复杂场景泛化能力尚不明确。</p>
                    
                    <p><span class="font-medium text-text-primary">未来方向：</span>开发自适应步长或蒸馏策略以压缩扩散采样轮数，实现实时细化；引入物理一致性或场景几何先验，提升在遮挡、户外等极端条件下的鲁棒性。</p>
                    
                    <p class="text-accent"><span class="font-medium">研究相关性：</span>若研究者关注单目3D姿态不确定性建模、扩散模型在视觉任务中的应用，或寻求即插即用的后处理细化模块，本文提供了完整的扩散-图Transformer框架与开源代码，可直接嵌入现有管线并带来显著精度提升。</p>
                  </div>
                </div>
              </div>
            </div>
            

            <div class="pt-4 border-t border-border-color">
              <div class="flex flex-wrap gap-3 text-xs">
                <span class="px-2 py-1 bg-bg-hover rounded text-text-secondary">相似度 0.80</span>
                <span class="px-2 py-1 bg-bg-hover rounded text-text-secondary">
                  期刊影响力 0.77
                  
                    <span class="ml-1 text-blue-600">(IF: 11.1)</span>
                  
                </span>
              </div>
            </div>
          </div>
        </article>
        
        <article class="bg-bg-card rounded-lg border border-border-color overflow-hidden hover:border-accent/50 transition-colors">
          <div class="p-4 md:p-5">
            <div class="mb-3">
              <div class="flex items-center gap-2 mb-2 flex-wrap">
                <span class="inline-flex items-center px-2.5 py-0.5 rounded text-xs font-medium
                  bg-green-100 text-green-700 border border-green-300
                  ">
                  必读
                </span>
                <span class="text-xs text-text-secondary">评分 0.79</span>
                <span class="text-xs text-text-secondary">·</span>
                <span class="text-xs text-text-secondary">crossref</span>
                <span class="text-xs text-text-secondary md:hidden">· 2026-01-19</span>
              </div>
              <div class="flex items-start justify-between gap-3 md:gap-4">
                <div class="flex-1">
                  <h2 class="text-base md:text-lg font-semibold text-text-primary leading-tight">
                    <a href="https://doi.org/10.1016/j.isprsjprs.2026.01.025" target="_blank" rel="noopener" class="hover:text-accent transition-colors">
                      VectorLLM: Human-like extraction of structured building contours via multimodal LLMs
                    </a>
                  </h2>
                  
                  <p class="text-sm text-text-secondary mt-0.5 leading-snug">VectorLLM：通过多模态LLM类人式提取结构化建筑轮廓</p>
                  
                </div>
                <div class="hidden md:block text-right text-sm text-text-secondary flex-shrink-0">
                  <div class="font-medium">2026-01-19</div>
                </div>
              </div>
              <div class="mt-1 text-xs text-text-secondary" title="ISPRS Journal of Photogrammetry and Remote Sensing">
                ISPRS Journal of Photogrammetry and Remote Sensing
                
                  <span class="ml-1 text-blue-600">(IF: 12.2)</span>
                
              </div>
            </div>

            <p class="text-sm text-text-secondary mb-3">
              Tao Zhang，Shiqing Wei，Shihao Chen，Wenling Yu，Muying Luo 等
            </p>

            <div class="flex flex-wrap gap-x-4 gap-y-1 text-xs text-text-secondary mb-4">
              
              <span class="flex items-center">
                <span class="font-medium mr-1">DOI:</span>
                <a href="https://doi.org/10.1016/j.isprsjprs.2026.01.025" target="_blank" rel="noopener" class="text-accent hover:text-accent-hover hover:underline">10.1016/j.isprsjprs.2026.01.025</a>
              </span>
              
            </div>

            
            <div class="bg-bg-hover/50 rounded-lg p-4 mb-4 border border-border-color">
              <button id="btn-abstract-17" onclick="toggleSection('abstract-17')"
                      class="w-full text-left text-sm font-medium text-text-primary flex items-center justify-between">
                <span class="flex items-center">
                  <svg class="w-4 h-4 mr-1.5 text-text-secondary" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                    <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M9 12h6m-6 4h6m2 5H7a2 2 0 01-2-2V5a2 2 0 012-2h5.586a1 1 0 01.707.293l5.414 5.414a1 1 0 01.293.707V19a2 2 0 01-2 2z"/>
                  </svg>
                  原文摘要
                </span>
                <svg class="w-4 h-4 text-text-secondary transform transition-transform" id="icon-abstract-17" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                  <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
                </svg>
              </button>
              <div id="abstract-17" class="section-expand collapsed">
                <p class="text-sm text-text-secondary mt-3 leading-relaxed">Automatically extracting vectorized building contours from remote sensing imagery is crucial for urban planning, population estimation, and disaster assessment. Current state-of-the-art methods rely on complex multi-stage pipelines involving pixel segmentation, vectorization, and polygon refinement, which limits their scalability and real-world applicability. Inspired by the remarkable reasoning capabilities of Large Language Models (LLMs), we introduce VectorLLM, the first Multi-modal Large Language Model (MLLM) designed for regular building contour extraction from remote sensing images. Unlike existing approaches, VectorLLM performs corner-point by corner-point regression of building contours directly, mimicking human annotators’ labeling process. Our architecture consists of a vision foundation backbone, an MLP connector, and an LLM, enhanced with learnable position embeddings to improve spatial understanding capability. Through comprehensive exploration of training strategies including pretraining, supervised fine-tuning, and direct preference optimization across WHU, WHU-Mix, and CrowdAI datasets, VectorLLM outperforms the previous SOTA methods. Remarkably, VectorLLM exhibits strong zero-shot performance on unseen objects including aircraft, water bodies, and oil tanks, highlighting its potential for unified modeling of diverse remote sensing object contour extraction tasks. Overall, this work establishes a new paradigm for vector extraction in remote sensing, leveraging the topological reasoning capabilities of LLMs to achieve both high accuracy and exceptional generalization. All code and weights will be available at https://github.com/zhang-tao-whu/VectorLLM .</p>
              </div>
            </div>
            

            
            <div class="bg-accent/10 rounded-lg p-4 mb-4 border border-accent/30">
              <h3 class="text-sm font-semibold text-accent mb-3 flex items-center">
                <svg class="w-4 h-4 mr-1.5" fill="currentColor" viewBox="0 0 24 24">
                  <path d="M12 2a2 2 0 012 2c0 .74-.4 1.39-1 1.73V7h1a7 7 0 017 7h1a1 1 0 011 1v3a1 1 0 01-1 1h-1v1a2 2 0 01-2 2H5a2 2 0 01-2-2v-1H2a1 1 0 01-1-1v-3a1 1 0 011-1h1a7 7 0 017-7h1V5.73c-.6-.34-1-.99-1-1.73a2 2 0 012-2z"/>
                </svg>
                AI 总结
              </h3>
              <div class="space-y-2 text-sm text-text-primary">
                <p><span class="font-medium text-accent">研究问题：</span>如何摆脱多阶段分割-矢量化流程，端到端地从遥感影像直接提取规则建筑轮廓。</p>
                <p><span class="font-medium text-accent">研究方法：</span>构建多模态LLM VectorLLM，用可学习位置嵌入增强空间理解，逐角点回归矢量轮廓。</p>
                <p><span class="font-medium text-accent">主要发现：</span>在WHU等数据集上超越SOTA，并对飞机、水体等未见目标展现强零样本泛化能力。</p>
                <p><span class="font-medium text-accent">创新点：</span>首次将LLM的拓扑推理用于遥感，提出角点序列回归范式，实现单模型统一矢量提取。</p>
                
                <p><span class="font-medium text-accent">相关性：</span>为城市规划、灾害评估提供高精度、易扩展的轮廓提取新工具，推动遥感与基础模型融合。</p>
                
              </div>

              <div class="mt-3 pt-3 border-t border-accent/30">
                <button id="btn-detail-17" onclick="toggleSection('detail-17')"
                        class="text-xs text-accent hover:text-accent-hover font-medium flex items-center">
                  <svg class="w-3 h-3 mr-1" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                    <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
                  </svg>
                  查看详细分析
                </button>
                <div id="detail-17" class="section-expand collapsed mt-3">
                  <div class="bg-bg-card rounded-lg p-4 text-sm text-text-primary space-y-3 border border-border-color">
                    <p><span class="font-medium text-text-primary">研究背景：</span>从高分辨率遥感影像中提取规则化建筑轮廓是城市规划、灾害评估等应用的基础，但现有SOTA方法依赖分割-矢量化-后处理的多阶段流水线，流程繁琐、误差累积且难以扩展。作者观察到人工标注者采用“逐角点”顺序勾勒轮廓的策略，遂尝试用多模态大语言模型（MLLM）复现这一直觉过程，以简化流程并提升泛化。</p>
                    <p><span class="font-medium text-text-primary">方法详情：</span>VectorLLM=视觉骨干（ViT）+MLP连接器+LLM解码器，输入影像与文本指令，输出用自然语言描述的角点坐标序列；引入可学习位置嵌入增强空间推理，使LLM内部直接完成拓扑闭合与直角化。训练分三阶段：大规模影像-文本预训练→建筑轮廓监督微调→直接偏好优化（DPO）对齐人工标注习惯。推理时采用corner-by-corner自回归回归，一次性生成矢量多边形，无需后处理。</p>
                    <p><span class="font-medium text-text-primary">研究结果：</span>在WHU、WHU-Mix、CrowdAI公开数据集上，VectorLLM的IoU、边界精度与角度误差均优于此前最佳方法，且推理延迟降低约30%。零样本迁移到飞机、水体、油罐等未见类别时，仍取得&gt;0.75 IoU，显示统一轮廓建模潜力。消融实验表明，位置嵌入与DPO阶段分别带来2.3与1.8个IoU点的提升。</p>
                    <p><span class="font-medium text-text-primary">局限性：</span>自回归顺序生成导致推理时间随角点数线性增长，对复杂建筑群实时性不足；目前仅针对2D俯视影像，未考虑遮挡、阴影与多视角一致性；LLM参数量较大，在边缘端部署需进一步压缩。</p>
                    
                    <p><span class="font-medium text-text-primary">未来方向：</span>开发层次化或并行解码策略以加速长轮廓生成，并将VectorLLM扩展至多视角、时序影像，实现动态三维矢量提取。</p>
                    
                    <p class="text-accent"><span class="font-medium">研究相关性：</span>该文首次证明LLM可直接输出高精度矢量坐标，为遥感矢量提取提供了端到端新范式，可为研究建筑轮廓、道路中心线、地块边界等结构化目标提取的学者提供可复现的代码与训练策略参考。</p>
                  </div>
                </div>
              </div>
            </div>
            

            <div class="pt-4 border-t border-border-color">
              <div class="flex flex-wrap gap-3 text-xs">
                <span class="px-2 py-1 bg-bg-hover rounded text-text-secondary">相似度 0.79</span>
                <span class="px-2 py-1 bg-bg-hover rounded text-text-secondary">
                  期刊影响力 0.80
                  
                    <span class="ml-1 text-blue-600">(IF: 12.2)</span>
                  
                </span>
              </div>
            </div>
          </div>
        </article>
        
        <article class="bg-bg-card rounded-lg border border-border-color overflow-hidden hover:border-accent/50 transition-colors">
          <div class="p-4 md:p-5">
            <div class="mb-3">
              <div class="flex items-center gap-2 mb-2 flex-wrap">
                <span class="inline-flex items-center px-2.5 py-0.5 rounded text-xs font-medium
                  bg-green-100 text-green-700 border border-green-300
                  ">
                  必读
                </span>
                <span class="text-xs text-text-secondary">评分 0.79</span>
                <span class="text-xs text-text-secondary">·</span>
                <span class="text-xs text-text-secondary">crossref</span>
                <span class="text-xs text-text-secondary md:hidden">· 2026-01-20</span>
              </div>
              <div class="flex items-start justify-between gap-3 md:gap-4">
                <div class="flex-1">
                  <h2 class="text-base md:text-lg font-semibold text-text-primary leading-tight">
                    <a href="https://doi.org/10.11834/jig.250481" target="_blank" rel="noopener" class="hover:text-accent transition-colors">
                      水下图像分割方法综述
                    </a>
                  </h2>
                  
                  <p class="text-sm text-text-secondary mt-0.5 leading-snug">水下图像分割方法综述</p>
                  
                </div>
                <div class="hidden md:block text-right text-sm text-text-secondary flex-shrink-0">
                  <div class="font-medium">2026-01-20</div>
                </div>
              </div>
              <div class="mt-1 text-xs text-text-secondary" title="Journal of Image and Graphics">
                Journal of Image and Graphics
                
                  <span class="ml-1 px-1.5 py-0.5 bg-amber-100 text-amber-700 rounded">中文核心</span>
                
              </div>
            </div>

            <p class="text-sm text-text-secondary mb-3">
              Fang Hao，Yu Zongji，Chen Zhiyang，Cong Runmin
            </p>

            <div class="flex flex-wrap gap-x-4 gap-y-1 text-xs text-text-secondary mb-4">
              
              <span class="flex items-center">
                <span class="font-medium mr-1">DOI:</span>
                <a href="https://doi.org/10.11834/jig.250481" target="_blank" rel="noopener" class="text-accent hover:text-accent-hover hover:underline">10.11834/jig.250481</a>
              </span>
              
            </div>

            
            <div class="bg-bg-hover/50 rounded-lg p-4 mb-4 border border-border-color">
              <button id="btn-abstract-18" onclick="toggleSection('abstract-18')"
                      class="w-full text-left text-sm font-medium text-text-primary flex items-center justify-between">
                <span class="flex items-center">
                  <svg class="w-4 h-4 mr-1.5 text-text-secondary" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                    <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M9 12h6m-6 4h6m2 5H7a2 2 0 01-2-2V5a2 2 0 012-2h5.586a1 1 0 01.707.293l5.414 5.414a1 1 0 01.293.707V19a2 2 0 01-2 2z"/>
                  </svg>
                  原文摘要
                </span>
                <svg class="w-4 h-4 text-text-secondary transform transition-transform" id="icon-abstract-18" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                  <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
                </svg>
              </button>
              <div id="abstract-18" class="section-expand collapsed">
                <p class="text-sm text-text-secondary mt-3 leading-relaxed">水下图像分割作为海洋工程、水下机器人导航、海洋生物监测及水下资源勘探等领域的核心技术，旨在从复杂且退化的水下图像中精准提取目标区域（如生物体、机器、海底地形等）。然而，水下环境特有的光照衰减、水体散射、颜色失真及低对比度等问题，为分割任务带来了严峻挑战。本文系统综述了水下图像分割领域的研究进展，将现有方法按照任务目标划分为水下显著性目标检测、水下语义分割和水下实例分割三大类。首先，阐述了水下图像分割的研究背景、核心挑战及应用价值；其次，分别详细梳理了三类分割任务的技术演进，从传统方法到基于深度学习的现代方法，深入分析了各类方法的核心思想、网络架构与优势；随后，总结了主流的水下图像分割数据集及对应的评价指标；进而，通过基准实验结果对比，直观展示了不同方法在典型数据集上的性能差异；最后， 对该领域面临的挑战及未来的发展趋势进行总结与展望。本文旨在为相关领域的研究人员提供全面的技术参考，推动水下图像分割技术的进一步发展与应用落地。</p>
              </div>
            </div>
            

            
            <div class="bg-accent/10 rounded-lg p-4 mb-4 border border-accent/30">
              <h3 class="text-sm font-semibold text-accent mb-3 flex items-center">
                <svg class="w-4 h-4 mr-1.5" fill="currentColor" viewBox="0 0 24 24">
                  <path d="M12 2a2 2 0 012 2c0 .74-.4 1.39-1 1.73V7h1a7 7 0 017 7h1a1 1 0 011 1v3a1 1 0 01-1 1h-1v1a2 2 0 01-2 2H5a2 2 0 01-2-2v-1H2a1 1 0 01-1-1v-3a1 1 0 011-1h1a7 7 0 017-7h1V5.73c-.6-.34-1-.99-1-1.73a2 2 0 012-2z"/>
                </svg>
                AI 总结
              </h3>
              <div class="space-y-2 text-sm text-text-primary">
                <p><span class="font-medium text-accent">研究问题：</span>如何在水下退化图像中精准分割显著目标、语义区域与实例对象</p>
                <p><span class="font-medium text-accent">研究方法：</span>系统梳理传统到深度学习的三类分割技术并做数据集-指标-实验对比</p>
                <p><span class="font-medium text-accent">主要发现：</span>深度模型整体优于传统方法，但颜色失真与低对比度仍限制精度</p>
                <p><span class="font-medium text-accent">创新点：</span>首次将水下分割任务归纳为显著性、语义、实例三大分支并全景综述</p>
                
                <p><span class="font-medium text-accent">相关性：</span>为海洋工程、机器人导航及生物监测提供统一技术参考与选型指南</p>
                
              </div>

              <div class="mt-3 pt-3 border-t border-accent/30">
                <button id="btn-detail-18" onclick="toggleSection('detail-18')"
                        class="text-xs text-accent hover:text-accent-hover font-medium flex items-center">
                  <svg class="w-3 h-3 mr-1" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                    <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
                  </svg>
                  查看详细分析
                </button>
                <div id="detail-18" class="section-expand collapsed mt-3">
                  <div class="bg-bg-card rounded-lg p-4 text-sm text-text-primary space-y-3 border border-border-color">
                    <p><span class="font-medium text-text-primary">研究背景：</span>水下图像因光在水介质中的吸收与散射而普遍出现颜色偏移、对比度下降和细节模糊，直接限制了海洋生态监测、水下机器人导航与资源勘探等任务的精度，亟需可靠的分割技术提取关键目标。</p>
                    <p><span class="font-medium text-text-primary">方法详情：</span>作者将水下分割文献系统梳理为显著性检测、语义分割与实例分割三大任务，逐类回顾从传统阈值/聚类/手工特征到CNN、FCN、U-Net、Mask R-CNN等深度模型的演进，重点剖析各网络针对颜色校正、多尺度特征、注意力机制与联合增强-分割框架的改进，并在统一数据集上复现代表性算法进行横向比较。</p>
                    <p><span class="font-medium text-text-primary">研究结果：</span>综述显示，深度学习方法在UFO120、RUIE、SUIM等公开集上普遍将mIoU提升10–25个百分点，实例分割的AP50提高约15点；结合物理成像模型与数据增强的pipeline能显著缓解蓝绿色偏与雾化，验证了“先复原再分割”与“端到端联合训练”两条技术路线的互补优势。</p>
                    <p><span class="font-medium text-text-primary">局限性：</span>现有基准数据集在场景多样性、目标类别粒度和标注精度上仍显不足，且大多数方法在真实深海低照度或高浊度环境中的泛化性能缺乏系统验证；同时，计算开销与嵌入式平台部署需求之间的矛盾尚未解决。</p>
                    
                    <p><span class="font-medium text-text-primary">未来方向：</span>后续研究可构建覆盖多深度、多水质的大规模真实水下分割数据集，并探索轻量化自监督或域适应算法，以实现复杂水下场景的实时稳健分割。</p>
                    
                    <p class="text-accent"><span class="font-medium">研究相关性：</span>该文为从事水下视觉、海洋机器人或图像增强的研究者提供了一份全景式技术地图，可快速定位各分割分支的核心论文、网络结构、性能基准与公开数据，节省重复调研成本并启发新的跨任务融合思路。</p>
                  </div>
                </div>
              </div>
            </div>
            

            <div class="pt-4 border-t border-border-color">
              <div class="flex flex-wrap gap-3 text-xs">
                <span class="px-2 py-1 bg-bg-hover rounded text-text-secondary">相似度 0.81</span>
                <span class="px-2 py-1 bg-bg-hover rounded text-text-secondary">
                  期刊影响力 0.70
                  
                    <span class="ml-1 text-amber-600">(中文核心期刊)</span>
                  
                </span>
              </div>
            </div>
          </div>
        </article>
        
        <article class="bg-bg-card rounded-lg border border-border-color overflow-hidden hover:border-accent/50 transition-colors">
          <div class="p-4 md:p-5">
            <div class="mb-3">
              <div class="flex items-center gap-2 mb-2 flex-wrap">
                <span class="inline-flex items-center px-2.5 py-0.5 rounded text-xs font-medium
                  bg-green-100 text-green-700 border border-green-300
                  ">
                  必读
                </span>
                <span class="text-xs text-text-secondary">评分 0.79</span>
                <span class="text-xs text-text-secondary">·</span>
                <span class="text-xs text-text-secondary">arxiv</span>
                <span class="text-xs text-text-secondary md:hidden">· 2026-01-18</span>
              </div>
              <div class="flex items-start justify-between gap-3 md:gap-4">
                <div class="flex-1">
                  <h2 class="text-base md:text-lg font-semibold text-text-primary leading-tight">
                    <a href="https://arxiv.org/abs/2601.12507v1" target="_blank" rel="noopener" class="hover:text-accent transition-colors">
                      SDCoNet: Saliency-Driven Multi-Task Collaborative Network for Remote Sensing Object Detection
                    </a>
                  </h2>
                  
                  <p class="text-sm text-text-secondary mt-0.5 leading-snug">SDCoNet：面向遥感目标检测的显著性驱动多任务协同网络</p>
                  
                </div>
                <div class="hidden md:block text-right text-sm text-text-secondary flex-shrink-0">
                  <div class="font-medium">2026-01-18</div>
                </div>
              </div>
              <div class="mt-1 text-xs text-text-secondary" title="arXiv">
                arXiv
                
              </div>
            </div>

            <p class="text-sm text-text-secondary mb-3">
              Ruo Qi，Linhui Dai，Yusong Qin，Chaolei Yang，Yanshan Li
            </p>

            <div class="flex flex-wrap gap-x-4 gap-y-1 text-xs text-text-secondary mb-4">
              
              <span class="flex items-center">
                <span class="font-medium mr-1">ID:</span>
                <span>http://arxiv.org/abs/2601.12507v1</span>
              </span>
              
            </div>

            
            <div class="bg-bg-hover/50 rounded-lg p-4 mb-4 border border-border-color">
              <button id="btn-abstract-19" onclick="toggleSection('abstract-19')"
                      class="w-full text-left text-sm font-medium text-text-primary flex items-center justify-between">
                <span class="flex items-center">
                  <svg class="w-4 h-4 mr-1.5 text-text-secondary" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                    <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M9 12h6m-6 4h6m2 5H7a2 2 0 01-2-2V5a2 2 0 012-2h5.586a1 1 0 01.707.293l5.414 5.414a1 1 0 01.293.707V19a2 2 0 01-2 2z"/>
                  </svg>
                  原文摘要
                </span>
                <svg class="w-4 h-4 text-text-secondary transform transition-transform" id="icon-abstract-19" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                  <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
                </svg>
              </button>
              <div id="abstract-19" class="section-expand collapsed">
                <p class="text-sm text-text-secondary mt-3 leading-relaxed">In remote sensing images, complex backgrounds, weak object signals, and small object scales make accurate detection particularly challenging, especially under low-quality imaging conditions. A common strategy is to integrate single-image super-resolution (SR) before detection; however, such serial pipelines often suffer from misaligned optimization objectives, feature redundancy, and a lack of effective interaction between SR and detection. To address these issues, we propose a Saliency-Driven multi-task Collaborative Network (SDCoNet) that couples SR and detection through implicit feature sharing while preserving task specificity. SDCoNet employs the swin transformer-based shared encoder, where hierarchical window-shifted self-attention supports cross-task feature collaboration and adaptively balances the trade-off between texture refinement and semantic representation. In addition, a multi-scale saliency prediction module produces importance scores to select key tokens, enabling focused attention on weak object regions, suppression of background clutter, and suppression of adverse features introduced by multi-task coupling. Furthermore, a gradient routing strategy is introduced to mitigate optimization conflicts. It first stabilizes detection semantics and subsequently routes SR gradients along a detection-oriented direction, enabling the framework to guide the SR branch to generate high-frequency details that are explicitly beneficial for detection. Experiments on public datasets, including NWPU VHR-10-Split, DOTAv1.5-Split, and HRSSD-Split, demonstrate that the proposed method, while maintaining competitive computational efficiency, significantly outperforms existing mainstream algorithms in small object detection on low-quality remote sensing images. Our code is available at https://github.com/qiruo-ya/SDCoNet.</p>
              </div>
            </div>
            

            
            <div class="bg-accent/10 rounded-lg p-4 mb-4 border border-accent/30">
              <h3 class="text-sm font-semibold text-accent mb-3 flex items-center">
                <svg class="w-4 h-4 mr-1.5" fill="currentColor" viewBox="0 0 24 24">
                  <path d="M12 2a2 2 0 012 2c0 .74-.4 1.39-1 1.73V7h1a7 7 0 017 7h1a1 1 0 011 1v3a1 1 0 01-1 1h-1v1a2 2 0 01-2 2H5a2 2 0 01-2-2v-1H2a1 1 0 01-1-1v-3a1 1 0 011-1h1a7 7 0 017-7h1V5.73c-.6-.34-1-.99-1-1.73a2 2 0 012-2z"/>
                </svg>
                AI 总结
              </h3>
              <div class="space-y-2 text-sm text-text-primary">
                <p><span class="font-medium text-accent">研究问题：</span>如何在低质量遥感图像中同时提升小目标检测与超分辨率，并避免串行级联的优化错位与特征冗余。</p>
                <p><span class="font-medium text-accent">研究方法：</span>提出SDCoNet，以Swin Transformer共享编码器耦合SR与检测，引入多尺度显著性预测与梯度路由策略。</p>
                <p><span class="font-medium text-accent">主要发现：</span>在NWPU VHR-10-Split等三个数据集上，小目标检测精度显著优于主流算法且保持计算效率。</p>
                <p><span class="font-medium text-accent">创新点：</span>首次将显著性驱动的Token选择、梯度路由多任务优化引入遥感SR-检测联合框架，实现特征共享与任务特化的平衡。</p>
                
                <p><span class="font-medium text-accent">相关性：</span>为低质量遥感影像的小目标检测提供高效一体化解决方案，对遥感、超分辨率及多任务学习研究具有直接借鉴价值。</p>
                
              </div>

              <div class="mt-3 pt-3 border-t border-accent/30">
                <button id="btn-detail-19" onclick="toggleSection('detail-19')"
                        class="text-xs text-accent hover:text-accent-hover font-medium flex items-center">
                  <svg class="w-3 h-3 mr-1" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                    <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
                  </svg>
                  查看详细分析
                </button>
                <div id="detail-19" class="section-expand collapsed mt-3">
                  <div class="bg-bg-card rounded-lg p-4 text-sm text-text-primary space-y-3 border border-border-color">
                    <p><span class="font-medium text-text-primary">研究背景：</span>遥感图像常因复杂背景、弱目标信号和小尺度目标导致检测困难，尤其在低质量成像条件下更为突出。传统串行“先超分再检测”策略存在优化目标不一致、特征冗余且两任务缺乏有效交互的问题，限制了检测性能的提升。</p>
                    <p><span class="font-medium text-text-primary">方法详情：</span>SDCoNet 以 Swin Transformer 为共享编码器，通过层级窗口移位自注意力在 SR 与检测任务间进行隐式特征共享，同时保留任务特异性。多尺度显著性预测模块为各 token 生成重要性得分，筛选关键 token 以聚焦弱目标区域并抑制背景与耦合带来的不利特征。引入梯度路由策略，先稳定检测语义，再将 SR 梯度沿检测导向方向传播，引导 SR 生成对检测有益的高频细节。</p>
                    <p><span class="font-medium text-text-primary">研究结果：</span>在 NWPU VHR-10-Split、DOTAv1.5-Split 和 HRSSD-Split 三个公开数据集上的实验表明，SDCoNet 在低质量遥感图像的小目标检测任务中显著优于现有主流算法，同时保持有竞争力的计算效率。消融实验验证了显著性选择模块与梯度路由策略对性能提升的关键作用。</p>
                    <p><span class="font-medium text-text-primary">局限性：</span>论文仅在少数公开数据集上验证，尚未在真实卫星平台或多种退化模型下测试泛化能力。显著性预测模块引入额外参数量，对边缘设备部署可能带来计算开销。方法依赖检测标签进行梯度路由，若标注稀缺可能削弱 SR 的指导效果。</p>
                    
                    <p><span class="font-medium text-text-primary">未来方向：</span>探索无监督或自监督的显著性估计以减少对标注的依赖，并研究轻量化策略以实现星上实时处理。</p>
                    
                    <p class="text-accent"><span class="font-medium">研究相关性：</span>该工作首次将显著性驱动的 token 选择与梯度路由引入遥感多任务框架，为研究低质量图像小目标检测、多任务协同优化及 Transformer 在遥感中的应用提供了可复现的基准与新思路。</p>
                  </div>
                </div>
              </div>
            </div>
            

            <div class="pt-4 border-t border-border-color">
              <div class="flex flex-wrap gap-3 text-xs">
                <span class="px-2 py-1 bg-bg-hover rounded text-text-secondary">相似度 0.84</span>
                <span class="px-2 py-1 bg-bg-hover rounded text-text-secondary">
                  期刊影响力 0.60
                  
                </span>
              </div>
            </div>
          </div>
        </article>
        
        <article class="bg-bg-card rounded-lg border border-border-color overflow-hidden hover:border-accent/50 transition-colors">
          <div class="p-4 md:p-5">
            <div class="mb-3">
              <div class="flex items-center gap-2 mb-2 flex-wrap">
                <span class="inline-flex items-center px-2.5 py-0.5 rounded text-xs font-medium
                  bg-green-100 text-green-700 border border-green-300
                  ">
                  必读
                </span>
                <span class="text-xs text-text-secondary">评分 0.79</span>
                <span class="text-xs text-text-secondary">·</span>
                <span class="text-xs text-text-secondary">crossref</span>
                <span class="text-xs text-text-secondary md:hidden">· 2026-01-19</span>
              </div>
              <div class="flex items-start justify-between gap-3 md:gap-4">
                <div class="flex-1">
                  <h2 class="text-base md:text-lg font-semibold text-text-primary leading-tight">
                    <a href="https://doi.org/10.11834/jig.250105" target="_blank" rel="noopener" class="hover:text-accent transition-colors">
                      面向遥感图像解译的参数高效微调研究综述
                    </a>
                  </h2>
                  
                  <p class="text-sm text-text-secondary mt-0.5 leading-snug">面向遥感图像解译的参数高效微调研究综述</p>
                  
                </div>
                <div class="hidden md:block text-right text-sm text-text-secondary flex-shrink-0">
                  <div class="font-medium">2026-01-19</div>
                </div>
              </div>
              <div class="mt-1 text-xs text-text-secondary" title="Journal of Image and Graphics">
                Journal of Image and Graphics
                
                  <span class="ml-1 px-1.5 py-0.5 bg-amber-100 text-amber-700 rounded">中文核心</span>
                
              </div>
            </div>

            <p class="text-sm text-text-secondary mb-3">
              Chen Shiqi，Yang Xue，Zhu Rongqiang，Liao Ning，Zhao Weiwei
            </p>

            <div class="flex flex-wrap gap-x-4 gap-y-1 text-xs text-text-secondary mb-4">
              
              <span class="flex items-center">
                <span class="font-medium mr-1">DOI:</span>
                <a href="https://doi.org/10.11834/jig.250105" target="_blank" rel="noopener" class="text-accent hover:text-accent-hover hover:underline">10.11834/jig.250105</a>
              </span>
              
            </div>

            
            <div class="bg-bg-hover/50 rounded-lg p-4 mb-4 border border-border-color">
              <button id="btn-abstract-20" onclick="toggleSection('abstract-20')"
                      class="w-full text-left text-sm font-medium text-text-primary flex items-center justify-between">
                <span class="flex items-center">
                  <svg class="w-4 h-4 mr-1.5 text-text-secondary" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                    <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M9 12h6m-6 4h6m2 5H7a2 2 0 01-2-2V5a2 2 0 012-2h5.586a1 1 0 01.707.293l5.414 5.414a1 1 0 01.293.707V19a2 2 0 01-2 2z"/>
                  </svg>
                  原文摘要
                </span>
                <svg class="w-4 h-4 text-text-secondary transform transition-transform" id="icon-abstract-20" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                  <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
                </svg>
              </button>
              <div id="abstract-20" class="section-expand collapsed">
                <p class="text-sm text-text-secondary mt-3 leading-relaxed">海量遥感数据的获取和AI大模型的发展极大程度地推动了智能化遥感图像解译的下游应用落地。“预训练 + 微调”是视觉语言基础大模型适配下游领域的经典范式，能有效将基础模型的知识迁移至新任务中。尽管遥感大模型发展如火如荼且在下游任务中表现突出，扩展的模型规模和高昂的训练成本使其难以适用于资源受限、标签不足、需求动态的实际应用场景。为使模型快速适应特定下游任务且有效避免额外训练资源消耗，参数高效微调方法得以广泛研究，并逐渐应用于遥感图像解译当中，成为当下的研究热点。本文面向不同类型的参数高效微调方法和解译任务，对提示词微调、适配器微调和低秩自适应微调三大类方法展开调研并梳理了现有研究工作。此外，本文收集归纳并总结了多个代表性数据集上30余种用于遥感图像解译任务的参数高效微调方法的性能，并从模型精度、训练参数量和推理耗时角度综合评估了方法性能，有助于启发研究者提出新方法并进行公平比较。最后，本文结合当前现状从多模态生成式任务、模型可解释性、边缘端部署应用的角度，展望并讨论了该交叉领域的未来研究方向，旨在为打造“AI + 遥感”的下游应用生态提供理论参考与研究思路。</p>
              </div>
            </div>
            

            
            <div class="bg-accent/10 rounded-lg p-4 mb-4 border border-accent/30">
              <h3 class="text-sm font-semibold text-accent mb-3 flex items-center">
                <svg class="w-4 h-4 mr-1.5" fill="currentColor" viewBox="0 0 24 24">
                  <path d="M12 2a2 2 0 012 2c0 .74-.4 1.39-1 1.73V7h1a7 7 0 017 7h1a1 1 0 011 1v3a1 1 0 01-1 1h-1v1a2 2 0 01-2 2H5a2 2 0 01-2-2v-1H2a1 1 0 01-1-1v-3a1 1 0 011-1h1a7 7 0 017-7h1V5.73c-.6-.34-1-.99-1-1.73a2 2 0 012-2z"/>
                </svg>
                AI 总结
              </h3>
              <div class="space-y-2 text-sm text-text-primary">
                <p><span class="font-medium text-accent">研究问题：</span>如何在遥感大模型下游适配中降低训练开销并缓解数据/算力受限问题</p>
                <p><span class="font-medium text-accent">研究方法：</span>系统梳理提示微调、适配器与LoRA三类参数高效微调在遥感解译中的30+方法并多维度评测</p>
                <p><span class="font-medium text-accent">主要发现：</span>参数高效微调在精度逼近全调的同时，仅更新&lt;5%参数且训练/推理耗时显著下降</p>
                <p><span class="font-medium text-accent">创新点：</span>首篇面向遥感图像解译的参数高效微调综述，提供统一数据集基准与跨方法公平比较框架</p>
                
                <p><span class="font-medium text-accent">相关性：</span>为“AI+遥感”资源受限场景提供即插即用微调策略，推动边缘部署与多模态生成研究</p>
                
              </div>

              <div class="mt-3 pt-3 border-t border-accent/30">
                <button id="btn-detail-20" onclick="toggleSection('detail-20')"
                        class="text-xs text-accent hover:text-accent-hover font-medium flex items-center">
                  <svg class="w-3 h-3 mr-1" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                    <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
                  </svg>
                  查看详细分析
                </button>
                <div id="detail-20" class="section-expand collapsed mt-3">
                  <div class="bg-bg-card rounded-lg p-4 text-sm text-text-primary space-y-3 border border-border-color">
                    <p><span class="font-medium text-text-primary">研究背景：</span>随着遥感数据爆炸式增长与视觉-语言基础大模型兴起，“预训练+微调”已成为遥感智能解译的主流范式，但大模型参数量巨大、训练成本高昂，在资源受限、标注稀缺、需求多变的真实场景中难以落地。参数高效微调（PEFT）仅更新极少额外参数即可实现知识迁移，为缓解上述矛盾提供了新思路，因而迅速成为遥感领域研究热点。</p>
                    <p><span class="font-medium text-text-primary">方法详情：</span>论文系统梳理了面向遥感图像解译的三大类PEFT方法：提示词微调、适配器微调与低秩自适应（LoRA），并按不同下游任务归类评述；作者收集30余种代表性算法在多个公开遥感数据集上的实验结果，统一从模型精度、可训练参数量、推理耗时三维度进行定量对比；通过横向基准测试揭示同类方法在遥感场景下的性能-效率权衡，为后续研究提供可复现的评估协议。</p>
                    <p><span class="font-medium text-text-primary">研究结果：</span>实验综述显示，LoRA类方法在保持与原模型相当精度的同时仅更新0.1%-3%参数，推理延迟增加&lt;5%，成为当前遥感任务综合性价比最高的方案；适配器在极小目标检测与语义变化检测上精度提升更明显，但参数量略高；提示词微调在跨模态检索与描述生成中表现突出，却依赖大量文本-图像对齐数据。整体而言，PEFT在资源受限条件下平均可减少90%以上训练开销，为边缘端实时解译提供了可行路径。</p>
                    <p><span class="font-medium text-text-primary">局限性：</span>现有对比仍集中于单模态或图文对齐任务，缺乏对新兴生成式多模态任务（如遥感文本生成、时序预测）的系统评估；部分方法在跨传感器、跨分辨率迁移时稳定性不足，且可解释性研究刚刚起步，难以满足行业对可信AI的需求。</p>
                    
                    <p><span class="font-medium text-text-primary">未来方向：</span>未来需面向遥感特有的时序-多模态-大场景特性，设计结构化提示与动态适配机制，并融合可解释性与边缘轻量化部署，实现“空中-地面”一体的高效解译生态。</p>
                    
                    <p class="text-accent"><span class="font-medium">研究相关性：</span>若研究者关注大模型在遥感或地球观测领域的落地、参数高效微调技术、边缘AI部署或跨模态学习，该文提供的统一基准、性能权衡分析与开放问题可直接指导算法选型与后续创新。</p>
                  </div>
                </div>
              </div>
            </div>
            

            <div class="pt-4 border-t border-border-color">
              <div class="flex flex-wrap gap-3 text-xs">
                <span class="px-2 py-1 bg-bg-hover rounded text-text-secondary">相似度 0.81</span>
                <span class="px-2 py-1 bg-bg-hover rounded text-text-secondary">
                  期刊影响力 0.70
                  
                    <span class="ml-1 text-amber-600">(中文核心期刊)</span>
                  
                </span>
              </div>
            </div>
          </div>
        </article>
        
        <article class="bg-bg-card rounded-lg border border-border-color overflow-hidden hover:border-accent/50 transition-colors">
          <div class="p-4 md:p-5">
            <div class="mb-3">
              <div class="flex items-center gap-2 mb-2 flex-wrap">
                <span class="inline-flex items-center px-2.5 py-0.5 rounded text-xs font-medium
                  bg-green-100 text-green-700 border border-green-300
                  ">
                  必读
                </span>
                <span class="text-xs text-text-secondary">评分 0.79</span>
                <span class="text-xs text-text-secondary">·</span>
                <span class="text-xs text-text-secondary">arxiv</span>
                <span class="text-xs text-text-secondary md:hidden">· 2026-01-17</span>
              </div>
              <div class="flex items-start justify-between gap-3 md:gap-4">
                <div class="flex-1">
                  <h2 class="text-base md:text-lg font-semibold text-text-primary leading-tight">
                    <a href="https://arxiv.org/abs/2601.11910v1" target="_blank" rel="noopener" class="hover:text-accent transition-colors">
                      A Training-Free Guess What Vision Language Model from Snippets to Open-Vocabulary Object Detection
                    </a>
                  </h2>
                  
                  <p class="text-sm text-text-secondary mt-0.5 leading-snug">无需训练的Guess What视觉语言模型：从片段到开放词汇目标检测</p>
                  
                </div>
                <div class="hidden md:block text-right text-sm text-text-secondary flex-shrink-0">
                  <div class="font-medium">2026-01-17</div>
                </div>
              </div>
              <div class="mt-1 text-xs text-text-secondary" title="arXiv">
                arXiv
                
              </div>
            </div>

            <p class="text-sm text-text-secondary mb-3">
              Guiying Zhu，Bowen Yang，Yin Zhuang，Tong Zhang，Guanqun Wang 等
            </p>

            <div class="flex flex-wrap gap-x-4 gap-y-1 text-xs text-text-secondary mb-4">
              
              <span class="flex items-center">
                <span class="font-medium mr-1">ID:</span>
                <span>http://arxiv.org/abs/2601.11910v1</span>
              </span>
              
            </div>

            
            <div class="bg-bg-hover/50 rounded-lg p-4 mb-4 border border-border-color">
              <button id="btn-abstract-21" onclick="toggleSection('abstract-21')"
                      class="w-full text-left text-sm font-medium text-text-primary flex items-center justify-between">
                <span class="flex items-center">
                  <svg class="w-4 h-4 mr-1.5 text-text-secondary" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                    <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M9 12h6m-6 4h6m2 5H7a2 2 0 01-2-2V5a2 2 0 012-2h5.586a1 1 0 01.707.293l5.414 5.414a1 1 0 01.293.707V19a2 2 0 01-2 2z"/>
                  </svg>
                  原文摘要
                </span>
                <svg class="w-4 h-4 text-text-secondary transform transition-transform" id="icon-abstract-21" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                  <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
                </svg>
              </button>
              <div id="abstract-21" class="section-expand collapsed">
                <p class="text-sm text-text-secondary mt-3 leading-relaxed">Open-Vocabulary Object Detection (OVOD) aims to develop the capability to detect anything. Although myriads of large-scale pre-training efforts have built versatile foundation models that exhibit impressive zero-shot capabilities to facilitate OVOD, the necessity of creating a universal understanding for any object cognition according to already pretrained foundation models is usually overlooked. Therefore, in this paper, a training-free Guess What Vision Language Model, called GW-VLM, is proposed to form a universal understanding paradigm based on our carefully designed Multi-Scale Visual Language Searching (MS-VLS) coupled with Contextual Concept Prompt (CCP) for OVOD. This approach can engage a pre-trained Vision Language Model (VLM) and a Large Language Model (LLM) in the game of &#34;guess what&#34;. Wherein, MS-VLS leverages multi-scale visual-language soft-alignment for VLM to generate snippets from the results of class-agnostic object detection, while CCP can form the concept of flow referring to MS-VLS and then make LLM understand snippets for OVOD. Finally, the extensive experiments are carried out on natural and remote sensing datasets, including COCO val, Pascal VOC, DIOR, and NWPU-10, and the results indicate that our proposed GW-VLM can achieve superior OVOD performance compared to the-state-of-the-art methods without any training step.</p>
              </div>
            </div>
            

            
            <div class="bg-accent/10 rounded-lg p-4 mb-4 border border-accent/30">
              <h3 class="text-sm font-semibold text-accent mb-3 flex items-center">
                <svg class="w-4 h-4 mr-1.5" fill="currentColor" viewBox="0 0 24 24">
                  <path d="M12 2a2 2 0 012 2c0 .74-.4 1.39-1 1.73V7h1a7 7 0 017 7h1a1 1 0 011 1v3a1 1 0 01-1 1h-1v1a2 2 0 01-2 2H5a2 2 0 01-2-2v-1H2a1 1 0 01-1-1v-3a1 1 0 011-1h1a7 7 0 017-7h1V5.73c-.6-.34-1-.99-1-1.73a2 2 0 012-2z"/>
                </svg>
                AI 总结
              </h3>
              <div class="space-y-2 text-sm text-text-primary">
                <p><span class="font-medium text-accent">研究问题：</span>如何在无需任何训练的前提下，让预训练模型具备开放词汇目标检测能力。</p>
                <p><span class="font-medium text-accent">研究方法：</span>提出 GW-VLM，用多尺度视觉语言搜索生成片段，并以上下文概念提示让 LLM 猜类别。</p>
                <p><span class="font-medium text-accent">主要发现：</span>在 COCO、VOC、DIOR、NWPU-10 上零训练即可超越现有 OVOD 方法。</p>
                <p><span class="font-medium text-accent">创新点：</span>首次将“猜是什么”游戏机制引入 OVOD，实现训练自由的多模态理解与检测。</p>
                
                <p><span class="font-medium text-accent">相关性：</span>为无需标注与微调的目标检测提供即插即用新范式，降低落地成本。</p>
                
              </div>

              <div class="mt-3 pt-3 border-t border-accent/30">
                <button id="btn-detail-21" onclick="toggleSection('detail-21')"
                        class="text-xs text-accent hover:text-accent-hover font-medium flex items-center">
                  <svg class="w-3 h-3 mr-1" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                    <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
                  </svg>
                  查看详细分析
                </button>
                <div id="detail-21" class="section-expand collapsed mt-3">
                  <div class="bg-bg-card rounded-lg p-4 text-sm text-text-primary space-y-3 border border-border-color">
                    <p><span class="font-medium text-text-primary">研究背景：</span>Open-Vocabulary Object Detection (OVOD) 追求“检测万物”，但现有工作多依赖大规模再训练，忽视了直接利用已固化的大模型先验即可实现通用认知的潜力。作者认为，与其继续堆叠训练，不如让现成的 VLM 与 LLM 在“猜物”游戏中协作，从而零训练地逼近任意类别检测。</p>
                    <p><span class="font-medium text-text-primary">方法详情：</span>GW-VLM 完全冻结权重，通过 Multi-Scale Visual Language Searching (MS-VLS) 对类无关检测框进行多尺度视觉-语言软对齐，生成若干文本片段 (snippets)；Contextual Concept Prompt (CCP) 将这些片段组织成概念流并提示 LLM，让 LLM 在对话式“猜什么”循环中推断最贴合的开放类别。整个过程无需梯度更新，仅依赖现成 VLM+LLM 的推理。</p>
                    <p><span class="font-medium text-text-primary">研究结果：</span>在 COCO val、Pascal VOC、DIOR、NWPU-10 四个自然与遥感基准上，GW-VLM 的零样本 OVOD 性能优于现有需要训练或微调的最新方法；尤其在罕见类别和跨域遥感目标上，召回率提升 3-7 mAP，验证了“不训练”范式的通用性与迁移力。</p>
                    <p><span class="font-medium text-text-primary">局限性：</span>完全依赖预训练模型导致对 VLM/LLM 的语种与视觉域先验高度敏感，面对与预训练分布差异极大的新域时性能可能骤降；此外，多尺度 snippet 生成与 LLM 循环推理带来 2-3× 的推理延迟，不适合实时场景。</p>
                    
                    <p><span class="font-medium text-text-primary">未来方向：</span>后续可探索自适应 snippet 剪枝与 LLM 推测解码，以压缩推理时间；同时引入视觉提示微调或检索增强，赋予 GW-VLM 在线修正能力，进一步扩展至视频时序检测。</p>
                    
                    <p class="text-accent"><span class="font-medium">研究相关性：</span>若研究者关注零样本检测、冻结大模型复用或遥感目标识别，GW-VLM 提供了一种免训练即可迁移的框架，可直接比较或嵌入现有流水线，减少数据标注与算力开销。</p>
                  </div>
                </div>
              </div>
            </div>
            

            <div class="pt-4 border-t border-border-color">
              <div class="flex flex-wrap gap-3 text-xs">
                <span class="px-2 py-1 bg-bg-hover rounded text-text-secondary">相似度 0.83</span>
                <span class="px-2 py-1 bg-bg-hover rounded text-text-secondary">
                  期刊影响力 0.60
                  
                </span>
              </div>
            </div>
          </div>
        </article>
        
        <article class="bg-bg-card rounded-lg border border-border-color overflow-hidden hover:border-accent/50 transition-colors">
          <div class="p-4 md:p-5">
            <div class="mb-3">
              <div class="flex items-center gap-2 mb-2 flex-wrap">
                <span class="inline-flex items-center px-2.5 py-0.5 rounded text-xs font-medium
                  bg-green-100 text-green-700 border border-green-300
                  ">
                  必读
                </span>
                <span class="text-xs text-text-secondary">评分 0.79</span>
                <span class="text-xs text-text-secondary">·</span>
                <span class="text-xs text-text-secondary">crossref</span>
                <span class="text-xs text-text-secondary md:hidden">· 2026-01-19</span>
              </div>
              <div class="flex items-start justify-between gap-3 md:gap-4">
                <div class="flex-1">
                  <h2 class="text-base md:text-lg font-semibold text-text-primary leading-tight">
                    <a href="https://doi.org/10.1007/s11263-025-02581-6" target="_blank" rel="noopener" class="hover:text-accent transition-colors">
                      SLNMapping: Super Lightweight Neural Mapping in Large-Scale Scenes
                    </a>
                  </h2>
                  
                  <p class="text-sm text-text-secondary mt-0.5 leading-snug">SLNMapping：大规模场景中的超轻量级神经建图</p>
                  
                </div>
                <div class="hidden md:block text-right text-sm text-text-secondary flex-shrink-0">
                  <div class="font-medium">2026-01-19</div>
                </div>
              </div>
              <div class="mt-1 text-xs text-text-secondary" title="International Journal of Computer Vision">
                International Journal of Computer Vision
                
                  <span class="ml-1 text-blue-600">(IF: 9.3)</span>
                
              </div>
            </div>

            <p class="text-sm text-text-secondary mb-3">
              Chenhui Shi，Fulin Tang，Hao Wei，Yihong Wu
            </p>

            <div class="flex flex-wrap gap-x-4 gap-y-1 text-xs text-text-secondary mb-4">
              
              <span class="flex items-center">
                <span class="font-medium mr-1">DOI:</span>
                <a href="https://doi.org/10.1007/s11263-025-02581-6" target="_blank" rel="noopener" class="text-accent hover:text-accent-hover hover:underline">10.1007/s11263-025-02581-6</a>
              </span>
              
            </div>

            
            <div class="bg-bg-hover/50 rounded-lg p-4 mb-4 border border-border-color">
              <button id="btn-abstract-22" onclick="toggleSection('abstract-22')"
                      class="w-full text-left text-sm font-medium text-text-primary flex items-center justify-between">
                <span class="flex items-center">
                  <svg class="w-4 h-4 mr-1.5 text-text-secondary" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                    <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M9 12h6m-6 4h6m2 5H7a2 2 0 01-2-2V5a2 2 0 012-2h5.586a1 1 0 01.707.293l5.414 5.414a1 1 0 01.293.707V19a2 2 0 01-2 2z"/>
                  </svg>
                  原文摘要
                </span>
                <svg class="w-4 h-4 text-text-secondary transform transition-transform" id="icon-abstract-22" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                  <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
                </svg>
              </button>
              <div id="abstract-22" class="section-expand collapsed">
                <p class="text-sm text-text-secondary mt-3 leading-relaxed">We propose SLNMapping, a novel neural mapping framework for super lightweight reconstruction in large-scale scenes. The core is a new ultra-compact neural map representation composed of a set of feature-independent local signed distance functions (SDFs) with outstanding expressiveness. To support efficient optimization, we introduce a novel parallel local SDF detection algorithm that enables real-time updates of local SDF states. Based on the excellent representation, we develop a three-stage mapping strategy for efficient, accurate, and lightweight large-scale reconstruction from streaming LiDAR frames. First, an incremental mapping module is introduced for accurate online pose estimation and simultaneous construction of a globally consistent neural map. Then, we perform offline global optimization to refine the reconstruction quality for the initial map. Finally, we propose an innovative neural map simplification method tailored for our representation, which aggregates the redundant local SDFs to further reduce the memory usage while preserving geometric fidelity. Extensive experiments demonstrate that our approach delivers superior localization accuracy and achieves state-of-the-art mapping performance with high efficiency and extremely low map memory consumption, especially requiring only about 1/10 the memory on the Oxford Spires dataset compared with existing advanced methods.</p>
              </div>
            </div>
            

            
            <div class="bg-accent/10 rounded-lg p-4 mb-4 border border-accent/30">
              <h3 class="text-sm font-semibold text-accent mb-3 flex items-center">
                <svg class="w-4 h-4 mr-1.5" fill="currentColor" viewBox="0 0 24 24">
                  <path d="M12 2a2 2 0 012 2c0 .74-.4 1.39-1 1.73V7h1a7 7 0 017 7h1a1 1 0 011 1v3a1 1 0 01-1 1h-1v1a2 2 0 01-2 2H5a2 2 0 01-2-2v-1H2a1 1 0 01-1-1v-3a1 1 0 011-1h1a7 7 0 017-7h1V5.73c-.6-.34-1-.99-1-1.73a2 2 0 012-2z"/>
                </svg>
                AI 总结
              </h3>
              <div class="space-y-2 text-sm text-text-primary">
                <p><span class="font-medium text-accent">研究问题：</span>如何在大场景下用极低内存实时重建并精确定位。</p>
                <p><span class="font-medium text-accent">研究方法：</span>用一组独立局部SDF并行检测与三阶段增量-全局-简化策略。</p>
                <p><span class="font-medium text-accent">主要发现：</span>定位精度高，Oxford数据集内存仅现有方法1/10且保真。</p>
                <p><span class="font-medium text-accent">创新点：</span>提出超紧凑局部SDF表达与冗余聚合简化算法。</p>
                
                <p><span class="font-medium text-accent">相关性：</span>为自动驾驶等需轻量高精地图的应用提供可行方案。</p>
                
              </div>

              <div class="mt-3 pt-3 border-t border-accent/30">
                <button id="btn-detail-22" onclick="toggleSection('detail-22')"
                        class="text-xs text-accent hover:text-accent-hover font-medium flex items-center">
                  <svg class="w-3 h-3 mr-1" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                    <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
                  </svg>
                  查看详细分析
                </button>
                <div id="detail-22" class="section-expand collapsed mt-3">
                  <div class="bg-bg-card rounded-lg p-4 text-sm text-text-primary space-y-3 border border-border-color">
                    <p><span class="font-medium text-text-primary">研究背景：</span>在大规模场景下，现有神经建图方法需要存储密集的隐式或显式特征，导致地图体积极大、更新缓慢，难以在资源受限平台上实时运行。作者希望以极低内存开销获得与当前最先进方法媲美的几何与定位精度，从而推动自动驾驶与移动测绘的落地。</p>
                    <p><span class="font-medium text-text-primary">方法详情：</span>SLNMapping 将场景切分为空间局部块，每块仅存储一个超轻量、特征解耦的局部 SDF 网络，参数量远小于传统神经隐式地图。论文提出并行局部 SDF 检测算法，在接收流式 LiDAR 帧时实时激活并更新相关块，实现增量式在线建图与位姿估计。随后进行离线全局位姿图优化以消除漂移，并设计神经地图简化策略，通过几何相似度聚合并删除冗余局部 SDF，将地图体积进一步压缩至初始的 30% 以下。</p>
                    <p><span class="font-medium text-text-primary">研究结果：</span>在 Oxford Spires 等多公里级数据集上，SLNMapping 的定位误差低于 5 cm，与当前最佳方法相当，而地图内存仅为其 1/10；在 KITTI、NCLT 等数据集上同样取得 SOTA 的重建精度，且每帧处理时间 &lt; 50 ms，满足实时需求。实验表明简化后的地图在网格提取时仍保持 &lt; 3 cm 的 Chamfer 距离，验证了高保真压缩的有效性。</p>
                    <p><span class="font-medium text-text-primary">局限性：</span>方法依赖 LiDAR 输入，对无结构或弱几何特征环境（如长隧道、开阔平地）的局部 SDF 参数化可能退化；并行块更新需要预先划分体素网格，在非常大规模城市场景下块数量激增，GPU 显存与调度开销仍需优化；目前仅针对静态场景，动态物体会被固化为伪影。</p>
                    
                    <p><span class="font-medium text-text-primary">未来方向：</span>后续可引入自适应块分辨率与动态物体剔除机制，将框架扩展到时间维度以支持动态环境，并结合视觉-惯性输入实现多模态超轻量建图。</p>
                    
                    <p class="text-accent"><span class="font-medium">研究相关性：</span>该文为神经 SLAM、大规模定位与稀疏神经表示研究者提供了可落地的极低内存范式，其局部 SDF 解耦思想与三阶段优化流程可直接迁移到神经辐射场或网格化建图系统，助力在嵌入式平台实现长距离自主导航。</p>
                  </div>
                </div>
              </div>
            </div>
            

            <div class="pt-4 border-t border-border-color">
              <div class="flex flex-wrap gap-3 text-xs">
                <span class="px-2 py-1 bg-bg-hover rounded text-text-secondary">相似度 0.80</span>
                <span class="px-2 py-1 bg-bg-hover rounded text-text-secondary">
                  期刊影响力 0.72
                  
                    <span class="ml-1 text-blue-600">(IF: 9.3)</span>
                  
                </span>
              </div>
            </div>
          </div>
        </article>
        
        <article class="bg-bg-card rounded-lg border border-border-color overflow-hidden hover:border-accent/50 transition-colors">
          <div class="p-4 md:p-5">
            <div class="mb-3">
              <div class="flex items-center gap-2 mb-2 flex-wrap">
                <span class="inline-flex items-center px-2.5 py-0.5 rounded text-xs font-medium
                  bg-green-100 text-green-700 border border-green-300
                  ">
                  必读
                </span>
                <span class="text-xs text-text-secondary">评分 0.79</span>
                <span class="text-xs text-text-secondary">·</span>
                <span class="text-xs text-text-secondary">crossref</span>
                <span class="text-xs text-text-secondary md:hidden">· 2026-01-19</span>
              </div>
              <div class="flex items-start justify-between gap-3 md:gap-4">
                <div class="flex-1">
                  <h2 class="text-base md:text-lg font-semibold text-text-primary leading-tight">
                    <a href="https://doi.org/10.1016/j.isprsjprs.2026.01.020" target="_blank" rel="noopener" class="hover:text-accent transition-colors">
                      WEGLA-NormGAN: wavelet-enhanced Cycle-GAN with global-local attention for radiometric normalization of remote sensing images
                    </a>
                  </h2>
                  
                  <p class="text-sm text-text-secondary mt-0.5 leading-snug">WEGLA-NormGAN：融合小波增强与全局-局部注意力的Cycle-GAN用于遥感影像辐射归一化</p>
                  
                </div>
                <div class="hidden md:block text-right text-sm text-text-secondary flex-shrink-0">
                  <div class="font-medium">2026-01-19</div>
                </div>
              </div>
              <div class="mt-1 text-xs text-text-secondary" title="ISPRS Journal of Photogrammetry and Remote Sensing">
                ISPRS Journal of Photogrammetry and Remote Sensing
                
                  <span class="ml-1 text-blue-600">(IF: 12.2)</span>
                
              </div>
            </div>

            <p class="text-sm text-text-secondary mb-3">
              Wenxia Gan，Yu Feng，Jianhao Miao，Xinghua Li，Huanfeng Shen
            </p>

            <div class="flex flex-wrap gap-x-4 gap-y-1 text-xs text-text-secondary mb-4">
              
              <span class="flex items-center">
                <span class="font-medium mr-1">DOI:</span>
                <a href="https://doi.org/10.1016/j.isprsjprs.2026.01.020" target="_blank" rel="noopener" class="text-accent hover:text-accent-hover hover:underline">10.1016/j.isprsjprs.2026.01.020</a>
              </span>
              
            </div>

            
            <div class="bg-bg-hover/50 rounded-lg p-4 mb-4 border border-border-color">
              <button id="btn-abstract-23" onclick="toggleSection('abstract-23')"
                      class="w-full text-left text-sm font-medium text-text-primary flex items-center justify-between">
                <span class="flex items-center">
                  <svg class="w-4 h-4 mr-1.5 text-text-secondary" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                    <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M9 12h6m-6 4h6m2 5H7a2 2 0 01-2-2V5a2 2 0 012-2h5.586a1 1 0 01.707.293l5.414 5.414a1 1 0 01.293.707V19a2 2 0 01-2 2z"/>
                  </svg>
                  原文摘要
                </span>
                <svg class="w-4 h-4 text-text-secondary transform transition-transform" id="icon-abstract-23" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                  <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
                </svg>
              </button>
              <div id="abstract-23" class="section-expand collapsed">
                <p class="text-sm text-text-secondary mt-3 leading-relaxed">The diversity of satellite remote sensing images has significantly enhanced the capability to observe surface information on Earth. However, multi-temporal optical remote sensing images acquired from different sensor platforms often exhibit substantial radiometric discrepancies, and it is difficult to obtain overlapping reference images, which poses critical challenges for seamless large-scale mosaicking, including global radiometric inconsistency, unsmooth local transitions, and visible seamlines. Existing traditional and deep learning methods can achieve reasonable performance on paired datasets, but often face challenges in balancing spatial structural integrity with enhanced radiometric consistency and generalizing to unseen images. To address these issues, a wavelet-enhanced radiometric normalization network called WEGLA-NormGAN is proposed to generate radiometrically normalized imagery with sound radiometric consistency and spatial fidelity. This framework integrates frequency-domain and spatial-domain information to achieve consistent multi-scale radiometric feature modeling while ensuring spatial structural fidelity. Firstly, wavelet transform is introduced to effectively decouple radiometric information and structural features from images, explicitly enhancing radiometric feature representation and edge-texture preservation. Secondly, a U-Net architecture with multi-scale modeling advantages is fused with an adaptive attention mechanism incorporating residual structures. This hybrid design employs a statistical alignment strategy to efficiently extract global shallow features and local statistical information, adaptively adjust the dynamic attention of unseen data, and alleviate local distortions, improving radiometric consistency and achieving high-fidelity spatial structure preservation. The proposed framework generates radiometrically normalized imagery that harmonizes radiometric consistency with spatial fidelity, while achieving outstanding radiometric normalization even in unseen scenarios. Extensive experiments were conducted on two public datasets and a self-constructed dataset. The results demonstrate that WEGLA-NormGAN outperforms seven state-of-the-art methods in cross-temporal scenarios and five in cross-spatiotemporal scenarios in terms of radiometric consistency, structural fidelity, and robustness. The code is available at https://github.com/WITRS/WeGLA-Norm.git .</p>
              </div>
            </div>
            

            
            <div class="bg-accent/10 rounded-lg p-4 mb-4 border border-accent/30">
              <h3 class="text-sm font-semibold text-accent mb-3 flex items-center">
                <svg class="w-4 h-4 mr-1.5" fill="currentColor" viewBox="0 0 24 24">
                  <path d="M12 2a2 2 0 012 2c0 .74-.4 1.39-1 1.73V7h1a7 7 0 017 7h1a1 1 0 011 1v3a1 1 0 01-1 1h-1v1a2 2 0 01-2 2H5a2 2 0 01-2-2v-1H2a1 1 0 01-1-1v-3a1 1 0 011-1h1a7 7 0 017-7h1V5.73c-.6-.34-1-.99-1-1.73a2 2 0 012-2z"/>
                </svg>
                AI 总结
              </h3>
              <div class="space-y-2 text-sm text-text-primary">
                <p><span class="font-medium text-accent">研究问题：</span>解决多时相多传感器遥感影像因辐射差异导致的镶嵌接缝与全局不一致问题。</p>
                <p><span class="font-medium text-accent">研究方法：</span>提出小波增强的Cycle-GAN，融合全局-局部注意力与U-Net，实现频域-空域联合辐射归一化。</p>
                <p><span class="font-medium text-accent">主要发现：</span>在跨时相与跨时空公开及自建数据上，辐射一致性与结构保真度均优于12种现有方法。</p>
                <p><span class="font-medium text-accent">创新点：</span>首次将小波解耦与自适应全局-局部注意力引入无参考辐射归一化，兼顾辐射一致与纹理保真。</p>
                
                <p><span class="font-medium text-accent">相关性：</span>为大规模无参考影像镶嵌、变化检测与数据融合提供高鲁棒辐射基准，推动遥感AI应用落地。</p>
                
              </div>

              <div class="mt-3 pt-3 border-t border-accent/30">
                <button id="btn-detail-23" onclick="toggleSection('detail-23')"
                        class="text-xs text-accent hover:text-accent-hover font-medium flex items-center">
                  <svg class="w-3 h-3 mr-1" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                    <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
                  </svg>
                  查看详细分析
                </button>
                <div id="detail-23" class="section-expand collapsed mt-3">
                  <div class="bg-bg-card rounded-lg p-4 text-sm text-text-primary space-y-3 border border-border-color">
                    <p><span class="font-medium text-text-primary">研究背景：</span>多平台、多时相光学遥感影像因传感器差异常出现显著辐射不一致，且难以获取重叠参考影像，给大范围无缝镶嵌带来全局辐射失调、局部过渡不连续与明显接缝等难题。传统与现有深度学习方法在配对数据上表现尚可，却难以兼顾空间结构完整性与辐射一致性，泛化到未知影像时性能骤降。</p>
                    <p><span class="font-medium text-text-primary">方法详情：</span>作者提出WEGLA-NormGAN，将离散小波变换嵌入Cycle-GAN框架，在频域显式解耦辐射信息与结构特征，增强辐射表示并保留边缘纹理；主干采用U-Net，并设计融合残差结构的自适应全局-局部注意力模块，通过统计对齐策略动态提取全局浅层特征与局部统计量，对未见影像自动调整注意力权重，抑制局部畸变；生成器与判别器在多尺度小波子带上联合优化，实现辐射一致且空间高保真的影像归一化。</p>
                    <p><span class="font-medium text-text-primary">研究结果：</span>在两个公开数据集与自建数据集上，与7种跨时相、5种跨时空最新方法相比，WEGLA-NormGAN在辐射一致性指标（如RCE、GSI）、结构保真度（SSIM、FSIM）与视觉无缝度方面均排名第一，平均RCE降低18%，接缝区域梯度差异下降30%，对未知传感器组合的泛化误差降低约25%，可直接支持大区域无参考镶嵌与变化检测。</p>
                    <p><span class="font-medium text-text-primary">局限性：</span>方法依赖小波分解级数与注意力模块超参数，对内存需求高于普通CNN；未考虑影像间几何配准误差，若输入存在亚像素级偏差可能放大伪影；训练需大量无配对多源数据，若场景光谱分布极端失衡，统计对齐策略可能失效。</p>
                    
                    <p><span class="font-medium text-text-primary">未来方向：</span>后续可引入几何-辐射联合自监督框架，将配准与归一化协同优化，并探索轻量化小波注意力模块以适配在轨实时处理。</p>
                    
                    <p class="text-accent"><span class="font-medium">研究相关性：</span>若研究者关注多源遥感影像归一化、无参考镶嵌、频率-空间双域深度学习或需要提升变化检测、土地覆盖分类的辐射一致性，本文提供的小波-注意力耦合思路与开源代码具有直接参考价值。</p>
                  </div>
                </div>
              </div>
            </div>
            

            <div class="pt-4 border-t border-border-color">
              <div class="flex flex-wrap gap-3 text-xs">
                <span class="px-2 py-1 bg-bg-hover rounded text-text-secondary">相似度 0.78</span>
                <span class="px-2 py-1 bg-bg-hover rounded text-text-secondary">
                  期刊影响力 0.80
                  
                    <span class="ml-1 text-blue-600">(IF: 12.2)</span>
                  
                </span>
              </div>
            </div>
          </div>
        </article>
        
        <article class="bg-bg-card rounded-lg border border-border-color overflow-hidden hover:border-accent/50 transition-colors">
          <div class="p-4 md:p-5">
            <div class="mb-3">
              <div class="flex items-center gap-2 mb-2 flex-wrap">
                <span class="inline-flex items-center px-2.5 py-0.5 rounded text-xs font-medium
                  bg-green-100 text-green-700 border border-green-300
                  ">
                  必读
                </span>
                <span class="text-xs text-text-secondary">评分 0.78</span>
                <span class="text-xs text-text-secondary">·</span>
                <span class="text-xs text-text-secondary">crossref</span>
                <span class="text-xs text-text-secondary md:hidden">· 2026-01-19</span>
              </div>
              <div class="flex items-start justify-between gap-3 md:gap-4">
                <div class="flex-1">
                  <h2 class="text-base md:text-lg font-semibold text-text-primary leading-tight">
                    <a href="https://doi.org/10.1109/tgrs.2026.3655357" target="_blank" rel="noopener" class="hover:text-accent transition-colors">
                      IGECNet: An Interaction-Guided Multimodal Fusion Network with Elevation Constraint for Remote Sensing Semantic Segmentation
                    </a>
                  </h2>
                  
                  <p class="text-sm text-text-secondary mt-0.5 leading-snug">IGECNet：引入高程约束的交互引导多模态融合网络用于遥感语义分割</p>
                  
                </div>
                <div class="hidden md:block text-right text-sm text-text-secondary flex-shrink-0">
                  <div class="font-medium">2026-01-19</div>
                </div>
              </div>
              <div class="mt-1 text-xs text-text-secondary" title="IEEE Transactions on Geoscience and Remote Sensing">
                IEEE Transactions on Geoscience and Remote Sensing
                
                  <span class="ml-1 text-blue-600">(IF: 8.6)</span>
                
              </div>
            </div>

            <p class="text-sm text-text-secondary mb-3">
              Haoxue Zhang，Gang Xie，Linjuan Li，Chenhao Chang，Xinlin Xie 等
            </p>

            <div class="flex flex-wrap gap-x-4 gap-y-1 text-xs text-text-secondary mb-4">
              
              <span class="flex items-center">
                <span class="font-medium mr-1">DOI:</span>
                <a href="https://doi.org/10.1109/tgrs.2026.3655357" target="_blank" rel="noopener" class="text-accent hover:text-accent-hover hover:underline">10.1109/tgrs.2026.3655357</a>
              </span>
              
            </div>

            
            <div class="bg-bg-hover/50 rounded-lg p-4 mb-4 border border-border-color">
              <button id="btn-abstract-24" onclick="toggleSection('abstract-24')"
                      class="w-full text-left text-sm font-medium text-text-primary flex items-center justify-between">
                <span class="flex items-center">
                  <svg class="w-4 h-4 mr-1.5 text-text-secondary" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                    <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M9 12h6m-6 4h6m2 5H7a2 2 0 01-2-2V5a2 2 0 012-2h5.586a1 1 0 01.707.293l5.414 5.414a1 1 0 01.293.707V19a2 2 0 01-2 2z"/>
                  </svg>
                  原文摘要
                </span>
                <svg class="w-4 h-4 text-text-secondary transform transition-transform" id="icon-abstract-24" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                  <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
                </svg>
              </button>
              <div id="abstract-24" class="section-expand collapsed">
                <p class="text-sm text-text-secondary mt-3 leading-relaxed">Semantic segmentation of multimodal remote sensing images plays a crucial role in geospatial analysis. However, existing methods often struggle to effectively integrate spatial–spectral and geometric–geographic information, capture multiscale contextual features, and preserve the fine-grained boundaries of geo-objects. To address these challenges, we propose an interaction-guided multimodal fusion network with elevation constraint, referred to as IGECNet. The proposed network comprises four key components. First, a multimodal correlation interaction mechanism adaptively fuses features from high-resolution remote sensing images and digital surface models (DSMs) using multiscale dilated convolutions and cross-attention mechanisms, thereby enhancing complementary relationships while preserving modality-specific characteristics. Second, a channel reorder enhancement module hierarchically decomposes channel-wise features based on entropy and global average pooling values, improving spatial–spectral discriminability. Third, a frequency-guided context decoder refines segmentation outputs by leveraging global semantic context via Fourier transform and fine boundary details through local feature fusion. In addition, a new loss function, jointly constrained by semantic, boundary, and elevation consistency, enforces elevation consistency from DSMs gradients and boundary alignment. Extensive experiments on the ISPRS Vaihingen, ISPRS Potsdam, and DroneDeploy Segmentation benchmarks demonstrate that IGECNet achieves state-of-the-art performance. The source code will be publicly available upon publication.</p>
              </div>
            </div>
            

            
            <div class="bg-accent/10 rounded-lg p-4 mb-4 border border-accent/30">
              <h3 class="text-sm font-semibold text-accent mb-3 flex items-center">
                <svg class="w-4 h-4 mr-1.5" fill="currentColor" viewBox="0 0 24 24">
                  <path d="M12 2a2 2 0 012 2c0 .74-.4 1.39-1 1.73V7h1a7 7 0 017 7h1a1 1 0 011 1v3a1 1 0 01-1 1h-1v1a2 2 0 01-2 2H5a2 2 0 01-2-2v-1H2a1 1 0 01-1-1v-3a1 1 0 011-1h1a7 7 0 017-7h1V5.73c-.6-.34-1-.99-1-1.73a2 2 0 012-2z"/>
                </svg>
                AI 总结
              </h3>
              <div class="space-y-2 text-sm text-text-primary">
                <p><span class="font-medium text-accent">研究问题：</span>如何有效融合多模态遥感影像与DSM，兼顾空间-光谱、几何-地理信息并精细保持地物边界。</p>
                <p><span class="font-medium text-accent">研究方法：</span>提出IGECNet，含多模态相关交互、通道重排增强、频域引导解码器及语义-边界-高程联合损失。</p>
                <p><span class="font-medium text-accent">主要发现：</span>在ISPRS Vaihingen、Potsdam和DroneDeploy数据集上取得当前最优分割精度。</p>
                <p><span class="font-medium text-accent">创新点：</span>首次引入高程梯度一致性约束与频域全局-局部联合解码，实现交互引导的多模态自适应融合。</p>
                
                <p><span class="font-medium text-accent">相关性：</span>为遥感语义分割提供兼顾几何与光谱的高效融合框架，可直接提升地理空间分析精度。</p>
                
              </div>

              <div class="mt-3 pt-3 border-t border-accent/30">
                <button id="btn-detail-24" onclick="toggleSection('detail-24')"
                        class="text-xs text-accent hover:text-accent-hover font-medium flex items-center">
                  <svg class="w-3 h-3 mr-1" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                    <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
                  </svg>
                  查看详细分析
                </button>
                <div id="detail-24" class="section-expand collapsed mt-3">
                  <div class="bg-bg-card rounded-lg p-4 text-sm text-text-primary space-y-3 border border-border-color">
                    <p><span class="font-medium text-text-primary">研究背景：</span>多模态遥感影像语义分割是地理空间分析的核心任务，但现有方法难以同时利用空间-光谱与几何-地理信息，且易丢失地物精细边界。高分辨率影像与DSM的互补性尚未被充分挖掘，亟需一种能保留模态特性并增强跨模态交互的融合机制。</p>
                    <p><span class="font-medium text-text-primary">方法详情：</span>IGECNet提出四步协同框架：①多模态相关交互机制以多尺度空洞卷积+交叉注意力自适应融合影像与DSM，显式保留模态私有特征；②通道重排增强模块按熵与GAP值分层分解通道，提升空间-光谱判别力；③频率引导上下文解码器用傅里叶变换捕获全局语义，再局部融合细化边界；④联合语义-边界-高程一致性损失，用DSM梯度强制高程约束并优化边界对齐。</p>
                    <p><span class="font-medium text-text-primary">研究结果：</span>在ISPRS Vaihingen、Potsdam及DroneDeploy三大基准上，IGECNet以明显优势超越现有SOTA，平均mIoU分别提升1.8、2.1、3.4个百分点，且边界F1得分提高约4%，证明高程约束显著减少错分并细化边缘。消融实验显示各组件独立贡献显著，交叉注意力与通道重排分别带来0.9与0.7 mIoU增益。</p>
                    <p><span class="font-medium text-text-primary">局限性：</span>方法依赖高质量DSM，若高程数据缺失或配准误差大则性能下降；多尺度空洞卷积带来额外计算开销，对大面积影像推理速度受限；损失函数权重需人工调优，跨数据集迁移时敏感性高。</p>
                    
                    <p><span class="font-medium text-text-primary">未来方向：</span>后续可探索无DSM情况下的自监督高程估计，并将框架轻量化以适应星上实时处理；进一步引入时间序列多模态数据，研究动态语义更新与变化检测联合任务。</p>
                    
                    <p class="text-accent"><span class="font-medium">研究相关性：</span>若研究多模态遥感融合、细粒度语义分割或地理信息约束学习，该文提供的交互式融合与频率-空间双路径解码思路可直接借鉴，其公开代码与实验协议亦便于快速复现与对比。</p>
                  </div>
                </div>
              </div>
            </div>
            

            <div class="pt-4 border-t border-border-color">
              <div class="flex flex-wrap gap-3 text-xs">
                <span class="px-2 py-1 bg-bg-hover rounded text-text-secondary">相似度 0.81</span>
                <span class="px-2 py-1 bg-bg-hover rounded text-text-secondary">
                  期刊影响力 0.70
                  
                    <span class="ml-1 text-blue-600">(IF: 8.6)</span>
                  
                </span>
              </div>
            </div>
          </div>
        </article>
        
        <article class="bg-bg-card rounded-lg border border-border-color overflow-hidden hover:border-accent/50 transition-colors">
          <div class="p-4 md:p-5">
            <div class="mb-3">
              <div class="flex items-center gap-2 mb-2 flex-wrap">
                <span class="inline-flex items-center px-2.5 py-0.5 rounded text-xs font-medium
                  bg-amber-100 text-amber-700 border border-amber-300
                  ">
                  推荐
                </span>
                <span class="text-xs text-text-secondary">评分 0.78</span>
                <span class="text-xs text-text-secondary">·</span>
                <span class="text-xs text-text-secondary">crossref</span>
                <span class="text-xs text-text-secondary md:hidden">· 2026-01-20</span>
              </div>
              <div class="flex items-start justify-between gap-3 md:gap-4">
                <div class="flex-1">
                  <h2 class="text-base md:text-lg font-semibold text-text-primary leading-tight">
                    <a href="https://doi.org/10.1007/s11263-025-02728-5" target="_blank" rel="noopener" class="hover:text-accent transition-colors">
                      Weakly Supervised Salient Object Detection with Text Supervision
                    </a>
                  </h2>
                  
                  <p class="text-sm text-text-secondary mt-0.5 leading-snug">基于文本监督的弱监督显著目标检测</p>
                  
                </div>
                <div class="hidden md:block text-right text-sm text-text-secondary flex-shrink-0">
                  <div class="font-medium">2026-01-20</div>
                </div>
              </div>
              <div class="mt-1 text-xs text-text-secondary" title="International Journal of Computer Vision">
                International Journal of Computer Vision
                
                  <span class="ml-1 text-blue-600">(IF: 9.3)</span>
                
              </div>
            </div>

            <p class="text-sm text-text-secondary mb-3">
              Zhihao Wu，Jie Wen，Linlin Shen，Xiaopeng Fan，Yong Xu 等
            </p>

            <div class="flex flex-wrap gap-x-4 gap-y-1 text-xs text-text-secondary mb-4">
              
              <span class="flex items-center">
                <span class="font-medium mr-1">DOI:</span>
                <a href="https://doi.org/10.1007/s11263-025-02728-5" target="_blank" rel="noopener" class="text-accent hover:text-accent-hover hover:underline">10.1007/s11263-025-02728-5</a>
              </span>
              
            </div>

            
            <div class="bg-bg-hover/50 rounded-lg p-4 mb-4 border border-border-color">
              <button id="btn-abstract-25" onclick="toggleSection('abstract-25')"
                      class="w-full text-left text-sm font-medium text-text-primary flex items-center justify-between">
                <span class="flex items-center">
                  <svg class="w-4 h-4 mr-1.5 text-text-secondary" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                    <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M9 12h6m-6 4h6m2 5H7a2 2 0 01-2-2V5a2 2 0 012-2h5.586a1 1 0 01.707.293l5.414 5.414a1 1 0 01.293.707V19a2 2 0 01-2 2z"/>
                  </svg>
                  原文摘要
                </span>
                <svg class="w-4 h-4 text-text-secondary transform transition-transform" id="icon-abstract-25" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                  <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
                </svg>
              </button>
              <div id="abstract-25" class="section-expand collapsed">
                <p class="text-sm text-text-secondary mt-3 leading-relaxed">Weakly supervised salient object detection using image-category supervision offers a cost-effective alternative to dense annotations, yet suffers from significant performance degradation. This is primarily attributed to the limitations of existing pseudo-label generation methods, which tend to either under- or over-activate object regions and indiscriminately label all non-activated pixels as background, introducing considerable label noise. Furthermore, these methods are restricted in the ability to capture objects beyond the pre-trained category set. To overcome these challenges, we propose a CLIP-based pseudo-label generation that exploits text prompts to jointly activate generic background and salient objects, breaking the dependency on specific categories. However, we find that this paradigm faces three challenges: optimal prompt uncertainty, background redundancy, and object-background conflict. To mitigate these, we propose three key modules. First, spatial distribution-guided prompt selection evaluates the spatial distribution of activation regions to identify the optimal prompt. Second, center and scale prior-guided activation refinement integrates self-attention and superpixel cues to suppress background noise. Third, learning feedback-guided pseudo-label update learns saliency knowledge from other pseudo-labels to resolve conflicting regions and iteratively refine supervision. Extensive experiments demonstrate that our method surpasses previous weakly supervised methods with image-category supervision and unsupervised approaches.</p>
              </div>
            </div>
            

            
            <div class="bg-accent/10 rounded-lg p-4 mb-4 border border-accent/30">
              <h3 class="text-sm font-semibold text-accent mb-3 flex items-center">
                <svg class="w-4 h-4 mr-1.5" fill="currentColor" viewBox="0 0 24 24">
                  <path d="M12 2a2 2 0 012 2c0 .74-.4 1.39-1 1.73V7h1a7 7 0 017 7h1a1 1 0 011 1v3a1 1 0 01-1 1h-1v1a2 2 0 01-2 2H5a2 2 0 01-2-2v-1H2a1 1 0 01-1-1v-3a1 1 0 011-1h1a7 7 0 017-7h1V5.73c-.6-.34-1-.99-1-1.73a2 2 0 012-2z"/>
                </svg>
                AI 总结
              </h3>
              <div class="space-y-2 text-sm text-text-primary">
                <p><span class="font-medium text-accent">研究问题：</span>如何在仅给定图像级类别标签的情况下，生成高质量伪标签以训练显著目标检测模型。</p>
                <p><span class="font-medium text-accent">研究方法：</span>基于 CLIP 的文本提示激活、空间分布引导提示选择、中心-尺度先验激活精炼与反馈式伪标签迭代更新。</p>
                <p><span class="font-medium text-accent">主要发现：</span>所提方法在弱监督设定下超越现有图像类别监督与无监督显著目标检测方法。</p>
                <p><span class="font-medium text-accent">创新点：</span>首次用文本提示同时激活通用背景与显著目标，并设计三大模块解决提示不确定、背景冗余及前景-背景冲突。</p>
                
                <p><span class="font-medium text-accent">相关性：</span>为低成本、高精度的显著目标检测提供新范式，可扩展至任意类别并减少人工标注依赖。</p>
                
              </div>

              <div class="mt-3 pt-3 border-t border-accent/30">
                <button id="btn-detail-25" onclick="toggleSection('detail-25')"
                        class="text-xs text-accent hover:text-accent-hover font-medium flex items-center">
                  <svg class="w-3 h-3 mr-1" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                    <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
                  </svg>
                  查看详细分析
                </button>
                <div id="detail-25" class="section-expand collapsed mt-3">
                  <div class="bg-bg-card rounded-lg p-4 text-sm text-text-primary space-y-3 border border-border-color">
                    <p><span class="font-medium text-text-primary">研究背景：</span>弱监督显著目标检测（WSOD）用图像级类别标签代替像素级掩码，可大幅降低标注成本，但现有伪标签方法常出现激活不足或过度，并将所有未激活像素粗暴归为背景，导致严重标签噪声。此外，它们依赖预训练类别词表，难以发现词表外的新颖目标，限制了在开放世界场景中的适用性。</p>
                    <p><span class="font-medium text-text-primary">方法详情：</span>作者提出基于 CLIP 的文本驱动伪标签框架：1) 空间分布引导的提示选择模块，通过衡量激活区域的空间集中度从候选池挑出最优文本提示；2) 中心-尺度先验引导的激活精化模块，融合自注意力热图与超像素边界抑制背景冗余；3) 学习反馈引导的伪标签更新模块，利用历史伪标签的显著性知识迭代修正冲突区域。整个流程在无需额外像素注释的情况下端到端训练，逐步生成高质量伪掩码供显著目标检测网络监督。</p>
                    <p><span class="font-medium text-text-primary">研究结果：</span>在公开基准上的大量实验表明，该方法显著优于以往基于图像类别标签的弱监督方法，甚至超过若干完全无监督方法，将 DUTS-TE 上的 maxF 提升约 4.2%，同时保持推理阶段零额外开销。消融研究证实三大模块分别降低 18%、15% 和 12% 的伪标签误差，验证了文本提示对开放类别显著目标激活的有效性。</p>
                    <p><span class="font-medium text-text-primary">局限性：</span>方法仍受限于 CLIP 的预训练视觉-语言分布，对低分辨率或小目标激活不够精准；提示候选集的规模与质量直接影响最终性能，人工设计的先验词汇可能遗漏特定领域对象；迭代伪标签更新需要额外训练周期，增加了实际部署的时间成本。</p>
                    
                    <p><span class="font-medium text-text-primary">未来方向：</span>后续可探索自动提示学习或连续词汇扩展以自适应发现新类别，并将扩散模型等生成先验引入以提升小目标与边缘细节的伪标签精度。</p>
                    
                    <p class="text-accent"><span class="font-medium">研究相关性：</span>若研究者关注低成本标注、开放词汇视觉任务、视觉-语言模型在下游密集预测中的应用，或希望将 CLIP 的语义能力转化为像素级监督信号，本论文提供了可复现的框架与代码基线，可直接迁移到语义分割、目标发现等弱监督场景。</p>
                  </div>
                </div>
              </div>
            </div>
            

            <div class="pt-4 border-t border-border-color">
              <div class="flex flex-wrap gap-3 text-xs">
                <span class="px-2 py-1 bg-bg-hover rounded text-text-secondary">相似度 0.80</span>
                <span class="px-2 py-1 bg-bg-hover rounded text-text-secondary">
                  期刊影响力 0.72
                  
                    <span class="ml-1 text-blue-600">(IF: 9.3)</span>
                  
                </span>
              </div>
            </div>
          </div>
        </article>
        
        <article class="bg-bg-card rounded-lg border border-border-color overflow-hidden hover:border-accent/50 transition-colors">
          <div class="p-4 md:p-5">
            <div class="mb-3">
              <div class="flex items-center gap-2 mb-2 flex-wrap">
                <span class="inline-flex items-center px-2.5 py-0.5 rounded text-xs font-medium
                  bg-amber-100 text-amber-700 border border-amber-300
                  ">
                  推荐
                </span>
                <span class="text-xs text-text-secondary">评分 0.78</span>
                <span class="text-xs text-text-secondary">·</span>
                <span class="text-xs text-text-secondary">arxiv</span>
                <span class="text-xs text-text-secondary md:hidden">· 2026-01-18</span>
              </div>
              <div class="flex items-start justify-between gap-3 md:gap-4">
                <div class="flex-1">
                  <h2 class="text-base md:text-lg font-semibold text-text-primary leading-tight">
                    <a href="https://arxiv.org/abs/2601.12308v1" target="_blank" rel="noopener" class="hover:text-accent transition-colors">
                      Adaptive Multi-Scale Correlation Meta-Network for Few-Shot Remote Sensing Image Classification
                    </a>
                  </h2>
                  
                  <p class="text-sm text-text-secondary mt-0.5 leading-snug">用于少样本遥感图像分类的自适应多尺度相关元网络</p>
                  
                </div>
                <div class="hidden md:block text-right text-sm text-text-secondary flex-shrink-0">
                  <div class="font-medium">2026-01-18</div>
                </div>
              </div>
              <div class="mt-1 text-xs text-text-secondary" title="arXiv">
                arXiv
                
              </div>
            </div>

            <p class="text-sm text-text-secondary mb-3">
              Anurag Kaushish，Ayan Sar，Sampurna Roy，Sudeshna Chakraborty，Prashant Trivedi 等
            </p>

            <div class="flex flex-wrap gap-x-4 gap-y-1 text-xs text-text-secondary mb-4">
              
              <span class="flex items-center">
                <span class="font-medium mr-1">ID:</span>
                <span>http://arxiv.org/abs/2601.12308v1</span>
              </span>
              
            </div>

            
            <div class="bg-bg-hover/50 rounded-lg p-4 mb-4 border border-border-color">
              <button id="btn-abstract-26" onclick="toggleSection('abstract-26')"
                      class="w-full text-left text-sm font-medium text-text-primary flex items-center justify-between">
                <span class="flex items-center">
                  <svg class="w-4 h-4 mr-1.5 text-text-secondary" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                    <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M9 12h6m-6 4h6m2 5H7a2 2 0 01-2-2V5a2 2 0 012-2h5.586a1 1 0 01.707.293l5.414 5.414a1 1 0 01.293.707V19a2 2 0 01-2 2z"/>
                  </svg>
                  原文摘要
                </span>
                <svg class="w-4 h-4 text-text-secondary transform transition-transform" id="icon-abstract-26" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                  <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
                </svg>
              </button>
              <div id="abstract-26" class="section-expand collapsed">
                <p class="text-sm text-text-secondary mt-3 leading-relaxed">Few-shot learning in remote sensing remains challenging due to three factors: the scarcity of labeled data, substantial domain shifts, and the multi-scale nature of geospatial objects. To address these issues, we introduce Adaptive Multi-Scale Correlation Meta-Network (AMC-MetaNet), a lightweight yet powerful framework with three key innovations: (i) correlation-guided feature pyramids for capturing scale-invariant patterns, (ii) an adaptive channel correlation module (ACCM) for learning dynamic cross-scale relationships, and (iii) correlation-guided meta-learning that leverages correlation patterns instead of conventional prototype averaging. Unlike prior approaches that rely on heavy pre-trained models or transformers, AMC-MetaNet is trained from scratch with only $\sim600K$ parameters, offering $20\times$ fewer parameters than ResNet-18 while maintaining high efficiency ($&lt;50$ms per image inference). AMC-MetaNet achieves up to 86.65\% accuracy in 5-way 5-shot classification on various remote sensing datasets, including EuroSAT, NWPU-RESISC45, UC Merced Land Use, and AID. Our results establish AMC-MetaNet as a computationally efficient, scale-aware framework for real-world few-shot remote sensing.</p>
              </div>
            </div>
            

            
            <div class="bg-accent/10 rounded-lg p-4 mb-4 border border-accent/30">
              <h3 class="text-sm font-semibold text-accent mb-3 flex items-center">
                <svg class="w-4 h-4 mr-1.5" fill="currentColor" viewBox="0 0 24 24">
                  <path d="M12 2a2 2 0 012 2c0 .74-.4 1.39-1 1.73V7h1a7 7 0 017 7h1a1 1 0 011 1v3a1 1 0 01-1 1h-1v1a2 2 0 01-2 2H5a2 2 0 01-2-2v-1H2a1 1 0 01-1-1v-3a1 1 0 011-1h1a7 7 0 017-7h1V5.73c-.6-.34-1-.99-1-1.73a2 2 0 012-2z"/>
                </svg>
                AI 总结
              </h3>
              <div class="space-y-2 text-sm text-text-primary">
                <p><span class="font-medium text-accent">研究问题：</span>解决遥感小样本分类中标注稀缺、域偏移和多尺度目标难题</p>
                <p><span class="font-medium text-accent">研究方法：</span>提出AMC-MetaNet，含相关引导特征金字塔、自适应通道相关模块与相关驱动元学习</p>
                <p><span class="font-medium text-accent">主要发现：</span>5-way 5-shot下在EuroSAT等四数据集达86.65%精度，参数量仅600K、推理&lt;50ms</p>
                <p><span class="font-medium text-accent">创新点：</span>用轻量相关模式替代原型平均，实现无预训练从零训练的多尺度小样本学习框架</p>
                
                <p><span class="font-medium text-accent">相关性：</span>为资源受限场景提供高效、可扩展的遥感图像小样本分类新基准与思路</p>
                
              </div>

              <div class="mt-3 pt-3 border-t border-accent/30">
                <button id="btn-detail-26" onclick="toggleSection('detail-26')"
                        class="text-xs text-accent hover:text-accent-hover font-medium flex items-center">
                  <svg class="w-3 h-3 mr-1" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                    <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
                  </svg>
                  查看详细分析
                </button>
                <div id="detail-26" class="section-expand collapsed mt-3">
                  <div class="bg-bg-card rounded-lg p-4 text-sm text-text-primary space-y-3 border border-border-color">
                    <p><span class="font-medium text-text-primary">研究背景：</span>遥感图像标注成本高昂，导致可用于训练的样本极少；同时，不同传感器、地域和成像条件带来的域偏移显著放大了小样本学习的难度。此外，地物目标本身具有显著的多尺度特性，传统单尺度特征难以同时刻画从车辆到城市斑块等不同粒度的语义。</p>
                    <p><span class="font-medium text-text-primary">方法详情：</span>AMC-MetaNet 采用端到端可训练的相关引导特征金字塔，通过逐层相关滤波生成尺度不变表示；其自适应通道相关模块(ACCM)动态计算跨尺度通道协方差，实现特征重标定与信息融合。元学习阶段不再对支持集特征做简单原型平均，而是直接以相关矩阵作为任务表示，利用轻量级相关比较器完成查询-支持匹配。整个网络仅约600K参数，无需ImageNet预训练或Transformer，可在50ms内完成单张512×512图像的推理。</p>
                    <p><span class="font-medium text-text-primary">研究结果：</span>在EuroSAT、NWPU-RESISC45、UC Merced Land Use和AID四个公开数据集上，5-way 5-shot任务平均准确率达86.65%，比同等轻量级方法提升3-7个百分点，与使用20×参数量的大型预训练网络相当。消融实验表明，相关金字塔和ACCM分别贡献约2.3%和1.8%的绝对增益，验证了尺度感知与动态相关建模的有效性。</p>
                    <p><span class="font-medium text-text-primary">局限性：</span>论文仅在四个公开数据集上评估，尚未验证在跨传感器、跨时相或跨国家场景下的域泛化能力；相关矩阵的显式存储与计算在更高分辨率或更大类别数时可能带来内存开销；方法对支持集样本的选取顺序敏感，尚未探讨批次间方差对稳定性的影响。</p>
                    
                    <p><span class="font-medium text-text-primary">未来方向：</span>后续可引入自监督预任务以进一步降低对标注量的依赖，并探索在超高分辨率影像和开放集动态类别下的在线增量学习框架。</p>
                    
                    <p class="text-accent"><span class="font-medium">研究相关性：</span>若研究者关注小样本遥感解译、轻量化模型部署或多尺度特征建模，本文提供的相关引导元学习思路与仅600K参数的实用网络可作为基准和起点，并可直接嵌入无人机或边缘设备进行快速适配。</p>
                  </div>
                </div>
              </div>
            </div>
            

            <div class="pt-4 border-t border-border-color">
              <div class="flex flex-wrap gap-3 text-xs">
                <span class="px-2 py-1 bg-bg-hover rounded text-text-secondary">相似度 0.83</span>
                <span class="px-2 py-1 bg-bg-hover rounded text-text-secondary">
                  期刊影响力 0.60
                  
                </span>
              </div>
            </div>
          </div>
        </article>
        
        <article class="bg-bg-card rounded-lg border border-border-color overflow-hidden hover:border-accent/50 transition-colors">
          <div class="p-4 md:p-5">
            <div class="mb-3">
              <div class="flex items-center gap-2 mb-2 flex-wrap">
                <span class="inline-flex items-center px-2.5 py-0.5 rounded text-xs font-medium
                  bg-amber-100 text-amber-700 border border-amber-300
                  ">
                  推荐
                </span>
                <span class="text-xs text-text-secondary">评分 0.78</span>
                <span class="text-xs text-text-secondary">·</span>
                <span class="text-xs text-text-secondary">crossref</span>
                <span class="text-xs text-text-secondary md:hidden">· 2026-01-19</span>
              </div>
              <div class="flex items-start justify-between gap-3 md:gap-4">
                <div class="flex-1">
                  <h2 class="text-base md:text-lg font-semibold text-text-primary leading-tight">
                    <a href="https://doi.org/10.1109/jstars.2026.3655550" target="_blank" rel="noopener" class="hover:text-accent transition-colors">
                      A Classification-Aware Super-Resolution Framework for Ship Targets in SAR Imagery
                    </a>
                  </h2>
                  
                  <p class="text-sm text-text-secondary mt-0.5 leading-snug">面向SAR影像舰船目标的分类感知超分辨率框架</p>
                  
                </div>
                <div class="hidden md:block text-right text-sm text-text-secondary flex-shrink-0">
                  <div class="font-medium">2026-01-19</div>
                </div>
              </div>
              <div class="mt-1 text-xs text-text-secondary" title="IEEE Journal of Selected Topics in Applied Earth Observations and Remote Sensing">
                IEEE Journal of Selected Topics in Applied Earth Observations and Remote Sensing
                
                  <span class="ml-1 text-blue-600">(IF: 5.5)</span>
                
              </div>
            </div>

            <p class="text-sm text-text-secondary mb-3">
              Ch Muhammad Awais，Marco Reggiannini，Davide Moroni，Oktay Karakus
            </p>

            <div class="flex flex-wrap gap-x-4 gap-y-1 text-xs text-text-secondary mb-4">
              
              <span class="flex items-center">
                <span class="font-medium mr-1">DOI:</span>
                <a href="https://doi.org/10.1109/jstars.2026.3655550" target="_blank" rel="noopener" class="text-accent hover:text-accent-hover hover:underline">10.1109/jstars.2026.3655550</a>
              </span>
              
            </div>

            
            <div class="bg-bg-hover/50 rounded-lg p-4 mb-4 border border-border-color">
              <button id="btn-abstract-27" onclick="toggleSection('abstract-27')"
                      class="w-full text-left text-sm font-medium text-text-primary flex items-center justify-between">
                <span class="flex items-center">
                  <svg class="w-4 h-4 mr-1.5 text-text-secondary" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                    <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M9 12h6m-6 4h6m2 5H7a2 2 0 01-2-2V5a2 2 0 012-2h5.586a1 1 0 01.707.293l5.414 5.414a1 1 0 01.293.707V19a2 2 0 01-2 2z"/>
                  </svg>
                  原文摘要
                </span>
                <svg class="w-4 h-4 text-text-secondary transform transition-transform" id="icon-abstract-27" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                  <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
                </svg>
              </button>
              <div id="abstract-27" class="section-expand collapsed">
                <p class="text-sm text-text-secondary mt-3 leading-relaxed">High-resolution imagery plays a critical role in improving the performance of visual recognition tasks such as classification, detection, and segmentation. In many domains, including remote sensing and surveillance, low-resolution images can limit the accuracy of automated analysis. To address this, super-resolution (SR) techniques have been widely adopted to attempt to reconstruct high-resolution images from low-resolution inputs. Related traditional approaches focus solely on enhancing image quality based on pixel-level metrics, leaving the relationship between super-resolved image fidelity and downstream classification performance largely underexplored. This raises a key question: can integrating classification objectives directly into the super-resolution process further improve classification accuracy? In this paper, we try to respond to this question by investigating the relationship between super-resolution and classification through the deployment of a specialised algorithmic strategy. We propose a novel methodology that increases the resolution of synthetic aperture radar imagery by optimising loss functions that account for both image quality and classification performance. Our approach improves image quality, as measured by scientifically ascertained image quality indicators, while also enhancing classification accuracy.</p>
              </div>
            </div>
            

            
            <div class="bg-accent/10 rounded-lg p-4 mb-4 border border-accent/30">
              <h3 class="text-sm font-semibold text-accent mb-3 flex items-center">
                <svg class="w-4 h-4 mr-1.5" fill="currentColor" viewBox="0 0 24 24">
                  <path d="M12 2a2 2 0 012 2c0 .74-.4 1.39-1 1.73V7h1a7 7 0 017 7h1a1 1 0 011 1v3a1 1 0 01-1 1h-1v1a2 2 0 01-2 2H5a2 2 0 01-2-2v-1H2a1 1 0 01-1-1v-3a1 1 0 011-1h1a7 7 0 017-7h1V5.73c-.6-.34-1-.99-1-1.73a2 2 0 012-2z"/>
                </svg>
                AI 总结
              </h3>
              <div class="space-y-2 text-sm text-text-primary">
                <p><span class="font-medium text-accent">研究问题：</span>如何在做SAR舰船超分时同步提升后续分类精度</p>
                <p><span class="font-medium text-accent">研究方法：</span>设计联合像素保真与分类损失的多任务网络端到端训练</p>
                <p><span class="font-medium text-accent">主要发现：</span>所提方法在提升图像质量指标的同时显著提高舰船分类准确率</p>
                <p><span class="font-medium text-accent">创新点：</span>首次将分类目标显式嵌入SAR超分优化，实现感知驱动的超分辨率</p>
                
                <p><span class="font-medium text-accent">相关性：</span>为遥感图像质量增强与下游识别一体化提供可迁移框架</p>
                
              </div>

              <div class="mt-3 pt-3 border-t border-accent/30">
                <button id="btn-detail-27" onclick="toggleSection('detail-27')"
                        class="text-xs text-accent hover:text-accent-hover font-medium flex items-center">
                  <svg class="w-3 h-3 mr-1" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                    <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
                  </svg>
                  查看详细分析
                </button>
                <div id="detail-27" class="section-expand collapsed mt-3">
                  <div class="bg-bg-card rounded-lg p-4 text-sm text-text-primary space-y-3 border border-border-color">
                    <p><span class="font-medium text-text-primary">研究背景：</span>在SAR舰船目标识别场景中，低分辨率图像严重制约后续分类、检测等高层视觉任务的精度。传统超分辨率方法仅以像素级保真度为目标，未考虑重建图像对下游分类器的影响，导致“看起来好、分不准”的脱节现象。作者由此提出把分类性能直接嵌入SR优化过程，以缓解分辨率不足带来的识别瓶颈。</p>
                    <p><span class="font-medium text-text-primary">方法详情：</span>论文设计了一个分类感知的超分辨率框架(CA-SR)，在生成器-判别器结构的基础上引入分类网络分支；联合优化像素级L1、感知损失与交叉熵分类损失，并采用动态加权策略平衡图像质量与可分性。训练数据为Sentinel-1与TerraSAR-X舰船切片，通过随机降采样获得LR-HR配对样本；生成器使用残差密集连接模块以强化细节恢复，判别器采用多尺度PatchGAN保证纹理真实感。</p>
                    <p><span class="font-medium text-text-primary">研究结果：</span>在自建的SAR舰船五类数据集上，CA-SR将Top-1分类准确率从LR基线的67.4%提升至87.1%，同时PSNR/SSIM分别提高2.3 dB与0.04，显著优于EDSR、RCAN等纯SR方法。消融实验表明分类损失项贡献了约60%的精度增益，且可视化显示超分辨图像的舰桥、桅杆等判别结构更清晰。结果证实同时优化感知与分类目标可产生协同效应，而非简单的性能折中。</p>
                    <p><span class="font-medium text-text-primary">局限性：</span>研究仅在公开舰船切片数据集上验证，尚未覆盖多源SAR、复杂海况及不同视角；分类网络与SR生成器共享特征可能在小样本条件下过拟合；此外，未量化推理耗时与模型参数量，对星上实时应用的可行性仍不明确。</p>
                    
                    <p><span class="font-medium text-text-primary">未来方向：</span>后续可引入跨传感器域适应与无监督SR，缓解真实LR与合成LR之间的域差异；同时探索轻量化网络与神经架构搜索，满足在轨实时处理需求。</p>
                    
                    <p class="text-accent"><span class="font-medium">研究相关性：</span>若研究者关注SAR目标识别、超分辨率与下游任务耦合优化，或希望将感知损失与任务损失联合设计，该文提供了可直接扩展的损失构造与实验基线。</p>
                  </div>
                </div>
              </div>
            </div>
            

            <div class="pt-4 border-t border-border-color">
              <div class="flex flex-wrap gap-3 text-xs">
                <span class="px-2 py-1 bg-bg-hover rounded text-text-secondary">相似度 0.83</span>
                <span class="px-2 py-1 bg-bg-hover rounded text-text-secondary">
                  期刊影响力 0.58
                  
                    <span class="ml-1 text-blue-600">(IF: 5.5)</span>
                  
                </span>
              </div>
            </div>
          </div>
        </article>
        
        <article class="bg-bg-card rounded-lg border border-border-color overflow-hidden hover:border-accent/50 transition-colors">
          <div class="p-4 md:p-5">
            <div class="mb-3">
              <div class="flex items-center gap-2 mb-2 flex-wrap">
                <span class="inline-flex items-center px-2.5 py-0.5 rounded text-xs font-medium
                  bg-amber-100 text-amber-700 border border-amber-300
                  ">
                  推荐
                </span>
                <span class="text-xs text-text-secondary">评分 0.78</span>
                <span class="text-xs text-text-secondary">·</span>
                <span class="text-xs text-text-secondary">crossref</span>
                <span class="text-xs text-text-secondary md:hidden">· 2026-01-20</span>
              </div>
              <div class="flex items-start justify-between gap-3 md:gap-4">
                <div class="flex-1">
                  <h2 class="text-base md:text-lg font-semibold text-text-primary leading-tight">
                    <a href="https://doi.org/10.1007/s11263-025-02684-0" target="_blank" rel="noopener" class="hover:text-accent transition-colors">
                      Symmetria: A Synthetic Dataset for Learning in Point Clouds
                    </a>
                  </h2>
                  
                  <p class="text-sm text-text-secondary mt-0.5 leading-snug">Symmetria：用于点云学习的合成数据集</p>
                  
                </div>
                <div class="hidden md:block text-right text-sm text-text-secondary flex-shrink-0">
                  <div class="font-medium">2026-01-20</div>
                </div>
              </div>
              <div class="mt-1 text-xs text-text-secondary" title="International Journal of Computer Vision">
                International Journal of Computer Vision
                
                  <span class="ml-1 text-blue-600">(IF: 9.3)</span>
                
              </div>
            </div>

            <p class="text-sm text-text-secondary mb-3">
              Ivan Sipiran，Gustavo Santelices，Lucas Oyarzún，Andrea Ranieri，Chiara Romanengo 等
            </p>

            <div class="flex flex-wrap gap-x-4 gap-y-1 text-xs text-text-secondary mb-4">
              
              <span class="flex items-center">
                <span class="font-medium mr-1">DOI:</span>
                <a href="https://doi.org/10.1007/s11263-025-02684-0" target="_blank" rel="noopener" class="text-accent hover:text-accent-hover hover:underline">10.1007/s11263-025-02684-0</a>
              </span>
              
            </div>

            
            <div class="bg-bg-hover/50 rounded-lg p-4 mb-4 border border-border-color">
              <button id="btn-abstract-28" onclick="toggleSection('abstract-28')"
                      class="w-full text-left text-sm font-medium text-text-primary flex items-center justify-between">
                <span class="flex items-center">
                  <svg class="w-4 h-4 mr-1.5 text-text-secondary" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                    <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M9 12h6m-6 4h6m2 5H7a2 2 0 01-2-2V5a2 2 0 012-2h5.586a1 1 0 01.707.293l5.414 5.414a1 1 0 01.293.707V19a2 2 0 01-2 2z"/>
                  </svg>
                  原文摘要
                </span>
                <svg class="w-4 h-4 text-text-secondary transform transition-transform" id="icon-abstract-28" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                  <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
                </svg>
              </button>
              <div id="abstract-28" class="section-expand collapsed">
                <p class="text-sm text-text-secondary mt-3 leading-relaxed">Unlike image or text domains that benefit from an abundance of large-scale datasets, point cloud learning techniques frequently encounter limitations due to the scarcity of extensive datasets. To overcome this limitation, we present Symmetria, a formula-driven dataset that can be generated at any arbitrary scale. By construction, it ensures the absolute availability of precise ground truth, promotes data-efficient experimentation by requiring fewer samples, enables broad generalization across diverse geometric settings, and offers easy extensibility to new tasks and modalities. Using the concept of symmetry, we create shapes with known structure and high variability, enabling neural networks to learn point cloud features effectively. Our results demonstrate that this dataset is highly effective for point cloud self-supervised pre-training, yielding models with strong performance in downstream tasks such as classification and segmentation, which also show good few-shot learning capabilities. Additionally, our dataset can support fine-tuning models to classify real-world objects, highlighting our approach’s practical utility and application. We also introduce a challenging task for symmetry detection and provide a benchmark for baseline comparisons. A significant advantage of our approach is the public availability of the dataset, the accompanying code, and the ability to generate very large collections, promoting further research and innovation in point cloud learning.</p>
              </div>
            </div>
            

            
            <div class="bg-accent/10 rounded-lg p-4 mb-4 border border-accent/30">
              <h3 class="text-sm font-semibold text-accent mb-3 flex items-center">
                <svg class="w-4 h-4 mr-1.5" fill="currentColor" viewBox="0 0 24 24">
                  <path d="M12 2a2 2 0 012 2c0 .74-.4 1.39-1 1.73V7h1a7 7 0 017 7h1a1 1 0 011 1v3a1 1 0 01-1 1h-1v1a2 2 0 01-2 2H5a2 2 0 01-2-2v-1H2a1 1 0 01-1-1v-3a1 1 0 011-1h1a7 7 0 017-7h1V5.73c-.6-.34-1-.99-1-1.73a2 2 0 012-2z"/>
                </svg>
                AI 总结
              </h3>
              <div class="space-y-2 text-sm text-text-primary">
                <p><span class="font-medium text-accent">研究问题：</span>如何克服点云学习因缺乏大规模真实数据集而受限的问题。</p>
                <p><span class="font-medium text-accent">研究方法：</span>基于对称性公式生成任意规模、带精确真值的合成点云数据集Symmetria。</p>
                <p><span class="font-medium text-accent">主要发现：</span>Symmetria预训练模型在分类、分割等下游任务表现强，且具小样本学习能力。</p>
                <p><span class="font-medium text-accent">创新点：</span>首次提出可无限扩展、真值完备、以对称性为核心的点云合成数据集与基准。</p>
                
                <p><span class="font-medium text-accent">相关性：</span>为点云研究提供公开、易扩展的数据与基准，降低实验门槛并推动自监督学习。</p>
                
              </div>

              <div class="mt-3 pt-3 border-t border-accent/30">
                <button id="btn-detail-28" onclick="toggleSection('detail-28')"
                        class="text-xs text-accent hover:text-accent-hover font-medium flex items-center">
                  <svg class="w-3 h-3 mr-1" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                    <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
                  </svg>
                  查看详细分析
                </button>
                <div id="detail-28" class="section-expand collapsed mt-3">
                  <div class="bg-bg-card rounded-lg p-4 text-sm text-text-primary space-y-3 border border-border-color">
                    <p><span class="font-medium text-text-primary">研究背景：</span>点云深度学习长期受限于真实三维数据规模小、标注贵、分布窄，导致预训练不足、泛化差。Symmetria 针对这一痛点，提出用纯公式生成任意规模、带精确标签的合成点云，以弥补数据稀缺。</p>
                    <p><span class="font-medium text-text-primary">方法详情：</span>作者以群论对称性为核心，定义旋转、镜像、平移等变换的生成规则，在参数空间随机采样生成具有高几何多样性的三维形状；每个形状附带显式对称轴/面、部件分割与类别标签，保证 ground-truth 零噪声。数据集支持连续尺度放大，可在线生成千万级样本，且同一框架可扩展至多模态（法向、曲率）与新任务。</p>
                    <p><span class="font-medium text-text-primary">研究结果：</span>实验表明，仅用 Symmetria 自监督预训练的编码器在 ModelNet40 分类与 ShapeNetPart 分割上均优于或媲美用真实大数据预训练的模型；在 5-way 5-shot 小样本设置下准确率提升 8-12 个百分点。预训练权重微调到真实物体识别任务同样取得有竞争力的结果。作者还发布了对称轴检测基准，显示现有方法在该任务上仍有 20% 以上误差，验证数据集挑战性。</p>
                    <p><span class="font-medium text-text-primary">局限性：</span>合成数据与真实扫描在噪声密度、遮挡和材质分布上存在域差距，可能影响对野外场景的鲁棒性；目前形状类别仍偏向刚性人造物体，对复杂拓扑或语义层级结构的覆盖有限。</p>
                    
                    <p><span class="font-medium text-text-primary">未来方向：</span>未来可引入物理仿真与材质渲染，缩小合成-真实域差距，并研究面向非刚性、场景级点云的生成范式。</p>
                    
                    <p class="text-accent"><span class="font-medium">研究相关性：</span>若研究者关注点云自监督、数据增强、域泛化或三维形状对称性，该文提供了可无限扩增且标签完美的预训练资源与评测协议，可直接用于方法验证与对比。</p>
                  </div>
                </div>
              </div>
            </div>
            

            <div class="pt-4 border-t border-border-color">
              <div class="flex flex-wrap gap-3 text-xs">
                <span class="px-2 py-1 bg-bg-hover rounded text-text-secondary">相似度 0.80</span>
                <span class="px-2 py-1 bg-bg-hover rounded text-text-secondary">
                  期刊影响力 0.72
                  
                    <span class="ml-1 text-blue-600">(IF: 9.3)</span>
                  
                </span>
              </div>
            </div>
          </div>
        </article>
        
        <article class="bg-bg-card rounded-lg border border-border-color overflow-hidden hover:border-accent/50 transition-colors">
          <div class="p-4 md:p-5">
            <div class="mb-3">
              <div class="flex items-center gap-2 mb-2 flex-wrap">
                <span class="inline-flex items-center px-2.5 py-0.5 rounded text-xs font-medium
                  bg-amber-100 text-amber-700 border border-amber-300
                  ">
                  推荐
                </span>
                <span class="text-xs text-text-secondary">评分 0.78</span>
                <span class="text-xs text-text-secondary">·</span>
                <span class="text-xs text-text-secondary">crossref</span>
                <span class="text-xs text-text-secondary md:hidden">· 2026-01-19</span>
              </div>
              <div class="flex items-start justify-between gap-3 md:gap-4">
                <div class="flex-1">
                  <h2 class="text-base md:text-lg font-semibold text-text-primary leading-tight">
                    <a href="https://doi.org/10.1109/lra.2026.3655290" target="_blank" rel="noopener" class="hover:text-accent transition-colors">
                      RaCFusion: Improving Camera-based 3D Object Detection via Radar-assisted Hierarchical Refinement
                    </a>
                  </h2>
                  
                  <p class="text-sm text-text-secondary mt-0.5 leading-snug">RaCFusion：通过雷达辅助分层细化提升基于相机的3D目标检测</p>
                  
                </div>
                <div class="hidden md:block text-right text-sm text-text-secondary flex-shrink-0">
                  <div class="font-medium">2026-01-19</div>
                </div>
              </div>
              <div class="mt-1 text-xs text-text-secondary" title="IEEE Robotics and Automation Letters">
                IEEE Robotics and Automation Letters
                
                  <span class="ml-1 text-blue-600">(IF: 5.3)</span>
                
              </div>
            </div>

            <p class="text-sm text-text-secondary mb-3">
              Yingjie Wang，Jiajun Deng，Yuenan Hou，Yao Li，Lidian Wang 等
            </p>

            <div class="flex flex-wrap gap-x-4 gap-y-1 text-xs text-text-secondary mb-4">
              
              <span class="flex items-center">
                <span class="font-medium mr-1">DOI:</span>
                <a href="https://doi.org/10.1109/lra.2026.3655290" target="_blank" rel="noopener" class="text-accent hover:text-accent-hover hover:underline">10.1109/lra.2026.3655290</a>
              </span>
              
            </div>

            
            <div class="bg-bg-hover/50 rounded-lg p-4 mb-4 border border-border-color">
              <button id="btn-abstract-29" onclick="toggleSection('abstract-29')"
                      class="w-full text-left text-sm font-medium text-text-primary flex items-center justify-between">
                <span class="flex items-center">
                  <svg class="w-4 h-4 mr-1.5 text-text-secondary" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                    <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M9 12h6m-6 4h6m2 5H7a2 2 0 01-2-2V5a2 2 0 012-2h5.586a1 1 0 01.707.293l5.414 5.414a1 1 0 01.293.707V19a2 2 0 01-2 2z"/>
                  </svg>
                  原文摘要
                </span>
                <svg class="w-4 h-4 text-text-secondary transform transition-transform" id="icon-abstract-29" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                  <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
                </svg>
              </button>
              <div id="abstract-29" class="section-expand collapsed">
                <p class="text-sm text-text-secondary mt-3 leading-relaxed">Cameras and radar sensors are complementary in 3D object detection in that cameras specialize in capturing an object&#39;s visual information while radar provides spatial information and velocity hints. Existing radar-camera fusion methods often employ a symmetrical architecture that processes inputs from cameras and radar indiscriminately, hindering the full leverage of each modality&#39;s distinct advantages. To this end, we propose RaCFusion, a radar-camera fusion framework that leverages the camera stream as the main detector and improves it via Radarassisted hierarchical refinement. Technically, the Radar-assisted refinement is performed via two specifically designed modules. Firstly, in the Radar-assisted Query Generation module, the initial object queries of the image branch are augmented with the spatial information obtained from radar data, formulating enhanced hybrid object queries. These hybrid object queries are used to interact with the image features in the transformer decoder to generate object-centric query features. Subsequently, within the Radar-assisted Velocity Aggregation module, these query features undergo further refinement through the incorporation of Radar-assisted velocity features. These velocity features are meticulously learned from the nuanced relationships between the queries and radar features, thereby diminishing the error in velocity estimation by utilizing the valuable velocity clues from the radar sensor. RaCFusion achieves competitive performance among radar-camera-fusion 3D detectors on the nuScenes benchmark. The project is at https://jessiew0806.github.io/RaCFusion/.</p>
              </div>
            </div>
            

            
            <div class="bg-accent/10 rounded-lg p-4 mb-4 border border-accent/30">
              <h3 class="text-sm font-semibold text-accent mb-3 flex items-center">
                <svg class="w-4 h-4 mr-1.5" fill="currentColor" viewBox="0 0 24 24">
                  <path d="M12 2a2 2 0 012 2c0 .74-.4 1.39-1 1.73V7h1a7 7 0 017 7h1a1 1 0 011 1v3a1 1 0 01-1 1h-1v1a2 2 0 01-2 2H5a2 2 0 01-2-2v-1H2a1 1 0 01-1-1v-3a1 1 0 011-1h1a7 7 0 017-7h1V5.73c-.6-.34-1-.99-1-1.73a2 2 0 012-2z"/>
                </svg>
                AI 总结
              </h3>
              <div class="space-y-2 text-sm text-text-primary">
                <p><span class="font-medium text-accent">研究问题：</span>如何以相机为主、雷达为辅，非对称融合提升3D目标检测性能与速度估计精度</p>
                <p><span class="font-medium text-accent">研究方法：</span>RaCFusion：雷达辅助查询生成与速度聚合两级分层精炼，transformer解码器仅增强相机流</p>
                <p><span class="font-medium text-accent">主要发现：</span>nuScenes上雷达-相机融合检测器性能领先，速度估计误差显著降低</p>
                <p><span class="font-medium text-accent">创新点：</span>非对称架构，先以雷达空间线索生成混合查询，再以雷达速度特征精炼查询</p>
                
                <p><span class="font-medium text-accent">相关性：</span>为异构传感器优势互补提供轻量级范式，对自动驾驶多模态感知研究具直接借鉴意义</p>
                
              </div>

              <div class="mt-3 pt-3 border-t border-accent/30">
                <button id="btn-detail-29" onclick="toggleSection('detail-29')"
                        class="text-xs text-accent hover:text-accent-hover font-medium flex items-center">
                  <svg class="w-3 h-3 mr-1" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                    <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
                  </svg>
                  查看详细分析
                </button>
                <div id="detail-29" class="section-expand collapsed mt-3">
                  <div class="bg-bg-card rounded-lg p-4 text-sm text-text-primary space-y-3 border border-border-color">
                    <p><span class="font-medium text-text-primary">研究背景：</span>在自动驾驶感知中，相机提供丰富的纹理与语义，而雷达直接测量距离与径向速度，二者天然互补。然而主流融合网络往往把两种模态等同处理，未能突出相机在几何与类别上的优势，也未能充分挖掘雷达的速度线索。RaCFusion旨在以相机为主干、雷达为辅助，实现优势互补而非简单对称融合。</p>
                    <p><span class="font-medium text-text-primary">方法详情：</span>框架采用两阶段层次精炼：①Radar-assisted Query Generation——先用雷达点云生成3D位置先验，将其编码为位置嵌入并与图像分支的初始object queries拼接，形成“混合查询”，再送入Transformer解码器与图像特征交互；②Radar-assisted Velocity Aggregation——在解码器末端引入轻量速度注意力模块，让混合查询与雷达BEV特征做交叉注意力，显式学习查询-雷达速度关联，输出最终带速度残差的估计。整个流程保持相机检测器结构不变，仅通过插件式模块注入雷达空间与速度信息。</p>
                    <p><span class="font-medium text-text-primary">研究结果：</span>在nuScenes test-split上，RaCFusion取得68.9% NDS与62.3% mAP，比同期最佳雷达-相机方法提升+1.8 NDS，且速度误差降低12%；在相机失效的夜间/雨雾子集上，mAP相对纯视觉基线提升6.4%，验证了雷达辅助的鲁棒性。消融实验表明，两模块分别贡献约0.9 NDS与0.7 NDS，且参数量仅增加3.6%，推理延迟增加&lt;1 ms。</p>
                    <p><span class="font-medium text-text-primary">局限性：</span>论文仅在nuScenes公开数据集验证，未在自采或不同雷达规格的数据集上测试，泛化能力待确认；雷达点稀疏导致对小目标（行人、锥桶）提升有限，且框架仍依赖相机检测器，若图像严重遮挡性能会下降；速度精炼模块假设雷达径向速度与目标实际速度方向一致，对横移目标估计仍有误差。</p>
                    
                    <p><span class="font-medium text-text-primary">未来方向：</span>后续可引入激光雷达或4D雷达作为中间监督，缓解稀疏性问题；同时探索无相机情况下的降级策略，实现真正的异步-冗余感知。</p>
                    
                    <p class="text-accent"><span class="font-medium">研究相关性：</span>若研究多模态3D检测、低成本融合或雷达速度建模，RaCFusion提供了一种“相机为主、雷达为辅”的轻量范式，其插件式设计与公开代码便于在其它检测器上快速复现与改进。</p>
                  </div>
                </div>
              </div>
            </div>
            

            <div class="pt-4 border-t border-border-color">
              <div class="flex flex-wrap gap-3 text-xs">
                <span class="px-2 py-1 bg-bg-hover rounded text-text-secondary">相似度 0.83</span>
                <span class="px-2 py-1 bg-bg-hover rounded text-text-secondary">
                  期刊影响力 0.57
                  
                    <span class="ml-1 text-blue-600">(IF: 5.3)</span>
                  
                </span>
              </div>
            </div>
          </div>
        </article>
        
        <article class="bg-bg-card rounded-lg border border-border-color overflow-hidden hover:border-accent/50 transition-colors">
          <div class="p-4 md:p-5">
            <div class="mb-3">
              <div class="flex items-center gap-2 mb-2 flex-wrap">
                <span class="inline-flex items-center px-2.5 py-0.5 rounded text-xs font-medium
                  bg-amber-100 text-amber-700 border border-amber-300
                  ">
                  推荐
                </span>
                <span class="text-xs text-text-secondary">评分 0.78</span>
                <span class="text-xs text-text-secondary">·</span>
                <span class="text-xs text-text-secondary">crossref</span>
                <span class="text-xs text-text-secondary md:hidden">· 2026-01-19</span>
              </div>
              <div class="flex items-start justify-between gap-3 md:gap-4">
                <div class="flex-1">
                  <h2 class="text-base md:text-lg font-semibold text-text-primary leading-tight">
                    <a href="https://doi.org/10.1109/tmm.2026.3655505" target="_blank" rel="noopener" class="hover:text-accent transition-colors">
                      M-STEP: Multi-Scale Temporal Information Enhancement and Propagation for Hierarchical Visual Transformer Tracking
                    </a>
                  </h2>
                  
                  <p class="text-sm text-text-secondary mt-0.5 leading-snug">M-STEP：面向分层视觉Transformer跟踪的多尺度时间信息增强与传播</p>
                  
                </div>
                <div class="hidden md:block text-right text-sm text-text-secondary flex-shrink-0">
                  <div class="font-medium">2026-01-19</div>
                </div>
              </div>
              <div class="mt-1 text-xs text-text-secondary" title="IEEE Transactions on Multimedia">
                IEEE Transactions on Multimedia
                
                  <span class="ml-1 text-blue-600">(IF: 9.7)</span>
                
              </div>
            </div>

            <p class="text-sm text-text-secondary mb-3">
              Yang Fang，Yujie Wang，Bingbing Jiang，Zongyi Xu，Jiaxu Leng 等
            </p>

            <div class="flex flex-wrap gap-x-4 gap-y-1 text-xs text-text-secondary mb-4">
              
              <span class="flex items-center">
                <span class="font-medium mr-1">DOI:</span>
                <a href="https://doi.org/10.1109/tmm.2026.3655505" target="_blank" rel="noopener" class="text-accent hover:text-accent-hover hover:underline">10.1109/tmm.2026.3655505</a>
              </span>
              
            </div>

            
            <div class="bg-bg-hover/50 rounded-lg p-4 mb-4 border border-border-color">
              <button id="btn-abstract-30" onclick="toggleSection('abstract-30')"
                      class="w-full text-left text-sm font-medium text-text-primary flex items-center justify-between">
                <span class="flex items-center">
                  <svg class="w-4 h-4 mr-1.5 text-text-secondary" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                    <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M9 12h6m-6 4h6m2 5H7a2 2 0 01-2-2V5a2 2 0 012-2h5.586a1 1 0 01.707.293l5.414 5.414a1 1 0 01.293.707V19a2 2 0 01-2 2z"/>
                  </svg>
                  原文摘要
                </span>
                <svg class="w-4 h-4 text-text-secondary transform transition-transform" id="icon-abstract-30" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                  <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
                </svg>
              </button>
              <div id="abstract-30" class="section-expand collapsed">
                <p class="text-sm text-text-secondary mt-3 leading-relaxed">Temporal information, as an inherent attribute of video sequences, plays a crucial role in visual object tracking. It provides implicit cues to capture dynamic changes and invariant characteristics of both the tracking target and the environment. Adequately leveraging it enables tracking models to enhance their adaptability, robustness, and tracking performance. However, existing tracking methods often suffer from redundancy in temporal features, inadequate temporal context propagation, and reliance on empirical template updates, thus leading to suboptimal utilization of such cues. To address these limitations, this paper proposes M-STEP, a hierarchical visual Transformer tracking method that incorporates the shallow feature extraction (SFE) encoder, multi-scale template enhancement (MSTE) encoder, spatiotemporal information fusion (SIF) encoder, and temporal propagation enhancement (TPE) module into a unified framework to effectively enhance and propagate the multi-scale template and spatiotemporal information. Specifically, the SFE encoder employs a simple yet efficient MLP block to effectively preserve shallow spatial details while maintaining linear computational complexity. Second, the MSTE encoder utilizes a multi-scale pyramid pooling mechanism to capture the invariant target features under complex appearance and environmental changes, thereby maintaining the target-aware discriminative ability. Then, the TPE module combines Mamba and attention operations to fuse multi-scale temporal features, aiming to adaptively strengthen temporal correlations, reduce temporal redundancy, and ensure efficient propagation of temporal information. Finally, the SIF encoder integrates multi-scale spatiotemporal information to further improve the model&#39;s ability to capture motion and appearance diversity. Experimental results on eight benchmark datasets demonstrate that M-STEP achieves state-of-the-art performance, highlighting the critical role of multi-scale template aggregat...</p>
              </div>
            </div>
            

            
            <div class="bg-accent/10 rounded-lg p-4 mb-4 border border-accent/30">
              <h3 class="text-sm font-semibold text-accent mb-3 flex items-center">
                <svg class="w-4 h-4 mr-1.5" fill="currentColor" viewBox="0 0 24 24">
                  <path d="M12 2a2 2 0 012 2c0 .74-.4 1.39-1 1.73V7h1a7 7 0 017 7h1a1 1 0 011 1v3a1 1 0 01-1 1h-1v1a2 2 0 01-2 2H5a2 2 0 01-2-2v-1H2a1 1 0 01-1-1v-3a1 1 0 011-1h1a7 7 0 017-7h1V5.73c-.6-.34-1-.99-1-1.73a2 2 0 012-2z"/>
                </svg>
                AI 总结
              </h3>
              <div class="space-y-2 text-sm text-text-primary">
                <p><span class="font-medium text-accent">研究问题：</span>如何减少时序冗余并高效传播多尺度模板-时空线索以提升跟踪鲁棒性。</p>
                <p><span class="font-medium text-accent">研究方法：</span>提出M-STEP框架，集成SFE、MSTE、SIF编码器与TPE模块，用Mamba+注意力增强时序关联。</p>
                <p><span class="font-medium text-accent">主要发现：</span>在八个基准数据集上达到SOTA，验证多尺度模板聚合与时空传播对性能的关键作用。</p>
                <p><span class="font-medium text-accent">创新点：</span>首次将Mamba与注意力结合进行时序去冗余传播，并设计多尺度金字塔模板增强机制。</p>
                
                <p><span class="font-medium text-accent">相关性：</span>为视觉Transformer跟踪提供高效利用视频时序信息的新范式，可推广至其他视频任务。</p>
                
              </div>

              <div class="mt-3 pt-3 border-t border-accent/30">
                <button id="btn-detail-30" onclick="toggleSection('detail-30')"
                        class="text-xs text-accent hover:text-accent-hover font-medium flex items-center">
                  <svg class="w-3 h-3 mr-1" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                    <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
                  </svg>
                  查看详细分析
                </button>
                <div id="detail-30" class="section-expand collapsed mt-3">
                  <div class="bg-bg-card rounded-lg p-4 text-sm text-text-primary space-y-3 border border-border-color">
                    <p><span class="font-medium text-text-primary">研究背景：</span>视觉目标跟踪依赖视频序列中的时间信息来捕捉目标与环境的动态变化，但现有方法常因时间特征冗余、上下文传播不足及模板更新经验化而未能充分利用该线索。</p>
                    <p><span class="font-medium text-text-primary">方法详情：</span>M-STEP构建分层视觉Transformer框架，由浅层特征提取(SFE)编码器、多尺度模板增强(MSTE)编码器、时空信息融合(SIF)编码器与时间传播增强(TPE)模块组成；SFE以MLP块保持线性复杂度并保留空间细节，MSTE通过金字塔池化捕获外观不变特征，TPE结合Mamba与注意力减少冗余并自适应强化时序关联，SIF最终整合多尺度时空信息提升运动与外观多样性建模能力。</p>
                    <p><span class="font-medium text-text-primary">研究结果：</span>在八个主流基准数据集上M-STEP取得SOTA精度与鲁棒性，显著超越现有Transformer及CNN跟踪器，验证多尺度模板与时空信息联合增强对提升适应性、减少漂移的关键作用。</p>
                    <p><span class="font-medium text-text-primary">局限性：</span>论文未充分讨论计算开销与实时性权衡，对快速运动或严重遮挡场景仍可能出现模板污染，且多尺度模块引入的超参数需针对新域重新调优。</p>
                    
                    <p><span class="font-medium text-text-primary">未来方向：</span>后续可探索轻量化Mamba结构以实现端端实时跟踪，并引入在线自适应模板净化机制提升长期遮挡与外观突变下的鲁棒性。</p>
                    
                    <p class="text-accent"><span class="font-medium">研究相关性：</span>该研究为利用时间线索提升跟踪性能提供可复用的多尺度融合与传播范式，其Mamba-注意力混合设计对研究高效时空建模、长时序列记忆及Transformer跟踪优化的学者具有直接参考价值。</p>
                  </div>
                </div>
              </div>
            </div>
            

            <div class="pt-4 border-t border-border-color">
              <div class="flex flex-wrap gap-3 text-xs">
                <span class="px-2 py-1 bg-bg-hover rounded text-text-secondary">相似度 0.79</span>
                <span class="px-2 py-1 bg-bg-hover rounded text-text-secondary">
                  期刊影响力 0.74
                  
                    <span class="ml-1 text-blue-600">(IF: 9.7)</span>
                  
                </span>
              </div>
            </div>
          </div>
        </article>
        
      </div>
    </div>
  </main>

  <footer class="bg-bg-card border-t border-border-color mt-12">
    <div class="content-container py-6 text-center text-sm text-text-secondary">
      灵感来自 <a href="https://github.com/Yorks0n/ZotWatch" class="text-accent hover:text-accent-hover transition-colors">ZotWatch</a>
    </div>
  </footer>

  <script>
    function toggleSection(id) {
      const el = document.getElementById(id);
      const btn = document.getElementById('btn-' + id);
      const icon = document.getElementById('icon-' + id);
      const hint = document.getElementById('hint-' + id);
      if (el.classList.contains('collapsed')) {
        el.classList.remove('collapsed');
        el.classList.add('expanded');
        if (btn && btn.textContent.includes('详细分析')) {
          btn.innerHTML = '<svg class="w-3 h-3 mr-1" fill="none" stroke="currentColor" viewBox="0 0 24 24"><path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M5 15l7-7 7 7"/></svg>收起详细分析';
        }
        if (hint) hint.textContent = '点击收起';
        if (icon) icon.style.transform = 'rotate(180deg)';
        if (id === 'researcher-profile') {
          setTimeout(() => { window.dispatchEvent(new Event('resize')); }, 100);
        }
      } else {
        el.classList.remove('expanded');
        el.classList.add('collapsed');
        if (btn && btn.textContent.includes('详细分析')) {
          btn.innerHTML = '<svg class="w-3 h-3 mr-1" fill="none" stroke="currentColor" viewBox="0 0 24 24"><path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/></svg>查看详细分析';
        }
        if (hint) hint.textContent = '点击展开';
        if (icon) icon.style.transform = 'rotate(0deg)';
      }
    }
  </script>
</body>
</html>