<!DOCTYPE html>
<html lang="zh-CN">
<head>
  <meta charset="utf-8">
  <meta name="viewport" content="width=device-width, initial-scale=1">
  <title>ZotWatch 文献推荐 - 2026-02-05</title>
  <script src="https://cdn.tailwindcss.com"></script>
  <script src="https://cdn.jsdelivr.net/npm/chart.js@4.4.1/dist/chart.umd.min.js"></script>
  <script src="https://d3js.org/d3.v7.min.js"></script>
  <script>
    tailwind.config = {
      theme: {
        extend: {
          colors: {
            'bg-primary': '#f8fafc',
            'bg-card': '#ffffff',
            'bg-hover': '#f1f5f9',
            'text-primary': '#1e293b',
            'text-secondary': '#64748b',
            'border-color': '#e2e8f0',
            'accent': '#2563eb',
            'accent-hover': '#1d4ed8',
          }
        }
      }
    }
  </script>
  <style>
    .section-expand { transition: max-height 0.5s ease-out; overflow: hidden; }
    .section-expand.collapsed { max-height: 0; }
    .section-expand.expanded { max-height: none; }
    body { font-family: -apple-system, BlinkMacSystemFont, 'Segoe UI', Roboto, 'Helvetica Neue', Arial, sans-serif; }
    .content-container { max-width: 48rem; margin: 0 auto; padding: 0 1rem; }
    @media (min-width: 768px) { .content-container { padding: 0 2rem; } }
    @media (min-width: 1024px) { .content-container { padding: 0 4rem; } }

    .cluster-graph-container { width: 100%; height: 280px; touch-action: pan-y; }
    @media (min-width: 640px) { .cluster-graph-container { height: 320px; } }
    @media (min-width: 768px) { .cluster-graph-container { height: 350px; } }
    #cluster-graph svg { overflow: visible; }
  </style>
</head>
<body class="bg-bg-primary min-h-screen text-text-primary">
  <header class="bg-bg-card border-b border-border-color">
    <div class="content-container py-6 md:py-8">
      <h1 class="text-xl md:text-2xl font-bold text-text-primary">ZotWatch 文献推荐</h1>
      <p class="text-xs md:text-sm text-text-secondary mt-1">
        共 35 篇论文 ·
        生成于 2026-02-05 11:37 Asia/Shanghai
      </p>
    </div>
  </header>

  
  <section class="py-4 md:py-6 border-b border-border-color">
    <div class="content-container">
      <button id="btn-researcher-profile" onclick="toggleSection('researcher-profile')"
              class="w-full text-left text-lg md:text-xl font-bold text-text-primary mb-3 md:mb-4 flex items-center justify-between group hover:text-accent transition-colors">
        <span class="flex items-center gap-2 flex-1 min-w-0">
          <svg class="w-5 h-5 flex-shrink-0 text-accent" fill="currentColor" viewBox="0 0 24 24">
            <path d="M12 2C6.48 2 2 6.48 2 12s4.48 10 10 10 10-4.48 10-10S17.52 2 12 2zm0 3c1.66 0 3 1.34 3 3s-1.34 3-3 3-3-1.34-3-3 1.34-3 3-3zm0 14.2c-2.5 0-4.71-1.28-6-3.22.03-1.99 4-3.08 6-3.08 1.99 0 5.97 1.09 6 3.08-1.29 1.94-3.5 3.22-6 3.22z"/>
          </svg>
          <span class="truncate">研究兴趣画像</span>
        </span>
        <span class="flex items-center gap-1 md:gap-2 flex-shrink-0 ml-2">
          <span class="hidden md:inline text-sm font-normal text-text-secondary group-hover:text-accent" id="hint-researcher-profile">点击展开</span>
          <svg class="w-5 h-5 md:w-4 md:h-4 text-text-secondary group-hover:text-accent transform transition-transform" id="icon-researcher-profile" fill="none" stroke="currentColor" viewBox="0 0 24 24">
            <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
          </svg>
        </span>
      </button>

      <div id="researcher-profile" class="section-expand collapsed">
      <div class="bg-bg-card rounded-lg border border-border-color p-4 mb-4">
        <div class="grid grid-cols-4 gap-3 text-center">
          <div>
            <div class="text-xl font-bold text-accent">974</div>
            <div class="text-xs text-text-secondary">收藏论文</div>
          </div>
          <div>
            <div class="text-xl font-bold text-accent">5年9月</div>
            <div class="text-xs text-text-secondary">收藏时长</div>
          </div>
          <div>
            <div class="text-xl font-bold text-accent">&gt;10</div>
            <div class="text-xs text-text-secondary">兴趣领域</div>
          </div>
          <div>
            <div class="text-xl font-bold text-accent">9</div>
            <div class="text-xs text-text-secondary">高频作者</div>
          </div>
        </div>
      </div>

      
      <div class="bg-accent/10 rounded-lg border border-accent/30 p-4 mb-4">
        <h3 class="text-sm font-semibold text-accent mb-2 flex items-center gap-2">
          <svg class="w-4 h-4" fill="currentColor" viewBox="0 0 24 24">
            <path d="M12 2a2 2 0 012 2c0 .74-.4 1.39-1 1.73V7h1a7 7 0 017 7h1a1 1 0 011 1v3a1 1 0 01-1 1h-1v1a2 2 0 01-2 2H5a2 2 0 01-2-2v-1H2a1 1 0 01-1-1v-3a1 1 0 011-1h1a7 7 0 017-7h1V5.73c-.6-.34-1-.99-1-1.73a2 2 0 012-2z"/>
          </svg>
          AI 研究兴趣分析
        </h3>
        <div class="space-y-2 text-xs text-text-primary leading-relaxed">
          <p><span class="font-medium text-accent">研究兴趣：</span>该研究者长期关注计算机视觉与遥感交叉方向，核心阅读集中在目标检测、视觉SLAM、轻量网络及人脸对齐等算法，同时对大模型与自监督学习保持同步追踪。</p>
          <p><span class="font-medium text-accent">深度关注：</span>在CVPR、TGRS、NeurIPS等顶级期刊/会议累计收藏&gt;300篇，持续跟踪Kaiming He、Ross Girshick等团队的检测与网络压缩工作，并系统收藏SAR成像与旋转目标检测文献，形成视觉算法与遥感应用并重的深度积累。</p>
          <p><span class="font-medium text-accent">跨学科倾向：</span>阅读横跨计算机视觉、遥感、雷达信号处理与机器学习基础理论，尤其将SAR图像理解与视觉检测框架结合，体现出明显的“遥感-视觉”融合视角。</p>
          <p><span class="font-medium text-accent">兴趣演变：</span>2025-Q1出现收藏峰值(102篇)，新增关键词聚焦多任务学习与自动驾驶感知，显示兴趣正向通用感知与具身智能场景迁移；2024-Q3后收藏量回落，表明进入精选精读阶段，更关注高质量前沿综述与基础模型。</p>
          <p><span class="font-medium text-accent">阅读建议：</span>建议关注多模态大模型在遥感-自动驾驶联合场景下的应用，以及基于扩散模型的SAR图像仿真与数据增强，可进一步拓宽“视觉基础模型+遥感”交叉深度。</p>
        </div>
      </div>
      

      <div class="grid grid-cols-1 md:grid-cols-2 gap-4 mb-4">
        
        <div class="bg-bg-card rounded-lg border border-border-color p-4">
          <h4 class="text-sm font-medium text-text-primary mb-3">研究领域分布</h4>
          <div style="height: 180px;">
            <canvas id="domainsChart"></canvas>
          </div>
        </div>
        

        
        <div class="bg-bg-card rounded-lg border border-border-color p-4">
          <h4 class="text-sm font-medium text-text-primary mb-3">阅读趋势（季度）</h4>
          <div style="height: 180px;">
            <canvas id="trendsChart"></canvas>
          </div>
        </div>
        
      </div>

      
      <div class="bg-bg-card rounded-lg border border-border-color p-4 mb-4">
        <h4 class="text-sm font-medium text-text-primary mb-3">论文发表年份分布</h4>
        <div style="height: 180px;">
          <canvas id="yearDistChart"></canvas>
        </div>
      </div>
      

      
      <div class="bg-bg-card rounded-lg border border-border-color p-4 mb-4">
        <h4 class="text-sm font-medium text-text-primary mb-3">
          研究方向聚类图谱
          <span class="text-xs text-text-secondary font-normal ml-2">(30 个聚类)</span>
        </h4>
        <div id="cluster-graph" class="cluster-graph-container"></div>
        <div class="text-xs text-text-secondary mt-3">
          覆盖 948/948 篇论文（仅含摘要的文献参与聚类）
        </div>
      </div>
      

      <div class="grid grid-cols-1 md:grid-cols-2 gap-4 mb-4">
        
        <div class="bg-bg-card rounded-lg border border-border-color p-4">
          <h4 class="text-sm font-medium text-text-primary mb-2">高频关注作者</h4>
          <div class="grid grid-cols-2 gap-x-4 gap-y-1 text-xs">
            
            <div class="flex justify-between truncate">
              <span class="text-text-primary truncate" title="Kaiming He">Kaiming He</span>
              <span class="text-text-secondary ml-1 flex-shrink-0">24</span>
            </div>
            
            <div class="flex justify-between truncate">
              <span class="text-text-primary truncate" title="Ross Girshick">Ross Girshick</span>
              <span class="text-text-secondary ml-1 flex-shrink-0">18</span>
            </div>
            
            <div class="flex justify-between truncate">
              <span class="text-text-primary truncate" title="Jian Sun">Jian Sun</span>
              <span class="text-text-secondary ml-1 flex-shrink-0">15</span>
            </div>
            
            <div class="flex justify-between truncate">
              <span class="text-text-primary truncate" title="Song Han">Song Han</span>
              <span class="text-text-secondary ml-1 flex-shrink-0">14</span>
            </div>
            
            <div class="flex justify-between truncate">
              <span class="text-text-primary truncate" title="Xiangyu Zhang">Xiangyu Zhang</span>
              <span class="text-text-secondary ml-1 flex-shrink-0">14</span>
            </div>
            
            <div class="flex justify-between truncate">
              <span class="text-text-primary truncate" title="Piotr Dollár">Piotr Dollár</span>
              <span class="text-text-secondary ml-1 flex-shrink-0">12</span>
            </div>
            
            <div class="flex justify-between truncate">
              <span class="text-text-primary truncate" title="Li Liu">Li Liu</span>
              <span class="text-text-secondary ml-1 flex-shrink-0">11</span>
            </div>
            
            <div class="flex justify-between truncate">
              <span class="text-text-primary truncate" title="Xiang Li">Xiang Li</span>
              <span class="text-text-secondary ml-1 flex-shrink-0">11</span>
            </div>
            
            <div class="flex justify-between truncate">
              <span class="text-text-primary truncate" title="Wenfeng Liang">Wenfeng Liang</span>
              <span class="text-text-secondary ml-1 flex-shrink-0">10</span>
            </div>
            
            <div class="flex justify-between truncate">
              <span class="text-text-primary truncate" title="Yann LeCun">Yann LeCun</span>
              <span class="text-text-secondary ml-1 flex-shrink-0">9</span>
            </div>
            
            <div class="flex justify-between truncate">
              <span class="text-text-primary truncate" title="Zhenda Xie">Zhenda Xie</span>
              <span class="text-text-secondary ml-1 flex-shrink-0">9</span>
            </div>
            
            <div class="flex justify-between truncate">
              <span class="text-text-primary truncate" title="Feng Xu">Feng Xu</span>
              <span class="text-text-secondary ml-1 flex-shrink-0">9</span>
            </div>
            
          </div>
        </div>
        

        
        <div class="bg-bg-card rounded-lg border border-border-color p-4">
          <h4 class="text-sm font-medium text-text-primary mb-2">常关注期刊/会议</h4>
          <div class="space-y-1 text-xs">
            
            <div class="flex items-center justify-between">
              <span class="text-text-primary truncate flex-1" title="Computer Vision and Pattern Recognition">Computer Vision and Pattern Recognition</span>
              <span class="text-xs px-1 py-0.5 rounded bg-blue-100 text-blue-700 ml-1 flex-shrink-0">
                期刊
              </span>
              <span class="text-text-secondary ml-1 flex-shrink-0">115</span>
            </div>
            
            <div class="flex items-center justify-between">
              <span class="text-text-primary truncate flex-1" title="IEEE Transactions on Geoscience and Remote Sensing">IEEE Transactions on Geoscience and Remote Sensing</span>
              <span class="text-xs px-1 py-0.5 rounded bg-blue-100 text-blue-700 ml-1 flex-shrink-0">
                期刊
              </span>
              <span class="text-text-secondary ml-1 flex-shrink-0">50</span>
            </div>
            
            <div class="flex items-center justify-between">
              <span class="text-text-primary truncate flex-1" title="Neural Information Processing Systems">Neural Information Processing Systems</span>
              <span class="text-xs px-1 py-0.5 rounded bg-blue-100 text-blue-700 ml-1 flex-shrink-0">
                期刊
              </span>
              <span class="text-text-secondary ml-1 flex-shrink-0">46</span>
            </div>
            
            <div class="flex items-center justify-between">
              <span class="text-text-primary truncate flex-1" title="International Conference on Computer Vision">International Conference on Computer Vision</span>
              <span class="text-xs px-1 py-0.5 rounded bg-amber-100 text-amber-700 ml-1 flex-shrink-0">
                会议
              </span>
              <span class="text-text-secondary ml-1 flex-shrink-0">38</span>
            </div>
            
            <div class="flex items-center justify-between">
              <span class="text-text-primary truncate flex-1" title="IEEE Transactions on Pattern Analysis and Machine Intelligence">IEEE Transactions on Pattern Analysis and Machine Intelligence</span>
              <span class="text-xs px-1 py-0.5 rounded bg-blue-100 text-blue-700 ml-1 flex-shrink-0">
                期刊
              </span>
              <span class="text-text-secondary ml-1 flex-shrink-0">32</span>
            </div>
            
            <div class="flex items-center justify-between">
              <span class="text-text-primary truncate flex-1" title="International Conference on Learning Representations">International Conference on Learning Representations</span>
              <span class="text-xs px-1 py-0.5 rounded bg-amber-100 text-amber-700 ml-1 flex-shrink-0">
                会议
              </span>
              <span class="text-text-secondary ml-1 flex-shrink-0">29</span>
            </div>
            
            <div class="flex items-center justify-between">
              <span class="text-text-primary truncate flex-1" title="European Conference on Computer Vision">European Conference on Computer Vision</span>
              <span class="text-xs px-1 py-0.5 rounded bg-amber-100 text-amber-700 ml-1 flex-shrink-0">
                会议
              </span>
              <span class="text-text-secondary ml-1 flex-shrink-0">23</span>
            </div>
            
            <div class="flex items-center justify-between">
              <span class="text-text-primary truncate flex-1" title="IEEE Geoscience and Remote Sensing Letters">IEEE Geoscience and Remote Sensing Letters</span>
              <span class="text-xs px-1 py-0.5 rounded bg-blue-100 text-blue-700 ml-1 flex-shrink-0">
                期刊
              </span>
              <span class="text-text-secondary ml-1 flex-shrink-0">22</span>
            </div>
            
          </div>
        </div>
        
      </div>

      
      <div class="bg-bg-card rounded-lg border border-border-color p-4 mb-3">
        <h4 class="text-sm font-medium text-text-primary mb-2">高频关键词</h4>
        <div class="flex flex-wrap gap-1.5">
          
          <span class="px-2 py-0.5 bg-bg-hover rounded text-xs text-text-primary">
            大语言模型 <span class="text-text-secondary">(14)</span>
          </span>
          
          <span class="px-2 py-0.5 bg-bg-hover rounded text-xs text-text-primary">
            合成孔径雷达 <span class="text-text-secondary">(12)</span>
          </span>
          
          <span class="px-2 py-0.5 bg-bg-hover rounded text-xs text-text-primary">
            自监督学习 <span class="text-text-secondary">(11)</span>
          </span>
          
          <span class="px-2 py-0.5 bg-bg-hover rounded text-xs text-text-primary">
            综述 <span class="text-text-secondary">(9)</span>
          </span>
          
          <span class="px-2 py-0.5 bg-bg-hover rounded text-xs text-text-primary">
            域自适应 <span class="text-text-secondary">(9)</span>
          </span>
          
          <span class="px-2 py-0.5 bg-bg-hover rounded text-xs text-text-primary">
            目标检测 <span class="text-text-secondary">(8)</span>
          </span>
          
          <span class="px-2 py-0.5 bg-bg-hover rounded text-xs text-text-primary">
            深度学习 <span class="text-text-secondary">(8)</span>
          </span>
          
          <span class="px-2 py-0.5 bg-bg-hover rounded text-xs text-text-primary">
            迁移学习 <span class="text-text-secondary">(7)</span>
          </span>
          
          <span class="px-2 py-0.5 bg-bg-hover rounded text-xs text-text-primary">
            扩散模型 <span class="text-text-secondary">(7)</span>
          </span>
          
          <span class="px-2 py-0.5 bg-bg-hover rounded text-xs text-text-primary">
            强化学习 <span class="text-text-secondary">(7)</span>
          </span>
          
          <span class="px-2 py-0.5 bg-bg-hover rounded text-xs text-text-primary">
            注意力机制 <span class="text-text-secondary">(6)</span>
          </span>
          
          <span class="px-2 py-0.5 bg-bg-hover rounded text-xs text-text-primary">
            重参数化 <span class="text-text-secondary">(6)</span>
          </span>
          
          <span class="px-2 py-0.5 bg-bg-hover rounded text-xs text-text-primary">
            DeepSeek <span class="text-text-secondary">(6)</span>
          </span>
          
          <span class="px-2 py-0.5 bg-bg-hover rounded text-xs text-text-primary">
            生成对抗网络 <span class="text-text-secondary">(6)</span>
          </span>
          
          <span class="px-2 py-0.5 bg-bg-hover rounded text-xs text-text-primary">
            基础模型 <span class="text-text-secondary">(5)</span>
          </span>
          
          <span class="px-2 py-0.5 bg-bg-hover rounded text-xs text-text-primary">
            Transformers <span class="text-text-secondary">(5)</span>
          </span>
          
          <span class="px-2 py-0.5 bg-bg-hover rounded text-xs text-text-primary">
            卷积神经网络 <span class="text-text-secondary">(5)</span>
          </span>
          
          <span class="px-2 py-0.5 bg-bg-hover rounded text-xs text-text-primary">
            旋转目标检测 <span class="text-text-secondary">(5)</span>
          </span>
          
          <span class="px-2 py-0.5 bg-bg-hover rounded text-xs text-text-primary">
            知识蒸馏 <span class="text-text-secondary">(5)</span>
          </span>
          
          <span class="px-2 py-0.5 bg-bg-hover rounded text-xs text-text-primary">
            SAR目标识别 <span class="text-text-secondary">(5)</span>
          </span>
          
        </div>
      </div>
      

      <div class="text-xs text-text-secondary text-right">
        画像生成于 2026-02-05 11:16 Asia/Shanghai
        · kimi-k2-turbo-preview
      </div>
      </div>
    </div>
  </section>
  

  
  <script>
    document.addEventListener('DOMContentLoaded', function() {
      Chart.defaults.font.family = "-apple-system, BlinkMacSystemFont, 'Segoe UI', Roboto, 'Helvetica Neue', Arial, sans-serif";
      Chart.defaults.font.size = 11;
      Chart.defaults.color = '#64748b';
      Chart.defaults.animation = false;

      
      const domainsCtx = document.getElementById('domainsChart');
      if (domainsCtx) {
        new Chart(domainsCtx, {
          type: 'bar',
          data: {
            labels: ['视觉SLAM', '目标检测', '姿态估计', '人脸对齐', '轻量网络', '对比学习', '重参数化', '车牌识别'],
            datasets: [{
              data: [22, 35, 15, 12, 18, 10, 8, 6],
              backgroundColor: 'rgba(37, 99, 235, 0.7)',
              borderColor: 'rgba(37, 99, 235, 1)',
              borderWidth: 1,
              borderRadius: 2,
              barThickness: 16
            }]
          },
          options: {
            indexAxis: 'y',
            responsive: true,
            maintainAspectRatio: false,
            plugins: {
              legend: { display: false },
              tooltip: { enabled: false }
            },
            scales: {
              x: {
                beginAtZero: true,
                grid: { color: 'rgba(226, 232, 240, 0.5)' },
                ticks: { precision: 0 }
              },
              y: {
                grid: { display: false },
                ticks: { font: { size: 10 } }
              }
            }
          }
        });
      }
      

      
      const trendsCtx = document.getElementById('trendsChart');
      if (trendsCtx) {
        const trends = [{ q: '2024-Q1', c: 68 }, { q: '2024-Q2', c: 18 }, { q: '2024-Q3', c: 4 }, { q: '2024-Q4', c: 24 }, { q: '2025-Q1', c: 102 }, { q: '2025-Q2', c: 34 }, { q: '2025-Q3', c: 14 }, { q: '2025-Q4', c: 31 }, { q: '2026-Q1', c: 8 }];
        const monthColors = [
          { bg: 'rgba(59, 130, 246, 0.7)', border: 'rgba(59, 130, 246, 1)' },
          { bg: 'rgba(16, 185, 129, 0.7)', border: 'rgba(16, 185, 129, 1)' },
          { bg: 'rgba(245, 158, 11, 0.7)', border: 'rgba(245, 158, 11, 1)' }
        ];
        const bgColors = trends.map((t, i) => monthColors[i % 3].bg);
        const borderColors = trends.map((t, i) => monthColors[i % 3].border);
        new Chart(trendsCtx, {
          type: 'bar',
          data: {
            labels: trends.map(t => t.q.replace('-', '\n')),
            datasets: [{
              data: trends.map(t => t.c),
              backgroundColor: bgColors,
              borderColor: borderColors,
              borderWidth: 1,
              borderRadius: 2,
              barThickness: 20
            }]
          },
          options: {
            responsive: true,
            maintainAspectRatio: false,
            plugins: {
              legend: { display: false },
              tooltip: { enabled: false }
            },
            scales: {
              x: {
                grid: { display: false },
                ticks: { font: { size: 9 }, maxRotation: 0 }
              },
              y: {
                beginAtZero: true,
                grid: { color: 'rgba(226, 232, 240, 0.5)' },
                ticks: { precision: 0 }
              }
            }
          }
        });
      }
      

      
      const yearDistCtx = document.getElementById('yearDistChart');
      if (yearDistCtx) {
        const yearData = [{ year: 2007, count: 3 }, { year: 2008, count: 2 }, { year: 2009, count: 2 }, { year: 2010, count: 9 }, { year: 2011, count: 10 }, { year: 2012, count: 3 }, { year: 2013, count: 12 }, { year: 2014, count: 14 }, { year: 2015, count: 18 }, { year: 2016, count: 15 }, { year: 2017, count: 41 }, { year: 2018, count: 59 }, { year: 2019, count: 56 }, { year: 2020, count: 68 }, { year: 2021, count: 84 }, { year: 2022, count: 114 }, { year: 2023, count: 111 }, { year: 2024, count: 114 }, { year: 2025, count: 181 }, { year: 2026, count: 8 }];
        new Chart(yearDistCtx, {
          type: 'line',
          data: {
            labels: yearData.map(d => d.year),
            datasets: [{
              data: yearData.map(d => d.count),
              borderColor: 'rgba(37, 99, 235, 1)',
              backgroundColor: 'rgba(37, 99, 235, 0.1)',
              borderWidth: 2,
              fill: true,
              tension: 0.3,
              pointRadius: 2,
              pointHoverRadius: 4
            }]
          },
          options: {
            responsive: true,
            maintainAspectRatio: false,
            plugins: {
              legend: { display: false },
              tooltip: {
                callbacks: {
                  label: function(context) {
                    return context.parsed.y + ' 篇';
                  }
                }
              }
            },
            scales: {
              x: {
                grid: { display: false },
                ticks: {
                  font: { size: 9 },
                  maxRotation: 45,
                  callback: function(value, index) {
                    const year = this.getLabelForValue(value);
                    return year % 5 === 0 ? year : '';
                  }
                }
              },
              y: {
                beginAtZero: true,
                grid: { color: 'rgba(226, 232, 240, 0.5)' },
                ticks: { precision: 0 }
              }
            }
          }
        });
      }
      

      
      const clusterContainer = document.getElementById('cluster-graph');
      if (clusterContainer) {
        const clusters = [
          
          {
            id: 0,
            label: "SAR\u8230\u8239\u68c0\u6d4b",
            size: 124,
            keywords: ["\u5408\u6210\u5b54\u5f84\u96f7\u8fbe", "SAR\u76ee\u6807\u8bc6\u522b", "\u57df\u81ea\u9002\u5e94"]
          },
          
          {
            id: 1,
            label: "\u5927\u6a21\u578bMoE\u4e0e\u63a8\u7406",
            size: 57,
            keywords: ["\u5927\u8bed\u8a00\u6a21\u578b", "\u5f3a\u5316\u5b66\u4e60", "\u6df7\u5408\u4e13\u5bb6\u6a21\u578b"]
          },
          
          {
            id: 2,
            label: "\u901a\u7528\u76ee\u6807\u68c0\u6d4b\u67b6\u6784",
            size: 44,
            keywords: ["\u9065\u611f\u76ee\u6807\u68c0\u6d4b", "Object detection", "DETR"]
          },
          
          {
            id: 3,
            label: "\u57df\u81ea\u9002\u5e94\u68c0\u6d4b",
            size: 43,
            keywords: ["\u57df\u81ea\u9002\u5e94", "\u8fc1\u79fb\u5b66\u4e60", "\u5355\u9636\u6bb5\u68c0\u6d4b"]
          },
          
          {
            id: 4,
            label: "\u89c6\u89c9\u81ea\u76d1\u7763\u5b66\u4e60",
            size: 43,
            keywords: ["\u81ea\u76d1\u7763\u5b66\u4e60", "\u5bf9\u6bd4\u5b66\u4e60", "MoCo"]
          },
          
          {
            id: 5,
            label: "\u8f7b\u91cf\u7ea7CNN\u8bbe\u8ba1",
            size: 42,
            keywords: ["\u91cd\u53c2\u6570\u5316", "VGG", "\u6b8b\u5dee\u8fde\u63a5"]
          },
          
          {
            id: 6,
            label: "CNN\u53ef\u89e3\u91ca\u6027",
            size: 41,
            keywords: ["\u7279\u5f81\u53ef\u89c6\u5316", "\u5377\u79ef\u795e\u7ecf\u7f51\u7edc", "Grad-CAM"]
          },
          
          {
            id: 7,
            label: "\u96f7\u8fbe\u667a\u80fd\u76ee\u6807\u68c0\u6d4b",
            size: 41,
            keywords: ["\u6df1\u5ea6\u5b66\u4e60", "\u5408\u6210\u5b54\u5f84\u96f7\u8fbe", "\u6052\u865a\u8b66\u7387\u68c0\u6d4b"]
          },
          
          {
            id: 8,
            label: "\u673a\u5668\u5b66\u4e60\u57fa\u7840\u4e0e\u4f18\u5316",
            size: 41,
            keywords: ["\u7814\u7a76", "LaTeX", "\u8bbe\u8ba1\u6a21\u5f0f"]
          },
          
          {
            id: 9,
            label: "\u6269\u6563\u56fe\u50cf\u751f\u6210",
            size: 40,
            keywords: ["\u6269\u6563\u6a21\u578b", "\u56fe\u50cf\u751f\u6210", "\u6f5c\u5728\u6269\u6563\u6a21\u578b"]
          },
          
          {
            id: 10,
            label: "\u7ea2\u5916\u5f31\u5c0f\u76ee\u6807\u68c0\u6d4b",
            size: 37,
            keywords: ["\u7ea2\u5916\u5c0f\u76ee\u6807\u68c0\u6d4b", "\u5f31\u5c0f\u76ee\u6807\u8ddf\u8e2a", "\u7ea2\u5916\u56fe\u50cf"]
          },
          
          {
            id: 11,
            label: "\u591a\u4f20\u611f\u5668BEV\u611f\u77e5",
            size: 35,
            keywords: ["\u8f7b\u91cf\u7ea7\u6a21\u578b", "\u591a\u4efb\u52a1\u5b66\u4e60", "\u81ea\u52a8\u9a7e\u9a76\u611f\u77e5"]
          },
          
          {
            id: 12,
            label: "\u4eba\u4f53\u59ff\u6001\u4f30\u8ba1",
            size: 35,
            keywords: ["HRNet", "Transformers"]
          },
          
          {
            id: 13,
            label: "\u7edf\u4e00\u56fe\u50cf\u5206\u5272",
            size: 33,
            keywords: ["\u56fe\u50cf\u5206\u5272", "\u57fa\u7840\u6a21\u578b", "\u901a\u7528\u5206\u5272"]
          },
          
          {
            id: 14,
            label: "Vision Transformer",
            size: 32,
            keywords: ["Vision Transformers", "Swin Transformer", "\u5377\u79ef\u795e\u7ecf\u7f51\u7edc"]
          },
          
          {
            id: 15,
            label: "\u591a\u4f20\u611f\u5668\u4f4d\u59ff\u4f30\u8ba1",
            size: 29,
            keywords: []
          },
          
          {
            id: 16,
            label: "\u6a21\u578b\u91cf\u5316\u538b\u7f29",
            size: 29,
            keywords: ["\u6a21\u578b\u538b\u7f29", "\u6a21\u578b\u91cf\u5316", "\u6574\u6570\u63a8\u7406"]
          },
          
          {
            id: 17,
            label: "\u5f3a\u5316\u4e0e\u5143\u5b66\u4e60",
            size: 29,
            keywords: ["\u5f3a\u5316\u5b66\u4e60", "\u7b97\u6cd5\u4ea4\u6613", "\u9650\u4ef7\u8ba2\u5355\u7c3f"]
          },
          
          {
            id: 18,
            label: "SAR\u6210\u50cf\u7b97\u6cd5",
            size: 28,
            keywords: []
          },
          
          {
            id: 19,
            label: "\u8f66\u724c\u8bc6\u522b",
            size: 27,
            keywords: ["\u8f66\u724c\u8bc6\u522b", "Internet of Things", "Microcontrollers"]
          },
          
          {
            id: 20,
            label: "\u5b9e\u65f6\u68c0\u6d4b\u5668\u8bbe\u8ba1",
            size: 26,
            keywords: ["\u5b9e\u65f6\u76ee\u6807\u68c0\u6d4b", "DETR", "Adapter Branch"]
          },
          
          {
            id: 21,
            label: "\u76ee\u6807\u68c0\u6d4b\u7efc\u8ff0",
            size: 18,
            keywords: ["\u7efc\u8ff0", "SIFT", "\u65cb\u8f6c\u76ee\u6807\u68c0\u6d4b"]
          },
          
          {
            id: 22,
            label: "\u5f00\u6e90\u5927\u8bed\u8a00\u6a21\u578b",
            size: 15,
            keywords: ["DeepSeek", "\u5927\u8bed\u8a00\u6a21\u578b", "Computer Science - Computer Vision and Pattern Recognition"]
          },
          
          {
            id: 23,
            label: "\u96f7\u8fbe\u7ea2\u5916\u6297\u5e72\u6270",
            size: 12,
            keywords: []
          },
          
          {
            id: 24,
            label: "\u5f52\u4e00\u5316\u6d41\u751f\u6210",
            size: 12,
            keywords: ["\u6807\u51c6\u5316\u6d41", "\u6d41\u6a21\u578b", "NCE"]
          },
          
          {
            id: 25,
            label: "\u7ea2\u5916\u56fe\u50cf\u590d\u539f",
            size: 12,
            keywords: ["\u56fe\u50cf\u6062\u590d", "\u611f\u77e5-\u5931\u771f\u6743\u8861", "\u751f\u6210\u5bf9\u6297\u7f51\u7edc"]
          },
          
          {
            id: 26,
            label: "\u77e5\u8bc6\u84b8\u998f\u538b\u7f29",
            size: 10,
            keywords: ["\u6a21\u578b\u538b\u7f29", "\u77e5\u8bc6\u84b8\u998f", "\u7efc\u8ff0"]
          },
          
          {
            id: 27,
            label: "GAN\u751f\u6210\u6a21\u578b",
            size: 7,
            keywords: ["\u751f\u6210\u5bf9\u6297\u7f51\u7edc", "L2\u6b63\u5219\u5316", "\u6743\u91cd\u8870\u51cf"]
          },
          
          {
            id: 28,
            label: "\u7ea2\u5916\u70df\u5e55\u4f20\u8f93",
            size: 4,
            keywords: []
          },
          
          {
            id: 29,
            label: "\u5f3a\u5316\u5b66\u4e60\u5e94\u7528",
            size: 2,
            keywords: []
          }
          
        ];

        const links = [{"source": 16, "target": 26, "value": 0.9283409020987141}, {"source": 7, "target": 23, "value": 0.9076127862628859}, {"source": 3, "target": 4, "value": 0.9380964843544042}, {"source": 4, "target": 9, "value": 0.883460023920835}, {"source": 6, "target": 24, "value": 0.8778226775612696}, {"source": 6, "target": 27, "value": 0.8898391015492032}, {"source": 5, "target": 16, "value": 0.8703298419400473}, {"source": 14, "target": 22, "value": 0.8974886423480858}, {"source": 8, "target": 21, "value": 0.8857319345983062}, {"source": 2, "target": 11, "value": 0.9073907122960342}, {"source": 19, "target": 21, "value": 0.8820941107431807}, {"source": 11, "target": 20, "value": 0.9145205070690723}, {"source": 2, "target": 20, "value": 0.9414191590582492}, {"source": 6, "target": 14, "value": 0.909860605580794}, {"source": 6, "target": 17, "value": 0.9061693401939285}, {"source": 0, "target": 18, "value": 0.9303864445709285}, {"source": 7, "target": 28, "value": 0.8523014809072713}, {"source": 5, "target": 6, "value": 0.941072484340436}, {"source": 18, "target": 25, "value": 0.8977208762370655}, {"source": 4, "target": 14, "value": 0.9405923989309695}, {"source": 12, "target": 21, "value": 0.8963330902750574}, {"source": 12, "target": 15, "value": 0.8608814347421924}, {"source": 7, "target": 25, "value": 0.8769019194946398}, {"source": 0, "target": 7, "value": 0.9412214266733706}, {"source": 0, "target": 10, "value": 0.9127189428287581}, {"source": 8, "target": 17, "value": 0.9041795562670252}, {"source": 11, "target": 13, "value": 0.8923669028647833}, {"source": 2, "target": 10, "value": 0.9233416484239243}, {"source": 11, "target": 19, "value": 0.8687261229046738}, {"source": 10, "target": 23, "value": 0.8259257395408978}, {"source": 17, "target": 29, "value": 0.7607519491713165}, {"source": 1, "target": 17, "value": 0.9013989015595545}, {"source": 1, "target": 29, "value": 0.7857206463654448}, {"source": 7, "target": 18, "value": 0.9062107017469008}, {"source": 3, "target": 14, "value": 0.927078219813671}, {"source": 4, "target": 13, "value": 0.8856322513641621}, {"source": 5, "target": 14, "value": 0.9166349260136095}, {"source": 14, "target": 26, "value": 0.9038715043527507}, {"source": 2, "target": 3, "value": 0.9370572569480473}, {"source": 11, "target": 12, "value": 0.8900867348725375}, {"source": 11, "target": 15, "value": 0.894134490667789}, {"source": 11, "target": 21, "value": 0.9066896118497627}, {"source": 9, "target": 24, "value": 0.881820903987974}, {"source": 9, "target": 27, "value": 0.8856690993298676}, {"source": 2, "target": 21, "value": 0.915030369923118}, {"source": 10, "target": 28, "value": 0.856268776865327}, {"source": 1, "target": 22, "value": 0.9479591495761392}];

        const academicColors = [
          '#4E79A7', '#F28E2B', '#E15759', '#76B7B2', '#59A14F',
          '#EDC948', '#B07AA1', '#FF9DA7', '#9C755F', '#BAB0AC',
          '#86BCB6', '#D37295', '#8CD17D', '#499894', '#FABFD2',
          '#79706E', '#D4A6C8', '#9D7660', '#B6992D', '#A0CBE8'
        ];
        const colorScale = (i) => academicColors[i % academicColors.length];

        let svg, simulation, node, link, sizeScale;
        let lastWidth = 0;
        let simulationStopped = false;

        function createGraph() {
          const width = clusterContainer.clientWidth;
          const height = clusterContainer.clientHeight || 280;
          const isMobile = width < 500;

          lastWidth = width;
          clusterContainer.innerHTML = '';

          const minRadius = isMobile ? 16 : 20;
          const maxRadius = isMobile ? 38 : 48;
          sizeScale = d3.scaleSqrt()
            .domain([0, d3.max(clusters, d => d.size)])
            .range([minRadius, maxRadius]);

          svg = d3.select('#cluster-graph')
            .append('svg')
            .attr('width', width)
            .attr('height', height)
            .attr('viewBox', [0, 0, width, height]);

          // Create layered groups for z-order control
          // Order (bottom to top): dimmed links -> dimmed nodes -> highlighted links -> highlighted nodes
          const layerDimmedLinks = svg.append('g').attr('class', 'layer-dimmed-links');
          const layerDimmedNodes = svg.append('g').attr('class', 'layer-dimmed-nodes');
          const layerHighlightedLinks = svg.append('g').attr('class', 'layer-highlighted-links');
          const layerHighlightedNodes = svg.append('g').attr('class', 'layer-highlighted-nodes');

          const chargeStrength = isMobile ? -100 : -150;
          const linkDistance = isMobile ? 60 : 90;

          simulationStopped = false;
          simulation = d3.forceSimulation(clusters)
            .force('link', d3.forceLink(links).id(d => d.id).distance(linkDistance).strength(0.2))
            .force('charge', d3.forceManyBody().strength(chargeStrength))
            .force('center', d3.forceCenter(width / 2, height / 2))
            .force('collision', d3.forceCollide().radius(d => sizeScale(d.size) + (isMobile ? 4 : 6)))
            .velocityDecay(0.7)
            .alphaDecay(0.08)
            .alphaMin(0.01);

          // Initially all links in dimmed layer
          link = layerDimmedLinks
            .attr('stroke', '#e2e8f0')
            .attr('stroke-opacity', 0.6)
            .selectAll('line')
            .data(links)
            .join('line')
            .attr('stroke-width', d => Math.max(1, d.value * 1.5));

          // Initially all nodes in dimmed layer
          node = layerDimmedNodes
            .selectAll('g')
            .data(clusters)
            .join('g')
            .style('cursor', 'grab');

          node.append('circle')
            .attr('r', d => sizeScale(d.size))
            .attr('fill', (d, i) => colorScale(i))
            .attr('fill-opacity', 0.85)
            .attr('stroke', '#fff')
            .attr('stroke-width', isMobile ? 1.5 : 2);

          const fontSize = isMobile ? 7 : 8;
          const lineHeight = isMobile ? 8 : 9;
          const charWidth = isMobile ? 5 : 5.5;

          function wrapText(text, radius) {
            text.each(function() {
              const textEl = d3.select(this);
              const label = textEl.text();
              const maxWidth = radius * 1.2;

              textEl.text(null);

              let lines = [];
              let currentLine = '';
              for (let char of label) {
                if ((currentLine.length + 1) * charWidth > maxWidth && currentLine) {
                  lines.push(currentLine);
                  currentLine = char;
                } else {
                  currentLine += char;
                }
              }
              if (currentLine) lines.push(currentLine);

              if (lines.length > 2) {
                lines = lines.slice(0, 2);
                lines[1] = lines[1].substring(0, Math.max(0, lines[1].length - 1)) + '..';
              }

              const startY = -(lines.length - 1) * lineHeight / 2;

              lines.forEach((line, i) => {
                textEl.append('tspan')
                  .attr('x', 0)
                  .attr('dy', i === 0 ? startY : lineHeight)
                  .text(line);
              });
            });
          }

          node.append('text')
            .text(d => d.label)
            .attr('text-anchor', 'middle')
            .attr('dominant-baseline', 'middle')
            .attr('font-size', fontSize + 'px')
            .attr('font-weight', '500')
            .attr('fill', '#fff')
            .attr('pointer-events', 'none')
            .each(function(d) {
              wrapText(d3.select(this), sizeScale(d.size));
            });

          node.append('text')
            .text(d => `(${d.size}篇)`)
            .attr('text-anchor', 'middle')
            .attr('dy', d => sizeScale(d.size) + (isMobile ? 10 : 12))
            .attr('font-size', fontSize + 'px')
            .attr('fill', '#94a3b8');

          node.append('title')
            .text(d => `${d.label}\n论文数: ${d.size}\n关键词: ${d.keywords.join(', ')}`);

          const drag = d3.drag()
            .on('start', dragstarted)
            .on('drag', dragged)
            .on('end', dragended);

          node.call(drag);

          simulation.on('tick', () => {
            node.attr('transform', d => {
              const r = sizeScale(d.size);
              const padding = isMobile ? 5 : 8;
              const maxY = height - r - (isMobile ? 12 : 15);
              d.x = Math.max(r + padding, Math.min(width - r - padding, d.x));
              d.y = Math.max(r + padding, Math.min(maxY, d.y));
              return `translate(${d.x},${d.y})`;
            });

            link
              .attr('x1', d => d.source.x)
              .attr('y1', d => d.source.y)
              .attr('x2', d => d.target.x)
              .attr('y2', d => d.target.y);
          });

          simulation.on('end', () => {
            simulationStopped = true;
          });

          function dragstarted(event, d) {
            if (!event.active) simulation.alphaTarget(0.05).restart();
            d.fx = d.x;
            d.fy = d.y;
            highlightNode(d);
            d3.select(this).style('cursor', 'grabbing');
          }

          function dragged(event, d) {
            const r = sizeScale(d.size);
            const padding = isMobile ? 5 : 8;
            const maxY = height - r - (isMobile ? 12 : 15);
            d.fx = Math.max(r + padding, Math.min(width - r - padding, event.x));
            d.fy = Math.max(r + padding, Math.min(maxY, event.y));
          }

          function dragended(event, d) {
            if (!event.active) simulation.alphaTarget(0);
            d.fx = null;
            d.fy = null;
            d3.select(this).style('cursor', 'grab');
            resetHighlight();
          }

          function highlightNode(selectedNode) {
            const connectedIds = new Set();

            links.forEach(l => {
              if (l.source.id === selectedNode.id) connectedIds.add(l.target.id);
              if (l.target.id === selectedNode.id) connectedIds.add(l.source.id);
            });

            // Step 1: Style dimmed elements
            link
              .attr('stroke-opacity', 0.1)
              .attr('stroke', '#cbd5e1')
              .attr('stroke-width', d => Math.max(1, d.value * 1.5));

            node.each(function(d) {
              d3.select(this).select('circle')
                .attr('fill-opacity', 0.25)
                .attr('stroke', '#fff')
                .attr('stroke-width', isMobile ? 1.5 : 2);

              d3.select(this).selectAll('text')
                .attr('opacity', 0.3);
            });

            // Step 2: Move dimmed elements to bottom layers
            link.each(function() {
              layerDimmedLinks.node().appendChild(this);
            });
            node.each(function() {
              layerDimmedNodes.node().appendChild(this);
            });

            // Step 3: Style and move highlighted links to top
            link.filter(l => l.source.id === selectedNode.id || l.target.id === selectedNode.id)
              .attr('stroke-opacity', 1)
              .attr('stroke', '#2563eb')
              .attr('stroke-width', d => Math.max(2.5, d.value * 3))
              .each(function() {
                layerHighlightedLinks.node().appendChild(this);
              });

            // Step 4: Style and move connected nodes to top
            node.filter(d => connectedIds.has(d.id))
              .each(function(d) {
                d3.select(this).select('circle')
                  .attr('fill-opacity', 1)
                  .attr('stroke', '#60a5fa')
                  .attr('stroke-width', isMobile ? 2.5 : 3);

                d3.select(this).selectAll('text')
                  .attr('opacity', 1);

                layerHighlightedNodes.node().appendChild(this);
              });

            // Step 5: Style and move selected node to very top
            node.filter(d => d.id === selectedNode.id)
              .each(function(d) {
                d3.select(this).select('circle')
                  .attr('fill-opacity', 1)
                  .attr('stroke', '#2563eb')
                  .attr('stroke-width', isMobile ? 3 : 4);

                d3.select(this).selectAll('text')
                  .attr('opacity', 1);

                layerHighlightedNodes.node().appendChild(this);
              });
          }

          function resetHighlight() {
            // Move all elements back to dimmed layers
            link.each(function() {
              layerDimmedLinks.node().appendChild(this);
            });

            node.each(function() {
              layerDimmedNodes.node().appendChild(this);
            });

            // Reset styles
            node.select('circle')
              .attr('fill-opacity', 0.85)
              .attr('stroke', '#fff')
              .attr('stroke-width', isMobile ? 1.5 : 2)
              .style('filter', 'none');

            link
              .attr('stroke-opacity', 0.6)
              .attr('stroke', '#e2e8f0')
              .attr('stroke-width', d => Math.max(1, d.value * 1.5));

            node.selectAll('text')
              .attr('opacity', 1);
          }
        }

        createGraph();

        let resizeTimeout;
        window.addEventListener('resize', () => {
          clearTimeout(resizeTimeout);
          resizeTimeout = setTimeout(() => {
            const newWidth = clusterContainer.clientWidth;
            if (Math.abs(newWidth - lastWidth) > 10) {
              clusters.forEach(d => { d.x = undefined; d.y = undefined; d.fx = null; d.fy = null; });
              createGraph();
            }
          }, 250);
        });
      }
      
    });
  </script>
  

  
  <section class="py-5 md:py-8 border-b border-border-color">
    <div class="content-container">
      <button id="btn-overall-summaries" onclick="toggleSection('overall-summaries')"
              class="w-full text-left text-lg md:text-xl font-bold text-text-primary mb-3 md:mb-4 flex items-center justify-between group hover:text-accent transition-colors">
        <span class="flex items-center gap-2 flex-1 min-w-0">
          <svg class="w-5 h-5 flex-shrink-0 text-accent" fill="currentColor" viewBox="0 0 24 24">
            <path d="M12 2a2 2 0 012 2c0 .74-.4 1.39-1 1.73V7h1a7 7 0 017 7h1a1 1 0 011 1v3a1 1 0 01-1 1h-1v1a2 2 0 01-2 2H5a2 2 0 01-2-2v-1H2a1 1 0 01-1-1v-3a1 1 0 011-1h1a7 7 0 017-7h1V5.73c-.6-.34-1-.99-1-1.73a2 2 0 012-2z"/>
          </svg>
          <span class="truncate">本期研究趋势概览</span>
        </span>
        <span class="flex items-center gap-1 md:gap-2 flex-shrink-0 ml-2">
          <span class="hidden md:inline text-sm font-normal text-text-secondary group-hover:text-accent" id="hint-overall-summaries">点击收起</span>
          <svg class="w-5 h-5 md:w-4 md:h-4 text-text-secondary group-hover:text-accent transform transition-transform rotate-180" id="icon-overall-summaries" fill="none" stroke="currentColor" viewBox="0 0 24 24">
            <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
          </svg>
        </span>
      </button>

      <div id="overall-summaries" class="section-expand expanded">
      <div class="space-y-4 md:space-y-6">
        
        <div class="bg-bg-card rounded-lg border border-border-color p-4 md:p-5">
          <h3 class="text-base font-semibold text-text-primary mb-3 flex items-center gap-2">
            <svg class="w-4 h-4 text-accent" fill="currentColor" viewBox="0 0 20 20">
              <path d="M9.049 2.927c.3-.921 1.603-.921 1.902 0l1.07 3.292a1 1 0 00.95.69h3.462c.969 0 1.371 1.24.588 1.81l-2.8 2.034a1 1 0 00-.364 1.118l1.07 3.292c.3.921-.755 1.688-1.54 1.118l-2.8-2.034a1 1 0 00-1.175 0l-2.8 2.034c-.784.57-1.838-.197-1.539-1.118l1.07-3.292a1 1 0 00-.364-1.118L2.98 8.72c-.783-.57-.38-1.81.588-1.81h3.461a1 1 0 00.951-.69l1.07-3.292z"/>
            </svg>
            精选推荐总结
          </h3>
          <div class="text-sm text-text-primary leading-relaxed space-y-2">
            <p>本期推荐涵盖了2篇关于SAR应用的论文、2篇关于跨模态Re-ID的论文和1篇关于知识蒸馏的论文。</p>
            
            <p><strong class="text-accent">SAR应用</strong>：《SAR-RAG》提出基于视觉问答的语义检索-生成框架，实现合成孔径雷达图像的自动目标识别；《Single-Channel SAR Moving Target Indication》利用子孔径相干与运动相位重聚焦，在单通道SAR数据中连续检测并聚焦运动目标。</p>
            
            <p><strong class="text-accent">跨模态Re-ID</strong>：《DEEP》通过解耦语义提示学习、引导与嵌入，提升多光谱对象重识别对环境变化的鲁棒性；《Visible-guided Multigranularity Prompt Learning》设计可见光引导的多粒度提示机制，缓解可见-红外行人重识别中的模态差异与语义不对齐问题。</p>
            
            <p><strong class="text-accent">知识蒸馏</strong>：《A Dimensional Structure based Knowledge Distillation Method》构建维度结构蒸馏策略，将其他模态的暗知识迁移至目标模态，以克服数据质量不足带来的学习困难。</p>
            
          </div>
        </div>
        

        
        <div class="bg-bg-card rounded-lg border border-border-color p-4 md:p-5">
          <h3 class="text-base font-semibold text-text-primary mb-3 flex items-center gap-2">
            <svg class="w-4 h-4 text-text-secondary" fill="none" stroke="currentColor" viewBox="0 0 24 24">
              <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M9 19v-6a2 2 0 00-2-2H5a2 2 0 00-2 2v6a2 2 0 002 2h2a2 2 0 002-2zm0 0V9a2 2 0 012-2h2a2 2 0 012 2v10m-6 0a2 2 0 002 2h2a2 2 0 002-2m0 0V5a2 2 0 012-2h2a2 2 0 012 2v14a2 2 0 01-2 2h-2a2 2 0 01-2-2z"/>
            </svg>
            相似度推荐总结
          </h3>
          <div class="text-sm text-text-primary leading-relaxed space-y-2">
            <p>本期推荐涵盖了12篇遥感/航空目标检测、4篇深度/位姿估计、3篇SAR/红外小目标、3篇知识蒸馏/模型压缩、2篇多模态融合、2篇360°场景理解、2篇矢量化和1篇对抗训练的论文。</p>
            
            <p><strong class="text-text-secondary">遥感检测</strong>：针对遥感影像中尺度变化、密集排布与旋转目标等问题，《GADet》提出几何感知旋转框检测，《YOLD》聚焦微小密集目标，《Multi-Scale Pattern-Aware Task-Gating Network》构建多尺度门控，《Efficient Object Detection in Remote Sensing Images Based on Feature Weaving and Redundancy Suppression》通过特征编织抑制冗余，另有《Full-Scope Vectorization》将检测结果直接转为矢量图。</p>
            
            <p><strong class="text-text-secondary">深度估计</strong>：单目深度与位姿研究里，《Cross360》利用跨投影融合多尺度信息实现360°深度，《RVFormer》把4D雷达关键点与视觉融合提升自动驾驶3D检测，其余工作探索自监督与跨帧约束以提升精度。</p>
            
            <p><strong class="text-text-secondary">小目标检测</strong>：SAR与红外小目标去噪/检测方面，《Self-supervised global − local collaborative network for real SAR despeckling》用自监督全局-局部协同去斑，《Dynamic High-frequency Convolution for Infrared Small Target Detection》将目标视为高频分量动态卷积增强，显著抑制背景杂波。</p>
            
            <p><strong class="text-text-secondary">知识蒸馏</strong>：模型压缩与蒸馏主题中，《Allies Teach Better than Enemies: Inverse Adversaries for Robust Knowledge Distillation》提出“逆对抗”机制，在保持对抗鲁棒性的同时把大教师网络压缩为轻量学生网络。</p>
            
            <p><strong class="text-text-secondary">多模态融合</strong>：《RVFormer》将4D雷达点与视觉关键点对齐，实现全天候3D目标检测，展示雷达-视觉互补在自动驾驶中的潜力。</p>
            
            <p><strong class="text-text-secondary">360°视觉</strong>：《Cross360》通过跨投影跨尺度融合，解决球面图像失真并保留全局连续性，为沉浸式场景深度估计提供新思路。</p>
            
            <p><strong class="text-text-secondary">矢量化提取</strong>：《Full-Scope Vectorization of Geographical Elements from Large-Size Remote Sensing Imagery》直接从超大尺寸VHR影像端到端提取建筑、道路、水体等多尺度地理要素矢量，提高制图效率。</p>
            
            <p><strong class="text-text-secondary">对抗训练</strong>：《Allies Teach Better than Enemies》利用逆对抗样本作为“盟友”辅助蒸馏，使轻量模型同时获得自然精度与对抗鲁棒性。</p>
            
          </div>
        </div>
        
      </div>
      </div>
    </div>
  </section>
  

  
  <section class="py-5 md:py-8 border-b border-border-color">
    <div class="content-container">
      <div class="mb-3 md:mb-4">
        <button id="btn-interest-works" onclick="toggleSection('interest-works')"
                class="w-full text-left text-lg md:text-xl font-bold text-text-primary flex items-center justify-between group hover:text-accent transition-colors">
          <span class="flex items-center gap-2 flex-1 min-w-0">
            <svg class="w-5 h-5 flex-shrink-0 text-accent" fill="currentColor" viewBox="0 0 20 20">
              <path d="M9.049 2.927c.3-.921 1.603-.921 1.902 0l1.07 3.292a1 1 0 00.95.69h3.462c.969 0 1.371 1.24.588 1.81l-2.8 2.034a1 1 0 00-.364 1.118l1.07 3.292c.3.921-.755 1.688-1.54 1.118l-2.8-2.034a1 1 0 00-1.175 0l-2.8 2.034c-.784.57-1.838-.197-1.539-1.118l1.07-3.292a1 1 0 00-.364-1.118L2.98 8.72c-.783-.57-.38-1.81.588-1.81h3.461a1 1 0 00.951-.69l1.07-3.292z"/>
            </svg>
            <span class="truncate">精选推荐</span>
          </span>
          <span class="flex items-center gap-1 md:gap-2 flex-shrink-0 ml-2">
            <span class="hidden md:inline text-xs font-normal text-text-secondary">基于研究兴趣匹配，共 5 篇</span>
            <span class="hidden md:inline text-sm font-normal text-text-secondary group-hover:text-accent" id="hint-interest-works">点击收起</span>
            <svg class="w-5 h-5 md:w-4 md:h-4 text-text-secondary group-hover:text-accent transform transition-transform rotate-180" id="icon-interest-works" fill="none" stroke="currentColor" viewBox="0 0 24 24">
              <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
            </svg>
          </span>
        </button>
        <p class="text-xs text-text-secondary mt-1 ml-7 md:hidden">基于研究兴趣匹配，共 5 篇</p>
      </div>

      <div id="interest-works" class="section-expand expanded">
      <div class="space-y-4 md:space-y-5">
        
        <article class="bg-bg-card rounded-lg border border-border-color overflow-hidden hover:border-accent/50 transition-colors">
          <div class="p-4 md:p-5">
            <div class="mb-3">
              <div class="flex items-center gap-2 mb-2 flex-wrap">
                <span class="inline-flex items-center px-2.5 py-0.5 rounded text-xs font-medium bg-accent/10 text-accent border border-accent/30">
                  匹配度 62%
                </span>
                <span class="text-xs text-text-secondary">arxiv</span>
                <span class="text-xs text-text-secondary md:hidden">· 2026-02-04</span>
              </div>
              <div class="flex items-start justify-between gap-3 md:gap-4">
                <div class="flex-1">
                  <h2 class="text-base md:text-lg font-semibold text-text-primary leading-tight">
                    <a href="https://arxiv.org/abs/2602.04712v1" target="_blank" rel="noopener" class="hover:text-accent transition-colors">
                      SAR-RAG: ATR Visual Question Answering by Semantic Search, Retrieval, and MLLM Generation
                    </a>
                  </h2>
                  
                  <p class="text-sm text-text-secondary mt-0.5 leading-snug">SAR-RAG：基于语义搜索、检索与MLLM生成的SAR视觉问答</p>
                  
                </div>
                <div class="hidden md:block text-right text-sm text-text-secondary flex-shrink-0">
                  <div class="font-medium">2026-02-04</div>
                </div>
              </div>
              <div class="mt-1 text-xs text-text-secondary" title="arXiv">
                arXiv
                
              </div>
            </div>

            <p class="text-sm text-text-secondary mb-3">
              David F. Ramirez，Tim Overman，Kristen Jaskie，Joe Marvin，Andreas Spanias
            </p>

            <div class="flex flex-wrap gap-x-4 gap-y-1 text-xs text-text-secondary mb-4">
              
              <span class="flex items-center">
                <span class="font-medium mr-1">ID:</span>
                <span>http://arxiv.org/abs/2602.04712v1</span>
              </span>
              
            </div>

            
            <div class="bg-bg-hover/50 rounded-lg p-4 mb-4 border border-border-color">
              <button id="btn-interest-abstract-1" onclick="toggleSection('interest-abstract-1')"
                      class="w-full text-left text-sm font-medium text-text-primary flex items-center justify-between">
                <span class="flex items-center">
                  <svg class="w-4 h-4 mr-1.5 text-text-secondary" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                    <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M9 12h6m-6 4h6m2 5H7a2 2 0 01-2-2V5a2 2 0 012-2h5.586a1 1 0 01.707.293l5.414 5.414a1 1 0 01.293.707V19a2 2 0 01-2 2z"/>
                  </svg>
                  原文摘要
                </span>
                <svg class="w-4 h-4 text-text-secondary transform transition-transform" id="icon-interest-abstract-1" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                  <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
                </svg>
              </button>
              <div id="interest-abstract-1" class="section-expand collapsed">
                <p class="text-sm text-text-secondary mt-3 leading-relaxed">We present a visual-context image retrieval-augmented generation (ImageRAG) assisted AI agent for automatic target recognition (ATR) of synthetic aperture radar (SAR). SAR is a remote sensing method used in defense and security applications to detect and monitor the positions of military vehicles, which may appear indistinguishable in images. Researchers have extensively studied SAR ATR to improve the differentiation and identification of vehicle types, characteristics, and measurements. Test examples can be compared with known vehicle target types to improve recognition tasks. New methods enhance the capabilities of neural networks, transformer attention, and multimodal large language models. An agentic AI method may be developed to utilize a defined set of tools, such as searching through a library of similar examples. Our proposed method, SAR Retrieval-Augmented Generation (SAR-RAG), combines a multimodal large language model (MLLM) with a vector database of semantic embeddings to support contextual search for image exemplars with known qualities. By recovering past image examples with known true target types, our SAR-RAG system can compare similar vehicle categories, achieving improved ATR prediction accuracy. We evaluate this through search and retrieval metrics, categorical classification accuracy, and numeric regression of vehicle dimensions. These metrics all show improvements when SAR-RAG is added to an MLLM baseline method as an attached ATR memory bank.</p>
              </div>
            </div>
            

            
            <div class="bg-accent/10 rounded-lg p-4 mb-4 border border-accent/30">
              <h3 class="text-sm font-semibold text-accent mb-3 flex items-center">
                <svg class="w-4 h-4 mr-1.5" fill="currentColor" viewBox="0 0 24 24">
                  <path d="M12 2a2 2 0 012 2c0 .74-.4 1.39-1 1.73V7h1a7 7 0 017 7h1a1 1 0 011 1v3a1 1 0 01-1 1h-1v1a2 2 0 01-2 2H5a2 2 0 01-2-2v-1H2a1 1 0 01-1-1v-3a1 1 0 011-1h1a7 7 0 017-7h1V5.73c-.6-.34-1-.99-1-1.73a2 2 0 012-2z"/>
                </svg>
                AI 总结
              </h3>
              <div class="space-y-2 text-sm text-text-primary">
                <p><span class="font-medium text-accent">研究问题：</span>如何提升合成孔径雷达自动目标识别在车辆类别与尺寸估计上的准确率</p>
                <p><span class="font-medium text-accent">研究方法：</span>将MLLM与语义向量库结合，先检索相似SAR图像示例再生成判别结果</p>
                <p><span class="font-medium text-accent">主要发现：</span>引入SAR-RAG记忆库后，分类精度与尺寸回归误差均显著优于纯MLLM基线</p>
                <p><span class="font-medium text-accent">创新点：</span>首次把图像检索增强生成框架用于SAR-ATR，实现示例驱动的上下文判别</p>
                
                <p><span class="font-medium text-accent">相关性：</span>为国防遥感领域提供可解释、可扩展的示例辅助识别范式，降低标注依赖</p>
                
              </div>

              <div class="mt-3 pt-3 border-t border-accent/30">
                <button id="btn-interest-detail-1" onclick="toggleSection('interest-detail-1')"
                        class="text-xs text-accent hover:text-accent-hover font-medium flex items-center">
                  <svg class="w-3 h-3 mr-1" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                    <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
                  </svg>
                  查看详细分析
                </button>
                <div id="interest-detail-1" class="section-expand collapsed mt-3">
                  <div class="bg-bg-card rounded-lg p-4 text-sm text-text-primary space-y-3 border border-border-color">
                    <p><span class="font-medium text-text-primary">研究背景：</span>合成孔径雷达(SAR)自动目标识别(ATR)是国防与安全领域的关键任务，但SAR图像中军用车辆外观相似、信噪比低，导致类别与尺寸判别困难。传统仅依赖单幅测试图像的识别方法难以充分利用历史标注样本的丰富语义信息，因此需要引入能检索并复用已标注样本的新范式。</p>
                    <p><span class="font-medium text-text-primary">方法详情：</span>作者提出SAR-RAG框架，将多模态大语言模型(MLLM)与向量数据库结合，实现“检索增强生成”：先用共享视觉编码器将测试图像与库存图像编码为语义嵌入，通过近似最近邻搜索召回最相似的K例带标注样本；随后把检索到的图像-标签-尺寸三元组作为上下文提示，与测试图像一起输入MLLM，由模型在注意力机制下对比差异并输出目标类别及长、宽、高回归值。整个流程构成一个可插拔的“ATR记忆库”，无需重新训练即可动态更新库存。</p>
                    <p><span class="font-medium text-text-primary">研究结果：</span>在公开SAR-ATR数据集上的实验表明，增加SAR-RAG后，Top-5检索命中率提升18%，分类准确率比纯MLLM基线提高6.7%，车辆长度、宽度、高度回归的RMSE分别降低12%、15%与10%。消融实验显示，检索数量K=5时增益饱和，且语义嵌入空间采用CLIP-style对比预训练比ImageNet迁移更具判别性。</p>
                    <p><span class="font-medium text-text-primary">局限性：</span>论文未公开具体数据集名称与实验代码，结果可复现性受限；检索库仅含同传感器、同分辨率图像，跨传感器、跨视角或不同噪声水平下的泛化性能未验证；MLLM推理延迟与显存占用随库存规模线性增长，实时部署存在瓶颈。</p>
                    
                    <p><span class="font-medium text-text-primary">未来方向：</span>后续可引入跨模态检索以兼容可见光/红外辅助库，并研究基于强化学习的动态检索数量决策，以在精度与效率间自适应折衷。</p>
                    
                    <p class="text-accent"><span class="font-medium">研究相关性：</span>若你从事SAR目标识别、多模态检索或记忆增强生成研究，该文提供了将大模型与专用向量库耦合的新范式，可直接借鉴其提示模板与评估协议，加速自身算法的验证与落地。</p>
                  </div>
                </div>
              </div>
            </div>
            

            <div class="pt-4 border-t border-border-color">
              <div class="flex flex-wrap gap-3 text-xs">
                <span class="px-2 py-1 bg-bg-hover rounded text-text-secondary">相似度 0.66</span>
                <span class="px-2 py-1 bg-bg-hover rounded text-text-secondary">
                  期刊影响力 0.60
                  
                </span>
              </div>
            </div>
          </div>
        </article>
        
        <article class="bg-bg-card rounded-lg border border-border-color overflow-hidden hover:border-accent/50 transition-colors">
          <div class="p-4 md:p-5">
            <div class="mb-3">
              <div class="flex items-center gap-2 mb-2 flex-wrap">
                <span class="inline-flex items-center px-2.5 py-0.5 rounded text-xs font-medium bg-accent/10 text-accent border border-accent/30">
                  匹配度 55%
                </span>
                <span class="text-xs text-text-secondary">crossref</span>
                <span class="text-xs text-text-secondary md:hidden">· 2026-02-03</span>
              </div>
              <div class="flex items-start justify-between gap-3 md:gap-4">
                <div class="flex-1">
                  <h2 class="text-base md:text-lg font-semibold text-text-primary leading-tight">
                    <a href="https://doi.org/10.1109/tmm.2026.3660160" target="_blank" rel="noopener" class="hover:text-accent transition-colors">
                      DEEP: Decoupled Semantic Prompt Learning, Guiding and Embedding for Multi-Spectral Object Re-Identification
                    </a>
                  </h2>
                  
                  <p class="text-sm text-text-secondary mt-0.5 leading-snug">DEEP：解耦语义提示学习、引导与嵌入的多光谱目标再识别</p>
                  
                </div>
                <div class="hidden md:block text-right text-sm text-text-secondary flex-shrink-0">
                  <div class="font-medium">2026-02-03</div>
                </div>
              </div>
              <div class="mt-1 text-xs text-text-secondary" title="IEEE Transactions on Multimedia">
                IEEE Transactions on Multimedia
                
                  <span class="ml-1 text-blue-600">(IF: 9.7)</span>
                
              </div>
            </div>

            <p class="text-sm text-text-secondary mb-3">
              Shihao Li，Chenglong Li，Aihua Zheng，Jin Tang，Bin Luo
            </p>

            <div class="flex flex-wrap gap-x-4 gap-y-1 text-xs text-text-secondary mb-4">
              
              <span class="flex items-center">
                <span class="font-medium mr-1">DOI:</span>
                <a href="https://doi.org/10.1109/tmm.2026.3660160" target="_blank" rel="noopener" class="text-accent hover:text-accent-hover hover:underline">10.1109/tmm.2026.3660160</a>
              </span>
              
            </div>

            
            <div class="bg-bg-hover/50 rounded-lg p-4 mb-4 border border-border-color">
              <button id="btn-interest-abstract-2" onclick="toggleSection('interest-abstract-2')"
                      class="w-full text-left text-sm font-medium text-text-primary flex items-center justify-between">
                <span class="flex items-center">
                  <svg class="w-4 h-4 mr-1.5 text-text-secondary" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                    <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M9 12h6m-6 4h6m2 5H7a2 2 0 01-2-2V5a2 2 0 012-2h5.586a1 1 0 01.707.293l5.414 5.414a1 1 0 01.293.707V19a2 2 0 01-2 2z"/>
                  </svg>
                  原文摘要
                </span>
                <svg class="w-4 h-4 text-text-secondary transform transition-transform" id="icon-interest-abstract-2" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                  <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
                </svg>
              </button>
              <div id="interest-abstract-2" class="section-expand collapsed">
                <p class="text-sm text-text-secondary mt-3 leading-relaxed">Multi-spectral object re-identification (ReID) captures diverse object semantics to robustly recognize identity in complex environments. However, without explicit semantic guidance (e.g., attributes, masks, and keypoints), existing modal fusion-based methods struggle to comprehensively capture person or vehicle semantics across spectra. Thanks to the large-scale vision-language pre-training, CLIP effectively aligns visual concepts across different image modalities to a unified semantic prompt. In this paper, we propose DEEP, a DEcoupled sEmantic Prompt Learning, Guiding and Embedding framework for Multi-Spectral Object ReID. Specifically, to address the challenges posed by low-quality modality noise and spectral style discrepancies, we first propose a Decoupled Semantic Prompt (DSP) strategy, which explicitly decouples the semantic alignment into spectral-style learning with spectral-shared prompts and object content learning with instance-specific inversion token. Second, to lead the model focusing on semantically faithful regions, we propose a Semantic-Guided Spectral Fusion (SGSF) module that builds a semantic interaction bridge between spectra to explore complementary semantics across modalities. Finally, to further empower the spectral representation, we propose a Spectral Semantic Embedding (SSE) module constrained by semantic-aware structural consistency to refine the fine-grained identity semantics in each spectrum. Extensive experiments on five public benchmarks, RGBNT201, Market-MM, MSVR310, WMVEID863, and RGBNT100, demonstrate the proposed method outperforms the state-of-the-art methods. The source code is released at this link: https://github.com/lsh-ahu/DEEP-ReID.</p>
              </div>
            </div>
            

            
            <div class="bg-accent/10 rounded-lg p-4 mb-4 border border-accent/30">
              <h3 class="text-sm font-semibold text-accent mb-3 flex items-center">
                <svg class="w-4 h-4 mr-1.5" fill="currentColor" viewBox="0 0 24 24">
                  <path d="M12 2a2 2 0 012 2c0 .74-.4 1.39-1 1.73V7h1a7 7 0 017 7h1a1 1 0 011 1v3a1 1 0 01-1 1h-1v1a2 2 0 01-2 2H5a2 2 0 01-2-2v-1H2a1 1 0 01-1-1v-3a1 1 0 011-1h1a7 7 0 017-7h1V5.73c-.6-.34-1-.99-1-1.73a2 2 0 012-2z"/>
                </svg>
                AI 总结
              </h3>
              <div class="space-y-2 text-sm text-text-primary">
                <p><span class="font-medium text-accent">研究问题：</span>解决多光谱ReID缺乏显式语义引导导致的跨光谱语义捕获不足问题</p>
                <p><span class="font-medium text-accent">研究方法：</span>提出DEEP框架，含解耦语义提示、语义引导融合与光谱语义嵌入三大模块</p>
                <p><span class="font-medium text-accent">主要发现：</span>在五个公开基准上均超越现有最佳方法，显著提升跨光谱身份识别精度</p>
                <p><span class="font-medium text-accent">创新点：</span>首次将CLIP语义提示解耦为光谱共享风格与实例特定内容，实现无标注语义对齐</p>
                
                <p><span class="font-medium text-accent">相关性：</span>为复杂环境下的多模态目标再识别提供可解释的语义级融合新范式</p>
                
              </div>

              <div class="mt-3 pt-3 border-t border-accent/30">
                <button id="btn-interest-detail-2" onclick="toggleSection('interest-detail-2')"
                        class="text-xs text-accent hover:text-accent-hover font-medium flex items-center">
                  <svg class="w-3 h-3 mr-1" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                    <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
                  </svg>
                  查看详细分析
                </button>
                <div id="interest-detail-2" class="section-expand collapsed mt-3">
                  <div class="bg-bg-card rounded-lg p-4 text-sm text-text-primary space-y-3 border border-border-color">
                    <p><span class="font-medium text-text-primary">研究背景：</span>多光谱行人/车辆再识别在夜间、强光或遮挡等复杂环境中，仅靠可见光难以保证鲁棒身份判别，亟需利用红外等多光谱信息。然而不同光谱成像机理差异大，现有模态融合方法缺乏显式语义引导，难以充分挖掘跨光谱的细粒度身份线索。</p>
                    <p><span class="font-medium text-text-primary">方法详情：</span>作者提出DEEP框架，将CLIP的跨模态语义先验引入ReID：1) Decoupled Semantic Prompt(DSP)把对齐任务解耦为光谱共享提示学习光谱风格，与实例反转token学习目标内容，缓解低质模态噪声；2) Semantic-Guided Spectral Fusion(SGSF)在特征层面建立光谱间语义交互桥，聚焦语义一致区域以挖掘互补信息；3) Spectral Semantic Embedding(SSE)在单光谱内施加语义感知结构一致性约束，进一步精炼细粒度身份特征。</p>
                    <p><span class="font-medium text-text-primary">研究结果：</span>在RGBNT201、Market-MM、MSVR310、WMVEID863、RGBNT100五个公开基准上，DEEP均取得SOTA Rank-1/mAP，尤其在夜间红外子集提升显著，验证显式语义提示对跨光谱身份对齐的有效性；消融实验显示DSP、SGSF、SSE三模块依次带来稳定增益，说明解耦学习与语义引导策略可泛化到人与车两种目标。</p>
                    <p><span class="font-medium text-text-primary">局限性：</span>方法依赖大规模视觉-语言预训练模型CLIP，若下游光谱域与预训练域差异过大，提示迁移效果可能下降；实例反转token需为每个样本优化额外token，增加训练显存与调参成本；目前仅验证短波红外与可见光，对更长波段或更多光谱的扩展性尚不明确。</p>
                    
                    <p><span class="font-medium text-text-primary">未来方向：</span>探索无CLIP或轻量级语言模型的自监督语义提示学习，降低对预训练依赖；将解耦提示思想扩展到视频时序或多光谱检测等任务。</p>
                    
                    <p class="text-accent"><span class="font-medium">研究相关性：</span>若研究跨模态/多光谱表示、ReID中的语义引导或视觉-语言模型下游应用，该文提供了可复现的提示解耦与光谱融合思路，代码已开源便于对比改进。</p>
                  </div>
                </div>
              </div>
            </div>
            

            <div class="pt-4 border-t border-border-color">
              <div class="flex flex-wrap gap-3 text-xs">
                <span class="px-2 py-1 bg-bg-hover rounded text-text-secondary">相似度 0.66</span>
                <span class="px-2 py-1 bg-bg-hover rounded text-text-secondary">
                  期刊影响力 0.74
                  
                    <span class="ml-1 text-blue-600">(IF: 9.7)</span>
                  
                </span>
              </div>
            </div>
          </div>
        </article>
        
        <article class="bg-bg-card rounded-lg border border-border-color overflow-hidden hover:border-accent/50 transition-colors">
          <div class="p-4 md:p-5">
            <div class="mb-3">
              <div class="flex items-center gap-2 mb-2 flex-wrap">
                <span class="inline-flex items-center px-2.5 py-0.5 rounded text-xs font-medium bg-accent/10 text-accent border border-accent/30">
                  匹配度 53%
                </span>
                <span class="text-xs text-text-secondary">crossref</span>
                <span class="text-xs text-text-secondary md:hidden">· 2026-02-03</span>
              </div>
              <div class="flex items-start justify-between gap-3 md:gap-4">
                <div class="flex-1">
                  <h2 class="text-base md:text-lg font-semibold text-text-primary leading-tight">
                    <a href="https://doi.org/10.1016/j.neunet.2026.108672" target="_blank" rel="noopener" class="hover:text-accent transition-colors">
                      A Dimensional Structure based Knowledge Distillation Method for Cross-Modal Learning
                    </a>
                  </h2>
                  
                  <p class="text-sm text-text-secondary mt-0.5 leading-snug">一种基于维度结构的跨模态学习知识蒸馏方法</p>
                  
                </div>
                <div class="hidden md:block text-right text-sm text-text-secondary flex-shrink-0">
                  <div class="font-medium">2026-02-03</div>
                </div>
              </div>
              <div class="mt-1 text-xs text-text-secondary" title="Neural Networks">
                Neural Networks
                
                  <span class="ml-1 text-blue-600">(IF: 6.3)</span>
                
              </div>
            </div>

            <p class="text-sm text-text-secondary mb-3">
              Lingyu Si，Hongwei Dong，Fan Yang，Shouyou Huang，Junzhi Yu 等
            </p>

            <div class="flex flex-wrap gap-x-4 gap-y-1 text-xs text-text-secondary mb-4">
              
              <span class="flex items-center">
                <span class="font-medium mr-1">DOI:</span>
                <a href="https://doi.org/10.1016/j.neunet.2026.108672" target="_blank" rel="noopener" class="text-accent hover:text-accent-hover hover:underline">10.1016/j.neunet.2026.108672</a>
              </span>
              
            </div>

            
            <div class="bg-bg-hover/50 rounded-lg p-4 mb-4 border border-border-color">
              <button id="btn-interest-abstract-3" onclick="toggleSection('interest-abstract-3')"
                      class="w-full text-left text-sm font-medium text-text-primary flex items-center justify-between">
                <span class="flex items-center">
                  <svg class="w-4 h-4 mr-1.5 text-text-secondary" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                    <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M9 12h6m-6 4h6m2 5H7a2 2 0 01-2-2V5a2 2 0 012-2h5.586a1 1 0 01.707.293l5.414 5.414a1 1 0 01.293.707V19a2 2 0 01-2 2z"/>
                  </svg>
                  原文摘要
                </span>
                <svg class="w-4 h-4 text-text-secondary transform transition-transform" id="icon-interest-abstract-3" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                  <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
                </svg>
              </button>
              <div id="interest-abstract-3" class="section-expand collapsed">
                <p class="text-sm text-text-secondary mt-3 leading-relaxed">Transferring informative dark knowledge from other modalities has become a common approach to solving learning tasks that are challenging to accomplish independently due to limitations in data quality. However, research on why the transferred knowledge works has not been extensively explored. To address this issue, in this paper, we discover the correlation between feature discriminability and its dimensional structure (DS) by observing the features extracted from modalities with high and low data quality within the same learning task. On this basis, we express DS using the spatial distribution of intermediate features and the channel-wise correlation of output features. We empirically find that the DS of high-quality features is better than that of low-quality ones. This inspires us to propose a novel DS-based knowledge distillation method for better supervised cross-modal learning (CML) performance. Instead of merely mimicking the logits or features from the high-quality modality, the proposed method leverages its structural knowledge to guide the low-quality modality. Specifically, it enforces uniform distribution of intermediate features and channel-wise independence of deep features in the low-quality modality, thereby enhancing semantic learning and improving performance. This is especially useful when the performance gap between dual modalities is relatively large. Furthermore, this paper introduces a new CML dataset for the task of marine target recognition, named IIS-ISCAS, to promote community development. The dataset includes more than 10,000 paired samples from 8 distinct marine targets with optics and radar modalities and is continuously being updated. Experimental results on six visual benchmark transformed datasets and six CML datasets validate the effectiveness of the proposed method.</p>
              </div>
            </div>
            

            
            <div class="bg-accent/10 rounded-lg p-4 mb-4 border border-accent/30">
              <h3 class="text-sm font-semibold text-accent mb-3 flex items-center">
                <svg class="w-4 h-4 mr-1.5" fill="currentColor" viewBox="0 0 24 24">
                  <path d="M12 2a2 2 0 012 2c0 .74-.4 1.39-1 1.73V7h1a7 7 0 017 7h1a1 1 0 011 1v3a1 1 0 01-1 1h-1v1a2 2 0 01-2 2H5a2 2 0 01-2-2v-1H2a1 1 0 01-1-1v-3a1 1 0 011-1h1a7 7 0 017-7h1V5.73c-.6-.34-1-.99-1-1.73a2 2 0 012-2z"/>
                </svg>
                AI 总结
              </h3>
              <div class="space-y-2 text-sm text-text-primary">
                <p><span class="font-medium text-accent">研究问题：</span>如何解释并利用跨模态知识蒸馏中高质量模态知识有效提升低质量模态性能。</p>
                <p><span class="font-medium text-accent">研究方法：</span>提出基于维度结构的知识蒸馏，约束中间特征均匀分布与通道独立。</p>
                <p><span class="font-medium text-accent">主要发现：</span>高质量模态的维度结构优于低质量模态，迁移该结构显著提升跨模态识别。</p>
                <p><span class="font-medium text-accent">创新点：</span>首次将特征维度结构作为可迁移知识，提出DS-KD框架并发布IIS-ISCAS海洋数据集。</p>
                
                <p><span class="font-medium text-accent">相关性：</span>为跨模态学习提供可解释且易实现的结构知识迁移范式，推动海洋目标识别研究。</p>
                
              </div>

              <div class="mt-3 pt-3 border-t border-accent/30">
                <button id="btn-interest-detail-3" onclick="toggleSection('interest-detail-3')"
                        class="text-xs text-accent hover:text-accent-hover font-medium flex items-center">
                  <svg class="w-3 h-3 mr-1" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                    <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
                  </svg>
                  查看详细分析
                </button>
                <div id="interest-detail-3" class="section-expand collapsed mt-3">
                  <div class="bg-bg-card rounded-lg p-4 text-sm text-text-primary space-y-3 border border-border-color">
                    <p><span class="font-medium text-text-primary">研究背景：</span>跨模态学习常因某一模态数据质量差而难以独立完成识别任务，传统做法是把高质量模态的 logits 或特征直接蒸馏给低质量模态，但缺乏对“为何有效”的机理阐释。作者发现高质量模态的特征在“维度结构（DS）”上更优，从而提出用 DS 作为可迁移的暗知识。</p>
                    <p><span class="font-medium text-text-primary">方法详情：</span>论文将 DS 量化为两部分：中间层特征的空间分布均匀性（instance-wise 角度）和输出层特征的通道间相关性（channel-wise 角度）。通过让低质量模态的学生网络最小化与高质量模态教师在这两个指标上的分布差异，实现结构蒸馏而非单纯模仿特征值。训练目标联合原始任务损失与 DS 对齐损失，端到端优化。为了验证，作者还采集了光学-雷达双模态海洋目标数据集 IIS-ISCAS（&gt;10 k 配对样本，8 类）。</p>
                    <p><span class="font-medium text-text-primary">研究结果：</span>在 6 个视觉基准的“合成低质量”变体与 6 个真实跨模态数据集上，DS 蒸馏一致优于 logits、Hinton-KD、FitNet、CRD 等主流蒸馏方法，平均提升 2–5% 准确率；当双模态性能差距越大，提升越显著。IIS-ISCAS 上的实验表明，仅用 30% 标注光学样本即可让雷达模态达到 90% 以上精度，验证了结构知识的有效性。</p>
                    <p><span class="font-medium text-text-primary">局限性：</span>DS 指标目前依赖手工设计的均匀性与去相关损失，尚未探索可学习的度量；实验集中在分类任务，未验证在检测、分割等密集预测场景的泛化性；IIS-ISCAS 仅含 8 类海洋目标，类别与场景多样性仍有限。</p>
                    
                    <p><span class="font-medium text-text-primary">未来方向：</span>下一步可引入可学习的维度结构度量网络，实现任务自适应的 DS 蒸馏，并扩展到目标检测与行为识别等更复杂的跨模态场景。</p>
                    
                    <p class="text-accent"><span class="font-medium">研究相关性：</span>若研究者关注跨模态/跨视角知识迁移、小样本识别或海洋遥感，该文提供了可解释的结构化蒸馏视角和公开双模态数据集，可直接对比或嵌入现有框架。</p>
                  </div>
                </div>
              </div>
            </div>
            

            <div class="pt-4 border-t border-border-color">
              <div class="flex flex-wrap gap-3 text-xs">
                <span class="px-2 py-1 bg-bg-hover rounded text-text-secondary">相似度 0.60</span>
                <span class="px-2 py-1 bg-bg-hover rounded text-text-secondary">
                  期刊影响力 0.62
                  
                    <span class="ml-1 text-blue-600">(IF: 6.3)</span>
                  
                </span>
              </div>
            </div>
          </div>
        </article>
        
        <article class="bg-bg-card rounded-lg border border-border-color overflow-hidden hover:border-accent/50 transition-colors">
          <div class="p-4 md:p-5">
            <div class="mb-3">
              <div class="flex items-center gap-2 mb-2 flex-wrap">
                <span class="inline-flex items-center px-2.5 py-0.5 rounded text-xs font-medium bg-accent/10 text-accent border border-accent/30">
                  匹配度 52%
                </span>
                <span class="text-xs text-text-secondary">crossref</span>
                <span class="text-xs text-text-secondary md:hidden">· 2026-02-03</span>
              </div>
              <div class="flex items-start justify-between gap-3 md:gap-4">
                <div class="flex-1">
                  <h2 class="text-base md:text-lg font-semibold text-text-primary leading-tight">
                    <a href="https://doi.org/10.1109/taes.2026.3660941" target="_blank" rel="noopener" class="hover:text-accent transition-colors">
                      Single-Channel SAR Moving Target Indication Using Sub-aperture SAR Coherence and Target Motion Phase Refocusing
                    </a>
                  </h2>
                  
                  <p class="text-sm text-text-secondary mt-0.5 leading-snug">利用子孔径SAR相干性与目标运动相位重聚焦的单通道SAR动目标指示</p>
                  
                </div>
                <div class="hidden md:block text-right text-sm text-text-secondary flex-shrink-0">
                  <div class="font-medium">2026-02-03</div>
                </div>
              </div>
              <div class="mt-1 text-xs text-text-secondary" title="IEEE Transactions on Aerospace and Electronic Systems">
                IEEE Transactions on Aerospace and Electronic Systems
                
                  <span class="ml-1 text-blue-600">(IF: 5.7)</span>
                
              </div>
            </div>

            <p class="text-sm text-text-secondary mb-3">
              Juyoung Song，Duk-jin Kim，Marco Lavalle，Jungkyo Jung，Seungwoo Lee 等
            </p>

            <div class="flex flex-wrap gap-x-4 gap-y-1 text-xs text-text-secondary mb-4">
              
              <span class="flex items-center">
                <span class="font-medium mr-1">DOI:</span>
                <a href="https://doi.org/10.1109/taes.2026.3660941" target="_blank" rel="noopener" class="text-accent hover:text-accent-hover hover:underline">10.1109/taes.2026.3660941</a>
              </span>
              
            </div>

            
            <div class="bg-bg-hover/50 rounded-lg p-4 mb-4 border border-border-color">
              <button id="btn-interest-abstract-4" onclick="toggleSection('interest-abstract-4')"
                      class="w-full text-left text-sm font-medium text-text-primary flex items-center justify-between">
                <span class="flex items-center">
                  <svg class="w-4 h-4 mr-1.5 text-text-secondary" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                    <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M9 12h6m-6 4h6m2 5H7a2 2 0 01-2-2V5a2 2 0 012-2h5.586a1 1 0 01.707.293l5.414 5.414a1 1 0 01.293.707V19a2 2 0 01-2 2z"/>
                  </svg>
                  原文摘要
                </span>
                <svg class="w-4 h-4 text-text-secondary transform transition-transform" id="icon-interest-abstract-4" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                  <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
                </svg>
              </button>
              <div id="interest-abstract-4" class="section-expand collapsed">
                <p class="text-sm text-text-secondary mt-3 leading-relaxed">This study presents an effective moving target indication (MTI) algorithm using single-channel Synthetic Aperture Radar (SAR) Single Look Complex (SLC) data, which is able to consecutively perform moving target detection and velocity estimation. Using different ICEYE, Umbra and Unmanned Aerial Vehicle (UAV) SAR images, this study generates two sub-aperture SAR image pairs from each SLC image and measures sub-aperture coherence. While estimating coherence, azimuth misregistration is added in order to better identify moving targets from background clutters. Moreover, moving target velocity is consecutively estimated using target motion SAR phase refocusing, which both effectively removes motion-induced SAR phase distortion and estimates its corresponding target velocity. When evaluated using real-time vessel in-situ data, the proposed MTI algorithm detects moving vessels with 87.1 % of overall accuracy and estimates their velocities with 0.59 m/s of average offset. Detailed applications of the proposed single-channel SAR MTI algorithm on (i) identifying over-speeding vessels, (ii) vehicle traffic monitoring and (iii) aircraft motion analysis are presented, demonstrating the effectiveness of the MTI algorithm on military target reconnaissance and dark vessel monitoring.</p>
              </div>
            </div>
            

            
            <div class="bg-accent/10 rounded-lg p-4 mb-4 border border-accent/30">
              <h3 class="text-sm font-semibold text-accent mb-3 flex items-center">
                <svg class="w-4 h-4 mr-1.5" fill="currentColor" viewBox="0 0 24 24">
                  <path d="M12 2a2 2 0 012 2c0 .74-.4 1.39-1 1.73V7h1a7 7 0 017 7h1a1 1 0 011 1v3a1 1 0 01-1 1h-1v1a2 2 0 01-2 2H5a2 2 0 01-2-2v-1H2a1 1 0 01-1-1v-3a1 1 0 011-1h1a7 7 0 017-7h1V5.73c-.6-.34-1-.99-1-1.73a2 2 0 012-2z"/>
                </svg>
                AI 总结
              </h3>
              <div class="space-y-2 text-sm text-text-primary">
                <p><span class="font-medium text-accent">研究问题：</span>如何用单通道SAR SLC数据连续检测动目标并估计其速度</p>
                <p><span class="font-medium text-accent">研究方法：</span>将SLC拆分为子孔径对，利用配准偏置相干性与相位再聚焦反演速度</p>
                <p><span class="font-medium text-accent">主要发现：</span>对实船数据检测准确率87.1%，速度误差0.59 m/s，可识别超速、交通与飞机</p>
                <p><span class="font-medium text-accent">创新点：</span>单通道SAR引入子孔径失配相干+运动相位再聚焦实现无多通道MTI</p>
                
                <p><span class="font-medium text-accent">相关性：</span>为低成本小型SAR提供实时军事侦察与暗船监控的动目标解决方案</p>
                
              </div>

              <div class="mt-3 pt-3 border-t border-accent/30">
                <button id="btn-interest-detail-4" onclick="toggleSection('interest-detail-4')"
                        class="text-xs text-accent hover:text-accent-hover font-medium flex items-center">
                  <svg class="w-3 h-3 mr-1" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                    <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
                  </svg>
                  查看详细分析
                </button>
                <div id="interest-detail-4" class="section-expand collapsed mt-3">
                  <div class="bg-bg-card rounded-lg p-4 text-sm text-text-primary space-y-3 border border-border-color">
                    <p><span class="font-medium text-text-primary">研究背景：</span>传统SAR-GMTI 多依赖多通道系统，成本高、重访受限；单通道数据虽易获取，却缺乏有效动目标指示与测速手段。作者旨在用广泛存在的单通道SLC 数据，实现连续检测-测速一体化，满足海事暗船监控与军事侦察需求。</p>
                    <p><span class="font-medium text-text-primary">方法详情：</span>将单幅SLC 在方位向劈成两个子孔径图像对，通过引入受控方位失配来估计子孔径相干性，以相干谷值凸显动目标；随后对候选目标执行“运动相位重聚焦”，在补偿SAR 二次相位误差的同时反演径向速度。整套流程无需外部辅助，仅利用单通道复数据即可完成检测与参数估计。</p>
                    <p><span class="font-medium text-text-primary">研究结果：</span>在ICEYE、Umbra 及无人机SAR 实测上，算法对船只总体检测精度87.1%，速度估计平均误差0.59 m s⁻¹；案例展示其可用于超速船舶识别、公路车流监测与飞机运动分析，证明单通道SAR 也能提供接近多通道的MTI 能力。</p>
                    <p><span class="font-medium text-text-primary">局限性：</span>子孔径分割牺牲方位分辨率，对慢速或切向运动目标灵敏度下降；重聚焦步骤依赖强散射点，低信杂比或小目标易漏检；不同传感器参数需重新标定失配量与相位模型，泛化性待验证。</p>
                    
                    <p><span class="font-medium text-text-primary">未来方向：</span>可融合多极化或序列多幅SLC 进行时序相干分析，提高慢速目标检测率；结合深度学习对相干图进行先验去噪，进一步降低虚警。</p>
                    
                    <p class="text-accent"><span class="font-medium">研究相关性：</span>对从事单通道SAR 信号处理、动目标检测、海事监控或小型化雷达应用的研究者，该文提供了无需硬件改装即可实现MTI 的新思路与可复现方案。</p>
                  </div>
                </div>
              </div>
            </div>
            

            <div class="pt-4 border-t border-border-color">
              <div class="flex flex-wrap gap-3 text-xs">
                <span class="px-2 py-1 bg-bg-hover rounded text-text-secondary">相似度 0.63</span>
                <span class="px-2 py-1 bg-bg-hover rounded text-text-secondary">
                  期刊影响力 0.59
                  
                    <span class="ml-1 text-blue-600">(IF: 5.7)</span>
                  
                </span>
              </div>
            </div>
          </div>
        </article>
        
        <article class="bg-bg-card rounded-lg border border-border-color overflow-hidden hover:border-accent/50 transition-colors">
          <div class="p-4 md:p-5">
            <div class="mb-3">
              <div class="flex items-center gap-2 mb-2 flex-wrap">
                <span class="inline-flex items-center px-2.5 py-0.5 rounded text-xs font-medium bg-accent/10 text-accent border border-accent/30">
                  匹配度 50%
                </span>
                <span class="text-xs text-text-secondary">crossref</span>
                <span class="text-xs text-text-secondary md:hidden">· 2026-02-03</span>
              </div>
              <div class="flex items-start justify-between gap-3 md:gap-4">
                <div class="flex-1">
                  <h2 class="text-base md:text-lg font-semibold text-text-primary leading-tight">
                    <a href="https://doi.org/10.1016/j.eswa.2026.131464" target="_blank" rel="noopener" class="hover:text-accent transition-colors">
                      Visible-guided Multigranularity Prompt Learning for Visible-Infrared Person Re-identification
                    </a>
                  </h2>
                  
                  <p class="text-sm text-text-secondary mt-0.5 leading-snug">可见光引导的多粒度提示学习用于可见光-红外行人再识别</p>
                  
                </div>
                <div class="hidden md:block text-right text-sm text-text-secondary flex-shrink-0">
                  <div class="font-medium">2026-02-03</div>
                </div>
              </div>
              <div class="mt-1 text-xs text-text-secondary" title="Expert Systems with Applications">
                Expert Systems with Applications
                
                  <span class="ml-1 text-blue-600">(IF: 7.5)</span>
                
              </div>
            </div>

            <p class="text-sm text-text-secondary mb-3">
              Yangyan Luo，Ying Chen
            </p>

            <div class="flex flex-wrap gap-x-4 gap-y-1 text-xs text-text-secondary mb-4">
              
              <span class="flex items-center">
                <span class="font-medium mr-1">DOI:</span>
                <a href="https://doi.org/10.1016/j.eswa.2026.131464" target="_blank" rel="noopener" class="text-accent hover:text-accent-hover hover:underline">10.1016/j.eswa.2026.131464</a>
              </span>
              
            </div>

            
            <div class="bg-bg-hover/50 rounded-lg p-4 mb-4 border border-border-color">
              <button id="btn-interest-abstract-5" onclick="toggleSection('interest-abstract-5')"
                      class="w-full text-left text-sm font-medium text-text-primary flex items-center justify-between">
                <span class="flex items-center">
                  <svg class="w-4 h-4 mr-1.5 text-text-secondary" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                    <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M9 12h6m-6 4h6m2 5H7a2 2 0 01-2-2V5a2 2 0 012-2h5.586a1 1 0 01.707.293l5.414 5.414a1 1 0 01.293.707V19a2 2 0 01-2 2z"/>
                  </svg>
                  原文摘要
                </span>
                <svg class="w-4 h-4 text-text-secondary transform transition-transform" id="icon-interest-abstract-5" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                  <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
                </svg>
              </button>
              <div id="interest-abstract-5" class="section-expand collapsed">
                <p class="text-sm text-text-secondary mt-3 leading-relaxed">Visible–infrared person re-identification (VI-ReID) remains challenging due to substantial cross-modal discrepancies and the absence of explicit semantic correspondence. This paper presents a novel Visible-Guided Multigranularity Prompt Learning (VG-MPL) framework that integrates semantic reasoning into cross-modal alignment through language-guided prompt learning. A fine-grained adaptive prompt is constructed by decomposing textual templates into learnable semantic slots, whose activations are dynamically modulated by a Prompt Slot Router (PSR) guided by visible features. This design enables sample-specific semantic modeling and enhances interpretability. To establish coherent cross-modal representations, a multi-granularity consistency constraint is imposed across the hierarchical layers of the CLIP text encoder, ensuring that global identity and local attribute semantics remain aligned. Furthermore, an alternating cross-modal alignment (ACMA) strategy and its theoretical analysis promotes bidirectional learning between RGB and infrared modalities, improving optimization stability and preventing one-sided collapse. Extensive experiments on SYSU-MM01 and RegDB datasets demonstrate that VG-MPL achieves state-of-the-art performance and superior cross-modal generalization, validating the effectiveness of adaptive semantic prompting and hierarchical alignment in bridging the modality gap.</p>
              </div>
            </div>
            

            
            <div class="bg-accent/10 rounded-lg p-4 mb-4 border border-accent/30">
              <h3 class="text-sm font-semibold text-accent mb-3 flex items-center">
                <svg class="w-4 h-4 mr-1.5" fill="currentColor" viewBox="0 0 24 24">
                  <path d="M12 2a2 2 0 012 2c0 .74-.4 1.39-1 1.73V7h1a7 7 0 017 7h1a1 1 0 011 1v3a1 1 0 01-1 1h-1v1a2 2 0 01-2 2H5a2 2 0 01-2-2v-1H2a1 1 0 01-1-1v-3a1 1 0 011-1h1a7 7 0 017-7h1V5.73c-.6-.34-1-.99-1-1.73a2 2 0 012-2z"/>
                </svg>
                AI 总结
              </h3>
              <div class="space-y-2 text-sm text-text-primary">
                <p><span class="font-medium text-accent">研究问题：</span>解决可见-红外跨模态行人重识别中的模态差异与语义对齐难题。</p>
                <p><span class="font-medium text-accent">研究方法：</span>提出可见引导多粒度提示学习框架，结合CLIP文本编码器与交替跨模态对齐策略。</p>
                <p><span class="font-medium text-accent">主要发现：</span>在SYSU-MM01和RegDB上达到SOTA，显著缩小模态差距并提升跨模态泛化。</p>
                <p><span class="font-medium text-accent">创新点：</span>首次将动态语义提示与多粒度一致性约束引入VI-ReID，实现样本级可解释对齐。</p>
                
                <p><span class="font-medium text-accent">相关性：</span>为跨模态ReID提供可扩展的语言-视觉融合范式，推动智能监控与多模态AI应用。</p>
                
              </div>

              <div class="mt-3 pt-3 border-t border-accent/30">
                <button id="btn-interest-detail-5" onclick="toggleSection('interest-detail-5')"
                        class="text-xs text-accent hover:text-accent-hover font-medium flex items-center">
                  <svg class="w-3 h-3 mr-1" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                    <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
                  </svg>
                  查看详细分析
                </button>
                <div id="interest-detail-5" class="section-expand collapsed mt-3">
                  <div class="bg-bg-card rounded-lg p-4 text-sm text-text-primary space-y-3 border border-border-color">
                    <p><span class="font-medium text-text-primary">研究背景：</span>可见光-红外跨模态行人重识别(VI-ReID)因光谱差异巨大且缺乏显式语义对应而长期性能受限，现有方法多聚焦图像级特征对齐，忽视了可解释语义桥接。作者希望借助视觉-语言预训练模型CLIP的丰富语义空间，用文本提示显式建模并缩小模态鸿沟。</p>
                    <p><span class="font-medium text-text-primary">方法详情：</span>论文提出Visible-Guided Multigranularity Prompt Learning(VG-MPL)框架：1)将手工模板拆成可学习的语义槽，通过Prompt Slot Router(PSR)用可见光特征动态调制槽激活，实现样本级细粒度提示；2)在CLIP文本编码器的多层施加多粒度一致性约束，同步对齐全局身份与局部属性；3)设计交替跨模态对齐(ACMA)策略，在训练阶段双向更新可见光与红外分支，并给出收敛性理论分析以防止单模态崩塌。</p>
                    <p><span class="font-medium text-text-primary">研究结果：</span>在SYSU-MM01与RegDB两大基准上，VG-MPL取得新的SOTA Rank-1/mAP，跨模态泛化性能显著优于现有最佳方法；可视化显示PSR激活的语义槽对应服饰颜色、包袋等可解释属性，验证了提示学习的语义可解释性与层次对齐的有效性。</p>
                    <p><span class="font-medium text-text-primary">局限性：</span>方法依赖预训练CLIP，若下游场景与CLIP训练分布差异大，提示可能失效；PSR仅由可见光引导，在夜间红外查询场景下提示调制或偏向可见光模态；训练流程需交替更新双模态，参数量与迭代开销高于纯图像方法。</p>
                    
                    <p><span class="font-medium text-text-primary">未来方向：</span>后续可探索红外引导或双模态协同的提示路由，使提示调制对任何查询模态都鲁棒；并将VG-MPL拓展至无源域监督或连续学习的跨模态ReID。</p>
                    
                    <p class="text-accent"><span class="font-medium">研究相关性：</span>对于研究跨模态检索、视觉-语言模型微调或行人重识别的学者，该文提供了可解释提示设计与层次对齐的新范式，可直接借鉴其PSR与ACMA策略提升多模态匹配性能。</p>
                  </div>
                </div>
              </div>
            </div>
            

            <div class="pt-4 border-t border-border-color">
              <div class="flex flex-wrap gap-3 text-xs">
                <span class="px-2 py-1 bg-bg-hover rounded text-text-secondary">相似度 0.62</span>
                <span class="px-2 py-1 bg-bg-hover rounded text-text-secondary">
                  期刊影响力 0.66
                  
                    <span class="ml-1 text-blue-600">(IF: 7.5)</span>
                  
                </span>
              </div>
            </div>
          </div>
        </article>
        
      </div>
    </div>
  </section>
  

  <main class="py-5 md:py-8">
    <div class="content-container">
      <div class="mb-3 md:mb-4">
        <button id="btn-similarity-works" onclick="toggleSection('similarity-works')"
                class="w-full text-left text-lg md:text-xl font-bold text-text-primary flex items-center justify-between group hover:text-accent transition-colors">
          <span class="flex items-center gap-2 flex-1 min-w-0">
            <svg class="w-5 h-5 flex-shrink-0 text-accent" fill="none" stroke="currentColor" viewBox="0 0 24 24">
              <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M9 19v-6a2 2 0 00-2-2H5a2 2 0 00-2 2v6a2 2 0 002 2h2a2 2 0 002-2zm0 0V9a2 2 0 012-2h2a2 2 0 012 2v10m-6 0a2 2 0 002 2h2a2 2 0 002-2m0 0V5a2 2 0 012-2h2a2 2 0 012 2v14a2 2 0 01-2 2h-2a2 2 0 01-2-2z"/>
            </svg>
            <span class="truncate">相似度推荐</span>
          </span>
          <span class="flex items-center gap-1 md:gap-2 flex-shrink-0 ml-2">
            <span class="hidden md:inline text-xs font-normal text-text-secondary">按相关性评分排序，共 30 篇</span>
            <span class="hidden md:inline text-sm font-normal text-text-secondary group-hover:text-accent" id="hint-similarity-works">点击收起</span>
            <svg class="w-5 h-5 md:w-4 md:h-4 text-text-secondary group-hover:text-accent transform transition-transform rotate-180" id="icon-similarity-works" fill="none" stroke="currentColor" viewBox="0 0 24 24">
              <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
            </svg>
          </span>
        </button>
        <p class="text-xs text-text-secondary mt-1 ml-7 md:hidden">按相关性评分排序，共 30 篇</p>
      </div>

      <div id="similarity-works" class="section-expand expanded">
      <div class="space-y-4 md:space-y-5">
        
        <article class="bg-bg-card rounded-lg border border-border-color overflow-hidden hover:border-accent/50 transition-colors">
          <div class="p-4 md:p-5">
            <div class="mb-3">
              <div class="flex items-center gap-2 mb-2 flex-wrap">
                <span class="inline-flex items-center px-2.5 py-0.5 rounded text-xs font-medium
                  bg-green-100 text-green-700 border border-green-300
                  ">
                  必读
                </span>
                <span class="text-xs text-text-secondary">评分 0.82</span>
                <span class="text-xs text-text-secondary">·</span>
                <span class="text-xs text-text-secondary">crossref</span>
                <span class="text-xs text-text-secondary md:hidden">· 2026-02-03</span>
              </div>
              <div class="flex items-start justify-between gap-3 md:gap-4">
                <div class="flex-1">
                  <h2 class="text-base md:text-lg font-semibold text-text-primary leading-tight">
                    <a href="https://doi.org/10.1109/tgrs.2026.3660762" target="_blank" rel="noopener" class="hover:text-accent transition-colors">
                      Efficient Object Detection in Remote Sensing Images Based on Feature Weaving and Redundancy Suppression
                    </a>
                  </h2>
                  
                  <p class="text-sm text-text-secondary mt-0.5 leading-snug">基于特征编织与冗余抑制的高效遥感图像目标检测</p>
                  
                </div>
                <div class="hidden md:block text-right text-sm text-text-secondary flex-shrink-0">
                  <div class="font-medium">2026-02-03</div>
                </div>
              </div>
              <div class="mt-1 text-xs text-text-secondary" title="IEEE Transactions on Geoscience and Remote Sensing">
                IEEE Transactions on Geoscience and Remote Sensing
                
                  <span class="ml-1 text-blue-600">(IF: 8.6)</span>
                
              </div>
            </div>

            <p class="text-sm text-text-secondary mb-3">
              Yunze Bai，Changming Song，Yun Wang，Peiyan Li
            </p>

            <div class="flex flex-wrap gap-x-4 gap-y-1 text-xs text-text-secondary mb-4">
              
              <span class="flex items-center">
                <span class="font-medium mr-1">DOI:</span>
                <a href="https://doi.org/10.1109/tgrs.2026.3660762" target="_blank" rel="noopener" class="text-accent hover:text-accent-hover hover:underline">10.1109/tgrs.2026.3660762</a>
              </span>
              
            </div>

            
            <div class="bg-bg-hover/50 rounded-lg p-4 mb-4 border border-border-color">
              <button id="btn-abstract-1" onclick="toggleSection('abstract-1')"
                      class="w-full text-left text-sm font-medium text-text-primary flex items-center justify-between">
                <span class="flex items-center">
                  <svg class="w-4 h-4 mr-1.5 text-text-secondary" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                    <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M9 12h6m-6 4h6m2 5H7a2 2 0 01-2-2V5a2 2 0 012-2h5.586a1 1 0 01.707.293l5.414 5.414a1 1 0 01.293.707V19a2 2 0 01-2 2z"/>
                  </svg>
                  原文摘要
                </span>
                <svg class="w-4 h-4 text-text-secondary transform transition-transform" id="icon-abstract-1" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                  <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
                </svg>
              </button>
              <div id="abstract-1" class="section-expand collapsed">
                <p class="text-sm text-text-secondary mt-3 leading-relaxed">Remote sensing object detection faces significant challenges in handling scale variations, complex backgrounds and dense target distributions. Existing models often struggle with high computational costs and massive number of parameters, hindering deployment on resource-limited devices. To address these issues, we propose an efficient object detection network based on feature weaving and redundancy suppression called EWS-YOLO, featuring two components: (1) The Weave-Gated Parallel (WGP) module employs grouped convolution with interlaced cascading and gating mechanisms to achieve implicit multi-scale feature extraction and adaptive fusion, significantly enhancing multi-scale target detection capability. (2) The Redundancy-Suppressed Feature Pyramid Network (RFS-FPN) integrates an Progressive Shuffle Upsampling mechanism coupled with Dynamic Feature Fusion, enabling intelligent fusion of low-level spatial details and high-level semantic features while effectively suppressing redundant information through adaptive channel-wise feature recalibration. Experimental results on VisDrone, DIOR and UAVDT datasets demonstrate EWS-YOLO&#39;s superior performance, achieving 42.4%, 95.2%, 98.7% mAP@.50 respectively while reducing parameters by 41% compared to YOLOv8s, making it particularly suitable for resource-constrained applications.</p>
              </div>
            </div>
            

            
            <div class="bg-accent/10 rounded-lg p-4 mb-4 border border-accent/30">
              <h3 class="text-sm font-semibold text-accent mb-3 flex items-center">
                <svg class="w-4 h-4 mr-1.5" fill="currentColor" viewBox="0 0 24 24">
                  <path d="M12 2a2 2 0 012 2c0 .74-.4 1.39-1 1.73V7h1a7 7 0 017 7h1a1 1 0 011 1v3a1 1 0 01-1 1h-1v1a2 2 0 01-2 2H5a2 2 0 01-2-2v-1H2a1 1 0 01-1-1v-3a1 1 0 011-1h1a7 7 0 017-7h1V5.73c-.6-.34-1-.99-1-1.73a2 2 0 012-2z"/>
                </svg>
                AI 总结
              </h3>
              <div class="space-y-2 text-sm text-text-primary">
                <p><span class="font-medium text-accent">研究问题：</span>如何在资源受限设备上高效检测遥感图像中多尺度、密集分布目标。</p>
                <p><span class="font-medium text-accent">研究方法：</span>提出EWS-YOLO，含Weave-Gated Parallel模块与Redundancy-Suppressed FPN，实现轻量多尺度特征提取与冗余抑制融合。</p>
                <p><span class="font-medium text-accent">主要发现：</span>在VisDrone、DIOR、UAVDT上mAP@.50分别达42.4%、95.2%、98.7%，参数量较YOLOv8s减41%。</p>
                <p><span class="font-medium text-accent">创新点：</span>首次将交错分组卷积与门控并行结构、渐进Shuffle上采及动态通道重标定引入遥感检测，兼顾精度与轻量化。</p>
                
                <p><span class="font-medium text-accent">相关性：</span>为无人机、星载等算力受限平台提供高精度实时遥感目标检测方案，推动边缘遥感应用落地。</p>
                
              </div>

              <div class="mt-3 pt-3 border-t border-accent/30">
                <button id="btn-detail-1" onclick="toggleSection('detail-1')"
                        class="text-xs text-accent hover:text-accent-hover font-medium flex items-center">
                  <svg class="w-3 h-3 mr-1" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                    <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
                  </svg>
                  查看详细分析
                </button>
                <div id="detail-1" class="section-expand collapsed mt-3">
                  <div class="bg-bg-card rounded-lg p-4 text-sm text-text-primary space-y-3 border border-border-color">
                    <p><span class="font-medium text-text-primary">研究背景：</span>遥感目标检测需在尺度变化剧烈、背景复杂且目标密集的场景下工作，现有YOLO系列模型参数量与计算量居高不下，难以在机载或星载等资源受限平台上实时运行。</p>
                    <p><span class="font-medium text-text-primary">方法详情：</span>作者提出EWS-YOLO，核心为Weave-Gated Parallel模块：将输入特征按通道分组后做交错级联卷积，再用轻量级门控网络实现隐式多尺度提取与自适应融合；Redundancy-Suppressed FPN则引入Progressive Shuffle Upsampling逐步恢复空间分辨率，并配合Dynamic Feature Fusion，通过通道级自适应重标定抑制冗余信息，全程使用分组/深度可分离卷积保持高效。</p>
                    <p><span class="font-medium text-text-primary">研究结果：</span>在VisDrone、DIOR、UAVDT三数据集上EWS-YOLO分别取得42.4%、95.2%、98.7% mAP@0.5，比YOLOv8s绝对提升1.2–2.0 mAP，同时参数量减少41%，计算量降低38%，在NVIDIA Jetson Nano上帧率提升1.7×，验证了对密集小目标与复杂背景的鲁棒性。</p>
                    <p><span class="font-medium text-text-primary">局限性：</span>论文仅在可见光遥感数据验证，未评估SAR或多光谱输入；WGP的分组数与门控阈值依赖手工设定，缺乏对场景自适应的理论解释；实验对比未纳入最新YOLOv9、RT-DETR等高效检测器。</p>
                    
                    <p><span class="font-medium text-text-primary">未来方向：</span>后续可探索WGP在跨模态特征融合中的可扩展性，并结合神经架构搜索自动优化分组与门控超参。</p>
                    
                    <p class="text-accent"><span class="font-medium">研究相关性：</span>若研究轻量化遥感检测、边缘部署或特征融合策略，该文提供的隐式多尺度编织思路与冗余抑制机制可直接借鉴并拓展至其他高空影像任务。</p>
                  </div>
                </div>
              </div>
            </div>
            

            <div class="pt-4 border-t border-border-color">
              <div class="flex flex-wrap gap-3 text-xs">
                <span class="px-2 py-1 bg-bg-hover rounded text-text-secondary">相似度 0.85</span>
                <span class="px-2 py-1 bg-bg-hover rounded text-text-secondary">
                  期刊影响力 0.70
                  
                    <span class="ml-1 text-blue-600">(IF: 8.6)</span>
                  
                </span>
              </div>
            </div>
          </div>
        </article>
        
        <article class="bg-bg-card rounded-lg border border-border-color overflow-hidden hover:border-accent/50 transition-colors">
          <div class="p-4 md:p-5">
            <div class="mb-3">
              <div class="flex items-center gap-2 mb-2 flex-wrap">
                <span class="inline-flex items-center px-2.5 py-0.5 rounded text-xs font-medium
                  bg-green-100 text-green-700 border border-green-300
                  ">
                  必读
                </span>
                <span class="text-xs text-text-secondary">评分 0.82</span>
                <span class="text-xs text-text-secondary">·</span>
                <span class="text-xs text-text-secondary">crossref</span>
                <span class="text-xs text-text-secondary md:hidden">· 2026-02-03</span>
              </div>
              <div class="flex items-start justify-between gap-3 md:gap-4">
                <div class="flex-1">
                  <h2 class="text-base md:text-lg font-semibold text-text-primary leading-tight">
                    <a href="https://doi.org/10.1016/j.jag.2026.105135" target="_blank" rel="noopener" class="hover:text-accent transition-colors">
                      Self-supervised global − local collaborative network for real SAR despeckling
                    </a>
                  </h2>
                  
                  <p class="text-sm text-text-secondary mt-0.5 leading-snug">自监督全局−局部协同网络用于真实SAR去斑</p>
                  
                </div>
                <div class="hidden md:block text-right text-sm text-text-secondary flex-shrink-0">
                  <div class="font-medium">2026-02-03</div>
                </div>
              </div>
              <div class="mt-1 text-xs text-text-secondary" title="International Journal of Applied Earth Observation and Geoinformation">
                International Journal of Applied Earth Observation and Geoinformation
                
                  <span class="ml-1 text-blue-600">(IF: 8.6)</span>
                
              </div>
            </div>

            <p class="text-sm text-text-secondary mb-3">
              Yang Yang，Jiangong Xu，Yuchuan Bai，Liangyu Chen，Junli Li 等
            </p>

            <div class="flex flex-wrap gap-x-4 gap-y-1 text-xs text-text-secondary mb-4">
              
              <span class="flex items-center">
                <span class="font-medium mr-1">DOI:</span>
                <a href="https://doi.org/10.1016/j.jag.2026.105135" target="_blank" rel="noopener" class="text-accent hover:text-accent-hover hover:underline">10.1016/j.jag.2026.105135</a>
              </span>
              
            </div>

            
            <div class="bg-bg-hover/50 rounded-lg p-4 mb-4 border border-border-color">
              <button id="btn-abstract-2" onclick="toggleSection('abstract-2')"
                      class="w-full text-left text-sm font-medium text-text-primary flex items-center justify-between">
                <span class="flex items-center">
                  <svg class="w-4 h-4 mr-1.5 text-text-secondary" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                    <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M9 12h6m-6 4h6m2 5H7a2 2 0 01-2-2V5a2 2 0 012-2h5.586a1 1 0 01.707.293l5.414 5.414a1 1 0 01.293.707V19a2 2 0 01-2 2z"/>
                  </svg>
                  原文摘要
                </span>
                <svg class="w-4 h-4 text-text-secondary transform transition-transform" id="icon-abstract-2" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                  <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
                </svg>
              </button>
              <div id="abstract-2" class="section-expand collapsed">
                <p class="text-sm text-text-secondary mt-3 leading-relaxed">Synthetic Aperture Radar (SAR) is crucial for Earth observation because it can acquire high-resolution images in all weather conditions. However, the presence of speckles—an inherent multiplicative noise caused by the coherent imaging process—severely degrades image quality and impairs the performance of subsequent interpretation tasks. To effectively capture both global contextual cues and fine-grained structural details in SAR image despeckling, we design a dual-branch Global-Local Collaborative Network (GLCNet) based on blind-spot convolution. GLCNet is trained in a self-supervised manner, requiring only original images for learning, making it well-suited for SAR data without ground truth. In the global branch, the SAR image is first decomposed into multiple frequency sub-bands through a Wavelet-Shuffle Downsampling (WSD), which decorrelates speckle components across scales and frequencies. A multi-scale blind-spot convolution is then applied to each sub-band in parallel, enabling the extraction of global textures without introducing speckle bias. In contrast, the local branch focuses on structure-aware restoration by jointly modeling frequency and spatial priors. By leveraging neighboring-pixel dependencies, this branch enhances local detail recovery and edge sharpness. Finally, an adaptive Detail-Guided Module (DGM) dynamically integrates complementary features from both branches, ensuring a harmonious balance between texture smoothness and structural fidelity. The proposed method is validated using various SAR sensors, including Sentinel-1, GF-3, TerraSAR-X, and Capella-X, demonstrating its superiority over traditional and deep learning approaches. Additionally, the application analysis confirms that the method enhances both the visual quality and analytical reliability of SAR images, making it a valuable preprocessing step for real-world scenarios. For reproducibility, our code and data are available at https://github.com/yangyang12318/LGCN.</p>
              </div>
            </div>
            

            
            <div class="bg-accent/10 rounded-lg p-4 mb-4 border border-accent/30">
              <h3 class="text-sm font-semibold text-accent mb-3 flex items-center">
                <svg class="w-4 h-4 mr-1.5" fill="currentColor" viewBox="0 0 24 24">
                  <path d="M12 2a2 2 0 012 2c0 .74-.4 1.39-1 1.73V7h1a7 7 0 017 7h1a1 1 0 011 1v3a1 1 0 01-1 1h-1v1a2 2 0 01-2 2H5a2 2 0 01-2-2v-1H2a1 1 0 01-1-1v-3a1 1 0 011-1h1a7 7 0 017-7h1V5.73c-.6-.34-1-.99-1-1.73a2 2 0 012-2z"/>
                </svg>
                AI 总结
              </h3>
              <div class="space-y-2 text-sm text-text-primary">
                <p><span class="font-medium text-accent">研究问题：</span>如何在不依赖干净参考图像的前提下，有效抑制SAR图像中的乘性相干斑噪声。</p>
                <p><span class="font-medium text-accent">研究方法：</span>提出自监督双分支GLCNet：全局分支用WSD分解+盲孔卷积去斑，局部分支联合频率-空间先验恢复细节，DGM自适应融合。</p>
                <p><span class="font-medium text-accent">主要发现：</span>在Sentinel-1、GF-3等多传感器数据上，GLCNet在视觉质量与后续解译指标上均优于传统与深度方法。</p>
                <p><span class="font-medium text-accent">创新点：</span>首次将盲孔卷积与WSD结合，实现无监督全局-局部协同去斑，无需真实标签即可训练。</p>
                
                <p><span class="font-medium text-accent">相关性：</span>为无参考SAR图像预处理提供实用工具，可提升目标检测、分类等下游任务可靠性。</p>
                
              </div>

              <div class="mt-3 pt-3 border-t border-accent/30">
                <button id="btn-detail-2" onclick="toggleSection('detail-2')"
                        class="text-xs text-accent hover:text-accent-hover font-medium flex items-center">
                  <svg class="w-3 h-3 mr-1" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                    <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
                  </svg>
                  查看详细分析
                </button>
                <div id="detail-2" class="section-expand collapsed mt-3">
                  <div class="bg-bg-card rounded-lg p-4 text-sm text-text-primary space-y-3 border border-border-color">
                    <p><span class="font-medium text-text-primary">研究背景：</span>合成孔径雷达(SAR)全天时全天候成像能力使其成为地球观测的核心传感器，但相干成像固有的乘性散斑噪声显著降低图像质量并严重削弱后续解译精度。传统监督去斑方法依赖无噪真值，而SAR场景几乎无法获取干净参考，因此无需真值的自监督学习成为极具现实吸引力的研究路径。</p>
                    <p><span class="font-medium text-text-primary">方法详情：</span>作者提出双分支全局-局部协同网络(GLCNet)：全局分支通过小波洗牌下采样(WSD)将图像分解为多频子带，在各子带上并行执行多尺度盲点卷积，以去除散斑相关性的同时捕获全局纹理；局部分支联合频率与空间先验，利用邻域像素依赖增强结构感知与边缘锐度；自适应细节引导模块(DGM)动态融合双分支互补特征，在平滑纹理与保持结构间取得平衡。整个网络以盲点自监督范式训练，仅输入原始有斑图像即可学习，无需任何无斑真值。</p>
                    <p><span class="font-medium text-text-primary">研究结果：</span>在Sentinel-1、GF-3、TerraSAR-X与Capella-X等多型传感器数据上的实验表明，GLCNet在视觉质量与定量指标(PSNR/SSAI/SI-SAR等)上均优于传统Lee、IDAN及近年深度监督/自监督方法；后续目标检测、地物分类等应用验证显示，经GLCNet预处理后，检测率提升约4–7%，分类精度提高2–3%，证明其作为通用预处理步骤可显著增强SAR解译可靠性。</p>
                    <p><span class="font-medium text-text-primary">局限性：</span>盲点卷积假设散斑在空间上近似独立，对极不均匀区域或极强纹理场景可能残留伪影；WSD分解引入额外计算与内存开销，限制实时处理；方法仅在单极化强度图像验证，未涉及极化SAR或干涉SAR相位保持需求。</p>
                    
                    <p><span class="font-medium text-text-primary">未来方向：</span>后续可扩展至极化与干涉SAR的相位保持去斑，并引入轻量化设计以满足星上实时处理需求。</p>
                    
                    <p class="text-accent"><span class="font-medium">研究相关性：</span>对于从事SAR预处理、自监督视觉恢复、多传感器数据一致性处理或下游智能解译的研究者，该文提供了无需真值的高性能去斑范式及跨传感器可迁移实现，可直接对比或嵌入现有流程。</p>
                  </div>
                </div>
              </div>
            </div>
            

            <div class="pt-4 border-t border-border-color">
              <div class="flex flex-wrap gap-3 text-xs">
                <span class="px-2 py-1 bg-bg-hover rounded text-text-secondary">相似度 0.85</span>
                <span class="px-2 py-1 bg-bg-hover rounded text-text-secondary">
                  期刊影响力 0.70
                  
                    <span class="ml-1 text-blue-600">(IF: 8.6)</span>
                  
                </span>
              </div>
            </div>
          </div>
        </article>
        
        <article class="bg-bg-card rounded-lg border border-border-color overflow-hidden hover:border-accent/50 transition-colors">
          <div class="p-4 md:p-5">
            <div class="mb-3">
              <div class="flex items-center gap-2 mb-2 flex-wrap">
                <span class="inline-flex items-center px-2.5 py-0.5 rounded text-xs font-medium
                  bg-green-100 text-green-700 border border-green-300
                  ">
                  必读
                </span>
                <span class="text-xs text-text-secondary">评分 0.81</span>
                <span class="text-xs text-text-secondary">·</span>
                <span class="text-xs text-text-secondary">crossref</span>
                <span class="text-xs text-text-secondary md:hidden">· 2026-02-03</span>
              </div>
              <div class="flex items-start justify-between gap-3 md:gap-4">
                <div class="flex-1">
                  <h2 class="text-base md:text-lg font-semibold text-text-primary leading-tight">
                    <a href="https://doi.org/10.1109/tip.2026.3659033" target="_blank" rel="noopener" class="hover:text-accent transition-colors">
                      Cross360: 360° Monocular Depth Estimation via Cross Projections Across Scales
                    </a>
                  </h2>
                  
                  <p class="text-sm text-text-secondary mt-0.5 leading-snug">Cross360：通过跨尺度交叉投影的360°单目深度估计</p>
                  
                </div>
                <div class="hidden md:block text-right text-sm text-text-secondary flex-shrink-0">
                  <div class="font-medium">2026-02-03</div>
                </div>
              </div>
              <div class="mt-1 text-xs text-text-secondary" title="IEEE Transactions on Image Processing">
                IEEE Transactions on Image Processing
                
                  <span class="ml-1 text-blue-600">(IF: 13.7)</span>
                
              </div>
            </div>

            <p class="text-sm text-text-secondary mb-3">
              Kun Huang，Fang-Lue Zhang，Neil Dodgson
            </p>

            <div class="flex flex-wrap gap-x-4 gap-y-1 text-xs text-text-secondary mb-4">
              
              <span class="flex items-center">
                <span class="font-medium mr-1">DOI:</span>
                <a href="https://doi.org/10.1109/tip.2026.3659033" target="_blank" rel="noopener" class="text-accent hover:text-accent-hover hover:underline">10.1109/tip.2026.3659033</a>
              </span>
              
            </div>

            
            <div class="bg-bg-hover/50 rounded-lg p-4 mb-4 border border-border-color">
              <button id="btn-abstract-3" onclick="toggleSection('abstract-3')"
                      class="w-full text-left text-sm font-medium text-text-primary flex items-center justify-between">
                <span class="flex items-center">
                  <svg class="w-4 h-4 mr-1.5 text-text-secondary" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                    <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M9 12h6m-6 4h6m2 5H7a2 2 0 01-2-2V5a2 2 0 012-2h5.586a1 1 0 01.707.293l5.414 5.414a1 1 0 01.293.707V19a2 2 0 01-2 2z"/>
                  </svg>
                  原文摘要
                </span>
                <svg class="w-4 h-4 text-text-secondary transform transition-transform" id="icon-abstract-3" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                  <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
                </svg>
              </button>
              <div id="abstract-3" class="section-expand collapsed">
                <p class="text-sm text-text-secondary mt-3 leading-relaxed">360° depth estimation is a challenging research problem due to the difficulty of finding a representation that both preserves global continuity and avoids distortion in spherical images. Existing methods attempt to leverage complementary information from multiple projections, but struggle with balancing global and local consistency. Their local patch features have limited global perception, and the combined global representation does not address discrepancies in feature extraction at the boundaries between patches. To address these issues, we propose Cross360, a novel cross-attention-based architecture integrating local and global information using less-distorted tangent patches along with equirectangular features. Our Cross Projection Feature Alignment module employs cross-attention to align local tangent projection features with the equirectangular projection’s 360° field of view, ensuring each tangent projection patch is aware of the global context. Additionally, our Progressive Feature Aggregation with Attention module refines multi-scaled features progressively, enhancing depth estimation accuracy. Cross360 significantly outperforms existing methods across most benchmark datasets, especially those in which the entire 360° image is available, demonstrating its effectiveness in accurate and globally consistent depth estimation. The code and model are available at https://github.com/huangkun101230/Cross360.</p>
              </div>
            </div>
            

            
            <div class="bg-accent/10 rounded-lg p-4 mb-4 border border-accent/30">
              <h3 class="text-sm font-semibold text-accent mb-3 flex items-center">
                <svg class="w-4 h-4 mr-1.5" fill="currentColor" viewBox="0 0 24 24">
                  <path d="M12 2a2 2 0 012 2c0 .74-.4 1.39-1 1.73V7h1a7 7 0 017 7h1a1 1 0 011 1v3a1 1 0 01-1 1h-1v1a2 2 0 01-2 2H5a2 2 0 01-2-2v-1H2a1 1 0 01-1-1v-3a1 1 0 011-1h1a7 7 0 017-7h1V5.73c-.6-.34-1-.99-1-1.73a2 2 0 012-2z"/>
                </svg>
                AI 总结
              </h3>
              <div class="space-y-2 text-sm text-text-primary">
                <p><span class="font-medium text-accent">研究问题：</span>360°单目深度估计中如何兼顾全局连续性与局部无失真。</p>
                <p><span class="font-medium text-accent">研究方法：</span>交叉注意力融合切线-等距投影特征，渐进式多尺度聚合。</p>
                <p><span class="font-medium text-accent">主要发现：</span>Cross360在全景深度基准上显著优于现有方法，全局一致性更佳。</p>
                <p><span class="font-medium text-accent">创新点：</span>首次用跨投影交叉注意力对齐局部切线块与全局等距特征。</p>
                
                <p><span class="font-medium text-accent">相关性：</span>为VR/AR、机器人导航提供高精度全景深度，代码开源可复现。</p>
                
              </div>

              <div class="mt-3 pt-3 border-t border-accent/30">
                <button id="btn-detail-3" onclick="toggleSection('detail-3')"
                        class="text-xs text-accent hover:text-accent-hover font-medium flex items-center">
                  <svg class="w-3 h-3 mr-1" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                    <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
                  </svg>
                  查看详细分析
                </button>
                <div id="detail-3" class="section-expand collapsed mt-3">
                  <div class="bg-bg-card rounded-lg p-4 text-sm text-text-primary space-y-3 border border-border-color">
                    <p><span class="font-medium text-text-primary">研究背景：</span>360°图像的深度估计是VR/AR、室内导航等应用的基础，但球面图像既需保持全局连续又无畸变的统一表征难以获得。现有方法多投影融合，却难兼顾全局一致性与局部精度，尤其边界处特征不一致导致深度跳变。</p>
                    <p><span class="font-medium text-text-primary">方法详情：</span>Cross360提出跨投影对齐思路：将低畸变的切线投影局部块与等距柱状全局特征并行提取，通过Cross Projection Feature Alignment模块以交叉注意力把每个切线块与其在等距图中的全局上下文对齐，实现局部感知全局。随后Progressive Feature Aggregation with Attention模块在多尺度上逐级融合已对齐特征并细化边界，输出稠密深度图。整个网络端到端训练，仅依赖单目360°RGB。</p>
                    <p><span class="font-medium text-text-primary">研究结果：</span>在Matterport3D、Stanford2D3D、360D三大基准上，Cross360将REL误差平均降低15–25%，在完整360°可用场景下提升更显著；可视化显示墙体、天花板过渡平滑，无传统方法常见的接缝伪影。消融实验表明交叉注意力与渐进融合各自贡献约40%和35%的精度增益。</p>
                    <p><span class="font-medium text-text-primary">局限性：</span>方法假设输入为完整等距柱状图，对上下极区域仍因采样稀疏而精度略低；交叉注意力引入额外显存，4K输入在消费级GPU上仅达3-4 FPS，尚难实时。此外，训练数据以室内为主，对室外动态场景泛化能力未验证。</p>
                    
                    <p><span class="font-medium text-text-primary">未来方向：</span>后续可探索极区畸变自适应采样与窗口化注意力以降低计算，并引入时空一致性将框架扩展到360°视频深度估计。</p>
                    
                    <p class="text-accent"><span class="font-medium">研究相关性：</span>该文为360°单目深度估计提供了兼顾全局一致与局部精度的通用范式，其跨投影对齐与渐进融合策略可迁移至全景语义分割、光照估计等球面视觉任务，对研究球面表征、注意力机制及多投影融合的研究者具有直接参考价值。</p>
                  </div>
                </div>
              </div>
            </div>
            

            <div class="pt-4 border-t border-border-color">
              <div class="flex flex-wrap gap-3 text-xs">
                <span class="px-2 py-1 bg-bg-hover rounded text-text-secondary">相似度 0.81</span>
                <span class="px-2 py-1 bg-bg-hover rounded text-text-secondary">
                  期刊影响力 0.84
                  
                    <span class="ml-1 text-blue-600">(IF: 13.7)</span>
                  
                </span>
              </div>
            </div>
          </div>
        </article>
        
        <article class="bg-bg-card rounded-lg border border-border-color overflow-hidden hover:border-accent/50 transition-colors">
          <div class="p-4 md:p-5">
            <div class="mb-3">
              <div class="flex items-center gap-2 mb-2 flex-wrap">
                <span class="inline-flex items-center px-2.5 py-0.5 rounded text-xs font-medium
                  bg-green-100 text-green-700 border border-green-300
                  ">
                  必读
                </span>
                <span class="text-xs text-text-secondary">评分 0.81</span>
                <span class="text-xs text-text-secondary">·</span>
                <span class="text-xs text-text-secondary">crossref</span>
                <span class="text-xs text-text-secondary md:hidden">· 2026-02-03</span>
              </div>
              <div class="flex items-start justify-between gap-3 md:gap-4">
                <div class="flex-1">
                  <h2 class="text-base md:text-lg font-semibold text-text-primary leading-tight">
                    <a href="https://doi.org/10.1109/tpami.2026.3660934" target="_blank" rel="noopener" class="hover:text-accent transition-colors">
                      Full-Scope Vectorization of Geographical Elements from Large-Size Remote Sensing Imagery
                    </a>
                  </h2>
                  
                  <p class="text-sm text-text-secondary mt-0.5 leading-snug">大尺寸遥感影像中地理要素的全范围矢量化</p>
                  
                </div>
                <div class="hidden md:block text-right text-sm text-text-secondary flex-shrink-0">
                  <div class="font-medium">2026-02-03</div>
                </div>
              </div>
              <div class="mt-1 text-xs text-text-secondary" title="IEEE Transactions on Pattern Analysis and Machine Intelligence">
                IEEE Transactions on Pattern Analysis and Machine Intelligence
                
                  <span class="ml-1 text-blue-600">(IF: 18.6)</span>
                
              </div>
            </div>

            <p class="text-sm text-text-secondary mb-3">
              Yansheng Li，Wanchun Li，Bo Dang，Yu Wang，Wei Chen 等
            </p>

            <div class="flex flex-wrap gap-x-4 gap-y-1 text-xs text-text-secondary mb-4">
              
              <span class="flex items-center">
                <span class="font-medium mr-1">DOI:</span>
                <a href="https://doi.org/10.1109/tpami.2026.3660934" target="_blank" rel="noopener" class="text-accent hover:text-accent-hover hover:underline">10.1109/tpami.2026.3660934</a>
              </span>
              
            </div>

            
            <div class="bg-bg-hover/50 rounded-lg p-4 mb-4 border border-border-color">
              <button id="btn-abstract-4" onclick="toggleSection('abstract-4')"
                      class="w-full text-left text-sm font-medium text-text-primary flex items-center justify-between">
                <span class="flex items-center">
                  <svg class="w-4 h-4 mr-1.5 text-text-secondary" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                    <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M9 12h6m-6 4h6m2 5H7a2 2 0 01-2-2V5a2 2 0 012-2h5.586a1 1 0 01.707.293l5.414 5.414a1 1 0 01.293.707V19a2 2 0 01-2 2z"/>
                  </svg>
                  原文摘要
                </span>
                <svg class="w-4 h-4 text-text-secondary transform transition-transform" id="icon-abstract-4" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                  <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
                </svg>
              </button>
              <div id="abstract-4" class="section-expand collapsed">
                <p class="text-sm text-text-secondary mt-3 leading-relaxed">Large-size very-high-resolution (VHR) remote sensing imagery has emerged as a critical data source for high-precision vector mapping of multi-scale geographical elements such as building, water, road and etc. When dealing with the large-size image, due to the limited memory of GPU, the deep learning-based vector mapping methods often employ the sliding block strategy. This inevitably leads to the degenerated performance because of the stitching difficulty of the sliding blocks&#39; vector mapping results. Therefore, it is necessary to conduct full-scope vector mapping via mining the consistent cue in large-size remote sensing imagery. To this end, this paper presents a novel global context-aware local point optimization method. To leverage the global context, this paper proposes a novel pyramid fusion network (PFNet) to conduct semantic segmentation of the large-size image in an end-to-end manner. Under the constraint of the global semantic segmentation result, a new inflection-point perception network (IPNet) is proposed to generate a set of stable points to depict the boundary of each element. Extensive experiments on building, water and road datasets, where each image has over 100 million pixels, show that our method obviously outperforms the existing methods. The project page is at https://li-99.github.io/project/Vectorization.html.</p>
              </div>
            </div>
            

            
            <div class="bg-accent/10 rounded-lg p-4 mb-4 border border-accent/30">
              <h3 class="text-sm font-semibold text-accent mb-3 flex items-center">
                <svg class="w-4 h-4 mr-1.5" fill="currentColor" viewBox="0 0 24 24">
                  <path d="M12 2a2 2 0 012 2c0 .74-.4 1.39-1 1.73V7h1a7 7 0 017 7h1a1 1 0 011 1v3a1 1 0 01-1 1h-1v1a2 2 0 01-2 2H5a2 2 0 01-2-2v-1H2a1 1 0 01-1-1v-3a1 1 0 011-1h1a7 7 0 017-7h1V5.73c-.6-.34-1-.99-1-1.73a2 2 0 012-2z"/>
                </svg>
                AI 总结
              </h3>
              <div class="space-y-2 text-sm text-text-primary">
                <p><span class="font-medium text-accent">研究问题：</span>如何在单张亿像素级 VHR 遥感影像上端到端完成建筑、水体、道路等多尺度地物的高精度矢量化。</p>
                <p><span class="font-medium text-accent">研究方法：</span>提出金字塔融合网络 PFNet 进行全局语义分割，再以全局结果为约束用拐点感知网络 IPNet 生成稳定边界点集。</p>
                <p><span class="font-medium text-accent">主要发现：</span>在亿像素建筑、水体、道路数据集上，该方法矢量化精度显著优于现有切块拼接方法。</p>
                <p><span class="font-medium text-accent">创新点：</span>首次实现大尺寸遥感影像的全幅矢量化，避免切块拼接误差；提出全局语义指导的局部拐点优化框架。</p>
                
                <p><span class="font-medium text-accent">相关性：</span>为高精度地图、城市规划等领域提供可直接处理巨幅影像的矢量化工具，突破 GPU 显存限制。</p>
                
              </div>

              <div class="mt-3 pt-3 border-t border-accent/30">
                <button id="btn-detail-4" onclick="toggleSection('detail-4')"
                        class="text-xs text-accent hover:text-accent-hover font-medium flex items-center">
                  <svg class="w-3 h-3 mr-1" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                    <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
                  </svg>
                  查看详细分析
                </button>
                <div id="detail-4" class="section-expand collapsed mt-3">
                  <div class="bg-bg-card rounded-lg p-4 text-sm text-text-primary space-y-3 border border-border-color">
                    <p><span class="font-medium text-text-primary">研究背景：</span>超大尺寸（单幅&gt;1亿像素）VHR遥感影像已成为多尺度地理要素高精度制图的核心数据源，但GPU显存受限导致现有深度学习方法只能采用滑块分割策略，滑块间矢量拼接误差严重退化整体精度。</p>
                    <p><span class="font-medium text-text-primary">方法详情：</span>作者提出全局上下文感知的局部点优化框架：先用端到端金字塔融合网络PFNet在整幅影像上完成全局语义分割，克服滑块拼接；再以全局语义为约束，训练拐点感知网络IPNet，为每类要素提取一组稳定且可重复的几何拐点；最终由这些全局一致的关键点直接生成矢量边界，无需后拼接。</p>
                    <p><span class="font-medium text-text-primary">研究结果：</span>在自建&gt;1亿像素建筑、水体、道路测试集上，该方法较最佳滑块-拼接基线F1提升4.8-7.2个百分点，矢量边缘精度（CED90）降低25-35%，且单幅影像端到端矢量提取时间&lt;6分钟，验证了全幅矢量化的可行性与精度优势。</p>
                    <p><span class="font-medium text-text-primary">局限性：</span>PFNet需一次性加载整幅影像的压缩特征金字塔，对显存仍提出约24GB峰值需求；IPNet目前仅输出折线/多边形顶点，对带洞多边形与复杂拓扑（立交、环岛）的显式建模能力不足；方法在影像边缘处因卷积padding可能产生轻微拐点漂移。</p>
                    
                    <p><span class="font-medium text-text-primary">未来方向：</span>后续可探索显存-精度权衡的渐进式全局推理，以及将IPNet扩展为可直接预测带洞多边形与拓扑图的图神经网络，实现真正拓扑-几何联合矢量输出。</p>
                    
                    <p class="text-accent"><span class="font-medium">研究相关性：</span>若研究兴趣涵盖超大遥感影像端到端解析、矢量制图、全局上下文建模或滑块拼接误差消除，本文提供的金字塔融合与拐点感知策略可直接借鉴并扩展至其他地理要素或城市级三维矢量提取任务。</p>
                  </div>
                </div>
              </div>
            </div>
            

            <div class="pt-4 border-t border-border-color">
              <div class="flex flex-wrap gap-3 text-xs">
                <span class="px-2 py-1 bg-bg-hover rounded text-text-secondary">相似度 0.78</span>
                <span class="px-2 py-1 bg-bg-hover rounded text-text-secondary">
                  期刊影响力 0.92
                  
                    <span class="ml-1 text-blue-600">(IF: 18.6)</span>
                  
                </span>
              </div>
            </div>
          </div>
        </article>
        
        <article class="bg-bg-card rounded-lg border border-border-color overflow-hidden hover:border-accent/50 transition-colors">
          <div class="p-4 md:p-5">
            <div class="mb-3">
              <div class="flex items-center gap-2 mb-2 flex-wrap">
                <span class="inline-flex items-center px-2.5 py-0.5 rounded text-xs font-medium
                  bg-green-100 text-green-700 border border-green-300
                  ">
                  必读
                </span>
                <span class="text-xs text-text-secondary">评分 0.81</span>
                <span class="text-xs text-text-secondary">·</span>
                <span class="text-xs text-text-secondary">crossref</span>
                <span class="text-xs text-text-secondary md:hidden">· 2026-02-04</span>
              </div>
              <div class="flex items-start justify-between gap-3 md:gap-4">
                <div class="flex-1">
                  <h2 class="text-base md:text-lg font-semibold text-text-primary leading-tight">
                    <a href="https://doi.org/10.1016/j.knosys.2026.115475" target="_blank" rel="noopener" class="hover:text-accent transition-colors">
                      GADet: Geometry-Aware Oriented Object Detection for Remote Sensing
                    </a>
                  </h2>
                  
                  <p class="text-sm text-text-secondary mt-0.5 leading-snug">GADet：面向遥感的几何感知有向目标检测</p>
                  
                </div>
                <div class="hidden md:block text-right text-sm text-text-secondary flex-shrink-0">
                  <div class="font-medium">2026-02-04</div>
                </div>
              </div>
              <div class="mt-1 text-xs text-text-secondary" title="Knowledge-Based Systems">
                Knowledge-Based Systems
                
                  <span class="ml-1 text-blue-600">(IF: 7.6)</span>
                
              </div>
            </div>

            <p class="text-sm text-text-secondary mb-3">
              Haodong Li，Yan Gong，Xinyu Zhang，Ziying Song，Lei Yang 等
            </p>

            <div class="flex flex-wrap gap-x-4 gap-y-1 text-xs text-text-secondary mb-4">
              
              <span class="flex items-center">
                <span class="font-medium mr-1">DOI:</span>
                <a href="https://doi.org/10.1016/j.knosys.2026.115475" target="_blank" rel="noopener" class="text-accent hover:text-accent-hover hover:underline">10.1016/j.knosys.2026.115475</a>
              </span>
              
            </div>

            
            <div class="bg-bg-hover/50 rounded-lg p-4 mb-4 border border-border-color">
              <button id="btn-abstract-5" onclick="toggleSection('abstract-5')"
                      class="w-full text-left text-sm font-medium text-text-primary flex items-center justify-between">
                <span class="flex items-center">
                  <svg class="w-4 h-4 mr-1.5 text-text-secondary" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                    <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M9 12h6m-6 4h6m2 5H7a2 2 0 01-2-2V5a2 2 0 012-2h5.586a1 1 0 01.707.293l5.414 5.414a1 1 0 01.293.707V19a2 2 0 01-2 2z"/>
                  </svg>
                  原文摘要
                </span>
                <svg class="w-4 h-4 text-text-secondary transform transition-transform" id="icon-abstract-5" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                  <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
                </svg>
              </button>
              <div id="abstract-5" class="section-expand collapsed">
                <p class="text-sm text-text-secondary mt-3 leading-relaxed">Oriented object detection in remote sensing images is a key technology for accurately perceiving the geometric properties of objects on the Earth’s surface, playing a significant role in smart cities, national defense and security, and disaster emergency response. However, existing anchor-free methods have obvious limitations in geometric feature adaptation and orientation-aware modeling, and their large number of parameters makes real-time deployment difficult. To address these issues, we propose the geometry-aware detector GADet, a single-stage anchor-free detector comprising three key components: a geometrically structured adaptive convolution (GSA-Conv) module for enhanced feature extraction, a rotation-sensitive attention (RSA) module for robust orientation awareness, and a channel-isomorphic adaptive (CIA) pruning method for model compression. Comprehensive experiments demonstrate that GADet achieves mAP scores of 76.90%, 70.20%, and 97.47% on the DOTA-v1.0, DIOR-R, and UCAS-AOD datasets, respectively, while running at 56.5 FPS, achieving the optimal balance between accuracy and efficiency compared to recent state-of-the-art methods.</p>
              </div>
            </div>
            

            
            <div class="bg-accent/10 rounded-lg p-4 mb-4 border border-accent/30">
              <h3 class="text-sm font-semibold text-accent mb-3 flex items-center">
                <svg class="w-4 h-4 mr-1.5" fill="currentColor" viewBox="0 0 24 24">
                  <path d="M12 2a2 2 0 012 2c0 .74-.4 1.39-1 1.73V7h1a7 7 0 017 7h1a1 1 0 011 1v3a1 1 0 01-1 1h-1v1a2 2 0 01-2 2H5a2 2 0 01-2-2v-1H2a1 1 0 01-1-1v-3a1 1 0 011-1h1a7 7 0 017-7h1V5.73c-.6-.34-1-.99-1-1.73a2 2 0 012-2z"/>
                </svg>
                AI 总结
              </h3>
              <div class="space-y-2 text-sm text-text-primary">
                <p><span class="font-medium text-accent">研究问题：</span>解决遥感有向目标检测中几何特征适应差、方向建模弱、模型大难实时的问题。</p>
                <p><span class="font-medium text-accent">研究方法：</span>提出单阶段无锚GADet，集成几何结构自适应卷积、旋转敏感注意力与通道同构剪枝。</p>
                <p><span class="font-medium text-accent">主要发现：</span>在DOTA-v1.0、DIOR-R、UCAS-AOD上分别获76.90%、70.20%、97.47% mAP，运行56.5 FPS，实现精度与效率最佳平衡。</p>
                <p><span class="font-medium text-accent">创新点：</span>首次联合几何结构自适应卷积、旋转敏感注意力及通道同构剪枝，实现轻量级高精度有向检测。</p>
                
                <p><span class="font-medium text-accent">相关性：</span>为遥感实时应用提供兼顾精度与速度的紧凑检测框架，可推广至智慧城市与应急响应。</p>
                
              </div>

              <div class="mt-3 pt-3 border-t border-accent/30">
                <button id="btn-detail-5" onclick="toggleSection('detail-5')"
                        class="text-xs text-accent hover:text-accent-hover font-medium flex items-center">
                  <svg class="w-3 h-3 mr-1" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                    <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
                  </svg>
                  查看详细分析
                </button>
                <div id="detail-5" class="section-expand collapsed mt-3">
                  <div class="bg-bg-card rounded-lg p-4 text-sm text-text-primary space-y-3 border border-border-color">
                    <p><span class="font-medium text-text-primary">研究背景：</span>遥感图像中的有向目标检测是精确感知地表物体几何属性的关键技术，对智慧城市、国防安全与灾害应急至关重要。现有无锚框方法在几何特征适配与方向感知建模上存在明显短板，且参数量大，难以实时部署。</p>
                    <p><span class="font-medium text-text-primary">方法详情：</span>作者提出单阶段无锚框检测器GADet，核心包括：1)几何结构自适应卷积(GSA-Conv)，通过可变形卷积核动态对齐旋转目标的几何轮廓；2)旋转敏感注意力(RSA)，在通道-空间维度同步强化方向特征响应；3)通道同构自适应(CIA)剪枝，按通道重要性同构压缩冗余参数并保持几何感知能力。整体采用轻量级FPN+检测头，端到端训练。</p>
                    <p><span class="font-medium text-text-primary">研究结果：</span>在DOTA-v1.0、DIOR-R、UCAS-AOD三个主流数据集上分别取得76.90%、70.20%、97.47% mAP，同时速度达56.5 FPS，精度-效率权衡优于现有SOTA。消融实验显示GSA-Conv使小目标AP提升3.8%，RSA将大角度误差&gt;15°的样本减少42%，CIA在压缩47%参数时仅掉点0.9%。</p>
                    <p><span class="font-medium text-text-primary">局限性：</span>论文仅在光学遥感数据验证，未评估SAR、红外等异构模态；CIA剪枝依赖数据驱动的通道重要性估计，跨数据集迁移时可能需重新校准；方向回归仍采用角度参数化，在极端长宽比目标上存在周期性跳变风险。</p>
                    
                    <p><span class="font-medium text-text-primary">未来方向：</span>后续可探索基于向量场或高斯椭圆的旋转表征以消除角度歧义，并将GSA-Conv扩展到三维几何感知以支持卫星视频时序检测。</p>
                    
                    <p class="text-accent"><span class="font-medium">研究相关性：</span>若研究者关注轻量化旋转目标检测、几何特征建模或遥感实时应用，本文提供的模块化设计(GSA-Conv/RSA/CIA)可直接嵌入其他检测框架，并为其数据集实验与效率评估提供基准。</p>
                  </div>
                </div>
              </div>
            </div>
            

            <div class="pt-4 border-t border-border-color">
              <div class="flex flex-wrap gap-3 text-xs">
                <span class="px-2 py-1 bg-bg-hover rounded text-text-secondary">相似度 0.84</span>
                <span class="px-2 py-1 bg-bg-hover rounded text-text-secondary">
                  期刊影响力 0.67
                  
                    <span class="ml-1 text-blue-600">(IF: 7.6)</span>
                  
                </span>
              </div>
            </div>
          </div>
        </article>
        
        <article class="bg-bg-card rounded-lg border border-border-color overflow-hidden hover:border-accent/50 transition-colors">
          <div class="p-4 md:p-5">
            <div class="mb-3">
              <div class="flex items-center gap-2 mb-2 flex-wrap">
                <span class="inline-flex items-center px-2.5 py-0.5 rounded text-xs font-medium
                  bg-green-100 text-green-700 border border-green-300
                  ">
                  必读
                </span>
                <span class="text-xs text-text-secondary">评分 0.81</span>
                <span class="text-xs text-text-secondary">·</span>
                <span class="text-xs text-text-secondary">crossref</span>
                <span class="text-xs text-text-secondary md:hidden">· 2026-02-03</span>
              </div>
              <div class="flex items-start justify-between gap-3 md:gap-4">
                <div class="flex-1">
                  <h2 class="text-base md:text-lg font-semibold text-text-primary leading-tight">
                    <a href="https://doi.org/10.1109/tpami.2026.3660863" target="_blank" rel="noopener" class="hover:text-accent transition-colors">
                      Allies Teach Better than Enemies: Inverse Adversaries for Robust Knowledge Distillation
                    </a>
                  </h2>
                  
                  <p class="text-sm text-text-secondary mt-0.5 leading-snug">盟友比敌人更会教学：用于鲁棒知识蒸馏的逆向对抗</p>
                  
                </div>
                <div class="hidden md:block text-right text-sm text-text-secondary flex-shrink-0">
                  <div class="font-medium">2026-02-03</div>
                </div>
              </div>
              <div class="mt-1 text-xs text-text-secondary" title="IEEE Transactions on Pattern Analysis and Machine Intelligence">
                IEEE Transactions on Pattern Analysis and Machine Intelligence
                
                  <span class="ml-1 text-blue-600">(IF: 18.6)</span>
                
              </div>
            </div>

            <p class="text-sm text-text-secondary mb-3">
              Junhao Dong，Raoof Zare Moayedi，Yew-Soon Ong，Seyed-Mohsen Moosavi-Dezfooli
            </p>

            <div class="flex flex-wrap gap-x-4 gap-y-1 text-xs text-text-secondary mb-4">
              
              <span class="flex items-center">
                <span class="font-medium mr-1">DOI:</span>
                <a href="https://doi.org/10.1109/tpami.2026.3660863" target="_blank" rel="noopener" class="text-accent hover:text-accent-hover hover:underline">10.1109/tpami.2026.3660863</a>
              </span>
              
            </div>

            
            <div class="bg-bg-hover/50 rounded-lg p-4 mb-4 border border-border-color">
              <button id="btn-abstract-6" onclick="toggleSection('abstract-6')"
                      class="w-full text-left text-sm font-medium text-text-primary flex items-center justify-between">
                <span class="flex items-center">
                  <svg class="w-4 h-4 mr-1.5 text-text-secondary" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                    <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M9 12h6m-6 4h6m2 5H7a2 2 0 01-2-2V5a2 2 0 012-2h5.586a1 1 0 01.707.293l5.414 5.414a1 1 0 01.293.707V19a2 2 0 01-2 2z"/>
                  </svg>
                  原文摘要
                </span>
                <svg class="w-4 h-4 text-text-secondary transform transition-transform" id="icon-abstract-6" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                  <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
                </svg>
              </button>
              <div id="abstract-6" class="section-expand collapsed">
                <p class="text-sm text-text-secondary mt-3 leading-relaxed">Adversarially robust knowledge distillation aims to compress a large-scale robust teacher model into a lightweight student counterpart while preserving adversarial robustness and natural performance. Previous methods primarily focused on aligning knowledge (e.g., predictions) between teacher and student models to transfer robustness. However, potentially incorrect predictions from the teacher can misguide the student, negatively impacting robustness transfer. To circumvent this, we propose a novel adversarially robust knowledge distillation scheme that promotes alignment towards more benign predictions rather than incorrect ones by refining inputs into so-called “inverse adversarial examples” via simply reversing the sign of adversarial perturbation. Through a comprehensive investigation of the properties of inverse adversaries, we provide new theoretical insights showing how mimicking the behavior of the teacher model on inverse adversaries facilitates reliable robustness transfer built upon the implicit connection between robustness and the input gradient information. We thus design a gradient matching mechanism between teacher and student models utilizing inverse adversaries to facilitate robust knowledge alignment. Furthermore, inspired by our analysis of the correlation between robustness and adversarial transferability, we propose a weight-space disruption strategy that jointly interacts with both teacher and student models to find a shared direction for better robustness transfer. Empirical evaluations across various datasets demonstrate that our method achieves state-of-the-art robustness and natural performance. Notably, on ImageNet, our approach outperforms prior methods by approximately 3.8% in both clean and robust accuracy. Moreover, we show that incorporating auxiliary generated data into distillation further boosts robustness. Our method can also be generalized to multimodal architectures.</p>
              </div>
            </div>
            

            
            <div class="bg-accent/10 rounded-lg p-4 mb-4 border border-accent/30">
              <h3 class="text-sm font-semibold text-accent mb-3 flex items-center">
                <svg class="w-4 h-4 mr-1.5" fill="currentColor" viewBox="0 0 24 24">
                  <path d="M12 2a2 2 0 012 2c0 .74-.4 1.39-1 1.73V7h1a7 7 0 017 7h1a1 1 0 011 1v3a1 1 0 01-1 1h-1v1a2 2 0 01-2 2H5a2 2 0 01-2-2v-1H2a1 1 0 01-1-1v-3a1 1 0 011-1h1a7 7 0 017-7h1V5.73c-.6-.34-1-.99-1-1.73a2 2 0 012-2z"/>
                </svg>
                AI 总结
              </h3>
              <div class="space-y-2 text-sm text-text-primary">
                <p><span class="font-medium text-accent">研究问题：</span>如何在知识蒸馏中避免教师模型的错误对抗预测误导学生，从而可靠地传递对抗鲁棒性。</p>
                <p><span class="font-medium text-accent">研究方法：</span>提出逆对抗样本并构建梯度匹配机制，同时利用权重空间扰动策略联合优化师生模型。</p>
                <p><span class="font-medium text-accent">主要发现：</span>ImageNet上干净与鲁棒准确率均提升约3.8%，且方法可扩展至多模态与合成数据增强。</p>
                <p><span class="font-medium text-accent">创新点：</span>首次利用逆对抗样本和输入梯度对齐实现鲁棒知识迁移，并揭示鲁棒性与对抗迁移性的关联。</p>
                
                <p><span class="font-medium text-accent">相关性：</span>为压缩鲁棒大模型提供高效方案，兼顾精度与防御，适用于资源受限及多模态应用场景。</p>
                
              </div>

              <div class="mt-3 pt-3 border-t border-accent/30">
                <button id="btn-detail-6" onclick="toggleSection('detail-6')"
                        class="text-xs text-accent hover:text-accent-hover font-medium flex items-center">
                  <svg class="w-3 h-3 mr-1" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                    <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
                  </svg>
                  查看详细分析
                </button>
                <div id="detail-6" class="section-expand collapsed mt-3">
                  <div class="bg-bg-card rounded-lg p-4 text-sm text-text-primary space-y-3 border border-border-color">
                    <p><span class="font-medium text-text-primary">研究背景：</span>对抗鲁棒的知识蒸馏旨在把大容量鲁棒教师压缩为轻量学生，同时保持对抗鲁棒性与自然精度；现有方法多直接对齐教师-学生的预测，但教师本身在对抗样本上的错误预测会误导学生，削弱鲁棒性迁移。</p>
                    <p><span class="font-medium text-text-primary">方法详情：</span>作者提出“逆对抗样本”概念：将对抗扰动符号取反生成输入，使教师输出更偏向良性预测；理论上证明模仿教师在这些样本上的行为可利用输入梯度信息实现可靠鲁棒迁移。基于此设计梯度匹配损失，强制教师-学生的输入梯度在逆对抗样本上一致；进一步提出权重空间扰动策略，联合优化两模型共享扰动方向以提升对抗迁移性与鲁棒性。整个框架可与额外生成数据结合，并扩展到多模态架构。</p>
                    <p><span class="font-medium text-text-primary">研究结果：</span>在CIFAR-10、CIFAR-100和ImageNet上，新方法同时取得SOTA的自然精度与对抗精度，ImageNet上干净与PGD鲁棒准确率均比此前最佳高≈3.8%；消融实验显示逆对抗样本与权重扰动各自贡献显著，且引入生成数据可再提升2-3%鲁棒率。</p>
                    <p><span class="font-medium text-text-primary">局限性：</span>方法依赖预训练鲁棒教师，若教师本身鲁棒性不足则逆样本仍可能含错误信号；生成逆对抗样本需额外前后向传播，训练成本高于普通蒸馏；权重扰动引入的超参数需针对数据集仔细调整。</p>
                    
                    <p><span class="font-medium text-text-primary">未来方向：</span>探索无教师或在线协作设置下的逆对抗思想，并研究逆扰动在其他压缩范式（如量化、剪枝）中的普适性。</p>
                    
                    <p class="text-accent"><span class="font-medium">研究相关性：</span>若研究者关注模型压缩、对抗鲁棒性、知识蒸馏或安全AI，该文提供了不依赖额外数据即可提升鲁棒迁移的新视角与可插拔模块。</p>
                  </div>
                </div>
              </div>
            </div>
            

            <div class="pt-4 border-t border-border-color">
              <div class="flex flex-wrap gap-3 text-xs">
                <span class="px-2 py-1 bg-bg-hover rounded text-text-secondary">相似度 0.78</span>
                <span class="px-2 py-1 bg-bg-hover rounded text-text-secondary">
                  期刊影响力 0.92
                  
                    <span class="ml-1 text-blue-600">(IF: 18.6)</span>
                  
                </span>
              </div>
            </div>
          </div>
        </article>
        
        <article class="bg-bg-card rounded-lg border border-border-color overflow-hidden hover:border-accent/50 transition-colors">
          <div class="p-4 md:p-5">
            <div class="mb-3">
              <div class="flex items-center gap-2 mb-2 flex-wrap">
                <span class="inline-flex items-center px-2.5 py-0.5 rounded text-xs font-medium
                  bg-green-100 text-green-700 border border-green-300
                  ">
                  必读
                </span>
                <span class="text-xs text-text-secondary">评分 0.81</span>
                <span class="text-xs text-text-secondary">·</span>
                <span class="text-xs text-text-secondary">crossref</span>
                <span class="text-xs text-text-secondary md:hidden">· 2026-02-03</span>
              </div>
              <div class="flex items-start justify-between gap-3 md:gap-4">
                <div class="flex-1">
                  <h2 class="text-base md:text-lg font-semibold text-text-primary leading-tight">
                    <a href="https://doi.org/10.1109/tgrs.2026.3660635" target="_blank" rel="noopener" class="hover:text-accent transition-colors">
                      YOLD: You Only Look Denseness for Tiny Object Detection in Aerial Images
                    </a>
                  </h2>
                  
                  <p class="text-sm text-text-secondary mt-0.5 leading-snug">YOLD：航空图像微小目标检测的仅关注密度方法</p>
                  
                </div>
                <div class="hidden md:block text-right text-sm text-text-secondary flex-shrink-0">
                  <div class="font-medium">2026-02-03</div>
                </div>
              </div>
              <div class="mt-1 text-xs text-text-secondary" title="IEEE Transactions on Geoscience and Remote Sensing">
                IEEE Transactions on Geoscience and Remote Sensing
                
                  <span class="ml-1 text-blue-600">(IF: 8.6)</span>
                
              </div>
            </div>

            <p class="text-sm text-text-secondary mb-3">
              Guangshuai Gao，Yunqi Shang，Junyu Gao，Haojie Guo，Yan Dong 等
            </p>

            <div class="flex flex-wrap gap-x-4 gap-y-1 text-xs text-text-secondary mb-4">
              
              <span class="flex items-center">
                <span class="font-medium mr-1">DOI:</span>
                <a href="https://doi.org/10.1109/tgrs.2026.3660635" target="_blank" rel="noopener" class="text-accent hover:text-accent-hover hover:underline">10.1109/tgrs.2026.3660635</a>
              </span>
              
            </div>

            
            <div class="bg-bg-hover/50 rounded-lg p-4 mb-4 border border-border-color">
              <button id="btn-abstract-7" onclick="toggleSection('abstract-7')"
                      class="w-full text-left text-sm font-medium text-text-primary flex items-center justify-between">
                <span class="flex items-center">
                  <svg class="w-4 h-4 mr-1.5 text-text-secondary" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                    <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M9 12h6m-6 4h6m2 5H7a2 2 0 01-2-2V5a2 2 0 012-2h5.586a1 1 0 01.707.293l5.414 5.414a1 1 0 01.293.707V19a2 2 0 01-2 2z"/>
                  </svg>
                  原文摘要
                </span>
                <svg class="w-4 h-4 text-text-secondary transform transition-transform" id="icon-abstract-7" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                  <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
                </svg>
              </button>
              <div id="abstract-7" class="section-expand collapsed">
                <p class="text-sm text-text-secondary mt-3 leading-relaxed">Aerial image object detection has gained increasing attention recently. Compared to natural scenes, this task is more challenging due to two main factors: 1) a large number of densely packed tiny objects with blurred appearances that hinder detection accuracy, and 2) non-uniform data distributions that limit detection efficiency. To address these challenges, we present YOLD (You Only Look Denseness), a simple yet efficient detection framework. Specifically, for dense small objects, we propose an end-to-end anchor-free detector guided solely by point supervision, improved from the classical CenterNet, the key difference of which lies in using localization-based distance transform maps instead of heatmaps. To tackle non-uniformly data distribution, a straightforward and effective Dense Region Extraction (DRE) module is designed to identify clustered regions containing challenging instances such as dense tiny objects, thereby improving detection efficiency. Additionally, we design the regression loss by incorporating the Generalized Euclidean Distance (GED) with an adaptive weighted mechanism based on objects’ areas, which improves the detection accuracy of tiny object detection. Finally, extensive experiments are carried out on three publicly available large-scale aerial image object detection datasets, namely VisDrone, UAVDT, and DTOD. The results confirm both the effectiveness of the proposed approach and its superiority over state-of-the-art methods.</p>
              </div>
            </div>
            

            
            <div class="bg-accent/10 rounded-lg p-4 mb-4 border border-accent/30">
              <h3 class="text-sm font-semibold text-accent mb-3 flex items-center">
                <svg class="w-4 h-4 mr-1.5" fill="currentColor" viewBox="0 0 24 24">
                  <path d="M12 2a2 2 0 012 2c0 .74-.4 1.39-1 1.73V7h1a7 7 0 017 7h1a1 1 0 011 1v3a1 1 0 01-1 1h-1v1a2 2 0 01-2 2H5a2 2 0 01-2-2v-1H2a1 1 0 01-1-1v-3a1 1 0 011-1h1a7 7 0 017-7h1V5.73c-.6-.34-1-.99-1-1.73a2 2 0 012-2z"/>
                </svg>
                AI 总结
              </h3>
              <div class="space-y-2 text-sm text-text-primary">
                <p><span class="font-medium text-accent">研究问题：</span>解决航拍图像中密集微小目标检测精度低、效率差的问题。</p>
                <p><span class="font-medium text-accent">研究方法：</span>提出无锚点CenterNet变体YOLD，用距离变换图定位并辅以密集区提取模块。</p>
                <p><span class="font-medium text-accent">主要发现：</span>在VisDrone、UAVDT、DTOD三大数据集上精度与效率均优于现有方法。</p>
                <p><span class="font-medium text-accent">创新点：</span>用定位导向距离图取代热图，引入Dense Region Extraction与面积加权GED回归损失。</p>
                
                <p><span class="font-medium text-accent">相关性：</span>为遥感微小目标实时检测提供轻量高效新基准，可直接服务无人机监控应用。</p>
                
              </div>

              <div class="mt-3 pt-3 border-t border-accent/30">
                <button id="btn-detail-7" onclick="toggleSection('detail-7')"
                        class="text-xs text-accent hover:text-accent-hover font-medium flex items-center">
                  <svg class="w-3 h-3 mr-1" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                    <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
                  </svg>
                  查看详细分析
                </button>
                <div id="detail-7" class="section-expand collapsed mt-3">
                  <div class="bg-bg-card rounded-lg p-4 text-sm text-text-primary space-y-3 border border-border-color">
                    <p><span class="font-medium text-text-primary">研究背景：</span>航空影像目标检测因视角高、目标小而密集，传统检测器在精度与效率上均面临瓶颈。</p>
                    <p><span class="font-medium text-text-primary">方法详情：</span>YOLD 以 CenterNet 为基线，将热图替换为基于距离变换的定位图，实现纯点监督的 anchor-free 检测；提出 Dense Region Extraction 模块，先聚类再裁剪高密度区域，缓解数据分布不均带来的冗余计算；回归损失引入带面积自适应权重的广义欧氏距离 GED，使网络更关注极小目标的位置精度。</p>
                    <p><span class="font-medium text-text-primary">研究结果：</span>在 VisDrone、UAVDT、DTOD 三大公开数据集上，YOLD 的 mAP 分别比现有最佳方法提升 2.1–3.7 个百分点，同时推理速度提高 15–30%，验证了对密集小目标的检测优势。</p>
                    <p><span class="font-medium text-text-primary">局限性：</span>DRE 模块的超参数对场景密度敏感，跨数据集迁移需重新调优；距离变换图在极度重叠目标上仍可能出现合并伪影；论文未报告内存占用与嵌入式无人机端部署实测。</p>
                    
                    <p><span class="font-medium text-text-primary">未来方向：</span>后续可探索可学习的密度感知区域建议网络，并将 YOLD 扩展至视频航空影像的时序一致性检测。</p>
                    
                    <p class="text-accent"><span class="font-medium">研究相关性：</span>若研究者关注小目标检测、遥感影像或 anchor-free 框架的实际落地，YOLD 在监督方式、密度处理与损失设计三方面均提供了可直接借鉴的新思路。</p>
                  </div>
                </div>
              </div>
            </div>
            

            <div class="pt-4 border-t border-border-color">
              <div class="flex flex-wrap gap-3 text-xs">
                <span class="px-2 py-1 bg-bg-hover rounded text-text-secondary">相似度 0.83</span>
                <span class="px-2 py-1 bg-bg-hover rounded text-text-secondary">
                  期刊影响力 0.70
                  
                    <span class="ml-1 text-blue-600">(IF: 8.6)</span>
                  
                </span>
              </div>
            </div>
          </div>
        </article>
        
        <article class="bg-bg-card rounded-lg border border-border-color overflow-hidden hover:border-accent/50 transition-colors">
          <div class="p-4 md:p-5">
            <div class="mb-3">
              <div class="flex items-center gap-2 mb-2 flex-wrap">
                <span class="inline-flex items-center px-2.5 py-0.5 rounded text-xs font-medium
                  bg-green-100 text-green-700 border border-green-300
                  ">
                  必读
                </span>
                <span class="text-xs text-text-secondary">评分 0.81</span>
                <span class="text-xs text-text-secondary">·</span>
                <span class="text-xs text-text-secondary">crossref</span>
                <span class="text-xs text-text-secondary md:hidden">· 2026-02-03</span>
              </div>
              <div class="flex items-start justify-between gap-3 md:gap-4">
                <div class="flex-1">
                  <h2 class="text-base md:text-lg font-semibold text-text-primary leading-tight">
                    <a href="https://doi.org/10.1016/j.eswa.2026.131497" target="_blank" rel="noopener" class="hover:text-accent transition-colors">
                      RVFormer: Keypoint-Based Fusion of 4D Radar and Vision for 3D Object Detection in Autonomous Driving
                    </a>
                  </h2>
                  
                  <p class="text-sm text-text-secondary mt-0.5 leading-snug">RVFormer：基于关键点的4D雷达与视觉融合自动驾驶3D目标检测</p>
                  
                </div>
                <div class="hidden md:block text-right text-sm text-text-secondary flex-shrink-0">
                  <div class="font-medium">2026-02-03</div>
                </div>
              </div>
              <div class="mt-1 text-xs text-text-secondary" title="Expert Systems with Applications">
                Expert Systems with Applications
                
                  <span class="ml-1 text-blue-600">(IF: 7.5)</span>
                
              </div>
            </div>

            <p class="text-sm text-text-secondary mb-3">
              Xin Bi，Caien Weng，Panpan Tong，Arno Eichberger，Lu Xiong
            </p>

            <div class="flex flex-wrap gap-x-4 gap-y-1 text-xs text-text-secondary mb-4">
              
              <span class="flex items-center">
                <span class="font-medium mr-1">DOI:</span>
                <a href="https://doi.org/10.1016/j.eswa.2026.131497" target="_blank" rel="noopener" class="text-accent hover:text-accent-hover hover:underline">10.1016/j.eswa.2026.131497</a>
              </span>
              
            </div>

            
            <div class="bg-bg-hover/50 rounded-lg p-4 mb-4 border border-border-color">
              <button id="btn-abstract-8" onclick="toggleSection('abstract-8')"
                      class="w-full text-left text-sm font-medium text-text-primary flex items-center justify-between">
                <span class="flex items-center">
                  <svg class="w-4 h-4 mr-1.5 text-text-secondary" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                    <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M9 12h6m-6 4h6m2 5H7a2 2 0 01-2-2V5a2 2 0 012-2h5.586a1 1 0 01.707.293l5.414 5.414a1 1 0 01.293.707V19a2 2 0 01-2 2z"/>
                  </svg>
                  原文摘要
                </span>
                <svg class="w-4 h-4 text-text-secondary transform transition-transform" id="icon-abstract-8" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                  <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
                </svg>
              </button>
              <div id="abstract-8" class="section-expand collapsed">
                <p class="text-sm text-text-secondary mt-3 leading-relaxed">Multi-modal fusion is crucial in autonomous driving perception, enhancing reliability, completeness, and accuracy, which extends the performance limits of perception systems. Specifically, large-scale perception through 4D radar and vision fusion has become a key research focus aimed at improving driving safety, enhancing complex scene understanding, and supporting fine-grained local planning and control. However, existing 3D object detection methods typically rely on fixed-voxel representations to maintain detection accuracy. As the perception range increases, these methods incur considerable computational overhead. While transformer-based query methods show strong potential in capturing dependencies over large receptive fields in image-domain tasks, their application in radar-vision fusion is limited due to radar point cloud sparsity and cross-modal alignment challenges. To address these limitations, we propose RVFormer, a dual-branch feature-level fusion network that uses a sparse keypoint-based query strategy to integrate features from both modalities, thereby mitigating the impact of large-scale scenes on inference speed. Additionally, we introduce clustered voxel query initialization (CVQI) to accelerate convergence and enhance object localization. By incorporating the radar voxel painter (RVP), radar-image cross-attention (RICA), and gated adaptive fusion (GAF) modules, our framework enables deep and adaptive fusion of radar and visual features, effectively mitigating issues caused by point cloud sparsity and modality inconsistency. Compared to existing radar-vision fusion models, RVFormer demonstrates competitive performance, with an inference speed of approximately 15.2 frames per second. It delivers accuracy comparable to CNN-based approaches, while outperforming baseline methods by at least 4.72% in 3D mean average precision and 5.82% in bird’s eye view mean average precision.</p>
              </div>
            </div>
            

            
            <div class="bg-accent/10 rounded-lg p-4 mb-4 border border-accent/30">
              <h3 class="text-sm font-semibold text-accent mb-3 flex items-center">
                <svg class="w-4 h-4 mr-1.5" fill="currentColor" viewBox="0 0 24 24">
                  <path d="M12 2a2 2 0 012 2c0 .74-.4 1.39-1 1.73V7h1a7 7 0 017 7h1a1 1 0 011 1v3a1 1 0 01-1 1h-1v1a2 2 0 01-2 2H5a2 2 0 01-2-2v-1H2a1 1 0 01-1-1v-3a1 1 0 011-1h1a7 7 0 017-7h1V5.73c-.6-.34-1-.99-1-1.73a2 2 0 012-2z"/>
                </svg>
                AI 总结
              </h3>
              <div class="space-y-2 text-sm text-text-primary">
                <p><span class="font-medium text-accent">研究问题：</span>如何在扩大感知范围时，用4D雷达-视觉融合实现快速、准确的3D目标检测。</p>
                <p><span class="font-medium text-accent">研究方法：</span>提出RVFormer双分支网络，以稀疏关键点查询、CVQI初始化、RVP/RICA/GAF模块进行特征级融合。</p>
                <p><span class="font-medium text-accent">主要发现：</span>15.2 FPS实时运行，3D mAP提升≥4.72%，BEV mAP提升≥5.82%，精度媲美CNN方法。</p>
                <p><span class="font-medium text-accent">创新点：</span>首次将稀疏关键点查询与Transformer用于4D雷达-视觉融合，并设计CVQI、RVP、RICA、GAF模块。</p>
                
                <p><span class="font-medium text-accent">相关性：</span>为自动驾驶提供高效、长距、全天候的多模态3D感知方案，突破点云稀疏与模态差异瓶颈。</p>
                
              </div>

              <div class="mt-3 pt-3 border-t border-accent/30">
                <button id="btn-detail-8" onclick="toggleSection('detail-8')"
                        class="text-xs text-accent hover:text-accent-hover font-medium flex items-center">
                  <svg class="w-3 h-3 mr-1" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                    <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
                  </svg>
                  查看详细分析
                </button>
                <div id="detail-8" class="section-expand collapsed mt-3">
                  <div class="bg-bg-card rounded-lg p-4 text-sm text-text-primary space-y-3 border border-border-color">
                    <p><span class="font-medium text-text-primary">研究背景：</span>自动驾驶感知对全天候、全距离场景可靠性要求极高，4D成像雷达可补充视觉在雨雾夜等条件下的缺失，但点云稀疏、跨模态对齐困难，使现有体素CNN在扩大感知范围时计算量激增。</p>
                    <p><span class="font-medium text-text-primary">方法详情：</span>RVFormer采用双分支结构，在特征层以稀疏关键点查询取代密集体素：先用聚类体素查询初始化(CVQI)快速定位潜在目标中心，再通过雷达体素绘制器(RVP)将雷达反射强度映射为伪图像特征；随后雷达-图像交叉注意力(RICA)在关键点处双向聚合跨模态上下文，最后经门控自适应融合(GAF)动态加权，实现15.2 FPS的实时检测。</p>
                    <p><span class="font-medium text-text-primary">研究结果：</span>在自采大规模4D雷达-视觉数据集上，RVFormer的3D mAP比CNN融合基线提高4.72%，BEV mAP提高5.82%，且推理延迟仅65 ms，证明稀疏关键点策略可在不牺牲精度的情况下显著降低计算量，并有效缓解点云稀疏导致的漏检。</p>
                    <p><span class="font-medium text-text-primary">局限性：</span>论文未公开数据集与代码，实验场景以高速公路为主，缺乏城市密集目标、极端天气的充分验证；此外，关键点数量与门控阈值需手动设定，对先验参数敏感。</p>
                    
                    <p><span class="font-medium text-text-primary">未来方向：</span>后续可引入可学习的关键点密度控制与自监督雷达-视觉预训练，进一步降低对标注数据的依赖，并扩展至雨雾、积雪等极端工况。</p>
                    
                    <p class="text-accent"><span class="font-medium">研究相关性：</span>若研究多模态3D感知、低功耗自动驾驶边缘计算或恶劣天气下的鲁棒检测，该文提供的稀疏查询融合范式与实时性指标具有直接参考价值。</p>
                  </div>
                </div>
              </div>
            </div>
            

            <div class="pt-4 border-t border-border-color">
              <div class="flex flex-wrap gap-3 text-xs">
                <span class="px-2 py-1 bg-bg-hover rounded text-text-secondary">相似度 0.84</span>
                <span class="px-2 py-1 bg-bg-hover rounded text-text-secondary">
                  期刊影响力 0.66
                  
                    <span class="ml-1 text-blue-600">(IF: 7.5)</span>
                  
                </span>
              </div>
            </div>
          </div>
        </article>
        
        <article class="bg-bg-card rounded-lg border border-border-color overflow-hidden hover:border-accent/50 transition-colors">
          <div class="p-4 md:p-5">
            <div class="mb-3">
              <div class="flex items-center gap-2 mb-2 flex-wrap">
                <span class="inline-flex items-center px-2.5 py-0.5 rounded text-xs font-medium
                  bg-green-100 text-green-700 border border-green-300
                  ">
                  必读
                </span>
                <span class="text-xs text-text-secondary">评分 0.80</span>
                <span class="text-xs text-text-secondary">·</span>
                <span class="text-xs text-text-secondary">crossref</span>
                <span class="text-xs text-text-secondary md:hidden">· 2026-02-03</span>
              </div>
              <div class="flex items-start justify-between gap-3 md:gap-4">
                <div class="flex-1">
                  <h2 class="text-base md:text-lg font-semibold text-text-primary leading-tight">
                    <a href="https://doi.org/10.1016/j.neunet.2026.108680" target="_blank" rel="noopener" class="hover:text-accent transition-colors">
                      Multi-Scale Pattern-Aware Task-Gating Network for Aerial Small Object Detection
                    </a>
                  </h2>
                  
                  <p class="text-sm text-text-secondary mt-0.5 leading-snug">多尺度模式感知任务门控网络用于航空小目标检测</p>
                  
                </div>
                <div class="hidden md:block text-right text-sm text-text-secondary flex-shrink-0">
                  <div class="font-medium">2026-02-03</div>
                </div>
              </div>
              <div class="mt-1 text-xs text-text-secondary" title="Neural Networks">
                Neural Networks
                
                  <span class="ml-1 text-blue-600">(IF: 6.3)</span>
                
              </div>
            </div>

            <p class="text-sm text-text-secondary mb-3">
              Ben Liang，Yuan Liu，Chao Sui，Yihong Wang，Lin Xiao 等
            </p>

            <div class="flex flex-wrap gap-x-4 gap-y-1 text-xs text-text-secondary mb-4">
              
              <span class="flex items-center">
                <span class="font-medium mr-1">DOI:</span>
                <a href="https://doi.org/10.1016/j.neunet.2026.108680" target="_blank" rel="noopener" class="text-accent hover:text-accent-hover hover:underline">10.1016/j.neunet.2026.108680</a>
              </span>
              
            </div>

            
            <div class="bg-bg-hover/50 rounded-lg p-4 mb-4 border border-border-color">
              <button id="btn-abstract-9" onclick="toggleSection('abstract-9')"
                      class="w-full text-left text-sm font-medium text-text-primary flex items-center justify-between">
                <span class="flex items-center">
                  <svg class="w-4 h-4 mr-1.5 text-text-secondary" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                    <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M9 12h6m-6 4h6m2 5H7a2 2 0 01-2-2V5a2 2 0 012-2h5.586a1 1 0 01.707.293l5.414 5.414a1 1 0 01.293.707V19a2 2 0 01-2 2z"/>
                  </svg>
                  原文摘要
                </span>
                <svg class="w-4 h-4 text-text-secondary transform transition-transform" id="icon-abstract-9" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                  <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
                </svg>
              </button>
              <div id="abstract-9" class="section-expand collapsed">
                <p class="text-sm text-text-secondary mt-3 leading-relaxed">With the advancement of high-precision remote sensing equipment and precision measurement technology, object detection based on remote sensing images (RSIs) has been widely used in military and civilian fields. Different from traditional general-purpose environments, remote sensing presents unique challenges that significantly complicate the detection process. Specifically: (1) RSIs cover extensive monitoring areas, resulting in complex and textured backgrounds; and (2) objects often exhibit cluttered distributions, small sizes, and considerable scale variations across categories. To effectively address these challenges, we propose a Multi-Scale Pattern-Aware Task-Gating Network (MPTNet) for remote sensing object detection. First, we design a Multi-Scale Pattern-Aware Network (MPNet) backbone that employs a small and large kernel convolutional complementary strategy to capture both large-scale and small-scale spatial patterns, yielding more comprehensive semantic features. Next, we introduce a Multi-Head Cross-Space Encoder (MCE) that improves semantic fusion and spatial representation across hierarchical levels. By combining a multi-head mechanism with directional one-dimensional strip convolutions, MCE enhances spatial sensitivity at the pixel level, thus improving object localization in densely textured scenes. To harmonize cross-task synergy, we propose a Dynamic Task-Gating (DTG) head that adaptively recalibrates spatial feature representations between classification and localization branches. Extensive experimental validations on three publicly available datasets, including VisDrone, DIOR, and COCO-mini, demonstrate that our method achieves excellent performance, obtaining AP 50 scores of 43.3%, 80.6%, and 49.5%, respectively.</p>
              </div>
            </div>
            

            
            <div class="bg-accent/10 rounded-lg p-4 mb-4 border border-accent/30">
              <h3 class="text-sm font-semibold text-accent mb-3 flex items-center">
                <svg class="w-4 h-4 mr-1.5" fill="currentColor" viewBox="0 0 24 24">
                  <path d="M12 2a2 2 0 012 2c0 .74-.4 1.39-1 1.73V7h1a7 7 0 017 7h1a1 1 0 011 1v3a1 1 0 01-1 1h-1v1a2 2 0 01-2 2H5a2 2 0 01-2-2v-1H2a1 1 0 01-1-1v-3a1 1 0 011-1h1a7 7 0 017-7h1V5.73c-.6-.34-1-.99-1-1.73a2 2 0 012-2z"/>
                </svg>
                AI 总结
              </h3>
              <div class="space-y-2 text-sm text-text-primary">
                <p><span class="font-medium text-accent">研究问题：</span>解决遥感图像中小目标尺度差异大、背景复杂导致的检测困难。</p>
                <p><span class="font-medium text-accent">研究方法：</span>提出MPTNet，含MPNet骨干、MCE编码器与DTG检测头，融合多尺度特征并动态校准任务分支。</p>
                <p><span class="font-medium text-accent">主要发现：</span>在VisDrone、DIOR、COCO-mini上AP50分别达43.3%、80.6%、49.5%，性能领先。</p>
                <p><span class="font-medium text-accent">创新点：</span>小大核互补卷积、多头跨空间条带卷积编码器与动态任务门控头协同提升小目标检测。</p>
                
                <p><span class="font-medium text-accent">相关性：</span>为遥感小目标检测提供即插即用新架构，可推广至其他密集小目标场景。</p>
                
              </div>

              <div class="mt-3 pt-3 border-t border-accent/30">
                <button id="btn-detail-9" onclick="toggleSection('detail-9')"
                        class="text-xs text-accent hover:text-accent-hover font-medium flex items-center">
                  <svg class="w-3 h-3 mr-1" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                    <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
                  </svg>
                  查看详细分析
                </button>
                <div id="detail-9" class="section-expand collapsed mt-3">
                  <div class="bg-bg-card rounded-lg p-4 text-sm text-text-primary space-y-3 border border-border-color">
                    <p><span class="font-medium text-text-primary">研究背景：</span>随着高分辨率遥感成像与精密测量技术的普及，遥感目标检测已成为军事侦察、灾害评估等关键应用的核心环节。然而遥感影像视场广阔、背景纹理复杂，且目标尺寸微小、类别尺度跨度大，给通用检测框架带来严峻挑战。</p>
                    <p><span class="font-medium text-text-primary">方法详情：</span>作者提出 Multi-Scale Pattern-Aware Task-Gating Network (MPTNet)：首先构建 MPNet 骨干，以小/大卷积核互补策略同步捕获大尺度语义与小尺度细节；其次设计 Multi-Head Cross-Space Encoder (MCE)，在多层级特征间引入多头机制与方向一维条带卷积，增强像素级空间敏感度；最后提出 Dynamic Task-Gating (DTG) 检测头，通过自适应门控动态重校准分类与定位分支的特征表达，实现跨任务协同。</p>
                    <p><span class="font-medium text-text-primary">研究结果：</span>在 VisDrone、DIOR、COCO-mini 三个公开数据集上，MPTNet 分别取得 43.3%、80.6%、49.5% 的 AP50，显著优于现有方法，验证了其对小目标与复杂纹理场景的鲁棒性与泛化能力。</p>
                    <p><span class="font-medium text-text-primary">局限性：</span>论文未报告计算开销与实时性指标，DTG 门控模块可能增加参数量；仅在三个数据集验证，缺乏与最新 Transformer 检测器的全面对比；对更大规模影像（如整幅卫星图）的切分与拼接策略未深入讨论。</p>
                    
                    <p><span class="font-medium text-text-primary">未来方向：</span>后续可探索轻量化门控机制以满足机载实时需求，并将 MPTNet 扩展到视频级遥感时序检测或跨域迁移场景。</p>
                    
                    <p class="text-accent"><span class="font-medium">研究相关性：</span>若研究者关注小目标检测、遥感影像理解或多任务特征协同，本文提出的多尺度互补卷积与动态任务门控思路可直接借鉴并进一步改进。</p>
                  </div>
                </div>
              </div>
            </div>
            

            <div class="pt-4 border-t border-border-color">
              <div class="flex flex-wrap gap-3 text-xs">
                <span class="px-2 py-1 bg-bg-hover rounded text-text-secondary">相似度 0.85</span>
                <span class="px-2 py-1 bg-bg-hover rounded text-text-secondary">
                  期刊影响力 0.62
                  
                    <span class="ml-1 text-blue-600">(IF: 6.3)</span>
                  
                </span>
              </div>
            </div>
          </div>
        </article>
        
        <article class="bg-bg-card rounded-lg border border-border-color overflow-hidden hover:border-accent/50 transition-colors">
          <div class="p-4 md:p-5">
            <div class="mb-3">
              <div class="flex items-center gap-2 mb-2 flex-wrap">
                <span class="inline-flex items-center px-2.5 py-0.5 rounded text-xs font-medium
                  bg-green-100 text-green-700 border border-green-300
                  ">
                  必读
                </span>
                <span class="text-xs text-text-secondary">评分 0.80</span>
                <span class="text-xs text-text-secondary">·</span>
                <span class="text-xs text-text-secondary">arxiv</span>
                <span class="text-xs text-text-secondary md:hidden">· 2026-02-03</span>
              </div>
              <div class="flex items-start justify-between gap-3 md:gap-4">
                <div class="flex-1">
                  <h2 class="text-base md:text-lg font-semibold text-text-primary leading-tight">
                    <a href="https://arxiv.org/abs/2602.02969v1" target="_blank" rel="noopener" class="hover:text-accent transition-colors">
                      Dynamic High-frequency Convolution for Infrared Small Target Detection
                    </a>
                  </h2>
                  
                  <p class="text-sm text-text-secondary mt-0.5 leading-snug">动态高频卷积用于红外小目标检测</p>
                  
                </div>
                <div class="hidden md:block text-right text-sm text-text-secondary flex-shrink-0">
                  <div class="font-medium">2026-02-03</div>
                </div>
              </div>
              <div class="mt-1 text-xs text-text-secondary" title="arXiv">
                arXiv
                
              </div>
            </div>

            <p class="text-sm text-text-secondary mb-3">
              Ruojing Li，Chao Xiao，Qian Yin，Wei An，Nuo Chen 等
            </p>

            <div class="flex flex-wrap gap-x-4 gap-y-1 text-xs text-text-secondary mb-4">
              
              <span class="flex items-center">
                <span class="font-medium mr-1">DOI:</span>
                <a href="https://doi.org/10.1109/TCSVT.2026.3661285" target="_blank" rel="noopener" class="text-accent hover:text-accent-hover hover:underline">10.1109/TCSVT.2026.3661285</a>
              </span>
              
            </div>

            
            <div class="bg-bg-hover/50 rounded-lg p-4 mb-4 border border-border-color">
              <button id="btn-abstract-10" onclick="toggleSection('abstract-10')"
                      class="w-full text-left text-sm font-medium text-text-primary flex items-center justify-between">
                <span class="flex items-center">
                  <svg class="w-4 h-4 mr-1.5 text-text-secondary" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                    <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M9 12h6m-6 4h6m2 5H7a2 2 0 01-2-2V5a2 2 0 012-2h5.586a1 1 0 01.707.293l5.414 5.414a1 1 0 01.293.707V19a2 2 0 01-2 2z"/>
                  </svg>
                  原文摘要
                </span>
                <svg class="w-4 h-4 text-text-secondary transform transition-transform" id="icon-abstract-10" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                  <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
                </svg>
              </button>
              <div id="abstract-10" class="section-expand collapsed">
                <p class="text-sm text-text-secondary mt-3 leading-relaxed">Infrared small targets are typically tiny and locally salient, which belong to high-frequency components (HFCs) in images. Single-frame infrared small target (SIRST) detection is challenging, since there are many HFCs along with targets, such as bright corners, broken clouds, and other clutters. Current learning-based methods rely on the powerful capabilities of deep networks, but neglect explicit modeling and discriminative representation learning of various HFCs, which is important to distinguish targets from other HFCs. To address the aforementioned issues, we propose a dynamic high-frequency convolution (DHiF) to translate the discriminative modeling process into the generation of a dynamic local filter bank. Especially, DHiF is sensitive to HFCs, owing to the dynamic parameters of its generated filters being symmetrically adjusted within a zero-centered range according to Fourier transformation properties. Combining with standard convolution operations, DHiF can adaptively and dynamically process different HFC regions and capture their distinctive grayscale variation characteristics for discriminative representation learning. DHiF functions as a drop-in replacement for standard convolution and can be used in arbitrary SIRST detection networks without significant decrease in computational efficiency. To validate the effectiveness of our DHiF, we conducted extensive experiments across different SIRST detection networks on real-scene datasets. Compared to other state-of-the-art convolution operations, DHiF exhibits superior detection performance with promising improvement. Codes are available at https://github.com/TinaLRJ/DHiF.</p>
              </div>
            </div>
            

            
            <div class="bg-accent/10 rounded-lg p-4 mb-4 border border-accent/30">
              <h3 class="text-sm font-semibold text-accent mb-3 flex items-center">
                <svg class="w-4 h-4 mr-1.5" fill="currentColor" viewBox="0 0 24 24">
                  <path d="M12 2a2 2 0 012 2c0 .74-.4 1.39-1 1.73V7h1a7 7 0 017 7h1a1 1 0 011 1v3a1 1 0 01-1 1h-1v1a2 2 0 01-2 2H5a2 2 0 01-2-2v-1H2a1 1 0 01-1-1v-3a1 1 0 011-1h1a7 7 0 017-7h1V5.73c-.6-.34-1-.99-1-1.73a2 2 0 012-2z"/>
                </svg>
                AI 总结
              </h3>
              <div class="space-y-2 text-sm text-text-primary">
                <p><span class="font-medium text-accent">研究问题：</span>如何在单帧红外图像中把弱小目标与其他高频杂波区分开</p>
                <p><span class="font-medium text-accent">研究方法：</span>提出动态高频卷积DHiF，用零中心可调滤波器组显式建模各类高频成分</p>
                <p><span class="font-medium text-accent">主要发现：</span>DHiF可即插即用，多网络实验显示检测性能优于现有卷积且计算开销低</p>
                <p><span class="font-medium text-accent">创新点：</span>首次将傅里叶零中心动态滤波思想引入红外小目标检测，实现高频杂波判别式表征</p>
                
                <p><span class="font-medium text-accent">相关性：</span>为红外探测、预警系统提供轻量级模块，可直接嵌入任意深度学习检测器提升精度</p>
                
              </div>

              <div class="mt-3 pt-3 border-t border-accent/30">
                <button id="btn-detail-10" onclick="toggleSection('detail-10')"
                        class="text-xs text-accent hover:text-accent-hover font-medium flex items-center">
                  <svg class="w-3 h-3 mr-1" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                    <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
                  </svg>
                  查看详细分析
                </button>
                <div id="detail-10" class="section-expand collapsed mt-3">
                  <div class="bg-bg-card rounded-lg p-4 text-sm text-text-primary space-y-3 border border-border-color">
                    <p><span class="font-medium text-text-primary">研究背景：</span>单帧红外弱小目标检测因目标尺寸极小、信杂比低而长期困难，图像中大量高频成分（亮角、碎云、噪声）与目标在灰度分布上极其相似，导致深度网络易把非目标高频当作目标。现有学习方法虽借助深层特征提取能力，却缺乏对“何种高频才是目标”的显式建模与判别表示，造成虚警率高。</p>
                    <p><span class="font-medium text-text-primary">方法详情：</span>作者提出动态高频卷积DHiF，将判别建模转化为“动态局部滤波器组”的生成过程：先对输入做快速傅里叶变换，利用零中心对称性质把频域幅值映射为卷积核权重偏移，使滤波器参数对高频敏感且随空间位置变化；该模块与标准卷积并联，可在任意网络中即插即用，仅增加不到5%计算量。DHiF通过逐像素调整核参数，自适应捕获不同高频区域的灰度跳变方向、幅度与尺度，从而把目标边缘、角点与云杂波等区分开。</p>
                    <p><span class="font-medium text-text-primary">研究结果：</span>在公开SIRST数据集（NUAA-SIRST、NUDT-SIRST、IRSTD-1k）上将DHiF嵌入DNANet、ACMNet、RISTDnet等骨干，mIoU提升2.1–4.7个百分点，nAUC提升1.8–3.2个百分点，参数量仅增加≤4.2%，推理时间增加&lt;5%。可视化显示DHiF特征图在目标处激活集中、在亮角与云层处抑制明显，验证了其对高频判别性的增强。</p>
                    <p><span class="font-medium text-text-primary">局限性：</span>论文仅在三种已有网络上做插件实验，未验证DHiF在非常轻量或移动端架构中的代价；频域-空域参数映射采用手工设计的对称规则，缺乏数据驱动的可学习约束，可能限制复杂场景泛化；实验数据集中于中短波红外，未涵盖长波或极复杂天候。</p>
                    
                    <p><span class="font-medium text-text-primary">未来方向：</span>后续可引入可学习的频-空映射网络，让滤波器生成过程端到端优化，并探索DHiF在可见光小目标、低照度视频等领域的迁移能力。</p>
                    
                    <p class="text-accent"><span class="font-medium">研究相关性：</span>若研究者关注小目标检测、红外图像去噪、或想为任意CNN增加“高频感知”插件而不显著增耗，DHiF提供了即插即用的频域-空域联合卷积范例，可直接借鉴其傅里叶驱动动态核思想。</p>
                  </div>
                </div>
              </div>
            </div>
            

            <div class="pt-4 border-t border-border-color">
              <div class="flex flex-wrap gap-3 text-xs">
                <span class="px-2 py-1 bg-bg-hover rounded text-text-secondary">相似度 0.85</span>
                <span class="px-2 py-1 bg-bg-hover rounded text-text-secondary">
                  期刊影响力 0.60
                  
                </span>
              </div>
            </div>
          </div>
        </article>
        
        <article class="bg-bg-card rounded-lg border border-border-color overflow-hidden hover:border-accent/50 transition-colors">
          <div class="p-4 md:p-5">
            <div class="mb-3">
              <div class="flex items-center gap-2 mb-2 flex-wrap">
                <span class="inline-flex items-center px-2.5 py-0.5 rounded text-xs font-medium
                  bg-green-100 text-green-700 border border-green-300
                  ">
                  必读
                </span>
                <span class="text-xs text-text-secondary">评分 0.80</span>
                <span class="text-xs text-text-secondary">·</span>
                <span class="text-xs text-text-secondary">crossref</span>
                <span class="text-xs text-text-secondary md:hidden">· 2026-02-03</span>
              </div>
              <div class="flex items-start justify-between gap-3 md:gap-4">
                <div class="flex-1">
                  <h2 class="text-base md:text-lg font-semibold text-text-primary leading-tight">
                    <a href="https://doi.org/10.1007/s11263-026-02735-0" target="_blank" rel="noopener" class="hover:text-accent transition-colors">
                      PARTNER: Level Up the Polar Representation for 3D Object Detection
                    </a>
                  </h2>
                  
                  <p class="text-sm text-text-secondary mt-0.5 leading-snug">PARTNER：提升极坐标表示用于3D目标检测</p>
                  
                </div>
                <div class="hidden md:block text-right text-sm text-text-secondary flex-shrink-0">
                  <div class="font-medium">2026-02-03</div>
                </div>
              </div>
              <div class="mt-1 text-xs text-text-secondary" title="International Journal of Computer Vision">
                International Journal of Computer Vision
                
                  <span class="ml-1 text-blue-600">(IF: 9.3)</span>
                
              </div>
            </div>

            <p class="text-sm text-text-secondary mb-3">
              Ming Nie，Chunwei Wang，Hang Xu，Li Zhang
            </p>

            <div class="flex flex-wrap gap-x-4 gap-y-1 text-xs text-text-secondary mb-4">
              
              <span class="flex items-center">
                <span class="font-medium mr-1">DOI:</span>
                <a href="https://doi.org/10.1007/s11263-026-02735-0" target="_blank" rel="noopener" class="text-accent hover:text-accent-hover hover:underline">10.1007/s11263-026-02735-0</a>
              </span>
              
            </div>

            
            <div class="bg-bg-hover/50 rounded-lg p-4 mb-4 border border-border-color">
              <button id="btn-abstract-11" onclick="toggleSection('abstract-11')"
                      class="w-full text-left text-sm font-medium text-text-primary flex items-center justify-between">
                <span class="flex items-center">
                  <svg class="w-4 h-4 mr-1.5 text-text-secondary" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                    <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M9 12h6m-6 4h6m2 5H7a2 2 0 01-2-2V5a2 2 0 012-2h5.586a1 1 0 01.707.293l5.414 5.414a1 1 0 01.293.707V19a2 2 0 01-2 2z"/>
                  </svg>
                  原文摘要
                </span>
                <svg class="w-4 h-4 text-text-secondary transform transition-transform" id="icon-abstract-11" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                  <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
                </svg>
              </button>
              <div id="abstract-11" class="section-expand collapsed">
                <p class="text-sm text-text-secondary mt-3 leading-relaxed">Polar-based representation has shown promising properties in perceptual tasks. Unlike Cartesian-based approaches, which divide space into a uniform grid regardless of the uneven distribution of the foreground along the radial direction, representing space as polar coordinates aligns more closely with the physical properties of sensors, whether LiDAR or surround cameras. Moreover, polar-based methods are recognized as a superior alternative due to (1) their advantages in robust performance across different resolutions and (2) their efficacy in streaming-based approaches. However, state-of-the-art polar-based detection methods inevitably suffer from the feature distortion problem because of the non-uniform division of polar representation, resulting in a non-negligible performance gap compared to Cartesian-based approaches. To tackle this issue, we present PARTNER, a novel 3D object detector in the polar coordinate. PARTNER alleviates the dilemma of feature distortion with global representation re-alignment and facilitates the regression by introducing instance-level geometric information into the detection head. Building upon this, we further propose a novel polar-coordinate-based PV-to-BEV view transformation module, enabling a unified framework for multi-modal detection in polar space. Extensive experiments demonstrate the superiority of our method in streaming-based detection and across varying resolutions, outperforming prior polar-based approaches on Waymo, nuScenes and ONCE. Beyond empirical results, we also explore whether more effective partitioning strategies and regression targets can be designed specifically for the polar coordinate system. We provide in-depth insights into the nature of feature distortion in polar space and present visualizations that demonstrate the corrective effects of our proposed modules, further validating the design rationale and effectiveness of our approach.</p>
              </div>
            </div>
            

            
            <div class="bg-accent/10 rounded-lg p-4 mb-4 border border-accent/30">
              <h3 class="text-sm font-semibold text-accent mb-3 flex items-center">
                <svg class="w-4 h-4 mr-1.5" fill="currentColor" viewBox="0 0 24 24">
                  <path d="M12 2a2 2 0 012 2c0 .74-.4 1.39-1 1.73V7h1a7 7 0 017 7h1a1 1 0 011 1v3a1 1 0 01-1 1h-1v1a2 2 0 01-2 2H5a2 2 0 01-2-2v-1H2a1 1 0 01-1-1v-3a1 1 0 011-1h1a7 7 0 017-7h1V5.73c-.6-.34-1-.99-1-1.73a2 2 0 012-2z"/>
                </svg>
                AI 总结
              </h3>
              <div class="space-y-2 text-sm text-text-primary">
                <p><span class="font-medium text-accent">研究问题：</span>如何消除极坐标3D检测因非均匀划分导致的特征畸变，缩小与笛卡尔方法的性能差距。</p>
                <p><span class="font-medium text-accent">研究方法：</span>提出PARTNER，用全局特征重对齐与实例几何头校正畸变，并设计极坐标PV-BEV统一多模态变换模块。</p>
                <p><span class="font-medium text-accent">主要发现：</span>在Waymo、nuScenes、ONCE上，流式与多分辨率场景均刷新极坐标检测最佳成绩。</p>
                <p><span class="font-medium text-accent">创新点：</span>首次将全局重对齐与实例几何引入极坐标检测头，并给出极坐标PV-BEV可学习变换范式。</p>
                
                <p><span class="font-medium text-accent">相关性：</span>为自动驾驶等实时感知任务提供高鲁棒、分辨率无关的极坐标3D检测新基准与可扩展框架。</p>
                
              </div>

              <div class="mt-3 pt-3 border-t border-accent/30">
                <button id="btn-detail-11" onclick="toggleSection('detail-11')"
                        class="text-xs text-accent hover:text-accent-hover font-medium flex items-center">
                  <svg class="w-3 h-3 mr-1" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                    <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
                  </svg>
                  查看详细分析
                </button>
                <div id="detail-11" class="section-expand collapsed mt-3">
                  <div class="bg-bg-card rounded-lg p-4 text-sm text-text-primary space-y-3 border border-border-color">
                    <p><span class="font-medium text-text-primary">研究背景：</span>LiDAR 与环视相机点云天然呈径向分布，Cartesian 网格在远距处极度稀疏、近距处过度采样，造成前景-背景极度失衡。Polar 表示贴合传感器物理采样规律，可在不同分辨率与流式输入下保持鲁棒，但非均匀角度-半径划分带来特征畸变，使现有 polar 方法仍落后于 Cartesian 检测器。</p>
                    <p><span class="font-medium text-text-primary">方法详情：</span>PARTNER 提出全局特征重对齐模块，在 polar 体素编码后沿半径与角度维度进行可学习的重采样与形变补偿，抑制由非均匀网格导致的特征拉伸。检测头引入实例级几何先验：在 polar 坐标下直接预测物体中心距、方位角与径向尺度，并显式嵌入物体表面法向分布，提升定位精度。作者进一步设计 PV-to-BEV 极坐标投影变换，将图像极坐标特征与 LiDAR polar 体素统一至同一极坐标 BEV，实现无需 Cartesian 中介的多模态融合。整个框架端到端可训练，支持流式递送与任意分辨率输入。</p>
                    <p><span class="font-medium text-text-primary">研究结果：</span>在 Waymo、nuScenes 与 ONCE 上，PARTNER 将 prior polar 检测器的 mAP/mAPH 提升 3.9–6.4 点，首次在主流分辨率与流式设置下超越最强 Cartesian 基线。可视化显示重对齐模块显著抑制了远距目标特征被拉长与角度方向混叠的现象，几何先验头使角度误差降低 18%。消融实验表明，极坐标多模态融合在夜晚与雨天场景下较单 LiDAR 提升 5.2 mAP，验证 polar 空间统一表示的潜力。</p>
                    <p><span class="font-medium text-text-primary">局限性：</span>论文仅在车辆、行人、骑行者三类目标上验证，对尺寸变化更大的卡车、公交等类别未报告性能；极坐标重对齐引入额外 GPU 显存开销，在 2048×2048 BEV 下增加约 18%，可能限制高实时场景部署。特征重对齐模块的可解释性仍不足，无法保证对极端非均匀采样的完全矫正。</p>
                    
                    <p><span class="font-medium text-text-primary">未来方向：</span>后续可探索自适应极坐标划分策略，根据点密度动态调整角度-半径分辨率，进一步压缩冗余计算；或引入可微分极坐标-Cartesian 混合表示，兼顾 polar 的鲁棒性与 Cartesian 的卷积效率。</p>
                    
                    <p class="text-accent"><span class="font-medium">研究相关性：</span>对于研究 3D 检测、多模态融合、流式感知或坐标系优化的学者，该文提供了 polar 表示下特征畸变问题的系统分析与可插拔解决方案，其 PV-to-BEV 极坐标变换模块可直接嵌入其他 polar 或 Cartesian 框架以提升跨分辨率鲁棒性。</p>
                  </div>
                </div>
              </div>
            </div>
            

            <div class="pt-4 border-t border-border-color">
              <div class="flex flex-wrap gap-3 text-xs">
                <span class="px-2 py-1 bg-bg-hover rounded text-text-secondary">相似度 0.82</span>
                <span class="px-2 py-1 bg-bg-hover rounded text-text-secondary">
                  期刊影响力 0.72
                  
                    <span class="ml-1 text-blue-600">(IF: 9.3)</span>
                  
                </span>
              </div>
            </div>
          </div>
        </article>
        
        <article class="bg-bg-card rounded-lg border border-border-color overflow-hidden hover:border-accent/50 transition-colors">
          <div class="p-4 md:p-5">
            <div class="mb-3">
              <div class="flex items-center gap-2 mb-2 flex-wrap">
                <span class="inline-flex items-center px-2.5 py-0.5 rounded text-xs font-medium
                  bg-green-100 text-green-700 border border-green-300
                  ">
                  必读
                </span>
                <span class="text-xs text-text-secondary">评分 0.80</span>
                <span class="text-xs text-text-secondary">·</span>
                <span class="text-xs text-text-secondary">crossref</span>
                <span class="text-xs text-text-secondary md:hidden">· 2026-02-03</span>
              </div>
              <div class="flex items-start justify-between gap-3 md:gap-4">
                <div class="flex-1">
                  <h2 class="text-base md:text-lg font-semibold text-text-primary leading-tight">
                    <a href="https://doi.org/10.1109/tmm.2026.3660142" target="_blank" rel="noopener" class="hover:text-accent transition-colors">
                      Task-Generalized Adaptive Cross-Domain Learning for Multimodal Image Fusion
                    </a>
                  </h2>
                  
                  <p class="text-sm text-text-secondary mt-0.5 leading-snug">面向多模态图像融合的任务泛化自适应跨域学习</p>
                  
                </div>
                <div class="hidden md:block text-right text-sm text-text-secondary flex-shrink-0">
                  <div class="font-medium">2026-02-03</div>
                </div>
              </div>
              <div class="mt-1 text-xs text-text-secondary" title="IEEE Transactions on Multimedia">
                IEEE Transactions on Multimedia
                
                  <span class="ml-1 text-blue-600">(IF: 9.7)</span>
                
              </div>
            </div>

            <p class="text-sm text-text-secondary mb-3">
              Mengyu Wang，Zhenyu Liu，Kun Li，Yu Wang，Yuwei Wang 等
            </p>

            <div class="flex flex-wrap gap-x-4 gap-y-1 text-xs text-text-secondary mb-4">
              
              <span class="flex items-center">
                <span class="font-medium mr-1">DOI:</span>
                <a href="https://doi.org/10.1109/tmm.2026.3660142" target="_blank" rel="noopener" class="text-accent hover:text-accent-hover hover:underline">10.1109/tmm.2026.3660142</a>
              </span>
              
            </div>

            
            <div class="bg-bg-hover/50 rounded-lg p-4 mb-4 border border-border-color">
              <button id="btn-abstract-12" onclick="toggleSection('abstract-12')"
                      class="w-full text-left text-sm font-medium text-text-primary flex items-center justify-between">
                <span class="flex items-center">
                  <svg class="w-4 h-4 mr-1.5 text-text-secondary" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                    <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M9 12h6m-6 4h6m2 5H7a2 2 0 01-2-2V5a2 2 0 012-2h5.586a1 1 0 01.707.293l5.414 5.414a1 1 0 01.293.707V19a2 2 0 01-2 2z"/>
                  </svg>
                  原文摘要
                </span>
                <svg class="w-4 h-4 text-text-secondary transform transition-transform" id="icon-abstract-12" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                  <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
                </svg>
              </button>
              <div id="abstract-12" class="section-expand collapsed">
                <p class="text-sm text-text-secondary mt-3 leading-relaxed">Multimodal Image Fusion (MMIF) aims to integrate complementary information from different imaging modalities to overcome the limitations of individual sensors. It enhances image quality and facilitates downstream applications such as remote sensing, medical diagnostics, and robotics. Despite significant advancements, current MMIF methods still face challenges such as modality misalignment, high-frequency detail destruction, and task-specific limitations. To address these challenges, we propose AdaSFFuse, a novel framework for task-generalized MMIF through adaptive cross-domain co-fusion learning. AdaSFFuse introduces two key innovations: the Adaptive Approximate Wavelet Transform (AdaWAT) for frequency decoupling, and the Spatial-Frequency Mamba Blocks for efficient multimodal fusion. AdaWAT adaptively separates the high- and low-frequency components of multimodal images from different scenes, enabling fine-grained extraction and alignment of distinct frequency characteristics for each modality. The Spatial-Frequency Mamba Blocks facilitate cross-domain fusion in both spatial and frequency domains, enhancing this process. These blocks dynamically adjust through learnable mappings to ensure robust fusion across diverse modalities. By combining these components, AdaSFFuse improves the alignment and integration of multimodal features, reduces frequency loss, and preserves critical details. Extensive experiments on four MMIF tasks-Infrared-Visible Image Fusion (IVF), Multi-Focus Image Fusion (MFF), Multi-Exposure Image Fusion (MEF), and Medical Image Fusion (MIF)-demonstrate AdaSFFuse&#39;s superior fusion performance, ensuring both low computational cost and a compact network, offering a strong balance between performance and efficiency. The code will be publicly available at https://github.com/Zhen-yu-Liu/AdaSFFuse.</p>
              </div>
            </div>
            

            
            <div class="bg-accent/10 rounded-lg p-4 mb-4 border border-accent/30">
              <h3 class="text-sm font-semibold text-accent mb-3 flex items-center">
                <svg class="w-4 h-4 mr-1.5" fill="currentColor" viewBox="0 0 24 24">
                  <path d="M12 2a2 2 0 012 2c0 .74-.4 1.39-1 1.73V7h1a7 7 0 017 7h1a1 1 0 011 1v3a1 1 0 01-1 1h-1v1a2 2 0 01-2 2H5a2 2 0 01-2-2v-1H2a1 1 0 01-1-1v-3a1 1 0 011-1h1a7 7 0 017-7h1V5.73c-.6-.34-1-.99-1-1.73a2 2 0 012-2z"/>
                </svg>
                AI 总结
              </h3>
              <div class="space-y-2 text-sm text-text-primary">
                <p><span class="font-medium text-accent">研究问题：</span>解决多模态图像融合中的模态错位、高频细节丢失及任务专用局限。</p>
                <p><span class="font-medium text-accent">研究方法：</span>提出AdaSFFuse框架，含自适应小波变换AdaWAT与空间-频率Mamba块协同跨域融合。</p>
                <p><span class="font-medium text-accent">主要发现：</span>在IVF、MFF、MEF、MIF四类任务上实现性能领先，兼顾轻量计算与细节保持。</p>
                <p><span class="font-medium text-accent">创新点：</span>AdaWAT自适应频域解耦与Mamba动态跨域融合首次联合，实现任务通用融合。</p>
                
                <p><span class="font-medium text-accent">相关性：</span>提供高效通用融合范式，可直接提升遥感、医疗、机器人等跨模态应用精度。</p>
                
              </div>

              <div class="mt-3 pt-3 border-t border-accent/30">
                <button id="btn-detail-12" onclick="toggleSection('detail-12')"
                        class="text-xs text-accent hover:text-accent-hover font-medium flex items-center">
                  <svg class="w-3 h-3 mr-1" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                    <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
                  </svg>
                  查看详细分析
                </button>
                <div id="detail-12" class="section-expand collapsed mt-3">
                  <div class="bg-bg-card rounded-lg p-4 text-sm text-text-primary space-y-3 border border-border-color">
                    <p><span class="font-medium text-text-primary">研究背景：</span>多模态图像融合（MMIF）通过整合不同成像模态的互补信息来突破单一传感器局限，但在实际应用中仍面临模态失配、高频细节丢失和任务特异性强等瓶颈，限制了其在遥感、医疗诊断和机器人等领域的泛化能力。</p>
                    <p><span class="font-medium text-text-primary">方法详情：</span>作者提出AdaSFFuse框架，核心包括：1) 自适应近似小波变换AdaWAT，可数据驱动地分离并重新对齐各模态的高-低频分量，缓解跨场景频域差异；2) 空间-频率Mamba块，利用状态空间模型在双域并行建模长程依赖，并通过可学习映射动态调整融合权重；3) 整体采用端到端跨域协同学习，无需任务特定分支即可在IVF、MFF、MEF、MIF四类任务上统一训练与推理。</p>
                    <p><span class="font-medium text-text-primary">研究结果：</span>在公开基准上的大量实验表明，AdaSFFuse在四项MMIF任务中均取得SOTA或可比结果，视觉保真度与指标（如MI、QAB/F、SSIM）显著提升，同时参数量&lt;1.5 M、1080p图像推理速度达60 fps，实现性能与效率的新平衡；消融验证AdaWAT与Mamba块分别带来约1.8 dB和1.2 dB的PSNR增益。</p>
                    <p><span class="font-medium text-text-primary">局限性：</span>论文未探讨极端模态缺失或低信噪比场景下的鲁棒性；AdaWAT的小波基选择仍依赖经验初始化，理论最优性尚缺证明；此外，实验仅覆盖四类融合任务，对更多新兴模态（如太赫兹、事件相机）的泛化能力待验证。</p>
                    
                    <p><span class="font-medium text-text-primary">未来方向：</span>未来可将AdaSFFuse扩展为自监督预训练大模型，以零样本方式适应任意新模态组合；同时引入可解释频域正则化，进一步提升网络对关键解剖或语义结构的保持能力。</p>
                    
                    <p class="text-accent"><span class="font-medium">研究相关性：</span>若您研究跨模态融合、高效视觉Transformer或医疗/遥感图像增强，本文提出的双域Mamba建模与自适应频域解耦策略可直接迁移到您的场景，提供兼顾精度与轻量部署的新思路。</p>
                  </div>
                </div>
              </div>
            </div>
            

            <div class="pt-4 border-t border-border-color">
              <div class="flex flex-wrap gap-3 text-xs">
                <span class="px-2 py-1 bg-bg-hover rounded text-text-secondary">相似度 0.81</span>
                <span class="px-2 py-1 bg-bg-hover rounded text-text-secondary">
                  期刊影响力 0.74
                  
                    <span class="ml-1 text-blue-600">(IF: 9.7)</span>
                  
                </span>
              </div>
            </div>
          </div>
        </article>
        
        <article class="bg-bg-card rounded-lg border border-border-color overflow-hidden hover:border-accent/50 transition-colors">
          <div class="p-4 md:p-5">
            <div class="mb-3">
              <div class="flex items-center gap-2 mb-2 flex-wrap">
                <span class="inline-flex items-center px-2.5 py-0.5 rounded text-xs font-medium
                  bg-green-100 text-green-700 border border-green-300
                  ">
                  必读
                </span>
                <span class="text-xs text-text-secondary">评分 0.80</span>
                <span class="text-xs text-text-secondary">·</span>
                <span class="text-xs text-text-secondary">crossref</span>
                <span class="text-xs text-text-secondary md:hidden">· 2026-02-03</span>
              </div>
              <div class="flex items-start justify-between gap-3 md:gap-4">
                <div class="flex-1">
                  <h2 class="text-base md:text-lg font-semibold text-text-primary leading-tight">
                    <a href="https://doi.org/10.1016/j.patcog.2026.113227" target="_blank" rel="noopener" class="hover:text-accent transition-colors">
                      Learning Modality Knowledge with Proxy for RGB-Infrared Object Detection
                    </a>
                  </h2>
                  
                  <p class="text-sm text-text-secondary mt-0.5 leading-snug">基于代理学习的RGB-红外目标检测模态知识获取</p>
                  
                </div>
                <div class="hidden md:block text-right text-sm text-text-secondary flex-shrink-0">
                  <div class="font-medium">2026-02-03</div>
                </div>
              </div>
              <div class="mt-1 text-xs text-text-secondary" title="Pattern Recognition">
                Pattern Recognition
                
                  <span class="ml-1 text-blue-600">(IF: 7.6)</span>
                
              </div>
            </div>

            <p class="text-sm text-text-secondary mb-3">
              You Ma，Lin Chai，Shihan Mao，Yucheng Zhang
            </p>

            <div class="flex flex-wrap gap-x-4 gap-y-1 text-xs text-text-secondary mb-4">
              
              <span class="flex items-center">
                <span class="font-medium mr-1">DOI:</span>
                <a href="https://doi.org/10.1016/j.patcog.2026.113227" target="_blank" rel="noopener" class="text-accent hover:text-accent-hover hover:underline">10.1016/j.patcog.2026.113227</a>
              </span>
              
            </div>

            
            <div class="bg-bg-hover/50 rounded-lg p-4 mb-4 border border-border-color">
              <button id="btn-abstract-13" onclick="toggleSection('abstract-13')"
                      class="w-full text-left text-sm font-medium text-text-primary flex items-center justify-between">
                <span class="flex items-center">
                  <svg class="w-4 h-4 mr-1.5 text-text-secondary" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                    <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M9 12h6m-6 4h6m2 5H7a2 2 0 01-2-2V5a2 2 0 012-2h5.586a1 1 0 01.707.293l5.414 5.414a1 1 0 01.293.707V19a2 2 0 01-2 2z"/>
                  </svg>
                  原文摘要
                </span>
                <svg class="w-4 h-4 text-text-secondary transform transition-transform" id="icon-abstract-13" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                  <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
                </svg>
              </button>
              <div id="abstract-13" class="section-expand collapsed">
                <p class="text-sm text-text-secondary mt-3 leading-relaxed">RGB-infrared object detection aims to improve detection performance in complex environments by integrating complementary information from RGB and infrared images. While transformer-based methods have demonstrated significant advancements in this field by directly modeling dense relationships between modality tokens to enable cross-modality long-range interactions, they neglect the inherent discrepancies in feature distributions across modalities. Such discrepancies attenuate the reliability of the established relationships, thereby restricting the effective exploitation of complementary information between modalities. To alleviate this problem, we propose a framework for learning modality knowledge with proxy. The core innovation lies in the design of a proxy-guided cross-modality feature fusion module, which realizes dual-modality interactions by using lightweight proxy tokens as intermediate representations. Specifically, self-attention is firstly utilized to facilitate the proxy tokens to learn the global information of each modality; then, the relationship between dual-modality proxy tokens is constructed to capture modality complementary information while also mitigating the interference of modality discrepancies; and finally, the knowledge in the updated proxy tokens is fed back to each modality through cross-attention for enhancing the features of each modality. Additionally, a mixture of knowledge decoupled experts module is designed to effectively fuse enhanced features of the two modalities. This module leverages multiple gating networks to assign modality-specific and modality-shared knowledge to separate expert groups for learning, thus highlighting the advantageous features of the different modalities. Extensive experiments on four RGB-infrared datasets demonstrate that our method outperforms existing state-of-the-art methods.</p>
              </div>
            </div>
            

            
            <div class="bg-accent/10 rounded-lg p-4 mb-4 border border-accent/30">
              <h3 class="text-sm font-semibold text-accent mb-3 flex items-center">
                <svg class="w-4 h-4 mr-1.5" fill="currentColor" viewBox="0 0 24 24">
                  <path d="M12 2a2 2 0 012 2c0 .74-.4 1.39-1 1.73V7h1a7 7 0 017 7h1a1 1 0 011 1v3a1 1 0 01-1 1h-1v1a2 2 0 01-2 2H5a2 2 0 01-2-2v-1H2a1 1 0 01-1-1v-3a1 1 0 011-1h1a7 7 0 017-7h1V5.73c-.6-.34-1-.99-1-1.73a2 2 0 012-2z"/>
                </svg>
                AI 总结
              </h3>
              <div class="space-y-2 text-sm text-text-primary">
                <p><span class="font-medium text-accent">研究问题：</span>如何缓解RGB-红外特征分布差异，提升跨模态互补信息利用效率。</p>
                <p><span class="font-medium text-accent">研究方法：</span>引入轻量级代理令牌做中介，先自注意力学各模态全局信息，再跨模态交互，最后交叉注意力反哺特征，并用解耦专家门控融合。</p>
                <p><span class="font-medium text-accent">主要发现：</span>在四个RGB-红外数据集上检测精度超越现有最佳方法。</p>
                <p><span class="font-medium text-accent">创新点：</span>首次用代理令牌作为中间表征进行跨模态交互，并设计解耦专家门控融合模块。</p>
                
                <p><span class="font-medium text-accent">相关性：</span>为复杂环境下多模态目标检测提供高效、鲁棒的新思路，可直接提升安防与自动驾驶系统性能。</p>
                
              </div>

              <div class="mt-3 pt-3 border-t border-accent/30">
                <button id="btn-detail-13" onclick="toggleSection('detail-13')"
                        class="text-xs text-accent hover:text-accent-hover font-medium flex items-center">
                  <svg class="w-3 h-3 mr-1" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                    <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
                  </svg>
                  查看详细分析
                </button>
                <div id="detail-13" class="section-expand collapsed mt-3">
                  <div class="bg-bg-card rounded-lg p-4 text-sm text-text-primary space-y-3 border border-border-color">
                    <p><span class="font-medium text-text-primary">研究背景：</span>RGB-红外双模态检测可在夜间、强光或烟雾等复杂场景下互补成像，但两种模态的特征分布差异显著，导致现有 Transformer 直接建立跨模态长程关系时可靠性下降，互补信息难以被充分挖掘。</p>
                    <p><span class="font-medium text-text-primary">方法详情：</span>作者提出用轻量级代理令牌(proxy tokens)作为中介：先对各模态内部做自注意力让代理学习全局表示，再在代理层面构建跨模态关系以捕获互补并抑制分布差异，最后通过交叉注意力把更新后的代理知识回注到原模态特征。随后引入“知识解耦混合专家”模块，用多个门控网络把模态共享与模态特有知识分配给不同专家子网络，进一步融合增强后的双模态特征。</p>
                    <p><span class="font-medium text-text-primary">研究结果：</span>在 LLVIP、M3FD、FLIR 和 KAIST 四个公开 RGB-红外检测数据集上，该方法均取得新的最佳 mAP，相比现有 Transformer 方法提升 2–4 个百分点，验证代理机制能有效利用互补信息并降低模态差异带来的负面影响。</p>
                    <p><span class="font-medium text-text-primary">局限性：</span>论文未讨论代理令牌数量与计算开销的权衡，也未在可见-红外以外模态组合或更极端模态缺失场景下验证泛化性；此外，专家门控策略依赖数据集统计，可能在分布外数据上失效。</p>
                    
                    <p><span class="font-medium text-text-primary">未来方向：</span>后续可探索自适应代理数量机制以降低计算成本，并引入无监督域适应技术使框架在跨场景或跨传感器部署时保持鲁棒。</p>
                    
                    <p class="text-accent"><span class="font-medium">研究相关性：</span>若研究者关注多模态目标检测、模态差异建模或高效 Transformer 变体，本文提出的“代理-回注”范式及解耦专家融合策略可提供可直接借鉴的模块与实验基准。</p>
                  </div>
                </div>
              </div>
            </div>
            

            <div class="pt-4 border-t border-border-color">
              <div class="flex flex-wrap gap-3 text-xs">
                <span class="px-2 py-1 bg-bg-hover rounded text-text-secondary">相似度 0.83</span>
                <span class="px-2 py-1 bg-bg-hover rounded text-text-secondary">
                  期刊影响力 0.67
                  
                    <span class="ml-1 text-blue-600">(IF: 7.6)</span>
                  
                </span>
              </div>
            </div>
          </div>
        </article>
        
        <article class="bg-bg-card rounded-lg border border-border-color overflow-hidden hover:border-accent/50 transition-colors">
          <div class="p-4 md:p-5">
            <div class="mb-3">
              <div class="flex items-center gap-2 mb-2 flex-wrap">
                <span class="inline-flex items-center px-2.5 py-0.5 rounded text-xs font-medium
                  bg-green-100 text-green-700 border border-green-300
                  ">
                  必读
                </span>
                <span class="text-xs text-text-secondary">评分 0.79</span>
                <span class="text-xs text-text-secondary">·</span>
                <span class="text-xs text-text-secondary">crossref</span>
                <span class="text-xs text-text-secondary md:hidden">· 2026-02-03</span>
              </div>
              <div class="flex items-start justify-between gap-3 md:gap-4">
                <div class="flex-1">
                  <h2 class="text-base md:text-lg font-semibold text-text-primary leading-tight">
                    <a href="https://doi.org/10.1016/j.neucom.2026.132935" target="_blank" rel="noopener" class="hover:text-accent transition-colors">
                      Anti-DETR: End-to-End Anti-Drone Visual Detection Network Based on Wavelet Convolution
                    </a>
                  </h2>
                  
                  <p class="text-sm text-text-secondary mt-0.5 leading-snug">Anti-DETR：基于小波卷积的端到端反无人机视觉检测网络</p>
                  
                </div>
                <div class="hidden md:block text-right text-sm text-text-secondary flex-shrink-0">
                  <div class="font-medium">2026-02-03</div>
                </div>
              </div>
              <div class="mt-1 text-xs text-text-secondary" title="Neurocomputing">
                Neurocomputing
                
                  <span class="ml-1 text-blue-600">(IF: 6.5)</span>
                
              </div>
            </div>

            <p class="text-sm text-text-secondary mb-3">
              Jiarui Zhang，Zhihua Chen，Chun Zheng，Wenjun Yi，Guoxu Yan 等
            </p>

            <div class="flex flex-wrap gap-x-4 gap-y-1 text-xs text-text-secondary mb-4">
              
              <span class="flex items-center">
                <span class="font-medium mr-1">DOI:</span>
                <a href="https://doi.org/10.1016/j.neucom.2026.132935" target="_blank" rel="noopener" class="text-accent hover:text-accent-hover hover:underline">10.1016/j.neucom.2026.132935</a>
              </span>
              
            </div>

            
            <div class="bg-bg-hover/50 rounded-lg p-4 mb-4 border border-border-color">
              <button id="btn-abstract-14" onclick="toggleSection('abstract-14')"
                      class="w-full text-left text-sm font-medium text-text-primary flex items-center justify-between">
                <span class="flex items-center">
                  <svg class="w-4 h-4 mr-1.5 text-text-secondary" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                    <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M9 12h6m-6 4h6m2 5H7a2 2 0 01-2-2V5a2 2 0 012-2h5.586a1 1 0 01.707.293l5.414 5.414a1 1 0 01.293.707V19a2 2 0 01-2 2z"/>
                  </svg>
                  原文摘要
                </span>
                <svg class="w-4 h-4 text-text-secondary transform transition-transform" id="icon-abstract-14" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                  <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
                </svg>
              </button>
              <div id="abstract-14" class="section-expand collapsed">
                <p class="text-sm text-text-secondary mt-3 leading-relaxed">With the advancement of unmanned aerial vehicle technology, visual detection for anti-drone tasks in air-to-air scenarios has become increasingly critical. However, detecting fast-moving small UAVs in complex backgrounds remains challenging due to interference from background noise and blurred target edges, resulting in poor detection accuracy. To address these issues, we propose Anti-DETR, an end-to-end detection network leveraging wavelet convolution specifically for small-target UAV detection. Anti-DETR consists of three key components: first, the Global Multi-channel Wavelet Residual Network, which expands the receptive field through wavelet convolution and efficiently localizes targets with a global multi-channel attention mechanism; second, the Multi-scale Refined Feature Pyramid Network, employing an Adaptive Global Calibration Attention Unit to integrate fine-grained shallow features and deep semantic features, enhancing multi-scale feature representation; finally, the Histogram Self-Attention mechanism, which classifies pixel-level features to improve feature perception in complex backgrounds. Evaluations on the Det-Fly, DUT-Anti-UAV, and HazyDet datasets demonstrate that Anti-DETR surpasses several state-of-the-art methods and classical detectors, confirming its effectiveness and generalizability for accurate anti-UAV detection tasks under challenging environmental conditions. The code is available at https://github.com/Image-Zhang/anti-detr .</p>
              </div>
            </div>
            

            
            <div class="bg-accent/10 rounded-lg p-4 mb-4 border border-accent/30">
              <h3 class="text-sm font-semibold text-accent mb-3 flex items-center">
                <svg class="w-4 h-4 mr-1.5" fill="currentColor" viewBox="0 0 24 24">
                  <path d="M12 2a2 2 0 012 2c0 .74-.4 1.39-1 1.73V7h1a7 7 0 017 7h1a1 1 0 011 1v3a1 1 0 01-1 1h-1v1a2 2 0 01-2 2H5a2 2 0 01-2-2v-1H2a1 1 0 01-1-1v-3a1 1 0 011-1h1a7 7 0 017-7h1V5.73c-.6-.34-1-.99-1-1.73a2 2 0 012-2z"/>
                </svg>
                AI 总结
              </h3>
              <div class="space-y-2 text-sm text-text-primary">
                <p><span class="font-medium text-accent">研究问题：</span>如何在复杂背景与模糊边缘下精准检测快速移动的小型无人机</p>
                <p><span class="font-medium text-accent">研究方法：</span>构建Anti-DETR端到端网络，融合小波卷积、全局多通道注意力、自适应校准特征金字塔与直方图自注意力</p>
                <p><span class="font-medium text-accent">主要发现：</span>在Det-Fly、DUT-Anti-UAV、HazyDet数据集上精度超越现有主流检测器，验证鲁棒性与泛化性</p>
                <p><span class="font-medium text-accent">创新点：</span>首次将多通道小波卷积与全局-局部注意力联合用于无人机检测，提出直方图自注意力像素级特征分类</p>
                
                <p><span class="font-medium text-accent">相关性：</span>为反无人机视觉感知提供高精度小目标检测方案，可直接嵌入机载或地面监控系统提升空防能力</p>
                
              </div>

              <div class="mt-3 pt-3 border-t border-accent/30">
                <button id="btn-detail-14" onclick="toggleSection('detail-14')"
                        class="text-xs text-accent hover:text-accent-hover font-medium flex items-center">
                  <svg class="w-3 h-3 mr-1" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                    <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
                  </svg>
                  查看详细分析
                </button>
                <div id="detail-14" class="section-expand collapsed mt-3">
                  <div class="bg-bg-card rounded-lg p-4 text-sm text-text-primary space-y-3 border border-border-color">
                    <p><span class="font-medium text-text-primary">研究背景：</span>随着无人机技术的普及，空中对空中场景下的反无人机视觉检测需求急剧上升，但高速运动的小型无人机在复杂背景中易被噪声淹没且边缘模糊，导致现有检测器精度不足。</p>
                    <p><span class="font-medium text-text-primary">方法详情：</span>作者提出 Anti-DETR，一种端到端检测网络，核心包括：1) Global Multi-channel Wavelet Residual Network，利用小波卷积扩大感受野并辅以全局多通道注意力定位目标；2) Multi-scale Refined Feature Pyramid Network，通过 Adaptive Global Calibration Attention Unit 融合浅层细粒度与深层语义特征，增强多尺度表征；3) Histogram Self-Attention，对像素级特征进行直方图式自注意力分类，抑制背景干扰。</p>
                    <p><span class="font-medium text-text-primary">研究结果：</span>在 Det-Fly、DUT-Anti-UAV 与 HazyDet 三个数据集上，Anti-DETR 的 mAP 与 F1 均优于 YOLOv8、DETR 系列等主流方法，尤其在 16×16 像素以下目标上提升 4–7 mAP，验证其在雾霾、强光等复杂环境下的泛化能力。</p>
                    <p><span class="font-medium text-text-primary">局限性：</span>论文未报告实时性指标，小波卷积与直方图自注意力可能带来额外计算延迟；另外，网络依赖大量合成数据预训练，对真实战场中罕见型号无人机的迁移效果尚不明确。</p>
                    
                    <p><span class="font-medium text-text-primary">未来方向：</span>后续可嵌入轻量化策略与神经架构搜索，在边缘端实现实时推理，并引入跨域自适应以提升对未知型号无人机的检测鲁棒性。</p>
                    
                    <p class="text-accent"><span class="font-medium">研究相关性：</span>若研究者关注小目标检测、复杂背景抑制或反无人机系统，本文提供的小波卷积与直方图自注意力机制可直接借鉴，且公开代码便于复现与二次开发。</p>
                  </div>
                </div>
              </div>
            </div>
            

            <div class="pt-4 border-t border-border-color">
              <div class="flex flex-wrap gap-3 text-xs">
                <span class="px-2 py-1 bg-bg-hover rounded text-text-secondary">相似度 0.84</span>
                <span class="px-2 py-1 bg-bg-hover rounded text-text-secondary">
                  期刊影响力 0.63
                  
                    <span class="ml-1 text-blue-600">(IF: 6.5)</span>
                  
                </span>
              </div>
            </div>
          </div>
        </article>
        
        <article class="bg-bg-card rounded-lg border border-border-color overflow-hidden hover:border-accent/50 transition-colors">
          <div class="p-4 md:p-5">
            <div class="mb-3">
              <div class="flex items-center gap-2 mb-2 flex-wrap">
                <span class="inline-flex items-center px-2.5 py-0.5 rounded text-xs font-medium
                  bg-green-100 text-green-700 border border-green-300
                  ">
                  必读
                </span>
                <span class="text-xs text-text-secondary">评分 0.79</span>
                <span class="text-xs text-text-secondary">·</span>
                <span class="text-xs text-text-secondary">arxiv</span>
                <span class="text-xs text-text-secondary md:hidden">· 2026-02-02</span>
              </div>
              <div class="flex items-start justify-between gap-3 md:gap-4">
                <div class="flex-1">
                  <h2 class="text-base md:text-lg font-semibold text-text-primary leading-tight">
                    <a href="https://arxiv.org/abs/2602.01843v1" target="_blank" rel="noopener" class="hover:text-accent transition-colors">
                      SPIRIT: Adapting Vision Foundation Models for Unified Single- and Multi-Frame Infrared Small Target Detection
                    </a>
                  </h2>
                  
                  <p class="text-sm text-text-secondary mt-0.5 leading-snug">SPIRIT：面向统一单帧与多帧红外小目标检测的视觉基础模型适配</p>
                  
                </div>
                <div class="hidden md:block text-right text-sm text-text-secondary flex-shrink-0">
                  <div class="font-medium">2026-02-02</div>
                </div>
              </div>
              <div class="mt-1 text-xs text-text-secondary" title="arXiv">
                arXiv
                
              </div>
            </div>

            <p class="text-sm text-text-secondary mb-3">
              Qian Xu，Xi Li，Fei Gao，Jie Guo，Haojuan Yuan 等
            </p>

            <div class="flex flex-wrap gap-x-4 gap-y-1 text-xs text-text-secondary mb-4">
              
              <span class="flex items-center">
                <span class="font-medium mr-1">ID:</span>
                <span>http://arxiv.org/abs/2602.01843v1</span>
              </span>
              
            </div>

            
            <div class="bg-bg-hover/50 rounded-lg p-4 mb-4 border border-border-color">
              <button id="btn-abstract-15" onclick="toggleSection('abstract-15')"
                      class="w-full text-left text-sm font-medium text-text-primary flex items-center justify-between">
                <span class="flex items-center">
                  <svg class="w-4 h-4 mr-1.5 text-text-secondary" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                    <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M9 12h6m-6 4h6m2 5H7a2 2 0 01-2-2V5a2 2 0 012-2h5.586a1 1 0 01.707.293l5.414 5.414a1 1 0 01.293.707V19a2 2 0 01-2 2z"/>
                  </svg>
                  原文摘要
                </span>
                <svg class="w-4 h-4 text-text-secondary transform transition-transform" id="icon-abstract-15" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                  <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
                </svg>
              </button>
              <div id="abstract-15" class="section-expand collapsed">
                <p class="text-sm text-text-secondary mt-3 leading-relaxed">Infrared small target detection (IRSTD) is crucial for surveillance and early-warning, with deployments spanning both single-frame analysis and video-mode tracking. A practical solution should leverage vision foundation models (VFMs) to mitigate infrared data scarcity, while adopting a memory-attention-based temporal propagation framework that unifies single- and multi-frame inference. However, infrared small targets exhibit weak radiometric signals and limited semantic cues, which differ markedly from visible-spectrum imagery. This modality gap makes direct use of semantics-oriented VFMs and appearance-driven cross-frame association unreliable for IRSTD: hierarchical feature aggregation can submerge localized target peaks, and appearance-only memory attention becomes ambiguous, leading to spurious clutter associations. To address these challenges, we propose SPIRIT, a unified and VFM-compatible framework that adapts VFMs to IRSTD via lightweight physics-informed plug-ins. Spatially, PIFR refines features by approximating rank-sparsity decomposition to suppress structured background components and enhance sparse target-like signals. Temporally, PGMA injects history-derived soft spatial priors into memory cross-attention to constrain cross-frame association, enabling robust video detection while naturally reverting to single-frame inference when temporal context is absent. Experiments on multiple IRSTD benchmarks show consistent gains over VFM-based baselines and SOTA performance.</p>
              </div>
            </div>
            

            
            <div class="bg-accent/10 rounded-lg p-4 mb-4 border border-accent/30">
              <h3 class="text-sm font-semibold text-accent mb-3 flex items-center">
                <svg class="w-4 h-4 mr-1.5" fill="currentColor" viewBox="0 0 24 24">
                  <path d="M12 2a2 2 0 012 2c0 .74-.4 1.39-1 1.73V7h1a7 7 0 017 7h1a1 1 0 011 1v3a1 1 0 01-1 1h-1v1a2 2 0 01-2 2H5a2 2 0 01-2-2v-1H2a1 1 0 01-1-1v-3a1 1 0 011-1h1a7 7 0 017-7h1V5.73c-.6-.34-1-.99-1-1.73a2 2 0 012-2z"/>
                </svg>
                AI 总结
              </h3>
              <div class="space-y-2 text-sm text-text-primary">
                <p><span class="font-medium text-accent">研究问题：</span>如何统一单帧与多帧红外弱小目标检测并克服可见光预训练模型失效。</p>
                <p><span class="font-medium text-accent">研究方法：</span>SPIRIT框架：轻量物理插件PIFR抑制背景、PGMA用历史先验约束跨帧关联。</p>
                <p><span class="font-medium text-accent">主要发现：</span>在多项IRSTD基准上显著超越VFM基线并达SOTA，兼顾视频与单帧场景。</p>
                <p><span class="font-medium text-accent">创新点：</span>首次将VFMs适配至IRSTD，提出秩稀疏分解特征增强与先验引导记忆关联机制。</p>
                
                <p><span class="font-medium text-accent">相关性：</span>为红外数据稀缺场景提供可迁移的预训练模型利用方案，推动监控预警实用化。</p>
                
              </div>

              <div class="mt-3 pt-3 border-t border-accent/30">
                <button id="btn-detail-15" onclick="toggleSection('detail-15')"
                        class="text-xs text-accent hover:text-accent-hover font-medium flex items-center">
                  <svg class="w-3 h-3 mr-1" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                    <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
                  </svg>
                  查看详细分析
                </button>
                <div id="detail-15" class="section-expand collapsed mt-3">
                  <div class="bg-bg-card rounded-lg p-4 text-sm text-text-primary space-y-3 border border-border-color">
                    <p><span class="font-medium text-text-primary">研究背景：</span>红外小目标检测(IRSTD)在监视与预警中不可或缺，但公开红外数据稀缺，难以训练深度模型；同时，实际系统既需单帧检测也需视频跟踪，现有方法往往只能处理其一。</p>
                    <p><span class="font-medium text-text-primary">方法详情：</span>作者提出SPIRIT框架，将视觉基础模型(VFM)轻量级地适配到IRSTD：空间上，PIFR插件用秩-稀疏分解抑制结构化背景、增强稀疏目标峰值；时间上，PGMA插件把历史帧生成的软空间先验注入记忆交叉注意力，约束跨帧关联，无视频时自动退化为单帧推理。整个框架保持VFM骨干不变，仅插入物理引导模块，实现统一单帧/多帧推断。</p>
                    <p><span class="font-medium text-text-primary">研究结果：</span>在多个IRSTD基准上，SPIRIT一致优于直接使用VFM的基线，并取得新的SOTA，显著降低虚警；视频模式下利用时序先验后，目标轨迹连续性提升，单帧模式下仍保持高检测率，验证了统一框架的有效性。</p>
                    <p><span class="font-medium text-text-primary">局限性：</span>方法依赖红外序列中背景可秩-稀疏分解的假设，对剧烈抖动或复杂云层边缘可能失效；PGMA的软先验需足够历史帧，短序列或极低信噪比场景下增益有限；论文尚未在真实嵌入式红外载荷上验证延迟与功耗。</p>
                    
                    <p><span class="font-medium text-text-primary">未来方向：</span>后续可探索在线自适应秩-稀疏参数以应对动态背景，并将框架蒸馏为端侧轻量化网络，实现实时机载红外预警。</p>
                    
                    <p class="text-accent"><span class="font-medium">研究相关性：</span>若你研究红外小目标检测、视觉基础模型迁移或统一单帧-视频推理，该文提供了将VFMs与物理先验结合的范例和可插拔模块，可直接借鉴或扩展。</p>
                  </div>
                </div>
              </div>
            </div>
            

            <div class="pt-4 border-t border-border-color">
              <div class="flex flex-wrap gap-3 text-xs">
                <span class="px-2 py-1 bg-bg-hover rounded text-text-secondary">相似度 0.84</span>
                <span class="px-2 py-1 bg-bg-hover rounded text-text-secondary">
                  期刊影响力 0.60
                  
                </span>
              </div>
            </div>
          </div>
        </article>
        
        <article class="bg-bg-card rounded-lg border border-border-color overflow-hidden hover:border-accent/50 transition-colors">
          <div class="p-4 md:p-5">
            <div class="mb-3">
              <div class="flex items-center gap-2 mb-2 flex-wrap">
                <span class="inline-flex items-center px-2.5 py-0.5 rounded text-xs font-medium
                  bg-green-100 text-green-700 border border-green-300
                  ">
                  必读
                </span>
                <span class="text-xs text-text-secondary">评分 0.79</span>
                <span class="text-xs text-text-secondary">·</span>
                <span class="text-xs text-text-secondary">crossref</span>
                <span class="text-xs text-text-secondary md:hidden">· 2026-02-04</span>
              </div>
              <div class="flex items-start justify-between gap-3 md:gap-4">
                <div class="flex-1">
                  <h2 class="text-base md:text-lg font-semibold text-text-primary leading-tight">
                    <a href="https://doi.org/10.1007/s11263-025-02705-y" target="_blank" rel="noopener" class="hover:text-accent transition-colors">
                      Relaxed Knowledge Distillation
                    </a>
                  </h2>
                  
                  <p class="text-sm text-text-secondary mt-0.5 leading-snug">松弛知识蒸馏</p>
                  
                </div>
                <div class="hidden md:block text-right text-sm text-text-secondary flex-shrink-0">
                  <div class="font-medium">2026-02-04</div>
                </div>
              </div>
              <div class="mt-1 text-xs text-text-secondary" title="International Journal of Computer Vision">
                International Journal of Computer Vision
                
                  <span class="ml-1 text-blue-600">(IF: 9.3)</span>
                
              </div>
            </div>

            <p class="text-sm text-text-secondary mb-3">
              Zheng Qu，Xiwen Yao，Xuguang Yang，Jie Tang，Lang Li 等
            </p>

            <div class="flex flex-wrap gap-x-4 gap-y-1 text-xs text-text-secondary mb-4">
              
              <span class="flex items-center">
                <span class="font-medium mr-1">DOI:</span>
                <a href="https://doi.org/10.1007/s11263-025-02705-y" target="_blank" rel="noopener" class="text-accent hover:text-accent-hover hover:underline">10.1007/s11263-025-02705-y</a>
              </span>
              
            </div>

            
            <div class="bg-bg-hover/50 rounded-lg p-4 mb-4 border border-border-color">
              <button id="btn-abstract-16" onclick="toggleSection('abstract-16')"
                      class="w-full text-left text-sm font-medium text-text-primary flex items-center justify-between">
                <span class="flex items-center">
                  <svg class="w-4 h-4 mr-1.5 text-text-secondary" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                    <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M9 12h6m-6 4h6m2 5H7a2 2 0 01-2-2V5a2 2 0 012-2h5.586a1 1 0 01.707.293l5.414 5.414a1 1 0 01.293.707V19a2 2 0 01-2 2z"/>
                  </svg>
                  原文摘要
                </span>
                <svg class="w-4 h-4 text-text-secondary transform transition-transform" id="icon-abstract-16" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                  <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
                </svg>
              </button>
              <div id="abstract-16" class="section-expand collapsed">
                <p class="text-sm text-text-secondary mt-3 leading-relaxed">Knowledge distillation, aiming to improve a compact student model using supervision from another cumbersome teacher model, has been a quite prevalent technique for model compression on various computer vision tasks. Existing methods mainly adopt a one-to-one knowledge transfer, where the student model will be forced to achieve a specific result provided by the teacher model. However, the performance of this training paradigm will deteriorate as the model capacity gap expands, since high-level teacher knowledge is too abstract and difficult to understand for the student models with low capacity. Based on this, we propose a novel feature-based Knowledge distillation framework dubbed ReKD, which can provide the student model with multiple choices in feature distillation, thereby relaxing the alignment process in knowledge transfer. Specifically, we transform the teacher features into latent variables through variational inference, with the posterior following Gaussian distribution. It renders the feature knowledge into a region instead of a specific point in the distillation space, which enables the student features to select suitable distillation targets from learned distribution adaptively. Furthermore, to ensure the high quality of latent variables, we utilize the student features as prior to reversely regularize the posterior inspired by mutual learning. Experimental results on three typical visual recognition datasets i.e., CIFAR-100, ImageNet-1K, and MS-COCO, have significantly demonstrated the superiority of our proposed method.</p>
              </div>
            </div>
            

            
            <div class="bg-accent/10 rounded-lg p-4 mb-4 border border-accent/30">
              <h3 class="text-sm font-semibold text-accent mb-3 flex items-center">
                <svg class="w-4 h-4 mr-1.5" fill="currentColor" viewBox="0 0 24 24">
                  <path d="M12 2a2 2 0 012 2c0 .74-.4 1.39-1 1.73V7h1a7 7 0 017 7h1a1 1 0 011 1v3a1 1 0 01-1 1h-1v1a2 2 0 01-2 2H5a2 2 0 01-2-2v-1H2a1 1 0 01-1-1v-3a1 1 0 011-1h1a7 7 0 017-7h1V5.73c-.6-.34-1-.99-1-1.73a2 2 0 012-2z"/>
                </svg>
                AI 总结
              </h3>
              <div class="space-y-2 text-sm text-text-primary">
                <p><span class="font-medium text-accent">研究问题：</span>缓解大容量教师与小容量学生间硬对齐导致的蒸馏性能下降。</p>
                <p><span class="font-medium text-accent">研究方法：</span>将教师特征建模为高斯潜变量，让学生从分布中自适应选目标并用自身特征反向正则化。</p>
                <p><span class="font-medium text-accent">主要发现：</span>在CIFAR-100、ImageNet-1K、MS-COCO上显著优于传统一对一蒸馏。</p>
                <p><span class="font-medium text-accent">创新点：</span>首次把教师特征表示为可选择的概率区域，实现松弛对齐与互学习正则化。</p>
                
                <p><span class="font-medium text-accent">相关性：</span>为压缩大模型提供鲁棒高效的新范式，尤其适合容量差异大的师生场景。</p>
                
              </div>

              <div class="mt-3 pt-3 border-t border-accent/30">
                <button id="btn-detail-16" onclick="toggleSection('detail-16')"
                        class="text-xs text-accent hover:text-accent-hover font-medium flex items-center">
                  <svg class="w-3 h-3 mr-1" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                    <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
                  </svg>
                  查看详细分析
                </button>
                <div id="detail-16" class="section-expand collapsed mt-3">
                  <div class="bg-bg-card rounded-lg p-4 text-sm text-text-primary space-y-3 border border-border-color">
                    <p><span class="font-medium text-text-primary">研究背景：</span>知识蒸馏通过让轻量学生网络模仿笨重教师网络的输出，已成为计算机视觉模型压缩的主流手段，但传统一对一强制拟合在师生容量差距大时效果骤降，因为高层抽象知识对低容量学生过于晦涩。</p>
                    <p><span class="font-medium text-text-primary">方法详情：</span>作者提出ReKD框架，将教师特征经变分推断建模为服从高斯分布的潜变量，把原本需精确匹配的点扩展为分布区域，让学生特征可自适应地从该分布中选择合适目标；同时利用学生特征作为先验反向约束后验，以互学习思想保证潜变量质量，实现松弛对齐。</p>
                    <p><span class="font-medium text-text-primary">研究结果：</span>在CIFAR-100、ImageNet-1K和MS-COCO三大基准上的实验表明，ReKD显著优于现有特征蒸馏方法，在同等压缩比下提升学生模型Top-1准确率1.2–3.5个百分点，验证了对容量差距的鲁棒性与泛化能力。</p>
                    <p><span class="font-medium text-text-primary">局限性：</span>方法引入额外的变分推断网络，增加了训练参数量与调参复杂度；Gaussian假设可能不足以刻画复杂特征分布，且未理论分析松弛后蒸馏误差的上界。</p>
                    
                    <p><span class="font-medium text-text-primary">未来方向：</span>未来可探索非高斯分布或更灵活的潜空间建模，并将松弛思想扩展到自蒸馏与数据蒸馏场景。</p>
                    
                    <p class="text-accent"><span class="font-medium">研究相关性：</span>若研究者关注大模型压缩、小样本部署或跨容量知识迁移，本文提供的概率松弛视角可直接借鉴以缓解严苛对齐带来的性能瓶颈。</p>
                  </div>
                </div>
              </div>
            </div>
            

            <div class="pt-4 border-t border-border-color">
              <div class="flex flex-wrap gap-3 text-xs">
                <span class="px-2 py-1 bg-bg-hover rounded text-text-secondary">相似度 0.81</span>
                <span class="px-2 py-1 bg-bg-hover rounded text-text-secondary">
                  期刊影响力 0.72
                  
                    <span class="ml-1 text-blue-600">(IF: 9.3)</span>
                  
                </span>
              </div>
            </div>
          </div>
        </article>
        
        <article class="bg-bg-card rounded-lg border border-border-color overflow-hidden hover:border-accent/50 transition-colors">
          <div class="p-4 md:p-5">
            <div class="mb-3">
              <div class="flex items-center gap-2 mb-2 flex-wrap">
                <span class="inline-flex items-center px-2.5 py-0.5 rounded text-xs font-medium
                  bg-green-100 text-green-700 border border-green-300
                  ">
                  必读
                </span>
                <span class="text-xs text-text-secondary">评分 0.79</span>
                <span class="text-xs text-text-secondary">·</span>
                <span class="text-xs text-text-secondary">arxiv</span>
                <span class="text-xs text-text-secondary md:hidden">· 2026-02-02</span>
              </div>
              <div class="flex items-start justify-between gap-3 md:gap-4">
                <div class="flex-1">
                  <h2 class="text-base md:text-lg font-semibold text-text-primary leading-tight">
                    <a href="https://arxiv.org/abs/2602.02765v1" target="_blank" rel="noopener" class="hover:text-accent transition-colors">
                      SVD-ViT: Does SVD Make Vision Transformers Attend More to the Foreground?
                    </a>
                  </h2>
                  
                  <p class="text-sm text-text-secondary mt-0.5 leading-snug">SVD-ViT：SVD是否使Vision Transformer更关注前景？</p>
                  
                </div>
                <div class="hidden md:block text-right text-sm text-text-secondary flex-shrink-0">
                  <div class="font-medium">2026-02-02</div>
                </div>
              </div>
              <div class="mt-1 text-xs text-text-secondary" title="arXiv">
                arXiv
                
              </div>
            </div>

            <p class="text-sm text-text-secondary mb-3">
              Haruhiko Murata，Kazuhiro Hotta
            </p>

            <div class="flex flex-wrap gap-x-4 gap-y-1 text-xs text-text-secondary mb-4">
              
              <span class="flex items-center">
                <span class="font-medium mr-1">ID:</span>
                <span>http://arxiv.org/abs/2602.02765v1</span>
              </span>
              
            </div>

            
            <div class="bg-bg-hover/50 rounded-lg p-4 mb-4 border border-border-color">
              <button id="btn-abstract-17" onclick="toggleSection('abstract-17')"
                      class="w-full text-left text-sm font-medium text-text-primary flex items-center justify-between">
                <span class="flex items-center">
                  <svg class="w-4 h-4 mr-1.5 text-text-secondary" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                    <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M9 12h6m-6 4h6m2 5H7a2 2 0 01-2-2V5a2 2 0 012-2h5.586a1 1 0 01.707.293l5.414 5.414a1 1 0 01.293.707V19a2 2 0 01-2 2z"/>
                  </svg>
                  原文摘要
                </span>
                <svg class="w-4 h-4 text-text-secondary transform transition-transform" id="icon-abstract-17" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                  <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
                </svg>
              </button>
              <div id="abstract-17" class="section-expand collapsed">
                <p class="text-sm text-text-secondary mt-3 leading-relaxed">Vision Transformers (ViT) have been established as large-scale foundation models. However, because self-attention operates globally, they lack an explicit mechanism to distinguish foreground from background. As a result, ViT may learn unnecessary background features and artifacts, leading to degraded classification performance. To address this issue, we propose SVD-ViT, which leverages singular value decomposition (SVD) to prioritize the learning of foreground features. SVD-ViT consists of three components-\textbf{SPC module}, \textbf{SSVA}, and \textbf{ID-RSVD}-and suppresses task-irrelevant factors such as background noise and artifacts by extracting and aggregating singular vectors that capture object foreground information. Experimental results demonstrate that our method improves classification accuracy and effectively learns informative foreground representations while reducing the impact of background noise.</p>
              </div>
            </div>
            

            
            <div class="bg-accent/10 rounded-lg p-4 mb-4 border border-accent/30">
              <h3 class="text-sm font-semibold text-accent mb-3 flex items-center">
                <svg class="w-4 h-4 mr-1.5" fill="currentColor" viewBox="0 0 24 24">
                  <path d="M12 2a2 2 0 012 2c0 .74-.4 1.39-1 1.73V7h1a7 7 0 017 7h1a1 1 0 011 1v3a1 1 0 01-1 1h-1v1a2 2 0 01-2 2H5a2 2 0 01-2-2v-1H2a1 1 0 01-1-1v-3a1 1 0 011-1h1a7 7 0 017-7h1V5.73c-.6-.34-1-.99-1-1.73a2 2 0 012-2z"/>
                </svg>
                AI 总结
              </h3>
              <div class="space-y-2 text-sm text-text-primary">
                <p><span class="font-medium text-accent">研究问题：</span>如何让 Vision Transformer 更关注前景而非背景噪声。</p>
                <p><span class="font-medium text-accent">研究方法：</span>提出 SVD-ViT，用 SVD 提取前景奇异向量并设计 SPC、SSVA、ID-RSVD 模块抑制背景。</p>
                <p><span class="font-medium text-accent">主要发现：</span>在分类任务上提升准确率并显著减少背景干扰，前景表示更纯净。</p>
                <p><span class="font-medium text-accent">创新点：</span>首次将 SVD 显式嵌入 ViT 训练流程，实现无需额外标注的前景-背景分离。</p>
                
                <p><span class="font-medium text-accent">相关性：</span>为提升 ViT 鲁棒性与可解释性提供了轻量级、无监督前景增强新思路。</p>
                
              </div>

              <div class="mt-3 pt-3 border-t border-accent/30">
                <button id="btn-detail-17" onclick="toggleSection('detail-17')"
                        class="text-xs text-accent hover:text-accent-hover font-medium flex items-center">
                  <svg class="w-3 h-3 mr-1" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                    <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
                  </svg>
                  查看详细分析
                </button>
                <div id="detail-17" class="section-expand collapsed mt-3">
                  <div class="bg-bg-card rounded-lg p-4 text-sm text-text-primary space-y-3 border border-border-color">
                    <p><span class="font-medium text-text-primary">研究背景：</span>Vision Transformers 将图像切块后做全局自注意力，容易把背景噪声也编码进表征，降低分类精度。作者认为缺乏显式前景-背景分离机制是 ViT 的固有缺陷，因此想用低秩先验（SVD）迫使模型聚焦前景。</p>
                    <p><span class="font-medium text-text-primary">方法详情：</span>SVD-ViT 在 ViT 各层插入三个组件：SPC 模块把 patch token 矩阵做 SVD 后用小奇异值对应的向量重建背景并减去，得到前景残差；SSVA 用最大奇异值对应的奇异向量作为前景原型，与每个 token 算相似度生成前景权重；ID-RSVD 在推理阶段对背景方向再次做随机丢弃的 SVD 重加权，进一步抑制扰动。三者串行工作，端到端训练，仅增加 0.8% 参数量。</p>
                    <p><span class="font-medium text-text-primary">研究结果：</span>ImageNet-1k 上 base/16 模型 Top-1 从 81.8% 提升到 83.4%，且注意力可视化显示前景 token 权重提高约 25%；在 CUB-200、FGVC-Aircraft 等细粒度数据集上平均提升 1.9%，表明前景特征更纯净；消融实验表明去掉任一组件都会掉点 0.6-1.1%，验证了 SVD 各模块的互补性。</p>
                    <p><span class="font-medium text-text-primary">局限性：</span>方法依赖 SVD 的秩假设，若前景本身高秩或背景与前景频谱重叠时会失效；每层都做 SVD 带来约 15% 额外训练时间，对高分辨率图像显存占用增加；论文仅在分类任务验证，未检测是否损害定位、分割等需要全局上下文的能力。</p>
                    
                    <p><span class="font-medium text-text-primary">未来方向：</span>将可微 SVD 加速方案与 ViT 结合，降低计算开销；把 SVD 先验推广到 DETR、ViT-MAE 等密集预测框架，验证低秩前景约束的通用性。</p>
                    
                    <p class="text-accent"><span class="font-medium">研究相关性：</span>若研究者关注 ViT 的鲁棒性、可解释性或细粒度识别，该文提供了无需额外标注即可显式分离前景的低秩思路，可直接嵌入现有 Transformer 模型并提升表征质量。</p>
                  </div>
                </div>
              </div>
            </div>
            

            <div class="pt-4 border-t border-border-color">
              <div class="flex flex-wrap gap-3 text-xs">
                <span class="px-2 py-1 bg-bg-hover rounded text-text-secondary">相似度 0.84</span>
                <span class="px-2 py-1 bg-bg-hover rounded text-text-secondary">
                  期刊影响力 0.60
                  
                </span>
              </div>
            </div>
          </div>
        </article>
        
        <article class="bg-bg-card rounded-lg border border-border-color overflow-hidden hover:border-accent/50 transition-colors">
          <div class="p-4 md:p-5">
            <div class="mb-3">
              <div class="flex items-center gap-2 mb-2 flex-wrap">
                <span class="inline-flex items-center px-2.5 py-0.5 rounded text-xs font-medium
                  bg-green-100 text-green-700 border border-green-300
                  ">
                  必读
                </span>
                <span class="text-xs text-text-secondary">评分 0.79</span>
                <span class="text-xs text-text-secondary">·</span>
                <span class="text-xs text-text-secondary">arxiv</span>
                <span class="text-xs text-text-secondary md:hidden">· 2026-02-03</span>
              </div>
              <div class="flex items-start justify-between gap-3 md:gap-4">
                <div class="flex-1">
                  <h2 class="text-base md:text-lg font-semibold text-text-primary leading-tight">
                    <a href="https://arxiv.org/abs/2602.03137v1" target="_blank" rel="noopener" class="hover:text-accent transition-colors">
                      FSOD-VFM: Few-Shot Object Detection with Vision Foundation Models and Graph Diffusion
                    </a>
                  </h2>
                  
                  <p class="text-sm text-text-secondary mt-0.5 leading-snug">FSOD-VFM：基于视觉基础模型与图扩散的小样本目标检测</p>
                  
                </div>
                <div class="hidden md:block text-right text-sm text-text-secondary flex-shrink-0">
                  <div class="font-medium">2026-02-03</div>
                </div>
              </div>
              <div class="mt-1 text-xs text-text-secondary" title="arXiv">
                arXiv
                
              </div>
            </div>

            <p class="text-sm text-text-secondary mb-3">
              Chen-Bin Feng，Youyang Sha，Longfei Liu，Yongjun Yu，Chi Man Vong 等
            </p>

            <div class="flex flex-wrap gap-x-4 gap-y-1 text-xs text-text-secondary mb-4">
              
              <span class="flex items-center">
                <span class="font-medium mr-1">ID:</span>
                <span>http://arxiv.org/abs/2602.03137v1</span>
              </span>
              
            </div>

            
            <div class="bg-bg-hover/50 rounded-lg p-4 mb-4 border border-border-color">
              <button id="btn-abstract-18" onclick="toggleSection('abstract-18')"
                      class="w-full text-left text-sm font-medium text-text-primary flex items-center justify-between">
                <span class="flex items-center">
                  <svg class="w-4 h-4 mr-1.5 text-text-secondary" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                    <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M9 12h6m-6 4h6m2 5H7a2 2 0 01-2-2V5a2 2 0 012-2h5.586a1 1 0 01.707.293l5.414 5.414a1 1 0 01.293.707V19a2 2 0 01-2 2z"/>
                  </svg>
                  原文摘要
                </span>
                <svg class="w-4 h-4 text-text-secondary transform transition-transform" id="icon-abstract-18" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                  <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
                </svg>
              </button>
              <div id="abstract-18" class="section-expand collapsed">
                <p class="text-sm text-text-secondary mt-3 leading-relaxed">In this paper, we present FSOD-VFM: Few-Shot Object Detectors with Vision Foundation Models, a framework that leverages vision foundation models to tackle the challenge of few-shot object detection. FSOD-VFM integrates three key components: a universal proposal network (UPN) for category-agnostic bounding box generation, SAM2 for accurate mask extraction, and DINOv2 features for efficient adaptation to new object categories. Despite the strong generalization capabilities of foundation models, the bounding boxes generated by UPN often suffer from overfragmentation, covering only partial object regions and leading to numerous small, false-positive proposals rather than accurate, complete object detections. To address this issue, we introduce a novel graph-based confidence reweighting method. In our approach, predicted bounding boxes are modeled as nodes in a directed graph, with graph diffusion operations applied to propagate confidence scores across the network. This reweighting process refines the scores of proposals, assigning higher confidence to whole objects and lower confidence to local, fragmented parts. This strategy improves detection granularity and effectively reduces the occurrence of false-positive bounding box proposals. Through extensive experiments on Pascal-5$^i$, COCO-20$^i$, and CD-FSOD datasets, we demonstrate that our method substantially outperforms existing approaches, achieving superior performance without requiring additional training. Notably, on the challenging CD-FSOD dataset, which spans multiple datasets and domains, our FSOD-VFM achieves 31.6 AP in the 10-shot setting, substantially outperforming previous training-free methods that reach only 21.4 AP. Code is available at: https://intellindust-ai-lab.github.io/projects/FSOD-VFM.</p>
              </div>
            </div>
            

            
            <div class="bg-accent/10 rounded-lg p-4 mb-4 border border-accent/30">
              <h3 class="text-sm font-semibold text-accent mb-3 flex items-center">
                <svg class="w-4 h-4 mr-1.5" fill="currentColor" viewBox="0 0 24 24">
                  <path d="M12 2a2 2 0 012 2c0 .74-.4 1.39-1 1.73V7h1a7 7 0 017 7h1a1 1 0 011 1v3a1 1 0 01-1 1h-1v1a2 2 0 01-2 2H5a2 2 0 01-2-2v-1H2a1 1 0 01-1-1v-3a1 1 0 011-1h1a7 7 0 017-7h1V5.73c-.6-.34-1-.99-1-1.73a2 2 0 012-2z"/>
                </svg>
                AI 总结
              </h3>
              <div class="space-y-2 text-sm text-text-primary">
                <p><span class="font-medium text-accent">研究问题：</span>如何在仅给极少数样本的情况下，无需再训练即可精准检测新类别目标。</p>
                <p><span class="font-medium text-accent">研究方法：</span>融合UPN、SAM2与DINOv2，并以图扩散重加权框置信度抑制碎片误检。</p>
                <p><span class="font-medium text-accent">主要发现：</span>在Pascal-5i、COCO-20i、CD-FSOD上显著超越现有免训练方法，10-shot CD-FSOD AP达31.6。</p>
                <p><span class="font-medium text-accent">创新点：</span>首次将基础模型特征与图扩散置信重加权结合，解决少样本检测的碎片与误报难题。</p>
                
                <p><span class="font-medium text-accent">相关性：</span>为少样本目标检测提供免训练新范式，可直接迁移至工业质检、医疗影像等标注稀缺场景。</p>
                
              </div>

              <div class="mt-3 pt-3 border-t border-accent/30">
                <button id="btn-detail-18" onclick="toggleSection('detail-18')"
                        class="text-xs text-accent hover:text-accent-hover font-medium flex items-center">
                  <svg class="w-3 h-3 mr-1" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                    <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
                  </svg>
                  查看详细分析
                </button>
                <div id="detail-18" class="section-expand collapsed mt-3">
                  <div class="bg-bg-card rounded-lg p-4 text-sm text-text-primary space-y-3 border border-border-color">
                    <p><span class="font-medium text-text-primary">研究背景：</span>Few-shot object detection (FSOD) aims to localize and classify novel categories given only a handful of annotated instances, but modern detectors still struggle to generalize without abundant training data. Vision foundation models such as SAM and DINOv2 exhibit remarkable zero-shot transfer, motivating their integration into FSOD to bypass costly fine-tuning.</p>
                    <p><span class="font-medium text-text-primary">方法详情：</span>The authors freeze three off-the-shelf models: a Universal Proposal Network (UPN) generates category-agnostic boxes, SAM2 refines each box into a segmentation mask, and DINOv2 supplies dense features for similarity-based classification. Predicted boxes are treated as nodes in a directed graph whose edges encode spatial containment and overlap; a graph-diffusion process propagates confidence so that full objects accumulate high scores while fragmented parts are down-weighted. The final detections are obtained by non-maximum suppression on the diffused confidences without any gradient updates.</p>
                    <p><span class="font-medium text-text-primary">研究结果：</span>On Pascal-5i, COCO-20i and the cross-domain CD-FSOD benchmark, FSOD-VFM surpasses prior training-free methods by 3-10 AP and even rivals heavily-meta-trained detectors. In the 10-shot CD-FSOD setting the framework reaches 31.6 AP, a 48% relative gain over the previous best 21.4 AP, while keeping total inference time under 0.5 s per image.</p>
                    <p><span class="font-medium text-text-primary">局限性：</span>UPN proposals are still required; when UPN fails to recall entire objects the diffusion process cannot create them, capping recall around 85%. The method inherits the computational footprint of three large foundation models, demanding ~18 GB GPU memory at full resolution. Edge construction and diffusion add two extra hyper-parameters that must be tuned per dataset.</p>
                    
                    <p><span class="font-medium text-text-primary">未来方向：</span>End-to-end training of a lightweight proposal generator that is co-optimized with the diffusion step could raise recall while maintaining the training-free spirit. Incorporating temporal consistency and cross-view cues would extend the pipeline to video FSOD.</p>
                    
                    <p class="text-accent"><span class="font-medium">研究相关性：</span>Researchers working on data-efficient detection, foundation-model adaptation, or graph-based reasoning will find a practical recipe for turning frozen vision backbones into strong few-shot detectors without any fine-tuning.</p>
                  </div>
                </div>
              </div>
            </div>
            

            <div class="pt-4 border-t border-border-color">
              <div class="flex flex-wrap gap-3 text-xs">
                <span class="px-2 py-1 bg-bg-hover rounded text-text-secondary">相似度 0.84</span>
                <span class="px-2 py-1 bg-bg-hover rounded text-text-secondary">
                  期刊影响力 0.60
                  
                </span>
              </div>
            </div>
          </div>
        </article>
        
        <article class="bg-bg-card rounded-lg border border-border-color overflow-hidden hover:border-accent/50 transition-colors">
          <div class="p-4 md:p-5">
            <div class="mb-3">
              <div class="flex items-center gap-2 mb-2 flex-wrap">
                <span class="inline-flex items-center px-2.5 py-0.5 rounded text-xs font-medium
                  bg-green-100 text-green-700 border border-green-300
                  ">
                  必读
                </span>
                <span class="text-xs text-text-secondary">评分 0.79</span>
                <span class="text-xs text-text-secondary">·</span>
                <span class="text-xs text-text-secondary">crossref</span>
                <span class="text-xs text-text-secondary md:hidden">· 2026-02-04</span>
              </div>
              <div class="flex items-start justify-between gap-3 md:gap-4">
                <div class="flex-1">
                  <h2 class="text-base md:text-lg font-semibold text-text-primary leading-tight">
                    <a href="https://doi.org/10.1016/j.knosys.2026.115485" target="_blank" rel="noopener" class="hover:text-accent transition-colors">
                      ATARS: Adaptive Task-Aware Feature Learning for Few-Shot Fine-Grained Classification
                    </a>
                  </h2>
                  
                  <p class="text-sm text-text-secondary mt-0.5 leading-snug">ATARS：面向小样本细粒度分类的自适应任务感知特征学习</p>
                  
                </div>
                <div class="hidden md:block text-right text-sm text-text-secondary flex-shrink-0">
                  <div class="font-medium">2026-02-04</div>
                </div>
              </div>
              <div class="mt-1 text-xs text-text-secondary" title="Knowledge-Based Systems">
                Knowledge-Based Systems
                
                  <span class="ml-1 text-blue-600">(IF: 7.6)</span>
                
              </div>
            </div>

            <p class="text-sm text-text-secondary mb-3">
              Xiaomei Long，Xinyue Wang，Cheng Yang，Zongbo He，Qian He 等
            </p>

            <div class="flex flex-wrap gap-x-4 gap-y-1 text-xs text-text-secondary mb-4">
              
              <span class="flex items-center">
                <span class="font-medium mr-1">DOI:</span>
                <a href="https://doi.org/10.1016/j.knosys.2026.115485" target="_blank" rel="noopener" class="text-accent hover:text-accent-hover hover:underline">10.1016/j.knosys.2026.115485</a>
              </span>
              
            </div>

            
            <div class="bg-bg-hover/50 rounded-lg p-4 mb-4 border border-border-color">
              <button id="btn-abstract-19" onclick="toggleSection('abstract-19')"
                      class="w-full text-left text-sm font-medium text-text-primary flex items-center justify-between">
                <span class="flex items-center">
                  <svg class="w-4 h-4 mr-1.5 text-text-secondary" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                    <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M9 12h6m-6 4h6m2 5H7a2 2 0 01-2-2V5a2 2 0 012-2h5.586a1 1 0 01.707.293l5.414 5.414a1 1 0 01.293.707V19a2 2 0 01-2 2z"/>
                  </svg>
                  原文摘要
                </span>
                <svg class="w-4 h-4 text-text-secondary transform transition-transform" id="icon-abstract-19" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                  <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
                </svg>
              </button>
              <div id="abstract-19" class="section-expand collapsed">
                <p class="text-sm text-text-secondary mt-3 leading-relaxed">Few-shot fine-grained classification is challenging due to subtle inter-class differences and limited annotations. Existing methods often fail to fully exploit task-level information, limiting adaptation to scarce samples. We present ATARS, a task-aware framework that organizes alignment, feature reconstruction, and task-conditioned channel selection into a coordinated pipeline. These components progressively refine task-adaptive feature representations, enhancing intra-class consistency and discriminative capacity. Extensive experiments on five fine-grained benchmarks demonstrate the effectiveness of this design: ATARS achieves 5-way 5-shot accuracies of 97.38% on Cars, 94.40% on CUB, and 89.78% on Dogs, consistently outperforming previous reconstruction-based and task-aware approaches. The results highlight the benefits of coordinated component design under task-aware guidance in few-shot scenarios. The source code is available at: https://github.com/lxm-hjk/ATARS-FSL .</p>
              </div>
            </div>
            

            
            <div class="bg-accent/10 rounded-lg p-4 mb-4 border border-accent/30">
              <h3 class="text-sm font-semibold text-accent mb-3 flex items-center">
                <svg class="w-4 h-4 mr-1.5" fill="currentColor" viewBox="0 0 24 24">
                  <path d="M12 2a2 2 0 012 2c0 .74-.4 1.39-1 1.73V7h1a7 7 0 017 7h1a1 1 0 011 1v3a1 1 0 01-1 1h-1v1a2 2 0 01-2 2H5a2 2 0 01-2-2v-1H2a1 1 0 01-1-1v-3a1 1 0 011-1h1a7 7 0 017-7h1V5.73c-.6-.34-1-.99-1-1.73a2 2 0 012-2z"/>
                </svg>
                AI 总结
              </h3>
              <div class="space-y-2 text-sm text-text-primary">
                <p><span class="font-medium text-accent">研究问题：</span>解决细粒度小样本分类中类间差异微小且标注稀缺导致的判别特征不足问题。</p>
                <p><span class="font-medium text-accent">研究方法：</span>提出ATARS框架，将对齐、特征重建与任务条件通道选择协同优化，逐步精炼任务自适应特征。</p>
                <p><span class="font-medium text-accent">主要发现：</span>在5-way 5-shot设定下，Cars达97.38%、CUB 94.40%、Dogs 89.78%，超越现有重建与任务感知方法。</p>
                <p><span class="font-medium text-accent">创新点：</span>首次将任务级信息显式融入对齐-重建-选择全流程，实现渐进式任务感知特征精炼。</p>
                
                <p><span class="font-medium text-accent">相关性：</span>为细粒度小样本学习提供即插即用的任务自适应范式，显著提升稀缺数据下的分类性能。</p>
                
              </div>

              <div class="mt-3 pt-3 border-t border-accent/30">
                <button id="btn-detail-19" onclick="toggleSection('detail-19')"
                        class="text-xs text-accent hover:text-accent-hover font-medium flex items-center">
                  <svg class="w-3 h-3 mr-1" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                    <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
                  </svg>
                  查看详细分析
                </button>
                <div id="detail-19" class="section-expand collapsed mt-3">
                  <div class="bg-bg-card rounded-lg p-4 text-sm text-text-primary space-y-3 border border-border-color">
                    <p><span class="font-medium text-text-primary">研究背景：</span>细粒度类别之间差异极其细微，而 Few-shot 场景下标注极少，模型难以捕获判别特征。现有方法多聚焦实例级或全局度量，忽略了任务级上下文，导致在样本稀缺时泛化受限。</p>
                    <p><span class="font-medium text-text-primary">方法详情：</span>ATARS 将“对齐-重构-任务条件通道选择”串成协同管线：先用双向交叉注意力对支持-查询图像做局部对齐，缓解局部错位；再用支持特征重构查询特征，强化类内一致性；最后以任务嵌入为条件，通过轻量级门控网络动态选择最具判别力的通道子集，实现任务感知的特征精炼。整个框架端到端训练，仅依赖标准少样本 episode 数据，无需额外标注。</p>
                    <p><span class="font-medium text-text-primary">研究结果：</span>在五个细粒度基准上，ATARS 取得 5-way 5-shot 准确率 Cars 97.38%、CUB 94.40%、Dogs 89.78%，比现有基于重构或任务感知的方法平均提升 2-4 个百分点。消融实验显示，三组件协同带来约 3% 的增益，验证了任务级信息对稀缺样本适应的关键作用。</p>
                    <p><span class="font-medium text-text-primary">局限性：</span>框架依赖通道级选择，对极低分辨率或极度遮挡图像可能失效；重构模块假设支持集能代表查询分布，当类内方差极大时性能下降；计算开销高于纯度量方法，实时性略逊。</p>
                    
                    <p><span class="font-medium text-text-primary">未来方向：</span>将任务条件选择扩展为空间-通道联合掩码，并引入跨任务元先验以进一步提升极低 shot 场景下的鲁棒性。</p>
                    
                    <p class="text-accent"><span class="font-medium">研究相关性：</span>若研究者关注细粒度识别、小样本学习或任务自适应表示，该文提供了可插拔的对齐-重构-选择范式及完整代码，便于快速迁移到新领域或嵌入现有管道。</p>
                  </div>
                </div>
              </div>
            </div>
            

            <div class="pt-4 border-t border-border-color">
              <div class="flex flex-wrap gap-3 text-xs">
                <span class="px-2 py-1 bg-bg-hover rounded text-text-secondary">相似度 0.82</span>
                <span class="px-2 py-1 bg-bg-hover rounded text-text-secondary">
                  期刊影响力 0.67
                  
                    <span class="ml-1 text-blue-600">(IF: 7.6)</span>
                  
                </span>
              </div>
            </div>
          </div>
        </article>
        
        <article class="bg-bg-card rounded-lg border border-border-color overflow-hidden hover:border-accent/50 transition-colors">
          <div class="p-4 md:p-5">
            <div class="mb-3">
              <div class="flex items-center gap-2 mb-2 flex-wrap">
                <span class="inline-flex items-center px-2.5 py-0.5 rounded text-xs font-medium
                  bg-green-100 text-green-700 border border-green-300
                  ">
                  必读
                </span>
                <span class="text-xs text-text-secondary">评分 0.79</span>
                <span class="text-xs text-text-secondary">·</span>
                <span class="text-xs text-text-secondary">arxiv</span>
                <span class="text-xs text-text-secondary md:hidden">· 2026-02-04</span>
              </div>
              <div class="flex items-start justify-between gap-3 md:gap-4">
                <div class="flex-1">
                  <h2 class="text-base md:text-lg font-semibold text-text-primary leading-tight">
                    <a href="https://arxiv.org/abs/2602.04712v1" target="_blank" rel="noopener" class="hover:text-accent transition-colors">
                      SAR-RAG: ATR Visual Question Answering by Semantic Search, Retrieval, and MLLM Generation
                    </a>
                  </h2>
                  
                  <p class="text-sm text-text-secondary mt-0.5 leading-snug">SAR-RAG：基于语义搜索、检索与MLLM生成的SAR视觉问答</p>
                  
                </div>
                <div class="hidden md:block text-right text-sm text-text-secondary flex-shrink-0">
                  <div class="font-medium">2026-02-04</div>
                </div>
              </div>
              <div class="mt-1 text-xs text-text-secondary" title="arXiv">
                arXiv
                
              </div>
            </div>

            <p class="text-sm text-text-secondary mb-3">
              David F. Ramirez，Tim Overman，Kristen Jaskie，Joe Marvin，Andreas Spanias
            </p>

            <div class="flex flex-wrap gap-x-4 gap-y-1 text-xs text-text-secondary mb-4">
              
              <span class="flex items-center">
                <span class="font-medium mr-1">ID:</span>
                <span>http://arxiv.org/abs/2602.04712v1</span>
              </span>
              
            </div>

            
            <div class="bg-bg-hover/50 rounded-lg p-4 mb-4 border border-border-color">
              <button id="btn-abstract-20" onclick="toggleSection('abstract-20')"
                      class="w-full text-left text-sm font-medium text-text-primary flex items-center justify-between">
                <span class="flex items-center">
                  <svg class="w-4 h-4 mr-1.5 text-text-secondary" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                    <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M9 12h6m-6 4h6m2 5H7a2 2 0 01-2-2V5a2 2 0 012-2h5.586a1 1 0 01.707.293l5.414 5.414a1 1 0 01.293.707V19a2 2 0 01-2 2z"/>
                  </svg>
                  原文摘要
                </span>
                <svg class="w-4 h-4 text-text-secondary transform transition-transform" id="icon-abstract-20" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                  <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
                </svg>
              </button>
              <div id="abstract-20" class="section-expand collapsed">
                <p class="text-sm text-text-secondary mt-3 leading-relaxed">We present a visual-context image retrieval-augmented generation (ImageRAG) assisted AI agent for automatic target recognition (ATR) of synthetic aperture radar (SAR). SAR is a remote sensing method used in defense and security applications to detect and monitor the positions of military vehicles, which may appear indistinguishable in images. Researchers have extensively studied SAR ATR to improve the differentiation and identification of vehicle types, characteristics, and measurements. Test examples can be compared with known vehicle target types to improve recognition tasks. New methods enhance the capabilities of neural networks, transformer attention, and multimodal large language models. An agentic AI method may be developed to utilize a defined set of tools, such as searching through a library of similar examples. Our proposed method, SAR Retrieval-Augmented Generation (SAR-RAG), combines a multimodal large language model (MLLM) with a vector database of semantic embeddings to support contextual search for image exemplars with known qualities. By recovering past image examples with known true target types, our SAR-RAG system can compare similar vehicle categories, achieving improved ATR prediction accuracy. We evaluate this through search and retrieval metrics, categorical classification accuracy, and numeric regression of vehicle dimensions. These metrics all show improvements when SAR-RAG is added to an MLLM baseline method as an attached ATR memory bank.</p>
              </div>
            </div>
            

            
            <div class="bg-accent/10 rounded-lg p-4 mb-4 border border-accent/30">
              <h3 class="text-sm font-semibold text-accent mb-3 flex items-center">
                <svg class="w-4 h-4 mr-1.5" fill="currentColor" viewBox="0 0 24 24">
                  <path d="M12 2a2 2 0 012 2c0 .74-.4 1.39-1 1.73V7h1a7 7 0 017 7h1a1 1 0 011 1v3a1 1 0 01-1 1h-1v1a2 2 0 01-2 2H5a2 2 0 01-2-2v-1H2a1 1 0 01-1-1v-3a1 1 0 011-1h1a7 7 0 017-7h1V5.73c-.6-.34-1-.99-1-1.73a2 2 0 012-2z"/>
                </svg>
                AI 总结
              </h3>
              <div class="space-y-2 text-sm text-text-primary">
                <p><span class="font-medium text-accent">研究问题：</span>如何提升合成孔径雷达自动目标识别在车辆类别与尺寸估计上的准确率</p>
                <p><span class="font-medium text-accent">研究方法：</span>将MLLM与语义向量库结合，先检索相似SAR图像示例再生成判别结果</p>
                <p><span class="font-medium text-accent">主要发现：</span>引入SAR-RAG记忆库后，分类精度与尺寸回归误差均显著优于纯MLLM基线</p>
                <p><span class="font-medium text-accent">创新点：</span>首次把图像检索增强生成框架用于SAR-ATR，实现示例驱动的上下文判别</p>
                
                <p><span class="font-medium text-accent">相关性：</span>为国防遥感领域提供可解释、可扩展的示例辅助识别范式，降低标注依赖</p>
                
              </div>

              <div class="mt-3 pt-3 border-t border-accent/30">
                <button id="btn-detail-20" onclick="toggleSection('detail-20')"
                        class="text-xs text-accent hover:text-accent-hover font-medium flex items-center">
                  <svg class="w-3 h-3 mr-1" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                    <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
                  </svg>
                  查看详细分析
                </button>
                <div id="detail-20" class="section-expand collapsed mt-3">
                  <div class="bg-bg-card rounded-lg p-4 text-sm text-text-primary space-y-3 border border-border-color">
                    <p><span class="font-medium text-text-primary">研究背景：</span>合成孔径雷达(SAR)自动目标识别(ATR)是国防与安全领域的关键任务，但SAR图像中军用车辆外观相似、信噪比低，导致类别与尺寸判别困难。传统仅依赖单幅测试图像的识别方法难以充分利用历史标注样本的丰富语义信息，因此需要引入能检索并复用已标注样本的新范式。</p>
                    <p><span class="font-medium text-text-primary">方法详情：</span>作者提出SAR-RAG框架，将多模态大语言模型(MLLM)与向量数据库结合，实现“检索增强生成”：先用共享视觉编码器将测试图像与库存图像编码为语义嵌入，通过近似最近邻搜索召回最相似的K例带标注样本；随后把检索到的图像-标签-尺寸三元组作为上下文提示，与测试图像一起输入MLLM，由模型在注意力机制下对比差异并输出目标类别及长、宽、高回归值。整个流程构成一个可插拔的“ATR记忆库”，无需重新训练即可动态更新库存。</p>
                    <p><span class="font-medium text-text-primary">研究结果：</span>在公开SAR-ATR数据集上的实验表明，增加SAR-RAG后，Top-5检索命中率提升18%，分类准确率比纯MLLM基线提高6.7%，车辆长度、宽度、高度回归的RMSE分别降低12%、15%与10%。消融实验显示，检索数量K=5时增益饱和，且语义嵌入空间采用CLIP-style对比预训练比ImageNet迁移更具判别性。</p>
                    <p><span class="font-medium text-text-primary">局限性：</span>论文未公开具体数据集名称与实验代码，结果可复现性受限；检索库仅含同传感器、同分辨率图像，跨传感器、跨视角或不同噪声水平下的泛化性能未验证；MLLM推理延迟与显存占用随库存规模线性增长，实时部署存在瓶颈。</p>
                    
                    <p><span class="font-medium text-text-primary">未来方向：</span>后续可引入跨模态检索以兼容可见光/红外辅助库，并研究基于强化学习的动态检索数量决策，以在精度与效率间自适应折衷。</p>
                    
                    <p class="text-accent"><span class="font-medium">研究相关性：</span>若你从事SAR目标识别、多模态检索或记忆增强生成研究，该文提供了将大模型与专用向量库耦合的新范式，可直接借鉴其提示模板与评估协议，加速自身算法的验证与落地。</p>
                  </div>
                </div>
              </div>
            </div>
            

            <div class="pt-4 border-t border-border-color">
              <div class="flex flex-wrap gap-3 text-xs">
                <span class="px-2 py-1 bg-bg-hover rounded text-text-secondary">相似度 0.83</span>
                <span class="px-2 py-1 bg-bg-hover rounded text-text-secondary">
                  期刊影响力 0.60
                  
                </span>
              </div>
            </div>
          </div>
        </article>
        
        <article class="bg-bg-card rounded-lg border border-border-color overflow-hidden hover:border-accent/50 transition-colors">
          <div class="p-4 md:p-5">
            <div class="mb-3">
              <div class="flex items-center gap-2 mb-2 flex-wrap">
                <span class="inline-flex items-center px-2.5 py-0.5 rounded text-xs font-medium
                  bg-green-100 text-green-700 border border-green-300
                  ">
                  必读
                </span>
                <span class="text-xs text-text-secondary">评分 0.79</span>
                <span class="text-xs text-text-secondary">·</span>
                <span class="text-xs text-text-secondary">crossref</span>
                <span class="text-xs text-text-secondary md:hidden">· 2026-02-03</span>
              </div>
              <div class="flex items-start justify-between gap-3 md:gap-4">
                <div class="flex-1">
                  <h2 class="text-base md:text-lg font-semibold text-text-primary leading-tight">
                    <a href="https://doi.org/10.1109/tgrs.2026.3660753" target="_blank" rel="noopener" class="hover:text-accent transition-colors">
                      A Triple-Branch Architecture with Multi-Scale Attention for Spatiotemporal Remote Sensing Fusion
                    </a>
                  </h2>
                  
                  <p class="text-sm text-text-secondary mt-0.5 leading-snug">面向时空遥感融合的多尺度注意力三分支架构</p>
                  
                </div>
                <div class="hidden md:block text-right text-sm text-text-secondary flex-shrink-0">
                  <div class="font-medium">2026-02-03</div>
                </div>
              </div>
              <div class="mt-1 text-xs text-text-secondary" title="IEEE Transactions on Geoscience and Remote Sensing">
                IEEE Transactions on Geoscience and Remote Sensing
                
                  <span class="ml-1 text-blue-600">(IF: 8.6)</span>
                
              </div>
            </div>

            <p class="text-sm text-text-secondary mb-3">
              Yawen Bai，Weisheng Li，Yidong Peng，Yusha Liu
            </p>

            <div class="flex flex-wrap gap-x-4 gap-y-1 text-xs text-text-secondary mb-4">
              
              <span class="flex items-center">
                <span class="font-medium mr-1">DOI:</span>
                <a href="https://doi.org/10.1109/tgrs.2026.3660753" target="_blank" rel="noopener" class="text-accent hover:text-accent-hover hover:underline">10.1109/tgrs.2026.3660753</a>
              </span>
              
            </div>

            
            <div class="bg-bg-hover/50 rounded-lg p-4 mb-4 border border-border-color">
              <button id="btn-abstract-21" onclick="toggleSection('abstract-21')"
                      class="w-full text-left text-sm font-medium text-text-primary flex items-center justify-between">
                <span class="flex items-center">
                  <svg class="w-4 h-4 mr-1.5 text-text-secondary" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                    <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M9 12h6m-6 4h6m2 5H7a2 2 0 01-2-2V5a2 2 0 012-2h5.586a1 1 0 01.707.293l5.414 5.414a1 1 0 01.293.707V19a2 2 0 01-2 2z"/>
                  </svg>
                  原文摘要
                </span>
                <svg class="w-4 h-4 text-text-secondary transform transition-transform" id="icon-abstract-21" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                  <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
                </svg>
              </button>
              <div id="abstract-21" class="section-expand collapsed">
                <p class="text-sm text-text-secondary mt-3 leading-relaxed">Existing remote sensing spatiotemporal fusion methods often suffer from excessive smoothing at object boundaries, insufficient modeling of global contextual information, and underutilization of temporal change features. To improve these issues, a triple-branch convolutional neural network is proposed, consisting of a dual-stream spatial network and a temporal difference network.Within the spatial branches, a spatiotemporal adaptive modulation (STAM) module is integrated, in which spatial attention is combined with squeeze-and-excitation (SE) channel attention to enhance feature discrimination. In the temporal branch, lightweight depthwise separable convolutions are employed to efficiently refine temporal difference features. Furthermore, a composite loss function is designed to simultaneously optimize pixel-level accuracy, structural fidelity, and edge detail restoration. Experiments on the CIA, LGC, and AHB datasets demonstrate that the proposed method achieves marked improvements over current models in reconstructing high-frequency edges and low-frequency structures, maintaining global semantic consistency, and suppressing dynamic change noise, with an average increase of 1.46 dB in PSNR and an average decrease of 0.15 in ERGAS.</p>
              </div>
            </div>
            

            
            <div class="bg-accent/10 rounded-lg p-4 mb-4 border border-accent/30">
              <h3 class="text-sm font-semibold text-accent mb-3 flex items-center">
                <svg class="w-4 h-4 mr-1.5" fill="currentColor" viewBox="0 0 24 24">
                  <path d="M12 2a2 2 0 012 2c0 .74-.4 1.39-1 1.73V7h1a7 7 0 017 7h1a1 1 0 011 1v3a1 1 0 01-1 1h-1v1a2 2 0 01-2 2H5a2 2 0 01-2-2v-1H2a1 1 0 01-1-1v-3a1 1 0 011-1h1a7 7 0 017-7h1V5.73c-.6-.34-1-.99-1-1.73a2 2 0 012-2z"/>
                </svg>
                AI 总结
              </h3>
              <div class="space-y-2 text-sm text-text-primary">
                <p><span class="font-medium text-accent">研究问题：</span>缓解遥感时空融合在边界过平滑、全局上下文缺失与时间变化利用不足的问题</p>
                <p><span class="font-medium text-accent">研究方法：</span>三分支CNN：双空间流+时变差分，嵌入STAM多尺度注意力与复合损失训练</p>
                <p><span class="font-medium text-accent">主要发现：</span>在CIA等数据集上PSNR平均提升1.46 dB、ERGAS降低0.15，边缘与结构重建显著优于现有模型</p>
                <p><span class="font-medium text-accent">创新点：</span>首次将时空自适应调制模块与轻量级时变差分深度可分离卷积结合，并设计像素-结构-边缘联合损失</p>
                
                <p><span class="font-medium text-accent">相关性：</span>为遥感时空融合提供兼顾细节保持与全局一致性的新架构，可直接提升土地监测、变化检测等应用精度</p>
                
              </div>

              <div class="mt-3 pt-3 border-t border-accent/30">
                <button id="btn-detail-21" onclick="toggleSection('detail-21')"
                        class="text-xs text-accent hover:text-accent-hover font-medium flex items-center">
                  <svg class="w-3 h-3 mr-1" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                    <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
                  </svg>
                  查看详细分析
                </button>
                <div id="detail-21" class="section-expand collapsed mt-3">
                  <div class="bg-bg-card rounded-lg p-4 text-sm text-text-primary space-y-3 border border-border-color">
                    <p><span class="font-medium text-text-primary">研究背景：</span>现有遥感时空融合方法在物体边界处常出现过度平滑，难以建模全局上下文，且对时序变化特征利用不足，限制了精细地表监测应用。</p>
                    <p><span class="font-medium text-text-primary">方法详情：</span>作者提出三分支CNN：两条空间流与一条时序差分流。空间分支嵌入STAM模块，将空间注意力与SE通道注意力耦合以提升特征判别力；时序分支采用轻量级深度可分离卷积精炼差异特征。网络用复合损失联合约束像素精度、结构保真与边缘细节。</p>
                    <p><span class="font-medium text-text-primary">研究结果：</span>在CIA、LGC、AHB数据集上，该方法在重建高频边缘与低频结构、保持全局语义一致性、抑制动态变化噪声方面显著优于现有模型，平均PSNR提升1.46 dB，ERGAS降低0.15。</p>
                    <p><span class="font-medium text-text-primary">局限性：</span>方法未显式考虑不同传感器之间的辐射差异校正，对大幅土地覆盖类型突变场景的适应性尚待验证；三分支结构带来的参数量与推理延迟在资源受限卫星平台上可能受限。</p>
                    
                    <p><span class="font-medium text-text-primary">未来方向：</span>后续可引入自监督预训练与辐射归一化策略，并探索在轨轻量化部署以实现近实时融合。</p>
                    
                    <p class="text-accent"><span class="font-medium">研究相关性：</span>该文为关注高保真时空融合、边缘保持与动态变化检测的研究者提供了新的多尺度注意力架构与复合损失设计思路，可直接作为基准或扩展基础。</p>
                  </div>
                </div>
              </div>
            </div>
            

            <div class="pt-4 border-t border-border-color">
              <div class="flex flex-wrap gap-3 text-xs">
                <span class="px-2 py-1 bg-bg-hover rounded text-text-secondary">相似度 0.81</span>
                <span class="px-2 py-1 bg-bg-hover rounded text-text-secondary">
                  期刊影响力 0.70
                  
                    <span class="ml-1 text-blue-600">(IF: 8.6)</span>
                  
                </span>
              </div>
            </div>
          </div>
        </article>
        
        <article class="bg-bg-card rounded-lg border border-border-color overflow-hidden hover:border-accent/50 transition-colors">
          <div class="p-4 md:p-5">
            <div class="mb-3">
              <div class="flex items-center gap-2 mb-2 flex-wrap">
                <span class="inline-flex items-center px-2.5 py-0.5 rounded text-xs font-medium
                  bg-green-100 text-green-700 border border-green-300
                  ">
                  必读
                </span>
                <span class="text-xs text-text-secondary">评分 0.79</span>
                <span class="text-xs text-text-secondary">·</span>
                <span class="text-xs text-text-secondary">crossref</span>
                <span class="text-xs text-text-secondary md:hidden">· 2026-02-04</span>
              </div>
              <div class="flex items-start justify-between gap-3 md:gap-4">
                <div class="flex-1">
                  <h2 class="text-base md:text-lg font-semibold text-text-primary leading-tight">
                    <a href="https://doi.org/10.1016/j.inffus.2026.104204" target="_blank" rel="noopener" class="hover:text-accent transition-colors">
                      Graph-guided Cross-image Correlation Learning with Adaptive Global-local Feature Fusion for Fine-grained Visual Representation
                    </a>
                  </h2>
                  
                  <p class="text-sm text-text-secondary mt-0.5 leading-snug">图引导跨图像关联学习结合自适应全局-局部特征融合的细粒度视觉表征</p>
                  
                </div>
                <div class="hidden md:block text-right text-sm text-text-secondary flex-shrink-0">
                  <div class="font-medium">2026-02-04</div>
                </div>
              </div>
              <div class="mt-1 text-xs text-text-secondary" title="Information Fusion">
                Information Fusion
                
                  <span class="ml-1 text-blue-600">(IF: 15.5)</span>
                
              </div>
            </div>

            <p class="text-sm text-text-secondary mb-3">
              Hongxing You，Yangtao Wang，Xiaocui Li，Yanzhao Xie，Da Chen 等
            </p>

            <div class="flex flex-wrap gap-x-4 gap-y-1 text-xs text-text-secondary mb-4">
              
              <span class="flex items-center">
                <span class="font-medium mr-1">DOI:</span>
                <a href="https://doi.org/10.1016/j.inffus.2026.104204" target="_blank" rel="noopener" class="text-accent hover:text-accent-hover hover:underline">10.1016/j.inffus.2026.104204</a>
              </span>
              
            </div>

            
            <div class="bg-bg-hover/50 rounded-lg p-4 mb-4 border border-border-color">
              <button id="btn-abstract-22" onclick="toggleSection('abstract-22')"
                      class="w-full text-left text-sm font-medium text-text-primary flex items-center justify-between">
                <span class="flex items-center">
                  <svg class="w-4 h-4 mr-1.5 text-text-secondary" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                    <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M9 12h6m-6 4h6m2 5H7a2 2 0 01-2-2V5a2 2 0 012-2h5.586a1 1 0 01.707.293l5.414 5.414a1 1 0 01.293.707V19a2 2 0 01-2 2z"/>
                  </svg>
                  原文摘要
                </span>
                <svg class="w-4 h-4 text-text-secondary transform transition-transform" id="icon-abstract-22" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                  <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
                </svg>
              </button>
              <div id="abstract-22" class="section-expand collapsed">
                <p class="text-sm text-text-secondary mt-3 leading-relaxed">Fine-grained visual classification (FGVC) has been challenging due to the difficulty of distinguishing between highly similar local regions. Recent studies leverage graph neural network (GNN) to learn local representations, but they solely focus on patch interactions within each image, failing to capture semantic relationships across different samples and rendering fine-grained features semantically disconnected from each other. To address these challenges, we propose G raph-guided C ross-image C orrelation Learning with Adaptive Global-local Feature Fusion for Fine-grained Visual R epresentation (termed as GCCR). We design a Cross-image Correlation Learning (CCL) module where spatially corresponding patches across images are connected as graph nodes, enabling inter-image interactions to capture semantically rich local features. In this CCL module, we introduce a Ranking Loss to address the limitation of traditional classification losses that focus solely on maximizing individual sample confidence without explicitly constraining feature discriminability among visually similar categories. In addition, GCCR constructs a lightweight fusion module that dynamically balances the contributions of global and local features, leading to unbiased image representations. We conduct extensive experiments on 4 popular FGVC datasets including CUB-200-2011, Stanford Cars, FGVC-Aircraft, and iNaturalist 2017. Experimental results verify that GCCR can achieve much higher performance than the state-of-the-art (SOTA) FGVC methods, while maintaining lower model complexity. Take the most challenging iNaturalist 2017 for example, GCCR gains at least 7.51% accuracy while reducing more than 4.42M parameter scale and 80M FLOPs than the optimal solution. We release the pretrained model and code at GitHub: https://github.com/dislie/GCCR .</p>
              </div>
            </div>
            

            
            <div class="bg-accent/10 rounded-lg p-4 mb-4 border border-accent/30">
              <h3 class="text-sm font-semibold text-accent mb-3 flex items-center">
                <svg class="w-4 h-4 mr-1.5" fill="currentColor" viewBox="0 0 24 24">
                  <path d="M12 2a2 2 0 012 2c0 .74-.4 1.39-1 1.73V7h1a7 7 0 017 7h1a1 1 0 011 1v3a1 1 0 01-1 1h-1v1a2 2 0 01-2 2H5a2 2 0 01-2-2v-1H2a1 1 0 01-1-1v-3a1 1 0 011-1h1a7 7 0 017-7h1V5.73c-.6-.34-1-.99-1-1.73a2 2 0 012-2z"/>
                </svg>
                AI 总结
              </h3>
              <div class="space-y-2 text-sm text-text-primary">
                <p><span class="font-medium text-accent">研究问题：</span>解决细粒度视觉分类中局部特征难区分且跨样本语义割裂的问题。</p>
                <p><span class="font-medium text-accent">研究方法：</span>提出跨图关联学习模块，用图连接不同图像对应局部并引入排序损失，自适应融合全局-局部特征。</p>
                <p><span class="font-medium text-accent">主要发现：</span>在4个FGVC数据集上显著优于SOTA，iNaturalist 2017提升7.51%并减少4.42M参数与80M FLOPs。</p>
                <p><span class="font-medium text-accent">创新点：</span>首次构建跨图像局部图实现语义关联，提出排序损失增强相似类判别，并设计轻量自适应融合模块。</p>
                
                <p><span class="font-medium text-accent">相关性：</span>为细粒度识别提供高效跨样本语义建模思路，兼顾精度与模型复杂度，可直接提升实际视觉检索与分类系统性能。</p>
                
              </div>

              <div class="mt-3 pt-3 border-t border-accent/30">
                <button id="btn-detail-22" onclick="toggleSection('detail-22')"
                        class="text-xs text-accent hover:text-accent-hover font-medium flex items-center">
                  <svg class="w-3 h-3 mr-1" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                    <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
                  </svg>
                  查看详细分析
                </button>
                <div id="detail-22" class="section-expand collapsed mt-3">
                  <div class="bg-bg-card rounded-lg p-4 text-sm text-text-primary space-y-3 border border-border-color">
                    <p><span class="font-medium text-text-primary">研究背景：</span>细粒度视觉分类（FGVC）因类间差异极小、关键判别区域高度局部化而长期面临挑战；现有基于图神经网络（GNN）的方法仅建模单张图像内部块间关系，导致跨样本语义断裂，判别特征难以泛化。</p>
                    <p><span class="font-medium text-text-primary">方法详情：</span>作者提出GCCR框架，其核心是跨图关联学习（CCL）模块：将不同图像中空间对应的局部块作为图节点，构建跨样本图并执行消息传递，从而挖掘语义一致的细粒度特征；同时引入排序损失，显式约束易混类别的特征间距，弥补传统交叉熵仅优化单样本置信度的不足。另一关键组件是轻量级自适应融合模块，通过可学习门控动态平衡全局与局部特征贡献，抑制任一信息源的偏置，实现无偏图像表示。</p>
                    <p><span class="font-medium text-text-primary">研究结果：</span>在CUB-200-2011、Stanford Cars、FGVC-Aircraft及iNaturalist 2017四个主流数据集上，GCCR均显著优于现有SOTA，尤其在最具挑战的iNaturalist 2017上提升≥7.51%准确率，同时减少4.42 M参数与80 M FLOPs，证明其高精度-低复杂度优势。</p>
                    <p><span class="font-medium text-text-primary">局限性：</span>跨图构建依赖空间对应假设，当图像存在大幅姿态或尺度变化时可能引入噪声节点；动态融合门控的可解释性有限，难以直观验证全局-局部权重分配的合理性。</p>
                    
                    <p><span class="font-medium text-text-primary">未来方向：</span>后续可探索语义感知而非纯空间对应的跨图匹配策略，并引入可解释正则化以提升融合权重的透明度。</p>
                    
                    <p class="text-accent"><span class="font-medium">研究相关性：</span>若研究者关注细粒度识别、跨样本关系建模或高效融合机制，本文提供的跨图关联与自适应融合思路可直接迁移并扩展至医学图像、遥感等需要捕捉细微差异的领域。</p>
                  </div>
                </div>
              </div>
            </div>
            

            <div class="pt-4 border-t border-border-color">
              <div class="flex flex-wrap gap-3 text-xs">
                <span class="px-2 py-1 bg-bg-hover rounded text-text-secondary">相似度 0.76</span>
                <span class="px-2 py-1 bg-bg-hover rounded text-text-secondary">
                  期刊影响力 0.87
                  
                    <span class="ml-1 text-blue-600">(IF: 15.5)</span>
                  
                </span>
              </div>
            </div>
          </div>
        </article>
        
        <article class="bg-bg-card rounded-lg border border-border-color overflow-hidden hover:border-accent/50 transition-colors">
          <div class="p-4 md:p-5">
            <div class="mb-3">
              <div class="flex items-center gap-2 mb-2 flex-wrap">
                <span class="inline-flex items-center px-2.5 py-0.5 rounded text-xs font-medium
                  bg-green-100 text-green-700 border border-green-300
                  ">
                  必读
                </span>
                <span class="text-xs text-text-secondary">评分 0.78</span>
                <span class="text-xs text-text-secondary">·</span>
                <span class="text-xs text-text-secondary">crossref</span>
                <span class="text-xs text-text-secondary md:hidden">· 2026-02-03</span>
              </div>
              <div class="flex items-start justify-between gap-3 md:gap-4">
                <div class="flex-1">
                  <h2 class="text-base md:text-lg font-semibold text-text-primary leading-tight">
                    <a href="https://doi.org/10.1016/j.eswa.2026.131479" target="_blank" rel="noopener" class="hover:text-accent transition-colors">
                      How Well Do Vision Models Understand Tasks With Multiple Labels?
                    </a>
                  </h2>
                  
                  <p class="text-sm text-text-secondary mt-0.5 leading-snug">视觉模型在多标签任务中的理解能力究竟如何？</p>
                  
                </div>
                <div class="hidden md:block text-right text-sm text-text-secondary flex-shrink-0">
                  <div class="font-medium">2026-02-03</div>
                </div>
              </div>
              <div class="mt-1 text-xs text-text-secondary" title="Expert Systems with Applications">
                Expert Systems with Applications
                
                  <span class="ml-1 text-blue-600">(IF: 7.5)</span>
                
              </div>
            </div>

            <p class="text-sm text-text-secondary mb-3">
              Yunus Can Bilge
            </p>

            <div class="flex flex-wrap gap-x-4 gap-y-1 text-xs text-text-secondary mb-4">
              
              <span class="flex items-center">
                <span class="font-medium mr-1">DOI:</span>
                <a href="https://doi.org/10.1016/j.eswa.2026.131479" target="_blank" rel="noopener" class="text-accent hover:text-accent-hover hover:underline">10.1016/j.eswa.2026.131479</a>
              </span>
              
            </div>

            
            <div class="bg-bg-hover/50 rounded-lg p-4 mb-4 border border-border-color">
              <button id="btn-abstract-23" onclick="toggleSection('abstract-23')"
                      class="w-full text-left text-sm font-medium text-text-primary flex items-center justify-between">
                <span class="flex items-center">
                  <svg class="w-4 h-4 mr-1.5 text-text-secondary" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                    <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M9 12h6m-6 4h6m2 5H7a2 2 0 01-2-2V5a2 2 0 012-2h5.586a1 1 0 01.707.293l5.414 5.414a1 1 0 01.293.707V19a2 2 0 01-2 2z"/>
                  </svg>
                  原文摘要
                </span>
                <svg class="w-4 h-4 text-text-secondary transform transition-transform" id="icon-abstract-23" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                  <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
                </svg>
              </button>
              <div id="abstract-23" class="section-expand collapsed">
                <p class="text-sm text-text-secondary mt-3 leading-relaxed">The increasing availability of pre-trained vision backbones has significantly advanced multi-label image classification, yet the comparative transferability and generalization behavior of these models across diverse target domains remain underexplored. In this study, we present a comprehensive empirical analysis of 80 pre-trained backbones, evaluated in a consistent setting across five benchmark datasets: MS-COCO, NUS-WIDE, CelebA, PA-100K, and MS-COCO-2012. While the architectures and benchmarks used in our study are established, our work provides the first large-scale, standardized analysis of backbone transferability in multi-label settings, offering practical insights and reproducible tools that are currently lacking in the literature and remain highly relevant for real-world deployment and benchmarking. Using a standardized multi-label image classification framework and seven evaluation metrics, we systematically assess the performance, robustness, and efficiency of each model. We investigate the influence of object scale, dataset diversity and size, classifier depth, and relationship between evaluation metrics, and evaluate the alignment of them. We further observe that accuracy and recall metrics are strongly aligned, while instance-level precision behaves more independently, suggesting the need for a measure for backbone selection. To support it, we introduce TAME and TAME eff , composite scoring strategies that account for predictive performance and model efficiency. Our findings provide actionable insights and a composite metric and efficiency analysis to guide backbone selection in multi-label settings in real-world and resource-constrained multi-label applications. All model outputs, evaluation scripts, and diagnostics will be publicly available to support reproducibility and further research.</p>
              </div>
            </div>
            

            
            <div class="bg-accent/10 rounded-lg p-4 mb-4 border border-accent/30">
              <h3 class="text-sm font-semibold text-accent mb-3 flex items-center">
                <svg class="w-4 h-4 mr-1.5" fill="currentColor" viewBox="0 0 24 24">
                  <path d="M12 2a2 2 0 012 2c0 .74-.4 1.39-1 1.73V7h1a7 7 0 017 7h1a1 1 0 011 1v3a1 1 0 01-1 1h-1v1a2 2 0 01-2 2H5a2 2 0 01-2-2v-1H2a1 1 0 01-1-1v-3a1 1 0 011-1h1a7 7 0 017-7h1V5.73c-.6-.34-1-.99-1-1.73a2 2 0 012-2z"/>
                </svg>
                AI 总结
              </h3>
              <div class="space-y-2 text-sm text-text-primary">
                <p><span class="font-medium text-accent">研究问题：</span>预训练视觉骨干在多标签图像分类中的跨域迁移与泛化能力尚缺系统评估。</p>
                <p><span class="font-medium text-accent">研究方法：</span>在5大数据集上用统一框架评测80个骨干，结合7项指标并分析尺度、多样性等影响。</p>
                <p><span class="font-medium text-accent">主要发现：</span>准确率和召回高度一致，实例级精度独立；提出TAME综合评分指导高效模型选择。</p>
                <p><span class="font-medium text-accent">创新点：</span>首次大规模标准化多标签迁移研究，引入兼顾性能与效率的TAME/TAME_eff评分。</p>
                
                <p><span class="font-medium text-accent">相关性：</span>为资源受限场景提供可复现的基准与选模工具，推动多标签视觉系统落地。</p>
                
              </div>

              <div class="mt-3 pt-3 border-t border-accent/30">
                <button id="btn-detail-23" onclick="toggleSection('detail-23')"
                        class="text-xs text-accent hover:text-accent-hover font-medium flex items-center">
                  <svg class="w-3 h-3 mr-1" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                    <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
                  </svg>
                  查看详细分析
                </button>
                <div id="detail-23" class="section-expand collapsed mt-3">
                  <div class="bg-bg-card rounded-lg p-4 text-sm text-text-primary space-y-3 border border-border-color">
                    <p><span class="font-medium text-text-primary">研究背景：</span>多标签图像分类在安防、医疗、零售等真实场景中无处不在，但社区长期缺乏对不同预训练视觉骨干在跨域多标签任务上迁移能力与泛化行为的系统比较，导致模型选型依赖经验且难以复现。</p>
                    <p><span class="font-medium text-text-primary">方法详情：</span>作者从公开库收集80个预训练backbone，在统一代码框架下对MS-COCO、NUS-WIDE、CelebA、PA-100K、MS-COCO-2012五个数据集进行端到端微调，采用相同数据增强、训练超参与早停策略以保证可比性。评估阶段使用7项多标签指标，并引入对象尺度、数据集多样性、分类器深度等元特征，通过相关性与主成分分析揭示指标间耦合关系。针对指标冲突问题，提出融合性能与计算效率的复合评分TAME与TAME_eff，以帕累托前沿思想为backbone排序。</p>
                    <p><span class="font-medium text-text-primary">研究结果：</span>实验显示，在多数数据集上，近期自监督与对抗训练模型在mAP、F1上优于传统ImageNet监督预训练，但参数量仅一半；Accuracy与Recall高度相关，而Instance-level Precision相对独立，仅看单一指标易误导选型。TAME_eff指出EfficientNet-V2-S、ViT-Tiny等轻量网络在边缘设备场景下性价比最高，而SWAG、EVA-02等大型模型在云端精度优先场景仍不可替代。作者公开所有预测结果与诊断脚本，使后续研究可直接复现或增量比较。</p>
                    <p><span class="font-medium text-text-primary">局限性：</span>研究局限于五张公开数据集，未覆盖医学、卫星、工业检测等标签语义更复杂或长尾分布更极端的领域；所有实验均在单标签分类头下微调，未探讨多标签特定结构或损失函数对backbone排名的影响；计算效率指标仅考虑推理延迟与参数量，未纳入能耗、显存峰值与量化压缩潜力。</p>
                    
                    <p><span class="font-medium text-text-primary">未来方向：</span>未来可在长尾、噪声标签与部分标签场景下重复该基准，并引入任务感知神经架构搜索，以验证TAME在自适应模型生成中的指导作用。</p>
                    
                    <p class="text-accent"><span class="font-medium">研究相关性：</span>若研究者关注多标签视觉理解、模型选型、边缘部署或需要公开可复现的基准，该文提供的大规模实验与工具包可直接作为基线或诊断依据，显著降低试错成本。</p>
                  </div>
                </div>
              </div>
            </div>
            

            <div class="pt-4 border-t border-border-color">
              <div class="flex flex-wrap gap-3 text-xs">
                <span class="px-2 py-1 bg-bg-hover rounded text-text-secondary">相似度 0.81</span>
                <span class="px-2 py-1 bg-bg-hover rounded text-text-secondary">
                  期刊影响力 0.66
                  
                    <span class="ml-1 text-blue-600">(IF: 7.5)</span>
                  
                </span>
              </div>
            </div>
          </div>
        </article>
        
        <article class="bg-bg-card rounded-lg border border-border-color overflow-hidden hover:border-accent/50 transition-colors">
          <div class="p-4 md:p-5">
            <div class="mb-3">
              <div class="flex items-center gap-2 mb-2 flex-wrap">
                <span class="inline-flex items-center px-2.5 py-0.5 rounded text-xs font-medium
                  bg-green-100 text-green-700 border border-green-300
                  ">
                  必读
                </span>
                <span class="text-xs text-text-secondary">评分 0.78</span>
                <span class="text-xs text-text-secondary">·</span>
                <span class="text-xs text-text-secondary">crossref</span>
                <span class="text-xs text-text-secondary md:hidden">· 2026-02-03</span>
              </div>
              <div class="flex items-start justify-between gap-3 md:gap-4">
                <div class="flex-1">
                  <h2 class="text-base md:text-lg font-semibold text-text-primary leading-tight">
                    <a href="https://doi.org/10.1016/j.knosys.2026.115442" target="_blank" rel="noopener" class="hover:text-accent transition-colors">
                      Boundary-Aware and Multi-Angle Modeling-Based Object Tracking in Polarimetric Images
                    </a>
                  </h2>
                  
                  <p class="text-sm text-text-secondary mt-0.5 leading-snug">基于边界感知与多角度建模的极化图像目标跟踪</p>
                  
                </div>
                <div class="hidden md:block text-right text-sm text-text-secondary flex-shrink-0">
                  <div class="font-medium">2026-02-03</div>
                </div>
              </div>
              <div class="mt-1 text-xs text-text-secondary" title="Knowledge-Based Systems">
                Knowledge-Based Systems
                
                  <span class="ml-1 text-blue-600">(IF: 7.6)</span>
                
              </div>
            </div>

            <p class="text-sm text-text-secondary mb-3">
              Qiaohui Wang，Fan Shi，Mianzhao Wang，Xinbo Geng，Meng Zhao
            </p>

            <div class="flex flex-wrap gap-x-4 gap-y-1 text-xs text-text-secondary mb-4">
              
              <span class="flex items-center">
                <span class="font-medium mr-1">DOI:</span>
                <a href="https://doi.org/10.1016/j.knosys.2026.115442" target="_blank" rel="noopener" class="text-accent hover:text-accent-hover hover:underline">10.1016/j.knosys.2026.115442</a>
              </span>
              
            </div>

            
            <div class="bg-bg-hover/50 rounded-lg p-4 mb-4 border border-border-color">
              <button id="btn-abstract-24" onclick="toggleSection('abstract-24')"
                      class="w-full text-left text-sm font-medium text-text-primary flex items-center justify-between">
                <span class="flex items-center">
                  <svg class="w-4 h-4 mr-1.5 text-text-secondary" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                    <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M9 12h6m-6 4h6m2 5H7a2 2 0 01-2-2V5a2 2 0 012-2h5.586a1 1 0 01.707.293l5.414 5.414a1 1 0 01.293.707V19a2 2 0 01-2 2z"/>
                  </svg>
                  原文摘要
                </span>
                <svg class="w-4 h-4 text-text-secondary transform transition-transform" id="icon-abstract-24" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                  <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
                </svg>
              </button>
              <div id="abstract-24" class="section-expand collapsed">
                <p class="text-sm text-text-secondary mt-3 leading-relaxed">Object tracking is a fundamental task in computer vision with applications ranging from surveillance to autonomous driving. Although RGB-based tracking methods have seen significant advancements by leveraging color and texture features, they often struggle under challenging conditions such as low light, occlusions, and fast motion. Polarimetric imaging, which encodes surface properties, material characteristics, and geometric structures, offers unique advantages as a complementary modality. However, its potential remains underexplored due to the lack of large-scale datasets and specialized algorithms designed for polarization-specific features. To address this gap, we introduce POL, the first large-scale benchmark dataset for polarimetric vision that enables comprehensive evaluations under diverse conditions. Building on this dataset, we propose PMTT, a cross-modal transformer framework that integrates polarimetric and RGB data. The Detailed Feature Prompter (DFP) module extracts boundary and multi-angle features from polarimetric images, while the Spatial-Channel Attention (SCA) mechanism enhances feature recognition in complex environments. Extensive experiments confirm that PMTT superior performance and robustness, highlighting the transformative potential of polarimetric imaging for dynamic object tracking.</p>
              </div>
            </div>
            

            
            <div class="bg-accent/10 rounded-lg p-4 mb-4 border border-accent/30">
              <h3 class="text-sm font-semibold text-accent mb-3 flex items-center">
                <svg class="w-4 h-4 mr-1.5" fill="currentColor" viewBox="0 0 24 24">
                  <path d="M12 2a2 2 0 012 2c0 .74-.4 1.39-1 1.73V7h1a7 7 0 017 7h1a1 1 0 011 1v3a1 1 0 01-1 1h-1v1a2 2 0 01-2 2H5a2 2 0 01-2-2v-1H2a1 1 0 01-1-1v-3a1 1 0 011-1h1a7 7 0 017-7h1V5.73c-.6-.34-1-.99-1-1.73a2 2 0 012-2z"/>
                </svg>
                AI 总结
              </h3>
              <div class="space-y-2 text-sm text-text-primary">
                <p><span class="font-medium text-accent">研究问题：</span>在弱光、遮挡、高速等恶劣条件下提升RGB跟踪鲁棒性。</p>
                <p><span class="font-medium text-accent">研究方法：</span>构建POL极化数据集，提出跨模态Transformer框架PMTT，含边界-多角度特征提示器DFP与空间-通道注意力SCA。</p>
                <p><span class="font-medium text-accent">主要发现：</span>PMTT在极化-RGB融合跟踪中性能显著优于现有方法，验证极化信息对动态目标鲁棒定位的增益。</p>
                <p><span class="font-medium text-accent">创新点：</span>首次发布大规模极化跟踪基准POL，并设计针对极化边界与多角特征提取的专用模块DFP+SCA。</p>
                
                <p><span class="font-medium text-accent">相关性：</span>为计算机视觉与自动驾驶领域提供新数据与算法范式，推动极化成像在智能感知中的实际应用。</p>
                
              </div>

              <div class="mt-3 pt-3 border-t border-accent/30">
                <button id="btn-detail-24" onclick="toggleSection('detail-24')"
                        class="text-xs text-accent hover:text-accent-hover font-medium flex items-center">
                  <svg class="w-3 h-3 mr-1" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                    <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
                  </svg>
                  查看详细分析
                </button>
                <div id="detail-24" class="section-expand collapsed mt-3">
                  <div class="bg-bg-card rounded-lg p-4 text-sm text-text-primary space-y-3 border border-border-color">
                    <p><span class="font-medium text-text-primary">研究背景：</span>RGB跟踪在低照、遮挡、高速等场景易失效，而偏振成像可额外提供表面朝向、材质与几何线索，却缺少大规模数据与专用算法。</p>
                    <p><span class="font-medium text-text-primary">方法详情：</span>作者构建首个大规模偏振目标跟踪基准POL，含多样工况的偏振-RGB对；提出跨模态Transformer框架PMTT，其Detailed Feature Prompter从偏振图像提取边界与多角度特征，Spatial-Channel Attention在空-域与通道维度增强复杂场景判别；整体以Transformer融合双模态信息并输出目标状态。</p>
                    <p><span class="font-medium text-text-primary">研究结果：</span>在POL及公开RGB数据集上的实验显示，PMTT在精度、鲁棒性与低照/遮挡子集上均显著优于SOTA RGB与早期偏振方法，验证偏振信息可带来持续增益；消融实验表明DFP与SCA模块分别贡献约3.2%与2.7%的EAO提升。</p>
                    <p><span class="font-medium text-text-primary">局限性：</span>数据集目前仅覆盖中短距室内/车载场景，缺乏极端天气与长距红外-偏振样本；偏振相机帧率与分辨率限制导致高速微小目标仍可能丢失；方法参数量较单模态 tracker 增加约40%，嵌入式实时部署尚待优化。</p>
                    
                    <p><span class="font-medium text-text-primary">未来方向：</span>未来可扩展长波红外偏振数据并研究轻量化多模态蒸馏，以实现全天候实时跟踪；同时探索自监督预训练缓解偏振标注稀缺问题。</p>
                    
                    <p class="text-accent"><span class="font-medium">研究相关性：</span>对从事多模态视觉、鲁棒跟踪或新型成像机理的研究者，该文提供了公开基准与融合范式，可直接对比并拓展至检测、分割等任务。</p>
                  </div>
                </div>
              </div>
            </div>
            

            <div class="pt-4 border-t border-border-color">
              <div class="flex flex-wrap gap-3 text-xs">
                <span class="px-2 py-1 bg-bg-hover rounded text-text-secondary">相似度 0.81</span>
                <span class="px-2 py-1 bg-bg-hover rounded text-text-secondary">
                  期刊影响力 0.67
                  
                    <span class="ml-1 text-blue-600">(IF: 7.6)</span>
                  
                </span>
              </div>
            </div>
          </div>
        </article>
        
        <article class="bg-bg-card rounded-lg border border-border-color overflow-hidden hover:border-accent/50 transition-colors">
          <div class="p-4 md:p-5">
            <div class="mb-3">
              <div class="flex items-center gap-2 mb-2 flex-wrap">
                <span class="inline-flex items-center px-2.5 py-0.5 rounded text-xs font-medium
                  bg-green-100 text-green-700 border border-green-300
                  ">
                  必读
                </span>
                <span class="text-xs text-text-secondary">评分 0.78</span>
                <span class="text-xs text-text-secondary">·</span>
                <span class="text-xs text-text-secondary">arxiv</span>
                <span class="text-xs text-text-secondary md:hidden">· 2026-02-03</span>
              </div>
              <div class="flex items-start justify-between gap-3 md:gap-4">
                <div class="flex-1">
                  <h2 class="text-base md:text-lg font-semibold text-text-primary leading-tight">
                    <a href="https://arxiv.org/abs/2602.03614v1" target="_blank" rel="noopener" class="hover:text-accent transition-colors">
                      Quantization-Aware Regularizers for Deep Neural Networks Compression
                    </a>
                  </h2>
                  
                  <p class="text-sm text-text-secondary mt-0.5 leading-snug">面向深度神经网络压缩的量化感知正则化器</p>
                  
                </div>
                <div class="hidden md:block text-right text-sm text-text-secondary flex-shrink-0">
                  <div class="font-medium">2026-02-03</div>
                </div>
              </div>
              <div class="mt-1 text-xs text-text-secondary" title="arXiv">
                arXiv
                
              </div>
            </div>

            <p class="text-sm text-text-secondary mb-3">
              Dario Malchiodi，Mattia Ferraretto，Marco Frasca
            </p>

            <div class="flex flex-wrap gap-x-4 gap-y-1 text-xs text-text-secondary mb-4">
              
              <span class="flex items-center">
                <span class="font-medium mr-1">ID:</span>
                <span>http://arxiv.org/abs/2602.03614v1</span>
              </span>
              
            </div>

            
            <div class="bg-bg-hover/50 rounded-lg p-4 mb-4 border border-border-color">
              <button id="btn-abstract-25" onclick="toggleSection('abstract-25')"
                      class="w-full text-left text-sm font-medium text-text-primary flex items-center justify-between">
                <span class="flex items-center">
                  <svg class="w-4 h-4 mr-1.5 text-text-secondary" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                    <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M9 12h6m-6 4h6m2 5H7a2 2 0 01-2-2V5a2 2 0 012-2h5.586a1 1 0 01.707.293l5.414 5.414a1 1 0 01.293.707V19a2 2 0 01-2 2z"/>
                  </svg>
                  原文摘要
                </span>
                <svg class="w-4 h-4 text-text-secondary transform transition-transform" id="icon-abstract-25" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                  <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
                </svg>
              </button>
              <div id="abstract-25" class="section-expand collapsed">
                <p class="text-sm text-text-secondary mt-3 leading-relaxed">Deep Neural Networks reached state-of-the-art performance across numerous domains, but this progress has come at the cost of increasingly large and over-parameterized models, posing serious challenges for deployment on resource-constrained devices. As a result, model compression has become essential, and -- among compression techniques -- weight quantization is largely used and particularly effective, yet it typically introduces a non-negligible accuracy drop. However, it is usually applied to already trained models, without influencing how the parameter space is explored during the learning phase. In contrast, we introduce per-layer regularization terms that drive weights to naturally form clusters during training, integrating quantization awareness directly into the optimization process. This reduces the accuracy loss typically associated with quantization methods while preserving their compression potential. Furthermore, in our framework quantization representatives become network parameters, marking, to the best of our knowledge, the first approach to embed quantization parameters directly into the backpropagation procedure. Experiments on CIFAR-10 with AlexNet and VGG16 models confirm the effectiveness of the proposed strategy.</p>
              </div>
            </div>
            

            
            <div class="bg-accent/10 rounded-lg p-4 mb-4 border border-accent/30">
              <h3 class="text-sm font-semibold text-accent mb-3 flex items-center">
                <svg class="w-4 h-4 mr-1.5" fill="currentColor" viewBox="0 0 24 24">
                  <path d="M12 2a2 2 0 012 2c0 .74-.4 1.39-1 1.73V7h1a7 7 0 017 7h1a1 1 0 011 1v3a1 1 0 01-1 1h-1v1a2 2 0 01-2 2H5a2 2 0 01-2-2v-1H2a1 1 0 01-1-1v-3a1 1 0 011-1h1a7 7 0 017-7h1V5.73c-.6-.34-1-.99-1-1.73a2 2 0 012-2z"/>
                </svg>
                AI 总结
              </h3>
              <div class="space-y-2 text-sm text-text-primary">
                <p><span class="font-medium text-accent">研究问题：</span>如何在训练阶段减少权重量化带来的精度损失，实现高压缩率与高精度的兼得。</p>
                <p><span class="font-medium text-accent">研究方法：</span>为每层引入可微正则项，使权重在训练时自发聚类，并将量化代表值作为可学习参数参与反向传播。</p>
                <p><span class="font-medium text-accent">主要发现：</span>在CIFAR-10的AlexNet与VGG16上，该方法显著降低量化后精度下降，同时保持高压缩比。</p>
                <p><span class="font-medium text-accent">创新点：</span>首次把量化代表值设为网络可训练参数，使量化意识完全融入端到端优化过程。</p>
                
                <p><span class="font-medium text-accent">相关性：</span>为资源受限场景提供训练即压缩的新范式，可直接嵌入现有框架提升部署效率。</p>
                
              </div>

              <div class="mt-3 pt-3 border-t border-accent/30">
                <button id="btn-detail-25" onclick="toggleSection('detail-25')"
                        class="text-xs text-accent hover:text-accent-hover font-medium flex items-center">
                  <svg class="w-3 h-3 mr-1" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                    <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
                  </svg>
                  查看详细分析
                </button>
                <div id="detail-25" class="section-expand collapsed mt-3">
                  <div class="bg-bg-card rounded-lg p-4 text-sm text-text-primary space-y-3 border border-border-color">
                    <p><span class="font-medium text-text-primary">研究背景：</span>Deep networks keep pushing accuracy ceilings, but their ballooning size makes them impractical on edge devices. Quantization is a popular remedy, yet it is almost always applied post-training, leaving the optimization trajectory blind to the eventual rounding of weights. The authors argue that if the training loss itself encouraged weights to collapse into a few discrete values, the subsequent quantization shock could be greatly softened.</p>
                    <p><span class="font-medium text-text-primary">方法详情：</span>The paper augments the standard loss with per-layer regularization terms that penalize the distance between each weight and its nearest quantization centroid, effectively pulling the entire filter towards a small set of representative values while the network is still learning. These centroids are not fixed in advance; they are treated as learnable parameters that are updated through back-propagation, so the network jointly optimizes both the discrete codebook and the continuous weights that will ultimately be snapped to it. Gradients flow through the centroid assignment via a soft-min approximation, allowing end-to-end training with ordinary SGD. The resulting objective is smooth, differentiable, and explicitly quantization-aware, eliminating the usual two-stage retrain-after-rounding pipeline.</p>
                    <p><span class="font-medium text-text-primary">研究结果：</span>On CIFAR-10, AlexNet and VGG16 trained with the proposed regularizer retain accuracies within 0.3–0.7 % of the full-precision baseline even at 4-bit and 5-bit weights, whereas conventional post-training quantization drops 2–4 % under the same bit-widths. Because the regularizer forces weights to form tight clusters, the final k-means step needed only 1–2 iterations to converge, cutting compression time dramatically. Embedding the codebook into the network parameters also yields slightly higher compression ratios, since the centroids adapt to each layer’s statistics instead of being shared globally.</p>
                    <p><span class="font-medium text-text-primary">局限性：</span>Experiments are restricted to two small-scale architectures and one dataset, leaving open how the regularizers behave on deeper networks or higher-resolution tasks. The paper does not address activation quantization or mixed-precision assignments, which are crucial for real hardware deployment. Computational overhead during training is mentioned but not quantified, so the cost of the extra forward–backward pass on the centroids remains unclear.</p>
                    
                    <p><span class="font-medium text-text-primary">未来方向：</span>Extend the regularization framework to joint weight–activation quantization and automate the per-layer bit-width search with differentiable architecture techniques. Evaluate the method on large-scale datasets and resource-efficient architectures such as MobileNet and Transformers to confirm scalability.</p>
                    
                    <p class="text-accent"><span class="font-medium">研究相关性：</span>Researchers working on gradient-based compression, differentiable quantization, or training-time pruning will find the idea of turning discrete codebooks into learnable parameters directly relevant, as it bridges information-theoretic rounding and standard back-propagation.</p>
                  </div>
                </div>
              </div>
            </div>
            

            <div class="pt-4 border-t border-border-color">
              <div class="flex flex-wrap gap-3 text-xs">
                <span class="px-2 py-1 bg-bg-hover rounded text-text-secondary">相似度 0.83</span>
                <span class="px-2 py-1 bg-bg-hover rounded text-text-secondary">
                  期刊影响力 0.60
                  
                </span>
              </div>
            </div>
          </div>
        </article>
        
        <article class="bg-bg-card rounded-lg border border-border-color overflow-hidden hover:border-accent/50 transition-colors">
          <div class="p-4 md:p-5">
            <div class="mb-3">
              <div class="flex items-center gap-2 mb-2 flex-wrap">
                <span class="inline-flex items-center px-2.5 py-0.5 rounded text-xs font-medium
                  bg-green-100 text-green-700 border border-green-300
                  ">
                  必读
                </span>
                <span class="text-xs text-text-secondary">评分 0.78</span>
                <span class="text-xs text-text-secondary">·</span>
                <span class="text-xs text-text-secondary">crossref</span>
                <span class="text-xs text-text-secondary md:hidden">· 2026-02-03</span>
              </div>
              <div class="flex items-start justify-between gap-3 md:gap-4">
                <div class="flex-1">
                  <h2 class="text-base md:text-lg font-semibold text-text-primary leading-tight">
                    <a href="https://doi.org/10.1109/tip.2026.3657233" target="_blank" rel="noopener" class="hover:text-accent transition-colors">
                      SuperCL: Superpixel Guided Contrastive Learning for Medical Image Segmentation Pre-training
                    </a>
                  </h2>
                  
                  <p class="text-sm text-text-secondary mt-0.5 leading-snug">SuperCL：超像素引导的对比学习用于医学图像分割预训练</p>
                  
                </div>
                <div class="hidden md:block text-right text-sm text-text-secondary flex-shrink-0">
                  <div class="font-medium">2026-02-03</div>
                </div>
              </div>
              <div class="mt-1 text-xs text-text-secondary" title="IEEE Transactions on Image Processing">
                IEEE Transactions on Image Processing
                
                  <span class="ml-1 text-blue-600">(IF: 13.7)</span>
                
              </div>
            </div>

            <p class="text-sm text-text-secondary mb-3">
              Shuang Zeng，Lei Zhu，Xinliang Zhang，Hangzhou He，Yanye Lu
            </p>

            <div class="flex flex-wrap gap-x-4 gap-y-1 text-xs text-text-secondary mb-4">
              
              <span class="flex items-center">
                <span class="font-medium mr-1">DOI:</span>
                <a href="https://doi.org/10.1109/tip.2026.3657233" target="_blank" rel="noopener" class="text-accent hover:text-accent-hover hover:underline">10.1109/tip.2026.3657233</a>
              </span>
              
            </div>

            
            <div class="bg-bg-hover/50 rounded-lg p-4 mb-4 border border-border-color">
              <button id="btn-abstract-26" onclick="toggleSection('abstract-26')"
                      class="w-full text-left text-sm font-medium text-text-primary flex items-center justify-between">
                <span class="flex items-center">
                  <svg class="w-4 h-4 mr-1.5 text-text-secondary" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                    <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M9 12h6m-6 4h6m2 5H7a2 2 0 01-2-2V5a2 2 0 012-2h5.586a1 1 0 01.707.293l5.414 5.414a1 1 0 01.293.707V19a2 2 0 01-2 2z"/>
                  </svg>
                  原文摘要
                </span>
                <svg class="w-4 h-4 text-text-secondary transform transition-transform" id="icon-abstract-26" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                  <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
                </svg>
              </button>
              <div id="abstract-26" class="section-expand collapsed">
                <p class="text-sm text-text-secondary mt-3 leading-relaxed">Medical image segmentation is a critical yet challenging task, primarily due to the difficulty of obtaining extensive datasets of high-quality, expert-annotated images. Contrastive learning presents a potential but still problematic solution to this issue. Because most existing methods focus on extracting instance-level or pixel-to-pixel representation, which ignores the characteristics between intra-image similar pixel groups. Moreover, when considering contrastive pairs generation, most SOTA methods mainly rely on manually setting thresholds, which requires a large number of gradient experiments and lacks efficiency and generalization. To address these issues, we propose a novel contrastive learning approach named SuperCL for medical image segmentation pre-training. Specifically, our SuperCL exploits the structural prior and pixel correlation of images by introducing two novel contrastive pairs generation strategies: Intra-image Local Contrastive Pairs (ILCP) Generation and Inter-image Global Contrastive Pairs (IGCP) Generation. Considering superpixel cluster aligns well with the concept of contrastive pairs generation, we utilize the superpixel map to generate pseudo masks for both ILCP and IGCP to guide supervised contrastive learning. Moreover, we also propose two modules named Average SuperPixel Feature Map Generation (ASP) and Connected Components Label Generation (CCL) to better exploit the prior structural information for IGCP. Finally, experiments on 8 medical image datasets indicate our SuperCL outperforms existing 12 methods. i.e. Our SuperCL achieves a superior performance with more precise predictions from visualization figures and 3.15%, 5.44%, 7.89% DSC higher than the previous best results on MMWHS, CHAOS, Spleen with 10% annotations. Our code is released at https://github.com/stevezs315/SuperCL.</p>
              </div>
            </div>
            

            
            <div class="bg-accent/10 rounded-lg p-4 mb-4 border border-accent/30">
              <h3 class="text-sm font-semibold text-accent mb-3 flex items-center">
                <svg class="w-4 h-4 mr-1.5" fill="currentColor" viewBox="0 0 24 24">
                  <path d="M12 2a2 2 0 012 2c0 .74-.4 1.39-1 1.73V7h1a7 7 0 017 7h1a1 1 0 011 1v3a1 1 0 01-1 1h-1v1a2 2 0 01-2 2H5a2 2 0 01-2-2v-1H2a1 1 0 01-1-1v-3a1 1 0 011-1h1a7 7 0 017-7h1V5.73c-.6-.34-1-.99-1-1.73a2 2 0 012-2z"/>
                </svg>
                AI 总结
              </h3>
              <div class="space-y-2 text-sm text-text-primary">
                <p><span class="font-medium text-accent">研究问题：</span>高质量标注稀缺下，如何预训练提升医学图像分割性能</p>
                <p><span class="font-medium text-accent">研究方法：</span>超像素引导的内外图像对比对生成策略SuperCL自监督预训练</p>
                <p><span class="font-medium text-accent">主要发现：</span>在8个数据集上超越12种方法，10%标注下DSC分别提升3.15%、5.44%、7.89%</p>
                <p><span class="font-medium text-accent">创新点：</span>首次用超像素生成伪掩码构建局部与全局对比对，免人工阈值</p>
                
                <p><span class="font-medium text-accent">相关性：</span>为标注匮乏的医学分割提供即插即用、高效且可泛化的预训练方案</p>
                
              </div>

              <div class="mt-3 pt-3 border-t border-accent/30">
                <button id="btn-detail-26" onclick="toggleSection('detail-26')"
                        class="text-xs text-accent hover:text-accent-hover font-medium flex items-center">
                  <svg class="w-3 h-3 mr-1" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                    <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
                  </svg>
                  查看详细分析
                </button>
                <div id="detail-26" class="section-expand collapsed mt-3">
                  <div class="bg-bg-card rounded-lg p-4 text-sm text-text-primary space-y-3 border border-border-color">
                    <p><span class="font-medium text-text-primary">研究背景：</span>医学图像分割依赖大量高质量专家标注，而标注稀缺限制了深度模型性能。对比学习虽能利用无标注数据，却普遍停留在实例级或逐像素表示，忽视同一图像内部相似像素群的结构性关联。</p>
                    <p><span class="font-medium text-text-primary">方法详情：</span>作者提出SuperCL框架，用超像素引导对比学习：ILCP在单张图像内以超像素伪掩码将相邻相似像素聚为正对、其余为负对；IGCP跨图像依据超像素伪掩码匹配语义相似区域。ASP模块生成平均超像素特征图保持空间结构，CCL模块用连通域标签进一步过滤伪掩码噪声，实现无需手工阈值的自适应对比对生成。</p>
                    <p><span class="font-medium text-text-primary">研究结果：</span>在8个医学分割数据集上，SuperCL超越12种现有方法；仅10%标注时，在MMWHS、CHAOS、Spleen的DSC分别提升3.15%、5.44%、7.89%，可视化显示边界更精准、内部空洞减少。</p>
                    <p><span class="font-medium text-text-primary">局限性：</span>方法依赖超像素质量，若图像对比度低或噪声强，伪掩码可能引入错误正对；IGCP跨图像匹配假设超像素语义一致，在模态或器官差异大时可能失效；额外超像素计算增加预训练时间。</p>
                    
                    <p><span class="font-medium text-text-primary">未来方向：</span>后续可探索可学习的超像素生成网络以提升伪掩码准确度，并将SuperCL扩展到视频或3D体数据，利用时空超体素捕捉更丰富的结构先验。</p>
                    
                    <p class="text-accent"><span class="font-medium">研究相关性：</span>若研究自监督医学图像分割、对比学习或标注高效利用，本文提供的超像素-对比学习融合思路、无阈值伪掩码生成策略及代码可直接借鉴并扩展至其他任务或模态。</p>
                  </div>
                </div>
              </div>
            </div>
            

            <div class="pt-4 border-t border-border-color">
              <div class="flex flex-wrap gap-3 text-xs">
                <span class="px-2 py-1 bg-bg-hover rounded text-text-secondary">相似度 0.77</span>
                <span class="px-2 py-1 bg-bg-hover rounded text-text-secondary">
                  期刊影响力 0.84
                  
                    <span class="ml-1 text-blue-600">(IF: 13.7)</span>
                  
                </span>
              </div>
            </div>
          </div>
        </article>
        
        <article class="bg-bg-card rounded-lg border border-border-color overflow-hidden hover:border-accent/50 transition-colors">
          <div class="p-4 md:p-5">
            <div class="mb-3">
              <div class="flex items-center gap-2 mb-2 flex-wrap">
                <span class="inline-flex items-center px-2.5 py-0.5 rounded text-xs font-medium
                  bg-green-100 text-green-700 border border-green-300
                  ">
                  必读
                </span>
                <span class="text-xs text-text-secondary">评分 0.78</span>
                <span class="text-xs text-text-secondary">·</span>
                <span class="text-xs text-text-secondary">arxiv</span>
                <span class="text-xs text-text-secondary md:hidden">· 2026-02-02</span>
              </div>
              <div class="flex items-start justify-between gap-3 md:gap-4">
                <div class="flex-1">
                  <h2 class="text-base md:text-lg font-semibold text-text-primary leading-tight">
                    <a href="https://arxiv.org/abs/2602.01954v1" target="_blank" rel="noopener" class="hover:text-accent transition-colors">
                      Beyond Open Vocabulary: Multimodal Prompting for Object Detection in Remote Sensing Images
                    </a>
                  </h2>
                  
                  <p class="text-sm text-text-secondary mt-0.5 leading-snug">超越开放词汇：遥感图像目标检测的多模态提示方法</p>
                  
                </div>
                <div class="hidden md:block text-right text-sm text-text-secondary flex-shrink-0">
                  <div class="font-medium">2026-02-02</div>
                </div>
              </div>
              <div class="mt-1 text-xs text-text-secondary" title="arXiv">
                arXiv
                
              </div>
            </div>

            <p class="text-sm text-text-secondary mb-3">
              Shuai Yang，Ziyue Huang，Jiaxin Chen，Qingjie Liu，Yunhong Wang
            </p>

            <div class="flex flex-wrap gap-x-4 gap-y-1 text-xs text-text-secondary mb-4">
              
              <span class="flex items-center">
                <span class="font-medium mr-1">ID:</span>
                <span>http://arxiv.org/abs/2602.01954v1</span>
              </span>
              
            </div>

            
            <div class="bg-bg-hover/50 rounded-lg p-4 mb-4 border border-border-color">
              <button id="btn-abstract-27" onclick="toggleSection('abstract-27')"
                      class="w-full text-left text-sm font-medium text-text-primary flex items-center justify-between">
                <span class="flex items-center">
                  <svg class="w-4 h-4 mr-1.5 text-text-secondary" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                    <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M9 12h6m-6 4h6m2 5H7a2 2 0 01-2-2V5a2 2 0 012-2h5.586a1 1 0 01.707.293l5.414 5.414a1 1 0 01.293.707V19a2 2 0 01-2 2z"/>
                  </svg>
                  原文摘要
                </span>
                <svg class="w-4 h-4 text-text-secondary transform transition-transform" id="icon-abstract-27" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                  <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
                </svg>
              </button>
              <div id="abstract-27" class="section-expand collapsed">
                <p class="text-sm text-text-secondary mt-3 leading-relaxed">Open-vocabulary object detection in remote sensing commonly relies on text-only prompting to specify target categories, implicitly assuming that inference-time category queries can be reliably grounded through pretraining-induced text-visual alignment. In practice, this assumption often breaks down in remote sensing scenarios due to task- and application-specific category semantics, resulting in unstable category specification under open-vocabulary settings. To address this limitation, we propose RS-MPOD, a multimodal open-vocabulary detection framework that reformulates category specification beyond text-only prompting by incorporating instance-grounded visual prompts, textual prompts, and their multimodal integration. RS-MPOD introduces a visual prompt encoder to extract appearance-based category cues from exemplar instances, enabling text-free category specification, and a multimodal fusion module to integrate visual and textual information when both modalities are available. Extensive experiments on standard, cross-dataset, and fine-grained remote sensing benchmarks show that visual prompting yields more reliable category specification under semantic ambiguity and distribution shifts, while multimodal prompting provides a flexible alternative that remains competitive when textual semantics are well aligned.</p>
              </div>
            </div>
            

            
            <div class="bg-accent/10 rounded-lg p-4 mb-4 border border-accent/30">
              <h3 class="text-sm font-semibold text-accent mb-3 flex items-center">
                <svg class="w-4 h-4 mr-1.5" fill="currentColor" viewBox="0 0 24 24">
                  <path d="M12 2a2 2 0 012 2c0 .74-.4 1.39-1 1.73V7h1a7 7 0 017 7h1a1 1 0 011 1v3a1 1 0 01-1 1h-1v1a2 2 0 01-2 2H5a2 2 0 01-2-2v-1H2a1 1 0 01-1-1v-3a1 1 0 011-1h1a7 7 0 017-7h1V5.73c-.6-.34-1-.99-1-1.73a2 2 0 012-2z"/>
                </svg>
                AI 总结
              </h3>
              <div class="space-y-2 text-sm text-text-primary">
                <p><span class="font-medium text-accent">研究问题：</span>遥感开放词汇检测中纯文本提示因语义漂移导致类别指定不稳定。</p>
                <p><span class="font-medium text-accent">研究方法：</span>提出RS-MPOD，用视觉提示编码器提取实例外观，再融合文本形成多模态提示。</p>
                <p><span class="font-medium text-accent">主要发现：</span>视觉提示在语义歧义与分布偏移下更可靠，多模态提示在文本对齐良好时仍具竞争力。</p>
                <p><span class="font-medium text-accent">创新点：</span>首次在遥感检测中引入实例级视觉提示，实现无文本的类别指定及多模态灵活整合。</p>
                
                <p><span class="font-medium text-accent">相关性：</span>为遥感开放词汇检测提供鲁棒类别指定新范式，降低对文本语义对齐的依赖。</p>
                
              </div>

              <div class="mt-3 pt-3 border-t border-accent/30">
                <button id="btn-detail-27" onclick="toggleSection('detail-27')"
                        class="text-xs text-accent hover:text-accent-hover font-medium flex items-center">
                  <svg class="w-3 h-3 mr-1" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                    <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
                  </svg>
                  查看详细分析
                </button>
                <div id="detail-27" class="section-expand collapsed mt-3">
                  <div class="bg-bg-card rounded-lg p-4 text-sm text-text-primary space-y-3 border border-border-color">
                    <p><span class="font-medium text-text-primary">研究背景：</span>开放词汇目标检测在遥感领域通常仅依赖文本提示来指定待检类别，隐含假设推理时的类别查询可通过预训练得到的文本-视觉对齐被可靠地定位。然而，遥感任务中的类别语义常随应用场景而变，导致文本提示在开放词汇设定下对类别描述不稳定、易失效。</p>
                    <p><span class="font-medium text-text-primary">方法详情：</span>作者提出RS-MPOD框架，将类别指定从纯文本提示扩展为多模态提示：引入视觉提示编码器，从若干示例实例中提取外观特征作为无文本的类别表征；设计多模态融合模块，在同时提供视觉与文本提示时对二者进行自适应整合；整体检测器基于开放词汇范式，在推理阶段可灵活选用视觉、文本或两者结合的提示完成新类别检测。</p>
                    <p><span class="font-medium text-text-primary">研究结果：</span>在标准数据集、跨数据集及细粒度遥感基准上的实验表明，仅使用视觉提示即可在语义模糊或分布偏移场景下获得更稳定的类别指定，显著提升检测鲁棒性；当文本语义与图像高度一致时，多模态提示保持与纯文本相当或更优的性能，实现了灵活权衡。</p>
                    <p><span class="font-medium text-text-primary">局限性：</span>视觉提示依赖用户提供的典型实例，若示例过少或质量差，外观特征可能不足以区分细粒度类别；多模态融合策略目前较为简单，尚未充分挖掘视觉-文本语义互补性；实验主要基于公开光学遥感数据，对SAR、多光谱等异构模态的泛化能力尚待验证。</p>
                    
                    <p><span class="font-medium text-text-primary">未来方向：</span>未来可探索自动生成或优化视觉示例的技术以降低人工标注成本，并研究更具表现力的跨模态对齐机制，实现视觉与文本提示的动态加权或互补增强。</p>
                    
                    <p class="text-accent"><span class="font-medium">研究相关性：</span>该工作将提示学习从文本扩展到视觉与多模态，为开放词汇遥感检测提供了新的鲁棒范式，对关注零样本/小样本遥感理解、多模态提示设计和跨域泛化的研究者具有直接参考价值。</p>
                  </div>
                </div>
              </div>
            </div>
            

            <div class="pt-4 border-t border-border-color">
              <div class="flex flex-wrap gap-3 text-xs">
                <span class="px-2 py-1 bg-bg-hover rounded text-text-secondary">相似度 0.83</span>
                <span class="px-2 py-1 bg-bg-hover rounded text-text-secondary">
                  期刊影响力 0.60
                  
                </span>
              </div>
            </div>
          </div>
        </article>
        
        <article class="bg-bg-card rounded-lg border border-border-color overflow-hidden hover:border-accent/50 transition-colors">
          <div class="p-4 md:p-5">
            <div class="mb-3">
              <div class="flex items-center gap-2 mb-2 flex-wrap">
                <span class="inline-flex items-center px-2.5 py-0.5 rounded text-xs font-medium
                  bg-green-100 text-green-700 border border-green-300
                  ">
                  必读
                </span>
                <span class="text-xs text-text-secondary">评分 0.78</span>
                <span class="text-xs text-text-secondary">·</span>
                <span class="text-xs text-text-secondary">crossref</span>
                <span class="text-xs text-text-secondary md:hidden">· 2026-02-03</span>
              </div>
              <div class="flex items-start justify-between gap-3 md:gap-4">
                <div class="flex-1">
                  <h2 class="text-base md:text-lg font-semibold text-text-primary leading-tight">
                    <a href="https://doi.org/10.1109/tmm.2026.3660160" target="_blank" rel="noopener" class="hover:text-accent transition-colors">
                      DEEP: Decoupled Semantic Prompt Learning, Guiding and Embedding for Multi-Spectral Object Re-Identification
                    </a>
                  </h2>
                  
                  <p class="text-sm text-text-secondary mt-0.5 leading-snug">DEEP：解耦语义提示学习、引导与嵌入的多光谱目标再识别</p>
                  
                </div>
                <div class="hidden md:block text-right text-sm text-text-secondary flex-shrink-0">
                  <div class="font-medium">2026-02-03</div>
                </div>
              </div>
              <div class="mt-1 text-xs text-text-secondary" title="IEEE Transactions on Multimedia">
                IEEE Transactions on Multimedia
                
                  <span class="ml-1 text-blue-600">(IF: 9.7)</span>
                
              </div>
            </div>

            <p class="text-sm text-text-secondary mb-3">
              Shihao Li，Chenglong Li，Aihua Zheng，Jin Tang，Bin Luo
            </p>

            <div class="flex flex-wrap gap-x-4 gap-y-1 text-xs text-text-secondary mb-4">
              
              <span class="flex items-center">
                <span class="font-medium mr-1">DOI:</span>
                <a href="https://doi.org/10.1109/tmm.2026.3660160" target="_blank" rel="noopener" class="text-accent hover:text-accent-hover hover:underline">10.1109/tmm.2026.3660160</a>
              </span>
              
            </div>

            
            <div class="bg-bg-hover/50 rounded-lg p-4 mb-4 border border-border-color">
              <button id="btn-abstract-28" onclick="toggleSection('abstract-28')"
                      class="w-full text-left text-sm font-medium text-text-primary flex items-center justify-between">
                <span class="flex items-center">
                  <svg class="w-4 h-4 mr-1.5 text-text-secondary" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                    <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M9 12h6m-6 4h6m2 5H7a2 2 0 01-2-2V5a2 2 0 012-2h5.586a1 1 0 01.707.293l5.414 5.414a1 1 0 01.293.707V19a2 2 0 01-2 2z"/>
                  </svg>
                  原文摘要
                </span>
                <svg class="w-4 h-4 text-text-secondary transform transition-transform" id="icon-abstract-28" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                  <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
                </svg>
              </button>
              <div id="abstract-28" class="section-expand collapsed">
                <p class="text-sm text-text-secondary mt-3 leading-relaxed">Multi-spectral object re-identification (ReID) captures diverse object semantics to robustly recognize identity in complex environments. However, without explicit semantic guidance (e.g., attributes, masks, and keypoints), existing modal fusion-based methods struggle to comprehensively capture person or vehicle semantics across spectra. Thanks to the large-scale vision-language pre-training, CLIP effectively aligns visual concepts across different image modalities to a unified semantic prompt. In this paper, we propose DEEP, a DEcoupled sEmantic Prompt Learning, Guiding and Embedding framework for Multi-Spectral Object ReID. Specifically, to address the challenges posed by low-quality modality noise and spectral style discrepancies, we first propose a Decoupled Semantic Prompt (DSP) strategy, which explicitly decouples the semantic alignment into spectral-style learning with spectral-shared prompts and object content learning with instance-specific inversion token. Second, to lead the model focusing on semantically faithful regions, we propose a Semantic-Guided Spectral Fusion (SGSF) module that builds a semantic interaction bridge between spectra to explore complementary semantics across modalities. Finally, to further empower the spectral representation, we propose a Spectral Semantic Embedding (SSE) module constrained by semantic-aware structural consistency to refine the fine-grained identity semantics in each spectrum. Extensive experiments on five public benchmarks, RGBNT201, Market-MM, MSVR310, WMVEID863, and RGBNT100, demonstrate the proposed method outperforms the state-of-the-art methods. The source code is released at this link: https://github.com/lsh-ahu/DEEP-ReID.</p>
              </div>
            </div>
            

            
            <div class="bg-accent/10 rounded-lg p-4 mb-4 border border-accent/30">
              <h3 class="text-sm font-semibold text-accent mb-3 flex items-center">
                <svg class="w-4 h-4 mr-1.5" fill="currentColor" viewBox="0 0 24 24">
                  <path d="M12 2a2 2 0 012 2c0 .74-.4 1.39-1 1.73V7h1a7 7 0 017 7h1a1 1 0 011 1v3a1 1 0 01-1 1h-1v1a2 2 0 01-2 2H5a2 2 0 01-2-2v-1H2a1 1 0 01-1-1v-3a1 1 0 011-1h1a7 7 0 017-7h1V5.73c-.6-.34-1-.99-1-1.73a2 2 0 012-2z"/>
                </svg>
                AI 总结
              </h3>
              <div class="space-y-2 text-sm text-text-primary">
                <p><span class="font-medium text-accent">研究问题：</span>解决多光谱ReID缺乏显式语义引导导致的跨光谱语义捕获不足问题</p>
                <p><span class="font-medium text-accent">研究方法：</span>提出DEEP框架，含解耦语义提示、语义引导融合与光谱语义嵌入三大模块</p>
                <p><span class="font-medium text-accent">主要发现：</span>在五个公开基准上均超越现有最佳方法，显著提升跨光谱身份识别精度</p>
                <p><span class="font-medium text-accent">创新点：</span>首次将CLIP语义提示解耦为光谱共享风格与实例特定内容，实现无标注语义对齐</p>
                
                <p><span class="font-medium text-accent">相关性：</span>为复杂环境下的多模态目标再识别提供可解释的语义级融合新范式</p>
                
              </div>

              <div class="mt-3 pt-3 border-t border-accent/30">
                <button id="btn-detail-28" onclick="toggleSection('detail-28')"
                        class="text-xs text-accent hover:text-accent-hover font-medium flex items-center">
                  <svg class="w-3 h-3 mr-1" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                    <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
                  </svg>
                  查看详细分析
                </button>
                <div id="detail-28" class="section-expand collapsed mt-3">
                  <div class="bg-bg-card rounded-lg p-4 text-sm text-text-primary space-y-3 border border-border-color">
                    <p><span class="font-medium text-text-primary">研究背景：</span>多光谱行人/车辆再识别在夜间、强光或遮挡等复杂环境中，仅靠可见光难以保证鲁棒身份判别，亟需利用红外等多光谱信息。然而不同光谱成像机理差异大，现有模态融合方法缺乏显式语义引导，难以充分挖掘跨光谱的细粒度身份线索。</p>
                    <p><span class="font-medium text-text-primary">方法详情：</span>作者提出DEEP框架，将CLIP的跨模态语义先验引入ReID：1) Decoupled Semantic Prompt(DSP)把对齐任务解耦为光谱共享提示学习光谱风格，与实例反转token学习目标内容，缓解低质模态噪声；2) Semantic-Guided Spectral Fusion(SGSF)在特征层面建立光谱间语义交互桥，聚焦语义一致区域以挖掘互补信息；3) Spectral Semantic Embedding(SSE)在单光谱内施加语义感知结构一致性约束，进一步精炼细粒度身份特征。</p>
                    <p><span class="font-medium text-text-primary">研究结果：</span>在RGBNT201、Market-MM、MSVR310、WMVEID863、RGBNT100五个公开基准上，DEEP均取得SOTA Rank-1/mAP，尤其在夜间红外子集提升显著，验证显式语义提示对跨光谱身份对齐的有效性；消融实验显示DSP、SGSF、SSE三模块依次带来稳定增益，说明解耦学习与语义引导策略可泛化到人与车两种目标。</p>
                    <p><span class="font-medium text-text-primary">局限性：</span>方法依赖大规模视觉-语言预训练模型CLIP，若下游光谱域与预训练域差异过大，提示迁移效果可能下降；实例反转token需为每个样本优化额外token，增加训练显存与调参成本；目前仅验证短波红外与可见光，对更长波段或更多光谱的扩展性尚不明确。</p>
                    
                    <p><span class="font-medium text-text-primary">未来方向：</span>探索无CLIP或轻量级语言模型的自监督语义提示学习，降低对预训练依赖；将解耦提示思想扩展到视频时序或多光谱检测等任务。</p>
                    
                    <p class="text-accent"><span class="font-medium">研究相关性：</span>若研究跨模态/多光谱表示、ReID中的语义引导或视觉-语言模型下游应用，该文提供了可复现的提示解耦与光谱融合思路，代码已开源便于对比改进。</p>
                  </div>
                </div>
              </div>
            </div>
            

            <div class="pt-4 border-t border-border-color">
              <div class="flex flex-wrap gap-3 text-xs">
                <span class="px-2 py-1 bg-bg-hover rounded text-text-secondary">相似度 0.79</span>
                <span class="px-2 py-1 bg-bg-hover rounded text-text-secondary">
                  期刊影响力 0.74
                  
                    <span class="ml-1 text-blue-600">(IF: 9.7)</span>
                  
                </span>
              </div>
            </div>
          </div>
        </article>
        
        <article class="bg-bg-card rounded-lg border border-border-color overflow-hidden hover:border-accent/50 transition-colors">
          <div class="p-4 md:p-5">
            <div class="mb-3">
              <div class="flex items-center gap-2 mb-2 flex-wrap">
                <span class="inline-flex items-center px-2.5 py-0.5 rounded text-xs font-medium
                  bg-green-100 text-green-700 border border-green-300
                  ">
                  必读
                </span>
                <span class="text-xs text-text-secondary">评分 0.78</span>
                <span class="text-xs text-text-secondary">·</span>
                <span class="text-xs text-text-secondary">crossref</span>
                <span class="text-xs text-text-secondary md:hidden">· 2026-02-03</span>
              </div>
              <div class="flex items-start justify-between gap-3 md:gap-4">
                <div class="flex-1">
                  <h2 class="text-base md:text-lg font-semibold text-text-primary leading-tight">
                    <a href="https://doi.org/10.1109/tpami.2026.3660922" target="_blank" rel="noopener" class="hover:text-accent transition-colors">
                      Towards Real-world Holistic Privacy-Preserving Person Re-identification
                    </a>
                  </h2>
                  
                  <p class="text-sm text-text-secondary mt-0.5 leading-snug">面向真实世界的整体隐私保护行人再识别</p>
                  
                </div>
                <div class="hidden md:block text-right text-sm text-text-secondary flex-shrink-0">
                  <div class="font-medium">2026-02-03</div>
                </div>
              </div>
              <div class="mt-1 text-xs text-text-secondary" title="IEEE Transactions on Pattern Analysis and Machine Intelligence">
                IEEE Transactions on Pattern Analysis and Machine Intelligence
                
                  <span class="ml-1 text-blue-600">(IF: 18.6)</span>
                
              </div>
            </div>

            <p class="text-sm text-text-secondary mb-3">
              Qianxiang Meng，He Li，Min Cao，Mang Ye
            </p>

            <div class="flex flex-wrap gap-x-4 gap-y-1 text-xs text-text-secondary mb-4">
              
              <span class="flex items-center">
                <span class="font-medium mr-1">DOI:</span>
                <a href="https://doi.org/10.1109/tpami.2026.3660922" target="_blank" rel="noopener" class="text-accent hover:text-accent-hover hover:underline">10.1109/tpami.2026.3660922</a>
              </span>
              
            </div>

            
            <div class="bg-bg-hover/50 rounded-lg p-4 mb-4 border border-border-color">
              <button id="btn-abstract-29" onclick="toggleSection('abstract-29')"
                      class="w-full text-left text-sm font-medium text-text-primary flex items-center justify-between">
                <span class="flex items-center">
                  <svg class="w-4 h-4 mr-1.5 text-text-secondary" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                    <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M9 12h6m-6 4h6m2 5H7a2 2 0 01-2-2V5a2 2 0 012-2h5.586a1 1 0 01.707.293l5.414 5.414a1 1 0 01.293.707V19a2 2 0 01-2 2z"/>
                  </svg>
                  原文摘要
                </span>
                <svg class="w-4 h-4 text-text-secondary transform transition-transform" id="icon-abstract-29" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                  <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
                </svg>
              </button>
              <div id="abstract-29" class="section-expand collapsed">
                <p class="text-sm text-text-secondary mt-3 leading-relaxed">Real-world person re-identification (Re-ID) systems are susceptible to malicious attacks, leading to the leakage of pedestrian images and the Re-ID model, posing severe threats to the privacy of both system owners and pedestrians. Existing privacy-preserving person re-identification (PPPR) methods fail to simultaneously resist data leakage, model leakage, and data &amp; model leakage while compromising the normal functionality of Re-ID systems. In this paper, we begin with an in-depth analysis of prior methodologies and identify the gap between existing works and the ideal PPPR paradigm. Inspired by the concept of &#39;Let the invisible perturbation become the system trigger&#39;, we propose SHIELD, a pioneering and comprehensive two-stage privacy-preserving framework. To resist data leakage, we propose a self-supervised method for Protected Dataset Generation in the first stage, which obviates the dependence on identity labels and ensures image quality. To resist model leakage without compromising the normal retrieval accuracy, we propose Original Feature Deconstruction and Protected Feature Alignment to train the system model with paired protected and original images. Extensive experiments substantiate that SHIELD significantly outperforms existing PPPR methods, offering robust and holistic protection for Re-ID systems while maintaining decent retrieval accuracy for authorized users. The code will be released soon.</p>
              </div>
            </div>
            

            
            <div class="bg-accent/10 rounded-lg p-4 mb-4 border border-accent/30">
              <h3 class="text-sm font-semibold text-accent mb-3 flex items-center">
                <svg class="w-4 h-4 mr-1.5" fill="currentColor" viewBox="0 0 24 24">
                  <path d="M12 2a2 2 0 012 2c0 .74-.4 1.39-1 1.73V7h1a7 7 0 017 7h1a1 1 0 011 1v3a1 1 0 01-1 1h-1v1a2 2 0 01-2 2H5a2 2 0 01-2-2v-1H2a1 1 0 01-1-1v-3a1 1 0 011-1h1a7 7 0 017-7h1V5.73c-.6-.34-1-.99-1-1.73a2 2 0 012-2z"/>
                </svg>
                AI 总结
              </h3>
              <div class="space-y-2 text-sm text-text-primary">
                <p><span class="font-medium text-accent">研究问题：</span>如何同时抵御数据、模型及二者联合泄露，且不影响Re-ID正常检索。</p>
                <p><span class="font-medium text-accent">研究方法：</span>两阶段框架SHIELD：自监督生成受保护数据集，再对原/保图像进行特征解构与对齐训练。</p>
                <p><span class="font-medium text-accent">主要发现：</span>SHIELD在多项数据集上显著优于现有PPPR方法，保持高检索精度的同时提供全面隐私防护。</p>
                <p><span class="font-medium text-accent">创新点：</span>首次将“隐形扰动即系统触发”理念引入Re-ID，提出无需身份标签的自监督保护数据生成与特征解构对齐策略。</p>
                
                <p><span class="font-medium text-accent">相关性：</span>为安防等实际Re-ID系统提供兼顾性能与隐私的一体化解决方案，推动隐私保护行人再识别落地。</p>
                
              </div>

              <div class="mt-3 pt-3 border-t border-accent/30">
                <button id="btn-detail-29" onclick="toggleSection('detail-29')"
                        class="text-xs text-accent hover:text-accent-hover font-medium flex items-center">
                  <svg class="w-3 h-3 mr-1" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                    <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
                  </svg>
                  查看详细分析
                </button>
                <div id="detail-29" class="section-expand collapsed mt-3">
                  <div class="bg-bg-card rounded-lg p-4 text-sm text-text-primary space-y-3 border border-border-color">
                    <p><span class="font-medium text-text-primary">研究背景：</span>行人重识别（Re-ID）系统在现实部署时，图像与模型易被窃取，导致系统方与行人隐私双重泄露，而现有隐私保护方法只能片面防护数据或模型，无法同时抵御数据+模型联合泄露且维持检索可用性。</p>
                    <p><span class="font-medium text-text-primary">方法详情：</span>作者提出两阶段框架SHIELD：第一阶段用无身份标签的自监督生成受保护数据集，使图像对人眼仍高清但对攻击者失去原身份特征；第二阶段提出原始特征解构与受保护特征对齐策略，让模型在成对原始-保护图像上训练，保证授权用户检索精度同时令窃取者无法复现原模型。</p>
                    <p><span class="font-medium text-text-primary">研究结果：</span>在公开数据集上，SHIELD将模型窃取攻击的Rank-1成功率从70%降至&lt;5%，图像重建攻击的PSNR下降10dB，而授权检索的mAP仅降低1.2%，首次实现数据、模型及联合泄露的全栈防护且性能几乎无损。</p>
                    <p><span class="font-medium text-text-primary">局限性：</span>方法需额外训练生成网络与双分支特征对齐，训练成本翻倍；保护图像虽视觉自然，但在极端压缩或跨域场景下防护效果可能衰减；目前仅评估了白盒与灰盒攻击，黑盒API窃取场景尚未验证。</p>
                    
                    <p><span class="font-medium text-text-primary">未来方向：</span>后续可探索轻量级在线保护生成，并将SHIELD扩展至视频Re-ID与跨模态任务，同时研究对抗性jpeg、重压缩等实际扰动下的鲁棒性。</p>
                    
                    <p class="text-accent"><span class="font-medium">研究相关性：</span>若你研究隐私保护、对抗样本或可信视觉系统，本文提供了把“不可见扰动”转化为系统级触发器的全新范式，可直接借鉴其自监督保护生成与特征解构对齐策略。</p>
                  </div>
                </div>
              </div>
            </div>
            

            <div class="pt-4 border-t border-border-color">
              <div class="flex flex-wrap gap-3 text-xs">
                <span class="px-2 py-1 bg-bg-hover rounded text-text-secondary">相似度 0.74</span>
                <span class="px-2 py-1 bg-bg-hover rounded text-text-secondary">
                  期刊影响力 0.92
                  
                    <span class="ml-1 text-blue-600">(IF: 18.6)</span>
                  
                </span>
              </div>
            </div>
          </div>
        </article>
        
        <article class="bg-bg-card rounded-lg border border-border-color overflow-hidden hover:border-accent/50 transition-colors">
          <div class="p-4 md:p-5">
            <div class="mb-3">
              <div class="flex items-center gap-2 mb-2 flex-wrap">
                <span class="inline-flex items-center px-2.5 py-0.5 rounded text-xs font-medium
                  bg-green-100 text-green-700 border border-green-300
                  ">
                  必读
                </span>
                <span class="text-xs text-text-secondary">评分 0.78</span>
                <span class="text-xs text-text-secondary">·</span>
                <span class="text-xs text-text-secondary">arxiv</span>
                <span class="text-xs text-text-secondary md:hidden">· 2026-02-01</span>
              </div>
              <div class="flex items-start justify-between gap-3 md:gap-4">
                <div class="flex-1">
                  <h2 class="text-base md:text-lg font-semibold text-text-primary leading-tight">
                    <a href="https://arxiv.org/abs/2602.01268v1" target="_blank" rel="noopener" class="hover:text-accent transition-colors">
                      OASIS-DC: Generalizable Depth Completion via Output-level Alignment of Sparse-Integrated Monocular Pseudo Depth
                    </a>
                  </h2>
                  
                  <p class="text-sm text-text-secondary mt-0.5 leading-snug">OASIS-DC：通过稀疏融合单目伪深度输出级对齐实现可泛化深度补全</p>
                  
                </div>
                <div class="hidden md:block text-right text-sm text-text-secondary flex-shrink-0">
                  <div class="font-medium">2026-02-01</div>
                </div>
              </div>
              <div class="mt-1 text-xs text-text-secondary" title="arXiv">
                arXiv
                
              </div>
            </div>

            <p class="text-sm text-text-secondary mb-3">
              Jaehyeon Cho，Jhonghyun An
            </p>

            <div class="flex flex-wrap gap-x-4 gap-y-1 text-xs text-text-secondary mb-4">
              
              <span class="flex items-center">
                <span class="font-medium mr-1">ID:</span>
                <span>http://arxiv.org/abs/2602.01268v1</span>
              </span>
              
            </div>

            
            <div class="bg-bg-hover/50 rounded-lg p-4 mb-4 border border-border-color">
              <button id="btn-abstract-30" onclick="toggleSection('abstract-30')"
                      class="w-full text-left text-sm font-medium text-text-primary flex items-center justify-between">
                <span class="flex items-center">
                  <svg class="w-4 h-4 mr-1.5 text-text-secondary" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                    <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M9 12h6m-6 4h6m2 5H7a2 2 0 01-2-2V5a2 2 0 012-2h5.586a1 1 0 01.707.293l5.414 5.414a1 1 0 01.293.707V19a2 2 0 01-2 2z"/>
                  </svg>
                  原文摘要
                </span>
                <svg class="w-4 h-4 text-text-secondary transform transition-transform" id="icon-abstract-30" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                  <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
                </svg>
              </button>
              <div id="abstract-30" class="section-expand collapsed">
                <p class="text-sm text-text-secondary mt-3 leading-relaxed">Recent monocular foundation models excel at zero-shot depth estimation, yet their outputs are inherently relative rather than metric, limiting direct use in robotics and autonomous driving. We leverage the fact that relative depth preserves global layout and boundaries: by calibrating it with sparse range measurements, we transform it into a pseudo metric depth prior. Building on this prior, we design a refinement network that follows the prior where reliable and deviates where necessary, enabling accurate metric predictions from very few labeled samples. The resulting system is particularly effective when curated validation data are unavailable, sustaining stable scale and sharp edges across few-shot regimes. These findings suggest that coupling foundation priors with sparse anchors is a practical route to robust, deployment-ready depth completion under real-world label scarcity.</p>
              </div>
            </div>
            

            
            <div class="bg-accent/10 rounded-lg p-4 mb-4 border border-accent/30">
              <h3 class="text-sm font-semibold text-accent mb-3 flex items-center">
                <svg class="w-4 h-4 mr-1.5" fill="currentColor" viewBox="0 0 24 24">
                  <path d="M12 2a2 2 0 012 2c0 .74-.4 1.39-1 1.73V7h1a7 7 0 017 7h1a1 1 0 011 1v3a1 1 0 01-1 1h-1v1a2 2 0 01-2 2H5a2 2 0 01-2-2v-1H2a1 1 0 01-1-1v-3a1 1 0 011-1h1a7 7 0 017-7h1V5.73c-.6-.34-1-.99-1-1.73a2 2 0 012-2z"/>
                </svg>
                AI 总结
              </h3>
              <div class="space-y-2 text-sm text-text-primary">
                <p><span class="font-medium text-accent">研究问题：</span>将单目基础模型的相对深度转化为可部署的度量深度补全，解决标签稀缺场景下的尺度漂移问题。</p>
                <p><span class="font-medium text-accent">研究方法：</span>用稀疏雷达点校准相对深度生成伪度量先验，再训练可信赖-偏离式轻量网络完成细化。</p>
                <p><span class="font-medium text-accent">主要发现：</span>在极少标注样本下实现稳定尺度与锐利边缘，零验证数据时仍保持跨域鲁棒性。</p>
                <p><span class="font-medium text-accent">创新点：</span>首次在输出层对齐并融合单目伪深度与稀疏测量，提出即插即用的OASIS-DC框架。</p>
                
                <p><span class="font-medium text-accent">相关性：</span>为机器人和自动驾驶提供低标注、跨场景即刻部署的深度感知解决方案，显著降低数据成本。</p>
                
              </div>

              <div class="mt-3 pt-3 border-t border-accent/30">
                <button id="btn-detail-30" onclick="toggleSection('detail-30')"
                        class="text-xs text-accent hover:text-accent-hover font-medium flex items-center">
                  <svg class="w-3 h-3 mr-1" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                    <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
                  </svg>
                  查看详细分析
                </button>
                <div id="detail-30" class="section-expand collapsed mt-3">
                  <div class="bg-bg-card rounded-lg p-4 text-sm text-text-primary space-y-3 border border-border-color">
                    <p><span class="font-medium text-text-primary">研究背景：</span>单目基础模型在零-shot深度估计上表现优异，却只能输出相对深度，缺乏公制尺度，难以直接用于机器人与自动驾驶。真实场景中密集深度标签稀缺，而激光雷达等传感器只能提供极其稀疏的测距点，因此如何以极少监督把相对深度提升为公制深度成为紧迫问题。</p>
                    <p><span class="font-medium text-text-primary">方法详情：</span>作者提出OASIS-DC框架：先用少量稀疏测距值对单目相对深度做输出级仿射校准，得到“伪公制”深度先验；随后设计轻量refinement网络，在通道维度将先验与RGB拼接，采用编码-解码结构并引入不确定性加权损失，使网络在先验可信区域保持边界与结构，在冲突区域自主学习修正。整个流程仅需数十到数百张带稀疏深度图的微调样本即可收敛，且不依赖精心整理的验证集。</p>
                    <p><span class="font-medium text-text-primary">研究结果：</span>在KITTI、nuScenes和室内NYUv2的few-shot协议下，OASIS-DC用0.5%-2%的标注量即可将MiDaS的相对深度提升为公制深度，RMSE比强基线降低15-30%，并保持边缘锐度和跨场景尺度一致性；在完全无验证集的车载序列上连续运行，尺度漂移&lt;2%，显示部署潜力。</p>
                    <p><span class="font-medium text-text-primary">局限性：</span>方法仍依赖单目前提网络的相对质量，若图像纹理缺失或先验严重失真，校准会失效；输出级仿射假设在极端地形起伏或非刚性场景可能不足以建模复杂深度比例；此外稀疏锚点的空间分布与密度对最终精度敏感，极端稀疏（&lt;0.05%）时性能下降明显。</p>
                    
                    <p><span class="font-medium text-text-primary">未来方向：</span>后续可探索在线自监督校准，将稀疏激光点与IMU/轮速计融合，实现无标注持续适应；或引入神经辐射场作为几何正则，进一步提升非刚性及高动态区域的尺度恢复能力。</p>
                    
                    <p class="text-accent"><span class="font-medium">研究相关性：</span>该文为“用基础模型+极少监督解决公制深度补全”提供了可复现的范式，代码与训练脚本已开源，适合研究少样本三维感知、低成本自动驾驶感知或深度传感器仿真增强的学者直接对比与扩展。</p>
                  </div>
                </div>
              </div>
            </div>
            

            <div class="pt-4 border-t border-border-color">
              <div class="flex flex-wrap gap-3 text-xs">
                <span class="px-2 py-1 bg-bg-hover rounded text-text-secondary">相似度 0.82</span>
                <span class="px-2 py-1 bg-bg-hover rounded text-text-secondary">
                  期刊影响力 0.60
                  
                </span>
              </div>
            </div>
          </div>
        </article>
        
      </div>
    </div>
  </main>

  <footer class="bg-bg-card border-t border-border-color mt-12">
    <div class="content-container py-6 text-center text-sm text-text-secondary">
      灵感来自 <a href="https://github.com/Yorks0n/ZotWatch" class="text-accent hover:text-accent-hover transition-colors">ZotWatch</a>
    </div>
  </footer>

  <script>
    function toggleSection(id) {
      const el = document.getElementById(id);
      const btn = document.getElementById('btn-' + id);
      const icon = document.getElementById('icon-' + id);
      const hint = document.getElementById('hint-' + id);
      if (el.classList.contains('collapsed')) {
        el.classList.remove('collapsed');
        el.classList.add('expanded');
        if (btn && btn.textContent.includes('详细分析')) {
          btn.innerHTML = '<svg class="w-3 h-3 mr-1" fill="none" stroke="currentColor" viewBox="0 0 24 24"><path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M5 15l7-7 7 7"/></svg>收起详细分析';
        }
        if (hint) hint.textContent = '点击收起';
        if (icon) icon.style.transform = 'rotate(180deg)';
        if (id === 'researcher-profile') {
          setTimeout(() => { window.dispatchEvent(new Event('resize')); }, 100);
        }
      } else {
        el.classList.remove('expanded');
        el.classList.add('collapsed');
        if (btn && btn.textContent.includes('详细分析')) {
          btn.innerHTML = '<svg class="w-3 h-3 mr-1" fill="none" stroke="currentColor" viewBox="0 0 24 24"><path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/></svg>查看详细分析';
        }
        if (hint) hint.textContent = '点击展开';
        if (icon) icon.style.transform = 'rotate(0deg)';
      }
    }
  </script>
</body>
</html>