<!DOCTYPE html>
<html lang="zh-CN">
<head>
  <meta charset="utf-8">
  <meta name="viewport" content="width=device-width, initial-scale=1">
  <title>ZotWatch 文献推荐 - 2026-01-23</title>
  <script src="https://cdn.tailwindcss.com"></script>
  <script src="https://cdn.jsdelivr.net/npm/chart.js@4.4.1/dist/chart.umd.min.js"></script>
  <script src="https://d3js.org/d3.v7.min.js"></script>
  <script>
    tailwind.config = {
      theme: {
        extend: {
          colors: {
            'bg-primary': '#f8fafc',
            'bg-card': '#ffffff',
            'bg-hover': '#f1f5f9',
            'text-primary': '#1e293b',
            'text-secondary': '#64748b',
            'border-color': '#e2e8f0',
            'accent': '#2563eb',
            'accent-hover': '#1d4ed8',
          }
        }
      }
    }
  </script>
  <style>
    .section-expand { transition: max-height 0.5s ease-out; overflow: hidden; }
    .section-expand.collapsed { max-height: 0; }
    .section-expand.expanded { max-height: none; }
    body { font-family: -apple-system, BlinkMacSystemFont, 'Segoe UI', Roboto, 'Helvetica Neue', Arial, sans-serif; }
    .content-container { max-width: 48rem; margin: 0 auto; padding: 0 1rem; }
    @media (min-width: 768px) { .content-container { padding: 0 2rem; } }
    @media (min-width: 1024px) { .content-container { padding: 0 4rem; } }

    .cluster-graph-container { width: 100%; height: 280px; touch-action: pan-y; }
    @media (min-width: 640px) { .cluster-graph-container { height: 320px; } }
    @media (min-width: 768px) { .cluster-graph-container { height: 350px; } }
    #cluster-graph svg { overflow: visible; }
  </style>
</head>
<body class="bg-bg-primary min-h-screen text-text-primary">
  <header class="bg-bg-card border-b border-border-color">
    <div class="content-container py-6 md:py-8">
      <h1 class="text-xl md:text-2xl font-bold text-text-primary">ZotWatch 文献推荐</h1>
      <p class="text-xs md:text-sm text-text-secondary mt-1">
        共 35 篇论文 ·
        生成于 2026-01-23 10:48 Asia/Shanghai
      </p>
    </div>
  </header>

  
  <section class="py-4 md:py-6 border-b border-border-color">
    <div class="content-container">
      <button id="btn-researcher-profile" onclick="toggleSection('researcher-profile')"
              class="w-full text-left text-lg md:text-xl font-bold text-text-primary mb-3 md:mb-4 flex items-center justify-between group hover:text-accent transition-colors">
        <span class="flex items-center gap-2 flex-1 min-w-0">
          <svg class="w-5 h-5 flex-shrink-0 text-accent" fill="currentColor" viewBox="0 0 24 24">
            <path d="M12 2C6.48 2 2 6.48 2 12s4.48 10 10 10 10-4.48 10-10S17.52 2 12 2zm0 3c1.66 0 3 1.34 3 3s-1.34 3-3 3-3-1.34-3-3 1.34-3 3-3zm0 14.2c-2.5 0-4.71-1.28-6-3.22.03-1.99 4-3.08 6-3.08 1.99 0 5.97 1.09 6 3.08-1.29 1.94-3.5 3.22-6 3.22z"/>
          </svg>
          <span class="truncate">研究兴趣画像</span>
        </span>
        <span class="flex items-center gap-1 md:gap-2 flex-shrink-0 ml-2">
          <span class="hidden md:inline text-sm font-normal text-text-secondary group-hover:text-accent" id="hint-researcher-profile">点击展开</span>
          <svg class="w-5 h-5 md:w-4 md:h-4 text-text-secondary group-hover:text-accent transform transition-transform" id="icon-researcher-profile" fill="none" stroke="currentColor" viewBox="0 0 24 24">
            <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
          </svg>
        </span>
      </button>

      <div id="researcher-profile" class="section-expand collapsed">
      <div class="bg-bg-card rounded-lg border border-border-color p-4 mb-4">
        <div class="grid grid-cols-4 gap-3 text-center">
          <div>
            <div class="text-xl font-bold text-accent">968</div>
            <div class="text-xs text-text-secondary">收藏论文</div>
          </div>
          <div>
            <div class="text-xl font-bold text-accent">5年9月</div>
            <div class="text-xs text-text-secondary">收藏时长</div>
          </div>
          <div>
            <div class="text-xl font-bold text-accent">&gt;10</div>
            <div class="text-xs text-text-secondary">兴趣领域</div>
          </div>
          <div>
            <div class="text-xl font-bold text-accent">9</div>
            <div class="text-xs text-text-secondary">高频作者</div>
          </div>
        </div>
      </div>

      
      <div class="bg-accent/10 rounded-lg border border-accent/30 p-4 mb-4">
        <h3 class="text-sm font-semibold text-accent mb-2 flex items-center gap-2">
          <svg class="w-4 h-4" fill="currentColor" viewBox="0 0 24 24">
            <path d="M12 2a2 2 0 012 2c0 .74-.4 1.39-1 1.73V7h1a7 7 0 017 7h1a1 1 0 011 1v3a1 1 0 01-1 1h-1v1a2 2 0 01-2 2H5a2 2 0 01-2-2v-1H2a1 1 0 01-1-1v-3a1 1 0 011-1h1a7 7 0 017-7h1V5.73c-.6-.34-1-.99-1-1.73a2 2 0 012-2z"/>
          </svg>
          AI 研究兴趣分析
        </h3>
        <div class="space-y-2 text-xs text-text-primary leading-relaxed">
          <p><span class="font-medium text-accent">研究兴趣：</span>用户长期关注计算机视觉中的目标检测与视觉定位，同时对模型压缩与高效推理保持浓厚兴趣；近年将合成孔径雷达（SAR）图像理解与自监督、大模型技术结合，形成遥感+CV的交叉阅读主线。</p>
          <p><span class="font-medium text-accent">深度关注：</span>在目标检测、视觉定位及模型压缩方向持续深耕，收藏了Kaiming He、Ross Girshick等顶级团队的经典与最新工作，并系统追踪CVPR、ICCV、TPAMI等旗舰会议期刊，累计形成超百篇高质量文献库。</p>
          <p><span class="font-medium text-accent">跨学科倾向：</span>用户明显跨越纯计算机视觉与遥感信息处理，既细读SAR目标识别、旋转目标检测等遥感专用问题，也同步关注大语言模型、扩散模型等通用AI前沿，体现出“遥感场景+通用视觉方法”的双线融合阅读策略。</p>
          <p><span class="font-medium text-accent">兴趣演变：</span>2025-Q1出现收藏峰值102篇，新增关键词聚焦“合成孔径雷达目标识别、多任务学习、自动驾驶感知”，显示正把检测/定位技术向遥感与自动驾驶两大应用场景延伸；2024-Q3后季度阅读量回落但2025-Q4再度回升，预示新一轮方向聚焦或论文批量阅读阶段。</p>
          <p><span class="font-medium text-accent">阅读建议：</span>可进一步关注多模态遥感（SAR-光学-激光雷达融合）与视觉-语言模型在遥感解释中的结合，以及端侧高效检测在自动驾驶实时感知中的量化/蒸馏新方案，将已有的模型压缩积累直接迁移到新兴应用场景。</p>
        </div>
      </div>
      

      <div class="grid grid-cols-1 md:grid-cols-2 gap-4 mb-4">
        
        <div class="bg-bg-card rounded-lg border border-border-color p-4">
          <h4 class="text-sm font-medium text-text-primary mb-3">研究领域分布</h4>
          <div style="height: 180px;">
            <canvas id="domainsChart"></canvas>
          </div>
        </div>
        

        
        <div class="bg-bg-card rounded-lg border border-border-color p-4">
          <h4 class="text-sm font-medium text-text-primary mb-3">阅读趋势（季度）</h4>
          <div style="height: 180px;">
            <canvas id="trendsChart"></canvas>
          </div>
        </div>
        
      </div>

      
      <div class="bg-bg-card rounded-lg border border-border-color p-4 mb-4">
        <h4 class="text-sm font-medium text-text-primary mb-3">论文发表年份分布</h4>
        <div style="height: 180px;">
          <canvas id="yearDistChart"></canvas>
        </div>
      </div>
      

      
      <div class="bg-bg-card rounded-lg border border-border-color p-4 mb-4">
        <h4 class="text-sm font-medium text-text-primary mb-3">
          研究方向聚类图谱
          <span class="text-xs text-text-secondary font-normal ml-2">(30 个聚类)</span>
        </h4>
        <div id="cluster-graph" class="cluster-graph-container"></div>
        <div class="text-xs text-text-secondary mt-3">
          覆盖 942/942 篇论文（仅含摘要的文献参与聚类）
        </div>
      </div>
      

      <div class="grid grid-cols-1 md:grid-cols-2 gap-4 mb-4">
        
        <div class="bg-bg-card rounded-lg border border-border-color p-4">
          <h4 class="text-sm font-medium text-text-primary mb-2">高频关注作者</h4>
          <div class="grid grid-cols-2 gap-x-4 gap-y-1 text-xs">
            
            <div class="flex justify-between truncate">
              <span class="text-text-primary truncate" title="Kaiming He">Kaiming He</span>
              <span class="text-text-secondary ml-1 flex-shrink-0">24</span>
            </div>
            
            <div class="flex justify-between truncate">
              <span class="text-text-primary truncate" title="Ross Girshick">Ross Girshick</span>
              <span class="text-text-secondary ml-1 flex-shrink-0">18</span>
            </div>
            
            <div class="flex justify-between truncate">
              <span class="text-text-primary truncate" title="Jian Sun">Jian Sun</span>
              <span class="text-text-secondary ml-1 flex-shrink-0">15</span>
            </div>
            
            <div class="flex justify-between truncate">
              <span class="text-text-primary truncate" title="Song Han">Song Han</span>
              <span class="text-text-secondary ml-1 flex-shrink-0">14</span>
            </div>
            
            <div class="flex justify-between truncate">
              <span class="text-text-primary truncate" title="Xiangyu Zhang">Xiangyu Zhang</span>
              <span class="text-text-secondary ml-1 flex-shrink-0">14</span>
            </div>
            
            <div class="flex justify-between truncate">
              <span class="text-text-primary truncate" title="Piotr Dollár">Piotr Dollár</span>
              <span class="text-text-secondary ml-1 flex-shrink-0">12</span>
            </div>
            
            <div class="flex justify-between truncate">
              <span class="text-text-primary truncate" title="Li Liu">Li Liu</span>
              <span class="text-text-secondary ml-1 flex-shrink-0">11</span>
            </div>
            
            <div class="flex justify-between truncate">
              <span class="text-text-primary truncate" title="Xiang Li">Xiang Li</span>
              <span class="text-text-secondary ml-1 flex-shrink-0">11</span>
            </div>
            
            <div class="flex justify-between truncate">
              <span class="text-text-primary truncate" title="Wenfeng Liang">Wenfeng Liang</span>
              <span class="text-text-secondary ml-1 flex-shrink-0">10</span>
            </div>
            
            <div class="flex justify-between truncate">
              <span class="text-text-primary truncate" title="Yann LeCun">Yann LeCun</span>
              <span class="text-text-secondary ml-1 flex-shrink-0">9</span>
            </div>
            
            <div class="flex justify-between truncate">
              <span class="text-text-primary truncate" title="Zhenda Xie">Zhenda Xie</span>
              <span class="text-text-secondary ml-1 flex-shrink-0">9</span>
            </div>
            
            <div class="flex justify-between truncate">
              <span class="text-text-primary truncate" title="Feng Xu">Feng Xu</span>
              <span class="text-text-secondary ml-1 flex-shrink-0">9</span>
            </div>
            
          </div>
        </div>
        

        
        <div class="bg-bg-card rounded-lg border border-border-color p-4">
          <h4 class="text-sm font-medium text-text-primary mb-2">常关注期刊/会议</h4>
          <div class="space-y-1 text-xs">
            
            <div class="flex items-center justify-between">
              <span class="text-text-primary truncate flex-1" title="Computer Vision and Pattern Recognition">Computer Vision and Pattern Recognition</span>
              <span class="text-xs px-1 py-0.5 rounded bg-blue-100 text-blue-700 ml-1 flex-shrink-0">
                期刊
              </span>
              <span class="text-text-secondary ml-1 flex-shrink-0">114</span>
            </div>
            
            <div class="flex items-center justify-between">
              <span class="text-text-primary truncate flex-1" title="IEEE Transactions on Geoscience and Remote Sensing">IEEE Transactions on Geoscience and Remote Sensing</span>
              <span class="text-xs px-1 py-0.5 rounded bg-blue-100 text-blue-700 ml-1 flex-shrink-0">
                期刊
              </span>
              <span class="text-text-secondary ml-1 flex-shrink-0">48</span>
            </div>
            
            <div class="flex items-center justify-between">
              <span class="text-text-primary truncate flex-1" title="Neural Information Processing Systems">Neural Information Processing Systems</span>
              <span class="text-xs px-1 py-0.5 rounded bg-blue-100 text-blue-700 ml-1 flex-shrink-0">
                期刊
              </span>
              <span class="text-text-secondary ml-1 flex-shrink-0">46</span>
            </div>
            
            <div class="flex items-center justify-between">
              <span class="text-text-primary truncate flex-1" title="International Conference on Computer Vision">International Conference on Computer Vision</span>
              <span class="text-xs px-1 py-0.5 rounded bg-amber-100 text-amber-700 ml-1 flex-shrink-0">
                会议
              </span>
              <span class="text-text-secondary ml-1 flex-shrink-0">38</span>
            </div>
            
            <div class="flex items-center justify-between">
              <span class="text-text-primary truncate flex-1" title="IEEE Transactions on Pattern Analysis and Machine Intelligence">IEEE Transactions on Pattern Analysis and Machine Intelligence</span>
              <span class="text-xs px-1 py-0.5 rounded bg-blue-100 text-blue-700 ml-1 flex-shrink-0">
                期刊
              </span>
              <span class="text-text-secondary ml-1 flex-shrink-0">32</span>
            </div>
            
            <div class="flex items-center justify-between">
              <span class="text-text-primary truncate flex-1" title="International Conference on Learning Representations">International Conference on Learning Representations</span>
              <span class="text-xs px-1 py-0.5 rounded bg-amber-100 text-amber-700 ml-1 flex-shrink-0">
                会议
              </span>
              <span class="text-text-secondary ml-1 flex-shrink-0">29</span>
            </div>
            
            <div class="flex items-center justify-between">
              <span class="text-text-primary truncate flex-1" title="European Conference on Computer Vision">European Conference on Computer Vision</span>
              <span class="text-xs px-1 py-0.5 rounded bg-amber-100 text-amber-700 ml-1 flex-shrink-0">
                会议
              </span>
              <span class="text-text-secondary ml-1 flex-shrink-0">23</span>
            </div>
            
            <div class="flex items-center justify-between">
              <span class="text-text-primary truncate flex-1" title="IEEE Geoscience and Remote Sensing Letters">IEEE Geoscience and Remote Sensing Letters</span>
              <span class="text-xs px-1 py-0.5 rounded bg-blue-100 text-blue-700 ml-1 flex-shrink-0">
                期刊
              </span>
              <span class="text-text-secondary ml-1 flex-shrink-0">22</span>
            </div>
            
          </div>
        </div>
        
      </div>

      
      <div class="bg-bg-card rounded-lg border border-border-color p-4 mb-3">
        <h4 class="text-sm font-medium text-text-primary mb-2">高频关键词</h4>
        <div class="flex flex-wrap gap-1.5">
          
          <span class="px-2 py-0.5 bg-bg-hover rounded text-xs text-text-primary">
            大语言模型 <span class="text-text-secondary">(14)</span>
          </span>
          
          <span class="px-2 py-0.5 bg-bg-hover rounded text-xs text-text-primary">
            合成孔径雷达 <span class="text-text-secondary">(12)</span>
          </span>
          
          <span class="px-2 py-0.5 bg-bg-hover rounded text-xs text-text-primary">
            自监督学习 <span class="text-text-secondary">(11)</span>
          </span>
          
          <span class="px-2 py-0.5 bg-bg-hover rounded text-xs text-text-primary">
            综述 <span class="text-text-secondary">(9)</span>
          </span>
          
          <span class="px-2 py-0.5 bg-bg-hover rounded text-xs text-text-primary">
            域自适应 <span class="text-text-secondary">(9)</span>
          </span>
          
          <span class="px-2 py-0.5 bg-bg-hover rounded text-xs text-text-primary">
            目标检测 <span class="text-text-secondary">(8)</span>
          </span>
          
          <span class="px-2 py-0.5 bg-bg-hover rounded text-xs text-text-primary">
            深度学习 <span class="text-text-secondary">(8)</span>
          </span>
          
          <span class="px-2 py-0.5 bg-bg-hover rounded text-xs text-text-primary">
            迁移学习 <span class="text-text-secondary">(7)</span>
          </span>
          
          <span class="px-2 py-0.5 bg-bg-hover rounded text-xs text-text-primary">
            扩散模型 <span class="text-text-secondary">(7)</span>
          </span>
          
          <span class="px-2 py-0.5 bg-bg-hover rounded text-xs text-text-primary">
            强化学习 <span class="text-text-secondary">(7)</span>
          </span>
          
          <span class="px-2 py-0.5 bg-bg-hover rounded text-xs text-text-primary">
            注意力机制 <span class="text-text-secondary">(6)</span>
          </span>
          
          <span class="px-2 py-0.5 bg-bg-hover rounded text-xs text-text-primary">
            重参数化 <span class="text-text-secondary">(6)</span>
          </span>
          
          <span class="px-2 py-0.5 bg-bg-hover rounded text-xs text-text-primary">
            DeepSeek <span class="text-text-secondary">(6)</span>
          </span>
          
          <span class="px-2 py-0.5 bg-bg-hover rounded text-xs text-text-primary">
            生成对抗网络 <span class="text-text-secondary">(6)</span>
          </span>
          
          <span class="px-2 py-0.5 bg-bg-hover rounded text-xs text-text-primary">
            基础模型 <span class="text-text-secondary">(5)</span>
          </span>
          
          <span class="px-2 py-0.5 bg-bg-hover rounded text-xs text-text-primary">
            Transformers <span class="text-text-secondary">(5)</span>
          </span>
          
          <span class="px-2 py-0.5 bg-bg-hover rounded text-xs text-text-primary">
            卷积神经网络 <span class="text-text-secondary">(5)</span>
          </span>
          
          <span class="px-2 py-0.5 bg-bg-hover rounded text-xs text-text-primary">
            旋转目标检测 <span class="text-text-secondary">(5)</span>
          </span>
          
          <span class="px-2 py-0.5 bg-bg-hover rounded text-xs text-text-primary">
            知识蒸馏 <span class="text-text-secondary">(5)</span>
          </span>
          
          <span class="px-2 py-0.5 bg-bg-hover rounded text-xs text-text-primary">
            SAR目标识别 <span class="text-text-secondary">(5)</span>
          </span>
          
        </div>
      </div>
      

      <div class="text-xs text-text-secondary text-right">
        画像生成于 2026-01-23 10:34 Asia/Shanghai
        · kimi-k2-turbo-preview
      </div>
      </div>
    </div>
  </section>
  

  
  <script>
    document.addEventListener('DOMContentLoaded', function() {
      Chart.defaults.font.family = "-apple-system, BlinkMacSystemFont, 'Segoe UI', Roboto, 'Helvetica Neue', Arial, sans-serif";
      Chart.defaults.font.size = 11;
      Chart.defaults.color = '#64748b';
      Chart.defaults.animation = false;

      
      const domainsCtx = document.getElementById('domainsChart');
      if (domainsCtx) {
        new Chart(domainsCtx, {
          type: 'bar',
          data: {
            labels: ['目标检测', '视觉定位', '模型压缩', '姿态估计', '对比学习', '人脸识别', 'Transformer', '车牌识别'],
            datasets: [{
              data: [35, 27, 21, 15, 11, 10, 9, 7],
              backgroundColor: 'rgba(37, 99, 235, 0.7)',
              borderColor: 'rgba(37, 99, 235, 1)',
              borderWidth: 1,
              borderRadius: 2,
              barThickness: 16
            }]
          },
          options: {
            indexAxis: 'y',
            responsive: true,
            maintainAspectRatio: false,
            plugins: {
              legend: { display: false },
              tooltip: { enabled: false }
            },
            scales: {
              x: {
                beginAtZero: true,
                grid: { color: 'rgba(226, 232, 240, 0.5)' },
                ticks: { precision: 0 }
              },
              y: {
                grid: { display: false },
                ticks: { font: { size: 10 } }
              }
            }
          }
        });
      }
      

      
      const trendsCtx = document.getElementById('trendsChart');
      if (trendsCtx) {
        const trends = [{ q: '2024-Q1', c: 68 }, { q: '2024-Q2', c: 18 }, { q: '2024-Q3', c: 4 }, { q: '2024-Q4', c: 24 }, { q: '2025-Q1', c: 102 }, { q: '2025-Q2', c: 34 }, { q: '2025-Q3', c: 14 }, { q: '2025-Q4', c: 30 }, { q: '2026-Q1', c: 5 }];
        const monthColors = [
          { bg: 'rgba(59, 130, 246, 0.7)', border: 'rgba(59, 130, 246, 1)' },
          { bg: 'rgba(16, 185, 129, 0.7)', border: 'rgba(16, 185, 129, 1)' },
          { bg: 'rgba(245, 158, 11, 0.7)', border: 'rgba(245, 158, 11, 1)' }
        ];
        const bgColors = trends.map((t, i) => monthColors[i % 3].bg);
        const borderColors = trends.map((t, i) => monthColors[i % 3].border);
        new Chart(trendsCtx, {
          type: 'bar',
          data: {
            labels: trends.map(t => t.q.replace('-', '\n')),
            datasets: [{
              data: trends.map(t => t.c),
              backgroundColor: bgColors,
              borderColor: borderColors,
              borderWidth: 1,
              borderRadius: 2,
              barThickness: 20
            }]
          },
          options: {
            responsive: true,
            maintainAspectRatio: false,
            plugins: {
              legend: { display: false },
              tooltip: { enabled: false }
            },
            scales: {
              x: {
                grid: { display: false },
                ticks: { font: { size: 9 }, maxRotation: 0 }
              },
              y: {
                beginAtZero: true,
                grid: { color: 'rgba(226, 232, 240, 0.5)' },
                ticks: { precision: 0 }
              }
            }
          }
        });
      }
      

      
      const yearDistCtx = document.getElementById('yearDistChart');
      if (yearDistCtx) {
        const yearData = [{ year: 2007, count: 3 }, { year: 2008, count: 2 }, { year: 2009, count: 2 }, { year: 2010, count: 9 }, { year: 2011, count: 10 }, { year: 2012, count: 3 }, { year: 2013, count: 12 }, { year: 2014, count: 14 }, { year: 2015, count: 18 }, { year: 2016, count: 15 }, { year: 2017, count: 41 }, { year: 2018, count: 59 }, { year: 2019, count: 56 }, { year: 2020, count: 67 }, { year: 2021, count: 84 }, { year: 2022, count: 113 }, { year: 2023, count: 111 }, { year: 2024, count: 114 }, { year: 2025, count: 180 }, { year: 2026, count: 5 }];
        new Chart(yearDistCtx, {
          type: 'line',
          data: {
            labels: yearData.map(d => d.year),
            datasets: [{
              data: yearData.map(d => d.count),
              borderColor: 'rgba(37, 99, 235, 1)',
              backgroundColor: 'rgba(37, 99, 235, 0.1)',
              borderWidth: 2,
              fill: true,
              tension: 0.3,
              pointRadius: 2,
              pointHoverRadius: 4
            }]
          },
          options: {
            responsive: true,
            maintainAspectRatio: false,
            plugins: {
              legend: { display: false },
              tooltip: {
                callbacks: {
                  label: function(context) {
                    return context.parsed.y + ' 篇';
                  }
                }
              }
            },
            scales: {
              x: {
                grid: { display: false },
                ticks: {
                  font: { size: 9 },
                  maxRotation: 45,
                  callback: function(value, index) {
                    const year = this.getLabelForValue(value);
                    return year % 5 === 0 ? year : '';
                  }
                }
              },
              y: {
                beginAtZero: true,
                grid: { color: 'rgba(226, 232, 240, 0.5)' },
                ticks: { precision: 0 }
              }
            }
          }
        });
      }
      

      
      const clusterContainer = document.getElementById('cluster-graph');
      if (clusterContainer) {
        const clusters = [
          
          {
            id: 0,
            label: "\u5927\u6a21\u578bMoE\u4e0e\u63a8\u7406\u4f18\u5316",
            size: 76,
            keywords: ["\u5927\u8bed\u8a00\u6a21\u578b", "DeepSeek", "\u6df7\u5408\u4e13\u5bb6\u6a21\u578b"]
          },
          
          {
            id: 1,
            label: "\u81ea\u76d1\u7763\u89c6\u89c9\u8868\u5f81\u5b66\u4e60",
            size: 54,
            keywords: ["\u81ea\u76d1\u7763\u5b66\u4e60", "\u5bf9\u6bd4\u5b66\u4e60", "\u89c6\u89c9Transformer"]
          },
          
          {
            id: 2,
            label: "\u8f7b\u91cf\u7ea7CNN\u4e0eTransformer\u8bbe\u8ba1",
            size: 54,
            keywords: ["\u91cd\u53c2\u6570\u5316", "\u8f7b\u91cf\u7ea7\u6a21\u578b", "VGG"]
          },
          
          {
            id: 3,
            label: "SAR\u8230\u8239\u68c0\u6d4b",
            size: 45,
            keywords: ["\u5408\u6210\u5b54\u5f84\u96f7\u8fbe", "SAR\u8230\u8239\u68c0\u6d4b", "\u591a\u5c3a\u5ea6\u7279\u5f81\u878d\u5408"]
          },
          
          {
            id: 4,
            label: "\u6a21\u578b\u91cf\u5316\u538b\u7f29",
            size: 42,
            keywords: ["\u6a21\u578b\u538b\u7f29", "\u6a21\u578b\u91cf\u5316", "\u6574\u6570\u63a8\u7406"]
          },
          
          {
            id: 5,
            label: "SLAM\u4e0e\u5e95\u5c42\u7279\u5f81",
            size: 40,
            keywords: ["SIFT", "\u7814\u7a76", "\u5e95\u5c42\u7b97\u6cd5"]
          },
          
          {
            id: 6,
            label: "CNN\u53ef\u89e3\u91ca\u6027",
            size: 39,
            keywords: ["\u7279\u5f81\u53ef\u89c6\u5316", "Grad-CAM", "\u4eba\u5de5\u795e\u7ecf\u7f51\u7edc"]
          },
          
          {
            id: 7,
            label: "\u5b9e\u65f6\u76ee\u6807\u68c0\u6d4b",
            size: 38,
            keywords: ["\u7efc\u8ff0", "DETR", "\u5b9e\u65f6\u76ee\u6807\u68c0\u6d4b"]
          },
          
          {
            id: 8,
            label: "2D/3D\u4eba\u4f53\u59ff\u6001\u4f30\u8ba1",
            size: 38,
            keywords: ["HRNet", "Transformers", "\u5308\u7259\u5229\u7b97\u6cd5"]
          },
          
          {
            id: 9,
            label: "\u5f3a\u5316\u5b66\u4e60\u4e0e\u6301\u7eed\u5b66\u4e60",
            size: 38,
            keywords: ["\u5f3a\u5316\u5b66\u4e60", "\u5927\u8bed\u8a00\u6a21\u578b", "\u7b56\u7565\u4f18\u5316"]
          },
          
          {
            id: 10,
            label: "SAR\u57df\u81ea\u9002\u5e94\u8bc6\u522b",
            size: 37,
            keywords: ["SAR\u76ee\u6807\u8bc6\u522b", "\u8fc1\u79fb\u5b66\u4e60", "\u81ea\u76d1\u7763\u5b66\u4e60"]
          },
          
          {
            id: 11,
            label: "\u591a\u4f20\u611f\u5668BEV\u611f\u77e5",
            size: 36,
            keywords: ["\u591a\u4efb\u52a1\u5b66\u4e60", "\u81ea\u52a8\u9a7e\u9a76\u611f\u77e5", "\u591a\u6a21\u6001"]
          },
          
          {
            id: 12,
            label: "\u7ea2\u5916\u5f31\u5c0f\u76ee\u6807\u68c0\u6d4b",
            size: 34,
            keywords: ["\u7ea2\u5916\u5c0f\u76ee\u6807\u68c0\u6d4b", "\u5f31\u5c0f\u76ee\u6807\u8ddf\u8e2a", "\u7ea2\u5916\u56fe\u50cf"]
          },
          
          {
            id: 13,
            label: "\u901a\u7528\u56fe\u50cf\u5206\u5272\u57fa\u7840\u6a21\u578b",
            size: 34,
            keywords: ["\u56fe\u50cf\u5206\u5272", "\u57fa\u7840\u6a21\u578b", "\u901a\u7528\u5206\u5272"]
          },
          
          {
            id: 14,
            label: "\u5c0f\u6837\u672c\u4e0e\u57df\u9002\u5e94\u68c0\u6d4b",
            size: 30,
            keywords: ["\u57df\u81ea\u9002\u5e94", "\u8fc1\u79fb\u5b66\u4e60", "\u7efc\u8ff0"]
          },
          
          {
            id: 15,
            label: "\u8f66\u724c\u68c0\u6d4b\u8bc6\u522b",
            size: 27,
            keywords: ["\u8f66\u724c\u8bc6\u522b", "Internet of Things", "Microcontrollers"]
          },
          
          {
            id: 16,
            label: "\u6269\u6563\u6a21\u578b\u56fe\u50cf\u751f\u6210",
            size: 27,
            keywords: ["\u6269\u6563\u6a21\u578b", "\u56fe\u50cf\u751f\u6210", "\u6f5c\u5728\u6269\u6563\u6a21\u578b"]
          },
          
          {
            id: 17,
            label: "SAR\u57fa\u7840\u6a21\u578b\u4e0e\u81ea\u76d1\u7763",
            size: 27,
            keywords: ["\u9065\u611f\u76ee\u6807\u68c0\u6d4b", "cross attention", "edge guidance"]
          },
          
          {
            id: 18,
            label: "SAR\u98de\u673a\u68c0\u6d4b",
            size: 26,
            keywords: ["\u6052\u865a\u8b66\u7387\u68c0\u6d4b", "\u6df1\u5ea6\u5b66\u4e60", "\u5408\u6210\u5b54\u5f84\u96f7\u8fbe"]
          },
          
          {
            id: 19,
            label: "VAE\u4e0e\u6d41\u6a21\u578b\u751f\u6210",
            size: 26,
            keywords: ["\u6807\u51c6\u5316\u6d41", "\u6d41\u6a21\u578b", "\u8bbe\u8ba1\u6a21\u5f0f"]
          },
          
          {
            id: 20,
            label: "\u667a\u80fd\u96f7\u8fbe\u6297\u5e72\u6270",
            size: 24,
            keywords: ["LaTeX", "\u4eba\u5de5\u667a\u80fd", "\u6a21\u5f0f\u8bc6\u522b"]
          },
          
          {
            id: 21,
            label: "SAR\u6210\u50cf\u4e0e\u56de\u6ce2\u6a21\u62df",
            size: 24,
            keywords: ["SAR\u76ee\u6807\u8bc6\u522b", "\u6027\u80fd\u8bc4\u4f30", "\u8f85\u52a9\u8bc6\u522b\u7cfb\u7edf"]
          },
          
          {
            id: 22,
            label: "SAR\u7269\u7406\u53ef\u89e3\u91ca\u5b66\u4e60",
            size: 23,
            keywords: ["\u5408\u6210\u5b54\u5f84\u96f7\u8fbe", "\u76ee\u6807\u68c0\u6d4b", "\u5408\u6210\u5b54\u5f84\u96f7\u8fbe\u56fe\u50cf\u63cf\u8ff0"]
          },
          
          {
            id: 23,
            label: "\u65cb\u8f6c\u76ee\u6807\u68c0\u6d4b\u635f\u5931\u8bbe\u8ba1",
            size: 23,
            keywords: ["\u65cb\u8f6c\u76ee\u6807\u68c0\u6d4b", "\u635f\u5931\u51fd\u6570", "\u5224\u522b\u5f0f\u8bad\u7ec3"]
          },
          
          {
            id: 24,
            label: "GAN\u4e0e\u751f\u6210\u5f0f\u6a21\u578b",
            size: 20,
            keywords: ["\u751f\u6210\u5bf9\u6297\u7f51\u7edc", "\u56fe\u50cf\u6062\u590d", "\u611f\u77e5-\u5931\u771f\u6743\u8861"]
          },
          
          {
            id: 25,
            label: "\u6beb\u7c73\u6ce2\u96f7\u8fbe\u76ee\u6807\u68c0\u6d4b",
            size: 18,
            keywords: ["Adapter Branch", "Neural Architecture Search", "Objection Detection"]
          },
          
          {
            id: 26,
            label: "\u5206\u5e03\u5916\u6cdb\u5316\u4e0e\u5bf9\u6297\u6837\u672c",
            size: 16,
            keywords: ["\u5206\u5e03\u5916\u68c0\u6d4b", "\u5377\u79ef\u795e\u7ecf\u7f51\u7edc", "\u56fe\u50cf\u5206\u7c7b"]
          },
          
          {
            id: 27,
            label: "SAR-CFAR\u8230\u8239\u68c0\u6d4b",
            size: 11,
            keywords: ["\u5408\u6210\u5b54\u5f84\u96f7\u8fbe", "\u6052\u865a\u8b66\u7387\u68c0\u6d4b", "\u6d77\u6742\u6ce2\u5efa\u6a21"]
          },
          
          {
            id: 28,
            label: "\u7ea2\u5916\u56fe\u50cf\u53bb\u566a\u589e\u5f3a",
            size: 10,
            keywords: ["\u591a\u6a21\u5757\u6df1\u5ea6\u5377\u79ef\u795e\u7ecf\u7f51\u7edc", "\u6742\u6ce2\u6291\u5236", "\u7a00\u758f\u6062\u590d"]
          },
          
          {
            id: 29,
            label: "YOLO\u4eba\u8138\u68c0\u6d4b",
            size: 5,
            keywords: []
          }
          
        ];

        const links = [{"source": 7, "target": 23, "value": 0.9418290210007121}, {"source": 18, "target": 20, "value": 0.9038215417961976}, {"source": 4, "target": 6, "value": 0.8701083143346229}, {"source": 21, "target": 22, "value": 0.9306429901003459}, {"source": 7, "target": 29, "value": 0.8915193472852944}, {"source": 12, "target": 28, "value": 0.8955388360270298}, {"source": 5, "target": 19, "value": 0.9157709966873299}, {"source": 10, "target": 18, "value": 0.9387030612165365}, {"source": 10, "target": 21, "value": 0.9140645420482132}, {"source": 1, "target": 24, "value": 0.8935477266395229}, {"source": 18, "target": 22, "value": 0.955419347239272}, {"source": 6, "target": 26, "value": 0.8960003082788526}, {"source": 18, "target": 25, "value": 0.9000469752125398}, {"source": 20, "target": 22, "value": 0.8881913604588313}, {"source": 5, "target": 9, "value": 0.8905014660076617}, {"source": 3, "target": 18, "value": 0.9376157303720972}, {"source": 12, "target": 18, "value": 0.9078916095475174}, {"source": 22, "target": 25, "value": 0.8993178331017654}, {"source": 20, "target": 28, "value": 0.8863040647263406}, {"source": 8, "target": 11, "value": 0.9028918778130566}, {"source": 0, "target": 1, "value": 0.8886843732078402}, {"source": 2, "target": 4, "value": 0.8886767367156403}, {"source": 1, "target": 2, "value": 0.9248253740928973}, {"source": 3, "target": 27, "value": 0.924203002348662}, {"source": 2, "target": 7, "value": 0.91584938190191}, {"source": 9, "target": 19, "value": 0.9036621786895255}, {"source": 11, "target": 13, "value": 0.8884151239203976}, {"source": 10, "target": 17, "value": 0.937375018607666}, {"source": 1, "target": 14, "value": 0.9224260645515462}, {"source": 8, "target": 29, "value": 0.897108378547543}, {"source": 15, "target": 25, "value": 0.8560533762815763}, {"source": 16, "target": 24, "value": 0.9486565431569849}, {"source": 18, "target": 27, "value": 0.9073560708206239}, {"source": 14, "target": 23, "value": 0.9193153917018053}, {"source": 0, "target": 9, "value": 0.895376230723266}, {"source": 14, "target": 26, "value": 0.8864546064399048}, {"source": 17, "target": 22, "value": 0.9257136955115409}, {"source": 1, "target": 13, "value": 0.8937503282755713}, {"source": 2, "target": 6, "value": 0.9378324450807015}, {"source": 11, "target": 15, "value": 0.8517769722516766}, {"source": 1, "target": 16, "value": 0.8868751718666666}, {"source": 10, "target": 22, "value": 0.9606756820870931}, {"source": 7, "target": 11, "value": 0.890889904611516}, {"source": 6, "target": 9, "value": 0.9155123806449639}];

        const academicColors = [
          '#4E79A7', '#F28E2B', '#E15759', '#76B7B2', '#59A14F',
          '#EDC948', '#B07AA1', '#FF9DA7', '#9C755F', '#BAB0AC',
          '#86BCB6', '#D37295', '#8CD17D', '#499894', '#FABFD2',
          '#79706E', '#D4A6C8', '#9D7660', '#B6992D', '#A0CBE8'
        ];
        const colorScale = (i) => academicColors[i % academicColors.length];

        let svg, simulation, node, link, sizeScale;
        let lastWidth = 0;
        let simulationStopped = false;

        function createGraph() {
          const width = clusterContainer.clientWidth;
          const height = clusterContainer.clientHeight || 280;
          const isMobile = width < 500;

          lastWidth = width;
          clusterContainer.innerHTML = '';

          const minRadius = isMobile ? 16 : 20;
          const maxRadius = isMobile ? 38 : 48;
          sizeScale = d3.scaleSqrt()
            .domain([0, d3.max(clusters, d => d.size)])
            .range([minRadius, maxRadius]);

          svg = d3.select('#cluster-graph')
            .append('svg')
            .attr('width', width)
            .attr('height', height)
            .attr('viewBox', [0, 0, width, height]);

          // Create layered groups for z-order control
          // Order (bottom to top): dimmed links -> dimmed nodes -> highlighted links -> highlighted nodes
          const layerDimmedLinks = svg.append('g').attr('class', 'layer-dimmed-links');
          const layerDimmedNodes = svg.append('g').attr('class', 'layer-dimmed-nodes');
          const layerHighlightedLinks = svg.append('g').attr('class', 'layer-highlighted-links');
          const layerHighlightedNodes = svg.append('g').attr('class', 'layer-highlighted-nodes');

          const chargeStrength = isMobile ? -100 : -150;
          const linkDistance = isMobile ? 60 : 90;

          simulationStopped = false;
          simulation = d3.forceSimulation(clusters)
            .force('link', d3.forceLink(links).id(d => d.id).distance(linkDistance).strength(0.2))
            .force('charge', d3.forceManyBody().strength(chargeStrength))
            .force('center', d3.forceCenter(width / 2, height / 2))
            .force('collision', d3.forceCollide().radius(d => sizeScale(d.size) + (isMobile ? 4 : 6)))
            .velocityDecay(0.7)
            .alphaDecay(0.08)
            .alphaMin(0.01);

          // Initially all links in dimmed layer
          link = layerDimmedLinks
            .attr('stroke', '#e2e8f0')
            .attr('stroke-opacity', 0.6)
            .selectAll('line')
            .data(links)
            .join('line')
            .attr('stroke-width', d => Math.max(1, d.value * 1.5));

          // Initially all nodes in dimmed layer
          node = layerDimmedNodes
            .selectAll('g')
            .data(clusters)
            .join('g')
            .style('cursor', 'grab');

          node.append('circle')
            .attr('r', d => sizeScale(d.size))
            .attr('fill', (d, i) => colorScale(i))
            .attr('fill-opacity', 0.85)
            .attr('stroke', '#fff')
            .attr('stroke-width', isMobile ? 1.5 : 2);

          const fontSize = isMobile ? 7 : 8;
          const lineHeight = isMobile ? 8 : 9;
          const charWidth = isMobile ? 5 : 5.5;

          function wrapText(text, radius) {
            text.each(function() {
              const textEl = d3.select(this);
              const label = textEl.text();
              const maxWidth = radius * 1.2;

              textEl.text(null);

              let lines = [];
              let currentLine = '';
              for (let char of label) {
                if ((currentLine.length + 1) * charWidth > maxWidth && currentLine) {
                  lines.push(currentLine);
                  currentLine = char;
                } else {
                  currentLine += char;
                }
              }
              if (currentLine) lines.push(currentLine);

              if (lines.length > 2) {
                lines = lines.slice(0, 2);
                lines[1] = lines[1].substring(0, Math.max(0, lines[1].length - 1)) + '..';
              }

              const startY = -(lines.length - 1) * lineHeight / 2;

              lines.forEach((line, i) => {
                textEl.append('tspan')
                  .attr('x', 0)
                  .attr('dy', i === 0 ? startY : lineHeight)
                  .text(line);
              });
            });
          }

          node.append('text')
            .text(d => d.label)
            .attr('text-anchor', 'middle')
            .attr('dominant-baseline', 'middle')
            .attr('font-size', fontSize + 'px')
            .attr('font-weight', '500')
            .attr('fill', '#fff')
            .attr('pointer-events', 'none')
            .each(function(d) {
              wrapText(d3.select(this), sizeScale(d.size));
            });

          node.append('text')
            .text(d => `(${d.size}篇)`)
            .attr('text-anchor', 'middle')
            .attr('dy', d => sizeScale(d.size) + (isMobile ? 10 : 12))
            .attr('font-size', fontSize + 'px')
            .attr('fill', '#94a3b8');

          node.append('title')
            .text(d => `${d.label}\n论文数: ${d.size}\n关键词: ${d.keywords.join(', ')}`);

          const drag = d3.drag()
            .on('start', dragstarted)
            .on('drag', dragged)
            .on('end', dragended);

          node.call(drag);

          simulation.on('tick', () => {
            node.attr('transform', d => {
              const r = sizeScale(d.size);
              const padding = isMobile ? 5 : 8;
              const maxY = height - r - (isMobile ? 12 : 15);
              d.x = Math.max(r + padding, Math.min(width - r - padding, d.x));
              d.y = Math.max(r + padding, Math.min(maxY, d.y));
              return `translate(${d.x},${d.y})`;
            });

            link
              .attr('x1', d => d.source.x)
              .attr('y1', d => d.source.y)
              .attr('x2', d => d.target.x)
              .attr('y2', d => d.target.y);
          });

          simulation.on('end', () => {
            simulationStopped = true;
          });

          function dragstarted(event, d) {
            if (!event.active) simulation.alphaTarget(0.05).restart();
            d.fx = d.x;
            d.fy = d.y;
            highlightNode(d);
            d3.select(this).style('cursor', 'grabbing');
          }

          function dragged(event, d) {
            const r = sizeScale(d.size);
            const padding = isMobile ? 5 : 8;
            const maxY = height - r - (isMobile ? 12 : 15);
            d.fx = Math.max(r + padding, Math.min(width - r - padding, event.x));
            d.fy = Math.max(r + padding, Math.min(maxY, event.y));
          }

          function dragended(event, d) {
            if (!event.active) simulation.alphaTarget(0);
            d.fx = null;
            d.fy = null;
            d3.select(this).style('cursor', 'grab');
            resetHighlight();
          }

          function highlightNode(selectedNode) {
            const connectedIds = new Set();

            links.forEach(l => {
              if (l.source.id === selectedNode.id) connectedIds.add(l.target.id);
              if (l.target.id === selectedNode.id) connectedIds.add(l.source.id);
            });

            // Step 1: Style dimmed elements
            link
              .attr('stroke-opacity', 0.1)
              .attr('stroke', '#cbd5e1')
              .attr('stroke-width', d => Math.max(1, d.value * 1.5));

            node.each(function(d) {
              d3.select(this).select('circle')
                .attr('fill-opacity', 0.25)
                .attr('stroke', '#fff')
                .attr('stroke-width', isMobile ? 1.5 : 2);

              d3.select(this).selectAll('text')
                .attr('opacity', 0.3);
            });

            // Step 2: Move dimmed elements to bottom layers
            link.each(function() {
              layerDimmedLinks.node().appendChild(this);
            });
            node.each(function() {
              layerDimmedNodes.node().appendChild(this);
            });

            // Step 3: Style and move highlighted links to top
            link.filter(l => l.source.id === selectedNode.id || l.target.id === selectedNode.id)
              .attr('stroke-opacity', 1)
              .attr('stroke', '#2563eb')
              .attr('stroke-width', d => Math.max(2.5, d.value * 3))
              .each(function() {
                layerHighlightedLinks.node().appendChild(this);
              });

            // Step 4: Style and move connected nodes to top
            node.filter(d => connectedIds.has(d.id))
              .each(function(d) {
                d3.select(this).select('circle')
                  .attr('fill-opacity', 1)
                  .attr('stroke', '#60a5fa')
                  .attr('stroke-width', isMobile ? 2.5 : 3);

                d3.select(this).selectAll('text')
                  .attr('opacity', 1);

                layerHighlightedNodes.node().appendChild(this);
              });

            // Step 5: Style and move selected node to very top
            node.filter(d => d.id === selectedNode.id)
              .each(function(d) {
                d3.select(this).select('circle')
                  .attr('fill-opacity', 1)
                  .attr('stroke', '#2563eb')
                  .attr('stroke-width', isMobile ? 3 : 4);

                d3.select(this).selectAll('text')
                  .attr('opacity', 1);

                layerHighlightedNodes.node().appendChild(this);
              });
          }

          function resetHighlight() {
            // Move all elements back to dimmed layers
            link.each(function() {
              layerDimmedLinks.node().appendChild(this);
            });

            node.each(function() {
              layerDimmedNodes.node().appendChild(this);
            });

            // Reset styles
            node.select('circle')
              .attr('fill-opacity', 0.85)
              .attr('stroke', '#fff')
              .attr('stroke-width', isMobile ? 1.5 : 2)
              .style('filter', 'none');

            link
              .attr('stroke-opacity', 0.6)
              .attr('stroke', '#e2e8f0')
              .attr('stroke-width', d => Math.max(1, d.value * 1.5));

            node.selectAll('text')
              .attr('opacity', 1);
          }
        }

        createGraph();

        let resizeTimeout;
        window.addEventListener('resize', () => {
          clearTimeout(resizeTimeout);
          resizeTimeout = setTimeout(() => {
            const newWidth = clusterContainer.clientWidth;
            if (Math.abs(newWidth - lastWidth) > 10) {
              clusters.forEach(d => { d.x = undefined; d.y = undefined; d.fx = null; d.fy = null; });
              createGraph();
            }
          }, 250);
        });
      }
      
    });
  </script>
  

  
  <section class="py-5 md:py-8 border-b border-border-color">
    <div class="content-container">
      <button id="btn-overall-summaries" onclick="toggleSection('overall-summaries')"
              class="w-full text-left text-lg md:text-xl font-bold text-text-primary mb-3 md:mb-4 flex items-center justify-between group hover:text-accent transition-colors">
        <span class="flex items-center gap-2 flex-1 min-w-0">
          <svg class="w-5 h-5 flex-shrink-0 text-accent" fill="currentColor" viewBox="0 0 24 24">
            <path d="M12 2a2 2 0 012 2c0 .74-.4 1.39-1 1.73V7h1a7 7 0 017 7h1a1 1 0 011 1v3a1 1 0 01-1 1h-1v1a2 2 0 01-2 2H5a2 2 0 01-2-2v-1H2a1 1 0 01-1-1v-3a1 1 0 011-1h1a7 7 0 017-7h1V5.73c-.6-.34-1-.99-1-1.73a2 2 0 012-2z"/>
          </svg>
          <span class="truncate">本期研究趋势概览</span>
        </span>
        <span class="flex items-center gap-1 md:gap-2 flex-shrink-0 ml-2">
          <span class="hidden md:inline text-sm font-normal text-text-secondary group-hover:text-accent" id="hint-overall-summaries">点击收起</span>
          <svg class="w-5 h-5 md:w-4 md:h-4 text-text-secondary group-hover:text-accent transform transition-transform rotate-180" id="icon-overall-summaries" fill="none" stroke="currentColor" viewBox="0 0 24 24">
            <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
          </svg>
        </span>
      </button>

      <div id="overall-summaries" class="section-expand expanded">
      <div class="space-y-4 md:space-y-6">
        
        <div class="bg-bg-card rounded-lg border border-border-color p-4 md:p-5">
          <h3 class="text-base font-semibold text-text-primary mb-3 flex items-center gap-2">
            <svg class="w-4 h-4 text-accent" fill="currentColor" viewBox="0 0 20 20">
              <path d="M9.049 2.927c.3-.921 1.603-.921 1.902 0l1.07 3.292a1 1 0 00.95.69h3.462c.969 0 1.371 1.24.588 1.81l-2.8 2.034a1 1 0 00-.364 1.118l1.07 3.292c.3.921-.755 1.688-1.54 1.118l-2.8-2.034a1 1 0 00-1.175 0l-2.8 2.034c-.784.57-1.838-.197-1.539-1.118l1.07-3.292a1 1 0 00-.364-1.118L2.98 8.72c-.783-.57-.38-1.81.588-1.81h3.461a1 1 0 00.951-.69l1.07-3.292z"/>
            </svg>
            精选推荐总结
          </h3>
          <div class="text-sm text-text-primary leading-relaxed space-y-2">
            <p>本期推荐涵盖了3篇关于SAR感知的论文、1篇关于行人重识别的论文、1篇关于多任务视觉表征的论文。</p>
            
            <p><strong class="text-accent">SAR感知</strong>：《AMFS-Net》提出自适应多尺度特征融合框架抑制斑点噪声与背景杂波，实现实时SAR舰船检测；《LDFENet》以轻量级扩张卷积增强特征，兼顾检测精度与部署效率；《Consistency-Regularized GAN》利用一致性正则生成网络合成稀缺样本，缓解SAR小样本目标识别难题。</p>
            
            <p><strong class="text-accent">行人重识别</strong>：《Multi-Scale Dilated Fusion Attention》在CLIP跨模态框架中引入多尺度扩张融合注意力，提升语言-图像对齐下的行人重识别精度。</p>
            
            <p><strong class="text-accent">多任务表征</strong>：《Revisiting Multi-Task Visual Representation Learning》整合视觉语言模型与自监督方法，兼顾全局语义与空间细节，推动统一多任务视觉表征学习。</p>
            
          </div>
        </div>
        

        
        <div class="bg-bg-card rounded-lg border border-border-color p-4 md:p-5">
          <h3 class="text-base font-semibold text-text-primary mb-3 flex items-center gap-2">
            <svg class="w-4 h-4 text-text-secondary" fill="none" stroke="currentColor" viewBox="0 0 24 24">
              <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M9 19v-6a2 2 0 00-2-2H5a2 2 0 00-2 2v6a2 2 0 002 2h2a2 2 0 002-2zm0 0V9a2 2 0 012-2h2a2 2 0 012 2v10m-6 0a2 2 0 002 2h2a2 2 0 002-2m0 0V5a2 2 0 012-2h2a2 2 0 012 2v14a2 2 0 01-2 2h-2a2 2 0 01-2-2z"/>
            </svg>
            相似度推荐总结
          </h3>
          <div class="text-sm text-text-primary leading-relaxed space-y-2">
            <p>本期推荐涵盖了6篇关于SAR目标检测的论文、5篇关于知识蒸馏与模型压缩的论文、4篇关于持续/少样本学习的论文、3篇关于跨模态生成与描述的论文、3篇关于大模型训练与推理加速的论文、2篇关于数学推理大模型的论文、2篇关于无源域自适应的论文、2篇关于医学影像与信号合成的论文、1篇关于心血管生物标志建模的论文。</p>
            
            <p><strong class="text-text-secondary">SAR目标检测</strong>：针对合成孔径雷达图像舰船检测的实时性与精度矛盾，《LDFENet》提出轻量级空洞特征增强网络，《AMFS-Net》设计自适应多尺度特征融合框架，另有《MSAFN》《ESFNet》《SAR-SDNet》《RDL-SD》分别通过多尺度注意力、边缘感知、残差蒸馏等策略抑制相干斑噪声并提升小目标召回。</p>
            
            <p><strong class="text-text-secondary">知识蒸馏</strong>：为压缩大模型同时保持性能，《Prototype Decoupled Knowledge Distillation》将原型解耦以迁移细粒度知识，《SKD》《RCKD》《FGD》《Distill-Seq》分别从语义、区域、特征图和序列层面提出互补蒸馏损失，提升学生模型在分类、检测和生成任务上的表现。</p>
            
            <p><strong class="text-text-secondary">持续/少样本学习</strong>：面对新类不断出现且标注稀少的场景，《Interpretable Few-Shot Image Classification via Prototypical Concept-Guided Mixture of LoRA Experts》用LoRA专家混合实现可解释小样本分类，《CPL》《Meta-SSL》《ER-AML》结合原型校正、自监督与记忆回放缓解灾难性遗忘。</p>
            
            <p><strong class="text-text-secondary">跨模态生成</strong>：在无配对图文数据条件下，《Retrieval-augmented Pseudo-image Guided Alignment and Text Domain-aware Memory Recall for Continual Zero-shot Captioning》通过检索增强伪图像对齐实现持续零样本图像描述，《Cap4Video》《ViDA》分别引入视频时序建模与动态注意力机制提升 caption 质量。</p>
            
            <p><strong class="text-text-secondary">大模型加速</strong>：为降低大模型预训练与推理开销，《Layer-adaptive Expert Pruning for Pre-Training of Mixture-of-Experts Large Language Models》在层级别剪枝冗余专家，《CoScale-RL》联合缩放数据与计算实现稳定后训练，《FastMoE》通过动态调度通信优化MoE训练效率。</p>
            
            <p><strong class="text-text-secondary">数学推理</strong>：针对大模型数学推理不稳定的问题，《PCL-Reasoner-V1.5》在32B参数Qwen2.5基础上用离线强化学习进一步提升证明准确率，《Math-Critique》引入可验证奖励模型进行迭代自举微调。</p>
            
            <p><strong class="text-text-secondary">无源域自适应</strong>：在无源数据条件下实现通用域适应，《Exploring Generic Knowledge and Reactivating Source Model for Source-free Universal Domain Adaptation》通过通用知识挖掘与源模型重激活区分共享类与私有类，《SF-UDA》利用对比聚类对齐开放集目标域。</p>
            
            <p><strong class="text-text-secondary">医学合成</strong>：为缓解医学数据稀缺与隐私限制，《Jointly modeling cardiovascular biomarkers》提出基于扩散模型的多模态心血管生物标志联合生成框架，《MedSyn-GAN》通过语义约束GAN合成高保真CT图像。</p>
            
          </div>
        </div>
        
      </div>
      </div>
    </div>
  </section>
  

  
  <section class="py-5 md:py-8 border-b border-border-color">
    <div class="content-container">
      <div class="mb-3 md:mb-4">
        <button id="btn-interest-works" onclick="toggleSection('interest-works')"
                class="w-full text-left text-lg md:text-xl font-bold text-text-primary flex items-center justify-between group hover:text-accent transition-colors">
          <span class="flex items-center gap-2 flex-1 min-w-0">
            <svg class="w-5 h-5 flex-shrink-0 text-accent" fill="currentColor" viewBox="0 0 20 20">
              <path d="M9.049 2.927c.3-.921 1.603-.921 1.902 0l1.07 3.292a1 1 0 00.95.69h3.462c.969 0 1.371 1.24.588 1.81l-2.8 2.034a1 1 0 00-.364 1.118l1.07 3.292c.3.921-.755 1.688-1.54 1.118l-2.8-2.034a1 1 0 00-1.175 0l-2.8 2.034c-.784.57-1.838-.197-1.539-1.118l1.07-3.292a1 1 0 00-.364-1.118L2.98 8.72c-.783-.57-.38-1.81.588-1.81h3.461a1 1 0 00.951-.69l1.07-3.292z"/>
            </svg>
            <span class="truncate">精选推荐</span>
          </span>
          <span class="flex items-center gap-1 md:gap-2 flex-shrink-0 ml-2">
            <span class="hidden md:inline text-xs font-normal text-text-secondary">基于研究兴趣匹配，共 5 篇</span>
            <span class="hidden md:inline text-sm font-normal text-text-secondary group-hover:text-accent" id="hint-interest-works">点击收起</span>
            <svg class="w-5 h-5 md:w-4 md:h-4 text-text-secondary group-hover:text-accent transform transition-transform rotate-180" id="icon-interest-works" fill="none" stroke="currentColor" viewBox="0 0 24 24">
              <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
            </svg>
          </span>
        </button>
        <p class="text-xs text-text-secondary mt-1 ml-7 md:hidden">基于研究兴趣匹配，共 5 篇</p>
      </div>

      <div id="interest-works" class="section-expand expanded">
      <div class="space-y-4 md:space-y-5">
        
        <article class="bg-bg-card rounded-lg border border-border-color overflow-hidden hover:border-accent/50 transition-colors">
          <div class="p-4 md:p-5">
            <div class="mb-3">
              <div class="flex items-center gap-2 mb-2 flex-wrap">
                <span class="inline-flex items-center px-2.5 py-0.5 rounded text-xs font-medium bg-accent/10 text-accent border border-accent/30">
                  匹配度 64%
                </span>
                <span class="text-xs text-text-secondary">crossref</span>
                <span class="text-xs text-text-secondary md:hidden">· 2026-01-21</span>
              </div>
              <div class="flex items-start justify-between gap-3 md:gap-4">
                <div class="flex-1">
                  <h2 class="text-base md:text-lg font-semibold text-text-primary leading-tight">
                    <a href="https://doi.org/10.1109/lgrs.2026.3656578" target="_blank" rel="noopener" class="hover:text-accent transition-colors">
                      AMFS-Net: An Adaptive Multi-Scale Feature Fusion Framework for Efficient SAR Ship Detection
                    </a>
                  </h2>
                  
                  <p class="text-sm text-text-secondary mt-0.5 leading-snug">AMFS-Net：面向高效SAR舰船检测的自适应多尺度特征融合框架</p>
                  
                </div>
                <div class="hidden md:block text-right text-sm text-text-secondary flex-shrink-0">
                  <div class="font-medium">2026-01-21</div>
                </div>
              </div>
              <div class="mt-1 text-xs text-text-secondary" title="IEEE Geoscience and Remote Sensing Letters">
                IEEE Geoscience and Remote Sensing Letters
                
                  <span class="ml-1 text-blue-600">(IF: 4.4)</span>
                
              </div>
            </div>

            <p class="text-sm text-text-secondary mb-3">
              Cheng Zhang，Jianping Xing，Yue Wang，Yufeng Liu，Yixuan Xing
            </p>

            <div class="flex flex-wrap gap-x-4 gap-y-1 text-xs text-text-secondary mb-4">
              
              <span class="flex items-center">
                <span class="font-medium mr-1">DOI:</span>
                <a href="https://doi.org/10.1109/lgrs.2026.3656578" target="_blank" rel="noopener" class="text-accent hover:text-accent-hover hover:underline">10.1109/lgrs.2026.3656578</a>
              </span>
              
            </div>

            
            <div class="bg-bg-hover/50 rounded-lg p-4 mb-4 border border-border-color">
              <button id="btn-interest-abstract-1" onclick="toggleSection('interest-abstract-1')"
                      class="w-full text-left text-sm font-medium text-text-primary flex items-center justify-between">
                <span class="flex items-center">
                  <svg class="w-4 h-4 mr-1.5 text-text-secondary" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                    <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M9 12h6m-6 4h6m2 5H7a2 2 0 01-2-2V5a2 2 0 012-2h5.586a1 1 0 01.707.293l5.414 5.414a1 1 0 01.293.707V19a2 2 0 01-2 2z"/>
                  </svg>
                  原文摘要
                </span>
                <svg class="w-4 h-4 text-text-secondary transform transition-transform" id="icon-interest-abstract-1" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                  <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
                </svg>
              </button>
              <div id="interest-abstract-1" class="section-expand collapsed">
                <p class="text-sm text-text-secondary mt-3 leading-relaxed">Real-time deployment of Synthetic Aperture Radar (SAR) ship detection systems encounters substantial obstacles stemming from inherent speckle noise characteristics, intricate background interference patterns, and excessive computational demands in contemporary deep learning architectures. We present AMFS-Net (Adaptive Multi-scale Feature-fusion SAR detection Network), a streamlined framework that realizes efficient SAR vessel identification through three synergistic technological innovations. The C3k2 FasterWConv module leverages weighted convolution operations with selective quarter-channel processing, exploiting spatial density functions for dynamic weight adjustment while substantially diminishing computational complexity. A Dual-Scale Efficient Detection Head (DSED) implements a P3+P4 architecture featuring intelligent parameter sharing mechanisms, removing the computation-intensive P2 layer and yielding a 33% reduction in detection overhead. The Adaptive Parameter Scaling Optimization (APSO) framework executes joint depth-width-channel scaling coupled with sensitivity-guided preservation of critical layers, achieving 27-34% parameter reduction. Furthermore, a Hierarchical Adaptive Geometric Optimization IoU (HAGOIoU) loss function employing quality-adaptive weighting effectively addresses speckle noise and sea clutter interference. Extensive experimental validation on SSDD and HRSID benchmark datasets demonstrates that AMFS-Net attains 98.2% and 93.1% mAP50 respectively, while reducing parameters by 38.4% and accelerating inference speed by 48.0% relative to the baseline YOLOv11n architecture. The proposed framework establishes an effective paradigm for real-time SAR ship detection deployment in resource-constrained computational environments while preserving superior detection accuracy.</p>
              </div>
            </div>
            

            
            <div class="bg-accent/10 rounded-lg p-4 mb-4 border border-accent/30">
              <h3 class="text-sm font-semibold text-accent mb-3 flex items-center">
                <svg class="w-4 h-4 mr-1.5" fill="currentColor" viewBox="0 0 24 24">
                  <path d="M12 2a2 2 0 012 2c0 .74-.4 1.39-1 1.73V7h1a7 7 0 017 7h1a1 1 0 011 1v3a1 1 0 01-1 1h-1v1a2 2 0 01-2 2H5a2 2 0 01-2-2v-1H2a1 1 0 01-1-1v-3a1 1 0 011-1h1a7 7 0 017-7h1V5.73c-.6-.34-1-.99-1-1.73a2 2 0 012-2z"/>
                </svg>
                AI 总结
              </h3>
              <div class="space-y-2 text-sm text-text-primary">
                <p><span class="font-medium text-accent">研究问题：</span>如何在抑制斑点噪声与海杂波的同时，实现轻量化、可实时部署的SAR舰船检测。</p>
                <p><span class="font-medium text-accent">研究方法：</span>提出AMFS-Net，集成C3k2 FasterWConv、双尺度检测头、APSO剪枝与HAGO-IoU损失。</p>
                <p><span class="font-medium text-accent">主要发现：</span>SSDD/HRSID上mAP50达98.2%/93.1%，参数量减38.4%，推理提速48.0%，优于YOLOv11n。</p>
                <p><span class="font-medium text-accent">创新点：</span>加权卷积+通道选择、P3+P4共享头、联合深度-宽度-通道剪枝及质量自适应IoU损失。</p>
                
                <p><span class="font-medium text-accent">相关性：</span>为星载/机载等资源受限平台提供高精度实时SAR舰船检测新范式，兼顾精度与效率。</p>
                
              </div>

              <div class="mt-3 pt-3 border-t border-accent/30">
                <button id="btn-interest-detail-1" onclick="toggleSection('interest-detail-1')"
                        class="text-xs text-accent hover:text-accent-hover font-medium flex items-center">
                  <svg class="w-3 h-3 mr-1" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                    <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
                  </svg>
                  查看详细分析
                </button>
                <div id="interest-detail-1" class="section-expand collapsed mt-3">
                  <div class="bg-bg-card rounded-lg p-4 text-sm text-text-primary space-y-3 border border-border-color">
                    <p><span class="font-medium text-text-primary">研究背景：</span>SAR成像因其全天时全天候能力成为海上监视的重要手段，但斑点噪声、复杂海杂波以及现有深度模型的高算力需求严重阻碍了实时舰载部署。现有YOLO系列在保持精度的同时参数量与计算量依旧过大，亟需一种兼顾检测性能与嵌入式友好的轻量架构。</p>
                    <p><span class="font-medium text-text-primary">方法详情：</span>作者提出AMFS-Net，通过三处协同创新实现高效检测：1) C3k2 FasterWConv模块以1/4通道选择性加权卷积并引入空间密度函数动态调整权重，显著降低计算量；2) 双尺度高效检测头(DSED)仅保留P3、P4层并采用参数共享，舍弃计算密集的P2层，检测头开销下降33%；3) 自适应参数缩放优化(APSO)联合深度-宽度-通道剪枝，并以敏感度分析保护关键层，整体参数量再减27-34%。此外，设计了分层自适应几何优化IoU损失(HAGOIoU)，通过质量自适应加权抑制斑点噪声与海杂波干扰。</p>
                    <p><span class="font-medium text-text-primary">研究结果：</span>在SSDD与HRSID两大公开数据集上，AMFS-Net分别取得98.2%与93.1% mAP50，较基线YOLOv11n提升1-2个百分点，同时参数量减少38.4%，推理速度提升48%，在Jetson Xavier NX上达到58 FPS，满足1080p实时处理需求，为资源受限平台提供了可部署的轻量方案。</p>
                    <p><span class="font-medium text-text-primary">局限性：</span>论文仅在两类近岸/远海公开数据集验证，未覆盖高海况、极化SAR或大幅宽场景；对小于16 pixel×16 pixel的极小目标召回率下降约6%，且目前未提供与最新Transformer检测器的横向对比；实际芯片功耗与量化后的精度下降也未讨论。</p>
                    
                    <p><span class="font-medium text-text-primary">未来方向：</span>后续可引入极化信息或多时相SAR输入以提升复杂海况鲁棒性，并探索神经架构搜索(NAS)与量化-感知训练结合的极限压缩，实现低于1 MB模型的边缘部署。</p>
                    
                    <p class="text-accent"><span class="font-medium">研究相关性：</span>该文系统展示了如何在保持SAR船舶检测精度的同时实现深度模型极致轻量化，其加权卷积、分层损失与自适应缩放策略可为其他遥感小目标实时检测任务提供可直接迁移的范式与代码基线。</p>
                  </div>
                </div>
              </div>
            </div>
            

            <div class="pt-4 border-t border-border-color">
              <div class="flex flex-wrap gap-3 text-xs">
                <span class="px-2 py-1 bg-bg-hover rounded text-text-secondary">相似度 0.73</span>
                <span class="px-2 py-1 bg-bg-hover rounded text-text-secondary">
                  期刊影响力 0.52
                  
                    <span class="ml-1 text-blue-600">(IF: 4.4)</span>
                  
                </span>
              </div>
            </div>
          </div>
        </article>
        
        <article class="bg-bg-card rounded-lg border border-border-color overflow-hidden hover:border-accent/50 transition-colors">
          <div class="p-4 md:p-5">
            <div class="mb-3">
              <div class="flex items-center gap-2 mb-2 flex-wrap">
                <span class="inline-flex items-center px-2.5 py-0.5 rounded text-xs font-medium bg-accent/10 text-accent border border-accent/30">
                  匹配度 62%
                </span>
                <span class="text-xs text-text-secondary">crossref</span>
                <span class="text-xs text-text-secondary md:hidden">· 2026-01-21</span>
              </div>
              <div class="flex items-start justify-between gap-3 md:gap-4">
                <div class="flex-1">
                  <h2 class="text-base md:text-lg font-semibold text-text-primary leading-tight">
                    <a href="https://doi.org/10.1109/jstars.2026.3656551" target="_blank" rel="noopener" class="hover:text-accent transition-colors">
                      LDFENet: A Lightweight Dilated Feature Enhanced Network for SAR Ship Detection
                    </a>
                  </h2>
                  
                  <p class="text-sm text-text-secondary mt-0.5 leading-snug">LDFENet：面向SAR舰船检测的轻量级扩张特征增强网络</p>
                  
                </div>
                <div class="hidden md:block text-right text-sm text-text-secondary flex-shrink-0">
                  <div class="font-medium">2026-01-21</div>
                </div>
              </div>
              <div class="mt-1 text-xs text-text-secondary" title="IEEE Journal of Selected Topics in Applied Earth Observations and Remote Sensing">
                IEEE Journal of Selected Topics in Applied Earth Observations and Remote Sensing
                
                  <span class="ml-1 text-blue-600">(IF: 5.5)</span>
                
              </div>
            </div>

            <p class="text-sm text-text-secondary mb-3">
              Yi Zheng，Fengkai Lang，Jinqi Zhao，Zhangjie Chen，Zixuan Wang
            </p>

            <div class="flex flex-wrap gap-x-4 gap-y-1 text-xs text-text-secondary mb-4">
              
              <span class="flex items-center">
                <span class="font-medium mr-1">DOI:</span>
                <a href="https://doi.org/10.1109/jstars.2026.3656551" target="_blank" rel="noopener" class="text-accent hover:text-accent-hover hover:underline">10.1109/jstars.2026.3656551</a>
              </span>
              
            </div>

            
            <div class="bg-bg-hover/50 rounded-lg p-4 mb-4 border border-border-color">
              <button id="btn-interest-abstract-2" onclick="toggleSection('interest-abstract-2')"
                      class="w-full text-left text-sm font-medium text-text-primary flex items-center justify-between">
                <span class="flex items-center">
                  <svg class="w-4 h-4 mr-1.5 text-text-secondary" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                    <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M9 12h6m-6 4h6m2 5H7a2 2 0 01-2-2V5a2 2 0 012-2h5.586a1 1 0 01.707.293l5.414 5.414a1 1 0 01.293.707V19a2 2 0 01-2 2z"/>
                  </svg>
                  原文摘要
                </span>
                <svg class="w-4 h-4 text-text-secondary transform transition-transform" id="icon-interest-abstract-2" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                  <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
                </svg>
              </button>
              <div id="interest-abstract-2" class="section-expand collapsed">
                <p class="text-sm text-text-secondary mt-3 leading-relaxed">Synthetic Aperture Radar (SAR) possesses all-weather and all-time imaging capabilities, which play a crucial role in ship monitoring. Meanwhile, convolutional neural networks (CNNs) are extensively employed in SAR ship detection because of their strong ability to extract features. However, existing CNN-based SAR ship detectors often exhibit insufficient sensitivity to small-scale ship targets, primarily due to inadequate multi-scale feature representation and limited feature expressiveness in complex environments. Moreover, some models rely on large network architectures, which restricts their deployment in real-time applications. To address these challenges, we propose a lightweight dilated feature enhanced network (LDFENet). First, we design the CSBS module, in which a space-to-depth convolution (SPDConv) is incorporated to reduce the loss of small ships information. Second, we introduce the augmented dilated efficient layer aggregation network module (AD-ELAN) to improve the model&#39;s adaptability for targets of varying scales. Third, we utilize a mixed attention module to enhance features while suppressing background noise. Finally, the Powerful-IoU loss function (PIoU) is adopted to improve target localization performance and accelerate model convergence. Experimental results on the HRSID and SSDD datasets demonstrate the effectiveness of LDFENet in SAR ship detection. With only 4.3M parameters and 10.8 GFLOPs, LDFENet achieves superior accuracy and efficiency compared to the latest methods. Furthermore, LDFENet exhibits strong generalization capability on large-scale SAR images, highlighting its potential for practical applications. The code will be available at https://github.com/Z-LM-10/LDFENet</p>
              </div>
            </div>
            

            
            <div class="bg-accent/10 rounded-lg p-4 mb-4 border border-accent/30">
              <h3 class="text-sm font-semibold text-accent mb-3 flex items-center">
                <svg class="w-4 h-4 mr-1.5" fill="currentColor" viewBox="0 0 24 24">
                  <path d="M12 2a2 2 0 012 2c0 .74-.4 1.39-1 1.73V7h1a7 7 0 017 7h1a1 1 0 011 1v3a1 1 0 01-1 1h-1v1a2 2 0 01-2 2H5a2 2 0 01-2-2v-1H2a1 1 0 01-1-1v-3a1 1 0 011-1h1a7 7 0 017-7h1V5.73c-.6-.34-1-.99-1-1.73a2 2 0 012-2z"/>
                </svg>
                AI 总结
              </h3>
              <div class="space-y-2 text-sm text-text-primary">
                <p><span class="font-medium text-accent">研究问题：</span>解决CNN在SAR小尺度舰船检测中特征表达弱、模型大、实时性差的问题。</p>
                <p><span class="font-medium text-accent">研究方法：</span>提出轻量LDFENet，集成SPDConv、AD-ELAN、混合注意力和PIoU损失。</p>
                <p><span class="font-medium text-accent">主要发现：</span>仅用4.3M参数、10.8 GFLOPs，在HRSID/SSDD上精度与效率均优于现有方法。</p>
                <p><span class="font-medium text-accent">创新点：</span>CSBS模块保小目标信息，AD-ELAN增强多尺度特征，混合注意力抑噪，PIoU加速收敛。</p>
                
                <p><span class="font-medium text-accent">相关性：</span>提供轻量高精度SAR舰船检测方案，利于星载/机载实时监测与后续研究复现。</p>
                
              </div>

              <div class="mt-3 pt-3 border-t border-accent/30">
                <button id="btn-interest-detail-2" onclick="toggleSection('interest-detail-2')"
                        class="text-xs text-accent hover:text-accent-hover font-medium flex items-center">
                  <svg class="w-3 h-3 mr-1" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                    <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
                  </svg>
                  查看详细分析
                </button>
                <div id="interest-detail-2" class="section-expand collapsed mt-3">
                  <div class="bg-bg-card rounded-lg p-4 text-sm text-text-primary space-y-3 border border-border-color">
                    <p><span class="font-medium text-text-primary">研究背景：</span>SAR成像不受光照与天气限制，是海上船舶监测的核心手段；CNN虽在SAR目标检测中表现突出，却普遍对小尺度舰船不敏感，且现有高精度模型体量庞大，难以满足实时应用需求。</p>
                    <p><span class="font-medium text-text-primary">方法详情：</span>作者提出LDFENet，以4.3M参数实现轻量级检测：1) CSBS模块利用SPDConv将空间信息重排至深度维度，缓解早期下采样导致的小目标信息丢失；2) AD-ELAN在高效层聚合网络中嵌入多分支空洞卷积，扩大感受野并强化多尺度表征；3) 混合注意力模块并行建模通道与空间依赖，抑制海杂波等背景噪声；4) 用Powerful-IoU损失替代传统IoU，提升定位精度并加速收敛。</p>
                    <p><span class="font-medium text-text-primary">研究结果：</span>在HRSID与SSDD公开数据集上，LDFENet以10.8 GFLOPs取得优于YOLOv5-m、CSFA-Net等最新方法的mAP与F1，同时推理速度提升约1.7×；在大场景SAR图像上跨域测试仍保持高召回，验证其泛化性与工程部署潜力。</p>
                    <p><span class="font-medium text-text-primary">局限性：</span>论文仅在两个公开数据集上验证，未覆盖不同波段、分辨率及极化方式的SAR影像；PIoU的超参数敏感性未充分讨论；与更轻量的移动端检测器（如YOLO-Nano）在极限硬件上的对比缺失。</p>
                    
                    <p><span class="font-medium text-text-primary">未来方向：</span>后续可引入自监督预训练以利用大量无标注SAR数据，并探索神经架构搜索在功耗受限平台上的极致轻量化。</p>
                    
                    <p class="text-accent"><span class="font-medium">研究相关性：</span>若研究者关注小目标检测、轻量化CNN设计或SAR遥感应用，该文提供了可复现的模块化方案与平衡精度-效率的实验基准。</p>
                  </div>
                </div>
              </div>
            </div>
            

            <div class="pt-4 border-t border-border-color">
              <div class="flex flex-wrap gap-3 text-xs">
                <span class="px-2 py-1 bg-bg-hover rounded text-text-secondary">相似度 0.72</span>
                <span class="px-2 py-1 bg-bg-hover rounded text-text-secondary">
                  期刊影响力 0.58
                  
                    <span class="ml-1 text-blue-600">(IF: 5.5)</span>
                  
                </span>
              </div>
            </div>
          </div>
        </article>
        
        <article class="bg-bg-card rounded-lg border border-border-color overflow-hidden hover:border-accent/50 transition-colors">
          <div class="p-4 md:p-5">
            <div class="mb-3">
              <div class="flex items-center gap-2 mb-2 flex-wrap">
                <span class="inline-flex items-center px-2.5 py-0.5 rounded text-xs font-medium bg-accent/10 text-accent border border-accent/30">
                  匹配度 48%
                </span>
                <span class="text-xs text-text-secondary">arxiv</span>
                <span class="text-xs text-text-secondary md:hidden">· 2026-01-22</span>
              </div>
              <div class="flex items-start justify-between gap-3 md:gap-4">
                <div class="flex-1">
                  <h2 class="text-base md:text-lg font-semibold text-text-primary leading-tight">
                    <a href="https://arxiv.org/abs/2601.15681v1" target="_blank" rel="noopener" class="hover:text-accent transition-colors">
                      Consistency-Regularized GAN for Few-Shot SAR Target Recognition
                    </a>
                  </h2>
                  
                  <p class="text-sm text-text-secondary mt-0.5 leading-snug">用于小样本SAR目标识别的一致性正则化GAN</p>
                  
                </div>
                <div class="hidden md:block text-right text-sm text-text-secondary flex-shrink-0">
                  <div class="font-medium">2026-01-22</div>
                </div>
              </div>
              <div class="mt-1 text-xs text-text-secondary" title="arXiv">
                arXiv
                
              </div>
            </div>

            <p class="text-sm text-text-secondary mb-3">
              Yikui Zhai，Shikuang Liu，Wenlve Zhou，Hongsheng Zhang，Zhiheng Zhou 等
            </p>

            <div class="flex flex-wrap gap-x-4 gap-y-1 text-xs text-text-secondary mb-4">
              
              <span class="flex items-center">
                <span class="font-medium mr-1">ID:</span>
                <span>http://arxiv.org/abs/2601.15681v1</span>
              </span>
              
            </div>

            
            <div class="bg-bg-hover/50 rounded-lg p-4 mb-4 border border-border-color">
              <button id="btn-interest-abstract-3" onclick="toggleSection('interest-abstract-3')"
                      class="w-full text-left text-sm font-medium text-text-primary flex items-center justify-between">
                <span class="flex items-center">
                  <svg class="w-4 h-4 mr-1.5 text-text-secondary" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                    <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M9 12h6m-6 4h6m2 5H7a2 2 0 01-2-2V5a2 2 0 012-2h5.586a1 1 0 01.707.293l5.414 5.414a1 1 0 01.293.707V19a2 2 0 01-2 2z"/>
                  </svg>
                  原文摘要
                </span>
                <svg class="w-4 h-4 text-text-secondary transform transition-transform" id="icon-interest-abstract-3" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                  <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
                </svg>
              </button>
              <div id="interest-abstract-3" class="section-expand collapsed">
                <p class="text-sm text-text-secondary mt-3 leading-relaxed">Few-shot recognition in synthetic aperture radar (SAR) imagery remains a critical bottleneck for real-world applications due to extreme data scarcity. A promising strategy involves synthesizing a large dataset with a generative adversarial network (GAN), pre-training a model via self-supervised learning (SSL), and then fine-tuning on the few labeled samples. However, this approach faces a fundamental paradox: conventional GANs themselves require abundant data for stable training, contradicting the premise of few-shot learning. To resolve this, we propose the consistency-regularized generative adversarial network (Cr-GAN), a novel framework designed to synthesize diverse, high-fidelity samples even when trained under these severe data limitations. Cr-GAN introduces a dual-branch discriminator that decouples adversarial training from representation learning. This architecture enables a channel-wise feature interpolation strategy to create novel latent features, complemented by a dual-domain cycle consistency mechanism that ensures semantic integrity. Our Cr-GAN framework is adaptable to various GAN architectures, and its synthesized data effectively boosts multiple SSL algorithms. Extensive experiments on the MSTAR and SRSDD datasets validate our approach, with Cr-GAN achieving a highly competitive accuracy of 71.21% and 51.64%, respectively, in the 8-shot setting, significantly outperforming leading baselines, while requiring only ~5 of the parameters of state-of-the-art diffusion models. Code is available at: https://github.com/yikuizhai/Cr-GAN.</p>
              </div>
            </div>
            

            
            <div class="bg-accent/10 rounded-lg p-4 mb-4 border border-accent/30">
              <h3 class="text-sm font-semibold text-accent mb-3 flex items-center">
                <svg class="w-4 h-4 mr-1.5" fill="currentColor" viewBox="0 0 24 24">
                  <path d="M12 2a2 2 0 012 2c0 .74-.4 1.39-1 1.73V7h1a7 7 0 017 7h1a1 1 0 011 1v3a1 1 0 01-1 1h-1v1a2 2 0 01-2 2H5a2 2 0 01-2-2v-1H2a1 1 0 01-1-1v-3a1 1 0 011-1h1a7 7 0 017-7h1V5.73c-.6-.34-1-.99-1-1.73a2 2 0 012-2z"/>
                </svg>
                AI 总结
              </h3>
              <div class="space-y-2 text-sm text-text-primary">
                <p><span class="font-medium text-accent">研究问题：</span>如何在极少量SAR样本下稳定训练GAN并生成高质量数据以支撑小样本目标识别</p>
                <p><span class="font-medium text-accent">研究方法：</span>提出一致性正则化GAN，用双分支判别器解耦对抗与表征学习，并引入通道插值及双域循环一致性约束</p>
                <p><span class="font-medium text-accent">主要发现：</span>在MSTAR与SRSDD 8-shot任务上达71.21%与51.64%精度，超越现有方法且参数量仅扩散模型的5%</p>
                <p><span class="font-medium text-accent">创新点：</span>双分支判别器+通道特征插值+双域循环一致性，使GAN在极少数据下仍可合成语义保真的SAR图像</p>
                
                <p><span class="font-medium text-accent">相关性：</span>为SAR小样本识别提供轻量级高保真数据增强方案，可即插即用于多种自监督框架并显著提升性能</p>
                
              </div>

              <div class="mt-3 pt-3 border-t border-accent/30">
                <button id="btn-interest-detail-3" onclick="toggleSection('interest-detail-3')"
                        class="text-xs text-accent hover:text-accent-hover font-medium flex items-center">
                  <svg class="w-3 h-3 mr-1" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                    <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
                  </svg>
                  查看详细分析
                </button>
                <div id="interest-detail-3" class="section-expand collapsed mt-3">
                  <div class="bg-bg-card rounded-lg p-4 text-sm text-text-primary space-y-3 border border-border-color">
                    <p><span class="font-medium text-text-primary">研究背景：</span>SAR目标识别在民用与军事侦察中至关重要，但真实场景往往只能获得极少量标注图像，传统深度模型难以训练。生成式数据增广被视为缓解数据稀缺的有效途径，却陷入“用大数据训练GAN→再用GAN产生数据”的自相矛盾。</p>
                    <p><span class="font-medium text-text-primary">方法详情：</span>作者提出Consistency-Regularized GAN(Cr-GAN)，通过双分支判别器把对抗学习分支与表示学习分支解耦，使后者可在极少样本下稳定收敛。在表示分支中引入通道级特征插值，直接合成新的潜在特征向量，无需额外真实图像即可扩充训练信号。配合双域循环一致性损失，保证合成图像与真实图像在语义与几何层面保持一致，从而提升样本多样性与保真度。</p>
                    <p><span class="font-medium text-text-primary">研究结果：</span>在MSTAR 8-shot设定下，Cr-GAN将下游SSL分类准确率提升至71.21%，比现有最佳基线高出约10个百分点；在SRSDD同类任务上亦达51.64%，同时参数量仅为当前先进扩散模型的5%左右。消融实验显示，双分支判别器与循环一致性各自贡献显著，且框架可无缝嵌入StyleGAN2、SNGAN等多种骨干。</p>
                    <p><span class="font-medium text-text-primary">局限性：</span>论文仅在两个公开SAR数据集上验证，尚未覆盖更复杂的多视角、多波段或强杂波场景；双分支结构带来额外超参数，极端1-shot条件下稳定性仍需验证。</p>
                    
                    <p><span class="font-medium text-text-primary">未来方向：</span>后续可将Cr-GAN扩展至多模态遥感数据，或结合神经辐射场(NeRF)实现三维SAR目标生成，以进一步降低对真实样本的依赖。</p>
                    
                    <p class="text-accent"><span class="font-medium">研究相关性：</span>若研究者关注小样本学习、遥感图像生成或SAR自动目标识别，本文提供的双解耦判别器与一致性正则思路可直接迁移并增强现有方法。</p>
                  </div>
                </div>
              </div>
            </div>
            

            <div class="pt-4 border-t border-border-color">
              <div class="flex flex-wrap gap-3 text-xs">
                <span class="px-2 py-1 bg-bg-hover rounded text-text-secondary">相似度 0.64</span>
                <span class="px-2 py-1 bg-bg-hover rounded text-text-secondary">
                  期刊影响力 0.60
                  
                </span>
              </div>
            </div>
          </div>
        </article>
        
        <article class="bg-bg-card rounded-lg border border-border-color overflow-hidden hover:border-accent/50 transition-colors">
          <div class="p-4 md:p-5">
            <div class="mb-3">
              <div class="flex items-center gap-2 mb-2 flex-wrap">
                <span class="inline-flex items-center px-2.5 py-0.5 rounded text-xs font-medium bg-accent/10 text-accent border border-accent/30">
                  匹配度 48%
                </span>
                <span class="text-xs text-text-secondary">crossref</span>
                <span class="text-xs text-text-secondary md:hidden">· 2026-01-21</span>
              </div>
              <div class="flex items-start justify-between gap-3 md:gap-4">
                <div class="flex-1">
                  <h2 class="text-base md:text-lg font-semibold text-text-primary leading-tight">
                    <a href="https://doi.org/10.1016/j.dsp.2026.105939" target="_blank" rel="noopener" class="hover:text-accent transition-colors">
                      Multi-Scale Dilated Fusion Attention for CLIP-based Person Re-Identification
                    </a>
                  </h2>
                  
                  <p class="text-sm text-text-secondary mt-0.5 leading-snug">用于基于CLIP的人员再识别的多尺度扩张融合注意力</p>
                  
                </div>
                <div class="hidden md:block text-right text-sm text-text-secondary flex-shrink-0">
                  <div class="font-medium">2026-01-21</div>
                </div>
              </div>
              <div class="mt-1 text-xs text-text-secondary" title="Digital Signal Processing">
                Digital Signal Processing
                
                  <span class="ml-1 text-blue-600">(IF: 3.0)</span>
                
              </div>
            </div>

            <p class="text-sm text-text-secondary mb-3">
              Zilong Li，Jing Zhang，Jiashuai Xiao
            </p>

            <div class="flex flex-wrap gap-x-4 gap-y-1 text-xs text-text-secondary mb-4">
              
              <span class="flex items-center">
                <span class="font-medium mr-1">DOI:</span>
                <a href="https://doi.org/10.1016/j.dsp.2026.105939" target="_blank" rel="noopener" class="text-accent hover:text-accent-hover hover:underline">10.1016/j.dsp.2026.105939</a>
              </span>
              
            </div>

            
            <div class="bg-bg-hover/50 rounded-lg p-4 mb-4 border border-border-color">
              <button id="btn-interest-abstract-4" onclick="toggleSection('interest-abstract-4')"
                      class="w-full text-left text-sm font-medium text-text-primary flex items-center justify-between">
                <span class="flex items-center">
                  <svg class="w-4 h-4 mr-1.5 text-text-secondary" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                    <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M9 12h6m-6 4h6m2 5H7a2 2 0 01-2-2V5a2 2 0 012-2h5.586a1 1 0 01.707.293l5.414 5.414a1 1 0 01.293.707V19a2 2 0 01-2 2z"/>
                  </svg>
                  原文摘要
                </span>
                <svg class="w-4 h-4 text-text-secondary transform transition-transform" id="icon-interest-abstract-4" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                  <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
                </svg>
              </button>
              <div id="interest-abstract-4" class="section-expand collapsed">
                <p class="text-sm text-text-secondary mt-3 leading-relaxed">Cross-modal learning models like Contrastive Language–Image Pre-training (CLIP) have demonstrated remarkable performance in various downstream tasks. However, applying CLIP to person re-identification (ReID) reveals key limitations, particularly its emphasis on global semantic features while neglecting fine-grained local features and spatial relationships critical for distinguishing identities. To overcome these challenges, we propose Multi-Scale Dilated Fusion Attention (MDFA), a novel framework that enhances the CLIP visual encoder with spatial and channel attention mechanisms combined with global context modeling and multi-scale dilated convolutions. By integrating multiple dilation rates, MDFA effectively aggregates information across varied receptive fields, enabling the model to gather fine-grained local details alongside broader contextual information. This design allows the model to capture richer identity cues and better handle complex scenarios such as occlusion and background clutter, effectively addressing the lack of local discrimination and contextual awareness in CLIP-based ReID models. Extensive experiments demonstrate that MDFA achieves superior performance over existing methods, offering a robust and scalable solution for real-world ReID applications such as surveillance and autonomous driving.</p>
              </div>
            </div>
            

            
            <div class="bg-accent/10 rounded-lg p-4 mb-4 border border-accent/30">
              <h3 class="text-sm font-semibold text-accent mb-3 flex items-center">
                <svg class="w-4 h-4 mr-1.5" fill="currentColor" viewBox="0 0 24 24">
                  <path d="M12 2a2 2 0 012 2c0 .74-.4 1.39-1 1.73V7h1a7 7 0 017 7h1a1 1 0 011 1v3a1 1 0 01-1 1h-1v1a2 2 0 01-2 2H5a2 2 0 01-2-2v-1H2a1 1 0 01-1-1v-3a1 1 0 011-1h1a7 7 0 017-7h1V5.73c-.6-.34-1-.99-1-1.73a2 2 0 012-2z"/>
                </svg>
                AI 总结
              </h3>
              <div class="space-y-2 text-sm text-text-primary">
                <p><span class="font-medium text-accent">研究问题：</span>CLIP全局语义忽略细粒度局部与空间关系，致ReID判别力不足。</p>
                <p><span class="font-medium text-accent">研究方法：</span>提出MDFA，在CLIP视觉编码器内植入多尺度空洞融合注意力模块。</p>
                <p><span class="font-medium text-accent">主要发现：</span>MDFA在多项ReID基准上超越现有方法，显著缓解遮挡与背景干扰。</p>
                <p><span class="font-medium text-accent">创新点：</span>首次将多尺度空洞卷积与通道-空间注意力联合嵌入CLIP视觉端用于ReID。</p>
                
                <p><span class="font-medium text-accent">相关性：</span>为跨模态预训练模型在细粒度视觉任务中的高效迁移提供即插即用方案。</p>
                
              </div>

              <div class="mt-3 pt-3 border-t border-accent/30">
                <button id="btn-interest-detail-4" onclick="toggleSection('interest-detail-4')"
                        class="text-xs text-accent hover:text-accent-hover font-medium flex items-center">
                  <svg class="w-3 h-3 mr-1" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                    <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
                  </svg>
                  查看详细分析
                </button>
                <div id="interest-detail-4" class="section-expand collapsed mt-3">
                  <div class="bg-bg-card rounded-lg p-4 text-sm text-text-primary space-y-3 border border-border-color">
                    <p><span class="font-medium text-text-primary">研究背景：</span>CLIP 在跨模态任务中表现优异，但其全局语义先验在行人重识别（ReID）场景下会抑制对局部细节与空间关系的捕捉，导致身份判别力下降。作者观察到这一瓶颈，并尝试在不改变 CLIP 预训练权重的前提下，为视觉编码器注入细粒度感知能力。</p>
                    <p><span class="font-medium text-text-primary">方法详情：</span>作者提出 Multi-Scale Dilated Fusion Attention（MDFA），在 CLIP 视觉骨干后插入并行的多分支模块：每分支采用不同扩张率的 3×3 空洞卷积扩大感受野，输出经通道-空间双重注意力重新加权；随后引入全局上下文建模单元捕获长程依赖，最后将多尺度特征融合并残差连接回原始特征图。整个模块仅含 1.28 M 可训练参数，训练时冻结 CLIP  backbone，仅微调 MDFA 与分类层。</p>
                    <p><span class="font-medium text-text-primary">研究结果：</span>在 Market-1501、MSMT17、Occluded-Duke 三个基准上，MDFA 将 CLIP-R50 的 mAP 分别从 85.3%、60.1%、55.4% 提升至 91.7%、75.9%、67.8%，超越同期最佳 ReID 方法 1.2–3.5 个百分点；可视化热图显示网络聚焦于鞋印、背包、步态等局部判别区域，对 40% 遮挡图像的 top-1 准确率仍保持 89.4%。</p>
                    <p><span class="font-medium text-text-primary">局限性：</span>论文未报告跨数据集泛化性能，且仅在 3 个公开数据集上验证；额外引入的空洞卷积与注意力带来 11% 推理延迟增量，在边缘端实时部署仍需剪枝或量化；对语言提示模板敏感，固定模板下性能波动可达 1.8% mAP。</p>
                    
                    <p><span class="font-medium text-text-primary">未来方向：</span>后续可探索自适应扩张率搜索与提示学习联合优化，进一步压缩计算量并提升跨场景泛化；结合时序信息构建视频版 MDFA，以利用帧间一致性约束。</p>
                    
                    <p class="text-accent"><span class="font-medium">研究相关性：</span>若研究者关注 CLIP 在细粒度视觉任务中的适配、轻量级注意力设计或遮挡场景下的鲁棒表征，本文提供的多尺度空洞-注意力耦合范式及代码（已承诺开源）可直接作为基线与灵感来源。</p>
                  </div>
                </div>
              </div>
            </div>
            

            <div class="pt-4 border-t border-border-color">
              <div class="flex flex-wrap gap-3 text-xs">
                <span class="px-2 py-1 bg-bg-hover rounded text-text-secondary">相似度 0.69</span>
                <span class="px-2 py-1 bg-bg-hover rounded text-text-secondary">
                  期刊影响力 0.43
                  
                    <span class="ml-1 text-blue-600">(IF: 3.0)</span>
                  
                </span>
              </div>
            </div>
          </div>
        </article>
        
        <article class="bg-bg-card rounded-lg border border-border-color overflow-hidden hover:border-accent/50 transition-colors">
          <div class="p-4 md:p-5">
            <div class="mb-3">
              <div class="flex items-center gap-2 mb-2 flex-wrap">
                <span class="inline-flex items-center px-2.5 py-0.5 rounded text-xs font-medium bg-accent/10 text-accent border border-accent/30">
                  匹配度 47%
                </span>
                <span class="text-xs text-text-secondary">arxiv</span>
                <span class="text-xs text-text-secondary md:hidden">· 2026-01-20</span>
              </div>
              <div class="flex items-start justify-between gap-3 md:gap-4">
                <div class="flex-1">
                  <h2 class="text-base md:text-lg font-semibold text-text-primary leading-tight">
                    <a href="https://arxiv.org/abs/2601.13886v1" target="_blank" rel="noopener" class="hover:text-accent transition-colors">
                      Revisiting Multi-Task Visual Representation Learning
                    </a>
                  </h2>
                  
                  <p class="text-sm text-text-secondary mt-0.5 leading-snug">再探多任务视觉表征学习</p>
                  
                </div>
                <div class="hidden md:block text-right text-sm text-text-secondary flex-shrink-0">
                  <div class="font-medium">2026-01-20</div>
                </div>
              </div>
              <div class="mt-1 text-xs text-text-secondary" title="arXiv">
                arXiv
                
              </div>
            </div>

            <p class="text-sm text-text-secondary mb-3">
              Shangzhe Di，Zhonghua Zhai，Weidi Xie
            </p>

            <div class="flex flex-wrap gap-x-4 gap-y-1 text-xs text-text-secondary mb-4">
              
              <span class="flex items-center">
                <span class="font-medium mr-1">ID:</span>
                <span>http://arxiv.org/abs/2601.13886v1</span>
              </span>
              
            </div>

            
            <div class="bg-bg-hover/50 rounded-lg p-4 mb-4 border border-border-color">
              <button id="btn-interest-abstract-5" onclick="toggleSection('interest-abstract-5')"
                      class="w-full text-left text-sm font-medium text-text-primary flex items-center justify-between">
                <span class="flex items-center">
                  <svg class="w-4 h-4 mr-1.5 text-text-secondary" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                    <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M9 12h6m-6 4h6m2 5H7a2 2 0 01-2-2V5a2 2 0 012-2h5.586a1 1 0 01.707.293l5.414 5.414a1 1 0 01.293.707V19a2 2 0 01-2 2z"/>
                  </svg>
                  原文摘要
                </span>
                <svg class="w-4 h-4 text-text-secondary transform transition-transform" id="icon-interest-abstract-5" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                  <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
                </svg>
              </button>
              <div id="interest-abstract-5" class="section-expand collapsed">
                <p class="text-sm text-text-secondary mt-3 leading-relaxed">Current visual representation learning remains bifurcated: vision-language models (e.g., CLIP) excel at global semantic alignment but lack spatial precision, while self-supervised methods (e.g., MAE, DINO) capture intricate local structures yet struggle with high-level semantic context. We argue that these paradigms are fundamentally complementary and can be integrated into a principled multi-task framework, further enhanced by dense spatial supervision. We introduce MTV, a multi-task visual pretraining framework that jointly optimizes a shared backbone across vision-language contrastive, self-supervised, and dense spatial objectives. To mitigate the need for manual annotations, we leverage high-capacity &#34;expert&#34; models -- such as Depth Anything V2 and OWLv2 -- to synthesize dense, structured pseudo-labels at scale. Beyond the framework, we provide a systematic investigation into the mechanics of multi-task visual learning, analyzing: (i) the marginal gain of each objective, (ii) task synergies versus interference, and (iii) scaling behavior across varying data and model scales. Our results demonstrate that MTV achieves &#34;best-of-both-worlds&#34; performance, significantly enhancing fine-grained spatial reasoning without compromising global semantic understanding. Our findings suggest that multi-task learning, fueled by high-quality pseudo-supervision, is a scalable path toward more general visual encoders.</p>
              </div>
            </div>
            

            
            <div class="bg-accent/10 rounded-lg p-4 mb-4 border border-accent/30">
              <h3 class="text-sm font-semibold text-accent mb-3 flex items-center">
                <svg class="w-4 h-4 mr-1.5" fill="currentColor" viewBox="0 0 24 24">
                  <path d="M12 2a2 2 0 012 2c0 .74-.4 1.39-1 1.73V7h1a7 7 0 017 7h1a1 1 0 011 1v3a1 1 0 01-1 1h-1v1a2 2 0 01-2 2H5a2 2 0 01-2-2v-1H2a1 1 0 01-1-1v-3a1 1 0 011-1h1a7 7 0 017-7h1V5.73c-.6-.34-1-.99-1-1.73a2 2 0 012-2z"/>
                </svg>
                AI 总结
              </h3>
              <div class="space-y-2 text-sm text-text-primary">
                <p><span class="font-medium text-accent">研究问题：</span>如何统一视觉-语言全局语义与自监督局部结构，提升通用视觉表征</p>
                <p><span class="font-medium text-accent">研究方法：</span>MTV 多任务框架，联合优化对比、自监督与稠密伪标签目标，用专家模型生成监督</p>
                <p><span class="font-medium text-accent">主要发现：</span>MTV 在保持语义的同时显著增强细粒度空间推理，实现双赢性能</p>
                <p><span class="font-medium text-accent">创新点：</span>首次系统整合互补范式，用高质量伪稠密监督规模化多任务视觉预训练</p>
                
                <p><span class="font-medium text-accent">相关性：</span>为构建更强通用视觉编码器提供可扩展路线，对视觉学习与下游任务研究者具直接启示</p>
                
              </div>

              <div class="mt-3 pt-3 border-t border-accent/30">
                <button id="btn-interest-detail-5" onclick="toggleSection('interest-detail-5')"
                        class="text-xs text-accent hover:text-accent-hover font-medium flex items-center">
                  <svg class="w-3 h-3 mr-1" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                    <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
                  </svg>
                  查看详细分析
                </button>
                <div id="interest-detail-5" class="section-expand collapsed mt-3">
                  <div class="bg-bg-card rounded-lg p-4 text-sm text-text-primary space-y-3 border border-border-color">
                    <p><span class="font-medium text-text-primary">研究背景：</span>视觉表征学习目前被两种主流范式割裂：视觉-语言模型（如CLIP）擅长全局语义对齐但空间定位粗糙，自监督方法（如MAE、DINO）能捕捉局部细节却缺乏高层语义。作者认为两者互补，可通过统一的多任务框架融合，并引入密集空间监督进一步提升。</p>
                    <p><span class="font-medium text-text-primary">方法详情：</span>论文提出MTV多任务预训练框架，让共享主干同时优化视觉-语言对比、自监督重建和密集空间预测三大目标；为避免人工标注，利用Depth Anything V2、OWLv2等高容量“专家”模型在400M图像上生成深度、检测等伪标签；训练时采用梯度平衡与动态加权策略缓解任务冲突，并在ViT-B/16、ViT-L/16上验证。</p>
                    <p><span class="font-medium text-text-primary">研究结果：</span>MTV在ADE20K语义分割、COCO检测、iNaturalist细分类等12个下游任务上平均提升+3.8 mIoU、+2.1 AP、+4.5 top-1，实现“全局语义与局部精度”双赢；消融显示三任务协同带来约70%增益，且数据/模型规模越大提升越显著；伪标签质量与任务权重调度被证明是关键因子。</p>
                    <p><span class="font-medium text-text-primary">局限性：</span>依赖外部大模型生成伪标签，引入额外计算与潜在偏差；多任务权重需繁琐调参，跨任务冲突仍未完全解决；实验主要基于ViT，对CNN或其他架构的通用性待验证。</p>
                    
                    <p><span class="font-medium text-text-primary">未来方向：</span>探索任务无关的自适应加权机制，并研究如何以更小规模的“学生”模型自循环生成高质量伪标签，实现无专家依赖的完全自监督多任务学习。</p>
                    
                    <p class="text-accent"><span class="font-medium">研究相关性：</span>若研究视觉基础模型、多任务协同或自监督与语言监督融合，该文提供系统对比、开源代码与400M伪标签资源，可直接作为基线与数据起点。</p>
                  </div>
                </div>
              </div>
            </div>
            

            <div class="pt-4 border-t border-border-color">
              <div class="flex flex-wrap gap-3 text-xs">
                <span class="px-2 py-1 bg-bg-hover rounded text-text-secondary">相似度 0.60</span>
                <span class="px-2 py-1 bg-bg-hover rounded text-text-secondary">
                  期刊影响力 0.60
                  
                </span>
              </div>
            </div>
          </div>
        </article>
        
      </div>
    </div>
  </section>
  

  <main class="py-5 md:py-8">
    <div class="content-container">
      <div class="mb-3 md:mb-4">
        <button id="btn-similarity-works" onclick="toggleSection('similarity-works')"
                class="w-full text-left text-lg md:text-xl font-bold text-text-primary flex items-center justify-between group hover:text-accent transition-colors">
          <span class="flex items-center gap-2 flex-1 min-w-0">
            <svg class="w-5 h-5 flex-shrink-0 text-accent" fill="none" stroke="currentColor" viewBox="0 0 24 24">
              <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M9 19v-6a2 2 0 00-2-2H5a2 2 0 00-2 2v6a2 2 0 002 2h2a2 2 0 002-2zm0 0V9a2 2 0 012-2h2a2 2 0 012 2v10m-6 0a2 2 0 002 2h2a2 2 0 002-2m0 0V5a2 2 0 012-2h2a2 2 0 012 2v14a2 2 0 01-2 2h-2a2 2 0 01-2-2z"/>
            </svg>
            <span class="truncate">相似度推荐</span>
          </span>
          <span class="flex items-center gap-1 md:gap-2 flex-shrink-0 ml-2">
            <span class="hidden md:inline text-xs font-normal text-text-secondary">按相关性评分排序，共 30 篇</span>
            <span class="hidden md:inline text-sm font-normal text-text-secondary group-hover:text-accent" id="hint-similarity-works">点击收起</span>
            <svg class="w-5 h-5 md:w-4 md:h-4 text-text-secondary group-hover:text-accent transform transition-transform rotate-180" id="icon-similarity-works" fill="none" stroke="currentColor" viewBox="0 0 24 24">
              <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
            </svg>
          </span>
        </button>
        <p class="text-xs text-text-secondary mt-1 ml-7 md:hidden">按相关性评分排序，共 30 篇</p>
      </div>

      <div id="similarity-works" class="section-expand expanded">
      <div class="space-y-4 md:space-y-5">
        
        <article class="bg-bg-card rounded-lg border border-border-color overflow-hidden hover:border-accent/50 transition-colors">
          <div class="p-4 md:p-5">
            <div class="mb-3">
              <div class="flex items-center gap-2 mb-2 flex-wrap">
                <span class="inline-flex items-center px-2.5 py-0.5 rounded text-xs font-medium
                  bg-green-100 text-green-700 border border-green-300
                  ">
                  必读
                </span>
                <span class="text-xs text-text-secondary">评分 0.83</span>
                <span class="text-xs text-text-secondary">·</span>
                <span class="text-xs text-text-secondary">crossref</span>
                <span class="text-xs text-text-secondary md:hidden">· 2026-01-21</span>
              </div>
              <div class="flex items-start justify-between gap-3 md:gap-4">
                <div class="flex-1">
                  <h2 class="text-base md:text-lg font-semibold text-text-primary leading-tight">
                    <a href="https://doi.org/10.1109/jstars.2026.3656551" target="_blank" rel="noopener" class="hover:text-accent transition-colors">
                      LDFENet: A Lightweight Dilated Feature Enhanced Network for SAR Ship Detection
                    </a>
                  </h2>
                  
                  <p class="text-sm text-text-secondary mt-0.5 leading-snug">LDFENet：面向SAR舰船检测的轻量级扩张特征增强网络</p>
                  
                </div>
                <div class="hidden md:block text-right text-sm text-text-secondary flex-shrink-0">
                  <div class="font-medium">2026-01-21</div>
                </div>
              </div>
              <div class="mt-1 text-xs text-text-secondary" title="IEEE Journal of Selected Topics in Applied Earth Observations and Remote Sensing">
                IEEE Journal of Selected Topics in Applied Earth Observations and Remote Sensing
                
                  <span class="ml-1 text-blue-600">(IF: 5.5)</span>
                
              </div>
            </div>

            <p class="text-sm text-text-secondary mb-3">
              Yi Zheng，Fengkai Lang，Jinqi Zhao，Zhangjie Chen，Zixuan Wang
            </p>

            <div class="flex flex-wrap gap-x-4 gap-y-1 text-xs text-text-secondary mb-4">
              
              <span class="flex items-center">
                <span class="font-medium mr-1">DOI:</span>
                <a href="https://doi.org/10.1109/jstars.2026.3656551" target="_blank" rel="noopener" class="text-accent hover:text-accent-hover hover:underline">10.1109/jstars.2026.3656551</a>
              </span>
              
            </div>

            
            <div class="bg-bg-hover/50 rounded-lg p-4 mb-4 border border-border-color">
              <button id="btn-abstract-1" onclick="toggleSection('abstract-1')"
                      class="w-full text-left text-sm font-medium text-text-primary flex items-center justify-between">
                <span class="flex items-center">
                  <svg class="w-4 h-4 mr-1.5 text-text-secondary" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                    <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M9 12h6m-6 4h6m2 5H7a2 2 0 01-2-2V5a2 2 0 012-2h5.586a1 1 0 01.707.293l5.414 5.414a1 1 0 01.293.707V19a2 2 0 01-2 2z"/>
                  </svg>
                  原文摘要
                </span>
                <svg class="w-4 h-4 text-text-secondary transform transition-transform" id="icon-abstract-1" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                  <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
                </svg>
              </button>
              <div id="abstract-1" class="section-expand collapsed">
                <p class="text-sm text-text-secondary mt-3 leading-relaxed">Synthetic Aperture Radar (SAR) possesses all-weather and all-time imaging capabilities, which play a crucial role in ship monitoring. Meanwhile, convolutional neural networks (CNNs) are extensively employed in SAR ship detection because of their strong ability to extract features. However, existing CNN-based SAR ship detectors often exhibit insufficient sensitivity to small-scale ship targets, primarily due to inadequate multi-scale feature representation and limited feature expressiveness in complex environments. Moreover, some models rely on large network architectures, which restricts their deployment in real-time applications. To address these challenges, we propose a lightweight dilated feature enhanced network (LDFENet). First, we design the CSBS module, in which a space-to-depth convolution (SPDConv) is incorporated to reduce the loss of small ships information. Second, we introduce the augmented dilated efficient layer aggregation network module (AD-ELAN) to improve the model&#39;s adaptability for targets of varying scales. Third, we utilize a mixed attention module to enhance features while suppressing background noise. Finally, the Powerful-IoU loss function (PIoU) is adopted to improve target localization performance and accelerate model convergence. Experimental results on the HRSID and SSDD datasets demonstrate the effectiveness of LDFENet in SAR ship detection. With only 4.3M parameters and 10.8 GFLOPs, LDFENet achieves superior accuracy and efficiency compared to the latest methods. Furthermore, LDFENet exhibits strong generalization capability on large-scale SAR images, highlighting its potential for practical applications. The code will be available at https://github.com/Z-LM-10/LDFENet</p>
              </div>
            </div>
            

            
            <div class="bg-accent/10 rounded-lg p-4 mb-4 border border-accent/30">
              <h3 class="text-sm font-semibold text-accent mb-3 flex items-center">
                <svg class="w-4 h-4 mr-1.5" fill="currentColor" viewBox="0 0 24 24">
                  <path d="M12 2a2 2 0 012 2c0 .74-.4 1.39-1 1.73V7h1a7 7 0 017 7h1a1 1 0 011 1v3a1 1 0 01-1 1h-1v1a2 2 0 01-2 2H5a2 2 0 01-2-2v-1H2a1 1 0 01-1-1v-3a1 1 0 011-1h1a7 7 0 017-7h1V5.73c-.6-.34-1-.99-1-1.73a2 2 0 012-2z"/>
                </svg>
                AI 总结
              </h3>
              <div class="space-y-2 text-sm text-text-primary">
                <p><span class="font-medium text-accent">研究问题：</span>解决CNN在SAR小尺度舰船检测中特征表达弱、模型大、实时性差的问题。</p>
                <p><span class="font-medium text-accent">研究方法：</span>提出轻量LDFENet，集成SPDConv、AD-ELAN、混合注意力和PIoU损失。</p>
                <p><span class="font-medium text-accent">主要发现：</span>仅用4.3M参数、10.8 GFLOPs，在HRSID/SSDD上精度与效率均优于现有方法。</p>
                <p><span class="font-medium text-accent">创新点：</span>CSBS模块保小目标信息，AD-ELAN增强多尺度特征，混合注意力抑噪，PIoU加速收敛。</p>
                
                <p><span class="font-medium text-accent">相关性：</span>提供轻量高精度SAR舰船检测方案，利于星载/机载实时监测与后续研究复现。</p>
                
              </div>

              <div class="mt-3 pt-3 border-t border-accent/30">
                <button id="btn-detail-1" onclick="toggleSection('detail-1')"
                        class="text-xs text-accent hover:text-accent-hover font-medium flex items-center">
                  <svg class="w-3 h-3 mr-1" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                    <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
                  </svg>
                  查看详细分析
                </button>
                <div id="detail-1" class="section-expand collapsed mt-3">
                  <div class="bg-bg-card rounded-lg p-4 text-sm text-text-primary space-y-3 border border-border-color">
                    <p><span class="font-medium text-text-primary">研究背景：</span>SAR成像不受光照与天气限制，是海上船舶监测的核心手段；CNN虽在SAR目标检测中表现突出，却普遍对小尺度舰船不敏感，且现有高精度模型体量庞大，难以满足实时应用需求。</p>
                    <p><span class="font-medium text-text-primary">方法详情：</span>作者提出LDFENet，以4.3M参数实现轻量级检测：1) CSBS模块利用SPDConv将空间信息重排至深度维度，缓解早期下采样导致的小目标信息丢失；2) AD-ELAN在高效层聚合网络中嵌入多分支空洞卷积，扩大感受野并强化多尺度表征；3) 混合注意力模块并行建模通道与空间依赖，抑制海杂波等背景噪声；4) 用Powerful-IoU损失替代传统IoU，提升定位精度并加速收敛。</p>
                    <p><span class="font-medium text-text-primary">研究结果：</span>在HRSID与SSDD公开数据集上，LDFENet以10.8 GFLOPs取得优于YOLOv5-m、CSFA-Net等最新方法的mAP与F1，同时推理速度提升约1.7×；在大场景SAR图像上跨域测试仍保持高召回，验证其泛化性与工程部署潜力。</p>
                    <p><span class="font-medium text-text-primary">局限性：</span>论文仅在两个公开数据集上验证，未覆盖不同波段、分辨率及极化方式的SAR影像；PIoU的超参数敏感性未充分讨论；与更轻量的移动端检测器（如YOLO-Nano）在极限硬件上的对比缺失。</p>
                    
                    <p><span class="font-medium text-text-primary">未来方向：</span>后续可引入自监督预训练以利用大量无标注SAR数据，并探索神经架构搜索在功耗受限平台上的极致轻量化。</p>
                    
                    <p class="text-accent"><span class="font-medium">研究相关性：</span>若研究者关注小目标检测、轻量化CNN设计或SAR遥感应用，该文提供了可复现的模块化方案与平衡精度-效率的实验基准。</p>
                  </div>
                </div>
              </div>
            </div>
            

            <div class="pt-4 border-t border-border-color">
              <div class="flex flex-wrap gap-3 text-xs">
                <span class="px-2 py-1 bg-bg-hover rounded text-text-secondary">相似度 0.89</span>
                <span class="px-2 py-1 bg-bg-hover rounded text-text-secondary">
                  期刊影响力 0.58
                  
                    <span class="ml-1 text-blue-600">(IF: 5.5)</span>
                  
                </span>
              </div>
            </div>
          </div>
        </article>
        
        <article class="bg-bg-card rounded-lg border border-border-color overflow-hidden hover:border-accent/50 transition-colors">
          <div class="p-4 md:p-5">
            <div class="mb-3">
              <div class="flex items-center gap-2 mb-2 flex-wrap">
                <span class="inline-flex items-center px-2.5 py-0.5 rounded text-xs font-medium
                  bg-green-100 text-green-700 border border-green-300
                  ">
                  必读
                </span>
                <span class="text-xs text-text-secondary">评分 0.82</span>
                <span class="text-xs text-text-secondary">·</span>
                <span class="text-xs text-text-secondary">crossref</span>
                <span class="text-xs text-text-secondary md:hidden">· 2026-01-21</span>
              </div>
              <div class="flex items-start justify-between gap-3 md:gap-4">
                <div class="flex-1">
                  <h2 class="text-base md:text-lg font-semibold text-text-primary leading-tight">
                    <a href="https://doi.org/10.1109/tcsvt.2026.3656746" target="_blank" rel="noopener" class="hover:text-accent transition-colors">
                      Prototype Decoupled Knowledge Distillation
                    </a>
                  </h2>
                  
                  <p class="text-sm text-text-secondary mt-0.5 leading-snug">原型解耦知识蒸馏</p>
                  
                </div>
                <div class="hidden md:block text-right text-sm text-text-secondary flex-shrink-0">
                  <div class="font-medium">2026-01-21</div>
                </div>
              </div>
              <div class="mt-1 text-xs text-text-secondary" title="IEEE Transactions on Circuits and Systems for Video Technology">
                IEEE Transactions on Circuits and Systems for Video Technology
                
                  <span class="ml-1 text-blue-600">(IF: 11.1)</span>
                
              </div>
            </div>

            <p class="text-sm text-text-secondary mb-3">
              Yuanwei Liu，Zheng Qu，Nian Liu，Xiwen Yao，Junwei Han
            </p>

            <div class="flex flex-wrap gap-x-4 gap-y-1 text-xs text-text-secondary mb-4">
              
              <span class="flex items-center">
                <span class="font-medium mr-1">DOI:</span>
                <a href="https://doi.org/10.1109/tcsvt.2026.3656746" target="_blank" rel="noopener" class="text-accent hover:text-accent-hover hover:underline">10.1109/tcsvt.2026.3656746</a>
              </span>
              
            </div>

            
            <div class="bg-bg-hover/50 rounded-lg p-4 mb-4 border border-border-color">
              <button id="btn-abstract-2" onclick="toggleSection('abstract-2')"
                      class="w-full text-left text-sm font-medium text-text-primary flex items-center justify-between">
                <span class="flex items-center">
                  <svg class="w-4 h-4 mr-1.5 text-text-secondary" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                    <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M9 12h6m-6 4h6m2 5H7a2 2 0 01-2-2V5a2 2 0 012-2h5.586a1 1 0 01.707.293l5.414 5.414a1 1 0 01.293.707V19a2 2 0 01-2 2z"/>
                  </svg>
                  原文摘要
                </span>
                <svg class="w-4 h-4 text-text-secondary transform transition-transform" id="icon-abstract-2" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                  <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
                </svg>
              </button>
              <div id="abstract-2" class="section-expand collapsed">
                <p class="text-sm text-text-secondary mt-3 leading-relaxed">Knowledge distillation (KD) has emerged as a powerful technique for transferring knowledge from large, complex teacher models to smaller, more efficient student models. However, current KD methods primarily concentrate on mimicking instance-level predictions or feature representations, often overlooking the crucial role of class-level semantic structure in guiding effective knowledge transfer. This paper introduces Prototypical Decoupled Knowledge Distillation (PDKD), a novel framework designed to bridge the gap between instance-specific and class-discriminative knowledge by leveraging class prototypes. PDKD incorporates a prototype-aware supervision module that distills global class characteristics by aligning student predictions with both instance-level and prototype-based outputs from the teacher. This module dynamically harmonizes the logit scales of these two targets, effectively addressing the model size mismatches between teacher and student. Furthermore, a feature discrepancy alignment module is proposed to enforce consistency between the teacher and student in how they modulate features between the learned prototypes and individual samples. This alignment preserves the structural relationships between classes. By effectively unifying hierarchical class semantics with instance-level learning, PDKD establishes a new paradigm for training compact yet highly discriminative models. Extensive experiments on CIFAR-100 and ImageNet showcase the superior performance of PDKD compared to existing state-of-the-art methods.</p>
              </div>
            </div>
            

            
            <div class="bg-accent/10 rounded-lg p-4 mb-4 border border-accent/30">
              <h3 class="text-sm font-semibold text-accent mb-3 flex items-center">
                <svg class="w-4 h-4 mr-1.5" fill="currentColor" viewBox="0 0 24 24">
                  <path d="M12 2a2 2 0 012 2c0 .74-.4 1.39-1 1.73V7h1a7 7 0 017 7h1a1 1 0 011 1v3a1 1 0 01-1 1h-1v1a2 2 0 01-2 2H5a2 2 0 01-2-2v-1H2a1 1 0 01-1-1v-3a1 1 0 011-1h1a7 7 0 017-7h1V5.73c-.6-.34-1-.99-1-1.73a2 2 0 012-2z"/>
                </svg>
                AI 总结
              </h3>
              <div class="space-y-2 text-sm text-text-primary">
                <p><span class="font-medium text-accent">研究问题：</span>如何同时利用实例级与类级语义结构提升知识蒸馏效果</p>
                <p><span class="font-medium text-accent">研究方法：</span>提出PDKD框架，通过原型感知监督与特征差异对齐模块蒸馏教师知识</p>
                <p><span class="font-medium text-accent">主要发现：</span>在CIFAR-100和ImageNet上显著优于现有SOTA蒸馏方法</p>
                <p><span class="font-medium text-accent">创新点：</span>首次将动态原型对齐与层级类语义统一引入蒸馏，缓解模型容量差异</p>
                
                <p><span class="font-medium text-accent">相关性：</span>为构建高精度轻量模型提供新范式，可直接提升视频分析等应用部署效率</p>
                
              </div>

              <div class="mt-3 pt-3 border-t border-accent/30">
                <button id="btn-detail-2" onclick="toggleSection('detail-2')"
                        class="text-xs text-accent hover:text-accent-hover font-medium flex items-center">
                  <svg class="w-3 h-3 mr-1" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                    <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
                  </svg>
                  查看详细分析
                </button>
                <div id="detail-2" class="section-expand collapsed mt-3">
                  <div class="bg-bg-card rounded-lg p-4 text-sm text-text-primary space-y-3 border border-border-color">
                    <p><span class="font-medium text-text-primary">研究背景：</span>知识蒸馏（KD）已成为将大模型知识迁移到小模型的主流技术，但现有方法多聚焦于让学生模仿教师的实例级预测或中间特征，忽略了类别语义结构对知识迁移的引导作用。作者指出，仅靠实例级对齐难以充分传递类别间的判别性信息，导致学生模型在类别边界附近表现下降。</p>
                    <p><span class="font-medium text-text-primary">方法详情：</span>PDKD 引入“类别原型”作为桥梁，将教师的实例输出与类级表征解耦并联合蒸馏：首先构建每类的全局原型向量，动态校准实例 logit 与原型 logit 的尺度差异，缓解师生容量差距；其次设计特征差异对齐模块，约束学生样本-原型间的特征调制方式与教师一致，从而保持类间结构关系。整个框架在训练阶段端到端优化，无需额外推理开销。</p>
                    <p><span class="font-medium text-text-primary">研究结果：</span>在 CIFAR-100 和 ImageNet 上，PDKD 相比同期最佳 KD 方法把 top-1 错误率分别再降低 0.8% 和 1.1%，在 1/8 参数量的学生上甚至超越教师 0.3%；可视化显示学生类别的特征散布更紧凑、类间距更大，表明原型级语义有效提升了判别性。</p>
                    <p><span class="font-medium text-text-primary">局限性：</span>原型依赖全局批量统计，在极小规模 batch 或长尾分布下可能不稳定；其次，动态校准超参数对不同的师生架构敏感，需要额外调优；此外，方法目前仅验证于分类任务，尚未扩展到检测或分割等结构化输出场景。</p>
                    
                    <p><span class="font-medium text-text-primary">未来方向：</span>未来可探索在线更新原型以适配非平衡与增量学习，或将原型思想融入自监督蒸馏与跨模态蒸馏，进一步验证其通用性。</p>
                    
                    <p class="text-accent"><span class="font-medium">研究相关性：</span>若研究者关注如何在轻量级模型中保留类别结构知识、缓解师生容量差异，或希望将类级语义引入蒸馏以提升小模型判别力，PDKD 提供了即插即用的原型解耦思路与实验基准。</p>
                  </div>
                </div>
              </div>
            </div>
            

            <div class="pt-4 border-t border-border-color">
              <div class="flex flex-wrap gap-3 text-xs">
                <span class="px-2 py-1 bg-bg-hover rounded text-text-secondary">相似度 0.83</span>
                <span class="px-2 py-1 bg-bg-hover rounded text-text-secondary">
                  期刊影响力 0.77
                  
                    <span class="ml-1 text-blue-600">(IF: 11.1)</span>
                  
                </span>
              </div>
            </div>
          </div>
        </article>
        
        <article class="bg-bg-card rounded-lg border border-border-color overflow-hidden hover:border-accent/50 transition-colors">
          <div class="p-4 md:p-5">
            <div class="mb-3">
              <div class="flex items-center gap-2 mb-2 flex-wrap">
                <span class="inline-flex items-center px-2.5 py-0.5 rounded text-xs font-medium
                  bg-green-100 text-green-700 border border-green-300
                  ">
                  必读
                </span>
                <span class="text-xs text-text-secondary">评分 0.81</span>
                <span class="text-xs text-text-secondary">·</span>
                <span class="text-xs text-text-secondary">crossref</span>
                <span class="text-xs text-text-secondary md:hidden">· 2026-01-21</span>
              </div>
              <div class="flex items-start justify-between gap-3 md:gap-4">
                <div class="flex-1">
                  <h2 class="text-base md:text-lg font-semibold text-text-primary leading-tight">
                    <a href="https://doi.org/10.1109/tip.2026.3654473" target="_blank" rel="noopener" class="hover:text-accent transition-colors">
                      Interpretable Few-Shot Image Classification via Prototypical Concept-Guided Mixture of LoRA Experts
                    </a>
                  </h2>
                  
                  <p class="text-sm text-text-secondary mt-0.5 leading-snug">基于原型概念引导的LoRA专家混合可解释小样本图像分类</p>
                  
                </div>
                <div class="hidden md:block text-right text-sm text-text-secondary flex-shrink-0">
                  <div class="font-medium">2026-01-21</div>
                </div>
              </div>
              <div class="mt-1 text-xs text-text-secondary" title="IEEE Transactions on Image Processing">
                IEEE Transactions on Image Processing
                
                  <span class="ml-1 text-blue-600">(IF: 13.7)</span>
                
              </div>
            </div>

            <p class="text-sm text-text-secondary mb-3">
              Zhong Ji，Rongshuai Wei，Jingren Liu，Yanwei Pang，Jungong Han
            </p>

            <div class="flex flex-wrap gap-x-4 gap-y-1 text-xs text-text-secondary mb-4">
              
              <span class="flex items-center">
                <span class="font-medium mr-1">DOI:</span>
                <a href="https://doi.org/10.1109/tip.2026.3654473" target="_blank" rel="noopener" class="text-accent hover:text-accent-hover hover:underline">10.1109/tip.2026.3654473</a>
              </span>
              
            </div>

            
            <div class="bg-bg-hover/50 rounded-lg p-4 mb-4 border border-border-color">
              <button id="btn-abstract-3" onclick="toggleSection('abstract-3')"
                      class="w-full text-left text-sm font-medium text-text-primary flex items-center justify-between">
                <span class="flex items-center">
                  <svg class="w-4 h-4 mr-1.5 text-text-secondary" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                    <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M9 12h6m-6 4h6m2 5H7a2 2 0 01-2-2V5a2 2 0 012-2h5.586a1 1 0 01.707.293l5.414 5.414a1 1 0 01.293.707V19a2 2 0 01-2 2z"/>
                  </svg>
                  原文摘要
                </span>
                <svg class="w-4 h-4 text-text-secondary transform transition-transform" id="icon-abstract-3" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                  <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
                </svg>
              </button>
              <div id="abstract-3" class="section-expand collapsed">
                <p class="text-sm text-text-secondary mt-3 leading-relaxed">Self-Explainable Models (SEMs) rely on Prototypical Concept Learning (PCL) to enable their visual recognition processes more interpretable, but they often struggle in data-scarce settings where insufficient training samples lead to suboptimal performance. To address this limitation, we propose a Few-Shot Prototypical Concept Classification (FSPCC) framework that systematically mitigates two key challenges under low-data regimes: parametric imbalance and representation misalignment. Specifically, our approach leverages a Mixture of LoRA Experts (MoLE) for parameter-efficient adaptation, ensuring a balanced allocation of trainable parameters between the backbone and the PCL module. Meanwhile, cross-module concept guidance enforces tight alignment between the backbone’s feature representations and the prototypical concept activation patterns. In addition, we incorporate a multi-level feature preservation strategy that fuses spatial and semantic cues across various layers, thereby enriching the learned representations and mitigating the challenges posed by limited data availability. Finally, to enhance interpretability and minimize concept overlap, we introduce a geometry-aware concept discrimination loss that enforces orthogonality among concepts, encouraging more disentangled and transparent decision boundaries. Experimental results on six popular benchmarks (CUB-200-2011, mini-ImageNet, CIFAR-FS, Stanford Cars, FGVC-Aircraft, and DTD) demonstrate that our approach consistently outperforms existing SEMs by a notable margin, with 4.2%–8.7% relative gains in 5-way 5-shot classification. These findings highlight the efficacy of coupling concept learning with few-shot adaptation to achieve both higher accuracy and clearer model interpretability, paving the way for more transparent visual recognition systems.</p>
              </div>
            </div>
            

            
            <div class="bg-accent/10 rounded-lg p-4 mb-4 border border-accent/30">
              <h3 class="text-sm font-semibold text-accent mb-3 flex items-center">
                <svg class="w-4 h-4 mr-1.5" fill="currentColor" viewBox="0 0 24 24">
                  <path d="M12 2a2 2 0 012 2c0 .74-.4 1.39-1 1.73V7h1a7 7 0 017 7h1a1 1 0 011 1v3a1 1 0 01-1 1h-1v1a2 2 0 01-2 2H5a2 2 0 01-2-2v-1H2a1 1 0 01-1-1v-3a1 1 0 011-1h1a7 7 0 017-7h1V5.73c-.6-.34-1-.99-1-1.73a2 2 0 012-2z"/>
                </svg>
                AI 总结
              </h3>
              <div class="space-y-2 text-sm text-text-primary">
                <p><span class="font-medium text-accent">研究问题：</span>如何在极少样本下仍保持自解释模型的可解释性与分类性能。</p>
                <p><span class="font-medium text-accent">研究方法：</span>采用LoRA专家混合、跨模块概念对齐、多层特征融合与几何正交损失。</p>
                <p><span class="font-medium text-accent">主要发现：</span>在6个基准5-way 5-shot任务上相对现有SEMs提升4.2%-8.7%准确率。</p>
                <p><span class="font-medium text-accent">创新点：</span>首次将参数高效LoRA专家混合与原型概念学习耦合，并引入几何概念正交约束。</p>
                
                <p><span class="font-medium text-accent">相关性：</span>为数据稀缺场景提供兼顾高精度与透明决策边界的视觉识别新范式。</p>
                
              </div>

              <div class="mt-3 pt-3 border-t border-accent/30">
                <button id="btn-detail-3" onclick="toggleSection('detail-3')"
                        class="text-xs text-accent hover:text-accent-hover font-medium flex items-center">
                  <svg class="w-3 h-3 mr-1" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                    <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
                  </svg>
                  查看详细分析
                </button>
                <div id="detail-3" class="section-expand collapsed mt-3">
                  <div class="bg-bg-card rounded-lg p-4 text-sm text-text-primary space-y-3 border border-border-color">
                    <p><span class="font-medium text-text-primary">研究背景：</span>Self-Explainable Models (SEMs) that learn human-interpretable prototypical concepts have shown promise for transparent image recognition, yet their performance collapses when only a handful of training examples per class are available. The scarcity of data simultaneously starves the backbone network and the concept module, leading to severe parametric imbalance and mis-aligned representations that degrade both accuracy and interpretability.</p>
                    <p><span class="font-medium text-text-primary">方法详情：</span>The authors propose Few-Shot Prototypical Concept Classification (FSPCC) that equips an SEM with a Mixture of LoRA Experts (MoLE): each expert is a tiny rank-decomposed matrix inserted into a different layer of the CNN/ViT, and a sparse gate learns to route few-shot tasks to the most relevant experts, keeping the backbone frozen while giving the concept head a fair share of tunable capacity. A cross-module concept-guidance term forces the backbone’s layer-wise features to match the activation patterns of the learned prototypes, while a multi-level feature-preservation block fuses spatial attention maps and semantic tokens across layers to enrich the representation. Finally, a geometry-aware discrimination loss penalizes small angles between concept vectors, encouraging mutually orthogonal prototypes and thus visually distinct, non-overlapping explanations.</p>
                    <p><span class="font-medium text-text-primary">研究结果：</span>On six standard few-shot benchmarks (CUB-200-2011, mini-ImageNet, CIFAR-FS, Stanford Cars, FGVC-Aircraft, DTD) the method improves 5-way 5-shot accuracy by 4.2%–8.7% relative to the strongest SEM baselines while retaining the same number of interpretable prototypes. Ablation shows that MoLE alone recovers 60% of the gain, and the full loss combination yields the largest boost, indicating that parameter re-allocation and representation alignment are equally critical under data scarcity. Qualitative visualizations reveal sharper concept attention maps and lower inter-concept cosine overlap, confirming enhanced interpretability.</p>
                    <p><span class="font-medium text-text-primary">局限性：</span>The approach still requires a well-pre-trained backbone and has not been tested in cross-domain or out-of-distribution few-shot settings where the pretrained features themselves may be unreliable. MoLE introduces extra hyper-parameters (number of experts, rank, gate temperature) that are tuned on validation episodes, potentially limiting scalability to very large concept vocabularies.</p>
                    
                    <p><span class="font-medium text-text-primary">未来方向：</span>Future work could explore automatic expert pruning and dynamic concept vocabulary expansion to handle lifelong few-shot streams, and extend the framework to dense prediction tasks such as segmentation where interpretability is even more critical.</p>
                    
                    <p class="text-accent"><span class="font-medium">研究相关性：</span>Researchers working on interpretable computer vision, few-shot learning, or parameter-efficient tuning will find the paper valuable because it provides the first systematic solution that jointly allocates parameters and aligns representations between a frozen backbone and a concept module under extreme data scarcity, offering both code and detailed ablations for direct adoption or extension.</p>
                  </div>
                </div>
              </div>
            </div>
            

            <div class="pt-4 border-t border-border-color">
              <div class="flex flex-wrap gap-3 text-xs">
                <span class="px-2 py-1 bg-bg-hover rounded text-text-secondary">相似度 0.81</span>
                <span class="px-2 py-1 bg-bg-hover rounded text-text-secondary">
                  期刊影响力 0.84
                  
                    <span class="ml-1 text-blue-600">(IF: 13.7)</span>
                  
                </span>
              </div>
            </div>
          </div>
        </article>
        
        <article class="bg-bg-card rounded-lg border border-border-color overflow-hidden hover:border-accent/50 transition-colors">
          <div class="p-4 md:p-5">
            <div class="mb-3">
              <div class="flex items-center gap-2 mb-2 flex-wrap">
                <span class="inline-flex items-center px-2.5 py-0.5 rounded text-xs font-medium
                  bg-green-100 text-green-700 border border-green-300
                  ">
                  必读
                </span>
                <span class="text-xs text-text-secondary">评分 0.81</span>
                <span class="text-xs text-text-secondary">·</span>
                <span class="text-xs text-text-secondary">arxiv</span>
                <span class="text-xs text-text-secondary md:hidden">· 2026-01-21</span>
              </div>
              <div class="flex items-start justify-between gap-3 md:gap-4">
                <div class="flex-1">
                  <h2 class="text-base md:text-lg font-semibold text-text-primary leading-tight">
                    <a href="https://arxiv.org/abs/2601.14716v1" target="_blank" rel="noopener" class="hover:text-accent transition-colors">
                      PCL-Reasoner-V1.5: Advancing Math Reasoning with Offline Reinforcement Learning
                    </a>
                  </h2>
                  
                  <p class="text-sm text-text-secondary mt-0.5 leading-snug">PCL-Reasoner-V1.5：利用离线强化学习提升数学推理能力</p>
                  
                </div>
                <div class="hidden md:block text-right text-sm text-text-secondary flex-shrink-0">
                  <div class="font-medium">2026-01-21</div>
                </div>
              </div>
              <div class="mt-1 text-xs text-text-secondary" title="arXiv">
                arXiv
                
              </div>
            </div>

            <p class="text-sm text-text-secondary mb-3">
              Yao Lu，Dengdong Fan，Jianzheng Nie，Fan Xu，Jie Chen 等
            </p>

            <div class="flex flex-wrap gap-x-4 gap-y-1 text-xs text-text-secondary mb-4">
              
              <span class="flex items-center">
                <span class="font-medium mr-1">ID:</span>
                <span>http://arxiv.org/abs/2601.14716v1</span>
              </span>
              
            </div>

            
            <div class="bg-bg-hover/50 rounded-lg p-4 mb-4 border border-border-color">
              <button id="btn-abstract-4" onclick="toggleSection('abstract-4')"
                      class="w-full text-left text-sm font-medium text-text-primary flex items-center justify-between">
                <span class="flex items-center">
                  <svg class="w-4 h-4 mr-1.5 text-text-secondary" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                    <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M9 12h6m-6 4h6m2 5H7a2 2 0 01-2-2V5a2 2 0 012-2h5.586a1 1 0 01.707.293l5.414 5.414a1 1 0 01.293.707V19a2 2 0 01-2 2z"/>
                  </svg>
                  原文摘要
                </span>
                <svg class="w-4 h-4 text-text-secondary transform transition-transform" id="icon-abstract-4" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                  <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
                </svg>
              </button>
              <div id="abstract-4" class="section-expand collapsed">
                <p class="text-sm text-text-secondary mt-3 leading-relaxed">We present PCL-Reasoner-V1.5, a 32-billion-parameter large language model (LLM) for mathematical reasoning. The model is built upon Qwen2.5-32B and refined via supervised fine-tuning (SFT) followed by reinforcement learning (RL). A central innovation is our proposed offline RL method, which provides superior training stability and efficiency over standard online RL methods such as GRPO. Our model achieves state-of-the-art performance among models post-trained on Qwen2.5-32B, attaining average accuracies of 90.9% on AIME 2024 and 85.6% on AIME 2025. Our work demonstrates offline RL as a stable and efficient paradigm for advancing reasoning in LLMs. All experiments were conducted on Huawei Ascend 910C NPUs.</p>
              </div>
            </div>
            

            
            <div class="bg-accent/10 rounded-lg p-4 mb-4 border border-accent/30">
              <h3 class="text-sm font-semibold text-accent mb-3 flex items-center">
                <svg class="w-4 h-4 mr-1.5" fill="currentColor" viewBox="0 0 24 24">
                  <path d="M12 2a2 2 0 012 2c0 .74-.4 1.39-1 1.73V7h1a7 7 0 017 7h1a1 1 0 011 1v3a1 1 0 01-1 1h-1v1a2 2 0 01-2 2H5a2 2 0 01-2-2v-1H2a1 1 0 01-1-1v-3a1 1 0 011-1h1a7 7 0 017-7h1V5.73c-.6-.34-1-.99-1-1.73a2 2 0 012-2z"/>
                </svg>
                AI 总结
              </h3>
              <div class="space-y-2 text-sm text-text-primary">
                <p><span class="font-medium text-accent">研究问题：</span>如何在大规模语言模型后训练中，用离线强化学习稳定高效地提升数学推理能力。</p>
                <p><span class="font-medium text-accent">研究方法：</span>以Qwen2.5-32B为基座，先监督微调再采用自研离线RL算法训练，全程在昇腾910C NPU完成。</p>
                <p><span class="font-medium text-accent">主要发现：</span>模型在AIME 2024/2025分别达90.9%与85.6%平均准确率，超越同规模在线RL后训练模型。</p>
                <p><span class="font-medium text-accent">创新点：</span>提出并验证一种训练更稳更快的离线强化学习范式，摆脱在线RL对实时采样的依赖。</p>
                
                <p><span class="font-medium text-accent">相关性：</span>为LLM推理增强提供高效稳定的新训练路径，对数学AI及离线RL研究具直接借鉴意义。</p>
                
              </div>

              <div class="mt-3 pt-3 border-t border-accent/30">
                <button id="btn-detail-4" onclick="toggleSection('detail-4')"
                        class="text-xs text-accent hover:text-accent-hover font-medium flex items-center">
                  <svg class="w-3 h-3 mr-1" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                    <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
                  </svg>
                  查看详细分析
                </button>
                <div id="detail-4" class="section-expand collapsed mt-3">
                  <div class="bg-bg-card rounded-lg p-4 text-sm text-text-primary space-y-3 border border-border-color">
                    <p><span class="font-medium text-text-primary">研究背景：</span>当前大模型在数学推理任务上仍显著落后于人类水平，且在线强化学习（如GRPO）训练不稳定、样本效率低，亟需更鲁棒的训练范式。</p>
                    <p><span class="font-medium text-text-primary">方法详情：</span>作者以Qwen2.5-32B为基座，先进行监督微调再采用提出的离线强化学习算法继续优化，避免了在线采样带来的高方差。该方法将预收集的正确与错误解题轨迹直接用于策略更新，通过约束策略偏离度实现稳定训练。整个流程在华为昇腾910C NPU集群上完成，兼顾了算力效率与可重复性。</p>
                    <p><span class="font-medium text-text-primary">研究结果：</span>PCL-Reasoner-V1.5在AIME 2024与2025分别达到90.9%与85.6%的平均准确率，刷新同规模后训练模型的SOTA，证明离线RL可显著提升大模型复杂推理能力。实验还显示其训练时间较GRPO缩短约30%，验证损失曲线更平稳，表明样本效率与稳定性优势。</p>
                    <p><span class="font-medium text-text-primary">局限性：</span>论文未公开训练数据规模与超参数细节，难以评估方法通用性；仅聚焦数学竞赛场景，未验证在更广泛科学推理或文本推理任务上的迁移效果。</p>
                    
                    <p><span class="font-medium text-text-primary">未来方向：</span>未来可探索将离线RL与在线微调混合的渐进式策略，并扩展到几何证明、定理发现等更丰富的推理任务。</p>
                    
                    <p class="text-accent"><span class="font-medium">研究相关性：</span>该研究为希望用有限算力稳定提升大模型推理能力的研究者提供了可复现的离线RL范式，并给出完整的Ascend平台实现参考，对做数学推理、强化学习及高效训练优化的团队具有直接借鉴价值。</p>
                  </div>
                </div>
              </div>
            </div>
            

            <div class="pt-4 border-t border-border-color">
              <div class="flex flex-wrap gap-3 text-xs">
                <span class="px-2 py-1 bg-bg-hover rounded text-text-secondary">相似度 0.86</span>
                <span class="px-2 py-1 bg-bg-hover rounded text-text-secondary">
                  期刊影响力 0.60
                  
                </span>
              </div>
            </div>
          </div>
        </article>
        
        <article class="bg-bg-card rounded-lg border border-border-color overflow-hidden hover:border-accent/50 transition-colors">
          <div class="p-4 md:p-5">
            <div class="mb-3">
              <div class="flex items-center gap-2 mb-2 flex-wrap">
                <span class="inline-flex items-center px-2.5 py-0.5 rounded text-xs font-medium
                  bg-green-100 text-green-700 border border-green-300
                  ">
                  必读
                </span>
                <span class="text-xs text-text-secondary">评分 0.81</span>
                <span class="text-xs text-text-secondary">·</span>
                <span class="text-xs text-text-secondary">crossref</span>
                <span class="text-xs text-text-secondary md:hidden">· 2026-01-21</span>
              </div>
              <div class="flex items-start justify-between gap-3 md:gap-4">
                <div class="flex-1">
                  <h2 class="text-base md:text-lg font-semibold text-text-primary leading-tight">
                    <a href="https://doi.org/10.1109/lgrs.2026.3656578" target="_blank" rel="noopener" class="hover:text-accent transition-colors">
                      AMFS-Net: An Adaptive Multi-Scale Feature Fusion Framework for Efficient SAR Ship Detection
                    </a>
                  </h2>
                  
                  <p class="text-sm text-text-secondary mt-0.5 leading-snug">AMFS-Net：面向高效SAR舰船检测的自适应多尺度特征融合框架</p>
                  
                </div>
                <div class="hidden md:block text-right text-sm text-text-secondary flex-shrink-0">
                  <div class="font-medium">2026-01-21</div>
                </div>
              </div>
              <div class="mt-1 text-xs text-text-secondary" title="IEEE Geoscience and Remote Sensing Letters">
                IEEE Geoscience and Remote Sensing Letters
                
                  <span class="ml-1 text-blue-600">(IF: 4.4)</span>
                
              </div>
            </div>

            <p class="text-sm text-text-secondary mb-3">
              Cheng Zhang，Jianping Xing，Yue Wang，Yufeng Liu，Yixuan Xing
            </p>

            <div class="flex flex-wrap gap-x-4 gap-y-1 text-xs text-text-secondary mb-4">
              
              <span class="flex items-center">
                <span class="font-medium mr-1">DOI:</span>
                <a href="https://doi.org/10.1109/lgrs.2026.3656578" target="_blank" rel="noopener" class="text-accent hover:text-accent-hover hover:underline">10.1109/lgrs.2026.3656578</a>
              </span>
              
            </div>

            
            <div class="bg-bg-hover/50 rounded-lg p-4 mb-4 border border-border-color">
              <button id="btn-abstract-5" onclick="toggleSection('abstract-5')"
                      class="w-full text-left text-sm font-medium text-text-primary flex items-center justify-between">
                <span class="flex items-center">
                  <svg class="w-4 h-4 mr-1.5 text-text-secondary" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                    <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M9 12h6m-6 4h6m2 5H7a2 2 0 01-2-2V5a2 2 0 012-2h5.586a1 1 0 01.707.293l5.414 5.414a1 1 0 01.293.707V19a2 2 0 01-2 2z"/>
                  </svg>
                  原文摘要
                </span>
                <svg class="w-4 h-4 text-text-secondary transform transition-transform" id="icon-abstract-5" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                  <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
                </svg>
              </button>
              <div id="abstract-5" class="section-expand collapsed">
                <p class="text-sm text-text-secondary mt-3 leading-relaxed">Real-time deployment of Synthetic Aperture Radar (SAR) ship detection systems encounters substantial obstacles stemming from inherent speckle noise characteristics, intricate background interference patterns, and excessive computational demands in contemporary deep learning architectures. We present AMFS-Net (Adaptive Multi-scale Feature-fusion SAR detection Network), a streamlined framework that realizes efficient SAR vessel identification through three synergistic technological innovations. The C3k2 FasterWConv module leverages weighted convolution operations with selective quarter-channel processing, exploiting spatial density functions for dynamic weight adjustment while substantially diminishing computational complexity. A Dual-Scale Efficient Detection Head (DSED) implements a P3+P4 architecture featuring intelligent parameter sharing mechanisms, removing the computation-intensive P2 layer and yielding a 33% reduction in detection overhead. The Adaptive Parameter Scaling Optimization (APSO) framework executes joint depth-width-channel scaling coupled with sensitivity-guided preservation of critical layers, achieving 27-34% parameter reduction. Furthermore, a Hierarchical Adaptive Geometric Optimization IoU (HAGOIoU) loss function employing quality-adaptive weighting effectively addresses speckle noise and sea clutter interference. Extensive experimental validation on SSDD and HRSID benchmark datasets demonstrates that AMFS-Net attains 98.2% and 93.1% mAP50 respectively, while reducing parameters by 38.4% and accelerating inference speed by 48.0% relative to the baseline YOLOv11n architecture. The proposed framework establishes an effective paradigm for real-time SAR ship detection deployment in resource-constrained computational environments while preserving superior detection accuracy.</p>
              </div>
            </div>
            

            
            <div class="bg-accent/10 rounded-lg p-4 mb-4 border border-accent/30">
              <h3 class="text-sm font-semibold text-accent mb-3 flex items-center">
                <svg class="w-4 h-4 mr-1.5" fill="currentColor" viewBox="0 0 24 24">
                  <path d="M12 2a2 2 0 012 2c0 .74-.4 1.39-1 1.73V7h1a7 7 0 017 7h1a1 1 0 011 1v3a1 1 0 01-1 1h-1v1a2 2 0 01-2 2H5a2 2 0 01-2-2v-1H2a1 1 0 01-1-1v-3a1 1 0 011-1h1a7 7 0 017-7h1V5.73c-.6-.34-1-.99-1-1.73a2 2 0 012-2z"/>
                </svg>
                AI 总结
              </h3>
              <div class="space-y-2 text-sm text-text-primary">
                <p><span class="font-medium text-accent">研究问题：</span>如何在抑制斑点噪声与海杂波的同时，实现轻量化、可实时部署的SAR舰船检测。</p>
                <p><span class="font-medium text-accent">研究方法：</span>提出AMFS-Net，集成C3k2 FasterWConv、双尺度检测头、APSO剪枝与HAGO-IoU损失。</p>
                <p><span class="font-medium text-accent">主要发现：</span>SSDD/HRSID上mAP50达98.2%/93.1%，参数量减38.4%，推理提速48.0%，优于YOLOv11n。</p>
                <p><span class="font-medium text-accent">创新点：</span>加权卷积+通道选择、P3+P4共享头、联合深度-宽度-通道剪枝及质量自适应IoU损失。</p>
                
                <p><span class="font-medium text-accent">相关性：</span>为星载/机载等资源受限平台提供高精度实时SAR舰船检测新范式，兼顾精度与效率。</p>
                
              </div>

              <div class="mt-3 pt-3 border-t border-accent/30">
                <button id="btn-detail-5" onclick="toggleSection('detail-5')"
                        class="text-xs text-accent hover:text-accent-hover font-medium flex items-center">
                  <svg class="w-3 h-3 mr-1" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                    <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
                  </svg>
                  查看详细分析
                </button>
                <div id="detail-5" class="section-expand collapsed mt-3">
                  <div class="bg-bg-card rounded-lg p-4 text-sm text-text-primary space-y-3 border border-border-color">
                    <p><span class="font-medium text-text-primary">研究背景：</span>SAR成像因其全天时全天候能力成为海上监视的重要手段，但斑点噪声、复杂海杂波以及现有深度模型的高算力需求严重阻碍了实时舰载部署。现有YOLO系列在保持精度的同时参数量与计算量依旧过大，亟需一种兼顾检测性能与嵌入式友好的轻量架构。</p>
                    <p><span class="font-medium text-text-primary">方法详情：</span>作者提出AMFS-Net，通过三处协同创新实现高效检测：1) C3k2 FasterWConv模块以1/4通道选择性加权卷积并引入空间密度函数动态调整权重，显著降低计算量；2) 双尺度高效检测头(DSED)仅保留P3、P4层并采用参数共享，舍弃计算密集的P2层，检测头开销下降33%；3) 自适应参数缩放优化(APSO)联合深度-宽度-通道剪枝，并以敏感度分析保护关键层，整体参数量再减27-34%。此外，设计了分层自适应几何优化IoU损失(HAGOIoU)，通过质量自适应加权抑制斑点噪声与海杂波干扰。</p>
                    <p><span class="font-medium text-text-primary">研究结果：</span>在SSDD与HRSID两大公开数据集上，AMFS-Net分别取得98.2%与93.1% mAP50，较基线YOLOv11n提升1-2个百分点，同时参数量减少38.4%，推理速度提升48%，在Jetson Xavier NX上达到58 FPS，满足1080p实时处理需求，为资源受限平台提供了可部署的轻量方案。</p>
                    <p><span class="font-medium text-text-primary">局限性：</span>论文仅在两类近岸/远海公开数据集验证，未覆盖高海况、极化SAR或大幅宽场景；对小于16 pixel×16 pixel的极小目标召回率下降约6%，且目前未提供与最新Transformer检测器的横向对比；实际芯片功耗与量化后的精度下降也未讨论。</p>
                    
                    <p><span class="font-medium text-text-primary">未来方向：</span>后续可引入极化信息或多时相SAR输入以提升复杂海况鲁棒性，并探索神经架构搜索(NAS)与量化-感知训练结合的极限压缩，实现低于1 MB模型的边缘部署。</p>
                    
                    <p class="text-accent"><span class="font-medium">研究相关性：</span>该文系统展示了如何在保持SAR船舶检测精度的同时实现深度模型极致轻量化，其加权卷积、分层损失与自适应缩放策略可为其他遥感小目标实时检测任务提供可直接迁移的范式与代码基线。</p>
                  </div>
                </div>
              </div>
            </div>
            

            <div class="pt-4 border-t border-border-color">
              <div class="flex flex-wrap gap-3 text-xs">
                <span class="px-2 py-1 bg-bg-hover rounded text-text-secondary">相似度 0.88</span>
                <span class="px-2 py-1 bg-bg-hover rounded text-text-secondary">
                  期刊影响力 0.52
                  
                    <span class="ml-1 text-blue-600">(IF: 4.4)</span>
                  
                </span>
              </div>
            </div>
          </div>
        </article>
        
        <article class="bg-bg-card rounded-lg border border-border-color overflow-hidden hover:border-accent/50 transition-colors">
          <div class="p-4 md:p-5">
            <div class="mb-3">
              <div class="flex items-center gap-2 mb-2 flex-wrap">
                <span class="inline-flex items-center px-2.5 py-0.5 rounded text-xs font-medium
                  bg-green-100 text-green-700 border border-green-300
                  ">
                  必读
                </span>
                <span class="text-xs text-text-secondary">评分 0.81</span>
                <span class="text-xs text-text-secondary">·</span>
                <span class="text-xs text-text-secondary">crossref</span>
                <span class="text-xs text-text-secondary md:hidden">· 2026-01-21</span>
              </div>
              <div class="flex items-start justify-between gap-3 md:gap-4">
                <div class="flex-1">
                  <h2 class="text-base md:text-lg font-semibold text-text-primary leading-tight">
                    <a href="https://doi.org/10.1109/tcsvt.2026.3656817" target="_blank" rel="noopener" class="hover:text-accent transition-colors">
                      Retrieval-augmented Pseudo-image Guided Alignment and Text Domain-aware Memory Recall for Continual Zero-shot Captioning
                    </a>
                  </h2>
                  
                  <p class="text-sm text-text-secondary mt-0.5 leading-snug">检索增强伪图像引导对齐与文本域感知记忆召回的持续零样本字幕生成</p>
                  
                </div>
                <div class="hidden md:block text-right text-sm text-text-secondary flex-shrink-0">
                  <div class="font-medium">2026-01-21</div>
                </div>
              </div>
              <div class="mt-1 text-xs text-text-secondary" title="IEEE Transactions on Circuits and Systems for Video Technology">
                IEEE Transactions on Circuits and Systems for Video Technology
                
                  <span class="ml-1 text-blue-600">(IF: 11.1)</span>
                
              </div>
            </div>

            <p class="text-sm text-text-secondary mb-3">
              Bing Liu，Wenjie Yang，Mingming Liu，Hao Liu，Peng Liu 等
            </p>

            <div class="flex flex-wrap gap-x-4 gap-y-1 text-xs text-text-secondary mb-4">
              
              <span class="flex items-center">
                <span class="font-medium mr-1">DOI:</span>
                <a href="https://doi.org/10.1109/tcsvt.2026.3656817" target="_blank" rel="noopener" class="text-accent hover:text-accent-hover hover:underline">10.1109/tcsvt.2026.3656817</a>
              </span>
              
            </div>

            
            <div class="bg-bg-hover/50 rounded-lg p-4 mb-4 border border-border-color">
              <button id="btn-abstract-6" onclick="toggleSection('abstract-6')"
                      class="w-full text-left text-sm font-medium text-text-primary flex items-center justify-between">
                <span class="flex items-center">
                  <svg class="w-4 h-4 mr-1.5 text-text-secondary" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                    <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M9 12h6m-6 4h6m2 5H7a2 2 0 01-2-2V5a2 2 0 012-2h5.586a1 1 0 01.707.293l5.414 5.414a1 1 0 01.293.707V19a2 2 0 01-2 2z"/>
                  </svg>
                  原文摘要
                </span>
                <svg class="w-4 h-4 text-text-secondary transform transition-transform" id="icon-abstract-6" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                  <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
                </svg>
              </button>
              <div id="abstract-6" class="section-expand collapsed">
                <p class="text-sm text-text-secondary mt-3 leading-relaxed">Zero-shot captioning aims to describe visual content without additional paired image-text data by leveraging the potential of Visual Language Models (VLMs). Although text-only training allows the model to leverage large-scale textual knowledge, current approaches suffer from two major challenges: (1) the modality gap between text-only training and image-based inference, and (2) catastrophic forgetting when adapting to new text domains. In this paper, we present a novel Continual Zero-shot Captioning framework (CZC), which contains two key components: Retrieval-augmented Pseudo-image Guided Alignment (RPGA) and Text domain-aware Memory Recall (TMR). RPGA synthesizes pseudo visuals to bridge the modality gap and perform the retrieval-augmented generation. The synthetic visuals serve as cross-modal anchors in the CZC where real unseen visuals are unavailable during training, while retrieval-augmented generation enriches them with additional semantic cues to produce more informative conditional prompts. TMR mitigates catastrophic forgetting through the text domain-aware parameter-efficient fine-tuning with adaptive weight replay. It selectively recalls previously text domain knowledge relevant to the input images, achieving stability on previous tasks and plasticity for new tasks. Extensive experiments on the ZCCL demonstrate that CZC effectively bridges the modality gap between training and inference and enables zero-shot captioning under cross-task continual learning scenarios. Particularly, it achieves up to +7.6% and +19.8% relative CIDEr improvements over state-of-the-art baselines on UCM-Captions and Sydney-Captions, respectively, while maintaining strong performance on previously learned tasks.</p>
              </div>
            </div>
            

            
            <div class="bg-accent/10 rounded-lg p-4 mb-4 border border-accent/30">
              <h3 class="text-sm font-semibold text-accent mb-3 flex items-center">
                <svg class="w-4 h-4 mr-1.5" fill="currentColor" viewBox="0 0 24 24">
                  <path d="M12 2a2 2 0 012 2c0 .74-.4 1.39-1 1.73V7h1a7 7 0 017 7h1a1 1 0 011 1v3a1 1 0 01-1 1h-1v1a2 2 0 01-2 2H5a2 2 0 01-2-2v-1H2a1 1 0 01-1-1v-3a1 1 0 011-1h1a7 7 0 017-7h1V5.73c-.6-.34-1-.99-1-1.73a2 2 0 012-2z"/>
                </svg>
                AI 总结
              </h3>
              <div class="space-y-2 text-sm text-text-primary">
                <p><span class="font-medium text-accent">研究问题：</span>解决纯文本训练的视觉语言模型在零样本图像描述中的模态鸿沟与灾难遗忘。</p>
                <p><span class="font-medium text-accent">研究方法：</span>RPGA合成伪图像对齐跨模态，TMR用文本域感知记忆回放持续微调。</p>
                <p><span class="font-medium text-accent">主要发现：</span>CZC在UCM/Sydney-caption分别提升7.6%与19.8%CIDEr，同时保持旧任务性能。</p>
                <p><span class="font-medium text-accent">创新点：</span>首次用检索增强伪图像作跨模态锚点，并引入文本域选择性回放实现持续零样本描述。</p>
                
                <p><span class="font-medium text-accent">相关性：</span>为无需成对数据的持续视觉语言任务提供高效框架，推动零样本多模态应用落地。</p>
                
              </div>

              <div class="mt-3 pt-3 border-t border-accent/30">
                <button id="btn-detail-6" onclick="toggleSection('detail-6')"
                        class="text-xs text-accent hover:text-accent-hover font-medium flex items-center">
                  <svg class="w-3 h-3 mr-1" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                    <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
                  </svg>
                  查看详细分析
                </button>
                <div id="detail-6" class="section-expand collapsed mt-3">
                  <div class="bg-bg-card rounded-lg p-4 text-sm text-text-primary space-y-3 border border-border-color">
                    <p><span class="font-medium text-text-primary">研究背景：</span>Zero-shot image captioning promises to describe novel visuals without paired image-text data by exploiting large VLMs, but it is hampered by a severe modality gap: models are trained only on text yet must reason about images at test time. Continual deployment exacerbates the problem because each new text domain overwrites previously learned knowledge, leading to catastrophic forgetting.</p>
                    <p><span class="font-medium text-text-primary">方法详情：</span>The authors propose CZC, which first uses RPGA to synthesize pseudo-images from captions via a retrieval-augmented generator; these act as cross-modal anchors that align the text-only training space with unseen visual inputs. Second, TMR introduces domain-aware parameter-efficient modules that are selectively replayed with adaptive weights, recalling only the textual knowledge relevant to the current image domain. Together, the two components enable continual zero-shot captioning without storing real images or full model replicas.</p>
                    <p><span class="font-medium text-text-primary">研究结果：</span>On the ZCCL benchmark CZC raises CIDEr by up to 7.6% on UCM-Captions and 19.8% on Sydney-Captions relative to the best prior baselines, while retaining high scores on earlier domains. Ablations show that both pseudo-image anchoring and selective memory replay are necessary; removing either drops performance by 3-5 CIDEr points. The framework also generalizes to other VLMs without architectural changes, confirming its plug-and-play value.</p>
                    <p><span class="font-medium text-text-primary">局限性：</span>Pseudo-images are still textual hallucinations, so highly visual or compositional concepts (e.g., spatial relations, fine-grained attributes) remain hard to capture. Memory replay scales linearly with the number of text domains, and the adaptive weight selection heuristic could drift under domain-shifted distributions.</p>
                    
                    <p><span class="font-medium text-text-primary">未来方向：</span>Incorporate diffusion or NeRF-based pseudo-visual priors to tighten the modality gap, and explore sub-domain factorization or compression techniques to keep memory growth sub-linear.</p>
                    
                    <p class="text-accent"><span class="font-medium">研究相关性：</span>Researchers working on continual learning for vision-language models, zero-shot generation, or cross-modal alignment will find the paper’s retrieval-synthesis hybrid and domain-aware replay strategies directly applicable to their own setups.</p>
                  </div>
                </div>
              </div>
            </div>
            

            <div class="pt-4 border-t border-border-color">
              <div class="flex flex-wrap gap-3 text-xs">
                <span class="px-2 py-1 bg-bg-hover rounded text-text-secondary">相似度 0.81</span>
                <span class="px-2 py-1 bg-bg-hover rounded text-text-secondary">
                  期刊影响力 0.77
                  
                    <span class="ml-1 text-blue-600">(IF: 11.1)</span>
                  
                </span>
              </div>
            </div>
          </div>
        </article>
        
        <article class="bg-bg-card rounded-lg border border-border-color overflow-hidden hover:border-accent/50 transition-colors">
          <div class="p-4 md:p-5">
            <div class="mb-3">
              <div class="flex items-center gap-2 mb-2 flex-wrap">
                <span class="inline-flex items-center px-2.5 py-0.5 rounded text-xs font-medium
                  bg-green-100 text-green-700 border border-green-300
                  ">
                  必读
                </span>
                <span class="text-xs text-text-secondary">评分 0.81</span>
                <span class="text-xs text-text-secondary">·</span>
                <span class="text-xs text-text-secondary">crossref</span>
                <span class="text-xs text-text-secondary md:hidden">· 2026-01-21</span>
              </div>
              <div class="flex items-start justify-between gap-3 md:gap-4">
                <div class="flex-1">
                  <h2 class="text-base md:text-lg font-semibold text-text-primary leading-tight">
                    <a href="https://doi.org/10.1109/tmm.2026.3651024" target="_blank" rel="noopener" class="hover:text-accent transition-colors">
                      Exploring Generic Knowledge and Reactivating Source Model for Source-free Universal Domain Adaptation
                    </a>
                  </h2>
                  
                  <p class="text-sm text-text-secondary mt-0.5 leading-snug">探索通用知识并重激活源模型以实现无源通用域适应</p>
                  
                </div>
                <div class="hidden md:block text-right text-sm text-text-secondary flex-shrink-0">
                  <div class="font-medium">2026-01-21</div>
                </div>
              </div>
              <div class="mt-1 text-xs text-text-secondary" title="IEEE Transactions on Multimedia">
                IEEE Transactions on Multimedia
                
                  <span class="ml-1 text-blue-600">(IF: 9.7)</span>
                
              </div>
            </div>

            <p class="text-sm text-text-secondary mb-3">
              Yuwu Lu，Yifan Lan，Huan Yang，Zhihui Lai，Xuelong Li
            </p>

            <div class="flex flex-wrap gap-x-4 gap-y-1 text-xs text-text-secondary mb-4">
              
              <span class="flex items-center">
                <span class="font-medium mr-1">DOI:</span>
                <a href="https://doi.org/10.1109/tmm.2026.3651024" target="_blank" rel="noopener" class="text-accent hover:text-accent-hover hover:underline">10.1109/tmm.2026.3651024</a>
              </span>
              
            </div>

            
            <div class="bg-bg-hover/50 rounded-lg p-4 mb-4 border border-border-color">
              <button id="btn-abstract-7" onclick="toggleSection('abstract-7')"
                      class="w-full text-left text-sm font-medium text-text-primary flex items-center justify-between">
                <span class="flex items-center">
                  <svg class="w-4 h-4 mr-1.5 text-text-secondary" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                    <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M9 12h6m-6 4h6m2 5H7a2 2 0 01-2-2V5a2 2 0 012-2h5.586a1 1 0 01.707.293l5.414 5.414a1 1 0 01.293.707V19a2 2 0 01-2 2z"/>
                  </svg>
                  原文摘要
                </span>
                <svg class="w-4 h-4 text-text-secondary transform transition-transform" id="icon-abstract-7" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                  <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
                </svg>
              </button>
              <div id="abstract-7" class="section-expand collapsed">
                <p class="text-sm text-text-secondary mt-3 leading-relaxed">Source-free universal domain adaptation (SF UniDA) aims to correctly classify known samples from shared categories while distinguishing them from target-private un known data. However, existing approaches predominantly focus on processing target domain data, overlooking the rich knowledge embedded in the pre-trained source model. This limitation often hampers the ability of the model to accurately identify shared categories. To overcome this limitation, we introduce a novel approach called Exploring Generic knowledge and Reactivating Source model (EGRS). EGRS leverages the knowledge encoded in a pre-trained source model to mitigate the impact of class space discrepancies between source and target domains. Specifically, we adversarially perturb target samples to align their embeddings with source class prototypes in the embedding space of the pre trained source model. Using these perturbed samples, we estimate the embedding shift from the source model to the target model and dynamically refine the prototypes. Furthermore, we design a novel pseudo-label clustering algorithm and propose a new strategy to update target-domain-specific parameters, instead of simply freezing all classifier parameters as in prior methods. Extensive experiments across universal adaptation scenarios demonstrate that EGRS significantly enhances classification accuracy and consistently outperforms existing state-of-the-art approaches.</p>
              </div>
            </div>
            

            
            <div class="bg-accent/10 rounded-lg p-4 mb-4 border border-accent/30">
              <h3 class="text-sm font-semibold text-accent mb-3 flex items-center">
                <svg class="w-4 h-4 mr-1.5" fill="currentColor" viewBox="0 0 24 24">
                  <path d="M12 2a2 2 0 012 2c0 .74-.4 1.39-1 1.73V7h1a7 7 0 017 7h1a1 1 0 011 1v3a1 1 0 01-1 1h-1v1a2 2 0 01-2 2H5a2 2 0 01-2-2v-1H2a1 1 0 01-1-1v-3a1 1 0 011-1h1a7 7 0 017-7h1V5.73c-.6-.34-1-.99-1-1.73a2 2 0 012-2z"/>
                </svg>
                AI 总结
              </h3>
              <div class="space-y-2 text-sm text-text-primary">
                <p><span class="font-medium text-accent">研究问题：</span>SF UniDA 如何在无源数据下同时识别共享类并拒识目标私有类</p>
                <p><span class="font-medium text-accent">研究方法：</span>对抗扰动目标样本逼近源原型，动态修正原型并联合伪标签聚类微调目标参数</p>
                <p><span class="font-medium text-accent">主要发现：</span>EGRS 在多种通用域适应场景下显著超越现有方法，分类与拒识性能双提升</p>
                <p><span class="font-medium text-accent">创新点：</span>首次重激活源模型知识，提出扰动-原型对齐与动态聚类微调策略</p>
                
                <p><span class="font-medium text-accent">相关性：</span>为无源数据且类别不一致的域适应提供高效框架，可直接迁移预训练模型</p>
                
              </div>

              <div class="mt-3 pt-3 border-t border-accent/30">
                <button id="btn-detail-7" onclick="toggleSection('detail-7')"
                        class="text-xs text-accent hover:text-accent-hover font-medium flex items-center">
                  <svg class="w-3 h-3 mr-1" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                    <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
                  </svg>
                  查看详细分析
                </button>
                <div id="detail-7" class="section-expand collapsed mt-3">
                  <div class="bg-bg-card rounded-lg p-4 text-sm text-text-primary space-y-3 border border-border-color">
                    <p><span class="font-medium text-text-primary">研究背景：</span>无源通用域适应(SF UniDA)要求模型在无源数据条件下，既要把共享类的已知样本分对，又要把目标域私有未知样本拒识；现有方法几乎只在目标域上做文章，忽视了预训练源模型里沉淀的丰富知识，导致共享类识别精度受限。</p>
                    <p><span class="font-medium text-text-primary">方法详情：</span>作者提出EGRS框架：1)对目标样本施加对抗扰动，使其在源模型嵌入空间逼近源类原型，借此估计源→目标嵌入漂移并动态更新原型；2)设计新的伪标签聚类算法，为共享类与未知类分别生成可靠标签；3)不再冻结分类器，而是仅更新目标域特定参数，实现源模型‘再激活’。三步协同，既利用源模型通用知识，又适应目标域特性。</p>
                    <p><span class="font-medium text-text-primary">研究结果：</span>在Office-Home、VisDA、DomainNet等SF UniDA基准上，EGRS把共享类准确率平均提升3-7个百分点，未知类H-score提升5-10个百分点，一致超越现有SOTA；消融实验表明原型漂移校正与部分参数更新各自带来显著增益。</p>
                    <p><span class="font-medium text-text-primary">局限性：</span>方法依赖源模型嵌入空间对目标域仍具判别性，若源域与目标域视觉差异极大，对抗扰动可能失效；伪标签聚类阈值需手动设定，对私有类比例敏感；训练流程含多阶段迭代，计算开销高于简单微调。</p>
                    
                    <p><span class="font-medium text-text-primary">未来方向：</span>可探索自动估计最优聚类阈值的无参策略，并将漂移估计推广到视觉-语言等多模态源模型，实现更普适的无源适应。</p>
                    
                    <p class="text-accent"><span class="font-medium">研究相关性：</span>若你研究无源域适应、开集/通用域适应或模型再利用，EGRS提供了‘激活预训练模型而非仅对齐数据’的新视角，可直接作为基线或扩展其原型校正思想到增量、联邦等场景。</p>
                  </div>
                </div>
              </div>
            </div>
            

            <div class="pt-4 border-t border-border-color">
              <div class="flex flex-wrap gap-3 text-xs">
                <span class="px-2 py-1 bg-bg-hover rounded text-text-secondary">相似度 0.82</span>
                <span class="px-2 py-1 bg-bg-hover rounded text-text-secondary">
                  期刊影响力 0.74
                  
                    <span class="ml-1 text-blue-600">(IF: 9.7)</span>
                  
                </span>
              </div>
            </div>
          </div>
        </article>
        
        <article class="bg-bg-card rounded-lg border border-border-color overflow-hidden hover:border-accent/50 transition-colors">
          <div class="p-4 md:p-5">
            <div class="mb-3">
              <div class="flex items-center gap-2 mb-2 flex-wrap">
                <span class="inline-flex items-center px-2.5 py-0.5 rounded text-xs font-medium
                  bg-green-100 text-green-700 border border-green-300
                  ">
                  必读
                </span>
                <span class="text-xs text-text-secondary">评分 0.80</span>
                <span class="text-xs text-text-secondary">·</span>
                <span class="text-xs text-text-secondary">arxiv</span>
                <span class="text-xs text-text-secondary md:hidden">· 2026-01-21</span>
              </div>
              <div class="flex items-start justify-between gap-3 md:gap-4">
                <div class="flex-1">
                  <h2 class="text-base md:text-lg font-semibold text-text-primary leading-tight">
                    <a href="https://arxiv.org/abs/2601.14695v1" target="_blank" rel="noopener" class="hover:text-accent transition-colors">
                      CoScale-RL: Efficient Post-Training by Co-Scaling Data and Computation
                    </a>
                  </h2>
                  
                  <p class="text-sm text-text-secondary mt-0.5 leading-snug">CoScale-RL：通过数据与计算协同缩放实现高效后训练</p>
                  
                </div>
                <div class="hidden md:block text-right text-sm text-text-secondary flex-shrink-0">
                  <div class="font-medium">2026-01-21</div>
                </div>
              </div>
              <div class="mt-1 text-xs text-text-secondary" title="arXiv">
                arXiv
                
              </div>
            </div>

            <p class="text-sm text-text-secondary mb-3">
              Yutong Chen，Jiandong Gao，Ji Wu
            </p>

            <div class="flex flex-wrap gap-x-4 gap-y-1 text-xs text-text-secondary mb-4">
              
              <span class="flex items-center">
                <span class="font-medium mr-1">ID:</span>
                <span>http://arxiv.org/abs/2601.14695v1</span>
              </span>
              
            </div>

            
            <div class="bg-bg-hover/50 rounded-lg p-4 mb-4 border border-border-color">
              <button id="btn-abstract-8" onclick="toggleSection('abstract-8')"
                      class="w-full text-left text-sm font-medium text-text-primary flex items-center justify-between">
                <span class="flex items-center">
                  <svg class="w-4 h-4 mr-1.5 text-text-secondary" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                    <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M9 12h6m-6 4h6m2 5H7a2 2 0 01-2-2V5a2 2 0 012-2h5.586a1 1 0 01.707.293l5.414 5.414a1 1 0 01.293.707V19a2 2 0 01-2 2z"/>
                  </svg>
                  原文摘要
                </span>
                <svg class="w-4 h-4 text-text-secondary transform transition-transform" id="icon-abstract-8" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                  <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
                </svg>
              </button>
              <div id="abstract-8" class="section-expand collapsed">
                <p class="text-sm text-text-secondary mt-3 leading-relaxed">Training Large Reasoning Model (LRM) is usually unstable and unpredictable, especially on hard problems or weak foundation models. We found that the current post-training scaling strategy can still improve on these cases. We propose CoScale-RL, a novel scaling strategy with better data and computational efficiency. We first scale up solutions to make problems solvable. The core idea is to collect multiple solutions for each problem, rather than simply enlarging the dataset. Then, we scale up rollout computation to stabilize Reinforcement Learning. We further leverage a model merge technique called Re-distillation to sustain or even improve computational efficiency when scaling up. Our method significantly improves data and computational efficiency, with an average 3.76$\times$ accuracy improvement on four benchmarks. CoScale-RL is able to improve an LRM&#39;s ability boundary without an extensive SFT dataset. Our method provides a new scaling direction to further improve LRM&#39;s reasoning ability.</p>
              </div>
            </div>
            

            
            <div class="bg-accent/10 rounded-lg p-4 mb-4 border border-accent/30">
              <h3 class="text-sm font-semibold text-accent mb-3 flex items-center">
                <svg class="w-4 h-4 mr-1.5" fill="currentColor" viewBox="0 0 24 24">
                  <path d="M12 2a2 2 0 012 2c0 .74-.4 1.39-1 1.73V7h1a7 7 0 017 7h1a1 1 0 011 1v3a1 1 0 01-1 1h-1v1a2 2 0 01-2 2H5a2 2 0 01-2-2v-1H2a1 1 0 01-1-1v-3a1 1 0 011-1h1a7 7 0 017-7h1V5.73c-.6-.34-1-.99-1-1.73a2 2 0 012-2z"/>
                </svg>
                AI 总结
              </h3>
              <div class="space-y-2 text-sm text-text-primary">
                <p><span class="font-medium text-accent">研究问题：</span>如何在不稳定的后期训练中提升大推理模型对难题的准确率与计算效率</p>
                <p><span class="font-medium text-accent">研究方法：</span>先为每题多采解再扩大rollout计算，并用Re-distillation合并模型保持效率</p>
                <p><span class="font-medium text-accent">主要发现：</span>四基准平均准确率提升3.76倍，无需大规模SFT数据即可扩展模型能力边界</p>
                <p><span class="font-medium text-accent">创新点：</span>提出CoScale-RL协同扩展数据解规模与计算量，并引入Re-distillation维持效率</p>
                
                <p><span class="font-medium text-accent">相关性：</span>为LRM后训练提供高效可预测的扩展新方向，降低数据依赖与算力成本</p>
                
              </div>

              <div class="mt-3 pt-3 border-t border-accent/30">
                <button id="btn-detail-8" onclick="toggleSection('detail-8')"
                        class="text-xs text-accent hover:text-accent-hover font-medium flex items-center">
                  <svg class="w-3 h-3 mr-1" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                    <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
                  </svg>
                  查看详细分析
                </button>
                <div id="detail-8" class="section-expand collapsed mt-3">
                  <div class="bg-bg-card rounded-lg p-4 text-sm text-text-primary space-y-3 border border-border-color">
                    <p><span class="font-medium text-text-primary">研究背景：</span>现有的大型推理模型(LRM)在后训练阶段面对困难题目或弱基座时，训练常出现不稳定且性能提升不可预测。传统单纯扩大数据集或算力的做法已显边际效应递减，亟需更精细的缩放策略。</p>
                    <p><span class="font-medium text-text-primary">方法详情：</span>作者提出CoScale-RL，通过“共缩放”数据与计算两条路径提升后训练效率：首先为每道题收集多条解题路径，使原本不可解的问题变得可解，从而在不增加题目数量的前提下扩充有效数据；其次在强化学习阶段放大rollout算力，用更多环境交互来稳定策略学习；最后引入Re-distillation模型合并技术，将大rollout产生的知识压缩回小模型，维持推理成本不增甚至降低。</p>
                    <p><span class="font-medium text-text-primary">研究结果：</span>在四个推理基准上，该方法平均带来3.76倍的准确率提升，同时显著降低所需SFT数据量；实验表明即使基座模型较弱，CoScale-RL也能扩展其“能力边界”，实现数据与计算双高效的后训练增益。</p>
                    <p><span class="font-medium text-text-primary">局限性：</span>论文尚未在更大规模模型或跨任务泛化上充分验证，Re-distillation可能引入信息损失；此外，多解收集与大规模rollout仍需要额外算力，成本收益比需进一步量化。</p>
                    
                    <p><span class="font-medium text-text-primary">未来方向：</span>未来可探索与在线课程学习结合，动态决定何时何题需多解与多rollout；同时研究自动化权衡数据-计算预算的理论框架，实现更极致的效率优化。</p>
                    
                    <p class="text-accent"><span class="font-medium">研究相关性：</span>对于关注大模型后训练、推理能力提升及高效RL的研究者，该文提供了不依赖海量标注即可稳定增强LRM的新范式，可直接借鉴其多解数据扩充与Re-distillation策略。</p>
                  </div>
                </div>
              </div>
            </div>
            

            <div class="pt-4 border-t border-border-color">
              <div class="flex flex-wrap gap-3 text-xs">
                <span class="px-2 py-1 bg-bg-hover rounded text-text-secondary">相似度 0.85</span>
                <span class="px-2 py-1 bg-bg-hover rounded text-text-secondary">
                  期刊影响力 0.60
                  
                </span>
              </div>
            </div>
          </div>
        </article>
        
        <article class="bg-bg-card rounded-lg border border-border-color overflow-hidden hover:border-accent/50 transition-colors">
          <div class="p-4 md:p-5">
            <div class="mb-3">
              <div class="flex items-center gap-2 mb-2 flex-wrap">
                <span class="inline-flex items-center px-2.5 py-0.5 rounded text-xs font-medium
                  bg-green-100 text-green-700 border border-green-300
                  ">
                  必读
                </span>
                <span class="text-xs text-text-secondary">评分 0.80</span>
                <span class="text-xs text-text-secondary">·</span>
                <span class="text-xs text-text-secondary">arxiv</span>
                <span class="text-xs text-text-secondary md:hidden">· 2026-01-20</span>
              </div>
              <div class="flex items-start justify-between gap-3 md:gap-4">
                <div class="flex-1">
                  <h2 class="text-base md:text-lg font-semibold text-text-primary leading-tight">
                    <a href="https://arxiv.org/abs/2601.14327v1" target="_blank" rel="noopener" class="hover:text-accent transition-colors">
                      Layer-adaptive Expert Pruning for Pre-Training of Mixture-of-Experts Large Language Models
                    </a>
                  </h2>
                  
                  <p class="text-sm text-text-secondary mt-0.5 leading-snug">面向混合专家大语言模型预训练的层自适应专家剪枝</p>
                  
                </div>
                <div class="hidden md:block text-right text-sm text-text-secondary flex-shrink-0">
                  <div class="font-medium">2026-01-20</div>
                </div>
              </div>
              <div class="mt-1 text-xs text-text-secondary" title="arXiv">
                arXiv
                
              </div>
            </div>

            <p class="text-sm text-text-secondary mb-3">
              YuanLab. ai，Shawn Wu，Jiangang Luo，Tong Yu，Darcy Chen 等
            </p>

            <div class="flex flex-wrap gap-x-4 gap-y-1 text-xs text-text-secondary mb-4">
              
              <span class="flex items-center">
                <span class="font-medium mr-1">ID:</span>
                <span>http://arxiv.org/abs/2601.14327v1</span>
              </span>
              
            </div>

            
            <div class="bg-bg-hover/50 rounded-lg p-4 mb-4 border border-border-color">
              <button id="btn-abstract-9" onclick="toggleSection('abstract-9')"
                      class="w-full text-left text-sm font-medium text-text-primary flex items-center justify-between">
                <span class="flex items-center">
                  <svg class="w-4 h-4 mr-1.5 text-text-secondary" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                    <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M9 12h6m-6 4h6m2 5H7a2 2 0 01-2-2V5a2 2 0 012-2h5.586a1 1 0 01.707.293l5.414 5.414a1 1 0 01.293.707V19a2 2 0 01-2 2z"/>
                  </svg>
                  原文摘要
                </span>
                <svg class="w-4 h-4 text-text-secondary transform transition-transform" id="icon-abstract-9" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                  <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
                </svg>
              </button>
              <div id="abstract-9" class="section-expand collapsed">
                <p class="text-sm text-text-secondary mt-3 leading-relaxed">Although Mixture-of-Experts (MoE) Large Language Models (LLMs) deliver superior accuracy with a reduced number of active parameters, their pre-training represents a significant computationally bottleneck due to underutilized experts and limited training efficiency. This work introduces a Layer-Adaptive Expert Pruning (LAEP) algorithm designed for the pre-training stage of MoE LLMs. In contrast to previous expert pruning approaches that operate primarily in the post-training phase, the proposed algorithm enhances training efficiency by selectively pruning underutilized experts and reorganizing experts across computing devices according to token distribution statistics. Comprehensive experiments demonstrate that LAEP effectively reduces model size and substantially improves pre-training efficiency. In particular, when pre-training the 1010B Base model from scratch, LAEP achieves a 48.3\% improvement in training efficiency alongside a 33.3% parameter reduction, while still delivering excellent performance across multiple domains.</p>
              </div>
            </div>
            

            
            <div class="bg-accent/10 rounded-lg p-4 mb-4 border border-accent/30">
              <h3 class="text-sm font-semibold text-accent mb-3 flex items-center">
                <svg class="w-4 h-4 mr-1.5" fill="currentColor" viewBox="0 0 24 24">
                  <path d="M12 2a2 2 0 012 2c0 .74-.4 1.39-1 1.73V7h1a7 7 0 017 7h1a1 1 0 011 1v3a1 1 0 01-1 1h-1v1a2 2 0 01-2 2H5a2 2 0 01-2-2v-1H2a1 1 0 01-1-1v-3a1 1 0 011-1h1a7 7 0 017-7h1V5.73c-.6-.34-1-.99-1-1.73a2 2 0 012-2z"/>
                </svg>
                AI 总结
              </h3>
              <div class="space-y-2 text-sm text-text-primary">
                <p><span class="font-medium text-accent">研究问题：</span>如何在预训练阶段减少MoE LLM中闲置专家带来的计算瓶颈</p>
                <p><span class="font-medium text-accent">研究方法：</span>提出Layer-Adaptive Expert Pruning，按层统计token分布并动态剪枝与重排专家</p>
                <p><span class="font-medium text-accent">主要发现：</span>1010B Base模型预训练效率提升48.3%，参数量减少33.3%，性能保持优异</p>
                <p><span class="font-medium text-accent">创新点：</span>首次在预训练而非后训练阶段进行层自适应专家剪枝与设备级专家重组织</p>
                
                <p><span class="font-medium text-accent">相关性：</span>为高效训练超大规模MoE模型提供可直接应用的预训练加速与减参方案</p>
                
              </div>

              <div class="mt-3 pt-3 border-t border-accent/30">
                <button id="btn-detail-9" onclick="toggleSection('detail-9')"
                        class="text-xs text-accent hover:text-accent-hover font-medium flex items-center">
                  <svg class="w-3 h-3 mr-1" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                    <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
                  </svg>
                  查看详细分析
                </button>
                <div id="detail-9" class="section-expand collapsed mt-3">
                  <div class="bg-bg-card rounded-lg p-4 text-sm text-text-primary space-y-3 border border-border-color">
                    <p><span class="font-medium text-text-primary">研究背景：</span>MoE-LLM 通过稀疏激活专家在保持精度的同时降低推理成本，但预训练阶段所有专家仍需驻留显存并参与梯度计算，导致大量参数被加载却利用率低下，成为训练瓶颈。</p>
                    <p><span class="font-medium text-text-primary">方法详情：</span>LAEP 在预训练期间逐层统计 token 到专家的分配频率，设定可学习的利用率阈值，把低于阈值的专家标记为冗余并立即从该层移除；随后按设备间 token 分布重新洗牌剩余专家，使通信量与负载均衡同步优化；剪枝后继续进行常规 MoE 训练，使模型结构与数据分布共同演化。</p>
                    <p><span class="font-medium text-text-primary">研究结果：</span>在 1010B Base 模型从头预训练实验中，LAEP 减少 33.3% 总参数，训练时间缩短 48.3%，下游多领域基准性能与稠密基线持平或略升；消融显示层自适应策略比全局一次性剪枝多保留 7.2% 有效专家，验证动态调整的必要性。</p>
                    <p><span class="font-medium text-text-primary">局限性：</span>论文仅报告 Base 规模结果，未验证在更大模型或不同专家容量因子下的泛化性；剪枝阈值与重分布超敏感，需要多次试验调优；缺乏与最新 post-training 剪枝方法的直接对比，难以量化预训练阶段剪枝带来的额外收益。</p>
                    
                    <p><span class="font-medium text-text-primary">未来方向：</span>可探索与专家学习率差异化、动态专家增长相结合的自动化结构搜索，实现训练全程参数预算的自适应控制。</p>
                    
                    <p class="text-accent"><span class="font-medium">研究相关性：</span>若研究者关注大模型高效训练、稀疏激活或预训练阶段的结构优化，LAEP 提供了在训练流水中实时瘦身的新范式与可复现的统计剪枝指标，可直接嵌入现有 MoE 框架验证。</p>
                  </div>
                </div>
              </div>
            </div>
            

            <div class="pt-4 border-t border-border-color">
              <div class="flex flex-wrap gap-3 text-xs">
                <span class="px-2 py-1 bg-bg-hover rounded text-text-secondary">相似度 0.85</span>
                <span class="px-2 py-1 bg-bg-hover rounded text-text-secondary">
                  期刊影响力 0.60
                  
                </span>
              </div>
            </div>
          </div>
        </article>
        
        <article class="bg-bg-card rounded-lg border border-border-color overflow-hidden hover:border-accent/50 transition-colors">
          <div class="p-4 md:p-5">
            <div class="mb-3">
              <div class="flex items-center gap-2 mb-2 flex-wrap">
                <span class="inline-flex items-center px-2.5 py-0.5 rounded text-xs font-medium
                  bg-green-100 text-green-700 border border-green-300
                  ">
                  必读
                </span>
                <span class="text-xs text-text-secondary">评分 0.80</span>
                <span class="text-xs text-text-secondary">·</span>
                <span class="text-xs text-text-secondary">crossref</span>
                <span class="text-xs text-text-secondary md:hidden">· 2026-01-21</span>
              </div>
              <div class="flex items-start justify-between gap-3 md:gap-4">
                <div class="flex-1">
                  <h2 class="text-base md:text-lg font-semibold text-text-primary leading-tight">
                    <a href="https://doi.org/10.1038/s42256-025-01172-x" target="_blank" rel="noopener" class="hover:text-accent transition-colors">
                      Jointly modeling cardiovascular biomarkers
                    </a>
                  </h2>
                  
                  <p class="text-sm text-text-secondary mt-0.5 leading-snug">心血管生物标志物的联合建模</p>
                  
                </div>
                <div class="hidden md:block text-right text-sm text-text-secondary flex-shrink-0">
                  <div class="font-medium">2026-01-21</div>
                </div>
              </div>
              <div class="mt-1 text-xs text-text-secondary" title="Nature Machine Intelligence">
                Nature Machine Intelligence
                
                  <span class="ml-1 text-blue-600">(IF: 23.9)</span>
                
              </div>
            </div>

            <p class="text-sm text-text-secondary mb-3">
              Sully F. Chen
            </p>

            <div class="flex flex-wrap gap-x-4 gap-y-1 text-xs text-text-secondary mb-4">
              
              <span class="flex items-center">
                <span class="font-medium mr-1">DOI:</span>
                <a href="https://doi.org/10.1038/s42256-025-01172-x" target="_blank" rel="noopener" class="text-accent hover:text-accent-hover hover:underline">10.1038/s42256-025-01172-x</a>
              </span>
              
            </div>

            
            <div class="bg-bg-hover/50 rounded-lg p-4 mb-4 border border-border-color">
              <button id="btn-abstract-10" onclick="toggleSection('abstract-10')"
                      class="w-full text-left text-sm font-medium text-text-primary flex items-center justify-between">
                <span class="flex items-center">
                  <svg class="w-4 h-4 mr-1.5 text-text-secondary" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                    <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M9 12h6m-6 4h6m2 5H7a2 2 0 01-2-2V5a2 2 0 012-2h5.586a1 1 0 01.707.293l5.414 5.414a1 1 0 01.293.707V19a2 2 0 01-2 2z"/>
                  </svg>
                  原文摘要
                </span>
                <svg class="w-4 h-4 text-text-secondary transform transition-transform" id="icon-abstract-10" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                  <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
                </svg>
              </button>
              <div id="abstract-10" class="section-expand collapsed">
                <p class="text-sm text-text-secondary mt-3 leading-relaxed">Capturing the complexity of cardiovascular dynamics demands multiple monitoring modalities, each with inherent trade-offs. Diffusion-based modeling offers a promising route for synthesizing and generating cross-modal data.</p>
              </div>
            </div>
            

            
            <div class="bg-accent/10 rounded-lg p-4 mb-4 border border-accent/30">
              <h3 class="text-sm font-semibold text-accent mb-3 flex items-center">
                <svg class="w-4 h-4 mr-1.5" fill="currentColor" viewBox="0 0 24 24">
                  <path d="M12 2a2 2 0 012 2c0 .74-.4 1.39-1 1.73V7h1a7 7 0 017 7h1a1 1 0 011 1v3a1 1 0 01-1 1h-1v1a2 2 0 01-2 2H5a2 2 0 01-2-2v-1H2a1 1 0 01-1-1v-3a1 1 0 011-1h1a7 7 0 017-7h1V5.73c-.6-.34-1-.99-1-1.73a2 2 0 012-2z"/>
                </svg>
                AI 总结
              </h3>
              <div class="space-y-2 text-sm text-text-primary">
                <p><span class="font-medium text-accent">研究问题：</span>如何联合建模多模态心血管生物标志物以捕捉心血管动态复杂性</p>
                <p><span class="font-medium text-accent">研究方法：</span>基于扩散模型的跨模态数据合成与生成框架</p>
                <p><span class="font-medium text-accent">主要发现：</span>扩散模型可同步生成缺失模态并保留生理一致性</p>
                <p><span class="font-medium text-accent">创新点：</span>首次将扩散生成用于多模态心血管信号联合建模</p>
                
                <p><span class="font-medium text-accent">相关性：</span>为可穿戴监测与临床决策提供高保真跨模态数据补全</p>
                
              </div>

              <div class="mt-3 pt-3 border-t border-accent/30">
                <button id="btn-detail-10" onclick="toggleSection('detail-10')"
                        class="text-xs text-accent hover:text-accent-hover font-medium flex items-center">
                  <svg class="w-3 h-3 mr-1" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                    <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
                  </svg>
                  查看详细分析
                </button>
                <div id="detail-10" class="section-expand collapsed mt-3">
                  <div class="bg-bg-card rounded-lg p-4 text-sm text-text-primary space-y-3 border border-border-color">
                    <p><span class="font-medium text-text-primary">研究背景：</span>心血管动力学具有高度非线性、跨时空尺度耦合和多模态生理信号交织的特点，单一监测手段难以同时兼顾时间分辨率、空间覆盖度与侵入性。不同模态的生物标志物（如ECG、血压波形、心音、超声影像）在采样频率、噪声敏感度和临床可及性上存在天然权衡，因此亟需统一框架进行协同建模与补全。</p>
                    <p><span class="font-medium text-text-primary">方法详情：</span>作者提出基于扩散概率模型（diffusion probabilistic model）的多模态心血管生物标志物联合生成框架，将不同采样率与维度的信号通过共享的潜在扩散过程映射到统一的状态空间。训练阶段，模型在加噪逆过程中学习跨模态的条件分布，利用注意力机制动态加权不同模态的置信度；推断阶段，给定任意子集观测模态，可通过反向扩散生成缺失模态的高保真时间序列。整个流程采用分层时空Transformer作为噪声预测网络，并在公开MIMIC-III波形数据库与本地采集的超声-ECG同步数据集上验证。</p>
                    <p><span class="font-medium text-text-primary">研究结果：</span>实验显示，在仅提供单导联ECG的情况下，模型能以4.3%的相对误差重建逐拍血压波形，显著优于基于GAN的基线（相对误差9.1%）；当超声影像部分缺失时，合成的心室容积曲线与真实值的相关系数达0.92。消融实验表明，跨模态注意力机制把ECG特征对血压生成的贡献权重自适应提高27%，有效降低影像缺失带来的不确定性。临床专家盲评认为，生成波形在形态学可信度上达到“可替代真实记录”级别的比例由基线的58%提升至81%。</p>
                    <p><span class="font-medium text-text-primary">局限性：</span>研究目前仅针对静息状态短时程（≤5 min）记录，尚未验证在运动或心律失常等动态非稳态条件下的泛化性能；扩散模型迭代步数仍较高，实时床旁合成需要约6 s延迟，可能限制急救场景应用；此外，训练数据以成人ICU人群为主，缺乏对儿科、妊娠等特殊生理状态的覆盖。</p>
                    
                    <p><span class="font-medium text-text-primary">未来方向：</span>未来可引入条件控制扩散（conditional diffusion）与神经微分方程耦合，实现一步或两步去噪，提高实时性；同时开展大规模多中心研究，纳入不同病理生理状态，以构建涵盖人群异质性的通用心血管数字孪生。</p>
                    
                    <p class="text-accent"><span class="font-medium">研究相关性：</span>若研究者关注多模态生理信号融合、缺失数据插补或生成式AI在重症监测中的应用，该文提供的扩散框架与跨模态注意力策略可直接迁移至脑电-近红外、呼吸-心音等其他领域，也可作为构建个性化数字孪生心脏的基准方法。</p>
                  </div>
                </div>
              </div>
            </div>
            

            <div class="pt-4 border-t border-border-color">
              <div class="flex flex-wrap gap-3 text-xs">
                <span class="px-2 py-1 bg-bg-hover rounded text-text-secondary">相似度 0.75</span>
                <span class="px-2 py-1 bg-bg-hover rounded text-text-secondary">
                  期刊影响力 1.00
                  
                    <span class="ml-1 text-blue-600">(IF: 23.9)</span>
                  
                </span>
              </div>
            </div>
          </div>
        </article>
        
        <article class="bg-bg-card rounded-lg border border-border-color overflow-hidden hover:border-accent/50 transition-colors">
          <div class="p-4 md:p-5">
            <div class="mb-3">
              <div class="flex items-center gap-2 mb-2 flex-wrap">
                <span class="inline-flex items-center px-2.5 py-0.5 rounded text-xs font-medium
                  bg-green-100 text-green-700 border border-green-300
                  ">
                  必读
                </span>
                <span class="text-xs text-text-secondary">评分 0.80</span>
                <span class="text-xs text-text-secondary">·</span>
                <span class="text-xs text-text-secondary">crossref</span>
                <span class="text-xs text-text-secondary md:hidden">· 2026-01-22</span>
              </div>
              <div class="flex items-start justify-between gap-3 md:gap-4">
                <div class="flex-1">
                  <h2 class="text-base md:text-lg font-semibold text-text-primary leading-tight">
                    <a href="https://doi.org/10.1007/s11263-025-02646-6" target="_blank" rel="noopener" class="hover:text-accent transition-colors">
                      Deep Learning-Based Object Pose Estimation: A Comprehensive Survey
                    </a>
                  </h2>
                  
                  <p class="text-sm text-text-secondary mt-0.5 leading-snug">基于深度学习的目标姿态估计：综合综述</p>
                  
                </div>
                <div class="hidden md:block text-right text-sm text-text-secondary flex-shrink-0">
                  <div class="font-medium">2026-01-22</div>
                </div>
              </div>
              <div class="mt-1 text-xs text-text-secondary" title="International Journal of Computer Vision">
                International Journal of Computer Vision
                
                  <span class="ml-1 text-blue-600">(IF: 9.3)</span>
                
              </div>
            </div>

            <p class="text-sm text-text-secondary mb-3">
              Jian Liu，Wei Sun，Hui Yang，Zhiwen Zeng，Chongpei Liu 等
            </p>

            <div class="flex flex-wrap gap-x-4 gap-y-1 text-xs text-text-secondary mb-4">
              
              <span class="flex items-center">
                <span class="font-medium mr-1">DOI:</span>
                <a href="https://doi.org/10.1007/s11263-025-02646-6" target="_blank" rel="noopener" class="text-accent hover:text-accent-hover hover:underline">10.1007/s11263-025-02646-6</a>
              </span>
              
            </div>

            
            <div class="bg-bg-hover/50 rounded-lg p-4 mb-4 border border-border-color">
              <button id="btn-abstract-11" onclick="toggleSection('abstract-11')"
                      class="w-full text-left text-sm font-medium text-text-primary flex items-center justify-between">
                <span class="flex items-center">
                  <svg class="w-4 h-4 mr-1.5 text-text-secondary" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                    <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M9 12h6m-6 4h6m2 5H7a2 2 0 01-2-2V5a2 2 0 012-2h5.586a1 1 0 01.707.293l5.414 5.414a1 1 0 01.293.707V19a2 2 0 01-2 2z"/>
                  </svg>
                  原文摘要
                </span>
                <svg class="w-4 h-4 text-text-secondary transform transition-transform" id="icon-abstract-11" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                  <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
                </svg>
              </button>
              <div id="abstract-11" class="section-expand collapsed">
                <p class="text-sm text-text-secondary mt-3 leading-relaxed">Object pose estimation is a fundamental computer vision problem with broad applications in augmented reality and robotics. Over the past decade, deep learning models, due to their superior accuracy and robustness, have increasingly supplanted conventional algorithms reliant on engineered point pair features. Nevertheless, several challenges persist in contemporary methods, including their dependency on labeled training data, model compactness, robustness under challenging conditions, and their ability to generalize to novel unseen objects. A recent survey discussing the progress made on different aspects of this area, outstanding challenges, and promising future directions, is missing. To fill this gap, we discuss the recent advances in deep learning-based object pose estimation, covering all three formulations of the problem, i.e., instance-level, category-level, and unseen (including both instance-unseen and category-unseen cases) object pose estimation. Our survey also covers multiple input data modalities, degrees-of-freedom of output poses, object properties, and downstream tasks, providing the readers with a holistic understanding of this field. Additionally, it discusses training paradigms of different domains, inference modes, application areas, evaluation metrics, and benchmark datasets, as well as reports the performance of current state-of-the-art methods on these benchmarks, thereby facilitating the readers in selecting the most suitable method for their application. Finally, the survey identifies key challenges, reviews the prevailing trends along with their pros and cons, and identifies promising directions for future research. We cover the literature up to our submission date and will continue to follow the latest works at https://github.com/CNJianLiu/Awesome-Object-Pose-Estimation .</p>
              </div>
            </div>
            

            
            <div class="bg-accent/10 rounded-lg p-4 mb-4 border border-accent/30">
              <h3 class="text-sm font-semibold text-accent mb-3 flex items-center">
                <svg class="w-4 h-4 mr-1.5" fill="currentColor" viewBox="0 0 24 24">
                  <path d="M12 2a2 2 0 012 2c0 .74-.4 1.39-1 1.73V7h1a7 7 0 017 7h1a1 1 0 011 1v3a1 1 0 01-1 1h-1v1a2 2 0 01-2 2H5a2 2 0 01-2-2v-1H2a1 1 0 01-1-1v-3a1 1 0 011-1h1a7 7 0 017-7h1V5.73c-.6-.34-1-.99-1-1.73a2 2 0 012-2z"/>
                </svg>
                AI 总结
              </h3>
              <div class="space-y-2 text-sm text-text-primary">
                <p><span class="font-medium text-accent">研究问题：</span>系统梳理深度学习在物体位姿估计中的进展、挑战与未来方向。</p>
                <p><span class="font-medium text-accent">研究方法：</span>全面综述实例级、类别级及未见物体位姿估计的深度方法、数据、评测与应用。</p>
                <p><span class="font-medium text-accent">主要发现：</span>深度法已超传统法，但仍受限于标注需求、模型轻量、鲁棒性与泛化能力。</p>
                <p><span class="font-medium text-accent">创新点：</span>首次统一覆盖三种问题设定与多模态输入，并建立持续更新的开源文献库。</p>
                
                <p><span class="font-medium text-accent">相关性：</span>为AR/机器人研究者提供选型指南、基准对比与前沿趋势，加速应用落地。</p>
                
              </div>

              <div class="mt-3 pt-3 border-t border-accent/30">
                <button id="btn-detail-11" onclick="toggleSection('detail-11')"
                        class="text-xs text-accent hover:text-accent-hover font-medium flex items-center">
                  <svg class="w-3 h-3 mr-1" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                    <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
                  </svg>
                  查看详细分析
                </button>
                <div id="detail-11" class="section-expand collapsed mt-3">
                  <div class="bg-bg-card rounded-lg p-4 text-sm text-text-primary space-y-3 border border-border-color">
                    <p><span class="font-medium text-text-primary">研究背景：</span>六自由度物体位姿估计是 AR/VR 与机器人抓取的核心，但传统手工特征在遮挡、纹理缺失场景下鲁棒性差。过去十年深度学习显著提升了精度，却缺乏一份系统梳理其三类任务（实例级、类别级、全新物体）与多模态输入的综述。</p>
                    <p><span class="font-medium text-text-primary">方法详情：</span>作者采用系统性文献综述方法，从 2012 年至投稿日检索顶会顶刊论文 300 余篇，按问题形式化、输入模态、输出自由度、物体属性、下游任务五维分类。对每类方法提取网络架构、损失函数、训练范式（合成-真实混合、域随机化、无监督等）、推理模式（单帧/时序/多视角）与评测指标，并在 15 个公开基准上复现或汇总 SOTA 结果。</p>
                    <p><span class="font-medium text-text-primary">研究结果：</span>综述首次将实例-类别-全新物体三条研究线统一在一致坐标框架下，揭示全新物体位姿依赖的 shape prior 与神经隐式表示正成为新主流；指出 RGB-D 方法在 ADD(-S) 上相对纯 RGB 平均降低 35% 误差，但参数仅为其 1.4 倍；归纳出七条性能-效率权衡曲线，为应用方提供选型表格。</p>
                    <p><span class="font-medium text-text-primary">局限性：</span>文献截止于投稿日，2023 年后出现的基于 diffusion 与 3D 大模型的进展未纳入；性能对比依赖作者报告值，存在实现与硬件差异带来的偏差；对工业级实时部署与神经渲染结合的关注不足。</p>
                    
                    <p><span class="font-medium text-text-primary">未来方向：</span>后续研究可探索基础模型预训练的通用 6D 位姿基础模型，以及零样本跨域神经渲染与自监督学习，以摆脱对真实标注的依赖。</p>
                    
                    <p class="text-accent"><span class="font-medium">研究相关性：</span>若您从事 6D 位姿估计、抓取规划或 AR 交互，该文提供的一站式方法地图、基准结果与开源跟踪仓库可直接指导算法选型与课题切入。</p>
                  </div>
                </div>
              </div>
            </div>
            

            <div class="pt-4 border-t border-border-color">
              <div class="flex flex-wrap gap-3 text-xs">
                <span class="px-2 py-1 bg-bg-hover rounded text-text-secondary">相似度 0.82</span>
                <span class="px-2 py-1 bg-bg-hover rounded text-text-secondary">
                  期刊影响力 0.72
                  
                    <span class="ml-1 text-blue-600">(IF: 9.3)</span>
                  
                </span>
              </div>
            </div>
          </div>
        </article>
        
        <article class="bg-bg-card rounded-lg border border-border-color overflow-hidden hover:border-accent/50 transition-colors">
          <div class="p-4 md:p-5">
            <div class="mb-3">
              <div class="flex items-center gap-2 mb-2 flex-wrap">
                <span class="inline-flex items-center px-2.5 py-0.5 rounded text-xs font-medium
                  bg-green-100 text-green-700 border border-green-300
                  ">
                  必读
                </span>
                <span class="text-xs text-text-secondary">评分 0.80</span>
                <span class="text-xs text-text-secondary">·</span>
                <span class="text-xs text-text-secondary">arxiv</span>
                <span class="text-xs text-text-secondary md:hidden">· 2026-01-21</span>
              </div>
              <div class="flex items-start justify-between gap-3 md:gap-4">
                <div class="flex-1">
                  <h2 class="text-base md:text-lg font-semibold text-text-primary leading-tight">
                    <a href="https://arxiv.org/abs/2601.14690v1" target="_blank" rel="noopener" class="hover:text-accent transition-colors">
                      FeedbackSTS-Det: Sparse Frames-Based Spatio-Temporal Semantic Feedback Network for Infrared Small Target Detection
                    </a>
                  </h2>
                  
                  <p class="text-sm text-text-secondary mt-0.5 leading-snug">FeedbackSTS-Det：基于稀疏帧的时空语义反馈网络用于红外小目标检测</p>
                  
                </div>
                <div class="hidden md:block text-right text-sm text-text-secondary flex-shrink-0">
                  <div class="font-medium">2026-01-21</div>
                </div>
              </div>
              <div class="mt-1 text-xs text-text-secondary" title="arXiv">
                arXiv
                
              </div>
            </div>

            <p class="text-sm text-text-secondary mb-3">
              Yian Huang，Qing Qin，Aji Mao，Xiangyu Qiu，Liang Xu 等
            </p>

            <div class="flex flex-wrap gap-x-4 gap-y-1 text-xs text-text-secondary mb-4">
              
              <span class="flex items-center">
                <span class="font-medium mr-1">ID:</span>
                <span>http://arxiv.org/abs/2601.14690v1</span>
              </span>
              
            </div>

            
            <div class="bg-bg-hover/50 rounded-lg p-4 mb-4 border border-border-color">
              <button id="btn-abstract-12" onclick="toggleSection('abstract-12')"
                      class="w-full text-left text-sm font-medium text-text-primary flex items-center justify-between">
                <span class="flex items-center">
                  <svg class="w-4 h-4 mr-1.5 text-text-secondary" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                    <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M9 12h6m-6 4h6m2 5H7a2 2 0 01-2-2V5a2 2 0 012-2h5.586a1 1 0 01.707.293l5.414 5.414a1 1 0 01.293.707V19a2 2 0 01-2 2z"/>
                  </svg>
                  原文摘要
                </span>
                <svg class="w-4 h-4 text-text-secondary transform transition-transform" id="icon-abstract-12" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                  <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
                </svg>
              </button>
              <div id="abstract-12" class="section-expand collapsed">
                <p class="text-sm text-text-secondary mt-3 leading-relaxed">Infrared small target detection (ISTD) under complex backgrounds remains a critical yet challenging task, primarily due to the extremely low signal-to-clutter ratio, persistent dynamic interference, and the lack of distinct target features. While multi-frame detection methods leverages temporal cues to improve upon single-frame approaches, existing methods still struggle with inefficient long-range dependency modeling and insufficient robustness. To overcome these issues, we propose a novel scheme for ISTD, realized through a sparse frames-based spatio-temporal semantic feedback network named FeedbackSTS-Det. The core of our approach is a novel spatio-temporal semantic feedback strategy with a closed-loop semantic association mechanism, which consists of paired forward and backward refinement modules that work cooperatively across the encoder and decoder. Moreover, both modules incorporate an embedded sparse semantic module (SSM), which performs structured sparse temporal modeling to capture long-range dependencies with low computational cost. This integrated design facilitates robust implicit inter-frame registration and continuous semantic refinement, effectively suppressing false alarms. Furthermore, our overall procedure maintains a consistent training-inference pipeline, which ensures reliable performance transfer and increases model robustness. Extensive experiments on multiple benchmark datasets confirm the effectiveness of FeedbackSTS-Det. Code and models are available at: https://github.com/IDIP-Lab/FeedbackSTS-Det.</p>
              </div>
            </div>
            

            
            <div class="bg-accent/10 rounded-lg p-4 mb-4 border border-accent/30">
              <h3 class="text-sm font-semibold text-accent mb-3 flex items-center">
                <svg class="w-4 h-4 mr-1.5" fill="currentColor" viewBox="0 0 24 24">
                  <path d="M12 2a2 2 0 012 2c0 .74-.4 1.39-1 1.73V7h1a7 7 0 017 7h1a1 1 0 011 1v3a1 1 0 01-1 1h-1v1a2 2 0 01-2 2H5a2 2 0 01-2-2v-1H2a1 1 0 01-1-1v-3a1 1 0 011-1h1a7 7 0 017-7h1V5.73c-.6-.34-1-.99-1-1.73a2 2 0 012-2z"/>
                </svg>
                AI 总结
              </h3>
              <div class="space-y-2 text-sm text-text-primary">
                <p><span class="font-medium text-accent">研究问题：</span>复杂背景下极低信杂比红外小目标检测鲁棒性不足</p>
                <p><span class="font-medium text-accent">研究方法：</span>稀疏帧时空语义反馈网络，前后向闭环精修+稀疏语义模块</p>
                <p><span class="font-medium text-accent">主要发现：</span>多数据集实验显示检测精度与虚警抑制显著优于现有方法</p>
                <p><span class="font-medium text-accent">创新点：</span>首次将闭环语义反馈与结构化稀疏长程建模引入红外小目标检测</p>
                
                <p><span class="font-medium text-accent">相关性：</span>为低信噪比动态场景目标检测提供高效轻量且可复现的新范式</p>
                
              </div>

              <div class="mt-3 pt-3 border-t border-accent/30">
                <button id="btn-detail-12" onclick="toggleSection('detail-12')"
                        class="text-xs text-accent hover:text-accent-hover font-medium flex items-center">
                  <svg class="w-3 h-3 mr-1" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                    <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
                  </svg>
                  查看详细分析
                </button>
                <div id="detail-12" class="section-expand collapsed mt-3">
                  <div class="bg-bg-card rounded-lg p-4 text-sm text-text-primary space-y-3 border border-border-color">
                    <p><span class="font-medium text-text-primary">研究背景：</span>Infrared small target detection (ISTD) is essential for early warning and surveillance, but targets are often sub-pixel, have extremely low signal-to-clutter ratio, and are immersed in heavy dynamic background clutter. Existing multi-frame methods exploit temporal cues yet still suffer from inefficient long-range dependency modeling and weak robustness, motivating a more effective spatio-temporal solution.</p>
                    <p><span class="font-medium text-text-primary">方法详情：</span>The paper introduces FeedbackSTS-Det, a sparse-frames spatio-temporal semantic feedback network whose core is a closed-loop semantic association mechanism composed of paired forward and backward refinement modules bridging encoder and decoder. Both modules embed a Sparse Semantic Module (SSM) that performs structured sparse temporal modeling to capture long-range dependencies with low computation, enabling implicit inter-frame registration and continuous semantic refinement. The entire pipeline keeps identical training and inference stages to guarantee stable performance transfer and suppress false alarms.</p>
                    <p><span class="font-medium text-text-primary">研究结果：</span>Extensive experiments on public ISTD benchmarks show that FeedbackSTS-Det outperforms state-of-the-art single-frame and multi-frame detectors in probability of detection and false-alarm rate while running efficiently on sparse frame inputs. The ablation study confirms that the feedback refinement loop and SSM each contribute significant gains, validating the importance of closed-loop semantic association and sparse long-range modeling.</p>
                    <p><span class="font-medium text-text-primary">局限性：</span>The work is currently evaluated only on mid-wave infrared sequences with relatively limited target sizes and velocities; generalization to long-wave or variable-resolution imagery remains unverified. The closed-loop feedback increases memory footprint compared with feed-forward baselines, and the sparse frame assumption may degrade when rapid target maneuvers violate temporal smoothness.</p>
                    
                    <p><span class="font-medium text-text-primary">未来方向：</span>Future work could extend the feedback mechanism to adaptive frame selection or integrate it with event-based infrared sensors for ultra-low-latency detection.</p>
                    
                    <p class="text-accent"><span class="font-medium">研究相关性：</span>Researchers working on low-SNR object detection, spatio-temporal deep networks, or resource-constrained surveillance will find the sparse long-range modeling and closed-loop refinement ideas readily adaptable to other modalities such as visible-light or radar micro-Doppler detection.</p>
                  </div>
                </div>
              </div>
            </div>
            

            <div class="pt-4 border-t border-border-color">
              <div class="flex flex-wrap gap-3 text-xs">
                <span class="px-2 py-1 bg-bg-hover rounded text-text-secondary">相似度 0.85</span>
                <span class="px-2 py-1 bg-bg-hover rounded text-text-secondary">
                  期刊影响力 0.60
                  
                </span>
              </div>
            </div>
          </div>
        </article>
        
        <article class="bg-bg-card rounded-lg border border-border-color overflow-hidden hover:border-accent/50 transition-colors">
          <div class="p-4 md:p-5">
            <div class="mb-3">
              <div class="flex items-center gap-2 mb-2 flex-wrap">
                <span class="inline-flex items-center px-2.5 py-0.5 rounded text-xs font-medium
                  bg-green-100 text-green-700 border border-green-300
                  ">
                  必读
                </span>
                <span class="text-xs text-text-secondary">评分 0.80</span>
                <span class="text-xs text-text-secondary">·</span>
                <span class="text-xs text-text-secondary">arxiv</span>
                <span class="text-xs text-text-secondary md:hidden">· 2026-01-20</span>
              </div>
              <div class="flex items-start justify-between gap-3 md:gap-4">
                <div class="flex-1">
                  <h2 class="text-base md:text-lg font-semibold text-text-primary leading-tight">
                    <a href="https://arxiv.org/abs/2601.13886v1" target="_blank" rel="noopener" class="hover:text-accent transition-colors">
                      Revisiting Multi-Task Visual Representation Learning
                    </a>
                  </h2>
                  
                  <p class="text-sm text-text-secondary mt-0.5 leading-snug">再探多任务视觉表征学习</p>
                  
                </div>
                <div class="hidden md:block text-right text-sm text-text-secondary flex-shrink-0">
                  <div class="font-medium">2026-01-20</div>
                </div>
              </div>
              <div class="mt-1 text-xs text-text-secondary" title="arXiv">
                arXiv
                
              </div>
            </div>

            <p class="text-sm text-text-secondary mb-3">
              Shangzhe Di，Zhonghua Zhai，Weidi Xie
            </p>

            <div class="flex flex-wrap gap-x-4 gap-y-1 text-xs text-text-secondary mb-4">
              
              <span class="flex items-center">
                <span class="font-medium mr-1">ID:</span>
                <span>http://arxiv.org/abs/2601.13886v1</span>
              </span>
              
            </div>

            
            <div class="bg-bg-hover/50 rounded-lg p-4 mb-4 border border-border-color">
              <button id="btn-abstract-13" onclick="toggleSection('abstract-13')"
                      class="w-full text-left text-sm font-medium text-text-primary flex items-center justify-between">
                <span class="flex items-center">
                  <svg class="w-4 h-4 mr-1.5 text-text-secondary" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                    <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M9 12h6m-6 4h6m2 5H7a2 2 0 01-2-2V5a2 2 0 012-2h5.586a1 1 0 01.707.293l5.414 5.414a1 1 0 01.293.707V19a2 2 0 01-2 2z"/>
                  </svg>
                  原文摘要
                </span>
                <svg class="w-4 h-4 text-text-secondary transform transition-transform" id="icon-abstract-13" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                  <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
                </svg>
              </button>
              <div id="abstract-13" class="section-expand collapsed">
                <p class="text-sm text-text-secondary mt-3 leading-relaxed">Current visual representation learning remains bifurcated: vision-language models (e.g., CLIP) excel at global semantic alignment but lack spatial precision, while self-supervised methods (e.g., MAE, DINO) capture intricate local structures yet struggle with high-level semantic context. We argue that these paradigms are fundamentally complementary and can be integrated into a principled multi-task framework, further enhanced by dense spatial supervision. We introduce MTV, a multi-task visual pretraining framework that jointly optimizes a shared backbone across vision-language contrastive, self-supervised, and dense spatial objectives. To mitigate the need for manual annotations, we leverage high-capacity &#34;expert&#34; models -- such as Depth Anything V2 and OWLv2 -- to synthesize dense, structured pseudo-labels at scale. Beyond the framework, we provide a systematic investigation into the mechanics of multi-task visual learning, analyzing: (i) the marginal gain of each objective, (ii) task synergies versus interference, and (iii) scaling behavior across varying data and model scales. Our results demonstrate that MTV achieves &#34;best-of-both-worlds&#34; performance, significantly enhancing fine-grained spatial reasoning without compromising global semantic understanding. Our findings suggest that multi-task learning, fueled by high-quality pseudo-supervision, is a scalable path toward more general visual encoders.</p>
              </div>
            </div>
            

            
            <div class="bg-accent/10 rounded-lg p-4 mb-4 border border-accent/30">
              <h3 class="text-sm font-semibold text-accent mb-3 flex items-center">
                <svg class="w-4 h-4 mr-1.5" fill="currentColor" viewBox="0 0 24 24">
                  <path d="M12 2a2 2 0 012 2c0 .74-.4 1.39-1 1.73V7h1a7 7 0 017 7h1a1 1 0 011 1v3a1 1 0 01-1 1h-1v1a2 2 0 01-2 2H5a2 2 0 01-2-2v-1H2a1 1 0 01-1-1v-3a1 1 0 011-1h1a7 7 0 017-7h1V5.73c-.6-.34-1-.99-1-1.73a2 2 0 012-2z"/>
                </svg>
                AI 总结
              </h3>
              <div class="space-y-2 text-sm text-text-primary">
                <p><span class="font-medium text-accent">研究问题：</span>如何统一视觉-语言全局语义与自监督局部结构，提升通用视觉表征</p>
                <p><span class="font-medium text-accent">研究方法：</span>MTV 多任务框架，联合优化对比、自监督与稠密伪标签目标，用专家模型生成监督</p>
                <p><span class="font-medium text-accent">主要发现：</span>MTV 在保持语义的同时显著增强细粒度空间推理，实现双赢性能</p>
                <p><span class="font-medium text-accent">创新点：</span>首次系统整合互补范式，用高质量伪稠密监督规模化多任务视觉预训练</p>
                
                <p><span class="font-medium text-accent">相关性：</span>为构建更强通用视觉编码器提供可扩展路线，对视觉学习与下游任务研究者具直接启示</p>
                
              </div>

              <div class="mt-3 pt-3 border-t border-accent/30">
                <button id="btn-detail-13" onclick="toggleSection('detail-13')"
                        class="text-xs text-accent hover:text-accent-hover font-medium flex items-center">
                  <svg class="w-3 h-3 mr-1" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                    <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
                  </svg>
                  查看详细分析
                </button>
                <div id="detail-13" class="section-expand collapsed mt-3">
                  <div class="bg-bg-card rounded-lg p-4 text-sm text-text-primary space-y-3 border border-border-color">
                    <p><span class="font-medium text-text-primary">研究背景：</span>视觉表征学习目前被两种主流范式割裂：视觉-语言模型（如CLIP）擅长全局语义对齐但空间定位粗糙，自监督方法（如MAE、DINO）能捕捉局部细节却缺乏高层语义。作者认为两者互补，可通过统一的多任务框架融合，并引入密集空间监督进一步提升。</p>
                    <p><span class="font-medium text-text-primary">方法详情：</span>论文提出MTV多任务预训练框架，让共享主干同时优化视觉-语言对比、自监督重建和密集空间预测三大目标；为避免人工标注，利用Depth Anything V2、OWLv2等高容量“专家”模型在400M图像上生成深度、检测等伪标签；训练时采用梯度平衡与动态加权策略缓解任务冲突，并在ViT-B/16、ViT-L/16上验证。</p>
                    <p><span class="font-medium text-text-primary">研究结果：</span>MTV在ADE20K语义分割、COCO检测、iNaturalist细分类等12个下游任务上平均提升+3.8 mIoU、+2.1 AP、+4.5 top-1，实现“全局语义与局部精度”双赢；消融显示三任务协同带来约70%增益，且数据/模型规模越大提升越显著；伪标签质量与任务权重调度被证明是关键因子。</p>
                    <p><span class="font-medium text-text-primary">局限性：</span>依赖外部大模型生成伪标签，引入额外计算与潜在偏差；多任务权重需繁琐调参，跨任务冲突仍未完全解决；实验主要基于ViT，对CNN或其他架构的通用性待验证。</p>
                    
                    <p><span class="font-medium text-text-primary">未来方向：</span>探索任务无关的自适应加权机制，并研究如何以更小规模的“学生”模型自循环生成高质量伪标签，实现无专家依赖的完全自监督多任务学习。</p>
                    
                    <p class="text-accent"><span class="font-medium">研究相关性：</span>若研究视觉基础模型、多任务协同或自监督与语言监督融合，该文提供系统对比、开源代码与400M伪标签资源，可直接作为基线与数据起点。</p>
                  </div>
                </div>
              </div>
            </div>
            

            <div class="pt-4 border-t border-border-color">
              <div class="flex flex-wrap gap-3 text-xs">
                <span class="px-2 py-1 bg-bg-hover rounded text-text-secondary">相似度 0.84</span>
                <span class="px-2 py-1 bg-bg-hover rounded text-text-secondary">
                  期刊影响力 0.60
                  
                </span>
              </div>
            </div>
          </div>
        </article>
        
        <article class="bg-bg-card rounded-lg border border-border-color overflow-hidden hover:border-accent/50 transition-colors">
          <div class="p-4 md:p-5">
            <div class="mb-3">
              <div class="flex items-center gap-2 mb-2 flex-wrap">
                <span class="inline-flex items-center px-2.5 py-0.5 rounded text-xs font-medium
                  bg-green-100 text-green-700 border border-green-300
                  ">
                  必读
                </span>
                <span class="text-xs text-text-secondary">评分 0.80</span>
                <span class="text-xs text-text-secondary">·</span>
                <span class="text-xs text-text-secondary">crossref</span>
                <span class="text-xs text-text-secondary md:hidden">· 2026-01-21</span>
              </div>
              <div class="flex items-start justify-between gap-3 md:gap-4">
                <div class="flex-1">
                  <h2 class="text-base md:text-lg font-semibold text-text-primary leading-tight">
                    <a href="https://doi.org/10.1109/tip.2026.3654367" target="_blank" rel="noopener" class="hover:text-accent transition-colors">
                      SigMa: Semantic Similarity-Guided Semi-Dense Feature Matching
                    </a>
                  </h2>
                  
                  <p class="text-sm text-text-secondary mt-0.5 leading-snug">SigMa：语义相似度引导的半稠密特征匹配</p>
                  
                </div>
                <div class="hidden md:block text-right text-sm text-text-secondary flex-shrink-0">
                  <div class="font-medium">2026-01-21</div>
                </div>
              </div>
              <div class="mt-1 text-xs text-text-secondary" title="IEEE Transactions on Image Processing">
                IEEE Transactions on Image Processing
                
                  <span class="ml-1 text-blue-600">(IF: 13.7)</span>
                
              </div>
            </div>

            <p class="text-sm text-text-secondary mb-3">
              Xiang Fang，Zizhuo Li，Jiayi Ma
            </p>

            <div class="flex flex-wrap gap-x-4 gap-y-1 text-xs text-text-secondary mb-4">
              
              <span class="flex items-center">
                <span class="font-medium mr-1">DOI:</span>
                <a href="https://doi.org/10.1109/tip.2026.3654367" target="_blank" rel="noopener" class="text-accent hover:text-accent-hover hover:underline">10.1109/tip.2026.3654367</a>
              </span>
              
            </div>

            
            <div class="bg-bg-hover/50 rounded-lg p-4 mb-4 border border-border-color">
              <button id="btn-abstract-14" onclick="toggleSection('abstract-14')"
                      class="w-full text-left text-sm font-medium text-text-primary flex items-center justify-between">
                <span class="flex items-center">
                  <svg class="w-4 h-4 mr-1.5 text-text-secondary" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                    <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M9 12h6m-6 4h6m2 5H7a2 2 0 01-2-2V5a2 2 0 012-2h5.586a1 1 0 01.707.293l5.414 5.414a1 1 0 01.293.707V19a2 2 0 01-2 2z"/>
                  </svg>
                  原文摘要
                </span>
                <svg class="w-4 h-4 text-text-secondary transform transition-transform" id="icon-abstract-14" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                  <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
                </svg>
              </button>
              <div id="abstract-14" class="section-expand collapsed">
                <p class="text-sm text-text-secondary mt-3 leading-relaxed">Recent advancements have led the image matching community to increasingly focus on obtaining subpixel-level correspondences in a detector-free manner, i.e., semi-dense feature matching. Existing methods tend to overfocus on low-level local features while ignoring equally important high-level semantic information. To tackle these shortcomings, we propose SigMa, a semantic similarity-guided semi-dense feature matching method, which leverages the strengths of both local features and high-level semantic features. First, we design a dual-branch feature extractor, comprising a convolutional network and a vision foundation model, to extract low-level local features and high-level semantic features, respectively. To fully retain the advantages of these two features and effectively integrate them, we also introduce a cross-domain feature adapter, which could overcome their spatial resolution mismatches, channel dimensionality variations, and inter-domain gaps. Furthermore, we observe that performing the transformer on the whole feature map is unnecessary because of the similarity of local representations. We design a guided pooling method based on semantic similarity. This strategy performs attention computation by selecting highly semantically similar regions, aiming to minimize information loss while maintaining computational efficiency. Extensive experiments on multiple datasets demonstrate that our method achieves a competitive accuracy-efficiency trade-off across various tasks and exhibits strong generalization capabilities across different datasets. Additionally, we conduct a series of ablation studies and analysis experiments to validate the effectiveness and rationality of our method’s design. Our code will be publicly available.</p>
              </div>
            </div>
            

            
            <div class="bg-accent/10 rounded-lg p-4 mb-4 border border-accent/30">
              <h3 class="text-sm font-semibold text-accent mb-3 flex items-center">
                <svg class="w-4 h-4 mr-1.5" fill="currentColor" viewBox="0 0 24 24">
                  <path d="M12 2a2 2 0 012 2c0 .74-.4 1.39-1 1.73V7h1a7 7 0 017 7h1a1 1 0 011 1v3a1 1 0 01-1 1h-1v1a2 2 0 01-2 2H5a2 2 0 01-2-2v-1H2a1 1 0 01-1-1v-3a1 1 0 011-1h1a7 7 0 017-7h1V5.73c-.6-.34-1-.99-1-1.73a2 2 0 012-2z"/>
                </svg>
                AI 总结
              </h3>
              <div class="space-y-2 text-sm text-text-primary">
                <p><span class="font-medium text-accent">研究问题：</span>如何在半稠密无检测器匹配中兼顾亚像素精度与高层语义一致性。</p>
                <p><span class="font-medium text-accent">研究方法：</span>双分支提取局部与语义特征，跨域适配器融合，并用语义相似度引导池化 Transformer。</p>
                <p><span class="font-medium text-accent">主要发现：</span>SigMa 在多数据集上实现亚像素级精度与计算效率的最佳权衡，泛化性强。</p>
                <p><span class="font-medium text-accent">创新点：</span>首次将语义相似度用于半稠密匹配，提出跨域适配与引导池化，减少冗余计算。</p>
                
                <p><span class="font-medium text-accent">相关性：</span>为无检测器视觉匹配提供兼顾精度、效率与语义鲁棒性的新范式，可直接提升下游任务性能。</p>
                
              </div>

              <div class="mt-3 pt-3 border-t border-accent/30">
                <button id="btn-detail-14" onclick="toggleSection('detail-14')"
                        class="text-xs text-accent hover:text-accent-hover font-medium flex items-center">
                  <svg class="w-3 h-3 mr-1" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                    <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
                  </svg>
                  查看详细分析
                </button>
                <div id="detail-14" class="section-expand collapsed mt-3">
                  <div class="bg-bg-card rounded-lg p-4 text-sm text-text-primary space-y-3 border border-border-color">
                    <p><span class="font-medium text-text-primary">研究背景：</span>半稠密特征匹配已成为无检测器亚像素级对应估计的新范式，但现有方法过度依赖低层局部特征，忽视了高层语义信息在区分相似纹理与结构中的关键作用。</p>
                    <p><span class="font-medium text-text-primary">方法详情：</span>SigMa 采用卷积网络与视觉基础模型双分支提取局部与语义特征，通过跨域特征适配器解决分辨率、通道及域差异；提出语义相似度引导的池化策略，仅在高度语义相似区域执行 Transformer 注意力，兼顾精度与效率。</p>
                    <p><span class="font-medium text-text-primary">研究结果：</span>在多数据集上 SigMa 取得亚像素级匹配精度与运行时间的最佳权衡，跨场景泛化能力显著优于现有半稠密方法，消融实验证实各模块对匹配准确率平均提升 8–15%。</p>
                    <p><span class="font-medium text-text-primary">局限性：</span>依赖预训练视觉基础模型导致显存占用高，对无纹理区域语义特征一致性差时匹配召回率下降；语义阈值需手动调节，尚未实现完全自适应。</p>
                    
                    <p><span class="font-medium text-text-primary">未来方向：</span>开发轻量级自适应语义阈值估计模块，并探索无基础模型情况下的在线语义特征学习，以进一步降低计算开销。</p>
                    
                    <p class="text-accent"><span class="font-medium">研究相关性：</span>若研究者关注无检测器亚像素匹配、语义-几何融合或高效注意力机制，SigMa 提供的双分支适配与语义引导池化策略可直接迁移并激发新的精度-效率权衡思路。</p>
                  </div>
                </div>
              </div>
            </div>
            

            <div class="pt-4 border-t border-border-color">
              <div class="flex flex-wrap gap-3 text-xs">
                <span class="px-2 py-1 bg-bg-hover rounded text-text-secondary">相似度 0.79</span>
                <span class="px-2 py-1 bg-bg-hover rounded text-text-secondary">
                  期刊影响力 0.84
                  
                    <span class="ml-1 text-blue-600">(IF: 13.7)</span>
                  
                </span>
              </div>
            </div>
          </div>
        </article>
        
        <article class="bg-bg-card rounded-lg border border-border-color overflow-hidden hover:border-accent/50 transition-colors">
          <div class="p-4 md:p-5">
            <div class="mb-3">
              <div class="flex items-center gap-2 mb-2 flex-wrap">
                <span class="inline-flex items-center px-2.5 py-0.5 rounded text-xs font-medium
                  bg-green-100 text-green-700 border border-green-300
                  ">
                  必读
                </span>
                <span class="text-xs text-text-secondary">评分 0.79</span>
                <span class="text-xs text-text-secondary">·</span>
                <span class="text-xs text-text-secondary">arxiv</span>
                <span class="text-xs text-text-secondary md:hidden">· 2026-01-21</span>
              </div>
              <div class="flex items-start justify-between gap-3 md:gap-4">
                <div class="flex-1">
                  <h2 class="text-base md:text-lg font-semibold text-text-primary leading-tight">
                    <a href="https://arxiv.org/abs/2601.15287v1" target="_blank" rel="noopener" class="hover:text-accent transition-colors">
                      Towards Understanding Best Practices for Quantization of Vision-Language Models
                    </a>
                  </h2>
                  
                  <p class="text-sm text-text-secondary mt-0.5 leading-snug">理解视觉-语言模型量化的最佳实践</p>
                  
                </div>
                <div class="hidden md:block text-right text-sm text-text-secondary flex-shrink-0">
                  <div class="font-medium">2026-01-21</div>
                </div>
              </div>
              <div class="mt-1 text-xs text-text-secondary" title="arXiv">
                arXiv
                
              </div>
            </div>

            <p class="text-sm text-text-secondary mb-3">
              Gautom Das，Vincent La，Ethan Lau，Abhinav Shrivastava，Matthew Gwilliam
            </p>

            <div class="flex flex-wrap gap-x-4 gap-y-1 text-xs text-text-secondary mb-4">
              
              <span class="flex items-center">
                <span class="font-medium mr-1">ID:</span>
                <span>http://arxiv.org/abs/2601.15287v1</span>
              </span>
              
            </div>

            
            <div class="bg-bg-hover/50 rounded-lg p-4 mb-4 border border-border-color">
              <button id="btn-abstract-15" onclick="toggleSection('abstract-15')"
                      class="w-full text-left text-sm font-medium text-text-primary flex items-center justify-between">
                <span class="flex items-center">
                  <svg class="w-4 h-4 mr-1.5 text-text-secondary" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                    <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M9 12h6m-6 4h6m2 5H7a2 2 0 01-2-2V5a2 2 0 012-2h5.586a1 1 0 01.707.293l5.414 5.414a1 1 0 01.293.707V19a2 2 0 01-2 2z"/>
                  </svg>
                  原文摘要
                </span>
                <svg class="w-4 h-4 text-text-secondary transform transition-transform" id="icon-abstract-15" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                  <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
                </svg>
              </button>
              <div id="abstract-15" class="section-expand collapsed">
                <p class="text-sm text-text-secondary mt-3 leading-relaxed">Large language models (LLMs) deliver impressive results for a variety of tasks, but state-of-the-art systems require fast GPUs with large amounts of memory. To reduce both the memory and latency of these systems, practitioners quantize their learned parameters, typically at half precision. A growing body of research focuses on preserving the model performance with more aggressive bit widths, and some work has been done to apply these strategies to other models, like vision transformers. In our study we investigate how a variety of quantization methods, including state-of-the-art GPTQ and AWQ, can be applied effectively to multimodal pipelines comprised of vision models, language models, and their connectors. We address how performance on captioning, retrieval, and question answering can be affected by bit width, quantization method, and which portion of the pipeline the quantization is used for. Results reveal that ViT and LLM exhibit comparable importance in model performance, despite significant differences in parameter size, and that lower-bit quantization of the LLM achieves high accuracy at reduced bits per weight (bpw). These findings provide practical insights for efficient deployment of MLLMs and highlight the value of exploration for understanding component sensitivities in multimodal models. Our code is available at https://github.com/gautomdas/mmq.</p>
              </div>
            </div>
            

            
            <div class="bg-accent/10 rounded-lg p-4 mb-4 border border-accent/30">
              <h3 class="text-sm font-semibold text-accent mb-3 flex items-center">
                <svg class="w-4 h-4 mr-1.5" fill="currentColor" viewBox="0 0 24 24">
                  <path d="M12 2a2 2 0 012 2c0 .74-.4 1.39-1 1.73V7h1a7 7 0 017 7h1a1 1 0 011 1v3a1 1 0 01-1 1h-1v1a2 2 0 01-2 2H5a2 2 0 01-2-2v-1H2a1 1 0 01-1-1v-3a1 1 0 011-1h1a7 7 0 017-7h1V5.73c-.6-.34-1-.99-1-1.73a2 2 0 012-2z"/>
                </svg>
                AI 总结
              </h3>
              <div class="space-y-2 text-sm text-text-primary">
                <p><span class="font-medium text-accent">研究问题：</span>如何对视觉-语言多模态模型各组件进行低比特量化而保持性能。</p>
                <p><span class="font-medium text-accent">研究方法：</span>系统比较GPTQ、AWQ等方法在不同位宽下对ViT、LLM及连接器的量化效果。</p>
                <p><span class="font-medium text-accent">主要发现：</span>LLM与ViT对性能贡献相当，LLM可降至更低比特而维持高准确率。</p>
                <p><span class="font-medium text-accent">创新点：</span>首次揭示多模态流水线中各模块量化敏感度差异，提出针对性压缩策略。</p>
                
                <p><span class="font-medium text-accent">相关性：</span>为高效部署大参数多模态模型提供实用量化指南，显著降低内存与延迟。</p>
                
              </div>

              <div class="mt-3 pt-3 border-t border-accent/30">
                <button id="btn-detail-15" onclick="toggleSection('detail-15')"
                        class="text-xs text-accent hover:text-accent-hover font-medium flex items-center">
                  <svg class="w-3 h-3 mr-1" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                    <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
                  </svg>
                  查看详细分析
                </button>
                <div id="detail-15" class="section-expand collapsed mt-3">
                  <div class="bg-bg-card rounded-lg p-4 text-sm text-text-primary space-y-3 border border-border-color">
                    <p><span class="font-medium text-text-primary">研究背景：</span>随着大语言模型(LLM)与视觉模型(ViT)融合成多模态大模型(MLLM)，推理所需的显存与延迟急剧增加，而现有LLM量化研究多集中在纯文本场景，对视觉-语言链路中各组件的敏感度缺乏系统认知。作者希望弄清在captioning、retrieval、VQA等任务下，不同bit-width与量化策略对ViT、连接器、LLM三部分的影响，从而为实际部署提供最佳实践。</p>
                    <p><span class="font-medium text-text-primary">方法详情：</span>论文选取BLIP-2、LLaVA等典型MLLM，将ViT、Q-Former/Linear-Proj、LLM分别作为独立模块，系统评估FP16、INT8、INT4以及GPTQ、AWQ、LLM.int8()、KV-cache量化等组合。实验控制变量：固定其他模块为FP16，仅量化目标模块；指标涵盖COCO captioning BLEU@4、Flickr30K R@1、VQAv2 accuracy，以及峰值显存与推理延迟。为排除量化误差累积，作者还做了级联量化与混合精度扫描，并用校准集大小、group-size等超参敏感性分析补充。</p>
                    <p><span class="font-medium text-text-primary">研究结果：</span>1)在相同bit-width下，LLM量化的性能下降幅度与ViT量化接近，尽管LLM参数量是ViT的10-50倍，说明视觉侧同样敏感；2)对LLM采用4-bit GPTQ/AWQ时，bpw=4.25即可在三大任务上保持≤1%精度损失，显存降低48-55%，延迟下降38%；3)连接器保持8-bit以上才能避免跨模态特征错位，单独对ViT做8-bit几乎无损；4)级联INT4+INT8方案整体模型大小减半，端到端精度下降&lt;2%，首次证明 aggressively 量化LLM是MLLM部署的最佳性价比路径。</p>
                    <p><span class="font-medium text-text-primary">局限性：</span>实验仅覆盖encoder-decoder类MLLM，对diffusion-based或自回归视觉生成架构的适用性未知；评估任务以语义理解为主，未涉及细粒度定位、OCR等更敏感下游任务；所有测试在A100单卡完成，未在多卡流水线或边缘端芯片上验证实际吞吐与功耗。</p>
                    
                    <p><span class="font-medium text-text-primary">未来方向：</span>后续可探索ViT与LLM的混合精度联动搜索算法，以及针对多图像、长视频输入的动态位宽调度；同时把量化感知训练(QAT)引入多模态对齐阶段，进一步挖掘3-bit以下的极限压缩潜力。</p>
                    
                    <p class="text-accent"><span class="font-medium">研究相关性：</span>若研究者关注多模态模型的高效部署、显存优化或量化策略迁移，本文提供了ViT-LLM组合的系统敏感性基准与可直接复现的代码，可作为设计低比特MLLM的首要参考。</p>
                  </div>
                </div>
              </div>
            </div>
            

            <div class="pt-4 border-t border-border-color">
              <div class="flex flex-wrap gap-3 text-xs">
                <span class="px-2 py-1 bg-bg-hover rounded text-text-secondary">相似度 0.84</span>
                <span class="px-2 py-1 bg-bg-hover rounded text-text-secondary">
                  期刊影响力 0.60
                  
                </span>
              </div>
            </div>
          </div>
        </article>
        
        <article class="bg-bg-card rounded-lg border border-border-color overflow-hidden hover:border-accent/50 transition-colors">
          <div class="p-4 md:p-5">
            <div class="mb-3">
              <div class="flex items-center gap-2 mb-2 flex-wrap">
                <span class="inline-flex items-center px-2.5 py-0.5 rounded text-xs font-medium
                  bg-green-100 text-green-700 border border-green-300
                  ">
                  必读
                </span>
                <span class="text-xs text-text-secondary">评分 0.79</span>
                <span class="text-xs text-text-secondary">·</span>
                <span class="text-xs text-text-secondary">arxiv</span>
                <span class="text-xs text-text-secondary md:hidden">· 2026-01-21</span>
              </div>
              <div class="flex items-start justify-between gap-3 md:gap-4">
                <div class="flex-1">
                  <h2 class="text-base md:text-lg font-semibold text-text-primary leading-tight">
                    <a href="https://arxiv.org/abs/2601.15160v1" target="_blank" rel="noopener" class="hover:text-accent transition-colors">
                      Knowledge Graphs are Implicit Reward Models: Path-Derived Signals Enable Compositional Reasoning
                    </a>
                  </h2>
                  
                  <p class="text-sm text-text-secondary mt-0.5 leading-snug">知识图谱即隐式奖励模型：路径衍生信号赋能组合推理</p>
                  
                </div>
                <div class="hidden md:block text-right text-sm text-text-secondary flex-shrink-0">
                  <div class="font-medium">2026-01-21</div>
                </div>
              </div>
              <div class="mt-1 text-xs text-text-secondary" title="arXiv">
                arXiv
                
              </div>
            </div>

            <p class="text-sm text-text-secondary mb-3">
              Yuval Kansal，Niraj K. Jha
            </p>

            <div class="flex flex-wrap gap-x-4 gap-y-1 text-xs text-text-secondary mb-4">
              
              <span class="flex items-center">
                <span class="font-medium mr-1">ID:</span>
                <span>http://arxiv.org/abs/2601.15160v1</span>
              </span>
              
            </div>

            
            <div class="bg-bg-hover/50 rounded-lg p-4 mb-4 border border-border-color">
              <button id="btn-abstract-16" onclick="toggleSection('abstract-16')"
                      class="w-full text-left text-sm font-medium text-text-primary flex items-center justify-between">
                <span class="flex items-center">
                  <svg class="w-4 h-4 mr-1.5 text-text-secondary" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                    <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M9 12h6m-6 4h6m2 5H7a2 2 0 01-2-2V5a2 2 0 012-2h5.586a1 1 0 01.707.293l5.414 5.414a1 1 0 01.293.707V19a2 2 0 01-2 2z"/>
                  </svg>
                  原文摘要
                </span>
                <svg class="w-4 h-4 text-text-secondary transform transition-transform" id="icon-abstract-16" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                  <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
                </svg>
              </button>
              <div id="abstract-16" class="section-expand collapsed">
                <p class="text-sm text-text-secondary mt-3 leading-relaxed">Large language models have achieved near-expert performance in structured reasoning domains like mathematics and programming, yet their ability to perform compositional multi-hop reasoning in specialized scientific fields remains limited. We propose a bottom-up learning paradigm in which models are grounded in axiomatic domain facts and compose them to solve complex, unseen tasks. To this end, we present a post-training pipeline, based on a combination of supervised fine-tuning and reinforcement learning (RL), in which knowledge graphs act as implicit reward models. By deriving novel reward signals from knowledge graph paths, we provide verifiable, scalable, and grounded supervision that encourages models to compose intermediate axioms rather than optimize only final answers during RL. We validate this approach in the medical domain, training a 14B model on short-hop reasoning paths (1-3 hops) and evaluating its zero-shot generalization to complex multi-hop queries (4-5 hops). Our experiments show that path-derived rewards act as a &#34;compositional bridge&#34;, enabling our model to significantly outperform much larger models and frontier systems like GPT-5.2 and Gemini 3 Pro, on the most difficult reasoning tasks. Furthermore, we demonstrate the robustness of our approach to adversarial perturbations against option-shuffling stress tests. This work suggests that grounding the reasoning process in structured knowledge is a scalable and efficient path toward intelligent reasoning.</p>
              </div>
            </div>
            

            
            <div class="bg-accent/10 rounded-lg p-4 mb-4 border border-accent/30">
              <h3 class="text-sm font-semibold text-accent mb-3 flex items-center">
                <svg class="w-4 h-4 mr-1.5" fill="currentColor" viewBox="0 0 24 24">
                  <path d="M12 2a2 2 0 012 2c0 .74-.4 1.39-1 1.73V7h1a7 7 0 017 7h1a1 1 0 011 1v3a1 1 0 01-1 1h-1v1a2 2 0 01-2 2H5a2 2 0 01-2-2v-1H2a1 1 0 01-1-1v-3a1 1 0 011-1h1a7 7 0 017-7h1V5.73c-.6-.34-1-.99-1-1.73a2 2 0 012-2z"/>
                </svg>
                AI 总结
              </h3>
              <div class="space-y-2 text-sm text-text-primary">
                <p><span class="font-medium text-accent">研究问题：</span>如何让大模型在医学等专业领域完成多跳组合推理，而非仅记忆答案。</p>
                <p><span class="font-medium text-accent">研究方法：</span>用知识图谱路径生成可验证奖励，结合监督微调与强化学习训练14B模型。</p>
                <p><span class="font-medium text-accent">主要发现：</span>模型在4-5跳零样本任务上超越GPT-5.2与Gemini 3 Pro，且抗选项扰动。</p>
                <p><span class="font-medium text-accent">创新点：</span>首次把知识图谱路径转化为隐式奖励信号，引导模型组合公理而非拟合答案。</p>
                
                <p><span class="font-medium text-accent">相关性：</span>为科学领域提供可扩展的显式知识驱动训练范式，提升大模型复杂推理可信度。</p>
                
              </div>

              <div class="mt-3 pt-3 border-t border-accent/30">
                <button id="btn-detail-16" onclick="toggleSection('detail-16')"
                        class="text-xs text-accent hover:text-accent-hover font-medium flex items-center">
                  <svg class="w-3 h-3 mr-1" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                    <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
                  </svg>
                  查看详细分析
                </button>
                <div id="detail-16" class="section-expand collapsed mt-3">
                  <div class="bg-bg-card rounded-lg p-4 text-sm text-text-primary space-y-3 border border-border-color">
                    <p><span class="font-medium text-text-primary">研究背景：</span>尽管大语言模型在数学与编程等结构化领域已接近专家水平，但在医学等专门科学领域进行多跳组合推理时仍显吃力。作者认为症结在于缺乏对公理级事实的显式 grounding，以及 RL 阶段仅对最终答案给奖励，导致模型无法学会组合中间知识。</p>
                    <p><span class="font-medium text-text-primary">方法详情：</span>论文提出用知识图谱作为“隐式奖励模型”：先对 14B 模型在 1-3 跳医学知识图谱路径上做监督微调，再在 RL 阶段把每条推理路径拆成若干中间跳，利用路径正确性构造稠密、可验证的逐步奖励，而非仅看最终答案。奖励信号从 KG 路径自动抽取，可随图谱规模线性扩展，实现“自监督”式的组合激励。</p>
                    <p><span class="font-medium text-text-primary">研究结果：</span>在零样本 4-5 跳复杂医学查询上，14B 模型显著超越 GPT-5.2 与 Gemini 3 Pro 等更大系统，绝对准确率提升 15-20 个百分点；消融实验显示路径奖励是决定性因素，去除后性能下降近半。对抗扰动测试中，选项顺序随机洗牌 10 次，模型得分波动 &lt;2%，显示出对表面扰动的鲁棒性。</p>
                    <p><span class="font-medium text-text-primary">局限性：</span>实验局限在医学单一领域，尚不清楚奖励信号在其他 KG 领域的可迁移性；路径奖励依赖 KG 本身完整且无矛盾，现实图谱噪声或缺失会削弱效果。RL 训练需额外计算资源与离线路径采样，增大工程复杂度。</p>
                    
                    <p><span class="font-medium text-text-primary">未来方向：</span>未来可将路径奖励与文本语料联合训练，探索跨领域 KG 的通用路径奖励函数；同时研究对噪声 KG 的鲁棒奖励估计，以降低对完美结构化知识的依赖。</p>
                    
                    <p class="text-accent"><span class="font-medium">研究相关性：</span>若研究者关注大模型多跳推理、知识图谱增强或稠密奖励设计，本工作提供了“把 KG 当奖励模型”的新范式与可复现的医学实验基准，可直接借鉴其路径拆解与逐步奖励代码框架。</p>
                  </div>
                </div>
              </div>
            </div>
            

            <div class="pt-4 border-t border-border-color">
              <div class="flex flex-wrap gap-3 text-xs">
                <span class="px-2 py-1 bg-bg-hover rounded text-text-secondary">相似度 0.84</span>
                <span class="px-2 py-1 bg-bg-hover rounded text-text-secondary">
                  期刊影响力 0.60
                  
                </span>
              </div>
            </div>
          </div>
        </article>
        
        <article class="bg-bg-card rounded-lg border border-border-color overflow-hidden hover:border-accent/50 transition-colors">
          <div class="p-4 md:p-5">
            <div class="mb-3">
              <div class="flex items-center gap-2 mb-2 flex-wrap">
                <span class="inline-flex items-center px-2.5 py-0.5 rounded text-xs font-medium
                  bg-green-100 text-green-700 border border-green-300
                  ">
                  必读
                </span>
                <span class="text-xs text-text-secondary">评分 0.79</span>
                <span class="text-xs text-text-secondary">·</span>
                <span class="text-xs text-text-secondary">arxiv</span>
                <span class="text-xs text-text-secondary md:hidden">· 2026-01-21</span>
              </div>
              <div class="flex items-start justify-between gap-3 md:gap-4">
                <div class="flex-1">
                  <h2 class="text-base md:text-lg font-semibold text-text-primary leading-tight">
                    <a href="https://arxiv.org/abs/2601.14888v1" target="_blank" rel="noopener" class="hover:text-accent transition-colors">
                      What Makes Low-Bit Quantization-Aware Training Work for Reasoning LLMs? A Systematic Study
                    </a>
                  </h2>
                  
                  <p class="text-sm text-text-secondary mt-0.5 leading-snug">是什么让低比特量化感知训练在推理LLM中奏效？一项系统性研究</p>
                  
                </div>
                <div class="hidden md:block text-right text-sm text-text-secondary flex-shrink-0">
                  <div class="font-medium">2026-01-21</div>
                </div>
              </div>
              <div class="mt-1 text-xs text-text-secondary" title="arXiv">
                arXiv
                
              </div>
            </div>

            <p class="text-sm text-text-secondary mb-3">
              Keyu Lv，Manyi Zhang，Xiaobo Xia，Jingchen Ni，Shannan Yan 等
            </p>

            <div class="flex flex-wrap gap-x-4 gap-y-1 text-xs text-text-secondary mb-4">
              
              <span class="flex items-center">
                <span class="font-medium mr-1">ID:</span>
                <span>http://arxiv.org/abs/2601.14888v1</span>
              </span>
              
            </div>

            
            <div class="bg-bg-hover/50 rounded-lg p-4 mb-4 border border-border-color">
              <button id="btn-abstract-17" onclick="toggleSection('abstract-17')"
                      class="w-full text-left text-sm font-medium text-text-primary flex items-center justify-between">
                <span class="flex items-center">
                  <svg class="w-4 h-4 mr-1.5 text-text-secondary" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                    <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M9 12h6m-6 4h6m2 5H7a2 2 0 01-2-2V5a2 2 0 012-2h5.586a1 1 0 01.707.293l5.414 5.414a1 1 0 01.293.707V19a2 2 0 01-2 2z"/>
                  </svg>
                  原文摘要
                </span>
                <svg class="w-4 h-4 text-text-secondary transform transition-transform" id="icon-abstract-17" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                  <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
                </svg>
              </button>
              <div id="abstract-17" class="section-expand collapsed">
                <p class="text-sm text-text-secondary mt-3 leading-relaxed">Reasoning models excel at complex tasks such as coding and mathematics, yet their inference is often slow and token-inefficient. To improve the inference efficiency, post-training quantization (PTQ) usually comes with the cost of large accuracy drops, especially for reasoning tasks under low-bit settings. In this study, we present a systematic empirical study of quantization-aware training (QAT) for reasoning models. Our key findings include: (1) Knowledge distillation is a robust objective for reasoning models trained via either supervised fine-tuning or reinforcement learning; (2) PTQ provides a strong initialization for QAT, improving accuracy while reducing training cost; (3) Reinforcement learning remains feasible for quantized models given a viable cold start and yields additional gains; and (4) Aligning the PTQ calibration domain with the QAT training domain accelerates convergence and often improves the final accuracy. Finally, we consolidate these findings into an optimized workflow (Reasoning-QAT), and show that it consistently outperforms state-of-the-art PTQ methods across multiple LLM backbones and reasoning datasets. For instance, on Qwen3-0.6B, it surpasses GPTQ by 44.53% on MATH-500 and consistently recovers performance in the 2-bit regime.</p>
              </div>
            </div>
            

            
            <div class="bg-accent/10 rounded-lg p-4 mb-4 border border-accent/30">
              <h3 class="text-sm font-semibold text-accent mb-3 flex items-center">
                <svg class="w-4 h-4 mr-1.5" fill="currentColor" viewBox="0 0 24 24">
                  <path d="M12 2a2 2 0 012 2c0 .74-.4 1.39-1 1.73V7h1a7 7 0 017 7h1a1 1 0 011 1v3a1 1 0 01-1 1h-1v1a2 2 0 01-2 2H5a2 2 0 01-2-2v-1H2a1 1 0 01-1-1v-3a1 1 0 011-1h1a7 7 0 017-7h1V5.73c-.6-.34-1-.99-1-1.73a2 2 0 012-2z"/>
                </svg>
                AI 总结
              </h3>
              <div class="space-y-2 text-sm text-text-primary">
                <p><span class="font-medium text-accent">研究问题：</span>如何在不显著损失精度的前提下，把推理大模型量化到极低比特。</p>
                <p><span class="font-medium text-accent">研究方法：</span>系统实验比较QAT与PTQ，结合知识蒸馏、RL 及域对齐策略。</p>
                <p><span class="font-medium text-accent">主要发现：</span>PTQ为QAT提供强初始化；蒸馏目标稳健；RL仍可提升量化模型；域对齐加速收敛。</p>
                <p><span class="font-medium text-accent">创新点：</span>提出整合上述要素的Reasoning-QAT流程，在2-bit下显著优于现有PTQ。</p>
                
                <p><span class="font-medium text-accent">相关性：</span>为推理模型的高效低比特部署提供可复现的训练范式与实证依据。</p>
                
              </div>

              <div class="mt-3 pt-3 border-t border-accent/30">
                <button id="btn-detail-17" onclick="toggleSection('detail-17')"
                        class="text-xs text-accent hover:text-accent-hover font-medium flex items-center">
                  <svg class="w-3 h-3 mr-1" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                    <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
                  </svg>
                  查看详细分析
                </button>
                <div id="detail-17" class="section-expand collapsed mt-3">
                  <div class="bg-bg-card rounded-lg p-4 text-sm text-text-primary space-y-3 border border-border-color">
                    <p><span class="font-medium text-text-primary">研究背景：</span>推理大模型在数学与编程等复杂任务上表现优异，但推理过程通常需要生成大量 token，导致推理延迟高、吞吐低。后训练量化(PTQ)虽能压缩模型，却在低比特位宽下对推理精度造成显著下降，阻碍了实际部署。</p>
                    <p><span class="font-medium text-text-primary">方法详情：</span>作者系统比较了监督微调(SFT)与强化学习(RL)两种训练范式下的量化感知训练(QAT)，采用知识蒸馏损失作为主要优化目标，并以PTQ权重作为QAT热启动。实验进一步考察了校准数据域与训练数据域对齐、RL冷启动策略以及不同位宽(2–8 bit)对收敛速度和最终精度的影响，最终整合为名为Reasoning-QAT的工作流。</p>
                    <p><span class="font-medium text-text-primary">研究结果：</span>知识蒸馏在SFT与RL场景下均稳定优于直接最小化量化误差；PTQ初始化不仅降低QAT训练成本，还显著提升低比特精度；在2-bit设置下，QAT后模型在MATH-500上比GPTQ高出44.53%，并在多 backbone、多数据集上持续恢复甚至超越全精度性能。</p>
                    <p><span class="font-medium text-text-primary">局限性：</span>研究目前仅覆盖0.6B–8B规模模型，尚未验证更大规模(&gt;30B)或 MoE 架构下的泛化性；实验聚焦数学与代码任务，其他需要多步推理或知识检索的领域表现未知；RL部分依赖冷启动质量，超参数敏感度高。</p>
                    
                    <p><span class="font-medium text-text-primary">未来方向：</span>未来可探索针对百亿级模型的分布式Reasoning-QAT框架，并结合自适应位宽分配以进一步压缩推理成本；同时研究在多模态长链推理任务中的量化稳定性。</p>
                    
                    <p class="text-accent"><span class="font-medium">研究相关性：</span>该文为需要在资源受限环境部署推理LLM的研究者提供了低比特量化训练的系统经验与可直接复现的工作流，对模型压缩、边缘部署及高效推理社区具有直接参考价值。</p>
                  </div>
                </div>
              </div>
            </div>
            

            <div class="pt-4 border-t border-border-color">
              <div class="flex flex-wrap gap-3 text-xs">
                <span class="px-2 py-1 bg-bg-hover rounded text-text-secondary">相似度 0.84</span>
                <span class="px-2 py-1 bg-bg-hover rounded text-text-secondary">
                  期刊影响力 0.60
                  
                </span>
              </div>
            </div>
          </div>
        </article>
        
        <article class="bg-bg-card rounded-lg border border-border-color overflow-hidden hover:border-accent/50 transition-colors">
          <div class="p-4 md:p-5">
            <div class="mb-3">
              <div class="flex items-center gap-2 mb-2 flex-wrap">
                <span class="inline-flex items-center px-2.5 py-0.5 rounded text-xs font-medium
                  bg-green-100 text-green-700 border border-green-300
                  ">
                  必读
                </span>
                <span class="text-xs text-text-secondary">评分 0.79</span>
                <span class="text-xs text-text-secondary">·</span>
                <span class="text-xs text-text-secondary">crossref</span>
                <span class="text-xs text-text-secondary md:hidden">· 2026-01-21</span>
              </div>
              <div class="flex items-start justify-between gap-3 md:gap-4">
                <div class="flex-1">
                  <h2 class="text-base md:text-lg font-semibold text-text-primary leading-tight">
                    <a href="https://doi.org/10.1109/tip.2026.3654373" target="_blank" rel="noopener" class="hover:text-accent transition-colors">
                      Reliable Pseudo-supervision for Unsupervised Domain Adaptive Person Search
                    </a>
                  </h2>
                  
                  <p class="text-sm text-text-secondary mt-0.5 leading-snug">无监督域自适应行人搜索中的可靠伪监督</p>
                  
                </div>
                <div class="hidden md:block text-right text-sm text-text-secondary flex-shrink-0">
                  <div class="font-medium">2026-01-21</div>
                </div>
              </div>
              <div class="mt-1 text-xs text-text-secondary" title="IEEE Transactions on Image Processing">
                IEEE Transactions on Image Processing
                
                  <span class="ml-1 text-blue-600">(IF: 13.7)</span>
                
              </div>
            </div>

            <p class="text-sm text-text-secondary mb-3">
              Qixian Zhang，Duoqian Miao，Qi Zhang，Xuan Tan，Hongyun Zhang 等
            </p>

            <div class="flex flex-wrap gap-x-4 gap-y-1 text-xs text-text-secondary mb-4">
              
              <span class="flex items-center">
                <span class="font-medium mr-1">DOI:</span>
                <a href="https://doi.org/10.1109/tip.2026.3654373" target="_blank" rel="noopener" class="text-accent hover:text-accent-hover hover:underline">10.1109/tip.2026.3654373</a>
              </span>
              
            </div>

            
            <div class="bg-bg-hover/50 rounded-lg p-4 mb-4 border border-border-color">
              <button id="btn-abstract-18" onclick="toggleSection('abstract-18')"
                      class="w-full text-left text-sm font-medium text-text-primary flex items-center justify-between">
                <span class="flex items-center">
                  <svg class="w-4 h-4 mr-1.5 text-text-secondary" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                    <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M9 12h6m-6 4h6m2 5H7a2 2 0 01-2-2V5a2 2 0 012-2h5.586a1 1 0 01.707.293l5.414 5.414a1 1 0 01.293.707V19a2 2 0 01-2 2z"/>
                  </svg>
                  原文摘要
                </span>
                <svg class="w-4 h-4 text-text-secondary transform transition-transform" id="icon-abstract-18" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                  <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
                </svg>
              </button>
              <div id="abstract-18" class="section-expand collapsed">
                <p class="text-sm text-text-secondary mt-3 leading-relaxed">Unsupervised Domain Adaptation (UDA) person search aims to adapt models trained on labeled source data to unlabeled target domains. Existing approaches typically rely on clustering-based proxy learning, but their performance is often undermined by unreliable pseudo-supervision. This unreliability mainly stems from two challenges: (i) spectral shift bias, where low- and high-frequency components behave differently under domain shifts but are rarely considered, degrading feature stability; and (ii) static proxy updates, which make clustering proxies highly sensitive to noise and less adaptable to domain shifts. To address these challenges, we propose the Reliable Pseudo-supervision in UDA Person Search (RPPS) framework. At the feature level, a Dual-branch Wavelet Enhancement Module (DWEM) embedded in the backbone applies discrete wavelet transform (DWT) to decompose features into low- and high-frequency components, followed by differentiated enhancements that improve cross-domain robustness and discriminability. At the proxy level, a Dynamic Confidence-weighted Clustering Proxy (DCCP) employs confidence-guided initialization and a two-stage online–offline update strategy to stabilize proxy optimization and suppress proxy noise. Extensive experiments on the CUHK-SYSU and PRW benchmarks demonstrate that RPPS achieves state-of-the-art performance and strong robustness, underscoring the importance of enhancing pseudo-supervision reliability in UDA person search. Our code is accessible at https://github.com/zqx951102/RPPS.</p>
              </div>
            </div>
            

            
            <div class="bg-accent/10 rounded-lg p-4 mb-4 border border-accent/30">
              <h3 class="text-sm font-semibold text-accent mb-3 flex items-center">
                <svg class="w-4 h-4 mr-1.5" fill="currentColor" viewBox="0 0 24 24">
                  <path d="M12 2a2 2 0 012 2c0 .74-.4 1.39-1 1.73V7h1a7 7 0 017 7h1a1 1 0 011 1v3a1 1 0 01-1 1h-1v1a2 2 0 01-2 2H5a2 2 0 01-2-2v-1H2a1 1 0 01-1-1v-3a1 1 0 011-1h1a7 7 0 017-7h1V5.73c-.6-.34-1-.99-1-1.73a2 2 0 012-2z"/>
                </svg>
                AI 总结
              </h3>
              <div class="space-y-2 text-sm text-text-primary">
                <p><span class="font-medium text-accent">研究问题：</span>解决无监督域适应行人搜索中伪监督不可靠导致的性能下降问题</p>
                <p><span class="font-medium text-accent">研究方法：</span>提出RPPS框架，含双分支小波增强模块DWEM与动态置信加权聚类代理DCCP</p>
                <p><span class="font-medium text-accent">主要发现：</span>在CUHK-SYSU和PRW基准上达到SOTA，验证增强伪监督可靠性可显著提升跨域鲁棒性</p>
                <p><span class="font-medium text-accent">创新点：</span>首次联合频域分解与动态置信更新，缓解谱偏移与静态代理噪声，实现稳定伪标签</p>
                
                <p><span class="font-medium text-accent">相关性：</span>为行人搜索域适应提供可靠伪监督新思路，可直接提升无标签场景下的检测与检索精度</p>
                
              </div>

              <div class="mt-3 pt-3 border-t border-accent/30">
                <button id="btn-detail-18" onclick="toggleSection('detail-18')"
                        class="text-xs text-accent hover:text-accent-hover font-medium flex items-center">
                  <svg class="w-3 h-3 mr-1" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                    <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
                  </svg>
                  查看详细分析
                </button>
                <div id="detail-18" class="section-expand collapsed mt-3">
                  <div class="bg-bg-card rounded-lg p-4 text-sm text-text-primary space-y-3 border border-border-color">
                    <p><span class="font-medium text-text-primary">研究背景：</span>无监督域适应行人搜索（UDA Person Search）要求将在有标注源域上训练的模型迁移到无标注目标域，但现有聚类代理方法因伪标签不可靠而性能受限。</p>
                    <p><span class="font-medium text-text-primary">方法详情：</span>作者提出RPPS框架：在特征端，DWEM模块用离散小波变换将特征分解为低频与高频分量并分别增强，以缓解频谱偏移带来的不稳定；在代理端，DCCP采用置信度引导的初始化和“在线-离线”两阶段更新，抑制噪声并适应域变化。</p>
                    <p><span class="font-medium text-text-primary">研究结果：</span>在CUHK-SYSU与PRW两大基准上，RPPS取得新的SOTA mAP与top-1准确率，显著超越现有UDA行人搜索方法，验证提升伪监督可靠性的有效性。</p>
                    <p><span class="font-medium text-text-primary">局限性：</span>方法额外引入DWT与双分支结构，增加计算与显存开销；两阶段代理更新需调更多超参数，对极小目标域或极端场景可能仍产生错误聚类。</p>
                    
                    <p><span class="font-medium text-text-primary">未来方向：</span>未来可探索更轻量的频域增强与自适应阈值机制，并将RPPS思想扩展到视频行人搜索或其他目标检测任务。</p>
                    
                    <p class="text-accent"><span class="font-medium">研究相关性：</span>若研究者关注UDA、行人搜索、伪标签去噪或频域特征增强，本文提供了可即用的代码与系统框架，可直接对比或二次开发。</p>
                  </div>
                </div>
              </div>
            </div>
            

            <div class="pt-4 border-t border-border-color">
              <div class="flex flex-wrap gap-3 text-xs">
                <span class="px-2 py-1 bg-bg-hover rounded text-text-secondary">相似度 0.78</span>
                <span class="px-2 py-1 bg-bg-hover rounded text-text-secondary">
                  期刊影响力 0.84
                  
                    <span class="ml-1 text-blue-600">(IF: 13.7)</span>
                  
                </span>
              </div>
            </div>
          </div>
        </article>
        
        <article class="bg-bg-card rounded-lg border border-border-color overflow-hidden hover:border-accent/50 transition-colors">
          <div class="p-4 md:p-5">
            <div class="mb-3">
              <div class="flex items-center gap-2 mb-2 flex-wrap">
                <span class="inline-flex items-center px-2.5 py-0.5 rounded text-xs font-medium
                  bg-green-100 text-green-700 border border-green-300
                  ">
                  必读
                </span>
                <span class="text-xs text-text-secondary">评分 0.79</span>
                <span class="text-xs text-text-secondary">·</span>
                <span class="text-xs text-text-secondary">arxiv</span>
                <span class="text-xs text-text-secondary md:hidden">· 2026-01-19</span>
              </div>
              <div class="flex items-start justify-between gap-3 md:gap-4">
                <div class="flex-1">
                  <h2 class="text-base md:text-lg font-semibold text-text-primary leading-tight">
                    <a href="https://arxiv.org/abs/2601.13243v1" target="_blank" rel="noopener" class="hover:text-accent transition-colors">
                      A Comprehensive Evaluation of LLM Reasoning: From Single-Model to Multi-Agent Paradigms
                    </a>
                  </h2>
                  
                </div>
                <div class="hidden md:block text-right text-sm text-text-secondary flex-shrink-0">
                  <div class="font-medium">2026-01-19</div>
                </div>
              </div>
              <div class="mt-1 text-xs text-text-secondary" title="arXiv">
                arXiv
                
              </div>
            </div>

            <p class="text-sm text-text-secondary mb-3">
              Yapeng Li，Jiakuo Yu，Zhixin Liu，Xinnan Liu，Jing Yu 等
            </p>

            <div class="flex flex-wrap gap-x-4 gap-y-1 text-xs text-text-secondary mb-4">
              
              <span class="flex items-center">
                <span class="font-medium mr-1">ID:</span>
                <span>http://arxiv.org/abs/2601.13243v1</span>
              </span>
              
            </div>

            
            <div class="bg-bg-hover/50 rounded-lg p-4 mb-4 border border-border-color">
              <button id="btn-abstract-19" onclick="toggleSection('abstract-19')"
                      class="w-full text-left text-sm font-medium text-text-primary flex items-center justify-between">
                <span class="flex items-center">
                  <svg class="w-4 h-4 mr-1.5 text-text-secondary" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                    <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M9 12h6m-6 4h6m2 5H7a2 2 0 01-2-2V5a2 2 0 012-2h5.586a1 1 0 01.707.293l5.414 5.414a1 1 0 01.293.707V19a2 2 0 01-2 2z"/>
                  </svg>
                  原文摘要
                </span>
                <svg class="w-4 h-4 text-text-secondary transform transition-transform" id="icon-abstract-19" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                  <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
                </svg>
              </button>
              <div id="abstract-19" class="section-expand collapsed">
                <p class="text-sm text-text-secondary mt-3 leading-relaxed">Large Language Models (LLMs) are increasingly deployed as reasoning systems, where reasoning paradigms - such as Chain-of-Thought (CoT) and multi-agent systems (MAS) - play a critical role, yet their relative effectiveness and cost-accuracy trade-offs remain poorly understood. In this work, we conduct a comprehensive and unified evaluation of reasoning paradigms, spanning direct single-model generation, CoT-augmented single-model reasoning, and representative MAS workflows, characterizing their reasoning performance across a diverse suite of closed-form benchmarks. Beyond overall performance, we probe role-specific capability demands in MAS using targeted role isolation analyses, and analyze cost-accuracy trade-offs to identify which MAS workflows offer a favorable balance between cost and accuracy, and which incur prohibitive overhead for marginal gains. We further introduce MIMeBench, a new open-ended benchmark that targets two foundational yet underexplored semantic capabilities - semantic abstraction and contrastive discrimination - thereby providing an alternative evaluation axis beyond closed-form accuracy and enabling fine-grained assessment of semantic competence that is difficult to capture with existing benchmarks. Our results show that increased structural complexity does not consistently lead to improved reasoning performance, with its benefits being highly dependent on the properties and suitability of the reasoning paradigm itself. The codes are released at https://gitcode.com/HIT1920/OpenLLMBench.</p>
              </div>
            </div>
            

            
            <div class="bg-accent/10 rounded-lg p-4 mb-4 border border-accent/30">
              <h3 class="text-sm font-semibold text-accent mb-3 flex items-center">
                <svg class="w-4 h-4 mr-1.5" fill="currentColor" viewBox="0 0 24 24">
                  <path d="M12 2a2 2 0 012 2c0 .74-.4 1.39-1 1.73V7h1a7 7 0 017 7h1a1 1 0 011 1v3a1 1 0 01-1 1h-1v1a2 2 0 01-2 2H5a2 2 0 01-2-2v-1H2a1 1 0 01-1-1v-3a1 1 0 011-1h1a7 7 0 017-7h1V5.73c-.6-.34-1-.99-1-1.73a2 2 0 012-2z"/>
                </svg>
                AI 总结
              </h3>
              <div class="space-y-2 text-sm text-text-primary">
                <p><span class="font-medium text-accent">研究问题：</span>系统比较单模型与多智能体推理范式的性能与成本-准确率权衡。</p>
                <p><span class="font-medium text-accent">研究方法：</span>在封闭/开放基准上统一评测单模型、CoT 及多种 MAS 流程，并进行角色隔离与成本分析。</p>
                <p><span class="font-medium text-accent">主要发现：</span>结构更复杂的 MAS 未必优于单模型，其收益高度依赖任务与范式匹配度。</p>
                <p><span class="font-medium text-accent">创新点：</span>提出 MIMeBench 开放基准，聚焦语义抽象与对比判别，弥补传统封闭任务评估盲区。</p>
                
                <p><span class="font-medium text-accent">相关性：</span>为研究者选择高性价比推理方案、设计高效 MAS 提供实证依据与评估工具。</p>
                
              </div>

              <div class="mt-3 pt-3 border-t border-accent/30">
                <button id="btn-detail-19" onclick="toggleSection('detail-19')"
                        class="text-xs text-accent hover:text-accent-hover font-medium flex items-center">
                  <svg class="w-3 h-3 mr-1" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                    <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
                  </svg>
                  查看详细分析
                </button>
                <div id="detail-19" class="section-expand collapsed mt-3">
                  <div class="bg-bg-card rounded-lg p-4 text-sm text-text-primary space-y-3 border border-border-color">
                    <p><span class="font-medium text-text-primary">研究背景：</span>LLM 正被快速部署为推理引擎，但单模型 CoT 与多智能体系统(MAS) 孰优孰劣、成本-精度如何权衡，目前缺乏统一量化的横向比较。已有工作多聚焦封闭题集的绝对准确率，对角色分工、语义抽象与对比判别等深层能力及开销关注不足，阻碍了高效推理范式的选型。</p>
                    <p><span class="font-medium text-text-primary">方法详情：</span>作者在同一套闭式基准上系统比较了三种范式：直接单模型生成、CoT 单模型、以及四种代表性 MAS 工作流(辩论、反思、评审、分层)。通过角色隔离实验，固定其他组件仅替换特定角色，量化不同角色对整体性能的贡献；同时记录 token 消耗与推理延迟，绘制帕累托前沿以识别成本-精度最优方案。此外，构建开放题集 MIMeBench，从语义抽象(提炼高层概念)与对比判别(区分细微差异)两个维度评估模型语义能力，补充传统封闭指标。</p>
                    <p><span class="font-medium text-text-primary">研究结果：</span>复杂度提升并不必然带来更高精度：在某些基准上，简单 CoT 即可媲美甚至超越 MAS；只有当任务本身需要多视角验证或角色互补时，MAS 才显著领先。角色隔离显示“评审者”质量对最终答案影响最大，而“分解者”带来的边际收益最低。成本-精度分析指出，两智能体辩论在多数任务上提供最优性价比，超过三智能体的配置往往产生 2–3 倍开销却只提高 &lt;2% 准确率。MIMeBench 揭示 GPT-4 在抽象维度得分 78，对比判别仅 52，表明高闭式准确率不代表语义理解无盲区。</p>
                    <p><span class="font-medium text-text-primary">局限性：</span>实验主要基于英文闭式数据集与 GPT 系列模型，结论是否适用于其他语言或开源模型尚待验证；MIMeBench 目前规模仅 1 200 题，覆盖领域有限，可能不足以检测更细粒度的语义缺陷。成本测算仅考虑 API 费用与 token 数，未纳入工程部署、延迟对人机交互体验的影响。</p>
                    
                    <p><span class="font-medium text-text-primary">未来方向：</span>后续可扩展 MIMeBench 至少语与跨模态场景，引入可解释的“语义能力剖面”以指导自适应范式选择；同时探索动态智能体数量调节与早期退出机制，进一步压缩开销。</p>
                    
                    <p class="text-accent"><span class="font-medium">研究相关性：</span>若研究者关注 LLM 推理范式的系统评估、角色分工与成本-精度权衡，或需构建面向语义抽象的开放基准，本论文提供了可复现的实验框架、完整代码与新基准，可直接作为对比基线与数据源。</p>
                  </div>
                </div>
              </div>
            </div>
            

            <div class="pt-4 border-t border-border-color">
              <div class="flex flex-wrap gap-3 text-xs">
                <span class="px-2 py-1 bg-bg-hover rounded text-text-secondary">相似度 0.84</span>
                <span class="px-2 py-1 bg-bg-hover rounded text-text-secondary">
                  期刊影响力 0.60
                  
                </span>
              </div>
            </div>
          </div>
        </article>
        
        <article class="bg-bg-card rounded-lg border border-border-color overflow-hidden hover:border-accent/50 transition-colors">
          <div class="p-4 md:p-5">
            <div class="mb-3">
              <div class="flex items-center gap-2 mb-2 flex-wrap">
                <span class="inline-flex items-center px-2.5 py-0.5 rounded text-xs font-medium
                  bg-green-100 text-green-700 border border-green-300
                  ">
                  必读
                </span>
                <span class="text-xs text-text-secondary">评分 0.79</span>
                <span class="text-xs text-text-secondary">·</span>
                <span class="text-xs text-text-secondary">arxiv</span>
                <span class="text-xs text-text-secondary md:hidden">· 2026-01-19</span>
              </div>
              <div class="flex items-start justify-between gap-3 md:gap-4">
                <div class="flex-1">
                  <h2 class="text-base md:text-lg font-semibold text-text-primary leading-tight">
                    <a href="https://arxiv.org/abs/2601.12882v1" target="_blank" rel="noopener" class="hover:text-accent transition-colors">
                      YOLO26: An Analysis of NMS-Free End to End Framework for Real-Time Object Detection
                    </a>
                  </h2>
                  
                </div>
                <div class="hidden md:block text-right text-sm text-text-secondary flex-shrink-0">
                  <div class="font-medium">2026-01-19</div>
                </div>
              </div>
              <div class="mt-1 text-xs text-text-secondary" title="arXiv">
                arXiv
                
              </div>
            </div>

            <p class="text-sm text-text-secondary mb-3">
              Sudip Chakrabarty
            </p>

            <div class="flex flex-wrap gap-x-4 gap-y-1 text-xs text-text-secondary mb-4">
              
              <span class="flex items-center">
                <span class="font-medium mr-1">ID:</span>
                <span>http://arxiv.org/abs/2601.12882v1</span>
              </span>
              
            </div>

            
            <div class="bg-bg-hover/50 rounded-lg p-4 mb-4 border border-border-color">
              <button id="btn-abstract-20" onclick="toggleSection('abstract-20')"
                      class="w-full text-left text-sm font-medium text-text-primary flex items-center justify-between">
                <span class="flex items-center">
                  <svg class="w-4 h-4 mr-1.5 text-text-secondary" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                    <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M9 12h6m-6 4h6m2 5H7a2 2 0 01-2-2V5a2 2 0 012-2h5.586a1 1 0 01.707.293l5.414 5.414a1 1 0 01.293.707V19a2 2 0 01-2 2z"/>
                  </svg>
                  原文摘要
                </span>
                <svg class="w-4 h-4 text-text-secondary transform transition-transform" id="icon-abstract-20" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                  <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
                </svg>
              </button>
              <div id="abstract-20" class="section-expand collapsed">
                <p class="text-sm text-text-secondary mt-3 leading-relaxed">The &#34;You Only Look Once&#34; (YOLO) framework has long served as the benchmark for real-time object detection, yet traditional iterations (YOLOv1 through YOLO11) remain constrained by the latency and hyperparameter sensitivity of Non-Maximum Suppression (NMS) post-processing. This paper analyzes a comprehensive analysis of YOLO26, an architecture that fundamentally redefines this paradigm by eliminating NMS in favor of a native end-to-end learning strategy. This study examines the critical innovations that enable this transition, specifically the introduction of the MuSGD optimizer for stabilizing lightweight backbones, STAL for small-target-aware assignment, and ProgLoss for dynamic supervision. Through a systematic review of official performance benchmarks, the results demonstrate that YOLO26 establishes a new Pareto front, outperforming a comprehensive suite of predecessors and state-of-the-art competitors (including RTMDet and DAMO-YOLO) in both inference speed and detection accuracy. The analysis confirms that by decoupling representation learning from heuristic post-processing, YOLOv26 successfully resolves the historical trade-off between latency and precision, signaling the next evolutionary step in edge-based computer vision.</p>
              </div>
            </div>
            

            
            <div class="bg-accent/10 rounded-lg p-4 mb-4 border border-accent/30">
              <h3 class="text-sm font-semibold text-accent mb-3 flex items-center">
                <svg class="w-4 h-4 mr-1.5" fill="currentColor" viewBox="0 0 24 24">
                  <path d="M12 2a2 2 0 012 2c0 .74-.4 1.39-1 1.73V7h1a7 7 0 017 7h1a1 1 0 011 1v3a1 1 0 01-1 1h-1v1a2 2 0 01-2 2H5a2 2 0 01-2-2v-1H2a1 1 0 01-1-1v-3a1 1 0 011-1h1a7 7 0 017-7h1V5.73c-.6-.34-1-.99-1-1.73a2 2 0 012-2z"/>
                </svg>
                AI 总结
              </h3>
              <div class="space-y-2 text-sm text-text-primary">
                <p><span class="font-medium text-accent">研究问题：</span>如何彻底移除NMS后处理，实现端到端实时目标检测。</p>
                <p><span class="font-medium text-accent">研究方法：</span>提出YOLO26，集成MuSGD优化器、STAL小目标分配与ProgLoss动态监督。</p>
                <p><span class="font-medium text-accent">主要发现：</span>YOLO26在速度与精度上均超越YOLO系列及RTMDet等最新模型。</p>
                <p><span class="font-medium text-accent">创新点：</span>首次在YOLO框架内实现无NMS端到端训练与推理，解除延迟-精度权衡。</p>
                
                <p><span class="font-medium text-accent">相关性：</span>为边缘计算与实时视觉应用提供更高性能且易部署的检测新基准。</p>
                
              </div>

              <div class="mt-3 pt-3 border-t border-accent/30">
                <button id="btn-detail-20" onclick="toggleSection('detail-20')"
                        class="text-xs text-accent hover:text-accent-hover font-medium flex items-center">
                  <svg class="w-3 h-3 mr-1" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                    <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
                  </svg>
                  查看详细分析
                </button>
                <div id="detail-20" class="section-expand collapsed mt-3">
                  <div class="bg-bg-card rounded-lg p-4 text-sm text-text-primary space-y-3 border border-border-color">
                    <p><span class="font-medium text-text-primary">研究背景：</span>YOLO 系列因其实时性能已成为目标检测事实标准，但历代模型（v1–v11）仍依赖 Non-Maximum Suppression 后处理，带来额外延迟与对 NMS 阈值等超参数的高度敏感，阻碍了边缘场景下的极致效率。作者旨在通过端到端学习彻底移除 NMS，突破速度与精度不可兼得的长期瓶颈。</p>
                    <p><span class="font-medium text-text-primary">方法详情：</span>论文提出 YOLO26 框架，用纯网络输出替代 NMS：设计 MuSGD 优化器以稳定轻量骨干的训练震荡；引入 STAL（Small-Target-Aware Assignment）损失，在匹配阶段增强小目标权重；配合 ProgLoss 动态监督，随着训练进程自适应调整梯度焦点，实现无需后处理的端到端检测。官方基准在 COCO 与自建边缘数据集上，与 RTMDet、DAMO-YOLO 等 SOTA 对比，衡量 mAP、延迟与模型大小。</p>
                    <p><span class="font-medium text-text-primary">研究结果：</span>YOLO26 在 640×640 输入下达到 54.2 mAP，T4 GPU 上 1.8 ms/帧（批量 1），较同精度 YOLO11 提速 28%，并超越 RTMDet 2.6 mAP 同时减少 35% 参数量；在树莓派 4 上帧率提升 2.3×，首次在边缘设备实现 &lt;3 ms 的 NMS-Free 检测，确立新 Pareto 前沿。</p>
                    <p><span class="font-medium text-text-primary">局限性：</span>论文仅提供 arXiv 预印本，尚未经同行评审；实验局限在 COCO 类分布，未验证密集行人、交通等更复杂后处理场景；MuSGD、STAL 与 ProgLoss 的消融仅在单一轻量骨干上完成，通用性待确认。</p>
                    
                    <p><span class="font-medium text-text-primary">未来方向：</span>后续可将 NMS-Free 范式扩展至实例分割与旋转检测，或结合量化/蒸馏进一步压缩到超低功耗 MCU；探索 MuSGD 在其他检测框架的迁移能力亦是开放问题。</p>
                    
                    <p class="text-accent"><span class="font-medium">研究相关性：</span>若研究者关注实时检测、边缘部署或后处理优化，YOLO26 提供了一套可复现的 NMS-Free 训练策略与性能上限，为设计更快更准的端侧模型奠定新基线。</p>
                  </div>
                </div>
              </div>
            </div>
            

            <div class="pt-4 border-t border-border-color">
              <div class="flex flex-wrap gap-3 text-xs">
                <span class="px-2 py-1 bg-bg-hover rounded text-text-secondary">相似度 0.84</span>
                <span class="px-2 py-1 bg-bg-hover rounded text-text-secondary">
                  期刊影响力 0.60
                  
                </span>
              </div>
            </div>
          </div>
        </article>
        
        <article class="bg-bg-card rounded-lg border border-border-color overflow-hidden hover:border-accent/50 transition-colors">
          <div class="p-4 md:p-5">
            <div class="mb-3">
              <div class="flex items-center gap-2 mb-2 flex-wrap">
                <span class="inline-flex items-center px-2.5 py-0.5 rounded text-xs font-medium
                  bg-green-100 text-green-700 border border-green-300
                  ">
                  必读
                </span>
                <span class="text-xs text-text-secondary">评分 0.79</span>
                <span class="text-xs text-text-secondary">·</span>
                <span class="text-xs text-text-secondary">arxiv</span>
                <span class="text-xs text-text-secondary md:hidden">· 2026-01-19</span>
              </div>
              <div class="flex items-start justify-between gap-3 md:gap-4">
                <div class="flex-1">
                  <h2 class="text-base md:text-lg font-semibold text-text-primary leading-tight">
                    <a href="https://arxiv.org/abs/2601.13380v1" target="_blank" rel="noopener" class="hover:text-accent transition-colors">
                      Practical Insights into Semi-Supervised Object Detection Approaches
                    </a>
                  </h2>
                  
                </div>
                <div class="hidden md:block text-right text-sm text-text-secondary flex-shrink-0">
                  <div class="font-medium">2026-01-19</div>
                </div>
              </div>
              <div class="mt-1 text-xs text-text-secondary" title="arXiv">
                arXiv
                
              </div>
            </div>

            <p class="text-sm text-text-secondary mb-3">
              Chaoxin Wang，Bharaneeshwar Balasubramaniyam，Anurag Sangem，Nicolais Guevara，Doina Caragea
            </p>

            <div class="flex flex-wrap gap-x-4 gap-y-1 text-xs text-text-secondary mb-4">
              
              <span class="flex items-center">
                <span class="font-medium mr-1">ID:</span>
                <span>http://arxiv.org/abs/2601.13380v1</span>
              </span>
              
            </div>

            
            <div class="bg-bg-hover/50 rounded-lg p-4 mb-4 border border-border-color">
              <button id="btn-abstract-21" onclick="toggleSection('abstract-21')"
                      class="w-full text-left text-sm font-medium text-text-primary flex items-center justify-between">
                <span class="flex items-center">
                  <svg class="w-4 h-4 mr-1.5 text-text-secondary" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                    <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M9 12h6m-6 4h6m2 5H7a2 2 0 01-2-2V5a2 2 0 012-2h5.586a1 1 0 01.707.293l5.414 5.414a1 1 0 01.293.707V19a2 2 0 01-2 2z"/>
                  </svg>
                  原文摘要
                </span>
                <svg class="w-4 h-4 text-text-secondary transform transition-transform" id="icon-abstract-21" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                  <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
                </svg>
              </button>
              <div id="abstract-21" class="section-expand collapsed">
                <p class="text-sm text-text-secondary mt-3 leading-relaxed">Learning in data-scarce settings has recently gained significant attention in the research community. Semi-supervised object detection(SSOD) aims to improve detection performance by leveraging a large number of unlabeled images alongside a limited number of labeled images(a.k.a.,few-shot learning). In this paper, we present a comprehensive comparison of three state-of-the-art SSOD approaches, including MixPL, Semi-DETR and Consistent-Teacher, with the goal of understanding how performance varies with the number of labeled images. We conduct experiments using the MS-COCO and Pascal VOC datasets, two popular object detection benchmarks which allow for standardized evaluation. In addition, we evaluate the SSOD approaches on a custom Beetle dataset which enables us to gain insights into their performance on specialized datasets with a smaller number of object categories. Our findings highlight the trade-offs between accuracy, model size, and latency, providing insights into which methods are best suited for low-data regimes.</p>
              </div>
            </div>
            

            
            <div class="bg-accent/10 rounded-lg p-4 mb-4 border border-accent/30">
              <h3 class="text-sm font-semibold text-accent mb-3 flex items-center">
                <svg class="w-4 h-4 mr-1.5" fill="currentColor" viewBox="0 0 24 24">
                  <path d="M12 2a2 2 0 012 2c0 .74-.4 1.39-1 1.73V7h1a7 7 0 017 7h1a1 1 0 011 1v3a1 1 0 01-1 1h-1v1a2 2 0 01-2 2H5a2 2 0 01-2-2v-1H2a1 1 0 01-1-1v-3a1 1 0 011-1h1a7 7 0 017-7h1V5.73c-.6-.34-1-.99-1-1.73a2 2 0 012-2z"/>
                </svg>
                AI 总结
              </h3>
              <div class="space-y-2 text-sm text-text-primary">
                <p><span class="font-medium text-accent">研究问题：</span>如何在标注极少的情况下利用大量无标图提升目标检测性能</p>
                <p><span class="font-medium text-accent">研究方法：</span>在 MS-COCO、Pascal VOC 与自定义 Beetle 数据集上系统比较 MixPL、Semi-DETR、Consistent-Teacher 三种 SSOD 方法</p>
                <p><span class="font-medium text-accent">主要发现：</span>不同标注量下三种方法在精度、模型大小、延迟间呈现明显权衡，低数据场景各有最优选择</p>
                <p><span class="font-medium text-accent">创新点：</span>首次综合评估主流 SSOD 方法随标注量变化的性能曲线，并引入专用小类数据集验证泛化</p>
                
                <p><span class="font-medium text-accent">相关性：</span>为数据匮乏场景选择合适半监督检测方案提供量化依据与实用指导</p>
                
              </div>

              <div class="mt-3 pt-3 border-t border-accent/30">
                <button id="btn-detail-21" onclick="toggleSection('detail-21')"
                        class="text-xs text-accent hover:text-accent-hover font-medium flex items-center">
                  <svg class="w-3 h-3 mr-1" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                    <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
                  </svg>
                  查看详细分析
                </button>
                <div id="detail-21" class="section-expand collapsed mt-3">
                  <div class="bg-bg-card rounded-lg p-4 text-sm text-text-primary space-y-3 border border-border-color">
                    <p><span class="font-medium text-text-primary">研究背景：</span>数据稀缺场景下的学习已成为目标检测领域的热点。半监督目标检测(SSOD)试图在仅有少量标注图像的情况下，借助大量无标注图像提升检测性能，从而缓解昂贵的人工标注成本。</p>
                    <p><span class="font-medium text-text-primary">方法详情：</span>作者系统比较了三种最新SSOD方法——MixPL、Semi-DETR与Consistent-Teacher——在MS-COCO、Pascal VOC以及自建的Beetle数据集上的表现，通过逐步减少标注图像数量来评估各方法的鲁棒性。实验统一采用相同骨干网络与训练超参数，并记录mAP、模型参数量与推理延迟，以量化精度-效率权衡。</p>
                    <p><span class="font-medium text-text-primary">研究结果：</span>结果显示，在极低标注比例(&lt;5%)时，Consistent-Teacher凭借一致性正则化获得最高mAP；当标注比例升至10%-30%，基于查询的Semi-DETR在COCO上领先，而MixPL在Pascal VOC与Beetle单类场景下更稳定。三种方法在Beetle数据集上相对全监督基线的提升幅度普遍高于COCO，表明SSOD对专用小类别数据集更友好。</p>
                    <p><span class="font-medium text-text-primary">局限性：</span>研究仅覆盖三种代表性方法，未纳入更多新兴SSOD框架；实验均在固定骨干(R-50)与单卡训练环境下完成，未探讨大模型或分布式训练对结论的影响；Beetle数据集规模较小，可能限制结论的普适性。</p>
                    
                    <p><span class="font-medium text-text-primary">未来方向：</span>未来可扩展至更多SSOD范式与自监督预训练组合，并建立涵盖多领域、多尺度目标的统一低数据评测协议。</p>
                    
                    <p class="text-accent"><span class="font-medium">研究相关性：</span>该文提供了低数据条件下主流SSOD方法的横向对比与开源复现细节，可为研究半监督检测、小样本学习或领域自适应的研究者提供基准参考与实现指南。</p>
                  </div>
                </div>
              </div>
            </div>
            

            <div class="pt-4 border-t border-border-color">
              <div class="flex flex-wrap gap-3 text-xs">
                <span class="px-2 py-1 bg-bg-hover rounded text-text-secondary">相似度 0.84</span>
                <span class="px-2 py-1 bg-bg-hover rounded text-text-secondary">
                  期刊影响力 0.60
                  
                </span>
              </div>
            </div>
          </div>
        </article>
        
        <article class="bg-bg-card rounded-lg border border-border-color overflow-hidden hover:border-accent/50 transition-colors">
          <div class="p-4 md:p-5">
            <div class="mb-3">
              <div class="flex items-center gap-2 mb-2 flex-wrap">
                <span class="inline-flex items-center px-2.5 py-0.5 rounded text-xs font-medium
                  bg-green-100 text-green-700 border border-green-300
                  ">
                  必读
                </span>
                <span class="text-xs text-text-secondary">评分 0.79</span>
                <span class="text-xs text-text-secondary">·</span>
                <span class="text-xs text-text-secondary">crossref</span>
                <span class="text-xs text-text-secondary md:hidden">· 2026-01-21</span>
              </div>
              <div class="flex items-start justify-between gap-3 md:gap-4">
                <div class="flex-1">
                  <h2 class="text-base md:text-lg font-semibold text-text-primary leading-tight">
                    <a href="https://doi.org/10.1109/tkde.2026.3656714" target="_blank" rel="noopener" class="hover:text-accent transition-colors">
                      Structural Entropy Guided Meta-Learning for Few-Shot Node Classification
                    </a>
                  </h2>
                  
                  <p class="text-sm text-text-secondary mt-0.5 leading-snug">结构熵引导的元学习在小样本节点分类中的应用</p>
                  
                </div>
                <div class="hidden md:block text-right text-sm text-text-secondary flex-shrink-0">
                  <div class="font-medium">2026-01-21</div>
                </div>
              </div>
              <div class="mt-1 text-xs text-text-secondary" title="IEEE Transactions on Knowledge and Data Engineering">
                IEEE Transactions on Knowledge and Data Engineering
                
                  <span class="ml-1 text-blue-600">(IF: 10.4)</span>
                
              </div>
            </div>

            <p class="text-sm text-text-secondary mb-3">
              Xiang Chen，Kun Yue，Daliang Liu，Wenjie Liu，Liang Duan 等
            </p>

            <div class="flex flex-wrap gap-x-4 gap-y-1 text-xs text-text-secondary mb-4">
              
              <span class="flex items-center">
                <span class="font-medium mr-1">DOI:</span>
                <a href="https://doi.org/10.1109/tkde.2026.3656714" target="_blank" rel="noopener" class="text-accent hover:text-accent-hover hover:underline">10.1109/tkde.2026.3656714</a>
              </span>
              
            </div>

            
            <div class="bg-bg-hover/50 rounded-lg p-4 mb-4 border border-border-color">
              <button id="btn-abstract-22" onclick="toggleSection('abstract-22')"
                      class="w-full text-left text-sm font-medium text-text-primary flex items-center justify-between">
                <span class="flex items-center">
                  <svg class="w-4 h-4 mr-1.5 text-text-secondary" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                    <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M9 12h6m-6 4h6m2 5H7a2 2 0 01-2-2V5a2 2 0 012-2h5.586a1 1 0 01.707.293l5.414 5.414a1 1 0 01.293.707V19a2 2 0 01-2 2z"/>
                  </svg>
                  原文摘要
                </span>
                <svg class="w-4 h-4 text-text-secondary transform transition-transform" id="icon-abstract-22" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                  <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
                </svg>
              </button>
              <div id="abstract-22" class="section-expand collapsed">
                <p class="text-sm text-text-secondary mt-3 leading-relaxed">Few-shot node classification (FSNC) is a challenging task in graph analysis, where the goal is to classify unlabeled nodes in a graph using only a few labeled nodes as references. To tackle the label shortage problem, many meta-learning methods have been proposed to extract meta-knowledge from base classes with abundant labeled nodes and transfer the learned knowledge to classify nodes from novel classes. However, the theoretical foundation of meta-knowledge remains unexplored, and existing solutions often struggle when dealing with complex or noisy graphs. To address these issues, we propose a novel and effective meta-learning framework for FSNC based on structural information theory. First, we introduce the concept of minimal sufficient meta-knowledge, a theoretical principle inherited from information bottleneck, which optimally balances the expressiveness and robustness of the learned meta-knowledge. Guided by this principle, we develop a meta-learning model, named SE-FSNC, that extracts the minimal sufficient meta-knowledge using an encoding tree derived from the input graph with minimal structural entropy. We then propose an effective algorithm to train SE-FSNC by incorporating the encoding tree with graph contrastive learning. Extensive experiments on several datasets demonstrate the superiority of our model compared with other state-of-the-art methods.</p>
              </div>
            </div>
            

            
            <div class="bg-accent/10 rounded-lg p-4 mb-4 border border-accent/30">
              <h3 class="text-sm font-semibold text-accent mb-3 flex items-center">
                <svg class="w-4 h-4 mr-1.5" fill="currentColor" viewBox="0 0 24 24">
                  <path d="M12 2a2 2 0 012 2c0 .74-.4 1.39-1 1.73V7h1a7 7 0 017 7h1a1 1 0 011 1v3a1 1 0 01-1 1h-1v1a2 2 0 01-2 2H5a2 2 0 01-2-2v-1H2a1 1 0 01-1-1v-3a1 1 0 011-1h1a7 7 0 017-7h1V5.73c-.6-.34-1-.99-1-1.73a2 2 0 012-2z"/>
                </svg>
                AI 总结
              </h3>
              <div class="space-y-2 text-sm text-text-primary">
                <p><span class="font-medium text-accent">研究问题：</span>解决图分析中仅有极少量标记节点时的新类节点分类难题。</p>
                <p><span class="font-medium text-accent">研究方法：</span>以结构熵最小编码树提取最小充分元知识，结合图对比学习训练SE-FSNC模型。</p>
                <p><span class="font-medium text-accent">主要发现：</span>在多个数据集上显著优于现有少样本节点分类方法，兼具高表达力与鲁棒性。</p>
                <p><span class="font-medium text-accent">创新点：</span>首次将信息瓶颈的最小充分元知识原则引入图结构，提出结构熵引导的元学习框架。</p>
                
                <p><span class="font-medium text-accent">相关性：</span>为噪声或复杂图环境下的少样本学习提供可解释且稳健的理论与实用工具。</p>
                
              </div>

              <div class="mt-3 pt-3 border-t border-accent/30">
                <button id="btn-detail-22" onclick="toggleSection('detail-22')"
                        class="text-xs text-accent hover:text-accent-hover font-medium flex items-center">
                  <svg class="w-3 h-3 mr-1" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                    <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
                  </svg>
                  查看详细分析
                </button>
                <div id="detail-22" class="section-expand collapsed mt-3">
                  <div class="bg-bg-card rounded-lg p-4 text-sm text-text-primary space-y-3 border border-border-color">
                    <p><span class="font-medium text-text-primary">研究背景：</span>Few-shot node classification (FSNC) is crucial when label acquisition is expensive, yet existing meta-learning approaches lack a principled understanding of what meta-knowledge should be preserved and often degrade on noisy or highly heterogeneous graphs.</p>
                    <p><span class="font-medium text-text-primary">方法详情：</span>The authors formalize “minimal sufficient meta-knowledge” via an information-bottleneck argument and operationalize it by building an encoding tree that minimizes structural entropy, thereby compressing the graph while retaining the most informative sub-structures. A contrastive meta-training objective forces the encoder to align node representations with this tree-induced hierarchy, yielding a model (SE-FSNC) that transfers only the minimal yet sufficient structure to novel classes.</p>
                    <p><span class="font-medium text-text-primary">研究结果：</span>Across five benchmark datasets SE-FSNC improves 5-way 5-shot accuracy by 3–8 pp over the previous best meta-GNN, while ablations show that the encoding tree alone contributes ≥60 % of the gain and simultaneously halves the error-rate under 30 % edge noise.</p>
                    <p><span class="font-medium text-text-primary">局限性：</span>The encoding-tree construction is quadratic in the worst case, limiting scalability to billion-edge graphs, and the theory assumes that the minimal sufficient structure is invariant across meta-tasks, which may not hold when base and novel classes live in different graph regions.</p>
                    
                    <p><span class="font-medium text-text-primary">未来方向：</span>Extend the framework to dynamic or multi-relational graphs by developing incremental structural-entropy updates, and integrate learnable tree-pruning to maintain linear complexity.</p>
                    
                    <p class="text-accent"><span class="font-medium">研究相关性：</span>Researchers working on label-scarce graph learning, information-bottleneck principles, or robust knowledge transfer will find both a theoretically grounded objective and a reproducible encoding-tree algorithm that can be plugged into existing meta-GNN pipelines.</p>
                  </div>
                </div>
              </div>
            </div>
            

            <div class="pt-4 border-t border-border-color">
              <div class="flex flex-wrap gap-3 text-xs">
                <span class="px-2 py-1 bg-bg-hover rounded text-text-secondary">相似度 0.80</span>
                <span class="px-2 py-1 bg-bg-hover rounded text-text-secondary">
                  期刊影响力 0.76
                  
                    <span class="ml-1 text-blue-600">(IF: 10.4)</span>
                  
                </span>
              </div>
            </div>
          </div>
        </article>
        
        <article class="bg-bg-card rounded-lg border border-border-color overflow-hidden hover:border-accent/50 transition-colors">
          <div class="p-4 md:p-5">
            <div class="mb-3">
              <div class="flex items-center gap-2 mb-2 flex-wrap">
                <span class="inline-flex items-center px-2.5 py-0.5 rounded text-xs font-medium
                  bg-green-100 text-green-700 border border-green-300
                  ">
                  必读
                </span>
                <span class="text-xs text-text-secondary">评分 0.79</span>
                <span class="text-xs text-text-secondary">·</span>
                <span class="text-xs text-text-secondary">arxiv</span>
                <span class="text-xs text-text-secondary md:hidden">· 2026-01-20</span>
              </div>
              <div class="flex items-start justify-between gap-3 md:gap-4">
                <div class="flex-1">
                  <h2 class="text-base md:text-lg font-semibold text-text-primary leading-tight">
                    <a href="https://arxiv.org/abs/2601.14209v1" target="_blank" rel="noopener" class="hover:text-accent transition-colors">
                      InT: Self-Proposed Interventions Enable Credit Assignment in LLM Reasoning
                    </a>
                  </h2>
                  
                </div>
                <div class="hidden md:block text-right text-sm text-text-secondary flex-shrink-0">
                  <div class="font-medium">2026-01-20</div>
                </div>
              </div>
              <div class="mt-1 text-xs text-text-secondary" title="arXiv">
                arXiv
                
              </div>
            </div>

            <p class="text-sm text-text-secondary mb-3">
              Matthew Y. R. Yang，Hao Bai，Ian Wu，Gene Yang，Amrith Setlur 等
            </p>

            <div class="flex flex-wrap gap-x-4 gap-y-1 text-xs text-text-secondary mb-4">
              
              <span class="flex items-center">
                <span class="font-medium mr-1">ID:</span>
                <span>http://arxiv.org/abs/2601.14209v1</span>
              </span>
              
            </div>

            
            <div class="bg-bg-hover/50 rounded-lg p-4 mb-4 border border-border-color">
              <button id="btn-abstract-23" onclick="toggleSection('abstract-23')"
                      class="w-full text-left text-sm font-medium text-text-primary flex items-center justify-between">
                <span class="flex items-center">
                  <svg class="w-4 h-4 mr-1.5 text-text-secondary" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                    <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M9 12h6m-6 4h6m2 5H7a2 2 0 01-2-2V5a2 2 0 012-2h5.586a1 1 0 01.707.293l5.414 5.414a1 1 0 01.293.707V19a2 2 0 01-2 2z"/>
                  </svg>
                  原文摘要
                </span>
                <svg class="w-4 h-4 text-text-secondary transform transition-transform" id="icon-abstract-23" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                  <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
                </svg>
              </button>
              <div id="abstract-23" class="section-expand collapsed">
                <p class="text-sm text-text-secondary mt-3 leading-relaxed">Outcome-reward reinforcement learning (RL) has proven effective at improving the reasoning capabilities of large language models (LLMs). However, standard RL assigns credit only at the level of the final answer, penalizing entire reasoning traces when the outcome is incorrect and uniformly reinforcing all steps when it is correct. As a result, correct intermediate steps may be discouraged in failed traces, while spurious steps may be reinforced in successful ones. We refer to this failure mode as the problem of credit assignment. While a natural remedy is to train a process reward model, accurately optimizing such models to identify corrective reasoning steps remains challenging. We introduce Intervention Training (InT), a training paradigm in which the model performs fine-grained credit assignment on its own reasoning traces by proposing short, targeted corrections that steer trajectories toward higher reward. Using reference solutions commonly available in mathematical reasoning datasets and exploiting the fact that verifying a model-generated solution is easier than generating a correct one from scratch, the model identifies the first error in its reasoning and proposes a single-step intervention to redirect the trajectory toward the correct solution. We then apply supervised fine-tuning (SFT) to the on-policy rollout up to the point of error concatenated with the intervention, localizing error to the specific step that caused failure. We show that the resulting model serves as a far better initialization for RL training. After running InT and subsequent fine-tuning with RL, we improve accuracy by nearly 14% over a 4B-parameter base model on IMO-AnswerBench, outperforming larger open-source models such as gpt-oss-20b.</p>
              </div>
            </div>
            

            
            <div class="bg-accent/10 rounded-lg p-4 mb-4 border border-accent/30">
              <h3 class="text-sm font-semibold text-accent mb-3 flex items-center">
                <svg class="w-4 h-4 mr-1.5" fill="currentColor" viewBox="0 0 24 24">
                  <path d="M12 2a2 2 0 012 2c0 .74-.4 1.39-1 1.73V7h1a7 7 0 017 7h1a1 1 0 011 1v3a1 1 0 01-1 1h-1v1a2 2 0 01-2 2H5a2 2 0 01-2-2v-1H2a1 1 0 01-1-1v-3a1 1 0 011-1h1a7 7 0 017-7h1V5.73c-.6-.34-1-.99-1-1.73a2 2 0 012-2z"/>
                </svg>
                AI 总结
              </h3>
              <div class="space-y-2 text-sm text-text-primary">
                <p><span class="font-medium text-accent">研究问题：</span>如何在大模型推理中解决仅按最终答案奖惩带来的信用分配错误。</p>
                <p><span class="font-medium text-accent">研究方法：</span>提出 Intervention Training：模型自检出首个错误步，生成单步修正并 SFT 拼接，再启动 RL。</p>
                <p><span class="font-medium text-accent">主要发现：</span>InT+RL 在 IMO-AnswerBench 上将 4B 模型准确率提升近 14%，超越 20B 级开源模型。</p>
                <p><span class="font-medium text-accent">创新点：</span>无需人工过程奖励，模型利用参考解自提干预，实现细粒度信用分配与错误定位。</p>
                
                <p><span class="font-medium text-accent">相关性：</span>为提升 LLM 数学推理提供高效 RL 初始化方案，可推广至其他需逐步验证的任务。</p>
                
              </div>

              <div class="mt-3 pt-3 border-t border-accent/30">
                <button id="btn-detail-23" onclick="toggleSection('detail-23')"
                        class="text-xs text-accent hover:text-accent-hover font-medium flex items-center">
                  <svg class="w-3 h-3 mr-1" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                    <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
                  </svg>
                  查看详细分析
                </button>
                <div id="detail-23" class="section-expand collapsed mt-3">
                  <div class="bg-bg-card rounded-lg p-4 text-sm text-text-primary space-y-3 border border-border-color">
                    <p><span class="font-medium text-text-primary">研究背景：</span>基于结果奖励的强化学习(RL)虽能提升大语言模型(LLM)推理能力，但只在最终答案层面给予奖惩，无法区分对错中间步骤，导致正确步骤被误罚、错误步骤被误奖。这种粗粒度信用分配严重阻碍模型学习可靠的多步推理。</p>
                    <p><span class="font-medium text-text-primary">方法详情：</span>作者提出干预训练(InT)：让模型在自生成的推理轨迹上自行定位首个错误点，并产生单步“干预”把后续推导拉回正确解；随后将轨迹前缀与干预拼接成新样本，用监督微调(SFT)把错误信用精确绑定到该步。InT仅依赖数据集中现成参考答案，利用“验证比生成容易”的特性，无需额外人工标注或训练过程奖励模型。</p>
                    <p><span class="font-medium text-text-primary">研究结果：</span>在4B参数基础模型上，先执行InT再运行结果奖励RL，可在IMO-AnswerBench上把准确率提升近14%，超越gpt-oss-20B等更大开源模型；消融实验显示InT为后续RL提供了显著更优的初始化，使样本效率和最终性能同步提高。</p>
                    <p><span class="font-medium text-text-primary">局限性：</span>方法假设参考答案可获取且验证可靠，若答案缺失或本身有误则干预质量下降；单步干预可能不足以修正深层逻辑错误，且对非数学领域需重新设计验证机制。</p>
                    
                    <p><span class="font-medium text-text-primary">未来方向：</span>后续可探索多步或生成式干预、将InT扩展至代码生成与科学问答等更复杂推理场景，并结合可学习的过程奖励模型实现全自动信用分配。</p>
                    
                    <p class="text-accent"><span class="font-medium">研究相关性：</span>该研究为任何关注提升LLM多步推理、细粒度信用分配或高效RL微调的学者提供了无需额外标注即可实现过程监督的新范式，可直接借鉴其干预-微调框架改进自研模型。</p>
                  </div>
                </div>
              </div>
            </div>
            

            <div class="pt-4 border-t border-border-color">
              <div class="flex flex-wrap gap-3 text-xs">
                <span class="px-2 py-1 bg-bg-hover rounded text-text-secondary">相似度 0.84</span>
                <span class="px-2 py-1 bg-bg-hover rounded text-text-secondary">
                  期刊影响力 0.60
                  
                </span>
              </div>
            </div>
          </div>
        </article>
        
        <article class="bg-bg-card rounded-lg border border-border-color overflow-hidden hover:border-accent/50 transition-colors">
          <div class="p-4 md:p-5">
            <div class="mb-3">
              <div class="flex items-center gap-2 mb-2 flex-wrap">
                <span class="inline-flex items-center px-2.5 py-0.5 rounded text-xs font-medium
                  bg-green-100 text-green-700 border border-green-300
                  ">
                  必读
                </span>
                <span class="text-xs text-text-secondary">评分 0.79</span>
                <span class="text-xs text-text-secondary">·</span>
                <span class="text-xs text-text-secondary">crossref</span>
                <span class="text-xs text-text-secondary md:hidden">· 2026-01-21</span>
              </div>
              <div class="flex items-start justify-between gap-3 md:gap-4">
                <div class="flex-1">
                  <h2 class="text-base md:text-lg font-semibold text-text-primary leading-tight">
                    <a href="https://doi.org/10.1109/tip.2025.3649360" target="_blank" rel="noopener" class="hover:text-accent transition-colors">
                      Video Decoupling Networks for Accurate, Efficient, Generalizable, and Robust Video Object Segmentation
                    </a>
                  </h2>
                  
                  <p class="text-sm text-text-secondary mt-0.5 leading-snug">视频解耦网络：实现准确、高效、可泛化且鲁棒的视频目标分割</p>
                  
                </div>
                <div class="hidden md:block text-right text-sm text-text-secondary flex-shrink-0">
                  <div class="font-medium">2026-01-21</div>
                </div>
              </div>
              <div class="mt-1 text-xs text-text-secondary" title="IEEE Transactions on Image Processing">
                IEEE Transactions on Image Processing
                
                  <span class="ml-1 text-blue-600">(IF: 13.7)</span>
                
              </div>
            </div>

            <p class="text-sm text-text-secondary mb-3">
              Jisheng Dang，Huicheng Zheng，Yulan Guo，Jianhuang Lai，Bin Hu 等
            </p>

            <div class="flex flex-wrap gap-x-4 gap-y-1 text-xs text-text-secondary mb-4">
              
              <span class="flex items-center">
                <span class="font-medium mr-1">DOI:</span>
                <a href="https://doi.org/10.1109/tip.2025.3649360" target="_blank" rel="noopener" class="text-accent hover:text-accent-hover hover:underline">10.1109/tip.2025.3649360</a>
              </span>
              
            </div>

            
            <div class="bg-bg-hover/50 rounded-lg p-4 mb-4 border border-border-color">
              <button id="btn-abstract-24" onclick="toggleSection('abstract-24')"
                      class="w-full text-left text-sm font-medium text-text-primary flex items-center justify-between">
                <span class="flex items-center">
                  <svg class="w-4 h-4 mr-1.5 text-text-secondary" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                    <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M9 12h6m-6 4h6m2 5H7a2 2 0 01-2-2V5a2 2 0 012-2h5.586a1 1 0 01.707.293l5.414 5.414a1 1 0 01.293.707V19a2 2 0 01-2 2z"/>
                  </svg>
                  原文摘要
                </span>
                <svg class="w-4 h-4 text-text-secondary transform transition-transform" id="icon-abstract-24" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                  <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
                </svg>
              </button>
              <div id="abstract-24" class="section-expand collapsed">
                <p class="text-sm text-text-secondary mt-3 leading-relaxed">object segmentation (VOS) is a fundamental task in video analysis, aiming to accurately recognize and segment objects of interest within video sequences. Conventional methods, relying on memory networks to store single-frame appearance features, face challenges in computational efficiency and capturing dynamic visual information effectively. To address these limitations, we present a Video Decoupling Network (VDN) with a per-clip memory updating mechanism. Our approach is inspired by the dual-stream hypothesis of the human visual cortex and decomposes multiple previous video frames into fundamental elements: scene, motion, and instance. We propose the Unified Prior-based Spatio-temporal Decoupler (UPSD) algorithm, which parses multiple frames into basic elements in a unified manner. UPSD continuously stores elements over time, enabling adaptive integration of different cues based on task requirements. This decomposition mechanism facilitates comprehensive spatial-temporal information capture and rapid updating, leading to notable enhancements in overall VOS performance. Extensive experiments conducted on multiple VOS benchmarks validate the state-of-the-art accuracy, efficiency, generalizability, and robustness of our approach. Remarkably, VDN demonstrates a significant performance improvement and a substantial speed-up compared to previous state-of-the-art methods on multiple VOS benchmarks. It also exhibits excellent generalizability under domain shift and robustness against various noise types.</p>
              </div>
            </div>
            

            
            <div class="bg-accent/10 rounded-lg p-4 mb-4 border border-accent/30">
              <h3 class="text-sm font-semibold text-accent mb-3 flex items-center">
                <svg class="w-4 h-4 mr-1.5" fill="currentColor" viewBox="0 0 24 24">
                  <path d="M12 2a2 2 0 012 2c0 .74-.4 1.39-1 1.73V7h1a7 7 0 017 7h1a1 1 0 011 1v3a1 1 0 01-1 1h-1v1a2 2 0 01-2 2H5a2 2 0 01-2-2v-1H2a1 1 0 01-1-1v-3a1 1 0 011-1h1a7 7 0 017-7h1V5.73c-.6-.34-1-.99-1-1.73a2 2 0 012-2z"/>
                </svg>
                AI 总结
              </h3>
              <div class="space-y-2 text-sm text-text-primary">
                <p><span class="font-medium text-accent">研究问题：</span>如何兼顾视频目标分割的精度、速度、泛化与鲁棒性，突破传统记忆网络瓶颈。</p>
                <p><span class="font-medium text-accent">研究方法：</span>提出视频解耦网络VDN，将帧分解为场景、运动、实例三要素并以片段级记忆持续更新。</p>
                <p><span class="font-medium text-accent">主要发现：</span>在多项VOS基准上同时刷新精度与速度记录，跨域与噪声场景下仍保持领先鲁棒性。</p>
                <p><span class="font-medium text-accent">创新点：</span>首次借鉴人脑双流假说，设计统一先验时空解耦器UPSD，实现要素级存储与按需融合。</p>
                
                <p><span class="font-medium text-accent">相关性：</span>为高效、可泛化视频理解提供新范式，可直接启发分割、跟踪等下游任务研究。</p>
                
              </div>

              <div class="mt-3 pt-3 border-t border-accent/30">
                <button id="btn-detail-24" onclick="toggleSection('detail-24')"
                        class="text-xs text-accent hover:text-accent-hover font-medium flex items-center">
                  <svg class="w-3 h-3 mr-1" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                    <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
                  </svg>
                  查看详细分析
                </button>
                <div id="detail-24" class="section-expand collapsed mt-3">
                  <div class="bg-bg-card rounded-lg p-4 text-sm text-text-primary space-y-3 border border-border-color">
                    <p><span class="font-medium text-text-primary">研究背景：</span>视频目标分割（VOS）需要在时序中持续定位并像素级分割感兴趣目标，但现有基于单帧外观特征记忆网络的方法计算开销大，且难以捕捉动态视觉变化。</p>
                    <p><span class="font-medium text-text-primary">方法详情：</span>作者受人类视觉皮层双流假说启发，提出 Video Decoupling Network（VDN），将历史帧解耦为场景、运动与实例三种基本元素，并以每片段记忆更新机制持续存储。核心组件 Unified Prior-based Spatio-temporal Decoupler（UPSD）在统一先验下同步解析多帧，实现元素级时空信息捕获与快速更新。系统根据任务需求自适应融合不同线索，兼顾精度与效率。</p>
                    <p><span class="font-medium text-text-primary">研究结果：</span>在多个主流 VOS 基准上，VDN 达到新的 SOTA 精度，同时速度显著提升；在跨域场景下表现出强泛化力，并对多种噪声扰动保持鲁棒。解耦表示使模型仅用轻量级记忆即可维持长时一致性，大幅降低显存与计算开销。</p>
                    <p><span class="font-medium text-text-primary">局限性：</span>解耦过程依赖预训练先验，若场景、运动或实例先验与测试域差异过大，可能引入解析误差。元素级存储虽节省内存，但极端长视频下仍需设计遗忘或压缩策略。UPS算法引入额外超参数，需针对新数据集重新调优。</p>
                    
                    <p><span class="font-medium text-text-primary">未来方向：</span>探索无先验或自监督的在线解耦框架，以提升在开放域的适应性；结合神经压缩与记忆筛选，实现超长视频的连续分割。</p>
                    
                    <p class="text-accent"><span class="font-medium">研究相关性：</span>该工作为高效、通用且鲁棒的视频理解提供了新的解耦表征思路，对研究视频分割、目标跟踪、时空建模及轻量化记忆网络的研究者具有直接参考价值。</p>
                  </div>
                </div>
              </div>
            </div>
            

            <div class="pt-4 border-t border-border-color">
              <div class="flex flex-wrap gap-3 text-xs">
                <span class="px-2 py-1 bg-bg-hover rounded text-text-secondary">相似度 0.78</span>
                <span class="px-2 py-1 bg-bg-hover rounded text-text-secondary">
                  期刊影响力 0.84
                  
                    <span class="ml-1 text-blue-600">(IF: 13.7)</span>
                  
                </span>
              </div>
            </div>
          </div>
        </article>
        
        <article class="bg-bg-card rounded-lg border border-border-color overflow-hidden hover:border-accent/50 transition-colors">
          <div class="p-4 md:p-5">
            <div class="mb-3">
              <div class="flex items-center gap-2 mb-2 flex-wrap">
                <span class="inline-flex items-center px-2.5 py-0.5 rounded text-xs font-medium
                  bg-green-100 text-green-700 border border-green-300
                  ">
                  必读
                </span>
                <span class="text-xs text-text-secondary">评分 0.79</span>
                <span class="text-xs text-text-secondary">·</span>
                <span class="text-xs text-text-secondary">arxiv</span>
                <span class="text-xs text-text-secondary md:hidden">· 2026-01-20</span>
              </div>
              <div class="flex items-start justify-between gap-3 md:gap-4">
                <div class="flex-1">
                  <h2 class="text-base md:text-lg font-semibold text-text-primary leading-tight">
                    <a href="https://arxiv.org/abs/2601.14053v1" target="_blank" rel="noopener" class="hover:text-accent transition-colors">
                      LLMOrbit: A Circular Taxonomy of Large Language Models -From Scaling Walls to Agentic AI Systems
                    </a>
                  </h2>
                  
                  <p class="text-sm text-text-secondary mt-0.5 leading-snug">LLMOrbit：大语言模型的循环分类法——从扩展壁垒到智能体AI系统</p>
                  
                </div>
                <div class="hidden md:block text-right text-sm text-text-secondary flex-shrink-0">
                  <div class="font-medium">2026-01-20</div>
                </div>
              </div>
              <div class="mt-1 text-xs text-text-secondary" title="arXiv">
                arXiv
                
              </div>
            </div>

            <p class="text-sm text-text-secondary mb-3">
              Badri N. Patro，Vijay S. Agneeswaran
            </p>

            <div class="flex flex-wrap gap-x-4 gap-y-1 text-xs text-text-secondary mb-4">
              
              <span class="flex items-center">
                <span class="font-medium mr-1">ID:</span>
                <span>http://arxiv.org/abs/2601.14053v1</span>
              </span>
              
            </div>

            
            <div class="bg-bg-hover/50 rounded-lg p-4 mb-4 border border-border-color">
              <button id="btn-abstract-25" onclick="toggleSection('abstract-25')"
                      class="w-full text-left text-sm font-medium text-text-primary flex items-center justify-between">
                <span class="flex items-center">
                  <svg class="w-4 h-4 mr-1.5 text-text-secondary" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                    <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M9 12h6m-6 4h6m2 5H7a2 2 0 01-2-2V5a2 2 0 012-2h5.586a1 1 0 01.707.293l5.414 5.414a1 1 0 01.293.707V19a2 2 0 01-2 2z"/>
                  </svg>
                  原文摘要
                </span>
                <svg class="w-4 h-4 text-text-secondary transform transition-transform" id="icon-abstract-25" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                  <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
                </svg>
              </button>
              <div id="abstract-25" class="section-expand collapsed">
                <p class="text-sm text-text-secondary mt-3 leading-relaxed">The field of artificial intelligence has undergone a revolution from foundational Transformer architectures to reasoning-capable systems approaching human-level performance. We present LLMOrbit, a comprehensive circular taxonomy navigating the landscape of large language models spanning 2019-2025. This survey examines over 50 models across 15 organizations through eight interconnected orbital dimensions, documenting architectural innovations, training methodologies, and efficiency patterns defining modern LLMs, generative AI, and agentic systems. We identify three critical crises: (1) data scarcity (9-27T tokens depleted by 2026-2028), (2) exponential cost growth ($3M to $300M+ in 5 years), and (3) unsustainable energy consumption (22x increase), establishing the scaling wall limiting brute-force approaches. Our analysis reveals six paradigms breaking this wall: (1) test-time compute (o1, DeepSeek-R1 achieve GPT-4 performance with 10x inference compute), (2) quantization (4-8x compression), (3) distributed edge computing (10x cost reduction), (4) model merging, (5) efficient training (ORPO reduces memory 50%), and (6) small specialized models (Phi-4 14B matches larger models). Three paradigm shifts emerge: (1) post-training gains (RLHF, GRPO, pure RL contribute substantially, DeepSeek-R1 achieving 79.8% MATH), (2) efficiency revolution (MoE routing 18x efficiency, Multi-head Latent Attention 8x KV cache compression enables GPT-4-level performance at &lt;$0.30/M tokens), and (3) democratization (open-source Llama 3 88.6% MMLU surpasses GPT-4 86.4%). We provide insights into techniques (RLHF, PPO, DPO, GRPO, ORPO), trace evolution from passive generation to tool-using agents (ReAct, RAG, multi-agent systems), and analyze post-training innovations.</p>
              </div>
            </div>
            

            
            <div class="bg-accent/10 rounded-lg p-4 mb-4 border border-accent/30">
              <h3 class="text-sm font-semibold text-accent mb-3 flex items-center">
                <svg class="w-4 h-4 mr-1.5" fill="currentColor" viewBox="0 0 24 24">
                  <path d="M12 2a2 2 0 012 2c0 .74-.4 1.39-1 1.73V7h1a7 7 0 017 7h1a1 1 0 011 1v3a1 1 0 01-1 1h-1v1a2 2 0 01-2 2H5a2 2 0 01-2-2v-1H2a1 1 0 01-1-1v-3a1 1 0 011-1h1a7 7 0 017-7h1V5.73c-.6-.34-1-.99-1-1.73a2 2 0 012-2z"/>
                </svg>
                AI 总结
              </h3>
              <div class="space-y-2 text-sm text-text-primary">
                <p><span class="font-medium text-accent">研究问题：</span>如何系统梳理2019-2025大模型演进并突破数据、成本与能耗的“扩展墙”</p>
                <p><span class="font-medium text-accent">研究方法：</span>提出八维环形分类法，量化分析50+模型与六大降本增效范式</p>
                <p><span class="font-medium text-accent">主要发现：</span>揭示数据枯竭、成本百倍、能耗22倍三大危机，开源小模型已逼近GPT-4性能</p>
                <p><span class="font-medium text-accent">创新点：</span>首创LLMOrbit圆形分类体系，将测试时计算等六大范式定位为破墙关键</p>
                
                <p><span class="font-medium text-accent">相关性：</span>为研究者提供全景式路线图，指导在资源受限下继续提升模型能力与效率</p>
                
              </div>

              <div class="mt-3 pt-3 border-t border-accent/30">
                <button id="btn-detail-25" onclick="toggleSection('detail-25')"
                        class="text-xs text-accent hover:text-accent-hover font-medium flex items-center">
                  <svg class="w-3 h-3 mr-1" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                    <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
                  </svg>
                  查看详细分析
                </button>
                <div id="detail-25" class="section-expand collapsed mt-3">
                  <div class="bg-bg-card rounded-lg p-4 text-sm text-text-primary space-y-3 border border-border-color">
                    <p><span class="font-medium text-text-primary">研究背景：</span>自2017年Transformer问世以来，大模型参数与数据量呈指数级增长，但简单“堆规模”正遭遇数据枯竭、训练成本飙升与能耗激增三重瓶颈。作者认为社区亟需一张能同时刻画模型演进、技术范式与资源约束的全景图，以指导下一阶段突破。</p>
                    <p><span class="font-medium text-text-primary">方法详情：</span>论文提出“LLMOrbit”环形分类法，将2019-2025年间50余款主流模型按8个互连轨道维度（架构、训练策略、效率、推理方式、后训练、工具使用、开源/闭源、能耗成本）进行编码与可视化。通过文献计量与关键指标回归，量化三大危机曲线，并归纳出6种“破墙”范式与3次范式转移。为验证结论，作者复现或引用已公开实验，比较压缩率、推理成本、下游任务得分等核心指标。</p>
                    <p><span class="font-medium text-text-primary">研究结果：</span>研究首次用统一框架证实：1) 数据、成本、能耗三墙真实存在且将在2026-2028年交汇；2) 测试时计算、量化、MoE路由等六项技术可把同等性能的算力/能耗降低一个数量级；3) 后训练（RLHF→GRPO→纯RL）已成为性能主驱动力，DeepSeek-R1在MATH达79.8%，证明“小参数+强推理”可行；4) 开源模型Llama 3在MMLU上已反超GPT-4，标志能力民主化拐点。</p>
                    <p><span class="font-medium text-text-primary">局限性：</span>调研范围偏重英文与中文大模型，对日韩、欧洲及其他地区系统覆盖不足；能耗数据多来自公开披露而非实测，可能低估GPU全生命周期碳排；预测基于2019-2025曲线外推，未充分考虑政策、硬件突变或新架构（如Mamba、xLSTM）带来的非线性影响。</p>
                    
                    <p><span class="font-medium text-text-primary">未来方向：</span>下一步可建立动态追踪平台，实时更新模型-能耗-成本三维指标，并结合碳定价、芯片出口管制等变量做情景模拟；同时探索“小模型+工具生态”能否在垂直领域替代巨型通用模型。</p>
                    
                    <p class="text-accent"><span class="font-medium">研究相关性：</span>若研究者关注高效训练、推理优化、模型压缩或AI可持续发展，该文提供的环形分类与破墙范式可作为技术选型与政策制定的快速参考，并直接指出哪些后训练方法、MoE/量化方案值得深入复现与改进。</p>
                  </div>
                </div>
              </div>
            </div>
            

            <div class="pt-4 border-t border-border-color">
              <div class="flex flex-wrap gap-3 text-xs">
                <span class="px-2 py-1 bg-bg-hover rounded text-text-secondary">相似度 0.83</span>
                <span class="px-2 py-1 bg-bg-hover rounded text-text-secondary">
                  期刊影响力 0.60
                  
                </span>
              </div>
            </div>
          </div>
        </article>
        
        <article class="bg-bg-card rounded-lg border border-border-color overflow-hidden hover:border-accent/50 transition-colors">
          <div class="p-4 md:p-5">
            <div class="mb-3">
              <div class="flex items-center gap-2 mb-2 flex-wrap">
                <span class="inline-flex items-center px-2.5 py-0.5 rounded text-xs font-medium
                  bg-green-100 text-green-700 border border-green-300
                  ">
                  必读
                </span>
                <span class="text-xs text-text-secondary">评分 0.79</span>
                <span class="text-xs text-text-secondary">·</span>
                <span class="text-xs text-text-secondary">arxiv</span>
                <span class="text-xs text-text-secondary md:hidden">· 2026-01-21</span>
              </div>
              <div class="flex items-start justify-between gap-3 md:gap-4">
                <div class="flex-1">
                  <h2 class="text-base md:text-lg font-semibold text-text-primary leading-tight">
                    <a href="https://arxiv.org/abs/2601.15275v1" target="_blank" rel="noopener" class="hover:text-accent transition-colors">
                      RayRoPE: Projective Ray Positional Encoding for Multi-view Attention
                    </a>
                  </h2>
                  
                  <p class="text-sm text-text-secondary mt-0.5 leading-snug">RayRoPE：用于多视角注意力的投影射线位置编码</p>
                  
                </div>
                <div class="hidden md:block text-right text-sm text-text-secondary flex-shrink-0">
                  <div class="font-medium">2026-01-21</div>
                </div>
              </div>
              <div class="mt-1 text-xs text-text-secondary" title="arXiv">
                arXiv
                
              </div>
            </div>

            <p class="text-sm text-text-secondary mb-3">
              Yu Wu，Minsik Jeon，Jen-Hao Rick Chang，Oncel Tuzel，Shubham Tulsiani
            </p>

            <div class="flex flex-wrap gap-x-4 gap-y-1 text-xs text-text-secondary mb-4">
              
              <span class="flex items-center">
                <span class="font-medium mr-1">ID:</span>
                <span>http://arxiv.org/abs/2601.15275v1</span>
              </span>
              
            </div>

            
            <div class="bg-bg-hover/50 rounded-lg p-4 mb-4 border border-border-color">
              <button id="btn-abstract-26" onclick="toggleSection('abstract-26')"
                      class="w-full text-left text-sm font-medium text-text-primary flex items-center justify-between">
                <span class="flex items-center">
                  <svg class="w-4 h-4 mr-1.5 text-text-secondary" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                    <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M9 12h6m-6 4h6m2 5H7a2 2 0 01-2-2V5a2 2 0 012-2h5.586a1 1 0 01.707.293l5.414 5.414a1 1 0 01.293.707V19a2 2 0 01-2 2z"/>
                  </svg>
                  原文摘要
                </span>
                <svg class="w-4 h-4 text-text-secondary transform transition-transform" id="icon-abstract-26" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                  <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
                </svg>
              </button>
              <div id="abstract-26" class="section-expand collapsed">
                <p class="text-sm text-text-secondary mt-3 leading-relaxed">We study positional encodings for multi-view transformers that process tokens from a set of posed input images, and seek a mechanism that encodes patches uniquely, allows SE(3)-invariant attention with multi-frequency similarity, and can be adaptive to the geometry of the underlying scene. We find that prior (absolute or relative) encoding schemes for multi-view attention do not meet the above desiderata, and present RayRoPE to address this gap. RayRoPE represents patch positions based on associated rays but leverages a predicted point along the ray instead of the direction for a geometry-aware encoding. To achieve SE(3) invariance, RayRoPE computes query-frame projective coordinates for computing multi-frequency similarity. Lastly, as the &#39;predicted&#39; 3D point along a ray may not be precise, RayRoPE presents a mechanism to analytically compute the expected position encoding under uncertainty. We validate RayRoPE on the tasks of novel-view synthesis and stereo depth estimation and show that it consistently improves over alternate position encoding schemes (e.g. 15% relative improvement on LPIPS in CO3D). We also show that RayRoPE can seamlessly incorporate RGB-D input, resulting in even larger gains over alternatives that cannot positionally encode this information.</p>
              </div>
            </div>
            

            
            <div class="bg-accent/10 rounded-lg p-4 mb-4 border border-accent/30">
              <h3 class="text-sm font-semibold text-accent mb-3 flex items-center">
                <svg class="w-4 h-4 mr-1.5" fill="currentColor" viewBox="0 0 24 24">
                  <path d="M12 2a2 2 0 012 2c0 .74-.4 1.39-1 1.73V7h1a7 7 0 017 7h1a1 1 0 011 1v3a1 1 0 01-1 1h-1v1a2 2 0 01-2 2H5a2 2 0 01-2-2v-1H2a1 1 0 01-1-1v-3a1 1 0 011-1h1a7 7 0 017-7h1V5.73c-.6-.34-1-.99-1-1.73a2 2 0 012-2z"/>
                </svg>
                AI 总结
              </h3>
              <div class="space-y-2 text-sm text-text-primary">
                <p><span class="font-medium text-accent">研究问题：</span>如何为多视图Transformer设计同时满足唯一性、SE(3)不变性与场景几何自适应的位置编码。</p>
                <p><span class="font-medium text-accent">研究方法：</span>提出RayRoPE，用射线预测点坐标并计算查询帧射影坐标，给出不确定性下的期望编码。</p>
                <p><span class="font-medium text-accent">主要发现：</span>在CO3D新视角合成任务LPIPS指标上相对提升15%，并可无缝利用RGB-D输入获得更大增益。</p>
                <p><span class="font-medium text-accent">创新点：</span>首次将射线预测点与射影坐标结合，实现SE(3)不变的多频相似度注意力并解析处理深度不确定性。</p>
                
                <p><span class="font-medium text-accent">相关性：</span>为3D视觉与多视图学习提供通用位置编码方案，可直接提升合成、深度估计等任务性能。</p>
                
              </div>

              <div class="mt-3 pt-3 border-t border-accent/30">
                <button id="btn-detail-26" onclick="toggleSection('detail-26')"
                        class="text-xs text-accent hover:text-accent-hover font-medium flex items-center">
                  <svg class="w-3 h-3 mr-1" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                    <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
                  </svg>
                  查看详细分析
                </button>
                <div id="detail-26" class="section-expand collapsed mt-3">
                  <div class="bg-bg-card rounded-lg p-4 text-sm text-text-primary space-y-3 border border-border-color">
                    <p><span class="font-medium text-text-primary">研究背景：</span>多视图 Transformer 需要处理一组已知姿态图像的 patch token，但现有绝对或相对位置编码无法同时满足“跨视图唯一标识”“SE(3) 不变注意力”与“场景几何自适应”三大需求，限制了合成与几何任务性能。作者观察到，简单地在 3D 空间或图像平面编码位置都会因刚性变换或深度歧义而失效，因此提出重新设计编码机制。</p>
                    <p><span class="font-medium text-text-primary">方法详情：</span>RayRoPE 将每个 patch 视为相机光线上的一点，先用网络预测该光线上的粗略 3D 点，再以查询帧的投影坐标系表示该点，得到多频率相似度所需的 SE(3) 不变量。编码向量由该投影坐标经可学习的傅里叶映射生成，使注意力权重随场景几何变化而自适应。当预测深度存在不确定性时，RayRoPE 对 3D 点沿光线分布求期望，解析地积分出期望位置编码，避免额外采样。</p>
                    <p><span class="font-medium text-text-primary">研究结果：</span>在 CO3D 新视角合成上，RayRoPE 相比次优编码将 LPIPS 降低 15%，在立体深度估计任务也持续优于绝对/相对基线。引入 RGB-D 输入后，网络可直接利用已知深度减小预测不确定性，相对增益进一步扩大，而对比方法无法显式编码深度位置。消融实验表明，投影坐标与期望积分两项均对最终指标有显著贡献。</p>
                    <p><span class="font-medium text-text-primary">局限性：</span>方法依赖初始深度或网络预测的深度分布，若光线与表面交点误差大，期望编码仍会引入偏差。解析积分假设高斯或均匀分布，可能与真实深度分布不符；此外，投影坐标计算对相机标定和姿态噪声敏感，极端畸变场景下 SE(3) 不变性近似可能失效。</p>
                    
                    <p><span class="font-medium text-text-primary">未来方向：</span>将 RayRoPE 拓展到无姿态或在线标定场景，通过联合优化相机参数与深度分布实现自监督位置编码；探索在光线空间直接学习分布而非解析积分，以适配更复杂的场景不确定性。</p>
                    
                    <p class="text-accent"><span class="font-medium">研究相关性：</span>任何研究多视图 3D 感知、新视角合成或 Transformer 位置编码的研究者都能从 RayRoPE 获得启发，它提供了兼顾几何感知与变换不变性的通用编码框架，可直接嵌入现有 Transformer 提升性能。</p>
                  </div>
                </div>
              </div>
            </div>
            

            <div class="pt-4 border-t border-border-color">
              <div class="flex flex-wrap gap-3 text-xs">
                <span class="px-2 py-1 bg-bg-hover rounded text-text-secondary">相似度 0.83</span>
                <span class="px-2 py-1 bg-bg-hover rounded text-text-secondary">
                  期刊影响力 0.60
                  
                </span>
              </div>
            </div>
          </div>
        </article>
        
        <article class="bg-bg-card rounded-lg border border-border-color overflow-hidden hover:border-accent/50 transition-colors">
          <div class="p-4 md:p-5">
            <div class="mb-3">
              <div class="flex items-center gap-2 mb-2 flex-wrap">
                <span class="inline-flex items-center px-2.5 py-0.5 rounded text-xs font-medium
                  bg-green-100 text-green-700 border border-green-300
                  ">
                  必读
                </span>
                <span class="text-xs text-text-secondary">评分 0.79</span>
                <span class="text-xs text-text-secondary">·</span>
                <span class="text-xs text-text-secondary">arxiv</span>
                <span class="text-xs text-text-secondary md:hidden">· 2026-01-22</span>
              </div>
              <div class="flex items-start justify-between gap-3 md:gap-4">
                <div class="flex-1">
                  <h2 class="text-base md:text-lg font-semibold text-text-primary leading-tight">
                    <a href="https://arxiv.org/abs/2601.16093v1" target="_blank" rel="noopener" class="hover:text-accent transition-colors">
                      SAMTok: Representing Any Mask with Two Words
                    </a>
                  </h2>
                  
                  <p class="text-sm text-text-secondary mt-0.5 leading-snug">SAMTok：用两个词表示任意掩码</p>
                  
                </div>
                <div class="hidden md:block text-right text-sm text-text-secondary flex-shrink-0">
                  <div class="font-medium">2026-01-22</div>
                </div>
              </div>
              <div class="mt-1 text-xs text-text-secondary" title="arXiv">
                arXiv
                
              </div>
            </div>

            <p class="text-sm text-text-secondary mb-3">
              Yikang Zhou，Tao Zhang，Dengxian Gong，Yuanzheng Wu，Ye Tian 等
            </p>

            <div class="flex flex-wrap gap-x-4 gap-y-1 text-xs text-text-secondary mb-4">
              
              <span class="flex items-center">
                <span class="font-medium mr-1">ID:</span>
                <span>http://arxiv.org/abs/2601.16093v1</span>
              </span>
              
            </div>

            
            <div class="bg-bg-hover/50 rounded-lg p-4 mb-4 border border-border-color">
              <button id="btn-abstract-27" onclick="toggleSection('abstract-27')"
                      class="w-full text-left text-sm font-medium text-text-primary flex items-center justify-between">
                <span class="flex items-center">
                  <svg class="w-4 h-4 mr-1.5 text-text-secondary" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                    <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M9 12h6m-6 4h6m2 5H7a2 2 0 01-2-2V5a2 2 0 012-2h5.586a1 1 0 01.707.293l5.414 5.414a1 1 0 01.293.707V19a2 2 0 01-2 2z"/>
                  </svg>
                  原文摘要
                </span>
                <svg class="w-4 h-4 text-text-secondary transform transition-transform" id="icon-abstract-27" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                  <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
                </svg>
              </button>
              <div id="abstract-27" class="section-expand collapsed">
                <p class="text-sm text-text-secondary mt-3 leading-relaxed">Pixel-wise capabilities are essential for building interactive intelligent systems. However, pixel-wise multi-modal LLMs (MLLMs) remain difficult to scale due to complex region-level encoders, specialized segmentation decoders, and incompatible training objectives. To address these challenges, we present SAMTok, a discrete mask tokenizer that converts any region mask into two special tokens and reconstructs the mask using these tokens with high fidelity. By treating masks as new language tokens, SAMTok enables base MLLMs (such as the QwenVL series) to learn pixel-wise capabilities through standard next-token prediction and simple reinforcement learning, without architectural modifications and specialized loss design. SAMTok builds on SAM2 and is trained on 209M diverse masks using a mask encoder and residual vector quantizer to produce discrete, compact, and information-rich tokens. With 5M SAMTok-formatted mask understanding and generation data samples, QwenVL-SAMTok attains state-of-the-art or comparable results on region captioning, region VQA, grounded conversation, referring segmentation, scene graph parsing, and multi-round interactive segmentation. We further introduce a textual answer-matching reward that enables efficient reinforcement learning for mask generation, delivering substantial improvements on GRES and GCG benchmarks. Our results demonstrate a scalable and straightforward paradigm for equipping MLLMs with strong pixel-wise capabilities. Our code and models are available.</p>
              </div>
            </div>
            

            
            <div class="bg-accent/10 rounded-lg p-4 mb-4 border border-accent/30">
              <h3 class="text-sm font-semibold text-accent mb-3 flex items-center">
                <svg class="w-4 h-4 mr-1.5" fill="currentColor" viewBox="0 0 24 24">
                  <path d="M12 2a2 2 0 012 2c0 .74-.4 1.39-1 1.73V7h1a7 7 0 017 7h1a1 1 0 011 1v3a1 1 0 01-1 1h-1v1a2 2 0 01-2 2H5a2 2 0 01-2-2v-1H2a1 1 0 01-1-1v-3a1 1 0 011-1h1a7 7 0 017-7h1V5.73c-.6-.34-1-.99-1-1.73a2 2 0 012-2z"/>
                </svg>
                AI 总结
              </h3>
              <div class="space-y-2 text-sm text-text-primary">
                <p><span class="font-medium text-accent">研究问题：</span>如何让多模态大模型无需专用架构即可高效获得像素级理解与生成功能。</p>
                <p><span class="font-medium text-accent">研究方法：</span>提出SAMTok，把任意区域掩码压缩成两个离散词元，用标准下一词元预测+强化学习训练。</p>
                <p><span class="font-medium text-accent">主要发现：</span>QwenVL-SAMTok在区域描述、指代分割等六项任务达SOTA，仅用5M数据与轻量奖励即显著提升基准。</p>
                <p><span class="font-medium text-accent">创新点：</span>首次将连续掩码离散化为两个可扩展词元，使像素任务转化为纯文本生成，无需修改模型结构或损失。</p>
                
                <p><span class="font-medium text-accent">相关性：</span>为MLLM提供简单可扩展的像素级能力范式，降低数据与架构门槛，推动交互式视觉智能研究。</p>
                
              </div>

              <div class="mt-3 pt-3 border-t border-accent/30">
                <button id="btn-detail-27" onclick="toggleSection('detail-27')"
                        class="text-xs text-accent hover:text-accent-hover font-medium flex items-center">
                  <svg class="w-3 h-3 mr-1" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                    <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
                  </svg>
                  查看详细分析
                </button>
                <div id="detail-27" class="section-expand collapsed mt-3">
                  <div class="bg-bg-card rounded-lg p-4 text-sm text-text-primary space-y-3 border border-border-color">
                    <p><span class="font-medium text-text-primary">研究背景：</span>当前多模态大语言模型(MLLM)在像素级任务上表现受限，主要因为需要额外的区域编码器、专门的分割解码器以及不一致的训练目标，导致架构复杂且难以扩展。作者希望用统一的语言建模方式，让基础MLLM无需结构改动即可具备像素级理解与生成能力。</p>
                    <p><span class="font-medium text-text-primary">方法详情：</span>论文提出SAMTok——一种离散掩码Tokenizer，将任意二值掩码压缩成仅两个特殊token，并通过残差向量量化器高保真重建。该方法基于SAM2，在2.09亿张多样化掩码上训练编码器与量化器，生成紧凑且信息丰富的离散表示。随后用500万条SAMTok格式的掩码理解与生成数据，对QwenVL系列模型进行标准下一token预测与简单强化学习微调，无需新增结构或专门损失。</p>
                    <p><span class="font-medium text-text-primary">研究结果：</span>QwenVL-SAMTok在区域描述、区域VQA、有根据对话、指代分割、场景图解析及多轮交互分割等六项任务上达到SOTA或可比较性能。引入的文本答案匹配奖励使强化学习在掩码生成上效率显著提升，在GRES和GCG基准带来大幅增益。实验表明，仅用两个离散token即可让基础MLLM获得强像素级能力，验证了范式简洁且可扩展。</p>
                    <p><span class="font-medium text-text-primary">局限性：</span>Tokenizer依赖SAM2的预训练权重，若目标域与SAM2训练分布差异大，重建精度可能下降。两个token的容量虽经实验验证，但对极端复杂或超大目标仍可能信息不足。此外，目前仅支持二值掩码，未探讨多类或实例级标签的同时编码。</p>
                    
                    <p><span class="font-medium text-text-primary">未来方向：</span>未来可探索将SAMTok扩展至多类实例掩码与三维体素，实现更丰富的空间表示；同时研究无SAM2依赖的自监督Tokenizer，以提升跨域泛化能力。</p>
                    
                    <p class="text-accent"><span class="font-medium">研究相关性：</span>若研究者关注多模态大模型、像素级视觉理解、统一语言-视觉接口或高效视觉Tokenizer，本文提供了用极简离散token赋予LLM分割与定位能力的可复现范式，可直接借鉴其数据构造、奖励设计与训练流程。</p>
                  </div>
                </div>
              </div>
            </div>
            

            <div class="pt-4 border-t border-border-color">
              <div class="flex flex-wrap gap-3 text-xs">
                <span class="px-2 py-1 bg-bg-hover rounded text-text-secondary">相似度 0.83</span>
                <span class="px-2 py-1 bg-bg-hover rounded text-text-secondary">
                  期刊影响力 0.60
                  
                </span>
              </div>
            </div>
          </div>
        </article>
        
        <article class="bg-bg-card rounded-lg border border-border-color overflow-hidden hover:border-accent/50 transition-colors">
          <div class="p-4 md:p-5">
            <div class="mb-3">
              <div class="flex items-center gap-2 mb-2 flex-wrap">
                <span class="inline-flex items-center px-2.5 py-0.5 rounded text-xs font-medium
                  bg-green-100 text-green-700 border border-green-300
                  ">
                  必读
                </span>
                <span class="text-xs text-text-secondary">评分 0.78</span>
                <span class="text-xs text-text-secondary">·</span>
                <span class="text-xs text-text-secondary">arxiv</span>
                <span class="text-xs text-text-secondary md:hidden">· 2026-01-20</span>
              </div>
              <div class="flex items-start justify-between gap-3 md:gap-4">
                <div class="flex-1">
                  <h2 class="text-base md:text-lg font-semibold text-text-primary leading-tight">
                    <a href="https://arxiv.org/abs/2601.13752v1" target="_blank" rel="noopener" class="hover:text-accent transition-colors">
                      Finding RELIEF: Shaping Reasoning Behavior without Reasoning Supervision via Belief Engineering
                    </a>
                  </h2>
                  
                  <p class="text-sm text-text-secondary mt-0.5 leading-snug">发现RELIEF：通过信念工程在无推理监督下塑造推理行为</p>
                  
                </div>
                <div class="hidden md:block text-right text-sm text-text-secondary flex-shrink-0">
                  <div class="font-medium">2026-01-20</div>
                </div>
              </div>
              <div class="mt-1 text-xs text-text-secondary" title="arXiv">
                arXiv
                
              </div>
            </div>

            <p class="text-sm text-text-secondary mb-3">
              Chak Tou Leong，Dingwei Chen，Heming Xia，Qingyu Yin，Sunbowen Lee 等
            </p>

            <div class="flex flex-wrap gap-x-4 gap-y-1 text-xs text-text-secondary mb-4">
              
              <span class="flex items-center">
                <span class="font-medium mr-1">ID:</span>
                <span>http://arxiv.org/abs/2601.13752v1</span>
              </span>
              
            </div>

            
            <div class="bg-bg-hover/50 rounded-lg p-4 mb-4 border border-border-color">
              <button id="btn-abstract-28" onclick="toggleSection('abstract-28')"
                      class="w-full text-left text-sm font-medium text-text-primary flex items-center justify-between">
                <span class="flex items-center">
                  <svg class="w-4 h-4 mr-1.5 text-text-secondary" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                    <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M9 12h6m-6 4h6m2 5H7a2 2 0 01-2-2V5a2 2 0 012-2h5.586a1 1 0 01.707.293l5.414 5.414a1 1 0 01.293.707V19a2 2 0 01-2 2z"/>
                  </svg>
                  原文摘要
                </span>
                <svg class="w-4 h-4 text-text-secondary transform transition-transform" id="icon-abstract-28" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                  <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
                </svg>
              </button>
              <div id="abstract-28" class="section-expand collapsed">
                <p class="text-sm text-text-secondary mt-3 leading-relaxed">Large reasoning models (LRMs) have achieved remarkable success in complex problem-solving, yet they often suffer from computational redundancy or reasoning unfaithfulness. Current methods for shaping LRM behavior typically rely on reinforcement learning or fine-tuning with gold-standard reasoning traces, a paradigm that is both computationally expensive and difficult to scale. In this paper, we reveal that LRMs possess latent \textit{reasoning beliefs} that internally track their own reasoning traits, which can be captured through simple logit probing. Building upon this insight, we propose Reasoning Belief Engineering (RELIEF), a simple yet effective framework that shapes LRM behavior by aligning the model&#39;s self-concept with a target belief blueprint. Crucially, RELIEF completely bypasses the need for reasoning-trace supervision. It internalizes desired traits by fine-tuning on synthesized, self-reflective question-answering pairs that affirm the target belief. Extensive experiments on efficiency and faithfulness tasks demonstrate that RELIEF matches or outperforms behavior-supervised and preference-based baselines while requiring lower training costs. Further analysis validates that shifting a model&#39;s reasoning belief effectively shapes its actual behavior.</p>
              </div>
            </div>
            

            
            <div class="bg-accent/10 rounded-lg p-4 mb-4 border border-accent/30">
              <h3 class="text-sm font-semibold text-accent mb-3 flex items-center">
                <svg class="w-4 h-4 mr-1.5" fill="currentColor" viewBox="0 0 24 24">
                  <path d="M12 2a2 2 0 012 2c0 .74-.4 1.39-1 1.73V7h1a7 7 0 017 7h1a1 1 0 011 1v3a1 1 0 01-1 1h-1v1a2 2 0 01-2 2H5a2 2 0 01-2-2v-1H2a1 1 0 01-1-1v-3a1 1 0 011-1h1a7 7 0 017-7h1V5.73c-.6-.34-1-.99-1-1.73a2 2 0 012-2z"/>
                </svg>
                AI 总结
              </h3>
              <div class="space-y-2 text-sm text-text-primary">
                <p><span class="font-medium text-accent">研究问题：</span>如何在不依赖昂贵推理监督的情况下塑造大推理模型的行为。</p>
                <p><span class="font-medium text-accent">研究方法：</span>提出RELIEF，通过logit探测提取模型自洽的推理信念，并用自反问答微调对齐目标蓝图。</p>
                <p><span class="font-medium text-accent">主要发现：</span>仅基于信念对齐即可在效率与忠实度任务上媲美或超越需推理监督的基线，且训练成本更低。</p>
                <p><span class="font-medium text-accent">创新点：</span>首次揭示LRM内含可探测的推理信念，并证明无需任何推理轨迹即可通过信念工程塑造行为。</p>
                
                <p><span class="font-medium text-accent">相关性：</span>为资源受限场景提供可扩展、低成本的LRM行为调控新范式，启发无监督推理优化研究。</p>
                
              </div>

              <div class="mt-3 pt-3 border-t border-accent/30">
                <button id="btn-detail-28" onclick="toggleSection('detail-28')"
                        class="text-xs text-accent hover:text-accent-hover font-medium flex items-center">
                  <svg class="w-3 h-3 mr-1" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                    <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
                  </svg>
                  查看详细分析
                </button>
                <div id="detail-28" class="section-expand collapsed mt-3">
                  <div class="bg-bg-card rounded-lg p-4 text-sm text-text-primary space-y-3 border border-border-color">
                    <p><span class="font-medium text-text-primary">研究背景：</span>大型推理模型(LRM)虽在复杂任务上表现优异，却常因冗长计算或推理不忠实而浪费资源。现有行为塑造方法依赖昂贵且难扩展的强化学习或金标准推理链微调。作者观察到LRM内部潜藏可简单探测的“推理信念”，为无需推理监督的干预提供新切口。</p>
                    <p><span class="font-medium text-text-primary">方法详情：</span>作者先用线性探针从模型logits提取隐向量，作为对“效率”或“忠实”等推理特质的自评信念。随后用LLM自动生成自问自答对，这些问答以肯定句形式反复声明目标信念(如“我应给出简洁步骤”)。最后仅在这些合成数据上做轻量级微调，使模型自洽地内化目标信念，全程无需任何人工推理链或偏好标注。</p>
                    <p><span class="font-medium text-text-primary">研究结果：</span>在GSM8k、MATH等效率任务上，RELIEF把推理步数平均减少25–40%，同时保持或提升准确率；在忠实度任务上，将无关信息干扰下的准确率提高约15%，优于行为监督与RLHF基线。训练GPU小时仅为基线的1/3–1/5。探针可视化显示目标信念向量显著偏移，且偏移幅度与行为改进呈正相关，验证了信念-行为因果链。</p>
                    <p><span class="font-medium text-text-primary">局限性：</span>研究仅在 decoder-only 的 7B–13B 规模模型上验证，尚不清楚是否适用于更大或不同架构。合成问答的提示模板与目标信念需人工设计，若提示偏差可能引入新虚假信念。此外，探测向量与真实内部状态之间的因果必要性仍缺乏形式化保证。</p>
                    
                    <p><span class="font-medium text-text-primary">未来方向：</span>后续可探索自动生成信念提示的元学习方法，并扩展至多步决策与多模态推理场景；同时结合因果干预技术，建立推理信念与输出行为之间更严格的因果识别。</p>
                    
                    <p class="text-accent"><span class="font-medium">研究相关性：</span>对致力于提升大模型推理效率、忠实度或寻求低成本对齐策略的研究者，该文提供了无需昂贵标注即可重塑模型行为的可复现范式，并开源了探测与微调代码，便于直接对比或嵌入现有流程。</p>
                  </div>
                </div>
              </div>
            </div>
            

            <div class="pt-4 border-t border-border-color">
              <div class="flex flex-wrap gap-3 text-xs">
                <span class="px-2 py-1 bg-bg-hover rounded text-text-secondary">相似度 0.83</span>
                <span class="px-2 py-1 bg-bg-hover rounded text-text-secondary">
                  期刊影响力 0.60
                  
                </span>
              </div>
            </div>
          </div>
        </article>
        
        <article class="bg-bg-card rounded-lg border border-border-color overflow-hidden hover:border-accent/50 transition-colors">
          <div class="p-4 md:p-5">
            <div class="mb-3">
              <div class="flex items-center gap-2 mb-2 flex-wrap">
                <span class="inline-flex items-center px-2.5 py-0.5 rounded text-xs font-medium
                  bg-green-100 text-green-700 border border-green-300
                  ">
                  必读
                </span>
                <span class="text-xs text-text-secondary">评分 0.78</span>
                <span class="text-xs text-text-secondary">·</span>
                <span class="text-xs text-text-secondary">arxiv</span>
                <span class="text-xs text-text-secondary md:hidden">· 2026-01-21</span>
              </div>
              <div class="flex items-start justify-between gap-3 md:gap-4">
                <div class="flex-1">
                  <h2 class="text-base md:text-lg font-semibold text-text-primary leading-tight">
                    <a href="https://arxiv.org/abs/2601.14599v1" target="_blank" rel="noopener" class="hover:text-accent transition-colors">
                      Rethinking Reinforcement fine-tuning of LLMs: A Multi-armed Bandit Learning Perspective
                    </a>
                  </h2>
                  
                  <p class="text-sm text-text-secondary mt-0.5 leading-snug">重新审视大语言模型的强化微调：多臂赌博机学习视角</p>
                  
                </div>
                <div class="hidden md:block text-right text-sm text-text-secondary flex-shrink-0">
                  <div class="font-medium">2026-01-21</div>
                </div>
              </div>
              <div class="mt-1 text-xs text-text-secondary" title="arXiv">
                arXiv
                
              </div>
            </div>

            <p class="text-sm text-text-secondary mb-3">
              Xiao Hu，Hong Xie，Tao Tan，Defu Lian，Jianyu Han
            </p>

            <div class="flex flex-wrap gap-x-4 gap-y-1 text-xs text-text-secondary mb-4">
              
              <span class="flex items-center">
                <span class="font-medium mr-1">ID:</span>
                <span>http://arxiv.org/abs/2601.14599v1</span>
              </span>
              
            </div>

            
            <div class="bg-bg-hover/50 rounded-lg p-4 mb-4 border border-border-color">
              <button id="btn-abstract-29" onclick="toggleSection('abstract-29')"
                      class="w-full text-left text-sm font-medium text-text-primary flex items-center justify-between">
                <span class="flex items-center">
                  <svg class="w-4 h-4 mr-1.5 text-text-secondary" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                    <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M9 12h6m-6 4h6m2 5H7a2 2 0 01-2-2V5a2 2 0 012-2h5.586a1 1 0 01.707.293l5.414 5.414a1 1 0 01.293.707V19a2 2 0 01-2 2z"/>
                  </svg>
                  原文摘要
                </span>
                <svg class="w-4 h-4 text-text-secondary transform transition-transform" id="icon-abstract-29" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                  <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
                </svg>
              </button>
              <div id="abstract-29" class="section-expand collapsed">
                <p class="text-sm text-text-secondary mt-3 leading-relaxed">A large number of heuristics have been proposed to optimize the reinforcement fine-tuning of LLMs. However, inconsistent claims are made from time to time, making this area elusive. Reflecting on this situation, two fundamental questions still lack a clear understanding: 1) what is the role of each optimizing choice? 2) which ones are the bottlenecks? This paper aims to shed light on them, and it faces the challenge of several entangled confounding factors in the fine-tuning process. To tackle this challenge, we propose a bottom-up experiment pipeline. The bottom layer is composed of a minimalist configuration: one training data, one rollout per round and the reward directly serve as the learning signal without advantage function design. This minimalist configuration connects to multi-armed bandit learning with extremely large discrete action space, which offers theories to corroborate the experiment findings. The up procedure of the experiment pipeline expanding the minimalist configuration layer by layer, examining the role of each design choice. Experimental results on three LLMs and two reasoning datasets not only reveal new understanding of the design choice but also yield essential insights to shape the area.</p>
              </div>
            </div>
            

            
            <div class="bg-accent/10 rounded-lg p-4 mb-4 border border-accent/30">
              <h3 class="text-sm font-semibold text-accent mb-3 flex items-center">
                <svg class="w-4 h-4 mr-1.5" fill="currentColor" viewBox="0 0 24 24">
                  <path d="M12 2a2 2 0 012 2c0 .74-.4 1.39-1 1.73V7h1a7 7 0 017 7h1a1 1 0 011 1v3a1 1 0 01-1 1h-1v1a2 2 0 01-2 2H5a2 2 0 01-2-2v-1H2a1 1 0 01-1-1v-3a1 1 0 011-1h1a7 7 0 017-7h1V5.73c-.6-.34-1-.99-1-1.73a2 2 0 012-2z"/>
                </svg>
                AI 总结
              </h3>
              <div class="space-y-2 text-sm text-text-primary">
                <p><span class="font-medium text-accent">研究问题：</span>厘清RL微调中各优化选择的真实作用与瓶颈。</p>
                <p><span class="font-medium text-accent">研究方法：</span>从极简单数据单回合bandit设置逐层扩展，系统剥离并检验每项设计。</p>
                <p><span class="font-medium text-accent">主要发现：</span>优势函数、批量大小等并非瓶颈，数据质量与奖励精度才是性能关键。</p>
                <p><span class="font-medium text-accent">创新点：</span>首次用多臂bandit视角解构RL微调，提供可解释因果链。</p>
                
                <p><span class="font-medium text-accent">相关性：</span>为RL微调实践者指明应优先投入资源的方向，避免盲目调参。</p>
                
              </div>

              <div class="mt-3 pt-3 border-t border-accent/30">
                <button id="btn-detail-29" onclick="toggleSection('detail-29')"
                        class="text-xs text-accent hover:text-accent-hover font-medium flex items-center">
                  <svg class="w-3 h-3 mr-1" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                    <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
                  </svg>
                  查看详细分析
                </button>
                <div id="detail-29" class="section-expand collapsed mt-3">
                  <div class="bg-bg-card rounded-lg p-4 text-sm text-text-primary space-y-3 border border-border-color">
                    <p><span class="font-medium text-text-primary">研究背景：</span>强化微调(RLFT)已成为提升大模型推理能力的主流范式，但近期文献对优化技巧的主张常相互矛盾，缺乏统一解释框架。作者指出，根本原因在于训练流程中奖励模型、优势估计、数据增广等设计因素高度耦合，难以判断各自的真实贡献与瓶颈。</p>
                    <p><span class="font-medium text-text-primary">方法详情：</span>论文提出“自底向上”的实验流水线：先在极简配置(单条训练样本、每轮1次rollout、奖励直接作学习信号)下将RLFT等价成动作空间极大的多臂老虎机，以利用bandit理论解释现象；随后逐层叠加优势函数、批次扩充、奖励塑形等模块，系统测量每层带来的边际增益。实验覆盖3种规模LLM(1B-7B)与两个数学推理数据集，采用统一随机种子与统计检验保证结论稳健性。</p>
                    <p><span class="font-medium text-text-primary">研究结果：</span>研究发现：1)在极简bandit设定下，模型已能习得单步推理模式，证明“策略初始化+大动作空间”本身即可提供足够信号；2)优势函数主要起降低方差作用，对期望回报提升贡献&lt;5%，却带来&gt;30%训练时间开销；3)当动作空间&gt;10^4时，探索噪声的调度而非奖励精度成为首要瓶颈；4)去掉冗余设计后，1B模型在GSM8K上的pass@1仅比7B完整RLFT低2.4%，但计算成本降低6.8倍。</p>
                    <p><span class="font-medium text-text-primary">局限性：</span>实验仅聚焦数学与符号推理任务，尚不清楚结论是否适用于开放域对话或代码生成等场景；bandit类比假设每轮交互独立，忽略了LLM rollout中的状态-动作关联性，可能低估长期依赖的影响；此外，极简配置虽然降低耦合，却可能遗漏多组件协同带来的非线性增益。</p>
                    
                    <p><span class="font-medium text-text-primary">未来方向：</span>未来可将bandit视角扩展至带状态马尔可夫决策过程，研究在保持理论可解释性的同时如何引入历史上下文；也可设计自适应模块开关算法，根据训练动态决定何时启用优势估计、奖励模型再训练等高成本组件。</p>
                    
                    <p class="text-accent"><span class="font-medium">研究相关性：</span>若研究者正致力于提升大模型后训练效率、解释RLFT各组件的真实作用，或希望用轻量级方法在私有数据上快速复现推理能力提升，本论文提供的可解释实验范式与“去冗余”思路可直接指导算法裁剪与资源分配。</p>
                  </div>
                </div>
              </div>
            </div>
            

            <div class="pt-4 border-t border-border-color">
              <div class="flex flex-wrap gap-3 text-xs">
                <span class="px-2 py-1 bg-bg-hover rounded text-text-secondary">相似度 0.83</span>
                <span class="px-2 py-1 bg-bg-hover rounded text-text-secondary">
                  期刊影响力 0.60
                  
                </span>
              </div>
            </div>
          </div>
        </article>
        
        <article class="bg-bg-card rounded-lg border border-border-color overflow-hidden hover:border-accent/50 transition-colors">
          <div class="p-4 md:p-5">
            <div class="mb-3">
              <div class="flex items-center gap-2 mb-2 flex-wrap">
                <span class="inline-flex items-center px-2.5 py-0.5 rounded text-xs font-medium
                  bg-green-100 text-green-700 border border-green-300
                  ">
                  必读
                </span>
                <span class="text-xs text-text-secondary">评分 0.78</span>
                <span class="text-xs text-text-secondary">·</span>
                <span class="text-xs text-text-secondary">arxiv</span>
                <span class="text-xs text-text-secondary md:hidden">· 2026-01-21</span>
              </div>
              <div class="flex items-start justify-between gap-3 md:gap-4">
                <div class="flex-1">
                  <h2 class="text-base md:text-lg font-semibold text-text-primary leading-tight">
                    <a href="https://arxiv.org/abs/2601.15380v1" target="_blank" rel="noopener" class="hover:text-accent transition-colors">
                      You Need Better Attention Priors
                    </a>
                  </h2>
                  
                  <p class="text-sm text-text-secondary mt-0.5 leading-snug">你需要更好的注意力先验</p>
                  
                </div>
                <div class="hidden md:block text-right text-sm text-text-secondary flex-shrink-0">
                  <div class="font-medium">2026-01-21</div>
                </div>
              </div>
              <div class="mt-1 text-xs text-text-secondary" title="arXiv">
                arXiv
                
              </div>
            </div>

            <p class="text-sm text-text-secondary mb-3">
              Elon Litman，Gabe Guo
            </p>

            <div class="flex flex-wrap gap-x-4 gap-y-1 text-xs text-text-secondary mb-4">
              
              <span class="flex items-center">
                <span class="font-medium mr-1">ID:</span>
                <span>http://arxiv.org/abs/2601.15380v1</span>
              </span>
              
            </div>

            
            <div class="bg-bg-hover/50 rounded-lg p-4 mb-4 border border-border-color">
              <button id="btn-abstract-30" onclick="toggleSection('abstract-30')"
                      class="w-full text-left text-sm font-medium text-text-primary flex items-center justify-between">
                <span class="flex items-center">
                  <svg class="w-4 h-4 mr-1.5 text-text-secondary" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                    <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M9 12h6m-6 4h6m2 5H7a2 2 0 01-2-2V5a2 2 0 012-2h5.586a1 1 0 01.707.293l5.414 5.414a1 1 0 01.293.707V19a2 2 0 01-2 2z"/>
                  </svg>
                  原文摘要
                </span>
                <svg class="w-4 h-4 text-text-secondary transform transition-transform" id="icon-abstract-30" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                  <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
                </svg>
              </button>
              <div id="abstract-30" class="section-expand collapsed">
                <p class="text-sm text-text-secondary mt-3 leading-relaxed">We generalize the attention mechanism by viewing it through the lens of Entropic Optimal Transport, revealing that standard attention corresponds to a transport problem regularized by an implicit uniform prior. We introduce Generalized Optimal transport Attention with Trainable priors (GOAT), a new attention mechanism that replaces this naive assumption with a learnable, continuous prior. This prior maintains full compatibility with optimized kernels such as FlashAttention. GOAT also provides an EOT-based explanation of attention sinks and materializes a solution for them, avoiding the representational trade-offs of standard attention. Finally, by absorbing spatial information into the core attention computation, GOAT learns an extrapolatable prior that combines the flexibility of learned positional embeddings with the length generalization of fixed encodings.</p>
              </div>
            </div>
            

            
            <div class="bg-accent/10 rounded-lg p-4 mb-4 border border-accent/30">
              <h3 class="text-sm font-semibold text-accent mb-3 flex items-center">
                <svg class="w-4 h-4 mr-1.5" fill="currentColor" viewBox="0 0 24 24">
                  <path d="M12 2a2 2 0 012 2c0 .74-.4 1.39-1 1.73V7h1a7 7 0 017 7h1a1 1 0 011 1v3a1 1 0 01-1 1h-1v1a2 2 0 01-2 2H5a2 2 0 01-2-2v-1H2a1 1 0 01-1-1v-3a1 1 0 011-1h1a7 7 0 017-7h1V5.73c-.6-.34-1-.99-1-1.73a2 2 0 012-2z"/>
                </svg>
                AI 总结
              </h3>
              <div class="space-y-2 text-sm text-text-primary">
                <p><span class="font-medium text-accent">研究问题：</span>如何让注意力机制摆脱对均匀先验的默认假设，提升长度外推与表示能力。</p>
                <p><span class="font-medium text-accent">研究方法：</span>以熵正则最优传输重构注意力，提出可训练连续先验的GOAT模块并兼容FlashAttention。</p>
                <p><span class="font-medium text-accent">主要发现：</span>GOAT解释并消除attention sinks，在长度外推任务上显著优于标准与位置嵌入注意力。</p>
                <p><span class="font-medium text-accent">创新点：</span>首次将可学习最优传输先验嵌入注意力核函数，兼顾高效实现与长度泛化。</p>
                
                <p><span class="font-medium text-accent">相关性：</span>为Transformer类模型提供更灵活的位置建模与长度扩展方案，直接惠及长文本、视觉等应用。</p>
                
              </div>

              <div class="mt-3 pt-3 border-t border-accent/30">
                <button id="btn-detail-30" onclick="toggleSection('detail-30')"
                        class="text-xs text-accent hover:text-accent-hover font-medium flex items-center">
                  <svg class="w-3 h-3 mr-1" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                    <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
                  </svg>
                  查看详细分析
                </button>
                <div id="detail-30" class="section-expand collapsed mt-3">
                  <div class="bg-bg-card rounded-lg p-4 text-sm text-text-primary space-y-3 border border-border-color">
                    <p><span class="font-medium text-text-primary">研究背景：</span>Transformer 中的 softmax 注意力被普遍视为对查询-键相似度进行归一化的技巧，缺乏对“为何这样归一化”的第一性原理解释。近期熵正则最优传输（EOT）理论被用来重新阐释注意力，但仍默认使用均匀先验，限制了模型对位置或语义偏置的灵活编码。作者受此启发，希望用可学习的先验替代均匀假设，以统一解释并改进注意力设计。</p>
                    <p><span class="font-medium text-text-primary">方法详情：</span>作者将注意力抽象为带熵正则的最优传输问题，指出标准注意力等价于在均匀先验下的特例，并推导出其 EOT 目标函数。在此基础上提出 GOAT，用可训练、连续的概率测度作为先验，嵌入传输代价矩阵中；该先验与 FlashAttention 等 IO 优化核兼容，仅需在 softmax 前加一项与先验相关的偏置。GOAT 同时把空间坐标信息直接吸收进代价函数，使先验随序列长度平滑外推，无需额外位置编码。</p>
                    <p><span class="font-medium text-text-primary">研究结果：</span>实验表明 GOAT 在语言建模、图像分类与长程依赖任务上均优于标准注意力，且训练/推理开销几乎相同。理论推导首次给出“注意力汇聚”现象的 EOT 视角，并证明 GOAT 的先验可自动抑制汇聚 token 的过度权重，从而避免表示塌陷。外推测试显示，GOAT 在序列长度 2×–4× 于训练长度时仍保持较低困惑度，兼顾了可学习位置嵌入的灵活性与固定编码的长度泛化能力。</p>
                    <p><span class="font-medium text-text-primary">局限性：</span>GOAT 引入的连续先验需额外内存存储其参数，对极宽模型或超长序列仍可能带来显存开销。目前仅在编码器或自回归解码器的小到中型模型上验证，尚未在百亿参数规模或多模态大模型中测试其稳定性与收敛性。理论分析假设代价矩阵满足特定有界性，极端稀疏或动态图场景下的收敛保证尚未讨论。</p>
                    
                    <p><span class="font-medium text-text-primary">未来方向：</span>探索将 GOAT 与稀疏注意力模式结合，进一步降低长序列计算复杂度；研究先验的元学习或层次化超网络，以在任务间快速迁移注意力偏置。</p>
                    
                    <p class="text-accent"><span class="font-medium">研究相关性：</span>若研究者关注注意力机制的理论解释、长度外推、位置编码设计或高效 Transformer 实现，GOAT 提供了一种统一的可学习先验框架，可直接替换标准注意力并兼容现有优化库，具有即插即用的潜力。</p>
                  </div>
                </div>
              </div>
            </div>
            

            <div class="pt-4 border-t border-border-color">
              <div class="flex flex-wrap gap-3 text-xs">
                <span class="px-2 py-1 bg-bg-hover rounded text-text-secondary">相似度 0.83</span>
                <span class="px-2 py-1 bg-bg-hover rounded text-text-secondary">
                  期刊影响力 0.60
                  
                </span>
              </div>
            </div>
          </div>
        </article>
        
      </div>
    </div>
  </main>

  <footer class="bg-bg-card border-t border-border-color mt-12">
    <div class="content-container py-6 text-center text-sm text-text-secondary">
      灵感来自 <a href="https://github.com/Yorks0n/ZotWatch" class="text-accent hover:text-accent-hover transition-colors">ZotWatch</a>
    </div>
  </footer>

  <script>
    function toggleSection(id) {
      const el = document.getElementById(id);
      const btn = document.getElementById('btn-' + id);
      const icon = document.getElementById('icon-' + id);
      const hint = document.getElementById('hint-' + id);
      if (el.classList.contains('collapsed')) {
        el.classList.remove('collapsed');
        el.classList.add('expanded');
        if (btn && btn.textContent.includes('详细分析')) {
          btn.innerHTML = '<svg class="w-3 h-3 mr-1" fill="none" stroke="currentColor" viewBox="0 0 24 24"><path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M5 15l7-7 7 7"/></svg>收起详细分析';
        }
        if (hint) hint.textContent = '点击收起';
        if (icon) icon.style.transform = 'rotate(180deg)';
        if (id === 'researcher-profile') {
          setTimeout(() => { window.dispatchEvent(new Event('resize')); }, 100);
        }
      } else {
        el.classList.remove('expanded');
        el.classList.add('collapsed');
        if (btn && btn.textContent.includes('详细分析')) {
          btn.innerHTML = '<svg class="w-3 h-3 mr-1" fill="none" stroke="currentColor" viewBox="0 0 24 24"><path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/></svg>查看详细分析';
        }
        if (hint) hint.textContent = '点击展开';
        if (icon) icon.style.transform = 'rotate(0deg)';
      }
    }
  </script>
</body>
</html>