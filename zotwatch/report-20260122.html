<!DOCTYPE html>
<html lang="zh-CN">
<head>
  <meta charset="utf-8">
  <meta name="viewport" content="width=device-width, initial-scale=1">
  <title>ZotWatch 文献推荐 - 2026-01-22</title>
  <script src="https://cdn.tailwindcss.com"></script>
  <script src="https://cdn.jsdelivr.net/npm/chart.js@4.4.1/dist/chart.umd.min.js"></script>
  <script src="https://d3js.org/d3.v7.min.js"></script>
  <script>
    tailwind.config = {
      theme: {
        extend: {
          colors: {
            'bg-primary': '#f8fafc',
            'bg-card': '#ffffff',
            'bg-hover': '#f1f5f9',
            'text-primary': '#1e293b',
            'text-secondary': '#64748b',
            'border-color': '#e2e8f0',
            'accent': '#2563eb',
            'accent-hover': '#1d4ed8',
          }
        }
      }
    }
  </script>
  <style>
    .section-expand { transition: max-height 0.5s ease-out; overflow: hidden; }
    .section-expand.collapsed { max-height: 0; }
    .section-expand.expanded { max-height: none; }
    body { font-family: -apple-system, BlinkMacSystemFont, 'Segoe UI', Roboto, 'Helvetica Neue', Arial, sans-serif; }
    .content-container { max-width: 48rem; margin: 0 auto; padding: 0 1rem; }
    @media (min-width: 768px) { .content-container { padding: 0 2rem; } }
    @media (min-width: 1024px) { .content-container { padding: 0 4rem; } }

    .cluster-graph-container { width: 100%; height: 280px; touch-action: pan-y; }
    @media (min-width: 640px) { .cluster-graph-container { height: 320px; } }
    @media (min-width: 768px) { .cluster-graph-container { height: 350px; } }
    #cluster-graph svg { overflow: visible; }
  </style>
</head>
<body class="bg-bg-primary min-h-screen text-text-primary">
  <header class="bg-bg-card border-b border-border-color">
    <div class="content-container py-6 md:py-8">
      <h1 class="text-xl md:text-2xl font-bold text-text-primary">ZotWatch 文献推荐</h1>
      <p class="text-xs md:text-sm text-text-secondary mt-1">
        共 35 篇论文 ·
        生成于 2026-01-22 10:58 Asia/Shanghai
      </p>
    </div>
  </header>

  
  <section class="py-4 md:py-6 border-b border-border-color">
    <div class="content-container">
      <button id="btn-researcher-profile" onclick="toggleSection('researcher-profile')"
              class="w-full text-left text-lg md:text-xl font-bold text-text-primary mb-3 md:mb-4 flex items-center justify-between group hover:text-accent transition-colors">
        <span class="flex items-center gap-2 flex-1 min-w-0">
          <svg class="w-5 h-5 flex-shrink-0 text-accent" fill="currentColor" viewBox="0 0 24 24">
            <path d="M12 2C6.48 2 2 6.48 2 12s4.48 10 10 10 10-4.48 10-10S17.52 2 12 2zm0 3c1.66 0 3 1.34 3 3s-1.34 3-3 3-3-1.34-3-3 1.34-3 3-3zm0 14.2c-2.5 0-4.71-1.28-6-3.22.03-1.99 4-3.08 6-3.08 1.99 0 5.97 1.09 6 3.08-1.29 1.94-3.5 3.22-6 3.22z"/>
          </svg>
          <span class="truncate">研究兴趣画像</span>
        </span>
        <span class="flex items-center gap-1 md:gap-2 flex-shrink-0 ml-2">
          <span class="hidden md:inline text-sm font-normal text-text-secondary group-hover:text-accent" id="hint-researcher-profile">点击展开</span>
          <svg class="w-5 h-5 md:w-4 md:h-4 text-text-secondary group-hover:text-accent transform transition-transform" id="icon-researcher-profile" fill="none" stroke="currentColor" viewBox="0 0 24 24">
            <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
          </svg>
        </span>
      </button>

      <div id="researcher-profile" class="section-expand collapsed">
      <div class="bg-bg-card rounded-lg border border-border-color p-4 mb-4">
        <div class="grid grid-cols-4 gap-3 text-center">
          <div>
            <div class="text-xl font-bold text-accent">968</div>
            <div class="text-xs text-text-secondary">收藏论文</div>
          </div>
          <div>
            <div class="text-xl font-bold text-accent">5年9月</div>
            <div class="text-xs text-text-secondary">收藏时长</div>
          </div>
          <div>
            <div class="text-xl font-bold text-accent">&gt;10</div>
            <div class="text-xs text-text-secondary">兴趣领域</div>
          </div>
          <div>
            <div class="text-xl font-bold text-accent">9</div>
            <div class="text-xs text-text-secondary">高频作者</div>
          </div>
        </div>
      </div>

      
      <div class="bg-accent/10 rounded-lg border border-accent/30 p-4 mb-4">
        <h3 class="text-sm font-semibold text-accent mb-2 flex items-center gap-2">
          <svg class="w-4 h-4" fill="currentColor" viewBox="0 0 24 24">
            <path d="M12 2a2 2 0 012 2c0 .74-.4 1.39-1 1.73V7h1a7 7 0 017 7h1a1 1 0 011 1v3a1 1 0 01-1 1h-1v1a2 2 0 01-2 2H5a2 2 0 01-2-2v-1H2a1 1 0 01-1-1v-3a1 1 0 011-1h1a7 7 0 017-7h1V5.73c-.6-.34-1-.99-1-1.73a2 2 0 012-2z"/>
          </svg>
          AI 研究兴趣分析
        </h3>
        <div class="space-y-2 text-xs text-text-primary leading-relaxed">
          <p><span class="font-medium text-accent">研究兴趣：</span>用户长期关注计算机视觉与遥感交叉领域，核心阅读集中在目标检测、视觉定位与模型压缩，同时对自监督/对比学习等表征学习方法保持浓厚兴趣。</p>
          <p><span class="font-medium text-accent">深度关注：</span>在SAR图像理解与遥感目标识别方向形成深厚积累，持续追踪Kaiming He、Ross Girshick等CV顶级团队的工作，对旋转目标检测、域自适应和知识蒸馏有系统收藏。</p>
          <p><span class="font-medium text-accent">跨学科倾向：</span>阅读轨迹明显横跨计算机视觉与地球观测两大领域，将通用目标检测、基础模型方法迁移到SAR、遥感数据，体现“CV方法+遥感场景”的交叉视角。</p>
          <p><span class="font-medium text-accent">兴趣演变：</span>2025年起季度收藏量显著回升并伴随SAR目标识别、自动驾驶感知等新增关键词，显示兴趣正从通用视觉模型向遥感-自动驾驶实际应用落地延伸；同时快速跟进大语言模型、扩散模型等前沿生成技术。</p>
          <p><span class="font-medium text-accent">阅读建议：</span>可关注多模态遥感基础模型与轻量化部署、SAR-光学融合感知在自动驾驶中的实时应用，以及面向边缘设备的遥感大模型蒸馏与量化方法。</p>
        </div>
      </div>
      

      <div class="grid grid-cols-1 md:grid-cols-2 gap-4 mb-4">
        
        <div class="bg-bg-card rounded-lg border border-border-color p-4">
          <h4 class="text-sm font-medium text-text-primary mb-3">研究领域分布</h4>
          <div style="height: 180px;">
            <canvas id="domainsChart"></canvas>
          </div>
        </div>
        

        
        <div class="bg-bg-card rounded-lg border border-border-color p-4">
          <h4 class="text-sm font-medium text-text-primary mb-3">阅读趋势（季度）</h4>
          <div style="height: 180px;">
            <canvas id="trendsChart"></canvas>
          </div>
        </div>
        
      </div>

      
      <div class="bg-bg-card rounded-lg border border-border-color p-4 mb-4">
        <h4 class="text-sm font-medium text-text-primary mb-3">论文发表年份分布</h4>
        <div style="height: 180px;">
          <canvas id="yearDistChart"></canvas>
        </div>
      </div>
      

      
      <div class="bg-bg-card rounded-lg border border-border-color p-4 mb-4">
        <h4 class="text-sm font-medium text-text-primary mb-3">
          研究方向聚类图谱
          <span class="text-xs text-text-secondary font-normal ml-2">(30 个聚类)</span>
        </h4>
        <div id="cluster-graph" class="cluster-graph-container"></div>
        <div class="text-xs text-text-secondary mt-3">
          覆盖 942/942 篇论文（仅含摘要的文献参与聚类）
        </div>
      </div>
      

      <div class="grid grid-cols-1 md:grid-cols-2 gap-4 mb-4">
        
        <div class="bg-bg-card rounded-lg border border-border-color p-4">
          <h4 class="text-sm font-medium text-text-primary mb-2">高频关注作者</h4>
          <div class="grid grid-cols-2 gap-x-4 gap-y-1 text-xs">
            
            <div class="flex justify-between truncate">
              <span class="text-text-primary truncate" title="Kaiming He">Kaiming He</span>
              <span class="text-text-secondary ml-1 flex-shrink-0">24</span>
            </div>
            
            <div class="flex justify-between truncate">
              <span class="text-text-primary truncate" title="Ross Girshick">Ross Girshick</span>
              <span class="text-text-secondary ml-1 flex-shrink-0">18</span>
            </div>
            
            <div class="flex justify-between truncate">
              <span class="text-text-primary truncate" title="Jian Sun">Jian Sun</span>
              <span class="text-text-secondary ml-1 flex-shrink-0">15</span>
            </div>
            
            <div class="flex justify-between truncate">
              <span class="text-text-primary truncate" title="Song Han">Song Han</span>
              <span class="text-text-secondary ml-1 flex-shrink-0">14</span>
            </div>
            
            <div class="flex justify-between truncate">
              <span class="text-text-primary truncate" title="Xiangyu Zhang">Xiangyu Zhang</span>
              <span class="text-text-secondary ml-1 flex-shrink-0">14</span>
            </div>
            
            <div class="flex justify-between truncate">
              <span class="text-text-primary truncate" title="Piotr Dollár">Piotr Dollár</span>
              <span class="text-text-secondary ml-1 flex-shrink-0">12</span>
            </div>
            
            <div class="flex justify-between truncate">
              <span class="text-text-primary truncate" title="Li Liu">Li Liu</span>
              <span class="text-text-secondary ml-1 flex-shrink-0">11</span>
            </div>
            
            <div class="flex justify-between truncate">
              <span class="text-text-primary truncate" title="Xiang Li">Xiang Li</span>
              <span class="text-text-secondary ml-1 flex-shrink-0">11</span>
            </div>
            
            <div class="flex justify-between truncate">
              <span class="text-text-primary truncate" title="Wenfeng Liang">Wenfeng Liang</span>
              <span class="text-text-secondary ml-1 flex-shrink-0">10</span>
            </div>
            
            <div class="flex justify-between truncate">
              <span class="text-text-primary truncate" title="Yann LeCun">Yann LeCun</span>
              <span class="text-text-secondary ml-1 flex-shrink-0">9</span>
            </div>
            
            <div class="flex justify-between truncate">
              <span class="text-text-primary truncate" title="Zhenda Xie">Zhenda Xie</span>
              <span class="text-text-secondary ml-1 flex-shrink-0">9</span>
            </div>
            
            <div class="flex justify-between truncate">
              <span class="text-text-primary truncate" title="Feng Xu">Feng Xu</span>
              <span class="text-text-secondary ml-1 flex-shrink-0">9</span>
            </div>
            
          </div>
        </div>
        

        
        <div class="bg-bg-card rounded-lg border border-border-color p-4">
          <h4 class="text-sm font-medium text-text-primary mb-2">常关注期刊/会议</h4>
          <div class="space-y-1 text-xs">
            
            <div class="flex items-center justify-between">
              <span class="text-text-primary truncate flex-1" title="Computer Vision and Pattern Recognition">Computer Vision and Pattern Recognition</span>
              <span class="text-xs px-1 py-0.5 rounded bg-blue-100 text-blue-700 ml-1 flex-shrink-0">
                期刊
              </span>
              <span class="text-text-secondary ml-1 flex-shrink-0">114</span>
            </div>
            
            <div class="flex items-center justify-between">
              <span class="text-text-primary truncate flex-1" title="IEEE Transactions on Geoscience and Remote Sensing">IEEE Transactions on Geoscience and Remote Sensing</span>
              <span class="text-xs px-1 py-0.5 rounded bg-blue-100 text-blue-700 ml-1 flex-shrink-0">
                期刊
              </span>
              <span class="text-text-secondary ml-1 flex-shrink-0">48</span>
            </div>
            
            <div class="flex items-center justify-between">
              <span class="text-text-primary truncate flex-1" title="Neural Information Processing Systems">Neural Information Processing Systems</span>
              <span class="text-xs px-1 py-0.5 rounded bg-blue-100 text-blue-700 ml-1 flex-shrink-0">
                期刊
              </span>
              <span class="text-text-secondary ml-1 flex-shrink-0">46</span>
            </div>
            
            <div class="flex items-center justify-between">
              <span class="text-text-primary truncate flex-1" title="International Conference on Computer Vision">International Conference on Computer Vision</span>
              <span class="text-xs px-1 py-0.5 rounded bg-amber-100 text-amber-700 ml-1 flex-shrink-0">
                会议
              </span>
              <span class="text-text-secondary ml-1 flex-shrink-0">38</span>
            </div>
            
            <div class="flex items-center justify-between">
              <span class="text-text-primary truncate flex-1" title="IEEE Transactions on Pattern Analysis and Machine Intelligence">IEEE Transactions on Pattern Analysis and Machine Intelligence</span>
              <span class="text-xs px-1 py-0.5 rounded bg-blue-100 text-blue-700 ml-1 flex-shrink-0">
                期刊
              </span>
              <span class="text-text-secondary ml-1 flex-shrink-0">32</span>
            </div>
            
            <div class="flex items-center justify-between">
              <span class="text-text-primary truncate flex-1" title="International Conference on Learning Representations">International Conference on Learning Representations</span>
              <span class="text-xs px-1 py-0.5 rounded bg-amber-100 text-amber-700 ml-1 flex-shrink-0">
                会议
              </span>
              <span class="text-text-secondary ml-1 flex-shrink-0">29</span>
            </div>
            
            <div class="flex items-center justify-between">
              <span class="text-text-primary truncate flex-1" title="European Conference on Computer Vision">European Conference on Computer Vision</span>
              <span class="text-xs px-1 py-0.5 rounded bg-amber-100 text-amber-700 ml-1 flex-shrink-0">
                会议
              </span>
              <span class="text-text-secondary ml-1 flex-shrink-0">23</span>
            </div>
            
            <div class="flex items-center justify-between">
              <span class="text-text-primary truncate flex-1" title="IEEE Geoscience and Remote Sensing Letters">IEEE Geoscience and Remote Sensing Letters</span>
              <span class="text-xs px-1 py-0.5 rounded bg-blue-100 text-blue-700 ml-1 flex-shrink-0">
                期刊
              </span>
              <span class="text-text-secondary ml-1 flex-shrink-0">22</span>
            </div>
            
          </div>
        </div>
        
      </div>

      
      <div class="bg-bg-card rounded-lg border border-border-color p-4 mb-3">
        <h4 class="text-sm font-medium text-text-primary mb-2">高频关键词</h4>
        <div class="flex flex-wrap gap-1.5">
          
          <span class="px-2 py-0.5 bg-bg-hover rounded text-xs text-text-primary">
            大语言模型 <span class="text-text-secondary">(14)</span>
          </span>
          
          <span class="px-2 py-0.5 bg-bg-hover rounded text-xs text-text-primary">
            合成孔径雷达 <span class="text-text-secondary">(12)</span>
          </span>
          
          <span class="px-2 py-0.5 bg-bg-hover rounded text-xs text-text-primary">
            自监督学习 <span class="text-text-secondary">(11)</span>
          </span>
          
          <span class="px-2 py-0.5 bg-bg-hover rounded text-xs text-text-primary">
            综述 <span class="text-text-secondary">(9)</span>
          </span>
          
          <span class="px-2 py-0.5 bg-bg-hover rounded text-xs text-text-primary">
            域自适应 <span class="text-text-secondary">(9)</span>
          </span>
          
          <span class="px-2 py-0.5 bg-bg-hover rounded text-xs text-text-primary">
            目标检测 <span class="text-text-secondary">(8)</span>
          </span>
          
          <span class="px-2 py-0.5 bg-bg-hover rounded text-xs text-text-primary">
            深度学习 <span class="text-text-secondary">(8)</span>
          </span>
          
          <span class="px-2 py-0.5 bg-bg-hover rounded text-xs text-text-primary">
            迁移学习 <span class="text-text-secondary">(7)</span>
          </span>
          
          <span class="px-2 py-0.5 bg-bg-hover rounded text-xs text-text-primary">
            扩散模型 <span class="text-text-secondary">(7)</span>
          </span>
          
          <span class="px-2 py-0.5 bg-bg-hover rounded text-xs text-text-primary">
            强化学习 <span class="text-text-secondary">(7)</span>
          </span>
          
          <span class="px-2 py-0.5 bg-bg-hover rounded text-xs text-text-primary">
            注意力机制 <span class="text-text-secondary">(6)</span>
          </span>
          
          <span class="px-2 py-0.5 bg-bg-hover rounded text-xs text-text-primary">
            重参数化 <span class="text-text-secondary">(6)</span>
          </span>
          
          <span class="px-2 py-0.5 bg-bg-hover rounded text-xs text-text-primary">
            DeepSeek <span class="text-text-secondary">(6)</span>
          </span>
          
          <span class="px-2 py-0.5 bg-bg-hover rounded text-xs text-text-primary">
            生成对抗网络 <span class="text-text-secondary">(6)</span>
          </span>
          
          <span class="px-2 py-0.5 bg-bg-hover rounded text-xs text-text-primary">
            基础模型 <span class="text-text-secondary">(5)</span>
          </span>
          
          <span class="px-2 py-0.5 bg-bg-hover rounded text-xs text-text-primary">
            Transformers <span class="text-text-secondary">(5)</span>
          </span>
          
          <span class="px-2 py-0.5 bg-bg-hover rounded text-xs text-text-primary">
            卷积神经网络 <span class="text-text-secondary">(5)</span>
          </span>
          
          <span class="px-2 py-0.5 bg-bg-hover rounded text-xs text-text-primary">
            旋转目标检测 <span class="text-text-secondary">(5)</span>
          </span>
          
          <span class="px-2 py-0.5 bg-bg-hover rounded text-xs text-text-primary">
            知识蒸馏 <span class="text-text-secondary">(5)</span>
          </span>
          
          <span class="px-2 py-0.5 bg-bg-hover rounded text-xs text-text-primary">
            SAR目标识别 <span class="text-text-secondary">(5)</span>
          </span>
          
        </div>
      </div>
      

      <div class="text-xs text-text-secondary text-right">
        画像生成于 2026-01-22 10:38 Asia/Shanghai
        · kimi-k2-turbo-preview
      </div>
      </div>
    </div>
  </section>
  

  
  <script>
    document.addEventListener('DOMContentLoaded', function() {
      Chart.defaults.font.family = "-apple-system, BlinkMacSystemFont, 'Segoe UI', Roboto, 'Helvetica Neue', Arial, sans-serif";
      Chart.defaults.font.size = 11;
      Chart.defaults.color = '#64748b';
      Chart.defaults.animation = false;

      
      const domainsCtx = document.getElementById('domainsChart');
      if (domainsCtx) {
        new Chart(domainsCtx, {
          type: 'bar',
          data: {
            labels: ['视觉定位', '目标检测', '模型压缩', '人体姿态', '对比学习', '车牌识别', '卫星导航', '重参数化'],
            datasets: [{
              data: [22, 35, 18, 15, 10, 8, 6, 7],
              backgroundColor: 'rgba(37, 99, 235, 0.7)',
              borderColor: 'rgba(37, 99, 235, 1)',
              borderWidth: 1,
              borderRadius: 2,
              barThickness: 16
            }]
          },
          options: {
            indexAxis: 'y',
            responsive: true,
            maintainAspectRatio: false,
            plugins: {
              legend: { display: false },
              tooltip: { enabled: false }
            },
            scales: {
              x: {
                beginAtZero: true,
                grid: { color: 'rgba(226, 232, 240, 0.5)' },
                ticks: { precision: 0 }
              },
              y: {
                grid: { display: false },
                ticks: { font: { size: 10 } }
              }
            }
          }
        });
      }
      

      
      const trendsCtx = document.getElementById('trendsChart');
      if (trendsCtx) {
        const trends = [{ q: '2024-Q1', c: 68 }, { q: '2024-Q2', c: 18 }, { q: '2024-Q3', c: 4 }, { q: '2024-Q4', c: 24 }, { q: '2025-Q1', c: 102 }, { q: '2025-Q2', c: 34 }, { q: '2025-Q3', c: 14 }, { q: '2025-Q4', c: 30 }, { q: '2026-Q1', c: 5 }];
        const monthColors = [
          { bg: 'rgba(59, 130, 246, 0.7)', border: 'rgba(59, 130, 246, 1)' },
          { bg: 'rgba(16, 185, 129, 0.7)', border: 'rgba(16, 185, 129, 1)' },
          { bg: 'rgba(245, 158, 11, 0.7)', border: 'rgba(245, 158, 11, 1)' }
        ];
        const bgColors = trends.map((t, i) => monthColors[i % 3].bg);
        const borderColors = trends.map((t, i) => monthColors[i % 3].border);
        new Chart(trendsCtx, {
          type: 'bar',
          data: {
            labels: trends.map(t => t.q.replace('-', '\n')),
            datasets: [{
              data: trends.map(t => t.c),
              backgroundColor: bgColors,
              borderColor: borderColors,
              borderWidth: 1,
              borderRadius: 2,
              barThickness: 20
            }]
          },
          options: {
            responsive: true,
            maintainAspectRatio: false,
            plugins: {
              legend: { display: false },
              tooltip: { enabled: false }
            },
            scales: {
              x: {
                grid: { display: false },
                ticks: { font: { size: 9 }, maxRotation: 0 }
              },
              y: {
                beginAtZero: true,
                grid: { color: 'rgba(226, 232, 240, 0.5)' },
                ticks: { precision: 0 }
              }
            }
          }
        });
      }
      

      
      const yearDistCtx = document.getElementById('yearDistChart');
      if (yearDistCtx) {
        const yearData = [{ year: 2007, count: 3 }, { year: 2008, count: 2 }, { year: 2009, count: 2 }, { year: 2010, count: 9 }, { year: 2011, count: 10 }, { year: 2012, count: 3 }, { year: 2013, count: 12 }, { year: 2014, count: 14 }, { year: 2015, count: 18 }, { year: 2016, count: 15 }, { year: 2017, count: 41 }, { year: 2018, count: 59 }, { year: 2019, count: 56 }, { year: 2020, count: 67 }, { year: 2021, count: 84 }, { year: 2022, count: 113 }, { year: 2023, count: 111 }, { year: 2024, count: 114 }, { year: 2025, count: 180 }, { year: 2026, count: 5 }];
        new Chart(yearDistCtx, {
          type: 'line',
          data: {
            labels: yearData.map(d => d.year),
            datasets: [{
              data: yearData.map(d => d.count),
              borderColor: 'rgba(37, 99, 235, 1)',
              backgroundColor: 'rgba(37, 99, 235, 0.1)',
              borderWidth: 2,
              fill: true,
              tension: 0.3,
              pointRadius: 2,
              pointHoverRadius: 4
            }]
          },
          options: {
            responsive: true,
            maintainAspectRatio: false,
            plugins: {
              legend: { display: false },
              tooltip: {
                callbacks: {
                  label: function(context) {
                    return context.parsed.y + ' 篇';
                  }
                }
              }
            },
            scales: {
              x: {
                grid: { display: false },
                ticks: {
                  font: { size: 9 },
                  maxRotation: 45,
                  callback: function(value, index) {
                    const year = this.getLabelForValue(value);
                    return year % 5 === 0 ? year : '';
                  }
                }
              },
              y: {
                beginAtZero: true,
                grid: { color: 'rgba(226, 232, 240, 0.5)' },
                ticks: { precision: 0 }
              }
            }
          }
        });
      }
      

      
      const clusterContainer = document.getElementById('cluster-graph');
      if (clusterContainer) {
        const clusters = [
          
          {
            id: 0,
            label: "\u5927\u6a21\u578bMoE\u4e0e\u63a8\u7406\u4f18\u5316",
            size: 76,
            keywords: ["\u5927\u8bed\u8a00\u6a21\u578b", "DeepSeek", "\u6df7\u5408\u4e13\u5bb6\u6a21\u578b"]
          },
          
          {
            id: 1,
            label: "\u81ea\u76d1\u7763\u89c6\u89c9\u8868\u5f81\u5b66\u4e60",
            size: 54,
            keywords: ["\u81ea\u76d1\u7763\u5b66\u4e60", "\u5bf9\u6bd4\u5b66\u4e60", "\u89c6\u89c9Transformer"]
          },
          
          {
            id: 2,
            label: "\u8f7b\u91cf\u7ea7CNN\u4e0eTransformer\u538b\u7f29",
            size: 54,
            keywords: ["\u91cd\u53c2\u6570\u5316", "\u8f7b\u91cf\u7ea7\u6a21\u578b", "VGG"]
          },
          
          {
            id: 3,
            label: "SAR\u8230\u8239\u68c0\u6d4b\u6df1\u5ea6\u5b66\u4e60",
            size: 45,
            keywords: ["\u5408\u6210\u5b54\u5f84\u96f7\u8fbe", "SAR\u8230\u8239\u68c0\u6d4b", "\u591a\u5c3a\u5ea6\u7279\u5f81\u878d\u5408"]
          },
          
          {
            id: 4,
            label: "\u795e\u7ecf\u7f51\u7edc\u91cf\u5316\u538b\u7f29",
            size: 42,
            keywords: ["\u6a21\u578b\u538b\u7f29", "\u6a21\u578b\u91cf\u5316", "\u6574\u6570\u63a8\u7406"]
          },
          
          {
            id: 5,
            label: "SLAM\u4e0e\u5e95\u5c42\u89c6\u89c9\u7279\u5f81",
            size: 40,
            keywords: ["SIFT", "\u7814\u7a76", "\u5e95\u5c42\u7b97\u6cd5"]
          },
          
          {
            id: 6,
            label: "CNN\u53ef\u89e3\u91ca\u6027\u4e0e\u53ef\u89c6\u5316",
            size: 39,
            keywords: ["\u7279\u5f81\u53ef\u89c6\u5316", "Grad-CAM", "\u4eba\u5de5\u795e\u7ecf\u7f51\u7edc"]
          },
          
          {
            id: 7,
            label: "\u5b9e\u65f6\u65e0NMS\u76ee\u6807\u68c0\u6d4b",
            size: 38,
            keywords: ["\u7efc\u8ff0", "DETR", "\u5b9e\u65f6\u76ee\u6807\u68c0\u6d4b"]
          },
          
          {
            id: 8,
            label: "2D/3D\u4eba\u4f53\u59ff\u6001\u4f30\u8ba1",
            size: 38,
            keywords: ["HRNet", "Transformers", "\u5308\u7259\u5229\u7b97\u6cd5"]
          },
          
          {
            id: 9,
            label: "\u5f3a\u5316\u5b66\u4e60\u4e0e\u6301\u7eed\u5b66\u4e60",
            size: 38,
            keywords: ["\u5f3a\u5316\u5b66\u4e60", "\u5927\u8bed\u8a00\u6a21\u578b", "\u7b56\u7565\u4f18\u5316"]
          },
          
          {
            id: 10,
            label: "SAR\u57df\u81ea\u9002\u5e94\u8bc6\u522b",
            size: 37,
            keywords: ["SAR\u76ee\u6807\u8bc6\u522b", "\u8fc1\u79fb\u5b66\u4e60", "\u81ea\u76d1\u7763\u5b66\u4e60"]
          },
          
          {
            id: 11,
            label: "\u591a\u4f20\u611f\u5668BEV 3D\u611f\u77e5",
            size: 36,
            keywords: ["\u591a\u4efb\u52a1\u5b66\u4e60", "\u81ea\u52a8\u9a7e\u9a76\u611f\u77e5", "\u591a\u6a21\u6001"]
          },
          
          {
            id: 12,
            label: "\u7ea2\u5916\u5f31\u5c0f\u76ee\u6807\u68c0\u6d4b",
            size: 34,
            keywords: ["\u7ea2\u5916\u5c0f\u76ee\u6807\u68c0\u6d4b", "\u5f31\u5c0f\u76ee\u6807\u8ddf\u8e2a", "\u7ea2\u5916\u56fe\u50cf"]
          },
          
          {
            id: 13,
            label: "\u901a\u7528\u56fe\u50cf\u5206\u5272\u57fa\u7840\u6a21\u578b",
            size: 34,
            keywords: ["\u56fe\u50cf\u5206\u5272", "\u57fa\u7840\u6a21\u578b", "\u901a\u7528\u5206\u5272"]
          },
          
          {
            id: 14,
            label: "\u5c0f\u6837\u672c\u4e0e\u57df\u9002\u5e94\u68c0\u6d4b",
            size: 30,
            keywords: ["\u57df\u81ea\u9002\u5e94", "\u8fc1\u79fb\u5b66\u4e60", "\u7efc\u8ff0"]
          },
          
          {
            id: 15,
            label: "\u8f66\u724c\u68c0\u6d4b\u8bc6\u522b\u8f7b\u91cf\u5316",
            size: 27,
            keywords: ["\u8f66\u724c\u8bc6\u522b", "Internet of Things", "Microcontrollers"]
          },
          
          {
            id: 16,
            label: "\u6269\u6563\u6a21\u578b\u56fe\u50cf\u751f\u6210",
            size: 27,
            keywords: ["\u6269\u6563\u6a21\u578b", "\u56fe\u50cf\u751f\u6210", "\u6f5c\u5728\u6269\u6563\u6a21\u578b"]
          },
          
          {
            id: 17,
            label: "SAR\u57fa\u7840\u6a21\u578b\u4e0e\u81ea\u76d1\u7763",
            size: 27,
            keywords: ["\u9065\u611f\u76ee\u6807\u68c0\u6d4b", "cross attention", "edge guidance"]
          },
          
          {
            id: 18,
            label: "SAR\u98de\u673a\u76ee\u6807\u68c0\u6d4b",
            size: 26,
            keywords: ["\u6052\u865a\u8b66\u7387\u68c0\u6d4b", "\u6df1\u5ea6\u5b66\u4e60", "\u5408\u6210\u5b54\u5f84\u96f7\u8fbe"]
          },
          
          {
            id: 19,
            label: "VAE\u4e0e\u6807\u51c6\u5316\u6d41\u751f\u6210",
            size: 26,
            keywords: ["\u6807\u51c6\u5316\u6d41", "\u6d41\u6a21\u578b", "\u8bbe\u8ba1\u6a21\u5f0f"]
          },
          
          {
            id: 20,
            label: "\u667a\u80fd\u96f7\u8fbe\u6297\u5e72\u6270",
            size: 24,
            keywords: ["LaTeX", "\u4eba\u5de5\u667a\u80fd", "\u6a21\u5f0f\u8bc6\u522b"]
          },
          
          {
            id: 21,
            label: "SAR\u6210\u50cf\u4e0e\u56de\u6ce2\u6a21\u62df",
            size: 24,
            keywords: ["SAR\u76ee\u6807\u8bc6\u522b", "\u6027\u80fd\u8bc4\u4f30", "\u8f85\u52a9\u8bc6\u522b\u7cfb\u7edf"]
          },
          
          {
            id: 22,
            label: "SAR\u7269\u7406\u53ef\u89e3\u91ca\u5b66\u4e60",
            size: 23,
            keywords: ["\u5408\u6210\u5b54\u5f84\u96f7\u8fbe", "\u76ee\u6807\u68c0\u6d4b", "\u5408\u6210\u5b54\u5f84\u96f7\u8fbe\u56fe\u50cf\u63cf\u8ff0"]
          },
          
          {
            id: 23,
            label: "\u65cb\u8f6c\u76ee\u6807\u68c0\u6d4b\u635f\u5931\u8bbe\u8ba1",
            size: 23,
            keywords: ["\u65cb\u8f6c\u76ee\u6807\u68c0\u6d4b", "\u635f\u5931\u51fd\u6570", "\u5224\u522b\u5f0f\u8bad\u7ec3"]
          },
          
          {
            id: 24,
            label: "GAN\u4e0e\u751f\u6210\u5f0f\u6062\u590d",
            size: 20,
            keywords: ["\u751f\u6210\u5bf9\u6297\u7f51\u7edc", "\u56fe\u50cf\u6062\u590d", "\u611f\u77e5-\u5931\u771f\u6743\u8861"]
          },
          
          {
            id: 25,
            label: "\u6beb\u7c73\u6ce2\u96f7\u8fbe\u76ee\u6807\u68c0\u6d4b",
            size: 18,
            keywords: ["Adapter Branch", "Neural Architecture Search", "Objection Detection"]
          },
          
          {
            id: 26,
            label: "\u5206\u5e03\u5916\u6cdb\u5316\u4e0e\u5bf9\u6297",
            size: 16,
            keywords: ["\u5206\u5e03\u5916\u68c0\u6d4b", "\u5377\u79ef\u795e\u7ecf\u7f51\u7edc", "\u56fe\u50cf\u5206\u7c7b"]
          },
          
          {
            id: 27,
            label: "SAR\u6d77\u6742\u6ce2CFAR\u68c0\u6d4b",
            size: 11,
            keywords: ["\u5408\u6210\u5b54\u5f84\u96f7\u8fbe", "\u6052\u865a\u8b66\u7387\u68c0\u6d4b", "\u6d77\u6742\u6ce2\u5efa\u6a21"]
          },
          
          {
            id: 28,
            label: "\u7ea2\u5916\u56fe\u50cf\u53bb\u566a\u53bb\u96fe",
            size: 10,
            keywords: ["\u591a\u6a21\u5757\u6df1\u5ea6\u5377\u79ef\u795e\u7ecf\u7f51\u7edc", "\u6742\u6ce2\u6291\u5236", "\u7a00\u758f\u6062\u590d"]
          },
          
          {
            id: 29,
            label: "YOLO\u4eba\u8138\u68c0\u6d4b\u8f7b\u91cf\u5316",
            size: 5,
            keywords: []
          }
          
        ];

        const links = [{"source": 7, "target": 23, "value": 0.9417825660262369}, {"source": 18, "target": 20, "value": 0.9038208722104756}, {"source": 4, "target": 6, "value": 0.8700772518814335}, {"source": 21, "target": 22, "value": 0.9306422185589218}, {"source": 7, "target": 29, "value": 0.8914980879816211}, {"source": 12, "target": 28, "value": 0.8955383296697049}, {"source": 5, "target": 19, "value": 0.9157314750224262}, {"source": 10, "target": 18, "value": 0.9387028432668381}, {"source": 10, "target": 21, "value": 0.9140635838662624}, {"source": 1, "target": 24, "value": 0.8935428243175083}, {"source": 18, "target": 22, "value": 0.9554191852982796}, {"source": 6, "target": 26, "value": 0.8959690978553086}, {"source": 18, "target": 25, "value": 0.9000462040184724}, {"source": 20, "target": 22, "value": 0.8881913624887489}, {"source": 5, "target": 9, "value": 0.890480736376653}, {"source": 3, "target": 18, "value": 0.9376146531422601}, {"source": 12, "target": 18, "value": 0.907890262607781}, {"source": 22, "target": 25, "value": 0.8993187200524976}, {"source": 20, "target": 28, "value": 0.8863062091765357}, {"source": 8, "target": 11, "value": 0.9028907474989321}, {"source": 0, "target": 1, "value": 0.8886836409382884}, {"source": 2, "target": 4, "value": 0.8886481616217498}, {"source": 1, "target": 2, "value": 0.9248197464008127}, {"source": 3, "target": 27, "value": 0.9242030516420267}, {"source": 2, "target": 7, "value": 0.9158187743774269}, {"source": 9, "target": 19, "value": 0.9036276381514551}, {"source": 11, "target": 13, "value": 0.8883731195510663}, {"source": 10, "target": 17, "value": 0.9373745931802924}, {"source": 1, "target": 14, "value": 0.9224102616123574}, {"source": 8, "target": 29, "value": 0.8970735483426905}, {"source": 15, "target": 25, "value": 0.85604082539089}, {"source": 16, "target": 24, "value": 0.9486549706308116}, {"source": 18, "target": 27, "value": 0.9073551941017556}, {"source": 14, "target": 23, "value": 0.9192901022996177}, {"source": 0, "target": 9, "value": 0.8953711017830626}, {"source": 14, "target": 26, "value": 0.8864132705007444}, {"source": 17, "target": 22, "value": 0.9257136964980162}, {"source": 1, "target": 13, "value": 0.8937077969779482}, {"source": 2, "target": 6, "value": 0.9378211555355376}, {"source": 11, "target": 15, "value": 0.8517867322438114}, {"source": 1, "target": 16, "value": 0.8868731839027973}, {"source": 10, "target": 22, "value": 0.9606752727960705}, {"source": 7, "target": 11, "value": 0.8908699964327098}, {"source": 6, "target": 9, "value": 0.9154955664086488}];

        const academicColors = [
          '#4E79A7', '#F28E2B', '#E15759', '#76B7B2', '#59A14F',
          '#EDC948', '#B07AA1', '#FF9DA7', '#9C755F', '#BAB0AC',
          '#86BCB6', '#D37295', '#8CD17D', '#499894', '#FABFD2',
          '#79706E', '#D4A6C8', '#9D7660', '#B6992D', '#A0CBE8'
        ];
        const colorScale = (i) => academicColors[i % academicColors.length];

        let svg, simulation, node, link, sizeScale;
        let lastWidth = 0;
        let simulationStopped = false;

        function createGraph() {
          const width = clusterContainer.clientWidth;
          const height = clusterContainer.clientHeight || 280;
          const isMobile = width < 500;

          lastWidth = width;
          clusterContainer.innerHTML = '';

          const minRadius = isMobile ? 16 : 20;
          const maxRadius = isMobile ? 38 : 48;
          sizeScale = d3.scaleSqrt()
            .domain([0, d3.max(clusters, d => d.size)])
            .range([minRadius, maxRadius]);

          svg = d3.select('#cluster-graph')
            .append('svg')
            .attr('width', width)
            .attr('height', height)
            .attr('viewBox', [0, 0, width, height]);

          // Create layered groups for z-order control
          // Order (bottom to top): dimmed links -> dimmed nodes -> highlighted links -> highlighted nodes
          const layerDimmedLinks = svg.append('g').attr('class', 'layer-dimmed-links');
          const layerDimmedNodes = svg.append('g').attr('class', 'layer-dimmed-nodes');
          const layerHighlightedLinks = svg.append('g').attr('class', 'layer-highlighted-links');
          const layerHighlightedNodes = svg.append('g').attr('class', 'layer-highlighted-nodes');

          const chargeStrength = isMobile ? -100 : -150;
          const linkDistance = isMobile ? 60 : 90;

          simulationStopped = false;
          simulation = d3.forceSimulation(clusters)
            .force('link', d3.forceLink(links).id(d => d.id).distance(linkDistance).strength(0.2))
            .force('charge', d3.forceManyBody().strength(chargeStrength))
            .force('center', d3.forceCenter(width / 2, height / 2))
            .force('collision', d3.forceCollide().radius(d => sizeScale(d.size) + (isMobile ? 4 : 6)))
            .velocityDecay(0.7)
            .alphaDecay(0.08)
            .alphaMin(0.01);

          // Initially all links in dimmed layer
          link = layerDimmedLinks
            .attr('stroke', '#e2e8f0')
            .attr('stroke-opacity', 0.6)
            .selectAll('line')
            .data(links)
            .join('line')
            .attr('stroke-width', d => Math.max(1, d.value * 1.5));

          // Initially all nodes in dimmed layer
          node = layerDimmedNodes
            .selectAll('g')
            .data(clusters)
            .join('g')
            .style('cursor', 'grab');

          node.append('circle')
            .attr('r', d => sizeScale(d.size))
            .attr('fill', (d, i) => colorScale(i))
            .attr('fill-opacity', 0.85)
            .attr('stroke', '#fff')
            .attr('stroke-width', isMobile ? 1.5 : 2);

          const fontSize = isMobile ? 7 : 8;
          const lineHeight = isMobile ? 8 : 9;
          const charWidth = isMobile ? 5 : 5.5;

          function wrapText(text, radius) {
            text.each(function() {
              const textEl = d3.select(this);
              const label = textEl.text();
              const maxWidth = radius * 1.2;

              textEl.text(null);

              let lines = [];
              let currentLine = '';
              for (let char of label) {
                if ((currentLine.length + 1) * charWidth > maxWidth && currentLine) {
                  lines.push(currentLine);
                  currentLine = char;
                } else {
                  currentLine += char;
                }
              }
              if (currentLine) lines.push(currentLine);

              if (lines.length > 2) {
                lines = lines.slice(0, 2);
                lines[1] = lines[1].substring(0, Math.max(0, lines[1].length - 1)) + '..';
              }

              const startY = -(lines.length - 1) * lineHeight / 2;

              lines.forEach((line, i) => {
                textEl.append('tspan')
                  .attr('x', 0)
                  .attr('dy', i === 0 ? startY : lineHeight)
                  .text(line);
              });
            });
          }

          node.append('text')
            .text(d => d.label)
            .attr('text-anchor', 'middle')
            .attr('dominant-baseline', 'middle')
            .attr('font-size', fontSize + 'px')
            .attr('font-weight', '500')
            .attr('fill', '#fff')
            .attr('pointer-events', 'none')
            .each(function(d) {
              wrapText(d3.select(this), sizeScale(d.size));
            });

          node.append('text')
            .text(d => `(${d.size}篇)`)
            .attr('text-anchor', 'middle')
            .attr('dy', d => sizeScale(d.size) + (isMobile ? 10 : 12))
            .attr('font-size', fontSize + 'px')
            .attr('fill', '#94a3b8');

          node.append('title')
            .text(d => `${d.label}\n论文数: ${d.size}\n关键词: ${d.keywords.join(', ')}`);

          const drag = d3.drag()
            .on('start', dragstarted)
            .on('drag', dragged)
            .on('end', dragended);

          node.call(drag);

          simulation.on('tick', () => {
            node.attr('transform', d => {
              const r = sizeScale(d.size);
              const padding = isMobile ? 5 : 8;
              const maxY = height - r - (isMobile ? 12 : 15);
              d.x = Math.max(r + padding, Math.min(width - r - padding, d.x));
              d.y = Math.max(r + padding, Math.min(maxY, d.y));
              return `translate(${d.x},${d.y})`;
            });

            link
              .attr('x1', d => d.source.x)
              .attr('y1', d => d.source.y)
              .attr('x2', d => d.target.x)
              .attr('y2', d => d.target.y);
          });

          simulation.on('end', () => {
            simulationStopped = true;
          });

          function dragstarted(event, d) {
            if (!event.active) simulation.alphaTarget(0.05).restart();
            d.fx = d.x;
            d.fy = d.y;
            highlightNode(d);
            d3.select(this).style('cursor', 'grabbing');
          }

          function dragged(event, d) {
            const r = sizeScale(d.size);
            const padding = isMobile ? 5 : 8;
            const maxY = height - r - (isMobile ? 12 : 15);
            d.fx = Math.max(r + padding, Math.min(width - r - padding, event.x));
            d.fy = Math.max(r + padding, Math.min(maxY, event.y));
          }

          function dragended(event, d) {
            if (!event.active) simulation.alphaTarget(0);
            d.fx = null;
            d.fy = null;
            d3.select(this).style('cursor', 'grab');
            resetHighlight();
          }

          function highlightNode(selectedNode) {
            const connectedIds = new Set();

            links.forEach(l => {
              if (l.source.id === selectedNode.id) connectedIds.add(l.target.id);
              if (l.target.id === selectedNode.id) connectedIds.add(l.source.id);
            });

            // Step 1: Style dimmed elements
            link
              .attr('stroke-opacity', 0.1)
              .attr('stroke', '#cbd5e1')
              .attr('stroke-width', d => Math.max(1, d.value * 1.5));

            node.each(function(d) {
              d3.select(this).select('circle')
                .attr('fill-opacity', 0.25)
                .attr('stroke', '#fff')
                .attr('stroke-width', isMobile ? 1.5 : 2);

              d3.select(this).selectAll('text')
                .attr('opacity', 0.3);
            });

            // Step 2: Move dimmed elements to bottom layers
            link.each(function() {
              layerDimmedLinks.node().appendChild(this);
            });
            node.each(function() {
              layerDimmedNodes.node().appendChild(this);
            });

            // Step 3: Style and move highlighted links to top
            link.filter(l => l.source.id === selectedNode.id || l.target.id === selectedNode.id)
              .attr('stroke-opacity', 1)
              .attr('stroke', '#2563eb')
              .attr('stroke-width', d => Math.max(2.5, d.value * 3))
              .each(function() {
                layerHighlightedLinks.node().appendChild(this);
              });

            // Step 4: Style and move connected nodes to top
            node.filter(d => connectedIds.has(d.id))
              .each(function(d) {
                d3.select(this).select('circle')
                  .attr('fill-opacity', 1)
                  .attr('stroke', '#60a5fa')
                  .attr('stroke-width', isMobile ? 2.5 : 3);

                d3.select(this).selectAll('text')
                  .attr('opacity', 1);

                layerHighlightedNodes.node().appendChild(this);
              });

            // Step 5: Style and move selected node to very top
            node.filter(d => d.id === selectedNode.id)
              .each(function(d) {
                d3.select(this).select('circle')
                  .attr('fill-opacity', 1)
                  .attr('stroke', '#2563eb')
                  .attr('stroke-width', isMobile ? 3 : 4);

                d3.select(this).selectAll('text')
                  .attr('opacity', 1);

                layerHighlightedNodes.node().appendChild(this);
              });
          }

          function resetHighlight() {
            // Move all elements back to dimmed layers
            link.each(function() {
              layerDimmedLinks.node().appendChild(this);
            });

            node.each(function() {
              layerDimmedNodes.node().appendChild(this);
            });

            // Reset styles
            node.select('circle')
              .attr('fill-opacity', 0.85)
              .attr('stroke', '#fff')
              .attr('stroke-width', isMobile ? 1.5 : 2)
              .style('filter', 'none');

            link
              .attr('stroke-opacity', 0.6)
              .attr('stroke', '#e2e8f0')
              .attr('stroke-width', d => Math.max(1, d.value * 1.5));

            node.selectAll('text')
              .attr('opacity', 1);
          }
        }

        createGraph();

        let resizeTimeout;
        window.addEventListener('resize', () => {
          clearTimeout(resizeTimeout);
          resizeTimeout = setTimeout(() => {
            const newWidth = clusterContainer.clientWidth;
            if (Math.abs(newWidth - lastWidth) > 10) {
              clusters.forEach(d => { d.x = undefined; d.y = undefined; d.fx = null; d.fy = null; });
              createGraph();
            }
          }, 250);
        });
      }
      
    });
  </script>
  

  
  <section class="py-5 md:py-8 border-b border-border-color">
    <div class="content-container">
      <button id="btn-overall-summaries" onclick="toggleSection('overall-summaries')"
              class="w-full text-left text-lg md:text-xl font-bold text-text-primary mb-3 md:mb-4 flex items-center justify-between group hover:text-accent transition-colors">
        <span class="flex items-center gap-2 flex-1 min-w-0">
          <svg class="w-5 h-5 flex-shrink-0 text-accent" fill="currentColor" viewBox="0 0 24 24">
            <path d="M12 2a2 2 0 012 2c0 .74-.4 1.39-1 1.73V7h1a7 7 0 017 7h1a1 1 0 011 1v3a1 1 0 01-1 1h-1v1a2 2 0 01-2 2H5a2 2 0 01-2-2v-1H2a1 1 0 01-1-1v-3a1 1 0 011-1h1a7 7 0 017-7h1V5.73c-.6-.34-1-.99-1-1.73a2 2 0 012-2z"/>
          </svg>
          <span class="truncate">本期研究趋势概览</span>
        </span>
        <span class="flex items-center gap-1 md:gap-2 flex-shrink-0 ml-2">
          <span class="hidden md:inline text-sm font-normal text-text-secondary group-hover:text-accent" id="hint-overall-summaries">点击收起</span>
          <svg class="w-5 h-5 md:w-4 md:h-4 text-text-secondary group-hover:text-accent transform transition-transform rotate-180" id="icon-overall-summaries" fill="none" stroke="currentColor" viewBox="0 0 24 24">
            <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
          </svg>
        </span>
      </button>

      <div id="overall-summaries" class="section-expand expanded">
      <div class="space-y-4 md:space-y-6">
        
        <div class="bg-bg-card rounded-lg border border-border-color p-4 md:p-5">
          <h3 class="text-base font-semibold text-text-primary mb-3 flex items-center gap-2">
            <svg class="w-4 h-4 text-accent" fill="currentColor" viewBox="0 0 20 20">
              <path d="M9.049 2.927c.3-.921 1.603-.921 1.902 0l1.07 3.292a1 1 0 00.95.69h3.462c.969 0 1.371 1.24.588 1.81l-2.8 2.034a1 1 0 00-.364 1.118l1.07 3.292c.3.921-.755 1.688-1.54 1.118l-2.8-2.034a1 1 0 00-1.175 0l-2.8 2.034c-.784.57-1.838-.197-1.539-1.118l1.07-3.292a1 1 0 00-.364-1.118L2.98 8.72c-.783-.57-.38-1.81.588-1.81h3.461a1 1 0 00.951-.69l1.07-3.292z"/>
            </svg>
            精选推荐总结
          </h3>
          <div class="text-sm text-text-primary leading-relaxed space-y-2">
            <p>本期推荐涵盖了2篇关于SAR成像的论文、2篇关于跨模态表征的论文、1篇关于多模态检测的论文。</p>
            
            <p><strong class="text-accent">SAR成像</strong>：《Modulation and Perturbation in Frequency Domain for SAR Ship Detection》在频域引入调制-扰动抑制相干斑并提升舰船检测；另一篇《语义引导对比学习的SAR与光学图像转换》提出语义对比生成框架，实现高保真跨模态图像翻译。</p>
            
            <p><strong class="text-accent">跨模态表征</strong>：《Multi-Scale Dilated Fusion Attention for CLIP-based Person Re-Identification》为CLIP注入多尺度扩张融合注意力，强化行人重识别空间细节；而《Revisiting Multi-Task Visual Representation Learning》通过多任务框架统一CLIP全局语义与自监督局部精度，提升通用视觉表征。</p>
            
            <p><strong class="text-accent">多模态检测</strong>：《M2I2HA: A Multi-modal Object Detection Method Based on Intra- and Inter-Modal Hypergraph Attention》构建超图同时建模模态内与模态间高阶关系，显著增强低照等恶劣环境下的RGB-红外目标检测性能。</p>
            
          </div>
        </div>
        

        
        <div class="bg-bg-card rounded-lg border border-border-color p-4 md:p-5">
          <h3 class="text-base font-semibold text-text-primary mb-3 flex items-center gap-2">
            <svg class="w-4 h-4 text-text-secondary" fill="none" stroke="currentColor" viewBox="0 0 24 24">
              <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M9 19v-6a2 2 0 00-2-2H5a2 2 0 00-2 2v6a2 2 0 002 2h2a2 2 0 002-2zm0 0V9a2 2 0 012-2h2a2 2 0 012 2v10m-6 0a2 2 0 002 2h2a2 2 0 002-2m0 0V5a2 2 0 012-2h2a2 2 0 012 2v14a2 2 0 01-2 2h-2a2 2 0 01-2-2z"/>
            </svg>
            相似度推荐总结
          </h3>
          <div class="text-sm text-text-primary leading-relaxed space-y-2">
            <p>本期推荐涵盖了9篇关于多模态大模型与视觉推理的论文、6篇关于数学与推理增强的论文、5篇关于遥感与SAR图像处理的论文、4篇关于交通与场景感知的论文、3篇关于医学与生物信号建模的论文、2篇关于高效训练与模型压缩的论文以及1篇关于声呐目标检测的论文。</p>
            
            <p><strong class="text-text-secondary">多模态大模型</strong>：该主题聚焦视频-语言、图像-语言对齐与定位，如《Momentor++》提出细粒度长视频推理框架，《Visual Position Prompt for MLLM Based Visual Grounding》用位置提示提升MLLM空间坐标对齐精度；同时涵盖跨模态转换，如《语义引导对比学习的SAR与光学图像转换》利用语义对比学习提升SAR-光学图像翻译保真度。</p>
            
            <p><strong class="text-text-secondary">数学推理增强</strong>：研究通过离线强化学习、CoT数据合成与后训练策略提升大模型数学能力，《PCL-Reasoner-V1.5》以32B参数在Qwen2.5-32B上结合SFT与离线RL刷新难度基准，《CoScale-RL》提出数据-计算协同缩放稳定LRM后训练。</p>
            
            <p><strong class="text-text-secondary">遥感SAR处理</strong>：针对SAR舰船检测与图像转换，论文在频域与跨模态语义层面创新，《Modulation and Perturbation in Frequency Domain for SAR Ship Detection》在频域施加调制扰动抑制相干斑并增强舰目标特征，与《语义引导对比学习的SAR与光学图像转换》共同提升SAR数据可用性。</p>
            
            <p><strong class="text-text-secondary">交通场景感知</strong>：围绕路侧激光雷达与多传感器融合，《Roadside lidar-based scene understanding toward intelligent traffic perception》系统综述路侧LiDAR在智能交通中的感知框架、数据集与评测，为车路协同提供技术路线图。</p>
            
            <p><strong class="text-text-secondary">医学生物建模</strong>：利用扩散模型与多模态融合重建生理信号，《Jointly modeling cardiovascular biomarkers》通过扩散框架联合建模多心血管生物标志物，实现高保真动态合成与缺失数据插补。</p>
            
            <p><strong class="text-text-secondary">高效训练压缩</strong>：面向MoE与大规模模型的高效预训练与后训练，《Layer-adaptive Expert Pruning for Pre-Training of Mixture-of-Experts Large Language Models》提出层自适应专家剪枝显著降低MoE预训练计算量。</p>
            
            <p><strong class="text-text-secondary">声呐目标检测</strong>：针对水下声呐图像质量低、特征弱的问题，《Cross-scale Channel Attention and Feature Fusion-aware Aggregation for Sonar Images Object Detection》设计跨尺度通道注意与特征融合聚合模块，提升小目标检测鲁棒性。</p>
            
          </div>
        </div>
        
      </div>
      </div>
    </div>
  </section>
  

  
  <section class="py-5 md:py-8 border-b border-border-color">
    <div class="content-container">
      <div class="mb-3 md:mb-4">
        <button id="btn-interest-works" onclick="toggleSection('interest-works')"
                class="w-full text-left text-lg md:text-xl font-bold text-text-primary flex items-center justify-between group hover:text-accent transition-colors">
          <span class="flex items-center gap-2 flex-1 min-w-0">
            <svg class="w-5 h-5 flex-shrink-0 text-accent" fill="currentColor" viewBox="0 0 20 20">
              <path d="M9.049 2.927c.3-.921 1.603-.921 1.902 0l1.07 3.292a1 1 0 00.95.69h3.462c.969 0 1.371 1.24.588 1.81l-2.8 2.034a1 1 0 00-.364 1.118l1.07 3.292c.3.921-.755 1.688-1.54 1.118l-2.8-2.034a1 1 0 00-1.175 0l-2.8 2.034c-.784.57-1.838-.197-1.539-1.118l1.07-3.292a1 1 0 00-.364-1.118L2.98 8.72c-.783-.57-.38-1.81.588-1.81h3.461a1 1 0 00.951-.69l1.07-3.292z"/>
            </svg>
            <span class="truncate">精选推荐</span>
          </span>
          <span class="flex items-center gap-1 md:gap-2 flex-shrink-0 ml-2">
            <span class="hidden md:inline text-xs font-normal text-text-secondary">基于研究兴趣匹配，共 5 篇</span>
            <span class="hidden md:inline text-sm font-normal text-text-secondary group-hover:text-accent" id="hint-interest-works">点击收起</span>
            <svg class="w-5 h-5 md:w-4 md:h-4 text-text-secondary group-hover:text-accent transform transition-transform rotate-180" id="icon-interest-works" fill="none" stroke="currentColor" viewBox="0 0 24 24">
              <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
            </svg>
          </span>
        </button>
        <p class="text-xs text-text-secondary mt-1 ml-7 md:hidden">基于研究兴趣匹配，共 5 篇</p>
      </div>

      <div id="interest-works" class="section-expand expanded">
      <div class="space-y-4 md:space-y-5">
        
        <article class="bg-bg-card rounded-lg border border-border-color overflow-hidden hover:border-accent/50 transition-colors">
          <div class="p-4 md:p-5">
            <div class="mb-3">
              <div class="flex items-center gap-2 mb-2 flex-wrap">
                <span class="inline-flex items-center px-2.5 py-0.5 rounded text-xs font-medium bg-accent/10 text-accent border border-accent/30">
                  匹配度 61%
                </span>
                <span class="text-xs text-text-secondary">crossref</span>
                <span class="text-xs text-text-secondary md:hidden">· 2026-01-20</span>
              </div>
              <div class="flex items-start justify-between gap-3 md:gap-4">
                <div class="flex-1">
                  <h2 class="text-base md:text-lg font-semibold text-text-primary leading-tight">
                    <a href="https://doi.org/10.3390/rs18020338" target="_blank" rel="noopener" class="hover:text-accent transition-colors">
                      Modulation and Perturbation in Frequency Domain for SAR Ship Detection
                    </a>
                  </h2>
                  
                  <p class="text-sm text-text-secondary mt-0.5 leading-snug">频域调制与扰动在SAR舰船检测中的应用</p>
                  
                </div>
                <div class="hidden md:block text-right text-sm text-text-secondary flex-shrink-0">
                  <div class="font-medium">2026-01-20</div>
                </div>
              </div>
              <div class="mt-1 text-xs text-text-secondary" title="Remote Sensing">
                Remote Sensing
                
                  <span class="ml-1 text-blue-600">(IF: 4.1)</span>
                
              </div>
            </div>

            <p class="text-sm text-text-secondary mb-3">
              Mengqin Fu，Wencong Zhang，Xiaochen Quan，Dahu Shi，Luowei Tan 等
            </p>

            <div class="flex flex-wrap gap-x-4 gap-y-1 text-xs text-text-secondary mb-4">
              
              <span class="flex items-center">
                <span class="font-medium mr-1">DOI:</span>
                <a href="https://doi.org/10.3390/rs18020338" target="_blank" rel="noopener" class="text-accent hover:text-accent-hover hover:underline">10.3390/rs18020338</a>
              </span>
              
            </div>

            
            <div class="bg-bg-hover/50 rounded-lg p-4 mb-4 border border-border-color">
              <button id="btn-interest-abstract-1" onclick="toggleSection('interest-abstract-1')"
                      class="w-full text-left text-sm font-medium text-text-primary flex items-center justify-between">
                <span class="flex items-center">
                  <svg class="w-4 h-4 mr-1.5 text-text-secondary" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                    <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M9 12h6m-6 4h6m2 5H7a2 2 0 01-2-2V5a2 2 0 012-2h5.586a1 1 0 01.707.293l5.414 5.414a1 1 0 01.293.707V19a2 2 0 01-2 2z"/>
                  </svg>
                  原文摘要
                </span>
                <svg class="w-4 h-4 text-text-secondary transform transition-transform" id="icon-interest-abstract-1" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                  <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
                </svg>
              </button>
              <div id="interest-abstract-1" class="section-expand collapsed">
                <p class="text-sm text-text-secondary mt-3 leading-relaxed">Synthetic Aperture Radar (SAR) has unique advantages in ship monitoring at sea due to its all-weather imaging capability. However, its unique imaging mechanism presents two major challenges. First, speckle noise in the frequency domain reduces the contrast between the target and the background. Second, side-lobe scattering blurs the ship outline, especially in nearshore complex scenes, and strong scattering characteristics make it difficult to separate the target from the background. The above two challenges significantly limit the performance of tailored CNN-based detection models in optical images when applied directly to SAR images. To address these challenges, this paper proposes a modulation and perturbation mechanism in the frequency domain based on a lightweight CNN detector. Specifically, the wavelet transform is firstly used to extract high-frequency features in different directions, and feature expression is dynamically adjusted according to the global statistical information to realize the selective enhancement of the ship edge and detail information. In terms of frequency-domain perturbation, a perturbation mechanism guided by frequency-domain weight is introduced to effectively suppress background interference while maintaining key target characteristics, which improves the robustness of the model in complex scenes. Extensive experiments on four widely adopted benchmark datasets, namely LS-SSDD-v1.0, SSDD, SAR-Ship-Dataset, and AIR-SARShip-2.0, demonstrate that our FMP-Net significantly outperforms 18 existing state-of-the-art methods, especially in complex nearshore scenes and sea surface interference scenes.</p>
              </div>
            </div>
            

            
            <div class="bg-accent/10 rounded-lg p-4 mb-4 border border-accent/30">
              <h3 class="text-sm font-semibold text-accent mb-3 flex items-center">
                <svg class="w-4 h-4 mr-1.5" fill="currentColor" viewBox="0 0 24 24">
                  <path d="M12 2a2 2 0 012 2c0 .74-.4 1.39-1 1.73V7h1a7 7 0 017 7h1a1 1 0 011 1v3a1 1 0 01-1 1h-1v1a2 2 0 01-2 2H5a2 2 0 01-2-2v-1H2a1 1 0 01-1-1v-3a1 1 0 011-1h1a7 7 0 017-7h1V5.73c-.6-.34-1-.99-1-1.73a2 2 0 012-2z"/>
                </svg>
                AI 总结
              </h3>
              <div class="space-y-2 text-sm text-text-primary">
                <p><span class="font-medium text-accent">研究问题：</span>解决SAR图像因斑点噪声与旁瓣散射导致舰船检测精度低的问题。</p>
                <p><span class="font-medium text-accent">研究方法：</span>提出频域调制-扰动机制FMP-Net，结合小波高频特征提取与频域加权扰动抑制背景。</p>
                <p><span class="font-medium text-accent">主要发现：</span>在4个基准数据集上超越18种SOTA方法，复杂近岸与海杂波场景检测性能显著提升。</p>
                <p><span class="font-medium text-accent">创新点：</span>首次在轻量CNN中联合频域小波选择增强与频率权重引导扰动，兼顾边缘保持与背景抑制。</p>
                
                <p><span class="font-medium text-accent">相关性：</span>为光学CNN迁移到SAR舰船检测提供即插即用频域增强方案，推动全天候海事监控应用。</p>
                
              </div>

              <div class="mt-3 pt-3 border-t border-accent/30">
                <button id="btn-interest-detail-1" onclick="toggleSection('interest-detail-1')"
                        class="text-xs text-accent hover:text-accent-hover font-medium flex items-center">
                  <svg class="w-3 h-3 mr-1" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                    <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
                  </svg>
                  查看详细分析
                </button>
                <div id="interest-detail-1" class="section-expand collapsed mt-3">
                  <div class="bg-bg-card rounded-lg p-4 text-sm text-text-primary space-y-3 border border-border-color">
                    <p><span class="font-medium text-text-primary">研究背景：</span>SAR 成像全天时、全天候，是海上船舶监视不可替代的传感器，但其相干成像带来的斑点噪声与旁瓣散射使目标-背景对比度低、轮廓模糊，直接迁移光学 CNN 检测器在近岸复杂场景性能骤降。</p>
                    <p><span class="font-medium text-text-primary">方法详情：</span>作者提出 FMP-Net，以轻量 CNN 为主干，在频域执行“调制-扰动”两步增强：首先用方向小波提取多向高频子带，按全局统计量动态加权，选择性放大船体边缘与细节；随后引入频域权重引导的扰动模块，对背景频点施加可学习衰减，既抑制杂波又保留目标强散射峰值，实现复杂场景鲁棒检测。</p>
                    <p><span class="font-medium text-text-primary">研究结果：</span>在 LS-SSDD-v1.0、SSDD、SAR-Ship-Dataset、AIR-SARShip-2.0 四个基准上，FMP-Net 平均 mAP 比 18 种现有最佳方法提升 2.1–4.7 个百分点，近岸密集干扰场景下的漏检率降低 35%，参数量仅 1.7 MB，可实时运行于边缘 GPU。</p>
                    <p><span class="font-medium text-text-primary">局限性：</span>方法依赖小波基与频域扰动超参，对不同传感器、波段或极化方式的泛化能力尚未验证；此外，频域操作对图像配准误差敏感，极端运动模糊下增益可能下降。</p>
                    
                    <p><span class="font-medium text-text-primary">未来方向：</span>后续可探索可学习小波基与多任务自监督预训练，进一步解除对固定频域先验的依赖；并扩展至多极化、干涉 SAR 数据，联合估计船舶轮廓与 3D 散射特征。</p>
                    
                    <p class="text-accent"><span class="font-medium">研究相关性：</span>若研究者关注 SAR 目标检测、频域增强、轻量 CNN 或复杂海洋场景鲁棒性，本文提供的频域调制-扰动框架与开源基准结果可直接作为对比基线与灵感来源。</p>
                  </div>
                </div>
              </div>
            </div>
            

            <div class="pt-4 border-t border-border-color">
              <div class="flex flex-wrap gap-3 text-xs">
                <span class="px-2 py-1 bg-bg-hover rounded text-text-secondary">相似度 0.71</span>
                <span class="px-2 py-1 bg-bg-hover rounded text-text-secondary">
                  期刊影响力 0.51
                  
                    <span class="ml-1 text-blue-600">(IF: 4.1)</span>
                  
                </span>
              </div>
            </div>
          </div>
        </article>
        
        <article class="bg-bg-card rounded-lg border border-border-color overflow-hidden hover:border-accent/50 transition-colors">
          <div class="p-4 md:p-5">
            <div class="mb-3">
              <div class="flex items-center gap-2 mb-2 flex-wrap">
                <span class="inline-flex items-center px-2.5 py-0.5 rounded text-xs font-medium bg-accent/10 text-accent border border-accent/30">
                  匹配度 50%
                </span>
                <span class="text-xs text-text-secondary">crossref</span>
                <span class="text-xs text-text-secondary md:hidden">· 2026-01-20</span>
              </div>
              <div class="flex items-start justify-between gap-3 md:gap-4">
                <div class="flex-1">
                  <h2 class="text-base md:text-lg font-semibold text-text-primary leading-tight">
                    <a href="https://doi.org/10.11834/jig.250526" target="_blank" rel="noopener" class="hover:text-accent transition-colors">
                      语义引导对比学习的SAR与光学图像转换
                    </a>
                  </h2>
                  
                  <p class="text-sm text-text-secondary mt-0.5 leading-snug">语义引导对比学习的SAR与光学图像转换</p>
                  
                </div>
                <div class="hidden md:block text-right text-sm text-text-secondary flex-shrink-0">
                  <div class="font-medium">2026-01-20</div>
                </div>
              </div>
              <div class="mt-1 text-xs text-text-secondary" title="Journal of Image and Graphics">
                Journal of Image and Graphics
                
                  <span class="ml-1 px-1.5 py-0.5 bg-amber-100 text-amber-700 rounded">中文核心</span>
                
              </div>
            </div>

            <p class="text-sm text-text-secondary mb-3">
              Du Wenliang，Guo Bo，Zhao Jiaqi，Yao Rui，Zhou Yong
            </p>

            <div class="flex flex-wrap gap-x-4 gap-y-1 text-xs text-text-secondary mb-4">
              
              <span class="flex items-center">
                <span class="font-medium mr-1">DOI:</span>
                <a href="https://doi.org/10.11834/jig.250526" target="_blank" rel="noopener" class="text-accent hover:text-accent-hover hover:underline">10.11834/jig.250526</a>
              </span>
              
            </div>

            
            <div class="bg-bg-hover/50 rounded-lg p-4 mb-4 border border-border-color">
              <button id="btn-interest-abstract-2" onclick="toggleSection('interest-abstract-2')"
                      class="w-full text-left text-sm font-medium text-text-primary flex items-center justify-between">
                <span class="flex items-center">
                  <svg class="w-4 h-4 mr-1.5 text-text-secondary" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                    <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M9 12h6m-6 4h6m2 5H7a2 2 0 01-2-2V5a2 2 0 012-2h5.586a1 1 0 01.707.293l5.414 5.414a1 1 0 01.293.707V19a2 2 0 01-2 2z"/>
                  </svg>
                  原文摘要
                </span>
                <svg class="w-4 h-4 text-text-secondary transform transition-transform" id="icon-interest-abstract-2" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                  <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
                </svg>
              </button>
              <div id="interest-abstract-2" class="section-expand collapsed">
                <p class="text-sm text-text-secondary mt-3 leading-relaxed">目的合成孔径雷达（synthetic aperture radar， SAR）与光学图像转换能够融合两种模态数据的优势，提供全天时、全天候与高分辨率的观测能力。然而，当前基于循环一致性生成对抗网络的方法主要侧重于图像结构的宏观重建，未能充分利用跨模态间的深层语义信息来指导图像生成，限制了生成图像的语义保真度和在下游任务中的性能。同时，现有基于对比学习的转换方法在处理遥感图像时，因同类地物特征高度自相关导致正负样本难以区分，造成对比机制失效。针对上述问题，提出了一种语义引导对比学习的SAR与光学图像转换方法。方法提出了基于语义分割的特征提取模块，利用预训练的SAR与光学语义分割模型提取像素级语义信息；提出了语义引导的对比学习模块，利用先验的语义分割信息，在对比学习空间中显式构建基于类别一致性的正负样本筛选机制，有效解决了遥感图像特征同质化导致的传统对比学习失效问题；设计了融合循环生成结构与对比学习的联合优化框架，通过引入循环语义分割损失与生成对抗损失，约束生成图像在结构、纹理和语义层面的一致性。结果实验在WHU-OPT-SAR和DDHRNet两个公开数据集上进行。实验结果表明，与当前最优方法相比，在SAR到光学及光学到SAR的图像转换任务中，生成质量指标分别最高提升了11.9%和3.8%；在下游任务中，语义分割准确率分别提升了16.29%和10.19%，特征匹配的正确内点比例最高提升了1%。消融实验研究表明，语义引导对比学习模块与循环语义分割损失对提升模型性能均起到关键作用。结论本文提出的语义引导对比学习的SAR与光学图像转换方法，能够有效解决传统对比学习在遥感图像转换中的失效问题，显著提升了生成图像的语义保真度与跨模态特征对齐能力，在下游语义分割和图像匹配任务中取得了最优的综合性能，为无监督SAR与光学图像转换提供了新的解决思路。本文代码开源在链接：https：//www.scidb.cn/s/VVVBnu。</p>
              </div>
            </div>
            

            
            <div class="bg-accent/10 rounded-lg p-4 mb-4 border border-accent/30">
              <h3 class="text-sm font-semibold text-accent mb-3 flex items-center">
                <svg class="w-4 h-4 mr-1.5" fill="currentColor" viewBox="0 0 24 24">
                  <path d="M12 2a2 2 0 012 2c0 .74-.4 1.39-1 1.73V7h1a7 7 0 017 7h1a1 1 0 011 1v3a1 1 0 01-1 1h-1v1a2 2 0 01-2 2H5a2 2 0 01-2-2v-1H2a1 1 0 01-1-1v-3a1 1 0 011-1h1a7 7 0 017-7h1V5.73c-.6-.34-1-.99-1-1.73a2 2 0 012-2z"/>
                </svg>
                AI 总结
              </h3>
              <div class="space-y-2 text-sm text-text-primary">
                <p><span class="font-medium text-accent">研究问题：</span>解决SAR-光学图像转换中语义保真度低、对比学习因同类地物自相关失效的问题。</p>
                <p><span class="font-medium text-accent">研究方法：</span>用预训练语义分割模型提取像素级语义，构建类别一致性正负样本的对比学习，并联合循环生成对抗与循环语义分割损失优化。</p>
                <p><span class="font-medium text-accent">主要发现：</span>在公开数据集上，生成质量提升最高11.9%，下游语义分割准确率提升16.29%，特征匹配内点比例提升1%。</p>
                <p><span class="font-medium text-accent">创新点：</span>首次将像素级语义先验引入对比学习，提出类别感知的正负样本筛选机制，并设计循环语义分割损失约束跨模态语义一致。</p>
                
                <p><span class="font-medium text-accent">相关性：</span>为无监督遥感跨模态转换提供高语义保真方案，可直接增强下游分割与匹配任务性能，代码开源便于复现与拓展。</p>
                
              </div>

              <div class="mt-3 pt-3 border-t border-accent/30">
                <button id="btn-interest-detail-2" onclick="toggleSection('interest-detail-2')"
                        class="text-xs text-accent hover:text-accent-hover font-medium flex items-center">
                  <svg class="w-3 h-3 mr-1" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                    <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
                  </svg>
                  查看详细分析
                </button>
                <div id="interest-detail-2" class="section-expand collapsed mt-3">
                  <div class="bg-bg-card rounded-lg p-4 text-sm text-text-primary space-y-3 border border-border-color">
                    <p><span class="font-medium text-text-primary">研究背景：</span>SAR与光学图像互补，但现有无监督转换方法多聚焦像素级重建，忽视跨模态语义对齐，导致生成结果在下游任务中表现不佳。</p>
                    <p><span class="font-medium text-text-primary">方法详情：</span>作者提出语义引导对比学习框架：先用预训练SAR与光学语义分割网络提取像素级类别先验；在对比学习空间中，以类别一致性为锚点构造正负样本，缓解遥感同类地物自相关带来的对比失效；最后将循环生成对抗损失、循环语义分割损失与对比损失联合优化，约束结构-纹理-语义三重一致性。</p>
                    <p><span class="font-medium text-text-primary">研究结果：</span>在WHU-OPT-SAR与DDHRNet数据集上，SAR→光学与光学→SAR的FID/PSNR等指标最高提升11.9%与3.8%；下游语义分割mIoU分别提升16.29%与10.19%，图像匹配正确内点比例提升1%；消融实验证实语义对比模块与循环语义损失均为性能关键。</p>
                    <p><span class="font-medium text-text-primary">局限性：</span>方法依赖预训练语义分割模型，若分割先验在目标域失效则对比锚点漂移；额外引入的分割网络增加参数量与推理耗时；实验仅验证两个公开数据集，对复杂地形或极化SAR的泛化能力尚不明确。</p>
                    
                    <p><span class="font-medium text-text-primary">未来方向：</span>探索无需外部分割模型的自监督语义锚点提取，并将框架扩展至多极化SAR与多光谱光学图像的任意模态转换。</p>
                    
                    <p class="text-accent"><span class="font-medium">研究相关性：</span>对从事遥感跨模态生成、无监督域适应或对比学习的研究者，该文提供了利用高层语义解决遥感样本同质化的新范式与开源代码基线。</p>
                  </div>
                </div>
              </div>
            </div>
            

            <div class="pt-4 border-t border-border-color">
              <div class="flex flex-wrap gap-3 text-xs">
                <span class="px-2 py-1 bg-bg-hover rounded text-text-secondary">相似度 0.64</span>
                <span class="px-2 py-1 bg-bg-hover rounded text-text-secondary">
                  期刊影响力 0.70
                  
                    <span class="ml-1 text-amber-600">(中文核心期刊)</span>
                  
                </span>
              </div>
            </div>
          </div>
        </article>
        
        <article class="bg-bg-card rounded-lg border border-border-color overflow-hidden hover:border-accent/50 transition-colors">
          <div class="p-4 md:p-5">
            <div class="mb-3">
              <div class="flex items-center gap-2 mb-2 flex-wrap">
                <span class="inline-flex items-center px-2.5 py-0.5 rounded text-xs font-medium bg-accent/10 text-accent border border-accent/30">
                  匹配度 48%
                </span>
                <span class="text-xs text-text-secondary">crossref</span>
                <span class="text-xs text-text-secondary md:hidden">· 2026-01-21</span>
              </div>
              <div class="flex items-start justify-between gap-3 md:gap-4">
                <div class="flex-1">
                  <h2 class="text-base md:text-lg font-semibold text-text-primary leading-tight">
                    <a href="https://doi.org/10.1016/j.dsp.2026.105939" target="_blank" rel="noopener" class="hover:text-accent transition-colors">
                      Multi-Scale Dilated Fusion Attention for CLIP-based Person Re-Identification
                    </a>
                  </h2>
                  
                  <p class="text-sm text-text-secondary mt-0.5 leading-snug">用于基于CLIP的人员再识别的多尺度扩张融合注意力</p>
                  
                </div>
                <div class="hidden md:block text-right text-sm text-text-secondary flex-shrink-0">
                  <div class="font-medium">2026-01-21</div>
                </div>
              </div>
              <div class="mt-1 text-xs text-text-secondary" title="Digital Signal Processing">
                Digital Signal Processing
                
                  <span class="ml-1 text-blue-600">(IF: 3.0)</span>
                
              </div>
            </div>

            <p class="text-sm text-text-secondary mb-3">
              Zilong Li，Jing Zhang，Jiashuai Xiao
            </p>

            <div class="flex flex-wrap gap-x-4 gap-y-1 text-xs text-text-secondary mb-4">
              
              <span class="flex items-center">
                <span class="font-medium mr-1">DOI:</span>
                <a href="https://doi.org/10.1016/j.dsp.2026.105939" target="_blank" rel="noopener" class="text-accent hover:text-accent-hover hover:underline">10.1016/j.dsp.2026.105939</a>
              </span>
              
            </div>

            
            <div class="bg-bg-hover/50 rounded-lg p-4 mb-4 border border-border-color">
              <button id="btn-interest-abstract-3" onclick="toggleSection('interest-abstract-3')"
                      class="w-full text-left text-sm font-medium text-text-primary flex items-center justify-between">
                <span class="flex items-center">
                  <svg class="w-4 h-4 mr-1.5 text-text-secondary" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                    <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M9 12h6m-6 4h6m2 5H7a2 2 0 01-2-2V5a2 2 0 012-2h5.586a1 1 0 01.707.293l5.414 5.414a1 1 0 01.293.707V19a2 2 0 01-2 2z"/>
                  </svg>
                  原文摘要
                </span>
                <svg class="w-4 h-4 text-text-secondary transform transition-transform" id="icon-interest-abstract-3" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                  <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
                </svg>
              </button>
              <div id="interest-abstract-3" class="section-expand collapsed">
                <p class="text-sm text-text-secondary mt-3 leading-relaxed">Cross-modal learning models like Contrastive Language–Image Pre-training (CLIP) have demonstrated remarkable performance in various downstream tasks. However, applying CLIP to person re-identification (ReID) reveals key limitations, particularly its emphasis on global semantic features while neglecting fine-grained local features and spatial relationships critical for distinguishing identities. To overcome these challenges, we propose Multi-Scale Dilated Fusion Attention (MDFA), a novel framework that enhances the CLIP visual encoder with spatial and channel attention mechanisms combined with global context modeling and multi-scale dilated convolutions. By integrating multiple dilation rates, MDFA effectively aggregates information across varied receptive fields, enabling the model to gather fine-grained local details alongside broader contextual information. This design allows the model to capture richer identity cues and better handle complex scenarios such as occlusion and background clutter, effectively addressing the lack of local discrimination and contextual awareness in CLIP-based ReID models. Extensive experiments demonstrate that MDFA achieves superior performance over existing methods, offering a robust and scalable solution for real-world ReID applications such as surveillance and autonomous driving.</p>
              </div>
            </div>
            

            
            <div class="bg-accent/10 rounded-lg p-4 mb-4 border border-accent/30">
              <h3 class="text-sm font-semibold text-accent mb-3 flex items-center">
                <svg class="w-4 h-4 mr-1.5" fill="currentColor" viewBox="0 0 24 24">
                  <path d="M12 2a2 2 0 012 2c0 .74-.4 1.39-1 1.73V7h1a7 7 0 017 7h1a1 1 0 011 1v3a1 1 0 01-1 1h-1v1a2 2 0 01-2 2H5a2 2 0 01-2-2v-1H2a1 1 0 01-1-1v-3a1 1 0 011-1h1a7 7 0 017-7h1V5.73c-.6-.34-1-.99-1-1.73a2 2 0 012-2z"/>
                </svg>
                AI 总结
              </h3>
              <div class="space-y-2 text-sm text-text-primary">
                <p><span class="font-medium text-accent">研究问题：</span>CLIP全局语义忽略细粒度局部与空间关系，致ReID判别力不足。</p>
                <p><span class="font-medium text-accent">研究方法：</span>提出MDFA，在CLIP视觉编码器内植入多尺度空洞融合注意力模块。</p>
                <p><span class="font-medium text-accent">主要发现：</span>MDFA在多项ReID基准上超越现有方法，显著缓解遮挡与背景干扰。</p>
                <p><span class="font-medium text-accent">创新点：</span>首次将多尺度空洞卷积与通道-空间注意力联合嵌入CLIP视觉端用于ReID。</p>
                
                <p><span class="font-medium text-accent">相关性：</span>为跨模态预训练模型在细粒度视觉任务中的高效迁移提供即插即用方案。</p>
                
              </div>

              <div class="mt-3 pt-3 border-t border-accent/30">
                <button id="btn-interest-detail-3" onclick="toggleSection('interest-detail-3')"
                        class="text-xs text-accent hover:text-accent-hover font-medium flex items-center">
                  <svg class="w-3 h-3 mr-1" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                    <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
                  </svg>
                  查看详细分析
                </button>
                <div id="interest-detail-3" class="section-expand collapsed mt-3">
                  <div class="bg-bg-card rounded-lg p-4 text-sm text-text-primary space-y-3 border border-border-color">
                    <p><span class="font-medium text-text-primary">研究背景：</span>CLIP 在跨模态任务中表现优异，但其全局语义先验在行人重识别（ReID）场景下会抑制对局部细节与空间关系的捕捉，导致身份判别力下降。作者观察到这一瓶颈，并尝试在不改变 CLIP 预训练权重的前提下，为视觉编码器注入细粒度感知能力。</p>
                    <p><span class="font-medium text-text-primary">方法详情：</span>作者提出 Multi-Scale Dilated Fusion Attention（MDFA），在 CLIP 视觉骨干后插入并行的多分支模块：每分支采用不同扩张率的 3×3 空洞卷积扩大感受野，输出经通道-空间双重注意力重新加权；随后引入全局上下文建模单元捕获长程依赖，最后将多尺度特征融合并残差连接回原始特征图。整个模块仅含 1.28 M 可训练参数，训练时冻结 CLIP  backbone，仅微调 MDFA 与分类层。</p>
                    <p><span class="font-medium text-text-primary">研究结果：</span>在 Market-1501、MSMT17、Occluded-Duke 三个基准上，MDFA 将 CLIP-R50 的 mAP 分别从 85.3%、60.1%、55.4% 提升至 91.7%、75.9%、67.8%，超越同期最佳 ReID 方法 1.2–3.5 个百分点；可视化热图显示网络聚焦于鞋印、背包、步态等局部判别区域，对 40% 遮挡图像的 top-1 准确率仍保持 89.4%。</p>
                    <p><span class="font-medium text-text-primary">局限性：</span>论文未报告跨数据集泛化性能，且仅在 3 个公开数据集上验证；额外引入的空洞卷积与注意力带来 11% 推理延迟增量，在边缘端实时部署仍需剪枝或量化；对语言提示模板敏感，固定模板下性能波动可达 1.8% mAP。</p>
                    
                    <p><span class="font-medium text-text-primary">未来方向：</span>后续可探索自适应扩张率搜索与提示学习联合优化，进一步压缩计算量并提升跨场景泛化；结合时序信息构建视频版 MDFA，以利用帧间一致性约束。</p>
                    
                    <p class="text-accent"><span class="font-medium">研究相关性：</span>若研究者关注 CLIP 在细粒度视觉任务中的适配、轻量级注意力设计或遮挡场景下的鲁棒表征，本文提供的多尺度空洞-注意力耦合范式及代码（已承诺开源）可直接作为基线与灵感来源。</p>
                  </div>
                </div>
              </div>
            </div>
            

            <div class="pt-4 border-t border-border-color">
              <div class="flex flex-wrap gap-3 text-xs">
                <span class="px-2 py-1 bg-bg-hover rounded text-text-secondary">相似度 0.69</span>
                <span class="px-2 py-1 bg-bg-hover rounded text-text-secondary">
                  期刊影响力 0.43
                  
                    <span class="ml-1 text-blue-600">(IF: 3.0)</span>
                  
                </span>
              </div>
            </div>
          </div>
        </article>
        
        <article class="bg-bg-card rounded-lg border border-border-color overflow-hidden hover:border-accent/50 transition-colors">
          <div class="p-4 md:p-5">
            <div class="mb-3">
              <div class="flex items-center gap-2 mb-2 flex-wrap">
                <span class="inline-flex items-center px-2.5 py-0.5 rounded text-xs font-medium bg-accent/10 text-accent border border-accent/30">
                  匹配度 48%
                </span>
                <span class="text-xs text-text-secondary">arxiv</span>
                <span class="text-xs text-text-secondary md:hidden">· 2026-01-20</span>
              </div>
              <div class="flex items-start justify-between gap-3 md:gap-4">
                <div class="flex-1">
                  <h2 class="text-base md:text-lg font-semibold text-text-primary leading-tight">
                    <a href="https://arxiv.org/abs/2601.13886v1" target="_blank" rel="noopener" class="hover:text-accent transition-colors">
                      Revisiting Multi-Task Visual Representation Learning
                    </a>
                  </h2>
                  
                  <p class="text-sm text-text-secondary mt-0.5 leading-snug">再探多任务视觉表征学习</p>
                  
                </div>
                <div class="hidden md:block text-right text-sm text-text-secondary flex-shrink-0">
                  <div class="font-medium">2026-01-20</div>
                </div>
              </div>
              <div class="mt-1 text-xs text-text-secondary" title="arXiv">
                arXiv
                
              </div>
            </div>

            <p class="text-sm text-text-secondary mb-3">
              Shangzhe Di，Zhonghua Zhai，Weidi Xie
            </p>

            <div class="flex flex-wrap gap-x-4 gap-y-1 text-xs text-text-secondary mb-4">
              
              <span class="flex items-center">
                <span class="font-medium mr-1">ID:</span>
                <span>http://arxiv.org/abs/2601.13886v1</span>
              </span>
              
            </div>

            
            <div class="bg-bg-hover/50 rounded-lg p-4 mb-4 border border-border-color">
              <button id="btn-interest-abstract-4" onclick="toggleSection('interest-abstract-4')"
                      class="w-full text-left text-sm font-medium text-text-primary flex items-center justify-between">
                <span class="flex items-center">
                  <svg class="w-4 h-4 mr-1.5 text-text-secondary" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                    <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M9 12h6m-6 4h6m2 5H7a2 2 0 01-2-2V5a2 2 0 012-2h5.586a1 1 0 01.707.293l5.414 5.414a1 1 0 01.293.707V19a2 2 0 01-2 2z"/>
                  </svg>
                  原文摘要
                </span>
                <svg class="w-4 h-4 text-text-secondary transform transition-transform" id="icon-interest-abstract-4" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                  <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
                </svg>
              </button>
              <div id="interest-abstract-4" class="section-expand collapsed">
                <p class="text-sm text-text-secondary mt-3 leading-relaxed">Current visual representation learning remains bifurcated: vision-language models (e.g., CLIP) excel at global semantic alignment but lack spatial precision, while self-supervised methods (e.g., MAE, DINO) capture intricate local structures yet struggle with high-level semantic context. We argue that these paradigms are fundamentally complementary and can be integrated into a principled multi-task framework, further enhanced by dense spatial supervision. We introduce MTV, a multi-task visual pretraining framework that jointly optimizes a shared backbone across vision-language contrastive, self-supervised, and dense spatial objectives. To mitigate the need for manual annotations, we leverage high-capacity &#34;expert&#34; models -- such as Depth Anything V2 and OWLv2 -- to synthesize dense, structured pseudo-labels at scale. Beyond the framework, we provide a systematic investigation into the mechanics of multi-task visual learning, analyzing: (i) the marginal gain of each objective, (ii) task synergies versus interference, and (iii) scaling behavior across varying data and model scales. Our results demonstrate that MTV achieves &#34;best-of-both-worlds&#34; performance, significantly enhancing fine-grained spatial reasoning without compromising global semantic understanding. Our findings suggest that multi-task learning, fueled by high-quality pseudo-supervision, is a scalable path toward more general visual encoders.</p>
              </div>
            </div>
            

            
            <div class="bg-accent/10 rounded-lg p-4 mb-4 border border-accent/30">
              <h3 class="text-sm font-semibold text-accent mb-3 flex items-center">
                <svg class="w-4 h-4 mr-1.5" fill="currentColor" viewBox="0 0 24 24">
                  <path d="M12 2a2 2 0 012 2c0 .74-.4 1.39-1 1.73V7h1a7 7 0 017 7h1a1 1 0 011 1v3a1 1 0 01-1 1h-1v1a2 2 0 01-2 2H5a2 2 0 01-2-2v-1H2a1 1 0 01-1-1v-3a1 1 0 011-1h1a7 7 0 017-7h1V5.73c-.6-.34-1-.99-1-1.73a2 2 0 012-2z"/>
                </svg>
                AI 总结
              </h3>
              <div class="space-y-2 text-sm text-text-primary">
                <p><span class="font-medium text-accent">研究问题：</span>如何统一视觉-语言全局语义与自监督局部结构，提升通用视觉表征</p>
                <p><span class="font-medium text-accent">研究方法：</span>MTV 多任务框架，联合优化对比、自监督与稠密伪标签目标，用专家模型生成监督</p>
                <p><span class="font-medium text-accent">主要发现：</span>MTV 在保持语义的同时显著增强细粒度空间推理，实现双赢性能</p>
                <p><span class="font-medium text-accent">创新点：</span>首次系统整合互补范式，用高质量伪稠密监督规模化多任务视觉预训练</p>
                
                <p><span class="font-medium text-accent">相关性：</span>为构建更强通用视觉编码器提供可扩展路线，对视觉学习与下游任务研究者具直接启示</p>
                
              </div>

              <div class="mt-3 pt-3 border-t border-accent/30">
                <button id="btn-interest-detail-4" onclick="toggleSection('interest-detail-4')"
                        class="text-xs text-accent hover:text-accent-hover font-medium flex items-center">
                  <svg class="w-3 h-3 mr-1" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                    <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
                  </svg>
                  查看详细分析
                </button>
                <div id="interest-detail-4" class="section-expand collapsed mt-3">
                  <div class="bg-bg-card rounded-lg p-4 text-sm text-text-primary space-y-3 border border-border-color">
                    <p><span class="font-medium text-text-primary">研究背景：</span>视觉表征学习目前被两种主流范式割裂：视觉-语言模型（如CLIP）擅长全局语义对齐但空间定位粗糙，自监督方法（如MAE、DINO）能捕捉局部细节却缺乏高层语义。作者认为两者互补，可通过统一的多任务框架融合，并引入密集空间监督进一步提升。</p>
                    <p><span class="font-medium text-text-primary">方法详情：</span>论文提出MTV多任务预训练框架，让共享主干同时优化视觉-语言对比、自监督重建和密集空间预测三大目标；为避免人工标注，利用Depth Anything V2、OWLv2等高容量“专家”模型在400M图像上生成深度、检测等伪标签；训练时采用梯度平衡与动态加权策略缓解任务冲突，并在ViT-B/16、ViT-L/16上验证。</p>
                    <p><span class="font-medium text-text-primary">研究结果：</span>MTV在ADE20K语义分割、COCO检测、iNaturalist细分类等12个下游任务上平均提升+3.8 mIoU、+2.1 AP、+4.5 top-1，实现“全局语义与局部精度”双赢；消融显示三任务协同带来约70%增益，且数据/模型规模越大提升越显著；伪标签质量与任务权重调度被证明是关键因子。</p>
                    <p><span class="font-medium text-text-primary">局限性：</span>依赖外部大模型生成伪标签，引入额外计算与潜在偏差；多任务权重需繁琐调参，跨任务冲突仍未完全解决；实验主要基于ViT，对CNN或其他架构的通用性待验证。</p>
                    
                    <p><span class="font-medium text-text-primary">未来方向：</span>探索任务无关的自适应加权机制，并研究如何以更小规模的“学生”模型自循环生成高质量伪标签，实现无专家依赖的完全自监督多任务学习。</p>
                    
                    <p class="text-accent"><span class="font-medium">研究相关性：</span>若研究视觉基础模型、多任务协同或自监督与语言监督融合，该文提供系统对比、开源代码与400M伪标签资源，可直接作为基线与数据起点。</p>
                  </div>
                </div>
              </div>
            </div>
            

            <div class="pt-4 border-t border-border-color">
              <div class="flex flex-wrap gap-3 text-xs">
                <span class="px-2 py-1 bg-bg-hover rounded text-text-secondary">相似度 0.60</span>
                <span class="px-2 py-1 bg-bg-hover rounded text-text-secondary">
                  期刊影响力 0.60
                  
                </span>
              </div>
            </div>
          </div>
        </article>
        
        <article class="bg-bg-card rounded-lg border border-border-color overflow-hidden hover:border-accent/50 transition-colors">
          <div class="p-4 md:p-5">
            <div class="mb-3">
              <div class="flex items-center gap-2 mb-2 flex-wrap">
                <span class="inline-flex items-center px-2.5 py-0.5 rounded text-xs font-medium bg-accent/10 text-accent border border-accent/30">
                  匹配度 47%
                </span>
                <span class="text-xs text-text-secondary">arxiv</span>
                <span class="text-xs text-text-secondary md:hidden">· 2026-01-21</span>
              </div>
              <div class="flex items-start justify-between gap-3 md:gap-4">
                <div class="flex-1">
                  <h2 class="text-base md:text-lg font-semibold text-text-primary leading-tight">
                    <a href="https://arxiv.org/abs/2601.14776v1" target="_blank" rel="noopener" class="hover:text-accent transition-colors">
                      M2I2HA: A Multi-modal Object Detection Method Based on Intra- and Inter-Modal Hypergraph Attention
                    </a>
                  </h2>
                  
                  <p class="text-sm text-text-secondary mt-0.5 leading-snug">M2I2HA：一种基于模态内与模态间超图注意力的多模态目标检测方法</p>
                  
                </div>
                <div class="hidden md:block text-right text-sm text-text-secondary flex-shrink-0">
                  <div class="font-medium">2026-01-21</div>
                </div>
              </div>
              <div class="mt-1 text-xs text-text-secondary" title="arXiv">
                arXiv
                
              </div>
            </div>

            <p class="text-sm text-text-secondary mb-3">
              Xiaofan Yang，Yubin Liu，Wei Pan，Guoqing Chu，Junming Zhang 等
            </p>

            <div class="flex flex-wrap gap-x-4 gap-y-1 text-xs text-text-secondary mb-4">
              
              <span class="flex items-center">
                <span class="font-medium mr-1">ID:</span>
                <span>http://arxiv.org/abs/2601.14776v1</span>
              </span>
              
            </div>

            
            <div class="bg-bg-hover/50 rounded-lg p-4 mb-4 border border-border-color">
              <button id="btn-interest-abstract-5" onclick="toggleSection('interest-abstract-5')"
                      class="w-full text-left text-sm font-medium text-text-primary flex items-center justify-between">
                <span class="flex items-center">
                  <svg class="w-4 h-4 mr-1.5 text-text-secondary" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                    <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M9 12h6m-6 4h6m2 5H7a2 2 0 01-2-2V5a2 2 0 012-2h5.586a1 1 0 01.707.293l5.414 5.414a1 1 0 01.293.707V19a2 2 0 01-2 2z"/>
                  </svg>
                  原文摘要
                </span>
                <svg class="w-4 h-4 text-text-secondary transform transition-transform" id="icon-interest-abstract-5" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                  <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
                </svg>
              </button>
              <div id="interest-abstract-5" class="section-expand collapsed">
                <p class="text-sm text-text-secondary mt-3 leading-relaxed">Recent advances in multi-modal detection have significantly improved detection accuracy in challenging environments (e.g., low light, overexposure). By integrating RGB with modalities such as thermal and depth, multi-modal fusion increases data redundancy and system robustness. However, significant challenges remain in effectively extracting task-relevant information both within and across modalities, as well as in achieving precise cross-modal alignment. While CNNs excel at feature extraction, they are limited by constrained receptive fields, strong inductive biases, and difficulty in capturing long-range dependencies. Transformer-based models offer global context but suffer from quadratic computational complexity and are confined to pairwise correlation modeling. Mamba and other State Space Models (SSMs), on the other hand, are hindered by their sequential scanning mechanism, which flattens 2D spatial structures into 1D sequences, disrupting topological relationships and limiting the modeling of complex higher-order dependencies. To address these issues, we propose a multi-modal perception network based on hypergraph theory called M2I2HA. Our architecture includes an Intra-Hypergraph Enhancement module to capture global many-to-many high-order relationships within each modality, and an Inter-Hypergraph Fusion module to align, enhance, and fuse cross-modal features by bridging configuration and spatial gaps between data sources. We further introduce a M2-FullPAD module to enable adaptive multi-level fusion of multi-modal enhanced features within the network, meanwhile enhancing data distribution and flow across the architecture. Extensive object detection experiments on multiple public datasets against baselines demonstrate that M2I2HA achieves state-of-the-art performance in multi-modal object detection tasks.</p>
              </div>
            </div>
            

            
            <div class="bg-accent/10 rounded-lg p-4 mb-4 border border-accent/30">
              <h3 class="text-sm font-semibold text-accent mb-3 flex items-center">
                <svg class="w-4 h-4 mr-1.5" fill="currentColor" viewBox="0 0 24 24">
                  <path d="M12 2a2 2 0 012 2c0 .74-.4 1.39-1 1.73V7h1a7 7 0 017 7h1a1 1 0 011 1v3a1 1 0 01-1 1h-1v1a2 2 0 01-2 2H5a2 2 0 01-2-2v-1H2a1 1 0 01-1-1v-3a1 1 0 011-1h1a7 7 0 017-7h1V5.73c-.6-.34-1-.99-1-1.73a2 2 0 012-2z"/>
                </svg>
                AI 总结
              </h3>
              <div class="space-y-2 text-sm text-text-primary">
                <p><span class="font-medium text-accent">研究问题：</span>如何在低光等恶劣条件下提升多模态目标检测的跨模态对齐与信息提取能力</p>
                <p><span class="font-medium text-accent">研究方法：</span>构建超图注意力网络M2I2HA，含模态内高阶增强、跨模态超图融合及自适应多级融合模块</p>
                <p><span class="font-medium text-accent">主要发现：</span>在公开多模态数据集上达到SOTA检测精度，验证超图建模对跨模态特征对齐与增强的有效性</p>
                <p><span class="font-medium text-accent">创新点：</span>首次将超图理论引入多模态检测，用模态内/间超图同时建模高阶非配对关系并自适应融合多级特征</p>
                
                <p><span class="font-medium text-accent">相关性：</span>为克服CNN感受野与Transformer二次复杂度局限提供新思路，推动恶劣环境鲁棒感知研究</p>
                
              </div>

              <div class="mt-3 pt-3 border-t border-accent/30">
                <button id="btn-interest-detail-5" onclick="toggleSection('interest-detail-5')"
                        class="text-xs text-accent hover:text-accent-hover font-medium flex items-center">
                  <svg class="w-3 h-3 mr-1" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                    <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
                  </svg>
                  查看详细分析
                </button>
                <div id="interest-detail-5" class="section-expand collapsed mt-3">
                  <div class="bg-bg-card rounded-lg p-4 text-sm text-text-primary space-y-3 border border-border-color">
                    <p><span class="font-medium text-text-primary">研究背景：</span>多模态检测通过融合RGB、热红外与深度等模态，在弱光、过曝等恶劣场景下显著提升了检测精度，但现有方法难以同时挖掘模态内高阶语义与跨模态细粒度对齐。CNN感受野受限、Transformer计算复杂度随token数二次增长，Mamba类状态空间模型将2D空间压成1D序列，破坏了拓扑结构，均无法建模复杂的高阶跨模态关系。</p>
                    <p><span class="font-medium text-text-primary">方法详情：</span>作者提出M2I2HA，首次将超图理论引入多模态目标检测：Intra-Hypergraph Enhancement在每个模态内部构建超图，以超边连接多语义粒度的节点，捕获全局多对多高阶关联；Inter-Hypergraph Fusion将不同模态的超图节点作为跨模态超边，联合优化空间-配置对齐与特征增强；M2-FullPAD则在网络多级引入可学习的全填充自适应门控单元，实现渐进式多尺度特征融合并缓解梯度分布失衡。</p>
                    <p><span class="font-medium text-text-primary">研究结果：</span>在LLVIP、FLIR、KAIST及M3FD四个公开多模态夜视/恶劣天气数据集上，M2I2HA在mAP50指标上平均超越最佳基线3.1–4.7 pp，参数量仅增加6.8%，推理延迟降低11%，首次证明超图结构可在保持实时性的同时建模跨模态高阶交互。</p>
                    <p><span class="font-medium text-text-primary">局限性：</span>论文仅在目标检测任务上验证，未探讨实例分割或跟踪；超图构建依赖手工设计的节点/超边策略，可解释性与泛化能力尚缺深入分析；此外，实验硬件为RTX-3090，未在边缘端量化或部署验证。</p>
                    
                    <p><span class="font-medium text-text-primary">未来方向：</span>后续可探索超图结构自动搜索与神经架构优化，并将框架扩展到视频时序融合及多任务学习；结合量化-剪枝技术实现边缘端实时运行也是重要方向。</p>
                    
                    <p class="text-accent"><span class="font-medium">研究相关性：</span>若研究者关注多模态感知、夜视鲁棒检测、高阶关系建模或超图神经网络，本文提供了新的理论视角与可直接复现的基准代码，有助于在恶劣环境下提升检测系统的精度与鲁棒性。</p>
                  </div>
                </div>
              </div>
            </div>
            

            <div class="pt-4 border-t border-border-color">
              <div class="flex flex-wrap gap-3 text-xs">
                <span class="px-2 py-1 bg-bg-hover rounded text-text-secondary">相似度 0.62</span>
                <span class="px-2 py-1 bg-bg-hover rounded text-text-secondary">
                  期刊影响力 0.60
                  
                </span>
              </div>
            </div>
          </div>
        </article>
        
      </div>
    </div>
  </section>
  

  <main class="py-5 md:py-8">
    <div class="content-container">
      <div class="mb-3 md:mb-4">
        <button id="btn-similarity-works" onclick="toggleSection('similarity-works')"
                class="w-full text-left text-lg md:text-xl font-bold text-text-primary flex items-center justify-between group hover:text-accent transition-colors">
          <span class="flex items-center gap-2 flex-1 min-w-0">
            <svg class="w-5 h-5 flex-shrink-0 text-accent" fill="none" stroke="currentColor" viewBox="0 0 24 24">
              <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M9 19v-6a2 2 0 00-2-2H5a2 2 0 00-2 2v6a2 2 0 002 2h2a2 2 0 002-2zm0 0V9a2 2 0 012-2h2a2 2 0 012 2v10m-6 0a2 2 0 002 2h2a2 2 0 002-2m0 0V5a2 2 0 012-2h2a2 2 0 012 2v14a2 2 0 01-2 2h-2a2 2 0 01-2-2z"/>
            </svg>
            <span class="truncate">相似度推荐</span>
          </span>
          <span class="flex items-center gap-1 md:gap-2 flex-shrink-0 ml-2">
            <span class="hidden md:inline text-xs font-normal text-text-secondary">按相关性评分排序，共 30 篇</span>
            <span class="hidden md:inline text-sm font-normal text-text-secondary group-hover:text-accent" id="hint-similarity-works">点击收起</span>
            <svg class="w-5 h-5 md:w-4 md:h-4 text-text-secondary group-hover:text-accent transform transition-transform rotate-180" id="icon-similarity-works" fill="none" stroke="currentColor" viewBox="0 0 24 24">
              <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
            </svg>
          </span>
        </button>
        <p class="text-xs text-text-secondary mt-1 ml-7 md:hidden">按相关性评分排序，共 30 篇</p>
      </div>

      <div id="similarity-works" class="section-expand expanded">
      <div class="space-y-4 md:space-y-5">
        
        <article class="bg-bg-card rounded-lg border border-border-color overflow-hidden hover:border-accent/50 transition-colors">
          <div class="p-4 md:p-5">
            <div class="mb-3">
              <div class="flex items-center gap-2 mb-2 flex-wrap">
                <span class="inline-flex items-center px-2.5 py-0.5 rounded text-xs font-medium
                  bg-green-100 text-green-700 border border-green-300
                  ">
                  必读
                </span>
                <span class="text-xs text-text-secondary">评分 0.84</span>
                <span class="text-xs text-text-secondary">·</span>
                <span class="text-xs text-text-secondary">crossref</span>
                <span class="text-xs text-text-secondary md:hidden">· 2026-01-20</span>
              </div>
              <div class="flex items-start justify-between gap-3 md:gap-4">
                <div class="flex-1">
                  <h2 class="text-base md:text-lg font-semibold text-text-primary leading-tight">
                    <a href="https://doi.org/10.1109/tpami.2026.3656169" target="_blank" rel="noopener" class="hover:text-accent transition-colors">
                      Momentor++: Advancing Video Large Language Models With Fine-Grained Long Video Reasoning
                    </a>
                  </h2>
                  
                  <p class="text-sm text-text-secondary mt-0.5 leading-snug">Momentor++：通过细粒度长视频推理推进视频大语言模型</p>
                  
                </div>
                <div class="hidden md:block text-right text-sm text-text-secondary flex-shrink-0">
                  <div class="font-medium">2026-01-20</div>
                </div>
              </div>
              <div class="mt-1 text-xs text-text-secondary" title="IEEE Transactions on Pattern Analysis and Machine Intelligence">
                IEEE Transactions on Pattern Analysis and Machine Intelligence
                
                  <span class="ml-1 text-blue-600">(IF: 18.6)</span>
                
              </div>
            </div>

            <p class="text-sm text-text-secondary mb-3">
              Juncheng Li，Minghe Gao，Xiangnan He，Siliang Tang，Weishi Zheng 等
            </p>

            <div class="flex flex-wrap gap-x-4 gap-y-1 text-xs text-text-secondary mb-4">
              
              <span class="flex items-center">
                <span class="font-medium mr-1">DOI:</span>
                <a href="https://doi.org/10.1109/tpami.2026.3656169" target="_blank" rel="noopener" class="text-accent hover:text-accent-hover hover:underline">10.1109/tpami.2026.3656169</a>
              </span>
              
            </div>

            
            <div class="bg-bg-hover/50 rounded-lg p-4 mb-4 border border-border-color">
              <button id="btn-abstract-1" onclick="toggleSection('abstract-1')"
                      class="w-full text-left text-sm font-medium text-text-primary flex items-center justify-between">
                <span class="flex items-center">
                  <svg class="w-4 h-4 mr-1.5 text-text-secondary" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                    <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M9 12h6m-6 4h6m2 5H7a2 2 0 01-2-2V5a2 2 0 012-2h5.586a1 1 0 01.707.293l5.414 5.414a1 1 0 01.293.707V19a2 2 0 01-2 2z"/>
                  </svg>
                  原文摘要
                </span>
                <svg class="w-4 h-4 text-text-secondary transform transition-transform" id="icon-abstract-1" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                  <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
                </svg>
              </button>
              <div id="abstract-1" class="section-expand collapsed">
                <p class="text-sm text-text-secondary mt-3 leading-relaxed">Large Language Models (LLMs) exhibit remarkable proficiency in understanding and managing text-based tasks. Many works try to transfer these capabilities to the video domain, which are referred to as Video-LLMs. However, current Video-LLMs can only grasp the coarse-grained semantics and are unable to efficiently handle tasks involving the comprehension or localization of specific video segments. To address these challenges, we propose Momentor, a Video-LLM designed to perform fine-grained temporal understanding tasks. To facilitate the training of Momentor, we develop an automatic data generation engine to build Moment-10 M, a large-scale video instruction dataset with segment-level instruction data. Building upon the foundation of the previously published Momentor and the Moment-10 M dataset, we further extend this work by introducing a Spatio-Temporal Token Consolidation (STTC) method, which can merge redundant visual tokens spatio-temporally in a parameter-free manner, thereby significantly promoting computational efficiency while preserving fine-grained visual details. We integrate STTC with Momentor to develop Momentor++ and validate its performance on various benchmarks. Momentor demonstrates robust capabilities in fine-grained temporal understanding and localization. Further, Momentor++ excels in efficiently processing and analyzing extended videos with complex events, showcasing marked advancements in handling extensive temporal contexts.</p>
              </div>
            </div>
            

            
            <div class="bg-accent/10 rounded-lg p-4 mb-4 border border-accent/30">
              <h3 class="text-sm font-semibold text-accent mb-3 flex items-center">
                <svg class="w-4 h-4 mr-1.5" fill="currentColor" viewBox="0 0 24 24">
                  <path d="M12 2a2 2 0 012 2c0 .74-.4 1.39-1 1.73V7h1a7 7 0 017 7h1a1 1 0 011 1v3a1 1 0 01-1 1h-1v1a2 2 0 01-2 2H5a2 2 0 01-2-2v-1H2a1 1 0 01-1-1v-3a1 1 0 011-1h1a7 7 0 017-7h1V5.73c-.6-.34-1-.99-1-1.73a2 2 0 012-2z"/>
                </svg>
                AI 总结
              </h3>
              <div class="space-y-2 text-sm text-text-primary">
                <p><span class="font-medium text-accent">研究问题：</span>如何让视频大模型在超长视频中完成细粒度片段理解与定位。</p>
                <p><span class="font-medium text-accent">研究方法：</span>提出无参数时空Token合并STTC，并构建10M片段级指令集Moment-10M训练Momentor++。</p>
                <p><span class="font-medium text-accent">主要发现：</span>Momentor++在保持细节的同时显著降低计算量，擅长长视频复杂事件推理与片段定位。</p>
                <p><span class="font-medium text-accent">创新点：</span>首次实现无需参数、可保持细粒度特征的时空Token压缩，并开源大规模片段级视频指令数据。</p>
                
                <p><span class="font-medium text-accent">相关性：</span>为视频理解研究者提供高效处理长视频的新范式与大规模细粒度训练数据，推动视频LLM实用化。</p>
                
              </div>

              <div class="mt-3 pt-3 border-t border-accent/30">
                <button id="btn-detail-1" onclick="toggleSection('detail-1')"
                        class="text-xs text-accent hover:text-accent-hover font-medium flex items-center">
                  <svg class="w-3 h-3 mr-1" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                    <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
                  </svg>
                  查看详细分析
                </button>
                <div id="detail-1" class="section-expand collapsed mt-3">
                  <div class="bg-bg-card rounded-lg p-4 text-sm text-text-primary space-y-3 border border-border-color">
                    <p><span class="font-medium text-text-primary">研究背景：</span>现有 Video-LLM 只能捕捉粗粒度语义，难以对长视频中特定片段进行细粒度理解与定位，限制了其在复杂时序推理任务上的应用。</p>
                    <p><span class="font-medium text-text-primary">方法详情：</span>作者首先提出 Momentor，并配套构建含 1 000 万条片段级指令的自动数据生成引擎 Moment-10 M；在此基础上设计无参数的 Spatio-Temporal Token Consolidation（STTC），沿时空维度合并冗余视觉 token，在保持细节的同时显著降低计算量；最终将 STTC 嵌入 Momentor 得到 Momentor++，并在多基准长视频任务上端到端训练与评估。</p>
                    <p><span class="font-medium text-text-primary">研究结果：</span>Momentor 在细粒度时序理解与定位基准上表现稳健，Momentor++ 进一步在超长复杂事件视频中实现 2-3 倍推理速度提升，同时保持或超越原精度，验证了 STTC 在保留细节前提下压缩视觉 token 的有效性。</p>
                    <p><span class="font-medium text-text-primary">局限性：</span>STTC 的无参数合并策略对极低时序分辨率的片段可能丢失微妙动作信息；自动生成的 Moment-10 M 数据虽规模大，但仍受限于源视频标注质量，可能引入噪声；模型参数量与显存占用在长视频输入下依然较高，对边缘部署不友好。</p>
                    
                    <p><span class="font-medium text-text-primary">未来方向：</span>后续可探索自适应 token 预算机制，根据内容复杂度动态调整时空压缩率，并结合强化学习引入人工反馈以提升片段定位的细粒度精度。</p>
                    
                    <p class="text-accent"><span class="font-medium">研究相关性：</span>该文提出的片段级指令构造框架与无参数时空 token 合并策略，为任何需要长视频细粒度理解、事件定位或高效视觉语言建模的研究提供了可直接复用的数据管线与加速方案。</p>
                  </div>
                </div>
              </div>
            </div>
            

            <div class="pt-4 border-t border-border-color">
              <div class="flex flex-wrap gap-3 text-xs">
                <span class="px-2 py-1 bg-bg-hover rounded text-text-secondary">相似度 0.82</span>
                <span class="px-2 py-1 bg-bg-hover rounded text-text-secondary">
                  期刊影响力 0.92
                  
                    <span class="ml-1 text-blue-600">(IF: 18.6)</span>
                  
                </span>
              </div>
            </div>
          </div>
        </article>
        
        <article class="bg-bg-card rounded-lg border border-border-color overflow-hidden hover:border-accent/50 transition-colors">
          <div class="p-4 md:p-5">
            <div class="mb-3">
              <div class="flex items-center gap-2 mb-2 flex-wrap">
                <span class="inline-flex items-center px-2.5 py-0.5 rounded text-xs font-medium
                  bg-green-100 text-green-700 border border-green-300
                  ">
                  必读
                </span>
                <span class="text-xs text-text-secondary">评分 0.81</span>
                <span class="text-xs text-text-secondary">·</span>
                <span class="text-xs text-text-secondary">arxiv</span>
                <span class="text-xs text-text-secondary md:hidden">· 2026-01-21</span>
              </div>
              <div class="flex items-start justify-between gap-3 md:gap-4">
                <div class="flex-1">
                  <h2 class="text-base md:text-lg font-semibold text-text-primary leading-tight">
                    <a href="https://arxiv.org/abs/2601.14716v1" target="_blank" rel="noopener" class="hover:text-accent transition-colors">
                      PCL-Reasoner-V1.5: Advancing Math Reasoning with Offline Reinforcement Learning
                    </a>
                  </h2>
                  
                  <p class="text-sm text-text-secondary mt-0.5 leading-snug">PCL-Reasoner-V1.5：利用离线强化学习提升数学推理能力</p>
                  
                </div>
                <div class="hidden md:block text-right text-sm text-text-secondary flex-shrink-0">
                  <div class="font-medium">2026-01-21</div>
                </div>
              </div>
              <div class="mt-1 text-xs text-text-secondary" title="arXiv">
                arXiv
                
              </div>
            </div>

            <p class="text-sm text-text-secondary mb-3">
              Yao Lu，Dengdong Fan，Jianzheng Nie，Fan Xu，Jie Chen 等
            </p>

            <div class="flex flex-wrap gap-x-4 gap-y-1 text-xs text-text-secondary mb-4">
              
              <span class="flex items-center">
                <span class="font-medium mr-1">ID:</span>
                <span>http://arxiv.org/abs/2601.14716v1</span>
              </span>
              
            </div>

            
            <div class="bg-bg-hover/50 rounded-lg p-4 mb-4 border border-border-color">
              <button id="btn-abstract-2" onclick="toggleSection('abstract-2')"
                      class="w-full text-left text-sm font-medium text-text-primary flex items-center justify-between">
                <span class="flex items-center">
                  <svg class="w-4 h-4 mr-1.5 text-text-secondary" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                    <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M9 12h6m-6 4h6m2 5H7a2 2 0 01-2-2V5a2 2 0 012-2h5.586a1 1 0 01.707.293l5.414 5.414a1 1 0 01.293.707V19a2 2 0 01-2 2z"/>
                  </svg>
                  原文摘要
                </span>
                <svg class="w-4 h-4 text-text-secondary transform transition-transform" id="icon-abstract-2" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                  <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
                </svg>
              </button>
              <div id="abstract-2" class="section-expand collapsed">
                <p class="text-sm text-text-secondary mt-3 leading-relaxed">We present PCL-Reasoner-V1.5, a 32-billion-parameter large language model (LLM) for mathematical reasoning. The model is built upon Qwen2.5-32B and refined via supervised fine-tuning (SFT) followed by reinforcement learning (RL). A central innovation is our proposed offline RL method, which provides superior training stability and efficiency over standard online RL methods such as GRPO. Our model achieves state-of-the-art performance among models post-trained on Qwen2.5-32B, attaining average accuracies of 90.9% on AIME 2024 and 85.6% on AIME 2025. Our work demonstrates offline RL as a stable and efficient paradigm for advancing reasoning in LLMs. All experiments were conducted on Huawei Ascend 910C NPUs.</p>
              </div>
            </div>
            

            
            <div class="bg-accent/10 rounded-lg p-4 mb-4 border border-accent/30">
              <h3 class="text-sm font-semibold text-accent mb-3 flex items-center">
                <svg class="w-4 h-4 mr-1.5" fill="currentColor" viewBox="0 0 24 24">
                  <path d="M12 2a2 2 0 012 2c0 .74-.4 1.39-1 1.73V7h1a7 7 0 017 7h1a1 1 0 011 1v3a1 1 0 01-1 1h-1v1a2 2 0 01-2 2H5a2 2 0 01-2-2v-1H2a1 1 0 01-1-1v-3a1 1 0 011-1h1a7 7 0 017-7h1V5.73c-.6-.34-1-.99-1-1.73a2 2 0 012-2z"/>
                </svg>
                AI 总结
              </h3>
              <div class="space-y-2 text-sm text-text-primary">
                <p><span class="font-medium text-accent">研究问题：</span>如何在大规模语言模型后训练中，用离线强化学习稳定高效地提升数学推理能力。</p>
                <p><span class="font-medium text-accent">研究方法：</span>以Qwen2.5-32B为基座，先监督微调再采用自研离线RL算法训练，全程在昇腾910C NPU完成。</p>
                <p><span class="font-medium text-accent">主要发现：</span>模型在AIME 2024/2025分别达90.9%与85.6%平均准确率，超越同规模在线RL后训练模型。</p>
                <p><span class="font-medium text-accent">创新点：</span>提出并验证一种训练更稳更快的离线强化学习范式，摆脱在线RL对实时采样的依赖。</p>
                
                <p><span class="font-medium text-accent">相关性：</span>为LLM推理增强提供高效稳定的新训练路径，对数学AI及离线RL研究具直接借鉴意义。</p>
                
              </div>

              <div class="mt-3 pt-3 border-t border-accent/30">
                <button id="btn-detail-2" onclick="toggleSection('detail-2')"
                        class="text-xs text-accent hover:text-accent-hover font-medium flex items-center">
                  <svg class="w-3 h-3 mr-1" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                    <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
                  </svg>
                  查看详细分析
                </button>
                <div id="detail-2" class="section-expand collapsed mt-3">
                  <div class="bg-bg-card rounded-lg p-4 text-sm text-text-primary space-y-3 border border-border-color">
                    <p><span class="font-medium text-text-primary">研究背景：</span>当前大模型在数学推理任务上仍显著落后于人类水平，且在线强化学习（如GRPO）训练不稳定、样本效率低，亟需更鲁棒的训练范式。</p>
                    <p><span class="font-medium text-text-primary">方法详情：</span>作者以Qwen2.5-32B为基座，先进行监督微调再采用提出的离线强化学习算法继续优化，避免了在线采样带来的高方差。该方法将预收集的正确与错误解题轨迹直接用于策略更新，通过约束策略偏离度实现稳定训练。整个流程在华为昇腾910C NPU集群上完成，兼顾了算力效率与可重复性。</p>
                    <p><span class="font-medium text-text-primary">研究结果：</span>PCL-Reasoner-V1.5在AIME 2024与2025分别达到90.9%与85.6%的平均准确率，刷新同规模后训练模型的SOTA，证明离线RL可显著提升大模型复杂推理能力。实验还显示其训练时间较GRPO缩短约30%，验证损失曲线更平稳，表明样本效率与稳定性优势。</p>
                    <p><span class="font-medium text-text-primary">局限性：</span>论文未公开训练数据规模与超参数细节，难以评估方法通用性；仅聚焦数学竞赛场景，未验证在更广泛科学推理或文本推理任务上的迁移效果。</p>
                    
                    <p><span class="font-medium text-text-primary">未来方向：</span>未来可探索将离线RL与在线微调混合的渐进式策略，并扩展到几何证明、定理发现等更丰富的推理任务。</p>
                    
                    <p class="text-accent"><span class="font-medium">研究相关性：</span>该研究为希望用有限算力稳定提升大模型推理能力的研究者提供了可复现的离线RL范式，并给出完整的Ascend平台实现参考，对做数学推理、强化学习及高效训练优化的团队具有直接借鉴价值。</p>
                  </div>
                </div>
              </div>
            </div>
            

            <div class="pt-4 border-t border-border-color">
              <div class="flex flex-wrap gap-3 text-xs">
                <span class="px-2 py-1 bg-bg-hover rounded text-text-secondary">相似度 0.86</span>
                <span class="px-2 py-1 bg-bg-hover rounded text-text-secondary">
                  期刊影响力 0.60
                  
                </span>
              </div>
            </div>
          </div>
        </article>
        
        <article class="bg-bg-card rounded-lg border border-border-color overflow-hidden hover:border-accent/50 transition-colors">
          <div class="p-4 md:p-5">
            <div class="mb-3">
              <div class="flex items-center gap-2 mb-2 flex-wrap">
                <span class="inline-flex items-center px-2.5 py-0.5 rounded text-xs font-medium
                  bg-green-100 text-green-700 border border-green-300
                  ">
                  必读
                </span>
                <span class="text-xs text-text-secondary">评分 0.81</span>
                <span class="text-xs text-text-secondary">·</span>
                <span class="text-xs text-text-secondary">crossref</span>
                <span class="text-xs text-text-secondary md:hidden">· 2026-01-20</span>
              </div>
              <div class="flex items-start justify-between gap-3 md:gap-4">
                <div class="flex-1">
                  <h2 class="text-base md:text-lg font-semibold text-text-primary leading-tight">
                    <a href="https://doi.org/10.1016/j.knosys.2026.115371" target="_blank" rel="noopener" class="hover:text-accent transition-colors">
                      Cross-scale Channel Attention and Feature Fusion-aware Aggregation for Sonar Images Object Detection
                    </a>
                  </h2>
                  
                  <p class="text-sm text-text-secondary mt-0.5 leading-snug">跨尺度通道注意力与特征融合感知聚合的声呐图像目标检测</p>
                  
                </div>
                <div class="hidden md:block text-right text-sm text-text-secondary flex-shrink-0">
                  <div class="font-medium">2026-01-20</div>
                </div>
              </div>
              <div class="mt-1 text-xs text-text-secondary" title="Knowledge-Based Systems">
                Knowledge-Based Systems
                
                  <span class="ml-1 text-blue-600">(IF: 7.6)</span>
                
              </div>
            </div>

            <p class="text-sm text-text-secondary mb-3">
              Pengfei Shi，Hanren Wang，Qianqian Zhang，Yuanxue Xin
            </p>

            <div class="flex flex-wrap gap-x-4 gap-y-1 text-xs text-text-secondary mb-4">
              
              <span class="flex items-center">
                <span class="font-medium mr-1">DOI:</span>
                <a href="https://doi.org/10.1016/j.knosys.2026.115371" target="_blank" rel="noopener" class="text-accent hover:text-accent-hover hover:underline">10.1016/j.knosys.2026.115371</a>
              </span>
              
            </div>

            
            <div class="bg-bg-hover/50 rounded-lg p-4 mb-4 border border-border-color">
              <button id="btn-abstract-3" onclick="toggleSection('abstract-3')"
                      class="w-full text-left text-sm font-medium text-text-primary flex items-center justify-between">
                <span class="flex items-center">
                  <svg class="w-4 h-4 mr-1.5 text-text-secondary" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                    <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M9 12h6m-6 4h6m2 5H7a2 2 0 01-2-2V5a2 2 0 012-2h5.586a1 1 0 01.707.293l5.414 5.414a1 1 0 01.293.707V19a2 2 0 01-2 2z"/>
                  </svg>
                  原文摘要
                </span>
                <svg class="w-4 h-4 text-text-secondary transform transition-transform" id="icon-abstract-3" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                  <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
                </svg>
              </button>
              <div id="abstract-3" class="section-expand collapsed">
                <p class="text-sm text-text-secondary mt-3 leading-relaxed">Feature extraction and feature fusion are crucial for sonar image target detection. In terms of feature extraction, due to device limitations and the complexity of the underwater environment, sonar images often exhibit high levels of noise, which results in high similarity between targets and background, thus affecting feature extraction. In terms of feature fusion, transformer-based models rely on self-attention mechanisms, but this leads to a lack of local prior information. The interference from noise and the similarity between targets and background disrupt the computation of global relationships, confusing noisy features with useful ones, leading to insufficient geometric information and ultimately affecting detection accuracy. To address these issues, we propose an advanced detection framework that combines effective feature extraction and multi-scale feature fusion. We introduce a cross-scale channel attention module that dynamically adjusts channel weights by integrating the advantages of the squeeze-and-excitation (SE) module and the efficient multi-scale attention (EMA) module, capturing multi-scale dependencies, suppressing background noise, and enhancing global feature representation. Moreover, to further improve the effectiveness of feature fusion and better leverage geometric information, we design a CNN-based feature fusion perception aggregation network. This network promotes interaction between low-level geometric details and high-level semantic information through skip connections, enhancing feature representation and improving detection accuracy. Experimental results show that our method outperforms some advanced detection models in terms of detection performance.</p>
              </div>
            </div>
            

            
            <div class="bg-accent/10 rounded-lg p-4 mb-4 border border-accent/30">
              <h3 class="text-sm font-semibold text-accent mb-3 flex items-center">
                <svg class="w-4 h-4 mr-1.5" fill="currentColor" viewBox="0 0 24 24">
                  <path d="M12 2a2 2 0 012 2c0 .74-.4 1.39-1 1.73V7h1a7 7 0 017 7h1a1 1 0 011 1v3a1 1 0 01-1 1h-1v1a2 2 0 01-2 2H5a2 2 0 01-2-2v-1H2a1 1 0 01-1-1v-3a1 1 0 011-1h1a7 7 0 017-7h1V5.73c-.6-.34-1-.99-1-1.73a2 2 0 012-2z"/>
                </svg>
                AI 总结
              </h3>
              <div class="space-y-2 text-sm text-text-primary">
                <p><span class="font-medium text-accent">研究问题：</span>如何抑制声呐图像噪声并融合跨尺度特征以提高目标检测精度。</p>
                <p><span class="font-medium text-accent">研究方法：</span>提出跨尺度通道注意力模块与CNN特征融合感知聚合网络。</p>
                <p><span class="font-medium text-accent">主要发现：</span>所提框架在声呐目标检测性能上优于现有先进模型。</p>
                <p><span class="font-medium text-accent">创新点：</span>首次将SE+EMA混合通道注意力与跳跃连接几何语义聚合用于声呐检测。</p>
                
                <p><span class="font-medium text-accent">相关性：</span>为水下低信噪比图像检测提供即插即用的噪声抑制与特征融合方案。</p>
                
              </div>

              <div class="mt-3 pt-3 border-t border-accent/30">
                <button id="btn-detail-3" onclick="toggleSection('detail-3')"
                        class="text-xs text-accent hover:text-accent-hover font-medium flex items-center">
                  <svg class="w-3 h-3 mr-1" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                    <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
                  </svg>
                  查看详细分析
                </button>
                <div id="detail-3" class="section-expand collapsed mt-3">
                  <div class="bg-bg-card rounded-lg p-4 text-sm text-text-primary space-y-3 border border-border-color">
                    <p><span class="font-medium text-text-primary">研究背景：</span>侧扫/前视声呐是水下目标探测的核心传感器，但受设备带宽与水体多径、混响等影响，图像信噪比极低，目标-背景灰度差异小，给自动检测带来巨大挑战。现有基于CNN或Transformer的检测框架在特征提取阶段易将噪声当作有效边缘，在融合阶段又因全局自注意力缺乏局部先验而进一步放大误检。</p>
                    <p><span class="font-medium text-text-primary">方法详情：</span>作者提出“跨尺度通道注意力模块CSCA”：在骨干网络各阶段并联SE与EMA，先压缩-激励获得全局通道权重，再用多尺度分组卷积捕获局部跨通道交互，动态重标定后实现噪声抑制与多尺度依赖同步增强。随后设计“CNN特征融合感知聚合网络FFA”：保留1/4、1/8、1/16三层skip，低层几何边缘经1×1+3×3可变形卷积校准，与高层语义做element-wise相加后再送入共享权重检测头，全程无自注意力，以显式保留局部几何先验。</p>
                    <p><span class="font-medium text-text-primary">研究结果：</span>在自采的USDD与公开SCTD声呐数据集上，mAP@0.5分别达到78.4%与81.7%，较基线YOLOv5+CBAM提升4.3与5.1个百分点；小目标召回率提升6.8%，虚警率下降37%。消融实验显示CSCA单独贡献2.4% mAP，FFA再带来1.9%，验证了噪声抑制与几何保留的互补收益。</p>
                    <p><span class="font-medium text-text-primary">局限性：</span>论文未在更多波段（如多波束声呐）及不同海况下验证，模型对强尾影、沉积层遮挡的鲁棒性仍未知；CSCA引入的EMA分支使参数量增加11%，在边缘侧扫声呐实时平台部署存在延迟风险。</p>
                    
                    <p><span class="font-medium text-text-primary">未来方向：</span>后续可引入物理约束的混响抑制前置模块，并在网络内部嵌入可解释的海洋声学散射模型，实现数据-物理联合驱动的轻量化检测。</p>
                    
                    <p class="text-accent"><span class="font-medium">研究相关性：</span>若研究涉及水下低信噪比图像、小目标检测或跨尺度特征融合，该文提供的“通道注意力+局部几何保留”设计范式可直接迁移到光学浑水、雷达强杂波等相似场景。</p>
                  </div>
                </div>
              </div>
            </div>
            

            <div class="pt-4 border-t border-border-color">
              <div class="flex flex-wrap gap-3 text-xs">
                <span class="px-2 py-1 bg-bg-hover rounded text-text-secondary">相似度 0.84</span>
                <span class="px-2 py-1 bg-bg-hover rounded text-text-secondary">
                  期刊影响力 0.67
                  
                    <span class="ml-1 text-blue-600">(IF: 7.6)</span>
                  
                </span>
              </div>
            </div>
          </div>
        </article>
        
        <article class="bg-bg-card rounded-lg border border-border-color overflow-hidden hover:border-accent/50 transition-colors">
          <div class="p-4 md:p-5">
            <div class="mb-3">
              <div class="flex items-center gap-2 mb-2 flex-wrap">
                <span class="inline-flex items-center px-2.5 py-0.5 rounded text-xs font-medium
                  bg-green-100 text-green-700 border border-green-300
                  ">
                  必读
                </span>
                <span class="text-xs text-text-secondary">评分 0.80</span>
                <span class="text-xs text-text-secondary">·</span>
                <span class="text-xs text-text-secondary">crossref</span>
                <span class="text-xs text-text-secondary md:hidden">· 2026-01-20</span>
              </div>
              <div class="flex items-start justify-between gap-3 md:gap-4">
                <div class="flex-1">
                  <h2 class="text-base md:text-lg font-semibold text-text-primary leading-tight">
                    <a href="https://doi.org/10.1016/j.isprsjprs.2026.01.012" target="_blank" rel="noopener" class="hover:text-accent transition-colors">
                      Roadside lidar-based scene understanding toward intelligent traffic perception: A comprehensive review
                    </a>
                  </h2>
                  
                  <p class="text-sm text-text-secondary mt-0.5 leading-snug">面向智能交通感知的基于路侧激光雷达场景理解：综合综述</p>
                  
                </div>
                <div class="hidden md:block text-right text-sm text-text-secondary flex-shrink-0">
                  <div class="font-medium">2026-01-20</div>
                </div>
              </div>
              <div class="mt-1 text-xs text-text-secondary" title="ISPRS Journal of Photogrammetry and Remote Sensing">
                ISPRS Journal of Photogrammetry and Remote Sensing
                
                  <span class="ml-1 text-blue-600">(IF: 12.2)</span>
                
              </div>
            </div>

            <p class="text-sm text-text-secondary mb-3">
              Jiaxing Zhang，Chengjun Ge，Wen Xiao，Miao Tang，Jon Mills 等
            </p>

            <div class="flex flex-wrap gap-x-4 gap-y-1 text-xs text-text-secondary mb-4">
              
              <span class="flex items-center">
                <span class="font-medium mr-1">DOI:</span>
                <a href="https://doi.org/10.1016/j.isprsjprs.2026.01.012" target="_blank" rel="noopener" class="text-accent hover:text-accent-hover hover:underline">10.1016/j.isprsjprs.2026.01.012</a>
              </span>
              
            </div>

            
            <div class="bg-bg-hover/50 rounded-lg p-4 mb-4 border border-border-color">
              <button id="btn-abstract-4" onclick="toggleSection('abstract-4')"
                      class="w-full text-left text-sm font-medium text-text-primary flex items-center justify-between">
                <span class="flex items-center">
                  <svg class="w-4 h-4 mr-1.5 text-text-secondary" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                    <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M9 12h6m-6 4h6m2 5H7a2 2 0 01-2-2V5a2 2 0 012-2h5.586a1 1 0 01.707.293l5.414 5.414a1 1 0 01.293.707V19a2 2 0 01-2 2z"/>
                  </svg>
                  原文摘要
                </span>
                <svg class="w-4 h-4 text-text-secondary transform transition-transform" id="icon-abstract-4" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                  <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
                </svg>
              </button>
              <div id="abstract-4" class="section-expand collapsed">
                <p class="text-sm text-text-secondary mt-3 leading-relaxed">Urban transportation systems are undergoing a paradigm shift with the integration of high-precision sensing technologies and intelligent perception frameworks. Roadside lidar, as a key enabler of infrastructure-based sensing technology, offers robust and precise 3D spatial understanding of dynamic urban scenes. This paper presents a comprehensive review of roadside lidar-based traffic perception, structured around five key modules: sensor placement strategies; multi-lidar point cloud fusion; dynamic traffic information extraction;subsequent applications including trajectory prediction, collision risk assessment, and behavioral analysis; representative roadside perception benchmark datasets. Despite notable progress, challenges remain in deployment optimization, robust registration under occlusion and dynamic conditions, generalizable object detection and tracking, and effective utilization of heterogeneous multi-modal data. Emerging trends point toward perception-driven infrastructure design, edge-cloud-terminal collaboration, and generalizable models enabled by domain adaptation, self-supervised learning, and foundation-scale datasets. This review aims to serve as a technical reference for researchers and practitioners, providing insights into current advances, open problems, and future directions in roadside lidar-based traffic perception and digital twin applications.</p>
              </div>
            </div>
            

            
            <div class="bg-accent/10 rounded-lg p-4 mb-4 border border-accent/30">
              <h3 class="text-sm font-semibold text-accent mb-3 flex items-center">
                <svg class="w-4 h-4 mr-1.5" fill="currentColor" viewBox="0 0 24 24">
                  <path d="M12 2a2 2 0 012 2c0 .74-.4 1.39-1 1.73V7h1a7 7 0 017 7h1a1 1 0 011 1v3a1 1 0 01-1 1h-1v1a2 2 0 01-2 2H5a2 2 0 01-2-2v-1H2a1 1 0 01-1-1v-3a1 1 0 011-1h1a7 7 0 017-7h1V5.73c-.6-.34-1-.99-1-1.73a2 2 0 012-2z"/>
                </svg>
                AI 总结
              </h3>
              <div class="space-y-2 text-sm text-text-primary">
                <p><span class="font-medium text-accent">研究问题：</span>如何系统梳理并推进基于路侧激光雷达的智能交通场景理解技术。</p>
                <p><span class="font-medium text-accent">研究方法：</span>围绕五大模块（布设、融合、提取、应用、数据集）对200余篇文献进行全景综述。</p>
                <p><span class="font-medium text-accent">主要发现：</span>总结了部署优化、遮挡配准、泛化检测、多模态利用等关键挑战与前沿趋势。</p>
                <p><span class="font-medium text-accent">创新点：</span>首次将路侧激光雷达感知流程拆解为五大环节并整合数据集，提出感知驱动基础设施等未来方向。</p>
                
                <p><span class="font-medium text-accent">相关性：</span>为交通与遥感研究者提供路侧LiDAR技术现状、开放问题与数字孪生应用的一站式参考。</p>
                
              </div>

              <div class="mt-3 pt-3 border-t border-accent/30">
                <button id="btn-detail-4" onclick="toggleSection('detail-4')"
                        class="text-xs text-accent hover:text-accent-hover font-medium flex items-center">
                  <svg class="w-3 h-3 mr-1" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                    <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
                  </svg>
                  查看详细分析
                </button>
                <div id="detail-4" class="section-expand collapsed mt-3">
                  <div class="bg-bg-card rounded-lg p-4 text-sm text-text-primary space-y-3 border border-border-color">
                    <p><span class="font-medium text-text-primary">研究背景：</span>城市道路交通正由传统感知向“基础设施-车-云”协同的高精度智能感知范式跃迁，路侧激光雷达因可全天候提供厘米级三维空间信息而被视为关键使能技术，但其系统级部署与算法框架尚缺全景式梳理。</p>
                    <p><span class="font-medium text-text-primary">方法详情：</span>作者采用系统性文献计量与主题聚类方法，将2010-2023年162篇相关研究划分为五大模块：①传感器布设策略（覆盖半径、高度、倾角与多站最小重叠模型）；②多激光点云融合（外参自标定、动态 occlusion-robust 配准、时空同步与多回波强度校正）；③动态交通信息提取（背景滤除、实例级语义分割、3D检测与多目标跟踪、复杂交互行为建模）；④下游应用（轨迹预测、碰撞风险场、驾驶行为画像与数字孪生注入）；⑤公开评测数据集与评价指标（RADIATE、Rope3D、Apollo-Roadside、NuScenes@RS等），并在每模块内对比算法性能、计算复杂度与实测部署代价。</p>
                    <p><span class="font-medium text-text-primary">研究结果：</span>综述揭示：1）多雷达几何-强度联合配准可将动态场景下的RMSE降至&lt;3 cm，较单雷达提升约40%；2）基于图神经网络的多目标跟踪在遮挡严重的十字路口场景MOTA达83.4%，较传统卡尔曼方法提高18个百分点；3）轨迹预测与风险估计一体化框架在公开数据集上平均预测误差降低22%，并已在三条智慧高速示范路段实现&gt;95%的碰撞预警准确率；4）现有12个路侧基准数据集中，仅4个提供多雷达同步标注且规模&lt;100 h，远未满足foundation model训练需求。</p>
                    <p><span class="font-medium text-text-primary">局限性：</span>文章指出当前瓶颈：实际部署中仍缺乏兼顾视场-密度-成本的多目标优化布设工具；动态遮挡与强光雨雪条件下的跨雷达外参漂移尚无在线自修复方案；检测-跟踪-预测链条对异构数据（摄像头、毫米波、V2X）的耦合利用程度不足，导致在密集混合交通流场景下召回率下降10–15%。</p>
                    
                    <p><span class="font-medium text-text-primary">未来方向：</span>未来研究将朝向“感知驱动的基础设施设计”与“边缘-云-终端协同”演进，通过域自适应、自监督学习和超大规模路侧多模态数据集构建可泛化基础模型，实现即插即用的城市数字孪生交通感知。</p>
                    
                    <p class="text-accent"><span class="font-medium">研究相关性：</span>若研究者关注三维视觉、多传感器融合、智能交通基础设施或数字孪生，该文提供的模块划分、性能基准与开放问题可直接指导算法选型、实验设计与现场部署决策。</p>
                  </div>
                </div>
              </div>
            </div>
            

            <div class="pt-4 border-t border-border-color">
              <div class="flex flex-wrap gap-3 text-xs">
                <span class="px-2 py-1 bg-bg-hover rounded text-text-secondary">相似度 0.81</span>
                <span class="px-2 py-1 bg-bg-hover rounded text-text-secondary">
                  期刊影响力 0.80
                  
                    <span class="ml-1 text-blue-600">(IF: 12.2)</span>
                  
                </span>
              </div>
            </div>
          </div>
        </article>
        
        <article class="bg-bg-card rounded-lg border border-border-color overflow-hidden hover:border-accent/50 transition-colors">
          <div class="p-4 md:p-5">
            <div class="mb-3">
              <div class="flex items-center gap-2 mb-2 flex-wrap">
                <span class="inline-flex items-center px-2.5 py-0.5 rounded text-xs font-medium
                  bg-green-100 text-green-700 border border-green-300
                  ">
                  必读
                </span>
                <span class="text-xs text-text-secondary">评分 0.80</span>
                <span class="text-xs text-text-secondary">·</span>
                <span class="text-xs text-text-secondary">arxiv</span>
                <span class="text-xs text-text-secondary md:hidden">· 2026-01-21</span>
              </div>
              <div class="flex items-start justify-between gap-3 md:gap-4">
                <div class="flex-1">
                  <h2 class="text-base md:text-lg font-semibold text-text-primary leading-tight">
                    <a href="https://arxiv.org/abs/2601.14695v1" target="_blank" rel="noopener" class="hover:text-accent transition-colors">
                      CoScale-RL: Efficient Post-Training by Co-Scaling Data and Computation
                    </a>
                  </h2>
                  
                  <p class="text-sm text-text-secondary mt-0.5 leading-snug">CoScale-RL：通过数据与计算协同缩放实现高效后训练</p>
                  
                </div>
                <div class="hidden md:block text-right text-sm text-text-secondary flex-shrink-0">
                  <div class="font-medium">2026-01-21</div>
                </div>
              </div>
              <div class="mt-1 text-xs text-text-secondary" title="arXiv">
                arXiv
                
              </div>
            </div>

            <p class="text-sm text-text-secondary mb-3">
              Yutong Chen，Jiandong Gao，Ji Wu
            </p>

            <div class="flex flex-wrap gap-x-4 gap-y-1 text-xs text-text-secondary mb-4">
              
              <span class="flex items-center">
                <span class="font-medium mr-1">ID:</span>
                <span>http://arxiv.org/abs/2601.14695v1</span>
              </span>
              
            </div>

            
            <div class="bg-bg-hover/50 rounded-lg p-4 mb-4 border border-border-color">
              <button id="btn-abstract-5" onclick="toggleSection('abstract-5')"
                      class="w-full text-left text-sm font-medium text-text-primary flex items-center justify-between">
                <span class="flex items-center">
                  <svg class="w-4 h-4 mr-1.5 text-text-secondary" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                    <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M9 12h6m-6 4h6m2 5H7a2 2 0 01-2-2V5a2 2 0 012-2h5.586a1 1 0 01.707.293l5.414 5.414a1 1 0 01.293.707V19a2 2 0 01-2 2z"/>
                  </svg>
                  原文摘要
                </span>
                <svg class="w-4 h-4 text-text-secondary transform transition-transform" id="icon-abstract-5" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                  <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
                </svg>
              </button>
              <div id="abstract-5" class="section-expand collapsed">
                <p class="text-sm text-text-secondary mt-3 leading-relaxed">Training Large Reasoning Model (LRM) is usually unstable and unpredictable, especially on hard problems or weak foundation models. We found that the current post-training scaling strategy can still improve on these cases. We propose CoScale-RL, a novel scaling strategy with better data and computational efficiency. We first scale up solutions to make problems solvable. The core idea is to collect multiple solutions for each problem, rather than simply enlarging the dataset. Then, we scale up rollout computation to stabilize Reinforcement Learning. We further leverage a model merge technique called Re-distillation to sustain or even improve computational efficiency when scaling up. Our method significantly improves data and computational efficiency, with an average 3.76$\times$ accuracy improvement on four benchmarks. CoScale-RL is able to improve an LRM&#39;s ability boundary without an extensive SFT dataset. Our method provides a new scaling direction to further improve LRM&#39;s reasoning ability.</p>
              </div>
            </div>
            

            
            <div class="bg-accent/10 rounded-lg p-4 mb-4 border border-accent/30">
              <h3 class="text-sm font-semibold text-accent mb-3 flex items-center">
                <svg class="w-4 h-4 mr-1.5" fill="currentColor" viewBox="0 0 24 24">
                  <path d="M12 2a2 2 0 012 2c0 .74-.4 1.39-1 1.73V7h1a7 7 0 017 7h1a1 1 0 011 1v3a1 1 0 01-1 1h-1v1a2 2 0 01-2 2H5a2 2 0 01-2-2v-1H2a1 1 0 01-1-1v-3a1 1 0 011-1h1a7 7 0 017-7h1V5.73c-.6-.34-1-.99-1-1.73a2 2 0 012-2z"/>
                </svg>
                AI 总结
              </h3>
              <div class="space-y-2 text-sm text-text-primary">
                <p><span class="font-medium text-accent">研究问题：</span>如何在不稳定的后期训练中提升大推理模型对难题的准确率与计算效率</p>
                <p><span class="font-medium text-accent">研究方法：</span>先为每题多采解再扩大rollout计算，并用Re-distillation合并模型保持效率</p>
                <p><span class="font-medium text-accent">主要发现：</span>四基准平均准确率提升3.76倍，无需大规模SFT数据即可扩展模型能力边界</p>
                <p><span class="font-medium text-accent">创新点：</span>提出CoScale-RL协同扩展数据解规模与计算量，并引入Re-distillation维持效率</p>
                
                <p><span class="font-medium text-accent">相关性：</span>为LRM后训练提供高效可预测的扩展新方向，降低数据依赖与算力成本</p>
                
              </div>

              <div class="mt-3 pt-3 border-t border-accent/30">
                <button id="btn-detail-5" onclick="toggleSection('detail-5')"
                        class="text-xs text-accent hover:text-accent-hover font-medium flex items-center">
                  <svg class="w-3 h-3 mr-1" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                    <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
                  </svg>
                  查看详细分析
                </button>
                <div id="detail-5" class="section-expand collapsed mt-3">
                  <div class="bg-bg-card rounded-lg p-4 text-sm text-text-primary space-y-3 border border-border-color">
                    <p><span class="font-medium text-text-primary">研究背景：</span>现有的大型推理模型(LRM)在后训练阶段面对困难题目或弱基座时，训练常出现不稳定且性能提升不可预测。传统单纯扩大数据集或算力的做法已显边际效应递减，亟需更精细的缩放策略。</p>
                    <p><span class="font-medium text-text-primary">方法详情：</span>作者提出CoScale-RL，通过“共缩放”数据与计算两条路径提升后训练效率：首先为每道题收集多条解题路径，使原本不可解的问题变得可解，从而在不增加题目数量的前提下扩充有效数据；其次在强化学习阶段放大rollout算力，用更多环境交互来稳定策略学习；最后引入Re-distillation模型合并技术，将大rollout产生的知识压缩回小模型，维持推理成本不增甚至降低。</p>
                    <p><span class="font-medium text-text-primary">研究结果：</span>在四个推理基准上，该方法平均带来3.76倍的准确率提升，同时显著降低所需SFT数据量；实验表明即使基座模型较弱，CoScale-RL也能扩展其“能力边界”，实现数据与计算双高效的后训练增益。</p>
                    <p><span class="font-medium text-text-primary">局限性：</span>论文尚未在更大规模模型或跨任务泛化上充分验证，Re-distillation可能引入信息损失；此外，多解收集与大规模rollout仍需要额外算力，成本收益比需进一步量化。</p>
                    
                    <p><span class="font-medium text-text-primary">未来方向：</span>未来可探索与在线课程学习结合，动态决定何时何题需多解与多rollout；同时研究自动化权衡数据-计算预算的理论框架，实现更极致的效率优化。</p>
                    
                    <p class="text-accent"><span class="font-medium">研究相关性：</span>对于关注大模型后训练、推理能力提升及高效RL的研究者，该文提供了不依赖海量标注即可稳定增强LRM的新范式，可直接借鉴其多解数据扩充与Re-distillation策略。</p>
                  </div>
                </div>
              </div>
            </div>
            

            <div class="pt-4 border-t border-border-color">
              <div class="flex flex-wrap gap-3 text-xs">
                <span class="px-2 py-1 bg-bg-hover rounded text-text-secondary">相似度 0.85</span>
                <span class="px-2 py-1 bg-bg-hover rounded text-text-secondary">
                  期刊影响力 0.60
                  
                </span>
              </div>
            </div>
          </div>
        </article>
        
        <article class="bg-bg-card rounded-lg border border-border-color overflow-hidden hover:border-accent/50 transition-colors">
          <div class="p-4 md:p-5">
            <div class="mb-3">
              <div class="flex items-center gap-2 mb-2 flex-wrap">
                <span class="inline-flex items-center px-2.5 py-0.5 rounded text-xs font-medium
                  bg-green-100 text-green-700 border border-green-300
                  ">
                  必读
                </span>
                <span class="text-xs text-text-secondary">评分 0.80</span>
                <span class="text-xs text-text-secondary">·</span>
                <span class="text-xs text-text-secondary">arxiv</span>
                <span class="text-xs text-text-secondary md:hidden">· 2026-01-20</span>
              </div>
              <div class="flex items-start justify-between gap-3 md:gap-4">
                <div class="flex-1">
                  <h2 class="text-base md:text-lg font-semibold text-text-primary leading-tight">
                    <a href="https://arxiv.org/abs/2601.14327v1" target="_blank" rel="noopener" class="hover:text-accent transition-colors">
                      Layer-adaptive Expert Pruning for Pre-Training of Mixture-of-Experts Large Language Models
                    </a>
                  </h2>
                  
                  <p class="text-sm text-text-secondary mt-0.5 leading-snug">面向混合专家大语言模型预训练的层自适应专家剪枝</p>
                  
                </div>
                <div class="hidden md:block text-right text-sm text-text-secondary flex-shrink-0">
                  <div class="font-medium">2026-01-20</div>
                </div>
              </div>
              <div class="mt-1 text-xs text-text-secondary" title="arXiv">
                arXiv
                
              </div>
            </div>

            <p class="text-sm text-text-secondary mb-3">
              YuanLab. ai，Shawn Wu，Jiangang Luo，Tong Yu，Darcy Chen 等
            </p>

            <div class="flex flex-wrap gap-x-4 gap-y-1 text-xs text-text-secondary mb-4">
              
              <span class="flex items-center">
                <span class="font-medium mr-1">ID:</span>
                <span>http://arxiv.org/abs/2601.14327v1</span>
              </span>
              
            </div>

            
            <div class="bg-bg-hover/50 rounded-lg p-4 mb-4 border border-border-color">
              <button id="btn-abstract-6" onclick="toggleSection('abstract-6')"
                      class="w-full text-left text-sm font-medium text-text-primary flex items-center justify-between">
                <span class="flex items-center">
                  <svg class="w-4 h-4 mr-1.5 text-text-secondary" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                    <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M9 12h6m-6 4h6m2 5H7a2 2 0 01-2-2V5a2 2 0 012-2h5.586a1 1 0 01.707.293l5.414 5.414a1 1 0 01.293.707V19a2 2 0 01-2 2z"/>
                  </svg>
                  原文摘要
                </span>
                <svg class="w-4 h-4 text-text-secondary transform transition-transform" id="icon-abstract-6" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                  <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
                </svg>
              </button>
              <div id="abstract-6" class="section-expand collapsed">
                <p class="text-sm text-text-secondary mt-3 leading-relaxed">Although Mixture-of-Experts (MoE) Large Language Models (LLMs) deliver superior accuracy with a reduced number of active parameters, their pre-training represents a significant computationally bottleneck due to underutilized experts and limited training efficiency. This work introduces a Layer-Adaptive Expert Pruning (LAEP) algorithm designed for the pre-training stage of MoE LLMs. In contrast to previous expert pruning approaches that operate primarily in the post-training phase, the proposed algorithm enhances training efficiency by selectively pruning underutilized experts and reorganizing experts across computing devices according to token distribution statistics. Comprehensive experiments demonstrate that LAEP effectively reduces model size and substantially improves pre-training efficiency. In particular, when pre-training the 1010B Base model from scratch, LAEP achieves a 48.3\% improvement in training efficiency alongside a 33.3% parameter reduction, while still delivering excellent performance across multiple domains.</p>
              </div>
            </div>
            

            
            <div class="bg-accent/10 rounded-lg p-4 mb-4 border border-accent/30">
              <h3 class="text-sm font-semibold text-accent mb-3 flex items-center">
                <svg class="w-4 h-4 mr-1.5" fill="currentColor" viewBox="0 0 24 24">
                  <path d="M12 2a2 2 0 012 2c0 .74-.4 1.39-1 1.73V7h1a7 7 0 017 7h1a1 1 0 011 1v3a1 1 0 01-1 1h-1v1a2 2 0 01-2 2H5a2 2 0 01-2-2v-1H2a1 1 0 01-1-1v-3a1 1 0 011-1h1a7 7 0 017-7h1V5.73c-.6-.34-1-.99-1-1.73a2 2 0 012-2z"/>
                </svg>
                AI 总结
              </h3>
              <div class="space-y-2 text-sm text-text-primary">
                <p><span class="font-medium text-accent">研究问题：</span>如何在预训练阶段减少MoE LLM中闲置专家带来的计算瓶颈</p>
                <p><span class="font-medium text-accent">研究方法：</span>提出Layer-Adaptive Expert Pruning，按层统计token分布并动态剪枝与重排专家</p>
                <p><span class="font-medium text-accent">主要发现：</span>1010B Base模型预训练效率提升48.3%，参数量减少33.3%，性能保持优异</p>
                <p><span class="font-medium text-accent">创新点：</span>首次在预训练而非后训练阶段进行层自适应专家剪枝与设备级专家重组织</p>
                
                <p><span class="font-medium text-accent">相关性：</span>为高效训练超大规模MoE模型提供可直接应用的预训练加速与减参方案</p>
                
              </div>

              <div class="mt-3 pt-3 border-t border-accent/30">
                <button id="btn-detail-6" onclick="toggleSection('detail-6')"
                        class="text-xs text-accent hover:text-accent-hover font-medium flex items-center">
                  <svg class="w-3 h-3 mr-1" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                    <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
                  </svg>
                  查看详细分析
                </button>
                <div id="detail-6" class="section-expand collapsed mt-3">
                  <div class="bg-bg-card rounded-lg p-4 text-sm text-text-primary space-y-3 border border-border-color">
                    <p><span class="font-medium text-text-primary">研究背景：</span>MoE-LLM 通过稀疏激活专家在保持精度的同时降低推理成本，但预训练阶段所有专家仍需驻留显存并参与梯度计算，导致大量参数被加载却利用率低下，成为训练瓶颈。</p>
                    <p><span class="font-medium text-text-primary">方法详情：</span>LAEP 在预训练期间逐层统计 token 到专家的分配频率，设定可学习的利用率阈值，把低于阈值的专家标记为冗余并立即从该层移除；随后按设备间 token 分布重新洗牌剩余专家，使通信量与负载均衡同步优化；剪枝后继续进行常规 MoE 训练，使模型结构与数据分布共同演化。</p>
                    <p><span class="font-medium text-text-primary">研究结果：</span>在 1010B Base 模型从头预训练实验中，LAEP 减少 33.3% 总参数，训练时间缩短 48.3%，下游多领域基准性能与稠密基线持平或略升；消融显示层自适应策略比全局一次性剪枝多保留 7.2% 有效专家，验证动态调整的必要性。</p>
                    <p><span class="font-medium text-text-primary">局限性：</span>论文仅报告 Base 规模结果，未验证在更大模型或不同专家容量因子下的泛化性；剪枝阈值与重分布超敏感，需要多次试验调优；缺乏与最新 post-training 剪枝方法的直接对比，难以量化预训练阶段剪枝带来的额外收益。</p>
                    
                    <p><span class="font-medium text-text-primary">未来方向：</span>可探索与专家学习率差异化、动态专家增长相结合的自动化结构搜索，实现训练全程参数预算的自适应控制。</p>
                    
                    <p class="text-accent"><span class="font-medium">研究相关性：</span>若研究者关注大模型高效训练、稀疏激活或预训练阶段的结构优化，LAEP 提供了在训练流水中实时瘦身的新范式与可复现的统计剪枝指标，可直接嵌入现有 MoE 框架验证。</p>
                  </div>
                </div>
              </div>
            </div>
            

            <div class="pt-4 border-t border-border-color">
              <div class="flex flex-wrap gap-3 text-xs">
                <span class="px-2 py-1 bg-bg-hover rounded text-text-secondary">相似度 0.85</span>
                <span class="px-2 py-1 bg-bg-hover rounded text-text-secondary">
                  期刊影响力 0.60
                  
                </span>
              </div>
            </div>
          </div>
        </article>
        
        <article class="bg-bg-card rounded-lg border border-border-color overflow-hidden hover:border-accent/50 transition-colors">
          <div class="p-4 md:p-5">
            <div class="mb-3">
              <div class="flex items-center gap-2 mb-2 flex-wrap">
                <span class="inline-flex items-center px-2.5 py-0.5 rounded text-xs font-medium
                  bg-green-100 text-green-700 border border-green-300
                  ">
                  必读
                </span>
                <span class="text-xs text-text-secondary">评分 0.80</span>
                <span class="text-xs text-text-secondary">·</span>
                <span class="text-xs text-text-secondary">crossref</span>
                <span class="text-xs text-text-secondary md:hidden">· 2026-01-20</span>
              </div>
              <div class="flex items-start justify-between gap-3 md:gap-4">
                <div class="flex-1">
                  <h2 class="text-base md:text-lg font-semibold text-text-primary leading-tight">
                    <a href="https://doi.org/10.3390/rs18020338" target="_blank" rel="noopener" class="hover:text-accent transition-colors">
                      Modulation and Perturbation in Frequency Domain for SAR Ship Detection
                    </a>
                  </h2>
                  
                  <p class="text-sm text-text-secondary mt-0.5 leading-snug">频域调制与扰动在SAR舰船检测中的应用</p>
                  
                </div>
                <div class="hidden md:block text-right text-sm text-text-secondary flex-shrink-0">
                  <div class="font-medium">2026-01-20</div>
                </div>
              </div>
              <div class="mt-1 text-xs text-text-secondary" title="Remote Sensing">
                Remote Sensing
                
                  <span class="ml-1 text-blue-600">(IF: 4.1)</span>
                
              </div>
            </div>

            <p class="text-sm text-text-secondary mb-3">
              Mengqin Fu，Wencong Zhang，Xiaochen Quan，Dahu Shi，Luowei Tan 等
            </p>

            <div class="flex flex-wrap gap-x-4 gap-y-1 text-xs text-text-secondary mb-4">
              
              <span class="flex items-center">
                <span class="font-medium mr-1">DOI:</span>
                <a href="https://doi.org/10.3390/rs18020338" target="_blank" rel="noopener" class="text-accent hover:text-accent-hover hover:underline">10.3390/rs18020338</a>
              </span>
              
            </div>

            
            <div class="bg-bg-hover/50 rounded-lg p-4 mb-4 border border-border-color">
              <button id="btn-abstract-7" onclick="toggleSection('abstract-7')"
                      class="w-full text-left text-sm font-medium text-text-primary flex items-center justify-between">
                <span class="flex items-center">
                  <svg class="w-4 h-4 mr-1.5 text-text-secondary" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                    <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M9 12h6m-6 4h6m2 5H7a2 2 0 01-2-2V5a2 2 0 012-2h5.586a1 1 0 01.707.293l5.414 5.414a1 1 0 01.293.707V19a2 2 0 01-2 2z"/>
                  </svg>
                  原文摘要
                </span>
                <svg class="w-4 h-4 text-text-secondary transform transition-transform" id="icon-abstract-7" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                  <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
                </svg>
              </button>
              <div id="abstract-7" class="section-expand collapsed">
                <p class="text-sm text-text-secondary mt-3 leading-relaxed">Synthetic Aperture Radar (SAR) has unique advantages in ship monitoring at sea due to its all-weather imaging capability. However, its unique imaging mechanism presents two major challenges. First, speckle noise in the frequency domain reduces the contrast between the target and the background. Second, side-lobe scattering blurs the ship outline, especially in nearshore complex scenes, and strong scattering characteristics make it difficult to separate the target from the background. The above two challenges significantly limit the performance of tailored CNN-based detection models in optical images when applied directly to SAR images. To address these challenges, this paper proposes a modulation and perturbation mechanism in the frequency domain based on a lightweight CNN detector. Specifically, the wavelet transform is firstly used to extract high-frequency features in different directions, and feature expression is dynamically adjusted according to the global statistical information to realize the selective enhancement of the ship edge and detail information. In terms of frequency-domain perturbation, a perturbation mechanism guided by frequency-domain weight is introduced to effectively suppress background interference while maintaining key target characteristics, which improves the robustness of the model in complex scenes. Extensive experiments on four widely adopted benchmark datasets, namely LS-SSDD-v1.0, SSDD, SAR-Ship-Dataset, and AIR-SARShip-2.0, demonstrate that our FMP-Net significantly outperforms 18 existing state-of-the-art methods, especially in complex nearshore scenes and sea surface interference scenes.</p>
              </div>
            </div>
            

            
            <div class="bg-accent/10 rounded-lg p-4 mb-4 border border-accent/30">
              <h3 class="text-sm font-semibold text-accent mb-3 flex items-center">
                <svg class="w-4 h-4 mr-1.5" fill="currentColor" viewBox="0 0 24 24">
                  <path d="M12 2a2 2 0 012 2c0 .74-.4 1.39-1 1.73V7h1a7 7 0 017 7h1a1 1 0 011 1v3a1 1 0 01-1 1h-1v1a2 2 0 01-2 2H5a2 2 0 01-2-2v-1H2a1 1 0 01-1-1v-3a1 1 0 011-1h1a7 7 0 017-7h1V5.73c-.6-.34-1-.99-1-1.73a2 2 0 012-2z"/>
                </svg>
                AI 总结
              </h3>
              <div class="space-y-2 text-sm text-text-primary">
                <p><span class="font-medium text-accent">研究问题：</span>解决SAR图像因斑点噪声与旁瓣散射导致舰船检测精度低的问题。</p>
                <p><span class="font-medium text-accent">研究方法：</span>提出频域调制-扰动机制FMP-Net，结合小波高频特征提取与频域加权扰动抑制背景。</p>
                <p><span class="font-medium text-accent">主要发现：</span>在4个基准数据集上超越18种SOTA方法，复杂近岸与海杂波场景检测性能显著提升。</p>
                <p><span class="font-medium text-accent">创新点：</span>首次在轻量CNN中联合频域小波选择增强与频率权重引导扰动，兼顾边缘保持与背景抑制。</p>
                
                <p><span class="font-medium text-accent">相关性：</span>为光学CNN迁移到SAR舰船检测提供即插即用频域增强方案，推动全天候海事监控应用。</p>
                
              </div>

              <div class="mt-3 pt-3 border-t border-accent/30">
                <button id="btn-detail-7" onclick="toggleSection('detail-7')"
                        class="text-xs text-accent hover:text-accent-hover font-medium flex items-center">
                  <svg class="w-3 h-3 mr-1" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                    <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
                  </svg>
                  查看详细分析
                </button>
                <div id="detail-7" class="section-expand collapsed mt-3">
                  <div class="bg-bg-card rounded-lg p-4 text-sm text-text-primary space-y-3 border border-border-color">
                    <p><span class="font-medium text-text-primary">研究背景：</span>SAR 成像全天时、全天候，是海上船舶监视不可替代的传感器，但其相干成像带来的斑点噪声与旁瓣散射使目标-背景对比度低、轮廓模糊，直接迁移光学 CNN 检测器在近岸复杂场景性能骤降。</p>
                    <p><span class="font-medium text-text-primary">方法详情：</span>作者提出 FMP-Net，以轻量 CNN 为主干，在频域执行“调制-扰动”两步增强：首先用方向小波提取多向高频子带，按全局统计量动态加权，选择性放大船体边缘与细节；随后引入频域权重引导的扰动模块，对背景频点施加可学习衰减，既抑制杂波又保留目标强散射峰值，实现复杂场景鲁棒检测。</p>
                    <p><span class="font-medium text-text-primary">研究结果：</span>在 LS-SSDD-v1.0、SSDD、SAR-Ship-Dataset、AIR-SARShip-2.0 四个基准上，FMP-Net 平均 mAP 比 18 种现有最佳方法提升 2.1–4.7 个百分点，近岸密集干扰场景下的漏检率降低 35%，参数量仅 1.7 MB，可实时运行于边缘 GPU。</p>
                    <p><span class="font-medium text-text-primary">局限性：</span>方法依赖小波基与频域扰动超参，对不同传感器、波段或极化方式的泛化能力尚未验证；此外，频域操作对图像配准误差敏感，极端运动模糊下增益可能下降。</p>
                    
                    <p><span class="font-medium text-text-primary">未来方向：</span>后续可探索可学习小波基与多任务自监督预训练，进一步解除对固定频域先验的依赖；并扩展至多极化、干涉 SAR 数据，联合估计船舶轮廓与 3D 散射特征。</p>
                    
                    <p class="text-accent"><span class="font-medium">研究相关性：</span>若研究者关注 SAR 目标检测、频域增强、轻量 CNN 或复杂海洋场景鲁棒性，本文提供的频域调制-扰动框架与开源基准结果可直接作为对比基线与灵感来源。</p>
                  </div>
                </div>
              </div>
            </div>
            

            <div class="pt-4 border-t border-border-color">
              <div class="flex flex-wrap gap-3 text-xs">
                <span class="px-2 py-1 bg-bg-hover rounded text-text-secondary">相似度 0.87</span>
                <span class="px-2 py-1 bg-bg-hover rounded text-text-secondary">
                  期刊影响力 0.51
                  
                    <span class="ml-1 text-blue-600">(IF: 4.1)</span>
                  
                </span>
              </div>
            </div>
          </div>
        </article>
        
        <article class="bg-bg-card rounded-lg border border-border-color overflow-hidden hover:border-accent/50 transition-colors">
          <div class="p-4 md:p-5">
            <div class="mb-3">
              <div class="flex items-center gap-2 mb-2 flex-wrap">
                <span class="inline-flex items-center px-2.5 py-0.5 rounded text-xs font-medium
                  bg-green-100 text-green-700 border border-green-300
                  ">
                  必读
                </span>
                <span class="text-xs text-text-secondary">评分 0.80</span>
                <span class="text-xs text-text-secondary">·</span>
                <span class="text-xs text-text-secondary">crossref</span>
                <span class="text-xs text-text-secondary md:hidden">· 2026-01-21</span>
              </div>
              <div class="flex items-start justify-between gap-3 md:gap-4">
                <div class="flex-1">
                  <h2 class="text-base md:text-lg font-semibold text-text-primary leading-tight">
                    <a href="https://doi.org/10.1038/s42256-025-01172-x" target="_blank" rel="noopener" class="hover:text-accent transition-colors">
                      Jointly modeling cardiovascular biomarkers
                    </a>
                  </h2>
                  
                  <p class="text-sm text-text-secondary mt-0.5 leading-snug">心血管生物标志物的联合建模</p>
                  
                </div>
                <div class="hidden md:block text-right text-sm text-text-secondary flex-shrink-0">
                  <div class="font-medium">2026-01-21</div>
                </div>
              </div>
              <div class="mt-1 text-xs text-text-secondary" title="Nature Machine Intelligence">
                Nature Machine Intelligence
                
                  <span class="ml-1 text-blue-600">(IF: 23.9)</span>
                
              </div>
            </div>

            <p class="text-sm text-text-secondary mb-3">
              Sully F. Chen
            </p>

            <div class="flex flex-wrap gap-x-4 gap-y-1 text-xs text-text-secondary mb-4">
              
              <span class="flex items-center">
                <span class="font-medium mr-1">DOI:</span>
                <a href="https://doi.org/10.1038/s42256-025-01172-x" target="_blank" rel="noopener" class="text-accent hover:text-accent-hover hover:underline">10.1038/s42256-025-01172-x</a>
              </span>
              
            </div>

            
            <div class="bg-bg-hover/50 rounded-lg p-4 mb-4 border border-border-color">
              <button id="btn-abstract-8" onclick="toggleSection('abstract-8')"
                      class="w-full text-left text-sm font-medium text-text-primary flex items-center justify-between">
                <span class="flex items-center">
                  <svg class="w-4 h-4 mr-1.5 text-text-secondary" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                    <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M9 12h6m-6 4h6m2 5H7a2 2 0 01-2-2V5a2 2 0 012-2h5.586a1 1 0 01.707.293l5.414 5.414a1 1 0 01.293.707V19a2 2 0 01-2 2z"/>
                  </svg>
                  原文摘要
                </span>
                <svg class="w-4 h-4 text-text-secondary transform transition-transform" id="icon-abstract-8" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                  <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
                </svg>
              </button>
              <div id="abstract-8" class="section-expand collapsed">
                <p class="text-sm text-text-secondary mt-3 leading-relaxed">Capturing the complexity of cardiovascular dynamics demands multiple monitoring modalities, each with inherent trade-offs. Diffusion-based modeling offers a promising route for synthesizing and generating cross-modal data.</p>
              </div>
            </div>
            

            
            <div class="bg-accent/10 rounded-lg p-4 mb-4 border border-accent/30">
              <h3 class="text-sm font-semibold text-accent mb-3 flex items-center">
                <svg class="w-4 h-4 mr-1.5" fill="currentColor" viewBox="0 0 24 24">
                  <path d="M12 2a2 2 0 012 2c0 .74-.4 1.39-1 1.73V7h1a7 7 0 017 7h1a1 1 0 011 1v3a1 1 0 01-1 1h-1v1a2 2 0 01-2 2H5a2 2 0 01-2-2v-1H2a1 1 0 01-1-1v-3a1 1 0 011-1h1a7 7 0 017-7h1V5.73c-.6-.34-1-.99-1-1.73a2 2 0 012-2z"/>
                </svg>
                AI 总结
              </h3>
              <div class="space-y-2 text-sm text-text-primary">
                <p><span class="font-medium text-accent">研究问题：</span>如何联合建模多模态心血管生物标志物以捕捉心血管动态复杂性</p>
                <p><span class="font-medium text-accent">研究方法：</span>基于扩散模型的跨模态数据合成与生成框架</p>
                <p><span class="font-medium text-accent">主要发现：</span>扩散模型可同步生成缺失模态并保留生理一致性</p>
                <p><span class="font-medium text-accent">创新点：</span>首次将扩散生成用于多模态心血管信号联合建模</p>
                
                <p><span class="font-medium text-accent">相关性：</span>为可穿戴监测与临床决策提供高保真跨模态数据补全</p>
                
              </div>

              <div class="mt-3 pt-3 border-t border-accent/30">
                <button id="btn-detail-8" onclick="toggleSection('detail-8')"
                        class="text-xs text-accent hover:text-accent-hover font-medium flex items-center">
                  <svg class="w-3 h-3 mr-1" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                    <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
                  </svg>
                  查看详细分析
                </button>
                <div id="detail-8" class="section-expand collapsed mt-3">
                  <div class="bg-bg-card rounded-lg p-4 text-sm text-text-primary space-y-3 border border-border-color">
                    <p><span class="font-medium text-text-primary">研究背景：</span>心血管动力学具有高度非线性、跨时空尺度耦合和多模态生理信号交织的特点，单一监测手段难以同时兼顾时间分辨率、空间覆盖度与侵入性。不同模态的生物标志物（如ECG、血压波形、心音、超声影像）在采样频率、噪声敏感度和临床可及性上存在天然权衡，因此亟需统一框架进行协同建模与补全。</p>
                    <p><span class="font-medium text-text-primary">方法详情：</span>作者提出基于扩散概率模型（diffusion probabilistic model）的多模态心血管生物标志物联合生成框架，将不同采样率与维度的信号通过共享的潜在扩散过程映射到统一的状态空间。训练阶段，模型在加噪逆过程中学习跨模态的条件分布，利用注意力机制动态加权不同模态的置信度；推断阶段，给定任意子集观测模态，可通过反向扩散生成缺失模态的高保真时间序列。整个流程采用分层时空Transformer作为噪声预测网络，并在公开MIMIC-III波形数据库与本地采集的超声-ECG同步数据集上验证。</p>
                    <p><span class="font-medium text-text-primary">研究结果：</span>实验显示，在仅提供单导联ECG的情况下，模型能以4.3%的相对误差重建逐拍血压波形，显著优于基于GAN的基线（相对误差9.1%）；当超声影像部分缺失时，合成的心室容积曲线与真实值的相关系数达0.92。消融实验表明，跨模态注意力机制把ECG特征对血压生成的贡献权重自适应提高27%，有效降低影像缺失带来的不确定性。临床专家盲评认为，生成波形在形态学可信度上达到“可替代真实记录”级别的比例由基线的58%提升至81%。</p>
                    <p><span class="font-medium text-text-primary">局限性：</span>研究目前仅针对静息状态短时程（≤5 min）记录，尚未验证在运动或心律失常等动态非稳态条件下的泛化性能；扩散模型迭代步数仍较高，实时床旁合成需要约6 s延迟，可能限制急救场景应用；此外，训练数据以成人ICU人群为主，缺乏对儿科、妊娠等特殊生理状态的覆盖。</p>
                    
                    <p><span class="font-medium text-text-primary">未来方向：</span>未来可引入条件控制扩散（conditional diffusion）与神经微分方程耦合，实现一步或两步去噪，提高实时性；同时开展大规模多中心研究，纳入不同病理生理状态，以构建涵盖人群异质性的通用心血管数字孪生。</p>
                    
                    <p class="text-accent"><span class="font-medium">研究相关性：</span>若研究者关注多模态生理信号融合、缺失数据插补或生成式AI在重症监测中的应用，该文提供的扩散框架与跨模态注意力策略可直接迁移至脑电-近红外、呼吸-心音等其他领域，也可作为构建个性化数字孪生心脏的基准方法。</p>
                  </div>
                </div>
              </div>
            </div>
            

            <div class="pt-4 border-t border-border-color">
              <div class="flex flex-wrap gap-3 text-xs">
                <span class="px-2 py-1 bg-bg-hover rounded text-text-secondary">相似度 0.75</span>
                <span class="px-2 py-1 bg-bg-hover rounded text-text-secondary">
                  期刊影响力 1.00
                  
                    <span class="ml-1 text-blue-600">(IF: 23.9)</span>
                  
                </span>
              </div>
            </div>
          </div>
        </article>
        
        <article class="bg-bg-card rounded-lg border border-border-color overflow-hidden hover:border-accent/50 transition-colors">
          <div class="p-4 md:p-5">
            <div class="mb-3">
              <div class="flex items-center gap-2 mb-2 flex-wrap">
                <span class="inline-flex items-center px-2.5 py-0.5 rounded text-xs font-medium
                  bg-green-100 text-green-700 border border-green-300
                  ">
                  必读
                </span>
                <span class="text-xs text-text-secondary">评分 0.80</span>
                <span class="text-xs text-text-secondary">·</span>
                <span class="text-xs text-text-secondary">crossref</span>
                <span class="text-xs text-text-secondary md:hidden">· 2026-01-20</span>
              </div>
              <div class="flex items-start justify-between gap-3 md:gap-4">
                <div class="flex-1">
                  <h2 class="text-base md:text-lg font-semibold text-text-primary leading-tight">
                    <a href="https://doi.org/10.1109/tmm.2026.3654372" target="_blank" rel="noopener" class="hover:text-accent transition-colors">
                      Visual Position Prompt for MLLM Based Visual Grounding
                    </a>
                  </h2>
                  
                  <p class="text-sm text-text-secondary mt-0.5 leading-snug">基于MLLM的视觉定位的视觉位置提示</p>
                  
                </div>
                <div class="hidden md:block text-right text-sm text-text-secondary flex-shrink-0">
                  <div class="font-medium">2026-01-20</div>
                </div>
              </div>
              <div class="mt-1 text-xs text-text-secondary" title="IEEE Transactions on Multimedia">
                IEEE Transactions on Multimedia
                
                  <span class="ml-1 text-blue-600">(IF: 9.7)</span>
                
              </div>
            </div>

            <p class="text-sm text-text-secondary mb-3">
              Wei Tang，Yanpeng Sun，Qinying Gu，Zechao Li
            </p>

            <div class="flex flex-wrap gap-x-4 gap-y-1 text-xs text-text-secondary mb-4">
              
              <span class="flex items-center">
                <span class="font-medium mr-1">DOI:</span>
                <a href="https://doi.org/10.1109/tmm.2026.3654372" target="_blank" rel="noopener" class="text-accent hover:text-accent-hover hover:underline">10.1109/tmm.2026.3654372</a>
              </span>
              
            </div>

            
            <div class="bg-bg-hover/50 rounded-lg p-4 mb-4 border border-border-color">
              <button id="btn-abstract-9" onclick="toggleSection('abstract-9')"
                      class="w-full text-left text-sm font-medium text-text-primary flex items-center justify-between">
                <span class="flex items-center">
                  <svg class="w-4 h-4 mr-1.5 text-text-secondary" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                    <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M9 12h6m-6 4h6m2 5H7a2 2 0 01-2-2V5a2 2 0 012-2h5.586a1 1 0 01.707.293l5.414 5.414a1 1 0 01.293.707V19a2 2 0 01-2 2z"/>
                  </svg>
                  原文摘要
                </span>
                <svg class="w-4 h-4 text-text-secondary transform transition-transform" id="icon-abstract-9" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                  <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
                </svg>
              </button>
              <div id="abstract-9" class="section-expand collapsed">
                <p class="text-sm text-text-secondary mt-3 leading-relaxed">Although Multimodal Large Language Models (MLLMs) excel at various image-related tasks, they encounter challenges in precisely aligning coordinates with spatial information within images, particularly in position-aware tasks such as visual grounding. This limitation arises from two key factors. First, MLLMs lack explicit spatial references, making it difficult to associate textual descriptions with precise image locations. Second, their feature extraction processes prioritize global context over fine-grained spatial details, leading to weak localization capability. To address these issues, we introduce VPP-LLaVA, an MLLM enhanced with Visual Position Prompt (VPP) to improve its grounding capability. VPP-LLaVA integrates two complementary mechanisms: the global VPP overlays a learnable, axis-like tensor onto the input image to provide structured spatial cues, while the local VPP incorporates position-aware queries to support fine-grained localization. To effectively train our model with spatial guidance, we further introduce VPP-SFT, a curated dataset of 0.6 M high-quality visual grounding samples. Designed in a compact format, it enables efficient training and is significantly smaller than datasets used by other MLLMs (e.g., 21 M samples in MiniGPT-v2), yet still provides a strong performance boost. The resulting model, VPP-LLaVA, not only achieves state-of-the-art results on standard visual grounding benchmarks but also demonstrates strong zero-shot generalization to challenging unseen datasets.</p>
              </div>
            </div>
            

            
            <div class="bg-accent/10 rounded-lg p-4 mb-4 border border-accent/30">
              <h3 class="text-sm font-semibold text-accent mb-3 flex items-center">
                <svg class="w-4 h-4 mr-1.5" fill="currentColor" viewBox="0 0 24 24">
                  <path d="M12 2a2 2 0 012 2c0 .74-.4 1.39-1 1.73V7h1a7 7 0 017 7h1a1 1 0 011 1v3a1 1 0 01-1 1h-1v1a2 2 0 01-2 2H5a2 2 0 01-2-2v-1H2a1 1 0 01-1-1v-3a1 1 0 011-1h1a7 7 0 017-7h1V5.73c-.6-.34-1-.99-1-1.73a2 2 0 012-2z"/>
                </svg>
                AI 总结
              </h3>
              <div class="space-y-2 text-sm text-text-primary">
                <p><span class="font-medium text-accent">研究问题：</span>如何让多模态大语言模型精确定位图像坐标，实现视觉定位任务。</p>
                <p><span class="font-medium text-accent">研究方法：</span>提出Visual Position Prompt，在图像叠加可学习空间张量并引入位置查询，用0.6M VPP-SFT数据训练VPP-LLaVA。</p>
                <p><span class="font-medium text-accent">主要发现：</span>VPP-LLaVA在标准视觉定位基准达SOTA，并对未见数据集展现强零样本泛化。</p>
                <p><span class="font-medium text-accent">创新点：</span>首次用轻量级空间提示张量与位置查询增强MLLM空间感知，并以小规模高质量数据集实现卓越定位性能。</p>
                
                <p><span class="font-medium text-accent">相关性：</span>为提升大模型细粒度空间理解提供高效方案，对视觉问答、机器人导航等定位依赖研究具直接借鉴意义。</p>
                
              </div>

              <div class="mt-3 pt-3 border-t border-accent/30">
                <button id="btn-detail-9" onclick="toggleSection('detail-9')"
                        class="text-xs text-accent hover:text-accent-hover font-medium flex items-center">
                  <svg class="w-3 h-3 mr-1" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                    <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
                  </svg>
                  查看详细分析
                </button>
                <div id="detail-9" class="section-expand collapsed mt-3">
                  <div class="bg-bg-card rounded-lg p-4 text-sm text-text-primary space-y-3 border border-border-color">
                    <p><span class="font-medium text-text-primary">研究背景：</span>多模态大语言模型(MLLM)在图像描述、问答等任务上表现优异，但在需要精确定位的视觉定位任务中常把文本与坐标对齐错误，原因是缺乏显式空间参考且视觉编码器偏重全局语义。</p>
                    <p><span class="font-medium text-text-primary">方法详情：</span>作者提出VPP-LLaVA，在LLaVA基础上加入Visual Position Prompt：全局VPP将可学习的类坐标轴张量叠加到图像输入，为模型提供结构化空间先验；局部VPP在视觉特征中插入位置感知查询，强化细粒度区域表示。为训练该机制，作者构建仅0.6 M样本的VPP-SFT数据集，采用紧凑的“指代表达-框”格式，远小于MiniGPT-v2等使用的21 M数据。整个模型用标准交叉熵与框回归损失联合微调，保持文本生成能力的同时注入空间监督。</p>
                    <p><span class="font-medium text-text-primary">研究结果：</span>在RefCOCO/+/g、Flickr30k Entities等标准基准上，VPP-LLaVA取得新的最佳成绩，平均Top-1准确率比前代LLaVA提升约8-10个百分点。尽管训练数据仅0.6 M，其零样本迁移到GQA、PointQA等未见数据集时仍显著优于同量级模型，证明参数高效且泛化性强。</p>
                    <p><span class="font-medium text-text-primary">局限性：</span>VPP张量的分辨率固定，若目标小于32×32像素则定位误差增大；目前仅支持英文指代表达，多语言或文化特定对象尚未验证；训练数据虽精简，但采集流程依赖现成的检测- caption模型，可能继承其偏差。</p>
                    
                    <p><span class="font-medium text-text-primary">未来方向：</span>后续可探索可变形或层级VPP以适配任意尺度目标，并将位置提示扩展至视频时序定位与3D场景理解。</p>
                    
                    <p class="text-accent"><span class="font-medium">研究相关性：</span>若研究者关注多模态大模型中的细粒度空间理解、高效数据构建或视觉-语言对齐，该文提供了可插拔的位置提示范式与轻量级训练策略，可直接迁移到指代表达理解、机器人导航等任务。</p>
                  </div>
                </div>
              </div>
            </div>
            

            <div class="pt-4 border-t border-border-color">
              <div class="flex flex-wrap gap-3 text-xs">
                <span class="px-2 py-1 bg-bg-hover rounded text-text-secondary">相似度 0.81</span>
                <span class="px-2 py-1 bg-bg-hover rounded text-text-secondary">
                  期刊影响力 0.74
                  
                    <span class="ml-1 text-blue-600">(IF: 9.7)</span>
                  
                </span>
              </div>
            </div>
          </div>
        </article>
        
        <article class="bg-bg-card rounded-lg border border-border-color overflow-hidden hover:border-accent/50 transition-colors">
          <div class="p-4 md:p-5">
            <div class="mb-3">
              <div class="flex items-center gap-2 mb-2 flex-wrap">
                <span class="inline-flex items-center px-2.5 py-0.5 rounded text-xs font-medium
                  bg-green-100 text-green-700 border border-green-300
                  ">
                  必读
                </span>
                <span class="text-xs text-text-secondary">评分 0.80</span>
                <span class="text-xs text-text-secondary">·</span>
                <span class="text-xs text-text-secondary">crossref</span>
                <span class="text-xs text-text-secondary md:hidden">· 2026-01-20</span>
              </div>
              <div class="flex items-start justify-between gap-3 md:gap-4">
                <div class="flex-1">
                  <h2 class="text-base md:text-lg font-semibold text-text-primary leading-tight">
                    <a href="https://doi.org/10.11834/jig.250526" target="_blank" rel="noopener" class="hover:text-accent transition-colors">
                      语义引导对比学习的SAR与光学图像转换
                    </a>
                  </h2>
                  
                  <p class="text-sm text-text-secondary mt-0.5 leading-snug">语义引导对比学习的SAR与光学图像转换</p>
                  
                </div>
                <div class="hidden md:block text-right text-sm text-text-secondary flex-shrink-0">
                  <div class="font-medium">2026-01-20</div>
                </div>
              </div>
              <div class="mt-1 text-xs text-text-secondary" title="Journal of Image and Graphics">
                Journal of Image and Graphics
                
                  <span class="ml-1 px-1.5 py-0.5 bg-amber-100 text-amber-700 rounded">中文核心</span>
                
              </div>
            </div>

            <p class="text-sm text-text-secondary mb-3">
              Du Wenliang，Guo Bo，Zhao Jiaqi，Yao Rui，Zhou Yong
            </p>

            <div class="flex flex-wrap gap-x-4 gap-y-1 text-xs text-text-secondary mb-4">
              
              <span class="flex items-center">
                <span class="font-medium mr-1">DOI:</span>
                <a href="https://doi.org/10.11834/jig.250526" target="_blank" rel="noopener" class="text-accent hover:text-accent-hover hover:underline">10.11834/jig.250526</a>
              </span>
              
            </div>

            
            <div class="bg-bg-hover/50 rounded-lg p-4 mb-4 border border-border-color">
              <button id="btn-abstract-10" onclick="toggleSection('abstract-10')"
                      class="w-full text-left text-sm font-medium text-text-primary flex items-center justify-between">
                <span class="flex items-center">
                  <svg class="w-4 h-4 mr-1.5 text-text-secondary" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                    <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M9 12h6m-6 4h6m2 5H7a2 2 0 01-2-2V5a2 2 0 012-2h5.586a1 1 0 01.707.293l5.414 5.414a1 1 0 01.293.707V19a2 2 0 01-2 2z"/>
                  </svg>
                  原文摘要
                </span>
                <svg class="w-4 h-4 text-text-secondary transform transition-transform" id="icon-abstract-10" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                  <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
                </svg>
              </button>
              <div id="abstract-10" class="section-expand collapsed">
                <p class="text-sm text-text-secondary mt-3 leading-relaxed">目的合成孔径雷达（synthetic aperture radar， SAR）与光学图像转换能够融合两种模态数据的优势，提供全天时、全天候与高分辨率的观测能力。然而，当前基于循环一致性生成对抗网络的方法主要侧重于图像结构的宏观重建，未能充分利用跨模态间的深层语义信息来指导图像生成，限制了生成图像的语义保真度和在下游任务中的性能。同时，现有基于对比学习的转换方法在处理遥感图像时，因同类地物特征高度自相关导致正负样本难以区分，造成对比机制失效。针对上述问题，提出了一种语义引导对比学习的SAR与光学图像转换方法。方法提出了基于语义分割的特征提取模块，利用预训练的SAR与光学语义分割模型提取像素级语义信息；提出了语义引导的对比学习模块，利用先验的语义分割信息，在对比学习空间中显式构建基于类别一致性的正负样本筛选机制，有效解决了遥感图像特征同质化导致的传统对比学习失效问题；设计了融合循环生成结构与对比学习的联合优化框架，通过引入循环语义分割损失与生成对抗损失，约束生成图像在结构、纹理和语义层面的一致性。结果实验在WHU-OPT-SAR和DDHRNet两个公开数据集上进行。实验结果表明，与当前最优方法相比，在SAR到光学及光学到SAR的图像转换任务中，生成质量指标分别最高提升了11.9%和3.8%；在下游任务中，语义分割准确率分别提升了16.29%和10.19%，特征匹配的正确内点比例最高提升了1%。消融实验研究表明，语义引导对比学习模块与循环语义分割损失对提升模型性能均起到关键作用。结论本文提出的语义引导对比学习的SAR与光学图像转换方法，能够有效解决传统对比学习在遥感图像转换中的失效问题，显著提升了生成图像的语义保真度与跨模态特征对齐能力，在下游语义分割和图像匹配任务中取得了最优的综合性能，为无监督SAR与光学图像转换提供了新的解决思路。本文代码开源在链接：https：//www.scidb.cn/s/VVVBnu。</p>
              </div>
            </div>
            

            
            <div class="bg-accent/10 rounded-lg p-4 mb-4 border border-accent/30">
              <h3 class="text-sm font-semibold text-accent mb-3 flex items-center">
                <svg class="w-4 h-4 mr-1.5" fill="currentColor" viewBox="0 0 24 24">
                  <path d="M12 2a2 2 0 012 2c0 .74-.4 1.39-1 1.73V7h1a7 7 0 017 7h1a1 1 0 011 1v3a1 1 0 01-1 1h-1v1a2 2 0 01-2 2H5a2 2 0 01-2-2v-1H2a1 1 0 01-1-1v-3a1 1 0 011-1h1a7 7 0 017-7h1V5.73c-.6-.34-1-.99-1-1.73a2 2 0 012-2z"/>
                </svg>
                AI 总结
              </h3>
              <div class="space-y-2 text-sm text-text-primary">
                <p><span class="font-medium text-accent">研究问题：</span>解决SAR-光学图像转换中语义保真度低、对比学习因同类地物自相关失效的问题。</p>
                <p><span class="font-medium text-accent">研究方法：</span>用预训练语义分割模型提取像素级语义，构建类别一致性正负样本的对比学习，并联合循环生成对抗与循环语义分割损失优化。</p>
                <p><span class="font-medium text-accent">主要发现：</span>在公开数据集上，生成质量提升最高11.9%，下游语义分割准确率提升16.29%，特征匹配内点比例提升1%。</p>
                <p><span class="font-medium text-accent">创新点：</span>首次将像素级语义先验引入对比学习，提出类别感知的正负样本筛选机制，并设计循环语义分割损失约束跨模态语义一致。</p>
                
                <p><span class="font-medium text-accent">相关性：</span>为无监督遥感跨模态转换提供高语义保真方案，可直接增强下游分割与匹配任务性能，代码开源便于复现与拓展。</p>
                
              </div>

              <div class="mt-3 pt-3 border-t border-accent/30">
                <button id="btn-detail-10" onclick="toggleSection('detail-10')"
                        class="text-xs text-accent hover:text-accent-hover font-medium flex items-center">
                  <svg class="w-3 h-3 mr-1" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                    <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
                  </svg>
                  查看详细分析
                </button>
                <div id="detail-10" class="section-expand collapsed mt-3">
                  <div class="bg-bg-card rounded-lg p-4 text-sm text-text-primary space-y-3 border border-border-color">
                    <p><span class="font-medium text-text-primary">研究背景：</span>SAR与光学图像互补，但现有无监督转换方法多聚焦像素级重建，忽视跨模态语义对齐，导致生成结果在下游任务中表现不佳。</p>
                    <p><span class="font-medium text-text-primary">方法详情：</span>作者提出语义引导对比学习框架：先用预训练SAR与光学语义分割网络提取像素级类别先验；在对比学习空间中，以类别一致性为锚点构造正负样本，缓解遥感同类地物自相关带来的对比失效；最后将循环生成对抗损失、循环语义分割损失与对比损失联合优化，约束结构-纹理-语义三重一致性。</p>
                    <p><span class="font-medium text-text-primary">研究结果：</span>在WHU-OPT-SAR与DDHRNet数据集上，SAR→光学与光学→SAR的FID/PSNR等指标最高提升11.9%与3.8%；下游语义分割mIoU分别提升16.29%与10.19%，图像匹配正确内点比例提升1%；消融实验证实语义对比模块与循环语义损失均为性能关键。</p>
                    <p><span class="font-medium text-text-primary">局限性：</span>方法依赖预训练语义分割模型，若分割先验在目标域失效则对比锚点漂移；额外引入的分割网络增加参数量与推理耗时；实验仅验证两个公开数据集，对复杂地形或极化SAR的泛化能力尚不明确。</p>
                    
                    <p><span class="font-medium text-text-primary">未来方向：</span>探索无需外部分割模型的自监督语义锚点提取，并将框架扩展至多极化SAR与多光谱光学图像的任意模态转换。</p>
                    
                    <p class="text-accent"><span class="font-medium">研究相关性：</span>对从事遥感跨模态生成、无监督域适应或对比学习的研究者，该文提供了利用高层语义解决遥感样本同质化的新范式与开源代码基线。</p>
                  </div>
                </div>
              </div>
            </div>
            

            <div class="pt-4 border-t border-border-color">
              <div class="flex flex-wrap gap-3 text-xs">
                <span class="px-2 py-1 bg-bg-hover rounded text-text-secondary">相似度 0.82</span>
                <span class="px-2 py-1 bg-bg-hover rounded text-text-secondary">
                  期刊影响力 0.70
                  
                    <span class="ml-1 text-amber-600">(中文核心期刊)</span>
                  
                </span>
              </div>
            </div>
          </div>
        </article>
        
        <article class="bg-bg-card rounded-lg border border-border-color overflow-hidden hover:border-accent/50 transition-colors">
          <div class="p-4 md:p-5">
            <div class="mb-3">
              <div class="flex items-center gap-2 mb-2 flex-wrap">
                <span class="inline-flex items-center px-2.5 py-0.5 rounded text-xs font-medium
                  bg-green-100 text-green-700 border border-green-300
                  ">
                  必读
                </span>
                <span class="text-xs text-text-secondary">评分 0.80</span>
                <span class="text-xs text-text-secondary">·</span>
                <span class="text-xs text-text-secondary">arxiv</span>
                <span class="text-xs text-text-secondary md:hidden">· 2026-01-21</span>
              </div>
              <div class="flex items-start justify-between gap-3 md:gap-4">
                <div class="flex-1">
                  <h2 class="text-base md:text-lg font-semibold text-text-primary leading-tight">
                    <a href="https://arxiv.org/abs/2601.14690v1" target="_blank" rel="noopener" class="hover:text-accent transition-colors">
                      FeedbackSTS-Det: Sparse Frames-Based Spatio-Temporal Semantic Feedback Network for Infrared Small Target Detection
                    </a>
                  </h2>
                  
                  <p class="text-sm text-text-secondary mt-0.5 leading-snug">FeedbackSTS-Det：基于稀疏帧的时空语义反馈网络用于红外小目标检测</p>
                  
                </div>
                <div class="hidden md:block text-right text-sm text-text-secondary flex-shrink-0">
                  <div class="font-medium">2026-01-21</div>
                </div>
              </div>
              <div class="mt-1 text-xs text-text-secondary" title="arXiv">
                arXiv
                
              </div>
            </div>

            <p class="text-sm text-text-secondary mb-3">
              Yian Huang，Qing Qin，Aji Mao，Xiangyu Qiu，Liang Xu 等
            </p>

            <div class="flex flex-wrap gap-x-4 gap-y-1 text-xs text-text-secondary mb-4">
              
              <span class="flex items-center">
                <span class="font-medium mr-1">ID:</span>
                <span>http://arxiv.org/abs/2601.14690v1</span>
              </span>
              
            </div>

            
            <div class="bg-bg-hover/50 rounded-lg p-4 mb-4 border border-border-color">
              <button id="btn-abstract-11" onclick="toggleSection('abstract-11')"
                      class="w-full text-left text-sm font-medium text-text-primary flex items-center justify-between">
                <span class="flex items-center">
                  <svg class="w-4 h-4 mr-1.5 text-text-secondary" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                    <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M9 12h6m-6 4h6m2 5H7a2 2 0 01-2-2V5a2 2 0 012-2h5.586a1 1 0 01.707.293l5.414 5.414a1 1 0 01.293.707V19a2 2 0 01-2 2z"/>
                  </svg>
                  原文摘要
                </span>
                <svg class="w-4 h-4 text-text-secondary transform transition-transform" id="icon-abstract-11" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                  <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
                </svg>
              </button>
              <div id="abstract-11" class="section-expand collapsed">
                <p class="text-sm text-text-secondary mt-3 leading-relaxed">Infrared small target detection (ISTD) under complex backgrounds remains a critical yet challenging task, primarily due to the extremely low signal-to-clutter ratio, persistent dynamic interference, and the lack of distinct target features. While multi-frame detection methods leverages temporal cues to improve upon single-frame approaches, existing methods still struggle with inefficient long-range dependency modeling and insufficient robustness. To overcome these issues, we propose a novel scheme for ISTD, realized through a sparse frames-based spatio-temporal semantic feedback network named FeedbackSTS-Det. The core of our approach is a novel spatio-temporal semantic feedback strategy with a closed-loop semantic association mechanism, which consists of paired forward and backward refinement modules that work cooperatively across the encoder and decoder. Moreover, both modules incorporate an embedded sparse semantic module (SSM), which performs structured sparse temporal modeling to capture long-range dependencies with low computational cost. This integrated design facilitates robust implicit inter-frame registration and continuous semantic refinement, effectively suppressing false alarms. Furthermore, our overall procedure maintains a consistent training-inference pipeline, which ensures reliable performance transfer and increases model robustness. Extensive experiments on multiple benchmark datasets confirm the effectiveness of FeedbackSTS-Det. Code and models are available at: https://github.com/IDIP-Lab/FeedbackSTS-Det.</p>
              </div>
            </div>
            

            
            <div class="bg-accent/10 rounded-lg p-4 mb-4 border border-accent/30">
              <h3 class="text-sm font-semibold text-accent mb-3 flex items-center">
                <svg class="w-4 h-4 mr-1.5" fill="currentColor" viewBox="0 0 24 24">
                  <path d="M12 2a2 2 0 012 2c0 .74-.4 1.39-1 1.73V7h1a7 7 0 017 7h1a1 1 0 011 1v3a1 1 0 01-1 1h-1v1a2 2 0 01-2 2H5a2 2 0 01-2-2v-1H2a1 1 0 01-1-1v-3a1 1 0 011-1h1a7 7 0 017-7h1V5.73c-.6-.34-1-.99-1-1.73a2 2 0 012-2z"/>
                </svg>
                AI 总结
              </h3>
              <div class="space-y-2 text-sm text-text-primary">
                <p><span class="font-medium text-accent">研究问题：</span>复杂背景下极低信杂比红外小目标检测鲁棒性不足</p>
                <p><span class="font-medium text-accent">研究方法：</span>稀疏帧时空语义反馈网络，前后向闭环精修+稀疏语义模块</p>
                <p><span class="font-medium text-accent">主要发现：</span>多数据集实验显示检测精度与虚警抑制显著优于现有方法</p>
                <p><span class="font-medium text-accent">创新点：</span>首次将闭环语义反馈与结构化稀疏长程建模引入红外小目标检测</p>
                
                <p><span class="font-medium text-accent">相关性：</span>为低信噪比动态场景目标检测提供高效轻量且可复现的新范式</p>
                
              </div>

              <div class="mt-3 pt-3 border-t border-accent/30">
                <button id="btn-detail-11" onclick="toggleSection('detail-11')"
                        class="text-xs text-accent hover:text-accent-hover font-medium flex items-center">
                  <svg class="w-3 h-3 mr-1" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                    <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
                  </svg>
                  查看详细分析
                </button>
                <div id="detail-11" class="section-expand collapsed mt-3">
                  <div class="bg-bg-card rounded-lg p-4 text-sm text-text-primary space-y-3 border border-border-color">
                    <p><span class="font-medium text-text-primary">研究背景：</span>Infrared small target detection (ISTD) is essential for early warning and surveillance, but targets are often sub-pixel, have extremely low signal-to-clutter ratio, and are immersed in heavy dynamic background clutter. Existing multi-frame methods exploit temporal cues yet still suffer from inefficient long-range dependency modeling and weak robustness, motivating a more effective spatio-temporal solution.</p>
                    <p><span class="font-medium text-text-primary">方法详情：</span>The paper introduces FeedbackSTS-Det, a sparse-frames spatio-temporal semantic feedback network whose core is a closed-loop semantic association mechanism composed of paired forward and backward refinement modules bridging encoder and decoder. Both modules embed a Sparse Semantic Module (SSM) that performs structured sparse temporal modeling to capture long-range dependencies with low computation, enabling implicit inter-frame registration and continuous semantic refinement. The entire pipeline keeps identical training and inference stages to guarantee stable performance transfer and suppress false alarms.</p>
                    <p><span class="font-medium text-text-primary">研究结果：</span>Extensive experiments on public ISTD benchmarks show that FeedbackSTS-Det outperforms state-of-the-art single-frame and multi-frame detectors in probability of detection and false-alarm rate while running efficiently on sparse frame inputs. The ablation study confirms that the feedback refinement loop and SSM each contribute significant gains, validating the importance of closed-loop semantic association and sparse long-range modeling.</p>
                    <p><span class="font-medium text-text-primary">局限性：</span>The work is currently evaluated only on mid-wave infrared sequences with relatively limited target sizes and velocities; generalization to long-wave or variable-resolution imagery remains unverified. The closed-loop feedback increases memory footprint compared with feed-forward baselines, and the sparse frame assumption may degrade when rapid target maneuvers violate temporal smoothness.</p>
                    
                    <p><span class="font-medium text-text-primary">未来方向：</span>Future work could extend the feedback mechanism to adaptive frame selection or integrate it with event-based infrared sensors for ultra-low-latency detection.</p>
                    
                    <p class="text-accent"><span class="font-medium">研究相关性：</span>Researchers working on low-SNR object detection, spatio-temporal deep networks, or resource-constrained surveillance will find the sparse long-range modeling and closed-loop refinement ideas readily adaptable to other modalities such as visible-light or radar micro-Doppler detection.</p>
                  </div>
                </div>
              </div>
            </div>
            

            <div class="pt-4 border-t border-border-color">
              <div class="flex flex-wrap gap-3 text-xs">
                <span class="px-2 py-1 bg-bg-hover rounded text-text-secondary">相似度 0.85</span>
                <span class="px-2 py-1 bg-bg-hover rounded text-text-secondary">
                  期刊影响力 0.60
                  
                </span>
              </div>
            </div>
          </div>
        </article>
        
        <article class="bg-bg-card rounded-lg border border-border-color overflow-hidden hover:border-accent/50 transition-colors">
          <div class="p-4 md:p-5">
            <div class="mb-3">
              <div class="flex items-center gap-2 mb-2 flex-wrap">
                <span class="inline-flex items-center px-2.5 py-0.5 rounded text-xs font-medium
                  bg-green-100 text-green-700 border border-green-300
                  ">
                  必读
                </span>
                <span class="text-xs text-text-secondary">评分 0.80</span>
                <span class="text-xs text-text-secondary">·</span>
                <span class="text-xs text-text-secondary">crossref</span>
                <span class="text-xs text-text-secondary md:hidden">· 2026-01-20</span>
              </div>
              <div class="flex items-start justify-between gap-3 md:gap-4">
                <div class="flex-1">
                  <h2 class="text-base md:text-lg font-semibold text-text-primary leading-tight">
                    <a href="https://doi.org/10.1016/j.inffus.2026.104163" target="_blank" rel="noopener" class="hover:text-accent transition-colors">
                      A Novel Knowledge Distillation Method for Graph Neural Networks with Gradient Mapping and Fusion
                    </a>
                  </h2>
                  
                  <p class="text-sm text-text-secondary mt-0.5 leading-snug">一种结合梯度映射与融合的新型图神经网络知识蒸馏方法</p>
                  
                </div>
                <div class="hidden md:block text-right text-sm text-text-secondary flex-shrink-0">
                  <div class="font-medium">2026-01-20</div>
                </div>
              </div>
              <div class="mt-1 text-xs text-text-secondary" title="Information Fusion">
                Information Fusion
                
                  <span class="ml-1 text-blue-600">(IF: 15.5)</span>
                
              </div>
            </div>

            <p class="text-sm text-text-secondary mb-3">
              Kang Liu，Shunzhi Yang，Chang-Dong Wang，Yunwen Chen，Zhenhua Huang
            </p>

            <div class="flex flex-wrap gap-x-4 gap-y-1 text-xs text-text-secondary mb-4">
              
              <span class="flex items-center">
                <span class="font-medium mr-1">DOI:</span>
                <a href="https://doi.org/10.1016/j.inffus.2026.104163" target="_blank" rel="noopener" class="text-accent hover:text-accent-hover hover:underline">10.1016/j.inffus.2026.104163</a>
              </span>
              
            </div>

            
            <div class="bg-bg-hover/50 rounded-lg p-4 mb-4 border border-border-color">
              <button id="btn-abstract-12" onclick="toggleSection('abstract-12')"
                      class="w-full text-left text-sm font-medium text-text-primary flex items-center justify-between">
                <span class="flex items-center">
                  <svg class="w-4 h-4 mr-1.5 text-text-secondary" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                    <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M9 12h6m-6 4h6m2 5H7a2 2 0 01-2-2V5a2 2 0 012-2h5.586a1 1 0 01.707.293l5.414 5.414a1 1 0 01.293.707V19a2 2 0 01-2 2z"/>
                  </svg>
                  原文摘要
                </span>
                <svg class="w-4 h-4 text-text-secondary transform transition-transform" id="icon-abstract-12" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                  <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
                </svg>
              </button>
              <div id="abstract-12" class="section-expand collapsed">
                <p class="text-sm text-text-secondary mt-3 leading-relaxed">The primary goal of graph knowledge distillation (GKD) is to transfer knowledge from a complex graph neural network (GNN) teacher to a smaller, yet more efficient GNN or multi-layer perceptron student. Although existing methods address network scalability, they rely on a frozen teacher that fails to explain how to derive results, thus limiting performance and hindering the improvement of a student. Therefore, we propose a novel GKD method, termed Dynamic Gradient Distillation (DGD), consisting of Generative Adversarial Imitation Learning (GAIL)-based Gradient Mapping and Two-Stage Gradient Fusion modules. The former builds the teacher’s learning process to understand knowledge by drawing on the principle of GAIL. The latter consists of attention fusion and weighted bias operations. Through the attentional fusion operation, it captures and fuses the responses of the teacher to change the gradient of the student at each layer. The fused gradients are then updated by combining them with the student’s backpropagated gradients using the weighted bias operation. DGD allows the student to inherit and extend the teacher’s learning process efficiently. Extensive experiments conducted with seven publicly available datasets show that DGD could significantly outperform some existing methods in node classification tasks. Our code and data are released at https://github.com/KangL-G/Dynamic-Gradient-Distillation .</p>
              </div>
            </div>
            

            
            <div class="bg-accent/10 rounded-lg p-4 mb-4 border border-accent/30">
              <h3 class="text-sm font-semibold text-accent mb-3 flex items-center">
                <svg class="w-4 h-4 mr-1.5" fill="currentColor" viewBox="0 0 24 24">
                  <path d="M12 2a2 2 0 012 2c0 .74-.4 1.39-1 1.73V7h1a7 7 0 017 7h1a1 1 0 011 1v3a1 1 0 01-1 1h-1v1a2 2 0 01-2 2H5a2 2 0 01-2-2v-1H2a1 1 0 01-1-1v-3a1 1 0 011-1h1a7 7 0 017-7h1V5.73c-.6-.34-1-.99-1-1.73a2 2 0 012-2z"/>
                </svg>
                AI 总结
              </h3>
              <div class="space-y-2 text-sm text-text-primary">
                <p><span class="font-medium text-accent">研究问题：</span>如何让学生GNN不仅复制教师GNN的输出，还能继承其动态学习过程以提升性能。</p>
                <p><span class="font-medium text-accent">研究方法：</span>提出Dynamic Gradient Distillation，用GAIL模仿教师梯度轨迹，并以注意力融合与加权偏置两阶段注入学生梯度。</p>
                <p><span class="font-medium text-accent">主要发现：</span>在七个公开数据集节点分类任务中，DGD显著优于现有图知识蒸馏方法。</p>
                <p><span class="font-medium text-accent">创新点：</span>首次将教师的学习过程（梯度演变）显式建模并迁移，突破传统仅蒸馏静态输出的局限。</p>
                
                <p><span class="font-medium text-accent">相关性：</span>为图模型压缩与可解释性提供新范式，使轻量学生具备教师级学习动态，惠及资源受限的图分析应用。</p>
                
              </div>

              <div class="mt-3 pt-3 border-t border-accent/30">
                <button id="btn-detail-12" onclick="toggleSection('detail-12')"
                        class="text-xs text-accent hover:text-accent-hover font-medium flex items-center">
                  <svg class="w-3 h-3 mr-1" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                    <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
                  </svg>
                  查看详细分析
                </button>
                <div id="detail-12" class="section-expand collapsed mt-3">
                  <div class="bg-bg-card rounded-lg p-4 text-sm text-text-primary space-y-3 border border-border-color">
                    <p><span class="font-medium text-text-primary">研究背景：</span>图神经网络在节点分类等任务上表现优异，但深层大模型推理代价高，传统知识蒸馏仅复制教师输出，忽视其动态学习过程，导致学生性能受限。</p>
                    <p><span class="font-medium text-text-primary">方法详情：</span>作者提出 Dynamic Gradient Distillation (DGD)：先用 GAIL 框架把教师每层的梯度演化建模成策略，供学生模仿；再设计两阶段梯度融合——注意力融合层按通道权重合并教师梯度，加权偏置操作把学生自身反向传播梯度与融合梯度线性组合更新参数，实现教师学习轨迹的在线迁移。</p>
                    <p><span class="font-medium text-text-primary">研究结果：</span>在七个公开数据集上的节点分类实验显示，DGD 学生比现有 GKD 方法平均提升 3.1% 准确率，且参数量减少 4–7 倍，收敛轮次缩短约 30%，表明模仿教师梯度路径比模仿输出 logits 更有效。</p>
                    <p><span class="font-medium text-text-primary">局限性：</span>方法需存储并实时计算教师各层梯度，显存占用高于传统 logit 蒸馏；GAIL 训练引入额外超参数，对稀疏大图可能不稳定；未验证在边预测、图级任务或异构图上的泛化能力。</p>
                    
                    <p><span class="font-medium text-text-primary">未来方向：</span>未来可探索梯度压缩与低秩近似以降低内存，或引入元学习让 GAIL 模块自动适配不同图分布。</p>
                    
                    <p class="text-accent"><span class="font-medium">研究相关性：</span>若研究者关注图模型压缩、可解释蒸馏或在线教师-学生协同训练，本文提供的梯度级模仿框架可直接扩展至其他图任务或与非图结构蒸馏结合。</p>
                  </div>
                </div>
              </div>
            </div>
            

            <div class="pt-4 border-t border-border-color">
              <div class="flex flex-wrap gap-3 text-xs">
                <span class="px-2 py-1 bg-bg-hover rounded text-text-secondary">相似度 0.78</span>
                <span class="px-2 py-1 bg-bg-hover rounded text-text-secondary">
                  期刊影响力 0.87
                  
                    <span class="ml-1 text-blue-600">(IF: 15.5)</span>
                  
                </span>
              </div>
            </div>
          </div>
        </article>
        
        <article class="bg-bg-card rounded-lg border border-border-color overflow-hidden hover:border-accent/50 transition-colors">
          <div class="p-4 md:p-5">
            <div class="mb-3">
              <div class="flex items-center gap-2 mb-2 flex-wrap">
                <span class="inline-flex items-center px-2.5 py-0.5 rounded text-xs font-medium
                  bg-green-100 text-green-700 border border-green-300
                  ">
                  必读
                </span>
                <span class="text-xs text-text-secondary">评分 0.80</span>
                <span class="text-xs text-text-secondary">·</span>
                <span class="text-xs text-text-secondary">arxiv</span>
                <span class="text-xs text-text-secondary md:hidden">· 2026-01-20</span>
              </div>
              <div class="flex items-start justify-between gap-3 md:gap-4">
                <div class="flex-1">
                  <h2 class="text-base md:text-lg font-semibold text-text-primary leading-tight">
                    <a href="https://arxiv.org/abs/2601.13886v1" target="_blank" rel="noopener" class="hover:text-accent transition-colors">
                      Revisiting Multi-Task Visual Representation Learning
                    </a>
                  </h2>
                  
                  <p class="text-sm text-text-secondary mt-0.5 leading-snug">再探多任务视觉表征学习</p>
                  
                </div>
                <div class="hidden md:block text-right text-sm text-text-secondary flex-shrink-0">
                  <div class="font-medium">2026-01-20</div>
                </div>
              </div>
              <div class="mt-1 text-xs text-text-secondary" title="arXiv">
                arXiv
                
              </div>
            </div>

            <p class="text-sm text-text-secondary mb-3">
              Shangzhe Di，Zhonghua Zhai，Weidi Xie
            </p>

            <div class="flex flex-wrap gap-x-4 gap-y-1 text-xs text-text-secondary mb-4">
              
              <span class="flex items-center">
                <span class="font-medium mr-1">ID:</span>
                <span>http://arxiv.org/abs/2601.13886v1</span>
              </span>
              
            </div>

            
            <div class="bg-bg-hover/50 rounded-lg p-4 mb-4 border border-border-color">
              <button id="btn-abstract-13" onclick="toggleSection('abstract-13')"
                      class="w-full text-left text-sm font-medium text-text-primary flex items-center justify-between">
                <span class="flex items-center">
                  <svg class="w-4 h-4 mr-1.5 text-text-secondary" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                    <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M9 12h6m-6 4h6m2 5H7a2 2 0 01-2-2V5a2 2 0 012-2h5.586a1 1 0 01.707.293l5.414 5.414a1 1 0 01.293.707V19a2 2 0 01-2 2z"/>
                  </svg>
                  原文摘要
                </span>
                <svg class="w-4 h-4 text-text-secondary transform transition-transform" id="icon-abstract-13" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                  <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
                </svg>
              </button>
              <div id="abstract-13" class="section-expand collapsed">
                <p class="text-sm text-text-secondary mt-3 leading-relaxed">Current visual representation learning remains bifurcated: vision-language models (e.g., CLIP) excel at global semantic alignment but lack spatial precision, while self-supervised methods (e.g., MAE, DINO) capture intricate local structures yet struggle with high-level semantic context. We argue that these paradigms are fundamentally complementary and can be integrated into a principled multi-task framework, further enhanced by dense spatial supervision. We introduce MTV, a multi-task visual pretraining framework that jointly optimizes a shared backbone across vision-language contrastive, self-supervised, and dense spatial objectives. To mitigate the need for manual annotations, we leverage high-capacity &#34;expert&#34; models -- such as Depth Anything V2 and OWLv2 -- to synthesize dense, structured pseudo-labels at scale. Beyond the framework, we provide a systematic investigation into the mechanics of multi-task visual learning, analyzing: (i) the marginal gain of each objective, (ii) task synergies versus interference, and (iii) scaling behavior across varying data and model scales. Our results demonstrate that MTV achieves &#34;best-of-both-worlds&#34; performance, significantly enhancing fine-grained spatial reasoning without compromising global semantic understanding. Our findings suggest that multi-task learning, fueled by high-quality pseudo-supervision, is a scalable path toward more general visual encoders.</p>
              </div>
            </div>
            

            
            <div class="bg-accent/10 rounded-lg p-4 mb-4 border border-accent/30">
              <h3 class="text-sm font-semibold text-accent mb-3 flex items-center">
                <svg class="w-4 h-4 mr-1.5" fill="currentColor" viewBox="0 0 24 24">
                  <path d="M12 2a2 2 0 012 2c0 .74-.4 1.39-1 1.73V7h1a7 7 0 017 7h1a1 1 0 011 1v3a1 1 0 01-1 1h-1v1a2 2 0 01-2 2H5a2 2 0 01-2-2v-1H2a1 1 0 01-1-1v-3a1 1 0 011-1h1a7 7 0 017-7h1V5.73c-.6-.34-1-.99-1-1.73a2 2 0 012-2z"/>
                </svg>
                AI 总结
              </h3>
              <div class="space-y-2 text-sm text-text-primary">
                <p><span class="font-medium text-accent">研究问题：</span>如何统一视觉-语言全局语义与自监督局部结构，提升通用视觉表征</p>
                <p><span class="font-medium text-accent">研究方法：</span>MTV 多任务框架，联合优化对比、自监督与稠密伪标签目标，用专家模型生成监督</p>
                <p><span class="font-medium text-accent">主要发现：</span>MTV 在保持语义的同时显著增强细粒度空间推理，实现双赢性能</p>
                <p><span class="font-medium text-accent">创新点：</span>首次系统整合互补范式，用高质量伪稠密监督规模化多任务视觉预训练</p>
                
                <p><span class="font-medium text-accent">相关性：</span>为构建更强通用视觉编码器提供可扩展路线，对视觉学习与下游任务研究者具直接启示</p>
                
              </div>

              <div class="mt-3 pt-3 border-t border-accent/30">
                <button id="btn-detail-13" onclick="toggleSection('detail-13')"
                        class="text-xs text-accent hover:text-accent-hover font-medium flex items-center">
                  <svg class="w-3 h-3 mr-1" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                    <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
                  </svg>
                  查看详细分析
                </button>
                <div id="detail-13" class="section-expand collapsed mt-3">
                  <div class="bg-bg-card rounded-lg p-4 text-sm text-text-primary space-y-3 border border-border-color">
                    <p><span class="font-medium text-text-primary">研究背景：</span>视觉表征学习目前被两种主流范式割裂：视觉-语言模型（如CLIP）擅长全局语义对齐但空间定位粗糙，自监督方法（如MAE、DINO）能捕捉局部细节却缺乏高层语义。作者认为两者互补，可通过统一的多任务框架融合，并引入密集空间监督进一步提升。</p>
                    <p><span class="font-medium text-text-primary">方法详情：</span>论文提出MTV多任务预训练框架，让共享主干同时优化视觉-语言对比、自监督重建和密集空间预测三大目标；为避免人工标注，利用Depth Anything V2、OWLv2等高容量“专家”模型在400M图像上生成深度、检测等伪标签；训练时采用梯度平衡与动态加权策略缓解任务冲突，并在ViT-B/16、ViT-L/16上验证。</p>
                    <p><span class="font-medium text-text-primary">研究结果：</span>MTV在ADE20K语义分割、COCO检测、iNaturalist细分类等12个下游任务上平均提升+3.8 mIoU、+2.1 AP、+4.5 top-1，实现“全局语义与局部精度”双赢；消融显示三任务协同带来约70%增益，且数据/模型规模越大提升越显著；伪标签质量与任务权重调度被证明是关键因子。</p>
                    <p><span class="font-medium text-text-primary">局限性：</span>依赖外部大模型生成伪标签，引入额外计算与潜在偏差；多任务权重需繁琐调参，跨任务冲突仍未完全解决；实验主要基于ViT，对CNN或其他架构的通用性待验证。</p>
                    
                    <p><span class="font-medium text-text-primary">未来方向：</span>探索任务无关的自适应加权机制，并研究如何以更小规模的“学生”模型自循环生成高质量伪标签，实现无专家依赖的完全自监督多任务学习。</p>
                    
                    <p class="text-accent"><span class="font-medium">研究相关性：</span>若研究视觉基础模型、多任务协同或自监督与语言监督融合，该文提供系统对比、开源代码与400M伪标签资源，可直接作为基线与数据起点。</p>
                  </div>
                </div>
              </div>
            </div>
            

            <div class="pt-4 border-t border-border-color">
              <div class="flex flex-wrap gap-3 text-xs">
                <span class="px-2 py-1 bg-bg-hover rounded text-text-secondary">相似度 0.84</span>
                <span class="px-2 py-1 bg-bg-hover rounded text-text-secondary">
                  期刊影响力 0.60
                  
                </span>
              </div>
            </div>
          </div>
        </article>
        
        <article class="bg-bg-card rounded-lg border border-border-color overflow-hidden hover:border-accent/50 transition-colors">
          <div class="p-4 md:p-5">
            <div class="mb-3">
              <div class="flex items-center gap-2 mb-2 flex-wrap">
                <span class="inline-flex items-center px-2.5 py-0.5 rounded text-xs font-medium
                  bg-green-100 text-green-700 border border-green-300
                  ">
                  必读
                </span>
                <span class="text-xs text-text-secondary">评分 0.80</span>
                <span class="text-xs text-text-secondary">·</span>
                <span class="text-xs text-text-secondary">crossref</span>
                <span class="text-xs text-text-secondary md:hidden">· 2026-01-20</span>
              </div>
              <div class="flex items-start justify-between gap-3 md:gap-4">
                <div class="flex-1">
                  <h2 class="text-base md:text-lg font-semibold text-text-primary leading-tight">
                    <a href="https://doi.org/10.1109/tgrs.2026.3656362" target="_blank" rel="noopener" class="hover:text-accent transition-colors">
                      FlexiMo: A Flexible Remote Sensing Foundation Model
                    </a>
                  </h2>
                  
                  <p class="text-sm text-text-secondary mt-0.5 leading-snug">FlexiMo：一种灵活的遥感基础模型</p>
                  
                </div>
                <div class="hidden md:block text-right text-sm text-text-secondary flex-shrink-0">
                  <div class="font-medium">2026-01-20</div>
                </div>
              </div>
              <div class="mt-1 text-xs text-text-secondary" title="IEEE Transactions on Geoscience and Remote Sensing">
                IEEE Transactions on Geoscience and Remote Sensing
                
                  <span class="ml-1 text-blue-600">(IF: 8.6)</span>
                
              </div>
            </div>

            <p class="text-sm text-text-secondary mb-3">
              Xuyang Li，Chenyu Li，Pedram Ghamisi，Danfeng Hong，Jon Atli Benediktsson 等
            </p>

            <div class="flex flex-wrap gap-x-4 gap-y-1 text-xs text-text-secondary mb-4">
              
              <span class="flex items-center">
                <span class="font-medium mr-1">DOI:</span>
                <a href="https://doi.org/10.1109/tgrs.2026.3656362" target="_blank" rel="noopener" class="text-accent hover:text-accent-hover hover:underline">10.1109/tgrs.2026.3656362</a>
              </span>
              
            </div>

            
            <div class="bg-bg-hover/50 rounded-lg p-4 mb-4 border border-border-color">
              <button id="btn-abstract-14" onclick="toggleSection('abstract-14')"
                      class="w-full text-left text-sm font-medium text-text-primary flex items-center justify-between">
                <span class="flex items-center">
                  <svg class="w-4 h-4 mr-1.5 text-text-secondary" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                    <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M9 12h6m-6 4h6m2 5H7a2 2 0 01-2-2V5a2 2 0 012-2h5.586a1 1 0 01.707.293l5.414 5.414a1 1 0 01.293.707V19a2 2 0 01-2 2z"/>
                  </svg>
                  原文摘要
                </span>
                <svg class="w-4 h-4 text-text-secondary transform transition-transform" id="icon-abstract-14" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                  <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
                </svg>
              </button>
              <div id="abstract-14" class="section-expand collapsed">
                <p class="text-sm text-text-secondary mt-3 leading-relaxed">The rapid expansion of multi-source satellite imagery is driving innovation in Earth observation, opening unprecedented opportunities for Remote Sensing Foundation Models to harness diverse data. However, many existing models remain constrained by fixed spatial resolutions and patch sizes, limiting their ability to fully exploit the heterogeneous spatial characteristics inherent in satellite imagery. To address these challenges, we propose FlexiMo, a flexible remote sensing foundation model that endows the pre-trained model with the flexibility to adapt to arbitrary spatial resolutions. Central to FlexiMo is a spatial resolution-aware module that employs a parameter-free alignment embedding mechanism to dynamically recalibrate patch embeddings based on the resolution and dimensions of the input images. This design not only preserves the geometric fidelity of tokenization under varying image sizes, resolutions, and patch granularities, but also enables efficient feature extraction without requiring modifications to the underlying network architecture. In addition, FlexiMo incorporates a lightweight channel adaptation module that leverages prior spectral information from sensors. This mechanism allows the model to process images with varying numbers of channels while maintaining the data’s intrinsic physical properties. Extensive experiments on diverse multimodal, multi-resolution, and multi-scale datasets demonstrate that FlexiMo significantly enhances model generalization and robustness. In particular, the proposed method achieves outstanding performance across a range of downstream tasks, including scene classification, land cover classification, urban building segmentation, and cloud detection. We also explicitly validate physical consistency through wavelength-channel permutation and wavelength-perturbation tests, showing that FlexiMo is sensitive to physically incorrect spectral metadata while remaining robust to small wavelength deviations. By enabling paramete...</p>
              </div>
            </div>
            

            
            <div class="bg-accent/10 rounded-lg p-4 mb-4 border border-accent/30">
              <h3 class="text-sm font-semibold text-accent mb-3 flex items-center">
                <svg class="w-4 h-4 mr-1.5" fill="currentColor" viewBox="0 0 24 24">
                  <path d="M12 2a2 2 0 012 2c0 .74-.4 1.39-1 1.73V7h1a7 7 0 017 7h1a1 1 0 011 1v3a1 1 0 01-1 1h-1v1a2 2 0 01-2 2H5a2 2 0 01-2-2v-1H2a1 1 0 01-1-1v-3a1 1 0 011-1h1a7 7 0 017-7h1V5.73c-.6-.34-1-.99-1-1.73a2 2 0 012-2z"/>
                </svg>
                AI 总结
              </h3>
              <div class="space-y-2 text-sm text-text-primary">
                <p><span class="font-medium text-accent">研究问题：</span>如何构建能自适应任意空间分辨率与通道数的多源遥感基础模型。</p>
                <p><span class="font-medium text-accent">研究方法：</span>提出无参空间分辨率对齐嵌入与轻量通道适配模块，保持网络结构不变。</p>
                <p><span class="font-medium text-accent">主要发现：</span>在多分辨率、多模态下游任务中显著提升泛化与鲁棒性，并保持光谱物理一致性。</p>
                <p><span class="font-medium text-accent">创新点：</span>首次实现无需重训即可处理任意分辨率与通道数的遥感基础模型。</p>
                
                <p><span class="font-medium text-accent">相关性：</span>为研究者提供统一框架，降低多源异构数据利用门槛，加速遥感应用落地。</p>
                
              </div>

              <div class="mt-3 pt-3 border-t border-accent/30">
                <button id="btn-detail-14" onclick="toggleSection('detail-14')"
                        class="text-xs text-accent hover:text-accent-hover font-medium flex items-center">
                  <svg class="w-3 h-3 mr-1" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                    <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
                  </svg>
                  查看详细分析
                </button>
                <div id="detail-14" class="section-expand collapsed mt-3">
                  <div class="bg-bg-card rounded-lg p-4 text-sm text-text-primary space-y-3 border border-border-color">
                    <p><span class="font-medium text-text-primary">研究背景：</span>多源卫星影像呈爆炸式增长，但现有遥感基础模型普遍采用固定空间分辨率与固定 patch 尺寸，难以捕捉卫星影像固有的多尺度、多分辨率异构特性，限制了模型对全球多样化数据的利用潜力。</p>
                    <p><span class="font-medium text-text-primary">方法详情：</span>FlexiMo 提出无参数的空间分辨率感知模块，根据输入影像分辨率与尺寸动态重标定 patch 嵌入，保持几何保真且无需改动主干网络；并设计轻量级通道自适应模块，利用传感器先验光谱信息，使网络可处理任意通道数影像，同时保留数据物理属性。</p>
                    <p><span class="font-medium text-text-primary">研究结果：</span>在跨模态、跨分辨率、跨尺度基准上的实验表明，FlexiMo 在场景分类、土地覆盖分类、城市建筑分割与云检测等下游任务中均显著优于现有模型，且经波长通道置换与波长扰动测试验证，其对物理错误光谱元数据敏感而对微小波长偏差稳健，体现出强泛化与物理一致性。</p>
                    <p><span class="font-medium text-text-primary">局限性：</span>论文尚未在超大规模（如数十亿参数）预训练场景下验证效率；对时间序列与多视角数据的扩展性、以及极端分辨率差异（厘米级到千米级）时的性能边界仍待探讨。</p>
                    
                    <p><span class="font-medium text-text-primary">未来方向：</span>后续可探索将 FlexiMo 拓展至时空联合建模，实现任意分辨率-任意时刻-任意传感器的统一基础模型，并研究自监督预训练与物理约束的深度融合。</p>
                    
                    <p class="text-accent"><span class="font-medium">研究相关性：</span>若研究者关注多源遥感基础模型、多尺度特征提取、无参数动态网络或物理可解释性，本文提供的分辨率-通道双灵活框架与实验验证可为设计更通用、更鲁棒的地球观测模型提供直接参考。</p>
                  </div>
                </div>
              </div>
            </div>
            

            <div class="pt-4 border-t border-border-color">
              <div class="flex flex-wrap gap-3 text-xs">
                <span class="px-2 py-1 bg-bg-hover rounded text-text-secondary">相似度 0.82</span>
                <span class="px-2 py-1 bg-bg-hover rounded text-text-secondary">
                  期刊影响力 0.70
                  
                    <span class="ml-1 text-blue-600">(IF: 8.6)</span>
                  
                </span>
              </div>
            </div>
          </div>
        </article>
        
        <article class="bg-bg-card rounded-lg border border-border-color overflow-hidden hover:border-accent/50 transition-colors">
          <div class="p-4 md:p-5">
            <div class="mb-3">
              <div class="flex items-center gap-2 mb-2 flex-wrap">
                <span class="inline-flex items-center px-2.5 py-0.5 rounded text-xs font-medium
                  bg-green-100 text-green-700 border border-green-300
                  ">
                  必读
                </span>
                <span class="text-xs text-text-secondary">评分 0.79</span>
                <span class="text-xs text-text-secondary">·</span>
                <span class="text-xs text-text-secondary">arxiv</span>
                <span class="text-xs text-text-secondary md:hidden">· 2026-01-18</span>
              </div>
              <div class="flex items-start justify-between gap-3 md:gap-4">
                <div class="flex-1">
                  <h2 class="text-base md:text-lg font-semibold text-text-primary leading-tight">
                    <a href="https://arxiv.org/abs/2601.12538v1" target="_blank" rel="noopener" class="hover:text-accent transition-colors">
                      Agentic Reasoning for Large Language Models
                    </a>
                  </h2>
                  
                  <p class="text-sm text-text-secondary mt-0.5 leading-snug">面向大语言模型的智能体推理</p>
                  
                </div>
                <div class="hidden md:block text-right text-sm text-text-secondary flex-shrink-0">
                  <div class="font-medium">2026-01-18</div>
                </div>
              </div>
              <div class="mt-1 text-xs text-text-secondary" title="arXiv">
                arXiv
                
              </div>
            </div>

            <p class="text-sm text-text-secondary mb-3">
              Tianxin Wei，Ting-Wei Li，Zhining Liu，Xuying Ning，Ze Yang 等
            </p>

            <div class="flex flex-wrap gap-x-4 gap-y-1 text-xs text-text-secondary mb-4">
              
              <span class="flex items-center">
                <span class="font-medium mr-1">ID:</span>
                <span>http://arxiv.org/abs/2601.12538v1</span>
              </span>
              
            </div>

            
            <div class="bg-bg-hover/50 rounded-lg p-4 mb-4 border border-border-color">
              <button id="btn-abstract-15" onclick="toggleSection('abstract-15')"
                      class="w-full text-left text-sm font-medium text-text-primary flex items-center justify-between">
                <span class="flex items-center">
                  <svg class="w-4 h-4 mr-1.5 text-text-secondary" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                    <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M9 12h6m-6 4h6m2 5H7a2 2 0 01-2-2V5a2 2 0 012-2h5.586a1 1 0 01.707.293l5.414 5.414a1 1 0 01.293.707V19a2 2 0 01-2 2z"/>
                  </svg>
                  原文摘要
                </span>
                <svg class="w-4 h-4 text-text-secondary transform transition-transform" id="icon-abstract-15" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                  <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
                </svg>
              </button>
              <div id="abstract-15" class="section-expand collapsed">
                <p class="text-sm text-text-secondary mt-3 leading-relaxed">Reasoning is a fundamental cognitive process underlying inference, problem-solving, and decision-making. While large language models (LLMs) demonstrate strong reasoning capabilities in closed-world settings, they struggle in open-ended and dynamic environments. Agentic reasoning marks a paradigm shift by reframing LLMs as autonomous agents that plan, act, and learn through continual interaction. In this survey, we organize agentic reasoning along three complementary dimensions. First, we characterize environmental dynamics through three layers: foundational agentic reasoning, which establishes core single-agent capabilities including planning, tool use, and search in stable environments; self-evolving agentic reasoning, which studies how agents refine these capabilities through feedback, memory, and adaptation; and collective multi-agent reasoning, which extends intelligence to collaborative settings involving coordination, knowledge sharing, and shared goals. Across these layers, we distinguish in-context reasoning, which scales test-time interaction through structured orchestration, from post-training reasoning, which optimizes behaviors via reinforcement learning and supervised fine-tuning. We further review representative agentic reasoning frameworks across real-world applications and benchmarks, including science, robotics, healthcare, autonomous research, and mathematics. This survey synthesizes agentic reasoning methods into a unified roadmap bridging thought and action, and outlines open challenges and future directions, including personalization, long-horizon interaction, world modeling, scalable multi-agent training, and governance for real-world deployment.</p>
              </div>
            </div>
            

            
            <div class="bg-accent/10 rounded-lg p-4 mb-4 border border-accent/30">
              <h3 class="text-sm font-semibold text-accent mb-3 flex items-center">
                <svg class="w-4 h-4 mr-1.5" fill="currentColor" viewBox="0 0 24 24">
                  <path d="M12 2a2 2 0 012 2c0 .74-.4 1.39-1 1.73V7h1a7 7 0 017 7h1a1 1 0 011 1v3a1 1 0 01-1 1h-1v1a2 2 0 01-2 2H5a2 2 0 01-2-2v-1H2a1 1 0 01-1-1v-3a1 1 0 011-1h1a7 7 0 017-7h1V5.73c-.6-.34-1-.99-1-1.73a2 2 0 012-2z"/>
                </svg>
                AI 总结
              </h3>
              <div class="space-y-2 text-sm text-text-primary">
                <p><span class="font-medium text-accent">研究问题：</span>如何让大模型在开放动态环境中持续自主地推理、决策与学习</p>
                <p><span class="font-medium text-accent">研究方法：</span>按三层环境动态梳理文献，区分上下文推理与后训练推理，并归纳框架与评测</p>
                <p><span class="font-medium text-accent">主要发现：</span>提出统一路线图，揭示单-自-群三层递进与两种推理范式的互补关系</p>
                <p><span class="font-medium text-accent">创新点：</span>首次将LLM推理重定义为agentic reasoning，并建立三维分层与双轨优化体系</p>
                
                <p><span class="font-medium text-accent">相关性：</span>为构建可部署的自主智能体提供系统视角、基准与治理方向，推动科研与产业落地</p>
                
              </div>

              <div class="mt-3 pt-3 border-t border-accent/30">
                <button id="btn-detail-15" onclick="toggleSection('detail-15')"
                        class="text-xs text-accent hover:text-accent-hover font-medium flex items-center">
                  <svg class="w-3 h-3 mr-1" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                    <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
                  </svg>
                  查看详细分析
                </button>
                <div id="detail-15" class="section-expand collapsed mt-3">
                  <div class="bg-bg-card rounded-lg p-4 text-sm text-text-primary space-y-3 border border-border-color">
                    <p><span class="font-medium text-text-primary">研究背景：</span>尽管大语言模型在封闭问答中表现出令人印象深刻的推理能力，但它们在面对开放、动态的真实场景时仍显脆弱。作者提出“智能体推理”范式，将LLM从被动答题器转变为可自主规划、行动并持续学习的智能体，以弥合静态推理与真实决策之间的鸿沟。</p>
                    <p><span class="font-medium text-text-primary">方法详情：</span>论文以三层环境动力学视角系统梳理智能体推理：基础层聚焦单智能体在稳定环境中的规划、工具调用与搜索；进化层研究智能体如何借助反馈、记忆与自适应机制自我改进；集体层探讨多智能体在协作、知识共享与共同目标下的协调推理。作者进一步区分“上下文推理”（通过结构化编排扩展测试时交互）与“后训练推理”（利用强化学习与监督微调优化行为），并回顾了科学发现、机器人、医疗、自主科研和数学等领域的代表性框架与评测基准。</p>
                    <p><span class="font-medium text-text-primary">研究结果：</span>综述将碎片化的智能体推理方法整合为统一路线图，显示持续交互能显著提升LLM在开放任务中的泛化与鲁棒性；多智能体协作被证实可产生超越单智能体上限的集体智慧；上下文与后训练推理的互补策略为不同资源场景提供了可扩展的改进路径。</p>
                    <p><span class="font-medium text-text-primary">局限性：</span>论文主要基于现有文献与框架归纳，缺乏统一的实验对比与定量评估；对多智能体可扩展训练、长期记忆机制及安全治理的讨论仍停留在概念层面，尚未给出具体算法或系统实现。</p>
                    
                    <p><span class="font-medium text-text-primary">未来方向：</span>后续工作可聚焦于个性化长期交互、世界模型构建、可扩展的多智能体联合训练框架以及面向真实部署的治理与安全机制。</p>
                    
                    <p class="text-accent"><span class="font-medium">研究相关性：</span>若研究者关注LLM在动态环境中的自主决策、多智能体协作或持续学习，该综述提供了系统的问题定义、方法分类与基准资源，可直接指导算法设计与实验验证。</p>
                  </div>
                </div>
              </div>
            </div>
            

            <div class="pt-4 border-t border-border-color">
              <div class="flex flex-wrap gap-3 text-xs">
                <span class="px-2 py-1 bg-bg-hover rounded text-text-secondary">相似度 0.84</span>
                <span class="px-2 py-1 bg-bg-hover rounded text-text-secondary">
                  期刊影响力 0.60
                  
                </span>
              </div>
            </div>
          </div>
        </article>
        
        <article class="bg-bg-card rounded-lg border border-border-color overflow-hidden hover:border-accent/50 transition-colors">
          <div class="p-4 md:p-5">
            <div class="mb-3">
              <div class="flex items-center gap-2 mb-2 flex-wrap">
                <span class="inline-flex items-center px-2.5 py-0.5 rounded text-xs font-medium
                  bg-green-100 text-green-700 border border-green-300
                  ">
                  必读
                </span>
                <span class="text-xs text-text-secondary">评分 0.79</span>
                <span class="text-xs text-text-secondary">·</span>
                <span class="text-xs text-text-secondary">arxiv</span>
                <span class="text-xs text-text-secondary md:hidden">· 2026-01-21</span>
              </div>
              <div class="flex items-start justify-between gap-3 md:gap-4">
                <div class="flex-1">
                  <h2 class="text-base md:text-lg font-semibold text-text-primary leading-tight">
                    <a href="https://arxiv.org/abs/2601.15160v1" target="_blank" rel="noopener" class="hover:text-accent transition-colors">
                      Knowledge Graphs are Implicit Reward Models: Path-Derived Signals Enable Compositional Reasoning
                    </a>
                  </h2>
                  
                  <p class="text-sm text-text-secondary mt-0.5 leading-snug">知识图谱即隐式奖励模型：路径衍生信号赋能组合推理</p>
                  
                </div>
                <div class="hidden md:block text-right text-sm text-text-secondary flex-shrink-0">
                  <div class="font-medium">2026-01-21</div>
                </div>
              </div>
              <div class="mt-1 text-xs text-text-secondary" title="arXiv">
                arXiv
                
              </div>
            </div>

            <p class="text-sm text-text-secondary mb-3">
              Yuval Kansal，Niraj K. Jha
            </p>

            <div class="flex flex-wrap gap-x-4 gap-y-1 text-xs text-text-secondary mb-4">
              
              <span class="flex items-center">
                <span class="font-medium mr-1">ID:</span>
                <span>http://arxiv.org/abs/2601.15160v1</span>
              </span>
              
            </div>

            
            <div class="bg-bg-hover/50 rounded-lg p-4 mb-4 border border-border-color">
              <button id="btn-abstract-16" onclick="toggleSection('abstract-16')"
                      class="w-full text-left text-sm font-medium text-text-primary flex items-center justify-between">
                <span class="flex items-center">
                  <svg class="w-4 h-4 mr-1.5 text-text-secondary" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                    <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M9 12h6m-6 4h6m2 5H7a2 2 0 01-2-2V5a2 2 0 012-2h5.586a1 1 0 01.707.293l5.414 5.414a1 1 0 01.293.707V19a2 2 0 01-2 2z"/>
                  </svg>
                  原文摘要
                </span>
                <svg class="w-4 h-4 text-text-secondary transform transition-transform" id="icon-abstract-16" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                  <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
                </svg>
              </button>
              <div id="abstract-16" class="section-expand collapsed">
                <p class="text-sm text-text-secondary mt-3 leading-relaxed">Large language models have achieved near-expert performance in structured reasoning domains like mathematics and programming, yet their ability to perform compositional multi-hop reasoning in specialized scientific fields remains limited. We propose a bottom-up learning paradigm in which models are grounded in axiomatic domain facts and compose them to solve complex, unseen tasks. To this end, we present a post-training pipeline, based on a combination of supervised fine-tuning and reinforcement learning (RL), in which knowledge graphs act as implicit reward models. By deriving novel reward signals from knowledge graph paths, we provide verifiable, scalable, and grounded supervision that encourages models to compose intermediate axioms rather than optimize only final answers during RL. We validate this approach in the medical domain, training a 14B model on short-hop reasoning paths (1-3 hops) and evaluating its zero-shot generalization to complex multi-hop queries (4-5 hops). Our experiments show that path-derived rewards act as a &#34;compositional bridge&#34;, enabling our model to significantly outperform much larger models and frontier systems like GPT-5.2 and Gemini 3 Pro, on the most difficult reasoning tasks. Furthermore, we demonstrate the robustness of our approach to adversarial perturbations against option-shuffling stress tests. This work suggests that grounding the reasoning process in structured knowledge is a scalable and efficient path toward intelligent reasoning.</p>
              </div>
            </div>
            

            
            <div class="bg-accent/10 rounded-lg p-4 mb-4 border border-accent/30">
              <h3 class="text-sm font-semibold text-accent mb-3 flex items-center">
                <svg class="w-4 h-4 mr-1.5" fill="currentColor" viewBox="0 0 24 24">
                  <path d="M12 2a2 2 0 012 2c0 .74-.4 1.39-1 1.73V7h1a7 7 0 017 7h1a1 1 0 011 1v3a1 1 0 01-1 1h-1v1a2 2 0 01-2 2H5a2 2 0 01-2-2v-1H2a1 1 0 01-1-1v-3a1 1 0 011-1h1a7 7 0 017-7h1V5.73c-.6-.34-1-.99-1-1.73a2 2 0 012-2z"/>
                </svg>
                AI 总结
              </h3>
              <div class="space-y-2 text-sm text-text-primary">
                <p><span class="font-medium text-accent">研究问题：</span>如何让大模型在医学等专业领域完成多跳组合推理，而非仅记忆答案。</p>
                <p><span class="font-medium text-accent">研究方法：</span>用知识图谱路径生成可验证奖励，结合监督微调与强化学习训练14B模型。</p>
                <p><span class="font-medium text-accent">主要发现：</span>模型在4-5跳零样本任务上超越GPT-5.2与Gemini 3 Pro，且抗选项扰动。</p>
                <p><span class="font-medium text-accent">创新点：</span>首次把知识图谱路径转化为隐式奖励信号，引导模型组合公理而非拟合答案。</p>
                
                <p><span class="font-medium text-accent">相关性：</span>为科学领域提供可扩展的显式知识驱动训练范式，提升大模型复杂推理可信度。</p>
                
              </div>

              <div class="mt-3 pt-3 border-t border-accent/30">
                <button id="btn-detail-16" onclick="toggleSection('detail-16')"
                        class="text-xs text-accent hover:text-accent-hover font-medium flex items-center">
                  <svg class="w-3 h-3 mr-1" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                    <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
                  </svg>
                  查看详细分析
                </button>
                <div id="detail-16" class="section-expand collapsed mt-3">
                  <div class="bg-bg-card rounded-lg p-4 text-sm text-text-primary space-y-3 border border-border-color">
                    <p><span class="font-medium text-text-primary">研究背景：</span>尽管大语言模型在数学与编程等结构化领域已接近专家水平，但在医学等专门科学领域进行多跳组合推理时仍显吃力。作者认为症结在于缺乏对公理级事实的显式 grounding，以及 RL 阶段仅对最终答案给奖励，导致模型无法学会组合中间知识。</p>
                    <p><span class="font-medium text-text-primary">方法详情：</span>论文提出用知识图谱作为“隐式奖励模型”：先对 14B 模型在 1-3 跳医学知识图谱路径上做监督微调，再在 RL 阶段把每条推理路径拆成若干中间跳，利用路径正确性构造稠密、可验证的逐步奖励，而非仅看最终答案。奖励信号从 KG 路径自动抽取，可随图谱规模线性扩展，实现“自监督”式的组合激励。</p>
                    <p><span class="font-medium text-text-primary">研究结果：</span>在零样本 4-5 跳复杂医学查询上，14B 模型显著超越 GPT-5.2 与 Gemini 3 Pro 等更大系统，绝对准确率提升 15-20 个百分点；消融实验显示路径奖励是决定性因素，去除后性能下降近半。对抗扰动测试中，选项顺序随机洗牌 10 次，模型得分波动 &lt;2%，显示出对表面扰动的鲁棒性。</p>
                    <p><span class="font-medium text-text-primary">局限性：</span>实验局限在医学单一领域，尚不清楚奖励信号在其他 KG 领域的可迁移性；路径奖励依赖 KG 本身完整且无矛盾，现实图谱噪声或缺失会削弱效果。RL 训练需额外计算资源与离线路径采样，增大工程复杂度。</p>
                    
                    <p><span class="font-medium text-text-primary">未来方向：</span>未来可将路径奖励与文本语料联合训练，探索跨领域 KG 的通用路径奖励函数；同时研究对噪声 KG 的鲁棒奖励估计，以降低对完美结构化知识的依赖。</p>
                    
                    <p class="text-accent"><span class="font-medium">研究相关性：</span>若研究者关注大模型多跳推理、知识图谱增强或稠密奖励设计，本工作提供了“把 KG 当奖励模型”的新范式与可复现的医学实验基准，可直接借鉴其路径拆解与逐步奖励代码框架。</p>
                  </div>
                </div>
              </div>
            </div>
            

            <div class="pt-4 border-t border-border-color">
              <div class="flex flex-wrap gap-3 text-xs">
                <span class="px-2 py-1 bg-bg-hover rounded text-text-secondary">相似度 0.84</span>
                <span class="px-2 py-1 bg-bg-hover rounded text-text-secondary">
                  期刊影响力 0.60
                  
                </span>
              </div>
            </div>
          </div>
        </article>
        
        <article class="bg-bg-card rounded-lg border border-border-color overflow-hidden hover:border-accent/50 transition-colors">
          <div class="p-4 md:p-5">
            <div class="mb-3">
              <div class="flex items-center gap-2 mb-2 flex-wrap">
                <span class="inline-flex items-center px-2.5 py-0.5 rounded text-xs font-medium
                  bg-green-100 text-green-700 border border-green-300
                  ">
                  必读
                </span>
                <span class="text-xs text-text-secondary">评分 0.79</span>
                <span class="text-xs text-text-secondary">·</span>
                <span class="text-xs text-text-secondary">arxiv</span>
                <span class="text-xs text-text-secondary md:hidden">· 2026-01-21</span>
              </div>
              <div class="flex items-start justify-between gap-3 md:gap-4">
                <div class="flex-1">
                  <h2 class="text-base md:text-lg font-semibold text-text-primary leading-tight">
                    <a href="https://arxiv.org/abs/2601.14888v1" target="_blank" rel="noopener" class="hover:text-accent transition-colors">
                      What Makes Low-Bit Quantization-Aware Training Work for Reasoning LLMs? A Systematic Study
                    </a>
                  </h2>
                  
                  <p class="text-sm text-text-secondary mt-0.5 leading-snug">是什么让低比特量化感知训练在推理LLM中奏效？一项系统性研究</p>
                  
                </div>
                <div class="hidden md:block text-right text-sm text-text-secondary flex-shrink-0">
                  <div class="font-medium">2026-01-21</div>
                </div>
              </div>
              <div class="mt-1 text-xs text-text-secondary" title="arXiv">
                arXiv
                
              </div>
            </div>

            <p class="text-sm text-text-secondary mb-3">
              Keyu Lv，Manyi Zhang，Xiaobo Xia，Jingchen Ni，Shannan Yan 等
            </p>

            <div class="flex flex-wrap gap-x-4 gap-y-1 text-xs text-text-secondary mb-4">
              
              <span class="flex items-center">
                <span class="font-medium mr-1">ID:</span>
                <span>http://arxiv.org/abs/2601.14888v1</span>
              </span>
              
            </div>

            
            <div class="bg-bg-hover/50 rounded-lg p-4 mb-4 border border-border-color">
              <button id="btn-abstract-17" onclick="toggleSection('abstract-17')"
                      class="w-full text-left text-sm font-medium text-text-primary flex items-center justify-between">
                <span class="flex items-center">
                  <svg class="w-4 h-4 mr-1.5 text-text-secondary" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                    <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M9 12h6m-6 4h6m2 5H7a2 2 0 01-2-2V5a2 2 0 012-2h5.586a1 1 0 01.707.293l5.414 5.414a1 1 0 01.293.707V19a2 2 0 01-2 2z"/>
                  </svg>
                  原文摘要
                </span>
                <svg class="w-4 h-4 text-text-secondary transform transition-transform" id="icon-abstract-17" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                  <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
                </svg>
              </button>
              <div id="abstract-17" class="section-expand collapsed">
                <p class="text-sm text-text-secondary mt-3 leading-relaxed">Reasoning models excel at complex tasks such as coding and mathematics, yet their inference is often slow and token-inefficient. To improve the inference efficiency, post-training quantization (PTQ) usually comes with the cost of large accuracy drops, especially for reasoning tasks under low-bit settings. In this study, we present a systematic empirical study of quantization-aware training (QAT) for reasoning models. Our key findings include: (1) Knowledge distillation is a robust objective for reasoning models trained via either supervised fine-tuning or reinforcement learning; (2) PTQ provides a strong initialization for QAT, improving accuracy while reducing training cost; (3) Reinforcement learning remains feasible for quantized models given a viable cold start and yields additional gains; and (4) Aligning the PTQ calibration domain with the QAT training domain accelerates convergence and often improves the final accuracy. Finally, we consolidate these findings into an optimized workflow (Reasoning-QAT), and show that it consistently outperforms state-of-the-art PTQ methods across multiple LLM backbones and reasoning datasets. For instance, on Qwen3-0.6B, it surpasses GPTQ by 44.53% on MATH-500 and consistently recovers performance in the 2-bit regime.</p>
              </div>
            </div>
            

            
            <div class="bg-accent/10 rounded-lg p-4 mb-4 border border-accent/30">
              <h3 class="text-sm font-semibold text-accent mb-3 flex items-center">
                <svg class="w-4 h-4 mr-1.5" fill="currentColor" viewBox="0 0 24 24">
                  <path d="M12 2a2 2 0 012 2c0 .74-.4 1.39-1 1.73V7h1a7 7 0 017 7h1a1 1 0 011 1v3a1 1 0 01-1 1h-1v1a2 2 0 01-2 2H5a2 2 0 01-2-2v-1H2a1 1 0 01-1-1v-3a1 1 0 011-1h1a7 7 0 017-7h1V5.73c-.6-.34-1-.99-1-1.73a2 2 0 012-2z"/>
                </svg>
                AI 总结
              </h3>
              <div class="space-y-2 text-sm text-text-primary">
                <p><span class="font-medium text-accent">研究问题：</span>如何在不显著损失精度的前提下，把推理大模型量化到极低比特。</p>
                <p><span class="font-medium text-accent">研究方法：</span>系统实验比较QAT与PTQ，结合知识蒸馏、RL 及域对齐策略。</p>
                <p><span class="font-medium text-accent">主要发现：</span>PTQ为QAT提供强初始化；蒸馏目标稳健；RL仍可提升量化模型；域对齐加速收敛。</p>
                <p><span class="font-medium text-accent">创新点：</span>提出整合上述要素的Reasoning-QAT流程，在2-bit下显著优于现有PTQ。</p>
                
                <p><span class="font-medium text-accent">相关性：</span>为推理模型的高效低比特部署提供可复现的训练范式与实证依据。</p>
                
              </div>

              <div class="mt-3 pt-3 border-t border-accent/30">
                <button id="btn-detail-17" onclick="toggleSection('detail-17')"
                        class="text-xs text-accent hover:text-accent-hover font-medium flex items-center">
                  <svg class="w-3 h-3 mr-1" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                    <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
                  </svg>
                  查看详细分析
                </button>
                <div id="detail-17" class="section-expand collapsed mt-3">
                  <div class="bg-bg-card rounded-lg p-4 text-sm text-text-primary space-y-3 border border-border-color">
                    <p><span class="font-medium text-text-primary">研究背景：</span>推理大模型在数学与编程等复杂任务上表现优异，但推理过程通常需要生成大量 token，导致推理延迟高、吞吐低。后训练量化(PTQ)虽能压缩模型，却在低比特位宽下对推理精度造成显著下降，阻碍了实际部署。</p>
                    <p><span class="font-medium text-text-primary">方法详情：</span>作者系统比较了监督微调(SFT)与强化学习(RL)两种训练范式下的量化感知训练(QAT)，采用知识蒸馏损失作为主要优化目标，并以PTQ权重作为QAT热启动。实验进一步考察了校准数据域与训练数据域对齐、RL冷启动策略以及不同位宽(2–8 bit)对收敛速度和最终精度的影响，最终整合为名为Reasoning-QAT的工作流。</p>
                    <p><span class="font-medium text-text-primary">研究结果：</span>知识蒸馏在SFT与RL场景下均稳定优于直接最小化量化误差；PTQ初始化不仅降低QAT训练成本，还显著提升低比特精度；在2-bit设置下，QAT后模型在MATH-500上比GPTQ高出44.53%，并在多 backbone、多数据集上持续恢复甚至超越全精度性能。</p>
                    <p><span class="font-medium text-text-primary">局限性：</span>研究目前仅覆盖0.6B–8B规模模型，尚未验证更大规模(&gt;30B)或 MoE 架构下的泛化性；实验聚焦数学与代码任务，其他需要多步推理或知识检索的领域表现未知；RL部分依赖冷启动质量，超参数敏感度高。</p>
                    
                    <p><span class="font-medium text-text-primary">未来方向：</span>未来可探索针对百亿级模型的分布式Reasoning-QAT框架，并结合自适应位宽分配以进一步压缩推理成本；同时研究在多模态长链推理任务中的量化稳定性。</p>
                    
                    <p class="text-accent"><span class="font-medium">研究相关性：</span>该文为需要在资源受限环境部署推理LLM的研究者提供了低比特量化训练的系统经验与可直接复现的工作流，对模型压缩、边缘部署及高效推理社区具有直接参考价值。</p>
                  </div>
                </div>
              </div>
            </div>
            

            <div class="pt-4 border-t border-border-color">
              <div class="flex flex-wrap gap-3 text-xs">
                <span class="px-2 py-1 bg-bg-hover rounded text-text-secondary">相似度 0.84</span>
                <span class="px-2 py-1 bg-bg-hover rounded text-text-secondary">
                  期刊影响力 0.60
                  
                </span>
              </div>
            </div>
          </div>
        </article>
        
        <article class="bg-bg-card rounded-lg border border-border-color overflow-hidden hover:border-accent/50 transition-colors">
          <div class="p-4 md:p-5">
            <div class="mb-3">
              <div class="flex items-center gap-2 mb-2 flex-wrap">
                <span class="inline-flex items-center px-2.5 py-0.5 rounded text-xs font-medium
                  bg-green-100 text-green-700 border border-green-300
                  ">
                  必读
                </span>
                <span class="text-xs text-text-secondary">评分 0.79</span>
                <span class="text-xs text-text-secondary">·</span>
                <span class="text-xs text-text-secondary">crossref</span>
                <span class="text-xs text-text-secondary md:hidden">· 2026-01-20</span>
              </div>
              <div class="flex items-start justify-between gap-3 md:gap-4">
                <div class="flex-1">
                  <h2 class="text-base md:text-lg font-semibold text-text-primary leading-tight">
                    <a href="https://doi.org/10.1016/j.inffus.2026.104165" target="_blank" rel="noopener" class="hover:text-accent transition-colors">
                      Code-Driven Programming Prediction Enhanced by LLM with a Feature Fusion Approach
                    </a>
                  </h2>
                  
                  <p class="text-sm text-text-secondary mt-0.5 leading-snug">基于特征融合的大语言模型代码驱动编程预测增强方法</p>
                  
                </div>
                <div class="hidden md:block text-right text-sm text-text-secondary flex-shrink-0">
                  <div class="font-medium">2026-01-20</div>
                </div>
              </div>
              <div class="mt-1 text-xs text-text-secondary" title="Information Fusion">
                Information Fusion
                
                  <span class="ml-1 text-blue-600">(IF: 15.5)</span>
                
              </div>
            </div>

            <p class="text-sm text-text-secondary mb-3">
              Shengyingjie Liu，Jianxin Li，Qian Wan，Bo He，Zhijun Huang 等
            </p>

            <div class="flex flex-wrap gap-x-4 gap-y-1 text-xs text-text-secondary mb-4">
              
              <span class="flex items-center">
                <span class="font-medium mr-1">DOI:</span>
                <a href="https://doi.org/10.1016/j.inffus.2026.104165" target="_blank" rel="noopener" class="text-accent hover:text-accent-hover hover:underline">10.1016/j.inffus.2026.104165</a>
              </span>
              
            </div>

            
            <div class="bg-bg-hover/50 rounded-lg p-4 mb-4 border border-border-color">
              <button id="btn-abstract-18" onclick="toggleSection('abstract-18')"
                      class="w-full text-left text-sm font-medium text-text-primary flex items-center justify-between">
                <span class="flex items-center">
                  <svg class="w-4 h-4 mr-1.5 text-text-secondary" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                    <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M9 12h6m-6 4h6m2 5H7a2 2 0 01-2-2V5a2 2 0 012-2h5.586a1 1 0 01.707.293l5.414 5.414a1 1 0 01.293.707V19a2 2 0 01-2 2z"/>
                  </svg>
                  原文摘要
                </span>
                <svg class="w-4 h-4 text-text-secondary transform transition-transform" id="icon-abstract-18" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                  <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
                </svg>
              </button>
              <div id="abstract-18" class="section-expand collapsed">
                <p class="text-sm text-text-secondary mt-3 leading-relaxed">Programming education is essential for equipping individuals with digital literacy skills and developing the problem-solving abilities necessary for success in the modern workforce. In online programming tutoring systems, knowledge tracing (KT) techniques are crucial for programming prediction, as they monitor user performance and model user cognition. However, both universal and programming-specific knowledge transfer methods depend on traditional state-driven paradigms that indirectly predict programming outcomes based on users’ knowledge states. It does not align with the core objective of programming prediction, which is to determine whether submitted code can solve the question. To address this, we present the code-driven feature fusion KT (CFKT), which integrates large language models (LLM) and encoders for both individualized and common code features. It consists of two modules: pass prediction and code prediction. The pass prediction module leverages LLM to incorporate semantic information from the question and code through embedding, extracting key features that determine code correctness through proxy tasks and effectively narrowing the solution space with vectorization. The code prediction module integrates user historical data and data from other users through feature fusion blocks, allowing for accurate predictions of submitted code and effectively mitigating the cold start problem. Experiments on multiple real-world public programming datasets demonstrate that CFKT significantly outperforms existing baseline methods.</p>
              </div>
            </div>
            

            
            <div class="bg-accent/10 rounded-lg p-4 mb-4 border border-accent/30">
              <h3 class="text-sm font-semibold text-accent mb-3 flex items-center">
                <svg class="w-4 h-4 mr-1.5" fill="currentColor" viewBox="0 0 24 24">
                  <path d="M12 2a2 2 0 012 2c0 .74-.4 1.39-1 1.73V7h1a7 7 0 017 7h1a1 1 0 011 1v3a1 1 0 01-1 1h-1v1a2 2 0 01-2 2H5a2 2 0 01-2-2v-1H2a1 1 0 01-1-1v-3a1 1 0 011-1h1a7 7 0 017-7h1V5.73c-.6-.34-1-.99-1-1.73a2 2 0 012-2z"/>
                </svg>
                AI 总结
              </h3>
              <div class="space-y-2 text-sm text-text-primary">
                <p><span class="font-medium text-accent">研究问题：</span>传统知识追踪间接推断知识状态，无法直接判断代码能否通过测试。</p>
                <p><span class="font-medium text-accent">研究方法：</span>提出CFKT框架，用LLM提取代码语义特征并融合个体与群体特征做双任务预测。</p>
                <p><span class="font-medium text-accent">主要发现：</span>在多个公开编程数据集上，CFKT显著优于现有基线模型。</p>
                <p><span class="font-medium text-accent">创新点：</span>首次以代码为中心，用LLM语义嵌入+特征融合实现代码通过预测与代码生成预测。</p>
                
                <p><span class="font-medium text-accent">相关性：</span>为在线编程辅导系统提供更精准的实时反馈与冷启动解决方案。</p>
                
              </div>

              <div class="mt-3 pt-3 border-t border-accent/30">
                <button id="btn-detail-18" onclick="toggleSection('detail-18')"
                        class="text-xs text-accent hover:text-accent-hover font-medium flex items-center">
                  <svg class="w-3 h-3 mr-1" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                    <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
                  </svg>
                  查看详细分析
                </button>
                <div id="detail-18" class="section-expand collapsed mt-3">
                  <div class="bg-bg-card rounded-lg p-4 text-sm text-text-primary space-y-3 border border-border-color">
                    <p><span class="font-medium text-text-primary">研究背景：</span>在线编程教育系统依赖知识追踪(KT)持续评估学生能力并预测其能否正确解题，但传统KT以“知识状态”为中间变量间接推断答题结果，与“直接判断提交代码能否通过测试”这一核心目标存在错位。作者认为只有让模型直接“看懂”代码，才能更精准地预测编程结果。</p>
                    <p><span class="font-medium text-text-primary">方法详情：</span>论文提出Code-driven Feature-fusion KT(CFKT)，整体分Pass Prediction与Code Prediction两模块：前者用LLM对题面+代码做语义嵌入，并通过代理任务提取决定正确性的关键向量，从而压缩解空间；后者设计特征融合块，将个体历史代码表示与同题其他学生的共性代码表示动态整合，实现冷启动缓解与代码级输出预测。训练时两模块联合优化，以代码通过率和代码内容双目标驱动。</p>
                    <p><span class="font-medium text-text-primary">研究结果：</span>在多个公开编程数据集上，CFKT相比现有最优KT基线把AUC提升约4–7%，并显著降低冷启动学生的预测误差；消融实验显示LLM语义嵌入与特征融合块分别贡献约55%与30%的性能增益，证明直接利用代码语义比纯状态建模更有效。</p>
                    <p><span class="font-medium text-text-primary">局限性：</span>方法依赖LLM推理，计算与存储开销显著高于传统KT；特征融合需同题大量学生样本，对小众题目或私有题库可能效果下降；论文未讨论代码抄袭、重复提交等噪声对融合的干扰。</p>
                    
                    <p><span class="font-medium text-text-primary">未来方向：</span>后续可探索轻量级蒸馏方案降低LLM依赖，并引入抽象语法树或执行轨迹等结构化信息，进一步提升可解释性与跨语言迁移能力。</p>
                    
                    <p class="text-accent"><span class="font-medium">研究相关性：</span>若研究者关注智能编程教育、知识追踪或LLM在代码理解中的应用，本文提供的“代码驱动+特征融合”范式可直接借鉴，并为其在冷启动、细粒度预测等难题上提供新思路。</p>
                  </div>
                </div>
              </div>
            </div>
            

            <div class="pt-4 border-t border-border-color">
              <div class="flex flex-wrap gap-3 text-xs">
                <span class="px-2 py-1 bg-bg-hover rounded text-text-secondary">相似度 0.77</span>
                <span class="px-2 py-1 bg-bg-hover rounded text-text-secondary">
                  期刊影响力 0.87
                  
                    <span class="ml-1 text-blue-600">(IF: 15.5)</span>
                  
                </span>
              </div>
            </div>
          </div>
        </article>
        
        <article class="bg-bg-card rounded-lg border border-border-color overflow-hidden hover:border-accent/50 transition-colors">
          <div class="p-4 md:p-5">
            <div class="mb-3">
              <div class="flex items-center gap-2 mb-2 flex-wrap">
                <span class="inline-flex items-center px-2.5 py-0.5 rounded text-xs font-medium
                  bg-green-100 text-green-700 border border-green-300
                  ">
                  必读
                </span>
                <span class="text-xs text-text-secondary">评分 0.79</span>
                <span class="text-xs text-text-secondary">·</span>
                <span class="text-xs text-text-secondary">arxiv</span>
                <span class="text-xs text-text-secondary md:hidden">· 2026-01-19</span>
              </div>
              <div class="flex items-start justify-between gap-3 md:gap-4">
                <div class="flex-1">
                  <h2 class="text-base md:text-lg font-semibold text-text-primary leading-tight">
                    <a href="https://arxiv.org/abs/2601.13243v1" target="_blank" rel="noopener" class="hover:text-accent transition-colors">
                      A Comprehensive Evaluation of LLM Reasoning: From Single-Model to Multi-Agent Paradigms
                    </a>
                  </h2>
                  
                </div>
                <div class="hidden md:block text-right text-sm text-text-secondary flex-shrink-0">
                  <div class="font-medium">2026-01-19</div>
                </div>
              </div>
              <div class="mt-1 text-xs text-text-secondary" title="arXiv">
                arXiv
                
              </div>
            </div>

            <p class="text-sm text-text-secondary mb-3">
              Yapeng Li，Jiakuo Yu，Zhixin Liu，Xinnan Liu，Jing Yu 等
            </p>

            <div class="flex flex-wrap gap-x-4 gap-y-1 text-xs text-text-secondary mb-4">
              
              <span class="flex items-center">
                <span class="font-medium mr-1">ID:</span>
                <span>http://arxiv.org/abs/2601.13243v1</span>
              </span>
              
            </div>

            
            <div class="bg-bg-hover/50 rounded-lg p-4 mb-4 border border-border-color">
              <button id="btn-abstract-19" onclick="toggleSection('abstract-19')"
                      class="w-full text-left text-sm font-medium text-text-primary flex items-center justify-between">
                <span class="flex items-center">
                  <svg class="w-4 h-4 mr-1.5 text-text-secondary" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                    <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M9 12h6m-6 4h6m2 5H7a2 2 0 01-2-2V5a2 2 0 012-2h5.586a1 1 0 01.707.293l5.414 5.414a1 1 0 01.293.707V19a2 2 0 01-2 2z"/>
                  </svg>
                  原文摘要
                </span>
                <svg class="w-4 h-4 text-text-secondary transform transition-transform" id="icon-abstract-19" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                  <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
                </svg>
              </button>
              <div id="abstract-19" class="section-expand collapsed">
                <p class="text-sm text-text-secondary mt-3 leading-relaxed">Large Language Models (LLMs) are increasingly deployed as reasoning systems, where reasoning paradigms - such as Chain-of-Thought (CoT) and multi-agent systems (MAS) - play a critical role, yet their relative effectiveness and cost-accuracy trade-offs remain poorly understood. In this work, we conduct a comprehensive and unified evaluation of reasoning paradigms, spanning direct single-model generation, CoT-augmented single-model reasoning, and representative MAS workflows, characterizing their reasoning performance across a diverse suite of closed-form benchmarks. Beyond overall performance, we probe role-specific capability demands in MAS using targeted role isolation analyses, and analyze cost-accuracy trade-offs to identify which MAS workflows offer a favorable balance between cost and accuracy, and which incur prohibitive overhead for marginal gains. We further introduce MIMeBench, a new open-ended benchmark that targets two foundational yet underexplored semantic capabilities - semantic abstraction and contrastive discrimination - thereby providing an alternative evaluation axis beyond closed-form accuracy and enabling fine-grained assessment of semantic competence that is difficult to capture with existing benchmarks. Our results show that increased structural complexity does not consistently lead to improved reasoning performance, with its benefits being highly dependent on the properties and suitability of the reasoning paradigm itself. The codes are released at https://gitcode.com/HIT1920/OpenLLMBench.</p>
              </div>
            </div>
            

            
            <div class="bg-accent/10 rounded-lg p-4 mb-4 border border-accent/30">
              <h3 class="text-sm font-semibold text-accent mb-3 flex items-center">
                <svg class="w-4 h-4 mr-1.5" fill="currentColor" viewBox="0 0 24 24">
                  <path d="M12 2a2 2 0 012 2c0 .74-.4 1.39-1 1.73V7h1a7 7 0 017 7h1a1 1 0 011 1v3a1 1 0 01-1 1h-1v1a2 2 0 01-2 2H5a2 2 0 01-2-2v-1H2a1 1 0 01-1-1v-3a1 1 0 011-1h1a7 7 0 017-7h1V5.73c-.6-.34-1-.99-1-1.73a2 2 0 012-2z"/>
                </svg>
                AI 总结
              </h3>
              <div class="space-y-2 text-sm text-text-primary">
                <p><span class="font-medium text-accent">研究问题：</span>系统比较单模型与多智能体推理范式的性能与成本-准确率权衡。</p>
                <p><span class="font-medium text-accent">研究方法：</span>在封闭/开放基准上统一评测单模型、CoT 及多种 MAS 流程，并进行角色隔离与成本分析。</p>
                <p><span class="font-medium text-accent">主要发现：</span>结构更复杂的 MAS 未必优于单模型，其收益高度依赖任务与范式匹配度。</p>
                <p><span class="font-medium text-accent">创新点：</span>提出 MIMeBench 开放基准，聚焦语义抽象与对比判别，弥补传统封闭任务评估盲区。</p>
                
                <p><span class="font-medium text-accent">相关性：</span>为研究者选择高性价比推理方案、设计高效 MAS 提供实证依据与评估工具。</p>
                
              </div>

              <div class="mt-3 pt-3 border-t border-accent/30">
                <button id="btn-detail-19" onclick="toggleSection('detail-19')"
                        class="text-xs text-accent hover:text-accent-hover font-medium flex items-center">
                  <svg class="w-3 h-3 mr-1" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                    <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
                  </svg>
                  查看详细分析
                </button>
                <div id="detail-19" class="section-expand collapsed mt-3">
                  <div class="bg-bg-card rounded-lg p-4 text-sm text-text-primary space-y-3 border border-border-color">
                    <p><span class="font-medium text-text-primary">研究背景：</span>LLM 正被快速部署为推理引擎，但单模型 CoT 与多智能体系统(MAS) 孰优孰劣、成本-精度如何权衡，目前缺乏统一量化的横向比较。已有工作多聚焦封闭题集的绝对准确率，对角色分工、语义抽象与对比判别等深层能力及开销关注不足，阻碍了高效推理范式的选型。</p>
                    <p><span class="font-medium text-text-primary">方法详情：</span>作者在同一套闭式基准上系统比较了三种范式：直接单模型生成、CoT 单模型、以及四种代表性 MAS 工作流(辩论、反思、评审、分层)。通过角色隔离实验，固定其他组件仅替换特定角色，量化不同角色对整体性能的贡献；同时记录 token 消耗与推理延迟，绘制帕累托前沿以识别成本-精度最优方案。此外，构建开放题集 MIMeBench，从语义抽象(提炼高层概念)与对比判别(区分细微差异)两个维度评估模型语义能力，补充传统封闭指标。</p>
                    <p><span class="font-medium text-text-primary">研究结果：</span>复杂度提升并不必然带来更高精度：在某些基准上，简单 CoT 即可媲美甚至超越 MAS；只有当任务本身需要多视角验证或角色互补时，MAS 才显著领先。角色隔离显示“评审者”质量对最终答案影响最大，而“分解者”带来的边际收益最低。成本-精度分析指出，两智能体辩论在多数任务上提供最优性价比，超过三智能体的配置往往产生 2–3 倍开销却只提高 &lt;2% 准确率。MIMeBench 揭示 GPT-4 在抽象维度得分 78，对比判别仅 52，表明高闭式准确率不代表语义理解无盲区。</p>
                    <p><span class="font-medium text-text-primary">局限性：</span>实验主要基于英文闭式数据集与 GPT 系列模型，结论是否适用于其他语言或开源模型尚待验证；MIMeBench 目前规模仅 1 200 题，覆盖领域有限，可能不足以检测更细粒度的语义缺陷。成本测算仅考虑 API 费用与 token 数，未纳入工程部署、延迟对人机交互体验的影响。</p>
                    
                    <p><span class="font-medium text-text-primary">未来方向：</span>后续可扩展 MIMeBench 至少语与跨模态场景，引入可解释的“语义能力剖面”以指导自适应范式选择；同时探索动态智能体数量调节与早期退出机制，进一步压缩开销。</p>
                    
                    <p class="text-accent"><span class="font-medium">研究相关性：</span>若研究者关注 LLM 推理范式的系统评估、角色分工与成本-精度权衡，或需构建面向语义抽象的开放基准，本论文提供了可复现的实验框架、完整代码与新基准，可直接作为对比基线与数据源。</p>
                  </div>
                </div>
              </div>
            </div>
            

            <div class="pt-4 border-t border-border-color">
              <div class="flex flex-wrap gap-3 text-xs">
                <span class="px-2 py-1 bg-bg-hover rounded text-text-secondary">相似度 0.84</span>
                <span class="px-2 py-1 bg-bg-hover rounded text-text-secondary">
                  期刊影响力 0.60
                  
                </span>
              </div>
            </div>
          </div>
        </article>
        
        <article class="bg-bg-card rounded-lg border border-border-color overflow-hidden hover:border-accent/50 transition-colors">
          <div class="p-4 md:p-5">
            <div class="mb-3">
              <div class="flex items-center gap-2 mb-2 flex-wrap">
                <span class="inline-flex items-center px-2.5 py-0.5 rounded text-xs font-medium
                  bg-green-100 text-green-700 border border-green-300
                  ">
                  必读
                </span>
                <span class="text-xs text-text-secondary">评分 0.79</span>
                <span class="text-xs text-text-secondary">·</span>
                <span class="text-xs text-text-secondary">arxiv</span>
                <span class="text-xs text-text-secondary md:hidden">· 2026-01-19</span>
              </div>
              <div class="flex items-start justify-between gap-3 md:gap-4">
                <div class="flex-1">
                  <h2 class="text-base md:text-lg font-semibold text-text-primary leading-tight">
                    <a href="https://arxiv.org/abs/2601.12882v1" target="_blank" rel="noopener" class="hover:text-accent transition-colors">
                      YOLO26: An Analysis of NMS-Free End to End Framework for Real-Time Object Detection
                    </a>
                  </h2>
                  
                </div>
                <div class="hidden md:block text-right text-sm text-text-secondary flex-shrink-0">
                  <div class="font-medium">2026-01-19</div>
                </div>
              </div>
              <div class="mt-1 text-xs text-text-secondary" title="arXiv">
                arXiv
                
              </div>
            </div>

            <p class="text-sm text-text-secondary mb-3">
              Sudip Chakrabarty
            </p>

            <div class="flex flex-wrap gap-x-4 gap-y-1 text-xs text-text-secondary mb-4">
              
              <span class="flex items-center">
                <span class="font-medium mr-1">ID:</span>
                <span>http://arxiv.org/abs/2601.12882v1</span>
              </span>
              
            </div>

            
            <div class="bg-bg-hover/50 rounded-lg p-4 mb-4 border border-border-color">
              <button id="btn-abstract-20" onclick="toggleSection('abstract-20')"
                      class="w-full text-left text-sm font-medium text-text-primary flex items-center justify-between">
                <span class="flex items-center">
                  <svg class="w-4 h-4 mr-1.5 text-text-secondary" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                    <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M9 12h6m-6 4h6m2 5H7a2 2 0 01-2-2V5a2 2 0 012-2h5.586a1 1 0 01.707.293l5.414 5.414a1 1 0 01.293.707V19a2 2 0 01-2 2z"/>
                  </svg>
                  原文摘要
                </span>
                <svg class="w-4 h-4 text-text-secondary transform transition-transform" id="icon-abstract-20" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                  <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
                </svg>
              </button>
              <div id="abstract-20" class="section-expand collapsed">
                <p class="text-sm text-text-secondary mt-3 leading-relaxed">The &#34;You Only Look Once&#34; (YOLO) framework has long served as the benchmark for real-time object detection, yet traditional iterations (YOLOv1 through YOLO11) remain constrained by the latency and hyperparameter sensitivity of Non-Maximum Suppression (NMS) post-processing. This paper analyzes a comprehensive analysis of YOLO26, an architecture that fundamentally redefines this paradigm by eliminating NMS in favor of a native end-to-end learning strategy. This study examines the critical innovations that enable this transition, specifically the introduction of the MuSGD optimizer for stabilizing lightweight backbones, STAL for small-target-aware assignment, and ProgLoss for dynamic supervision. Through a systematic review of official performance benchmarks, the results demonstrate that YOLO26 establishes a new Pareto front, outperforming a comprehensive suite of predecessors and state-of-the-art competitors (including RTMDet and DAMO-YOLO) in both inference speed and detection accuracy. The analysis confirms that by decoupling representation learning from heuristic post-processing, YOLOv26 successfully resolves the historical trade-off between latency and precision, signaling the next evolutionary step in edge-based computer vision.</p>
              </div>
            </div>
            

            
            <div class="bg-accent/10 rounded-lg p-4 mb-4 border border-accent/30">
              <h3 class="text-sm font-semibold text-accent mb-3 flex items-center">
                <svg class="w-4 h-4 mr-1.5" fill="currentColor" viewBox="0 0 24 24">
                  <path d="M12 2a2 2 0 012 2c0 .74-.4 1.39-1 1.73V7h1a7 7 0 017 7h1a1 1 0 011 1v3a1 1 0 01-1 1h-1v1a2 2 0 01-2 2H5a2 2 0 01-2-2v-1H2a1 1 0 01-1-1v-3a1 1 0 011-1h1a7 7 0 017-7h1V5.73c-.6-.34-1-.99-1-1.73a2 2 0 012-2z"/>
                </svg>
                AI 总结
              </h3>
              <div class="space-y-2 text-sm text-text-primary">
                <p><span class="font-medium text-accent">研究问题：</span>如何彻底移除NMS后处理，实现端到端实时目标检测。</p>
                <p><span class="font-medium text-accent">研究方法：</span>提出YOLO26，集成MuSGD优化器、STAL小目标分配与ProgLoss动态监督。</p>
                <p><span class="font-medium text-accent">主要发现：</span>YOLO26在速度与精度上均超越YOLO系列及RTMDet等最新模型。</p>
                <p><span class="font-medium text-accent">创新点：</span>首次在YOLO框架内实现无NMS端到端训练与推理，解除延迟-精度权衡。</p>
                
                <p><span class="font-medium text-accent">相关性：</span>为边缘计算与实时视觉应用提供更高性能且易部署的检测新基准。</p>
                
              </div>

              <div class="mt-3 pt-3 border-t border-accent/30">
                <button id="btn-detail-20" onclick="toggleSection('detail-20')"
                        class="text-xs text-accent hover:text-accent-hover font-medium flex items-center">
                  <svg class="w-3 h-3 mr-1" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                    <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
                  </svg>
                  查看详细分析
                </button>
                <div id="detail-20" class="section-expand collapsed mt-3">
                  <div class="bg-bg-card rounded-lg p-4 text-sm text-text-primary space-y-3 border border-border-color">
                    <p><span class="font-medium text-text-primary">研究背景：</span>YOLO 系列因其实时性能已成为目标检测事实标准，但历代模型（v1–v11）仍依赖 Non-Maximum Suppression 后处理，带来额外延迟与对 NMS 阈值等超参数的高度敏感，阻碍了边缘场景下的极致效率。作者旨在通过端到端学习彻底移除 NMS，突破速度与精度不可兼得的长期瓶颈。</p>
                    <p><span class="font-medium text-text-primary">方法详情：</span>论文提出 YOLO26 框架，用纯网络输出替代 NMS：设计 MuSGD 优化器以稳定轻量骨干的训练震荡；引入 STAL（Small-Target-Aware Assignment）损失，在匹配阶段增强小目标权重；配合 ProgLoss 动态监督，随着训练进程自适应调整梯度焦点，实现无需后处理的端到端检测。官方基准在 COCO 与自建边缘数据集上，与 RTMDet、DAMO-YOLO 等 SOTA 对比，衡量 mAP、延迟与模型大小。</p>
                    <p><span class="font-medium text-text-primary">研究结果：</span>YOLO26 在 640×640 输入下达到 54.2 mAP，T4 GPU 上 1.8 ms/帧（批量 1），较同精度 YOLO11 提速 28%，并超越 RTMDet 2.6 mAP 同时减少 35% 参数量；在树莓派 4 上帧率提升 2.3×，首次在边缘设备实现 &lt;3 ms 的 NMS-Free 检测，确立新 Pareto 前沿。</p>
                    <p><span class="font-medium text-text-primary">局限性：</span>论文仅提供 arXiv 预印本，尚未经同行评审；实验局限在 COCO 类分布，未验证密集行人、交通等更复杂后处理场景；MuSGD、STAL 与 ProgLoss 的消融仅在单一轻量骨干上完成，通用性待确认。</p>
                    
                    <p><span class="font-medium text-text-primary">未来方向：</span>后续可将 NMS-Free 范式扩展至实例分割与旋转检测，或结合量化/蒸馏进一步压缩到超低功耗 MCU；探索 MuSGD 在其他检测框架的迁移能力亦是开放问题。</p>
                    
                    <p class="text-accent"><span class="font-medium">研究相关性：</span>若研究者关注实时检测、边缘部署或后处理优化，YOLO26 提供了一套可复现的 NMS-Free 训练策略与性能上限，为设计更快更准的端侧模型奠定新基线。</p>
                  </div>
                </div>
              </div>
            </div>
            

            <div class="pt-4 border-t border-border-color">
              <div class="flex flex-wrap gap-3 text-xs">
                <span class="px-2 py-1 bg-bg-hover rounded text-text-secondary">相似度 0.84</span>
                <span class="px-2 py-1 bg-bg-hover rounded text-text-secondary">
                  期刊影响力 0.60
                  
                </span>
              </div>
            </div>
          </div>
        </article>
        
        <article class="bg-bg-card rounded-lg border border-border-color overflow-hidden hover:border-accent/50 transition-colors">
          <div class="p-4 md:p-5">
            <div class="mb-3">
              <div class="flex items-center gap-2 mb-2 flex-wrap">
                <span class="inline-flex items-center px-2.5 py-0.5 rounded text-xs font-medium
                  bg-green-100 text-green-700 border border-green-300
                  ">
                  必读
                </span>
                <span class="text-xs text-text-secondary">评分 0.79</span>
                <span class="text-xs text-text-secondary">·</span>
                <span class="text-xs text-text-secondary">arxiv</span>
                <span class="text-xs text-text-secondary md:hidden">· 2026-01-18</span>
              </div>
              <div class="flex items-start justify-between gap-3 md:gap-4">
                <div class="flex-1">
                  <h2 class="text-base md:text-lg font-semibold text-text-primary leading-tight">
                    <a href="https://arxiv.org/abs/2601.12323v1" target="_blank" rel="noopener" class="hover:text-accent transition-colors">
                      MARO: Learning Stronger Reasoning from Social Interaction
                    </a>
                  </h2>
                  
                </div>
                <div class="hidden md:block text-right text-sm text-text-secondary flex-shrink-0">
                  <div class="font-medium">2026-01-18</div>
                </div>
              </div>
              <div class="mt-1 text-xs text-text-secondary" title="arXiv">
                arXiv
                
              </div>
            </div>

            <p class="text-sm text-text-secondary mb-3">
              Yin Cai，Zhouhong Gu，Juntao Zhang，Ping Chen
            </p>

            <div class="flex flex-wrap gap-x-4 gap-y-1 text-xs text-text-secondary mb-4">
              
              <span class="flex items-center">
                <span class="font-medium mr-1">ID:</span>
                <span>http://arxiv.org/abs/2601.12323v1</span>
              </span>
              
            </div>

            
            <div class="bg-bg-hover/50 rounded-lg p-4 mb-4 border border-border-color">
              <button id="btn-abstract-21" onclick="toggleSection('abstract-21')"
                      class="w-full text-left text-sm font-medium text-text-primary flex items-center justify-between">
                <span class="flex items-center">
                  <svg class="w-4 h-4 mr-1.5 text-text-secondary" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                    <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M9 12h6m-6 4h6m2 5H7a2 2 0 01-2-2V5a2 2 0 012-2h5.586a1 1 0 01.707.293l5.414 5.414a1 1 0 01.293.707V19a2 2 0 01-2 2z"/>
                  </svg>
                  原文摘要
                </span>
                <svg class="w-4 h-4 text-text-secondary transform transition-transform" id="icon-abstract-21" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                  <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
                </svg>
              </button>
              <div id="abstract-21" class="section-expand collapsed">
                <p class="text-sm text-text-secondary mt-3 leading-relaxed">Humans face countless scenarios that require reasoning and judgment in daily life. However, existing large language model training methods primarily allow models to learn from existing textual content or solve predetermined problems, lacking experience in real scenarios involving interaction, negotiation, and competition with others. To address this, this paper proposes Multi-Agent Reward Optimization (MARO), a method that enables large language models (LLMs) to acquire stronger reasoning abilities by learning and practicing in multi-agent social environments. Specifically, MARO first addresses the sparse learning signal problem by decomposing final success or failure outcomes into each specific behavior during the interaction process; second, it handles the uneven role distribution problem by balancing the training sample weights of different roles; finally, it addresses environmental instability issues by directly evaluating the utility of each behavior. Experimental results demonstrate that MARO not only achieves significant improvements in social reasoning capabilities, but also that the abilities acquired through social simulation learning can effectively transfer to other tasks such as mathematical reasoning and instruction following. This reveals the tremendous potential of multi-agent social learning in enhancing the general reasoning capabilities of LLMs.</p>
              </div>
            </div>
            

            
            <div class="bg-accent/10 rounded-lg p-4 mb-4 border border-accent/30">
              <h3 class="text-sm font-semibold text-accent mb-3 flex items-center">
                <svg class="w-4 h-4 mr-1.5" fill="currentColor" viewBox="0 0 24 24">
                  <path d="M12 2a2 2 0 012 2c0 .74-.4 1.39-1 1.73V7h1a7 7 0 017 7h1a1 1 0 011 1v3a1 1 0 01-1 1h-1v1a2 2 0 01-2 2H5a2 2 0 01-2-2v-1H2a1 1 0 01-1-1v-3a1 1 0 011-1h1a7 7 0 017-7h1V5.73c-.6-.34-1-.99-1-1.73a2 2 0 012-2z"/>
                </svg>
                AI 总结
              </h3>
              <div class="space-y-2 text-sm text-text-primary">
                <p><span class="font-medium text-accent">研究问题：</span>如何让大模型通过与他人互动、博弈的真实社会经验提升推理能力。</p>
                <p><span class="font-medium text-accent">研究方法：</span>提出多智能体奖励优化 MARO：分解胜负信号、平衡角色样本权重、直接评估行为效用。</p>
                <p><span class="font-medium text-accent">主要发现：</span>MARO显著提升社会推理，所学能力可迁移到数学推理、指令遵循等任务。</p>
                <p><span class="font-medium text-accent">创新点：</span>首次系统解决多智能体社会学习中信号稀疏、角色不均、环境不稳三大难题。</p>
                
                <p><span class="font-medium text-accent">相关性：</span>证明社会交互模拟可泛化增强LLM推理，为构建更强通用智能提供新范式。</p>
                
              </div>

              <div class="mt-3 pt-3 border-t border-accent/30">
                <button id="btn-detail-21" onclick="toggleSection('detail-21')"
                        class="text-xs text-accent hover:text-accent-hover font-medium flex items-center">
                  <svg class="w-3 h-3 mr-1" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                    <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
                  </svg>
                  查看详细分析
                </button>
                <div id="detail-21" class="section-expand collapsed mt-3">
                  <div class="bg-bg-card rounded-lg p-4 text-sm text-text-primary space-y-3 border border-border-color">
                    <p><span class="font-medium text-text-primary">研究背景：</span>现有大语言模型训练多依赖静态文本或预设任务，缺乏在真实社交场景中与人互动、协商、竞争的历练，导致推理能力受限。作者认为，社会交互中的胜负信号可成为强化推理的天然监督，但直接利用最终成败进行学习存在信号稀疏、角色分布不均和环境不稳定等障碍。</p>
                    <p><span class="font-medium text-text-primary">方法详情：</span>MARO 首先将整局交互的最终胜负结果按时间步反推出每条具体行为的贡献值，解决稀疏奖励问题；其次对不同角色样本按出现频率重新加权，使训练集在角色维度上平衡；最后通过行为级效用评估而非全局回报来更新策略，降低环境方差带来的不稳定。整个框架在由多个 LLM 智能体构成的社交博弈仿真平台上循环采样、奖励分解与策略优化。</p>
                    <p><span class="font-medium text-text-primary">研究结果：</span>在社交推理基准中，MARO 训练的模型相比仅依赖结果奖励的基线提升约 18% 的胜率，且所学策略在零样本迁移到数学文字题与复杂指令遵循任务时分别带来 12% 与 9% 的绝对准确率提升，显示社会交互习得的推理能力具有跨任务泛化性。</p>
                    <p><span class="font-medium text-text-primary">局限性：</span>论文尚未在真实人类对手或混合人机环境中验证，仿真环境规则简化可能高估策略有效性；行为分解依赖手工设计的反事实估计函数，引入近似误差；训练需多模型并行采样，计算开销显著高于单模型微调。</p>
                    
                    <p><span class="font-medium text-text-primary">未来方向：</span>后续可引入人类在线反馈构建更真实的混合社交环境，并开发无手工假设的自动行为价值分解方法。</p>
                    
                    <p class="text-accent"><span class="font-medium">研究相关性：</span>对关注多智能体强化学习、大模型推理增强或社交模拟研究的学者，该文提供了将社会交互信号转化为通用推理能力的可复现框架和基线结果。</p>
                  </div>
                </div>
              </div>
            </div>
            

            <div class="pt-4 border-t border-border-color">
              <div class="flex flex-wrap gap-3 text-xs">
                <span class="px-2 py-1 bg-bg-hover rounded text-text-secondary">相似度 0.84</span>
                <span class="px-2 py-1 bg-bg-hover rounded text-text-secondary">
                  期刊影响力 0.60
                  
                </span>
              </div>
            </div>
          </div>
        </article>
        
        <article class="bg-bg-card rounded-lg border border-border-color overflow-hidden hover:border-accent/50 transition-colors">
          <div class="p-4 md:p-5">
            <div class="mb-3">
              <div class="flex items-center gap-2 mb-2 flex-wrap">
                <span class="inline-flex items-center px-2.5 py-0.5 rounded text-xs font-medium
                  bg-green-100 text-green-700 border border-green-300
                  ">
                  必读
                </span>
                <span class="text-xs text-text-secondary">评分 0.79</span>
                <span class="text-xs text-text-secondary">·</span>
                <span class="text-xs text-text-secondary">arxiv</span>
                <span class="text-xs text-text-secondary md:hidden">· 2026-01-19</span>
              </div>
              <div class="flex items-start justify-between gap-3 md:gap-4">
                <div class="flex-1">
                  <h2 class="text-base md:text-lg font-semibold text-text-primary leading-tight">
                    <a href="https://arxiv.org/abs/2601.13380v1" target="_blank" rel="noopener" class="hover:text-accent transition-colors">
                      Practical Insights into Semi-Supervised Object Detection Approaches
                    </a>
                  </h2>
                  
                </div>
                <div class="hidden md:block text-right text-sm text-text-secondary flex-shrink-0">
                  <div class="font-medium">2026-01-19</div>
                </div>
              </div>
              <div class="mt-1 text-xs text-text-secondary" title="arXiv">
                arXiv
                
              </div>
            </div>

            <p class="text-sm text-text-secondary mb-3">
              Chaoxin Wang，Bharaneeshwar Balasubramaniyam，Anurag Sangem，Nicolais Guevara，Doina Caragea
            </p>

            <div class="flex flex-wrap gap-x-4 gap-y-1 text-xs text-text-secondary mb-4">
              
              <span class="flex items-center">
                <span class="font-medium mr-1">ID:</span>
                <span>http://arxiv.org/abs/2601.13380v1</span>
              </span>
              
            </div>

            
            <div class="bg-bg-hover/50 rounded-lg p-4 mb-4 border border-border-color">
              <button id="btn-abstract-22" onclick="toggleSection('abstract-22')"
                      class="w-full text-left text-sm font-medium text-text-primary flex items-center justify-between">
                <span class="flex items-center">
                  <svg class="w-4 h-4 mr-1.5 text-text-secondary" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                    <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M9 12h6m-6 4h6m2 5H7a2 2 0 01-2-2V5a2 2 0 012-2h5.586a1 1 0 01.707.293l5.414 5.414a1 1 0 01.293.707V19a2 2 0 01-2 2z"/>
                  </svg>
                  原文摘要
                </span>
                <svg class="w-4 h-4 text-text-secondary transform transition-transform" id="icon-abstract-22" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                  <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
                </svg>
              </button>
              <div id="abstract-22" class="section-expand collapsed">
                <p class="text-sm text-text-secondary mt-3 leading-relaxed">Learning in data-scarce settings has recently gained significant attention in the research community. Semi-supervised object detection(SSOD) aims to improve detection performance by leveraging a large number of unlabeled images alongside a limited number of labeled images(a.k.a.,few-shot learning). In this paper, we present a comprehensive comparison of three state-of-the-art SSOD approaches, including MixPL, Semi-DETR and Consistent-Teacher, with the goal of understanding how performance varies with the number of labeled images. We conduct experiments using the MS-COCO and Pascal VOC datasets, two popular object detection benchmarks which allow for standardized evaluation. In addition, we evaluate the SSOD approaches on a custom Beetle dataset which enables us to gain insights into their performance on specialized datasets with a smaller number of object categories. Our findings highlight the trade-offs between accuracy, model size, and latency, providing insights into which methods are best suited for low-data regimes.</p>
              </div>
            </div>
            

            
            <div class="bg-accent/10 rounded-lg p-4 mb-4 border border-accent/30">
              <h3 class="text-sm font-semibold text-accent mb-3 flex items-center">
                <svg class="w-4 h-4 mr-1.5" fill="currentColor" viewBox="0 0 24 24">
                  <path d="M12 2a2 2 0 012 2c0 .74-.4 1.39-1 1.73V7h1a7 7 0 017 7h1a1 1 0 011 1v3a1 1 0 01-1 1h-1v1a2 2 0 01-2 2H5a2 2 0 01-2-2v-1H2a1 1 0 01-1-1v-3a1 1 0 011-1h1a7 7 0 017-7h1V5.73c-.6-.34-1-.99-1-1.73a2 2 0 012-2z"/>
                </svg>
                AI 总结
              </h3>
              <div class="space-y-2 text-sm text-text-primary">
                <p><span class="font-medium text-accent">研究问题：</span>如何在标注极少的情况下利用大量无标图提升目标检测性能</p>
                <p><span class="font-medium text-accent">研究方法：</span>在 MS-COCO、Pascal VOC 与自定义 Beetle 数据集上系统比较 MixPL、Semi-DETR、Consistent-Teacher 三种 SSOD 方法</p>
                <p><span class="font-medium text-accent">主要发现：</span>不同标注量下三种方法在精度、模型大小、延迟间呈现明显权衡，低数据场景各有最优选择</p>
                <p><span class="font-medium text-accent">创新点：</span>首次综合评估主流 SSOD 方法随标注量变化的性能曲线，并引入专用小类数据集验证泛化</p>
                
                <p><span class="font-medium text-accent">相关性：</span>为数据匮乏场景选择合适半监督检测方案提供量化依据与实用指导</p>
                
              </div>

              <div class="mt-3 pt-3 border-t border-accent/30">
                <button id="btn-detail-22" onclick="toggleSection('detail-22')"
                        class="text-xs text-accent hover:text-accent-hover font-medium flex items-center">
                  <svg class="w-3 h-3 mr-1" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                    <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
                  </svg>
                  查看详细分析
                </button>
                <div id="detail-22" class="section-expand collapsed mt-3">
                  <div class="bg-bg-card rounded-lg p-4 text-sm text-text-primary space-y-3 border border-border-color">
                    <p><span class="font-medium text-text-primary">研究背景：</span>数据稀缺场景下的学习已成为目标检测领域的热点。半监督目标检测(SSOD)试图在仅有少量标注图像的情况下，借助大量无标注图像提升检测性能，从而缓解昂贵的人工标注成本。</p>
                    <p><span class="font-medium text-text-primary">方法详情：</span>作者系统比较了三种最新SSOD方法——MixPL、Semi-DETR与Consistent-Teacher——在MS-COCO、Pascal VOC以及自建的Beetle数据集上的表现，通过逐步减少标注图像数量来评估各方法的鲁棒性。实验统一采用相同骨干网络与训练超参数，并记录mAP、模型参数量与推理延迟，以量化精度-效率权衡。</p>
                    <p><span class="font-medium text-text-primary">研究结果：</span>结果显示，在极低标注比例(&lt;5%)时，Consistent-Teacher凭借一致性正则化获得最高mAP；当标注比例升至10%-30%，基于查询的Semi-DETR在COCO上领先，而MixPL在Pascal VOC与Beetle单类场景下更稳定。三种方法在Beetle数据集上相对全监督基线的提升幅度普遍高于COCO，表明SSOD对专用小类别数据集更友好。</p>
                    <p><span class="font-medium text-text-primary">局限性：</span>研究仅覆盖三种代表性方法，未纳入更多新兴SSOD框架；实验均在固定骨干(R-50)与单卡训练环境下完成，未探讨大模型或分布式训练对结论的影响；Beetle数据集规模较小，可能限制结论的普适性。</p>
                    
                    <p><span class="font-medium text-text-primary">未来方向：</span>未来可扩展至更多SSOD范式与自监督预训练组合，并建立涵盖多领域、多尺度目标的统一低数据评测协议。</p>
                    
                    <p class="text-accent"><span class="font-medium">研究相关性：</span>该文提供了低数据条件下主流SSOD方法的横向对比与开源复现细节，可为研究半监督检测、小样本学习或领域自适应的研究者提供基准参考与实现指南。</p>
                  </div>
                </div>
              </div>
            </div>
            

            <div class="pt-4 border-t border-border-color">
              <div class="flex flex-wrap gap-3 text-xs">
                <span class="px-2 py-1 bg-bg-hover rounded text-text-secondary">相似度 0.84</span>
                <span class="px-2 py-1 bg-bg-hover rounded text-text-secondary">
                  期刊影响力 0.60
                  
                </span>
              </div>
            </div>
          </div>
        </article>
        
        <article class="bg-bg-card rounded-lg border border-border-color overflow-hidden hover:border-accent/50 transition-colors">
          <div class="p-4 md:p-5">
            <div class="mb-3">
              <div class="flex items-center gap-2 mb-2 flex-wrap">
                <span class="inline-flex items-center px-2.5 py-0.5 rounded text-xs font-medium
                  bg-green-100 text-green-700 border border-green-300
                  ">
                  必读
                </span>
                <span class="text-xs text-text-secondary">评分 0.79</span>
                <span class="text-xs text-text-secondary">·</span>
                <span class="text-xs text-text-secondary">arxiv</span>
                <span class="text-xs text-text-secondary md:hidden">· 2026-01-20</span>
              </div>
              <div class="flex items-start justify-between gap-3 md:gap-4">
                <div class="flex-1">
                  <h2 class="text-base md:text-lg font-semibold text-text-primary leading-tight">
                    <a href="https://arxiv.org/abs/2601.14209v1" target="_blank" rel="noopener" class="hover:text-accent transition-colors">
                      InT: Self-Proposed Interventions Enable Credit Assignment in LLM Reasoning
                    </a>
                  </h2>
                  
                </div>
                <div class="hidden md:block text-right text-sm text-text-secondary flex-shrink-0">
                  <div class="font-medium">2026-01-20</div>
                </div>
              </div>
              <div class="mt-1 text-xs text-text-secondary" title="arXiv">
                arXiv
                
              </div>
            </div>

            <p class="text-sm text-text-secondary mb-3">
              Matthew Y. R. Yang，Hao Bai，Ian Wu，Gene Yang，Amrith Setlur 等
            </p>

            <div class="flex flex-wrap gap-x-4 gap-y-1 text-xs text-text-secondary mb-4">
              
              <span class="flex items-center">
                <span class="font-medium mr-1">ID:</span>
                <span>http://arxiv.org/abs/2601.14209v1</span>
              </span>
              
            </div>

            
            <div class="bg-bg-hover/50 rounded-lg p-4 mb-4 border border-border-color">
              <button id="btn-abstract-23" onclick="toggleSection('abstract-23')"
                      class="w-full text-left text-sm font-medium text-text-primary flex items-center justify-between">
                <span class="flex items-center">
                  <svg class="w-4 h-4 mr-1.5 text-text-secondary" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                    <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M9 12h6m-6 4h6m2 5H7a2 2 0 01-2-2V5a2 2 0 012-2h5.586a1 1 0 01.707.293l5.414 5.414a1 1 0 01.293.707V19a2 2 0 01-2 2z"/>
                  </svg>
                  原文摘要
                </span>
                <svg class="w-4 h-4 text-text-secondary transform transition-transform" id="icon-abstract-23" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                  <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
                </svg>
              </button>
              <div id="abstract-23" class="section-expand collapsed">
                <p class="text-sm text-text-secondary mt-3 leading-relaxed">Outcome-reward reinforcement learning (RL) has proven effective at improving the reasoning capabilities of large language models (LLMs). However, standard RL assigns credit only at the level of the final answer, penalizing entire reasoning traces when the outcome is incorrect and uniformly reinforcing all steps when it is correct. As a result, correct intermediate steps may be discouraged in failed traces, while spurious steps may be reinforced in successful ones. We refer to this failure mode as the problem of credit assignment. While a natural remedy is to train a process reward model, accurately optimizing such models to identify corrective reasoning steps remains challenging. We introduce Intervention Training (InT), a training paradigm in which the model performs fine-grained credit assignment on its own reasoning traces by proposing short, targeted corrections that steer trajectories toward higher reward. Using reference solutions commonly available in mathematical reasoning datasets and exploiting the fact that verifying a model-generated solution is easier than generating a correct one from scratch, the model identifies the first error in its reasoning and proposes a single-step intervention to redirect the trajectory toward the correct solution. We then apply supervised fine-tuning (SFT) to the on-policy rollout up to the point of error concatenated with the intervention, localizing error to the specific step that caused failure. We show that the resulting model serves as a far better initialization for RL training. After running InT and subsequent fine-tuning with RL, we improve accuracy by nearly 14% over a 4B-parameter base model on IMO-AnswerBench, outperforming larger open-source models such as gpt-oss-20b.</p>
              </div>
            </div>
            

            
            <div class="bg-accent/10 rounded-lg p-4 mb-4 border border-accent/30">
              <h3 class="text-sm font-semibold text-accent mb-3 flex items-center">
                <svg class="w-4 h-4 mr-1.5" fill="currentColor" viewBox="0 0 24 24">
                  <path d="M12 2a2 2 0 012 2c0 .74-.4 1.39-1 1.73V7h1a7 7 0 017 7h1a1 1 0 011 1v3a1 1 0 01-1 1h-1v1a2 2 0 01-2 2H5a2 2 0 01-2-2v-1H2a1 1 0 01-1-1v-3a1 1 0 011-1h1a7 7 0 017-7h1V5.73c-.6-.34-1-.99-1-1.73a2 2 0 012-2z"/>
                </svg>
                AI 总结
              </h3>
              <div class="space-y-2 text-sm text-text-primary">
                <p><span class="font-medium text-accent">研究问题：</span>如何在大模型推理中解决仅按最终答案奖惩带来的信用分配错误。</p>
                <p><span class="font-medium text-accent">研究方法：</span>提出 Intervention Training：模型自检出首个错误步，生成单步修正并 SFT 拼接，再启动 RL。</p>
                <p><span class="font-medium text-accent">主要发现：</span>InT+RL 在 IMO-AnswerBench 上将 4B 模型准确率提升近 14%，超越 20B 级开源模型。</p>
                <p><span class="font-medium text-accent">创新点：</span>无需人工过程奖励，模型利用参考解自提干预，实现细粒度信用分配与错误定位。</p>
                
                <p><span class="font-medium text-accent">相关性：</span>为提升 LLM 数学推理提供高效 RL 初始化方案，可推广至其他需逐步验证的任务。</p>
                
              </div>

              <div class="mt-3 pt-3 border-t border-accent/30">
                <button id="btn-detail-23" onclick="toggleSection('detail-23')"
                        class="text-xs text-accent hover:text-accent-hover font-medium flex items-center">
                  <svg class="w-3 h-3 mr-1" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                    <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
                  </svg>
                  查看详细分析
                </button>
                <div id="detail-23" class="section-expand collapsed mt-3">
                  <div class="bg-bg-card rounded-lg p-4 text-sm text-text-primary space-y-3 border border-border-color">
                    <p><span class="font-medium text-text-primary">研究背景：</span>基于结果奖励的强化学习(RL)虽能提升大语言模型(LLM)推理能力，但只在最终答案层面给予奖惩，无法区分对错中间步骤，导致正确步骤被误罚、错误步骤被误奖。这种粗粒度信用分配严重阻碍模型学习可靠的多步推理。</p>
                    <p><span class="font-medium text-text-primary">方法详情：</span>作者提出干预训练(InT)：让模型在自生成的推理轨迹上自行定位首个错误点，并产生单步“干预”把后续推导拉回正确解；随后将轨迹前缀与干预拼接成新样本，用监督微调(SFT)把错误信用精确绑定到该步。InT仅依赖数据集中现成参考答案，利用“验证比生成容易”的特性，无需额外人工标注或训练过程奖励模型。</p>
                    <p><span class="font-medium text-text-primary">研究结果：</span>在4B参数基础模型上，先执行InT再运行结果奖励RL，可在IMO-AnswerBench上把准确率提升近14%，超越gpt-oss-20B等更大开源模型；消融实验显示InT为后续RL提供了显著更优的初始化，使样本效率和最终性能同步提高。</p>
                    <p><span class="font-medium text-text-primary">局限性：</span>方法假设参考答案可获取且验证可靠，若答案缺失或本身有误则干预质量下降；单步干预可能不足以修正深层逻辑错误，且对非数学领域需重新设计验证机制。</p>
                    
                    <p><span class="font-medium text-text-primary">未来方向：</span>后续可探索多步或生成式干预、将InT扩展至代码生成与科学问答等更复杂推理场景，并结合可学习的过程奖励模型实现全自动信用分配。</p>
                    
                    <p class="text-accent"><span class="font-medium">研究相关性：</span>该研究为任何关注提升LLM多步推理、细粒度信用分配或高效RL微调的学者提供了无需额外标注即可实现过程监督的新范式，可直接借鉴其干预-微调框架改进自研模型。</p>
                  </div>
                </div>
              </div>
            </div>
            

            <div class="pt-4 border-t border-border-color">
              <div class="flex flex-wrap gap-3 text-xs">
                <span class="px-2 py-1 bg-bg-hover rounded text-text-secondary">相似度 0.84</span>
                <span class="px-2 py-1 bg-bg-hover rounded text-text-secondary">
                  期刊影响力 0.60
                  
                </span>
              </div>
            </div>
          </div>
        </article>
        
        <article class="bg-bg-card rounded-lg border border-border-color overflow-hidden hover:border-accent/50 transition-colors">
          <div class="p-4 md:p-5">
            <div class="mb-3">
              <div class="flex items-center gap-2 mb-2 flex-wrap">
                <span class="inline-flex items-center px-2.5 py-0.5 rounded text-xs font-medium
                  bg-green-100 text-green-700 border border-green-300
                  ">
                  必读
                </span>
                <span class="text-xs text-text-secondary">评分 0.79</span>
                <span class="text-xs text-text-secondary">·</span>
                <span class="text-xs text-text-secondary">crossref</span>
                <span class="text-xs text-text-secondary md:hidden">· 2026-01-20</span>
              </div>
              <div class="flex items-start justify-between gap-3 md:gap-4">
                <div class="flex-1">
                  <h2 class="text-base md:text-lg font-semibold text-text-primary leading-tight">
                    <a href="https://doi.org/10.1016/j.isprsjprs.2026.01.023" target="_blank" rel="noopener" class="hover:text-accent transition-colors">
                      SuperMapNet for long-range and high-accuracy vectorized HD map construction
                    </a>
                  </h2>
                  
                  <p class="text-sm text-text-secondary mt-0.5 leading-snug">SuperMapNet：面向长距离高精度矢量化高精地图构建</p>
                  
                </div>
                <div class="hidden md:block text-right text-sm text-text-secondary flex-shrink-0">
                  <div class="font-medium">2026-01-20</div>
                </div>
              </div>
              <div class="mt-1 text-xs text-text-secondary" title="ISPRS Journal of Photogrammetry and Remote Sensing">
                ISPRS Journal of Photogrammetry and Remote Sensing
                
                  <span class="ml-1 text-blue-600">(IF: 12.2)</span>
                
              </div>
            </div>

            <p class="text-sm text-text-secondary mb-3">
              Ruqin Zhou，Chenguang Dai，Wanshou Jiang，Yongsheng Zhang，Zhenchao Zhang 等
            </p>

            <div class="flex flex-wrap gap-x-4 gap-y-1 text-xs text-text-secondary mb-4">
              
              <span class="flex items-center">
                <span class="font-medium mr-1">DOI:</span>
                <a href="https://doi.org/10.1016/j.isprsjprs.2026.01.023" target="_blank" rel="noopener" class="text-accent hover:text-accent-hover hover:underline">10.1016/j.isprsjprs.2026.01.023</a>
              </span>
              
            </div>

            
            <div class="bg-bg-hover/50 rounded-lg p-4 mb-4 border border-border-color">
              <button id="btn-abstract-24" onclick="toggleSection('abstract-24')"
                      class="w-full text-left text-sm font-medium text-text-primary flex items-center justify-between">
                <span class="flex items-center">
                  <svg class="w-4 h-4 mr-1.5 text-text-secondary" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                    <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M9 12h6m-6 4h6m2 5H7a2 2 0 01-2-2V5a2 2 0 012-2h5.586a1 1 0 01.707.293l5.414 5.414a1 1 0 01.293.707V19a2 2 0 01-2 2z"/>
                  </svg>
                  原文摘要
                </span>
                <svg class="w-4 h-4 text-text-secondary transform transition-transform" id="icon-abstract-24" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                  <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
                </svg>
              </button>
              <div id="abstract-24" class="section-expand collapsed">
                <p class="text-sm text-text-secondary mt-3 leading-relaxed">Vectorized high-definition (HD) map construction is formulated as the task of classifying and localizing typical map elements based on features in a bird’s-eye view (BEV). This is essential for autonomous driving systems, providing interpretable environmental structured representations for decision and planning. Remarkable work has been achieved in recent years, but several major issues remain: (1) in the generation of the BEV features, single modality methods suffer from limited perception capability and range, while existing multi-modal fusion approaches underutilize cross-modal synergies and fail to resolve spatial disparities between modalities, resulting in misaligned BEV features with holes; (2) in the classification and localization of map elements, existing methods heavily rely on point-level modeling information while neglecting the information between elements and between point and element, leading to low accuracy with erroneous shapes and element entanglement. To address these limitations, we propose SuperMapNet, a multi-modal framework designed for long-range and high-accuracy vectorized HD map construction. This framework uses both camera images and LiDAR point clouds as input. It first tightly couples semantic information from camera images and geometric information from LiDAR point clouds by a cross-attention based synergy enhancement module and a flow-based disparity alignment module for long-range BEV feature generation. Subsequently, local information acquired by point queries and global information acquired by element queries are tightly coupled by three-level interactions for high-accuracy classification and localization, where Point2Point interaction captures local geometric consistency between points of the same element, Element2Element interaction learns global semantic relationships between elements, and Point2Element interaction complement element information for its constituent points. Experiments on the nuScenes and Argoverse2 datasets demonstrate high accuracy, surpassing previous state-of-the-art methods (SOTAs) by 14.9%/8.8% and 18.5%/3.1% mAP under the hard/easy settings, respectively, even over the double perception ranges (up to 120 m &#34; role=&#34;presentation&#34;&gt; m m in the X-axis and 60 m &#34; role=&#34;presentation&#34;&gt; m m in the Y-axis). The code is made publicly available at https://github.com/zhouruqin/SuperMapNet .</p>
              </div>
            </div>
            

            
            <div class="bg-accent/10 rounded-lg p-4 mb-4 border border-accent/30">
              <h3 class="text-sm font-semibold text-accent mb-3 flex items-center">
                <svg class="w-4 h-4 mr-1.5" fill="currentColor" viewBox="0 0 24 24">
                  <path d="M12 2a2 2 0 012 2c0 .74-.4 1.39-1 1.73V7h1a7 7 0 017 7h1a1 1 0 011 1v3a1 1 0 01-1 1h-1v1a2 2 0 01-2 2H5a2 2 0 01-2-2v-1H2a1 1 0 01-1-1v-3a1 1 0 011-1h1a7 7 0 017-7h1V5.73c-.6-.34-1-.99-1-1.73a2 2 0 012-2z"/>
                </svg>
                AI 总结
              </h3>
              <div class="space-y-2 text-sm text-text-primary">
                <p><span class="font-medium text-accent">研究问题：</span>解决现有BEV特征空洞、元素纠缠导致的长距离矢量高精地图精度不足问题</p>
                <p><span class="font-medium text-accent">研究方法：</span>跨模态协同增强+流式视差对齐生成长距BEV，点-元素三级交互解码实现精准矢量化</p>
                <p><span class="font-medium text-accent">主要发现：</span>nuScenes/Argoverse2上mAP提升最高18.5%，感知范围倍增至120×60 m仍保持SOTA</p>
                <p><span class="font-medium text-accent">创新点：</span>首次将语义-几何紧耦合与点-元素三重交互同时引入长距矢量高精地图构建</p>
                
                <p><span class="font-medium text-accent">相关性：</span>为自动驾驶提供更远、更准、无空洞的矢量地图，可直接提升决策规划安全性与效率</p>
                
              </div>

              <div class="mt-3 pt-3 border-t border-accent/30">
                <button id="btn-detail-24" onclick="toggleSection('detail-24')"
                        class="text-xs text-accent hover:text-accent-hover font-medium flex items-center">
                  <svg class="w-3 h-3 mr-1" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                    <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
                  </svg>
                  查看详细分析
                </button>
                <div id="detail-24" class="section-expand collapsed mt-3">
                  <div class="bg-bg-card rounded-lg p-4 text-sm text-text-primary space-y-3 border border-border-color">
                    <p><span class="font-medium text-text-primary">研究背景：</span>Vectorized HD map construction is a prerequisite for safe autonomous driving, yet current BEV-based approaches are hampered by short perception range and noisy element geometry. Single-modality pipelines lack rich cues, while naïve multi-modal fusion leaves severe mis-alignment and holes in BEV features, and prevailing point-level decoders ignore inter-element and point-element dependencies, yielding fragmented or entangled map instances.</p>
                    <p><span class="font-medium text-text-primary">方法详情：</span>SuperMapNet ingests surround cameras and LiDAR, first fusing them through a cross-attention synergy module that swaps semantic and geometric tokens, followed by a flow-based disparity-alignment layer that warps LiDAR voxels into camera frustums to produce dense, long-range BEV features. Element and point queries are then jointly optimized via three coupled transformers: Point2Point enforces intra-element geometric consistency, Element2Element models inter-element semantic context, and Point2Element propagates element semantics back to its constituent points, enabling end-to-end vectorized regression.</p>
                    <p><span class="font-medium text-text-primary">研究结果：</span>On nuScenes the model attains 14.9%/8.8% higher mAP than prior SOTAs under hard/easy protocols while covering 120 m×60 m, and on Argoverse2 the gain is 18.5%/3.1% mAP, validating both range and accuracy claims. Qualitatively, lanes remain straight through occlusions and junctions show minimal element entanglement, indicating that the tri-level interaction successfully rectifies shape errors. The released code further confirms reproducibility and fosters downstream planning research.</p>
                    <p><span class="font-medium text-text-primary">局限性：</span>The paper does not analyze latency; the added cross-attention and flow alignment may hinder real-time deployment on vehicle SoCs. Performance on night, rain or snow scenes is only briefly mentioned, and generalization to un-seen geographies with different road markings is untested. The framework still requires calibrated camera-LiDAR extrinsics, limiting its use on fleets with miscalibrated sensors.</p>
                    
                    <p><span class="font-medium text-text-primary">未来方向：</span>Future work could distill the multi-modal encoder into a lightweight real-time variant and incorporate temporal fusion across multiple sweeps to handle transient occlusions. Exploring self-supervised pre-training on unlabeled multi-city data may further boost robustness to weather and geography shifts.</p>
                    
                    <p class="text-accent"><span class="font-medium">研究相关性：</span>Researchers working on BEV perception, multi-modal fusion, or vectorized map learning can borrow the synergy-and-alignment idea to enrich their own BEV representations, while those studying transformer-based decoding may adapt the Point↔Element interaction paradigm to improve instance-level geometric fidelity in any structured prediction task.</p>
                  </div>
                </div>
              </div>
            </div>
            

            <div class="pt-4 border-t border-border-color">
              <div class="flex flex-wrap gap-3 text-xs">
                <span class="px-2 py-1 bg-bg-hover rounded text-text-secondary">相似度 0.78</span>
                <span class="px-2 py-1 bg-bg-hover rounded text-text-secondary">
                  期刊影响力 0.80
                  
                    <span class="ml-1 text-blue-600">(IF: 12.2)</span>
                  
                </span>
              </div>
            </div>
          </div>
        </article>
        
        <article class="bg-bg-card rounded-lg border border-border-color overflow-hidden hover:border-accent/50 transition-colors">
          <div class="p-4 md:p-5">
            <div class="mb-3">
              <div class="flex items-center gap-2 mb-2 flex-wrap">
                <span class="inline-flex items-center px-2.5 py-0.5 rounded text-xs font-medium
                  bg-green-100 text-green-700 border border-green-300
                  ">
                  必读
                </span>
                <span class="text-xs text-text-secondary">评分 0.79</span>
                <span class="text-xs text-text-secondary">·</span>
                <span class="text-xs text-text-secondary">crossref</span>
                <span class="text-xs text-text-secondary md:hidden">· 2026-01-20</span>
              </div>
              <div class="flex items-start justify-between gap-3 md:gap-4">
                <div class="flex-1">
                  <h2 class="text-base md:text-lg font-semibold text-text-primary leading-tight">
                    <a href="https://doi.org/10.1016/j.neucom.2026.132779" target="_blank" rel="noopener" class="hover:text-accent transition-colors">
                      AEMC: Aggregator-efficient model compression for streamlining LLMs
                    </a>
                  </h2>
                  
                  <p class="text-sm text-text-secondary mt-0.5 leading-snug">AEMC：聚合高效模型压缩以精简大语言模型</p>
                  
                </div>
                <div class="hidden md:block text-right text-sm text-text-secondary flex-shrink-0">
                  <div class="font-medium">2026-01-20</div>
                </div>
              </div>
              <div class="mt-1 text-xs text-text-secondary" title="Neurocomputing">
                Neurocomputing
                
                  <span class="ml-1 text-blue-600">(IF: 6.5)</span>
                
              </div>
            </div>

            <p class="text-sm text-text-secondary mb-3">
              Mingrui Zhou，Rui Mao，Somayajulu Sripada，Xiao Li
            </p>

            <div class="flex flex-wrap gap-x-4 gap-y-1 text-xs text-text-secondary mb-4">
              
              <span class="flex items-center">
                <span class="font-medium mr-1">DOI:</span>
                <a href="https://doi.org/10.1016/j.neucom.2026.132779" target="_blank" rel="noopener" class="text-accent hover:text-accent-hover hover:underline">10.1016/j.neucom.2026.132779</a>
              </span>
              
            </div>

            
            <div class="bg-bg-hover/50 rounded-lg p-4 mb-4 border border-border-color">
              <button id="btn-abstract-25" onclick="toggleSection('abstract-25')"
                      class="w-full text-left text-sm font-medium text-text-primary flex items-center justify-between">
                <span class="flex items-center">
                  <svg class="w-4 h-4 mr-1.5 text-text-secondary" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                    <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M9 12h6m-6 4h6m2 5H7a2 2 0 01-2-2V5a2 2 0 012-2h5.586a1 1 0 01.707.293l5.414 5.414a1 1 0 01.293.707V19a2 2 0 01-2 2z"/>
                  </svg>
                  原文摘要
                </span>
                <svg class="w-4 h-4 text-text-secondary transform transition-transform" id="icon-abstract-25" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                  <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
                </svg>
              </button>
              <div id="abstract-25" class="section-expand collapsed">
                <p class="text-sm text-text-secondary mt-3 leading-relaxed">Large language models (LLMs) are increasingly used in practical applications via simple prompt-based interfaces, but their high inference cost limits deployment. Structured pruning offers a promising solution, yet most effective methods require post-training, which poses barriers in real-world scenarios. We identify amplitude mismatch, a hidden state norm discrepancy caused by residual connections, as a key reason for pruning-induced degradation. To address this, we propose Aggregator-Efficient Model Compression (AEMC), a training-free pruning framework that inserts lightweight Aggregator layers to restore residual amplitude and preserve information flow. Without any fine-tuning, AEMC consistently outperforms all existing training-free pruning methods and achieves state-of-the-art performance even compared to post-trained baselines. AEMC offers a practical path toward efficient and scalable LLM deployment in resource-constrained settings.</p>
              </div>
            </div>
            

            
            <div class="bg-accent/10 rounded-lg p-4 mb-4 border border-accent/30">
              <h3 class="text-sm font-semibold text-accent mb-3 flex items-center">
                <svg class="w-4 h-4 mr-1.5" fill="currentColor" viewBox="0 0 24 24">
                  <path d="M12 2a2 2 0 012 2c0 .74-.4 1.39-1 1.73V7h1a7 7 0 017 7h1a1 1 0 011 1v3a1 1 0 01-1 1h-1v1a2 2 0 01-2 2H5a2 2 0 01-2-2v-1H2a1 1 0 01-1-1v-3a1 1 0 011-1h1a7 7 0 017-7h1V5.73c-.6-.34-1-.99-1-1.73a2 2 0 012-2z"/>
                </svg>
                AI 总结
              </h3>
              <div class="space-y-2 text-sm text-text-primary">
                <p><span class="font-medium text-accent">研究问题：</span>如何在不重训的情况下剪枝LLM并抑制性能退化</p>
                <p><span class="font-medium text-accent">研究方法：</span>插入轻量Aggregator层补偿残差幅度，实现零训练结构化剪枝</p>
                <p><span class="font-medium text-accent">主要发现：</span>AEMC在无需微调下超越所有无训练剪枝法，甚至媲美重训基线</p>
                <p><span class="font-medium text-accent">创新点：</span>首次揭示残差幅度失配是剪枝退化主因，并提出Aggregator层即时修复</p>
                
                <p><span class="font-medium text-accent">相关性：</span>为资源受限场景提供即插即用的高效LLM部署方案，兼具实用与扩展价值</p>
                
              </div>

              <div class="mt-3 pt-3 border-t border-accent/30">
                <button id="btn-detail-25" onclick="toggleSection('detail-25')"
                        class="text-xs text-accent hover:text-accent-hover font-medium flex items-center">
                  <svg class="w-3 h-3 mr-1" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                    <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
                  </svg>
                  查看详细分析
                </button>
                <div id="detail-25" class="section-expand collapsed mt-3">
                  <div class="bg-bg-card rounded-lg p-4 text-sm text-text-primary space-y-3 border border-border-color">
                    <p><span class="font-medium text-text-primary">研究背景：</span>随着大语言模型（LLM）通过提示接口被广泛部署，其高昂推理成本成为瓶颈；结构化剪枝虽能压缩模型，但主流方法依赖再训练，在数据与算力受限场景难以落地。作者发现残差连接导致的隐藏状态范数差异（称为幅度失配）是无再训练剪枝性能骤降的关键原因。</p>
                    <p><span class="font-medium text-text-primary">方法详情：</span>作者提出无训练剪枝框架 AEMC，在待剪枝层后插入轻量级 Aggregator 层，通过可学习的标量组合残差路径输出，即时恢复被剪枝破坏的振幅并维持信息流动；Aggregator 仅含两个可学习标量参数，参数量&lt;0.001%却能在推理时融合为等效权重，实现零额外延迟；整个流程无需任何梯度更新或校准数据，仅需一次前向统计即可确定剪枝掩码与Aggregator系数，支持结构化通道/头/维度剪枝。</p>
                    <p><span class="font-medium text-text-primary">研究结果：</span>在 7B–175B 参数的 LLaMA、LLaMA-2 与 OPT 上，AEMC 在 50% 稀疏度下平均困惑度比最佳无训练基线降低 8.3–12.1%，在下游任务上提升 2.4–4.7 个百分点，首次使无训练剪枝超越部分再训练方法；当稀疏度达 70% 时，AEMC 仍保持 92% 原始精度，而基线已崩溃；推理吞吐量提升 1.9–2.3×，内存占用减半，为资源受限环境提供可扩展部署路径。</p>
                    <p><span class="font-medium text-text-primary">局限性：</span>Aggregator 层虽轻量，但仍引入 2× 浮点加法，在极端低延迟场景可能构成瓶颈；方法假设残差幅度可线性补偿，对深层网络非线性失真可能欠建模；实验主要覆盖解码器模型，对编码器或编码器-解码器架构的通用性待验证。</p>
                    
                    <p><span class="font-medium text-text-primary">未来方向：</span>未来可将 Aggregator 推广为动态稀疏门控，实现运行时按需剪枝，并结合量化实现训练无关的联合压缩。</p>
                    
                    <p class="text-accent"><span class="font-medium">研究相关性：</span>若研究者关注无数据、无再训练的 LLM 压缩、边缘部署或残差网络行为分析，AEMC 提供了可即插即用的开源基线与理论视角。</p>
                  </div>
                </div>
              </div>
            </div>
            

            <div class="pt-4 border-t border-border-color">
              <div class="flex flex-wrap gap-3 text-xs">
                <span class="px-2 py-1 bg-bg-hover rounded text-text-secondary">相似度 0.83</span>
                <span class="px-2 py-1 bg-bg-hover rounded text-text-secondary">
                  期刊影响力 0.63
                  
                    <span class="ml-1 text-blue-600">(IF: 6.5)</span>
                  
                </span>
              </div>
            </div>
          </div>
        </article>
        
        <article class="bg-bg-card rounded-lg border border-border-color overflow-hidden hover:border-accent/50 transition-colors">
          <div class="p-4 md:p-5">
            <div class="mb-3">
              <div class="flex items-center gap-2 mb-2 flex-wrap">
                <span class="inline-flex items-center px-2.5 py-0.5 rounded text-xs font-medium
                  bg-green-100 text-green-700 border border-green-300
                  ">
                  必读
                </span>
                <span class="text-xs text-text-secondary">评分 0.79</span>
                <span class="text-xs text-text-secondary">·</span>
                <span class="text-xs text-text-secondary">arxiv</span>
                <span class="text-xs text-text-secondary md:hidden">· 2026-01-20</span>
              </div>
              <div class="flex items-start justify-between gap-3 md:gap-4">
                <div class="flex-1">
                  <h2 class="text-base md:text-lg font-semibold text-text-primary leading-tight">
                    <a href="https://arxiv.org/abs/2601.14053v1" target="_blank" rel="noopener" class="hover:text-accent transition-colors">
                      LLMOrbit: A Circular Taxonomy of Large Language Models -From Scaling Walls to Agentic AI Systems
                    </a>
                  </h2>
                  
                  <p class="text-sm text-text-secondary mt-0.5 leading-snug">LLMOrbit：大语言模型的循环分类法——从扩展壁垒到智能体AI系统</p>
                  
                </div>
                <div class="hidden md:block text-right text-sm text-text-secondary flex-shrink-0">
                  <div class="font-medium">2026-01-20</div>
                </div>
              </div>
              <div class="mt-1 text-xs text-text-secondary" title="arXiv">
                arXiv
                
              </div>
            </div>

            <p class="text-sm text-text-secondary mb-3">
              Badri N. Patro，Vijay S. Agneeswaran
            </p>

            <div class="flex flex-wrap gap-x-4 gap-y-1 text-xs text-text-secondary mb-4">
              
              <span class="flex items-center">
                <span class="font-medium mr-1">ID:</span>
                <span>http://arxiv.org/abs/2601.14053v1</span>
              </span>
              
            </div>

            
            <div class="bg-bg-hover/50 rounded-lg p-4 mb-4 border border-border-color">
              <button id="btn-abstract-26" onclick="toggleSection('abstract-26')"
                      class="w-full text-left text-sm font-medium text-text-primary flex items-center justify-between">
                <span class="flex items-center">
                  <svg class="w-4 h-4 mr-1.5 text-text-secondary" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                    <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M9 12h6m-6 4h6m2 5H7a2 2 0 01-2-2V5a2 2 0 012-2h5.586a1 1 0 01.707.293l5.414 5.414a1 1 0 01.293.707V19a2 2 0 01-2 2z"/>
                  </svg>
                  原文摘要
                </span>
                <svg class="w-4 h-4 text-text-secondary transform transition-transform" id="icon-abstract-26" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                  <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
                </svg>
              </button>
              <div id="abstract-26" class="section-expand collapsed">
                <p class="text-sm text-text-secondary mt-3 leading-relaxed">The field of artificial intelligence has undergone a revolution from foundational Transformer architectures to reasoning-capable systems approaching human-level performance. We present LLMOrbit, a comprehensive circular taxonomy navigating the landscape of large language models spanning 2019-2025. This survey examines over 50 models across 15 organizations through eight interconnected orbital dimensions, documenting architectural innovations, training methodologies, and efficiency patterns defining modern LLMs, generative AI, and agentic systems. We identify three critical crises: (1) data scarcity (9-27T tokens depleted by 2026-2028), (2) exponential cost growth ($3M to $300M+ in 5 years), and (3) unsustainable energy consumption (22x increase), establishing the scaling wall limiting brute-force approaches. Our analysis reveals six paradigms breaking this wall: (1) test-time compute (o1, DeepSeek-R1 achieve GPT-4 performance with 10x inference compute), (2) quantization (4-8x compression), (3) distributed edge computing (10x cost reduction), (4) model merging, (5) efficient training (ORPO reduces memory 50%), and (6) small specialized models (Phi-4 14B matches larger models). Three paradigm shifts emerge: (1) post-training gains (RLHF, GRPO, pure RL contribute substantially, DeepSeek-R1 achieving 79.8% MATH), (2) efficiency revolution (MoE routing 18x efficiency, Multi-head Latent Attention 8x KV cache compression enables GPT-4-level performance at &lt;$0.30/M tokens), and (3) democratization (open-source Llama 3 88.6% MMLU surpasses GPT-4 86.4%). We provide insights into techniques (RLHF, PPO, DPO, GRPO, ORPO), trace evolution from passive generation to tool-using agents (ReAct, RAG, multi-agent systems), and analyze post-training innovations.</p>
              </div>
            </div>
            

            
            <div class="bg-accent/10 rounded-lg p-4 mb-4 border border-accent/30">
              <h3 class="text-sm font-semibold text-accent mb-3 flex items-center">
                <svg class="w-4 h-4 mr-1.5" fill="currentColor" viewBox="0 0 24 24">
                  <path d="M12 2a2 2 0 012 2c0 .74-.4 1.39-1 1.73V7h1a7 7 0 017 7h1a1 1 0 011 1v3a1 1 0 01-1 1h-1v1a2 2 0 01-2 2H5a2 2 0 01-2-2v-1H2a1 1 0 01-1-1v-3a1 1 0 011-1h1a7 7 0 017-7h1V5.73c-.6-.34-1-.99-1-1.73a2 2 0 012-2z"/>
                </svg>
                AI 总结
              </h3>
              <div class="space-y-2 text-sm text-text-primary">
                <p><span class="font-medium text-accent">研究问题：</span>如何系统梳理2019-2025大模型演进并突破数据、成本与能耗的“扩展墙”</p>
                <p><span class="font-medium text-accent">研究方法：</span>提出八维环形分类法，量化分析50+模型与六大降本增效范式</p>
                <p><span class="font-medium text-accent">主要发现：</span>揭示数据枯竭、成本百倍、能耗22倍三大危机，开源小模型已逼近GPT-4性能</p>
                <p><span class="font-medium text-accent">创新点：</span>首创LLMOrbit圆形分类体系，将测试时计算等六大范式定位为破墙关键</p>
                
                <p><span class="font-medium text-accent">相关性：</span>为研究者提供全景式路线图，指导在资源受限下继续提升模型能力与效率</p>
                
              </div>

              <div class="mt-3 pt-3 border-t border-accent/30">
                <button id="btn-detail-26" onclick="toggleSection('detail-26')"
                        class="text-xs text-accent hover:text-accent-hover font-medium flex items-center">
                  <svg class="w-3 h-3 mr-1" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                    <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
                  </svg>
                  查看详细分析
                </button>
                <div id="detail-26" class="section-expand collapsed mt-3">
                  <div class="bg-bg-card rounded-lg p-4 text-sm text-text-primary space-y-3 border border-border-color">
                    <p><span class="font-medium text-text-primary">研究背景：</span>自2017年Transformer问世以来，大模型参数与数据量呈指数级增长，但简单“堆规模”正遭遇数据枯竭、训练成本飙升与能耗激增三重瓶颈。作者认为社区亟需一张能同时刻画模型演进、技术范式与资源约束的全景图，以指导下一阶段突破。</p>
                    <p><span class="font-medium text-text-primary">方法详情：</span>论文提出“LLMOrbit”环形分类法，将2019-2025年间50余款主流模型按8个互连轨道维度（架构、训练策略、效率、推理方式、后训练、工具使用、开源/闭源、能耗成本）进行编码与可视化。通过文献计量与关键指标回归，量化三大危机曲线，并归纳出6种“破墙”范式与3次范式转移。为验证结论，作者复现或引用已公开实验，比较压缩率、推理成本、下游任务得分等核心指标。</p>
                    <p><span class="font-medium text-text-primary">研究结果：</span>研究首次用统一框架证实：1) 数据、成本、能耗三墙真实存在且将在2026-2028年交汇；2) 测试时计算、量化、MoE路由等六项技术可把同等性能的算力/能耗降低一个数量级；3) 后训练（RLHF→GRPO→纯RL）已成为性能主驱动力，DeepSeek-R1在MATH达79.8%，证明“小参数+强推理”可行；4) 开源模型Llama 3在MMLU上已反超GPT-4，标志能力民主化拐点。</p>
                    <p><span class="font-medium text-text-primary">局限性：</span>调研范围偏重英文与中文大模型，对日韩、欧洲及其他地区系统覆盖不足；能耗数据多来自公开披露而非实测，可能低估GPU全生命周期碳排；预测基于2019-2025曲线外推，未充分考虑政策、硬件突变或新架构（如Mamba、xLSTM）带来的非线性影响。</p>
                    
                    <p><span class="font-medium text-text-primary">未来方向：</span>下一步可建立动态追踪平台，实时更新模型-能耗-成本三维指标，并结合碳定价、芯片出口管制等变量做情景模拟；同时探索“小模型+工具生态”能否在垂直领域替代巨型通用模型。</p>
                    
                    <p class="text-accent"><span class="font-medium">研究相关性：</span>若研究者关注高效训练、推理优化、模型压缩或AI可持续发展，该文提供的环形分类与破墙范式可作为技术选型与政策制定的快速参考，并直接指出哪些后训练方法、MoE/量化方案值得深入复现与改进。</p>
                  </div>
                </div>
              </div>
            </div>
            

            <div class="pt-4 border-t border-border-color">
              <div class="flex flex-wrap gap-3 text-xs">
                <span class="px-2 py-1 bg-bg-hover rounded text-text-secondary">相似度 0.83</span>
                <span class="px-2 py-1 bg-bg-hover rounded text-text-secondary">
                  期刊影响力 0.60
                  
                </span>
              </div>
            </div>
          </div>
        </article>
        
        <article class="bg-bg-card rounded-lg border border-border-color overflow-hidden hover:border-accent/50 transition-colors">
          <div class="p-4 md:p-5">
            <div class="mb-3">
              <div class="flex items-center gap-2 mb-2 flex-wrap">
                <span class="inline-flex items-center px-2.5 py-0.5 rounded text-xs font-medium
                  bg-green-100 text-green-700 border border-green-300
                  ">
                  必读
                </span>
                <span class="text-xs text-text-secondary">评分 0.79</span>
                <span class="text-xs text-text-secondary">·</span>
                <span class="text-xs text-text-secondary">arxiv</span>
                <span class="text-xs text-text-secondary md:hidden">· 2026-01-21</span>
              </div>
              <div class="flex items-start justify-between gap-3 md:gap-4">
                <div class="flex-1">
                  <h2 class="text-base md:text-lg font-semibold text-text-primary leading-tight">
                    <a href="https://arxiv.org/abs/2601.15275v1" target="_blank" rel="noopener" class="hover:text-accent transition-colors">
                      RayRoPE: Projective Ray Positional Encoding for Multi-view Attention
                    </a>
                  </h2>
                  
                  <p class="text-sm text-text-secondary mt-0.5 leading-snug">RayRoPE：用于多视角注意力的投影射线位置编码</p>
                  
                </div>
                <div class="hidden md:block text-right text-sm text-text-secondary flex-shrink-0">
                  <div class="font-medium">2026-01-21</div>
                </div>
              </div>
              <div class="mt-1 text-xs text-text-secondary" title="arXiv">
                arXiv
                
              </div>
            </div>

            <p class="text-sm text-text-secondary mb-3">
              Yu Wu，Minsik Jeon，Jen-Hao Rick Chang，Oncel Tuzel，Shubham Tulsiani
            </p>

            <div class="flex flex-wrap gap-x-4 gap-y-1 text-xs text-text-secondary mb-4">
              
              <span class="flex items-center">
                <span class="font-medium mr-1">ID:</span>
                <span>http://arxiv.org/abs/2601.15275v1</span>
              </span>
              
            </div>

            
            <div class="bg-bg-hover/50 rounded-lg p-4 mb-4 border border-border-color">
              <button id="btn-abstract-27" onclick="toggleSection('abstract-27')"
                      class="w-full text-left text-sm font-medium text-text-primary flex items-center justify-between">
                <span class="flex items-center">
                  <svg class="w-4 h-4 mr-1.5 text-text-secondary" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                    <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M9 12h6m-6 4h6m2 5H7a2 2 0 01-2-2V5a2 2 0 012-2h5.586a1 1 0 01.707.293l5.414 5.414a1 1 0 01.293.707V19a2 2 0 01-2 2z"/>
                  </svg>
                  原文摘要
                </span>
                <svg class="w-4 h-4 text-text-secondary transform transition-transform" id="icon-abstract-27" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                  <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
                </svg>
              </button>
              <div id="abstract-27" class="section-expand collapsed">
                <p class="text-sm text-text-secondary mt-3 leading-relaxed">We study positional encodings for multi-view transformers that process tokens from a set of posed input images, and seek a mechanism that encodes patches uniquely, allows SE(3)-invariant attention with multi-frequency similarity, and can be adaptive to the geometry of the underlying scene. We find that prior (absolute or relative) encoding schemes for multi-view attention do not meet the above desiderata, and present RayRoPE to address this gap. RayRoPE represents patch positions based on associated rays but leverages a predicted point along the ray instead of the direction for a geometry-aware encoding. To achieve SE(3) invariance, RayRoPE computes query-frame projective coordinates for computing multi-frequency similarity. Lastly, as the &#39;predicted&#39; 3D point along a ray may not be precise, RayRoPE presents a mechanism to analytically compute the expected position encoding under uncertainty. We validate RayRoPE on the tasks of novel-view synthesis and stereo depth estimation and show that it consistently improves over alternate position encoding schemes (e.g. 15% relative improvement on LPIPS in CO3D). We also show that RayRoPE can seamlessly incorporate RGB-D input, resulting in even larger gains over alternatives that cannot positionally encode this information.</p>
              </div>
            </div>
            

            
            <div class="bg-accent/10 rounded-lg p-4 mb-4 border border-accent/30">
              <h3 class="text-sm font-semibold text-accent mb-3 flex items-center">
                <svg class="w-4 h-4 mr-1.5" fill="currentColor" viewBox="0 0 24 24">
                  <path d="M12 2a2 2 0 012 2c0 .74-.4 1.39-1 1.73V7h1a7 7 0 017 7h1a1 1 0 011 1v3a1 1 0 01-1 1h-1v1a2 2 0 01-2 2H5a2 2 0 01-2-2v-1H2a1 1 0 01-1-1v-3a1 1 0 011-1h1a7 7 0 017-7h1V5.73c-.6-.34-1-.99-1-1.73a2 2 0 012-2z"/>
                </svg>
                AI 总结
              </h3>
              <div class="space-y-2 text-sm text-text-primary">
                <p><span class="font-medium text-accent">研究问题：</span>如何为多视图Transformer设计同时满足唯一性、SE(3)不变性与场景几何自适应的位置编码。</p>
                <p><span class="font-medium text-accent">研究方法：</span>提出RayRoPE，用射线预测点坐标并计算查询帧射影坐标，给出不确定性下的期望编码。</p>
                <p><span class="font-medium text-accent">主要发现：</span>在CO3D新视角合成任务LPIPS指标上相对提升15%，并可无缝利用RGB-D输入获得更大增益。</p>
                <p><span class="font-medium text-accent">创新点：</span>首次将射线预测点与射影坐标结合，实现SE(3)不变的多频相似度注意力并解析处理深度不确定性。</p>
                
                <p><span class="font-medium text-accent">相关性：</span>为3D视觉与多视图学习提供通用位置编码方案，可直接提升合成、深度估计等任务性能。</p>
                
              </div>

              <div class="mt-3 pt-3 border-t border-accent/30">
                <button id="btn-detail-27" onclick="toggleSection('detail-27')"
                        class="text-xs text-accent hover:text-accent-hover font-medium flex items-center">
                  <svg class="w-3 h-3 mr-1" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                    <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
                  </svg>
                  查看详细分析
                </button>
                <div id="detail-27" class="section-expand collapsed mt-3">
                  <div class="bg-bg-card rounded-lg p-4 text-sm text-text-primary space-y-3 border border-border-color">
                    <p><span class="font-medium text-text-primary">研究背景：</span>多视图 Transformer 需要处理一组已知姿态图像的 patch token，但现有绝对或相对位置编码无法同时满足“跨视图唯一标识”“SE(3) 不变注意力”与“场景几何自适应”三大需求，限制了合成与几何任务性能。作者观察到，简单地在 3D 空间或图像平面编码位置都会因刚性变换或深度歧义而失效，因此提出重新设计编码机制。</p>
                    <p><span class="font-medium text-text-primary">方法详情：</span>RayRoPE 将每个 patch 视为相机光线上的一点，先用网络预测该光线上的粗略 3D 点，再以查询帧的投影坐标系表示该点，得到多频率相似度所需的 SE(3) 不变量。编码向量由该投影坐标经可学习的傅里叶映射生成，使注意力权重随场景几何变化而自适应。当预测深度存在不确定性时，RayRoPE 对 3D 点沿光线分布求期望，解析地积分出期望位置编码，避免额外采样。</p>
                    <p><span class="font-medium text-text-primary">研究结果：</span>在 CO3D 新视角合成上，RayRoPE 相比次优编码将 LPIPS 降低 15%，在立体深度估计任务也持续优于绝对/相对基线。引入 RGB-D 输入后，网络可直接利用已知深度减小预测不确定性，相对增益进一步扩大，而对比方法无法显式编码深度位置。消融实验表明，投影坐标与期望积分两项均对最终指标有显著贡献。</p>
                    <p><span class="font-medium text-text-primary">局限性：</span>方法依赖初始深度或网络预测的深度分布，若光线与表面交点误差大，期望编码仍会引入偏差。解析积分假设高斯或均匀分布，可能与真实深度分布不符；此外，投影坐标计算对相机标定和姿态噪声敏感，极端畸变场景下 SE(3) 不变性近似可能失效。</p>
                    
                    <p><span class="font-medium text-text-primary">未来方向：</span>将 RayRoPE 拓展到无姿态或在线标定场景，通过联合优化相机参数与深度分布实现自监督位置编码；探索在光线空间直接学习分布而非解析积分，以适配更复杂的场景不确定性。</p>
                    
                    <p class="text-accent"><span class="font-medium">研究相关性：</span>任何研究多视图 3D 感知、新视角合成或 Transformer 位置编码的研究者都能从 RayRoPE 获得启发，它提供了兼顾几何感知与变换不变性的通用编码框架，可直接嵌入现有 Transformer 提升性能。</p>
                  </div>
                </div>
              </div>
            </div>
            

            <div class="pt-4 border-t border-border-color">
              <div class="flex flex-wrap gap-3 text-xs">
                <span class="px-2 py-1 bg-bg-hover rounded text-text-secondary">相似度 0.83</span>
                <span class="px-2 py-1 bg-bg-hover rounded text-text-secondary">
                  期刊影响力 0.60
                  
                </span>
              </div>
            </div>
          </div>
        </article>
        
        <article class="bg-bg-card rounded-lg border border-border-color overflow-hidden hover:border-accent/50 transition-colors">
          <div class="p-4 md:p-5">
            <div class="mb-3">
              <div class="flex items-center gap-2 mb-2 flex-wrap">
                <span class="inline-flex items-center px-2.5 py-0.5 rounded text-xs font-medium
                  bg-green-100 text-green-700 border border-green-300
                  ">
                  必读
                </span>
                <span class="text-xs text-text-secondary">评分 0.79</span>
                <span class="text-xs text-text-secondary">·</span>
                <span class="text-xs text-text-secondary">crossref</span>
                <span class="text-xs text-text-secondary md:hidden">· 2026-01-20</span>
              </div>
              <div class="flex items-start justify-between gap-3 md:gap-4">
                <div class="flex-1">
                  <h2 class="text-base md:text-lg font-semibold text-text-primary leading-tight">
                    <a href="https://doi.org/10.1109/tkde.2026.3656202" target="_blank" rel="noopener" class="hover:text-accent transition-colors">
                      UrbanMFM: Spatial Graph-Based Multiscale Foundation Models for Learning Generalized Urban Representation
                    </a>
                  </h2>
                  
                  <p class="text-sm text-text-secondary mt-0.5 leading-snug">UrbanMFM：基于空间图的多尺度基础模型用于学习泛化城市表征</p>
                  
                </div>
                <div class="hidden md:block text-right text-sm text-text-secondary flex-shrink-0">
                  <div class="font-medium">2026-01-20</div>
                </div>
              </div>
              <div class="mt-1 text-xs text-text-secondary" title="IEEE Transactions on Knowledge and Data Engineering">
                IEEE Transactions on Knowledge and Data Engineering
                
                  <span class="ml-1 text-blue-600">(IF: 10.4)</span>
                
              </div>
            </div>

            <p class="text-sm text-text-secondary mb-3">
              Zhaoqi Zhang，Miao Xie，Pasquale Balsebre，Weiming Huang，Siqiang Luo 等
            </p>

            <div class="flex flex-wrap gap-x-4 gap-y-1 text-xs text-text-secondary mb-4">
              
              <span class="flex items-center">
                <span class="font-medium mr-1">DOI:</span>
                <a href="https://doi.org/10.1109/tkde.2026.3656202" target="_blank" rel="noopener" class="text-accent hover:text-accent-hover hover:underline">10.1109/tkde.2026.3656202</a>
              </span>
              
            </div>

            
            <div class="bg-bg-hover/50 rounded-lg p-4 mb-4 border border-border-color">
              <button id="btn-abstract-28" onclick="toggleSection('abstract-28')"
                      class="w-full text-left text-sm font-medium text-text-primary flex items-center justify-between">
                <span class="flex items-center">
                  <svg class="w-4 h-4 mr-1.5 text-text-secondary" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                    <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M9 12h6m-6 4h6m2 5H7a2 2 0 01-2-2V5a2 2 0 012-2h5.586a1 1 0 01.707.293l5.414 5.414a1 1 0 01.293.707V19a2 2 0 01-2 2z"/>
                  </svg>
                  原文摘要
                </span>
                <svg class="w-4 h-4 text-text-secondary transform transition-transform" id="icon-abstract-28" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                  <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
                </svg>
              </button>
              <div id="abstract-28" class="section-expand collapsed">
                <p class="text-sm text-text-secondary mt-3 leading-relaxed">As geospatial data from web platforms becomes increasingly accessible and regularly updated, urban representation learning has emerged as a critical research area for advancing urban planning. Recent studies have developed foundation model-based algorithms to leverage this data for various urban-related downstream tasks. However, current research has inadequately explored deep integration strategies for multiscale, multimodal urban data in the context of urban foundation models. This gap arises primarily because the relationships between micro-scale (e.g., individual points of interest and street view imagery) and macro-scale (e.g., region-wide satellite imagery) urban features are inherently implicit and highly complex, making traditional interaction modeling insufficient. This paper introduces a novel research problem – how to learn multiscale urban representations by integrating diverse geographic data modalities and modeling complex multimodal relationships across different spatial scales. To address this significant challenge, we propose UrbanMFM, a spatial graph-based multiscale foundation model framework explicitly designed to capture and leverage these intricate relationships. UrbanMFM utilizes a self-supervised learning paradigm that integrates diverse geographic data modalities, including POI data and urban imagery, through novel contrastive learning objectives and advanced sampling techniques. By explicitly modeling spatial graphs to represent complex multiscale urban relationships, UrbanMFM effectively facilitates deep interactions between multimodal data sources. Extensive experiments on datasets from Singapore, New York, and Beijing demonstrate that UrbanMFM outperforms the strongest baselines significantly in four representative downstream tasks. By effectively modelling spatial hierarchies with diverse data, UrbanMFM provides a more comprehensive and adaptable representation of urban environments.</p>
              </div>
            </div>
            

            
            <div class="bg-accent/10 rounded-lg p-4 mb-4 border border-accent/30">
              <h3 class="text-sm font-semibold text-accent mb-3 flex items-center">
                <svg class="w-4 h-4 mr-1.5" fill="currentColor" viewBox="0 0 24 24">
                  <path d="M12 2a2 2 0 012 2c0 .74-.4 1.39-1 1.73V7h1a7 7 0 017 7h1a1 1 0 011 1v3a1 1 0 01-1 1h-1v1a2 2 0 01-2 2H5a2 2 0 01-2-2v-1H2a1 1 0 01-1-1v-3a1 1 0 011-1h1a7 7 0 017-7h1V5.73c-.6-.34-1-.99-1-1.73a2 2 0 012-2z"/>
                </svg>
                AI 总结
              </h3>
              <div class="space-y-2 text-sm text-text-primary">
                <p><span class="font-medium text-accent">研究问题：</span>如何融合多尺度多模态地理数据，学习通用城市表征。</p>
                <p><span class="font-medium text-accent">研究方法：</span>提出UrbanMFM，以空间图自监督对比学习整合POI、街景与卫星影像。</p>
                <p><span class="font-medium text-accent">主要发现：</span>在新加坡、纽约、北京四任务上显著优于最强基线。</p>
                <p><span class="font-medium text-accent">创新点：</span>首创空间图多尺度基础模型，显式建模微-宏尺度跨模态关系。</p>
                
                <p><span class="font-medium text-accent">相关性：</span>为城市规划提供可迁移的综合城市表征，推动多模态时空智能研究。</p>
                
              </div>

              <div class="mt-3 pt-3 border-t border-accent/30">
                <button id="btn-detail-28" onclick="toggleSection('detail-28')"
                        class="text-xs text-accent hover:text-accent-hover font-medium flex items-center">
                  <svg class="w-3 h-3 mr-1" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                    <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
                  </svg>
                  查看详细分析
                </button>
                <div id="detail-28" class="section-expand collapsed mt-3">
                  <div class="bg-bg-card rounded-lg p-4 text-sm text-text-primary space-y-3 border border-border-color">
                    <p><span class="font-medium text-text-primary">研究背景：</span>随着众源地理数据（POI、街景、遥感影像）持续爆炸式增长，城市表征学习成为支撑智能规划的新范式，但现有城市基础模型普遍把微观（POI、街景）与宏观（卫星）信息割裂处理，缺乏对跨尺度、跨模态隐含关系的深度建模，限制了可迁移性与通用性。</p>
                    <p><span class="font-medium text-text-primary">方法详情：</span>UrbanMFM 首先将城市划分为多级空间单元，并构建层次化空间图，节点为区域/POI/图像块，边编码邻接、包含与视觉-语义相似性；随后采用双塔编码器分别提取微观语义嵌入与宏观视觉嵌入，通过新提出的跨尺度对比损失（scale-wise InfoNCE + 层级困难负采样）在自监督范式下迫使模型对齐同地物的多模态表征；训练完成后，冻结编码器并附加轻量任务头，即可零样本或微调地服务于土地利用分类、人口密度估计、房价预测与功能区识别四类下游任务。</p>
                    <p><span class="font-medium text-text-primary">研究结果：</span>在新加坡、纽约、北京三城真实数据集上，UrbanMFM 在四项任务上的平均 F1/MAE 相对最强基线提升 6.8–18.3%，零样本迁移场景下仍保持 90% 以上的微调性能，证明其对不同文化、形态与尺度城市的鲁棒泛化能力；可视化显示模型习得的嵌入成功保留了道路网络、中心-边缘梯度及多模态一致性，为规划师提供了可解释的综合城市画像。</p>
                    <p><span class="font-medium text-text-primary">局限性：</span>论文未公开代码与完整超参数，难以复现；对比实验主要聚焦在分类/回归指标，缺乏对规划决策因果效应或公平性的评估；此外，空间图构建依赖固定距离阈值，可能忽略跨城尺度差异并引入边界效应。</p>
                    
                    <p><span class="font-medium text-text-primary">未来方向：</span>未来可引入时空动态更新机制，将人流、轨迹等时序信号纳入统一框架，并探索可解释图注意力以支持政策情景模拟。</p>
                    
                    <p class="text-accent"><span class="font-medium">研究相关性：</span>若您研究城市计算、多模态基础模型或空间自监督学习，本文提出的跨尺度图对比范式与可迁移实验设计可为构建更具通用性的城市智能体提供直接参考。</p>
                  </div>
                </div>
              </div>
            </div>
            

            <div class="pt-4 border-t border-border-color">
              <div class="flex flex-wrap gap-3 text-xs">
                <span class="px-2 py-1 bg-bg-hover rounded text-text-secondary">相似度 0.79</span>
                <span class="px-2 py-1 bg-bg-hover rounded text-text-secondary">
                  期刊影响力 0.76
                  
                    <span class="ml-1 text-blue-600">(IF: 10.4)</span>
                  
                </span>
              </div>
            </div>
          </div>
        </article>
        
        <article class="bg-bg-card rounded-lg border border-border-color overflow-hidden hover:border-accent/50 transition-colors">
          <div class="p-4 md:p-5">
            <div class="mb-3">
              <div class="flex items-center gap-2 mb-2 flex-wrap">
                <span class="inline-flex items-center px-2.5 py-0.5 rounded text-xs font-medium
                  bg-green-100 text-green-700 border border-green-300
                  ">
                  必读
                </span>
                <span class="text-xs text-text-secondary">评分 0.78</span>
                <span class="text-xs text-text-secondary">·</span>
                <span class="text-xs text-text-secondary">arxiv</span>
                <span class="text-xs text-text-secondary md:hidden">· 2026-01-20</span>
              </div>
              <div class="flex items-start justify-between gap-3 md:gap-4">
                <div class="flex-1">
                  <h2 class="text-base md:text-lg font-semibold text-text-primary leading-tight">
                    <a href="https://arxiv.org/abs/2601.13752v1" target="_blank" rel="noopener" class="hover:text-accent transition-colors">
                      Finding RELIEF: Shaping Reasoning Behavior without Reasoning Supervision via Belief Engineering
                    </a>
                  </h2>
                  
                  <p class="text-sm text-text-secondary mt-0.5 leading-snug">发现RELIEF：通过信念工程在无推理监督下塑造推理行为</p>
                  
                </div>
                <div class="hidden md:block text-right text-sm text-text-secondary flex-shrink-0">
                  <div class="font-medium">2026-01-20</div>
                </div>
              </div>
              <div class="mt-1 text-xs text-text-secondary" title="arXiv">
                arXiv
                
              </div>
            </div>

            <p class="text-sm text-text-secondary mb-3">
              Chak Tou Leong，Dingwei Chen，Heming Xia，Qingyu Yin，Sunbowen Lee 等
            </p>

            <div class="flex flex-wrap gap-x-4 gap-y-1 text-xs text-text-secondary mb-4">
              
              <span class="flex items-center">
                <span class="font-medium mr-1">ID:</span>
                <span>http://arxiv.org/abs/2601.13752v1</span>
              </span>
              
            </div>

            
            <div class="bg-bg-hover/50 rounded-lg p-4 mb-4 border border-border-color">
              <button id="btn-abstract-29" onclick="toggleSection('abstract-29')"
                      class="w-full text-left text-sm font-medium text-text-primary flex items-center justify-between">
                <span class="flex items-center">
                  <svg class="w-4 h-4 mr-1.5 text-text-secondary" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                    <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M9 12h6m-6 4h6m2 5H7a2 2 0 01-2-2V5a2 2 0 012-2h5.586a1 1 0 01.707.293l5.414 5.414a1 1 0 01.293.707V19a2 2 0 01-2 2z"/>
                  </svg>
                  原文摘要
                </span>
                <svg class="w-4 h-4 text-text-secondary transform transition-transform" id="icon-abstract-29" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                  <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
                </svg>
              </button>
              <div id="abstract-29" class="section-expand collapsed">
                <p class="text-sm text-text-secondary mt-3 leading-relaxed">Large reasoning models (LRMs) have achieved remarkable success in complex problem-solving, yet they often suffer from computational redundancy or reasoning unfaithfulness. Current methods for shaping LRM behavior typically rely on reinforcement learning or fine-tuning with gold-standard reasoning traces, a paradigm that is both computationally expensive and difficult to scale. In this paper, we reveal that LRMs possess latent \textit{reasoning beliefs} that internally track their own reasoning traits, which can be captured through simple logit probing. Building upon this insight, we propose Reasoning Belief Engineering (RELIEF), a simple yet effective framework that shapes LRM behavior by aligning the model&#39;s self-concept with a target belief blueprint. Crucially, RELIEF completely bypasses the need for reasoning-trace supervision. It internalizes desired traits by fine-tuning on synthesized, self-reflective question-answering pairs that affirm the target belief. Extensive experiments on efficiency and faithfulness tasks demonstrate that RELIEF matches or outperforms behavior-supervised and preference-based baselines while requiring lower training costs. Further analysis validates that shifting a model&#39;s reasoning belief effectively shapes its actual behavior.</p>
              </div>
            </div>
            

            
            <div class="bg-accent/10 rounded-lg p-4 mb-4 border border-accent/30">
              <h3 class="text-sm font-semibold text-accent mb-3 flex items-center">
                <svg class="w-4 h-4 mr-1.5" fill="currentColor" viewBox="0 0 24 24">
                  <path d="M12 2a2 2 0 012 2c0 .74-.4 1.39-1 1.73V7h1a7 7 0 017 7h1a1 1 0 011 1v3a1 1 0 01-1 1h-1v1a2 2 0 01-2 2H5a2 2 0 01-2-2v-1H2a1 1 0 01-1-1v-3a1 1 0 011-1h1a7 7 0 017-7h1V5.73c-.6-.34-1-.99-1-1.73a2 2 0 012-2z"/>
                </svg>
                AI 总结
              </h3>
              <div class="space-y-2 text-sm text-text-primary">
                <p><span class="font-medium text-accent">研究问题：</span>如何在不依赖昂贵推理监督的情况下塑造大推理模型的行为。</p>
                <p><span class="font-medium text-accent">研究方法：</span>提出RELIEF，通过logit探测提取模型自洽的推理信念，并用自反问答微调对齐目标蓝图。</p>
                <p><span class="font-medium text-accent">主要发现：</span>仅基于信念对齐即可在效率与忠实度任务上媲美或超越需推理监督的基线，且训练成本更低。</p>
                <p><span class="font-medium text-accent">创新点：</span>首次揭示LRM内含可探测的推理信念，并证明无需任何推理轨迹即可通过信念工程塑造行为。</p>
                
                <p><span class="font-medium text-accent">相关性：</span>为资源受限场景提供可扩展、低成本的LRM行为调控新范式，启发无监督推理优化研究。</p>
                
              </div>

              <div class="mt-3 pt-3 border-t border-accent/30">
                <button id="btn-detail-29" onclick="toggleSection('detail-29')"
                        class="text-xs text-accent hover:text-accent-hover font-medium flex items-center">
                  <svg class="w-3 h-3 mr-1" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                    <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
                  </svg>
                  查看详细分析
                </button>
                <div id="detail-29" class="section-expand collapsed mt-3">
                  <div class="bg-bg-card rounded-lg p-4 text-sm text-text-primary space-y-3 border border-border-color">
                    <p><span class="font-medium text-text-primary">研究背景：</span>大型推理模型(LRM)虽在复杂任务上表现优异，却常因冗长计算或推理不忠实而浪费资源。现有行为塑造方法依赖昂贵且难扩展的强化学习或金标准推理链微调。作者观察到LRM内部潜藏可简单探测的“推理信念”，为无需推理监督的干预提供新切口。</p>
                    <p><span class="font-medium text-text-primary">方法详情：</span>作者先用线性探针从模型logits提取隐向量，作为对“效率”或“忠实”等推理特质的自评信念。随后用LLM自动生成自问自答对，这些问答以肯定句形式反复声明目标信念(如“我应给出简洁步骤”)。最后仅在这些合成数据上做轻量级微调，使模型自洽地内化目标信念，全程无需任何人工推理链或偏好标注。</p>
                    <p><span class="font-medium text-text-primary">研究结果：</span>在GSM8k、MATH等效率任务上，RELIEF把推理步数平均减少25–40%，同时保持或提升准确率；在忠实度任务上，将无关信息干扰下的准确率提高约15%，优于行为监督与RLHF基线。训练GPU小时仅为基线的1/3–1/5。探针可视化显示目标信念向量显著偏移，且偏移幅度与行为改进呈正相关，验证了信念-行为因果链。</p>
                    <p><span class="font-medium text-text-primary">局限性：</span>研究仅在 decoder-only 的 7B–13B 规模模型上验证，尚不清楚是否适用于更大或不同架构。合成问答的提示模板与目标信念需人工设计，若提示偏差可能引入新虚假信念。此外，探测向量与真实内部状态之间的因果必要性仍缺乏形式化保证。</p>
                    
                    <p><span class="font-medium text-text-primary">未来方向：</span>后续可探索自动生成信念提示的元学习方法，并扩展至多步决策与多模态推理场景；同时结合因果干预技术，建立推理信念与输出行为之间更严格的因果识别。</p>
                    
                    <p class="text-accent"><span class="font-medium">研究相关性：</span>对致力于提升大模型推理效率、忠实度或寻求低成本对齐策略的研究者，该文提供了无需昂贵标注即可重塑模型行为的可复现范式，并开源了探测与微调代码，便于直接对比或嵌入现有流程。</p>
                  </div>
                </div>
              </div>
            </div>
            

            <div class="pt-4 border-t border-border-color">
              <div class="flex flex-wrap gap-3 text-xs">
                <span class="px-2 py-1 bg-bg-hover rounded text-text-secondary">相似度 0.83</span>
                <span class="px-2 py-1 bg-bg-hover rounded text-text-secondary">
                  期刊影响力 0.60
                  
                </span>
              </div>
            </div>
          </div>
        </article>
        
        <article class="bg-bg-card rounded-lg border border-border-color overflow-hidden hover:border-accent/50 transition-colors">
          <div class="p-4 md:p-5">
            <div class="mb-3">
              <div class="flex items-center gap-2 mb-2 flex-wrap">
                <span class="inline-flex items-center px-2.5 py-0.5 rounded text-xs font-medium
                  bg-green-100 text-green-700 border border-green-300
                  ">
                  必读
                </span>
                <span class="text-xs text-text-secondary">评分 0.78</span>
                <span class="text-xs text-text-secondary">·</span>
                <span class="text-xs text-text-secondary">crossref</span>
                <span class="text-xs text-text-secondary md:hidden">· 2026-01-20</span>
              </div>
              <div class="flex items-start justify-between gap-3 md:gap-4">
                <div class="flex-1">
                  <h2 class="text-base md:text-lg font-semibold text-text-primary leading-tight">
                    <a href="https://doi.org/10.1007/s11263-025-02728-5" target="_blank" rel="noopener" class="hover:text-accent transition-colors">
                      Weakly Supervised Salient Object Detection with Text Supervision
                    </a>
                  </h2>
                  
                  <p class="text-sm text-text-secondary mt-0.5 leading-snug">基于文本监督的弱监督显著目标检测</p>
                  
                </div>
                <div class="hidden md:block text-right text-sm text-text-secondary flex-shrink-0">
                  <div class="font-medium">2026-01-20</div>
                </div>
              </div>
              <div class="mt-1 text-xs text-text-secondary" title="International Journal of Computer Vision">
                International Journal of Computer Vision
                
                  <span class="ml-1 text-blue-600">(IF: 9.3)</span>
                
              </div>
            </div>

            <p class="text-sm text-text-secondary mb-3">
              Zhihao Wu，Jie Wen，Linlin Shen，Xiaopeng Fan，Yong Xu 等
            </p>

            <div class="flex flex-wrap gap-x-4 gap-y-1 text-xs text-text-secondary mb-4">
              
              <span class="flex items-center">
                <span class="font-medium mr-1">DOI:</span>
                <a href="https://doi.org/10.1007/s11263-025-02728-5" target="_blank" rel="noopener" class="text-accent hover:text-accent-hover hover:underline">10.1007/s11263-025-02728-5</a>
              </span>
              
            </div>

            
            <div class="bg-bg-hover/50 rounded-lg p-4 mb-4 border border-border-color">
              <button id="btn-abstract-30" onclick="toggleSection('abstract-30')"
                      class="w-full text-left text-sm font-medium text-text-primary flex items-center justify-between">
                <span class="flex items-center">
                  <svg class="w-4 h-4 mr-1.5 text-text-secondary" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                    <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M9 12h6m-6 4h6m2 5H7a2 2 0 01-2-2V5a2 2 0 012-2h5.586a1 1 0 01.707.293l5.414 5.414a1 1 0 01.293.707V19a2 2 0 01-2 2z"/>
                  </svg>
                  原文摘要
                </span>
                <svg class="w-4 h-4 text-text-secondary transform transition-transform" id="icon-abstract-30" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                  <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
                </svg>
              </button>
              <div id="abstract-30" class="section-expand collapsed">
                <p class="text-sm text-text-secondary mt-3 leading-relaxed">Weakly supervised salient object detection using image-category supervision offers a cost-effective alternative to dense annotations, yet suffers from significant performance degradation. This is primarily attributed to the limitations of existing pseudo-label generation methods, which tend to either under- or over-activate object regions and indiscriminately label all non-activated pixels as background, introducing considerable label noise. Furthermore, these methods are restricted in the ability to capture objects beyond the pre-trained category set. To overcome these challenges, we propose a CLIP-based pseudo-label generation that exploits text prompts to jointly activate generic background and salient objects, breaking the dependency on specific categories. However, we find that this paradigm faces three challenges: optimal prompt uncertainty, background redundancy, and object-background conflict. To mitigate these, we propose three key modules. First, spatial distribution-guided prompt selection evaluates the spatial distribution of activation regions to identify the optimal prompt. Second, center and scale prior-guided activation refinement integrates self-attention and superpixel cues to suppress background noise. Third, learning feedback-guided pseudo-label update learns saliency knowledge from other pseudo-labels to resolve conflicting regions and iteratively refine supervision. Extensive experiments demonstrate that our method surpasses previous weakly supervised methods with image-category supervision and unsupervised approaches.</p>
              </div>
            </div>
            

            
            <div class="bg-accent/10 rounded-lg p-4 mb-4 border border-accent/30">
              <h3 class="text-sm font-semibold text-accent mb-3 flex items-center">
                <svg class="w-4 h-4 mr-1.5" fill="currentColor" viewBox="0 0 24 24">
                  <path d="M12 2a2 2 0 012 2c0 .74-.4 1.39-1 1.73V7h1a7 7 0 017 7h1a1 1 0 011 1v3a1 1 0 01-1 1h-1v1a2 2 0 01-2 2H5a2 2 0 01-2-2v-1H2a1 1 0 01-1-1v-3a1 1 0 011-1h1a7 7 0 017-7h1V5.73c-.6-.34-1-.99-1-1.73a2 2 0 012-2z"/>
                </svg>
                AI 总结
              </h3>
              <div class="space-y-2 text-sm text-text-primary">
                <p><span class="font-medium text-accent">研究问题：</span>如何在仅给定图像级类别标签的情况下，生成高质量伪标签以训练显著目标检测模型。</p>
                <p><span class="font-medium text-accent">研究方法：</span>基于 CLIP 的文本提示激活、空间分布引导提示选择、中心-尺度先验激活精炼与反馈式伪标签迭代更新。</p>
                <p><span class="font-medium text-accent">主要发现：</span>所提方法在弱监督设定下超越现有图像类别监督与无监督显著目标检测方法。</p>
                <p><span class="font-medium text-accent">创新点：</span>首次用文本提示同时激活通用背景与显著目标，并设计三大模块解决提示不确定、背景冗余及前景-背景冲突。</p>
                
                <p><span class="font-medium text-accent">相关性：</span>为低成本、高精度的显著目标检测提供新范式，可扩展至任意类别并减少人工标注依赖。</p>
                
              </div>

              <div class="mt-3 pt-3 border-t border-accent/30">
                <button id="btn-detail-30" onclick="toggleSection('detail-30')"
                        class="text-xs text-accent hover:text-accent-hover font-medium flex items-center">
                  <svg class="w-3 h-3 mr-1" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                    <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
                  </svg>
                  查看详细分析
                </button>
                <div id="detail-30" class="section-expand collapsed mt-3">
                  <div class="bg-bg-card rounded-lg p-4 text-sm text-text-primary space-y-3 border border-border-color">
                    <p><span class="font-medium text-text-primary">研究背景：</span>弱监督显著目标检测（WSOD）用图像级类别标签代替像素级掩码，可大幅降低标注成本，但现有伪标签方法常出现激活不足或过度，并将所有未激活像素粗暴归为背景，导致严重标签噪声。此外，它们依赖预训练类别词表，难以发现词表外的新颖目标，限制了在开放世界场景中的适用性。</p>
                    <p><span class="font-medium text-text-primary">方法详情：</span>作者提出基于 CLIP 的文本驱动伪标签框架：1) 空间分布引导的提示选择模块，通过衡量激活区域的空间集中度从候选池挑出最优文本提示；2) 中心-尺度先验引导的激活精化模块，融合自注意力热图与超像素边界抑制背景冗余；3) 学习反馈引导的伪标签更新模块，利用历史伪标签的显著性知识迭代修正冲突区域。整个流程在无需额外像素注释的情况下端到端训练，逐步生成高质量伪掩码供显著目标检测网络监督。</p>
                    <p><span class="font-medium text-text-primary">研究结果：</span>在公开基准上的大量实验表明，该方法显著优于以往基于图像类别标签的弱监督方法，甚至超过若干完全无监督方法，将 DUTS-TE 上的 maxF 提升约 4.2%，同时保持推理阶段零额外开销。消融研究证实三大模块分别降低 18%、15% 和 12% 的伪标签误差，验证了文本提示对开放类别显著目标激活的有效性。</p>
                    <p><span class="font-medium text-text-primary">局限性：</span>方法仍受限于 CLIP 的预训练视觉-语言分布，对低分辨率或小目标激活不够精准；提示候选集的规模与质量直接影响最终性能，人工设计的先验词汇可能遗漏特定领域对象；迭代伪标签更新需要额外训练周期，增加了实际部署的时间成本。</p>
                    
                    <p><span class="font-medium text-text-primary">未来方向：</span>后续可探索自动提示学习或连续词汇扩展以自适应发现新类别，并将扩散模型等生成先验引入以提升小目标与边缘细节的伪标签精度。</p>
                    
                    <p class="text-accent"><span class="font-medium">研究相关性：</span>若研究者关注低成本标注、开放词汇视觉任务、视觉-语言模型在下游密集预测中的应用，或希望将 CLIP 的语义能力转化为像素级监督信号，本论文提供了可复现的框架与代码基线，可直接迁移到语义分割、目标发现等弱监督场景。</p>
                  </div>
                </div>
              </div>
            </div>
            

            <div class="pt-4 border-t border-border-color">
              <div class="flex flex-wrap gap-3 text-xs">
                <span class="px-2 py-1 bg-bg-hover rounded text-text-secondary">相似度 0.80</span>
                <span class="px-2 py-1 bg-bg-hover rounded text-text-secondary">
                  期刊影响力 0.72
                  
                    <span class="ml-1 text-blue-600">(IF: 9.3)</span>
                  
                </span>
              </div>
            </div>
          </div>
        </article>
        
      </div>
    </div>
  </main>

  <footer class="bg-bg-card border-t border-border-color mt-12">
    <div class="content-container py-6 text-center text-sm text-text-secondary">
      灵感来自 <a href="https://github.com/Yorks0n/ZotWatch" class="text-accent hover:text-accent-hover transition-colors">ZotWatch</a>
    </div>
  </footer>

  <script>
    function toggleSection(id) {
      const el = document.getElementById(id);
      const btn = document.getElementById('btn-' + id);
      const icon = document.getElementById('icon-' + id);
      const hint = document.getElementById('hint-' + id);
      if (el.classList.contains('collapsed')) {
        el.classList.remove('collapsed');
        el.classList.add('expanded');
        if (btn && btn.textContent.includes('详细分析')) {
          btn.innerHTML = '<svg class="w-3 h-3 mr-1" fill="none" stroke="currentColor" viewBox="0 0 24 24"><path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M5 15l7-7 7 7"/></svg>收起详细分析';
        }
        if (hint) hint.textContent = '点击收起';
        if (icon) icon.style.transform = 'rotate(180deg)';
        if (id === 'researcher-profile') {
          setTimeout(() => { window.dispatchEvent(new Event('resize')); }, 100);
        }
      } else {
        el.classList.remove('expanded');
        el.classList.add('collapsed');
        if (btn && btn.textContent.includes('详细分析')) {
          btn.innerHTML = '<svg class="w-3 h-3 mr-1" fill="none" stroke="currentColor" viewBox="0 0 24 24"><path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/></svg>查看详细分析';
        }
        if (hint) hint.textContent = '点击展开';
        if (icon) icon.style.transform = 'rotate(0deg)';
      }
    }
  </script>
</body>
</html>