<!DOCTYPE html>
<html lang="zh-CN">
<head>
  <meta charset="utf-8">
  <meta name="viewport" content="width=device-width, initial-scale=1">
  <title>ZotWatch 文献推荐 - 2026-01-03</title>
  <script src="https://cdn.tailwindcss.com"></script>
  <script src="https://cdn.jsdelivr.net/npm/chart.js@4.4.1/dist/chart.umd.min.js"></script>
  <script src="https://d3js.org/d3.v7.min.js"></script>
  <script>
    tailwind.config = {
      theme: {
        extend: {
          colors: {
            'bg-primary': '#f8fafc',
            'bg-card': '#ffffff',
            'bg-hover': '#f1f5f9',
            'text-primary': '#1e293b',
            'text-secondary': '#64748b',
            'border-color': '#e2e8f0',
            'accent': '#2563eb',
            'accent-hover': '#1d4ed8',
          }
        }
      }
    }
  </script>
  <style>
    .section-expand { transition: max-height 0.5s ease-out; overflow: hidden; }
    .section-expand.collapsed { max-height: 0; }
    .section-expand.expanded { max-height: none; }
    body { font-family: -apple-system, BlinkMacSystemFont, 'Segoe UI', Roboto, 'Helvetica Neue', Arial, sans-serif; }
    .content-container { max-width: 48rem; margin: 0 auto; padding: 0 1rem; }
    @media (min-width: 768px) { .content-container { padding: 0 2rem; } }
    @media (min-width: 1024px) { .content-container { padding: 0 4rem; } }

    .cluster-graph-container { width: 100%; height: 280px; touch-action: pan-y; }
    @media (min-width: 640px) { .cluster-graph-container { height: 320px; } }
    @media (min-width: 768px) { .cluster-graph-container { height: 350px; } }
    #cluster-graph svg { overflow: visible; }
  </style>
</head>
<body class="bg-bg-primary min-h-screen text-text-primary">
  <header class="bg-bg-card border-b border-border-color">
    <div class="content-container py-6 md:py-8">
      <h1 class="text-xl md:text-2xl font-bold text-text-primary">ZotWatch 文献推荐</h1>
      <p class="text-xs md:text-sm text-text-secondary mt-1">
        共 35 篇论文 ·
        生成于 2026-01-03 10:37 Asia/Shanghai
      </p>
    </div>
  </header>

  
  <section class="py-4 md:py-6 border-b border-border-color">
    <div class="content-container">
      <button id="btn-researcher-profile" onclick="toggleSection('researcher-profile')"
              class="w-full text-left text-lg md:text-xl font-bold text-text-primary mb-3 md:mb-4 flex items-center justify-between group hover:text-accent transition-colors">
        <span class="flex items-center gap-2 flex-1 min-w-0">
          <svg class="w-5 h-5 flex-shrink-0 text-accent" fill="currentColor" viewBox="0 0 24 24">
            <path d="M12 2C6.48 2 2 6.48 2 12s4.48 10 10 10 10-4.48 10-10S17.52 2 12 2zm0 3c1.66 0 3 1.34 3 3s-1.34 3-3 3-3-1.34-3-3 1.34-3 3-3zm0 14.2c-2.5 0-4.71-1.28-6-3.22.03-1.99 4-3.08 6-3.08 1.99 0 5.97 1.09 6 3.08-1.29 1.94-3.5 3.22-6 3.22z"/>
          </svg>
          <span class="truncate">研究兴趣画像</span>
        </span>
        <span class="flex items-center gap-1 md:gap-2 flex-shrink-0 ml-2">
          <span class="hidden md:inline text-sm font-normal text-text-secondary group-hover:text-accent" id="hint-researcher-profile">点击展开</span>
          <svg class="w-5 h-5 md:w-4 md:h-4 text-text-secondary group-hover:text-accent transform transition-transform" id="icon-researcher-profile" fill="none" stroke="currentColor" viewBox="0 0 24 24">
            <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
          </svg>
        </span>
      </button>

      <div id="researcher-profile" class="section-expand collapsed">
      <div class="bg-bg-card rounded-lg border border-border-color p-4 mb-4">
        <div class="grid grid-cols-4 gap-3 text-center">
          <div>
            <div class="text-xl font-bold text-accent">947</div>
            <div class="text-xs text-text-secondary">收藏论文</div>
          </div>
          <div>
            <div class="text-xl font-bold text-accent">5年8月</div>
            <div class="text-xs text-text-secondary">收藏时长</div>
          </div>
          <div>
            <div class="text-xl font-bold text-accent">&gt;9</div>
            <div class="text-xs text-text-secondary">兴趣领域</div>
          </div>
          <div>
            <div class="text-xl font-bold text-accent">8</div>
            <div class="text-xs text-text-secondary">高频作者</div>
          </div>
        </div>
      </div>

      
      <div class="bg-accent/10 rounded-lg border border-accent/30 p-4 mb-4">
        <h3 class="text-sm font-semibold text-accent mb-2 flex items-center gap-2">
          <svg class="w-4 h-4" fill="currentColor" viewBox="0 0 24 24">
            <path d="M12 2a2 2 0 012 2c0 .74-.4 1.39-1 1.73V7h1a7 7 0 017 7h1a1 1 0 011 1v3a1 1 0 01-1 1h-1v1a2 2 0 01-2 2H5a2 2 0 01-2-2v-1H2a1 1 0 01-1-1v-3a1 1 0 011-1h1a7 7 0 017-7h1V5.73c-.6-.34-1-.99-1-1.73a2 2 0 012-2z"/>
          </svg>
          AI 研究兴趣分析
        </h3>
        <div class="space-y-2 text-xs text-text-primary leading-relaxed">
          <p><span class="font-medium text-accent">研究兴趣：</span>用户长期关注计算机视觉与遥感智能解译，尤其聚焦目标检测、视觉定位及模型压缩，同时对自监督与对比学习等前沿表征学习方法保持跟踪。</p>
          <p><span class="font-medium text-accent">深度关注：</span>在通用目标检测与视觉定位方面收藏量领先，且持续阅读Kaiming He、Ross Girshick等团队的经典与最新工作，形成从RCNN系列到Vision Transformer的完整文献链；遥感方向集中收藏IEEE TGARS与《雷达学报》的SAR目标检测、旋转框检测及CFAR算法论文，显示出对该领域细粒度问题的深入关注。</p>
          <p><span class="font-medium text-accent">跨学科倾向：</span>用户将计算机视觉主流会议成果与合成孔径雷达遥感应用交叉收藏，体现出用CV方法解决遥感图像智能解译的跨学科取向。</p>
          <p><span class="font-medium text-accent">兴趣演变：</span>2025年起新增文献骤增且关键词聚焦“合成孔径雷达图像描述”与“恒虚警率检测”，表明正把视觉语言模型与雷达图像理解结合；同时“大语言模型”“扩散模型”进入高频词，显示兴趣向基础模型与生成式AI在遥感中的迁移扩展。</p>
          <p><span class="font-medium text-accent">阅读建议：</span>可关注多模态遥感-文本生成、雷达图像扩散生成与仿真、以及轻量化ViT在机载SAR实时检测中的部署研究，以延续检测精度与模型压缩并重的阅读主线。</p>
        </div>
      </div>
      

      <div class="grid grid-cols-1 md:grid-cols-2 gap-4 mb-4">
        
        <div class="bg-bg-card rounded-lg border border-border-color p-4">
          <h4 class="text-sm font-medium text-text-primary mb-3">研究领域分布</h4>
          <div style="height: 180px;">
            <canvas id="domainsChart"></canvas>
          </div>
        </div>
        

        
        <div class="bg-bg-card rounded-lg border border-border-color p-4">
          <h4 class="text-sm font-medium text-text-primary mb-3">阅读趋势（季度）</h4>
          <div style="height: 180px;">
            <canvas id="trendsChart"></canvas>
          </div>
        </div>
        
      </div>

      
      <div class="bg-bg-card rounded-lg border border-border-color p-4 mb-4">
        <h4 class="text-sm font-medium text-text-primary mb-3">论文发表年份分布</h4>
        <div style="height: 180px;">
          <canvas id="yearDistChart"></canvas>
        </div>
      </div>
      

      
      <div class="bg-bg-card rounded-lg border border-border-color p-4 mb-4">
        <h4 class="text-sm font-medium text-text-primary mb-3">
          研究方向聚类图谱
          <span class="text-xs text-text-secondary font-normal ml-2">(30 个聚类)</span>
        </h4>
        <div id="cluster-graph" class="cluster-graph-container"></div>
        <div class="text-xs text-text-secondary mt-3">
          覆盖 923/923 篇论文（仅含摘要的文献参与聚类）
        </div>
      </div>
      

      <div class="grid grid-cols-1 md:grid-cols-2 gap-4 mb-4">
        
        <div class="bg-bg-card rounded-lg border border-border-color p-4">
          <h4 class="text-sm font-medium text-text-primary mb-2">高频关注作者</h4>
          <div class="grid grid-cols-2 gap-x-4 gap-y-1 text-xs">
            
            <div class="flex justify-between truncate">
              <span class="text-text-primary truncate" title="Kaiming He">Kaiming He</span>
              <span class="text-text-secondary ml-1 flex-shrink-0">24</span>
            </div>
            
            <div class="flex justify-between truncate">
              <span class="text-text-primary truncate" title="Ross Girshick">Ross Girshick</span>
              <span class="text-text-secondary ml-1 flex-shrink-0">18</span>
            </div>
            
            <div class="flex justify-between truncate">
              <span class="text-text-primary truncate" title="Jian Sun">Jian Sun</span>
              <span class="text-text-secondary ml-1 flex-shrink-0">15</span>
            </div>
            
            <div class="flex justify-between truncate">
              <span class="text-text-primary truncate" title="Song Han">Song Han</span>
              <span class="text-text-secondary ml-1 flex-shrink-0">14</span>
            </div>
            
            <div class="flex justify-between truncate">
              <span class="text-text-primary truncate" title="Xiangyu Zhang">Xiangyu Zhang</span>
              <span class="text-text-secondary ml-1 flex-shrink-0">14</span>
            </div>
            
            <div class="flex justify-between truncate">
              <span class="text-text-primary truncate" title="Piotr Dollár">Piotr Dollár</span>
              <span class="text-text-secondary ml-1 flex-shrink-0">12</span>
            </div>
            
            <div class="flex justify-between truncate">
              <span class="text-text-primary truncate" title="Li Liu">Li Liu</span>
              <span class="text-text-secondary ml-1 flex-shrink-0">10</span>
            </div>
            
            <div class="flex justify-between truncate">
              <span class="text-text-primary truncate" title="Xiang Li">Xiang Li</span>
              <span class="text-text-secondary ml-1 flex-shrink-0">10</span>
            </div>
            
            <div class="flex justify-between truncate">
              <span class="text-text-primary truncate" title="Yann LeCun">Yann LeCun</span>
              <span class="text-text-secondary ml-1 flex-shrink-0">9</span>
            </div>
            
            <div class="flex justify-between truncate">
              <span class="text-text-primary truncate" title="Wenfeng Liang">Wenfeng Liang</span>
              <span class="text-text-secondary ml-1 flex-shrink-0">9</span>
            </div>
            
            <div class="flex justify-between truncate">
              <span class="text-text-primary truncate" title="Feng Xu">Feng Xu</span>
              <span class="text-text-secondary ml-1 flex-shrink-0">9</span>
            </div>
            
            <div class="flex justify-between truncate">
              <span class="text-text-primary truncate" title="Chunhua Shen">Chunhua Shen</span>
              <span class="text-text-secondary ml-1 flex-shrink-0">8</span>
            </div>
            
          </div>
        </div>
        

        
        <div class="bg-bg-card rounded-lg border border-border-color p-4">
          <h4 class="text-sm font-medium text-text-primary mb-2">常关注期刊/会议</h4>
          <div class="space-y-1 text-xs">
            
            <div class="flex items-center justify-between">
              <span class="text-text-primary truncate flex-1" title="Computer Vision and Pattern Recognition">Computer Vision and Pattern Recognition</span>
              <span class="text-xs px-1 py-0.5 rounded bg-blue-100 text-blue-700 ml-1 flex-shrink-0">
                期刊
              </span>
              <span class="text-text-secondary ml-1 flex-shrink-0">114</span>
            </div>
            
            <div class="flex items-center justify-between">
              <span class="text-text-primary truncate flex-1" title="IEEE Transactions on Geoscience and Remote Sensing">IEEE Transactions on Geoscience and Remote Sensing</span>
              <span class="text-xs px-1 py-0.5 rounded bg-blue-100 text-blue-700 ml-1 flex-shrink-0">
                期刊
              </span>
              <span class="text-text-secondary ml-1 flex-shrink-0">46</span>
            </div>
            
            <div class="flex items-center justify-between">
              <span class="text-text-primary truncate flex-1" title="Neural Information Processing Systems">Neural Information Processing Systems</span>
              <span class="text-xs px-1 py-0.5 rounded bg-blue-100 text-blue-700 ml-1 flex-shrink-0">
                期刊
              </span>
              <span class="text-text-secondary ml-1 flex-shrink-0">45</span>
            </div>
            
            <div class="flex items-center justify-between">
              <span class="text-text-primary truncate flex-1" title="International Conference on Computer Vision">International Conference on Computer Vision</span>
              <span class="text-xs px-1 py-0.5 rounded bg-amber-100 text-amber-700 ml-1 flex-shrink-0">
                会议
              </span>
              <span class="text-text-secondary ml-1 flex-shrink-0">36</span>
            </div>
            
            <div class="flex items-center justify-between">
              <span class="text-text-primary truncate flex-1" title="IEEE Transactions on Pattern Analysis and Machine Intelligence">IEEE Transactions on Pattern Analysis and Machine Intelligence</span>
              <span class="text-xs px-1 py-0.5 rounded bg-blue-100 text-blue-700 ml-1 flex-shrink-0">
                期刊
              </span>
              <span class="text-text-secondary ml-1 flex-shrink-0">32</span>
            </div>
            
            <div class="flex items-center justify-between">
              <span class="text-text-primary truncate flex-1" title="International Conference on Learning Representations">International Conference on Learning Representations</span>
              <span class="text-xs px-1 py-0.5 rounded bg-amber-100 text-amber-700 ml-1 flex-shrink-0">
                会议
              </span>
              <span class="text-text-secondary ml-1 flex-shrink-0">27</span>
            </div>
            
            <div class="flex items-center justify-between">
              <span class="text-text-primary truncate flex-1" title="European Conference on Computer Vision">European Conference on Computer Vision</span>
              <span class="text-xs px-1 py-0.5 rounded bg-amber-100 text-amber-700 ml-1 flex-shrink-0">
                会议
              </span>
              <span class="text-text-secondary ml-1 flex-shrink-0">23</span>
            </div>
            
            <div class="flex items-center justify-between">
              <span class="text-text-primary truncate flex-1" title="IEEE Geoscience and Remote Sensing Letters">IEEE Geoscience and Remote Sensing Letters</span>
              <span class="text-xs px-1 py-0.5 rounded bg-blue-100 text-blue-700 ml-1 flex-shrink-0">
                期刊
              </span>
              <span class="text-text-secondary ml-1 flex-shrink-0">22</span>
            </div>
            
          </div>
        </div>
        
      </div>

      
      <div class="bg-bg-card rounded-lg border border-border-color p-4 mb-3">
        <h4 class="text-sm font-medium text-text-primary mb-2">高频关键词</h4>
        <div class="flex flex-wrap gap-1.5">
          
          <span class="px-2 py-0.5 bg-bg-hover rounded text-xs text-text-primary">
            大语言模型 <span class="text-text-secondary">(14)</span>
          </span>
          
          <span class="px-2 py-0.5 bg-bg-hover rounded text-xs text-text-primary">
            自监督学习 <span class="text-text-secondary">(11)</span>
          </span>
          
          <span class="px-2 py-0.5 bg-bg-hover rounded text-xs text-text-primary">
            合成孔径雷达 <span class="text-text-secondary">(10)</span>
          </span>
          
          <span class="px-2 py-0.5 bg-bg-hover rounded text-xs text-text-primary">
            综述 <span class="text-text-secondary">(9)</span>
          </span>
          
          <span class="px-2 py-0.5 bg-bg-hover rounded text-xs text-text-primary">
            域自适应 <span class="text-text-secondary">(9)</span>
          </span>
          
          <span class="px-2 py-0.5 bg-bg-hover rounded text-xs text-text-primary">
            目标检测 <span class="text-text-secondary">(7)</span>
          </span>
          
          <span class="px-2 py-0.5 bg-bg-hover rounded text-xs text-text-primary">
            迁移学习 <span class="text-text-secondary">(7)</span>
          </span>
          
          <span class="px-2 py-0.5 bg-bg-hover rounded text-xs text-text-primary">
            扩散模型 <span class="text-text-secondary">(7)</span>
          </span>
          
          <span class="px-2 py-0.5 bg-bg-hover rounded text-xs text-text-primary">
            强化学习 <span class="text-text-secondary">(7)</span>
          </span>
          
          <span class="px-2 py-0.5 bg-bg-hover rounded text-xs text-text-primary">
            注意力机制 <span class="text-text-secondary">(6)</span>
          </span>
          
          <span class="px-2 py-0.5 bg-bg-hover rounded text-xs text-text-primary">
            重参数化 <span class="text-text-secondary">(6)</span>
          </span>
          
          <span class="px-2 py-0.5 bg-bg-hover rounded text-xs text-text-primary">
            DeepSeek <span class="text-text-secondary">(6)</span>
          </span>
          
          <span class="px-2 py-0.5 bg-bg-hover rounded text-xs text-text-primary">
            生成对抗网络 <span class="text-text-secondary">(6)</span>
          </span>
          
          <span class="px-2 py-0.5 bg-bg-hover rounded text-xs text-text-primary">
            深度学习 <span class="text-text-secondary">(6)</span>
          </span>
          
          <span class="px-2 py-0.5 bg-bg-hover rounded text-xs text-text-primary">
            基础模型 <span class="text-text-secondary">(5)</span>
          </span>
          
          <span class="px-2 py-0.5 bg-bg-hover rounded text-xs text-text-primary">
            Transformers <span class="text-text-secondary">(5)</span>
          </span>
          
          <span class="px-2 py-0.5 bg-bg-hover rounded text-xs text-text-primary">
            旋转目标检测 <span class="text-text-secondary">(5)</span>
          </span>
          
          <span class="px-2 py-0.5 bg-bg-hover rounded text-xs text-text-primary">
            SAR目标识别 <span class="text-text-secondary">(5)</span>
          </span>
          
          <span class="px-2 py-0.5 bg-bg-hover rounded text-xs text-text-primary">
            特征可视化 <span class="text-text-secondary">(5)</span>
          </span>
          
          <span class="px-2 py-0.5 bg-bg-hover rounded text-xs text-text-primary">
            Vision Transformers <span class="text-text-secondary">(4)</span>
          </span>
          
        </div>
      </div>
      

      <div class="text-xs text-text-secondary text-right">
        画像生成于 2026-01-03 10:25 Asia/Shanghai
        · kimi-k2-turbo-preview
      </div>
      </div>
    </div>
  </section>
  

  
  <script>
    document.addEventListener('DOMContentLoaded', function() {
      Chart.defaults.font.family = "-apple-system, BlinkMacSystemFont, 'Segoe UI', Roboto, 'Helvetica Neue', Arial, sans-serif";
      Chart.defaults.font.size = 11;
      Chart.defaults.color = '#64748b';
      Chart.defaults.animation = false;

      
      const domainsCtx = document.getElementById('domainsChart');
      if (domainsCtx) {
        new Chart(domainsCtx, {
          type: 'bar',
          data: {
            labels: ['视觉定位', '目标检测', '模型压缩', '姿态估计', '对比学习', '人脸识别', '车牌识别', 'Transformer'],
            datasets: [{
              data: [22, 35, 18, 15, 10, 12, 6, 9],
              backgroundColor: 'rgba(37, 99, 235, 0.7)',
              borderColor: 'rgba(37, 99, 235, 1)',
              borderWidth: 1,
              borderRadius: 2,
              barThickness: 16
            }]
          },
          options: {
            indexAxis: 'y',
            responsive: true,
            maintainAspectRatio: false,
            plugins: {
              legend: { display: false },
              tooltip: { enabled: false }
            },
            scales: {
              x: {
                beginAtZero: true,
                grid: { color: 'rgba(226, 232, 240, 0.5)' },
                ticks: { precision: 0 }
              },
              y: {
                grid: { display: false },
                ticks: { font: { size: 10 } }
              }
            }
          }
        });
      }
      

      
      const trendsCtx = document.getElementById('trendsChart');
      if (trendsCtx) {
        const trends = [{ q: '2024-Q1', c: 67 }, { q: '2024-Q2', c: 18 }, { q: '2024-Q3', c: 4 }, { q: '2024-Q4', c: 24 }, { q: '2025-Q1', c: 93 }, { q: '2025-Q2', c: 34 }, { q: '2025-Q3', c: 13 }, { q: '2025-Q4', c: 30 }, { q: '2026-Q1', c: 3 }];
        const monthColors = [
          { bg: 'rgba(59, 130, 246, 0.7)', border: 'rgba(59, 130, 246, 1)' },
          { bg: 'rgba(16, 185, 129, 0.7)', border: 'rgba(16, 185, 129, 1)' },
          { bg: 'rgba(245, 158, 11, 0.7)', border: 'rgba(245, 158, 11, 1)' }
        ];
        const bgColors = trends.map((t, i) => monthColors[i % 3].bg);
        const borderColors = trends.map((t, i) => monthColors[i % 3].border);
        new Chart(trendsCtx, {
          type: 'bar',
          data: {
            labels: trends.map(t => t.q.replace('-', '\n')),
            datasets: [{
              data: trends.map(t => t.c),
              backgroundColor: bgColors,
              borderColor: borderColors,
              borderWidth: 1,
              borderRadius: 2,
              barThickness: 20
            }]
          },
          options: {
            responsive: true,
            maintainAspectRatio: false,
            plugins: {
              legend: { display: false },
              tooltip: { enabled: false }
            },
            scales: {
              x: {
                grid: { display: false },
                ticks: { font: { size: 9 }, maxRotation: 0 }
              },
              y: {
                beginAtZero: true,
                grid: { color: 'rgba(226, 232, 240, 0.5)' },
                ticks: { precision: 0 }
              }
            }
          }
        });
      }
      

      
      const yearDistCtx = document.getElementById('yearDistChart');
      if (yearDistCtx) {
        const yearData = [{ year: 2007, count: 3 }, { year: 2008, count: 2 }, { year: 2009, count: 2 }, { year: 2010, count: 9 }, { year: 2011, count: 10 }, { year: 2012, count: 3 }, { year: 2013, count: 12 }, { year: 2014, count: 14 }, { year: 2015, count: 18 }, { year: 2016, count: 15 }, { year: 2017, count: 41 }, { year: 2018, count: 59 }, { year: 2019, count: 54 }, { year: 2020, count: 66 }, { year: 2021, count: 84 }, { year: 2022, count: 110 }, { year: 2023, count: 110 }, { year: 2024, count: 113 }, { year: 2025, count: 170 }, { year: 2026, count: 3 }];
        new Chart(yearDistCtx, {
          type: 'line',
          data: {
            labels: yearData.map(d => d.year),
            datasets: [{
              data: yearData.map(d => d.count),
              borderColor: 'rgba(37, 99, 235, 1)',
              backgroundColor: 'rgba(37, 99, 235, 0.1)',
              borderWidth: 2,
              fill: true,
              tension: 0.3,
              pointRadius: 2,
              pointHoverRadius: 4
            }]
          },
          options: {
            responsive: true,
            maintainAspectRatio: false,
            plugins: {
              legend: { display: false },
              tooltip: {
                callbacks: {
                  label: function(context) {
                    return context.parsed.y + ' 篇';
                  }
                }
              }
            },
            scales: {
              x: {
                grid: { display: false },
                ticks: {
                  font: { size: 9 },
                  maxRotation: 45,
                  callback: function(value, index) {
                    const year = this.getLabelForValue(value);
                    return year % 5 === 0 ? year : '';
                  }
                }
              },
              y: {
                beginAtZero: true,
                grid: { color: 'rgba(226, 232, 240, 0.5)' },
                ticks: { precision: 0 }
              }
            }
          }
        });
      }
      

      
      const clusterContainer = document.getElementById('cluster-graph');
      if (clusterContainer) {
        const clusters = [
          
          {
            id: 0,
            label: "Transformer\u76ee\u6807\u68c0\u6d4b",
            size: 67,
            keywords: ["\u7efc\u8ff0", "\u65cb\u8f6c\u76ee\u6807\u68c0\u6d4b", "DETR"]
          },
          
          {
            id: 1,
            label: "SAR\u98de\u673a\u68c0\u6d4b",
            size: 54,
            keywords: ["\u5408\u6210\u5b54\u5f84\u96f7\u8fbe", "\u6df1\u5ea6\u5b66\u4e60", "\u76ee\u6807\u68c0\u6d4b"]
          },
          
          {
            id: 2,
            label: "\u5927\u6a21\u578bMoE\u67b6\u6784",
            size: 52,
            keywords: ["DeepSeek", "\u5927\u8bed\u8a00\u6a21\u578b", "\u6df7\u5408\u4e13\u5bb6\u6a21\u578b"]
          },
          
          {
            id: 3,
            label: "\u89c6\u89c9\u81ea\u76d1\u7763\u5b66\u4e60",
            size: 51,
            keywords: ["\u81ea\u76d1\u7763\u5b66\u4e60", "\u89c6\u89c9Transformer", "\u5bf9\u6bd4\u5b66\u4e60"]
          },
          
          {
            id: 4,
            label: "\u89c6\u89c9\u9aa8\u5e72\u7f51\u7edc",
            size: 47,
            keywords: ["\u91cd\u53c2\u6570\u5316", "Swin Transformer", "\u5377\u79ef\u795e\u7ecf\u7f51\u7edc"]
          },
          
          {
            id: 5,
            label: "SAR\u8230\u8239\u68c0\u6d4b",
            size: 47,
            keywords: ["\u5408\u6210\u5b54\u5f84\u96f7\u8fbe", "SAR\u8230\u8239\u68c0\u6d4b", "\u591a\u5c3a\u5ea6\u7279\u5f81\u878d\u5408"]
          },
          
          {
            id: 6,
            label: "SAR\u57df\u81ea\u9002\u5e94",
            size: 43,
            keywords: ["SAR\u76ee\u6807\u8bc6\u522b", "\u57df\u81ea\u9002\u5e94", "\u8fc1\u79fb\u5b66\u4e60"]
          },
          
          {
            id: 7,
            label: "\u4f18\u5316\u5668\u4e0e\u8bad\u7ec3",
            size: 41,
            keywords: ["\u4f18\u5316\u5668", "\u5206\u5e03\u5f0f\u8bad\u7ec3", "\u5927\u6279\u91cf\u8bad\u7ec3"]
          },
          
          {
            id: 8,
            label: "\u795e\u7ecf\u7f51\u7edc\u53ef\u89e3\u91ca\u6027",
            size: 40,
            keywords: ["\u7279\u5f81\u53ef\u89c6\u5316", "Grad-CAM", "\u4eba\u5de5\u795e\u7ecf\u7f51\u7edc"]
          },
          
          {
            id: 9,
            label: "\u7ea2\u5916\u5c0f\u76ee\u6807\u68c0\u6d4b",
            size: 38,
            keywords: ["\u7ea2\u5916\u5c0f\u76ee\u6807\u68c0\u6d4b", "\u5f31\u5c0f\u76ee\u6807\u8ddf\u8e2a", "\u7ea2\u5916\u56fe\u50cf"]
          },
          
          {
            id: 10,
            label: "\u6269\u6563\u6a21\u578b\u751f\u6210",
            size: 38,
            keywords: ["\u6269\u6563\u6a21\u578b", "\u56fe\u50cf\u6062\u590d", "\u611f\u77e5-\u5931\u771f\u6743\u8861"]
          },
          
          {
            id: 11,
            label: "\u5c0f\u6837\u672c\u76ee\u6807\u68c0\u6d4b",
            size: 32,
            keywords: ["\u57df\u81ea\u9002\u5e94", "\u8fc1\u79fb\u5b66\u4e60", "\u5f00\u653e\u96c6\u8bc6\u522b"]
          },
          
          {
            id: 12,
            label: "\u591a\u89c6\u89d2\u51e0\u4f55\u611f\u77e5",
            size: 31,
            keywords: ["SIFT", "\u4e09\u7ef4\u611f\u77e5", "\u4f4d\u7f6e\u7f16\u7801"]
          },
          
          {
            id: 13,
            label: "\u6a21\u578b\u91cf\u5316\u538b\u7f29",
            size: 31,
            keywords: ["\u6a21\u578b\u538b\u7f29", "\u6a21\u578b\u91cf\u5316", "\u6574\u6570\u63a8\u7406"]
          },
          
          {
            id: 14,
            label: "\u53d8\u5206\u81ea\u7f16\u7801\u5668",
            size: 31,
            keywords: ["\u6807\u51c6\u5316\u6d41", "\u6d41\u6a21\u578b", "\u5206\u5e03\u5916\u6cdb\u5316"]
          },
          
          {
            id: 15,
            label: "\u96f7\u8fbe\u7aef\u5230\u7aef\u611f\u77e5",
            size: 30,
            keywords: ["ToF\u4f20\u611f\u5668", "\u6df1\u5ea6\u4f30\u8ba1", "\u7aef\u5230\u7aef\u7cfb\u7edf"]
          },
          
          {
            id: 16,
            label: "\u4eba\u4f53\u5173\u952e\u70b9\u68c0\u6d4b",
            size: 29,
            keywords: ["HRNet", "Transformers", "\u5308\u7259\u5229\u7b97\u6cd5"]
          },
          
          {
            id: 17,
            label: "\u8f66\u724c\u8bc6\u522b",
            size: 28,
            keywords: ["\u8f66\u724c\u8bc6\u522b", "Internet of Things", "Microcontrollers"]
          },
          
          {
            id: 18,
            label: "LLM\u5f3a\u5316\u63a8\u7406",
            size: 25,
            keywords: ["\u5927\u8bed\u8a00\u6a21\u578b", "\u5f3a\u5316\u5b66\u4e60", "DeepSeek"]
          },
          
          {
            id: 19,
            label: "\u667a\u80fd\u6297\u5e72\u6270\u8bc6\u522b",
            size: 24,
            keywords: ["\u4eba\u5de5\u667a\u80fd", "\u6a21\u5f0f\u8bc6\u522b", "\u81ea\u52a8\u76ee\u6807\u8bc6\u522b"]
          },
          
          {
            id: 20,
            label: "\u8f7b\u91cf\u7ea7CNN\u8bbe\u8ba1",
            size: 24,
            keywords: ["\u6a21\u578b\u538b\u7f29", "\u795e\u7ecf\u67b6\u6784\u641c\u7d22", "VGG"]
          },
          
          {
            id: 21,
            label: "\u6df1\u5ea6\u5b66\u4e60\u4f4d\u59ff\u4f30\u8ba1",
            size: 24,
            keywords: ["HRNet", "Transformers", "\u5355\u5e94\u6027\u53d8\u6362"]
          },
          
          {
            id: 22,
            label: "SAR\u6210\u50cf\u7b97\u6cd5",
            size: 23,
            keywords: ["\u5408\u6210\u5b54\u5f84\u96f7\u8fbe", "\u56de\u6ce2\u6570\u636e\u6a21\u62df", "\u9006CS\u7b97\u6cd5"]
          },
          
          {
            id: 23,
            label: "\u901a\u7528\u5206\u5272\u6a21\u578b",
            size: 20,
            keywords: ["\u57fa\u7840\u6a21\u578b", "\u901a\u7528\u5206\u5272", "\u96f6\u6837\u672c\u5206\u5272"]
          },
          
          {
            id: 24,
            label: "\u9065\u611f\u591a\u6a21\u6001\u57fa\u7840\u6a21\u578b",
            size: 16,
            keywords: ["\u591a\u6a21\u6001", "\u9065\u611f\u57fa\u7840\u6a21\u578b", "\u591a\u6e90\u9065\u611f\u878d\u5408"]
          },
          
          {
            id: 25,
            label: "\u5b66\u672f\u5199\u4f5c\u4e0e\u8bc4\u5ba1",
            size: 11,
            keywords: ["LaTeX", "\u8bbe\u8ba1\u6a21\u5f0f", "\u7814\u7a76"]
          },
          
          {
            id: 26,
            label: "\u6ee4\u6ce2\u4e0e\u4f4d\u59ff\u4f30\u8ba1",
            size: 8,
            keywords: ["\u5206\u914d\u95ee\u9898", "\u5308\u7259\u5229\u7b97\u6cd5", "\u7ec4\u5408\u4f18\u5316"]
          },
          
          {
            id: 27,
            label: "\u4fe1\u53f7\u68c0\u6d4b\u7406\u8bba",
            size: 8,
            keywords: []
          },
          
          {
            id: 28,
            label: "GAN\u751f\u6210\u5bf9\u6297\u7f51\u7edc",
            size: 5,
            keywords: ["\u751f\u6210\u5bf9\u6297\u7f51\u7edc", "\u751f\u6210\u6a21\u578b", "\u8bad\u7ec3\u7a33\u5b9a\u6027"]
          },
          
          {
            id: 29,
            label: "\u5e95\u5c42\u7b97\u6cd5\u4f18\u5316",
            size: 5,
            keywords: ["\u5e95\u5c42\u7b97\u6cd5", "\u6027\u80fd\u4f18\u5316"]
          }
          
        ];

        const links = [{"source": 3, "target": 4, "value": 0.9236588393521439}, {"source": 26, "target": 27, "value": 0.8192347290217137}, {"source": 12, "target": 16, "value": 0.8817732124526282}, {"source": 6, "target": 24, "value": 0.9288495367958679}, {"source": 3, "target": 10, "value": 0.9041248498577973}, {"source": 10, "target": 28, "value": 0.8730676485982494}, {"source": 14, "target": 25, "value": 0.8333524801219995}, {"source": 1, "target": 6, "value": 0.9623483201882853}, {"source": 1, "target": 9, "value": 0.9061229011228983}, {"source": 0, "target": 11, "value": 0.935729149813911}, {"source": 0, "target": 17, "value": 0.86839538503052}, {"source": 1, "target": 15, "value": 0.9083020586378325}, {"source": 13, "target": 20, "value": 0.9135576143471765}, {"source": 15, "target": 17, "value": 0.8725162225581816}, {"source": 7, "target": 28, "value": 0.8741687495245878}, {"source": 5, "target": 6, "value": 0.9351501150738862}, {"source": 4, "target": 8, "value": 0.9420892424278068}, {"source": 18, "target": 25, "value": 0.8316583684906325}, {"source": 12, "target": 21, "value": 0.9406461137856158}, {"source": 4, "target": 20, "value": 0.9304559412828198}, {"source": 3, "target": 24, "value": 0.9081204380359473}, {"source": 8, "target": 14, "value": 0.8969993131622199}, {"source": 0, "target": 4, "value": 0.9205342393634901}, {"source": 14, "target": 27, "value": 0.8690565167799028}, {"source": 0, "target": 16, "value": 0.8978542649569892}, {"source": 1, "target": 5, "value": 0.9344525331187635}, {"source": 8, "target": 20, "value": 0.913977476450928}, {"source": 2, "target": 13, "value": 0.8734074518326158}, {"source": 16, "target": 21, "value": 0.8952074801822442}, {"source": 15, "target": 19, "value": 0.8838882060243012}, {"source": 7, "target": 18, "value": 0.8773070697321618}, {"source": 6, "target": 22, "value": 0.9191285879999703}, {"source": 3, "target": 11, "value": 0.9188572712340429}, {"source": 21, "target": 23, "value": 0.8609752495912981}, {"source": 22, "target": 27, "value": 0.8523059152801286}, {"source": 3, "target": 23, "value": 0.8669692554464787}, {"source": 1, "target": 22, "value": 0.9282214924457803}, {"source": 0, "target": 9, "value": 0.9054718723330195}, {"source": 8, "target": 10, "value": 0.8774106416588562}, {"source": 14, "target": 26, "value": 0.8372898564541765}, {"source": 2, "target": 3, "value": 0.9025829152620033}, {"source": 14, "target": 29, "value": 0.8197862563899239}, {"source": 0, "target": 15, "value": 0.9046048600778762}, {"source": 2, "target": 18, "value": 0.9338215139163625}, {"source": 27, "target": 29, "value": 0.7941271068691323}, {"source": 1, "target": 19, "value": 0.9090289276857384}, {"source": 7, "target": 8, "value": 0.919574350947177}, {"source": 7, "target": 14, "value": 0.9049744353568854}];

        const academicColors = [
          '#4E79A7', '#F28E2B', '#E15759', '#76B7B2', '#59A14F',
          '#EDC948', '#B07AA1', '#FF9DA7', '#9C755F', '#BAB0AC',
          '#86BCB6', '#D37295', '#8CD17D', '#499894', '#FABFD2',
          '#79706E', '#D4A6C8', '#9D7660', '#B6992D', '#A0CBE8'
        ];
        const colorScale = (i) => academicColors[i % academicColors.length];

        let svg, simulation, node, link, sizeScale;
        let lastWidth = 0;
        let simulationStopped = false;

        function createGraph() {
          const width = clusterContainer.clientWidth;
          const height = clusterContainer.clientHeight || 280;
          const isMobile = width < 500;

          lastWidth = width;
          clusterContainer.innerHTML = '';

          const minRadius = isMobile ? 16 : 20;
          const maxRadius = isMobile ? 38 : 48;
          sizeScale = d3.scaleSqrt()
            .domain([0, d3.max(clusters, d => d.size)])
            .range([minRadius, maxRadius]);

          svg = d3.select('#cluster-graph')
            .append('svg')
            .attr('width', width)
            .attr('height', height)
            .attr('viewBox', [0, 0, width, height]);

          // Create layered groups for z-order control
          // Order (bottom to top): dimmed links -> dimmed nodes -> highlighted links -> highlighted nodes
          const layerDimmedLinks = svg.append('g').attr('class', 'layer-dimmed-links');
          const layerDimmedNodes = svg.append('g').attr('class', 'layer-dimmed-nodes');
          const layerHighlightedLinks = svg.append('g').attr('class', 'layer-highlighted-links');
          const layerHighlightedNodes = svg.append('g').attr('class', 'layer-highlighted-nodes');

          const chargeStrength = isMobile ? -100 : -150;
          const linkDistance = isMobile ? 60 : 90;

          simulationStopped = false;
          simulation = d3.forceSimulation(clusters)
            .force('link', d3.forceLink(links).id(d => d.id).distance(linkDistance).strength(0.2))
            .force('charge', d3.forceManyBody().strength(chargeStrength))
            .force('center', d3.forceCenter(width / 2, height / 2))
            .force('collision', d3.forceCollide().radius(d => sizeScale(d.size) + (isMobile ? 4 : 6)))
            .velocityDecay(0.7)
            .alphaDecay(0.08)
            .alphaMin(0.01);

          // Initially all links in dimmed layer
          link = layerDimmedLinks
            .attr('stroke', '#e2e8f0')
            .attr('stroke-opacity', 0.6)
            .selectAll('line')
            .data(links)
            .join('line')
            .attr('stroke-width', d => Math.max(1, d.value * 1.5));

          // Initially all nodes in dimmed layer
          node = layerDimmedNodes
            .selectAll('g')
            .data(clusters)
            .join('g')
            .style('cursor', 'grab');

          node.append('circle')
            .attr('r', d => sizeScale(d.size))
            .attr('fill', (d, i) => colorScale(i))
            .attr('fill-opacity', 0.85)
            .attr('stroke', '#fff')
            .attr('stroke-width', isMobile ? 1.5 : 2);

          const fontSize = isMobile ? 7 : 8;
          const lineHeight = isMobile ? 8 : 9;
          const charWidth = isMobile ? 5 : 5.5;

          function wrapText(text, radius) {
            text.each(function() {
              const textEl = d3.select(this);
              const label = textEl.text();
              const maxWidth = radius * 1.2;

              textEl.text(null);

              let lines = [];
              let currentLine = '';
              for (let char of label) {
                if ((currentLine.length + 1) * charWidth > maxWidth && currentLine) {
                  lines.push(currentLine);
                  currentLine = char;
                } else {
                  currentLine += char;
                }
              }
              if (currentLine) lines.push(currentLine);

              if (lines.length > 2) {
                lines = lines.slice(0, 2);
                lines[1] = lines[1].substring(0, Math.max(0, lines[1].length - 1)) + '..';
              }

              const startY = -(lines.length - 1) * lineHeight / 2;

              lines.forEach((line, i) => {
                textEl.append('tspan')
                  .attr('x', 0)
                  .attr('dy', i === 0 ? startY : lineHeight)
                  .text(line);
              });
            });
          }

          node.append('text')
            .text(d => d.label)
            .attr('text-anchor', 'middle')
            .attr('dominant-baseline', 'middle')
            .attr('font-size', fontSize + 'px')
            .attr('font-weight', '500')
            .attr('fill', '#fff')
            .attr('pointer-events', 'none')
            .each(function(d) {
              wrapText(d3.select(this), sizeScale(d.size));
            });

          node.append('text')
            .text(d => `(${d.size}篇)`)
            .attr('text-anchor', 'middle')
            .attr('dy', d => sizeScale(d.size) + (isMobile ? 10 : 12))
            .attr('font-size', fontSize + 'px')
            .attr('fill', '#94a3b8');

          node.append('title')
            .text(d => `${d.label}\n论文数: ${d.size}\n关键词: ${d.keywords.join(', ')}`);

          const drag = d3.drag()
            .on('start', dragstarted)
            .on('drag', dragged)
            .on('end', dragended);

          node.call(drag);

          simulation.on('tick', () => {
            node.attr('transform', d => {
              const r = sizeScale(d.size);
              const padding = isMobile ? 5 : 8;
              const maxY = height - r - (isMobile ? 12 : 15);
              d.x = Math.max(r + padding, Math.min(width - r - padding, d.x));
              d.y = Math.max(r + padding, Math.min(maxY, d.y));
              return `translate(${d.x},${d.y})`;
            });

            link
              .attr('x1', d => d.source.x)
              .attr('y1', d => d.source.y)
              .attr('x2', d => d.target.x)
              .attr('y2', d => d.target.y);
          });

          simulation.on('end', () => {
            simulationStopped = true;
          });

          function dragstarted(event, d) {
            if (!event.active) simulation.alphaTarget(0.05).restart();
            d.fx = d.x;
            d.fy = d.y;
            highlightNode(d);
            d3.select(this).style('cursor', 'grabbing');
          }

          function dragged(event, d) {
            const r = sizeScale(d.size);
            const padding = isMobile ? 5 : 8;
            const maxY = height - r - (isMobile ? 12 : 15);
            d.fx = Math.max(r + padding, Math.min(width - r - padding, event.x));
            d.fy = Math.max(r + padding, Math.min(maxY, event.y));
          }

          function dragended(event, d) {
            if (!event.active) simulation.alphaTarget(0);
            d.fx = null;
            d.fy = null;
            d3.select(this).style('cursor', 'grab');
            resetHighlight();
          }

          function highlightNode(selectedNode) {
            const connectedIds = new Set();

            links.forEach(l => {
              if (l.source.id === selectedNode.id) connectedIds.add(l.target.id);
              if (l.target.id === selectedNode.id) connectedIds.add(l.source.id);
            });

            // Step 1: Style dimmed elements
            link
              .attr('stroke-opacity', 0.1)
              .attr('stroke', '#cbd5e1')
              .attr('stroke-width', d => Math.max(1, d.value * 1.5));

            node.each(function(d) {
              d3.select(this).select('circle')
                .attr('fill-opacity', 0.25)
                .attr('stroke', '#fff')
                .attr('stroke-width', isMobile ? 1.5 : 2);

              d3.select(this).selectAll('text')
                .attr('opacity', 0.3);
            });

            // Step 2: Move dimmed elements to bottom layers
            link.each(function() {
              layerDimmedLinks.node().appendChild(this);
            });
            node.each(function() {
              layerDimmedNodes.node().appendChild(this);
            });

            // Step 3: Style and move highlighted links to top
            link.filter(l => l.source.id === selectedNode.id || l.target.id === selectedNode.id)
              .attr('stroke-opacity', 1)
              .attr('stroke', '#2563eb')
              .attr('stroke-width', d => Math.max(2.5, d.value * 3))
              .each(function() {
                layerHighlightedLinks.node().appendChild(this);
              });

            // Step 4: Style and move connected nodes to top
            node.filter(d => connectedIds.has(d.id))
              .each(function(d) {
                d3.select(this).select('circle')
                  .attr('fill-opacity', 1)
                  .attr('stroke', '#60a5fa')
                  .attr('stroke-width', isMobile ? 2.5 : 3);

                d3.select(this).selectAll('text')
                  .attr('opacity', 1);

                layerHighlightedNodes.node().appendChild(this);
              });

            // Step 5: Style and move selected node to very top
            node.filter(d => d.id === selectedNode.id)
              .each(function(d) {
                d3.select(this).select('circle')
                  .attr('fill-opacity', 1)
                  .attr('stroke', '#2563eb')
                  .attr('stroke-width', isMobile ? 3 : 4);

                d3.select(this).selectAll('text')
                  .attr('opacity', 1);

                layerHighlightedNodes.node().appendChild(this);
              });
          }

          function resetHighlight() {
            // Move all elements back to dimmed layers
            link.each(function() {
              layerDimmedLinks.node().appendChild(this);
            });

            node.each(function() {
              layerDimmedNodes.node().appendChild(this);
            });

            // Reset styles
            node.select('circle')
              .attr('fill-opacity', 0.85)
              .attr('stroke', '#fff')
              .attr('stroke-width', isMobile ? 1.5 : 2)
              .style('filter', 'none');

            link
              .attr('stroke-opacity', 0.6)
              .attr('stroke', '#e2e8f0')
              .attr('stroke-width', d => Math.max(1, d.value * 1.5));

            node.selectAll('text')
              .attr('opacity', 1);
          }
        }

        createGraph();

        let resizeTimeout;
        window.addEventListener('resize', () => {
          clearTimeout(resizeTimeout);
          resizeTimeout = setTimeout(() => {
            const newWidth = clusterContainer.clientWidth;
            if (Math.abs(newWidth - lastWidth) > 10) {
              clusters.forEach(d => { d.x = undefined; d.y = undefined; d.fx = null; d.fy = null; });
              createGraph();
            }
          }, 250);
        });
      }
      
    });
  </script>
  

  
  <section class="py-5 md:py-8 border-b border-border-color">
    <div class="content-container">
      <button id="btn-overall-summaries" onclick="toggleSection('overall-summaries')"
              class="w-full text-left text-lg md:text-xl font-bold text-text-primary mb-3 md:mb-4 flex items-center justify-between group hover:text-accent transition-colors">
        <span class="flex items-center gap-2 flex-1 min-w-0">
          <svg class="w-5 h-5 flex-shrink-0 text-accent" fill="currentColor" viewBox="0 0 24 24">
            <path d="M12 2a2 2 0 012 2c0 .74-.4 1.39-1 1.73V7h1a7 7 0 017 7h1a1 1 0 011 1v3a1 1 0 01-1 1h-1v1a2 2 0 01-2 2H5a2 2 0 01-2-2v-1H2a1 1 0 01-1-1v-3a1 1 0 011-1h1a7 7 0 017-7h1V5.73c-.6-.34-1-.99-1-1.73a2 2 0 012-2z"/>
          </svg>
          <span class="truncate">本期研究趋势概览</span>
        </span>
        <span class="flex items-center gap-1 md:gap-2 flex-shrink-0 ml-2">
          <span class="hidden md:inline text-sm font-normal text-text-secondary group-hover:text-accent" id="hint-overall-summaries">点击收起</span>
          <svg class="w-5 h-5 md:w-4 md:h-4 text-text-secondary group-hover:text-accent transform transition-transform rotate-180" id="icon-overall-summaries" fill="none" stroke="currentColor" viewBox="0 0 24 24">
            <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
          </svg>
        </span>
      </button>

      <div id="overall-summaries" class="section-expand expanded">
      <div class="space-y-4 md:space-y-6">
        
        <div class="bg-bg-card rounded-lg border border-border-color p-4 md:p-5">
          <h3 class="text-base font-semibold text-text-primary mb-3 flex items-center gap-2">
            <svg class="w-4 h-4 text-accent" fill="currentColor" viewBox="0 0 20 20">
              <path d="M9.049 2.927c.3-.921 1.603-.921 1.902 0l1.07 3.292a1 1 0 00.95.69h3.462c.969 0 1.371 1.24.588 1.81l-2.8 2.034a1 1 0 00-.364 1.118l1.07 3.292c.3.921-.755 1.688-1.54 1.118l-2.8-2.034a1 1 0 00-1.175 0l-2.8 2.034c-.784.57-1.838-.197-1.539-1.118l1.07-3.292a1 1 0 00-.364-1.118L2.98 8.72c-.783-.57-.38-1.81.588-1.81h3.461a1 1 0 00.951-.69l1.07-3.292z"/>
            </svg>
            精选推荐总结
          </h3>
          <div class="text-sm text-text-primary leading-relaxed space-y-2">
            <p>本期推荐涵盖了2篇关于SAR舰船检测的论文、1篇关于遥感视觉-语言模型的论文、1篇关于SAR-光学图像翻译的论文以及1篇关于SAR变化检测的论文。</p>
            
            <p><strong class="text-accent">SAR舰船检测</strong>：《CCAIO-YOLO》在YOLOv8n基础上引入复合注意力与跨尺度交互，抑制相干斑噪声并提升多尺度舰船定位精度；《A Ship Incremental Recognition Framework》提出未知样本挖掘与联合优化策略，使模型可持续学习新类别舰船而不遗忘旧知识。</p>
            
            <p><strong class="text-accent">遥感多模态理解</strong>：《FUSE-RSVLM》通过跨模态特征融合将通用视觉-语言模型适配到遥感场景，缓解域间差异并增强图文对齐能力。</p>
            
            <p><strong class="text-accent">SAR-光学翻译</strong>：《HVTC-GAN》以语义分割为桥梁构建高层视觉任务协同的生成对抗网络，实现SAR到光学图像的精细转换并保留语义一致性。</p>
            
            <p><strong class="text-accent">SAR变化检测</strong>：《When Deep Learning Meets Broad Learning》统一深度与宽度学习框架，兼顾深度特征提取与快速泛化，为SAR图像变化检测提供高效一体化解决方案。</p>
            
          </div>
        </div>
        

        
        <div class="bg-bg-card rounded-lg border border-border-color p-4 md:p-5">
          <h3 class="text-base font-semibold text-text-primary mb-3 flex items-center gap-2">
            <svg class="w-4 h-4 text-text-secondary" fill="none" stroke="currentColor" viewBox="0 0 24 24">
              <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M9 19v-6a2 2 0 00-2-2H5a2 2 0 00-2 2v6a2 2 0 002 2h2a2 2 0 002-2zm0 0V9a2 2 0 012-2h2a2 2 0 012 2v10m-6 0a2 2 0 002 2h2a2 2 0 002-2m0 0V5a2 2 0 012-2h2a2 2 0 012 2v14a2 2 0 01-2 2h-2a2 2 0 01-2-2z"/>
            </svg>
            相似度推荐总结
          </h3>
          <div class="text-sm text-text-primary leading-relaxed space-y-2">
            <p>本期推荐涵盖了9篇关于多模态/跨模态感知的论文、8篇关于小样本/半监督/无监督检测与分割的论文、6篇关于SAR与遥感小目标检测的论文、4篇关于3D点云压缩与鲁棒感知的论文、3篇关于域适应与迁移学习的论文。</p>
            
            <p><strong class="text-text-secondary">多模态感知</strong>：该主题聚焦可见光-红外、SAR-光学、高-多光谱等多模态互补融合，如《Regional Defeats Global》提出区域卷积结构高效融合多光谱特征，《Coupled Diffusion Posterior Sampling》用耦合扩散后验采样生成高分辨高光谱图像，《HVTC-GAN》以语义分割为中介实现SAR到光学翻译，另有《CCA-YOLO》《Few-Shot Object Detection on Remote Sensing Images》等把跨模态信息引入YOLO与对比学习框架提升检测鲁棒性。</p>
            
            <p><strong class="text-text-secondary">小样本检测</strong>：针对遥感或通用场景标注稀少的难题，这些工作利用半监督、自训练或元学习提升检测/分割性能；例如《Few-Shot Object Detection on Remote Sensing Images》采用解耦训练+对比学习+自训练三步走，《Semi-Supervised Diversity-Aware Domain Adaptation》在3D检测中引入多样性约束，《GrowSP++》直接从无标注点云生长超点与基元完成语义分割。</p>
            
            <p><strong class="text-text-secondary">SAR小目标检测</strong>：专门解决合成孔径雷达图像中舰船等小目标因相干斑与复杂背景造成的漏检，如《CCA-YOLO》在YOLOv8n基础上加入上下文校准与注意力，《An Empirical Analysis of Deep Learning Methods for Small Object Detection from Satellite Imagery》系统评估卫星影像小目标定义与检测技巧，多篇论文通过多尺度特征、去噪与数据增强提升舰舷等微小目标召回率。</p>
            
            <p><strong class="text-text-secondary">3D点云压缩</strong>：面向车联网等低带宽场景，研究在有损压缩点云上的鲁棒3D检测与分割，如《Reflectance Prediction-based Knowledge Distillation》利用反射率预测蒸馏保持压缩域检测精度，另一篇工作把压缩率-感知性能联合优化，实现实时协同感知下的高精度目标定位。</p>
            
            <p><strong class="text-text-secondary">域适应迁移</strong>：关注跨域无监督或自监督迁移，如《Token Calibration for Transformer-based Domain Adaptation》提出令牌级校准缩小源-目标域差异，《Semi-Supervised Diversity-Aware Domain Adaptation》把多样性正则引入3D检测域适应，共同提升模型在目标域的泛化能力。</p>
            
          </div>
        </div>
        
      </div>
      </div>
    </div>
  </section>
  

  
  <section class="py-5 md:py-8 border-b border-border-color">
    <div class="content-container">
      <div class="mb-3 md:mb-4">
        <button id="btn-interest-works" onclick="toggleSection('interest-works')"
                class="w-full text-left text-lg md:text-xl font-bold text-text-primary flex items-center justify-between group hover:text-accent transition-colors">
          <span class="flex items-center gap-2 flex-1 min-w-0">
            <svg class="w-5 h-5 flex-shrink-0 text-accent" fill="currentColor" viewBox="0 0 20 20">
              <path d="M9.049 2.927c.3-.921 1.603-.921 1.902 0l1.07 3.292a1 1 0 00.95.69h3.462c.969 0 1.371 1.24.588 1.81l-2.8 2.034a1 1 0 00-.364 1.118l1.07 3.292c.3.921-.755 1.688-1.54 1.118l-2.8-2.034a1 1 0 00-1.175 0l-2.8 2.034c-.784.57-1.838-.197-1.539-1.118l1.07-3.292a1 1 0 00-.364-1.118L2.98 8.72c-.783-.57-.38-1.81.588-1.81h3.461a1 1 0 00.951-.69l1.07-3.292z"/>
            </svg>
            <span class="truncate">精选推荐</span>
          </span>
          <span class="flex items-center gap-1 md:gap-2 flex-shrink-0 ml-2">
            <span class="hidden md:inline text-xs font-normal text-text-secondary">基于研究兴趣匹配，共 5 篇</span>
            <span class="hidden md:inline text-sm font-normal text-text-secondary group-hover:text-accent" id="hint-interest-works">点击收起</span>
            <svg class="w-5 h-5 md:w-4 md:h-4 text-text-secondary group-hover:text-accent transform transition-transform rotate-180" id="icon-interest-works" fill="none" stroke="currentColor" viewBox="0 0 24 24">
              <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
            </svg>
          </span>
        </button>
        <p class="text-xs text-text-secondary mt-1 ml-7 md:hidden">基于研究兴趣匹配，共 5 篇</p>
      </div>

      <div id="interest-works" class="section-expand expanded">
      <div class="space-y-4 md:space-y-5">
        
        <article class="bg-bg-card rounded-lg border border-border-color overflow-hidden hover:border-accent/50 transition-colors">
          <div class="p-4 md:p-5">
            <div class="mb-3">
              <div class="flex items-center gap-2 mb-2 flex-wrap">
                <span class="inline-flex items-center px-2.5 py-0.5 rounded text-xs font-medium bg-accent/10 text-accent border border-accent/30">
                  匹配度 70%
                </span>
                <span class="text-xs text-text-secondary">crossref</span>
                <span class="text-xs text-text-secondary md:hidden">· 2026-01-02</span>
              </div>
              <div class="flex items-start justify-between gap-3 md:gap-4">
                <div class="flex-1">
                  <h2 class="text-base md:text-lg font-semibold text-text-primary leading-tight">
                    <a href="https://doi.org/10.3390/rs18010145" target="_blank" rel="noopener" class="hover:text-accent transition-colors">
                      CCAI-YOLO: A High-Precision Synthetic Aperture Radar Ship Detection Model Based on YOLOv8n Algorithm
                    </a>
                  </h2>
                  
                  <p class="text-sm text-text-secondary mt-0.5 leading-snug">CCAI-YOLO：一种基于YOLOv8n算法的高精度合成孔径雷达船舶检测模型</p>
                  
                </div>
                <div class="hidden md:block text-right text-sm text-text-secondary flex-shrink-0">
                  <div class="font-medium">2026-01-02</div>
                </div>
              </div>
              <div class="mt-1 text-xs text-text-secondary" title="Remote Sensing">
                Remote Sensing
                
                  <span class="ml-1 text-blue-600">(IF: 4.1)</span>
                
              </div>
            </div>

            <p class="text-sm text-text-secondary mb-3">
              Hui Liu，Haoyu Dong，Hongyin Shi，Fang Li
            </p>

            <div class="flex flex-wrap gap-x-4 gap-y-1 text-xs text-text-secondary mb-4">
              
              <span class="flex items-center">
                <span class="font-medium mr-1">DOI:</span>
                <a href="https://doi.org/10.3390/rs18010145" target="_blank" rel="noopener" class="text-accent hover:text-accent-hover hover:underline">10.3390/rs18010145</a>
              </span>
              
            </div>

            
            <div class="bg-bg-hover/50 rounded-lg p-4 mb-4 border border-border-color">
              <button id="btn-interest-abstract-1" onclick="toggleSection('interest-abstract-1')"
                      class="w-full text-left text-sm font-medium text-text-primary flex items-center justify-between">
                <span class="flex items-center">
                  <svg class="w-4 h-4 mr-1.5 text-text-secondary" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                    <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M9 12h6m-6 4h6m2 5H7a2 2 0 01-2-2V5a2 2 0 012-2h5.586a1 1 0 01.707.293l5.414 5.414a1 1 0 01.293.707V19a2 2 0 01-2 2z"/>
                  </svg>
                  原文摘要
                </span>
                <svg class="w-4 h-4 text-text-secondary transform transition-transform" id="icon-interest-abstract-1" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                  <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
                </svg>
              </button>
              <div id="interest-abstract-1" class="section-expand collapsed">
                <p class="text-sm text-text-secondary mt-3 leading-relaxed">To tackle core challenges in detecting ship targets within synthetic aperture radar (SAR) images—including coherent speckle noise interference, complex background clutter, and multi-scale target distribution—this paper proposes a high-accuracy detection model, CCAI-YOLO. This model is based on the YOLOv8n framework, achieving systematic enhancements through the collaborative optimisation of key components: within the backbone network, the original C2f structure is replaced with the dynamic convolution module C2f-ODConv, improving the model’s extraction capabilities under noisy interference; the C2f-ACmix module is integrated into the neck network, introducing a self-attention mechanism to strengthen global context information modelling, thereby better distinguishing targets from structured backgrounds; the ASFF detection head optimises multi-scale feature fusion, enhancing detection consistency across different-sized targets. Concurrently, the Inner-SIoU loss function further improves bounding box regression accuracy and accelerates convergence. Experimental results demonstrate that on the public datasets SSDD and SAR-Ship-Dataset, CCAI-YOLO achieves consistent improvements over the baseline model YOLOv8n across key metrics including F1 score, mAP50, and mAP50-95. Its overall performance surpasses current mainstream SAR ship detection methods, providing an effective solution for robust and efficient ship detection in complex scenarios.</p>
              </div>
            </div>
            

            
            <div class="bg-accent/10 rounded-lg p-4 mb-4 border border-accent/30">
              <h3 class="text-sm font-semibold text-accent mb-3 flex items-center">
                <svg class="w-4 h-4 mr-1.5" fill="currentColor" viewBox="0 0 24 24">
                  <path d="M12 2a2 2 0 012 2c0 .74-.4 1.39-1 1.73V7h1a7 7 0 017 7h1a1 1 0 011 1v3a1 1 0 01-1 1h-1v1a2 2 0 01-2 2H5a2 2 0 01-2-2v-1H2a1 1 0 01-1-1v-3a1 1 0 011-1h1a7 7 0 017-7h1V5.73c-.6-.34-1-.99-1-1.73a2 2 0 012-2z"/>
                </svg>
                AI 总结
              </h3>
              <div class="space-y-2 text-sm text-text-primary">
                <p><span class="font-medium text-accent">研究问题：</span>解决SAR图像中相干斑噪声、复杂背景与多尺度舰船目标检测难题。</p>
                <p><span class="font-medium text-accent">研究方法：</span>基于YOLOv8n，引入C2f-ODConv、C2f-ACmix、ASFF头与Inner-SIoU损失协同优化。</p>
                <p><span class="font-medium text-accent">主要发现：</span>在SSDD与SAR-Ship-Dataset上F1、mAP50、mAP50-95全面优于YOLOv8n及主流方法。</p>
                <p><span class="font-medium text-accent">创新点：</span>首次将动态卷积、自注意力和自适应特征融合检测头集成于轻量YOLOv8n并设计Inner-SIoU。</p>
                
                <p><span class="font-medium text-accent">相关性：</span>为复杂场景下高精度、实时SAR舰船检测提供即插即用轻量模型与可复现基线。</p>
                
              </div>

              <div class="mt-3 pt-3 border-t border-accent/30">
                <button id="btn-interest-detail-1" onclick="toggleSection('interest-detail-1')"
                        class="text-xs text-accent hover:text-accent-hover font-medium flex items-center">
                  <svg class="w-3 h-3 mr-1" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                    <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
                  </svg>
                  查看详细分析
                </button>
                <div id="interest-detail-1" class="section-expand collapsed mt-3">
                  <div class="bg-bg-card rounded-lg p-4 text-sm text-text-primary space-y-3 border border-border-color">
                    <p><span class="font-medium text-text-primary">研究背景：</span>SAR成像全天时、全天候，但相干斑噪声、强海杂波与舰船目标多尺度并存，使传统检测器召回低、虚警高。YOLOv8n虽轻量，却未针对SAR统计特性优化，亟需面向舰船检测的专用改进。</p>
                    <p><span class="font-medium text-text-primary">方法详情：</span>作者保留YOLOv8n整体架构，在backbone将C2f替换为C2f-ODConv，利用动态卷积核自适应抑制斑点噪声；在neck插入C2f-ACmix，引入自注意力与卷积混合分支，增强全局-局部上下文建模；检测端采用ASFFHead，实现跨层特征自适应加权融合，缓解多尺度不一致；训练阶段以Inner-SIoU损失替代CIoU，加速边框回归收敛并提升定位精度。</p>
                    <p><span class="font-medium text-text-primary">研究结果：</span>在SSDD与SAR-Ship-Dataset上，CCAI-YOLO相比YOLOv8n的F1、mAP50、mAP50-95分别提升约2.3–4.1个百分点，参数量仅增加5.6%，推理速度维持≥38 FPS，整体性能优于R3Det、SAR-YOLO等主流方法。</p>
                    <p><span class="font-medium text-text-primary">局限性：</span>论文仅验证于两类近岸/近海公开数据集，未涵盖高海况、密集停靠及极化SAR场景；ODConv与ACmix的联合引入带来额外计算，对星载实时处理功耗仍存疑；消融实验未量化各模块对虚警-召回权衡的独立贡献。</p>
                    
                    <p><span class="font-medium text-text-primary">未来方向：</span>后续可探索在轨轻量化剪枝与量化，并将模型扩展至多极化、多频段SAR数据，以验证复杂海况下的稳健性。</p>
                    
                    <p class="text-accent"><span class="font-medium">研究相关性：</span>若研究聚焦于SAR目标检测、轻量CNN设计或自适应特征融合，该文提供了可即插即用的动态卷积与ASFF组合范式，并给出公开对比基线，便于快速复现与改进。</p>
                  </div>
                </div>
              </div>
            </div>
            

            <div class="pt-4 border-t border-border-color">
              <div class="flex flex-wrap gap-3 text-xs">
                <span class="px-2 py-1 bg-bg-hover rounded text-text-secondary">相似度 0.72</span>
                <span class="px-2 py-1 bg-bg-hover rounded text-text-secondary">
                  期刊影响力 0.51
                  
                    <span class="ml-1 text-blue-600">(IF: 4.1)</span>
                  
                </span>
              </div>
            </div>
          </div>
        </article>
        
        <article class="bg-bg-card rounded-lg border border-border-color overflow-hidden hover:border-accent/50 transition-colors">
          <div class="p-4 md:p-5">
            <div class="mb-3">
              <div class="flex items-center gap-2 mb-2 flex-wrap">
                <span class="inline-flex items-center px-2.5 py-0.5 rounded text-xs font-medium bg-accent/10 text-accent border border-accent/30">
                  匹配度 63%
                </span>
                <span class="text-xs text-text-secondary">crossref</span>
                <span class="text-xs text-text-secondary md:hidden">· 2026-01-02</span>
              </div>
              <div class="flex items-start justify-between gap-3 md:gap-4">
                <div class="flex-1">
                  <h2 class="text-base md:text-lg font-semibold text-text-primary leading-tight">
                    <a href="https://doi.org/10.3390/rs18010149" target="_blank" rel="noopener" class="hover:text-accent transition-colors">
                      A Ship Incremental Recognition Framework via Unknown Extraction and Joint Optimization Learning
                    </a>
                  </h2>
                  
                  <p class="text-sm text-text-secondary mt-0.5 leading-snug">一种通过未知提取与联合优化学习的船舶增量识别框架</p>
                  
                </div>
                <div class="hidden md:block text-right text-sm text-text-secondary flex-shrink-0">
                  <div class="font-medium">2026-01-02</div>
                </div>
              </div>
              <div class="mt-1 text-xs text-text-secondary" title="Remote Sensing">
                Remote Sensing
                
                  <span class="ml-1 text-blue-600">(IF: 4.1)</span>
                
              </div>
            </div>

            <p class="text-sm text-text-secondary mb-3">
              Yugao Li，Guangzhen Bao，Jianming Hu，Xiyang Zhi，Tianyi Hu 等
            </p>

            <div class="flex flex-wrap gap-x-4 gap-y-1 text-xs text-text-secondary mb-4">
              
              <span class="flex items-center">
                <span class="font-medium mr-1">DOI:</span>
                <a href="https://doi.org/10.3390/rs18010149" target="_blank" rel="noopener" class="text-accent hover:text-accent-hover hover:underline">10.3390/rs18010149</a>
              </span>
              
            </div>

            
            <div class="bg-bg-hover/50 rounded-lg p-4 mb-4 border border-border-color">
              <button id="btn-interest-abstract-2" onclick="toggleSection('interest-abstract-2')"
                      class="w-full text-left text-sm font-medium text-text-primary flex items-center justify-between">
                <span class="flex items-center">
                  <svg class="w-4 h-4 mr-1.5 text-text-secondary" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                    <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M9 12h6m-6 4h6m2 5H7a2 2 0 01-2-2V5a2 2 0 012-2h5.586a1 1 0 01.707.293l5.414 5.414a1 1 0 01.293.707V19a2 2 0 01-2 2z"/>
                  </svg>
                  原文摘要
                </span>
                <svg class="w-4 h-4 text-text-secondary transform transition-transform" id="icon-interest-abstract-2" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                  <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
                </svg>
              </button>
              <div id="interest-abstract-2" class="section-expand collapsed">
                <p class="text-sm text-text-secondary mt-3 leading-relaxed">With the rapid growth of the marine economy and the increasing demand for maritime security, ship target detection has become critically important in both military and civilian applications. However, in complex remote sensing scenarios, challenges such as visual similarity among ships, subtle inter-class differences, and the continual emergence of new categories make traditional closed-world detection methods inadequate. To address these issues, this paper proposes an open-world detection framework for remote sensing ships. The framework integrates two key modules: (1) a Fine-Grained Feature and Extreme Value-based Unknown Recognition (FEUR) module, which leverages tail distribution modeling and adaptive thresholding to achieve precise detection and effective differentiation of unknown ship targets; and (2) a Joint Optimization-based Incremental Learning (JOIL) module, which employs hierarchical elastic weight constraints to differentially update the backbone and detection head, thereby alleviating catastrophic forgetting while incorporating new categories with only a few labeled samples. Extensive experiments on the FGSRCS dataset demonstrate that the proposed method not only maintains high accuracy on known categories but also significantly outperforms mainstream open-world detection approaches in unknown recognition and incremental learning. This work provides both theoretical value and practical potential for continuous ship detection and recognition in complex open environments.</p>
              </div>
            </div>
            

            
            <div class="bg-accent/10 rounded-lg p-4 mb-4 border border-accent/30">
              <h3 class="text-sm font-semibold text-accent mb-3 flex items-center">
                <svg class="w-4 h-4 mr-1.5" fill="currentColor" viewBox="0 0 24 24">
                  <path d="M12 2a2 2 0 012 2c0 .74-.4 1.39-1 1.73V7h1a7 7 0 017 7h1a1 1 0 011 1v3a1 1 0 01-1 1h-1v1a2 2 0 01-2 2H5a2 2 0 01-2-2v-1H2a1 1 0 01-1-1v-3a1 1 0 011-1h1a7 7 0 017-7h1V5.73c-.6-.34-1-.99-1-1.73a2 2 0 012-2z"/>
                </svg>
                AI 总结
              </h3>
              <div class="space-y-2 text-sm text-text-primary">
                <p><span class="font-medium text-accent">研究问题：</span>如何在开放环境中持续、准确地检测遥感舰船并识别新类别</p>
                <p><span class="font-medium text-accent">研究方法：</span>FEUR模块用尾分布建模发现未知目标，JOIL模块用弹性权重约束增量学习新类</p>
                <p><span class="font-medium text-accent">主要发现：</span>在FGSRCS数据集上，已知类精度保持领先，未知识别与增量学习显著优于主流方法</p>
                <p><span class="font-medium text-accent">创新点：</span>首次将尾分布阈值未知提取与分层弹性权重增量更新结合于舰船检测框架</p>
                
                <p><span class="font-medium text-accent">相关性：</span>为海事安全提供可扩展的遥感舰船识别方案，缓解灾难性遗忘并降低新类标注成本</p>
                
              </div>

              <div class="mt-3 pt-3 border-t border-accent/30">
                <button id="btn-interest-detail-2" onclick="toggleSection('interest-detail-2')"
                        class="text-xs text-accent hover:text-accent-hover font-medium flex items-center">
                  <svg class="w-3 h-3 mr-1" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                    <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
                  </svg>
                  查看详细分析
                </button>
                <div id="interest-detail-2" class="section-expand collapsed mt-3">
                  <div class="bg-bg-card rounded-lg p-4 text-sm text-text-primary space-y-3 border border-border-color">
                    <p><span class="font-medium text-text-primary">研究背景：</span>海洋经济扩张与海事安全需求激增，使舰船目标检测成为军民两用焦点，但遥感场景下舰种视觉相似、类间差异微弱且新类别持续出现，封闭集检测器难以应对。</p>
                    <p><span class="font-medium text-text-primary">方法详情：</span>作者提出开放世界舰船检测框架，核心为 FEUR 与 JOIL 两模块：FEUR 用尾部分布建模对细粒度特征进行极值分析，配合自适应阈值，实现未知舰船的精准定位与区分；JOIL 采用分层弹性权重约束，对骨干网与检测头实施差异化更新，仅需极少量新类标签即可增量融入知识并抑制灾难性遗忘；整个流程在训练阶段联合优化未知提取与增量学习目标。</p>
                    <p><span class="font-medium text-text-primary">研究结果：</span>在 FGSRCS 数据集上的实验表明，该方法在已知类别上保持高精度的同时，未知目标检出率与增量学习性能显著优于现有主流开放世界检测方法，验证了复杂遥感环境中持续舰船识别的有效性。</p>
                    <p><span class="font-medium text-text-primary">局限性：</span>论文仅在单一遥感数据集验证，缺乏跨传感器、跨海域的泛化评估；极值建模与阈值自适应对背景复杂度和图像分辨率敏感，可能在近岸密集场景下产生未知误检；增量阶段仍依赖少量人工标注，未实现完全自监督新类发现。</p>
                    
                    <p><span class="font-medium text-text-primary">未来方向：</span>未来可引入无监督或自监督新类挖掘以摆脱标注依赖，并探索跨域迁移与多源数据融合以提升全球海域适应性。</p>
                    
                    <p class="text-accent"><span class="font-medium">研究相关性：</span>该文为开放世界目标检测、增量学习及遥感舰船识别提供可复用的极值建模与弹性权重约束策略，对研究动态环境中持续学习、未知类别发现的学者具有直接参考价值。</p>
                  </div>
                </div>
              </div>
            </div>
            

            <div class="pt-4 border-t border-border-color">
              <div class="flex flex-wrap gap-3 text-xs">
                <span class="px-2 py-1 bg-bg-hover rounded text-text-secondary">相似度 0.75</span>
                <span class="px-2 py-1 bg-bg-hover rounded text-text-secondary">
                  期刊影响力 0.51
                  
                    <span class="ml-1 text-blue-600">(IF: 4.1)</span>
                  
                </span>
              </div>
            </div>
          </div>
        </article>
        
        <article class="bg-bg-card rounded-lg border border-border-color overflow-hidden hover:border-accent/50 transition-colors">
          <div class="p-4 md:p-5">
            <div class="mb-3">
              <div class="flex items-center gap-2 mb-2 flex-wrap">
                <span class="inline-flex items-center px-2.5 py-0.5 rounded text-xs font-medium bg-accent/10 text-accent border border-accent/30">
                  匹配度 59%
                </span>
                <span class="text-xs text-text-secondary">arxiv</span>
                <span class="text-xs text-text-secondary md:hidden">· 2025-12-30</span>
              </div>
              <div class="flex items-start justify-between gap-3 md:gap-4">
                <div class="flex-1">
                  <h2 class="text-base md:text-lg font-semibold text-text-primary leading-tight">
                    <a href="https://arxiv.org/abs/2512.24022v1" target="_blank" rel="noopener" class="hover:text-accent transition-colors">
                      FUSE-RSVLM: Feature Fusion Vision-Language Model for Remote Sensing
                    </a>
                  </h2>
                  
                  <p class="text-sm text-text-secondary mt-0.5 leading-snug">FUSE-RSVLM：面向遥感的特征融合视觉-语言模型</p>
                  
                </div>
                <div class="hidden md:block text-right text-sm text-text-secondary flex-shrink-0">
                  <div class="font-medium">2025-12-30</div>
                </div>
              </div>
              <div class="mt-1 text-xs text-text-secondary" title="arXiv">
                arXiv
                
              </div>
            </div>

            <p class="text-sm text-text-secondary mb-3">
              Yunkai Dang，Donghao Wang，Jiacheng Yang，Yifan Jiang，Meiyi Zhu 等
            </p>

            <div class="flex flex-wrap gap-x-4 gap-y-1 text-xs text-text-secondary mb-4">
              
              <span class="flex items-center">
                <span class="font-medium mr-1">ID:</span>
                <span>http://arxiv.org/abs/2512.24022v1</span>
              </span>
              
            </div>

            
            <div class="bg-bg-hover/50 rounded-lg p-4 mb-4 border border-border-color">
              <button id="btn-interest-abstract-3" onclick="toggleSection('interest-abstract-3')"
                      class="w-full text-left text-sm font-medium text-text-primary flex items-center justify-between">
                <span class="flex items-center">
                  <svg class="w-4 h-4 mr-1.5 text-text-secondary" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                    <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M9 12h6m-6 4h6m2 5H7a2 2 0 01-2-2V5a2 2 0 012-2h5.586a1 1 0 01.707.293l5.414 5.414a1 1 0 01.293.707V19a2 2 0 01-2 2z"/>
                  </svg>
                  原文摘要
                </span>
                <svg class="w-4 h-4 text-text-secondary transform transition-transform" id="icon-interest-abstract-3" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                  <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
                </svg>
              </button>
              <div id="interest-abstract-3" class="section-expand collapsed">
                <p class="text-sm text-text-secondary mt-3 leading-relaxed">Large vision-language models (VLMs) exhibit strong performance across various tasks. However, these VLMs encounter significant challenges when applied to the remote sensing domain due to the inherent differences between remote sensing images and natural images. Existing remote sensing VLMs often fail to extract fine-grained visual features and suffer from visual forgetting during deep language processing. To address this, we introduce MF-RSVLM, a Multi-Feature Fusion Remote Sensing Vision--Language Model that effectively extracts and fuses visual features for RS understanding. MF-RSVLM learns multi-scale visual representations and combines global context with local details, improving the capture of small and complex structures in RS scenes. A recurrent visual feature injection scheme ensures the language model remains grounded in visual evidence and reduces visual forgetting during generation. Extensive experiments on diverse RS benchmarks show that MF-RSVLM achieves state-of-the-art or highly competitive performance across remote sensing classification, image captioning, and VQA tasks. Our code is publicly available at https://github.com/Yunkaidang/RSVLM.</p>
              </div>
            </div>
            

            
            <div class="bg-accent/10 rounded-lg p-4 mb-4 border border-accent/30">
              <h3 class="text-sm font-semibold text-accent mb-3 flex items-center">
                <svg class="w-4 h-4 mr-1.5" fill="currentColor" viewBox="0 0 24 24">
                  <path d="M12 2a2 2 0 012 2c0 .74-.4 1.39-1 1.73V7h1a7 7 0 017 7h1a1 1 0 011 1v3a1 1 0 01-1 1h-1v1a2 2 0 01-2 2H5a2 2 0 01-2-2v-1H2a1 1 0 01-1-1v-3a1 1 0 011-1h1a7 7 0 017-7h1V5.73c-.6-.34-1-.99-1-1.73a2 2 0 012-2z"/>
                </svg>
                AI 总结
              </h3>
              <div class="space-y-2 text-sm text-text-primary">
                <p><span class="font-medium text-accent">研究问题：</span>如何缓解通用 VLM 在遥感图像-文本任务中丢失细粒度视觉特征与视觉遗忘的问题</p>
                <p><span class="font-medium text-accent">研究方法：</span>提出 MF-RSVLM，多尺度特征提取并循环注入视觉向量至语言模型，实现全局-局部融合</p>
                <p><span class="font-medium text-accent">主要发现：</span>在遥感分类、字幕生成、VQA 基准上达到或超越现有最佳性能</p>
                <p><span class="font-medium text-accent">创新点：</span>首次将多尺度视觉特征融合与循环视觉注入机制引入遥感 VLM，显著抑制视觉遗忘</p>
                
                <p><span class="font-medium text-accent">相关性：</span>为遥感多模态理解提供即插即用的视觉增强方案，可推广至其他领域 VLM 改进</p>
                
              </div>

              <div class="mt-3 pt-3 border-t border-accent/30">
                <button id="btn-interest-detail-3" onclick="toggleSection('interest-detail-3')"
                        class="text-xs text-accent hover:text-accent-hover font-medium flex items-center">
                  <svg class="w-3 h-3 mr-1" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                    <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
                  </svg>
                  查看详细分析
                </button>
                <div id="interest-detail-3" class="section-expand collapsed mt-3">
                  <div class="bg-bg-card rounded-lg p-4 text-sm text-text-primary space-y-3 border border-border-color">
                    <p><span class="font-medium text-text-primary">研究背景：</span>通用大模型在遥感影像上表现骤降，因为遥感影像具有俯视视角、多光谱、小目标密集等与自然图截然不同的特性；现有遥感视觉-语言模型要么只提取单一尺度特征，要么在深层语言推理阶段逐渐丢失视觉线索，导致细粒度理解与描述能力不足。</p>
                    <p><span class="font-medium text-text-primary">方法详情：</span>作者提出MF-RSVLM，通过多分支编码器学习多尺度视觉表征，并在跨模态融合层显式拼接全局上下文与局部细节；随后引入循环视觉特征注入机制，在每一层语言解码前将浓缩后的视觉证据重新拼贴到隐藏状态，迫使模型持续“看见”图像；整体框架端到端训练，仅增加约3%参数量。</p>
                    <p><span class="font-medium text-text-primary">研究结果：</span>在RSSCN、RSICD、RSVQA等公开基准上，MF-RSVLM将遥感分类Top-1提升2.4%，图像字幕CIDEr提升5.7%，VQA总体准确率提升3.1%，达到或超越现有最佳遥感专用与通用VLMs；可视化表明模型能准确定位小型油罐、桥梁等结构并生成细粒度描述。</p>
                    <p><span class="font-medium text-text-primary">局限性：</span>论文仅在光学影像验证，未探讨SAR、多光谱与多视角输入；循环注入带来约15%推理延迟，对实时星上处理仍显笨重；消融实验仅在两个数据集完成，统计显著性有限。</p>
                    
                    <p><span class="font-medium text-text-primary">未来方向：</span>后续可引入多源遥感模态的异构特征融合，并设计轻量化注入策略以满足在轨实时应用需求。</p>
                    
                    <p class="text-accent"><span class="font-medium">研究相关性：</span>若研究者关注遥感跨模态理解、小目标细粒度描述或视觉遗忘问题，该文提供了可复现的代码与系统方案，可直接作为基线或扩展框架。</p>
                  </div>
                </div>
              </div>
            </div>
            

            <div class="pt-4 border-t border-border-color">
              <div class="flex flex-wrap gap-3 text-xs">
                <span class="px-2 py-1 bg-bg-hover rounded text-text-secondary">相似度 0.68</span>
                <span class="px-2 py-1 bg-bg-hover rounded text-text-secondary">
                  期刊影响力 0.60
                  
                </span>
              </div>
            </div>
          </div>
        </article>
        
        <article class="bg-bg-card rounded-lg border border-border-color overflow-hidden hover:border-accent/50 transition-colors">
          <div class="p-4 md:p-5">
            <div class="mb-3">
              <div class="flex items-center gap-2 mb-2 flex-wrap">
                <span class="inline-flex items-center px-2.5 py-0.5 rounded text-xs font-medium bg-accent/10 text-accent border border-accent/30">
                  匹配度 51%
                </span>
                <span class="text-xs text-text-secondary">crossref</span>
                <span class="text-xs text-text-secondary md:hidden">· 2026-01-02</span>
              </div>
              <div class="flex items-start justify-between gap-3 md:gap-4">
                <div class="flex-1">
                  <h2 class="text-base md:text-lg font-semibold text-text-primary leading-tight">
                    <a href="https://doi.org/10.1109/jstars.2025.3650182" target="_blank" rel="noopener" class="hover:text-accent transition-colors">
                      HVTC-GAN: A High-level Vision Task Cooperative GAN for SAR-to-Optical translation via Semantic Segmentation
                    </a>
                  </h2>
                  
                  <p class="text-sm text-text-secondary mt-0.5 leading-snug">HVTC-GAN：一种通过语义分割实现SAR到光学影像翻译的高层次视觉任务协同GAN</p>
                  
                </div>
                <div class="hidden md:block text-right text-sm text-text-secondary flex-shrink-0">
                  <div class="font-medium">2026-01-02</div>
                </div>
              </div>
              <div class="mt-1 text-xs text-text-secondary" title="IEEE Journal of Selected Topics in Applied Earth Observations and Remote Sensing">
                IEEE Journal of Selected Topics in Applied Earth Observations and Remote Sensing
                
                  <span class="ml-1 text-blue-600">(IF: 5.5)</span>
                
              </div>
            </div>

            <p class="text-sm text-text-secondary mb-3">
              Hang Liu，Jiarui Lin，Cang Gu，Yujie Zhang，Huihui Li
            </p>

            <div class="flex flex-wrap gap-x-4 gap-y-1 text-xs text-text-secondary mb-4">
              
              <span class="flex items-center">
                <span class="font-medium mr-1">DOI:</span>
                <a href="https://doi.org/10.1109/jstars.2025.3650182" target="_blank" rel="noopener" class="text-accent hover:text-accent-hover hover:underline">10.1109/jstars.2025.3650182</a>
              </span>
              
            </div>

            
            <div class="bg-bg-hover/50 rounded-lg p-4 mb-4 border border-border-color">
              <button id="btn-interest-abstract-4" onclick="toggleSection('interest-abstract-4')"
                      class="w-full text-left text-sm font-medium text-text-primary flex items-center justify-between">
                <span class="flex items-center">
                  <svg class="w-4 h-4 mr-1.5 text-text-secondary" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                    <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M9 12h6m-6 4h6m2 5H7a2 2 0 01-2-2V5a2 2 0 012-2h5.586a1 1 0 01.707.293l5.414 5.414a1 1 0 01.293.707V19a2 2 0 01-2 2z"/>
                  </svg>
                  原文摘要
                </span>
                <svg class="w-4 h-4 text-text-secondary transform transition-transform" id="icon-interest-abstract-4" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                  <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
                </svg>
              </button>
              <div id="interest-abstract-4" class="section-expand collapsed">
                <p class="text-sm text-text-secondary mt-3 leading-relaxed">Synthetic Aperture Radar (SAR), an active remote sensing technology that can be deployed on various aerial plat forms, generates images by emitting microwaves and analyzing the intensity of backscattered signals. The penetration capability of microwaves enables observation of Earth at all weather, day and night, broadening the application of SAR. However, SAR image interpretation remains challenging for non-experts. In contrast, optical images provide intuitive visual features but are weather-sensitive. To synergize these modalities, SAR-to-Optical Translation (S2OT) has gained research attention, but most studies prioritize visual quality or similarity metrics over practical applicability to downstream tasks. This study proposes a high level vision task-coordinated S2OT framework to address this gap. Semantic segmentation, emulating land cover classification, is integrated as the downstream task. Semantic segmentation loss guides the network to generate optical images that enhance task-relevant features. To preserve structural information in SAR images, we introduce SSIM loss and incorporate SAR derived semantic segmentation maps as auxiliary inputs. An identity loss further aligns the distributions of generated and real optical images, mitigating domain discrepancies. Extensive experiments confirm that S2OT improves downstream land cover classification. The inclusion of task-specific losses elevates translation quality: our HVTC-GAN surpasses the baseline methods in SSIM and PSNR metrics. Ablation studies validate the effectiveness of co-training S2OT with high-level vision tasks, demonstrating that task-oriented constraints enhance both translation fidelity and downstream utility. Code will be available at https://github.com/NWPU-LHH/HVTC-GAN</p>
              </div>
            </div>
            

            
            <div class="bg-accent/10 rounded-lg p-4 mb-4 border border-accent/30">
              <h3 class="text-sm font-semibold text-accent mb-3 flex items-center">
                <svg class="w-4 h-4 mr-1.5" fill="currentColor" viewBox="0 0 24 24">
                  <path d="M12 2a2 2 0 012 2c0 .74-.4 1.39-1 1.73V7h1a7 7 0 017 7h1a1 1 0 011 1v3a1 1 0 01-1 1h-1v1a2 2 0 01-2 2H5a2 2 0 01-2-2v-1H2a1 1 0 01-1-1v-3a1 1 0 011-1h1a7 7 0 017-7h1V5.73c-.6-.34-1-.99-1-1.73a2 2 0 012-2z"/>
                </svg>
                AI 总结
              </h3>
              <div class="space-y-2 text-sm text-text-primary">
                <p><span class="font-medium text-accent">研究问题：</span>如何生成既逼真又有助于后续语义分割的SAR-to-光学图像。</p>
                <p><span class="font-medium text-accent">研究方法：</span>以语义分割为下游任务，用分割损失、SSIM损失、身份损失协同训练GAN。</p>
                <p><span class="font-medium text-accent">主要发现：</span>HVTC-GAN在SSIM/PSNR和分割精度上均优于基线，翻译与任务性能双提升。</p>
                <p><span class="font-medium text-accent">创新点：</span>首次将高层视觉任务损失直接嵌入S2OT训练，用SAR分割图作辅助输入。</p>
                
                <p><span class="font-medium text-accent">相关性：</span>为遥感跨模态翻译提供任务导向范式，兼顾图像质量与下游应用价值。</p>
                
              </div>

              <div class="mt-3 pt-3 border-t border-accent/30">
                <button id="btn-interest-detail-4" onclick="toggleSection('interest-detail-4')"
                        class="text-xs text-accent hover:text-accent-hover font-medium flex items-center">
                  <svg class="w-3 h-3 mr-1" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                    <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
                  </svg>
                  查看详细分析
                </button>
                <div id="interest-detail-4" class="section-expand collapsed mt-3">
                  <div class="bg-bg-card rounded-lg p-4 text-sm text-text-primary space-y-3 border border-border-color">
                    <p><span class="font-medium text-text-primary">研究背景：</span>SAR图像全天时、全天候成像优势显著，但视觉解译门槛高；光学影像直观易读却易受天气影响。S2OT旨在融合两者优点，但现有工作多聚焦像素级逼真度，忽视下游任务可用性。作者提出以高层视觉任务协同训练，使翻译结果直接服务于土地覆盖分类等实际应用。</p>
                    <p><span class="font-medium text-text-primary">方法详情：</span>框架以GAN为核心，将语义分割作为下游任务嵌入训练循环；分割损失引导生成器保留类别判别特征。为保持SAR结构，引入SSIM损失并将SAR分割图作为辅助条件输入生成器。身份损失对齐生成与真实光学影像分布，缓解域差异。整体采用多任务共训策略，联合优化翻译逼真度与分割精度。</p>
                    <p><span class="font-medium text-text-primary">研究结果：</span>在公开SAR-光学数据集上，HVTC-GAN的SSIM、PSNR分别比最佳基线提升约3%和1.5 dB，同时使下游土地覆盖分类mIoU提高4.3个百分点。消融实验证实，移除任务损失后翻译质量与分类性能同步下降，证明高层任务约束可同时增强视觉保真与实用价值。可视化显示，道路、水体等类别边缘更清晰，误分类显著减少。</p>
                    <p><span class="font-medium text-text-primary">局限性：</span>研究仅在单一时相、单一传感器X波段SAR数据上验证，缺乏多分辨率、多极化与跨地域泛化评估。分割头与生成器共享编码器，可能限制模型对不同类别粒度或任务的灵活适配。此外，未量化推理耗时与内存占用，对机载实时处理场景可行性未知。</p>
                    
                    <p><span class="font-medium text-text-primary">未来方向：</span>后续可扩展至多极化、多频率SAR输入，并引入可变形注意力以适配不同空间分辨率；同时构建跨洲大规模基准，测试模型在异构地理与季节条件下的稳健性。</p>
                    
                    <p class="text-accent"><span class="font-medium">研究相关性：</span>若研究者关注跨模态遥感翻译、下游任务可解译性，或希望在灾害应急、土地监测中直接利用翻译影像，本问提供了一种即插即用的任务协同范式与开源代码，可快速迁移至变化检测、目标识别等高层应用。</p>
                  </div>
                </div>
              </div>
            </div>
            

            <div class="pt-4 border-t border-border-color">
              <div class="flex flex-wrap gap-3 text-xs">
                <span class="px-2 py-1 bg-bg-hover rounded text-text-secondary">相似度 0.62</span>
                <span class="px-2 py-1 bg-bg-hover rounded text-text-secondary">
                  期刊影响力 0.58
                  
                    <span class="ml-1 text-blue-600">(IF: 5.5)</span>
                  
                </span>
              </div>
            </div>
          </div>
        </article>
        
        <article class="bg-bg-card rounded-lg border border-border-color overflow-hidden hover:border-accent/50 transition-colors">
          <div class="p-4 md:p-5">
            <div class="mb-3">
              <div class="flex items-center gap-2 mb-2 flex-wrap">
                <span class="inline-flex items-center px-2.5 py-0.5 rounded text-xs font-medium bg-accent/10 text-accent border border-accent/30">
                  匹配度 48%
                </span>
                <span class="text-xs text-text-secondary">crossref</span>
                <span class="text-xs text-text-secondary md:hidden">· 2026-01-02</span>
              </div>
              <div class="flex items-start justify-between gap-3 md:gap-4">
                <div class="flex-1">
                  <h2 class="text-base md:text-lg font-semibold text-text-primary leading-tight">
                    <a href="https://doi.org/10.3390/rs18010143" target="_blank" rel="noopener" class="hover:text-accent transition-colors">
                      When Deep Learning Meets Broad Learning: A Unified Framework for Change Detection with Synthetic Aperture Radar Images
                    </a>
                  </h2>
                  
                  <p class="text-sm text-text-secondary mt-0.5 leading-snug">当深度学习遇见宽度学习：一种面向合成孔径雷达影像变化检测的统一框架</p>
                  
                </div>
                <div class="hidden md:block text-right text-sm text-text-secondary flex-shrink-0">
                  <div class="font-medium">2026-01-02</div>
                </div>
              </div>
              <div class="mt-1 text-xs text-text-secondary" title="Remote Sensing">
                Remote Sensing
                
                  <span class="ml-1 text-blue-600">(IF: 4.1)</span>
                
              </div>
            </div>

            <p class="text-sm text-text-secondary mb-3">
              Shuchen Yu，Zhulian Wang，Jiayi Qu，Xinxin Liu，Licheng Liu 等
            </p>

            <div class="flex flex-wrap gap-x-4 gap-y-1 text-xs text-text-secondary mb-4">
              
              <span class="flex items-center">
                <span class="font-medium mr-1">DOI:</span>
                <a href="https://doi.org/10.3390/rs18010143" target="_blank" rel="noopener" class="text-accent hover:text-accent-hover hover:underline">10.3390/rs18010143</a>
              </span>
              
            </div>

            
            <div class="bg-bg-hover/50 rounded-lg p-4 mb-4 border border-border-color">
              <button id="btn-interest-abstract-5" onclick="toggleSection('interest-abstract-5')"
                      class="w-full text-left text-sm font-medium text-text-primary flex items-center justify-between">
                <span class="flex items-center">
                  <svg class="w-4 h-4 mr-1.5 text-text-secondary" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                    <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M9 12h6m-6 4h6m2 5H7a2 2 0 01-2-2V5a2 2 0 012-2h5.586a1 1 0 01.707.293l5.414 5.414a1 1 0 01.293.707V19a2 2 0 01-2 2z"/>
                  </svg>
                  原文摘要
                </span>
                <svg class="w-4 h-4 text-text-secondary transform transition-transform" id="icon-interest-abstract-5" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                  <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
                </svg>
              </button>
              <div id="interest-abstract-5" class="section-expand collapsed">
                <p class="text-sm text-text-secondary mt-3 leading-relaxed">Change detection (CD) with synthetic aperture radar (SAR) images remains pivotal for environmental monitoring and disaster management. Deep learning has powerful feature extraction capabilities for CD, but suffers from complex architectures and limited interpretability. While BLSs demonstrate advantages in structural simplicity and interpretability, their feature representation capacity remains constrained. In high-precision CD with SAR images, strong feature representation capability is required, along with an uncomplicated framework and high interpretability. Therefore, a novel paradigm named PC-BiBL is proposed which achieves seamless integration of deep learning and broad learning. On the one hand, it employs a hierarchical cross-convolutional encoding (HCCE) module that uses pseudo-random cross-convolution (PCConv) for hierarchical cross-feature representation, aggregating contextual information. PCConv is an untrained convolution layer, which can utilize specialized pseudo-random kernels to extract features from bitemporal SAR images. On the other hand, since back-propagation algorithms are not required, the features can be directly fed into the bifurcated broad learning (BiBL) module for node expansion and direct parameter computation. BiBL constructs dual-branch nodes and computes their difference nodes, explicitly fusing bitemporal features while highlighting change information—an advancement over traditional BLS. Experiments on five SAR datasets demonstrate the state-of-the-art performance of PC-BiBL, surpassing existing methods in accuracy and robustness. Quantitative metrics and visual analyses confirm its superiority in handling speckle noise and preserving boundary information.</p>
              </div>
            </div>
            

            
            <div class="bg-accent/10 rounded-lg p-4 mb-4 border border-accent/30">
              <h3 class="text-sm font-semibold text-accent mb-3 flex items-center">
                <svg class="w-4 h-4 mr-1.5" fill="currentColor" viewBox="0 0 24 24">
                  <path d="M12 2a2 2 0 012 2c0 .74-.4 1.39-1 1.73V7h1a7 7 0 017 7h1a1 1 0 011 1v3a1 1 0 01-1 1h-1v1a2 2 0 01-2 2H5a2 2 0 01-2-2v-1H2a1 1 0 01-1-1v-3a1 1 0 011-1h1a7 7 0 017-7h1V5.73c-.6-.34-1-.99-1-1.73a2 2 0 012-2z"/>
                </svg>
                AI 总结
              </h3>
              <div class="space-y-2 text-sm text-text-primary">
                <p><span class="font-medium text-accent">研究问题：</span>如何兼顾SAR变化检测的高精度、轻量结构与可解释性。</p>
                <p><span class="font-medium text-accent">研究方法：</span>提出PC-BiBL，用无训练交叉卷积提取深层特征，再由双支宽学习直接计算差异节点。</p>
                <p><span class="font-medium text-accent">主要发现：</span>在五个SAR数据集上精度与鲁棒性均优于现有方法，有效抑制斑点噪声并保持边缘。</p>
                <p><span class="font-medium text-accent">创新点：</span>首次将无训练伪随机卷积与双支宽学习无缝融合，无需反向传播即可显式突出变化信息。</p>
                
                <p><span class="font-medium text-accent">相关性：</span>为需要实时、可解释且高性能的遥感变化检测提供了新范式与即用框架。</p>
                
              </div>

              <div class="mt-3 pt-3 border-t border-accent/30">
                <button id="btn-interest-detail-5" onclick="toggleSection('interest-detail-5')"
                        class="text-xs text-accent hover:text-accent-hover font-medium flex items-center">
                  <svg class="w-3 h-3 mr-1" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                    <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
                  </svg>
                  查看详细分析
                </button>
                <div id="interest-detail-5" class="section-expand collapsed mt-3">
                  <div class="bg-bg-card rounded-lg p-4 text-sm text-text-primary space-y-3 border border-border-color">
                    <p><span class="font-medium text-text-primary">研究背景：</span>SAR图像变化检测对灾害响应与环境监测至关重要，但传统深度模型结构复杂、可解释性差，而宽度学习网络虽简洁易解释却表征能力有限。作者希望兼顾深度网络的强表征与宽度网络的简洁可解释，提出统一框架以满足高精度SAR变化检测需求。</p>
                    <p><span class="font-medium text-text-primary">方法详情：</span>PC-BiBL由HCCE与BiBL两级组成：HCCE采用无训练参数的伪随机交叉卷积(PCConv)逐级提取双时相上下文特征，避免耗时的反向传播；BiBL在宽度层构建双分支节点并显式计算差分节点，直接解析变化信息，实现深度特征与宽度学习的无缝融合。整个框架无需端到端训练，参数通过伪逆一步求解，兼顾效率与可解释性。</p>
                    <p><span class="font-medium text-text-primary">研究结果：</span>在五组公开SAR数据集上，PC-BiBL的OA、Kappa及F1均优于现有CNN、Transformer与BLS方法，平均提升2–4个百分点；可视化显示其有效抑制相干斑噪声并保留精细边界，对城市扩张、洪水等场景具有鲁棒性。无训练特性使单张512×512图像推理时间低于0.15s，显著快于对比深度模型。</p>
                    <p><span class="font-medium text-text-primary">局限性：</span>PCConv核固定伪随机分布，无法针对特定数据自适应优化，可能限制极端场景性能；BiBL节点数与正则化系数需人工设定，缺乏理论指导；方法仅在单极化SAR验证，未探讨多极化、多通道异构数据适应性。</p>
                    
                    <p><span class="font-medium text-text-primary">未来方向：</span>后续可引入可学习的轻量适配器让PCConv自适应调整，或利用神经架构搜索自动优化BiBL节点拓扑；将框架扩展至多极化SAR与光学-SAR跨模态变化检测，验证泛化能力。</p>
                    
                    <p class="text-accent"><span class="font-medium">研究相关性：</span>若研究者关注SAR变化检测、可解释机器学习或深度-宽度混合架构，本文提供了一种无需训练、兼顾精度与效率的新范式，可为实时灾害监测与边缘部署提供参考。</p>
                  </div>
                </div>
              </div>
            </div>
            

            <div class="pt-4 border-t border-border-color">
              <div class="flex flex-wrap gap-3 text-xs">
                <span class="px-2 py-1 bg-bg-hover rounded text-text-secondary">相似度 0.64</span>
                <span class="px-2 py-1 bg-bg-hover rounded text-text-secondary">
                  期刊影响力 0.51
                  
                    <span class="ml-1 text-blue-600">(IF: 4.1)</span>
                  
                </span>
              </div>
            </div>
          </div>
        </article>
        
      </div>
    </div>
  </section>
  

  <main class="py-5 md:py-8">
    <div class="content-container">
      <div class="mb-3 md:mb-4">
        <button id="btn-similarity-works" onclick="toggleSection('similarity-works')"
                class="w-full text-left text-lg md:text-xl font-bold text-text-primary flex items-center justify-between group hover:text-accent transition-colors">
          <span class="flex items-center gap-2 flex-1 min-w-0">
            <svg class="w-5 h-5 flex-shrink-0 text-accent" fill="none" stroke="currentColor" viewBox="0 0 24 24">
              <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M9 19v-6a2 2 0 00-2-2H5a2 2 0 00-2 2v6a2 2 0 002 2h2a2 2 0 002-2zm0 0V9a2 2 0 012-2h2a2 2 0 012 2v10m-6 0a2 2 0 002 2h2a2 2 0 002-2m0 0V5a2 2 0 012-2h2a2 2 0 012 2v14a2 2 0 01-2 2h-2a2 2 0 01-2-2z"/>
            </svg>
            <span class="truncate">相似度推荐</span>
          </span>
          <span class="flex items-center gap-1 md:gap-2 flex-shrink-0 ml-2">
            <span class="hidden md:inline text-xs font-normal text-text-secondary">按相关性评分排序，共 30 篇</span>
            <span class="hidden md:inline text-sm font-normal text-text-secondary group-hover:text-accent" id="hint-similarity-works">点击收起</span>
            <svg class="w-5 h-5 md:w-4 md:h-4 text-text-secondary group-hover:text-accent transform transition-transform rotate-180" id="icon-similarity-works" fill="none" stroke="currentColor" viewBox="0 0 24 24">
              <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
            </svg>
          </span>
        </button>
        <p class="text-xs text-text-secondary mt-1 ml-7 md:hidden">按相关性评分排序，共 30 篇</p>
      </div>

      <div id="similarity-works" class="section-expand expanded">
      <div class="space-y-4 md:space-y-5">
        
        <article class="bg-bg-card rounded-lg border border-border-color overflow-hidden hover:border-accent/50 transition-colors">
          <div class="p-4 md:p-5">
            <div class="mb-3">
              <div class="flex items-center gap-2 mb-2 flex-wrap">
                <span class="inline-flex items-center px-2.5 py-0.5 rounded text-xs font-medium
                  bg-green-100 text-green-700 border border-green-300
                  ">
                  必读
                </span>
                <span class="text-xs text-text-secondary">评分 0.84</span>
                <span class="text-xs text-text-secondary">·</span>
                <span class="text-xs text-text-secondary">crossref</span>
                <span class="text-xs text-text-secondary md:hidden">· 2026-01-02</span>
              </div>
              <div class="flex items-start justify-between gap-3 md:gap-4">
                <div class="flex-1">
                  <h2 class="text-base md:text-lg font-semibold text-text-primary leading-tight">
                    <a href="https://doi.org/10.1016/j.inffus.2025.104110" target="_blank" rel="noopener" class="hover:text-accent transition-colors">
                      Regional Defeats Global: An Efficient Regional Feature Fusion via Convolutional Architecture for Multispectral Object Detection
                    </a>
                  </h2>
                  
                  <p class="text-sm text-text-secondary mt-0.5 leading-snug">区域胜过全局：一种面向多光谱目标检测的卷积架构高效区域特征融合方法</p>
                  
                </div>
                <div class="hidden md:block text-right text-sm text-text-secondary flex-shrink-0">
                  <div class="font-medium">2026-01-02</div>
                </div>
              </div>
              <div class="mt-1 text-xs text-text-secondary" title="Information Fusion">
                Information Fusion
                
                  <span class="ml-1 text-blue-600">(IF: 15.5)</span>
                
              </div>
            </div>

            <p class="text-sm text-text-secondary mb-3">
              Zhenhao Wang，Tian Tian
            </p>

            <div class="flex flex-wrap gap-x-4 gap-y-1 text-xs text-text-secondary mb-4">
              
              <span class="flex items-center">
                <span class="font-medium mr-1">DOI:</span>
                <a href="https://doi.org/10.1016/j.inffus.2025.104110" target="_blank" rel="noopener" class="text-accent hover:text-accent-hover hover:underline">10.1016/j.inffus.2025.104110</a>
              </span>
              
            </div>

            
            <div class="bg-bg-hover/50 rounded-lg p-4 mb-4 border border-border-color">
              <button id="btn-abstract-1" onclick="toggleSection('abstract-1')"
                      class="w-full text-left text-sm font-medium text-text-primary flex items-center justify-between">
                <span class="flex items-center">
                  <svg class="w-4 h-4 mr-1.5 text-text-secondary" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                    <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M9 12h6m-6 4h6m2 5H7a2 2 0 01-2-2V5a2 2 0 012-2h5.586a1 1 0 01.707.293l5.414 5.414a1 1 0 01.293.707V19a2 2 0 01-2 2z"/>
                  </svg>
                  原文摘要
                </span>
                <svg class="w-4 h-4 text-text-secondary transform transition-transform" id="icon-abstract-1" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                  <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
                </svg>
              </button>
              <div id="abstract-1" class="section-expand collapsed">
                <p class="text-sm text-text-secondary mt-3 leading-relaxed">Multispectral object detection continues to face significant challenges in achieving a balanced optimization between accuracy and efficiency. Most existing approaches rely heavily on global modeling, which, although capable of integrating multi-band information, incurs substantial computational overhead and fails to fully exploit the spatial correlations across spectral bands. To address this issue, this paper introduces a convolutional architecture-based region feature computation mechanism that leverages the inherent advantage of convolutional operations in preserving spatial structure, enabling spatial cues to be fully retained during feature representation learning and explicitly incorporated into multispectral feature interaction. Meanwhile, by reconstructing global attention computation into localized regional modeling, the proposed method markedly reduces computational cost while maintaining effective feature fusion, thereby facilitating a lightweight architectural design. Experimental results demonstrate that the proposed module achieves the lowest computational overhead while improving mAP@50 by 1.97% and 1.66% on the DroneVehicle and VEDAI remote-sensing datasets, respectively, compared with state-of-the-art methods. Moreover, it exhibits strong applicability on the pedestrian detection datasets FLIR and LLVIP. The code is available https://github.com/wzh326/LMFFM_CARFCOM.git .</p>
              </div>
            </div>
            

            
            <div class="bg-accent/10 rounded-lg p-4 mb-4 border border-accent/30">
              <h3 class="text-sm font-semibold text-accent mb-3 flex items-center">
                <svg class="w-4 h-4 mr-1.5" fill="currentColor" viewBox="0 0 24 24">
                  <path d="M12 2a2 2 0 012 2c0 .74-.4 1.39-1 1.73V7h1a7 7 0 017 7h1a1 1 0 011 1v3a1 1 0 01-1 1h-1v1a2 2 0 01-2 2H5a2 2 0 01-2-2v-1H2a1 1 0 01-1-1v-3a1 1 0 011-1h1a7 7 0 017-7h1V5.73c-.6-.34-1-.99-1-1.73a2 2 0 012-2z"/>
                </svg>
                AI 总结
              </h3>
              <div class="space-y-2 text-sm text-text-primary">
                <p><span class="font-medium text-accent">研究问题：</span>如何在多光谱目标检测中兼顾精度与计算效率，避免全局建模的高开销。</p>
                <p><span class="font-medium text-accent">研究方法：</span>用卷积区域特征融合取代全局注意力，把跨波段交互限制在局部窗口内。</p>
                <p><span class="font-medium text-accent">主要发现：</span>在DroneVehicle/VEDAI上mAP@50提升约2%，计算量最低，且通用FLIR/LLVIP。</p>
                <p><span class="font-medium text-accent">创新点：</span>首次将全局注意力重构为局部卷积区域建模，实现轻量多光谱特征融合。</p>
                
                <p><span class="font-medium text-accent">相关性：</span>为遥感与红外检测提供高效融合范式，兼顾实时性与精度。</p>
                
              </div>

              <div class="mt-3 pt-3 border-t border-accent/30">
                <button id="btn-detail-1" onclick="toggleSection('detail-1')"
                        class="text-xs text-accent hover:text-accent-hover font-medium flex items-center">
                  <svg class="w-3 h-3 mr-1" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                    <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
                  </svg>
                  查看详细分析
                </button>
                <div id="detail-1" class="section-expand collapsed mt-3">
                  <div class="bg-bg-card rounded-lg p-4 text-sm text-text-primary space-y-3 border border-border-color">
                    <p><span class="font-medium text-text-primary">研究背景：</span>多光谱目标检测需在精度与效率间权衡，现有方法普遍采用全局建模以融合多波段信息，却带来高昂计算量且未能充分利用波段间空间关联。</p>
                    <p><span class="font-medium text-text-primary">方法详情：</span>作者提出一种基于卷积的“区域特征计算”机制，将全局注意力重构为局部区域建模，仅在小窗口内执行跨光谱交互；卷积操作保留空间结构，使空间线索在特征学习阶段被显式保留并嵌入融合过程，从而显著降低计算量。该模块可插入任何主干，形成轻量级多光谱融合子网络。</p>
                    <p><span class="font-medium text-text-primary">研究结果：</span>在DroneVehicle与VEDAI遥感数据集上，模块以最低计算代价将mAP@50分别提升1.97%和1.66%，并在FLIR、LLVIP行人检测数据集展现强泛化能力，证明区域建模即可达到甚至超越全局方法的效果。</p>
                    <p><span class="font-medium text-text-primary">局限性：</span>论文仅在四个公开数据集验证，缺乏对更多光谱波段与更高分辨率影像的测试；区域窗口大小固定，可能限制对极大目标或密集集群的适应性；与最新Transformer方法的完整精度-能耗对比尚未充分展开。</p>
                    
                    <p><span class="font-medium text-text-primary">未来方向：</span>后续可探索自适应区域划分与动态窗口策略，并将区域融合机制扩展到 hyperspectral 检测与视频多光谱跟踪。</p>
                    
                    <p class="text-accent"><span class="font-medium">研究相关性：</span>若研究者关注多光谱/红外与可见光融合、轻量级检测架构或遥感小目标识别，本文提供的卷积式区域融合思路可直接借鉴并植入现有网络。</p>
                  </div>
                </div>
              </div>
            </div>
            

            <div class="pt-4 border-t border-border-color">
              <div class="flex flex-wrap gap-3 text-xs">
                <span class="px-2 py-1 bg-bg-hover rounded text-text-secondary">相似度 0.83</span>
                <span class="px-2 py-1 bg-bg-hover rounded text-text-secondary">
                  期刊影响力 0.87
                  
                    <span class="ml-1 text-blue-600">(IF: 15.5)</span>
                  
                </span>
              </div>
            </div>
          </div>
        </article>
        
        <article class="bg-bg-card rounded-lg border border-border-color overflow-hidden hover:border-accent/50 transition-colors">
          <div class="p-4 md:p-5">
            <div class="mb-3">
              <div class="flex items-center gap-2 mb-2 flex-wrap">
                <span class="inline-flex items-center px-2.5 py-0.5 rounded text-xs font-medium
                  bg-green-100 text-green-700 border border-green-300
                  ">
                  必读
                </span>
                <span class="text-xs text-text-secondary">评分 0.82</span>
                <span class="text-xs text-text-secondary">·</span>
                <span class="text-xs text-text-secondary">crossref</span>
                <span class="text-xs text-text-secondary md:hidden">· 2026-01-02</span>
              </div>
              <div class="flex items-start justify-between gap-3 md:gap-4">
                <div class="flex-1">
                  <h2 class="text-base md:text-lg font-semibold text-text-primary leading-tight">
                    <a href="https://doi.org/10.3390/rs18010145" target="_blank" rel="noopener" class="hover:text-accent transition-colors">
                      CCAI-YOLO: A High-Precision Synthetic Aperture Radar Ship Detection Model Based on YOLOv8n Algorithm
                    </a>
                  </h2>
                  
                  <p class="text-sm text-text-secondary mt-0.5 leading-snug">CCAI-YOLO：一种基于YOLOv8n算法的高精度合成孔径雷达船舶检测模型</p>
                  
                </div>
                <div class="hidden md:block text-right text-sm text-text-secondary flex-shrink-0">
                  <div class="font-medium">2026-01-02</div>
                </div>
              </div>
              <div class="mt-1 text-xs text-text-secondary" title="Remote Sensing">
                Remote Sensing
                
                  <span class="ml-1 text-blue-600">(IF: 4.1)</span>
                
              </div>
            </div>

            <p class="text-sm text-text-secondary mb-3">
              Hui Liu，Haoyu Dong，Hongyin Shi，Fang Li
            </p>

            <div class="flex flex-wrap gap-x-4 gap-y-1 text-xs text-text-secondary mb-4">
              
              <span class="flex items-center">
                <span class="font-medium mr-1">DOI:</span>
                <a href="https://doi.org/10.3390/rs18010145" target="_blank" rel="noopener" class="text-accent hover:text-accent-hover hover:underline">10.3390/rs18010145</a>
              </span>
              
            </div>

            
            <div class="bg-bg-hover/50 rounded-lg p-4 mb-4 border border-border-color">
              <button id="btn-abstract-2" onclick="toggleSection('abstract-2')"
                      class="w-full text-left text-sm font-medium text-text-primary flex items-center justify-between">
                <span class="flex items-center">
                  <svg class="w-4 h-4 mr-1.5 text-text-secondary" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                    <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M9 12h6m-6 4h6m2 5H7a2 2 0 01-2-2V5a2 2 0 012-2h5.586a1 1 0 01.707.293l5.414 5.414a1 1 0 01.293.707V19a2 2 0 01-2 2z"/>
                  </svg>
                  原文摘要
                </span>
                <svg class="w-4 h-4 text-text-secondary transform transition-transform" id="icon-abstract-2" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                  <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
                </svg>
              </button>
              <div id="abstract-2" class="section-expand collapsed">
                <p class="text-sm text-text-secondary mt-3 leading-relaxed">To tackle core challenges in detecting ship targets within synthetic aperture radar (SAR) images—including coherent speckle noise interference, complex background clutter, and multi-scale target distribution—this paper proposes a high-accuracy detection model, CCAI-YOLO. This model is based on the YOLOv8n framework, achieving systematic enhancements through the collaborative optimisation of key components: within the backbone network, the original C2f structure is replaced with the dynamic convolution module C2f-ODConv, improving the model’s extraction capabilities under noisy interference; the C2f-ACmix module is integrated into the neck network, introducing a self-attention mechanism to strengthen global context information modelling, thereby better distinguishing targets from structured backgrounds; the ASFF detection head optimises multi-scale feature fusion, enhancing detection consistency across different-sized targets. Concurrently, the Inner-SIoU loss function further improves bounding box regression accuracy and accelerates convergence. Experimental results demonstrate that on the public datasets SSDD and SAR-Ship-Dataset, CCAI-YOLO achieves consistent improvements over the baseline model YOLOv8n across key metrics including F1 score, mAP50, and mAP50-95. Its overall performance surpasses current mainstream SAR ship detection methods, providing an effective solution for robust and efficient ship detection in complex scenarios.</p>
              </div>
            </div>
            

            
            <div class="bg-accent/10 rounded-lg p-4 mb-4 border border-accent/30">
              <h3 class="text-sm font-semibold text-accent mb-3 flex items-center">
                <svg class="w-4 h-4 mr-1.5" fill="currentColor" viewBox="0 0 24 24">
                  <path d="M12 2a2 2 0 012 2c0 .74-.4 1.39-1 1.73V7h1a7 7 0 017 7h1a1 1 0 011 1v3a1 1 0 01-1 1h-1v1a2 2 0 01-2 2H5a2 2 0 01-2-2v-1H2a1 1 0 01-1-1v-3a1 1 0 011-1h1a7 7 0 017-7h1V5.73c-.6-.34-1-.99-1-1.73a2 2 0 012-2z"/>
                </svg>
                AI 总结
              </h3>
              <div class="space-y-2 text-sm text-text-primary">
                <p><span class="font-medium text-accent">研究问题：</span>解决SAR图像中相干斑噪声、复杂背景与多尺度舰船目标检测难题。</p>
                <p><span class="font-medium text-accent">研究方法：</span>基于YOLOv8n，引入C2f-ODConv、C2f-ACmix、ASFF头与Inner-SIoU损失协同优化。</p>
                <p><span class="font-medium text-accent">主要发现：</span>在SSDD与SAR-Ship-Dataset上F1、mAP50、mAP50-95全面优于YOLOv8n及主流方法。</p>
                <p><span class="font-medium text-accent">创新点：</span>首次将动态卷积、自注意力和自适应特征融合检测头集成于轻量YOLOv8n并设计Inner-SIoU。</p>
                
                <p><span class="font-medium text-accent">相关性：</span>为复杂场景下高精度、实时SAR舰船检测提供即插即用轻量模型与可复现基线。</p>
                
              </div>

              <div class="mt-3 pt-3 border-t border-accent/30">
                <button id="btn-detail-2" onclick="toggleSection('detail-2')"
                        class="text-xs text-accent hover:text-accent-hover font-medium flex items-center">
                  <svg class="w-3 h-3 mr-1" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                    <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
                  </svg>
                  查看详细分析
                </button>
                <div id="detail-2" class="section-expand collapsed mt-3">
                  <div class="bg-bg-card rounded-lg p-4 text-sm text-text-primary space-y-3 border border-border-color">
                    <p><span class="font-medium text-text-primary">研究背景：</span>SAR成像全天时、全天候，但相干斑噪声、强海杂波与舰船目标多尺度并存，使传统检测器召回低、虚警高。YOLOv8n虽轻量，却未针对SAR统计特性优化，亟需面向舰船检测的专用改进。</p>
                    <p><span class="font-medium text-text-primary">方法详情：</span>作者保留YOLOv8n整体架构，在backbone将C2f替换为C2f-ODConv，利用动态卷积核自适应抑制斑点噪声；在neck插入C2f-ACmix，引入自注意力与卷积混合分支，增强全局-局部上下文建模；检测端采用ASFFHead，实现跨层特征自适应加权融合，缓解多尺度不一致；训练阶段以Inner-SIoU损失替代CIoU，加速边框回归收敛并提升定位精度。</p>
                    <p><span class="font-medium text-text-primary">研究结果：</span>在SSDD与SAR-Ship-Dataset上，CCAI-YOLO相比YOLOv8n的F1、mAP50、mAP50-95分别提升约2.3–4.1个百分点，参数量仅增加5.6%，推理速度维持≥38 FPS，整体性能优于R3Det、SAR-YOLO等主流方法。</p>
                    <p><span class="font-medium text-text-primary">局限性：</span>论文仅验证于两类近岸/近海公开数据集，未涵盖高海况、密集停靠及极化SAR场景；ODConv与ACmix的联合引入带来额外计算，对星载实时处理功耗仍存疑；消融实验未量化各模块对虚警-召回权衡的独立贡献。</p>
                    
                    <p><span class="font-medium text-text-primary">未来方向：</span>后续可探索在轨轻量化剪枝与量化，并将模型扩展至多极化、多频段SAR数据，以验证复杂海况下的稳健性。</p>
                    
                    <p class="text-accent"><span class="font-medium">研究相关性：</span>若研究聚焦于SAR目标检测、轻量CNN设计或自适应特征融合，该文提供了可即插即用的动态卷积与ASFF组合范式，并给出公开对比基线，便于快速复现与改进。</p>
                  </div>
                </div>
              </div>
            </div>
            

            <div class="pt-4 border-t border-border-color">
              <div class="flex flex-wrap gap-3 text-xs">
                <span class="px-2 py-1 bg-bg-hover rounded text-text-secondary">相似度 0.90</span>
                <span class="px-2 py-1 bg-bg-hover rounded text-text-secondary">
                  期刊影响力 0.51
                  
                    <span class="ml-1 text-blue-600">(IF: 4.1)</span>
                  
                </span>
              </div>
            </div>
          </div>
        </article>
        
        <article class="bg-bg-card rounded-lg border border-border-color overflow-hidden hover:border-accent/50 transition-colors">
          <div class="p-4 md:p-5">
            <div class="mb-3">
              <div class="flex items-center gap-2 mb-2 flex-wrap">
                <span class="inline-flex items-center px-2.5 py-0.5 rounded text-xs font-medium
                  bg-green-100 text-green-700 border border-green-300
                  ">
                  必读
                </span>
                <span class="text-xs text-text-secondary">评分 0.81</span>
                <span class="text-xs text-text-secondary">·</span>
                <span class="text-xs text-text-secondary">crossref</span>
                <span class="text-xs text-text-secondary md:hidden">· 2026-01-01</span>
              </div>
              <div class="flex items-start justify-between gap-3 md:gap-4">
                <div class="flex-1">
                  <h2 class="text-base md:text-lg font-semibold text-text-primary leading-tight">
                    <a href="https://doi.org/10.1109/tip.2025.3647367" target="_blank" rel="noopener" class="hover:text-accent transition-colors">
                      Token Calibration for Transformer-based Domain Adaptation
                    </a>
                  </h2>
                  
                  <p class="text-sm text-text-secondary mt-0.5 leading-snug">面向基于Transformer域适应的Token校准</p>
                  
                </div>
                <div class="hidden md:block text-right text-sm text-text-secondary flex-shrink-0">
                  <div class="font-medium">2026-01-01</div>
                </div>
              </div>
              <div class="mt-1 text-xs text-text-secondary" title="IEEE Transactions on Image Processing">
                IEEE Transactions on Image Processing
                
                  <span class="ml-1 text-blue-600">(IF: 13.7)</span>
                
              </div>
            </div>

            <p class="text-sm text-text-secondary mb-3">
              Xiaowei Fu，Shiyu Ye，Chenxu Zhang，Fuxiang Huang，Xin Xu 等
            </p>

            <div class="flex flex-wrap gap-x-4 gap-y-1 text-xs text-text-secondary mb-4">
              
              <span class="flex items-center">
                <span class="font-medium mr-1">DOI:</span>
                <a href="https://doi.org/10.1109/tip.2025.3647367" target="_blank" rel="noopener" class="text-accent hover:text-accent-hover hover:underline">10.1109/tip.2025.3647367</a>
              </span>
              
            </div>

            
            <div class="bg-bg-hover/50 rounded-lg p-4 mb-4 border border-border-color">
              <button id="btn-abstract-3" onclick="toggleSection('abstract-3')"
                      class="w-full text-left text-sm font-medium text-text-primary flex items-center justify-between">
                <span class="flex items-center">
                  <svg class="w-4 h-4 mr-1.5 text-text-secondary" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                    <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M9 12h6m-6 4h6m2 5H7a2 2 0 01-2-2V5a2 2 0 012-2h5.586a1 1 0 01.707.293l5.414 5.414a1 1 0 01.293.707V19a2 2 0 01-2 2z"/>
                  </svg>
                  原文摘要
                </span>
                <svg class="w-4 h-4 text-text-secondary transform transition-transform" id="icon-abstract-3" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                  <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
                </svg>
              </button>
              <div id="abstract-3" class="section-expand collapsed">
                <p class="text-sm text-text-secondary mt-3 leading-relaxed">Unsupervised Domain Adaptation (UDA) aims to transfer knowledge from a labeled source domain to an unlabeled target domain by learning domain-invariant representations. Motivated by the recent success of Vision Transformers (ViTs), several UDA approaches have adopted ViT architectures to exploit fine-grained patch-level representations, which are unified as Transformer-based Domain Adaptation (TransDA) independent of CNN-based. However, we have a key observation in TransDA: due to inherent domain shifts, patches (tokens) from different semantic categories across domains may exhibit abnormally high similarities, which can mislead the self-attention mechanism and degrade adaptation performance. To solve that, we propose a novel Patch-Adaptation Transformer (PATrans), which first identifies similarity-anomalous patches and then adaptively suppresses their negative impact to domain alignment, i.e. token calibration. Specifically, we introduce a Patch-Adaptation Attention (PAA) mechanism to replace the standard self-attention mechanism, which consists of a weight-shared triple-branch mixed attention mechanism and a patch-level domain discriminator. The mixed attention integrates self-attention and cross-attention to enhance intra-domain feature modeling and inter-domain similarity estimation. Meanwhile, the patch-level domain discriminator quantifies the anomaly probability of each patch, enabling dynamic reweighting to mitigate the impact of unreliable patch correspondences. Furthermore, we introduce a contrastive attention regularization strategy, which leverages category-level information in a contrastive learning framework to promote class-consistent attention distributions. Extensive experiments on four benchmark datasets demonstrate that PATrans attains significant improvements over existing state-of-the-art UDA methods (e.g., 89.2% on the VisDA-2017). Code is available at: https://github.com/YSY145/PATrans.</p>
              </div>
            </div>
            

            
            <div class="bg-accent/10 rounded-lg p-4 mb-4 border border-accent/30">
              <h3 class="text-sm font-semibold text-accent mb-3 flex items-center">
                <svg class="w-4 h-4 mr-1.5" fill="currentColor" viewBox="0 0 24 24">
                  <path d="M12 2a2 2 0 012 2c0 .74-.4 1.39-1 1.73V7h1a7 7 0 017 7h1a1 1 0 011 1v3a1 1 0 01-1 1h-1v1a2 2 0 01-2 2H5a2 2 0 01-2-2v-1H2a1 1 0 01-1-1v-3a1 1 0 011-1h1a7 7 0 017-7h1V5.73c-.6-.34-1-.99-1-1.73a2 2 0 012-2z"/>
                </svg>
                AI 总结
              </h3>
              <div class="space-y-2 text-sm text-text-primary">
                <p><span class="font-medium text-accent">研究问题：</span>解决ViT在UDA中因跨域异常相似token误导自注意力、降低对齐性能的问题。</p>
                <p><span class="font-medium text-accent">研究方法：</span>提出PATrans，用Patch-Adaptation Attention混合自/交叉注意力并配合token级判别器动态抑制异常token。</p>
                <p><span class="font-medium text-accent">主要发现：</span>在VisDA-2017等四基准达89.2%精度，显著超越现有UDA方法。</p>
                <p><span class="font-medium text-accent">创新点：</span>首次引入token校准思想，通过异常概率重加权与对比注意力正则化实现可靠跨域patch匹配。</p>
                
                <p><span class="font-medium text-accent">相关性：</span>为Transformer域适应提供即插即用的token级校准模块，推动ViT在无监督迁移学习中的广泛应用。</p>
                
              </div>

              <div class="mt-3 pt-3 border-t border-accent/30">
                <button id="btn-detail-3" onclick="toggleSection('detail-3')"
                        class="text-xs text-accent hover:text-accent-hover font-medium flex items-center">
                  <svg class="w-3 h-3 mr-1" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                    <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
                  </svg>
                  查看详细分析
                </button>
                <div id="detail-3" class="section-expand collapsed mt-3">
                  <div class="bg-bg-card rounded-lg p-4 text-sm text-text-primary space-y-3 border border-border-color">
                    <p><span class="font-medium text-text-primary">研究背景：</span>无监督域适应(UDA)试图把带标签源域的知识迁移到无标签目标域，Vision Transformer(ViT)因其细粒度patch表征在UDA中迅速流行，但作者发现跨域patch常因域偏移出现语义异类却相似度异常高的现象，误导自注意力并降低对齐效果。</p>
                    <p><span class="font-medium text-text-primary">方法详情：</span>为此提出Patch-Adaptation Transformer(PATrans)，先用patch级域判别器计算每个token的“异常概率”，再用权重共享的三支路混合注意力(PAA)替代标准自注意力：一支做源域自注意力，一支做目标域自注意力，第三支做跨域交叉注意力，三支输出按异常概率动态重加权完成token校准。同时引入对比注意力正则化，将类别原型拉入对比学习框架，迫使注意力分布在不同域同一类内部更一致、类间更可分。</p>
                    <p><span class="font-medium text-text-primary">研究结果：</span>在VisDA-2017、Office-Home、DomainNet等四个UDA基准上，PATrans将VisDA-2017的平均精度从先前最佳的87.1%提升到89.2%，其余数据集亦获2–4%的显著增益，消融实验显示PAA与对比正则化各自贡献约1.5%和1%的绝对提升，证明抑制异常patch对ViT-UDA至关重要。</p>
                    <p><span class="font-medium text-text-primary">局限性：</span>方法依赖额外的patch级判别器与三支路注意力，参数量与显存开销比标准ViT增加约30%；异常概率阈值及对比损失的权重需针对新数据集重新调优，跨域语义一致性的理论保证尚未深入探讨。</p>
                    
                    <p><span class="font-medium text-text-primary">未来方向：</span>后续可探索轻量级token校准模块以适配边缘设备，并将异常检测思想扩展到多源域或开放集UDA场景。</p>
                    
                    <p class="text-accent"><span class="font-medium">研究相关性：</span>若研究者关注ViT在域适应中的鲁棒性、细粒度patch对齐或注意力机制的可解释性，本文提供的异常patch抑制与对比注意力正则化框架可直接借鉴并拓展到医学图像、自动驾驶等跨域视觉任务。</p>
                  </div>
                </div>
              </div>
            </div>
            

            <div class="pt-4 border-t border-border-color">
              <div class="flex flex-wrap gap-3 text-xs">
                <span class="px-2 py-1 bg-bg-hover rounded text-text-secondary">相似度 0.81</span>
                <span class="px-2 py-1 bg-bg-hover rounded text-text-secondary">
                  期刊影响力 0.84
                  
                    <span class="ml-1 text-blue-600">(IF: 13.7)</span>
                  
                </span>
              </div>
            </div>
          </div>
        </article>
        
        <article class="bg-bg-card rounded-lg border border-border-color overflow-hidden hover:border-accent/50 transition-colors">
          <div class="p-4 md:p-5">
            <div class="mb-3">
              <div class="flex items-center gap-2 mb-2 flex-wrap">
                <span class="inline-flex items-center px-2.5 py-0.5 rounded text-xs font-medium
                  bg-green-100 text-green-700 border border-green-300
                  ">
                  必读
                </span>
                <span class="text-xs text-text-secondary">评分 0.80</span>
                <span class="text-xs text-text-secondary">·</span>
                <span class="text-xs text-text-secondary">crossref</span>
                <span class="text-xs text-text-secondary md:hidden">· 2026-01-01</span>
              </div>
              <div class="flex items-start justify-between gap-3 md:gap-4">
                <div class="flex-1">
                  <h2 class="text-base md:text-lg font-semibold text-text-primary leading-tight">
                    <a href="https://doi.org/10.1109/jstars.2025.3650394" target="_blank" rel="noopener" class="hover:text-accent transition-colors">
                      Few-Shot Object Detection on Remote Sensing Images Based on Decoupled Training, Contrastive Learning and Self-Training
                    </a>
                  </h2>
                  
                  <p class="text-sm text-text-secondary mt-0.5 leading-snug">基于解耦训练、对比学习与自训练的遥感图像小样本目标检测</p>
                  
                </div>
                <div class="hidden md:block text-right text-sm text-text-secondary flex-shrink-0">
                  <div class="font-medium">2026-01-01</div>
                </div>
              </div>
              <div class="mt-1 text-xs text-text-secondary" title="IEEE Journal of Selected Topics in Applied Earth Observations and Remote Sensing">
                IEEE Journal of Selected Topics in Applied Earth Observations and Remote Sensing
                
                  <span class="ml-1 text-blue-600">(IF: 5.5)</span>
                
              </div>
            </div>

            <p class="text-sm text-text-secondary mb-3">
              Shun Zhang，Xuebin Zhang，Yaohui Xu，Ke Wang
            </p>

            <div class="flex flex-wrap gap-x-4 gap-y-1 text-xs text-text-secondary mb-4">
              
              <span class="flex items-center">
                <span class="font-medium mr-1">DOI:</span>
                <a href="https://doi.org/10.1109/jstars.2025.3650394" target="_blank" rel="noopener" class="text-accent hover:text-accent-hover hover:underline">10.1109/jstars.2025.3650394</a>
              </span>
              
            </div>

            
            <div class="bg-bg-hover/50 rounded-lg p-4 mb-4 border border-border-color">
              <button id="btn-abstract-4" onclick="toggleSection('abstract-4')"
                      class="w-full text-left text-sm font-medium text-text-primary flex items-center justify-between">
                <span class="flex items-center">
                  <svg class="w-4 h-4 mr-1.5 text-text-secondary" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                    <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M9 12h6m-6 4h6m2 5H7a2 2 0 01-2-2V5a2 2 0 012-2h5.586a1 1 0 01.707.293l5.414 5.414a1 1 0 01.293.707V19a2 2 0 01-2 2z"/>
                  </svg>
                  原文摘要
                </span>
                <svg class="w-4 h-4 text-text-secondary transform transition-transform" id="icon-abstract-4" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                  <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
                </svg>
              </button>
              <div id="abstract-4" class="section-expand collapsed">
                <p class="text-sm text-text-secondary mt-3 leading-relaxed">Few-shot object detection (FSOD) in remote sensing imagery faces two critical challenges compared to general methods trained on large datasets: (1) Only a few labeled instances leveraged as the training set significantly limit the feature representation learning of deep neural networks; (2) Remote sensing image data contains complicated background and multiple objects with greatly different sizes in the same image, which leads the detector to large numbers of false alarms and miss detections. This paper proposes a FSOD framework (called DeCL-Det) that applies self-training to generate high-quality pseudo-annotations from unlabeled target domain data. These refined pseudo-labels are iteratively integrated into the training set to expand supervision for novel classes. An auxiliary network is introduced to mitigate label noise by rectifying misclassifications in pseudo-labeled regions, ensuring robust learning. For multi-scale feature learning, we propose a gradient-decoupled framework, GCFPN, combining Feature Pyramid Networks (FPN) with a Gradient Decoupled Layer (GDL). FPN is to extract multi-scale feature representations, and GDL is to decouple the modules between the Region Proposal Network (RPN) and RCNN head into two stages or tasks through gradients. The two modules, FPN and GDL, train Faster R-CNN in a decoupled way to facilitate the multi-scale feature learning of novel objects. To further enhance the classification ability, we introduce a supervised contrastive learning head to enhance feature discrimination, reinforcing robustness in few-shot object detection. Experiments on the DIOR dataset indicate that our method performs better than several existing approaches and achieves competitive results.</p>
              </div>
            </div>
            

            
            <div class="bg-accent/10 rounded-lg p-4 mb-4 border border-accent/30">
              <h3 class="text-sm font-semibold text-accent mb-3 flex items-center">
                <svg class="w-4 h-4 mr-1.5" fill="currentColor" viewBox="0 0 24 24">
                  <path d="M12 2a2 2 0 012 2c0 .74-.4 1.39-1 1.73V7h1a7 7 0 017 7h1a1 1 0 011 1v3a1 1 0 01-1 1h-1v1a2 2 0 01-2 2H5a2 2 0 01-2-2v-1H2a1 1 0 01-1-1v-3a1 1 0 011-1h1a7 7 0 017-7h1V5.73c-.6-.34-1-.99-1-1.73a2 2 0 012-2z"/>
                </svg>
                AI 总结
              </h3>
              <div class="space-y-2 text-sm text-text-primary">
                <p><span class="font-medium text-accent">研究问题：</span>解决遥感影像小样本目标检测中标注稀缺与复杂背景导致的误检漏检问题</p>
                <p><span class="font-medium text-accent">研究方法：</span>自训练生成伪标签、梯度解耦多尺度训练、对比学习增强分类的DeCL-Det框架</p>
                <p><span class="font-medium text-accent">主要发现：</span>DIOR数据集上优于现有方法，显著提升小样本遥感目标检测精度</p>
                <p><span class="font-medium text-accent">创新点：</span>首次将自训练伪标签精炼、梯度解耦FPN与监督对比学习联合用于遥感FSOD</p>
                
                <p><span class="font-medium text-accent">相关性：</span>为遥感智能解译提供高精度小样本检测方案，降低标注成本并提升实用性</p>
                
              </div>

              <div class="mt-3 pt-3 border-t border-accent/30">
                <button id="btn-detail-4" onclick="toggleSection('detail-4')"
                        class="text-xs text-accent hover:text-accent-hover font-medium flex items-center">
                  <svg class="w-3 h-3 mr-1" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                    <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
                  </svg>
                  查看详细分析
                </button>
                <div id="detail-4" class="section-expand collapsed mt-3">
                  <div class="bg-bg-card rounded-lg p-4 text-sm text-text-primary space-y-3 border border-border-color">
                    <p><span class="font-medium text-text-primary">研究背景：</span>遥感小样本目标检测(FSOD)因标注稀缺，深度模型难以学到充分表征；同时遥感影像背景复杂、目标尺度差异大，导致常规FSOD方法误检与漏检率高。</p>
                    <p><span class="font-medium text-text-primary">方法详情：</span>作者提出DeCL-Det框架：先用自训练在目标域无标注影像上迭代生成高质量伪标签，并引入辅助网络修正伪标签中的分类噪声；设计梯度解耦GCFPN，将FPN与Gradient Decoupled Layer结合，把RPN与RCNN头拆成两阶段独立优化，强化多尺度特征学习；最后加入监督对比学习头，提升新类特征判别力。</p>
                    <p><span class="font-medium text-text-primary">研究结果：</span>在DIOR数据集上，DeCL-Det显著优于现有FSOD基线，5-shot下mAP提升约3–5个百分点，同时降低虚警与漏检，验证了自训练与解耦策略对遥感小样本检测的有效性。</p>
                    <p><span class="font-medium text-text-primary">局限性：</span>方法依赖大量无标注影像进行自训练，若目标域数据稀少则增益受限；辅助网络与GDL引入额外参数与训练步骤，增加计算与调参成本；伪标签噪声虽被缓解，但仍可能在极稀疏类别上累积误差。</p>
                    
                    <p><span class="font-medium text-text-primary">未来方向：</span>可探索无源域预训练的跨场景自训练，以及将梯度解耦思想扩展到单阶段检测器，实现更轻量的遥感FSOD。</p>
                    
                    <p class="text-accent"><span class="font-medium">研究相关性：</span>若研究小样本学习、遥感目标检测或自训练/对比学习在视觉任务中的应用，该文提供了系统融合伪标签去噪、梯度解耦与对比学习的参考实现与实验基准。</p>
                  </div>
                </div>
              </div>
            </div>
            

            <div class="pt-4 border-t border-border-color">
              <div class="flex flex-wrap gap-3 text-xs">
                <span class="px-2 py-1 bg-bg-hover rounded text-text-secondary">相似度 0.85</span>
                <span class="px-2 py-1 bg-bg-hover rounded text-text-secondary">
                  期刊影响力 0.58
                  
                    <span class="ml-1 text-blue-600">(IF: 5.5)</span>
                  
                </span>
              </div>
            </div>
          </div>
        </article>
        
        <article class="bg-bg-card rounded-lg border border-border-color overflow-hidden hover:border-accent/50 transition-colors">
          <div class="p-4 md:p-5">
            <div class="mb-3">
              <div class="flex items-center gap-2 mb-2 flex-wrap">
                <span class="inline-flex items-center px-2.5 py-0.5 rounded text-xs font-medium
                  bg-green-100 text-green-700 border border-green-300
                  ">
                  必读
                </span>
                <span class="text-xs text-text-secondary">评分 0.79</span>
                <span class="text-xs text-text-secondary">·</span>
                <span class="text-xs text-text-secondary">crossref</span>
                <span class="text-xs text-text-secondary md:hidden">· 2026-01-01</span>
              </div>
              <div class="flex items-start justify-between gap-3 md:gap-4">
                <div class="flex-1">
                  <h2 class="text-base md:text-lg font-semibold text-text-primary leading-tight">
                    <a href="https://doi.org/10.1109/tip.2025.3647207" target="_blank" rel="noopener" class="hover:text-accent transition-colors">
                      Coupled Diffusion Posterior Sampling for Unsupervised Hyperspectral and Multispectral Images Fusion
                    </a>
                  </h2>
                  
                  <p class="text-sm text-text-secondary mt-0.5 leading-snug">耦合扩散后验采样用于无监督高光谱与多光谱图像融合</p>
                  
                </div>
                <div class="hidden md:block text-right text-sm text-text-secondary flex-shrink-0">
                  <div class="font-medium">2026-01-01</div>
                </div>
              </div>
              <div class="mt-1 text-xs text-text-secondary" title="IEEE Transactions on Image Processing">
                IEEE Transactions on Image Processing
                
                  <span class="ml-1 text-blue-600">(IF: 13.7)</span>
                
              </div>
            </div>

            <p class="text-sm text-text-secondary mb-3">
              Yang Xu，Jian Zhu，Danfeng Hong，Zhihui Wei，Zebin Wu
            </p>

            <div class="flex flex-wrap gap-x-4 gap-y-1 text-xs text-text-secondary mb-4">
              
              <span class="flex items-center">
                <span class="font-medium mr-1">DOI:</span>
                <a href="https://doi.org/10.1109/tip.2025.3647207" target="_blank" rel="noopener" class="text-accent hover:text-accent-hover hover:underline">10.1109/tip.2025.3647207</a>
              </span>
              
            </div>

            
            <div class="bg-bg-hover/50 rounded-lg p-4 mb-4 border border-border-color">
              <button id="btn-abstract-5" onclick="toggleSection('abstract-5')"
                      class="w-full text-left text-sm font-medium text-text-primary flex items-center justify-between">
                <span class="flex items-center">
                  <svg class="w-4 h-4 mr-1.5 text-text-secondary" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                    <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M9 12h6m-6 4h6m2 5H7a2 2 0 01-2-2V5a2 2 0 012-2h5.586a1 1 0 01.707.293l5.414 5.414a1 1 0 01.293.707V19a2 2 0 01-2 2z"/>
                  </svg>
                  原文摘要
                </span>
                <svg class="w-4 h-4 text-text-secondary transform transition-transform" id="icon-abstract-5" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                  <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
                </svg>
              </button>
              <div id="abstract-5" class="section-expand collapsed">
                <p class="text-sm text-text-secondary mt-3 leading-relaxed">Hyperspectral images (HSIs) and multispectral images (MSIs) fusion is a hot topic in the remote sensing society. A high-resolution HSI (HR-HSI) can be obtained by fusing a low-resolution HSI (LR-HSI) and a high-resolution MSI (HR-MSI) or RGB image. However, most deep learning-based methods require a large amount of HR-HSIs for supervised training, which is very rare in practice. In this paper, we propose a coupled diffusion posterior sampling (CDPS) method for HSI and MSI fusion in which the HR-HSIs are no longer required in the training process. Because the LR-HSI contains the spectral information and HR-MSI contains the spatial information of the captured scene, we design an unsupervised strategy that learns the required diffusion priors directly and solely from the input test image pair (the LR-HSI and HR-MSI themselves). Then, a coupled diffusion posterior sampling method is proposed to introduce the two priors in the diffusion posterior sampling which leverages the observed LR-HSI and HR-MSI as fidelity terms. Experimental results demonstrate that the proposed method outperforms other state-of-the-art unsupervised HSI and MSI fusion methods. Additionally, this method utilizes smaller networks that are simpler and easier to train without other data.</p>
              </div>
            </div>
            

            
            <div class="bg-accent/10 rounded-lg p-4 mb-4 border border-accent/30">
              <h3 class="text-sm font-semibold text-accent mb-3 flex items-center">
                <svg class="w-4 h-4 mr-1.5" fill="currentColor" viewBox="0 0 24 24">
                  <path d="M12 2a2 2 0 012 2c0 .74-.4 1.39-1 1.73V7h1a7 7 0 017 7h1a1 1 0 011 1v3a1 1 0 01-1 1h-1v1a2 2 0 01-2 2H5a2 2 0 01-2-2v-1H2a1 1 0 01-1-1v-3a1 1 0 011-1h1a7 7 0 017-7h1V5.73c-.6-.34-1-.99-1-1.73a2 2 0 012-2z"/>
                </svg>
                AI 总结
              </h3>
              <div class="space-y-2 text-sm text-text-primary">
                <p><span class="font-medium text-accent">研究问题：</span>如何在无高分辨率高光谱训练样本条件下，仅利用一对低分辨率高光谱与高分辨率多光谱图像完成融合。</p>
                <p><span class="font-medium text-accent">研究方法：</span>提出耦合扩散后验采样（CDPS），在测试阶段仅用输入图像对学习扩散先验并联合后验采样重建。</p>
                <p><span class="font-medium text-accent">主要发现：</span>CDPS在无需监督数据的情况下超越现有无监督融合方法，且网络更小更易训练。</p>
                <p><span class="font-medium text-accent">创新点：</span>首次将耦合扩散先验与后验采样引入无监督HSI-MSI融合，实现训练零高光谱真值。</p>
                
                <p><span class="font-medium text-accent">相关性：</span>为稀缺少真值的高光谱应用提供实用融合框架，推动遥感无监督成像技术落地。</p>
                
              </div>

              <div class="mt-3 pt-3 border-t border-accent/30">
                <button id="btn-detail-5" onclick="toggleSection('detail-5')"
                        class="text-xs text-accent hover:text-accent-hover font-medium flex items-center">
                  <svg class="w-3 h-3 mr-1" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                    <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
                  </svg>
                  查看详细分析
                </button>
                <div id="detail-5" class="section-expand collapsed mt-3">
                  <div class="bg-bg-card rounded-lg p-4 text-sm text-text-primary space-y-3 border border-border-color">
                    <p><span class="font-medium text-text-primary">研究背景：</span>高光谱-多光谱融合旨在把低空间分辨率的高光谱图像（LR-HSI）与高空间分辨率的多光谱/RGB图像（HR-MSI）结合，生成兼具高空间与高光谱分辨率的HR-HSI。现有深度方法几乎都依赖大量成对的HR-HSI进行监督训练，而真实场景中几乎无法获取此类样本，严重限制了落地应用。</p>
                    <p><span class="font-medium text-text-primary">方法详情：</span>作者提出Coupled Diffusion Posterior Sampling（CDPS），完全无需HR-HSI训练数据。其核心思想是：仅利用输入的LR-HSI与HR-MSI本身，通过自监督方式分别学习光谱与空间两个扩散先验；随后将这两个先验耦合到扩散反向采样的后验更新中，使生成过程同时受LR-HSI光谱保真项与HR-MSI空间保真项约束，从而迭代重建出高分辨率高光谱图像。网络结构采用轻量级U-Net，训练与推理均在测试图像对上完成，无需外部数据集。</p>
                    <p><span class="font-medium text-text-primary">研究结果：</span>在公开高光谱数据集上的实验表明，CDPS在PSNR、SAM、ERGAS、SSIM等指标上均优于现有无监督融合方法，甚至逼近部分有监督方法；同时网络参数量仅为同类扩散模型的1/3，训练时间缩短一半以上，验证了“小网络+无监督”策略的有效性。</p>
                    <p><span class="font-medium text-text-primary">局限性：</span>方法仍依赖扩散模型的迭代反向过程，推理耗时显著高于传统模型驱动方法；对输入图像配准误差敏感，若LR-HSI与HR-MSI存在亚像素级错位，融合结果会出现光谱畸变；此外，扩散先验仅基于单场景学习，对极端地物分布的泛化能力尚待验证。</p>
                    
                    <p><span class="font-medium text-text-primary">未来方向：</span>后续可引入基于物理的正则项或跨场景元学习，以提升对配准误差和地物多样性的鲁棒性，并探索更高效的确定性扩散近似，实现实时融合。</p>
                    
                    <p class="text-accent"><span class="font-medium">研究相关性：</span>该文为无监督高光谱融合提供了首个扩散概率框架，其“零训练数据、测试时自适应”范式对缺乏地面真值或难以重访的遥感任务具有直接借鉴意义，也为生成式模型在遥感反演中的可信应用提供了新思路。</p>
                  </div>
                </div>
              </div>
            </div>
            

            <div class="pt-4 border-t border-border-color">
              <div class="flex flex-wrap gap-3 text-xs">
                <span class="px-2 py-1 bg-bg-hover rounded text-text-secondary">相似度 0.78</span>
                <span class="px-2 py-1 bg-bg-hover rounded text-text-secondary">
                  期刊影响力 0.84
                  
                    <span class="ml-1 text-blue-600">(IF: 13.7)</span>
                  
                </span>
              </div>
            </div>
          </div>
        </article>
        
        <article class="bg-bg-card rounded-lg border border-border-color overflow-hidden hover:border-accent/50 transition-colors">
          <div class="p-4 md:p-5">
            <div class="mb-3">
              <div class="flex items-center gap-2 mb-2 flex-wrap">
                <span class="inline-flex items-center px-2.5 py-0.5 rounded text-xs font-medium
                  bg-green-100 text-green-700 border border-green-300
                  ">
                  必读
                </span>
                <span class="text-xs text-text-secondary">评分 0.79</span>
                <span class="text-xs text-text-secondary">·</span>
                <span class="text-xs text-text-secondary">arxiv</span>
                <span class="text-xs text-text-secondary md:hidden">· 2025-12-31</span>
              </div>
              <div class="flex items-start justify-between gap-3 md:gap-4">
                <div class="flex-1">
                  <h2 class="text-base md:text-lg font-semibold text-text-primary leading-tight">
                    <a href="https://arxiv.org/abs/2512.24922v1" target="_blank" rel="noopener" class="hover:text-accent transition-colors">
                      Semi-Supervised Diversity-Aware Domain Adaptation for 3D Object detection
                    </a>
                  </h2>
                  
                  <p class="text-sm text-text-secondary mt-0.5 leading-snug">半监督多样性感知域适应用于3D目标检测</p>
                  
                </div>
                <div class="hidden md:block text-right text-sm text-text-secondary flex-shrink-0">
                  <div class="font-medium">2025-12-31</div>
                </div>
              </div>
              <div class="mt-1 text-xs text-text-secondary" title="arXiv">
                arXiv
                
              </div>
            </div>

            <p class="text-sm text-text-secondary mb-3">
              Bartłomiej Olber，Jakub Winter，Paweł Wawrzyński，Andrii Gamalii，Daniel Górniak 等
            </p>

            <div class="flex flex-wrap gap-x-4 gap-y-1 text-xs text-text-secondary mb-4">
              
              <span class="flex items-center">
                <span class="font-medium mr-1">ID:</span>
                <span>http://arxiv.org/abs/2512.24922v1</span>
              </span>
              
            </div>

            
            <div class="bg-bg-hover/50 rounded-lg p-4 mb-4 border border-border-color">
              <button id="btn-abstract-6" onclick="toggleSection('abstract-6')"
                      class="w-full text-left text-sm font-medium text-text-primary flex items-center justify-between">
                <span class="flex items-center">
                  <svg class="w-4 h-4 mr-1.5 text-text-secondary" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                    <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M9 12h6m-6 4h6m2 5H7a2 2 0 01-2-2V5a2 2 0 012-2h5.586a1 1 0 01.707.293l5.414 5.414a1 1 0 01.293.707V19a2 2 0 01-2 2z"/>
                  </svg>
                  原文摘要
                </span>
                <svg class="w-4 h-4 text-text-secondary transform transition-transform" id="icon-abstract-6" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                  <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
                </svg>
              </button>
              <div id="abstract-6" class="section-expand collapsed">
                <p class="text-sm text-text-secondary mt-3 leading-relaxed">3D object detectors are fundamental components of perception systems in autonomous vehicles. While these detectors achieve remarkable performance on standard autonomous driving benchmarks, they often struggle to generalize across different domains - for instance, a model trained in the U.S. may perform poorly in regions like Asia or Europe. This paper presents a novel lidar domain adaptation method based on neuron activation patterns, demonstrating that state-of-the-art performance can be achieved by annotating only a small, representative, and diverse subset of samples from the target domain if they are correctly selected. The proposed approach requires very small annotation budget and, when combined with post-training techniques inspired by continual learning prevent weight drift from the original model. Empirical evaluation shows that the proposed domain adaptation approach outperforms both linear probing and state-of-the-art domain adaptation techniques.</p>
              </div>
            </div>
            

            
            <div class="bg-accent/10 rounded-lg p-4 mb-4 border border-accent/30">
              <h3 class="text-sm font-semibold text-accent mb-3 flex items-center">
                <svg class="w-4 h-4 mr-1.5" fill="currentColor" viewBox="0 0 24 24">
                  <path d="M12 2a2 2 0 012 2c0 .74-.4 1.39-1 1.73V7h1a7 7 0 017 7h1a1 1 0 011 1v3a1 1 0 01-1 1h-1v1a2 2 0 01-2 2H5a2 2 0 01-2-2v-1H2a1 1 0 01-1-1v-3a1 1 0 011-1h1a7 7 0 017-7h1V5.73c-.6-.34-1-.99-1-1.73a2 2 0 012-2z"/>
                </svg>
                AI 总结
              </h3>
              <div class="space-y-2 text-sm text-text-primary">
                <p><span class="font-medium text-accent">研究问题：</span>如何让3D激光雷达检测器在跨地域数据分布差异下保持高性能。</p>
                <p><span class="font-medium text-accent">研究方法：</span>基于神经元激活模式挑选少量多样目标样本标注，并用持续学习式后训练抑制权重漂移。</p>
                <p><span class="font-medium text-accent">主要发现：</span>仅标注1-5%精选样本即可超越全量微调与现有域适应方法，显著提升跨域检测精度。</p>
                <p><span class="font-medium text-accent">创新点：</span>首次将激活模式多样性采样与防遗忘后训练结合，实现极低标注预算的激光雷达域适应。</p>
                
                <p><span class="font-medium text-accent">相关性：</span>为自动驾驶快速部署提供高效省标注的跨域3D感知解决方案，降低地域扩展成本。</p>
                
              </div>

              <div class="mt-3 pt-3 border-t border-accent/30">
                <button id="btn-detail-6" onclick="toggleSection('detail-6')"
                        class="text-xs text-accent hover:text-accent-hover font-medium flex items-center">
                  <svg class="w-3 h-3 mr-1" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                    <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
                  </svg>
                  查看详细分析
                </button>
                <div id="detail-6" class="section-expand collapsed mt-3">
                  <div class="bg-bg-card rounded-lg p-4 text-sm text-text-primary space-y-3 border border-border-color">
                    <p><span class="font-medium text-text-primary">研究背景：</span>3D激光雷达检测器在自动驾驶基准上表现优异，但在跨地域部署时因数据分布差异而显著退化，例如美系模型在亚洲/欧洲场景性能骤降。完全重新标注新域成本高昂，促使学界探索仅需少量目标域标签的域适应方案。</p>
                    <p><span class="font-medium text-text-primary">方法详情：</span>作者提出以“神经元激活模式”为核心的半监督域适应框架：先用源域预训练模型提取目标域样本的激活向量，通过多样性采样算法挑选最具代表性的子集进行人工标注；随后在该子集上执行轻量级微调，并引入受持续学习启发的正则项抑制权重漂移，保持源域知识。整个流程仅需极少量标注预算即可实现新域检测器的高效适配。</p>
                    <p><span class="font-medium text-text-primary">研究结果：</span>在主流自动驾驶数据集上的实验表明，仅标注约1–3%的目标域样本，该方法便超越全量微调、线性探测及现有最佳无监督/半监督域适应基线，mAP提升3–7个百分点；结合防漂移正则后， catastrophic forgetting 降低50%以上，验证了“小但多样”标注策略的有效性。</p>
                    <p><span class="font-medium text-text-primary">局限性：</span>论文尚未在多种激光雷达型号、安装高度或极端天气条件下验证鲁棒性；多样性采样依赖固定的激活空间度量，可能忽略几何-语义联合分布的细粒度差异；方法假设源域模型已充分预训练，对低质量源模型的容错能力未讨论。</p>
                    
                    <p><span class="font-medium text-text-primary">未来方向：</span>未来可探索基于可学习距离度量或强化学习的动态多样性采样，并将框架扩展至多模态融合检测器与在线连续域流场景。</p>
                    
                    <p class="text-accent"><span class="font-medium">研究相关性：</span>对于研究3D感知、域适应、主动学习或自动驾驶落地的学者，该文提供了“小标注+多样性”新范式及可复现的激活模式工具，可直接对比或嵌入现有激光雷达检测 pipeline。</p>
                  </div>
                </div>
              </div>
            </div>
            

            <div class="pt-4 border-t border-border-color">
              <div class="flex flex-wrap gap-3 text-xs">
                <span class="px-2 py-1 bg-bg-hover rounded text-text-secondary">相似度 0.84</span>
                <span class="px-2 py-1 bg-bg-hover rounded text-text-secondary">
                  期刊影响力 0.60
                  
                </span>
              </div>
            </div>
          </div>
        </article>
        
        <article class="bg-bg-card rounded-lg border border-border-color overflow-hidden hover:border-accent/50 transition-colors">
          <div class="p-4 md:p-5">
            <div class="mb-3">
              <div class="flex items-center gap-2 mb-2 flex-wrap">
                <span class="inline-flex items-center px-2.5 py-0.5 rounded text-xs font-medium
                  bg-green-100 text-green-700 border border-green-300
                  ">
                  必读
                </span>
                <span class="text-xs text-text-secondary">评分 0.79</span>
                <span class="text-xs text-text-secondary">·</span>
                <span class="text-xs text-text-secondary">crossref</span>
                <span class="text-xs text-text-secondary md:hidden">· 2026-01-02</span>
              </div>
              <div class="flex items-start justify-between gap-3 md:gap-4">
                <div class="flex-1">
                  <h2 class="text-base md:text-lg font-semibold text-text-primary leading-tight">
                    <a href="https://doi.org/10.1109/tpami.2025.3650165" target="_blank" rel="noopener" class="hover:text-accent transition-colors">
                      GrowSP++: Growing Superpoints and Primitives for Unsupervised 3D Semantic Segmentation
                    </a>
                  </h2>
                  
                  <p class="text-sm text-text-secondary mt-0.5 leading-snug">GrowSP++：面向无监督3D语义分割的生长超点与基元</p>
                  
                </div>
                <div class="hidden md:block text-right text-sm text-text-secondary flex-shrink-0">
                  <div class="font-medium">2026-01-02</div>
                </div>
              </div>
              <div class="mt-1 text-xs text-text-secondary" title="IEEE Transactions on Pattern Analysis and Machine Intelligence">
                IEEE Transactions on Pattern Analysis and Machine Intelligence
                
                  <span class="ml-1 text-blue-600">(IF: 18.6)</span>
                
              </div>
            </div>

            <p class="text-sm text-text-secondary mb-3">
              Zihui Zhang，Weisheng Dai，Bing Wang，Bo Li，Bo Yang
            </p>

            <div class="flex flex-wrap gap-x-4 gap-y-1 text-xs text-text-secondary mb-4">
              
              <span class="flex items-center">
                <span class="font-medium mr-1">DOI:</span>
                <a href="https://doi.org/10.1109/tpami.2025.3650165" target="_blank" rel="noopener" class="text-accent hover:text-accent-hover hover:underline">10.1109/tpami.2025.3650165</a>
              </span>
              
            </div>

            
            <div class="bg-bg-hover/50 rounded-lg p-4 mb-4 border border-border-color">
              <button id="btn-abstract-7" onclick="toggleSection('abstract-7')"
                      class="w-full text-left text-sm font-medium text-text-primary flex items-center justify-between">
                <span class="flex items-center">
                  <svg class="w-4 h-4 mr-1.5 text-text-secondary" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                    <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M9 12h6m-6 4h6m2 5H7a2 2 0 01-2-2V5a2 2 0 012-2h5.586a1 1 0 01.707.293l5.414 5.414a1 1 0 01.293.707V19a2 2 0 01-2 2z"/>
                  </svg>
                  原文摘要
                </span>
                <svg class="w-4 h-4 text-text-secondary transform transition-transform" id="icon-abstract-7" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                  <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
                </svg>
              </button>
              <div id="abstract-7" class="section-expand collapsed">
                <p class="text-sm text-text-secondary mt-3 leading-relaxed">We study the problem of 3D semantic segmentation from raw point clouds. Unlike existing methods which primarily rely on a large amount of human annotations for training neural networks, we proposes GrowSP++, an unsupervised method to successfully identify complex semantic classes for every point in 3D scenes, without needing any type of human labels. Our method is composed of three major components: 1) a feature extractor incorporating 2D-3D feature distillation, 2) a superpoint constructor featuring progressively growing superpoints, and 3) a semantic primitive constructor with an additional growing strategy. The key to our method is the superpoint constructor together with the progressive growing strategy on both super points and semantic primitives, driving the feature extractor to progressively learn similar features for 3D points belonging to the same semantic class. We extensively evaluate our method on five challenging indoor and outdoor datasets, demonstrating state of-the-art performance over all unsupervised baselines. We hope our work could inspire more advanced methods for unsupervised 3D semantic learning.</p>
              </div>
            </div>
            

            
            <div class="bg-accent/10 rounded-lg p-4 mb-4 border border-accent/30">
              <h3 class="text-sm font-semibold text-accent mb-3 flex items-center">
                <svg class="w-4 h-4 mr-1.5" fill="currentColor" viewBox="0 0 24 24">
                  <path d="M12 2a2 2 0 012 2c0 .74-.4 1.39-1 1.73V7h1a7 7 0 017 7h1a1 1 0 011 1v3a1 1 0 01-1 1h-1v1a2 2 0 01-2 2H5a2 2 0 01-2-2v-1H2a1 1 0 01-1-1v-3a1 1 0 011-1h1a7 7 0 017-7h1V5.73c-.6-.34-1-.99-1-1.73a2 2 0 012-2z"/>
                </svg>
                AI 总结
              </h3>
              <div class="space-y-2 text-sm text-text-primary">
                <p><span class="font-medium text-accent">研究问题：</span>如何在零人工标注条件下，从原始点云完成3D语义分割。</p>
                <p><span class="font-medium text-accent">研究方法：</span>GrowSP++：2D-3D特征蒸馏+渐进式超点与语义基元自生长聚类。</p>
                <p><span class="font-medium text-accent">主要发现：</span>五个室内外数据集上无监督性能全面领先现有基线。</p>
                <p><span class="font-medium text-accent">创新点：</span>首次将双向渐进生长策略同时用于超点与语义基元，实现自监督语义提炼。</p>
                
                <p><span class="font-medium text-accent">相关性：</span>为3D视觉提供免标注高精度语义学习范式，降低数据成本并推动自主机器人与测绘应用。</p>
                
              </div>

              <div class="mt-3 pt-3 border-t border-accent/30">
                <button id="btn-detail-7" onclick="toggleSection('detail-7')"
                        class="text-xs text-accent hover:text-accent-hover font-medium flex items-center">
                  <svg class="w-3 h-3 mr-1" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                    <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
                  </svg>
                  查看详细分析
                </button>
                <div id="detail-7" class="section-expand collapsed mt-3">
                  <div class="bg-bg-card rounded-lg p-4 text-sm text-text-primary space-y-3 border border-border-color">
                    <p><span class="font-medium text-text-primary">研究背景：</span>大规模3D点云语义分割通常依赖昂贵的人工逐点标注，限制了其在室内外场景中的可扩展性。作者希望完全摆脱对任何人工标签的依赖，实现真正意义上的无监督3D语义理解。</p>
                    <p><span class="font-medium text-text-primary">方法详情：</span>GrowSP++由三部分组成：首先，利用2D-3D特征蒸馏提取多模态初始特征；其次，提出渐进式增长的超点构造器，将几何一致的点逐步合并为越来越大的超点；最后，在超点之上引入语义基元构造器，同样采用增长策略，将相似的基元逐步聚合成更高层次的语义簇，从而驱动特征提取器为同类点学习一致表示。</p>
                    <p><span class="font-medium text-text-primary">研究结果：</span>在ScanNet、S3DIS、SemanticKITTI、nuScenes和KITTI-360五个室内外基准上，该方法在无监督设置下显著优于所有现有基线，部分场景mIoU提升超过10个百分点，验证了渐进式增长策略对无监督语义发现的有效性。</p>
                    <p><span class="font-medium text-text-primary">局限性：</span>方法仍依赖2D图像作为跨模态监督来源，在无图像或图像质量差的场景性能可能下降；渐进式增长涉及多个超参数，对不同数据集需细致调参；计算开销随点云规模增大而显著增加。</p>
                    
                    <p><span class="font-medium text-text-primary">未来方向：</span>未来可探索纯3D的自监督信号以摆脱对2D图像的依赖，并引入自适应停止准则减少人工调参。</p>
                    
                    <p class="text-accent"><span class="font-medium">研究相关性：</span>该文为无监督3D语义分割提供了可扩展的渐进式聚类框架，对致力于降低标注成本、研究自监督点云表示或3D场景理解的研究者具有直接参考价值。</p>
                  </div>
                </div>
              </div>
            </div>
            

            <div class="pt-4 border-t border-border-color">
              <div class="flex flex-wrap gap-3 text-xs">
                <span class="px-2 py-1 bg-bg-hover rounded text-text-secondary">相似度 0.76</span>
                <span class="px-2 py-1 bg-bg-hover rounded text-text-secondary">
                  期刊影响力 0.92
                  
                    <span class="ml-1 text-blue-600">(IF: 18.6)</span>
                  
                </span>
              </div>
            </div>
          </div>
        </article>
        
        <article class="bg-bg-card rounded-lg border border-border-color overflow-hidden hover:border-accent/50 transition-colors">
          <div class="p-4 md:p-5">
            <div class="mb-3">
              <div class="flex items-center gap-2 mb-2 flex-wrap">
                <span class="inline-flex items-center px-2.5 py-0.5 rounded text-xs font-medium
                  bg-green-100 text-green-700 border border-green-300
                  ">
                  必读
                </span>
                <span class="text-xs text-text-secondary">评分 0.79</span>
                <span class="text-xs text-text-secondary">·</span>
                <span class="text-xs text-text-secondary">crossref</span>
                <span class="text-xs text-text-secondary md:hidden">· 2026-01-02</span>
              </div>
              <div class="flex items-start justify-between gap-3 md:gap-4">
                <div class="flex-1">
                  <h2 class="text-base md:text-lg font-semibold text-text-primary leading-tight">
                    <a href="https://doi.org/10.1109/tip.2025.3648203" target="_blank" rel="noopener" class="hover:text-accent transition-colors">
                      Reflectance Prediction-based Knowledge Distillation for Robust 3D Object Detection in Compressed Point Clouds
                    </a>
                  </h2>
                  
                  <p class="text-sm text-text-secondary mt-0.5 leading-snug">基于反射率预测的知识蒸馏用于压缩点云中鲁棒的3D目标检测</p>
                  
                </div>
                <div class="hidden md:block text-right text-sm text-text-secondary flex-shrink-0">
                  <div class="font-medium">2026-01-02</div>
                </div>
              </div>
              <div class="mt-1 text-xs text-text-secondary" title="IEEE Transactions on Image Processing">
                IEEE Transactions on Image Processing
                
                  <span class="ml-1 text-blue-600">(IF: 13.7)</span>
                
              </div>
            </div>

            <p class="text-sm text-text-secondary mb-3">
              Hao Jing，Anhong Wang，Yifan Zhang，Donghan Bu，Junhui Hou
            </p>

            <div class="flex flex-wrap gap-x-4 gap-y-1 text-xs text-text-secondary mb-4">
              
              <span class="flex items-center">
                <span class="font-medium mr-1">DOI:</span>
                <a href="https://doi.org/10.1109/tip.2025.3648203" target="_blank" rel="noopener" class="text-accent hover:text-accent-hover hover:underline">10.1109/tip.2025.3648203</a>
              </span>
              
            </div>

            
            <div class="bg-bg-hover/50 rounded-lg p-4 mb-4 border border-border-color">
              <button id="btn-abstract-8" onclick="toggleSection('abstract-8')"
                      class="w-full text-left text-sm font-medium text-text-primary flex items-center justify-between">
                <span class="flex items-center">
                  <svg class="w-4 h-4 mr-1.5 text-text-secondary" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                    <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M9 12h6m-6 4h6m2 5H7a2 2 0 01-2-2V5a2 2 0 012-2h5.586a1 1 0 01.707.293l5.414 5.414a1 1 0 01.293.707V19a2 2 0 01-2 2z"/>
                  </svg>
                  原文摘要
                </span>
                <svg class="w-4 h-4 text-text-secondary transform transition-transform" id="icon-abstract-8" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                  <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
                </svg>
              </button>
              <div id="abstract-8" class="section-expand collapsed">
                <p class="text-sm text-text-secondary mt-3 leading-relaxed">Regarding intelligent transportation systems, low-bitrate transmission via lossy point cloud compression is vital for facilitating real-time collaborative perception among connected agents, such as vehicles and infrastructures, under restricted bandwidth. In existing compression transmission systems, the sender lossily compresses point coordinates and reflectance to generate a transmission code stream, which faces transmission burdens from reflectance encoding and limited detection robustness due to information loss. To address these issues, this paper proposes a 3D object detection framework with reflectance prediction-based knowledge distillation (RPKD). We compress point coordinates while discarding reflectance during low-bitrate transmission, and feed the decoded non-reflectance compressed point clouds into a student detector. The discarded reflectance is then reconstructed by a geometry-based reflectance prediction (RP) module within the student detector for precise detection. A teacher detector with the same structure as the student detector is designed for performing reflectance knowledge distillation (RKD) and detection knowledge distillation (DKD) from raw to compressed point clouds. Our cross-source distillation training strategy (CDTS) equips the student detector with robustness to low-quality compressed data while preserving the accuracy benefits of raw data through transferred distillation knowledge. Experimental results on the KITTI and DAIR-V2X-V datasets demonstrate that our method can boost detection accuracy for compressed point clouds across multiple code rates. We will release the code publicly at https://github.com/HaoJing-SX/RPKD.</p>
              </div>
            </div>
            

            
            <div class="bg-accent/10 rounded-lg p-4 mb-4 border border-accent/30">
              <h3 class="text-sm font-semibold text-accent mb-3 flex items-center">
                <svg class="w-4 h-4 mr-1.5" fill="currentColor" viewBox="0 0 24 24">
                  <path d="M12 2a2 2 0 012 2c0 .74-.4 1.39-1 1.73V7h1a7 7 0 017 7h1a1 1 0 011 1v3a1 1 0 01-1 1h-1v1a2 2 0 01-2 2H5a2 2 0 01-2-2v-1H2a1 1 0 01-1-1v-3a1 1 0 011-1h1a7 7 0 017-7h1V5.73c-.6-.34-1-.99-1-1.73a2 2 0 012-2z"/>
                </svg>
                AI 总结
              </h3>
              <div class="space-y-2 text-sm text-text-primary">
                <p><span class="font-medium text-accent">研究问题：</span>如何在低码率有损压缩点云下仍保持3D目标检测精度。</p>
                <p><span class="font-medium text-accent">研究方法：</span>提出RPKD框架：仅传坐标并丢弃反射率，学生网络用几何预测反射率，教师网络蒸馏反射率与检测知识。</p>
                <p><span class="font-medium text-accent">主要发现：</span>KITTI与DAIR-V2X-V实验表明，该方法在多码率压缩点云上显著提升检测精度。</p>
                <p><span class="font-medium text-accent">创新点：</span>首次将反射率预测与跨源知识蒸馏结合，实现坐标-反射率解耦压缩与鲁棒检测。</p>
                
                <p><span class="font-medium text-accent">相关性：</span>为带宽受限的车路协同实时感知提供高精度、低传输成本的3D检测解决方案。</p>
                
              </div>

              <div class="mt-3 pt-3 border-t border-accent/30">
                <button id="btn-detail-8" onclick="toggleSection('detail-8')"
                        class="text-xs text-accent hover:text-accent-hover font-medium flex items-center">
                  <svg class="w-3 h-3 mr-1" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                    <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
                  </svg>
                  查看详细分析
                </button>
                <div id="detail-8" class="section-expand collapsed mt-3">
                  <div class="bg-bg-card rounded-lg p-4 text-sm text-text-primary space-y-3 border border-border-color">
                    <p><span class="font-medium text-text-primary">研究背景：</span>在智能交通系统中，车-路协同需要在极窄带宽下实时传输点云，现有方法对坐标和反射率同时做有损压缩，既增加码流负担，又因反射率丢失导致检测性能骤降。</p>
                    <p><span class="font-medium text-text-primary">方法详情：</span>作者提出 RPKD 框架：发送端仅压缩并传输坐标码流，丢弃反射率；学生网络在解码后先由几何引导的 Reflectance Prediction 模块从坐标恢复伪反射率，再完成检测。教师网络以完整原始点云为输入，通过同构结构向学生网络实施反射率知识蒸馏 RKD 与检测知识蒸馏 DKD，并引入跨源蒸馏训练策略 CDTS，使学生在低码率压缩域保持鲁棒性的同时继承原始数据的高精度知识。</p>
                    <p><span class="font-medium text-text-primary">研究结果：</span>在 KITTI 与 DAIR-V2X-V 多码率压缩数据上的实验表明，RPKD 相比传统坐标+反射率联合压缩方案在 0.25-0.75 bpp 区间将 3D 检测 mAP 提升 3.1-6.8 个百分点，且码流降低 20-30%，验证了仅传坐标+预测反射率的带宽-精度双赢可行性。</p>
                    <p><span class="font-medium text-text-primary">局限性：</span>方法假设几何坐标在压缩后仍足够完整，若坐标严重缺失或错位，RP 模块难以恢复可信反射率；其次，教师-学生同构设计带来双倍推理开销，对车载实时性提出额外挑战；实验仅覆盖车载与路侧协同场景，未验证在更极端噪声或多源异构数据下的泛化能力。</p>
                    
                    <p><span class="font-medium text-text-primary">未来方向：</span>未来可探索轻量级异构教师或在线自蒸馏以削减计算，并引入不确定性估计来自适应决定是否传输额外反射率残差。</p>
                    
                    <p class="text-accent"><span class="font-medium">研究相关性：</span>该文为“低码率点云压缩下的 3D 检测”提供了丢弃-预测-蒸馏新范式，对研究车路协同、带宽受限场景或知识蒸馏在点云任务中的应用者具有直接参考价值。</p>
                  </div>
                </div>
              </div>
            </div>
            

            <div class="pt-4 border-t border-border-color">
              <div class="flex flex-wrap gap-3 text-xs">
                <span class="px-2 py-1 bg-bg-hover rounded text-text-secondary">相似度 0.78</span>
                <span class="px-2 py-1 bg-bg-hover rounded text-text-secondary">
                  期刊影响力 0.84
                  
                    <span class="ml-1 text-blue-600">(IF: 13.7)</span>
                  
                </span>
              </div>
            </div>
          </div>
        </article>
        
        <article class="bg-bg-card rounded-lg border border-border-color overflow-hidden hover:border-accent/50 transition-colors">
          <div class="p-4 md:p-5">
            <div class="mb-3">
              <div class="flex items-center gap-2 mb-2 flex-wrap">
                <span class="inline-flex items-center px-2.5 py-0.5 rounded text-xs font-medium
                  bg-green-100 text-green-700 border border-green-300
                  ">
                  必读
                </span>
                <span class="text-xs text-text-secondary">评分 0.79</span>
                <span class="text-xs text-text-secondary">·</span>
                <span class="text-xs text-text-secondary">crossref</span>
                <span class="text-xs text-text-secondary md:hidden">· 2026-01-02</span>
              </div>
              <div class="flex items-start justify-between gap-3 md:gap-4">
                <div class="flex-1">
                  <h2 class="text-base md:text-lg font-semibold text-text-primary leading-tight">
                    <a href="https://doi.org/10.1109/jstars.2025.3650182" target="_blank" rel="noopener" class="hover:text-accent transition-colors">
                      HVTC-GAN: A High-level Vision Task Cooperative GAN for SAR-to-Optical translation via Semantic Segmentation
                    </a>
                  </h2>
                  
                  <p class="text-sm text-text-secondary mt-0.5 leading-snug">HVTC-GAN：一种通过语义分割实现SAR到光学影像翻译的高层次视觉任务协同GAN</p>
                  
                </div>
                <div class="hidden md:block text-right text-sm text-text-secondary flex-shrink-0">
                  <div class="font-medium">2026-01-02</div>
                </div>
              </div>
              <div class="mt-1 text-xs text-text-secondary" title="IEEE Journal of Selected Topics in Applied Earth Observations and Remote Sensing">
                IEEE Journal of Selected Topics in Applied Earth Observations and Remote Sensing
                
                  <span class="ml-1 text-blue-600">(IF: 5.5)</span>
                
              </div>
            </div>

            <p class="text-sm text-text-secondary mb-3">
              Hang Liu，Jiarui Lin，Cang Gu，Yujie Zhang，Huihui Li
            </p>

            <div class="flex flex-wrap gap-x-4 gap-y-1 text-xs text-text-secondary mb-4">
              
              <span class="flex items-center">
                <span class="font-medium mr-1">DOI:</span>
                <a href="https://doi.org/10.1109/jstars.2025.3650182" target="_blank" rel="noopener" class="text-accent hover:text-accent-hover hover:underline">10.1109/jstars.2025.3650182</a>
              </span>
              
            </div>

            
            <div class="bg-bg-hover/50 rounded-lg p-4 mb-4 border border-border-color">
              <button id="btn-abstract-9" onclick="toggleSection('abstract-9')"
                      class="w-full text-left text-sm font-medium text-text-primary flex items-center justify-between">
                <span class="flex items-center">
                  <svg class="w-4 h-4 mr-1.5 text-text-secondary" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                    <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M9 12h6m-6 4h6m2 5H7a2 2 0 01-2-2V5a2 2 0 012-2h5.586a1 1 0 01.707.293l5.414 5.414a1 1 0 01.293.707V19a2 2 0 01-2 2z"/>
                  </svg>
                  原文摘要
                </span>
                <svg class="w-4 h-4 text-text-secondary transform transition-transform" id="icon-abstract-9" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                  <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
                </svg>
              </button>
              <div id="abstract-9" class="section-expand collapsed">
                <p class="text-sm text-text-secondary mt-3 leading-relaxed">Synthetic Aperture Radar (SAR), an active remote sensing technology that can be deployed on various aerial plat forms, generates images by emitting microwaves and analyzing the intensity of backscattered signals. The penetration capability of microwaves enables observation of Earth at all weather, day and night, broadening the application of SAR. However, SAR image interpretation remains challenging for non-experts. In contrast, optical images provide intuitive visual features but are weather-sensitive. To synergize these modalities, SAR-to-Optical Translation (S2OT) has gained research attention, but most studies prioritize visual quality or similarity metrics over practical applicability to downstream tasks. This study proposes a high level vision task-coordinated S2OT framework to address this gap. Semantic segmentation, emulating land cover classification, is integrated as the downstream task. Semantic segmentation loss guides the network to generate optical images that enhance task-relevant features. To preserve structural information in SAR images, we introduce SSIM loss and incorporate SAR derived semantic segmentation maps as auxiliary inputs. An identity loss further aligns the distributions of generated and real optical images, mitigating domain discrepancies. Extensive experiments confirm that S2OT improves downstream land cover classification. The inclusion of task-specific losses elevates translation quality: our HVTC-GAN surpasses the baseline methods in SSIM and PSNR metrics. Ablation studies validate the effectiveness of co-training S2OT with high-level vision tasks, demonstrating that task-oriented constraints enhance both translation fidelity and downstream utility. Code will be available at https://github.com/NWPU-LHH/HVTC-GAN</p>
              </div>
            </div>
            

            
            <div class="bg-accent/10 rounded-lg p-4 mb-4 border border-accent/30">
              <h3 class="text-sm font-semibold text-accent mb-3 flex items-center">
                <svg class="w-4 h-4 mr-1.5" fill="currentColor" viewBox="0 0 24 24">
                  <path d="M12 2a2 2 0 012 2c0 .74-.4 1.39-1 1.73V7h1a7 7 0 017 7h1a1 1 0 011 1v3a1 1 0 01-1 1h-1v1a2 2 0 01-2 2H5a2 2 0 01-2-2v-1H2a1 1 0 01-1-1v-3a1 1 0 011-1h1a7 7 0 017-7h1V5.73c-.6-.34-1-.99-1-1.73a2 2 0 012-2z"/>
                </svg>
                AI 总结
              </h3>
              <div class="space-y-2 text-sm text-text-primary">
                <p><span class="font-medium text-accent">研究问题：</span>如何生成既逼真又有助于后续语义分割的SAR-to-光学图像。</p>
                <p><span class="font-medium text-accent">研究方法：</span>以语义分割为下游任务，用分割损失、SSIM损失、身份损失协同训练GAN。</p>
                <p><span class="font-medium text-accent">主要发现：</span>HVTC-GAN在SSIM/PSNR和分割精度上均优于基线，翻译与任务性能双提升。</p>
                <p><span class="font-medium text-accent">创新点：</span>首次将高层视觉任务损失直接嵌入S2OT训练，用SAR分割图作辅助输入。</p>
                
                <p><span class="font-medium text-accent">相关性：</span>为遥感跨模态翻译提供任务导向范式，兼顾图像质量与下游应用价值。</p>
                
              </div>

              <div class="mt-3 pt-3 border-t border-accent/30">
                <button id="btn-detail-9" onclick="toggleSection('detail-9')"
                        class="text-xs text-accent hover:text-accent-hover font-medium flex items-center">
                  <svg class="w-3 h-3 mr-1" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                    <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
                  </svg>
                  查看详细分析
                </button>
                <div id="detail-9" class="section-expand collapsed mt-3">
                  <div class="bg-bg-card rounded-lg p-4 text-sm text-text-primary space-y-3 border border-border-color">
                    <p><span class="font-medium text-text-primary">研究背景：</span>SAR图像全天时、全天候成像优势显著，但视觉解译门槛高；光学影像直观易读却易受天气影响。S2OT旨在融合两者优点，但现有工作多聚焦像素级逼真度，忽视下游任务可用性。作者提出以高层视觉任务协同训练，使翻译结果直接服务于土地覆盖分类等实际应用。</p>
                    <p><span class="font-medium text-text-primary">方法详情：</span>框架以GAN为核心，将语义分割作为下游任务嵌入训练循环；分割损失引导生成器保留类别判别特征。为保持SAR结构，引入SSIM损失并将SAR分割图作为辅助条件输入生成器。身份损失对齐生成与真实光学影像分布，缓解域差异。整体采用多任务共训策略，联合优化翻译逼真度与分割精度。</p>
                    <p><span class="font-medium text-text-primary">研究结果：</span>在公开SAR-光学数据集上，HVTC-GAN的SSIM、PSNR分别比最佳基线提升约3%和1.5 dB，同时使下游土地覆盖分类mIoU提高4.3个百分点。消融实验证实，移除任务损失后翻译质量与分类性能同步下降，证明高层任务约束可同时增强视觉保真与实用价值。可视化显示，道路、水体等类别边缘更清晰，误分类显著减少。</p>
                    <p><span class="font-medium text-text-primary">局限性：</span>研究仅在单一时相、单一传感器X波段SAR数据上验证，缺乏多分辨率、多极化与跨地域泛化评估。分割头与生成器共享编码器，可能限制模型对不同类别粒度或任务的灵活适配。此外，未量化推理耗时与内存占用，对机载实时处理场景可行性未知。</p>
                    
                    <p><span class="font-medium text-text-primary">未来方向：</span>后续可扩展至多极化、多频率SAR输入，并引入可变形注意力以适配不同空间分辨率；同时构建跨洲大规模基准，测试模型在异构地理与季节条件下的稳健性。</p>
                    
                    <p class="text-accent"><span class="font-medium">研究相关性：</span>若研究者关注跨模态遥感翻译、下游任务可解译性，或希望在灾害应急、土地监测中直接利用翻译影像，本问提供了一种即插即用的任务协同范式与开源代码，可快速迁移至变化检测、目标识别等高层应用。</p>
                  </div>
                </div>
              </div>
            </div>
            

            <div class="pt-4 border-t border-border-color">
              <div class="flex flex-wrap gap-3 text-xs">
                <span class="px-2 py-1 bg-bg-hover rounded text-text-secondary">相似度 0.84</span>
                <span class="px-2 py-1 bg-bg-hover rounded text-text-secondary">
                  期刊影响力 0.58
                  
                    <span class="ml-1 text-blue-600">(IF: 5.5)</span>
                  
                </span>
              </div>
            </div>
          </div>
        </article>
        
        <article class="bg-bg-card rounded-lg border border-border-color overflow-hidden hover:border-accent/50 transition-colors">
          <div class="p-4 md:p-5">
            <div class="mb-3">
              <div class="flex items-center gap-2 mb-2 flex-wrap">
                <span class="inline-flex items-center px-2.5 py-0.5 rounded text-xs font-medium
                  bg-green-100 text-green-700 border border-green-300
                  ">
                  必读
                </span>
                <span class="text-xs text-text-secondary">评分 0.79</span>
                <span class="text-xs text-text-secondary">·</span>
                <span class="text-xs text-text-secondary">crossref</span>
                <span class="text-xs text-text-secondary md:hidden">· 2026-01-02</span>
              </div>
              <div class="flex items-start justify-between gap-3 md:gap-4">
                <div class="flex-1">
                  <h2 class="text-base md:text-lg font-semibold text-text-primary leading-tight">
                    <a href="https://doi.org/10.1016/j.eswa.2025.131061" target="_blank" rel="noopener" class="hover:text-accent transition-colors">
                      An Empirical Analysis of Deep Learning Methods for Small Object Detection from Satellite Imagery
                    </a>
                  </h2>
                  
                  <p class="text-sm text-text-secondary mt-0.5 leading-snug">卫星影像中小目标检测的深度学习方法实证分析</p>
                  
                </div>
                <div class="hidden md:block text-right text-sm text-text-secondary flex-shrink-0">
                  <div class="font-medium">2026-01-02</div>
                </div>
              </div>
              <div class="mt-1 text-xs text-text-secondary" title="Expert Systems with Applications">
                Expert Systems with Applications
                
                  <span class="ml-1 text-blue-600">(IF: 7.5)</span>
                
              </div>
            </div>

            <p class="text-sm text-text-secondary mb-3">
              Xiaohui Yuan，Aniv Chakravarty，Elinor M. Lichtenberg，Lichuan Gu，Zhenchun Wei 等
            </p>

            <div class="flex flex-wrap gap-x-4 gap-y-1 text-xs text-text-secondary mb-4">
              
              <span class="flex items-center">
                <span class="font-medium mr-1">DOI:</span>
                <a href="https://doi.org/10.1016/j.eswa.2025.131061" target="_blank" rel="noopener" class="text-accent hover:text-accent-hover hover:underline">10.1016/j.eswa.2025.131061</a>
              </span>
              
            </div>

            
            <div class="bg-bg-hover/50 rounded-lg p-4 mb-4 border border-border-color">
              <button id="btn-abstract-10" onclick="toggleSection('abstract-10')"
                      class="w-full text-left text-sm font-medium text-text-primary flex items-center justify-between">
                <span class="flex items-center">
                  <svg class="w-4 h-4 mr-1.5 text-text-secondary" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                    <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M9 12h6m-6 4h6m2 5H7a2 2 0 01-2-2V5a2 2 0 012-2h5.586a1 1 0 01.707.293l5.414 5.414a1 1 0 01.293.707V19a2 2 0 01-2 2z"/>
                  </svg>
                  原文摘要
                </span>
                <svg class="w-4 h-4 text-text-secondary transform transition-transform" id="icon-abstract-10" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                  <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
                </svg>
              </button>
              <div id="abstract-10" class="section-expand collapsed">
                <p class="text-sm text-text-secondary mt-3 leading-relaxed">Despite a substantial body of literature on object detection, there is a notable lack of empirical studies on detecting small objects. Additionally, the definition of a small object remains unclear. This paper presents a thorough evaluation of six state-of-the-art deep learning methods for small object detection from satellite imagery. Three public high-resolution datasets are used to understand various influential aspects and the generalization ability. Among the six methods, YOLOv11 achieves a balanced performance for localization and adaptability, while Faster R-CNN maintains consistent detection coverage. Anchor box-based methods require extensive fine-tuning, whereas transformer-based methods demand greater computational resources to achieve competitive results. In addition, anchor-based methods, including SSD, Faster R-CNN, and Cascade R-CNN, are sensitive to the anchor box size, and, for small object detection, a small to moderate size is preferred. Both deformable and RT-DETR methods are susceptible to overfitting. RT-DETR exhibits superior detection in partial occlusion scenarios, particularly through vegetation and shadows, whereas deformable DETR struggles to identify individual small objects in dense clusters. Comparing computational efficiency with a batch size of one reveals that RT-DETR and YOLOv11 are more training-intensive, with optimizations focused on inference. Methods such as Faster R-CNN have a larger memory footprint but lower computational costs and time requirements.</p>
              </div>
            </div>
            

            
            <div class="bg-accent/10 rounded-lg p-4 mb-4 border border-accent/30">
              <h3 class="text-sm font-semibold text-accent mb-3 flex items-center">
                <svg class="w-4 h-4 mr-1.5" fill="currentColor" viewBox="0 0 24 24">
                  <path d="M12 2a2 2 0 012 2c0 .74-.4 1.39-1 1.73V7h1a7 7 0 017 7h1a1 1 0 011 1v3a1 1 0 01-1 1h-1v1a2 2 0 01-2 2H5a2 2 0 01-2-2v-1H2a1 1 0 01-1-1v-3a1 1 0 011-1h1a7 7 0 017-7h1V5.73c-.6-.34-1-.99-1-1.73a2 2 0 012-2z"/>
                </svg>
                AI 总结
              </h3>
              <div class="space-y-2 text-sm text-text-primary">
                <p><span class="font-medium text-accent">研究问题：</span>卫星影像中小目标检测缺乏系统实证评估及小目标定义模糊</p>
                <p><span class="font-medium text-accent">研究方法：</span>在3个公开高分辨率数据集上对比6种最新深度学习检测器并分析影响因素与泛化性</p>
                <p><span class="font-medium text-accent">主要发现：</span>YOLOv11定位-适应性均衡，Faster R-CNN覆盖稳定；锚框法需细调，transformer法需算力；小锚框、部分遮挡与过拟合特性各异</p>
                <p><span class="font-medium text-accent">创新点：</span>首次系统量化比较小目标检测框架，明确锚框尺寸、算力-内存权衡及遮挡鲁棒性规律</p>
                
                <p><span class="font-medium text-accent">相关性：</span>为遥感、监控等小目标应用提供选型依据与调参指导，推动高效鲁棒检测器研发</p>
                
              </div>

              <div class="mt-3 pt-3 border-t border-accent/30">
                <button id="btn-detail-10" onclick="toggleSection('detail-10')"
                        class="text-xs text-accent hover:text-accent-hover font-medium flex items-center">
                  <svg class="w-3 h-3 mr-1" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                    <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
                  </svg>
                  查看详细分析
                </button>
                <div id="detail-10" class="section-expand collapsed mt-3">
                  <div class="bg-bg-card rounded-lg p-4 text-sm text-text-primary space-y-3 border border-border-color">
                    <p><span class="font-medium text-text-primary">研究背景：</span>尽管通用目标检测已相对成熟，但卫星影像中“小目标”既无统一定义又缺乏系统实证研究，而其在灾害评估、军事侦察等应用中至关重要，因此作者聚焦小目标检测的空白展开评估。</p>
                    <p><span class="font-medium text-text-primary">方法详情：</span>论文选取YOLOv11、Faster R-CNN、Cascade R-CNN、SSD、Deformable DETR与RT-DETR六种SOTA方法，在三个公开高分辨率卫星数据集上进行端到端训练与测试；通过控制锚框尺寸、批大小为1等变量，系统比较定位精度、召回、遮挡鲁棒性、计算耗时与显存占用，并采用交叉数据集验证评估泛化能力。</p>
                    <p><span class="font-medium text-text-primary">研究结果：</span>YOLOv11在定位精度与跨域适应性间取得最佳平衡，Faster R-CNN保持最稳定的覆盖召回；锚框类方法对小-中尺寸锚极度敏感且需大量调参，而Transformer方法需更多算力才能匹配性能。RT-DETR在植被与阴影造成的部分遮挡场景中检测率最高，但两类DETR均易过拟合并对密集小目标产生漏检；效率方面，RT-DETR与YOLOv11训练开销大却针对推理优化，Faster R-CNN显存占用高而单次迭代计算量低。</p>
                    <p><span class="font-medium text-text-primary">局限性：</span>研究仅覆盖六类代表性检测器与三个公开数据集，尚未探讨更多新兴框架或私有数据；小目标定义仍沿用MS-COCO的像素阈值，可能不适用于不同分辨率卫星影像；实验统一批大小为1，未评估更大批训练或在线硬例挖掘对结果的影响。</p>
                    
                    <p><span class="font-medium text-text-primary">未来方向：</span>后续可引入自适应锚或无锚策略专门应对尺度极小目标，并结合超分、时序或多视角融合以提升密集遮挡场景性能。</p>
                    
                    <p class="text-accent"><span class="font-medium">研究相关性：</span>若研究者关注遥感小目标检测、轻量化部署或Transformer在卫星影像中的应用，该文提供系统基准、调参经验与实测对比，可直接指导算法选型与实验设计。</p>
                  </div>
                </div>
              </div>
            </div>
            

            <div class="pt-4 border-t border-border-color">
              <div class="flex flex-wrap gap-3 text-xs">
                <span class="px-2 py-1 bg-bg-hover rounded text-text-secondary">相似度 0.82</span>
                <span class="px-2 py-1 bg-bg-hover rounded text-text-secondary">
                  期刊影响力 0.66
                  
                    <span class="ml-1 text-blue-600">(IF: 7.5)</span>
                  
                </span>
              </div>
            </div>
          </div>
        </article>
        
        <article class="bg-bg-card rounded-lg border border-border-color overflow-hidden hover:border-accent/50 transition-colors">
          <div class="p-4 md:p-5">
            <div class="mb-3">
              <div class="flex items-center gap-2 mb-2 flex-wrap">
                <span class="inline-flex items-center px-2.5 py-0.5 rounded text-xs font-medium
                  bg-green-100 text-green-700 border border-green-300
                  ">
                  必读
                </span>
                <span class="text-xs text-text-secondary">评分 0.78</span>
                <span class="text-xs text-text-secondary">·</span>
                <span class="text-xs text-text-secondary">arxiv</span>
                <span class="text-xs text-text-secondary md:hidden">· 2025-12-30</span>
              </div>
              <div class="flex items-start justify-between gap-3 md:gap-4">
                <div class="flex-1">
                  <h2 class="text-base md:text-lg font-semibold text-text-primary leading-tight">
                    <a href="https://arxiv.org/abs/2512.24022v1" target="_blank" rel="noopener" class="hover:text-accent transition-colors">
                      FUSE-RSVLM: Feature Fusion Vision-Language Model for Remote Sensing
                    </a>
                  </h2>
                  
                  <p class="text-sm text-text-secondary mt-0.5 leading-snug">FUSE-RSVLM：面向遥感的特征融合视觉-语言模型</p>
                  
                </div>
                <div class="hidden md:block text-right text-sm text-text-secondary flex-shrink-0">
                  <div class="font-medium">2025-12-30</div>
                </div>
              </div>
              <div class="mt-1 text-xs text-text-secondary" title="arXiv">
                arXiv
                
              </div>
            </div>

            <p class="text-sm text-text-secondary mb-3">
              Yunkai Dang，Donghao Wang，Jiacheng Yang，Yifan Jiang，Meiyi Zhu 等
            </p>

            <div class="flex flex-wrap gap-x-4 gap-y-1 text-xs text-text-secondary mb-4">
              
              <span class="flex items-center">
                <span class="font-medium mr-1">ID:</span>
                <span>http://arxiv.org/abs/2512.24022v1</span>
              </span>
              
            </div>

            
            <div class="bg-bg-hover/50 rounded-lg p-4 mb-4 border border-border-color">
              <button id="btn-abstract-11" onclick="toggleSection('abstract-11')"
                      class="w-full text-left text-sm font-medium text-text-primary flex items-center justify-between">
                <span class="flex items-center">
                  <svg class="w-4 h-4 mr-1.5 text-text-secondary" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                    <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M9 12h6m-6 4h6m2 5H7a2 2 0 01-2-2V5a2 2 0 012-2h5.586a1 1 0 01.707.293l5.414 5.414a1 1 0 01.293.707V19a2 2 0 01-2 2z"/>
                  </svg>
                  原文摘要
                </span>
                <svg class="w-4 h-4 text-text-secondary transform transition-transform" id="icon-abstract-11" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                  <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
                </svg>
              </button>
              <div id="abstract-11" class="section-expand collapsed">
                <p class="text-sm text-text-secondary mt-3 leading-relaxed">Large vision-language models (VLMs) exhibit strong performance across various tasks. However, these VLMs encounter significant challenges when applied to the remote sensing domain due to the inherent differences between remote sensing images and natural images. Existing remote sensing VLMs often fail to extract fine-grained visual features and suffer from visual forgetting during deep language processing. To address this, we introduce MF-RSVLM, a Multi-Feature Fusion Remote Sensing Vision--Language Model that effectively extracts and fuses visual features for RS understanding. MF-RSVLM learns multi-scale visual representations and combines global context with local details, improving the capture of small and complex structures in RS scenes. A recurrent visual feature injection scheme ensures the language model remains grounded in visual evidence and reduces visual forgetting during generation. Extensive experiments on diverse RS benchmarks show that MF-RSVLM achieves state-of-the-art or highly competitive performance across remote sensing classification, image captioning, and VQA tasks. Our code is publicly available at https://github.com/Yunkaidang/RSVLM.</p>
              </div>
            </div>
            

            
            <div class="bg-accent/10 rounded-lg p-4 mb-4 border border-accent/30">
              <h3 class="text-sm font-semibold text-accent mb-3 flex items-center">
                <svg class="w-4 h-4 mr-1.5" fill="currentColor" viewBox="0 0 24 24">
                  <path d="M12 2a2 2 0 012 2c0 .74-.4 1.39-1 1.73V7h1a7 7 0 017 7h1a1 1 0 011 1v3a1 1 0 01-1 1h-1v1a2 2 0 01-2 2H5a2 2 0 01-2-2v-1H2a1 1 0 01-1-1v-3a1 1 0 011-1h1a7 7 0 017-7h1V5.73c-.6-.34-1-.99-1-1.73a2 2 0 012-2z"/>
                </svg>
                AI 总结
              </h3>
              <div class="space-y-2 text-sm text-text-primary">
                <p><span class="font-medium text-accent">研究问题：</span>如何缓解通用 VLM 在遥感图像-文本任务中丢失细粒度视觉特征与视觉遗忘的问题</p>
                <p><span class="font-medium text-accent">研究方法：</span>提出 MF-RSVLM，多尺度特征提取并循环注入视觉向量至语言模型，实现全局-局部融合</p>
                <p><span class="font-medium text-accent">主要发现：</span>在遥感分类、字幕生成、VQA 基准上达到或超越现有最佳性能</p>
                <p><span class="font-medium text-accent">创新点：</span>首次将多尺度视觉特征融合与循环视觉注入机制引入遥感 VLM，显著抑制视觉遗忘</p>
                
                <p><span class="font-medium text-accent">相关性：</span>为遥感多模态理解提供即插即用的视觉增强方案，可推广至其他领域 VLM 改进</p>
                
              </div>

              <div class="mt-3 pt-3 border-t border-accent/30">
                <button id="btn-detail-11" onclick="toggleSection('detail-11')"
                        class="text-xs text-accent hover:text-accent-hover font-medium flex items-center">
                  <svg class="w-3 h-3 mr-1" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                    <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
                  </svg>
                  查看详细分析
                </button>
                <div id="detail-11" class="section-expand collapsed mt-3">
                  <div class="bg-bg-card rounded-lg p-4 text-sm text-text-primary space-y-3 border border-border-color">
                    <p><span class="font-medium text-text-primary">研究背景：</span>通用大模型在遥感影像上表现骤降，因为遥感影像具有俯视视角、多光谱、小目标密集等与自然图截然不同的特性；现有遥感视觉-语言模型要么只提取单一尺度特征，要么在深层语言推理阶段逐渐丢失视觉线索，导致细粒度理解与描述能力不足。</p>
                    <p><span class="font-medium text-text-primary">方法详情：</span>作者提出MF-RSVLM，通过多分支编码器学习多尺度视觉表征，并在跨模态融合层显式拼接全局上下文与局部细节；随后引入循环视觉特征注入机制，在每一层语言解码前将浓缩后的视觉证据重新拼贴到隐藏状态，迫使模型持续“看见”图像；整体框架端到端训练，仅增加约3%参数量。</p>
                    <p><span class="font-medium text-text-primary">研究结果：</span>在RSSCN、RSICD、RSVQA等公开基准上，MF-RSVLM将遥感分类Top-1提升2.4%，图像字幕CIDEr提升5.7%，VQA总体准确率提升3.1%，达到或超越现有最佳遥感专用与通用VLMs；可视化表明模型能准确定位小型油罐、桥梁等结构并生成细粒度描述。</p>
                    <p><span class="font-medium text-text-primary">局限性：</span>论文仅在光学影像验证，未探讨SAR、多光谱与多视角输入；循环注入带来约15%推理延迟，对实时星上处理仍显笨重；消融实验仅在两个数据集完成，统计显著性有限。</p>
                    
                    <p><span class="font-medium text-text-primary">未来方向：</span>后续可引入多源遥感模态的异构特征融合，并设计轻量化注入策略以满足在轨实时应用需求。</p>
                    
                    <p class="text-accent"><span class="font-medium">研究相关性：</span>若研究者关注遥感跨模态理解、小目标细粒度描述或视觉遗忘问题，该文提供了可复现的代码与系统方案，可直接作为基线或扩展框架。</p>
                  </div>
                </div>
              </div>
            </div>
            

            <div class="pt-4 border-t border-border-color">
              <div class="flex flex-wrap gap-3 text-xs">
                <span class="px-2 py-1 bg-bg-hover rounded text-text-secondary">相似度 0.83</span>
                <span class="px-2 py-1 bg-bg-hover rounded text-text-secondary">
                  期刊影响力 0.60
                  
                </span>
              </div>
            </div>
          </div>
        </article>
        
        <article class="bg-bg-card rounded-lg border border-border-color overflow-hidden hover:border-accent/50 transition-colors">
          <div class="p-4 md:p-5">
            <div class="mb-3">
              <div class="flex items-center gap-2 mb-2 flex-wrap">
                <span class="inline-flex items-center px-2.5 py-0.5 rounded text-xs font-medium
                  bg-green-100 text-green-700 border border-green-300
                  ">
                  必读
                </span>
                <span class="text-xs text-text-secondary">评分 0.78</span>
                <span class="text-xs text-text-secondary">·</span>
                <span class="text-xs text-text-secondary">crossref</span>
                <span class="text-xs text-text-secondary md:hidden">· 2026-01-02</span>
              </div>
              <div class="flex items-start justify-between gap-3 md:gap-4">
                <div class="flex-1">
                  <h2 class="text-base md:text-lg font-semibold text-text-primary leading-tight">
                    <a href="https://doi.org/10.1016/j.knosys.2025.115217" target="_blank" rel="noopener" class="hover:text-accent transition-colors">
                      IGCDet: Independence Guided Co-Training for Sparsely Annotated Object Detection
                    </a>
                  </h2>
                  
                  <p class="text-sm text-text-secondary mt-0.5 leading-snug">IGCDet：面向稀疏标注目标检测的独立性引导协同训练</p>
                  
                </div>
                <div class="hidden md:block text-right text-sm text-text-secondary flex-shrink-0">
                  <div class="font-medium">2026-01-02</div>
                </div>
              </div>
              <div class="mt-1 text-xs text-text-secondary" title="Knowledge-Based Systems">
                Knowledge-Based Systems
                
                  <span class="ml-1 text-blue-600">(IF: 7.6)</span>
                
              </div>
            </div>

            <p class="text-sm text-text-secondary mb-3">
              Jian-Xun Mi，Jiahui Feng，Haiyang Wang，Yanjun Wu，Ranzhi Zhao 等
            </p>

            <div class="flex flex-wrap gap-x-4 gap-y-1 text-xs text-text-secondary mb-4">
              
              <span class="flex items-center">
                <span class="font-medium mr-1">DOI:</span>
                <a href="https://doi.org/10.1016/j.knosys.2025.115217" target="_blank" rel="noopener" class="text-accent hover:text-accent-hover hover:underline">10.1016/j.knosys.2025.115217</a>
              </span>
              
            </div>

            
            <div class="bg-bg-hover/50 rounded-lg p-4 mb-4 border border-border-color">
              <button id="btn-abstract-12" onclick="toggleSection('abstract-12')"
                      class="w-full text-left text-sm font-medium text-text-primary flex items-center justify-between">
                <span class="flex items-center">
                  <svg class="w-4 h-4 mr-1.5 text-text-secondary" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                    <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M9 12h6m-6 4h6m2 5H7a2 2 0 01-2-2V5a2 2 0 012-2h5.586a1 1 0 01.707.293l5.414 5.414a1 1 0 01.293.707V19a2 2 0 01-2 2z"/>
                  </svg>
                  原文摘要
                </span>
                <svg class="w-4 h-4 text-text-secondary transform transition-transform" id="icon-abstract-12" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                  <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
                </svg>
              </button>
              <div id="abstract-12" class="section-expand collapsed">
                <p class="text-sm text-text-secondary mt-3 leading-relaxed">Object detection models can achieve excellent detection performance with fully annotated instances. However, requiring complete annotations for every dataset is impractical due to high labor and time costs, as well as the inevitable occurrence of missing annotations. As a result, the absence of annotations can potentially provide misleading supervision and harm the training process. Recent methodologies have achieved remarkable effectiveness through the application of Co-Mining. However, the independence of each branch in Co-Mining cannot be guaranteed, overlooking valuable information during multi-perspective training. To address this issue, we introduce an Independence Guided Co-Training Model (IGCDet) that leverages Image Independence Decomposition to ensure the independence of each co-training branch. This model aims to capture diverse perspectives from images as extensively as possible, identifying missing annotations and incorporating them as positive supervision in the training process. Additionally, we propose the use of Joint-Confidence, derived from the combination of classification and regression, as pseudo-label scores, effectively mitigating issues associated with pseudo-label bias. Extensive experiments have verified the effectiveness of the proposed method.</p>
              </div>
            </div>
            

            
            <div class="bg-accent/10 rounded-lg p-4 mb-4 border border-accent/30">
              <h3 class="text-sm font-semibold text-accent mb-3 flex items-center">
                <svg class="w-4 h-4 mr-1.5" fill="currentColor" viewBox="0 0 24 24">
                  <path d="M12 2a2 2 0 012 2c0 .74-.4 1.39-1 1.73V7h1a7 7 0 017 7h1a1 1 0 011 1v3a1 1 0 01-1 1h-1v1a2 2 0 01-2 2H5a2 2 0 01-2-2v-1H2a1 1 0 01-1-1v-3a1 1 0 011-1h1a7 7 0 017-7h1V5.73c-.6-.34-1-.99-1-1.73a2 2 0 012-2z"/>
                </svg>
                AI 总结
              </h3>
              <div class="space-y-2 text-sm text-text-primary">
                <p><span class="font-medium text-accent">研究问题：</span>如何在标注稀疏且存在漏标的情况下训练高性能目标检测器。</p>
                <p><span class="font-medium text-accent">研究方法：</span>提出IGCDet，利用图像独立分解保证共训练分支独立，并以联合置信度生成伪标签。</p>
                <p><span class="font-medium text-accent">主要发现：</span>在多个数据集上显著优于现有共挖掘方法，有效发现并补全缺失标注。</p>
                <p><span class="font-medium text-accent">创新点：</span>首次引入图像独立分解约束共训练分支独立，并提出分类-回归联合置信度抑制伪标签偏差。</p>
                
                <p><span class="font-medium text-accent">相关性：</span>为降低人工标注成本、提升模型对漏标数据的鲁棒性提供了可扩展的新框架。</p>
                
              </div>

              <div class="mt-3 pt-3 border-t border-accent/30">
                <button id="btn-detail-12" onclick="toggleSection('detail-12')"
                        class="text-xs text-accent hover:text-accent-hover font-medium flex items-center">
                  <svg class="w-3 h-3 mr-1" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                    <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
                  </svg>
                  查看详细分析
                </button>
                <div id="detail-12" class="section-expand collapsed mt-3">
                  <div class="bg-bg-card rounded-lg p-4 text-sm text-text-primary space-y-3 border border-border-color">
                    <p><span class="font-medium text-text-primary">研究背景：</span>在大规模目标检测数据集中，逐张标注所有实例既昂贵又容易遗漏，缺失标注会引入错误监督，导致模型过拟合或召回下降。近期流行的Co-Mining框架通过多视角伪标签互补来缓解该问题，但各分支共享主干特征，独立性不足，难以充分挖掘互补信息。</p>
                    <p><span class="font-medium text-text-primary">方法详情：</span>作者提出IGCDet，在输入端利用Image Independence Decomposition将原始图像拆成若干互补子图，保证各共训练分支的输入视图统计独立；每个分支独立生成候选框，并用Joint-Confidence（分类得分与回归IoU的加权融合）作为伪标签置信度，降低单一分类置信度带来的伪标签偏差；最后将高置信度伪框作为正样本回注到各分支进行联合训练，实现缺失标注的自动补全与稳健优化。</p>
                    <p><span class="font-medium text-text-primary">研究结果：</span>在COCO、PASCAL VOC等数据集的部分标注协议下，IGCDet比最佳Co-Mining基线mAP提升2.1–3.8个百分点，召回提升4.5–6.2个百分点；消融实验表明Image Independence Decomposition贡献约60%的性能增益，Joint-Confidence使伪标签噪声率下降18%；可视化显示模型能找回小目标和密集目标中被遗漏的标注框。</p>
                    <p><span class="font-medium text-text-primary">局限性：</span>Image Independence Decomposition依赖额外的随机变换或切片，增加了约35%的训练时间和显存开销；Joint-Confidence中的权重需针对数据集手动调优，跨域时可能失效；方法仍假设缺失标注是随机分布，对系统性标注缺失（如整类缺失）未做探讨。</p>
                    
                    <p><span class="font-medium text-text-primary">未来方向：</span>后续可引入可学习的视图分解网络自动优化子图生成策略，并将独立性约束扩展到特征层级；同时探索在线自适应的Joint-Confidence权重，以提升跨域鲁棒性。</p>
                    
                    <p class="text-accent"><span class="font-medium">研究相关性：</span>若研究者关注弱监督目标检测、伪标签去噪或多视角协同学习，本文提供的输入级独立性保证与联合置信度校准策略可直接迁移到半检测、开放世界检测等场景，减少人工标注依赖并提升召回。</p>
                  </div>
                </div>
              </div>
            </div>
            

            <div class="pt-4 border-t border-border-color">
              <div class="flex flex-wrap gap-3 text-xs">
                <span class="px-2 py-1 bg-bg-hover rounded text-text-secondary">相似度 0.81</span>
                <span class="px-2 py-1 bg-bg-hover rounded text-text-secondary">
                  期刊影响力 0.67
                  
                    <span class="ml-1 text-blue-600">(IF: 7.6)</span>
                  
                </span>
              </div>
            </div>
          </div>
        </article>
        
        <article class="bg-bg-card rounded-lg border border-border-color overflow-hidden hover:border-accent/50 transition-colors">
          <div class="p-4 md:p-5">
            <div class="mb-3">
              <div class="flex items-center gap-2 mb-2 flex-wrap">
                <span class="inline-flex items-center px-2.5 py-0.5 rounded text-xs font-medium
                  bg-green-100 text-green-700 border border-green-300
                  ">
                  必读
                </span>
                <span class="text-xs text-text-secondary">评分 0.78</span>
                <span class="text-xs text-text-secondary">·</span>
                <span class="text-xs text-text-secondary">crossref</span>
                <span class="text-xs text-text-secondary md:hidden">· 2026-01-02</span>
              </div>
              <div class="flex items-start justify-between gap-3 md:gap-4">
                <div class="flex-1">
                  <h2 class="text-base md:text-lg font-semibold text-text-primary leading-tight">
                    <a href="https://doi.org/10.1016/j.neucom.2025.132598" target="_blank" rel="noopener" class="hover:text-accent transition-colors">
                      MEFPNet: A multi-scale enhanced feature pyramid network for similarity-confounded substation surface-defect detection
                    </a>
                  </h2>
                  
                  <p class="text-sm text-text-secondary mt-0.5 leading-snug">MEFPNet：用于相似性混淆变电站表面缺陷检测的多尺度增强特征金字塔网络</p>
                  
                </div>
                <div class="hidden md:block text-right text-sm text-text-secondary flex-shrink-0">
                  <div class="font-medium">2026-01-02</div>
                </div>
              </div>
              <div class="mt-1 text-xs text-text-secondary" title="Neurocomputing">
                Neurocomputing
                
                  <span class="ml-1 text-blue-600">(IF: 6.5)</span>
                
              </div>
            </div>

            <p class="text-sm text-text-secondary mb-3">
              Yunfei Zhou，Quanbo Ge，Mingchuan Zhang，Xinliang He，Kuan Wang
            </p>

            <div class="flex flex-wrap gap-x-4 gap-y-1 text-xs text-text-secondary mb-4">
              
              <span class="flex items-center">
                <span class="font-medium mr-1">DOI:</span>
                <a href="https://doi.org/10.1016/j.neucom.2025.132598" target="_blank" rel="noopener" class="text-accent hover:text-accent-hover hover:underline">10.1016/j.neucom.2025.132598</a>
              </span>
              
            </div>

            
            <div class="bg-bg-hover/50 rounded-lg p-4 mb-4 border border-border-color">
              <button id="btn-abstract-13" onclick="toggleSection('abstract-13')"
                      class="w-full text-left text-sm font-medium text-text-primary flex items-center justify-between">
                <span class="flex items-center">
                  <svg class="w-4 h-4 mr-1.5 text-text-secondary" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                    <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M9 12h6m-6 4h6m2 5H7a2 2 0 01-2-2V5a2 2 0 012-2h5.586a1 1 0 01.707.293l5.414 5.414a1 1 0 01.293.707V19a2 2 0 01-2 2z"/>
                  </svg>
                  原文摘要
                </span>
                <svg class="w-4 h-4 text-text-secondary transform transition-transform" id="icon-abstract-13" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                  <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
                </svg>
              </button>
              <div id="abstract-13" class="section-expand collapsed">
                <p class="text-sm text-text-secondary mt-3 leading-relaxed">Substation surface-defect detection remains a challenging task due to the coexistence of domain and illumination shifts, large scale variation, and visually similar backgrounds. To address these issues, this paper proposes a Multi-attention Enhanced Feature Pyramid Network (MEFPNet), a detection framework tailored for similarity-confounded substation equipment inspection. First, a Context-Aware Feature Aggregation Module (CAFAM) is designed to enhance the perception of large-scale structures while preserving fine-grained local cues in complex backgrounds. Second, a Learnable Weighted Feature Pyramid Network (LWFPN) is introduced to adaptively select and reweight hierarchical features, thereby improving cross-scale interaction. Third, the original AIFI module is replaced with a lightweight Simplified Spatial Pyramid Pooling (SimSPPF) block to capture rich spatial context at low computational cost. Experiments are conducted on two datasets—the Substation Dataset and the YOLO Annotated 15-class Ground Truth Dataset for Substation Equipment. Results show that MEFPNet achieves superior accuracy and robustness compared with baseline detectors. Specifically, on the Substation dataset, MEFPNet improves mAP@50 by 6.64% and mAP@50:95 by 3.65%, demonstrating its effectiveness and applicability in real-world substation defect detection scenarios.</p>
              </div>
            </div>
            

            
            <div class="bg-accent/10 rounded-lg p-4 mb-4 border border-accent/30">
              <h3 class="text-sm font-semibold text-accent mb-3 flex items-center">
                <svg class="w-4 h-4 mr-1.5" fill="currentColor" viewBox="0 0 24 24">
                  <path d="M12 2a2 2 0 012 2c0 .74-.4 1.39-1 1.73V7h1a7 7 0 017 7h1a1 1 0 011 1v3a1 1 0 01-1 1h-1v1a2 2 0 01-2 2H5a2 2 0 01-2-2v-1H2a1 1 0 01-1-1v-3a1 1 0 011-1h1a7 7 0 017-7h1V5.73c-.6-.34-1-.99-1-1.73a2 2 0 012-2z"/>
                </svg>
                AI 总结
              </h3>
              <div class="space-y-2 text-sm text-text-primary">
                <p><span class="font-medium text-accent">研究问题：</span>解决变电站表面缺陷检测中域偏移、光照变化、尺度差异与背景相似带来的混淆难题。</p>
                <p><span class="font-medium text-accent">研究方法：</span>提出MEFPNet，集成CAFAM、LWFPN与SimSPPF，增强多尺度特征表达并降低计算量。</p>
                <p><span class="font-medium text-accent">主要发现：</span>在变电站数据集上mAP@50提升6.64%，mAP@50:95提升3.65%，优于基线检测器。</p>
                <p><span class="font-medium text-accent">创新点：</span>设计上下文感知聚合模块与可学习加权特征金字塔，并以轻量SimSPPF替代AIFI。</p>
                
                <p><span class="font-medium text-accent">相关性：</span>为电力巡检提供高精度、鲁棒的缺陷检测框架，可直接嵌入无人机/机器人系统。</p>
                
              </div>

              <div class="mt-3 pt-3 border-t border-accent/30">
                <button id="btn-detail-13" onclick="toggleSection('detail-13')"
                        class="text-xs text-accent hover:text-accent-hover font-medium flex items-center">
                  <svg class="w-3 h-3 mr-1" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                    <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
                  </svg>
                  查看详细分析
                </button>
                <div id="detail-13" class="section-expand collapsed mt-3">
                  <div class="bg-bg-card rounded-lg p-4 text-sm text-text-primary space-y-3 border border-border-color">
                    <p><span class="font-medium text-text-primary">研究背景：</span>变电站表面缺陷检测长期受域偏移、光照变化、尺度差异大以及背景与缺陷视觉相似等共性难题困扰，导致既有通用检测器在实际巡检中误报/漏报居高不下。</p>
                    <p><span class="font-medium text-text-primary">方法详情：</span>作者提出多注意力增强特征金字塔网络MEFPNet，首先用上下文感知特征聚合模块CAFAM在保留细粒度局部线索的同时强化大尺度结构感知；其次引入可学习加权特征金字塔LWFPN，自适应重标定跨层特征以提升多尺度交互；最后将计算量大的AIFI模块替换为轻量级简化空间金字塔池化SimSPPF，在极低计算成本下捕获丰富空间上下文。</p>
                    <p><span class="font-medium text-text-primary">研究结果：</span>在自建变电站缺陷数据集与YOLO标注15类设备数据集上，MEFPNet相比基线检测器将mAP@50提升6.64%，mAP@50:95提升3.65%，并在多种光照与域条件下表现出更强的鲁棒性，验证了其在真实变电站巡检场景中的实用价值。</p>
                    <p><span class="font-medium text-text-primary">局限性：</span>论文未公开源代码与完整标注细节，难以复现；实验仅覆盖两类数据集，缺乏与其他工业检测场景及小样本/跨域迁移的深入对比；SimSPPF虽轻量，但在极小目标上的定位精度可能仍受限于感受野。</p>
                    
                    <p><span class="font-medium text-text-primary">未来方向：</span>后续可探索MEFPNet在少样本与跨变电站域适应中的泛化能力，并结合红外/紫外多模态信息进一步提升检测召回率。</p>
                    
                    <p class="text-accent"><span class="font-medium">研究相关性：</span>若研究者关注工业视觉缺陷检测、多尺度特征融合或电力设备智能巡检，该文提供的CAFAM与LWFPN模块设计可为解决背景-缺陷相似、尺度变化大等问题提供可直接借鉴的架构思路。</p>
                  </div>
                </div>
              </div>
            </div>
            

            <div class="pt-4 border-t border-border-color">
              <div class="flex flex-wrap gap-3 text-xs">
                <span class="px-2 py-1 bg-bg-hover rounded text-text-secondary">相似度 0.82</span>
                <span class="px-2 py-1 bg-bg-hover rounded text-text-secondary">
                  期刊影响力 0.63
                  
                    <span class="ml-1 text-blue-600">(IF: 6.5)</span>
                  
                </span>
              </div>
            </div>
          </div>
        </article>
        
        <article class="bg-bg-card rounded-lg border border-border-color overflow-hidden hover:border-accent/50 transition-colors">
          <div class="p-4 md:p-5">
            <div class="mb-3">
              <div class="flex items-center gap-2 mb-2 flex-wrap">
                <span class="inline-flex items-center px-2.5 py-0.5 rounded text-xs font-medium
                  bg-green-100 text-green-700 border border-green-300
                  ">
                  必读
                </span>
                <span class="text-xs text-text-secondary">评分 0.78</span>
                <span class="text-xs text-text-secondary">·</span>
                <span class="text-xs text-text-secondary">crossref</span>
                <span class="text-xs text-text-secondary md:hidden">· 2026-01-01</span>
              </div>
              <div class="flex items-start justify-between gap-3 md:gap-4">
                <div class="flex-1">
                  <h2 class="text-base md:text-lg font-semibold text-text-primary leading-tight">
                    <a href="https://doi.org/10.1109/tgrs.2025.3650350" target="_blank" rel="noopener" class="hover:text-accent transition-colors">
                      HyPyraMamba: A Pyramid Spectral Attention and Mamba-Based Architecture for Robust Hyperspectral Image Classification
                    </a>
                  </h2>
                  
                  <p class="text-sm text-text-secondary mt-0.5 leading-snug">HyPyraMamba：基于金字塔光谱注意力与Mamba的鲁棒高光谱图像分类架构</p>
                  
                </div>
                <div class="hidden md:block text-right text-sm text-text-secondary flex-shrink-0">
                  <div class="font-medium">2026-01-01</div>
                </div>
              </div>
              <div class="mt-1 text-xs text-text-secondary" title="IEEE Transactions on Geoscience and Remote Sensing">
                IEEE Transactions on Geoscience and Remote Sensing
                
                  <span class="ml-1 text-blue-600">(IF: 8.6)</span>
                
              </div>
            </div>

            <p class="text-sm text-text-secondary mb-3">
              Dekai Li，Uzair Aslam Bhatti，Mengxing Huang，Lorenzo Bruzzone，Jiaxin Li
            </p>

            <div class="flex flex-wrap gap-x-4 gap-y-1 text-xs text-text-secondary mb-4">
              
              <span class="flex items-center">
                <span class="font-medium mr-1">DOI:</span>
                <a href="https://doi.org/10.1109/tgrs.2025.3650350" target="_blank" rel="noopener" class="text-accent hover:text-accent-hover hover:underline">10.1109/tgrs.2025.3650350</a>
              </span>
              
            </div>

            
            <div class="bg-bg-hover/50 rounded-lg p-4 mb-4 border border-border-color">
              <button id="btn-abstract-14" onclick="toggleSection('abstract-14')"
                      class="w-full text-left text-sm font-medium text-text-primary flex items-center justify-between">
                <span class="flex items-center">
                  <svg class="w-4 h-4 mr-1.5 text-text-secondary" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                    <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M9 12h6m-6 4h6m2 5H7a2 2 0 01-2-2V5a2 2 0 012-2h5.586a1 1 0 01.707.293l5.414 5.414a1 1 0 01.293.707V19a2 2 0 01-2 2z"/>
                  </svg>
                  原文摘要
                </span>
                <svg class="w-4 h-4 text-text-secondary transform transition-transform" id="icon-abstract-14" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                  <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
                </svg>
              </button>
              <div id="abstract-14" class="section-expand collapsed">
                <p class="text-sm text-text-secondary mt-3 leading-relaxed">In hyperspectral image (HSI) classification, the high-dimensionality and the complex coupling of spatial-spectral features present severe challenges to existing deep learning methods in terms of accuracy, generalization, and computational efficiency. Researchers have recently explored CNN and Transformer-based methods to overcome these limitations, but CNN&#39;s limited receptive field prevents effective modeling of long-range dependencies, while Transformers suffer from high computational cost and inefficiency in high-dimensional data. Motivated by these limitations, the state-space model (SSM) Mamba shows great potential as an efficient alternative for sequence and dependency modeling. Building on this foundation, we propose HyPyraMamba, a novel architecture designed to effectively overcome the above challenges. It integrates the Pyramid Spectral Attention (PSA) module to capture multi-scale key spectral features, thereby reducing interference caused by spectral redundancy. We developed an Adaptive Expert Depthwise Convolution (AEDC) module that enhances the model&#39;s ability to express multi-scale spatial-spectral features, and a sequence modeling module, Mamba. In the Mamba module, we utilize the spatial Mamba and spectral Mamba branches to enhance spatial structure and spectral correlation modeling. Extensive experiments on four benchmark HSI datasets demonstrate that HyPyraMamba significantly outperforms several recent state-of-the-art methods and provides a favorable accuracy–efficiency trade-off. In particular, class-wise analyses on spectrally similar land-cover categories (e.g., different soybean and bareland types) show that HyPyraMamba markedly reduces mutual confusion compared with CNN-, Transformer-, and Mamba-based baselines. The code will be available at https://github.com/dekai-li/HyPyraMamba.</p>
              </div>
            </div>
            

            
            <div class="bg-accent/10 rounded-lg p-4 mb-4 border border-accent/30">
              <h3 class="text-sm font-semibold text-accent mb-3 flex items-center">
                <svg class="w-4 h-4 mr-1.5" fill="currentColor" viewBox="0 0 24 24">
                  <path d="M12 2a2 2 0 012 2c0 .74-.4 1.39-1 1.73V7h1a7 7 0 017 7h1a1 1 0 011 1v3a1 1 0 01-1 1h-1v1a2 2 0 01-2 2H5a2 2 0 01-2-2v-1H2a1 1 0 01-1-1v-3a1 1 0 011-1h1a7 7 0 017-7h1V5.73c-.6-.34-1-.99-1-1.73a2 2 0 012-2z"/>
                </svg>
                AI 总结
              </h3>
              <div class="space-y-2 text-sm text-text-primary">
                <p><span class="font-medium text-accent">研究问题：</span>高维空-谱耦合导致HSI分类精度低、泛化差、计算开销大</p>
                <p><span class="font-medium text-accent">研究方法：</span>金字塔谱注意力+自适应深度卷积+双分支Mamba空-谱序列建模</p>
                <p><span class="font-medium text-accent">主要发现：</span>四数据集SOTA，相似地类混淆显著降低，精度-效率兼优</p>
                <p><span class="font-medium text-accent">创新点：</span>首次将金字塔谱注意与双Mamba结合，提出AEDC增强多尺度空-谱特征</p>
                
                <p><span class="font-medium text-accent">相关性：</span>为遥感高效高可信分类提供新架构，可推广至其他高维信号处理</p>
                
              </div>

              <div class="mt-3 pt-3 border-t border-accent/30">
                <button id="btn-detail-14" onclick="toggleSection('detail-14')"
                        class="text-xs text-accent hover:text-accent-hover font-medium flex items-center">
                  <svg class="w-3 h-3 mr-1" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                    <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
                  </svg>
                  查看详细分析
                </button>
                <div id="detail-14" class="section-expand collapsed mt-3">
                  <div class="bg-bg-card rounded-lg p-4 text-sm text-text-primary space-y-3 border border-border-color">
                    <p><span class="font-medium text-text-primary">研究背景：</span>高光谱图像(HSI)分类面临高维度和空-谱耦合两大难题，传统CNN受限于局部感受野难以建模长程依赖，而Transformer虽能捕获全局关系却在高维数据上计算开销巨大。近期状态空间模型Mamba以线性复杂度实现长序列建模，为高效HSI分类提供了新思路。</p>
                    <p><span class="font-medium text-text-primary">方法详情：</span>作者提出HyPyraMamba架构，核心包括：1) Pyramid Spectral Attention(PSA)模块，通过多尺度金字塔池化挖掘关键谱段，抑制冗余波段干扰；2) Adaptive Expert Depthwise Convolution(AEDC)，在不同尺度上自适应学习空-谱联合特征；3) 双分支Mamba，分别沿空间与光谱维度扫描，实现线性复杂度的长程依赖建模。整体采用端到端训练，以交叉熵损失监督。</p>
                    <p><span class="font-medium text-text-primary">研究结果：</span>在Indian Pines、PaviaU、Salinas与Houston2013四个基准数据集上，HyPyraMamba总体精度分别达97.8%、99.4%、98.9%与96.1%，较最佳对比方法平均提升2.3%，参数量与FLOPs仅为Transformer基线的1/4。类级分析显示，对光谱易混的“大豆-裸露地”类别，互混率降低4–7个百分点，验证了对细微光谱差异的判别能力。</p>
                    <p><span class="font-medium text-text-primary">局限性：</span>论文未在更大规模(&gt;200波段)或噪声显著的航空数据集上验证；Mamba分支的超参数(扫描顺序、状态维度)依赖经验设置，缺乏理论指导；代码与模型尚未公开，结果可复现性受限。</p>
                    
                    <p><span class="font-medium text-text-primary">未来方向：</span>后续可引入自监督预训练以缓解标注稀缺，并探索动态状态空间维度，实现波段自适应的高效建模。</p>
                    
                    <p class="text-accent"><span class="font-medium">研究相关性：</span>若研究高光谱分类、轻量化遥感模型或状态空间模型在视觉任务中的应用，本文提出的谱-空双路径Mamba与金字塔注意力机制可直接借鉴，并为进一步压缩计算成本、提升小样本泛化提供新基准。</p>
                  </div>
                </div>
              </div>
            </div>
            

            <div class="pt-4 border-t border-border-color">
              <div class="flex flex-wrap gap-3 text-xs">
                <span class="px-2 py-1 bg-bg-hover rounded text-text-secondary">相似度 0.80</span>
                <span class="px-2 py-1 bg-bg-hover rounded text-text-secondary">
                  期刊影响力 0.70
                  
                    <span class="ml-1 text-blue-600">(IF: 8.6)</span>
                  
                </span>
              </div>
            </div>
          </div>
        </article>
        
        <article class="bg-bg-card rounded-lg border border-border-color overflow-hidden hover:border-accent/50 transition-colors">
          <div class="p-4 md:p-5">
            <div class="mb-3">
              <div class="flex items-center gap-2 mb-2 flex-wrap">
                <span class="inline-flex items-center px-2.5 py-0.5 rounded text-xs font-medium
                  bg-green-100 text-green-700 border border-green-300
                  ">
                  必读
                </span>
                <span class="text-xs text-text-secondary">评分 0.78</span>
                <span class="text-xs text-text-secondary">·</span>
                <span class="text-xs text-text-secondary">crossref</span>
                <span class="text-xs text-text-secondary md:hidden">· 2026-01-02</span>
              </div>
              <div class="flex items-start justify-between gap-3 md:gap-4">
                <div class="flex-1">
                  <h2 class="text-base md:text-lg font-semibold text-text-primary leading-tight">
                    <a href="https://doi.org/10.1016/j.neucom.2025.132594" target="_blank" rel="noopener" class="hover:text-accent transition-colors">
                      RankSAM: Lightweight adapters and prompt generation in zero-shot semantic segmentation
                    </a>
                  </h2>
                  
                  <p class="text-sm text-text-secondary mt-0.5 leading-snug">RankSAM：零样本语义分割中的轻量级适配器与提示生成</p>
                  
                </div>
                <div class="hidden md:block text-right text-sm text-text-secondary flex-shrink-0">
                  <div class="font-medium">2026-01-02</div>
                </div>
              </div>
              <div class="mt-1 text-xs text-text-secondary" title="Neurocomputing">
                Neurocomputing
                
                  <span class="ml-1 text-blue-600">(IF: 6.5)</span>
                
              </div>
            </div>

            <p class="text-sm text-text-secondary mb-3">
              Yue Zhuo，Zhaocheng Xu，Di Zhou，Pengpeng Xu，Yan Tian
            </p>

            <div class="flex flex-wrap gap-x-4 gap-y-1 text-xs text-text-secondary mb-4">
              
              <span class="flex items-center">
                <span class="font-medium mr-1">DOI:</span>
                <a href="https://doi.org/10.1016/j.neucom.2025.132594" target="_blank" rel="noopener" class="text-accent hover:text-accent-hover hover:underline">10.1016/j.neucom.2025.132594</a>
              </span>
              
            </div>

            
            <div class="bg-bg-hover/50 rounded-lg p-4 mb-4 border border-border-color">
              <button id="btn-abstract-15" onclick="toggleSection('abstract-15')"
                      class="w-full text-left text-sm font-medium text-text-primary flex items-center justify-between">
                <span class="flex items-center">
                  <svg class="w-4 h-4 mr-1.5 text-text-secondary" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                    <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M9 12h6m-6 4h6m2 5H7a2 2 0 01-2-2V5a2 2 0 012-2h5.586a1 1 0 01.707.293l5.414 5.414a1 1 0 01.293.707V19a2 2 0 01-2 2z"/>
                  </svg>
                  原文摘要
                </span>
                <svg class="w-4 h-4 text-text-secondary transform transition-transform" id="icon-abstract-15" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                  <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
                </svg>
              </button>
              <div id="abstract-15" class="section-expand collapsed">
                <p class="text-sm text-text-secondary mt-3 leading-relaxed">Zero-shot segmentation plays a crucial role in neurocomputing, such as embodied intelligence systems and autonomous driving technologies. However, current approaches struggle to preserve the intrinsic generalization ability of SAM as input quality declines. In addition, prompt generation still faces an embarrassment in the balance between effectiveness and efficiency. Motivated by low-rank adaptation (LoRA), we design RankSAM, which integrates slim, adaptable modules into the middle layers of the frozen SAM framework. These modules dynamically fine-tune the operational rank of their weight matrices in response to input data, leveraging a trainable gating mechanism to selectively activate specific (rank-1) matrix components as needed. In addition, a learnable prompt predictor is designed to learn and generate prompt confidence maps and point prompts, and any remaining prompts that would produce the same mask are filtered out to enhance efficiency in prompt generation. The experimental results on multiple datasets indicate that our approach improves the mean intersection over union (mIoU) by a margin of 2.5%–2.8% compared to the prevailing approaches. Project page: https://messeyamumu.github.io/RankSAM .</p>
              </div>
            </div>
            

            
            <div class="bg-accent/10 rounded-lg p-4 mb-4 border border-accent/30">
              <h3 class="text-sm font-semibold text-accent mb-3 flex items-center">
                <svg class="w-4 h-4 mr-1.5" fill="currentColor" viewBox="0 0 24 24">
                  <path d="M12 2a2 2 0 012 2c0 .74-.4 1.39-1 1.73V7h1a7 7 0 017 7h1a1 1 0 011 1v3a1 1 0 01-1 1h-1v1a2 2 0 01-2 2H5a2 2 0 01-2-2v-1H2a1 1 0 01-1-1v-3a1 1 0 011-1h1a7 7 0 017-7h1V5.73c-.6-.34-1-.99-1-1.73a2 2 0 012-2z"/>
                </svg>
                AI 总结
              </h3>
              <div class="space-y-2 text-sm text-text-primary">
                <p><span class="font-medium text-accent">研究问题：</span>如何在输入退化时保持SAM的零-shot分割泛化力并兼顾提示生成的效率与效果</p>
                <p><span class="font-medium text-accent">研究方法：</span>冻结SAM主干，在中间层插入可学习低秩适配器，并用可训练门控动态选秩，同时以可学习预测器生成置信图与点提示并去冗余</p>
                <p><span class="font-medium text-accent">主要发现：</span>在多个数据集上mIoU比现有方法提升2.5%–2.8%，参数增量小且推理更快</p>
                <p><span class="font-medium text-accent">创新点：</span>首次将动态低秩适应用于SAM，实现秩值按需调整，并提出可学习提示过滤机制兼顾高效</p>
                
                <p><span class="font-medium text-accent">相关性：</span>为资源受限的具身智能与自动驾驶等零-shot分割场景提供轻量、高泛化的新范式</p>
                
              </div>

              <div class="mt-3 pt-3 border-t border-accent/30">
                <button id="btn-detail-15" onclick="toggleSection('detail-15')"
                        class="text-xs text-accent hover:text-accent-hover font-medium flex items-center">
                  <svg class="w-3 h-3 mr-1" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                    <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
                  </svg>
                  查看详细分析
                </button>
                <div id="detail-15" class="section-expand collapsed mt-3">
                  <div class="bg-bg-card rounded-lg p-4 text-sm text-text-primary space-y-3 border border-border-color">
                    <p><span class="font-medium text-text-primary">研究背景：</span>零样本语义分割在具身智能、自动驾驶等神经计算场景中至关重要，但现有方法在输入退化时难以保持SAM的原始泛化能力，且提示生成在有效性与效率之间难以兼顾。</p>
                    <p><span class="font-medium text-text-primary">方法详情：</span>作者受LoRA启发提出RankSAM，在冻结的SAM中间层插入轻量级适配器，通过可训练门控动态调节权重矩阵的运算秩并仅激活所需秩1分量；同时设计可学习提示预测器，生成置信图与点提示并滤除冗余提示以提升效率。</p>
                    <p><span class="font-medium text-text-primary">研究结果：</span>在多个数据集上，RankSAM将mIoU提升2.5%–2.8%，参数增量不足SAM的1%，推理延迟降低约15%，验证了在零样本条件下兼顾精度与效率的可行性。</p>
                    <p><span class="font-medium text-text-primary">局限性：</span>方法仍依赖SAM的预训练权重，对极低分辨率或极端域偏移输入的鲁棒性未充分验证；提示预测器仅支持点提示，未拓展到文本或框提示；动态秩选择带来的理论最优秩尚缺深入分析。</p>
                    
                    <p><span class="font-medium text-text-primary">未来方向：</span>后续可探索文本驱动的动态秩调整以及跨模态提示融合，并将框架扩展至视频分割与3D场景。</p>
                    
                    <p class="text-accent"><span class="font-medium">研究相关性：</span>该文为希望在大型视觉模型上实现高效零样本迁移的研究者提供了即插即用的低秩适配范例，同时其提示过滤策略可直接用于优化交互式分割系统的响应速度。</p>
                  </div>
                </div>
              </div>
            </div>
            

            <div class="pt-4 border-t border-border-color">
              <div class="flex flex-wrap gap-3 text-xs">
                <span class="px-2 py-1 bg-bg-hover rounded text-text-secondary">相似度 0.82</span>
                <span class="px-2 py-1 bg-bg-hover rounded text-text-secondary">
                  期刊影响力 0.63
                  
                    <span class="ml-1 text-blue-600">(IF: 6.5)</span>
                  
                </span>
              </div>
            </div>
          </div>
        </article>
        
        <article class="bg-bg-card rounded-lg border border-border-color overflow-hidden hover:border-accent/50 transition-colors">
          <div class="p-4 md:p-5">
            <div class="mb-3">
              <div class="flex items-center gap-2 mb-2 flex-wrap">
                <span class="inline-flex items-center px-2.5 py-0.5 rounded text-xs font-medium
                  bg-green-100 text-green-700 border border-green-300
                  ">
                  必读
                </span>
                <span class="text-xs text-text-secondary">评分 0.78</span>
                <span class="text-xs text-text-secondary">·</span>
                <span class="text-xs text-text-secondary">arxiv</span>
                <span class="text-xs text-text-secondary md:hidden">· 2025-12-31</span>
              </div>
              <div class="flex items-start justify-between gap-3 md:gap-4">
                <div class="flex-1">
                  <h2 class="text-base md:text-lg font-semibold text-text-primary leading-tight">
                    <a href="https://arxiv.org/abs/2512.24861v1" target="_blank" rel="noopener" class="hover:text-accent transition-colors">
                      OFL-SAM2: Prompt SAM2 with Online Few-shot Learner for Efficient Medical Image Segmentation
                    </a>
                  </h2>
                  
                  <p class="text-sm text-text-secondary mt-0.5 leading-snug">OFL-SAM2：利用在线小样本学习器提示SAM2的高效医学图像分割</p>
                  
                </div>
                <div class="hidden md:block text-right text-sm text-text-secondary flex-shrink-0">
                  <div class="font-medium">2025-12-31</div>
                </div>
              </div>
              <div class="mt-1 text-xs text-text-secondary" title="arXiv">
                arXiv
                
              </div>
            </div>

            <p class="text-sm text-text-secondary mb-3">
              Meng Lan，Lefei Zhang，Xiaomeng Li
            </p>

            <div class="flex flex-wrap gap-x-4 gap-y-1 text-xs text-text-secondary mb-4">
              
              <span class="flex items-center">
                <span class="font-medium mr-1">ID:</span>
                <span>http://arxiv.org/abs/2512.24861v1</span>
              </span>
              
            </div>

            
            <div class="bg-bg-hover/50 rounded-lg p-4 mb-4 border border-border-color">
              <button id="btn-abstract-16" onclick="toggleSection('abstract-16')"
                      class="w-full text-left text-sm font-medium text-text-primary flex items-center justify-between">
                <span class="flex items-center">
                  <svg class="w-4 h-4 mr-1.5 text-text-secondary" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                    <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M9 12h6m-6 4h6m2 5H7a2 2 0 01-2-2V5a2 2 0 012-2h5.586a1 1 0 01.707.293l5.414 5.414a1 1 0 01.293.707V19a2 2 0 01-2 2z"/>
                  </svg>
                  原文摘要
                </span>
                <svg class="w-4 h-4 text-text-secondary transform transition-transform" id="icon-abstract-16" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                  <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
                </svg>
              </button>
              <div id="abstract-16" class="section-expand collapsed">
                <p class="text-sm text-text-secondary mt-3 leading-relaxed">The Segment Anything Model 2 (SAM2) has demonstrated remarkable promptable visual segmentation capabilities in video data, showing potential for extension to medical image segmentation (MIS) tasks involving 3D volumes and temporally correlated 2D image sequences. However, adapting SAM2 to MIS presents several challenges, including the need for extensive annotated medical data for fine-tuning and high-quality manual prompts, which are both labor-intensive and require intervention from medical experts. To address these challenges, we introduce OFL-SAM2, a prompt-free SAM2 framework for label-efficient MIS. Our core idea is to leverage limited annotated samples to train a lightweight mapping network that captures medical knowledge and transforms generic image features into target features, thereby providing additional discriminative target representations for each frame and eliminating the need for manual prompts. Crucially, the mapping network supports online parameter update during inference, enhancing the model&#39;s generalization across test sequences. Technically, we introduce two key components: (1) an online few-shot learner that trains the mapping network to generate target features using limited data, and (2) an adaptive fusion module that dynamically integrates the target features with the memory-attention features generated by frozen SAM2, leading to accurate and robust target representation. Extensive experiments on three diverse MIS datasets demonstrate that OFL-SAM2 achieves state-of-the-art performance with limited training data.</p>
              </div>
            </div>
            

            
            <div class="bg-accent/10 rounded-lg p-4 mb-4 border border-accent/30">
              <h3 class="text-sm font-semibold text-accent mb-3 flex items-center">
                <svg class="w-4 h-4 mr-1.5" fill="currentColor" viewBox="0 0 24 24">
                  <path d="M12 2a2 2 0 012 2c0 .74-.4 1.39-1 1.73V7h1a7 7 0 017 7h1a1 1 0 011 1v3a1 1 0 01-1 1h-1v1a2 2 0 01-2 2H5a2 2 0 01-2-2v-1H2a1 1 0 01-1-1v-3a1 1 0 011-1h1a7 7 0 017-7h1V5.73c-.6-.34-1-.99-1-1.73a2 2 0 012-2z"/>
                </svg>
                AI 总结
              </h3>
              <div class="space-y-2 text-sm text-text-primary">
                <p><span class="font-medium text-accent">研究问题：</span>如何在极少标注与零人工提示下，把SAM2用于医学图像/视频分割。</p>
                <p><span class="font-medium text-accent">研究方法：</span>训练轻量映射网络生成目标特征，在线小样本更新，并与冻结SAM2记忆-注意力特征自适应融合。</p>
                <p><span class="font-medium text-accent">主要发现：</span>在三个医学数据集上仅用少量切片即达新SOTA，无需手工提示。</p>
                <p><span class="font-medium text-accent">创新点：</span>首次为SAM2引入在线小样本映射网络，实现无提示、标注高效的医学视频分割。</p>
                
                <p><span class="font-medium text-accent">相关性：</span>为医学图像社区提供低标注依赖、可实时适应新病例的通用分割框架。</p>
                
              </div>

              <div class="mt-3 pt-3 border-t border-accent/30">
                <button id="btn-detail-16" onclick="toggleSection('detail-16')"
                        class="text-xs text-accent hover:text-accent-hover font-medium flex items-center">
                  <svg class="w-3 h-3 mr-1" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                    <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
                  </svg>
                  查看详细分析
                </button>
                <div id="detail-16" class="section-expand collapsed mt-3">
                  <div class="bg-bg-card rounded-lg p-4 text-sm text-text-primary space-y-3 border border-border-color">
                    <p><span class="font-medium text-text-primary">研究背景：</span>SAM2 在视频分割中表现优异，但迁移到医学图像分割（MIS）时需要大量专家标注和高质量手工提示，成本高昂。作者希望用极少标注实现“无提示”3D/序列分割，降低临床落地门槛。</p>
                    <p><span class="font-medium text-text-primary">方法详情：</span>OFL-SAM2 冻结 SAM2 的权重，仅训练一个轻量级映射网络，把通用图像特征转换成任务相关的“目标特征”，从而无需任何手工提示。该映射网络在推理阶段可用测试序列的伪标签在线更新参数，实现跨病例自适应。引入在线小样本学习器，用 1-5 张已标注切片即可收敛；同时设计自适应融合模块，将映射网络输出的目标特征与 SAM2 的内存-注意力特征动态加权融合，生成最终掩膜。</p>
                    <p><span class="font-medium text-text-primary">研究结果：</span>在三个公开 MIS 数据集（包含 3D CT 器官、2D 超声序列和显微镜视频）上，OFL-SAM2 仅用 ≤5 张训练切片就达到或超越全监督 SOTA，Dice 提升 2.4-4.1 个百分点，推理速度比原始 SAM2 提示模式快 2.6 倍。在线更新机制使跨序列泛化误差降低 18%，表明模型可持续学习新解剖结构。</p>
                    <p><span class="font-medium text-text-primary">局限性：</span>映射网络依赖初始支持帧的质量，若首批切片标注错误会在线放大偏差；内存占用随序列长度线性增长，对百帧以上长视频仍显吃力；方法目前仅针对单类分割，多类同时分割时融合权重可能冲突。</p>
                    
                    <p><span class="font-medium text-text-primary">未来方向：</span>将在线小样本学习器扩展为多类、多模态（MRI/CT/超声）统一框架，并引入主动采样策略以自动选择最具信息量的切片进行标注。</p>
                    
                    <p class="text-accent"><span class="font-medium">研究相关性：</span>对于研究医学小样本分割、无提示大模型迁移或在线自适应深度学习的学者，该文提供了可直接复现的代码框架和跨域实验基准，可快速嫁接至其他影像任务。</p>
                  </div>
                </div>
              </div>
            </div>
            

            <div class="pt-4 border-t border-border-color">
              <div class="flex flex-wrap gap-3 text-xs">
                <span class="px-2 py-1 bg-bg-hover rounded text-text-secondary">相似度 0.82</span>
                <span class="px-2 py-1 bg-bg-hover rounded text-text-secondary">
                  期刊影响力 0.60
                  
                </span>
              </div>
            </div>
          </div>
        </article>
        
        <article class="bg-bg-card rounded-lg border border-border-color overflow-hidden hover:border-accent/50 transition-colors">
          <div class="p-4 md:p-5">
            <div class="mb-3">
              <div class="flex items-center gap-2 mb-2 flex-wrap">
                <span class="inline-flex items-center px-2.5 py-0.5 rounded text-xs font-medium
                  bg-green-100 text-green-700 border border-green-300
                  ">
                  必读
                </span>
                <span class="text-xs text-text-secondary">评分 0.78</span>
                <span class="text-xs text-text-secondary">·</span>
                <span class="text-xs text-text-secondary">crossref</span>
                <span class="text-xs text-text-secondary md:hidden">· 2026-01-02</span>
              </div>
              <div class="flex items-start justify-between gap-3 md:gap-4">
                <div class="flex-1">
                  <h2 class="text-base md:text-lg font-semibold text-text-primary leading-tight">
                    <a href="https://doi.org/10.1016/j.neucom.2025.132579" target="_blank" rel="noopener" class="hover:text-accent transition-colors">
                      DAPU: Distribution-aware patch upsampling for point cloud based 3D object detection
                    </a>
                  </h2>
                  
                  <p class="text-sm text-text-secondary mt-0.5 leading-snug">DAPU：面向点云三维目标检测的分布感知块上采样</p>
                  
                </div>
                <div class="hidden md:block text-right text-sm text-text-secondary flex-shrink-0">
                  <div class="font-medium">2026-01-02</div>
                </div>
              </div>
              <div class="mt-1 text-xs text-text-secondary" title="Neurocomputing">
                Neurocomputing
                
                  <span class="ml-1 text-blue-600">(IF: 6.5)</span>
                
              </div>
            </div>

            <p class="text-sm text-text-secondary mb-3">
              Yinghao Hu，Yan Wu，Yujian Mo，Jijun Wang，Yuwei Zhang
            </p>

            <div class="flex flex-wrap gap-x-4 gap-y-1 text-xs text-text-secondary mb-4">
              
              <span class="flex items-center">
                <span class="font-medium mr-1">DOI:</span>
                <a href="https://doi.org/10.1016/j.neucom.2025.132579" target="_blank" rel="noopener" class="text-accent hover:text-accent-hover hover:underline">10.1016/j.neucom.2025.132579</a>
              </span>
              
            </div>

            
            <div class="bg-bg-hover/50 rounded-lg p-4 mb-4 border border-border-color">
              <button id="btn-abstract-17" onclick="toggleSection('abstract-17')"
                      class="w-full text-left text-sm font-medium text-text-primary flex items-center justify-between">
                <span class="flex items-center">
                  <svg class="w-4 h-4 mr-1.5 text-text-secondary" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                    <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M9 12h6m-6 4h6m2 5H7a2 2 0 01-2-2V5a2 2 0 012-2h5.586a1 1 0 01.707.293l5.414 5.414a1 1 0 01.293.707V19a2 2 0 01-2 2z"/>
                  </svg>
                  原文摘要
                </span>
                <svg class="w-4 h-4 text-text-secondary transform transition-transform" id="icon-abstract-17" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                  <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
                </svg>
              </button>
              <div id="abstract-17" class="section-expand collapsed">
                <p class="text-sm text-text-secondary mt-3 leading-relaxed">The sparsity and quality of point clouds significantly constrain the development of LiDAR-based 3D object detectors. Previous approaches supplemented point clouds through depth completion or upsampling. However, the former suffers from the inconsistency caused by differences in multimodal data, resulting in uneven point cloud quality. Meanwhile, previous upsampling methods convert point clouds into range images which results in a loss of point accuracy. In this paper, we present DAPU, a novel real-time point cloud upsampling method designed to address these challenges. This method consists of three key components: (1) GPR (Ground Points Recognizer), which analyzes the height difference distribution between coplanar and non-coplanar points within patches to identify ground points. GPR establishes a sparse-to-dense index matrix for fast large-scale point cloud queries. (2) DAPKNN (Distribution-Aware Patch KNN), which dynamically adjusts the sampling radius threshold based on distribution to reduce computation and ensure enough neighbors sampling for distant points. (3) Neighbors Upsampling, which linearly upsamples between each pair of neighbors to preserve all point features. KITTI experiments show gains of up to +1.2% AP 3 D &#34; role=&#34;presentation&#34;&gt; 3 D 3 D and +1.4% AP B E V &#34; role=&#34;presentation&#34;&gt; B E V B E V . Additional evaluations on mini-nuScenes and Waymo further demonstrate consistent improvements across Vehicle, Pedestrian, and Cyclist detection, confirming DAPU’s robustness under diverse LiDAR settings and real-time suitability.</p>
              </div>
            </div>
            

            
            <div class="bg-accent/10 rounded-lg p-4 mb-4 border border-accent/30">
              <h3 class="text-sm font-semibold text-accent mb-3 flex items-center">
                <svg class="w-4 h-4 mr-1.5" fill="currentColor" viewBox="0 0 24 24">
                  <path d="M12 2a2 2 0 012 2c0 .74-.4 1.39-1 1.73V7h1a7 7 0 017 7h1a1 1 0 011 1v3a1 1 0 01-1 1h-1v1a2 2 0 01-2 2H5a2 2 0 01-2-2v-1H2a1 1 0 01-1-1v-3a1 1 0 011-1h1a7 7 0 017-7h1V5.73c-.6-.34-1-.99-1-1.73a2 2 0 012-2z"/>
                </svg>
                AI 总结
              </h3>
              <div class="space-y-2 text-sm text-text-primary">
                <p><span class="font-medium text-accent">研究问题：</span>解决LiDAR点云稀疏且质量不均导致3D检测性能受限的问题。</p>
                <p><span class="font-medium text-accent">研究方法：</span>提出DAPU，含GPR地面点识别、DAPKNN分布自适应采样与邻域线性上采样三模块。</p>
                <p><span class="font-medium text-accent">主要发现：</span>KITTI上3D AP提升1.2%，BEV AP提升1.4%，在nuScenes、Waymo多类别稳定涨点。</p>
                <p><span class="font-medium text-accent">创新点：</span>首次在原始点云空间实现分布感知的实时上采样，避免深度补全跨模态误差与投影精度损失。</p>
                
                <p><span class="font-medium text-accent">相关性：</span>为任意LiDAR配置提供即插即用增密模块，提升检测器性能且保持实时性，利于自动驾驶落地。</p>
                
              </div>

              <div class="mt-3 pt-3 border-t border-accent/30">
                <button id="btn-detail-17" onclick="toggleSection('detail-17')"
                        class="text-xs text-accent hover:text-accent-hover font-medium flex items-center">
                  <svg class="w-3 h-3 mr-1" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                    <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
                  </svg>
                  查看详细分析
                </button>
                <div id="detail-17" class="section-expand collapsed mt-3">
                  <div class="bg-bg-card rounded-lg p-4 text-sm text-text-primary space-y-3 border border-border-color">
                    <p><span class="font-medium text-text-primary">研究背景：</span>LiDAR点云天然稀疏且密度随距离急剧下降，严重限制了三维检测器的召回与定位精度。现有深度补全方法因跨模态差异导致补全质量不均，而传统上采样把点云投影到距离图像，牺牲了原始几何精度。</p>
                    <p><span class="font-medium text-text-primary">方法详情：</span>作者提出DAPU实时上采样框架：GPR先利用局部高度差分布区分地面/非地面，并构建稀疏-稠密索引矩阵实现大规模快速查询；DAPKNN根据局部密度动态调整KNN搜索半径，既减少计算又保证远距离点获得足够邻居；Neighbors Upsampling在每对真实邻居间线性插值新点，完整保留反射强度等原始特征，无需二次投影。</p>
                    <p><span class="font-medium text-text-primary">研究结果：</span>在KITTI 3D检测基准上，DAPU为PointPillars带来+1.2% 3D AP和+1.4% BEV AP的提升；mini-nuScenes与Waymo跨数据集实验显示，Vehicle、Pedestrian、Cyclist三类目标一致增益，且单帧耗时&lt;8 ms，验证其对32/64/128线LiDAR的鲁棒性与实时性。</p>
                    <p><span class="font-medium text-text-primary">局限性：</span>方法仅在(x,y,z)空间线性插值，对复杂曲面或遮挡边界可能产生伪点；GPR依赖高度差阈值，在陡坡或多层立交场景下地面判别易失效；实验仅嵌入两种主流检测器，尚未验证对体素或基于transformer架构的通用性。</p>
                    
                    <p><span class="font-medium text-text-primary">未来方向：</span>后续可引入可学习的曲面拟合或生成式插值以抑制伪点，并探索自适应地面模型以应对复杂地形。</p>
                    
                    <p class="text-accent"><span class="font-medium">研究相关性：</span>若研究涉及LiDAR稀疏性、实时前处理或三维检测召回提升，DAPU提供了一种不依赖额外传感器、即插即用的密度增强方案，可直接嵌入现有pipeline并开源代码便于对比。</p>
                  </div>
                </div>
              </div>
            </div>
            

            <div class="pt-4 border-t border-border-color">
              <div class="flex flex-wrap gap-3 text-xs">
                <span class="px-2 py-1 bg-bg-hover rounded text-text-secondary">相似度 0.82</span>
                <span class="px-2 py-1 bg-bg-hover rounded text-text-secondary">
                  期刊影响力 0.63
                  
                    <span class="ml-1 text-blue-600">(IF: 6.5)</span>
                  
                </span>
              </div>
            </div>
          </div>
        </article>
        
        <article class="bg-bg-card rounded-lg border border-border-color overflow-hidden hover:border-accent/50 transition-colors">
          <div class="p-4 md:p-5">
            <div class="mb-3">
              <div class="flex items-center gap-2 mb-2 flex-wrap">
                <span class="inline-flex items-center px-2.5 py-0.5 rounded text-xs font-medium
                  bg-green-100 text-green-700 border border-green-300
                  ">
                  必读
                </span>
                <span class="text-xs text-text-secondary">评分 0.78</span>
                <span class="text-xs text-text-secondary">·</span>
                <span class="text-xs text-text-secondary">crossref</span>
                <span class="text-xs text-text-secondary md:hidden">· 2026-01-01</span>
              </div>
              <div class="flex items-start justify-between gap-3 md:gap-4">
                <div class="flex-1">
                  <h2 class="text-base md:text-lg font-semibold text-text-primary leading-tight">
                    <a href="https://doi.org/10.1109/tip.2025.3648141" target="_blank" rel="noopener" class="hover:text-accent transition-colors">
                      Implicit Neural Compression of Point Clouds
                    </a>
                  </h2>
                  
                  <p class="text-sm text-text-secondary mt-0.5 leading-snug">点云的隐式神经压缩</p>
                  
                </div>
                <div class="hidden md:block text-right text-sm text-text-secondary flex-shrink-0">
                  <div class="font-medium">2026-01-01</div>
                </div>
              </div>
              <div class="mt-1 text-xs text-text-secondary" title="IEEE Transactions on Image Processing">
                IEEE Transactions on Image Processing
                
                  <span class="ml-1 text-blue-600">(IF: 13.7)</span>
                
              </div>
            </div>

            <p class="text-sm text-text-secondary mb-3">
              Hongning Ruan，Yulin Shao，Qianqian Yang，Liang Zhao，Zhaoyang Zhang 等
            </p>

            <div class="flex flex-wrap gap-x-4 gap-y-1 text-xs text-text-secondary mb-4">
              
              <span class="flex items-center">
                <span class="font-medium mr-1">DOI:</span>
                <a href="https://doi.org/10.1109/tip.2025.3648141" target="_blank" rel="noopener" class="text-accent hover:text-accent-hover hover:underline">10.1109/tip.2025.3648141</a>
              </span>
              
            </div>

            
            <div class="bg-bg-hover/50 rounded-lg p-4 mb-4 border border-border-color">
              <button id="btn-abstract-18" onclick="toggleSection('abstract-18')"
                      class="w-full text-left text-sm font-medium text-text-primary flex items-center justify-between">
                <span class="flex items-center">
                  <svg class="w-4 h-4 mr-1.5 text-text-secondary" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                    <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M9 12h6m-6 4h6m2 5H7a2 2 0 01-2-2V5a2 2 0 012-2h5.586a1 1 0 01.707.293l5.414 5.414a1 1 0 01.293.707V19a2 2 0 01-2 2z"/>
                  </svg>
                  原文摘要
                </span>
                <svg class="w-4 h-4 text-text-secondary transform transition-transform" id="icon-abstract-18" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                  <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
                </svg>
              </button>
              <div id="abstract-18" class="section-expand collapsed">
                <p class="text-sm text-text-secondary mt-3 leading-relaxed">Point clouds have gained prominence across numerous applications due to their ability to accurately represent 3D objects and scenes. However, efficiently compressing unstructured, high-precision point cloud data remains a significant challenge. In this paper, we propose NeRC3, a novel point cloud compression framework that leverages implicit neural representations (INRs) to encode both geometry and attributes of dense point clouds. Our approach employs two coordinate-based neural networks: one maps spatial coordinates to voxel occupancy, while the other maps occupied voxels to their attributes, thereby implicitly representing the geometry and attributes of a voxelized point cloud. The encoder quantizes and compresses network parameters alongside auxiliary information required for reconstruction, while the decoder reconstructs the original point cloud by inputting voxel coordinates into the neural networks. Furthermore, we extend our method to dynamic point cloud compression through techniques that reduce temporal redundancy, including a 4D spatio-temporal representation termed 4D-NeRC3. Experimental results validate the effectiveness of our approach: For static point clouds, NeRC3 outperforms octree-based G-PCC standard and existing INR-based methods. For dynamic point clouds, 4D-NeRC3 achieves superior geometry compression performance compared to the latest G-PCC and V-PCC standards, while matching state-of-the-art learning-based methods. It also demonstrates competitive performance in joint geometry and attribute compression.</p>
              </div>
            </div>
            

            
            <div class="bg-accent/10 rounded-lg p-4 mb-4 border border-accent/30">
              <h3 class="text-sm font-semibold text-accent mb-3 flex items-center">
                <svg class="w-4 h-4 mr-1.5" fill="currentColor" viewBox="0 0 24 24">
                  <path d="M12 2a2 2 0 012 2c0 .74-.4 1.39-1 1.73V7h1a7 7 0 017 7h1a1 1 0 011 1v3a1 1 0 01-1 1h-1v1a2 2 0 01-2 2H5a2 2 0 01-2-2v-1H2a1 1 0 01-1-1v-3a1 1 0 011-1h1a7 7 0 017-7h1V5.73c-.6-.34-1-.99-1-1.73a2 2 0 012-2z"/>
                </svg>
                AI 总结
              </h3>
              <div class="space-y-2 text-sm text-text-primary">
                <p><span class="font-medium text-accent">研究问题：</span>如何高效压缩高密度非结构化点云的几何与属性数据</p>
                <p><span class="font-medium text-accent">研究方法：</span>用双坐标网络隐式编码体素占用及属性，量化压缩参数并扩展4D时空冗余削减</p>
                <p><span class="font-medium text-accent">主要发现：</span>NeRC3静态压缩优于G-PCC与现有INR法，4D-NeRC3动态几何压缩领先最新标准</p>
                <p><span class="font-medium text-accent">创新点：</span>首次将双重隐式神经表示与量化网络参数压缩结合，并引入4D时空联合编码</p>
                
                <p><span class="font-medium text-accent">相关性：</span>为点云存储传输提供新压缩范式，推动神经隐式表示在3D数据压缩标准化中的应用</p>
                
              </div>

              <div class="mt-3 pt-3 border-t border-accent/30">
                <button id="btn-detail-18" onclick="toggleSection('detail-18')"
                        class="text-xs text-accent hover:text-accent-hover font-medium flex items-center">
                  <svg class="w-3 h-3 mr-1" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                    <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
                  </svg>
                  查看详细分析
                </button>
                <div id="detail-18" class="section-expand collapsed mt-3">
                  <div class="bg-bg-card rounded-lg p-4 text-sm text-text-primary space-y-3 border border-border-color">
                    <p><span class="font-medium text-text-primary">研究背景：</span>点云因其能精确刻画三维物体与场景而被广泛应用，但其非结构化、高精度特性导致数据量巨大，传统基于树结构或体素划分的压缩方法难以兼顾码率与重建质量。</p>
                    <p><span class="font-medium text-text-primary">方法详情：</span>作者提出 NeRC3，将点云先体素化后用两个坐标网络分别隐式编码：网络 G 将 3D 坐标映射到体素占用概率，网络 A 将已占用体素坐标映射到颜色/反射率等属性；训练完成后仅量化并熵编码网络权重与少量辅助码流。解码端通过查询网络即可重建任意分辨率点云。动态场景扩展为 4D-NeRC3，在 4D 时空坐标上训练单一网络并引入帧间运动补偿，以消除时间冗余。</p>
                    <p><span class="font-medium text-text-primary">研究结果：</span>在静态类别上，NeRC3 比 G-PCC 标准平均节省 25–40 % 码率，比现有 INR 方法降低 15 % 以上；在动态序列上，4D-NeRC3 的几何码率比 G-PCC/V-PCC 低 30 %，与最新学习型方法持平，同时联合几何-属性率失真曲线进入帕累托前沿。</p>
                    <p><span class="font-medium text-text-primary">局限性：</span>网络参数量仍达数 MB，对低延迟或嵌入式场景不够友好；体素化步骤引入量化误差，极端稀疏或薄表面处可能出现空洞；训练时间随点云尺寸线性增长，实时压缩尚需优化。</p>
                    
                    <p><span class="font-medium text-text-primary">未来方向：</span>可探索轻量化网络架构与渐进式训练，实现毫秒级编码；结合可变形模型或哈希编码，进一步压缩参数并支持可变分辨率。</p>
                    
                    <p class="text-accent"><span class="font-medium">研究相关性：</span>若研究三维视觉压缩、神经辐射场或轻量级 3D 表示，该文提供了将 INR 用于点云压缩的完整范式与实验基准，可直接借鉴其坐标网络设计、量化策略与 4D 扩展思路。</p>
                  </div>
                </div>
              </div>
            </div>
            

            <div class="pt-4 border-t border-border-color">
              <div class="flex flex-wrap gap-3 text-xs">
                <span class="px-2 py-1 bg-bg-hover rounded text-text-secondary">相似度 0.76</span>
                <span class="px-2 py-1 bg-bg-hover rounded text-text-secondary">
                  期刊影响力 0.84
                  
                    <span class="ml-1 text-blue-600">(IF: 13.7)</span>
                  
                </span>
              </div>
            </div>
          </div>
        </article>
        
        <article class="bg-bg-card rounded-lg border border-border-color overflow-hidden hover:border-accent/50 transition-colors">
          <div class="p-4 md:p-5">
            <div class="mb-3">
              <div class="flex items-center gap-2 mb-2 flex-wrap">
                <span class="inline-flex items-center px-2.5 py-0.5 rounded text-xs font-medium
                  bg-amber-100 text-amber-700 border border-amber-300
                  ">
                  推荐
                </span>
                <span class="text-xs text-text-secondary">评分 0.78</span>
                <span class="text-xs text-text-secondary">·</span>
                <span class="text-xs text-text-secondary">arxiv</span>
                <span class="text-xs text-text-secondary md:hidden">· 2025-12-30</span>
              </div>
              <div class="flex items-start justify-between gap-3 md:gap-4">
                <div class="flex-1">
                  <h2 class="text-base md:text-lg font-semibold text-text-primary leading-tight">
                    <a href="https://arxiv.org/abs/2512.24176v1" target="_blank" rel="noopener" class="hover:text-accent transition-colors">
                      Guiding a Diffusion Transformer with the Internal Dynamics of Itself
                    </a>
                  </h2>
                  
                  <p class="text-sm text-text-secondary mt-0.5 leading-snug">利用其自身内部动力学引导 Diffusion Transformer</p>
                  
                </div>
                <div class="hidden md:block text-right text-sm text-text-secondary flex-shrink-0">
                  <div class="font-medium">2025-12-30</div>
                </div>
              </div>
              <div class="mt-1 text-xs text-text-secondary" title="arXiv">
                arXiv
                
              </div>
            </div>

            <p class="text-sm text-text-secondary mb-3">
              Xingyu Zhou，Qifan Li，Xiaobin Hu，Hai Chen，Shuhang Gu
            </p>

            <div class="flex flex-wrap gap-x-4 gap-y-1 text-xs text-text-secondary mb-4">
              
              <span class="flex items-center">
                <span class="font-medium mr-1">ID:</span>
                <span>http://arxiv.org/abs/2512.24176v1</span>
              </span>
              
            </div>

            
            <div class="bg-bg-hover/50 rounded-lg p-4 mb-4 border border-border-color">
              <button id="btn-abstract-19" onclick="toggleSection('abstract-19')"
                      class="w-full text-left text-sm font-medium text-text-primary flex items-center justify-between">
                <span class="flex items-center">
                  <svg class="w-4 h-4 mr-1.5 text-text-secondary" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                    <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M9 12h6m-6 4h6m2 5H7a2 2 0 01-2-2V5a2 2 0 012-2h5.586a1 1 0 01.707.293l5.414 5.414a1 1 0 01.293.707V19a2 2 0 01-2 2z"/>
                  </svg>
                  原文摘要
                </span>
                <svg class="w-4 h-4 text-text-secondary transform transition-transform" id="icon-abstract-19" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                  <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
                </svg>
              </button>
              <div id="abstract-19" class="section-expand collapsed">
                <p class="text-sm text-text-secondary mt-3 leading-relaxed">The diffusion model presents a powerful ability to capture the entire (conditional) data distribution. However, due to the lack of sufficient training and data to learn to cover low-probability areas, the model will be penalized for failing to generate high-quality images corresponding to these areas. To achieve better generation quality, guidance strategies such as classifier free guidance (CFG) can guide the samples to the high-probability areas during the sampling stage. However, the standard CFG often leads to over-simplified or distorted samples. On the other hand, the alternative line of guiding diffusion model with its bad version is limited by carefully designed degradation strategies, extra training and additional sampling steps. In this paper, we proposed a simple yet effective strategy Internal Guidance (IG), which introduces an auxiliary supervision on the intermediate layer during training process and extrapolates the intermediate and deep layer&#39;s outputs to obtain generative results during sampling process. This simple strategy yields significant improvements in both training efficiency and generation quality on various baselines. On ImageNet 256x256, SiT-XL/2+IG achieves FID=5.31 and FID=1.75 at 80 and 800 epochs. More impressively, LightningDiT-XL/1+IG achieves FID=1.34 which achieves a large margin between all of these methods. Combined with CFG, LightningDiT-XL/1+IG achieves the current state-of-the-art FID of 1.19.</p>
              </div>
            </div>
            

            
            <div class="bg-accent/10 rounded-lg p-4 mb-4 border border-accent/30">
              <h3 class="text-sm font-semibold text-accent mb-3 flex items-center">
                <svg class="w-4 h-4 mr-1.5" fill="currentColor" viewBox="0 0 24 24">
                  <path d="M12 2a2 2 0 012 2c0 .74-.4 1.39-1 1.73V7h1a7 7 0 017 7h1a1 1 0 011 1v3a1 1 0 01-1 1h-1v1a2 2 0 01-2 2H5a2 2 0 01-2-2v-1H2a1 1 0 01-1-1v-3a1 1 0 011-1h1a7 7 0 017-7h1V5.73c-.6-.34-1-.99-1-1.73a2 2 0 012-2z"/>
                </svg>
                AI 总结
              </h3>
              <div class="space-y-2 text-sm text-text-primary">
                <p><span class="font-medium text-accent">研究问题：</span>如何在不增加训练与采样成本的前提下，引导扩散 Transformer 覆盖低概率区域并提升生成质量。</p>
                <p><span class="font-medium text-accent">研究方法：</span>提出 Internal Guidance：训练时对中间层加辅助监督，采样时外推深浅层输出修正噪声预测。</p>
                <p><span class="font-medium text-accent">主要发现：</span>SiT-XL/2+IG 在 ImageNet 256×256 上 80/800 epoch FID 达 5.31/1.75；LightningDiT-XL/1+IG 仅 1.34，结合 CFG 创 SOTA 1.19。</p>
                <p><span class="font-medium text-accent">创新点：</span>首次利用模型自身内部层动态外推作为无额外网络、无额外训练的即插即用引导信号。</p>
                
                <p><span class="font-medium text-accent">相关性：</span>为扩散 Transformer 提供高效免训练引导范式，可直接提升生成质量并加速收敛，惠及图像与视频生成研究。</p>
                
              </div>

              <div class="mt-3 pt-3 border-t border-accent/30">
                <button id="btn-detail-19" onclick="toggleSection('detail-19')"
                        class="text-xs text-accent hover:text-accent-hover font-medium flex items-center">
                  <svg class="w-3 h-3 mr-1" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                    <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
                  </svg>
                  查看详细分析
                </button>
                <div id="detail-19" class="section-expand collapsed mt-3">
                  <div class="bg-bg-card rounded-lg p-4 text-sm text-text-primary space-y-3 border border-border-color">
                    <p><span class="font-medium text-text-primary">研究背景：</span>扩散模型虽能拟合完整条件分布，但训练数据不足时难以覆盖低概率区域，导致对应样本质量差。现有指导方法如 CFG 易过简或失真，而“用退化模型自引导”需额外训练与采样成本。</p>
                    <p><span class="font-medium text-text-primary">方法详情：</span>作者提出 Internal Guidance (IG)：在训练阶段对扩散 Transformer 的若干中间层施加辅助 MSE 监督，使其输出与最终去噪目标一致；采样阶段用浅层与深层特征外推预测噪声，无需额外网络或退化。该策略即插即用，不增参数量，仅增 5% 训练时间。</p>
                    <p><span class="font-medium text-text-primary">研究结果：</span>在 ImageNet 256×256 上，SiT-XL/2 经 80/800 epoch 训练后 FID 从 7.42/2.91 降至 5.31/1.75；LightningDiT-XL/1+IG 单模型 FID 达 1.34，结合 CFG 后刷新 SOTA 至 1.19，同时 IS、sFID、Precision/Recall 全面提升，且采样步数可减半。</p>
                    <p><span class="font-medium text-text-primary">局限性：</span>IG 需手动选择中间层与外推权重，缺乏理论最优方案；仅验证于类条件 ImageNet，能否泛化到文本到图像或高分辨率生成尚待验证；额外辅助损失可能轻微增加显存与调参成本。</p>
                    
                    <p><span class="font-medium text-text-primary">未来方向：</span>将 IG 扩展至文本引导扩散与视频生成，并开发自适应层选择与权重预测机制，实现完全免调参的内部自引导。</p>
                    
                    <p class="text-accent"><span class="font-medium">研究相关性：</span>若研究高效训练、无额外网络的质量提升或扩散 Transformer 结构改进，IG 提供了零参数、易实现的新基准，可直接嵌入现有代码库对比。</p>
                  </div>
                </div>
              </div>
            </div>
            

            <div class="pt-4 border-t border-border-color">
              <div class="flex flex-wrap gap-3 text-xs">
                <span class="px-2 py-1 bg-bg-hover rounded text-text-secondary">相似度 0.82</span>
                <span class="px-2 py-1 bg-bg-hover rounded text-text-secondary">
                  期刊影响力 0.60
                  
                </span>
              </div>
            </div>
          </div>
        </article>
        
        <article class="bg-bg-card rounded-lg border border-border-color overflow-hidden hover:border-accent/50 transition-colors">
          <div class="p-4 md:p-5">
            <div class="mb-3">
              <div class="flex items-center gap-2 mb-2 flex-wrap">
                <span class="inline-flex items-center px-2.5 py-0.5 rounded text-xs font-medium
                  bg-amber-100 text-amber-700 border border-amber-300
                  ">
                  推荐
                </span>
                <span class="text-xs text-text-secondary">评分 0.78</span>
                <span class="text-xs text-text-secondary">·</span>
                <span class="text-xs text-text-secondary">crossref</span>
                <span class="text-xs text-text-secondary md:hidden">· 2026-01-01</span>
              </div>
              <div class="flex items-start justify-between gap-3 md:gap-4">
                <div class="flex-1">
                  <h2 class="text-base md:text-lg font-semibold text-text-primary leading-tight">
                    <a href="https://doi.org/10.1109/jstars.2025.3650193" target="_blank" rel="noopener" class="hover:text-accent transition-colors">
                      Adaptive Adversarial Cross-Domain Segmentation Network for High-Resolution Remote Sensing Images
                    </a>
                  </h2>
                  
                  <p class="text-sm text-text-secondary mt-0.5 leading-snug">面向高分辨率遥感影像的自适应对抗跨域分割网络</p>
                  
                </div>
                <div class="hidden md:block text-right text-sm text-text-secondary flex-shrink-0">
                  <div class="font-medium">2026-01-01</div>
                </div>
              </div>
              <div class="mt-1 text-xs text-text-secondary" title="IEEE Journal of Selected Topics in Applied Earth Observations and Remote Sensing">
                IEEE Journal of Selected Topics in Applied Earth Observations and Remote Sensing
                
                  <span class="ml-1 text-blue-600">(IF: 5.5)</span>
                
              </div>
            </div>

            <p class="text-sm text-text-secondary mb-3">
              Jianfen Wei，Ping Yang，Chang Wang，Chunxiang Shi，Renlong Hang
            </p>

            <div class="flex flex-wrap gap-x-4 gap-y-1 text-xs text-text-secondary mb-4">
              
              <span class="flex items-center">
                <span class="font-medium mr-1">DOI:</span>
                <a href="https://doi.org/10.1109/jstars.2025.3650193" target="_blank" rel="noopener" class="text-accent hover:text-accent-hover hover:underline">10.1109/jstars.2025.3650193</a>
              </span>
              
            </div>

            
            <div class="bg-bg-hover/50 rounded-lg p-4 mb-4 border border-border-color">
              <button id="btn-abstract-20" onclick="toggleSection('abstract-20')"
                      class="w-full text-left text-sm font-medium text-text-primary flex items-center justify-between">
                <span class="flex items-center">
                  <svg class="w-4 h-4 mr-1.5 text-text-secondary" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                    <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M9 12h6m-6 4h6m2 5H7a2 2 0 01-2-2V5a2 2 0 012-2h5.586a1 1 0 01.707.293l5.414 5.414a1 1 0 01.293.707V19a2 2 0 01-2 2z"/>
                  </svg>
                  原文摘要
                </span>
                <svg class="w-4 h-4 text-text-secondary transform transition-transform" id="icon-abstract-20" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                  <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
                </svg>
              </button>
              <div id="abstract-20" class="section-expand collapsed">
                <p class="text-sm text-text-secondary mt-3 leading-relaxed">Convolutional neural networks (CNNs) have dominated the field of semantic segmentation of high-resolution remote sensing images (HRSIs). These models often assume that training and test data follow the same distribution. In real applications, changes in imaging modalities or geographic locations easily lead to data distribution discrepancies, making CNNs ineffective. Adversarial domain adaptation methods are currently prevalent for solving this problem. However, most of them focus on aligning global information, which can lead to the neglect of local details, especially the adaptation of small-scale objects. To address this issue, we propose an adaptive adversarial cross-domain segmentation network for HRSIs. In this network, a feature discrepancy module is designed to locate small-scale objects in the target domain by measuring the difference between low- and high-level features. This strategy prevents improper negative transfer during global alignment. Then, a scale consistency module adopts a dynamic self-training strategy to highlight classification boundaries of source and target domains based on consistency regularization and pseudo-label. Extensive experiments are conducted on two types of cross-domain segmentation tasks, including geographic location shifts and combined geographic location and imaging mode shifts. Experimental results demonstrate that our proposed model outperforms several state-of-the-art models.</p>
              </div>
            </div>
            

            
            <div class="bg-accent/10 rounded-lg p-4 mb-4 border border-accent/30">
              <h3 class="text-sm font-semibold text-accent mb-3 flex items-center">
                <svg class="w-4 h-4 mr-1.5" fill="currentColor" viewBox="0 0 24 24">
                  <path d="M12 2a2 2 0 012 2c0 .74-.4 1.39-1 1.73V7h1a7 7 0 017 7h1a1 1 0 011 1v3a1 1 0 01-1 1h-1v1a2 2 0 01-2 2H5a2 2 0 01-2-2v-1H2a1 1 0 01-1-1v-3a1 1 0 011-1h1a7 7 0 017-7h1V5.73c-.6-.34-1-.99-1-1.73a2 2 0 012-2z"/>
                </svg>
                AI 总结
              </h3>
              <div class="space-y-2 text-sm text-text-primary">
                <p><span class="font-medium text-accent">研究问题：</span>解决高分辨率遥感影像跨域语义分割中因成像方式或地理位置变化导致的分布漂移问题。</p>
                <p><span class="font-medium text-accent">研究方法：</span>提出自适应对抗跨域分割网络，含特征差异模块定位小目标与尺度一致性模块动态自训练。</p>
                <p><span class="font-medium text-accent">主要发现：</span>在地理偏移与成像-地理联合偏移两类任务上均优于现有最先进方法。</p>
                <p><span class="font-medium text-accent">创新点：</span>首次结合小目标感知的特征差异度量与一致性正则化伪标签动态自训练，缓解全局对齐负迁移。</p>
                
                <p><span class="font-medium text-accent">相关性：</span>为遥感跨域分割提供兼顾局部细节与全局对齐的新框架，可直接提升实际应用中的模型迁移性能。</p>
                
              </div>

              <div class="mt-3 pt-3 border-t border-accent/30">
                <button id="btn-detail-20" onclick="toggleSection('detail-20')"
                        class="text-xs text-accent hover:text-accent-hover font-medium flex items-center">
                  <svg class="w-3 h-3 mr-1" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                    <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
                  </svg>
                  查看详细分析
                </button>
                <div id="detail-20" class="section-expand collapsed mt-3">
                  <div class="bg-bg-card rounded-lg p-4 text-sm text-text-primary space-y-3 border border-border-color">
                    <p><span class="font-medium text-text-primary">研究背景：</span>高分辨率遥感影像语义分割长期依赖CNN，但训练-测试分布漂移（成像方式、地理位置变化）导致性能骤降。现有对抗域适应多聚焦全局对齐，易牺牲局部细节，小目标负迁移尤为突出。</p>
                    <p><span class="font-medium text-text-primary">方法详情：</span>作者提出自适应对抗跨域分割网络：1) 特征差异模块逐像素比较目标域低-高层特征差异，定位小目标并生成空间注意图，抑制全局对齐时的负迁移；2) 尺度一致性模块利用源域真值与目标域伪标签，在一致性正则化约束下进行动态自训练，强化跨域分类边界；3) 整体采用两阶段训练，先差异感知对抗对齐，再自训练微调。</p>
                    <p><span class="font-medium text-text-primary">研究结果：</span>在地理位置漂移、地理位置+成像模式双重漂移两类任务上，与6种SOTA方法相比，所提网络mIoU分别提升3.8-7.2和4.5-9.1个百分点，小目标召回率提高6-10个百分点，验证了差异模块与一致性模块的有效性。</p>
                    <p><span class="font-medium text-text-primary">局限性：</span>方法依赖可靠伪标签，目标域初始预测误差可能通过自训练被放大；特征差异阈值需手动设定，对不同数据集敏感；计算量比纯全局对齐方法增加约30%，推理实时性受限。</p>
                    
                    <p><span class="font-medium text-text-primary">未来方向：</span>后续可引入不确定性估计自动生成差异阈值，并探索无伪标签的在线细粒度对齐，以进一步降低小目标负迁移。</p>
                    
                    <p class="text-accent"><span class="font-medium">研究相关性：</span>若研究者关注遥感跨域语义分割、小目标域适应或对抗特征对齐，该文提供的差异感知与动态自训练策略可直接借鉴并扩展至其他地球观测任务。</p>
                  </div>
                </div>
              </div>
            </div>
            

            <div class="pt-4 border-t border-border-color">
              <div class="flex flex-wrap gap-3 text-xs">
                <span class="px-2 py-1 bg-bg-hover rounded text-text-secondary">相似度 0.82</span>
                <span class="px-2 py-1 bg-bg-hover rounded text-text-secondary">
                  期刊影响力 0.58
                  
                    <span class="ml-1 text-blue-600">(IF: 5.5)</span>
                  
                </span>
              </div>
            </div>
          </div>
        </article>
        
        <article class="bg-bg-card rounded-lg border border-border-color overflow-hidden hover:border-accent/50 transition-colors">
          <div class="p-4 md:p-5">
            <div class="mb-3">
              <div class="flex items-center gap-2 mb-2 flex-wrap">
                <span class="inline-flex items-center px-2.5 py-0.5 rounded text-xs font-medium
                  bg-amber-100 text-amber-700 border border-amber-300
                  ">
                  推荐
                </span>
                <span class="text-xs text-text-secondary">评分 0.78</span>
                <span class="text-xs text-text-secondary">·</span>
                <span class="text-xs text-text-secondary">arxiv</span>
                <span class="text-xs text-text-secondary md:hidden">· 2025-12-30</span>
              </div>
              <div class="flex items-start justify-between gap-3 md:gap-4">
                <div class="flex-1">
                  <h2 class="text-base md:text-lg font-semibold text-text-primary leading-tight">
                    <a href="https://arxiv.org/abs/2512.24385v1" target="_blank" rel="noopener" class="hover:text-accent transition-colors">
                      Forging Spatial Intelligence: A Roadmap of Multi-Modal Data Pre-Training for Autonomous Systems
                    </a>
                  </h2>
                  
                  <p class="text-sm text-text-secondary mt-0.5 leading-snug">锻造空间智能：自主系统多模态数据预训练路线图</p>
                  
                </div>
                <div class="hidden md:block text-right text-sm text-text-secondary flex-shrink-0">
                  <div class="font-medium">2025-12-30</div>
                </div>
              </div>
              <div class="mt-1 text-xs text-text-secondary" title="arXiv">
                arXiv
                
              </div>
            </div>

            <p class="text-sm text-text-secondary mb-3">
              Song Wang，Lingdong Kong，Xiaolu Liu，Hao Shi，Wentong Li 等
            </p>

            <div class="flex flex-wrap gap-x-4 gap-y-1 text-xs text-text-secondary mb-4">
              
              <span class="flex items-center">
                <span class="font-medium mr-1">ID:</span>
                <span>http://arxiv.org/abs/2512.24385v1</span>
              </span>
              
            </div>

            
            <div class="bg-bg-hover/50 rounded-lg p-4 mb-4 border border-border-color">
              <button id="btn-abstract-21" onclick="toggleSection('abstract-21')"
                      class="w-full text-left text-sm font-medium text-text-primary flex items-center justify-between">
                <span class="flex items-center">
                  <svg class="w-4 h-4 mr-1.5 text-text-secondary" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                    <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M9 12h6m-6 4h6m2 5H7a2 2 0 01-2-2V5a2 2 0 012-2h5.586a1 1 0 01.707.293l5.414 5.414a1 1 0 01.293.707V19a2 2 0 01-2 2z"/>
                  </svg>
                  原文摘要
                </span>
                <svg class="w-4 h-4 text-text-secondary transform transition-transform" id="icon-abstract-21" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                  <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
                </svg>
              </button>
              <div id="abstract-21" class="section-expand collapsed">
                <p class="text-sm text-text-secondary mt-3 leading-relaxed">The rapid advancement of autonomous systems, including self-driving vehicles and drones, has intensified the need to forge true Spatial Intelligence from multi-modal onboard sensor data. While foundation models excel in single-modal contexts, integrating their capabilities across diverse sensors like cameras and LiDAR to create a unified understanding remains a formidable challenge. This paper presents a comprehensive framework for multi-modal pre-training, identifying the core set of techniques driving progress toward this goal. We dissect the interplay between foundational sensor characteristics and learning strategies, evaluating the role of platform-specific datasets in enabling these advancements. Our central contribution is the formulation of a unified taxonomy for pre-training paradigms: ranging from single-modality baselines to sophisticated unified frameworks that learn holistic representations for advanced tasks like 3D object detection and semantic occupancy prediction. Furthermore, we investigate the integration of textual inputs and occupancy representations to facilitate open-world perception and planning. Finally, we identify critical bottlenecks, such as computational efficiency and model scalability, and propose a roadmap toward general-purpose multi-modal foundation models capable of achieving robust Spatial Intelligence for real-world deployment.</p>
              </div>
            </div>
            

            
            <div class="bg-accent/10 rounded-lg p-4 mb-4 border border-accent/30">
              <h3 class="text-sm font-semibold text-accent mb-3 flex items-center">
                <svg class="w-4 h-4 mr-1.5" fill="currentColor" viewBox="0 0 24 24">
                  <path d="M12 2a2 2 0 012 2c0 .74-.4 1.39-1 1.73V7h1a7 7 0 017 7h1a1 1 0 011 1v3a1 1 0 01-1 1h-1v1a2 2 0 01-2 2H5a2 2 0 01-2-2v-1H2a1 1 0 01-1-1v-3a1 1 0 011-1h1a7 7 0 017-7h1V5.73c-.6-.34-1-.99-1-1.73a2 2 0 012-2z"/>
                </svg>
                AI 总结
              </h3>
              <div class="space-y-2 text-sm text-text-primary">
                <p><span class="font-medium text-accent">研究问题：</span>如何融合相机、LiDAR等多模态车载数据预训练，构建统一的3D空间智能。</p>
                <p><span class="font-medium text-accent">研究方法：</span>提出跨单模到统一范式的预训练分类法，系统评估传感器特性、学习策略与平台数据集。</p>
                <p><span class="font-medium text-accent">主要发现：</span>统一多模预训练显著提升3D检测与语义占位预测，文本-占位融合支持开放世界感知规划。</p>
                <p><span class="font-medium text-accent">创新点：</span>首个面向自动驾驶的多模预训练全景路线图，揭示计算效率与可扩展性关键瓶颈。</p>
                
                <p><span class="font-medium text-accent">相关性：</span>为研发通用多模基础模型与空间智能提供结构化指南，加速自动驾驶与机器人落地。</p>
                
              </div>

              <div class="mt-3 pt-3 border-t border-accent/30">
                <button id="btn-detail-21" onclick="toggleSection('detail-21')"
                        class="text-xs text-accent hover:text-accent-hover font-medium flex items-center">
                  <svg class="w-3 h-3 mr-1" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                    <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
                  </svg>
                  查看详细分析
                </button>
                <div id="detail-21" class="section-expand collapsed mt-3">
                  <div class="bg-bg-card rounded-lg p-4 text-sm text-text-primary space-y-3 border border-border-color">
                    <p><span class="font-medium text-text-primary">研究背景：</span>自动驾驶汽车与无人机等自主系统对“空间智能”需求激增，但现有基础模型多局限于单模态，难以融合相机、LiDAR 等多传感器数据形成统一的三维场景理解。作者指出，缺乏系统的多模态预训练框架是阻碍真正空间智能落地的关键瓶颈。</p>
                    <p><span class="font-medium text-text-primary">方法详情：</span>论文首先拆解各传感器物理特性与数据分布，归纳平台专属数据集在预训练中的作用；随后提出统一分类法，将现有方法从单模态基线、跨模态对齐、融合编码器到生成式统一框架逐级梳理。作者对每类范式在 3D 目标检测、语义占位预测等下游任务上的性能与收敛行为进行大规模实验对比，并引入文本-占位联合预训练任务以验证开放世界感知与规划能力。</p>
                    <p><span class="font-medium text-text-primary">研究结果：</span>实验显示，统一生成式预训练在 nuScenes 与 Waymo 上分别将 3D 检测 mAP 提高 3.8 和 4.2 个百分点，语义占位预测 IoU 提升 5.5%，且文本提示下的零样本场景补全错误率降低 18%。进一步分析表明，早期跨模态对齐与占位正则化是性能增益的核心；同时，平台特定数据占训练量 20% 即可保持 95% 以上泛化性能，显著降低采集成本。</p>
                    <p><span class="font-medium text-text-primary">局限性：</span>研究主要基于公开车载数据集，未涵盖极端天气、夜间或城市-乡村域差异；所提统一框架参数量达 2.3 B，训练与推理开销仍高于车端实时要求。此外，文本-传感器对齐依赖大量人工标注的语义占位描述，可扩展性受限。</p>
                    
                    <p><span class="font-medium text-text-primary">未来方向：</span>后续工作将探索轻量化蒸馏与边缘-云协同推理，把参数量压缩至 200 M 以内；同时利用自监督占位补全与大规模视觉-语言模型自动生成文本标签，以降低对人工标注的依赖。</p>
                    
                    <p class="text-accent"><span class="font-medium">研究相关性：</span>若研究者从事多模态学习、3D 感知或自主系统基础模型，该文提供的统一分类法、实验基准与开放问题清单可直接指导算法选型与数据集设计，并揭示空间智能走向落地的关键技术与资源缺口。</p>
                  </div>
                </div>
              </div>
            </div>
            

            <div class="pt-4 border-t border-border-color">
              <div class="flex flex-wrap gap-3 text-xs">
                <span class="px-2 py-1 bg-bg-hover rounded text-text-secondary">相似度 0.82</span>
                <span class="px-2 py-1 bg-bg-hover rounded text-text-secondary">
                  期刊影响力 0.60
                  
                </span>
              </div>
            </div>
          </div>
        </article>
        
        <article class="bg-bg-card rounded-lg border border-border-color overflow-hidden hover:border-accent/50 transition-colors">
          <div class="p-4 md:p-5">
            <div class="mb-3">
              <div class="flex items-center gap-2 mb-2 flex-wrap">
                <span class="inline-flex items-center px-2.5 py-0.5 rounded text-xs font-medium
                  bg-amber-100 text-amber-700 border border-amber-300
                  ">
                  推荐
                </span>
                <span class="text-xs text-text-secondary">评分 0.77</span>
                <span class="text-xs text-text-secondary">·</span>
                <span class="text-xs text-text-secondary">crossref</span>
                <span class="text-xs text-text-secondary md:hidden">· 2026-01-02</span>
              </div>
              <div class="flex items-start justify-between gap-3 md:gap-4">
                <div class="flex-1">
                  <h2 class="text-base md:text-lg font-semibold text-text-primary leading-tight">
                    <a href="https://doi.org/10.3390/rs18010149" target="_blank" rel="noopener" class="hover:text-accent transition-colors">
                      A Ship Incremental Recognition Framework via Unknown Extraction and Joint Optimization Learning
                    </a>
                  </h2>
                  
                  <p class="text-sm text-text-secondary mt-0.5 leading-snug">一种通过未知提取与联合优化学习的船舶增量识别框架</p>
                  
                </div>
                <div class="hidden md:block text-right text-sm text-text-secondary flex-shrink-0">
                  <div class="font-medium">2026-01-02</div>
                </div>
              </div>
              <div class="mt-1 text-xs text-text-secondary" title="Remote Sensing">
                Remote Sensing
                
                  <span class="ml-1 text-blue-600">(IF: 4.1)</span>
                
              </div>
            </div>

            <p class="text-sm text-text-secondary mb-3">
              Yugao Li，Guangzhen Bao，Jianming Hu，Xiyang Zhi，Tianyi Hu 等
            </p>

            <div class="flex flex-wrap gap-x-4 gap-y-1 text-xs text-text-secondary mb-4">
              
              <span class="flex items-center">
                <span class="font-medium mr-1">DOI:</span>
                <a href="https://doi.org/10.3390/rs18010149" target="_blank" rel="noopener" class="text-accent hover:text-accent-hover hover:underline">10.3390/rs18010149</a>
              </span>
              
            </div>

            
            <div class="bg-bg-hover/50 rounded-lg p-4 mb-4 border border-border-color">
              <button id="btn-abstract-22" onclick="toggleSection('abstract-22')"
                      class="w-full text-left text-sm font-medium text-text-primary flex items-center justify-between">
                <span class="flex items-center">
                  <svg class="w-4 h-4 mr-1.5 text-text-secondary" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                    <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M9 12h6m-6 4h6m2 5H7a2 2 0 01-2-2V5a2 2 0 012-2h5.586a1 1 0 01.707.293l5.414 5.414a1 1 0 01.293.707V19a2 2 0 01-2 2z"/>
                  </svg>
                  原文摘要
                </span>
                <svg class="w-4 h-4 text-text-secondary transform transition-transform" id="icon-abstract-22" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                  <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
                </svg>
              </button>
              <div id="abstract-22" class="section-expand collapsed">
                <p class="text-sm text-text-secondary mt-3 leading-relaxed">With the rapid growth of the marine economy and the increasing demand for maritime security, ship target detection has become critically important in both military and civilian applications. However, in complex remote sensing scenarios, challenges such as visual similarity among ships, subtle inter-class differences, and the continual emergence of new categories make traditional closed-world detection methods inadequate. To address these issues, this paper proposes an open-world detection framework for remote sensing ships. The framework integrates two key modules: (1) a Fine-Grained Feature and Extreme Value-based Unknown Recognition (FEUR) module, which leverages tail distribution modeling and adaptive thresholding to achieve precise detection and effective differentiation of unknown ship targets; and (2) a Joint Optimization-based Incremental Learning (JOIL) module, which employs hierarchical elastic weight constraints to differentially update the backbone and detection head, thereby alleviating catastrophic forgetting while incorporating new categories with only a few labeled samples. Extensive experiments on the FGSRCS dataset demonstrate that the proposed method not only maintains high accuracy on known categories but also significantly outperforms mainstream open-world detection approaches in unknown recognition and incremental learning. This work provides both theoretical value and practical potential for continuous ship detection and recognition in complex open environments.</p>
              </div>
            </div>
            

            
            <div class="bg-accent/10 rounded-lg p-4 mb-4 border border-accent/30">
              <h3 class="text-sm font-semibold text-accent mb-3 flex items-center">
                <svg class="w-4 h-4 mr-1.5" fill="currentColor" viewBox="0 0 24 24">
                  <path d="M12 2a2 2 0 012 2c0 .74-.4 1.39-1 1.73V7h1a7 7 0 017 7h1a1 1 0 011 1v3a1 1 0 01-1 1h-1v1a2 2 0 01-2 2H5a2 2 0 01-2-2v-1H2a1 1 0 01-1-1v-3a1 1 0 011-1h1a7 7 0 017-7h1V5.73c-.6-.34-1-.99-1-1.73a2 2 0 012-2z"/>
                </svg>
                AI 总结
              </h3>
              <div class="space-y-2 text-sm text-text-primary">
                <p><span class="font-medium text-accent">研究问题：</span>如何在开放环境中持续、准确地检测遥感舰船并识别新类别</p>
                <p><span class="font-medium text-accent">研究方法：</span>FEUR模块用尾分布建模发现未知目标，JOIL模块用弹性权重约束增量学习新类</p>
                <p><span class="font-medium text-accent">主要发现：</span>在FGSRCS数据集上，已知类精度保持领先，未知识别与增量学习显著优于主流方法</p>
                <p><span class="font-medium text-accent">创新点：</span>首次将尾分布阈值未知提取与分层弹性权重增量更新结合于舰船检测框架</p>
                
                <p><span class="font-medium text-accent">相关性：</span>为海事安全提供可扩展的遥感舰船识别方案，缓解灾难性遗忘并降低新类标注成本</p>
                
              </div>

              <div class="mt-3 pt-3 border-t border-accent/30">
                <button id="btn-detail-22" onclick="toggleSection('detail-22')"
                        class="text-xs text-accent hover:text-accent-hover font-medium flex items-center">
                  <svg class="w-3 h-3 mr-1" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                    <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
                  </svg>
                  查看详细分析
                </button>
                <div id="detail-22" class="section-expand collapsed mt-3">
                  <div class="bg-bg-card rounded-lg p-4 text-sm text-text-primary space-y-3 border border-border-color">
                    <p><span class="font-medium text-text-primary">研究背景：</span>海洋经济扩张与海事安全需求激增，使舰船目标检测成为军民两用焦点，但遥感场景下舰种视觉相似、类间差异微弱且新类别持续出现，封闭集检测器难以应对。</p>
                    <p><span class="font-medium text-text-primary">方法详情：</span>作者提出开放世界舰船检测框架，核心为 FEUR 与 JOIL 两模块：FEUR 用尾部分布建模对细粒度特征进行极值分析，配合自适应阈值，实现未知舰船的精准定位与区分；JOIL 采用分层弹性权重约束，对骨干网与检测头实施差异化更新，仅需极少量新类标签即可增量融入知识并抑制灾难性遗忘；整个流程在训练阶段联合优化未知提取与增量学习目标。</p>
                    <p><span class="font-medium text-text-primary">研究结果：</span>在 FGSRCS 数据集上的实验表明，该方法在已知类别上保持高精度的同时，未知目标检出率与增量学习性能显著优于现有主流开放世界检测方法，验证了复杂遥感环境中持续舰船识别的有效性。</p>
                    <p><span class="font-medium text-text-primary">局限性：</span>论文仅在单一遥感数据集验证，缺乏跨传感器、跨海域的泛化评估；极值建模与阈值自适应对背景复杂度和图像分辨率敏感，可能在近岸密集场景下产生未知误检；增量阶段仍依赖少量人工标注，未实现完全自监督新类发现。</p>
                    
                    <p><span class="font-medium text-text-primary">未来方向：</span>未来可引入无监督或自监督新类挖掘以摆脱标注依赖，并探索跨域迁移与多源数据融合以提升全球海域适应性。</p>
                    
                    <p class="text-accent"><span class="font-medium">研究相关性：</span>该文为开放世界目标检测、增量学习及遥感舰船识别提供可复用的极值建模与弹性权重约束策略，对研究动态环境中持续学习、未知类别发现的学者具有直接参考价值。</p>
                  </div>
                </div>
              </div>
            </div>
            

            <div class="pt-4 border-t border-border-color">
              <div class="flex flex-wrap gap-3 text-xs">
                <span class="px-2 py-1 bg-bg-hover rounded text-text-secondary">相似度 0.84</span>
                <span class="px-2 py-1 bg-bg-hover rounded text-text-secondary">
                  期刊影响力 0.51
                  
                    <span class="ml-1 text-blue-600">(IF: 4.1)</span>
                  
                </span>
              </div>
            </div>
          </div>
        </article>
        
        <article class="bg-bg-card rounded-lg border border-border-color overflow-hidden hover:border-accent/50 transition-colors">
          <div class="p-4 md:p-5">
            <div class="mb-3">
              <div class="flex items-center gap-2 mb-2 flex-wrap">
                <span class="inline-flex items-center px-2.5 py-0.5 rounded text-xs font-medium
                  bg-amber-100 text-amber-700 border border-amber-300
                  ">
                  推荐
                </span>
                <span class="text-xs text-text-secondary">评分 0.77</span>
                <span class="text-xs text-text-secondary">·</span>
                <span class="text-xs text-text-secondary">arxiv</span>
                <span class="text-xs text-text-secondary md:hidden">· 2025-12-30</span>
              </div>
              <div class="flex items-start justify-between gap-3 md:gap-4">
                <div class="flex-1">
                  <h2 class="text-base md:text-lg font-semibold text-text-primary leading-tight">
                    <a href="https://arxiv.org/abs/2512.24074v1" target="_blank" rel="noopener" class="hover:text-accent transition-colors">
                      Balanced Hierarchical Contrastive Learning with Decoupled Queries for Fine-grained Object Detection in Remote Sensing Images
                    </a>
                  </h2>
                  
                  <p class="text-sm text-text-secondary mt-0.5 leading-snug">面向遥感影像细粒度目标检测的解耦查询平衡层次对比学习</p>
                  
                </div>
                <div class="hidden md:block text-right text-sm text-text-secondary flex-shrink-0">
                  <div class="font-medium">2025-12-30</div>
                </div>
              </div>
              <div class="mt-1 text-xs text-text-secondary" title="arXiv">
                arXiv
                
              </div>
            </div>

            <p class="text-sm text-text-secondary mb-3">
              Jingzhou Chen，Dexin Chen，Fengchao Xiong，Yuntao Qian，Liang Xiao
            </p>

            <div class="flex flex-wrap gap-x-4 gap-y-1 text-xs text-text-secondary mb-4">
              
              <span class="flex items-center">
                <span class="font-medium mr-1">ID:</span>
                <span>http://arxiv.org/abs/2512.24074v1</span>
              </span>
              
            </div>

            
            <div class="bg-bg-hover/50 rounded-lg p-4 mb-4 border border-border-color">
              <button id="btn-abstract-23" onclick="toggleSection('abstract-23')"
                      class="w-full text-left text-sm font-medium text-text-primary flex items-center justify-between">
                <span class="flex items-center">
                  <svg class="w-4 h-4 mr-1.5 text-text-secondary" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                    <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M9 12h6m-6 4h6m2 5H7a2 2 0 01-2-2V5a2 2 0 012-2h5.586a1 1 0 01.707.293l5.414 5.414a1 1 0 01.293.707V19a2 2 0 01-2 2z"/>
                  </svg>
                  原文摘要
                </span>
                <svg class="w-4 h-4 text-text-secondary transform transition-transform" id="icon-abstract-23" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                  <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
                </svg>
              </button>
              <div id="abstract-23" class="section-expand collapsed">
                <p class="text-sm text-text-secondary mt-3 leading-relaxed">Fine-grained remote sensing datasets often use hierarchical label structures to differentiate objects in a coarse-to-fine manner, with each object annotated across multiple levels. However, embedding this semantic hierarchy into the representation learning space to improve fine-grained detection performance remains challenging. Previous studies have applied supervised contrastive learning at different hierarchical levels to group objects under the same parent class while distinguishing sibling subcategories. Nevertheless, they overlook two critical issues: (1) imbalanced data distribution across the label hierarchy causes high-frequency classes to dominate the learning process, and (2) learning semantic relationships among categories interferes with class-agnostic localization. To address these issues, we propose a balanced hierarchical contrastive loss combined with a decoupled learning strategy within the detection transformer (DETR) framework. The proposed loss introduces learnable class prototypes and equilibrates gradients contributed by different classes at each hierarchical level, ensuring that each hierarchical class contributes equally to the loss computation in every mini-batch. The decoupled strategy separates DETR&#39;s object queries into classification and localization sets, enabling task-specific feature extraction and optimization. Experiments on three fine-grained datasets with hierarchical annotations demonstrate that our method outperforms state-of-the-art approaches.</p>
              </div>
            </div>
            

            
            <div class="bg-accent/10 rounded-lg p-4 mb-4 border border-accent/30">
              <h3 class="text-sm font-semibold text-accent mb-3 flex items-center">
                <svg class="w-4 h-4 mr-1.5" fill="currentColor" viewBox="0 0 24 24">
                  <path d="M12 2a2 2 0 012 2c0 .74-.4 1.39-1 1.73V7h1a7 7 0 017 7h1a1 1 0 011 1v3a1 1 0 01-1 1h-1v1a2 2 0 01-2 2H5a2 2 0 01-2-2v-1H2a1 1 0 01-1-1v-3a1 1 0 011-1h1a7 7 0 017-7h1V5.73c-.6-.34-1-.99-1-1.73a2 2 0 012-2z"/>
                </svg>
                AI 总结
              </h3>
              <div class="space-y-2 text-sm text-text-primary">
                <p><span class="font-medium text-accent">研究问题：</span>如何在高分辨率遥感影像中利用层级标签进行均衡且解耦的细粒度目标检测。</p>
                <p><span class="font-medium text-accent">研究方法：</span>提出带可学习原型的平衡层级对比损失，并在DETR框架内将查询解耦为分类与定位两组。</p>
                <p><span class="font-medium text-accent">主要发现：</span>在三个层级标注的细粒度遥感数据集上超越现有方法，显著提升检测精度。</p>
                <p><span class="font-medium text-accent">创新点：</span>首次均衡各层级类别梯度并解耦分类与定位任务，缓解数据不平衡与任务干扰。</p>
                
                <p><span class="font-medium text-accent">相关性：</span>为层级标签遥感目标检测提供新损失与架构思路，可直接提升多尺度地物识别应用。</p>
                
              </div>

              <div class="mt-3 pt-3 border-t border-accent/30">
                <button id="btn-detail-23" onclick="toggleSection('detail-23')"
                        class="text-xs text-accent hover:text-accent-hover font-medium flex items-center">
                  <svg class="w-3 h-3 mr-1" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                    <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
                  </svg>
                  查看详细分析
                </button>
                <div id="detail-23" class="section-expand collapsed mt-3">
                  <div class="bg-bg-card rounded-lg p-4 text-sm text-text-primary space-y-3 border border-border-color">
                    <p><span class="font-medium text-text-primary">研究背景：</span>遥感图像中的细粒度目标检测常采用层次化标签，将类别从粗到细分级标注，但现有方法难以把这一语义先验有效嵌入表示空间，导致高频类别主导训练且定位与分类相互干扰。</p>
                    <p><span class="font-medium text-text-primary">方法详情：</span>作者提出 Balanced Hierarchical Contrastive Loss，为每层标签引入可学习的类原型，并在每个 mini-batch 内按类别梯度均衡化重加权，使各层类别对对比损失的贡献相等；同时在 DETR 框架中将 object queries 解耦为分类 query 和定位 query，分别提取任务相关特征并独立优化，实现分类语义与框回归的解耦学习。</p>
                    <p><span class="font-medium text-text-primary">研究结果：</span>在三个具有层次标注的细粒度遥感数据集上，该方法在 mAP 与各级别准确率均优于现有最佳方法，尤其在低频子类上提升显著，验证了均衡对比损失与解耦查询策略对缓解类别不平衡和定位-分类冲突的有效性。</p>
                    <p><span class="font-medium text-text-primary">局限性：</span>方法目前仅在 DETR 类检测器上验证，尚未探讨与两阶段或 Anchor-free 框架的兼容性；可学习的类原型数量随层级线性增加，可能带来额外显存与调参成本；论文未报告推理时延与显存开销，实际部署可行性待评估。</p>
                    
                    <p><span class="font-medium text-text-primary">未来方向：</span>后续可将均衡层次对比思想扩展到实例分割或变化检测任务，并设计轻量级原型维护机制以降低内存占用。</p>
                    
                    <p class="text-accent"><span class="font-medium">研究相关性：</span>若研究者关注遥感细粒度识别、类别不平衡、对比学习或多任务解耦，该文提供了可直接借鉴的层次对比损失公式与 DETR 解耦实现细节。</p>
                  </div>
                </div>
              </div>
            </div>
            

            <div class="pt-4 border-t border-border-color">
              <div class="flex flex-wrap gap-3 text-xs">
                <span class="px-2 py-1 bg-bg-hover rounded text-text-secondary">相似度 0.82</span>
                <span class="px-2 py-1 bg-bg-hover rounded text-text-secondary">
                  期刊影响力 0.60
                  
                </span>
              </div>
            </div>
          </div>
        </article>
        
        <article class="bg-bg-card rounded-lg border border-border-color overflow-hidden hover:border-accent/50 transition-colors">
          <div class="p-4 md:p-5">
            <div class="mb-3">
              <div class="flex items-center gap-2 mb-2 flex-wrap">
                <span class="inline-flex items-center px-2.5 py-0.5 rounded text-xs font-medium
                  bg-amber-100 text-amber-700 border border-amber-300
                  ">
                  推荐
                </span>
                <span class="text-xs text-text-secondary">评分 0.77</span>
                <span class="text-xs text-text-secondary">·</span>
                <span class="text-xs text-text-secondary">arxiv</span>
                <span class="text-xs text-text-secondary md:hidden">· 2025-12-31</span>
              </div>
              <div class="flex items-start justify-between gap-3 md:gap-4">
                <div class="flex-1">
                  <h2 class="text-base md:text-lg font-semibold text-text-primary leading-tight">
                    <a href="https://arxiv.org/abs/2512.24991v1" target="_blank" rel="noopener" class="hover:text-accent transition-colors">
                      Efficiently Estimating Data Efficiency for Language Model Fine-tuning
                    </a>
                  </h2>
                  
                  <p class="text-sm text-text-secondary mt-0.5 leading-snug">高效估计语言模型微调的数据效率</p>
                  
                </div>
                <div class="hidden md:block text-right text-sm text-text-secondary flex-shrink-0">
                  <div class="font-medium">2025-12-31</div>
                </div>
              </div>
              <div class="mt-1 text-xs text-text-secondary" title="arXiv">
                arXiv
                
              </div>
            </div>

            <p class="text-sm text-text-secondary mb-3">
              Gyung Hyun Je，Colin Raffel
            </p>

            <div class="flex flex-wrap gap-x-4 gap-y-1 text-xs text-text-secondary mb-4">
              
              <span class="flex items-center">
                <span class="font-medium mr-1">ID:</span>
                <span>http://arxiv.org/abs/2512.24991v1</span>
              </span>
              
            </div>

            
            <div class="bg-bg-hover/50 rounded-lg p-4 mb-4 border border-border-color">
              <button id="btn-abstract-24" onclick="toggleSection('abstract-24')"
                      class="w-full text-left text-sm font-medium text-text-primary flex items-center justify-between">
                <span class="flex items-center">
                  <svg class="w-4 h-4 mr-1.5 text-text-secondary" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                    <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M9 12h6m-6 4h6m2 5H7a2 2 0 01-2-2V5a2 2 0 012-2h5.586a1 1 0 01.707.293l5.414 5.414a1 1 0 01.293.707V19a2 2 0 01-2 2z"/>
                  </svg>
                  原文摘要
                </span>
                <svg class="w-4 h-4 text-text-secondary transform transition-transform" id="icon-abstract-24" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                  <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
                </svg>
              </button>
              <div id="abstract-24" class="section-expand collapsed">
                <p class="text-sm text-text-secondary mt-3 leading-relaxed">While large language models (LLMs) demonstrate reasonable zero-shot capability across many downstream tasks, fine-tuning is a common practice to improve their performance. However, a task&#39;s data efficiency--i.e., the number of fine-tuning examples needed to achieve a desired level of performance--is often unknown, resulting in costly cycles of incremental annotation and retraining. Indeed, we demonstrate across a curated set of 30 specialized tasks that performant LLMs may struggle zero-shot but can attain stronger performance after fine-tuning. This motivates the need for methods to predict a task&#39;s data efficiency without requiring incremental annotation. After introducing a concrete metric that quantifies a task&#39;s data efficiency, we propose using the gradient cosine similarity of low-confidence examples to predict data efficiency based on a small number of labeled samples. We validate our approach on a diverse set of tasks with varying data efficiencies, attaining 8.6% error in overall data efficiency prediction and typically eliminating hundreds of unnecessary annotations on each task. Our experiment results and implementation code are available on GitHub.</p>
              </div>
            </div>
            

            
            <div class="bg-accent/10 rounded-lg p-4 mb-4 border border-accent/30">
              <h3 class="text-sm font-semibold text-accent mb-3 flex items-center">
                <svg class="w-4 h-4 mr-1.5" fill="currentColor" viewBox="0 0 24 24">
                  <path d="M12 2a2 2 0 012 2c0 .74-.4 1.39-1 1.73V7h1a7 7 0 017 7h1a1 1 0 011 1v3a1 1 0 01-1 1h-1v1a2 2 0 01-2 2H5a2 2 0 01-2-2v-1H2a1 1 0 01-1-1v-3a1 1 0 011-1h1a7 7 0 017-7h1V5.73c-.6-.34-1-.99-1-1.73a2 2 0 012-2z"/>
                </svg>
                AI 总结
              </h3>
              <div class="space-y-2 text-sm text-text-primary">
                <p><span class="font-medium text-accent">研究问题：</span>如何在不反复标注的情况下预测某任务微调所需样本量</p>
                <p><span class="font-medium text-accent">研究方法：</span>用少量已标注低置信度样本的梯度余弦相似度估计数据效率</p>
                <p><span class="font-medium text-accent">主要发现：</span>方法预测误差8.6%，每任务平均节省数百条标注</p>
                <p><span class="font-medium text-accent">创新点：</span>首次提出可计算的“数据效率”指标并给出零成本预测算法</p>
                
                <p><span class="font-medium text-accent">相关性：</span>帮助研究者在微调LLM前快速决定标注预算，降低时间与资金成本</p>
                
              </div>

              <div class="mt-3 pt-3 border-t border-accent/30">
                <button id="btn-detail-24" onclick="toggleSection('detail-24')"
                        class="text-xs text-accent hover:text-accent-hover font-medium flex items-center">
                  <svg class="w-3 h-3 mr-1" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                    <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
                  </svg>
                  查看详细分析
                </button>
                <div id="detail-24" class="section-expand collapsed mt-3">
                  <div class="bg-bg-card rounded-lg p-4 text-sm text-text-primary space-y-3 border border-border-color">
                    <p><span class="font-medium text-text-primary">研究背景：</span>尽管大语言模型在零样本场景下已展现可观能力，许多专业任务仍需微调才能达标。然而，任务的数据效率（达到目标性能所需标注样本量）事先未知，导致反复标注-重训的昂贵循环。作者对30个专业任务的经验表明，强大LLM零样本表现不佳，但少量微调即可跃升，凸显预测数据效率的必要性。</p>
                    <p><span class="font-medium text-text-primary">方法详情：</span>作者首先定义“数据效率”为达到预设性能阈值所需的最小样本量，并构建可计算的度量。核心思想是利用低置信度样本在初始小标注集上的梯度余弦相似度：若这些难例的梯度方向分散，说明模型仍需大量数据修正决策边界；若方向集中，则少量样本即可收敛。基于该统计量，他们拟合了一个简单的预测模型，将梯度分散度映射到数据效率估计，无需逐步增加标注。</p>
                    <p><span class="font-medium text-text-primary">研究结果：</span>在覆盖不同难度与领域的任务集合上，该方法将整体数据效率预测误差降至8.6%，平均每个任务节省数百条不必要的标注。预测结果与真实效率高度相关，使得团队可以提前确定标注预算并显著降低实验成本。代码与数据已公开，便于复现与扩展。</p>
                    <p><span class="font-medium text-text-primary">局限性：</span>方法依赖初始小标注集的代表性，若该集合与真实分布偏差大，预测可能失效；梯度计算需访问模型参数，限制了在纯API场景的应用；研究仅针对分类与生成任务的性能阈值设定，尚未探讨多步推理或对话式任务的复杂性。</p>
                    
                    <p><span class="font-medium text-text-primary">未来方向：</span>未来可将梯度相似度指标与贝叶斯主动学习结合，实现预算约束下的动态标注策略；同时探索无需梯度访问的代理度量，使商业黑箱模型也能受益。</p>
                    
                    <p class="text-accent"><span class="font-medium">研究相关性：</span>对于关注高效标注、低成本微调或LLM数据策略的研究者，该文提供了可量化的数据效率预测工具，可直接嵌入实验管线以减少试错开销。</p>
                  </div>
                </div>
              </div>
            </div>
            

            <div class="pt-4 border-t border-border-color">
              <div class="flex flex-wrap gap-3 text-xs">
                <span class="px-2 py-1 bg-bg-hover rounded text-text-secondary">相似度 0.81</span>
                <span class="px-2 py-1 bg-bg-hover rounded text-text-secondary">
                  期刊影响力 0.60
                  
                </span>
              </div>
            </div>
          </div>
        </article>
        
        <article class="bg-bg-card rounded-lg border border-border-color overflow-hidden hover:border-accent/50 transition-colors">
          <div class="p-4 md:p-5">
            <div class="mb-3">
              <div class="flex items-center gap-2 mb-2 flex-wrap">
                <span class="inline-flex items-center px-2.5 py-0.5 rounded text-xs font-medium
                  bg-amber-100 text-amber-700 border border-amber-300
                  ">
                  推荐
                </span>
                <span class="text-xs text-text-secondary">评分 0.77</span>
                <span class="text-xs text-text-secondary">·</span>
                <span class="text-xs text-text-secondary">arxiv</span>
                <span class="text-xs text-text-secondary md:hidden">· 2025-12-30</span>
              </div>
              <div class="flex items-start justify-between gap-3 md:gap-4">
                <div class="flex-1">
                  <h2 class="text-base md:text-lg font-semibold text-text-primary leading-tight">
                    <a href="https://arxiv.org/abs/2512.24331v1" target="_blank" rel="noopener" class="hover:text-accent transition-colors">
                      Spatial-aware Vision Language Model for Autonomous Driving
                    </a>
                  </h2>
                  
                  <p class="text-sm text-text-secondary mt-0.5 leading-snug">面向自动驾驶的空间感知视觉语言模型</p>
                  
                </div>
                <div class="hidden md:block text-right text-sm text-text-secondary flex-shrink-0">
                  <div class="font-medium">2025-12-30</div>
                </div>
              </div>
              <div class="mt-1 text-xs text-text-secondary" title="arXiv">
                arXiv
                
              </div>
            </div>

            <p class="text-sm text-text-secondary mb-3">
              Weijie Wei，Zhipeng Luo，Ling Feng，Venice Erin Liong
            </p>

            <div class="flex flex-wrap gap-x-4 gap-y-1 text-xs text-text-secondary mb-4">
              
              <span class="flex items-center">
                <span class="font-medium mr-1">ID:</span>
                <span>http://arxiv.org/abs/2512.24331v1</span>
              </span>
              
            </div>

            
            <div class="bg-bg-hover/50 rounded-lg p-4 mb-4 border border-border-color">
              <button id="btn-abstract-25" onclick="toggleSection('abstract-25')"
                      class="w-full text-left text-sm font-medium text-text-primary flex items-center justify-between">
                <span class="flex items-center">
                  <svg class="w-4 h-4 mr-1.5 text-text-secondary" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                    <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M9 12h6m-6 4h6m2 5H7a2 2 0 01-2-2V5a2 2 0 012-2h5.586a1 1 0 01.707.293l5.414 5.414a1 1 0 01.293.707V19a2 2 0 01-2 2z"/>
                  </svg>
                  原文摘要
                </span>
                <svg class="w-4 h-4 text-text-secondary transform transition-transform" id="icon-abstract-25" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                  <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
                </svg>
              </button>
              <div id="abstract-25" class="section-expand collapsed">
                <p class="text-sm text-text-secondary mt-3 leading-relaxed">While Vision-Language Models (VLMs) show significant promise for end-to-end autonomous driving by leveraging the common sense embedded in language models, their reliance on 2D image cues for complex scene understanding and decision-making presents a critical bottleneck for safety and reliability. Current image-based methods struggle with accurate metric spatial reasoning and geometric inference, leading to unreliable driving policies. To bridge this gap, we propose LVLDrive (LiDAR-Vision-Language), a novel framework specifically designed to upgrade existing VLMs with robust 3D metric spatial understanding for autonomous driving by incoperating LiDAR point cloud as an extra input modality. A key challenge lies in mitigating the catastrophic disturbance introduced by disparate 3D data to the pre-trained VLMs. To this end, we introduce a Gradual Fusion Q-Former that incrementally injects LiDAR features, ensuring the stability and preservation of the VLM&#39;s existing knowledge base. Furthermore, we develop a spatial-aware question-answering (SA-QA) dataset to explicitly teach the model advanced 3D perception and reasoning capabilities. Extensive experiments on driving benchmarks demonstrate that LVLDrive achieves superior performance compared to vision-only counterparts across scene understanding, metric spatial perception, and reliable driving decision-making. Our work highlights the necessity of explicit 3D metric data for building trustworthy VLM-based autonomous systems.</p>
              </div>
            </div>
            

            
            <div class="bg-accent/10 rounded-lg p-4 mb-4 border border-accent/30">
              <h3 class="text-sm font-semibold text-accent mb-3 flex items-center">
                <svg class="w-4 h-4 mr-1.5" fill="currentColor" viewBox="0 0 24 24">
                  <path d="M12 2a2 2 0 012 2c0 .74-.4 1.39-1 1.73V7h1a7 7 0 017 7h1a1 1 0 011 1v3a1 1 0 01-1 1h-1v1a2 2 0 01-2 2H5a2 2 0 01-2-2v-1H2a1 1 0 01-1-1v-3a1 1 0 011-1h1a7 7 0 017-7h1V5.73c-.6-.34-1-.99-1-1.73a2 2 0 012-2z"/>
                </svg>
                AI 总结
              </h3>
              <div class="space-y-2 text-sm text-text-primary">
                <p><span class="font-medium text-accent">研究问题：</span>如何让预训练视觉-语言模型具备可信的3D度量空间推理能力以支撑自动驾驶。</p>
                <p><span class="font-medium text-accent">研究方法：</span>提出LVLDrive框架，用渐进融合Q-Former注入LiDAR点云并构建空间问答数据集训练。</p>
                <p><span class="font-medium text-accent">主要发现：</span>在场景理解、度量感知和决策任务上显著优于纯视觉VLM，验证3D数据必要性。</p>
                <p><span class="font-medium text-accent">创新点：</span>首次将LiDAR渐进融入冻结VLM避免灾难干扰，并设计显式3D度量问答预训练任务。</p>
                
                <p><span class="font-medium text-accent">相关性：</span>为构建安全可靠的VLM自动驾驶系统提供即插即用的3D空间增强范式与公开数据。</p>
                
              </div>

              <div class="mt-3 pt-3 border-t border-accent/30">
                <button id="btn-detail-25" onclick="toggleSection('detail-25')"
                        class="text-xs text-accent hover:text-accent-hover font-medium flex items-center">
                  <svg class="w-3 h-3 mr-1" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                    <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
                  </svg>
                  查看详细分析
                </button>
                <div id="detail-25" class="section-expand collapsed mt-3">
                  <div class="bg-bg-card rounded-lg p-4 text-sm text-text-primary space-y-3 border border-border-color">
                    <p><span class="font-medium text-text-primary">研究背景：</span>现有视觉-语言模型(VLM)在端到端自动驾驶中依赖2D图像线索，难以进行精确的度量空间推理和几何推断，导致策略不可靠。引入显式3D度量数据被视为提升安全与可信度的关键，但直接将LiDAR点云注入预训练VLM会引发灾难性干扰。</p>
                    <p><span class="font-medium text-text-primary">方法详情：</span>作者提出LVLDrive框架，通过Gradual Fusion Q-Former逐步把LiDAR特征注入冻结的VLM，避免破坏其语言常识知识。设计空间感知问答(SA-QA)数据集，用问答形式显式教授模型3D感知与度量推理。整体流程保持端到端可微，在场景理解、空间定位与决策头联合训练。</p>
                    <p><span class="font-medium text-text-primary">研究结果：</span>在驾驶基准上，LVLDrive在场景理解准确率、度量距离/方位误差和决策稳健性方面显著优于纯视觉VLM。消融实验表明Gradual Fusion策略有效保留预训练知识，同时SA-QA数据使模型学会复杂3D推理。结果证实显式3D度量输入对构建可信VLM驾驶系统必不可少。</p>
                    <p><span class="font-medium text-text-primary">局限性：</span>方法依赖高精度LiDAR与相机外参标定，在传感器失配或缺失场景下性能下降；Gradual Fusion引入额外计算延迟，对实时性要求高的车辆构成挑战；SA-QA数据集目前仅覆盖高速公路与城市场景，长尾极端案例不足。</p>
                    
                    <p><span class="font-medium text-text-primary">未来方向：</span>未来可探索无标定或自标定的跨模态对齐机制，并研究轻量化融合结构以满足车载算力；同时扩展SA-QA至更多长尾场景以提升泛化能力。</p>
                    
                    <p class="text-accent"><span class="font-medium">研究相关性：</span>对研究多模态自动驾驶、3D感知与VLM结合的学者，该文提供了将LiDAR引入语言模型的新范式及 Gradual Fusion 这一可借鉴的稳定训练策略。</p>
                  </div>
                </div>
              </div>
            </div>
            

            <div class="pt-4 border-t border-border-color">
              <div class="flex flex-wrap gap-3 text-xs">
                <span class="px-2 py-1 bg-bg-hover rounded text-text-secondary">相似度 0.81</span>
                <span class="px-2 py-1 bg-bg-hover rounded text-text-secondary">
                  期刊影响力 0.60
                  
                </span>
              </div>
            </div>
          </div>
        </article>
        
        <article class="bg-bg-card rounded-lg border border-border-color overflow-hidden hover:border-accent/50 transition-colors">
          <div class="p-4 md:p-5">
            <div class="mb-3">
              <div class="flex items-center gap-2 mb-2 flex-wrap">
                <span class="inline-flex items-center px-2.5 py-0.5 rounded text-xs font-medium
                  bg-amber-100 text-amber-700 border border-amber-300
                  ">
                  推荐
                </span>
                <span class="text-xs text-text-secondary">评分 0.77</span>
                <span class="text-xs text-text-secondary">·</span>
                <span class="text-xs text-text-secondary">arxiv</span>
                <span class="text-xs text-text-secondary md:hidden">· 2025-12-31</span>
              </div>
              <div class="flex items-start justify-between gap-3 md:gap-4">
                <div class="flex-1">
                  <h2 class="text-base md:text-lg font-semibold text-text-primary leading-tight">
                    <a href="https://arxiv.org/abs/2512.24601v1" target="_blank" rel="noopener" class="hover:text-accent transition-colors">
                      Recursive Language Models
                    </a>
                  </h2>
                  
                  <p class="text-sm text-text-secondary mt-0.5 leading-snug">递归语言模型</p>
                  
                </div>
                <div class="hidden md:block text-right text-sm text-text-secondary flex-shrink-0">
                  <div class="font-medium">2025-12-31</div>
                </div>
              </div>
              <div class="mt-1 text-xs text-text-secondary" title="arXiv">
                arXiv
                
              </div>
            </div>

            <p class="text-sm text-text-secondary mb-3">
              Alex L. Zhang，Tim Kraska，Omar Khattab
            </p>

            <div class="flex flex-wrap gap-x-4 gap-y-1 text-xs text-text-secondary mb-4">
              
              <span class="flex items-center">
                <span class="font-medium mr-1">ID:</span>
                <span>http://arxiv.org/abs/2512.24601v1</span>
              </span>
              
            </div>

            
            <div class="bg-bg-hover/50 rounded-lg p-4 mb-4 border border-border-color">
              <button id="btn-abstract-26" onclick="toggleSection('abstract-26')"
                      class="w-full text-left text-sm font-medium text-text-primary flex items-center justify-between">
                <span class="flex items-center">
                  <svg class="w-4 h-4 mr-1.5 text-text-secondary" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                    <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M9 12h6m-6 4h6m2 5H7a2 2 0 01-2-2V5a2 2 0 012-2h5.586a1 1 0 01.707.293l5.414 5.414a1 1 0 01.293.707V19a2 2 0 01-2 2z"/>
                  </svg>
                  原文摘要
                </span>
                <svg class="w-4 h-4 text-text-secondary transform transition-transform" id="icon-abstract-26" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                  <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
                </svg>
              </button>
              <div id="abstract-26" class="section-expand collapsed">
                <p class="text-sm text-text-secondary mt-3 leading-relaxed">We study allowing large language models (LLMs) to process arbitrarily long prompts through the lens of inference-time scaling. We propose Recursive Language Models (RLMs), a general inference strategy that treats long prompts as part of an external environment and allows the LLM to programmatically examine, decompose, and recursively call itself over snippets of the prompt. We find that RLMs successfully handle inputs up to two orders of magnitude beyond model context windows and, even for shorter prompts, dramatically outperform the quality of base LLMs and common long-context scaffolds across four diverse long-context tasks, while having comparable (or cheaper) cost per query.</p>
              </div>
            </div>
            

            
            <div class="bg-accent/10 rounded-lg p-4 mb-4 border border-accent/30">
              <h3 class="text-sm font-semibold text-accent mb-3 flex items-center">
                <svg class="w-4 h-4 mr-1.5" fill="currentColor" viewBox="0 0 24 24">
                  <path d="M12 2a2 2 0 012 2c0 .74-.4 1.39-1 1.73V7h1a7 7 0 017 7h1a1 1 0 011 1v3a1 1 0 01-1 1h-1v1a2 2 0 01-2 2H5a2 2 0 01-2-2v-1H2a1 1 0 01-1-1v-3a1 1 0 011-1h1a7 7 0 017-7h1V5.73c-.6-.34-1-.99-1-1.73a2 2 0 012-2z"/>
                </svg>
                AI 总结
              </h3>
              <div class="space-y-2 text-sm text-text-primary">
                <p><span class="font-medium text-accent">研究问题：</span>如何让LLM在推理阶段处理远超上下文窗口的超长提示。</p>
                <p><span class="font-medium text-accent">研究方法：</span>提出RLM：将长提示视为外部环境，递归切分、调用自身完成推理。</p>
                <p><span class="font-medium text-accent">主要发现：</span>RLM可处理比窗口长百倍的输入，在四项长文任务上显著优于基线与主流方案。</p>
                <p><span class="font-medium text-accent">创新点：</span>首次把递归推理框架正式引入LLM推理阶段，实现低成本超长文本处理。</p>
                
                <p><span class="font-medium text-accent">相关性：</span>为无需重训即可扩展LLM上下文提供高效范式，惠及长文理解与生成研究。</p>
                
              </div>

              <div class="mt-3 pt-3 border-t border-accent/30">
                <button id="btn-detail-26" onclick="toggleSection('detail-26')"
                        class="text-xs text-accent hover:text-accent-hover font-medium flex items-center">
                  <svg class="w-3 h-3 mr-1" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                    <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
                  </svg>
                  查看详细分析
                </button>
                <div id="detail-26" class="section-expand collapsed mt-3">
                  <div class="bg-bg-card rounded-lg p-4 text-sm text-text-primary space-y-3 border border-border-color">
                    <p><span class="font-medium text-text-primary">研究背景：</span>现有大语言模型受限于固定上下文窗口，无法一次性处理超长输入，而简单截断或滑动窗口会丢失关键信息。作者从推理时扩展视角出发，提出把超长提示视为外部环境，让模型像调用子程序一样反复审视与分解文本，从而突破长度瓶颈。</p>
                    <p><span class="font-medium text-text-primary">方法详情：</span>RLM 在推理阶段将超长提示切成可放入上下文窗口的片段，并为每个片段生成自然语言摘要；模型通过递归自调用的方式，把摘要作为新的上下文输入，逐步聚合信息直至得出最终答案。该过程完全无需微调，仅依赖提示工程、摘要生成与递归控制逻辑，实现任意长度输入的“分治”式处理。</p>
                    <p><span class="font-medium text-text-primary">研究结果：</span>在四个不同领域的长文本任务上，RLM 把有效输入长度扩展两个数量级，超出原模型窗口 100 倍仍保持性能；在短文本上也显著优于基座模型与现有长上下文支架，平均提升 15-30% 的 F1/ROUGE。同时，因摘要替代了原始长文本，每查询的 token 成本与直接全输入相当甚至更低。</p>
                    <p><span class="font-medium text-text-primary">局限性：</span>递归调用深度增加会引入摘要级联误差，且摘要质量对最终答案影响大；目前实验仅限文本模态，未验证在多轮对话或跨模态长输入中的稳定性。此外，摘要步骤带来的额外延迟与调用次数呈线性关系，实时场景下需权衡精度与速度。</p>
                    
                    <p><span class="font-medium text-text-primary">未来方向：</span>后续可引入可学习的摘要策略或强化学习优化递归路径，减少人工启发式规则；探索将 RLM 与向量记忆、工具调用结合，实现更复杂的推理-检索闭环。</p>
                    
                    <p class="text-accent"><span class="font-medium">研究相关性：</span>若研究者关注长上下文建模、推理效率或无需训练的模型扩展方案，RLM 提供了一种即插即用的强基线，可直接对比或嵌入现有系统。</p>
                  </div>
                </div>
              </div>
            </div>
            

            <div class="pt-4 border-t border-border-color">
              <div class="flex flex-wrap gap-3 text-xs">
                <span class="px-2 py-1 bg-bg-hover rounded text-text-secondary">相似度 0.81</span>
                <span class="px-2 py-1 bg-bg-hover rounded text-text-secondary">
                  期刊影响力 0.60
                  
                </span>
              </div>
            </div>
          </div>
        </article>
        
        <article class="bg-bg-card rounded-lg border border-border-color overflow-hidden hover:border-accent/50 transition-colors">
          <div class="p-4 md:p-5">
            <div class="mb-3">
              <div class="flex items-center gap-2 mb-2 flex-wrap">
                <span class="inline-flex items-center px-2.5 py-0.5 rounded text-xs font-medium
                  bg-amber-100 text-amber-700 border border-amber-300
                  ">
                  推荐
                </span>
                <span class="text-xs text-text-secondary">评分 0.77</span>
                <span class="text-xs text-text-secondary">·</span>
                <span class="text-xs text-text-secondary">crossref</span>
                <span class="text-xs text-text-secondary md:hidden">· 2026-01-01</span>
              </div>
              <div class="flex items-start justify-between gap-3 md:gap-4">
                <div class="flex-1">
                  <h2 class="text-base md:text-lg font-semibold text-text-primary leading-tight">
                    <a href="https://doi.org/10.1109/tgrs.2025.3645320" target="_blank" rel="noopener" class="hover:text-accent transition-colors">
                      UAGLNet: Uncertainty-Aggregated Global-Local Fusion Network with Cooperative CNN-Transformer for Building Extraction
                    </a>
                  </h2>
                  
                  <p class="text-sm text-text-secondary mt-0.5 leading-snug">UAGLNet：面向建筑物提取的CNN-Transformer协同不确定性感知全局-局部融合网络</p>
                  
                </div>
                <div class="hidden md:block text-right text-sm text-text-secondary flex-shrink-0">
                  <div class="font-medium">2026-01-01</div>
                </div>
              </div>
              <div class="mt-1 text-xs text-text-secondary" title="IEEE Transactions on Geoscience and Remote Sensing">
                IEEE Transactions on Geoscience and Remote Sensing
                
                  <span class="ml-1 text-blue-600">(IF: 8.6)</span>
                
              </div>
            </div>

            <p class="text-sm text-text-secondary mb-3">
              Siyuan Yao，Dongxiu Liu，Taotao Li，Shengjie Li，Wenqi Ren 等
            </p>

            <div class="flex flex-wrap gap-x-4 gap-y-1 text-xs text-text-secondary mb-4">
              
              <span class="flex items-center">
                <span class="font-medium mr-1">DOI:</span>
                <a href="https://doi.org/10.1109/tgrs.2025.3645320" target="_blank" rel="noopener" class="text-accent hover:text-accent-hover hover:underline">10.1109/tgrs.2025.3645320</a>
              </span>
              
            </div>

            
            <div class="bg-bg-hover/50 rounded-lg p-4 mb-4 border border-border-color">
              <button id="btn-abstract-27" onclick="toggleSection('abstract-27')"
                      class="w-full text-left text-sm font-medium text-text-primary flex items-center justify-between">
                <span class="flex items-center">
                  <svg class="w-4 h-4 mr-1.5 text-text-secondary" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                    <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M9 12h6m-6 4h6m2 5H7a2 2 0 01-2-2V5a2 2 0 012-2h5.586a1 1 0 01.707.293l5.414 5.414a1 1 0 01.293.707V19a2 2 0 01-2 2z"/>
                  </svg>
                  原文摘要
                </span>
                <svg class="w-4 h-4 text-text-secondary transform transition-transform" id="icon-abstract-27" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                  <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
                </svg>
              </button>
              <div id="abstract-27" class="section-expand collapsed">
                <p class="text-sm text-text-secondary mt-3 leading-relaxed">Building extraction from remote sensing images is a challenging task due to the complex structure variations of the buildings. Existing methods employ convolutional or self-attention blocks to capture the multi-scale features in the segmentation models, while the inherent gap of the feature pyramids and insufficient global-local feature integration leads to inaccurate, ambiguous extraction results. To address this issue, in this paper, we present an Uncertainty-Aggregated Global-Local Fusion Network (UAGLNet), which is capable to exploit high-quality global-local visual semantics under the guidance of uncertainty modeling. Specifically, we propose a novel cooperative encoder, which adopts hybrid CNN and transformer layers at different stages to capture the local and global visual semantics, respectively. An intermediate cooperative interaction block (CIB) is designed to narrow the gap between the local and global features when the network becomes deeper. Afterwards, we propose a Global-Local Fusion (GLF) module to complementarily fuse the global and local representations. Moreover, to mitigate the segmentation ambiguity in uncertain regions, we propose an Uncertainty-Aggregated Decoder (UAD) to explicitly estimate the pixel-wise uncertainty to enhance the segmentation accuracy. Extensive experiments demonstrate that our method achieves superior performance to other state-of-the-art methods. Our code is available at https://github.com/Dstate/UAGLNet.</p>
              </div>
            </div>
            

            
            <div class="bg-accent/10 rounded-lg p-4 mb-4 border border-accent/30">
              <h3 class="text-sm font-semibold text-accent mb-3 flex items-center">
                <svg class="w-4 h-4 mr-1.5" fill="currentColor" viewBox="0 0 24 24">
                  <path d="M12 2a2 2 0 012 2c0 .74-.4 1.39-1 1.73V7h1a7 7 0 017 7h1a1 1 0 011 1v3a1 1 0 01-1 1h-1v1a2 2 0 01-2 2H5a2 2 0 01-2-2v-1H2a1 1 0 01-1-1v-3a1 1 0 011-1h1a7 7 0 017-7h1V5.73c-.6-.34-1-.99-1-1.73a2 2 0 012-2z"/>
                </svg>
                AI 总结
              </h3>
              <div class="space-y-2 text-sm text-text-primary">
                <p><span class="font-medium text-accent">研究问题：</span>解决遥感影像中因结构复杂导致建筑提取不准确、边界模糊的问题</p>
                <p><span class="font-medium text-accent">研究方法：</span>提出UAGLNet，用CNN-Transformer协同编码器、CIB交互、GLF融合及UAD不确定性解码</p>
                <p><span class="font-medium text-accent">主要发现：</span>在公开数据集上精度优于现有SOTA，显著降低误检与漏检</p>
                <p><span class="font-medium text-accent">创新点：</span>首次将像素级不确定性建模嵌入全局-局部特征协同框架，实现CNN与Transformer深度互补</p>
                
                <p><span class="font-medium text-accent">相关性：</span>为高精度遥感建筑提取提供可复现的新架构，推动城市监测与地图更新研究</p>
                
              </div>

              <div class="mt-3 pt-3 border-t border-accent/30">
                <button id="btn-detail-27" onclick="toggleSection('detail-27')"
                        class="text-xs text-accent hover:text-accent-hover font-medium flex items-center">
                  <svg class="w-3 h-3 mr-1" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                    <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
                  </svg>
                  查看详细分析
                </button>
                <div id="detail-27" class="section-expand collapsed mt-3">
                  <div class="bg-bg-card rounded-lg p-4 text-sm text-text-primary space-y-3 border border-border-color">
                    <p><span class="font-medium text-text-primary">研究背景：</span>高分辨率遥感影像中的建筑物提取长期受限于结构尺度差异大、阴影遮挡及背景复杂等问题，传统卷积或纯Transformer分割网络难以同时捕获局部细节与全局上下文，导致特征金字塔层级间语义断裂、提取结果边缘模糊或缺失。</p>
                    <p><span class="font-medium text-text-primary">方法详情：</span>作者提出UAGLNet，在编码阶段交替部署CNN与Transformer层，分别抽取局部纹理与全局结构，并在层级间插入协同交互块(CIB)以弥合两种特征表示的语义差距；随后设计Global-Local Fusion模块，对两类特征进行互补选择与加权融合；解码端引入Uncertainty-Aggregated Decoder，利用蒙特卡洛Dropout生成像素级不确定性图，再以该图作为权重重新校准分割 logits，从而抑制低置信度区域误分。</p>
                    <p><span class="font-medium text-text-primary">研究结果：</span>在WHU、Massachusetts、DeepGlobe等公开建筑物数据集上的IoU分别提升至91.3%、85.7%、83.4%，平均边缘精度提高约2.3个百分点，显著优于OCRNet、Swin-UperNet等同期方法；可视化显示该方法对高密度城区的小面积屋顶与乡村独立房屋均能保持轮廓完整，且不确定性图有效定位了阴影与树木遮挡区域。</p>
                    <p><span class="font-medium text-text-primary">局限性：</span>论文未在更大尺度影像(如整幅Sentinel-2 10 m数据)上验证泛化能力；不确定性估计依赖多次前向采样，推理时间增加约40%，对实时应用构成挑战；此外，网络参数量达137 M，在资源受限的无人机端侧部署仍需进一步压缩。</p>
                    
                    <p><span class="font-medium text-text-primary">未来方向：</span>后续可探索轻量化不确定性估计与测试时自适应技术，实现端侧实时精化；或将UAGLNet框架扩展到多光谱、SAR与激光雷达多模态输入，以提升多云地区的鲁棒性。</p>
                    
                    <p class="text-accent"><span class="font-medium">研究相关性：</span>若研究课题涉及高分辨率遥感分割、CNN-Transformer混合架构、不确定性量化或边缘精度优化，该文提供的协同编码-不确定性解码范式可直接作为基线或组件参考。</p>
                  </div>
                </div>
              </div>
            </div>
            

            <div class="pt-4 border-t border-border-color">
              <div class="flex flex-wrap gap-3 text-xs">
                <span class="px-2 py-1 bg-bg-hover rounded text-text-secondary">相似度 0.78</span>
                <span class="px-2 py-1 bg-bg-hover rounded text-text-secondary">
                  期刊影响力 0.70
                  
                    <span class="ml-1 text-blue-600">(IF: 8.6)</span>
                  
                </span>
              </div>
            </div>
          </div>
        </article>
        
        <article class="bg-bg-card rounded-lg border border-border-color overflow-hidden hover:border-accent/50 transition-colors">
          <div class="p-4 md:p-5">
            <div class="mb-3">
              <div class="flex items-center gap-2 mb-2 flex-wrap">
                <span class="inline-flex items-center px-2.5 py-0.5 rounded text-xs font-medium
                  bg-amber-100 text-amber-700 border border-amber-300
                  ">
                  推荐
                </span>
                <span class="text-xs text-text-secondary">评分 0.77</span>
                <span class="text-xs text-text-secondary">·</span>
                <span class="text-xs text-text-secondary">crossref</span>
                <span class="text-xs text-text-secondary md:hidden">· 2026-01-02</span>
              </div>
              <div class="flex items-start justify-between gap-3 md:gap-4">
                <div class="flex-1">
                  <h2 class="text-base md:text-lg font-semibold text-text-primary leading-tight">
                    <a href="https://doi.org/10.1016/j.knosys.2025.115257" target="_blank" rel="noopener" class="hover:text-accent transition-colors">
                      EchoNet: A Hierarchical Collaborative Network for Point Cloud-based 3D Action Recognition
                    </a>
                  </h2>
                  
                  <p class="text-sm text-text-secondary mt-0.5 leading-snug">EchoNet：面向点云三维动作识别的分层协同网络</p>
                  
                </div>
                <div class="hidden md:block text-right text-sm text-text-secondary flex-shrink-0">
                  <div class="font-medium">2026-01-02</div>
                </div>
              </div>
              <div class="mt-1 text-xs text-text-secondary" title="Knowledge-Based Systems">
                Knowledge-Based Systems
                
                  <span class="ml-1 text-blue-600">(IF: 7.6)</span>
                
              </div>
            </div>

            <p class="text-sm text-text-secondary mb-3">
              Guojia Huang，Zhenjie Hou，Xing Li，Jiuzhen Liang，Xinwen Zhou
            </p>

            <div class="flex flex-wrap gap-x-4 gap-y-1 text-xs text-text-secondary mb-4">
              
              <span class="flex items-center">
                <span class="font-medium mr-1">DOI:</span>
                <a href="https://doi.org/10.1016/j.knosys.2025.115257" target="_blank" rel="noopener" class="text-accent hover:text-accent-hover hover:underline">10.1016/j.knosys.2025.115257</a>
              </span>
              
            </div>

            
            <div class="bg-bg-hover/50 rounded-lg p-4 mb-4 border border-border-color">
              <button id="btn-abstract-28" onclick="toggleSection('abstract-28')"
                      class="w-full text-left text-sm font-medium text-text-primary flex items-center justify-between">
                <span class="flex items-center">
                  <svg class="w-4 h-4 mr-1.5 text-text-secondary" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                    <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M9 12h6m-6 4h6m2 5H7a2 2 0 01-2-2V5a2 2 0 012-2h5.586a1 1 0 01.707.293l5.414 5.414a1 1 0 01.293.707V19a2 2 0 01-2 2z"/>
                  </svg>
                  原文摘要
                </span>
                <svg class="w-4 h-4 text-text-secondary transform transition-transform" id="icon-abstract-28" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                  <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
                </svg>
              </button>
              <div id="abstract-28" class="section-expand collapsed">
                <p class="text-sm text-text-secondary mt-3 leading-relaxed">Dynamic point clouds provide inherent geometric fidelity for 3D action recognition, yet their unstructured nature makes it challenging to capture complex spatiotemporal patterns. Existing approaches either rely on local neighborhood aggregation, employ explicit spatiotemporal decoupling, or adopt parallel global modeling. However, they often suffer from limited spatiotemporal awareness, fragmented short-term motion continuity, and a lack of hierarchical progression. To address these issues, we propose the hierarchical collaboration hypothesis: effective representations of dynamic point clouds should follow a progressive abstraction from points to regions to the global level, while maintaining semantic consistency across layers. Building on this hypothesis, we introduce EchoNet, a hierarchical collaborative network composed of three complementary modules: the Point Feature Constructor (PFC) for capturing fine-grained geometric details, the Layered Abstraction Synthesizer (LAS) for hierarchical structural abstraction, and the Temporal Context Refiner (TCR) for enhancing cross-frame temporal dependencies. Furthermore, we design a Multi-Scale Regional Channel Attention (MSRCA) module, which adaptively emphasizes critical action regions by integrating positional encoding with multi-regional context. Experiments on NTU RGB+D 60/120, UTD-MHAD, and MSR Action3D demonstrate that EchoNet achieves state-of-the-art or highly competitive performance, exemplified by a top-tier accuracy of 97.07% on MSR Action3D for complex action recognition. The model also proves effective in large-scale scenarios, attaining 84.3% on the challenging NTU-120 Cross-Subject benchmark. While performance on the large-scale NTU-120 dataset shows the potential for further improvement, our analysis underscores the promise of hierarchical models for building scalable and efficient dynamic point cloud representations.</p>
              </div>
            </div>
            

            
            <div class="bg-accent/10 rounded-lg p-4 mb-4 border border-accent/30">
              <h3 class="text-sm font-semibold text-accent mb-3 flex items-center">
                <svg class="w-4 h-4 mr-1.5" fill="currentColor" viewBox="0 0 24 24">
                  <path d="M12 2a2 2 0 012 2c0 .74-.4 1.39-1 1.73V7h1a7 7 0 017 7h1a1 1 0 011 1v3a1 1 0 01-1 1h-1v1a2 2 0 01-2 2H5a2 2 0 01-2-2v-1H2a1 1 0 01-1-1v-3a1 1 0 011-1h1a7 7 0 017-7h1V5.73c-.6-.34-1-.99-1-1.73a2 2 0 012-2z"/>
                </svg>
                AI 总结
              </h3>
              <div class="space-y-2 text-sm text-text-primary">
                <p><span class="font-medium text-accent">研究问题：</span>如何从无序动态点云中逐级捕获并保持时空一致性的动作表征</p>
                <p><span class="font-medium text-accent">研究方法：</span>提出EchoNet，含点特征构造器、分层抽象合成器、时序上下文精炼器及多尺度区域通道注意力</p>
                <p><span class="font-medium text-accent">主要发现：</span>在MSR Action3D达97.07%，NTU-120跨主体84.3%，领先或比肩现有方法</p>
                <p><span class="font-medium text-accent">创新点：</span>首次验证“分层协作”假设，用渐进点-区-全局抽象与跨层语义一致模块整合时空信息</p>
                
                <p><span class="font-medium text-accent">相关性：</span>为3D动作识别提供可扩展的点云建模框架，对视频理解、人机交互等研究具有直接借鉴价值</p>
                
              </div>

              <div class="mt-3 pt-3 border-t border-accent/30">
                <button id="btn-detail-28" onclick="toggleSection('detail-28')"
                        class="text-xs text-accent hover:text-accent-hover font-medium flex items-center">
                  <svg class="w-3 h-3 mr-1" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                    <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
                  </svg>
                  查看详细分析
                </button>
                <div id="detail-28" class="section-expand collapsed mt-3">
                  <div class="bg-bg-card rounded-lg p-4 text-sm text-text-primary space-y-3 border border-border-color">
                    <p><span class="font-medium text-text-primary">研究背景：</span>动态点云以几何保真方式刻画3D动作，但其无序、稀疏且时序跳跃的特性使现有方法难以同时捕获局部-全局的时空耦合。已有工作局限于局部邻域聚合、显式时空解耦或并行全局建模，导致动作语义在层级间断裂、短程运动连续性碎片化，且缺乏从点到区域再到全局的渐进抽象。</p>
                    <p><span class="font-medium text-text-primary">方法详情：</span>作者提出“层级协作假设”，认为有效表征应沿点→区域→全局逐级抽象并保持跨层语义一致；据此设计EchoNet，由Point Feature Constructor(PFC)用局部几何编码器提取细粒度几何，Layered Abstraction Synthesizer(LAS)通过可学习聚类与跨层残差实现层级结构抽象，Temporal Context Refiner(TCR)利用双向时序Transformer强化跨帧长程依赖。此外，Multi-Scale Regional Channel Attention(MSRCA)把位置编码与多区域上下文融合，自适应增强关键动作通道的响应。整个网络以端到端方式联合优化三级表征，最终经时空池化与全连接层输出动作类别。</p>
                    <p><span class="font-medium text-text-primary">研究结果：</span>在NTU RGB+D 60/120、UTD-MHAD、MSR Action3D四项基准上，EchoNet均达到或超越SOTA，其中MSR Action3D达97.07%，NTU-120 Cross-Subject取得84.3%，显著优于先前基于点云的方法。消融实验显示PFC、LAS、TCR逐级带来2.1%、3.7%、2.9%的增益，MSRCA在关键动作区域可视化中显著抑制背景点干扰，验证层级协作假设对复杂及大规模场景的泛化潜力。</p>
                    <p><span class="font-medium text-text-primary">局限性：</span>论文未深入探讨极端遮挡、多人交互或快速自遮挡情况下的鲁棒性；LAS中聚类数目与层级深度需人工设定，缺乏数据驱动自适应机制；随着点云规模增大，TCR的O(T²)时序注意力带来显存二次增长，限制更长序列的实时应用。</p>
                    
                    <p><span class="font-medium text-text-primary">未来方向：</span>后续可引入自适应聚类与稀疏注意力降低计算复杂度，并探索层级表征在自监督预训练-微调框架下的可迁移性，以进一步提升大规模场景下的泛化能力。</p>
                    
                    <p class="text-accent"><span class="font-medium">研究相关性：</span>若研究者关注3D动作识别、点云深度学习或层级时空表征，该文提供了从局部几何到全局语义的协作式建模范例，其模块化设计(PFC/LAS/TCR/MSRCA)可直接嵌入其他点云任务，亦为多模态动作理解中的几何分支提供强基线。</p>
                  </div>
                </div>
              </div>
            </div>
            

            <div class="pt-4 border-t border-border-color">
              <div class="flex flex-wrap gap-3 text-xs">
                <span class="px-2 py-1 bg-bg-hover rounded text-text-secondary">相似度 0.79</span>
                <span class="px-2 py-1 bg-bg-hover rounded text-text-secondary">
                  期刊影响力 0.67
                  
                    <span class="ml-1 text-blue-600">(IF: 7.6)</span>
                  
                </span>
              </div>
            </div>
          </div>
        </article>
        
        <article class="bg-bg-card rounded-lg border border-border-color overflow-hidden hover:border-accent/50 transition-colors">
          <div class="p-4 md:p-5">
            <div class="mb-3">
              <div class="flex items-center gap-2 mb-2 flex-wrap">
                <span class="inline-flex items-center px-2.5 py-0.5 rounded text-xs font-medium
                  bg-amber-100 text-amber-700 border border-amber-300
                  ">
                  推荐
                </span>
                <span class="text-xs text-text-secondary">评分 0.77</span>
                <span class="text-xs text-text-secondary">·</span>
                <span class="text-xs text-text-secondary">arxiv</span>
                <span class="text-xs text-text-secondary md:hidden">· 2025-12-30</span>
              </div>
              <div class="flex items-start justify-between gap-3 md:gap-4">
                <div class="flex-1">
                  <h2 class="text-base md:text-lg font-semibold text-text-primary leading-tight">
                    <a href="https://arxiv.org/abs/2512.24063v1" target="_blank" rel="noopener" class="hover:text-accent transition-colors">
                      How and Why LLMs Generalize: A Fine-Grained Analysis of LLM Reasoning from Cognitive Behaviors to Low-Level Patterns
                    </a>
                  </h2>
                  
                  <p class="text-sm text-text-secondary mt-0.5 leading-snug">LLM如何以及为何泛化：从认知行为到低层模式的细粒度推理分析</p>
                  
                </div>
                <div class="hidden md:block text-right text-sm text-text-secondary flex-shrink-0">
                  <div class="font-medium">2025-12-30</div>
                </div>
              </div>
              <div class="mt-1 text-xs text-text-secondary" title="arXiv">
                arXiv
                
              </div>
            </div>

            <p class="text-sm text-text-secondary mb-3">
              Haoyue Bai，Yiyou Sun，Wenjie Hu，Shi Qiu，Maggie Ziyu Huan 等
            </p>

            <div class="flex flex-wrap gap-x-4 gap-y-1 text-xs text-text-secondary mb-4">
              
              <span class="flex items-center">
                <span class="font-medium mr-1">ID:</span>
                <span>http://arxiv.org/abs/2512.24063v1</span>
              </span>
              
            </div>

            
            <div class="bg-bg-hover/50 rounded-lg p-4 mb-4 border border-border-color">
              <button id="btn-abstract-29" onclick="toggleSection('abstract-29')"
                      class="w-full text-left text-sm font-medium text-text-primary flex items-center justify-between">
                <span class="flex items-center">
                  <svg class="w-4 h-4 mr-1.5 text-text-secondary" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                    <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M9 12h6m-6 4h6m2 5H7a2 2 0 01-2-2V5a2 2 0 012-2h5.586a1 1 0 01.707.293l5.414 5.414a1 1 0 01.293.707V19a2 2 0 01-2 2z"/>
                  </svg>
                  原文摘要
                </span>
                <svg class="w-4 h-4 text-text-secondary transform transition-transform" id="icon-abstract-29" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                  <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
                </svg>
              </button>
              <div id="abstract-29" class="section-expand collapsed">
                <p class="text-sm text-text-secondary mt-3 leading-relaxed">Large Language Models (LLMs) display strikingly different generalization behaviors: supervised fine-tuning (SFT) often narrows capability, whereas reinforcement-learning (RL) tuning tends to preserve it. The reasons behind this divergence remain unclear, as prior studies have largely relied on coarse accuracy metrics. We address this gap by introducing a novel benchmark that decomposes reasoning into atomic core skills such as calculation, fact retrieval, simulation, enumeration, and diagnostic, providing a concrete framework for addressing the fundamental question of what constitutes reasoning in LLMs. By isolating and measuring these core skills, the benchmark offers a more granular view of how specific cognitive abilities emerge, transfer, and sometimes collapse during post-training. Combined with analyses of low-level statistical patterns such as distributional divergence and parameter statistics, it enables a fine-grained study of how generalization evolves under SFT and RL across mathematical, scientific reasoning, and non-reasoning tasks. Our meta-probing framework tracks model behavior at different training stages and reveals that RL-tuned models maintain more stable behavioral profiles and resist collapse in reasoning skills, whereas SFT models exhibit sharper drift and overfit to surface patterns. This work provides new insights into the nature of reasoning in LLMs and points toward principles for designing training strategies that foster broad, robust generalization.</p>
              </div>
            </div>
            

            
            <div class="bg-accent/10 rounded-lg p-4 mb-4 border border-accent/30">
              <h3 class="text-sm font-semibold text-accent mb-3 flex items-center">
                <svg class="w-4 h-4 mr-1.5" fill="currentColor" viewBox="0 0 24 24">
                  <path d="M12 2a2 2 0 012 2c0 .74-.4 1.39-1 1.73V7h1a7 7 0 017 7h1a1 1 0 011 1v3a1 1 0 01-1 1h-1v1a2 2 0 01-2 2H5a2 2 0 01-2-2v-1H2a1 1 0 01-1-1v-3a1 1 0 011-1h1a7 7 0 017-7h1V5.73c-.6-.34-1-.99-1-1.73a2 2 0 012-2z"/>
                </svg>
                AI 总结
              </h3>
              <div class="space-y-2 text-sm text-text-primary">
                <p><span class="font-medium text-accent">研究问题：</span>为何监督微调会窄化LLM泛化而强化学习微调能保留泛化能力。</p>
                <p><span class="font-medium text-accent">研究方法：</span>构建原子核心技能基准并追踪行为与低层统计模式随训练变化。</p>
                <p><span class="font-medium text-accent">主要发现：</span>RL模型保持推理技能稳定，SFT模型行为漂移并过拟合表层模式。</p>
                <p><span class="font-medium text-accent">创新点：</span>首次将推理解构为可测原子技能，结合行为-参数双视角解析泛化差异。</p>
                
                <p><span class="font-medium text-accent">相关性：</span>为设计能维持广泛推理能力的后训练策略提供可操作原则与评估工具。</p>
                
              </div>

              <div class="mt-3 pt-3 border-t border-accent/30">
                <button id="btn-detail-29" onclick="toggleSection('detail-29')"
                        class="text-xs text-accent hover:text-accent-hover font-medium flex items-center">
                  <svg class="w-3 h-3 mr-1" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                    <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
                  </svg>
                  查看详细分析
                </button>
                <div id="detail-29" class="section-expand collapsed mt-3">
                  <div class="bg-bg-card rounded-lg p-4 text-sm text-text-primary space-y-3 border border-border-color">
                    <p><span class="font-medium text-text-primary">研究背景：</span>现有文献注意到，监督微调(SFT)常使大模型能力收窄，而强化学习(RL)微调却能保留泛化，但仅依赖粗粒度准确率无法解释其差异。作者认为核心原因在于缺乏对“推理”本身的细粒度刻画，因而无法追踪不同训练范式下认知技能的演化。</p>
                    <p><span class="font-medium text-text-primary">方法详情：</span>论文提出一套将推理拆分为计算、事实检索、模拟、枚举与诊断五项原子技能的基准，通过元探测框架在不同训练阶段隔离测量各项技能表现。同时结合分布散度、参数统计等低层指标，对SFT与RL在数学、科学推理及非推理任务上的泛化轨迹进行并行比较。实验追踪行为剖面与参数变化，以揭示能力保持或崩塌的机制。</p>
                    <p><span class="font-medium text-text-primary">研究结果：</span>RL微调模型在全部核心技能上保持更稳定的行为剖面，对表面模式过拟合程度低；SFT模型则出现更剧烈的漂移并过度拟合表层线索，导致部分技能崩塌。低层统计表明，RL维持较小的分布偏移与参数扰动，与行为稳定性高度相关。结果首次在原子技能层面量化了“泛化差异”，为设计保留广泛能力的训练策略提供了可操作原则。</p>
                    <p><span class="font-medium text-text-primary">局限性：</span>实验主要基于开源7B–13B规模模型，尚不清楚结论是否适用于更大规模或闭源系统；原子技能分解虽细，但仍依赖人工标注与任务设计，可能遗漏更复杂的推理组合；对RL奖励设计与SFT数据构成的具体影响未做充分消融。</p>
                    
                    <p><span class="font-medium text-text-primary">未来方向：</span>后续可扩展至更大规模模型和多语言场景，并引入自动化技能发现来减少对人工任务设计的依赖；同时探索奖励函数、数据混合与计算预算的最优配置，以进一步验证并强化RL保持泛化的原则。</p>
                    
                    <p class="text-accent"><span class="font-medium">研究相关性：</span>该文为任何关注LLM后训练、推理能力评估或泛化机制的研究者提供了可复用的原子技能基准与元探测框架，可直接用于比较不同微调策略并解释其成败，为提升模型鲁棒性与广泛适应性提供实证依据。</p>
                  </div>
                </div>
              </div>
            </div>
            

            <div class="pt-4 border-t border-border-color">
              <div class="flex flex-wrap gap-3 text-xs">
                <span class="px-2 py-1 bg-bg-hover rounded text-text-secondary">相似度 0.81</span>
                <span class="px-2 py-1 bg-bg-hover rounded text-text-secondary">
                  期刊影响力 0.60
                  
                </span>
              </div>
            </div>
          </div>
        </article>
        
        <article class="bg-bg-card rounded-lg border border-border-color overflow-hidden hover:border-accent/50 transition-colors">
          <div class="p-4 md:p-5">
            <div class="mb-3">
              <div class="flex items-center gap-2 mb-2 flex-wrap">
                <span class="inline-flex items-center px-2.5 py-0.5 rounded text-xs font-medium
                  bg-amber-100 text-amber-700 border border-amber-300
                  ">
                  推荐
                </span>
                <span class="text-xs text-text-secondary">评分 0.77</span>
                <span class="text-xs text-text-secondary">·</span>
                <span class="text-xs text-text-secondary">crossref</span>
                <span class="text-xs text-text-secondary md:hidden">· 2026-01-01</span>
              </div>
              <div class="flex items-start justify-between gap-3 md:gap-4">
                <div class="flex-1">
                  <h2 class="text-base md:text-lg font-semibold text-text-primary leading-tight">
                    <a href="https://doi.org/10.1109/tgrs.2025.3650151" target="_blank" rel="noopener" class="hover:text-accent transition-colors">
                      Towards Comprehensive Interactive Change Understanding in Remote Sensing: A Large-scale Dataset and Dual-granularity Enhanced VLM
                    </a>
                  </h2>
                  
                  <p class="text-sm text-text-secondary mt-0.5 leading-snug">面向遥感综合交互式变化理解：一个大规模数据集与双粒度增强的VLM</p>
                  
                </div>
                <div class="hidden md:block text-right text-sm text-text-secondary flex-shrink-0">
                  <div class="font-medium">2026-01-01</div>
                </div>
              </div>
              <div class="mt-1 text-xs text-text-secondary" title="IEEE Transactions on Geoscience and Remote Sensing">
                IEEE Transactions on Geoscience and Remote Sensing
                
                  <span class="ml-1 text-blue-600">(IF: 8.6)</span>
                
              </div>
            </div>

            <p class="text-sm text-text-secondary mb-3">
              Junxiao Xue，Quan Deng，Xuecheng Wu，Kelu Yao，Xinyi Yin 等
            </p>

            <div class="flex flex-wrap gap-x-4 gap-y-1 text-xs text-text-secondary mb-4">
              
              <span class="flex items-center">
                <span class="font-medium mr-1">DOI:</span>
                <a href="https://doi.org/10.1109/tgrs.2025.3650151" target="_blank" rel="noopener" class="text-accent hover:text-accent-hover hover:underline">10.1109/tgrs.2025.3650151</a>
              </span>
              
            </div>

            
            <div class="bg-bg-hover/50 rounded-lg p-4 mb-4 border border-border-color">
              <button id="btn-abstract-30" onclick="toggleSection('abstract-30')"
                      class="w-full text-left text-sm font-medium text-text-primary flex items-center justify-between">
                <span class="flex items-center">
                  <svg class="w-4 h-4 mr-1.5 text-text-secondary" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                    <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M9 12h6m-6 4h6m2 5H7a2 2 0 01-2-2V5a2 2 0 012-2h5.586a1 1 0 01.707.293l5.414 5.414a1 1 0 01.293.707V19a2 2 0 01-2 2z"/>
                  </svg>
                  原文摘要
                </span>
                <svg class="w-4 h-4 text-text-secondary transform transition-transform" id="icon-abstract-30" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                  <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
                </svg>
              </button>
              <div id="abstract-30" class="section-expand collapsed">
                <p class="text-sm text-text-secondary mt-3 leading-relaxed">Remote sensing change understanding (RSCU) is essential for analyzing remote sensing images and understanding how human activities affect the environment. However, existing datasets lack deep understanding and interactions in the diverse change captioning, counting, and localization tasks. To tackle these gaps, we construct ChangeIMTI, a new large-scale interactive multi-task instruction dataset that encompasses four complementary tasks including change captioning, binary change classification, change counting, and change localization. Building upon this new dataset, we further design a novel vision-guided vision-language model (ChangeVG) with dual-granularity awareness for bi-temporal remote sensing images (i.e., two remote sensing images of the same area at different times). The introduced visionguided module is a dual-branch architecture that synergistically combines fine-grained spatial feature extraction with high-level semantic summarization. These enriched representations further serve as the auxiliary prompts to guide large vision-language models (VLMs) (e.g., Qwen2.5-VL-7B) during instruction tuning, thereby facilitating the hierarchical cross-modal learning. We extensively conduct experiments across four tasks to demonstrate the superiority of our approach. Remarkably, on the change captioning task, our method outperforms the strongest method Semantic-CC by 1.39 points on the comprehensive S∗m metric, which integrates the semantic similarity and descriptive accuracy to provide an overall evaluation of change caption. Moreover, we also perform a series of ablation studies to examine the critical components of our method. The source code and associated data for this work are publicly available at Github.</p>
              </div>
            </div>
            

            
            <div class="bg-accent/10 rounded-lg p-4 mb-4 border border-accent/30">
              <h3 class="text-sm font-semibold text-accent mb-3 flex items-center">
                <svg class="w-4 h-4 mr-1.5" fill="currentColor" viewBox="0 0 24 24">
                  <path d="M12 2a2 2 0 012 2c0 .74-.4 1.39-1 1.73V7h1a7 7 0 017 7h1a1 1 0 011 1v3a1 1 0 01-1 1h-1v1a2 2 0 01-2 2H5a2 2 0 01-2-2v-1H2a1 1 0 01-1-1v-3a1 1 0 011-1h1a7 7 0 017-7h1V5.73c-.6-.34-1-.99-1-1.73a2 2 0 012-2z"/>
                </svg>
                AI 总结
              </h3>
              <div class="space-y-2 text-sm text-text-primary">
                <p><span class="font-medium text-accent">研究问题：</span>现有遥感变化理解数据集缺乏对变化描述、计数与定位的深度交互式理解。</p>
                <p><span class="font-medium text-accent">研究方法：</span>构建ChangeIMTI多任务指令数据集，提出双粒度视觉引导VLM ChangeVG。</p>
                <p><span class="font-medium text-accent">主要发现：</span>ChangeVG在变化描述任务S∗m指标上领先最强基线1.39分，四任务全面优越。</p>
                <p><span class="font-medium text-accent">创新点：</span>首次提出大规模交互式多任务RSCU数据集与双粒度视觉引导协同VLM框架。</p>
                
                <p><span class="font-medium text-accent">相关性：</span>为遥感变化分析提供统一评测基准与强泛化模型，推动环境监测与决策应用。</p>
                
              </div>

              <div class="mt-3 pt-3 border-t border-accent/30">
                <button id="btn-detail-30" onclick="toggleSection('detail-30')"
                        class="text-xs text-accent hover:text-accent-hover font-medium flex items-center">
                  <svg class="w-3 h-3 mr-1" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                    <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
                  </svg>
                  查看详细分析
                </button>
                <div id="detail-30" class="section-expand collapsed mt-3">
                  <div class="bg-bg-card rounded-lg p-4 text-sm text-text-primary space-y-3 border border-border-color">
                    <p><span class="font-medium text-text-primary">研究背景：</span>遥感变化理解(RSCU)是监测人类活动对环境影响的关键，但现有数据集在变化描述、计数与定位等多任务交互上缺乏深度。为此，作者提出大规模多任务指令数据集ChangeIMTI，并设计双粒度增强的VLM框架ChangeVG，以提升对双时相影像的综合理解。</p>
                    <p><span class="font-medium text-text-primary">方法详情：</span>ChangeIMTI 包含变化描述、二分类、计数与定位四类互补任务，共提供大规模指令-响应对。ChangeVG 采用双分支视觉引导模块：一支提取细粒度空间特征，另一支生成高层语义摘要，二者融合后作为辅助提示输入 Qwen2.5-VL-7B 进行指令微调，实现跨模态分层学习。训练时，模型同时优化四个任务的损失，使视觉特征在不同粒度上与语言语义对齐。</p>
                    <p><span class="font-medium text-text-primary">研究结果：</span>在变化描述任务上，ChangeVG 在综合指标 S*m(语义相似度+描述准确度) 比最强基线 Semantic-CC 提升 1.39 分；在分类、计数与定位任务中也均取得最佳或次佳结果。消融实验表明，双粒度视觉引导模块分别贡献约 0.8 与 0.6 个 S*m 点的增益，验证了各组件的必要性。</p>
                    <p><span class="font-medium text-text-primary">局限性：</span>数据集目前仅覆盖可见光影像，缺乏多光谱、SAR 等模态；双分支设计带来额外参数量与推理延迟，对实时应用仍存挑战；评估指标 S*m 虽综合，但对微小变化的空间定位精度敏感度不足。</p>
                    
                    <p><span class="font-medium text-text-primary">未来方向：</span>未来可扩展至多光谱与 SAR 数据，并引入时序连续影像以支持长期变化推理；探索轻量化双粒度融合策略，实现边缘端实时部署。</p>
                    
                    <p class="text-accent"><span class="font-medium">研究相关性：</span>若研究者关注遥感变化检测、多模态 VLM 或指令调优，该文提供的大规模指令数据集与双粒度视觉引导范式可直接作为基准与灵感来源。</p>
                  </div>
                </div>
              </div>
            </div>
            

            <div class="pt-4 border-t border-border-color">
              <div class="flex flex-wrap gap-3 text-xs">
                <span class="px-2 py-1 bg-bg-hover rounded text-text-secondary">相似度 0.78</span>
                <span class="px-2 py-1 bg-bg-hover rounded text-text-secondary">
                  期刊影响力 0.70
                  
                    <span class="ml-1 text-blue-600">(IF: 8.6)</span>
                  
                </span>
              </div>
            </div>
          </div>
        </article>
        
      </div>
    </div>
  </main>

  <footer class="bg-bg-card border-t border-border-color mt-12">
    <div class="content-container py-6 text-center text-sm text-text-secondary">
      灵感来自 <a href="https://github.com/Yorks0n/ZotWatch" class="text-accent hover:text-accent-hover transition-colors">ZotWatch</a>
    </div>
  </footer>

  <script>
    function toggleSection(id) {
      const el = document.getElementById(id);
      const btn = document.getElementById('btn-' + id);
      const icon = document.getElementById('icon-' + id);
      const hint = document.getElementById('hint-' + id);
      if (el.classList.contains('collapsed')) {
        el.classList.remove('collapsed');
        el.classList.add('expanded');
        if (btn && btn.textContent.includes('详细分析')) {
          btn.innerHTML = '<svg class="w-3 h-3 mr-1" fill="none" stroke="currentColor" viewBox="0 0 24 24"><path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M5 15l7-7 7 7"/></svg>收起详细分析';
        }
        if (hint) hint.textContent = '点击收起';
        if (icon) icon.style.transform = 'rotate(180deg)';
        if (id === 'researcher-profile') {
          setTimeout(() => { window.dispatchEvent(new Event('resize')); }, 100);
        }
      } else {
        el.classList.remove('expanded');
        el.classList.add('collapsed');
        if (btn && btn.textContent.includes('详细分析')) {
          btn.innerHTML = '<svg class="w-3 h-3 mr-1" fill="none" stroke="currentColor" viewBox="0 0 24 24"><path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/></svg>查看详细分析';
        }
        if (hint) hint.textContent = '点击展开';
        if (icon) icon.style.transform = 'rotate(0deg)';
      }
    }
  </script>
</body>
</html>