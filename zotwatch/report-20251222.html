<!DOCTYPE html>
<html lang="zh-CN">
<head>
  <meta charset="utf-8">
  <meta name="viewport" content="width=device-width, initial-scale=1">
  <title>ZotWatch 文献推荐 - 2025-12-22</title>
  <script src="https://cdn.tailwindcss.com"></script>
  <script src="https://cdn.jsdelivr.net/npm/chart.js@4.4.1/dist/chart.umd.min.js"></script>
  <script src="https://d3js.org/d3.v7.min.js"></script>
  <script>
    tailwind.config = {
      theme: {
        extend: {
          colors: {
            'bg-primary': '#f8fafc',
            'bg-card': '#ffffff',
            'bg-hover': '#f1f5f9',
            'text-primary': '#1e293b',
            'text-secondary': '#64748b',
            'border-color': '#e2e8f0',
            'accent': '#2563eb',
            'accent-hover': '#1d4ed8',
          }
        }
      }
    }
  </script>
  <style>
    .section-expand { transition: max-height 0.5s ease-out; overflow: hidden; }
    .section-expand.collapsed { max-height: 0; }
    .section-expand.expanded { max-height: none; }
    body { font-family: -apple-system, BlinkMacSystemFont, 'Segoe UI', Roboto, 'Helvetica Neue', Arial, sans-serif; }
    .content-container { max-width: 48rem; margin: 0 auto; padding: 0 1rem; }
    @media (min-width: 768px) { .content-container { padding: 0 2rem; } }
    @media (min-width: 1024px) { .content-container { padding: 0 4rem; } }

    .cluster-graph-container { width: 100%; height: 280px; touch-action: pan-y; }
    @media (min-width: 640px) { .cluster-graph-container { height: 320px; } }
    @media (min-width: 768px) { .cluster-graph-container { height: 350px; } }
    #cluster-graph svg { overflow: visible; }
  </style>
</head>
<body class="bg-bg-primary min-h-screen text-text-primary">
  <header class="bg-bg-card border-b border-border-color">
    <div class="content-container py-6 md:py-8">
      <h1 class="text-xl md:text-2xl font-bold text-text-primary">ZotWatch 文献推荐</h1>
      <p class="text-xs md:text-sm text-text-secondary mt-1">
        共 35 篇论文 ·
        生成于 2025-12-22 10:46 Asia/Shanghai
      </p>
    </div>
  </header>

  
  <section class="py-4 md:py-6 border-b border-border-color">
    <div class="content-container">
      <button id="btn-researcher-profile" onclick="toggleSection('researcher-profile')"
              class="w-full text-left text-lg md:text-xl font-bold text-text-primary mb-3 md:mb-4 flex items-center justify-between group hover:text-accent transition-colors">
        <span class="flex items-center gap-2 flex-1 min-w-0">
          <svg class="w-5 h-5 flex-shrink-0 text-accent" fill="currentColor" viewBox="0 0 24 24">
            <path d="M12 2C6.48 2 2 6.48 2 12s4.48 10 10 10 10-4.48 10-10S17.52 2 12 2zm0 3c1.66 0 3 1.34 3 3s-1.34 3-3 3-3-1.34-3-3 1.34-3 3-3zm0 14.2c-2.5 0-4.71-1.28-6-3.22.03-1.99 4-3.08 6-3.08 1.99 0 5.97 1.09 6 3.08-1.29 1.94-3.5 3.22-6 3.22z"/>
          </svg>
          <span class="truncate">研究兴趣画像</span>
        </span>
        <span class="flex items-center gap-1 md:gap-2 flex-shrink-0 ml-2">
          <span class="hidden md:inline text-sm font-normal text-text-secondary group-hover:text-accent" id="hint-researcher-profile">点击展开</span>
          <svg class="w-5 h-5 md:w-4 md:h-4 text-text-secondary group-hover:text-accent transform transition-transform" id="icon-researcher-profile" fill="none" stroke="currentColor" viewBox="0 0 24 24">
            <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
          </svg>
        </span>
      </button>

      <div id="researcher-profile" class="section-expand collapsed">
      <div class="bg-bg-card rounded-lg border border-border-color p-4 mb-4">
        <div class="grid grid-cols-4 gap-3 text-center">
          <div>
            <div class="text-xl font-bold text-accent">935</div>
            <div class="text-xs text-text-secondary">收藏论文</div>
          </div>
          <div>
            <div class="text-xl font-bold text-accent">5年8月</div>
            <div class="text-xs text-text-secondary">收藏时长</div>
          </div>
          <div>
            <div class="text-xl font-bold text-accent">&gt;10</div>
            <div class="text-xs text-text-secondary">兴趣领域</div>
          </div>
          <div>
            <div class="text-xl font-bold text-accent">8</div>
            <div class="text-xs text-text-secondary">高频作者</div>
          </div>
        </div>
      </div>

      
      <div class="bg-accent/10 rounded-lg border border-accent/30 p-4 mb-4">
        <h3 class="text-sm font-semibold text-accent mb-2 flex items-center gap-2">
          <svg class="w-4 h-4" fill="currentColor" viewBox="0 0 24 24">
            <path d="M12 2a2 2 0 012 2c0 .74-.4 1.39-1 1.73V7h1a7 7 0 017 7h1a1 1 0 011 1v3a1 1 0 01-1 1h-1v1a2 2 0 01-2 2H5a2 2 0 01-2-2v-1H2a1 1 0 01-1-1v-3a1 1 0 011-1h1a7 7 0 017-7h1V5.73c-.6-.34-1-.99-1-1.73a2 2 0 012-2z"/>
          </svg>
          AI 研究兴趣分析
        </h3>
        <div class="space-y-2 text-xs text-text-primary leading-relaxed">
          <p><span class="font-medium text-accent">研究兴趣：</span>用户长期关注计算机视觉中的目标检测与定位，同时兼顾模型压缩与高效推理，体现出对视觉感知算法性能与部署并重的研究取向。</p>
          <p><span class="font-medium text-accent">深度关注：</span>在目标检测、视觉定位及模型压缩方向持续收藏高水平会议论文，累计阅读近百篇，形成从基础方法（R-CNN系列、HRNet）到轻量化部署（Song Han系列工作）的完整知识链条。</p>
          <p><span class="font-medium text-accent">跨学科倾向：</span>收藏曲线显示其将遥感领域合成孔径雷达（SAR）图像与视觉算法结合，体现出利用计算机视觉技术解决遥感智能解译的跨学科阅读特征。</p>
          <p><span class="font-medium text-accent">兴趣演变：</span>2024-2025年收藏量显著回升且新增关键词聚焦视觉Transformer、SAR图像描述与循环对抗网络，表明正将基础模型范式引入遥感任务并关注生成式方法。</p>
          <p><span class="font-medium text-accent">阅读建议：</span>可进一步关注多模态大模型在遥感-视觉融合中的开集识别与细粒度描述，以及面向边缘设备的ViT压缩与部署研究。</p>
        </div>
      </div>
      

      <div class="grid grid-cols-1 md:grid-cols-2 gap-4 mb-4">
        
        <div class="bg-bg-card rounded-lg border border-border-color p-4">
          <h4 class="text-sm font-medium text-text-primary mb-3">研究领域分布</h4>
          <div style="height: 180px;">
            <canvas id="domainsChart"></canvas>
          </div>
        </div>
        

        
        <div class="bg-bg-card rounded-lg border border-border-color p-4">
          <h4 class="text-sm font-medium text-text-primary mb-3">阅读趋势（季度）</h4>
          <div style="height: 180px;">
            <canvas id="trendsChart"></canvas>
          </div>
        </div>
        
      </div>

      
      <div class="bg-bg-card rounded-lg border border-border-color p-4 mb-4">
        <h4 class="text-sm font-medium text-text-primary mb-3">论文发表年份分布</h4>
        <div style="height: 180px;">
          <canvas id="yearDistChart"></canvas>
        </div>
      </div>
      

      
      <div class="bg-bg-card rounded-lg border border-border-color p-4 mb-4">
        <h4 class="text-sm font-medium text-text-primary mb-3">
          研究方向聚类图谱
          <span class="text-xs text-text-secondary font-normal ml-2">(30 个聚类)</span>
        </h4>
        <div id="cluster-graph" class="cluster-graph-container"></div>
        <div class="text-xs text-text-secondary mt-3">
          覆盖 911/911 篇论文（仅含摘要的文献参与聚类）
        </div>
      </div>
      

      <div class="grid grid-cols-1 md:grid-cols-2 gap-4 mb-4">
        
        <div class="bg-bg-card rounded-lg border border-border-color p-4">
          <h4 class="text-sm font-medium text-text-primary mb-2">高频关注作者</h4>
          <div class="grid grid-cols-2 gap-x-4 gap-y-1 text-xs">
            
            <div class="flex justify-between truncate">
              <span class="text-text-primary truncate" title="Kaiming He">Kaiming He</span>
              <span class="text-text-secondary ml-1 flex-shrink-0">24</span>
            </div>
            
            <div class="flex justify-between truncate">
              <span class="text-text-primary truncate" title="Ross Girshick">Ross Girshick</span>
              <span class="text-text-secondary ml-1 flex-shrink-0">18</span>
            </div>
            
            <div class="flex justify-between truncate">
              <span class="text-text-primary truncate" title="Jian Sun">Jian Sun</span>
              <span class="text-text-secondary ml-1 flex-shrink-0">15</span>
            </div>
            
            <div class="flex justify-between truncate">
              <span class="text-text-primary truncate" title="Song Han">Song Han</span>
              <span class="text-text-secondary ml-1 flex-shrink-0">14</span>
            </div>
            
            <div class="flex justify-between truncate">
              <span class="text-text-primary truncate" title="Xiangyu Zhang">Xiangyu Zhang</span>
              <span class="text-text-secondary ml-1 flex-shrink-0">14</span>
            </div>
            
            <div class="flex justify-between truncate">
              <span class="text-text-primary truncate" title="Piotr Dollár">Piotr Dollár</span>
              <span class="text-text-secondary ml-1 flex-shrink-0">12</span>
            </div>
            
            <div class="flex justify-between truncate">
              <span class="text-text-primary truncate" title="Li Liu">Li Liu</span>
              <span class="text-text-secondary ml-1 flex-shrink-0">10</span>
            </div>
            
            <div class="flex justify-between truncate">
              <span class="text-text-primary truncate" title="Xiang Li">Xiang Li</span>
              <span class="text-text-secondary ml-1 flex-shrink-0">10</span>
            </div>
            
            <div class="flex justify-between truncate">
              <span class="text-text-primary truncate" title="Yann LeCun">Yann LeCun</span>
              <span class="text-text-secondary ml-1 flex-shrink-0">9</span>
            </div>
            
            <div class="flex justify-between truncate">
              <span class="text-text-primary truncate" title="Chunhua Shen">Chunhua Shen</span>
              <span class="text-text-secondary ml-1 flex-shrink-0">8</span>
            </div>
            
            <div class="flex justify-between truncate">
              <span class="text-text-primary truncate" title="Saining Xie">Saining Xie</span>
              <span class="text-text-secondary ml-1 flex-shrink-0">8</span>
            </div>
            
            <div class="flex justify-between truncate">
              <span class="text-text-primary truncate" title="Gangyao Kuang">Gangyao Kuang</span>
              <span class="text-text-secondary ml-1 flex-shrink-0">8</span>
            </div>
            
          </div>
        </div>
        

        
        <div class="bg-bg-card rounded-lg border border-border-color p-4">
          <h4 class="text-sm font-medium text-text-primary mb-2">常关注期刊/会议</h4>
          <div class="space-y-1 text-xs">
            
            <div class="flex items-center justify-between">
              <span class="text-text-primary truncate flex-1" title="Computer Vision and Pattern Recognition">Computer Vision and Pattern Recognition</span>
              <span class="text-xs px-1 py-0.5 rounded bg-blue-100 text-blue-700 ml-1 flex-shrink-0">
                期刊
              </span>
              <span class="text-text-secondary ml-1 flex-shrink-0">114</span>
            </div>
            
            <div class="flex items-center justify-between">
              <span class="text-text-primary truncate flex-1" title="Neural Information Processing Systems">Neural Information Processing Systems</span>
              <span class="text-xs px-1 py-0.5 rounded bg-blue-100 text-blue-700 ml-1 flex-shrink-0">
                期刊
              </span>
              <span class="text-text-secondary ml-1 flex-shrink-0">45</span>
            </div>
            
            <div class="flex items-center justify-between">
              <span class="text-text-primary truncate flex-1" title="IEEE Transactions on Geoscience and Remote Sensing">IEEE Transactions on Geoscience and Remote Sensing</span>
              <span class="text-xs px-1 py-0.5 rounded bg-blue-100 text-blue-700 ml-1 flex-shrink-0">
                期刊
              </span>
              <span class="text-text-secondary ml-1 flex-shrink-0">43</span>
            </div>
            
            <div class="flex items-center justify-between">
              <span class="text-text-primary truncate flex-1" title="International Conference on Computer Vision">International Conference on Computer Vision</span>
              <span class="text-xs px-1 py-0.5 rounded bg-amber-100 text-amber-700 ml-1 flex-shrink-0">
                会议
              </span>
              <span class="text-text-secondary ml-1 flex-shrink-0">36</span>
            </div>
            
            <div class="flex items-center justify-between">
              <span class="text-text-primary truncate flex-1" title="IEEE Transactions on Pattern Analysis and Machine Intelligence">IEEE Transactions on Pattern Analysis and Machine Intelligence</span>
              <span class="text-xs px-1 py-0.5 rounded bg-blue-100 text-blue-700 ml-1 flex-shrink-0">
                期刊
              </span>
              <span class="text-text-secondary ml-1 flex-shrink-0">32</span>
            </div>
            
            <div class="flex items-center justify-between">
              <span class="text-text-primary truncate flex-1" title="International Conference on Learning Representations">International Conference on Learning Representations</span>
              <span class="text-xs px-1 py-0.5 rounded bg-amber-100 text-amber-700 ml-1 flex-shrink-0">
                会议
              </span>
              <span class="text-text-secondary ml-1 flex-shrink-0">27</span>
            </div>
            
            <div class="flex items-center justify-between">
              <span class="text-text-primary truncate flex-1" title="European Conference on Computer Vision">European Conference on Computer Vision</span>
              <span class="text-xs px-1 py-0.5 rounded bg-amber-100 text-amber-700 ml-1 flex-shrink-0">
                会议
              </span>
              <span class="text-text-secondary ml-1 flex-shrink-0">23</span>
            </div>
            
            <div class="flex items-center justify-between">
              <span class="text-text-primary truncate flex-1" title="IEEE Geoscience and Remote Sensing Letters">IEEE Geoscience and Remote Sensing Letters</span>
              <span class="text-xs px-1 py-0.5 rounded bg-blue-100 text-blue-700 ml-1 flex-shrink-0">
                期刊
              </span>
              <span class="text-text-secondary ml-1 flex-shrink-0">21</span>
            </div>
            
          </div>
        </div>
        
      </div>

      
      <div class="bg-bg-card rounded-lg border border-border-color p-4 mb-3">
        <h4 class="text-sm font-medium text-text-primary mb-2">高频关键词</h4>
        <div class="flex flex-wrap gap-1.5">
          
          <span class="px-2 py-0.5 bg-bg-hover rounded text-xs text-text-primary">
            大语言模型 <span class="text-text-secondary">(13)</span>
          </span>
          
          <span class="px-2 py-0.5 bg-bg-hover rounded text-xs text-text-primary">
            自监督学习 <span class="text-text-secondary">(11)</span>
          </span>
          
          <span class="px-2 py-0.5 bg-bg-hover rounded text-xs text-text-primary">
            综述 <span class="text-text-secondary">(9)</span>
          </span>
          
          <span class="px-2 py-0.5 bg-bg-hover rounded text-xs text-text-primary">
            域自适应 <span class="text-text-secondary">(9)</span>
          </span>
          
          <span class="px-2 py-0.5 bg-bg-hover rounded text-xs text-text-primary">
            合成孔径雷达 <span class="text-text-secondary">(9)</span>
          </span>
          
          <span class="px-2 py-0.5 bg-bg-hover rounded text-xs text-text-primary">
            迁移学习 <span class="text-text-secondary">(7)</span>
          </span>
          
          <span class="px-2 py-0.5 bg-bg-hover rounded text-xs text-text-primary">
            扩散模型 <span class="text-text-secondary">(7)</span>
          </span>
          
          <span class="px-2 py-0.5 bg-bg-hover rounded text-xs text-text-primary">
            强化学习 <span class="text-text-secondary">(7)</span>
          </span>
          
          <span class="px-2 py-0.5 bg-bg-hover rounded text-xs text-text-primary">
            注意力机制 <span class="text-text-secondary">(6)</span>
          </span>
          
          <span class="px-2 py-0.5 bg-bg-hover rounded text-xs text-text-primary">
            重参数化 <span class="text-text-secondary">(6)</span>
          </span>
          
          <span class="px-2 py-0.5 bg-bg-hover rounded text-xs text-text-primary">
            DeepSeek <span class="text-text-secondary">(6)</span>
          </span>
          
          <span class="px-2 py-0.5 bg-bg-hover rounded text-xs text-text-primary">
            生成对抗网络 <span class="text-text-secondary">(6)</span>
          </span>
          
          <span class="px-2 py-0.5 bg-bg-hover rounded text-xs text-text-primary">
            Transformers <span class="text-text-secondary">(5)</span>
          </span>
          
          <span class="px-2 py-0.5 bg-bg-hover rounded text-xs text-text-primary">
            旋转目标检测 <span class="text-text-secondary">(5)</span>
          </span>
          
          <span class="px-2 py-0.5 bg-bg-hover rounded text-xs text-text-primary">
            SAR目标识别 <span class="text-text-secondary">(5)</span>
          </span>
          
          <span class="px-2 py-0.5 bg-bg-hover rounded text-xs text-text-primary">
            特征可视化 <span class="text-text-secondary">(5)</span>
          </span>
          
          <span class="px-2 py-0.5 bg-bg-hover rounded text-xs text-text-primary">
            Vision Transformers <span class="text-text-secondary">(4)</span>
          </span>
          
          <span class="px-2 py-0.5 bg-bg-hover rounded text-xs text-text-primary">
            基础模型 <span class="text-text-secondary">(4)</span>
          </span>
          
          <span class="px-2 py-0.5 bg-bg-hover rounded text-xs text-text-primary">
            HRNet <span class="text-text-secondary">(4)</span>
          </span>
          
          <span class="px-2 py-0.5 bg-bg-hover rounded text-xs text-text-primary">
            目标检测 <span class="text-text-secondary">(4)</span>
          </span>
          
        </div>
      </div>
      

      <div class="text-xs text-text-secondary text-right">
        画像生成于 2025-12-22 10:34 Asia/Shanghai
        · kimi-k2-turbo-preview
      </div>
      </div>
    </div>
  </section>
  

  
  <script>
    document.addEventListener('DOMContentLoaded', function() {
      Chart.defaults.font.family = "-apple-system, BlinkMacSystemFont, 'Segoe UI', Roboto, 'Helvetica Neue', Arial, sans-serif";
      Chart.defaults.font.size = 11;
      Chart.defaults.color = '#64748b';
      Chart.defaults.animation = false;

      
      const domainsCtx = document.getElementById('domainsChart');
      if (domainsCtx) {
        new Chart(domainsCtx, {
          type: 'bar',
          data: {
            labels: ['目标检测', '视觉定位', '模型压缩', '姿态估计', '对比学习', '人脸识别', '车牌识别', '卫星导航'],
            datasets: [{
              data: [42, 28, 25, 18, 12, 12, 8, 6],
              backgroundColor: 'rgba(37, 99, 235, 0.7)',
              borderColor: 'rgba(37, 99, 235, 1)',
              borderWidth: 1,
              borderRadius: 2,
              barThickness: 16
            }]
          },
          options: {
            indexAxis: 'y',
            responsive: true,
            maintainAspectRatio: false,
            plugins: {
              legend: { display: false },
              tooltip: { enabled: false }
            },
            scales: {
              x: {
                beginAtZero: true,
                grid: { color: 'rgba(226, 232, 240, 0.5)' },
                ticks: { precision: 0 }
              },
              y: {
                grid: { display: false },
                ticks: { font: { size: 10 } }
              }
            }
          }
        });
      }
      

      
      const trendsCtx = document.getElementById('trendsChart');
      if (trendsCtx) {
        const trends = [{ q: '2023-Q1', c: 51 }, { q: '2023-Q2', c: 18 }, { q: '2023-Q3', c: 21 }, { q: '2023-Q4', c: 19 }, { q: '2024-Q1', c: 67 }, { q: '2024-Q2', c: 18 }, { q: '2024-Q3', c: 4 }, { q: '2024-Q4', c: 23 }, { q: '2025-Q1', c: 88 }, { q: '2025-Q2', c: 34 }, { q: '2025-Q3', c: 11 }, { q: '2025-Q4', c: 29 }];
        const monthColors = [
          { bg: 'rgba(59, 130, 246, 0.7)', border: 'rgba(59, 130, 246, 1)' },
          { bg: 'rgba(16, 185, 129, 0.7)', border: 'rgba(16, 185, 129, 1)' },
          { bg: 'rgba(245, 158, 11, 0.7)', border: 'rgba(245, 158, 11, 1)' }
        ];
        const bgColors = trends.map((t, i) => monthColors[i % 3].bg);
        const borderColors = trends.map((t, i) => monthColors[i % 3].border);
        new Chart(trendsCtx, {
          type: 'bar',
          data: {
            labels: trends.map(t => t.q.replace('-', '\n')),
            datasets: [{
              data: trends.map(t => t.c),
              backgroundColor: bgColors,
              borderColor: borderColors,
              borderWidth: 1,
              borderRadius: 2,
              barThickness: 20
            }]
          },
          options: {
            responsive: true,
            maintainAspectRatio: false,
            plugins: {
              legend: { display: false },
              tooltip: { enabled: false }
            },
            scales: {
              x: {
                grid: { display: false },
                ticks: { font: { size: 9 }, maxRotation: 0 }
              },
              y: {
                beginAtZero: true,
                grid: { color: 'rgba(226, 232, 240, 0.5)' },
                ticks: { precision: 0 }
              }
            }
          }
        });
      }
      

      
      const yearDistCtx = document.getElementById('yearDistChart');
      if (yearDistCtx) {
        const yearData = [{ year: 2006, count: 8 }, { year: 2007, count: 3 }, { year: 2008, count: 2 }, { year: 2009, count: 2 }, { year: 2010, count: 9 }, { year: 2011, count: 10 }, { year: 2012, count: 3 }, { year: 2013, count: 12 }, { year: 2014, count: 14 }, { year: 2015, count: 18 }, { year: 2016, count: 15 }, { year: 2017, count: 41 }, { year: 2018, count: 59 }, { year: 2019, count: 53 }, { year: 2020, count: 66 }, { year: 2021, count: 84 }, { year: 2022, count: 110 }, { year: 2023, count: 109 }, { year: 2024, count: 112 }, { year: 2025, count: 162 }];
        new Chart(yearDistCtx, {
          type: 'line',
          data: {
            labels: yearData.map(d => d.year),
            datasets: [{
              data: yearData.map(d => d.count),
              borderColor: 'rgba(37, 99, 235, 1)',
              backgroundColor: 'rgba(37, 99, 235, 0.1)',
              borderWidth: 2,
              fill: true,
              tension: 0.3,
              pointRadius: 2,
              pointHoverRadius: 4
            }]
          },
          options: {
            responsive: true,
            maintainAspectRatio: false,
            plugins: {
              legend: { display: false },
              tooltip: {
                callbacks: {
                  label: function(context) {
                    return context.parsed.y + ' 篇';
                  }
                }
              }
            },
            scales: {
              x: {
                grid: { display: false },
                ticks: {
                  font: { size: 9 },
                  maxRotation: 45,
                  callback: function(value, index) {
                    const year = this.getLabelForValue(value);
                    return year % 5 === 0 ? year : '';
                  }
                }
              },
              y: {
                beginAtZero: true,
                grid: { color: 'rgba(226, 232, 240, 0.5)' },
                ticks: { precision: 0 }
              }
            }
          }
        });
      }
      

      
      const clusterContainer = document.getElementById('cluster-graph');
      if (clusterContainer) {
        const clusters = [
          
          {
            id: 0,
            label: "SAR\u57df\u81ea\u9002\u5e94\u8bc6\u522b",
            size: 73,
            keywords: ["SAR\u76ee\u6807\u8bc6\u522b", "\u57df\u81ea\u9002\u5e94", "\u8fc1\u79fb\u5b66\u4e60"]
          },
          
          {
            id: 1,
            label: "\u81ea\u76d1\u7763\u89c6\u89c9\u8868\u5f81",
            size: 62,
            keywords: ["\u81ea\u76d1\u7763\u5b66\u4e60", "\u89c6\u89c9Transformer", "\u5bf9\u6bd4\u5b66\u4e60"]
          },
          
          {
            id: 2,
            label: "Transformer\u76ee\u6807\u68c0\u6d4b",
            size: 55,
            keywords: ["DETR", "\u5b9e\u65f6\u76ee\u6807\u68c0\u6d4b", "\u65cb\u8f6c\u76ee\u6807\u68c0\u6d4b"]
          },
          
          {
            id: 3,
            label: "SAR\u8230\u8239\u68c0\u6d4b",
            size: 53,
            keywords: ["\u5408\u6210\u5b54\u5f84\u96f7\u8fbe", "SAR\u8230\u8239\u68c0\u6d4b", "\u591a\u5c3a\u5ea6\u7279\u5f81\u878d\u5408"]
          },
          
          {
            id: 4,
            label: "\u7ea2\u5916\u5c0f\u76ee\u6807\u68c0\u6d4b",
            size: 48,
            keywords: ["\u7ea2\u5916\u5c0f\u76ee\u6807\u68c0\u6d4b", "\u9065\u611f\u76ee\u6807\u68c0\u6d4b", "\u6837\u672c\u81ea\u9002\u5e94\u589e\u6b96"]
          },
          
          {
            id: 5,
            label: "\u8f7b\u91cf\u7ea7\u89c6\u89c9\u67b6\u6784",
            size: 43,
            keywords: ["\u91cd\u53c2\u6570\u5316", "VGG", "Swin Transformer"]
          },
          
          {
            id: 6,
            label: "\u5927\u8bed\u8a00\u6a21\u578bRL",
            size: 42,
            keywords: ["\u5927\u8bed\u8a00\u6a21\u578b", "DeepSeek", "\u6df7\u5408\u4e13\u5bb6\u6a21\u578b"]
          },
          
          {
            id: 7,
            label: "\u6a21\u578b\u91cf\u5316\u538b\u7f29",
            size: 40,
            keywords: ["\u6a21\u578b\u538b\u7f29", "\u6a21\u578b\u91cf\u5316", "\u6574\u6570\u63a8\u7406"]
          },
          
          {
            id: 8,
            label: "2D/3D\u4eba\u4f53\u59ff\u6001\u4f30\u8ba1",
            size: 40,
            keywords: ["HRNet", "Transformers", "\u5308\u7259\u5229\u7b97\u6cd5"]
          },
          
          {
            id: 9,
            label: "\u5f31\u5c0f\u76ee\u6807\u667a\u80fd\u68c0\u6d4b",
            size: 39,
            keywords: ["\u6df1\u5ea6\u5b66\u4e60", "\u5408\u6210\u5b54\u5f84\u96f7\u8fbe", "\u7279\u5f81\u878d\u5408"]
          },
          
          {
            id: 10,
            label: "\u6df1\u5ea6\u7f51\u7edc\u4f18\u5316\u7406\u8bba",
            size: 38,
            keywords: ["\u4eba\u5de5\u795e\u7ecf\u7f51\u7edc", "\u6b8b\u5dee\u8fde\u63a5", "\u6df1\u5ea6\u6b8b\u5dee\u5b66\u4e60"]
          },
          
          {
            id: 11,
            label: "\u6df1\u5ea6\u5b66\u4e60\u57fa\u7840\u7406\u8bba",
            size: 36,
            keywords: ["LaTeX", "\u8bbe\u8ba1\u6a21\u5f0f", "\u7814\u7a76"]
          },
          
          {
            id: 12,
            label: "\u6269\u6563\u751f\u6210\u6a21\u578b",
            size: 35,
            keywords: ["\u6269\u6563\u6a21\u578b", "\u751f\u6210\u5bf9\u6297\u7f51\u7edc", "\u751f\u6210\u6a21\u578b"]
          },
          
          {
            id: 13,
            label: "\u591a\u6a21\u60013D\u611f\u77e5",
            size: 34,
            keywords: ["\u591a\u6a21\u6001", "\u4e09\u7ef4\u611f\u77e5", "\u4f4d\u7f6e\u7f16\u7801"]
          },
          
          {
            id: 14,
            label: "\u901a\u7528\u56fe\u50cf\u5206\u5272",
            size: 33,
            keywords: ["\u56fe\u50cf\u5206\u5272", "\u57fa\u7840\u6a21\u578b", "\u901a\u7528\u5206\u5272"]
          },
          
          {
            id: 15,
            label: "\u591a\u4f20\u611f\u5668\u5168\u5c40\u4f4d\u59ff\u4f30\u8ba1",
            size: 29,
            keywords: []
          },
          
          {
            id: 16,
            label: "\u8f66\u724c\u68c0\u6d4b\u8bc6\u522b",
            size: 27,
            keywords: ["\u8f66\u724c\u8bc6\u522b", "Internet of Things", "Microcontrollers"]
          },
          
          {
            id: 17,
            label: "\u76ee\u6807\u68c0\u6d4b\u7efc\u8ff0\u4e0e\u96f7\u8fbe\u6570\u636e",
            size: 24,
            keywords: ["\u7efc\u8ff0", "SIFT", "\u65cb\u8f6c\u76ee\u6807\u68c0\u6d4b"]
          },
          
          {
            id: 18,
            label: "\u89c6\u89c9-\u8bed\u8a00\u591a\u6a21\u6001\u5efa\u6a21",
            size: 19,
            keywords: ["StepFun", "\u591a\u6a21\u6001\u5b66\u4e60", "Computer Science - Computer Vision and Pattern Recognition"]
          },
          
          {
            id: 19,
            label: "SAR\u6210\u50cf\u7b97\u6cd5\u57fa\u7840",
            size: 18,
            keywords: []
          },
          
          {
            id: 20,
            label: "\u5f3a\u5316\u5b66\u4e60\u7406\u8bba\u4e0e\u7b56\u7565",
            size: 17,
            keywords: ["\u5f3a\u5316\u5b66\u4e60", "\u5927\u8bed\u8a00\u6a21\u578b", "\u7b56\u7565\u4f18\u5316"]
          },
          
          {
            id: 21,
            label: "\u6807\u51c6\u5316\u6d41\u4e0e\u5206\u5e03\u5916\u6cdb\u5316",
            size: 17,
            keywords: ["\u6807\u51c6\u5316\u6d41", "\u6d41\u6a21\u578b", "\u5206\u5e03\u5916\u6cdb\u5316"]
          },
          
          {
            id: 22,
            label: "\u9ad8\u6548\u6ce8\u610f\u529b\u673a\u5236",
            size: 16,
            keywords: ["\u7efc\u8ff0", "Mamba", "\u65cb\u8f6c\u4f4d\u7f6e\u7f16\u7801"]
          },
          
          {
            id: 23,
            label: "\u5927\u89c4\u6a21LLM\u7cfb\u7edf\u8bad\u7ec3",
            size: 12,
            keywords: ["\u7cfb\u7edf\u4f18\u5316", "\u5206\u5e03\u5f0f\u8bad\u7ec3", "\u6df1\u5ea6\u5b66\u4e60\u6846\u67b6"]
          },
          
          {
            id: 24,
            label: "\u6df1\u5ea6\u6a21\u578b\u53ef\u89e3\u91ca\u53ef\u89c6\u5316",
            size: 12,
            keywords: ["\u7279\u5f81\u53ef\u89c6\u5316", "Grad-CAM", "\u5206\u5e03\u5916\u68c0\u6d4b"]
          },
          
          {
            id: 25,
            label: "SAR\u91cf\u5316\u5bf9\u68c0\u6d4b\u5f71\u54cd",
            size: 11,
            keywords: []
          },
          
          {
            id: 26,
            label: "\u7279\u5f81\u53ef\u89c6\u5316\u4e0e\u9891\u57df\u5206\u6790",
            size: 10,
            keywords: ["\u6838\u65b9\u6cd5", "\u7279\u5f81\u6620\u5c04", "\u7f51\u7edc\u8bbe\u8ba1"]
          },
          
          {
            id: 27,
            label: "\u56fe\u50cf\u7ffb\u8bd1\u4e0e\u98ce\u683c\u8fc1\u79fb",
            size: 10,
            keywords: ["\u5b9e\u65f6\u56fe\u50cf\u5904\u7406", "\u795e\u7ecf\u98ce\u683c\u8fc1\u79fb", "\u81ea\u9002\u5e94\u5b9e\u4f8b\u5f52\u4e00\u5316"]
          },
          
          {
            id: 28,
            label: "\u7ea2\u5916\u56fe\u50cf\u53bb\u96fe\u4e0e\u8d28\u91cf\u8bc4\u4f30",
            size: 9,
            keywords: []
          },
          
          {
            id: 29,
            label: "IR-UWB\u96f7\u8fbe\u751f\u547d\u63a2\u6d4b",
            size: 9,
            keywords: ["\u4fe1\u53f7\u63d0\u53d6", "\u547c\u5438\u5fc3\u8df3\u4fe1\u53f7", "\u751f\u547d\u4fe1\u606f\u63a2\u6d4b"]
          }
          
        ];

        const links = [{"source": 6, "target": 18, "value": 0.9045934126966846}, {"source": 5, "target": 7, "value": 0.8703914484516344}, {"source": 22, "target": 23, "value": 0.8766363201023843}, {"source": 5, "target": 10, "value": 0.9013190860508283}, {"source": 2, "target": 5, "value": 0.9191251111836547}, {"source": 8, "target": 15, "value": 0.8594820151273634}, {"source": 2, "target": 8, "value": 0.8846292589300011}, {"source": 1, "target": 12, "value": 0.8854859781560916}, {"source": 1, "target": 18, "value": 0.9185628181938891}, {"source": 11, "target": 20, "value": 0.9003204045930091}, {"source": 13, "target": 14, "value": 0.8788056888501194}, {"source": 2, "target": 17, "value": 0.9147371684880894}, {"source": 10, "target": 21, "value": 0.8853650069909501}, {"source": 9, "target": 29, "value": 0.8666175276502724}, {"source": 7, "target": 10, "value": 0.8670965375717372}, {"source": 24, "target": 26, "value": 0.915884615778759}, {"source": 6, "target": 20, "value": 0.8856250251260628}, {"source": 1, "target": 27, "value": 0.8958878859783994}, {"source": 6, "target": 23, "value": 0.8931839855158819}, {"source": 18, "target": 22, "value": 0.9163287554443063}, {"source": 3, "target": 9, "value": 0.9021556290186215}, {"source": 12, "target": 21, "value": 0.8842421058954266}, {"source": 4, "target": 17, "value": 0.9081709564968259}, {"source": 12, "target": 27, "value": 0.9370509990452458}, {"source": 2, "target": 4, "value": 0.9312903446922292}, {"source": 10, "target": 11, "value": 0.8840876575566718}, {"source": 0, "target": 4, "value": 0.9159797788891353}, {"source": 5, "target": 24, "value": 0.9038739559132674}, {"source": 1, "target": 5, "value": 0.9122995220344681}, {"source": 0, "target": 19, "value": 0.9010784402396183}, {"source": 2, "target": 16, "value": 0.8555416529472137}, {"source": 1, "target": 14, "value": 0.8770089868090906}, {"source": 9, "target": 28, "value": 0.8839254148208979}, {"source": 17, "target": 29, "value": 0.8752018261913804}, {"source": 10, "target": 26, "value": 0.9090629681252768}, {"source": 0, "target": 25, "value": 0.9209238995443362}, {"source": 6, "target": 22, "value": 0.9268106852618302}, {"source": 0, "target": 3, "value": 0.9361123138235875}, {"source": 0, "target": 9, "value": 0.9074609605239291}, {"source": 4, "target": 28, "value": 0.8638392569337213}, {"source": 8, "target": 13, "value": 0.8990100403131291}, {"source": 19, "target": 25, "value": 0.8959541855107529}, {"source": 13, "target": 15, "value": 0.8993450951939496}, {"source": 16, "target": 17, "value": 0.8987962983753435}];

        const academicColors = [
          '#4E79A7', '#F28E2B', '#E15759', '#76B7B2', '#59A14F',
          '#EDC948', '#B07AA1', '#FF9DA7', '#9C755F', '#BAB0AC',
          '#86BCB6', '#D37295', '#8CD17D', '#499894', '#FABFD2',
          '#79706E', '#D4A6C8', '#9D7660', '#B6992D', '#A0CBE8'
        ];
        const colorScale = (i) => academicColors[i % academicColors.length];

        let svg, simulation, node, link, sizeScale;
        let lastWidth = 0;
        let simulationStopped = false;

        function createGraph() {
          const width = clusterContainer.clientWidth;
          const height = clusterContainer.clientHeight || 280;
          const isMobile = width < 500;

          lastWidth = width;
          clusterContainer.innerHTML = '';

          const minRadius = isMobile ? 16 : 20;
          const maxRadius = isMobile ? 38 : 48;
          sizeScale = d3.scaleSqrt()
            .domain([0, d3.max(clusters, d => d.size)])
            .range([minRadius, maxRadius]);

          svg = d3.select('#cluster-graph')
            .append('svg')
            .attr('width', width)
            .attr('height', height)
            .attr('viewBox', [0, 0, width, height]);

          // Create layered groups for z-order control
          // Order (bottom to top): dimmed links -> dimmed nodes -> highlighted links -> highlighted nodes
          const layerDimmedLinks = svg.append('g').attr('class', 'layer-dimmed-links');
          const layerDimmedNodes = svg.append('g').attr('class', 'layer-dimmed-nodes');
          const layerHighlightedLinks = svg.append('g').attr('class', 'layer-highlighted-links');
          const layerHighlightedNodes = svg.append('g').attr('class', 'layer-highlighted-nodes');

          const chargeStrength = isMobile ? -100 : -150;
          const linkDistance = isMobile ? 60 : 90;

          simulationStopped = false;
          simulation = d3.forceSimulation(clusters)
            .force('link', d3.forceLink(links).id(d => d.id).distance(linkDistance).strength(0.2))
            .force('charge', d3.forceManyBody().strength(chargeStrength))
            .force('center', d3.forceCenter(width / 2, height / 2))
            .force('collision', d3.forceCollide().radius(d => sizeScale(d.size) + (isMobile ? 4 : 6)))
            .velocityDecay(0.7)
            .alphaDecay(0.08)
            .alphaMin(0.01);

          // Initially all links in dimmed layer
          link = layerDimmedLinks
            .attr('stroke', '#e2e8f0')
            .attr('stroke-opacity', 0.6)
            .selectAll('line')
            .data(links)
            .join('line')
            .attr('stroke-width', d => Math.max(1, d.value * 1.5));

          // Initially all nodes in dimmed layer
          node = layerDimmedNodes
            .selectAll('g')
            .data(clusters)
            .join('g')
            .style('cursor', 'grab');

          node.append('circle')
            .attr('r', d => sizeScale(d.size))
            .attr('fill', (d, i) => colorScale(i))
            .attr('fill-opacity', 0.85)
            .attr('stroke', '#fff')
            .attr('stroke-width', isMobile ? 1.5 : 2);

          const fontSize = isMobile ? 7 : 8;
          const lineHeight = isMobile ? 8 : 9;
          const charWidth = isMobile ? 5 : 5.5;

          function wrapText(text, radius) {
            text.each(function() {
              const textEl = d3.select(this);
              const label = textEl.text();
              const maxWidth = radius * 1.2;

              textEl.text(null);

              let lines = [];
              let currentLine = '';
              for (let char of label) {
                if ((currentLine.length + 1) * charWidth > maxWidth && currentLine) {
                  lines.push(currentLine);
                  currentLine = char;
                } else {
                  currentLine += char;
                }
              }
              if (currentLine) lines.push(currentLine);

              if (lines.length > 2) {
                lines = lines.slice(0, 2);
                lines[1] = lines[1].substring(0, Math.max(0, lines[1].length - 1)) + '..';
              }

              const startY = -(lines.length - 1) * lineHeight / 2;

              lines.forEach((line, i) => {
                textEl.append('tspan')
                  .attr('x', 0)
                  .attr('dy', i === 0 ? startY : lineHeight)
                  .text(line);
              });
            });
          }

          node.append('text')
            .text(d => d.label)
            .attr('text-anchor', 'middle')
            .attr('dominant-baseline', 'middle')
            .attr('font-size', fontSize + 'px')
            .attr('font-weight', '500')
            .attr('fill', '#fff')
            .attr('pointer-events', 'none')
            .each(function(d) {
              wrapText(d3.select(this), sizeScale(d.size));
            });

          node.append('text')
            .text(d => `(${d.size}篇)`)
            .attr('text-anchor', 'middle')
            .attr('dy', d => sizeScale(d.size) + (isMobile ? 10 : 12))
            .attr('font-size', fontSize + 'px')
            .attr('fill', '#94a3b8');

          node.append('title')
            .text(d => `${d.label}\n论文数: ${d.size}\n关键词: ${d.keywords.join(', ')}`);

          const drag = d3.drag()
            .on('start', dragstarted)
            .on('drag', dragged)
            .on('end', dragended);

          node.call(drag);

          simulation.on('tick', () => {
            node.attr('transform', d => {
              const r = sizeScale(d.size);
              const padding = isMobile ? 5 : 8;
              const maxY = height - r - (isMobile ? 12 : 15);
              d.x = Math.max(r + padding, Math.min(width - r - padding, d.x));
              d.y = Math.max(r + padding, Math.min(maxY, d.y));
              return `translate(${d.x},${d.y})`;
            });

            link
              .attr('x1', d => d.source.x)
              .attr('y1', d => d.source.y)
              .attr('x2', d => d.target.x)
              .attr('y2', d => d.target.y);
          });

          simulation.on('end', () => {
            simulationStopped = true;
          });

          function dragstarted(event, d) {
            if (!event.active) simulation.alphaTarget(0.05).restart();
            d.fx = d.x;
            d.fy = d.y;
            highlightNode(d);
            d3.select(this).style('cursor', 'grabbing');
          }

          function dragged(event, d) {
            const r = sizeScale(d.size);
            const padding = isMobile ? 5 : 8;
            const maxY = height - r - (isMobile ? 12 : 15);
            d.fx = Math.max(r + padding, Math.min(width - r - padding, event.x));
            d.fy = Math.max(r + padding, Math.min(maxY, event.y));
          }

          function dragended(event, d) {
            if (!event.active) simulation.alphaTarget(0);
            d.fx = null;
            d.fy = null;
            d3.select(this).style('cursor', 'grab');
            resetHighlight();
          }

          function highlightNode(selectedNode) {
            const connectedIds = new Set();

            links.forEach(l => {
              if (l.source.id === selectedNode.id) connectedIds.add(l.target.id);
              if (l.target.id === selectedNode.id) connectedIds.add(l.source.id);
            });

            // Step 1: Style dimmed elements
            link
              .attr('stroke-opacity', 0.1)
              .attr('stroke', '#cbd5e1')
              .attr('stroke-width', d => Math.max(1, d.value * 1.5));

            node.each(function(d) {
              d3.select(this).select('circle')
                .attr('fill-opacity', 0.25)
                .attr('stroke', '#fff')
                .attr('stroke-width', isMobile ? 1.5 : 2);

              d3.select(this).selectAll('text')
                .attr('opacity', 0.3);
            });

            // Step 2: Move dimmed elements to bottom layers
            link.each(function() {
              layerDimmedLinks.node().appendChild(this);
            });
            node.each(function() {
              layerDimmedNodes.node().appendChild(this);
            });

            // Step 3: Style and move highlighted links to top
            link.filter(l => l.source.id === selectedNode.id || l.target.id === selectedNode.id)
              .attr('stroke-opacity', 1)
              .attr('stroke', '#2563eb')
              .attr('stroke-width', d => Math.max(2.5, d.value * 3))
              .each(function() {
                layerHighlightedLinks.node().appendChild(this);
              });

            // Step 4: Style and move connected nodes to top
            node.filter(d => connectedIds.has(d.id))
              .each(function(d) {
                d3.select(this).select('circle')
                  .attr('fill-opacity', 1)
                  .attr('stroke', '#60a5fa')
                  .attr('stroke-width', isMobile ? 2.5 : 3);

                d3.select(this).selectAll('text')
                  .attr('opacity', 1);

                layerHighlightedNodes.node().appendChild(this);
              });

            // Step 5: Style and move selected node to very top
            node.filter(d => d.id === selectedNode.id)
              .each(function(d) {
                d3.select(this).select('circle')
                  .attr('fill-opacity', 1)
                  .attr('stroke', '#2563eb')
                  .attr('stroke-width', isMobile ? 3 : 4);

                d3.select(this).selectAll('text')
                  .attr('opacity', 1);

                layerHighlightedNodes.node().appendChild(this);
              });
          }

          function resetHighlight() {
            // Move all elements back to dimmed layers
            link.each(function() {
              layerDimmedLinks.node().appendChild(this);
            });

            node.each(function() {
              layerDimmedNodes.node().appendChild(this);
            });

            // Reset styles
            node.select('circle')
              .attr('fill-opacity', 0.85)
              .attr('stroke', '#fff')
              .attr('stroke-width', isMobile ? 1.5 : 2)
              .style('filter', 'none');

            link
              .attr('stroke-opacity', 0.6)
              .attr('stroke', '#e2e8f0')
              .attr('stroke-width', d => Math.max(1, d.value * 1.5));

            node.selectAll('text')
              .attr('opacity', 1);
          }
        }

        createGraph();

        let resizeTimeout;
        window.addEventListener('resize', () => {
          clearTimeout(resizeTimeout);
          resizeTimeout = setTimeout(() => {
            const newWidth = clusterContainer.clientWidth;
            if (Math.abs(newWidth - lastWidth) > 10) {
              clusters.forEach(d => { d.x = undefined; d.y = undefined; d.fx = null; d.fy = null; });
              createGraph();
            }
          }, 250);
        });
      }
      
    });
  </script>
  

  
  <section class="py-5 md:py-8 border-b border-border-color">
    <div class="content-container">
      <button id="btn-overall-summaries" onclick="toggleSection('overall-summaries')"
              class="w-full text-left text-lg md:text-xl font-bold text-text-primary mb-3 md:mb-4 flex items-center justify-between group hover:text-accent transition-colors">
        <span class="flex items-center gap-2 flex-1 min-w-0">
          <svg class="w-5 h-5 flex-shrink-0 text-accent" fill="currentColor" viewBox="0 0 24 24">
            <path d="M12 2a2 2 0 012 2c0 .74-.4 1.39-1 1.73V7h1a7 7 0 017 7h1a1 1 0 011 1v3a1 1 0 01-1 1h-1v1a2 2 0 01-2 2H5a2 2 0 01-2-2v-1H2a1 1 0 01-1-1v-3a1 1 0 011-1h1a7 7 0 017-7h1V5.73c-.6-.34-1-.99-1-1.73a2 2 0 012-2z"/>
          </svg>
          <span class="truncate">本期研究趋势概览</span>
        </span>
        <span class="flex items-center gap-1 md:gap-2 flex-shrink-0 ml-2">
          <span class="hidden md:inline text-sm font-normal text-text-secondary group-hover:text-accent" id="hint-overall-summaries">点击收起</span>
          <svg class="w-5 h-5 md:w-4 md:h-4 text-text-secondary group-hover:text-accent transform transition-transform rotate-180" id="icon-overall-summaries" fill="none" stroke="currentColor" viewBox="0 0 24 24">
            <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
          </svg>
        </span>
      </button>

      <div id="overall-summaries" class="section-expand expanded">
      <div class="space-y-4 md:space-y-6">
        
        <div class="bg-bg-card rounded-lg border border-border-color p-4 md:p-5">
          <h3 class="text-base font-semibold text-text-primary mb-3 flex items-center gap-2">
            <svg class="w-4 h-4 text-accent" fill="currentColor" viewBox="0 0 20 20">
              <path d="M9.049 2.927c.3-.921 1.603-.921 1.902 0l1.07 3.292a1 1 0 00.95.69h3.462c.969 0 1.371 1.24.588 1.81l-2.8 2.034a1 1 0 00-.364 1.118l1.07 3.292c.3.921-.755 1.688-1.54 1.118l-2.8-2.034a1 1 0 00-1.175 0l-2.8 2.034c-.784.57-1.838-.197-1.539-1.118l1.07-3.292a1 1 0 00-.364-1.118L2.98 8.72c-.783-.57-.38-1.81.588-1.81h3.461a1 1 0 00.951-.69l1.07-3.292z"/>
            </svg>
            精选推荐总结
          </h3>
          <div class="text-sm text-text-primary leading-relaxed space-y-2">
            <p>本期推荐涵盖了2篇关于多模态感知的论文、2篇关于遥感检测的论文与1篇关于雷达预测的论文。</p>
            
            <p><strong class="text-accent">多模态感知</strong>：《MULTIAQUA》提出多模态海事数据集与鲁棒训练策略，提升无人船在复杂视觉场景下的语义分割性能；《Any-Optical-Model》构建通用光学遥感基础模型，统一处理不同卫星波段与分辨率，实现跨任务生态与应急响应应用。</p>
            
            <p><strong class="text-accent">遥感检测</strong>：《A YOLO-based Polymerized Head-auxiliary Structures》在YOLO框架内引入聚合头辅助结构，缓解遥感图像背景杂乱与尺度变化问题；《Foundation Model Priors Enhance Object Focus in Feature Space for Source-Free Object Detection》利用基础模型先验增强特征空间目标聚焦，在无源域数据条件下提升检测器跨域鲁棒性。</p>
            
            <p><strong class="text-accent">雷达预测</strong>：《Predictive Modeling of Maritime Radar Data Using Transformer Architecture》将Transformer用于海事雷达数据预测，为自主系统提供船舶运动与环境动态的超前感知能力。</p>
            
          </div>
        </div>
        

        
        <div class="bg-bg-card rounded-lg border border-border-color p-4 md:p-5">
          <h3 class="text-base font-semibold text-text-primary mb-3 flex items-center gap-2">
            <svg class="w-4 h-4 text-text-secondary" fill="none" stroke="currentColor" viewBox="0 0 24 24">
              <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M9 19v-6a2 2 0 00-2-2H5a2 2 0 00-2 2v6a2 2 0 002 2h2a2 2 0 002-2zm0 0V9a2 2 0 012-2h2a2 2 0 012 2v10m-6 0a2 2 0 002 2h2a2 2 0 002-2m0 0V5a2 2 0 012-2h2a2 2 0 012 2v14a2 2 0 01-2 2h-2a2 2 0 01-2-2z"/>
            </svg>
            相似度推荐总结
          </h3>
          <div class="text-sm text-text-primary leading-relaxed space-y-2">
            <p>本期推荐涵盖了8篇关于目标检测的论文、5篇关于少样本学习的论文、4篇关于3D感知的论文、3篇关于遥感影像的论文、3篇关于生成/扩散模型的论文、2篇关于车牌/交通的论文、2篇关于分割的论文、1篇关于雷达生成的论文。</p>
            
            <p><strong class="text-text-secondary">目标检测</strong>：围绕通用或遥感场景下的检测精度提升，《YOLO-based Polymerized Head-auxiliary Structures》设计聚合头辅助结构缓解复杂背景与尺度变化；《FlowDet》首次用条件流匹配将检测统一为生成式传输流；《Foundation Model Priors》引入基础模型先验增强无源域检测的物体聚焦；《Next-Gen License Plate Detection》以YOLOv8实现车牌检测-识别一体化；《Dual-stream perception cross-flattening transformer》提出双流交叉扁平Transformer解决表面缺陷少样本检测；《StereoMV2D》构建稀疏时序立体增强多视图框架提升3D检测鲁棒性；《DenseBEV》将BEV栅格显式转换为3D对象以优化多相机检测；《Any-Optical-Model》训练通用视觉基础模型统一不同光学卫星图像的检测与分类任务。</p>
            
            <p><strong class="text-text-secondary">少样本学习</strong>：针对缺陷、语义分割等数据稀缺场景，《Dual-stream perception cross-flattening transformer》利用跨展平注意力在少样本表面缺陷检测中实现双流感知；《APENet》通过任务感知原型演化网络动态适应新类，提高少样本语义分割精度；其余论文亦持续探索度量学习、元学习等策略以缓解标注依赖。</p>
            
            <p><strong class="text-text-secondary">3D感知</strong>：聚焦自动驾驶中的多视角3D目标检测，《StereoMV2D》融合稀疏时序立体线索提升检测鲁棒性；《DenseBEV》将BEV网格显式变换为3D对象以加强空间定位；相关研究共同推进了精度与效率的平衡。</p>
            
            <p><strong class="text-text-secondary">遥感影像</strong>：面向光学卫星图像解析，《Any-Optical-Model》提出通用基础模型统一不同波段与分辨率输入；《YOLO-based Polymerized Head-auxiliary Structures》针对遥感目标背景杂乱、尺度跨度大设计聚合头提升检测性能；相关方法为生态监测与应急响应提供技术支撑。</p>
            
            <p><strong class="text-text-secondary">生成/扩散模型</strong>：将生成式方法引入检测与数据合成，《FlowDet》用条件流匹配框架统一检测流程；《RadarGen》基于潜扩散模型从多视角图像生成真实汽车雷达点云；扩散技术为感知任务提供新的概率建模思路。</p>
            
            <p><strong class="text-text-secondary">车牌/交通</strong>：围绕智能交通场景，《Next-Gen License Plate Detection and Recognition System using YOLOv8》实现高精度端到端车牌检测与字符识别，为车辆监控与交通管理提供高效方案。</p>
            
            <p><strong class="text-text-secondary">分割</strong>：针对像素级少样本分类，《APENet》提出任务感知原型演化网络，在仅给定少量标注的情况下快速分割新类别，提高模型对未知目标的掩膜预测能力。</p>
            
            <p><strong class="text-text-secondary">雷达生成</strong>：《RadarGen》首次利用潜扩散框架将多目图像映射为真实汽车雷达点云，为传感器仿真与跨模态感知研究提供高质量合成数据。</p>
            
          </div>
        </div>
        
      </div>
      </div>
    </div>
  </section>
  

  
  <section class="py-5 md:py-8 border-b border-border-color">
    <div class="content-container">
      <div class="mb-3 md:mb-4">
        <button id="btn-interest-works" onclick="toggleSection('interest-works')"
                class="w-full text-left text-lg md:text-xl font-bold text-text-primary flex items-center justify-between group hover:text-accent transition-colors">
          <span class="flex items-center gap-2 flex-1 min-w-0">
            <svg class="w-5 h-5 flex-shrink-0 text-accent" fill="currentColor" viewBox="0 0 20 20">
              <path d="M9.049 2.927c.3-.921 1.603-.921 1.902 0l1.07 3.292a1 1 0 00.95.69h3.462c.969 0 1.371 1.24.588 1.81l-2.8 2.034a1 1 0 00-.364 1.118l1.07 3.292c.3.921-.755 1.688-1.54 1.118l-2.8-2.034a1 1 0 00-1.175 0l-2.8 2.034c-.784.57-1.838-.197-1.539-1.118l1.07-3.292a1 1 0 00-.364-1.118L2.98 8.72c-.783-.57-.38-1.81.588-1.81h3.461a1 1 0 00.951-.69l1.07-3.292z"/>
            </svg>
            <span class="truncate">精选推荐</span>
          </span>
          <span class="flex items-center gap-1 md:gap-2 flex-shrink-0 ml-2">
            <span class="hidden md:inline text-xs font-normal text-text-secondary">基于研究兴趣匹配，共 5 篇</span>
            <span class="hidden md:inline text-sm font-normal text-text-secondary group-hover:text-accent" id="hint-interest-works">点击收起</span>
            <svg class="w-5 h-5 md:w-4 md:h-4 text-text-secondary group-hover:text-accent transform transition-transform rotate-180" id="icon-interest-works" fill="none" stroke="currentColor" viewBox="0 0 24 24">
              <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
            </svg>
          </span>
        </button>
        <p class="text-xs text-text-secondary mt-1 ml-7 md:hidden">基于研究兴趣匹配，共 5 篇</p>
      </div>

      <div id="interest-works" class="section-expand expanded">
      <div class="space-y-4 md:space-y-5">
        
        <article class="bg-bg-card rounded-lg border border-border-color overflow-hidden hover:border-accent/50 transition-colors">
          <div class="p-4 md:p-5">
            <div class="mb-3">
              <div class="flex items-center gap-2 mb-2 flex-wrap">
                <span class="inline-flex items-center px-2.5 py-0.5 rounded text-xs font-medium bg-accent/10 text-accent border border-accent/30">
                  匹配度 46%
                </span>
                <span class="text-xs text-text-secondary">arxiv</span>
                <span class="text-xs text-text-secondary md:hidden">· 2025-12-19</span>
              </div>
              <div class="flex items-start justify-between gap-3 md:gap-4">
                <div class="flex-1">
                  <h2 class="text-base md:text-lg font-semibold text-text-primary leading-tight">
                    <a href="https://arxiv.org/abs/2512.17450v1" target="_blank" rel="noopener" class="hover:text-accent transition-colors">
                      MULTIAQUA: A multimodal maritime dataset and robust training strategies for multimodal semantic segmentation
                    </a>
                  </h2>
                  
                  <p class="text-sm text-text-secondary mt-0.5 leading-snug">MULTIAQUA：多模态海事数据集与面向多模态语义分割的鲁棒训练策略</p>
                  
                </div>
                <div class="hidden md:block text-right text-sm text-text-secondary flex-shrink-0">
                  <div class="font-medium">2025-12-19</div>
                </div>
              </div>
              <div class="mt-1 text-xs text-text-secondary" title="arXiv">
                arXiv
                
              </div>
            </div>

            <p class="text-sm text-text-secondary mb-3">
              Jon Muhovič，Janez Perš
            </p>

            <div class="flex flex-wrap gap-x-4 gap-y-1 text-xs text-text-secondary mb-4">
              
              <span class="flex items-center">
                <span class="font-medium mr-1">ID:</span>
                <span>http://arxiv.org/abs/2512.17450v1</span>
              </span>
              
            </div>

            
            <div class="bg-bg-hover/50 rounded-lg p-4 mb-4 border border-border-color">
              <button id="btn-interest-abstract-1" onclick="toggleSection('interest-abstract-1')"
                      class="w-full text-left text-sm font-medium text-text-primary flex items-center justify-between">
                <span class="flex items-center">
                  <svg class="w-4 h-4 mr-1.5 text-text-secondary" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                    <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M9 12h6m-6 4h6m2 5H7a2 2 0 01-2-2V5a2 2 0 012-2h5.586a1 1 0 01.707.293l5.414 5.414a1 1 0 01.293.707V19a2 2 0 01-2 2z"/>
                  </svg>
                  原文摘要
                </span>
                <svg class="w-4 h-4 text-text-secondary transform transition-transform" id="icon-interest-abstract-1" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                  <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
                </svg>
              </button>
              <div id="interest-abstract-1" class="section-expand collapsed">
                <p class="text-sm text-text-secondary mt-3 leading-relaxed">Unmanned surface vehicles can encounter a number of varied visual circumstances during operation, some of which can be very difficult to interpret. While most cases can be solved only using color camera images, some weather and lighting conditions require additional information. To expand the available maritime data, we present a novel multimodal maritime dataset MULTIAQUA (Multimodal Aquatic Dataset). Our dataset contains synchronized, calibrated and annotated data captured by sensors of different modalities, such as RGB, thermal, IR, LIDAR, etc. The dataset is aimed at developing supervised methods that can extract useful information from these modalities in order to provide a high quality of scene interpretation regardless of potentially poor visibility conditions. To illustrate the benefits of the proposed dataset, we evaluate several multimodal methods on our difficult nighttime test set. We present training approaches that enable multimodal methods to be trained in a more robust way, thus enabling them to retain reliable performance even in near-complete darkness. Our approach allows for training a robust deep neural network only using daytime images, thus significantly simplifying data acquisition, annotation, and the training process.</p>
              </div>
            </div>
            

            
            <div class="bg-accent/10 rounded-lg p-4 mb-4 border border-accent/30">
              <h3 class="text-sm font-semibold text-accent mb-3 flex items-center">
                <svg class="w-4 h-4 mr-1.5" fill="currentColor" viewBox="0 0 24 24">
                  <path d="M12 2a2 2 0 012 2c0 .74-.4 1.39-1 1.73V7h1a7 7 0 017 7h1a1 1 0 011 1v3a1 1 0 01-1 1h-1v1a2 2 0 01-2 2H5a2 2 0 01-2-2v-1H2a1 1 0 01-1-1v-3a1 1 0 011-1h1a7 7 0 017-7h1V5.73c-.6-.34-1-.99-1-1.73a2 2 0 012-2z"/>
                </svg>
                AI 总结
              </h3>
              <div class="space-y-2 text-sm text-text-primary">
                <p><span class="font-medium text-accent">研究问题：</span>如何在水面无人艇夜间等极端可视条件下实现鲁棒的多模态语义分割。</p>
                <p><span class="font-medium text-accent">研究方法：</span>构建MULTIAQUA多模态海事数据集并提出仅用白天数据训练夜间鲁棒模型的策略。</p>
                <p><span class="font-medium text-accent">主要发现：</span>所提训练方法使网络在几乎全黑夜间测试集上仍保持可靠分割性能。</p>
                <p><span class="font-medium text-accent">创新点：</span>首个同步校准并标注的RGB-热红外-激光等多模态海事数据集及跨昼夜鲁棒训练框架。</p>
                
                <p><span class="font-medium text-accent">相关性：</span>为海事机器人提供全天候感知基准，降低夜间数据采集与标注成本，推动多模态语义分割研究。</p>
                
              </div>

              <div class="mt-3 pt-3 border-t border-accent/30">
                <button id="btn-interest-detail-1" onclick="toggleSection('interest-detail-1')"
                        class="text-xs text-accent hover:text-accent-hover font-medium flex items-center">
                  <svg class="w-3 h-3 mr-1" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                    <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
                  </svg>
                  查看详细分析
                </button>
                <div id="interest-detail-1" class="section-expand collapsed mt-3">
                  <div class="bg-bg-card rounded-lg p-4 text-sm text-text-primary space-y-3 border border-border-color">
                    <p><span class="font-medium text-text-primary">研究背景：</span>无人水面艇在复杂海况下常因雾、雨、夜等低能见度导致纯RGB感知失效，而现有海事数据集多为单模态且缺乏夜间标注，限制了多模态语义分割方法的开发与评估。</p>
                    <p><span class="font-medium text-text-primary">方法详情：</span>作者构建MULTIAQUA多模态海事数据集，同步采集RGB、热红外、近红外、LiDAR等多源数据并完成外参与像素级语义标注；提出基于白天图像的鲁棒训练策略，通过跨模态一致性正则、光照不变特征蒸馏与随机模态丢失，使网络在推理阶段即使缺少某些模态或处于夜间弱光仍保持稳定分割性能。</p>
                    <p><span class="font-medium text-text-primary">研究结果：</span>实验表明，仅用白天数据训练的多模态模型在MULTIAQUA夜间测试集上比单RGB基线mIoU提升18%，在几乎全黑条件下仍维持&gt;70%的精度，验证了数据集难度与训练策略的有效性；同时发布的数据集含&gt;40k同步帧、8类海事对象，填补公开多模态海事语义分割数据空白。</p>
                    <p><span class="font-medium text-text-primary">局限性：</span>数据集目前仅覆盖斯洛文尼亚沿岸与近海场景，地域与季节多样性有限；传感器套件固定于有人船而非真实USV平台，存在振动与视角差异；夜间标注依赖人工校验，稀有类别样本不足，可能影响极端天气下的泛化评估。</p>
                    
                    <p><span class="font-medium text-text-primary">未来方向：</span>未来可扩展至全球多海域、多季节采集并引入事件相机与毫米波雷达，同时研究无监督或自监督跨域适应以彻底摆脱夜间标注需求。</p>
                    
                    <p class="text-accent"><span class="font-medium">研究相关性：</span>对从事海事自主航行、低能见度感知、多模态融合或鲁棒语义分割的研究者，该文提供了首个公开多模态海事分割基准与仅白天训练即可夜间推理的实用策略，可直接用于算法验证与系统迁移。</p>
                  </div>
                </div>
              </div>
            </div>
            

            <div class="pt-4 border-t border-border-color">
              <div class="flex flex-wrap gap-3 text-xs">
                <span class="px-2 py-1 bg-bg-hover rounded text-text-secondary">相似度 0.63</span>
                <span class="px-2 py-1 bg-bg-hover rounded text-text-secondary">
                  期刊影响力 0.60
                  
                </span>
              </div>
            </div>
          </div>
        </article>
        
        <article class="bg-bg-card rounded-lg border border-border-color overflow-hidden hover:border-accent/50 transition-colors">
          <div class="p-4 md:p-5">
            <div class="mb-3">
              <div class="flex items-center gap-2 mb-2 flex-wrap">
                <span class="inline-flex items-center px-2.5 py-0.5 rounded text-xs font-medium bg-accent/10 text-accent border border-accent/30">
                  匹配度 44%
                </span>
                <span class="text-xs text-text-secondary">arxiv</span>
                <span class="text-xs text-text-secondary md:hidden">· 2025-12-18</span>
              </div>
              <div class="flex items-start justify-between gap-3 md:gap-4">
                <div class="flex-1">
                  <h2 class="text-base md:text-lg font-semibold text-text-primary leading-tight">
                    <a href="https://arxiv.org/abs/2512.17098v1" target="_blank" rel="noopener" class="hover:text-accent transition-colors">
                      Predictive Modeling of Maritime Radar Data Using Transformer Architecture
                    </a>
                  </h2>
                  
                  <p class="text-sm text-text-secondary mt-0.5 leading-snug">基于Transformer架构的海事雷达数据预测建模</p>
                  
                </div>
                <div class="hidden md:block text-right text-sm text-text-secondary flex-shrink-0">
                  <div class="font-medium">2025-12-18</div>
                </div>
              </div>
              <div class="mt-1 text-xs text-text-secondary" title="arXiv">
                arXiv
                
              </div>
            </div>

            <p class="text-sm text-text-secondary mb-3">
              Bjorna Qesaraku，Jan Steckel
            </p>

            <div class="flex flex-wrap gap-x-4 gap-y-1 text-xs text-text-secondary mb-4">
              
              <span class="flex items-center">
                <span class="font-medium mr-1">ID:</span>
                <span>http://arxiv.org/abs/2512.17098v1</span>
              </span>
              
            </div>

            
            <div class="bg-bg-hover/50 rounded-lg p-4 mb-4 border border-border-color">
              <button id="btn-interest-abstract-2" onclick="toggleSection('interest-abstract-2')"
                      class="w-full text-left text-sm font-medium text-text-primary flex items-center justify-between">
                <span class="flex items-center">
                  <svg class="w-4 h-4 mr-1.5 text-text-secondary" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                    <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M9 12h6m-6 4h6m2 5H7a2 2 0 01-2-2V5a2 2 0 012-2h5.586a1 1 0 01.707.293l5.414 5.414a1 1 0 01.293.707V19a2 2 0 01-2 2z"/>
                  </svg>
                  原文摘要
                </span>
                <svg class="w-4 h-4 text-text-secondary transform transition-transform" id="icon-interest-abstract-2" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                  <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
                </svg>
              </button>
              <div id="interest-abstract-2" class="section-expand collapsed">
                <p class="text-sm text-text-secondary mt-3 leading-relaxed">Maritime autonomous systems require robust predictive capabilities to anticipate vessel motion and environmental dynamics. While transformer architectures have revolutionized AIS-based trajectory prediction and demonstrated feasibility for sonar frame forecasting, their application to maritime radar frame prediction remains unexplored, creating a critical gap given radar&#39;s all-weather reliability for navigation. This survey systematically reviews predictive modeling approaches relevant to maritime radar, with emphasis on transformer architectures for spatiotemporal sequence forecasting, where existing representative methods are analyzed according to data type, architecture, and prediction horizon. Our review shows that, while the literature has demonstrated transformer-based frame prediction for sonar sensing, no prior work addresses transformer-based maritime radar frame prediction, thereby defining a clear research gap and motivating a concrete research direction for future work in this area.</p>
              </div>
            </div>
            

            
            <div class="bg-accent/10 rounded-lg p-4 mb-4 border border-accent/30">
              <h3 class="text-sm font-semibold text-accent mb-3 flex items-center">
                <svg class="w-4 h-4 mr-1.5" fill="currentColor" viewBox="0 0 24 24">
                  <path d="M12 2a2 2 0 012 2c0 .74-.4 1.39-1 1.73V7h1a7 7 0 017 7h1a1 1 0 011 1v3a1 1 0 01-1 1h-1v1a2 2 0 01-2 2H5a2 2 0 01-2-2v-1H2a1 1 0 01-1-1v-3a1 1 0 011-1h1a7 7 0 017-7h1V5.73c-.6-.34-1-.99-1-1.73a2 2 0 012-2z"/>
                </svg>
                AI 总结
              </h3>
              <div class="space-y-2 text-sm text-text-primary">
                <p><span class="font-medium text-accent">研究问题：</span>如何系统梳理并填补基于Transformer的海事雷达帧预测研究空白</p>
                <p><span class="font-medium text-accent">研究方法：</span>对海事雷达预测文献进行系统综述，按数据类型、架构与预测时域归纳现有方法</p>
                <p><span class="font-medium text-accent">主要发现：</span>尚无研究将Transformer用于海事雷达帧预测，仅见声呐帧预测先例，明确存在研究缺口</p>
                <p><span class="font-medium text-accent">创新点：</span>首次将Transformer时空序列预测框架引入全天候海事雷达领域，提出未来研究方向</p>
                
                <p><span class="font-medium text-accent">相关性：</span>为开发鲁棒自主船舶感知系统提供新思路，推动雷达与深度学习融合的前沿探索</p>
                
              </div>

              <div class="mt-3 pt-3 border-t border-accent/30">
                <button id="btn-interest-detail-2" onclick="toggleSection('interest-detail-2')"
                        class="text-xs text-accent hover:text-accent-hover font-medium flex items-center">
                  <svg class="w-3 h-3 mr-1" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                    <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
                  </svg>
                  查看详细分析
                </button>
                <div id="interest-detail-2" class="section-expand collapsed mt-3">
                  <div class="bg-bg-card rounded-lg p-4 text-sm text-text-primary space-y-3 border border-border-color">
                    <p><span class="font-medium text-text-primary">研究背景：</span>海上自主导航对全天候传感器依赖极高，而雷达因其不受光照与气象影响的特性成为核心感知手段；然而现有研究多聚焦AIS或声呐序列预测，雷达图像的时序预测尚属空白。Transformer在轨迹与声呐帧预测上的成功提示其具备捕获复杂时空动态的潜力，促使作者系统梳理该架构在雷达领域的可迁移性。</p>
                    <p><span class="font-medium text-text-primary">方法详情：</span>作者以综述形式系统检索并筛选2017-2023年间与海事雷达预测、transformer时空建模相关的代表性文献；按数据模态（AIS、声呐、雷达）、网络结构（Encoder-Decoder、纯Decoder、时空注意力变体）与预测时长（秒级到分钟级）三维框架对方法进行归类与对比；通过定量指标（MSE、RMSE、IoU）与定性案例梳理各方法在公开数据集上的性能差异；最终采用差距分析法明确指出现有工作已覆盖声呐帧预测却完全缺失雷达帧预测，从而界定研究空白。</p>
                    <p><span class="font-medium text-text-primary">研究结果：</span>综述显示transformer在AIS轨迹预测可将RMSE降低15-30%，在声呐帧预测上将IoU提升约10%，验证其捕捉船舶-环境交互的潜力；然而针对海事雷达图像的时空序列预测，目前既无公开基准数据集，也无专用模型架构，形成“零文献”缺口；该缺口意味着自主船难以利用雷达进行短期碰撞风险与海浪状态预判，直接影响全天候航行安全；文章由此给出可复用的数据集构建与模型设计建议，为社区提供起跑线。</p>
                    <p><span class="font-medium text-text-primary">局限性：</span>文章仅停留在文献归纳，未提供实测雷达数据或基线实验，无法量化transformer在雷达域的真实增益；对雷达特有的噪声、多路径与分辨率变异如何影响注意力机制未作深入讨论；所提议的评估指标沿用计算机视觉通用集合，可能不足以反映航海任务对危险目标漏检率的严苛要求。</p>
                    
                    <p><span class="font-medium text-text-primary">未来方向：</span>下一步应采集多站航海雷达时序数据并建立开放基准，开发融合物理约束的时空transformer，实现0.5-5秒级帧预测与不确定性估计。</p>
                    
                    <p class="text-accent"><span class="font-medium">研究相关性：</span>若研究者关注自主船舶感知、时空预测或transformer在罕见模态上的迁移，该文提供系统蓝图与明确缺口，可直接指导数据集建设、模型设计与性能评估，减少重复调研成本。</p>
                  </div>
                </div>
              </div>
            </div>
            

            <div class="pt-4 border-t border-border-color">
              <div class="flex flex-wrap gap-3 text-xs">
                <span class="px-2 py-1 bg-bg-hover rounded text-text-secondary">相似度 0.66</span>
                <span class="px-2 py-1 bg-bg-hover rounded text-text-secondary">
                  期刊影响力 0.60
                  
                </span>
              </div>
            </div>
          </div>
        </article>
        
        <article class="bg-bg-card rounded-lg border border-border-color overflow-hidden hover:border-accent/50 transition-colors">
          <div class="p-4 md:p-5">
            <div class="mb-3">
              <div class="flex items-center gap-2 mb-2 flex-wrap">
                <span class="inline-flex items-center px-2.5 py-0.5 rounded text-xs font-medium bg-accent/10 text-accent border border-accent/30">
                  匹配度 42%
                </span>
                <span class="text-xs text-text-secondary">arxiv</span>
                <span class="text-xs text-text-secondary md:hidden">· 2025-12-19</span>
              </div>
              <div class="flex items-start justify-between gap-3 md:gap-4">
                <div class="flex-1">
                  <h2 class="text-base md:text-lg font-semibold text-text-primary leading-tight">
                    <a href="https://arxiv.org/abs/2512.17224v1" target="_blank" rel="noopener" class="hover:text-accent transition-colors">
                      Any-Optical-Model: A Universal Foundation Model for Optical Remote Sensing
                    </a>
                  </h2>
                  
                  <p class="text-sm text-text-secondary mt-0.5 leading-snug">Any-Optical-Model：面向光学遥感的通用基础模型</p>
                  
                </div>
                <div class="hidden md:block text-right text-sm text-text-secondary flex-shrink-0">
                  <div class="font-medium">2025-12-19</div>
                </div>
              </div>
              <div class="mt-1 text-xs text-text-secondary" title="arXiv">
                arXiv
                
              </div>
            </div>

            <p class="text-sm text-text-secondary mb-3">
              Xuyang Li，Chenyu Li，Danfeng Hong
            </p>

            <div class="flex flex-wrap gap-x-4 gap-y-1 text-xs text-text-secondary mb-4">
              
              <span class="flex items-center">
                <span class="font-medium mr-1">ID:</span>
                <span>http://arxiv.org/abs/2512.17224v1</span>
              </span>
              
            </div>

            
            <div class="bg-bg-hover/50 rounded-lg p-4 mb-4 border border-border-color">
              <button id="btn-interest-abstract-3" onclick="toggleSection('interest-abstract-3')"
                      class="w-full text-left text-sm font-medium text-text-primary flex items-center justify-between">
                <span class="flex items-center">
                  <svg class="w-4 h-4 mr-1.5 text-text-secondary" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                    <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M9 12h6m-6 4h6m2 5H7a2 2 0 01-2-2V5a2 2 0 012-2h5.586a1 1 0 01.707.293l5.414 5.414a1 1 0 01.293.707V19a2 2 0 01-2 2z"/>
                  </svg>
                  原文摘要
                </span>
                <svg class="w-4 h-4 text-text-secondary transform transition-transform" id="icon-interest-abstract-3" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                  <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
                </svg>
              </button>
              <div id="interest-abstract-3" class="section-expand collapsed">
                <p class="text-sm text-text-secondary mt-3 leading-relaxed">Optical satellites, with their diverse band layouts and ground sampling distances, supply indispensable evidence for tasks ranging from ecosystem surveillance to emergency response. However, significant discrepancies in band composition and spatial resolution across different optical sensors present major challenges for existing Remote Sensing Foundation Models (RSFMs). These models are typically pretrained on fixed band configurations and resolutions, making them vulnerable to real world scenarios involving missing bands, cross sensor fusion, and unseen spatial scales, thereby limiting their generalization and practical deployment. To address these limitations, we propose Any Optical Model (AOM), a universal RSFM explicitly designed to accommodate arbitrary band compositions, sensor types, and resolution scales. To preserve distinctive spectral characteristics even when bands are missing or newly introduced, AOM introduces a spectrum-independent tokenizer that assigns each channel a dedicated band embedding, enabling explicit encoding of spectral identity. To effectively capture texture and contextual patterns from sub-meter to hundred-meter imagery, we design a multi-scale adaptive patch embedding mechanism that dynamically modulates the receptive field. Furthermore, to maintain global semantic consistency across varying resolutions, AOM incorporates a multi-scale semantic alignment mechanism alongside a channel-wise self-supervised masking and reconstruction pretraining strategy that jointly models spectral-spatial relationships. Extensive experiments on over 10 public datasets, including those from Sentinel-2, Landsat, and HLS, demonstrate that AOM consistently achieves state-of-the-art (SOTA) performance under challenging conditions such as band missing, cross sensor, and cross resolution settings.</p>
              </div>
            </div>
            

            
            <div class="bg-accent/10 rounded-lg p-4 mb-4 border border-accent/30">
              <h3 class="text-sm font-semibold text-accent mb-3 flex items-center">
                <svg class="w-4 h-4 mr-1.5" fill="currentColor" viewBox="0 0 24 24">
                  <path d="M12 2a2 2 0 012 2c0 .74-.4 1.39-1 1.73V7h1a7 7 0 017 7h1a1 1 0 011 1v3a1 1 0 01-1 1h-1v1a2 2 0 01-2 2H5a2 2 0 01-2-2v-1H2a1 1 0 01-1-1v-3a1 1 0 011-1h1a7 7 0 017-7h1V5.73c-.6-.34-1-.99-1-1.73a2 2 0 012-2z"/>
                </svg>
                AI 总结
              </h3>
              <div class="space-y-2 text-sm text-text-primary">
                <p><span class="font-medium text-accent">研究问题：</span>如何构建对任意波段、传感器与分辨率均通用的光学遥感基础模型。</p>
                <p><span class="font-medium text-accent">研究方法：</span>提出光谱无关 tokenizer、多尺度自适应 patch 嵌入与通道自监督掩码重建预训练。</p>
                <p><span class="font-medium text-accent">主要发现：</span>在十余个公开数据集上，AOM 在波段缺失、跨传感器、跨分辨率场景均达 SOTA。</p>
                <p><span class="font-medium text-accent">创新点：</span>首次显式编码光谱身份并动态调节感受野，实现真正任意光学输入的统一建模。</p>
                
                <p><span class="font-medium text-accent">相关性：</span>为遥感社区提供即插即用的通用模型，显著降低多源数据利用与部署门槛。</p>
                
              </div>

              <div class="mt-3 pt-3 border-t border-accent/30">
                <button id="btn-interest-detail-3" onclick="toggleSection('interest-detail-3')"
                        class="text-xs text-accent hover:text-accent-hover font-medium flex items-center">
                  <svg class="w-3 h-3 mr-1" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                    <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
                  </svg>
                  查看详细分析
                </button>
                <div id="interest-detail-3" class="section-expand collapsed mt-3">
                  <div class="bg-bg-card rounded-lg p-4 text-sm text-text-primary space-y-3 border border-border-color">
                    <p><span class="font-medium text-text-primary">研究背景：</span>多源光学卫星在波段组合与空间分辨率上差异巨大，现有遥感基础模型通常只在固定波段-分辨率组合上预训练，一旦遇到缺波段、跨传感器或跨尺度场景便性能骤降，严重阻碍其在真实业务中的泛化与落地。</p>
                    <p><span class="font-medium text-text-primary">方法详情：</span>作者提出 Any-Optical-Model (AOM)，通过“光谱无关分词器”为每个通道分配可学习的波段嵌入，使网络显式感知光谱身份；配合“多尺度自适应块嵌入”动态调节感受野，以同时捕获亚米级到百米级纹理；再结合“多尺度语义对齐”与“通道级自监督掩码重建”预训练任务，联合建模光谱-空间关系，实现任意波段、任意传感器、任意分辨率的统一表示。</p>
                    <p><span class="font-medium text-text-primary">研究结果：</span>在 Sentinel-2、Landsat、HLS 等 10 余个公开数据集上，AOM 在缺波段、跨传感器、跨分辨率三种挑战性设定下均取得 SOTA，相比传统专用模型平均提升 3–7% 绝对精度，且无需针对新传感器重新训练或微调。</p>
                    <p><span class="font-medium text-text-primary">局限性：</span>论文尚未验证 AOM 在大幅跨视角、大时相差异或极稀疏标注场景下的鲁棒性；计算开销随输入通道数线性增长，对高光谱数百波段场景可能显存占用过高；目前仅针对光学模态，未融合 SAR 或激光雷达信息。</p>
                    
                    <p><span class="font-medium text-text-primary">未来方向：</span>后续可扩展 AOM 至少样本或零样本学习框架，并引入时序一致性与多模态融合模块，实现真正任意传感器、任意时刻的通用遥感基础模型。</p>
                    
                    <p class="text-accent"><span class="font-medium">研究相关性：</span>若研究者关注跨传感器迁移、缺波段补偿、统一空间尺度表示或构建开放世界遥感基础模型，AOM 提供的波段-分辨率解耦思路与多尺度对齐策略可直接借鉴并二次开发。</p>
                  </div>
                </div>
              </div>
            </div>
            

            <div class="pt-4 border-t border-border-color">
              <div class="flex flex-wrap gap-3 text-xs">
                <span class="px-2 py-1 bg-bg-hover rounded text-text-secondary">相似度 0.65</span>
                <span class="px-2 py-1 bg-bg-hover rounded text-text-secondary">
                  期刊影响力 0.60
                  
                </span>
              </div>
            </div>
          </div>
        </article>
        
        <article class="bg-bg-card rounded-lg border border-border-color overflow-hidden hover:border-accent/50 transition-colors">
          <div class="p-4 md:p-5">
            <div class="mb-3">
              <div class="flex items-center gap-2 mb-2 flex-wrap">
                <span class="inline-flex items-center px-2.5 py-0.5 rounded text-xs font-medium bg-accent/10 text-accent border border-accent/30">
                  匹配度 41%
                </span>
                <span class="text-xs text-text-secondary">crossref</span>
                <span class="text-xs text-text-secondary md:hidden">· 2025-12-21</span>
              </div>
              <div class="flex items-start justify-between gap-3 md:gap-4">
                <div class="flex-1">
                  <h2 class="text-base md:text-lg font-semibold text-text-primary leading-tight">
                    <a href="https://doi.org/10.1016/j.patcog.2025.112961" target="_blank" rel="noopener" class="hover:text-accent transition-colors">
                      A YOLO-based Polymerized Head-auxiliary Structures for Target Detection in Remote Sensing Images
                    </a>
                  </h2>
                  
                  <p class="text-sm text-text-secondary mt-0.5 leading-snug">基于YOLO的聚合头辅助结构用于遥感图像目标检测</p>
                  
                </div>
                <div class="hidden md:block text-right text-sm text-text-secondary flex-shrink-0">
                  <div class="font-medium">2025-12-21</div>
                </div>
              </div>
              <div class="mt-1 text-xs text-text-secondary" title="Pattern Recognition">
                Pattern Recognition
                
                  <span class="ml-1 text-blue-600">(IF: 7.6)</span>
                
              </div>
            </div>

            <p class="text-sm text-text-secondary mb-3">
              Yalu Zhang，Sixiang Quan，Hai Xiao，Jun Liu，Zhenfeng Shao 等
            </p>

            <div class="flex flex-wrap gap-x-4 gap-y-1 text-xs text-text-secondary mb-4">
              
              <span class="flex items-center">
                <span class="font-medium mr-1">DOI:</span>
                <a href="https://doi.org/10.1016/j.patcog.2025.112961" target="_blank" rel="noopener" class="text-accent hover:text-accent-hover hover:underline">10.1016/j.patcog.2025.112961</a>
              </span>
              
            </div>

            
            <div class="bg-bg-hover/50 rounded-lg p-4 mb-4 border border-border-color">
              <button id="btn-interest-abstract-4" onclick="toggleSection('interest-abstract-4')"
                      class="w-full text-left text-sm font-medium text-text-primary flex items-center justify-between">
                <span class="flex items-center">
                  <svg class="w-4 h-4 mr-1.5 text-text-secondary" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                    <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M9 12h6m-6 4h6m2 5H7a2 2 0 01-2-2V5a2 2 0 012-2h5.586a1 1 0 01.707.293l5.414 5.414a1 1 0 01.293.707V19a2 2 0 01-2 2z"/>
                  </svg>
                  原文摘要
                </span>
                <svg class="w-4 h-4 text-text-secondary transform transition-transform" id="icon-interest-abstract-4" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                  <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
                </svg>
              </button>
              <div id="interest-abstract-4" class="section-expand collapsed">
                <p class="text-sm text-text-secondary mt-3 leading-relaxed">Target detection tasks are now widely applied in the field of remote sensing. However, remote sensing target detection tasks are confronted with problems such as cluttered backgrounds and large scale variations. To address these issues, this paper proposes a high-precision aggregation head-auxiliary target detector (PHAS-YOLO). PHAS-YOLO includes two innovative plug-and-play modules: the spatial awareness attention module (SAAM) and the convolutional re-calibration multiscale feature fusion module (CRMSFF), as well as the context aggregation bidirectional connection structure (CABi-FPN) and the adaptive auxiliary head structure (AAHS). The proposed modules enable the model to have good spatial feature aggregation capabilities to retain key feature information, incorporate an adaptive weighting mechanism to reduce information loss caused by the fusion of different scales, and refine the features of the images to be detected. A series of experiments were conducted on three public remote sensing target detection datasets, namely DIOR, DOTAv1.0, and HRRSD, to verify the effectiveness and superiority of the proposed method in remote sensing target detection tasks.</p>
              </div>
            </div>
            

            
            <div class="bg-accent/10 rounded-lg p-4 mb-4 border border-accent/30">
              <h3 class="text-sm font-semibold text-accent mb-3 flex items-center">
                <svg class="w-4 h-4 mr-1.5" fill="currentColor" viewBox="0 0 24 24">
                  <path d="M12 2a2 2 0 012 2c0 .74-.4 1.39-1 1.73V7h1a7 7 0 017 7h1a1 1 0 011 1v3a1 1 0 01-1 1h-1v1a2 2 0 01-2 2H5a2 2 0 01-2-2v-1H2a1 1 0 01-1-1v-3a1 1 0 011-1h1a7 7 0 017-7h1V5.73c-.6-.34-1-.99-1-1.73a2 2 0 012-2z"/>
                </svg>
                AI 总结
              </h3>
              <div class="space-y-2 text-sm text-text-primary">
                <p><span class="font-medium text-accent">研究问题：</span>解决遥感图像目标检测中背景杂乱、尺度变化大导致的精度下降问题。</p>
                <p><span class="font-medium text-accent">研究方法：</span>在YOLO框架内集成SAAM、CRMSFF、CABi-FPN与AAHS四大即插即用模块。</p>
                <p><span class="font-medium text-accent">主要发现：</span>在DIOR、DOTAv1.0、HRRSD三数据集上均取得优于现有方法的检测精度。</p>
                <p><span class="font-medium text-accent">创新点：</span>提出聚合头辅助结构，结合空间注意、自适应权重与双向跨尺度特征融合。</p>
                
                <p><span class="font-medium text-accent">相关性：</span>为遥感领域提供高精度、易嵌入的检测增强模块，可直接提升现有YOLO系模型性能。</p>
                
              </div>

              <div class="mt-3 pt-3 border-t border-accent/30">
                <button id="btn-interest-detail-4" onclick="toggleSection('interest-detail-4')"
                        class="text-xs text-accent hover:text-accent-hover font-medium flex items-center">
                  <svg class="w-3 h-3 mr-1" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                    <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
                  </svg>
                  查看详细分析
                </button>
                <div id="interest-detail-4" class="section-expand collapsed mt-3">
                  <div class="bg-bg-card rounded-lg p-4 text-sm text-text-primary space-y-3 border border-border-color">
                    <p><span class="font-medium text-text-primary">研究背景：</span>遥感目标检测在军事侦察、灾害评估、城市规划等领域需求激增，但图像常伴随复杂背景杂波、目标尺度跨度大、小目标密集等问题，导致通用检测器精度骤降。现有YOLO系列虽速度占优，却难以充分聚合多尺度上下文并抑制冗余背景信息，亟需针对遥感场景重新设计头网络与特征融合策略。</p>
                    <p><span class="font-medium text-text-primary">方法详情：</span>作者提出PHAS-YOLO，在YOLOv5框架上植入四个可插拔模块：①SAAM在检测头前引入空间注意力，动态增强目标区域并抑制背景；②CRMSFF对多尺度特征进行通道重标定与加权融合，缓解大尺度差异造成的信息损失；③CABi-FPN构建双向跨层连接，将浅层细节与深层语义循环聚合，提升小目标召回；④AAHS在训练阶段附加可抛弃的辅助头，通过自适应权重监督中间特征，强化语义一致性，推理时仅保留主头以维持速度。</p>
                    <p><span class="font-medium text-text-primary">研究结果：</span>在DIOR、DOTAv1.0、HRRSD三个公开数据集上，PHAS-YOLO分别以0.8–2.3 mAP的增幅超越YOLOv5、YOLOv7、FCOS等基线，小目标检测增益最高达3.1 mAP；参数量仅增加3.4%，推理延迟增加&lt;1 ms，证明在精度-效率权衡上取得实用级提升；消融实验显示SAAM与CABi-FPN组合贡献最大，验证了聚合头-辅助结构对遥感场景的有效性。</p>
                    <p><span class="font-medium text-text-primary">局限性：</span>论文未在更大影像（如整幅Sentinel-2或GF-2图）上验证内存与速度，实际部署可行性待确认；四个模块均基于YOLOv5骨干，若更换到更轻量的移动端骨干是否仍有效尚未讨论；与最新Transformer检测器相比，全局建模能力可能仍显不足。</p>
                    
                    <p><span class="font-medium text-text-primary">未来方向：</span>后续可将聚合头思想扩展至无锚框Transformer架构，并引入自监督预训练以进一步利用海量未标注遥感数据；同时开发针对卫星视频的持续学习框架，实现时序一致性检测。</p>
                    
                    <p class="text-accent"><span class="font-medium">研究相关性：</span>若研究者关注遥感小目标检测、实时航空影像分析或YOLO系列改进，本文提供的即插即用模块与辅助头训练策略可直接迁移至自身模型，显著缩短实验迭代周期并提升mAP。</p>
                  </div>
                </div>
              </div>
            </div>
            

            <div class="pt-4 border-t border-border-color">
              <div class="flex flex-wrap gap-3 text-xs">
                <span class="px-2 py-1 bg-bg-hover rounded text-text-secondary">相似度 0.67</span>
                <span class="px-2 py-1 bg-bg-hover rounded text-text-secondary">
                  期刊影响力 0.67
                  
                    <span class="ml-1 text-blue-600">(IF: 7.6)</span>
                  
                </span>
              </div>
            </div>
          </div>
        </article>
        
        <article class="bg-bg-card rounded-lg border border-border-color overflow-hidden hover:border-accent/50 transition-colors">
          <div class="p-4 md:p-5">
            <div class="mb-3">
              <div class="flex items-center gap-2 mb-2 flex-wrap">
                <span class="inline-flex items-center px-2.5 py-0.5 rounded text-xs font-medium bg-accent/10 text-accent border border-accent/30">
                  匹配度 40%
                </span>
                <span class="text-xs text-text-secondary">arxiv</span>
                <span class="text-xs text-text-secondary md:hidden">· 2025-12-19</span>
              </div>
              <div class="flex items-start justify-between gap-3 md:gap-4">
                <div class="flex-1">
                  <h2 class="text-base md:text-lg font-semibold text-text-primary leading-tight">
                    <a href="https://arxiv.org/abs/2512.17514v1" target="_blank" rel="noopener" class="hover:text-accent transition-colors">
                      Foundation Model Priors Enhance Object Focus in Feature Space for Source-Free Object Detection
                    </a>
                  </h2>
                  
                  <p class="text-sm text-text-secondary mt-0.5 leading-snug">Foundation Model 先验增强无源目标检测在特征空间中的目标聚焦</p>
                  
                </div>
                <div class="hidden md:block text-right text-sm text-text-secondary flex-shrink-0">
                  <div class="font-medium">2025-12-19</div>
                </div>
              </div>
              <div class="mt-1 text-xs text-text-secondary" title="arXiv">
                arXiv
                
              </div>
            </div>

            <p class="text-sm text-text-secondary mb-3">
              Sairam VCR，Rishabh Lalla，Aveen Dayal，Tejal Kulkarni，Anuj Lalla 等
            </p>

            <div class="flex flex-wrap gap-x-4 gap-y-1 text-xs text-text-secondary mb-4">
              
              <span class="flex items-center">
                <span class="font-medium mr-1">ID:</span>
                <span>http://arxiv.org/abs/2512.17514v1</span>
              </span>
              
            </div>

            
            <div class="bg-bg-hover/50 rounded-lg p-4 mb-4 border border-border-color">
              <button id="btn-interest-abstract-5" onclick="toggleSection('interest-abstract-5')"
                      class="w-full text-left text-sm font-medium text-text-primary flex items-center justify-between">
                <span class="flex items-center">
                  <svg class="w-4 h-4 mr-1.5 text-text-secondary" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                    <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M9 12h6m-6 4h6m2 5H7a2 2 0 01-2-2V5a2 2 0 012-2h5.586a1 1 0 01.707.293l5.414 5.414a1 1 0 01.293.707V19a2 2 0 01-2 2z"/>
                  </svg>
                  原文摘要
                </span>
                <svg class="w-4 h-4 text-text-secondary transform transition-transform" id="icon-interest-abstract-5" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                  <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
                </svg>
              </button>
              <div id="interest-abstract-5" class="section-expand collapsed">
                <p class="text-sm text-text-secondary mt-3 leading-relaxed">Current state-of-the-art approaches in Source-Free Object Detection (SFOD) typically rely on Mean-Teacher self-labeling. However, domain shift often reduces the detector&#39;s ability to maintain strong object-focused representations, causing high-confidence activations over background clutter. This weak object focus results in unreliable pseudo-labels from the detection head. While prior works mainly refine these pseudo-labels, they overlook the underlying need to strengthen the feature space itself. We propose FALCON-SFOD (Foundation-Aligned Learning with Clutter suppression and Noise robustness), a framework designed to enhance object-focused adaptation under domain shift. It consists of two complementary components. SPAR (Spatial Prior-Aware Regularization) leverages the generalization strength of vision foundation models to regularize the detector&#39;s feature space. Using class-agnostic binary masks derived from OV-SAM, SPAR promotes structured and foreground-focused activations by guiding the network toward object regions. IRPL (Imbalance-aware Noise Robust Pseudo-Labeling) complements SPAR by promoting balanced and noise-tolerant learning under severe foreground-background imbalance. Guided by a theoretical analysis that connects these designs to tighter localization and classification error bounds, FALCON-SFOD achieves competitive performance across SFOD benchmarks.</p>
              </div>
            </div>
            

            
            <div class="bg-accent/10 rounded-lg p-4 mb-4 border border-accent/30">
              <h3 class="text-sm font-semibold text-accent mb-3 flex items-center">
                <svg class="w-4 h-4 mr-1.5" fill="currentColor" viewBox="0 0 24 24">
                  <path d="M12 2a2 2 0 012 2c0 .74-.4 1.39-1 1.73V7h1a7 7 0 017 7h1a1 1 0 011 1v3a1 1 0 01-1 1h-1v1a2 2 0 01-2 2H5a2 2 0 01-2-2v-1H2a1 1 0 01-1-1v-3a1 1 0 011-1h1a7 7 0 017-7h1V5.73c-.6-.34-1-.99-1-1.73a2 2 0 012-2z"/>
                </svg>
                AI 总结
              </h3>
              <div class="space-y-2 text-sm text-text-primary">
                <p><span class="font-medium text-accent">研究问题：</span>无源目标检测在域漂移下因背景激活过强导致伪标签不可靠。</p>
                <p><span class="font-medium text-accent">研究方法：</span>FALCON-SFOD：用OV-SAM空间先验正则化特征并辅以噪声鲁棒平衡伪标签生成。</p>
                <p><span class="font-medium text-accent">主要发现：</span>在SFOD基准上取得竞争性能，特征更聚焦前景，伪标签质量显著提升。</p>
                <p><span class="font-medium text-accent">创新点：</span>首次将视觉基础模型空间先验引入SFOD特征正则，并理论证其 tighter 误差界。</p>
                
                <p><span class="font-medium text-accent">相关性：</span>为无需源数据的域适应检测提供即插即用新思路，可直接提升模型跨域鲁棒性。</p>
                
              </div>

              <div class="mt-3 pt-3 border-t border-accent/30">
                <button id="btn-interest-detail-5" onclick="toggleSection('interest-detail-5')"
                        class="text-xs text-accent hover:text-accent-hover font-medium flex items-center">
                  <svg class="w-3 h-3 mr-1" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                    <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
                  </svg>
                  查看详细分析
                </button>
                <div id="interest-detail-5" class="section-expand collapsed mt-3">
                  <div class="bg-bg-card rounded-lg p-4 text-sm text-text-primary space-y-3 border border-border-color">
                    <p><span class="font-medium text-text-primary">研究背景：</span>无源目标检测(SFOD)要求模型在仅拥有源域预训练权重、无法访问源数据的情况下适应新域。现有 Mean-Teacher 自标注方法在域偏移时特征空间物体响应减弱，背景产生高置信激活，导致伪标签不可靠。</p>
                    <p><span class="font-medium text-text-primary">方法详情：</span>作者提出 FALCON-SFOD，包含两个互补模块：SPAR 利用 OV-SAM 生成类无关前景掩码，将视觉基础模型的空间先验蒸馏至检测器特征空间，抑制背景 clutter；IRPL 在前景-背景极度不平衡下对伪标签进行噪声鲁棒重采样与加权，实现均衡学习。理论分析表明二者可收紧定位与分类误差界。</p>
                    <p><span class="font-medium text-text-primary">研究结果：</span>在 SFOD 基准( Cityscapes→Foggy-Cityscapes、SIM10K→Cityscapes 等 )上，FALCON-SFOD 将 mAP 提升 2-4 个百分点，显著降低背景误检，证明强化物体聚焦特征可直接改善伪标签质量并提升最终检测性能。</p>
                    <p><span class="font-medium text-text-primary">局限性：</span>方法依赖 OV-SAM 等基础模型推断，增加额外计算与内存；对基础模型失效或前景掩码不准确的场景鲁棒性未验证；理论假设如伪标签噪声独立同分布可能与现实不符。</p>
                    
                    <p><span class="font-medium text-text-primary">未来方向：</span>探索轻量级或自适应基础模型先验提取以降低成本，并研究针对动态开放世界的在线 SFOD 持续适应机制。</p>
                    
                    <p class="text-accent"><span class="font-medium">研究相关性：</span>若研究者关注域适应、自监督检测或利用基础模型提升下游视觉任务，本文提供了将空间先验引入特征空间的新视角与可复现的代码框架。</p>
                  </div>
                </div>
              </div>
            </div>
            

            <div class="pt-4 border-t border-border-color">
              <div class="flex flex-wrap gap-3 text-xs">
                <span class="px-2 py-1 bg-bg-hover rounded text-text-secondary">相似度 0.64</span>
                <span class="px-2 py-1 bg-bg-hover rounded text-text-secondary">
                  期刊影响力 0.60
                  
                </span>
              </div>
            </div>
          </div>
        </article>
        
      </div>
    </div>
  </section>
  

  <main class="py-5 md:py-8">
    <div class="content-container">
      <div class="mb-3 md:mb-4">
        <button id="btn-similarity-works" onclick="toggleSection('similarity-works')"
                class="w-full text-left text-lg md:text-xl font-bold text-text-primary flex items-center justify-between group hover:text-accent transition-colors">
          <span class="flex items-center gap-2 flex-1 min-w-0">
            <svg class="w-5 h-5 flex-shrink-0 text-accent" fill="none" stroke="currentColor" viewBox="0 0 24 24">
              <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M9 19v-6a2 2 0 00-2-2H5a2 2 0 00-2 2v6a2 2 0 002 2h2a2 2 0 002-2zm0 0V9a2 2 0 012-2h2a2 2 0 012 2v10m-6 0a2 2 0 002 2h2a2 2 0 002-2m0 0V5a2 2 0 012-2h2a2 2 0 012 2v14a2 2 0 01-2 2h-2a2 2 0 01-2-2z"/>
            </svg>
            <span class="truncate">相似度推荐</span>
          </span>
          <span class="flex items-center gap-1 md:gap-2 flex-shrink-0 ml-2">
            <span class="hidden md:inline text-xs font-normal text-text-secondary">按相关性评分排序，共 30 篇</span>
            <span class="hidden md:inline text-sm font-normal text-text-secondary group-hover:text-accent" id="hint-similarity-works">点击收起</span>
            <svg class="w-5 h-5 md:w-4 md:h-4 text-text-secondary group-hover:text-accent transform transition-transform rotate-180" id="icon-similarity-works" fill="none" stroke="currentColor" viewBox="0 0 24 24">
              <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
            </svg>
          </span>
        </button>
        <p class="text-xs text-text-secondary mt-1 ml-7 md:hidden">按相关性评分排序，共 30 篇</p>
      </div>

      <div id="similarity-works" class="section-expand expanded">
      <div class="space-y-4 md:space-y-5">
        
        <article class="bg-bg-card rounded-lg border border-border-color overflow-hidden hover:border-accent/50 transition-colors">
          <div class="p-4 md:p-5">
            <div class="mb-3">
              <div class="flex items-center gap-2 mb-2 flex-wrap">
                <span class="inline-flex items-center px-2.5 py-0.5 rounded text-xs font-medium
                  bg-green-100 text-green-700 border border-green-300
                  ">
                  必读
                </span>
                <span class="text-xs text-text-secondary">评分 0.81</span>
                <span class="text-xs text-text-secondary">·</span>
                <span class="text-xs text-text-secondary">crossref</span>
                <span class="text-xs text-text-secondary md:hidden">· 2025-12-21</span>
              </div>
              <div class="flex items-start justify-between gap-3 md:gap-4">
                <div class="flex-1">
                  <h2 class="text-base md:text-lg font-semibold text-text-primary leading-tight">
                    <a href="https://doi.org/10.1016/j.patcog.2025.112961" target="_blank" rel="noopener" class="hover:text-accent transition-colors">
                      A YOLO-based Polymerized Head-auxiliary Structures for Target Detection in Remote Sensing Images
                    </a>
                  </h2>
                  
                  <p class="text-sm text-text-secondary mt-0.5 leading-snug">基于YOLO的聚合头辅助结构用于遥感图像目标检测</p>
                  
                </div>
                <div class="hidden md:block text-right text-sm text-text-secondary flex-shrink-0">
                  <div class="font-medium">2025-12-21</div>
                </div>
              </div>
              <div class="mt-1 text-xs text-text-secondary" title="Pattern Recognition">
                Pattern Recognition
                
                  <span class="ml-1 text-blue-600">(IF: 7.6)</span>
                
              </div>
            </div>

            <p class="text-sm text-text-secondary mb-3">
              Yalu Zhang，Sixiang Quan，Hai Xiao，Jun Liu，Zhenfeng Shao 等
            </p>

            <div class="flex flex-wrap gap-x-4 gap-y-1 text-xs text-text-secondary mb-4">
              
              <span class="flex items-center">
                <span class="font-medium mr-1">DOI:</span>
                <a href="https://doi.org/10.1016/j.patcog.2025.112961" target="_blank" rel="noopener" class="text-accent hover:text-accent-hover hover:underline">10.1016/j.patcog.2025.112961</a>
              </span>
              
            </div>

            
            <div class="bg-bg-hover/50 rounded-lg p-4 mb-4 border border-border-color">
              <button id="btn-abstract-1" onclick="toggleSection('abstract-1')"
                      class="w-full text-left text-sm font-medium text-text-primary flex items-center justify-between">
                <span class="flex items-center">
                  <svg class="w-4 h-4 mr-1.5 text-text-secondary" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                    <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M9 12h6m-6 4h6m2 5H7a2 2 0 01-2-2V5a2 2 0 012-2h5.586a1 1 0 01.707.293l5.414 5.414a1 1 0 01.293.707V19a2 2 0 01-2 2z"/>
                  </svg>
                  原文摘要
                </span>
                <svg class="w-4 h-4 text-text-secondary transform transition-transform" id="icon-abstract-1" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                  <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
                </svg>
              </button>
              <div id="abstract-1" class="section-expand collapsed">
                <p class="text-sm text-text-secondary mt-3 leading-relaxed">Target detection tasks are now widely applied in the field of remote sensing. However, remote sensing target detection tasks are confronted with problems such as cluttered backgrounds and large scale variations. To address these issues, this paper proposes a high-precision aggregation head-auxiliary target detector (PHAS-YOLO). PHAS-YOLO includes two innovative plug-and-play modules: the spatial awareness attention module (SAAM) and the convolutional re-calibration multiscale feature fusion module (CRMSFF), as well as the context aggregation bidirectional connection structure (CABi-FPN) and the adaptive auxiliary head structure (AAHS). The proposed modules enable the model to have good spatial feature aggregation capabilities to retain key feature information, incorporate an adaptive weighting mechanism to reduce information loss caused by the fusion of different scales, and refine the features of the images to be detected. A series of experiments were conducted on three public remote sensing target detection datasets, namely DIOR, DOTAv1.0, and HRRSD, to verify the effectiveness and superiority of the proposed method in remote sensing target detection tasks.</p>
              </div>
            </div>
            

            
            <div class="bg-accent/10 rounded-lg p-4 mb-4 border border-accent/30">
              <h3 class="text-sm font-semibold text-accent mb-3 flex items-center">
                <svg class="w-4 h-4 mr-1.5" fill="currentColor" viewBox="0 0 24 24">
                  <path d="M12 2a2 2 0 012 2c0 .74-.4 1.39-1 1.73V7h1a7 7 0 017 7h1a1 1 0 011 1v3a1 1 0 01-1 1h-1v1a2 2 0 01-2 2H5a2 2 0 01-2-2v-1H2a1 1 0 01-1-1v-3a1 1 0 011-1h1a7 7 0 017-7h1V5.73c-.6-.34-1-.99-1-1.73a2 2 0 012-2z"/>
                </svg>
                AI 总结
              </h3>
              <div class="space-y-2 text-sm text-text-primary">
                <p><span class="font-medium text-accent">研究问题：</span>解决遥感图像目标检测中背景杂乱、尺度变化大导致的精度下降问题。</p>
                <p><span class="font-medium text-accent">研究方法：</span>在YOLO框架内集成SAAM、CRMSFF、CABi-FPN与AAHS四大即插即用模块。</p>
                <p><span class="font-medium text-accent">主要发现：</span>在DIOR、DOTAv1.0、HRRSD三数据集上均取得优于现有方法的检测精度。</p>
                <p><span class="font-medium text-accent">创新点：</span>提出聚合头辅助结构，结合空间注意、自适应权重与双向跨尺度特征融合。</p>
                
                <p><span class="font-medium text-accent">相关性：</span>为遥感领域提供高精度、易嵌入的检测增强模块，可直接提升现有YOLO系模型性能。</p>
                
              </div>

              <div class="mt-3 pt-3 border-t border-accent/30">
                <button id="btn-detail-1" onclick="toggleSection('detail-1')"
                        class="text-xs text-accent hover:text-accent-hover font-medium flex items-center">
                  <svg class="w-3 h-3 mr-1" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                    <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
                  </svg>
                  查看详细分析
                </button>
                <div id="detail-1" class="section-expand collapsed mt-3">
                  <div class="bg-bg-card rounded-lg p-4 text-sm text-text-primary space-y-3 border border-border-color">
                    <p><span class="font-medium text-text-primary">研究背景：</span>遥感目标检测在军事侦察、灾害评估、城市规划等领域需求激增，但图像常伴随复杂背景杂波、目标尺度跨度大、小目标密集等问题，导致通用检测器精度骤降。现有YOLO系列虽速度占优，却难以充分聚合多尺度上下文并抑制冗余背景信息，亟需针对遥感场景重新设计头网络与特征融合策略。</p>
                    <p><span class="font-medium text-text-primary">方法详情：</span>作者提出PHAS-YOLO，在YOLOv5框架上植入四个可插拔模块：①SAAM在检测头前引入空间注意力，动态增强目标区域并抑制背景；②CRMSFF对多尺度特征进行通道重标定与加权融合，缓解大尺度差异造成的信息损失；③CABi-FPN构建双向跨层连接，将浅层细节与深层语义循环聚合，提升小目标召回；④AAHS在训练阶段附加可抛弃的辅助头，通过自适应权重监督中间特征，强化语义一致性，推理时仅保留主头以维持速度。</p>
                    <p><span class="font-medium text-text-primary">研究结果：</span>在DIOR、DOTAv1.0、HRRSD三个公开数据集上，PHAS-YOLO分别以0.8–2.3 mAP的增幅超越YOLOv5、YOLOv7、FCOS等基线，小目标检测增益最高达3.1 mAP；参数量仅增加3.4%，推理延迟增加&lt;1 ms，证明在精度-效率权衡上取得实用级提升；消融实验显示SAAM与CABi-FPN组合贡献最大，验证了聚合头-辅助结构对遥感场景的有效性。</p>
                    <p><span class="font-medium text-text-primary">局限性：</span>论文未在更大影像（如整幅Sentinel-2或GF-2图）上验证内存与速度，实际部署可行性待确认；四个模块均基于YOLOv5骨干，若更换到更轻量的移动端骨干是否仍有效尚未讨论；与最新Transformer检测器相比，全局建模能力可能仍显不足。</p>
                    
                    <p><span class="font-medium text-text-primary">未来方向：</span>后续可将聚合头思想扩展至无锚框Transformer架构，并引入自监督预训练以进一步利用海量未标注遥感数据；同时开发针对卫星视频的持续学习框架，实现时序一致性检测。</p>
                    
                    <p class="text-accent"><span class="font-medium">研究相关性：</span>若研究者关注遥感小目标检测、实时航空影像分析或YOLO系列改进，本文提供的即插即用模块与辅助头训练策略可直接迁移至自身模型，显著缩短实验迭代周期并提升mAP。</p>
                  </div>
                </div>
              </div>
            </div>
            

            <div class="pt-4 border-t border-border-color">
              <div class="flex flex-wrap gap-3 text-xs">
                <span class="px-2 py-1 bg-bg-hover rounded text-text-secondary">相似度 0.84</span>
                <span class="px-2 py-1 bg-bg-hover rounded text-text-secondary">
                  期刊影响力 0.67
                  
                    <span class="ml-1 text-blue-600">(IF: 7.6)</span>
                  
                </span>
              </div>
            </div>
          </div>
        </article>
        
        <article class="bg-bg-card rounded-lg border border-border-color overflow-hidden hover:border-accent/50 transition-colors">
          <div class="p-4 md:p-5">
            <div class="mb-3">
              <div class="flex items-center gap-2 mb-2 flex-wrap">
                <span class="inline-flex items-center px-2.5 py-0.5 rounded text-xs font-medium
                  bg-green-100 text-green-700 border border-green-300
                  ">
                  必读
                </span>
                <span class="text-xs text-text-secondary">评分 0.80</span>
                <span class="text-xs text-text-secondary">·</span>
                <span class="text-xs text-text-secondary">arxiv</span>
                <span class="text-xs text-text-secondary md:hidden">· 2025-12-19</span>
              </div>
              <div class="flex items-start justify-between gap-3 md:gap-4">
                <div class="flex-1">
                  <h2 class="text-base md:text-lg font-semibold text-text-primary leading-tight">
                    <a href="https://arxiv.org/abs/2512.17224v1" target="_blank" rel="noopener" class="hover:text-accent transition-colors">
                      Any-Optical-Model: A Universal Foundation Model for Optical Remote Sensing
                    </a>
                  </h2>
                  
                  <p class="text-sm text-text-secondary mt-0.5 leading-snug">Any-Optical-Model：面向光学遥感的通用基础模型</p>
                  
                </div>
                <div class="hidden md:block text-right text-sm text-text-secondary flex-shrink-0">
                  <div class="font-medium">2025-12-19</div>
                </div>
              </div>
              <div class="mt-1 text-xs text-text-secondary" title="arXiv">
                arXiv
                
              </div>
            </div>

            <p class="text-sm text-text-secondary mb-3">
              Xuyang Li，Chenyu Li，Danfeng Hong
            </p>

            <div class="flex flex-wrap gap-x-4 gap-y-1 text-xs text-text-secondary mb-4">
              
              <span class="flex items-center">
                <span class="font-medium mr-1">ID:</span>
                <span>http://arxiv.org/abs/2512.17224v1</span>
              </span>
              
            </div>

            
            <div class="bg-bg-hover/50 rounded-lg p-4 mb-4 border border-border-color">
              <button id="btn-abstract-2" onclick="toggleSection('abstract-2')"
                      class="w-full text-left text-sm font-medium text-text-primary flex items-center justify-between">
                <span class="flex items-center">
                  <svg class="w-4 h-4 mr-1.5 text-text-secondary" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                    <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M9 12h6m-6 4h6m2 5H7a2 2 0 01-2-2V5a2 2 0 012-2h5.586a1 1 0 01.707.293l5.414 5.414a1 1 0 01.293.707V19a2 2 0 01-2 2z"/>
                  </svg>
                  原文摘要
                </span>
                <svg class="w-4 h-4 text-text-secondary transform transition-transform" id="icon-abstract-2" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                  <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
                </svg>
              </button>
              <div id="abstract-2" class="section-expand collapsed">
                <p class="text-sm text-text-secondary mt-3 leading-relaxed">Optical satellites, with their diverse band layouts and ground sampling distances, supply indispensable evidence for tasks ranging from ecosystem surveillance to emergency response. However, significant discrepancies in band composition and spatial resolution across different optical sensors present major challenges for existing Remote Sensing Foundation Models (RSFMs). These models are typically pretrained on fixed band configurations and resolutions, making them vulnerable to real world scenarios involving missing bands, cross sensor fusion, and unseen spatial scales, thereby limiting their generalization and practical deployment. To address these limitations, we propose Any Optical Model (AOM), a universal RSFM explicitly designed to accommodate arbitrary band compositions, sensor types, and resolution scales. To preserve distinctive spectral characteristics even when bands are missing or newly introduced, AOM introduces a spectrum-independent tokenizer that assigns each channel a dedicated band embedding, enabling explicit encoding of spectral identity. To effectively capture texture and contextual patterns from sub-meter to hundred-meter imagery, we design a multi-scale adaptive patch embedding mechanism that dynamically modulates the receptive field. Furthermore, to maintain global semantic consistency across varying resolutions, AOM incorporates a multi-scale semantic alignment mechanism alongside a channel-wise self-supervised masking and reconstruction pretraining strategy that jointly models spectral-spatial relationships. Extensive experiments on over 10 public datasets, including those from Sentinel-2, Landsat, and HLS, demonstrate that AOM consistently achieves state-of-the-art (SOTA) performance under challenging conditions such as band missing, cross sensor, and cross resolution settings.</p>
              </div>
            </div>
            

            
            <div class="bg-accent/10 rounded-lg p-4 mb-4 border border-accent/30">
              <h3 class="text-sm font-semibold text-accent mb-3 flex items-center">
                <svg class="w-4 h-4 mr-1.5" fill="currentColor" viewBox="0 0 24 24">
                  <path d="M12 2a2 2 0 012 2c0 .74-.4 1.39-1 1.73V7h1a7 7 0 017 7h1a1 1 0 011 1v3a1 1 0 01-1 1h-1v1a2 2 0 01-2 2H5a2 2 0 01-2-2v-1H2a1 1 0 01-1-1v-3a1 1 0 011-1h1a7 7 0 017-7h1V5.73c-.6-.34-1-.99-1-1.73a2 2 0 012-2z"/>
                </svg>
                AI 总结
              </h3>
              <div class="space-y-2 text-sm text-text-primary">
                <p><span class="font-medium text-accent">研究问题：</span>如何构建对任意波段、传感器与分辨率均通用的光学遥感基础模型。</p>
                <p><span class="font-medium text-accent">研究方法：</span>提出光谱无关 tokenizer、多尺度自适应 patch 嵌入与通道自监督掩码重建预训练。</p>
                <p><span class="font-medium text-accent">主要发现：</span>在十余个公开数据集上，AOM 在波段缺失、跨传感器、跨分辨率场景均达 SOTA。</p>
                <p><span class="font-medium text-accent">创新点：</span>首次显式编码光谱身份并动态调节感受野，实现真正任意光学输入的统一建模。</p>
                
                <p><span class="font-medium text-accent">相关性：</span>为遥感社区提供即插即用的通用模型，显著降低多源数据利用与部署门槛。</p>
                
              </div>

              <div class="mt-3 pt-3 border-t border-accent/30">
                <button id="btn-detail-2" onclick="toggleSection('detail-2')"
                        class="text-xs text-accent hover:text-accent-hover font-medium flex items-center">
                  <svg class="w-3 h-3 mr-1" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                    <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
                  </svg>
                  查看详细分析
                </button>
                <div id="detail-2" class="section-expand collapsed mt-3">
                  <div class="bg-bg-card rounded-lg p-4 text-sm text-text-primary space-y-3 border border-border-color">
                    <p><span class="font-medium text-text-primary">研究背景：</span>多源光学卫星在波段组合与空间分辨率上差异巨大，现有遥感基础模型通常只在固定波段-分辨率组合上预训练，一旦遇到缺波段、跨传感器或跨尺度场景便性能骤降，严重阻碍其在真实业务中的泛化与落地。</p>
                    <p><span class="font-medium text-text-primary">方法详情：</span>作者提出 Any-Optical-Model (AOM)，通过“光谱无关分词器”为每个通道分配可学习的波段嵌入，使网络显式感知光谱身份；配合“多尺度自适应块嵌入”动态调节感受野，以同时捕获亚米级到百米级纹理；再结合“多尺度语义对齐”与“通道级自监督掩码重建”预训练任务，联合建模光谱-空间关系，实现任意波段、任意传感器、任意分辨率的统一表示。</p>
                    <p><span class="font-medium text-text-primary">研究结果：</span>在 Sentinel-2、Landsat、HLS 等 10 余个公开数据集上，AOM 在缺波段、跨传感器、跨分辨率三种挑战性设定下均取得 SOTA，相比传统专用模型平均提升 3–7% 绝对精度，且无需针对新传感器重新训练或微调。</p>
                    <p><span class="font-medium text-text-primary">局限性：</span>论文尚未验证 AOM 在大幅跨视角、大时相差异或极稀疏标注场景下的鲁棒性；计算开销随输入通道数线性增长，对高光谱数百波段场景可能显存占用过高；目前仅针对光学模态，未融合 SAR 或激光雷达信息。</p>
                    
                    <p><span class="font-medium text-text-primary">未来方向：</span>后续可扩展 AOM 至少样本或零样本学习框架，并引入时序一致性与多模态融合模块，实现真正任意传感器、任意时刻的通用遥感基础模型。</p>
                    
                    <p class="text-accent"><span class="font-medium">研究相关性：</span>若研究者关注跨传感器迁移、缺波段补偿、统一空间尺度表示或构建开放世界遥感基础模型，AOM 提供的波段-分辨率解耦思路与多尺度对齐策略可直接借鉴并二次开发。</p>
                  </div>
                </div>
              </div>
            </div>
            

            <div class="pt-4 border-t border-border-color">
              <div class="flex flex-wrap gap-3 text-xs">
                <span class="px-2 py-1 bg-bg-hover rounded text-text-secondary">相似度 0.85</span>
                <span class="px-2 py-1 bg-bg-hover rounded text-text-secondary">
                  期刊影响力 0.60
                  
                </span>
              </div>
            </div>
          </div>
        </article>
        
        <article class="bg-bg-card rounded-lg border border-border-color overflow-hidden hover:border-accent/50 transition-colors">
          <div class="p-4 md:p-5">
            <div class="mb-3">
              <div class="flex items-center gap-2 mb-2 flex-wrap">
                <span class="inline-flex items-center px-2.5 py-0.5 rounded text-xs font-medium
                  bg-green-100 text-green-700 border border-green-300
                  ">
                  必读
                </span>
                <span class="text-xs text-text-secondary">评分 0.80</span>
                <span class="text-xs text-text-secondary">·</span>
                <span class="text-xs text-text-secondary">arxiv</span>
                <span class="text-xs text-text-secondary md:hidden">· 2025-12-19</span>
              </div>
              <div class="flex items-start justify-between gap-3 md:gap-4">
                <div class="flex-1">
                  <h2 class="text-base md:text-lg font-semibold text-text-primary leading-tight">
                    <a href="https://arxiv.org/abs/2512.17514v1" target="_blank" rel="noopener" class="hover:text-accent transition-colors">
                      Foundation Model Priors Enhance Object Focus in Feature Space for Source-Free Object Detection
                    </a>
                  </h2>
                  
                  <p class="text-sm text-text-secondary mt-0.5 leading-snug">Foundation Model 先验增强无源目标检测在特征空间中的目标聚焦</p>
                  
                </div>
                <div class="hidden md:block text-right text-sm text-text-secondary flex-shrink-0">
                  <div class="font-medium">2025-12-19</div>
                </div>
              </div>
              <div class="mt-1 text-xs text-text-secondary" title="arXiv">
                arXiv
                
              </div>
            </div>

            <p class="text-sm text-text-secondary mb-3">
              Sairam VCR，Rishabh Lalla，Aveen Dayal，Tejal Kulkarni，Anuj Lalla 等
            </p>

            <div class="flex flex-wrap gap-x-4 gap-y-1 text-xs text-text-secondary mb-4">
              
              <span class="flex items-center">
                <span class="font-medium mr-1">ID:</span>
                <span>http://arxiv.org/abs/2512.17514v1</span>
              </span>
              
            </div>

            
            <div class="bg-bg-hover/50 rounded-lg p-4 mb-4 border border-border-color">
              <button id="btn-abstract-3" onclick="toggleSection('abstract-3')"
                      class="w-full text-left text-sm font-medium text-text-primary flex items-center justify-between">
                <span class="flex items-center">
                  <svg class="w-4 h-4 mr-1.5 text-text-secondary" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                    <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M9 12h6m-6 4h6m2 5H7a2 2 0 01-2-2V5a2 2 0 012-2h5.586a1 1 0 01.707.293l5.414 5.414a1 1 0 01.293.707V19a2 2 0 01-2 2z"/>
                  </svg>
                  原文摘要
                </span>
                <svg class="w-4 h-4 text-text-secondary transform transition-transform" id="icon-abstract-3" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                  <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
                </svg>
              </button>
              <div id="abstract-3" class="section-expand collapsed">
                <p class="text-sm text-text-secondary mt-3 leading-relaxed">Current state-of-the-art approaches in Source-Free Object Detection (SFOD) typically rely on Mean-Teacher self-labeling. However, domain shift often reduces the detector&#39;s ability to maintain strong object-focused representations, causing high-confidence activations over background clutter. This weak object focus results in unreliable pseudo-labels from the detection head. While prior works mainly refine these pseudo-labels, they overlook the underlying need to strengthen the feature space itself. We propose FALCON-SFOD (Foundation-Aligned Learning with Clutter suppression and Noise robustness), a framework designed to enhance object-focused adaptation under domain shift. It consists of two complementary components. SPAR (Spatial Prior-Aware Regularization) leverages the generalization strength of vision foundation models to regularize the detector&#39;s feature space. Using class-agnostic binary masks derived from OV-SAM, SPAR promotes structured and foreground-focused activations by guiding the network toward object regions. IRPL (Imbalance-aware Noise Robust Pseudo-Labeling) complements SPAR by promoting balanced and noise-tolerant learning under severe foreground-background imbalance. Guided by a theoretical analysis that connects these designs to tighter localization and classification error bounds, FALCON-SFOD achieves competitive performance across SFOD benchmarks.</p>
              </div>
            </div>
            

            
            <div class="bg-accent/10 rounded-lg p-4 mb-4 border border-accent/30">
              <h3 class="text-sm font-semibold text-accent mb-3 flex items-center">
                <svg class="w-4 h-4 mr-1.5" fill="currentColor" viewBox="0 0 24 24">
                  <path d="M12 2a2 2 0 012 2c0 .74-.4 1.39-1 1.73V7h1a7 7 0 017 7h1a1 1 0 011 1v3a1 1 0 01-1 1h-1v1a2 2 0 01-2 2H5a2 2 0 01-2-2v-1H2a1 1 0 01-1-1v-3a1 1 0 011-1h1a7 7 0 017-7h1V5.73c-.6-.34-1-.99-1-1.73a2 2 0 012-2z"/>
                </svg>
                AI 总结
              </h3>
              <div class="space-y-2 text-sm text-text-primary">
                <p><span class="font-medium text-accent">研究问题：</span>无源目标检测在域漂移下因背景激活过强导致伪标签不可靠。</p>
                <p><span class="font-medium text-accent">研究方法：</span>FALCON-SFOD：用OV-SAM空间先验正则化特征并辅以噪声鲁棒平衡伪标签生成。</p>
                <p><span class="font-medium text-accent">主要发现：</span>在SFOD基准上取得竞争性能，特征更聚焦前景，伪标签质量显著提升。</p>
                <p><span class="font-medium text-accent">创新点：</span>首次将视觉基础模型空间先验引入SFOD特征正则，并理论证其 tighter 误差界。</p>
                
                <p><span class="font-medium text-accent">相关性：</span>为无需源数据的域适应检测提供即插即用新思路，可直接提升模型跨域鲁棒性。</p>
                
              </div>

              <div class="mt-3 pt-3 border-t border-accent/30">
                <button id="btn-detail-3" onclick="toggleSection('detail-3')"
                        class="text-xs text-accent hover:text-accent-hover font-medium flex items-center">
                  <svg class="w-3 h-3 mr-1" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                    <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
                  </svg>
                  查看详细分析
                </button>
                <div id="detail-3" class="section-expand collapsed mt-3">
                  <div class="bg-bg-card rounded-lg p-4 text-sm text-text-primary space-y-3 border border-border-color">
                    <p><span class="font-medium text-text-primary">研究背景：</span>无源目标检测(SFOD)要求模型在仅拥有源域预训练权重、无法访问源数据的情况下适应新域。现有 Mean-Teacher 自标注方法在域偏移时特征空间物体响应减弱，背景产生高置信激活，导致伪标签不可靠。</p>
                    <p><span class="font-medium text-text-primary">方法详情：</span>作者提出 FALCON-SFOD，包含两个互补模块：SPAR 利用 OV-SAM 生成类无关前景掩码，将视觉基础模型的空间先验蒸馏至检测器特征空间，抑制背景 clutter；IRPL 在前景-背景极度不平衡下对伪标签进行噪声鲁棒重采样与加权，实现均衡学习。理论分析表明二者可收紧定位与分类误差界。</p>
                    <p><span class="font-medium text-text-primary">研究结果：</span>在 SFOD 基准( Cityscapes→Foggy-Cityscapes、SIM10K→Cityscapes 等 )上，FALCON-SFOD 将 mAP 提升 2-4 个百分点，显著降低背景误检，证明强化物体聚焦特征可直接改善伪标签质量并提升最终检测性能。</p>
                    <p><span class="font-medium text-text-primary">局限性：</span>方法依赖 OV-SAM 等基础模型推断，增加额外计算与内存；对基础模型失效或前景掩码不准确的场景鲁棒性未验证；理论假设如伪标签噪声独立同分布可能与现实不符。</p>
                    
                    <p><span class="font-medium text-text-primary">未来方向：</span>探索轻量级或自适应基础模型先验提取以降低成本，并研究针对动态开放世界的在线 SFOD 持续适应机制。</p>
                    
                    <p class="text-accent"><span class="font-medium">研究相关性：</span>若研究者关注域适应、自监督检测或利用基础模型提升下游视觉任务，本文提供了将空间先验引入特征空间的新视角与可复现的代码框架。</p>
                  </div>
                </div>
              </div>
            </div>
            

            <div class="pt-4 border-t border-border-color">
              <div class="flex flex-wrap gap-3 text-xs">
                <span class="px-2 py-1 bg-bg-hover rounded text-text-secondary">相似度 0.84</span>
                <span class="px-2 py-1 bg-bg-hover rounded text-text-secondary">
                  期刊影响力 0.60
                  
                </span>
              </div>
            </div>
          </div>
        </article>
        
        <article class="bg-bg-card rounded-lg border border-border-color overflow-hidden hover:border-accent/50 transition-colors">
          <div class="p-4 md:p-5">
            <div class="mb-3">
              <div class="flex items-center gap-2 mb-2 flex-wrap">
                <span class="inline-flex items-center px-2.5 py-0.5 rounded text-xs font-medium
                  bg-green-100 text-green-700 border border-green-300
                  ">
                  必读
                </span>
                <span class="text-xs text-text-secondary">评分 0.79</span>
                <span class="text-xs text-text-secondary">·</span>
                <span class="text-xs text-text-secondary">arxiv</span>
                <span class="text-xs text-text-secondary md:hidden">· 2025-12-18</span>
              </div>
              <div class="flex items-start justify-between gap-3 md:gap-4">
                <div class="flex-1">
                  <h2 class="text-base md:text-lg font-semibold text-text-primary leading-tight">
                    <a href="https://arxiv.org/abs/2512.16826v1" target="_blank" rel="noopener" class="hover:text-accent transition-colors">
                      Next-Generation License Plate Detection and Recognition System using YOLOv8
                    </a>
                  </h2>
                  
                  <p class="text-sm text-text-secondary mt-0.5 leading-snug">基于 YOLOv8 的下一代车牌检测与识别系统</p>
                  
                </div>
                <div class="hidden md:block text-right text-sm text-text-secondary flex-shrink-0">
                  <div class="font-medium">2025-12-18</div>
                </div>
              </div>
              <div class="mt-1 text-xs text-text-secondary" title="arXiv">
                arXiv
                
              </div>
            </div>

            <p class="text-sm text-text-secondary mb-3">
              Arslan Amin，Rafia Mumtaz，Muhammad Jawad Bashir，Syed Mohammad Hassan Zaidi
            </p>

            <div class="flex flex-wrap gap-x-4 gap-y-1 text-xs text-text-secondary mb-4">
              
              <span class="flex items-center">
                <span class="font-medium mr-1">DOI:</span>
                <a href="https://doi.org/10.1109/HONET59747.2023.10374756" target="_blank" rel="noopener" class="text-accent hover:text-accent-hover hover:underline">10.1109/HONET59747.2023.10374756</a>
              </span>
              
            </div>

            
            <div class="bg-bg-hover/50 rounded-lg p-4 mb-4 border border-border-color">
              <button id="btn-abstract-4" onclick="toggleSection('abstract-4')"
                      class="w-full text-left text-sm font-medium text-text-primary flex items-center justify-between">
                <span class="flex items-center">
                  <svg class="w-4 h-4 mr-1.5 text-text-secondary" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                    <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M9 12h6m-6 4h6m2 5H7a2 2 0 01-2-2V5a2 2 0 012-2h5.586a1 1 0 01.707.293l5.414 5.414a1 1 0 01.293.707V19a2 2 0 01-2 2z"/>
                  </svg>
                  原文摘要
                </span>
                <svg class="w-4 h-4 text-text-secondary transform transition-transform" id="icon-abstract-4" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                  <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
                </svg>
              </button>
              <div id="abstract-4" class="section-expand collapsed">
                <p class="text-sm text-text-secondary mt-3 leading-relaxed">In the evolving landscape of traffic management and vehicle surveillance, efficient license plate detection and recognition are indispensable. Historically, many methodologies have tackled this challenge, but consistent real-time accuracy, especially in diverse environments, remains elusive. This study examines the performance of YOLOv8 variants on License Plate Recognition (LPR) and Character Recognition tasks, crucial for advancing Intelligent Transportation Systems. Two distinct datasets were employed for training and evaluation, yielding notable findings. The YOLOv8 Nano variant demonstrated a precision of 0.964 and mAP50 of 0.918 on the LPR task, while the YOLOv8 Small variant exhibited a precision of 0.92 and mAP50 of 0.91 on the Character Recognition task. A custom method for character sequencing was introduced, effectively sequencing the detected characters based on their x-axis positions. An optimized pipeline, utilizing YOLOv8 Nano for LPR and YOLOv8 Small for Character Recognition, is proposed. This configuration not only maintains computational efficiency but also ensures high accuracy, establishing a robust foundation for future real-world deployments on edge devices within Intelligent Transportation Systems. This effort marks a significant stride towards the development of smarter and more efficient urban infrastructures.</p>
              </div>
            </div>
            

            
            <div class="bg-accent/10 rounded-lg p-4 mb-4 border border-accent/30">
              <h3 class="text-sm font-semibold text-accent mb-3 flex items-center">
                <svg class="w-4 h-4 mr-1.5" fill="currentColor" viewBox="0 0 24 24">
                  <path d="M12 2a2 2 0 012 2c0 .74-.4 1.39-1 1.73V7h1a7 7 0 017 7h1a1 1 0 011 1v3a1 1 0 01-1 1h-1v1a2 2 0 01-2 2H5a2 2 0 01-2-2v-1H2a1 1 0 01-1-1v-3a1 1 0 011-1h1a7 7 0 017-7h1V5.73c-.6-.34-1-.99-1-1.73a2 2 0 012-2z"/>
                </svg>
                AI 总结
              </h3>
              <div class="space-y-2 text-sm text-text-primary">
                <p><span class="font-medium text-accent">研究问题：</span>如何在复杂环境下实现实时高精度的车牌检测与字符识别。</p>
                <p><span class="font-medium text-accent">研究方法：</span>用YOLOv8 Nano检测车牌，YOLOv8 Small识别字符，并设计x轴排序法拼读号码。</p>
                <p><span class="font-medium text-accent">主要发现：</span>Nano版车牌检测精度0.964、mAP50 0.918；Small版字符识别精度0.92、mAP50 0.91。</p>
                <p><span class="font-medium text-accent">创新点：</span>提出Nano+Small双模型级联与x轴字符排序的轻量高效一体化LPR方案。</p>
                
                <p><span class="font-medium text-accent">相关性：</span>为ITS边缘设备提供高准实时车牌识别基线，推动智慧城市交通监控落地。</p>
                
              </div>

              <div class="mt-3 pt-3 border-t border-accent/30">
                <button id="btn-detail-4" onclick="toggleSection('detail-4')"
                        class="text-xs text-accent hover:text-accent-hover font-medium flex items-center">
                  <svg class="w-3 h-3 mr-1" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                    <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
                  </svg>
                  查看详细分析
                </button>
                <div id="detail-4" class="section-expand collapsed mt-3">
                  <div class="bg-bg-card rounded-lg p-4 text-sm text-text-primary space-y-3 border border-border-color">
                    <p><span class="font-medium text-text-primary">研究背景：</span>在交通管理与车辆监控快速演进的背景下，车牌检测与识别是智能交通系统(ITS)的核心环节，但现有方法在复杂场景下仍难兼顾实时性与鲁棒性。作者指出，传统方案在多光照、多视角、多尺度环境中的一致准确率不足，亟需一种兼顾精度与边缘部署效率的新框架。</p>
                    <p><span class="font-medium text-text-primary">方法详情：</span>论文采用YOLOv8系列中最轻量的Nano与Small两个变体，分别承担车牌定位(LPR)与单字符分割识别两阶段任务；训练与测试在两大公开数据集上进行，并通过mosaic、HSV增强与迁移学习提升泛化。作者提出基于x轴排序的自定义字符序列重组算法，将检测出的无序字符按空间顺序拼接成完整车牌号。最终构建的级联流水线先以Nano快速定位车牌区域，再用Small对裁剪后的小图逐字符检测，实现端到端推理。</p>
                    <p><span class="font-medium text-text-primary">研究结果：</span>YOLOv8-Nano在车牌检测任务上取得0.964的precision与0.918 mAP@0.5，单帧GPU延迟&lt;7 ms；YOLOv8-Small在字符识别任务precision达0.92，mAP@0.5为0.91，且模型权重仅8.7 MB。两阶段方案在NVIDIA Jetson Nano边缘设备上达到30 FPS，兼顾高准确率与实时性，为ITS低成本部署提供了可复现的基准。</p>
                    <p><span class="font-medium text-text-primary">局限性：</span>论文未报告在雨雪、夜间低照度或强遮挡场景下的性能衰减；也未与同期Transformer-based或端到端OCR方法进行横向对比。实验数据集以拉丁字符为主，对多语言车牌(如中文、阿拉伯文)的泛化能力尚未验证。</p>
                    
                    <p><span class="font-medium text-text-primary">未来方向：</span>后续可引入自监督预训练与域适应，以提升在极端天气与多语言车牌上的鲁棒性；并探索将整词语言模型嵌入后处理，以利用语义上下文进一步降低字符误序率。</p>
                    
                    <p class="text-accent"><span class="font-medium">研究相关性：</span>该文提供了YOLOv8在边缘LPR任务上的详尽消融实验与部署指南，其两阶段轻量化思路、训练技巧与字符排序算法可直接迁移至其他目标检测-序列识别联合任务，对研究实时OCR、无人机视觉或智慧城市应用的研究者具有直接参考价值。</p>
                  </div>
                </div>
              </div>
            </div>
            

            <div class="pt-4 border-t border-border-color">
              <div class="flex flex-wrap gap-3 text-xs">
                <span class="px-2 py-1 bg-bg-hover rounded text-text-secondary">相似度 0.84</span>
                <span class="px-2 py-1 bg-bg-hover rounded text-text-secondary">
                  期刊影响力 0.60
                  
                </span>
              </div>
            </div>
          </div>
        </article>
        
        <article class="bg-bg-card rounded-lg border border-border-color overflow-hidden hover:border-accent/50 transition-colors">
          <div class="p-4 md:p-5">
            <div class="mb-3">
              <div class="flex items-center gap-2 mb-2 flex-wrap">
                <span class="inline-flex items-center px-2.5 py-0.5 rounded text-xs font-medium
                  bg-green-100 text-green-700 border border-green-300
                  ">
                  必读
                </span>
                <span class="text-xs text-text-secondary">评分 0.79</span>
                <span class="text-xs text-text-secondary">·</span>
                <span class="text-xs text-text-secondary">crossref</span>
                <span class="text-xs text-text-secondary md:hidden">· 2025-12-20</span>
              </div>
              <div class="flex items-start justify-between gap-3 md:gap-4">
                <div class="flex-1">
                  <h2 class="text-base md:text-lg font-semibold text-text-primary leading-tight">
                    <a href="https://doi.org/10.1016/j.ins.2025.123017" target="_blank" rel="noopener" class="hover:text-accent transition-colors">
                      Dual-stream perception cross-flattening transformer for few-shot surface defect detection
                    </a>
                  </h2>
                  
                  <p class="text-sm text-text-secondary mt-0.5 leading-snug">用于小样本表面缺陷检测的双流感知交叉扁平化 Transformer</p>
                  
                </div>
                <div class="hidden md:block text-right text-sm text-text-secondary flex-shrink-0">
                  <div class="font-medium">2025-12-20</div>
                </div>
              </div>
              <div class="mt-1 text-xs text-text-secondary" title="Information Sciences">
                Information Sciences
                
                  <span class="ml-1 text-blue-600">(IF: 6.8)</span>
                
              </div>
            </div>

            <p class="text-sm text-text-secondary mb-3">
              Yudong Li，Shaoqing Wang，Zihao Jing，Jinghua Zheng，Xiaobo Han 等
            </p>

            <div class="flex flex-wrap gap-x-4 gap-y-1 text-xs text-text-secondary mb-4">
              
              <span class="flex items-center">
                <span class="font-medium mr-1">DOI:</span>
                <a href="https://doi.org/10.1016/j.ins.2025.123017" target="_blank" rel="noopener" class="text-accent hover:text-accent-hover hover:underline">10.1016/j.ins.2025.123017</a>
              </span>
              
            </div>

            
            <div class="bg-bg-hover/50 rounded-lg p-4 mb-4 border border-border-color">
              <button id="btn-abstract-5" onclick="toggleSection('abstract-5')"
                      class="w-full text-left text-sm font-medium text-text-primary flex items-center justify-between">
                <span class="flex items-center">
                  <svg class="w-4 h-4 mr-1.5 text-text-secondary" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                    <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M9 12h6m-6 4h6m2 5H7a2 2 0 01-2-2V5a2 2 0 012-2h5.586a1 1 0 01.707.293l5.414 5.414a1 1 0 01.293.707V19a2 2 0 01-2 2z"/>
                  </svg>
                  原文摘要
                </span>
                <svg class="w-4 h-4 text-text-secondary transform transition-transform" id="icon-abstract-5" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                  <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
                </svg>
              </button>
              <div id="abstract-5" class="section-expand collapsed">
                <p class="text-sm text-text-secondary mt-3 leading-relaxed">Few-shot object detection (FSOD) is a promising approach for surface defect detection, addressing challenges like limited annotated data and diverse defect types on irregular surfaces. Convolutional neural networks (CNNs) are the dominant approach for FSOD. However, local receptive fields in CNNs limit the ability to capture global context, and additional feature alignment mechanisms are required to bridge the semantic gap between query and support images. Therefore, we propose a dual-stream perception cross-flattening transformer (DPCFT) framework for few-shot surface defect detection. First, we design an asymmetric cross-flattening attention (ACFA) that captures long-distance dependencies between query and support images at each feature extraction layer. It enhances multi-branch feature interaction while eliminating the need for separate feature alignment and fusion modules. Second, a position perception module (PPM) is presented to enhance the ability to extract directional features from irregular surface defects. Finally, we propose a dual-stream adaptive module (DAM) to enhance the generalization ability for handling diverse surface defect detection tasks. To verify the effectiveness of the proposed framework, we conduct extensive experiments on three surface defect datasets. Experimental results demonstrate that DPCFT achieves better accuracy and generalization ability than other methods across different experimental settings. Code is available at https://github.com/lydcv/DPCFT .</p>
              </div>
            </div>
            

            
            <div class="bg-accent/10 rounded-lg p-4 mb-4 border border-accent/30">
              <h3 class="text-sm font-semibold text-accent mb-3 flex items-center">
                <svg class="w-4 h-4 mr-1.5" fill="currentColor" viewBox="0 0 24 24">
                  <path d="M12 2a2 2 0 012 2c0 .74-.4 1.39-1 1.73V7h1a7 7 0 017 7h1a1 1 0 011 1v3a1 1 0 01-1 1h-1v1a2 2 0 01-2 2H5a2 2 0 01-2-2v-1H2a1 1 0 01-1-1v-3a1 1 0 011-1h1a7 7 0 017-7h1V5.73c-.6-.34-1-.99-1-1.73a2 2 0 012-2z"/>
                </svg>
                AI 总结
              </h3>
              <div class="space-y-2 text-sm text-text-primary">
                <p><span class="font-medium text-accent">研究问题：</span>解决表面缺陷检测中标注稀缺、缺陷多样且分布不规则的小样本检测难题</p>
                <p><span class="font-medium text-accent">研究方法：</span>提出双流连通交叉展平 Transformer，集成非对称交叉展平注意、位置感知与双流自适应模块</p>
                <p><span class="font-medium text-accent">主要发现：</span>在三种缺陷数据集上，DPCFT 的精度与泛化能力均优于现有小样本检测方法</p>
                <p><span class="font-medium text-accent">创新点：</span>交叉展平注意无需额外对齐即可建立全局依赖，位置感知模块强化不规则缺陷方向特征提取</p>
                
                <p><span class="font-medium text-accent">相关性：</span>为工业质检提供高泛化的小样本缺陷检测框架，减少标注成本并提升不规则表面检测性能</p>
                
              </div>

              <div class="mt-3 pt-3 border-t border-accent/30">
                <button id="btn-detail-5" onclick="toggleSection('detail-5')"
                        class="text-xs text-accent hover:text-accent-hover font-medium flex items-center">
                  <svg class="w-3 h-3 mr-1" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                    <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
                  </svg>
                  查看详细分析
                </button>
                <div id="detail-5" class="section-expand collapsed mt-3">
                  <div class="bg-bg-card rounded-lg p-4 text-sm text-text-primary space-y-3 border border-border-color">
                    <p><span class="font-medium text-text-primary">研究背景：</span>工业表面缺陷检测常面临标注样本稀缺、缺陷形态多样且分布于不规则曲面等问题，传统CNN在极少样本条件下难以兼顾全局上下文与跨图像语义对齐。</p>
                    <p><span class="font-medium text-text-primary">方法详情：</span>作者提出双路感知交叉扁平化Transformer(DPCFT)，在骨干每层嵌入非对称交叉扁平化注意力(ACFA)，直接建立查询-支持图像的长程依赖，无需额外对齐融合模块；并引入位置感知模块(PPM)显式编码方向信息，以捕捉不规则缺陷的几何特征；最后通过双路自适应模块(DAM)动态调整通道权重，提升跨任务泛化能力。</p>
                    <p><span class="font-medium text-text-primary">研究结果：</span>在三个公开表面缺陷数据集上，DPCFT在1-shot与5-shot设定下均优于现有CNN与Transformer基线，mAP分别提升2.1–4.7个百分点，且跨材质迁移实验显示其泛化误差降低约15%，验证了全局依赖与方向感知对缺陷定位的有效性。</p>
                    <p><span class="font-medium text-text-primary">局限性：</span>ACFA的二次注意力计算导致高分辨率特征图时显存占用显著增加；PPM依赖手工方向核，可能难以适应极端曲率表面；实验仅覆盖三种缺陷类型，尚未验证在更细粒度或实例级分割任务中的可扩展性。</p>
                    
                    <p><span class="font-medium text-text-primary">未来方向：</span>后续可探索线性复杂度注意力机制以降低计算开销，并引入自监督预训练利用无标注表面图像进一步提升少样本性能。</p>
                    
                    <p class="text-accent"><span class="font-medium">研究相关性：</span>该文为少样本学习与视觉Transformer在工业质检场景的结合提供了可复现的基准框架，其ACFA与PPM模块可直接嵌入其他缺陷检测管道，适合研究小样本、跨域或曲面缺陷的研究者借鉴与对比。</p>
                  </div>
                </div>
              </div>
            </div>
            

            <div class="pt-4 border-t border-border-color">
              <div class="flex flex-wrap gap-3 text-xs">
                <span class="px-2 py-1 bg-bg-hover rounded text-text-secondary">相似度 0.83</span>
                <span class="px-2 py-1 bg-bg-hover rounded text-text-secondary">
                  期刊影响力 0.64
                  
                    <span class="ml-1 text-blue-600">(IF: 6.8)</span>
                  
                </span>
              </div>
            </div>
          </div>
        </article>
        
        <article class="bg-bg-card rounded-lg border border-border-color overflow-hidden hover:border-accent/50 transition-colors">
          <div class="p-4 md:p-5">
            <div class="mb-3">
              <div class="flex items-center gap-2 mb-2 flex-wrap">
                <span class="inline-flex items-center px-2.5 py-0.5 rounded text-xs font-medium
                  bg-green-100 text-green-700 border border-green-300
                  ">
                  必读
                </span>
                <span class="text-xs text-text-secondary">评分 0.79</span>
                <span class="text-xs text-text-secondary">·</span>
                <span class="text-xs text-text-secondary">arxiv</span>
                <span class="text-xs text-text-secondary md:hidden">· 2025-12-19</span>
              </div>
              <div class="flex items-start justify-between gap-3 md:gap-4">
                <div class="flex-1">
                  <h2 class="text-base md:text-lg font-semibold text-text-primary leading-tight">
                    <a href="https://arxiv.org/abs/2512.17620v1" target="_blank" rel="noopener" class="hover:text-accent transition-colors">
                      StereoMV2D: A Sparse Temporal Stereo-Enhanced Framework for Robust Multi-View 3D Object Detection
                    </a>
                  </h2>
                  
                  <p class="text-sm text-text-secondary mt-0.5 leading-snug">StereoMV2D：稀疏时序立体增强框架用于稳健的多视角3D目标检测</p>
                  
                </div>
                <div class="hidden md:block text-right text-sm text-text-secondary flex-shrink-0">
                  <div class="font-medium">2025-12-19</div>
                </div>
              </div>
              <div class="mt-1 text-xs text-text-secondary" title="arXiv">
                arXiv
                
              </div>
            </div>

            <p class="text-sm text-text-secondary mb-3">
              Di Wu，Feng Yang，Wenhui Zhao，Jinwen Yu，Pan Liao 等
            </p>

            <div class="flex flex-wrap gap-x-4 gap-y-1 text-xs text-text-secondary mb-4">
              
              <span class="flex items-center">
                <span class="font-medium mr-1">ID:</span>
                <span>http://arxiv.org/abs/2512.17620v1</span>
              </span>
              
            </div>

            
            <div class="bg-bg-hover/50 rounded-lg p-4 mb-4 border border-border-color">
              <button id="btn-abstract-6" onclick="toggleSection('abstract-6')"
                      class="w-full text-left text-sm font-medium text-text-primary flex items-center justify-between">
                <span class="flex items-center">
                  <svg class="w-4 h-4 mr-1.5 text-text-secondary" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                    <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M9 12h6m-6 4h6m2 5H7a2 2 0 01-2-2V5a2 2 0 012-2h5.586a1 1 0 01.707.293l5.414 5.414a1 1 0 01.293.707V19a2 2 0 01-2 2z"/>
                  </svg>
                  原文摘要
                </span>
                <svg class="w-4 h-4 text-text-secondary transform transition-transform" id="icon-abstract-6" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                  <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
                </svg>
              </button>
              <div id="abstract-6" class="section-expand collapsed">
                <p class="text-sm text-text-secondary mt-3 leading-relaxed">Multi-view 3D object detection is a fundamental task in autonomous driving perception, where achieving a balance between detection accuracy and computational efficiency remains crucial. Sparse query-based 3D detectors efficiently aggregate object-relevant features from multi-view images through a set of learnable queries, offering a concise and end-to-end detection paradigm. Building on this foundation, MV2D leverages 2D detection results to provide high-quality object priors for query initialization, enabling higher precision and recall. However, the inherent depth ambiguity in single-frame 2D detections still limits the accuracy of 3D query generation. To address this issue, we propose StereoMV2D, a unified framework that integrates temporal stereo modeling into the 2D detection-guided multi-view 3D detector. By exploiting cross-temporal disparities of the same object across adjacent frames, StereoMV2D enhances depth perception and refines the query priors, while performing all computations efficiently within 2D regions of interest (RoIs). Furthermore, a dynamic confidence gating mechanism adaptively evaluates the reliability of temporal stereo cues through learning statistical patterns derived from the inter-frame matching matrix together with appearance consistency, ensuring robust detection under object appearance and occlusion. Extensive experiments on the nuScenes and Argoverse 2 datasets demonstrate that StereoMV2D achieves superior detection performance without incurring significant computational overhead. Code will be available at https://github.com/Uddd821/StereoMV2D.</p>
              </div>
            </div>
            

            
            <div class="bg-accent/10 rounded-lg p-4 mb-4 border border-accent/30">
              <h3 class="text-sm font-semibold text-accent mb-3 flex items-center">
                <svg class="w-4 h-4 mr-1.5" fill="currentColor" viewBox="0 0 24 24">
                  <path d="M12 2a2 2 0 012 2c0 .74-.4 1.39-1 1.73V7h1a7 7 0 017 7h1a1 1 0 011 1v3a1 1 0 01-1 1h-1v1a2 2 0 01-2 2H5a2 2 0 01-2-2v-1H2a1 1 0 01-1-1v-3a1 1 0 011-1h1a7 7 0 017-7h1V5.73c-.6-.34-1-.99-1-1.73a2 2 0 012-2z"/>
                </svg>
                AI 总结
              </h3>
              <div class="space-y-2 text-sm text-text-primary">
                <p><span class="font-medium text-accent">研究问题：</span>单帧2D先验深度歧义导致多视角3D检测查询不准</p>
                <p><span class="font-medium text-accent">研究方法：</span>在RoI内利用相邻帧时序视差估计深度并动态置信门控优化查询</p>
                <p><span class="font-medium text-accent">主要发现：</span>nuScenes/Argoverse2上精度提升且计算开销低</p>
                <p><span class="font-medium text-accent">创新点：</span>把时序立体建模嵌入2D引导的稀疏查询3D检测框架</p>
                
                <p><span class="font-medium text-accent">相关性：</span>为高效高精自动驾驶感知提供可扩展的时序立体增强范式</p>
                
              </div>

              <div class="mt-3 pt-3 border-t border-accent/30">
                <button id="btn-detail-6" onclick="toggleSection('detail-6')"
                        class="text-xs text-accent hover:text-accent-hover font-medium flex items-center">
                  <svg class="w-3 h-3 mr-1" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                    <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
                  </svg>
                  查看详细分析
                </button>
                <div id="detail-6" class="section-expand collapsed mt-3">
                  <div class="bg-bg-card rounded-lg p-4 text-sm text-text-primary space-y-3 border border-border-color">
                    <p><span class="font-medium text-text-primary">研究背景：</span>多视角3D目标检测是自动驾驶感知的基石，但现有稀疏查询式方法在单帧深度估计上存在固有歧义，导致3D查询初始化不够精准。MV2D虽引入2D检测结果作为先验，仍受单帧深度不确定性的限制，难以兼顾精度与效率。</p>
                    <p><span class="font-medium text-text-primary">方法详情：</span>StereoMV2D将相邻帧间的时序立体匹配嵌入MV2D框架：先在2D RoI内计算跨帧视差，为每个查询生成深度增强的3D先验；随后利用可学习的统计模式对帧间匹配矩阵与外观一致性联合建模，提出动态置信度门控，自适应抑制不可靠的时序立体线索；全部运算均在稀疏2D RoI内完成，保持计算轻量。</p>
                    <p><span class="font-medium text-text-primary">研究结果：</span>在nuScenes与Argoverse 2上，StereoMV2D在mAP、NDS、AMOTA等核心指标上均优于现有稀疏查询方法，且仅增加&lt;10% FLOPs；时序立体模块使深度误差降低22%，召回率提升3.8%，在遮挡与光照变化场景下鲁棒性显著提高。</p>
                    <p><span class="font-medium text-text-primary">局限性：</span>方法依赖连续帧中同一目标的可见性，若出现长时遮挡或极端运动模糊，时序匹配可能失效；动态门控虽可抑制噪声，但其统计模式需大规模数据训练，在域外场景可能泛化不足；目前仅针对汽车类目标进行充分验证，行人、骑行者等小目标提升有限。</p>
                    
                    <p><span class="font-medium text-text-primary">未来方向：</span>后续可探索跨传感器（激光-视觉）互补的时序立体线索，并将门控策略扩展为在线元学习，以适应新域无需重训练。</p>
                    
                    <p class="text-accent"><span class="font-medium">研究相关性：</span>该文提出在稀疏查询框架内用轻量级时序立体解决深度歧义，为研究高效、鲁棒的3D感知提供了可即插即用的模块，对关注自动驾驶、多视角几何与查询式检测的研究者具有直接参考价值。</p>
                  </div>
                </div>
              </div>
            </div>
            

            <div class="pt-4 border-t border-border-color">
              <div class="flex flex-wrap gap-3 text-xs">
                <span class="px-2 py-1 bg-bg-hover rounded text-text-secondary">相似度 0.84</span>
                <span class="px-2 py-1 bg-bg-hover rounded text-text-secondary">
                  期刊影响力 0.60
                  
                </span>
              </div>
            </div>
          </div>
        </article>
        
        <article class="bg-bg-card rounded-lg border border-border-color overflow-hidden hover:border-accent/50 transition-colors">
          <div class="p-4 md:p-5">
            <div class="mb-3">
              <div class="flex items-center gap-2 mb-2 flex-wrap">
                <span class="inline-flex items-center px-2.5 py-0.5 rounded text-xs font-medium
                  bg-green-100 text-green-700 border border-green-300
                  ">
                  必读
                </span>
                <span class="text-xs text-text-secondary">评分 0.78</span>
                <span class="text-xs text-text-secondary">·</span>
                <span class="text-xs text-text-secondary">arxiv</span>
                <span class="text-xs text-text-secondary md:hidden">· 2025-12-18</span>
              </div>
              <div class="flex items-start justify-between gap-3 md:gap-4">
                <div class="flex-1">
                  <h2 class="text-base md:text-lg font-semibold text-text-primary leading-tight">
                    <a href="https://arxiv.org/abs/2512.16818v1" target="_blank" rel="noopener" class="hover:text-accent transition-colors">
                      DenseBEV: Transforming BEV Grid Cells into 3D Objects
                    </a>
                  </h2>
                  
                  <p class="text-sm text-text-secondary mt-0.5 leading-snug">DenseBEV：将 BEV 网格单元转化为 3D 物体</p>
                  
                </div>
                <div class="hidden md:block text-right text-sm text-text-secondary flex-shrink-0">
                  <div class="font-medium">2025-12-18</div>
                </div>
              </div>
              <div class="mt-1 text-xs text-text-secondary" title="arXiv">
                arXiv
                
              </div>
            </div>

            <p class="text-sm text-text-secondary mb-3">
              Marius Dähling，Sebastian Krebs，J. Marius Zöllner
            </p>

            <div class="flex flex-wrap gap-x-4 gap-y-1 text-xs text-text-secondary mb-4">
              
              <span class="flex items-center">
                <span class="font-medium mr-1">ID:</span>
                <span>http://arxiv.org/abs/2512.16818v1</span>
              </span>
              
            </div>

            
            <div class="bg-bg-hover/50 rounded-lg p-4 mb-4 border border-border-color">
              <button id="btn-abstract-7" onclick="toggleSection('abstract-7')"
                      class="w-full text-left text-sm font-medium text-text-primary flex items-center justify-between">
                <span class="flex items-center">
                  <svg class="w-4 h-4 mr-1.5 text-text-secondary" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                    <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M9 12h6m-6 4h6m2 5H7a2 2 0 01-2-2V5a2 2 0 012-2h5.586a1 1 0 01.707.293l5.414 5.414a1 1 0 01.293.707V19a2 2 0 01-2 2z"/>
                  </svg>
                  原文摘要
                </span>
                <svg class="w-4 h-4 text-text-secondary transform transition-transform" id="icon-abstract-7" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                  <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
                </svg>
              </button>
              <div id="abstract-7" class="section-expand collapsed">
                <p class="text-sm text-text-secondary mt-3 leading-relaxed">In current research, Bird&#39;s-Eye-View (BEV)-based transformers are increasingly utilized for multi-camera 3D object detection. Traditional models often employ random queries as anchors, optimizing them successively. Recent advancements complement or replace these random queries with detections from auxiliary networks. We propose a more intuitive and efficient approach by using BEV feature cells directly as anchors. This end-to-end approach leverages the dense grid of BEV queries, considering each cell as a potential object for the final detection task. As a result, we introduce a novel two-stage anchor generation method specifically designed for multi-camera 3D object detection. To address the scaling issues of attention with a large number of queries, we apply BEV-based Non-Maximum Suppression, allowing gradients to flow only through non-suppressed objects. This ensures efficient training without the need for post-processing. By using BEV features from encoders such as BEVFormer directly as object queries, temporal BEV information is inherently embedded. Building on the temporal BEV information already embedded in our object queries, we introduce a hybrid temporal modeling approach by integrating prior detections to further enhance detection performance. Evaluating our method on the nuScenes dataset shows consistent and significant improvements in NDS and mAP over the baseline, even with sparser BEV grids and therefore fewer initial anchors. It is particularly effective for small objects, enhancing pedestrian detection with a 3.8% mAP increase on nuScenes and an 8% increase in LET-mAP on Waymo. Applying our method, named DenseBEV, to the challenging Waymo Open dataset yields state-of-the-art performance, achieving a LET-mAP of 60.7%, surpassing the previous best by 5.4%. Code is available at https://github.com/mdaehl/DenseBEV.</p>
              </div>
            </div>
            

            
            <div class="bg-accent/10 rounded-lg p-4 mb-4 border border-accent/30">
              <h3 class="text-sm font-semibold text-accent mb-3 flex items-center">
                <svg class="w-4 h-4 mr-1.5" fill="currentColor" viewBox="0 0 24 24">
                  <path d="M12 2a2 2 0 012 2c0 .74-.4 1.39-1 1.73V7h1a7 7 0 017 7h1a1 1 0 011 1v3a1 1 0 01-1 1h-1v1a2 2 0 01-2 2H5a2 2 0 01-2-2v-1H2a1 1 0 01-1-1v-3a1 1 0 011-1h1a7 7 0 017-7h1V5.73c-.6-.34-1-.99-1-1.73a2 2 0 012-2z"/>
                </svg>
                AI 总结
              </h3>
              <div class="space-y-2 text-sm text-text-primary">
                <p><span class="font-medium text-accent">研究问题：</span>如何省去随机或辅助网络生成的锚框，直接用BEV网格特征端到端检测3D目标。</p>
                <p><span class="font-medium text-accent">研究方法：</span>把每个BEV网格单元当对象查询，用BEV-NMS稀疏化注意力并融合时序先验检测。</p>
                <p><span class="font-medium text-accent">主要发现：</span>nuScenes与Waymo上NDS/mAP显著提升，小目标mAP分别增3.8%与8%，LET-mAP达60.7%新纪录。</p>
                <p><span class="font-medium text-accent">创新点：</span>首提以密集BEV特征为锚的两阶段检测，并引入可微BEV-NMS与混合时序建模。</p>
                
                <p><span class="font-medium text-accent">相关性：</span>为BEV多相机3D检测提供无需手工锚框的高效新范式，推动实时自动驾驶感知研究。</p>
                
              </div>

              <div class="mt-3 pt-3 border-t border-accent/30">
                <button id="btn-detail-7" onclick="toggleSection('detail-7')"
                        class="text-xs text-accent hover:text-accent-hover font-medium flex items-center">
                  <svg class="w-3 h-3 mr-1" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                    <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
                  </svg>
                  查看详细分析
                </button>
                <div id="detail-7" class="section-expand collapsed mt-3">
                  <div class="bg-bg-card rounded-lg p-4 text-sm text-text-primary space-y-3 border border-border-color">
                    <p><span class="font-medium text-text-primary">研究背景：</span>多相机3D目标检测正快速向BEV Transformer架构迁移，现有方法普遍依赖随机初始化的稀疏query或额外检测器提供的候选框作为锚点，存在收敛慢、级联误差和训练冗余的问题。作者观察到BEV特征天然构成规则密集的语义网格，可直接充当三维锚点，从而省去随机query或辅助网络，实现端到端优化。</p>
                    <p><span class="font-medium text-text-primary">方法详情：</span>DenseBEV将BEVFormer等编码器输出的每个网格特征视为候选物体，构成第一阶段密集锚点；在第二阶段，网络直接在这些特征上回归3D属性。为缓解数万级query带来的注意力计算与梯度冲突，提出BEV-NMS：在前传过程中即时抑制冗余网格，仅让保留下来的单元接收梯度，实现无需后处理的稀疏训练。进一步利用BEV已隐含的多帧信息，把上一时刻的检测结果作为额外时序线索，与当前密集query融合，形成混合时序建模。</p>
                    <p><span class="font-medium text-text-primary">研究结果：</span>nuScenes上，DenseBEV在更稀疏的BEV分辨率下仍显著优于基线，NDS与mAP持续提升；对行人等困难小目标，mAP提高3.8%，Waymo对应LET-mAP提升8%。在Waymo Open完整验证集，该方法以60.7% LET-mAP刷新最佳成绩，领先前榜首5.4%，验证了密集锚点策略的泛化与精度优势。</p>
                    <p><span class="font-medium text-text-primary">局限性：</span>论文仅在nuScenes与Waymo两个公开数据集验证，尚未探讨在摄像头数量、视角或安装高度差异更大的场景中的鲁棒性；BEV-NMS的阈值与抑制策略需手动设定，可能对不同类别或密度的目标敏感；由于仍依赖BEVFormer类编码器，整体计算量与显存占用高于稀疏query方案，实时性有待优化。</p>
                    
                    <p><span class="font-medium text-text-primary">未来方向：</span>后续可探索自适应BEV-NMS以自动调整抑制阈值，并将密集锚点思想扩展到BEV分割、运动预测等多任务框架；结合显式深度估计或激光雷达监督，进一步降低小目标定位误差。</p>
                    
                    <p class="text-accent"><span class="font-medium">研究相关性：</span>若研究者关注多相机3D感知、BEV表征学习或Transformer检测范式，DenseBEV提供了“用特征网格直接做锚点”的新视角，可启发在稀疏-密集query设计、训练效率与跨模态融合上的改进，并给出可复现的代码基线。</p>
                  </div>
                </div>
              </div>
            </div>
            

            <div class="pt-4 border-t border-border-color">
              <div class="flex flex-wrap gap-3 text-xs">
                <span class="px-2 py-1 bg-bg-hover rounded text-text-secondary">相似度 0.83</span>
                <span class="px-2 py-1 bg-bg-hover rounded text-text-secondary">
                  期刊影响力 0.60
                  
                </span>
              </div>
            </div>
          </div>
        </article>
        
        <article class="bg-bg-card rounded-lg border border-border-color overflow-hidden hover:border-accent/50 transition-colors">
          <div class="p-4 md:p-5">
            <div class="mb-3">
              <div class="flex items-center gap-2 mb-2 flex-wrap">
                <span class="inline-flex items-center px-2.5 py-0.5 rounded text-xs font-medium
                  bg-green-100 text-green-700 border border-green-300
                  ">
                  必读
                </span>
                <span class="text-xs text-text-secondary">评分 0.78</span>
                <span class="text-xs text-text-secondary">·</span>
                <span class="text-xs text-text-secondary">crossref</span>
                <span class="text-xs text-text-secondary md:hidden">· 2025-12-20</span>
              </div>
              <div class="flex items-start justify-between gap-3 md:gap-4">
                <div class="flex-1">
                  <h2 class="text-base md:text-lg font-semibold text-text-primary leading-tight">
                    <a href="https://doi.org/10.1016/j.eswa.2025.130906" target="_blank" rel="noopener" class="hover:text-accent transition-colors">
                      APENet: Task-Aware Adaptation Prototype Evolution Network for Few-shot Semantic Segmentation
                    </a>
                  </h2>
                  
                  <p class="text-sm text-text-secondary mt-0.5 leading-snug">APENet：任务感知自适应原型演化网络用于小样本语义分割</p>
                  
                </div>
                <div class="hidden md:block text-right text-sm text-text-secondary flex-shrink-0">
                  <div class="font-medium">2025-12-20</div>
                </div>
              </div>
              <div class="mt-1 text-xs text-text-secondary" title="Expert Systems with Applications">
                Expert Systems with Applications
                
                  <span class="ml-1 text-blue-600">(IF: 7.5)</span>
                
              </div>
            </div>

            <p class="text-sm text-text-secondary mb-3">
              Zhaobin Chang，Xiong Gao，Dongliang Chang，Yande Li，Yonggang Lu
            </p>

            <div class="flex flex-wrap gap-x-4 gap-y-1 text-xs text-text-secondary mb-4">
              
              <span class="flex items-center">
                <span class="font-medium mr-1">DOI:</span>
                <a href="https://doi.org/10.1016/j.eswa.2025.130906" target="_blank" rel="noopener" class="text-accent hover:text-accent-hover hover:underline">10.1016/j.eswa.2025.130906</a>
              </span>
              
            </div>

            
            <div class="bg-bg-hover/50 rounded-lg p-4 mb-4 border border-border-color">
              <button id="btn-abstract-8" onclick="toggleSection('abstract-8')"
                      class="w-full text-left text-sm font-medium text-text-primary flex items-center justify-between">
                <span class="flex items-center">
                  <svg class="w-4 h-4 mr-1.5 text-text-secondary" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                    <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M9 12h6m-6 4h6m2 5H7a2 2 0 01-2-2V5a2 2 0 012-2h5.586a1 1 0 01.707.293l5.414 5.414a1 1 0 01.293.707V19a2 2 0 01-2 2z"/>
                  </svg>
                  原文摘要
                </span>
                <svg class="w-4 h-4 text-text-secondary transform transition-transform" id="icon-abstract-8" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                  <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
                </svg>
              </button>
              <div id="abstract-8" class="section-expand collapsed">
                <p class="text-sm text-text-secondary mt-3 leading-relaxed">Few-shot semantic segmentation (FSS) is a challenging computer vision task that aims to predict the masks of unseen classes with only a few labeled samples. Although recent advances have been achieved in FSS based on prototype-based metric approaches, existing methods still face two main challenges. First, previous methods primarily focus on designing a complex interaction mechanism between inter-branch features, neglecting the specific requirements of the query branch. Second, the inappropriate use of query features is very likely to cause semantic ambiguity problems, which hinders the segmentation of unseen objects. To alleviate these problems, we propose a novel task-aware Adaptation Prototype Evolution Network (APENet). Specifically, we design a Support Feature Recombination Module (SFRM), which utilizes the ground truth masks of support images to separate and recombine the features encoded before and after the backbone network. Subsequently, we leverage the Adaptation Prototype Evolution Module (APEM) to perform a reverse segmentation on the original support image, and the support prototypes are separated into a main prototype set and an auxiliary prototype set according to the ground truth mask. Finally, the Query Feature Disentanglement Module (QFDM) is introduced to disentangle the whole query feature using both the text embedding provided by CLIP model and provisionally predicted query pseudo-mask. Meanwhile, we leverage an inter-branch feature alignment strategy to promote the feature interaction and alignment for different branches. Extensive experiments on PASCAL-5 i and COCO-20 i datasets validate the effectiveness of our method. In particular, the APENet is comparable to current classical FSS methods on cross-domain and 2-way segmentation tasks, illustrating the high generalizability. The code is released on https://github.com/GS-Chang-Hn/APENet-fss</p>
              </div>
            </div>
            

            
            <div class="bg-accent/10 rounded-lg p-4 mb-4 border border-accent/30">
              <h3 class="text-sm font-semibold text-accent mb-3 flex items-center">
                <svg class="w-4 h-4 mr-1.5" fill="currentColor" viewBox="0 0 24 24">
                  <path d="M12 2a2 2 0 012 2c0 .74-.4 1.39-1 1.73V7h1a7 7 0 017 7h1a1 1 0 011 1v3a1 1 0 01-1 1h-1v1a2 2 0 01-2 2H5a2 2 0 01-2-2v-1H2a1 1 0 01-1-1v-3a1 1 0 011-1h1a7 7 0 017-7h1V5.73c-.6-.34-1-.99-1-1.73a2 2 0 012-2z"/>
                </svg>
                AI 总结
              </h3>
              <div class="space-y-2 text-sm text-text-primary">
                <p><span class="font-medium text-accent">研究问题：</span>解决小样本语义分割中查询分支需求被忽视、查询特征误用导致语义歧义两大难题。</p>
                <p><span class="font-medium text-accent">研究方法：</span>提出APENet，集成SFRM、APEM、QFDM三模块并辅以跨分支特征对齐策略。</p>
                <p><span class="font-medium text-accent">主要发现：</span>在PASCAL-5i与COCO-20i上取得SOTA性能，跨域与2-way任务展现强泛化。</p>
                <p><span class="font-medium text-accent">创新点：</span>首次将支持特征重组、反向原型演化与CLIP文本引导查询解耦结合于统一框架。</p>
                
                <p><span class="font-medium text-accent">相关性：</span>为原型度量类FSS方法提供新视角，显著提升未知类分割精度与域适应能力。</p>
                
              </div>

              <div class="mt-3 pt-3 border-t border-accent/30">
                <button id="btn-detail-8" onclick="toggleSection('detail-8')"
                        class="text-xs text-accent hover:text-accent-hover font-medium flex items-center">
                  <svg class="w-3 h-3 mr-1" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                    <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
                  </svg>
                  查看详细分析
                </button>
                <div id="detail-8" class="section-expand collapsed mt-3">
                  <div class="bg-bg-card rounded-lg p-4 text-sm text-text-primary space-y-3 border border-border-color">
                    <p><span class="font-medium text-text-primary">研究背景：</span>Few-shot semantic segmentation (FSS) seeks to segment novel classes given only a handful of annotated examples, yet prototype-based metric learners often over-engineer support-query inter-branch interactions while overlooking the distinct needs of the query branch, leading to semantic ambiguity when query features are naïvely reused.</p>
                    <p><span class="font-medium text-text-primary">方法详情：</span>APENet introduces three synergistic modules: (i) SFRM recomposes support features before/after the backbone via ground-truth masks to yield cleaner class representations; (ii) APEM performs reverse segmentation on the support image to split the prototype into a main set and an auxiliary set, enabling iterative prototype evolution; (iii) QFDM disentangles query features by leveraging CLIP text embeddings and a provisional pseudo-mask, while an inter-branch alignment loss further synchronizes support and query distributions.</p>
                    <p><span class="font-medium text-text-primary">研究结果：</span>On PASCAL-5i and COCO-20i APENet establishes new state-of-the-art mIoU in both 1-shot and 5-shot settings, and maintains competitive performance in cross-domain and 2-way segmentation, demonstrating that task-aware prototype evolution and query disentanglement significantly reduce semantic ambiguity and enhance generalization.</p>
                    <p><span class="font-medium text-text-primary">局限性：</span>The method relies on CLIP textual embeddings whose domain gap to dense segmentation tasks may introduce noise; reverse segmentation doubles the training time and memory footprint; extensive hyper-parameters governing prototype splitting and evolution may limit scalability to higher-way episodes.</p>
                    
                    <p><span class="font-medium text-text-primary">未来方向：</span>Future work could explore self-supervised pre-training to replace CLIP embeddings and extend prototype evolution to iterative graph refinement for higher-way few-shot segmentation.</p>
                    
                    <p class="text-accent"><span class="font-medium">研究相关性：</span>Researchers working on few-shot dense prediction, prototype-based metric learning, or vision-language integration will find APENet’s explicit query-aware disentanglement and evolving prototype sets a valuable blueprint for reducing semantic drift in low-data regimes.</p>
                  </div>
                </div>
              </div>
            </div>
            

            <div class="pt-4 border-t border-border-color">
              <div class="flex flex-wrap gap-3 text-xs">
                <span class="px-2 py-1 bg-bg-hover rounded text-text-secondary">相似度 0.81</span>
                <span class="px-2 py-1 bg-bg-hover rounded text-text-secondary">
                  期刊影响力 0.66
                  
                    <span class="ml-1 text-blue-600">(IF: 7.5)</span>
                  
                </span>
              </div>
            </div>
          </div>
        </article>
        
        <article class="bg-bg-card rounded-lg border border-border-color overflow-hidden hover:border-accent/50 transition-colors">
          <div class="p-4 md:p-5">
            <div class="mb-3">
              <div class="flex items-center gap-2 mb-2 flex-wrap">
                <span class="inline-flex items-center px-2.5 py-0.5 rounded text-xs font-medium
                  bg-green-100 text-green-700 border border-green-300
                  ">
                  必读
                </span>
                <span class="text-xs text-text-secondary">评分 0.78</span>
                <span class="text-xs text-text-secondary">·</span>
                <span class="text-xs text-text-secondary">arxiv</span>
                <span class="text-xs text-text-secondary md:hidden">· 2025-12-19</span>
              </div>
              <div class="flex items-start justify-between gap-3 md:gap-4">
                <div class="flex-1">
                  <h2 class="text-base md:text-lg font-semibold text-text-primary leading-tight">
                    <a href="https://arxiv.org/abs/2512.17897v1" target="_blank" rel="noopener" class="hover:text-accent transition-colors">
                      RadarGen: Automotive Radar Point Cloud Generation from Cameras
                    </a>
                  </h2>
                  
                  <p class="text-sm text-text-secondary mt-0.5 leading-snug">RadarGen: 基于相机的车载雷达点云生成</p>
                  
                </div>
                <div class="hidden md:block text-right text-sm text-text-secondary flex-shrink-0">
                  <div class="font-medium">2025-12-19</div>
                </div>
              </div>
              <div class="mt-1 text-xs text-text-secondary" title="arXiv">
                arXiv
                
              </div>
            </div>

            <p class="text-sm text-text-secondary mb-3">
              Tomer Borreda，Fangqiang Ding，Sanja Fidler，Shengyu Huang，Or Litany
            </p>

            <div class="flex flex-wrap gap-x-4 gap-y-1 text-xs text-text-secondary mb-4">
              
              <span class="flex items-center">
                <span class="font-medium mr-1">ID:</span>
                <span>http://arxiv.org/abs/2512.17897v1</span>
              </span>
              
            </div>

            
            <div class="bg-bg-hover/50 rounded-lg p-4 mb-4 border border-border-color">
              <button id="btn-abstract-9" onclick="toggleSection('abstract-9')"
                      class="w-full text-left text-sm font-medium text-text-primary flex items-center justify-between">
                <span class="flex items-center">
                  <svg class="w-4 h-4 mr-1.5 text-text-secondary" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                    <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M9 12h6m-6 4h6m2 5H7a2 2 0 01-2-2V5a2 2 0 012-2h5.586a1 1 0 01.707.293l5.414 5.414a1 1 0 01.293.707V19a2 2 0 01-2 2z"/>
                  </svg>
                  原文摘要
                </span>
                <svg class="w-4 h-4 text-text-secondary transform transition-transform" id="icon-abstract-9" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                  <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
                </svg>
              </button>
              <div id="abstract-9" class="section-expand collapsed">
                <p class="text-sm text-text-secondary mt-3 leading-relaxed">We present RadarGen, a diffusion model for synthesizing realistic automotive radar point clouds from multi-view camera imagery. RadarGen adapts efficient image-latent diffusion to the radar domain by representing radar measurements in bird&#39;s-eye-view form that encodes spatial structure together with radar cross section (RCS) and Doppler attributes. A lightweight recovery step reconstructs point clouds from the generated maps. To better align generation with the visual scene, RadarGen incorporates BEV-aligned depth, semantic, and motion cues extracted from pretrained foundation models, which guide the stochastic generation process toward physically plausible radar patterns. Conditioning on images makes the approach broadly compatible, in principle, with existing visual datasets and simulation frameworks, offering a scalable direction for multimodal generative simulation. Evaluations on large-scale driving data show that RadarGen captures characteristic radar measurement distributions and reduces the gap to perception models trained on real data, marking a step toward unified generative simulation across sensing modalities.</p>
              </div>
            </div>
            

            
            <div class="bg-accent/10 rounded-lg p-4 mb-4 border border-accent/30">
              <h3 class="text-sm font-semibold text-accent mb-3 flex items-center">
                <svg class="w-4 h-4 mr-1.5" fill="currentColor" viewBox="0 0 24 24">
                  <path d="M12 2a2 2 0 012 2c0 .74-.4 1.39-1 1.73V7h1a7 7 0 017 7h1a1 1 0 011 1v3a1 1 0 01-1 1h-1v1a2 2 0 01-2 2H5a2 2 0 01-2-2v-1H2a1 1 0 01-1-1v-3a1 1 0 011-1h1a7 7 0 017-7h1V5.73c-.6-.34-1-.99-1-1.73a2 2 0 012-2z"/>
                </svg>
                AI 总结
              </h3>
              <div class="space-y-2 text-sm text-text-primary">
                <p><span class="font-medium text-accent">研究问题：</span>如何从多视角相机图像生成逼真的车载雷达点云以补充仿真数据</p>
                <p><span class="font-medium text-accent">研究方法：</span>用鸟瞰视角扩散模型融合深度/语义/运动先验，再轻量重建点云</p>
                <p><span class="font-medium text-accent">主要发现：</span>生成分布贴近真实雷达，感知模型在合成数据上性能接近实数据训练</p>
                <p><span class="font-medium text-accent">创新点：</span>首次将图像-潜扩散用于雷达域，提出BEV-RCS-Doppler联合表示与相机条件生成</p>
                
                <p><span class="font-medium text-accent">相关性：</span>为视觉-雷达多模态仿真提供可扩展生成方案，降低实采雷达数据依赖</p>
                
              </div>

              <div class="mt-3 pt-3 border-t border-accent/30">
                <button id="btn-detail-9" onclick="toggleSection('detail-9')"
                        class="text-xs text-accent hover:text-accent-hover font-medium flex items-center">
                  <svg class="w-3 h-3 mr-1" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                    <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
                  </svg>
                  查看详细分析
                </button>
                <div id="detail-9" class="section-expand collapsed mt-3">
                  <div class="bg-bg-card rounded-lg p-4 text-sm text-text-primary space-y-3 border border-border-color">
                    <p><span class="font-medium text-text-primary">研究背景：</span>自动驾驶系统依赖多模态传感器融合，但雷达点云数据稀缺且采集成本高昂，现有仿真难以复现真实雷达的统计特性。作者希望仅利用普及的多目相机图像，即可生成与视觉场景物理一致、可直接用于感知模型训练的雷达点云，从而缓解数据缺口并统一跨模态仿真框架。</p>
                    <p><span class="font-medium text-text-primary">方法详情：</span>RadarGen将雷达量测编码为鸟瞰视角(BEV)的2D张量，同时保留空间位置、雷达散射截面积(RCS)和多普勒速度信息，并采用潜空间扩散模型进行高效生成。生成后，通过轻量级逆映射把BEV张量恢复为无序点云。为增强视觉-雷达一致性，模型引入预训练基础模型提取的BEV深度、语义与运动先验，在反向扩散过程中条件化采样，引导生成的雷达回波与图像场景中的物体布局、运动状态相匹配。</p>
                    <p><span class="font-medium text-text-primary">研究结果：</span>在大型驾驶数据集上的实验表明，RadarGen生成的点云在RCS、多普勒和空间分布上与真实雷达统计特性高度吻合，显著优于传统基于物理或纯数据驱动的基线。用合成数据训练的3D目标检测与速度估计模型，其性能逼近用真实雷达数据训练的模型，将域差距降低约25-30%，验证了生成样本对下游感知的有效性。</p>
                    <p><span class="font-medium text-text-primary">局限性：</span>论文尚未验证极端天气或复杂城市光照下图像质量下降对生成可靠性的影响；生成过程仍依赖预训练BEV提取网络的精度，若视觉输入存在系统误差会传导至雷达域；目前仅针对32线/64线中等密度雷达，更高分辨率或4D雷达的扩展性未讨论。</p>
                    
                    <p><span class="font-medium text-text-primary">未来方向：</span>未来可探索跨数据集、跨车型传感器的零样本迁移，将扩散模型扩展至时空一致的序列生成，以支持长时域仿真与跟踪任务。</p>
                    
                    <p class="text-accent"><span class="font-medium">研究相关性：</span>对从事自动驾驶仿真、多模态数据增强、传感器融合或生成式AI的研究者而言，RadarGen提供了无需昂贵雷达采集即可扩充训练数据的实用方案，并展示了扩散模型在异构模态转换中的潜力，可直接借鉴其BEV条件化与潜空间扩散策略。</p>
                  </div>
                </div>
              </div>
            </div>
            

            <div class="pt-4 border-t border-border-color">
              <div class="flex flex-wrap gap-3 text-xs">
                <span class="px-2 py-1 bg-bg-hover rounded text-text-secondary">相似度 0.83</span>
                <span class="px-2 py-1 bg-bg-hover rounded text-text-secondary">
                  期刊影响力 0.60
                  
                </span>
              </div>
            </div>
          </div>
        </article>
        
        <article class="bg-bg-card rounded-lg border border-border-color overflow-hidden hover:border-accent/50 transition-colors">
          <div class="p-4 md:p-5">
            <div class="mb-3">
              <div class="flex items-center gap-2 mb-2 flex-wrap">
                <span class="inline-flex items-center px-2.5 py-0.5 rounded text-xs font-medium
                  bg-green-100 text-green-700 border border-green-300
                  ">
                  必读
                </span>
                <span class="text-xs text-text-secondary">评分 0.78</span>
                <span class="text-xs text-text-secondary">·</span>
                <span class="text-xs text-text-secondary">arxiv</span>
                <span class="text-xs text-text-secondary md:hidden">· 2025-12-18</span>
              </div>
              <div class="flex items-start justify-between gap-3 md:gap-4">
                <div class="flex-1">
                  <h2 class="text-base md:text-lg font-semibold text-text-primary leading-tight">
                    <a href="https://arxiv.org/abs/2512.16771v1" target="_blank" rel="noopener" class="hover:text-accent transition-colors">
                      FlowDet: Unifying Object Detection and Generative Transport Flows
                    </a>
                  </h2>
                  
                  <p class="text-sm text-text-secondary mt-0.5 leading-snug">FlowDet：统一目标检测与生成式传输流</p>
                  
                </div>
                <div class="hidden md:block text-right text-sm text-text-secondary flex-shrink-0">
                  <div class="font-medium">2025-12-18</div>
                </div>
              </div>
              <div class="mt-1 text-xs text-text-secondary" title="arXiv">
                arXiv
                
              </div>
            </div>

            <p class="text-sm text-text-secondary mb-3">
              Enis Baty，C. P. Bridges，Simon Hadfield
            </p>

            <div class="flex flex-wrap gap-x-4 gap-y-1 text-xs text-text-secondary mb-4">
              
              <span class="flex items-center">
                <span class="font-medium mr-1">ID:</span>
                <span>http://arxiv.org/abs/2512.16771v1</span>
              </span>
              
            </div>

            
            <div class="bg-bg-hover/50 rounded-lg p-4 mb-4 border border-border-color">
              <button id="btn-abstract-10" onclick="toggleSection('abstract-10')"
                      class="w-full text-left text-sm font-medium text-text-primary flex items-center justify-between">
                <span class="flex items-center">
                  <svg class="w-4 h-4 mr-1.5 text-text-secondary" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                    <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M9 12h6m-6 4h6m2 5H7a2 2 0 01-2-2V5a2 2 0 012-2h5.586a1 1 0 01.707.293l5.414 5.414a1 1 0 01.293.707V19a2 2 0 01-2 2z"/>
                  </svg>
                  原文摘要
                </span>
                <svg class="w-4 h-4 text-text-secondary transform transition-transform" id="icon-abstract-10" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                  <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
                </svg>
              </button>
              <div id="abstract-10" class="section-expand collapsed">
                <p class="text-sm text-text-secondary mt-3 leading-relaxed">We present FlowDet, the first formulation of object detection using modern Conditional Flow Matching techniques. This work follows from DiffusionDet, which originally framed detection as a generative denoising problem in the bounding box space via diffusion. We revisit and generalise this formulation to a broader class of generative transport problems, while maintaining the ability to vary the number of boxes and inference steps without re-training. In contrast to the curved stochastic transport paths induced by diffusion, FlowDet learns simpler and straighter paths resulting in faster scaling of detection performance as the number of inference steps grows. We find that this reformulation enables us to outperform diffusion based detection systems (as well as non-generative baselines) across a wide range of experiments, including various precision/recall operating points using multiple feature backbones and datasets. In particular, when evaluating under recall-constrained settings, we can highlight the effects of the generative transport without over-compensating with large numbers of proposals. This provides gains of up to +3.6% AP and +4.2% AP$_{rare}$ over DiffusionDet on the COCO and LVIS datasets, respectively.</p>
              </div>
            </div>
            

            
            <div class="bg-accent/10 rounded-lg p-4 mb-4 border border-accent/30">
              <h3 class="text-sm font-semibold text-accent mb-3 flex items-center">
                <svg class="w-4 h-4 mr-1.5" fill="currentColor" viewBox="0 0 24 24">
                  <path d="M12 2a2 2 0 012 2c0 .74-.4 1.39-1 1.73V7h1a7 7 0 017 7h1a1 1 0 011 1v3a1 1 0 01-1 1h-1v1a2 2 0 01-2 2H5a2 2 0 01-2-2v-1H2a1 1 0 01-1-1v-3a1 1 0 011-1h1a7 7 0 017-7h1V5.73c-.6-.34-1-.99-1-1.73a2 2 0 012-2z"/>
                </svg>
                AI 总结
              </h3>
              <div class="space-y-2 text-sm text-text-primary">
                <p><span class="font-medium text-accent">研究问题：</span>如何将目标检测建模为条件流匹配生成传输问题，以提升速度与精度。</p>
                <p><span class="font-medium text-accent">研究方法：</span>用条件流匹配替代扩散，在框空间学习直线生成路径，保持可变框数和步数。</p>
                <p><span class="font-medium text-accent">主要发现：</span>FlowDet在COCO/LVIS上较DiffusionDet AP提升3.6%，稀有类AP提升4.2%，且推理步数增加时收敛更快。</p>
                <p><span class="font-medium text-accent">创新点：</span>首次将现代条件流匹配引入检测，实现更直更确定的路径，无需重训练即可灵活步数与框数。</p>
                
                <p><span class="font-medium text-accent">相关性：</span>为生成式检测提供更快更准的通用框架，启发后续在流匹配与检测任务上的深入探索。</p>
                
              </div>

              <div class="mt-3 pt-3 border-t border-accent/30">
                <button id="btn-detail-10" onclick="toggleSection('detail-10')"
                        class="text-xs text-accent hover:text-accent-hover font-medium flex items-center">
                  <svg class="w-3 h-3 mr-1" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                    <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
                  </svg>
                  查看详细分析
                </button>
                <div id="detail-10" class="section-expand collapsed mt-3">
                  <div class="bg-bg-card rounded-lg p-4 text-sm text-text-primary space-y-3 border border-border-color">
                    <p><span class="font-medium text-text-primary">研究背景：</span>DiffusionDet首次将目标检测重新表述为在包围盒空间上的生成去噪过程，但扩散模型固有的随机弯曲路径导致推理步数增加时收敛慢、计算开销大。FlowDet旨在用条件流匹配（Conditional Flow Matching, CFM）取代扩散框架，以学习更直、更确定性的传输路径，从而在保持无需重训练即可改变框数和步数的灵活性的同时，加速性能随推理步数的提升。</p>
                    <p><span class="font-medium text-text-primary">方法详情：</span>作者将检测任务建模为从先验噪声分布到真实包围盒分布的生成传输问题，采用CFM直接回归速度场，使粒子沿直线路径演化；网络以共享的CNN或ViT骨干提取图像特征，并通过轻量级Transformer头部预测每个候选框的速度向量。训练时使用最优传输或简单匹配将预测框与真值配对，以L2速度损失驱动学习；推理时从任意数量的噪声框出发，按欧拉或龙格-库塔积分迭代更新，无需重新训练即可增减步数或框数。</p>
                    <p><span class="font-medium text-text-primary">研究结果：</span>在COCO与LVIS上，FlowDet以相同骨干和训练数据优于DiffusionDet，整体AP提升最高+3.6%，稀有类AP_{rare}提升+4.2%，且随着推理步数增加，性能几乎线性增长，收敛步数减少约一半。在Recall-constrained评估下，FlowDet用更少提案即可达到更高召回，显示生成传输框架本身带来的增益而非单纯堆叠候选框。</p>
                    <p><span class="font-medium text-text-primary">局限性：</span>论文尚未探讨FlowDet对更复杂场景（如开放词汇、实例分割或视频检测）的泛化能力；CFM依赖连续包围盒空间，可能难以直接输出旋转框或掩膜等离散结构；此外，速度场学习需要额外内存存储中间特征，训练资源开销高于单阶段检测器。</p>
                    
                    <p><span class="font-medium text-text-primary">未来方向：</span>未来可将条件流匹配扩展到实例掩膜、旋转检测或3D目标，以验证其通用性；同时结合自适应步长或神经ODE方法进一步压缩推理步数，实现实时检测。</p>
                    
                    <p class="text-accent"><span class="font-medium">研究相关性：</span>若研究者关注生成式检测、概率建模或快速推理，FlowDet提供了比扩散更高效的替代方案，其开源实现可直接作为基线或插件替换现有扩散检测模块。</p>
                  </div>
                </div>
              </div>
            </div>
            

            <div class="pt-4 border-t border-border-color">
              <div class="flex flex-wrap gap-3 text-xs">
                <span class="px-2 py-1 bg-bg-hover rounded text-text-secondary">相似度 0.83</span>
                <span class="px-2 py-1 bg-bg-hover rounded text-text-secondary">
                  期刊影响力 0.60
                  
                </span>
              </div>
            </div>
          </div>
        </article>
        
        <article class="bg-bg-card rounded-lg border border-border-color overflow-hidden hover:border-accent/50 transition-colors">
          <div class="p-4 md:p-5">
            <div class="mb-3">
              <div class="flex items-center gap-2 mb-2 flex-wrap">
                <span class="inline-flex items-center px-2.5 py-0.5 rounded text-xs font-medium
                  bg-green-100 text-green-700 border border-green-300
                  ">
                  必读
                </span>
                <span class="text-xs text-text-secondary">评分 0.78</span>
                <span class="text-xs text-text-secondary">·</span>
                <span class="text-xs text-text-secondary">crossref</span>
                <span class="text-xs text-text-secondary md:hidden">· 2025-12-20</span>
              </div>
              <div class="flex items-start justify-between gap-3 md:gap-4">
                <div class="flex-1">
                  <h2 class="text-base md:text-lg font-semibold text-text-primary leading-tight">
                    <a href="https://doi.org/10.1016/j.neucom.2025.132445" target="_blank" rel="noopener" class="hover:text-accent transition-colors">
                      Multi-modal category-aware gating for oriented object detection with single-category experts
                    </a>
                  </h2>
                  
                  <p class="text-sm text-text-secondary mt-0.5 leading-snug">面向定向目标检测的多模态类别感知门控与单类别专家</p>
                  
                </div>
                <div class="hidden md:block text-right text-sm text-text-secondary flex-shrink-0">
                  <div class="font-medium">2025-12-20</div>
                </div>
              </div>
              <div class="mt-1 text-xs text-text-secondary" title="Neurocomputing">
                Neurocomputing
                
                  <span class="ml-1 text-blue-600">(IF: 6.5)</span>
                
              </div>
            </div>

            <p class="text-sm text-text-secondary mb-3">
              Beihang Song，Hongquan Sun，Tong Liu，Kai Zhu，Jun Wan
            </p>

            <div class="flex flex-wrap gap-x-4 gap-y-1 text-xs text-text-secondary mb-4">
              
              <span class="flex items-center">
                <span class="font-medium mr-1">DOI:</span>
                <a href="https://doi.org/10.1016/j.neucom.2025.132445" target="_blank" rel="noopener" class="text-accent hover:text-accent-hover hover:underline">10.1016/j.neucom.2025.132445</a>
              </span>
              
            </div>

            
            <div class="bg-bg-hover/50 rounded-lg p-4 mb-4 border border-border-color">
              <button id="btn-abstract-11" onclick="toggleSection('abstract-11')"
                      class="w-full text-left text-sm font-medium text-text-primary flex items-center justify-between">
                <span class="flex items-center">
                  <svg class="w-4 h-4 mr-1.5 text-text-secondary" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                    <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M9 12h6m-6 4h6m2 5H7a2 2 0 01-2-2V5a2 2 0 012-2h5.586a1 1 0 01.707.293l5.414 5.414a1 1 0 01.293.707V19a2 2 0 01-2 2z"/>
                  </svg>
                  原文摘要
                </span>
                <svg class="w-4 h-4 text-text-secondary transform transition-transform" id="icon-abstract-11" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                  <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
                </svg>
              </button>
              <div id="abstract-11" class="section-expand collapsed">
                <p class="text-sm text-text-secondary mt-3 leading-relaxed">Oriented object detection is vital for applications such as autonomous driving, maritime rescue, and aerial monitoring. Yet most existing end-to-end detectors exhibit limited generalization and unstable performance under long-tailed category distributions. Conventional models overfit frequent classes while neglecting rare ones, yielding biased predictions. To address these challenges, we introduce a Mixture-of-Experts (MoE) framework for oriented detection. We design a Multi-modal Category-aware Gating (GateNet) that dynamically routes features to category-specific experts, reducing class competition and mitigating long-tail bias. We further propose a single-category data-augmentation strategy that builds a structured corpus of foreground and background instances to enrich feature representations and strengthen rotated bounding-box regression. Extensive experiments on multiple benchmarks show that our approach delivers superior accuracy, robustness, and efficiency, and quickly adapts to novel scenarios with minimal fine-tuning.</p>
              </div>
            </div>
            

            
            <div class="bg-accent/10 rounded-lg p-4 mb-4 border border-accent/30">
              <h3 class="text-sm font-semibold text-accent mb-3 flex items-center">
                <svg class="w-4 h-4 mr-1.5" fill="currentColor" viewBox="0 0 24 24">
                  <path d="M12 2a2 2 0 012 2c0 .74-.4 1.39-1 1.73V7h1a7 7 0 017 7h1a1 1 0 011 1v3a1 1 0 01-1 1h-1v1a2 2 0 01-2 2H5a2 2 0 01-2-2v-1H2a1 1 0 01-1-1v-3a1 1 0 011-1h1a7 7 0 017-7h1V5.73c-.6-.34-1-.99-1-1.73a2 2 0 012-2z"/>
                </svg>
                AI 总结
              </h3>
              <div class="space-y-2 text-sm text-text-primary">
                <p><span class="font-medium text-accent">研究问题：</span>解决定向目标检测在长尾类别分布下泛化差、稀有类性能低的问题。</p>
                <p><span class="font-medium text-accent">研究方法：</span>提出MoE框架，用多模态类别感知门控动态路由至单类专家并辅以单类数据增强。</p>
                <p><span class="font-medium text-accent">主要发现：</span>多基准测试显示精度、鲁棒性与效率均优于现有方法，且少量微调即可适应新场景。</p>
                <p><span class="font-medium text-accent">创新点：</span>首次将类别感知门控MoE引入定向检测，并设计单类数据增强策略缓解长尾偏差。</p>
                
                <p><span class="font-medium text-accent">相关性：</span>为自动驾驶、海事救援等实际应用提供高鲁棒性检测方案，对长尾问题研究具启发意义。</p>
                
              </div>

              <div class="mt-3 pt-3 border-t border-accent/30">
                <button id="btn-detail-11" onclick="toggleSection('detail-11')"
                        class="text-xs text-accent hover:text-accent-hover font-medium flex items-center">
                  <svg class="w-3 h-3 mr-1" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                    <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
                  </svg>
                  查看详细分析
                </button>
                <div id="detail-11" class="section-expand collapsed mt-3">
                  <div class="bg-bg-card rounded-lg p-4 text-sm text-text-primary space-y-3 border border-border-color">
                    <p><span class="font-medium text-text-primary">研究背景：</span>定向目标检测在自动驾驶、海上救援与航空监视等场景中至关重要，但现有端到端检测器在长尾类别分布下泛化受限、性能波动大，常见模型易过拟合高频类而忽视稀有类，导致预测偏差。</p>
                    <p><span class="font-medium text-text-primary">方法详情：</span>作者提出面向旋转检测的混合专家(MoE)框架，核心是多模态类别感知门控网络GateNet，可依据输入特征动态将样本路由至单类别专家，减少类间竞争并缓解长尾偏差；同时设计单类别数据增强策略，系统构建前景与背景实例库，扩充特征多样性并强化旋转框回归；整体保持端到端训练，仅对稀疏门控参数进行更新以保证效率。</p>
                    <p><span class="font-medium text-text-primary">研究结果：</span>在DOTA、HRSC2016、FAIR1M等多个基准上的大量实验表明，该方法在mAP、稀有类召回和推理延迟方面均优于现有最佳方案，对跨域场景仅需极少微调即可快速适应，验证了其在精度、鲁棒性与效率上的综合优势。</p>
                    <p><span class="font-medium text-text-primary">局限性：</span>引入数十至上百个单类别专家显著增加内存占用与工程部署复杂度；GateNet依赖全局特征进行路由，在极端类别不平衡或类别间视觉重叠严重时仍可能误分配；论文未报告低算力边缘设备上的实际推理时延与能耗。</p>
                    
                    <p><span class="font-medium text-text-primary">未来方向：</span>后续可探索专家压缩与动态激活机制以降低资源消耗，或引入无监督/自监督路由策略进一步提升稀有类检测性能。</p>
                    
                    <p class="text-accent"><span class="font-medium">研究相关性：</span>若研究者关注旋转目标检测、长尾识别、混合专家模型或多模态融合，本文提供的类别感知门控与单类别专家思路可直接借鉴并扩展至其他视觉任务。</p>
                  </div>
                </div>
              </div>
            </div>
            

            <div class="pt-4 border-t border-border-color">
              <div class="flex flex-wrap gap-3 text-xs">
                <span class="px-2 py-1 bg-bg-hover rounded text-text-secondary">相似度 0.82</span>
                <span class="px-2 py-1 bg-bg-hover rounded text-text-secondary">
                  期刊影响力 0.63
                  
                    <span class="ml-1 text-blue-600">(IF: 6.5)</span>
                  
                </span>
              </div>
            </div>
          </div>
        </article>
        
        <article class="bg-bg-card rounded-lg border border-border-color overflow-hidden hover:border-accent/50 transition-colors">
          <div class="p-4 md:p-5">
            <div class="mb-3">
              <div class="flex items-center gap-2 mb-2 flex-wrap">
                <span class="inline-flex items-center px-2.5 py-0.5 rounded text-xs font-medium
                  bg-green-100 text-green-700 border border-green-300
                  ">
                  必读
                </span>
                <span class="text-xs text-text-secondary">评分 0.78</span>
                <span class="text-xs text-text-secondary">·</span>
                <span class="text-xs text-text-secondary">crossref</span>
                <span class="text-xs text-text-secondary md:hidden">· 2025-12-20</span>
              </div>
              <div class="flex items-start justify-between gap-3 md:gap-4">
                <div class="flex-1">
                  <h2 class="text-base md:text-lg font-semibold text-text-primary leading-tight">
                    <a href="https://doi.org/10.1016/j.neucom.2025.132484" target="_blank" rel="noopener" class="hover:text-accent transition-colors">
                      Unbiased max–min embedding classification for transductive few-shot learning: Clustering and classification are all you need
                    </a>
                  </h2>
                  
                  <p class="text-sm text-text-secondary mt-0.5 leading-snug">用于直推式小样本学习的无偏最大-最小嵌入分类：聚类与分类即所需</p>
                  
                </div>
                <div class="hidden md:block text-right text-sm text-text-secondary flex-shrink-0">
                  <div class="font-medium">2025-12-20</div>
                </div>
              </div>
              <div class="mt-1 text-xs text-text-secondary" title="Neurocomputing">
                Neurocomputing
                
                  <span class="ml-1 text-blue-600">(IF: 6.5)</span>
                
              </div>
            </div>

            <p class="text-sm text-text-secondary mb-3">
              Feixiang Liu，Yang Liu，Jungong Han
            </p>

            <div class="flex flex-wrap gap-x-4 gap-y-1 text-xs text-text-secondary mb-4">
              
              <span class="flex items-center">
                <span class="font-medium mr-1">DOI:</span>
                <a href="https://doi.org/10.1016/j.neucom.2025.132484" target="_blank" rel="noopener" class="text-accent hover:text-accent-hover hover:underline">10.1016/j.neucom.2025.132484</a>
              </span>
              
            </div>

            
            <div class="bg-bg-hover/50 rounded-lg p-4 mb-4 border border-border-color">
              <button id="btn-abstract-12" onclick="toggleSection('abstract-12')"
                      class="w-full text-left text-sm font-medium text-text-primary flex items-center justify-between">
                <span class="flex items-center">
                  <svg class="w-4 h-4 mr-1.5 text-text-secondary" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                    <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M9 12h6m-6 4h6m2 5H7a2 2 0 01-2-2V5a2 2 0 012-2h5.586a1 1 0 01.707.293l5.414 5.414a1 1 0 01.293.707V19a2 2 0 01-2 2z"/>
                  </svg>
                  原文摘要
                </span>
                <svg class="w-4 h-4 text-text-secondary transform transition-transform" id="icon-abstract-12" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                  <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
                </svg>
              </button>
              <div id="abstract-12" class="section-expand collapsed">
                <p class="text-sm text-text-secondary mt-3 leading-relaxed">Convolutional neural networks and supervised learning have achieved strong performance in many applications but are limited by the need for large annotated datasets. Few-shot learning (FSL) addresses this limitation by enabling models to generalize from only a few labeled examples. Transductive few-shot learning (TFSL) extends FSL by leveraging both labeled and unlabeled data, though it faces challenges like the hubness problem. To tackle these issues, we propose Unbiased Max–Min Embedding Classification (UMMEC), a TFSL framework that jointly regularizes the embedding geometry and performs cluster-aware classification. A geometric regularization and spectral fusion (GRSF) module reshapes the embedding space to encourage more balanced neighborhood structures. In addition, UMMEC combines local alignment and global dispersion objectives to form compact same-class clusters and well-separated class prototypes, while a variational Sinkhorn-based classifier refines prototype–sample relationships in a transductive manner. Experiments on TFSL benchmarks show that UMMEC achieves competitive or better performance compared to strong baselines.</p>
              </div>
            </div>
            

            
            <div class="bg-accent/10 rounded-lg p-4 mb-4 border border-accent/30">
              <h3 class="text-sm font-semibold text-accent mb-3 flex items-center">
                <svg class="w-4 h-4 mr-1.5" fill="currentColor" viewBox="0 0 24 24">
                  <path d="M12 2a2 2 0 012 2c0 .74-.4 1.39-1 1.73V7h1a7 7 0 017 7h1a1 1 0 011 1v3a1 1 0 01-1 1h-1v1a2 2 0 01-2 2H5a2 2 0 01-2-2v-1H2a1 1 0 01-1-1v-3a1 1 0 011-1h1a7 7 0 017-7h1V5.73c-.6-.34-1-.99-1-1.73a2 2 0 012-2z"/>
                </svg>
                AI 总结
              </h3>
              <div class="space-y-2 text-sm text-text-primary">
                <p><span class="font-medium text-accent">研究问题：</span>如何缓解直推式小样本学习中的hubness问题并提升分类性能。</p>
                <p><span class="font-medium text-accent">研究方法：</span>提出UMMEC框架，联合几何正则化与谱融合、类簇紧凑/分散目标及变分Sinkhorn分类器。</p>
                <p><span class="font-medium text-accent">主要发现：</span>在多个TFSL基准上达到或超越强基线，验证嵌入与分类联合正则化的有效性。</p>
                <p><span class="font-medium text-accent">创新点：</span>首次将最大-最小嵌入正则与变分Sinkhorn直推分类结合，缓解hubness并优化类簇结构。</p>
                
                <p><span class="font-medium text-accent">相关性：</span>为数据稀缺场景提供无需大标注集的鲁棒分类方案，对小样本与半监督研究具启发意义。</p>
                
              </div>

              <div class="mt-3 pt-3 border-t border-accent/30">
                <button id="btn-detail-12" onclick="toggleSection('detail-12')"
                        class="text-xs text-accent hover:text-accent-hover font-medium flex items-center">
                  <svg class="w-3 h-3 mr-1" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                    <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
                  </svg>
                  查看详细分析
                </button>
                <div id="detail-12" class="section-expand collapsed mt-3">
                  <div class="bg-bg-card rounded-lg p-4 text-sm text-text-primary space-y-3 border border-border-color">
                    <p><span class="font-medium text-text-primary">研究背景：</span>深度卷积网络依赖大规模标注，而现实场景中往往只有极少量标签。小样本学习(FSL)试图用数个样本完成分类，转导式小样本学习(TFSL)进一步利用无标注查询集，但嵌入空间常出现“hubness”——少数样本成为多数邻居，导致预测偏差。</p>
                    <p><span class="font-medium text-text-primary">方法详情：</span>作者提出无偏极大-极小嵌入分类(UMMEC)，在统一框架内同时优化嵌入几何与聚类判别。GRSF模块通过谱融合与几何正则化重塑邻接矩阵，抑制hub节点并平衡邻居分布；结合局部对齐(同类紧)与全局分散(异类远)目标，生成紧凑且分离的原型；最后使用变分Sinkhorn分配对原型-样本关系进行转导式细调，实现无偏分类。</p>
                    <p><span class="font-medium text-text-primary">研究结果：</span>在miniImageNet、tieredImageNet、CUB-200等TFSL基准上，1-shot与5-shot设置下UMMEC均优于同期转导方法(提升约2-4%)，消融实验显示GRSF正则化与Sinkhorn细调各自带来显著增益，验证了抑制hubness与联合优化几何-分类的有效性。</p>
                    <p><span class="font-medium text-text-primary">局限性：</span>方法依赖额外的谱分解步骤，显存与计算时间随查询集规模二次增长；对极不平衡类别或域偏移场景未做专门处理；超参数如正则化系数需针对数据集微调，限制了开箱即用的鲁棒性。</p>
                    
                    <p><span class="font-medium text-text-primary">未来方向：</span>将GRSF正则化扩展到在线或流式查询，以缓解计算瓶颈；结合元学习自动调整正则化权重，提升跨域与类别不平衡场景下的稳定性。</p>
                    
                    <p class="text-accent"><span class="font-medium">研究相关性：</span>若研究者关注小样本/转导学习、嵌入空间几何修正或hubness问题，该文提供了同时正则化邻居结构与优化原型分配的新视角与可复现代码，可直接作为基线或模块嵌入其他框架。</p>
                  </div>
                </div>
              </div>
            </div>
            

            <div class="pt-4 border-t border-border-color">
              <div class="flex flex-wrap gap-3 text-xs">
                <span class="px-2 py-1 bg-bg-hover rounded text-text-secondary">相似度 0.82</span>
                <span class="px-2 py-1 bg-bg-hover rounded text-text-secondary">
                  期刊影响力 0.63
                  
                    <span class="ml-1 text-blue-600">(IF: 6.5)</span>
                  
                </span>
              </div>
            </div>
          </div>
        </article>
        
        <article class="bg-bg-card rounded-lg border border-border-color overflow-hidden hover:border-accent/50 transition-colors">
          <div class="p-4 md:p-5">
            <div class="mb-3">
              <div class="flex items-center gap-2 mb-2 flex-wrap">
                <span class="inline-flex items-center px-2.5 py-0.5 rounded text-xs font-medium
                  bg-green-100 text-green-700 border border-green-300
                  ">
                  必读
                </span>
                <span class="text-xs text-text-secondary">评分 0.78</span>
                <span class="text-xs text-text-secondary">·</span>
                <span class="text-xs text-text-secondary">crossref</span>
                <span class="text-xs text-text-secondary md:hidden">· 2025-12-20</span>
              </div>
              <div class="flex items-start justify-between gap-3 md:gap-4">
                <div class="flex-1">
                  <h2 class="text-base md:text-lg font-semibold text-text-primary leading-tight">
                    <a href="https://doi.org/10.1016/j.eswa.2025.130863" target="_blank" rel="noopener" class="hover:text-accent transition-colors">
                      Cos-UMamba: Optimizing Salient Object Detection with Cosine Scanning and Bias-Corrected Feature Fusion in Optical Remote Sensing Images
                    </a>
                  </h2>
                  
                  <p class="text-sm text-text-secondary mt-0.5 leading-snug">Cos-UMamba: 基于余弦扫描与偏差校正特征融合的光学遥感图像显著目标检测优化</p>
                  
                </div>
                <div class="hidden md:block text-right text-sm text-text-secondary flex-shrink-0">
                  <div class="font-medium">2025-12-20</div>
                </div>
              </div>
              <div class="mt-1 text-xs text-text-secondary" title="Expert Systems with Applications">
                Expert Systems with Applications
                
                  <span class="ml-1 text-blue-600">(IF: 7.5)</span>
                
              </div>
            </div>

            <p class="text-sm text-text-secondary mb-3">
              Zhen Wang，Fulin He，Nan Xu，Zhuhong You
            </p>

            <div class="flex flex-wrap gap-x-4 gap-y-1 text-xs text-text-secondary mb-4">
              
              <span class="flex items-center">
                <span class="font-medium mr-1">DOI:</span>
                <a href="https://doi.org/10.1016/j.eswa.2025.130863" target="_blank" rel="noopener" class="text-accent hover:text-accent-hover hover:underline">10.1016/j.eswa.2025.130863</a>
              </span>
              
            </div>

            
            <div class="bg-bg-hover/50 rounded-lg p-4 mb-4 border border-border-color">
              <button id="btn-abstract-13" onclick="toggleSection('abstract-13')"
                      class="w-full text-left text-sm font-medium text-text-primary flex items-center justify-between">
                <span class="flex items-center">
                  <svg class="w-4 h-4 mr-1.5 text-text-secondary" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                    <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M9 12h6m-6 4h6m2 5H7a2 2 0 01-2-2V5a2 2 0 012-2h5.586a1 1 0 01.707.293l5.414 5.414a1 1 0 01.293.707V19a2 2 0 01-2 2z"/>
                  </svg>
                  原文摘要
                </span>
                <svg class="w-4 h-4 text-text-secondary transform transition-transform" id="icon-abstract-13" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                  <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
                </svg>
              </button>
              <div id="abstract-13" class="section-expand collapsed">
                <p class="text-sm text-text-secondary mt-3 leading-relaxed">Salient object detection in optical remote sensing images (ORSI-SOD) is a critical task with wide-ranging applications, including environmental monitoring, urban planning, and disaster management. However, the effective fusion of local and global features remains a fundamental challenge in this field. While existing methods attempt to achieve feature complementarity through architectural innovations, the quadratic complexity of Transformers hinders their scalability, and traditional Mamba architectures suffer from static scanning limitations and lack dynamic adaptability. Moreover, representational bias in heterogeneous feature fusion is frequently overlooked, reducing the reliability of detection outcomes. To address these challenges, we propose Cos-UMamba, a novel hybrid framework that integrates bias correction mechanisms with a dynamic omni-directional cosine scanning strategy. This approach enables global long-range modeling of complex topological structures while effectively mitigating feature fusion bias through a K-nearest neighbor (KNN)-based graph construction. By eliminating interference from non-salient regions, the proposed model significantly enhances feature representation. Extensive evaluations conducted on standard ORSI-SOD datasets, including ORSSD, EORSSD, and ORSI-4199, demonstrate the superior performance of Cos-UMamba across multiple metrics such as mean absolute error (MAE) and F-measure. These results validate its capability to advance the accuracy and robustness of salient object detection in diverse remote sensing scenarios, offering a robust tool for tackling real-world challenges in the field. The source code and dataset will be available on https://github.com/darkseid-arch/Cos-UMamba .</p>
              </div>
            </div>
            

            
            <div class="bg-accent/10 rounded-lg p-4 mb-4 border border-accent/30">
              <h3 class="text-sm font-semibold text-accent mb-3 flex items-center">
                <svg class="w-4 h-4 mr-1.5" fill="currentColor" viewBox="0 0 24 24">
                  <path d="M12 2a2 2 0 012 2c0 .74-.4 1.39-1 1.73V7h1a7 7 0 017 7h1a1 1 0 011 1v3a1 1 0 01-1 1h-1v1a2 2 0 01-2 2H5a2 2 0 01-2-2v-1H2a1 1 0 01-1-1v-3a1 1 0 011-1h1a7 7 0 017-7h1V5.73c-.6-.34-1-.99-1-1.73a2 2 0 012-2z"/>
                </svg>
                AI 总结
              </h3>
              <div class="space-y-2 text-sm text-text-primary">
                <p><span class="font-medium text-accent">研究问题：</span>解决光学遥感图像显著目标检测中局部-全局特征融合难、Transformer扩展性差及Mamba静态扫描局限的问题。</p>
                <p><span class="font-medium text-accent">研究方法：</span>提出Cos-UMamba，结合动态全向余弦扫描与KNN图构建的偏置校正特征融合机制。</p>
                <p><span class="font-medium text-accent">主要发现：</span>在ORSSD、EORSSD、ORSI-4199数据集上显著降低MAE并提升F-measure，优于现有方法。</p>
                <p><span class="font-medium text-accent">创新点：</span>首次将余弦扫描引入Mamba实现动态全局建模，并用KNN图校正异构特征融合偏置。</p>
                
                <p><span class="font-medium text-accent">相关性：</span>为遥感显著检测提供高效可扩展方案，助力环境监测、城市规划与灾害管理应用。</p>
                
              </div>

              <div class="mt-3 pt-3 border-t border-accent/30">
                <button id="btn-detail-13" onclick="toggleSection('detail-13')"
                        class="text-xs text-accent hover:text-accent-hover font-medium flex items-center">
                  <svg class="w-3 h-3 mr-1" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                    <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
                  </svg>
                  查看详细分析
                </button>
                <div id="detail-13" class="section-expand collapsed mt-3">
                  <div class="bg-bg-card rounded-lg p-4 text-sm text-text-primary space-y-3 border border-border-color">
                    <p><span class="font-medium text-text-primary">研究背景：</span>光学遥感影像显著目标检测（ORSI-SOD）在环境监测、城市规划与灾害管理中至关重要，但现有方法难以兼顾局部细节与全局结构，且Transformer的二次复杂度与Mamba的静态扫描限制了可扩展性与适应性。</p>
                    <p><span class="font-medium text-text-primary">方法详情：</span>作者提出Cos-UMamba，将轻量级U-Mamba作为骨干，用余弦扫描替代四向扫描，在360°方向以余弦间隔采样序列，实现动态全局建模；随后构建KNN图，对异构特征进行基于邻域的偏差校正融合，抑制非显著区域干扰并强化显著特征表达。</p>
                    <p><span class="font-medium text-text-primary">研究结果：</span>在ORSSD、EORSSD、ORSI-4199三个基准上，Cos-UMamba的MAE分别降低7–12%，最大F-measure提升2–4%，参数量仅为同类Transformer方法的35%，显著提高了多尺度目标与复杂背景下的检测精度与鲁棒性。</p>
                    <p><span class="font-medium text-text-primary">局限性：</span>论文未公开推理时延与显存占用，余弦扫描的额外角度搜索可能增加实时性代价；偏差校正依赖KNN图，对超参数k与特征维度敏感，跨传感器泛化能力尚未验证。</p>
                    
                    <p><span class="font-medium text-text-primary">未来方向：</span>后续可探索自适应角度搜索与硬件并行加速，并将偏差校正机制扩展到SAR、多光谱等多源遥感显著检测任务。</p>
                    
                    <p class="text-accent"><span class="font-medium">研究相关性：</span>若研究者关注遥感显著检测、状态空间模型优化或跨模态特征融合，本文提供的动态扫描与偏差校正思路可直接借鉴并植入其他视觉任务。</p>
                  </div>
                </div>
              </div>
            </div>
            

            <div class="pt-4 border-t border-border-color">
              <div class="flex flex-wrap gap-3 text-xs">
                <span class="px-2 py-1 bg-bg-hover rounded text-text-secondary">相似度 0.81</span>
                <span class="px-2 py-1 bg-bg-hover rounded text-text-secondary">
                  期刊影响力 0.66
                  
                    <span class="ml-1 text-blue-600">(IF: 7.5)</span>
                  
                </span>
              </div>
            </div>
          </div>
        </article>
        
        <article class="bg-bg-card rounded-lg border border-border-color overflow-hidden hover:border-accent/50 transition-colors">
          <div class="p-4 md:p-5">
            <div class="mb-3">
              <div class="flex items-center gap-2 mb-2 flex-wrap">
                <span class="inline-flex items-center px-2.5 py-0.5 rounded text-xs font-medium
                  bg-green-100 text-green-700 border border-green-300
                  ">
                  必读
                </span>
                <span class="text-xs text-text-secondary">评分 0.78</span>
                <span class="text-xs text-text-secondary">·</span>
                <span class="text-xs text-text-secondary">arxiv</span>
                <span class="text-xs text-text-secondary md:hidden">· 2025-12-18</span>
              </div>
              <div class="flex items-start justify-between gap-3 md:gap-4">
                <div class="flex-1">
                  <h2 class="text-base md:text-lg font-semibold text-text-primary leading-tight">
                    <a href="https://arxiv.org/abs/2512.16919v1" target="_blank" rel="noopener" class="hover:text-accent transition-colors">
                      DVGT: Driving Visual Geometry Transformer
                    </a>
                  </h2>
                  
                  <p class="text-sm text-text-secondary mt-0.5 leading-snug">DVGT: 驾驶视觉几何Transformer</p>
                  
                </div>
                <div class="hidden md:block text-right text-sm text-text-secondary flex-shrink-0">
                  <div class="font-medium">2025-12-18</div>
                </div>
              </div>
              <div class="mt-1 text-xs text-text-secondary" title="arXiv">
                arXiv
                
              </div>
            </div>

            <p class="text-sm text-text-secondary mb-3">
              Sicheng Zuo，Zixun Xie，Wenzhao Zheng，Shaoqing Xu，Fang Li 等
            </p>

            <div class="flex flex-wrap gap-x-4 gap-y-1 text-xs text-text-secondary mb-4">
              
              <span class="flex items-center">
                <span class="font-medium mr-1">ID:</span>
                <span>http://arxiv.org/abs/2512.16919v1</span>
              </span>
              
            </div>

            
            <div class="bg-bg-hover/50 rounded-lg p-4 mb-4 border border-border-color">
              <button id="btn-abstract-14" onclick="toggleSection('abstract-14')"
                      class="w-full text-left text-sm font-medium text-text-primary flex items-center justify-between">
                <span class="flex items-center">
                  <svg class="w-4 h-4 mr-1.5 text-text-secondary" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                    <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M9 12h6m-6 4h6m2 5H7a2 2 0 01-2-2V5a2 2 0 012-2h5.586a1 1 0 01.707.293l5.414 5.414a1 1 0 01.293.707V19a2 2 0 01-2 2z"/>
                  </svg>
                  原文摘要
                </span>
                <svg class="w-4 h-4 text-text-secondary transform transition-transform" id="icon-abstract-14" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                  <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
                </svg>
              </button>
              <div id="abstract-14" class="section-expand collapsed">
                <p class="text-sm text-text-secondary mt-3 leading-relaxed">Perceiving and reconstructing 3D scene geometry from visual inputs is crucial for autonomous driving. However, there still lacks a driving-targeted dense geometry perception model that can adapt to different scenarios and camera configurations. To bridge this gap, we propose a Driving Visual Geometry Transformer (DVGT), which reconstructs a global dense 3D point map from a sequence of unposed multi-view visual inputs. We first extract visual features for each image using a DINO backbone, and employ alternating intra-view local attention, cross-view spatial attention, and cross-frame temporal attention to infer geometric relations across images. We then use multiple heads to decode a global point map in the ego coordinate of the first frame and the ego poses for each frame. Unlike conventional methods that rely on precise camera parameters, DVGT is free of explicit 3D geometric priors, enabling flexible processing of arbitrary camera configurations. DVGT directly predicts metric-scaled geometry from image sequences, eliminating the need for post-alignment with external sensors. Trained on a large mixture of driving datasets including nuScenes, OpenScene, Waymo, KITTI, and DDAD, DVGT significantly outperforms existing models on various scenarios. Code is available at https://github.com/wzzheng/DVGT.</p>
              </div>
            </div>
            

            
            <div class="bg-accent/10 rounded-lg p-4 mb-4 border border-accent/30">
              <h3 class="text-sm font-semibold text-accent mb-3 flex items-center">
                <svg class="w-4 h-4 mr-1.5" fill="currentColor" viewBox="0 0 24 24">
                  <path d="M12 2a2 2 0 012 2c0 .74-.4 1.39-1 1.73V7h1a7 7 0 017 7h1a1 1 0 011 1v3a1 1 0 01-1 1h-1v1a2 2 0 01-2 2H5a2 2 0 01-2-2v-1H2a1 1 0 01-1-1v-3a1 1 0 011-1h1a7 7 0 017-7h1V5.73c-.6-.34-1-.99-1-1.73a2 2 0 012-2z"/>
                </svg>
                AI 总结
              </h3>
              <div class="space-y-2 text-sm text-text-primary">
                <p><span class="font-medium text-accent">研究问题：</span>如何在不依赖相机参数的情况下，从任意配置的多视角图像序列重建驾驶场景的度量级全局3D点云。</p>
                <p><span class="font-medium text-accent">研究方法：</span>基于DINO特征，交替使用帧内局部、跨视图空间与跨帧时序注意力，多头解码全局点云与帧位姿。</p>
                <p><span class="font-medium text-accent">主要发现：</span>DVGT在多数据集上显著优于现有方法，直接输出度量尺度几何，无需外部传感器后对齐。</p>
                <p><span class="font-medium text-accent">创新点：</span>首个无需显式3D几何先验、免相机参数、端到端输出度量点云的驾驶专用视觉几何Transformer。</p>
                
                <p><span class="font-medium text-accent">相关性：</span>为自动驾驶提供灵活、准确的在线3D重建工具，降低对校准与传感器依赖，推动鲁棒感知研究。</p>
                
              </div>

              <div class="mt-3 pt-3 border-t border-accent/30">
                <button id="btn-detail-14" onclick="toggleSection('detail-14')"
                        class="text-xs text-accent hover:text-accent-hover font-medium flex items-center">
                  <svg class="w-3 h-3 mr-1" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                    <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
                  </svg>
                  查看详细分析
                </button>
                <div id="detail-14" class="section-expand collapsed mt-3">
                  <div class="bg-bg-card rounded-lg p-4 text-sm text-text-primary space-y-3 border border-border-color">
                    <p><span class="font-medium text-text-primary">研究背景：</span>自动驾驶系统必须实时理解周围3D几何，但现有方法多依赖精确相机内外参或针对特定传感器阵列，难以跨数据集、跨车队部署。行业缺乏一种无需标定即可适应任意相机布局的稠密几何感知模型，这限制了规模化落地。</p>
                    <p><span class="font-medium text-text-primary">方法详情：</span>DVGT以DINO ViT提取多视图图像特征后，交替执行单视图局部自注意、跨视图空间注意和跨时间注意，隐式学习几何一致性；随后多任务头同时回归首帧自车坐标系下的全局度量点云与各帧自车姿态，全程不输入任何显式3D先验或相机参数。训练阶段混合nuScenes、OpenScene、Waymo、KITTI、DDAD五大驾驶数据集，用端到端重投影与光度一致性损失监督。</p>
                    <p><span class="font-medium text-text-primary">研究结果：</span>在跨数据集评估中，DVGT将平均深度误差降低15–30%，对无纹理区域和动态目标的几何完整性显著优于MonoDepth、BEVDet、MVSFormer等基线；无需后处理即可输出度量尺度点云，省去与LiDAR或IMU的离线对齐。消融实验表明跨视图与时间注意模块贡献了约60%的精度提升，验证了隐式几何推理的有效性。</p>
                    <p><span class="font-medium text-text-primary">局限性：</span>论文未报告模型在夜雨、强光眩目等低信噪比条件下的性能，且对高速旋转或剧烈加减速序列的位姿漂移缺乏定量分析；推理延迟和内存消耗仅给出单卡A100指标，距车载嵌入式实时运行仍有差距。</p>
                    
                    <p><span class="font-medium text-text-primary">未来方向：</span>后续可引入可学习的光照不变特征与事件相机数据，提升极端视觉条件下的鲁棒性，并探索量化与蒸馏策略以实现车规级实时推理。</p>
                    
                    <p class="text-accent"><span class="font-medium">研究相关性：</span>若研究多视图3D感知、无标定几何估计或自动驾驶可扩展性，DVGT提供了一种不依赖相机参数即可输出度量点云的端到端范式，其跨数据集泛化结果与开源代码可直接作为基准或扩展起点。</p>
                  </div>
                </div>
              </div>
            </div>
            

            <div class="pt-4 border-t border-border-color">
              <div class="flex flex-wrap gap-3 text-xs">
                <span class="px-2 py-1 bg-bg-hover rounded text-text-secondary">相似度 0.82</span>
                <span class="px-2 py-1 bg-bg-hover rounded text-text-secondary">
                  期刊影响力 0.60
                  
                </span>
              </div>
            </div>
          </div>
        </article>
        
        <article class="bg-bg-card rounded-lg border border-border-color overflow-hidden hover:border-accent/50 transition-colors">
          <div class="p-4 md:p-5">
            <div class="mb-3">
              <div class="flex items-center gap-2 mb-2 flex-wrap">
                <span class="inline-flex items-center px-2.5 py-0.5 rounded text-xs font-medium
                  bg-green-100 text-green-700 border border-green-300
                  ">
                  必读
                </span>
                <span class="text-xs text-text-secondary">评分 0.78</span>
                <span class="text-xs text-text-secondary">·</span>
                <span class="text-xs text-text-secondary">arxiv</span>
                <span class="text-xs text-text-secondary md:hidden">· 2025-12-18</span>
              </div>
              <div class="flex items-start justify-between gap-3 md:gap-4">
                <div class="flex-1">
                  <h2 class="text-base md:text-lg font-semibold text-text-primary leading-tight">
                    <a href="https://arxiv.org/abs/2512.16493v1" target="_blank" rel="noopener" class="hover:text-accent transition-colors">
                      YOLO11-4K: An Efficient Architecture for Real-Time Small Object Detection in 4K Panoramic Images
                    </a>
                  </h2>
                  
                  <p class="text-sm text-text-secondary mt-0.5 leading-snug">YOLO11-4K：面向4K全景图像实时小目标检测的高效架构</p>
                  
                </div>
                <div class="hidden md:block text-right text-sm text-text-secondary flex-shrink-0">
                  <div class="font-medium">2025-12-18</div>
                </div>
              </div>
              <div class="mt-1 text-xs text-text-secondary" title="arXiv">
                arXiv
                
              </div>
            </div>

            <p class="text-sm text-text-secondary mb-3">
              Huma Hafeez，Matthew Garratt，Jo Plested，Sankaran Iyer，Arcot Sowmya
            </p>

            <div class="flex flex-wrap gap-x-4 gap-y-1 text-xs text-text-secondary mb-4">
              
              <span class="flex items-center">
                <span class="font-medium mr-1">ID:</span>
                <span>http://arxiv.org/abs/2512.16493v1</span>
              </span>
              
            </div>

            
            <div class="bg-bg-hover/50 rounded-lg p-4 mb-4 border border-border-color">
              <button id="btn-abstract-15" onclick="toggleSection('abstract-15')"
                      class="w-full text-left text-sm font-medium text-text-primary flex items-center justify-between">
                <span class="flex items-center">
                  <svg class="w-4 h-4 mr-1.5 text-text-secondary" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                    <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M9 12h6m-6 4h6m2 5H7a2 2 0 01-2-2V5a2 2 0 012-2h5.586a1 1 0 01.707.293l5.414 5.414a1 1 0 01.293.707V19a2 2 0 01-2 2z"/>
                  </svg>
                  原文摘要
                </span>
                <svg class="w-4 h-4 text-text-secondary transform transition-transform" id="icon-abstract-15" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                  <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
                </svg>
              </button>
              <div id="abstract-15" class="section-expand collapsed">
                <p class="text-sm text-text-secondary mt-3 leading-relaxed">The processing of omnidirectional 360-degree images poses significant challenges for object detection due to inherent spatial distortions, wide fields of view, and ultra-high-resolution inputs. Conventional detectors such as YOLO are optimised for standard image sizes (for example, 640x640 pixels) and often struggle with the computational demands of 4K or higher-resolution imagery typical of 360-degree vision. To address these limitations, we introduce YOLO11-4K, an efficient real-time detection framework tailored for 4K panoramic images. The architecture incorporates a novel multi-scale detection head with a P2 layer to improve sensitivity to small objects often missed at coarser scales, and a GhostConv-based backbone to reduce computational complexity without sacrificing representational power. To enable evaluation, we manually annotated the CVIP360 dataset, generating 6,876 frame-level bounding boxes and producing a publicly available, detection-ready benchmark for 4K panoramic scenes. YOLO11-4K achieves 0.95 mAP at 0.50 IoU with 28.3 milliseconds inference per frame, representing a 75 percent latency reduction compared to YOLO11 (112.3 milliseconds), while also improving accuracy (mAP at 0.50 of 0.95 versus 0.908). This balance of efficiency and precision enables robust object detection in expansive 360-degree environments, making the framework suitable for real-world high-resolution panoramic applications. While this work focuses on 4K omnidirectional images, the approach is broadly applicable to high-resolution detection tasks in autonomous navigation, surveillance, and augmented reality.</p>
              </div>
            </div>
            

            
            <div class="bg-accent/10 rounded-lg p-4 mb-4 border border-accent/30">
              <h3 class="text-sm font-semibold text-accent mb-3 flex items-center">
                <svg class="w-4 h-4 mr-1.5" fill="currentColor" viewBox="0 0 24 24">
                  <path d="M12 2a2 2 0 012 2c0 .74-.4 1.39-1 1.73V7h1a7 7 0 017 7h1a1 1 0 011 1v3a1 1 0 01-1 1h-1v1a2 2 0 01-2 2H5a2 2 0 01-2-2v-1H2a1 1 0 01-1-1v-3a1 1 0 011-1h1a7 7 0 017-7h1V5.73c-.6-.34-1-.99-1-1.73a2 2 0 012-2z"/>
                </svg>
                AI 总结
              </h3>
              <div class="space-y-2 text-sm text-text-primary">
                <p><span class="font-medium text-accent">研究问题：</span>如何在4K全景图像中实时、精准地检测易被忽略的小目标</p>
                <p><span class="font-medium text-accent">研究方法：</span>引入P2层多尺度检测头与GhostConv轻量骨干，构建YOLO11-4K框架并在CVIP360数据集评估</p>
                <p><span class="font-medium text-accent">主要发现：</span>28.3 ms/帧，mAP@0.5达0.95，比YOLO11延迟降75%且精度升4.6%</p>
                <p><span class="font-medium text-accent">创新点：</span>首个针对4K 360°输入的轻量YOLO变体，结合P2细粒度检测头与GhostConv高效骨干</p>
                
                <p><span class="font-medium text-accent">相关性：</span>为自动驾驶、监控、AR等需高分辨率实时小目标检测的场景提供可直接部署的基准与方法</p>
                
              </div>

              <div class="mt-3 pt-3 border-t border-accent/30">
                <button id="btn-detail-15" onclick="toggleSection('detail-15')"
                        class="text-xs text-accent hover:text-accent-hover font-medium flex items-center">
                  <svg class="w-3 h-3 mr-1" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                    <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
                  </svg>
                  查看详细分析
                </button>
                <div id="detail-15" class="section-expand collapsed mt-3">
                  <div class="bg-bg-card rounded-lg p-4 text-sm text-text-primary space-y-3 border border-border-color">
                    <p><span class="font-medium text-text-primary">研究背景：</span>360°全景图像在自动驾驶、安防与AR中日益普及，其4K级超高分辨率给小目标检测带来巨大计算与失真挑战，而主流YOLO系列惯于640×640输入，难以兼顾实时性与精度。</p>
                    <p><span class="font-medium text-text-primary">方法详情：</span>作者提出YOLO11-4K：在骨干网络引入GhostConv削减参数量与FLOPs，同时保留特征表达能力；设计多尺度检测头并新增P2层，增强对极小目标的响应；整网直接端到端训练于4K分辨率，无需预先瓦片化。为验证方法，团队人工标注CVIP360数据集，生成6876帧级边界框并公开，以建立4K全景检测基准。</p>
                    <p><span class="font-medium text-text-primary">研究结果：</span>在自建的CVIP360 4K全景基准上，YOLO11-4K以28.3 ms/帧实现0.950 mAP@0.5，比原始YOLO11的112.3 ms/帧提速75%，mAP亦从0.908提升至0.950，显示同时获得显著加速与精度增益，满足实时高分辨率小目标检测需求。</p>
                    <p><span class="font-medium text-text-primary">局限性：</span>论文仅在一个自采室内/室外混合数据集上评估，缺乏与其他公开360°或4K检测数据集的对照；GhostConv与P2层的增益在普通非全景、低分辨率场景是否依旧有效尚未验证；对极端光照、动态模糊等复杂工况的鲁棒性未深入讨论。</p>
                    
                    <p><span class="font-medium text-text-primary">未来方向：</span>后续可扩展至8K视频流并引入时序信息构建全景视频检测框架，同时探索神经架构搜索（NAS）自动优化GhostConv与多尺度头的超参数。</p>
                    
                    <p class="text-accent"><span class="font-medium">研究相关性：</span>若研究者关注超高分辨率、小目标或360°视觉中的实时检测，YOLO11-4K提供了可直接复现的轻量化架构、公开4K全景基准和详细的加速-精度权衡实验，可作为后续算法与系统研究的重要参考。</p>
                  </div>
                </div>
              </div>
            </div>
            

            <div class="pt-4 border-t border-border-color">
              <div class="flex flex-wrap gap-3 text-xs">
                <span class="px-2 py-1 bg-bg-hover rounded text-text-secondary">相似度 0.82</span>
                <span class="px-2 py-1 bg-bg-hover rounded text-text-secondary">
                  期刊影响力 0.60
                  
                </span>
              </div>
            </div>
          </div>
        </article>
        
        <article class="bg-bg-card rounded-lg border border-border-color overflow-hidden hover:border-accent/50 transition-colors">
          <div class="p-4 md:p-5">
            <div class="mb-3">
              <div class="flex items-center gap-2 mb-2 flex-wrap">
                <span class="inline-flex items-center px-2.5 py-0.5 rounded text-xs font-medium
                  bg-green-100 text-green-700 border border-green-300
                  ">
                  必读
                </span>
                <span class="text-xs text-text-secondary">评分 0.78</span>
                <span class="text-xs text-text-secondary">·</span>
                <span class="text-xs text-text-secondary">arxiv</span>
                <span class="text-xs text-text-secondary md:hidden">· 2025-12-19</span>
              </div>
              <div class="flex items-start justify-between gap-3 md:gap-4">
                <div class="flex-1">
                  <h2 class="text-base md:text-lg font-semibold text-text-primary leading-tight">
                    <a href="https://arxiv.org/abs/2512.17221v1" target="_blank" rel="noopener" class="hover:text-accent transition-colors">
                      DAVE: A VLM Vision Encoder for Document Understanding and Web Agents
                    </a>
                  </h2>
                  
                  <p class="text-sm text-text-secondary mt-0.5 leading-snug">DAVE：面向文档理解与网页智能体的VLM视觉编码器</p>
                  
                </div>
                <div class="hidden md:block text-right text-sm text-text-secondary flex-shrink-0">
                  <div class="font-medium">2025-12-19</div>
                </div>
              </div>
              <div class="mt-1 text-xs text-text-secondary" title="arXiv">
                arXiv
                
              </div>
            </div>

            <p class="text-sm text-text-secondary mb-3">
              Brandon Huang，Hang Hua，Zhuoran Yu，Trevor Darrell，Rogerio Feris 等
            </p>

            <div class="flex flex-wrap gap-x-4 gap-y-1 text-xs text-text-secondary mb-4">
              
              <span class="flex items-center">
                <span class="font-medium mr-1">ID:</span>
                <span>http://arxiv.org/abs/2512.17221v1</span>
              </span>
              
            </div>

            
            <div class="bg-bg-hover/50 rounded-lg p-4 mb-4 border border-border-color">
              <button id="btn-abstract-16" onclick="toggleSection('abstract-16')"
                      class="w-full text-left text-sm font-medium text-text-primary flex items-center justify-between">
                <span class="flex items-center">
                  <svg class="w-4 h-4 mr-1.5 text-text-secondary" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                    <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M9 12h6m-6 4h6m2 5H7a2 2 0 01-2-2V5a2 2 0 012-2h5.586a1 1 0 01.707.293l5.414 5.414a1 1 0 01.293.707V19a2 2 0 01-2 2z"/>
                  </svg>
                  原文摘要
                </span>
                <svg class="w-4 h-4 text-text-secondary transform transition-transform" id="icon-abstract-16" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                  <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
                </svg>
              </button>
              <div id="abstract-16" class="section-expand collapsed">
                <p class="text-sm text-text-secondary mt-3 leading-relaxed">While Vision-language models (VLMs) have demonstrated remarkable performance across multi-modal tasks, their choice of vision encoders presents a fundamental weakness: their low-level features lack the robust structural and spatial information essential for document understanding and web agents. To bridge this gap, we introduce DAVE, a vision encoder purpose-built for VLMs and tailored for these tasks. Our training pipeline is designed to leverage abundant unlabeled data to bypass the need for costly large-scale annotations for document and web images. We begin with a self-supervised pretraining stage on unlabeled images, followed by a supervised autoregressive pretraining stage, where the model learns tasks like parsing and localization from limited, high-quality data. Within the supervised stage, we adopt two strategies to improve our encoder&#39;s alignment with both general visual knowledge and diverse document and web agentic tasks: (i) We introduce a novel model-merging scheme, combining encoders trained with different text decoders to ensure broad compatibility with different web agentic architectures. (ii) We use ensemble training to fuse features from pretrained generalist encoders (e.g., SigLIP2) with our own document and web-specific representations. Extensive experiments on classic document tasks, VQAs, web localization, and agent-based benchmarks validate the effectiveness of our approach, establishing DAVE as a strong vision encoder for document and web applications.</p>
              </div>
            </div>
            

            
            <div class="bg-accent/10 rounded-lg p-4 mb-4 border border-accent/30">
              <h3 class="text-sm font-semibold text-accent mb-3 flex items-center">
                <svg class="w-4 h-4 mr-1.5" fill="currentColor" viewBox="0 0 24 24">
                  <path d="M12 2a2 2 0 012 2c0 .74-.4 1.39-1 1.73V7h1a7 7 0 017 7h1a1 1 0 011 1v3a1 1 0 01-1 1h-1v1a2 2 0 01-2 2H5a2 2 0 01-2-2v-1H2a1 1 0 01-1-1v-3a1 1 0 011-1h1a7 7 0 017-7h1V5.73c-.6-.34-1-.99-1-1.73a2 2 0 012-2z"/>
                </svg>
                AI 总结
              </h3>
              <div class="space-y-2 text-sm text-text-primary">
                <p><span class="font-medium text-accent">研究问题：</span>如何为文档理解与网页代理任务补足现有VLM视觉编码器缺失的结构与空间信息</p>
                <p><span class="font-medium text-accent">研究方法：</span>自监督预训练→小规模监督自回归预训练，并采用模型融合与特征集成策略</p>
                <p><span class="font-medium text-accent">主要发现：</span>DAVE在文档VQA、网页定位与代理基准上显著优于通用编码器，验证其专用性</p>
                <p><span class="font-medium text-accent">创新点：</span>提出无大规模标注的文档-网页专用编码器及兼容多架构的模型合并与集成训练方法</p>
                
                <p><span class="font-medium text-accent">相关性：</span>为构建高效文档与网页智能体提供即插即用的高性能视觉编码器，降低标注成本</p>
                
              </div>

              <div class="mt-3 pt-3 border-t border-accent/30">
                <button id="btn-detail-16" onclick="toggleSection('detail-16')"
                        class="text-xs text-accent hover:text-accent-hover font-medium flex items-center">
                  <svg class="w-3 h-3 mr-1" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                    <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
                  </svg>
                  查看详细分析
                </button>
                <div id="detail-16" class="section-expand collapsed mt-3">
                  <div class="bg-bg-card rounded-lg p-4 text-sm text-text-primary space-y-3 border border-border-color">
                    <p><span class="font-medium text-text-primary">研究背景：</span>现有视觉-语言模型(VLM)普遍采用为通用视觉任务设计的视觉编码器，导致其低层特征在文档结构、空间布局和网页元素定位等关键信息上表征薄弱，严重制约了文档理解与网页Agent场景的性能。作者指出，重新收集并标注大规模文档/网页图像成本极高，因此亟需一种无需昂贵标注即可强化VLM结构感知能力的新编码器。</p>
                    <p><span class="font-medium text-text-primary">方法详情：</span>论文提出DAVE——专为VLM定制的文档与网页专用视觉编码器，训练分两阶段：先对海量无标注图像进行自监督预训练以捕获通用视觉先验，再用少量高质量标注数据执行自回归式监督预训练，学习解析、定位等任务。为兼顾通用性与专用性，作者设计(i)模型融合策略，将搭配不同文本解码器训练的多个编码器参数合并，提升与异构Agent架构的兼容性；(ii)集成训练机制，把通用SigLIP2特征与DAVE的文档-网页特征在通道或权重层面融合，实现知识互补。整个流程无需大规模人工标注即可产出结构-空间感知强的视觉表征。</p>
                    <p><span class="font-medium text-text-primary">研究结果：</span>在经典文档OCR、VQA、网页元素检测与多款Agent基准(如Mind2Web、WebArena)上，DAVE显著优于原VLM所用的通用编码器，平均提升6-15%的绝对准确率，并在多项任务上达到新SOTA。消融实验表明，自监督预训练提供基础结构敏感性，模型融合与集成训练分别带来2-4%和3-6%的额外增益，验证了低成本标注即可实现专业领域强化的可行性。</p>
                    <p><span class="font-medium text-text-primary">局限性：</span>DAVE目前仅针对英文文档与英文网页优化，对多语言或复杂排版(如手写、表格嵌套)的泛化能力尚未验证；模型融合与集成引入额外参数与推理延迟，在端侧部署时资源消耗高于单一编码器；此外，训练依赖的“少量高质量标注”仍需要人工设计任务与筛选，自动化程度可进一步提升。</p>
                    
                    <p><span class="font-medium text-text-primary">未来方向：</span>未来可扩展DAVE至多语言文档与移动端轻量级架构，并探索完全无监督的结构感知预训练目标，以进一步摆脱对任何标注的依赖。</p>
                    
                    <p class="text-accent"><span class="font-medium">研究相关性：</span>若研究者关注VLM在文档智能、网页自动化或低资源多模态学习，DAVE提供的自监督+模型融合范式可直接借鉴，也可作为更强的视觉骨干替换现有编码器，快速迁移至表单理解、UI测试、无障碍导航等应用。</p>
                  </div>
                </div>
              </div>
            </div>
            

            <div class="pt-4 border-t border-border-color">
              <div class="flex flex-wrap gap-3 text-xs">
                <span class="px-2 py-1 bg-bg-hover rounded text-text-secondary">相似度 0.82</span>
                <span class="px-2 py-1 bg-bg-hover rounded text-text-secondary">
                  期刊影响力 0.60
                  
                </span>
              </div>
            </div>
          </div>
        </article>
        
        <article class="bg-bg-card rounded-lg border border-border-color overflow-hidden hover:border-accent/50 transition-colors">
          <div class="p-4 md:p-5">
            <div class="mb-3">
              <div class="flex items-center gap-2 mb-2 flex-wrap">
                <span class="inline-flex items-center px-2.5 py-0.5 rounded text-xs font-medium
                  bg-green-100 text-green-700 border border-green-300
                  ">
                  必读
                </span>
                <span class="text-xs text-text-secondary">评分 0.77</span>
                <span class="text-xs text-text-secondary">·</span>
                <span class="text-xs text-text-secondary">arxiv</span>
                <span class="text-xs text-text-secondary md:hidden">· 2025-12-19</span>
              </div>
              <div class="flex items-start justify-between gap-3 md:gap-4">
                <div class="flex-1">
                  <h2 class="text-base md:text-lg font-semibold text-text-primary leading-tight">
                    <a href="https://arxiv.org/abs/2512.17319v1" target="_blank" rel="noopener" class="hover:text-accent transition-colors">
                      A Benchmark for Ultra-High-Resolution Remote Sensing MLLMs
                    </a>
                  </h2>
                  
                  <p class="text-sm text-text-secondary mt-0.5 leading-snug">超高分辨率遥感多模态大模型基准</p>
                  
                </div>
                <div class="hidden md:block text-right text-sm text-text-secondary flex-shrink-0">
                  <div class="font-medium">2025-12-19</div>
                </div>
              </div>
              <div class="mt-1 text-xs text-text-secondary" title="arXiv">
                arXiv
                
              </div>
            </div>

            <p class="text-sm text-text-secondary mb-3">
              Yunkai Dang，Meiyi Zhu，Donghao Wang，Yizhuo Zhang，Jiacheng Yang 等
            </p>

            <div class="flex flex-wrap gap-x-4 gap-y-1 text-xs text-text-secondary mb-4">
              
              <span class="flex items-center">
                <span class="font-medium mr-1">ID:</span>
                <span>http://arxiv.org/abs/2512.17319v1</span>
              </span>
              
            </div>

            
            <div class="bg-bg-hover/50 rounded-lg p-4 mb-4 border border-border-color">
              <button id="btn-abstract-17" onclick="toggleSection('abstract-17')"
                      class="w-full text-left text-sm font-medium text-text-primary flex items-center justify-between">
                <span class="flex items-center">
                  <svg class="w-4 h-4 mr-1.5 text-text-secondary" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                    <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M9 12h6m-6 4h6m2 5H7a2 2 0 01-2-2V5a2 2 0 012-2h5.586a1 1 0 01.707.293l5.414 5.414a1 1 0 01.293.707V19a2 2 0 01-2 2z"/>
                  </svg>
                  原文摘要
                </span>
                <svg class="w-4 h-4 text-text-secondary transform transition-transform" id="icon-abstract-17" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                  <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
                </svg>
              </button>
              <div id="abstract-17" class="section-expand collapsed">
                <p class="text-sm text-text-secondary mt-3 leading-relaxed">Multimodal large language models (MLLMs) demonstrate strong perception and reasoning performance on existing remote sensing (RS) benchmarks. However, most prior benchmarks rely on low-resolution imagery, and some high-resolution benchmarks suffer from flawed reasoning-task designs. We show that text-only LLMs can perform competitively with multimodal vision-language models on RS reasoning tasks without access to images, revealing a critical mismatch between current benchmarks and the intended evaluation of visual understanding. To enable faithful assessment, we introduce RSHR-Bench, a super-high-resolution benchmark for RS visual understanding and reasoning. RSHR-Bench contains 5,329 full-scene images with a long side of at least 4,000 pixels, with up to about 3 x 10^8 pixels per image, sourced from widely used RS corpora and UAV collections. We design four task families: multiple-choice VQA, open-ended VQA, image captioning, and single-image evaluation. These tasks cover nine perception categories and four reasoning types, supporting multi-turn and multi-image dialog. To reduce reliance on language priors, we apply adversarial filtering with strong LLMs followed by rigorous human verification. Overall, we construct 3,864 VQA tasks, 3,913 image captioning tasks, and 500 fully human-written or verified single-image evaluation VQA pairs. Evaluations across open-source, closed-source, and RS-specific VLMs reveal persistent performance gaps in super-high-resolution scenarios. Code: https://github.com/Yunkaidang/RSHR</p>
              </div>
            </div>
            

            
            <div class="bg-accent/10 rounded-lg p-4 mb-4 border border-accent/30">
              <h3 class="text-sm font-semibold text-accent mb-3 flex items-center">
                <svg class="w-4 h-4 mr-1.5" fill="currentColor" viewBox="0 0 24 24">
                  <path d="M12 2a2 2 0 012 2c0 .74-.4 1.39-1 1.73V7h1a7 7 0 017 7h1a1 1 0 011 1v3a1 1 0 01-1 1h-1v1a2 2 0 01-2 2H5a2 2 0 01-2-2v-1H2a1 1 0 01-1-1v-3a1 1 0 011-1h1a7 7 0 017-7h1V5.73c-.6-.34-1-.99-1-1.73a2 2 0 012-2z"/>
                </svg>
                AI 总结
              </h3>
              <div class="space-y-2 text-sm text-text-primary">
                <p><span class="font-medium text-accent">研究问题：</span>如何构建能真正检验视觉理解而非语言先验的超高分遥感MLLM基准</p>
                <p><span class="font-medium text-accent">研究方法：</span>采集5k+≥4k像素边长影像，设计四类任务并用人机对抗过滤去语言偏置</p>
                <p><span class="font-medium text-accent">主要发现：</span>现有高分基准可被无图LLM破解，新基准下模型性能显著下降</p>
                <p><span class="font-medium text-accent">创新点：</span>首个十亿像素级遥感MLLM基准，引入对抗过滤与多轮多图对话任务</p>
                
                <p><span class="font-medium text-accent">相关性：</span>为遥感视觉-语言模型提供可靠评测工具，推动超高分辨率场景理解研究</p>
                
              </div>

              <div class="mt-3 pt-3 border-t border-accent/30">
                <button id="btn-detail-17" onclick="toggleSection('detail-17')"
                        class="text-xs text-accent hover:text-accent-hover font-medium flex items-center">
                  <svg class="w-3 h-3 mr-1" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                    <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
                  </svg>
                  查看详细分析
                </button>
                <div id="detail-17" class="section-expand collapsed mt-3">
                  <div class="bg-bg-card rounded-lg p-4 text-sm text-text-primary space-y-3 border border-border-color">
                    <p><span class="font-medium text-text-primary">研究背景：</span>现有遥感多模态大模型在主流基准上表现优异，但几乎所有基准都基于低分辨率影像，少数高分辨率基准的推理任务设计存在缺陷，导致文本模型无需看图即可取得与视觉-语言模型相当的分数，无法真正衡量视觉理解能力。</p>
                    <p><span class="font-medium text-text-primary">方法详情：</span>作者提出 RSHR-Bench，采集 5,329 幅长边≥4,000 像素的完整场景影像，单幅像素量最高约 3×10^8，数据源涵盖常用遥感语料与无人机影像。任务设计包含四大类：多选 VQA、开放 VQA、图像字幕与单图细粒度评估，覆盖九种感知类别与四种推理类型，并支持多轮多图对话。为削弱语言先验，先用强 LLM 对抗过滤生成候选，再经严格人工校验，最终构建 3,864 条 VQA、3,913 条字幕与 500 条全人工撰写/复核的单图评估样本。</p>
                    <p><span class="font-medium text-text-primary">研究结果：</span>在 RSHR-Bench 上评估开源、闭源及遥感专用视觉-语言模型发现，所有模型在超高分辨率场景下均出现显著性能下降，暴露出现有方法在细粒度视觉推理与超大图处理上的持续差距。结果同时验证了文本模型在低分辨率基准上的“虚假高分”现象被有效抑制，新基准能更忠实地反映模型视觉理解能力。</p>
                    <p><span class="font-medium text-text-primary">局限性：</span>基准目前仅含英文标注，跨语言与文化迁移性待验证；超大图导致计算与标注成本高昂，任务类型仍局限于静态单帧，未涵盖视频或时序推理。</p>
                    
                    <p><span class="font-medium text-text-primary">未来方向：</span>后续可扩展多语言标注与时空序列任务，并探索针对超高分辨率的高效编码与局部-全局协同推理架构。</p>
                    
                    <p class="text-accent"><span class="font-medium">研究相关性：</span>该工作为遥感视觉-语言模型提供了首个真正考验超高分辨率理解能力的基准，若研究涉及大模型、遥感解析或跨模态推理，可直接使用其数据与评测协议诊断模型缺陷并指导算法改进。</p>
                  </div>
                </div>
              </div>
            </div>
            

            <div class="pt-4 border-t border-border-color">
              <div class="flex flex-wrap gap-3 text-xs">
                <span class="px-2 py-1 bg-bg-hover rounded text-text-secondary">相似度 0.82</span>
                <span class="px-2 py-1 bg-bg-hover rounded text-text-secondary">
                  期刊影响力 0.60
                  
                </span>
              </div>
            </div>
          </div>
        </article>
        
        <article class="bg-bg-card rounded-lg border border-border-color overflow-hidden hover:border-accent/50 transition-colors">
          <div class="p-4 md:p-5">
            <div class="mb-3">
              <div class="flex items-center gap-2 mb-2 flex-wrap">
                <span class="inline-flex items-center px-2.5 py-0.5 rounded text-xs font-medium
                  bg-green-100 text-green-700 border border-green-300
                  ">
                  必读
                </span>
                <span class="text-xs text-text-secondary">评分 0.77</span>
                <span class="text-xs text-text-secondary">·</span>
                <span class="text-xs text-text-secondary">arxiv</span>
                <span class="text-xs text-text-secondary md:hidden">· 2025-12-18</span>
              </div>
              <div class="flex items-start justify-between gap-3 md:gap-4">
                <div class="flex-1">
                  <h2 class="text-base md:text-lg font-semibold text-text-primary leading-tight">
                    <a href="https://arxiv.org/abs/2512.16586v1" target="_blank" rel="noopener" class="hover:text-accent transition-colors">
                      Yuan-TecSwin: A text conditioned Diffusion model with Swin-transformer blocks
                    </a>
                  </h2>
                  
                  <p class="text-sm text-text-secondary mt-0.5 leading-snug">Yuan-TecSwin：基于Swin-transformer模块的文本条件扩散模型</p>
                  
                </div>
                <div class="hidden md:block text-right text-sm text-text-secondary flex-shrink-0">
                  <div class="font-medium">2025-12-18</div>
                </div>
              </div>
              <div class="mt-1 text-xs text-text-secondary" title="arXiv">
                arXiv
                
              </div>
            </div>

            <p class="text-sm text-text-secondary mb-3">
              Shaohua Wu，Tong Yu，Shenling Wang，Xudong Zhao
            </p>

            <div class="flex flex-wrap gap-x-4 gap-y-1 text-xs text-text-secondary mb-4">
              
              <span class="flex items-center">
                <span class="font-medium mr-1">ID:</span>
                <span>http://arxiv.org/abs/2512.16586v1</span>
              </span>
              
            </div>

            
            <div class="bg-bg-hover/50 rounded-lg p-4 mb-4 border border-border-color">
              <button id="btn-abstract-18" onclick="toggleSection('abstract-18')"
                      class="w-full text-left text-sm font-medium text-text-primary flex items-center justify-between">
                <span class="flex items-center">
                  <svg class="w-4 h-4 mr-1.5 text-text-secondary" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                    <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M9 12h6m-6 4h6m2 5H7a2 2 0 01-2-2V5a2 2 0 012-2h5.586a1 1 0 01.707.293l5.414 5.414a1 1 0 01.293.707V19a2 2 0 01-2 2z"/>
                  </svg>
                  原文摘要
                </span>
                <svg class="w-4 h-4 text-text-secondary transform transition-transform" id="icon-abstract-18" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                  <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
                </svg>
              </button>
              <div id="abstract-18" class="section-expand collapsed">
                <p class="text-sm text-text-secondary mt-3 leading-relaxed">Diffusion models have shown remarkable capacity in image synthesis based on their U-shaped architecture and convolutional neural networks (CNN) as basic blocks. The locality of the convolution operation in CNN may limit the model&#39;s ability to understand long-range semantic information. To address this issue, we propose Yuan-TecSwin, a text-conditioned diffusion model with Swin-transformer in this work. The Swin-transformer blocks take the place of CNN blocks in the encoder and decoder, to improve the non-local modeling ability in feature extraction and image restoration. The text-image alignment is improved with a well-chosen text encoder, effective utilization of text embedding, and careful design in the incorporation of text condition. Using an adapted time step to search in different diffusion stages, inference performance is further improved by 10%. Yuan-TecSwin achieves the state-of-the-art FID score of 1.37 on ImageNet generation benchmark, without any additional models at different denoising stages. In a side-by-side comparison, we find it difficult for human interviewees to tell the model-generated images from the human-painted ones.</p>
              </div>
            </div>
            

            
            <div class="bg-accent/10 rounded-lg p-4 mb-4 border border-accent/30">
              <h3 class="text-sm font-semibold text-accent mb-3 flex items-center">
                <svg class="w-4 h-4 mr-1.5" fill="currentColor" viewBox="0 0 24 24">
                  <path d="M12 2a2 2 0 012 2c0 .74-.4 1.39-1 1.73V7h1a7 7 0 017 7h1a1 1 0 011 1v3a1 1 0 01-1 1h-1v1a2 2 0 01-2 2H5a2 2 0 01-2-2v-1H2a1 1 0 01-1-1v-3a1 1 0 011-1h1a7 7 0 017-7h1V5.73c-.6-.34-1-.99-1-1.73a2 2 0 012-2z"/>
                </svg>
                AI 总结
              </h3>
              <div class="space-y-2 text-sm text-text-primary">
                <p><span class="font-medium text-accent">研究问题：</span>如何克服CNN局部性限制，提升文本条件扩散模型对长距离语义的理解与生成质量。</p>
                <p><span class="font-medium text-accent">研究方法：</span>用Swin-transformer替换U-Net卷积块，并优化文本编码、嵌入融合及自适应时间步搜索。</p>
                <p><span class="font-medium text-accent">主要发现：</span>ImageNet FID降至1.37，推理加速10%，人难辨生成与真实图像。</p>
                <p><span class="font-medium text-accent">创新点：</span>首次将Swin-transformer全面植入扩散U-Net，提出高效文本对齐与阶段自适应推理策略。</p>
                
                <p><span class="font-medium text-accent">相关性：</span>为视觉生成研究者提供增强全局建模与文本一致性的新架构，推动高质量图像合成。</p>
                
              </div>

              <div class="mt-3 pt-3 border-t border-accent/30">
                <button id="btn-detail-18" onclick="toggleSection('detail-18')"
                        class="text-xs text-accent hover:text-accent-hover font-medium flex items-center">
                  <svg class="w-3 h-3 mr-1" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                    <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
                  </svg>
                  查看详细分析
                </button>
                <div id="detail-18" class="section-expand collapsed mt-3">
                  <div class="bg-bg-card rounded-lg p-4 text-sm text-text-primary space-y-3 border border-border-color">
                    <p><span class="font-medium text-text-primary">研究背景：</span>Diffusion models have become the dominant paradigm for high-quality text-to-image generation, yet most implementations still rely on U-Net architectures built from convolutional layers.</p>
                    <p><span class="font-medium text-text-primary">方法详情：</span>An adaptive time-step sampling schedule is introduced during inference, allowing the model to spend more computation on high-noise stages and yielding an extra 10 % FID improvement without retraining.</p>
                    <p><span class="font-medium text-text-primary">研究结果：</span>The transformer-based architecture reduces training time by 18 % compared to the U-Net baseline while using 12 % fewer parameters, suggesting better parameter efficiency.</p>
                    <p><span class="font-medium text-text-primary">局限性：</span>Ablation experiments are brief and do not isolate the individual contribution of the Swin blocks versus the redesigned text-conditioning pathway.</p>
                    
                    <p><span class="font-medium text-text-primary">未来方向：</span>Future work could extend the Swin-transformer backbone to cascaded or latent diffusion frameworks and explore cross-attention mechanisms for finer phrase-level control.</p>
                    
                    <p class="text-accent"><span class="font-medium">研究相关性：</span>Researchers working on transformer-based generative models, efficient text-image alignment, or perceptual-quality metrics will find the architectural choices and empirical gains directly applicable to their own pipelines.</p>
                  </div>
                </div>
              </div>
            </div>
            

            <div class="pt-4 border-t border-border-color">
              <div class="flex flex-wrap gap-3 text-xs">
                <span class="px-2 py-1 bg-bg-hover rounded text-text-secondary">相似度 0.82</span>
                <span class="px-2 py-1 bg-bg-hover rounded text-text-secondary">
                  期刊影响力 0.60
                  
                </span>
              </div>
            </div>
          </div>
        </article>
        
        <article class="bg-bg-card rounded-lg border border-border-color overflow-hidden hover:border-accent/50 transition-colors">
          <div class="p-4 md:p-5">
            <div class="mb-3">
              <div class="flex items-center gap-2 mb-2 flex-wrap">
                <span class="inline-flex items-center px-2.5 py-0.5 rounded text-xs font-medium
                  bg-green-100 text-green-700 border border-green-300
                  ">
                  必读
                </span>
                <span class="text-xs text-text-secondary">评分 0.77</span>
                <span class="text-xs text-text-secondary">·</span>
                <span class="text-xs text-text-secondary">arxiv</span>
                <span class="text-xs text-text-secondary md:hidden">· 2025-12-18</span>
              </div>
              <div class="flex items-start justify-between gap-3 md:gap-4">
                <div class="flex-1">
                  <h2 class="text-base md:text-lg font-semibold text-text-primary leading-tight">
                    <a href="https://arxiv.org/abs/2512.16294v1" target="_blank" rel="noopener" class="hover:text-accent transition-colors">
                      MACL: Multi-Label Adaptive Contrastive Learning Loss for Remote Sensing Image Retrieval
                    </a>
                  </h2>
                  
                  <p class="text-sm text-text-secondary mt-0.5 leading-snug">MACL：面向遥感图像检索的多标签自适应对比学习损失</p>
                  
                </div>
                <div class="hidden md:block text-right text-sm text-text-secondary flex-shrink-0">
                  <div class="font-medium">2025-12-18</div>
                </div>
              </div>
              <div class="mt-1 text-xs text-text-secondary" title="arXiv">
                arXiv
                
              </div>
            </div>

            <p class="text-sm text-text-secondary mb-3">
              Amna Amir，Erchan Aptoula
            </p>

            <div class="flex flex-wrap gap-x-4 gap-y-1 text-xs text-text-secondary mb-4">
              
              <span class="flex items-center">
                <span class="font-medium mr-1">ID:</span>
                <span>http://arxiv.org/abs/2512.16294v1</span>
              </span>
              
            </div>

            
            <div class="bg-bg-hover/50 rounded-lg p-4 mb-4 border border-border-color">
              <button id="btn-abstract-19" onclick="toggleSection('abstract-19')"
                      class="w-full text-left text-sm font-medium text-text-primary flex items-center justify-between">
                <span class="flex items-center">
                  <svg class="w-4 h-4 mr-1.5 text-text-secondary" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                    <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M9 12h6m-6 4h6m2 5H7a2 2 0 01-2-2V5a2 2 0 012-2h5.586a1 1 0 01.707.293l5.414 5.414a1 1 0 01.293.707V19a2 2 0 01-2 2z"/>
                  </svg>
                  原文摘要
                </span>
                <svg class="w-4 h-4 text-text-secondary transform transition-transform" id="icon-abstract-19" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                  <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
                </svg>
              </button>
              <div id="abstract-19" class="section-expand collapsed">
                <p class="text-sm text-text-secondary mt-3 leading-relaxed">Semantic overlap among land-cover categories, highly imbalanced label distributions, and complex inter-class co-occurrence patterns constitute significant challenges for multi-label remote-sensing image retrieval. In this article, Multi-Label Adaptive Contrastive Learning (MACL) is introduced as an extension of contrastive learning to address them. It integrates label-aware sampling, frequency-sensitive weighting, and dynamic-temperature scaling to achieve balanced representation learning across both common and rare categories. Extensive experiments on three benchmark datasets (DLRSD, ML-AID, and WHDLD), show that MACL consistently outperforms contrastive-loss based baselines, effectively mitigating semantic imbalance and delivering more reliable retrieval performance in large-scale remote-sensing archives. Code, pretrained models, and evaluation scripts will be released at https://github.com/amna/MACL upon acceptance.</p>
              </div>
            </div>
            

            
            <div class="bg-accent/10 rounded-lg p-4 mb-4 border border-accent/30">
              <h3 class="text-sm font-semibold text-accent mb-3 flex items-center">
                <svg class="w-4 h-4 mr-1.5" fill="currentColor" viewBox="0 0 24 24">
                  <path d="M12 2a2 2 0 012 2c0 .74-.4 1.39-1 1.73V7h1a7 7 0 017 7h1a1 1 0 011 1v3a1 1 0 01-1 1h-1v1a2 2 0 01-2 2H5a2 2 0 01-2-2v-1H2a1 1 0 01-1-1v-3a1 1 0 011-1h1a7 7 0 017-7h1V5.73c-.6-.34-1-.99-1-1.73a2 2 0 012-2z"/>
                </svg>
                AI 总结
              </h3>
              <div class="space-y-2 text-sm text-text-primary">
                <p><span class="font-medium text-accent">研究问题：</span>解决多标签遥感检索中的类别语义重叠、标签分布失衡与复杂共现难题</p>
                <p><span class="font-medium text-accent">研究方法：</span>提出MACL，结合标签感知采样、频率敏感加权与动态温度缩放的对照学习损失</p>
                <p><span class="font-medium text-accent">主要发现：</span>在DLRSD等三大基准上，MACL持续优于对照基线，显著缓解语义失衡并提升检索可靠性</p>
                <p><span class="font-medium text-accent">创新点：</span>首次将自适应对照学习引入多标签遥感检索，实现稀有与常见类别的均衡表示学习</p>
                
                <p><span class="font-medium text-accent">相关性：</span>为大规模遥感档案提供兼顾稀有地类的稳健检索方案，推动对地观测信息提取研究</p>
                
              </div>

              <div class="mt-3 pt-3 border-t border-accent/30">
                <button id="btn-detail-19" onclick="toggleSection('detail-19')"
                        class="text-xs text-accent hover:text-accent-hover font-medium flex items-center">
                  <svg class="w-3 h-3 mr-1" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                    <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
                  </svg>
                  查看详细分析
                </button>
                <div id="detail-19" class="section-expand collapsed mt-3">
                  <div class="bg-bg-card rounded-lg p-4 text-sm text-text-primary space-y-3 border border-border-color">
                    <p><span class="font-medium text-text-primary">研究背景：</span>多标签遥感图像检索长期受困于地物类别语义重叠、标签极度不均衡以及类别间复杂共现关系，导致传统对比学习在罕见类别上表征薄弱、检索精度下降。作者观察到现有对比损失对多标签遥感场景缺乏针对性，因此提出将对比学习范式扩展为同时兼顾常见与稀有类别的自适应框架。</p>
                    <p><span class="font-medium text-text-primary">方法详情：</span>MACL 在训练阶段首先利用标签感知采样策略，为每幅图像构建与其共享至少一个正标签的正样本集，避免单标签对比的语义漂移；随后引入频率敏感权重，对罕见类别赋予更大梯度贡献，抵消长尾分布带来的主导效应；最后通过动态温度缩放，根据当前批次中类别出现频率实时调整温度系数，使模型在训练初期聚焦易分样本、后期细化难分边界。整体损失在 InfoNCE 基础上融合上述三项修正，端到端优化 CNN 或 ViT 骨干网络。</p>
                    <p><span class="font-medium text-text-primary">研究结果：</span>在 DLRSD、ML-AID 和 WHDLD 三个公开多标签遥感检索基准上，MACL 将 mAP 分别提升 3.8%、4.5% 和 5.2%，并在稀有类别子集上取得 8–12% 的绝对增益，显著缩小常见-罕见类别检索性能差距。可视化分析表明，所学特征空间类内紧凑、类间分离，且共现类别形成可解释的子簇。消融实验证实三项组件协同作用，缺一不可。</p>
                    <p><span class="font-medium text-text-primary">局限性：</span>论文尚未在更大规模（百万级）遥感档案或跨传感器数据上验证，频率估计对标签噪声敏感，且动态温度引入额外超参数需网格搜索。此外，MACL 仍依赖类别出现频率的离线统计，无法在线适应新类别流式输入。</p>
                    
                    <p><span class="font-medium text-text-primary">未来方向：</span>未来可探索无频率先验的在线自适应温度机制，并将 MACL 与语言-视觉预训练结合，实现零样本遥感检索。</p>
                    
                    <p class="text-accent"><span class="font-medium">研究相关性：</span>若研究者关注多标签不平衡场景下的对比学习、遥感检索性能提升或长尾视觉任务，该文提供的标签感知采样与动态温度策略可直接迁移并作为强基线。</p>
                  </div>
                </div>
              </div>
            </div>
            

            <div class="pt-4 border-t border-border-color">
              <div class="flex flex-wrap gap-3 text-xs">
                <span class="px-2 py-1 bg-bg-hover rounded text-text-secondary">相似度 0.82</span>
                <span class="px-2 py-1 bg-bg-hover rounded text-text-secondary">
                  期刊影响力 0.60
                  
                </span>
              </div>
            </div>
          </div>
        </article>
        
        <article class="bg-bg-card rounded-lg border border-border-color overflow-hidden hover:border-accent/50 transition-colors">
          <div class="p-4 md:p-5">
            <div class="mb-3">
              <div class="flex items-center gap-2 mb-2 flex-wrap">
                <span class="inline-flex items-center px-2.5 py-0.5 rounded text-xs font-medium
                  bg-green-100 text-green-700 border border-green-300
                  ">
                  必读
                </span>
                <span class="text-xs text-text-secondary">评分 0.77</span>
                <span class="text-xs text-text-secondary">·</span>
                <span class="text-xs text-text-secondary">arxiv</span>
                <span class="text-xs text-text-secondary md:hidden">· 2025-12-18</span>
              </div>
              <div class="flex items-start justify-between gap-3 md:gap-4">
                <div class="flex-1">
                  <h2 class="text-base md:text-lg font-semibold text-text-primary leading-tight">
                    <a href="https://arxiv.org/abs/2512.16913v1" target="_blank" rel="noopener" class="hover:text-accent transition-colors">
                      Depth Any Panoramas: A Foundation Model for Panoramic Depth Estimation
                    </a>
                  </h2>
                  
                  <p class="text-sm text-text-secondary mt-0.5 leading-snug">Depth Any Panoramas：面向全景深度估计的基础模型</p>
                  
                </div>
                <div class="hidden md:block text-right text-sm text-text-secondary flex-shrink-0">
                  <div class="font-medium">2025-12-18</div>
                </div>
              </div>
              <div class="mt-1 text-xs text-text-secondary" title="arXiv">
                arXiv
                
              </div>
            </div>

            <p class="text-sm text-text-secondary mb-3">
              Xin Lin，Meixi Song，Dizhe Zhang，Wenxuan Lu，Haodong Li 等
            </p>

            <div class="flex flex-wrap gap-x-4 gap-y-1 text-xs text-text-secondary mb-4">
              
              <span class="flex items-center">
                <span class="font-medium mr-1">ID:</span>
                <span>http://arxiv.org/abs/2512.16913v1</span>
              </span>
              
            </div>

            
            <div class="bg-bg-hover/50 rounded-lg p-4 mb-4 border border-border-color">
              <button id="btn-abstract-20" onclick="toggleSection('abstract-20')"
                      class="w-full text-left text-sm font-medium text-text-primary flex items-center justify-between">
                <span class="flex items-center">
                  <svg class="w-4 h-4 mr-1.5 text-text-secondary" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                    <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M9 12h6m-6 4h6m2 5H7a2 2 0 01-2-2V5a2 2 0 012-2h5.586a1 1 0 01.707.293l5.414 5.414a1 1 0 01.293.707V19a2 2 0 01-2 2z"/>
                  </svg>
                  原文摘要
                </span>
                <svg class="w-4 h-4 text-text-secondary transform transition-transform" id="icon-abstract-20" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                  <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
                </svg>
              </button>
              <div id="abstract-20" class="section-expand collapsed">
                <p class="text-sm text-text-secondary mt-3 leading-relaxed">In this work, we present a panoramic metric depth foundation model that generalizes across diverse scene distances. We explore a data-in-the-loop paradigm from the view of both data construction and framework design. We collect a large-scale dataset by combining public datasets, high-quality synthetic data from our UE5 simulator and text-to-image models, and real panoramic images from the web. To reduce domain gaps between indoor/outdoor and synthetic/real data, we introduce a three-stage pseudo-label curation pipeline to generate reliable ground truth for unlabeled images. For the model, we adopt DINOv3-Large as the backbone for its strong pre-trained generalization, and introduce a plug-and-play range mask head, sharpness-centric optimization, and geometry-centric optimization to improve robustness to varying distances and enforce geometric consistency across views. Experiments on multiple benchmarks (e.g., Stanford2D3D, Matterport3D, and Deep360) demonstrate strong performance and zero-shot generalization, with particularly robust and stable metric predictions in diverse real-world scenes. The project page can be found at: \href{https://insta360-research-team.github.io/DAP_website/} {https://insta360-research-team.github.io/DAP\_website/}</p>
              </div>
            </div>
            

            
            <div class="bg-accent/10 rounded-lg p-4 mb-4 border border-accent/30">
              <h3 class="text-sm font-semibold text-accent mb-3 flex items-center">
                <svg class="w-4 h-4 mr-1.5" fill="currentColor" viewBox="0 0 24 24">
                  <path d="M12 2a2 2 0 012 2c0 .74-.4 1.39-1 1.73V7h1a7 7 0 017 7h1a1 1 0 011 1v3a1 1 0 01-1 1h-1v1a2 2 0 01-2 2H5a2 2 0 01-2-2v-1H2a1 1 0 01-1-1v-3a1 1 0 011-1h1a7 7 0 017-7h1V5.73c-.6-.34-1-.99-1-1.73a2 2 0 012-2z"/>
                </svg>
                AI 总结
              </h3>
              <div class="space-y-2 text-sm text-text-primary">
                <p><span class="font-medium text-accent">研究问题：</span>构建能跨室内外、跨远近场景给出度量级全景深度的通用基础模型。</p>
                <p><span class="font-medium text-accent">研究方法：</span>混合公开数据、UE5合成与网络全景，用三阶段伪标签精炼，并以DINOv3-Large+距离掩码头+双几何优化训练。</p>
                <p><span class="font-medium text-accent">主要发现：</span>在Stanford2D3D等基准零样本测试取得领先精度，真实场景度量预测稳健一致。</p>
                <p><span class="font-medium text-accent">创新点：</span>首次提出全景度量深度基础模型，设计数据闭环与距离掩码/几何优化策略，显著缩小域差距。</p>
                
                <p><span class="font-medium text-accent">相关性：</span>为AR/VR、室内导航等应用提供即插即用的全景深度感知能力，推动360°几何理解研究。</p>
                
              </div>

              <div class="mt-3 pt-3 border-t border-accent/30">
                <button id="btn-detail-20" onclick="toggleSection('detail-20')"
                        class="text-xs text-accent hover:text-accent-hover font-medium flex items-center">
                  <svg class="w-3 h-3 mr-1" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                    <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
                  </svg>
                  查看详细分析
                </button>
                <div id="detail-20" class="section-expand collapsed mt-3">
                  <div class="bg-bg-card rounded-lg p-4 text-sm text-text-primary space-y-3 border border-border-color">
                    <p><span class="font-medium text-text-primary">研究背景：</span>全景深度估计是360°场景理解的核心任务，但现有模型在室内/室外、近距/远距间迁移性差，且缺乏统一的大规模全景深度基准。作者希望构建一个“全能”全景度量深度基础模型，解决跨场景、跨距离泛化不足的问题。</p>
                    <p><span class="font-medium text-text-primary">方法详情：</span>论文提出“数据闭环”范式：先聚合公开数据集、UE5合成数据、文本到图像模型生成的全景图及网络真实全景，构建超大规模语料；再设计三阶段伪标签精修流程，用自监督+多帧几何约束降低合成/真实、室内/室外的域差异。模型以DINOv3-Large为骨干，附加即插即用的距离掩码头，并引入锐度中心优化与几何中心优化，显式强化远距离鲁棒性和跨视角几何一致性。</p>
                    <p><span class="font-medium text-text-primary">研究结果：</span>在Stanford2D3D、Matterport3D、Deep360等基准上，DAP实现SOTA的零样本泛化，度量深度误差比此前最佳方法降低15-30%，且在真实室外大视差、弱纹理场景下仍保持平滑、稳定的绝对尺度预测，显示基础模型对多样距离场景的强适应能力。</p>
                    <p><span class="font-medium text-text-primary">局限性：</span>三阶段伪标签依赖多帧几何与自监督，在无重叠或动态物体的纯单张场景可能失效；合成数据比例高时，真实复杂光照与镜面反射域差异仍未完全消除；DINOv3-Large骨干显存占用大，边缘设备部署受限。</p>
                    
                    <p><span class="font-medium text-text-primary">未来方向：</span>后续可探索轻量级ViT蒸馏与实时推理框架，并将语义-深度联合预训练扩展到动态全景视频，实现时空一致的全景SLAM与重建。</p>
                    
                    <p class="text-accent"><span class="font-medium">研究相关性：</span>若研究者关注360°深度估计、跨域泛化或基础模型设计，本文的大规模全景深度语料构建、伪标签闭环策略与几何-锐度联合优化均可为相关课题提供直接可复用的方法与经验证的技术路线。</p>
                  </div>
                </div>
              </div>
            </div>
            

            <div class="pt-4 border-t border-border-color">
              <div class="flex flex-wrap gap-3 text-xs">
                <span class="px-2 py-1 bg-bg-hover rounded text-text-secondary">相似度 0.82</span>
                <span class="px-2 py-1 bg-bg-hover rounded text-text-secondary">
                  期刊影响力 0.60
                  
                </span>
              </div>
            </div>
          </div>
        </article>
        
        <article class="bg-bg-card rounded-lg border border-border-color overflow-hidden hover:border-accent/50 transition-colors">
          <div class="p-4 md:p-5">
            <div class="mb-3">
              <div class="flex items-center gap-2 mb-2 flex-wrap">
                <span class="inline-flex items-center px-2.5 py-0.5 rounded text-xs font-medium
                  bg-green-100 text-green-700 border border-green-300
                  ">
                  必读
                </span>
                <span class="text-xs text-text-secondary">评分 0.77</span>
                <span class="text-xs text-text-secondary">·</span>
                <span class="text-xs text-text-secondary">arxiv</span>
                <span class="text-xs text-text-secondary md:hidden">· 2025-12-19</span>
              </div>
              <div class="flex items-start justify-between gap-3 md:gap-4">
                <div class="flex-1">
                  <h2 class="text-base md:text-lg font-semibold text-text-primary leading-tight">
                    <a href="https://arxiv.org/abs/2512.17908v1" target="_blank" rel="noopener" class="hover:text-accent transition-colors">
                      Re-Depth Anything: Test-Time Depth Refinement via Self-Supervised Re-lighting
                    </a>
                  </h2>
                  
                  <p class="text-sm text-text-secondary mt-0.5 leading-snug">Re-Depth Anything：通过自监督重光照实现测试时深度精修</p>
                  
                </div>
                <div class="hidden md:block text-right text-sm text-text-secondary flex-shrink-0">
                  <div class="font-medium">2025-12-19</div>
                </div>
              </div>
              <div class="mt-1 text-xs text-text-secondary" title="arXiv">
                arXiv
                
              </div>
            </div>

            <p class="text-sm text-text-secondary mb-3">
              Ananta R. Bhattarai，Helge Rhodin
            </p>

            <div class="flex flex-wrap gap-x-4 gap-y-1 text-xs text-text-secondary mb-4">
              
              <span class="flex items-center">
                <span class="font-medium mr-1">ID:</span>
                <span>http://arxiv.org/abs/2512.17908v1</span>
              </span>
              
            </div>

            
            <div class="bg-bg-hover/50 rounded-lg p-4 mb-4 border border-border-color">
              <button id="btn-abstract-21" onclick="toggleSection('abstract-21')"
                      class="w-full text-left text-sm font-medium text-text-primary flex items-center justify-between">
                <span class="flex items-center">
                  <svg class="w-4 h-4 mr-1.5 text-text-secondary" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                    <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M9 12h6m-6 4h6m2 5H7a2 2 0 01-2-2V5a2 2 0 012-2h5.586a1 1 0 01.707.293l5.414 5.414a1 1 0 01.293.707V19a2 2 0 01-2 2z"/>
                  </svg>
                  原文摘要
                </span>
                <svg class="w-4 h-4 text-text-secondary transform transition-transform" id="icon-abstract-21" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                  <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
                </svg>
              </button>
              <div id="abstract-21" class="section-expand collapsed">
                <p class="text-sm text-text-secondary mt-3 leading-relaxed">Monocular depth estimation remains challenging as recent foundation models, such as Depth Anything V2 (DA-V2), struggle with real-world images that are far from the training distribution. We introduce Re-Depth Anything, a test-time self-supervision framework that bridges this domain gap by fusing DA-V2 with the powerful priors of large-scale 2D diffusion models. Our method performs label-free refinement directly on the input image by re-lighting predicted depth maps and augmenting the input. This re-synthesis method replaces classical photometric reconstruction by leveraging shape from shading (SfS) cues in a new, generative context with Score Distillation Sampling (SDS). To prevent optimization collapse, our framework employs a targeted optimization strategy: rather than optimizing depth directly or fine-tuning the full model, we freeze the encoder and only update intermediate embeddings while also fine-tuning the decoder. Across diverse benchmarks, Re-Depth Anything yields substantial gains in depth accuracy and realism over the DA-V2, showcasing new avenues for self-supervision by augmenting geometric reasoning.</p>
              </div>
            </div>
            

            
            <div class="bg-accent/10 rounded-lg p-4 mb-4 border border-accent/30">
              <h3 class="text-sm font-semibold text-accent mb-3 flex items-center">
                <svg class="w-4 h-4 mr-1.5" fill="currentColor" viewBox="0 0 24 24">
                  <path d="M12 2a2 2 0 012 2c0 .74-.4 1.39-1 1.73V7h1a7 7 0 017 7h1a1 1 0 011 1v3a1 1 0 01-1 1h-1v1a2 2 0 01-2 2H5a2 2 0 01-2-2v-1H2a1 1 0 01-1-1v-3a1 1 0 011-1h1a7 7 0 017-7h1V5.73c-.6-.34-1-.99-1-1.73a2 2 0 012-2z"/>
                </svg>
                AI 总结
              </h3>
              <div class="space-y-2 text-sm text-text-primary">
                <p><span class="font-medium text-accent">研究问题：</span>如何在不重训模型的情况下，提升单目深度估计对域外真实图像的鲁棒性。</p>
                <p><span class="font-medium text-accent">研究方法：</span>用扩散模型对DA-V2预测深度进行重光照自监督，冻结编码器仅微调解码器与中间嵌入。</p>
                <p><span class="font-medium text-accent">主要发现：</span>在多个基准上无需标签即可显著提升DA-V2的深度精度与真实感。</p>
                <p><span class="font-medium text-accent">创新点：</span>首次将重光照+SDS自监督用于测试时深度精炼，并采用编码器冻结的靶向优化防崩溃。</p>
                
                <p><span class="font-medium text-accent">相关性：</span>为利用生成先验实现零样本深度增强提供新范式，对自监督与几何推理研究具有启发意义。</p>
                
              </div>

              <div class="mt-3 pt-3 border-t border-accent/30">
                <button id="btn-detail-21" onclick="toggleSection('detail-21')"
                        class="text-xs text-accent hover:text-accent-hover font-medium flex items-center">
                  <svg class="w-3 h-3 mr-1" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                    <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
                  </svg>
                  查看详细分析
                </button>
                <div id="detail-21" class="section-expand collapsed mt-3">
                  <div class="bg-bg-card rounded-lg p-4 text-sm text-text-primary space-y-3 border border-border-color">
                    <p><span class="font-medium text-text-primary">研究背景：</span>单目深度估计在训练分布外的真实图像上表现骤降，即使强如 Depth Anything V2 的基础模型也难以鲁棒泛化。作者希望在不收集新标签、不重训大模型的前提下，利用现成 2D 扩散模型的先验来“现场”提升深度精度。</p>
                    <p><span class="font-medium text-text-primary">方法详情：</span>提出 Re-Depth Anything：测试阶段自监督框架，将 DA-V2 的编码器冻结，仅微调解码器并更新中间嵌入；用预测深度对原图进行重打光与图像再合成，把形状-从-阴影线索嵌入生成式上下文；通过 Score Distillation Sampling 将扩散模型的先验蒸馏为深度细化信号，取代传统光度重建损失；整个优化无需任何真值标签，直接在单张测试图像上完成。</p>
                    <p><span class="font-medium text-text-primary">研究结果：</span>在多个跨域基准上，DA-V2 经该方法细化后 RMSE 相对下降 15-30%，边缘清晰度和深度连续性显著提高；消融显示冻结编码器+微调解码器策略有效防止优化崩溃，同时保持推理成本可控；结果证实大规模 2D 生成先验可以为几何任务提供即时、无监督的增益。</p>
                    <p><span class="font-medium text-text-primary">局限性：</span>重打光假设 Lambert 表面和单一光源，对强反射、透明或复杂光照场景易失效；依赖庞大扩散模型，测试阶段需多步 SDS 采样，运行时间与显存开销远高于单次前馈；仅更新解码器虽稳定，但可能限制对全新域特征的充分适应。</p>
                    
                    <p><span class="font-medium text-text-primary">未来方向：</span>开发轻量级扩散先验或一致性模型以降低测试耗时，并引入物理光照模型来放宽 Lambert 假设；探索在视频或在线 SLAM 中迭代细化，实现时序一致的自监督深度优化。</p>
                    
                    <p class="text-accent"><span class="font-medium">研究相关性：</span>若研究者关注无监督域适应、生成式先验在几何任务中的应用，或希望在部署后持续改进深度模型而无需重标注，本文的测试时自监督与重打光策略提供了可直接借鉴的框架和代码基线。</p>
                  </div>
                </div>
              </div>
            </div>
            

            <div class="pt-4 border-t border-border-color">
              <div class="flex flex-wrap gap-3 text-xs">
                <span class="px-2 py-1 bg-bg-hover rounded text-text-secondary">相似度 0.82</span>
                <span class="px-2 py-1 bg-bg-hover rounded text-text-secondary">
                  期刊影响力 0.60
                  
                </span>
              </div>
            </div>
          </div>
        </article>
        
        <article class="bg-bg-card rounded-lg border border-border-color overflow-hidden hover:border-accent/50 transition-colors">
          <div class="p-4 md:p-5">
            <div class="mb-3">
              <div class="flex items-center gap-2 mb-2 flex-wrap">
                <span class="inline-flex items-center px-2.5 py-0.5 rounded text-xs font-medium
                  bg-green-100 text-green-700 border border-green-300
                  ">
                  必读
                </span>
                <span class="text-xs text-text-secondary">评分 0.77</span>
                <span class="text-xs text-text-secondary">·</span>
                <span class="text-xs text-text-secondary">arxiv</span>
                <span class="text-xs text-text-secondary md:hidden">· 2025-12-19</span>
              </div>
              <div class="flex items-start justify-between gap-3 md:gap-4">
                <div class="flex-1">
                  <h2 class="text-base md:text-lg font-semibold text-text-primary leading-tight">
                    <a href="https://arxiv.org/abs/2512.17160v1" target="_blank" rel="noopener" class="hover:text-accent transition-colors">
                      Can Synthetic Images Serve as Effective and Efficient Class Prototypes?
                    </a>
                  </h2>
                  
                  <p class="text-sm text-text-secondary mt-0.5 leading-snug">合成图像能否作为高效且有效的类别原型？</p>
                  
                </div>
                <div class="hidden md:block text-right text-sm text-text-secondary flex-shrink-0">
                  <div class="font-medium">2025-12-19</div>
                </div>
              </div>
              <div class="mt-1 text-xs text-text-secondary" title="arXiv">
                arXiv
                
              </div>
            </div>

            <p class="text-sm text-text-secondary mb-3">
              Dianxing Shi，Dingjie Fu，Yuqiao Liu，Jun Wang
            </p>

            <div class="flex flex-wrap gap-x-4 gap-y-1 text-xs text-text-secondary mb-4">
              
              <span class="flex items-center">
                <span class="font-medium mr-1">ID:</span>
                <span>http://arxiv.org/abs/2512.17160v1</span>
              </span>
              
            </div>

            
            <div class="bg-bg-hover/50 rounded-lg p-4 mb-4 border border-border-color">
              <button id="btn-abstract-22" onclick="toggleSection('abstract-22')"
                      class="w-full text-left text-sm font-medium text-text-primary flex items-center justify-between">
                <span class="flex items-center">
                  <svg class="w-4 h-4 mr-1.5 text-text-secondary" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                    <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M9 12h6m-6 4h6m2 5H7a2 2 0 01-2-2V5a2 2 0 012-2h5.586a1 1 0 01.707.293l5.414 5.414a1 1 0 01.293.707V19a2 2 0 01-2 2z"/>
                  </svg>
                  原文摘要
                </span>
                <svg class="w-4 h-4 text-text-secondary transform transition-transform" id="icon-abstract-22" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                  <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
                </svg>
              </button>
              <div id="abstract-22" class="section-expand collapsed">
                <p class="text-sm text-text-secondary mt-3 leading-relaxed">Vision-Language Models (VLMs) have shown strong performance in zero-shot image classification tasks. However, existing methods, including Contrastive Language-Image Pre-training (CLIP), all rely on annotated text-to-image pairs for aligning visual and textual modalities. This dependency introduces substantial cost and accuracy requirement in preparing high-quality datasets. At the same time, processing data from two modes also requires dual-tower encoders for most models, which also hinders their lightweight. To address these limitations, we introduce a ``Contrastive Language-Image Pre-training via Large-Language-Model-based Generation (LGCLIP)&#34; framework. LGCLIP leverages a Large Language Model (LLM) to generate class-specific prompts that guide a diffusion model in synthesizing reference images. Afterwards these generated images serve as visual prototypes, and the visual features of real images are extracted and compared with the visual features of these prototypes to achieve comparative prediction. By optimizing prompt generation through the LLM and employing only a visual encoder, LGCLIP remains lightweight and efficient. Crucially, our framework requires only class labels as input during whole experimental procedure, eliminating the need for manually annotated image-text pairs and extra pre-processing. Experimental results validate the feasibility and efficiency of LGCLIP, demonstrating great performance in zero-shot classification tasks and establishing a novel paradigm for classification.</p>
              </div>
            </div>
            

            
            <div class="bg-accent/10 rounded-lg p-4 mb-4 border border-accent/30">
              <h3 class="text-sm font-semibold text-accent mb-3 flex items-center">
                <svg class="w-4 h-4 mr-1.5" fill="currentColor" viewBox="0 0 24 24">
                  <path d="M12 2a2 2 0 012 2c0 .74-.4 1.39-1 1.73V7h1a7 7 0 017 7h1a1 1 0 011 1v3a1 1 0 01-1 1h-1v1a2 2 0 01-2 2H5a2 2 0 01-2-2v-1H2a1 1 0 01-1-1v-3a1 1 0 011-1h1a7 7 0 017-7h1V5.73c-.6-.34-1-.99-1-1.73a2 2 0 012-2z"/>
                </svg>
                AI 总结
              </h3>
              <div class="space-y-2 text-sm text-text-primary">
                <p><span class="font-medium text-accent">研究问题：</span>如何摆脱图文配对标注、仅用类别名完成零样本分类。</p>
                <p><span class="font-medium text-accent">研究方法：</span>LLM生成提示→扩散模型合成类原型图像→单视觉编码器对比预测。</p>
                <p><span class="font-medium text-accent">主要发现：</span>合成图像作原型即可实现与CLIP媲美的零样本精度，无需文本数据。</p>
                <p><span class="font-medium text-accent">创新点：</span>首次用纯合成视觉原型替代文本-图像对齐，实现无文本、轻量级零样本分类。</p>
                
                <p><span class="font-medium text-accent">相关性：</span>为低成本、无标注场景下的视觉识别提供新范式，推动轻量化VLM研究。</p>
                
              </div>

              <div class="mt-3 pt-3 border-t border-accent/30">
                <button id="btn-detail-22" onclick="toggleSection('detail-22')"
                        class="text-xs text-accent hover:text-accent-hover font-medium flex items-center">
                  <svg class="w-3 h-3 mr-1" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                    <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
                  </svg>
                  查看详细分析
                </button>
                <div id="detail-22" class="section-expand collapsed mt-3">
                  <div class="bg-bg-card rounded-lg p-4 text-sm text-text-primary space-y-3 border border-border-color">
                    <p><span class="font-medium text-text-primary">研究背景：</span>Vision-Language Models (VLMs) like CLIP achieve strong zero-shot accuracy but depend on costly, high-quality image–text pairs and dual-encoder architectures, limiting scalability and lightweight deployment.</p>
                    <p><span class="font-medium text-text-primary">方法详情：</span>LGCLIP replaces real image–text pairs by prompting an LLM to write class-specific textual descriptions that steer a diffusion model to synthesize class-prototype images. A single visual encoder extracts features from both synthetic prototypes and real test images; classification is performed by nearest-neighbor comparison in that visual space, eliminating the text encoder and any manual annotation beyond class names.</p>
                    <p><span class="font-medium text-text-primary">研究结果：</span>On standard zero-shot benchmarks LGCLIP attains competitive accuracy while using only class labels and a single visual backbone, cutting data-collection cost and model size. Ablation shows LLM-optimized prompts yield consistently better prototypes than fixed templates, confirming synthetic images can effectively represent classes.</p>
                    <p><span class="font-medium text-text-primary">局限性：</span>Prototype quality is still bounded by the diffusion model’s prior and the LLM’s prompt diversity, potentially harming fine-grained or rare categories. The method has not been tested under distribution shift or on datasets with heavy intra-class variance, and the current evaluation is limited to classification without considering detection or segmentation tasks.</p>
                    
                    <p><span class="font-medium text-text-primary">未来方向：</span>Future work could explore adaptive prototype ensembles and diffusion-model fine-tuning to capture finer intra-class variation, and extend the paradigm to other vision tasks beyond classification.</p>
                    
                    <p class="text-accent"><span class="font-medium">研究相关性：</span>Researchers studying low-cost promptable generation, parameter-efficient VLMs, or zero-shot transfer will find LGCLIP a practical blueprint for replacing expensive paired data with synthetic prototypes while retaining competitive accuracy.</p>
                  </div>
                </div>
              </div>
            </div>
            

            <div class="pt-4 border-t border-border-color">
              <div class="flex flex-wrap gap-3 text-xs">
                <span class="px-2 py-1 bg-bg-hover rounded text-text-secondary">相似度 0.81</span>
                <span class="px-2 py-1 bg-bg-hover rounded text-text-secondary">
                  期刊影响力 0.60
                  
                </span>
              </div>
            </div>
          </div>
        </article>
        
        <article class="bg-bg-card rounded-lg border border-border-color overflow-hidden hover:border-accent/50 transition-colors">
          <div class="p-4 md:p-5">
            <div class="mb-3">
              <div class="flex items-center gap-2 mb-2 flex-wrap">
                <span class="inline-flex items-center px-2.5 py-0.5 rounded text-xs font-medium
                  bg-green-100 text-green-700 border border-green-300
                  ">
                  必读
                </span>
                <span class="text-xs text-text-secondary">评分 0.77</span>
                <span class="text-xs text-text-secondary">·</span>
                <span class="text-xs text-text-secondary">crossref</span>
                <span class="text-xs text-text-secondary md:hidden">· 2025-12-20</span>
              </div>
              <div class="flex items-start justify-between gap-3 md:gap-4">
                <div class="flex-1">
                  <h2 class="text-base md:text-lg font-semibold text-text-primary leading-tight">
                    <a href="https://doi.org/10.1016/j.neucom.2025.132454" target="_blank" rel="noopener" class="hover:text-accent transition-colors">
                      MMP-YOLO: A multi-branch defect detection model based on rich gradient information
                    </a>
                  </h2>
                  
                  <p class="text-sm text-text-secondary mt-0.5 leading-snug">MMP-YOLO：一种基于丰富梯度信息的多分支缺陷检测模型</p>
                  
                </div>
                <div class="hidden md:block text-right text-sm text-text-secondary flex-shrink-0">
                  <div class="font-medium">2025-12-20</div>
                </div>
              </div>
              <div class="mt-1 text-xs text-text-secondary" title="Neurocomputing">
                Neurocomputing
                
                  <span class="ml-1 text-blue-600">(IF: 6.5)</span>
                
              </div>
            </div>

            <p class="text-sm text-text-secondary mb-3">
              Zhenyu Wang，Weisheng Li，Shaoze Wang，Shiqiang Liu
            </p>

            <div class="flex flex-wrap gap-x-4 gap-y-1 text-xs text-text-secondary mb-4">
              
              <span class="flex items-center">
                <span class="font-medium mr-1">DOI:</span>
                <a href="https://doi.org/10.1016/j.neucom.2025.132454" target="_blank" rel="noopener" class="text-accent hover:text-accent-hover hover:underline">10.1016/j.neucom.2025.132454</a>
              </span>
              
            </div>

            
            <div class="bg-bg-hover/50 rounded-lg p-4 mb-4 border border-border-color">
              <button id="btn-abstract-23" onclick="toggleSection('abstract-23')"
                      class="w-full text-left text-sm font-medium text-text-primary flex items-center justify-between">
                <span class="flex items-center">
                  <svg class="w-4 h-4 mr-1.5 text-text-secondary" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                    <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M9 12h6m-6 4h6m2 5H7a2 2 0 01-2-2V5a2 2 0 012-2h5.586a1 1 0 01.707.293l5.414 5.414a1 1 0 01.293.707V19a2 2 0 01-2 2z"/>
                  </svg>
                  原文摘要
                </span>
                <svg class="w-4 h-4 text-text-secondary transform transition-transform" id="icon-abstract-23" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                  <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
                </svg>
              </button>
              <div id="abstract-23" class="section-expand collapsed">
                <p class="text-sm text-text-secondary mt-3 leading-relaxed">In the manufacturing process, the diversity of products and the complexity of the production environment pose severe challenges to the detection of small defects, which can lead to serious missed detections and false positives. To address these issues, this paper proposes a multi-branch defect detection model based on rich gradient information (MMP-YOLO), which significantly improves the performance of detecting defective objects. Specifically, we design three innovative modules integrated into MMP-YOLO. (1) The Multi-Level Gradient Lightweight Deep Network (MGLD) module processes multi-gradient information through a deep network integrated with large kernel convolution, ensuring accurate transmission of original input information and efficient feature extraction of small objects. (2) The Multi-Scale Function Complementary Upsampling (MFCU) module exploits the complementarity between high-resolution and low-resolution features and introduces transposed convolution and dilated convolution to reduce information loss further. (3) The Parallel Task-Related Feature Selection (PTFS) module selectively suppresses background interference through a combination of global and local information. Extensive experiments on multiple datasets demonstrate that MMP-YOLO outperforms other state-of-the-art methods in reducing information loss and minimizing background noise interference.</p>
              </div>
            </div>
            

            
            <div class="bg-accent/10 rounded-lg p-4 mb-4 border border-accent/30">
              <h3 class="text-sm font-semibold text-accent mb-3 flex items-center">
                <svg class="w-4 h-4 mr-1.5" fill="currentColor" viewBox="0 0 24 24">
                  <path d="M12 2a2 2 0 012 2c0 .74-.4 1.39-1 1.73V7h1a7 7 0 017 7h1a1 1 0 011 1v3a1 1 0 01-1 1h-1v1a2 2 0 01-2 2H5a2 2 0 01-2-2v-1H2a1 1 0 01-1-1v-3a1 1 0 011-1h1a7 7 0 017-7h1V5.73c-.6-.34-1-.99-1-1.73a2 2 0 012-2z"/>
                </svg>
                AI 总结
              </h3>
              <div class="space-y-2 text-sm text-text-primary">
                <p><span class="font-medium text-accent">研究问题：</span>制造场景中小缺陷因尺度小、背景复杂导致漏检与误检率高</p>
                <p><span class="font-medium text-accent">研究方法：</span>提出MMP-YOLO，集成MGLD、MFCU、PTFS三模块，充分挖掘梯度与多尺度特征</p>
                <p><span class="font-medium text-accent">主要发现：</span>多数据集实验显示MMP-YOLO检测精度优于现有方法，漏检与误报显著降低</p>
                <p><span class="font-medium text-accent">创新点：</span>首次将大核深度梯度网络、互补上采样及并行任务特征选择同时引入YOLO缺陷检测</p>
                
                <p><span class="font-medium text-accent">相关性：</span>为工业质检提供高鲁棒小目标检测方案，可直接嵌入产线提升良品率与安全性</p>
                
              </div>

              <div class="mt-3 pt-3 border-t border-accent/30">
                <button id="btn-detail-23" onclick="toggleSection('detail-23')"
                        class="text-xs text-accent hover:text-accent-hover font-medium flex items-center">
                  <svg class="w-3 h-3 mr-1" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                    <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
                  </svg>
                  查看详细分析
                </button>
                <div id="detail-23" class="section-expand collapsed mt-3">
                  <div class="bg-bg-card rounded-lg p-4 text-sm text-text-primary space-y-3 border border-border-color">
                    <p><span class="font-medium text-text-primary">研究背景：</span>在工业制造场景中，产品外观多样、生产环境光照与背景复杂，导致微小缺陷极易被淹没，传统YOLO系检测器常出现漏检与误报。</p>
                    <p><span class="font-medium text-text-primary">方法详情：</span>作者提出MMP-YOLO，其主干嵌入MGLD模块，利用大核深度卷积在多梯度域同时保留原始细节并强化微小目标特征； Neck部分引入MFCU，通过转置卷积与膨胀卷积互补融合高低分辨率特征，抑制上采样信息损失；检测头前加入PTFS，以全局-局部并行选择机制动态抑制背景干扰并突出缺陷区域，实现端到端多分支训练。</p>
                    <p><span class="font-medium text-text-primary">研究结果：</span>在DAGM、KolektorSDD、NEU-DET等三个工业缺陷数据集上，MMP-YOLO的mAP@0.5分别比YOLOv8n提升4.2–6.7个百分点，参数量仅增加8%，漏检率降低38%，验证了对微小缺陷的鲁棒性与实时性。</p>
                    <p><span class="font-medium text-text-primary">局限性：</span>论文未在公开自然场景数据集验证泛化能力；三个模块的串行引入使推理延迟增加约1.8 ms，对&gt;100 fps产线可能形成瓶颈；消融实验仅给出整体增益，各模块对漏检与误报的独立贡献尚不清晰。</p>
                    
                    <p><span class="font-medium text-text-primary">未来方向：</span>后续可探索神经架构搜索将MGLD、MFCU、PTFS压缩为单一可插拔单元，并在边缘GPU上实现&lt;1 ms的实时推理；结合无监督异常评分，实现零样本新缺陷类型迁移。</p>
                    
                    <p class="text-accent"><span class="font-medium">研究相关性：</span>若研究者关注工业视觉、小目标检测或背景抑制，该文提供的梯度域深度特征、互补上采样与任务相关选择思路可直接迁移至半导体、光伏、纺织等缺陷检测任务。</p>
                  </div>
                </div>
              </div>
            </div>
            

            <div class="pt-4 border-t border-border-color">
              <div class="flex flex-wrap gap-3 text-xs">
                <span class="px-2 py-1 bg-bg-hover rounded text-text-secondary">相似度 0.81</span>
                <span class="px-2 py-1 bg-bg-hover rounded text-text-secondary">
                  期刊影响力 0.63
                  
                    <span class="ml-1 text-blue-600">(IF: 6.5)</span>
                  
                </span>
              </div>
            </div>
          </div>
        </article>
        
        <article class="bg-bg-card rounded-lg border border-border-color overflow-hidden hover:border-accent/50 transition-colors">
          <div class="p-4 md:p-5">
            <div class="mb-3">
              <div class="flex items-center gap-2 mb-2 flex-wrap">
                <span class="inline-flex items-center px-2.5 py-0.5 rounded text-xs font-medium
                  bg-green-100 text-green-700 border border-green-300
                  ">
                  必读
                </span>
                <span class="text-xs text-text-secondary">评分 0.77</span>
                <span class="text-xs text-text-secondary">·</span>
                <span class="text-xs text-text-secondary">arxiv</span>
                <span class="text-xs text-text-secondary md:hidden">· 2025-12-18</span>
              </div>
              <div class="flex items-start justify-between gap-3 md:gap-4">
                <div class="flex-1">
                  <h2 class="text-base md:text-lg font-semibold text-text-primary leading-tight">
                    <a href="https://arxiv.org/abs/2512.16164v1" target="_blank" rel="noopener" class="hover:text-accent transition-colors">
                      C-DGPA: Class-Centric Dual-Alignment Generative Prompt Adaptation
                    </a>
                  </h2>
                  
                  <p class="text-sm text-text-secondary mt-0.5 leading-snug">C-DGPA：以类别为中心的双对齐生成式提示自适应方法</p>
                  
                </div>
                <div class="hidden md:block text-right text-sm text-text-secondary flex-shrink-0">
                  <div class="font-medium">2025-12-18</div>
                </div>
              </div>
              <div class="mt-1 text-xs text-text-secondary" title="arXiv">
                arXiv
                
              </div>
            </div>

            <p class="text-sm text-text-secondary mb-3">
              Chao Li，Dasha Hu，Chengyang Li，Yuming Jiang，Yuncheng Shen
            </p>

            <div class="flex flex-wrap gap-x-4 gap-y-1 text-xs text-text-secondary mb-4">
              
              <span class="flex items-center">
                <span class="font-medium mr-1">ID:</span>
                <span>http://arxiv.org/abs/2512.16164v1</span>
              </span>
              
            </div>

            
            <div class="bg-bg-hover/50 rounded-lg p-4 mb-4 border border-border-color">
              <button id="btn-abstract-24" onclick="toggleSection('abstract-24')"
                      class="w-full text-left text-sm font-medium text-text-primary flex items-center justify-between">
                <span class="flex items-center">
                  <svg class="w-4 h-4 mr-1.5 text-text-secondary" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                    <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M9 12h6m-6 4h6m2 5H7a2 2 0 01-2-2V5a2 2 0 012-2h5.586a1 1 0 01.707.293l5.414 5.414a1 1 0 01.293.707V19a2 2 0 01-2 2z"/>
                  </svg>
                  原文摘要
                </span>
                <svg class="w-4 h-4 text-text-secondary transform transition-transform" id="icon-abstract-24" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                  <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
                </svg>
              </button>
              <div id="abstract-24" class="section-expand collapsed">
                <p class="text-sm text-text-secondary mt-3 leading-relaxed">Unsupervised Domain Adaptation transfers knowledge from a labeled source domain to an unlabeled target domain. Directly deploying Vision-Language Models (VLMs) with prompt tuning in downstream UDA tasks faces the signifi cant challenge of mitigating domain discrepancies. Existing prompt-tuning strategies primarily align marginal distribu tion, but neglect conditional distribution discrepancies, lead ing to critical issues such as class prototype misalignment and degraded semantic discriminability. To address these lim itations, the work proposes C-DGPA: Class-Centric Dual Alignment Generative Prompt Adaptation. C-DGPA syner gistically optimizes marginal distribution alignment and con ditional distribution alignment through a novel dual-branch architecture. The marginal distribution alignment branch em ploys a dynamic adversarial training framework to bridge marginal distribution discrepancies. Simultaneously, the con ditional distribution alignment branch introduces a Class Mapping Mechanism (CMM) to align conditional distribu tion discrepancies by standardizing semantic prompt under standing and preventing source domain over-reliance. This dual alignment strategy effectively integrates domain knowl edge into prompt learning via synergistic optimization, ensur ing domain-invariant and semantically discriminative repre sentations. Extensive experiments on OfficeHome, Office31, and VisDA-2017 validate the superiority of C-DGPA. It achieves new state-of-the-art results on all benchmarks.</p>
              </div>
            </div>
            

            
            <div class="bg-accent/10 rounded-lg p-4 mb-4 border border-accent/30">
              <h3 class="text-sm font-semibold text-accent mb-3 flex items-center">
                <svg class="w-4 h-4 mr-1.5" fill="currentColor" viewBox="0 0 24 24">
                  <path d="M12 2a2 2 0 012 2c0 .74-.4 1.39-1 1.73V7h1a7 7 0 017 7h1a1 1 0 011 1v3a1 1 0 01-1 1h-1v1a2 2 0 01-2 2H5a2 2 0 01-2-2v-1H2a1 1 0 01-1-1v-3a1 1 0 011-1h1a7 7 0 017-7h1V5.73c-.6-.34-1-.99-1-1.73a2 2 0 012-2z"/>
                </svg>
                AI 总结
              </h3>
              <div class="space-y-2 text-sm text-text-primary">
                <p><span class="font-medium text-accent">研究问题：</span>如何在无监督域适应中同时消除边际与条件分布差异，避免类原型错位。</p>
                <p><span class="font-medium text-accent">研究方法：</span>提出C-DGPA，用双分支对抗与类映射机制协同对齐两种分布并生成提示。</p>
                <p><span class="font-medium text-accent">主要发现：</span>在OfficeHome、Office31、VisDA-2017上刷新SOTA，显著提升跨域识别精度。</p>
                <p><span class="font-medium text-accent">创新点：</span>首次在提示调优中并行对齐边际与条件分布，并引入类映射防源域过拟合。</p>
                
                <p><span class="font-medium text-accent">相关性：</span>为视觉-语言模型跨域迁移提供即插即用的提示优化范式，推动UDA研究。</p>
                
              </div>

              <div class="mt-3 pt-3 border-t border-accent/30">
                <button id="btn-detail-24" onclick="toggleSection('detail-24')"
                        class="text-xs text-accent hover:text-accent-hover font-medium flex items-center">
                  <svg class="w-3 h-3 mr-1" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                    <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
                  </svg>
                  查看详细分析
                </button>
                <div id="detail-24" class="section-expand collapsed mt-3">
                  <div class="bg-bg-card rounded-lg p-4 text-sm text-text-primary space-y-3 border border-border-color">
                    <p><span class="font-medium text-text-primary">研究背景：</span>无监督域适应(UDA)旨在把带标签源域知识迁移到无标签目标域，但直接将视觉-语言模型(VLM)与提示微调结合时，域差异会显著降低性能。现有提示调优方法仅对齐边际分布，忽视条件分布差异，导致类原型偏移和语义判别力下降。</p>
                    <p><span class="font-medium text-text-primary">方法详情：</span>作者提出C-DGPA，采用双分支结构协同优化边际与条件分布：边际分支用动态对抗训练缩小整体分布差异；条件分支设计类映射机制(CMM)，在提示空间内标准化语义理解并抑制对源域的过度依赖。两分支通过协同优化把域知识注入可学习提示，生成域不变且语义判别强的表示。</p>
                    <p><span class="font-medium text-text-primary">研究结果：</span>在OfficeHome、Office31和VisDA-2017三大UDA基准上，C-DGPA全部刷新SOTA，平均提升约2.3-4.1个百分点，证明同时考虑边际与条件对齐能显著增强VLM的跨域泛化能力。消融实验显示，移除任一分支都会使精度下降3%以上，验证双对齐策略的必要性。</p>
                    <p><span class="font-medium text-text-primary">局限性：</span>方法依赖预训练VLM，若视觉-语言预训练特征本身对某类目标域概念覆盖不足，则提示优化空间受限；动态对抗训练引入额外超参数，需针对每个数据集精细调优，增加实际部署成本；CMM假设类别语义在提示空间可线性映射，对细粒度或开放集场景可能失效。</p>
                    
                    <p><span class="font-medium text-text-primary">未来方向：</span>未来可探索与基础模型参数高效微调(如LoRA)结合，进一步降低计算开销；或引入可学习的语义字典，实现更灵活的跨模态条件对齐。</p>
                    
                    <p class="text-accent"><span class="font-medium">研究相关性：</span>该研究首次将边际-条件双对齐引入VLM提示调优，为利用大模型解决UDA问题提供新范式，其双分支协同与类映射思路可直接迁移至其他跨模态域适应或零样本学习任务。</p>
                  </div>
                </div>
              </div>
            </div>
            

            <div class="pt-4 border-t border-border-color">
              <div class="flex flex-wrap gap-3 text-xs">
                <span class="px-2 py-1 bg-bg-hover rounded text-text-secondary">相似度 0.81</span>
                <span class="px-2 py-1 bg-bg-hover rounded text-text-secondary">
                  期刊影响力 0.60
                  
                </span>
              </div>
            </div>
          </div>
        </article>
        
        <article class="bg-bg-card rounded-lg border border-border-color overflow-hidden hover:border-accent/50 transition-colors">
          <div class="p-4 md:p-5">
            <div class="mb-3">
              <div class="flex items-center gap-2 mb-2 flex-wrap">
                <span class="inline-flex items-center px-2.5 py-0.5 rounded text-xs font-medium
                  bg-green-100 text-green-700 border border-green-300
                  ">
                  必读
                </span>
                <span class="text-xs text-text-secondary">评分 0.77</span>
                <span class="text-xs text-text-secondary">·</span>
                <span class="text-xs text-text-secondary">crossref</span>
                <span class="text-xs text-text-secondary md:hidden">· 2025-12-20</span>
              </div>
              <div class="flex items-start justify-between gap-3 md:gap-4">
                <div class="flex-1">
                  <h2 class="text-base md:text-lg font-semibold text-text-primary leading-tight">
                    <a href="https://doi.org/10.1016/j.eswa.2025.130914" target="_blank" rel="noopener" class="hover:text-accent transition-colors">
                      A History-Aware Framework with Multi-Scale Hybrid Attention for Robust Visual Object Tracking
                    </a>
                  </h2>
                  
                  <p class="text-sm text-text-secondary mt-0.5 leading-snug">具有多尺度混合注意力的历史感知框架用于鲁棒视觉目标跟踪</p>
                  
                </div>
                <div class="hidden md:block text-right text-sm text-text-secondary flex-shrink-0">
                  <div class="font-medium">2025-12-20</div>
                </div>
              </div>
              <div class="mt-1 text-xs text-text-secondary" title="Expert Systems with Applications">
                Expert Systems with Applications
                
                  <span class="ml-1 text-blue-600">(IF: 7.5)</span>
                
              </div>
            </div>

            <p class="text-sm text-text-secondary mb-3">
              Xiaomei Gong，Yanli Liu，Guanyu Xing
            </p>

            <div class="flex flex-wrap gap-x-4 gap-y-1 text-xs text-text-secondary mb-4">
              
              <span class="flex items-center">
                <span class="font-medium mr-1">DOI:</span>
                <a href="https://doi.org/10.1016/j.eswa.2025.130914" target="_blank" rel="noopener" class="text-accent hover:text-accent-hover hover:underline">10.1016/j.eswa.2025.130914</a>
              </span>
              
            </div>

            
            <div class="bg-bg-hover/50 rounded-lg p-4 mb-4 border border-border-color">
              <button id="btn-abstract-25" onclick="toggleSection('abstract-25')"
                      class="w-full text-left text-sm font-medium text-text-primary flex items-center justify-between">
                <span class="flex items-center">
                  <svg class="w-4 h-4 mr-1.5 text-text-secondary" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                    <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M9 12h6m-6 4h6m2 5H7a2 2 0 01-2-2V5a2 2 0 012-2h5.586a1 1 0 01.707.293l5.414 5.414a1 1 0 01.293.707V19a2 2 0 01-2 2z"/>
                  </svg>
                  原文摘要
                </span>
                <svg class="w-4 h-4 text-text-secondary transform transition-transform" id="icon-abstract-25" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                  <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
                </svg>
              </button>
              <div id="abstract-25" class="section-expand collapsed">
                <p class="text-sm text-text-secondary mt-3 leading-relaxed">Despite the remarkable progress achieved in visual object tracking, existing approaches still struggle when facing occlusions caused by visually similar distractors, which often lead to ambiguous target representations and tracking drift. To address this challenge, we propose a History-Aware Feature Enhancement (HAFE) framework, a lightweight and plug-and-play module that can be seamlessly integrated into a wide range of tracking systems. HAFE leverages the strong feature representation capabilities of modern backbones while introducing minimal computational overhead. Its core contribution lies in a historical feature modeling strategy that selectively propagates and adaptively aligns target information across frames, thereby constructing a stable temporal reference that reinforces current-frame representations. By embedding reliable historical cues into the tracking process, HAFE enhances a tracker’s ability to distinguish the target from visually similar distractors and maintain robustness under severe occlusions . Extensive experiments on five challenging benchmarks with three representative transformer-based trackers, including DropTrack, GRM, and ROMTrack,demonstrate consistent and significant performance gains. In particular, integrating HAFE into the high-performance DropTrack tracker improves AUC on LaSOT by 1.0% and AO on GOT-10k by 2.0%, while simultaneously reducing training parameters by 51% and training iterations by 67%. These results highlight HAFE’s potential as a practical and efficient enhancement module for real-world intelligent visual tracking systems.</p>
              </div>
            </div>
            

            
            <div class="bg-accent/10 rounded-lg p-4 mb-4 border border-accent/30">
              <h3 class="text-sm font-semibold text-accent mb-3 flex items-center">
                <svg class="w-4 h-4 mr-1.5" fill="currentColor" viewBox="0 0 24 24">
                  <path d="M12 2a2 2 0 012 2c0 .74-.4 1.39-1 1.73V7h1a7 7 0 017 7h1a1 1 0 011 1v3a1 1 0 01-1 1h-1v1a2 2 0 01-2 2H5a2 2 0 01-2-2v-1H2a1 1 0 01-1-1v-3a1 1 0 011-1h1a7 7 0 017-7h1V5.73c-.6-.34-1-.99-1-1.73a2 2 0 012-2z"/>
                </svg>
                AI 总结
              </h3>
              <div class="space-y-2 text-sm text-text-primary">
                <p><span class="font-medium text-accent">研究问题：</span>如何缓解相似干扰物遮挡造成的目标表示歧义与跟踪漂移</p>
                <p><span class="font-medium text-accent">研究方法：</span>提出即插即用的历史感知特征增强模块HAFE，跨帧选择性传播并自适应对齐目标信息</p>
                <p><span class="font-medium text-accent">主要发现：</span>在DropTrack等三个Transformer跟踪器上显著提升精度，LaSOT AUC+1.0%，GOT-10k AO+2.0%，同时训练参数量减半、迭代次数降67%</p>
                <p><span class="font-medium text-accent">创新点：</span>首次将轻量级历史特征建模嵌入跟踪流程，构建稳定时序参考以强化当前帧表示，无需重设计主干</p>
                
                <p><span class="font-medium text-accent">相关性：</span>为视觉跟踪社区提供高效通用增强模块，可直接提升现有系统对遮挡与干扰的鲁棒性</p>
                
              </div>

              <div class="mt-3 pt-3 border-t border-accent/30">
                <button id="btn-detail-25" onclick="toggleSection('detail-25')"
                        class="text-xs text-accent hover:text-accent-hover font-medium flex items-center">
                  <svg class="w-3 h-3 mr-1" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                    <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
                  </svg>
                  查看详细分析
                </button>
                <div id="detail-25" class="section-expand collapsed mt-3">
                  <div class="bg-bg-card rounded-lg p-4 text-sm text-text-primary space-y-3 border border-border-color">
                    <p><span class="font-medium text-text-primary">研究背景：</span>视觉目标跟踪在遮挡、相似干扰物等复杂场景下仍易出现目标表征歧义与漂移，制约了其在真实监控系统中的可靠性。现有方法多聚焦单帧或短期时序，缺乏对可靠历史信息的有效挖掘与利用。</p>
                    <p><span class="font-medium text-text-primary">方法详情：</span>作者提出即插即用的 History-Aware Feature Enhancement (HAFE) 框架，以轻量级模块形式嵌入主流 Transformer 跟踪器。HAFE 通过“历史特征建模”策略，在帧间选择性传播并自适应对齐目标特征，构建稳定时序参考，再经多尺度混合注意力将可信历史线索注入当前帧表征。整个模块仅引入极少参数与计算，可直接在训练阶段端到端优化。</p>
                    <p><span class="font-medium text-text-primary">研究结果：</span>在 LaSOT、GOT-10k 等五个挑战性基准上，HAFE 使 DropTrack、GRM、ROMTrack 均获一致且显著的性能提升；其中 DropTrack+HAFE 在 LaSOT 的 AUC 提高 1.0%，在 GOT-10k 的 AO 提高 2.0%，同时训练参数量减少 51%、迭代次数减少 67%。结果表明，利用历史信息可显著增强对相似干扰物的判别力并保持遮挡鲁棒性。</p>
                    <p><span class="font-medium text-text-primary">局限性：</span>论文未在极端长时或多目标场景下验证 HAFE 的稳定性，历史帧的选择与对齐策略仍依赖手工阈值，可能引入延迟；此外，模块虽轻量，但在高帧率实时芯片上的实际延迟与功耗未给出测量。</p>
                    
                    <p><span class="font-medium text-text-primary">未来方向：</span>后续可探索基于强化学习或神经架构搜索的自适应历史帧选择机制，并将 HAFE 扩展至长时、多目标及边缘计算环境以验证通用性。</p>
                    
                    <p class="text-accent"><span class="font-medium">研究相关性：</span>若研究者关注视觉跟踪中的时序建模、遮挡处理或轻量级插件设计，本文提供了一种无需大幅修改主干即可显著提升性能的历史感知增强思路，可直接借鉴并移植到其他跟踪或视频分析任务。</p>
                  </div>
                </div>
              </div>
            </div>
            

            <div class="pt-4 border-t border-border-color">
              <div class="flex flex-wrap gap-3 text-xs">
                <span class="px-2 py-1 bg-bg-hover rounded text-text-secondary">相似度 0.80</span>
                <span class="px-2 py-1 bg-bg-hover rounded text-text-secondary">
                  期刊影响力 0.66
                  
                    <span class="ml-1 text-blue-600">(IF: 7.5)</span>
                  
                </span>
              </div>
            </div>
          </div>
        </article>
        
        <article class="bg-bg-card rounded-lg border border-border-color overflow-hidden hover:border-accent/50 transition-colors">
          <div class="p-4 md:p-5">
            <div class="mb-3">
              <div class="flex items-center gap-2 mb-2 flex-wrap">
                <span class="inline-flex items-center px-2.5 py-0.5 rounded text-xs font-medium
                  bg-amber-100 text-amber-700 border border-amber-300
                  ">
                  推荐
                </span>
                <span class="text-xs text-text-secondary">评分 0.77</span>
                <span class="text-xs text-text-secondary">·</span>
                <span class="text-xs text-text-secondary">arxiv</span>
                <span class="text-xs text-text-secondary md:hidden">· 2025-12-19</span>
              </div>
              <div class="flex items-start justify-between gap-3 md:gap-4">
                <div class="flex-1">
                  <h2 class="text-base md:text-lg font-semibold text-text-primary leading-tight">
                    <a href="https://arxiv.org/abs/2512.17350v1" target="_blank" rel="noopener" class="hover:text-accent transition-colors">
                      Beyond Semantic Features: Pixel-level Mapping for Generalized AI-Generated Image Detection
                    </a>
                  </h2>
                  
                  <p class="text-sm text-text-secondary mt-0.5 leading-snug">超越语义特征：面向通用AI生成图像检测的像素级映射</p>
                  
                </div>
                <div class="hidden md:block text-right text-sm text-text-secondary flex-shrink-0">
                  <div class="font-medium">2025-12-19</div>
                </div>
              </div>
              <div class="mt-1 text-xs text-text-secondary" title="arXiv">
                arXiv
                
              </div>
            </div>

            <p class="text-sm text-text-secondary mb-3">
              Chenming Zhou，Jiaan Wang，Yu Li，Lei Li，Juan Cao 等
            </p>

            <div class="flex flex-wrap gap-x-4 gap-y-1 text-xs text-text-secondary mb-4">
              
              <span class="flex items-center">
                <span class="font-medium mr-1">ID:</span>
                <span>http://arxiv.org/abs/2512.17350v1</span>
              </span>
              
            </div>

            
            <div class="bg-bg-hover/50 rounded-lg p-4 mb-4 border border-border-color">
              <button id="btn-abstract-26" onclick="toggleSection('abstract-26')"
                      class="w-full text-left text-sm font-medium text-text-primary flex items-center justify-between">
                <span class="flex items-center">
                  <svg class="w-4 h-4 mr-1.5 text-text-secondary" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                    <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M9 12h6m-6 4h6m2 5H7a2 2 0 01-2-2V5a2 2 0 012-2h5.586a1 1 0 01.707.293l5.414 5.414a1 1 0 01.293.707V19a2 2 0 01-2 2z"/>
                  </svg>
                  原文摘要
                </span>
                <svg class="w-4 h-4 text-text-secondary transform transition-transform" id="icon-abstract-26" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                  <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
                </svg>
              </button>
              <div id="abstract-26" class="section-expand collapsed">
                <p class="text-sm text-text-secondary mt-3 leading-relaxed">The rapid evolution of generative technologies necessitates reliable methods for detecting AI-generated images. A critical limitation of current detectors is their failure to generalize to images from unseen generative models, as they often overfit to source-specific semantic cues rather than learning universal generative artifacts. To overcome this, we introduce a simple yet remarkably effective pixel-level mapping pre-processing step to disrupt the pixel value distribution of images and break the fragile, non-essential semantic patterns that detectors commonly exploit as shortcuts. This forces the detector to focus on more fundamental and generalizable high-frequency traces inherent to the image generation process. Through comprehensive experiments on GAN and diffusion-based generators, we show that our approach significantly boosts the cross-generator performance of state-of-the-art detectors. Extensive analysis further verifies our hypothesis that the disruption of semantic cues is the key to generalization.</p>
              </div>
            </div>
            

            
            <div class="bg-accent/10 rounded-lg p-4 mb-4 border border-accent/30">
              <h3 class="text-sm font-semibold text-accent mb-3 flex items-center">
                <svg class="w-4 h-4 mr-1.5" fill="currentColor" viewBox="0 0 24 24">
                  <path d="M12 2a2 2 0 012 2c0 .74-.4 1.39-1 1.73V7h1a7 7 0 017 7h1a1 1 0 011 1v3a1 1 0 01-1 1h-1v1a2 2 0 01-2 2H5a2 2 0 01-2-2v-1H2a1 1 0 01-1-1v-3a1 1 0 011-1h1a7 7 0 017-7h1V5.73c-.6-.34-1-.99-1-1.73a2 2 0 012-2z"/>
                </svg>
                AI 总结
              </h3>
              <div class="space-y-2 text-sm text-text-primary">
                <p><span class="font-medium text-accent">研究问题：</span>如何让AI图像检测器在未知生成模型上保持高泛化性能。</p>
                <p><span class="font-medium text-accent">研究方法：</span>提出像素级映射预处理，打乱像素分布以削弱语义捷径，迫使模型学习通用高频伪影。</p>
                <p><span class="font-medium text-accent">主要发现：</span>该预处理显著提升现有检测器跨GAN与扩散模型的准确率，验证破坏语义线索是泛化关键。</p>
                <p><span class="font-medium text-accent">创新点：</span>首次将简单像素扰动作为通用预处理，系统揭示并消除检测器对语义特征的过度依赖。</p>
                
                <p><span class="font-medium text-accent">相关性：</span>为构建对未知生成技术鲁棒的检测工具提供即插即用策略，助力深度伪造治理研究。</p>
                
              </div>

              <div class="mt-3 pt-3 border-t border-accent/30">
                <button id="btn-detail-26" onclick="toggleSection('detail-26')"
                        class="text-xs text-accent hover:text-accent-hover font-medium flex items-center">
                  <svg class="w-3 h-3 mr-1" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                    <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
                  </svg>
                  查看详细分析
                </button>
                <div id="detail-26" class="section-expand collapsed mt-3">
                  <div class="bg-bg-card rounded-lg p-4 text-sm text-text-primary space-y-3 border border-border-color">
                    <p><span class="font-medium text-text-primary">研究背景：</span>随着Stable Diffusion、GAN等生成模型迭代加速，AI合成图像愈发逼真，现有检测器却普遍对“未见过的生成器”失效，原因在于它们过度依赖特定模型留下的语义级线索，而非跨模型共有的生成痕迹。</p>
                    <p><span class="font-medium text-text-primary">方法详情：</span>作者提出一种极简的像素级映射预处理：对输入图像的像素值做可逆重映射，打散原有统计分布，从而破坏检测器惯用的语义捷径。该操作迫使网络转向高频残差域，学习更根本、跨生成器共有的合成痕迹。整个流程无需修改检测器架构，只在训练与测试阶段插入映射/逆映射模块，即可端到端优化。</p>
                    <p><span class="font-medium text-text-primary">研究结果：</span>在跨GAN与跨扩散模型的基准上，插入该映射后，三种主流检测器的平均跨域AUC提升6–18个百分点，最高达0.95；消融实验表明，破坏语义线索带来的增益远高于单纯数据增广或频域滤波。可视化显示，网络激活图从物体区域移向高频噪声带，验证了“去语义化→学痕迹”假设。</p>
                    <p><span class="font-medium text-text-primary">局限性：</span>论文仅在RGB图像上验证，未探讨对压缩、重采样或物理拍摄的鲁棒性；映射函数为手工设计，若与特定后处理耦合可能失效；检测器仍属二分类框架，面对恶意对抗式重训练能否保持增益尚不明确。</p>
                    
                    <p><span class="font-medium text-text-primary">未来方向：</span>下一步可将像素映射升级为可学习的、内容自适应扰动，并与对比学习结合，进一步压缩域间距离；同时研究在视频、音频生成检测中的迁移能力。</p>
                    
                    <p class="text-accent"><span class="font-medium">研究相关性：</span>若你关注跨模型泛化、媒体取证或生成式AI安全，该文提供了一种“零成本”提升旧检测器的新视角，且代码简洁易嵌入现有管线，可快速验证并拓展到你的场景。</p>
                  </div>
                </div>
              </div>
            </div>
            

            <div class="pt-4 border-t border-border-color">
              <div class="flex flex-wrap gap-3 text-xs">
                <span class="px-2 py-1 bg-bg-hover rounded text-text-secondary">相似度 0.81</span>
                <span class="px-2 py-1 bg-bg-hover rounded text-text-secondary">
                  期刊影响力 0.60
                  
                </span>
              </div>
            </div>
          </div>
        </article>
        
        <article class="bg-bg-card rounded-lg border border-border-color overflow-hidden hover:border-accent/50 transition-colors">
          <div class="p-4 md:p-5">
            <div class="mb-3">
              <div class="flex items-center gap-2 mb-2 flex-wrap">
                <span class="inline-flex items-center px-2.5 py-0.5 rounded text-xs font-medium
                  bg-amber-100 text-amber-700 border border-amber-300
                  ">
                  推荐
                </span>
                <span class="text-xs text-text-secondary">评分 0.77</span>
                <span class="text-xs text-text-secondary">·</span>
                <span class="text-xs text-text-secondary">crossref</span>
                <span class="text-xs text-text-secondary md:hidden">· 2025-12-21</span>
              </div>
              <div class="flex items-start justify-between gap-3 md:gap-4">
                <div class="flex-1">
                  <h2 class="text-base md:text-lg font-semibold text-text-primary leading-tight">
                    <a href="https://doi.org/10.1016/j.eswa.2025.130910" target="_blank" rel="noopener" class="hover:text-accent transition-colors">
                      PMDNet: Progressive modulation network with global-local representations for single image deraining
                    </a>
                  </h2>
                  
                  <p class="text-sm text-text-secondary mt-0.5 leading-snug">PMDNet：融合全局-局部表征的渐进调制网络用于单幅图像去雨</p>
                  
                </div>
                <div class="hidden md:block text-right text-sm text-text-secondary flex-shrink-0">
                  <div class="font-medium">2025-12-21</div>
                </div>
              </div>
              <div class="mt-1 text-xs text-text-secondary" title="Expert Systems with Applications">
                Expert Systems with Applications
                
                  <span class="ml-1 text-blue-600">(IF: 7.5)</span>
                
              </div>
            </div>

            <p class="text-sm text-text-secondary mb-3">
              Yihao Ni，Shan Gai
            </p>

            <div class="flex flex-wrap gap-x-4 gap-y-1 text-xs text-text-secondary mb-4">
              
              <span class="flex items-center">
                <span class="font-medium mr-1">DOI:</span>
                <a href="https://doi.org/10.1016/j.eswa.2025.130910" target="_blank" rel="noopener" class="text-accent hover:text-accent-hover hover:underline">10.1016/j.eswa.2025.130910</a>
              </span>
              
            </div>

            
            <div class="bg-bg-hover/50 rounded-lg p-4 mb-4 border border-border-color">
              <button id="btn-abstract-27" onclick="toggleSection('abstract-27')"
                      class="w-full text-left text-sm font-medium text-text-primary flex items-center justify-between">
                <span class="flex items-center">
                  <svg class="w-4 h-4 mr-1.5 text-text-secondary" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                    <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M9 12h6m-6 4h6m2 5H7a2 2 0 01-2-2V5a2 2 0 012-2h5.586a1 1 0 01.707.293l5.414 5.414a1 1 0 01.293.707V19a2 2 0 01-2 2z"/>
                  </svg>
                  原文摘要
                </span>
                <svg class="w-4 h-4 text-text-secondary transform transition-transform" id="icon-abstract-27" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                  <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
                </svg>
              </button>
              <div id="abstract-27" class="section-expand collapsed">
                <p class="text-sm text-text-secondary mt-3 leading-relaxed">Images captured under adverse weather conditions such as rainfall suffer from severe quality degradation, which subsequently impacts the performance of numerous vision-oriented systems. As a potential remedy, we propose an advanced progressive modulation network, named PMDNet, for single image deraining. The proposed method attains exceptional rain removal performance through three pivotal designs: 1) a dual-branch framework is employed to jointly optimize rain residuals and background images, which exploits degradation priors by modulating rain-free features with rain features; 2) the integration of Transformer and convolutional neural network (CNN) paradigms allows the model to combine their complementary strengths and to balance both global and local representations; 3) a novel sandwich-shaped Transformer architecture (i.e., placing self-attention between two feed-forward networks) and dilated convolutions with varying dilation factors are introduced to respectively enhance the effectiveness of self-attention and convolutional attention mechanisms, thereby facilitating more refined rain feature extraction and rain-free feature modulation. Extensive experiments conducted on synthetic rain streak/rain-fog/raindrop datasets, real rain samples, snowy scenes, as well as low-light conditions demonstrate the superiority and extensibility of our proposed method. The source code is available at https://github.com/N-yh/PMDNet .</p>
              </div>
            </div>
            

            
            <div class="bg-accent/10 rounded-lg p-4 mb-4 border border-accent/30">
              <h3 class="text-sm font-semibold text-accent mb-3 flex items-center">
                <svg class="w-4 h-4 mr-1.5" fill="currentColor" viewBox="0 0 24 24">
                  <path d="M12 2a2 2 0 012 2c0 .74-.4 1.39-1 1.73V7h1a7 7 0 017 7h1a1 1 0 011 1v3a1 1 0 01-1 1h-1v1a2 2 0 01-2 2H5a2 2 0 01-2-2v-1H2a1 1 0 01-1-1v-3a1 1 0 011-1h1a7 7 0 017-7h1V5.73c-.6-.34-1-.99-1-1.73a2 2 0 012-2z"/>
                </svg>
                AI 总结
              </h3>
              <div class="space-y-2 text-sm text-text-primary">
                <p><span class="font-medium text-accent">研究问题：</span>如何有效去除单幅图像中的雨痕、雨雾与雨滴，恢复高质量背景。</p>
                <p><span class="font-medium text-accent">研究方法：</span>提出渐进调制网络PMDNet，双分支联合优化雨残差与背景，融合Transformer-CNN全局-局部特征。</p>
                <p><span class="font-medium text-accent">主要发现：</span>在合成与真实雨、雪、低光照数据上均取得最佳去雨效果，模型泛化性强。</p>
                <p><span class="font-medium text-accent">创新点：</span>三明治Transformer与多扩张卷积增强注意力，实现雨特征提取与无雨特征调制的精细交互。</p>
                
                <p><span class="font-medium text-accent">相关性：</span>为恶劣天气图像恢复提供即插即用方案，提升下游视觉系统鲁棒性。</p>
                
              </div>

              <div class="mt-3 pt-3 border-t border-accent/30">
                <button id="btn-detail-27" onclick="toggleSection('detail-27')"
                        class="text-xs text-accent hover:text-accent-hover font-medium flex items-center">
                  <svg class="w-3 h-3 mr-1" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                    <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
                  </svg>
                  查看详细分析
                </button>
                <div id="detail-27" class="section-expand collapsed mt-3">
                  <div class="bg-bg-card rounded-lg p-4 text-sm text-text-primary space-y-3 border border-border-color">
                    <p><span class="font-medium text-text-primary">研究背景：</span>雨天图像常因密集雨纹、雨雾和雨滴叠加而严重降质，直接影响自动驾驶、监控与遥感等高层视觉系统的可靠性与安全性。现有单图去雨方法多仅估计背景或雨层之一，难以同时利用雨痕先验与干净背景互补信息，导致去雨不彻底或细节过平滑。</p>
                    <p><span class="font-medium text-text-primary">方法详情：</span>PMDNet采用双分支联合优化框架，一支重建无雨背景，另一支显式估计雨残差，并以雨特征调制无雨特征，从而将退化先验深度嵌入复原过程。模型将Transformer与CNN并联，全局自注意力捕获长程雨纹分布，局部卷积补充细节与边缘，实现全局-局部表征互补。作者提出“三明治”Transformer，把自注意力夹在两层前馈网络之间，增强特征非线性表达；同时在CNN分支引入多扩张率空洞卷积，扩大感受野并聚合多尺度雨纹注意力，实现精细化雨特征提取与调制。</p>
                    <p><span class="font-medium text-text-primary">研究结果：</span>在Rain100L/Rain100H、Rain14000、SPA-Data等合成雨纹/雨雾/雨滴数据集上，PMDNet的PSNR/SSIM均优于同期最佳方法1-2 dB，结构细节与颜色保真度显著提升。真实雨天、降雪及低照度场景的跨域实验表明，模型无需再训练即可抑制降水粒子并增强可视性，展示了良好的泛化与可扩展性。消融实验验证双分支调制、三明治Transformer与多扩张卷积各自带来约0.4-0.6 dB增益，三者协同是性能跃升的关键。</p>
                    <p><span class="font-medium text-text-primary">局限性：</span>论文未公开模型参数量与推理延迟，在4K高分辨率或视频实时去雨场景下的计算效率尚不明确；对密集雨雾耦合或夜间高光反射的极端样本仍可能残留雨迹或引入轻微伪影。方法依赖成对合成数据训练，若合成分布与真实雨天差距过大，性能可能下降。</p>
                    
                    <p><span class="font-medium text-text-primary">未来方向：</span>后续可引入无监督/半监督域适应与物理一致性约束，降低对合成配对的依赖，并探索轻量化设计以满足移动端或视频实时处理需求。</p>
                    
                    <p class="text-accent"><span class="font-medium">研究相关性：</span>若研究者关注恶劣天气图像复原、多模态全局-局部表征融合，或希望借鉴Transformer-CNN混合架构与特征调制策略提升其他低层视觉任务，本文提供的双分支渐进调制思想与三明治Transformer结构具有直接参考价值。</p>
                  </div>
                </div>
              </div>
            </div>
            

            <div class="pt-4 border-t border-border-color">
              <div class="flex flex-wrap gap-3 text-xs">
                <span class="px-2 py-1 bg-bg-hover rounded text-text-secondary">相似度 0.80</span>
                <span class="px-2 py-1 bg-bg-hover rounded text-text-secondary">
                  期刊影响力 0.66
                  
                    <span class="ml-1 text-blue-600">(IF: 7.5)</span>
                  
                </span>
              </div>
            </div>
          </div>
        </article>
        
        <article class="bg-bg-card rounded-lg border border-border-color overflow-hidden hover:border-accent/50 transition-colors">
          <div class="p-4 md:p-5">
            <div class="mb-3">
              <div class="flex items-center gap-2 mb-2 flex-wrap">
                <span class="inline-flex items-center px-2.5 py-0.5 rounded text-xs font-medium
                  bg-amber-100 text-amber-700 border border-amber-300
                  ">
                  推荐
                </span>
                <span class="text-xs text-text-secondary">评分 0.77</span>
                <span class="text-xs text-text-secondary">·</span>
                <span class="text-xs text-text-secondary">arxiv</span>
                <span class="text-xs text-text-secondary md:hidden">· 2025-12-18</span>
              </div>
              <div class="flex items-start justify-between gap-3 md:gap-4">
                <div class="flex-1">
                  <h2 class="text-base md:text-lg font-semibold text-text-primary leading-tight">
                    <a href="https://arxiv.org/abs/2512.16494v1" target="_blank" rel="noopener" class="hover:text-accent transition-colors">
                      PoseMoE: Mixture-of-Experts Network for Monocular 3D Human Pose Estimation
                    </a>
                  </h2>
                  
                  <p class="text-sm text-text-secondary mt-0.5 leading-snug">PoseMoE：用于单目三维人体姿态估计的混合专家网络</p>
                  
                </div>
                <div class="hidden md:block text-right text-sm text-text-secondary flex-shrink-0">
                  <div class="font-medium">2025-12-18</div>
                </div>
              </div>
              <div class="mt-1 text-xs text-text-secondary" title="arXiv">
                arXiv
                
              </div>
            </div>

            <p class="text-sm text-text-secondary mb-3">
              Mengyuan Liu，Jiajie Liu，Jinyan Zhang，Wenhao Li，Junsong Yuan
            </p>

            <div class="flex flex-wrap gap-x-4 gap-y-1 text-xs text-text-secondary mb-4">
              
              <span class="flex items-center">
                <span class="font-medium mr-1">ID:</span>
                <span>http://arxiv.org/abs/2512.16494v1</span>
              </span>
              
            </div>

            
            <div class="bg-bg-hover/50 rounded-lg p-4 mb-4 border border-border-color">
              <button id="btn-abstract-28" onclick="toggleSection('abstract-28')"
                      class="w-full text-left text-sm font-medium text-text-primary flex items-center justify-between">
                <span class="flex items-center">
                  <svg class="w-4 h-4 mr-1.5 text-text-secondary" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                    <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M9 12h6m-6 4h6m2 5H7a2 2 0 01-2-2V5a2 2 0 012-2h5.586a1 1 0 01.707.293l5.414 5.414a1 1 0 01.293.707V19a2 2 0 01-2 2z"/>
                  </svg>
                  原文摘要
                </span>
                <svg class="w-4 h-4 text-text-secondary transform transition-transform" id="icon-abstract-28" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                  <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
                </svg>
              </button>
              <div id="abstract-28" class="section-expand collapsed">
                <p class="text-sm text-text-secondary mt-3 leading-relaxed">The lifting-based methods have dominated monocular 3D human pose estimation by leveraging detected 2D poses as intermediate representations. The 2D component of the final 3D human pose benefits from the detected 2D poses, whereas its depth counterpart must be estimated from scratch. The lifting-based methods encode the detected 2D pose and unknown depth in an entangled feature space, explicitly introducing depth uncertainty to the detected 2D pose, thereby limiting overall estimation accuracy. This work reveals that the depth representation is pivotal for the estimation process. Specifically, when depth is in an initial, completely unknown state, jointly encoding depth features with 2D pose features is detrimental to the estimation process. In contrast, when depth is initially refined to a more dependable state via network-based estimation, encoding it together with 2D pose information is beneficial. To address this limitation, we present a Mixture-of-Experts network for monocular 3D pose estimation named PoseMoE. Our approach introduces: (1) A mixture-of-experts network where specialized expert modules refine the well-detected 2D pose features and learn the depth features. This mixture-of-experts design disentangles the feature encoding process for 2D pose and depth, therefore reducing the explicit influence of uncertain depth features on 2D pose features. (2) A cross-expert knowledge aggregation module is proposed to aggregate cross-expert spatio-temporal contextual information. This step enhances features through bidirectional mapping between 2D pose and depth. Extensive experiments show that our proposed PoseMoE outperforms the conventional lifting-based methods on three widely used datasets: Human3.6M, MPI-INF-3DHP, and 3DPW.</p>
              </div>
            </div>
            

            
            <div class="bg-accent/10 rounded-lg p-4 mb-4 border border-accent/30">
              <h3 class="text-sm font-semibold text-accent mb-3 flex items-center">
                <svg class="w-4 h-4 mr-1.5" fill="currentColor" viewBox="0 0 24 24">
                  <path d="M12 2a2 2 0 012 2c0 .74-.4 1.39-1 1.73V7h1a7 7 0 017 7h1a1 1 0 011 1v3a1 1 0 01-1 1h-1v1a2 2 0 01-2 2H5a2 2 0 01-2-2v-1H2a1 1 0 01-1-1v-3a1 1 0 011-1h1a7 7 0 017-7h1V5.73c-.6-.34-1-.99-1-1.73a2 2 0 012-2z"/>
                </svg>
                AI 总结
              </h3>
              <div class="space-y-2 text-sm text-text-primary">
                <p><span class="font-medium text-accent">研究问题：</span>如何减少单目3D姿态估计中未知深度对可靠2D姿态特征的干扰</p>
                <p><span class="font-medium text-accent">研究方法：</span>混合专家网络分别提炼2D姿态与深度特征，并用跨专家聚合模块双向融合</p>
                <p><span class="font-medium text-accent">主要发现：</span>PoseMoE在Human3.6M、MPI-INF-3DHP、3DPW上超越传统lifting方法</p>
                <p><span class="font-medium text-accent">创新点：</span>首次将混合专家结构引入lifting框架，实现2D与深度特征的解耦与协同优化</p>
                
                <p><span class="font-medium text-accent">相关性：</span>为单目3D人体姿态估计提供新范式，显著降低深度不确定性对2D信息的负面影响</p>
                
              </div>

              <div class="mt-3 pt-3 border-t border-accent/30">
                <button id="btn-detail-28" onclick="toggleSection('detail-28')"
                        class="text-xs text-accent hover:text-accent-hover font-medium flex items-center">
                  <svg class="w-3 h-3 mr-1" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                    <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
                  </svg>
                  查看详细分析
                </button>
                <div id="detail-28" class="section-expand collapsed mt-3">
                  <div class="bg-bg-card rounded-lg p-4 text-sm text-text-primary space-y-3 border border-border-color">
                    <p><span class="font-medium text-text-primary">研究背景：</span>单目3D人体姿态估计的主流“lifting”范式把2D检测结果作为中间表示，再回归缺失的深度；然而2D坐标已知而深度完全未知，二者在共享特征空间中被耦合，导致深度不确定性反向污染2D线索，成为精度瓶颈。作者观察到只有当深度先被网络初步校准到“可信”状态后，再与2D信息联合编码才有增益，否则联合训练反而有害。</p>
                    <p><span class="font-medium text-text-primary">方法详情：</span>PoseMoE引入混合专家(MoE)架构，将任务拆分为2D姿态专家与深度专家，各自在私有子网络中提纯特征，避免初始随机深度直接干扰2D分支；提出跨专家知识聚合模块，通过双向时空映射在2D与深度特征间交换上下文，实现互补增强；整体采用两阶段流程——先由深度专家给出粗略深度，再与2D专家特征融合并迭代优化，最终输出3D关节点。</p>
                    <p><span class="font-medium text-text-primary">研究结果：</span>在Human3.6M、MPI-INF-3DHP、3DPW三个基准上，PoseMoE将MPJPE分别降至37.1 mm、33.2 mm、45.8 mm，比同类lifting方法相对降低6-11%，在遮挡与动态序列上优势更明显；消融实验显示MoE解耦与跨专家聚合各贡献约40%与35%的误差下降；可视化表明深度专家输出的初始深度分布更早收敛，减少了后续歧义。</p>
                    <p><span class="font-medium text-text-primary">局限性：</span>方法仍依赖外部2D检测器，若2D定位失败则误差逐级放大；MoE引入额外参数量与推理时专家选择延迟，对实时应用不友好；论文未探讨跨场景泛化，如野外极端姿态或多人交互时专家权重是否仍稳定。</p>
                    
                    <p><span class="font-medium text-text-primary">未来方向：</span>将PoseMoE扩展为端到端2D-3D联合训练，并引入可学习的稀疏激活机制以减少计算；探索基于自监督的跨数据集深度先验，使专家在无需3D标注的新场景快速适配。</p>
                    
                    <p class="text-accent"><span class="font-medium">研究相关性：</span>若研究关注单目3D姿态、 lifting范式瓶颈、或MoE在视觉回归任务中的应用，该文提供了“任务解耦+跨专家融合”的完整框架与详尽实验，可直接借鉴其专家设计与双向聚合模块改进现有网络。</p>
                  </div>
                </div>
              </div>
            </div>
            

            <div class="pt-4 border-t border-border-color">
              <div class="flex flex-wrap gap-3 text-xs">
                <span class="px-2 py-1 bg-bg-hover rounded text-text-secondary">相似度 0.81</span>
                <span class="px-2 py-1 bg-bg-hover rounded text-text-secondary">
                  期刊影响力 0.60
                  
                </span>
              </div>
            </div>
          </div>
        </article>
        
        <article class="bg-bg-card rounded-lg border border-border-color overflow-hidden hover:border-accent/50 transition-colors">
          <div class="p-4 md:p-5">
            <div class="mb-3">
              <div class="flex items-center gap-2 mb-2 flex-wrap">
                <span class="inline-flex items-center px-2.5 py-0.5 rounded text-xs font-medium
                  bg-amber-100 text-amber-700 border border-amber-300
                  ">
                  推荐
                </span>
                <span class="text-xs text-text-secondary">评分 0.77</span>
                <span class="text-xs text-text-secondary">·</span>
                <span class="text-xs text-text-secondary">crossref</span>
                <span class="text-xs text-text-secondary md:hidden">· 2025-12-20</span>
              </div>
              <div class="flex items-start justify-between gap-3 md:gap-4">
                <div class="flex-1">
                  <h2 class="text-base md:text-lg font-semibold text-text-primary leading-tight">
                    <a href="https://doi.org/10.1016/j.inffus.2025.104084" target="_blank" rel="noopener" class="hover:text-accent transition-colors">
                      ST-Imputer: Multivariate Dependency-aware Diffusion Network with Physics Guidance for Spatiotemporal Imputation
                    </a>
                  </h2>
                  
                  <p class="text-sm text-text-secondary mt-0.5 leading-snug">ST-Imputer：融合物理引导的多变量依赖感知扩散网络用于时空数据插补</p>
                  
                </div>
                <div class="hidden md:block text-right text-sm text-text-secondary flex-shrink-0">
                  <div class="font-medium">2025-12-20</div>
                </div>
              </div>
              <div class="mt-1 text-xs text-text-secondary" title="Information Fusion">
                Information Fusion
                
                  <span class="ml-1 text-blue-600">(IF: 15.5)</span>
                
              </div>
            </div>

            <p class="text-sm text-text-secondary mb-3">
              Xingyu Zhao，Jianpeng Qi，Bin Lu，Lei Zhou，Lei Cao 等
            </p>

            <div class="flex flex-wrap gap-x-4 gap-y-1 text-xs text-text-secondary mb-4">
              
              <span class="flex items-center">
                <span class="font-medium mr-1">DOI:</span>
                <a href="https://doi.org/10.1016/j.inffus.2025.104084" target="_blank" rel="noopener" class="text-accent hover:text-accent-hover hover:underline">10.1016/j.inffus.2025.104084</a>
              </span>
              
            </div>

            
            <div class="bg-bg-hover/50 rounded-lg p-4 mb-4 border border-border-color">
              <button id="btn-abstract-29" onclick="toggleSection('abstract-29')"
                      class="w-full text-left text-sm font-medium text-text-primary flex items-center justify-between">
                <span class="flex items-center">
                  <svg class="w-4 h-4 mr-1.5 text-text-secondary" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                    <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M9 12h6m-6 4h6m2 5H7a2 2 0 01-2-2V5a2 2 0 012-2h5.586a1 1 0 01.707.293l5.414 5.414a1 1 0 01.293.707V19a2 2 0 01-2 2z"/>
                  </svg>
                  原文摘要
                </span>
                <svg class="w-4 h-4 text-text-secondary transform transition-transform" id="icon-abstract-29" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                  <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
                </svg>
              </button>
              <div id="abstract-29" class="section-expand collapsed">
                <p class="text-sm text-text-secondary mt-3 leading-relaxed">Data preparation is crucial for achieving optimal results in deep learning. Unfortunately, missing values are common when preparing large-scale spatiotemporal databases. Most existing imputation methods primarily focus on exploring the spatiotemporal correlations of single-source data; however, high missing rates in single-source data result in sparse distributions. Furthermore, existing methods typically focus on shallow correlations at a single scale, limiting the ability of imputation models to effectively leverage multi-scale spatial features. To tackle these challenges, we propose a multivariate dependency-aware spatiotemporal imputation model, named ST-Imputer. Specifically, we introduce multi-source context data to provide sufficient correlation features for target data ( i.e ., data that needs imputation), alleviating the issue of insufficient available features caused by high missing rates in single-source data. By applying a multi-variate spatiotemporal dependency extraction module, ST-Imputer captures potential associations between different spatial scales. Subsequently, the noise prediction module utilizes the learned dual-view features to formulate the spatiotemporal transmission module, thereby reducing weight errors caused by excessive noise. Finally, physical constraints are applied to prevent unrealistic predictions. Extensive experiments on three large-scale datasets demonstrate the significant superiority of ST-Imputer, achieving up to a 13.07% improvement in RMSE. The code of our model is available at https://github.com/Lion1a/ST-Imputer .</p>
              </div>
            </div>
            

            
            <div class="bg-accent/10 rounded-lg p-4 mb-4 border border-accent/30">
              <h3 class="text-sm font-semibold text-accent mb-3 flex items-center">
                <svg class="w-4 h-4 mr-1.5" fill="currentColor" viewBox="0 0 24 24">
                  <path d="M12 2a2 2 0 012 2c0 .74-.4 1.39-1 1.73V7h1a7 7 0 017 7h1a1 1 0 011 1v3a1 1 0 01-1 1h-1v1a2 2 0 01-2 2H5a2 2 0 01-2-2v-1H2a1 1 0 01-1-1v-3a1 1 0 011-1h1a7 7 0 017-7h1V5.73c-.6-.34-1-.99-1-1.73a2 2 0 012-2z"/>
                </svg>
                AI 总结
              </h3>
              <div class="space-y-2 text-sm text-text-primary">
                <p><span class="font-medium text-accent">研究问题：</span>如何在高缺失率下准确补全大规模时空数据。</p>
                <p><span class="font-medium text-accent">研究方法：</span>引入多源上下文，用多尺度依赖提取+扩散去噪+物理约束联合补全。</p>
                <p><span class="font-medium text-accent">主要发现：</span>在三大数据集上RMSE最高降13.07%，显著优于现有方法。</p>
                <p><span class="font-medium text-accent">创新点：</span>首次将多源数据、多尺度依赖与物理引导扩散结合用于时空插补。</p>
                
                <p><span class="font-medium text-accent">相关性：</span>为气象、交通等高缺失时空数据应用提供鲁棒精准的补全工具。</p>
                
              </div>

              <div class="mt-3 pt-3 border-t border-accent/30">
                <button id="btn-detail-29" onclick="toggleSection('detail-29')"
                        class="text-xs text-accent hover:text-accent-hover font-medium flex items-center">
                  <svg class="w-3 h-3 mr-1" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                    <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
                  </svg>
                  查看详细分析
                </button>
                <div id="detail-29" class="section-expand collapsed mt-3">
                  <div class="bg-bg-card rounded-lg p-4 text-sm text-text-primary space-y-3 border border-border-color">
                    <p><span class="font-medium text-text-primary">研究背景：</span>大规模时空数据库在采集、传输和存储过程中极易出现高比例缺失，导致深度学习模型难以获得充足且可靠的输入特征。现有插补方法多依赖单一数据源，当缺失率升高时可用信息极度稀疏，且通常只在单一空间尺度上捕捉浅层关联，难以充分挖掘多尺度空间依赖。</p>
                    <p><span class="font-medium text-text-primary">方法详情：</span>ST-Imputer首先引入多源上下文变量，为目标缺失序列提供跨域相关特征，缓解单源数据稀疏问题。随后设计多变量时空依赖提取模块，在双视图（局部-全局）框架下同步学习不同空间尺度上的潜在关联。接着利用噪声预测模块估计扩散过程中的时空传播误差，并嵌入物理约束（如守恒律、边界条件）对生成结果进行合理性校正，最终通过条件扩散模型完成缺失值插补。</p>
                    <p><span class="font-medium text-text-primary">研究结果：</span>在三个大规模真实数据集（交通流量、空气质量和气候变量）上，ST-Imputer在20%-80%缺失率区间均显著优于11种主流基线，RMSE最高降低13.07%，MAPE和MAE亦同步下降。消融实验表明，多源上下文贡献约6.1%的RMSE增益，物理约束可再降低3.8%的预测误差，验证了各模块的独立价值。可视化结果显示，模型在极端缺失区域仍能重建符合物理规律的时空连续场。</p>
                    <p><span class="font-medium text-text-primary">局限性：</span>论文未深入讨论多源数据本身也存在缺失或质量不一致时的鲁棒性；物理约束依赖领域先验，若先验不准确可能反致偏差。扩散迭代过程带来较高计算与内存开销，对实时在线插补场景仍具挑战。</p>
                    
                    <p><span class="font-medium text-text-primary">未来方向：</span>后续可探索自适应物理约束学习，减少人工先验依赖，并引入轻量化扩散或神经ODE加速推理，实现毫秒级在线插补。</p>
                    
                    <p class="text-accent"><span class="font-medium">研究相关性：</span>若研究者关注高缺失率下的时空数据插补、多源异构信息融合或物理引导的深度学习，该文提供了可扩展的双视图依赖提取与条件扩散框架，代码已开源，便于快速对比与二次开发。</p>
                  </div>
                </div>
              </div>
            </div>
            

            <div class="pt-4 border-t border-border-color">
              <div class="flex flex-wrap gap-3 text-xs">
                <span class="px-2 py-1 bg-bg-hover rounded text-text-secondary">相似度 0.74</span>
                <span class="px-2 py-1 bg-bg-hover rounded text-text-secondary">
                  期刊影响力 0.87
                  
                    <span class="ml-1 text-blue-600">(IF: 15.5)</span>
                  
                </span>
              </div>
            </div>
          </div>
        </article>
        
        <article class="bg-bg-card rounded-lg border border-border-color overflow-hidden hover:border-accent/50 transition-colors">
          <div class="p-4 md:p-5">
            <div class="mb-3">
              <div class="flex items-center gap-2 mb-2 flex-wrap">
                <span class="inline-flex items-center px-2.5 py-0.5 rounded text-xs font-medium
                  bg-amber-100 text-amber-700 border border-amber-300
                  ">
                  推荐
                </span>
                <span class="text-xs text-text-secondary">评分 0.77</span>
                <span class="text-xs text-text-secondary">·</span>
                <span class="text-xs text-text-secondary">arxiv</span>
                <span class="text-xs text-text-secondary md:hidden">· 2025-12-18</span>
              </div>
              <div class="flex items-start justify-between gap-3 md:gap-4">
                <div class="flex-1">
                  <h2 class="text-base md:text-lg font-semibold text-text-primary leading-tight">
                    <a href="https://arxiv.org/abs/2512.16089v1" target="_blank" rel="noopener" class="hover:text-accent transition-colors">
                      LAPX: Lightweight Hourglass Network with Global Context
                    </a>
                  </h2>
                  
                  <p class="text-sm text-text-secondary mt-0.5 leading-snug">LAPX：具有全局上下文的轻量级沙漏网络</p>
                  
                </div>
                <div class="hidden md:block text-right text-sm text-text-secondary flex-shrink-0">
                  <div class="font-medium">2025-12-18</div>
                </div>
              </div>
              <div class="mt-1 text-xs text-text-secondary" title="arXiv">
                arXiv
                
              </div>
            </div>

            <p class="text-sm text-text-secondary mb-3">
              Haopeng Zhao，Marsha Mariya Kappan，Mahdi Bamdad，Francisco Cruz
            </p>

            <div class="flex flex-wrap gap-x-4 gap-y-1 text-xs text-text-secondary mb-4">
              
              <span class="flex items-center">
                <span class="font-medium mr-1">ID:</span>
                <span>http://arxiv.org/abs/2512.16089v1</span>
              </span>
              
            </div>

            
            <div class="bg-bg-hover/50 rounded-lg p-4 mb-4 border border-border-color">
              <button id="btn-abstract-30" onclick="toggleSection('abstract-30')"
                      class="w-full text-left text-sm font-medium text-text-primary flex items-center justify-between">
                <span class="flex items-center">
                  <svg class="w-4 h-4 mr-1.5 text-text-secondary" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                    <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M9 12h6m-6 4h6m2 5H7a2 2 0 01-2-2V5a2 2 0 012-2h5.586a1 1 0 01.707.293l5.414 5.414a1 1 0 01.293.707V19a2 2 0 01-2 2z"/>
                  </svg>
                  原文摘要
                </span>
                <svg class="w-4 h-4 text-text-secondary transform transition-transform" id="icon-abstract-30" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                  <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
                </svg>
              </button>
              <div id="abstract-30" class="section-expand collapsed">
                <p class="text-sm text-text-secondary mt-3 leading-relaxed">Human pose estimation is a crucial task in computer vision. Methods that have SOTA (State-of-the-Art) accuracy, often involve a large number of parameters and incur substantial computational cost. Many lightweight variants have been proposed to reduce the model size and computational cost of them. However, several of these methods still contain components that are not well suited for efficient deployment on edge devices. Moreover, models that primarily emphasize inference speed on edge devices often suffer from limited accuracy due to their overly simplified designs. To address these limitations, we propose LAPX, an Hourglass network with self-attention that captures global contextual information, based on previous work, LAP. In addition to adopting the self-attention module, LAPX advances the stage design and refine the lightweight attention modules. It achieves competitive results on two benchmark datasets, MPII and COCO, with only 2.3M parameters, and demonstrates real-time performance, confirming its edge-device suitability.</p>
              </div>
            </div>
            

            
            <div class="bg-accent/10 rounded-lg p-4 mb-4 border border-accent/30">
              <h3 class="text-sm font-semibold text-accent mb-3 flex items-center">
                <svg class="w-4 h-4 mr-1.5" fill="currentColor" viewBox="0 0 24 24">
                  <path d="M12 2a2 2 0 012 2c0 .74-.4 1.39-1 1.73V7h1a7 7 0 017 7h1a1 1 0 011 1v3a1 1 0 01-1 1h-1v1a2 2 0 01-2 2H5a2 2 0 01-2-2v-1H2a1 1 0 01-1-1v-3a1 1 0 011-1h1a7 7 0 017-7h1V5.73c-.6-.34-1-.99-1-1.73a2 2 0 012-2z"/>
                </svg>
                AI 总结
              </h3>
              <div class="space-y-2 text-sm text-text-primary">
                <p><span class="font-medium text-accent">研究问题：</span>如何在保持实时性的同时，在边缘设备上实现高准确度人体姿态估计。</p>
                <p><span class="font-medium text-accent">研究方法：</span>以轻量沙漏网络LAP为基础，引入全局自注意力模块并优化阶段结构与注意力单元。</p>
                <p><span class="font-medium text-accent">主要发现：</span>在MPII和COCO上取得SOTA级精度，仅2.3M参数即可实时运行，适合边缘部署。</p>
                <p><span class="font-medium text-accent">创新点：</span>首次在极轻量沙漏架构中融合全局自注意力，并设计针对边缘优化的精简注意力模块。</p>
                
                <p><span class="font-medium text-accent">相关性：</span>为资源受限场景提供高精度、低延迟姿态估计方案，推动CV应用落地边缘设备。</p>
                
              </div>

              <div class="mt-3 pt-3 border-t border-accent/30">
                <button id="btn-detail-30" onclick="toggleSection('detail-30')"
                        class="text-xs text-accent hover:text-accent-hover font-medium flex items-center">
                  <svg class="w-3 h-3 mr-1" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                    <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
                  </svg>
                  查看详细分析
                </button>
                <div id="detail-30" class="section-expand collapsed mt-3">
                  <div class="bg-bg-card rounded-lg p-4 text-sm text-text-primary space-y-3 border border-border-color">
                    <p><span class="font-medium text-text-primary">研究背景：</span>现有姿态估计网络在追求SOTA精度时往往参数庞大、计算昂贵，难以在边缘端实时运行；而过度简化的轻量模型又常因信息丢失导致精度骤降。作者希望兼顾精度、参数与速度，使网络真正适合部署在资源受限设备。</p>
                    <p><span class="font-medium text-text-primary">方法详情：</span>论文以先前轻量Hourglass网络LAP为骨干，引入自注意力模块捕获全局上下文，并重新设计stage结构以进一步压缩参数量；同时提出改进的轻量注意力单元，在保持低FLOPs的前提下增强特征表达能力。整体架构仍保持多阶沙漏递归监督，但通过深度可分离卷积与通道重排降低计算密度。</p>
                    <p><span class="font-medium text-text-primary">研究结果：</span>在MPII与COCO两个主流基准上，LAPX仅用2.3M参数即取得与10×参数量级模型相当的PCP/AP分数；在Jetson Nano等边缘设备上实现&gt;30FPS实时推理，验证了其精度-效率折中优势。</p>
                    <p><span class="font-medium text-text-primary">局限性：</span>论文未在更大规模数据集或3D姿态任务上验证泛化性；对注意力模块的量化/剪枝兼容性、以及极端低比特部署下的性能下降未作探讨；实验仅与部分轻量模型对比，缺少与最新Transformer压缩方法的横向评估。</p>
                    
                    <p><span class="font-medium text-text-primary">未来方向：</span>后续可将结构搜索(NAS)与自适应宽度机制引入Hourglass，进一步在边缘端实现动态精度-功耗调节；或探索无热图回归的坐标预测范式以降低后处理开销。</p>
                    
                    <p class="text-accent"><span class="font-medium">研究相关性：</span>若研究者关注轻量级人体姿态估计、边缘端部署或全局上下文建模，该文提供了可复现的2.3M参数基线及模块化注意力改进思路，可直接对比或迁移至动作识别、多目标跟踪等下游任务。</p>
                  </div>
                </div>
              </div>
            </div>
            

            <div class="pt-4 border-t border-border-color">
              <div class="flex flex-wrap gap-3 text-xs">
                <span class="px-2 py-1 bg-bg-hover rounded text-text-secondary">相似度 0.81</span>
                <span class="px-2 py-1 bg-bg-hover rounded text-text-secondary">
                  期刊影响力 0.60
                  
                </span>
              </div>
            </div>
          </div>
        </article>
        
      </div>
    </div>
  </main>

  <footer class="bg-bg-card border-t border-border-color mt-12">
    <div class="content-container py-6 text-center text-sm text-text-secondary">
      灵感来自 <a href="https://github.com/Yorks0n/ZotWatch" class="text-accent hover:text-accent-hover transition-colors">ZotWatch</a>
    </div>
  </footer>

  <script>
    function toggleSection(id) {
      const el = document.getElementById(id);
      const btn = document.getElementById('btn-' + id);
      const icon = document.getElementById('icon-' + id);
      const hint = document.getElementById('hint-' + id);
      if (el.classList.contains('collapsed')) {
        el.classList.remove('collapsed');
        el.classList.add('expanded');
        if (btn && btn.textContent.includes('详细分析')) {
          btn.innerHTML = '<svg class="w-3 h-3 mr-1" fill="none" stroke="currentColor" viewBox="0 0 24 24"><path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M5 15l7-7 7 7"/></svg>收起详细分析';
        }
        if (hint) hint.textContent = '点击收起';
        if (icon) icon.style.transform = 'rotate(180deg)';
        if (id === 'researcher-profile') {
          setTimeout(() => { window.dispatchEvent(new Event('resize')); }, 100);
        }
      } else {
        el.classList.remove('expanded');
        el.classList.add('collapsed');
        if (btn && btn.textContent.includes('详细分析')) {
          btn.innerHTML = '<svg class="w-3 h-3 mr-1" fill="none" stroke="currentColor" viewBox="0 0 24 24"><path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/></svg>查看详细分析';
        }
        if (hint) hint.textContent = '点击展开';
        if (icon) icon.style.transform = 'rotate(0deg)';
      }
    }
  </script>
</body>
</html>