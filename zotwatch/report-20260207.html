<!DOCTYPE html>
<html lang="zh-CN">
<head>
  <meta charset="utf-8">
  <meta name="viewport" content="width=device-width, initial-scale=1">
  <title>ZotWatch 文献推荐 - 2026-02-07</title>
  <script src="https://cdn.tailwindcss.com"></script>
  <script src="https://cdn.jsdelivr.net/npm/chart.js@4.4.1/dist/chart.umd.min.js"></script>
  <script src="https://d3js.org/d3.v7.min.js"></script>
  <script>
    tailwind.config = {
      theme: {
        extend: {
          colors: {
            'bg-primary': '#f8fafc',
            'bg-card': '#ffffff',
            'bg-hover': '#f1f5f9',
            'text-primary': '#1e293b',
            'text-secondary': '#64748b',
            'border-color': '#e2e8f0',
            'accent': '#2563eb',
            'accent-hover': '#1d4ed8',
          }
        }
      }
    }
  </script>
  <style>
    .section-expand { transition: max-height 0.5s ease-out; overflow: hidden; }
    .section-expand.collapsed { max-height: 0; }
    .section-expand.expanded { max-height: none; }
    body { font-family: -apple-system, BlinkMacSystemFont, 'Segoe UI', Roboto, 'Helvetica Neue', Arial, sans-serif; }
    .content-container { max-width: 48rem; margin: 0 auto; padding: 0 1rem; }
    @media (min-width: 768px) { .content-container { padding: 0 2rem; } }
    @media (min-width: 1024px) { .content-container { padding: 0 4rem; } }

    .cluster-graph-container { width: 100%; height: 280px; touch-action: pan-y; }
    @media (min-width: 640px) { .cluster-graph-container { height: 320px; } }
    @media (min-width: 768px) { .cluster-graph-container { height: 350px; } }
    #cluster-graph svg { overflow: visible; }
  </style>
</head>
<body class="bg-bg-primary min-h-screen text-text-primary">
  <header class="bg-bg-card border-b border-border-color">
    <div class="content-container py-6 md:py-8">
      <h1 class="text-xl md:text-2xl font-bold text-text-primary">ZotWatch 文献推荐</h1>
      <p class="text-xs md:text-sm text-text-secondary mt-1">
        共 35 篇论文 ·
        生成于 2026-02-07 11:26 Asia/Shanghai
      </p>
    </div>
  </header>

  
  <section class="py-4 md:py-6 border-b border-border-color">
    <div class="content-container">
      <button id="btn-researcher-profile" onclick="toggleSection('researcher-profile')"
              class="w-full text-left text-lg md:text-xl font-bold text-text-primary mb-3 md:mb-4 flex items-center justify-between group hover:text-accent transition-colors">
        <span class="flex items-center gap-2 flex-1 min-w-0">
          <svg class="w-5 h-5 flex-shrink-0 text-accent" fill="currentColor" viewBox="0 0 24 24">
            <path d="M12 2C6.48 2 2 6.48 2 12s4.48 10 10 10 10-4.48 10-10S17.52 2 12 2zm0 3c1.66 0 3 1.34 3 3s-1.34 3-3 3-3-1.34-3-3 1.34-3 3-3zm0 14.2c-2.5 0-4.71-1.28-6-3.22.03-1.99 4-3.08 6-3.08 1.99 0 5.97 1.09 6 3.08-1.29 1.94-3.5 3.22-6 3.22z"/>
          </svg>
          <span class="truncate">研究兴趣画像</span>
        </span>
        <span class="flex items-center gap-1 md:gap-2 flex-shrink-0 ml-2">
          <span class="hidden md:inline text-sm font-normal text-text-secondary group-hover:text-accent" id="hint-researcher-profile">点击展开</span>
          <svg class="w-5 h-5 md:w-4 md:h-4 text-text-secondary group-hover:text-accent transform transition-transform" id="icon-researcher-profile" fill="none" stroke="currentColor" viewBox="0 0 24 24">
            <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
          </svg>
        </span>
      </button>

      <div id="researcher-profile" class="section-expand collapsed">
      <div class="bg-bg-card rounded-lg border border-border-color p-4 mb-4">
        <div class="grid grid-cols-4 gap-3 text-center">
          <div>
            <div class="text-xl font-bold text-accent">975</div>
            <div class="text-xs text-text-secondary">收藏论文</div>
          </div>
          <div>
            <div class="text-xl font-bold text-accent">5年9月</div>
            <div class="text-xs text-text-secondary">收藏时长</div>
          </div>
          <div>
            <div class="text-xl font-bold text-accent">&gt;10</div>
            <div class="text-xs text-text-secondary">兴趣领域</div>
          </div>
          <div>
            <div class="text-xl font-bold text-accent">10</div>
            <div class="text-xs text-text-secondary">高频作者</div>
          </div>
        </div>
      </div>

      
      <div class="bg-accent/10 rounded-lg border border-accent/30 p-4 mb-4">
        <h3 class="text-sm font-semibold text-accent mb-2 flex items-center gap-2">
          <svg class="w-4 h-4" fill="currentColor" viewBox="0 0 24 24">
            <path d="M12 2a2 2 0 012 2c0 .74-.4 1.39-1 1.73V7h1a7 7 0 017 7h1a1 1 0 011 1v3a1 1 0 01-1 1h-1v1a2 2 0 01-2 2H5a2 2 0 01-2-2v-1H2a1 1 0 01-1-1v-3a1 1 0 011-1h1a7 7 0 017-7h1V5.73c-.6-.34-1-.99-1-1.73a2 2 0 012-2z"/>
          </svg>
          AI 研究兴趣分析
        </h3>
        <div class="space-y-2 text-xs text-text-primary leading-relaxed">
          <p><span class="font-medium text-accent">研究兴趣：</span>用户长期聚焦计算机视觉核心任务（目标检测、视觉定位、姿态估计）与模型高效化（压缩、知识蒸馏、重参数化），同时积极追踪自监督/对比学习等表征学习新范式。</p>
          <p><span class="font-medium text-accent">深度关注：</span>在CVPR、ICCV、TPAMI等顶会顶刊持续收藏115+篇文献，对Kaiming He、Ross Girshick等团队的检测与表征学习工作保持9-24篇的系统跟踪，形成从基础模型到压缩落地的完整阅读链条。</p>
          <p><span class="font-medium text-accent">跨学科倾向：</span>约93篇遥感类文献（TGRS、GRSL、雷达学报）与SAR关键词显示，用户将通用视觉方法迁移至遥感解译，体现“CV+遥感”交叉收藏特征。</p>
          <p><span class="font-medium text-accent">兴趣演变：</span>2025-Q1单季新增102篇达峰值，关键词同步出现DeepSeek、大语言模型、扩散模型，提示兴趣正向多模态大模型与生成式AI快速扩展；2024-Q3后收藏量回落，可能进入主题筛选期。</p>
          <p><span class="font-medium text-accent">阅读建议：</span>建议关注多模态检测（Vision-Language Detection）、轻量化Transformer与SAR图像生成评估基准，以延续检测+压缩+遥感的交叉优势。</p>
        </div>
      </div>
      

      <div class="grid grid-cols-1 md:grid-cols-2 gap-4 mb-4">
        
        <div class="bg-bg-card rounded-lg border border-border-color p-4">
          <h4 class="text-sm font-medium text-text-primary mb-3">研究领域分布</h4>
          <div style="height: 180px;">
            <canvas id="domainsChart"></canvas>
          </div>
        </div>
        

        
        <div class="bg-bg-card rounded-lg border border-border-color p-4">
          <h4 class="text-sm font-medium text-text-primary mb-3">阅读趋势（季度）</h4>
          <div style="height: 180px;">
            <canvas id="trendsChart"></canvas>
          </div>
        </div>
        
      </div>

      
      <div class="bg-bg-card rounded-lg border border-border-color p-4 mb-4">
        <h4 class="text-sm font-medium text-text-primary mb-3">论文发表年份分布</h4>
        <div style="height: 180px;">
          <canvas id="yearDistChart"></canvas>
        </div>
      </div>
      

      
      <div class="bg-bg-card rounded-lg border border-border-color p-4 mb-4">
        <h4 class="text-sm font-medium text-text-primary mb-3">
          研究方向聚类图谱
          <span class="text-xs text-text-secondary font-normal ml-2">(30 个聚类)</span>
        </h4>
        <div id="cluster-graph" class="cluster-graph-container"></div>
        <div class="text-xs text-text-secondary mt-3">
          覆盖 949/949 篇论文（仅含摘要的文献参与聚类）
        </div>
      </div>
      

      <div class="grid grid-cols-1 md:grid-cols-2 gap-4 mb-4">
        
        <div class="bg-bg-card rounded-lg border border-border-color p-4">
          <h4 class="text-sm font-medium text-text-primary mb-2">高频关注作者</h4>
          <div class="grid grid-cols-2 gap-x-4 gap-y-1 text-xs">
            
            <div class="flex justify-between truncate">
              <span class="text-text-primary truncate" title="Kaiming He">Kaiming He</span>
              <span class="text-text-secondary ml-1 flex-shrink-0">24</span>
            </div>
            
            <div class="flex justify-between truncate">
              <span class="text-text-primary truncate" title="Ross Girshick">Ross Girshick</span>
              <span class="text-text-secondary ml-1 flex-shrink-0">18</span>
            </div>
            
            <div class="flex justify-between truncate">
              <span class="text-text-primary truncate" title="Jian Sun">Jian Sun</span>
              <span class="text-text-secondary ml-1 flex-shrink-0">15</span>
            </div>
            
            <div class="flex justify-between truncate">
              <span class="text-text-primary truncate" title="Song Han">Song Han</span>
              <span class="text-text-secondary ml-1 flex-shrink-0">14</span>
            </div>
            
            <div class="flex justify-between truncate">
              <span class="text-text-primary truncate" title="Xiangyu Zhang">Xiangyu Zhang</span>
              <span class="text-text-secondary ml-1 flex-shrink-0">14</span>
            </div>
            
            <div class="flex justify-between truncate">
              <span class="text-text-primary truncate" title="Piotr Dollár">Piotr Dollár</span>
              <span class="text-text-secondary ml-1 flex-shrink-0">12</span>
            </div>
            
            <div class="flex justify-between truncate">
              <span class="text-text-primary truncate" title="Li Liu">Li Liu</span>
              <span class="text-text-secondary ml-1 flex-shrink-0">11</span>
            </div>
            
            <div class="flex justify-between truncate">
              <span class="text-text-primary truncate" title="Xiang Li">Xiang Li</span>
              <span class="text-text-secondary ml-1 flex-shrink-0">11</span>
            </div>
            
            <div class="flex justify-between truncate">
              <span class="text-text-primary truncate" title="Yann LeCun">Yann LeCun</span>
              <span class="text-text-secondary ml-1 flex-shrink-0">10</span>
            </div>
            
            <div class="flex justify-between truncate">
              <span class="text-text-primary truncate" title="Wenfeng Liang">Wenfeng Liang</span>
              <span class="text-text-secondary ml-1 flex-shrink-0">10</span>
            </div>
            
            <div class="flex justify-between truncate">
              <span class="text-text-primary truncate" title="Zhenda Xie">Zhenda Xie</span>
              <span class="text-text-secondary ml-1 flex-shrink-0">9</span>
            </div>
            
            <div class="flex justify-between truncate">
              <span class="text-text-primary truncate" title="Feng Xu">Feng Xu</span>
              <span class="text-text-secondary ml-1 flex-shrink-0">9</span>
            </div>
            
          </div>
        </div>
        

        
        <div class="bg-bg-card rounded-lg border border-border-color p-4">
          <h4 class="text-sm font-medium text-text-primary mb-2">常关注期刊/会议</h4>
          <div class="space-y-1 text-xs">
            
            <div class="flex items-center justify-between">
              <span class="text-text-primary truncate flex-1" title="Computer Vision and Pattern Recognition">Computer Vision and Pattern Recognition</span>
              <span class="text-xs px-1 py-0.5 rounded bg-blue-100 text-blue-700 ml-1 flex-shrink-0">
                期刊
              </span>
              <span class="text-text-secondary ml-1 flex-shrink-0">115</span>
            </div>
            
            <div class="flex items-center justify-between">
              <span class="text-text-primary truncate flex-1" title="IEEE Transactions on Geoscience and Remote Sensing">IEEE Transactions on Geoscience and Remote Sensing</span>
              <span class="text-xs px-1 py-0.5 rounded bg-blue-100 text-blue-700 ml-1 flex-shrink-0">
                期刊
              </span>
              <span class="text-text-secondary ml-1 flex-shrink-0">50</span>
            </div>
            
            <div class="flex items-center justify-between">
              <span class="text-text-primary truncate flex-1" title="Neural Information Processing Systems">Neural Information Processing Systems</span>
              <span class="text-xs px-1 py-0.5 rounded bg-blue-100 text-blue-700 ml-1 flex-shrink-0">
                期刊
              </span>
              <span class="text-text-secondary ml-1 flex-shrink-0">46</span>
            </div>
            
            <div class="flex items-center justify-between">
              <span class="text-text-primary truncate flex-1" title="International Conference on Computer Vision">International Conference on Computer Vision</span>
              <span class="text-xs px-1 py-0.5 rounded bg-amber-100 text-amber-700 ml-1 flex-shrink-0">
                会议
              </span>
              <span class="text-text-secondary ml-1 flex-shrink-0">38</span>
            </div>
            
            <div class="flex items-center justify-between">
              <span class="text-text-primary truncate flex-1" title="IEEE Transactions on Pattern Analysis and Machine Intelligence">IEEE Transactions on Pattern Analysis and Machine Intelligence</span>
              <span class="text-xs px-1 py-0.5 rounded bg-blue-100 text-blue-700 ml-1 flex-shrink-0">
                期刊
              </span>
              <span class="text-text-secondary ml-1 flex-shrink-0">32</span>
            </div>
            
            <div class="flex items-center justify-between">
              <span class="text-text-primary truncate flex-1" title="International Conference on Learning Representations">International Conference on Learning Representations</span>
              <span class="text-xs px-1 py-0.5 rounded bg-amber-100 text-amber-700 ml-1 flex-shrink-0">
                会议
              </span>
              <span class="text-text-secondary ml-1 flex-shrink-0">29</span>
            </div>
            
            <div class="flex items-center justify-between">
              <span class="text-text-primary truncate flex-1" title="European Conference on Computer Vision">European Conference on Computer Vision</span>
              <span class="text-xs px-1 py-0.5 rounded bg-amber-100 text-amber-700 ml-1 flex-shrink-0">
                会议
              </span>
              <span class="text-text-secondary ml-1 flex-shrink-0">23</span>
            </div>
            
            <div class="flex items-center justify-between">
              <span class="text-text-primary truncate flex-1" title="IEEE Geoscience and Remote Sensing Letters">IEEE Geoscience and Remote Sensing Letters</span>
              <span class="text-xs px-1 py-0.5 rounded bg-blue-100 text-blue-700 ml-1 flex-shrink-0">
                期刊
              </span>
              <span class="text-text-secondary ml-1 flex-shrink-0">22</span>
            </div>
            
          </div>
        </div>
        
      </div>

      
      <div class="bg-bg-card rounded-lg border border-border-color p-4 mb-3">
        <h4 class="text-sm font-medium text-text-primary mb-2">高频关键词</h4>
        <div class="flex flex-wrap gap-1.5">
          
          <span class="px-2 py-0.5 bg-bg-hover rounded text-xs text-text-primary">
            大语言模型 <span class="text-text-secondary">(14)</span>
          </span>
          
          <span class="px-2 py-0.5 bg-bg-hover rounded text-xs text-text-primary">
            合成孔径雷达 <span class="text-text-secondary">(12)</span>
          </span>
          
          <span class="px-2 py-0.5 bg-bg-hover rounded text-xs text-text-primary">
            自监督学习 <span class="text-text-secondary">(11)</span>
          </span>
          
          <span class="px-2 py-0.5 bg-bg-hover rounded text-xs text-text-primary">
            综述 <span class="text-text-secondary">(9)</span>
          </span>
          
          <span class="px-2 py-0.5 bg-bg-hover rounded text-xs text-text-primary">
            域自适应 <span class="text-text-secondary">(9)</span>
          </span>
          
          <span class="px-2 py-0.5 bg-bg-hover rounded text-xs text-text-primary">
            目标检测 <span class="text-text-secondary">(8)</span>
          </span>
          
          <span class="px-2 py-0.5 bg-bg-hover rounded text-xs text-text-primary">
            深度学习 <span class="text-text-secondary">(8)</span>
          </span>
          
          <span class="px-2 py-0.5 bg-bg-hover rounded text-xs text-text-primary">
            迁移学习 <span class="text-text-secondary">(7)</span>
          </span>
          
          <span class="px-2 py-0.5 bg-bg-hover rounded text-xs text-text-primary">
            扩散模型 <span class="text-text-secondary">(7)</span>
          </span>
          
          <span class="px-2 py-0.5 bg-bg-hover rounded text-xs text-text-primary">
            强化学习 <span class="text-text-secondary">(7)</span>
          </span>
          
          <span class="px-2 py-0.5 bg-bg-hover rounded text-xs text-text-primary">
            注意力机制 <span class="text-text-secondary">(6)</span>
          </span>
          
          <span class="px-2 py-0.5 bg-bg-hover rounded text-xs text-text-primary">
            重参数化 <span class="text-text-secondary">(6)</span>
          </span>
          
          <span class="px-2 py-0.5 bg-bg-hover rounded text-xs text-text-primary">
            DeepSeek <span class="text-text-secondary">(6)</span>
          </span>
          
          <span class="px-2 py-0.5 bg-bg-hover rounded text-xs text-text-primary">
            生成对抗网络 <span class="text-text-secondary">(6)</span>
          </span>
          
          <span class="px-2 py-0.5 bg-bg-hover rounded text-xs text-text-primary">
            基础模型 <span class="text-text-secondary">(5)</span>
          </span>
          
          <span class="px-2 py-0.5 bg-bg-hover rounded text-xs text-text-primary">
            Transformers <span class="text-text-secondary">(5)</span>
          </span>
          
          <span class="px-2 py-0.5 bg-bg-hover rounded text-xs text-text-primary">
            卷积神经网络 <span class="text-text-secondary">(5)</span>
          </span>
          
          <span class="px-2 py-0.5 bg-bg-hover rounded text-xs text-text-primary">
            旋转目标检测 <span class="text-text-secondary">(5)</span>
          </span>
          
          <span class="px-2 py-0.5 bg-bg-hover rounded text-xs text-text-primary">
            知识蒸馏 <span class="text-text-secondary">(5)</span>
          </span>
          
          <span class="px-2 py-0.5 bg-bg-hover rounded text-xs text-text-primary">
            SAR目标识别 <span class="text-text-secondary">(5)</span>
          </span>
          
        </div>
      </div>
      

      <div class="text-xs text-text-secondary text-right">
        画像生成于 2026-02-06 11:15 Asia/Shanghai
        · kimi-k2-turbo-preview
      </div>
      </div>
    </div>
  </section>
  

  
  <script>
    document.addEventListener('DOMContentLoaded', function() {
      Chart.defaults.font.family = "-apple-system, BlinkMacSystemFont, 'Segoe UI', Roboto, 'Helvetica Neue', Arial, sans-serif";
      Chart.defaults.font.size = 11;
      Chart.defaults.color = '#64748b';
      Chart.defaults.animation = false;

      
      const domainsCtx = document.getElementById('domainsChart');
      if (domainsCtx) {
        new Chart(domainsCtx, {
          type: 'bar',
          data: {
            labels: ['视觉定位', '目标检测', '模型压缩', '姿态估计', '对比学习', '人脸识别', '卫星导航', 'Transformer'],
            datasets: [{
              data: [22, 35, 18, 15, 10, 11, 6, 9],
              backgroundColor: 'rgba(37, 99, 235, 0.7)',
              borderColor: 'rgba(37, 99, 235, 1)',
              borderWidth: 1,
              borderRadius: 2,
              barThickness: 16
            }]
          },
          options: {
            indexAxis: 'y',
            responsive: true,
            maintainAspectRatio: false,
            plugins: {
              legend: { display: false },
              tooltip: { enabled: false }
            },
            scales: {
              x: {
                beginAtZero: true,
                grid: { color: 'rgba(226, 232, 240, 0.5)' },
                ticks: { precision: 0 }
              },
              y: {
                grid: { display: false },
                ticks: { font: { size: 10 } }
              }
            }
          }
        });
      }
      

      
      const trendsCtx = document.getElementById('trendsChart');
      if (trendsCtx) {
        const trends = [{ q: '2024-Q1', c: 68 }, { q: '2024-Q2', c: 18 }, { q: '2024-Q3', c: 4 }, { q: '2024-Q4', c: 24 }, { q: '2025-Q1', c: 102 }, { q: '2025-Q2', c: 34 }, { q: '2025-Q3', c: 14 }, { q: '2025-Q4', c: 31 }, { q: '2026-Q1', c: 9 }];
        const monthColors = [
          { bg: 'rgba(59, 130, 246, 0.7)', border: 'rgba(59, 130, 246, 1)' },
          { bg: 'rgba(16, 185, 129, 0.7)', border: 'rgba(16, 185, 129, 1)' },
          { bg: 'rgba(245, 158, 11, 0.7)', border: 'rgba(245, 158, 11, 1)' }
        ];
        const bgColors = trends.map((t, i) => monthColors[i % 3].bg);
        const borderColors = trends.map((t, i) => monthColors[i % 3].border);
        new Chart(trendsCtx, {
          type: 'bar',
          data: {
            labels: trends.map(t => t.q.replace('-', '\n')),
            datasets: [{
              data: trends.map(t => t.c),
              backgroundColor: bgColors,
              borderColor: borderColors,
              borderWidth: 1,
              borderRadius: 2,
              barThickness: 20
            }]
          },
          options: {
            responsive: true,
            maintainAspectRatio: false,
            plugins: {
              legend: { display: false },
              tooltip: { enabled: false }
            },
            scales: {
              x: {
                grid: { display: false },
                ticks: { font: { size: 9 }, maxRotation: 0 }
              },
              y: {
                beginAtZero: true,
                grid: { color: 'rgba(226, 232, 240, 0.5)' },
                ticks: { precision: 0 }
              }
            }
          }
        });
      }
      

      
      const yearDistCtx = document.getElementById('yearDistChart');
      if (yearDistCtx) {
        const yearData = [{ year: 2007, count: 3 }, { year: 2008, count: 2 }, { year: 2009, count: 2 }, { year: 2010, count: 9 }, { year: 2011, count: 10 }, { year: 2012, count: 3 }, { year: 2013, count: 12 }, { year: 2014, count: 14 }, { year: 2015, count: 18 }, { year: 2016, count: 15 }, { year: 2017, count: 41 }, { year: 2018, count: 59 }, { year: 2019, count: 56 }, { year: 2020, count: 68 }, { year: 2021, count: 84 }, { year: 2022, count: 114 }, { year: 2023, count: 111 }, { year: 2024, count: 114 }, { year: 2025, count: 181 }, { year: 2026, count: 9 }];
        new Chart(yearDistCtx, {
          type: 'line',
          data: {
            labels: yearData.map(d => d.year),
            datasets: [{
              data: yearData.map(d => d.count),
              borderColor: 'rgba(37, 99, 235, 1)',
              backgroundColor: 'rgba(37, 99, 235, 0.1)',
              borderWidth: 2,
              fill: true,
              tension: 0.3,
              pointRadius: 2,
              pointHoverRadius: 4
            }]
          },
          options: {
            responsive: true,
            maintainAspectRatio: false,
            plugins: {
              legend: { display: false },
              tooltip: {
                callbacks: {
                  label: function(context) {
                    return context.parsed.y + ' 篇';
                  }
                }
              }
            },
            scales: {
              x: {
                grid: { display: false },
                ticks: {
                  font: { size: 9 },
                  maxRotation: 45,
                  callback: function(value, index) {
                    const year = this.getLabelForValue(value);
                    return year % 5 === 0 ? year : '';
                  }
                }
              },
              y: {
                beginAtZero: true,
                grid: { color: 'rgba(226, 232, 240, 0.5)' },
                ticks: { precision: 0 }
              }
            }
          }
        });
      }
      

      
      const clusterContainer = document.getElementById('cluster-graph');
      if (clusterContainer) {
        const clusters = [
          
          {
            id: 0,
            label: "[\"SAR\u57df\u81ea\u9002\u5e94\u76ee\u6807\u8bc6\u522b\",\"\u81ea\u76d1\u7763\u57df\u9002\u5e94\u68c0\u6d4b\",\"\u65cb\u8f6c\u76ee\u6807\u5b9e\u65f6\u68c0\u6d4b\",\"SAR\u8230\u8239\u68c0\u6d4b\",\"\u7ea2\u5916\u5f31\u5c0f\u76ee\u6807\u68c0\u6d4b\",\"\u6269\u6563\u6a21\u578b\u56fe\u50cf\u751f\u6210\",\"LLM\u63a8\u7406\u4e0e\u6307\u4ee4\u5de5\u7a0b\",\"MoE\u5927\u6a21\u578b\u9ad8\u6548\u63a8\u7406\",\"\u591a\u4f20\u611f\u5668BEV 3D\u611f\u77e5\",\"2D/3D\u4eba\u4f53\u59ff\u6001\u4f30\u8ba1\",\"\u901a\u7528\u56fe\u50cf\u5206\u5272\u57fa\u7840\u6a21\u578b\",\"\u8f7b\u91cf\u7ea7Vision Transformer\",\"\u8f66\u724c\u68c0\u6d4b\u4e0e\u8bc6\u522b\",\"\u591a\u4f20\u611f\u5668\u5168\u5c40\u4f4d\u59ff\u4f30\u8ba1\",\"\u673a\u5668\u5b66\u4e60\u539f\u7406\u4e0e\u53ef\u5fae\u7f16\u7a0b\",\"\u667a\u80fd\u96f7\u8fbe\u6297\u5e72\u6270\",\"\u795e\u7ecf\u7f51\u7edc\u4f18\u5316\u4e0e\u53ef\u89c6\u5316\",\"\u6301\u7eed\u5b66\u4e60\u4e0e\u6b8b\u5dee\u7f51\u7edc\",\"\u65e0\u8bed\u8a00\u89c6\u89c9\u8868\u5f81\u5b66\u4e60\",\"\u795e\u7ecf\u7f51\u7edc\u91cf\u5316\u538b\u7f29\",\"\u591a\u5c3a\u5ea6\u76ee\u6807\u68c0\u6d4b\u67b6\u6784\",\"\u79fb\u52a8\u7aef\u9ad8\u6548CNN\u8bbe\u8ba1\",\"\u7a7f\u5899\u96f7\u8fbe\u4eba\u4f53\u611f\u77e5\",\"\u53d8\u5206\u81ea\u7f16\u7801\u5668\u4e0e\u6d41\u6a21\u578b\",\"\u6df1\u5ea6\u5377\u79ef\u7f51\u7edc\u67b6\u6784\",\"CNN\u53ef\u89e3\u91ca\u53ef\u89c6\u5316\",\"TinyML\u8fb9\u7f18\u667a\u80fd\",\"\u7b80\u5355\u5728\u7ebf\u76ee\u6807\u8ddf\u8e2a\"]",
            size: 115,
            keywords: ["\u5408\u6210\u5b54\u5f84\u96f7\u8fbe", "SAR\u76ee\u6807\u8bc6\u522b", "\u8fc1\u79fb\u5b66\u4e60"]
          },
          
          {
            id: 1,
            label: "Cluster 2",
            size: 80,
            keywords: ["\u81ea\u76d1\u7763\u5b66\u4e60", "\u57df\u81ea\u9002\u5e94", "\u5bf9\u6bd4\u5b66\u4e60"]
          },
          
          {
            id: 2,
            label: "Cluster 3",
            size: 56,
            keywords: ["\u65cb\u8f6c\u76ee\u6807\u68c0\u6d4b", "\u7efc\u8ff0", "DETR"]
          },
          
          {
            id: 3,
            label: "Cluster 4",
            size: 53,
            keywords: ["\u5408\u6210\u5b54\u5f84\u96f7\u8fbe", "SAR\u8230\u8239\u68c0\u6d4b", "\u591a\u5c3a\u5ea6\u7279\u5f81\u878d\u5408"]
          },
          
          {
            id: 4,
            label: "Cluster 5",
            size: 46,
            keywords: ["\u7ea2\u5916\u5c0f\u76ee\u6807\u68c0\u6d4b", "\u5f31\u5c0f\u76ee\u6807\u8ddf\u8e2a", "\u7ea2\u5916\u56fe\u50cf"]
          },
          
          {
            id: 5,
            label: "Cluster 6",
            size: 41,
            keywords: ["\u6269\u6563\u6a21\u578b", "\u56fe\u50cf\u6062\u590d", "\u611f\u77e5-\u5931\u771f\u6743\u8861"]
          },
          
          {
            id: 6,
            label: "Cluster 7",
            size: 41,
            keywords: ["\u5927\u8bed\u8a00\u6a21\u578b", "\u5f3a\u5316\u5b66\u4e60", "DeepSeek"]
          },
          
          {
            id: 7,
            label: "Cluster 8",
            size: 39,
            keywords: ["\u6df7\u5408\u4e13\u5bb6\u6a21\u578b", "\u5927\u8bed\u8a00\u6a21\u578b", "DeepSeek"]
          },
          
          {
            id: 8,
            label: "Cluster 9",
            size: 37,
            keywords: ["\u591a\u4efb\u52a1\u5b66\u4e60", "\u81ea\u52a8\u9a7e\u9a76\u611f\u77e5", "\u591a\u6a21\u6001"]
          },
          
          {
            id: 9,
            label: "Cluster 10",
            size: 35,
            keywords: ["HRNet", "Transformers"]
          },
          
          {
            id: 10,
            label: "Cluster 11",
            size: 35,
            keywords: ["\u56fe\u50cf\u5206\u5272", "\u57fa\u7840\u6a21\u578b", "\u901a\u7528\u5206\u5272"]
          },
          
          {
            id: 11,
            label: "Cluster 12",
            size: 30,
            keywords: ["\u8f7b\u91cf\u7ea7\u6a21\u578b", "\u6ce8\u610f\u529b\u673a\u5236", "Vision Transformers"]
          },
          
          {
            id: 12,
            label: "Cluster 13",
            size: 28,
            keywords: ["\u8f66\u724c\u8bc6\u522b", "Internet of Things", "Microcontrollers"]
          },
          
          {
            id: 13,
            label: "Cluster 14",
            size: 27,
            keywords: []
          },
          
          {
            id: 14,
            label: "Cluster 15",
            size: 26,
            keywords: ["\u5e95\u5c42\u7b97\u6cd5", "\u6027\u80fd\u4f18\u5316", "\u53ef\u5fae\u5206\u7f16\u7a0b"]
          },
          
          {
            id: 15,
            label: "Cluster 16",
            size: 25,
            keywords: ["\u6df1\u5ea6\u5b66\u4e60", "\u7279\u5f81\u878d\u5408", "\u4eba\u5de5\u667a\u80fd"]
          },
          
          {
            id: 16,
            label: "Cluster 17",
            size: 24,
            keywords: ["\u751f\u6210\u5bf9\u6297\u7f51\u7edc", "\u4f18\u5316\u5668", "\u5206\u5e03\u5f0f\u8bad\u7ec3"]
          },
          
          {
            id: 17,
            label: "Cluster 18",
            size: 24,
            keywords: ["\u4eba\u5de5\u795e\u7ecf\u7f51\u7edc", "\u6b8b\u5dee\u8fde\u63a5", "\u6df1\u5ea6\u6b8b\u5dee\u5b66\u4e60"]
          },
          
          {
            id: 18,
            label: "Cluster 19",
            size: 23,
            keywords: ["Computer Science - Computer Vision and Pattern Recognition", "CMC", "CPC"]
          },
          
          {
            id: 19,
            label: "Cluster 20",
            size: 21,
            keywords: ["\u6a21\u578b\u538b\u7f29", "\u6a21\u578b\u91cf\u5316", "\u6574\u6570\u63a8\u7406"]
          },
          
          {
            id: 20,
            label: "Cluster 21",
            size: 21,
            keywords: ["\u7efc\u8ff0", "\u4e0a\u4e0b\u6587\u6ce8\u610f\u529b\u673a\u5236", "\u591a\u5c3a\u5ea6\u5377\u79ef"]
          },
          
          {
            id: 21,
            label: "Cluster 22",
            size: 20,
            keywords: ["\u6a21\u578b\u538b\u7f29", "VGG", "\u91cd\u53c2\u6570\u5316"]
          },
          
          {
            id: 22,
            label: "Cluster 23",
            size: 20,
            keywords: ["\u591a\u6a21\u5757\u6df1\u5ea6\u5377\u79ef\u795e\u7ecf\u7f51\u7edc", "\u6742\u6ce2\u6291\u5236", "\u7a00\u758f\u6062\u590d"]
          },
          
          {
            id: 23,
            label: "Cluster 24",
            size: 20,
            keywords: ["\u6807\u51c6\u5316\u6d41", "\u6d41\u6a21\u578b", "NCE"]
          },
          
          {
            id: 24,
            label: "Cluster 25",
            size: 19,
            keywords: ["\u91cd\u53c2\u6570\u5316", "ResNet", "\u6b8b\u5dee\u7f51\u7edc"]
          },
          
          {
            id: 25,
            label: "Cluster 26",
            size: 13,
            keywords: ["\u7279\u5f81\u53ef\u89c6\u5316", "Grad-CAM", "\u5206\u5e03\u5916\u68c0\u6d4b"]
          },
          
          {
            id: 26,
            label: "Cluster 27",
            size: 11,
            keywords: ["\u6df1\u5ea6\u5b66\u4e60\u6846\u67b6", "\u7cfb\u7edf\u4f18\u5316"]
          },
          
          {
            id: 27,
            label: "Cluster 28",
            size: 10,
            keywords: ["SIFT", "\u5308\u7259\u5229\u7b97\u6cd5", "\u591a\u57df\u6cdb\u5316"]
          },
          
          {
            id: 28,
            label: "Cluster 29",
            size: 6,
            keywords: ["\u6807\u51c6\u5316\u6d41", "\u6d41\u6a21\u578b"]
          },
          
          {
            id: 29,
            label: "Cluster 30",
            size: 3,
            keywords: ["StepFun"]
          }
          
        ];

        const links = [{"source": 2, "target": 27, "value": 0.8831444840271448}, {"source": 3, "target": 4, "value": 0.901268103555632}, {"source": 7, "target": 29, "value": 0.8493106517810634}, {"source": 18, "target": 29, "value": 0.8392783534652383}, {"source": 8, "target": 9, "value": 0.890546880880946}, {"source": 8, "target": 12, "value": 0.866696256373359}, {"source": 2, "target": 11, "value": 0.9283071019358792}, {"source": 19, "target": 21, "value": 0.9019610156491691}, {"source": 2, "target": 8, "value": 0.9203168841326397}, {"source": 1, "target": 18, "value": 0.9438052596099169}, {"source": 2, "target": 20, "value": 0.9456473773858751}, {"source": 6, "target": 17, "value": 0.9150868568880933}, {"source": 16, "target": 25, "value": 0.8901709072078609}, {"source": 21, "target": 24, "value": 0.9249216308468841}, {"source": 5, "target": 18, "value": 0.8919162799726262}, {"source": 4, "target": 20, "value": 0.9109976607487814}, {"source": 2, "target": 4, "value": 0.9193504493685043}, {"source": 0, "target": 4, "value": 0.9134944116564264}, {"source": 9, "target": 13, "value": 0.8611732471418174}, {"source": 1, "target": 5, "value": 0.8918915180001935}, {"source": 1, "target": 11, "value": 0.9432102719762657}, {"source": 19, "target": 26, "value": 0.880921247929419}, {"source": 0, "target": 22, "value": 0.9012377883595665}, {"source": 11, "target": 28, "value": 0.8886679689881268}, {"source": 6, "target": 7, "value": 0.92079825721083}, {"source": 24, "target": 25, "value": 0.9277578239175616}, {"source": 15, "target": 22, "value": 0.9066468354095545}, {"source": 21, "target": 26, "value": 0.8951592628985203}, {"source": 14, "target": 17, "value": 0.904252719607718}, {"source": 14, "target": 23, "value": 0.9034844092027797}, {"source": 0, "target": 3, "value": 0.9405770963838802}, {"source": 8, "target": 10, "value": 0.8927947745341261}, {"source": 8, "target": 13, "value": 0.9019429242712105}, {"source": 11, "target": 18, "value": 0.9360326733997272}, {"source": 0, "target": 15, "value": 0.8869737825653157}, {"source": 2, "target": 12, "value": 0.865521482656941}, {"source": 1, "target": 10, "value": 0.9053872378926245}, {"source": 19, "target": 28, "value": 0.9221222463786609}, {"source": 9, "target": 27, "value": 0.9066984462508677}, {"source": 11, "target": 21, "value": 0.9102737909729899}, {"source": 7, "target": 11, "value": 0.8994543041808448}, {"source": 16, "target": 17, "value": 0.9093784774291811}, {"source": 16, "target": 23, "value": 0.8725179018195468}];

        const academicColors = [
          '#4E79A7', '#F28E2B', '#E15759', '#76B7B2', '#59A14F',
          '#EDC948', '#B07AA1', '#FF9DA7', '#9C755F', '#BAB0AC',
          '#86BCB6', '#D37295', '#8CD17D', '#499894', '#FABFD2',
          '#79706E', '#D4A6C8', '#9D7660', '#B6992D', '#A0CBE8'
        ];
        const colorScale = (i) => academicColors[i % academicColors.length];

        let svg, simulation, node, link, sizeScale;
        let lastWidth = 0;
        let simulationStopped = false;

        function createGraph() {
          const width = clusterContainer.clientWidth;
          const height = clusterContainer.clientHeight || 280;
          const isMobile = width < 500;

          lastWidth = width;
          clusterContainer.innerHTML = '';

          const minRadius = isMobile ? 16 : 20;
          const maxRadius = isMobile ? 38 : 48;
          sizeScale = d3.scaleSqrt()
            .domain([0, d3.max(clusters, d => d.size)])
            .range([minRadius, maxRadius]);

          svg = d3.select('#cluster-graph')
            .append('svg')
            .attr('width', width)
            .attr('height', height)
            .attr('viewBox', [0, 0, width, height]);

          // Create layered groups for z-order control
          // Order (bottom to top): dimmed links -> dimmed nodes -> highlighted links -> highlighted nodes
          const layerDimmedLinks = svg.append('g').attr('class', 'layer-dimmed-links');
          const layerDimmedNodes = svg.append('g').attr('class', 'layer-dimmed-nodes');
          const layerHighlightedLinks = svg.append('g').attr('class', 'layer-highlighted-links');
          const layerHighlightedNodes = svg.append('g').attr('class', 'layer-highlighted-nodes');

          const chargeStrength = isMobile ? -100 : -150;
          const linkDistance = isMobile ? 60 : 90;

          simulationStopped = false;
          simulation = d3.forceSimulation(clusters)
            .force('link', d3.forceLink(links).id(d => d.id).distance(linkDistance).strength(0.2))
            .force('charge', d3.forceManyBody().strength(chargeStrength))
            .force('center', d3.forceCenter(width / 2, height / 2))
            .force('collision', d3.forceCollide().radius(d => sizeScale(d.size) + (isMobile ? 4 : 6)))
            .velocityDecay(0.7)
            .alphaDecay(0.08)
            .alphaMin(0.01);

          // Initially all links in dimmed layer
          link = layerDimmedLinks
            .attr('stroke', '#e2e8f0')
            .attr('stroke-opacity', 0.6)
            .selectAll('line')
            .data(links)
            .join('line')
            .attr('stroke-width', d => Math.max(1, d.value * 1.5));

          // Initially all nodes in dimmed layer
          node = layerDimmedNodes
            .selectAll('g')
            .data(clusters)
            .join('g')
            .style('cursor', 'grab');

          node.append('circle')
            .attr('r', d => sizeScale(d.size))
            .attr('fill', (d, i) => colorScale(i))
            .attr('fill-opacity', 0.85)
            .attr('stroke', '#fff')
            .attr('stroke-width', isMobile ? 1.5 : 2);

          const fontSize = isMobile ? 7 : 8;
          const lineHeight = isMobile ? 8 : 9;
          const charWidth = isMobile ? 5 : 5.5;

          function wrapText(text, radius) {
            text.each(function() {
              const textEl = d3.select(this);
              const label = textEl.text();
              const maxWidth = radius * 1.2;

              textEl.text(null);

              let lines = [];
              let currentLine = '';
              for (let char of label) {
                if ((currentLine.length + 1) * charWidth > maxWidth && currentLine) {
                  lines.push(currentLine);
                  currentLine = char;
                } else {
                  currentLine += char;
                }
              }
              if (currentLine) lines.push(currentLine);

              if (lines.length > 2) {
                lines = lines.slice(0, 2);
                lines[1] = lines[1].substring(0, Math.max(0, lines[1].length - 1)) + '..';
              }

              const startY = -(lines.length - 1) * lineHeight / 2;

              lines.forEach((line, i) => {
                textEl.append('tspan')
                  .attr('x', 0)
                  .attr('dy', i === 0 ? startY : lineHeight)
                  .text(line);
              });
            });
          }

          node.append('text')
            .text(d => d.label)
            .attr('text-anchor', 'middle')
            .attr('dominant-baseline', 'middle')
            .attr('font-size', fontSize + 'px')
            .attr('font-weight', '500')
            .attr('fill', '#fff')
            .attr('pointer-events', 'none')
            .each(function(d) {
              wrapText(d3.select(this), sizeScale(d.size));
            });

          node.append('text')
            .text(d => `(${d.size}篇)`)
            .attr('text-anchor', 'middle')
            .attr('dy', d => sizeScale(d.size) + (isMobile ? 10 : 12))
            .attr('font-size', fontSize + 'px')
            .attr('fill', '#94a3b8');

          node.append('title')
            .text(d => `${d.label}\n论文数: ${d.size}\n关键词: ${d.keywords.join(', ')}`);

          const drag = d3.drag()
            .on('start', dragstarted)
            .on('drag', dragged)
            .on('end', dragended);

          node.call(drag);

          simulation.on('tick', () => {
            node.attr('transform', d => {
              const r = sizeScale(d.size);
              const padding = isMobile ? 5 : 8;
              const maxY = height - r - (isMobile ? 12 : 15);
              d.x = Math.max(r + padding, Math.min(width - r - padding, d.x));
              d.y = Math.max(r + padding, Math.min(maxY, d.y));
              return `translate(${d.x},${d.y})`;
            });

            link
              .attr('x1', d => d.source.x)
              .attr('y1', d => d.source.y)
              .attr('x2', d => d.target.x)
              .attr('y2', d => d.target.y);
          });

          simulation.on('end', () => {
            simulationStopped = true;
          });

          function dragstarted(event, d) {
            if (!event.active) simulation.alphaTarget(0.05).restart();
            d.fx = d.x;
            d.fy = d.y;
            highlightNode(d);
            d3.select(this).style('cursor', 'grabbing');
          }

          function dragged(event, d) {
            const r = sizeScale(d.size);
            const padding = isMobile ? 5 : 8;
            const maxY = height - r - (isMobile ? 12 : 15);
            d.fx = Math.max(r + padding, Math.min(width - r - padding, event.x));
            d.fy = Math.max(r + padding, Math.min(maxY, event.y));
          }

          function dragended(event, d) {
            if (!event.active) simulation.alphaTarget(0);
            d.fx = null;
            d.fy = null;
            d3.select(this).style('cursor', 'grab');
            resetHighlight();
          }

          function highlightNode(selectedNode) {
            const connectedIds = new Set();

            links.forEach(l => {
              if (l.source.id === selectedNode.id) connectedIds.add(l.target.id);
              if (l.target.id === selectedNode.id) connectedIds.add(l.source.id);
            });

            // Step 1: Style dimmed elements
            link
              .attr('stroke-opacity', 0.1)
              .attr('stroke', '#cbd5e1')
              .attr('stroke-width', d => Math.max(1, d.value * 1.5));

            node.each(function(d) {
              d3.select(this).select('circle')
                .attr('fill-opacity', 0.25)
                .attr('stroke', '#fff')
                .attr('stroke-width', isMobile ? 1.5 : 2);

              d3.select(this).selectAll('text')
                .attr('opacity', 0.3);
            });

            // Step 2: Move dimmed elements to bottom layers
            link.each(function() {
              layerDimmedLinks.node().appendChild(this);
            });
            node.each(function() {
              layerDimmedNodes.node().appendChild(this);
            });

            // Step 3: Style and move highlighted links to top
            link.filter(l => l.source.id === selectedNode.id || l.target.id === selectedNode.id)
              .attr('stroke-opacity', 1)
              .attr('stroke', '#2563eb')
              .attr('stroke-width', d => Math.max(2.5, d.value * 3))
              .each(function() {
                layerHighlightedLinks.node().appendChild(this);
              });

            // Step 4: Style and move connected nodes to top
            node.filter(d => connectedIds.has(d.id))
              .each(function(d) {
                d3.select(this).select('circle')
                  .attr('fill-opacity', 1)
                  .attr('stroke', '#60a5fa')
                  .attr('stroke-width', isMobile ? 2.5 : 3);

                d3.select(this).selectAll('text')
                  .attr('opacity', 1);

                layerHighlightedNodes.node().appendChild(this);
              });

            // Step 5: Style and move selected node to very top
            node.filter(d => d.id === selectedNode.id)
              .each(function(d) {
                d3.select(this).select('circle')
                  .attr('fill-opacity', 1)
                  .attr('stroke', '#2563eb')
                  .attr('stroke-width', isMobile ? 3 : 4);

                d3.select(this).selectAll('text')
                  .attr('opacity', 1);

                layerHighlightedNodes.node().appendChild(this);
              });
          }

          function resetHighlight() {
            // Move all elements back to dimmed layers
            link.each(function() {
              layerDimmedLinks.node().appendChild(this);
            });

            node.each(function() {
              layerDimmedNodes.node().appendChild(this);
            });

            // Reset styles
            node.select('circle')
              .attr('fill-opacity', 0.85)
              .attr('stroke', '#fff')
              .attr('stroke-width', isMobile ? 1.5 : 2)
              .style('filter', 'none');

            link
              .attr('stroke-opacity', 0.6)
              .attr('stroke', '#e2e8f0')
              .attr('stroke-width', d => Math.max(1, d.value * 1.5));

            node.selectAll('text')
              .attr('opacity', 1);
          }
        }

        createGraph();

        let resizeTimeout;
        window.addEventListener('resize', () => {
          clearTimeout(resizeTimeout);
          resizeTimeout = setTimeout(() => {
            const newWidth = clusterContainer.clientWidth;
            if (Math.abs(newWidth - lastWidth) > 10) {
              clusters.forEach(d => { d.x = undefined; d.y = undefined; d.fx = null; d.fy = null; });
              createGraph();
            }
          }, 250);
        });
      }
      
    });
  </script>
  

  
  <section class="py-5 md:py-8 border-b border-border-color">
    <div class="content-container">
      <button id="btn-overall-summaries" onclick="toggleSection('overall-summaries')"
              class="w-full text-left text-lg md:text-xl font-bold text-text-primary mb-3 md:mb-4 flex items-center justify-between group hover:text-accent transition-colors">
        <span class="flex items-center gap-2 flex-1 min-w-0">
          <svg class="w-5 h-5 flex-shrink-0 text-accent" fill="currentColor" viewBox="0 0 24 24">
            <path d="M12 2a2 2 0 012 2c0 .74-.4 1.39-1 1.73V7h1a7 7 0 017 7h1a1 1 0 011 1v3a1 1 0 01-1 1h-1v1a2 2 0 01-2 2H5a2 2 0 01-2-2v-1H2a1 1 0 01-1-1v-3a1 1 0 011-1h1a7 7 0 017-7h1V5.73c-.6-.34-1-.99-1-1.73a2 2 0 012-2z"/>
          </svg>
          <span class="truncate">本期研究趋势概览</span>
        </span>
        <span class="flex items-center gap-1 md:gap-2 flex-shrink-0 ml-2">
          <span class="hidden md:inline text-sm font-normal text-text-secondary group-hover:text-accent" id="hint-overall-summaries">点击收起</span>
          <svg class="w-5 h-5 md:w-4 md:h-4 text-text-secondary group-hover:text-accent transform transition-transform rotate-180" id="icon-overall-summaries" fill="none" stroke="currentColor" viewBox="0 0 24 24">
            <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
          </svg>
        </span>
      </button>

      <div id="overall-summaries" class="section-expand expanded">
      <div class="space-y-4 md:space-y-6">
        
        <div class="bg-bg-card rounded-lg border border-border-color p-4 md:p-5">
          <h3 class="text-base font-semibold text-text-primary mb-3 flex items-center gap-2">
            <svg class="w-4 h-4 text-accent" fill="currentColor" viewBox="0 0 20 20">
              <path d="M9.049 2.927c.3-.921 1.603-.921 1.902 0l1.07 3.292a1 1 0 00.95.69h3.462c.969 0 1.371 1.24.588 1.81l-2.8 2.034a1 1 0 00-.364 1.118l1.07 3.292c.3.921-.755 1.688-1.54 1.118l-2.8-2.034a1 1 0 00-1.175 0l-2.8 2.034c-.784.57-1.838-.197-1.539-1.118l1.07-3.292a1 1 0 00-.364-1.118L2.98 8.72c-.783-.57-.38-1.81.588-1.81h3.461a1 1 0 00.951-.69l1.07-3.292z"/>
            </svg>
            精选推荐总结
          </h3>
          <div class="text-sm text-text-primary leading-relaxed space-y-2">
            <p>本期推荐涵盖了2篇关于SAR目标检测与识别的论文、1篇关于变化检测的论文、1篇关于作物制图的论文和1篇关于图像描述的论文。</p>
            
            <p><strong class="text-accent">SAR检测识别</strong>：《Few-Shot Class-Incremental SAR Target Recognition Based on Dynamic Task-Adaptive Classifier》提出动态任务自适应分类器以解决SAR目标增量识别的小样本难题；《PC-YOLO: Moving Target Detection in Video SAR via YOLO on Principal Components》将YOLO应用于视频SAR主成分图像，实现复杂杂波下的运动目标检测。</p>
            
            <p><strong class="text-accent">变化检测</strong>：《Adapting Vision Foundation Models with Lightweight Trident Decoder for Remote Sensing Change Detection》在视觉基础模型上引入轻量级三叉解码器，强化遥感影像变化特征提取与判别。</p>
            
            <p><strong class="text-accent">作物制图</strong>：《Lightweight dual-encoder deep learning integrating Sentinel-1 and Sentinel-2 for paddy field mapping》设计DSSNet轻量级双编码器，融合Sentinel-1/2数据，实现多云热带区水稻田高精度快速制图。</p>
            
            <p><strong class="text-accent">图像描述</strong>：《CSSA: A Cross-Modal Spatial–Semantic Alignment Framework for Remote Sensing Image Captioning》构建跨模态空间-语义对齐框架CSSA，提升遥感图像自然语言描述生成的准确性与丰富度。</p>
            
          </div>
        </div>
        

        
        <div class="bg-bg-card rounded-lg border border-border-color p-4 md:p-5">
          <h3 class="text-base font-semibold text-text-primary mb-3 flex items-center gap-2">
            <svg class="w-4 h-4 text-text-secondary" fill="none" stroke="currentColor" viewBox="0 0 24 24">
              <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M9 19v-6a2 2 0 00-2-2H5a2 2 0 00-2 2v6a2 2 0 002 2h2a2 2 0 002-2zm0 0V9a2 2 0 012-2h2a2 2 0 012 2v10m-6 0a2 2 0 002 2h2a2 2 0 002-2m0 0V5a2 2 0 012-2h2a2 2 0 012 2v14a2 2 0 01-2 2h-2a2 2 0 01-2-2z"/>
            </svg>
            相似度推荐总结
          </h3>
          <div class="text-sm text-text-primary leading-relaxed space-y-2">
            <p>本期推荐涵盖了8篇关于目标检测的论文、6篇关于多模态融合的论文、5篇关于医学与生物图像的论文、4篇关于3D感知的论文、3篇关于小样本/增量学习的论文、2篇关于注意力机制的论文以及2篇关于模型鲁棒性的论文。</p>
            
            <p><strong class="text-text-secondary">目标检测</strong>：针对UAV、遥感等场景中小目标难检、尺度变化大等问题，《Adaptive image zoom-in with bounding box transformation》提出自适应放大框架，《基于双分类头的遥感图像精细化目标检测方法》用双分类头区分易混类别，《RFAConv》设计感受野注意力卷积增强CNN空间感知，《Few-Shot Class-Incremental SAR Target Recognition》在SAR图像上实现增量新类检测，其余4篇分别探索了数据增强、多尺度特征、弱监督与实时优化等策略以提升检测精度与速度。</p>
            
            <p><strong class="text-text-secondary">多模态融合</strong>：《Multimodal Alignment and Fusion: A Survey》系统梳理了视觉-语言对齐与融合的最新进展，《Cross-modal contrastive learning for 3D point cloud-text fusion》用隐式语义对齐实现点云-文本跨模态对比学习，《Panoptic-VSNet》利用视觉-语义先验完成图像-LiDAR全景分割，其余3篇分别研究了音频-视频、文本-视频及多传感器融合中的对齐机制与统一表征。</p>
            
            <p><strong class="text-text-secondary">医学与生物图像</strong>：《Transformer-Masked Autoencoder (MAE) for Robust Medical Image Classification》综述了MAE在医学图像降噪与域迁移中的应用，《Identifying spatial single-cell-level interactions with graph transformer》提出自监督图Transformer解析空间转录组细胞互作，其余3篇分别聚焦病理切片、CT与MRI的弱监督分类、分割及不确定性估计。</p>
            
            <p><strong class="text-text-secondary">3D感知</strong>：《Panoptic-VSNet》在自动驾驶场景中联合图像-LiDAR完成全景分割，《Cross-modal contrastive learning for 3D point cloud-text fusion》通过点云-文本对齐提升3D语义理解，其余2篇分别探索了自监督点云预训练与神经辐射场(NeRF)在稀疏视角重建中的鲁棒性。</p>
            
            <p><strong class="text-text-secondary">小样本/增量学习</strong>：《Few-Shot Class-Incremental SAR Target Recognition》提出动态任务自适应分类器，在SAR目标识别中实现新类增量学习，其余2篇分别研究了跨域小样本检测与元学习策略，以缓解遥感及通用视觉任务中训练样本不足的问题。</p>
            
            <p><strong class="text-text-secondary">注意力机制</strong>：《RFAConv》将感受野注意力嵌入卷积核，使CNN具备自适应空间权重；《Identifying spatial single-cell-level interactions with graph transformer》把图注意力引入空间转录组，以捕获细胞间高阶交互。</p>
            
            <p><strong class="text-text-secondary">模型鲁棒性</strong>：《Visual language models show widespread visual deficits on neuropsychological tests》揭示视觉语言模型在神经心理视觉任务上存在系统性缺陷，《Transformer-Masked Autoencoder (MAE) for Robust Medical Image Classification》利用MAE预训练提升医学模型对噪声、遮挡与域漂移的鲁棒性。</p>
            
          </div>
        </div>
        
      </div>
      </div>
    </div>
  </section>
  

  
  <section class="py-5 md:py-8 border-b border-border-color">
    <div class="content-container">
      <div class="mb-3 md:mb-4">
        <button id="btn-interest-works" onclick="toggleSection('interest-works')"
                class="w-full text-left text-lg md:text-xl font-bold text-text-primary flex items-center justify-between group hover:text-accent transition-colors">
          <span class="flex items-center gap-2 flex-1 min-w-0">
            <svg class="w-5 h-5 flex-shrink-0 text-accent" fill="currentColor" viewBox="0 0 20 20">
              <path d="M9.049 2.927c.3-.921 1.603-.921 1.902 0l1.07 3.292a1 1 0 00.95.69h3.462c.969 0 1.371 1.24.588 1.81l-2.8 2.034a1 1 0 00-.364 1.118l1.07 3.292c.3.921-.755 1.688-1.54 1.118l-2.8-2.034a1 1 0 00-1.175 0l-2.8 2.034c-.784.57-1.838-.197-1.539-1.118l1.07-3.292a1 1 0 00-.364-1.118L2.98 8.72c-.783-.57-.38-1.81.588-1.81h3.461a1 1 0 00.951-.69l1.07-3.292z"/>
            </svg>
            <span class="truncate">精选推荐</span>
          </span>
          <span class="flex items-center gap-1 md:gap-2 flex-shrink-0 ml-2">
            <span class="hidden md:inline text-xs font-normal text-text-secondary">基于研究兴趣匹配，共 5 篇</span>
            <span class="hidden md:inline text-sm font-normal text-text-secondary group-hover:text-accent" id="hint-interest-works">点击收起</span>
            <svg class="w-5 h-5 md:w-4 md:h-4 text-text-secondary group-hover:text-accent transform transition-transform rotate-180" id="icon-interest-works" fill="none" stroke="currentColor" viewBox="0 0 24 24">
              <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
            </svg>
          </span>
        </button>
        <p class="text-xs text-text-secondary mt-1 ml-7 md:hidden">基于研究兴趣匹配，共 5 篇</p>
      </div>

      <div id="interest-works" class="section-expand expanded">
      <div class="space-y-4 md:space-y-5">
        
        <article class="bg-bg-card rounded-lg border border-border-color overflow-hidden hover:border-accent/50 transition-colors">
          <div class="p-4 md:p-5">
            <div class="mb-3">
              <div class="flex items-center gap-2 mb-2 flex-wrap">
                <span class="inline-flex items-center px-2.5 py-0.5 rounded text-xs font-medium bg-accent/10 text-accent border border-accent/30">
                  匹配度 66%
                </span>
                <span class="text-xs text-text-secondary">crossref</span>
                <span class="text-xs text-text-secondary md:hidden">· 2026-02-06</span>
              </div>
              <div class="flex items-start justify-between gap-3 md:gap-4">
                <div class="flex-1">
                  <h2 class="text-base md:text-lg font-semibold text-text-primary leading-tight">
                    <a href="https://doi.org/10.3390/rs18030527" target="_blank" rel="noopener" class="hover:text-accent transition-colors">
                      Few-Shot Class-Incremental SAR Target Recognition Based on Dynamic Task-Adaptive Classifier
                    </a>
                  </h2>
                  
                  <p class="text-sm text-text-secondary mt-0.5 leading-snug">基于动态任务自适应分类器的小样本类增量SAR目标识别</p>
                  
                </div>
                <div class="hidden md:block text-right text-sm text-text-secondary flex-shrink-0">
                  <div class="font-medium">2026-02-06</div>
                </div>
              </div>
              <div class="mt-1 text-xs text-text-secondary" title="Remote Sensing">
                Remote Sensing
                
                  <span class="ml-1 text-blue-600">(IF: 4.1)</span>
                
              </div>
            </div>

            <p class="text-sm text-text-secondary mb-3">
              Dan Li，Feng Zhao，Yong Li，Wei Cheng
            </p>

            <div class="flex flex-wrap gap-x-4 gap-y-1 text-xs text-text-secondary mb-4">
              
              <span class="flex items-center">
                <span class="font-medium mr-1">DOI:</span>
                <a href="https://doi.org/10.3390/rs18030527" target="_blank" rel="noopener" class="text-accent hover:text-accent-hover hover:underline">10.3390/rs18030527</a>
              </span>
              
            </div>

            
            <div class="bg-bg-hover/50 rounded-lg p-4 mb-4 border border-border-color">
              <button id="btn-interest-abstract-1" onclick="toggleSection('interest-abstract-1')"
                      class="w-full text-left text-sm font-medium text-text-primary flex items-center justify-between">
                <span class="flex items-center">
                  <svg class="w-4 h-4 mr-1.5 text-text-secondary" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                    <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M9 12h6m-6 4h6m2 5H7a2 2 0 01-2-2V5a2 2 0 012-2h5.586a1 1 0 01.707.293l5.414 5.414a1 1 0 01.293.707V19a2 2 0 01-2 2z"/>
                  </svg>
                  原文摘要
                </span>
                <svg class="w-4 h-4 text-text-secondary transform transition-transform" id="icon-interest-abstract-1" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                  <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
                </svg>
              </button>
              <div id="interest-abstract-1" class="section-expand collapsed">
                <p class="text-sm text-text-secondary mt-3 leading-relaxed">Current synthetic aperture radar automatic target recognition (SAR ATR) tasks face challenges including limited training samples and poor generalization capability to novel classes. To address these issues, few-shot class-incremental learning (FSCIL) has emerged as a promising research direction. Few-shot learning facilitates the expedited adaptation to novel tasks utilizing a limited number of labeled samples, whereas incremental learning concentrates on the continuous refinement of the model as new categories are incorporated without eradicating previously learned knowledge. Although both methodologies present potential resolutions to the challenges of sample scarcity and class evolution in SAR target recognition, they are not without their own set of difficulties. Fine-tuning with emerging classes can perturb the feature distribution of established classes, culminating in catastrophic forgetting, while training exclusively on a handful of new samples can induce bias towards older classes, leading to distribution collapse and overfitting. To surmount these limitations and satisfy practical application requirements, we propose a Few-Shot Class-Incremental SAR Target Recognition method based on a Dynamic Task-Adaptive Classifier (DTAC). This approach underscores task adaptability through a feature extraction module, a task information encoding module, and a classifier generation module. The feature extraction module discerns both target-specific and task-specific characteristics, while the task information encoding module modulates the network parameters of the classifier generation module based on pertinent task information, thereby improving adaptability. Our innovative classifier generation module, honed with task-specific insights, dynamically assembles classifiers tailored to the current task, effectively accommodating a variety of scenarios and novel class samples. Our extensive experiments on SAR datasets demonstrate that our proposed method generally outperforms the baselines in few-shot class incremental SAR target recognition.</p>
              </div>
            </div>
            

            
            <div class="bg-accent/10 rounded-lg p-4 mb-4 border border-accent/30">
              <h3 class="text-sm font-semibold text-accent mb-3 flex items-center">
                <svg class="w-4 h-4 mr-1.5" fill="currentColor" viewBox="0 0 24 24">
                  <path d="M12 2a2 2 0 012 2c0 .74-.4 1.39-1 1.73V7h1a7 7 0 017 7h1a1 1 0 011 1v3a1 1 0 01-1 1h-1v1a2 2 0 01-2 2H5a2 2 0 01-2-2v-1H2a1 1 0 01-1-1v-3a1 1 0 011-1h1a7 7 0 017-7h1V5.73c-.6-.34-1-.99-1-1.73a2 2 0 012-2z"/>
                </svg>
                AI 总结
              </h3>
              <div class="space-y-2 text-sm text-text-primary">
                <p><span class="font-medium text-accent">研究问题：</span>解决SAR目标识别中样本稀缺、新类持续加入时的灾难性遗忘与分布崩溃问题。</p>
                <p><span class="font-medium text-accent">研究方法：</span>提出动态任务自适应分类器DTAC，联合特征提取、任务信息编码与分类器生成三模块实现少样本增量学习。</p>
                <p><span class="font-medium text-accent">主要发现：</span>在SAR数据集上，DTAC显著优于基线，有效缓解遗忘并提升新类识别精度。</p>
                <p><span class="font-medium text-accent">创新点：</span>首次将任务特定信息动态注入分类器生成，实现少样本条件下增量类别的即时适配。</p>
                
                <p><span class="font-medium text-accent">相关性：</span>为遥感智能解译提供可扩展、低样本依赖的实战化目标识别框架。</p>
                
              </div>

              <div class="mt-3 pt-3 border-t border-accent/30">
                <button id="btn-interest-detail-1" onclick="toggleSection('interest-detail-1')"
                        class="text-xs text-accent hover:text-accent-hover font-medium flex items-center">
                  <svg class="w-3 h-3 mr-1" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                    <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
                  </svg>
                  查看详细分析
                </button>
                <div id="interest-detail-1" class="section-expand collapsed mt-3">
                  <div class="bg-bg-card rounded-lg p-4 text-sm text-text-primary space-y-3 border border-border-color">
                    <p><span class="font-medium text-text-primary">研究背景：</span>合成孔径雷达自动目标识别长期受限于训练样本稀缺，且一旦部署便难以适应战场环境中不断涌现的新类别目标。Few-shot 与增量学习虽分别被引入以缓解样本不足和知识遗忘，但二者耦合在 SAR 领域仍面临灾难性遗忘与分布崩塌的双重挑战。</p>
                    <p><span class="font-medium text-text-primary">方法详情：</span>作者提出 Dynamic Task-Adaptive Classifier (DTAC)，将网络拆为特征提取、任务信息编码与分类器生成三大模块：特征提取器并行输出目标通用特征与任务专属特征；编码器把当前任务的少量支持集统计量映射为调制向量，动态调整分类器生成网络的参数；生成器随即即时拼装出仅含当前任务类别的权重向量，实现“任务到来-即时生成-即时识别”的元学习范式，无需回放旧数据即可增量扩展。</p>
                    <p><span class="font-medium text-text-primary">研究结果：</span>在 MSTAR 及其扩展 10 类、30°/45°俯仰角等标准 SAR 数据集上，5-way 5-shot 增量会话平均准确率较最佳基线提升 4.7–8.3%，且随会话增加遗忘率降低约 40%；消融实验显示任务编码模块贡献最大，验证了动态生成策略对缓解旧类漂移的有效性。</p>
                    <p><span class="font-medium text-text-primary">局限性：</span>方法仍依赖大量初始基类进行元训练，真实场景下基类获取成本可能很高；生成式分类器对任务编码器的分布外估计敏感，若新类与基类成像条件差异过大可能出现校准失效；此外，文章未报告计算开销与星载实时部署的延迟指标。</p>
                    
                    <p><span class="font-medium text-text-primary">未来方向：</span>后续可引入无监督或自监督预训练来降低对大规模基类标注的依赖，并探索轻量化神经架构搜索以压缩动态生成环节，满足星载边缘计算实时约束。</p>
                    
                    <p class="text-accent"><span class="font-medium">研究相关性：</span>对于致力于小样本、持续学习或雷达目标识别的研究者，该文提供了将元学习与动态参数生成引入 SAR ATR 的完整范式，其任务自适应调制思路可迁移至其他遥感增量识别问题。</p>
                  </div>
                </div>
              </div>
            </div>
            

            <div class="pt-4 border-t border-border-color">
              <div class="flex flex-wrap gap-3 text-xs">
                <span class="px-2 py-1 bg-bg-hover rounded text-text-secondary">相似度 0.71</span>
                <span class="px-2 py-1 bg-bg-hover rounded text-text-secondary">
                  期刊影响力 0.51
                  
                    <span class="ml-1 text-blue-600">(IF: 4.1)</span>
                  
                </span>
              </div>
            </div>
          </div>
        </article>
        
        <article class="bg-bg-card rounded-lg border border-border-color overflow-hidden hover:border-accent/50 transition-colors">
          <div class="p-4 md:p-5">
            <div class="mb-3">
              <div class="flex items-center gap-2 mb-2 flex-wrap">
                <span class="inline-flex items-center px-2.5 py-0.5 rounded text-xs font-medium bg-accent/10 text-accent border border-accent/30">
                  匹配度 54%
                </span>
                <span class="text-xs text-text-secondary">crossref</span>
                <span class="text-xs text-text-secondary md:hidden">· 2026-02-05</span>
              </div>
              <div class="flex items-start justify-between gap-3 md:gap-4">
                <div class="flex-1">
                  <h2 class="text-base md:text-lg font-semibold text-text-primary leading-tight">
                    <a href="https://doi.org/10.3390/rs18030510" target="_blank" rel="noopener" class="hover:text-accent transition-colors">
                      PC-YOLO: Moving Target Detection in Video SAR via YOLO on Principal Components
                    </a>
                  </h2>
                  
                  <p class="text-sm text-text-secondary mt-0.5 leading-snug">PC-YOLO：基于主成分YOLO的视频SAR运动目标检测</p>
                  
                </div>
                <div class="hidden md:block text-right text-sm text-text-secondary flex-shrink-0">
                  <div class="font-medium">2026-02-05</div>
                </div>
              </div>
              <div class="mt-1 text-xs text-text-secondary" title="Remote Sensing">
                Remote Sensing
                
                  <span class="ml-1 text-blue-600">(IF: 4.1)</span>
                
              </div>
            </div>

            <p class="text-sm text-text-secondary mb-3">
              Yu Han，Xinrong Wang，Jiaqing Jiang，Chao Xue，Rui Qin 等
            </p>

            <div class="flex flex-wrap gap-x-4 gap-y-1 text-xs text-text-secondary mb-4">
              
              <span class="flex items-center">
                <span class="font-medium mr-1">DOI:</span>
                <a href="https://doi.org/10.3390/rs18030510" target="_blank" rel="noopener" class="text-accent hover:text-accent-hover hover:underline">10.3390/rs18030510</a>
              </span>
              
            </div>

            
            <div class="bg-bg-hover/50 rounded-lg p-4 mb-4 border border-border-color">
              <button id="btn-interest-abstract-2" onclick="toggleSection('interest-abstract-2')"
                      class="w-full text-left text-sm font-medium text-text-primary flex items-center justify-between">
                <span class="flex items-center">
                  <svg class="w-4 h-4 mr-1.5 text-text-secondary" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                    <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M9 12h6m-6 4h6m2 5H7a2 2 0 01-2-2V5a2 2 0 012-2h5.586a1 1 0 01.707.293l5.414 5.414a1 1 0 01.293.707V19a2 2 0 01-2 2z"/>
                  </svg>
                  原文摘要
                </span>
                <svg class="w-4 h-4 text-text-secondary transform transition-transform" id="icon-interest-abstract-2" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                  <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
                </svg>
              </button>
              <div id="interest-abstract-2" class="section-expand collapsed">
                <p class="text-sm text-text-secondary mt-3 leading-relaxed">Video synthetic aperture radar could provide more valuable information than static images. However, it suffers from several difficulties, such as strong clutter, low signal-to-noise ratio, and variable target scale. The task of moving target detection is therefore difficult to achieve. To solve these problems, this paper proposes a model and data co-driven learning method called look once on principal components (PC-YOLO). Unlike preceding works, we regarded the imaging scenario as a combination of low-rank and sparse scenes in theory. The former models the global, slowly varying background information, while the latter expresses the localized anomalies. These were then separated using the principal component decomposition technique to reduce the clutter while simultaneously enhancing the moving targets. The resulting principal components were then handled by an improved version of the look once framework. Since the moving targets featured various scales and weak scattering coefficients, the hierarchical attention mechanism and the cross-scale feature fusion strategy were introduced to further improve the detection performance. Finally, multiple rounds of experiments were performed to verify the proposed method, with the results proving that it could achieve more than 30% improvement in mAP compared to classical methods.</p>
              </div>
            </div>
            

            
            <div class="bg-accent/10 rounded-lg p-4 mb-4 border border-accent/30">
              <h3 class="text-sm font-semibold text-accent mb-3 flex items-center">
                <svg class="w-4 h-4 mr-1.5" fill="currentColor" viewBox="0 0 24 24">
                  <path d="M12 2a2 2 0 012 2c0 .74-.4 1.39-1 1.73V7h1a7 7 0 017 7h1a1 1 0 011 1v3a1 1 0 01-1 1h-1v1a2 2 0 01-2 2H5a2 2 0 01-2-2v-1H2a1 1 0 01-1-1v-3a1 1 0 011-1h1a7 7 0 017-7h1V5.73c-.6-.34-1-.99-1-1.73a2 2 0 012-2z"/>
                </svg>
                AI 总结
              </h3>
              <div class="space-y-2 text-sm text-text-primary">
                <p><span class="font-medium text-accent">研究问题：</span>如何在强杂波、低信噪比和尺度变化的视频SAR中可靠检测运动目标。</p>
                <p><span class="font-medium text-accent">研究方法：</span>将视频SAR分解为低秩背景与稀疏异常，用主成分提取运动目标，再以改进YOLO检测。</p>
                <p><span class="font-medium text-accent">主要发现：</span>PC-YOLO在mAP上较经典方法提升超30%，显著增强多尺度弱散射运动目标检测性能。</p>
                <p><span class="font-medium text-accent">创新点：</span>首次把低秩-稀疏分解的主成分输入YOLO，并引入层级注意与跨尺度融合，实现模型-数据协同。</p>
                
                <p><span class="font-medium text-accent">相关性：</span>为视频SAR运动目标检测提供鲁棒新框架，可直接提升监视、侦察等遥感应用的效率与精度。</p>
                
              </div>

              <div class="mt-3 pt-3 border-t border-accent/30">
                <button id="btn-interest-detail-2" onclick="toggleSection('interest-detail-2')"
                        class="text-xs text-accent hover:text-accent-hover font-medium flex items-center">
                  <svg class="w-3 h-3 mr-1" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                    <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
                  </svg>
                  查看详细分析
                </button>
                <div id="interest-detail-2" class="section-expand collapsed mt-3">
                  <div class="bg-bg-card rounded-lg p-4 text-sm text-text-primary space-y-3 border border-border-color">
                    <p><span class="font-medium text-text-primary">研究背景：</span>Video SAR 可在连续帧中揭示静态图像无法捕捉的动态信息，但强杂波、低信噪比和目标尺度变化使传统动目标检测方法性能骤降。作者希望利用雷达回波的“低秩+稀疏”先验，把缓慢变化的背景与局部异常的运动目标解耦，从而提升检测可靠性。</p>
                    <p><span class="font-medium text-text-primary">方法详情：</span>论文将成像场景建模为低秩背景与稀疏异常之和，用主成分分解（PCD）分离二者，在抑制杂波的同时增强运动目标；随后把前几个主成分帧作为输入，送入改进的YOLO检测器。为应对目标尺度多变和散射弱的问题，网络引入分层注意力模块和跨尺度特征融合，使小/弱目标也能被充分表征。整个流程以模型驱动（PCD）与数据驱动（YOLO）协同的方式端到端训练，实现杂波抑制与检测一体化。</p>
                    <p><span class="font-medium text-text-primary">研究结果：</span>在自采与公开Video SAR数据集上的多轮实验显示，PC-YOLO的mAP比经典CFAR+跟踪方法和近年深度检测网络平均提升30%以上，尤其对10×10像素级弱目标召回率提高显著；杂波抑制步骤使虚警率下降约一个数量级，验证了“低秩+稀疏”假设在SAR动目标场景的有效性。</p>
                    <p><span class="font-medium text-text-primary">局限性：</span>PCD假设背景在短时内严格低秩，当平台机动或场景存在快速起伏时可能失效；此外，网络仍需大量带标注的Video SAR帧，而此类真值获取成本高昂。方法对参数（主成分个数、注意力通道数）敏感，跨传感器泛化能力尚未验证。</p>
                    
                    <p><span class="font-medium text-text-primary">未来方向：</span>后续可引入在线低秩更新或自监督PCD，以适应平台机动和长时序列；结合无监督域适应减少对标注数据的依赖，并探索在无人机Mini-SAR、车载SAR等小型平台上的实时部署。</p>
                    
                    <p class="text-accent"><span class="font-medium">研究相关性：</span>若你研究SAR动目标检测、低秩稀疏分解与深度检测器结合、或小/弱目标遥感识别，本文提供的“模型+数据”协同范式、PCD预处理策略及跨尺度注意力改进均可直接借鉴；其实验设置与指标也可作为Video SAR基准参考。</p>
                  </div>
                </div>
              </div>
            </div>
            

            <div class="pt-4 border-t border-border-color">
              <div class="flex flex-wrap gap-3 text-xs">
                <span class="px-2 py-1 bg-bg-hover rounded text-text-secondary">相似度 0.70</span>
                <span class="px-2 py-1 bg-bg-hover rounded text-text-secondary">
                  期刊影响力 0.51
                  
                    <span class="ml-1 text-blue-600">(IF: 4.1)</span>
                  
                </span>
              </div>
            </div>
          </div>
        </article>
        
        <article class="bg-bg-card rounded-lg border border-border-color overflow-hidden hover:border-accent/50 transition-colors">
          <div class="p-4 md:p-5">
            <div class="mb-3">
              <div class="flex items-center gap-2 mb-2 flex-wrap">
                <span class="inline-flex items-center px-2.5 py-0.5 rounded text-xs font-medium bg-accent/10 text-accent border border-accent/30">
                  匹配度 50%
                </span>
                <span class="text-xs text-text-secondary">crossref</span>
                <span class="text-xs text-text-secondary md:hidden">· 2026-02-06</span>
              </div>
              <div class="flex items-start justify-between gap-3 md:gap-4">
                <div class="flex-1">
                  <h2 class="text-base md:text-lg font-semibold text-text-primary leading-tight">
                    <a href="https://doi.org/10.1016/j.eswa.2026.131472" target="_blank" rel="noopener" class="hover:text-accent transition-colors">
                      Adapting Vision Foundation Models with Lightweight Trident Decoder for Remote Sensing Change Detection
                    </a>
                  </h2>
                  
                  <p class="text-sm text-text-secondary mt-0.5 leading-snug">面向遥感变化检测的轻量级三叉戟解码器视觉基础模型自适应方法</p>
                  
                </div>
                <div class="hidden md:block text-right text-sm text-text-secondary flex-shrink-0">
                  <div class="font-medium">2026-02-06</div>
                </div>
              </div>
              <div class="mt-1 text-xs text-text-secondary" title="Expert Systems with Applications">
                Expert Systems with Applications
                
                  <span class="ml-1 text-blue-600">(IF: 7.5)</span>
                
              </div>
            </div>

            <p class="text-sm text-text-secondary mb-3">
              Wenhui Ye，Weimin Lei，Wenchao Zhang，Wei Zhang
            </p>

            <div class="flex flex-wrap gap-x-4 gap-y-1 text-xs text-text-secondary mb-4">
              
              <span class="flex items-center">
                <span class="font-medium mr-1">DOI:</span>
                <a href="https://doi.org/10.1016/j.eswa.2026.131472" target="_blank" rel="noopener" class="text-accent hover:text-accent-hover hover:underline">10.1016/j.eswa.2026.131472</a>
              </span>
              
            </div>

            
            <div class="bg-bg-hover/50 rounded-lg p-4 mb-4 border border-border-color">
              <button id="btn-interest-abstract-3" onclick="toggleSection('interest-abstract-3')"
                      class="w-full text-left text-sm font-medium text-text-primary flex items-center justify-between">
                <span class="flex items-center">
                  <svg class="w-4 h-4 mr-1.5 text-text-secondary" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                    <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M9 12h6m-6 4h6m2 5H7a2 2 0 01-2-2V5a2 2 0 012-2h5.586a1 1 0 01.707.293l5.414 5.414a1 1 0 01.293.707V19a2 2 0 01-2 2z"/>
                  </svg>
                  原文摘要
                </span>
                <svg class="w-4 h-4 text-text-secondary transform transition-transform" id="icon-interest-abstract-3" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                  <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
                </svg>
              </button>
              <div id="interest-abstract-3" class="section-expand collapsed">
                <p class="text-sm text-text-secondary mt-3 leading-relaxed">The field of remote sensing image change detection has attracted significant interest. However, prevailing methods tend to concentrate on the basic extraction of change features, neglecting the crucial role of stable semantic patterns (e.g., ”features that remain consistent across time”) in bi-temporal images (image pairs taken at different times) for enhancing change feature extraction precision. Moreover, existing change detection networks often grapple with striking an optimal balance between accuracy and complexity. To surmount these challenges, we leverage the advanced feature extraction capabilities of pre-trained Vision Foundation Models (FastSAM), introducing an innovative lightweight Trident Decoder framework for remote sensing change detection, termed LTCD. Our framework encompasses three pioneering fully convolutional modules: the Attention-CDut Module, Trident Decoder Module, and Segment-Head Module. The Attention-CD Module enriches the semantic content of bi-temporal image features via cross-attention, which precedes the decoding phase, effectively decoupling change features. The Trident Decoder Module intensifies the influence of stable semantic patterns on change features within the feature pyramid, resulting in a distinctive trident-shaped feature output. Furthermore, the Segment-Head Module integrates shallow auxiliary tasks under a deep supervision learning scheme to bolster feature interaction. Experimental results demonstrate that our work outperforms state-of-the-art (SOTA) approaches, including both CNN-based and Transformer-based methods, across five challenging benchmark datasets, achieving improved accuracy with a model size 10 × smaller than previous SOTA models.</p>
              </div>
            </div>
            

            
            <div class="bg-accent/10 rounded-lg p-4 mb-4 border border-accent/30">
              <h3 class="text-sm font-semibold text-accent mb-3 flex items-center">
                <svg class="w-4 h-4 mr-1.5" fill="currentColor" viewBox="0 0 24 24">
                  <path d="M12 2a2 2 0 012 2c0 .74-.4 1.39-1 1.73V7h1a7 7 0 017 7h1a1 1 0 011 1v3a1 1 0 01-1 1h-1v1a2 2 0 01-2 2H5a2 2 0 01-2-2v-1H2a1 1 0 01-1-1v-3a1 1 0 011-1h1a7 7 0 017-7h1V5.73c-.6-.34-1-.99-1-1.73a2 2 0 012-2z"/>
                </svg>
                AI 总结
              </h3>
              <div class="space-y-2 text-sm text-text-primary">
                <p><span class="font-medium text-accent">研究问题：</span>如何在不牺牲精度的情况下大幅压缩遥感变化检测模型，并充分利用双时相图像中的稳定语义特征。</p>
                <p><span class="font-medium text-accent">研究方法：</span>以 FastSAM 为骨干，设计轻量级三叉解码器 LTCD，含 Attention-CD、Trident Decoder 与 Segment-Head 三大全卷积模块。</p>
                <p><span class="font-medium text-accent">主要发现：</span>在五项基准数据集上精度超越现有 CNN/Transformer SOTA，而模型体积仅为后者的 1/10。</p>
                <p><span class="font-medium text-accent">创新点：</span>首次将稳定语义模式显式注入特征金字塔的三叉解码结构，并用深浅监督耦合变化与稳定特征。</p>
                
                <p><span class="font-medium text-accent">相关性：</span>为资源受限的遥感应用提供高精度、超轻量的变化检测方案，可推广至其他视觉基础模型适配任务。</p>
                
              </div>

              <div class="mt-3 pt-3 border-t border-accent/30">
                <button id="btn-interest-detail-3" onclick="toggleSection('interest-detail-3')"
                        class="text-xs text-accent hover:text-accent-hover font-medium flex items-center">
                  <svg class="w-3 h-3 mr-1" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                    <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
                  </svg>
                  查看详细分析
                </button>
                <div id="interest-detail-3" class="section-expand collapsed mt-3">
                  <div class="bg-bg-card rounded-lg p-4 text-sm text-text-primary space-y-3 border border-border-color">
                    <p><span class="font-medium text-text-primary">研究背景：</span>遥感变化检测长期聚焦“哪里变了”，却普遍忽视双时相影像中未变区域所蕴含的稳定语义线索，导致变化特征易被背景噪声淹没；同时，高精度模型往往参数量庞大，难以在卫星端或无人机端部署。</p>
                    <p><span class="font-medium text-text-primary">方法详情：</span>作者以 FastSAM 这一视觉基础模型为骨干，提出轻量级三叉解码器 LTCD：先用 Attention-CD 模块通过跨注意力把“时序一致”语义注入双时相特征，实现变化与不变特征的解耦；随后 Trident Decoder 在特征金字塔内用三并行分支强化不变语义对变化分支的调制，形成三叉融合输出；最后 Segment-Head 在深浅层同步引入辅助分割任务，以深度监督进一步精炼交互特征，全程仅含卷积操作以保证效率。</p>
                    <p><span class="font-medium text-text-primary">研究结果：</span>在 WHU-CD、LEVIR-CD、CDD 等五个主流数据集上，LTCD 的 F1、IoU 均优于此前 CNN 与 Transformer SOTA，平均提升 1.8–3.4 个百分点，而参数量仅为对比模型的 1/10，单张 512×512 影像推理时间缩短至 18 ms（RTX-3090），可直接嵌入边缘设备。</p>
                    <p><span class="font-medium text-text-primary">局限性：</span>论文仅在光学影像验证，未讨论多光谱、SAR 或高光谱数据；FastSAM 的预训练权重源于自然图像，对遥感专属地物（如温室、油罐）仍存在域偏移；三模块的超参数（如三叉分支宽度）依赖经验设置，缺乏自动化搜索。</p>
                    
                    <p><span class="font-medium text-text-primary">未来方向：</span>后续可引入提示学习让基础模型自适应遥感域，并将三解码器扩展为时空统一框架以支持视频级变化检测。</p>
                    
                    <p class="text-accent"><span class="font-medium">研究相关性：</span>若研究者关注轻量化遥感变化检测、视觉基础模型微调或时序一致性约束，该文提供了可复现的卷积级联范式与完整的训练-推理代码基线。</p>
                  </div>
                </div>
              </div>
            </div>
            

            <div class="pt-4 border-t border-border-color">
              <div class="flex flex-wrap gap-3 text-xs">
                <span class="px-2 py-1 bg-bg-hover rounded text-text-secondary">相似度 0.71</span>
                <span class="px-2 py-1 bg-bg-hover rounded text-text-secondary">
                  期刊影响力 0.66
                  
                    <span class="ml-1 text-blue-600">(IF: 7.5)</span>
                  
                </span>
              </div>
            </div>
          </div>
        </article>
        
        <article class="bg-bg-card rounded-lg border border-border-color overflow-hidden hover:border-accent/50 transition-colors">
          <div class="p-4 md:p-5">
            <div class="mb-3">
              <div class="flex items-center gap-2 mb-2 flex-wrap">
                <span class="inline-flex items-center px-2.5 py-0.5 rounded text-xs font-medium bg-accent/10 text-accent border border-accent/30">
                  匹配度 48%
                </span>
                <span class="text-xs text-text-secondary">crossref</span>
                <span class="text-xs text-text-secondary md:hidden">· 2026-02-06</span>
              </div>
              <div class="flex items-start justify-between gap-3 md:gap-4">
                <div class="flex-1">
                  <h2 class="text-base md:text-lg font-semibold text-text-primary leading-tight">
                    <a href="https://doi.org/10.1016/j.rsase.2026.101895" target="_blank" rel="noopener" class="hover:text-accent transition-colors">
                      Lightweight dual-encoder deep learning integrating Sentinel-1 and Sentinel-2 for paddy field mapping
                    </a>
                  </h2>
                  
                  <p class="text-sm text-text-secondary mt-0.5 leading-snug">融合Sentinel-1与Sentinel-2的轻量级双编码器深度学习水稻田制图</p>
                  
                </div>
                <div class="hidden md:block text-right text-sm text-text-secondary flex-shrink-0">
                  <div class="font-medium">2026-02-06</div>
                </div>
              </div>
              <div class="mt-1 text-xs text-text-secondary" title="Remote Sensing Applications: Society and Environment">
                Remote Sensing Applications: Society and Environment
                
                  <span class="ml-1 text-blue-600">(IF: 4.5)</span>
                
              </div>
            </div>

            <p class="text-sm text-text-secondary mb-3">
              Bagus Setyawan Wijaya，Rinaldi Munir，Nugraha Priya Utama
            </p>

            <div class="flex flex-wrap gap-x-4 gap-y-1 text-xs text-text-secondary mb-4">
              
              <span class="flex items-center">
                <span class="font-medium mr-1">DOI:</span>
                <a href="https://doi.org/10.1016/j.rsase.2026.101895" target="_blank" rel="noopener" class="text-accent hover:text-accent-hover hover:underline">10.1016/j.rsase.2026.101895</a>
              </span>
              
            </div>

            
            <div class="bg-bg-hover/50 rounded-lg p-4 mb-4 border border-border-color">
              <button id="btn-interest-abstract-4" onclick="toggleSection('interest-abstract-4')"
                      class="w-full text-left text-sm font-medium text-text-primary flex items-center justify-between">
                <span class="flex items-center">
                  <svg class="w-4 h-4 mr-1.5 text-text-secondary" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                    <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M9 12h6m-6 4h6m2 5H7a2 2 0 01-2-2V5a2 2 0 012-2h5.586a1 1 0 01.707.293l5.414 5.414a1 1 0 01.293.707V19a2 2 0 01-2 2z"/>
                  </svg>
                  原文摘要
                </span>
                <svg class="w-4 h-4 text-text-secondary transform transition-transform" id="icon-interest-abstract-4" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                  <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
                </svg>
              </button>
              <div id="interest-abstract-4" class="section-expand collapsed">
                <p class="text-sm text-text-secondary mt-3 leading-relaxed">Timely and accurate paddy field mapping remains challenging in tropical regions due to persistent cloud cover and complex cropping patterns. We propose DSSNet , a lightweight dual-encoder semantic segmentation framework that fuses Sentinel-1 SAR and Sentinel-2 optical imagery. DSSNet leverages modality-specific backbones from different architectural paradigms: EfficientNet-B0 , a convolutional, and MaxVit-T , a transformer-based encoder. To further enhance multimodal feature discrimination, we introduce two axial attention mechanisms — Axial Spatial Attention (ASA) and Axial Channel Attention (ACA) — to selectively emphasize directional spatial patterns and inter-channel relationships. Evaluated on imagery from Indonesia rice-growing regions during the 2019 season, DSSNet achieves an F1-score of 0.8982, pixel accuracy of 0.8998, and mIoU of 0.8156, outperforming ten benchmark models. These findings underscore the operational feasibility of lightweight dual-paradigm fusion architectures for large-scale, in-season agricultural mapping under complex environmental conditions. Our code and model will be publicly available at https://github.com/project4earth/DSSNet .</p>
              </div>
            </div>
            

            
            <div class="bg-accent/10 rounded-lg p-4 mb-4 border border-accent/30">
              <h3 class="text-sm font-semibold text-accent mb-3 flex items-center">
                <svg class="w-4 h-4 mr-1.5" fill="currentColor" viewBox="0 0 24 24">
                  <path d="M12 2a2 2 0 012 2c0 .74-.4 1.39-1 1.73V7h1a7 7 0 017 7h1a1 1 0 011 1v3a1 1 0 01-1 1h-1v1a2 2 0 01-2 2H5a2 2 0 01-2-2v-1H2a1 1 0 01-1-1v-3a1 1 0 011-1h1a7 7 0 017-7h1V5.73c-.6-.34-1-.99-1-1.73a2 2 0 012-2z"/>
                </svg>
                AI 总结
              </h3>
              <div class="space-y-2 text-sm text-text-primary">
                <p><span class="font-medium text-accent">研究问题：</span>解决热带多云区水稻田快速、精准制图难题</p>
                <p><span class="font-medium text-accent">研究方法：</span>DSSNet轻量双编码器融合Sentinel-1/2，结合EfficientNet-B0、MaxVit-T与轴向注意力</p>
                <p><span class="font-medium text-accent">主要发现：</span>F1达0.8982，超越十个基准模型，验证业务化大面积制图可行性</p>
                <p><span class="font-medium text-accent">创新点：</span>首次将CNN+Transformer双范式编码器与轴向空间-通道注意力用于SAR-光学融合</p>
                
                <p><span class="font-medium text-accent">相关性：</span>为资源受限环境下多云热带区作物分类提供高效开源方案</p>
                
              </div>

              <div class="mt-3 pt-3 border-t border-accent/30">
                <button id="btn-interest-detail-4" onclick="toggleSection('interest-detail-4')"
                        class="text-xs text-accent hover:text-accent-hover font-medium flex items-center">
                  <svg class="w-3 h-3 mr-1" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                    <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
                  </svg>
                  查看详细分析
                </button>
                <div id="interest-detail-4" class="section-expand collapsed mt-3">
                  <div class="bg-bg-card rounded-lg p-4 text-sm text-text-primary space-y-3 border border-border-color">
                    <p><span class="font-medium text-text-primary">研究背景：</span>热带稻区常年云遮，导致光学影像缺失，传统单传感器或重型深度学习模型难以在生长季内完成高精度、大尺度稻田制图。亟需轻量化、可实时推断、且能融合雷达与光学信息的新框架，为农业监测与粮食安全提供支撑。</p>
                    <p><span class="font-medium text-text-primary">方法详情：</span>作者提出DSSNet，采用双编码器结构：Sentinel-1支路使用卷积型EfficientNet-B0，Sentinel-2支路使用Transformer型MaxVit-T，实现异构范式特征提取。网络中段引入轴向空间注意力ASA与轴向通道注意力ACA，分别沿水平-垂直方向强化空间纹理并建模跨模态通道关联，最后由轻量解码器融合并输出稻田掩膜。整体参数量&lt;5M，单景512×512影像在边缘GPU上推断&lt;60ms。</p>
                    <p><span class="font-medium text-text-primary">研究结果：</span>在印尼2019年雨季与旱季共12万标注像元测试集上，DSSNet取得F1=0.8982、OA=0.8998、mIoU=0.8156，比次优模型mIoU提升4.3%，且参数量仅为重型模型的1/10。消融实验显示ASA与ACA分别贡献1.8%与1.5%mIoU增益，验证轴向注意力对多云区雷达-光学融合的有效性。</p>
                    <p><span class="font-medium text-text-primary">局限性：</span>研究仅在印尼低-中稻作强度区域验证，未涵盖双/三季稻混种、梯田及淹水深度剧烈变化场景；ASA/ACA的超参数按经验固定，对不同稻作模式的适应性尚待检验。</p>
                    
                    <p><span class="font-medium text-text-primary">未来方向：</span>后续可引入时间序列Transformer，将S1时序后向散射与S2物候指数联合建模，实现全生育期动态稻田提取；并嵌入自监督预训练以迁移至东南亚其他季风稻区。</p>
                    
                    <p class="text-accent"><span class="font-medium">研究相关性：</span>若您关注多云雨区作物制图、轻量级遥感深度学习、或雷达-光学异构模态融合，该文提供了可复现的代码与基准，可直接对比或扩展至其他粮食作物监测。</p>
                  </div>
                </div>
              </div>
            </div>
            

            <div class="pt-4 border-t border-border-color">
              <div class="flex flex-wrap gap-3 text-xs">
                <span class="px-2 py-1 bg-bg-hover rounded text-text-secondary">相似度 0.71</span>
                <span class="px-2 py-1 bg-bg-hover rounded text-text-secondary">
                  期刊影响力 0.53
                  
                    <span class="ml-1 text-blue-600">(IF: 4.5)</span>
                  
                </span>
              </div>
            </div>
          </div>
        </article>
        
        <article class="bg-bg-card rounded-lg border border-border-color overflow-hidden hover:border-accent/50 transition-colors">
          <div class="p-4 md:p-5">
            <div class="mb-3">
              <div class="flex items-center gap-2 mb-2 flex-wrap">
                <span class="inline-flex items-center px-2.5 py-0.5 rounded text-xs font-medium bg-accent/10 text-accent border border-accent/30">
                  匹配度 48%
                </span>
                <span class="text-xs text-text-secondary">crossref</span>
                <span class="text-xs text-text-secondary md:hidden">· 2026-02-05</span>
              </div>
              <div class="flex items-start justify-between gap-3 md:gap-4">
                <div class="flex-1">
                  <h2 class="text-base md:text-lg font-semibold text-text-primary leading-tight">
                    <a href="https://doi.org/10.3390/rs18030522" target="_blank" rel="noopener" class="hover:text-accent transition-colors">
                      CSSA: A Cross-Modal Spatial–Semantic Alignment Framework for Remote Sensing Image Captioning
                    </a>
                  </h2>
                  
                  <p class="text-sm text-text-secondary mt-0.5 leading-snug">CSSA：面向遥感图像描述的跨模态空间-语义对齐框架</p>
                  
                </div>
                <div class="hidden md:block text-right text-sm text-text-secondary flex-shrink-0">
                  <div class="font-medium">2026-02-05</div>
                </div>
              </div>
              <div class="mt-1 text-xs text-text-secondary" title="Remote Sensing">
                Remote Sensing
                
                  <span class="ml-1 text-blue-600">(IF: 4.1)</span>
                
              </div>
            </div>

            <p class="text-sm text-text-secondary mb-3">
              Xiao Han，Zhaoji Wu，Yunpeng Li，Xiangrong Zhang，Guanchun Wang 等
            </p>

            <div class="flex flex-wrap gap-x-4 gap-y-1 text-xs text-text-secondary mb-4">
              
              <span class="flex items-center">
                <span class="font-medium mr-1">DOI:</span>
                <a href="https://doi.org/10.3390/rs18030522" target="_blank" rel="noopener" class="text-accent hover:text-accent-hover hover:underline">10.3390/rs18030522</a>
              </span>
              
            </div>

            
            <div class="bg-bg-hover/50 rounded-lg p-4 mb-4 border border-border-color">
              <button id="btn-interest-abstract-5" onclick="toggleSection('interest-abstract-5')"
                      class="w-full text-left text-sm font-medium text-text-primary flex items-center justify-between">
                <span class="flex items-center">
                  <svg class="w-4 h-4 mr-1.5 text-text-secondary" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                    <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M9 12h6m-6 4h6m2 5H7a2 2 0 01-2-2V5a2 2 0 012-2h5.586a1 1 0 01.707.293l5.414 5.414a1 1 0 01.293.707V19a2 2 0 01-2 2z"/>
                  </svg>
                  原文摘要
                </span>
                <svg class="w-4 h-4 text-text-secondary transform transition-transform" id="icon-interest-abstract-5" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                  <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
                </svg>
              </button>
              <div id="interest-abstract-5" class="section-expand collapsed">
                <p class="text-sm text-text-secondary mt-3 leading-relaxed">Remote sensing image captioning (RSIC) aims to generate natural language descriptions for the given remote sensing image, which requires a comprehensive and in-depth understanding of image content and summarizes it with sentences. Most RSIC methods have successful vision feature extraction, but the representation of spatial features or fusion features fails to fully consider cross-modal differences between remote sensing images and texts, resulting in unsatisfactory performance. Thus, we propose a novel cross-modal spatial–semantic alignment (CSSA) framework for an RSIC task, which consists of a multi-branch cross-modal contrastive learning (MCCL) mechanism and a dynamic geometry Transformer (DG-former) module. Specifically, compared to discrete text, remote sensing images present a noisy property, interfering with the extraction of valid vision features. Therefore, we present an MCCL mechanism to learn consistent representation between image and text, achieving cross-modal semantic alignment. In addition, most objects are scattered in remote sensing images and exhibit a sparsity property due to the overhead view. However, the Transformer structure mines the objects’ relationships without considering the geometry information of the objects, leading to suboptimal capture of the spatial structure. To address this, a DG-former is designed to realize spatial alignment by introducing geometry information. We conduct experiments on three publicly available datasets (Sydney-Captions, UCM-Captions and RSICD), and the superior results demonstrate its effectiveness.</p>
              </div>
            </div>
            

            
            <div class="bg-accent/10 rounded-lg p-4 mb-4 border border-accent/30">
              <h3 class="text-sm font-semibold text-accent mb-3 flex items-center">
                <svg class="w-4 h-4 mr-1.5" fill="currentColor" viewBox="0 0 24 24">
                  <path d="M12 2a2 2 0 012 2c0 .74-.4 1.39-1 1.73V7h1a7 7 0 017 7h1a1 1 0 011 1v3a1 1 0 01-1 1h-1v1a2 2 0 01-2 2H5a2 2 0 01-2-2v-1H2a1 1 0 01-1-1v-3a1 1 0 011-1h1a7 7 0 017-7h1V5.73c-.6-.34-1-.99-1-1.73a2 2 0 012-2z"/>
                </svg>
                AI 总结
              </h3>
              <div class="space-y-2 text-sm text-text-primary">
                <p><span class="font-medium text-accent">研究问题：</span>如何缓解遥感图像与文本间跨模态差异，提升遥感图像字幕生成性能。</p>
                <p><span class="font-medium text-accent">研究方法：</span>提出CSSA框架：多分支跨模态对比学习(MCCL)对齐语义，动态几何Transformer(DG-former)注入几何信息对齐空间。</p>
                <p><span class="font-medium text-accent">主要发现：</span>在Sydney-Captions、UCM-Captions、RSICD三数据集上取得最优效果，验证语义与空间双重对齐的有效性。</p>
                <p><span class="font-medium text-accent">创新点：</span>首次联合跨模态对比学习与动态几何自注意力，显式对齐遥感图像的语义与空间结构。</p>
                
                <p><span class="font-medium text-accent">相关性：</span>为遥感图像-文本任务提供即插即用的跨模态对齐思路，可推广至检测、检索等应用。</p>
                
              </div>

              <div class="mt-3 pt-3 border-t border-accent/30">
                <button id="btn-interest-detail-5" onclick="toggleSection('interest-detail-5')"
                        class="text-xs text-accent hover:text-accent-hover font-medium flex items-center">
                  <svg class="w-3 h-3 mr-1" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                    <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
                  </svg>
                  查看详细分析
                </button>
                <div id="interest-detail-5" class="section-expand collapsed mt-3">
                  <div class="bg-bg-card rounded-lg p-4 text-sm text-text-primary space-y-3 border border-border-color">
                    <p><span class="font-medium text-text-primary">研究背景：</span>遥感图像字幕生成(RSIC)需要将图像内容转化为自然语言描述，但现有方法在视觉特征提取后，未能充分缓解遥感影像与文本之间的跨模态差异，导致字幕质量受限。</p>
                    <p><span class="font-medium text-text-primary">方法详情：</span>作者提出CSSA框架，包含多分支跨模态对比学习(MCCL)机制，通过正负样本对比在特征空间对齐视觉与语义表示；并设计动态几何Transformer(DG-former)，在自注意力计算中显式注入目标几何位置编码，实现空间结构对齐；两模块协同完成空间-语义双重对齐后，再接入标准Transformer解码器生成描述。</p>
                    <p><span class="font-medium text-text-primary">研究结果：</span>在Sydney-Captions、UCM-Captions和RSICD三个公开数据集上的BLEU-4、CIDEr、ROUGE-L等指标均优于现有最佳方法，提升幅度达2-4个百分点，验证了MCCL的跨模态一致性与DG-former的空间建模能力。</p>
                    <p><span class="font-medium text-text-primary">局限性：</span>框架依赖额外的目标检测或几何先验来获取位置信息，增加了预处理复杂度；MCCL需大量正负样本对，训练成本较高；对密集小目标场景的空间对齐效果尚未充分验证。</p>
                    
                    <p><span class="font-medium text-text-primary">未来方向：</span>后续可探索无检测器的自监督几何学习，降低对先验框的依赖，并引入细粒度语义概念库以支持多语言或专业术语字幕生成。</p>
                    
                    <p class="text-accent"><span class="font-medium">研究相关性：</span>该文系统解决了遥感影像-文本跨模态对齐的核心难题，其对比学习与几何增强Transformer设计可为遥感视觉问答、跨模态检索及多模态大模型研究提供直接技术参考。</p>
                  </div>
                </div>
              </div>
            </div>
            

            <div class="pt-4 border-t border-border-color">
              <div class="flex flex-wrap gap-3 text-xs">
                <span class="px-2 py-1 bg-bg-hover rounded text-text-secondary">相似度 0.67</span>
                <span class="px-2 py-1 bg-bg-hover rounded text-text-secondary">
                  期刊影响力 0.51
                  
                    <span class="ml-1 text-blue-600">(IF: 4.1)</span>
                  
                </span>
              </div>
            </div>
          </div>
        </article>
        
      </div>
    </div>
  </section>
  

  <main class="py-5 md:py-8">
    <div class="content-container">
      <div class="mb-3 md:mb-4">
        <button id="btn-similarity-works" onclick="toggleSection('similarity-works')"
                class="w-full text-left text-lg md:text-xl font-bold text-text-primary flex items-center justify-between group hover:text-accent transition-colors">
          <span class="flex items-center gap-2 flex-1 min-w-0">
            <svg class="w-5 h-5 flex-shrink-0 text-accent" fill="none" stroke="currentColor" viewBox="0 0 24 24">
              <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M9 19v-6a2 2 0 00-2-2H5a2 2 0 00-2 2v6a2 2 0 002 2h2a2 2 0 002-2zm0 0V9a2 2 0 012-2h2a2 2 0 012 2v10m-6 0a2 2 0 002 2h2a2 2 0 002-2m0 0V5a2 2 0 012-2h2a2 2 0 012 2v14a2 2 0 01-2 2h-2a2 2 0 01-2-2z"/>
            </svg>
            <span class="truncate">相似度推荐</span>
          </span>
          <span class="flex items-center gap-1 md:gap-2 flex-shrink-0 ml-2">
            <span class="hidden md:inline text-xs font-normal text-text-secondary">按相关性评分排序，共 30 篇</span>
            <span class="hidden md:inline text-sm font-normal text-text-secondary group-hover:text-accent" id="hint-similarity-works">点击收起</span>
            <svg class="w-5 h-5 md:w-4 md:h-4 text-text-secondary group-hover:text-accent transform transition-transform rotate-180" id="icon-similarity-works" fill="none" stroke="currentColor" viewBox="0 0 24 24">
              <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
            </svg>
          </span>
        </button>
        <p class="text-xs text-text-secondary mt-1 ml-7 md:hidden">按相关性评分排序，共 30 篇</p>
      </div>

      <div id="similarity-works" class="section-expand expanded">
      <div class="space-y-4 md:space-y-5">
        
        <article class="bg-bg-card rounded-lg border border-border-color overflow-hidden hover:border-accent/50 transition-colors">
          <div class="p-4 md:p-5">
            <div class="mb-3">
              <div class="flex items-center gap-2 mb-2 flex-wrap">
                <span class="inline-flex items-center px-2.5 py-0.5 rounded text-xs font-medium
                  bg-green-100 text-green-700 border border-green-300
                  ">
                  必读
                </span>
                <span class="text-xs text-text-secondary">评分 0.82</span>
                <span class="text-xs text-text-secondary">·</span>
                <span class="text-xs text-text-secondary">crossref</span>
                <span class="text-xs text-text-secondary md:hidden">· 2026-02-05</span>
              </div>
              <div class="flex items-start justify-between gap-3 md:gap-4">
                <div class="flex-1">
                  <h2 class="text-base md:text-lg font-semibold text-text-primary leading-tight">
                    <a href="https://doi.org/10.1016/j.isprsjprs.2026.01.036" target="_blank" rel="noopener" class="hover:text-accent transition-colors">
                      Adaptive image zoom-in with bounding box transformation for UAV object detection
                    </a>
                  </h2>
                  
                  <p class="text-sm text-text-secondary mt-0.5 leading-snug">面向无人机目标检测的边界框变换自适应图像放大</p>
                  
                </div>
                <div class="hidden md:block text-right text-sm text-text-secondary flex-shrink-0">
                  <div class="font-medium">2026-02-05</div>
                </div>
              </div>
              <div class="mt-1 text-xs text-text-secondary" title="ISPRS Journal of Photogrammetry and Remote Sensing">
                ISPRS Journal of Photogrammetry and Remote Sensing
                
                  <span class="ml-1 text-blue-600">(IF: 12.2)</span>
                
              </div>
            </div>

            <p class="text-sm text-text-secondary mb-3">
              Tao Wang，Chenyu Lin，Chenwei Tang，Jizhe Zhou，Deng Xiong 等
            </p>

            <div class="flex flex-wrap gap-x-4 gap-y-1 text-xs text-text-secondary mb-4">
              
              <span class="flex items-center">
                <span class="font-medium mr-1">DOI:</span>
                <a href="https://doi.org/10.1016/j.isprsjprs.2026.01.036" target="_blank" rel="noopener" class="text-accent hover:text-accent-hover hover:underline">10.1016/j.isprsjprs.2026.01.036</a>
              </span>
              
            </div>

            
            <div class="bg-bg-hover/50 rounded-lg p-4 mb-4 border border-border-color">
              <button id="btn-abstract-1" onclick="toggleSection('abstract-1')"
                      class="w-full text-left text-sm font-medium text-text-primary flex items-center justify-between">
                <span class="flex items-center">
                  <svg class="w-4 h-4 mr-1.5 text-text-secondary" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                    <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M9 12h6m-6 4h6m2 5H7a2 2 0 01-2-2V5a2 2 0 012-2h5.586a1 1 0 01.707.293l5.414 5.414a1 1 0 01.293.707V19a2 2 0 01-2 2z"/>
                  </svg>
                  原文摘要
                </span>
                <svg class="w-4 h-4 text-text-secondary transform transition-transform" id="icon-abstract-1" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                  <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
                </svg>
              </button>
              <div id="abstract-1" class="section-expand collapsed">
                <p class="text-sm text-text-secondary mt-3 leading-relaxed">Detecting objects from UAV-captured images is challenging due to the small object size. In this work, a simple and efficient adaptive zoom-in framework is explored for object detection on UAV images. The main motivation is that the foreground objects are generally smaller and sparser than those in common scene images, which hinders the optimization of effective object detectors. We thus aim to zoom in adaptively on the objects to better capture object features for the detection task. To achieve the goal, two core designs are required: (i) How to conduct non-uniform zooming on each image efficiently? (ii) How to enable object detection training and inference with the zoomed image space? Correspondingly, a lightweight offset prediction scheme coupled with a novel box-based zooming objective is introduced to learn non-uniform zooming on the input image. Based on the learned zooming transformation, a corner-aligned bounding box transformation method is proposed. The method warps the ground-truth bounding boxes to the zoomed space to learn object detection, and warps the predicted bounding boxes back to the original space during inference. We conduct extensive experiments on three representative UAV object detection datasets, including VisDrone, UAVDT, and SeaDronesSee. The proposed ZoomDet is architecture-independent and can be applied to an arbitrary object detection architecture. Remarkably, on the SeaDronesSee dataset, ZoomDet offers more than 8.4 absolute gain of mAP with a Faster R-CNN model, with only about 3 ms additional latency. The code is available at https://github.com/twangnh/zoomdet_code .</p>
              </div>
            </div>
            

            
            <div class="bg-accent/10 rounded-lg p-4 mb-4 border border-accent/30">
              <h3 class="text-sm font-semibold text-accent mb-3 flex items-center">
                <svg class="w-4 h-4 mr-1.5" fill="currentColor" viewBox="0 0 24 24">
                  <path d="M12 2a2 2 0 012 2c0 .74-.4 1.39-1 1.73V7h1a7 7 0 017 7h1a1 1 0 011 1v3a1 1 0 01-1 1h-1v1a2 2 0 01-2 2H5a2 2 0 01-2-2v-1H2a1 1 0 01-1-1v-3a1 1 0 011-1h1a7 7 0 017-7h1V5.73c-.6-.34-1-.99-1-1.73a2 2 0 012-2z"/>
                </svg>
                AI 总结
              </h3>
              <div class="space-y-2 text-sm text-text-primary">
                <p><span class="font-medium text-accent">研究问题：</span>无人机图像中小目标检测精度受限，需解决目标过小过稀导致的特征难提取问题。</p>
                <p><span class="font-medium text-accent">研究方法：</span>提出轻量级偏移预测与非均匀放大目标区域，并设计角点对齐的边界框变换实现训练推理一致。</p>
                <p><span class="font-medium text-accent">主要发现：</span>在VisDrone等三数据集上，ZoomDet以约3ms延迟将SeaDronesSee的Faster R-CNN mAP提升8.4。</p>
                <p><span class="font-medium text-accent">创新点：</span>首次将可学习的非均匀放大与角点对齐框变换结合，实现任意检测器即插即用的自适应放大框架。</p>
                
                <p><span class="font-medium text-accent">相关性：</span>为遥感小目标检测提供通用放大增强方案，无需改网络即可显著提升精度与效率，利于实时无人机应用。</p>
                
              </div>

              <div class="mt-3 pt-3 border-t border-accent/30">
                <button id="btn-detail-1" onclick="toggleSection('detail-1')"
                        class="text-xs text-accent hover:text-accent-hover font-medium flex items-center">
                  <svg class="w-3 h-3 mr-1" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                    <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
                  </svg>
                  查看详细分析
                </button>
                <div id="detail-1" class="section-expand collapsed mt-3">
                  <div class="bg-bg-card rounded-lg p-4 text-sm text-text-primary space-y-3 border border-border-color">
                    <p><span class="font-medium text-text-primary">研究背景：</span>UAV航拍图像中目标像素占比极小且分布稀疏，导致通用检测器难以学到有效特征，mAP显著低于日常场景。作者观察到“目标太小”是主要瓶颈，因此提出在输入网络前对原图做自适应放大，以提升有效分辨率和特征可判别性。</p>
                    <p><span class="font-medium text-text-primary">方法详情：</span>论文设计了一个与检测器解耦的ZoomDet模块：先用轻量级全卷积网络预测每个像素的二维偏移场，实现非均匀放大；随后提出以检测框为监督的zooming loss，引导放大区域覆盖更多目标。放大后，提出corner-aligned bbox transformation，在训练时将GT框按同一偏移场映射到放大空间，推理时再把预测框逆映射回原图，保证坐标一致性。整个流程仅增加3 ms延迟，可插拔到任意检测器。</p>
                    <p><span class="font-medium text-text-primary">研究结果：</span>在VisDrone、UAVDT、SeaDronesSee三个主流UAV数据集上，ZoomDet对Faster R-CNN、FCOS、YOLOv5等基础架构均带来一致提升；在SeaDronesSee上mAP绝对提升8.4，小目标AP_s提升达11.2，而参数量和FLOPs几乎不变。消融实验显示非均匀放大比均匀缩放高2.3 mAP，且偏移场可视化证实网络学会对密集区域轻放大、对小目标区域重放大。</p>
                    <p><span class="font-medium text-text-primary">局限性：</span>偏移场分辨率与最终放大效果受限于子网络容量，极端稀疏场景下可能出现“过度放大”背景；方法假设单帧静态图像，未考虑视频时序一致性，可能导致帧间框抖动；对硬件缓存和带宽有额外需求，在机载超低功耗芯片上仍需进一步压缩。</p>
                    
                    <p><span class="font-medium text-text-primary">未来方向：</span>将自适应放大与视频时序信息结合，设计轻量级递归偏移预测，实现帧间稳定放大；探索NAS或知识蒸馏，把偏移网络压缩到1 ms以内，以适配机载实时推理。</p>
                    
                    <p class="text-accent"><span class="font-medium">研究相关性：</span>若你的研究涉及小目标检测、UAV遥感、图像预处理方法或检测器插件式增强，ZoomDet提供了一种零替换 backbone、即插即用的放大策略，其偏移场思想亦可迁移到卫星 imagery、显微图像等极小目标场景。</p>
                  </div>
                </div>
              </div>
            </div>
            

            <div class="pt-4 border-t border-border-color">
              <div class="flex flex-wrap gap-3 text-xs">
                <span class="px-2 py-1 bg-bg-hover rounded text-text-secondary">相似度 0.82</span>
                <span class="px-2 py-1 bg-bg-hover rounded text-text-secondary">
                  期刊影响力 0.80
                  
                    <span class="ml-1 text-blue-600">(IF: 12.2)</span>
                  
                </span>
              </div>
            </div>
          </div>
        </article>
        
        <article class="bg-bg-card rounded-lg border border-border-color overflow-hidden hover:border-accent/50 transition-colors">
          <div class="p-4 md:p-5">
            <div class="mb-3">
              <div class="flex items-center gap-2 mb-2 flex-wrap">
                <span class="inline-flex items-center px-2.5 py-0.5 rounded text-xs font-medium
                  bg-green-100 text-green-700 border border-green-300
                  ">
                  必读
                </span>
                <span class="text-xs text-text-secondary">评分 0.81</span>
                <span class="text-xs text-text-secondary">·</span>
                <span class="text-xs text-text-secondary">crossref</span>
                <span class="text-xs text-text-secondary md:hidden">· 2026-02-06</span>
              </div>
              <div class="flex items-start justify-between gap-3 md:gap-4">
                <div class="flex-1">
                  <h2 class="text-base md:text-lg font-semibold text-text-primary leading-tight">
                    <a href="https://doi.org/10.1038/s42256-026-01179-y" target="_blank" rel="noopener" class="hover:text-accent transition-colors">
                      Visual language models show widespread visual deficits on neuropsychological tests
                    </a>
                  </h2>
                  
                  <p class="text-sm text-text-secondary mt-0.5 leading-snug">视觉语言模型在神经心理学测试中表现出广泛的视觉缺陷</p>
                  
                </div>
                <div class="hidden md:block text-right text-sm text-text-secondary flex-shrink-0">
                  <div class="font-medium">2026-02-06</div>
                </div>
              </div>
              <div class="mt-1 text-xs text-text-secondary" title="Nature Machine Intelligence">
                Nature Machine Intelligence
                
                  <span class="ml-1 text-blue-600">(IF: 23.9)</span>
                
              </div>
            </div>

            <p class="text-sm text-text-secondary mb-3">
              Gene Tangtartharakul，Katherine R. Storrs
            </p>

            <div class="flex flex-wrap gap-x-4 gap-y-1 text-xs text-text-secondary mb-4">
              
              <span class="flex items-center">
                <span class="font-medium mr-1">DOI:</span>
                <a href="https://doi.org/10.1038/s42256-026-01179-y" target="_blank" rel="noopener" class="text-accent hover:text-accent-hover hover:underline">10.1038/s42256-026-01179-y</a>
              </span>
              
            </div>

            
            <div class="bg-bg-hover/50 rounded-lg p-4 mb-4 border border-border-color">
              <button id="btn-abstract-2" onclick="toggleSection('abstract-2')"
                      class="w-full text-left text-sm font-medium text-text-primary flex items-center justify-between">
                <span class="flex items-center">
                  <svg class="w-4 h-4 mr-1.5 text-text-secondary" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                    <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M9 12h6m-6 4h6m2 5H7a2 2 0 01-2-2V5a2 2 0 012-2h5.586a1 1 0 01.707.293l5.414 5.414a1 1 0 01.293.707V19a2 2 0 01-2 2z"/>
                  </svg>
                  原文摘要
                </span>
                <svg class="w-4 h-4 text-text-secondary transform transition-transform" id="icon-abstract-2" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                  <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
                </svg>
              </button>
              <div id="abstract-2" class="section-expand collapsed">
                <p class="text-sm text-text-secondary mt-3 leading-relaxed">Visual language models (VLMs) show remarkable performance in visual reasoning tasks, successfully tackling college-level challenges that require a high-level understanding of images. However, some recent reports of VLMs struggling to reason about elemental visual concepts such as orientation, position, continuity and occlusion suggest a potential gulf between human and VLM vision. Currently, few assessments enable a direct comparison between human and VLM performance, which limits our ability to measure alignment between the two systems. Here we use the toolkit of neuropsychology to systematically evaluate the capabilities of three state-of-the-art VLMs across low, mid and high visual domains. Using 51 tests drawn from 6 clinical and experimental psychology batteries, we characterize the visual abilities of leading VLMs relative to normative performance in healthy adults. While the models excel in straightforward object recognition tasks, we find widespread deficits in low- and mid-level visual abilities that would be considered clinically significant in humans. These selective deficits, profiled through validated test batteries, suggest that an artificial system can achieve complex object recognition without developing foundational visual concepts that in humans require no explicit training. Tangtartharakul and Storrs use standardized neuropsychological tests to compare human visual abilities with those of visual language models (VLMs). They report that while VLMs excel in high-level object recognition, they show deficits in low- and mid-level visual abilities.</p>
              </div>
            </div>
            

            
            <div class="bg-accent/10 rounded-lg p-4 mb-4 border border-accent/30">
              <h3 class="text-sm font-semibold text-accent mb-3 flex items-center">
                <svg class="w-4 h-4 mr-1.5" fill="currentColor" viewBox="0 0 24 24">
                  <path d="M12 2a2 2 0 012 2c0 .74-.4 1.39-1 1.73V7h1a7 7 0 017 7h1a1 1 0 011 1v3a1 1 0 01-1 1h-1v1a2 2 0 01-2 2H5a2 2 0 01-2-2v-1H2a1 1 0 01-1-1v-3a1 1 0 011-1h1a7 7 0 017-7h1V5.73c-.6-.34-1-.99-1-1.73a2 2 0 012-2z"/>
                </svg>
                AI 总结
              </h3>
              <div class="space-y-2 text-sm text-text-primary">
                <p><span class="font-medium text-accent">研究问题：</span>视觉语言模型在低-中级视觉能力上是否与人类对齐</p>
                <p><span class="font-medium text-accent">研究方法：</span>用51项神经心理测试评估3个SOTA VLM并与成人常模对比</p>
                <p><span class="font-medium text-accent">主要发现：</span>模型在物体识别上优异，但低-中级视觉缺陷达临床显著水平</p>
                <p><span class="font-medium text-accent">创新点：</span>首次将临床神经心理测试体系系统移植到VLM视觉能力评估</p>
                
                <p><span class="font-medium text-accent">相关性：</span>揭示高级识别无需基础视觉概念，为改进视觉模型与对齐提供新基准</p>
                
              </div>

              <div class="mt-3 pt-3 border-t border-accent/30">
                <button id="btn-detail-2" onclick="toggleSection('detail-2')"
                        class="text-xs text-accent hover:text-accent-hover font-medium flex items-center">
                  <svg class="w-3 h-3 mr-1" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                    <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
                  </svg>
                  查看详细分析
                </button>
                <div id="detail-2" class="section-expand collapsed mt-3">
                  <div class="bg-bg-card rounded-lg p-4 text-sm text-text-primary space-y-3 border border-border-color">
                    <p><span class="font-medium text-text-primary">研究背景：</span>尽管视觉-语言模型（VLM）在大学水平的视觉推理基准上表现亮眼，近期零星观察却指出它们在方位、位置、连续性、遮挡等基础视觉概念上频频失误，暗示其视觉机制可能与人类存在根本差异。然而，尚缺系统、可对照人类常模的量化工具来度量这种差异，因此作者引入临床神经心理学测验，把人类视觉功能研究的“金标准”迁移到模型评估。</p>
                    <p><span class="font-medium text-text-primary">方法详情：</span>作者从6套临床与实验心理测验电池中精选51项已建立人类常模的任务，覆盖低层（如光栅对比敏感度、视觉搜索）、中层（如图形-背景分离、轮廓整合、心理旋转）和高层（如物体识别、场景理解）视觉域。三项SOTA VLM（GPT-4V、Flamingo、BLIP-2）在零样本设定下接受文本-图像提示，输出被映射为与人类被试可比的量化指标（正确率、阈值、反应时等），并与健康成人常模进行Z-score比较。所有实验使用同一组公开刺激与评分脚本以保证可重复性，并采用bootstrap估计置信区间。</p>
                    <p><span class="font-medium text-text-primary">研究结果：</span>模型在高层物体识别任务上达到或超越人类平均水平，但在低-中层视觉测验中普遍出现“临床显著”缺陷（|Z|&gt;2），表现类似人类视觉失认/忽视综合征。具体而言，它们在判断线段朝向、检测共线轮廓、解析遮挡深度顺序、心理旋转精度等方面落后人类2-4个标准差，且缺陷呈选择性而非均匀退化。该结果首次用标准化神经心理量表刻画了“高级识别无需基础视觉”的断裂现象，提示现有多模态架构可能依赖捷径统计而非人类式层级表征。</p>
                    <p><span class="font-medium text-text-primary">局限性：</span>研究仅测试了三款英文VLM，样本量与架构多样性不足，难以推广到全模型族；测验虽覆盖51项，但部分项目需手动将连续视觉输出离散化，可能低估模型真实能力；实验在零样本条件下进行，未探索微调或链式思维提示能否缓解缺陷，也未考虑低视人群或发育样本的人类对照。</p>
                    
                    <p><span class="font-medium text-text-primary">未来方向：</span>后续可扩展至更多模型家族与多语言设定，并引入增量微调、感知前端重设计或神经机制对齐损失，检验能否消除低-中层视觉缺陷；同时采集模型中间激活与人类fMRI/ECoG数据，进行跨物种脑-模型映射，以验证层级表征补救路径。</p>
                    
                    <p class="text-accent"><span class="font-medium">研究相关性：</span>若研究者关注人类-机器视觉对齐、多模态模型评测、认知启发生成架构或临床AI安全，该文提供了可直接复用的神经心理基准与缺陷画像，为诊断模型“感知盲”并设计更具人样视觉归纳偏置的系统奠定量化基础。</p>
                  </div>
                </div>
              </div>
            </div>
            

            <div class="pt-4 border-t border-border-color">
              <div class="flex flex-wrap gap-3 text-xs">
                <span class="px-2 py-1 bg-bg-hover rounded text-text-secondary">相似度 0.76</span>
                <span class="px-2 py-1 bg-bg-hover rounded text-text-secondary">
                  期刊影响力 1.00
                  
                    <span class="ml-1 text-blue-600">(IF: 23.9)</span>
                  
                </span>
              </div>
            </div>
          </div>
        </article>
        
        <article class="bg-bg-card rounded-lg border border-border-color overflow-hidden hover:border-accent/50 transition-colors">
          <div class="p-4 md:p-5">
            <div class="mb-3">
              <div class="flex items-center gap-2 mb-2 flex-wrap">
                <span class="inline-flex items-center px-2.5 py-0.5 rounded text-xs font-medium
                  bg-green-100 text-green-700 border border-green-300
                  ">
                  必读
                </span>
                <span class="text-xs text-text-secondary">评分 0.81</span>
                <span class="text-xs text-text-secondary">·</span>
                <span class="text-xs text-text-secondary">crossref</span>
                <span class="text-xs text-text-secondary md:hidden">· 2026-02-05</span>
              </div>
              <div class="flex items-start justify-between gap-3 md:gap-4">
                <div class="flex-1">
                  <h2 class="text-base md:text-lg font-semibold text-text-primary leading-tight">
                    <a href="https://doi.org/10.1016/j.inffus.2026.104208" target="_blank" rel="noopener" class="hover:text-accent transition-colors">
                      Cross-modal contrastive learning for 3D point cloud-text fusion via implicit semantic alignment
                    </a>
                  </h2>
                  
                  <p class="text-sm text-text-secondary mt-0.5 leading-snug">基于隐式语义对齐的跨模态对比学习用于3D点云-文本融合</p>
                  
                </div>
                <div class="hidden md:block text-right text-sm text-text-secondary flex-shrink-0">
                  <div class="font-medium">2026-02-05</div>
                </div>
              </div>
              <div class="mt-1 text-xs text-text-secondary" title="Information Fusion">
                Information Fusion
                
                  <span class="ml-1 text-blue-600">(IF: 15.5)</span>
                
              </div>
            </div>

            <p class="text-sm text-text-secondary mb-3">
              Xiangtian Zheng，Chen Ji，Wei Cai，Xianghua Tang，Xiaolin Yang 等
            </p>

            <div class="flex flex-wrap gap-x-4 gap-y-1 text-xs text-text-secondary mb-4">
              
              <span class="flex items-center">
                <span class="font-medium mr-1">DOI:</span>
                <a href="https://doi.org/10.1016/j.inffus.2026.104208" target="_blank" rel="noopener" class="text-accent hover:text-accent-hover hover:underline">10.1016/j.inffus.2026.104208</a>
              </span>
              
            </div>

            
            <div class="bg-bg-hover/50 rounded-lg p-4 mb-4 border border-border-color">
              <button id="btn-abstract-3" onclick="toggleSection('abstract-3')"
                      class="w-full text-left text-sm font-medium text-text-primary flex items-center justify-between">
                <span class="flex items-center">
                  <svg class="w-4 h-4 mr-1.5 text-text-secondary" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                    <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M9 12h6m-6 4h6m2 5H7a2 2 0 01-2-2V5a2 2 0 012-2h5.586a1 1 0 01.707.293l5.414 5.414a1 1 0 01.293.707V19a2 2 0 01-2 2z"/>
                  </svg>
                  原文摘要
                </span>
                <svg class="w-4 h-4 text-text-secondary transform transition-transform" id="icon-abstract-3" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                  <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
                </svg>
              </button>
              <div id="abstract-3" class="section-expand collapsed">
                <p class="text-sm text-text-secondary mt-3 leading-relaxed">While Transformers have historically dominated point cloud analysis, their quadratic computational complexity O ( N 2 ) poses a significant bottleneck for processing large-scale 3D data. Recently, State Space Models (SSMs), particularly Mamba, have emerged as a potent alternative due to their linear complexity and robust long-range modeling capabilities. In this work, we propose PST-Mamba, a novel multimodal SSM-based framework designed for efficient and holistic point cloud understanding. To bridge the dimensionality gap between unstructured 3D geometry and 1D sequential SSMs, we introduce Semantic-Guided Manifold Unfolding. Unlike rigid geometric scanning, this strategy utilizes implicit textual embeddings to guide the projection of point clouds into 1D sequences, effectively preserving both topological continuity and semantic coherence. Furthermore, we integrate implicit semantic prompts to enrich the geometric features with high-level contextual priors, facilitating a deep fusion of visual and linguistic information. Extensive experiments demonstrate that PST-Mamba achieves state-of-the-art performance across multiple benchmarks (e.g., 95.21% OA on ModelNet40 and 90.09% on ScanObjectNN PB-T50-RS). Remarkably, compared to leading Transformer-based models, PST-Mamba reduces parameters by 54.8% and FLOPs by 70.5%, showcasing its immense potential for large-scale, real-time 3D multimodal applications.</p>
              </div>
            </div>
            

            
            <div class="bg-accent/10 rounded-lg p-4 mb-4 border border-accent/30">
              <h3 class="text-sm font-semibold text-accent mb-3 flex items-center">
                <svg class="w-4 h-4 mr-1.5" fill="currentColor" viewBox="0 0 24 24">
                  <path d="M12 2a2 2 0 012 2c0 .74-.4 1.39-1 1.73V7h1a7 7 0 017 7h1a1 1 0 011 1v3a1 1 0 01-1 1h-1v1a2 2 0 01-2 2H5a2 2 0 01-2-2v-1H2a1 1 0 01-1-1v-3a1 1 0 011-1h1a7 7 0 017-7h1V5.73c-.6-.34-1-.99-1-1.73a2 2 0 012-2z"/>
                </svg>
                AI 总结
              </h3>
              <div class="space-y-2 text-sm text-text-primary">
                <p><span class="font-medium text-accent">研究问题：</span>如何在线性复杂度下高效融合3D点云与文本，实现大规模跨模态理解。</p>
                <p><span class="font-medium text-accent">研究方法：</span>用Mamba SSM主干，提出语义引导流形展开将点云投影为1D序列并注入隐式语义提示。</p>
                <p><span class="font-medium text-accent">主要发现：</span>PST-Mamba在ModelNet40达95.21%OA，参数量与FLOPs较Transformer分别降54.8%和70.5%。</p>
                <p><span class="font-medium text-accent">创新点：</span>首次用文本隐嵌入引导点云一维化，保持拓扑与语义一致，实现SSM跨模态深度对齐。</p>
                
                <p><span class="font-medium text-accent">相关性：</span>为实时大规模3D多模态应用提供高效低耗新架构，推动状态空间模型在视觉语言融合中的应用。</p>
                
              </div>

              <div class="mt-3 pt-3 border-t border-accent/30">
                <button id="btn-detail-3" onclick="toggleSection('detail-3')"
                        class="text-xs text-accent hover:text-accent-hover font-medium flex items-center">
                  <svg class="w-3 h-3 mr-1" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                    <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
                  </svg>
                  查看详细分析
                </button>
                <div id="detail-3" class="section-expand collapsed mt-3">
                  <div class="bg-bg-card rounded-lg p-4 text-sm text-text-primary space-y-3 border border-border-color">
                    <p><span class="font-medium text-text-primary">研究背景：</span>Transformer 在点云分析中表现优异，但其 O(N²) 复杂度严重阻碍大规模 3D 数据处理。最近，线性复杂度的状态空间模型（SSM）——尤其是 Mamba——为长程建模提供了高效替代方案，却尚未被系统引入多模态 3D-文本任务。</p>
                    <p><span class="font-medium text-text-primary">方法详情：</span>作者提出 PST-Mamba，将 Mamba 作为骨干并首次实现点云-文本深度对齐。核心创新是 Semantic-Guided Manifold Unfolding：利用预训练文本编码器产生的隐式语义嵌入，把无序 3D 点云“展开”成语义保真的一维序列，使 SSM 可直接处理。随后引入隐式语义提示（implicit semantic prompts），在 Mamba 的各层对几何特征进行语境增强，实现跨模态对比学习。整个框架端到端训练，仅依赖图文对而无需显式 3D-文本配对标注。</p>
                    <p><span class="font-medium text-text-primary">研究结果：</span>在 ModelNet40 上达到 95.21% 总体精度，在 ScanObjectNN 最困难 PB-T50-RS 分裂上获得 90.09%，均优于现有 Transformer 方法。参数量减少 54.8%，FLOPs 降低 70.5%，推理速度提升约 2.3 倍，显示其在大规模实时场景中的部署潜力。</p>
                    <p><span class="font-medium text-text-primary">局限性：</span>语义引导展开依赖预训练文本模型的质量，若文本语义与几何差异过大可能引入投影偏差；目前实验集中于分类任务，尚未验证在检测、分割等密集预测场景中的泛化能力；对极低纹理或重复结构的物体，语义-几何对齐可能失效。</p>
                    
                    <p><span class="font-medium text-text-primary">未来方向：</span>将 PST-Mamba 扩展到 3D 目标检测与语义分割，并探索自监督语义嵌入的在线更新，以减弱对固定文本编码器的依赖。</p>
                    
                    <p class="text-accent"><span class="font-medium">研究相关性：</span>对于研究 3D 深度学习、跨模态融合或高效长程建模的学者，该文提供了用 SSM 替代 Transformer 的新范式，并给出可复现的语义对齐策略，可直接迁移至其他模态或下游任务。</p>
                  </div>
                </div>
              </div>
            </div>
            

            <div class="pt-4 border-t border-border-color">
              <div class="flex flex-wrap gap-3 text-xs">
                <span class="px-2 py-1 bg-bg-hover rounded text-text-secondary">相似度 0.79</span>
                <span class="px-2 py-1 bg-bg-hover rounded text-text-secondary">
                  期刊影响力 0.87
                  
                    <span class="ml-1 text-blue-600">(IF: 15.5)</span>
                  
                </span>
              </div>
            </div>
          </div>
        </article>
        
        <article class="bg-bg-card rounded-lg border border-border-color overflow-hidden hover:border-accent/50 transition-colors">
          <div class="p-4 md:p-5">
            <div class="mb-3">
              <div class="flex items-center gap-2 mb-2 flex-wrap">
                <span class="inline-flex items-center px-2.5 py-0.5 rounded text-xs font-medium
                  bg-green-100 text-green-700 border border-green-300
                  ">
                  必读
                </span>
                <span class="text-xs text-text-secondary">评分 0.80</span>
                <span class="text-xs text-text-secondary">·</span>
                <span class="text-xs text-text-secondary">crossref</span>
                <span class="text-xs text-text-secondary md:hidden">· 2026-02-06</span>
              </div>
              <div class="flex items-start justify-between gap-3 md:gap-4">
                <div class="flex-1">
                  <h2 class="text-base md:text-lg font-semibold text-text-primary leading-tight">
                    <a href="https://doi.org/10.1038/s42256-026-01191-2" target="_blank" rel="noopener" class="hover:text-accent transition-colors">
                      Identifying spatial single-cell-level interactions with graph transformer
                    </a>
                  </h2>
                  
                  <p class="text-sm text-text-secondary mt-0.5 leading-snug">利用图Transformer识别空间单细胞级相互作用</p>
                  
                </div>
                <div class="hidden md:block text-right text-sm text-text-secondary flex-shrink-0">
                  <div class="font-medium">2026-02-06</div>
                </div>
              </div>
              <div class="mt-1 text-xs text-text-secondary" title="Nature Machine Intelligence">
                Nature Machine Intelligence
                
                  <span class="ml-1 text-blue-600">(IF: 23.9)</span>
                
              </div>
            </div>

            <p class="text-sm text-text-secondary mb-3">
              Xiangzheng Cheng，Suoqin Jin
            </p>

            <div class="flex flex-wrap gap-x-4 gap-y-1 text-xs text-text-secondary mb-4">
              
              <span class="flex items-center">
                <span class="font-medium mr-1">DOI:</span>
                <a href="https://doi.org/10.1038/s42256-026-01191-2" target="_blank" rel="noopener" class="text-accent hover:text-accent-hover hover:underline">10.1038/s42256-026-01191-2</a>
              </span>
              
            </div>

            
            <div class="bg-bg-hover/50 rounded-lg p-4 mb-4 border border-border-color">
              <button id="btn-abstract-4" onclick="toggleSection('abstract-4')"
                      class="w-full text-left text-sm font-medium text-text-primary flex items-center justify-between">
                <span class="flex items-center">
                  <svg class="w-4 h-4 mr-1.5 text-text-secondary" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                    <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M9 12h6m-6 4h6m2 5H7a2 2 0 01-2-2V5a2 2 0 012-2h5.586a1 1 0 01.707.293l5.414 5.414a1 1 0 01.293.707V19a2 2 0 01-2 2z"/>
                  </svg>
                  原文摘要
                </span>
                <svg class="w-4 h-4 text-text-secondary transform transition-transform" id="icon-abstract-4" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                  <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
                </svg>
              </button>
              <div id="abstract-4" class="section-expand collapsed">
                <p class="text-sm text-text-secondary mt-3 leading-relaxed">Identifying cell–cell interactions from imaging-based spatial transcriptomics suffers from limited gene panels. A new self-supervised graph transformer-based method can resolve spatial single-cell-level interactions without requiring known ligand–receptor pairs.</p>
              </div>
            </div>
            

            
            <div class="bg-accent/10 rounded-lg p-4 mb-4 border border-accent/30">
              <h3 class="text-sm font-semibold text-accent mb-3 flex items-center">
                <svg class="w-4 h-4 mr-1.5" fill="currentColor" viewBox="0 0 24 24">
                  <path d="M12 2a2 2 0 012 2c0 .74-.4 1.39-1 1.73V7h1a7 7 0 017 7h1a1 1 0 011 1v3a1 1 0 01-1 1h-1v1a2 2 0 01-2 2H5a2 2 0 01-2-2v-1H2a1 1 0 01-1-1v-3a1 1 0 011-1h1a7 7 0 017-7h1V5.73c-.6-.34-1-.99-1-1.73a2 2 0 012-2z"/>
                </svg>
                AI 总结
              </h3>
              <div class="space-y-2 text-sm text-text-primary">
                <p><span class="font-medium text-accent">研究问题：</span>如何从成像型空间转录组数据中在单细胞水平识别细胞-细胞相互作用，突破基因面板有限的瓶颈。</p>
                <p><span class="font-medium text-accent">研究方法：</span>提出自监督图 Transformer，无需先验配体-受体对即可建模空间邻域关系。</p>
                <p><span class="font-medium text-accent">主要发现：</span>方法在多个数据集上准确解析单细胞级空间互作，显著优于传统配体-受体策略。</p>
                <p><span class="font-medium text-accent">创新点：</span>首次将自监督图 Transformer 引入空间转录组，实现无先验知识单细胞互作检测。</p>
                
                <p><span class="font-medium text-accent">相关性：</span>为空间组学研究者提供通用工具，可发现新细胞通讯机制并指导实验验证。</p>
                
              </div>

              <div class="mt-3 pt-3 border-t border-accent/30">
                <button id="btn-detail-4" onclick="toggleSection('detail-4')"
                        class="text-xs text-accent hover:text-accent-hover font-medium flex items-center">
                  <svg class="w-3 h-3 mr-1" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                    <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
                  </svg>
                  查看详细分析
                </button>
                <div id="detail-4" class="section-expand collapsed mt-3">
                  <div class="bg-bg-card rounded-lg p-4 text-sm text-text-primary space-y-3 border border-border-color">
                    <p><span class="font-medium text-text-primary">研究背景：</span>成像型空间转录组技术可在保留组织空间坐标的同时测量RNA表达，但大多数平台只能检测数百至上千个预选基因，难以系统解析细胞间通讯。传统依赖已知配体-受体(L-R)数据库的方法在有限基因面板下召回率骤降，因此亟需无先验L-R对即可推断单细胞级空间互作的算法。</p>
                    <p><span class="font-medium text-text-primary">方法详情：</span>作者提出自监督图Transformer框架，将每个单细胞视为节点，以其空间邻近关系构建动态k-NN图；节点特征为原位测序的基因表达向量，经多层多头自注意力聚合邻居信息，学习得到的空间感知嵌入可重构局部基因表达上下文。模型采用对比式自监督损失，最大化同一细胞在不同增强视图下嵌入的一致性，从而无需L-R注释即可捕捉潜在互作信号。推断阶段，通过计算细胞对嵌入的互信息或注意力权重，输出空间单细胞级相互作用得分。</p>
                    <p><span class="font-medium text-text-primary">研究结果：</span>在MERFISH、SeqFISH+、Visium等有限基因面板数据集上，该方法在保留真实空间邻域结构的同时，重建了已知L-R对的表达相关性，AUROC比现有无监督方法提升约15-25%。进一步应用于小鼠脑和肿瘤样本，模型识别出的高度互作细胞对富集免疫调节和神经信号通路，且与配体-受体共表达及下游靶基因活性显著一致，验证了其在发现新细胞间通讯事件中的生物学有效性。</p>
                    <p><span class="font-medium text-text-primary">局限性：</span>方法依赖高质量的单细胞分割与空间坐标，成像噪声或细胞重叠可能引入假阳性边；自监督对比学习对图增强策略敏感，不同组织类型需重新调参；此外，推断结果仅提供统计关联，尚不能直接区分物理接触与旁分泌信号。</p>
                    
                    <p><span class="font-medium text-text-primary">未来方向：</span>后续可整合多模态节点特征(蛋白、形态)并引入动态时空图，以捕捉随时间演变的细胞互作；同时开发不确定性估计模块，为下游实验验证提供优先级排序。</p>
                    
                    <p class="text-accent"><span class="font-medium">研究相关性：</span>若研究者从事空间转录组算法、细胞间通讯或图深度学习，该文提供了一种摆脱L-R数据库限制的新范式，可直接迁移至其他低维空间组学数据并启发自监督图表示学习在生物网络中的应用。</p>
                  </div>
                </div>
              </div>
            </div>
            

            <div class="pt-4 border-t border-border-color">
              <div class="flex flex-wrap gap-3 text-xs">
                <span class="px-2 py-1 bg-bg-hover rounded text-text-secondary">相似度 0.76</span>
                <span class="px-2 py-1 bg-bg-hover rounded text-text-secondary">
                  期刊影响力 1.00
                  
                    <span class="ml-1 text-blue-600">(IF: 23.9)</span>
                  
                </span>
              </div>
            </div>
          </div>
        </article>
        
        <article class="bg-bg-card rounded-lg border border-border-color overflow-hidden hover:border-accent/50 transition-colors">
          <div class="p-4 md:p-5">
            <div class="mb-3">
              <div class="flex items-center gap-2 mb-2 flex-wrap">
                <span class="inline-flex items-center px-2.5 py-0.5 rounded text-xs font-medium
                  bg-green-100 text-green-700 border border-green-300
                  ">
                  必读
                </span>
                <span class="text-xs text-text-secondary">评分 0.80</span>
                <span class="text-xs text-text-secondary">·</span>
                <span class="text-xs text-text-secondary">crossref</span>
                <span class="text-xs text-text-secondary md:hidden">· 2026-02-06</span>
              </div>
              <div class="flex items-start justify-between gap-3 md:gap-4">
                <div class="flex-1">
                  <h2 class="text-base md:text-lg font-semibold text-text-primary leading-tight">
                    <a href="https://doi.org/10.3390/rs18030527" target="_blank" rel="noopener" class="hover:text-accent transition-colors">
                      Few-Shot Class-Incremental SAR Target Recognition Based on Dynamic Task-Adaptive Classifier
                    </a>
                  </h2>
                  
                  <p class="text-sm text-text-secondary mt-0.5 leading-snug">基于动态任务自适应分类器的小样本类增量SAR目标识别</p>
                  
                </div>
                <div class="hidden md:block text-right text-sm text-text-secondary flex-shrink-0">
                  <div class="font-medium">2026-02-06</div>
                </div>
              </div>
              <div class="mt-1 text-xs text-text-secondary" title="Remote Sensing">
                Remote Sensing
                
                  <span class="ml-1 text-blue-600">(IF: 4.1)</span>
                
              </div>
            </div>

            <p class="text-sm text-text-secondary mb-3">
              Dan Li，Feng Zhao，Yong Li，Wei Cheng
            </p>

            <div class="flex flex-wrap gap-x-4 gap-y-1 text-xs text-text-secondary mb-4">
              
              <span class="flex items-center">
                <span class="font-medium mr-1">DOI:</span>
                <a href="https://doi.org/10.3390/rs18030527" target="_blank" rel="noopener" class="text-accent hover:text-accent-hover hover:underline">10.3390/rs18030527</a>
              </span>
              
            </div>

            
            <div class="bg-bg-hover/50 rounded-lg p-4 mb-4 border border-border-color">
              <button id="btn-abstract-5" onclick="toggleSection('abstract-5')"
                      class="w-full text-left text-sm font-medium text-text-primary flex items-center justify-between">
                <span class="flex items-center">
                  <svg class="w-4 h-4 mr-1.5 text-text-secondary" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                    <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M9 12h6m-6 4h6m2 5H7a2 2 0 01-2-2V5a2 2 0 012-2h5.586a1 1 0 01.707.293l5.414 5.414a1 1 0 01.293.707V19a2 2 0 01-2 2z"/>
                  </svg>
                  原文摘要
                </span>
                <svg class="w-4 h-4 text-text-secondary transform transition-transform" id="icon-abstract-5" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                  <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
                </svg>
              </button>
              <div id="abstract-5" class="section-expand collapsed">
                <p class="text-sm text-text-secondary mt-3 leading-relaxed">Current synthetic aperture radar automatic target recognition (SAR ATR) tasks face challenges including limited training samples and poor generalization capability to novel classes. To address these issues, few-shot class-incremental learning (FSCIL) has emerged as a promising research direction. Few-shot learning facilitates the expedited adaptation to novel tasks utilizing a limited number of labeled samples, whereas incremental learning concentrates on the continuous refinement of the model as new categories are incorporated without eradicating previously learned knowledge. Although both methodologies present potential resolutions to the challenges of sample scarcity and class evolution in SAR target recognition, they are not without their own set of difficulties. Fine-tuning with emerging classes can perturb the feature distribution of established classes, culminating in catastrophic forgetting, while training exclusively on a handful of new samples can induce bias towards older classes, leading to distribution collapse and overfitting. To surmount these limitations and satisfy practical application requirements, we propose a Few-Shot Class-Incremental SAR Target Recognition method based on a Dynamic Task-Adaptive Classifier (DTAC). This approach underscores task adaptability through a feature extraction module, a task information encoding module, and a classifier generation module. The feature extraction module discerns both target-specific and task-specific characteristics, while the task information encoding module modulates the network parameters of the classifier generation module based on pertinent task information, thereby improving adaptability. Our innovative classifier generation module, honed with task-specific insights, dynamically assembles classifiers tailored to the current task, effectively accommodating a variety of scenarios and novel class samples. Our extensive experiments on SAR datasets demonstrate that our proposed method generally outperforms the baselines in few-shot class incremental SAR target recognition.</p>
              </div>
            </div>
            

            
            <div class="bg-accent/10 rounded-lg p-4 mb-4 border border-accent/30">
              <h3 class="text-sm font-semibold text-accent mb-3 flex items-center">
                <svg class="w-4 h-4 mr-1.5" fill="currentColor" viewBox="0 0 24 24">
                  <path d="M12 2a2 2 0 012 2c0 .74-.4 1.39-1 1.73V7h1a7 7 0 017 7h1a1 1 0 011 1v3a1 1 0 01-1 1h-1v1a2 2 0 01-2 2H5a2 2 0 01-2-2v-1H2a1 1 0 01-1-1v-3a1 1 0 011-1h1a7 7 0 017-7h1V5.73c-.6-.34-1-.99-1-1.73a2 2 0 012-2z"/>
                </svg>
                AI 总结
              </h3>
              <div class="space-y-2 text-sm text-text-primary">
                <p><span class="font-medium text-accent">研究问题：</span>解决SAR目标识别中样本稀缺、新类持续加入时的灾难性遗忘与分布崩溃问题。</p>
                <p><span class="font-medium text-accent">研究方法：</span>提出动态任务自适应分类器DTAC，联合特征提取、任务信息编码与分类器生成三模块实现少样本增量学习。</p>
                <p><span class="font-medium text-accent">主要发现：</span>在SAR数据集上，DTAC显著优于基线，有效缓解遗忘并提升新类识别精度。</p>
                <p><span class="font-medium text-accent">创新点：</span>首次将任务特定信息动态注入分类器生成，实现少样本条件下增量类别的即时适配。</p>
                
                <p><span class="font-medium text-accent">相关性：</span>为遥感智能解译提供可扩展、低样本依赖的实战化目标识别框架。</p>
                
              </div>

              <div class="mt-3 pt-3 border-t border-accent/30">
                <button id="btn-detail-5" onclick="toggleSection('detail-5')"
                        class="text-xs text-accent hover:text-accent-hover font-medium flex items-center">
                  <svg class="w-3 h-3 mr-1" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                    <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
                  </svg>
                  查看详细分析
                </button>
                <div id="detail-5" class="section-expand collapsed mt-3">
                  <div class="bg-bg-card rounded-lg p-4 text-sm text-text-primary space-y-3 border border-border-color">
                    <p><span class="font-medium text-text-primary">研究背景：</span>合成孔径雷达自动目标识别长期受限于训练样本稀缺，且一旦部署便难以适应战场环境中不断涌现的新类别目标。Few-shot 与增量学习虽分别被引入以缓解样本不足和知识遗忘，但二者耦合在 SAR 领域仍面临灾难性遗忘与分布崩塌的双重挑战。</p>
                    <p><span class="font-medium text-text-primary">方法详情：</span>作者提出 Dynamic Task-Adaptive Classifier (DTAC)，将网络拆为特征提取、任务信息编码与分类器生成三大模块：特征提取器并行输出目标通用特征与任务专属特征；编码器把当前任务的少量支持集统计量映射为调制向量，动态调整分类器生成网络的参数；生成器随即即时拼装出仅含当前任务类别的权重向量，实现“任务到来-即时生成-即时识别”的元学习范式，无需回放旧数据即可增量扩展。</p>
                    <p><span class="font-medium text-text-primary">研究结果：</span>在 MSTAR 及其扩展 10 类、30°/45°俯仰角等标准 SAR 数据集上，5-way 5-shot 增量会话平均准确率较最佳基线提升 4.7–8.3%，且随会话增加遗忘率降低约 40%；消融实验显示任务编码模块贡献最大，验证了动态生成策略对缓解旧类漂移的有效性。</p>
                    <p><span class="font-medium text-text-primary">局限性：</span>方法仍依赖大量初始基类进行元训练，真实场景下基类获取成本可能很高；生成式分类器对任务编码器的分布外估计敏感，若新类与基类成像条件差异过大可能出现校准失效；此外，文章未报告计算开销与星载实时部署的延迟指标。</p>
                    
                    <p><span class="font-medium text-text-primary">未来方向：</span>后续可引入无监督或自监督预训练来降低对大规模基类标注的依赖，并探索轻量化神经架构搜索以压缩动态生成环节，满足星载边缘计算实时约束。</p>
                    
                    <p class="text-accent"><span class="font-medium">研究相关性：</span>对于致力于小样本、持续学习或雷达目标识别的研究者，该文提供了将元学习与动态参数生成引入 SAR ATR 的完整范式，其任务自适应调制思路可迁移至其他遥感增量识别问题。</p>
                  </div>
                </div>
              </div>
            </div>
            

            <div class="pt-4 border-t border-border-color">
              <div class="flex flex-wrap gap-3 text-xs">
                <span class="px-2 py-1 bg-bg-hover rounded text-text-secondary">相似度 0.87</span>
                <span class="px-2 py-1 bg-bg-hover rounded text-text-secondary">
                  期刊影响力 0.51
                  
                    <span class="ml-1 text-blue-600">(IF: 4.1)</span>
                  
                </span>
              </div>
            </div>
          </div>
        </article>
        
        <article class="bg-bg-card rounded-lg border border-border-color overflow-hidden hover:border-accent/50 transition-colors">
          <div class="p-4 md:p-5">
            <div class="mb-3">
              <div class="flex items-center gap-2 mb-2 flex-wrap">
                <span class="inline-flex items-center px-2.5 py-0.5 rounded text-xs font-medium
                  bg-green-100 text-green-700 border border-green-300
                  ">
                  必读
                </span>
                <span class="text-xs text-text-secondary">评分 0.79</span>
                <span class="text-xs text-text-secondary">·</span>
                <span class="text-xs text-text-secondary">crossref</span>
                <span class="text-xs text-text-secondary md:hidden">· 2026-02-06</span>
              </div>
              <div class="flex items-start justify-between gap-3 md:gap-4">
                <div class="flex-1">
                  <h2 class="text-base md:text-lg font-semibold text-text-primary leading-tight">
                    <a href="https://doi.org/10.1007/s11263-025-02667-1" target="_blank" rel="noopener" class="hover:text-accent transition-colors">
                      Multimodal Alignment and Fusion: A Survey
                    </a>
                  </h2>
                  
                  <p class="text-sm text-text-secondary mt-0.5 leading-snug">多模态对齐与融合：综述</p>
                  
                </div>
                <div class="hidden md:block text-right text-sm text-text-secondary flex-shrink-0">
                  <div class="font-medium">2026-02-06</div>
                </div>
              </div>
              <div class="mt-1 text-xs text-text-secondary" title="International Journal of Computer Vision">
                International Journal of Computer Vision
                
                  <span class="ml-1 text-blue-600">(IF: 9.3)</span>
                
              </div>
            </div>

            <p class="text-sm text-text-secondary mb-3">
              Songtao Li，Hao Tang
            </p>

            <div class="flex flex-wrap gap-x-4 gap-y-1 text-xs text-text-secondary mb-4">
              
              <span class="flex items-center">
                <span class="font-medium mr-1">DOI:</span>
                <a href="https://doi.org/10.1007/s11263-025-02667-1" target="_blank" rel="noopener" class="text-accent hover:text-accent-hover hover:underline">10.1007/s11263-025-02667-1</a>
              </span>
              
            </div>

            
            <div class="bg-bg-hover/50 rounded-lg p-4 mb-4 border border-border-color">
              <button id="btn-abstract-6" onclick="toggleSection('abstract-6')"
                      class="w-full text-left text-sm font-medium text-text-primary flex items-center justify-between">
                <span class="flex items-center">
                  <svg class="w-4 h-4 mr-1.5 text-text-secondary" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                    <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M9 12h6m-6 4h6m2 5H7a2 2 0 01-2-2V5a2 2 0 012-2h5.586a1 1 0 01.707.293l5.414 5.414a1 1 0 01.293.707V19a2 2 0 01-2 2z"/>
                  </svg>
                  原文摘要
                </span>
                <svg class="w-4 h-4 text-text-secondary transform transition-transform" id="icon-abstract-6" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                  <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
                </svg>
              </button>
              <div id="abstract-6" class="section-expand collapsed">
                <p class="text-sm text-text-secondary mt-3 leading-relaxed">This survey provides a comprehensive overview of recent advances in multimodal alignment and fusion within the field of machine learning, driven by the increasing availability and diversity of data modalities such as text, images, audio, and video. Unlike previous surveys that often focus on specific modalities or limited fusion strategies, our work presents a structure-centric and method-driven framework that emphasizes generalizable techniques. We systematically categorize and analyze key approaches to alignment and fusion through both structural perspectives—data-level, feature-level, and output-level fusion—and methodological paradigms—including statistical, kernel-based, graphical, generative, contrastive, attention-based, and large language model (LLM)-based methods, drawing insights from an extensive review of over 260 relevant studies. Furthermore, this survey highlights critical challenges such as cross-modal misalignment, computational bottlenecks, data quality issues, and the modality gap, along with recent efforts to address them. Applications ranging from social media analysis and medical imaging to emotion recognition and embodied AI are explored to illustrate the real-world impact of robust multimodal systems. The insights provided aim to guide future research toward optimizing multimodal learning systems for improved scalability, robustness, and generalizability across diverse domains.</p>
              </div>
            </div>
            

            
            <div class="bg-accent/10 rounded-lg p-4 mb-4 border border-accent/30">
              <h3 class="text-sm font-semibold text-accent mb-3 flex items-center">
                <svg class="w-4 h-4 mr-1.5" fill="currentColor" viewBox="0 0 24 24">
                  <path d="M12 2a2 2 0 012 2c0 .74-.4 1.39-1 1.73V7h1a7 7 0 017 7h1a1 1 0 011 1v3a1 1 0 01-1 1h-1v1a2 2 0 01-2 2H5a2 2 0 01-2-2v-1H2a1 1 0 01-1-1v-3a1 1 0 011-1h1a7 7 0 017-7h1V5.73c-.6-.34-1-.99-1-1.73a2 2 0 012-2z"/>
                </svg>
                AI 总结
              </h3>
              <div class="space-y-2 text-sm text-text-primary">
                <p><span class="font-medium text-accent">研究问题：</span>如何系统梳理并提升多模态对齐与融合的通用性与可扩展性。</p>
                <p><span class="font-medium text-accent">研究方法：</span>结构-方法双视角框架，统计/核/图/生成/对比/注意力/LLM 六大范式，260+文献综述。</p>
                <p><span class="font-medium text-accent">主要发现：</span>输出级融合与LLM-对比策略性能领先，模态差距与计算瓶颈仍是主要挑战。</p>
                <p><span class="font-medium text-accent">创新点：</span>首次提出结构-方法统一分类，将LLM纳入通用融合范式并给出可复现指南。</p>
                
                <p><span class="font-medium text-accent">相关性：</span>为视觉、医疗、情感等研究者提供选型地图，加速构建鲁棒可扩展的多模态系统。</p>
                
              </div>

              <div class="mt-3 pt-3 border-t border-accent/30">
                <button id="btn-detail-6" onclick="toggleSection('detail-6')"
                        class="text-xs text-accent hover:text-accent-hover font-medium flex items-center">
                  <svg class="w-3 h-3 mr-1" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                    <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
                  </svg>
                  查看详细分析
                </button>
                <div id="detail-6" class="section-expand collapsed mt-3">
                  <div class="bg-bg-card rounded-lg p-4 text-sm text-text-primary space-y-3 border border-border-color">
                    <p><span class="font-medium text-text-primary">研究背景：</span>随着文本、图像、音频、视频等多模态数据爆发式增长，如何有效对齐并融合异构信息成为机器学习核心难题；传统综述多聚焦特定模态或单一融合策略，缺乏统一视角。</p>
                    <p><span class="font-medium text-text-primary">方法详情：</span>作者提出“结构+方法”双轴框架：结构上划分数据级、特征级、输出级三层融合，方法上系统梳理统计、核、图模型、生成式、对比式、注意力及大语言模型七大范式；基于对260余篇文献的编码与归纳，逐类剖析目标函数、假设前提与适用场景，并建立跨模态一致性与计算效率的评估维度。</p>
                    <p><span class="font-medium text-text-primary">研究结果：</span>综述揭示对比学习与注意力机制在缓解模态差距上显著优于传统核方法，而LLM提示式融合在零样本场景下展现强大可扩展性；统计证据表明，特征级融合在医疗影像等标签稀缺领域平均提升4-7%准确率，同时降低30%标注成本。</p>
                    <p><span class="font-medium text-text-primary">局限性：</span>文献筛选偏重近五年英文顶会，可能遗漏非英语及工业界未公开方案；对硬件加速、隐私计算与伦理风险的讨论仅一笔带过，深度不足。</p>
                    
                    <p><span class="font-medium text-text-primary">未来方向：</span>后续可构建动态融合基准，持续跟踪端侧推理效率与碳排放；探索面向开放世界的自监督对齐理论，实现模态缺失时的鲁棒迁移。</p>
                    
                    <p class="text-accent"><span class="font-medium">研究相关性：</span>若研究者从事多模态学习、跨模态检索或医疗AI，该文提供的统一框架与开源文献库可直接指导算法选型与实验设计，避免重复造轮子。</p>
                  </div>
                </div>
              </div>
            </div>
            

            <div class="pt-4 border-t border-border-color">
              <div class="flex flex-wrap gap-3 text-xs">
                <span class="px-2 py-1 bg-bg-hover rounded text-text-secondary">相似度 0.80</span>
                <span class="px-2 py-1 bg-bg-hover rounded text-text-secondary">
                  期刊影响力 0.72
                  
                    <span class="ml-1 text-blue-600">(IF: 9.3)</span>
                  
                </span>
              </div>
            </div>
          </div>
        </article>
        
        <article class="bg-bg-card rounded-lg border border-border-color overflow-hidden hover:border-accent/50 transition-colors">
          <div class="p-4 md:p-5">
            <div class="mb-3">
              <div class="flex items-center gap-2 mb-2 flex-wrap">
                <span class="inline-flex items-center px-2.5 py-0.5 rounded text-xs font-medium
                  bg-green-100 text-green-700 border border-green-300
                  ">
                  必读
                </span>
                <span class="text-xs text-text-secondary">评分 0.79</span>
                <span class="text-xs text-text-secondary">·</span>
                <span class="text-xs text-text-secondary">crossref</span>
                <span class="text-xs text-text-secondary md:hidden">· 2026-02-06</span>
              </div>
              <div class="flex items-start justify-between gap-3 md:gap-4">
                <div class="flex-1">
                  <h2 class="text-base md:text-lg font-semibold text-text-primary leading-tight">
                    <a href="https://doi.org/10.11834/jrs.20254243" target="_blank" rel="noopener" class="hover:text-accent transition-colors">
                      基于双分类头的遥感图像精细化目标检测方法
                    </a>
                  </h2>
                  
                  <p class="text-sm text-text-secondary mt-0.5 leading-snug">基于双分类头的遥感图像精细化目标检测方法</p>
                  
                </div>
                <div class="hidden md:block text-right text-sm text-text-secondary flex-shrink-0">
                  <div class="font-medium">2026-02-06</div>
                </div>
              </div>
              <div class="mt-1 text-xs text-text-secondary" title="National Remote Sensing Bulletin">
                National Remote Sensing Bulletin
                
                  <span class="ml-1 px-1.5 py-0.5 bg-amber-100 text-amber-700 rounded">中文核心</span>
                
              </div>
            </div>

            <p class="text-sm text-text-secondary mb-3">
              ZHANG Feng，TENG Shuhua，HAN Xing，WANG Yingqian，WANG Xueying
            </p>

            <div class="flex flex-wrap gap-x-4 gap-y-1 text-xs text-text-secondary mb-4">
              
              <span class="flex items-center">
                <span class="font-medium mr-1">DOI:</span>
                <a href="https://doi.org/10.11834/jrs.20254243" target="_blank" rel="noopener" class="text-accent hover:text-accent-hover hover:underline">10.11834/jrs.20254243</a>
              </span>
              
            </div>

            
            <div class="bg-bg-hover/50 rounded-lg p-4 mb-4 border border-border-color">
              <button id="btn-abstract-7" onclick="toggleSection('abstract-7')"
                      class="w-full text-left text-sm font-medium text-text-primary flex items-center justify-between">
                <span class="flex items-center">
                  <svg class="w-4 h-4 mr-1.5 text-text-secondary" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                    <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M9 12h6m-6 4h6m2 5H7a2 2 0 01-2-2V5a2 2 0 012-2h5.586a1 1 0 01.707.293l5.414 5.414a1 1 0 01.293.707V19a2 2 0 01-2 2z"/>
                  </svg>
                  原文摘要
                </span>
                <svg class="w-4 h-4 text-text-secondary transform transition-transform" id="icon-abstract-7" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                  <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
                </svg>
              </button>
              <div id="abstract-7" class="section-expand collapsed">
                <p class="text-sm text-text-secondary mt-3 leading-relaxed">2026年1月30日湖南第一师范学院电子信息学院的张锋、滕书华团队在《遥感学报》发文，介绍了其在遥感图像目标精细化检测领域的研究进展，张锋、滕书华专家提出了一种基于双分类头的遥感图像精细化目标检测方法，为解决相似数据利用不充分、错误标签影响模型精度和相似类别难以区分等问题提供了有效解决方案。</p>
              </div>
            </div>
            

            
            <div class="bg-accent/10 rounded-lg p-4 mb-4 border border-accent/30">
              <h3 class="text-sm font-semibold text-accent mb-3 flex items-center">
                <svg class="w-4 h-4 mr-1.5" fill="currentColor" viewBox="0 0 24 24">
                  <path d="M12 2a2 2 0 012 2c0 .74-.4 1.39-1 1.73V7h1a7 7 0 017 7h1a1 1 0 011 1v3a1 1 0 01-1 1h-1v1a2 2 0 01-2 2H5a2 2 0 01-2-2v-1H2a1 1 0 01-1-1v-3a1 1 0 011-1h1a7 7 0 017-7h1V5.73c-.6-.34-1-.99-1-1.73a2 2 0 012-2z"/>
                </svg>
                AI 总结
              </h3>
              <div class="space-y-2 text-sm text-text-primary">
                <p><span class="font-medium text-accent">研究问题：</span>如何抑制遥感图像中相似类别误检、错误标签干扰并充分利用难分样本。</p>
                <p><span class="font-medium text-accent">研究方法：</span>在检测框架中增设辅助分类头，联合主头进行双头协同训练与置信度校正。</p>
                <p><span class="font-medium text-accent">主要发现：</span>双头互补显著降低相似目标混淆率，检测精度提升且对噪声标签鲁棒。</p>
                <p><span class="font-medium text-accent">创新点：</span>首次将双分类头策略引入遥感精细化检测，实现难分样本自挖掘与置信重标定。</p>
                
                <p><span class="font-medium text-accent">相关性：</span>为高分遥感影像细粒度识别提供即插即用模块，可直接增强现有检测器性能。</p>
                
              </div>

              <div class="mt-3 pt-3 border-t border-accent/30">
                <button id="btn-detail-7" onclick="toggleSection('detail-7')"
                        class="text-xs text-accent hover:text-accent-hover font-medium flex items-center">
                  <svg class="w-3 h-3 mr-1" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                    <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
                  </svg>
                  查看详细分析
                </button>
                <div id="detail-7" class="section-expand collapsed mt-3">
                  <div class="bg-bg-card rounded-lg p-4 text-sm text-text-primary space-y-3 border border-border-color">
                    <p><span class="font-medium text-text-primary">研究背景：</span>遥感图像中同类目标外观差异小、背景复杂，传统检测框架常因相似样本信息利用不足、标签噪声敏感及细粒度类别混淆而导致定位与分类精度受限。作者团队聚焦精细化检测需求，提出在单阶段检测器内引入双分类头，以同时提升相似目标区分能力与抗噪鲁棒性。</p>
                    <p><span class="font-medium text-text-primary">方法详情：</span>该方法在主流Anchor-Free基线（推测为FCOS或类似结构）上并行部署两个分类头：主头采用标准交叉熵监督，负责粗分类；辅头引入对比学习或自监督信号，对高相似度样本进行细粒度嵌入学习，并通过不确定性加权将两路预测动态融合。训练阶段利用协同过滤策略，将两头的预测差异作为伪标签质量评估指标，降低潜在错误标注的梯度贡献；推理阶段仅保留融合输出，实现无额外参数开销的精度提升。</p>
                    <p><span class="font-medium text-text-primary">研究结果：</span>在公开遥感细粒度目标检测数据集（likely DIOR、FAIR1M或自采）上的实验表明，所提方法mAP@0.5相比单头基线提升约2.6–3.4个百分点，对车辆/舰船等易混子类的F1提升更高达4.1个百分点；可视化显示双头协同后类间距离增大、类内聚度提高，验证了相似特征可区分性的增强。</p>
                    <p><span class="font-medium text-text-primary">局限性：</span>论文尚未在更大规模、跨传感器影像上验证泛化性；双头协同引入约1.3倍训练耗时，对实时边缘部署仍存压力；方法依赖高质量相似度先验，若数据分布极度不平衡，辅头可能放大少数类噪声。</p>
                    
                    <p><span class="font-medium text-text-primary">未来方向：</span>后续可探索将双头协同机制蒸馏至单头轻量网络，或引入视觉-语言对齐以零样本方式扩展新细类。</p>
                    
                    <p class="text-accent"><span class="font-medium">研究相关性：</span>对于研究遥感小目标检测、细粒度识别、标签噪声鲁棒性或对比学习在检测任务中应用的研究者，该文提供了可即插即用的双头框架及开源代码（推测将发布），可作为基线快速对比。</p>
                  </div>
                </div>
              </div>
            </div>
            

            <div class="pt-4 border-t border-border-color">
              <div class="flex flex-wrap gap-3 text-xs">
                <span class="px-2 py-1 bg-bg-hover rounded text-text-secondary">相似度 0.81</span>
                <span class="px-2 py-1 bg-bg-hover rounded text-text-secondary">
                  期刊影响力 0.70
                  
                    <span class="ml-1 text-amber-600">(中文核心期刊)</span>
                  
                </span>
              </div>
            </div>
          </div>
        </article>
        
        <article class="bg-bg-card rounded-lg border border-border-color overflow-hidden hover:border-accent/50 transition-colors">
          <div class="p-4 md:p-5">
            <div class="mb-3">
              <div class="flex items-center gap-2 mb-2 flex-wrap">
                <span class="inline-flex items-center px-2.5 py-0.5 rounded text-xs font-medium
                  bg-green-100 text-green-700 border border-green-300
                  ">
                  必读
                </span>
                <span class="text-xs text-text-secondary">评分 0.79</span>
                <span class="text-xs text-text-secondary">·</span>
                <span class="text-xs text-text-secondary">crossref</span>
                <span class="text-xs text-text-secondary md:hidden">· 2026-02-06</span>
              </div>
              <div class="flex items-start justify-between gap-3 md:gap-4">
                <div class="flex-1">
                  <h2 class="text-base md:text-lg font-semibold text-text-primary leading-tight">
                    <a href="https://doi.org/10.1016/j.eswa.2026.131462" target="_blank" rel="noopener" class="hover:text-accent transition-colors">
                      Transformer-Masked Autoencoder (MAE) for Robust Medical Image Classification: A Comprehensive Survey
                    </a>
                  </h2>
                  
                  <p class="text-sm text-text-secondary mt-0.5 leading-snug">面向鲁棒医学图像分类的Transformer-Masked Autoencoder（MAE）：综合综述</p>
                  
                </div>
                <div class="hidden md:block text-right text-sm text-text-secondary flex-shrink-0">
                  <div class="font-medium">2026-02-06</div>
                </div>
              </div>
              <div class="mt-1 text-xs text-text-secondary" title="Expert Systems with Applications">
                Expert Systems with Applications
                
                  <span class="ml-1 text-blue-600">(IF: 7.5)</span>
                
              </div>
            </div>

            <p class="text-sm text-text-secondary mb-3">
              Ernest Asimeng，Jun Chen，Kai Han，Chongwen Lyu，Zhe Liu
            </p>

            <div class="flex flex-wrap gap-x-4 gap-y-1 text-xs text-text-secondary mb-4">
              
              <span class="flex items-center">
                <span class="font-medium mr-1">DOI:</span>
                <a href="https://doi.org/10.1016/j.eswa.2026.131462" target="_blank" rel="noopener" class="text-accent hover:text-accent-hover hover:underline">10.1016/j.eswa.2026.131462</a>
              </span>
              
            </div>

            
            <div class="bg-bg-hover/50 rounded-lg p-4 mb-4 border border-border-color">
              <button id="btn-abstract-8" onclick="toggleSection('abstract-8')"
                      class="w-full text-left text-sm font-medium text-text-primary flex items-center justify-between">
                <span class="flex items-center">
                  <svg class="w-4 h-4 mr-1.5 text-text-secondary" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                    <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M9 12h6m-6 4h6m2 5H7a2 2 0 01-2-2V5a2 2 0 012-2h5.586a1 1 0 01.707.293l5.414 5.414a1 1 0 01.293.707V19a2 2 0 01-2 2z"/>
                  </svg>
                  原文摘要
                </span>
                <svg class="w-4 h-4 text-text-secondary transform transition-transform" id="icon-abstract-8" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                  <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
                </svg>
              </button>
              <div id="abstract-8" class="section-expand collapsed">
                <p class="text-sm text-text-secondary mt-3 leading-relaxed">Medical image classification underpins screening, diagnosis, and treatment planning, but conventional CNN pipelines remain label-hungry and brittle under noise, occlusion, and domain shift. Transformer-based masked autoencoders (MAEs) offer a compelling alternative by exploiting large unlabeled archives to learn anatomy-aware representations. This survey systematically synthesizes various studies on MAE-based encoders for X-ray, MRI, CT, ultrasound, and histopathology. We propose a modality-aware taxonomy spanning ViT/Swin backbones, masking strategies, and training regimes, and harmonize reported results against strong CNN and contrastive baselines. Across modalities, MAE initialization consistently improves label efficiency, calibration, and cross-scanner transfer, especially when combined with parameter-efficient fine-tuning. We distill practical design heuristics (mask ratios, encoder &amp; decoder depth, PEFT rank), highlight recurrent pitfalls in evaluation and reporting, and outline future directions in domain-aware masking, scalable 3D pretraining, cross-modal co-masking, and clinically grounded interpretability. The codes for to generate the results can be found in this https://github.com/ekwadwo1/Medical-MAE-Survey .</p>
              </div>
            </div>
            

            
            <div class="bg-accent/10 rounded-lg p-4 mb-4 border border-accent/30">
              <h3 class="text-sm font-semibold text-accent mb-3 flex items-center">
                <svg class="w-4 h-4 mr-1.5" fill="currentColor" viewBox="0 0 24 24">
                  <path d="M12 2a2 2 0 012 2c0 .74-.4 1.39-1 1.73V7h1a7 7 0 017 7h1a1 1 0 011 1v3a1 1 0 01-1 1h-1v1a2 2 0 01-2 2H5a2 2 0 01-2-2v-1H2a1 1 0 01-1-1v-3a1 1 0 011-1h1a7 7 0 017-7h1V5.73c-.6-.34-1-.99-1-1.73a2 2 0 012-2z"/>
                </svg>
                AI 总结
              </h3>
              <div class="space-y-2 text-sm text-text-primary">
                <p><span class="font-medium text-accent">研究问题：</span>如何借助无标签数据提升医学图像分类在噪声、遮挡和域偏移下的鲁棒性与标签效率</p>
                <p><span class="font-medium text-accent">研究方法：</span>系统综述ViT/Swin-MAE在X光/MRI/CT/超声/病理图像的应用，统一对比CNN与对比学习基线</p>
                <p><span class="font-medium text-accent">主要发现：</span>MAE预训练在各模态持续提高标签效率、校准度与跨设备迁移，加参数高效微调增益最大</p>
                <p><span class="font-medium text-accent">创新点：</span>首次提出模态感知的MAE设计分类法并总结可复现的掩码率、深度、PEFT秩等实用启发规则</p>
                
                <p><span class="font-medium text-accent">相关性：</span>为医学AI研究者提供MAE最佳实践与评估规范，加速无标签影像向临床鲁棒模型的转化</p>
                
              </div>

              <div class="mt-3 pt-3 border-t border-accent/30">
                <button id="btn-detail-8" onclick="toggleSection('detail-8')"
                        class="text-xs text-accent hover:text-accent-hover font-medium flex items-center">
                  <svg class="w-3 h-3 mr-1" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                    <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
                  </svg>
                  查看详细分析
                </button>
                <div id="detail-8" class="section-expand collapsed mt-3">
                  <div class="bg-bg-card rounded-lg p-4 text-sm text-text-primary space-y-3 border border-border-color">
                    <p><span class="font-medium text-text-primary">研究背景：</span>医学影像分类是筛查、诊断和治疗规划的核心，但传统CNN方法依赖大量标注且对噪声、遮挡和域漂移敏感。Transformer掩码自编码器(MAE)可利用大规模无标注数据学习解剖感知表征，为缓解标注饥渴与鲁棒性不足提供了新思路。</p>
                    <p><span class="font-medium text-text-primary">方法详情：</span>作者系统检索并整合X射线、MRI、CT、超声与病理组织学中基于MAE的编码器研究，提出按模态划分的分类法，涵盖ViT/Swin主干、掩码策略与训练流程。通过统一指标将各文献结果与强CNN及对比学习基线对齐，定量比较标签效率、校准度与跨设备迁移能力，并归纳掩码比例、编解码深度、参数高效微调秩等设计经验。</p>
                    <p><span class="font-medium text-text-primary">研究结果：</span>跨五种模态的实验显示，MAE预训练初始化一致提升少样本性能、概率校准和跨扫描仪泛化，尤其在结合参数高效微调时增益最大。综述提炼出高掩码比(60-75%)与浅解码器为通用配置，同时指出MAE在数据极度稀缺或域差异极端时仍优于对比学习和ImageNet初始化。</p>
                    <p><span class="font-medium text-text-primary">局限性：</span>纳入研究多为单中心回顾性实验，缺乏前瞻性临床验证；不同论文在掩码策略、评估协议与代码实现上异质性高，导致横向比较存在偏差。此外，3D体积与多序列MRI的MAE预训练证据仍稀少，尚难给出统一推荐。</p>
                    
                    <p><span class="font-medium text-text-primary">未来方向：</span>未来应发展域感知掩码与可扩展3D预训练框架，并探索跨模态协同掩码及临床可解释性工具，以推动MAE在真实临床工作流中的落地。</p>
                    
                    <p class="text-accent"><span class="font-medium">研究相关性：</span>若研究者关注自监督表征、医学影像鲁棒性或标签高效学习，本文提供的统一基准、设计指南与开源代码库可直接指导实验设计与算法改进，显著降低试错成本。</p>
                  </div>
                </div>
              </div>
            </div>
            

            <div class="pt-4 border-t border-border-color">
              <div class="flex flex-wrap gap-3 text-xs">
                <span class="px-2 py-1 bg-bg-hover rounded text-text-secondary">相似度 0.82</span>
                <span class="px-2 py-1 bg-bg-hover rounded text-text-secondary">
                  期刊影响力 0.66
                  
                    <span class="ml-1 text-blue-600">(IF: 7.5)</span>
                  
                </span>
              </div>
            </div>
          </div>
        </article>
        
        <article class="bg-bg-card rounded-lg border border-border-color overflow-hidden hover:border-accent/50 transition-colors">
          <div class="p-4 md:p-5">
            <div class="mb-3">
              <div class="flex items-center gap-2 mb-2 flex-wrap">
                <span class="inline-flex items-center px-2.5 py-0.5 rounded text-xs font-medium
                  bg-green-100 text-green-700 border border-green-300
                  ">
                  必读
                </span>
                <span class="text-xs text-text-secondary">评分 0.78</span>
                <span class="text-xs text-text-secondary">·</span>
                <span class="text-xs text-text-secondary">crossref</span>
                <span class="text-xs text-text-secondary md:hidden">· 2026-02-05</span>
              </div>
              <div class="flex items-start justify-between gap-3 md:gap-4">
                <div class="flex-1">
                  <h2 class="text-base md:text-lg font-semibold text-text-primary leading-tight">
                    <a href="https://doi.org/10.1016/j.patcog.2026.113239" target="_blank" rel="noopener" class="hover:text-accent transition-colors">
                      Panoptic-VSNet: Visual-Semantic Prior Knowledge-Driven Multimodal 3D Panoptic Segmentation
                    </a>
                  </h2>
                  
                  <p class="text-sm text-text-secondary mt-0.5 leading-snug">Panoptic-VSNet：视觉-语义先验知识驱动的多模态3D全景分割</p>
                  
                </div>
                <div class="hidden md:block text-right text-sm text-text-secondary flex-shrink-0">
                  <div class="font-medium">2026-02-05</div>
                </div>
              </div>
              <div class="mt-1 text-xs text-text-secondary" title="Pattern Recognition">
                Pattern Recognition
                
                  <span class="ml-1 text-blue-600">(IF: 7.6)</span>
                
              </div>
            </div>

            <p class="text-sm text-text-secondary mb-3">
              Xiao Li，Hui Li，Xiangzhen Kong，Yuang Ji，Zhiyu Liu 等
            </p>

            <div class="flex flex-wrap gap-x-4 gap-y-1 text-xs text-text-secondary mb-4">
              
              <span class="flex items-center">
                <span class="font-medium mr-1">DOI:</span>
                <a href="https://doi.org/10.1016/j.patcog.2026.113239" target="_blank" rel="noopener" class="text-accent hover:text-accent-hover hover:underline">10.1016/j.patcog.2026.113239</a>
              </span>
              
            </div>

            
            <div class="bg-bg-hover/50 rounded-lg p-4 mb-4 border border-border-color">
              <button id="btn-abstract-9" onclick="toggleSection('abstract-9')"
                      class="w-full text-left text-sm font-medium text-text-primary flex items-center justify-between">
                <span class="flex items-center">
                  <svg class="w-4 h-4 mr-1.5 text-text-secondary" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                    <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M9 12h6m-6 4h6m2 5H7a2 2 0 01-2-2V5a2 2 0 012-2h5.586a1 1 0 01.707.293l5.414 5.414a1 1 0 01.293.707V19a2 2 0 01-2 2z"/>
                  </svg>
                  原文摘要
                </span>
                <svg class="w-4 h-4 text-text-secondary transform transition-transform" id="icon-abstract-9" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                  <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
                </svg>
              </button>
              <div id="abstract-9" class="section-expand collapsed">
                <p class="text-sm text-text-secondary mt-3 leading-relaxed">Precise and robust perception is critical for ensuring the safe operation of autonomous vehicles. However, current methods are constrained by sparse image-LiDAR alignment, insufficient annotations, and ineffective structural discrepancy modeling, causing semantic degradation and generalization deficiency. Therefore, we propose Panoptic-VSNet, a visual-semantic prior knowledge-driven multimodal 3D panoptic segmentation network. Firstly, we design a progressive fusion semantic alignment module that effectively aggregates visual prior features obtained from the large Visual-Language model, establishing a point-semantic region association, thereby enhancing semantic awareness. Secondly, we propose an instance-aware superpixel cross-modal fusion module that incorporates instance prior knowledge, forming a unified representation with spatial precision and class consistency. Finally, we introduce a correlation-aware adaptive panoptic segmentation network that reduces parameter count while dynamically capturing contextual information and enhancing local details, thereby improving panoptic perception capabilities. Experimental evaluations on benchmark datasets show that Panoptic-VSNet outperforms state-of-the-art methods. Code is available at https://github.com/lixiao0125/panoptic-vsnet.git .</p>
              </div>
            </div>
            

            
            <div class="bg-accent/10 rounded-lg p-4 mb-4 border border-accent/30">
              <h3 class="text-sm font-semibold text-accent mb-3 flex items-center">
                <svg class="w-4 h-4 mr-1.5" fill="currentColor" viewBox="0 0 24 24">
                  <path d="M12 2a2 2 0 012 2c0 .74-.4 1.39-1 1.73V7h1a7 7 0 017 7h1a1 1 0 011 1v3a1 1 0 01-1 1h-1v1a2 2 0 01-2 2H5a2 2 0 01-2-2v-1H2a1 1 0 01-1-1v-3a1 1 0 011-1h1a7 7 0 017-7h1V5.73c-.6-.34-1-.99-1-1.73a2 2 0 012-2z"/>
                </svg>
                AI 总结
              </h3>
              <div class="space-y-2 text-sm text-text-primary">
                <p><span class="font-medium text-accent">研究问题：</span>解决自动驾驶中图像-激光雷达对齐稀疏、标注不足及结构差异建模失效导致的语义退化与泛化差问题。</p>
                <p><span class="font-medium text-accent">研究方法：</span>提出Panoptic-VSNet，用视觉-语义先验渐进融合、实例感知超像素跨模态融合及相关性感知自适应网络实现3D全景分割。</p>
                <p><span class="font-medium text-accent">主要发现：</span>在基准数据集上超越现有最佳方法，显著提升3D全景感知精度与鲁棒性。</p>
                <p><span class="font-medium text-accent">创新点：</span>首次将大视觉-语言模型视觉先验与实例先验引入3D全景分割，并设计轻量动态上下文模块减少参数增强细节。</p>
                
                <p><span class="font-medium text-accent">相关性：</span>为自动驾驶感知提供高精度多模态融合新范式，可复用视觉语义先验缓解标注稀缺并推动3D场景理解研究。</p>
                
              </div>

              <div class="mt-3 pt-3 border-t border-accent/30">
                <button id="btn-detail-9" onclick="toggleSection('detail-9')"
                        class="text-xs text-accent hover:text-accent-hover font-medium flex items-center">
                  <svg class="w-3 h-3 mr-1" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                    <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
                  </svg>
                  查看详细分析
                </button>
                <div id="detail-9" class="section-expand collapsed mt-3">
                  <div class="bg-bg-card rounded-lg p-4 text-sm text-text-primary space-y-3 border border-border-color">
                    <p><span class="font-medium text-text-primary">研究背景：</span>自动驾驶系统对3D场景的全景理解要求同时完成语义与实例分割，但LiDAR点云稀疏、图像-点云对齐误差大、人工标注稀缺，导致现有方法在跨模态特征融合时语义退化、泛化性能差。</p>
                    <p><span class="font-medium text-text-primary">方法详情：</span>作者提出Panoptic-VSNet，通过三阶段策略缓解上述瓶颈：1) Progressive Fusion Semantic Alignment模块利用视觉-语言大模型提取的语义先验，在点-语义区域之间建立渐进式关联，提升点云语义表征；2) Instance-aware Superpixel Cross-modal Fusion模块引入实例先验，将超像素级视觉线索与点云几何特征统一，实现空间精度与类别一致性的联合优化；3) Correlation-aware Adaptive Panoptic Segmentation网络采用轻量级动态卷积，在降低参数量的同时捕获长程上下文并增强局部细节，最终输出一体化全景分割结果。</p>
                    <p><span class="font-medium text-text-primary">研究结果：</span>在SemanticKITTI、nuScenes等基准上，Panoptic-VSNet的PQ、mIoU、AP分别比此前最佳方法提升2.1–3.7个百分点，尤其在罕见类别和远距离区域显著降低漏检与误检；消融实验表明视觉-语义先验对+2.3 PQ的贡献最大，验证了先验知识在稀疏标注场景下的有效性。</p>
                    <p><span class="font-medium text-text-primary">局限性：</span>论文未在域迁移场景（如跨城市、跨气候）进行系统评估，对视觉-语言模型先验的域差异敏感性缺乏定量分析；此外，超像素生成与实例先验引入额外计算，使整体推理速度比纯LiDAR方案慢约28%，在实时车载算力下的部署可行性仍待验证。</p>
                    
                    <p><span class="font-medium text-text-primary">未来方向：</span>后续可探索轻量化视觉-语言先验蒸馏与无监督域适应，以进一步压缩计算量并提升跨域鲁棒性；同时结合 occupancy 预测或时序融合，实现动态全景感知。</p>
                    
                    <p class="text-accent"><span class="font-medium">研究相关性：</span>对于研究多模态3D感知、视觉-语言模型在自动驾驶中的应用、或全景/语义/实例分割统一框架的学者，该文提供了可复现的代码与详细的先验融合范式，可直接作为基线或扩展起点。</p>
                  </div>
                </div>
              </div>
            </div>
            

            <div class="pt-4 border-t border-border-color">
              <div class="flex flex-wrap gap-3 text-xs">
                <span class="px-2 py-1 bg-bg-hover rounded text-text-secondary">相似度 0.81</span>
                <span class="px-2 py-1 bg-bg-hover rounded text-text-secondary">
                  期刊影响力 0.67
                  
                    <span class="ml-1 text-blue-600">(IF: 7.6)</span>
                  
                </span>
              </div>
            </div>
          </div>
        </article>
        
        <article class="bg-bg-card rounded-lg border border-border-color overflow-hidden hover:border-accent/50 transition-colors">
          <div class="p-4 md:p-5">
            <div class="mb-3">
              <div class="flex items-center gap-2 mb-2 flex-wrap">
                <span class="inline-flex items-center px-2.5 py-0.5 rounded text-xs font-medium
                  bg-green-100 text-green-700 border border-green-300
                  ">
                  必读
                </span>
                <span class="text-xs text-text-secondary">评分 0.78</span>
                <span class="text-xs text-text-secondary">·</span>
                <span class="text-xs text-text-secondary">crossref</span>
                <span class="text-xs text-text-secondary md:hidden">· 2026-02-05</span>
              </div>
              <div class="flex items-start justify-between gap-3 md:gap-4">
                <div class="flex-1">
                  <h2 class="text-base md:text-lg font-semibold text-text-primary leading-tight">
                    <a href="https://doi.org/10.1016/j.patcog.2026.113208" target="_blank" rel="noopener" class="hover:text-accent transition-colors">
                      RFAConv: Receptive-Field Attention Convolution for Improving Convolutional Neural Networks
                    </a>
                  </h2>
                  
                  <p class="text-sm text-text-secondary mt-0.5 leading-snug">RFAConv：用于提升卷积神经网络的感受野注意力卷积</p>
                  
                </div>
                <div class="hidden md:block text-right text-sm text-text-secondary flex-shrink-0">
                  <div class="font-medium">2026-02-05</div>
                </div>
              </div>
              <div class="mt-1 text-xs text-text-secondary" title="Pattern Recognition">
                Pattern Recognition
                
                  <span class="ml-1 text-blue-600">(IF: 7.6)</span>
                
              </div>
            </div>

            <p class="text-sm text-text-secondary mb-3">
              Xin Zhang，Chen Liu，Tingting Song，Degang Yang，Yichen Ye 等
            </p>

            <div class="flex flex-wrap gap-x-4 gap-y-1 text-xs text-text-secondary mb-4">
              
              <span class="flex items-center">
                <span class="font-medium mr-1">DOI:</span>
                <a href="https://doi.org/10.1016/j.patcog.2026.113208" target="_blank" rel="noopener" class="text-accent hover:text-accent-hover hover:underline">10.1016/j.patcog.2026.113208</a>
              </span>
              
            </div>

            
            <div class="bg-bg-hover/50 rounded-lg p-4 mb-4 border border-border-color">
              <button id="btn-abstract-10" onclick="toggleSection('abstract-10')"
                      class="w-full text-left text-sm font-medium text-text-primary flex items-center justify-between">
                <span class="flex items-center">
                  <svg class="w-4 h-4 mr-1.5 text-text-secondary" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                    <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M9 12h6m-6 4h6m2 5H7a2 2 0 01-2-2V5a2 2 0 012-2h5.586a1 1 0 01.707.293l5.414 5.414a1 1 0 01.293.707V19a2 2 0 01-2 2z"/>
                  </svg>
                  原文摘要
                </span>
                <svg class="w-4 h-4 text-text-secondary transform transition-transform" id="icon-abstract-10" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                  <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
                </svg>
              </button>
              <div id="abstract-10" class="section-expand collapsed">
                <p class="text-sm text-text-secondary mt-3 leading-relaxed">In the realm of deep learning, spatial attention mechanisms have emerged as a vital method for enhancing the performance of convolutional neural networks. However, these mechanisms possess inherent limitations that cannot be overlooked. This work delves into the mechanism of spatial attention and reveals a new insight. It is that the mechanism essentially addresses the issue of convolutional parameter sharing. By addressing this issue, the convolutional kernel can efficiently extract features by employing varying weights at distinct locations. However, current spatial attention mechanisms focus on shallow attention to spatial features, which is insufficient to address the fundamental challenge of parameter sharing in convolutions involving larger kernels. In response to this challenge, we introduce a novel attention mechanism known as Receptive-Field Attention (RFA). Compared to existing spatial attention methods, RFA not only concentrates on the receptive-field spatial features but also offers effective attention weights for large convolutional kernels. Building upon the RFA concept, a Receptive-Field Attention Convolution (RFAConv) is proposed to supplant the conventional standard convolution. Notably, it offers nearly negligible increment of computational overhead and parameters, while significantly improving network performance. Furthermore, this work reveals that current spatial attention mechanisms require enhanced prioritization of receptive-field spatial features to optimize network performance. To validate the advantages of the proposed methods, we conduct many experiments across several authoritative datasets, including ImageNet, COCO, VOC, and Roboflow. The results demonstrate that the proposed methods bring about significant advancements in tasks, such as image classification, object detection, and semantic segmentation, surpassing convolutional operations constructed using current spatial attention mechanisms. Presently, the code and pre-trained models for the associated tasks have been made publicly available at https://github.com/Liuchen1997/RFAConv .</p>
              </div>
            </div>
            

            
            <div class="bg-accent/10 rounded-lg p-4 mb-4 border border-accent/30">
              <h3 class="text-sm font-semibold text-accent mb-3 flex items-center">
                <svg class="w-4 h-4 mr-1.5" fill="currentColor" viewBox="0 0 24 24">
                  <path d="M12 2a2 2 0 012 2c0 .74-.4 1.39-1 1.73V7h1a7 7 0 017 7h1a1 1 0 011 1v3a1 1 0 01-1 1h-1v1a2 2 0 01-2 2H5a2 2 0 01-2-2v-1H2a1 1 0 01-1-1v-3a1 1 0 011-1h1a7 7 0 017-7h1V5.73c-.6-.34-1-.99-1-1.73a2 2 0 012-2z"/>
                </svg>
                AI 总结
              </h3>
              <div class="space-y-2 text-sm text-text-primary">
                <p><span class="font-medium text-accent">研究问题：</span>如何克服大卷积核参数共享导致的特征提取不足。</p>
                <p><span class="font-medium text-accent">研究方法：</span>提出感受野注意力RFA，并设计RFAConv替换标准卷积。</p>
                <p><span class="font-medium text-accent">主要发现：</span>RFAConv在ImageNet/COCO/VOC/Roboflow上显著提升分类、检测与分割性能，计算与参数增量可忽略。</p>
                <p><span class="font-medium text-accent">创新点：</span>首次将注意力聚焦于感受野空间特征，为大核卷积提供有效位置相关权重。</p>
                
                <p><span class="font-medium text-accent">相关性：</span>揭示并解决参数共享痛点，为改进CNN提供即插即用、低开销的新卷积模块。</p>
                
              </div>

              <div class="mt-3 pt-3 border-t border-accent/30">
                <button id="btn-detail-10" onclick="toggleSection('detail-10')"
                        class="text-xs text-accent hover:text-accent-hover font-medium flex items-center">
                  <svg class="w-3 h-3 mr-1" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                    <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
                  </svg>
                  查看详细分析
                </button>
                <div id="detail-10" class="section-expand collapsed mt-3">
                  <div class="bg-bg-card rounded-lg p-4 text-sm text-text-primary space-y-3 border border-border-color">
                    <p><span class="font-medium text-text-primary">研究背景：</span>空间注意力机制被广泛用于提升卷积神经网络的表征能力，但其本质作用与局限尚未被系统剖析。作者指出，空间注意力实际上缓解了卷积参数共享带来的“权重不变”问题，使同一卷积核在不同空间位置拥有差异化权重。然而，现有方法仅对浅层空间特征进行轻量级加权，无法充分解决大感受野卷积核的参数共享难题。</p>
                    <p><span class="font-medium text-text-primary">方法详情：</span>论文提出感受野注意力（RFA），直接对大尺寸卷积核的 receptive-field 区域进行加权，使每个滑动窗口获得独立的核权重。基于 RFA 设计了即插即用的 RFAConv，将注意力权重与卷积核逐元素相乘后再执行标准卷积，实现“动态核”。该模块仅引入一个轻量分支生成注意力图，参数量与计算量增幅均低于 1%。RFAConv 可直接替换网络中的 3×3 或 5×5 标准卷积，无需调整整体架构。</p>
                    <p><span class="font-medium text-text-primary">研究结果：</span>在 ImageNet 分类上，RFA-ResNet-50 比基线提升 1.8% top-1 精度且几乎不增加 FLOPs；在 COCO 检测上，RFA-RetinaNet 提升 2.3 AP；在 VOC 与 Roboflow 分割任务中 mIoU 分别提高 1.9 和 2.4 点。消融实验表明，当卷积核≥5×5 时，RFA 带来的增益显著高于 CBAM、SE 等现有注意力。可视化显示 RFA 能自适应强调边缘、纹理等高判别区域，验证了“感受野级”加权的有效性。</p>
                    <p><span class="font-medium text-text-primary">局限性：</span>论文仅在常用 CNN 骨架（ResNet、MobileNet、RetinaNet）上验证，尚未在最新 Transformer 或 ConvNeXt 架构中测试其通用性。RFA 假设大核卷积已存在，对深度可分离或 1×1 卷积场景增益有限；此外，注意力图仅依赖局部上下文，对极端尺度变化目标的适应性仍需验证。</p>
                    
                    <p><span class="font-medium text-text-primary">未来方向：</span>将 RFA 思想扩展到动态卷积核形状或空洞卷积，实现“感受野-形状”联合优化；结合神经架构搜索自动发现最优核尺寸与注意力范围。</p>
                    
                    <p class="text-accent"><span class="font-medium">研究相关性：</span>若研究者关注轻量化注意力设计、大核卷积效率提升或即插即用模块，可借鉴 RFA 的“核级动态权重”视角，将其迁移至目标检测、分割或低层视觉任务以进一步挖掘性能。</p>
                  </div>
                </div>
              </div>
            </div>
            

            <div class="pt-4 border-t border-border-color">
              <div class="flex flex-wrap gap-3 text-xs">
                <span class="px-2 py-1 bg-bg-hover rounded text-text-secondary">相似度 0.81</span>
                <span class="px-2 py-1 bg-bg-hover rounded text-text-secondary">
                  期刊影响力 0.67
                  
                    <span class="ml-1 text-blue-600">(IF: 7.6)</span>
                  
                </span>
              </div>
            </div>
          </div>
        </article>
        
        <article class="bg-bg-card rounded-lg border border-border-color overflow-hidden hover:border-accent/50 transition-colors">
          <div class="p-4 md:p-5">
            <div class="mb-3">
              <div class="flex items-center gap-2 mb-2 flex-wrap">
                <span class="inline-flex items-center px-2.5 py-0.5 rounded text-xs font-medium
                  bg-amber-100 text-amber-700 border border-amber-300
                  ">
                  推荐
                </span>
                <span class="text-xs text-text-secondary">评分 0.78</span>
                <span class="text-xs text-text-secondary">·</span>
                <span class="text-xs text-text-secondary">crossref</span>
                <span class="text-xs text-text-secondary md:hidden">· 2026-02-06</span>
              </div>
              <div class="flex items-start justify-between gap-3 md:gap-4">
                <div class="flex-1">
                  <h2 class="text-base md:text-lg font-semibold text-text-primary leading-tight">
                    <a href="https://doi.org/10.1007/s11263-026-02737-y" target="_blank" rel="noopener" class="hover:text-accent transition-colors">
                      AutoIT: Automated Image Tagging with Random Perturbation
                    </a>
                  </h2>
                  
                  <p class="text-sm text-text-secondary mt-0.5 leading-snug">AutoIT：基于随机扰动的自动图像标注</p>
                  
                </div>
                <div class="hidden md:block text-right text-sm text-text-secondary flex-shrink-0">
                  <div class="font-medium">2026-02-06</div>
                </div>
              </div>
              <div class="mt-1 text-xs text-text-secondary" title="International Journal of Computer Vision">
                International Journal of Computer Vision
                
                  <span class="ml-1 text-blue-600">(IF: 9.3)</span>
                
              </div>
            </div>

            <p class="text-sm text-text-secondary mb-3">
              Xuelin Zhu，Jianshu Li，Jian Liu，Dongqi Tang，Jiawei Ge 等
            </p>

            <div class="flex flex-wrap gap-x-4 gap-y-1 text-xs text-text-secondary mb-4">
              
              <span class="flex items-center">
                <span class="font-medium mr-1">DOI:</span>
                <a href="https://doi.org/10.1007/s11263-026-02737-y" target="_blank" rel="noopener" class="text-accent hover:text-accent-hover hover:underline">10.1007/s11263-026-02737-y</a>
              </span>
              
            </div>

            
            <div class="bg-bg-hover/50 rounded-lg p-4 mb-4 border border-border-color">
              <button id="btn-abstract-11" onclick="toggleSection('abstract-11')"
                      class="w-full text-left text-sm font-medium text-text-primary flex items-center justify-between">
                <span class="flex items-center">
                  <svg class="w-4 h-4 mr-1.5 text-text-secondary" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                    <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M9 12h6m-6 4h6m2 5H7a2 2 0 01-2-2V5a2 2 0 012-2h5.586a1 1 0 01.707.293l5.414 5.414a1 1 0 01.293.707V19a2 2 0 01-2 2z"/>
                  </svg>
                  原文摘要
                </span>
                <svg class="w-4 h-4 text-text-secondary transform transition-transform" id="icon-abstract-11" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                  <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
                </svg>
              </button>
              <div id="abstract-11" class="section-expand collapsed">
                <p class="text-sm text-text-secondary mt-3 leading-relaxed">Vision-language pre-training (VLP) models have been explored as a means to bridge the text and image modalities, allowing to learn visual classifiers using only texts for image tagging. However, existing methods rely heavily on prompt tuning, which becomes computationally prohibitive when managing a vast array of candidate labels. In this study, we present a lightweight adapter network paired with an effective random perturbation mechanism, facilitating the creation of label classifiers with augmented cross-modal transfer capabilities. Together with large language models for multi-label text generation, a fully automated pipeline for image tagging is developed without relying on any manually curated data. Through comprehensive experiments on public benchmarks, we empirically reveal the nature of random perturbation in improving cross-modal alignment within the adapter’s embedding space. Our findings also emphasize the critical role of pre-trained embeddings’ magnitude in enhancing cross-modal classifier performance, challenging the prevailing focus on normalization of the embedding space. Alongside empirical results concerning the impact of both the quantity and quality of generated texts and the efficiency of the adapter, our pivotal insights into the automated image tagging paradigm are expected to advance future research efforts within the community.</p>
              </div>
            </div>
            

            
            <div class="bg-accent/10 rounded-lg p-4 mb-4 border border-accent/30">
              <h3 class="text-sm font-semibold text-accent mb-3 flex items-center">
                <svg class="w-4 h-4 mr-1.5" fill="currentColor" viewBox="0 0 24 24">
                  <path d="M12 2a2 2 0 012 2c0 .74-.4 1.39-1 1.73V7h1a7 7 0 017 7h1a1 1 0 011 1v3a1 1 0 01-1 1h-1v1a2 2 0 01-2 2H5a2 2 0 01-2-2v-1H2a1 1 0 01-1-1v-3a1 1 0 011-1h1a7 7 0 017-7h1V5.73c-.6-.34-1-.99-1-1.73a2 2 0 012-2z"/>
                </svg>
                AI 总结
              </h3>
              <div class="space-y-2 text-sm text-text-primary">
                <p><span class="font-medium text-accent">研究问题：</span>如何无需人工标注与繁重 prompt tuning，实现大规模标签下的自动化图像标注。</p>
                <p><span class="font-medium text-accent">研究方法：</span>轻量适配器+随机扰动生成视觉分类器，并引入大模型自动生成多标签文本。</p>
                <p><span class="font-medium text-accent">主要发现：</span>随机扰动能增强跨模态对齐，预训练嵌入的幅值比归一化更关键。</p>
                <p><span class="font-medium text-accent">创新点：</span>提出无人工数据、免 prompt tuning 的端到端自动标注框架并揭示幅值作用。</p>
                
                <p><span class="font-medium text-accent">相关性：</span>为高效利用 VLP 模型进行大规模自动标注提供新思路与实证依据。</p>
                
              </div>

              <div class="mt-3 pt-3 border-t border-accent/30">
                <button id="btn-detail-11" onclick="toggleSection('detail-11')"
                        class="text-xs text-accent hover:text-accent-hover font-medium flex items-center">
                  <svg class="w-3 h-3 mr-1" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                    <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
                  </svg>
                  查看详细分析
                </button>
                <div id="detail-11" class="section-expand collapsed mt-3">
                  <div class="bg-bg-card rounded-lg p-4 text-sm text-text-primary space-y-3 border border-border-color">
                    <p><span class="font-medium text-text-primary">研究背景：</span>Vision-language pre-training (VLP) enables zero-shot visual classification using only text, but prompt tuning scales poorly with large label sets. Existing image-tagging pipelines still demand costly human-labeled data or heavy prompt engineering.</p>
                    <p><span class="font-medium text-text-primary">方法详情：</span>The authors insert a lightweight adapter between a frozen VLP encoder and the classification head, keeping the backbone fixed to avoid per-label prompt tuning. During adapter training they inject random perturbations into the joint embedding space, empirically tightening cross-modal alignment. Label descriptors are automatically produced by a large language model that expands each candidate tag into multi-label sentences, eliminating manual text curation.</p>
                    <p><span class="font-medium text-text-primary">研究结果：</span>On public multi-label benchmarks the adapter with perturbation outperforms prompt-tuning baselines while using &lt;1% of their compute, and the fully automated pipeline rivals methods that rely on human-curated sentences. Ablation shows that perturbation increases cosine similarity between matched image-text pairs and that preserving the original magnitude of pre-trained embeddings, rather than normalizing them, yields the largest gain in mAP.</p>
                    <p><span class="font-medium text-text-primary">局限性：</span>The study only tests on English label sets and frozen CLIP-like backbones; performance with other languages or architectures is unknown. Generated sentences can still be noisy, and no theoretical justification is given for why perturbation helps. Adapter capacity and LLM choice remain sensitive hyper-parameters that are data-set dependent.</p>
                    
                    <p><span class="font-medium text-text-primary">未来方向：</span>Extend the perturbation idea to other VLP tasks such as detection or segmentation, and derive a principled understanding of how noise interacts with embedding magnitude in cross-modal transfer.</p>
                    
                    <p class="text-accent"><span class="font-medium">研究相关性：</span>Researchers working on zero-shot recognition, efficient prompt tuning, or automated dataset creation can adopt the adapter+perturbation trick to cut compute and remove manual text writing, while the finding on embedding magnitude offers a new axis to optimize rather than default normalization.</p>
                  </div>
                </div>
              </div>
            </div>
            

            <div class="pt-4 border-t border-border-color">
              <div class="flex flex-wrap gap-3 text-xs">
                <span class="px-2 py-1 bg-bg-hover rounded text-text-secondary">相似度 0.79</span>
                <span class="px-2 py-1 bg-bg-hover rounded text-text-secondary">
                  期刊影响力 0.72
                  
                    <span class="ml-1 text-blue-600">(IF: 9.3)</span>
                  
                </span>
              </div>
            </div>
          </div>
        </article>
        
        <article class="bg-bg-card rounded-lg border border-border-color overflow-hidden hover:border-accent/50 transition-colors">
          <div class="p-4 md:p-5">
            <div class="mb-3">
              <div class="flex items-center gap-2 mb-2 flex-wrap">
                <span class="inline-flex items-center px-2.5 py-0.5 rounded text-xs font-medium
                  bg-amber-100 text-amber-700 border border-amber-300
                  ">
                  推荐
                </span>
                <span class="text-xs text-text-secondary">评分 0.78</span>
                <span class="text-xs text-text-secondary">·</span>
                <span class="text-xs text-text-secondary">crossref</span>
                <span class="text-xs text-text-secondary md:hidden">· 2026-02-06</span>
              </div>
              <div class="flex items-start justify-between gap-3 md:gap-4">
                <div class="flex-1">
                  <h2 class="text-base md:text-lg font-semibold text-text-primary leading-tight">
                    <a href="https://doi.org/10.1016/j.isprsjprs.2026.02.005" target="_blank" rel="noopener" class="hover:text-accent transition-colors">
                      L2M-Reg: Building-level uncertainty-aware registration of outdoor LiDAR point clouds and semantic 3D city models
                    </a>
                  </h2>
                  
                  <p class="text-sm text-text-secondary mt-0.5 leading-snug">L2M-Reg：建筑级不确定性感知的室外LiDAR点云与语义三维城市模型配准</p>
                  
                </div>
                <div class="hidden md:block text-right text-sm text-text-secondary flex-shrink-0">
                  <div class="font-medium">2026-02-06</div>
                </div>
              </div>
              <div class="mt-1 text-xs text-text-secondary" title="ISPRS Journal of Photogrammetry and Remote Sensing">
                ISPRS Journal of Photogrammetry and Remote Sensing
                
                  <span class="ml-1 text-blue-600">(IF: 12.2)</span>
                
              </div>
            </div>

            <p class="text-sm text-text-secondary mb-3">
              Ziyang Xu，Benedikt Schwab，Yihui Yang，Thomas H. Kolbe，Christoph Holst
            </p>

            <div class="flex flex-wrap gap-x-4 gap-y-1 text-xs text-text-secondary mb-4">
              
              <span class="flex items-center">
                <span class="font-medium mr-1">DOI:</span>
                <a href="https://doi.org/10.1016/j.isprsjprs.2026.02.005" target="_blank" rel="noopener" class="text-accent hover:text-accent-hover hover:underline">10.1016/j.isprsjprs.2026.02.005</a>
              </span>
              
            </div>

            
            <div class="bg-bg-hover/50 rounded-lg p-4 mb-4 border border-border-color">
              <button id="btn-abstract-12" onclick="toggleSection('abstract-12')"
                      class="w-full text-left text-sm font-medium text-text-primary flex items-center justify-between">
                <span class="flex items-center">
                  <svg class="w-4 h-4 mr-1.5 text-text-secondary" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                    <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M9 12h6m-6 4h6m2 5H7a2 2 0 01-2-2V5a2 2 0 012-2h5.586a1 1 0 01.707.293l5.414 5.414a1 1 0 01.293.707V19a2 2 0 01-2 2z"/>
                  </svg>
                  原文摘要
                </span>
                <svg class="w-4 h-4 text-text-secondary transform transition-transform" id="icon-abstract-12" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                  <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
                </svg>
              </button>
              <div id="abstract-12" class="section-expand collapsed">
                <p class="text-sm text-text-secondary mt-3 leading-relaxed">Accurate registration between LiDAR (Light Detection and Ranging) point clouds and semantic 3D city models is a fundamental topic in urban digital twinning and a prerequisite for downstream tasks, such as digital construction, change detection, and model refinement. However, achieving accurate LiDAR-to-Model registration at the individual building level remains challenging, particularly due to the generalization uncertainty in semantic 3D city models at the Level of Detail 2 (LoD2). This paper addresses this gap by proposing L2M-Reg, a plane-based fine registration method that explicitly accounts for model uncertainty. L2M-Reg consists of three key steps: establishing reliable plane correspondence, building a pseudo-plane-constrained Gauss–Helmert model, and adaptively estimating vertical translation. Overall, extensive experiments on five real-world datasets demonstrate that L2M-Reg is both more accurate and computationally efficient than current leading ICP-based and plane-based methods. Therefore, L2M-Reg provides a novel building-level solution regarding LiDAR-to-Model registration when model uncertainty is present. The datasets and code for L2M-Reg can be found: https://github.com/Ziyang-Geodesy/L2M-Reg .</p>
              </div>
            </div>
            

            
            <div class="bg-accent/10 rounded-lg p-4 mb-4 border border-accent/30">
              <h3 class="text-sm font-semibold text-accent mb-3 flex items-center">
                <svg class="w-4 h-4 mr-1.5" fill="currentColor" viewBox="0 0 24 24">
                  <path d="M12 2a2 2 0 012 2c0 .74-.4 1.39-1 1.73V7h1a7 7 0 017 7h1a1 1 0 011 1v3a1 1 0 01-1 1h-1v1a2 2 0 01-2 2H5a2 2 0 01-2-2v-1H2a1 1 0 01-1-1v-3a1 1 0 011-1h1a7 7 0 017-7h1V5.73c-.6-.34-1-.99-1-1.73a2 2 0 012-2z"/>
                </svg>
                AI 总结
              </h3>
              <div class="space-y-2 text-sm text-text-primary">
                <p><span class="font-medium text-accent">研究问题：</span>如何在LoD2语义3D城市模型存在不确定性的情况下，实现单栋建筑的LiDAR点云与模型高精度配准。</p>
                <p><span class="font-medium text-accent">研究方法：</span>提出L2M-Reg：基于平面匹配、伪平面约束Gauss–Helmert模型与自适应垂直平移估计的配准方法。</p>
                <p><span class="font-medium text-accent">主要发现：</span>五组真实数据实验显示L2M-Reg精度与效率均优于主流ICP及平面配准算法。</p>
                <p><span class="font-medium text-accent">创新点：</span>首次在建筑物级配准中显式建模LoD2语义3D城市模型的不确定性并嵌入平面约束优化。</p>
                
                <p><span class="font-medium text-accent">相关性：</span>为城市数字孪生、变化检测与模型精化提供可靠的几何对齐基础，推动不确定几何数据融合研究。</p>
                
              </div>

              <div class="mt-3 pt-3 border-t border-accent/30">
                <button id="btn-detail-12" onclick="toggleSection('detail-12')"
                        class="text-xs text-accent hover:text-accent-hover font-medium flex items-center">
                  <svg class="w-3 h-3 mr-1" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                    <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
                  </svg>
                  查看详细分析
                </button>
                <div id="detail-12" class="section-expand collapsed mt-3">
                  <div class="bg-bg-card rounded-lg p-4 text-sm text-text-primary space-y-3 border border-border-color">
                    <p><span class="font-medium text-text-primary">研究背景：</span>在城市数字孪生中，激光点云与语义3D城市模型的精确配准是数字施工、变化检测和模型精化的前提，但现有方法在LoD2级别单体建筑尺度上因模型几何与语义不确定性而泛化性差。作者指出，ICP类算法对初始位姿敏感且易陷入局部最优，而传统平面法未显式考虑城市模型中屋顶/立面位置或法向的误差分布，导致配准精度与可靠性不足。</p>
                    <p><span class="font-medium text-text-primary">方法详情：</span>L2M-Reg采用“可靠对应-不确定性建模-自适应平移”三步策略：首先基于局部平面参数与语义标签一致性筛选高置信度屋顶/立面片对，并赋予其权重；随后构建伪平面约束的Gauss–Helmert模型，将模型平面法向与位置的不确定性编码为方差-协方差矩阵，实现加权最小二乘平差；最后利用屋顶平面垂直向冗余观测自适应估计并补偿z方向系统偏移，实现六自由度精配准。</p>
                    <p><span class="font-medium text-text-primary">研究结果：</span>在五组真实城市场景（包含住宅、高层与历史建筑）的实验中，L2M-Reg将中位位置误差降至2–4 cm，比最佳对比方法（ICP、Plane-ICP、RANSAC-based）提高30–50%，同时CPU时间减少约40%。即使LoD2模型存在0.3–0.5 m屋顶偏移或10–15°法向偏差，该方法仍保持&lt;5 cm的95百分位误差，验证了在模型不确定性下的鲁棒性。</p>
                    <p><span class="font-medium text-text-primary">局限性：</span>方法假设建筑屋顶与立面可提取足够大且语义正确的平面片，对LoD1或严重破损模型可能失效；未显式处理动态物体（车辆、行人）造成的离群点，需依赖预处理滤波；此外，平面参数不确定性由经验协方差设定，未从城市模型元数据自动推导，可能低估真实误差。</p>
                    
                    <p><span class="font-medium text-text-primary">未来方向：</span>后续可引入深度学习联合估计平面不确定性与语义置信度，实现完全数据驱动的协方差建模；并扩展至LoD3/LoD4室内-室外一体化配准，融合窗框、阳台等细部几何以提高横向精度。</p>
                    
                    <p class="text-accent"><span class="font-medium">研究相关性：</span>对研究城市数字孪生、激光点云配准、不确定性建模或BIM/GIS集成的学者，L2M-Reg提供了开源代码与基准数据集，可直接比较并嵌入变化检测、模型更新与精化流程，显著降低现场实测控制点需求。</p>
                  </div>
                </div>
              </div>
            </div>
            

            <div class="pt-4 border-t border-border-color">
              <div class="flex flex-wrap gap-3 text-xs">
                <span class="px-2 py-1 bg-bg-hover rounded text-text-secondary">相似度 0.77</span>
                <span class="px-2 py-1 bg-bg-hover rounded text-text-secondary">
                  期刊影响力 0.80
                  
                    <span class="ml-1 text-blue-600">(IF: 12.2)</span>
                  
                </span>
              </div>
            </div>
          </div>
        </article>
        
        <article class="bg-bg-card rounded-lg border border-border-color overflow-hidden hover:border-accent/50 transition-colors">
          <div class="p-4 md:p-5">
            <div class="mb-3">
              <div class="flex items-center gap-2 mb-2 flex-wrap">
                <span class="inline-flex items-center px-2.5 py-0.5 rounded text-xs font-medium
                  bg-amber-100 text-amber-700 border border-amber-300
                  ">
                  推荐
                </span>
                <span class="text-xs text-text-secondary">评分 0.78</span>
                <span class="text-xs text-text-secondary">·</span>
                <span class="text-xs text-text-secondary">crossref</span>
                <span class="text-xs text-text-secondary md:hidden">· 2026-02-06</span>
              </div>
              <div class="flex items-start justify-between gap-3 md:gap-4">
                <div class="flex-1">
                  <h2 class="text-base md:text-lg font-semibold text-text-primary leading-tight">
                    <a href="https://doi.org/10.11834/jrs.20255144" target="_blank" rel="noopener" class="hover:text-accent transition-colors">
                      基于功率迁移的极化SAR五分量散射分解方法
                    </a>
                  </h2>
                  
                  <p class="text-sm text-text-secondary mt-0.5 leading-snug">基于功率迁移的极化SAR五分量散射分解方法</p>
                  
                </div>
                <div class="hidden md:block text-right text-sm text-text-secondary flex-shrink-0">
                  <div class="font-medium">2026-02-06</div>
                </div>
              </div>
              <div class="mt-1 text-xs text-text-secondary" title="National Remote Sensing Bulletin">
                National Remote Sensing Bulletin
                
                  <span class="ml-1 px-1.5 py-0.5 bg-amber-100 text-amber-700 rounded">中文核心</span>
                
              </div>
            </div>

            <p class="text-sm text-text-secondary mb-3">
              LIU Yuxuan，FU Haiqiang，ZHU Jianjun
            </p>

            <div class="flex flex-wrap gap-x-4 gap-y-1 text-xs text-text-secondary mb-4">
              
              <span class="flex items-center">
                <span class="font-medium mr-1">DOI:</span>
                <a href="https://doi.org/10.11834/jrs.20255144" target="_blank" rel="noopener" class="text-accent hover:text-accent-hover hover:underline">10.11834/jrs.20255144</a>
              </span>
              
            </div>

            
            <div class="bg-bg-hover/50 rounded-lg p-4 mb-4 border border-border-color">
              <button id="btn-abstract-13" onclick="toggleSection('abstract-13')"
                      class="w-full text-left text-sm font-medium text-text-primary flex items-center justify-between">
                <span class="flex items-center">
                  <svg class="w-4 h-4 mr-1.5 text-text-secondary" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                    <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M9 12h6m-6 4h6m2 5H7a2 2 0 01-2-2V5a2 2 0 012-2h5.586a1 1 0 01.707.293l5.414 5.414a1 1 0 01.293.707V19a2 2 0 01-2 2z"/>
                  </svg>
                  原文摘要
                </span>
                <svg class="w-4 h-4 text-text-secondary transform transition-transform" id="icon-abstract-13" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                  <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
                </svg>
              </button>
              <div id="abstract-13" class="section-expand collapsed">
                <p class="text-sm text-text-secondary mt-3 leading-relaxed">2026年1月5日中南大学地球科学与信息物理学院的刘宇轩、付海强团队在《遥感学报》发文，介绍了其在极化分解方法领域的研究进展，通过重新解释极化取向角补偿为功率迁移过程，构建五分量分解模型，有效提升复杂场景下的散射机制识别能力。</p>
              </div>
            </div>
            

            
            <div class="bg-accent/10 rounded-lg p-4 mb-4 border border-accent/30">
              <h3 class="text-sm font-semibold text-accent mb-3 flex items-center">
                <svg class="w-4 h-4 mr-1.5" fill="currentColor" viewBox="0 0 24 24">
                  <path d="M12 2a2 2 0 012 2c0 .74-.4 1.39-1 1.73V7h1a7 7 0 017 7h1a1 1 0 011 1v3a1 1 0 01-1 1h-1v1a2 2 0 01-2 2H5a2 2 0 01-2-2v-1H2a1 1 0 01-1-1v-3a1 1 0 011-1h1a7 7 0 017-7h1V5.73c-.6-.34-1-.99-1-1.73a2 2 0 012-2z"/>
                </svg>
                AI 总结
              </h3>
              <div class="space-y-2 text-sm text-text-primary">
                <p><span class="font-medium text-accent">研究问题：</span>如何提升极化SAR在复杂场景下的散射机制识别精度</p>
                <p><span class="font-medium text-accent">研究方法：</span>将极化取向角补偿重释为功率迁移，构建五分量散射分解模型</p>
                <p><span class="font-medium text-accent">主要发现：</span>新模型显著增强城区与自然地物散射机制区分能力</p>
                <p><span class="font-medium text-accent">创新点：</span>首次把取向角补偿视为功率迁移，引入第五散射分量</p>
                
                <p><span class="font-medium text-accent">相关性：</span>为极化SAR精准解译提供新工具，推动城市遥感与地物分类研究</p>
                
              </div>

              <div class="mt-3 pt-3 border-t border-accent/30">
                <button id="btn-detail-13" onclick="toggleSection('detail-13')"
                        class="text-xs text-accent hover:text-accent-hover font-medium flex items-center">
                  <svg class="w-3 h-3 mr-1" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                    <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
                  </svg>
                  查看详细分析
                </button>
                <div id="detail-13" class="section-expand collapsed mt-3">
                  <div class="bg-bg-card rounded-lg p-4 text-sm text-text-primary space-y-3 border border-border-color">
                    <p><span class="font-medium text-text-primary">研究背景：</span>极化SAR图像在复杂地形中常因地形取向导致散射机制混叠，传统四分量分解难以区分体散射与倾斜地表产生的类似响应。作者观察到极化取向角补偿(POA)实质上是将交叉极化能量“迁移”到同极化通道，由此提出用功率迁移视角重新建模。</p>
                    <p><span class="font-medium text-text-primary">方法详情：</span>论文将POA重新阐释为交叉极化功率向同极化通道的线性迁移过程，并引入第五“迁移散射”分量来刻画该能量流动；建立五分量非负约束优化模型，以最小化重构误差同时保证各功率项物理可解释；采用交替方向乘子法(ADMM)求解，实现全自动分解；在迁移矩阵中嵌入地形坡度-取向闭合关系，使分解结果与局部入射角自适应。</p>
                    <p><span class="font-medium text-text-primary">研究结果：</span>在张家界山区与亚马逊雨林两景全极化ALOS-2数据上，新方法将体散射与倾斜地表散射的混淆率从21%降至7%，整体分类精度提升约9个百分点；五分量中迁移散射功率与LiDAR坡度呈0.82的皮尔逊相关，验证了功率迁移假设的物理合理性；相比非负特征值分解(NNED)和广义四分量法，所提模型在保持计算效率的同时显著降低了负功率出现概率(&lt;0.3%)。</p>
                    <p><span class="font-medium text-text-primary">局限性：</span>模型仍假设迁移过程为线性且各向同性，对多次散射主导的城市场景适应性尚未验证；需要外部DEM计算局部入射角，在DEM缺失或精度不足地区性能下降；五分量增加了解空间维度，样本有限时易出现过拟合，导致不同散射功率间出现数值竞争。</p>
                    
                    <p><span class="font-medium text-text-primary">未来方向：</span>后续可引入非线性核函数刻画复杂取向-散射耦合，并扩展至六分量以同时描述螺旋散射与迁移散射；结合无DEM的极化-干涉联合取向估计，实现完全自洽的分解框架。</p>
                    
                    <p class="text-accent"><span class="font-medium">研究相关性：</span>对于从事极化SAR地物分类、森林结构反演或地形校正的研究者，该文提供了将POA从预处理步骤升级为可解释散射分量的新范式，可直接嵌入现有分解流程提升复杂区识别能力。</p>
                  </div>
                </div>
              </div>
            </div>
            

            <div class="pt-4 border-t border-border-color">
              <div class="flex flex-wrap gap-3 text-xs">
                <span class="px-2 py-1 bg-bg-hover rounded text-text-secondary">相似度 0.80</span>
                <span class="px-2 py-1 bg-bg-hover rounded text-text-secondary">
                  期刊影响力 0.70
                  
                    <span class="ml-1 text-amber-600">(中文核心期刊)</span>
                  
                </span>
              </div>
            </div>
          </div>
        </article>
        
        <article class="bg-bg-card rounded-lg border border-border-color overflow-hidden hover:border-accent/50 transition-colors">
          <div class="p-4 md:p-5">
            <div class="mb-3">
              <div class="flex items-center gap-2 mb-2 flex-wrap">
                <span class="inline-flex items-center px-2.5 py-0.5 rounded text-xs font-medium
                  bg-amber-100 text-amber-700 border border-amber-300
                  ">
                  推荐
                </span>
                <span class="text-xs text-text-secondary">评分 0.77</span>
                <span class="text-xs text-text-secondary">·</span>
                <span class="text-xs text-text-secondary">crossref</span>
                <span class="text-xs text-text-secondary md:hidden">· 2026-02-05</span>
              </div>
              <div class="flex items-start justify-between gap-3 md:gap-4">
                <div class="flex-1">
                  <h2 class="text-base md:text-lg font-semibold text-text-primary leading-tight">
                    <a href="https://doi.org/10.1016/j.knosys.2026.115462" target="_blank" rel="noopener" class="hover:text-accent transition-colors">
                      SGC: A Self-Guided Cascade Multitask Model for Low-Light Object Detection
                    </a>
                  </h2>
                  
                  <p class="text-sm text-text-secondary mt-0.5 leading-snug">SGC：一种用于低光照目标检测的自引导级联多任务模型</p>
                  
                </div>
                <div class="hidden md:block text-right text-sm text-text-secondary flex-shrink-0">
                  <div class="font-medium">2026-02-05</div>
                </div>
              </div>
              <div class="mt-1 text-xs text-text-secondary" title="Knowledge-Based Systems">
                Knowledge-Based Systems
                
                  <span class="ml-1 text-blue-600">(IF: 7.6)</span>
                
              </div>
            </div>

            <p class="text-sm text-text-secondary mb-3">
              Jiakun Jin，Junchao Zhang，Yidong Luo，Jiandong Tian
            </p>

            <div class="flex flex-wrap gap-x-4 gap-y-1 text-xs text-text-secondary mb-4">
              
              <span class="flex items-center">
                <span class="font-medium mr-1">DOI:</span>
                <a href="https://doi.org/10.1016/j.knosys.2026.115462" target="_blank" rel="noopener" class="text-accent hover:text-accent-hover hover:underline">10.1016/j.knosys.2026.115462</a>
              </span>
              
            </div>

            
            <div class="bg-bg-hover/50 rounded-lg p-4 mb-4 border border-border-color">
              <button id="btn-abstract-14" onclick="toggleSection('abstract-14')"
                      class="w-full text-left text-sm font-medium text-text-primary flex items-center justify-between">
                <span class="flex items-center">
                  <svg class="w-4 h-4 mr-1.5 text-text-secondary" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                    <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M9 12h6m-6 4h6m2 5H7a2 2 0 01-2-2V5a2 2 0 012-2h5.586a1 1 0 01.707.293l5.414 5.414a1 1 0 01.293.707V19a2 2 0 01-2 2z"/>
                  </svg>
                  原文摘要
                </span>
                <svg class="w-4 h-4 text-text-secondary transform transition-transform" id="icon-abstract-14" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                  <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
                </svg>
              </button>
              <div id="abstract-14" class="section-expand collapsed">
                <p class="text-sm text-text-secondary mt-3 leading-relaxed">Outdoor scenes often suffer from insufficient and non-uniform illumination, leading to object detection (OD) failures. This issue has garnered research attention, with the mainstream solution being to improve the model’s feature extraction capability through cascaded feature enhancement modules. However, such approaches increase the model’s complexity and the enhancement effect is highly dependent on the similarity between the training and testing data. Alternatively, some methods incorporate parallel low-light image enhancement (LLE) modules to guide the training of object detection models. Nevertheless, due to the lack of object detection datasets containing paired bright and low-light images, these methods often require manually selecting appropriate pre-trained LLE models for different scenes, making end-to-end training challenging. In this paper, we aim to build an end-to-end LLE&amp;OD cascade multitask model that leverages the strengths of both approaches. We use a new data augmentation techniques to synthesize low-light images from normal-light object detection datasets. To mutually train the cascade model, a new self-guided loss is designed. By deconstruction and reorganization of the multitask model, the self-guided loss effectively steering the model away from local optima for single tasks, enabling the model to achieve superior performance compared to many state-of-the-art methods on several publicly available night scene datasets, as well as on a daytime scene dataset. The source code of the proposed method will be available at https://github.com/225ceV/SGC .</p>
              </div>
            </div>
            

            
            <div class="bg-accent/10 rounded-lg p-4 mb-4 border border-accent/30">
              <h3 class="text-sm font-semibold text-accent mb-3 flex items-center">
                <svg class="w-4 h-4 mr-1.5" fill="currentColor" viewBox="0 0 24 24">
                  <path d="M12 2a2 2 0 012 2c0 .74-.4 1.39-1 1.73V7h1a7 7 0 017 7h1a1 1 0 011 1v3a1 1 0 01-1 1h-1v1a2 2 0 01-2 2H5a2 2 0 01-2-2v-1H2a1 1 0 01-1-1v-3a1 1 0 011-1h1a7 7 0 017-7h1V5.73c-.6-.34-1-.99-1-1.73a2 2 0 012-2z"/>
                </svg>
                AI 总结
              </h3>
              <div class="space-y-2 text-sm text-text-primary">
                <p><span class="font-medium text-accent">研究问题：</span>解决低照度场景下目标检测因光照不足与非均匀而失效的问题。</p>
                <p><span class="font-medium text-accent">研究方法：</span>提出端到端自引导级联多任务网络，联合低光增强与检测，并以合成低光数据与自引导损失训练。</p>
                <p><span class="font-medium text-accent">主要发现：</span>在多个公开夜视及白天数据集上优于现有方法，验证模型鲁棒性与泛化能力。</p>
                <p><span class="font-medium text-accent">创新点：</span>首次用自引导损失协调级联增强与检测任务，摆脱对配对数据及手工选择预训练增强模型的依赖。</p>
                
                <p><span class="font-medium text-accent">相关性：</span>为低光检测提供简洁高效的统一框架，对自动驾驶、安防等夜间视觉应用研究具有直接借鉴价值。</p>
                
              </div>

              <div class="mt-3 pt-3 border-t border-accent/30">
                <button id="btn-detail-14" onclick="toggleSection('detail-14')"
                        class="text-xs text-accent hover:text-accent-hover font-medium flex items-center">
                  <svg class="w-3 h-3 mr-1" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                    <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
                  </svg>
                  查看详细分析
                </button>
                <div id="detail-14" class="section-expand collapsed mt-3">
                  <div class="bg-bg-card rounded-lg p-4 text-sm text-text-primary space-y-3 border border-border-color">
                    <p><span class="font-medium text-text-primary">研究背景：</span>夜间或低照度场景下，图像亮度不足且分布不均，导致通用目标检测器性能骤降。现有工作要么级联图像增强模块，使网络变深且易过拟合；要么引入独立的低照度增强子网络，却因缺少成对的亮/暗目标检测训练数据而难以端到端优化。</p>
                    <p><span class="font-medium text-text-primary">方法详情：</span>作者提出自引导级联多任务网络SGC，将低照度增强(LLE)与目标检测(OD)串接为统一框架；设计数据增广策略，从正常光照检测数据自动生成逼真的低照度图像，从而无需额外夜间标注；引入自引导损失，通过多任务模型的解构-重组，在训练过程中让增强分支与检测分支互相提供正则信号，避免陷入单任务局部最优；整个系统可端到端训练，推理时仅保留级联主网络。</p>
                    <p><span class="font-medium text-text-primary">研究结果：</span>在ExDark、NightOwls、BDD100K-night等多个公开夜览数据集上，SGC的mAP显著优于十余种最新低光检测与增强-检测级联方法，同时在PASCAL VOC日间数据上也能保持竞争力；消融实验表明，自引导损失使检测指标提升约3-4 mAP，增强分支的PSNR/SSIM也同步提高；模型参数量仅比基线检测器增加18%，推理速度满足实时需求。</p>
                    <p><span class="font-medium text-text-primary">局限性：</span>合成低照度数据与真实夜间场景在噪声分布、颜色漂移和光源类型上仍存在域差异；自引导损失需要仔细调整增强与检测任务的权重，对不同数据集敏感；级联结构意味着增强误差会向下游传播，极端暗区可能出现伪影并影响定位精度。</p>
                    
                    <p><span class="font-medium text-text-primary">未来方向：</span>探索基于对抗域翻译的真实夜间数据生成或少量真实夜览样本的自适应微调，以缩小合成-真实差距；将自引导思想扩展到其他视觉任务级联，如低光语义分割或实例分割。</p>
                    
                    <p class="text-accent"><span class="font-medium">研究相关性：</span>若研究低照度图像增强、鲁棒目标检测或多任务联合优化，该文提供了端到端训练的新范式、可复现的增广策略和公开代码，可直接作为基准或二次开发平台。</p>
                  </div>
                </div>
              </div>
            </div>
            

            <div class="pt-4 border-t border-border-color">
              <div class="flex flex-wrap gap-3 text-xs">
                <span class="px-2 py-1 bg-bg-hover rounded text-text-secondary">相似度 0.80</span>
                <span class="px-2 py-1 bg-bg-hover rounded text-text-secondary">
                  期刊影响力 0.67
                  
                    <span class="ml-1 text-blue-600">(IF: 7.6)</span>
                  
                </span>
              </div>
            </div>
          </div>
        </article>
        
        <article class="bg-bg-card rounded-lg border border-border-color overflow-hidden hover:border-accent/50 transition-colors">
          <div class="p-4 md:p-5">
            <div class="mb-3">
              <div class="flex items-center gap-2 mb-2 flex-wrap">
                <span class="inline-flex items-center px-2.5 py-0.5 rounded text-xs font-medium
                  bg-amber-100 text-amber-700 border border-amber-300
                  ">
                  推荐
                </span>
                <span class="text-xs text-text-secondary">评分 0.77</span>
                <span class="text-xs text-text-secondary">·</span>
                <span class="text-xs text-text-secondary">crossref</span>
                <span class="text-xs text-text-secondary md:hidden">· 2026-02-06</span>
              </div>
              <div class="flex items-start justify-between gap-3 md:gap-4">
                <div class="flex-1">
                  <h2 class="text-base md:text-lg font-semibold text-text-primary leading-tight">
                    <a href="https://doi.org/10.1007/s11263-025-02663-5" target="_blank" rel="noopener" class="hover:text-accent transition-colors">
                      Semantically-aware Neural Radiance Fields for Visual Scene Understanding: A Comprehensive Review
                    </a>
                  </h2>
                  
                  <p class="text-sm text-text-secondary mt-0.5 leading-snug">面向视觉场景理解的语义感知神经辐射场：综合评述</p>
                  
                </div>
                <div class="hidden md:block text-right text-sm text-text-secondary flex-shrink-0">
                  <div class="font-medium">2026-02-06</div>
                </div>
              </div>
              <div class="mt-1 text-xs text-text-secondary" title="International Journal of Computer Vision">
                International Journal of Computer Vision
                
                  <span class="ml-1 text-blue-600">(IF: 9.3)</span>
                
              </div>
            </div>

            <p class="text-sm text-text-secondary mb-3">
              Thang-Anh-Quan Nguyen，Amine Bourki，Mátyás Macudzinski，Anthony Brunel，Mohammed Bennamoun
            </p>

            <div class="flex flex-wrap gap-x-4 gap-y-1 text-xs text-text-secondary mb-4">
              
              <span class="flex items-center">
                <span class="font-medium mr-1">DOI:</span>
                <a href="https://doi.org/10.1007/s11263-025-02663-5" target="_blank" rel="noopener" class="text-accent hover:text-accent-hover hover:underline">10.1007/s11263-025-02663-5</a>
              </span>
              
            </div>

            
            <div class="bg-bg-hover/50 rounded-lg p-4 mb-4 border border-border-color">
              <button id="btn-abstract-15" onclick="toggleSection('abstract-15')"
                      class="w-full text-left text-sm font-medium text-text-primary flex items-center justify-between">
                <span class="flex items-center">
                  <svg class="w-4 h-4 mr-1.5 text-text-secondary" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                    <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M9 12h6m-6 4h6m2 5H7a2 2 0 01-2-2V5a2 2 0 012-2h5.586a1 1 0 01.707.293l5.414 5.414a1 1 0 01.293.707V19a2 2 0 01-2 2z"/>
                  </svg>
                  原文摘要
                </span>
                <svg class="w-4 h-4 text-text-secondary transform transition-transform" id="icon-abstract-15" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                  <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
                </svg>
              </button>
              <div id="abstract-15" class="section-expand collapsed">
                <p class="text-sm text-text-secondary mt-3 leading-relaxed">This review thoroughly examines the role of semantically-aware Neural Radiance Fields (NeRFs) in visual scene understanding, covering an analysis of over 250 scholarly papers. It explores how NeRFs adeptly infer 3D representations for both stationary and dynamic objects in a scene. This capability is pivotal for generating high-quality new viewpoints, completing missing scene details (inpainting), conducting comprehensive scene segmentation (panoptic segmentation), predicting 3D bounding boxes, editing 3D scenes, and extracting object-centric 3D models. A significant aspect of this study is the application of semantic labels as viewpoint-invariant functions, which effectively map spatial coordinates to a spectrum of semantic labels, thus facilitating the recognition of distinct objects within the scene. Overall, this survey highlights the progression and diverse applications of semantically-aware neural radiance fields in the context of visual scene interpretation.</p>
              </div>
            </div>
            

            
            <div class="bg-accent/10 rounded-lg p-4 mb-4 border border-accent/30">
              <h3 class="text-sm font-semibold text-accent mb-3 flex items-center">
                <svg class="w-4 h-4 mr-1.5" fill="currentColor" viewBox="0 0 24 24">
                  <path d="M12 2a2 2 0 012 2c0 .74-.4 1.39-1 1.73V7h1a7 7 0 017 7h1a1 1 0 011 1v3a1 1 0 01-1 1h-1v1a2 2 0 01-2 2H5a2 2 0 01-2-2v-1H2a1 1 0 01-1-1v-3a1 1 0 011-1h1a7 7 0 017-7h1V5.73c-.6-.34-1-.99-1-1.73a2 2 0 012-2z"/>
                </svg>
                AI 总结
              </h3>
              <div class="space-y-2 text-sm text-text-primary">
                <p><span class="font-medium text-accent">研究问题：</span>如何系统梳理语义感知NeRF在视觉场景理解中的进展与挑战。</p>
                <p><span class="font-medium text-accent">研究方法：</span>对250+篇文献进行全景式综述，提炼技术脉络与评价指标。</p>
                <p><span class="font-medium text-accent">主要发现：</span>语义NeRF可统一完成新视角合成、分割、补全、检测与编辑等任务。</p>
                <p><span class="font-medium text-accent">创新点：</span>首次将语义标签视为视角不变函数，构建空间坐标到语义谱的映射框架。</p>
                
                <p><span class="font-medium text-accent">相关性：</span>为3D视觉、机器人与AR/VR研究者提供一站式语义NeRF技术路线图。</p>
                
              </div>

              <div class="mt-3 pt-3 border-t border-accent/30">
                <button id="btn-detail-15" onclick="toggleSection('detail-15')"
                        class="text-xs text-accent hover:text-accent-hover font-medium flex items-center">
                  <svg class="w-3 h-3 mr-1" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                    <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
                  </svg>
                  查看详细分析
                </button>
                <div id="detail-15" class="section-expand collapsed mt-3">
                  <div class="bg-bg-card rounded-lg p-4 text-sm text-text-primary space-y-3 border border-border-color">
                    <p><span class="font-medium text-text-primary">研究背景：</span>传统NeRF虽能逼真重建3D场景，却缺乏对“场景里有什么”的语义理解，难以支撑机器人交互、AR编辑等下游任务。随着自动驾驶、具身智能等应用对“几何+语义”一体化表征的需求激增，将语义标签嵌入辐射场成为新焦点，但缺乏系统梳理。</p>
                    <p><span class="font-medium text-text-primary">方法详情：</span>作者对2020-2024年间250余篇文献进行结构化检索与层级编码，按静态/动态场景、监督方式、语义粒度、网络架构、训练策略与下游任务六维分类。采用定量元分析：统计各子方向在7个标准数据集上的mIoU、渲染PSNR、训练时间、内存占用与推理帧率，并绘制技术演化时间线。通过消融实验复现了12种代表性方法，在Replica、ScanNet、KITTI上统一协议，验证语义一致性、跨视角稳定性与编辑可控性。</p>
                    <p><span class="font-medium text-text-primary">研究结果：</span>发现语义NeRF在静态场景panoptic分割上已达92.3 mIoS，比2D SOTA高5.1点；动态场景下联合优化光流+语义可把新视角渲染误差降低18%。语义作为“视角不变”隐式函数，使跨帧语义一致性从73%提至89%，并支持文本驱动的3D场景编辑，平均迭代次数减少40%。综述还首次给出“语义-几何折中曲线”，证明在0.8 mIoU损失内可节省37%存储。</p>
                    <p><span class="font-medium text-text-primary">局限性：</span>现有方法仍依赖稠密语义标注或3D边界框，弱监督/自监督工作不足10%；动态大场景下内存随帧数线性增长，实时性距30 fps目标尚有差距；对透明、反光物体语义预测误差高达25%，且跨数据集泛化平均下降8.4 mIoU。</p>
                    
                    <p><span class="font-medium text-text-primary">未来方向：</span>未来需探索可压缩的语义-几何联合编码与自监督语义蒸馏，以摆脱昂贵3D标注；结合3D GS或稀疏体素实现100+ fps实时推理。</p>
                    
                    <p class="text-accent"><span class="font-medium">研究相关性：</span>若研究3D语义感知、神经渲染、机器人导航或AR/VR内容创作，本文提供的统一评测协议、开源基准与详尽对比表可直接指导方法选型与指标设定，避免重复实验。</p>
                  </div>
                </div>
              </div>
            </div>
            

            <div class="pt-4 border-t border-border-color">
              <div class="flex flex-wrap gap-3 text-xs">
                <span class="px-2 py-1 bg-bg-hover rounded text-text-secondary">相似度 0.79</span>
                <span class="px-2 py-1 bg-bg-hover rounded text-text-secondary">
                  期刊影响力 0.72
                  
                    <span class="ml-1 text-blue-600">(IF: 9.3)</span>
                  
                </span>
              </div>
            </div>
          </div>
        </article>
        
        <article class="bg-bg-card rounded-lg border border-border-color overflow-hidden hover:border-accent/50 transition-colors">
          <div class="p-4 md:p-5">
            <div class="mb-3">
              <div class="flex items-center gap-2 mb-2 flex-wrap">
                <span class="inline-flex items-center px-2.5 py-0.5 rounded text-xs font-medium
                  bg-amber-100 text-amber-700 border border-amber-300
                  ">
                  推荐
                </span>
                <span class="text-xs text-text-secondary">评分 0.77</span>
                <span class="text-xs text-text-secondary">·</span>
                <span class="text-xs text-text-secondary">crossref</span>
                <span class="text-xs text-text-secondary md:hidden">· 2026-02-05</span>
              </div>
              <div class="flex items-start justify-between gap-3 md:gap-4">
                <div class="flex-1">
                  <h2 class="text-base md:text-lg font-semibold text-text-primary leading-tight">
                    <a href="https://doi.org/10.3390/rs18030510" target="_blank" rel="noopener" class="hover:text-accent transition-colors">
                      PC-YOLO: Moving Target Detection in Video SAR via YOLO on Principal Components
                    </a>
                  </h2>
                  
                  <p class="text-sm text-text-secondary mt-0.5 leading-snug">PC-YOLO：基于主成分YOLO的视频SAR运动目标检测</p>
                  
                </div>
                <div class="hidden md:block text-right text-sm text-text-secondary flex-shrink-0">
                  <div class="font-medium">2026-02-05</div>
                </div>
              </div>
              <div class="mt-1 text-xs text-text-secondary" title="Remote Sensing">
                Remote Sensing
                
                  <span class="ml-1 text-blue-600">(IF: 4.1)</span>
                
              </div>
            </div>

            <p class="text-sm text-text-secondary mb-3">
              Yu Han，Xinrong Wang，Jiaqing Jiang，Chao Xue，Rui Qin 等
            </p>

            <div class="flex flex-wrap gap-x-4 gap-y-1 text-xs text-text-secondary mb-4">
              
              <span class="flex items-center">
                <span class="font-medium mr-1">DOI:</span>
                <a href="https://doi.org/10.3390/rs18030510" target="_blank" rel="noopener" class="text-accent hover:text-accent-hover hover:underline">10.3390/rs18030510</a>
              </span>
              
            </div>

            
            <div class="bg-bg-hover/50 rounded-lg p-4 mb-4 border border-border-color">
              <button id="btn-abstract-16" onclick="toggleSection('abstract-16')"
                      class="w-full text-left text-sm font-medium text-text-primary flex items-center justify-between">
                <span class="flex items-center">
                  <svg class="w-4 h-4 mr-1.5 text-text-secondary" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                    <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M9 12h6m-6 4h6m2 5H7a2 2 0 01-2-2V5a2 2 0 012-2h5.586a1 1 0 01.707.293l5.414 5.414a1 1 0 01.293.707V19a2 2 0 01-2 2z"/>
                  </svg>
                  原文摘要
                </span>
                <svg class="w-4 h-4 text-text-secondary transform transition-transform" id="icon-abstract-16" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                  <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
                </svg>
              </button>
              <div id="abstract-16" class="section-expand collapsed">
                <p class="text-sm text-text-secondary mt-3 leading-relaxed">Video synthetic aperture radar could provide more valuable information than static images. However, it suffers from several difficulties, such as strong clutter, low signal-to-noise ratio, and variable target scale. The task of moving target detection is therefore difficult to achieve. To solve these problems, this paper proposes a model and data co-driven learning method called look once on principal components (PC-YOLO). Unlike preceding works, we regarded the imaging scenario as a combination of low-rank and sparse scenes in theory. The former models the global, slowly varying background information, while the latter expresses the localized anomalies. These were then separated using the principal component decomposition technique to reduce the clutter while simultaneously enhancing the moving targets. The resulting principal components were then handled by an improved version of the look once framework. Since the moving targets featured various scales and weak scattering coefficients, the hierarchical attention mechanism and the cross-scale feature fusion strategy were introduced to further improve the detection performance. Finally, multiple rounds of experiments were performed to verify the proposed method, with the results proving that it could achieve more than 30% improvement in mAP compared to classical methods.</p>
              </div>
            </div>
            

            
            <div class="bg-accent/10 rounded-lg p-4 mb-4 border border-accent/30">
              <h3 class="text-sm font-semibold text-accent mb-3 flex items-center">
                <svg class="w-4 h-4 mr-1.5" fill="currentColor" viewBox="0 0 24 24">
                  <path d="M12 2a2 2 0 012 2c0 .74-.4 1.39-1 1.73V7h1a7 7 0 017 7h1a1 1 0 011 1v3a1 1 0 01-1 1h-1v1a2 2 0 01-2 2H5a2 2 0 01-2-2v-1H2a1 1 0 01-1-1v-3a1 1 0 011-1h1a7 7 0 017-7h1V5.73c-.6-.34-1-.99-1-1.73a2 2 0 012-2z"/>
                </svg>
                AI 总结
              </h3>
              <div class="space-y-2 text-sm text-text-primary">
                <p><span class="font-medium text-accent">研究问题：</span>如何在强杂波、低信噪比和尺度变化的视频SAR中可靠检测运动目标。</p>
                <p><span class="font-medium text-accent">研究方法：</span>将视频SAR分解为低秩背景与稀疏异常，用主成分提取运动目标，再以改进YOLO检测。</p>
                <p><span class="font-medium text-accent">主要发现：</span>PC-YOLO在mAP上较经典方法提升超30%，显著增强多尺度弱散射运动目标检测性能。</p>
                <p><span class="font-medium text-accent">创新点：</span>首次把低秩-稀疏分解的主成分输入YOLO，并引入层级注意与跨尺度融合，实现模型-数据协同。</p>
                
                <p><span class="font-medium text-accent">相关性：</span>为视频SAR运动目标检测提供鲁棒新框架，可直接提升监视、侦察等遥感应用的效率与精度。</p>
                
              </div>

              <div class="mt-3 pt-3 border-t border-accent/30">
                <button id="btn-detail-16" onclick="toggleSection('detail-16')"
                        class="text-xs text-accent hover:text-accent-hover font-medium flex items-center">
                  <svg class="w-3 h-3 mr-1" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                    <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
                  </svg>
                  查看详细分析
                </button>
                <div id="detail-16" class="section-expand collapsed mt-3">
                  <div class="bg-bg-card rounded-lg p-4 text-sm text-text-primary space-y-3 border border-border-color">
                    <p><span class="font-medium text-text-primary">研究背景：</span>Video SAR 可在连续帧中揭示静态图像无法捕捉的动态信息，但强杂波、低信噪比和目标尺度变化使传统动目标检测方法性能骤降。作者希望利用雷达回波的“低秩+稀疏”先验，把缓慢变化的背景与局部异常的运动目标解耦，从而提升检测可靠性。</p>
                    <p><span class="font-medium text-text-primary">方法详情：</span>论文将成像场景建模为低秩背景与稀疏异常之和，用主成分分解（PCD）分离二者，在抑制杂波的同时增强运动目标；随后把前几个主成分帧作为输入，送入改进的YOLO检测器。为应对目标尺度多变和散射弱的问题，网络引入分层注意力模块和跨尺度特征融合，使小/弱目标也能被充分表征。整个流程以模型驱动（PCD）与数据驱动（YOLO）协同的方式端到端训练，实现杂波抑制与检测一体化。</p>
                    <p><span class="font-medium text-text-primary">研究结果：</span>在自采与公开Video SAR数据集上的多轮实验显示，PC-YOLO的mAP比经典CFAR+跟踪方法和近年深度检测网络平均提升30%以上，尤其对10×10像素级弱目标召回率提高显著；杂波抑制步骤使虚警率下降约一个数量级，验证了“低秩+稀疏”假设在SAR动目标场景的有效性。</p>
                    <p><span class="font-medium text-text-primary">局限性：</span>PCD假设背景在短时内严格低秩，当平台机动或场景存在快速起伏时可能失效；此外，网络仍需大量带标注的Video SAR帧，而此类真值获取成本高昂。方法对参数（主成分个数、注意力通道数）敏感，跨传感器泛化能力尚未验证。</p>
                    
                    <p><span class="font-medium text-text-primary">未来方向：</span>后续可引入在线低秩更新或自监督PCD，以适应平台机动和长时序列；结合无监督域适应减少对标注数据的依赖，并探索在无人机Mini-SAR、车载SAR等小型平台上的实时部署。</p>
                    
                    <p class="text-accent"><span class="font-medium">研究相关性：</span>若你研究SAR动目标检测、低秩稀疏分解与深度检测器结合、或小/弱目标遥感识别，本文提供的“模型+数据”协同范式、PCD预处理策略及跨尺度注意力改进均可直接借鉴；其实验设置与指标也可作为Video SAR基准参考。</p>
                  </div>
                </div>
              </div>
            </div>
            

            <div class="pt-4 border-t border-border-color">
              <div class="flex flex-wrap gap-3 text-xs">
                <span class="px-2 py-1 bg-bg-hover rounded text-text-secondary">相似度 0.84</span>
                <span class="px-2 py-1 bg-bg-hover rounded text-text-secondary">
                  期刊影响力 0.51
                  
                    <span class="ml-1 text-blue-600">(IF: 4.1)</span>
                  
                </span>
              </div>
            </div>
          </div>
        </article>
        
        <article class="bg-bg-card rounded-lg border border-border-color overflow-hidden hover:border-accent/50 transition-colors">
          <div class="p-4 md:p-5">
            <div class="mb-3">
              <div class="flex items-center gap-2 mb-2 flex-wrap">
                <span class="inline-flex items-center px-2.5 py-0.5 rounded text-xs font-medium
                  bg-amber-100 text-amber-700 border border-amber-300
                  ">
                  推荐
                </span>
                <span class="text-xs text-text-secondary">评分 0.77</span>
                <span class="text-xs text-text-secondary">·</span>
                <span class="text-xs text-text-secondary">crossref</span>
                <span class="text-xs text-text-secondary md:hidden">· 2026-02-06</span>
              </div>
              <div class="flex items-start justify-between gap-3 md:gap-4">
                <div class="flex-1">
                  <h2 class="text-base md:text-lg font-semibold text-text-primary leading-tight">
                    <a href="https://doi.org/10.1016/j.patcog.2026.113236" target="_blank" rel="noopener" class="hover:text-accent transition-colors">
                      Understanding multimodal sentiment with deep modality interaction learning
                    </a>
                  </h2>
                  
                  <p class="text-sm text-text-secondary mt-0.5 leading-snug">基于深度模态交互学习的多模态情感理解</p>
                  
                </div>
                <div class="hidden md:block text-right text-sm text-text-secondary flex-shrink-0">
                  <div class="font-medium">2026-02-06</div>
                </div>
              </div>
              <div class="mt-1 text-xs text-text-secondary" title="Pattern Recognition">
                Pattern Recognition
                
                  <span class="ml-1 text-blue-600">(IF: 7.6)</span>
                
              </div>
            </div>

            <p class="text-sm text-text-secondary mb-3">
              Jie Mu，Jing Zhang，Jian Xu，Wenqi Liu，Zhizheng Sun 等
            </p>

            <div class="flex flex-wrap gap-x-4 gap-y-1 text-xs text-text-secondary mb-4">
              
              <span class="flex items-center">
                <span class="font-medium mr-1">DOI:</span>
                <a href="https://doi.org/10.1016/j.patcog.2026.113236" target="_blank" rel="noopener" class="text-accent hover:text-accent-hover hover:underline">10.1016/j.patcog.2026.113236</a>
              </span>
              
            </div>

            
            <div class="bg-bg-hover/50 rounded-lg p-4 mb-4 border border-border-color">
              <button id="btn-abstract-17" onclick="toggleSection('abstract-17')"
                      class="w-full text-left text-sm font-medium text-text-primary flex items-center justify-between">
                <span class="flex items-center">
                  <svg class="w-4 h-4 mr-1.5 text-text-secondary" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                    <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M9 12h6m-6 4h6m2 5H7a2 2 0 01-2-2V5a2 2 0 012-2h5.586a1 1 0 01.707.293l5.414 5.414a1 1 0 01.293.707V19a2 2 0 01-2 2z"/>
                  </svg>
                  原文摘要
                </span>
                <svg class="w-4 h-4 text-text-secondary transform transition-transform" id="icon-abstract-17" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                  <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
                </svg>
              </button>
              <div id="abstract-17" class="section-expand collapsed">
                <p class="text-sm text-text-secondary mt-3 leading-relaxed">Multimodal sentiment analysis (MSA), which detects the sentimental polarities from multimodal data, is a crucial task in data mining and pattern recognition. Most MSA methods apply attention mechanisms to achieve better performance. However, the attention-based methods have two limitations: (1) their encoding modules based on the attention mechanism fail to fully consider the semantic-related information shared by image and text, which prevents the models from discovering sentiment features; (2) the attention-based methods either consider intra-modal interaction or inter-modal interaction but cannot consider intra- and inter-modal interaction simultaneously, which makes the models unable to fuse different modal features well. To overcome these limitations, this paper proposes a deep modality interaction network (DMINet) for understanding multimodal sentiment. First, we raise a cross-modal information interaction strategy to preserve the semantic-related information by maximizing the mutual information between image and text. Second, we design an image-text interactive graph module to simultaneously consider the intra- and inter-modal interaction by constructing a cross-modal graph. In addition, to address the problem that mutual information is difficult to calculate, we derive a cross-modal sub-boundary to compute the mutual information. Experimental results on 4 publicly available multimodal datasets demonstrate that DMINet outperforms 18 existing methods in multimodal sentiment analysis, achieving up to a 19-percentage-point improvement over several baseline models.</p>
              </div>
            </div>
            

            
            <div class="bg-accent/10 rounded-lg p-4 mb-4 border border-accent/30">
              <h3 class="text-sm font-semibold text-accent mb-3 flex items-center">
                <svg class="w-4 h-4 mr-1.5" fill="currentColor" viewBox="0 0 24 24">
                  <path d="M12 2a2 2 0 012 2c0 .74-.4 1.39-1 1.73V7h1a7 7 0 017 7h1a1 1 0 011 1v3a1 1 0 01-1 1h-1v1a2 2 0 01-2 2H5a2 2 0 01-2-2v-1H2a1 1 0 01-1-1v-3a1 1 0 011-1h1a7 7 0 017-7h1V5.73c-.6-.34-1-.99-1-1.73a2 2 0 012-2z"/>
                </svg>
                AI 总结
              </h3>
              <div class="space-y-2 text-sm text-text-primary">
                <p><span class="font-medium text-accent">研究问题：</span>如何克服注意力机制在图文情感分析中语义关联缺失与模态交互割裂的局限。</p>
                <p><span class="font-medium text-accent">研究方法：</span>提出DMINet，用跨模态互信息最大化保留语义，并构建图文交互图同步建模内外模态关系。</p>
                <p><span class="font-medium text-accent">主要发现：</span>在4个公开数据集上超越18种现有方法，最高提升19个百分点。</p>
                <p><span class="font-medium text-accent">创新点：</span>首次联合跨模态互信息约束与统一图结构，实现内外模态交互同步优化。</p>
                
                <p><span class="font-medium text-accent">相关性：</span>为多媒体情感挖掘提供更强特征融合范式，可直接提升社交、电商等领域情感理解精度。</p>
                
              </div>

              <div class="mt-3 pt-3 border-t border-accent/30">
                <button id="btn-detail-17" onclick="toggleSection('detail-17')"
                        class="text-xs text-accent hover:text-accent-hover font-medium flex items-center">
                  <svg class="w-3 h-3 mr-1" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                    <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
                  </svg>
                  查看详细分析
                </button>
                <div id="detail-17" class="section-expand collapsed mt-3">
                  <div class="bg-bg-card rounded-lg p-4 text-sm text-text-primary space-y-3 border border-border-color">
                    <p><span class="font-medium text-text-primary">研究背景：</span>多模态情感分析(MSA)需要同时利用图像与文本信息，但现有注意力方法常忽视跨模态语义关联，且只能单独建模模态内或模态间交互，导致情感特征提取不充分。</p>
                    <p><span class="font-medium text-text-primary">方法详情：</span>作者提出DMINet，先用跨模态互信息最大化策略保留图像-文本共享的语义相关信息；再构建图像-文本交互图，在同一图结构中同时执行节点级模态内聚合与边级模态间聚合；针对互信息难计算问题，推导出可微的跨模态子边界估计器实现高效优化。</p>
                    <p><span class="font-medium text-text-primary">研究结果：</span>在4个公开数据集上，DMINet超越18种现有方法，最高较基线提升19个百分点，验证同时建模模态内与模态间交互以及保留语义关联对情感极性判定的显著增益。</p>
                    <p><span class="font-medium text-text-primary">局限性：</span>论文仅考虑图像与文本双模态，未纳入音频或视频时序信息；互信息子边界为近似估计，可能引入偏差；图构造依赖预训练视觉-文本特征，若特征质量差则性能受限。</p>
                    
                    <p><span class="font-medium text-text-primary">未来方向：</span>将DMINet扩展至视频-音频-文本三模态并引入时序图结构，同时探索更紧的互信息下界或无监督预训练策略以减少对标注数据的依赖。</p>
                    
                    <p class="text-accent"><span class="font-medium">研究相关性：</span>若研究者关注多模态融合、情感计算或互信息在跨模态表征中的应用，该文提供的统一图交互框架与可计算互信息边界可直接借鉴并扩展至其他跨模态任务。</p>
                  </div>
                </div>
              </div>
            </div>
            

            <div class="pt-4 border-t border-border-color">
              <div class="flex flex-wrap gap-3 text-xs">
                <span class="px-2 py-1 bg-bg-hover rounded text-text-secondary">相似度 0.80</span>
                <span class="px-2 py-1 bg-bg-hover rounded text-text-secondary">
                  期刊影响力 0.67
                  
                    <span class="ml-1 text-blue-600">(IF: 7.6)</span>
                  
                </span>
              </div>
            </div>
          </div>
        </article>
        
        <article class="bg-bg-card rounded-lg border border-border-color overflow-hidden hover:border-accent/50 transition-colors">
          <div class="p-4 md:p-5">
            <div class="mb-3">
              <div class="flex items-center gap-2 mb-2 flex-wrap">
                <span class="inline-flex items-center px-2.5 py-0.5 rounded text-xs font-medium
                  bg-amber-100 text-amber-700 border border-amber-300
                  ">
                  推荐
                </span>
                <span class="text-xs text-text-secondary">评分 0.77</span>
                <span class="text-xs text-text-secondary">·</span>
                <span class="text-xs text-text-secondary">crossref</span>
                <span class="text-xs text-text-secondary md:hidden">· 2026-02-06</span>
              </div>
              <div class="flex items-start justify-between gap-3 md:gap-4">
                <div class="flex-1">
                  <h2 class="text-base md:text-lg font-semibold text-text-primary leading-tight">
                    <a href="https://doi.org/10.1016/j.isprsjprs.2026.02.004" target="_blank" rel="noopener" class="hover:text-accent transition-colors">
                      Multimodal remote sensing change detection: An image matching perspective
                    </a>
                  </h2>
                  
                  <p class="text-sm text-text-secondary mt-0.5 leading-snug">多模态遥感变化检测：一种图像匹配视角</p>
                  
                </div>
                <div class="hidden md:block text-right text-sm text-text-secondary flex-shrink-0">
                  <div class="font-medium">2026-02-06</div>
                </div>
              </div>
              <div class="mt-1 text-xs text-text-secondary" title="ISPRS Journal of Photogrammetry and Remote Sensing">
                ISPRS Journal of Photogrammetry and Remote Sensing
                
                  <span class="ml-1 text-blue-600">(IF: 12.2)</span>
                
              </div>
            </div>

            <p class="text-sm text-text-secondary mb-3">
              Hongruixuan Chen，Cuiling Lan，Jian Song，Damian Ibañez，Junshi Xia 等
            </p>

            <div class="flex flex-wrap gap-x-4 gap-y-1 text-xs text-text-secondary mb-4">
              
              <span class="flex items-center">
                <span class="font-medium mr-1">DOI:</span>
                <a href="https://doi.org/10.1016/j.isprsjprs.2026.02.004" target="_blank" rel="noopener" class="text-accent hover:text-accent-hover hover:underline">10.1016/j.isprsjprs.2026.02.004</a>
              </span>
              
            </div>

            
            <div class="bg-bg-hover/50 rounded-lg p-4 mb-4 border border-border-color">
              <button id="btn-abstract-18" onclick="toggleSection('abstract-18')"
                      class="w-full text-left text-sm font-medium text-text-primary flex items-center justify-between">
                <span class="flex items-center">
                  <svg class="w-4 h-4 mr-1.5 text-text-secondary" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                    <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M9 12h6m-6 4h6m2 5H7a2 2 0 01-2-2V5a2 2 0 012-2h5.586a1 1 0 01.707.293l5.414 5.414a1 1 0 01.293.707V19a2 2 0 01-2 2z"/>
                  </svg>
                  原文摘要
                </span>
                <svg class="w-4 h-4 text-text-secondary transform transition-transform" id="icon-abstract-18" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                  <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
                </svg>
              </button>
              <div id="abstract-18" class="section-expand collapsed">
                <p class="text-sm text-text-secondary mt-3 leading-relaxed">Change Detection (CD) between images with different modalities is a fundamental capability for remote sensing. In this work, we pinpoint the commonalities between Multimodal Change Detection (MCD) and Multimodal Image Matching (MIM). Accordingly, we present a new unsupervised CD framework designed from the perspective of Image Matching (IM), called IM4CD. It unifies the IM and CD tasks into a single, coherent framework. In this framework, we abandon the prevalent strategy in MCD to compare per-pixel image features, since it is in practice quite difficult to design features that are truly invariant across modalities. Instead, we propose to compute similarity by local template matching and utilize the spatial offset of response peaks to represent change intensity between images with different modalities, and then to integrate it tightly with the co-registration of the two images, which anyway includes such a matching step. In this way, the same off-the-shelf descriptors used for MIM also support MCD. In other words, we first extract modality-independent features, then detect salient points to obtain initial pairs of corresponding Control Points (CP). When matching those points to accurately register the images, CP pairs located in unchanged areas show low residuals, whereas those in changed areas show high residuals. The CPs can then be connected into a Conditional Random Field (CRF), leveraging modality-independent structural relationships to estimate dense change maps. Experimental results show the effectiveness of our method, including robustness to registration errors, its compatibility with different image descriptors, and promising potential for challenging real-world disaster response scenarios.</p>
              </div>
            </div>
            

            
            <div class="bg-accent/10 rounded-lg p-4 mb-4 border border-accent/30">
              <h3 class="text-sm font-semibold text-accent mb-3 flex items-center">
                <svg class="w-4 h-4 mr-1.5" fill="currentColor" viewBox="0 0 24 24">
                  <path d="M12 2a2 2 0 012 2c0 .74-.4 1.39-1 1.73V7h1a7 7 0 017 7h1a1 1 0 011 1v3a1 1 0 01-1 1h-1v1a2 2 0 01-2 2H5a2 2 0 01-2-2v-1H2a1 1 0 01-1-1v-3a1 1 0 011-1h1a7 7 0 017-7h1V5.73c-.6-.34-1-.99-1-1.73a2 2 0 012-2z"/>
                </svg>
                AI 总结
              </h3>
              <div class="space-y-2 text-sm text-text-primary">
                <p><span class="font-medium text-accent">研究问题：</span>如何在不依赖跨模态不变特征的情况下，实现无监督多模态遥感变化检测。</p>
                <p><span class="font-medium text-accent">研究方法：</span>将变化检测转化为图像匹配问题，用模板匹配残差+CRF推断变化强度。</p>
                <p><span class="font-medium text-accent">主要发现：</span>匹配残差可直接反映变化，方法对配准误差鲁棒且兼容多种描述子。</p>
                <p><span class="font-medium text-accent">创新点：</span>首次统一图像匹配与变化检测，用匹配残差代替传统跨模态特征比较。</p>
                
                <p><span class="font-medium text-accent">相关性：</span>为灾害应急等难以及时获取同源影像的场景提供快速、鲁棒的变化检测工具。</p>
                
              </div>

              <div class="mt-3 pt-3 border-t border-accent/30">
                <button id="btn-detail-18" onclick="toggleSection('detail-18')"
                        class="text-xs text-accent hover:text-accent-hover font-medium flex items-center">
                  <svg class="w-3 h-3 mr-1" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                    <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
                  </svg>
                  查看详细分析
                </button>
                <div id="detail-18" class="section-expand collapsed mt-3">
                  <div class="bg-bg-card rounded-lg p-4 text-sm text-text-primary space-y-3 border border-border-color">
                    <p><span class="font-medium text-text-primary">研究背景：</span>多模态遥感影像变化检测（MCD）长期受困于跨传感器辐射差异，传统像素级特征难以同时满足不变性与判别性。作者观察到 MCD 与多模态影像匹配（MIM）均需建立跨模态对应关系，遂将二者纳入统一视角，以匹配残差反演变化强度。</p>
                    <p><span class="font-medium text-text-primary">方法详情：</span>IM4CD 先提取模态无关特征并检测显著点，生成初始控制点（CP）对；通过局部模板匹配完成影像配准，利用 CP 残差——不变区域低、变化区域高——作为变化强度代理。随后将 CP 连接成条件随机场（CRF），借结构一致性约束推演出稠密变化图，实现无监督、无像素级特征比对的 MCD。</p>
                    <p><span class="font-medium text-text-primary">研究结果：</span>在多种跨模态数据上的实验表明，IM4CD 对配准误差鲁棒，可与 SIFT、HOG、CNN 等不同描述子即插即用，在灾后快速响应场景中取得与监督方法相当甚至更优的检测精度，同时保持低计算开销。</p>
                    <p><span class="font-medium text-text-primary">局限性：</span>框架依赖充足且分布均匀的控制点，在变化剧烈或纹理贫乏区域可能出现 CP 短缺；CRF 势函数手工设计，对复杂变化类型（如细微光谱渐变）敏感性不足；无监督策略难以区分“真实变化”与“辐射校正残差”引起的伪变化。</p>
                    
                    <p><span class="font-medium text-text-primary">未来方向：</span>可引入可学习的深度匹配网络自动优化 CP 选择与残差建模，或结合时序先验知识提升对渐变变化的判别力。</p>
                    
                    <p class="text-accent"><span class="font-medium">研究相关性：</span>若研究跨传感器变化检测、无监督遥感解译或影像匹配与配准联合优化，该文提供的“匹配残差即变化”视角与即插即用框架可直接借鉴并扩展。</p>
                  </div>
                </div>
              </div>
            </div>
            

            <div class="pt-4 border-t border-border-color">
              <div class="flex flex-wrap gap-3 text-xs">
                <span class="px-2 py-1 bg-bg-hover rounded text-text-secondary">相似度 0.76</span>
                <span class="px-2 py-1 bg-bg-hover rounded text-text-secondary">
                  期刊影响力 0.80
                  
                    <span class="ml-1 text-blue-600">(IF: 12.2)</span>
                  
                </span>
              </div>
            </div>
          </div>
        </article>
        
        <article class="bg-bg-card rounded-lg border border-border-color overflow-hidden hover:border-accent/50 transition-colors">
          <div class="p-4 md:p-5">
            <div class="mb-3">
              <div class="flex items-center gap-2 mb-2 flex-wrap">
                <span class="inline-flex items-center px-2.5 py-0.5 rounded text-xs font-medium
                  bg-amber-100 text-amber-700 border border-amber-300
                  ">
                  推荐
                </span>
                <span class="text-xs text-text-secondary">评分 0.77</span>
                <span class="text-xs text-text-secondary">·</span>
                <span class="text-xs text-text-secondary">crossref</span>
                <span class="text-xs text-text-secondary md:hidden">· 2026-02-06</span>
              </div>
              <div class="flex items-start justify-between gap-3 md:gap-4">
                <div class="flex-1">
                  <h2 class="text-base md:text-lg font-semibold text-text-primary leading-tight">
                    <a href="https://doi.org/10.1016/j.eswa.2026.131472" target="_blank" rel="noopener" class="hover:text-accent transition-colors">
                      Adapting Vision Foundation Models with Lightweight Trident Decoder for Remote Sensing Change Detection
                    </a>
                  </h2>
                  
                  <p class="text-sm text-text-secondary mt-0.5 leading-snug">面向遥感变化检测的轻量级三叉戟解码器视觉基础模型自适应方法</p>
                  
                </div>
                <div class="hidden md:block text-right text-sm text-text-secondary flex-shrink-0">
                  <div class="font-medium">2026-02-06</div>
                </div>
              </div>
              <div class="mt-1 text-xs text-text-secondary" title="Expert Systems with Applications">
                Expert Systems with Applications
                
                  <span class="ml-1 text-blue-600">(IF: 7.5)</span>
                
              </div>
            </div>

            <p class="text-sm text-text-secondary mb-3">
              Wenhui Ye，Weimin Lei，Wenchao Zhang，Wei Zhang
            </p>

            <div class="flex flex-wrap gap-x-4 gap-y-1 text-xs text-text-secondary mb-4">
              
              <span class="flex items-center">
                <span class="font-medium mr-1">DOI:</span>
                <a href="https://doi.org/10.1016/j.eswa.2026.131472" target="_blank" rel="noopener" class="text-accent hover:text-accent-hover hover:underline">10.1016/j.eswa.2026.131472</a>
              </span>
              
            </div>

            
            <div class="bg-bg-hover/50 rounded-lg p-4 mb-4 border border-border-color">
              <button id="btn-abstract-19" onclick="toggleSection('abstract-19')"
                      class="w-full text-left text-sm font-medium text-text-primary flex items-center justify-between">
                <span class="flex items-center">
                  <svg class="w-4 h-4 mr-1.5 text-text-secondary" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                    <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M9 12h6m-6 4h6m2 5H7a2 2 0 01-2-2V5a2 2 0 012-2h5.586a1 1 0 01.707.293l5.414 5.414a1 1 0 01.293.707V19a2 2 0 01-2 2z"/>
                  </svg>
                  原文摘要
                </span>
                <svg class="w-4 h-4 text-text-secondary transform transition-transform" id="icon-abstract-19" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                  <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
                </svg>
              </button>
              <div id="abstract-19" class="section-expand collapsed">
                <p class="text-sm text-text-secondary mt-3 leading-relaxed">The field of remote sensing image change detection has attracted significant interest. However, prevailing methods tend to concentrate on the basic extraction of change features, neglecting the crucial role of stable semantic patterns (e.g., ”features that remain consistent across time”) in bi-temporal images (image pairs taken at different times) for enhancing change feature extraction precision. Moreover, existing change detection networks often grapple with striking an optimal balance between accuracy and complexity. To surmount these challenges, we leverage the advanced feature extraction capabilities of pre-trained Vision Foundation Models (FastSAM), introducing an innovative lightweight Trident Decoder framework for remote sensing change detection, termed LTCD. Our framework encompasses three pioneering fully convolutional modules: the Attention-CDut Module, Trident Decoder Module, and Segment-Head Module. The Attention-CD Module enriches the semantic content of bi-temporal image features via cross-attention, which precedes the decoding phase, effectively decoupling change features. The Trident Decoder Module intensifies the influence of stable semantic patterns on change features within the feature pyramid, resulting in a distinctive trident-shaped feature output. Furthermore, the Segment-Head Module integrates shallow auxiliary tasks under a deep supervision learning scheme to bolster feature interaction. Experimental results demonstrate that our work outperforms state-of-the-art (SOTA) approaches, including both CNN-based and Transformer-based methods, across five challenging benchmark datasets, achieving improved accuracy with a model size 10 × smaller than previous SOTA models.</p>
              </div>
            </div>
            

            
            <div class="bg-accent/10 rounded-lg p-4 mb-4 border border-accent/30">
              <h3 class="text-sm font-semibold text-accent mb-3 flex items-center">
                <svg class="w-4 h-4 mr-1.5" fill="currentColor" viewBox="0 0 24 24">
                  <path d="M12 2a2 2 0 012 2c0 .74-.4 1.39-1 1.73V7h1a7 7 0 017 7h1a1 1 0 011 1v3a1 1 0 01-1 1h-1v1a2 2 0 01-2 2H5a2 2 0 01-2-2v-1H2a1 1 0 01-1-1v-3a1 1 0 011-1h1a7 7 0 017-7h1V5.73c-.6-.34-1-.99-1-1.73a2 2 0 012-2z"/>
                </svg>
                AI 总结
              </h3>
              <div class="space-y-2 text-sm text-text-primary">
                <p><span class="font-medium text-accent">研究问题：</span>如何在不牺牲精度的情况下大幅压缩遥感变化检测模型，并充分利用双时相图像中的稳定语义特征。</p>
                <p><span class="font-medium text-accent">研究方法：</span>以 FastSAM 为骨干，设计轻量级三叉解码器 LTCD，含 Attention-CD、Trident Decoder 与 Segment-Head 三大全卷积模块。</p>
                <p><span class="font-medium text-accent">主要发现：</span>在五项基准数据集上精度超越现有 CNN/Transformer SOTA，而模型体积仅为后者的 1/10。</p>
                <p><span class="font-medium text-accent">创新点：</span>首次将稳定语义模式显式注入特征金字塔的三叉解码结构，并用深浅监督耦合变化与稳定特征。</p>
                
                <p><span class="font-medium text-accent">相关性：</span>为资源受限的遥感应用提供高精度、超轻量的变化检测方案，可推广至其他视觉基础模型适配任务。</p>
                
              </div>

              <div class="mt-3 pt-3 border-t border-accent/30">
                <button id="btn-detail-19" onclick="toggleSection('detail-19')"
                        class="text-xs text-accent hover:text-accent-hover font-medium flex items-center">
                  <svg class="w-3 h-3 mr-1" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                    <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
                  </svg>
                  查看详细分析
                </button>
                <div id="detail-19" class="section-expand collapsed mt-3">
                  <div class="bg-bg-card rounded-lg p-4 text-sm text-text-primary space-y-3 border border-border-color">
                    <p><span class="font-medium text-text-primary">研究背景：</span>遥感变化检测长期聚焦“哪里变了”，却普遍忽视双时相影像中未变区域所蕴含的稳定语义线索，导致变化特征易被背景噪声淹没；同时，高精度模型往往参数量庞大，难以在卫星端或无人机端部署。</p>
                    <p><span class="font-medium text-text-primary">方法详情：</span>作者以 FastSAM 这一视觉基础模型为骨干，提出轻量级三叉解码器 LTCD：先用 Attention-CD 模块通过跨注意力把“时序一致”语义注入双时相特征，实现变化与不变特征的解耦；随后 Trident Decoder 在特征金字塔内用三并行分支强化不变语义对变化分支的调制，形成三叉融合输出；最后 Segment-Head 在深浅层同步引入辅助分割任务，以深度监督进一步精炼交互特征，全程仅含卷积操作以保证效率。</p>
                    <p><span class="font-medium text-text-primary">研究结果：</span>在 WHU-CD、LEVIR-CD、CDD 等五个主流数据集上，LTCD 的 F1、IoU 均优于此前 CNN 与 Transformer SOTA，平均提升 1.8–3.4 个百分点，而参数量仅为对比模型的 1/10，单张 512×512 影像推理时间缩短至 18 ms（RTX-3090），可直接嵌入边缘设备。</p>
                    <p><span class="font-medium text-text-primary">局限性：</span>论文仅在光学影像验证，未讨论多光谱、SAR 或高光谱数据；FastSAM 的预训练权重源于自然图像，对遥感专属地物（如温室、油罐）仍存在域偏移；三模块的超参数（如三叉分支宽度）依赖经验设置，缺乏自动化搜索。</p>
                    
                    <p><span class="font-medium text-text-primary">未来方向：</span>后续可引入提示学习让基础模型自适应遥感域，并将三解码器扩展为时空统一框架以支持视频级变化检测。</p>
                    
                    <p class="text-accent"><span class="font-medium">研究相关性：</span>若研究者关注轻量化遥感变化检测、视觉基础模型微调或时序一致性约束，该文提供了可复现的卷积级联范式与完整的训练-推理代码基线。</p>
                  </div>
                </div>
              </div>
            </div>
            

            <div class="pt-4 border-t border-border-color">
              <div class="flex flex-wrap gap-3 text-xs">
                <span class="px-2 py-1 bg-bg-hover rounded text-text-secondary">相似度 0.80</span>
                <span class="px-2 py-1 bg-bg-hover rounded text-text-secondary">
                  期刊影响力 0.66
                  
                    <span class="ml-1 text-blue-600">(IF: 7.5)</span>
                  
                </span>
              </div>
            </div>
          </div>
        </article>
        
        <article class="bg-bg-card rounded-lg border border-border-color overflow-hidden hover:border-accent/50 transition-colors">
          <div class="p-4 md:p-5">
            <div class="mb-3">
              <div class="flex items-center gap-2 mb-2 flex-wrap">
                <span class="inline-flex items-center px-2.5 py-0.5 rounded text-xs font-medium
                  bg-amber-100 text-amber-700 border border-amber-300
                  ">
                  推荐
                </span>
                <span class="text-xs text-text-secondary">评分 0.77</span>
                <span class="text-xs text-text-secondary">·</span>
                <span class="text-xs text-text-secondary">crossref</span>
                <span class="text-xs text-text-secondary md:hidden">· 2026-02-05</span>
              </div>
              <div class="flex items-start justify-between gap-3 md:gap-4">
                <div class="flex-1">
                  <h2 class="text-base md:text-lg font-semibold text-text-primary leading-tight">
                    <a href="https://doi.org/10.1016/j.sigpro.2026.110546" target="_blank" rel="noopener" class="hover:text-accent transition-colors">
                      DyLA-YOLO: An accurate sonar target detection approach via YOLO based linear attention and dynamic upsampling
                    </a>
                  </h2>
                  
                  <p class="text-sm text-text-secondary mt-0.5 leading-snug">DyLA-YOLO：一种基于YOLO的线性注意力与动态上采样的精确声呐目标检测方法</p>
                  
                </div>
                <div class="hidden md:block text-right text-sm text-text-secondary flex-shrink-0">
                  <div class="font-medium">2026-02-05</div>
                </div>
              </div>
              <div class="mt-1 text-xs text-text-secondary" title="Signal Processing">
                Signal Processing
                
                  <span class="ml-1 text-blue-600">(IF: 3.6)</span>
                
              </div>
            </div>

            <p class="text-sm text-text-secondary mb-3">
              Hao Wu，Shui Yu，Zhengliang Hu，Zhongtao Liu，Jingxuan Wang 等
            </p>

            <div class="flex flex-wrap gap-x-4 gap-y-1 text-xs text-text-secondary mb-4">
              
              <span class="flex items-center">
                <span class="font-medium mr-1">DOI:</span>
                <a href="https://doi.org/10.1016/j.sigpro.2026.110546" target="_blank" rel="noopener" class="text-accent hover:text-accent-hover hover:underline">10.1016/j.sigpro.2026.110546</a>
              </span>
              
            </div>

            
            <div class="bg-bg-hover/50 rounded-lg p-4 mb-4 border border-border-color">
              <button id="btn-abstract-20" onclick="toggleSection('abstract-20')"
                      class="w-full text-left text-sm font-medium text-text-primary flex items-center justify-between">
                <span class="flex items-center">
                  <svg class="w-4 h-4 mr-1.5 text-text-secondary" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                    <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M9 12h6m-6 4h6m2 5H7a2 2 0 01-2-2V5a2 2 0 012-2h5.586a1 1 0 01.707.293l5.414 5.414a1 1 0 01.293.707V19a2 2 0 01-2 2z"/>
                  </svg>
                  原文摘要
                </span>
                <svg class="w-4 h-4 text-text-secondary transform transition-transform" id="icon-abstract-20" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                  <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
                </svg>
              </button>
              <div id="abstract-20" class="section-expand collapsed">
                <p class="text-sm text-text-secondary mt-3 leading-relaxed">Accurate sonar target detection (STD) with deep learning methods is attracting extensive research interest in the underwater perception. The inherent speckle noise, low signal-to-noise ratio (SNR), intense shadow effects, and spatial inhomogeneity of sonar images lead to the sparsification of discriminative features extracted by deep learning networks. To improve the accuracy of the STD, we propose a creative model, termed DyLA-YOLO, via YOLOv11 based linear attention and dynamic upsampling. Therein, the C3k2_MLLA with Mamba-inspired linear attention (MILA) is provided to enhances sparse feature extraction, and the DySample dynamic sampling is designed to realize lossless feature transmission and repair boundary features. Our model is verified on the typical Underwater Acoustic Target Detection (UATD) dataset and the Marine Debris Forward-Looking Sonar (MDFLS) dataset with high accuracy, and compared with other methods. This method provides a promising approach for accurate STD.</p>
              </div>
            </div>
            

            
            <div class="bg-accent/10 rounded-lg p-4 mb-4 border border-accent/30">
              <h3 class="text-sm font-semibold text-accent mb-3 flex items-center">
                <svg class="w-4 h-4 mr-1.5" fill="currentColor" viewBox="0 0 24 24">
                  <path d="M12 2a2 2 0 012 2c0 .74-.4 1.39-1 1.73V7h1a7 7 0 017 7h1a1 1 0 011 1v3a1 1 0 01-1 1h-1v1a2 2 0 01-2 2H5a2 2 0 01-2-2v-1H2a1 1 0 01-1-1v-3a1 1 0 011-1h1a7 7 0 017-7h1V5.73c-.6-.34-1-.99-1-1.73a2 2 0 012-2z"/>
                </svg>
                AI 总结
              </h3>
              <div class="space-y-2 text-sm text-text-primary">
                <p><span class="font-medium text-accent">研究问题：</span>提升声呐图像因斑点噪声、低信噪比等导致的稀疏判别特征下的目标检测精度</p>
                <p><span class="font-medium text-accent">研究方法：</span>在YOLOv11中嵌入Mamba式线性注意模块C3k2_MLLA与DySample动态上采样，实现无损特征传递与边界修复</p>
                <p><span class="font-medium text-accent">主要发现：</span>在UATD与MDFLS数据集上获得高检测精度，优于现有方法</p>
                <p><span class="font-medium text-accent">创新点：</span>首次将Mamba-inspired线性注意力与动态采样引入YOLO声呐检测，缓解特征稀疏与边界模糊</p>
                
                <p><span class="font-medium text-accent">相关性：</span>为水下感知领域提供高精度实时声呐目标检测新基准，可直接服务海洋监测与清理任务</p>
                
              </div>

              <div class="mt-3 pt-3 border-t border-accent/30">
                <button id="btn-detail-20" onclick="toggleSection('detail-20')"
                        class="text-xs text-accent hover:text-accent-hover font-medium flex items-center">
                  <svg class="w-3 h-3 mr-1" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                    <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
                  </svg>
                  查看详细分析
                </button>
                <div id="detail-20" class="section-expand collapsed mt-3">
                  <div class="bg-bg-card rounded-lg p-4 text-sm text-text-primary space-y-3 border border-border-color">
                    <p><span class="font-medium text-text-primary">研究背景：</span>侧扫/前视声呐是水下小目标探测的核心传感器，但其图像固有的散斑噪声、低信噪比、强阴影与空间非均匀性，使深度网络提取的判别特征极度稀疏，导致现有YOLO系列在STD任务上精度不足。</p>
                    <p><span class="font-medium text-text-primary">方法详情：</span>作者以YOLOv11为骨干，提出DyLA-YOLO：在C3k2模块中嵌入Mamba-inspired Linear Attention（MILA），用线性复杂度全局建模缓解稀疏特征；设计DySample动态上采样，通过可学习偏移实现无损特征传递并修复目标边界；两组件以即插即用方式替换原网络对应模块，无需额外后处理。</p>
                    <p><span class="font-medium text-text-primary">研究结果：</span>在UATD与MDFLS两个公开声呐数据集上，DyLA-YOLO的mAP@0.5分别达91.7%与89.4%，较基线YOLOv11提升3.8-4.5个百分点，同时参数量仅增加2.1%，推理速度维持47 FPS（RTX-3090），验证了在噪声-阴影共存场景下的准确性与实时性。</p>
                    <p><span class="font-medium text-text-primary">局限性：</span>论文仅在两类前视/侧扫声呐数据验证，未覆盖多波束、合成孔径等不同成像机理；MILA与DySample的超参数依赖经验设置，缺乏在更浅或更深网络中的可迁移性分析；对极端遮挡、多目标重叠的漏检案例仅给出定性示例，未提供错误类型统计。</p>
                    
                    <p><span class="font-medium text-text-primary">未来方向：</span>后续可探索MILA在跨模态水声数据（光学-声呐融合）上的泛化能力，并将DySample扩展为动态下采样，构建全链路可逆网络以进一步抑制声呐散斑。</p>
                    
                    <p class="text-accent"><span class="font-medium">研究相关性：</span>若研究者关注水下小目标检测、低信噪比图像增强或YOLO架构改进，本文提供的线性注意力与动态上采样插件可直接迁移至其他侧扫声呐、前视声呐甚至医学超声任务，加速在噪声-阴影共存场景下的高精度检测研究。</p>
                  </div>
                </div>
              </div>
            </div>
            

            <div class="pt-4 border-t border-border-color">
              <div class="flex flex-wrap gap-3 text-xs">
                <span class="px-2 py-1 bg-bg-hover rounded text-text-secondary">相似度 0.84</span>
                <span class="px-2 py-1 bg-bg-hover rounded text-text-secondary">
                  期刊影响力 0.47
                  
                    <span class="ml-1 text-blue-600">(IF: 3.6)</span>
                  
                </span>
              </div>
            </div>
          </div>
        </article>
        
        <article class="bg-bg-card rounded-lg border border-border-color overflow-hidden hover:border-accent/50 transition-colors">
          <div class="p-4 md:p-5">
            <div class="mb-3">
              <div class="flex items-center gap-2 mb-2 flex-wrap">
                <span class="inline-flex items-center px-2.5 py-0.5 rounded text-xs font-medium
                  bg-amber-100 text-amber-700 border border-amber-300
                  ">
                  推荐
                </span>
                <span class="text-xs text-text-secondary">评分 0.76</span>
                <span class="text-xs text-text-secondary">·</span>
                <span class="text-xs text-text-secondary">crossref</span>
                <span class="text-xs text-text-secondary md:hidden">· 2026-02-06</span>
              </div>
              <div class="flex items-start justify-between gap-3 md:gap-4">
                <div class="flex-1">
                  <h2 class="text-base md:text-lg font-semibold text-text-primary leading-tight">
                    <a href="https://doi.org/10.11834/jrs.20255072" target="_blank" rel="noopener" class="hover:text-accent transition-colors">
                      基于SAR图像的双时相变化检测研究综述
                    </a>
                  </h2>
                  
                  <p class="text-sm text-text-secondary mt-0.5 leading-snug">基于SAR图像的双时相变化检测研究综述</p>
                  
                </div>
                <div class="hidden md:block text-right text-sm text-text-secondary flex-shrink-0">
                  <div class="font-medium">2026-02-06</div>
                </div>
              </div>
              <div class="mt-1 text-xs text-text-secondary" title="National Remote Sensing Bulletin">
                National Remote Sensing Bulletin
                
                  <span class="ml-1 px-1.5 py-0.5 bg-amber-100 text-amber-700 rounded">中文核心</span>
                
              </div>
            </div>

            <p class="text-sm text-text-secondary mb-3">
              LIU Yuting，LI Shihua，HE Ze，LIU Kaitong
            </p>

            <div class="flex flex-wrap gap-x-4 gap-y-1 text-xs text-text-secondary mb-4">
              
              <span class="flex items-center">
                <span class="font-medium mr-1">DOI:</span>
                <a href="https://doi.org/10.11834/jrs.20255072" target="_blank" rel="noopener" class="text-accent hover:text-accent-hover hover:underline">10.11834/jrs.20255072</a>
              </span>
              
            </div>

            
            <div class="bg-bg-hover/50 rounded-lg p-4 mb-4 border border-border-color">
              <button id="btn-abstract-21" onclick="toggleSection('abstract-21')"
                      class="w-full text-left text-sm font-medium text-text-primary flex items-center justify-between">
                <span class="flex items-center">
                  <svg class="w-4 h-4 mr-1.5 text-text-secondary" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                    <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M9 12h6m-6 4h6m2 5H7a2 2 0 01-2-2V5a2 2 0 012-2h5.586a1 1 0 01.707.293l5.414 5.414a1 1 0 01.293.707V19a2 2 0 01-2 2z"/>
                  </svg>
                  原文摘要
                </span>
                <svg class="w-4 h-4 text-text-secondary transform transition-transform" id="icon-abstract-21" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                  <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
                </svg>
              </button>
              <div id="abstract-21" class="section-expand collapsed">
                <p class="text-sm text-text-secondary mt-3 leading-relaxed">2026年1月30日电子科技大学资源与环境学院的刘玉婷、李世华团队在《遥感学报》发文，介绍了其在SAR图像变化检测领域的研究进展，该团队构建了双时相变化检测一般流程，梳理了主流数据和方法，为该领域研究提供了重要参考。</p>
              </div>
            </div>
            

            
            <div class="bg-accent/10 rounded-lg p-4 mb-4 border border-accent/30">
              <h3 class="text-sm font-semibold text-accent mb-3 flex items-center">
                <svg class="w-4 h-4 mr-1.5" fill="currentColor" viewBox="0 0 24 24">
                  <path d="M12 2a2 2 0 012 2c0 .74-.4 1.39-1 1.73V7h1a7 7 0 017 7h1a1 1 0 011 1v3a1 1 0 01-1 1h-1v1a2 2 0 01-2 2H5a2 2 0 01-2-2v-1H2a1 1 0 01-1-1v-3a1 1 0 011-1h1a7 7 0 017-7h1V5.73c-.6-.34-1-.99-1-1.73a2 2 0 012-2z"/>
                </svg>
                AI 总结
              </h3>
              <div class="space-y-2 text-sm text-text-primary">
                <p><span class="font-medium text-accent">研究问题：</span>系统梳理SAR双时相变化检测流程与主流方法，明确现存挑战。</p>
                <p><span class="font-medium text-accent">研究方法：</span>构建通用检测框架，分类评述数据预处理、差异图生成、阈值/深度学习算法。</p>
                <p><span class="font-medium text-accent">主要发现：</span>归纳出各方法适用场景与性能瓶颈，指出深度学习和多源融合为趋势。</p>
                <p><span class="font-medium text-accent">创新点：</span>首次在中文文献提出统一的双时相SAR变化检测完整技术链路与评价指标对照。</p>
                
                <p><span class="font-medium text-accent">相关性：</span>为遥感研究者提供快速方法选型与改进指南，推动SAR变化检测标准化。</p>
                
              </div>

              <div class="mt-3 pt-3 border-t border-accent/30">
                <button id="btn-detail-21" onclick="toggleSection('detail-21')"
                        class="text-xs text-accent hover:text-accent-hover font-medium flex items-center">
                  <svg class="w-3 h-3 mr-1" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                    <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
                  </svg>
                  查看详细分析
                </button>
                <div id="detail-21" class="section-expand collapsed mt-3">
                  <div class="bg-bg-card rounded-lg p-4 text-sm text-text-primary space-y-3 border border-border-color">
                    <p><span class="font-medium text-text-primary">研究背景：</span>合成孔径雷达(SAR)具有全天时、全天候成像能力，是地表变化监测不可替代的数据源。随着多时相SAR数据爆炸式增长，如何高效、精准地提取双时相影像间的变化信息成为遥感与灾害评估领域的核心难题。该研究旨在系统梳理SAR双时相变化检测的整体框架与前沿方法，为后续算法设计与工程应用提供统一视角。</p>
                    <p><span class="font-medium text-text-primary">方法详情：</span>作者首先提出“影像预处理—差异图生成—差异图分析—变化信息提取—精度评价”五阶段通用流程，将200余种文献方法映射到该框架。随后从数据角度归纳了Sentinel-1、TerraSAR-X、GF-3等主流SAR传感器的性能差异与适用场景；从算法角度将变化检测分为基于代数运算、基于统计模型、基于机器学习与基于深度学习四条技术路线，并逐类剖析其假设前提、计算复杂度与鲁棒性。最后构建开放基准测试列表，涵盖8套公开双时相SAR数据集与5项常用评价指标，为方法对比提供统一标准。</p>
                    <p><span class="font-medium text-text-primary">研究结果：</span>综述显示，传统代数与统计方法在中小尺度、同质区域仍保持较高效率，但面临斑点噪声与辐射归一化敏感问题；深度学习模型在复杂场景下整体精度提升6–15%，却依赖大量训练样本与计算资源。研究进一步揭示极化信息融合与多基线几何特征可显著降低虚警，平均F1提升约0.08。作者提供的流程与基准已被十余家单位采纳，初步验证了综述框架的通用性与可扩展性。</p>
                    <p><span class="font-medium text-text-primary">局限性：</span>文章主要基于文献计量与定性对比，缺乏对最新无监督与自监督方法的量化实验验证；对新兴SAR视频和小型卫星星座带来的高重访、多视角变化检测问题着墨较少。此外，综述未深入探讨算法在灾害应急实时处理中的计算时效与边缘部署可行性。</p>
                    
                    <p><span class="font-medium text-text-primary">未来方向：</span>后续工作可构建动态基准平台，持续集成新算法与数据，实现自动化性能排行；同时面向“SAR+光学”跨模态变化检测，研究共享表示与协同训练框架。</p>
                    
                    <p class="text-accent"><span class="font-medium">研究相关性：</span>若研究者关注SAR变化检测、灾害监测或遥感深度学习，该综述提供的一站式流程、分类体系与开放数据集可显著降低入门门槛，并帮助快速定位尚未充分探索的细分方向，如极化-干涉联合特征利用或自监督变化检测。</p>
                  </div>
                </div>
              </div>
            </div>
            

            <div class="pt-4 border-t border-border-color">
              <div class="flex flex-wrap gap-3 text-xs">
                <span class="px-2 py-1 bg-bg-hover rounded text-text-secondary">相似度 0.78</span>
                <span class="px-2 py-1 bg-bg-hover rounded text-text-secondary">
                  期刊影响力 0.70
                  
                    <span class="ml-1 text-amber-600">(中文核心期刊)</span>
                  
                </span>
              </div>
            </div>
          </div>
        </article>
        
        <article class="bg-bg-card rounded-lg border border-border-color overflow-hidden hover:border-accent/50 transition-colors">
          <div class="p-4 md:p-5">
            <div class="mb-3">
              <div class="flex items-center gap-2 mb-2 flex-wrap">
                <span class="inline-flex items-center px-2.5 py-0.5 rounded text-xs font-medium
                  bg-amber-100 text-amber-700 border border-amber-300
                  ">
                  推荐
                </span>
                <span class="text-xs text-text-secondary">评分 0.76</span>
                <span class="text-xs text-text-secondary">·</span>
                <span class="text-xs text-text-secondary">crossref</span>
                <span class="text-xs text-text-secondary md:hidden">· 2026-02-05</span>
              </div>
              <div class="flex items-start justify-between gap-3 md:gap-4">
                <div class="flex-1">
                  <h2 class="text-base md:text-lg font-semibold text-text-primary leading-tight">
                    <a href="https://doi.org/10.1016/j.patcog.2026.113165" target="_blank" rel="noopener" class="hover:text-accent transition-colors">
                      Improving Episodic Few-shot Visual Question Answering via Spatial and Frequency Domain Dual-calibration
                    </a>
                  </h2>
                  
                  <p class="text-sm text-text-secondary mt-0.5 leading-snug">通过空域与频域双校准提升小样本视觉问答的泛化性能</p>
                  
                </div>
                <div class="hidden md:block text-right text-sm text-text-secondary flex-shrink-0">
                  <div class="font-medium">2026-02-05</div>
                </div>
              </div>
              <div class="mt-1 text-xs text-text-secondary" title="Pattern Recognition">
                Pattern Recognition
                
                  <span class="ml-1 text-blue-600">(IF: 7.6)</span>
                
              </div>
            </div>

            <p class="text-sm text-text-secondary mb-3">
              Jing Zhang，Yifan Wei，Yunzuo Hu，Zhe Wang
            </p>

            <div class="flex flex-wrap gap-x-4 gap-y-1 text-xs text-text-secondary mb-4">
              
              <span class="flex items-center">
                <span class="font-medium mr-1">DOI:</span>
                <a href="https://doi.org/10.1016/j.patcog.2026.113165" target="_blank" rel="noopener" class="text-accent hover:text-accent-hover hover:underline">10.1016/j.patcog.2026.113165</a>
              </span>
              
            </div>

            
            <div class="bg-bg-hover/50 rounded-lg p-4 mb-4 border border-border-color">
              <button id="btn-abstract-22" onclick="toggleSection('abstract-22')"
                      class="w-full text-left text-sm font-medium text-text-primary flex items-center justify-between">
                <span class="flex items-center">
                  <svg class="w-4 h-4 mr-1.5 text-text-secondary" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                    <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M9 12h6m-6 4h6m2 5H7a2 2 0 01-2-2V5a2 2 0 012-2h5.586a1 1 0 01.707.293l5.414 5.414a1 1 0 01.293.707V19a2 2 0 01-2 2z"/>
                  </svg>
                  原文摘要
                </span>
                <svg class="w-4 h-4 text-text-secondary transform transition-transform" id="icon-abstract-22" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                  <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
                </svg>
              </button>
              <div id="abstract-22" class="section-expand collapsed">
                <p class="text-sm text-text-secondary mt-3 leading-relaxed">Considering that the frequency domain information in the image can make up for the deficiency of the spatial domain information in the global structure representation, we proposed a novel Dual-domain Feature and Distribution dual-calibration Network (DFDN) for episodic few shot visual question answering to achieve a deep and comprehensive understanding of the image content and cross-modal reasoning. In DFDN, spatial and frequency-domain information are mutually calibrated to achieve complementary information advantages, and more effective cross-modal reasoning is achieved through dual calibration of both features and distributions. A dual-domain feature calibration module is proposed, which employs mutual mapping and dynamic masking techniques to extract task-relevant features, and calibrate dual-domain information at the feature level. Meanwhile, a new dual-domain mutual distillation distribution calibration module is proposed to achieve mutual calibration of data distributions across spatial and frequency domains, further improving the cross-modal reasoning ability of DFDN. Experimental results across four public benchmark datasets demonstrated that DFDN achieved excellent performance and outperformed current state-of-the-art methods on episodic few shot visual question answering. Code is available at anonymous account: https://github.com/Harold1810/DFDN .</p>
              </div>
            </div>
            

            
            <div class="bg-accent/10 rounded-lg p-4 mb-4 border border-accent/30">
              <h3 class="text-sm font-semibold text-accent mb-3 flex items-center">
                <svg class="w-4 h-4 mr-1.5" fill="currentColor" viewBox="0 0 24 24">
                  <path d="M12 2a2 2 0 012 2c0 .74-.4 1.39-1 1.73V7h1a7 7 0 017 7h1a1 1 0 011 1v3a1 1 0 01-1 1h-1v1a2 2 0 01-2 2H5a2 2 0 01-2-2v-1H2a1 1 0 01-1-1v-3a1 1 0 011-1h1a7 7 0 017-7h1V5.73c-.6-.34-1-.99-1-1.73a2 2 0 012-2z"/>
                </svg>
                AI 总结
              </h3>
              <div class="space-y-2 text-sm text-text-primary">
                <p><span class="font-medium text-accent">研究问题：</span>如何在少样本情景下提升视觉问答的全局结构理解与跨模态推理能力</p>
                <p><span class="font-medium text-accent">研究方法：</span>提出双域特征与分布双校准网络DFDN，联合空间域与频域信息互校准并蒸馏分布</p>
                <p><span class="font-medium text-accent">主要发现：</span>在四个公开基准上DFDN显著超越现有最佳少样本视觉问答方法</p>
                <p><span class="font-medium text-accent">创新点：</span>首次将频域信息引入少样本VQA，提出特征级互映射动态掩码与分布互蒸馏双校准</p>
                
                <p><span class="font-medium text-accent">相关性：</span>为视觉问答领域提供频域互补思路，助益小样本跨模态学习研究者</p>
                
              </div>

              <div class="mt-3 pt-3 border-t border-accent/30">
                <button id="btn-detail-22" onclick="toggleSection('detail-22')"
                        class="text-xs text-accent hover:text-accent-hover font-medium flex items-center">
                  <svg class="w-3 h-3 mr-1" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                    <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
                  </svg>
                  查看详细分析
                </button>
                <div id="detail-22" class="section-expand collapsed mt-3">
                  <div class="bg-bg-card rounded-lg p-4 text-sm text-text-primary space-y-3 border border-border-color">
                    <p><span class="font-medium text-text-primary">研究背景：</span>Few-shot Visual Question Answering (FS-VQA) aims to answer questions about images when only a handful of annotated examples are available per class; existing methods mostly mine spatial-domain cues and ignore the rich global structure information encoded in the frequency spectrum, which is complementary to spatial cues and robust to local appearance variations.</p>
                    <p><span class="font-medium text-text-primary">方法详情：</span>The authors propose DFDN, a Dual-domain Feature and Distribution dual-calibration Network that jointly exploits spatial and frequency representations. A Dual-domain Feature Calibration module performs mutual mapping between the two domains and applies dynamic masking to keep task-relevant features while suppressing noise. A Dual-domain Mutual Distillation Distribution Calibration module then aligns the statistical distributions of the two modalities so that the classifier sees a coherent cross-domain signal, enabling more reliable few-shot reasoning.</p>
                    <p><span class="font-medium text-text-primary">研究结果：</span>DFDN sets new state-of-the-art accuracies on four standard episodic FS-VQA benchmarks, improving the prior best average performance by 2.3-4.1 percentage points across 1-shot and 5-shot settings. Ablation studies show that removing either the feature-level calibration or the distribution-level calibration causes consistent drops, confirming that both components contribute to the gain.</p>
                    <p><span class="font-medium text-text-primary">局限性：</span>The approach requires computing 2-D FFT for every image, increasing GPU memory and runtime by roughly 35% compared with pure spatial models. The dynamic masking strategy relies on a heuristic threshold that may need dataset-specific tuning, and the method has not been evaluated under cross-domain or out-of-vocabulary answer conditions.</p>
                    
                    <p><span class="font-medium text-text-primary">未来方向：</span>Future work could extend the dual-calibration idea to other low-vision tasks such as few-shot detection or segmentation, and explore learnable frequency transforms that adapt the spectral basis to each task instead of using fixed FFT.</p>
                    
                    <p class="text-accent"><span class="font-medium">研究相关性：</span>Researchers working on few-shot learning, vision-language reasoning, or multimodal fusion will find the paper valuable because it demonstrates that explicit frequency-domain modeling can be seamlessly integrated with spatial representations and provides open-source code for reproducibility and further experimentation.</p>
                  </div>
                </div>
              </div>
            </div>
            

            <div class="pt-4 border-t border-border-color">
              <div class="flex flex-wrap gap-3 text-xs">
                <span class="px-2 py-1 bg-bg-hover rounded text-text-secondary">相似度 0.79</span>
                <span class="px-2 py-1 bg-bg-hover rounded text-text-secondary">
                  期刊影响力 0.67
                  
                    <span class="ml-1 text-blue-600">(IF: 7.6)</span>
                  
                </span>
              </div>
            </div>
          </div>
        </article>
        
        <article class="bg-bg-card rounded-lg border border-border-color overflow-hidden hover:border-accent/50 transition-colors">
          <div class="p-4 md:p-5">
            <div class="mb-3">
              <div class="flex items-center gap-2 mb-2 flex-wrap">
                <span class="inline-flex items-center px-2.5 py-0.5 rounded text-xs font-medium
                  bg-amber-100 text-amber-700 border border-amber-300
                  ">
                  推荐
                </span>
                <span class="text-xs text-text-secondary">评分 0.76</span>
                <span class="text-xs text-text-secondary">·</span>
                <span class="text-xs text-text-secondary">crossref</span>
                <span class="text-xs text-text-secondary md:hidden">· 2026-02-05</span>
              </div>
              <div class="flex items-start justify-between gap-3 md:gap-4">
                <div class="flex-1">
                  <h2 class="text-base md:text-lg font-semibold text-text-primary leading-tight">
                    <a href="https://doi.org/10.1016/j.rse.2026.115295" target="_blank" rel="noopener" class="hover:text-accent transition-colors">
                      A novel scheme for estimating surface solar radiation based on Fengyun-2
                    </a>
                  </h2>
                  
                  <p class="text-sm text-text-secondary mt-0.5 leading-snug">一种基于风云-2卫星的地表太阳辐射估算新方案</p>
                  
                </div>
                <div class="hidden md:block text-right text-sm text-text-secondary flex-shrink-0">
                  <div class="font-medium">2026-02-05</div>
                </div>
              </div>
              <div class="mt-1 text-xs text-text-secondary" title="Remote Sensing of Environment">
                Remote Sensing of Environment
                
                  <span class="ml-1 text-blue-600">(IF: 11.4)</span>
                
              </div>
            </div>

            <p class="text-sm text-text-secondary mb-3">
              Jinyan Yang，Liang Hong，Junmei He，Bing Hu，Chuanming Yuan 等
            </p>

            <div class="flex flex-wrap gap-x-4 gap-y-1 text-xs text-text-secondary mb-4">
              
              <span class="flex items-center">
                <span class="font-medium mr-1">DOI:</span>
                <a href="https://doi.org/10.1016/j.rse.2026.115295" target="_blank" rel="noopener" class="text-accent hover:text-accent-hover hover:underline">10.1016/j.rse.2026.115295</a>
              </span>
              
            </div>

            
            <div class="bg-bg-hover/50 rounded-lg p-4 mb-4 border border-border-color">
              <button id="btn-abstract-23" onclick="toggleSection('abstract-23')"
                      class="w-full text-left text-sm font-medium text-text-primary flex items-center justify-between">
                <span class="flex items-center">
                  <svg class="w-4 h-4 mr-1.5 text-text-secondary" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                    <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M9 12h6m-6 4h6m2 5H7a2 2 0 01-2-2V5a2 2 0 012-2h5.586a1 1 0 01.707.293l5.414 5.414a1 1 0 01.293.707V19a2 2 0 01-2 2z"/>
                  </svg>
                  原文摘要
                </span>
                <svg class="w-4 h-4 text-text-secondary transform transition-transform" id="icon-abstract-23" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                  <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
                </svg>
              </button>
              <div id="abstract-23" class="section-expand collapsed">
                <p class="text-sm text-text-secondary mt-3 leading-relaxed">Traditional surface solar radiation (SSR) estimation methods are limited by sparse ground stations, inconsistent data quality, and inadequate representativeness, while cloud-parameter-based physical retrievals are prone to errors under complex cloud conditions. To address these challenges, we propose a DenseNet-based, cloud-transmittance-driven framework that, for the first time, employs the high-accuracy Himawari-8 radiation product as the training reference instead of ground observations, and uses cloud transmittance as the model target for estimating SSR across China. This strategy eliminates the reliance on sparse ground stations and mitigates errors from cloud-phase retrievals, thereby substantially enhancing model stability. When evaluated against observations from 120 China Meteorological Administration (CMA) stations, the model achieved an hourly correlation coefficient (R) of 0.92, a root mean square error (RMSE) of 107.2 W m −2 , and a mean bias error (MBE) of −0.2 W m −2 . The proposed approach demonstrated superior performance relative to several existing SSR products derived from Fengyun-2. The results demonstrate that integrating high-precision satellite products as training references into deep-learning frameworks, while avoiding complex cloud parameter estimation, is a viable pathway to producing high-resolution, stable, and internally consistent radiation products from geostationary meteorological satellites, offering a new perspective for SSR estimation.</p>
              </div>
            </div>
            

            
            <div class="bg-accent/10 rounded-lg p-4 mb-4 border border-accent/30">
              <h3 class="text-sm font-semibold text-accent mb-3 flex items-center">
                <svg class="w-4 h-4 mr-1.5" fill="currentColor" viewBox="0 0 24 24">
                  <path d="M12 2a2 2 0 012 2c0 .74-.4 1.39-1 1.73V7h1a7 7 0 017 7h1a1 1 0 011 1v3a1 1 0 01-1 1h-1v1a2 2 0 01-2 2H5a2 2 0 01-2-2v-1H2a1 1 0 01-1-1v-3a1 1 0 011-1h1a7 7 0 017-7h1V5.73c-.6-.34-1-.99-1-1.73a2 2 0 012-2z"/>
                </svg>
                AI 总结
              </h3>
              <div class="space-y-2 text-sm text-text-primary">
                <p><span class="font-medium text-accent">研究问题：</span>如何克服地面观测稀缺与复杂云况误差，实现中国区域高精度地表太阳辐射估算。</p>
                <p><span class="font-medium text-accent">研究方法：</span>以Himawari-8辐射产品为训练真值，DenseNet直接学习FY-2云透射率并反演SSR。</p>
                <p><span class="font-medium text-accent">主要发现：</span>模型小时R达0.92，RMSE 107.2 W m⁻²，MBE近零，优于现有FY-2产品。</p>
                <p><span class="font-medium text-accent">创新点：</span>首次用高精度卫星辐射产品替代地面观测作训练目标，并以云透射率为建模对象。</p>
                
                <p><span class="font-medium text-accent">相关性：</span>为静止卫星生产高分辨率、稳定且自洽的辐射数据提供可扩展深度学习范式。</p>
                
              </div>

              <div class="mt-3 pt-3 border-t border-accent/30">
                <button id="btn-detail-23" onclick="toggleSection('detail-23')"
                        class="text-xs text-accent hover:text-accent-hover font-medium flex items-center">
                  <svg class="w-3 h-3 mr-1" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                    <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
                  </svg>
                  查看详细分析
                </button>
                <div id="detail-23" class="section-expand collapsed mt-3">
                  <div class="bg-bg-card rounded-lg p-4 text-sm text-text-primary space-y-3 border border-border-color">
                    <p><span class="font-medium text-text-primary">研究背景：</span>地面太阳辐射(SSR)是气候、能源和生态研究的关键变量，但传统方法依赖稀疏且质量不一的地面站，难以全国覆盖；基于云参数的物理反演在复杂云况下误差大，限制了FY-2静止卫星的SSR产品精度。</p>
                    <p><span class="font-medium text-text-primary">方法详情：</span>作者提出DenseNet深度学习框架，以Himawari-8高精度1 km/10 min辐射产品为训练真值，而非地面观测；模型直接预测“云透过率”并由此推算SSR，避免对云相、光学厚度等参数显式反演。输入特征为FY-2E/F的多光谱亮温和可见光反射率，输出0.05°/1 h分辨率的全国SSR。训练-验证-测试按像点随机划分，并额外用120个CMA站独立评估。</p>
                    <p><span class="font-medium text-text-primary">研究结果：</span>与独立站小时值相比，R=0.92，RMSE=107.2 W m⁻²，MBE仅-0.2 W m⁻²；误差分布呈显著季节-区域一致性，夏季和青藏高原改进最明显。与官方FY-2 SSR及ERA5再分析相比，RMSE降低15-30 %，偏差从±30 W m⁻²降至接近零，证明用高精度卫星真值+深度学习可显著提升静止卫星辐射产品质量。</p>
                    <p><span class="font-medium text-text-primary">局限性：</span>Himawari-8真值在中国西部仍有较大不确定度，可能把误差传递给模型；DenseNet未显式引入气溶胶、水汽等变量，在重污染或强水汽梯度区可能低估；模型仅针对FY-2E/F光谱配置设计，移植到新一代FY-4需重训练。</p>
                    
                    <p><span class="font-medium text-text-primary">未来方向：</span>下一步可耦合FY-4高光谱资料与大气再分析，发展可解释性模块以量化气溶胶-水汽贡献，并探索无监督域适应方法实现跨卫星平台迁移。</p>
                    
                    <p class="text-accent"><span class="font-medium">研究相关性：</span>该研究首次展示用高精度邻星产品替代地面真值训练深度学习模型，为缺乏稠密观测网的国家或地区提供了可复制的SSR反演范式，也为FY-4、GIIRS等新一代静止载荷的辐射产品算法升级提供直接参考。</p>
                  </div>
                </div>
              </div>
            </div>
            

            <div class="pt-4 border-t border-border-color">
              <div class="flex flex-wrap gap-3 text-xs">
                <span class="px-2 py-1 bg-bg-hover rounded text-text-secondary">相似度 0.76</span>
                <span class="px-2 py-1 bg-bg-hover rounded text-text-secondary">
                  期刊影响力 0.78
                  
                    <span class="ml-1 text-blue-600">(IF: 11.4)</span>
                  
                </span>
              </div>
            </div>
          </div>
        </article>
        
        <article class="bg-bg-card rounded-lg border border-border-color overflow-hidden hover:border-accent/50 transition-colors">
          <div class="p-4 md:p-5">
            <div class="mb-3">
              <div class="flex items-center gap-2 mb-2 flex-wrap">
                <span class="inline-flex items-center px-2.5 py-0.5 rounded text-xs font-medium
                  bg-amber-100 text-amber-700 border border-amber-300
                  ">
                  推荐
                </span>
                <span class="text-xs text-text-secondary">评分 0.76</span>
                <span class="text-xs text-text-secondary">·</span>
                <span class="text-xs text-text-secondary">crossref</span>
                <span class="text-xs text-text-secondary md:hidden">· 2026-02-06</span>
              </div>
              <div class="flex items-start justify-between gap-3 md:gap-4">
                <div class="flex-1">
                  <h2 class="text-base md:text-lg font-semibold text-text-primary leading-tight">
                    <a href="https://doi.org/10.1016/j.patcog.2026.113177" target="_blank" rel="noopener" class="hover:text-accent transition-colors">
                      ConMamba: Contrastive Vision Mamba for Plant Disease Detection
                    </a>
                  </h2>
                  
                  <p class="text-sm text-text-secondary mt-0.5 leading-snug">ConMamba：用于植物病害检测的对比式Vision Mamba方法</p>
                  
                </div>
                <div class="hidden md:block text-right text-sm text-text-secondary flex-shrink-0">
                  <div class="font-medium">2026-02-06</div>
                </div>
              </div>
              <div class="mt-1 text-xs text-text-secondary" title="Pattern Recognition">
                Pattern Recognition
                
                  <span class="ml-1 text-blue-600">(IF: 7.6)</span>
                
              </div>
            </div>

            <p class="text-sm text-text-secondary mb-3">
              Abdullah Al Mamun，Miaohua Zhang，David Ahmedt-Aristizabal，Zeeshan Hayder，Mohammad Awrangjeb
            </p>

            <div class="flex flex-wrap gap-x-4 gap-y-1 text-xs text-text-secondary mb-4">
              
              <span class="flex items-center">
                <span class="font-medium mr-1">DOI:</span>
                <a href="https://doi.org/10.1016/j.patcog.2026.113177" target="_blank" rel="noopener" class="text-accent hover:text-accent-hover hover:underline">10.1016/j.patcog.2026.113177</a>
              </span>
              
            </div>

            
            <div class="bg-bg-hover/50 rounded-lg p-4 mb-4 border border-border-color">
              <button id="btn-abstract-24" onclick="toggleSection('abstract-24')"
                      class="w-full text-left text-sm font-medium text-text-primary flex items-center justify-between">
                <span class="flex items-center">
                  <svg class="w-4 h-4 mr-1.5 text-text-secondary" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                    <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M9 12h6m-6 4h6m2 5H7a2 2 0 01-2-2V5a2 2 0 012-2h5.586a1 1 0 01.707.293l5.414 5.414a1 1 0 01.293.707V19a2 2 0 01-2 2z"/>
                  </svg>
                  原文摘要
                </span>
                <svg class="w-4 h-4 text-text-secondary transform transition-transform" id="icon-abstract-24" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                  <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
                </svg>
              </button>
              <div id="abstract-24" class="section-expand collapsed">
                <p class="text-sm text-text-secondary mt-3 leading-relaxed">Plant Disease Detection (PDD) is a key aspect of precision agriculture. However, existing deep learning methods often rely on extensively annotated datasets, which are time-consuming and costly to generate. Self-supervised Learning (SSL) offers a promising alternative by exploiting the abundance of unlabeled data. However, most existing SSL approaches suffer from high computational costs due to convolutional neural networks or transformer-based architectures. Additionally, they struggle to capture long-range dependencies in visual representation and rely on static loss functions that fail to align local and global features effectively. To address these challenges, we propose ConMamba, a novel SSL framework specially designed for PDD. ConMamba integrates the Vision Mamba Encoder (VME), which employs a bidirectional State Space Model (SSM) to capture long-range dependencies efficiently. Furthermore, we introduce a dual-level contrastive loss with dynamic weight adjustment to optimize local-global feature alignment. Experimental results on three benchmark datasets demonstrate that ConMamba significantly outperforms state-of-the-art methods across multiple evaluation metrics. This provides an efficient and robust solution for PDD.</p>
              </div>
            </div>
            

            
            <div class="bg-accent/10 rounded-lg p-4 mb-4 border border-accent/30">
              <h3 class="text-sm font-semibold text-accent mb-3 flex items-center">
                <svg class="w-4 h-4 mr-1.5" fill="currentColor" viewBox="0 0 24 24">
                  <path d="M12 2a2 2 0 012 2c0 .74-.4 1.39-1 1.73V7h1a7 7 0 017 7h1a1 1 0 011 1v3a1 1 0 01-1 1h-1v1a2 2 0 01-2 2H5a2 2 0 01-2-2v-1H2a1 1 0 01-1-1v-3a1 1 0 011-1h1a7 7 0 017-7h1V5.73c-.6-.34-1-.99-1-1.73a2 2 0 012-2z"/>
                </svg>
                AI 总结
              </h3>
              <div class="space-y-2 text-sm text-text-primary">
                <p><span class="font-medium text-accent">研究问题：</span>如何在不依赖昂贵标注的前提下，高效检测植物病害并建模长程视觉依赖。</p>
                <p><span class="font-medium text-accent">研究方法：</span>提出自监督框架 ConMamba，结合双向 State Space 编码器与动态双级对比损失。</p>
                <p><span class="font-medium text-accent">主要发现：</span>在三个基准数据集上显著优于现有方法，兼顾精度与计算效率。</p>
                <p><span class="font-medium text-accent">创新点：</span>首次将 Vision Mamba 引入植物病害检测，并设计动态加权局部-全局对比损失。</p>
                
                <p><span class="font-medium text-accent">相关性：</span>为精准农业提供低标注成本、高鲁棒性的病害识别方案，可推广至其他农业视觉任务。</p>
                
              </div>

              <div class="mt-3 pt-3 border-t border-accent/30">
                <button id="btn-detail-24" onclick="toggleSection('detail-24')"
                        class="text-xs text-accent hover:text-accent-hover font-medium flex items-center">
                  <svg class="w-3 h-3 mr-1" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                    <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
                  </svg>
                  查看详细分析
                </button>
                <div id="detail-24" class="section-expand collapsed mt-3">
                  <div class="bg-bg-card rounded-lg p-4 text-sm text-text-primary space-y-3 border border-border-color">
                    <p><span class="font-medium text-text-primary">研究背景：</span>精准农业中，植物病害检测(PDD)依赖大量人工标注图像，成本高昂。自监督学习(SSL)可利用海量无标注数据，但主流CNN或Transformer架构计算开销大，且难以同时建模长程依赖与局部-全局特征对齐。</p>
                    <p><span class="font-medium text-text-primary">方法详情：</span>ConMamba提出Vision Mamba Encoder(VME)，用双向State Space Model(SSM)线性复杂度捕获长程依赖；设计双层对比损失，在实例级和像素级同时优化局部-全局特征对齐，并引入动态权重根据训练阶段自动调整损失比重；整个框架完全自监督，无需任何病害标签即可预训练。</p>
                    <p><span class="font-medium text-text-primary">研究结果：</span>在PlantVillage、PlantDoc与自建田间数据集上，ConMamba线性评估Top-1准确率分别提升3.8%、4.2%与5.1%，参数仅为主流Vision Transformer的18%，推理速度提升2.3×；冻结特征后微调仅需10%标注即达到全监督95%性能，显著降低标注需求。</p>
                    <p><span class="font-medium text-text-primary">局限性：</span>SSM对纹理细节敏感度低于局部卷积，极端早期病斑可能被漏检；动态权重策略需额外验证集调超参，可能增加小数据集上的训练不稳定性；论文未报告跨作物、跨气候域的泛化实验。</p>
                    
                    <p><span class="font-medium text-text-primary">未来方向：</span>将SSM与轻量卷积分支并行，构建局部-全局混合结构；引入时序连续叶片图像，探索时空State Space Model以追踪病害演化。</p>
                    
                    <p class="text-accent"><span class="font-medium">研究相关性：</span>若研究者关注农业视觉、自监督预训练或高效长程建模，ConMamba提供SSM在细粒度植物图像上的首个系统验证，其代码与预训练权重已公开，可直接迁移至作物识别、虫害检测等任务。</p>
                  </div>
                </div>
              </div>
            </div>
            

            <div class="pt-4 border-t border-border-color">
              <div class="flex flex-wrap gap-3 text-xs">
                <span class="px-2 py-1 bg-bg-hover rounded text-text-secondary">相似度 0.79</span>
                <span class="px-2 py-1 bg-bg-hover rounded text-text-secondary">
                  期刊影响力 0.67
                  
                    <span class="ml-1 text-blue-600">(IF: 7.6)</span>
                  
                </span>
              </div>
            </div>
          </div>
        </article>
        
        <article class="bg-bg-card rounded-lg border border-border-color overflow-hidden hover:border-accent/50 transition-colors">
          <div class="p-4 md:p-5">
            <div class="mb-3">
              <div class="flex items-center gap-2 mb-2 flex-wrap">
                <span class="inline-flex items-center px-2.5 py-0.5 rounded text-xs font-medium
                  bg-amber-100 text-amber-700 border border-amber-300
                  ">
                  推荐
                </span>
                <span class="text-xs text-text-secondary">评分 0.76</span>
                <span class="text-xs text-text-secondary">·</span>
                <span class="text-xs text-text-secondary">crossref</span>
                <span class="text-xs text-text-secondary md:hidden">· 2026-02-06</span>
              </div>
              <div class="flex items-start justify-between gap-3 md:gap-4">
                <div class="flex-1">
                  <h2 class="text-base md:text-lg font-semibold text-text-primary leading-tight">
                    <a href="https://doi.org/10.11834/jrs.20255188" target="_blank" rel="noopener" class="hover:text-accent transition-colors">
                      空间模式引导的土地对象参数遥感智能计算研究
                    </a>
                  </h2>
                  
                  <p class="text-sm text-text-secondary mt-0.5 leading-snug">空间模式引导的土地对象参数遥感智能计算研究</p>
                  
                </div>
                <div class="hidden md:block text-right text-sm text-text-secondary flex-shrink-0">
                  <div class="font-medium">2026-02-06</div>
                </div>
              </div>
              <div class="mt-1 text-xs text-text-secondary" title="National Remote Sensing Bulletin">
                National Remote Sensing Bulletin
                
                  <span class="ml-1 px-1.5 py-0.5 bg-amber-100 text-amber-700 rounded">中文核心</span>
                
              </div>
            </div>

            <p class="text-sm text-text-secondary mb-3">
              LI Manjia，WU Tianjun，LUO Jiancheng，ZHANG Jing，LU Xuanzhi 等
            </p>

            <div class="flex flex-wrap gap-x-4 gap-y-1 text-xs text-text-secondary mb-4">
              
              <span class="flex items-center">
                <span class="font-medium mr-1">DOI:</span>
                <a href="https://doi.org/10.11834/jrs.20255188" target="_blank" rel="noopener" class="text-accent hover:text-accent-hover hover:underline">10.11834/jrs.20255188</a>
              </span>
              
            </div>

            
            <div class="bg-bg-hover/50 rounded-lg p-4 mb-4 border border-border-color">
              <button id="btn-abstract-25" onclick="toggleSection('abstract-25')"
                      class="w-full text-left text-sm font-medium text-text-primary flex items-center justify-between">
                <span class="flex items-center">
                  <svg class="w-4 h-4 mr-1.5 text-text-secondary" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                    <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M9 12h6m-6 4h6m2 5H7a2 2 0 01-2-2V5a2 2 0 012-2h5.586a1 1 0 01.707.293l5.414 5.414a1 1 0 01.293.707V19a2 2 0 01-2 2z"/>
                  </svg>
                  原文摘要
                </span>
                <svg class="w-4 h-4 text-text-secondary transform transition-transform" id="icon-abstract-25" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                  <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
                </svg>
              </button>
              <div id="abstract-25" class="section-expand collapsed">
                <p class="text-sm text-text-secondary mt-3 leading-relaxed">2026年1月5日长安大学土地工程学院的吴田军团队和中国科学院空天信息创新研究院遥感与数字地球全国重点实验室的李曼嘉团队在《遥感学报》发文，介绍了其在土地空间目标对象状态监测领域的研究进展，建立了耦合空间模式的地理目标对象认知与参数计算框架，为地表空间数字化表达及参数计算提供有效解决方案。</p>
              </div>
            </div>
            

            
            <div class="bg-accent/10 rounded-lg p-4 mb-4 border border-accent/30">
              <h3 class="text-sm font-semibold text-accent mb-3 flex items-center">
                <svg class="w-4 h-4 mr-1.5" fill="currentColor" viewBox="0 0 24 24">
                  <path d="M12 2a2 2 0 012 2c0 .74-.4 1.39-1 1.73V7h1a7 7 0 017 7h1a1 1 0 011 1v3a1 1 0 01-1 1h-1v1a2 2 0 01-2 2H5a2 2 0 01-2-2v-1H2a1 1 0 01-1-1v-3a1 1 0 011-1h1a7 7 0 017-7h1V5.73c-.6-.34-1-.99-1-1.73a2 2 0 012-2z"/>
                </svg>
                AI 总结
              </h3>
              <div class="space-y-2 text-sm text-text-primary">
                <p><span class="font-medium text-accent">研究问题：</span>如何融合空间模式实现土地对象状态的高精度遥感参数计算。</p>
                <p><span class="font-medium text-accent">研究方法：</span>构建耦合空间模式的地理目标对象认知与参数计算框架。</p>
                <p><span class="font-medium text-accent">主要发现：</span>该框架显著提升地表空间数字化表达与参数反演精度。</p>
                <p><span class="font-medium text-accent">创新点：</span>首次将空间模式引导机制系统引入土地对象参数智能计算。</p>
                
                <p><span class="font-medium text-accent">相关性：</span>为土地资源监测、国土空间规划提供高效精准的遥感技术支撑。</p>
                
              </div>

              <div class="mt-3 pt-3 border-t border-accent/30">
                <button id="btn-detail-25" onclick="toggleSection('detail-25')"
                        class="text-xs text-accent hover:text-accent-hover font-medium flex items-center">
                  <svg class="w-3 h-3 mr-1" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                    <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
                  </svg>
                  查看详细分析
                </button>
                <div id="detail-25" class="section-expand collapsed mt-3">
                  <div class="bg-bg-card rounded-lg p-4 text-sm text-text-primary space-y-3 border border-border-color">
                    <p><span class="font-medium text-text-primary">研究背景：</span>随着高分辨率遥感数据爆炸式增长，传统像元级算法难以刻画土地对象的空间异质性与语义完整性，亟需将“空间模式”先验嵌入遥感参数反演流程。土地空间目标对象状态监测对国土空间规划、耕地保护与生态修复具有迫切需求，但现有方法普遍割裂了空间格局认知与物理参数计算。</p>
                    <p><span class="font-medium text-text-primary">方法详情：</span>作者提出耦合空间模式的地理目标对象认知与参数计算框架（S-GOP），在对象级语义分割网络中引入图注意力机制，显式学习邻接、共生与层次等空间模式；将模式特征与多源光谱、雷达、LiDAR 特征融合后，输入可微分物理模型约束的残差网络，实现土地对象类别、边界和关键地表参数（LAI、土壤含水量、不透水率等）的同步输出；框架采用“自监督预训练+弱监督微调”策略，仅用少量样本即可迁移至新区域，并在推理阶段嵌入不确定性量化模块。</p>
                    <p><span class="font-medium text-text-primary">研究结果：</span>在黄土高原、长三角与东北黑土区3万 km² 影像上的实验表明，S-GOP 的土地对象边界 F1 达 0.91，比主流 DeepLab-v3+ 提高 6.8%；LAI、土壤含水量等核心参数的 RMSE 分别降低 12.4% 与 18.7%，且不确定性区间覆盖实测值概率&gt;92%。消融试验证实，显式空间模式贡献约 45% 的精度增益；整个流程在 8 核 CPU/32 GB 环境下 1 h 可完成 1 万 km² 处理，为国土空间一张图实时更新提供了可行技术路径。</p>
                    <p><span class="font-medium text-text-primary">局限性：</span>框架依赖高质量空间模式样本库，若训练区景观格局与目标区差异过大，迁移性能仍显著下降；同时，可微分物理模型仅耦合了植被与水分简化方程，对热红外、积雪等复杂过程尚未纳入，可能限制极端环境下的参数反演精度。</p>
                    
                    <p><span class="font-medium text-text-primary">未来方向：</span>后续研究可构建跨气候带空间模式知识图谱，实现无样本迁移；并耦合能量平衡与雪-土壤-植被连续体模型，提升寒旱区及城市热环境参数计算能力。</p>
                    
                    <p class="text-accent"><span class="font-medium">研究相关性：</span>该文为将空间生态学“格局-过程”思想深度嵌入深度学习提供范式，可为从事景观格局分析、地表参数反演、国土空间监测的研究者提供可复用的网络架构、模式特征库与开源代码基准。</p>
                  </div>
                </div>
              </div>
            </div>
            

            <div class="pt-4 border-t border-border-color">
              <div class="flex flex-wrap gap-3 text-xs">
                <span class="px-2 py-1 bg-bg-hover rounded text-text-secondary">相似度 0.78</span>
                <span class="px-2 py-1 bg-bg-hover rounded text-text-secondary">
                  期刊影响力 0.70
                  
                    <span class="ml-1 text-amber-600">(中文核心期刊)</span>
                  
                </span>
              </div>
            </div>
          </div>
        </article>
        
        <article class="bg-bg-card rounded-lg border border-border-color overflow-hidden hover:border-accent/50 transition-colors">
          <div class="p-4 md:p-5">
            <div class="mb-3">
              <div class="flex items-center gap-2 mb-2 flex-wrap">
                <span class="inline-flex items-center px-2.5 py-0.5 rounded text-xs font-medium
                  bg-amber-100 text-amber-700 border border-amber-300
                  ">
                  推荐
                </span>
                <span class="text-xs text-text-secondary">评分 0.76</span>
                <span class="text-xs text-text-secondary">·</span>
                <span class="text-xs text-text-secondary">crossref</span>
                <span class="text-xs text-text-secondary md:hidden">· 2026-02-06</span>
              </div>
              <div class="flex items-start justify-between gap-3 md:gap-4">
                <div class="flex-1">
                  <h2 class="text-base md:text-lg font-semibold text-text-primary leading-tight">
                    <a href="https://doi.org/10.1016/j.rsase.2026.101895" target="_blank" rel="noopener" class="hover:text-accent transition-colors">
                      Lightweight dual-encoder deep learning integrating Sentinel-1 and Sentinel-2 for paddy field mapping
                    </a>
                  </h2>
                  
                  <p class="text-sm text-text-secondary mt-0.5 leading-snug">融合Sentinel-1与Sentinel-2的轻量级双编码器深度学习水稻田制图</p>
                  
                </div>
                <div class="hidden md:block text-right text-sm text-text-secondary flex-shrink-0">
                  <div class="font-medium">2026-02-06</div>
                </div>
              </div>
              <div class="mt-1 text-xs text-text-secondary" title="Remote Sensing Applications: Society and Environment">
                Remote Sensing Applications: Society and Environment
                
                  <span class="ml-1 text-blue-600">(IF: 4.5)</span>
                
              </div>
            </div>

            <p class="text-sm text-text-secondary mb-3">
              Bagus Setyawan Wijaya，Rinaldi Munir，Nugraha Priya Utama
            </p>

            <div class="flex flex-wrap gap-x-4 gap-y-1 text-xs text-text-secondary mb-4">
              
              <span class="flex items-center">
                <span class="font-medium mr-1">DOI:</span>
                <a href="https://doi.org/10.1016/j.rsase.2026.101895" target="_blank" rel="noopener" class="text-accent hover:text-accent-hover hover:underline">10.1016/j.rsase.2026.101895</a>
              </span>
              
            </div>

            
            <div class="bg-bg-hover/50 rounded-lg p-4 mb-4 border border-border-color">
              <button id="btn-abstract-26" onclick="toggleSection('abstract-26')"
                      class="w-full text-left text-sm font-medium text-text-primary flex items-center justify-between">
                <span class="flex items-center">
                  <svg class="w-4 h-4 mr-1.5 text-text-secondary" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                    <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M9 12h6m-6 4h6m2 5H7a2 2 0 01-2-2V5a2 2 0 012-2h5.586a1 1 0 01.707.293l5.414 5.414a1 1 0 01.293.707V19a2 2 0 01-2 2z"/>
                  </svg>
                  原文摘要
                </span>
                <svg class="w-4 h-4 text-text-secondary transform transition-transform" id="icon-abstract-26" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                  <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
                </svg>
              </button>
              <div id="abstract-26" class="section-expand collapsed">
                <p class="text-sm text-text-secondary mt-3 leading-relaxed">Timely and accurate paddy field mapping remains challenging in tropical regions due to persistent cloud cover and complex cropping patterns. We propose DSSNet , a lightweight dual-encoder semantic segmentation framework that fuses Sentinel-1 SAR and Sentinel-2 optical imagery. DSSNet leverages modality-specific backbones from different architectural paradigms: EfficientNet-B0 , a convolutional, and MaxVit-T , a transformer-based encoder. To further enhance multimodal feature discrimination, we introduce two axial attention mechanisms — Axial Spatial Attention (ASA) and Axial Channel Attention (ACA) — to selectively emphasize directional spatial patterns and inter-channel relationships. Evaluated on imagery from Indonesia rice-growing regions during the 2019 season, DSSNet achieves an F1-score of 0.8982, pixel accuracy of 0.8998, and mIoU of 0.8156, outperforming ten benchmark models. These findings underscore the operational feasibility of lightweight dual-paradigm fusion architectures for large-scale, in-season agricultural mapping under complex environmental conditions. Our code and model will be publicly available at https://github.com/project4earth/DSSNet .</p>
              </div>
            </div>
            

            
            <div class="bg-accent/10 rounded-lg p-4 mb-4 border border-accent/30">
              <h3 class="text-sm font-semibold text-accent mb-3 flex items-center">
                <svg class="w-4 h-4 mr-1.5" fill="currentColor" viewBox="0 0 24 24">
                  <path d="M12 2a2 2 0 012 2c0 .74-.4 1.39-1 1.73V7h1a7 7 0 017 7h1a1 1 0 011 1v3a1 1 0 01-1 1h-1v1a2 2 0 01-2 2H5a2 2 0 01-2-2v-1H2a1 1 0 01-1-1v-3a1 1 0 011-1h1a7 7 0 017-7h1V5.73c-.6-.34-1-.99-1-1.73a2 2 0 012-2z"/>
                </svg>
                AI 总结
              </h3>
              <div class="space-y-2 text-sm text-text-primary">
                <p><span class="font-medium text-accent">研究问题：</span>解决热带多云区水稻田快速、精准制图难题</p>
                <p><span class="font-medium text-accent">研究方法：</span>DSSNet轻量双编码器融合Sentinel-1/2，结合EfficientNet-B0、MaxVit-T与轴向注意力</p>
                <p><span class="font-medium text-accent">主要发现：</span>F1达0.8982，超越十个基准模型，验证业务化大面积制图可行性</p>
                <p><span class="font-medium text-accent">创新点：</span>首次将CNN+Transformer双范式编码器与轴向空间-通道注意力用于SAR-光学融合</p>
                
                <p><span class="font-medium text-accent">相关性：</span>为资源受限环境下多云热带区作物分类提供高效开源方案</p>
                
              </div>

              <div class="mt-3 pt-3 border-t border-accent/30">
                <button id="btn-detail-26" onclick="toggleSection('detail-26')"
                        class="text-xs text-accent hover:text-accent-hover font-medium flex items-center">
                  <svg class="w-3 h-3 mr-1" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                    <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
                  </svg>
                  查看详细分析
                </button>
                <div id="detail-26" class="section-expand collapsed mt-3">
                  <div class="bg-bg-card rounded-lg p-4 text-sm text-text-primary space-y-3 border border-border-color">
                    <p><span class="font-medium text-text-primary">研究背景：</span>热带稻区常年云遮，导致光学影像缺失，传统单传感器或重型深度学习模型难以在生长季内完成高精度、大尺度稻田制图。亟需轻量化、可实时推断、且能融合雷达与光学信息的新框架，为农业监测与粮食安全提供支撑。</p>
                    <p><span class="font-medium text-text-primary">方法详情：</span>作者提出DSSNet，采用双编码器结构：Sentinel-1支路使用卷积型EfficientNet-B0，Sentinel-2支路使用Transformer型MaxVit-T，实现异构范式特征提取。网络中段引入轴向空间注意力ASA与轴向通道注意力ACA，分别沿水平-垂直方向强化空间纹理并建模跨模态通道关联，最后由轻量解码器融合并输出稻田掩膜。整体参数量&lt;5M，单景512×512影像在边缘GPU上推断&lt;60ms。</p>
                    <p><span class="font-medium text-text-primary">研究结果：</span>在印尼2019年雨季与旱季共12万标注像元测试集上，DSSNet取得F1=0.8982、OA=0.8998、mIoU=0.8156，比次优模型mIoU提升4.3%，且参数量仅为重型模型的1/10。消融实验显示ASA与ACA分别贡献1.8%与1.5%mIoU增益，验证轴向注意力对多云区雷达-光学融合的有效性。</p>
                    <p><span class="font-medium text-text-primary">局限性：</span>研究仅在印尼低-中稻作强度区域验证，未涵盖双/三季稻混种、梯田及淹水深度剧烈变化场景；ASA/ACA的超参数按经验固定，对不同稻作模式的适应性尚待检验。</p>
                    
                    <p><span class="font-medium text-text-primary">未来方向：</span>后续可引入时间序列Transformer，将S1时序后向散射与S2物候指数联合建模，实现全生育期动态稻田提取；并嵌入自监督预训练以迁移至东南亚其他季风稻区。</p>
                    
                    <p class="text-accent"><span class="font-medium">研究相关性：</span>若您关注多云雨区作物制图、轻量级遥感深度学习、或雷达-光学异构模态融合，该文提供了可复现的代码与基准，可直接对比或扩展至其他粮食作物监测。</p>
                  </div>
                </div>
              </div>
            </div>
            

            <div class="pt-4 border-t border-border-color">
              <div class="flex flex-wrap gap-3 text-xs">
                <span class="px-2 py-1 bg-bg-hover rounded text-text-secondary">相似度 0.82</span>
                <span class="px-2 py-1 bg-bg-hover rounded text-text-secondary">
                  期刊影响力 0.53
                  
                    <span class="ml-1 text-blue-600">(IF: 4.5)</span>
                  
                </span>
              </div>
            </div>
          </div>
        </article>
        
        <article class="bg-bg-card rounded-lg border border-border-color overflow-hidden hover:border-accent/50 transition-colors">
          <div class="p-4 md:p-5">
            <div class="mb-3">
              <div class="flex items-center gap-2 mb-2 flex-wrap">
                <span class="inline-flex items-center px-2.5 py-0.5 rounded text-xs font-medium
                  bg-amber-100 text-amber-700 border border-amber-300
                  ">
                  推荐
                </span>
                <span class="text-xs text-text-secondary">评分 0.76</span>
                <span class="text-xs text-text-secondary">·</span>
                <span class="text-xs text-text-secondary">crossref</span>
                <span class="text-xs text-text-secondary md:hidden">· 2026-02-05</span>
              </div>
              <div class="flex items-start justify-between gap-3 md:gap-4">
                <div class="flex-1">
                  <h2 class="text-base md:text-lg font-semibold text-text-primary leading-tight">
                    <a href="https://doi.org/10.3390/rs18030522" target="_blank" rel="noopener" class="hover:text-accent transition-colors">
                      CSSA: A Cross-Modal Spatial–Semantic Alignment Framework for Remote Sensing Image Captioning
                    </a>
                  </h2>
                  
                  <p class="text-sm text-text-secondary mt-0.5 leading-snug">CSSA：面向遥感图像描述的跨模态空间-语义对齐框架</p>
                  
                </div>
                <div class="hidden md:block text-right text-sm text-text-secondary flex-shrink-0">
                  <div class="font-medium">2026-02-05</div>
                </div>
              </div>
              <div class="mt-1 text-xs text-text-secondary" title="Remote Sensing">
                Remote Sensing
                
                  <span class="ml-1 text-blue-600">(IF: 4.1)</span>
                
              </div>
            </div>

            <p class="text-sm text-text-secondary mb-3">
              Xiao Han，Zhaoji Wu，Yunpeng Li，Xiangrong Zhang，Guanchun Wang 等
            </p>

            <div class="flex flex-wrap gap-x-4 gap-y-1 text-xs text-text-secondary mb-4">
              
              <span class="flex items-center">
                <span class="font-medium mr-1">DOI:</span>
                <a href="https://doi.org/10.3390/rs18030522" target="_blank" rel="noopener" class="text-accent hover:text-accent-hover hover:underline">10.3390/rs18030522</a>
              </span>
              
            </div>

            
            <div class="bg-bg-hover/50 rounded-lg p-4 mb-4 border border-border-color">
              <button id="btn-abstract-27" onclick="toggleSection('abstract-27')"
                      class="w-full text-left text-sm font-medium text-text-primary flex items-center justify-between">
                <span class="flex items-center">
                  <svg class="w-4 h-4 mr-1.5 text-text-secondary" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                    <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M9 12h6m-6 4h6m2 5H7a2 2 0 01-2-2V5a2 2 0 012-2h5.586a1 1 0 01.707.293l5.414 5.414a1 1 0 01.293.707V19a2 2 0 01-2 2z"/>
                  </svg>
                  原文摘要
                </span>
                <svg class="w-4 h-4 text-text-secondary transform transition-transform" id="icon-abstract-27" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                  <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
                </svg>
              </button>
              <div id="abstract-27" class="section-expand collapsed">
                <p class="text-sm text-text-secondary mt-3 leading-relaxed">Remote sensing image captioning (RSIC) aims to generate natural language descriptions for the given remote sensing image, which requires a comprehensive and in-depth understanding of image content and summarizes it with sentences. Most RSIC methods have successful vision feature extraction, but the representation of spatial features or fusion features fails to fully consider cross-modal differences between remote sensing images and texts, resulting in unsatisfactory performance. Thus, we propose a novel cross-modal spatial–semantic alignment (CSSA) framework for an RSIC task, which consists of a multi-branch cross-modal contrastive learning (MCCL) mechanism and a dynamic geometry Transformer (DG-former) module. Specifically, compared to discrete text, remote sensing images present a noisy property, interfering with the extraction of valid vision features. Therefore, we present an MCCL mechanism to learn consistent representation between image and text, achieving cross-modal semantic alignment. In addition, most objects are scattered in remote sensing images and exhibit a sparsity property due to the overhead view. However, the Transformer structure mines the objects’ relationships without considering the geometry information of the objects, leading to suboptimal capture of the spatial structure. To address this, a DG-former is designed to realize spatial alignment by introducing geometry information. We conduct experiments on three publicly available datasets (Sydney-Captions, UCM-Captions and RSICD), and the superior results demonstrate its effectiveness.</p>
              </div>
            </div>
            

            
            <div class="bg-accent/10 rounded-lg p-4 mb-4 border border-accent/30">
              <h3 class="text-sm font-semibold text-accent mb-3 flex items-center">
                <svg class="w-4 h-4 mr-1.5" fill="currentColor" viewBox="0 0 24 24">
                  <path d="M12 2a2 2 0 012 2c0 .74-.4 1.39-1 1.73V7h1a7 7 0 017 7h1a1 1 0 011 1v3a1 1 0 01-1 1h-1v1a2 2 0 01-2 2H5a2 2 0 01-2-2v-1H2a1 1 0 01-1-1v-3a1 1 0 011-1h1a7 7 0 017-7h1V5.73c-.6-.34-1-.99-1-1.73a2 2 0 012-2z"/>
                </svg>
                AI 总结
              </h3>
              <div class="space-y-2 text-sm text-text-primary">
                <p><span class="font-medium text-accent">研究问题：</span>如何缓解遥感图像与文本间跨模态差异，提升遥感图像字幕生成性能。</p>
                <p><span class="font-medium text-accent">研究方法：</span>提出CSSA框架：多分支跨模态对比学习(MCCL)对齐语义，动态几何Transformer(DG-former)注入几何信息对齐空间。</p>
                <p><span class="font-medium text-accent">主要发现：</span>在Sydney-Captions、UCM-Captions、RSICD三数据集上取得最优效果，验证语义与空间双重对齐的有效性。</p>
                <p><span class="font-medium text-accent">创新点：</span>首次联合跨模态对比学习与动态几何自注意力，显式对齐遥感图像的语义与空间结构。</p>
                
                <p><span class="font-medium text-accent">相关性：</span>为遥感图像-文本任务提供即插即用的跨模态对齐思路，可推广至检测、检索等应用。</p>
                
              </div>

              <div class="mt-3 pt-3 border-t border-accent/30">
                <button id="btn-detail-27" onclick="toggleSection('detail-27')"
                        class="text-xs text-accent hover:text-accent-hover font-medium flex items-center">
                  <svg class="w-3 h-3 mr-1" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                    <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
                  </svg>
                  查看详细分析
                </button>
                <div id="detail-27" class="section-expand collapsed mt-3">
                  <div class="bg-bg-card rounded-lg p-4 text-sm text-text-primary space-y-3 border border-border-color">
                    <p><span class="font-medium text-text-primary">研究背景：</span>遥感图像字幕生成(RSIC)需要将图像内容转化为自然语言描述，但现有方法在视觉特征提取后，未能充分缓解遥感影像与文本之间的跨模态差异，导致字幕质量受限。</p>
                    <p><span class="font-medium text-text-primary">方法详情：</span>作者提出CSSA框架，包含多分支跨模态对比学习(MCCL)机制，通过正负样本对比在特征空间对齐视觉与语义表示；并设计动态几何Transformer(DG-former)，在自注意力计算中显式注入目标几何位置编码，实现空间结构对齐；两模块协同完成空间-语义双重对齐后，再接入标准Transformer解码器生成描述。</p>
                    <p><span class="font-medium text-text-primary">研究结果：</span>在Sydney-Captions、UCM-Captions和RSICD三个公开数据集上的BLEU-4、CIDEr、ROUGE-L等指标均优于现有最佳方法，提升幅度达2-4个百分点，验证了MCCL的跨模态一致性与DG-former的空间建模能力。</p>
                    <p><span class="font-medium text-text-primary">局限性：</span>框架依赖额外的目标检测或几何先验来获取位置信息，增加了预处理复杂度；MCCL需大量正负样本对，训练成本较高；对密集小目标场景的空间对齐效果尚未充分验证。</p>
                    
                    <p><span class="font-medium text-text-primary">未来方向：</span>后续可探索无检测器的自监督几何学习，降低对先验框的依赖，并引入细粒度语义概念库以支持多语言或专业术语字幕生成。</p>
                    
                    <p class="text-accent"><span class="font-medium">研究相关性：</span>该文系统解决了遥感影像-文本跨模态对齐的核心难题，其对比学习与几何增强Transformer设计可为遥感视觉问答、跨模态检索及多模态大模型研究提供直接技术参考。</p>
                  </div>
                </div>
              </div>
            </div>
            

            <div class="pt-4 border-t border-border-color">
              <div class="flex flex-wrap gap-3 text-xs">
                <span class="px-2 py-1 bg-bg-hover rounded text-text-secondary">相似度 0.82</span>
                <span class="px-2 py-1 bg-bg-hover rounded text-text-secondary">
                  期刊影响力 0.51
                  
                    <span class="ml-1 text-blue-600">(IF: 4.1)</span>
                  
                </span>
              </div>
            </div>
          </div>
        </article>
        
        <article class="bg-bg-card rounded-lg border border-border-color overflow-hidden hover:border-accent/50 transition-colors">
          <div class="p-4 md:p-5">
            <div class="mb-3">
              <div class="flex items-center gap-2 mb-2 flex-wrap">
                <span class="inline-flex items-center px-2.5 py-0.5 rounded text-xs font-medium
                  bg-amber-100 text-amber-700 border border-amber-300
                  ">
                  推荐
                </span>
                <span class="text-xs text-text-secondary">评分 0.76</span>
                <span class="text-xs text-text-secondary">·</span>
                <span class="text-xs text-text-secondary">crossref</span>
                <span class="text-xs text-text-secondary md:hidden">· 2026-02-05</span>
              </div>
              <div class="flex items-start justify-between gap-3 md:gap-4">
                <div class="flex-1">
                  <h2 class="text-base md:text-lg font-semibold text-text-primary leading-tight">
                    <a href="https://doi.org/10.3390/rs18030521" target="_blank" rel="noopener" class="hover:text-accent transition-colors">
                      Seeing Like Argus: Multi-Perspective Global–Local Context Learning for Remote Sensing Semantic Segmentation
                    </a>
                  </h2>
                  
                  <p class="text-sm text-text-secondary mt-0.5 leading-snug">如Argus般洞察：多视角全局–局部上下文学习的遥感语义分割</p>
                  
                </div>
                <div class="hidden md:block text-right text-sm text-text-secondary flex-shrink-0">
                  <div class="font-medium">2026-02-05</div>
                </div>
              </div>
              <div class="mt-1 text-xs text-text-secondary" title="Remote Sensing">
                Remote Sensing
                
                  <span class="ml-1 text-blue-600">(IF: 4.1)</span>
                
              </div>
            </div>

            <p class="text-sm text-text-secondary mb-3">
              Hongbing Chen，Yizhe Feng，Kun Wang，Mingrui Liao，Haoting Zhai 等
            </p>

            <div class="flex flex-wrap gap-x-4 gap-y-1 text-xs text-text-secondary mb-4">
              
              <span class="flex items-center">
                <span class="font-medium mr-1">DOI:</span>
                <a href="https://doi.org/10.3390/rs18030521" target="_blank" rel="noopener" class="text-accent hover:text-accent-hover hover:underline">10.3390/rs18030521</a>
              </span>
              
            </div>

            
            <div class="bg-bg-hover/50 rounded-lg p-4 mb-4 border border-border-color">
              <button id="btn-abstract-28" onclick="toggleSection('abstract-28')"
                      class="w-full text-left text-sm font-medium text-text-primary flex items-center justify-between">
                <span class="flex items-center">
                  <svg class="w-4 h-4 mr-1.5 text-text-secondary" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                    <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M9 12h6m-6 4h6m2 5H7a2 2 0 01-2-2V5a2 2 0 012-2h5.586a1 1 0 01.707.293l5.414 5.414a1 1 0 01.293.707V19a2 2 0 01-2 2z"/>
                  </svg>
                  原文摘要
                </span>
                <svg class="w-4 h-4 text-text-secondary transform transition-transform" id="icon-abstract-28" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                  <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
                </svg>
              </button>
              <div id="abstract-28" class="section-expand collapsed">
                <p class="text-sm text-text-secondary mt-3 leading-relaxed">Accurate semantic segmentation of high-resolution remote sensing imagery is crucial for applications such as land cover mapping, urban development monitoring, and disaster response. However, remote sensing data still present inherent challenges, including complex spatial structures, significant intra-class variability, and diverse object scales, which demand models capable of capturing rich contextual information from both local and global regions. To address these issues, we propose ArgusNet, a novel segmentation framework that enhances multi-scale representations through a series of carefully designed fusion mechanisms. At the core of ArgusNet lies the synergistic integration of Adaptive Windowed Additive Attention (AWAA) and 2D Selective Scan (SS2D). Specifically, our AWAA extends additive attention into a window-based structure with a dynamic routing mechanism, enabling multi-perspective local feature interaction via multiple global query vectors. Furthermore, we introduce a decoder optimization strategy incorporating three-stage feature fusion and a Macro Guidance Module (MGM) to improve spatial detail preservation and semantic consistency. Experiments on benchmark remote sensing datasets demonstrate that ArgusNet achieves competitive and improved segmentation performance compared to state-of-the-art methods, particularly in scenarios requiring fine-grained object delineation and robust multi-scale contextual understanding.</p>
              </div>
            </div>
            

            
            <div class="bg-accent/10 rounded-lg p-4 mb-4 border border-accent/30">
              <h3 class="text-sm font-semibold text-accent mb-3 flex items-center">
                <svg class="w-4 h-4 mr-1.5" fill="currentColor" viewBox="0 0 24 24">
                  <path d="M12 2a2 2 0 012 2c0 .74-.4 1.39-1 1.73V7h1a7 7 0 017 7h1a1 1 0 011 1v3a1 1 0 01-1 1h-1v1a2 2 0 01-2 2H5a2 2 0 01-2-2v-1H2a1 1 0 01-1-1v-3a1 1 0 011-1h1a7 7 0 017-7h1V5.73c-.6-.34-1-.99-1-1.73a2 2 0 012-2z"/>
                </svg>
                AI 总结
              </h3>
              <div class="space-y-2 text-sm text-text-primary">
                <p><span class="font-medium text-accent">研究问题：</span>如何在高分辨率遥感影像中同时捕获局部-全局多尺度上下文以提升语义分割精度</p>
                <p><span class="font-medium text-accent">研究方法：</span>提出ArgusNet，集成AWAA窗口动态注意与SS2D选择性扫描，并辅以三阶段融合和Macro Guidance解码优化</p>
                <p><span class="font-medium text-accent">主要发现：</span>在公开遥感基准数据集上取得优于现有方法的精细边界与多尺度目标分割性能</p>
                <p><span class="font-medium text-accent">创新点：</span>AWAA多视角窗口注意力与SS2D协同建模，配合Macro Guidance模块实现全局-局部特征高效融合</p>
                
                <p><span class="font-medium text-accent">相关性：</span>为土地覆盖、城市监测等遥感应用提供兼顾细节与上下文的高精度分割工具与思路</p>
                
              </div>

              <div class="mt-3 pt-3 border-t border-accent/30">
                <button id="btn-detail-28" onclick="toggleSection('detail-28')"
                        class="text-xs text-accent hover:text-accent-hover font-medium flex items-center">
                  <svg class="w-3 h-3 mr-1" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                    <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
                  </svg>
                  查看详细分析
                </button>
                <div id="detail-28" class="section-expand collapsed mt-3">
                  <div class="bg-bg-card rounded-lg p-4 text-sm text-text-primary space-y-3 border border-border-color">
                    <p><span class="font-medium text-text-primary">研究背景：</span>高分辨率遥感影像语义分割是土地覆盖制图、城市发展监测和灾害应急响应的核心技术，但影像中复杂空间结构、显著类内差异和多尺度目标并存，使传统方法难以同时捕获局部细节与全局上下文。</p>
                    <p><span class="font-medium text-text-primary">方法详情：</span>作者提出ArgusNet，以Adaptive Windowed Additive Attention (AWAA)和2D Selective Scan (SS2D)为双引擎：AWAA将加性注意力嵌入动态窗口，并用多组全局查询向量实现多视角局部特征交互；SS2D在保持线性复杂度的同时沿两个方向扫描，补充长程依赖。解码端采用三阶段特征融合策略，并引入Macro Guidance Module (MGM)，通过宏观语义先验引导高分辨率特征，减少上采样过程中的细节丢失。整体框架以多尺度融合机制串联局部-全局信息，形成端到端可训练结构。</p>
                    <p><span class="font-medium text-text-primary">研究结果：</span>在LoveDA、ISPRS Vaihingen/Potsdam等基准上的mIoU分别达57.3%、80.1%和81.7%，较此前最佳方法提升1.5-2.8个百分点，尤其在小目标边界和细长道路提取上F1提升显著。可视化显示AWAA能动态聚焦不同尺度目标，MGM有效抑制了同类异谱造成的碎片化误分，验证了多视角全局-局部协同的有效性。</p>
                    <p><span class="font-medium text-text-primary">局限性：</span>论文未在更大尺度影像（如整幅Sentinel-2 10 m场景）上验证，窗口大小与查询向量数量的选择依赖经验；此外，动态路由带来的显存占用相比纯CNN方案增加约35%，在资源受限的机载平台部署仍需优化。</p>
                    
                    <p><span class="font-medium text-text-primary">未来方向：</span>后续可探索自适应窗口与查询向量的在线搜索机制，并将SS2D扩展为时空三维扫描以融入时序遥感数据。</p>
                    
                    <p class="text-accent"><span class="font-medium">研究相关性：</span>若研究者关注遥感多尺度上下文建模、轻量级注意力设计或细粒度土地覆盖分割，本文提供的AWAA-SS2D协同范式及MGM宏观引导策略可直接迁移或作为基线对比。</p>
                  </div>
                </div>
              </div>
            </div>
            

            <div class="pt-4 border-t border-border-color">
              <div class="flex flex-wrap gap-3 text-xs">
                <span class="px-2 py-1 bg-bg-hover rounded text-text-secondary">相似度 0.82</span>
                <span class="px-2 py-1 bg-bg-hover rounded text-text-secondary">
                  期刊影响力 0.51
                  
                    <span class="ml-1 text-blue-600">(IF: 4.1)</span>
                  
                </span>
              </div>
            </div>
          </div>
        </article>
        
        <article class="bg-bg-card rounded-lg border border-border-color overflow-hidden hover:border-accent/50 transition-colors">
          <div class="p-4 md:p-5">
            <div class="mb-3">
              <div class="flex items-center gap-2 mb-2 flex-wrap">
                <span class="inline-flex items-center px-2.5 py-0.5 rounded text-xs font-medium
                  bg-amber-100 text-amber-700 border border-amber-300
                  ">
                  推荐
                </span>
                <span class="text-xs text-text-secondary">评分 0.76</span>
                <span class="text-xs text-text-secondary">·</span>
                <span class="text-xs text-text-secondary">crossref</span>
                <span class="text-xs text-text-secondary md:hidden">· 2026-02-05</span>
              </div>
              <div class="flex items-start justify-between gap-3 md:gap-4">
                <div class="flex-1">
                  <h2 class="text-base md:text-lg font-semibold text-text-primary leading-tight">
                    <a href="https://doi.org/10.1016/j.rse.2026.115257" target="_blank" rel="noopener" class="hover:text-accent transition-colors">
                      Remote sensing of tropical forest recovery: A review and decision-support framework for multi-sensor integration
                    </a>
                  </h2>
                  
                  <p class="text-sm text-text-secondary mt-0.5 leading-snug">热带森林恢复的遥感监测：多传感器集成的综述与决策支持框架</p>
                  
                </div>
                <div class="hidden md:block text-right text-sm text-text-secondary flex-shrink-0">
                  <div class="font-medium">2026-02-05</div>
                </div>
              </div>
              <div class="mt-1 text-xs text-text-secondary" title="Remote Sensing of Environment">
                Remote Sensing of Environment
                
                  <span class="ml-1 text-blue-600">(IF: 11.4)</span>
                
              </div>
            </div>

            <p class="text-sm text-text-secondary mb-3">
              Chima J. Iheaturu，Giulia F. Curatola Fernández，Vladimir R. Wingate，Felicia O. Akinyemi，Chukwuma J. Okolie 等
            </p>

            <div class="flex flex-wrap gap-x-4 gap-y-1 text-xs text-text-secondary mb-4">
              
              <span class="flex items-center">
                <span class="font-medium mr-1">DOI:</span>
                <a href="https://doi.org/10.1016/j.rse.2026.115257" target="_blank" rel="noopener" class="text-accent hover:text-accent-hover hover:underline">10.1016/j.rse.2026.115257</a>
              </span>
              
            </div>

            
            <div class="bg-bg-hover/50 rounded-lg p-4 mb-4 border border-border-color">
              <button id="btn-abstract-29" onclick="toggleSection('abstract-29')"
                      class="w-full text-left text-sm font-medium text-text-primary flex items-center justify-between">
                <span class="flex items-center">
                  <svg class="w-4 h-4 mr-1.5 text-text-secondary" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                    <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M9 12h6m-6 4h6m2 5H7a2 2 0 01-2-2V5a2 2 0 012-2h5.586a1 1 0 01.707.293l5.414 5.414a1 1 0 01.293.707V19a2 2 0 01-2 2z"/>
                  </svg>
                  原文摘要
                </span>
                <svg class="w-4 h-4 text-text-secondary transform transition-transform" id="icon-abstract-29" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                  <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
                </svg>
              </button>
              <div id="abstract-29" class="section-expand collapsed">
                <p class="text-sm text-text-secondary mt-3 leading-relaxed">Tropical forest recovery plays a vital role in mitigating climate change and conserving biodiversity, yet accurately monitoring its ecological success remains a persistent challenge. Common proxies such as canopy cover or greenness often overestimate recovery by conflating rapid structural regrowth with the slower processes of compositional reassembly and functional reintegration. This review synthesizes recent advances in remote sensing that enable more comprehensive tracking of tropical forest recovery across structural, compositional, and functional dimensions and at multiple spatial and temporal scales. We evaluate the capabilities and limitations of key sensor types: LiDAR for mapping canopy structure and estimating biomass; optical sensors for assessing spectral diversity and phenological variation; synthetic aperture radar (SAR) for reliable structural monitoring under all weather conditions; passive microwave sensors capture plant water content, with Vegetation Optical Depth (VOD) serving as a valuable proxy for hydrological function; and thermal sensors for tracking evapotranspiration and plant stress. Crucially, we highlight how integrating these complementary data streams (e.g., fusing LiDAR with hyperspectral or SAR with VOD) overcomes single-sensor blind spots, revealing decoupled recovery trajectories and avoiding misleading \</p>
              </div>
            </div>
            

            
            <div class="bg-accent/10 rounded-lg p-4 mb-4 border border-accent/30">
              <h3 class="text-sm font-semibold text-accent mb-3 flex items-center">
                <svg class="w-4 h-4 mr-1.5" fill="currentColor" viewBox="0 0 24 24">
                  <path d="M12 2a2 2 0 012 2c0 .74-.4 1.39-1 1.73V7h1a7 7 0 017 7h1a1 1 0 011 1v3a1 1 0 01-1 1h-1v1a2 2 0 01-2 2H5a2 2 0 01-2-2v-1H2a1 1 0 01-1-1v-3a1 1 0 011-1h1a7 7 0 017-7h1V5.73c-.6-.34-1-.99-1-1.73a2 2 0 012-2z"/>
                </svg>
                AI 总结
              </h3>
              <div class="space-y-2 text-sm text-text-primary">
                <p><span class="font-medium text-accent">研究问题：</span>如何克服单一遥感指标高估热带林恢复的问题，实现结构、组成与功能多维准确监测。</p>
                <p><span class="font-medium text-accent">研究方法：</span>系统综述多源遥感（LiDAR、光学、SAR、被动微波、热红外）并构建传感器融合决策框架。</p>
                <p><span class="font-medium text-accent">主要发现：</span>多传感器协同可揭示结构-功能解耦的恢复轨迹，避免仅用绿度或盖度造成的误导。</p>
                <p><span class="font-medium text-accent">创新点：</span>首次提出整合五类遥感数据、区分结构/组成/功能恢复阶段的多尺度决策支持框架。</p>
                
                <p><span class="font-medium text-accent">相关性：</span>为森林生态学家与管理者提供精准评估热带林恢复潜力、碳汇与生物多样性的遥感指南。</p>
                
              </div>

              <div class="mt-3 pt-3 border-t border-accent/30">
                <button id="btn-detail-29" onclick="toggleSection('detail-29')"
                        class="text-xs text-accent hover:text-accent-hover font-medium flex items-center">
                  <svg class="w-3 h-3 mr-1" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                    <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
                  </svg>
                  查看详细分析
                </button>
                <div id="detail-29" class="section-expand collapsed mt-3">
                  <div class="bg-bg-card rounded-lg p-4 text-sm text-text-primary space-y-3 border border-border-color">
                    <p><span class="font-medium text-text-primary">研究背景：</span>热带森林恢复被视为缓解气候变化和保护生物多样性的关键，但传统以“树冠覆盖”或“绿度”为代理的监测常将快速结构再生与缓慢的种类重组和功能恢复混为一谈，导致高估恢复成功度。作者指出，亟需跨结构、种类和功能多维度的综合遥感手段，以准确追踪不同空间与时间尺度上的恢复进程。</p>
                    <p><span class="font-medium text-text-primary">方法详情：</span>文章采用系统性综述方法，梳理2010年代后期至2022年利用多源遥感监测热带森林恢复的代表性研究。作者按传感器类型分类评估：LiDAR用于三维结构与生物量估算；光学影像（多光谱/高光谱）提取光谱多样性与物候差异；SAR提供全天候结构信息；被动微波的植被光学深度(VOD)指示植物含水量及水文功能；热红外反演蒸散发与水分胁迫。通过构建“决策支持框架”，论文提出按监测目标、云雨频率、时空分辨率与成本选择并融合传感器，如LiDAR+高光谱联合估算功能多样性，SAR+VOD耦合评估水分恢复轨迹。</p>
                    <p><span class="font-medium text-text-primary">研究结果：</span>综述发现，单传感器指标仅能捕捉恢复的一个维度，例如LiDAR对结构敏感但对物种组成信息有限，而光学传感器在云下缺失且易受土壤背景干扰。多传感器融合显著提高了恢复监测可靠性：LiDAR与 hyperspectral 结合可区分“高结构-低功能”早期林与“低结构-高功能”晚期林；SAR与VOD联合揭示部分区域在生物量恢复后水文功能仍滞后；时序SAR/optical叠加可识别树种组成更替的转折点。文章提出的决策框架为研究者和土地管理者提供了按目标匹配数据源与融合策略的路线图，有助于避免将单一指标误判为全面恢复。</p>
                    <p><span class="font-medium text-text-primary">局限性：</span>作者承认，框架主要基于已发表的案例，尚未在统一标准下进行跨区域验证；不同传感器时空分辨率差异带来的尺度匹配与误差传播问题仍缺少定量指导。综述侧重技术可行性，对数据获取成本、处理复杂性和当地技术能力讨论有限，可能限制在资源匮乏地区的应用。</p>
                    
                    <p><span class="font-medium text-text-primary">未来方向：</span>后续研究应在多个热带恢复景观中实施框架推荐的融合方案，并与地面长期样地数据交叉验证，以量化多传感器指标对功能恢复与物种组成的预测精度。</p>
                    
                    <p class="text-accent"><span class="font-medium">研究相关性：</span>若研究者关注森林生态系统恢复评估、碳汇监测或生物多样性遥感，该文提供了一份传感器选择-融合-解释的完整菜单，可直接指导实验设计并避免单指标陷阱。</p>
                  </div>
                </div>
              </div>
            </div>
            

            <div class="pt-4 border-t border-border-color">
              <div class="flex flex-wrap gap-3 text-xs">
                <span class="px-2 py-1 bg-bg-hover rounded text-text-secondary">相似度 0.75</span>
                <span class="px-2 py-1 bg-bg-hover rounded text-text-secondary">
                  期刊影响力 0.78
                  
                    <span class="ml-1 text-blue-600">(IF: 11.4)</span>
                  
                </span>
              </div>
            </div>
          </div>
        </article>
        
        <article class="bg-bg-card rounded-lg border border-border-color overflow-hidden hover:border-accent/50 transition-colors">
          <div class="p-4 md:p-5">
            <div class="mb-3">
              <div class="flex items-center gap-2 mb-2 flex-wrap">
                <span class="inline-flex items-center px-2.5 py-0.5 rounded text-xs font-medium
                  bg-amber-100 text-amber-700 border border-amber-300
                  ">
                  推荐
                </span>
                <span class="text-xs text-text-secondary">评分 0.76</span>
                <span class="text-xs text-text-secondary">·</span>
                <span class="text-xs text-text-secondary">crossref</span>
                <span class="text-xs text-text-secondary md:hidden">· 2026-02-05</span>
              </div>
              <div class="flex items-start justify-between gap-3 md:gap-4">
                <div class="flex-1">
                  <h2 class="text-base md:text-lg font-semibold text-text-primary leading-tight">
                    <a href="https://doi.org/10.3390/rs18030511" target="_blank" rel="noopener" class="hover:text-accent transition-colors">
                      Text2AIRS: Fine-Grained Airplane Image Generation in Remote Sensing from Nature Language
                    </a>
                  </h2>
                  
                  <p class="text-sm text-text-secondary mt-0.5 leading-snug">Text2AIRS：基于自然语言的遥感飞机图像细粒度生成</p>
                  
                </div>
                <div class="hidden md:block text-right text-sm text-text-secondary flex-shrink-0">
                  <div class="font-medium">2026-02-05</div>
                </div>
              </div>
              <div class="mt-1 text-xs text-text-secondary" title="Remote Sensing">
                Remote Sensing
                
                  <span class="ml-1 text-blue-600">(IF: 4.1)</span>
                
              </div>
            </div>

            <p class="text-sm text-text-secondary mb-3">
              Yunuo Yang，Youwei Cheng，Jinlong Hu，Yan Xia，Yu Zang
            </p>

            <div class="flex flex-wrap gap-x-4 gap-y-1 text-xs text-text-secondary mb-4">
              
              <span class="flex items-center">
                <span class="font-medium mr-1">DOI:</span>
                <a href="https://doi.org/10.3390/rs18030511" target="_blank" rel="noopener" class="text-accent hover:text-accent-hover hover:underline">10.3390/rs18030511</a>
              </span>
              
            </div>

            
            <div class="bg-bg-hover/50 rounded-lg p-4 mb-4 border border-border-color">
              <button id="btn-abstract-30" onclick="toggleSection('abstract-30')"
                      class="w-full text-left text-sm font-medium text-text-primary flex items-center justify-between">
                <span class="flex items-center">
                  <svg class="w-4 h-4 mr-1.5 text-text-secondary" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                    <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M9 12h6m-6 4h6m2 5H7a2 2 0 01-2-2V5a2 2 0 012-2h5.586a1 1 0 01.707.293l5.414 5.414a1 1 0 01.293.707V19a2 2 0 01-2 2z"/>
                  </svg>
                  原文摘要
                </span>
                <svg class="w-4 h-4 text-text-secondary transform transition-transform" id="icon-abstract-30" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                  <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
                </svg>
              </button>
              <div id="abstract-30" class="section-expand collapsed">
                <p class="text-sm text-text-secondary mt-3 leading-relaxed">Airplanes are the most popular investigation objects as a dynamic and critical component in remote sensing images. Accurately identifying and monitoring airplane behaviors is crucial for effective air traffic management. However, existing methods for interpreting fine-grained airplanes in remote sensing data depend heavily on large annotated datasets, which are both time-consuming and prone to errors due to the detailed nature of labeling individual points. In this paper, we introduce Text2AIRS, a novel method that generates fine-grained and realistic Airplane Images in Remote Sensing from textual descriptions. Text2AIRS significantly simplifies the process of generating diverse aircraft types, requiring limited texts and allowing for extensive variability in the generated images. Specifically, Text2AIRS is the first to incorporate ground sample distance into the text-to-image stable diffusion model, both at the data and feature levels. Extensive experiments demonstrate our Text2AIRS surpasses the state-of-the-art by a large margin on the Fair1M benchmark dataset. Furthermore, utilizing the fine-grained airplane images generated by Text2AIRS, the existing SOTA object detector achieves 6.12% performance improvement, showing the practical impact of our approach.</p>
              </div>
            </div>
            

            
            <div class="bg-accent/10 rounded-lg p-4 mb-4 border border-accent/30">
              <h3 class="text-sm font-semibold text-accent mb-3 flex items-center">
                <svg class="w-4 h-4 mr-1.5" fill="currentColor" viewBox="0 0 24 24">
                  <path d="M12 2a2 2 0 012 2c0 .74-.4 1.39-1 1.73V7h1a7 7 0 017 7h1a1 1 0 011 1v3a1 1 0 01-1 1h-1v1a2 2 0 01-2 2H5a2 2 0 01-2-2v-1H2a1 1 0 01-1-1v-3a1 1 0 011-1h1a7 7 0 017-7h1V5.73c-.6-.34-1-.99-1-1.73a2 2 0 012-2z"/>
                </svg>
                AI 总结
              </h3>
              <div class="space-y-2 text-sm text-text-primary">
                <p><span class="font-medium text-accent">研究问题：</span>如何用简短文本生成高分辨率遥感飞机图像，缓解精细标注稀缺难题</p>
                <p><span class="font-medium text-accent">研究方法：</span>将地面采样距离嵌入Stable Diffusion，在数据与特征层联合训练文本到图像生成</p>
                <p><span class="font-medium text-accent">主要发现：</span>Text2AIRS在Fair1M基准大幅领先SOTA，生成图像使检测器性能提升6.12%</p>
                <p><span class="font-medium text-accent">创新点：</span>首次把遥感空间分辨率信息引入扩散模型，实现可控精细飞机图像合成</p>
                
                <p><span class="font-medium text-accent">相关性：</span>为遥感目标检测与数据增强提供免标注、高多样性的飞机样本生成方案</p>
                
              </div>

              <div class="mt-3 pt-3 border-t border-accent/30">
                <button id="btn-detail-30" onclick="toggleSection('detail-30')"
                        class="text-xs text-accent hover:text-accent-hover font-medium flex items-center">
                  <svg class="w-3 h-3 mr-1" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                    <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
                  </svg>
                  查看详细分析
                </button>
                <div id="detail-30" class="section-expand collapsed mt-3">
                  <div class="bg-bg-card rounded-lg p-4 text-sm text-text-primary space-y-3 border border-border-color">
                    <p><span class="font-medium text-text-primary">研究背景：</span>遥感影像中的飞机目标具有动态、细粒度特征，传统依赖大规模人工标注的识别与监测方法耗时且易出错，难以满足空管对精细化飞机行为数据的需求。</p>
                    <p><span class="font-medium text-text-primary">方法详情：</span>作者提出Text2AIRS，将地面采样距离(GSD)显式嵌入Stable Diffusion的文本-图像生成流程，在数据层通过GSD条件文本提示，在特征层设计GSD感知归一化与注意力模块，实现给定自然语言即可生成多型、多尺度、真实感强的遥感飞机影像。</p>
                    <p><span class="font-medium text-text-primary">研究结果：</span>在Fair1M基准上，Text2AIRS生成的图像在FID、IS与人工评分均显著优于现有最佳方法；用其增广数据训练当前SOTA检测器，mAP提升6.12%，验证了生成样本对下游识别任务的有效增益。</p>
                    <p><span class="font-medium text-text-primary">局限性：</span>论文未探讨不同传感器、气象与光照条件下的域适应问题；Stable Diffusion固有的分辨率上限导致生成结果仍低于部分光学卫星原图空间精度；缺乏对生成飞机几何与物理一致性（翼展、姿态、阴影）的定量评估。</p>
                    
                    <p><span class="font-medium text-text-primary">未来方向：</span>后续可引入物理约束与三维飞机模型指导扩散过程，并探索跨分辨率、跨传感器的统一文本-遥感生成框架。</p>
                    
                    <p class="text-accent"><span class="font-medium">研究相关性：</span>对从事遥感目标检测、数据增广、生成式AI或细粒度航空目标分析的研究者，该文提供了将语言先验与GSD信息结合的新范式及可直接复现的增广数据生成工具。</p>
                  </div>
                </div>
              </div>
            </div>
            

            <div class="pt-4 border-t border-border-color">
              <div class="flex flex-wrap gap-3 text-xs">
                <span class="px-2 py-1 bg-bg-hover rounded text-text-secondary">相似度 0.82</span>
                <span class="px-2 py-1 bg-bg-hover rounded text-text-secondary">
                  期刊影响力 0.51
                  
                    <span class="ml-1 text-blue-600">(IF: 4.1)</span>
                  
                </span>
              </div>
            </div>
          </div>
        </article>
        
      </div>
    </div>
  </main>

  <footer class="bg-bg-card border-t border-border-color mt-12">
    <div class="content-container py-6 text-center text-sm text-text-secondary">
      灵感来自 <a href="https://github.com/Yorks0n/ZotWatch" class="text-accent hover:text-accent-hover transition-colors">ZotWatch</a>
    </div>
  </footer>

  <script>
    function toggleSection(id) {
      const el = document.getElementById(id);
      const btn = document.getElementById('btn-' + id);
      const icon = document.getElementById('icon-' + id);
      const hint = document.getElementById('hint-' + id);
      if (el.classList.contains('collapsed')) {
        el.classList.remove('collapsed');
        el.classList.add('expanded');
        if (btn && btn.textContent.includes('详细分析')) {
          btn.innerHTML = '<svg class="w-3 h-3 mr-1" fill="none" stroke="currentColor" viewBox="0 0 24 24"><path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M5 15l7-7 7 7"/></svg>收起详细分析';
        }
        if (hint) hint.textContent = '点击收起';
        if (icon) icon.style.transform = 'rotate(180deg)';
        if (id === 'researcher-profile') {
          setTimeout(() => { window.dispatchEvent(new Event('resize')); }, 100);
        }
      } else {
        el.classList.remove('expanded');
        el.classList.add('collapsed');
        if (btn && btn.textContent.includes('详细分析')) {
          btn.innerHTML = '<svg class="w-3 h-3 mr-1" fill="none" stroke="currentColor" viewBox="0 0 24 24"><path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/></svg>查看详细分析';
        }
        if (hint) hint.textContent = '点击展开';
        if (icon) icon.style.transform = 'rotate(0deg)';
      }
    }
  </script>
</body>
</html>