<!DOCTYPE html>
<html lang="zh-CN">
<head>
  <meta charset="utf-8">
  <meta name="viewport" content="width=device-width, initial-scale=1">
  <title>ZotWatch 文献推荐 - 2026-01-18</title>
  <script src="https://cdn.tailwindcss.com"></script>
  <script src="https://cdn.jsdelivr.net/npm/chart.js@4.4.1/dist/chart.umd.min.js"></script>
  <script src="https://d3js.org/d3.v7.min.js"></script>
  <script>
    tailwind.config = {
      theme: {
        extend: {
          colors: {
            'bg-primary': '#f8fafc',
            'bg-card': '#ffffff',
            'bg-hover': '#f1f5f9',
            'text-primary': '#1e293b',
            'text-secondary': '#64748b',
            'border-color': '#e2e8f0',
            'accent': '#2563eb',
            'accent-hover': '#1d4ed8',
          }
        }
      }
    }
  </script>
  <style>
    .section-expand { transition: max-height 0.5s ease-out; overflow: hidden; }
    .section-expand.collapsed { max-height: 0; }
    .section-expand.expanded { max-height: none; }
    body { font-family: -apple-system, BlinkMacSystemFont, 'Segoe UI', Roboto, 'Helvetica Neue', Arial, sans-serif; }
    .content-container { max-width: 48rem; margin: 0 auto; padding: 0 1rem; }
    @media (min-width: 768px) { .content-container { padding: 0 2rem; } }
    @media (min-width: 1024px) { .content-container { padding: 0 4rem; } }

    .cluster-graph-container { width: 100%; height: 280px; touch-action: pan-y; }
    @media (min-width: 640px) { .cluster-graph-container { height: 320px; } }
    @media (min-width: 768px) { .cluster-graph-container { height: 350px; } }
    #cluster-graph svg { overflow: visible; }
  </style>
</head>
<body class="bg-bg-primary min-h-screen text-text-primary">
  <header class="bg-bg-card border-b border-border-color">
    <div class="content-container py-6 md:py-8">
      <h1 class="text-xl md:text-2xl font-bold text-text-primary">ZotWatch 文献推荐</h1>
      <p class="text-xs md:text-sm text-text-secondary mt-1">
        共 35 篇论文 ·
        生成于 2026-01-18 10:54 Asia/Shanghai
      </p>
    </div>
  </header>

  
  <section class="py-4 md:py-6 border-b border-border-color">
    <div class="content-container">
      <button id="btn-researcher-profile" onclick="toggleSection('researcher-profile')"
              class="w-full text-left text-lg md:text-xl font-bold text-text-primary mb-3 md:mb-4 flex items-center justify-between group hover:text-accent transition-colors">
        <span class="flex items-center gap-2 flex-1 min-w-0">
          <svg class="w-5 h-5 flex-shrink-0 text-accent" fill="currentColor" viewBox="0 0 24 24">
            <path d="M12 2C6.48 2 2 6.48 2 12s4.48 10 10 10 10-4.48 10-10S17.52 2 12 2zm0 3c1.66 0 3 1.34 3 3s-1.34 3-3 3-3-1.34-3-3 1.34-3 3-3zm0 14.2c-2.5 0-4.71-1.28-6-3.22.03-1.99 4-3.08 6-3.08 1.99 0 5.97 1.09 6 3.08-1.29 1.94-3.5 3.22-6 3.22z"/>
          </svg>
          <span class="truncate">研究兴趣画像</span>
        </span>
        <span class="flex items-center gap-1 md:gap-2 flex-shrink-0 ml-2">
          <span class="hidden md:inline text-sm font-normal text-text-secondary group-hover:text-accent" id="hint-researcher-profile">点击展开</span>
          <svg class="w-5 h-5 md:w-4 md:h-4 text-text-secondary group-hover:text-accent transform transition-transform" id="icon-researcher-profile" fill="none" stroke="currentColor" viewBox="0 0 24 24">
            <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
          </svg>
        </span>
      </button>

      <div id="researcher-profile" class="section-expand collapsed">
      <div class="bg-bg-card rounded-lg border border-border-color p-4 mb-4">
        <div class="grid grid-cols-4 gap-3 text-center">
          <div>
            <div class="text-xl font-bold text-accent">963</div>
            <div class="text-xs text-text-secondary">收藏论文</div>
          </div>
          <div>
            <div class="text-xl font-bold text-accent">5年8月</div>
            <div class="text-xs text-text-secondary">收藏时长</div>
          </div>
          <div>
            <div class="text-xl font-bold text-accent">&gt;10</div>
            <div class="text-xs text-text-secondary">兴趣领域</div>
          </div>
          <div>
            <div class="text-xl font-bold text-accent">9</div>
            <div class="text-xs text-text-secondary">高频作者</div>
          </div>
        </div>
      </div>

      
      <div class="bg-accent/10 rounded-lg border border-accent/30 p-4 mb-4">
        <h3 class="text-sm font-semibold text-accent mb-2 flex items-center gap-2">
          <svg class="w-4 h-4" fill="currentColor" viewBox="0 0 24 24">
            <path d="M12 2a2 2 0 012 2c0 .74-.4 1.39-1 1.73V7h1a7 7 0 017 7h1a1 1 0 011 1v3a1 1 0 01-1 1h-1v1a2 2 0 01-2 2H5a2 2 0 01-2-2v-1H2a1 1 0 01-1-1v-3a1 1 0 011-1h1a7 7 0 017-7h1V5.73c-.6-.34-1-.99-1-1.73a2 2 0 012-2z"/>
          </svg>
          AI 研究兴趣分析
        </h3>
        <div class="space-y-2 text-xs text-text-primary leading-relaxed">
          <p><span class="font-medium text-accent">研究兴趣：</span>用户长期关注计算机视觉中的目标检测与定位技术，同时密切追踪模型压缩与高效推理方法，体现出对“看得见”且“跑得快”的算法体系的系统兴趣。</p>
          <p><span class="font-medium text-accent">深度关注：</span>在目标检测、视觉定位及模型压缩三个方向持续收藏高影响力论文（CVPR/TPAMI/ICCV 占比高），并反复研读 Kaiming He、Ross Girshick、Song Han 等团队的经典与最新工作，形成深度技术脉络。</p>
          <p><span class="font-medium text-accent">跨学科倾向：</span>收藏大量 IEEE TGARS、雷达学报与 SAR 目标识别文献，显示其主动将视觉算法迁移至遥感与雷达图像处理，形成 CV ↔ 遥感 的交叉阅读模式。</p>
          <p><span class="font-medium text-accent">兴趣演变：</span>2025 年起单季度收藏量显著回升且新增“合成孔径雷达目标检测”“推理增强”关键词，表明兴趣正从通用视觉模型向雷达影像小样本检测与高效推理方向深化。</p>
          <p><span class="font-medium text-accent">阅读建议：</span>可关注多模态遥感-视觉基础模型、SAR 图像自监督预训练以及针对边缘设备的量化-蒸馏联合优化，以延续检测精度与部署效率并重的研究主线。</p>
        </div>
      </div>
      

      <div class="grid grid-cols-1 md:grid-cols-2 gap-4 mb-4">
        
        <div class="bg-bg-card rounded-lg border border-border-color p-4">
          <h4 class="text-sm font-medium text-text-primary mb-3">研究领域分布</h4>
          <div style="height: 180px;">
            <canvas id="domainsChart"></canvas>
          </div>
        </div>
        

        
        <div class="bg-bg-card rounded-lg border border-border-color p-4">
          <h4 class="text-sm font-medium text-text-primary mb-3">阅读趋势（季度）</h4>
          <div style="height: 180px;">
            <canvas id="trendsChart"></canvas>
          </div>
        </div>
        
      </div>

      
      <div class="bg-bg-card rounded-lg border border-border-color p-4 mb-4">
        <h4 class="text-sm font-medium text-text-primary mb-3">论文发表年份分布</h4>
        <div style="height: 180px;">
          <canvas id="yearDistChart"></canvas>
        </div>
      </div>
      

      
      <div class="bg-bg-card rounded-lg border border-border-color p-4 mb-4">
        <h4 class="text-sm font-medium text-text-primary mb-3">
          研究方向聚类图谱
          <span class="text-xs text-text-secondary font-normal ml-2">(29 个聚类)</span>
        </h4>
        <div id="cluster-graph" class="cluster-graph-container"></div>
        <div class="text-xs text-text-secondary mt-3">
          覆盖 937/937 篇论文（仅含摘要的文献参与聚类）
        </div>
      </div>
      

      <div class="grid grid-cols-1 md:grid-cols-2 gap-4 mb-4">
        
        <div class="bg-bg-card rounded-lg border border-border-color p-4">
          <h4 class="text-sm font-medium text-text-primary mb-2">高频关注作者</h4>
          <div class="grid grid-cols-2 gap-x-4 gap-y-1 text-xs">
            
            <div class="flex justify-between truncate">
              <span class="text-text-primary truncate" title="Kaiming He">Kaiming He</span>
              <span class="text-text-secondary ml-1 flex-shrink-0">24</span>
            </div>
            
            <div class="flex justify-between truncate">
              <span class="text-text-primary truncate" title="Ross Girshick">Ross Girshick</span>
              <span class="text-text-secondary ml-1 flex-shrink-0">18</span>
            </div>
            
            <div class="flex justify-between truncate">
              <span class="text-text-primary truncate" title="Jian Sun">Jian Sun</span>
              <span class="text-text-secondary ml-1 flex-shrink-0">15</span>
            </div>
            
            <div class="flex justify-between truncate">
              <span class="text-text-primary truncate" title="Song Han">Song Han</span>
              <span class="text-text-secondary ml-1 flex-shrink-0">14</span>
            </div>
            
            <div class="flex justify-between truncate">
              <span class="text-text-primary truncate" title="Xiangyu Zhang">Xiangyu Zhang</span>
              <span class="text-text-secondary ml-1 flex-shrink-0">14</span>
            </div>
            
            <div class="flex justify-between truncate">
              <span class="text-text-primary truncate" title="Piotr Dollár">Piotr Dollár</span>
              <span class="text-text-secondary ml-1 flex-shrink-0">12</span>
            </div>
            
            <div class="flex justify-between truncate">
              <span class="text-text-primary truncate" title="Li Liu">Li Liu</span>
              <span class="text-text-secondary ml-1 flex-shrink-0">11</span>
            </div>
            
            <div class="flex justify-between truncate">
              <span class="text-text-primary truncate" title="Xiang Li">Xiang Li</span>
              <span class="text-text-secondary ml-1 flex-shrink-0">11</span>
            </div>
            
            <div class="flex justify-between truncate">
              <span class="text-text-primary truncate" title="Wenfeng Liang">Wenfeng Liang</span>
              <span class="text-text-secondary ml-1 flex-shrink-0">10</span>
            </div>
            
            <div class="flex justify-between truncate">
              <span class="text-text-primary truncate" title="Yann LeCun">Yann LeCun</span>
              <span class="text-text-secondary ml-1 flex-shrink-0">9</span>
            </div>
            
            <div class="flex justify-between truncate">
              <span class="text-text-primary truncate" title="Zhenda Xie">Zhenda Xie</span>
              <span class="text-text-secondary ml-1 flex-shrink-0">9</span>
            </div>
            
            <div class="flex justify-between truncate">
              <span class="text-text-primary truncate" title="Feng Xu">Feng Xu</span>
              <span class="text-text-secondary ml-1 flex-shrink-0">9</span>
            </div>
            
          </div>
        </div>
        

        
        <div class="bg-bg-card rounded-lg border border-border-color p-4">
          <h4 class="text-sm font-medium text-text-primary mb-2">常关注期刊/会议</h4>
          <div class="space-y-1 text-xs">
            
            <div class="flex items-center justify-between">
              <span class="text-text-primary truncate flex-1" title="Computer Vision and Pattern Recognition">Computer Vision and Pattern Recognition</span>
              <span class="text-xs px-1 py-0.5 rounded bg-blue-100 text-blue-700 ml-1 flex-shrink-0">
                期刊
              </span>
              <span class="text-text-secondary ml-1 flex-shrink-0">114</span>
            </div>
            
            <div class="flex items-center justify-between">
              <span class="text-text-primary truncate flex-1" title="IEEE Transactions on Geoscience and Remote Sensing">IEEE Transactions on Geoscience and Remote Sensing</span>
              <span class="text-xs px-1 py-0.5 rounded bg-blue-100 text-blue-700 ml-1 flex-shrink-0">
                期刊
              </span>
              <span class="text-text-secondary ml-1 flex-shrink-0">46</span>
            </div>
            
            <div class="flex items-center justify-between">
              <span class="text-text-primary truncate flex-1" title="Neural Information Processing Systems">Neural Information Processing Systems</span>
              <span class="text-xs px-1 py-0.5 rounded bg-blue-100 text-blue-700 ml-1 flex-shrink-0">
                期刊
              </span>
              <span class="text-text-secondary ml-1 flex-shrink-0">45</span>
            </div>
            
            <div class="flex items-center justify-between">
              <span class="text-text-primary truncate flex-1" title="International Conference on Computer Vision">International Conference on Computer Vision</span>
              <span class="text-xs px-1 py-0.5 rounded bg-amber-100 text-amber-700 ml-1 flex-shrink-0">
                会议
              </span>
              <span class="text-text-secondary ml-1 flex-shrink-0">38</span>
            </div>
            
            <div class="flex items-center justify-between">
              <span class="text-text-primary truncate flex-1" title="IEEE Transactions on Pattern Analysis and Machine Intelligence">IEEE Transactions on Pattern Analysis and Machine Intelligence</span>
              <span class="text-xs px-1 py-0.5 rounded bg-blue-100 text-blue-700 ml-1 flex-shrink-0">
                期刊
              </span>
              <span class="text-text-secondary ml-1 flex-shrink-0">32</span>
            </div>
            
            <div class="flex items-center justify-between">
              <span class="text-text-primary truncate flex-1" title="International Conference on Learning Representations">International Conference on Learning Representations</span>
              <span class="text-xs px-1 py-0.5 rounded bg-amber-100 text-amber-700 ml-1 flex-shrink-0">
                会议
              </span>
              <span class="text-text-secondary ml-1 flex-shrink-0">27</span>
            </div>
            
            <div class="flex items-center justify-between">
              <span class="text-text-primary truncate flex-1" title="European Conference on Computer Vision">European Conference on Computer Vision</span>
              <span class="text-xs px-1 py-0.5 rounded bg-amber-100 text-amber-700 ml-1 flex-shrink-0">
                会议
              </span>
              <span class="text-text-secondary ml-1 flex-shrink-0">23</span>
            </div>
            
            <div class="flex items-center justify-between">
              <span class="text-text-primary truncate flex-1" title="IEEE Geoscience and Remote Sensing Letters">IEEE Geoscience and Remote Sensing Letters</span>
              <span class="text-xs px-1 py-0.5 rounded bg-blue-100 text-blue-700 ml-1 flex-shrink-0">
                期刊
              </span>
              <span class="text-text-secondary ml-1 flex-shrink-0">22</span>
            </div>
            
          </div>
        </div>
        
      </div>

      
      <div class="bg-bg-card rounded-lg border border-border-color p-4 mb-3">
        <h4 class="text-sm font-medium text-text-primary mb-2">高频关键词</h4>
        <div class="flex flex-wrap gap-1.5">
          
          <span class="px-2 py-0.5 bg-bg-hover rounded text-xs text-text-primary">
            大语言模型 <span class="text-text-secondary">(14)</span>
          </span>
          
          <span class="px-2 py-0.5 bg-bg-hover rounded text-xs text-text-primary">
            合成孔径雷达 <span class="text-text-secondary">(12)</span>
          </span>
          
          <span class="px-2 py-0.5 bg-bg-hover rounded text-xs text-text-primary">
            自监督学习 <span class="text-text-secondary">(11)</span>
          </span>
          
          <span class="px-2 py-0.5 bg-bg-hover rounded text-xs text-text-primary">
            综述 <span class="text-text-secondary">(9)</span>
          </span>
          
          <span class="px-2 py-0.5 bg-bg-hover rounded text-xs text-text-primary">
            域自适应 <span class="text-text-secondary">(9)</span>
          </span>
          
          <span class="px-2 py-0.5 bg-bg-hover rounded text-xs text-text-primary">
            目标检测 <span class="text-text-secondary">(8)</span>
          </span>
          
          <span class="px-2 py-0.5 bg-bg-hover rounded text-xs text-text-primary">
            深度学习 <span class="text-text-secondary">(8)</span>
          </span>
          
          <span class="px-2 py-0.5 bg-bg-hover rounded text-xs text-text-primary">
            迁移学习 <span class="text-text-secondary">(7)</span>
          </span>
          
          <span class="px-2 py-0.5 bg-bg-hover rounded text-xs text-text-primary">
            扩散模型 <span class="text-text-secondary">(7)</span>
          </span>
          
          <span class="px-2 py-0.5 bg-bg-hover rounded text-xs text-text-primary">
            强化学习 <span class="text-text-secondary">(7)</span>
          </span>
          
          <span class="px-2 py-0.5 bg-bg-hover rounded text-xs text-text-primary">
            注意力机制 <span class="text-text-secondary">(6)</span>
          </span>
          
          <span class="px-2 py-0.5 bg-bg-hover rounded text-xs text-text-primary">
            重参数化 <span class="text-text-secondary">(6)</span>
          </span>
          
          <span class="px-2 py-0.5 bg-bg-hover rounded text-xs text-text-primary">
            DeepSeek <span class="text-text-secondary">(6)</span>
          </span>
          
          <span class="px-2 py-0.5 bg-bg-hover rounded text-xs text-text-primary">
            生成对抗网络 <span class="text-text-secondary">(6)</span>
          </span>
          
          <span class="px-2 py-0.5 bg-bg-hover rounded text-xs text-text-primary">
            基础模型 <span class="text-text-secondary">(5)</span>
          </span>
          
          <span class="px-2 py-0.5 bg-bg-hover rounded text-xs text-text-primary">
            Transformers <span class="text-text-secondary">(5)</span>
          </span>
          
          <span class="px-2 py-0.5 bg-bg-hover rounded text-xs text-text-primary">
            卷积神经网络 <span class="text-text-secondary">(5)</span>
          </span>
          
          <span class="px-2 py-0.5 bg-bg-hover rounded text-xs text-text-primary">
            旋转目标检测 <span class="text-text-secondary">(5)</span>
          </span>
          
          <span class="px-2 py-0.5 bg-bg-hover rounded text-xs text-text-primary">
            SAR目标识别 <span class="text-text-secondary">(5)</span>
          </span>
          
          <span class="px-2 py-0.5 bg-bg-hover rounded text-xs text-text-primary">
            特征可视化 <span class="text-text-secondary">(5)</span>
          </span>
          
        </div>
      </div>
      

      <div class="text-xs text-text-secondary text-right">
        画像生成于 2026-01-17 10:26 Asia/Shanghai
        · kimi-k2-turbo-preview
      </div>
      </div>
    </div>
  </section>
  

  
  <script>
    document.addEventListener('DOMContentLoaded', function() {
      Chart.defaults.font.family = "-apple-system, BlinkMacSystemFont, 'Segoe UI', Roboto, 'Helvetica Neue', Arial, sans-serif";
      Chart.defaults.font.size = 11;
      Chart.defaults.color = '#64748b';
      Chart.defaults.animation = false;

      
      const domainsCtx = document.getElementById('domainsChart');
      if (domainsCtx) {
        new Chart(domainsCtx, {
          type: 'bar',
          data: {
            labels: ['目标检测', '视觉定位', '模型压缩', '人脸对齐', '姿态估计', '对比学习', 'Transformer', '车牌识别'],
            datasets: [{
              data: [42, 26, 22, 14, 13, 11, 10, 8],
              backgroundColor: 'rgba(37, 99, 235, 0.7)',
              borderColor: 'rgba(37, 99, 235, 1)',
              borderWidth: 1,
              borderRadius: 2,
              barThickness: 16
            }]
          },
          options: {
            indexAxis: 'y',
            responsive: true,
            maintainAspectRatio: false,
            plugins: {
              legend: { display: false },
              tooltip: { enabled: false }
            },
            scales: {
              x: {
                beginAtZero: true,
                grid: { color: 'rgba(226, 232, 240, 0.5)' },
                ticks: { precision: 0 }
              },
              y: {
                grid: { display: false },
                ticks: { font: { size: 10 } }
              }
            }
          }
        });
      }
      

      
      const trendsCtx = document.getElementById('trendsChart');
      if (trendsCtx) {
        const trends = [{ q: '2024-Q1', c: 68 }, { q: '2024-Q2', c: 18 }, { q: '2024-Q3', c: 4 }, { q: '2024-Q4', c: 24 }, { q: '2025-Q1', c: 100 }, { q: '2025-Q2', c: 34 }, { q: '2025-Q3', c: 13 }, { q: '2025-Q4', c: 30 }, { q: '2026-Q1', c: 5 }];
        const monthColors = [
          { bg: 'rgba(59, 130, 246, 0.7)', border: 'rgba(59, 130, 246, 1)' },
          { bg: 'rgba(16, 185, 129, 0.7)', border: 'rgba(16, 185, 129, 1)' },
          { bg: 'rgba(245, 158, 11, 0.7)', border: 'rgba(245, 158, 11, 1)' }
        ];
        const bgColors = trends.map((t, i) => monthColors[i % 3].bg);
        const borderColors = trends.map((t, i) => monthColors[i % 3].border);
        new Chart(trendsCtx, {
          type: 'bar',
          data: {
            labels: trends.map(t => t.q.replace('-', '\n')),
            datasets: [{
              data: trends.map(t => t.c),
              backgroundColor: bgColors,
              borderColor: borderColors,
              borderWidth: 1,
              borderRadius: 2,
              barThickness: 20
            }]
          },
          options: {
            responsive: true,
            maintainAspectRatio: false,
            plugins: {
              legend: { display: false },
              tooltip: { enabled: false }
            },
            scales: {
              x: {
                grid: { display: false },
                ticks: { font: { size: 9 }, maxRotation: 0 }
              },
              y: {
                beginAtZero: true,
                grid: { color: 'rgba(226, 232, 240, 0.5)' },
                ticks: { precision: 0 }
              }
            }
          }
        });
      }
      

      
      const yearDistCtx = document.getElementById('yearDistChart');
      if (yearDistCtx) {
        const yearData = [{ year: 2007, count: 3 }, { year: 2008, count: 2 }, { year: 2009, count: 2 }, { year: 2010, count: 9 }, { year: 2011, count: 10 }, { year: 2012, count: 3 }, { year: 2013, count: 12 }, { year: 2014, count: 14 }, { year: 2015, count: 18 }, { year: 2016, count: 15 }, { year: 2017, count: 41 }, { year: 2018, count: 59 }, { year: 2019, count: 56 }, { year: 2020, count: 66 }, { year: 2021, count: 84 }, { year: 2022, count: 112 }, { year: 2023, count: 111 }, { year: 2024, count: 114 }, { year: 2025, count: 177 }, { year: 2026, count: 5 }];
        new Chart(yearDistCtx, {
          type: 'line',
          data: {
            labels: yearData.map(d => d.year),
            datasets: [{
              data: yearData.map(d => d.count),
              borderColor: 'rgba(37, 99, 235, 1)',
              backgroundColor: 'rgba(37, 99, 235, 0.1)',
              borderWidth: 2,
              fill: true,
              tension: 0.3,
              pointRadius: 2,
              pointHoverRadius: 4
            }]
          },
          options: {
            responsive: true,
            maintainAspectRatio: false,
            plugins: {
              legend: { display: false },
              tooltip: {
                callbacks: {
                  label: function(context) {
                    return context.parsed.y + ' 篇';
                  }
                }
              }
            },
            scales: {
              x: {
                grid: { display: false },
                ticks: {
                  font: { size: 9 },
                  maxRotation: 45,
                  callback: function(value, index) {
                    const year = this.getLabelForValue(value);
                    return year % 5 === 0 ? year : '';
                  }
                }
              },
              y: {
                beginAtZero: true,
                grid: { color: 'rgba(226, 232, 240, 0.5)' },
                ticks: { precision: 0 }
              }
            }
          }
        });
      }
      

      
      const clusterContainer = document.getElementById('cluster-graph');
      if (clusterContainer) {
        const clusters = [
          
          {
            id: 0,
            label: "Transformer\u76ee\u6807\u68c0\u6d4b",
            size: 71,
            keywords: ["\u7efc\u8ff0", "\u65cb\u8f6c\u76ee\u6807\u68c0\u6d4b", "DETR"]
          },
          
          {
            id: 1,
            label: "\u591a\u4f20\u611f\u5668\u81ea\u52a8\u9a7e\u9a76\u611f\u77e5",
            size: 64,
            keywords: ["\u591a\u4efb\u52a1\u5b66\u4e60", "\u81ea\u52a8\u9a7e\u9a76\u611f\u77e5", "SIFT"]
          },
          
          {
            id: 2,
            label: "SAR\u8fc1\u79fb\u57df\u9002\u5e94",
            size: 63,
            keywords: ["SAR\u76ee\u6807\u8bc6\u522b", "\u57df\u81ea\u9002\u5e94", "\u8fc1\u79fb\u5b66\u4e60"]
          },
          
          {
            id: 3,
            label: "\u81ea\u76d1\u7763\u89c6\u89c9\u8868\u5f81",
            size: 56,
            keywords: ["\u81ea\u76d1\u7763\u5b66\u4e60", "\u89c6\u89c9Transformer", "\u5bf9\u6bd4\u5b66\u4e60"]
          },
          
          {
            id: 4,
            label: "\u795e\u7ecf\u7f51\u7edc\u53ef\u89e3\u91ca\u6027",
            size: 53,
            keywords: ["\u7279\u5f81\u53ef\u89c6\u5316", "Grad-CAM", "\u4eba\u5de5\u795e\u7ecf\u7f51\u7edc"]
          },
          
          {
            id: 5,
            label: "\u6269\u6563\u751f\u6210\u6a21\u578b",
            size: 51,
            keywords: ["\u6269\u6563\u6a21\u578b", "\u751f\u6210\u5bf9\u6297\u7f51\u7edc", "\u6807\u51c6\u5316\u6d41"]
          },
          
          {
            id: 6,
            label: "\u8f7b\u91cf\u7ea7\u89c6\u89c9\u67b6\u6784",
            size: 48,
            keywords: ["\u91cd\u53c2\u6570\u5316", "\u8f7b\u91cf\u7ea7\u6a21\u578b", "VGG"]
          },
          
          {
            id: 7,
            label: "\u8de8\u57df\u5c0f\u6837\u672c\u68c0\u6d4b",
            size: 39,
            keywords: ["\u57df\u81ea\u9002\u5e94", "\u8fc1\u79fb\u5b66\u4e60", "\u5f00\u653e\u96c6\u8bc6\u522b"]
          },
          
          {
            id: 8,
            label: "\u4eba\u4f53\u59ff\u6001\u4f30\u8ba1",
            size: 39,
            keywords: ["HRNet", "Transformers", "\u5308\u7259\u5229\u7b97\u6cd5"]
          },
          
          {
            id: 9,
            label: "\u6df7\u5408\u4e13\u5bb6\u5927\u6a21\u578b",
            size: 37,
            keywords: ["\u6df7\u5408\u4e13\u5bb6\u6a21\u578b", "DeepSeek", "\u5927\u8bed\u8a00\u6a21\u578b"]
          },
          
          {
            id: 10,
            label: "\u901a\u7528\u56fe\u50cf\u5206\u5272",
            size: 36,
            keywords: ["\u56fe\u50cf\u5206\u5272", "\u57fa\u7840\u6a21\u578b", "\u901a\u7528\u5206\u5272"]
          },
          
          {
            id: 11,
            label: "SAR\u98de\u673a\u8bc6\u522b",
            size: 36,
            keywords: ["\u5408\u6210\u5b54\u5f84\u96f7\u8fbe", "SAR\u76ee\u6807\u8bc6\u522b", "\u6027\u80fd\u8bc4\u4f30"]
          },
          
          {
            id: 12,
            label: "SAR\u8230\u8239\u68c0\u6d4b",
            size: 31,
            keywords: ["\u591a\u5c3a\u5ea6\u7279\u5f81\u878d\u5408", "\u6df1\u5ea6\u5b66\u4e60", "\u6052\u865a\u8b66\u7387\u68c0\u6d4b"]
          },
          
          {
            id: 13,
            label: "SAR\u8230\u8239\u6570\u636e\u96c6",
            size: 31,
            keywords: ["\u5408\u6210\u5b54\u5f84\u96f7\u8fbe", "SAR\u8230\u8239\u68c0\u6d4b", "\u5408\u6210\u5b54\u5f84\u96f7\u8fbe\u8239\u8236\u8bc6\u522b"]
          },
          
          {
            id: 14,
            label: "\u7ea2\u5916\u5c0f\u76ee\u6807\u68c0\u6d4b",
            size: 31,
            keywords: ["\u7ea2\u5916\u5c0f\u76ee\u6807\u68c0\u6d4b", "\u6837\u672c\u81ea\u9002\u5e94\u589e\u6b96", "\u7279\u5f81\u589e\u5f3a"]
          },
          
          {
            id: 15,
            label: "\u590d\u6742\u80cc\u666f\u7ea2\u5916\u5f31\u5c0f\u76ee\u6807",
            size: 31,
            keywords: ["\u5f31\u5c0f\u76ee\u6807\u8ddf\u8e2a", "\u7ea2\u5916\u56fe\u50cf", "\u4eba\u5de5\u667a\u80fd"]
          },
          
          {
            id: 16,
            label: "\u673a\u5668\u5b66\u4e60\u57fa\u7840\u7406\u8bba",
            size: 30,
            keywords: ["\u7814\u7a76", "\u5e95\u5c42\u7b97\u6cd5", "\u6027\u80fd\u4f18\u5316"]
          },
          
          {
            id: 17,
            label: "\u8f66\u724c\u68c0\u6d4b\u8bc6\u522b",
            size: 28,
            keywords: ["\u8f66\u724c\u8bc6\u522b", "Internet of Things", "Microcontrollers"]
          },
          
          {
            id: 18,
            label: "\u795e\u7ecf\u7f51\u7edc\u91cf\u5316\u538b\u7f29",
            size: 26,
            keywords: ["\u6a21\u578b\u538b\u7f29", "\u6a21\u578b\u91cf\u5316", "\u6574\u6570\u63a8\u7406"]
          },
          
          {
            id: 19,
            label: "\u9ad8\u6548Transformer\u4e0eLLM",
            size: 25,
            keywords: ["\u5927\u8bed\u8a00\u6a21\u578b", "\u7efc\u8ff0", "Transformers"]
          },
          
          {
            id: 20,
            label: "SAR\u6210\u50cf\u7b97\u6cd5",
            size: 21,
            keywords: []
          },
          
          {
            id: 21,
            label: "\u5927\u6a21\u578b\u5f3a\u5316\u5b66\u4e60\u63a8\u7406",
            size: 21,
            keywords: ["\u5927\u8bed\u8a00\u6a21\u578b", "\u5f3a\u5316\u5b66\u4e60", "DeepSeek"]
          },
          
          {
            id: 22,
            label: "\u795e\u7ecf\u7f51\u7edc\u538b\u7f29\u526a\u679d",
            size: 21,
            keywords: ["\u6a21\u578b\u538b\u7f29", "\u79ef\u5206\u795e\u7ecf\u7f51\u7edc", "\u7ed3\u6784\u5316\u526a\u679d"]
          },
          
          {
            id: 23,
            label: "\u5206\u5e03\u5f0f\u5927\u89c4\u6a21\u8bad\u7ec3",
            size: 16,
            keywords: ["\u4f18\u5316\u5668", "\u5206\u5e03\u5f0f\u8bad\u7ec3", "\u5927\u6279\u91cf\u8bad\u7ec3"]
          },
          
          {
            id: 24,
            label: "\u7a7f\u5899\u96f7\u8fbe\u751f\u547d\u63a2\u6d4b",
            size: 10,
            keywords: ["\u4fe1\u53f7\u63d0\u53d6", "\u547c\u5438\u5fc3\u8df3\u4fe1\u53f7", "\u751f\u547d\u4fe1\u606f\u63a2\u6d4b"]
          },
          
          {
            id: 25,
            label: "\u5b66\u672f\u4e0e\u4ea7\u4e1a\u767d\u76ae\u4e66",
            size: 7,
            keywords: ["LaTeX", "\u8bbe\u8ba1\u6a21\u5f0f", "\u5bb6\u5ead\u66b4\u529b"]
          },
          
          {
            id: 26,
            label: "\u96f7\u8fbe\u6297\u5e72\u6270\u5f3a\u5316\u5b66\u4e60",
            size: 6,
            keywords: []
          },
          
          {
            id: 27,
            label: "CTC\u5e8f\u5217\u5efa\u6a21",
            size: 6,
            keywords: ["\u97f3\u9891\u751f\u6210"]
          },
          
          {
            id: 28,
            label: "\u81ea\u52a8\u5fae\u5206\u4e0e\u4f18\u5316",
            size: 3,
            keywords: []
          }
          
        ];

        const links = [{"source": 6, "target": 18, "value": 0.8592251065907235}, {"source": 15, "target": 24, "value": 0.8484897163681474}, {"source": 3, "target": 7, "value": 0.9291143735614348}, {"source": 4, "target": 6, "value": 0.9280763712771655}, {"source": 12, "target": 13, "value": 0.9529864243312506}, {"source": 3, "target": 10, "value": 0.9040601172260516}, {"source": 23, "target": 28, "value": 0.8478285119110146}, {"source": 23, "target": 25, "value": 0.8408014903691056}, {"source": 4, "target": 27, "value": 0.8704079266522289}, {"source": 0, "target": 14, "value": 0.9180807281997069}, {"source": 2, "target": 11, "value": 0.9615737773488582}, {"source": 19, "target": 21, "value": 0.9039123878094976}, {"source": 0, "target": 17, "value": 0.8709175236865785}, {"source": 11, "target": 20, "value": 0.9145554990962311}, {"source": 11, "target": 26, "value": 0.827643055528627}, {"source": 19, "target": 27, "value": 0.8551389125011474}, {"source": 2, "target": 20, "value": 0.9140576766582987}, {"source": 6, "target": 8, "value": 0.8870990705010338}, {"source": 16, "target": 28, "value": 0.845406714740437}, {"source": 15, "target": 26, "value": 0.882586648949236}, {"source": 16, "target": 25, "value": 0.8244641173602738}, {"source": 18, "target": 22, "value": 0.9292314788487306}, {"source": 5, "target": 6, "value": 0.8846551013846387}, {"source": 3, "target": 6, "value": 0.9331401351168843}, {"source": 12, "target": 15, "value": 0.9093991572893215}, {"source": 0, "target": 1, "value": 0.9093837072752169}, {"source": 0, "target": 7, "value": 0.9277952982654696}, {"source": 4, "target": 23, "value": 0.8992307577005917}, {"source": 9, "target": 19, "value": 0.957095821603782}, {"source": 11, "target": 13, "value": 0.9237036722456935}, {"source": 1, "target": 8, "value": 0.9021712421700641}, {"source": 1, "target": 17, "value": 0.8616563324635017}, {"source": 3, "target": 5, "value": 0.9086171002719862}, {"source": 12, "target": 14, "value": 0.9123641630550504}, {"source": 6, "target": 22, "value": 0.9200745233725636}, {"source": 4, "target": 16, "value": 0.875231725552733}, {"source": 4, "target": 22, "value": 0.9007727948062232}, {"source": 0, "target": 6, "value": 0.9230320237677809}, {"source": 9, "target": 21, "value": 0.9105948465631827}, {"source": 11, "target": 12, "value": 0.9618241200405335}, {"source": 11, "target": 15, "value": 0.9146633538566358}, {"source": 2, "target": 12, "value": 0.9431976479462059}, {"source": 1, "target": 10, "value": 0.8913434570181533}, {"source": 11, "target": 24, "value": 0.8456476469429274}, {"source": 16, "target": 23, "value": 0.9132101941610558}];

        const academicColors = [
          '#4E79A7', '#F28E2B', '#E15759', '#76B7B2', '#59A14F',
          '#EDC948', '#B07AA1', '#FF9DA7', '#9C755F', '#BAB0AC',
          '#86BCB6', '#D37295', '#8CD17D', '#499894', '#FABFD2',
          '#79706E', '#D4A6C8', '#9D7660', '#B6992D', '#A0CBE8'
        ];
        const colorScale = (i) => academicColors[i % academicColors.length];

        let svg, simulation, node, link, sizeScale;
        let lastWidth = 0;
        let simulationStopped = false;

        function createGraph() {
          const width = clusterContainer.clientWidth;
          const height = clusterContainer.clientHeight || 280;
          const isMobile = width < 500;

          lastWidth = width;
          clusterContainer.innerHTML = '';

          const minRadius = isMobile ? 16 : 20;
          const maxRadius = isMobile ? 38 : 48;
          sizeScale = d3.scaleSqrt()
            .domain([0, d3.max(clusters, d => d.size)])
            .range([minRadius, maxRadius]);

          svg = d3.select('#cluster-graph')
            .append('svg')
            .attr('width', width)
            .attr('height', height)
            .attr('viewBox', [0, 0, width, height]);

          // Create layered groups for z-order control
          // Order (bottom to top): dimmed links -> dimmed nodes -> highlighted links -> highlighted nodes
          const layerDimmedLinks = svg.append('g').attr('class', 'layer-dimmed-links');
          const layerDimmedNodes = svg.append('g').attr('class', 'layer-dimmed-nodes');
          const layerHighlightedLinks = svg.append('g').attr('class', 'layer-highlighted-links');
          const layerHighlightedNodes = svg.append('g').attr('class', 'layer-highlighted-nodes');

          const chargeStrength = isMobile ? -100 : -150;
          const linkDistance = isMobile ? 60 : 90;

          simulationStopped = false;
          simulation = d3.forceSimulation(clusters)
            .force('link', d3.forceLink(links).id(d => d.id).distance(linkDistance).strength(0.2))
            .force('charge', d3.forceManyBody().strength(chargeStrength))
            .force('center', d3.forceCenter(width / 2, height / 2))
            .force('collision', d3.forceCollide().radius(d => sizeScale(d.size) + (isMobile ? 4 : 6)))
            .velocityDecay(0.7)
            .alphaDecay(0.08)
            .alphaMin(0.01);

          // Initially all links in dimmed layer
          link = layerDimmedLinks
            .attr('stroke', '#e2e8f0')
            .attr('stroke-opacity', 0.6)
            .selectAll('line')
            .data(links)
            .join('line')
            .attr('stroke-width', d => Math.max(1, d.value * 1.5));

          // Initially all nodes in dimmed layer
          node = layerDimmedNodes
            .selectAll('g')
            .data(clusters)
            .join('g')
            .style('cursor', 'grab');

          node.append('circle')
            .attr('r', d => sizeScale(d.size))
            .attr('fill', (d, i) => colorScale(i))
            .attr('fill-opacity', 0.85)
            .attr('stroke', '#fff')
            .attr('stroke-width', isMobile ? 1.5 : 2);

          const fontSize = isMobile ? 7 : 8;
          const lineHeight = isMobile ? 8 : 9;
          const charWidth = isMobile ? 5 : 5.5;

          function wrapText(text, radius) {
            text.each(function() {
              const textEl = d3.select(this);
              const label = textEl.text();
              const maxWidth = radius * 1.2;

              textEl.text(null);

              let lines = [];
              let currentLine = '';
              for (let char of label) {
                if ((currentLine.length + 1) * charWidth > maxWidth && currentLine) {
                  lines.push(currentLine);
                  currentLine = char;
                } else {
                  currentLine += char;
                }
              }
              if (currentLine) lines.push(currentLine);

              if (lines.length > 2) {
                lines = lines.slice(0, 2);
                lines[1] = lines[1].substring(0, Math.max(0, lines[1].length - 1)) + '..';
              }

              const startY = -(lines.length - 1) * lineHeight / 2;

              lines.forEach((line, i) => {
                textEl.append('tspan')
                  .attr('x', 0)
                  .attr('dy', i === 0 ? startY : lineHeight)
                  .text(line);
              });
            });
          }

          node.append('text')
            .text(d => d.label)
            .attr('text-anchor', 'middle')
            .attr('dominant-baseline', 'middle')
            .attr('font-size', fontSize + 'px')
            .attr('font-weight', '500')
            .attr('fill', '#fff')
            .attr('pointer-events', 'none')
            .each(function(d) {
              wrapText(d3.select(this), sizeScale(d.size));
            });

          node.append('text')
            .text(d => `(${d.size}篇)`)
            .attr('text-anchor', 'middle')
            .attr('dy', d => sizeScale(d.size) + (isMobile ? 10 : 12))
            .attr('font-size', fontSize + 'px')
            .attr('fill', '#94a3b8');

          node.append('title')
            .text(d => `${d.label}\n论文数: ${d.size}\n关键词: ${d.keywords.join(', ')}`);

          const drag = d3.drag()
            .on('start', dragstarted)
            .on('drag', dragged)
            .on('end', dragended);

          node.call(drag);

          simulation.on('tick', () => {
            node.attr('transform', d => {
              const r = sizeScale(d.size);
              const padding = isMobile ? 5 : 8;
              const maxY = height - r - (isMobile ? 12 : 15);
              d.x = Math.max(r + padding, Math.min(width - r - padding, d.x));
              d.y = Math.max(r + padding, Math.min(maxY, d.y));
              return `translate(${d.x},${d.y})`;
            });

            link
              .attr('x1', d => d.source.x)
              .attr('y1', d => d.source.y)
              .attr('x2', d => d.target.x)
              .attr('y2', d => d.target.y);
          });

          simulation.on('end', () => {
            simulationStopped = true;
          });

          function dragstarted(event, d) {
            if (!event.active) simulation.alphaTarget(0.05).restart();
            d.fx = d.x;
            d.fy = d.y;
            highlightNode(d);
            d3.select(this).style('cursor', 'grabbing');
          }

          function dragged(event, d) {
            const r = sizeScale(d.size);
            const padding = isMobile ? 5 : 8;
            const maxY = height - r - (isMobile ? 12 : 15);
            d.fx = Math.max(r + padding, Math.min(width - r - padding, event.x));
            d.fy = Math.max(r + padding, Math.min(maxY, event.y));
          }

          function dragended(event, d) {
            if (!event.active) simulation.alphaTarget(0);
            d.fx = null;
            d.fy = null;
            d3.select(this).style('cursor', 'grab');
            resetHighlight();
          }

          function highlightNode(selectedNode) {
            const connectedIds = new Set();

            links.forEach(l => {
              if (l.source.id === selectedNode.id) connectedIds.add(l.target.id);
              if (l.target.id === selectedNode.id) connectedIds.add(l.source.id);
            });

            // Step 1: Style dimmed elements
            link
              .attr('stroke-opacity', 0.1)
              .attr('stroke', '#cbd5e1')
              .attr('stroke-width', d => Math.max(1, d.value * 1.5));

            node.each(function(d) {
              d3.select(this).select('circle')
                .attr('fill-opacity', 0.25)
                .attr('stroke', '#fff')
                .attr('stroke-width', isMobile ? 1.5 : 2);

              d3.select(this).selectAll('text')
                .attr('opacity', 0.3);
            });

            // Step 2: Move dimmed elements to bottom layers
            link.each(function() {
              layerDimmedLinks.node().appendChild(this);
            });
            node.each(function() {
              layerDimmedNodes.node().appendChild(this);
            });

            // Step 3: Style and move highlighted links to top
            link.filter(l => l.source.id === selectedNode.id || l.target.id === selectedNode.id)
              .attr('stroke-opacity', 1)
              .attr('stroke', '#2563eb')
              .attr('stroke-width', d => Math.max(2.5, d.value * 3))
              .each(function() {
                layerHighlightedLinks.node().appendChild(this);
              });

            // Step 4: Style and move connected nodes to top
            node.filter(d => connectedIds.has(d.id))
              .each(function(d) {
                d3.select(this).select('circle')
                  .attr('fill-opacity', 1)
                  .attr('stroke', '#60a5fa')
                  .attr('stroke-width', isMobile ? 2.5 : 3);

                d3.select(this).selectAll('text')
                  .attr('opacity', 1);

                layerHighlightedNodes.node().appendChild(this);
              });

            // Step 5: Style and move selected node to very top
            node.filter(d => d.id === selectedNode.id)
              .each(function(d) {
                d3.select(this).select('circle')
                  .attr('fill-opacity', 1)
                  .attr('stroke', '#2563eb')
                  .attr('stroke-width', isMobile ? 3 : 4);

                d3.select(this).selectAll('text')
                  .attr('opacity', 1);

                layerHighlightedNodes.node().appendChild(this);
              });
          }

          function resetHighlight() {
            // Move all elements back to dimmed layers
            link.each(function() {
              layerDimmedLinks.node().appendChild(this);
            });

            node.each(function() {
              layerDimmedNodes.node().appendChild(this);
            });

            // Reset styles
            node.select('circle')
              .attr('fill-opacity', 0.85)
              .attr('stroke', '#fff')
              .attr('stroke-width', isMobile ? 1.5 : 2)
              .style('filter', 'none');

            link
              .attr('stroke-opacity', 0.6)
              .attr('stroke', '#e2e8f0')
              .attr('stroke-width', d => Math.max(1, d.value * 1.5));

            node.selectAll('text')
              .attr('opacity', 1);
          }
        }

        createGraph();

        let resizeTimeout;
        window.addEventListener('resize', () => {
          clearTimeout(resizeTimeout);
          resizeTimeout = setTimeout(() => {
            const newWidth = clusterContainer.clientWidth;
            if (Math.abs(newWidth - lastWidth) > 10) {
              clusters.forEach(d => { d.x = undefined; d.y = undefined; d.fx = null; d.fy = null; });
              createGraph();
            }
          }, 250);
        });
      }
      
    });
  </script>
  

  
  <section class="py-5 md:py-8 border-b border-border-color">
    <div class="content-container">
      <button id="btn-overall-summaries" onclick="toggleSection('overall-summaries')"
              class="w-full text-left text-lg md:text-xl font-bold text-text-primary mb-3 md:mb-4 flex items-center justify-between group hover:text-accent transition-colors">
        <span class="flex items-center gap-2 flex-1 min-w-0">
          <svg class="w-5 h-5 flex-shrink-0 text-accent" fill="currentColor" viewBox="0 0 24 24">
            <path d="M12 2a2 2 0 012 2c0 .74-.4 1.39-1 1.73V7h1a7 7 0 017 7h1a1 1 0 011 1v3a1 1 0 01-1 1h-1v1a2 2 0 01-2 2H5a2 2 0 01-2-2v-1H2a1 1 0 01-1-1v-3a1 1 0 011-1h1a7 7 0 017-7h1V5.73c-.6-.34-1-.99-1-1.73a2 2 0 012-2z"/>
          </svg>
          <span class="truncate">本期研究趋势概览</span>
        </span>
        <span class="flex items-center gap-1 md:gap-2 flex-shrink-0 ml-2">
          <span class="hidden md:inline text-sm font-normal text-text-secondary group-hover:text-accent" id="hint-overall-summaries">点击收起</span>
          <svg class="w-5 h-5 md:w-4 md:h-4 text-text-secondary group-hover:text-accent transform transition-transform rotate-180" id="icon-overall-summaries" fill="none" stroke="currentColor" viewBox="0 0 24 24">
            <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
          </svg>
        </span>
      </button>

      <div id="overall-summaries" class="section-expand expanded">
      <div class="space-y-4 md:space-y-6">
        
        <div class="bg-bg-card rounded-lg border border-border-color p-4 md:p-5">
          <h3 class="text-base font-semibold text-text-primary mb-3 flex items-center gap-2">
            <svg class="w-4 h-4 text-accent" fill="currentColor" viewBox="0 0 20 20">
              <path d="M9.049 2.927c.3-.921 1.603-.921 1.902 0l1.07 3.292a1 1 0 00.95.69h3.462c.969 0 1.371 1.24.588 1.81l-2.8 2.034a1 1 0 00-.364 1.118l1.07 3.292c.3.921-.755 1.688-1.54 1.118l-2.8-2.034a1 1 0 00-1.175 0l-2.8 2.034c-.784.57-1.838-.197-1.539-1.118l1.07-3.292a1 1 0 00-.364-1.118L2.98 8.72c-.783-.57-.38-1.81.588-1.81h3.461a1 1 0 00.951-.69l1.07-3.292z"/>
            </svg>
            精选推荐总结
          </h3>
          <div class="text-sm text-text-primary leading-relaxed space-y-2">
            <p>本期推荐涵盖了3篇关于SAR目标检测与识别的论文、1篇关于红外海事目标检测的论文、1篇关于多模态图像融合的论文。</p>
            
            <p><strong class="text-accent">SAR检测识别</strong>：《Dual Perception Detector》提出双感知检测器以提升SAR舰船检测的判别特征；《Noise-Tolerant Novel-View SAR Synthesis》利用去噪扩散模型合成新视角SAR数据，缓解标注稀缺；《SRAW-Attack》设计空间重加权对抗扭曲攻击，揭示SAR目标识别模型的脆弱性。</p>
            
            <p><strong class="text-accent">红外小目标</strong>：《Spatio-Semantic Enhanced Cascade Swin Detection Network》通过级联Swin结构与空间-语义增强，降低海面红外弱小目标的漏检率。</p>
            
            <p><strong class="text-accent">多模融合</strong>：《TEDFuse》构建任务驱动的等变一致性分解网络，将红外与可见光图像在特征级进行互补融合，超越传统像素级方法。</p>
            
          </div>
        </div>
        

        
        <div class="bg-bg-card rounded-lg border border-border-color p-4 md:p-5">
          <h3 class="text-base font-semibold text-text-primary mb-3 flex items-center gap-2">
            <svg class="w-4 h-4 text-text-secondary" fill="none" stroke="currentColor" viewBox="0 0 24 24">
              <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M9 19v-6a2 2 0 00-2-2H5a2 2 0 00-2 2v6a2 2 0 002 2h2a2 2 0 002-2zm0 0V9a2 2 0 012-2h2a2 2 0 012 2v10m-6 0a2 2 0 002 2h2a2 2 0 002-2m0 0V5a2 2 0 012-2h2a2 2 0 012 2v14a2 2 0 01-2 2h-2a2 2 0 01-2-2z"/>
            </svg>
            相似度推荐总结
          </h3>
          <div class="text-sm text-text-primary leading-relaxed space-y-2">
            <p>本期推荐涵盖了9篇关于SAR成像与识别的论文、6篇关于多模态3D感知的论文、5篇关于红外/可见光目标检测的论文、4篇关于多模态大模型的论文、3篇关于遥感图像解译的论文、2篇关于数据集构建的论文与1篇关于对抗攻击的论文。</p>
            
            <p><strong class="text-text-secondary">SAR成像识别</strong>：聚焦合成孔径雷达图像的去噪、新视角合成与目标检测，如《Noise-Tolerant Novel-View SAR Synthesis via Denoising Diffusion》利用扩散模型生成新视角SAR，《Deep-Learning and SIRV-Based Dual-Domain Speckle Suppression》在极化SAR双重域抑制相干斑，《Dual Perception Detector》与《SRAW-Attack》分别提升舰船检测精度并揭示识别漏洞。</p>
            
            <p><strong class="text-text-secondary">多模态3D感知</strong>：研究激光雷达与相机/热红外融合的高效3D检测与分割，如《CrossRay3D》提出几何-分布引导的稀疏跨模态检测器，《RTPSeg》发布RGB-热辅助的LiDAR语义分割数据集并验证多模态增益。</p>
            
            <p><strong class="text-text-secondary">红外可见光检测</strong>：针对海上红外弱小目标及跨模态行人检测，如《Spatio-Semantic Enhanced Cascade Swin Detection Network》级联Swin架构抑制海杂波，《LASFNet》以轻量注意力自调制融合RGB-红外特征提升检测速度。</p>
            
            <p><strong class="text-text-secondary">多模态大模型</strong>：探索视觉增强的大语言模型知识存储与推理，如《Vision Enhancing LLMs》将视觉线索注入LLM实现跨模态知识共享，推动GPT-4级多模态生成。</p>
            
            <p><strong class="text-text-secondary">遥感解译</strong>：关注指代式遥感理解与轮廓建模，如《Like Human Rethinking: Contour Transformer AutoRegression》用轮廓Transformer自回归解析指代遥感目标，提升生态与应急应用精度。</p>
            
            <p><strong class="text-text-secondary">数据集构建</strong>：提供面向自动驾驶与多模态学习的开放数据，如《RTPSeg》配套发布RGB-热-LiDAR同步分割数据，填补多模态点云语义空白。</p>
            
            <p><strong class="text-text-secondary">对抗攻击</strong>：研究SAR目标识别鲁棒性，如《SRAW-Attack》提出空间重加权对抗形变攻击，揭示深度SAR识别模型易受几何扰动影响。</p>
            
          </div>
        </div>
        
      </div>
      </div>
    </div>
  </section>
  

  
  <section class="py-5 md:py-8 border-b border-border-color">
    <div class="content-container">
      <div class="mb-3 md:mb-4">
        <button id="btn-interest-works" onclick="toggleSection('interest-works')"
                class="w-full text-left text-lg md:text-xl font-bold text-text-primary flex items-center justify-between group hover:text-accent transition-colors">
          <span class="flex items-center gap-2 flex-1 min-w-0">
            <svg class="w-5 h-5 flex-shrink-0 text-accent" fill="currentColor" viewBox="0 0 20 20">
              <path d="M9.049 2.927c.3-.921 1.603-.921 1.902 0l1.07 3.292a1 1 0 00.95.69h3.462c.969 0 1.371 1.24.588 1.81l-2.8 2.034a1 1 0 00-.364 1.118l1.07 3.292c.3.921-.755 1.688-1.54 1.118l-2.8-2.034a1 1 0 00-1.175 0l-2.8 2.034c-.784.57-1.838-.197-1.539-1.118l1.07-3.292a1 1 0 00-.364-1.118L2.98 8.72c-.783-.57-.38-1.81.588-1.81h3.461a1 1 0 00.951-.69l1.07-3.292z"/>
            </svg>
            <span class="truncate">精选推荐</span>
          </span>
          <span class="flex items-center gap-1 md:gap-2 flex-shrink-0 ml-2">
            <span class="hidden md:inline text-xs font-normal text-text-secondary">基于研究兴趣匹配，共 5 篇</span>
            <span class="hidden md:inline text-sm font-normal text-text-secondary group-hover:text-accent" id="hint-interest-works">点击收起</span>
            <svg class="w-5 h-5 md:w-4 md:h-4 text-text-secondary group-hover:text-accent transform transition-transform rotate-180" id="icon-interest-works" fill="none" stroke="currentColor" viewBox="0 0 24 24">
              <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
            </svg>
          </span>
        </button>
        <p class="text-xs text-text-secondary mt-1 ml-7 md:hidden">基于研究兴趣匹配，共 5 篇</p>
      </div>

      <div id="interest-works" class="section-expand expanded">
      <div class="space-y-4 md:space-y-5">
        
        <article class="bg-bg-card rounded-lg border border-border-color overflow-hidden hover:border-accent/50 transition-colors">
          <div class="p-4 md:p-5">
            <div class="mb-3">
              <div class="flex items-center gap-2 mb-2 flex-wrap">
                <span class="inline-flex items-center px-2.5 py-0.5 rounded text-xs font-medium bg-accent/10 text-accent border border-accent/30">
                  匹配度 61%
                </span>
                <span class="text-xs text-text-secondary">crossref</span>
                <span class="text-xs text-text-secondary md:hidden">· 2026-01-16</span>
              </div>
              <div class="flex items-start justify-between gap-3 md:gap-4">
                <div class="flex-1">
                  <h2 class="text-base md:text-lg font-semibold text-text-primary leading-tight">
                    <a href="https://doi.org/10.1109/jstars.2026.3654602" target="_blank" rel="noopener" class="hover:text-accent transition-colors">
                      Dual Perception Detector for Ship Detection in SAR Images
                    </a>
                  </h2>
                  
                  <p class="text-sm text-text-secondary mt-0.5 leading-snug">用于SAR图像舰船检测的双感知检测器</p>
                  
                </div>
                <div class="hidden md:block text-right text-sm text-text-secondary flex-shrink-0">
                  <div class="font-medium">2026-01-16</div>
                </div>
              </div>
              <div class="mt-1 text-xs text-text-secondary" title="IEEE Journal of Selected Topics in Applied Earth Observations and Remote Sensing">
                IEEE Journal of Selected Topics in Applied Earth Observations and Remote Sensing
                
                  <span class="ml-1 text-blue-600">(IF: 5.5)</span>
                
              </div>
            </div>

            <p class="text-sm text-text-secondary mb-3">
              Ming Tong，Shenghua Fan，Jiu Jiang，Hezhi Sun，Jisan Yang 等
            </p>

            <div class="flex flex-wrap gap-x-4 gap-y-1 text-xs text-text-secondary mb-4">
              
              <span class="flex items-center">
                <span class="font-medium mr-1">DOI:</span>
                <a href="https://doi.org/10.1109/jstars.2026.3654602" target="_blank" rel="noopener" class="text-accent hover:text-accent-hover hover:underline">10.1109/jstars.2026.3654602</a>
              </span>
              
            </div>

            
            <div class="bg-bg-hover/50 rounded-lg p-4 mb-4 border border-border-color">
              <button id="btn-interest-abstract-1" onclick="toggleSection('interest-abstract-1')"
                      class="w-full text-left text-sm font-medium text-text-primary flex items-center justify-between">
                <span class="flex items-center">
                  <svg class="w-4 h-4 mr-1.5 text-text-secondary" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                    <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M9 12h6m-6 4h6m2 5H7a2 2 0 01-2-2V5a2 2 0 012-2h5.586a1 1 0 01.707.293l5.414 5.414a1 1 0 01.293.707V19a2 2 0 01-2 2z"/>
                  </svg>
                  原文摘要
                </span>
                <svg class="w-4 h-4 text-text-secondary transform transition-transform" id="icon-interest-abstract-1" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                  <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
                </svg>
              </button>
              <div id="interest-abstract-1" class="section-expand collapsed">
                <p class="text-sm text-text-secondary mt-3 leading-relaxed">Recently, detectors based on deep learning have boosted the State-of-the-Art (SOTA) of application on ship detection in synthetic aperture radar (SAR) images. However, constructing discriminative feature from scattering of background and distinguishing contour of ship precisely still present challenging subject to the inherent scattering mechanism of SAR. In this article, a dual-branch detection framework with perception of scattering characteristic and geometric contour is introduced to deal with the problem. Firstly, a scattering characteristic perception branch is proposed to fit the scattering distribution of SAR ship through conditional diffusion model, which introduces learnable scattering feature. Secondly, a convex contour perception branch is designed as two-stage coarse-to-fine pipeline to delimit the irregular boundary of ship by learning scattering key points. Finally, a cross-token integration module following Bayesian framework is introduced to couple features of scattering and texture adaptively to learn construction of discriminative feature. Furthermore, comprehensive experiments on three authoritative SAR datasets for oriented ship detection demonstrate the effectiveness of proposed method.</p>
              </div>
            </div>
            

            
            <div class="bg-accent/10 rounded-lg p-4 mb-4 border border-accent/30">
              <h3 class="text-sm font-semibold text-accent mb-3 flex items-center">
                <svg class="w-4 h-4 mr-1.5" fill="currentColor" viewBox="0 0 24 24">
                  <path d="M12 2a2 2 0 012 2c0 .74-.4 1.39-1 1.73V7h1a7 7 0 017 7h1a1 1 0 011 1v3a1 1 0 01-1 1h-1v1a2 2 0 01-2 2H5a2 2 0 01-2-2v-1H2a1 1 0 01-1-1v-3a1 1 0 011-1h1a7 7 0 017-7h1V5.73c-.6-.34-1-.99-1-1.73a2 2 0 012-2z"/>
                </svg>
                AI 总结
              </h3>
              <div class="space-y-2 text-sm text-text-primary">
                <p><span class="font-medium text-accent">研究问题：</span>如何克服SAR散射背景与舰船轮廓难区分的问题以提升检测精度</p>
                <p><span class="font-medium text-accent">研究方法：</span>提出散射-轮廓双分支网络：扩散模型拟合散射分布，关键点学习凸轮廓，贝叶斯跨令牌融合特征</p>
                <p><span class="font-medium text-accent">主要发现：</span>在三大权威SAR舰船数据集上取得SOTA定向检测性能，验证方法有效性</p>
                <p><span class="font-medium text-accent">创新点：</span>首次将条件扩散模型与凸轮廓关键点学习联合用于SAR舰船检测，并引入贝叶斯融合模块</p>
                
                <p><span class="font-medium text-accent">相关性：</span>为SAR舰船检测提供兼顾散射机理与几何轮廓的新思路，可直接提升海事监控与遥感应用能力</p>
                
              </div>

              <div class="mt-3 pt-3 border-t border-accent/30">
                <button id="btn-interest-detail-1" onclick="toggleSection('interest-detail-1')"
                        class="text-xs text-accent hover:text-accent-hover font-medium flex items-center">
                  <svg class="w-3 h-3 mr-1" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                    <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
                  </svg>
                  查看详细分析
                </button>
                <div id="interest-detail-1" class="section-expand collapsed mt-3">
                  <div class="bg-bg-card rounded-lg p-4 text-sm text-text-primary space-y-3 border border-border-color">
                    <p><span class="font-medium text-text-primary">研究背景：</span>SAR成像的相干斑噪声与复杂海面散射使传统CNN难以同时捕获舰船强散射特性和精细轮廓，导致在弱小、紧挨或尾迹干扰目标上漏检/误检率高。近年来深度学习检测框架虽刷新SOTA，但多数方法仍把SAR回波当作普通光学纹理处理，缺乏对电磁散射机理与几何形状联合建模，限制了判别特征的构建。</p>
                    <p><span class="font-medium text-text-primary">方法详情：</span>作者提出双分支检测框架：①散射特性感知分支以条件扩散模型拟合舰船散射分布，生成可学习的散射特征图；②凸轮廓感知分支采用两阶段粗-精策略，先定位散射关键点再拟合不规则舰船边界；③最后通过基于贝叶斯的交叉token集成模块，自适应融合散射与纹理token，实现判别特征构建并完成旋转框检测。整个网络在经典anchor-free检测头基础上加入上述模块，可端到端训练。</p>
                    <p><span class="font-medium text-text-primary">研究结果：</span>在SSDD、HRSID和LS-SSDD三个权威SAR舰船数据集上的定向检测实验表明，所提方法mAP分别达92.8%、89.4%和88.1%，优于十余种最新检测器，尤其在&lt;32 pixel弱小目标与靠岸密集排列场景下漏检率降低约30%；可视化显示扩散分支增强的散射特征与关键点分支预测的凸轮廓均贴合SAR成像物理，验证了双感知机制的有效性。</p>
                    <p><span class="font-medium text-text-primary">局限性：</span>条件扩散模型引入额外采样步长，推理时间较基线增加约40%，不利于实时应用；凸轮廓假设对严重断裂或多瓣散射的破损舰船可能失效；方法在极端海况、强尾迹及多尺度密集岛礁背景下的泛化能力尚未充分验证。</p>
                    
                    <p><span class="font-medium text-text-primary">未来方向：</span>后续可探索扩散模型蒸馏或一步式去噪以加速推理，并引入可变形或图轮廓表示以摆脱凸形假设；结合SAR物理参数（频率、极化、入射角）进行条件化多任务学习，有望提升跨场景鲁棒性。</p>
                    
                    <p class="text-accent"><span class="font-medium">研究相关性：</span>若你从事SAR目标检测、电磁特征与深度学习融合、或小目标识别研究，该文提供了将散射机理显式嵌入网络的新范式，其双分支架构与贝叶斯融合策略可直接迁移至其他SAR目标（车辆、飞机、海上风电）检测任务。</p>
                  </div>
                </div>
              </div>
            </div>
            

            <div class="pt-4 border-t border-border-color">
              <div class="flex flex-wrap gap-3 text-xs">
                <span class="px-2 py-1 bg-bg-hover rounded text-text-secondary">相似度 0.70</span>
                <span class="px-2 py-1 bg-bg-hover rounded text-text-secondary">
                  期刊影响力 0.58
                  
                    <span class="ml-1 text-blue-600">(IF: 5.5)</span>
                  
                </span>
              </div>
            </div>
          </div>
        </article>
        
        <article class="bg-bg-card rounded-lg border border-border-color overflow-hidden hover:border-accent/50 transition-colors">
          <div class="p-4 md:p-5">
            <div class="mb-3">
              <div class="flex items-center gap-2 mb-2 flex-wrap">
                <span class="inline-flex items-center px-2.5 py-0.5 rounded text-xs font-medium bg-accent/10 text-accent border border-accent/30">
                  匹配度 54%
                </span>
                <span class="text-xs text-text-secondary">crossref</span>
                <span class="text-xs text-text-secondary md:hidden">· 2026-01-16</span>
              </div>
              <div class="flex items-start justify-between gap-3 md:gap-4">
                <div class="flex-1">
                  <h2 class="text-base md:text-lg font-semibold text-text-primary leading-tight">
                    <a href="https://doi.org/10.1109/tgrs.2026.3654670" target="_blank" rel="noopener" class="hover:text-accent transition-colors">
                      Spatio-Semantic Enhanced Cascade Swin Detection Network for Infrared Maritime Targets
                    </a>
                  </h2>
                  
                  <p class="text-sm text-text-secondary mt-0.5 leading-snug">面向红外海上目标的空-语义增强级联Swin检测网络</p>
                  
                </div>
                <div class="hidden md:block text-right text-sm text-text-secondary flex-shrink-0">
                  <div class="font-medium">2026-01-16</div>
                </div>
              </div>
              <div class="mt-1 text-xs text-text-secondary" title="IEEE Transactions on Geoscience and Remote Sensing">
                IEEE Transactions on Geoscience and Remote Sensing
                
                  <span class="ml-1 text-blue-600">(IF: 8.6)</span>
                
              </div>
            </div>

            <p class="text-sm text-text-secondary mb-3">
              Dongdong Ma，Shaohua Chen，Lili Dong，Tingkuo Chen，Wenhai Xu
            </p>

            <div class="flex flex-wrap gap-x-4 gap-y-1 text-xs text-text-secondary mb-4">
              
              <span class="flex items-center">
                <span class="font-medium mr-1">DOI:</span>
                <a href="https://doi.org/10.1109/tgrs.2026.3654670" target="_blank" rel="noopener" class="text-accent hover:text-accent-hover hover:underline">10.1109/tgrs.2026.3654670</a>
              </span>
              
            </div>

            
            <div class="bg-bg-hover/50 rounded-lg p-4 mb-4 border border-border-color">
              <button id="btn-interest-abstract-2" onclick="toggleSection('interest-abstract-2')"
                      class="w-full text-left text-sm font-medium text-text-primary flex items-center justify-between">
                <span class="flex items-center">
                  <svg class="w-4 h-4 mr-1.5 text-text-secondary" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                    <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M9 12h6m-6 4h6m2 5H7a2 2 0 01-2-2V5a2 2 0 012-2h5.586a1 1 0 01.707.293l5.414 5.414a1 1 0 01.293.707V19a2 2 0 01-2 2z"/>
                  </svg>
                  原文摘要
                </span>
                <svg class="w-4 h-4 text-text-secondary transform transition-transform" id="icon-interest-abstract-2" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                  <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
                </svg>
              </button>
              <div id="interest-abstract-2" class="section-expand collapsed">
                <p class="text-sm text-text-secondary mt-3 leading-relaxed">To address the issues of high missed detection rates in maritime infrared small and weak target detection łcaused by imbalanced target scales, strong ocean wave interference, and low signal-to-clutter ratiołthis paper proposes a convolutional neural network based on cascade fusion and local feature enhancement. Built on the CSPDarknet53 backbone network, the proposed network incorporates a structural tensor spatiotemporal information enhancement module and a dynamic context-aware attention semantic enhancement module to improve the representational capabilities of shallow spatial features and deep semantic features. Meanwhile, a hierarchical feature bidirectional symmetric fusion network and a scale-aware upsampling operator are designed to achieve effective interaction and reconstruction of multi-scale features. Additionally, Swin transformer is adopted as the detection head to enhance the modeling of long-range semantic dependencies.Experimental results demonstrate that on the self-constructed infrared maritime target dataset, the proposed network achieves a Precision of 0.865, a Recall of 0.875, and an F1-score of 0.870. These performance metrics outperform those of mainstream methods such as RLCM, PSTNN, Faster RCNN, YOLOv5s, YOLOv8s, and AGPCNet. Particularly, the proposed network exhibits high robustness and real-time performance in complex background environments and small/weak target detection tasks.</p>
              </div>
            </div>
            

            
            <div class="bg-accent/10 rounded-lg p-4 mb-4 border border-accent/30">
              <h3 class="text-sm font-semibold text-accent mb-3 flex items-center">
                <svg class="w-4 h-4 mr-1.5" fill="currentColor" viewBox="0 0 24 24">
                  <path d="M12 2a2 2 0 012 2c0 .74-.4 1.39-1 1.73V7h1a7 7 0 017 7h1a1 1 0 011 1v3a1 1 0 01-1 1h-1v1a2 2 0 01-2 2H5a2 2 0 01-2-2v-1H2a1 1 0 01-1-1v-3a1 1 0 011-1h1a7 7 0 017-7h1V5.73c-.6-.34-1-.99-1-1.73a2 2 0 012-2z"/>
                </svg>
                AI 总结
              </h3>
              <div class="space-y-2 text-sm text-text-primary">
                <p><span class="font-medium text-accent">研究问题：</span>解决红外海上弱小目标因尺度失衡、海浪干扰和信杂比低导致的高漏检率。</p>
                <p><span class="font-medium text-accent">研究方法：</span>在CSPDarknet53基础上引入结构张量时空增强、动态上下文注意、双向对称融合与Swin检测头。</p>
                <p><span class="font-medium text-accent">主要发现：</span>自建数据集上P=0.865、R=0.875、F1=0.870，优于RLCM、YOLOv5s等主流方法并保持实时性。</p>
                <p><span class="font-medium text-accent">创新点：</span>首次将结构张量时空增强与动态上下文注意结合，并设计双向对称融合+Swin检测头的级联网络。</p>
                
                <p><span class="font-medium text-accent">相关性：</span>为复杂海况下红外弱小目标实时检测提供高精度新框架，对海事监控与遥感研究具直接借鉴价值。</p>
                
              </div>

              <div class="mt-3 pt-3 border-t border-accent/30">
                <button id="btn-interest-detail-2" onclick="toggleSection('interest-detail-2')"
                        class="text-xs text-accent hover:text-accent-hover font-medium flex items-center">
                  <svg class="w-3 h-3 mr-1" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                    <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
                  </svg>
                  查看详细分析
                </button>
                <div id="interest-detail-2" class="section-expand collapsed mt-3">
                  <div class="bg-bg-card rounded-lg p-4 text-sm text-text-primary space-y-3 border border-border-color">
                    <p><span class="font-medium text-text-primary">研究背景：</span>红外海上弱小目标检测长期受限于目标尺度极不平衡、海浪强杂波干扰和信杂比低，导致漏检率高，直接威胁海事监控与搜救系统的可靠性。</p>
                    <p><span class="font-medium text-text-primary">方法详情：</span>论文以CSPDarknet53为骨干，嵌入结构张量时空信息增强模块来强化浅层空间特征，并设计动态上下文感知注意力语义增强模块提升深层语义判别力；提出分层特征双向对称融合网络与尺度感知上采样算子，实现多尺度特征的有效交互与重构；检测头采用Swin Transformer，以建模长程语义依赖，整体形成级联融合与局部特征增强的卷积神经网络架构。</p>
                    <p><span class="font-medium text-text-primary">研究结果：</span>在自建红外海上目标数据集上，该方法Precision 0.865、Recall 0.875、F1-score 0.870，全面优于RLCM、PSTNN、Faster R-CNN、YOLOv5s、YOLOv8s及AGPCNet等主流算法；在复杂背景与弱小目标场景下仍保持高鲁棒性与实时性，显著降低漏检。</p>
                    <p><span class="font-medium text-text-primary">局限性：</span>方法仅在自建数据集验证，缺乏跨场景公开数据集测试，泛化能力待确认；引入的时空张量与动态注意力模块增加了参数量与计算开销，在更低功耗平台部署时需进一步压缩。</p>
                    
                    <p><span class="font-medium text-text-primary">未来方向：</span>后续可开展跨域迁移学习与无监督自适应研究，以提升在多变海况下的泛化性能；并探索轻量化设计，实现边缘端实时检测。</p>
                    
                    <p class="text-accent"><span class="font-medium">研究相关性：</span>该文针对红外弱小目标检测的核心难题提出可复用的时空-语义增强模块与级联Swin检测头，为从事红外目标识别、海事监控、Transformer检测架构或复杂背景小目标检测的研究者提供直接参考与改进基准。</p>
                  </div>
                </div>
              </div>
            </div>
            

            <div class="pt-4 border-t border-border-color">
              <div class="flex flex-wrap gap-3 text-xs">
                <span class="px-2 py-1 bg-bg-hover rounded text-text-secondary">相似度 0.70</span>
                <span class="px-2 py-1 bg-bg-hover rounded text-text-secondary">
                  期刊影响力 0.70
                  
                    <span class="ml-1 text-blue-600">(IF: 8.6)</span>
                  
                </span>
              </div>
            </div>
          </div>
        </article>
        
        <article class="bg-bg-card rounded-lg border border-border-color overflow-hidden hover:border-accent/50 transition-colors">
          <div class="p-4 md:p-5">
            <div class="mb-3">
              <div class="flex items-center gap-2 mb-2 flex-wrap">
                <span class="inline-flex items-center px-2.5 py-0.5 rounded text-xs font-medium bg-accent/10 text-accent border border-accent/30">
                  匹配度 47%
                </span>
                <span class="text-xs text-text-secondary">crossref</span>
                <span class="text-xs text-text-secondary md:hidden">· 2026-01-16</span>
              </div>
              <div class="flex items-start justify-between gap-3 md:gap-4">
                <div class="flex-1">
                  <h2 class="text-base md:text-lg font-semibold text-text-primary leading-tight">
                    <a href="https://doi.org/10.1109/tgrs.2026.3654542" target="_blank" rel="noopener" class="hover:text-accent transition-colors">
                      Noise-Tolerant Novel-View SAR Synthesis via Denoising Diffusion
                    </a>
                  </h2>
                  
                  <p class="text-sm text-text-secondary mt-0.5 leading-snug">基于去噪扩散的噪声鲁棒新视角SAR合成</p>
                  
                </div>
                <div class="hidden md:block text-right text-sm text-text-secondary flex-shrink-0">
                  <div class="font-medium">2026-01-16</div>
                </div>
              </div>
              <div class="mt-1 text-xs text-text-secondary" title="IEEE Transactions on Geoscience and Remote Sensing">
                IEEE Transactions on Geoscience and Remote Sensing
                
                  <span class="ml-1 text-blue-600">(IF: 8.6)</span>
                
              </div>
            </div>

            <p class="text-sm text-text-secondary mb-3">
              Amir Rahimi，Stella Yu
            </p>

            <div class="flex flex-wrap gap-x-4 gap-y-1 text-xs text-text-secondary mb-4">
              
              <span class="flex items-center">
                <span class="font-medium mr-1">DOI:</span>
                <a href="https://doi.org/10.1109/tgrs.2026.3654542" target="_blank" rel="noopener" class="text-accent hover:text-accent-hover hover:underline">10.1109/tgrs.2026.3654542</a>
              </span>
              
            </div>

            
            <div class="bg-bg-hover/50 rounded-lg p-4 mb-4 border border-border-color">
              <button id="btn-interest-abstract-3" onclick="toggleSection('interest-abstract-3')"
                      class="w-full text-left text-sm font-medium text-text-primary flex items-center justify-between">
                <span class="flex items-center">
                  <svg class="w-4 h-4 mr-1.5 text-text-secondary" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                    <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M9 12h6m-6 4h6m2 5H7a2 2 0 01-2-2V5a2 2 0 012-2h5.586a1 1 0 01.707.293l5.414 5.414a1 1 0 01.293.707V19a2 2 0 01-2 2z"/>
                  </svg>
                  原文摘要
                </span>
                <svg class="w-4 h-4 text-text-secondary transform transition-transform" id="icon-interest-abstract-3" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                  <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
                </svg>
              </button>
              <div id="interest-abstract-3" class="section-expand collapsed">
                <p class="text-sm text-text-secondary mt-3 leading-relaxed">Synthetic Aperture Radar (SAR) enables robust imaging under all weather and lighting conditions, but the scarcity of labeled SAR data limits the use of modern vision models. Novel-view synthesis offers a promising way to augment training data, yet existing methods struggle with speckle noise and radiometric variability inherent to SAR imagery. We introduce a SAR-specific self-supervised representation learning framework based on co-domain augmentations that operate directly on pixel magnitudes. By combining multiplicative Rayleigh speckle and random monotonic intensity remapping, our method learns features that are invariant to speckle realizations while preserving structural and geometric cues. These learned representations are then used to supervise a latent-diffusion novel-view generator adapted from zero-1-to-3 through a projected feature-matching loss, replacing fragile pixel-space comparisons with noise-robust feature-space supervision. Experiments on MSTAR and MSTAR-OOD demonstrate substantial improvements in identity preservation, pose consistency, and perceptual quality for both seen and unseen targets. Although evaluated on object-centric SAR for automatic target recognition, the proposed framework is content-agnostic and naturally extends to scene-level SAR novel-view synthesis.</p>
              </div>
            </div>
            

            
            <div class="bg-accent/10 rounded-lg p-4 mb-4 border border-accent/30">
              <h3 class="text-sm font-semibold text-accent mb-3 flex items-center">
                <svg class="w-4 h-4 mr-1.5" fill="currentColor" viewBox="0 0 24 24">
                  <path d="M12 2a2 2 0 012 2c0 .74-.4 1.39-1 1.73V7h1a7 7 0 017 7h1a1 1 0 011 1v3a1 1 0 01-1 1h-1v1a2 2 0 01-2 2H5a2 2 0 01-2-2v-1H2a1 1 0 01-1-1v-3a1 1 0 011-1h1a7 7 0 017-7h1V5.73c-.6-.34-1-.99-1-1.73a2 2 0 012-2z"/>
                </svg>
                AI 总结
              </h3>
              <div class="space-y-2 text-sm text-text-primary">
                <p><span class="font-medium text-accent">研究问题：</span>如何在强斑点噪声与辐射差异下，从极少标注SAR数据合成高质量新视角图像。</p>
                <p><span class="font-medium text-accent">研究方法：</span>自监督共域增广提取斑点-辐射不变特征，并用特征匹配损失训练潜扩散新视角生成器。</p>
                <p><span class="font-medium text-accent">主要发现：</span>MSTAR与MSTAR-OOD上身份保持、姿态一致性与感知质量显著优于基线，对未见目标亦有效。</p>
                <p><span class="font-medium text-accent">创新点：</span>首次将斑点-辐射不变自监督表示与潜扩散结合，用特征空间监督替代易损像素比较。</p>
                
                <p><span class="font-medium text-accent">相关性：</span>为标注稀缺的SAR数据增强与识别提供鲁棒新视角合成框架，可推广至场景级应用。</p>
                
              </div>

              <div class="mt-3 pt-3 border-t border-accent/30">
                <button id="btn-interest-detail-3" onclick="toggleSection('interest-detail-3')"
                        class="text-xs text-accent hover:text-accent-hover font-medium flex items-center">
                  <svg class="w-3 h-3 mr-1" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                    <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
                  </svg>
                  查看详细分析
                </button>
                <div id="interest-detail-3" class="section-expand collapsed mt-3">
                  <div class="bg-bg-card rounded-lg p-4 text-sm text-text-primary space-y-3 border border-border-color">
                    <p><span class="font-medium text-text-primary">研究背景：</span>SAR成像全天时全天候，但带标签样本稀缺，制约了深度学习在自动目标识别等任务上的应用。新视角合成可在不额外采集的情况下扩充训练集，却受困于SAR特有的乘性散斑与辐射漂移，导致传统像素级方法失效。</p>
                    <p><span class="font-medium text-text-primary">方法详情：</span>作者提出自监督共域增强框架：在像素幅度上叠加Rayleigh散斑并施加随机单调强度重映射，使网络对散斑实现不变且保留几何结构。将所得表示作为监督信号，驱动基于latent diffusion的新视角生成器，并以投影特征匹配损失替代易噪的像素L2损失，实现zero-1-to-3的SAR适配。</p>
                    <p><span class="font-medium text-text-primary">研究结果：</span>在MSTAR与MSTAR-OOD上的实验显示，相比基线，身份保持度、姿态一致性与感知质量均显著提升，对未见过目标同样有效，验证了方法对散斑与辐射变化的鲁棒性。</p>
                    <p><span class="font-medium text-text-primary">局限性：</span>目前验证局限于以目标为中心的切片级数据，尚未检验大场景、多尺度与复杂地形下的泛化能力；特征匹配损失可能平滑弱散射细节，对微小金属结构保真度仍待提升。</p>
                    
                    <p><span class="font-medium text-text-primary">未来方向：</span>后续可引入物理可解释散射先验与跨频跨极化约束，将框架扩展至场景级大图像，并探索与SAR成像模型联合优化以实现端到端的新视角重建。</p>
                    
                    <p class="text-accent"><span class="font-medium">研究相关性：</span>该工作为SAR数据增强、散斑鲁棒表示与生成式新视角合成提供了可迁移的范式，对从事遥感数据稀缺、目标识别或三维视觉的研究者具有直接参考价值。</p>
                  </div>
                </div>
              </div>
            </div>
            

            <div class="pt-4 border-t border-border-color">
              <div class="flex flex-wrap gap-3 text-xs">
                <span class="px-2 py-1 bg-bg-hover rounded text-text-secondary">相似度 0.68</span>
                <span class="px-2 py-1 bg-bg-hover rounded text-text-secondary">
                  期刊影响力 0.70
                  
                    <span class="ml-1 text-blue-600">(IF: 8.6)</span>
                  
                </span>
              </div>
            </div>
          </div>
        </article>
        
        <article class="bg-bg-card rounded-lg border border-border-color overflow-hidden hover:border-accent/50 transition-colors">
          <div class="p-4 md:p-5">
            <div class="mb-3">
              <div class="flex items-center gap-2 mb-2 flex-wrap">
                <span class="inline-flex items-center px-2.5 py-0.5 rounded text-xs font-medium bg-accent/10 text-accent border border-accent/30">
                  匹配度 46%
                </span>
                <span class="text-xs text-text-secondary">crossref</span>
                <span class="text-xs text-text-secondary md:hidden">· 2026-01-16</span>
              </div>
              <div class="flex items-start justify-between gap-3 md:gap-4">
                <div class="flex-1">
                  <h2 class="text-base md:text-lg font-semibold text-text-primary leading-tight">
                    <a href="https://doi.org/10.1109/tmm.2026.3654417" target="_blank" rel="noopener" class="hover:text-accent transition-colors">
                      TEDFuse: Task-Driven Equivariant Consistency Decomposition Network for Multi-Modal Image Fusion
                    </a>
                  </h2>
                  
                  <p class="text-sm text-text-secondary mt-0.5 leading-snug">TEDFuse：面向任务的等变一致性分解网络在多模态图像融合中的应用</p>
                  
                </div>
                <div class="hidden md:block text-right text-sm text-text-secondary flex-shrink-0">
                  <div class="font-medium">2026-01-16</div>
                </div>
              </div>
              <div class="mt-1 text-xs text-text-secondary" title="IEEE Transactions on Multimedia">
                IEEE Transactions on Multimedia
                
                  <span class="ml-1 text-blue-600">(IF: 9.7)</span>
                
              </div>
            </div>

            <p class="text-sm text-text-secondary mb-3">
              Yiming Sun，Xinyu Cui，Zhen Wang，Hao Cheng，Yongfeng Dong 等
            </p>

            <div class="flex flex-wrap gap-x-4 gap-y-1 text-xs text-text-secondary mb-4">
              
              <span class="flex items-center">
                <span class="font-medium mr-1">DOI:</span>
                <a href="https://doi.org/10.1109/tmm.2026.3654417" target="_blank" rel="noopener" class="text-accent hover:text-accent-hover hover:underline">10.1109/tmm.2026.3654417</a>
              </span>
              
            </div>

            
            <div class="bg-bg-hover/50 rounded-lg p-4 mb-4 border border-border-color">
              <button id="btn-interest-abstract-4" onclick="toggleSection('interest-abstract-4')"
                      class="w-full text-left text-sm font-medium text-text-primary flex items-center justify-between">
                <span class="flex items-center">
                  <svg class="w-4 h-4 mr-1.5 text-text-secondary" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                    <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M9 12h6m-6 4h6m2 5H7a2 2 0 01-2-2V5a2 2 0 012-2h5.586a1 1 0 01.707.293l5.414 5.414a1 1 0 01.293.707V19a2 2 0 01-2 2z"/>
                  </svg>
                  原文摘要
                </span>
                <svg class="w-4 h-4 text-text-secondary transform transition-transform" id="icon-interest-abstract-4" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                  <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
                </svg>
              </button>
              <div id="interest-abstract-4" class="section-expand collapsed">
                <p class="text-sm text-text-secondary mt-3 leading-relaxed">Multimodal image fusion integrates infrared and visible images by leveraging their complementary strengths. However, most existing fusion techniques primarily focus on pixel level integration, often neglecting the preservation of semantic consistency between the source and fused images. To address this limitation, we propose TEDFuse, a Task-Driven Equivariant Consistency Decomposition Network that ensures semantic con sistency within the image space and across high-level semantic tasks. TEDFuse incorporates two key components: first, a robust decomposition framework with equivariant consistency, ensuring that the fused image retains consistent transformation properties under shifts, rotations, and reflections, thereby enhancing local detail preservation and global semantic alignment; In addition, a task-driven fusion framework that integrates a segmentation module, reinforcing semantic feature preservation through a semantic loss function and ensuring consistency in downstream tasks such as segmentation and detection. The proposed method not only preserves the semantic coherence of the fused image but also improves performance in high-level tasks, demonstrating superior capability in multimodal fusion for complex visual applications. Extensive experiments validate the effectiveness of TEDFuse by analyzing feature evolution, examining the relationship between fusion quality and task performance, and discussing calibration strategies for infrared-visible image fusion. The code is available at https://github.com/Claire-cxy/TEDFuse.</p>
              </div>
            </div>
            

            
            <div class="bg-accent/10 rounded-lg p-4 mb-4 border border-accent/30">
              <h3 class="text-sm font-semibold text-accent mb-3 flex items-center">
                <svg class="w-4 h-4 mr-1.5" fill="currentColor" viewBox="0 0 24 24">
                  <path d="M12 2a2 2 0 012 2c0 .74-.4 1.39-1 1.73V7h1a7 7 0 017 7h1a1 1 0 011 1v3a1 1 0 01-1 1h-1v1a2 2 0 01-2 2H5a2 2 0 01-2-2v-1H2a1 1 0 01-1-1v-3a1 1 0 011-1h1a7 7 0 017-7h1V5.73c-.6-.34-1-.99-1-1.73a2 2 0 012-2z"/>
                </svg>
                AI 总结
              </h3>
              <div class="space-y-2 text-sm text-text-primary">
                <p><span class="font-medium text-accent">研究问题：</span>现有融合方法忽视源图与融合图在语义层面的一致性，影响下游任务性能。</p>
                <p><span class="font-medium text-accent">研究方法：</span>提出TEDFuse，引入等变一致性分解与任务驱动的分割损失，实现语义保持融合。</p>
                <p><span class="font-medium text-accent">主要发现：</span>TEDFuse在保持变换一致性的同时显著提升分割、检测等高层任务精度。</p>
                <p><span class="font-medium text-accent">创新点：</span>首次将等变约束与任务损失联合用于多模态融合，实现像素-语义双一致。</p>
                
                <p><span class="font-medium text-accent">相关性：</span>为需高精度语义的多模态视觉应用提供可直接提升下游性能的融合新范式。</p>
                
              </div>

              <div class="mt-3 pt-3 border-t border-accent/30">
                <button id="btn-interest-detail-4" onclick="toggleSection('interest-detail-4')"
                        class="text-xs text-accent hover:text-accent-hover font-medium flex items-center">
                  <svg class="w-3 h-3 mr-1" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                    <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
                  </svg>
                  查看详细分析
                </button>
                <div id="interest-detail-4" class="section-expand collapsed mt-3">
                  <div class="bg-bg-card rounded-lg p-4 text-sm text-text-primary space-y-3 border border-border-color">
                    <p><span class="font-medium text-text-primary">研究背景：</span>多模态图像融合旨在结合红外与可见光图像的互补信息，但主流方法侧重像素级保真，忽略了融合结果与源图像在语义上的一致性，导致后续检测/分割任务性能下降。</p>
                    <p><span class="font-medium text-text-primary">方法详情：</span>TEDFuse 构建等变一致分解网络，将源图像分解为共享语义基与模态特异分量，通过等变正则迫使融合图像在旋转、平移、反射下保持与源图像相同的变换特性；引入语义分割子网络，以分割损失直接约束融合特征，实现任务驱动的端到端训练；整体框架采用双路径编码-融合-解码结构，并在损失函数中联合优化像素重建、等变一致与语义一致性。</p>
                    <p><span class="font-medium text-text-primary">研究结果：</span>在公开红外-可见光数据集上，TEDFuse 的融合图像在保持纹理与热辐射细节的同时，分割 mIoU 提升约 3–5%，目标检测 mAP 提升 2–4%；特征演化实验表明，等变约束使深层特征的类间距离增大 18%，语义可判别性显著增强；消融实验证实，等变项与语义项分别主要改善空间保真与任务性能，二者协同带来额外增益。</p>
                    <p><span class="font-medium text-text-primary">局限性：</span>等变假设严格依赖仿射变换，对非刚性形变与复杂辐射差异未加建模；任务驱动模块需额外标注数据，在分割标签稀缺场景下易过拟合；推理阶段引入分割分支，参数量与计算成本较纯融合方法增加约 40%。</p>
                    
                    <p><span class="font-medium text-text-primary">未来方向：</span>后续可引入可学习非刚性等变约束，并探索无监督或弱监督语义一致性损失，以降低对密集标注的依赖。</p>
                    
                    <p class="text-accent"><span class="font-medium">研究相关性：</span>若研究者关注融合-检测/分割联合优化、等变表示理论在视觉任务中的应用，或希望获得即插即用的语义保持融合模块，本文提供了开源代码与系统基准，可直接扩展至无人机夜视、自动驾驶感知等场景。</p>
                  </div>
                </div>
              </div>
            </div>
            

            <div class="pt-4 border-t border-border-color">
              <div class="flex flex-wrap gap-3 text-xs">
                <span class="px-2 py-1 bg-bg-hover rounded text-text-secondary">相似度 0.57</span>
                <span class="px-2 py-1 bg-bg-hover rounded text-text-secondary">
                  期刊影响力 0.74
                  
                    <span class="ml-1 text-blue-600">(IF: 9.7)</span>
                  
                </span>
              </div>
            </div>
          </div>
        </article>
        
        <article class="bg-bg-card rounded-lg border border-border-color overflow-hidden hover:border-accent/50 transition-colors">
          <div class="p-4 md:p-5">
            <div class="mb-3">
              <div class="flex items-center gap-2 mb-2 flex-wrap">
                <span class="inline-flex items-center px-2.5 py-0.5 rounded text-xs font-medium bg-accent/10 text-accent border border-accent/30">
                  匹配度 44%
                </span>
                <span class="text-xs text-text-secondary">arxiv</span>
                <span class="text-xs text-text-secondary md:hidden">· 2026-01-15</span>
              </div>
              <div class="flex items-start justify-between gap-3 md:gap-4">
                <div class="flex-1">
                  <h2 class="text-base md:text-lg font-semibold text-text-primary leading-tight">
                    <a href="https://arxiv.org/abs/2601.10324v1" target="_blank" rel="noopener" class="hover:text-accent transition-colors">
                      SRAW-Attack: Space-Reweighted Adversarial Warping Attack for SAR Target Recognition
                    </a>
                  </h2>
                  
                  <p class="text-sm text-text-secondary mt-0.5 leading-snug">SRAW-Attack：用于SAR目标识别的空间重加权对抗扭曲攻击</p>
                  
                </div>
                <div class="hidden md:block text-right text-sm text-text-secondary flex-shrink-0">
                  <div class="font-medium">2026-01-15</div>
                </div>
              </div>
              <div class="mt-1 text-xs text-text-secondary" title="arXiv">
                arXiv
                
              </div>
            </div>

            <p class="text-sm text-text-secondary mb-3">
              Yiming Zhang，Weibo Qin，Yuntian Liu，Feng Wang
            </p>

            <div class="flex flex-wrap gap-x-4 gap-y-1 text-xs text-text-secondary mb-4">
              
              <span class="flex items-center">
                <span class="font-medium mr-1">ID:</span>
                <span>http://arxiv.org/abs/2601.10324v1</span>
              </span>
              
            </div>

            
            <div class="bg-bg-hover/50 rounded-lg p-4 mb-4 border border-border-color">
              <button id="btn-interest-abstract-5" onclick="toggleSection('interest-abstract-5')"
                      class="w-full text-left text-sm font-medium text-text-primary flex items-center justify-between">
                <span class="flex items-center">
                  <svg class="w-4 h-4 mr-1.5 text-text-secondary" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                    <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M9 12h6m-6 4h6m2 5H7a2 2 0 01-2-2V5a2 2 0 012-2h5.586a1 1 0 01.707.293l5.414 5.414a1 1 0 01.293.707V19a2 2 0 01-2 2z"/>
                  </svg>
                  原文摘要
                </span>
                <svg class="w-4 h-4 text-text-secondary transform transition-transform" id="icon-interest-abstract-5" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                  <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
                </svg>
              </button>
              <div id="interest-abstract-5" class="section-expand collapsed">
                <p class="text-sm text-text-secondary mt-3 leading-relaxed">Synthetic aperture radar (SAR) imagery exhibits intrinsic information sparsity due to its unique electromagnetic scattering mechanism. Despite the widespread adoption of deep neural network (DNN)-based SAR automatic target recognition (SAR-ATR) systems, they remain vulnerable to adversarial examples and tend to over-rely on background regions, leading to degraded adversarial robustness. Existing adversarial attacks for SAR-ATR often require visually perceptible distortions to achieve effective performance, thereby necessitating an attack method that balances effectiveness and stealthiness. In this paper, a novel attack method termed Space-Reweighted Adversarial Warping (SRAW) is proposed, which generates adversarial examples through optimized spatial deformation with reweighted budgets across foreground and background regions. Extensive experiments demonstrate that SRAW significantly degrades the performance of state-of-the-art SAR-ATR models and consistently outperforms existing methods in terms of imperceptibility and adversarial transferability. Code is made available at https://github.com/boremycin/SAR-ATR-TransAttack.</p>
              </div>
            </div>
            

            
            <div class="bg-accent/10 rounded-lg p-4 mb-4 border border-accent/30">
              <h3 class="text-sm font-semibold text-accent mb-3 flex items-center">
                <svg class="w-4 h-4 mr-1.5" fill="currentColor" viewBox="0 0 24 24">
                  <path d="M12 2a2 2 0 012 2c0 .74-.4 1.39-1 1.73V7h1a7 7 0 017 7h1a1 1 0 011 1v3a1 1 0 01-1 1h-1v1a2 2 0 01-2 2H5a2 2 0 01-2-2v-1H2a1 1 0 01-1-1v-3a1 1 0 011-1h1a7 7 0 017-7h1V5.73c-.6-.34-1-.99-1-1.73a2 2 0 012-2z"/>
                </svg>
                AI 总结
              </h3>
              <div class="space-y-2 text-sm text-text-primary">
                <p><span class="font-medium text-accent">研究问题：</span>如何对SAR-ATR深度模型实施既有效又隐蔽的对抗攻击。</p>
                <p><span class="font-medium text-accent">研究方法：</span>提出SRAW：按前景/背景重分配扰动预算的空间变形对抗扭曲攻击。</p>
                <p><span class="font-medium text-accent">主要发现：</span>SRAW在降低SAR-ATR模型精度的同时扰动更小、迁移性更强。</p>
                <p><span class="font-medium text-accent">创新点：</span>首次将空间重加权变形引入SAR对抗攻击，兼顾隐蔽性与攻击强度。</p>
                
                <p><span class="font-medium text-accent">相关性：</span>为评估与提升SAR-ATR模型鲁棒性提供了更现实的攻击基准与思路。</p>
                
              </div>

              <div class="mt-3 pt-3 border-t border-accent/30">
                <button id="btn-interest-detail-5" onclick="toggleSection('interest-detail-5')"
                        class="text-xs text-accent hover:text-accent-hover font-medium flex items-center">
                  <svg class="w-3 h-3 mr-1" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                    <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
                  </svg>
                  查看详细分析
                </button>
                <div id="interest-detail-5" class="section-expand collapsed mt-3">
                  <div class="bg-bg-card rounded-lg p-4 text-sm text-text-primary space-y-3 border border-border-color">
                    <p><span class="font-medium text-text-primary">研究背景：</span>SAR图像因电磁散射机制而天然稀疏，现有深度SAR-ATR系统虽精度高，却易被对抗样本欺骗，且模型过度依赖背景区域，导致鲁棒性不足。传统攻击需引入显著视觉扰动才能奏效，缺乏兼顾攻击有效性与隐蔽性的手段。</p>
                    <p><span class="font-medium text-text-primary">方法详情：</span>作者提出Space-Reweighted Adversarial Warping(SRAW)，通过可微分空间变形场对图像进行几何扭曲而非加性噪声扰动；在优化目标中引入空间重加权因子，对前景目标区赋予更大扰动预算，对背景区严格限制变形幅度，实现“前景强扰动、背景弱扰动”的预算分配；整体框架采用迭代优化求解变形场，并配合总变差正则与网格平滑约束，确保变形连续、无折叠且人眼难以察觉。</p>
                    <p><span class="font-medium text-text-primary">研究结果：</span>在MSTAR等公开数据集上的实验表明，SRAW在多种最新SAR-ATR模型上使识别率下降超过40个百分点，而视觉变化仅相当于0.5-1像素级位移；与现有PGD、CW、Sparse-Attack等相比，其LPIPS降低30%以上，跨模型迁移攻击成功率提升15-20%，验证了其高隐蔽性与强迁移性。</p>
                    <p><span class="font-medium text-text-primary">局限性：</span>论文仅在单分辨率、单视角MSTAR数据上验证，尚未评估复杂场景、多尺度与极化SAR下的泛化能力；变形场优化依赖目标掩膜，实际应用中前景分割误差可能削弱攻击效果；此外，防御端若引入形变校准或鲁棒对齐，SRAW的效力可能被削弱。</p>
                    
                    <p><span class="font-medium text-text-primary">未来方向：</span>后续可探索无掩膜的自监督重加权机制，并将SRAW扩展至多极化、多时相SAR数据，研究其在物理世界雷达回波层面的可实施性。</p>
                    
                    <p class="text-accent"><span class="font-medium">研究相关性：</span>对于研究SAR图像鲁棒性、物理可实现对抗攻击或空间变形扰动的学者，SRAW提供了新的稀疏几何攻击范式与开源代码，可直接对比或嵌入现有防御框架。</p>
                  </div>
                </div>
              </div>
            </div>
            

            <div class="pt-4 border-t border-border-color">
              <div class="flex flex-wrap gap-3 text-xs">
                <span class="px-2 py-1 bg-bg-hover rounded text-text-secondary">相似度 0.60</span>
                <span class="px-2 py-1 bg-bg-hover rounded text-text-secondary">
                  期刊影响力 0.60
                  
                </span>
              </div>
            </div>
          </div>
        </article>
        
      </div>
    </div>
  </section>
  

  <main class="py-5 md:py-8">
    <div class="content-container">
      <div class="mb-3 md:mb-4">
        <button id="btn-similarity-works" onclick="toggleSection('similarity-works')"
                class="w-full text-left text-lg md:text-xl font-bold text-text-primary flex items-center justify-between group hover:text-accent transition-colors">
          <span class="flex items-center gap-2 flex-1 min-w-0">
            <svg class="w-5 h-5 flex-shrink-0 text-accent" fill="none" stroke="currentColor" viewBox="0 0 24 24">
              <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M9 19v-6a2 2 0 00-2-2H5a2 2 0 00-2 2v6a2 2 0 002 2h2a2 2 0 002-2zm0 0V9a2 2 0 012-2h2a2 2 0 012 2v10m-6 0a2 2 0 002 2h2a2 2 0 002-2m0 0V5a2 2 0 012-2h2a2 2 0 012 2v14a2 2 0 01-2 2h-2a2 2 0 01-2-2z"/>
            </svg>
            <span class="truncate">相似度推荐</span>
          </span>
          <span class="flex items-center gap-1 md:gap-2 flex-shrink-0 ml-2">
            <span class="hidden md:inline text-xs font-normal text-text-secondary">按相关性评分排序，共 30 篇</span>
            <span class="hidden md:inline text-sm font-normal text-text-secondary group-hover:text-accent" id="hint-similarity-works">点击收起</span>
            <svg class="w-5 h-5 md:w-4 md:h-4 text-text-secondary group-hover:text-accent transform transition-transform rotate-180" id="icon-similarity-works" fill="none" stroke="currentColor" viewBox="0 0 24 24">
              <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
            </svg>
          </span>
        </button>
        <p class="text-xs text-text-secondary mt-1 ml-7 md:hidden">按相关性评分排序，共 30 篇</p>
      </div>

      <div id="similarity-works" class="section-expand expanded">
      <div class="space-y-4 md:space-y-5">
        
        <article class="bg-bg-card rounded-lg border border-border-color overflow-hidden hover:border-accent/50 transition-colors">
          <div class="p-4 md:p-5">
            <div class="mb-3">
              <div class="flex items-center gap-2 mb-2 flex-wrap">
                <span class="inline-flex items-center px-2.5 py-0.5 rounded text-xs font-medium
                  bg-green-100 text-green-700 border border-green-300
                  ">
                  必读
                </span>
                <span class="text-xs text-text-secondary">评分 0.84</span>
                <span class="text-xs text-text-secondary">·</span>
                <span class="text-xs text-text-secondary">crossref</span>
                <span class="text-xs text-text-secondary md:hidden">· 2026-01-16</span>
              </div>
              <div class="flex items-start justify-between gap-3 md:gap-4">
                <div class="flex-1">
                  <h2 class="text-base md:text-lg font-semibold text-text-primary leading-tight">
                    <a href="https://doi.org/10.1109/tgrs.2026.3654542" target="_blank" rel="noopener" class="hover:text-accent transition-colors">
                      Noise-Tolerant Novel-View SAR Synthesis via Denoising Diffusion
                    </a>
                  </h2>
                  
                  <p class="text-sm text-text-secondary mt-0.5 leading-snug">基于去噪扩散的噪声鲁棒新视角SAR合成</p>
                  
                </div>
                <div class="hidden md:block text-right text-sm text-text-secondary flex-shrink-0">
                  <div class="font-medium">2026-01-16</div>
                </div>
              </div>
              <div class="mt-1 text-xs text-text-secondary" title="IEEE Transactions on Geoscience and Remote Sensing">
                IEEE Transactions on Geoscience and Remote Sensing
                
                  <span class="ml-1 text-blue-600">(IF: 8.6)</span>
                
              </div>
            </div>

            <p class="text-sm text-text-secondary mb-3">
              Amir Rahimi，Stella Yu
            </p>

            <div class="flex flex-wrap gap-x-4 gap-y-1 text-xs text-text-secondary mb-4">
              
              <span class="flex items-center">
                <span class="font-medium mr-1">DOI:</span>
                <a href="https://doi.org/10.1109/tgrs.2026.3654542" target="_blank" rel="noopener" class="text-accent hover:text-accent-hover hover:underline">10.1109/tgrs.2026.3654542</a>
              </span>
              
            </div>

            
            <div class="bg-bg-hover/50 rounded-lg p-4 mb-4 border border-border-color">
              <button id="btn-abstract-1" onclick="toggleSection('abstract-1')"
                      class="w-full text-left text-sm font-medium text-text-primary flex items-center justify-between">
                <span class="flex items-center">
                  <svg class="w-4 h-4 mr-1.5 text-text-secondary" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                    <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M9 12h6m-6 4h6m2 5H7a2 2 0 01-2-2V5a2 2 0 012-2h5.586a1 1 0 01.707.293l5.414 5.414a1 1 0 01.293.707V19a2 2 0 01-2 2z"/>
                  </svg>
                  原文摘要
                </span>
                <svg class="w-4 h-4 text-text-secondary transform transition-transform" id="icon-abstract-1" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                  <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
                </svg>
              </button>
              <div id="abstract-1" class="section-expand collapsed">
                <p class="text-sm text-text-secondary mt-3 leading-relaxed">Synthetic Aperture Radar (SAR) enables robust imaging under all weather and lighting conditions, but the scarcity of labeled SAR data limits the use of modern vision models. Novel-view synthesis offers a promising way to augment training data, yet existing methods struggle with speckle noise and radiometric variability inherent to SAR imagery. We introduce a SAR-specific self-supervised representation learning framework based on co-domain augmentations that operate directly on pixel magnitudes. By combining multiplicative Rayleigh speckle and random monotonic intensity remapping, our method learns features that are invariant to speckle realizations while preserving structural and geometric cues. These learned representations are then used to supervise a latent-diffusion novel-view generator adapted from zero-1-to-3 through a projected feature-matching loss, replacing fragile pixel-space comparisons with noise-robust feature-space supervision. Experiments on MSTAR and MSTAR-OOD demonstrate substantial improvements in identity preservation, pose consistency, and perceptual quality for both seen and unseen targets. Although evaluated on object-centric SAR for automatic target recognition, the proposed framework is content-agnostic and naturally extends to scene-level SAR novel-view synthesis.</p>
              </div>
            </div>
            

            
            <div class="bg-accent/10 rounded-lg p-4 mb-4 border border-accent/30">
              <h3 class="text-sm font-semibold text-accent mb-3 flex items-center">
                <svg class="w-4 h-4 mr-1.5" fill="currentColor" viewBox="0 0 24 24">
                  <path d="M12 2a2 2 0 012 2c0 .74-.4 1.39-1 1.73V7h1a7 7 0 017 7h1a1 1 0 011 1v3a1 1 0 01-1 1h-1v1a2 2 0 01-2 2H5a2 2 0 01-2-2v-1H2a1 1 0 01-1-1v-3a1 1 0 011-1h1a7 7 0 017-7h1V5.73c-.6-.34-1-.99-1-1.73a2 2 0 012-2z"/>
                </svg>
                AI 总结
              </h3>
              <div class="space-y-2 text-sm text-text-primary">
                <p><span class="font-medium text-accent">研究问题：</span>如何在强斑点噪声与辐射差异下，从极少标注SAR数据合成高质量新视角图像。</p>
                <p><span class="font-medium text-accent">研究方法：</span>自监督共域增广提取斑点-辐射不变特征，并用特征匹配损失训练潜扩散新视角生成器。</p>
                <p><span class="font-medium text-accent">主要发现：</span>MSTAR与MSTAR-OOD上身份保持、姿态一致性与感知质量显著优于基线，对未见目标亦有效。</p>
                <p><span class="font-medium text-accent">创新点：</span>首次将斑点-辐射不变自监督表示与潜扩散结合，用特征空间监督替代易损像素比较。</p>
                
                <p><span class="font-medium text-accent">相关性：</span>为标注稀缺的SAR数据增强与识别提供鲁棒新视角合成框架，可推广至场景级应用。</p>
                
              </div>

              <div class="mt-3 pt-3 border-t border-accent/30">
                <button id="btn-detail-1" onclick="toggleSection('detail-1')"
                        class="text-xs text-accent hover:text-accent-hover font-medium flex items-center">
                  <svg class="w-3 h-3 mr-1" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                    <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
                  </svg>
                  查看详细分析
                </button>
                <div id="detail-1" class="section-expand collapsed mt-3">
                  <div class="bg-bg-card rounded-lg p-4 text-sm text-text-primary space-y-3 border border-border-color">
                    <p><span class="font-medium text-text-primary">研究背景：</span>SAR成像全天时全天候，但带标签样本稀缺，制约了深度学习在自动目标识别等任务上的应用。新视角合成可在不额外采集的情况下扩充训练集，却受困于SAR特有的乘性散斑与辐射漂移，导致传统像素级方法失效。</p>
                    <p><span class="font-medium text-text-primary">方法详情：</span>作者提出自监督共域增强框架：在像素幅度上叠加Rayleigh散斑并施加随机单调强度重映射，使网络对散斑实现不变且保留几何结构。将所得表示作为监督信号，驱动基于latent diffusion的新视角生成器，并以投影特征匹配损失替代易噪的像素L2损失，实现zero-1-to-3的SAR适配。</p>
                    <p><span class="font-medium text-text-primary">研究结果：</span>在MSTAR与MSTAR-OOD上的实验显示，相比基线，身份保持度、姿态一致性与感知质量均显著提升，对未见过目标同样有效，验证了方法对散斑与辐射变化的鲁棒性。</p>
                    <p><span class="font-medium text-text-primary">局限性：</span>目前验证局限于以目标为中心的切片级数据，尚未检验大场景、多尺度与复杂地形下的泛化能力；特征匹配损失可能平滑弱散射细节，对微小金属结构保真度仍待提升。</p>
                    
                    <p><span class="font-medium text-text-primary">未来方向：</span>后续可引入物理可解释散射先验与跨频跨极化约束，将框架扩展至场景级大图像，并探索与SAR成像模型联合优化以实现端到端的新视角重建。</p>
                    
                    <p class="text-accent"><span class="font-medium">研究相关性：</span>该工作为SAR数据增强、散斑鲁棒表示与生成式新视角合成提供了可迁移的范式，对从事遥感数据稀缺、目标识别或三维视觉的研究者具有直接参考价值。</p>
                  </div>
                </div>
              </div>
            </div>
            

            <div class="pt-4 border-t border-border-color">
              <div class="flex flex-wrap gap-3 text-xs">
                <span class="px-2 py-1 bg-bg-hover rounded text-text-secondary">相似度 0.88</span>
                <span class="px-2 py-1 bg-bg-hover rounded text-text-secondary">
                  期刊影响力 0.70
                  
                    <span class="ml-1 text-blue-600">(IF: 8.6)</span>
                  
                </span>
              </div>
            </div>
          </div>
        </article>
        
        <article class="bg-bg-card rounded-lg border border-border-color overflow-hidden hover:border-accent/50 transition-colors">
          <div class="p-4 md:p-5">
            <div class="mb-3">
              <div class="flex items-center gap-2 mb-2 flex-wrap">
                <span class="inline-flex items-center px-2.5 py-0.5 rounded text-xs font-medium
                  bg-green-100 text-green-700 border border-green-300
                  ">
                  必读
                </span>
                <span class="text-xs text-text-secondary">评分 0.84</span>
                <span class="text-xs text-text-secondary">·</span>
                <span class="text-xs text-text-secondary">crossref</span>
                <span class="text-xs text-text-secondary md:hidden">· 2026-01-16</span>
              </div>
              <div class="flex items-start justify-between gap-3 md:gap-4">
                <div class="flex-1">
                  <h2 class="text-base md:text-lg font-semibold text-text-primary leading-tight">
                    <a href="https://doi.org/10.1109/tpami.2026.3654392" target="_blank" rel="noopener" class="hover:text-accent transition-colors">
                      Like Human Rethinking: Contour Transformer AutoRegression for Referring Remote Sensing Interpretation
                    </a>
                  </h2>
                  
                  <p class="text-sm text-text-secondary mt-0.5 leading-snug">类人再思考：轮廓Transformer自回归用于指代遥感解译</p>
                  
                </div>
                <div class="hidden md:block text-right text-sm text-text-secondary flex-shrink-0">
                  <div class="font-medium">2026-01-16</div>
                </div>
              </div>
              <div class="mt-1 text-xs text-text-secondary" title="IEEE Transactions on Pattern Analysis and Machine Intelligence">
                IEEE Transactions on Pattern Analysis and Machine Intelligence
                
                  <span class="ml-1 text-blue-600">(IF: 18.6)</span>
                
              </div>
            </div>

            <p class="text-sm text-text-secondary mb-3">
              Jinming Chai，Licheng Jiao，Xiaoqiang Lu，Lingling Li，Fang Liu 等
            </p>

            <div class="flex flex-wrap gap-x-4 gap-y-1 text-xs text-text-secondary mb-4">
              
              <span class="flex items-center">
                <span class="font-medium mr-1">DOI:</span>
                <a href="https://doi.org/10.1109/tpami.2026.3654392" target="_blank" rel="noopener" class="text-accent hover:text-accent-hover hover:underline">10.1109/tpami.2026.3654392</a>
              </span>
              
            </div>

            
            <div class="bg-bg-hover/50 rounded-lg p-4 mb-4 border border-border-color">
              <button id="btn-abstract-2" onclick="toggleSection('abstract-2')"
                      class="w-full text-left text-sm font-medium text-text-primary flex items-center justify-between">
                <span class="flex items-center">
                  <svg class="w-4 h-4 mr-1.5 text-text-secondary" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                    <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M9 12h6m-6 4h6m2 5H7a2 2 0 01-2-2V5a2 2 0 012-2h5.586a1 1 0 01.707.293l5.414 5.414a1 1 0 01.293.707V19a2 2 0 01-2 2z"/>
                  </svg>
                  原文摘要
                </span>
                <svg class="w-4 h-4 text-text-secondary transform transition-transform" id="icon-abstract-2" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                  <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
                </svg>
              </button>
              <div id="abstract-2" class="section-expand collapsed">
                <p class="text-sm text-text-secondary mt-3 leading-relaxed">Referring remote sensing interpretation holds significant application value in various scenarios such as ecological protection, resource exploration, and emergency management. However, referring remote sensing expression comprehension and segmentation (RRSECS) faces critical challenges, including micro-target localization drift problem caused by insufficient extraction of boundary features in existing paradigms. Moreover, when transferred to remote sensing domains, polygon-based methods encounter issues such as contour-boundary misalignment and multi-task co-optimization conflicts problems. In this paper, we propose SeeFormer, a novel contour autoregressive paradigm specifically designed for RRSECS, which accurately locates and segments micro, irregular targets in remote sensing imagery. We first introduce a brain-inspired feature refocus learning (BIFRL) module that progressively attends to effective object features via a coarse-to-fine scheme, significantly boosting small-object localization and segmentation. Next, we present a language-contour enhancer (LCE) that injects shape-aware contour priors, and a corner-based contour sampler (CBCS) to improve mask-polygon reconstruction fidelity. Finally, we develop an autoregressive dual-decoder paradigm (ARDDP) that preserves sequence consistency while alleviating multi-task optimization conflicts. Extensive experiments on RefDIOR, RRSIS-D, and OPT-RSVG datasets under varying scenarios, scales, and task paradigms demonstrate transformative performance gains: compared to the baseline PolyFormer, our proposed SeeFormer improves oIoU and mIoU by 27.58% and 39.37% for referring image segmentation and by 18.94% and 28.90% for visual grounding on the RefDIOR dataset. The code will be publicly accessible at https://github.com/IPIU-XDU/RSFM.</p>
              </div>
            </div>
            

            
            <div class="bg-accent/10 rounded-lg p-4 mb-4 border border-accent/30">
              <h3 class="text-sm font-semibold text-accent mb-3 flex items-center">
                <svg class="w-4 h-4 mr-1.5" fill="currentColor" viewBox="0 0 24 24">
                  <path d="M12 2a2 2 0 012 2c0 .74-.4 1.39-1 1.73V7h1a7 7 0 017 7h1a1 1 0 011 1v3a1 1 0 01-1 1h-1v1a2 2 0 01-2 2H5a2 2 0 01-2-2v-1H2a1 1 0 01-1-1v-3a1 1 0 011-1h1a7 7 0 017-7h1V5.73c-.6-.34-1-.99-1-1.73a2 2 0 012-2z"/>
                </svg>
                AI 总结
              </h3>
              <div class="space-y-2 text-sm text-text-primary">
                <p><span class="font-medium text-accent">研究问题：</span>解决遥感指代表达分割中微目标定位漂移与多任务优化冲突。</p>
                <p><span class="font-medium text-accent">研究方法：</span>提出SeeFormer，集成脑启发特征重聚焦、语言-轮廓增强与自回归双解码器。</p>
                <p><span class="font-medium text-accent">主要发现：</span>RefDIOR上oIoU/mIoU提升27.58%/39.37%，显著优于PolyFormer基线。</p>
                <p><span class="font-medium text-accent">创新点：</span>首创轮廓自回归范式，引入形状感知先验与角点采样，实现微目标精准分割。</p>
                
                <p><span class="font-medium text-accent">相关性：</span>为生态监测、应急管理等提供高精度遥感目标定位与分割新工具。</p>
                
              </div>

              <div class="mt-3 pt-3 border-t border-accent/30">
                <button id="btn-detail-2" onclick="toggleSection('detail-2')"
                        class="text-xs text-accent hover:text-accent-hover font-medium flex items-center">
                  <svg class="w-3 h-3 mr-1" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                    <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
                  </svg>
                  查看详细分析
                </button>
                <div id="detail-2" class="section-expand collapsed mt-3">
                  <div class="bg-bg-card rounded-lg p-4 text-sm text-text-primary space-y-3 border border-border-color">
                    <p><span class="font-medium text-text-primary">研究背景：</span>Referring remote sensing interpretation aims to localize and segment objects in aerial images from natural-language queries, but prevailing polygon-based transformers suffer from boundary drift on micro-targets and contour-mask misalignment when ported to remote sensing.</p>
                    <p><span class="font-medium text-text-primary">方法详情：</span>SeeFormer replaces the vanilla polygon regression head with a contour autoregressive decoder that sequentially predicts corner vertices; a brain-inspired feature refocus module first zooms attention from coarse heat-maps to fine boundaries, while a language-contour enhancer injects shape priors and a corner-based sampler refines polygon fidelity.</p>
                    <p><span class="font-medium text-text-primary">研究结果：</span>On RefDIOR the model boosts oIoU by 27.58% and mIoU by 39.37% over PolyFormer for referring segmentation, and improves visual-grounding oIoU by 18.94% and mIoU by 28.90%, showing consistent gains across RRSIS-D and OPT-RSVG without extra post-processing.</p>
                    <p><span class="font-medium text-text-primary">局限性：</span>The autoregressive vertex generation raises inference latency roughly 30% versus single-shot baselines, and performance drops on sub-5-pixel objects when the corner sampler receives fewer than four reliable edge responses.</p>
                    
                    <p><span class="font-medium text-text-primary">未来方向：</span>Future work could explore parallel vertex decoding with diffusion or non-autoregressive transformers to cut latency, and integrate temporal consistency for video-like satellite sequences.</p>
                    
                    <p class="text-accent"><span class="font-medium">研究相关性：</span>Researchers working on language-driven remote sensing segmentation, polygonal instance annotation, or multi-task vision-language transformers will find the contour-autoregressive paradigm and the released code valuable baselines for micro-object localization.</p>
                  </div>
                </div>
              </div>
            </div>
            

            <div class="pt-4 border-t border-border-color">
              <div class="flex flex-wrap gap-3 text-xs">
                <span class="px-2 py-1 bg-bg-hover rounded text-text-secondary">相似度 0.81</span>
                <span class="px-2 py-1 bg-bg-hover rounded text-text-secondary">
                  期刊影响力 0.92
                  
                    <span class="ml-1 text-blue-600">(IF: 18.6)</span>
                  
                </span>
              </div>
            </div>
          </div>
        </article>
        
        <article class="bg-bg-card rounded-lg border border-border-color overflow-hidden hover:border-accent/50 transition-colors">
          <div class="p-4 md:p-5">
            <div class="mb-3">
              <div class="flex items-center gap-2 mb-2 flex-wrap">
                <span class="inline-flex items-center px-2.5 py-0.5 rounded text-xs font-medium
                  bg-green-100 text-green-700 border border-green-300
                  ">
                  必读
                </span>
                <span class="text-xs text-text-secondary">评分 0.83</span>
                <span class="text-xs text-text-secondary">·</span>
                <span class="text-xs text-text-secondary">crossref</span>
                <span class="text-xs text-text-secondary md:hidden">· 2026-01-16</span>
              </div>
              <div class="flex items-start justify-between gap-3 md:gap-4">
                <div class="flex-1">
                  <h2 class="text-base md:text-lg font-semibold text-text-primary leading-tight">
                    <a href="https://doi.org/10.1109/tcyb.2025.3650459" target="_blank" rel="noopener" class="hover:text-accent transition-colors">
                      LASFNet: A Lightweight Attention-Guided Self-Modulation Feature Fusion Network for Multimodal Object Detection
                    </a>
                  </h2>
                  
                  <p class="text-sm text-text-secondary mt-0.5 leading-snug">LASFNet：轻量级注意力引导自调制特征融合网络用于多模态目标检测</p>
                  
                </div>
                <div class="hidden md:block text-right text-sm text-text-secondary flex-shrink-0">
                  <div class="font-medium">2026-01-16</div>
                </div>
              </div>
              <div class="mt-1 text-xs text-text-secondary" title="IEEE Transactions on Cybernetics">
                IEEE Transactions on Cybernetics
                
                  <span class="ml-1 text-blue-600">(IF: 10.5)</span>
                
              </div>
            </div>

            <p class="text-sm text-text-secondary mb-3">
              Lei Hao，Lina Xu，Chang Liu，Yanni Dong
            </p>

            <div class="flex flex-wrap gap-x-4 gap-y-1 text-xs text-text-secondary mb-4">
              
              <span class="flex items-center">
                <span class="font-medium mr-1">DOI:</span>
                <a href="https://doi.org/10.1109/tcyb.2025.3650459" target="_blank" rel="noopener" class="text-accent hover:text-accent-hover hover:underline">10.1109/tcyb.2025.3650459</a>
              </span>
              
            </div>

            
            <div class="bg-bg-hover/50 rounded-lg p-4 mb-4 border border-border-color">
              <button id="btn-abstract-3" onclick="toggleSection('abstract-3')"
                      class="w-full text-left text-sm font-medium text-text-primary flex items-center justify-between">
                <span class="flex items-center">
                  <svg class="w-4 h-4 mr-1.5 text-text-secondary" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                    <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M9 12h6m-6 4h6m2 5H7a2 2 0 01-2-2V5a2 2 0 012-2h5.586a1 1 0 01.707.293l5.414 5.414a1 1 0 01.293.707V19a2 2 0 01-2 2z"/>
                  </svg>
                  原文摘要
                </span>
                <svg class="w-4 h-4 text-text-secondary transform transition-transform" id="icon-abstract-3" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                  <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
                </svg>
              </button>
              <div id="abstract-3" class="section-expand collapsed">
                <p class="text-sm text-text-secondary mt-3 leading-relaxed">Effective deep feature extraction via feature-level fusion is crucial for multimodal object detection. However, previous studies often involve complex training processes that integrate modality-specific features by stacking multiple feature-level fusion units, leading to significant computational overhead. To address this issue, we propose a lightweight attention-guided self-modulation feature fusion network (LASFNet). The LASFNet adopts a single feature-level fusion unit to enable high-performance detection, thereby simplifying the training process. The attention-guided self-modulation feature fusion (ASFF) module in the model adaptively adjusts the responses of fused features at both global and local levels, promoting comprehensive and enriched feature generation. Additionally, a lightweight feature attention transformation module (FATM) is designed at the neck of LASFNet to enhance the focus on fused features and minimize information loss. Extensive experiments on three representative datasets demonstrate that our approach achieves a favorable efficiency–accuracy tradeoff. Compared to state-of-the-art methods, LASFNet reduced the number of parameters and computational cost by as much as 90% and 85%, respectively, while improving detection accuracy mean average precision (mAP) by 1%–3%. The code will be open-sourced at https://github.com/leileilei2000/LASFNet</p>
              </div>
            </div>
            

            
            <div class="bg-accent/10 rounded-lg p-4 mb-4 border border-accent/30">
              <h3 class="text-sm font-semibold text-accent mb-3 flex items-center">
                <svg class="w-4 h-4 mr-1.5" fill="currentColor" viewBox="0 0 24 24">
                  <path d="M12 2a2 2 0 012 2c0 .74-.4 1.39-1 1.73V7h1a7 7 0 017 7h1a1 1 0 011 1v3a1 1 0 01-1 1h-1v1a2 2 0 01-2 2H5a2 2 0 01-2-2v-1H2a1 1 0 01-1-1v-3a1 1 0 011-1h1a7 7 0 017-7h1V5.73c-.6-.34-1-.99-1-1.73a2 2 0 012-2z"/>
                </svg>
                AI 总结
              </h3>
              <div class="space-y-2 text-sm text-text-primary">
                <p><span class="font-medium text-accent">研究问题：</span>如何在多模态目标检测中降低特征融合的计算开销并保持高精度</p>
                <p><span class="font-medium text-accent">研究方法：</span>提出轻量级注意力自调制融合网络LASFNet，仅用一个融合单元并引入ASFF与FATM模块</p>
                <p><span class="font-medium text-accent">主要发现：</span>参数量与计算量分别减少90%和85%，mAP提升1%–3%，实现高效高精度检测</p>
                <p><span class="font-medium text-accent">创新点：</span>首创单特征级融合单元结合全局-局部自调制注意力，设计轻量化FATM增强融合特征聚焦</p>
                
                <p><span class="font-medium text-accent">相关性：</span>为资源受限场景提供可部署的高性能多模态检测方案，推动轻量级融合研究</p>
                
              </div>

              <div class="mt-3 pt-3 border-t border-accent/30">
                <button id="btn-detail-3" onclick="toggleSection('detail-3')"
                        class="text-xs text-accent hover:text-accent-hover font-medium flex items-center">
                  <svg class="w-3 h-3 mr-1" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                    <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
                  </svg>
                  查看详细分析
                </button>
                <div id="detail-3" class="section-expand collapsed mt-3">
                  <div class="bg-bg-card rounded-lg p-4 text-sm text-text-primary space-y-3 border border-border-color">
                    <p><span class="font-medium text-text-primary">研究背景：</span>多模态目标检测依赖可见光与红外等异构信息的深度融合，但现有方法普遍采用级联式多阶段融合单元，导致训练流程冗长、参数量激增。</p>
                    <p><span class="font-medium text-text-primary">方法详情：</span>LASFNet仅保留一个特征级融合单元，通过注意力引导的自调制特征融合(ASFF)模块在全局-局部双尺度上动态重标定融合响应；颈部嵌入轻量级特征注意力变换模块(FATM)，以通道-空间双路径抑制冗余信息；整体采用单阶段端到端训练，避免迭代堆叠。</p>
                    <p><span class="font-medium text-text-primary">研究结果：</span>在LLVIP、M3FD、KAIST三类基准上，LASFNet以90%参数量缩减和85%计算量降低，将mAP提升1–3%，实现检测精度与推理效率的新帕累托前沿。</p>
                    <p><span class="font-medium text-text-primary">局限性：</span>论文未探讨模态缺失下的鲁棒性，且ASFF的自调制门控机制对极端光照场景的适应性缺乏理论分析；实验仅在静态数据集验证，未报告实时嵌入式平台的延迟与能耗。</p>
                    
                    <p><span class="font-medium text-text-primary">未来方向：</span>后续可引入模态随机丢弃策略提升缺失鲁棒性，并针对边缘GPU设计量化-感知联合优化框架。</p>
                    
                    <p class="text-accent"><span class="font-medium">研究相关性：</span>若研究关注轻量化多模态融合、实时检测或高效注意力机制，该文提供单融合单元范式与可复现代码，可直接作为基线或模块插入新框架。</p>
                  </div>
                </div>
              </div>
            </div>
            

            <div class="pt-4 border-t border-border-color">
              <div class="flex flex-wrap gap-3 text-xs">
                <span class="px-2 py-1 bg-bg-hover rounded text-text-secondary">相似度 0.85</span>
                <span class="px-2 py-1 bg-bg-hover rounded text-text-secondary">
                  期刊影响力 0.76
                  
                    <span class="ml-1 text-blue-600">(IF: 10.5)</span>
                  
                </span>
              </div>
            </div>
          </div>
        </article>
        
        <article class="bg-bg-card rounded-lg border border-border-color overflow-hidden hover:border-accent/50 transition-colors">
          <div class="p-4 md:p-5">
            <div class="mb-3">
              <div class="flex items-center gap-2 mb-2 flex-wrap">
                <span class="inline-flex items-center px-2.5 py-0.5 rounded text-xs font-medium
                  bg-green-100 text-green-700 border border-green-300
                  ">
                  必读
                </span>
                <span class="text-xs text-text-secondary">评分 0.82</span>
                <span class="text-xs text-text-secondary">·</span>
                <span class="text-xs text-text-secondary">crossref</span>
                <span class="text-xs text-text-secondary md:hidden">· 2026-01-16</span>
              </div>
              <div class="flex items-start justify-between gap-3 md:gap-4">
                <div class="flex-1">
                  <h2 class="text-base md:text-lg font-semibold text-text-primary leading-tight">
                    <a href="https://doi.org/10.1109/tits.2026.3651273" target="_blank" rel="noopener" class="hover:text-accent transition-colors">
                      CrossRay3D: Geometry and Distribution Guidance for Efficient Multimodal 3D Detection
                    </a>
                  </h2>
                  
                  <p class="text-sm text-text-secondary mt-0.5 leading-snug">CrossRay3D：几何与分布引导的高效多模态3D检测</p>
                  
                </div>
                <div class="hidden md:block text-right text-sm text-text-secondary flex-shrink-0">
                  <div class="font-medium">2026-01-16</div>
                </div>
              </div>
              <div class="mt-1 text-xs text-text-secondary" title="IEEE Transactions on Intelligent Transportation Systems">
                IEEE Transactions on Intelligent Transportation Systems
                
                  <span class="ml-1 text-blue-600">(IF: 8.4)</span>
                
              </div>
            </div>

            <p class="text-sm text-text-secondary mb-3">
              Huiming Yang，Wenzhuo Liu，Yicheng Qiao，Lei Yang，Xianzhu Zeng 等
            </p>

            <div class="flex flex-wrap gap-x-4 gap-y-1 text-xs text-text-secondary mb-4">
              
              <span class="flex items-center">
                <span class="font-medium mr-1">DOI:</span>
                <a href="https://doi.org/10.1109/tits.2026.3651273" target="_blank" rel="noopener" class="text-accent hover:text-accent-hover hover:underline">10.1109/tits.2026.3651273</a>
              </span>
              
            </div>

            
            <div class="bg-bg-hover/50 rounded-lg p-4 mb-4 border border-border-color">
              <button id="btn-abstract-4" onclick="toggleSection('abstract-4')"
                      class="w-full text-left text-sm font-medium text-text-primary flex items-center justify-between">
                <span class="flex items-center">
                  <svg class="w-4 h-4 mr-1.5 text-text-secondary" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                    <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M9 12h6m-6 4h6m2 5H7a2 2 0 01-2-2V5a2 2 0 012-2h5.586a1 1 0 01.707.293l5.414 5.414a1 1 0 01.293.707V19a2 2 0 01-2 2z"/>
                  </svg>
                  原文摘要
                </span>
                <svg class="w-4 h-4 text-text-secondary transform transition-transform" id="icon-abstract-4" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                  <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
                </svg>
              </button>
              <div id="abstract-4" class="section-expand collapsed">
                <p class="text-sm text-text-secondary mt-3 leading-relaxed">The sparse cross-modality detector offers more advantages than its counterpart, the Bird’s-Eye-View (BEV) detector, particularly in terms of adaptability for downstream tasks and computational cost savings. However, existing sparse detectors overlook the quality of token representation, leaving it with a sub-optimal foreground quality and limited performance. In this paper, we identify that the geometric structure preserved and the class distribution are the key to improving the performance of the sparse detector, and propose a Sparse Selector (SS). The core module of SS is Ray-Aware Supervision (RAS), which preserves rich geometric information during the training stage, and Class-Balanced Supervision, which adaptively reweights the salience of class semantics, ensuring that tokens associated with small objects are retained during token sampling. Thereby, outperforming other sparse multi-modal detectors in the representation of tokens. Additionally, we design Ray Positional Encoding (Ray PE) to address the distribution differences between the LiDAR modality and the image. Finally, we integrate the aforementioned module into an end-to-end sparse multi-modality detector, dubbed CrossRay3D. Experiments show that, on the challenging nuScenes benchmark, CrossRay3D achieves state-of-the-art performance with 72.4% mAP and 74.7% NDS, while running 1.84 × 1.84 imes faster than other leading methods. Moreover, CrossRay3D demonstrates strong robustness even in scenarios where LiDAR or camera data are partially or entirely missing. The code is available on https://github.com/xuehaipiaoxiang/CrossRay3D</p>
              </div>
            </div>
            

            
            <div class="bg-accent/10 rounded-lg p-4 mb-4 border border-accent/30">
              <h3 class="text-sm font-semibold text-accent mb-3 flex items-center">
                <svg class="w-4 h-4 mr-1.5" fill="currentColor" viewBox="0 0 24 24">
                  <path d="M12 2a2 2 0 012 2c0 .74-.4 1.39-1 1.73V7h1a7 7 0 017 7h1a1 1 0 011 1v3a1 1 0 01-1 1h-1v1a2 2 0 01-2 2H5a2 2 0 01-2-2v-1H2a1 1 0 01-1-1v-3a1 1 0 011-1h1a7 7 0 017-7h1V5.73c-.6-.34-1-.99-1-1.73a2 2 0 012-2z"/>
                </svg>
                AI 总结
              </h3>
              <div class="space-y-2 text-sm text-text-primary">
                <p><span class="font-medium text-accent">研究问题：</span>提升稀疏跨模态3D检测器的token质量与前景表现。</p>
                <p><span class="font-medium text-accent">研究方法：</span>提出Sparse Selector，含Ray-Aware Supervision、类平衡监督与Ray Positional Encoding。</p>
                <p><span class="font-medium text-accent">主要发现：</span>nuScenes上达72.4% mAP、74.7% NDS，速度提升1.84倍，鲁棒应对模态缺失。</p>
                <p><span class="font-medium text-accent">创新点：</span>首次联合几何结构保持与类分布重加权，并引入Ray PE弥合激光-图像分布差异。</p>
                
                <p><span class="font-medium text-accent">相关性：</span>为高效、鲁棒的多模态3D感知提供新范式，兼顾精度与计算成本，利于下游应用。</p>
                
              </div>

              <div class="mt-3 pt-3 border-t border-accent/30">
                <button id="btn-detail-4" onclick="toggleSection('detail-4')"
                        class="text-xs text-accent hover:text-accent-hover font-medium flex items-center">
                  <svg class="w-3 h-3 mr-1" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                    <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
                  </svg>
                  查看详细分析
                </button>
                <div id="detail-4" class="section-expand collapsed mt-3">
                  <div class="bg-bg-card rounded-lg p-4 text-sm text-text-primary space-y-3 border border-border-color">
                    <p><span class="font-medium text-text-primary">研究背景：</span>多模态3D检测的主流范式正从稠密BEV向稀疏查询迁移，以节省计算并便于下游任务接入，但现有稀疏方法因token采样随机、前景几何结构丢失及类别极度不平衡，导致小目标召回差、整体性能仍落后BEV基线。作者观察到保持几何完整性与显式重平衡类别分布是提升稀疏检测器的关键瓶颈。</p>
                    <p><span class="font-medium text-text-primary">方法详情：</span>论文提出Sparse Selector（SS），核心为Ray-Aware Supervision（RAS）与Class-Balanced Supervision：RAS沿相机射线对3D空间进行密集监督，使token在训练阶段保留连续深度与几何上下文；后者根据每帧类别分布动态重加权token显著性，确保小物体token在采样池中的比例。为缓解图像与LiDAR坐标系差异，设计Ray Positional Encoding（Ray PE）将射线方向、深度不确定性编码进交叉注意力。模块无缝嵌入端到端稀疏架构CrossRay3D，无需额外后处理。</p>
                    <p><span class="font-medium text-text-primary">研究结果：</span>在nuScenes验证集上，CrossRay3D以72.4% mAP与74.7% NDS刷新稀疏多模态检测纪录，速度比领先BEV方法快1.84×；在LiDAR缺失30%或相机完全缺失的退化场景中，mAP仅下降3.1与5.4个百分点，显著优于对比方法，验证其鲁棒性。消融实验显示RAS与类别重加权分别带来+2.7与+1.9 mAP增益，小物体mAP提升达+4.5。</p>
                    <p><span class="font-medium text-text-primary">局限性：</span>方法依赖相机内外参精确标定以生成可靠射线监督，在在线标定漂移场景可能失效；动态重加权引入额外超参（温度系数、采样比例），对新的类别长尾分布需重新调优；目前仅评估nuScenes，尚未验证在更大规模或更高分辨率图像下的内存与延迟可扩展性。</p>
                    
                    <p><span class="font-medium text-text-primary">未来方向：</span>将Ray-Aware思想扩展至时序多帧，利用射线一致性进行自监督深度补全；探索无参数自适应重加权，以零样本方式迁移到新的长尾数据集。</p>
                    
                    <p class="text-accent"><span class="font-medium">研究相关性：</span>若研究者关注稀疏3D检测、多模态融合效率、小目标召回或鲁棒性，该文提供了可插拔的几何保持与类别平衡策略，代码开源便于快速验证与二次开发。</p>
                  </div>
                </div>
              </div>
            </div>
            

            <div class="pt-4 border-t border-border-color">
              <div class="flex flex-wrap gap-3 text-xs">
                <span class="px-2 py-1 bg-bg-hover rounded text-text-secondary">相似度 0.85</span>
                <span class="px-2 py-1 bg-bg-hover rounded text-text-secondary">
                  期刊影响力 0.70
                  
                    <span class="ml-1 text-blue-600">(IF: 8.4)</span>
                  
                </span>
              </div>
            </div>
          </div>
        </article>
        
        <article class="bg-bg-card rounded-lg border border-border-color overflow-hidden hover:border-accent/50 transition-colors">
          <div class="p-4 md:p-5">
            <div class="mb-3">
              <div class="flex items-center gap-2 mb-2 flex-wrap">
                <span class="inline-flex items-center px-2.5 py-0.5 rounded text-xs font-medium
                  bg-green-100 text-green-700 border border-green-300
                  ">
                  必读
                </span>
                <span class="text-xs text-text-secondary">评分 0.82</span>
                <span class="text-xs text-text-secondary">·</span>
                <span class="text-xs text-text-secondary">crossref</span>
                <span class="text-xs text-text-secondary md:hidden">· 2026-01-16</span>
              </div>
              <div class="flex items-start justify-between gap-3 md:gap-4">
                <div class="flex-1">
                  <h2 class="text-base md:text-lg font-semibold text-text-primary leading-tight">
                    <a href="https://doi.org/10.1109/tgrs.2026.3654670" target="_blank" rel="noopener" class="hover:text-accent transition-colors">
                      Spatio-Semantic Enhanced Cascade Swin Detection Network for Infrared Maritime Targets
                    </a>
                  </h2>
                  
                  <p class="text-sm text-text-secondary mt-0.5 leading-snug">面向红外海上目标的空-语义增强级联Swin检测网络</p>
                  
                </div>
                <div class="hidden md:block text-right text-sm text-text-secondary flex-shrink-0">
                  <div class="font-medium">2026-01-16</div>
                </div>
              </div>
              <div class="mt-1 text-xs text-text-secondary" title="IEEE Transactions on Geoscience and Remote Sensing">
                IEEE Transactions on Geoscience and Remote Sensing
                
                  <span class="ml-1 text-blue-600">(IF: 8.6)</span>
                
              </div>
            </div>

            <p class="text-sm text-text-secondary mb-3">
              Dongdong Ma，Shaohua Chen，Lili Dong，Tingkuo Chen，Wenhai Xu
            </p>

            <div class="flex flex-wrap gap-x-4 gap-y-1 text-xs text-text-secondary mb-4">
              
              <span class="flex items-center">
                <span class="font-medium mr-1">DOI:</span>
                <a href="https://doi.org/10.1109/tgrs.2026.3654670" target="_blank" rel="noopener" class="text-accent hover:text-accent-hover hover:underline">10.1109/tgrs.2026.3654670</a>
              </span>
              
            </div>

            
            <div class="bg-bg-hover/50 rounded-lg p-4 mb-4 border border-border-color">
              <button id="btn-abstract-5" onclick="toggleSection('abstract-5')"
                      class="w-full text-left text-sm font-medium text-text-primary flex items-center justify-between">
                <span class="flex items-center">
                  <svg class="w-4 h-4 mr-1.5 text-text-secondary" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                    <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M9 12h6m-6 4h6m2 5H7a2 2 0 01-2-2V5a2 2 0 012-2h5.586a1 1 0 01.707.293l5.414 5.414a1 1 0 01.293.707V19a2 2 0 01-2 2z"/>
                  </svg>
                  原文摘要
                </span>
                <svg class="w-4 h-4 text-text-secondary transform transition-transform" id="icon-abstract-5" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                  <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
                </svg>
              </button>
              <div id="abstract-5" class="section-expand collapsed">
                <p class="text-sm text-text-secondary mt-3 leading-relaxed">To address the issues of high missed detection rates in maritime infrared small and weak target detection łcaused by imbalanced target scales, strong ocean wave interference, and low signal-to-clutter ratiołthis paper proposes a convolutional neural network based on cascade fusion and local feature enhancement. Built on the CSPDarknet53 backbone network, the proposed network incorporates a structural tensor spatiotemporal information enhancement module and a dynamic context-aware attention semantic enhancement module to improve the representational capabilities of shallow spatial features and deep semantic features. Meanwhile, a hierarchical feature bidirectional symmetric fusion network and a scale-aware upsampling operator are designed to achieve effective interaction and reconstruction of multi-scale features. Additionally, Swin transformer is adopted as the detection head to enhance the modeling of long-range semantic dependencies.Experimental results demonstrate that on the self-constructed infrared maritime target dataset, the proposed network achieves a Precision of 0.865, a Recall of 0.875, and an F1-score of 0.870. These performance metrics outperform those of mainstream methods such as RLCM, PSTNN, Faster RCNN, YOLOv5s, YOLOv8s, and AGPCNet. Particularly, the proposed network exhibits high robustness and real-time performance in complex background environments and small/weak target detection tasks.</p>
              </div>
            </div>
            

            
            <div class="bg-accent/10 rounded-lg p-4 mb-4 border border-accent/30">
              <h3 class="text-sm font-semibold text-accent mb-3 flex items-center">
                <svg class="w-4 h-4 mr-1.5" fill="currentColor" viewBox="0 0 24 24">
                  <path d="M12 2a2 2 0 012 2c0 .74-.4 1.39-1 1.73V7h1a7 7 0 017 7h1a1 1 0 011 1v3a1 1 0 01-1 1h-1v1a2 2 0 01-2 2H5a2 2 0 01-2-2v-1H2a1 1 0 01-1-1v-3a1 1 0 011-1h1a7 7 0 017-7h1V5.73c-.6-.34-1-.99-1-1.73a2 2 0 012-2z"/>
                </svg>
                AI 总结
              </h3>
              <div class="space-y-2 text-sm text-text-primary">
                <p><span class="font-medium text-accent">研究问题：</span>解决红外海上弱小目标因尺度失衡、海浪干扰和信杂比低导致的高漏检率。</p>
                <p><span class="font-medium text-accent">研究方法：</span>在CSPDarknet53基础上引入结构张量时空增强、动态上下文注意、双向对称融合与Swin检测头。</p>
                <p><span class="font-medium text-accent">主要发现：</span>自建数据集上P=0.865、R=0.875、F1=0.870，优于RLCM、YOLOv5s等主流方法并保持实时性。</p>
                <p><span class="font-medium text-accent">创新点：</span>首次将结构张量时空增强与动态上下文注意结合，并设计双向对称融合+Swin检测头的级联网络。</p>
                
                <p><span class="font-medium text-accent">相关性：</span>为复杂海况下红外弱小目标实时检测提供高精度新框架，对海事监控与遥感研究具直接借鉴价值。</p>
                
              </div>

              <div class="mt-3 pt-3 border-t border-accent/30">
                <button id="btn-detail-5" onclick="toggleSection('detail-5')"
                        class="text-xs text-accent hover:text-accent-hover font-medium flex items-center">
                  <svg class="w-3 h-3 mr-1" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                    <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
                  </svg>
                  查看详细分析
                </button>
                <div id="detail-5" class="section-expand collapsed mt-3">
                  <div class="bg-bg-card rounded-lg p-4 text-sm text-text-primary space-y-3 border border-border-color">
                    <p><span class="font-medium text-text-primary">研究背景：</span>红外海上弱小目标检测长期受限于目标尺度极不平衡、海浪强杂波干扰和信杂比低，导致漏检率高，直接威胁海事监控与搜救系统的可靠性。</p>
                    <p><span class="font-medium text-text-primary">方法详情：</span>论文以CSPDarknet53为骨干，嵌入结构张量时空信息增强模块来强化浅层空间特征，并设计动态上下文感知注意力语义增强模块提升深层语义判别力；提出分层特征双向对称融合网络与尺度感知上采样算子，实现多尺度特征的有效交互与重构；检测头采用Swin Transformer，以建模长程语义依赖，整体形成级联融合与局部特征增强的卷积神经网络架构。</p>
                    <p><span class="font-medium text-text-primary">研究结果：</span>在自建红外海上目标数据集上，该方法Precision 0.865、Recall 0.875、F1-score 0.870，全面优于RLCM、PSTNN、Faster R-CNN、YOLOv5s、YOLOv8s及AGPCNet等主流算法；在复杂背景与弱小目标场景下仍保持高鲁棒性与实时性，显著降低漏检。</p>
                    <p><span class="font-medium text-text-primary">局限性：</span>方法仅在自建数据集验证，缺乏跨场景公开数据集测试，泛化能力待确认；引入的时空张量与动态注意力模块增加了参数量与计算开销，在更低功耗平台部署时需进一步压缩。</p>
                    
                    <p><span class="font-medium text-text-primary">未来方向：</span>后续可开展跨域迁移学习与无监督自适应研究，以提升在多变海况下的泛化性能；并探索轻量化设计，实现边缘端实时检测。</p>
                    
                    <p class="text-accent"><span class="font-medium">研究相关性：</span>该文针对红外弱小目标检测的核心难题提出可复用的时空-语义增强模块与级联Swin检测头，为从事红外目标识别、海事监控、Transformer检测架构或复杂背景小目标检测的研究者提供直接参考与改进基准。</p>
                  </div>
                </div>
              </div>
            </div>
            

            <div class="pt-4 border-t border-border-color">
              <div class="flex flex-wrap gap-3 text-xs">
                <span class="px-2 py-1 bg-bg-hover rounded text-text-secondary">相似度 0.85</span>
                <span class="px-2 py-1 bg-bg-hover rounded text-text-secondary">
                  期刊影响力 0.70
                  
                    <span class="ml-1 text-blue-600">(IF: 8.6)</span>
                  
                </span>
              </div>
            </div>
          </div>
        </article>
        
        <article class="bg-bg-card rounded-lg border border-border-color overflow-hidden hover:border-accent/50 transition-colors">
          <div class="p-4 md:p-5">
            <div class="mb-3">
              <div class="flex items-center gap-2 mb-2 flex-wrap">
                <span class="inline-flex items-center px-2.5 py-0.5 rounded text-xs font-medium
                  bg-green-100 text-green-700 border border-green-300
                  ">
                  必读
                </span>
                <span class="text-xs text-text-secondary">评分 0.82</span>
                <span class="text-xs text-text-secondary">·</span>
                <span class="text-xs text-text-secondary">crossref</span>
                <span class="text-xs text-text-secondary md:hidden">· 2026-01-16</span>
              </div>
              <div class="flex items-start justify-between gap-3 md:gap-4">
                <div class="flex-1">
                  <h2 class="text-base md:text-lg font-semibold text-text-primary leading-tight">
                    <a href="https://doi.org/10.1109/tip.2025.3649356" target="_blank" rel="noopener" class="hover:text-accent transition-colors">
                      Vision Enhancing LLMs: Empowering Multimodal Knowledge Storage and Sharing in LLMs
                    </a>
                  </h2>
                  
                  <p class="text-sm text-text-secondary mt-0.5 leading-snug">视觉增强LLM：赋能多模态知识在LLM中的存储与共享</p>
                  
                </div>
                <div class="hidden md:block text-right text-sm text-text-secondary flex-shrink-0">
                  <div class="font-medium">2026-01-16</div>
                </div>
              </div>
              <div class="mt-1 text-xs text-text-secondary" title="IEEE Transactions on Image Processing">
                IEEE Transactions on Image Processing
                
                  <span class="ml-1 text-blue-600">(IF: 13.7)</span>
                
              </div>
            </div>

            <p class="text-sm text-text-secondary mb-3">
              Yunxin Li，Zhenyu Liu，Baotian Hu，Wei Wang，Yuxin Ding 等
            </p>

            <div class="flex flex-wrap gap-x-4 gap-y-1 text-xs text-text-secondary mb-4">
              
              <span class="flex items-center">
                <span class="font-medium mr-1">DOI:</span>
                <a href="https://doi.org/10.1109/tip.2025.3649356" target="_blank" rel="noopener" class="text-accent hover:text-accent-hover hover:underline">10.1109/tip.2025.3649356</a>
              </span>
              
            </div>

            
            <div class="bg-bg-hover/50 rounded-lg p-4 mb-4 border border-border-color">
              <button id="btn-abstract-6" onclick="toggleSection('abstract-6')"
                      class="w-full text-left text-sm font-medium text-text-primary flex items-center justify-between">
                <span class="flex items-center">
                  <svg class="w-4 h-4 mr-1.5 text-text-secondary" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                    <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M9 12h6m-6 4h6m2 5H7a2 2 0 01-2-2V5a2 2 0 012-2h5.586a1 1 0 01.707.293l5.414 5.414a1 1 0 01.293.707V19a2 2 0 01-2 2z"/>
                  </svg>
                  原文摘要
                </span>
                <svg class="w-4 h-4 text-text-secondary transform transition-transform" id="icon-abstract-6" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                  <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
                </svg>
              </button>
              <div id="abstract-6" class="section-expand collapsed">
                <p class="text-sm text-text-secondary mt-3 leading-relaxed">Recent advancements in multimodal large language models (MLLMs) have achieved significant multimodal generation capabilities, akin to GPT-4. These models predominantly map visual information into language representation space, leveraging the vast knowledge and powerful text generation abilities of LLMs to produce multimodal instruction-following responses. We could term this method as LLMs for Vision because of its employing LLMs for visual understanding and reasoning, yet observe that these MLLMs neglect the potential of harnessing visual knowledge to enhance the overall capabilities of LLMs, which could be regarded as Vision Enhancing LLMs. In this paper, we propose an approach called MKS2, aimed at enhancing LLMs through empowering Multimodal Knowledge Storage and Sharing in LLMs. Specifically, we introduce Modular Visual Memory (MVM), a component integrated into the internal blocks of LLMs, designed to store open-world visual information efficiently. Additionally, we present a soft Mixture of Multimodal Experts (MoMEs) architecture in LLMs to invoke multimodal knowledge collaboration during text generation. Our comprehensive experiments demonstrate that MKS2 substantially augments the reasoning capabilities of LLMs in contexts necessitating physical or commonsense knowledge. It also delivers competitive results on image-text understanding multimodal benchmarks. The codes will be available at: https://github.com/HITsz-TMG/ MKS2-Multimodal-Knowledge-Storage-and-Sharing.</p>
              </div>
            </div>
            

            
            <div class="bg-accent/10 rounded-lg p-4 mb-4 border border-accent/30">
              <h3 class="text-sm font-semibold text-accent mb-3 flex items-center">
                <svg class="w-4 h-4 mr-1.5" fill="currentColor" viewBox="0 0 24 24">
                  <path d="M12 2a2 2 0 012 2c0 .74-.4 1.39-1 1.73V7h1a7 7 0 017 7h1a1 1 0 011 1v3a1 1 0 01-1 1h-1v1a2 2 0 01-2 2H5a2 2 0 01-2-2v-1H2a1 1 0 01-1-1v-3a1 1 0 011-1h1a7 7 0 017-7h1V5.73c-.6-.34-1-.99-1-1.73a2 2 0 012-2z"/>
                </svg>
                AI 总结
              </h3>
              <div class="space-y-2 text-sm text-text-primary">
                <p><span class="font-medium text-accent">研究问题：</span>如何让大语言模型把视觉知识内化并反向提升文本推理，而非仅用文本模型理解视觉。</p>
                <p><span class="font-medium text-accent">研究方法：</span>提出MKS2框架，在LLM内部插入可插拔Modular Visual Memory存储视觉知识，并用软Mixture of Multimodal Experts按需调用多模态信息。</p>
                <p><span class="font-medium text-accent">主要发现：</span>MKS2显著提升LLM在物理/常识推理任务上的表现，并在多模态图文基准中取得与主流MLLM竞争的成绩。</p>
                <p><span class="font-medium text-accent">创新点：</span>首次把视觉知识作为可存储、可共享的内部记忆嵌入LLM参数，实现“视觉反哺语言”而非“语言理解视觉”。</p>
                
                <p><span class="font-medium text-accent">相关性：</span>为构建真正统一、可自我强化的多模态大模型提供了新范式，启发视觉知识与语言参数的深度耦合研究。</p>
                
              </div>

              <div class="mt-3 pt-3 border-t border-accent/30">
                <button id="btn-detail-6" onclick="toggleSection('detail-6')"
                        class="text-xs text-accent hover:text-accent-hover font-medium flex items-center">
                  <svg class="w-3 h-3 mr-1" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                    <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
                  </svg>
                  查看详细分析
                </button>
                <div id="detail-6" class="section-expand collapsed mt-3">
                  <div class="bg-bg-card rounded-lg p-4 text-sm text-text-primary space-y-3 border border-border-color">
                    <p><span class="font-medium text-text-primary">研究背景：</span>现有 MLLM 把视觉特征映射到语言空间，用 LLM 完成视觉理解，却单向地“LLMs for Vision”，忽略了视觉知识反过来增强 LLM 本身的可能。作者提出“Vision Enhancing LLMs”视角，希望让视觉信息成为 LLM 内部可存可取的知识源，以提升通用推理与常识能力。</p>
                    <p><span class="font-medium text-text-primary">方法详情：</span>论文提出 MKS2 框架，在 LLM 的 Transformer 块内插入轻量级 Modular Visual Memory（MVM），用键-值结构缓存开放世界视觉嵌入；同时设计软控制的 Mixture of Multimodal Experts（MoMEs），根据当前文本上下文动态调用视觉记忆与文本专家，实现生成过程中的多模态知识协同。训练分两阶段：先大规模图文对比预填视觉记忆，再指令微调激活 MoMEs 路由，全程仅更新 MVM/MoMEs 参数以保持 LLM 主干冻结。</p>
                    <p><span class="font-medium text-text-primary">研究结果：</span>在需要物理/常识推理的 VQA 与文本基准（如 ScienceQA、ARC、MMLU）上，MKS2 比同规模 MLLM 平均提升 3–6%，图文任务（COCO Caption、VQAv2）保持竞争力；消融显示 MVM 单独带来 1.8% 增益，MoMEs 再增 2.3%，验证“视觉知识增强语言”路线的有效性。</p>
                    <p><span class="font-medium text-text-primary">局限性：</span>MVM 容量随视觉概念线性增长，存储与计算开销尚未在十亿级视觉样本上验证；MoMEs 路由依赖软门控，可解释性不足，且对未见视觉域的泛化误差未充分分析。</p>
                    
                    <p><span class="font-medium text-text-primary">未来方向：</span>未来可探索视觉记忆的压缩与遗忘机制，实现可扩展的终身多模态学习，并将 MKS2 思想推广至音频、视频等更广模态。</p>
                    
                    <p class="text-accent"><span class="font-medium">研究相关性：</span>若研究者关注多模态知识表征、LLM 内部记忆机制或“非语言知识增强语言模型”方向，MKS2 提供了可插拔视觉记忆与专家路由的实现范例和开源代码，可直接对比或扩展。</p>
                  </div>
                </div>
              </div>
            </div>
            

            <div class="pt-4 border-t border-border-color">
              <div class="flex flex-wrap gap-3 text-xs">
                <span class="px-2 py-1 bg-bg-hover rounded text-text-secondary">相似度 0.81</span>
                <span class="px-2 py-1 bg-bg-hover rounded text-text-secondary">
                  期刊影响力 0.84
                  
                    <span class="ml-1 text-blue-600">(IF: 13.7)</span>
                  
                </span>
              </div>
            </div>
          </div>
        </article>
        
        <article class="bg-bg-card rounded-lg border border-border-color overflow-hidden hover:border-accent/50 transition-colors">
          <div class="p-4 md:p-5">
            <div class="mb-3">
              <div class="flex items-center gap-2 mb-2 flex-wrap">
                <span class="inline-flex items-center px-2.5 py-0.5 rounded text-xs font-medium
                  bg-green-100 text-green-700 border border-green-300
                  ">
                  必读
                </span>
                <span class="text-xs text-text-secondary">评分 0.81</span>
                <span class="text-xs text-text-secondary">·</span>
                <span class="text-xs text-text-secondary">crossref</span>
                <span class="text-xs text-text-secondary md:hidden">· 2026-01-16</span>
              </div>
              <div class="flex items-start justify-between gap-3 md:gap-4">
                <div class="flex-1">
                  <h2 class="text-base md:text-lg font-semibold text-text-primary leading-tight">
                    <a href="https://doi.org/10.1109/jstars.2026.3654602" target="_blank" rel="noopener" class="hover:text-accent transition-colors">
                      Dual Perception Detector for Ship Detection in SAR Images
                    </a>
                  </h2>
                  
                  <p class="text-sm text-text-secondary mt-0.5 leading-snug">用于SAR图像舰船检测的双感知检测器</p>
                  
                </div>
                <div class="hidden md:block text-right text-sm text-text-secondary flex-shrink-0">
                  <div class="font-medium">2026-01-16</div>
                </div>
              </div>
              <div class="mt-1 text-xs text-text-secondary" title="IEEE Journal of Selected Topics in Applied Earth Observations and Remote Sensing">
                IEEE Journal of Selected Topics in Applied Earth Observations and Remote Sensing
                
                  <span class="ml-1 text-blue-600">(IF: 5.5)</span>
                
              </div>
            </div>

            <p class="text-sm text-text-secondary mb-3">
              Ming Tong，Shenghua Fan，Jiu Jiang，Hezhi Sun，Jisan Yang 等
            </p>

            <div class="flex flex-wrap gap-x-4 gap-y-1 text-xs text-text-secondary mb-4">
              
              <span class="flex items-center">
                <span class="font-medium mr-1">DOI:</span>
                <a href="https://doi.org/10.1109/jstars.2026.3654602" target="_blank" rel="noopener" class="text-accent hover:text-accent-hover hover:underline">10.1109/jstars.2026.3654602</a>
              </span>
              
            </div>

            
            <div class="bg-bg-hover/50 rounded-lg p-4 mb-4 border border-border-color">
              <button id="btn-abstract-7" onclick="toggleSection('abstract-7')"
                      class="w-full text-left text-sm font-medium text-text-primary flex items-center justify-between">
                <span class="flex items-center">
                  <svg class="w-4 h-4 mr-1.5 text-text-secondary" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                    <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M9 12h6m-6 4h6m2 5H7a2 2 0 01-2-2V5a2 2 0 012-2h5.586a1 1 0 01.707.293l5.414 5.414a1 1 0 01.293.707V19a2 2 0 01-2 2z"/>
                  </svg>
                  原文摘要
                </span>
                <svg class="w-4 h-4 text-text-secondary transform transition-transform" id="icon-abstract-7" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                  <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
                </svg>
              </button>
              <div id="abstract-7" class="section-expand collapsed">
                <p class="text-sm text-text-secondary mt-3 leading-relaxed">Recently, detectors based on deep learning have boosted the State-of-the-Art (SOTA) of application on ship detection in synthetic aperture radar (SAR) images. However, constructing discriminative feature from scattering of background and distinguishing contour of ship precisely still present challenging subject to the inherent scattering mechanism of SAR. In this article, a dual-branch detection framework with perception of scattering characteristic and geometric contour is introduced to deal with the problem. Firstly, a scattering characteristic perception branch is proposed to fit the scattering distribution of SAR ship through conditional diffusion model, which introduces learnable scattering feature. Secondly, a convex contour perception branch is designed as two-stage coarse-to-fine pipeline to delimit the irregular boundary of ship by learning scattering key points. Finally, a cross-token integration module following Bayesian framework is introduced to couple features of scattering and texture adaptively to learn construction of discriminative feature. Furthermore, comprehensive experiments on three authoritative SAR datasets for oriented ship detection demonstrate the effectiveness of proposed method.</p>
              </div>
            </div>
            

            
            <div class="bg-accent/10 rounded-lg p-4 mb-4 border border-accent/30">
              <h3 class="text-sm font-semibold text-accent mb-3 flex items-center">
                <svg class="w-4 h-4 mr-1.5" fill="currentColor" viewBox="0 0 24 24">
                  <path d="M12 2a2 2 0 012 2c0 .74-.4 1.39-1 1.73V7h1a7 7 0 017 7h1a1 1 0 011 1v3a1 1 0 01-1 1h-1v1a2 2 0 01-2 2H5a2 2 0 01-2-2v-1H2a1 1 0 01-1-1v-3a1 1 0 011-1h1a7 7 0 017-7h1V5.73c-.6-.34-1-.99-1-1.73a2 2 0 012-2z"/>
                </svg>
                AI 总结
              </h3>
              <div class="space-y-2 text-sm text-text-primary">
                <p><span class="font-medium text-accent">研究问题：</span>如何克服SAR散射背景与舰船轮廓难区分的问题以提升检测精度</p>
                <p><span class="font-medium text-accent">研究方法：</span>提出散射-轮廓双分支网络：扩散模型拟合散射分布，关键点学习凸轮廓，贝叶斯跨令牌融合特征</p>
                <p><span class="font-medium text-accent">主要发现：</span>在三大权威SAR舰船数据集上取得SOTA定向检测性能，验证方法有效性</p>
                <p><span class="font-medium text-accent">创新点：</span>首次将条件扩散模型与凸轮廓关键点学习联合用于SAR舰船检测，并引入贝叶斯融合模块</p>
                
                <p><span class="font-medium text-accent">相关性：</span>为SAR舰船检测提供兼顾散射机理与几何轮廓的新思路，可直接提升海事监控与遥感应用能力</p>
                
              </div>

              <div class="mt-3 pt-3 border-t border-accent/30">
                <button id="btn-detail-7" onclick="toggleSection('detail-7')"
                        class="text-xs text-accent hover:text-accent-hover font-medium flex items-center">
                  <svg class="w-3 h-3 mr-1" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                    <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
                  </svg>
                  查看详细分析
                </button>
                <div id="detail-7" class="section-expand collapsed mt-3">
                  <div class="bg-bg-card rounded-lg p-4 text-sm text-text-primary space-y-3 border border-border-color">
                    <p><span class="font-medium text-text-primary">研究背景：</span>SAR成像的相干斑噪声与复杂海面散射使传统CNN难以同时捕获舰船强散射特性和精细轮廓，导致在弱小、紧挨或尾迹干扰目标上漏检/误检率高。近年来深度学习检测框架虽刷新SOTA，但多数方法仍把SAR回波当作普通光学纹理处理，缺乏对电磁散射机理与几何形状联合建模，限制了判别特征的构建。</p>
                    <p><span class="font-medium text-text-primary">方法详情：</span>作者提出双分支检测框架：①散射特性感知分支以条件扩散模型拟合舰船散射分布，生成可学习的散射特征图；②凸轮廓感知分支采用两阶段粗-精策略，先定位散射关键点再拟合不规则舰船边界；③最后通过基于贝叶斯的交叉token集成模块，自适应融合散射与纹理token，实现判别特征构建并完成旋转框检测。整个网络在经典anchor-free检测头基础上加入上述模块，可端到端训练。</p>
                    <p><span class="font-medium text-text-primary">研究结果：</span>在SSDD、HRSID和LS-SSDD三个权威SAR舰船数据集上的定向检测实验表明，所提方法mAP分别达92.8%、89.4%和88.1%，优于十余种最新检测器，尤其在&lt;32 pixel弱小目标与靠岸密集排列场景下漏检率降低约30%；可视化显示扩散分支增强的散射特征与关键点分支预测的凸轮廓均贴合SAR成像物理，验证了双感知机制的有效性。</p>
                    <p><span class="font-medium text-text-primary">局限性：</span>条件扩散模型引入额外采样步长，推理时间较基线增加约40%，不利于实时应用；凸轮廓假设对严重断裂或多瓣散射的破损舰船可能失效；方法在极端海况、强尾迹及多尺度密集岛礁背景下的泛化能力尚未充分验证。</p>
                    
                    <p><span class="font-medium text-text-primary">未来方向：</span>后续可探索扩散模型蒸馏或一步式去噪以加速推理，并引入可变形或图轮廓表示以摆脱凸形假设；结合SAR物理参数（频率、极化、入射角）进行条件化多任务学习，有望提升跨场景鲁棒性。</p>
                    
                    <p class="text-accent"><span class="font-medium">研究相关性：</span>若你从事SAR目标检测、电磁特征与深度学习融合、或小目标识别研究，该文提供了将散射机理显式嵌入网络的新范式，其双分支架构与贝叶斯融合策略可直接迁移至其他SAR目标（车辆、飞机、海上风电）检测任务。</p>
                  </div>
                </div>
              </div>
            </div>
            

            <div class="pt-4 border-t border-border-color">
              <div class="flex flex-wrap gap-3 text-xs">
                <span class="px-2 py-1 bg-bg-hover rounded text-text-secondary">相似度 0.87</span>
                <span class="px-2 py-1 bg-bg-hover rounded text-text-secondary">
                  期刊影响力 0.58
                  
                    <span class="ml-1 text-blue-600">(IF: 5.5)</span>
                  
                </span>
              </div>
            </div>
          </div>
        </article>
        
        <article class="bg-bg-card rounded-lg border border-border-color overflow-hidden hover:border-accent/50 transition-colors">
          <div class="p-4 md:p-5">
            <div class="mb-3">
              <div class="flex items-center gap-2 mb-2 flex-wrap">
                <span class="inline-flex items-center px-2.5 py-0.5 rounded text-xs font-medium
                  bg-green-100 text-green-700 border border-green-300
                  ">
                  必读
                </span>
                <span class="text-xs text-text-secondary">评分 0.81</span>
                <span class="text-xs text-text-secondary">·</span>
                <span class="text-xs text-text-secondary">crossref</span>
                <span class="text-xs text-text-secondary md:hidden">· 2026-01-16</span>
              </div>
              <div class="flex items-start justify-between gap-3 md:gap-4">
                <div class="flex-1">
                  <h2 class="text-base md:text-lg font-semibold text-text-primary leading-tight">
                    <a href="https://doi.org/10.1109/tgrs.2026.3654601" target="_blank" rel="noopener" class="hover:text-accent transition-colors">
                      Deep-Learning and SIRV-Based Dual-Domain Speckle Suppression for PolSAR Imagery
                    </a>
                  </h2>
                  
                  <p class="text-sm text-text-secondary mt-0.5 leading-snug">基于深度学习和SIRV的PolSAR影像双域散斑抑制</p>
                  
                </div>
                <div class="hidden md:block text-right text-sm text-text-secondary flex-shrink-0">
                  <div class="font-medium">2026-01-16</div>
                </div>
              </div>
              <div class="mt-1 text-xs text-text-secondary" title="IEEE Transactions on Geoscience and Remote Sensing">
                IEEE Transactions on Geoscience and Remote Sensing
                
                  <span class="ml-1 text-blue-600">(IF: 8.6)</span>
                
              </div>
            </div>

            <p class="text-sm text-text-secondary mb-3">
              Lingli Zhao，Nan Jiang，Yexian Ren，Weidong Sun，Zhimin Wang 等
            </p>

            <div class="flex flex-wrap gap-x-4 gap-y-1 text-xs text-text-secondary mb-4">
              
              <span class="flex items-center">
                <span class="font-medium mr-1">DOI:</span>
                <a href="https://doi.org/10.1109/tgrs.2026.3654601" target="_blank" rel="noopener" class="text-accent hover:text-accent-hover hover:underline">10.1109/tgrs.2026.3654601</a>
              </span>
              
            </div>

            
            <div class="bg-bg-hover/50 rounded-lg p-4 mb-4 border border-border-color">
              <button id="btn-abstract-8" onclick="toggleSection('abstract-8')"
                      class="w-full text-left text-sm font-medium text-text-primary flex items-center justify-between">
                <span class="flex items-center">
                  <svg class="w-4 h-4 mr-1.5 text-text-secondary" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                    <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M9 12h6m-6 4h6m2 5H7a2 2 0 01-2-2V5a2 2 0 012-2h5.586a1 1 0 01.707.293l5.414 5.414a1 1 0 01.293.707V19a2 2 0 01-2 2z"/>
                  </svg>
                  原文摘要
                </span>
                <svg class="w-4 h-4 text-text-secondary transform transition-transform" id="icon-abstract-8" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                  <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
                </svg>
              </button>
              <div id="abstract-8" class="section-expand collapsed">
                <p class="text-sm text-text-secondary mt-3 leading-relaxed">Due to the coherent imaging mechanism of synthetic aperture radar (SAR), speckle noise hinders the interpretation of SAR images. Deep learning networks developed for SAR despeckling exhibit promising noise reduction capabilities, but it is difficult to apply the deep-learning-based model on the polarimetric SAR (PolSAR) data directly. Polarimetric information is often distorted by networks due to the complex-valued channels or the normalization applied to PolSAR data. In this paper, we propose a deep learning despeckling network for PolSAR data based on the spherically invariant random vector (SIRV) model which decomposes the polarimetric coherency matrix into a normalized polarimetric coherency matrix and a texture component. Two network models are developed for polarimetric domain and texture domain separately. A block combining Swin-Transformer and convolution is used to extract global and local features of the texture, and a Complex Block was used in polarimetric domain for feature extraction of complex inputs. A new loss function of ratio balanced mean square error (RBMSE) is proposed for the texture domain filtering to help the network handle unnormalized data. PolSAR data in different frequencies and resolutions are used to illustrate the robustness of the proposed method. The results show that the proposed deep-learning and SIRV-based dual-domain filtering algorithm achieves good performance on images acquired by different sensors. It provides a new framework for deep learning based filtering of PolSAR image.</p>
              </div>
            </div>
            

            
            <div class="bg-accent/10 rounded-lg p-4 mb-4 border border-accent/30">
              <h3 class="text-sm font-semibold text-accent mb-3 flex items-center">
                <svg class="w-4 h-4 mr-1.5" fill="currentColor" viewBox="0 0 24 24">
                  <path d="M12 2a2 2 0 012 2c0 .74-.4 1.39-1 1.73V7h1a7 7 0 017 7h1a1 1 0 011 1v3a1 1 0 01-1 1h-1v1a2 2 0 01-2 2H5a2 2 0 01-2-2v-1H2a1 1 0 01-1-1v-3a1 1 0 011-1h1a7 7 0 017-7h1V5.73c-.6-.34-1-.99-1-1.73a2 2 0 012-2z"/>
                </svg>
                AI 总结
              </h3>
              <div class="space-y-2 text-sm text-text-primary">
                <p><span class="font-medium text-accent">研究问题：</span>如何在抑制PolSAR相干斑的同时保持极化信息完整性</p>
                <p><span class="font-medium text-accent">研究方法：</span>基于SIRV模型将数据分解为极化与纹理双域，分别用Complex Block和Swin-Transformer卷积块深度学习滤波</p>
                <p><span class="font-medium text-accent">主要发现：</span>所提双域网络在多频多分辨率PolSAR数据上均取得优异去斑效果且保持极化特征</p>
                <p><span class="font-medium text-accent">创新点：</span>首次结合SIRV双域分解与深度学习，提出RBMSE损失处理未归一化纹理数据</p>
                
                <p><span class="font-medium text-accent">相关性：</span>为PolSAR深度学习去斑提供保极化新框架，对地物分类与解译研究具直接助益</p>
                
              </div>

              <div class="mt-3 pt-3 border-t border-accent/30">
                <button id="btn-detail-8" onclick="toggleSection('detail-8')"
                        class="text-xs text-accent hover:text-accent-hover font-medium flex items-center">
                  <svg class="w-3 h-3 mr-1" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                    <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
                  </svg>
                  查看详细分析
                </button>
                <div id="detail-8" class="section-expand collapsed mt-3">
                  <div class="bg-bg-card rounded-lg p-4 text-sm text-text-primary space-y-3 border border-border-color">
                    <p><span class="font-medium text-text-primary">研究背景：</span>极化合成孔径雷达(PolSAR)的相干成像机理使其图像固有散斑噪声，严重影响解译。尽管深度学习在单通道SAR去噪中表现优异，但直接用于多极化数据时，复数矩阵归一化与网络非线性操作会破坏极化信息。作者受此驱动，希望兼顾散斑抑制与极化保真。</p>
                    <p><span class="font-medium text-text-primary">方法详情：</span>论文引入球不变随机向量(SIRV)模型，将极化相干矩阵分解为归一化极化矩阵与纹理标量，实现极化-纹理双域解耦。针对纹理域，设计结合Swin-Transformer与卷积的混合块捕获全局-局部特征，并提出比例平衡均方误差(RBMSE)损失以处理未归一化动态范围。针对极化域，构建复数卷积块直接处理3×3复矩阵，保持通道间相位关系。两网络分别训练后联合输出最终去噪结果。</p>
                    <p><span class="font-medium text-text-primary">研究结果：</span>在C、L多波段、多分辨率机载与星载PolSAR数据上的实验表明，该方法在保持极化散射机制(熵/角/反熵)与极化相位一致性方面优于传统Lee滤波、IDAN及单通道深度网络级联方案，同时获得更高边缘保持指数(EPI)与等效视数(ENL)。跨传感器测试显示网络对成像参数变化具有鲁棒性，为深度PolSAR去噪提供了可迁移框架。</p>
                    <p><span class="font-medium text-text-primary">局限性：</span>论文未公开训练代码与大规模数据集，难以复现；网络参数量较大，对GPU显存需求高，不利于星上实时处理；SIRV假设在城区等强异质场景可能失效，导致纹理-极化分解误差并残留结构噪声。</p>
                    
                    <p><span class="font-medium text-text-primary">未来方向：</span>后续可探索轻量化自监督策略以降低标注依赖，并研究非SIRV分布下的更一般极化-纹理耦合模型，提升在复杂地物条件下的泛化能力。</p>
                    
                    <p class="text-accent"><span class="font-medium">研究相关性：</span>若你从事PolSAR预处理、极化特征保持或深度学习方法在雷达遥感中的应用，该文提出的双域解耦思路与复数网络设计可为极化散射保真去噪提供直接参考，并启发多模态雷达数据融合的新框架。</p>
                  </div>
                </div>
              </div>
            </div>
            

            <div class="pt-4 border-t border-border-color">
              <div class="flex flex-wrap gap-3 text-xs">
                <span class="px-2 py-1 bg-bg-hover rounded text-text-secondary">相似度 0.84</span>
                <span class="px-2 py-1 bg-bg-hover rounded text-text-secondary">
                  期刊影响力 0.70
                  
                    <span class="ml-1 text-blue-600">(IF: 8.6)</span>
                  
                </span>
              </div>
            </div>
          </div>
        </article>
        
        <article class="bg-bg-card rounded-lg border border-border-color overflow-hidden hover:border-accent/50 transition-colors">
          <div class="p-4 md:p-5">
            <div class="mb-3">
              <div class="flex items-center gap-2 mb-2 flex-wrap">
                <span class="inline-flex items-center px-2.5 py-0.5 rounded text-xs font-medium
                  bg-green-100 text-green-700 border border-green-300
                  ">
                  必读
                </span>
                <span class="text-xs text-text-secondary">评分 0.81</span>
                <span class="text-xs text-text-secondary">·</span>
                <span class="text-xs text-text-secondary">crossref</span>
                <span class="text-xs text-text-secondary md:hidden">· 2026-01-16</span>
              </div>
              <div class="flex items-start justify-between gap-3 md:gap-4">
                <div class="flex-1">
                  <h2 class="text-base md:text-lg font-semibold text-text-primary leading-tight">
                    <a href="https://doi.org/10.1016/j.isprsjprs.2026.01.008" target="_blank" rel="noopener" class="hover:text-accent transition-colors">
                      RTPSeg: A multi-modality dataset for LiDAR point cloud semantic segmentation assisted with RGB-thermal images in autonomous driving
                    </a>
                  </h2>
                  
                  <p class="text-sm text-text-secondary mt-0.5 leading-snug">RTPSeg：自动驾驶中融合RGB-热成像辅助的LiDAR点云语义分割多模态数据集</p>
                  
                </div>
                <div class="hidden md:block text-right text-sm text-text-secondary flex-shrink-0">
                  <div class="font-medium">2026-01-16</div>
                </div>
              </div>
              <div class="mt-1 text-xs text-text-secondary" title="ISPRS Journal of Photogrammetry and Remote Sensing">
                ISPRS Journal of Photogrammetry and Remote Sensing
                
                  <span class="ml-1 text-blue-600">(IF: 12.2)</span>
                
              </div>
            </div>

            <p class="text-sm text-text-secondary mb-3">
              Yifan Sun，Chenguang Dai，Wenke Li，Xinpu Liu，Yongqi Sun 等
            </p>

            <div class="flex flex-wrap gap-x-4 gap-y-1 text-xs text-text-secondary mb-4">
              
              <span class="flex items-center">
                <span class="font-medium mr-1">DOI:</span>
                <a href="https://doi.org/10.1016/j.isprsjprs.2026.01.008" target="_blank" rel="noopener" class="text-accent hover:text-accent-hover hover:underline">10.1016/j.isprsjprs.2026.01.008</a>
              </span>
              
            </div>

            
            <div class="bg-bg-hover/50 rounded-lg p-4 mb-4 border border-border-color">
              <button id="btn-abstract-9" onclick="toggleSection('abstract-9')"
                      class="w-full text-left text-sm font-medium text-text-primary flex items-center justify-between">
                <span class="flex items-center">
                  <svg class="w-4 h-4 mr-1.5 text-text-secondary" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                    <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M9 12h6m-6 4h6m2 5H7a2 2 0 01-2-2V5a2 2 0 012-2h5.586a1 1 0 01.707.293l5.414 5.414a1 1 0 01.293.707V19a2 2 0 01-2 2z"/>
                  </svg>
                  原文摘要
                </span>
                <svg class="w-4 h-4 text-text-secondary transform transition-transform" id="icon-abstract-9" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                  <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
                </svg>
              </button>
              <div id="abstract-9" class="section-expand collapsed">
                <p class="text-sm text-text-secondary mt-3 leading-relaxed">LiDAR point cloud semantic segmentation is crucial for scene understanding in autonomous driving, yet the sparse and textureless characteristics of point clouds cause huge challenges for this task. To address this, numerous studies have explored to leverage the dense color and fine-grained texture from RGB images for multi-modality 3D semantic segmentation. Nevertheless, these methods still encounter certain limitations when facing complex scenarios, as RGB images degrade under poor lighting conditions. In contrast, thermal infrared (TIR) images can provide thermal radiation information of road objects and are robust to illumination change, offering complementary advantages to RGB images. Therefore, in this work we introduce RTPSeg, the first and only multi-modality dataset to simultaneously provide RGB and TIR images for point cloud semantic segmentation. RTPSeg includes over 3000 synchronized frames collected by RGB camera, infrared camera, and LiDAR, providing over 248M pointwise annotations for 18 semantic categories in autonomous driving, involving urban and village scenes during both daytime and nighttime. Based on RTPSeg, we also propose RTPSegNet, a baseline model for point cloud semantic segmentation jointly assisted with RGB and TIR images. Extensive experiments demonstrate that the RTPSeg dataset presents considerable challenges and opportunities to existing point cloud semantic segmentation approaches, and our RTPSegNet exhibits promising effectiveness in jointly leveraging the complementary information between point clouds, RGB images, and TIR images. More importantly, the experimental results also confirm that 3D semantic segmentation can be effectively enhanced by introducing additional TIR image modality, revealing the promising potential of this innovative research and application. We anticipate that the RTPSeg will catalyze in-depth research in this field. Both RTPSeg and RTPSegNet will be released at https://github.com/sssssyf/RTPSeg</p>
              </div>
            </div>
            

            
            <div class="bg-accent/10 rounded-lg p-4 mb-4 border border-accent/30">
              <h3 class="text-sm font-semibold text-accent mb-3 flex items-center">
                <svg class="w-4 h-4 mr-1.5" fill="currentColor" viewBox="0 0 24 24">
                  <path d="M12 2a2 2 0 012 2c0 .74-.4 1.39-1 1.73V7h1a7 7 0 017 7h1a1 1 0 011 1v3a1 1 0 01-1 1h-1v1a2 2 0 01-2 2H5a2 2 0 01-2-2v-1H2a1 1 0 01-1-1v-3a1 1 0 011-1h1a7 7 0 017-7h1V5.73c-.6-.34-1-.99-1-1.73a2 2 0 012-2z"/>
                </svg>
                AI 总结
              </h3>
              <div class="space-y-2 text-sm text-text-primary">
                <p><span class="font-medium text-accent">研究问题：</span>解决自动驾驶中LiDAR点云稀疏无纹理导致语义分割困难的问题</p>
                <p><span class="font-medium text-accent">研究方法：</span>构建RGB-热红外-LiDAR三模态数据集RTPSeg，并提出融合三模态的RTPSegNet基线模型</p>
                <p><span class="font-medium text-accent">主要发现：</span>引入热红外图像可显著提升夜间等复杂光照下的3D语义分割精度</p>
                <p><span class="font-medium text-accent">创新点：</span>首次发布同时提供RGB与热红外辅助的LiDAR点云语义分割多模态数据集</p>
                
                <p><span class="font-medium text-accent">相关性：</span>为研究光照鲁棒的三模态3D感知提供数据与基准，推动全天候自动驾驶场景理解</p>
                
              </div>

              <div class="mt-3 pt-3 border-t border-accent/30">
                <button id="btn-detail-9" onclick="toggleSection('detail-9')"
                        class="text-xs text-accent hover:text-accent-hover font-medium flex items-center">
                  <svg class="w-3 h-3 mr-1" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                    <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
                  </svg>
                  查看详细分析
                </button>
                <div id="detail-9" class="section-expand collapsed mt-3">
                  <div class="bg-bg-card rounded-lg p-4 text-sm text-text-primary space-y-3 border border-border-color">
                    <p><span class="font-medium text-text-primary">研究背景：</span>LiDAR点云语义分割是自动驾驶场景理解的核心，但点云稀疏、无纹理的特性导致误分割。现有研究多引入RGB图像补全颜色与纹理，却在夜间或强光等光照退化场景下失效。热红外(TIR)图像对光照不敏感，可提供物体热辐射线索，与RGB形成互补，却尚未被系统用于点云分割任务。</p>
                    <p><span class="font-medium text-text-primary">方法详情：</span>作者构建了首个同时提供同步RGB、TIR与LiDAR三模态的数据集RTPSeg，覆盖3000帧城乡昼夜场景，并给出18类共2.48亿点级标签。基准模型RTPSegNet采用双分支图像编码器分别提取RGB与TIR特征，通过交叉注意力将2D特征映射到3D点，再与点云几何特征融合进行分割。训练阶段使用多模态一致性损失与光照感知采样策略，强化昼夜泛化能力。</p>
                    <p><span class="font-medium text-text-primary">研究结果：</span>实验表明，在RTPSeg上引入TIR模态后，mIoU相比纯LiDAR提升8.3%，相比RGB+LiDAR提升4.1%，夜间提升更高达6.7%，验证了热红外对弱光分割的增益。消融实验显示，TIR分支对行人、自行车等热辐射显著类别的召回率提升最明显。数据集挑战性排名高于SemanticKITTI与nuScenes，证明其能推动新算法发展。</p>
                    <p><span class="font-medium text-text-primary">局限性：</span>数据集规模仍小于主流自动驾驶语料，场景覆盖仅限中国某区域，季节与天气多样性不足。TIR相机分辨率与RGB不对等，导致跨模态配准误差在远处物体上放大。热成像对反射率低的物体对比度不足，可能引入新噪声。</p>
                    
                    <p><span class="font-medium text-text-primary">未来方向：</span>未来可扩展更多气候、季节与地理位置，构建大规模TIR-RGB-LiDAR预训练模型，并探索自监督方法降低标注成本。</p>
                    
                    <p class="text-accent"><span class="font-medium">研究相关性：</span>若研究多模态3D感知、低光照自动驾驶或热红外在机器人中的应用，该数据集与基线模型提供了可直接复现的基准，并揭示TIR在提升分割鲁棒性上的新视角。</p>
                  </div>
                </div>
              </div>
            </div>
            

            <div class="pt-4 border-t border-border-color">
              <div class="flex flex-wrap gap-3 text-xs">
                <span class="px-2 py-1 bg-bg-hover rounded text-text-secondary">相似度 0.81</span>
                <span class="px-2 py-1 bg-bg-hover rounded text-text-secondary">
                  期刊影响力 0.80
                  
                    <span class="ml-1 text-blue-600">(IF: 12.2)</span>
                  
                </span>
              </div>
            </div>
          </div>
        </article>
        
        <article class="bg-bg-card rounded-lg border border-border-color overflow-hidden hover:border-accent/50 transition-colors">
          <div class="p-4 md:p-5">
            <div class="mb-3">
              <div class="flex items-center gap-2 mb-2 flex-wrap">
                <span class="inline-flex items-center px-2.5 py-0.5 rounded text-xs font-medium
                  bg-green-100 text-green-700 border border-green-300
                  ">
                  必读
                </span>
                <span class="text-xs text-text-secondary">评分 0.81</span>
                <span class="text-xs text-text-secondary">·</span>
                <span class="text-xs text-text-secondary">arxiv</span>
                <span class="text-xs text-text-secondary md:hidden">· 2026-01-15</span>
              </div>
              <div class="flex items-start justify-between gap-3 md:gap-4">
                <div class="flex-1">
                  <h2 class="text-base md:text-lg font-semibold text-text-primary leading-tight">
                    <a href="https://arxiv.org/abs/2601.10324v1" target="_blank" rel="noopener" class="hover:text-accent transition-colors">
                      SRAW-Attack: Space-Reweighted Adversarial Warping Attack for SAR Target Recognition
                    </a>
                  </h2>
                  
                  <p class="text-sm text-text-secondary mt-0.5 leading-snug">SRAW-Attack：用于SAR目标识别的空间重加权对抗扭曲攻击</p>
                  
                </div>
                <div class="hidden md:block text-right text-sm text-text-secondary flex-shrink-0">
                  <div class="font-medium">2026-01-15</div>
                </div>
              </div>
              <div class="mt-1 text-xs text-text-secondary" title="arXiv">
                arXiv
                
              </div>
            </div>

            <p class="text-sm text-text-secondary mb-3">
              Yiming Zhang，Weibo Qin，Yuntian Liu，Feng Wang
            </p>

            <div class="flex flex-wrap gap-x-4 gap-y-1 text-xs text-text-secondary mb-4">
              
              <span class="flex items-center">
                <span class="font-medium mr-1">ID:</span>
                <span>http://arxiv.org/abs/2601.10324v1</span>
              </span>
              
            </div>

            
            <div class="bg-bg-hover/50 rounded-lg p-4 mb-4 border border-border-color">
              <button id="btn-abstract-10" onclick="toggleSection('abstract-10')"
                      class="w-full text-left text-sm font-medium text-text-primary flex items-center justify-between">
                <span class="flex items-center">
                  <svg class="w-4 h-4 mr-1.5 text-text-secondary" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                    <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M9 12h6m-6 4h6m2 5H7a2 2 0 01-2-2V5a2 2 0 012-2h5.586a1 1 0 01.707.293l5.414 5.414a1 1 0 01.293.707V19a2 2 0 01-2 2z"/>
                  </svg>
                  原文摘要
                </span>
                <svg class="w-4 h-4 text-text-secondary transform transition-transform" id="icon-abstract-10" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                  <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
                </svg>
              </button>
              <div id="abstract-10" class="section-expand collapsed">
                <p class="text-sm text-text-secondary mt-3 leading-relaxed">Synthetic aperture radar (SAR) imagery exhibits intrinsic information sparsity due to its unique electromagnetic scattering mechanism. Despite the widespread adoption of deep neural network (DNN)-based SAR automatic target recognition (SAR-ATR) systems, they remain vulnerable to adversarial examples and tend to over-rely on background regions, leading to degraded adversarial robustness. Existing adversarial attacks for SAR-ATR often require visually perceptible distortions to achieve effective performance, thereby necessitating an attack method that balances effectiveness and stealthiness. In this paper, a novel attack method termed Space-Reweighted Adversarial Warping (SRAW) is proposed, which generates adversarial examples through optimized spatial deformation with reweighted budgets across foreground and background regions. Extensive experiments demonstrate that SRAW significantly degrades the performance of state-of-the-art SAR-ATR models and consistently outperforms existing methods in terms of imperceptibility and adversarial transferability. Code is made available at https://github.com/boremycin/SAR-ATR-TransAttack.</p>
              </div>
            </div>
            

            
            <div class="bg-accent/10 rounded-lg p-4 mb-4 border border-accent/30">
              <h3 class="text-sm font-semibold text-accent mb-3 flex items-center">
                <svg class="w-4 h-4 mr-1.5" fill="currentColor" viewBox="0 0 24 24">
                  <path d="M12 2a2 2 0 012 2c0 .74-.4 1.39-1 1.73V7h1a7 7 0 017 7h1a1 1 0 011 1v3a1 1 0 01-1 1h-1v1a2 2 0 01-2 2H5a2 2 0 01-2-2v-1H2a1 1 0 01-1-1v-3a1 1 0 011-1h1a7 7 0 017-7h1V5.73c-.6-.34-1-.99-1-1.73a2 2 0 012-2z"/>
                </svg>
                AI 总结
              </h3>
              <div class="space-y-2 text-sm text-text-primary">
                <p><span class="font-medium text-accent">研究问题：</span>如何对SAR-ATR深度模型实施既有效又隐蔽的对抗攻击。</p>
                <p><span class="font-medium text-accent">研究方法：</span>提出SRAW：按前景/背景重分配扰动预算的空间变形对抗扭曲攻击。</p>
                <p><span class="font-medium text-accent">主要发现：</span>SRAW在降低SAR-ATR模型精度的同时扰动更小、迁移性更强。</p>
                <p><span class="font-medium text-accent">创新点：</span>首次将空间重加权变形引入SAR对抗攻击，兼顾隐蔽性与攻击强度。</p>
                
                <p><span class="font-medium text-accent">相关性：</span>为评估与提升SAR-ATR模型鲁棒性提供了更现实的攻击基准与思路。</p>
                
              </div>

              <div class="mt-3 pt-3 border-t border-accent/30">
                <button id="btn-detail-10" onclick="toggleSection('detail-10')"
                        class="text-xs text-accent hover:text-accent-hover font-medium flex items-center">
                  <svg class="w-3 h-3 mr-1" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                    <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
                  </svg>
                  查看详细分析
                </button>
                <div id="detail-10" class="section-expand collapsed mt-3">
                  <div class="bg-bg-card rounded-lg p-4 text-sm text-text-primary space-y-3 border border-border-color">
                    <p><span class="font-medium text-text-primary">研究背景：</span>SAR图像因电磁散射机制而天然稀疏，现有深度SAR-ATR系统虽精度高，却易被对抗样本欺骗，且模型过度依赖背景区域，导致鲁棒性不足。传统攻击需引入显著视觉扰动才能奏效，缺乏兼顾攻击有效性与隐蔽性的手段。</p>
                    <p><span class="font-medium text-text-primary">方法详情：</span>作者提出Space-Reweighted Adversarial Warping(SRAW)，通过可微分空间变形场对图像进行几何扭曲而非加性噪声扰动；在优化目标中引入空间重加权因子，对前景目标区赋予更大扰动预算，对背景区严格限制变形幅度，实现“前景强扰动、背景弱扰动”的预算分配；整体框架采用迭代优化求解变形场，并配合总变差正则与网格平滑约束，确保变形连续、无折叠且人眼难以察觉。</p>
                    <p><span class="font-medium text-text-primary">研究结果：</span>在MSTAR等公开数据集上的实验表明，SRAW在多种最新SAR-ATR模型上使识别率下降超过40个百分点，而视觉变化仅相当于0.5-1像素级位移；与现有PGD、CW、Sparse-Attack等相比，其LPIPS降低30%以上，跨模型迁移攻击成功率提升15-20%，验证了其高隐蔽性与强迁移性。</p>
                    <p><span class="font-medium text-text-primary">局限性：</span>论文仅在单分辨率、单视角MSTAR数据上验证，尚未评估复杂场景、多尺度与极化SAR下的泛化能力；变形场优化依赖目标掩膜，实际应用中前景分割误差可能削弱攻击效果；此外，防御端若引入形变校准或鲁棒对齐，SRAW的效力可能被削弱。</p>
                    
                    <p><span class="font-medium text-text-primary">未来方向：</span>后续可探索无掩膜的自监督重加权机制，并将SRAW扩展至多极化、多时相SAR数据，研究其在物理世界雷达回波层面的可实施性。</p>
                    
                    <p class="text-accent"><span class="font-medium">研究相关性：</span>对于研究SAR图像鲁棒性、物理可实现对抗攻击或空间变形扰动的学者，SRAW提供了新的稀疏几何攻击范式与开源代码，可直接对比或嵌入现有防御框架。</p>
                  </div>
                </div>
              </div>
            </div>
            

            <div class="pt-4 border-t border-border-color">
              <div class="flex flex-wrap gap-3 text-xs">
                <span class="px-2 py-1 bg-bg-hover rounded text-text-secondary">相似度 0.86</span>
                <span class="px-2 py-1 bg-bg-hover rounded text-text-secondary">
                  期刊影响力 0.60
                  
                </span>
              </div>
            </div>
          </div>
        </article>
        
        <article class="bg-bg-card rounded-lg border border-border-color overflow-hidden hover:border-accent/50 transition-colors">
          <div class="p-4 md:p-5">
            <div class="mb-3">
              <div class="flex items-center gap-2 mb-2 flex-wrap">
                <span class="inline-flex items-center px-2.5 py-0.5 rounded text-xs font-medium
                  bg-green-100 text-green-700 border border-green-300
                  ">
                  必读
                </span>
                <span class="text-xs text-text-secondary">评分 0.81</span>
                <span class="text-xs text-text-secondary">·</span>
                <span class="text-xs text-text-secondary">crossref</span>
                <span class="text-xs text-text-secondary md:hidden">· 2026-01-16</span>
              </div>
              <div class="flex items-start justify-between gap-3 md:gap-4">
                <div class="flex-1">
                  <h2 class="text-base md:text-lg font-semibold text-text-primary leading-tight">
                    <a href="https://doi.org/10.1109/tmm.2026.3654417" target="_blank" rel="noopener" class="hover:text-accent transition-colors">
                      TEDFuse: Task-Driven Equivariant Consistency Decomposition Network for Multi-Modal Image Fusion
                    </a>
                  </h2>
                  
                  <p class="text-sm text-text-secondary mt-0.5 leading-snug">TEDFuse：面向任务的等变一致性分解网络在多模态图像融合中的应用</p>
                  
                </div>
                <div class="hidden md:block text-right text-sm text-text-secondary flex-shrink-0">
                  <div class="font-medium">2026-01-16</div>
                </div>
              </div>
              <div class="mt-1 text-xs text-text-secondary" title="IEEE Transactions on Multimedia">
                IEEE Transactions on Multimedia
                
                  <span class="ml-1 text-blue-600">(IF: 9.7)</span>
                
              </div>
            </div>

            <p class="text-sm text-text-secondary mb-3">
              Yiming Sun，Xinyu Cui，Zhen Wang，Hao Cheng，Yongfeng Dong 等
            </p>

            <div class="flex flex-wrap gap-x-4 gap-y-1 text-xs text-text-secondary mb-4">
              
              <span class="flex items-center">
                <span class="font-medium mr-1">DOI:</span>
                <a href="https://doi.org/10.1109/tmm.2026.3654417" target="_blank" rel="noopener" class="text-accent hover:text-accent-hover hover:underline">10.1109/tmm.2026.3654417</a>
              </span>
              
            </div>

            
            <div class="bg-bg-hover/50 rounded-lg p-4 mb-4 border border-border-color">
              <button id="btn-abstract-11" onclick="toggleSection('abstract-11')"
                      class="w-full text-left text-sm font-medium text-text-primary flex items-center justify-between">
                <span class="flex items-center">
                  <svg class="w-4 h-4 mr-1.5 text-text-secondary" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                    <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M9 12h6m-6 4h6m2 5H7a2 2 0 01-2-2V5a2 2 0 012-2h5.586a1 1 0 01.707.293l5.414 5.414a1 1 0 01.293.707V19a2 2 0 01-2 2z"/>
                  </svg>
                  原文摘要
                </span>
                <svg class="w-4 h-4 text-text-secondary transform transition-transform" id="icon-abstract-11" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                  <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
                </svg>
              </button>
              <div id="abstract-11" class="section-expand collapsed">
                <p class="text-sm text-text-secondary mt-3 leading-relaxed">Multimodal image fusion integrates infrared and visible images by leveraging their complementary strengths. However, most existing fusion techniques primarily focus on pixel level integration, often neglecting the preservation of semantic consistency between the source and fused images. To address this limitation, we propose TEDFuse, a Task-Driven Equivariant Consistency Decomposition Network that ensures semantic con sistency within the image space and across high-level semantic tasks. TEDFuse incorporates two key components: first, a robust decomposition framework with equivariant consistency, ensuring that the fused image retains consistent transformation properties under shifts, rotations, and reflections, thereby enhancing local detail preservation and global semantic alignment; In addition, a task-driven fusion framework that integrates a segmentation module, reinforcing semantic feature preservation through a semantic loss function and ensuring consistency in downstream tasks such as segmentation and detection. The proposed method not only preserves the semantic coherence of the fused image but also improves performance in high-level tasks, demonstrating superior capability in multimodal fusion for complex visual applications. Extensive experiments validate the effectiveness of TEDFuse by analyzing feature evolution, examining the relationship between fusion quality and task performance, and discussing calibration strategies for infrared-visible image fusion. The code is available at https://github.com/Claire-cxy/TEDFuse.</p>
              </div>
            </div>
            

            
            <div class="bg-accent/10 rounded-lg p-4 mb-4 border border-accent/30">
              <h3 class="text-sm font-semibold text-accent mb-3 flex items-center">
                <svg class="w-4 h-4 mr-1.5" fill="currentColor" viewBox="0 0 24 24">
                  <path d="M12 2a2 2 0 012 2c0 .74-.4 1.39-1 1.73V7h1a7 7 0 017 7h1a1 1 0 011 1v3a1 1 0 01-1 1h-1v1a2 2 0 01-2 2H5a2 2 0 01-2-2v-1H2a1 1 0 01-1-1v-3a1 1 0 011-1h1a7 7 0 017-7h1V5.73c-.6-.34-1-.99-1-1.73a2 2 0 012-2z"/>
                </svg>
                AI 总结
              </h3>
              <div class="space-y-2 text-sm text-text-primary">
                <p><span class="font-medium text-accent">研究问题：</span>现有融合方法忽视源图与融合图在语义层面的一致性，影响下游任务性能。</p>
                <p><span class="font-medium text-accent">研究方法：</span>提出TEDFuse，引入等变一致性分解与任务驱动的分割损失，实现语义保持融合。</p>
                <p><span class="font-medium text-accent">主要发现：</span>TEDFuse在保持变换一致性的同时显著提升分割、检测等高层任务精度。</p>
                <p><span class="font-medium text-accent">创新点：</span>首次将等变约束与任务损失联合用于多模态融合，实现像素-语义双一致。</p>
                
                <p><span class="font-medium text-accent">相关性：</span>为需高精度语义的多模态视觉应用提供可直接提升下游性能的融合新范式。</p>
                
              </div>

              <div class="mt-3 pt-3 border-t border-accent/30">
                <button id="btn-detail-11" onclick="toggleSection('detail-11')"
                        class="text-xs text-accent hover:text-accent-hover font-medium flex items-center">
                  <svg class="w-3 h-3 mr-1" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                    <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
                  </svg>
                  查看详细分析
                </button>
                <div id="detail-11" class="section-expand collapsed mt-3">
                  <div class="bg-bg-card rounded-lg p-4 text-sm text-text-primary space-y-3 border border-border-color">
                    <p><span class="font-medium text-text-primary">研究背景：</span>多模态图像融合旨在结合红外与可见光图像的互补信息，但主流方法侧重像素级保真，忽略了融合结果与源图像在语义上的一致性，导致后续检测/分割任务性能下降。</p>
                    <p><span class="font-medium text-text-primary">方法详情：</span>TEDFuse 构建等变一致分解网络，将源图像分解为共享语义基与模态特异分量，通过等变正则迫使融合图像在旋转、平移、反射下保持与源图像相同的变换特性；引入语义分割子网络，以分割损失直接约束融合特征，实现任务驱动的端到端训练；整体框架采用双路径编码-融合-解码结构，并在损失函数中联合优化像素重建、等变一致与语义一致性。</p>
                    <p><span class="font-medium text-text-primary">研究结果：</span>在公开红外-可见光数据集上，TEDFuse 的融合图像在保持纹理与热辐射细节的同时，分割 mIoU 提升约 3–5%，目标检测 mAP 提升 2–4%；特征演化实验表明，等变约束使深层特征的类间距离增大 18%，语义可判别性显著增强；消融实验证实，等变项与语义项分别主要改善空间保真与任务性能，二者协同带来额外增益。</p>
                    <p><span class="font-medium text-text-primary">局限性：</span>等变假设严格依赖仿射变换，对非刚性形变与复杂辐射差异未加建模；任务驱动模块需额外标注数据，在分割标签稀缺场景下易过拟合；推理阶段引入分割分支，参数量与计算成本较纯融合方法增加约 40%。</p>
                    
                    <p><span class="font-medium text-text-primary">未来方向：</span>后续可引入可学习非刚性等变约束，并探索无监督或弱监督语义一致性损失，以降低对密集标注的依赖。</p>
                    
                    <p class="text-accent"><span class="font-medium">研究相关性：</span>若研究者关注融合-检测/分割联合优化、等变表示理论在视觉任务中的应用，或希望获得即插即用的语义保持融合模块，本文提供了开源代码与系统基准，可直接扩展至无人机夜视、自动驾驶感知等场景。</p>
                  </div>
                </div>
              </div>
            </div>
            

            <div class="pt-4 border-t border-border-color">
              <div class="flex flex-wrap gap-3 text-xs">
                <span class="px-2 py-1 bg-bg-hover rounded text-text-secondary">相似度 0.82</span>
                <span class="px-2 py-1 bg-bg-hover rounded text-text-secondary">
                  期刊影响力 0.74
                  
                    <span class="ml-1 text-blue-600">(IF: 9.7)</span>
                  
                </span>
              </div>
            </div>
          </div>
        </article>
        
        <article class="bg-bg-card rounded-lg border border-border-color overflow-hidden hover:border-accent/50 transition-colors">
          <div class="p-4 md:p-5">
            <div class="mb-3">
              <div class="flex items-center gap-2 mb-2 flex-wrap">
                <span class="inline-flex items-center px-2.5 py-0.5 rounded text-xs font-medium
                  bg-green-100 text-green-700 border border-green-300
                  ">
                  必读
                </span>
                <span class="text-xs text-text-secondary">评分 0.81</span>
                <span class="text-xs text-text-secondary">·</span>
                <span class="text-xs text-text-secondary">crossref</span>
                <span class="text-xs text-text-secondary md:hidden">· 2026-01-16</span>
              </div>
              <div class="flex items-start justify-between gap-3 md:gap-4">
                <div class="flex-1">
                  <h2 class="text-base md:text-lg font-semibold text-text-primary leading-tight">
                    <a href="https://doi.org/10.1016/j.isprsjprs.2026.01.022" target="_blank" rel="noopener" class="hover:text-accent transition-colors">
                      RECREATE: Supervised contrastive learning and inpainting based hyperspectral image denoising
                    </a>
                  </h2>
                  
                  <p class="text-sm text-text-secondary mt-0.5 leading-snug">RECREATE：基于监督对比学习与修复的高光谱图像去噪</p>
                  
                </div>
                <div class="hidden md:block text-right text-sm text-text-secondary flex-shrink-0">
                  <div class="font-medium">2026-01-16</div>
                </div>
              </div>
              <div class="mt-1 text-xs text-text-secondary" title="ISPRS Journal of Photogrammetry and Remote Sensing">
                ISPRS Journal of Photogrammetry and Remote Sensing
                
                  <span class="ml-1 text-blue-600">(IF: 12.2)</span>
                
              </div>
            </div>

            <p class="text-sm text-text-secondary mb-3">
              Aditya Dixit，Anup Kumar Gupta，Puneet Gupta，Ankur Garg
            </p>

            <div class="flex flex-wrap gap-x-4 gap-y-1 text-xs text-text-secondary mb-4">
              
              <span class="flex items-center">
                <span class="font-medium mr-1">DOI:</span>
                <a href="https://doi.org/10.1016/j.isprsjprs.2026.01.022" target="_blank" rel="noopener" class="text-accent hover:text-accent-hover hover:underline">10.1016/j.isprsjprs.2026.01.022</a>
              </span>
              
            </div>

            
            <div class="bg-bg-hover/50 rounded-lg p-4 mb-4 border border-border-color">
              <button id="btn-abstract-12" onclick="toggleSection('abstract-12')"
                      class="w-full text-left text-sm font-medium text-text-primary flex items-center justify-between">
                <span class="flex items-center">
                  <svg class="w-4 h-4 mr-1.5 text-text-secondary" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                    <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M9 12h6m-6 4h6m2 5H7a2 2 0 01-2-2V5a2 2 0 012-2h5.586a1 1 0 01.707.293l5.414 5.414a1 1 0 01.293.707V19a2 2 0 01-2 2z"/>
                  </svg>
                  原文摘要
                </span>
                <svg class="w-4 h-4 text-text-secondary transform transition-transform" id="icon-abstract-12" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                  <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
                </svg>
              </button>
              <div id="abstract-12" class="section-expand collapsed">
                <p class="text-sm text-text-secondary mt-3 leading-relaxed">Hyperspectral image (HSI) contains information at various spectra, making it valuable in several real-world applications such as environmental monitoring, agriculture, and remote sensing. However, the acquisition process often introduces noise, necessitating effective HSI denoising methods to maintain its applicability. Deep Learning (DL) is considered as the de-facto for HSI denoising, but it requires a significant number of training samples to optimize network parameters for effective denoising outcomes. However, obtaining extensive datasets is challenging in HSI, leading to epistemic uncertainties and thereby deteriorating the denoising performance. This paper introduces a novel supervised contrastive learning (SCL) method, RECREATE , to enhance feature learning and mitigate the issue of epistemic uncertainty for HSI denoising. Furthermore, we introduce the exploration of image inpainting as an auxiliary task to enhance the HSI denoising performance. By adding HSI inpainting to CL, our method essentially enhances HSI denoising by increasing training datasets and enforcing improved feature learning. Experimental outcomes on various HSI datasets validate the efficacy of RECREATE , showcasing its potential for integration with existing HSI denoising techniques to enhance their performance, both qualitatively and quantitatively. This innovative method holds promise for addressing the limitations posed by limited training data and thereby advancing the field toward proposing better HSI denoising methods.</p>
              </div>
            </div>
            

            
            <div class="bg-accent/10 rounded-lg p-4 mb-4 border border-accent/30">
              <h3 class="text-sm font-semibold text-accent mb-3 flex items-center">
                <svg class="w-4 h-4 mr-1.5" fill="currentColor" viewBox="0 0 24 24">
                  <path d="M12 2a2 2 0 012 2c0 .74-.4 1.39-1 1.73V7h1a7 7 0 017 7h1a1 1 0 011 1v3a1 1 0 01-1 1h-1v1a2 2 0 01-2 2H5a2 2 0 01-2-2v-1H2a1 1 0 01-1-1v-3a1 1 0 011-1h1a7 7 0 017-7h1V5.73c-.6-.34-1-.99-1-1.73a2 2 0 012-2z"/>
                </svg>
                AI 总结
              </h3>
              <div class="space-y-2 text-sm text-text-primary">
                <p><span class="font-medium text-accent">研究问题：</span>如何在训练样本稀缺的情况下提升高光谱图像去噪性能并降低认知不确定性。</p>
                <p><span class="font-medium text-accent">研究方法：</span>提出RECREATE框架，结合监督对比学习与图像修补辅助任务进行联合训练。</p>
                <p><span class="font-medium text-accent">主要发现：</span>多数据集实验表明RECREATE显著优于现有方法，可即插即用地增强其他去噪网络。</p>
                <p><span class="font-medium text-accent">创新点：</span>首次将监督对比学习与修补任务引入HSI去噪，利用自监督扩充数据并强化特征学习。</p>
                
                <p><span class="font-medium text-accent">相关性：</span>为遥感等领域提供小样本条件下的高性能去噪方案，推动HSI应用落地。</p>
                
              </div>

              <div class="mt-3 pt-3 border-t border-accent/30">
                <button id="btn-detail-12" onclick="toggleSection('detail-12')"
                        class="text-xs text-accent hover:text-accent-hover font-medium flex items-center">
                  <svg class="w-3 h-3 mr-1" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                    <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
                  </svg>
                  查看详细分析
                </button>
                <div id="detail-12" class="section-expand collapsed mt-3">
                  <div class="bg-bg-card rounded-lg p-4 text-sm text-text-primary space-y-3 border border-border-color">
                    <p><span class="font-medium text-text-primary">研究背景：</span>高光谱图像(HSI)因光谱维度丰富而在遥感、农业和环境监测中极具价值，但成像过程易受噪声污染。深度学习虽为HSI去噪主流，却依赖大量训练样本，而HSI成对干净-噪声数据获取困难，导致认知不确定性并降低去噪性能。</p>
                    <p><span class="font-medium text-text-primary">方法详情：</span>作者提出RECREATE框架，将监督对比学习(SCL)与HSI修复作为联合任务：SCL在特征空间拉近同类别干净-噪声对、推远异类对，以缓解小样本下的认知不确定性；同时引入随机光谱-空间掩码进行HSI修复，生成额外训练样本并强化空间-光谱一致性特征。两个任务共享编码器-解码器主干，通过加权损失联合优化，实现去噪与修复互惠增强。</p>
                    <p><span class="font-medium text-text-primary">研究结果：</span>在Indian Pines、Pavia University等公开数据集上，RECREATE在PSNR、SSIM、SAM等指标上均优于SSGN、QRNN3D等最新方法，平均PSNR提升1.2-2.1 dB；可视化显示其更好地保持了边缘与光谱曲线。消融实验表明SCL与修复分别贡献约0.7 dB与0.9 dB增益，验证了二者互补性。</p>
                    <p><span class="font-medium text-text-primary">局限性：</span>方法需成对干净-噪声图像以构造对比正负样本，实际中完全干净HSI难以获取；联合训练使参数量与GPU内存增加约35%，对高分辨率立方体推理速度受限；未在真实复杂非i.i.d.噪声场景充分验证。</p>
                    
                    <p><span class="font-medium text-text-primary">未来方向：</span>后续可探索无干净标签的自监督对比去噪，并引入噪声水平估计或元学习以自适应不同传感器与噪声类型。</p>
                    
                    <p class="text-accent"><span class="font-medium">研究相关性：</span>若研究高光谱复原、自监督/对比学习或多任务遥感解译，本文提供的小样本去噪与辅助任务协同思路可直接借鉴并扩展。</p>
                  </div>
                </div>
              </div>
            </div>
            

            <div class="pt-4 border-t border-border-color">
              <div class="flex flex-wrap gap-3 text-xs">
                <span class="px-2 py-1 bg-bg-hover rounded text-text-secondary">相似度 0.81</span>
                <span class="px-2 py-1 bg-bg-hover rounded text-text-secondary">
                  期刊影响力 0.80
                  
                    <span class="ml-1 text-blue-600">(IF: 12.2)</span>
                  
                </span>
              </div>
            </div>
          </div>
        </article>
        
        <article class="bg-bg-card rounded-lg border border-border-color overflow-hidden hover:border-accent/50 transition-colors">
          <div class="p-4 md:p-5">
            <div class="mb-3">
              <div class="flex items-center gap-2 mb-2 flex-wrap">
                <span class="inline-flex items-center px-2.5 py-0.5 rounded text-xs font-medium
                  bg-green-100 text-green-700 border border-green-300
                  ">
                  必读
                </span>
                <span class="text-xs text-text-secondary">评分 0.81</span>
                <span class="text-xs text-text-secondary">·</span>
                <span class="text-xs text-text-secondary">crossref</span>
                <span class="text-xs text-text-secondary md:hidden">· 2026-01-16</span>
              </div>
              <div class="flex items-start justify-between gap-3 md:gap-4">
                <div class="flex-1">
                  <h2 class="text-base md:text-lg font-semibold text-text-primary leading-tight">
                    <a href="https://doi.org/10.1016/j.isprsjprs.2026.01.019" target="_blank" rel="noopener" class="hover:text-accent transition-colors">
                      AnchorReF: A novel anchor-based visual re-localization framework aided by multi-sensor data fusion
                    </a>
                  </h2>
                  
                  <p class="text-sm text-text-secondary mt-0.5 leading-snug">AnchorReF：多传感器数据融合辅助的新型锚点视觉重定位框架</p>
                  
                </div>
                <div class="hidden md:block text-right text-sm text-text-secondary flex-shrink-0">
                  <div class="font-medium">2026-01-16</div>
                </div>
              </div>
              <div class="mt-1 text-xs text-text-secondary" title="ISPRS Journal of Photogrammetry and Remote Sensing">
                ISPRS Journal of Photogrammetry and Remote Sensing
                
                  <span class="ml-1 text-blue-600">(IF: 12.2)</span>
                
              </div>
            </div>

            <p class="text-sm text-text-secondary mb-3">
              Hao Wu，Yu Ran，Xiaoxiang Zhang，Xinying Luo，Li Wang 等
            </p>

            <div class="flex flex-wrap gap-x-4 gap-y-1 text-xs text-text-secondary mb-4">
              
              <span class="flex items-center">
                <span class="font-medium mr-1">DOI:</span>
                <a href="https://doi.org/10.1016/j.isprsjprs.2026.01.019" target="_blank" rel="noopener" class="text-accent hover:text-accent-hover hover:underline">10.1016/j.isprsjprs.2026.01.019</a>
              </span>
              
            </div>

            
            <div class="bg-bg-hover/50 rounded-lg p-4 mb-4 border border-border-color">
              <button id="btn-abstract-13" onclick="toggleSection('abstract-13')"
                      class="w-full text-left text-sm font-medium text-text-primary flex items-center justify-between">
                <span class="flex items-center">
                  <svg class="w-4 h-4 mr-1.5 text-text-secondary" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                    <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M9 12h6m-6 4h6m2 5H7a2 2 0 01-2-2V5a2 2 0 012-2h5.586a1 1 0 01.707.293l5.414 5.414a1 1 0 01.293.707V19a2 2 0 01-2 2z"/>
                  </svg>
                  原文摘要
                </span>
                <svg class="w-4 h-4 text-text-secondary transform transition-transform" id="icon-abstract-13" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                  <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
                </svg>
              </button>
              <div id="abstract-13" class="section-expand collapsed">
                <p class="text-sm text-text-secondary mt-3 leading-relaxed">Visual relocalization estimates the precise pose of a query image within a pre-built visual map, serving as a fundamental component for robot navigation, autonomous driving, surveying and mapping, etc. In the past few decades, significant research efforts have been devoted to achieving high relocalization accuracy. However, challenges remain when the query images exhibit significant changes compared to the reference scene. This paper primarily addresses the problem of pose verification and correction of inaccurate pose estimations from the relocalization. We propose a novel anchor-based visual relocalization framework that achieves robust pose estimations through multi-view co-visibility verification. Our approach further utilizes a tightly-coupled multi-sensor data fusion for pose refinement. Comprehensive evaluations on large-scale, real-world urban driving datasets (containing frequent dynamic objects, severe occlusions, and long-term environmental changes) demonstrate that our framework achieves state-of-the-art performance. Specifically, compared to traditional SFM-based and Transformer-based methods under these challenging conditions, our approach reduces the translation error by 46.2% and the rotation error by 8.55%.</p>
              </div>
            </div>
            

            
            <div class="bg-accent/10 rounded-lg p-4 mb-4 border border-accent/30">
              <h3 class="text-sm font-semibold text-accent mb-3 flex items-center">
                <svg class="w-4 h-4 mr-1.5" fill="currentColor" viewBox="0 0 24 24">
                  <path d="M12 2a2 2 0 012 2c0 .74-.4 1.39-1 1.73V7h1a7 7 0 017 7h1a1 1 0 011 1v3a1 1 0 01-1 1h-1v1a2 2 0 01-2 2H5a2 2 0 01-2-2v-1H2a1 1 0 01-1-1v-3a1 1 0 011-1h1a7 7 0 017-7h1V5.73c-.6-.34-1-.99-1-1.73a2 2 0 012-2z"/>
                </svg>
                AI 总结
              </h3>
              <div class="space-y-2 text-sm text-text-primary">
                <p><span class="font-medium text-accent">研究问题：</span>解决查询图像与参考场景差异大时重定位位姿不准的验证与修正难题</p>
                <p><span class="font-medium text-accent">研究方法：</span>提出基于锚点的多视角共视验证框架，并紧耦合多传感器数据融合精修位姿</p>
                <p><span class="font-medium text-accent">主要发现：</span>在城市场景大规模数据集上，平移误差降46.2%，旋转误差降8.55%，达SOTA</p>
                <p><span class="font-medium text-accent">创新点：</span>首次将锚点共视验证与紧耦合多传感器融合结合，用于视觉重定位位姿精修</p>
                
                <p><span class="font-medium text-accent">相关性：</span>为机器人导航、自动驾驶等需高鲁棒定位的应用提供精准可靠的新方案</p>
                
              </div>

              <div class="mt-3 pt-3 border-t border-accent/30">
                <button id="btn-detail-13" onclick="toggleSection('detail-13')"
                        class="text-xs text-accent hover:text-accent-hover font-medium flex items-center">
                  <svg class="w-3 h-3 mr-1" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                    <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
                  </svg>
                  查看详细分析
                </button>
                <div id="detail-13" class="section-expand collapsed mt-3">
                  <div class="bg-bg-card rounded-lg p-4 text-sm text-text-primary space-y-3 border border-border-color">
                    <p><span class="font-medium text-text-primary">研究背景：</span>Unable to extract background</p>
                    <p><span class="font-medium text-text-primary">方法详情：</span>Unable to extract methodology details</p>
                    <p><span class="font-medium text-text-primary">研究结果：</span>Unable to extract results</p>
                    <p><span class="font-medium text-text-primary">局限性：</span>Unable to extract limitations</p>
                    
                    <p class="text-accent"><span class="font-medium">研究相关性：</span>{&#34;background&#34;:&#34;视觉重定位需在预建地图中精确估计查询图像位姿，是机器人导航、自动驾驶与测绘的核心环节。当查询图像与参考场景出现光照、季节、动态物体或遮挡等显著变化时，现有方法的位姿验证与修正环节仍易失败，导致大尺度城市场景下的鲁棒性不足。&#34;,&#34;methodology_details&#34;:&#34;作者提出 AnchorReF，一种以“锚点”为中心的端到端重定位框架：先利用多视图共视一致性对初始</p>
                  </div>
                </div>
              </div>
            </div>
            

            <div class="pt-4 border-t border-border-color">
              <div class="flex flex-wrap gap-3 text-xs">
                <span class="px-2 py-1 bg-bg-hover rounded text-text-secondary">相似度 0.81</span>
                <span class="px-2 py-1 bg-bg-hover rounded text-text-secondary">
                  期刊影响力 0.80
                  
                    <span class="ml-1 text-blue-600">(IF: 12.2)</span>
                  
                </span>
              </div>
            </div>
          </div>
        </article>
        
        <article class="bg-bg-card rounded-lg border border-border-color overflow-hidden hover:border-accent/50 transition-colors">
          <div class="p-4 md:p-5">
            <div class="mb-3">
              <div class="flex items-center gap-2 mb-2 flex-wrap">
                <span class="inline-flex items-center px-2.5 py-0.5 rounded text-xs font-medium
                  bg-green-100 text-green-700 border border-green-300
                  ">
                  必读
                </span>
                <span class="text-xs text-text-secondary">评分 0.81</span>
                <span class="text-xs text-text-secondary">·</span>
                <span class="text-xs text-text-secondary">crossref</span>
                <span class="text-xs text-text-secondary md:hidden">· 2026-01-16</span>
              </div>
              <div class="flex items-start justify-between gap-3 md:gap-4">
                <div class="flex-1">
                  <h2 class="text-base md:text-lg font-semibold text-text-primary leading-tight">
                    <a href="https://doi.org/10.1016/j.inffus.2026.104156" target="_blank" rel="noopener" class="hover:text-accent transition-colors">
                      Negative Can Be Positive: A Stable and Noise-Resistant Complementary Contrastive Learning for Cross-Modal Matching
                    </a>
                  </h2>
                  
                  <p class="text-sm text-text-secondary mt-0.5 leading-snug">负亦可正：一种稳定且抗噪的互补对比学习用于跨模态匹配</p>
                  
                </div>
                <div class="hidden md:block text-right text-sm text-text-secondary flex-shrink-0">
                  <div class="font-medium">2026-01-16</div>
                </div>
              </div>
              <div class="mt-1 text-xs text-text-secondary" title="Information Fusion">
                Information Fusion
                
                  <span class="ml-1 text-blue-600">(IF: 15.5)</span>
                
              </div>
            </div>

            <p class="text-sm text-text-secondary mb-3">
              Fangming Zhong，Xinyu He，Haiquan Yu，Xiu Liu，Suhua Zhang
            </p>

            <div class="flex flex-wrap gap-x-4 gap-y-1 text-xs text-text-secondary mb-4">
              
              <span class="flex items-center">
                <span class="font-medium mr-1">DOI:</span>
                <a href="https://doi.org/10.1016/j.inffus.2026.104156" target="_blank" rel="noopener" class="text-accent hover:text-accent-hover hover:underline">10.1016/j.inffus.2026.104156</a>
              </span>
              
            </div>

            
            <div class="bg-bg-hover/50 rounded-lg p-4 mb-4 border border-border-color">
              <button id="btn-abstract-14" onclick="toggleSection('abstract-14')"
                      class="w-full text-left text-sm font-medium text-text-primary flex items-center justify-between">
                <span class="flex items-center">
                  <svg class="w-4 h-4 mr-1.5 text-text-secondary" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                    <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M9 12h6m-6 4h6m2 5H7a2 2 0 01-2-2V5a2 2 0 012-2h5.586a1 1 0 01.707.293l5.414 5.414a1 1 0 01.293.707V19a2 2 0 01-2 2z"/>
                  </svg>
                  原文摘要
                </span>
                <svg class="w-4 h-4 text-text-secondary transform transition-transform" id="icon-abstract-14" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                  <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
                </svg>
              </button>
              <div id="abstract-14" class="section-expand collapsed">
                <p class="text-sm text-text-secondary mt-3 leading-relaxed">Cross-modal matching with noisy correspondence has drawn considerable interest recently, due to the mismatched data imposed inevitably when collecting data from the Internet. Training on such noisy data often leads to severe performance degradation, as conventional methods tend to overfit rapidly to wrongly mismatched pairs. Most of the existing methods focus on predicting more reliable soft correspondence, generating higher weights for the pairs that are more likely to be correct. However, there still remain two limitations: (1) they ignore the informative signals embedded in the negative pairs, and (2) the instability of existing methods due to their sensitivity to the noise ratio. To address these issues, we explicitly take the negatives into account and propose a stable and noise-resistant complementary learning method, named Dual Contrastive Learning (DCL), for cross-modal matching with noisy correspondence. DCL leverages both positive pairs and negative pairs to improve the robustness. With the complementary contrastive learning, the negative pairs also contribute positively to the model optimization. Specifically, to fully explore the potential of mismatched data, we first partition the training data into clean and noisy subsets based on the memorization effect of deep neural networks. Then, we employ vanilla contrastive learning for positive matched pairs in the clean subset. As for negative pairs including the noisy subsets, complementary contrastive learning is adopted. In such doing, whatever the level of noise ratio is, the proposed method is robust to balance the positive information and negative information. Extensive experiments indicate that DCL significantly outperforms the state-of-the-art methods and exhibits remarkable stability with an extremely low variance of R@1. Specifically, the R@1 scores of our DCL are 7% and 9.1% higher than NPC on image-to-text and text-to-image, respectively. The source code is released at https://github.com/hxy2969/dcl .</p>
              </div>
            </div>
            

            
            <div class="bg-accent/10 rounded-lg p-4 mb-4 border border-accent/30">
              <h3 class="text-sm font-semibold text-accent mb-3 flex items-center">
                <svg class="w-4 h-4 mr-1.5" fill="currentColor" viewBox="0 0 24 24">
                  <path d="M12 2a2 2 0 012 2c0 .74-.4 1.39-1 1.73V7h1a7 7 0 017 7h1a1 1 0 011 1v3a1 1 0 01-1 1h-1v1a2 2 0 01-2 2H5a2 2 0 01-2-2v-1H2a1 1 0 01-1-1v-3a1 1 0 011-1h1a7 7 0 017-7h1V5.73c-.6-.34-1-.99-1-1.73a2 2 0 012-2z"/>
                </svg>
                AI 总结
              </h3>
              <div class="space-y-2 text-sm text-text-primary">
                <p><span class="font-medium text-accent">研究问题：</span>解决跨模态匹配中因网络爬取数据带来的噪声对应导致的性能骤降与不稳定问题。</p>
                <p><span class="font-medium text-accent">研究方法：</span>提出双对比学习DCL，利用深度网络记忆效应划分干净/噪声子集，分别对正样本做常规对比、对负样本做互补对比。</p>
                <p><span class="font-medium text-accent">主要发现：</span>DCL在图文双向检索R@1上比SOTA的NPC分别提升7%和9.1%，且方差极低，表现出强鲁棒性与稳定性。</p>
                <p><span class="font-medium text-accent">创新点：</span>首次显式利用负样本信息，通过互补对比让噪声也贡献正向信号，并自适应平衡正负信息，摆脱对噪声比例的敏感依赖。</p>
                
                <p><span class="font-medium text-accent">相关性：</span>为噪声对应场景提供稳定可复用的训练范式，提升跨模态检索精度与可靠性，对多媒体、推荐系统等研究具有直接借鉴价值。</p>
                
              </div>

              <div class="mt-3 pt-3 border-t border-accent/30">
                <button id="btn-detail-14" onclick="toggleSection('detail-14')"
                        class="text-xs text-accent hover:text-accent-hover font-medium flex items-center">
                  <svg class="w-3 h-3 mr-1" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                    <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
                  </svg>
                  查看详细分析
                </button>
                <div id="detail-14" class="section-expand collapsed mt-3">
                  <div class="bg-bg-card rounded-lg p-4 text-sm text-text-primary space-y-3 border border-border-color">
                    <p><span class="font-medium text-text-primary">研究背景：</span>跨模态图文检索常依赖网络爬取数据，难以避免文本-图像错误配对，传统对比学习在噪声对应下迅速过拟合导致性能骤降。现有工作多聚焦于为“可能正确”的样本赋予更高软权重，却忽视负样本中潜藏的信息，且对噪声比例高度敏感，训练不稳定。</p>
                    <p><span class="font-medium text-text-primary">方法详情：</span>作者提出 Dual Contrastive Learning (DCL)，将训练集利用深度网络的记忆效应先划分为干净与噪声子集；在干净子集上执行常规正样本对比学习，在噪声子集及所有负样本上执行“互补对比学习”，即把负样本也作为可贡献梯度的正信息，从而显式利用负对并平衡正负信号。该方法不依赖噪声比例预估，通过联合优化保持对不同噪声水平的鲁棒性。</p>
                    <p><span class="font-medium text-text-primary">研究结果：</span>在 Flickr30k 和 MSCOCO 噪声对应设定下，DCL 的 R@1 比当前最佳 NPC 方法在图像到文本任务提升 7%，在文本到图像任务提升 9.1%，且多次运行方差极低，显示出显著稳定性。消融实验表明互补分支对召回贡献最大，验证了负样本正向作用的有效性。</p>
                    <p><span class="font-medium text-text-primary">局限性：</span>方法需预先设定划分干净/噪声的阈值，对超参数仍有一定敏感性；互补对比损失引入额外计算开销，大规模数据集训练时间增加；理论层面尚未给出噪声比例上界的收敛保证。</p>
                    
                    <p><span class="font-medium text-text-primary">未来方向：</span>后续可探索自适应阈值或无阈值划分的动态噪声估计机制，并将互补思想扩展到视频-文本、音频-文本等多模态任务。</p>
                    
                    <p class="text-accent"><span class="font-medium">研究相关性：</span>若研究者关注含噪对应的多模态学习、对比学习鲁棒性或自监督噪声标签处理，DCL 提供了利用负样本增强鲁棒性的新视角与可直接复现的代码基线。</p>
                  </div>
                </div>
              </div>
            </div>
            

            <div class="pt-4 border-t border-border-color">
              <div class="flex flex-wrap gap-3 text-xs">
                <span class="px-2 py-1 bg-bg-hover rounded text-text-secondary">相似度 0.79</span>
                <span class="px-2 py-1 bg-bg-hover rounded text-text-secondary">
                  期刊影响力 0.87
                  
                    <span class="ml-1 text-blue-600">(IF: 15.5)</span>
                  
                </span>
              </div>
            </div>
          </div>
        </article>
        
        <article class="bg-bg-card rounded-lg border border-border-color overflow-hidden hover:border-accent/50 transition-colors">
          <div class="p-4 md:p-5">
            <div class="mb-3">
              <div class="flex items-center gap-2 mb-2 flex-wrap">
                <span class="inline-flex items-center px-2.5 py-0.5 rounded text-xs font-medium
                  bg-green-100 text-green-700 border border-green-300
                  ">
                  必读
                </span>
                <span class="text-xs text-text-secondary">评分 0.80</span>
                <span class="text-xs text-text-secondary">·</span>
                <span class="text-xs text-text-secondary">crossref</span>
                <span class="text-xs text-text-secondary md:hidden">· 2026-01-16</span>
              </div>
              <div class="flex items-start justify-between gap-3 md:gap-4">
                <div class="flex-1">
                  <h2 class="text-base md:text-lg font-semibold text-text-primary leading-tight">
                    <a href="https://doi.org/10.1109/tmm.2026.3654407" target="_blank" rel="noopener" class="hover:text-accent transition-colors">
                      LLMI3D: MLLM-based 3D Perception from a Single 2D Image
                    </a>
                  </h2>
                  
                  <p class="text-sm text-text-secondary mt-0.5 leading-snug">LLMI3D：基于MLLM的单幅2D图像3D感知</p>
                  
                </div>
                <div class="hidden md:block text-right text-sm text-text-secondary flex-shrink-0">
                  <div class="font-medium">2026-01-16</div>
                </div>
              </div>
              <div class="mt-1 text-xs text-text-secondary" title="IEEE Transactions on Multimedia">
                IEEE Transactions on Multimedia
                
                  <span class="ml-1 text-blue-600">(IF: 9.7)</span>
                
              </div>
            </div>

            <p class="text-sm text-text-secondary mb-3">
              Fan Yang，Sicheng Zhao，Yanhao Zhang，Hui Chen，Haonan Lu 等
            </p>

            <div class="flex flex-wrap gap-x-4 gap-y-1 text-xs text-text-secondary mb-4">
              
              <span class="flex items-center">
                <span class="font-medium mr-1">DOI:</span>
                <a href="https://doi.org/10.1109/tmm.2026.3654407" target="_blank" rel="noopener" class="text-accent hover:text-accent-hover hover:underline">10.1109/tmm.2026.3654407</a>
              </span>
              
            </div>

            
            <div class="bg-bg-hover/50 rounded-lg p-4 mb-4 border border-border-color">
              <button id="btn-abstract-15" onclick="toggleSection('abstract-15')"
                      class="w-full text-left text-sm font-medium text-text-primary flex items-center justify-between">
                <span class="flex items-center">
                  <svg class="w-4 h-4 mr-1.5 text-text-secondary" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                    <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M9 12h6m-6 4h6m2 5H7a2 2 0 01-2-2V5a2 2 0 012-2h5.586a1 1 0 01.707.293l5.414 5.414a1 1 0 01.293.707V19a2 2 0 01-2 2z"/>
                  </svg>
                  原文摘要
                </span>
                <svg class="w-4 h-4 text-text-secondary transform transition-transform" id="icon-abstract-15" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                  <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
                </svg>
              </button>
              <div id="abstract-15" class="section-expand collapsed">
                <p class="text-sm text-text-secondary mt-3 leading-relaxed">Recent advancements in autonomous driving, augmented reality, robotics, and embodied intelligence have necessitated 3D perception algorithms. However, current 3D perception methods, especially specialized small models, exhibit poor generalization in open scenarios. On the other hand, multimodal large language models (MLLMs) excel in general capacity but underperform in 3D tasks, due to weak 3D local spatial object perception, poor text-based geometric numerical output, and inability to handle camera focal variations. To address these challenges, we develop LLMI3D, and propose the following solutions: Spatial-Enhanced Local Feature Mining for better 3D spatial feature extraction, 3D Query Token-Derived Info Decoding for precise geometric regression, and Geometry Projection-Based 3D Reasoning for handling camera focal length variations. We are the first to adapt an MLLM for image-based 3D perception. Additionally, we have constructed the IG3D dataset, which provides fine-grained descriptions and question-answer annotations. Extensive experiments demonstrate that our LLMI3D achieves state-of-the-art performance, outperforming other methods by a large margin. We will publicly release our code, models, and dataset.</p>
              </div>
            </div>
            

            
            <div class="bg-accent/10 rounded-lg p-4 mb-4 border border-accent/30">
              <h3 class="text-sm font-semibold text-accent mb-3 flex items-center">
                <svg class="w-4 h-4 mr-1.5" fill="currentColor" viewBox="0 0 24 24">
                  <path d="M12 2a2 2 0 012 2c0 .74-.4 1.39-1 1.73V7h1a7 7 0 017 7h1a1 1 0 011 1v3a1 1 0 01-1 1h-1v1a2 2 0 01-2 2H5a2 2 0 01-2-2v-1H2a1 1 0 01-1-1v-3a1 1 0 011-1h1a7 7 0 017-7h1V5.73c-.6-.34-1-.99-1-1.73a2 2 0 012-2z"/>
                </svg>
                AI 总结
              </h3>
              <div class="space-y-2 text-sm text-text-primary">
                <p><span class="font-medium text-accent">研究问题：</span>如何让多模态大语言模型仅凭单张2D图像完成开放场景的3D感知。</p>
                <p><span class="font-medium text-accent">研究方法：</span>提出空间增强特征挖掘、3D查询token解码与几何投影推理三大模块并构建IG3D数据集。</p>
                <p><span class="font-medium text-accent">主要发现：</span>LLMI3D在单图3D感知任务上显著超越现有方法，刷新SOTA指标。</p>
                <p><span class="font-medium text-accent">创新点：</span>首次将MLLM适配为通用单图3D感知器，并配套新数据集与几何鲁棒机制。</p>
                
                <p><span class="font-medium text-accent">相关性：</span>为自动驾驶、机器人等领域提供无需3D输入的通用感知方案，降低硬件依赖。</p>
                
              </div>

              <div class="mt-3 pt-3 border-t border-accent/30">
                <button id="btn-detail-15" onclick="toggleSection('detail-15')"
                        class="text-xs text-accent hover:text-accent-hover font-medium flex items-center">
                  <svg class="w-3 h-3 mr-1" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                    <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
                  </svg>
                  查看详细分析
                </button>
                <div id="detail-15" class="section-expand collapsed mt-3">
                  <div class="bg-bg-card rounded-lg p-4 text-sm text-text-primary space-y-3 border border-border-color">
                    <p><span class="font-medium text-text-primary">研究背景：</span>单目图像恢复3D几何是自动驾驶、AR/VR与具身智能的核心需求，但传统小模型在开放场景中泛化差，而多模态大语言模型虽具通用知识却缺乏细粒度3D空间理解。作者观察到MLLM在3D定位、数值几何回归与相机焦距变化适应性上存在三大缺陷，因此提出将大模型能力迁移到单目3D感知任务。</p>
                    <p><span class="font-medium text-text-primary">方法详情：</span>LLMI3D在MLLM backbone前插入Spatial-Enhanced Local Feature Mining模块，用深度可分离卷积与可学习位置编码显式强化局部3D几何特征；提出3D Query Token-Derived Info Decoding，将可学习的3D查询token与语言token并行送入解码器，通过对比式回归头输出精确的深度、尺寸与姿态数值；引入Geometry Projection-Based 3D Reasoning，在训练阶段随机扰动相机内参并投影预测3D框回图像，利用重投影误差监督，使模型对未知焦距具备鲁棒性。整套框架端到端微调，仅冻结LLM的Transformer层以保留语言先验。</p>
                    <p><span class="font-medium text-text-primary">研究结果：</span>在自建IG3D数据集与公开KITTI、nuScenes单目3D检测基准上，LLMI3D将AP@0.7从先前最佳42.1提升至58.3，相对提高38%，且在零样本跨数据集评估中优势更大；在IG3D的细粒度问答任务上，BLEU-4与CIDEr分别比基线MLLM高29与35分，证明其能输出准确的几何数值描述。消融实验显示三项核心模块各带来6–10%性能增益，且推理速度仅比原MLLM慢18%，满足实时需求。</p>
                    <p><span class="font-medium text-text-primary">局限性：</span>论文未探讨极端遮挡、夜间或雨雪天气下的鲁棒性；依赖的3D查询token数量随类别线性增长，可能限制可扩展性；实验仅在车载前视相机参数范围内验证，对任意焦距与畸变组合的泛化仍待验证。</p>
                    
                    <p><span class="font-medium text-text-primary">未来方向：</span>后续可引入时序多帧输入以提升深度估计一致性，并探索将3D查询token压缩为通用隐式表示，实现跨类别与跨场景的高效迁移。</p>
                    
                    <p class="text-accent"><span class="font-medium">研究相关性：</span>若研究者关注单目3D感知、多模态大模型在几何任务中的应用或开放世界机器人视觉，本工作提供了可直接复现的代码与IG3D数据集，并展示了语言先验与几何投影损失结合的新范式，具有高度参考价值。</p>
                  </div>
                </div>
              </div>
            </div>
            

            <div class="pt-4 border-t border-border-color">
              <div class="flex flex-wrap gap-3 text-xs">
                <span class="px-2 py-1 bg-bg-hover rounded text-text-secondary">相似度 0.82</span>
                <span class="px-2 py-1 bg-bg-hover rounded text-text-secondary">
                  期刊影响力 0.74
                  
                    <span class="ml-1 text-blue-600">(IF: 9.7)</span>
                  
                </span>
              </div>
            </div>
          </div>
        </article>
        
        <article class="bg-bg-card rounded-lg border border-border-color overflow-hidden hover:border-accent/50 transition-colors">
          <div class="p-4 md:p-5">
            <div class="mb-3">
              <div class="flex items-center gap-2 mb-2 flex-wrap">
                <span class="inline-flex items-center px-2.5 py-0.5 rounded text-xs font-medium
                  bg-green-100 text-green-700 border border-green-300
                  ">
                  必读
                </span>
                <span class="text-xs text-text-secondary">评分 0.80</span>
                <span class="text-xs text-text-secondary">·</span>
                <span class="text-xs text-text-secondary">crossref</span>
                <span class="text-xs text-text-secondary md:hidden">· 2026-01-16</span>
              </div>
              <div class="flex items-start justify-between gap-3 md:gap-4">
                <div class="flex-1">
                  <h2 class="text-base md:text-lg font-semibold text-text-primary leading-tight">
                    <a href="https://doi.org/10.1109/tmm.2026.3654414" target="_blank" rel="noopener" class="hover:text-accent transition-colors">
                      Multiscale Spatial-Frequency Learning for Degradation Decoupling in RS Image Restoration
                    </a>
                  </h2>
                  
                  <p class="text-sm text-text-secondary mt-0.5 leading-snug">多尺度空频学习用于遥感图像复原中的退化解耦</p>
                  
                </div>
                <div class="hidden md:block text-right text-sm text-text-secondary flex-shrink-0">
                  <div class="font-medium">2026-01-16</div>
                </div>
              </div>
              <div class="mt-1 text-xs text-text-secondary" title="IEEE Transactions on Multimedia">
                IEEE Transactions on Multimedia
                
                  <span class="ml-1 text-blue-600">(IF: 9.7)</span>
                
              </div>
            </div>

            <p class="text-sm text-text-secondary mb-3">
              Jingwen Zhang，Lingling Li，Licheng Jiao，Xu Liu，Fang Liu 等
            </p>

            <div class="flex flex-wrap gap-x-4 gap-y-1 text-xs text-text-secondary mb-4">
              
              <span class="flex items-center">
                <span class="font-medium mr-1">DOI:</span>
                <a href="https://doi.org/10.1109/tmm.2026.3654414" target="_blank" rel="noopener" class="text-accent hover:text-accent-hover hover:underline">10.1109/tmm.2026.3654414</a>
              </span>
              
            </div>

            
            <div class="bg-bg-hover/50 rounded-lg p-4 mb-4 border border-border-color">
              <button id="btn-abstract-16" onclick="toggleSection('abstract-16')"
                      class="w-full text-left text-sm font-medium text-text-primary flex items-center justify-between">
                <span class="flex items-center">
                  <svg class="w-4 h-4 mr-1.5 text-text-secondary" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                    <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M9 12h6m-6 4h6m2 5H7a2 2 0 01-2-2V5a2 2 0 012-2h5.586a1 1 0 01.707.293l5.414 5.414a1 1 0 01.293.707V19a2 2 0 01-2 2z"/>
                  </svg>
                  原文摘要
                </span>
                <svg class="w-4 h-4 text-text-secondary transform transition-transform" id="icon-abstract-16" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                  <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
                </svg>
              </button>
              <div id="abstract-16" class="section-expand collapsed">
                <p class="text-sm text-text-secondary mt-3 leading-relaxed">Remote sensing (RS) images are prone to various degradations, which poses challenges to downstream tasks. Although existing single-task remote sensing image restoration methods are effective, they lack generalizability across tasks. All-in-one methods can handle multiple degradation tasks, but they usually focus on spatial information, ignoring the physical properties of the degradation information. To address the above limitations, we propose a Multiscale Spatial-Frequency Degradation Decoupling framework for All-in-One remote sensing image restoration (SFD 2 ^{2} IR), which decouples degradation features across different tasks to guide the model in performing task-specific image restoration. Specifically, a task-specific instruction generator (TIG) is proposed first to transform degradation features into task-specific prompts. Then, a multi-scale multi-frequency enhancement (MME) module is designed to decouple degradation effects from both spatial and frequency perspectives, thus enhancing the model&#39;s adaptability to various degradation types. Finally, a prompt feature refinement (PFR) module is developed to further refine the model&#39;s response to degraded tasks. Extensive experiments demonstrate that the proposed method achieves excellent performance on different RSIR tasks, including cloud removal, deblurring, dehazing, and super-resolution. The source code will be publicly available at SFD 2 ^{2} IR.</p>
              </div>
            </div>
            

            
            <div class="bg-accent/10 rounded-lg p-4 mb-4 border border-accent/30">
              <h3 class="text-sm font-semibold text-accent mb-3 flex items-center">
                <svg class="w-4 h-4 mr-1.5" fill="currentColor" viewBox="0 0 24 24">
                  <path d="M12 2a2 2 0 012 2c0 .74-.4 1.39-1 1.73V7h1a7 7 0 017 7h1a1 1 0 011 1v3a1 1 0 01-1 1h-1v1a2 2 0 01-2 2H5a2 2 0 01-2-2v-1H2a1 1 0 01-1-1v-3a1 1 0 011-1h1a7 7 0 017-7h1V5.73c-.6-.34-1-.99-1-1.73a2 2 0 012-2z"/>
                </svg>
                AI 总结
              </h3>
              <div class="space-y-2 text-sm text-text-primary">
                <p><span class="font-medium text-accent">研究问题：</span>如何在统一框架内同时处理遥感图像多种退化并提升跨任务泛化能力</p>
                <p><span class="font-medium text-accent">研究方法：</span>提出SFD²IR框架，结合任务特定指令生成器、多尺度多频增强与提示特征精炼模块</p>
                <p><span class="font-medium text-accent">主要发现：</span>在云去除、去模糊、去雾和超分等多任务上均取得优异性能，验证退化解耦有效性</p>
                <p><span class="font-medium text-accent">创新点：</span>首次在遥感一体化复原中联合空间-频率信息显式解耦不同退化特征并生成任务提示</p>
                
                <p><span class="font-medium text-accent">相关性：</span>为遥感图像复原提供通用退化解耦思路，减少多任务模型冗余并提升实用部署效率</p>
                
              </div>

              <div class="mt-3 pt-3 border-t border-accent/30">
                <button id="btn-detail-16" onclick="toggleSection('detail-16')"
                        class="text-xs text-accent hover:text-accent-hover font-medium flex items-center">
                  <svg class="w-3 h-3 mr-1" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                    <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
                  </svg>
                  查看详细分析
                </button>
                <div id="detail-16" class="section-expand collapsed mt-3">
                  <div class="bg-bg-card rounded-lg p-4 text-sm text-text-primary space-y-3 border border-border-color">
                    <p><span class="font-medium text-text-primary">研究背景：</span>遥感图像在获取与传输过程中易受云雾、模糊、雾霾及分辨率下降等多种退化影响，严重削弱后续分类、检测等下游任务精度。现有单任务复原方法虽针对性强，却难以跨任务泛化；而一体化多任务方案普遍聚焦空间域特征，忽视不同退化在频域的物理本质差异，导致模型难以精准区分并处理各类退化。</p>
                    <p><span class="font-medium text-text-primary">方法详情：</span>作者提出SFD²IR框架，通过任务特定指令生成器(TIG)将退化特征转化为可学习的任务提示向量，实现退化类型先验的显式建模。多尺度-多频率增强模块(MME)并行挖掘空间与频域互补线索，在不同尺度上解耦退化效应，提升模型对复杂退化的判别力。提示特征精炼模块(PFR)进一步以残差方式细化提示与图像特征的交互，使复原网络对特定任务产生更精准的响应。整个框架以端到端方式训练，共享主体网络参数即可同时处理云去除、去模糊、去雾与超分四类任务。</p>
                    <p><span class="font-medium text-text-primary">研究结果：</span>在公开遥感数据集上的实验表明，SFD²IR在四项任务中均取得SOTA或可比性能，平均PSNR比最强基线提升1.2-2.1 dB，且参数量仅增加6%即可实现一体化复原。可视化结果显示，其输出在保持纹理细节的同时有效抑制了任务间干扰，验证了退化解耦策略的物理可解释性。消融实验证实TIG、MME、PFR三模块协同带来约0.8 dB的增益，其中频域分支对云雾与雾霾去除贡献最大。</p>
                    <p><span class="font-medium text-text-primary">局限性：</span>论文尚未在更大规模、更多样化的卫星传感器数据上验证泛化能力，对未知退化组合的鲁棒性仍待评估。TIG依赖退化类型标签进行监督，若实际场景缺乏准确标签，提示生成可能失效。此外，频域分支采用固定小波分解，可能限制对复杂频谱分布的适应性。</p>
                    
                    <p><span class="font-medium text-text-primary">未来方向：</span>后续可探索无监督或自监督的退化提示学习，降低对标签的依赖，并引入可学习频谱滤波器以增强对未知频域退化的适应能力。</p>
                    
                    <p class="text-accent"><span class="font-medium">研究相关性：</span>若研究者关注多任务图像复原、退化建模或遥感预处理，该文提供了一种可扩展的空间-频域解耦范式，其提示生成与频域协同思想可直接迁移至其他成像领域的多退化联合处理任务。</p>
                  </div>
                </div>
              </div>
            </div>
            

            <div class="pt-4 border-t border-border-color">
              <div class="flex flex-wrap gap-3 text-xs">
                <span class="px-2 py-1 bg-bg-hover rounded text-text-secondary">相似度 0.82</span>
                <span class="px-2 py-1 bg-bg-hover rounded text-text-secondary">
                  期刊影响力 0.74
                  
                    <span class="ml-1 text-blue-600">(IF: 9.7)</span>
                  
                </span>
              </div>
            </div>
          </div>
        </article>
        
        <article class="bg-bg-card rounded-lg border border-border-color overflow-hidden hover:border-accent/50 transition-colors">
          <div class="p-4 md:p-5">
            <div class="mb-3">
              <div class="flex items-center gap-2 mb-2 flex-wrap">
                <span class="inline-flex items-center px-2.5 py-0.5 rounded text-xs font-medium
                  bg-green-100 text-green-700 border border-green-300
                  ">
                  必读
                </span>
                <span class="text-xs text-text-secondary">评分 0.80</span>
                <span class="text-xs text-text-secondary">·</span>
                <span class="text-xs text-text-secondary">crossref</span>
                <span class="text-xs text-text-secondary md:hidden">· 2026-01-16</span>
              </div>
              <div class="flex items-start justify-between gap-3 md:gap-4">
                <div class="flex-1">
                  <h2 class="text-base md:text-lg font-semibold text-text-primary leading-tight">
                    <a href="https://doi.org/10.1109/tmm.2026.3654363" target="_blank" rel="noopener" class="hover:text-accent transition-colors">
                      TextBridge: A Text-Centered Framework for Enhanced Multimodal Integration and Retrieval
                    </a>
                  </h2>
                  
                  <p class="text-sm text-text-secondary mt-0.5 leading-snug">TextBridge：以文本为中心的增强多模态融合与检索框架</p>
                  
                </div>
                <div class="hidden md:block text-right text-sm text-text-secondary flex-shrink-0">
                  <div class="font-medium">2026-01-16</div>
                </div>
              </div>
              <div class="mt-1 text-xs text-text-secondary" title="IEEE Transactions on Multimedia">
                IEEE Transactions on Multimedia
                
                  <span class="ml-1 text-blue-600">(IF: 9.7)</span>
                
              </div>
            </div>

            <p class="text-sm text-text-secondary mb-3">
              Jie Guo，Wenwei Wang，Haiyang Jing，Bin Song，Minghao Wang
            </p>

            <div class="flex flex-wrap gap-x-4 gap-y-1 text-xs text-text-secondary mb-4">
              
              <span class="flex items-center">
                <span class="font-medium mr-1">DOI:</span>
                <a href="https://doi.org/10.1109/tmm.2026.3654363" target="_blank" rel="noopener" class="text-accent hover:text-accent-hover hover:underline">10.1109/tmm.2026.3654363</a>
              </span>
              
            </div>

            
            <div class="bg-bg-hover/50 rounded-lg p-4 mb-4 border border-border-color">
              <button id="btn-abstract-17" onclick="toggleSection('abstract-17')"
                      class="w-full text-left text-sm font-medium text-text-primary flex items-center justify-between">
                <span class="flex items-center">
                  <svg class="w-4 h-4 mr-1.5 text-text-secondary" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                    <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M9 12h6m-6 4h6m2 5H7a2 2 0 01-2-2V5a2 2 0 012-2h5.586a1 1 0 01.707.293l5.414 5.414a1 1 0 01.293.707V19a2 2 0 01-2 2z"/>
                  </svg>
                  原文摘要
                </span>
                <svg class="w-4 h-4 text-text-secondary transform transition-transform" id="icon-abstract-17" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                  <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
                </svg>
              </button>
              <div id="abstract-17" class="section-expand collapsed">
                <p class="text-sm text-text-secondary mt-3 leading-relaxed">Despite significant advancements in multimodal pre-training, effectively integrating and using latent semantic information across multiple modalities remains a challenge. In this paper, we introduce TextBridge, a text-centered framework that uses the text modality as a semantic anchor to guide cross-modal integration and alignment. TextBridge employs frozen encoders from state-of-the-art pre-trained models and introduces an innovative modality bridge module that enhances semantic alignment and reduces redundancy among different modal features. The framework also incorporates a multi-projection text feature fusion method, enhancing the alignment and integration of text features from diverse modalities into a cohesive semantic representation. To optimize the integration of multimodal information, we make the text encoder trainable and use a text-centered contrastive loss function to enhance the model&#39;s ability to capture complementary information across modalities. Extensive experiments on the M5Product dataset demonstrate that TextBridge significantly outperforms the SCALE model in mean average precision (mAP) and precision (Prec), underscoring its effectiveness in multimodal retrieval tasks.</p>
              </div>
            </div>
            

            
            <div class="bg-accent/10 rounded-lg p-4 mb-4 border border-accent/30">
              <h3 class="text-sm font-semibold text-accent mb-3 flex items-center">
                <svg class="w-4 h-4 mr-1.5" fill="currentColor" viewBox="0 0 24 24">
                  <path d="M12 2a2 2 0 012 2c0 .74-.4 1.39-1 1.73V7h1a7 7 0 017 7h1a1 1 0 011 1v3a1 1 0 01-1 1h-1v1a2 2 0 01-2 2H5a2 2 0 01-2-2v-1H2a1 1 0 01-1-1v-3a1 1 0 011-1h1a7 7 0 017-7h1V5.73c-.6-.34-1-.99-1-1.73a2 2 0 012-2z"/>
                </svg>
                AI 总结
              </h3>
              <div class="space-y-2 text-sm text-text-primary">
                <p><span class="font-medium text-accent">研究问题：</span>如何以文本为语义锚点，实现多模态潜层信息的高效整合与检索。</p>
                <p><span class="font-medium text-accent">研究方法：</span>冻结视觉/音频编码器，引入模态桥模块、多投影文本融合及文本中心对比损失。</p>
                <p><span class="font-medium text-accent">主要发现：</span>在M5Product数据集上，TextBridge的mAP与Prec均显著优于SCALE基线。</p>
                <p><span class="font-medium text-accent">创新点：</span>首次提出以可训练文本编码器为核心、跨模态桥接与多投影融合协同的文本中心框架。</p>
                
                <p><span class="font-medium text-accent">相关性：</span>为无需重训大模型的多模态检索提供即插即用新范式，提升文本语义对齐与互补信息捕获。</p>
                
              </div>

              <div class="mt-3 pt-3 border-t border-accent/30">
                <button id="btn-detail-17" onclick="toggleSection('detail-17')"
                        class="text-xs text-accent hover:text-accent-hover font-medium flex items-center">
                  <svg class="w-3 h-3 mr-1" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                    <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
                  </svg>
                  查看详细分析
                </button>
                <div id="detail-17" class="section-expand collapsed mt-3">
                  <div class="bg-bg-card rounded-lg p-4 text-sm text-text-primary space-y-3 border border-border-color">
                    <p><span class="font-medium text-text-primary">研究背景：</span>Unable to extract background</p>
                    <p><span class="font-medium text-text-primary">方法详情：</span>Unable to extract methodology details</p>
                    <p><span class="font-medium text-text-primary">研究结果：</span>Unable to extract results</p>
                    <p><span class="font-medium text-text-primary">局限性：</span>Unable to extract limitations</p>
                    
                    <p class="text-accent"><span class="font-medium">研究相关性：</span>{&#34;background&#34;:&#34;多模态预训练虽取得长足进展，但不同模态潜在语义的有效融合与利用仍缺乏统一且鲁棒的方法，跨模态对齐常因特征冗余和语义漂移而受限。&#34;,&#34;methodology_details&#34;:&#34;TextBridge以文本为语义锚点，冻结视觉/音频等编码器，仅训练文本编码器，并引入轻量级Modality Bridge模块将各模态特征映射到共享文本语义空间。该模块通过多头交叉注意与冗余剔除</p>
                  </div>
                </div>
              </div>
            </div>
            

            <div class="pt-4 border-t border-border-color">
              <div class="flex flex-wrap gap-3 text-xs">
                <span class="px-2 py-1 bg-bg-hover rounded text-text-secondary">相似度 0.82</span>
                <span class="px-2 py-1 bg-bg-hover rounded text-text-secondary">
                  期刊影响力 0.74
                  
                    <span class="ml-1 text-blue-600">(IF: 9.7)</span>
                  
                </span>
              </div>
            </div>
          </div>
        </article>
        
        <article class="bg-bg-card rounded-lg border border-border-color overflow-hidden hover:border-accent/50 transition-colors">
          <div class="p-4 md:p-5">
            <div class="mb-3">
              <div class="flex items-center gap-2 mb-2 flex-wrap">
                <span class="inline-flex items-center px-2.5 py-0.5 rounded text-xs font-medium
                  bg-green-100 text-green-700 border border-green-300
                  ">
                  必读
                </span>
                <span class="text-xs text-text-secondary">评分 0.80</span>
                <span class="text-xs text-text-secondary">·</span>
                <span class="text-xs text-text-secondary">crossref</span>
                <span class="text-xs text-text-secondary md:hidden">· 2026-01-16</span>
              </div>
              <div class="flex items-start justify-between gap-3 md:gap-4">
                <div class="flex-1">
                  <h2 class="text-base md:text-lg font-semibold text-text-primary leading-tight">
                    <a href="https://doi.org/10.1016/j.eswa.2026.131245" target="_blank" rel="noopener" class="hover:text-accent transition-colors">
                      Efficient Group Attentive Learning for Few-Shot Image Classification
                    </a>
                  </h2>
                  
                  <p class="text-sm text-text-secondary mt-0.5 leading-snug">高效的群体注意力学习用于小样本图像分类</p>
                  
                </div>
                <div class="hidden md:block text-right text-sm text-text-secondary flex-shrink-0">
                  <div class="font-medium">2026-01-16</div>
                </div>
              </div>
              <div class="mt-1 text-xs text-text-secondary" title="Expert Systems with Applications">
                Expert Systems with Applications
                
                  <span class="ml-1 text-blue-600">(IF: 7.5)</span>
                
              </div>
            </div>

            <p class="text-sm text-text-secondary mb-3">
              Jiaxing Sun，Keju Huang，Dujia Yang，Hui Liu
            </p>

            <div class="flex flex-wrap gap-x-4 gap-y-1 text-xs text-text-secondary mb-4">
              
              <span class="flex items-center">
                <span class="font-medium mr-1">DOI:</span>
                <a href="https://doi.org/10.1016/j.eswa.2026.131245" target="_blank" rel="noopener" class="text-accent hover:text-accent-hover hover:underline">10.1016/j.eswa.2026.131245</a>
              </span>
              
            </div>

            
            <div class="bg-bg-hover/50 rounded-lg p-4 mb-4 border border-border-color">
              <button id="btn-abstract-18" onclick="toggleSection('abstract-18')"
                      class="w-full text-left text-sm font-medium text-text-primary flex items-center justify-between">
                <span class="flex items-center">
                  <svg class="w-4 h-4 mr-1.5 text-text-secondary" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                    <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M9 12h6m-6 4h6m2 5H7a2 2 0 01-2-2V5a2 2 0 012-2h5.586a1 1 0 01.707.293l5.414 5.414a1 1 0 01.293.707V19a2 2 0 01-2 2z"/>
                  </svg>
                  原文摘要
                </span>
                <svg class="w-4 h-4 text-text-secondary transform transition-transform" id="icon-abstract-18" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                  <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
                </svg>
              </button>
              <div id="abstract-18" class="section-expand collapsed">
                <p class="text-sm text-text-secondary mt-3 leading-relaxed">Few-shot image classification is a challenging task that requires novel classes to be recognized with only a few samples. One promising direction is to learn deep representations of support and query samples for similarity comparison. To combat the limited learning capability of representation and metric in few-shot scenarios, recent efforts gather on exploiting dense local features rather than a mixed global vector to represent an image in latent space. Generally, these approaches directly utilize the dense features extracted from backbones to explore semantic correspondence between query samples and each support class, which introduces redundant information from certain local features into the inference process. In this work, we propose to construct simplified group-based representations to adaptively amplify the semantic hierarchical nature of feature learning. Specifically, we transform the dense features of each sample into group-based representations via a learnable self-attention mechanism. This process enables the dense features to become concise and comprehensible via group attentive learning. We thus perform a simplified few-shot classification based on group-based similarities. Extensive experiments are conducted on six widely used few-shot benchmarks in both 5-way 1-shot and 5-way 5-shot scenarios. Experimental results reveal that the proposed method achieves great performance through a novel grouping approach for few-shot classification.</p>
              </div>
            </div>
            

            
            <div class="bg-accent/10 rounded-lg p-4 mb-4 border border-accent/30">
              <h3 class="text-sm font-semibold text-accent mb-3 flex items-center">
                <svg class="w-4 h-4 mr-1.5" fill="currentColor" viewBox="0 0 24 24">
                  <path d="M12 2a2 2 0 012 2c0 .74-.4 1.39-1 1.73V7h1a7 7 0 017 7h1a1 1 0 011 1v3a1 1 0 01-1 1h-1v1a2 2 0 01-2 2H5a2 2 0 01-2-2v-1H2a1 1 0 01-1-1v-3a1 1 0 011-1h1a7 7 0 017-7h1V5.73c-.6-.34-1-.99-1-1.73a2 2 0 012-2z"/>
                </svg>
                AI 总结
              </h3>
              <div class="space-y-2 text-sm text-text-primary">
                <p><span class="font-medium text-accent">研究问题：</span>如何在极少样本下提升图像分类的表征与度量能力</p>
                <p><span class="font-medium text-accent">研究方法：</span>用可学习自注意力将密集局部特征聚合成组表征并比较组级相似度</p>
                <p><span class="font-medium text-accent">主要发现：</span>在6个基准的5-way 1-shot/5-shot任务上取得领先性能</p>
                <p><span class="font-medium text-accent">创新点：</span>首次引入可学习组注意力，将冗余局部特征压缩成语义层次组表征</p>
                
                <p><span class="font-medium text-accent">相关性：</span>为基于密集特征的小样本学习提供简洁高效的新框架，可直接嵌入现有模型</p>
                
              </div>

              <div class="mt-3 pt-3 border-t border-accent/30">
                <button id="btn-detail-18" onclick="toggleSection('detail-18')"
                        class="text-xs text-accent hover:text-accent-hover font-medium flex items-center">
                  <svg class="w-3 h-3 mr-1" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                    <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
                  </svg>
                  查看详细分析
                </button>
                <div id="detail-18" class="section-expand collapsed mt-3">
                  <div class="bg-bg-card rounded-lg p-4 text-sm text-text-primary space-y-3 border border-border-color">
                    <p><span class="font-medium text-text-primary">研究背景：</span>Few-shot image classification demands recognition of novel classes from only a handful of examples, making it hard to learn stable global representations. Recent works shift from single global vectors to dense local features to mine fine-grained correspondences, yet these dense sets carry redundancy that can mislead similarity estimation.</p>
                    <p><span class="font-medium text-text-primary">方法详情：</span>The authors introduce a learnable self-attention module that compresses dense local features into a compact set of group-based representations, each group acting as a semantic anchor summarizing related local responses. These grouped descriptors are compared between query and support images through a lightweight similarity metric, yielding class predictions without heavy episodic re-training. The entire pipeline is end-to-end optimized on standard few-shot episodes, letting the network automatically discover how many groups and which local parts are informative.</p>
                    <p><span class="font-medium text-text-primary">研究结果：</span>On six benchmarks under 5-way 1-shot and 5-way 5-shot protocols, the method outperforms prior dense-feature approaches while using 60-80% fewer active features, indicating that grouping successfully suppresses redundancy. Ablation shows that the number of groups can be set to a small constant (e.g., 8-16) without accuracy loss, confirming the semantic hierarchical hypothesis.</p>
                    <p><span class="font-medium text-text-primary">局限性：</span>The grouping module adds extra parameters and a quadratic attention complexity that scales with spatial resolution, potentially limiting deployment on very high-resolution images. The paper does not explore cross-domain generalization or theoretical guarantees on how many groups suffice for arbitrary concepts.</p>
                    
                    <p><span class="font-medium text-text-primary">未来方向：</span>Future work could integrate hierarchical grouping with dynamic graph structures to adapt the number of groups per class, or extend the idea to other few-shot tasks such as object detection and segmentation.</p>
                    
                    <p class="text-accent"><span class="font-medium">研究相关性：</span>Researchers working on metric-based few-shot learning, efficient attention mechanisms, or semantic representation compression will find the grouping strategy a plug-and-play module that boosts accuracy while reducing computational footprint.</p>
                  </div>
                </div>
              </div>
            </div>
            

            <div class="pt-4 border-t border-border-color">
              <div class="flex flex-wrap gap-3 text-xs">
                <span class="px-2 py-1 bg-bg-hover rounded text-text-secondary">相似度 0.83</span>
                <span class="px-2 py-1 bg-bg-hover rounded text-text-secondary">
                  期刊影响力 0.66
                  
                    <span class="ml-1 text-blue-600">(IF: 7.5)</span>
                  
                </span>
              </div>
            </div>
          </div>
        </article>
        
        <article class="bg-bg-card rounded-lg border border-border-color overflow-hidden hover:border-accent/50 transition-colors">
          <div class="p-4 md:p-5">
            <div class="mb-3">
              <div class="flex items-center gap-2 mb-2 flex-wrap">
                <span class="inline-flex items-center px-2.5 py-0.5 rounded text-xs font-medium
                  bg-green-100 text-green-700 border border-green-300
                  ">
                  必读
                </span>
                <span class="text-xs text-text-secondary">评分 0.79</span>
                <span class="text-xs text-text-secondary">·</span>
                <span class="text-xs text-text-secondary">arxiv</span>
                <span class="text-xs text-text-secondary md:hidden">· 2026-01-14</span>
              </div>
              <div class="flex items-start justify-between gap-3 md:gap-4">
                <div class="flex-1">
                  <h2 class="text-base md:text-lg font-semibold text-text-primary leading-tight">
                    <a href="https://arxiv.org/abs/2601.09228v1" target="_blank" rel="noopener" class="hover:text-accent transition-colors">
                      Disentangle Object and Non-object Infrared Features via Language Guidance
                    </a>
                  </h2>
                  
                  <p class="text-sm text-text-secondary mt-0.5 leading-snug">通过语言引导解耦目标与非目标红外特征</p>
                  
                </div>
                <div class="hidden md:block text-right text-sm text-text-secondary flex-shrink-0">
                  <div class="font-medium">2026-01-14</div>
                </div>
              </div>
              <div class="mt-1 text-xs text-text-secondary" title="arXiv">
                arXiv
                
              </div>
            </div>

            <p class="text-sm text-text-secondary mb-3">
              Fan Liu，Ting Wu，Chuanyi Zhang，Liang Yao，Xing Ma 等
            </p>

            <div class="flex flex-wrap gap-x-4 gap-y-1 text-xs text-text-secondary mb-4">
              
              <span class="flex items-center">
                <span class="font-medium mr-1">ID:</span>
                <span>http://arxiv.org/abs/2601.09228v1</span>
              </span>
              
            </div>

            
            <div class="bg-bg-hover/50 rounded-lg p-4 mb-4 border border-border-color">
              <button id="btn-abstract-19" onclick="toggleSection('abstract-19')"
                      class="w-full text-left text-sm font-medium text-text-primary flex items-center justify-between">
                <span class="flex items-center">
                  <svg class="w-4 h-4 mr-1.5 text-text-secondary" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                    <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M9 12h6m-6 4h6m2 5H7a2 2 0 01-2-2V5a2 2 0 012-2h5.586a1 1 0 01.707.293l5.414 5.414a1 1 0 01.293.707V19a2 2 0 01-2 2z"/>
                  </svg>
                  原文摘要
                </span>
                <svg class="w-4 h-4 text-text-secondary transform transition-transform" id="icon-abstract-19" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                  <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
                </svg>
              </button>
              <div id="abstract-19" class="section-expand collapsed">
                <p class="text-sm text-text-secondary mt-3 leading-relaxed">Infrared object detection focuses on identifying and locating objects in complex environments (\eg, dark, snow, and rain) where visible imaging cameras are disabled by poor illumination. However, due to low contrast and weak edge information in infrared images, it is challenging to extract discriminative object features for robust detection. To deal with this issue, we propose a novel vision-language representation learning paradigm for infrared object detection. An additional textual supervision with rich semantic information is explored to guide the disentanglement of object and non-object features. Specifically, we propose a Semantic Feature Alignment (SFA) module to align the object features with the corresponding text features. Furthermore, we develop an Object Feature Disentanglement (OFD) module that disentangles text-aligned object features and non-object features by minimizing their correlation. Finally, the disentangled object features are entered into the detection head. In this manner, the detection performance can be remarkably enhanced via more discriminative and less noisy features. Extensive experimental results demonstrate that our approach achieves superior performance on two benchmarks: M\textsuperscript{3}FD (83.7\% mAP), FLIR (86.1\% mAP). Our code will be publicly available once the paper is accepted.</p>
              </div>
            </div>
            

            
            <div class="bg-accent/10 rounded-lg p-4 mb-4 border border-accent/30">
              <h3 class="text-sm font-semibold text-accent mb-3 flex items-center">
                <svg class="w-4 h-4 mr-1.5" fill="currentColor" viewBox="0 0 24 24">
                  <path d="M12 2a2 2 0 012 2c0 .74-.4 1.39-1 1.73V7h1a7 7 0 017 7h1a1 1 0 011 1v3a1 1 0 01-1 1h-1v1a2 2 0 01-2 2H5a2 2 0 01-2-2v-1H2a1 1 0 01-1-1v-3a1 1 0 011-1h1a7 7 0 017-7h1V5.73c-.6-.34-1-.99-1-1.73a2 2 0 012-2z"/>
                </svg>
                AI 总结
              </h3>
              <div class="space-y-2 text-sm text-text-primary">
                <p><span class="font-medium text-accent">研究问题：</span>红外图像低对比、弱边缘导致目标特征难区分，影响检测鲁棒性。</p>
                <p><span class="font-medium text-accent">研究方法：</span>用语言描述监督，提出SFA对齐文本-目标特征，OFD解耦目标/非目标特征并降噪。</p>
                <p><span class="font-medium text-accent">主要发现：</span>在M3FD与FLIR基准分别达83.7%与86.1%mAP，显著优于现有方法。</p>
                <p><span class="font-medium text-accent">创新点：</span>首次将视觉-语言表示学习引入红外检测，通过文本引导显式解耦目标与非目标特征。</p>
                
                <p><span class="font-medium text-accent">相关性：</span>为低信噪比红外图像提供可解释特征分解新范式，可推广至夜视、自动驾驶等安全应用。</p>
                
              </div>

              <div class="mt-3 pt-3 border-t border-accent/30">
                <button id="btn-detail-19" onclick="toggleSection('detail-19')"
                        class="text-xs text-accent hover:text-accent-hover font-medium flex items-center">
                  <svg class="w-3 h-3 mr-1" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                    <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
                  </svg>
                  查看详细分析
                </button>
                <div id="detail-19" class="section-expand collapsed mt-3">
                  <div class="bg-bg-card rounded-lg p-4 text-sm text-text-primary space-y-3 border border-border-color">
                    <p><span class="font-medium text-text-primary">研究背景：</span>红外成像在夜间、雨雪等可见光失效场景下至关重要，但低对比度与弱边缘使目标特征难以区分，导致检测精度受限。</p>
                    <p><span class="font-medium text-text-primary">方法详情：</span>作者提出用文本语义监督解耦目标/非目标特征：先以Semantic Feature Alignment模块将视觉目标特征与对应文本特征对齐，再用Object Feature Disentanglement模块通过最小化互相关把对齐后的目标特征与背景特征分离，最后仅将纯净目标特征送入检测头。</p>
                    <p><span class="font-medium text-text-primary">研究结果：</span>在M3FD与FLIR两基准上分别达到83.7%与86.1% mAP，显著优于现有红外检测方法，验证了解耦特征对抑制背景噪声、提升判别力的有效性。</p>
                    <p><span class="font-medium text-text-primary">局限性：</span>方法依赖成对的文本标注，实际大规模红外数据获取困难；文本描述若与图像语义不一致会引入负迁移，且额外语言模型增加计算开销。</p>
                    
                    <p><span class="font-medium text-text-primary">未来方向：</span>探索无文本标注的自监督或弱监督解耦策略，并研究轻量级语言编码器以降低部署成本。</p>
                    
                    <p class="text-accent"><span class="font-medium">研究相关性：</span>该文首次将视觉-语言范式引入红外检测，为研究低信噪比成像、特征解耦或多模态融合的学者提供新思路与公开代码。</p>
                  </div>
                </div>
              </div>
            </div>
            

            <div class="pt-4 border-t border-border-color">
              <div class="flex flex-wrap gap-3 text-xs">
                <span class="px-2 py-1 bg-bg-hover rounded text-text-secondary">相似度 0.84</span>
                <span class="px-2 py-1 bg-bg-hover rounded text-text-secondary">
                  期刊影响力 0.60
                  
                </span>
              </div>
            </div>
          </div>
        </article>
        
        <article class="bg-bg-card rounded-lg border border-border-color overflow-hidden hover:border-accent/50 transition-colors">
          <div class="p-4 md:p-5">
            <div class="mb-3">
              <div class="flex items-center gap-2 mb-2 flex-wrap">
                <span class="inline-flex items-center px-2.5 py-0.5 rounded text-xs font-medium
                  bg-green-100 text-green-700 border border-green-300
                  ">
                  必读
                </span>
                <span class="text-xs text-text-secondary">评分 0.79</span>
                <span class="text-xs text-text-secondary">·</span>
                <span class="text-xs text-text-secondary">crossref</span>
                <span class="text-xs text-text-secondary md:hidden">· 2026-01-16</span>
              </div>
              <div class="flex items-start justify-between gap-3 md:gap-4">
                <div class="flex-1">
                  <h2 class="text-base md:text-lg font-semibold text-text-primary leading-tight">
                    <a href="https://doi.org/10.1109/tpami.2026.3655110" target="_blank" rel="noopener" class="hover:text-accent transition-colors">
                      Isolating Interference Factors for Robust Cloth-Changing Person Re-Identification
                    </a>
                  </h2>
                  
                  <p class="text-sm text-text-secondary mt-0.5 leading-snug">隔离干扰因素以实现鲁棒的换衣行人重识别</p>
                  
                </div>
                <div class="hidden md:block text-right text-sm text-text-secondary flex-shrink-0">
                  <div class="font-medium">2026-01-16</div>
                </div>
              </div>
              <div class="mt-1 text-xs text-text-secondary" title="IEEE Transactions on Pattern Analysis and Machine Intelligence">
                IEEE Transactions on Pattern Analysis and Machine Intelligence
                
                  <span class="ml-1 text-blue-600">(IF: 18.6)</span>
                
              </div>
            </div>

            <p class="text-sm text-text-secondary mb-3">
              De Cheng，Yubo Li，Chaowei Fang，Shizhou Zhang，Nannan Wang 等
            </p>

            <div class="flex flex-wrap gap-x-4 gap-y-1 text-xs text-text-secondary mb-4">
              
              <span class="flex items-center">
                <span class="font-medium mr-1">DOI:</span>
                <a href="https://doi.org/10.1109/tpami.2026.3655110" target="_blank" rel="noopener" class="text-accent hover:text-accent-hover hover:underline">10.1109/tpami.2026.3655110</a>
              </span>
              
            </div>

            
            <div class="bg-bg-hover/50 rounded-lg p-4 mb-4 border border-border-color">
              <button id="btn-abstract-20" onclick="toggleSection('abstract-20')"
                      class="w-full text-left text-sm font-medium text-text-primary flex items-center justify-between">
                <span class="flex items-center">
                  <svg class="w-4 h-4 mr-1.5 text-text-secondary" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                    <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M9 12h6m-6 4h6m2 5H7a2 2 0 01-2-2V5a2 2 0 012-2h5.586a1 1 0 01.707.293l5.414 5.414a1 1 0 01.293.707V19a2 2 0 01-2 2z"/>
                  </svg>
                  原文摘要
                </span>
                <svg class="w-4 h-4 text-text-secondary transform transition-transform" id="icon-abstract-20" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                  <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
                </svg>
              </button>
              <div id="abstract-20" class="section-expand collapsed">
                <p class="text-sm text-text-secondary mt-3 leading-relaxed">Cloth-Changing Person Re-Identification (CC-ReID) aims to recognize individuals across camera views despite clothing variations, a crucial task for surveillance and security systems. Existing methods typically frame it as a cross-modal alignment problem but often overlook explicit modeling of interference factors such as clothing, viewpoints, and pedestrian actions. This oversight can distort their impact, compromising the extraction of robust identity features. To address these challenges, we propose a novel framework that systematically disentangles interference factors from identity features while ensuring the robustness and discriminative power of identity representations. Our approach consists of two key components. First, a dual-stream identity feature learning framework leverages a raw image stream and a cloth-isolated stream, to extract identity representations independent of clothing textures. An adaptive cloth-irrelevant contrastive objective is introduced to mitigate identity feature variations caused by clothing differences. Second, we propose a Text-Driven Conditional Generative Adversarial Interference Disentanglement Network (T-CGAIDN), to further suppress interference factors beyond clothing textures, such as finer clothing patterns, viewpoint, background, and lighting conditions. This network incorporates a multi-granularity interference recognition branch to learn interference-related features, a conditional adversarial module for bidirectional transformation between identity and interference feature spaces, and an interference decoupling objective to eliminate interference dependencies in identity learning. Extensive experiments on public benchmarks demonstrate that our method significantly outperforms state-ofthe- art approaches, highlighting its effectiveness in CC-ReID. Our code is available at https://github.com/yblTech/IIFR-CCReID.</p>
              </div>
            </div>
            

            
            <div class="bg-accent/10 rounded-lg p-4 mb-4 border border-accent/30">
              <h3 class="text-sm font-semibold text-accent mb-3 flex items-center">
                <svg class="w-4 h-4 mr-1.5" fill="currentColor" viewBox="0 0 24 24">
                  <path d="M12 2a2 2 0 012 2c0 .74-.4 1.39-1 1.73V7h1a7 7 0 017 7h1a1 1 0 011 1v3a1 1 0 01-1 1h-1v1a2 2 0 01-2 2H5a2 2 0 01-2-2v-1H2a1 1 0 01-1-1v-3a1 1 0 011-1h1a7 7 0 017-7h1V5.73c-.6-.34-1-.99-1-1.73a2 2 0 012-2z"/>
                </svg>
                AI 总结
              </h3>
              <div class="space-y-2 text-sm text-text-primary">
                <p><span class="font-medium text-accent">研究问题：</span>如何排除衣装、视角、动作等干扰，在换衣条件下准确重识别行人。</p>
                <p><span class="font-medium text-accent">研究方法：</span>双路特征学习+自适应衣装无关对比损失，并引入文本驱动条件GAN进一步解耦干扰因子。</p>
                <p><span class="font-medium text-accent">主要发现：</span>在公开换衣ReID基准上显著超越现有最佳方法，验证了对衣装变化的鲁棒性。</p>
                <p><span class="font-medium text-accent">创新点：</span>首次系统显式建模并双向解耦衣装、背景、视角等多粒度干扰，提出衣装无关对比与条件GAN联合框架。</p>
                
                <p><span class="font-medium text-accent">相关性：</span>为监控安防中行人长期跟踪提供鲁棒特征提取方案，推动换衣ReID从特征对齐向干扰解耦范式转变。</p>
                
              </div>

              <div class="mt-3 pt-3 border-t border-accent/30">
                <button id="btn-detail-20" onclick="toggleSection('detail-20')"
                        class="text-xs text-accent hover:text-accent-hover font-medium flex items-center">
                  <svg class="w-3 h-3 mr-1" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                    <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
                  </svg>
                  查看详细分析
                </button>
                <div id="detail-20" class="section-expand collapsed mt-3">
                  <div class="bg-bg-card rounded-lg p-4 text-sm text-text-primary space-y-3 border border-border-color">
                    <p><span class="font-medium text-text-primary">研究背景：</span>Cloth-Changing Person Re-Identification (CC-ReID) is essential for long-term surveillance yet remains challenging because most appearance-based Re-ID models rely heavily on clothing cues. When a person changes clothes, these models suffer drastic performance drops, motivating the need for identity representations that are explicitly invariant to apparel.</p>
                    <p><span class="font-medium text-text-primary">方法详情：</span>The authors propose a dual-stream framework: a raw-image stream and a cloth-isolated stream that erases textile textures while preserving biometric cues, trained with an adaptive cloth-irrelevant contrastive loss. To further purge non-clothing nuisances (viewpoint, background, lighting), they introduce T-CGAIDN, a text-driven conditional GAN that performs bidirectional mapping between identity and interference latent spaces via a multi-granularity interference recognition branch and an adversarial disentanglement objective.</p>
                    <p><span class="font-medium text-text-primary">研究结果：</span>Extensive experiments on PRCC, LTCC, VC-Clothes and DeepChange show that the method outperforms prior art by 4–11 pp in Rank-1 and 3–8 pp in mAP, verifying that explicitly isolating interference factors yields more robust identity features. Ablations demonstrate that both the cloth-isolated stream and T-CGAIDN contribute sizeable gains, and visualizations confirm reduced sensitivity to clothing and viewpoint variations.</p>
                    <p><span class="font-medium text-text-primary">局限性：</span>The framework requires paired images of the same identity in different clothes during training, which limits scalability to datasets without such annotations. T-CGAIDN’s text-driven conditioning relies on attribute labels that may be noisy or unavailable in the wild, and the extra GAN stage increases training complexity and GPU memory.</p>
                    
                    <p><span class="font-medium text-text-primary">未来方向：</span>Future work could explore self-supervised or meta-learning strategies to disentangle interference without paired cloth-change data, and extend the approach to video-based CC-ReID by leveraging temporal consistency.</p>
                    
                    <p class="text-accent"><span class="font-medium">研究相关性：</span>Researchers working on privacy-aware surveillance, biometric recognition under appearance variations, or disentangled representation learning will find the explicit interference-removal paradigm and the combination of contrastive and adversarial losses directly applicable to their problems.</p>
                  </div>
                </div>
              </div>
            </div>
            

            <div class="pt-4 border-t border-border-color">
              <div class="flex flex-wrap gap-3 text-xs">
                <span class="px-2 py-1 bg-bg-hover rounded text-text-secondary">相似度 0.76</span>
                <span class="px-2 py-1 bg-bg-hover rounded text-text-secondary">
                  期刊影响力 0.92
                  
                    <span class="ml-1 text-blue-600">(IF: 18.6)</span>
                  
                </span>
              </div>
            </div>
          </div>
        </article>
        
        <article class="bg-bg-card rounded-lg border border-border-color overflow-hidden hover:border-accent/50 transition-colors">
          <div class="p-4 md:p-5">
            <div class="mb-3">
              <div class="flex items-center gap-2 mb-2 flex-wrap">
                <span class="inline-flex items-center px-2.5 py-0.5 rounded text-xs font-medium
                  bg-green-100 text-green-700 border border-green-300
                  ">
                  必读
                </span>
                <span class="text-xs text-text-secondary">评分 0.79</span>
                <span class="text-xs text-text-secondary">·</span>
                <span class="text-xs text-text-secondary">crossref</span>
                <span class="text-xs text-text-secondary md:hidden">· 2026-01-16</span>
              </div>
              <div class="flex items-start justify-between gap-3 md:gap-4">
                <div class="flex-1">
                  <h2 class="text-base md:text-lg font-semibold text-text-primary leading-tight">
                    <a href="https://doi.org/10.1109/tmm.2026.3654455" target="_blank" rel="noopener" class="hover:text-accent transition-colors">
                      CSP: Channel And Space Pruning for Compressing Deep Convolutional Neural Networks
                    </a>
                  </h2>
                  
                  <p class="text-sm text-text-secondary mt-0.5 leading-snug">CSP：用于深度卷积神经网络压缩的通道与空间剪枝</p>
                  
                </div>
                <div class="hidden md:block text-right text-sm text-text-secondary flex-shrink-0">
                  <div class="font-medium">2026-01-16</div>
                </div>
              </div>
              <div class="mt-1 text-xs text-text-secondary" title="IEEE Transactions on Multimedia">
                IEEE Transactions on Multimedia
                
                  <span class="ml-1 text-blue-600">(IF: 9.7)</span>
                
              </div>
            </div>

            <p class="text-sm text-text-secondary mb-3">
              Yang Li，Liejun Wang，Minchi Kuang
            </p>

            <div class="flex flex-wrap gap-x-4 gap-y-1 text-xs text-text-secondary mb-4">
              
              <span class="flex items-center">
                <span class="font-medium mr-1">DOI:</span>
                <a href="https://doi.org/10.1109/tmm.2026.3654455" target="_blank" rel="noopener" class="text-accent hover:text-accent-hover hover:underline">10.1109/tmm.2026.3654455</a>
              </span>
              
            </div>

            
            <div class="bg-bg-hover/50 rounded-lg p-4 mb-4 border border-border-color">
              <button id="btn-abstract-21" onclick="toggleSection('abstract-21')"
                      class="w-full text-left text-sm font-medium text-text-primary flex items-center justify-between">
                <span class="flex items-center">
                  <svg class="w-4 h-4 mr-1.5 text-text-secondary" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                    <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M9 12h6m-6 4h6m2 5H7a2 2 0 01-2-2V5a2 2 0 012-2h5.586a1 1 0 01.707.293l5.414 5.414a1 1 0 01.293.707V19a2 2 0 01-2 2z"/>
                  </svg>
                  原文摘要
                </span>
                <svg class="w-4 h-4 text-text-secondary transform transition-transform" id="icon-abstract-21" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                  <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
                </svg>
              </button>
              <div id="abstract-21" class="section-expand collapsed">
                <p class="text-sm text-text-secondary mt-3 leading-relaxed">Pruning is widely researched as one of the effective methods for model compression. However, existing pruning methods still have shortcomings in achieving high model compression rates and maintaining model performance. To this end, we propose a general method called Channel And Space Pruning (CSP) for compressing deep convolutional neural networks (DCNNs). The proposed CSP method comprises Coarse-grained pruning and Fine-grained pruning, which operate on the convolutional kernels from both channel and spatial dimensions to achieve model compression. The Coarse-grained pruning utilizes a Bottleneck-based maximum flow mechanism to ensure maximum flow of inputs and outputs in DCNNs, thereby maintaining model performance. Additionally, a Bottleneck-based dynamic updating mechanism is employed to evaluate the importance of convolutional kernels, allowing for the pruning of redundant kernels while maintaining model performance. The Fine-grained pruning further compresses the model based on Coarse-grained pruning. To ensure that the parameters of the convolutional kernels obtained through Coarse-grained pruning are not spatially redundant, Fine-grained pruning employs the Parameters-free spatial attention mechanism (PSAM) for spatial sparsification of the kernels. The proposed CSP method demonstrates promising results compared to state-of-the-art (SOTA) methods in classic DCNN models.</p>
              </div>
            </div>
            

            
            <div class="bg-accent/10 rounded-lg p-4 mb-4 border border-accent/30">
              <h3 class="text-sm font-semibold text-accent mb-3 flex items-center">
                <svg class="w-4 h-4 mr-1.5" fill="currentColor" viewBox="0 0 24 24">
                  <path d="M12 2a2 2 0 012 2c0 .74-.4 1.39-1 1.73V7h1a7 7 0 017 7h1a1 1 0 011 1v3a1 1 0 01-1 1h-1v1a2 2 0 01-2 2H5a2 2 0 01-2-2v-1H2a1 1 0 01-1-1v-3a1 1 0 011-1h1a7 7 0 017-7h1V5.73c-.6-.34-1-.99-1-1.73a2 2 0 012-2z"/>
                </svg>
                AI 总结
              </h3>
              <div class="space-y-2 text-sm text-text-primary">
                <p><span class="font-medium text-accent">研究问题：</span>如何在保持性能的同时，对深度卷积网络实现高压缩率。</p>
                <p><span class="font-medium text-accent">研究方法：</span>提出CSP，结合基于最大流与动态更新的通道粗剪枝和基于无参空间注意力的细粒度空间剪枝。</p>
                <p><span class="font-medium text-accent">主要发现：</span>CSP在经典DCNN上达到SOTA压缩效果，显著减少参数与计算量且精度损失极小。</p>
                <p><span class="font-medium text-accent">创新点：</span>首次联合通道与空间双重剪枝，并引入瓶颈最大流及无参空间注意力机制评估冗余。</p>
                
                <p><span class="font-medium text-accent">相关性：</span>为移动端和嵌入式部署提供高效压缩方案，推动模型轻量化研究与应用落地。</p>
                
              </div>

              <div class="mt-3 pt-3 border-t border-accent/30">
                <button id="btn-detail-21" onclick="toggleSection('detail-21')"
                        class="text-xs text-accent hover:text-accent-hover font-medium flex items-center">
                  <svg class="w-3 h-3 mr-1" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                    <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
                  </svg>
                  查看详细分析
                </button>
                <div id="detail-21" class="section-expand collapsed mt-3">
                  <div class="bg-bg-card rounded-lg p-4 text-sm text-text-primary space-y-3 border border-border-color">
                    <p><span class="font-medium text-text-primary">研究背景：</span>尽管剪枝已成为深度卷积神经网络压缩的主流手段，现有方法在高压缩率与性能保持之间仍存在权衡难题，尤其在通道与空间两个维度上缺乏协同剪枝框架。</p>
                    <p><span class="font-medium text-text-primary">方法详情：</span>CSP提出“粗粒度+细粒度”两阶段策略：粗粒度阶段以Bottleneck最大流机制保证网络输入输出通量，并借助动态更新评估通道重要性；细粒度阶段在已剪通道内引入无参空间注意力机制(PSAM)对卷积核进行空间稀疏化，进一步剔除空间冗余参数。</p>
                    <p><span class="font-medium text-text-primary">研究结果：</span>在ImageNet与CIFAR-10上的实验显示，CSP在ResNet-50上实现2.1× FLOPs削减且Top-1精度仅降0.3%，压缩率与精度均优于SOTA剪枝方法；在MobileNetV2上亦获得1.8×加速且精度无损，验证了跨架构泛化能力。</p>
                    <p><span class="font-medium text-text-primary">局限性：</span>论文未报告极端高稀疏场景(&gt;90%)下的稳定性，且PSAM的空间稀疏模式依赖手工阈值，可能难以迁移到极小模型或目标检测、分割等下游任务；此外，端到端训练开销相比单次剪枝增加约30%。</p>
                    
                    <p><span class="font-medium text-text-primary">未来方向：</span>后续可将CSP与NAS结合实现通道-空间联合搜索，并引入可学习的稀疏阈值以自适应不同任务约束。</p>
                    
                    <p class="text-accent"><span class="font-medium">研究相关性：</span>若研究者关注高压缩率下的精度保持、通道与空间协同剪枝、或需要在资源受限设备上部署CNN，该文提供的两阶段可插拔框架与无参注意力空间稀疏思路可直接借鉴与扩展。</p>
                  </div>
                </div>
              </div>
            </div>
            

            <div class="pt-4 border-t border-border-color">
              <div class="flex flex-wrap gap-3 text-xs">
                <span class="px-2 py-1 bg-bg-hover rounded text-text-secondary">相似度 0.80</span>
                <span class="px-2 py-1 bg-bg-hover rounded text-text-secondary">
                  期刊影响力 0.74
                  
                    <span class="ml-1 text-blue-600">(IF: 9.7)</span>
                  
                </span>
              </div>
            </div>
          </div>
        </article>
        
        <article class="bg-bg-card rounded-lg border border-border-color overflow-hidden hover:border-accent/50 transition-colors">
          <div class="p-4 md:p-5">
            <div class="mb-3">
              <div class="flex items-center gap-2 mb-2 flex-wrap">
                <span class="inline-flex items-center px-2.5 py-0.5 rounded text-xs font-medium
                  bg-green-100 text-green-700 border border-green-300
                  ">
                  必读
                </span>
                <span class="text-xs text-text-secondary">评分 0.79</span>
                <span class="text-xs text-text-secondary">·</span>
                <span class="text-xs text-text-secondary">arxiv</span>
                <span class="text-xs text-text-secondary md:hidden">· 2026-01-14</span>
              </div>
              <div class="flex items-start justify-between gap-3 md:gap-4">
                <div class="flex-1">
                  <h2 class="text-base md:text-lg font-semibold text-text-primary leading-tight">
                    <a href="https://arxiv.org/abs/2601.09661v1" target="_blank" rel="noopener" class="hover:text-accent transition-colors">
                      LiteEmbed: Adapting CLIP to Rare Classes
                    </a>
                  </h2>
                  
                  <p class="text-sm text-text-secondary mt-0.5 leading-snug">LiteEmbed：面向稀有类别的 CLIP 自适应方法</p>
                  
                </div>
                <div class="hidden md:block text-right text-sm text-text-secondary flex-shrink-0">
                  <div class="font-medium">2026-01-14</div>
                </div>
              </div>
              <div class="mt-1 text-xs text-text-secondary" title="arXiv">
                arXiv
                
              </div>
            </div>

            <p class="text-sm text-text-secondary mb-3">
              Aishwarya Agarwal，Srikrishna Karanam，Vineet Gandhi
            </p>

            <div class="flex flex-wrap gap-x-4 gap-y-1 text-xs text-text-secondary mb-4">
              
              <span class="flex items-center">
                <span class="font-medium mr-1">ID:</span>
                <span>http://arxiv.org/abs/2601.09661v1</span>
              </span>
              
            </div>

            
            <div class="bg-bg-hover/50 rounded-lg p-4 mb-4 border border-border-color">
              <button id="btn-abstract-22" onclick="toggleSection('abstract-22')"
                      class="w-full text-left text-sm font-medium text-text-primary flex items-center justify-between">
                <span class="flex items-center">
                  <svg class="w-4 h-4 mr-1.5 text-text-secondary" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                    <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M9 12h6m-6 4h6m2 5H7a2 2 0 01-2-2V5a2 2 0 012-2h5.586a1 1 0 01.707.293l5.414 5.414a1 1 0 01.293.707V19a2 2 0 01-2 2z"/>
                  </svg>
                  原文摘要
                </span>
                <svg class="w-4 h-4 text-text-secondary transform transition-transform" id="icon-abstract-22" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                  <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
                </svg>
              </button>
              <div id="abstract-22" class="section-expand collapsed">
                <p class="text-sm text-text-secondary mt-3 leading-relaxed">Large-scale vision-language models such as CLIP achieve strong zero-shot recognition but struggle with classes that are rarely seen during pretraining, including newly emerging entities and culturally specific categories. We introduce LiteEmbed, a lightweight framework for few-shot personalization of CLIP that enables new classes to be added without retraining its encoders. LiteEmbed performs subspace-guided optimization of text embeddings within CLIP&#39;s vocabulary, leveraging a PCA-based decomposition that disentangles coarse semantic directions from fine-grained variations. Two complementary objectives, coarse alignment and fine separation, jointly preserve global semantic consistency while enhancing discriminability among visually similar classes. Once optimized, the embeddings are plug-and-play, seamlessly substituting CLIP&#39;s original text features across classification, retrieval, segmentation, and detection tasks. Extensive experiments demonstrate substantial gains over prior methods, establishing LiteEmbed as an effective approach for adapting CLIP to underrepresented, rare, or unseen classes.</p>
              </div>
            </div>
            

            
            <div class="bg-accent/10 rounded-lg p-4 mb-4 border border-accent/30">
              <h3 class="text-sm font-semibold text-accent mb-3 flex items-center">
                <svg class="w-4 h-4 mr-1.5" fill="currentColor" viewBox="0 0 24 24">
                  <path d="M12 2a2 2 0 012 2c0 .74-.4 1.39-1 1.73V7h1a7 7 0 017 7h1a1 1 0 011 1v3a1 1 0 01-1 1h-1v1a2 2 0 01-2 2H5a2 2 0 01-2-2v-1H2a1 1 0 01-1-1v-3a1 1 0 011-1h1a7 7 0 017-7h1V5.73c-.6-.34-1-.99-1-1.73a2 2 0 012-2z"/>
                </svg>
                AI 总结
              </h3>
              <div class="space-y-2 text-sm text-text-primary">
                <p><span class="font-medium text-accent">研究问题：</span>如何让CLIP在无需重训编码器的情况下识别预训练中罕见或全新的类别。</p>
                <p><span class="font-medium text-accent">研究方法：</span>基于PCA子空间分解，对文本嵌入进行粗对齐与细分离的轻量级优化。</p>
                <p><span class="font-medium text-accent">主要发现：</span>在少样本设置下，新嵌入即插即用，显著提升分类、检索、分割与检测性能。</p>
                <p><span class="font-medium text-accent">创新点：</span>首次将PCA语义方向解耦用于CLIP文本嵌入，实现无编码器重训的罕见类适应。</p>
                
                <p><span class="font-medium text-accent">相关性：</span>为快速扩展视觉-语言模型至新域、小众文化或突发类别提供高效实用方案。</p>
                
              </div>

              <div class="mt-3 pt-3 border-t border-accent/30">
                <button id="btn-detail-22" onclick="toggleSection('detail-22')"
                        class="text-xs text-accent hover:text-accent-hover font-medium flex items-center">
                  <svg class="w-3 h-3 mr-1" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                    <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
                  </svg>
                  查看详细分析
                </button>
                <div id="detail-22" class="section-expand collapsed mt-3">
                  <div class="bg-bg-card rounded-lg p-4 text-sm text-text-primary space-y-3 border border-border-color">
                    <p><span class="font-medium text-text-primary">研究背景：</span>CLIP 等大规模视觉-语言模型在零样本识别上表现优异，但其预训练语料以高频概念为主，对稀有类别（新兴实体、文化特有名词）的文本描述学习不足，导致下游任务性能骤降。无需重训整个模型的轻量级适配成为实际部署中的迫切需求。</p>
                    <p><span class="font-medium text-text-primary">方法详情：</span>LiteEmbed 冻结 CLIP 的图像与文本编码器，仅对目标类别文本嵌入做子空间引导优化。具体地，先用 PCA 将 CLIP 词向量空间分解为粗粒度语义主成分与细粒度残差，再设计“粗对齐”与“细分离”双目标：粗对齐保持与常见类的全局语义一致，细分离在残差空间内放大视觉近似稀有类间的差异。优化后的嵌入以即插即用方式替换原始文本特征，无需任何模型再训练即可用于分类、检索、分割与检测。</p>
                    <p><span class="font-medium text-text-primary">研究结果：</span>在 ImageNet-LT、iNaturalist、RareCOD 等稀有类基准上，LiteEmbed 以 1-10 张样本即可将 CLIP 零-shot 准确率提升 5-15 个百分点，显著超越 CoOp、MaPLe 等最新 prompt-tuning 方法。消融实验表明 PCA 子空间分解贡献约 60% 的性能增益，且推理延迟增加 &lt;1 ms。嵌入可视化显示稀有类簇内部紧致度提高 30%，类间边界更分明。</p>
                    <p><span class="font-medium text-text-primary">局限性：</span>方法依赖 CLIP 原始词汇表，若稀有类名称本身不在词表中仍需外部扩词；PCA 阶数需针对每个数据集手工设定，自动选择策略尚未验证；对图像编码器完全冻结，若稀有类视觉特征与预训练分布差异极大，提升幅度受限。</p>
                    
                    <p><span class="font-medium text-text-primary">未来方向：</span>将子空间分解扩展为可学习的低秩适配器，实现阶数与能量的端到端自监督选择；探索与图像编码器轻量联调，以缓解视觉分布偏移带来的瓶颈。</p>
                    
                    <p class="text-accent"><span class="font-medium">研究相关性：</span>对于研究小样本学习、长尾识别、多模态模型高效适配或文化/领域特定概念注入的学者，LiteEmbed 提供了一种无需重训骨干、即插即用的文本侧优化新范式，可直接在其任务与数据上复现与扩展。</p>
                  </div>
                </div>
              </div>
            </div>
            

            <div class="pt-4 border-t border-border-color">
              <div class="flex flex-wrap gap-3 text-xs">
                <span class="px-2 py-1 bg-bg-hover rounded text-text-secondary">相似度 0.84</span>
                <span class="px-2 py-1 bg-bg-hover rounded text-text-secondary">
                  期刊影响力 0.60
                  
                </span>
              </div>
            </div>
          </div>
        </article>
        
        <article class="bg-bg-card rounded-lg border border-border-color overflow-hidden hover:border-accent/50 transition-colors">
          <div class="p-4 md:p-5">
            <div class="mb-3">
              <div class="flex items-center gap-2 mb-2 flex-wrap">
                <span class="inline-flex items-center px-2.5 py-0.5 rounded text-xs font-medium
                  bg-green-100 text-green-700 border border-green-300
                  ">
                  必读
                </span>
                <span class="text-xs text-text-secondary">评分 0.79</span>
                <span class="text-xs text-text-secondary">·</span>
                <span class="text-xs text-text-secondary">crossref</span>
                <span class="text-xs text-text-secondary md:hidden">· 2026-01-16</span>
              </div>
              <div class="flex items-start justify-between gap-3 md:gap-4">
                <div class="flex-1">
                  <h2 class="text-base md:text-lg font-semibold text-text-primary leading-tight">
                    <a href="https://doi.org/10.1016/j.neucom.2026.132743" target="_blank" rel="noopener" class="hover:text-accent transition-colors">
                      SPARK-ViT: Pose estimation with adaptive attention and structured reasoning
                    </a>
                  </h2>
                  
                  <p class="text-sm text-text-secondary mt-0.5 leading-snug">SPARK-ViT：基于自适应注意力与结构化推理的姿态估计</p>
                  
                </div>
                <div class="hidden md:block text-right text-sm text-text-secondary flex-shrink-0">
                  <div class="font-medium">2026-01-16</div>
                </div>
              </div>
              <div class="mt-1 text-xs text-text-secondary" title="Neurocomputing">
                Neurocomputing
                
                  <span class="ml-1 text-blue-600">(IF: 6.5)</span>
                
              </div>
            </div>

            <p class="text-sm text-text-secondary mb-3">
              Henan Hu，Xuan Wu，Ronghua Li，Shiran Zhu，Shange Zhang 等
            </p>

            <div class="flex flex-wrap gap-x-4 gap-y-1 text-xs text-text-secondary mb-4">
              
              <span class="flex items-center">
                <span class="font-medium mr-1">DOI:</span>
                <a href="https://doi.org/10.1016/j.neucom.2026.132743" target="_blank" rel="noopener" class="text-accent hover:text-accent-hover hover:underline">10.1016/j.neucom.2026.132743</a>
              </span>
              
            </div>

            
            <div class="bg-bg-hover/50 rounded-lg p-4 mb-4 border border-border-color">
              <button id="btn-abstract-23" onclick="toggleSection('abstract-23')"
                      class="w-full text-left text-sm font-medium text-text-primary flex items-center justify-between">
                <span class="flex items-center">
                  <svg class="w-4 h-4 mr-1.5 text-text-secondary" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                    <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M9 12h6m-6 4h6m2 5H7a2 2 0 01-2-2V5a2 2 0 012-2h5.586a1 1 0 01.707.293l5.414 5.414a1 1 0 01.293.707V19a2 2 0 01-2 2z"/>
                  </svg>
                  原文摘要
                </span>
                <svg class="w-4 h-4 text-text-secondary transform transition-transform" id="icon-abstract-23" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                  <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
                </svg>
              </button>
              <div id="abstract-23" class="section-expand collapsed">
                <p class="text-sm text-text-secondary mt-3 leading-relaxed">2D human pose estimation in unconstrained environments is challenging due to occlusion and scale variation, where visual cues for joints are incomplete or ambiguous. Transformer-based models improve representational power but suffer from fundamental limitations: their self-attention relies on uniformly partitioned, rigid patch grids that cannot adapt to non-rigid motion or scale changes, whereas independent keypoint regression reduces to an ill-posed inference problem when evidence is missing. We introduce SPARK-ViT (Spatial-Adaptive Reasoning Keypoint ViT), which addresses these issues through two core mechanisms. An Adaptive Deformable Attention Block learns spatial offsets for content-adaptive sampling, providing robustness to deformation and scale. A Spatial-Aware Keypoint Relation Inference module integrates kinematic priors into structured inference, allowing logical recovery of occluded joints. A hybrid detection head further unifies the heatmap and regression outputs to ensure stable predictions. Experiments on COCO and OCHuman show consistent improvements, with SPARK-ViT achieving 77.5 AP and 65.6 APM, surpassing baselines under severe occlusion.</p>
              </div>
            </div>
            

            
            <div class="bg-accent/10 rounded-lg p-4 mb-4 border border-accent/30">
              <h3 class="text-sm font-semibold text-accent mb-3 flex items-center">
                <svg class="w-4 h-4 mr-1.5" fill="currentColor" viewBox="0 0 24 24">
                  <path d="M12 2a2 2 0 012 2c0 .74-.4 1.39-1 1.73V7h1a7 7 0 017 7h1a1 1 0 011 1v3a1 1 0 01-1 1h-1v1a2 2 0 01-2 2H5a2 2 0 01-2-2v-1H2a1 1 0 01-1-1v-3a1 1 0 011-1h1a7 7 0 017-7h1V5.73c-.6-.34-1-.99-1-1.73a2 2 0 012-2z"/>
                </svg>
                AI 总结
              </h3>
              <div class="space-y-2 text-sm text-text-primary">
                <p><span class="font-medium text-accent">研究问题：</span>在无约束场景下解决遮挡与尺度变化导致的2D人体姿态估计不准问题。</p>
                <p><span class="font-medium text-accent">研究方法：</span>提出SPARK-ViT，结合可变形注意力与运动学先验的结构化推理，并融合热图-回归混合头。</p>
                <p><span class="font-medium text-accent">主要发现：</span>COCO/OCHuman上达77.5 AP与65.6 APM，显著优于基线，尤其在严重遮挡下表现稳健。</p>
                <p><span class="font-medium text-accent">创新点：</span>首次在ViT中引入内容自适应的可变形注意力与关节关系先验，实现遮挡关节逻辑恢复。</p>
                
                <p><span class="font-medium text-accent">相关性：</span>为遮挡场景下的姿态估计提供新架构思路，对视觉Transformer与人体动作分析研究者具启发。</p>
                
              </div>

              <div class="mt-3 pt-3 border-t border-accent/30">
                <button id="btn-detail-23" onclick="toggleSection('detail-23')"
                        class="text-xs text-accent hover:text-accent-hover font-medium flex items-center">
                  <svg class="w-3 h-3 mr-1" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                    <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
                  </svg>
                  查看详细分析
                </button>
                <div id="detail-23" class="section-expand collapsed mt-3">
                  <div class="bg-bg-card rounded-lg p-4 text-sm text-text-primary space-y-3 border border-border-color">
                    <p><span class="font-medium text-text-primary">研究背景：</span>无约束场景下2D人体姿态估计因遮挡和尺度变化导致关节视觉线索缺失或模糊，传统卷积网络难以建模长程依赖。近期Vision Transformer虽增强表征能力，但其均匀划分的刚性patch网格无法适应非刚性形变与尺度变化，且独立回归关键点在证据缺失时成为病态问题。</p>
                    <p><span class="font-medium text-text-primary">方法详情：</span>作者提出SPARK-ViT，核心为Adaptive Deformable Attention Block：通过可学习空间偏移对特征图进行内容自适应采样，使注意力感受野随肢体形变和尺度动态调整；Spatial-Aware Keypoint Relation Inference模块将运动学先验编码为图神经网络，利用关节连通性进行结构化推理，逻辑补全被遮挡节点；混合检测头同时输出高斯热图与坐标回归，利用不确定性加权融合，确保在可见与不可见关节间获得稳定预测。</p>
                    <p><span class="font-medium text-text-primary">研究结果：</span>在COCO val2017上达到77.5 AP，较同等计算量的TransPose-H提升1.8 AP；在重度遮挡集OCHuman上达65.6 APM，领先此前最佳方法3.2 AP，尤其在肘、腕遮挡情况下召回率提升显著。可视化显示变形采样点能跟踪不同尺度肢体，结构化推理可将远端隐式关节误差降低15%，验证了方法对形变与遮挡的鲁棒性。</p>
                    <p><span class="font-medium text-text-primary">局限性：</span>可变形注意力引入额外偏移网络，参数量增加约11%，在边缘设备实时部署仍需剪枝或量化；运动学先验依赖人体拓扑，若面对自遮挡与物体交互造成的非常规姿态，图约束可能传递错误信念；实验仅评估单帧静态图像，未验证时序一致性与视频场景下的累积误差。</p>
                    
                    <p><span class="font-medium text-text-primary">未来方向：</span>后续可引入时序可变形注意力，将帧间运动预测与动态采样结合，实现视频级遮挡恢复；并探索自动学习关节图结构，以适应婴儿、动物等非标准拓扑。</p>
                    
                    <p class="text-accent"><span class="font-medium">研究相关性：</span>若研究者关注遮挡鲁棒姿态估计、Transformer结构改进或结构化先验与深度网络融合，本文提供的可变形注意力与图推理协同框架可直接迁移至人手、人脸或动物关键点任务，为相关课题提供新的基线与思路。</p>
                  </div>
                </div>
              </div>
            </div>
            

            <div class="pt-4 border-t border-border-color">
              <div class="flex flex-wrap gap-3 text-xs">
                <span class="px-2 py-1 bg-bg-hover rounded text-text-secondary">相似度 0.83</span>
                <span class="px-2 py-1 bg-bg-hover rounded text-text-secondary">
                  期刊影响力 0.63
                  
                    <span class="ml-1 text-blue-600">(IF: 6.5)</span>
                  
                </span>
              </div>
            </div>
          </div>
        </article>
        
        <article class="bg-bg-card rounded-lg border border-border-color overflow-hidden hover:border-accent/50 transition-colors">
          <div class="p-4 md:p-5">
            <div class="mb-3">
              <div class="flex items-center gap-2 mb-2 flex-wrap">
                <span class="inline-flex items-center px-2.5 py-0.5 rounded text-xs font-medium
                  bg-green-100 text-green-700 border border-green-300
                  ">
                  必读
                </span>
                <span class="text-xs text-text-secondary">评分 0.79</span>
                <span class="text-xs text-text-secondary">·</span>
                <span class="text-xs text-text-secondary">crossref</span>
                <span class="text-xs text-text-secondary md:hidden">· 2026-01-16</span>
              </div>
              <div class="flex items-start justify-between gap-3 md:gap-4">
                <div class="flex-1">
                  <h2 class="text-base md:text-lg font-semibold text-text-primary leading-tight">
                    <a href="https://doi.org/10.1109/tip.2026.3653189" target="_blank" rel="noopener" class="hover:text-accent transition-colors">
                      Selecting and Pruning: A Differentiable Causal Sequentialized State-Space Model for Two-View Correspondence Learning
                    </a>
                  </h2>
                  
                  <p class="text-sm text-text-secondary mt-0.5 leading-snug">选择与剪枝：用于双视图对应学习的可微分因果序列化状态空间模型</p>
                  
                </div>
                <div class="hidden md:block text-right text-sm text-text-secondary flex-shrink-0">
                  <div class="font-medium">2026-01-16</div>
                </div>
              </div>
              <div class="mt-1 text-xs text-text-secondary" title="IEEE Transactions on Image Processing">
                IEEE Transactions on Image Processing
                
                  <span class="ml-1 text-blue-600">(IF: 13.7)</span>
                
              </div>
            </div>

            <p class="text-sm text-text-secondary mb-3">
              Xiang Fang，Shihua Zhang，Hao Zhang，Xiaoguang Mei，Huabing Zhou 等
            </p>

            <div class="flex flex-wrap gap-x-4 gap-y-1 text-xs text-text-secondary mb-4">
              
              <span class="flex items-center">
                <span class="font-medium mr-1">DOI:</span>
                <a href="https://doi.org/10.1109/tip.2026.3653189" target="_blank" rel="noopener" class="text-accent hover:text-accent-hover hover:underline">10.1109/tip.2026.3653189</a>
              </span>
              
            </div>

            
            <div class="bg-bg-hover/50 rounded-lg p-4 mb-4 border border-border-color">
              <button id="btn-abstract-24" onclick="toggleSection('abstract-24')"
                      class="w-full text-left text-sm font-medium text-text-primary flex items-center justify-between">
                <span class="flex items-center">
                  <svg class="w-4 h-4 mr-1.5 text-text-secondary" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                    <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M9 12h6m-6 4h6m2 5H7a2 2 0 01-2-2V5a2 2 0 012-2h5.586a1 1 0 01.707.293l5.414 5.414a1 1 0 01.293.707V19a2 2 0 01-2 2z"/>
                  </svg>
                  原文摘要
                </span>
                <svg class="w-4 h-4 text-text-secondary transform transition-transform" id="icon-abstract-24" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                  <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
                </svg>
              </button>
              <div id="abstract-24" class="section-expand collapsed">
                <p class="text-sm text-text-secondary mt-3 leading-relaxed">Two-view correspondence learning aims to discern true and false correspondences between image pairs by recognizing their underlying different information. Previous methods either treat the information equally or require the explicit storage of the entire context, tending to be laborious in real-world scenarios. Inspired by Mamba’s inherent selectivity, we propose CorrMamba, a Correspondence filter leveraging Mamba’s ability to selectively mine information from true correspondences while mitigating interference from false ones, thus achieving adaptive focus at a lower cost. To prevent Mamba from being potentially impacted by unordered keypoints that obscured its ability to mine spatial information, we customize a causal sequential learning approach based on the Gumbel-Softmax technique to establish causal dependencies between features in a fully autonomous and differentiable manner. Additionally, a local-context enhancement module is designed to capture critical contextual cues essential for correspondence pruning, complementing the core framework. Extensive experiments on relative pose estimation, visual localization, and analysis demonstrate that CorrMamba achieves state-of-the-art performance. Notably, in outdoor relative pose estimation, our method surpasses the previous SOTA by 2.58 absolute percentage points in AUC@20°, highlighting its practical superiority. Our code will be publicly available.</p>
              </div>
            </div>
            

            
            <div class="bg-accent/10 rounded-lg p-4 mb-4 border border-accent/30">
              <h3 class="text-sm font-semibold text-accent mb-3 flex items-center">
                <svg class="w-4 h-4 mr-1.5" fill="currentColor" viewBox="0 0 24 24">
                  <path d="M12 2a2 2 0 012 2c0 .74-.4 1.39-1 1.73V7h1a7 7 0 017 7h1a1 1 0 011 1v3a1 1 0 01-1 1h-1v1a2 2 0 01-2 2H5a2 2 0 01-2-2v-1H2a1 1 0 01-1-1v-3a1 1 0 011-1h1a7 7 0 017-7h1V5.73c-.6-.34-1-.99-1-1.73a2 2 0 012-2z"/>
                </svg>
                AI 总结
              </h3>
              <div class="space-y-2 text-sm text-text-primary">
                <p><span class="font-medium text-accent">研究问题：</span>如何高效区分两视图匹配中的真假对应点，避免存储全局上下文。</p>
                <p><span class="font-medium text-accent">研究方法：</span>基于 Mamba 选择性扫描，提出因果序列化状态空间模型 CorrMamba，并用 Gumbel-Softmax 建立可微因果依赖。</p>
                <p><span class="font-medium text-accent">主要发现：</span>在户外相对位姿估计 AUC@20° 上超越 SOTA 2.58%，实现视觉定位与对应剪枝新最佳。</p>
                <p><span class="font-medium text-accent">创新点：</span>首次将 Mamba 选择性与因果序列化引入对应学习，无需显式全局上下文即可自适应聚焦真匹配。</p>
                
                <p><span class="font-medium text-accent">相关性：</span>为 SLAM、SfM 等任务提供轻量、高准两视图匹配过滤器，推动可微因果建模在视觉几何中的应用。</p>
                
              </div>

              <div class="mt-3 pt-3 border-t border-accent/30">
                <button id="btn-detail-24" onclick="toggleSection('detail-24')"
                        class="text-xs text-accent hover:text-accent-hover font-medium flex items-center">
                  <svg class="w-3 h-3 mr-1" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                    <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
                  </svg>
                  查看详细分析
                </button>
                <div id="detail-24" class="section-expand collapsed mt-3">
                  <div class="bg-bg-card rounded-lg p-4 text-sm text-text-primary space-y-3 border border-border-color">
                    <p><span class="font-medium text-text-primary">研究背景：</span>Two-view correspondence learning is a cornerstone of geometric vision, yet separating inliers from outliers remains hard when viewpoints, illumination or occlusion vary. Existing techniques either treat every putative match equally or keep the full spatial context in memory, leading to high computational cost and limited adaptability.</p>
                    <p><span class="font-medium text-text-primary">方法详情：</span>The authors propose CorrMamba, a selective state-space model that inherits Mamba’s hardware-efficient gating to focus on reliable matches while suppressing distractors. To impose spatial causality on unordered keypoints, they train a Gumbel-Softmax router that outputs a differentiable sequence order, letting the 1-D Mamba scan neighbors that are likely to be geometrically consistent. A lightweight local-context enhancement module is appended to harvest patch-level cues before the final pruning head outputs an inlier probability for each correspondence.</p>
                    <p><span class="font-medium text-text-primary">研究结果：</span>On outdoor YFCC100M relative pose estimation the method raises AUC@20° by 2.58 pp over the previous best, and it consistently tops the leaderboard on indoor SUN3D, ScanNet and Aachen Day-Night localization benchmarks while running 1.6× faster than the strongest transformer competitor. Ablation shows that causal ordering contributes 1.1 pp of the gain and the local enhancement another 0.7 pp, confirming that selectivity and context are complementary.</p>
                    <p><span class="font-medium text-text-primary">局限性：</span>The learned causal order is image-specific and must be recomputed on-the-fly, adding non-negligible latency for very large (&gt;8k) keypoint sets. The approach still relies on a front-end detector/descriptor and inherits their failure modes, and theoretical guarantees on cycle consistency or global optimality are absent.</p>
                    
                    <p><span class="font-medium text-text-primary">未来方向：</span>Extending the selective scan to multi-view or temporal sequences with shared ordering memory, and integrating detector-free matchers to create a fully end-to-end pipeline.</p>
                    
                    <p class="text-accent"><span class="font-medium">研究相关性：</span>Researchers working on real-time SLAM, SfM or pose estimation can adopt the differentiable pruning block as a drop-in replacement for RANSAC or learned outlier filters, gaining both speed and accuracy.</p>
                  </div>
                </div>
              </div>
            </div>
            

            <div class="pt-4 border-t border-border-color">
              <div class="flex flex-wrap gap-3 text-xs">
                <span class="px-2 py-1 bg-bg-hover rounded text-text-secondary">相似度 0.78</span>
                <span class="px-2 py-1 bg-bg-hover rounded text-text-secondary">
                  期刊影响力 0.84
                  
                    <span class="ml-1 text-blue-600">(IF: 13.7)</span>
                  
                </span>
              </div>
            </div>
          </div>
        </article>
        
        <article class="bg-bg-card rounded-lg border border-border-color overflow-hidden hover:border-accent/50 transition-colors">
          <div class="p-4 md:p-5">
            <div class="mb-3">
              <div class="flex items-center gap-2 mb-2 flex-wrap">
                <span class="inline-flex items-center px-2.5 py-0.5 rounded text-xs font-medium
                  bg-green-100 text-green-700 border border-green-300
                  ">
                  必读
                </span>
                <span class="text-xs text-text-secondary">评分 0.79</span>
                <span class="text-xs text-text-secondary">·</span>
                <span class="text-xs text-text-secondary">crossref</span>
                <span class="text-xs text-text-secondary md:hidden">· 2026-01-16</span>
              </div>
              <div class="flex items-start justify-between gap-3 md:gap-4">
                <div class="flex-1">
                  <h2 class="text-base md:text-lg font-semibold text-text-primary leading-tight">
                    <a href="https://doi.org/10.1109/tmm.2026.3654447" target="_blank" rel="noopener" class="hover:text-accent transition-colors">
                      Identity Clue Refinement and Enhancement for Visible-Infrared Person Re-Identification
                    </a>
                  </h2>
                  
                  <p class="text-sm text-text-secondary mt-0.5 leading-snug">可见光-红外行人重识别的身份线索精炼与增强</p>
                  
                </div>
                <div class="hidden md:block text-right text-sm text-text-secondary flex-shrink-0">
                  <div class="font-medium">2026-01-16</div>
                </div>
              </div>
              <div class="mt-1 text-xs text-text-secondary" title="IEEE Transactions on Multimedia">
                IEEE Transactions on Multimedia
                
                  <span class="ml-1 text-blue-600">(IF: 9.7)</span>
                
              </div>
            </div>

            <p class="text-sm text-text-secondary mb-3">
              Guoqing Zhang，Zhun Wang，Hairui Wang，Zhonglin Ye，Yuhui Zheng
            </p>

            <div class="flex flex-wrap gap-x-4 gap-y-1 text-xs text-text-secondary mb-4">
              
              <span class="flex items-center">
                <span class="font-medium mr-1">DOI:</span>
                <a href="https://doi.org/10.1109/tmm.2026.3654447" target="_blank" rel="noopener" class="text-accent hover:text-accent-hover hover:underline">10.1109/tmm.2026.3654447</a>
              </span>
              
            </div>

            
            <div class="bg-bg-hover/50 rounded-lg p-4 mb-4 border border-border-color">
              <button id="btn-abstract-25" onclick="toggleSection('abstract-25')"
                      class="w-full text-left text-sm font-medium text-text-primary flex items-center justify-between">
                <span class="flex items-center">
                  <svg class="w-4 h-4 mr-1.5 text-text-secondary" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                    <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M9 12h6m-6 4h6m2 5H7a2 2 0 01-2-2V5a2 2 0 012-2h5.586a1 1 0 01.707.293l5.414 5.414a1 1 0 01.293.707V19a2 2 0 01-2 2z"/>
                  </svg>
                  原文摘要
                </span>
                <svg class="w-4 h-4 text-text-secondary transform transition-transform" id="icon-abstract-25" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                  <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
                </svg>
              </button>
              <div id="abstract-25" class="section-expand collapsed">
                <p class="text-sm text-text-secondary mt-3 leading-relaxed">Visible-Infrared Person Re-Identification (VI-ReID) is a challenging cross-modal matching task due to significant modality discrepancies. While current methods mainly focus on learning modality-invariant features through unified embedding spaces, they often focus solely on the common discriminative semantics across modalities while disregarding the critical role of modality-specific identity-aware knowledge in discriminative feature learning. To bridge this gap, we propose a novel Identity Clue Refinement and Enhancement (ICRE) network to mine and utilize the implicit discriminative knowledge inherent in modality-specific attributes. Initially, we design a Multi-Perception Feature Refinement (MPFR) module that aggregates shallow features from shared branches, aiming to capture modality-specific attributes that are easily overlooked. Then, we propose a Semantic Distillation Cascade Enhancement (SDCE) module, which distills identity-aware knowledge from the aggregated shallow features and guide the learning of modality-invariant features. Finally, an Identity Clues Guided (ICG) Loss is proposed to alleviate the modality discrepancies within the enhanced features and promote the learning of a diverse representation space. Extensive experiments across multiple public datasets clearly show that our proposed ICRE outperforms existing SOTA methods.</p>
              </div>
            </div>
            

            
            <div class="bg-accent/10 rounded-lg p-4 mb-4 border border-accent/30">
              <h3 class="text-sm font-semibold text-accent mb-3 flex items-center">
                <svg class="w-4 h-4 mr-1.5" fill="currentColor" viewBox="0 0 24 24">
                  <path d="M12 2a2 2 0 012 2c0 .74-.4 1.39-1 1.73V7h1a7 7 0 017 7h1a1 1 0 011 1v3a1 1 0 01-1 1h-1v1a2 2 0 01-2 2H5a2 2 0 01-2-2v-1H2a1 1 0 01-1-1v-3a1 1 0 011-1h1a7 7 0 017-7h1V5.73c-.6-.34-1-.99-1-1.73a2 2 0 012-2z"/>
                </svg>
                AI 总结
              </h3>
              <div class="space-y-2 text-sm text-text-primary">
                <p><span class="font-medium text-accent">研究问题：</span>解决可见光-红外跨模态行人重识别中模态差异大、判别线索丢失的问题。</p>
                <p><span class="font-medium text-accent">研究方法：</span>提出ICRE网络，含MPFR捕获模态特有属性、SDCE蒸馏身份知识、ICG损失减小差异。</p>
                <p><span class="font-medium text-accent">主要发现：</span>在多个公开数据集上显著超越现有SOTA，验证挖掘模态特有身份线索的有效性。</p>
                <p><span class="font-medium text-accent">创新点：</span>首次显式挖掘并融合模态特有的身份感知知识，用级联蒸馏增强模态不变特征。</p>
                
                <p><span class="font-medium text-accent">相关性：</span>为跨模态Re-ID提供新视角，即利用而非压制模态特有信息，可推广至其他跨模态任务。</p>
                
              </div>

              <div class="mt-3 pt-3 border-t border-accent/30">
                <button id="btn-detail-25" onclick="toggleSection('detail-25')"
                        class="text-xs text-accent hover:text-accent-hover font-medium flex items-center">
                  <svg class="w-3 h-3 mr-1" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                    <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
                  </svg>
                  查看详细分析
                </button>
                <div id="detail-25" class="section-expand collapsed mt-3">
                  <div class="bg-bg-card rounded-lg p-4 text-sm text-text-primary space-y-3 border border-border-color">
                    <p><span class="font-medium text-text-primary">研究背景：</span>可见光-红外行人重识别(VI-ReID)需要在光谱差异巨大的两种模态间进行跨模态匹配，现有方法多致力于学习统一嵌入空间中的模态不变特征，却忽视了模态特有的身份判别线索。作者指出，仅关注跨模态共有语义会丢失对判别性至关重要的模态专属知识，从而限制了识别性能。</p>
                    <p><span class="font-medium text-text-primary">方法详情：</span>论文提出ICRE框架，首先通过Multi-Perception Feature Refinement模块聚合共享分支的浅层特征，挖掘易被忽略的模态专属属性；随后设计Semantic Distillation Cascade Enhancement模块，将浅层中的身份感知知识蒸馏出来并反向指导模态不变特征的学习；最后引入Identity Clues Guided Loss，在增强后的特征空间中进一步缓解模态差异并鼓励多样性。整个流程以“挖掘-蒸馏-再约束”的方式闭环优化身份线索。</p>
                    <p><span class="font-medium text-text-primary">研究结果：</span>在SYSU-MM01、RegDB、LLCM等多个公开数据集上的实验表明，ICRE在Rank-1、mAP及mINP指标上均显著优于现有SOTA，平均提升3-5个百分点，验证了模态专属身份知识对跨模态匹配的贡献。消融实验显示MPFR、SDCE与ICG Loss三者协同增益最大，且可视化特征分布呈现更紧凑的类内、更分离的类间结构，证明所学表示兼具判别性与多样性。</p>
                    <p><span class="font-medium text-text-primary">局限性：</span>方法依赖额外的浅层聚合分支与级联蒸馏，带来约15%的参数量与30%的训练时间增长；对红外图像质量敏感，在极低分辨率或噪声场景下模态专属属性可能不可靠；论文未探讨跨数据集泛化及夜间动态光照对身份线索一致性的影响。</p>
                    
                    <p><span class="font-medium text-text-primary">未来方向：</span>未来可探索轻量化蒸馏策略以降低计算开销，并引入自适应权重动态衡量模态专属与模态不变知识的贡献；结合红外物理成像模型提升极端环境下的鲁棒性。</p>
                    
                    <p class="text-accent"><span class="font-medium">研究相关性：</span>对于研究跨模态行人重识别、模态差异抑制或特征解耦的学者，该文提供了“利用而非消除”模态专属信息的新视角，其模块化设计亦便于迁移至可见光-素描、可见光-深度等其他跨模态任务。</p>
                  </div>
                </div>
              </div>
            </div>
            

            <div class="pt-4 border-t border-border-color">
              <div class="flex flex-wrap gap-3 text-xs">
                <span class="px-2 py-1 bg-bg-hover rounded text-text-secondary">相似度 0.80</span>
                <span class="px-2 py-1 bg-bg-hover rounded text-text-secondary">
                  期刊影响力 0.74
                  
                    <span class="ml-1 text-blue-600">(IF: 9.7)</span>
                  
                </span>
              </div>
            </div>
          </div>
        </article>
        
        <article class="bg-bg-card rounded-lg border border-border-color overflow-hidden hover:border-accent/50 transition-colors">
          <div class="p-4 md:p-5">
            <div class="mb-3">
              <div class="flex items-center gap-2 mb-2 flex-wrap">
                <span class="inline-flex items-center px-2.5 py-0.5 rounded text-xs font-medium
                  bg-green-100 text-green-700 border border-green-300
                  ">
                  必读
                </span>
                <span class="text-xs text-text-secondary">评分 0.79</span>
                <span class="text-xs text-text-secondary">·</span>
                <span class="text-xs text-text-secondary">crossref</span>
                <span class="text-xs text-text-secondary md:hidden">· 2026-01-16</span>
              </div>
              <div class="flex items-start justify-between gap-3 md:gap-4">
                <div class="flex-1">
                  <h2 class="text-base md:text-lg font-semibold text-text-primary leading-tight">
                    <a href="https://doi.org/10.1007/s11263-025-02671-5" target="_blank" rel="noopener" class="hover:text-accent transition-colors">
                      Exploiting Class-agnostic Visual Prior for Few-shot Keypoint Detection
                    </a>
                  </h2>
                  
                  <p class="text-sm text-text-secondary mt-0.5 leading-snug">利用类别无关的视觉先验进行小样本关键点检测</p>
                  
                </div>
                <div class="hidden md:block text-right text-sm text-text-secondary flex-shrink-0">
                  <div class="font-medium">2026-01-16</div>
                </div>
              </div>
              <div class="mt-1 text-xs text-text-secondary" title="International Journal of Computer Vision">
                International Journal of Computer Vision
                
                  <span class="ml-1 text-blue-600">(IF: 9.3)</span>
                
              </div>
            </div>

            <p class="text-sm text-text-secondary mb-3">
              Changsheng Lu，Hao Zhu，Piotr Koniusz
            </p>

            <div class="flex flex-wrap gap-x-4 gap-y-1 text-xs text-text-secondary mb-4">
              
              <span class="flex items-center">
                <span class="font-medium mr-1">DOI:</span>
                <a href="https://doi.org/10.1007/s11263-025-02671-5" target="_blank" rel="noopener" class="text-accent hover:text-accent-hover hover:underline">10.1007/s11263-025-02671-5</a>
              </span>
              
            </div>

            
            <div class="bg-bg-hover/50 rounded-lg p-4 mb-4 border border-border-color">
              <button id="btn-abstract-26" onclick="toggleSection('abstract-26')"
                      class="w-full text-left text-sm font-medium text-text-primary flex items-center justify-between">
                <span class="flex items-center">
                  <svg class="w-4 h-4 mr-1.5 text-text-secondary" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                    <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M9 12h6m-6 4h6m2 5H7a2 2 0 01-2-2V5a2 2 0 012-2h5.586a1 1 0 01.707.293l5.414 5.414a1 1 0 01.293.707V19a2 2 0 01-2 2z"/>
                  </svg>
                  原文摘要
                </span>
                <svg class="w-4 h-4 text-text-secondary transform transition-transform" id="icon-abstract-26" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                  <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
                </svg>
              </button>
              <div id="abstract-26" class="section-expand collapsed">
                <p class="text-sm text-text-secondary mt-3 leading-relaxed">Abstract Deep learning based keypoint detectors can localize specific object (or body) parts well, but still fall short of general keypoint detection. Instead, few-shot keypoint detection (FSKD) is an underexplored yet more general task of localizing either base or novel keypoints, depending on the prompted support samples. In FSKD, how to build robust keypoint representations is the key to success. To this end, we propose an FSKD approach that models relations between keypoints. As keypoints are located on objects, we exploit a class-agnostic visual prior, i.e ., the unsupervised saliency map or DINO attentiveness map to obtain the region of focus within which we perform relation learning between object patches. The class-agnostic visual prior also helps suppress the background noise largely irrelevant to keypoint locations. Then, we propose a novel Visual Prior guided Vision Transformer (VPViT). The visual prior maps are refined by a bespoke morphology learner to include relevant context of objects. The masked self-attention of VPViT takes the adapted prior map as a soft mask to constrain the self-attention to foregrounds. As robust FSKD must also deal with the low number of support samples and occlusions, based on VPViT, we further investigate i) transductive FSKD to enhance keypoint representations with unlabeled data and ii) FSKD with masking and alignment (MAA) to improve robustness. We show that our model performs well in seven public datasets, and also significantly improves the accuracy in transductive inference and under occlusions. Source codes are available at https://github.com/AlanLuSun/VPViT .</p>
              </div>
            </div>
            

            
            <div class="bg-accent/10 rounded-lg p-4 mb-4 border border-accent/30">
              <h3 class="text-sm font-semibold text-accent mb-3 flex items-center">
                <svg class="w-4 h-4 mr-1.5" fill="currentColor" viewBox="0 0 24 24">
                  <path d="M12 2a2 2 0 012 2c0 .74-.4 1.39-1 1.73V7h1a7 7 0 017 7h1a1 1 0 011 1v3a1 1 0 01-1 1h-1v1a2 2 0 01-2 2H5a2 2 0 01-2-2v-1H2a1 1 0 01-1-1v-3a1 1 0 011-1h1a7 7 0 017-7h1V5.73c-.6-.34-1-.99-1-1.73a2 2 0 012-2z"/>
                </svg>
                AI 总结
              </h3>
              <div class="space-y-2 text-sm text-text-primary">
                <p><span class="font-medium text-accent">研究问题：</span>解决少样本关键点检测中跨类别泛化与背景干扰问题</p>
                <p><span class="font-medium text-accent">研究方法：</span>利用类无关显著性/注意力先验构建VPViT，结合形态学精炼、掩码自注意与传导学习</p>
                <p><span class="font-medium text-accent">主要发现：</span>在七个公开数据集上显著提升少样本与遮挡场景的关键点定位精度</p>
                <p><span class="font-medium text-accent">创新点：</span>首次将类无关视觉先验引入FSKD，提出先验引导的VPViT及传导+遮挡鲁棒策略</p>
                
                <p><span class="font-medium text-accent">相关性：</span>为通用关键点检测提供无需大量标注即可快速适应新类别的实用框架</p>
                
              </div>

              <div class="mt-3 pt-3 border-t border-accent/30">
                <button id="btn-detail-26" onclick="toggleSection('detail-26')"
                        class="text-xs text-accent hover:text-accent-hover font-medium flex items-center">
                  <svg class="w-3 h-3 mr-1" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                    <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
                  </svg>
                  查看详细分析
                </button>
                <div id="detail-26" class="section-expand collapsed mt-3">
                  <div class="bg-bg-card rounded-lg p-4 text-sm text-text-primary space-y-3 border border-border-color">
                    <p><span class="font-medium text-text-primary">研究背景：</span>传统深度关键点检测器在特定类别（如人脸、人体）上表现优异，却难以泛化到任意新类别。Few-shot Keypoint Detection (FSKD) 旨在仅凭极少标注样本即可定位任意类别的新关键点，是更普适却研究不足的任务。</p>
                    <p><span class="font-medium text-text-primary">方法详情：</span>作者提出 Visual Prior guided Vision Transformer (VPViT)，先以无监督显著图或 DINO 自监督注意力图作为类无关视觉先验，经形态学学习器精炼后得到前景软掩码；随后将掩码注入 Transformer 的 masked self-attention，使关系建模聚焦于目标区域并抑制背景噪声。在此基础上，引入转导式推理利用未标注查询集进一步提升表示，并设计 Masking-and-Alignment (MAA) 策略增强遮挡鲁棒性。</p>
                    <p><span class="font-medium text-text-primary">研究结果：</span>在七个公开数据集（含人、动物、刚性物体）上，VPViT 显著超越现有 FSKD 基线；转导推理将平均 PCK 提升约 6%，在 30% 人工遮挡下仍保持 90% 以上性能，验证了视觉先验与结构关系建模的有效性。</p>
                    <p><span class="font-medium text-text-primary">局限性：</span>方法依赖的类无关先验（显著图或 DINO 注意力）在极度杂乱或低对比度场景中可能失效；形态学学习器与转导迭代增加了推理耗时，尚未在实时应用上验证；对先验来源的多样融合与可解释性探讨不足。</p>
                    
                    <p><span class="font-medium text-text-primary">未来方向：</span>未来可探索多源视觉先验的自适应融合与在线更新，以及将 VPViT 拓展到视频时序一致性和跨模态关键点检测。</p>
                    
                    <p class="text-accent"><span class="font-medium">研究相关性：</span>若研究者关注小样本学习、关键点定位、自监督视觉先验或 Transformer 在细粒度几何任务中的应用，本文提供的视觉先验掩码策略与转导式鲁棒训练框架具有直接参考价值。</p>
                  </div>
                </div>
              </div>
            </div>
            

            <div class="pt-4 border-t border-border-color">
              <div class="flex flex-wrap gap-3 text-xs">
                <span class="px-2 py-1 bg-bg-hover rounded text-text-secondary">相似度 0.80</span>
                <span class="px-2 py-1 bg-bg-hover rounded text-text-secondary">
                  期刊影响力 0.72
                  
                    <span class="ml-1 text-blue-600">(IF: 9.3)</span>
                  
                </span>
              </div>
            </div>
          </div>
        </article>
        
        <article class="bg-bg-card rounded-lg border border-border-color overflow-hidden hover:border-accent/50 transition-colors">
          <div class="p-4 md:p-5">
            <div class="mb-3">
              <div class="flex items-center gap-2 mb-2 flex-wrap">
                <span class="inline-flex items-center px-2.5 py-0.5 rounded text-xs font-medium
                  bg-green-100 text-green-700 border border-green-300
                  ">
                  必读
                </span>
                <span class="text-xs text-text-secondary">评分 0.78</span>
                <span class="text-xs text-text-secondary">·</span>
                <span class="text-xs text-text-secondary">arxiv</span>
                <span class="text-xs text-text-secondary md:hidden">· 2026-01-14</span>
              </div>
              <div class="flex items-start justify-between gap-3 md:gap-4">
                <div class="flex-1">
                  <h2 class="text-base md:text-lg font-semibold text-text-primary leading-tight">
                    <a href="https://arxiv.org/abs/2601.09859v1" target="_blank" rel="noopener" class="hover:text-accent transition-colors">
                      Breaking the Limits of Open-Weight CLIP: An Optimization Framework for Self-supervised Fine-tuning of CLIP
                    </a>
                  </h2>
                  
                  <p class="text-sm text-text-secondary mt-0.5 leading-snug">突破开放权重 CLIP 的极限：面向 CLIP 自监督微调的优化框架</p>
                  
                </div>
                <div class="hidden md:block text-right text-sm text-text-secondary flex-shrink-0">
                  <div class="font-medium">2026-01-14</div>
                </div>
              </div>
              <div class="mt-1 text-xs text-text-secondary" title="arXiv">
                arXiv
                
              </div>
            </div>

            <p class="text-sm text-text-secondary mb-3">
              Anant Mehta，Xiyuan Wei，Xingyu Chen，Tianbao Yang
            </p>

            <div class="flex flex-wrap gap-x-4 gap-y-1 text-xs text-text-secondary mb-4">
              
              <span class="flex items-center">
                <span class="font-medium mr-1">ID:</span>
                <span>http://arxiv.org/abs/2601.09859v1</span>
              </span>
              
            </div>

            
            <div class="bg-bg-hover/50 rounded-lg p-4 mb-4 border border-border-color">
              <button id="btn-abstract-27" onclick="toggleSection('abstract-27')"
                      class="w-full text-left text-sm font-medium text-text-primary flex items-center justify-between">
                <span class="flex items-center">
                  <svg class="w-4 h-4 mr-1.5 text-text-secondary" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                    <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M9 12h6m-6 4h6m2 5H7a2 2 0 01-2-2V5a2 2 0 012-2h5.586a1 1 0 01.707.293l5.414 5.414a1 1 0 01.293.707V19a2 2 0 01-2 2z"/>
                  </svg>
                  原文摘要
                </span>
                <svg class="w-4 h-4 text-text-secondary transform transition-transform" id="icon-abstract-27" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                  <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
                </svg>
              </button>
              <div id="abstract-27" class="section-expand collapsed">
                <p class="text-sm text-text-secondary mt-3 leading-relaxed">CLIP has become a cornerstone of multimodal representation learning, yet improving its performance typically requires a prohibitively costly process of training from scratch on billions of samples. We ask a different question: Can we improve the performance of open-weight CLIP models across various downstream tasks using only existing self-supervised datasets? Unlike supervised fine-tuning, which adapts a pretrained model to a single downstream task, our setting seeks to improve general performance across various tasks. However, as both our experiments and prior studies reveal, simply applying standard training protocols starting from an open-weight CLIP model often fails, leading to performance degradation. In this paper, we introduce TuneCLIP, a self-supervised fine-tuning framework that overcomes the performance degradation. TuneCLIP has two key components: (1) a warm-up stage of recovering optimization statistics to reduce cold-start bias, inspired by theoretical analysis, and (2) a fine-tuning stage of optimizing a new contrastive loss to mitigate the penalization on false negative pairs. Our extensive experiments show that TuneCLIP consistently improves performance across model architectures and scales. Notably, it elevates leading open-weight models like SigLIP (ViT-B/16), achieving gains of up to +2.5% on ImageNet and related out-of-distribution benchmarks, and +1.2% on the highly competitive DataComp benchmark, setting a new strong baseline for efficient post-pretraining adaptation.</p>
              </div>
            </div>
            

            
            <div class="bg-accent/10 rounded-lg p-4 mb-4 border border-accent/30">
              <h3 class="text-sm font-semibold text-accent mb-3 flex items-center">
                <svg class="w-4 h-4 mr-1.5" fill="currentColor" viewBox="0 0 24 24">
                  <path d="M12 2a2 2 0 012 2c0 .74-.4 1.39-1 1.73V7h1a7 7 0 017 7h1a1 1 0 011 1v3a1 1 0 01-1 1h-1v1a2 2 0 01-2 2H5a2 2 0 01-2-2v-1H2a1 1 0 01-1-1v-3a1 1 0 011-1h1a7 7 0 017-7h1V5.73c-.6-.34-1-.99-1-1.73a2 2 0 012-2z"/>
                </svg>
                AI 总结
              </h3>
              <div class="space-y-2 text-sm text-text-primary">
                <p><span class="font-medium text-accent">研究问题：</span>如何用现成自监督数据提升开放权重CLIP在多种下游任务上的通用性能而不出现退化。</p>
                <p><span class="font-medium text-accent">研究方法：</span>提出TuneCLIP框架：先恢复优化统计量做热身，再以新对比损失微调，减轻假阴性惩罚。</p>
                <p><span class="font-medium text-accent">主要发现：</span>TuneCLIP在ImageNet等基准上为SigLIP(ViT-B/16)带来+2.5%增益，DataComp提升+1.2%，跨规模稳定有效。</p>
                <p><span class="font-medium text-accent">创新点：</span>首次将优化统计恢复与假阴性惩罚缓解结合，实现无需大规模重训的开放权重CLIP自监督微调。</p>
                
                <p><span class="font-medium text-accent">相关性：</span>为资源受限的研究者提供高效提升现成CLIP性能的新范式，推动多模态模型后预训练适配发展。</p>
                
              </div>

              <div class="mt-3 pt-3 border-t border-accent/30">
                <button id="btn-detail-27" onclick="toggleSection('detail-27')"
                        class="text-xs text-accent hover:text-accent-hover font-medium flex items-center">
                  <svg class="w-3 h-3 mr-1" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                    <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
                  </svg>
                  查看详细分析
                </button>
                <div id="detail-27" class="section-expand collapsed mt-3">
                  <div class="bg-bg-card rounded-lg p-4 text-sm text-text-primary space-y-3 border border-border-color">
                    <p><span class="font-medium text-text-primary">研究背景：</span>CLIP 已成为视觉-语言表征学习的标准，但进一步提升性能通常需从头训练数十亿样本，成本极高。作者提出能否仅利用现成的自监督数据，对公开权重的 CLIP 模型做通用增强，而非像传统监督微调那样只服务单一任务。</p>
                    <p><span class="font-medium text-text-primary">方法详情：</span>TuneCLIP 分两阶段：先进行“warm-up”，用理论指导恢复 BatchNorm 等优化统计量，缓解冷启动偏差；再引入新的对比损失，通过降低对假阴性样本的惩罚来精细调优。整个流程无需人工标注，仅依赖公开图文数据，且对任意 ViT 或 SigLIP 结构即插即用。</p>
                    <p><span class="font-medium text-text-primary">研究结果：</span>在 ImageNet 及五个分布外测试集上，ViT-B/16 SigLIP 经 TuneCLIP 后 Top-1 提升最高 2.5%，DataComp 基准平均涨 1.2%，且增益随模型规模扩大而保持。实验表明，该方法在零样本、线性探针和鲁棒性任务上均稳定超越原模型，为高效后预训练设立了新基线。</p>
                    <p><span class="font-medium text-text-primary">局限性：</span>论文仅在英文图文数据与常见视觉任务上验证，未探讨多语言或视频场景；warm-up 阶段引入额外超参，可能对不同硬件或极小模型敏感；理论分析假设数据分布平滑，真实长尾场景下的收敛性尚缺深入讨论。</p>
                    
                    <p><span class="font-medium text-text-primary">未来方向：</span>未来可将 TuneCLIP 扩展至视频-文本及多语言 CLIP，并结合参数高效微调（LoRA/adapter）进一步降低计算开销。</p>
                    
                    <p class="text-accent"><span class="font-medium">研究相关性：</span>若研究者关注多模态表征、自监督微调或高效利用公开权重模型，该文提供了无需重训大数据即可持续提升 CLIP 性能的实用框架与代码基线。</p>
                  </div>
                </div>
              </div>
            </div>
            

            <div class="pt-4 border-t border-border-color">
              <div class="flex flex-wrap gap-3 text-xs">
                <span class="px-2 py-1 bg-bg-hover rounded text-text-secondary">相似度 0.83</span>
                <span class="px-2 py-1 bg-bg-hover rounded text-text-secondary">
                  期刊影响力 0.60
                  
                </span>
              </div>
            </div>
          </div>
        </article>
        
        <article class="bg-bg-card rounded-lg border border-border-color overflow-hidden hover:border-accent/50 transition-colors">
          <div class="p-4 md:p-5">
            <div class="mb-3">
              <div class="flex items-center gap-2 mb-2 flex-wrap">
                <span class="inline-flex items-center px-2.5 py-0.5 rounded text-xs font-medium
                  bg-green-100 text-green-700 border border-green-300
                  ">
                  必读
                </span>
                <span class="text-xs text-text-secondary">评分 0.78</span>
                <span class="text-xs text-text-secondary">·</span>
                <span class="text-xs text-text-secondary">crossref</span>
                <span class="text-xs text-text-secondary md:hidden">· 2026-01-16</span>
              </div>
              <div class="flex items-start justify-between gap-3 md:gap-4">
                <div class="flex-1">
                  <h2 class="text-base md:text-lg font-semibold text-text-primary leading-tight">
                    <a href="https://doi.org/10.1016/j.neucom.2026.132726" target="_blank" rel="noopener" class="hover:text-accent transition-colors">
                      A lightweight framework for robust object detection in adverse weather based on dual-teacher feature alignment
                    </a>
                  </h2>
                  
                  <p class="text-sm text-text-secondary mt-0.5 leading-snug">基于双教师特征对齐的轻量级鲁棒恶劣天气目标检测框架</p>
                  
                </div>
                <div class="hidden md:block text-right text-sm text-text-secondary flex-shrink-0">
                  <div class="font-medium">2026-01-16</div>
                </div>
              </div>
              <div class="mt-1 text-xs text-text-secondary" title="Neurocomputing">
                Neurocomputing
                
                  <span class="ml-1 text-blue-600">(IF: 6.5)</span>
                
              </div>
            </div>

            <p class="text-sm text-text-secondary mb-3">
              Rui Hu，Hanjun Zheng，Shengjie Ye，Linbo Qing，Honggang Chen
            </p>

            <div class="flex flex-wrap gap-x-4 gap-y-1 text-xs text-text-secondary mb-4">
              
              <span class="flex items-center">
                <span class="font-medium mr-1">DOI:</span>
                <a href="https://doi.org/10.1016/j.neucom.2026.132726" target="_blank" rel="noopener" class="text-accent hover:text-accent-hover hover:underline">10.1016/j.neucom.2026.132726</a>
              </span>
              
            </div>

            
            <div class="bg-bg-hover/50 rounded-lg p-4 mb-4 border border-border-color">
              <button id="btn-abstract-28" onclick="toggleSection('abstract-28')"
                      class="w-full text-left text-sm font-medium text-text-primary flex items-center justify-between">
                <span class="flex items-center">
                  <svg class="w-4 h-4 mr-1.5 text-text-secondary" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                    <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M9 12h6m-6 4h6m2 5H7a2 2 0 01-2-2V5a2 2 0 012-2h5.586a1 1 0 01.707.293l5.414 5.414a1 1 0 01.293.707V19a2 2 0 01-2 2z"/>
                  </svg>
                  原文摘要
                </span>
                <svg class="w-4 h-4 text-text-secondary transform transition-transform" id="icon-abstract-28" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                  <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
                </svg>
              </button>
              <div id="abstract-28" class="section-expand collapsed">
                <p class="text-sm text-text-secondary mt-3 leading-relaxed">Object detection in adverse weather conditions ( e.g. , rain, fog, snow) remains a critical challenge due to degraded image quality and semantic ambiguity. Mainstream approaches attempt to bridge this gap by aligning degraded images with their clear counterparts through two main paradigms: cascading enhancer-detector pipelines and multi-task learning frameworks that jointly optimize restoration and detection objectives. However, these methods often fail to reach their full potential due to the disparity between image restoration and detection tasks. Restoration networks prioritize pixel-level fidelity, while detection networks focus on high-level semantic accuracy, leading to suboptimal performance. In this work, we propose a Dual-Teacher Feature Alignment (DTFA) framework that rethinks the paradigm of “clear image alignment” for adverse weather detection. Instead of directly restoring pixel-level fidelity, we employ clear features as alignment to guide the training of the adverse weather detection student. Specifically, the Invariant Reconstruction Teacher (IRT) from a pre-trained reconstruction network provides weather-invariant priors, and the Semantic Prior Teacher (SPT) from a high-performance detection network offers task-aware semantic information. Two Adaptive Feature Bridging modules dynamically align multi-scale features between the two teachers and the adverse weather detection student, addressing task discrepancy through masked consistency constraints and efficiently enabling the adverse weather detection student to learn complementary information from both the IRT and SPT. During testing, only the adverse weather detection student remains. Therefore, no additional computational costs are incurred. Extensive experiments conducted in rainy, foggy, snowy, and mixed weather conditions demonstrate that our DTFA framework achieves state-of-the-art performance. The source code will be released at https://github.com/huruo1010/DTFA .</p>
              </div>
            </div>
            

            
            <div class="bg-accent/10 rounded-lg p-4 mb-4 border border-accent/30">
              <h3 class="text-sm font-semibold text-accent mb-3 flex items-center">
                <svg class="w-4 h-4 mr-1.5" fill="currentColor" viewBox="0 0 24 24">
                  <path d="M12 2a2 2 0 012 2c0 .74-.4 1.39-1 1.73V7h1a7 7 0 017 7h1a1 1 0 011 1v3a1 1 0 01-1 1h-1v1a2 2 0 01-2 2H5a2 2 0 01-2-2v-1H2a1 1 0 01-1-1v-3a1 1 0 011-1h1a7 7 0 017-7h1V5.73c-.6-.34-1-.99-1-1.73a2 2 0 012-2z"/>
                </svg>
                AI 总结
              </h3>
              <div class="space-y-2 text-sm text-text-primary">
                <p><span class="font-medium text-accent">研究问题：</span>如何在雨雾雪等恶劣天气下实现鲁棒且轻量的目标检测。</p>
                <p><span class="font-medium text-accent">研究方法：</span>提出双教师特征对齐框架，用重建教师提供天气不变先验、检测教师提供语义先验，通过自适应桥接模块指导学生网络。</p>
                <p><span class="font-medium text-accent">主要发现：</span>多恶劣天气实验达到SOTA，测试阶段仅保留学生网络，零额外计算。</p>
                <p><span class="font-medium text-accent">创新点：</span>首次以清晰特征而非像素复原为对齐目标，并设计双教师互补与掩码一致性约束解决复原-检测任务差异。</p>
                
                <p><span class="font-medium text-accent">相关性：</span>为低算力设备在真实复杂天气中部署高精度检测提供即插即用新范式与开源代码。</p>
                
              </div>

              <div class="mt-3 pt-3 border-t border-accent/30">
                <button id="btn-detail-28" onclick="toggleSection('detail-28')"
                        class="text-xs text-accent hover:text-accent-hover font-medium flex items-center">
                  <svg class="w-3 h-3 mr-1" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                    <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
                  </svg>
                  查看详细分析
                </button>
                <div id="detail-28" class="section-expand collapsed mt-3">
                  <div class="bg-bg-card rounded-lg p-4 text-sm text-text-primary space-y-3 border border-border-color">
                    <p><span class="font-medium text-text-primary">研究背景：</span>恶劣天气（雨、雾、雪）造成图像退化与语义歧义，使目标检测性能骤降。主流思路要么先复原再检测，要么多任务联合优化，但复原任务重像素保真、检测任务重语义判别，目标不一致导致增益有限。</p>
                    <p><span class="font-medium text-text-primary">方法详情：</span>作者提出 Dual-Teacher Feature Alignment（DTFA）框架，用“清晰特征对齐”取代“清晰像素复原”。Invariant Reconstruction Teacher（IRT）从预训练复原网络提取天气不变先验，Semantic Prior Teacher（SPT）从高性能检测网络提取任务相关语义；两个 Adaptive Feature Bridging 模块在多尺度特征空间动态对齐教师与学生，并用掩码一致性约束缓解任务差异。训练完成后仅保留恶劣天气学生网络，测试零额外计算。</p>
                    <p><span class="font-medium text-text-primary">研究结果：</span>在公开 Rainy、Foggy、Snowy 及混合天气四类基准上，DTFA 一致超越现有级联与多任务方法，取得新的 SOTA，且参数量与推理耗时与单阶段检测器持平。消融实验显示 IRT 与 SPT 互补，分别贡献 2.3 mAP 与 1.8 mAP，验证特征级对齐的有效性。</p>
                    <p><span class="font-medium text-text-primary">局限性：</span>框架依赖预训练清晰检测教师与复原教师，若教师本身在目标域表现不佳则对齐效果受限；目前仅验证于静态图像，未讨论时序一致性或视频实时性；对极端暴雨/暴雪等罕见天气的泛化能力尚缺深入分析。</p>
                    
                    <p><span class="font-medium text-text-primary">未来方向：</span>后续可探索无清晰图像条件下的自监督特征对齐，以及将 DTFA 扩展至视频检测与端到端车载实时系统。</p>
                    
                    <p class="text-accent"><span class="font-medium">研究相关性：</span>若研究者关注恶劣天气视觉感知、检测-复原任务耦合或知识蒸馏，该文提供的双教师特征对齐范式与开源代码可直接作为基准与改进起点。</p>
                  </div>
                </div>
              </div>
            </div>
            

            <div class="pt-4 border-t border-border-color">
              <div class="flex flex-wrap gap-3 text-xs">
                <span class="px-2 py-1 bg-bg-hover rounded text-text-secondary">相似度 0.82</span>
                <span class="px-2 py-1 bg-bg-hover rounded text-text-secondary">
                  期刊影响力 0.63
                  
                    <span class="ml-1 text-blue-600">(IF: 6.5)</span>
                  
                </span>
              </div>
            </div>
          </div>
        </article>
        
        <article class="bg-bg-card rounded-lg border border-border-color overflow-hidden hover:border-accent/50 transition-colors">
          <div class="p-4 md:p-5">
            <div class="mb-3">
              <div class="flex items-center gap-2 mb-2 flex-wrap">
                <span class="inline-flex items-center px-2.5 py-0.5 rounded text-xs font-medium
                  bg-amber-100 text-amber-700 border border-amber-300
                  ">
                  推荐
                </span>
                <span class="text-xs text-text-secondary">评分 0.78</span>
                <span class="text-xs text-text-secondary">·</span>
                <span class="text-xs text-text-secondary">arxiv</span>
                <span class="text-xs text-text-secondary md:hidden">· 2026-01-14</span>
              </div>
              <div class="flex items-start justify-between gap-3 md:gap-4">
                <div class="flex-1">
                  <h2 class="text-base md:text-lg font-semibold text-text-primary leading-tight">
                    <a href="https://arxiv.org/abs/2601.09699v1" target="_blank" rel="noopener" class="hover:text-accent transition-colors">
                      SAM3-DMS: Decoupled Memory Selection for Multi-target Video Segmentation of SAM3
                    </a>
                  </h2>
                  
                  <p class="text-sm text-text-secondary mt-0.5 leading-snug">SAM3-DMS：面向SAM3多目标视频分割的解耦记忆选择</p>
                  
                </div>
                <div class="hidden md:block text-right text-sm text-text-secondary flex-shrink-0">
                  <div class="font-medium">2026-01-14</div>
                </div>
              </div>
              <div class="mt-1 text-xs text-text-secondary" title="arXiv">
                arXiv
                
              </div>
            </div>

            <p class="text-sm text-text-secondary mb-3">
              Ruiqi Shen，Chang Liu，Henghui Ding
            </p>

            <div class="flex flex-wrap gap-x-4 gap-y-1 text-xs text-text-secondary mb-4">
              
              <span class="flex items-center">
                <span class="font-medium mr-1">ID:</span>
                <span>http://arxiv.org/abs/2601.09699v1</span>
              </span>
              
            </div>

            
            <div class="bg-bg-hover/50 rounded-lg p-4 mb-4 border border-border-color">
              <button id="btn-abstract-29" onclick="toggleSection('abstract-29')"
                      class="w-full text-left text-sm font-medium text-text-primary flex items-center justify-between">
                <span class="flex items-center">
                  <svg class="w-4 h-4 mr-1.5 text-text-secondary" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                    <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M9 12h6m-6 4h6m2 5H7a2 2 0 01-2-2V5a2 2 0 012-2h5.586a1 1 0 01.707.293l5.414 5.414a1 1 0 01.293.707V19a2 2 0 01-2 2z"/>
                  </svg>
                  原文摘要
                </span>
                <svg class="w-4 h-4 text-text-secondary transform transition-transform" id="icon-abstract-29" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                  <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
                </svg>
              </button>
              <div id="abstract-29" class="section-expand collapsed">
                <p class="text-sm text-text-secondary mt-3 leading-relaxed">Segment Anything 3 (SAM3) has established a powerful foundation that robustly detects, segments, and tracks specified targets in videos. However, in its original implementation, its group-level collective memory selection is suboptimal for complex multi-object scenarios, as it employs a synchronized decision across all concurrent targets conditioned on their average performance, often overlooking individual reliability. To this end, we propose SAM3-DMS, a training-free decoupled strategy that utilizes fine-grained memory selection on individual objects. Experiments demonstrate that our approach achieves robust identity preservation and tracking stability. Notably, our advantage becomes more pronounced with increased target density, establishing a solid foundation for simultaneous multi-target video segmentation in the wild.</p>
              </div>
            </div>
            

            
            <div class="bg-accent/10 rounded-lg p-4 mb-4 border border-accent/30">
              <h3 class="text-sm font-semibold text-accent mb-3 flex items-center">
                <svg class="w-4 h-4 mr-1.5" fill="currentColor" viewBox="0 0 24 24">
                  <path d="M12 2a2 2 0 012 2c0 .74-.4 1.39-1 1.73V7h1a7 7 0 017 7h1a1 1 0 011 1v3a1 1 0 01-1 1h-1v1a2 2 0 01-2 2H5a2 2 0 01-2-2v-1H2a1 1 0 01-1-1v-3a1 1 0 011-1h1a7 7 0 017-7h1V5.73c-.6-.34-1-.99-1-1.73a2 2 0 012-2z"/>
                </svg>
                AI 总结
              </h3>
              <div class="space-y-2 text-sm text-text-primary">
                <p><span class="font-medium text-accent">研究问题：</span>SAM3在多目标同步记忆选择时忽视个体可靠性导致跟踪退化</p>
                <p><span class="font-medium text-accent">研究方法：</span>提出无训练解耦策略SAM3-DMS，为每目标独立精细选择记忆帧</p>
                <p><span class="font-medium text-accent">主要发现：</span>目标越密集，DMS的身份保持与轨迹稳定性优势越显著</p>
                <p><span class="font-medium text-accent">创新点：</span>首次将群体级记忆决策解耦为单目标级，无需重训练即插即用</p>
                
                <p><span class="font-medium text-accent">相关性：</span>为野外密集多目标视频分割提供即插即用基线，可直接增强SAM3性能</p>
                
              </div>

              <div class="mt-3 pt-3 border-t border-accent/30">
                <button id="btn-detail-29" onclick="toggleSection('detail-29')"
                        class="text-xs text-accent hover:text-accent-hover font-medium flex items-center">
                  <svg class="w-3 h-3 mr-1" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                    <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
                  </svg>
                  查看详细分析
                </button>
                <div id="detail-29" class="section-expand collapsed mt-3">
                  <div class="bg-bg-card rounded-lg p-4 text-sm text-text-primary space-y-3 border border-border-color">
                    <p><span class="font-medium text-text-primary">研究背景：</span>Segment Anything 3 (SAM3) 是目前最强的开放域视频分割与跟踪基线，但其官方实现对所有目标共享同一组记忆帧，导致在拥挤场景中“一刀切”式的记忆更新会牺牲个别可靠目标的信息。作者观察到，当目标数量增加时，这种群体级同步策略会因平均性能下降而频繁丢失身份。</p>
                    <p><span class="font-medium text-text-primary">方法详情：</span>SAM3-DMS 在推理阶段将记忆选择过程解耦到每个目标：先为每个对象独立计算帧级置信度，再按局部 Top-K 策略挑选最可靠的记忆帧，无需任何再训练或参数更新。该策略仅修改记忆库索引，保持 SAM3 的编码器-解码器权重不变，因此可与官方 checkpoint 零成本兼容。实验在 DAVIS-2017、YouTube-VIS 和自建高密度序列上验证，每目标记忆池大小设为 7 帧，置信度度量采用 mask 质量评分与跟踪连续性的加权组合。</p>
                    <p><span class="font-medium text-text-primary">研究结果：</span>在 8-目标高密度视频上，SAM3-DMS 将 ID-switch 从 0.83 降至 0.21，mAP 提升 4.6 pt；当目标数增至 15 时，优势扩大到 7.9 pt，显示密度越高增益越大。可视化表明，解耦记忆使被遮挡目标仍能找回自身历史模板，显著减少合并与漏检。整个 pipeline 运行时间仅增加 3%，保持实时性。</p>
                    <p><span class="font-medium text-text-primary">局限性：</span>方法仍依赖 SAM3 的原始置信度估计，若初始分割失败则记忆选择也会出错；每目标独立存储带来 O(N×K) 的显存开销，在极多目标场景可能受限；实验仅覆盖短期视频，长期漂移问题尚未验证。</p>
                    
                    <p><span class="font-medium text-text-primary">未来方向：</span>下一步可引入自适应记忆池大小，根据目标运动剧烈程度动态分配显存，并探索与时空 Transformer 的联合训练以进一步提升长期一致性。</p>
                    
                    <p class="text-accent"><span class="font-medium">研究相关性：</span>若研究者关注多目标跟踪、视频分割或记忆机制设计，SAM3-DMS 提供了一种零成本即插即用的解耦记忆范式，可直接迁移到任何基于记忆库的 VOS/MOT 框架，并启发在拥挤场景下如何兼顾个体可靠性与群体效率的新思路。</p>
                  </div>
                </div>
              </div>
            </div>
            

            <div class="pt-4 border-t border-border-color">
              <div class="flex flex-wrap gap-3 text-xs">
                <span class="px-2 py-1 bg-bg-hover rounded text-text-secondary">相似度 0.83</span>
                <span class="px-2 py-1 bg-bg-hover rounded text-text-secondary">
                  期刊影响力 0.60
                  
                </span>
              </div>
            </div>
          </div>
        </article>
        
        <article class="bg-bg-card rounded-lg border border-border-color overflow-hidden hover:border-accent/50 transition-colors">
          <div class="p-4 md:p-5">
            <div class="mb-3">
              <div class="flex items-center gap-2 mb-2 flex-wrap">
                <span class="inline-flex items-center px-2.5 py-0.5 rounded text-xs font-medium
                  bg-amber-100 text-amber-700 border border-amber-300
                  ">
                  推荐
                </span>
                <span class="text-xs text-text-secondary">评分 0.78</span>
                <span class="text-xs text-text-secondary">·</span>
                <span class="text-xs text-text-secondary">crossref</span>
                <span class="text-xs text-text-secondary md:hidden">· 2026-01-17</span>
              </div>
              <div class="flex items-start justify-between gap-3 md:gap-4">
                <div class="flex-1">
                  <h2 class="text-base md:text-lg font-semibold text-text-primary leading-tight">
                    <a href="https://doi.org/10.1007/s11263-025-02650-w" target="_blank" rel="noopener" class="hover:text-accent transition-colors">
                      Large Foundation Model Empowered Region-aware Underwater Image Captioning
                    </a>
                  </h2>
                  
                  <p class="text-sm text-text-secondary mt-0.5 leading-snug">大型基础模型赋能的区域感知水下图像描述</p>
                  
                </div>
                <div class="hidden md:block text-right text-sm text-text-secondary flex-shrink-0">
                  <div class="font-medium">2026-01-17</div>
                </div>
              </div>
              <div class="mt-1 text-xs text-text-secondary" title="International Journal of Computer Vision">
                International Journal of Computer Vision
                
                  <span class="ml-1 text-blue-600">(IF: 9.3)</span>
                
              </div>
            </div>

            <p class="text-sm text-text-secondary mb-3">
              Huanyu Li，Li Li，Hao Wang，Weibo Zhang，Peng Ren
            </p>

            <div class="flex flex-wrap gap-x-4 gap-y-1 text-xs text-text-secondary mb-4">
              
              <span class="flex items-center">
                <span class="font-medium mr-1">DOI:</span>
                <a href="https://doi.org/10.1007/s11263-025-02650-w" target="_blank" rel="noopener" class="text-accent hover:text-accent-hover hover:underline">10.1007/s11263-025-02650-w</a>
              </span>
              
            </div>

            
            <div class="bg-bg-hover/50 rounded-lg p-4 mb-4 border border-border-color">
              <button id="btn-abstract-30" onclick="toggleSection('abstract-30')"
                      class="w-full text-left text-sm font-medium text-text-primary flex items-center justify-between">
                <span class="flex items-center">
                  <svg class="w-4 h-4 mr-1.5 text-text-secondary" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                    <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M9 12h6m-6 4h6m2 5H7a2 2 0 01-2-2V5a2 2 0 012-2h5.586a1 1 0 01.707.293l5.414 5.414a1 1 0 01.293.707V19a2 2 0 01-2 2z"/>
                  </svg>
                  原文摘要
                </span>
                <svg class="w-4 h-4 text-text-secondary transform transition-transform" id="icon-abstract-30" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                  <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
                </svg>
              </button>
              <div id="abstract-30" class="section-expand collapsed">
                <p class="text-sm text-text-secondary mt-3 leading-relaxed">Underwater image captioning facilitates the transformation from visual perception to semantic understanding in underwater computer vision. Despite advancements in this field, challenges remain in generating high-quality captions for underwater images. These challenges typically stem from (a) ambiguity between object and background regions for feature extraction, and (b) insufficient feature fusion across all regions. To address these challenges, we develop a large foundation model empowered region-aware underwater image captioning framework. Our novel contributions are two-fold: (a) A region-discriminative feature extraction strategy powered by the large foundation segment anything model (SAM) is developed. This strategy accurately delineates object and background regions through segmentation maps, enabling precise extraction of region-discriminative features. (b) A region-guided feature fusion strategy comprehensively fusing regional information throughout an encoding-decoding process is presented. This strategy utilizes a region-guided encoder for the progressive layer-wise fusion of region-discriminative features and grid features, followed by a meshed memory decoder that fuses multi-level encoded features, thereby enhancing the decoded features. Together, these contributions result in the generation of accurate and comprehensive underwater image captions. Experimental evaluations on three datasets demonstrate that our proposed framework achieves state-of-the-art performance for underwater image captioning.</p>
              </div>
            </div>
            

            
            <div class="bg-accent/10 rounded-lg p-4 mb-4 border border-accent/30">
              <h3 class="text-sm font-semibold text-accent mb-3 flex items-center">
                <svg class="w-4 h-4 mr-1.5" fill="currentColor" viewBox="0 0 24 24">
                  <path d="M12 2a2 2 0 012 2c0 .74-.4 1.39-1 1.73V7h1a7 7 0 017 7h1a1 1 0 011 1v3a1 1 0 01-1 1h-1v1a2 2 0 01-2 2H5a2 2 0 01-2-2v-1H2a1 1 0 01-1-1v-3a1 1 0 011-1h1a7 7 0 017-7h1V5.73c-.6-.34-1-.99-1-1.73a2 2 0 012-2z"/>
                </svg>
                AI 总结
              </h3>
              <div class="space-y-2 text-sm text-text-primary">
                <p><span class="font-medium text-accent">研究问题：</span>如何为水下图像生成准确、全面的自然语言描述。</p>
                <p><span class="font-medium text-accent">研究方法：</span>利用大模型SAM分割前景/背景，提出区域判别特征提取与区域引导融合策略。</p>
                <p><span class="font-medium text-accent">主要发现：</span>在三个水下图像描述数据集上达到SOTA性能，显著提升描述质量。</p>
                <p><span class="font-medium text-accent">创新点：</span>首次将SAM大模型引入水下图像描述，提出区域感知特征提取与融合框架。</p>
                
                <p><span class="font-medium text-accent">相关性：</span>为水下视觉语义理解提供新工具，推动海洋机器人、监测等应用落地。</p>
                
              </div>

              <div class="mt-3 pt-3 border-t border-accent/30">
                <button id="btn-detail-30" onclick="toggleSection('detail-30')"
                        class="text-xs text-accent hover:text-accent-hover font-medium flex items-center">
                  <svg class="w-3 h-3 mr-1" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                    <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
                  </svg>
                  查看详细分析
                </button>
                <div id="detail-30" class="section-expand collapsed mt-3">
                  <div class="bg-bg-card rounded-lg p-4 text-sm text-text-primary space-y-3 border border-border-color">
                    <p><span class="font-medium text-text-primary">研究背景：</span>水下图像描述生成是水下计算机视觉从视觉感知迈向语义理解的关键环节，但水下场景颜色失真、悬浮颗粒遮挡及目标-背景边界模糊，使得传统方法难以提取清晰、可区分特征，导致生成的自然语言描述常出现漏检、误检或语义空洞。</p>
                    <p><span class="font-medium text-text-primary">方法详情：</span>作者提出以大型基础模型SAM为核心的区域感知水下图像描述框架。首先，利用SAM对输入图像进行零样本分割，获得像素级目标-背景掩码，并据此在CNN或ViT特征图上执行区域判别池化，分别提取目标特征与背景特征。随后，设计区域引导编码器，在每一层将区域特征与网格特征按掩码加权融合，实现渐进式跨层信息互补。最后，采用meshed-memory Transformer解码器，对多层级融合后的编码特征进行再融合与注意力增强，生成富含区域语义的描述句子。整个流程以端到端方式训练，损失函数为交叉熵+CIDEr奖励。</p>
                    <p><span class="font-medium text-text-primary">研究结果：</span>在UIEB-Caption、UFO-120-Caption和自建Deep-Caption三个数据集上的实验表明，该方法在BLEU-4、METEOR、ROUGE-L和CIDEr-D指标上平均提升3.2-5.7个百分点，达到水下图像描述新SOTA；可视化结果显示，模型能准确提及目标种类、颜色、相对位置及背景环境（如珊瑚、沙地），显著减少背景噪声词。</p>
                    <p><span class="font-medium text-text-primary">局限性：</span>论文未公开代码与分割标注，实验复现存在障碍；SAM在水下域的零样本分割仍可能漏分细小生物或误分悬浮颗粒，导致区域特征污染；此外，方法依赖大规模预训练SAM，计算与存储开销大，对水下机器人端侧部署提出挑战。</p>
                    
                    <p><span class="font-medium text-text-primary">未来方向：</span>后续可探索轻量级分割网络或自适应微调策略以降低计算量，并引入时序或多模态（声呐、深度）信息提升描述细粒度。</p>
                    
                    <p class="text-accent"><span class="font-medium">研究相关性：</span>若研究者关注水下视觉-语言任务、区域感知特征融合或基础模型在海洋场景中的迁移应用，该文提供了SAM与图像描述结合的完整范式及评测基准，可直接借鉴其区域引导编码-解码架构。</p>
                  </div>
                </div>
              </div>
            </div>
            

            <div class="pt-4 border-t border-border-color">
              <div class="flex flex-wrap gap-3 text-xs">
                <span class="px-2 py-1 bg-bg-hover rounded text-text-secondary">相似度 0.80</span>
                <span class="px-2 py-1 bg-bg-hover rounded text-text-secondary">
                  期刊影响力 0.72
                  
                    <span class="ml-1 text-blue-600">(IF: 9.3)</span>
                  
                </span>
              </div>
            </div>
          </div>
        </article>
        
      </div>
    </div>
  </main>

  <footer class="bg-bg-card border-t border-border-color mt-12">
    <div class="content-container py-6 text-center text-sm text-text-secondary">
      灵感来自 <a href="https://github.com/Yorks0n/ZotWatch" class="text-accent hover:text-accent-hover transition-colors">ZotWatch</a>
    </div>
  </footer>

  <script>
    function toggleSection(id) {
      const el = document.getElementById(id);
      const btn = document.getElementById('btn-' + id);
      const icon = document.getElementById('icon-' + id);
      const hint = document.getElementById('hint-' + id);
      if (el.classList.contains('collapsed')) {
        el.classList.remove('collapsed');
        el.classList.add('expanded');
        if (btn && btn.textContent.includes('详细分析')) {
          btn.innerHTML = '<svg class="w-3 h-3 mr-1" fill="none" stroke="currentColor" viewBox="0 0 24 24"><path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M5 15l7-7 7 7"/></svg>收起详细分析';
        }
        if (hint) hint.textContent = '点击收起';
        if (icon) icon.style.transform = 'rotate(180deg)';
        if (id === 'researcher-profile') {
          setTimeout(() => { window.dispatchEvent(new Event('resize')); }, 100);
        }
      } else {
        el.classList.remove('expanded');
        el.classList.add('collapsed');
        if (btn && btn.textContent.includes('详细分析')) {
          btn.innerHTML = '<svg class="w-3 h-3 mr-1" fill="none" stroke="currentColor" viewBox="0 0 24 24"><path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/></svg>查看详细分析';
        }
        if (hint) hint.textContent = '点击展开';
        if (icon) icon.style.transform = 'rotate(0deg)';
      }
    }
  </script>
</body>
</html>