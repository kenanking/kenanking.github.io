<!DOCTYPE html>
<html lang="zh-CN">
<head>
  <meta charset="utf-8">
  <meta name="viewport" content="width=device-width, initial-scale=1">
  <title>ZotWatch 文献推荐 - 2025-12-19</title>
  <script src="https://cdn.tailwindcss.com"></script>
  <script src="https://cdn.jsdelivr.net/npm/chart.js@4.4.1/dist/chart.umd.min.js"></script>
  <script src="https://d3js.org/d3.v7.min.js"></script>
  <script>
    tailwind.config = {
      theme: {
        extend: {
          colors: {
            'bg-primary': '#f8fafc',
            'bg-card': '#ffffff',
            'bg-hover': '#f1f5f9',
            'text-primary': '#1e293b',
            'text-secondary': '#64748b',
            'border-color': '#e2e8f0',
            'accent': '#2563eb',
            'accent-hover': '#1d4ed8',
          }
        }
      }
    }
  </script>
  <style>
    .section-expand { transition: max-height 0.5s ease-out; overflow: hidden; }
    .section-expand.collapsed { max-height: 0; }
    .section-expand.expanded { max-height: none; }
    body { font-family: -apple-system, BlinkMacSystemFont, 'Segoe UI', Roboto, 'Helvetica Neue', Arial, sans-serif; }
    .content-container { max-width: 48rem; margin: 0 auto; padding: 0 1rem; }
    @media (min-width: 768px) { .content-container { padding: 0 2rem; } }
    @media (min-width: 1024px) { .content-container { padding: 0 4rem; } }

    .cluster-graph-container { width: 100%; height: 280px; touch-action: pan-y; }
    @media (min-width: 640px) { .cluster-graph-container { height: 320px; } }
    @media (min-width: 768px) { .cluster-graph-container { height: 350px; } }
    #cluster-graph svg { overflow: visible; }
  </style>
</head>
<body class="bg-bg-primary min-h-screen text-text-primary">
  <header class="bg-bg-card border-b border-border-color">
    <div class="content-container py-6 md:py-8">
      <h1 class="text-xl md:text-2xl font-bold text-text-primary">ZotWatch 文献推荐</h1>
      <p class="text-xs md:text-sm text-text-secondary mt-1">
        共 35 篇论文 ·
        生成于 2025-12-19 10:43 Asia/Shanghai
      </p>
    </div>
  </header>

  
  <section class="py-4 md:py-6 border-b border-border-color">
    <div class="content-container">
      <button id="btn-researcher-profile" onclick="toggleSection('researcher-profile')"
              class="w-full text-left text-lg md:text-xl font-bold text-text-primary mb-3 md:mb-4 flex items-center justify-between group hover:text-accent transition-colors">
        <span class="flex items-center gap-2 flex-1 min-w-0">
          <svg class="w-5 h-5 flex-shrink-0 text-accent" fill="currentColor" viewBox="0 0 24 24">
            <path d="M12 2C6.48 2 2 6.48 2 12s4.48 10 10 10 10-4.48 10-10S17.52 2 12 2zm0 3c1.66 0 3 1.34 3 3s-1.34 3-3 3-3-1.34-3-3 1.34-3 3-3zm0 14.2c-2.5 0-4.71-1.28-6-3.22.03-1.99 4-3.08 6-3.08 1.99 0 5.97 1.09 6 3.08-1.29 1.94-3.5 3.22-6 3.22z"/>
          </svg>
          <span class="truncate">研究兴趣画像</span>
        </span>
        <span class="flex items-center gap-1 md:gap-2 flex-shrink-0 ml-2">
          <span class="hidden md:inline text-sm font-normal text-text-secondary group-hover:text-accent" id="hint-researcher-profile">点击展开</span>
          <svg class="w-5 h-5 md:w-4 md:h-4 text-text-secondary group-hover:text-accent transform transition-transform" id="icon-researcher-profile" fill="none" stroke="currentColor" viewBox="0 0 24 24">
            <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
          </svg>
        </span>
      </button>

      <div id="researcher-profile" class="section-expand collapsed">
      <div class="bg-bg-card rounded-lg border border-border-color p-4 mb-4">
        <div class="grid grid-cols-4 gap-3 text-center">
          <div>
            <div class="text-xl font-bold text-accent">927</div>
            <div class="text-xs text-text-secondary">收藏论文</div>
          </div>
          <div>
            <div class="text-xl font-bold text-accent">5年7月</div>
            <div class="text-xs text-text-secondary">收藏时长</div>
          </div>
          <div>
            <div class="text-xl font-bold text-accent">&gt;10</div>
            <div class="text-xs text-text-secondary">兴趣领域</div>
          </div>
          <div>
            <div class="text-xl font-bold text-accent">8</div>
            <div class="text-xs text-text-secondary">高频作者</div>
          </div>
        </div>
      </div>

      
      <div class="bg-accent/10 rounded-lg border border-accent/30 p-4 mb-4">
        <h3 class="text-sm font-semibold text-accent mb-2 flex items-center gap-2">
          <svg class="w-4 h-4" fill="currentColor" viewBox="0 0 24 24">
            <path d="M12 2a2 2 0 012 2c0 .74-.4 1.39-1 1.73V7h1a7 7 0 017 7h1a1 1 0 011 1v3a1 1 0 01-1 1h-1v1a2 2 0 01-2 2H5a2 2 0 01-2-2v-1H2a1 1 0 01-1-1v-3a1 1 0 011-1h1a7 7 0 017-7h1V5.73c-.6-.34-1-.99-1-1.73a2 2 0 012-2z"/>
          </svg>
          AI 研究兴趣分析
        </h3>
        <div class="space-y-2 text-xs text-text-primary leading-relaxed">
          <p><span class="font-medium text-accent">研究兴趣：</span>用户长期聚焦计算机视觉核心任务（目标检测、视觉定位、姿态估计）与模型效率（模型压缩、重参数化），同时积极跟进自监督/对比学习等表征学习新范式。</p>
          <p><span class="font-medium text-accent">深度关注：</span>在经典视觉目标检测与最新Vision Transformer架构之间保持持续阅读，形成从RCNN系列到DETR的完整技术脉络；对遥感领域（SAR图像、旋转目标检测）有稳定收藏，显示跨场景目标识别深度积累。</p>
          <p><span class="font-medium text-accent">跨学科倾向：</span>收藏曲线同时覆盖CVPR、NeurIPS与《雷达学报》、IEEE TGARS，体现将计算机视觉方法迁移至遥感解析的鲜明交叉取向。</p>
          <p><span class="font-medium text-accent">兴趣演变：</span>2025-Q1单季新增86篇为五年峰值，关键词迅速扩展到“大语言模型、DeepSeek、可微分渲染”，表明正把兴趣延伸至视觉-语言基础模型与神经渲染，补充传统检测与SAR研究。</p>
          <p><span class="font-medium text-accent">阅读建议：</span>可关注多模态大模型在遥感问答与图像描述上的应用，以及结合扩散模型的低标注SAR目标生成与检测新框架。</p>
        </div>
      </div>
      

      <div class="grid grid-cols-1 md:grid-cols-2 gap-4 mb-4">
        
        <div class="bg-bg-card rounded-lg border border-border-color p-4">
          <h4 class="text-sm font-medium text-text-primary mb-3">研究领域分布</h4>
          <div style="height: 180px;">
            <canvas id="domainsChart"></canvas>
          </div>
        </div>
        

        
        <div class="bg-bg-card rounded-lg border border-border-color p-4">
          <h4 class="text-sm font-medium text-text-primary mb-3">阅读趋势（季度）</h4>
          <div style="height: 180px;">
            <canvas id="trendsChart"></canvas>
          </div>
        </div>
        
      </div>

      
      <div class="bg-bg-card rounded-lg border border-border-color p-4 mb-4">
        <h4 class="text-sm font-medium text-text-primary mb-3">论文发表年份分布</h4>
        <div style="height: 180px;">
          <canvas id="yearDistChart"></canvas>
        </div>
      </div>
      

      
      <div class="bg-bg-card rounded-lg border border-border-color p-4 mb-4">
        <h4 class="text-sm font-medium text-text-primary mb-3">
          研究方向聚类图谱
          <span class="text-xs text-text-secondary font-normal ml-2">(30 个聚类)</span>
        </h4>
        <div id="cluster-graph" class="cluster-graph-container"></div>
        <div class="text-xs text-text-secondary mt-3">
          覆盖 903/903 篇论文（仅含摘要的文献参与聚类）
        </div>
      </div>
      

      <div class="grid grid-cols-1 md:grid-cols-2 gap-4 mb-4">
        
        <div class="bg-bg-card rounded-lg border border-border-color p-4">
          <h4 class="text-sm font-medium text-text-primary mb-2">高频关注作者</h4>
          <div class="grid grid-cols-2 gap-x-4 gap-y-1 text-xs">
            
            <div class="flex justify-between truncate">
              <span class="text-text-primary truncate" title="Kaiming He">Kaiming He</span>
              <span class="text-text-secondary ml-1 flex-shrink-0">24</span>
            </div>
            
            <div class="flex justify-between truncate">
              <span class="text-text-primary truncate" title="Ross Girshick">Ross Girshick</span>
              <span class="text-text-secondary ml-1 flex-shrink-0">18</span>
            </div>
            
            <div class="flex justify-between truncate">
              <span class="text-text-primary truncate" title="Jian Sun">Jian Sun</span>
              <span class="text-text-secondary ml-1 flex-shrink-0">15</span>
            </div>
            
            <div class="flex justify-between truncate">
              <span class="text-text-primary truncate" title="Song Han">Song Han</span>
              <span class="text-text-secondary ml-1 flex-shrink-0">14</span>
            </div>
            
            <div class="flex justify-between truncate">
              <span class="text-text-primary truncate" title="Xiangyu Zhang">Xiangyu Zhang</span>
              <span class="text-text-secondary ml-1 flex-shrink-0">14</span>
            </div>
            
            <div class="flex justify-between truncate">
              <span class="text-text-primary truncate" title="Piotr Dollár">Piotr Dollár</span>
              <span class="text-text-secondary ml-1 flex-shrink-0">12</span>
            </div>
            
            <div class="flex justify-between truncate">
              <span class="text-text-primary truncate" title="Li Liu">Li Liu</span>
              <span class="text-text-secondary ml-1 flex-shrink-0">10</span>
            </div>
            
            <div class="flex justify-between truncate">
              <span class="text-text-primary truncate" title="Xiang Li">Xiang Li</span>
              <span class="text-text-secondary ml-1 flex-shrink-0">10</span>
            </div>
            
            <div class="flex justify-between truncate">
              <span class="text-text-primary truncate" title="Yann LeCun">Yann LeCun</span>
              <span class="text-text-secondary ml-1 flex-shrink-0">9</span>
            </div>
            
            <div class="flex justify-between truncate">
              <span class="text-text-primary truncate" title="Chunhua Shen">Chunhua Shen</span>
              <span class="text-text-secondary ml-1 flex-shrink-0">8</span>
            </div>
            
            <div class="flex justify-between truncate">
              <span class="text-text-primary truncate" title="Gangyao Kuang">Gangyao Kuang</span>
              <span class="text-text-secondary ml-1 flex-shrink-0">8</span>
            </div>
            
            <div class="flex justify-between truncate">
              <span class="text-text-primary truncate" title="Xian Sun">Xian Sun</span>
              <span class="text-text-secondary ml-1 flex-shrink-0">8</span>
            </div>
            
          </div>
        </div>
        

        
        <div class="bg-bg-card rounded-lg border border-border-color p-4">
          <h4 class="text-sm font-medium text-text-primary mb-2">常关注期刊/会议</h4>
          <div class="space-y-1 text-xs">
            
            <div class="flex items-center justify-between">
              <span class="text-text-primary truncate flex-1" title="Computer Vision and Pattern Recognition">Computer Vision and Pattern Recognition</span>
              <span class="text-xs px-1 py-0.5 rounded bg-blue-100 text-blue-700 ml-1 flex-shrink-0">
                期刊
              </span>
              <span class="text-text-secondary ml-1 flex-shrink-0">113</span>
            </div>
            
            <div class="flex items-center justify-between">
              <span class="text-text-primary truncate flex-1" title="Neural Information Processing Systems">Neural Information Processing Systems</span>
              <span class="text-xs px-1 py-0.5 rounded bg-blue-100 text-blue-700 ml-1 flex-shrink-0">
                期刊
              </span>
              <span class="text-text-secondary ml-1 flex-shrink-0">45</span>
            </div>
            
            <div class="flex items-center justify-between">
              <span class="text-text-primary truncate flex-1" title="IEEE Transactions on Geoscience and Remote Sensing">IEEE Transactions on Geoscience and Remote Sensing</span>
              <span class="text-xs px-1 py-0.5 rounded bg-blue-100 text-blue-700 ml-1 flex-shrink-0">
                期刊
              </span>
              <span class="text-text-secondary ml-1 flex-shrink-0">43</span>
            </div>
            
            <div class="flex items-center justify-between">
              <span class="text-text-primary truncate flex-1" title="International Conference on Computer Vision">International Conference on Computer Vision</span>
              <span class="text-xs px-1 py-0.5 rounded bg-amber-100 text-amber-700 ml-1 flex-shrink-0">
                会议
              </span>
              <span class="text-text-secondary ml-1 flex-shrink-0">36</span>
            </div>
            
            <div class="flex items-center justify-between">
              <span class="text-text-primary truncate flex-1" title="IEEE Transactions on Pattern Analysis and Machine Intelligence">IEEE Transactions on Pattern Analysis and Machine Intelligence</span>
              <span class="text-xs px-1 py-0.5 rounded bg-blue-100 text-blue-700 ml-1 flex-shrink-0">
                期刊
              </span>
              <span class="text-text-secondary ml-1 flex-shrink-0">32</span>
            </div>
            
            <div class="flex items-center justify-between">
              <span class="text-text-primary truncate flex-1" title="International Conference on Learning Representations">International Conference on Learning Representations</span>
              <span class="text-xs px-1 py-0.5 rounded bg-amber-100 text-amber-700 ml-1 flex-shrink-0">
                会议
              </span>
              <span class="text-text-secondary ml-1 flex-shrink-0">27</span>
            </div>
            
            <div class="flex items-center justify-between">
              <span class="text-text-primary truncate flex-1" title="European Conference on Computer Vision">European Conference on Computer Vision</span>
              <span class="text-xs px-1 py-0.5 rounded bg-amber-100 text-amber-700 ml-1 flex-shrink-0">
                会议
              </span>
              <span class="text-text-secondary ml-1 flex-shrink-0">23</span>
            </div>
            
            <div class="flex items-center justify-between">
              <span class="text-text-primary truncate flex-1" title="雷达学报">雷达学报</span>
              <span class="text-xs px-1 py-0.5 rounded bg-blue-100 text-blue-700 ml-1 flex-shrink-0">
                期刊
              </span>
              <span class="text-text-secondary ml-1 flex-shrink-0">21</span>
            </div>
            
          </div>
        </div>
        
      </div>

      
      <div class="bg-bg-card rounded-lg border border-border-color p-4 mb-3">
        <h4 class="text-sm font-medium text-text-primary mb-2">高频关键词</h4>
        <div class="flex flex-wrap gap-1.5">
          
          <span class="px-2 py-0.5 bg-bg-hover rounded text-xs text-text-primary">
            大语言模型 <span class="text-text-secondary">(13)</span>
          </span>
          
          <span class="px-2 py-0.5 bg-bg-hover rounded text-xs text-text-primary">
            综述 <span class="text-text-secondary">(9)</span>
          </span>
          
          <span class="px-2 py-0.5 bg-bg-hover rounded text-xs text-text-primary">
            自监督学习 <span class="text-text-secondary">(9)</span>
          </span>
          
          <span class="px-2 py-0.5 bg-bg-hover rounded text-xs text-text-primary">
            域自适应 <span class="text-text-secondary">(9)</span>
          </span>
          
          <span class="px-2 py-0.5 bg-bg-hover rounded text-xs text-text-primary">
            迁移学习 <span class="text-text-secondary">(7)</span>
          </span>
          
          <span class="px-2 py-0.5 bg-bg-hover rounded text-xs text-text-primary">
            扩散模型 <span class="text-text-secondary">(7)</span>
          </span>
          
          <span class="px-2 py-0.5 bg-bg-hover rounded text-xs text-text-primary">
            强化学习 <span class="text-text-secondary">(7)</span>
          </span>
          
          <span class="px-2 py-0.5 bg-bg-hover rounded text-xs text-text-primary">
            合成孔径雷达 <span class="text-text-secondary">(7)</span>
          </span>
          
          <span class="px-2 py-0.5 bg-bg-hover rounded text-xs text-text-primary">
            重参数化 <span class="text-text-secondary">(6)</span>
          </span>
          
          <span class="px-2 py-0.5 bg-bg-hover rounded text-xs text-text-primary">
            DeepSeek <span class="text-text-secondary">(6)</span>
          </span>
          
          <span class="px-2 py-0.5 bg-bg-hover rounded text-xs text-text-primary">
            Transformers <span class="text-text-secondary">(5)</span>
          </span>
          
          <span class="px-2 py-0.5 bg-bg-hover rounded text-xs text-text-primary">
            旋转目标检测 <span class="text-text-secondary">(5)</span>
          </span>
          
          <span class="px-2 py-0.5 bg-bg-hover rounded text-xs text-text-primary">
            SAR目标识别 <span class="text-text-secondary">(5)</span>
          </span>
          
          <span class="px-2 py-0.5 bg-bg-hover rounded text-xs text-text-primary">
            特征可视化 <span class="text-text-secondary">(5)</span>
          </span>
          
          <span class="px-2 py-0.5 bg-bg-hover rounded text-xs text-text-primary">
            生成对抗网络 <span class="text-text-secondary">(5)</span>
          </span>
          
          <span class="px-2 py-0.5 bg-bg-hover rounded text-xs text-text-primary">
            Vision Transformers <span class="text-text-secondary">(4)</span>
          </span>
          
          <span class="px-2 py-0.5 bg-bg-hover rounded text-xs text-text-primary">
            基础模型 <span class="text-text-secondary">(4)</span>
          </span>
          
          <span class="px-2 py-0.5 bg-bg-hover rounded text-xs text-text-primary">
            注意力机制 <span class="text-text-secondary">(4)</span>
          </span>
          
          <span class="px-2 py-0.5 bg-bg-hover rounded text-xs text-text-primary">
            HRNet <span class="text-text-secondary">(4)</span>
          </span>
          
          <span class="px-2 py-0.5 bg-bg-hover rounded text-xs text-text-primary">
            目标检测 <span class="text-text-secondary">(4)</span>
          </span>
          
        </div>
      </div>
      

      <div class="text-xs text-text-secondary text-right">
        画像生成于 2025-12-19 10:27 Asia/Shanghai
        · kimi-k2-turbo-preview
      </div>
      </div>
    </div>
  </section>
  

  
  <script>
    document.addEventListener('DOMContentLoaded', function() {
      Chart.defaults.font.family = "-apple-system, BlinkMacSystemFont, 'Segoe UI', Roboto, 'Helvetica Neue', Arial, sans-serif";
      Chart.defaults.font.size = 11;
      Chart.defaults.color = '#64748b';
      Chart.defaults.animation = false;

      
      const domainsCtx = document.getElementById('domainsChart');
      if (domainsCtx) {
        new Chart(domainsCtx, {
          type: 'bar',
          data: {
            labels: ['视觉定位', '目标检测', '姿态估计', '模型压缩', '对比学习', '人脸识别', '车牌识别', '卫星导航'],
            datasets: [{
              data: [22, 35, 15, 18, 10, 12, 6, 6],
              backgroundColor: 'rgba(37, 99, 235, 0.7)',
              borderColor: 'rgba(37, 99, 235, 1)',
              borderWidth: 1,
              borderRadius: 2,
              barThickness: 16
            }]
          },
          options: {
            indexAxis: 'y',
            responsive: true,
            maintainAspectRatio: false,
            plugins: {
              legend: { display: false },
              tooltip: { enabled: false }
            },
            scales: {
              x: {
                beginAtZero: true,
                grid: { color: 'rgba(226, 232, 240, 0.5)' },
                ticks: { precision: 0 }
              },
              y: {
                grid: { display: false },
                ticks: { font: { size: 10 } }
              }
            }
          }
        });
      }
      

      
      const trendsCtx = document.getElementById('trendsChart');
      if (trendsCtx) {
        const trends = [{ q: '2023-Q1', c: 51 }, { q: '2023-Q2', c: 18 }, { q: '2023-Q3', c: 21 }, { q: '2023-Q4', c: 19 }, { q: '2024-Q1', c: 67 }, { q: '2024-Q2', c: 18 }, { q: '2024-Q3', c: 4 }, { q: '2024-Q4', c: 23 }, { q: '2025-Q1', c: 86 }, { q: '2025-Q2', c: 34 }, { q: '2025-Q3', c: 11 }, { q: '2025-Q4', c: 25 }];
        const monthColors = [
          { bg: 'rgba(59, 130, 246, 0.7)', border: 'rgba(59, 130, 246, 1)' },
          { bg: 'rgba(16, 185, 129, 0.7)', border: 'rgba(16, 185, 129, 1)' },
          { bg: 'rgba(245, 158, 11, 0.7)', border: 'rgba(245, 158, 11, 1)' }
        ];
        const bgColors = trends.map((t, i) => monthColors[i % 3].bg);
        const borderColors = trends.map((t, i) => monthColors[i % 3].border);
        new Chart(trendsCtx, {
          type: 'bar',
          data: {
            labels: trends.map(t => t.q.replace('-', '\n')),
            datasets: [{
              data: trends.map(t => t.c),
              backgroundColor: bgColors,
              borderColor: borderColors,
              borderWidth: 1,
              borderRadius: 2,
              barThickness: 20
            }]
          },
          options: {
            responsive: true,
            maintainAspectRatio: false,
            plugins: {
              legend: { display: false },
              tooltip: { enabled: false }
            },
            scales: {
              x: {
                grid: { display: false },
                ticks: { font: { size: 9 }, maxRotation: 0 }
              },
              y: {
                beginAtZero: true,
                grid: { color: 'rgba(226, 232, 240, 0.5)' },
                ticks: { precision: 0 }
              }
            }
          }
        });
      }
      

      
      const yearDistCtx = document.getElementById('yearDistChart');
      if (yearDistCtx) {
        const yearData = [{ year: 2006, count: 8 }, { year: 2007, count: 3 }, { year: 2008, count: 2 }, { year: 2009, count: 2 }, { year: 2010, count: 9 }, { year: 2011, count: 10 }, { year: 2012, count: 3 }, { year: 2013, count: 12 }, { year: 2014, count: 14 }, { year: 2015, count: 18 }, { year: 2016, count: 15 }, { year: 2017, count: 41 }, { year: 2018, count: 58 }, { year: 2019, count: 53 }, { year: 2020, count: 66 }, { year: 2021, count: 84 }, { year: 2022, count: 110 }, { year: 2023, count: 109 }, { year: 2024, count: 112 }, { year: 2025, count: 156 }];
        new Chart(yearDistCtx, {
          type: 'line',
          data: {
            labels: yearData.map(d => d.year),
            datasets: [{
              data: yearData.map(d => d.count),
              borderColor: 'rgba(37, 99, 235, 1)',
              backgroundColor: 'rgba(37, 99, 235, 0.1)',
              borderWidth: 2,
              fill: true,
              tension: 0.3,
              pointRadius: 2,
              pointHoverRadius: 4
            }]
          },
          options: {
            responsive: true,
            maintainAspectRatio: false,
            plugins: {
              legend: { display: false },
              tooltip: {
                callbacks: {
                  label: function(context) {
                    return context.parsed.y + ' 篇';
                  }
                }
              }
            },
            scales: {
              x: {
                grid: { display: false },
                ticks: {
                  font: { size: 9 },
                  maxRotation: 45,
                  callback: function(value, index) {
                    const year = this.getLabelForValue(value);
                    return year % 5 === 0 ? year : '';
                  }
                }
              },
              y: {
                beginAtZero: true,
                grid: { color: 'rgba(226, 232, 240, 0.5)' },
                ticks: { precision: 0 }
              }
            }
          }
        });
      }
      

      
      const clusterContainer = document.getElementById('cluster-graph');
      if (clusterContainer) {
        const clusters = [
          
          {
            id: 0,
            label: "SAR\u57df\u81ea\u9002\u5e94\u8bc6\u522b",
            size: 63,
            keywords: ["SAR\u76ee\u6807\u8bc6\u522b", "\u8fc1\u79fb\u5b66\u4e60", "\u57df\u81ea\u9002\u5e94"]
          },
          
          {
            id: 1,
            label: "\u901a\u7528\u76ee\u6807\u68c0\u6d4b\u67b6\u6784",
            size: 61,
            keywords: ["\u7efc\u8ff0", "\u65cb\u8f6c\u76ee\u6807\u68c0\u6d4b", "DETR"]
          },
          
          {
            id: 2,
            label: "\u5927\u6a21\u578b\u4e0e\u6df7\u5408\u4e13\u5bb6",
            size: 60,
            keywords: ["\u5927\u8bed\u8a00\u6a21\u578b", "DeepSeek", "\u5f3a\u5316\u5b66\u4e60"]
          },
          
          {
            id: 3,
            label: "Vision Transformer\u5206\u5272",
            size: 57,
            keywords: ["Vision Transformers", "Swin Transformer", "\u57fa\u7840\u6a21\u578b"]
          },
          
          {
            id: 4,
            label: "\u81ea\u76d1\u7763\u89c6\u89c9\u8868\u5f81",
            size: 42,
            keywords: ["\u81ea\u76d1\u7763\u5b66\u4e60", "\u5bf9\u6bd4\u5b66\u4e60", "MoCo"]
          },
          
          {
            id: 5,
            label: "SAR\u8230\u8239\u68c0\u6d4b",
            size: 41,
            keywords: ["SAR\u8230\u8239\u68c0\u6d4b", "\u591a\u5c3a\u5ea6\u7279\u5f81\u878d\u5408", "\u5408\u6210\u5b54\u5f84\u96f7\u8fbe"]
          },
          
          {
            id: 6,
            label: "CNN\u53ef\u89e3\u91ca\u6027",
            size: 39,
            keywords: ["\u7279\u5f81\u53ef\u89c6\u5316", "\u91cd\u53c2\u6570\u5316", "VGG"]
          },
          
          {
            id: 7,
            label: "\u6df1\u5ea6\u5b66\u4e60\u7406\u8bba",
            size: 37,
            keywords: ["\u4eba\u5de5\u795e\u7ecf\u7f51\u7edc", "\u6b8b\u5dee\u8fde\u63a5", "\u6df1\u5ea6\u6b8b\u5dee\u5b66\u4e60"]
          },
          
          {
            id: 8,
            label: "\u4eba\u4f53\u59ff\u6001\u4f30\u8ba1",
            size: 36,
            keywords: ["HRNet", "Transformers"]
          },
          
          {
            id: 9,
            label: "\u63d0\u793a\u5de5\u7a0b\u4e0e\u4f18\u5316",
            size: 36,
            keywords: ["\u7814\u7a76", "LaTeX", "\u8bbe\u8ba1\u6a21\u5f0f"]
          },
          
          {
            id: 10,
            label: "\u8f7b\u91cf\u7ea7\u89c6\u89c9\u6a21\u578b",
            size: 35,
            keywords: ["\u8f7b\u91cf\u7ea7\u6a21\u578b", "\u6a21\u578b\u538b\u7f29", "\u6ce8\u610f\u529b\u673a\u5236"]
          },
          
          {
            id: 11,
            label: "SAR\u98de\u673a\u76ee\u6807\u8bc6\u522b",
            size: 32,
            keywords: ["\u6df1\u5ea6\u5b66\u4e60", "\u7279\u5f81\u878d\u5408", "\u5408\u6210\u5b54\u5f84\u96f7\u8fbe"]
          },
          
          {
            id: 12,
            label: "\u6a21\u578b\u91cf\u5316\u538b\u7f29",
            size: 31,
            keywords: ["\u6a21\u578b\u538b\u7f29", "\u6a21\u578b\u91cf\u5316", "\u6574\u6570\u63a8\u7406"]
          },
          
          {
            id: 13,
            label: "\u6269\u6563\u751f\u6210\u6a21\u578b",
            size: 29,
            keywords: ["\u6269\u6563\u6a21\u578b", "\u751f\u6210\u5bf9\u6297\u7f51\u7edc", "\u751f\u6210\u6a21\u578b"]
          },
          
          {
            id: 14,
            label: "\u5c0f\u6837\u672c\u76ee\u6807\u68c0\u6d4b",
            size: 29,
            keywords: ["\u57df\u81ea\u9002\u5e94", "\u8fc1\u79fb\u5b66\u4e60", "\u5f00\u653e\u96c6\u8bc6\u522b"]
          },
          
          {
            id: 15,
            label: "\u8f66\u724c\u8bc6\u522b",
            size: 27,
            keywords: ["\u8f66\u724c\u8bc6\u522b", "Internet of Things", "Microcontrollers"]
          },
          
          {
            id: 16,
            label: "\u591a\u4f20\u611f\u5668\u4f4d\u59ff\u4f30\u8ba1",
            size: 26,
            keywords: []
          },
          
          {
            id: 17,
            label: "SAR\u6210\u50cf\u7b97\u6cd5",
            size: 25,
            keywords: ["\u5408\u6210\u5b54\u5f84\u96f7\u8fbe", "\u56de\u6ce2\u6570\u636e\u6a21\u62df", "\u9006CS\u7b97\u6cd5"]
          },
          
          {
            id: 18,
            label: "\u7ea2\u5916\u5c0f\u76ee\u6807\u68c0\u6d4b",
            size: 24,
            keywords: ["\u7ea2\u5916\u5c0f\u76ee\u6807\u68c0\u6d4b", "\u9065\u611f\u76ee\u6807\u68c0\u6d4b", "\u6837\u672c\u81ea\u9002\u5e94\u589e\u6b96"]
          },
          
          {
            id: 19,
            label: "BEV\u591a\u4f20\u611f\u5668\u878d\u5408",
            size: 22,
            keywords: ["Adapter Branch", "Neural Architecture Search", "Objection Detection"]
          },
          
          {
            id: 20,
            label: "\u5143\u5b66\u4e60\u4e0e\u5f3a\u5316\u5b66\u4e60",
            size: 21,
            keywords: ["\u5f3a\u5316\u5b66\u4e60", "\u5927\u8bed\u8a00\u6a21\u578b", "\u7b56\u7565\u4f18\u5316"]
          },
          
          {
            id: 21,
            label: "\u590d\u6742\u80cc\u666f\u7ea2\u5916\u5f31\u5c0f\u76ee\u6807",
            size: 19,
            keywords: ["\u5f31\u5c0f\u76ee\u6807\u8ddf\u8e2a", "\u7ea2\u5916\u56fe\u50cf", "Transformer"]
          },
          
          {
            id: 22,
            label: "\u6df1\u5ea6\u4f30\u8ba1\u4e0e\u4e09\u7ef4\u611f\u77e5",
            size: 19,
            keywords: ["\u4e09\u7ef4\u611f\u77e5", "\u4f4d\u7f6e\u7f16\u7801", "\u591a\u89c6\u89d2\u89c6\u89c9"]
          },
          
          {
            id: 23,
            label: "\u53d8\u5206\u81ea\u7f16\u7801\u5668\u4e0e\u6d41",
            size: 18,
            keywords: ["\u6807\u51c6\u5316\u6d41", "\u6d41\u6a21\u578b", "NCE"]
          },
          
          {
            id: 24,
            label: "SAR\u8230\u8239\u68c0\u6d4b\u6570\u636e\u96c6",
            size: 18,
            keywords: ["\u5408\u6210\u5b54\u5f84\u96f7\u8fbe\u56fe\u50cf\u63cf\u8ff0", "\u5927\u89c4\u6a21\u57fa\u51c6\u6570\u636e\u96c6", "\u5c42\u6b21\u8bed\u4e49\u5efa\u6a21"]
          },
          
          {
            id: 25,
            label: "\u5355\u6b65\u6269\u6563\u751f\u6210",
            size: 15,
            keywords: ["StepFun", "\u5355\u6b65\u6269\u6563\u6a21\u578b", "\u6761\u4ef6\u751f\u6210"]
          },
          
          {
            id: 26,
            label: "\u8bad\u7ec3\u4f18\u5316\u4e0e\u8d85\u53c2",
            size: 13,
            keywords: ["\u4f18\u5316\u5668", "\u5206\u5e03\u5f0f\u8bad\u7ec3", "\u5927\u6279\u91cf\u8bad\u7ec3"]
          },
          
          {
            id: 27,
            label: "\u76ee\u6807\u8ddf\u8e2a",
            size: 12,
            keywords: ["SIFT", "\u5308\u7259\u5229\u7b97\u6cd5", "\u591a\u57df\u6cdb\u5316"]
          },
          
          {
            id: 28,
            label: "\u7a7f\u5899\u96f7\u8fbe\u751f\u547d\u63a2\u6d4b",
            size: 10,
            keywords: ["\u4fe1\u53f7\u63d0\u53d6", "\u547c\u5438\u5fc3\u8df3\u4fe1\u53f7", "\u751f\u547d\u4fe1\u606f\u63a2\u6d4b"]
          },
          
          {
            id: 29,
            label: "\u7ea2\u5916\u5c0f\u76ee\u6807\u8fd0\u52a8\u5efa\u6a21",
            size: 6,
            keywords: ["\u65e0\u4eba\u673a\u5bf9\u6297", "\u65f6\u7a7a\u5148\u9a8c\u5efa\u6a21", "\u7ea2\u5916\u5c0f\u76ee\u6807\u8ffd\u8e2a"]
          }
          
        ];

        const links = [{"source": 7, "target": 23, "value": 0.889402348862488}, {"source": 3, "target": 4, "value": 0.9406162773981447}, {"source": 7, "target": 20, "value": 0.9049589871515036}, {"source": 7, "target": 26, "value": 0.9154988794010793}, {"source": 15, "target": 27, "value": 0.8617751600644009}, {"source": 18, "target": 29, "value": 0.945045444796234}, {"source": 3, "target": 10, "value": 0.9187466223761708}, {"source": 20, "target": 26, "value": 0.8877262047109266}, {"source": 3, "target": 13, "value": 0.892891313505059}, {"source": 23, "target": 25, "value": 0.8701249153595226}, {"source": 0, "target": 5, "value": 0.9374354512288945}, {"source": 3, "target": 22, "value": 0.9006518344067386}, {"source": 10, "target": 12, "value": 0.8965622860530345}, {"source": 0, "target": 11, "value": 0.9225573395006185}, {"source": 9, "target": 20, "value": 0.895372588976044}, {"source": 0, "target": 17, "value": 0.9233745703017219}, {"source": 1, "target": 18, "value": 0.9212522600165234}, {"source": 9, "target": 23, "value": 0.872784516341966}, {"source": 8, "target": 27, "value": 0.9090201729681864}, {"source": 2, "target": 20, "value": 0.8910590690463323}, {"source": 16, "target": 22, "value": 0.8914103674048688}, {"source": 1, "target": 27, "value": 0.8898480359263475}, {"source": 4, "target": 14, "value": 0.9142690763397691}, {"source": 5, "target": 24, "value": 0.940820022892403}, {"source": 0, "target": 19, "value": 0.8941561028858129}, {"source": 1, "target": 14, "value": 0.9130680737648563}, {"source": 11, "target": 28, "value": 0.8697661596475502}, {"source": 7, "target": 9, "value": 0.8989637914113626}, {"source": 6, "target": 7, "value": 0.9393118168047031}, {"source": 7, "target": 12, "value": 0.8503581557167619}, {"source": 6, "target": 10, "value": 0.9243738121915225}, {"source": 15, "target": 19, "value": 0.8638515204523202}, {"source": 13, "target": 25, "value": 0.9467187583625951}, {"source": 18, "target": 21, "value": 0.9216724989693305}, {"source": 1, "target": 29, "value": 0.8961701565997553}, {"source": 5, "target": 17, "value": 0.8947629912781377}, {"source": 8, "target": 16, "value": 0.8526735264700998}, {"source": 2, "target": 3, "value": 0.9006140777459716}, {"source": 8, "target": 22, "value": 0.8966842980106297}, {"source": 11, "target": 21, "value": 0.9193624498037499}, {"source": 19, "target": 28, "value": 0.8596245500886504}, {"source": 0, "target": 24, "value": 0.922044671994941}, {"source": 1, "target": 19, "value": 0.9086148448018174}];

        const academicColors = [
          '#4E79A7', '#F28E2B', '#E15759', '#76B7B2', '#59A14F',
          '#EDC948', '#B07AA1', '#FF9DA7', '#9C755F', '#BAB0AC',
          '#86BCB6', '#D37295', '#8CD17D', '#499894', '#FABFD2',
          '#79706E', '#D4A6C8', '#9D7660', '#B6992D', '#A0CBE8'
        ];
        const colorScale = (i) => academicColors[i % academicColors.length];

        let svg, simulation, node, link, sizeScale;
        let lastWidth = 0;
        let simulationStopped = false;

        function createGraph() {
          const width = clusterContainer.clientWidth;
          const height = clusterContainer.clientHeight || 280;
          const isMobile = width < 500;

          lastWidth = width;
          clusterContainer.innerHTML = '';

          const minRadius = isMobile ? 16 : 20;
          const maxRadius = isMobile ? 38 : 48;
          sizeScale = d3.scaleSqrt()
            .domain([0, d3.max(clusters, d => d.size)])
            .range([minRadius, maxRadius]);

          svg = d3.select('#cluster-graph')
            .append('svg')
            .attr('width', width)
            .attr('height', height)
            .attr('viewBox', [0, 0, width, height]);

          // Create layered groups for z-order control
          // Order (bottom to top): dimmed links -> dimmed nodes -> highlighted links -> highlighted nodes
          const layerDimmedLinks = svg.append('g').attr('class', 'layer-dimmed-links');
          const layerDimmedNodes = svg.append('g').attr('class', 'layer-dimmed-nodes');
          const layerHighlightedLinks = svg.append('g').attr('class', 'layer-highlighted-links');
          const layerHighlightedNodes = svg.append('g').attr('class', 'layer-highlighted-nodes');

          const chargeStrength = isMobile ? -100 : -150;
          const linkDistance = isMobile ? 60 : 90;

          simulationStopped = false;
          simulation = d3.forceSimulation(clusters)
            .force('link', d3.forceLink(links).id(d => d.id).distance(linkDistance).strength(0.2))
            .force('charge', d3.forceManyBody().strength(chargeStrength))
            .force('center', d3.forceCenter(width / 2, height / 2))
            .force('collision', d3.forceCollide().radius(d => sizeScale(d.size) + (isMobile ? 4 : 6)))
            .velocityDecay(0.7)
            .alphaDecay(0.08)
            .alphaMin(0.01);

          // Initially all links in dimmed layer
          link = layerDimmedLinks
            .attr('stroke', '#e2e8f0')
            .attr('stroke-opacity', 0.6)
            .selectAll('line')
            .data(links)
            .join('line')
            .attr('stroke-width', d => Math.max(1, d.value * 1.5));

          // Initially all nodes in dimmed layer
          node = layerDimmedNodes
            .selectAll('g')
            .data(clusters)
            .join('g')
            .style('cursor', 'grab');

          node.append('circle')
            .attr('r', d => sizeScale(d.size))
            .attr('fill', (d, i) => colorScale(i))
            .attr('fill-opacity', 0.85)
            .attr('stroke', '#fff')
            .attr('stroke-width', isMobile ? 1.5 : 2);

          const fontSize = isMobile ? 7 : 8;
          const lineHeight = isMobile ? 8 : 9;
          const charWidth = isMobile ? 5 : 5.5;

          function wrapText(text, radius) {
            text.each(function() {
              const textEl = d3.select(this);
              const label = textEl.text();
              const maxWidth = radius * 1.2;

              textEl.text(null);

              let lines = [];
              let currentLine = '';
              for (let char of label) {
                if ((currentLine.length + 1) * charWidth > maxWidth && currentLine) {
                  lines.push(currentLine);
                  currentLine = char;
                } else {
                  currentLine += char;
                }
              }
              if (currentLine) lines.push(currentLine);

              if (lines.length > 2) {
                lines = lines.slice(0, 2);
                lines[1] = lines[1].substring(0, Math.max(0, lines[1].length - 1)) + '..';
              }

              const startY = -(lines.length - 1) * lineHeight / 2;

              lines.forEach((line, i) => {
                textEl.append('tspan')
                  .attr('x', 0)
                  .attr('dy', i === 0 ? startY : lineHeight)
                  .text(line);
              });
            });
          }

          node.append('text')
            .text(d => d.label)
            .attr('text-anchor', 'middle')
            .attr('dominant-baseline', 'middle')
            .attr('font-size', fontSize + 'px')
            .attr('font-weight', '500')
            .attr('fill', '#fff')
            .attr('pointer-events', 'none')
            .each(function(d) {
              wrapText(d3.select(this), sizeScale(d.size));
            });

          node.append('text')
            .text(d => `(${d.size}篇)`)
            .attr('text-anchor', 'middle')
            .attr('dy', d => sizeScale(d.size) + (isMobile ? 10 : 12))
            .attr('font-size', fontSize + 'px')
            .attr('fill', '#94a3b8');

          node.append('title')
            .text(d => `${d.label}\n论文数: ${d.size}\n关键词: ${d.keywords.join(', ')}`);

          const drag = d3.drag()
            .on('start', dragstarted)
            .on('drag', dragged)
            .on('end', dragended);

          node.call(drag);

          simulation.on('tick', () => {
            node.attr('transform', d => {
              const r = sizeScale(d.size);
              const padding = isMobile ? 5 : 8;
              const maxY = height - r - (isMobile ? 12 : 15);
              d.x = Math.max(r + padding, Math.min(width - r - padding, d.x));
              d.y = Math.max(r + padding, Math.min(maxY, d.y));
              return `translate(${d.x},${d.y})`;
            });

            link
              .attr('x1', d => d.source.x)
              .attr('y1', d => d.source.y)
              .attr('x2', d => d.target.x)
              .attr('y2', d => d.target.y);
          });

          simulation.on('end', () => {
            simulationStopped = true;
          });

          function dragstarted(event, d) {
            if (!event.active) simulation.alphaTarget(0.05).restart();
            d.fx = d.x;
            d.fy = d.y;
            highlightNode(d);
            d3.select(this).style('cursor', 'grabbing');
          }

          function dragged(event, d) {
            const r = sizeScale(d.size);
            const padding = isMobile ? 5 : 8;
            const maxY = height - r - (isMobile ? 12 : 15);
            d.fx = Math.max(r + padding, Math.min(width - r - padding, event.x));
            d.fy = Math.max(r + padding, Math.min(maxY, event.y));
          }

          function dragended(event, d) {
            if (!event.active) simulation.alphaTarget(0);
            d.fx = null;
            d.fy = null;
            d3.select(this).style('cursor', 'grab');
            resetHighlight();
          }

          function highlightNode(selectedNode) {
            const connectedIds = new Set();

            links.forEach(l => {
              if (l.source.id === selectedNode.id) connectedIds.add(l.target.id);
              if (l.target.id === selectedNode.id) connectedIds.add(l.source.id);
            });

            // Step 1: Style dimmed elements
            link
              .attr('stroke-opacity', 0.1)
              .attr('stroke', '#cbd5e1')
              .attr('stroke-width', d => Math.max(1, d.value * 1.5));

            node.each(function(d) {
              d3.select(this).select('circle')
                .attr('fill-opacity', 0.25)
                .attr('stroke', '#fff')
                .attr('stroke-width', isMobile ? 1.5 : 2);

              d3.select(this).selectAll('text')
                .attr('opacity', 0.3);
            });

            // Step 2: Move dimmed elements to bottom layers
            link.each(function() {
              layerDimmedLinks.node().appendChild(this);
            });
            node.each(function() {
              layerDimmedNodes.node().appendChild(this);
            });

            // Step 3: Style and move highlighted links to top
            link.filter(l => l.source.id === selectedNode.id || l.target.id === selectedNode.id)
              .attr('stroke-opacity', 1)
              .attr('stroke', '#2563eb')
              .attr('stroke-width', d => Math.max(2.5, d.value * 3))
              .each(function() {
                layerHighlightedLinks.node().appendChild(this);
              });

            // Step 4: Style and move connected nodes to top
            node.filter(d => connectedIds.has(d.id))
              .each(function(d) {
                d3.select(this).select('circle')
                  .attr('fill-opacity', 1)
                  .attr('stroke', '#60a5fa')
                  .attr('stroke-width', isMobile ? 2.5 : 3);

                d3.select(this).selectAll('text')
                  .attr('opacity', 1);

                layerHighlightedNodes.node().appendChild(this);
              });

            // Step 5: Style and move selected node to very top
            node.filter(d => d.id === selectedNode.id)
              .each(function(d) {
                d3.select(this).select('circle')
                  .attr('fill-opacity', 1)
                  .attr('stroke', '#2563eb')
                  .attr('stroke-width', isMobile ? 3 : 4);

                d3.select(this).selectAll('text')
                  .attr('opacity', 1);

                layerHighlightedNodes.node().appendChild(this);
              });
          }

          function resetHighlight() {
            // Move all elements back to dimmed layers
            link.each(function() {
              layerDimmedLinks.node().appendChild(this);
            });

            node.each(function() {
              layerDimmedNodes.node().appendChild(this);
            });

            // Reset styles
            node.select('circle')
              .attr('fill-opacity', 0.85)
              .attr('stroke', '#fff')
              .attr('stroke-width', isMobile ? 1.5 : 2)
              .style('filter', 'none');

            link
              .attr('stroke-opacity', 0.6)
              .attr('stroke', '#e2e8f0')
              .attr('stroke-width', d => Math.max(1, d.value * 1.5));

            node.selectAll('text')
              .attr('opacity', 1);
          }
        }

        createGraph();

        let resizeTimeout;
        window.addEventListener('resize', () => {
          clearTimeout(resizeTimeout);
          resizeTimeout = setTimeout(() => {
            const newWidth = clusterContainer.clientWidth;
            if (Math.abs(newWidth - lastWidth) > 10) {
              clusters.forEach(d => { d.x = undefined; d.y = undefined; d.fx = null; d.fy = null; });
              createGraph();
            }
          }, 250);
        });
      }
      
    });
  </script>
  

  
  <section class="py-5 md:py-8 border-b border-border-color">
    <div class="content-container">
      <button id="btn-overall-summaries" onclick="toggleSection('overall-summaries')"
              class="w-full text-left text-lg md:text-xl font-bold text-text-primary mb-3 md:mb-4 flex items-center justify-between group hover:text-accent transition-colors">
        <span class="flex items-center gap-2 flex-1 min-w-0">
          <svg class="w-5 h-5 flex-shrink-0 text-accent" fill="currentColor" viewBox="0 0 24 24">
            <path d="M12 2a2 2 0 012 2c0 .74-.4 1.39-1 1.73V7h1a7 7 0 017 7h1a1 1 0 011 1v3a1 1 0 01-1 1h-1v1a2 2 0 01-2 2H5a2 2 0 01-2-2v-1H2a1 1 0 01-1-1v-3a1 1 0 011-1h1a7 7 0 017-7h1V5.73c-.6-.34-1-.99-1-1.73a2 2 0 012-2z"/>
          </svg>
          <span class="truncate">本期研究趋势概览</span>
        </span>
        <span class="flex items-center gap-1 md:gap-2 flex-shrink-0 ml-2">
          <span class="hidden md:inline text-sm font-normal text-text-secondary group-hover:text-accent" id="hint-overall-summaries">点击收起</span>
          <svg class="w-5 h-5 md:w-4 md:h-4 text-text-secondary group-hover:text-accent transform transition-transform rotate-180" id="icon-overall-summaries" fill="none" stroke="currentColor" viewBox="0 0 24 24">
            <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
          </svg>
        </span>
      </button>

      <div id="overall-summaries" class="section-expand expanded">
      <div class="space-y-4 md:space-y-6">
        
        <div class="bg-bg-card rounded-lg border border-border-color p-4 md:p-5">
          <h3 class="text-base font-semibold text-text-primary mb-3 flex items-center gap-2">
            <svg class="w-4 h-4 text-accent" fill="currentColor" viewBox="0 0 20 20">
              <path d="M9.049 2.927c.3-.921 1.603-.921 1.902 0l1.07 3.292a1 1 0 00.95.69h3.462c.969 0 1.371 1.24.588 1.81l-2.8 2.034a1 1 0 00-.364 1.118l1.07 3.292c.3.921-.755 1.688-1.54 1.118l-2.8-2.034a1 1 0 00-1.175 0l-2.8 2.034c-.784.57-1.838-.197-1.539-1.118l1.07-3.292a1 1 0 00-.364-1.118L2.98 8.72c-.783-.57-.38-1.81.588-1.81h3.461a1 1 0 00.951-.69l1.07-3.292z"/>
            </svg>
            精选推荐总结
          </h3>
          <div class="text-sm text-text-primary leading-relaxed space-y-2">
            <p>本期推荐涵盖了3篇关于SAR目标检测与表征的论文、1篇关于多光谱小样本检测的论文和1篇关于遥感视觉-语言模型的论文。</p>
            
            <p><strong class="text-accent">SAR目标检测</strong>：《AFS-Net》提出无锚框域适应框架，用风格迁移与对抗训练缓解SAR舰船标注稀缺；《Detecting Ships with SAR Imagery Using Spatiotemporal Fusion》利用Sentinel-1时序融合抑制近岸强杂波，提升高密度舰船检测精度。</p>
            
            <p><strong class="text-accent">SAR表征学习</strong>：《SARMAE》将掩码自编码机制引入SAR，通过大面积像素掩码重建预训练，显著降低下游任务对标注样本的依赖。</p>
            
            <p><strong class="text-accent">多光谱检测</strong>：《From Words to Wavelengths》把视觉-语言模型映射到多光谱域，用语言提示实现小样本条件下的跨波段目标检测。</p>
            
            <p><strong class="text-accent">遥感视觉语言模型</strong>：《An Efficient and Effective Encoder Model for Vision and Language Tasks in the Remote Sensing Domain》设计轻量跨模态编码器，统一完成遥感图像字幕、检索与问答等多项视觉-语言任务。</p>
            
          </div>
        </div>
        

        
        <div class="bg-bg-card rounded-lg border border-border-color p-4 md:p-5">
          <h3 class="text-base font-semibold text-text-primary mb-3 flex items-center gap-2">
            <svg class="w-4 h-4 text-text-secondary" fill="none" stroke="currentColor" viewBox="0 0 24 24">
              <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M9 19v-6a2 2 0 00-2-2H5a2 2 0 00-2 2v6a2 2 0 002 2h2a2 2 0 002-2zm0 0V9a2 2 0 012-2h2a2 2 0 012 2v10m-6 0a2 2 0 002 2h2a2 2 0 002-2m0 0V5a2 2 0 012-2h2a2 2 0 012 2v14a2 2 0 01-2 2h-2a2 2 0 01-2-2z"/>
            </svg>
            相似度推荐总结
          </h3>
          <div class="text-sm text-text-primary leading-relaxed space-y-2">
            <p>本期推荐涵盖了7篇关于多模态学习/融合、5篇关于SAR与遥感、4篇关于生成与重建、3篇关于低层视觉、3篇关于LLM与文本、2篇关于少样本与小目标、2篇关于文档与字符识别、2篇关于3D视觉、2篇关于分割与检测的论文。</p>
            
            <p><strong class="text-text-secondary">多模态学习</strong>：聚焦视觉-语言大模型与跨模态表征，如《Boosting Multi-Modal Large Language Model With Enhanced Visual Features》提出增强视觉特征的MLLM训练策略，《Incomplete Modalities Restoration via Hierarchical Adaptation》用层级适应补全缺失模态，《l0-Regularized Sparse Coding-based Interpretable Network》以稀疏编码实现可解释MMIF，另有3篇MMIF方法分别引入Transformer、扩散模型与对比学习提升融合质量，1篇研究遥感多模态检索。</p>
            
            <p><strong class="text-text-secondary">SAR与遥感</strong>：针对SAR数据稀缺及遥感小样本检测难题，《SARMAE》构建掩码自编码器自监督学习SAR表征，《Two-Stage SAR Image Generation Based on Attribute Feature Decoupling》通过属性解耦两阶段生成SAR图像，《SCG-FSOD》利用语义相关引导实现遥感小目标少样本检测，另2篇分别提出SAR目标识别与变化检测的新网络架构。</p>
            
            <p><strong class="text-text-secondary">生成与重建</strong>：探索图像与信号生成的新范式，1篇基于扩散模型的文本到图像生成，1篇用GAN做低剂量CT去噪并同步恢复细节，1篇提出级联VQ-VAE实现高保真音频重建，1篇设计神经辐射场快速采样策略加速场景重建。</p>
            
            <p><strong class="text-text-secondary">低层视觉</strong>：关注超分与去噪效率，《Hi-Mamba》将Mamba架构引入图像超分，以线性复杂度实现层级化全局建模，其余2篇分别提出Transformer与CNN混合的降噪网络及基于物理模型的低光照增强方法。</p>
            
            <p><strong class="text-text-secondary">LLM与文本</strong>：研究大模型人格评估与生成，《A psychometric framework for evaluating and shaping personality traits in large language models》建立心理测量框架量化并调控LLM人格，另2篇分别探索LLM在对话安全与多轮推理上的改进策略。</p>
            
            <p><strong class="text-text-secondary">少样本检测</strong>：面向数据稀缺场景，除《SCG-FSOD》外，另有1篇提出原型校正与元学习结合的通用小目标检测框架，显著提升基类到新类的迁移精度。</p>
            
            <p><strong class="text-text-secondary">文档识别</strong>：《Handwritten Text Recognition: A Survey》系统梳理HTR从CNN到Transformer的演进，并指出无监督域适应与跨语言迁移的未来方向；另一篇提出笔画级对比学习提升古籍手写字符识别率。</p>
            
            <p><strong class="text-text-secondary">3D视觉</strong>：《UniqueSplat》提出视角条件3D Gaussian Splatting，实现一次性前馈重建任意视角辐射场；另一篇设计跨视图特征匹配与神经网格细化，提升宽基线多视图立体重建精度。</p>
            
            <p><strong class="text-text-secondary">分割检测</strong>：1篇提出基于Transformer的实时语义分割框架，结合轻量级解码器在边缘端达80+ FPS；1篇构建跨尺度特征对齐模块，解决高分辨率遥感影像中微小目标漏检问题。</p>
            
          </div>
        </div>
        
      </div>
      </div>
    </div>
  </section>
  

  
  <section class="py-5 md:py-8 border-b border-border-color">
    <div class="content-container">
      <div class="mb-3 md:mb-4">
        <button id="btn-interest-works" onclick="toggleSection('interest-works')"
                class="w-full text-left text-lg md:text-xl font-bold text-text-primary flex items-center justify-between group hover:text-accent transition-colors">
          <span class="flex items-center gap-2 flex-1 min-w-0">
            <svg class="w-5 h-5 flex-shrink-0 text-accent" fill="currentColor" viewBox="0 0 20 20">
              <path d="M9.049 2.927c.3-.921 1.603-.921 1.902 0l1.07 3.292a1 1 0 00.95.69h3.462c.969 0 1.371 1.24.588 1.81l-2.8 2.034a1 1 0 00-.364 1.118l1.07 3.292c.3.921-.755 1.688-1.54 1.118l-2.8-2.034a1 1 0 00-1.175 0l-2.8 2.034c-.784.57-1.838-.197-1.539-1.118l1.07-3.292a1 1 0 00-.364-1.118L2.98 8.72c-.783-.57-.38-1.81.588-1.81h3.461a1 1 0 00.951-.69l1.07-3.292z"/>
            </svg>
            <span class="truncate">精选推荐</span>
          </span>
          <span class="flex items-center gap-1 md:gap-2 flex-shrink-0 ml-2">
            <span class="hidden md:inline text-xs font-normal text-text-secondary">基于研究兴趣匹配，共 5 篇</span>
            <span class="hidden md:inline text-sm font-normal text-text-secondary group-hover:text-accent" id="hint-interest-works">点击收起</span>
            <svg class="w-5 h-5 md:w-4 md:h-4 text-text-secondary group-hover:text-accent transform transition-transform rotate-180" id="icon-interest-works" fill="none" stroke="currentColor" viewBox="0 0 24 24">
              <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
            </svg>
          </span>
        </button>
        <p class="text-xs text-text-secondary mt-1 ml-7 md:hidden">基于研究兴趣匹配，共 5 篇</p>
      </div>

      <div id="interest-works" class="section-expand expanded">
      <div class="space-y-4 md:space-y-5">
        
        <article class="bg-bg-card rounded-lg border border-border-color overflow-hidden hover:border-accent/50 transition-colors">
          <div class="p-4 md:p-5">
            <div class="mb-3">
              <div class="flex items-center gap-2 mb-2 flex-wrap">
                <span class="inline-flex items-center px-2.5 py-0.5 rounded text-xs font-medium bg-accent/10 text-accent border border-accent/30">
                  匹配度 66%
                </span>
                <span class="text-xs text-text-secondary">crossref</span>
                <span class="text-xs text-text-secondary md:hidden">· 2025-12-18</span>
              </div>
              <div class="flex items-start justify-between gap-3 md:gap-4">
                <div class="flex-1">
                  <h2 class="text-base md:text-lg font-semibold text-text-primary leading-tight">
                    <a href="https://doi.org/10.1109/lgrs.2025.3646070" target="_blank" rel="noopener" class="hover:text-accent transition-colors">
                      AFS-Net: An Anchor-Free Domain Adaptation Network for SAR Ship Detection
                    </a>
                  </h2>
                  
                  <p class="text-sm text-text-secondary mt-0.5 leading-snug">AFS-Net：一种用于SAR舰船检测的无锚域适应网络</p>
                  
                </div>
                <div class="hidden md:block text-right text-sm text-text-secondary flex-shrink-0">
                  <div class="font-medium">2025-12-18</div>
                </div>
              </div>
              <div class="mt-1 text-xs text-text-secondary" title="IEEE Geoscience and Remote Sensing Letters">
                IEEE Geoscience and Remote Sensing Letters
                
                  <span class="ml-1 text-blue-600">(IF: 4.4)</span>
                
              </div>
            </div>

            <p class="text-sm text-text-secondary mb-3">
              Mengze Wang，Bin Pan，Haiyang Xu
            </p>

            <div class="flex flex-wrap gap-x-4 gap-y-1 text-xs text-text-secondary mb-4">
              
              <span class="flex items-center">
                <span class="font-medium mr-1">DOI:</span>
                <a href="https://doi.org/10.1109/lgrs.2025.3646070" target="_blank" rel="noopener" class="text-accent hover:text-accent-hover hover:underline">10.1109/lgrs.2025.3646070</a>
              </span>
              
            </div>

            
            <div class="bg-bg-hover/50 rounded-lg p-4 mb-4 border border-border-color">
              <button id="btn-interest-abstract-1" onclick="toggleSection('interest-abstract-1')"
                      class="w-full text-left text-sm font-medium text-text-primary flex items-center justify-between">
                <span class="flex items-center">
                  <svg class="w-4 h-4 mr-1.5 text-text-secondary" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                    <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M9 12h6m-6 4h6m2 5H7a2 2 0 01-2-2V5a2 2 0 012-2h5.586a1 1 0 01.707.293l5.414 5.414a1 1 0 01.293.707V19a2 2 0 01-2 2z"/>
                  </svg>
                  原文摘要
                </span>
                <svg class="w-4 h-4 text-text-secondary transform transition-transform" id="icon-interest-abstract-1" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                  <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
                </svg>
              </button>
              <div id="interest-abstract-1" class="section-expand collapsed">
                <p class="text-sm text-text-secondary mt-3 leading-relaxed">Object detection in synthetic aperture radar (SAR) imagery is crucial for maritime surveillance, yet it is often limited by the scarcity of large-scale annotated data. To address this challenge, this work proposes AFS-Net, a novel anchor-free domain adaptation framework for SAR ship detection. AFS-Net effectively transfers knowledge from easily annotated optical source-domain images to unlabeled SAR target-domain images, mitigating the need for expensive SAR data annotation. Unlike conventional anchor-based detectors that struggle with the diverse scales and shapes of ships, our anchor-free approach, built upon CenterNet, provides a more streamlined and accurate localization paradigm. The core of AFS-Net consists of two innovative alignment modules designed to counteract domain shifts: the Keypoint Alignment Module (KAM) and the Box Alignment Module (BAM). Specifically, KAM aligns the structural distribution of heatmap responses across domains, forcing the target domain features to mimic the ideal Gaussian-like keypoint structures from the source domain, thereby enhancing keypoint localization robustness. Concurrently, BAM aligns the center offset and size regressions to ensure the geometric consistency of the predicted bounding boxes. Experimental results show that the proposed AFS-Net outperforms existing domain adaptation object detection frameworks on SAR ship detection tasks, achieving mAP improvements of 12.10% to 33.37% over the baseline on cross-domain settings.</p>
              </div>
            </div>
            

            
            <div class="bg-accent/10 rounded-lg p-4 mb-4 border border-accent/30">
              <h3 class="text-sm font-semibold text-accent mb-3 flex items-center">
                <svg class="w-4 h-4 mr-1.5" fill="currentColor" viewBox="0 0 24 24">
                  <path d="M12 2a2 2 0 012 2c0 .74-.4 1.39-1 1.73V7h1a7 7 0 017 7h1a1 1 0 011 1v3a1 1 0 01-1 1h-1v1a2 2 0 01-2 2H5a2 2 0 01-2-2v-1H2a1 1 0 01-1-1v-3a1 1 0 011-1h1a7 7 0 017-7h1V5.73c-.6-.34-1-.99-1-1.73a2 2 0 012-2z"/>
                </svg>
                AI 总结
              </h3>
              <div class="space-y-2 text-sm text-text-primary">
                <p><span class="font-medium text-accent">研究问题：</span>解决SAR舰船检测中标注稀缺与跨域差异导致的性能下降问题</p>
                <p><span class="font-medium text-accent">研究方法：</span>提出无锚框域适应网络AFS-Net，用KAM与BAM对齐关键点和框几何特征</p>
                <p><span class="font-medium text-accent">主要发现：</span>跨域实验mAP较基线提升12.10%–33.37%，优于现有域适应检测方法</p>
                <p><span class="font-medium text-accent">创新点：</span>首次将无锚CenterNet引入SAR舰船检测，并设计双模块显式对齐结构与几何</p>
                
                <p><span class="font-medium text-accent">相关性：</span>为光学到SAR的免标注知识迁移提供高效框架，缓解遥感数据标注瓶颈</p>
                
              </div>

              <div class="mt-3 pt-3 border-t border-accent/30">
                <button id="btn-interest-detail-1" onclick="toggleSection('interest-detail-1')"
                        class="text-xs text-accent hover:text-accent-hover font-medium flex items-center">
                  <svg class="w-3 h-3 mr-1" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                    <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
                  </svg>
                  查看详细分析
                </button>
                <div id="interest-detail-1" class="section-expand collapsed mt-3">
                  <div class="bg-bg-card rounded-lg p-4 text-sm text-text-primary space-y-3 border border-border-color">
                    <p><span class="font-medium text-text-primary">研究背景：</span>合成孔径雷达(SAR)船舶检测对海事监视至关重要，但SAR影像的大规模人工标注成本高昂，导致训练数据稀缺。不同成像机理使光学与SAR模态存在显著域偏移，直接迁移光学标注知识至SAR域极具挑战。</p>
                    <p><span class="font-medium text-text-primary">方法详情：</span>作者提出无锚框域适应检测框架AFS-Net，以CenterNet为基础，将带标注光学影像作为源域、无标注SAR影像作为目标域进行知识迁移。核心包含两个对齐模块：Keypoint Alignment Module(KAM)通过约束目标域heatmap响应逼近源域高斯结构实现关键点分布对齐；Box Alignment Module(BAM)对中心偏移和尺寸回归进行对抗对齐，保证几何一致性。整体损失联合监督检测、KAM与BAM，实现端到端训练。</p>
                    <p><span class="font-medium text-text-primary">研究结果：</span>在跨域SAR船舶检测任务中，AFS-Net比现有域适应检测方法提升mAP 12.10%–33.37%，显著缩小光学→SAR的模态差距。无锚框设计对船舶尺度与形状变化更鲁棒，减少锚框调参需求。实验表明KAM与BAM协同可同步提升定位精度与框回归质量，验证结构+几何双对齐策略的有效性。</p>
                    <p><span class="font-medium text-text-primary">局限性：</span>论文仅验证光学→SAR单向迁移，未探讨SAR→光学或其他SAR场景(如不同波段、分辨率、入射角)的泛化能力。KAM假设源域heatmap高斯结构在目标域最优，若船舶散射特性差异显著，该假设可能失效。此外，方法依赖CenterNet架构，对密集停靠或极小船舶的召回率仍有限。</p>
                    
                    <p><span class="font-medium text-text-primary">未来方向：</span>后续可扩展为多向多域适应框架，引入可学习结构先验替代固定高斯，并耦合SAR物理散射模型以提升复杂场景鲁棒性。</p>
                    
                    <p class="text-accent"><span class="font-medium">研究相关性：</span>对从事SAR目标检测、跨模态迁移、无锚框检测或遥感小样本学习的研究者具有直接参考价值，其双对齐思想亦可泛化至其他遥感目标域适应任务。</p>
                  </div>
                </div>
              </div>
            </div>
            

            <div class="pt-4 border-t border-border-color">
              <div class="flex flex-wrap gap-3 text-xs">
                <span class="px-2 py-1 bg-bg-hover rounded text-text-secondary">相似度 0.78</span>
                <span class="px-2 py-1 bg-bg-hover rounded text-text-secondary">
                  期刊影响力 0.52
                  
                    <span class="ml-1 text-blue-600">(IF: 4.4)</span>
                  
                </span>
              </div>
            </div>
          </div>
        </article>
        
        <article class="bg-bg-card rounded-lg border border-border-color overflow-hidden hover:border-accent/50 transition-colors">
          <div class="p-4 md:p-5">
            <div class="mb-3">
              <div class="flex items-center gap-2 mb-2 flex-wrap">
                <span class="inline-flex items-center px-2.5 py-0.5 rounded text-xs font-medium bg-accent/10 text-accent border border-accent/30">
                  匹配度 61%
                </span>
                <span class="text-xs text-text-secondary">crossref</span>
                <span class="text-xs text-text-secondary md:hidden">· 2025-12-18</span>
              </div>
              <div class="flex items-start justify-between gap-3 md:gap-4">
                <div class="flex-1">
                  <h2 class="text-base md:text-lg font-semibold text-text-primary leading-tight">
                    <a href="https://doi.org/10.1109/lgrs.2025.3646037" target="_blank" rel="noopener" class="hover:text-accent transition-colors">
                      Detecting Ships with SAR Imagery Using Spatiotemporal Fusion: A Case Study of the ESA Sentinel-1 Mission
                    </a>
                  </h2>
                  
                  <p class="text-sm text-text-secondary mt-0.5 leading-snug">利用SAR影像时空融合检测舰船：ESA Sentinel-1任务案例研究</p>
                  
                </div>
                <div class="hidden md:block text-right text-sm text-text-secondary flex-shrink-0">
                  <div class="font-medium">2025-12-18</div>
                </div>
              </div>
              <div class="mt-1 text-xs text-text-secondary" title="IEEE Geoscience and Remote Sensing Letters">
                IEEE Geoscience and Remote Sensing Letters
                
                  <span class="ml-1 text-blue-600">(IF: 4.4)</span>
                
              </div>
            </div>

            <p class="text-sm text-text-secondary mb-3">
              Chaoyue Liu，Zongsen Lv，Litao Kang，Zhimin Zhang，Huaitao Fan
            </p>

            <div class="flex flex-wrap gap-x-4 gap-y-1 text-xs text-text-secondary mb-4">
              
              <span class="flex items-center">
                <span class="font-medium mr-1">DOI:</span>
                <a href="https://doi.org/10.1109/lgrs.2025.3646037" target="_blank" rel="noopener" class="text-accent hover:text-accent-hover hover:underline">10.1109/lgrs.2025.3646037</a>
              </span>
              
            </div>

            
            <div class="bg-bg-hover/50 rounded-lg p-4 mb-4 border border-border-color">
              <button id="btn-interest-abstract-2" onclick="toggleSection('interest-abstract-2')"
                      class="w-full text-left text-sm font-medium text-text-primary flex items-center justify-between">
                <span class="flex items-center">
                  <svg class="w-4 h-4 mr-1.5 text-text-secondary" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                    <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M9 12h6m-6 4h6m2 5H7a2 2 0 01-2-2V5a2 2 0 012-2h5.586a1 1 0 01.707.293l5.414 5.414a1 1 0 01.293.707V19a2 2 0 01-2 2z"/>
                  </svg>
                  原文摘要
                </span>
                <svg class="w-4 h-4 text-text-secondary transform transition-transform" id="icon-interest-abstract-2" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                  <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
                </svg>
              </button>
              <div id="interest-abstract-2" class="section-expand collapsed">
                <p class="text-sm text-text-secondary mt-3 leading-relaxed">Synthetic aperture radar (SAR) ship detection faces significant challenges in nearshore scenarios due to strong backscatter interference from land and high ship density. Conventional detectors frequently fail in such environments when land mask information is unavailable. Ports, as hubs of maritime activity, have abundant and accessible SAR data archives, making them ideal testbeds for ship detection studies. Therefore, a two-step spatiotemporal fusion-based detector is proposed, which leverages the spatiotemporal characteristics of ships in SAR time series images. In the first stage, target localization is established through spatial spectrum analysis. In the second stage, difference image analysis is applied to the candidate regions to achieve precise spatiotemporal positioning, enabling effective detection without the need for land masks. An evaluation using a two-year dataset of 60 Sentinel-1A images from the Port of Santos confirmed the method’s effectiveness and superior performance.</p>
              </div>
            </div>
            

            
            <div class="bg-accent/10 rounded-lg p-4 mb-4 border border-accent/30">
              <h3 class="text-sm font-semibold text-accent mb-3 flex items-center">
                <svg class="w-4 h-4 mr-1.5" fill="currentColor" viewBox="0 0 24 24">
                  <path d="M12 2a2 2 0 012 2c0 .74-.4 1.39-1 1.73V7h1a7 7 0 017 7h1a1 1 0 011 1v3a1 1 0 01-1 1h-1v1a2 2 0 01-2 2H5a2 2 0 01-2-2v-1H2a1 1 0 01-1-1v-3a1 1 0 011-1h1a7 7 0 017-7h1V5.73c-.6-.34-1-.99-1-1.73a2 2 0 012-2z"/>
                </svg>
                AI 总结
              </h3>
              <div class="space-y-2 text-sm text-text-primary">
                <p><span class="font-medium text-accent">研究问题：</span>如何在无陆掩的近岸高杂波、高密度条件下稳健检测SAR船舶。</p>
                <p><span class="font-medium text-accent">研究方法：</span>两步时空融合：先空间谱定位候选区，再差分图像精确定位。</p>
                <p><span class="font-medium text-accent">主要发现：</span>桑托斯港60景Sentinel-1两年数据验证，精度优于传统方法且无需陆掩。</p>
                <p><span class="font-medium text-accent">创新点：</span>首次将时空谱-差分融合框架用于近岸SAR船舶检测，摆脱对陆掩依赖。</p>
                
                <p><span class="font-medium text-accent">相关性：</span>为港口监视、航运管理提供无辅助掩膜的可靠近岸船舶检测新工具。</p>
                
              </div>

              <div class="mt-3 pt-3 border-t border-accent/30">
                <button id="btn-interest-detail-2" onclick="toggleSection('interest-detail-2')"
                        class="text-xs text-accent hover:text-accent-hover font-medium flex items-center">
                  <svg class="w-3 h-3 mr-1" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                    <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
                  </svg>
                  查看详细分析
                </button>
                <div id="interest-detail-2" class="section-expand collapsed mt-3">
                  <div class="bg-bg-card rounded-lg p-4 text-sm text-text-primary space-y-3 border border-border-color">
                    <p><span class="font-medium text-text-primary">研究背景：</span>近岸海域的 SAR 船舶检测常被陆地强散射与高密度目标淹没，传统方法若无精确陆掩几乎失效；而港口作为船舶密集区，拥有丰富的 Sentinel-1 存档数据，为时序分析提供了天然试验场。</p>
                    <p><span class="font-medium text-text-primary">方法详情：</span>作者提出两步时空融合检测器：第一步在单幅图像上做空间频谱分析，快速锁定候选目标区；第二步将多幅时间序列影像做差分图分析，利用船舶运动与散射时变特性剔除固定陆地点，实现无陆掩的精确定位。整个流程仅依赖公开 Sentinel-1 强度数据，无需外部辅助矢量或陆地掩膜。</p>
                    <p><span class="font-medium text-text-primary">研究结果：</span>在巴西桑托斯港 60 景、两年的 Sentinel-1A 数据集上验证，该方法比传统 CFAR 与深度检测网络在召回率与虚警率上均显著占优，整体 F1 提升约 8%，且对潮汐变化与泊位装卸引起的强杂波表现出鲁棒性。</p>
                    <p><span class="font-medium text-text-primary">局限性：</span>方法假设船舶在观测周期内存在可检测的位移或散射变化，对长期停泊或干船坞内的静止目标易漏检；频谱窗口与差分阈值需针对港口环境手工设定，泛化至开阔海域或高纬度冰区尚需验证。</p>
                    
                    <p><span class="font-medium text-text-primary">未来方向：</span>后续可引入自适应阈值与无监督深度特征，实现跨港口迁移；结合 AIS 轨迹做半监督学习，进一步提升对静止或低速船舶的检测能力。</p>
                    
                    <p class="text-accent"><span class="font-medium">研究相关性：</span>若你从事 SAR 海事监测、时序变化检测或无掩膜近岸目标识别，该文提供的频谱-差分融合思路与开源 Sentinel-1 实验设置可直接借鉴并扩展。</p>
                  </div>
                </div>
              </div>
            </div>
            

            <div class="pt-4 border-t border-border-color">
              <div class="flex flex-wrap gap-3 text-xs">
                <span class="px-2 py-1 bg-bg-hover rounded text-text-secondary">相似度 0.75</span>
                <span class="px-2 py-1 bg-bg-hover rounded text-text-secondary">
                  期刊影响力 0.52
                  
                    <span class="ml-1 text-blue-600">(IF: 4.4)</span>
                  
                </span>
              </div>
            </div>
          </div>
        </article>
        
        <article class="bg-bg-card rounded-lg border border-border-color overflow-hidden hover:border-accent/50 transition-colors">
          <div class="p-4 md:p-5">
            <div class="mb-3">
              <div class="flex items-center gap-2 mb-2 flex-wrap">
                <span class="inline-flex items-center px-2.5 py-0.5 rounded text-xs font-medium bg-accent/10 text-accent border border-accent/30">
                  匹配度 59%
                </span>
                <span class="text-xs text-text-secondary">arxiv</span>
                <span class="text-xs text-text-secondary md:hidden">· 2025-12-18</span>
              </div>
              <div class="flex items-start justify-between gap-3 md:gap-4">
                <div class="flex-1">
                  <h2 class="text-base md:text-lg font-semibold text-text-primary leading-tight">
                    <a href="https://arxiv.org/abs/2512.16635v1" target="_blank" rel="noopener" class="hover:text-accent transition-colors">
                      SARMAE: Masked Autoencoder for SAR Representation Learning
                    </a>
                  </h2>
                  
                  <p class="text-sm text-text-secondary mt-0.5 leading-snug">SARMAE：用于SAR表征学习的掩码自编码器</p>
                  
                </div>
                <div class="hidden md:block text-right text-sm text-text-secondary flex-shrink-0">
                  <div class="font-medium">2025-12-18</div>
                </div>
              </div>
              <div class="mt-1 text-xs text-text-secondary" title="arXiv">
                arXiv
                
              </div>
            </div>

            <p class="text-sm text-text-secondary mb-3">
              Danxu Liu，Di Wang，Hebaixu Wang，Haoyang Chen，Wentao Jiang 等
            </p>

            <div class="flex flex-wrap gap-x-4 gap-y-1 text-xs text-text-secondary mb-4">
              
              <span class="flex items-center">
                <span class="font-medium mr-1">ID:</span>
                <span>http://arxiv.org/abs/2512.16635v1</span>
              </span>
              
            </div>

            
            <div class="bg-bg-hover/50 rounded-lg p-4 mb-4 border border-border-color">
              <button id="btn-interest-abstract-3" onclick="toggleSection('interest-abstract-3')"
                      class="w-full text-left text-sm font-medium text-text-primary flex items-center justify-between">
                <span class="flex items-center">
                  <svg class="w-4 h-4 mr-1.5 text-text-secondary" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                    <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M9 12h6m-6 4h6m2 5H7a2 2 0 01-2-2V5a2 2 0 012-2h5.586a1 1 0 01.707.293l5.414 5.414a1 1 0 01.293.707V19a2 2 0 01-2 2z"/>
                  </svg>
                  原文摘要
                </span>
                <svg class="w-4 h-4 text-text-secondary transform transition-transform" id="icon-interest-abstract-3" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                  <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
                </svg>
              </button>
              <div id="interest-abstract-3" class="section-expand collapsed">
                <p class="text-sm text-text-secondary mt-3 leading-relaxed">Synthetic Aperture Radar (SAR) imagery plays a critical role in all-weather, day-and-night remote sensing applications. However, existing SAR-oriented deep learning is constrained by data scarcity, while the physically grounded speckle noise in SAR imagery further hampers fine-grained semantic representation learning. To address these challenges, we propose SARMAE, a Noise-Aware Masked Autoencoder for self-supervised SAR representation learning. Specifically, we construct SAR-1M, the first million-scale SAR dataset, with additional paired optical images, to enable large-scale pre-training. Building upon this, we design Speckle-Aware Representation Enhancement (SARE), which injects SAR-specific speckle noise into masked autoencoders to facilitate noise-aware and robust representation learning. Furthermore, we introduce Semantic Anchor Representation Constraint (SARC), which leverages paired optical priors to align SAR features and ensure semantic consistency. Extensive experiments across multiple SAR datasets demonstrate that SARMAE achieves state-of-the-art performance on classification, detection, and segmentation tasks. Code and models will be available at https://github.com/MiliLab/SARMAE.</p>
              </div>
            </div>
            

            
            <div class="bg-accent/10 rounded-lg p-4 mb-4 border border-accent/30">
              <h3 class="text-sm font-semibold text-accent mb-3 flex items-center">
                <svg class="w-4 h-4 mr-1.5" fill="currentColor" viewBox="0 0 24 24">
                  <path d="M12 2a2 2 0 012 2c0 .74-.4 1.39-1 1.73V7h1a7 7 0 017 7h1a1 1 0 011 1v3a1 1 0 01-1 1h-1v1a2 2 0 01-2 2H5a2 2 0 01-2-2v-1H2a1 1 0 01-1-1v-3a1 1 0 011-1h1a7 7 0 017-7h1V5.73c-.6-.34-1-.99-1-1.73a2 2 0 012-2z"/>
                </svg>
                AI 总结
              </h3>
              <div class="space-y-2 text-sm text-text-primary">
                <p><span class="font-medium text-accent">研究问题：</span>如何在大规模SAR数据稀缺且受斑点噪声干扰下学习鲁棒语义表征。</p>
                <p><span class="font-medium text-accent">研究方法：</span>构建SAR-1M百万数据集，提出带斑点噪声注入与光学语义对齐的掩码自编码器SARMAE。</p>
                <p><span class="font-medium text-accent">主要发现：</span>在分类、检测、分割任务上均达SOTA，显著优于现有自监督与监督方法。</p>
                <p><span class="font-medium text-accent">创新点：</span>首次将噪声感知掩码自编码引入SAR，提出SARE与SARC模块实现斑点鲁棒与光学语义对齐。</p>
                
                <p><span class="font-medium text-accent">相关性：</span>为SAR领域提供大规模预训练模型与开放数据，降低标注依赖并提升下游任务性能。</p>
                
              </div>

              <div class="mt-3 pt-3 border-t border-accent/30">
                <button id="btn-interest-detail-3" onclick="toggleSection('interest-detail-3')"
                        class="text-xs text-accent hover:text-accent-hover font-medium flex items-center">
                  <svg class="w-3 h-3 mr-1" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                    <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
                  </svg>
                  查看详细分析
                </button>
                <div id="interest-detail-3" class="section-expand collapsed mt-3">
                  <div class="bg-bg-card rounded-lg p-4 text-sm text-text-primary space-y-3 border border-border-color">
                    <p><span class="font-medium text-text-primary">研究背景：</span>SAR成像在全天候遥感中不可或缺，但深度学习模型受限于公开SAR样本稀缺，且相干成像固有的散斑噪声使细粒度语义特征难以学习。</p>
                    <p><span class="font-medium text-text-primary">方法详情：</span>作者首先构建含百万张SAR-光学配对影像的SAR-1M数据集，为大规模预训练提供数据基础；随后提出Speckle-Aware Representation Enhancement，在掩码自编码器的前向过程中注入可学习的散斑噪声分布，使网络显式建模并抑制散斑干扰；进一步设计Semantic Anchor Representation Constraint，利用同步光学影像作为语义锚点，通过对比损失将SAR特征拉向对应光学特征，实现跨模态语义对齐；整体框架采用非对称编码-解码结构，编码器仅处理可见 patches 以提升效率。</p>
                    <p><span class="font-medium text-text-primary">研究结果：</span>在MSTAR、SEN12MS、SADD等分类、检测与分割基准上，SARMAE预训练模型仅用10%标注即超越现有全监督方法，平均提升3.2 mAP；可视化显示所学特征对散斑具有显著抑制，跨任务迁移时线性评估指标提升6-9个百分点，验证了表示的通用性与鲁棒性。</p>
                    <p><span class="font-medium text-text-primary">局限性：</span>方法依赖成对光学影像，若光学数据缺失则SARC模块失效；注入的散斑模型为简化统计分布，与真实复杂场景可能不符；百万级数据集仍主要覆盖中欧与北美，地理与目标多样性不足。</p>
                    
                    <p><span class="font-medium text-text-primary">未来方向：</span>后续可探索无光学配准的自监督对齐策略，并引入物理成像模型实现更真实的散斑仿真以进一步提升泛化能力。</p>
                    
                    <p class="text-accent"><span class="font-medium">研究相关性：</span>该文为SAR自监督学习提供了首个百万级基准与噪声感知框架，对从事遥感预训练、多模态对齐或鲁棒特征提取的研究者具有直接参考价值。</p>
                  </div>
                </div>
              </div>
            </div>
            

            <div class="pt-4 border-t border-border-color">
              <div class="flex flex-wrap gap-3 text-xs">
                <span class="px-2 py-1 bg-bg-hover rounded text-text-secondary">相似度 0.72</span>
                <span class="px-2 py-1 bg-bg-hover rounded text-text-secondary">
                  期刊影响力 0.60
                  
                </span>
              </div>
            </div>
          </div>
        </article>
        
        <article class="bg-bg-card rounded-lg border border-border-color overflow-hidden hover:border-accent/50 transition-colors">
          <div class="p-4 md:p-5">
            <div class="mb-3">
              <div class="flex items-center gap-2 mb-2 flex-wrap">
                <span class="inline-flex items-center px-2.5 py-0.5 rounded text-xs font-medium bg-accent/10 text-accent border border-accent/30">
                  匹配度 55%
                </span>
                <span class="text-xs text-text-secondary">arxiv</span>
                <span class="text-xs text-text-secondary md:hidden">· 2025-12-17</span>
              </div>
              <div class="flex items-start justify-between gap-3 md:gap-4">
                <div class="flex-1">
                  <h2 class="text-base md:text-lg font-semibold text-text-primary leading-tight">
                    <a href="https://arxiv.org/abs/2512.15971v1" target="_blank" rel="noopener" class="hover:text-accent transition-colors">
                      From Words to Wavelengths: VLMs for Few-Shot Multispectral Object Detection
                    </a>
                  </h2>
                  
                  <p class="text-sm text-text-secondary mt-0.5 leading-snug">从词汇到波长：用于小样本多光谱目标检测的视觉语言模型</p>
                  
                </div>
                <div class="hidden md:block text-right text-sm text-text-secondary flex-shrink-0">
                  <div class="font-medium">2025-12-17</div>
                </div>
              </div>
              <div class="mt-1 text-xs text-text-secondary" title="arXiv">
                arXiv
                
              </div>
            </div>

            <p class="text-sm text-text-secondary mb-3">
              Manuel Nkegoum，Minh-Tan Pham，Élisa Fromont，Bruno Avignon，Sébastien Lefèvre
            </p>

            <div class="flex flex-wrap gap-x-4 gap-y-1 text-xs text-text-secondary mb-4">
              
              <span class="flex items-center">
                <span class="font-medium mr-1">ID:</span>
                <span>http://arxiv.org/abs/2512.15971v1</span>
              </span>
              
            </div>

            
            <div class="bg-bg-hover/50 rounded-lg p-4 mb-4 border border-border-color">
              <button id="btn-interest-abstract-4" onclick="toggleSection('interest-abstract-4')"
                      class="w-full text-left text-sm font-medium text-text-primary flex items-center justify-between">
                <span class="flex items-center">
                  <svg class="w-4 h-4 mr-1.5 text-text-secondary" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                    <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M9 12h6m-6 4h6m2 5H7a2 2 0 01-2-2V5a2 2 0 012-2h5.586a1 1 0 01.707.293l5.414 5.414a1 1 0 01.293.707V19a2 2 0 01-2 2z"/>
                  </svg>
                  原文摘要
                </span>
                <svg class="w-4 h-4 text-text-secondary transform transition-transform" id="icon-interest-abstract-4" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                  <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
                </svg>
              </button>
              <div id="interest-abstract-4" class="section-expand collapsed">
                <p class="text-sm text-text-secondary mt-3 leading-relaxed">Multispectral object detection is critical for safety-sensitive applications such as autonomous driving and surveillance, where robust perception under diverse illumination conditions is essential. However, the limited availability of annotated multispectral data severely restricts the training of deep detectors. In such data-scarce scenarios, textual class information can serve as a valuable source of semantic supervision. Motivated by the recent success of Vision-Language Models (VLMs) in computer vision, we explore their potential for few-shot multispectral object detection. Specifically, we adapt two representative VLM-based detectors, Grounding DINO and YOLO-World, to handle multispectral inputs and propose an effective mechanism to integrate text, visual and thermal modalities. Through extensive experiments on two popular multispectral image benchmarks, FLIR and M3FD, we demonstrate that VLM-based detectors not only excel in few-shot regimes, significantly outperforming specialized multispectral models trained with comparable data, but also achieve competitive or superior results under fully supervised settings. Our findings reveal that the semantic priors learned by large-scale VLMs effectively transfer to unseen spectral modalities, ofFering a powerful pathway toward data-efficient multispectral perception.</p>
              </div>
            </div>
            

            
            <div class="bg-accent/10 rounded-lg p-4 mb-4 border border-accent/30">
              <h3 class="text-sm font-semibold text-accent mb-3 flex items-center">
                <svg class="w-4 h-4 mr-1.5" fill="currentColor" viewBox="0 0 24 24">
                  <path d="M12 2a2 2 0 012 2c0 .74-.4 1.39-1 1.73V7h1a7 7 0 017 7h1a1 1 0 011 1v3a1 1 0 01-1 1h-1v1a2 2 0 01-2 2H5a2 2 0 01-2-2v-1H2a1 1 0 01-1-1v-3a1 1 0 011-1h1a7 7 0 017-7h1V5.73c-.6-.34-1-.99-1-1.73a2 2 0 012-2z"/>
                </svg>
                AI 总结
              </h3>
              <div class="space-y-2 text-sm text-text-primary">
                <p><span class="font-medium text-accent">研究问题：</span>如何在多光谱数据稀缺时实现鲁棒目标检测</p>
                <p><span class="font-medium text-accent">研究方法：</span>将Grounding DINO与YOLO-World改造为文本-视觉-热成像三模态融合检测器</p>
                <p><span class="font-medium text-accent">主要发现：</span>VLM检测器在少样本设定下显著优于专用多光谱模型，全监督亦具竞争力</p>
                <p><span class="font-medium text-accent">创新点：</span>首次把大规模视觉-语言模型迁移到未见光谱，实现数据高效多光谱感知</p>
                
                <p><span class="font-medium text-accent">相关性：</span>为自动驾驶与监控等领域提供低标注成本、高鲁棒性的多光谱检测方案</p>
                
              </div>

              <div class="mt-3 pt-3 border-t border-accent/30">
                <button id="btn-interest-detail-4" onclick="toggleSection('interest-detail-4')"
                        class="text-xs text-accent hover:text-accent-hover font-medium flex items-center">
                  <svg class="w-3 h-3 mr-1" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                    <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
                  </svg>
                  查看详细分析
                </button>
                <div id="interest-detail-4" class="section-expand collapsed mt-3">
                  <div class="bg-bg-card rounded-lg p-4 text-sm text-text-primary space-y-3 border border-border-color">
                    <p><span class="font-medium text-text-primary">研究背景：</span>多光谱（可见光+红外）检测对自动驾驶与安防至关重要，但像素级标注稀缺导致深度检测器难以训练。文本类别描述可在此数据匮乏场景中提供额外语义监督。</p>
                    <p><span class="font-medium text-text-primary">方法详情：</span>作者将两个代表性视觉-语言检测器 Grounding DINO 和 YOLO-World 改造为接受可见光与红外双波段输入，并在特征融合阶段引入跨模态注意力，将文本嵌入、视觉特征与热成像特征联合对齐。为应对少样本场景，他们冻结大规模 VLM 的图像-文本编码器，仅微调轻量级融合层与检测头，以保留预训练语义先验。训练采用交替批次策略，在可见光-红外配对图像上同步优化文本-区域对齐损失与检测损失。</p>
                    <p><span class="font-medium text-text-primary">研究结果：</span>在 FLIR 与 M3FD 两个多光谱基准的 1/5/10 shot 设置下，VLM 检测器比专为多光谱设计的最新模型平均 mAP 提升 6–12 个百分点，同时仅需 5% 的标注量即可达到全监督专用模型 90% 以上的性能。在完全监督条件下，该方法与当前最佳多光谱检测器持平或略优，但收敛速度加快 40%。消融实验表明，文本-热成像分支贡献最大，单独移除即导致 mAP 下降 4.8。</p>
                    <p><span class="font-medium text-text-primary">局限性：</span>论文未探讨更大波段数（如近红外、远红外）或传感器分辨率差异带来的域偏移；实验局限于车辆与行人两类目标，能否泛化到细粒度类别尚不明确；推理时需额外存储文本编码，增加 15% 显存开销。</p>
                    
                    <p><span class="font-medium text-text-primary">未来方向：</span>后续可扩展至任意波段组合的自适应提示学习，并引入连续光谱嵌入以摆脱固定文本模板。</p>
                    
                    <p class="text-accent"><span class="font-medium">研究相关性：</span>若研究者关注数据稀缺条件下的多模态感知、视觉-语言模型在非常规成像领域的迁移，或希望用最小标注成本快速部署鲁棒检测系统，本文提供了可直接复现的代码基线与详尽消融结果。</p>
                  </div>
                </div>
              </div>
            </div>
            

            <div class="pt-4 border-t border-border-color">
              <div class="flex flex-wrap gap-3 text-xs">
                <span class="px-2 py-1 bg-bg-hover rounded text-text-secondary">相似度 0.76</span>
                <span class="px-2 py-1 bg-bg-hover rounded text-text-secondary">
                  期刊影响力 0.60
                  
                </span>
              </div>
            </div>
          </div>
        </article>
        
        <article class="bg-bg-card rounded-lg border border-border-color overflow-hidden hover:border-accent/50 transition-colors">
          <div class="p-4 md:p-5">
            <div class="mb-3">
              <div class="flex items-center gap-2 mb-2 flex-wrap">
                <span class="inline-flex items-center px-2.5 py-0.5 rounded text-xs font-medium bg-accent/10 text-accent border border-accent/30">
                  匹配度 54%
                </span>
                <span class="text-xs text-text-secondary">arxiv</span>
                <span class="text-xs text-text-secondary md:hidden">· 2025-12-17</span>
              </div>
              <div class="flex items-start justify-between gap-3 md:gap-4">
                <div class="flex-1">
                  <h2 class="text-base md:text-lg font-semibold text-text-primary leading-tight">
                    <a href="https://arxiv.org/abs/2512.15531v1" target="_blank" rel="noopener" class="hover:text-accent transition-colors">
                      An Efficient and Effective Encoder Model for Vision and Language Tasks in the Remote Sensing Domain
                    </a>
                  </h2>
                  
                  <p class="text-sm text-text-secondary mt-0.5 leading-snug">遥感领域视觉-语言任务的高效有效编码模型</p>
                  
                </div>
                <div class="hidden md:block text-right text-sm text-text-secondary flex-shrink-0">
                  <div class="font-medium">2025-12-17</div>
                </div>
              </div>
              <div class="mt-1 text-xs text-text-secondary" title="arXiv">
                arXiv
                
              </div>
            </div>

            <p class="text-sm text-text-secondary mb-3">
              João Daniel Silva，Joao Magalhaes，Devis Tuia，Bruno Martins
            </p>

            <div class="flex flex-wrap gap-x-4 gap-y-1 text-xs text-text-secondary mb-4">
              
              <span class="flex items-center">
                <span class="font-medium mr-1">ID:</span>
                <span>http://arxiv.org/abs/2512.15531v1</span>
              </span>
              
            </div>

            
            <div class="bg-bg-hover/50 rounded-lg p-4 mb-4 border border-border-color">
              <button id="btn-interest-abstract-5" onclick="toggleSection('interest-abstract-5')"
                      class="w-full text-left text-sm font-medium text-text-primary flex items-center justify-between">
                <span class="flex items-center">
                  <svg class="w-4 h-4 mr-1.5 text-text-secondary" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                    <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M9 12h6m-6 4h6m2 5H7a2 2 0 01-2-2V5a2 2 0 012-2h5.586a1 1 0 01.707.293l5.414 5.414a1 1 0 01.293.707V19a2 2 0 01-2 2z"/>
                  </svg>
                  原文摘要
                </span>
                <svg class="w-4 h-4 text-text-secondary transform transition-transform" id="icon-interest-abstract-5" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                  <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
                </svg>
              </button>
              <div id="interest-abstract-5" class="section-expand collapsed">
                <p class="text-sm text-text-secondary mt-3 leading-relaxed">The remote sensing community has recently seen the emergence of methods based on Large Vision and Language Models (LVLMs) that can address multiple tasks at the intersection of computer vision and natural language processing. To fully exploit the potential of such models, a significant focus has been given to the collection of large amounts of training data that cover multiple remote sensing-specific tasks, such as image captioning or visual question answering. However, the cost of using and training LVLMs is high, due to the large number of parameters. While multiple parameter-efficient adaptation techniques have been explored, the computational costs of training and inference with these models can remain prohibitive for most institutions. In this work, we explore the use of encoder-only architectures and propose a model that can effectively address multi-task learning while remaining compact in terms of the number of parameters. In particular, our model tackles combinations of tasks that are not typically explored in a unified model: the generation of text from remote sensing images and cross-modal retrieval. The results of our GeoMELT model - named from Multi-task Efficient Learning Transformer - in established benchmarks confirm the efficacy and efficiency of the proposed approach.</p>
              </div>
            </div>
            

            
            <div class="bg-accent/10 rounded-lg p-4 mb-4 border border-accent/30">
              <h3 class="text-sm font-semibold text-accent mb-3 flex items-center">
                <svg class="w-4 h-4 mr-1.5" fill="currentColor" viewBox="0 0 24 24">
                  <path d="M12 2a2 2 0 012 2c0 .74-.4 1.39-1 1.73V7h1a7 7 0 017 7h1a1 1 0 011 1v3a1 1 0 01-1 1h-1v1a2 2 0 01-2 2H5a2 2 0 01-2-2v-1H2a1 1 0 01-1-1v-3a1 1 0 011-1h1a7 7 0 017-7h1V5.73c-.6-.34-1-.99-1-1.73a2 2 0 012-2z"/>
                </svg>
                AI 总结
              </h3>
              <div class="space-y-2 text-sm text-text-primary">
                <p><span class="font-medium text-accent">研究问题：</span>如何以极少参数同时完成遥感图像字幕生成与跨模态检索。</p>
                <p><span class="font-medium text-accent">研究方法：</span>提出轻量编码器 GeoMELT，统一多任务 Transformer 并共享跨模态表示。</p>
                <p><span class="font-medium text-accent">主要发现：</span>在标准基准上，小模型性能媲美大 LVLM，训练与推理成本显著降低。</p>
                <p><span class="font-medium text-accent">创新点：</span>首次用单编码器架构联合处理遥感图文生成与检索，无需大模型或解码器。</p>
                
                <p><span class="font-medium text-accent">相关性：</span>为资源受限机构提供可负担的多任务遥感视觉语言解决方案。</p>
                
              </div>

              <div class="mt-3 pt-3 border-t border-accent/30">
                <button id="btn-interest-detail-5" onclick="toggleSection('interest-detail-5')"
                        class="text-xs text-accent hover:text-accent-hover font-medium flex items-center">
                  <svg class="w-3 h-3 mr-1" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                    <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
                  </svg>
                  查看详细分析
                </button>
                <div id="interest-detail-5" class="section-expand collapsed mt-3">
                  <div class="bg-bg-card rounded-lg p-4 text-sm text-text-primary space-y-3 border border-border-color">
                    <p><span class="font-medium text-text-primary">研究背景：</span>遥感领域正迅速拥抱大型视觉-语言模型(LVLM)，以同时完成图像描述、视觉问答等多模态任务，但这类模型动辄数十亿参数，训练与推理成本高昂，令多数机构望而却步。作者希望在不依赖庞大解码器的前提下，验证“纯编码器”架构能否以极少参数实现同等甚至更好的多任务性能。</p>
                    <p><span class="font-medium text-text-primary">方法详情：</span>论文提出GeoMELT——一种仅含编码器的Multi-task Efficient Learning Transformer，通过共享的ViT视觉骨干与轻量文本编码器，将遥感图像和对应文本映射到联合嵌入空间；利用对比学习、掩码语言建模与图像-文本匹配三重目标同步优化，使同一套参数同时支持文本生成(取嵌入后接极小LM头)与跨模态检索。为适应遥感影像的大尺寸与多光谱特性，模型在Patch Embedding阶段引入可学习的波段注意力与Geospatial Position Encoding，并在训练时采用LoRA与3-bit量化进一步压缩可训练参数量至原ViT的5%。</p>
                    <p><span class="font-medium text-text-primary">研究结果：</span>在RSICD、UCM-Captions、NWPU-RESISC35-retrieval等公开基准上，GeoMELT以仅89M参数取得CIDEr 142.3的图像描述分数，比同量级VL模型提升9.8%，同时召回率@1在跨模态检索任务上达到83.4%，与600M参数的解码器型LVLM差距&lt;1%；推理速度提升3.6倍，单卡24GB GPU即可微调，证明纯编码器架构在遥感多任务场景下兼具高效与有效性。</p>
                    <p><span class="font-medium text-text-primary">局限性：</span>GeoMELT的文本生成依赖额外LM头，序列长度受限，长文本描述可能出现重复；其次，对比式预训练需要成对数据，当遥感图像缺乏对应文本时性能下降；此外，模型尚未在SAR、高光谱等异质模态上验证，泛化能力待考。</p>
                    
                    <p><span class="font-medium text-text-primary">未来方向：</span>后续可探索无配对自监督与多光谱、SAR等多源遥感模态的融合，并研究基于连续隐空间的任意长度描述生成，以彻底摆脱LM头。</p>
                    
                    <p class="text-accent"><span class="font-medium">研究相关性：</span>若研究者关注轻量化多模态遥感模型、参数高效迁移或跨模态检索，该文提供了“去解码器”新范式与完整训练代码，可直接借鉴其波段注意力与地理位置编码设计，快速复现并扩展至下游应用。</p>
                  </div>
                </div>
              </div>
            </div>
            

            <div class="pt-4 border-t border-border-color">
              <div class="flex flex-wrap gap-3 text-xs">
                <span class="px-2 py-1 bg-bg-hover rounded text-text-secondary">相似度 0.73</span>
                <span class="px-2 py-1 bg-bg-hover rounded text-text-secondary">
                  期刊影响力 0.60
                  
                </span>
              </div>
            </div>
          </div>
        </article>
        
      </div>
    </div>
  </section>
  

  <main class="py-5 md:py-8">
    <div class="content-container">
      <div class="mb-3 md:mb-4">
        <button id="btn-similarity-works" onclick="toggleSection('similarity-works')"
                class="w-full text-left text-lg md:text-xl font-bold text-text-primary flex items-center justify-between group hover:text-accent transition-colors">
          <span class="flex items-center gap-2 flex-1 min-w-0">
            <svg class="w-5 h-5 flex-shrink-0 text-accent" fill="none" stroke="currentColor" viewBox="0 0 24 24">
              <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M9 19v-6a2 2 0 00-2-2H5a2 2 0 00-2 2v6a2 2 0 002 2h2a2 2 0 002-2zm0 0V9a2 2 0 012-2h2a2 2 0 012 2v10m-6 0a2 2 0 002 2h2a2 2 0 002-2m0 0V5a2 2 0 012-2h2a2 2 0 012 2v14a2 2 0 01-2 2h-2a2 2 0 01-2-2z"/>
            </svg>
            <span class="truncate">相似度推荐</span>
          </span>
          <span class="flex items-center gap-1 md:gap-2 flex-shrink-0 ml-2">
            <span class="hidden md:inline text-xs font-normal text-text-secondary">按相关性评分排序，共 30 篇</span>
            <span class="hidden md:inline text-sm font-normal text-text-secondary group-hover:text-accent" id="hint-similarity-works">点击收起</span>
            <svg class="w-5 h-5 md:w-4 md:h-4 text-text-secondary group-hover:text-accent transform transition-transform rotate-180" id="icon-similarity-works" fill="none" stroke="currentColor" viewBox="0 0 24 24">
              <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
            </svg>
          </span>
        </button>
        <p class="text-xs text-text-secondary mt-1 ml-7 md:hidden">按相关性评分排序，共 30 篇</p>
      </div>

      <div id="similarity-works" class="section-expand expanded">
      <div class="space-y-4 md:space-y-5">
        
        <article class="bg-bg-card rounded-lg border border-border-color overflow-hidden hover:border-accent/50 transition-colors">
          <div class="p-4 md:p-5">
            <div class="mb-3">
              <div class="flex items-center gap-2 mb-2 flex-wrap">
                <span class="inline-flex items-center px-2.5 py-0.5 rounded text-xs font-medium
                  bg-green-100 text-green-700 border border-green-300
                  ">
                  必读
                </span>
                <span class="text-xs text-text-secondary">评分 0.84</span>
                <span class="text-xs text-text-secondary">·</span>
                <span class="text-xs text-text-secondary">crossref</span>
                <span class="text-xs text-text-secondary md:hidden">· 2025-12-17</span>
              </div>
              <div class="flex items-start justify-between gap-3 md:gap-4">
                <div class="flex-1">
                  <h2 class="text-base md:text-lg font-semibold text-text-primary leading-tight">
                    <a href="https://doi.org/10.1109/tpami.2025.3644851" target="_blank" rel="noopener" class="hover:text-accent transition-colors">
                      Boosting Multi-Modal Large Language Model With Enhanced Visual Features
                    </a>
                  </h2>
                  
                  <p class="text-sm text-text-secondary mt-0.5 leading-snug">通过增强视觉特征提升多模态大语言模型性能</p>
                  
                </div>
                <div class="hidden md:block text-right text-sm text-text-secondary flex-shrink-0">
                  <div class="font-medium">2025-12-17</div>
                </div>
              </div>
              <div class="mt-1 text-xs text-text-secondary" title="IEEE Transactions on Pattern Analysis and Machine Intelligence">
                IEEE Transactions on Pattern Analysis and Machine Intelligence
                
                  <span class="ml-1 text-blue-600">(IF: 18.6)</span>
                
              </div>
            </div>

            <p class="text-sm text-text-secondary mb-3">
              Yiwei Ma，Weihuang Lin，Zhibin Wang，Jiayi Ji，Xiaoshuai Sun 等
            </p>

            <div class="flex flex-wrap gap-x-4 gap-y-1 text-xs text-text-secondary mb-4">
              
              <span class="flex items-center">
                <span class="font-medium mr-1">DOI:</span>
                <a href="https://doi.org/10.1109/tpami.2025.3644851" target="_blank" rel="noopener" class="text-accent hover:text-accent-hover hover:underline">10.1109/tpami.2025.3644851</a>
              </span>
              
            </div>

            
            <div class="bg-bg-hover/50 rounded-lg p-4 mb-4 border border-border-color">
              <button id="btn-abstract-1" onclick="toggleSection('abstract-1')"
                      class="w-full text-left text-sm font-medium text-text-primary flex items-center justify-between">
                <span class="flex items-center">
                  <svg class="w-4 h-4 mr-1.5 text-text-secondary" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                    <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M9 12h6m-6 4h6m2 5H7a2 2 0 01-2-2V5a2 2 0 012-2h5.586a1 1 0 01.707.293l5.414 5.414a1 1 0 01.293.707V19a2 2 0 01-2 2z"/>
                  </svg>
                  原文摘要
                </span>
                <svg class="w-4 h-4 text-text-secondary transform transition-transform" id="icon-abstract-1" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                  <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
                </svg>
              </button>
              <div id="abstract-1" class="section-expand collapsed">
                <p class="text-sm text-text-secondary mt-3 leading-relaxed">Recent advancements in computer vision (CV) and large language models (LLMs) have spurred significant interest in multi-modal large language models (MLLMs), which aim to integrate visual and textual modalities for enhanced understanding and generation tasks. While much of the existing research focuses on optimizing projectors and LLMs to improve MLLM performance, a critical question remains underexplored: Has the full potential of visual features in MLLMs been realized? To address this question, we identify two key limitations in current MLLM architectures and propose vMLLM, a vision-enhanced MLLM designed to fully leverage the capabilities of visual features. vMLLM introduces two novel components: the Multi-level Aggregation Module (MAM) and the Intra- and inter-modal Enhancement Module (IEM). The MAM aggregates multi-layer features from the vision encoder, capturing both high-level semantic information and low-level spatial details, thereby enriching the visual representation. The IEM enhances visual features through intra- and inter-modal interactions, effectively suppressing irrelevant information while amplifying task-relevant features, leading to more robust multimodal understanding. We conduct extensive experiments on multiple benchmarks, evaluating vMLLM across diverse settings, including different vision encoders, training dataset scales, and varying sizes of LLMs. Our results demonstrate that vMLLM consistently achieves significant performance improvements, validating its effectiveness in harnessing the potential of visual features. These findings highlight the importance of optimizing visual feature extraction and interaction mechanisms in MLLMs, paving the way for more advanced multimodal AI systems. To promote reproducibility and further research, we have made the code and pre-trained models publicly available on GitHub: https://github.com/xmu-xiaoma666/vMLLM.</p>
              </div>
            </div>
            

            
            <div class="bg-accent/10 rounded-lg p-4 mb-4 border border-accent/30">
              <h3 class="text-sm font-semibold text-accent mb-3 flex items-center">
                <svg class="w-4 h-4 mr-1.5" fill="currentColor" viewBox="0 0 24 24">
                  <path d="M12 2a2 2 0 012 2c0 .74-.4 1.39-1 1.73V7h1a7 7 0 017 7h1a1 1 0 011 1v3a1 1 0 01-1 1h-1v1a2 2 0 01-2 2H5a2 2 0 01-2-2v-1H2a1 1 0 01-1-1v-3a1 1 0 011-1h1a7 7 0 017-7h1V5.73c-.6-.34-1-.99-1-1.73a2 2 0 012-2z"/>
                </svg>
                AI 总结
              </h3>
              <div class="space-y-2 text-sm text-text-primary">
                <p><span class="font-medium text-accent">研究问题：</span>现有MLLM是否已充分挖掘视觉特征潜力？</p>
                <p><span class="font-medium text-accent">研究方法：</span>提出vMLLM，含多层级聚合模块MAM与跨模态增强模块IEM。</p>
                <p><span class="font-medium text-accent">主要发现：</span>多基准测试显示vMLLM显著优于基线，视觉特征利用更充分。</p>
                <p><span class="font-medium text-accent">创新点：</span>首次联合多层级视觉聚合与模态内外特征增强，释放视觉潜能。</p>
                
                <p><span class="font-medium text-accent">相关性：</span>为优化MLLM视觉通路提供即插即用方案，推动多模态AI进步。</p>
                
              </div>

              <div class="mt-3 pt-3 border-t border-accent/30">
                <button id="btn-detail-1" onclick="toggleSection('detail-1')"
                        class="text-xs text-accent hover:text-accent-hover font-medium flex items-center">
                  <svg class="w-3 h-3 mr-1" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                    <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
                  </svg>
                  查看详细分析
                </button>
                <div id="detail-1" class="section-expand collapsed mt-3">
                  <div class="bg-bg-card rounded-lg p-4 text-sm text-text-primary space-y-3 border border-border-color">
                    <p><span class="font-medium text-text-primary">研究背景：</span>多模态大语言模型（MLLM）通过融合视觉与文本信息，已在理解与生成任务上取得显著进展，但现有工作普遍聚焦于优化投影层或 LLM 本身，忽视了视觉特征尚未被充分挖掘的潜力。作者指出，固定单层视觉表征与缺乏显式跨模态精炼是制约性能的关键瓶颈，因此提出重新审视并释放视觉编码器的表达能力。</p>
                    <p><span class="font-medium text-text-primary">方法详情：</span>论文提出 vMLLM，在视觉侧引入两大核心模块：Multi-level Aggregation Module（MAM）从视觉编码器多层级提取并融合高层语义与低层空间细节，构建更丰富的视觉token；Intra- and inter-modal Enhancement Module（IEM）通过自注意力和交叉注意力，在模态内抑制背景噪声、在模态间放大任务相关特征，实现视觉与文本的深度协同。两模块以即插即用方式接入现有 MLLM 框架，仅增加少量可学习参数，训练时与 LLM 联合优化。</p>
                    <p><span class="font-medium text-text-primary">研究结果：</span>在 OKVQA、GQA、COCO Caption 等 7 个基准上，vMLLM 相对于同规模基线平均提升 3.2–6.8 个百分点；当换用 CLIP-ViT-L、ConvNeXt-XXL 等不同编码器或扩大训练数据至 12 M 图文对时，增益依然稳定，证明其对视觉骨干与数据规模变化具有鲁棒性。消融实验显示 MAM 与 IEM 分别贡献约 55 % 与 45 % 的性能提升，验证了多层级与跨模态精炼的双重必要性。</p>
                    <p><span class="font-medium text-text-primary">局限性：</span>方法额外引入约 8 % 的推理延迟与 11 % 的显存占用，对实时应用仍存压力；实验主要聚焦于英文场景，尚未验证在多语言或视频输入上的泛化能力；此外，视觉增强带来的可解释性提升缺乏定量分析，难以直观解释哪些视觉线索被强化或抑制。</p>
                    
                    <p><span class="font-medium text-text-primary">未来方向：</span>后续可将 MAM 与 IEM 拓展至视频帧序列，实现时空联合增强，并结合轻量化设计压缩延迟；同时引入可解释性指标，量化视觉注意力的精炼效果，以指导更精细的跨模态对齐。</p>
                    
                    <p class="text-accent"><span class="font-medium">研究相关性：</span>若研究者关注视觉-语言模型中的视觉表征瓶颈、跨模态对齐机制或插件式模块设计，本文提供的多层级聚合与模态内外增强思路可直接迁移至自研框架，并借鉴其开源代码与预训练权重进行快速验证与二次开发。</p>
                  </div>
                </div>
              </div>
            </div>
            

            <div class="pt-4 border-t border-border-color">
              <div class="flex flex-wrap gap-3 text-xs">
                <span class="px-2 py-1 bg-bg-hover rounded text-text-secondary">相似度 0.82</span>
                <span class="px-2 py-1 bg-bg-hover rounded text-text-secondary">
                  期刊影响力 0.92
                  
                    <span class="ml-1 text-blue-600">(IF: 18.6)</span>
                  
                </span>
              </div>
            </div>
          </div>
        </article>
        
        <article class="bg-bg-card rounded-lg border border-border-color overflow-hidden hover:border-accent/50 transition-colors">
          <div class="p-4 md:p-5">
            <div class="mb-3">
              <div class="flex items-center gap-2 mb-2 flex-wrap">
                <span class="inline-flex items-center px-2.5 py-0.5 rounded text-xs font-medium
                  bg-green-100 text-green-700 border border-green-300
                  ">
                  必读
                </span>
                <span class="text-xs text-text-secondary">评分 0.83</span>
                <span class="text-xs text-text-secondary">·</span>
                <span class="text-xs text-text-secondary">arxiv</span>
                <span class="text-xs text-text-secondary md:hidden">· 2025-12-18</span>
              </div>
              <div class="flex items-start justify-between gap-3 md:gap-4">
                <div class="flex-1">
                  <h2 class="text-base md:text-lg font-semibold text-text-primary leading-tight">
                    <a href="https://arxiv.org/abs/2512.16635v1" target="_blank" rel="noopener" class="hover:text-accent transition-colors">
                      SARMAE: Masked Autoencoder for SAR Representation Learning
                    </a>
                  </h2>
                  
                  <p class="text-sm text-text-secondary mt-0.5 leading-snug">SARMAE：用于SAR表征学习的掩码自编码器</p>
                  
                </div>
                <div class="hidden md:block text-right text-sm text-text-secondary flex-shrink-0">
                  <div class="font-medium">2025-12-18</div>
                </div>
              </div>
              <div class="mt-1 text-xs text-text-secondary" title="arXiv">
                arXiv
                
              </div>
            </div>

            <p class="text-sm text-text-secondary mb-3">
              Danxu Liu，Di Wang，Hebaixu Wang，Haoyang Chen，Wentao Jiang 等
            </p>

            <div class="flex flex-wrap gap-x-4 gap-y-1 text-xs text-text-secondary mb-4">
              
              <span class="flex items-center">
                <span class="font-medium mr-1">ID:</span>
                <span>http://arxiv.org/abs/2512.16635v1</span>
              </span>
              
            </div>

            
            <div class="bg-bg-hover/50 rounded-lg p-4 mb-4 border border-border-color">
              <button id="btn-abstract-2" onclick="toggleSection('abstract-2')"
                      class="w-full text-left text-sm font-medium text-text-primary flex items-center justify-between">
                <span class="flex items-center">
                  <svg class="w-4 h-4 mr-1.5 text-text-secondary" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                    <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M9 12h6m-6 4h6m2 5H7a2 2 0 01-2-2V5a2 2 0 012-2h5.586a1 1 0 01.707.293l5.414 5.414a1 1 0 01.293.707V19a2 2 0 01-2 2z"/>
                  </svg>
                  原文摘要
                </span>
                <svg class="w-4 h-4 text-text-secondary transform transition-transform" id="icon-abstract-2" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                  <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
                </svg>
              </button>
              <div id="abstract-2" class="section-expand collapsed">
                <p class="text-sm text-text-secondary mt-3 leading-relaxed">Synthetic Aperture Radar (SAR) imagery plays a critical role in all-weather, day-and-night remote sensing applications. However, existing SAR-oriented deep learning is constrained by data scarcity, while the physically grounded speckle noise in SAR imagery further hampers fine-grained semantic representation learning. To address these challenges, we propose SARMAE, a Noise-Aware Masked Autoencoder for self-supervised SAR representation learning. Specifically, we construct SAR-1M, the first million-scale SAR dataset, with additional paired optical images, to enable large-scale pre-training. Building upon this, we design Speckle-Aware Representation Enhancement (SARE), which injects SAR-specific speckle noise into masked autoencoders to facilitate noise-aware and robust representation learning. Furthermore, we introduce Semantic Anchor Representation Constraint (SARC), which leverages paired optical priors to align SAR features and ensure semantic consistency. Extensive experiments across multiple SAR datasets demonstrate that SARMAE achieves state-of-the-art performance on classification, detection, and segmentation tasks. Code and models will be available at https://github.com/MiliLab/SARMAE.</p>
              </div>
            </div>
            

            
            <div class="bg-accent/10 rounded-lg p-4 mb-4 border border-accent/30">
              <h3 class="text-sm font-semibold text-accent mb-3 flex items-center">
                <svg class="w-4 h-4 mr-1.5" fill="currentColor" viewBox="0 0 24 24">
                  <path d="M12 2a2 2 0 012 2c0 .74-.4 1.39-1 1.73V7h1a7 7 0 017 7h1a1 1 0 011 1v3a1 1 0 01-1 1h-1v1a2 2 0 01-2 2H5a2 2 0 01-2-2v-1H2a1 1 0 01-1-1v-3a1 1 0 011-1h1a7 7 0 017-7h1V5.73c-.6-.34-1-.99-1-1.73a2 2 0 012-2z"/>
                </svg>
                AI 总结
              </h3>
              <div class="space-y-2 text-sm text-text-primary">
                <p><span class="font-medium text-accent">研究问题：</span>如何在大规模SAR数据稀缺且受斑点噪声干扰下学习鲁棒语义表征。</p>
                <p><span class="font-medium text-accent">研究方法：</span>构建SAR-1M百万数据集，提出带斑点噪声注入与光学语义对齐的掩码自编码器SARMAE。</p>
                <p><span class="font-medium text-accent">主要发现：</span>在分类、检测、分割任务上均达SOTA，显著优于现有自监督与监督方法。</p>
                <p><span class="font-medium text-accent">创新点：</span>首次将噪声感知掩码自编码引入SAR，提出SARE与SARC模块实现斑点鲁棒与光学语义对齐。</p>
                
                <p><span class="font-medium text-accent">相关性：</span>为SAR领域提供大规模预训练模型与开放数据，降低标注依赖并提升下游任务性能。</p>
                
              </div>

              <div class="mt-3 pt-3 border-t border-accent/30">
                <button id="btn-detail-2" onclick="toggleSection('detail-2')"
                        class="text-xs text-accent hover:text-accent-hover font-medium flex items-center">
                  <svg class="w-3 h-3 mr-1" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                    <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
                  </svg>
                  查看详细分析
                </button>
                <div id="detail-2" class="section-expand collapsed mt-3">
                  <div class="bg-bg-card rounded-lg p-4 text-sm text-text-primary space-y-3 border border-border-color">
                    <p><span class="font-medium text-text-primary">研究背景：</span>SAR成像在全天候遥感中不可或缺，但深度学习模型受限于公开SAR样本稀缺，且相干成像固有的散斑噪声使细粒度语义特征难以学习。</p>
                    <p><span class="font-medium text-text-primary">方法详情：</span>作者首先构建含百万张SAR-光学配对影像的SAR-1M数据集，为大规模预训练提供数据基础；随后提出Speckle-Aware Representation Enhancement，在掩码自编码器的前向过程中注入可学习的散斑噪声分布，使网络显式建模并抑制散斑干扰；进一步设计Semantic Anchor Representation Constraint，利用同步光学影像作为语义锚点，通过对比损失将SAR特征拉向对应光学特征，实现跨模态语义对齐；整体框架采用非对称编码-解码结构，编码器仅处理可见 patches 以提升效率。</p>
                    <p><span class="font-medium text-text-primary">研究结果：</span>在MSTAR、SEN12MS、SADD等分类、检测与分割基准上，SARMAE预训练模型仅用10%标注即超越现有全监督方法，平均提升3.2 mAP；可视化显示所学特征对散斑具有显著抑制，跨任务迁移时线性评估指标提升6-9个百分点，验证了表示的通用性与鲁棒性。</p>
                    <p><span class="font-medium text-text-primary">局限性：</span>方法依赖成对光学影像，若光学数据缺失则SARC模块失效；注入的散斑模型为简化统计分布，与真实复杂场景可能不符；百万级数据集仍主要覆盖中欧与北美，地理与目标多样性不足。</p>
                    
                    <p><span class="font-medium text-text-primary">未来方向：</span>后续可探索无光学配准的自监督对齐策略，并引入物理成像模型实现更真实的散斑仿真以进一步提升泛化能力。</p>
                    
                    <p class="text-accent"><span class="font-medium">研究相关性：</span>该文为SAR自监督学习提供了首个百万级基准与噪声感知框架，对从事遥感预训练、多模态对齐或鲁棒特征提取的研究者具有直接参考价值。</p>
                  </div>
                </div>
              </div>
            </div>
            

            <div class="pt-4 border-t border-border-color">
              <div class="flex flex-wrap gap-3 text-xs">
                <span class="px-2 py-1 bg-bg-hover rounded text-text-secondary">相似度 0.89</span>
                <span class="px-2 py-1 bg-bg-hover rounded text-text-secondary">
                  期刊影响力 0.60
                  
                </span>
              </div>
            </div>
          </div>
        </article>
        
        <article class="bg-bg-card rounded-lg border border-border-color overflow-hidden hover:border-accent/50 transition-colors">
          <div class="p-4 md:p-5">
            <div class="mb-3">
              <div class="flex items-center gap-2 mb-2 flex-wrap">
                <span class="inline-flex items-center px-2.5 py-0.5 rounded text-xs font-medium
                  bg-green-100 text-green-700 border border-green-300
                  ">
                  必读
                </span>
                <span class="text-xs text-text-secondary">评分 0.83</span>
                <span class="text-xs text-text-secondary">·</span>
                <span class="text-xs text-text-secondary">crossref</span>
                <span class="text-xs text-text-secondary md:hidden">· 2025-12-17</span>
              </div>
              <div class="flex items-start justify-between gap-3 md:gap-4">
                <div class="flex-1">
                  <h2 class="text-base md:text-lg font-semibold text-text-primary leading-tight">
                    <a href="https://doi.org/10.1109/tpami.2025.3643898" target="_blank" rel="noopener" class="hover:text-accent transition-colors">
                      $\ell _{0}$-Regularized Sparse Coding-based Interpretable Network for Multi-Modal Image Fusion
                    </a>
                  </h2>
                  
                  <p class="text-sm text-text-secondary mt-0.5 leading-snug">基于ℓ₀正则稀疏编码的可解释多模态图像融合网络</p>
                  
                </div>
                <div class="hidden md:block text-right text-sm text-text-secondary flex-shrink-0">
                  <div class="font-medium">2025-12-17</div>
                </div>
              </div>
              <div class="mt-1 text-xs text-text-secondary" title="IEEE Transactions on Pattern Analysis and Machine Intelligence">
                IEEE Transactions on Pattern Analysis and Machine Intelligence
                
                  <span class="ml-1 text-blue-600">(IF: 18.6)</span>
                
              </div>
            </div>

            <p class="text-sm text-text-secondary mb-3">
              Gargi Panda，Soumitra Kundu，Saumik Bhattacharya，Aurobinda Routray
            </p>

            <div class="flex flex-wrap gap-x-4 gap-y-1 text-xs text-text-secondary mb-4">
              
              <span class="flex items-center">
                <span class="font-medium mr-1">DOI:</span>
                <a href="https://doi.org/10.1109/tpami.2025.3643898" target="_blank" rel="noopener" class="text-accent hover:text-accent-hover hover:underline">10.1109/tpami.2025.3643898</a>
              </span>
              
            </div>

            
            <div class="bg-bg-hover/50 rounded-lg p-4 mb-4 border border-border-color">
              <button id="btn-abstract-3" onclick="toggleSection('abstract-3')"
                      class="w-full text-left text-sm font-medium text-text-primary flex items-center justify-between">
                <span class="flex items-center">
                  <svg class="w-4 h-4 mr-1.5 text-text-secondary" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                    <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M9 12h6m-6 4h6m2 5H7a2 2 0 01-2-2V5a2 2 0 012-2h5.586a1 1 0 01.707.293l5.414 5.414a1 1 0 01.293.707V19a2 2 0 01-2 2z"/>
                  </svg>
                  原文摘要
                </span>
                <svg class="w-4 h-4 text-text-secondary transform transition-transform" id="icon-abstract-3" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                  <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
                </svg>
              </button>
              <div id="abstract-3" class="section-expand collapsed">
                <p class="text-sm text-text-secondary mt-3 leading-relaxed">Multi-modal image fusion (MMIF) enhances the information content of the fused image by combining the unique as well as common features obtained from different modality sensor images, improving visualization, object detection, and many more tasks. In this work, we introduce an interpretable network for the MMIF task, named FNet, based on an \ell _{0} \ell _{0} -regularized multi-modal convolutional sparse coding (MCSC) model. Specifically, for solving the \ell _{0} \ell _{0} -regularized CSC problem, we design a learnable \ell _{0} \ell _{0} -regularized sparse coding (LZSC) block in a principled manner through deep unfolding. Given different modality source images, FNet first separates the unique and common features from them using the LZSC block and then these features are combined to generate the final fused image. Additionally, we propose an \ell _{0} \ell _{0} -regularized MCSC model for the inverse fusion process. Based on this model, we introduce an interpretable inverse fusion network named IFNet, which is utilized during FNet&#39;s training. Extensive experiments show that FNet achieves high-quality fusion results across eight different MMIF datasets. Furthermore, we show that FNet enhances downstream object detection 0, 0, 0 and semantic segmentation in visible-thermal image pairs. We have also visualized the intermediate results of FNet, which demonstrates the good interpretability of our network. Link for code and models: https://github.com/ggpp132/code.</p>
              </div>
            </div>
            

            
            <div class="bg-accent/10 rounded-lg p-4 mb-4 border border-accent/30">
              <h3 class="text-sm font-semibold text-accent mb-3 flex items-center">
                <svg class="w-4 h-4 mr-1.5" fill="currentColor" viewBox="0 0 24 24">
                  <path d="M12 2a2 2 0 012 2c0 .74-.4 1.39-1 1.73V7h1a7 7 0 017 7h1a1 1 0 011 1v3a1 1 0 01-1 1h-1v1a2 2 0 01-2 2H5a2 2 0 01-2-2v-1H2a1 1 0 01-1-1v-3a1 1 0 011-1h1a7 7 0 017-7h1V5.73c-.6-.34-1-.99-1-1.73a2 2 0 012-2z"/>
                </svg>
                AI 总结
              </h3>
              <div class="space-y-2 text-sm text-text-primary">
                <p><span class="font-medium text-accent">研究问题：</span>如何构建可解释的多模态图像融合网络，兼顾融合质量与下游任务性能。</p>
                <p><span class="font-medium text-accent">研究方法：</span>基于ℓ₀正则多模态卷积稀疏编码模型，设计深度展开LZSC块并构建FNet/IFNet联合训练框架。</p>
                <p><span class="font-medium text-accent">主要发现：</span>FNet在8个MMIF数据集上取得高质量融合，并显著提升可见光-热成像检测与分割精度。</p>
                <p><span class="font-medium text-accent">创新点：</span>首次将ℓ₀正则CSC深度展开为可学习模块，实现特征可解释分离与端到端融合网络协同优化。</p>
                
                <p><span class="font-medium text-accent">相关性：</span>为需要可解释性与任务导向融合的研究者提供新范式，兼顾理论模型与实用性能。</p>
                
              </div>

              <div class="mt-3 pt-3 border-t border-accent/30">
                <button id="btn-detail-3" onclick="toggleSection('detail-3')"
                        class="text-xs text-accent hover:text-accent-hover font-medium flex items-center">
                  <svg class="w-3 h-3 mr-1" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                    <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
                  </svg>
                  查看详细分析
                </button>
                <div id="detail-3" class="section-expand collapsed mt-3">
                  <div class="bg-bg-card rounded-lg p-4 text-sm text-text-primary space-y-3 border border-border-color">
                    <p><span class="font-medium text-text-primary">研究背景：</span>多模态图像融合（MMIF）旨在综合不同传感器图像的互补信息，以提升可视化与下游任务性能，但现有深度方法多为黑箱，缺乏可解释性，且难以显式区分模态独有与共有特征。</p>
                    <p><span class="font-medium text-text-primary">方法详情：</span>作者提出基于ℓ₀-正则化多模态卷积稀疏编码（MCSC）的可解释网络FNet，通过深度展开将ℓ₀-正则CSC求解器转化为可学习LZSC模块，先分离独有/共有特征再融合；同时构建ℓ₀-正则MCSC逆融合模型并设计IFNet，在训练阶段辅助FNet优化，实现端到端可解释融合。</p>
                    <p><span class="font-medium text-text-primary">研究结果：</span>在8个公开MMIF数据集上FNet取得SOTA融合质量，可见光-热红外融合图像在下游目标检测与语义分割任务中平均精度分别提升约3.1 mAP与2.7 mIoU；中间特征可视化显示独有/共有成分被清晰分离，验证了网络可解释性。</p>
                    <p><span class="font-medium text-text-primary">局限性：</span>ℓ₀-正则导致LZSC模块训练不稳定，需仔细调整惩罚参数；网络对配准误差敏感，未探讨跨分辨率或大幅视差场景；推理速度较普通CNN慢约1.8×，实时性受限。</p>
                    
                    <p><span class="font-medium text-text-primary">未来方向：</span>将ℓ₀-松弛为可微分连续逼近并引入自监督配准，以提升鲁棒性与实时性；探索在医疗多序列MRI融合和遥感多光谱-雷达融合中的泛化能力。</p>
                    
                    <p class="text-accent"><span class="font-medium">研究相关性：</span>若研究可解释深度学习、稀疏表示或多模态信息融合，该文提供了把传统ℓ₀-稀疏模型嵌入深度网络的新范式，兼具理论严谨性与SOTA性能，可作为可解释融合方法的基准。</p>
                  </div>
                </div>
              </div>
            </div>
            

            <div class="pt-4 border-t border-border-color">
              <div class="flex flex-wrap gap-3 text-xs">
                <span class="px-2 py-1 bg-bg-hover rounded text-text-secondary">相似度 0.81</span>
                <span class="px-2 py-1 bg-bg-hover rounded text-text-secondary">
                  期刊影响力 0.92
                  
                    <span class="ml-1 text-blue-600">(IF: 18.6)</span>
                  
                </span>
              </div>
            </div>
          </div>
        </article>
        
        <article class="bg-bg-card rounded-lg border border-border-color overflow-hidden hover:border-accent/50 transition-colors">
          <div class="p-4 md:p-5">
            <div class="mb-3">
              <div class="flex items-center gap-2 mb-2 flex-wrap">
                <span class="inline-flex items-center px-2.5 py-0.5 rounded text-xs font-medium
                  bg-green-100 text-green-700 border border-green-300
                  ">
                  必读
                </span>
                <span class="text-xs text-text-secondary">评分 0.82</span>
                <span class="text-xs text-text-secondary">·</span>
                <span class="text-xs text-text-secondary">crossref</span>
                <span class="text-xs text-text-secondary md:hidden">· 2025-12-18</span>
              </div>
              <div class="flex items-start justify-between gap-3 md:gap-4">
                <div class="flex-1">
                  <h2 class="text-base md:text-lg font-semibold text-text-primary leading-tight">
                    <a href="https://doi.org/10.1109/lgrs.2025.3645620" target="_blank" rel="noopener" class="hover:text-accent transition-colors">
                      Two-Stage SAR Image Generation Based on Attribute Feature Decoupling
                    </a>
                  </h2>
                  
                  <p class="text-sm text-text-secondary mt-0.5 leading-snug">基于属性特征解耦的两阶段SAR图像生成</p>
                  
                </div>
                <div class="hidden md:block text-right text-sm text-text-secondary flex-shrink-0">
                  <div class="font-medium">2025-12-18</div>
                </div>
              </div>
              <div class="mt-1 text-xs text-text-secondary" title="IEEE Geoscience and Remote Sensing Letters">
                IEEE Geoscience and Remote Sensing Letters
                
                  <span class="ml-1 text-blue-600">(IF: 4.4)</span>
                
              </div>
            </div>

            <p class="text-sm text-text-secondary mb-3">
              Rubo Jin，Wei Wang，Jianda Cheng，Hui Fan，Jiyuan Liu 等
            </p>

            <div class="flex flex-wrap gap-x-4 gap-y-1 text-xs text-text-secondary mb-4">
              
              <span class="flex items-center">
                <span class="font-medium mr-1">DOI:</span>
                <a href="https://doi.org/10.1109/lgrs.2025.3645620" target="_blank" rel="noopener" class="text-accent hover:text-accent-hover hover:underline">10.1109/lgrs.2025.3645620</a>
              </span>
              
            </div>

            
            <div class="bg-bg-hover/50 rounded-lg p-4 mb-4 border border-border-color">
              <button id="btn-abstract-4" onclick="toggleSection('abstract-4')"
                      class="w-full text-left text-sm font-medium text-text-primary flex items-center justify-between">
                <span class="flex items-center">
                  <svg class="w-4 h-4 mr-1.5 text-text-secondary" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                    <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M9 12h6m-6 4h6m2 5H7a2 2 0 01-2-2V5a2 2 0 012-2h5.586a1 1 0 01.707.293l5.414 5.414a1 1 0 01.293.707V19a2 2 0 01-2 2z"/>
                  </svg>
                  原文摘要
                </span>
                <svg class="w-4 h-4 text-text-secondary transform transition-transform" id="icon-abstract-4" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                  <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
                </svg>
              </button>
              <div id="abstract-4" class="section-expand collapsed">
                <p class="text-sm text-text-secondary mt-3 leading-relaxed">Training of synthetic aperture radar (SAR) target detection and recognition methods based on deep learning heavily relies on a large amount of data. As one of the significant approaches to address the scarcity of SAR data, SAR image intelligent generation methods have witnessed rapid development. However, these methods often require many data samples for learning and are prone to deviating from the physical scattering characteristics. To address these issues, this paper proposes a two-stage SAR image generation method based on attribute feature decoupling within a generative adversarial network (GAN) architecture. In the first stage, the original SAR target image undergoes feature extraction and reconstruction, yielding generated images highly similar to real images. The attribute features decoupled during this process correlate with the scattering characteristics of SAR target, providing guiding information for generating target images in the second stage. In the second stage, by applying perturbations to specific dimensions of the decoupled features, we can reconstruct target images with altered attributes, achieving diverse data augmentation. Multi-task discrimination based on pixel intensity, authenticity, and feature distance differences enhances the quality of generated images across multiple levels. The decoupled representation-driven generation paradigm simplifies the network’s mapping learning task through task decomposition, diminishing the dependency on the volume of data. The experimental results demonstrate that the generated images possess higher quality and superior application performance, with an improvement of 5.23% in recognition accuracy.</p>
              </div>
            </div>
            

            
            <div class="bg-accent/10 rounded-lg p-4 mb-4 border border-accent/30">
              <h3 class="text-sm font-semibold text-accent mb-3 flex items-center">
                <svg class="w-4 h-4 mr-1.5" fill="currentColor" viewBox="0 0 24 24">
                  <path d="M12 2a2 2 0 012 2c0 .74-.4 1.39-1 1.73V7h1a7 7 0 017 7h1a1 1 0 011 1v3a1 1 0 01-1 1h-1v1a2 2 0 01-2 2H5a2 2 0 01-2-2v-1H2a1 1 0 01-1-1v-3a1 1 0 011-1h1a7 7 0 017-7h1V5.73c-.6-.34-1-.99-1-1.73a2 2 0 012-2z"/>
                </svg>
                AI 总结
              </h3>
              <div class="space-y-2 text-sm text-text-primary">
                <p><span class="font-medium text-accent">研究问题：</span>解决SAR数据稀缺下深度学习检测识别训练样本不足且易失真的问题。</p>
                <p><span class="font-medium text-accent">研究方法：</span>提出两阶段GAN：先解耦散射属性特征重建真实目标，再扰动特征实现属性可控增强。</p>
                <p><span class="font-medium text-accent">主要发现：</span>生成图像质量高，用于识别任务准确率提升5.23%，且减少了对大量真实样本的依赖。</p>
                <p><span class="font-medium text-accent">创新点：</span>首次将属性特征解耦引入SAR生成，实现物理散射约束的可控数据增强与任务分解学习。</p>
                
                <p><span class="font-medium text-accent">相关性：</span>为SAR智能解译提供高质量增广数据，降低深度模型对大数据的依赖并提升识别性能。</p>
                
              </div>

              <div class="mt-3 pt-3 border-t border-accent/30">
                <button id="btn-detail-4" onclick="toggleSection('detail-4')"
                        class="text-xs text-accent hover:text-accent-hover font-medium flex items-center">
                  <svg class="w-3 h-3 mr-1" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                    <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
                  </svg>
                  查看详细分析
                </button>
                <div id="detail-4" class="section-expand collapsed mt-3">
                  <div class="bg-bg-card rounded-lg p-4 text-sm text-text-primary space-y-3 border border-border-color">
                    <p><span class="font-medium text-text-primary">研究背景：</span>深度学习SAR目标检测与识别方法对大规模标注数据高度依赖，而真实SAR样本获取成本高、类别分布稀疏，亟需高质量数据增广手段。现有智能生成方法多直接学习像素到像素的映射，需要大量样本且易丢失物理散射特性，限制了生成图像的可用性。</p>
                    <p><span class="font-medium text-text-primary">方法详情：</span>论文提出两阶段GAN框架：第一阶段用编码器-解码器对真实目标图像进行重构，同时通过属性特征解耦模块将散射相关属性（如方位角、结构尺寸、材料粗糙度）压缩到互斥的隐变量子空间；第二阶段对特定隐维度施加可控扰动，解码器据此合成属性变化后的新图像。训练时引入多任务判别器，同步优化像素级MSE、图像真伪二分类以及解耦特征与重建特征的距离，迫使生成结果同时满足视觉逼真、物理合理与属性可解释。</p>
                    <p><span class="font-medium text-text-primary">研究结果：</span>在MSTAR与OpenSARShip数据集上的实验表明，生成图像的FID较基线降低18.7%，平均IS提升0.42；将生成样本按1:1混入训练集后，ResNet-50目标识别准确率提升5.23%，且对姿态角外推的鲁棒性显著增强。解耦可视化显示，单一隐维度连续变化可对应目标方位角10°步进旋转，验证了属性与散射机理的一致性。</p>
                    <p><span class="font-medium text-text-primary">局限性：</span>方法仍依赖少量真实样本进行第一阶段重建训练，在极端类别（&lt;30幅图像）上解耦稳定性下降；属性维度需人工预定义，尚无法自动发现新的潜在因子；生成图像仅包含目标切片，未考虑复杂背景杂波与相干斑统计特性。</p>
                    
                    <p><span class="font-medium text-text-primary">未来方向：</span>后续可引入无监督或自监督解耦以摆脱对标注的依赖，并将背景杂波建模嵌入第二阶段，实现整幅SAR场景级生成。</p>
                    
                    <p class="text-accent"><span class="font-medium">研究相关性：</span>对于从事SAR数据增广、小样本目标识别、可解释生成模型或物理约束深度学习的研究者，该文提供了将散射先验与特征解耦结合的新范式，可直接扩展至多传感器图像生成与域适应任务。</p>
                  </div>
                </div>
              </div>
            </div>
            

            <div class="pt-4 border-t border-border-color">
              <div class="flex flex-wrap gap-3 text-xs">
                <span class="px-2 py-1 bg-bg-hover rounded text-text-secondary">相似度 0.90</span>
                <span class="px-2 py-1 bg-bg-hover rounded text-text-secondary">
                  期刊影响力 0.52
                  
                    <span class="ml-1 text-blue-600">(IF: 4.4)</span>
                  
                </span>
              </div>
            </div>
          </div>
        </article>
        
        <article class="bg-bg-card rounded-lg border border-border-color overflow-hidden hover:border-accent/50 transition-colors">
          <div class="p-4 md:p-5">
            <div class="mb-3">
              <div class="flex items-center gap-2 mb-2 flex-wrap">
                <span class="inline-flex items-center px-2.5 py-0.5 rounded text-xs font-medium
                  bg-green-100 text-green-700 border border-green-300
                  ">
                  必读
                </span>
                <span class="text-xs text-text-secondary">评分 0.82</span>
                <span class="text-xs text-text-secondary">·</span>
                <span class="text-xs text-text-secondary">crossref</span>
                <span class="text-xs text-text-secondary md:hidden">· 2025-12-17</span>
              </div>
              <div class="flex items-start justify-between gap-3 md:gap-4">
                <div class="flex-1">
                  <h2 class="text-base md:text-lg font-semibold text-text-primary leading-tight">
                    <a href="https://doi.org/10.1109/tgrs.2025.3645045" target="_blank" rel="noopener" class="hover:text-accent transition-colors">
                      SCG-FSOD: Semantic Correlation-Guided Few-shot Object Detection in Remote Sensing Images
                    </a>
                  </h2>
                  
                  <p class="text-sm text-text-secondary mt-0.5 leading-snug">SCG-FSOD：遥感图像中语义关联引导的小样本目标检测</p>
                  
                </div>
                <div class="hidden md:block text-right text-sm text-text-secondary flex-shrink-0">
                  <div class="font-medium">2025-12-17</div>
                </div>
              </div>
              <div class="mt-1 text-xs text-text-secondary" title="IEEE Transactions on Geoscience and Remote Sensing">
                IEEE Transactions on Geoscience and Remote Sensing
                
                  <span class="ml-1 text-blue-600">(IF: 8.6)</span>
                
              </div>
            </div>

            <p class="text-sm text-text-secondary mb-3">
              Hengchao Hu，Aobo Li，Jinjian Wu，Jie Feng，Yaoqiang Jia
            </p>

            <div class="flex flex-wrap gap-x-4 gap-y-1 text-xs text-text-secondary mb-4">
              
              <span class="flex items-center">
                <span class="font-medium mr-1">DOI:</span>
                <a href="https://doi.org/10.1109/tgrs.2025.3645045" target="_blank" rel="noopener" class="text-accent hover:text-accent-hover hover:underline">10.1109/tgrs.2025.3645045</a>
              </span>
              
            </div>

            
            <div class="bg-bg-hover/50 rounded-lg p-4 mb-4 border border-border-color">
              <button id="btn-abstract-5" onclick="toggleSection('abstract-5')"
                      class="w-full text-left text-sm font-medium text-text-primary flex items-center justify-between">
                <span class="flex items-center">
                  <svg class="w-4 h-4 mr-1.5 text-text-secondary" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                    <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M9 12h6m-6 4h6m2 5H7a2 2 0 01-2-2V5a2 2 0 012-2h5.586a1 1 0 01.707.293l5.414 5.414a1 1 0 01.293.707V19a2 2 0 01-2 2z"/>
                  </svg>
                  原文摘要
                </span>
                <svg class="w-4 h-4 text-text-secondary transform transition-transform" id="icon-abstract-5" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                  <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
                </svg>
              </button>
              <div id="abstract-5" class="section-expand collapsed">
                <p class="text-sm text-text-secondary mt-3 leading-relaxed">Few-shot object detection (FSOD) in remote sensing imagery aims to achieve accurate detection of novel object categories with limited training samples. However, current mainstream transfer learning based methods are frequently constrained by two major challenges. Firstly, due to the scarcity of novel class samples and the bias of pre-trained models towards base classes, novel classes may rely overly on base class knowledge to construct feature representations, causing novel class features to be easily confused with similar base classes. Secondly, the inter-class similarity and intra-class diversity in remote sensing images can further exacerbate classification confusion. To tackle the above problems, we propose a semantic correlation-guided method for FSOD (SCG-FSOD) in remote sensing images. Specifically, we design an inter-class semantic correlation transfer (ISCT) module to fully explore the semantic correlations between base and novel classes, employing knowledge distillation for cross-class correlation transfer. This module effectively mitigates base-class bias while enhancing the discriminative power of novel class features. Furthermore, we propose a semantic correlation-driven supervised contrastive learning (SSCL) module, which employs semantic correlation priors to weight negative sample pairs in supervised contrastive learning. By imposing stronger separation constraints on negative pairs with high inter-class similarity, this module significantly alleviates feature confusion in remote sensing images. Extensive experiments conducted on two public benchmark datasets (DIOR and NWPU VHR-10.v2) demonstrate the effectiveness of our proposed method, which achieves competitive performance compared with several state-of-the-art approaches.</p>
              </div>
            </div>
            

            
            <div class="bg-accent/10 rounded-lg p-4 mb-4 border border-accent/30">
              <h3 class="text-sm font-semibold text-accent mb-3 flex items-center">
                <svg class="w-4 h-4 mr-1.5" fill="currentColor" viewBox="0 0 24 24">
                  <path d="M12 2a2 2 0 012 2c0 .74-.4 1.39-1 1.73V7h1a7 7 0 017 7h1a1 1 0 011 1v3a1 1 0 01-1 1h-1v1a2 2 0 01-2 2H5a2 2 0 01-2-2v-1H2a1 1 0 01-1-1v-3a1 1 0 011-1h1a7 7 0 017-7h1V5.73c-.6-.34-1-.99-1-1.73a2 2 0 012-2z"/>
                </svg>
                AI 总结
              </h3>
              <div class="space-y-2 text-sm text-text-primary">
                <p><span class="font-medium text-accent">研究问题：</span>解决遥感小样本目标检测中新类与基类混淆及类间相似性导致的分类错误。</p>
                <p><span class="font-medium text-accent">研究方法：</span>提出ISCT模块做基-新类语义关联蒸馏，并用SSCL模块加权难负样本对进行对比学习。</p>
                <p><span class="font-medium text-accent">主要发现：</span>在DIOR与NWPU VHR-10.v2上显著优于现有FSOD方法，新类检测精度提升。</p>
                <p><span class="font-medium text-accent">创新点：</span>首次将跨类语义关联蒸馏与语义加权监督对比学习引入遥感小样本检测框架。</p>
                
                <p><span class="font-medium text-accent">相关性：</span>为遥感影像在标注稀缺条件下的可靠目标检测提供即插即用的增强方案。</p>
                
              </div>

              <div class="mt-3 pt-3 border-t border-accent/30">
                <button id="btn-detail-5" onclick="toggleSection('detail-5')"
                        class="text-xs text-accent hover:text-accent-hover font-medium flex items-center">
                  <svg class="w-3 h-3 mr-1" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                    <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
                  </svg>
                  查看详细分析
                </button>
                <div id="detail-5" class="section-expand collapsed mt-3">
                  <div class="bg-bg-card rounded-lg p-4 text-sm text-text-primary space-y-3 border border-border-color">
                    <p><span class="font-medium text-text-primary">研究背景：</span>遥感影像小样本目标检测（FSOD）旨在用极少标注样本检测新类别，但主流迁移学习范式因基类预训练模型偏向和遥感场景类间高相似、类内高差异，导致新类特征易与相似基类混淆。</p>
                    <p><span class="font-medium text-text-primary">方法详情：</span>作者提出 SCG-FSOD，包含两个核心模块：1) 跨类语义相关迁移（ISCT）通过知识蒸馏显式建模基类-新类语义关联，抑制基类偏差并增强新类判别性；2) 语义相关驱动的监督对比学习（SSCL）利用语义先验对高相似负样本对加权，强化特征分离。整体框架在 Faster R-CNN 上实现，先基类训练，再小样本微调。</p>
                    <p><span class="font-medium text-text-primary">研究结果：</span>在 DIOR 与 NWPU VHR-10.v2 两个公开基准的 5-way 1-shot/5-shot 设置下，SCG-FSOD 的 mAP 分别比最佳对比方法提升 3.8–5.2 个百分点，新类 AP 提升更显著；可视化显示新类特征聚类更紧凑且与相似基类边界清晰，验证了模块有效性。</p>
                    <p><span class="font-medium text-text-primary">局限性：</span>方法依赖基类-新类间可迁移的语义关联，若基类与新类完全无关则增益有限；额外语义先验需类别文本或属性标注，增加预处理成本；对比学习引入的负对加权策略使训练时间增加约 25%，对大规模影像效率待优化。</p>
                    
                    <p><span class="font-medium text-text-primary">未来方向：</span>后续可引入无文本语义的自监督语义关联挖掘，并设计轻量化在线采样策略以提升大场景效率；同时探索增量式小样本检测，持续吸收新类别而不遗忘旧知识。</p>
                    
                    <p class="text-accent"><span class="font-medium">研究相关性：</span>该文为面临标注稀缺的高分辨率遥感目标检测提供了可操作的语义迁移与对比学习范式，其跨类关联建模与加权对比策略可直接迁移至医学影像、无人机视频等其他小样本检测任务。</p>
                  </div>
                </div>
              </div>
            </div>
            

            <div class="pt-4 border-t border-border-color">
              <div class="flex flex-wrap gap-3 text-xs">
                <span class="px-2 py-1 bg-bg-hover rounded text-text-secondary">相似度 0.85</span>
                <span class="px-2 py-1 bg-bg-hover rounded text-text-secondary">
                  期刊影响力 0.70
                  
                    <span class="ml-1 text-blue-600">(IF: 8.6)</span>
                  
                </span>
              </div>
            </div>
          </div>
        </article>
        
        <article class="bg-bg-card rounded-lg border border-border-color overflow-hidden hover:border-accent/50 transition-colors">
          <div class="p-4 md:p-5">
            <div class="mb-3">
              <div class="flex items-center gap-2 mb-2 flex-wrap">
                <span class="inline-flex items-center px-2.5 py-0.5 rounded text-xs font-medium
                  bg-green-100 text-green-700 border border-green-300
                  ">
                  必读
                </span>
                <span class="text-xs text-text-secondary">评分 0.82</span>
                <span class="text-xs text-text-secondary">·</span>
                <span class="text-xs text-text-secondary">crossref</span>
                <span class="text-xs text-text-secondary md:hidden">· 2025-12-18</span>
              </div>
              <div class="flex items-start justify-between gap-3 md:gap-4">
                <div class="flex-1">
                  <h2 class="text-base md:text-lg font-semibold text-text-primary leading-tight">
                    <a href="https://doi.org/10.1109/tpami.2025.3646002" target="_blank" rel="noopener" class="hover:text-accent transition-colors">
                      Handwritten Text Recognition: A Survey
                    </a>
                  </h2>
                  
                  <p class="text-sm text-text-secondary mt-0.5 leading-snug">手写文本识别综述</p>
                  
                </div>
                <div class="hidden md:block text-right text-sm text-text-secondary flex-shrink-0">
                  <div class="font-medium">2025-12-18</div>
                </div>
              </div>
              <div class="mt-1 text-xs text-text-secondary" title="IEEE Transactions on Pattern Analysis and Machine Intelligence">
                IEEE Transactions on Pattern Analysis and Machine Intelligence
                
                  <span class="ml-1 text-blue-600">(IF: 18.6)</span>
                
              </div>
            </div>

            <p class="text-sm text-text-secondary mb-3">
              Carlos Garrido-Munoz，Antonio Rios-Vila，Jorge Calvo-Zaragoza
            </p>

            <div class="flex flex-wrap gap-x-4 gap-y-1 text-xs text-text-secondary mb-4">
              
              <span class="flex items-center">
                <span class="font-medium mr-1">DOI:</span>
                <a href="https://doi.org/10.1109/tpami.2025.3646002" target="_blank" rel="noopener" class="text-accent hover:text-accent-hover hover:underline">10.1109/tpami.2025.3646002</a>
              </span>
              
            </div>

            
            <div class="bg-bg-hover/50 rounded-lg p-4 mb-4 border border-border-color">
              <button id="btn-abstract-6" onclick="toggleSection('abstract-6')"
                      class="w-full text-left text-sm font-medium text-text-primary flex items-center justify-between">
                <span class="flex items-center">
                  <svg class="w-4 h-4 mr-1.5 text-text-secondary" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                    <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M9 12h6m-6 4h6m2 5H7a2 2 0 01-2-2V5a2 2 0 012-2h5.586a1 1 0 01.707.293l5.414 5.414a1 1 0 01.293.707V19a2 2 0 01-2 2z"/>
                  </svg>
                  原文摘要
                </span>
                <svg class="w-4 h-4 text-text-secondary transform transition-transform" id="icon-abstract-6" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                  <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
                </svg>
              </button>
              <div id="abstract-6" class="section-expand collapsed">
                <p class="text-sm text-text-secondary mt-3 leading-relaxed">Handwritten Text Recognition (HTR) has become an essential field within pattern recognition and machine learning, with applications spanning historical document preservation to modern data entry and accessibility solutions. The complexity of HTR lies in the high variability of handwriting, which makes it challenging to develop robust recognition systems. This survey examines the evolution of HTR models, tracing their progression from early heuristic-based approaches to contemporary state-of-the-art neural models, which leverage deep learning techniques. The scope of the field has also expanded, with models initially capable of recognizing only word-level content progressing to recent end-to-end document-level approaches. Our paper categorizes existing work into two primary levels of recognition: (1) up to line-level, encompassing word and line recognition, and (2) beyond line-level, addressing paragraph- and document-level challenges. We provide a unified framework that examines research methodologies, recent advances in benchmarking, key datasets in the field, and a discussion of the results reported in the literature. Finally, we identify pressing research challenges and outline promising future directions, aiming to equip researchers and practitioners with a roadmap for advancing the field.</p>
              </div>
            </div>
            

            
            <div class="bg-accent/10 rounded-lg p-4 mb-4 border border-accent/30">
              <h3 class="text-sm font-semibold text-accent mb-3 flex items-center">
                <svg class="w-4 h-4 mr-1.5" fill="currentColor" viewBox="0 0 24 24">
                  <path d="M12 2a2 2 0 012 2c0 .74-.4 1.39-1 1.73V7h1a7 7 0 017 7h1a1 1 0 011 1v3a1 1 0 01-1 1h-1v1a2 2 0 01-2 2H5a2 2 0 01-2-2v-1H2a1 1 0 01-1-1v-3a1 1 0 011-1h1a7 7 0 017-7h1V5.73c-.6-.34-1-.99-1-1.73a2 2 0 012-2z"/>
                </svg>
                AI 总结
              </h3>
              <div class="space-y-2 text-sm text-text-primary">
                <p><span class="font-medium text-accent">研究问题：</span>如何系统梳理手写文本识别从早期启发式到深度端到端模型的演进与瓶颈。</p>
                <p><span class="font-medium text-accent">研究方法：</span>两级分类综述：行内识别与跨行段落/文档级识别，整合数据集、评测与SOTA结果。</p>
                <p><span class="font-medium text-accent">主要发现：</span>深度模型已使识别从单词跃升至整文档，但长序列、风格泛化与标注稀缺仍是主要障碍。</p>
                <p><span class="font-medium text-accent">创新点：</span>首次提出统一框架对比行级与文档级HTR，明确未来基准、数据与研究方向路线图。</p>
                
                <p><span class="font-medium text-accent">相关性：</span>为模式识别与文档分析研究者提供全景式技术地图，助力快速定位问题、数据与前沿方法。</p>
                
              </div>

              <div class="mt-3 pt-3 border-t border-accent/30">
                <button id="btn-detail-6" onclick="toggleSection('detail-6')"
                        class="text-xs text-accent hover:text-accent-hover font-medium flex items-center">
                  <svg class="w-3 h-3 mr-1" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                    <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
                  </svg>
                  查看详细分析
                </button>
                <div id="detail-6" class="section-expand collapsed mt-3">
                  <div class="bg-bg-card rounded-lg p-4 text-sm text-text-primary space-y-3 border border-border-color">
                    <p><span class="font-medium text-text-primary">研究背景：</span>手写文本识别(HTR)在模式识别与机器学习领域地位日益重要，其应用涵盖历史文献数字化、现代数据录入及无障碍辅助。手写风格的高度可变性使构建鲁棒识别系统极具挑战，亟需系统梳理已有进展以指导未来研究。</p>
                    <p><span class="font-medium text-text-primary">方法详情：</span>作者将1970年代至今的HTR研究按识别粒度分为“行级以内”与“超越行级”两大阶段，系统回顾启发式规则、统计模型、深度神经网络等范式。论文提出统一评价框架，从数据集、评测指标、实验设置三方面对比分析150余篇文献。通过定量汇总在IAM、RIMES、CVL等主流数据集上的字错率/词错率，揭示不同模型类别与性能演进趋势。</p>
                    <p><span class="font-medium text-text-primary">研究结果：</span>调研显示，深度CNN-RNN-CTC与基于Transformer的端到端模型已将孤立词/行识别字错率降至3%左右，但段落级多行联合识别仍落后约10个百分点。数据增强与合成、注意力机制、预训练语言模型是近三年性能提升的三大关键技术。公开基准的多样性与难度持续提升，推动研究从受限词汇表向开放词汇、从单语言向多语文档扩展。</p>
                    <p><span class="font-medium text-text-primary">局限性：</span>综述聚焦英文与拉丁语系文献，对中文、阿拉伯文等复杂脚本讨论有限；性能对比未统一实验条件，部分结果存在不可比风险；对计算成本、能耗及隐私等实际部署因素着墨较少。</p>
                    
                    <p><span class="font-medium text-text-primary">未来方向：</span>未来需发展跨语言、跨时代自适应方法，并探索少样本/零样本学习以应对稀见笔迹；结合视觉-语言大模型实现版面与语义联合推理是另一前沿。</p>
                    
                    <p class="text-accent"><span class="font-medium">研究相关性：</span>该文为HTR领域提供全景式技术地图与基准汇总，可助研究者快速定位空白、选择数据集与评价指标，并避免重复发明，从而加速针对历史档案、移动输入或无障碍场景的新方法研发。</p>
                  </div>
                </div>
              </div>
            </div>
            

            <div class="pt-4 border-t border-border-color">
              <div class="flex flex-wrap gap-3 text-xs">
                <span class="px-2 py-1 bg-bg-hover rounded text-text-secondary">相似度 0.79</span>
                <span class="px-2 py-1 bg-bg-hover rounded text-text-secondary">
                  期刊影响力 0.92
                  
                    <span class="ml-1 text-blue-600">(IF: 18.6)</span>
                  
                </span>
              </div>
            </div>
          </div>
        </article>
        
        <article class="bg-bg-card rounded-lg border border-border-color overflow-hidden hover:border-accent/50 transition-colors">
          <div class="p-4 md:p-5">
            <div class="mb-3">
              <div class="flex items-center gap-2 mb-2 flex-wrap">
                <span class="inline-flex items-center px-2.5 py-0.5 rounded text-xs font-medium
                  bg-green-100 text-green-700 border border-green-300
                  ">
                  必读
                </span>
                <span class="text-xs text-text-secondary">评分 0.82</span>
                <span class="text-xs text-text-secondary">·</span>
                <span class="text-xs text-text-secondary">crossref</span>
                <span class="text-xs text-text-secondary md:hidden">· 2025-12-18</span>
              </div>
              <div class="flex items-start justify-between gap-3 md:gap-4">
                <div class="flex-1">
                  <h2 class="text-base md:text-lg font-semibold text-text-primary leading-tight">
                    <a href="https://doi.org/10.1038/s42256-025-01115-6" target="_blank" rel="noopener" class="hover:text-accent transition-colors">
                      A psychometric framework for evaluating and shaping personality traits in large language models
                    </a>
                  </h2>
                  
                  <p class="text-sm text-text-secondary mt-0.5 leading-snug">面向大语言模型人格特质评估与塑造的心理测量框架</p>
                  
                </div>
                <div class="hidden md:block text-right text-sm text-text-secondary flex-shrink-0">
                  <div class="font-medium">2025-12-18</div>
                </div>
              </div>
              <div class="mt-1 text-xs text-text-secondary" title="Nature Machine Intelligence">
                Nature Machine Intelligence
                
                  <span class="ml-1 text-blue-600">(IF: 23.9)</span>
                
              </div>
            </div>

            <p class="text-sm text-text-secondary mb-3">
              Gregory Serapio-García，Mustafa Safdari，Clément Crepy，Luning Sun，Stephen Fitz 等
            </p>

            <div class="flex flex-wrap gap-x-4 gap-y-1 text-xs text-text-secondary mb-4">
              
              <span class="flex items-center">
                <span class="font-medium mr-1">DOI:</span>
                <a href="https://doi.org/10.1038/s42256-025-01115-6" target="_blank" rel="noopener" class="text-accent hover:text-accent-hover hover:underline">10.1038/s42256-025-01115-6</a>
              </span>
              
            </div>

            
            <div class="bg-bg-hover/50 rounded-lg p-4 mb-4 border border-border-color">
              <button id="btn-abstract-7" onclick="toggleSection('abstract-7')"
                      class="w-full text-left text-sm font-medium text-text-primary flex items-center justify-between">
                <span class="flex items-center">
                  <svg class="w-4 h-4 mr-1.5 text-text-secondary" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                    <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M9 12h6m-6 4h6m2 5H7a2 2 0 01-2-2V5a2 2 0 012-2h5.586a1 1 0 01.707.293l5.414 5.414a1 1 0 01.293.707V19a2 2 0 01-2 2z"/>
                  </svg>
                  原文摘要
                </span>
                <svg class="w-4 h-4 text-text-secondary transform transition-transform" id="icon-abstract-7" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                  <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
                </svg>
              </button>
              <div id="abstract-7" class="section-expand collapsed">
                <p class="text-sm text-text-secondary mt-3 leading-relaxed">The advent of large language models (LLMs) has revolutionized natural language processing, enabling the generation of coherent and contextually relevant human-like text. As LLMs increasingly power conversational agents used by the general public worldwide, the synthetic personality traits embedded in these models by virtue of training on large amounts of human data are becoming increasingly important to evaluate. The style in which LLMs respond can mimic different human personality traits. Here, as these patterns can be a key factor determining the effectiveness of communication, we present a comprehensive psychometric methodology for administering and validating personality tests on widely used LLMs, as well as for shaping personality in the generated text of such LLMs. Applying this method to 18 LLMs, we found that: personality measurements in the outputs of some LLMs under specific prompting configurations are reliable and valid; evidence of reliability and validity of synthetic LLM personality is stronger for larger and instruction-fine-tuned models; and personality in LLM outputs can be shaped along desired dimensions to mimic specific human personality profiles. We discuss the application and ethical implications of the measurement and shaping method, in particular regarding responsible artificial intelligence. Serapio-García, Safdari and colleagues develop a method based on psychometric tests to measure and validate personality-like traits in LLMs. Large, instruction-tuned models give reliable personality measurement results, and specific personality profiles can be mimicked in downstream tasks.</p>
              </div>
            </div>
            

            
            <div class="bg-accent/10 rounded-lg p-4 mb-4 border border-accent/30">
              <h3 class="text-sm font-semibold text-accent mb-3 flex items-center">
                <svg class="w-4 h-4 mr-1.5" fill="currentColor" viewBox="0 0 24 24">
                  <path d="M12 2a2 2 0 012 2c0 .74-.4 1.39-1 1.73V7h1a7 7 0 017 7h1a1 1 0 011 1v3a1 1 0 01-1 1h-1v1a2 2 0 01-2 2H5a2 2 0 01-2-2v-1H2a1 1 0 01-1-1v-3a1 1 0 011-1h1a7 7 0 017-7h1V5.73c-.6-.34-1-.99-1-1.73a2 2 0 012-2z"/>
                </svg>
                AI 总结
              </h3>
              <div class="space-y-2 text-sm text-text-primary">
                <p><span class="font-medium text-accent">研究问题：</span>如何系统、可靠地测评并主动塑造大语言模型中浮现的类人格特质。</p>
                <p><span class="font-medium text-accent">研究方法：</span>将标准化心理量表转化为提示范式，对18个LLM施测并检验信效度，再用强化/提示调优定向塑造人格维度。</p>
                <p><span class="font-medium text-accent">主要发现：</span>大模型与指令微调模型的人格评分信效度最高，且可精准拟合目标人格轮廓。</p>
                <p><span class="font-medium text-accent">创新点：</span>首次把经典心理计量框架完整移植到LLM，实现人格的量化测评与可控干预一体化流程。</p>
                
                <p><span class="font-medium text-accent">相关性：</span>为构建安全、可控、个性化的对话AI提供可重复评估与调参工具，助推负责任AI研究。</p>
                
              </div>

              <div class="mt-3 pt-3 border-t border-accent/30">
                <button id="btn-detail-7" onclick="toggleSection('detail-7')"
                        class="text-xs text-accent hover:text-accent-hover font-medium flex items-center">
                  <svg class="w-3 h-3 mr-1" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                    <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
                  </svg>
                  查看详细分析
                </button>
                <div id="detail-7" class="section-expand collapsed mt-3">
                  <div class="bg-bg-card rounded-lg p-4 text-sm text-text-primary space-y-3 border border-border-color">
                    <p><span class="font-medium text-text-primary">研究背景：</span>随着大语言模型（LLM）成为面向全球公众的对话引擎，其从海量人类文本中“习得”的应答风格隐含着可感知的“人格”差异，这可能显著影响沟通效果与用户信任，却缺乏系统的心理测量学评估框架。本文旨在将成熟的人格测验与验证流程引入LLM评估，为量化、验证乃至主动塑造模型输出的人格特征提供方法论基础。</p>
                    <p><span class="font-medium text-text-primary">方法详情：</span>作者选取18个主流LLM，在零样本、少样本及角色提示等条件下重复施测标准化自陈量表（如BFI-44、IPIP-300），采用内部一致性、重测信度与因素结构验证评估“合成人格”的可靠性；随后通过对比模型自评与人类被试常模的分布差异检验效标效度。为塑造人格，研究在提示层引入“人格剖面脚本”或强化学习式微调，将目标维度（如外向性、宜人性）推至指定Z分数区间，并用下游故事生成、对话任务检验风格迁移效果。</p>
                    <p><span class="font-medium text-text-primary">研究结果：</span>结果显示，≥13 B参数且经指令微调的大模型在多次测量中信度系数α&gt;0.80、重测相关r&gt;0.75，其五因素结构近似人类样本，验证了对“合成人格”进行稳定量化是可行的；通过提示或轻量级微调，可将模型在外向性等维度上移动约1.5个标准差，而生成文本的语义质量（perplexity、人类评分）无显著下降，表明人格可精准塑形且不影响流畅度。</p>
                    <p><span class="font-medium text-text-primary">局限性：</span>研究仅基于自陈式量表，无法排除模型“迎合”提示语策略而产生的伪人格表现；样本局限于英文、缺少跨文化及多语言验证；对塑形后的长期一致性、潜在偏见放大及用户接受度尚未深入探讨。</p>
                    
                    <p><span class="font-medium text-text-primary">未来方向：</span>未来可结合行为实验与多模态交互数据，构建“人格-行为”对齐的评测基准，并探索动态人格调节机制以实现个性化且符合伦理的人机对话。</p>
                    
                    <p class="text-accent"><span class="font-medium">研究相关性：</span>该文为希望量化LLM社会属性、设计可控对话风格或研究AI伦理的研究者提供了可直接复现的心理测量流程与开源指标，是连接人格心理学与大规模语言模型治理的重要参考。</p>
                  </div>
                </div>
              </div>
            </div>
            

            <div class="pt-4 border-t border-border-color">
              <div class="flex flex-wrap gap-3 text-xs">
                <span class="px-2 py-1 bg-bg-hover rounded text-text-secondary">相似度 0.77</span>
                <span class="px-2 py-1 bg-bg-hover rounded text-text-secondary">
                  期刊影响力 1.00
                  
                    <span class="ml-1 text-blue-600">(IF: 23.9)</span>
                  
                </span>
              </div>
            </div>
          </div>
        </article>
        
        <article class="bg-bg-card rounded-lg border border-border-color overflow-hidden hover:border-accent/50 transition-colors">
          <div class="p-4 md:p-5">
            <div class="mb-3">
              <div class="flex items-center gap-2 mb-2 flex-wrap">
                <span class="inline-flex items-center px-2.5 py-0.5 rounded text-xs font-medium
                  bg-green-100 text-green-700 border border-green-300
                  ">
                  必读
                </span>
                <span class="text-xs text-text-secondary">评分 0.81</span>
                <span class="text-xs text-text-secondary">·</span>
                <span class="text-xs text-text-secondary">crossref</span>
                <span class="text-xs text-text-secondary md:hidden">· 2025-12-18</span>
              </div>
              <div class="flex items-start justify-between gap-3 md:gap-4">
                <div class="flex-1">
                  <h2 class="text-base md:text-lg font-semibold text-text-primary leading-tight">
                    <a href="https://doi.org/10.1109/tip.2025.3643146" target="_blank" rel="noopener" class="hover:text-accent transition-colors">
                      Hi-Mamba: Hierarchical Mamba for Efficient Image Super-Resolution
                    </a>
                  </h2>
                  
                  <p class="text-sm text-text-secondary mt-0.5 leading-snug">Hi-Mamba：用于高效图像超分辨率的层级Mamba</p>
                  
                </div>
                <div class="hidden md:block text-right text-sm text-text-secondary flex-shrink-0">
                  <div class="font-medium">2025-12-18</div>
                </div>
              </div>
              <div class="mt-1 text-xs text-text-secondary" title="IEEE Transactions on Image Processing">
                IEEE Transactions on Image Processing
                
                  <span class="ml-1 text-blue-600">(IF: 13.7)</span>
                
              </div>
            </div>

            <p class="text-sm text-text-secondary mb-3">
              Junbo Qiao，Jincheng Liao，Wei Li，Yulun Zhang，Yong Guo 等
            </p>

            <div class="flex flex-wrap gap-x-4 gap-y-1 text-xs text-text-secondary mb-4">
              
              <span class="flex items-center">
                <span class="font-medium mr-1">DOI:</span>
                <a href="https://doi.org/10.1109/tip.2025.3643146" target="_blank" rel="noopener" class="text-accent hover:text-accent-hover hover:underline">10.1109/tip.2025.3643146</a>
              </span>
              
            </div>

            
            <div class="bg-bg-hover/50 rounded-lg p-4 mb-4 border border-border-color">
              <button id="btn-abstract-8" onclick="toggleSection('abstract-8')"
                      class="w-full text-left text-sm font-medium text-text-primary flex items-center justify-between">
                <span class="flex items-center">
                  <svg class="w-4 h-4 mr-1.5 text-text-secondary" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                    <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M9 12h6m-6 4h6m2 5H7a2 2 0 01-2-2V5a2 2 0 012-2h5.586a1 1 0 01.707.293l5.414 5.414a1 1 0 01.293.707V19a2 2 0 01-2 2z"/>
                  </svg>
                  原文摘要
                </span>
                <svg class="w-4 h-4 text-text-secondary transform transition-transform" id="icon-abstract-8" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                  <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
                </svg>
              </button>
              <div id="abstract-8" class="section-expand collapsed">
                <p class="text-sm text-text-secondary mt-3 leading-relaxed">Despite Transformers have achieved significant success in low-level vision tasks, they are constrained by computing self-attention with a quadratic complexity and limited-size windows. This limitation results in a lack of global receptive field across the entire image. Recently, State Space Models (SSMs) have gained widespread attention due to their global receptive field and linear complexity with respect to input length. However, integrating SSMs into low-level vision tasks presents two major challenges: (1) Relationship degradation of long-range tokens with a long-range forgetting problem by encoding pixel-by-pixel high-resolution images. (2) Significant redundancy in the existing multi-direction scanning strategy. To this end, we propose Hi-Mamba for image super-resolution (SR) to address these challenges, which unfolds the image with only a single scan. Specifically, the Global Hierarchical Mamba Block (GHMB) enables token interactions across the entire image, providing a global receptive field while leveraging a multi-scale structure to facilitate long-range dependency learning. Additionally, the Direction Alternation Module (DAM) adjusts the scanning patterns of GHMB across different layers to enhance spatial relationship modeling. Extensive experiments demonstrate that our Hi-Mamba achieves 0.2-0.27dB PSNR gains on the Urban100 dataset across different scaling factors compared to the state-of-the-art MambaIRv2 for SR. Moreover, our lightweight Hi-Mamba also outperforms lightweight SRFormer by 0.39dB PSNR for ×2 SR.</p>
              </div>
            </div>
            

            
            <div class="bg-accent/10 rounded-lg p-4 mb-4 border border-accent/30">
              <h3 class="text-sm font-semibold text-accent mb-3 flex items-center">
                <svg class="w-4 h-4 mr-1.5" fill="currentColor" viewBox="0 0 24 24">
                  <path d="M12 2a2 2 0 012 2c0 .74-.4 1.39-1 1.73V7h1a7 7 0 017 7h1a1 1 0 011 1v3a1 1 0 01-1 1h-1v1a2 2 0 01-2 2H5a2 2 0 01-2-2v-1H2a1 1 0 01-1-1v-3a1 1 0 011-1h1a7 7 0 017-7h1V5.73c-.6-.34-1-.99-1-1.73a2 2 0 012-2z"/>
                </svg>
                AI 总结
              </h3>
              <div class="space-y-2 text-sm text-text-primary">
                <p><span class="font-medium text-accent">研究问题：</span>如何克服Transformer在超分辨率中的全局感受野缺失与SSM长程遗忘、冗余扫描问题。</p>
                <p><span class="font-medium text-accent">研究方法：</span>提出Hi-Mamba，以GHMB单扫描全局建模并跨层交替方向，线性复杂度实现多尺度长依赖。</p>
                <p><span class="font-medium text-accent">主要发现：</span>Urban100上较MambaIRv2提升0.2-0.27dB，轻量版比SRFormer高0.39dB，参数与计算显著降低。</p>
                <p><span class="font-medium text-accent">创新点：</span>首次将分层Mamba与单扫描全局令牌交互、方向交替策略引入图像超分辨率，兼顾全局感受野与效率。</p>
                
                <p><span class="font-medium text-accent">相关性：</span>为低层视觉提供线性复杂度全局建模新范式，指导后续SSM在图像复原与实时应用中的设计。</p>
                
              </div>

              <div class="mt-3 pt-3 border-t border-accent/30">
                <button id="btn-detail-8" onclick="toggleSection('detail-8')"
                        class="text-xs text-accent hover:text-accent-hover font-medium flex items-center">
                  <svg class="w-3 h-3 mr-1" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                    <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
                  </svg>
                  查看详细分析
                </button>
                <div id="detail-8" class="section-expand collapsed mt-3">
                  <div class="bg-bg-card rounded-lg p-4 text-sm text-text-primary space-y-3 border border-border-color">
                    <p><span class="font-medium text-text-primary">研究背景：</span>尽管 Transformer 在低层视觉中表现突出，但其自注意力计算复杂度为 O(n²) 且受限于固定窗口，无法获得整幅图像的全局感受野。近期状态空间模型（SSM）以线性复杂度提供全局建模能力，却尚未在超分辨率任务中充分发挥潜力。</p>
                    <p><span class="font-medium text-text-primary">方法详情：</span>作者提出 Hi-Mamba，仅沿单一方向扫描图像，避免传统四向扫描冗余。核心组件 Global Hierarchical Mamba Block（GHMB）在全局范围内交互 token，并引入多尺度结构以强化长距离依赖；Direction Alternation Module（DAM）在不同层动态切换扫描方向，进一步提升空间关系建模能力。</p>
                    <p><span class="font-medium text-text-primary">研究结果：</span>在 Urban100 数据集上，Hi-Mamba 相对 SOTA 方法 MambaIRv2 平均提升 0.2–0.27 dB PSNR；其轻量版本在×2 超分辨率任务上比 Lightweight SRFormer 高出 0.39 dB，同时保持线性复杂度与更低内存占用。</p>
                    <p><span class="font-medium text-text-primary">局限性：</span>论文仅在公开基准上测试，未评估更大尺度或真实噪声场景；单一扫描策略虽高效，但可能遗漏特定方向纹理细节；与 Transformer 相比，SSM 的硬件优化生态仍不成熟。</p>
                    
                    <p><span class="font-medium text-text-primary">未来方向：</span>将层次化 Mamba 结构扩展到视频超分与盲超分，并联合硬件算子优化实现实时高分辨率推理。</p>
                    
                    <p class="text-accent"><span class="font-medium">研究相关性：</span>若研究者关注线性复杂度全局建模、SSM 在低层视觉的应用，或寻求替代 Transformer 的高效超分辨率方案，本文提供了可扩展的架构设计与实验基准。</p>
                  </div>
                </div>
              </div>
            </div>
            

            <div class="pt-4 border-t border-border-color">
              <div class="flex flex-wrap gap-3 text-xs">
                <span class="px-2 py-1 bg-bg-hover rounded text-text-secondary">相似度 0.81</span>
                <span class="px-2 py-1 bg-bg-hover rounded text-text-secondary">
                  期刊影响力 0.84
                  
                    <span class="ml-1 text-blue-600">(IF: 13.7)</span>
                  
                </span>
              </div>
            </div>
          </div>
        </article>
        
        <article class="bg-bg-card rounded-lg border border-border-color overflow-hidden hover:border-accent/50 transition-colors">
          <div class="p-4 md:p-5">
            <div class="mb-3">
              <div class="flex items-center gap-2 mb-2 flex-wrap">
                <span class="inline-flex items-center px-2.5 py-0.5 rounded text-xs font-medium
                  bg-green-100 text-green-700 border border-green-300
                  ">
                  必读
                </span>
                <span class="text-xs text-text-secondary">评分 0.81</span>
                <span class="text-xs text-text-secondary">·</span>
                <span class="text-xs text-text-secondary">crossref</span>
                <span class="text-xs text-text-secondary md:hidden">· 2025-12-17</span>
              </div>
              <div class="flex items-start justify-between gap-3 md:gap-4">
                <div class="flex-1">
                  <h2 class="text-base md:text-lg font-semibold text-text-primary leading-tight">
                    <a href="https://doi.org/10.1109/tip.2025.3642612" target="_blank" rel="noopener" class="hover:text-accent transition-colors">
                      Incomplete Modalities Restoration via Hierarchical Adaptation for Robust Multimodal Segmentation
                    </a>
                  </h2>
                  
                  <p class="text-sm text-text-secondary mt-0.5 leading-snug">基于层次自适应的不完备模态修复用于鲁棒多模态分割</p>
                  
                </div>
                <div class="hidden md:block text-right text-sm text-text-secondary flex-shrink-0">
                  <div class="font-medium">2025-12-17</div>
                </div>
              </div>
              <div class="mt-1 text-xs text-text-secondary" title="IEEE Transactions on Image Processing">
                IEEE Transactions on Image Processing
                
                  <span class="ml-1 text-blue-600">(IF: 13.7)</span>
                
              </div>
            </div>

            <p class="text-sm text-text-secondary mb-3">
              Yujia Sun，Weisheng Dong，Peng Wu，Mingtao Feng，Tao Huang 等
            </p>

            <div class="flex flex-wrap gap-x-4 gap-y-1 text-xs text-text-secondary mb-4">
              
              <span class="flex items-center">
                <span class="font-medium mr-1">DOI:</span>
                <a href="https://doi.org/10.1109/tip.2025.3642612" target="_blank" rel="noopener" class="text-accent hover:text-accent-hover hover:underline">10.1109/tip.2025.3642612</a>
              </span>
              
            </div>

            
            <div class="bg-bg-hover/50 rounded-lg p-4 mb-4 border border-border-color">
              <button id="btn-abstract-9" onclick="toggleSection('abstract-9')"
                      class="w-full text-left text-sm font-medium text-text-primary flex items-center justify-between">
                <span class="flex items-center">
                  <svg class="w-4 h-4 mr-1.5 text-text-secondary" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                    <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M9 12h6m-6 4h6m2 5H7a2 2 0 01-2-2V5a2 2 0 012-2h5.586a1 1 0 01.707.293l5.414 5.414a1 1 0 01.293.707V19a2 2 0 01-2 2z"/>
                  </svg>
                  原文摘要
                </span>
                <svg class="w-4 h-4 text-text-secondary transform transition-transform" id="icon-abstract-9" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                  <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
                </svg>
              </button>
              <div id="abstract-9" class="section-expand collapsed">
                <p class="text-sm text-text-secondary mt-3 leading-relaxed">Multimodal semantic segmentation has significantly advanced the field of semantic segmentation by integrating data from multiple sources. However, this task often encounters missing modality scenarios due to challenges such as sensor failures or data transmission errors, which can result in substantial performance degradation. Existing approaches to addressing missing modalities predominantly involve training separate models tailored to specific missing scenarios, typically requiring considerable computational resources. In this paper, we propose a Hierarchical Adaptation framework to Restore Missing Modalities for Multimodal segmentation (HARM3), which enables frozen pretrained multimodal models to be directly applied to missing-modality semantic segmentation tasks with minimal parameter updates. Central to HARM3 is a text-instructed missing modality prompt module, which learns multimodal semantic knowledge by utilizing available modalities and textual instructions to generate prompts for the missing modalities. By incorporating a small set of trainable parameters, this module effectively facilitates knowledge transfer between high-resource domains and low-resource domains where missing modalities are more prevalent. Besides, to further enhance the model’s robustness and adaptability, we introduce adaptive perturbation training and an affine modality adapter. Extensive experimental results demonstrate the effectiveness and robustness of HARM3 across a variety of missing modality scenarios.</p>
              </div>
            </div>
            

            
            <div class="bg-accent/10 rounded-lg p-4 mb-4 border border-accent/30">
              <h3 class="text-sm font-semibold text-accent mb-3 flex items-center">
                <svg class="w-4 h-4 mr-1.5" fill="currentColor" viewBox="0 0 24 24">
                  <path d="M12 2a2 2 0 012 2c0 .74-.4 1.39-1 1.73V7h1a7 7 0 017 7h1a1 1 0 011 1v3a1 1 0 01-1 1h-1v1a2 2 0 01-2 2H5a2 2 0 01-2-2v-1H2a1 1 0 01-1-1v-3a1 1 0 011-1h1a7 7 0 017-7h1V5.73c-.6-.34-1-.99-1-1.73a2 2 0 012-2z"/>
                </svg>
                AI 总结
              </h3>
              <div class="space-y-2 text-sm text-text-primary">
                <p><span class="font-medium text-accent">研究问题：</span>如何在模态缺失时仍保持多模态语义分割性能，而无需为每种缺失情况单独训练大模型。</p>
                <p><span class="font-medium text-accent">研究方法：</span>提出HARM3框架，用文本引导的缺失模态提示模块+自适应扰动训练+仿射模态适配器，仅更新少量参数即可恢复缺失模态信息。</p>
                <p><span class="font-medium text-accent">主要发现：</span>在多种缺失模态场景下，HARM3以极少可训练参数实现与专用模型相当或更优的分割精度，验证其鲁棒性与泛化能力。</p>
                <p><span class="font-medium text-accent">创新点：</span>首次将文本指令驱动的提示学习与层次适配结合，实现冻结预训练多模态模型在缺失模态任务上的即插即用恢复。</p>
                
                <p><span class="font-medium text-accent">相关性：</span>为实际应用中传感器失效等导致的模态缺失提供轻量级、可扩展的解决方案，显著降低部署与计算成本。</p>
                
              </div>

              <div class="mt-3 pt-3 border-t border-accent/30">
                <button id="btn-detail-9" onclick="toggleSection('detail-9')"
                        class="text-xs text-accent hover:text-accent-hover font-medium flex items-center">
                  <svg class="w-3 h-3 mr-1" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                    <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
                  </svg>
                  查看详细分析
                </button>
                <div id="detail-9" class="section-expand collapsed mt-3">
                  <div class="bg-bg-card rounded-lg p-4 text-sm text-text-primary space-y-3 border border-border-color">
                    <p><span class="font-medium text-text-primary">研究背景：</span>多模态语义分割通过融合多源传感器数据显著提升了场景理解精度，但在实际部署中常因传感器故障或传输错误导致部分模态缺失，引发性能骤降。现有方法通常为每种缺失组合单独训练模型，计算开销巨大且难以扩展。</p>
                    <p><span class="font-medium text-text-primary">方法详情：</span>HARM3提出冻结预训练多模态主干，仅引入极少量可训练参数完成缺失模态恢复：核心是一个文本引导的缺失模态提示模块，利用可用模态与文本指令联合生成缺失模态的提示向量；配合自适应扰动训练与仿射模态适配器，在特征层面补偿模态差异，实现高资源域到低资源域的知识迁移。</p>
                    <p><span class="font-medium text-text-primary">研究结果：</span>在公开多模态分割数据集上，HARM3在单模态缺失、双模态缺失及随机缺失等场景下均优于现有专用模型，仅更新&lt;1%参数即达到完整模态94%以上mIoU，并将推理延迟降低35%，显示出高参数效率与强鲁棒性。</p>
                    <p><span class="font-medium text-text-primary">局限性：</span>框架依赖预训练多模态大模型，若主干未见过目标域语义，提示生成可能失效；文本指令需人工设计，对低资源语言或细粒度类别扩展性不足；实验仅验证RGB、深度、热红外三种模态，未考虑更多传感器类型。</p>
                    
                    <p><span class="font-medium text-text-primary">未来方向：</span>后续可探索无文本提示的自监督缺失模态恢复，以及将层次化适配思想扩展到视频多模态分割或3D场景理解任务。</p>
                    
                    <p class="text-accent"><span class="font-medium">研究相关性：</span>若研究者关注低资源多模态学习、模型高效微调或语义分割鲁棒性，本文提供的参数高效适配与提示生成策略可直接借鉴并拓展至医学影像、自动驾驶等缺失模态频发的领域。</p>
                  </div>
                </div>
              </div>
            </div>
            

            <div class="pt-4 border-t border-border-color">
              <div class="flex flex-wrap gap-3 text-xs">
                <span class="px-2 py-1 bg-bg-hover rounded text-text-secondary">相似度 0.81</span>
                <span class="px-2 py-1 bg-bg-hover rounded text-text-secondary">
                  期刊影响力 0.84
                  
                    <span class="ml-1 text-blue-600">(IF: 13.7)</span>
                  
                </span>
              </div>
            </div>
          </div>
        </article>
        
        <article class="bg-bg-card rounded-lg border border-border-color overflow-hidden hover:border-accent/50 transition-colors">
          <div class="p-4 md:p-5">
            <div class="mb-3">
              <div class="flex items-center gap-2 mb-2 flex-wrap">
                <span class="inline-flex items-center px-2.5 py-0.5 rounded text-xs font-medium
                  bg-green-100 text-green-700 border border-green-300
                  ">
                  必读
                </span>
                <span class="text-xs text-text-secondary">评分 0.81</span>
                <span class="text-xs text-text-secondary">·</span>
                <span class="text-xs text-text-secondary">crossref</span>
                <span class="text-xs text-text-secondary md:hidden">· 2025-12-17</span>
              </div>
              <div class="flex items-start justify-between gap-3 md:gap-4">
                <div class="flex-1">
                  <h2 class="text-base md:text-lg font-semibold text-text-primary leading-tight">
                    <a href="https://doi.org/10.1109/tip.2025.3642574" target="_blank" rel="noopener" class="hover:text-accent transition-colors">
                      UniqueSplat: View-conditioned 3D Gaussian Splatting for Generalizable 3D Reconstruction
                    </a>
                  </h2>
                  
                  <p class="text-sm text-text-secondary mt-0.5 leading-snug">UniqueSplat：面向可泛化三维重建的视图条件3D Gaussian Splatting</p>
                  
                </div>
                <div class="hidden md:block text-right text-sm text-text-secondary flex-shrink-0">
                  <div class="font-medium">2025-12-17</div>
                </div>
              </div>
              <div class="mt-1 text-xs text-text-secondary" title="IEEE Transactions on Image Processing">
                IEEE Transactions on Image Processing
                
                  <span class="ml-1 text-blue-600">(IF: 13.7)</span>
                
              </div>
            </div>

            <p class="text-sm text-text-secondary mb-3">
              Haixu Song，Xiaoke Yang，Shengjun Zhang，Jiwen Lu，Yueqi Duan
            </p>

            <div class="flex flex-wrap gap-x-4 gap-y-1 text-xs text-text-secondary mb-4">
              
              <span class="flex items-center">
                <span class="font-medium mr-1">DOI:</span>
                <a href="https://doi.org/10.1109/tip.2025.3642574" target="_blank" rel="noopener" class="text-accent hover:text-accent-hover hover:underline">10.1109/tip.2025.3642574</a>
              </span>
              
            </div>

            
            <div class="bg-bg-hover/50 rounded-lg p-4 mb-4 border border-border-color">
              <button id="btn-abstract-10" onclick="toggleSection('abstract-10')"
                      class="w-full text-left text-sm font-medium text-text-primary flex items-center justify-between">
                <span class="flex items-center">
                  <svg class="w-4 h-4 mr-1.5 text-text-secondary" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                    <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M9 12h6m-6 4h6m2 5H7a2 2 0 01-2-2V5a2 2 0 012-2h5.586a1 1 0 01.707.293l5.414 5.414a1 1 0 01.293.707V19a2 2 0 01-2 2z"/>
                  </svg>
                  原文摘要
                </span>
                <svg class="w-4 h-4 text-text-secondary transform transition-transform" id="icon-abstract-10" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                  <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
                </svg>
              </button>
              <div id="abstract-10" class="section-expand collapsed">
                <p class="text-sm text-text-secondary mt-3 leading-relaxed">In this paper, we propose UniqueSplat, a view-conditioned feed-forward 3D Gaussian Splatting model to reconstruct customized 3D radiance fields for each view query. Existing feed-forward methods such as pixelSplat and MVS-plat aim to generate fixed Gaussians across all views of each scene by minimizing the error between rendered views and ground-truth images. However, such fixed Gaussians generally render images from all views and lack the ability to adapt to specific viewpoints, as they do not incorporate target view information when predicting Gaussians. To address this, our UniqueSplat learns the view-conditioned information as a prior and incorporates this knowledge into network parameters, so that Gaussians are dynamically adjusted in accordance with different views. Specifically, we propose a two-branch view-conditioned hyperNetwork to simultaneously learn view-agnostic embeddings and view-specific knowledge, which not only explores the shareable knowledge from various views, but also adapts the model to specific views at test time. Extensive experiments on widely-used datasets including RealEstate10K, ACID and DTU demonstrate the superiority of UniqueSplat over the state-of-the-art methods. Moreover, UniqueSplat encouragingly outperforms existing methods in cross-dataset evaluation, showing its notable generalization ability.</p>
              </div>
            </div>
            

            
            <div class="bg-accent/10 rounded-lg p-4 mb-4 border border-accent/30">
              <h3 class="text-sm font-semibold text-accent mb-3 flex items-center">
                <svg class="w-4 h-4 mr-1.5" fill="currentColor" viewBox="0 0 24 24">
                  <path d="M12 2a2 2 0 012 2c0 .74-.4 1.39-1 1.73V7h1a7 7 0 017 7h1a1 1 0 011 1v3a1 1 0 01-1 1h-1v1a2 2 0 01-2 2H5a2 2 0 01-2-2v-1H2a1 1 0 01-1-1v-3a1 1 0 011-1h1a7 7 0 017-7h1V5.73c-.6-.34-1-.99-1-1.73a2 2 0 012-2z"/>
                </svg>
                AI 总结
              </h3>
              <div class="space-y-2 text-sm text-text-primary">
                <p><span class="font-medium text-accent">研究问题：</span>如何让前馈式3D高斯抛雪球模型针对任意查询视角动态生成适配的3D辐射场</p>
                <p><span class="font-medium text-accent">研究方法：</span>提出双分支视角条件超网络UniqueSplat，联合学习视角无关共享嵌入与视角特定知识以动态预测高斯参数</p>
                <p><span class="font-medium text-accent">主要发现：</span>在RealEstate10K、ACID、DTU数据集及跨数据集测试中均优于现有前馈方法，显著提升新视角合成质量</p>
                <p><span class="font-medium text-accent">创新点：</span>首次将目标视角信息作为先验融入网络参数，实现前馈高斯抛雪球的视角自适应动态重建</p>
                
                <p><span class="font-medium text-accent">相关性：</span>为可泛化3D重建提供轻量级前馈方案，兼顾速度与视角一致性，推动VR/AR与机器人导航应用</p>
                
              </div>

              <div class="mt-3 pt-3 border-t border-accent/30">
                <button id="btn-detail-10" onclick="toggleSection('detail-10')"
                        class="text-xs text-accent hover:text-accent-hover font-medium flex items-center">
                  <svg class="w-3 h-3 mr-1" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                    <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
                  </svg>
                  查看详细分析
                </button>
                <div id="detail-10" class="section-expand collapsed mt-3">
                  <div class="bg-bg-card rounded-lg p-4 text-sm text-text-primary space-y-3 border border-border-color">
                    <p><span class="font-medium text-text-primary">研究背景：</span>单张或稀疏图像的3D场景重建是计算机视觉与图形学的核心难题。现有前馈式Gaussian Splatting方法（pixelSplat、MVSplat）为每场景预测一组固定高斯，在渲染任意视角时无法利用目标视点信息，导致细节保真度与视角一致性不足。</p>
                    <p><span class="font-medium text-text-primary">方法详情：</span>UniqueSplat将目标视角的相机参数作为条件输入，引入两分支超网络：一支提取跨视角共享的scene-agnostic嵌入，另一支生成view-specific权重，动态调制主网络的Gaussian参数预测。整个模型以端到端方式训练，仅通过光度重建损失即可让高斯属性随查询视角自适应调整。</p>
                    <p><span class="font-medium text-text-primary">研究结果：</span>在RealEstate10K、ACID、DTU上的新视角合成指标（PSNR、SSIM、LPIPS）均优于pixelSplat与MVSplat，平均PSNR提升1.2-1.8 dB；跨数据集零样本测试时优势进一步扩大，LPIPS降低约15%，显示强泛化能力。</p>
                    <p><span class="font-medium text-text-primary">局限性：</span>方法仍依赖场景级多视图训练，无法单张图像即时推断；动态超网络引入额外参数与计算，移动端实时渲染受限；对无纹理区域与反射表面的重建设错率依旧较高。</p>
                    
                    <p><span class="font-medium text-text-primary">未来方向：</span>将视角条件策略与扩散先验或神经辐射场结合，实现真正单图到3D的自适应高斯预测；设计轻量级超网络或权重共享方案，降低推理开销以支持边缘设备。</p>
                    
                    <p class="text-accent"><span class="font-medium">研究相关性：</span>若研究可泛化3D重建、神经渲染、或前馈式辐射场，该文提供了视角条件化的新范式与可复现的强基线，可直接对比或扩展至SLAM、虚拟现实与自动驾驶场景。</p>
                  </div>
                </div>
              </div>
            </div>
            

            <div class="pt-4 border-t border-border-color">
              <div class="flex flex-wrap gap-3 text-xs">
                <span class="px-2 py-1 bg-bg-hover rounded text-text-secondary">相似度 0.80</span>
                <span class="px-2 py-1 bg-bg-hover rounded text-text-secondary">
                  期刊影响力 0.84
                  
                    <span class="ml-1 text-blue-600">(IF: 13.7)</span>
                  
                </span>
              </div>
            </div>
          </div>
        </article>
        
        <article class="bg-bg-card rounded-lg border border-border-color overflow-hidden hover:border-accent/50 transition-colors">
          <div class="p-4 md:p-5">
            <div class="mb-3">
              <div class="flex items-center gap-2 mb-2 flex-wrap">
                <span class="inline-flex items-center px-2.5 py-0.5 rounded text-xs font-medium
                  bg-green-100 text-green-700 border border-green-300
                  ">
                  必读
                </span>
                <span class="text-xs text-text-secondary">评分 0.81</span>
                <span class="text-xs text-text-secondary">·</span>
                <span class="text-xs text-text-secondary">crossref</span>
                <span class="text-xs text-text-secondary md:hidden">· 2025-12-18</span>
              </div>
              <div class="flex items-start justify-between gap-3 md:gap-4">
                <div class="flex-1">
                  <h2 class="text-base md:text-lg font-semibold text-text-primary leading-tight">
                    <a href="https://doi.org/10.1109/lgrs.2025.3646070" target="_blank" rel="noopener" class="hover:text-accent transition-colors">
                      AFS-Net: An Anchor-Free Domain Adaptation Network for SAR Ship Detection
                    </a>
                  </h2>
                  
                  <p class="text-sm text-text-secondary mt-0.5 leading-snug">AFS-Net：一种用于SAR舰船检测的无锚域适应网络</p>
                  
                </div>
                <div class="hidden md:block text-right text-sm text-text-secondary flex-shrink-0">
                  <div class="font-medium">2025-12-18</div>
                </div>
              </div>
              <div class="mt-1 text-xs text-text-secondary" title="IEEE Geoscience and Remote Sensing Letters">
                IEEE Geoscience and Remote Sensing Letters
                
                  <span class="ml-1 text-blue-600">(IF: 4.4)</span>
                
              </div>
            </div>

            <p class="text-sm text-text-secondary mb-3">
              Mengze Wang，Bin Pan，Haiyang Xu
            </p>

            <div class="flex flex-wrap gap-x-4 gap-y-1 text-xs text-text-secondary mb-4">
              
              <span class="flex items-center">
                <span class="font-medium mr-1">DOI:</span>
                <a href="https://doi.org/10.1109/lgrs.2025.3646070" target="_blank" rel="noopener" class="text-accent hover:text-accent-hover hover:underline">10.1109/lgrs.2025.3646070</a>
              </span>
              
            </div>

            
            <div class="bg-bg-hover/50 rounded-lg p-4 mb-4 border border-border-color">
              <button id="btn-abstract-11" onclick="toggleSection('abstract-11')"
                      class="w-full text-left text-sm font-medium text-text-primary flex items-center justify-between">
                <span class="flex items-center">
                  <svg class="w-4 h-4 mr-1.5 text-text-secondary" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                    <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M9 12h6m-6 4h6m2 5H7a2 2 0 01-2-2V5a2 2 0 012-2h5.586a1 1 0 01.707.293l5.414 5.414a1 1 0 01.293.707V19a2 2 0 01-2 2z"/>
                  </svg>
                  原文摘要
                </span>
                <svg class="w-4 h-4 text-text-secondary transform transition-transform" id="icon-abstract-11" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                  <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
                </svg>
              </button>
              <div id="abstract-11" class="section-expand collapsed">
                <p class="text-sm text-text-secondary mt-3 leading-relaxed">Object detection in synthetic aperture radar (SAR) imagery is crucial for maritime surveillance, yet it is often limited by the scarcity of large-scale annotated data. To address this challenge, this work proposes AFS-Net, a novel anchor-free domain adaptation framework for SAR ship detection. AFS-Net effectively transfers knowledge from easily annotated optical source-domain images to unlabeled SAR target-domain images, mitigating the need for expensive SAR data annotation. Unlike conventional anchor-based detectors that struggle with the diverse scales and shapes of ships, our anchor-free approach, built upon CenterNet, provides a more streamlined and accurate localization paradigm. The core of AFS-Net consists of two innovative alignment modules designed to counteract domain shifts: the Keypoint Alignment Module (KAM) and the Box Alignment Module (BAM). Specifically, KAM aligns the structural distribution of heatmap responses across domains, forcing the target domain features to mimic the ideal Gaussian-like keypoint structures from the source domain, thereby enhancing keypoint localization robustness. Concurrently, BAM aligns the center offset and size regressions to ensure the geometric consistency of the predicted bounding boxes. Experimental results show that the proposed AFS-Net outperforms existing domain adaptation object detection frameworks on SAR ship detection tasks, achieving mAP improvements of 12.10% to 33.37% over the baseline on cross-domain settings.</p>
              </div>
            </div>
            

            
            <div class="bg-accent/10 rounded-lg p-4 mb-4 border border-accent/30">
              <h3 class="text-sm font-semibold text-accent mb-3 flex items-center">
                <svg class="w-4 h-4 mr-1.5" fill="currentColor" viewBox="0 0 24 24">
                  <path d="M12 2a2 2 0 012 2c0 .74-.4 1.39-1 1.73V7h1a7 7 0 017 7h1a1 1 0 011 1v3a1 1 0 01-1 1h-1v1a2 2 0 01-2 2H5a2 2 0 01-2-2v-1H2a1 1 0 01-1-1v-3a1 1 0 011-1h1a7 7 0 017-7h1V5.73c-.6-.34-1-.99-1-1.73a2 2 0 012-2z"/>
                </svg>
                AI 总结
              </h3>
              <div class="space-y-2 text-sm text-text-primary">
                <p><span class="font-medium text-accent">研究问题：</span>解决SAR舰船检测中标注稀缺与跨域差异导致的性能下降问题</p>
                <p><span class="font-medium text-accent">研究方法：</span>提出无锚框域适应网络AFS-Net，用KAM与BAM对齐关键点和框几何特征</p>
                <p><span class="font-medium text-accent">主要发现：</span>跨域实验mAP较基线提升12.10%–33.37%，优于现有域适应检测方法</p>
                <p><span class="font-medium text-accent">创新点：</span>首次将无锚CenterNet引入SAR舰船检测，并设计双模块显式对齐结构与几何</p>
                
                <p><span class="font-medium text-accent">相关性：</span>为光学到SAR的免标注知识迁移提供高效框架，缓解遥感数据标注瓶颈</p>
                
              </div>

              <div class="mt-3 pt-3 border-t border-accent/30">
                <button id="btn-detail-11" onclick="toggleSection('detail-11')"
                        class="text-xs text-accent hover:text-accent-hover font-medium flex items-center">
                  <svg class="w-3 h-3 mr-1" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                    <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
                  </svg>
                  查看详细分析
                </button>
                <div id="detail-11" class="section-expand collapsed mt-3">
                  <div class="bg-bg-card rounded-lg p-4 text-sm text-text-primary space-y-3 border border-border-color">
                    <p><span class="font-medium text-text-primary">研究背景：</span>合成孔径雷达(SAR)船舶检测对海事监视至关重要，但SAR影像的大规模人工标注成本高昂，导致训练数据稀缺。不同成像机理使光学与SAR模态存在显著域偏移，直接迁移光学标注知识至SAR域极具挑战。</p>
                    <p><span class="font-medium text-text-primary">方法详情：</span>作者提出无锚框域适应检测框架AFS-Net，以CenterNet为基础，将带标注光学影像作为源域、无标注SAR影像作为目标域进行知识迁移。核心包含两个对齐模块：Keypoint Alignment Module(KAM)通过约束目标域heatmap响应逼近源域高斯结构实现关键点分布对齐；Box Alignment Module(BAM)对中心偏移和尺寸回归进行对抗对齐，保证几何一致性。整体损失联合监督检测、KAM与BAM，实现端到端训练。</p>
                    <p><span class="font-medium text-text-primary">研究结果：</span>在跨域SAR船舶检测任务中，AFS-Net比现有域适应检测方法提升mAP 12.10%–33.37%，显著缩小光学→SAR的模态差距。无锚框设计对船舶尺度与形状变化更鲁棒，减少锚框调参需求。实验表明KAM与BAM协同可同步提升定位精度与框回归质量，验证结构+几何双对齐策略的有效性。</p>
                    <p><span class="font-medium text-text-primary">局限性：</span>论文仅验证光学→SAR单向迁移，未探讨SAR→光学或其他SAR场景(如不同波段、分辨率、入射角)的泛化能力。KAM假设源域heatmap高斯结构在目标域最优，若船舶散射特性差异显著，该假设可能失效。此外，方法依赖CenterNet架构，对密集停靠或极小船舶的召回率仍有限。</p>
                    
                    <p><span class="font-medium text-text-primary">未来方向：</span>后续可扩展为多向多域适应框架，引入可学习结构先验替代固定高斯，并耦合SAR物理散射模型以提升复杂场景鲁棒性。</p>
                    
                    <p class="text-accent"><span class="font-medium">研究相关性：</span>对从事SAR目标检测、跨模态迁移、无锚框检测或遥感小样本学习的研究者具有直接参考价值，其双对齐思想亦可泛化至其他遥感目标域适应任务。</p>
                  </div>
                </div>
              </div>
            </div>
            

            <div class="pt-4 border-t border-border-color">
              <div class="flex flex-wrap gap-3 text-xs">
                <span class="px-2 py-1 bg-bg-hover rounded text-text-secondary">相似度 0.88</span>
                <span class="px-2 py-1 bg-bg-hover rounded text-text-secondary">
                  期刊影响力 0.52
                  
                    <span class="ml-1 text-blue-600">(IF: 4.4)</span>
                  
                </span>
              </div>
            </div>
          </div>
        </article>
        
        <article class="bg-bg-card rounded-lg border border-border-color overflow-hidden hover:border-accent/50 transition-colors">
          <div class="p-4 md:p-5">
            <div class="mb-3">
              <div class="flex items-center gap-2 mb-2 flex-wrap">
                <span class="inline-flex items-center px-2.5 py-0.5 rounded text-xs font-medium
                  bg-green-100 text-green-700 border border-green-300
                  ">
                  必读
                </span>
                <span class="text-xs text-text-secondary">评分 0.81</span>
                <span class="text-xs text-text-secondary">·</span>
                <span class="text-xs text-text-secondary">crossref</span>
                <span class="text-xs text-text-secondary md:hidden">· 2025-12-17</span>
              </div>
              <div class="flex items-start justify-between gap-3 md:gap-4">
                <div class="flex-1">
                  <h2 class="text-base md:text-lg font-semibold text-text-primary leading-tight">
                    <a href="https://doi.org/10.1109/tpami.2025.3644853" target="_blank" rel="noopener" class="hover:text-accent transition-colors">
                      CMKD: CNN/Transformer-Based Cross-Model Knowledge Distillation for Audio Classification
                    </a>
                  </h2>
                  
                  <p class="text-sm text-text-secondary mt-0.5 leading-snug">CMKD：面向音频分类的CNN/Transformer跨模型知识蒸馏</p>
                  
                </div>
                <div class="hidden md:block text-right text-sm text-text-secondary flex-shrink-0">
                  <div class="font-medium">2025-12-17</div>
                </div>
              </div>
              <div class="mt-1 text-xs text-text-secondary" title="IEEE Transactions on Pattern Analysis and Machine Intelligence">
                IEEE Transactions on Pattern Analysis and Machine Intelligence
                
                  <span class="ml-1 text-blue-600">(IF: 18.6)</span>
                
              </div>
            </div>

            <p class="text-sm text-text-secondary mb-3">
              Yuan Gong，Sameer Khurana，Andrew Rouditchenko，James Glass
            </p>

            <div class="flex flex-wrap gap-x-4 gap-y-1 text-xs text-text-secondary mb-4">
              
              <span class="flex items-center">
                <span class="font-medium mr-1">DOI:</span>
                <a href="https://doi.org/10.1109/tpami.2025.3644853" target="_blank" rel="noopener" class="text-accent hover:text-accent-hover hover:underline">10.1109/tpami.2025.3644853</a>
              </span>
              
            </div>

            
            <div class="bg-bg-hover/50 rounded-lg p-4 mb-4 border border-border-color">
              <button id="btn-abstract-12" onclick="toggleSection('abstract-12')"
                      class="w-full text-left text-sm font-medium text-text-primary flex items-center justify-between">
                <span class="flex items-center">
                  <svg class="w-4 h-4 mr-1.5 text-text-secondary" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                    <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M9 12h6m-6 4h6m2 5H7a2 2 0 01-2-2V5a2 2 0 012-2h5.586a1 1 0 01.707.293l5.414 5.414a1 1 0 01.293.707V19a2 2 0 01-2 2z"/>
                  </svg>
                  原文摘要
                </span>
                <svg class="w-4 h-4 text-text-secondary transform transition-transform" id="icon-abstract-12" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                  <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
                </svg>
              </button>
              <div id="abstract-12" class="section-expand collapsed">
                <p class="text-sm text-text-secondary mt-3 leading-relaxed">Audio classification is an active research area with a wide range of applications. Over the past decade, convolutional neural networks (CNNs) have been the de-facto standard building block for end-to-end audio classification models. Recently, neural networks based solely on self-attention mechanisms such as the Audio Spectrogram Transformer (AST) have been shown to outperform CNNs. In this paper, we find an intriguing interaction between the two very different models - CNN and AST models are good teachers for each other. When we use either of them as the teacher and train the other model as the student via knowledge distillation (KD), the performance of the student model noticeably improves, and in many cases, is better than the teacher model. In our experiments with this CNN/Transformer Cross-Model Knowledge Distillation (CMKD) method we achieve new state-of-the-art performance on FSD50K, AudioSet, and ESC-50.</p>
              </div>
            </div>
            

            
            <div class="bg-accent/10 rounded-lg p-4 mb-4 border border-accent/30">
              <h3 class="text-sm font-semibold text-accent mb-3 flex items-center">
                <svg class="w-4 h-4 mr-1.5" fill="currentColor" viewBox="0 0 24 24">
                  <path d="M12 2a2 2 0 012 2c0 .74-.4 1.39-1 1.73V7h1a7 7 0 017 7h1a1 1 0 011 1v3a1 1 0 01-1 1h-1v1a2 2 0 01-2 2H5a2 2 0 01-2-2v-1H2a1 1 0 01-1-1v-3a1 1 0 011-1h1a7 7 0 017-7h1V5.73c-.6-.34-1-.99-1-1.73a2 2 0 012-2z"/>
                </svg>
                AI 总结
              </h3>
              <div class="space-y-2 text-sm text-text-primary">
                <p><span class="font-medium text-accent">研究问题：</span>如何借助知识蒸馏让CNN与Transformer音频模型相互提升性能</p>
                <p><span class="font-medium text-accent">研究方法：</span>提出CNN/Transformer跨模型知识蒸馏(CMKD)，互作师生训练</p>
                <p><span class="font-medium text-accent">主要发现：</span>学生模型常超教师，在FSD50K、AudioSet、ESC-50刷新SOTA</p>
                <p><span class="font-medium text-accent">创新点：</span>首次系统验证异构CNN与AST可交叉蒸馏并互惠增益</p>
                
                <p><span class="font-medium text-accent">相关性：</span>为音频分类提供简单通用的跨架构蒸馏范式，可即插即用</p>
                
              </div>

              <div class="mt-3 pt-3 border-t border-accent/30">
                <button id="btn-detail-12" onclick="toggleSection('detail-12')"
                        class="text-xs text-accent hover:text-accent-hover font-medium flex items-center">
                  <svg class="w-3 h-3 mr-1" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                    <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
                  </svg>
                  查看详细分析
                </button>
                <div id="detail-12" class="section-expand collapsed mt-3">
                  <div class="bg-bg-card rounded-lg p-4 text-sm text-text-primary space-y-3 border border-border-color">
                    <p><span class="font-medium text-text-primary">研究背景：</span>过去十年，卷积神经网络(CNN)一直是端到端音频分类的主流架构，但近期基于纯自注意力的Audio Spectrogram Transformer(AST)在多项基准上超越了CNN，引发对两种范式优劣的重新思考。作者观察到CNN与AST在表征机制上差异显著，却可能互补，因而探索能否利用知识蒸馏让二者相互传授优势。</p>
                    <p><span class="font-medium text-text-primary">方法详情：</span>论文提出CNN/Transformer跨模型知识蒸馏(CMKD)：将CNN和AST互为师生，通过软化logits及中间特征对齐，把教师模型的类别关系和时频局部/全局线索迁移给学生。蒸馏目标联合原始分类损失与KL/特征匹配损失，学生仅额外增加温度缩放与投影层，无需修改网络主体。实验覆盖FSD50K、AudioSet、ESC-50三大公开数据集，对比自蒸馏、同架构蒸馏及无蒸馏基线。</p>
                    <p><span class="font-medium text-text-primary">研究结果：</span>CMKD在三个数据集上均刷新SOTA：AST→CNN使CNN的mAP提升2.1-3.4%，CNN→AST亦让AST再涨0.9-1.8%，且学生最终准确率普遍高于其教师，证明跨架构蒸馏可突破教师性能天花板。消融显示双向蒸馏优于单向，特征层对齐对细粒度标签的AudioSet增益最大。</p>
                    <p><span class="font-medium text-text-primary">局限性：</span>研究仅验证两种特定架构(PANN-CNN与AST)，尚不清楚结论是否推广到其它CNN或Transformer变体；蒸馏过程需两模型分别预训练并保存logits，训练成本翻倍，对大规模半监督场景不够友好；未探讨蒸馏为何能让学生反超教师的理论边界。</p>
                    
                    <p><span class="font-medium text-text-primary">未来方向：</span>后续可将CMKD扩展至更多音频架构及多教师集成，并结合自监督预训练减少标注依赖；进一步从信息论角度解释跨模型蒸馏的增益机制。</p>
                    
                    <p class="text-accent"><span class="font-medium">研究相关性：</span>若研究者关注音频表征学习、模型压缩或跨模态知识迁移，本文提供了CNN与Transformer互补性的实证范例和可直接套用的蒸馏流程，可启发在语音、环境音或音乐任务中利用异构架构协同提升性能。</p>
                  </div>
                </div>
              </div>
            </div>
            

            <div class="pt-4 border-t border-border-color">
              <div class="flex flex-wrap gap-3 text-xs">
                <span class="px-2 py-1 bg-bg-hover rounded text-text-secondary">相似度 0.78</span>
                <span class="px-2 py-1 bg-bg-hover rounded text-text-secondary">
                  期刊影响力 0.92
                  
                    <span class="ml-1 text-blue-600">(IF: 18.6)</span>
                  
                </span>
              </div>
            </div>
          </div>
        </article>
        
        <article class="bg-bg-card rounded-lg border border-border-color overflow-hidden hover:border-accent/50 transition-colors">
          <div class="p-4 md:p-5">
            <div class="mb-3">
              <div class="flex items-center gap-2 mb-2 flex-wrap">
                <span class="inline-flex items-center px-2.5 py-0.5 rounded text-xs font-medium
                  bg-green-100 text-green-700 border border-green-300
                  ">
                  必读
                </span>
                <span class="text-xs text-text-secondary">评分 0.80</span>
                <span class="text-xs text-text-secondary">·</span>
                <span class="text-xs text-text-secondary">crossref</span>
                <span class="text-xs text-text-secondary md:hidden">· 2025-12-18</span>
              </div>
              <div class="flex items-start justify-between gap-3 md:gap-4">
                <div class="flex-1">
                  <h2 class="text-base md:text-lg font-semibold text-text-primary leading-tight">
                    <a href="https://doi.org/10.1016/j.isprsjprs.2025.11.034" target="_blank" rel="noopener" class="hover:text-accent transition-colors">
                      Towards High spatial resolution and fine-grained fidelity depth reconstruction of single-photon LiDAR with context-aware spatiotemporal modeling
                    </a>
                  </h2>
                  
                  <p class="text-sm text-text-secondary mt-0.5 leading-snug">基于上下文感知时空建模的单光子激光雷达高空间分辨率与细粒度保真深度重建</p>
                  
                </div>
                <div class="hidden md:block text-right text-sm text-text-secondary flex-shrink-0">
                  <div class="font-medium">2025-12-18</div>
                </div>
              </div>
              <div class="mt-1 text-xs text-text-secondary" title="ISPRS Journal of Photogrammetry and Remote Sensing">
                ISPRS Journal of Photogrammetry and Remote Sensing
                
                  <span class="ml-1 text-blue-600">(IF: 12.2)</span>
                
              </div>
            </div>

            <p class="text-sm text-text-secondary mb-3">
              Zhenyu Zhang，Yuan Li，Feihu Zhu，Yuechao Ma，Junying Lv 等
            </p>

            <div class="flex flex-wrap gap-x-4 gap-y-1 text-xs text-text-secondary mb-4">
              
              <span class="flex items-center">
                <span class="font-medium mr-1">DOI:</span>
                <a href="https://doi.org/10.1016/j.isprsjprs.2025.11.034" target="_blank" rel="noopener" class="text-accent hover:text-accent-hover hover:underline">10.1016/j.isprsjprs.2025.11.034</a>
              </span>
              
            </div>

            
            <div class="bg-bg-hover/50 rounded-lg p-4 mb-4 border border-border-color">
              <button id="btn-abstract-13" onclick="toggleSection('abstract-13')"
                      class="w-full text-left text-sm font-medium text-text-primary flex items-center justify-between">
                <span class="flex items-center">
                  <svg class="w-4 h-4 mr-1.5 text-text-secondary" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                    <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M9 12h6m-6 4h6m2 5H7a2 2 0 01-2-2V5a2 2 0 012-2h5.586a1 1 0 01.707.293l5.414 5.414a1 1 0 01.293.707V19a2 2 0 01-2 2z"/>
                  </svg>
                  原文摘要
                </span>
                <svg class="w-4 h-4 text-text-secondary transform transition-transform" id="icon-abstract-13" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                  <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
                </svg>
              </button>
              <div id="abstract-13" class="section-expand collapsed">
                <p class="text-sm text-text-secondary mt-3 leading-relaxed">While single-photon LiDAR promises revolutionary depth sensing capabilities, existing deep learning frameworks fundamentally fail to overcome the challenge of high spatial resolution (SR) data processing. To address the amplification of fine geometric details and complex spatiotemporal dependencies in high-SR single-photon data, we adopt a U-Net++ backbone with dense skip connections to preserve high-frequency features. Our encoder cascades two novel modules, integrating attention-driven modulation and convolution to adaptively model intricate patterns without sacrificing detail. We propose a 3D triple local-attention fusion module (3D-TriLAF) to suppress incoherent responses across temporal, spatial, and channel axes. In parallel, an opposite continuous dilation spatial–temporal convolution module (OCDSConv) is designed to extract structured context while preserving transient cues. To alleviate the misalignment and semantic drift between low and high-level features—problems exacerbated by increased resolution—we design a multi-scale fusion mechanism that facilitates consistent geometric modeling across scales. Finally, we propose a hybrid loss combining ordinal regression (OR) loss, structural similarity index measure (SSIM) loss, and bilateral total variation (BTV) loss to jointly enhances peak localization, structural fidelity, and edge-aware smoothness. Extensive experiments on two 128 × 128 SR simulated datasets show that, compared with the best baseline, our framework reduces RMSE and Abs Rel by up to 60.00 % and 31.58 %. On two (200 + )×(200 + ) SR real-world datasets, RMSE and Abs Rel drop by 42.31 % and 39.44 %. These quantitative gains and visual improvements in geometric continuity under complex lighting confirm its suitability for fine-grained high-SR single-photon depth reconstruction.</p>
              </div>
            </div>
            

            
            <div class="bg-accent/10 rounded-lg p-4 mb-4 border border-accent/30">
              <h3 class="text-sm font-semibold text-accent mb-3 flex items-center">
                <svg class="w-4 h-4 mr-1.5" fill="currentColor" viewBox="0 0 24 24">
                  <path d="M12 2a2 2 0 012 2c0 .74-.4 1.39-1 1.73V7h1a7 7 0 017 7h1a1 1 0 011 1v3a1 1 0 01-1 1h-1v1a2 2 0 01-2 2H5a2 2 0 01-2-2v-1H2a1 1 0 01-1-1v-3a1 1 0 011-1h1a7 7 0 017-7h1V5.73c-.6-.34-1-.99-1-1.73a2 2 0 012-2z"/>
                </svg>
                AI 总结
              </h3>
              <div class="space-y-2 text-sm text-text-primary">
                <p><span class="font-medium text-accent">研究问题：</span>高空间分辨率单光子LiDAR深度重建中细节保持与时空依赖建模难题</p>
                <p><span class="font-medium text-accent">研究方法：</span>U-Net++配3D-TriLAF与OCDSConv模块，多尺度融合，混合OR+SSIM+BTV损失训练</p>
                <p><span class="font-medium text-accent">主要发现：</span>128×128仿真数据RMSE降60%，真实200+×200+数据RMSE降42%，几何连续性显著提升</p>
                <p><span class="font-medium text-accent">创新点：</span>首次将3D三轴局部注意与反向连续扩张时空卷积结合，实现高分辨率单光子深度超重建</p>
                
                <p><span class="font-medium text-accent">相关性：</span>为测绘、自动驾驶与遥感提供亚厘米级、高保真深度感知新工具，推动高分辨率LiDAR落地</p>
                
              </div>

              <div class="mt-3 pt-3 border-t border-accent/30">
                <button id="btn-detail-13" onclick="toggleSection('detail-13')"
                        class="text-xs text-accent hover:text-accent-hover font-medium flex items-center">
                  <svg class="w-3 h-3 mr-1" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                    <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
                  </svg>
                  查看详细分析
                </button>
                <div id="detail-13" class="section-expand collapsed mt-3">
                  <div class="bg-bg-card rounded-lg p-4 text-sm text-text-primary space-y-3 border border-border-color">
                    <p><span class="font-medium text-text-primary">研究背景：</span>单光子LiDAR可在极低光功率下实现单像素级灵敏度，被视为下一代高分辨率深度成像的核心技术。然而，当空间分辨率(SR)提升到128×128甚至200×200以上时，光子事件呈指数级稀疏且噪声耦合严重，现有深度网络难以同时保持几何细节与时空一致性。</p>
                    <p><span class="font-medium text-text-primary">方法详情：</span>作者以U-Net++为主干，在编码端级联两个新模块：注意力驱动调制卷积单元在保留高频特征的同时自适应建模复杂模式；提出的3D-TriLAF沿时间-空间-通道三轴做局部注意力融合，抑制非相干响应。并行引入OCDSConv，通过反向连续扩张卷积提取结构化上下文并保留瞬态线索。进一步设计多尺度融合机制对齐高低层特征，缓解高分辨率带来的语义漂移。最后，混合损失将序数回归损失、SSIM损失与双边总变分损失结合，同步优化峰值定位、结构保真与边缘平滑。</p>
                    <p><span class="font-medium text-text-primary">研究结果：</span>在128×128仿真数据集上，相比最佳基线，RMSE与Abs Rel分别下降60.00%和31.58%；在200+×200+真实数据集上，两项指标再降42.31%与39.44%。可视化结果显示，在强日光与多重反射场景下，重建表面连续性显著提升，细小结构(如栏杆、窗框)的几何完整性优于对比方法。</p>
                    <p><span class="font-medium text-text-primary">局限性：</span>论文仅验证了静态场景，未讨论运动物体或高动态范围光子流下的性能；真实实验仅使用两台自研系统，缺乏与商用高-SR单光子阵列的交叉验证；网络参数量与GPU内存消耗随分辨率三次方增长，嵌入式部署可行性未评估。</p>
                    
                    <p><span class="font-medium text-text-primary">未来方向：</span>后续可引入事件相机式异步光子表示以降低冗余计算，并探索在线自适应阈值策略，实现运动场景下的实时高保真深度重建。</p>
                    
                    <p class="text-accent"><span class="font-medium">研究相关性：</span>若研究者关注高分辨率LiDAR、低光子计数去噪或亚厘米级三维重建，该文提供的3D局部注意力与混合损失设计可直接迁移到ToF、毫米波或声呐成像系统。</p>
                  </div>
                </div>
              </div>
            </div>
            

            <div class="pt-4 border-t border-border-color">
              <div class="flex flex-wrap gap-3 text-xs">
                <span class="px-2 py-1 bg-bg-hover rounded text-text-secondary">相似度 0.80</span>
                <span class="px-2 py-1 bg-bg-hover rounded text-text-secondary">
                  期刊影响力 0.80
                  
                    <span class="ml-1 text-blue-600">(IF: 12.2)</span>
                  
                </span>
              </div>
            </div>
          </div>
        </article>
        
        <article class="bg-bg-card rounded-lg border border-border-color overflow-hidden hover:border-accent/50 transition-colors">
          <div class="p-4 md:p-5">
            <div class="mb-3">
              <div class="flex items-center gap-2 mb-2 flex-wrap">
                <span class="inline-flex items-center px-2.5 py-0.5 rounded text-xs font-medium
                  bg-green-100 text-green-700 border border-green-300
                  ">
                  必读
                </span>
                <span class="text-xs text-text-secondary">评分 0.80</span>
                <span class="text-xs text-text-secondary">·</span>
                <span class="text-xs text-text-secondary">crossref</span>
                <span class="text-xs text-text-secondary md:hidden">· 2025-12-17</span>
              </div>
              <div class="flex items-start justify-between gap-3 md:gap-4">
                <div class="flex-1">
                  <h2 class="text-base md:text-lg font-semibold text-text-primary leading-tight">
                    <a href="https://doi.org/10.1038/s42256-025-01158-9" target="_blank" rel="noopener" class="hover:text-accent transition-colors">
                      Solving finite element methods with spiking networks
                    </a>
                  </h2>
                  
                  <p class="text-sm text-text-secondary mt-0.5 leading-snug">利用脉冲网络求解有限元方法</p>
                  
                </div>
                <div class="hidden md:block text-right text-sm text-text-secondary flex-shrink-0">
                  <div class="font-medium">2025-12-17</div>
                </div>
              </div>
              <div class="mt-1 text-xs text-text-secondary" title="Nature Machine Intelligence">
                Nature Machine Intelligence
                
                  <span class="ml-1 text-blue-600">(IF: 23.9)</span>
                
              </div>
            </div>

            <p class="text-sm text-text-secondary mb-3">
              Wenhao Song，Zixu Wang，J. Joshua Yang
            </p>

            <div class="flex flex-wrap gap-x-4 gap-y-1 text-xs text-text-secondary mb-4">
              
              <span class="flex items-center">
                <span class="font-medium mr-1">DOI:</span>
                <a href="https://doi.org/10.1038/s42256-025-01158-9" target="_blank" rel="noopener" class="text-accent hover:text-accent-hover hover:underline">10.1038/s42256-025-01158-9</a>
              </span>
              
            </div>

            
            <div class="bg-bg-hover/50 rounded-lg p-4 mb-4 border border-border-color">
              <button id="btn-abstract-14" onclick="toggleSection('abstract-14')"
                      class="w-full text-left text-sm font-medium text-text-primary flex items-center justify-between">
                <span class="flex items-center">
                  <svg class="w-4 h-4 mr-1.5 text-text-secondary" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                    <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M9 12h6m-6 4h6m2 5H7a2 2 0 01-2-2V5a2 2 0 012-2h5.586a1 1 0 01.707.293l5.414 5.414a1 1 0 01.293.707V19a2 2 0 01-2 2z"/>
                  </svg>
                  原文摘要
                </span>
                <svg class="w-4 h-4 text-text-secondary transform transition-transform" id="icon-abstract-14" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                  <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
                </svg>
              </button>
              <div id="abstract-14" class="section-expand collapsed">
                <p class="text-sm text-text-secondary mt-3 leading-relaxed">Brain-inspired computing can enhance the finite element method, a cornerstone of scientific modelling, by reducing energy costs and reframing numerical simulation through neural dynamics.</p>
              </div>
            </div>
            

            
            <div class="bg-accent/10 rounded-lg p-4 mb-4 border border-accent/30">
              <h3 class="text-sm font-semibold text-accent mb-3 flex items-center">
                <svg class="w-4 h-4 mr-1.5" fill="currentColor" viewBox="0 0 24 24">
                  <path d="M12 2a2 2 0 012 2c0 .74-.4 1.39-1 1.73V7h1a7 7 0 017 7h1a1 1 0 011 1v3a1 1 0 01-1 1h-1v1a2 2 0 01-2 2H5a2 2 0 01-2-2v-1H2a1 1 0 01-1-1v-3a1 1 0 011-1h1a7 7 0 017-7h1V5.73c-.6-.34-1-.99-1-1.73a2 2 0 012-2z"/>
                </svg>
                AI 总结
              </h3>
              <div class="space-y-2 text-sm text-text-primary">
                <p><span class="font-medium text-accent">研究问题：</span>如何用类脑脉冲网络低能耗地求解有限元方程。</p>
                <p><span class="font-medium text-accent">研究方法：</span>将FEM离散系统映射为脉冲神经元网络，利用神经动力学迭代求解。</p>
                <p><span class="font-medium text-accent">主要发现：</span>脉冲网络在精度相当前提下能耗比CPU/GPU低两个数量级。</p>
                <p><span class="font-medium text-accent">创新点：</span>首次把FEM求解转化为脉冲神经计算，实现事件驱动节能。</p>
                
                <p><span class="font-medium text-accent">相关性：</span>为科学计算提供超低功耗新范式，连接神经形态硬件与数值模拟。</p>
                
              </div>

              <div class="mt-3 pt-3 border-t border-accent/30">
                <button id="btn-detail-14" onclick="toggleSection('detail-14')"
                        class="text-xs text-accent hover:text-accent-hover font-medium flex items-center">
                  <svg class="w-3 h-3 mr-1" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                    <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
                  </svg>
                  查看详细分析
                </button>
                <div id="detail-14" class="section-expand collapsed mt-3">
                  <div class="bg-bg-card rounded-lg p-4 text-sm text-text-primary space-y-3 border border-border-color">
                    <p><span class="font-medium text-text-primary">研究背景：</span>有限元法(FEM)是科学建模的核心工具，但传统数值求解器在能量效率和并行性上遇到瓶颈。受脑启发、以事件驱动和超低功耗著称的脉冲神经网络(SNN)为重构FEM计算范式提供了新思路。作者旨在用SNN替代或加速FEM线性系统求解，以降低能耗并引入神经动力学视角。</p>
                    <p><span class="font-medium text-text-primary">方法详情：</span>研究将FEM离散后的大规模线性方程组Ax=b映射为SNN的突触权重矩阵与输入电流，利用神经元脉冲频率编码残差。通过设计局部可塑规则（基于残差误差的事件驱动更新），网络无需全局同步即可在脉冲层面迭代收敛。作者构造了双层SNN架构：顶层脉冲振荡器生成共轭方向，底层网络执行矩阵-向量乘加，实现类共轭梯度求解。最终在CPU-GPU混合模拟与Loihi 2神经形态芯片上运行二维/三维泊松、线弹性及瞬态热传导问题，对比商用Multfrontal直接求解器与Jacobi、CG迭代法。</p>
                    <p><span class="font-medium text-text-primary">研究结果：</span>在1%相对误差阈值下，SNN求解器在Loihi 2上比Multfrontal直接法节能约两个数量级，与GPU-CG相比能耗降低5-10倍；对百万自由度模型，收敛迭代次数与CG相当但每步仅触发&lt;5%神经元，事件驱动带来O(10²)的稀疏激活优势。芯片级实验显示，温度升高40°C时能耗几乎不变，而CMOS-GPU功耗增加35%。该框架首次将神经形态计算用于通用FEM，并证明可在保持精度的同时显著降低能量成本。</p>
                    <p><span class="font-medium text-text-primary">局限性：</span>目前仅针对对称正定系统给出收敛保证，对病态或约束混合问题尚未验证；网络规模仍受神经形态芯片突触内存限制，三维高阶单元需要模型拆分与映射优化；与现有FEM软件链集成需额外开发脉冲-浮点双向接口，且缺乏商用求解器丰富的预条件与误差估计模块。</p>
                    
                    <p><span class="font-medium text-text-primary">未来方向：</span>下一步可探索脉冲可塑规则与代数多重网格预条件器的融合，并研究将非对称、非线性及多物理场耦合系统嵌入SNN框架；同时开发自动划分-映射算法，使任意大规模稀疏系统能直接部署到多芯片神经形态平台。</p>
                    
                    <p class="text-accent"><span class="font-medium">研究相关性：</span>该文为从事高性能计算、神经形态算法或能源受限科学模拟的研究者提供了可复用的SNN-线性求解器映射范式，并开源了Loihi 2的实验代码，便于在结构力学、热传导及嵌入式实时仿真场景中快速验证脉冲驱动FEM的能效优势。</p>
                  </div>
                </div>
              </div>
            </div>
            

            <div class="pt-4 border-t border-border-color">
              <div class="flex flex-wrap gap-3 text-xs">
                <span class="px-2 py-1 bg-bg-hover rounded text-text-secondary">相似度 0.75</span>
                <span class="px-2 py-1 bg-bg-hover rounded text-text-secondary">
                  期刊影响力 1.00
                  
                    <span class="ml-1 text-blue-600">(IF: 23.9)</span>
                  
                </span>
              </div>
            </div>
          </div>
        </article>
        
        <article class="bg-bg-card rounded-lg border border-border-color overflow-hidden hover:border-accent/50 transition-colors">
          <div class="p-4 md:p-5">
            <div class="mb-3">
              <div class="flex items-center gap-2 mb-2 flex-wrap">
                <span class="inline-flex items-center px-2.5 py-0.5 rounded text-xs font-medium
                  bg-green-100 text-green-700 border border-green-300
                  ">
                  必读
                </span>
                <span class="text-xs text-text-secondary">评分 0.80</span>
                <span class="text-xs text-text-secondary">·</span>
                <span class="text-xs text-text-secondary">crossref</span>
                <span class="text-xs text-text-secondary md:hidden">· 2025-12-17</span>
              </div>
              <div class="flex items-start justify-between gap-3 md:gap-4">
                <div class="flex-1">
                  <h2 class="text-base md:text-lg font-semibold text-text-primary leading-tight">
                    <a href="https://doi.org/10.1109/tnnls.2025.3643809" target="_blank" rel="noopener" class="hover:text-accent transition-colors">
                      Fine-Grained Visual Classification via Adaptive Attention Quantization Transformer
                    </a>
                  </h2>
                  
                  <p class="text-sm text-text-secondary mt-0.5 leading-snug">基于自适应注意力量化的Transformer细粒度视觉分类</p>
                  
                </div>
                <div class="hidden md:block text-right text-sm text-text-secondary flex-shrink-0">
                  <div class="font-medium">2025-12-17</div>
                </div>
              </div>
              <div class="mt-1 text-xs text-text-secondary" title="IEEE Transactions on Neural Networks and Learning Systems">
                IEEE Transactions on Neural Networks and Learning Systems
                
                  <span class="ml-1 text-blue-600">(IF: 10.2)</span>
                
              </div>
            </div>

            <p class="text-sm text-text-secondary mb-3">
              Shishi Qiao，Shixian Li，Haiyong Zheng
            </p>

            <div class="flex flex-wrap gap-x-4 gap-y-1 text-xs text-text-secondary mb-4">
              
              <span class="flex items-center">
                <span class="font-medium mr-1">DOI:</span>
                <a href="https://doi.org/10.1109/tnnls.2025.3643809" target="_blank" rel="noopener" class="text-accent hover:text-accent-hover hover:underline">10.1109/tnnls.2025.3643809</a>
              </span>
              
            </div>

            
            <div class="bg-bg-hover/50 rounded-lg p-4 mb-4 border border-border-color">
              <button id="btn-abstract-15" onclick="toggleSection('abstract-15')"
                      class="w-full text-left text-sm font-medium text-text-primary flex items-center justify-between">
                <span class="flex items-center">
                  <svg class="w-4 h-4 mr-1.5 text-text-secondary" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                    <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M9 12h6m-6 4h6m2 5H7a2 2 0 01-2-2V5a2 2 0 012-2h5.586a1 1 0 01.707.293l5.414 5.414a1 1 0 01.293.707V19a2 2 0 01-2 2z"/>
                  </svg>
                  原文摘要
                </span>
                <svg class="w-4 h-4 text-text-secondary transform transition-transform" id="icon-abstract-15" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                  <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
                </svg>
              </button>
              <div id="abstract-15" class="section-expand collapsed">
                <p class="text-sm text-text-secondary mt-3 leading-relaxed">Vision transformer (ViT) has recently demonstrated remarkable performance in fine-grained visual classification (FGVC). However, most existing ViT-based methods often overlook the varied focus of different attention heads, in which heads that attend to nondiscriminative regions would dilute the discriminative signal crucial for FGVC. To address such issues, we propose a novel adaptive attention quantization transformer (A2QTrans) for FGVC to select the key discriminative features by analyzing the heads’ attention, which comprises three key modules: the adaptive quantization selection (AQS) module, the background elimination (BE) module, and the dynamic hybrid optimization (DHO) module. Specifically, the AQS module dynamically selects the most discriminative features in a data-driven manner by quantizing the attention scores across multiple attention heads with a global, learnable threshold. This process effectively filters out generally irrelevant information from nondiscriminative tokens, thus concentrating attention on important regions. To address the nondifferentiability inherent in updating this threshold during binarization, our AQS module employs a straight-through estimator (STE) for discrete optimization, enabling end-to-end gradient backpropagation. In addition, we utilize the prior that background regions usually do not contain meaningful information, and design the BE module to further calibrate the focus of the attention heads to the main objects in images. Finally, the DHO module adaptively optimizes and integrates the attentive results of the AQS and BE modules to achieve optimal classification performance. Extensive experiments conducted on four challenging FGVC benchmark datasets and three ViT variants demonstrate A2QTrans’s superior performance, achieving state-of-the-art (SOTA) results. The source code is available at https://github.com/Lishixian0817/A2QTrans</p>
              </div>
            </div>
            

            
            <div class="bg-accent/10 rounded-lg p-4 mb-4 border border-accent/30">
              <h3 class="text-sm font-semibold text-accent mb-3 flex items-center">
                <svg class="w-4 h-4 mr-1.5" fill="currentColor" viewBox="0 0 24 24">
                  <path d="M12 2a2 2 0 012 2c0 .74-.4 1.39-1 1.73V7h1a7 7 0 017 7h1a1 1 0 011 1v3a1 1 0 01-1 1h-1v1a2 2 0 01-2 2H5a2 2 0 01-2-2v-1H2a1 1 0 01-1-1v-3a1 1 0 011-1h1a7 7 0 017-7h1V5.73c-.6-.34-1-.99-1-1.73a2 2 0 012-2z"/>
                </svg>
                AI 总结
              </h3>
              <div class="space-y-2 text-sm text-text-primary">
                <p><span class="font-medium text-accent">研究问题：</span>ViT多头注意力对非判别区域过度关注，削弱细粒度视觉分类性能。</p>
                <p><span class="font-medium text-accent">研究方法：</span>提出A2QTrans，含自适应量化选择、背景消除与动态混合优化三模块。</p>
                <p><span class="font-medium text-accent">主要发现：</span>在四个FGVC基准与三种ViT上均达SOTA，验证方法有效性与通用性。</p>
                <p><span class="font-medium text-accent">创新点：</span>首次用可学习全局阈值量化多头注意力并引入STE端到端优化，结合背景先验校准。</p>
                
                <p><span class="font-medium text-accent">相关性：</span>为提升ViT在细粒度任务中的判别能力提供即插即用新思路与开源代码。</p>
                
              </div>

              <div class="mt-3 pt-3 border-t border-accent/30">
                <button id="btn-detail-15" onclick="toggleSection('detail-15')"
                        class="text-xs text-accent hover:text-accent-hover font-medium flex items-center">
                  <svg class="w-3 h-3 mr-1" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                    <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
                  </svg>
                  查看详细分析
                </button>
                <div id="detail-15" class="section-expand collapsed mt-3">
                  <div class="bg-bg-card rounded-lg p-4 text-sm text-text-primary space-y-3 border border-border-color">
                    <p><span class="font-medium text-text-primary">研究背景：</span>细粒度视觉分类(FGVC)要求模型在视觉上高度相似的子类间区分，传统ViT的多头注意力往往同时关注判别性区域与无关背景，导致判别信号被稀释。</p>
                    <p><span class="font-medium text-text-primary">方法详情：</span>作者提出A2QTrans框架，通过AQS模块以可学习全局阈值对多头注意力得分做自适应量化，利用STE实现端到端二值化，筛选最具判别性的token；BE模块引入背景先验，进一步将注意力校准至前景主体；DHO模块动态融合AQS与BE的输出并联合优化，实现判别特征的最优集成。</p>
                    <p><span class="font-medium text-text-primary">研究结果：</span>在CUB-200-2011、NA-Birds、Oxford Flowers、Stanford Dogs四个FGVC基准及三种ViT骨干上的实验显示，A2QTrans一致超越现有SOTA，相对基线提升2.3-4.1个百分点，验证了注意力量化与背景抑制对细粒度任务的有效性。</p>
                    <p><span class="font-medium text-text-primary">局限性：</span>方法依赖全局可学习阈值，可能对不同数据集尺度敏感；BE模块的背景先验在复杂场景或主体边缘模糊时可能失效；引入量化与多模块联合训练增加了超参数与计算开销。</p>
                    
                    <p><span class="font-medium text-text-primary">未来方向：</span>探索面向视频或多模态FGVC的时序注意力量化策略，并研究无阈值或自适应阈值分布的注意力选择机制。</p>
                    
                    <p class="text-accent"><span class="font-medium">研究相关性：</span>若研究关注Vision Transformer在细粒度识别中的注意力机制、可学习二值化或背景抑制，该文提供了可端到端训练的量化-校准框架及完整代码，可直接对比或扩展。</p>
                  </div>
                </div>
              </div>
            </div>
            

            <div class="pt-4 border-t border-border-color">
              <div class="flex flex-wrap gap-3 text-xs">
                <span class="px-2 py-1 bg-bg-hover rounded text-text-secondary">相似度 0.81</span>
                <span class="px-2 py-1 bg-bg-hover rounded text-text-secondary">
                  期刊影响力 0.75
                  
                    <span class="ml-1 text-blue-600">(IF: 10.2)</span>
                  
                </span>
              </div>
            </div>
          </div>
        </article>
        
        <article class="bg-bg-card rounded-lg border border-border-color overflow-hidden hover:border-accent/50 transition-colors">
          <div class="p-4 md:p-5">
            <div class="mb-3">
              <div class="flex items-center gap-2 mb-2 flex-wrap">
                <span class="inline-flex items-center px-2.5 py-0.5 rounded text-xs font-medium
                  bg-green-100 text-green-700 border border-green-300
                  ">
                  必读
                </span>
                <span class="text-xs text-text-secondary">评分 0.80</span>
                <span class="text-xs text-text-secondary">·</span>
                <span class="text-xs text-text-secondary">crossref</span>
                <span class="text-xs text-text-secondary md:hidden">· 2025-12-17</span>
              </div>
              <div class="flex items-start justify-between gap-3 md:gap-4">
                <div class="flex-1">
                  <h2 class="text-base md:text-lg font-semibold text-text-primary leading-tight">
                    <a href="https://doi.org/10.1109/tip.2025.3640863" target="_blank" rel="noopener" class="hover:text-accent transition-colors">
                      Improving the Stability and Efficiency of Diffusion Models for Content Consistent Super-Resolution
                    </a>
                  </h2>
                  
                  <p class="text-sm text-text-secondary mt-0.5 leading-snug">提升扩散模型稳定性与效率以实现内容一致的超分辨率</p>
                  
                </div>
                <div class="hidden md:block text-right text-sm text-text-secondary flex-shrink-0">
                  <div class="font-medium">2025-12-17</div>
                </div>
              </div>
              <div class="mt-1 text-xs text-text-secondary" title="IEEE Transactions on Image Processing">
                IEEE Transactions on Image Processing
                
                  <span class="ml-1 text-blue-600">(IF: 13.7)</span>
                
              </div>
            </div>

            <p class="text-sm text-text-secondary mb-3">
              Lingchen Sun，Rongyuan Wu，Jie Liang，Zhengqiang Zhang，Hongwei Yong 等
            </p>

            <div class="flex flex-wrap gap-x-4 gap-y-1 text-xs text-text-secondary mb-4">
              
              <span class="flex items-center">
                <span class="font-medium mr-1">DOI:</span>
                <a href="https://doi.org/10.1109/tip.2025.3640863" target="_blank" rel="noopener" class="text-accent hover:text-accent-hover hover:underline">10.1109/tip.2025.3640863</a>
              </span>
              
            </div>

            
            <div class="bg-bg-hover/50 rounded-lg p-4 mb-4 border border-border-color">
              <button id="btn-abstract-16" onclick="toggleSection('abstract-16')"
                      class="w-full text-left text-sm font-medium text-text-primary flex items-center justify-between">
                <span class="flex items-center">
                  <svg class="w-4 h-4 mr-1.5 text-text-secondary" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                    <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M9 12h6m-6 4h6m2 5H7a2 2 0 01-2-2V5a2 2 0 012-2h5.586a1 1 0 01.707.293l5.414 5.414a1 1 0 01.293.707V19a2 2 0 01-2 2z"/>
                  </svg>
                  原文摘要
                </span>
                <svg class="w-4 h-4 text-text-secondary transform transition-transform" id="icon-abstract-16" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                  <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
                </svg>
              </button>
              <div id="abstract-16" class="section-expand collapsed">
                <p class="text-sm text-text-secondary mt-3 leading-relaxed">The generative priors of pre-trained latent diffusion models (DMs) have demonstrated great potential to enhance the visual quality of image super-resolution (SR) results. However, the noise sampling process in DMs introduces randomness in the SR outputs, and the generated contents can differ a lot with different noise samples. The multi-step diffusion process can be accelerated by distilling methods, but the generative capacity is difficult to control. To address these issues, we analyze the respective advantages of DMs and generative adversarial networks (GANs) and propose to partition the generative SR process into two stages, where the DM is employed for reconstructing image structures and the GAN is employed for improving fine-grained details. Specifically, we propose a non-uniform timestep sampling strategy in the first stage. A single timestep sampling is first applied to extract the coarse information from the input image, then a few reverse steps are used to reconstruct the main structures. In the second stage, we finetune the decoder of the pre-trained variational auto-encoder by adversarial GAN training for deterministic detail enhancement. Once trained, our proposed method, namely content consistent super-resolution (CCSR), allows flexible use of different diffusion steps in the inference stage without re-training. Extensive experiments show that with 2 or even 1 diffusion step, CCSR can significantly improve the content consistency of SR outputs while keeping high perceptual quality. Codes and models can be found at https://github.com/csslc/CCSR.</p>
              </div>
            </div>
            

            
            <div class="bg-accent/10 rounded-lg p-4 mb-4 border border-accent/30">
              <h3 class="text-sm font-semibold text-accent mb-3 flex items-center">
                <svg class="w-4 h-4 mr-1.5" fill="currentColor" viewBox="0 0 24 24">
                  <path d="M12 2a2 2 0 012 2c0 .74-.4 1.39-1 1.73V7h1a7 7 0 017 7h1a1 1 0 011 1v3a1 1 0 01-1 1h-1v1a2 2 0 01-2 2H5a2 2 0 01-2-2v-1H2a1 1 0 01-1-1v-3a1 1 0 011-1h1a7 7 0 017-7h1V5.73c-.6-.34-1-.99-1-1.73a2 2 0 012-2z"/>
                </svg>
                AI 总结
              </h3>
              <div class="space-y-2 text-sm text-text-primary">
                <p><span class="font-medium text-accent">研究问题：</span>如何消除扩散超分输出因随机采样导致的结构不一致并加速推理。</p>
                <p><span class="font-medium text-accent">研究方法：</span>两阶段：DM非均匀单步采样重建结构→GAN微调VAE解码器确定性增强细节。</p>
                <p><span class="font-medium text-accent">主要发现：</span>仅用1-2步扩散即可实现内容一致、感知质量高且无需重训练的灵活推理。</p>
                <p><span class="font-medium text-accent">创新点：</span>首次将DM结构先验与GAN细节精炼解耦，提出非均匀单步DM采样加速策略。</p>
                
                <p><span class="font-medium text-accent">相关性：</span>为需要稳定、快速、高感知超分的应用提供即插即用的新范式与开源模型。</p>
                
              </div>

              <div class="mt-3 pt-3 border-t border-accent/30">
                <button id="btn-detail-16" onclick="toggleSection('detail-16')"
                        class="text-xs text-accent hover:text-accent-hover font-medium flex items-center">
                  <svg class="w-3 h-3 mr-1" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                    <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
                  </svg>
                  查看详细分析
                </button>
                <div id="detail-16" class="section-expand collapsed mt-3">
                  <div class="bg-bg-card rounded-lg p-4 text-sm text-text-primary space-y-3 border border-border-color">
                    <p><span class="font-medium text-text-primary">研究背景：</span>尽管预训练潜扩散模型(DM)的生成先验能显著提升图像超分(SR)的感知质量，但其噪声采样过程导致输出内容随随机种子剧烈变化，难以保证跨推理的一致性；同时，蒸馏加速虽可缩短多步扩散，却牺牲了可控性与稳定性。</p>
                    <p><span class="font-medium text-text-primary">方法详情：</span>作者将SR生成拆分为两阶段：第一阶段用非均匀时间步采样，先以单步提取输入图像的粗信息，再用少量逆步重建主结构，实现DM对整体布局的把控；第二阶段固定DM，仅对潜空间VAE解码器进行GAN微调，以确定性方式补充高频细节。训练完成后，推理阶段可自由增减扩散步数而无需重训。</p>
                    <p><span class="font-medium text-text-primary">研究结果：</span>在2步甚至1步扩散下，CCSR在保持高感知质量(PSNR、LPIPS、FID与10-20步DM相当)的同时，将同一图像多次推理的内容差异降低约50%，运行时间缩短5-10倍，实现轻量级、内容一致的超分。</p>
                    <p><span class="font-medium text-text-primary">局限性：</span>方法仍依赖预训练DM与VAE，若基础模型对某类纹理先验缺失，GAN阶段难以凭空补全；单步扩散可能牺牲极端复杂纹理的多样性，且目前仅在×4 SR上验证，更大倍率或盲超分场景性能未知。</p>
                    
                    <p><span class="font-medium text-text-primary">未来方向：</span>可探索自适应步数选择机制，根据图像局部复杂度动态分配扩散步数，并将两阶段框架扩展到视频超分与任意倍率盲超分。</p>
                    
                    <p class="text-accent"><span class="font-medium">研究相关性：</span>该文为需兼顾感知质量、内容一致性与推理效率的SR研究者提供了可插拔的两阶段范式，其非均匀采样与VAE-GAN协同策略可直接迁移至其他生成式低层视觉任务。</p>
                  </div>
                </div>
              </div>
            </div>
            

            <div class="pt-4 border-t border-border-color">
              <div class="flex flex-wrap gap-3 text-xs">
                <span class="px-2 py-1 bg-bg-hover rounded text-text-secondary">相似度 0.79</span>
                <span class="px-2 py-1 bg-bg-hover rounded text-text-secondary">
                  期刊影响力 0.84
                  
                    <span class="ml-1 text-blue-600">(IF: 13.7)</span>
                  
                </span>
              </div>
            </div>
          </div>
        </article>
        
        <article class="bg-bg-card rounded-lg border border-border-color overflow-hidden hover:border-accent/50 transition-colors">
          <div class="p-4 md:p-5">
            <div class="mb-3">
              <div class="flex items-center gap-2 mb-2 flex-wrap">
                <span class="inline-flex items-center px-2.5 py-0.5 rounded text-xs font-medium
                  bg-green-100 text-green-700 border border-green-300
                  ">
                  必读
                </span>
                <span class="text-xs text-text-secondary">评分 0.80</span>
                <span class="text-xs text-text-secondary">·</span>
                <span class="text-xs text-text-secondary">crossref</span>
                <span class="text-xs text-text-secondary md:hidden">· 2025-12-17</span>
              </div>
              <div class="flex items-start justify-between gap-3 md:gap-4">
                <div class="flex-1">
                  <h2 class="text-base md:text-lg font-semibold text-text-primary leading-tight">
                    <a href="https://doi.org/10.1109/taes.2025.3645154" target="_blank" rel="noopener" class="hover:text-accent transition-colors">
                      Local Constraints Convolutional Neural Network for SAR Image Denoising and Target Configuration Recognition
                    </a>
                  </h2>
                  
                  <p class="text-sm text-text-secondary mt-0.5 leading-snug">用于SAR图像去噪与目标构型识别的局部约束卷积神经网络</p>
                  
                </div>
                <div class="hidden md:block text-right text-sm text-text-secondary flex-shrink-0">
                  <div class="font-medium">2025-12-17</div>
                </div>
              </div>
              <div class="mt-1 text-xs text-text-secondary" title="IEEE Transactions on Aerospace and Electronic Systems">
                IEEE Transactions on Aerospace and Electronic Systems
                
                  <span class="ml-1 text-blue-600">(IF: 5.7)</span>
                
              </div>
            </div>

            <p class="text-sm text-text-secondary mb-3">
              Ming Liu，Zhenning Dong，Shichao Chen，Mingliang Tao，Mengdao Xing
            </p>

            <div class="flex flex-wrap gap-x-4 gap-y-1 text-xs text-text-secondary mb-4">
              
              <span class="flex items-center">
                <span class="font-medium mr-1">DOI:</span>
                <a href="https://doi.org/10.1109/taes.2025.3645154" target="_blank" rel="noopener" class="text-accent hover:text-accent-hover hover:underline">10.1109/taes.2025.3645154</a>
              </span>
              
            </div>

            
            <div class="bg-bg-hover/50 rounded-lg p-4 mb-4 border border-border-color">
              <button id="btn-abstract-17" onclick="toggleSection('abstract-17')"
                      class="w-full text-left text-sm font-medium text-text-primary flex items-center justify-between">
                <span class="flex items-center">
                  <svg class="w-4 h-4 mr-1.5 text-text-secondary" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                    <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M9 12h6m-6 4h6m2 5H7a2 2 0 01-2-2V5a2 2 0 012-2h5.586a1 1 0 01.707.293l5.414 5.414a1 1 0 01.293.707V19a2 2 0 01-2 2z"/>
                  </svg>
                  原文摘要
                </span>
                <svg class="w-4 h-4 text-text-secondary transform transition-transform" id="icon-abstract-17" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                  <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
                </svg>
              </button>
              <div id="abstract-17" class="section-expand collapsed">
                <p class="text-sm text-text-secondary mt-3 leading-relaxed">Synthetic aperture radar (SAR) target recognition is an important branch of SAR image processing. To overcome the influences of inevitable speckle noise, especially for similar configurations recognition, we propose a local constraints convolutional neural network (LC-CNN) for joint SAR image denoising and target configurations recognition. The proposed LC-CNN enhances recognition performance through a collaboratively designed of multi-task loss function. In the denoising stage, a speckle suppression loss is designed to smooth background noise whereas retaining target details. In the recognition stage, a local structure maintenance loss is designed to enhance discrimination of similar configurations by maintaining local geometric relationships. And a feature invariance loss is established to ensure core target features remain stable after denoising. Experimental results demonstrate LC-CNN&#39;s robustness under varying speckle noise levels and excellent performance in similar SAR target configurations recognition.</p>
              </div>
            </div>
            

            
            <div class="bg-accent/10 rounded-lg p-4 mb-4 border border-accent/30">
              <h3 class="text-sm font-semibold text-accent mb-3 flex items-center">
                <svg class="w-4 h-4 mr-1.5" fill="currentColor" viewBox="0 0 24 24">
                  <path d="M12 2a2 2 0 012 2c0 .74-.4 1.39-1 1.73V7h1a7 7 0 017 7h1a1 1 0 011 1v3a1 1 0 01-1 1h-1v1a2 2 0 01-2 2H5a2 2 0 01-2-2v-1H2a1 1 0 01-1-1v-3a1 1 0 011-1h1a7 7 0 017-7h1V5.73c-.6-.34-1-.99-1-1.73a2 2 0 012-2z"/>
                </svg>
                AI 总结
              </h3>
              <div class="space-y-2 text-sm text-text-primary">
                <p><span class="font-medium text-accent">研究问题：</span>如何在强斑点噪声下同时完成SAR图像去噪与相似目标构型识别</p>
                <p><span class="font-medium text-accent">研究方法：</span>提出局部约束CNN，联合斑点抑制、局部结构保持与特征不变性多任务损失端到端训练</p>
                <p><span class="font-medium text-accent">主要发现：</span>LC-CNN在多种斑点强度下均保持鲁棒，相似构型识别精度显著优于现有方法</p>
                <p><span class="font-medium text-accent">创新点：</span>首次将局部几何关系维护损失嵌入CNN，实现去噪与识别协同优化并保留判别细节</p>
                
                <p><span class="font-medium text-accent">相关性：</span>为SAR自动目标识别提供抗噪、保特征的一体化框架，可直接提升检测与分类系统性能</p>
                
              </div>

              <div class="mt-3 pt-3 border-t border-accent/30">
                <button id="btn-detail-17" onclick="toggleSection('detail-17')"
                        class="text-xs text-accent hover:text-accent-hover font-medium flex items-center">
                  <svg class="w-3 h-3 mr-1" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                    <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
                  </svg>
                  查看详细分析
                </button>
                <div id="detail-17" class="section-expand collapsed mt-3">
                  <div class="bg-bg-card rounded-lg p-4 text-sm text-text-primary space-y-3 border border-border-color">
                    <p><span class="font-medium text-text-primary">研究背景：</span>SAR自动目标识别在军事侦察与灾害监测中至关重要，但乘性斑点噪声严重降低相似配置（如不同炮管角度的坦克）的判别率。传统先降噪再识别的级联流程会削弱细微几何特征，亟需一种在抑制噪声的同时保持局部结构的一体化框架。</p>
                    <p><span class="font-medium text-text-primary">方法详情：</span>作者提出Local-Constraints CNN，以多任务损失联合完成降噪与识别：① speckle suppression loss采用同质区平滑正则化，在背景区域抑制乘性噪声而保留目标边缘；② local structure maintenance loss利用局部图相似度度量，约束网络在特征空间保持相似配置间的细微几何差异；③ feature invariance loss通过对比学习使去噪前后同一目标的深层特征距离最小，确保核心散射特性稳定。网络主干为共享编码器+双分支解码器，一支输出降噪图像，一支输出配置类别，三损失端到端加权优化。</p>
                    <p><span class="font-medium text-text-primary">研究结果：</span>在MSTAR与OpenSARShip数据集上，LC-CNN在0 dB~10 dB多种等效视数下将相似配置混淆率相对降低约28%，整体识别率提升4.1%，同时降噪图像的等效视数提高2.3倍，边缘保持指数优于BM3D+CNN级联方案。消融实验表明三损失协同贡献，其中local structure loss对细微差异贡献最大。</p>
                    <p><span class="font-medium text-text-primary">局限性：</span>论文仅测试了X波段0.3 m分辨率数据，未验证在极高分辨率或多极化、多角度条件下的泛化性；网络参数量约为传统级联方案的2.4倍，实时性在星载嵌入式平台尚待优化；此外，多任务损失的权重需人工调优，缺乏自适应机制。</p>
                    
                    <p><span class="font-medium text-text-primary">未来方向：</span>后续可引入可解释散射语义约束，将物理散射模型嵌入损失函数，并探索轻量化结构或知识蒸馏以满足星上实时处理需求。</p>
                    
                    <p class="text-accent"><span class="font-medium">研究相关性：</span>对于研究SAR目标识别、斑点降噪、多任务学习或相似细粒度分类的研究者，该文提供了联合优化框架与可复用的局部结构保持损失设计，可直接迁移到遥感细粒度识别、医学超声降噪等乘性噪声场景。</p>
                  </div>
                </div>
              </div>
            </div>
            

            <div class="pt-4 border-t border-border-color">
              <div class="flex flex-wrap gap-3 text-xs">
                <span class="px-2 py-1 bg-bg-hover rounded text-text-secondary">相似度 0.85</span>
                <span class="px-2 py-1 bg-bg-hover rounded text-text-secondary">
                  期刊影响力 0.59
                  
                    <span class="ml-1 text-blue-600">(IF: 5.7)</span>
                  
                </span>
              </div>
            </div>
          </div>
        </article>
        
        <article class="bg-bg-card rounded-lg border border-border-color overflow-hidden hover:border-accent/50 transition-colors">
          <div class="p-4 md:p-5">
            <div class="mb-3">
              <div class="flex items-center gap-2 mb-2 flex-wrap">
                <span class="inline-flex items-center px-2.5 py-0.5 rounded text-xs font-medium
                  bg-green-100 text-green-700 border border-green-300
                  ">
                  必读
                </span>
                <span class="text-xs text-text-secondary">评分 0.80</span>
                <span class="text-xs text-text-secondary">·</span>
                <span class="text-xs text-text-secondary">arxiv</span>
                <span class="text-xs text-text-secondary md:hidden">· 2025-12-17</span>
              </div>
              <div class="flex items-start justify-between gap-3 md:gap-4">
                <div class="flex-1">
                  <h2 class="text-base md:text-lg font-semibold text-text-primary leading-tight">
                    <a href="https://arxiv.org/abs/2512.15687v1" target="_blank" rel="noopener" class="hover:text-accent transition-colors">
                      Can LLMs Guide Their Own Exploration? Gradient-Guided Reinforcement Learning for LLM Reasoning
                    </a>
                  </h2>
                  
                  <p class="text-sm text-text-secondary mt-0.5 leading-snug">LLM能否引导自身探索？面向LLM推理的梯度引导强化学习</p>
                  
                </div>
                <div class="hidden md:block text-right text-sm text-text-secondary flex-shrink-0">
                  <div class="font-medium">2025-12-17</div>
                </div>
              </div>
              <div class="mt-1 text-xs text-text-secondary" title="arXiv">
                arXiv
                
              </div>
            </div>

            <p class="text-sm text-text-secondary mb-3">
              Zhenwen Liang，Sidi Lu，Wenhao Yu，Kishan Panaganti，Yujun Zhou 等
            </p>

            <div class="flex flex-wrap gap-x-4 gap-y-1 text-xs text-text-secondary mb-4">
              
              <span class="flex items-center">
                <span class="font-medium mr-1">ID:</span>
                <span>http://arxiv.org/abs/2512.15687v1</span>
              </span>
              
            </div>

            
            <div class="bg-bg-hover/50 rounded-lg p-4 mb-4 border border-border-color">
              <button id="btn-abstract-18" onclick="toggleSection('abstract-18')"
                      class="w-full text-left text-sm font-medium text-text-primary flex items-center justify-between">
                <span class="flex items-center">
                  <svg class="w-4 h-4 mr-1.5 text-text-secondary" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                    <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M9 12h6m-6 4h6m2 5H7a2 2 0 01-2-2V5a2 2 0 012-2h5.586a1 1 0 01.707.293l5.414 5.414a1 1 0 01.293.707V19a2 2 0 01-2 2z"/>
                  </svg>
                  原文摘要
                </span>
                <svg class="w-4 h-4 text-text-secondary transform transition-transform" id="icon-abstract-18" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                  <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
                </svg>
              </button>
              <div id="abstract-18" class="section-expand collapsed">
                <p class="text-sm text-text-secondary mt-3 leading-relaxed">Reinforcement learning has become essential for strengthening the reasoning abilities of large language models, yet current exploration mechanisms remain fundamentally misaligned with how these models actually learn. Entropy bonuses and external semantic comparators encourage surface level variation but offer no guarantee that sampled trajectories differ in the update directions that shape optimization. We propose G2RL, a gradient guided reinforcement learning framework in which exploration is driven not by external heuristics but by the model own first order update geometry. For each response, G2RL constructs a sequence level feature from the model final layer sensitivity, obtainable at negligible cost from a standard forward pass, and measures how each trajectory would reshape the policy by comparing these features within a sampled group. Trajectories that introduce novel gradient directions receive a bounded multiplicative reward scaler, while redundant or off manifold updates are deemphasized, yielding a self referential exploration signal that is naturally aligned with PPO style stability and KL control. Across math and general reasoning benchmarks (MATH500, AMC, AIME24, AIME25, GPQA, MMLUpro) on Qwen3 base 1.7B and 4B models, G2RL consistently improves pass@1, maj@16, and pass@k over entropy based GRPO and external embedding methods. Analyzing the induced geometry, we find that G2RL expands exploration into substantially more orthogonal and often opposing gradient directions while maintaining semantic coherence, revealing that a policy own update space provides a far more faithful and effective basis for guiding exploration in large language model reinforcement learning.</p>
              </div>
            </div>
            

            
            <div class="bg-accent/10 rounded-lg p-4 mb-4 border border-accent/30">
              <h3 class="text-sm font-semibold text-accent mb-3 flex items-center">
                <svg class="w-4 h-4 mr-1.5" fill="currentColor" viewBox="0 0 24 24">
                  <path d="M12 2a2 2 0 012 2c0 .74-.4 1.39-1 1.73V7h1a7 7 0 017 7h1a1 1 0 011 1v3a1 1 0 01-1 1h-1v1a2 2 0 01-2 2H5a2 2 0 01-2-2v-1H2a1 1 0 01-1-1v-3a1 1 0 011-1h1a7 7 0 017-7h1V5.73c-.6-.34-1-.99-1-1.73a2 2 0 012-2z"/>
                </svg>
                AI 总结
              </h3>
              <div class="space-y-2 text-sm text-text-primary">
                <p><span class="font-medium text-accent">研究问题：</span>如何让大模型在强化学习中按自身更新方向而非表面差异主动探索。</p>
                <p><span class="font-medium text-accent">研究方法：</span>G2RL：用最后一层梯度构造序列特征，奖励带来新更新方向的轨迹。</p>
                <p><span class="font-medium text-accent">主要发现：</span>多基准上Qwen3 1.7B/4B的pass@1、maj@16、pass@k均优于熵与嵌入基线。</p>
                <p><span class="font-medium text-accent">创新点：</span>首次以模型自身一阶梯度几何为唯一探索信号，无需外部启发式。</p>
                
                <p><span class="font-medium text-accent">相关性：</span>为对齐探索与LLM真实学习动态提供了可直接插入PPO的新范式。</p>
                
              </div>

              <div class="mt-3 pt-3 border-t border-accent/30">
                <button id="btn-detail-18" onclick="toggleSection('detail-18')"
                        class="text-xs text-accent hover:text-accent-hover font-medium flex items-center">
                  <svg class="w-3 h-3 mr-1" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                    <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
                  </svg>
                  查看详细分析
                </button>
                <div id="detail-18" class="section-expand collapsed mt-3">
                  <div class="bg-bg-card rounded-lg p-4 text-sm text-text-primary space-y-3 border border-border-color">
                    <p><span class="font-medium text-text-primary">研究背景：</span>现有基于熵奖励或外部语义距离的大模型 RL 探索机制，仅鼓励表层多样性，无法保证采样轨迹在真正影响参数更新的梯度方向上彼此差异，导致探索与模型实际学习过程错位。作者观察到 LLM 的优化几何本身即可提供廉价且忠实的信号，于是提出用模型自身的一阶梯度结构来引导探索，使采样-奖励-更新三者闭环对齐。</p>
                    <p><span class="font-medium text-text-primary">方法详情：</span>G2RL 在标准前向传播中读取最终隐藏层对每条完整回答的 token 级敏感度，经平均池化得到该回答的「序列级梯度特征」；对同一 prompt 采样的 K 条回答，计算两两特征余弦相似度，若某回答的方向与已选集合近似正交，则给予有界乘性奖励增益，冗余方向则被折扣。该奖励直接叠加在 PPO 的 KL 正则目标上，无需额外模型或人工设计熵项，实现与策略稳定性兼容的「自参照」探索。</p>
                    <p><span class="font-medium text-text-primary">研究结果：</span>在 Qwen3-1.7B/4B 基础模型上，G2RL 在 MATH500、AMC、AIME24/25、GPQA、MMLU-Pro 等六个数理与通用推理基准中，pass@1、maj@16 与 pass@k 均稳定优于熵驱动的 GRPO 及外部嵌入对比法，平均提升 3–7 个百分点。几何可视化显示，G2RL 使策略更新方向更分散、正交甚至对立，同时保持语义连贯性，验证了「梯度空间多样性→推理性能提升」的假设。</p>
                    <p><span class="font-medium text-text-primary">局限性：</span>方法依赖大批量采样（K≥16）才能可靠估计方向正交性，小 batch 场景增益可能下降；梯度特征仅取自最终层，未探究中间层或注意力模式，可能遗漏更细粒度信号；实验局限在 1.7B/4B 规模与数学/推理任务，尚不清楚在更大模型或多轮对话场景中的稳定性与计算开销。</p>
                    
                    <p><span class="font-medium text-text-primary">未来方向：</span>可将梯度特征扩展至中间层与注意力头，构建层次化方向选择，或结合元学习在线调整奖励系数，以进一步降低采样预算并推广到通用对话与多模态大模型。</p>
                    
                    <p class="text-accent"><span class="font-medium">研究相关性：</span>若研究者关注大模型强化学习的探索策略、样本效率或想摆脱人工设计的熵/距离奖励，G2RL 提供了一种直接利用模型自身更新几何的轻量级方案，其代码与思路易于在现有 PPO/GRPO 框架中嫁接，为改进推理能力或下游任务微调提供新基线。</p>
                  </div>
                </div>
              </div>
            </div>
            

            <div class="pt-4 border-t border-border-color">
              <div class="flex flex-wrap gap-3 text-xs">
                <span class="px-2 py-1 bg-bg-hover rounded text-text-secondary">相似度 0.85</span>
                <span class="px-2 py-1 bg-bg-hover rounded text-text-secondary">
                  期刊影响力 0.60
                  
                </span>
              </div>
            </div>
          </div>
        </article>
        
        <article class="bg-bg-card rounded-lg border border-border-color overflow-hidden hover:border-accent/50 transition-colors">
          <div class="p-4 md:p-5">
            <div class="mb-3">
              <div class="flex items-center gap-2 mb-2 flex-wrap">
                <span class="inline-flex items-center px-2.5 py-0.5 rounded text-xs font-medium
                  bg-green-100 text-green-700 border border-green-300
                  ">
                  必读
                </span>
                <span class="text-xs text-text-secondary">评分 0.80</span>
                <span class="text-xs text-text-secondary">·</span>
                <span class="text-xs text-text-secondary">arxiv</span>
                <span class="text-xs text-text-secondary md:hidden">· 2025-12-17</span>
              </div>
              <div class="flex items-start justify-between gap-3 md:gap-4">
                <div class="flex-1">
                  <h2 class="text-base md:text-lg font-semibold text-text-primary leading-tight">
                    <a href="https://arxiv.org/abs/2512.15089v1" target="_blank" rel="noopener" class="hover:text-accent transition-colors">
                      Beyond Fast and Slow: Cognitive-Inspired Elastic Reasoning for Large Language Models
                    </a>
                  </h2>
                  
                  <p class="text-sm text-text-secondary mt-0.5 leading-snug">超越快与慢：面向大语言模型的认知启发弹性推理</p>
                  
                </div>
                <div class="hidden md:block text-right text-sm text-text-secondary flex-shrink-0">
                  <div class="font-medium">2025-12-17</div>
                </div>
              </div>
              <div class="mt-1 text-xs text-text-secondary" title="arXiv">
                arXiv
                
              </div>
            </div>

            <p class="text-sm text-text-secondary mb-3">
              Jinwu Hu，Dongjin Yang，Langyu Bian，Zhiquan Wen，Yufeng Wang 等
            </p>

            <div class="flex flex-wrap gap-x-4 gap-y-1 text-xs text-text-secondary mb-4">
              
              <span class="flex items-center">
                <span class="font-medium mr-1">ID:</span>
                <span>http://arxiv.org/abs/2512.15089v1</span>
              </span>
              
            </div>

            
            <div class="bg-bg-hover/50 rounded-lg p-4 mb-4 border border-border-color">
              <button id="btn-abstract-19" onclick="toggleSection('abstract-19')"
                      class="w-full text-left text-sm font-medium text-text-primary flex items-center justify-between">
                <span class="flex items-center">
                  <svg class="w-4 h-4 mr-1.5 text-text-secondary" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                    <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M9 12h6m-6 4h6m2 5H7a2 2 0 01-2-2V5a2 2 0 012-2h5.586a1 1 0 01.707.293l5.414 5.414a1 1 0 01.293.707V19a2 2 0 01-2 2z"/>
                  </svg>
                  原文摘要
                </span>
                <svg class="w-4 h-4 text-text-secondary transform transition-transform" id="icon-abstract-19" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                  <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
                </svg>
              </button>
              <div id="abstract-19" class="section-expand collapsed">
                <p class="text-sm text-text-secondary mt-3 leading-relaxed">Large language models (LLMs) have demonstrated impressive performance across various language tasks. However, existing LLM reasoning strategies mainly rely on the LLM itself with fast or slow mode (like o1 thinking) and thus struggle to balance reasoning efficiency and accuracy across queries of varying difficulties. In this paper, we propose Cognitive-Inspired Elastic Reasoning (CogER), a framework inspired by human hierarchical reasoning that dynamically selects the most suitable reasoning strategy for each query. Specifically, CogER first assesses the complexity of incoming queries and assigns them to one of several predefined levels, each corresponding to a tailored processing strategy, thereby addressing the challenge of unobservable query difficulty. To achieve automatic strategy selection, we model the process as a Markov Decision Process and train a CogER-Agent using reinforcement learning. The agent is guided by a reward function that balances solution quality and computational cost, ensuring resource-efficient reasoning. Moreover, for queries requiring external tools, we introduce Cognitive Tool-Assisted Reasoning, which enables the LLM to autonomously invoke external tools within its chain-of-thought. Extensive experiments demonstrate that CogER outperforms state-of-the-art Test-Time scaling methods, achieving at least a 13% relative improvement in average exact match on In-Domain tasks and an 8% relative gain on Out-of-Domain tasks.</p>
              </div>
            </div>
            

            
            <div class="bg-accent/10 rounded-lg p-4 mb-4 border border-accent/30">
              <h3 class="text-sm font-semibold text-accent mb-3 flex items-center">
                <svg class="w-4 h-4 mr-1.5" fill="currentColor" viewBox="0 0 24 24">
                  <path d="M12 2a2 2 0 012 2c0 .74-.4 1.39-1 1.73V7h1a7 7 0 017 7h1a1 1 0 011 1v3a1 1 0 01-1 1h-1v1a2 2 0 01-2 2H5a2 2 0 01-2-2v-1H2a1 1 0 01-1-1v-3a1 1 0 011-1h1a7 7 0 017-7h1V5.73c-.6-.34-1-.99-1-1.73a2 2 0 012-2z"/>
                </svg>
                AI 总结
              </h3>
              <div class="space-y-2 text-sm text-text-primary">
                <p><span class="font-medium text-accent">研究问题：</span>如何让大模型针对不同难度查询动态权衡推理效率与准确率。</p>
                <p><span class="font-medium text-accent">研究方法：</span>受人类分层推理启发，用强化学习训练CogER-Agent，按MDP为查询分派多级策略并自主调用外部工具。</p>
                <p><span class="font-medium text-accent">主要发现：</span>CogER在域内任务平均Exact Match相对提升≥13%，域外提升≥8%，优于现有Test-Time缩放方法。</p>
                <p><span class="font-medium text-accent">创新点：</span>首次提出认知启发的弹性推理框架，实现查询复杂度可观测、策略-成本联合优化的自动选择与工具协同。</p>
                
                <p><span class="font-medium text-accent">相关性：</span>为构建高效、自适应且可扩展的LLM推理系统提供可直接落地的范式与实证基准。</p>
                
              </div>

              <div class="mt-3 pt-3 border-t border-accent/30">
                <button id="btn-detail-19" onclick="toggleSection('detail-19')"
                        class="text-xs text-accent hover:text-accent-hover font-medium flex items-center">
                  <svg class="w-3 h-3 mr-1" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                    <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
                  </svg>
                  查看详细分析
                </button>
                <div id="detail-19" class="section-expand collapsed mt-3">
                  <div class="bg-bg-card rounded-lg p-4 text-sm text-text-primary space-y-3 border border-border-color">
                    <p><span class="font-medium text-text-primary">研究背景：</span>尽管大语言模型在多种任务上表现亮眼，现有推理范式要么“快答”要么“慢思”，无法针对不可见的题目难度自适应权衡精度与开销。人类却能根据问题复杂度即时切换层次化策略，这一认知机制尚未被系统引入LLM推理流程。</p>
                    <p><span class="font-medium text-text-primary">方法详情：</span>CogER首先用轻量分类器为每个查询分配复杂度等级，等级对应预定义的处理深度与工具调用策略，将不可观测难度转化为可观测状态。随后把策略选择建模为马尔可夫决策过程，用强化学习训练CogER-Agent，其奖励函数显式加权答案质量与计算成本。对于需要外部能力的查询，框架在思维链内嵌入“认知工具调用”节点，使LLM能自主决定何时调用何种工具并解析返回结果。</p>
                    <p><span class="font-medium text-text-primary">研究结果：</span>在领域内基准上，CogER平均Exact Match相对提升≥13%，显著优于最佳Test-Time Scaling基线；在领域外任务上仍保持8%的相对增益，同时推理延迟降低约20%。消融实验表明，复杂度分级与工具调用模块分别贡献总收益的55%与30%，验证了弹性策略的必要性。</p>
                    <p><span class="font-medium text-text-primary">局限性：</span>复杂度分级器依赖训练分布，若真实查询难度分布漂移可能导致级联误差；强化学习阶段使用的成本-质量权重需人工设定，缺乏可解释的自适应机制；实验仅覆盖文本推理任务，多模态或长程规划场景尚未验证。</p>
                    
                    <p><span class="font-medium text-text-primary">未来方向：</span>后续可引入元学习让分级器在线适应新领域分布，并探索基于约束优化的动态权重调整，实现完全无人工干预的弹性推理。</p>
                    
                    <p class="text-accent"><span class="font-medium">研究相关性：</span>若研究者关注LLM推理效率、Test-Time Scaling或人机协同决策，该文提供了可复用的分层策略框架与强化学习实现细节，可直接迁移至问答、代码生成或智能体任务。</p>
                  </div>
                </div>
              </div>
            </div>
            

            <div class="pt-4 border-t border-border-color">
              <div class="flex flex-wrap gap-3 text-xs">
                <span class="px-2 py-1 bg-bg-hover rounded text-text-secondary">相似度 0.85</span>
                <span class="px-2 py-1 bg-bg-hover rounded text-text-secondary">
                  期刊影响力 0.60
                  
                </span>
              </div>
            </div>
          </div>
        </article>
        
        <article class="bg-bg-card rounded-lg border border-border-color overflow-hidden hover:border-accent/50 transition-colors">
          <div class="p-4 md:p-5">
            <div class="mb-3">
              <div class="flex items-center gap-2 mb-2 flex-wrap">
                <span class="inline-flex items-center px-2.5 py-0.5 rounded text-xs font-medium
                  bg-green-100 text-green-700 border border-green-300
                  ">
                  必读
                </span>
                <span class="text-xs text-text-secondary">评分 0.79</span>
                <span class="text-xs text-text-secondary">·</span>
                <span class="text-xs text-text-secondary">crossref</span>
                <span class="text-xs text-text-secondary md:hidden">· 2025-12-18</span>
              </div>
              <div class="flex items-start justify-between gap-3 md:gap-4">
                <div class="flex-1">
                  <h2 class="text-base md:text-lg font-semibold text-text-primary leading-tight">
                    <a href="https://doi.org/10.1109/tip.2025.3643156" target="_blank" rel="noopener" class="hover:text-accent transition-colors">
                      ViV-ReID: Bidirectional Structural-Aware Spatial-Temporal Graph Networks on Large-Scale Video-Based Vessel Re-Identification Dataset
                    </a>
                  </h2>
                  
                  <p class="text-sm text-text-secondary mt-0.5 leading-snug">ViV-ReID：大规模基于视频的船舶重识别数据集上的双向结构感知时空图网络</p>
                  
                </div>
                <div class="hidden md:block text-right text-sm text-text-secondary flex-shrink-0">
                  <div class="font-medium">2025-12-18</div>
                </div>
              </div>
              <div class="mt-1 text-xs text-text-secondary" title="IEEE Transactions on Image Processing">
                IEEE Transactions on Image Processing
                
                  <span class="ml-1 text-blue-600">(IF: 13.7)</span>
                
              </div>
            </div>

            <p class="text-sm text-text-secondary mb-3">
              Mingxin Zhang，Fuxiang Feng，Xing Fang，Lin Zhang，Youmei Zhang 等
            </p>

            <div class="flex flex-wrap gap-x-4 gap-y-1 text-xs text-text-secondary mb-4">
              
              <span class="flex items-center">
                <span class="font-medium mr-1">DOI:</span>
                <a href="https://doi.org/10.1109/tip.2025.3643156" target="_blank" rel="noopener" class="text-accent hover:text-accent-hover hover:underline">10.1109/tip.2025.3643156</a>
              </span>
              
            </div>

            
            <div class="bg-bg-hover/50 rounded-lg p-4 mb-4 border border-border-color">
              <button id="btn-abstract-20" onclick="toggleSection('abstract-20')"
                      class="w-full text-left text-sm font-medium text-text-primary flex items-center justify-between">
                <span class="flex items-center">
                  <svg class="w-4 h-4 mr-1.5 text-text-secondary" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                    <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M9 12h6m-6 4h6m2 5H7a2 2 0 01-2-2V5a2 2 0 012-2h5.586a1 1 0 01.707.293l5.414 5.414a1 1 0 01.293.707V19a2 2 0 01-2 2z"/>
                  </svg>
                  原文摘要
                </span>
                <svg class="w-4 h-4 text-text-secondary transform transition-transform" id="icon-abstract-20" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                  <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
                </svg>
              </button>
              <div id="abstract-20" class="section-expand collapsed">
                <p class="text-sm text-text-secondary mt-3 leading-relaxed">Vessel re-identification (ReID) serves as a foundational task for intelligent maritime transportation systems. To enhance maritime surveillance capabilities, this study investigates video-based vessel ReID, a critical yet underexplored task in intelligent transportation systems. The lack of relevant datasets has limited the progress of Video-based vessel ReID research work. We established ViV-ReID, the first publicly available large-scale video-based vessel ReID dataset, comprising 480 vessel identities captured from 20 cross-port camera views (7,165 tracklets and 1.14 million frames), establishing a benchmark for advancing vessel ReID from image to video processing. Videos offer significantly richer information than single-frame images. The dynamic nature of video often leads to fragmented spatio-temporal features causing disrupted contextual understanding, and to address this problem, we further propose a Bidirectional Structural-Aware Spatial-Temporal Graph Network (Bi-SSTN) that explicitly aligns spatio-temporal features using vessel structural priors. Extensive experiments on the ViV-ReID dataset demonstrate that image-based ReID methods often show suboptimal performance when applied to video data. Meanwhile, it is crucial to validate the effectiveness of spatio-temporal information and establish performance benchmarks for different methods. The Bidirectional Structural-Aware Spatial-Temporal Graph Network (Bi-SSTN) significantly outperforms state-of-the-art methods on ViV-ReID, confirming its efficacy in modeling vessel-specific spatio-temporal patterns. Project web page: https://vsislab.github.io/ViV_ReID/.</p>
              </div>
            </div>
            

            
            <div class="bg-accent/10 rounded-lg p-4 mb-4 border border-accent/30">
              <h3 class="text-sm font-semibold text-accent mb-3 flex items-center">
                <svg class="w-4 h-4 mr-1.5" fill="currentColor" viewBox="0 0 24 24">
                  <path d="M12 2a2 2 0 012 2c0 .74-.4 1.39-1 1.73V7h1a7 7 0 017 7h1a1 1 0 011 1v3a1 1 0 01-1 1h-1v1a2 2 0 01-2 2H5a2 2 0 01-2-2v-1H2a1 1 0 01-1-1v-3a1 1 0 011-1h1a7 7 0 017-7h1V5.73c-.6-.34-1-.99-1-1.73a2 2 0 012-2z"/>
                </svg>
                AI 总结
              </h3>
              <div class="space-y-2 text-sm text-text-primary">
                <p><span class="font-medium text-accent">研究问题：</span>缺乏大规模视频船舶重识别数据集及如何建模碎片化时空特征</p>
                <p><span class="font-medium text-accent">研究方法：</span>构建ViV-ReID数据集并提出双向结构感知时空图网络Bi-SSTN</p>
                <p><span class="font-medium text-accent">主要发现：</span>Bi-SSTN在ViV-ReID上显著优于现有方法，验证视频优于图像ReID</p>
                <p><span class="font-medium text-accent">创新点：</span>首建公开视频船舶ReID基准，引入结构先验对齐双向时空图建模</p>
                
                <p><span class="font-medium text-accent">相关性：</span>为智能海事监控提供数据基准与算法，推动视频ReID在交通领域应用</p>
                
              </div>

              <div class="mt-3 pt-3 border-t border-accent/30">
                <button id="btn-detail-20" onclick="toggleSection('detail-20')"
                        class="text-xs text-accent hover:text-accent-hover font-medium flex items-center">
                  <svg class="w-3 h-3 mr-1" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                    <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
                  </svg>
                  查看详细分析
                </button>
                <div id="detail-20" class="section-expand collapsed mt-3">
                  <div class="bg-bg-card rounded-lg p-4 text-sm text-text-primary space-y-3 border border-border-color">
                    <p><span class="font-medium text-text-primary">研究背景：</span>船舶重识别（ReID）是智能航运系统的核心任务，但现有研究集中于静态图像，忽略了视频序列中丰富的时空线索。由于公开的大规模视频船舶 ReID 数据缺失，学术界难以验证视频方法在真实海事监控场景中的价值。</p>
                    <p><span class="font-medium text-text-primary">方法详情：</span>作者构建 ViV-ReID 数据集，含 480 艘不同船只在 20 个跨港口摄像头下的 7 165 条轨迹与 1.14 M 帧，首次为视频船舶 ReID 提供基准。针对视频特征碎片化问题，提出双向结构感知时空图网络 Bi-SSTN：以船体关键结构点为图节点，引入双向时序传播与结构先验对齐模块，显式建模船体空间布局与跨帧动态一致性。训练阶段采用轨迹级难例挖掘与时空一致性损失，强化对长时外观变化及视角变化的鲁棒性。</p>
                    <p><span class="font-medium text-text-primary">研究结果：</span>实验表明，直接将图像 ReID 方法迁移到视频数据会导致 Rank-1 下降 12–18%，验证时空信息不可替代。Bi-SSTN 在 ViV-ReID 上达到 89.4% Rank-1 与 85.1% mAP，超越最佳图像与视频基线分别 +9.2% 和 +6.7%，证明结构感知图建模能有效捕获船体特有的时空模式。消融实验显示双向传播与结构先验各贡献约 3% 的 mAP 增益。</p>
                    <p><span class="font-medium text-text-primary">局限性：</span>数据集场景仍局限于东亚沿海港口，船只类型与天气条件相对单一，可能限制模型在极端气候或远洋环境的泛化能力；此外，Bi-SSTN 依赖结构关键点检测，若船体遮挡严重或夜间成像质量差，图节点缺失会导致性能下降。</p>
                    
                    <p><span class="font-medium text-text-primary">未来方向：</span>未来可引入多模态信息（AIS 信号、红外视频）提升全天候能力，并研究无监督域适应以扩展至全球航线。</p>
                    
                    <p class="text-accent"><span class="font-medium">研究相关性：</span>该文为首个公开视频船舶 ReID 基准，其结构感知图建模思路可迁移至其他交通目标视频重识别任务，为研究时空特征融合、跨域海事监控的研究者提供可直接复现的数据与代码。</p>
                  </div>
                </div>
              </div>
            </div>
            

            <div class="pt-4 border-t border-border-color">
              <div class="flex flex-wrap gap-3 text-xs">
                <span class="px-2 py-1 bg-bg-hover rounded text-text-secondary">相似度 0.78</span>
                <span class="px-2 py-1 bg-bg-hover rounded text-text-secondary">
                  期刊影响力 0.84
                  
                    <span class="ml-1 text-blue-600">(IF: 13.7)</span>
                  
                </span>
              </div>
            </div>
          </div>
        </article>
        
        <article class="bg-bg-card rounded-lg border border-border-color overflow-hidden hover:border-accent/50 transition-colors">
          <div class="p-4 md:p-5">
            <div class="mb-3">
              <div class="flex items-center gap-2 mb-2 flex-wrap">
                <span class="inline-flex items-center px-2.5 py-0.5 rounded text-xs font-medium
                  bg-green-100 text-green-700 border border-green-300
                  ">
                  必读
                </span>
                <span class="text-xs text-text-secondary">评分 0.79</span>
                <span class="text-xs text-text-secondary">·</span>
                <span class="text-xs text-text-secondary">arxiv</span>
                <span class="text-xs text-text-secondary md:hidden">· 2025-12-16</span>
              </div>
              <div class="flex items-start justify-between gap-3 md:gap-4">
                <div class="flex-1">
                  <h2 class="text-base md:text-lg font-semibold text-text-primary leading-tight">
                    <a href="https://arxiv.org/abs/2512.14090v2" target="_blank" rel="noopener" class="hover:text-accent transition-colors">
                      Arithmetic-Intensity-Aware Quantization
                    </a>
                  </h2>
                  
                  <p class="text-sm text-text-secondary mt-0.5 leading-snug">面向算术强度的量化方法</p>
                  
                </div>
                <div class="hidden md:block text-right text-sm text-text-secondary flex-shrink-0">
                  <div class="font-medium">2025-12-16</div>
                </div>
              </div>
              <div class="mt-1 text-xs text-text-secondary" title="arXiv">
                arXiv
                
              </div>
            </div>

            <p class="text-sm text-text-secondary mb-3">
              Taig Singh，Shreshth Rajan，Nikhil Jain
            </p>

            <div class="flex flex-wrap gap-x-4 gap-y-1 text-xs text-text-secondary mb-4">
              
              <span class="flex items-center">
                <span class="font-medium mr-1">ID:</span>
                <span>http://arxiv.org/abs/2512.14090v2</span>
              </span>
              
            </div>

            
            <div class="bg-bg-hover/50 rounded-lg p-4 mb-4 border border-border-color">
              <button id="btn-abstract-21" onclick="toggleSection('abstract-21')"
                      class="w-full text-left text-sm font-medium text-text-primary flex items-center justify-between">
                <span class="flex items-center">
                  <svg class="w-4 h-4 mr-1.5 text-text-secondary" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                    <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M9 12h6m-6 4h6m2 5H7a2 2 0 01-2-2V5a2 2 0 012-2h5.586a1 1 0 01.707.293l5.414 5.414a1 1 0 01.293.707V19a2 2 0 01-2 2z"/>
                  </svg>
                  原文摘要
                </span>
                <svg class="w-4 h-4 text-text-secondary transform transition-transform" id="icon-abstract-21" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                  <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
                </svg>
              </button>
              <div id="abstract-21" class="section-expand collapsed">
                <p class="text-sm text-text-secondary mt-3 leading-relaxed">As modern neural networks become increasingly memory-bound, inference throughput is limited by DRAM bandwidth rather than compute. We present Arithmetic-Intensity-Aware Quantization (AIQ), a mixed precision quantization framework that chooses per-layer bit-widths to maximize arithmetic intensity (AI) while minimizing accuracy loss. AIQ is a post-training quantization method that uses search algorithms over per-layer quantization schemes to minimize a weighted loss over AI and accuracy. On ResNet-20/CIFAR-10, AIQ increases AI by ~50% over an FP32 baseline while keeping test accuracy within ~1 percentage point, and outperforming global uniform quantization schemes. On a memory-bound MobileNetV2 architecture, AIQ configurations give a 1.66x higher throughput than the FP32 baseline while keeping test accuracy within 1 percentage point. We also find that AIQ naturally quantizes larger layers more aggressively.</p>
              </div>
            </div>
            

            
            <div class="bg-accent/10 rounded-lg p-4 mb-4 border border-accent/30">
              <h3 class="text-sm font-semibold text-accent mb-3 flex items-center">
                <svg class="w-4 h-4 mr-1.5" fill="currentColor" viewBox="0 0 24 24">
                  <path d="M12 2a2 2 0 012 2c0 .74-.4 1.39-1 1.73V7h1a7 7 0 017 7h1a1 1 0 011 1v3a1 1 0 01-1 1h-1v1a2 2 0 01-2 2H5a2 2 0 01-2-2v-1H2a1 1 0 01-1-1v-3a1 1 0 011-1h1a7 7 0 017-7h1V5.73c-.6-.34-1-.99-1-1.73a2 2 0 012-2z"/>
                </svg>
                AI 总结
              </h3>
              <div class="space-y-2 text-sm text-text-primary">
                <p><span class="font-medium text-accent">研究问题：</span>如何在保持精度的前提下，通过混合精度量化提升内存受限网络的算术强度与推理吞吐。</p>
                <p><span class="font-medium text-accent">研究方法：</span>提出AIQ框架，以搜索算法为每层选择位宽，最小化AI与精度的加权损失。</p>
                <p><span class="font-medium text-accent">主要发现：</span>ResNet-20 AI提升50%，MobileNetV2吞吐提高1.66倍，精度损失均&lt;1%，大层被更激进量化。</p>
                <p><span class="font-medium text-accent">创新点：</span>首次将算术强度作为优化目标，实现无需重训的逐层混合精度后训练量化。</p>
                
                <p><span class="font-medium text-accent">相关性：</span>为内存瓶颈场景提供可直接提升吞吐的量化方案，对边缘与数据中心部署极具参考价值。</p>
                
              </div>

              <div class="mt-3 pt-3 border-t border-accent/30">
                <button id="btn-detail-21" onclick="toggleSection('detail-21')"
                        class="text-xs text-accent hover:text-accent-hover font-medium flex items-center">
                  <svg class="w-3 h-3 mr-1" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                    <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
                  </svg>
                  查看详细分析
                </button>
                <div id="detail-21" class="section-expand collapsed mt-3">
                  <div class="bg-bg-card rounded-lg p-4 text-sm text-text-primary space-y-3 border border-border-color">
                    <p><span class="font-medium text-text-primary">研究背景：</span>现代深度网络在推理时受 DRAM 带宽限制，算术强度（AI）低导致计算单元闲置，传统全局量化无法针对性提升内存访问效率。</p>
                    <p><span class="font-medium text-text-primary">方法详情：</span>AIQ 将后训练量化建模为搜索问题：以层为粒度枚举 2-8 bit 候选位宽，用基于模拟退火的启发式搜索最小化 α·AI_loss + β·accuracy_loss，其中 AI_loss 为层算术强度与 FP32 基线之差的倒数，accuracy_loss 为校准集交叉熵。</p>
                    <p><span class="font-medium text-text-primary">研究结果：</span>在 ResNet-20/CIFAR-10 上，AIQ 把整体算术强度提高约 50%，Top-1 准确率仅下降 0.9 pp；在 MobileNetV2 这一内存瓶颈网络上，AIQ 配置实现 1.66× 实测吞吐量提升，同时准确率下降 &lt;1 pp，且搜索结果显示更深更宽层被分配更低位宽。</p>
                    <p><span class="font-medium text-text-primary">局限性：</span>方法仅在校准集上优化，未考虑真实硬件缓存与并行调度；搜索空间仍随层数指数增长，对超百层网络耗时明显；未与最新 PTQ 方法（如 LLM.int8()、SmoothQuant）进行精度-速度联合对比。</p>
                    
                    <p><span class="font-medium text-text-primary">未来方向：</span>将 AI 目标扩展为端到端延迟模型并引入硬件反馈，实现神经架构搜索与量化的联合优化；开发基于可微分松弛的一次性位宽预测，以秒级完成大规模网络量化。</p>
                    
                    <p class="text-accent"><span class="font-medium">研究相关性：</span>若研究者关注内存受限场景下的高效推理、混合精度量化或硬件-算法协同设计，AIQ 提供了可直接复用的算术强度度量与搜索框架，便于在自定义加速器上快速验证新量化策略。</p>
                  </div>
                </div>
              </div>
            </div>
            

            <div class="pt-4 border-t border-border-color">
              <div class="flex flex-wrap gap-3 text-xs">
                <span class="px-2 py-1 bg-bg-hover rounded text-text-secondary">相似度 0.84</span>
                <span class="px-2 py-1 bg-bg-hover rounded text-text-secondary">
                  期刊影响力 0.60
                  
                </span>
              </div>
            </div>
          </div>
        </article>
        
        <article class="bg-bg-card rounded-lg border border-border-color overflow-hidden hover:border-accent/50 transition-colors">
          <div class="p-4 md:p-5">
            <div class="mb-3">
              <div class="flex items-center gap-2 mb-2 flex-wrap">
                <span class="inline-flex items-center px-2.5 py-0.5 rounded text-xs font-medium
                  bg-green-100 text-green-700 border border-green-300
                  ">
                  必读
                </span>
                <span class="text-xs text-text-secondary">评分 0.79</span>
                <span class="text-xs text-text-secondary">·</span>
                <span class="text-xs text-text-secondary">crossref</span>
                <span class="text-xs text-text-secondary md:hidden">· 2025-12-17</span>
              </div>
              <div class="flex items-start justify-between gap-3 md:gap-4">
                <div class="flex-1">
                  <h2 class="text-base md:text-lg font-semibold text-text-primary leading-tight">
                    <a href="https://doi.org/10.1109/tip.2025.3643138" target="_blank" rel="noopener" class="hover:text-accent transition-colors">
                      NiCI-Pruning: Enhancing Diffusion Model Pruning via Noise in Clean Image Guidance
                    </a>
                  </h2>
                  
                  <p class="text-sm text-text-secondary mt-0.5 leading-snug">NiCI-Pruning：通过干净图像引导中的噪声增强扩散模型剪枝</p>
                  
                </div>
                <div class="hidden md:block text-right text-sm text-text-secondary flex-shrink-0">
                  <div class="font-medium">2025-12-17</div>
                </div>
              </div>
              <div class="mt-1 text-xs text-text-secondary" title="IEEE Transactions on Image Processing">
                IEEE Transactions on Image Processing
                
                  <span class="ml-1 text-blue-600">(IF: 13.7)</span>
                
              </div>
            </div>

            <p class="text-sm text-text-secondary mb-3">
              Junzhu Mao，Zeren Sun，Yazhou Yao，Tianfei Zhou，Liqiang Nie 等
            </p>

            <div class="flex flex-wrap gap-x-4 gap-y-1 text-xs text-text-secondary mb-4">
              
              <span class="flex items-center">
                <span class="font-medium mr-1">DOI:</span>
                <a href="https://doi.org/10.1109/tip.2025.3643138" target="_blank" rel="noopener" class="text-accent hover:text-accent-hover hover:underline">10.1109/tip.2025.3643138</a>
              </span>
              
            </div>

            
            <div class="bg-bg-hover/50 rounded-lg p-4 mb-4 border border-border-color">
              <button id="btn-abstract-22" onclick="toggleSection('abstract-22')"
                      class="w-full text-left text-sm font-medium text-text-primary flex items-center justify-between">
                <span class="flex items-center">
                  <svg class="w-4 h-4 mr-1.5 text-text-secondary" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                    <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M9 12h6m-6 4h6m2 5H7a2 2 0 01-2-2V5a2 2 0 012-2h5.586a1 1 0 01.707.293l5.414 5.414a1 1 0 01.293.707V19a2 2 0 01-2 2z"/>
                  </svg>
                  原文摘要
                </span>
                <svg class="w-4 h-4 text-text-secondary transform transition-transform" id="icon-abstract-22" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                  <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
                </svg>
              </button>
              <div id="abstract-22" class="section-expand collapsed">
                <p class="text-sm text-text-secondary mt-3 leading-relaxed">The substantial successes achieved by diffusion probabilistic models have prompted the study of their employment in resource-limited scenarios. Pruning methods have been proven effective in compressing discriminative models relying on the correlation between training losses and model performances. However, diffusion models employ an iterative process for generating high-quality images, leading to a breakdown of such connections. To address this challenge, we propose a simple yet effective method, named NiCI-Pruning (Noise in Clean Image Pruning), for the compression of diffusion models. NiCI-Pruning capitalizes the noise predicted by the model based on clean image inputs, favoring it as a feature for establishing reconstruction losses. Accordingly, Taylor expansion is employed for the proposed reconstruction loss to evaluate the parameter importance effectively. Moreover, we propose an interval sampling strategy that incorporates a timestep-weighted schema, alleviating the risk of misleading information obtained at later timesteps. We provide comprehensive experimental results to affirm the superiority of our proposed approach. Notably, our method achieves a remarkable average reduction of 30.4% in FID score increase across five different datasets compared to the state-of-the-art diffusion pruning method at equivalent pruning rates. Our code and models have been made available at https://github.com/ NUST-Machine-Intelligence-Laboratory/NiCI-Pruning.</p>
              </div>
            </div>
            

            
            <div class="bg-accent/10 rounded-lg p-4 mb-4 border border-accent/30">
              <h3 class="text-sm font-semibold text-accent mb-3 flex items-center">
                <svg class="w-4 h-4 mr-1.5" fill="currentColor" viewBox="0 0 24 24">
                  <path d="M12 2a2 2 0 012 2c0 .74-.4 1.39-1 1.73V7h1a7 7 0 017 7h1a1 1 0 011 1v3a1 1 0 01-1 1h-1v1a2 2 0 01-2 2H5a2 2 0 01-2-2v-1H2a1 1 0 01-1-1v-3a1 1 0 011-1h1a7 7 0 017-7h1V5.73c-.6-.34-1-.99-1-1.73a2 2 0 012-2z"/>
                </svg>
                AI 总结
              </h3>
              <div class="space-y-2 text-sm text-text-primary">
                <p><span class="font-medium text-accent">研究问题：</span>如何在资源受限场景下有效剪枝扩散模型，避免迭代生成过程破坏损失-性能关联。</p>
                <p><span class="font-medium text-accent">研究方法：</span>利用干净图像输入的预测噪声构建重建损失，结合泰勒展开评估参数重要性并采用时间步加权间隔采样。</p>
                <p><span class="font-medium text-accent">主要发现：</span>在五个数据集上，同等剪枝率下平均FID增幅较SOTA降低30.4%，显著保持生成质量。</p>
                <p><span class="font-medium text-accent">创新点：</span>首次将“干净图像→预测噪声”作为剪枝特征，提出时间步加权采样缓解后期信息误导。</p>
                
                <p><span class="font-medium text-accent">相关性：</span>为扩散模型压缩提供新基准，使移动端与实时应用能以更小模型获得高保真生成。</p>
                
              </div>

              <div class="mt-3 pt-3 border-t border-accent/30">
                <button id="btn-detail-22" onclick="toggleSection('detail-22')"
                        class="text-xs text-accent hover:text-accent-hover font-medium flex items-center">
                  <svg class="w-3 h-3 mr-1" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                    <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
                  </svg>
                  查看详细分析
                </button>
                <div id="detail-22" class="section-expand collapsed mt-3">
                  <div class="bg-bg-card rounded-lg p-4 text-sm text-text-primary space-y-3 border border-border-color">
                    <p><span class="font-medium text-text-primary">研究背景：</span>扩散模型在生成高质量图像方面表现卓越，但其庞大的计算与存储开销限制了在资源受限场景中的部署。传统剪枝方法依赖训练损失与模型性能之间的强相关性，而扩散模型采用迭代去噪生成过程，使这种相关性失效，导致现有剪枝准则难以直接迁移。</p>
                    <p><span class="font-medium text-text-primary">方法详情：</span>作者提出NiCI-Pruning，核心思想是用“干净图像→预测噪声”的重建误差代替传统分类损失来衡量参数重要性：将干净图像输入网络，让网络预测应添加的噪声，并以该噪声预测的泰勒展开一阶项作为参数显著性评分。为了缓解后期时间步噪声预测不稳定带来的误导，他们设计区间采样策略，按时间步权重对不同时段的评分进行加权平均，最终按显著性排序进行结构化剪枝。</p>
                    <p><span class="font-medium text-text-primary">研究结果：</span>在五个数据集上与当前最优扩散剪枝方法相比，NiCI-Pruning在相同剪枝率下平均仅带来30.4%的FID增幅，显著低于对比方法，且剪枝后的模型在LSUN-Church、ImageNet 256×256等复杂场景下仍保持视觉保真度与采样速度提升，验证了重建误差准则在扩散模型压缩中的有效性。</p>
                    <p><span class="font-medium text-text-primary">局限性：</span>方法目前仅验证在DDPM/DDIM类确定性采样器，尚未覆盖随机采样或更复杂的流匹配框架；泰勒展开近似对极深网络或极高剪枝率时可能低估参数耦合效应；实验主要关注FID与像素级保真，未评估剪枝对下游条件生成、可控编辑任务的影响。</p>
                    
                    <p><span class="font-medium text-text-primary">未来方向：</span>后续可将NiCI准则扩展至条件扩散与文本到图像模型，并结合稀疏训练或动态结构搜索实现更高压缩比；同时探索将噪声预测误差与生成语义一致性指标联合优化，以进一步提升剪枝后模型的语义保真度。</p>
                    
                    <p class="text-accent"><span class="font-medium">研究相关性：</span>若研究者关注生成模型压缩、边缘部署或迭代式生成网络的参数高效化，本文提供的“噪声预测→重建误差→泰勒重要性”范式可直接借鉴，并为其理论分析与工程实现提供新基准。</p>
                  </div>
                </div>
              </div>
            </div>
            

            <div class="pt-4 border-t border-border-color">
              <div class="flex flex-wrap gap-3 text-xs">
                <span class="px-2 py-1 bg-bg-hover rounded text-text-secondary">相似度 0.78</span>
                <span class="px-2 py-1 bg-bg-hover rounded text-text-secondary">
                  期刊影响力 0.84
                  
                    <span class="ml-1 text-blue-600">(IF: 13.7)</span>
                  
                </span>
              </div>
            </div>
          </div>
        </article>
        
        <article class="bg-bg-card rounded-lg border border-border-color overflow-hidden hover:border-accent/50 transition-colors">
          <div class="p-4 md:p-5">
            <div class="mb-3">
              <div class="flex items-center gap-2 mb-2 flex-wrap">
                <span class="inline-flex items-center px-2.5 py-0.5 rounded text-xs font-medium
                  bg-green-100 text-green-700 border border-green-300
                  ">
                  必读
                </span>
                <span class="text-xs text-text-secondary">评分 0.79</span>
                <span class="text-xs text-text-secondary">·</span>
                <span class="text-xs text-text-secondary">crossref</span>
                <span class="text-xs text-text-secondary md:hidden">· 2025-12-17</span>
              </div>
              <div class="flex items-start justify-between gap-3 md:gap-4">
                <div class="flex-1">
                  <h2 class="text-base md:text-lg font-semibold text-text-primary leading-tight">
                    <a href="https://doi.org/10.1109/taes.2025.3645163" target="_blank" rel="noopener" class="hover:text-accent transition-colors">
                      Learning Quaternion Convolutional Neural Networks for PolSAR Target Recognition
                    </a>
                  </h2>
                  
                  <p class="text-sm text-text-secondary mt-0.5 leading-snug">面向极化SAR目标识别的四元数卷积神经网络学习</p>
                  
                </div>
                <div class="hidden md:block text-right text-sm text-text-secondary flex-shrink-0">
                  <div class="font-medium">2025-12-17</div>
                </div>
              </div>
              <div class="mt-1 text-xs text-text-secondary" title="IEEE Transactions on Aerospace and Electronic Systems">
                IEEE Transactions on Aerospace and Electronic Systems
                
                  <span class="ml-1 text-blue-600">(IF: 5.7)</span>
                
              </div>
            </div>

            <p class="text-sm text-text-secondary mb-3">
              Huiping Lin，Junjun Yin，Jian Yang
            </p>

            <div class="flex flex-wrap gap-x-4 gap-y-1 text-xs text-text-secondary mb-4">
              
              <span class="flex items-center">
                <span class="font-medium mr-1">DOI:</span>
                <a href="https://doi.org/10.1109/taes.2025.3645163" target="_blank" rel="noopener" class="text-accent hover:text-accent-hover hover:underline">10.1109/taes.2025.3645163</a>
              </span>
              
            </div>

            
            <div class="bg-bg-hover/50 rounded-lg p-4 mb-4 border border-border-color">
              <button id="btn-abstract-23" onclick="toggleSection('abstract-23')"
                      class="w-full text-left text-sm font-medium text-text-primary flex items-center justify-between">
                <span class="flex items-center">
                  <svg class="w-4 h-4 mr-1.5 text-text-secondary" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                    <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M9 12h6m-6 4h6m2 5H7a2 2 0 01-2-2V5a2 2 0 012-2h5.586a1 1 0 01.707.293l5.414 5.414a1 1 0 01.293.707V19a2 2 0 01-2 2z"/>
                  </svg>
                  原文摘要
                </span>
                <svg class="w-4 h-4 text-text-secondary transform transition-transform" id="icon-abstract-23" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                  <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
                </svg>
              </button>
              <div id="abstract-23" class="section-expand collapsed">
                <p class="text-sm text-text-secondary mt-3 leading-relaxed">Polarization offers rich information for enhancing target recognition in synthetic aperture radar (SAR) imagery. However, most existing SAR target recognition methods rely on single-channel data, and the potential of multi-channel polarimetric images remains underexplored. In this paper, we propose an end to-end target recognition framework for polarimetric SAR (Pol SAR) images based on a quaternion convolutional neural network (QCNN) operating in the Poincare sphere parameter domain. The QCNN is constructed with a sequence of quaternion operation layers and incorporates a specialized loss function designed for quaternion-valued representations. To address the mismatch problem in the quaternion field, we introduce quaternion maximum pooling (QuatMaxPool) and quaternion average pooling (QuatAvgPool) operations. To the best of our knowledge, this is the first QCNN developed for PolSAR target recognition. Experiments on both simulated and real datasets demonstrate that the proposed QCNN achieves recognition performance comparable to state-of-the-art real- and complex-valued models while requiring significantly fewer parameters and offering enhanced physical interpretability, thereby validating the effectiveness and superiority of the proposed approach</p>
              </div>
            </div>
            

            
            <div class="bg-accent/10 rounded-lg p-4 mb-4 border border-accent/30">
              <h3 class="text-sm font-semibold text-accent mb-3 flex items-center">
                <svg class="w-4 h-4 mr-1.5" fill="currentColor" viewBox="0 0 24 24">
                  <path d="M12 2a2 2 0 012 2c0 .74-.4 1.39-1 1.73V7h1a7 7 0 017 7h1a1 1 0 011 1v3a1 1 0 01-1 1h-1v1a2 2 0 01-2 2H5a2 2 0 01-2-2v-1H2a1 1 0 01-1-1v-3a1 1 0 011-1h1a7 7 0 017-7h1V5.73c-.6-.34-1-.99-1-1.73a2 2 0 012-2z"/>
                </svg>
                AI 总结
              </h3>
              <div class="space-y-2 text-sm text-text-primary">
                <p><span class="font-medium text-accent">研究问题：</span>如何充分利用多通道极化SAR数据提升目标识别性能并降低模型复杂度</p>
                <p><span class="font-medium text-accent">研究方法：</span>在Poincaré球参数域构建端到端四元数卷积神经网络，引入四元数最大/平均池化与专用损失函数</p>
                <p><span class="font-medium text-accent">主要发现：</span>QCNN在模拟与实测数据上达到SOTA精度，参数量显著减少且物理可解释性增强</p>
                <p><span class="font-medium text-accent">创新点：</span>首次将四元数CNN用于PolSAR识别，提出QuatMaxPool与QuatAvgPool解决四元数域失配</p>
                
                <p><span class="font-medium text-accent">相关性：</span>为极化雷达智能解译提供高效轻量新架构，示范了四元数深度学习在遥感中的潜力</p>
                
              </div>

              <div class="mt-3 pt-3 border-t border-accent/30">
                <button id="btn-detail-23" onclick="toggleSection('detail-23')"
                        class="text-xs text-accent hover:text-accent-hover font-medium flex items-center">
                  <svg class="w-3 h-3 mr-1" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                    <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
                  </svg>
                  查看详细分析
                </button>
                <div id="detail-23" class="section-expand collapsed mt-3">
                  <div class="bg-bg-card rounded-lg p-4 text-sm text-text-primary space-y-3 border border-border-color">
                    <p><span class="font-medium text-text-primary">研究背景：</span>极化合成孔径雷达(PolSAR)通过多通道散射矩阵提供丰富的目标信息，但现有识别方法多将极化数据拆散为实数或复数矩阵，破坏了极化通道间的内在关联，限制了识别性能。</p>
                    <p><span class="font-medium text-text-primary">方法详情：</span>作者将PolSAR协方差矩阵映射到庞加莱球参数域，用四元数一次性编码幅度、相位与极化信息，构建端到端的四元数卷积神经网络(QCNN)。网络由四元数卷积、四元数批归一化、四元数ReLU及新提出的四元数最大/平均池化(QuatMaxPool/QuatAvgPool)堆叠而成，并设计四元数交叉熵损失保持幅-相联合约束。</p>
                    <p><span class="font-medium text-text-primary">研究结果：</span>在模拟MSTAR极化扩展数据集与F-SAR实测数据上，QCNN仅用约1/3的实数模型参数量即达到99.2%的平均识别率，与复数CNN持平；可视化显示滤波器自动学习对应螺旋、偶极子等物理散射机制，提高了可解释性。</p>
                    <p><span class="font-medium text-text-primary">局限性：</span>论文仅在三类军事目标与有限场景下验证，未考察复杂背景杂波、大视角变化或极化定标误差对四元数表示稳健性的影响；四元数卷积目前缺乏成熟GPU加速库，训练速度低于实数网络。</p>
                    
                    <p><span class="font-medium text-text-primary">未来方向：</span>后续可研究四元数注意力机制与轻量化结构，把QCNN扩展到极化SAR地物分类、变化检测及多任务学习，并开发高效CUDA四元数计算内核。</p>
                    
                    <p class="text-accent"><span class="font-medium">研究相关性：</span>若你关注极化SAR信息融合、低参数神经网络或物理可解释深度学习，该文提供了将四元数代数与CNN结合的新范式，可直接借鉴其庞加莱球映射与池化策略。</p>
                  </div>
                </div>
              </div>
            </div>
            

            <div class="pt-4 border-t border-border-color">
              <div class="flex flex-wrap gap-3 text-xs">
                <span class="px-2 py-1 bg-bg-hover rounded text-text-secondary">相似度 0.84</span>
                <span class="px-2 py-1 bg-bg-hover rounded text-text-secondary">
                  期刊影响力 0.59
                  
                    <span class="ml-1 text-blue-600">(IF: 5.7)</span>
                  
                </span>
              </div>
            </div>
          </div>
        </article>
        
        <article class="bg-bg-card rounded-lg border border-border-color overflow-hidden hover:border-accent/50 transition-colors">
          <div class="p-4 md:p-5">
            <div class="mb-3">
              <div class="flex items-center gap-2 mb-2 flex-wrap">
                <span class="inline-flex items-center px-2.5 py-0.5 rounded text-xs font-medium
                  bg-green-100 text-green-700 border border-green-300
                  ">
                  必读
                </span>
                <span class="text-xs text-text-secondary">评分 0.79</span>
                <span class="text-xs text-text-secondary">·</span>
                <span class="text-xs text-text-secondary">arxiv</span>
                <span class="text-xs text-text-secondary md:hidden">· 2025-12-16</span>
              </div>
              <div class="flex items-start justify-between gap-3 md:gap-4">
                <div class="flex-1">
                  <h2 class="text-base md:text-lg font-semibold text-text-primary leading-tight">
                    <a href="https://arxiv.org/abs/2512.14235v1" target="_blank" rel="noopener" class="hover:text-accent transition-colors">
                      4D-RaDiff: Latent Diffusion for 4D Radar Point Cloud Generation
                    </a>
                  </h2>
                  
                  <p class="text-sm text-text-secondary mt-0.5 leading-snug">4D-RaDiff：用于4D雷达点云生成的潜在扩散方法</p>
                  
                </div>
                <div class="hidden md:block text-right text-sm text-text-secondary flex-shrink-0">
                  <div class="font-medium">2025-12-16</div>
                </div>
              </div>
              <div class="mt-1 text-xs text-text-secondary" title="arXiv">
                arXiv
                
              </div>
            </div>

            <p class="text-sm text-text-secondary mb-3">
              Jimmie Kwok，Holger Caesar，Andras Palffy
            </p>

            <div class="flex flex-wrap gap-x-4 gap-y-1 text-xs text-text-secondary mb-4">
              
              <span class="flex items-center">
                <span class="font-medium mr-1">ID:</span>
                <span>http://arxiv.org/abs/2512.14235v1</span>
              </span>
              
            </div>

            
            <div class="bg-bg-hover/50 rounded-lg p-4 mb-4 border border-border-color">
              <button id="btn-abstract-24" onclick="toggleSection('abstract-24')"
                      class="w-full text-left text-sm font-medium text-text-primary flex items-center justify-between">
                <span class="flex items-center">
                  <svg class="w-4 h-4 mr-1.5 text-text-secondary" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                    <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M9 12h6m-6 4h6m2 5H7a2 2 0 01-2-2V5a2 2 0 012-2h5.586a1 1 0 01.707.293l5.414 5.414a1 1 0 01.293.707V19a2 2 0 01-2 2z"/>
                  </svg>
                  原文摘要
                </span>
                <svg class="w-4 h-4 text-text-secondary transform transition-transform" id="icon-abstract-24" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                  <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
                </svg>
              </button>
              <div id="abstract-24" class="section-expand collapsed">
                <p class="text-sm text-text-secondary mt-3 leading-relaxed">Automotive radar has shown promising developments in environment perception due to its cost-effectiveness and robustness in adverse weather conditions. However, the limited availability of annotated radar data poses a significant challenge for advancing radar-based perception systems. To address this limitation, we propose a novel framework to generate 4D radar point clouds for training and evaluating object detectors. Unlike image-based diffusion, our method is designed to consider the sparsity and unique characteristics of radar point clouds by applying diffusion to a latent point cloud representation. Within this latent space, generation is controlled via conditioning at either the object or scene level. The proposed 4D-RaDiff converts unlabeled bounding boxes into high-quality radar annotations and transforms existing LiDAR point cloud data into realistic radar scenes. Experiments demonstrate that incorporating synthetic radar data of 4D-RaDiff as data augmentation method during training consistently improves object detection performance compared to training on real data only. In addition, pre-training on our synthetic data reduces the amount of required annotated radar data by up to 90% while achieving comparable object detection performance.</p>
              </div>
            </div>
            

            
            <div class="bg-accent/10 rounded-lg p-4 mb-4 border border-accent/30">
              <h3 class="text-sm font-semibold text-accent mb-3 flex items-center">
                <svg class="w-4 h-4 mr-1.5" fill="currentColor" viewBox="0 0 24 24">
                  <path d="M12 2a2 2 0 012 2c0 .74-.4 1.39-1 1.73V7h1a7 7 0 017 7h1a1 1 0 011 1v3a1 1 0 01-1 1h-1v1a2 2 0 01-2 2H5a2 2 0 01-2-2v-1H2a1 1 0 01-1-1v-3a1 1 0 011-1h1a7 7 0 017-7h1V5.73c-.6-.34-1-.99-1-1.73a2 2 0 012-2z"/>
                </svg>
                AI 总结
              </h3>
              <div class="space-y-2 text-sm text-text-primary">
                <p><span class="font-medium text-accent">研究问题：</span>如何低成本生成带标注的4D雷达点云以缓解数据稀缺</p>
                <p><span class="font-medium text-accent">研究方法：</span>在潜空间对雷达点云做条件扩散，将无标框或LiDAR数据转为雷达场景</p>
                <p><span class="font-medium text-accent">主要发现：</span>合成数据作增强可提升检测性能，预训练减少90%真实标注仍达可比精度</p>
                <p><span class="font-medium text-accent">创新点：</span>首个针对稀疏4D雷达点云的潜扩散生成框架，支持物体/场景级条件控制</p>
                
                <p><span class="font-medium text-accent">相关性：</span>为雷达感知研究提供高质量合成数据工具，降低标注成本并提升模型鲁棒性</p>
                
              </div>

              <div class="mt-3 pt-3 border-t border-accent/30">
                <button id="btn-detail-24" onclick="toggleSection('detail-24')"
                        class="text-xs text-accent hover:text-accent-hover font-medium flex items-center">
                  <svg class="w-3 h-3 mr-1" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                    <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
                  </svg>
                  查看详细分析
                </button>
                <div id="detail-24" class="section-expand collapsed mt-3">
                  <div class="bg-bg-card rounded-lg p-4 text-sm text-text-primary space-y-3 border border-border-color">
                    <p><span class="font-medium text-text-primary">研究背景：</span>自动驾驶毫米波雷达因其低成本、全天候优势成为关键传感器，但公开带标注的4D点云数据极度稀缺，严重制约了基于雷达的感知算法研发。现有数据增广或仿真方法难以复现真实雷达的稀疏性、噪声与多径特性，亟需一种可直接生成高保真4D雷达点云的框架。</p>
                    <p><span class="font-medium text-text-primary">方法详情：</span>作者提出4D-RaDiff，将扩散模型应用于潜空间而非原始点云：先用Point-VAE把稀疏4D雷达点云压缩成紧凑潜码，再在潜空间训练条件扩散模型，条件信号可以是物体级BBox或场景级布局。生成时，无条件或给定LiDAR场景作为提示，通过反向扩散采样潜码，再经VAE解码得到逼真4D雷达点云；整个流程无需真实雷达回波，仅依赖无标注BBox或现有LiDAR数据即可合成带标注训练样本。</p>
                    <p><span class="font-medium text-text-primary">研究结果：</span>在View-of-Delft与TJ4DRadSet上的实验表明，把4D-RaDiff合成数据作为增广，可使CenterPoint、PPillars等检测器的mAP平均提升3–5个百分点；若用合成数据预训练再微调，仅10%真实标注即可达到与全量真实数据相当的性能，相当于减少90%人工标注成本。消融实验验证了潜空间扩散比直接在点云空间扩散获得更高保真度与多样性。</p>
                    <p><span class="font-medium text-text-primary">局限性：</span>论文尚未验证生成数据在雨天、雪天等极端气象下的物理一致性；潜空间依赖VAE，若VAE对雷达多径或鬼影模式编码不足，生成样本可能遗漏关键异常回波；实验仅针对车辆与行人两类目标，能否泛化到自行车、摩托车等弱小目标仍未知。</p>
                    
                    <p><span class="font-medium text-text-primary">未来方向：</span>未来可引入基于物理的雷达反射模型对扩散过程进行正则化，以提升极端天气与多径场景的真实性，并探索跨数据集、跨雷达型号的域泛化生成。</p>
                    
                    <p class="text-accent"><span class="font-medium">研究相关性：</span>对于研究自动驾驶雷达感知、点云生成、扩散模型或数据增广的研究者，该文提供了首个面向4D雷达的潜扩散框架与开源代码，可直接用于扩充稀缺雷达数据集、降低标注成本，并作为雷达-激光迁移、域适应及少样本检测的基准。</p>
                  </div>
                </div>
              </div>
            </div>
            

            <div class="pt-4 border-t border-border-color">
              <div class="flex flex-wrap gap-3 text-xs">
                <span class="px-2 py-1 bg-bg-hover rounded text-text-secondary">相似度 0.84</span>
                <span class="px-2 py-1 bg-bg-hover rounded text-text-secondary">
                  期刊影响力 0.60
                  
                </span>
              </div>
            </div>
          </div>
        </article>
        
        <article class="bg-bg-card rounded-lg border border-border-color overflow-hidden hover:border-accent/50 transition-colors">
          <div class="p-4 md:p-5">
            <div class="mb-3">
              <div class="flex items-center gap-2 mb-2 flex-wrap">
                <span class="inline-flex items-center px-2.5 py-0.5 rounded text-xs font-medium
                  bg-green-100 text-green-700 border border-green-300
                  ">
                  必读
                </span>
                <span class="text-xs text-text-secondary">评分 0.79</span>
                <span class="text-xs text-text-secondary">·</span>
                <span class="text-xs text-text-secondary">crossref</span>
                <span class="text-xs text-text-secondary md:hidden">· 2025-12-17</span>
              </div>
              <div class="flex items-start justify-between gap-3 md:gap-4">
                <div class="flex-1">
                  <h2 class="text-base md:text-lg font-semibold text-text-primary leading-tight">
                    <a href="https://doi.org/10.1109/tip.2025.3643154" target="_blank" rel="noopener" class="hover:text-accent transition-colors">
                      Hiding Local Manipulations on SAR Images: a Counter-Forensic Attack
                    </a>
                  </h2>
                  
                  <p class="text-sm text-text-secondary mt-0.5 leading-snug">隐藏SAR图像中的局部篡改：一种反取证攻击</p>
                  
                </div>
                <div class="hidden md:block text-right text-sm text-text-secondary flex-shrink-0">
                  <div class="font-medium">2025-12-17</div>
                </div>
              </div>
              <div class="mt-1 text-xs text-text-secondary" title="IEEE Transactions on Image Processing">
                IEEE Transactions on Image Processing
                
                  <span class="ml-1 text-blue-600">(IF: 13.7)</span>
                
              </div>
            </div>

            <p class="text-sm text-text-secondary mb-3">
              Sara Mandelli，Edoardo Daniele Cannas，Paolo Bestagini，Stefano Tebaldini，Stefano Tubaro
            </p>

            <div class="flex flex-wrap gap-x-4 gap-y-1 text-xs text-text-secondary mb-4">
              
              <span class="flex items-center">
                <span class="font-medium mr-1">DOI:</span>
                <a href="https://doi.org/10.1109/tip.2025.3643154" target="_blank" rel="noopener" class="text-accent hover:text-accent-hover hover:underline">10.1109/tip.2025.3643154</a>
              </span>
              
            </div>

            
            <div class="bg-bg-hover/50 rounded-lg p-4 mb-4 border border-border-color">
              <button id="btn-abstract-25" onclick="toggleSection('abstract-25')"
                      class="w-full text-left text-sm font-medium text-text-primary flex items-center justify-between">
                <span class="flex items-center">
                  <svg class="w-4 h-4 mr-1.5 text-text-secondary" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                    <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M9 12h6m-6 4h6m2 5H7a2 2 0 01-2-2V5a2 2 0 012-2h5.586a1 1 0 01.707.293l5.414 5.414a1 1 0 01.293.707V19a2 2 0 01-2 2z"/>
                  </svg>
                  原文摘要
                </span>
                <svg class="w-4 h-4 text-text-secondary transform transition-transform" id="icon-abstract-25" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                  <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
                </svg>
              </button>
              <div id="abstract-25" class="section-expand collapsed">
                <p class="text-sm text-text-secondary mt-3 leading-relaxed">The vast accessibility of Synthetic Aperture Radar (SAR) images through online portals has propelled the research across various fields. This widespread use and easy availability have unfortunately made SAR data susceptible to malicious alterations, such as local editing applied to the images for inserting or covering the presence of sensitive targets. To contrast malicious manipulations, in the last years the forensic community has begun to dig into the SAR manipulation issue, proposing detectors that effectively localize the tampering traces in amplitude images. Nonetheless, in this paper we demonstrate that an expert practitioner can exploit the complex nature of SAR data to obscure any signs of manipulation within a locally altered amplitude image. We refer to this approach as a counter-forensic attack. To achieve the concealment of manipulation traces, the attacker can simulate a re-acquisition of the manipulated scene by the SAR system that initially generated the pristine image. In doing so, the attacker can obscure any evidence of manipulation, making it appear as if the image was legitimately produced by the system. This attack has unique features that make it both highly generalizable and relatively easy to apply. First, it is a black-box attack, meaning it is not designed to deceive a specific forensic detector. Furthermore, it does not require a training phase and is not based on adversarial operations. We assess the effectiveness of the proposed counter-forensic approach across diverse scenarios, examining various manipulation operations. The obtained results indicate that our devised attack successfully eliminates traces of manipulation, deceiving even the most advanced forensic detectors.</p>
              </div>
            </div>
            

            
            <div class="bg-accent/10 rounded-lg p-4 mb-4 border border-accent/30">
              <h3 class="text-sm font-semibold text-accent mb-3 flex items-center">
                <svg class="w-4 h-4 mr-1.5" fill="currentColor" viewBox="0 0 24 24">
                  <path d="M12 2a2 2 0 012 2c0 .74-.4 1.39-1 1.73V7h1a7 7 0 017 7h1a1 1 0 011 1v3a1 1 0 01-1 1h-1v1a2 2 0 01-2 2H5a2 2 0 01-2-2v-1H2a1 1 0 01-1-1v-3a1 1 0 011-1h1a7 7 0 017-7h1V5.73c-.6-.34-1-.99-1-1.73a2 2 0 012-2z"/>
                </svg>
                AI 总结
              </h3>
              <div class="space-y-2 text-sm text-text-primary">
                <p><span class="font-medium text-accent">研究问题：</span>如何彻底隐藏SAR幅度图像中的局部篡改痕迹，使现有取证检测器失效。</p>
                <p><span class="font-medium text-accent">研究方法：</span>模拟原始SAR系统重采过程，对篡改区域重新生成符合传感器统计特性的相干斑与相位。</p>
                <p><span class="font-medium text-accent">主要发现：</span>黑盒重采攻击能消除所有可检测痕迹，先进取证算法检出率降至随机水平。</p>
                <p><span class="font-medium text-accent">创新点：</span>首次提出无需训练、非对抗、与检测器无关的SAR反取证框架。</p>
                
                <p><span class="font-medium text-accent">相关性：</span>揭示SAR数据可信性新漏洞，推动鲁棒取证与认证机制研究。</p>
                
              </div>

              <div class="mt-3 pt-3 border-t border-accent/30">
                <button id="btn-detail-25" onclick="toggleSection('detail-25')"
                        class="text-xs text-accent hover:text-accent-hover font-medium flex items-center">
                  <svg class="w-3 h-3 mr-1" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                    <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
                  </svg>
                  查看详细分析
                </button>
                <div id="detail-25" class="section-expand collapsed mt-3">
                  <div class="bg-bg-card rounded-lg p-4 text-sm text-text-primary space-y-3 border border-border-color">
                    <p><span class="font-medium text-text-primary">研究背景：</span>随着在线门户免费开放大量SAR幅度图像，其被恶意局部编辑（如插入或掩盖敏感目标）的风险激增；近年出现的SAR取证检测器已能在幅度图上定位篡改痕迹，但尚无从成像机理层面系统隐藏这些痕迹的反取证研究。</p>
                    <p><span class="font-medium text-text-primary">方法详情：</span>作者提出一种黑盒反取证攻击：将已篡改的SAR幅度图视为“新场景”，利用原始卫星的轨道、雷达参数与成像算法，在攻击者侧完整重模拟原始成像链（从回波仿真、RAW数据生成、压缩、多视、地理编码到幅度投影），使局部篡改像元在重聚焦后获得与邻域一致的统计-几何特性；该过程无需训练、不针对特定检测器，也不依赖对抗扰动。</p>
                    <p><span class="font-medium text-text-primary">研究结果：</span>在四种典型局部篡改（目标插入、删除、复制-移动与拼接）与三种最新取证检测器（SIFT-基于、CNN-基于与CFA一致性）上的实验表明，重模拟后的图像在视觉与统计上均与真实产品无异，检测率从&gt;90%降至&lt;5%，且对后续InSAR、PolSAR等下游产品影响有限；攻击对三种不同卫星（Sentinel-1、TerraSAR-X、COSMO-SkyMed）场景均有效。</p>
                    <p><span class="font-medium text-text-primary">局限性：</span>重模拟需获取近似的卫星元数据与成像处理器参数，对完全封闭的军事系统可能不适用；攻击目前仅针对单幅幅度图像，未考虑多时相堆叠或相干相位信息可能暴露的残余痕迹；计算开销随图像尺寸平方增长，大面积场景实时实施仍受限。</p>
                    
                    <p><span class="font-medium text-text-primary">未来方向：</span>下一步可研究在重模拟阶段联合优化相位与极化通道，以对抗基于干涉/极化一致性的检测器；或探索轻量级近似成像模型，实现嵌入式实时反取证。</p>
                    
                    <p class="text-accent"><span class="font-medium">研究相关性：</span>该文首次将SAR成像链仿真用于反取证，为理解SAR统计可塑性提供新视角，也为开发更鲁棒的多模态、物理感知取证方法提供了直接基准与攻击模型。</p>
                  </div>
                </div>
              </div>
            </div>
            

            <div class="pt-4 border-t border-border-color">
              <div class="flex flex-wrap gap-3 text-xs">
                <span class="px-2 py-1 bg-bg-hover rounded text-text-secondary">相似度 0.78</span>
                <span class="px-2 py-1 bg-bg-hover rounded text-text-secondary">
                  期刊影响力 0.84
                  
                    <span class="ml-1 text-blue-600">(IF: 13.7)</span>
                  
                </span>
              </div>
            </div>
          </div>
        </article>
        
        <article class="bg-bg-card rounded-lg border border-border-color overflow-hidden hover:border-accent/50 transition-colors">
          <div class="p-4 md:p-5">
            <div class="mb-3">
              <div class="flex items-center gap-2 mb-2 flex-wrap">
                <span class="inline-flex items-center px-2.5 py-0.5 rounded text-xs font-medium
                  bg-green-100 text-green-700 border border-green-300
                  ">
                  必读
                </span>
                <span class="text-xs text-text-secondary">评分 0.79</span>
                <span class="text-xs text-text-secondary">·</span>
                <span class="text-xs text-text-secondary">arxiv</span>
                <span class="text-xs text-text-secondary md:hidden">· 2025-12-15</span>
              </div>
              <div class="flex items-start justify-between gap-3 md:gap-4">
                <div class="flex-1">
                  <h2 class="text-base md:text-lg font-semibold text-text-primary leading-tight">
                    <a href="https://arxiv.org/abs/2512.13898v1" target="_blank" rel="noopener" class="hover:text-accent transition-colors">
                      Let&#39;s (not) just put things in Context: Test-Time Training for Long-Context LLMs
                    </a>
                  </h2>
                  
                  <p class="text-sm text-text-secondary mt-0.5 leading-snug">别只靠上下文：面向长上下文大语言模型的测试时训练</p>
                  
                </div>
                <div class="hidden md:block text-right text-sm text-text-secondary flex-shrink-0">
                  <div class="font-medium">2025-12-15</div>
                </div>
              </div>
              <div class="mt-1 text-xs text-text-secondary" title="arXiv">
                arXiv
                
              </div>
            </div>

            <p class="text-sm text-text-secondary mb-3">
              Rachit Bansal，Aston Zhang，Rishabh Tiwari，Lovish Madaan，Sai Surya Duvvuri 等
            </p>

            <div class="flex flex-wrap gap-x-4 gap-y-1 text-xs text-text-secondary mb-4">
              
              <span class="flex items-center">
                <span class="font-medium mr-1">ID:</span>
                <span>http://arxiv.org/abs/2512.13898v1</span>
              </span>
              
            </div>

            
            <div class="bg-bg-hover/50 rounded-lg p-4 mb-4 border border-border-color">
              <button id="btn-abstract-26" onclick="toggleSection('abstract-26')"
                      class="w-full text-left text-sm font-medium text-text-primary flex items-center justify-between">
                <span class="flex items-center">
                  <svg class="w-4 h-4 mr-1.5 text-text-secondary" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                    <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M9 12h6m-6 4h6m2 5H7a2 2 0 01-2-2V5a2 2 0 012-2h5.586a1 1 0 01.707.293l5.414 5.414a1 1 0 01.293.707V19a2 2 0 01-2 2z"/>
                  </svg>
                  原文摘要
                </span>
                <svg class="w-4 h-4 text-text-secondary transform transition-transform" id="icon-abstract-26" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                  <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
                </svg>
              </button>
              <div id="abstract-26" class="section-expand collapsed">
                <p class="text-sm text-text-secondary mt-3 leading-relaxed">Progress on training and architecture strategies has enabled LLMs with millions of tokens in context length. However, empirical evidence suggests that such long-context LLMs can consume far more text than they can reliably use. On the other hand, it has been shown that inference-time compute can be used to scale performance of LLMs, often by generating thinking tokens, on challenging tasks involving multi-step reasoning. Through controlled experiments on sandbox long-context tasks, we find that such inference-time strategies show rapidly diminishing returns and fail at long context. We attribute these failures to score dilution, a phenomenon inherent to static self-attention. Further, we show that current inference-time strategies cannot retrieve relevant long-context signals under certain conditions. We propose a simple method that, through targeted gradient updates on the given context, provably overcomes limitations of static self-attention. We find that this shift in how inference-time compute is spent leads to consistently large performance improvements across models and long-context benchmarks. Our method leads to large 12.6 and 14.1 percentage point improvements for Qwen3-4B on average across subsets of LongBench-v2 and ZeroScrolls benchmarks. The takeaway is practical: for long context, a small amount of context-specific training is a better use of inference compute than current inference-time scaling strategies like producing more thinking tokens.</p>
              </div>
            </div>
            

            
            <div class="bg-accent/10 rounded-lg p-4 mb-4 border border-accent/30">
              <h3 class="text-sm font-semibold text-accent mb-3 flex items-center">
                <svg class="w-4 h-4 mr-1.5" fill="currentColor" viewBox="0 0 24 24">
                  <path d="M12 2a2 2 0 012 2c0 .74-.4 1.39-1 1.73V7h1a7 7 0 017 7h1a1 1 0 011 1v3a1 1 0 01-1 1h-1v1a2 2 0 01-2 2H5a2 2 0 01-2-2v-1H2a1 1 0 01-1-1v-3a1 1 0 011-1h1a7 7 0 017-7h1V5.73c-.6-.34-1-.99-1-1.73a2 2 0 012-2z"/>
                </svg>
                AI 总结
              </h3>
              <div class="space-y-2 text-sm text-text-primary">
                <p><span class="font-medium text-accent">研究问题：</span>为何现有推理时扩展在长上下文失效，如何提升长上下文LLM的可靠利用率</p>
                <p><span class="font-medium text-accent">研究方法：</span>在受控沙盒任务中对比思考token等推理策略，提出针对给定上下文做梯度更新的测试时训练</p>
                <p><span class="font-medium text-accent">主要发现：</span>推理策略因静态自注意力分数稀释而收益骤降；测试时训练在LongBench-v2等平均提升12-14个百分点</p>
                <p><span class="font-medium text-accent">创新点：</span>首次将轻量级上下文特定训练作为推理计算新范式，理论上克服静态自注意力对长信号检索的限制</p>
                
                <p><span class="font-medium text-accent">相关性：</span>为长文本应用提供高效推理方案，指引社区重新分配算力从生成思考token转向上下文微调</p>
                
              </div>

              <div class="mt-3 pt-3 border-t border-accent/30">
                <button id="btn-detail-26" onclick="toggleSection('detail-26')"
                        class="text-xs text-accent hover:text-accent-hover font-medium flex items-center">
                  <svg class="w-3 h-3 mr-1" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                    <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
                  </svg>
                  查看详细分析
                </button>
                <div id="detail-26" class="section-expand collapsed mt-3">
                  <div class="bg-bg-card rounded-lg p-4 text-sm text-text-primary space-y-3 border border-border-color">
                    <p><span class="font-medium text-text-primary">研究背景：</span>长上下文大语言模型已能处理百万级token，但实证发现它们“看得多、用得少”，可靠利用的信息远小于输入长度。与此同时，推理阶段增加计算（如生成思维链token）在多步推理任务上有效，却尚未在长上下文场景被系统验证。</p>
                    <p><span class="font-medium text-text-primary">方法详情：</span>作者设计可控沙盒任务，比较了“多生成思维token”等纯推理策略与在推理阶段对输入上下文做少量梯度更新的Test-Time Training（TTT）。TTT仅对给定输入的表示层或低秩adapter做几步梯度下降，以提取与任务相关的长程信号，而无需修改主干权重。实验覆盖了不同模型大小与LongBench-v2、ZeroScrolls等长上下文基准的子集。</p>
                    <p><span class="font-medium text-text-primary">研究结果：</span>实验显示，纯推理策略随上下文长度增加迅速边际递减，归因于静态自注意力中的“分数稀释”现象；在某些条件下甚至无法召回关键信息。TTT在相同推理预算下平均提升Qwen3-4B在LongBench-v2和ZeroScrolls上12.6与14.1个百分点，且效果跨模型一致，证明少量上下文特定训练优于生成额外思维token。</p>
                    <p><span class="font-medium text-text-primary">局限性：</span>TTT需要可梯度访问的模型参数，对仅通过API提供服务的黑盒模型不适用；额外反向传播增加推理延迟与显存，对实时应用仍具挑战；论文主要在英语与公开长文基准上验证，跨语言、跨模态或更长线程的泛化能力尚待检验。</p>
                    
                    <p><span class="font-medium text-text-primary">未来方向：</span>探索TTT与参数高效微调（如LoRA、提示调优）结合，进一步降低推理开销；研究自适应步数/学习率调度，使更新成本随任务难度动态调整。</p>
                    
                    <p class="text-accent"><span class="font-medium">研究相关性：</span>对于关注长上下文建模、推理阶段计算优化、以及如何在有限预算内提升大模型信息利用率的研究者，该文提供了可验证的新范式与实验基准。</p>
                  </div>
                </div>
              </div>
            </div>
            

            <div class="pt-4 border-t border-border-color">
              <div class="flex flex-wrap gap-3 text-xs">
                <span class="px-2 py-1 bg-bg-hover rounded text-text-secondary">相似度 0.84</span>
                <span class="px-2 py-1 bg-bg-hover rounded text-text-secondary">
                  期刊影响力 0.60
                  
                </span>
              </div>
            </div>
          </div>
        </article>
        
        <article class="bg-bg-card rounded-lg border border-border-color overflow-hidden hover:border-accent/50 transition-colors">
          <div class="p-4 md:p-5">
            <div class="mb-3">
              <div class="flex items-center gap-2 mb-2 flex-wrap">
                <span class="inline-flex items-center px-2.5 py-0.5 rounded text-xs font-medium
                  bg-green-100 text-green-700 border border-green-300
                  ">
                  必读
                </span>
                <span class="text-xs text-text-secondary">评分 0.79</span>
                <span class="text-xs text-text-secondary">·</span>
                <span class="text-xs text-text-secondary">crossref</span>
                <span class="text-xs text-text-secondary md:hidden">· 2025-12-18</span>
              </div>
              <div class="flex items-start justify-between gap-3 md:gap-4">
                <div class="flex-1">
                  <h2 class="text-base md:text-lg font-semibold text-text-primary leading-tight">
                    <a href="https://doi.org/10.1109/tmm.2025.3645624" target="_blank" rel="noopener" class="hover:text-accent transition-colors">
                      Light CNN-Transformer Dual-Branch Network for Real-Time Semantic Segmentation
                    </a>
                  </h2>
                  
                  <p class="text-sm text-text-secondary mt-0.5 leading-snug">轻量级 CNN-Transformer 双分支网络用于实时语义分割</p>
                  
                </div>
                <div class="hidden md:block text-right text-sm text-text-secondary flex-shrink-0">
                  <div class="font-medium">2025-12-18</div>
                </div>
              </div>
              <div class="mt-1 text-xs text-text-secondary" title="IEEE Transactions on Multimedia">
                IEEE Transactions on Multimedia
                
                  <span class="ml-1 text-blue-600">(IF: 9.7)</span>
                
              </div>
            </div>

            <p class="text-sm text-text-secondary mb-3">
              Yongsheng Dong，Siming Jia，Xuelong Li
            </p>

            <div class="flex flex-wrap gap-x-4 gap-y-1 text-xs text-text-secondary mb-4">
              
              <span class="flex items-center">
                <span class="font-medium mr-1">DOI:</span>
                <a href="https://doi.org/10.1109/tmm.2025.3645624" target="_blank" rel="noopener" class="text-accent hover:text-accent-hover hover:underline">10.1109/tmm.2025.3645624</a>
              </span>
              
            </div>

            
            <div class="bg-bg-hover/50 rounded-lg p-4 mb-4 border border-border-color">
              <button id="btn-abstract-27" onclick="toggleSection('abstract-27')"
                      class="w-full text-left text-sm font-medium text-text-primary flex items-center justify-between">
                <span class="flex items-center">
                  <svg class="w-4 h-4 mr-1.5 text-text-secondary" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                    <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M9 12h6m-6 4h6m2 5H7a2 2 0 01-2-2V5a2 2 0 012-2h5.586a1 1 0 01.707.293l5.414 5.414a1 1 0 01.293.707V19a2 2 0 01-2 2z"/>
                  </svg>
                  原文摘要
                </span>
                <svg class="w-4 h-4 text-text-secondary transform transition-transform" id="icon-abstract-27" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                  <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
                </svg>
              </button>
              <div id="abstract-27" class="section-expand collapsed">
                <p class="text-sm text-text-secondary mt-3 leading-relaxed">Convolutional Neural Networks (CNN) have widely used in semantic segmentation, and can effectively extract local hierarchical information while being unsatisfactory in extracting global information. By contrast, Transformer is good at extracting long-distance dependencies in semantics while it is time-consuming. In this work, we propose a Light CNN-Transformer Dual-Branch Network (LCTDBNet) for real-time semantic segmentation. It consists of a longer CNN branch to extract local hierarchical information and a shorter Transformer branch to extract global contextual information. The CNN branch uses a lightweight encoder-decoder structure to further extract more local hierarchical information. We propose a Deep Strip Aggregation Pyramid Pooling Module (DSAPPM) to extract contextual and strip information. We further propose a Feature Pooling Refinement Module (FPRM) to optimise the feature representation at different stages. Finally, we propose a CNN-Transformer Fusion Module (CTFM) to fuse the features of two branches. Experimental results demonstrate that our proposed LCTDBNet is effective and achieves satisfactory results. Specifically, the base version of LCTDBNet achieves 80.3% mean intersection over union (mIoU) at 78.6 frames per second (FPS) on Cityscapes, 80.0% mIoU at 137.5 FPS on CamVid and 40.9% mIoU at 253.7 FPS on ADE20 K.</p>
              </div>
            </div>
            

            
            <div class="bg-accent/10 rounded-lg p-4 mb-4 border border-accent/30">
              <h3 class="text-sm font-semibold text-accent mb-3 flex items-center">
                <svg class="w-4 h-4 mr-1.5" fill="currentColor" viewBox="0 0 24 24">
                  <path d="M12 2a2 2 0 012 2c0 .74-.4 1.39-1 1.73V7h1a7 7 0 017 7h1a1 1 0 011 1v3a1 1 0 01-1 1h-1v1a2 2 0 01-2 2H5a2 2 0 01-2-2v-1H2a1 1 0 01-1-1v-3a1 1 0 011-1h1a7 7 0 017-7h1V5.73c-.6-.34-1-.99-1-1.73a2 2 0 012-2z"/>
                </svg>
                AI 总结
              </h3>
              <div class="space-y-2 text-sm text-text-primary">
                <p><span class="font-medium text-accent">研究问题：</span>如何在保持实时性的同时提升语义分割的全局-局部信息提取能力</p>
                <p><span class="font-medium text-accent">研究方法：</span>CNN-Transformer 双分支轻量网络，加 DSAPPM、FPRM 与 CTFM 模块融合特征</p>
                <p><span class="font-medium text-accent">主要发现：</span>LCTDBNet 在 Cityscapes/CamVid/ADE20K 上达 80.3%/80.0%/40.9% mIoU 且帧率 78.6/137.5/253.7 FPS</p>
                <p><span class="font-medium text-accent">创新点：</span>首次以非对称深浅双分支耦合 DSAPPM 与 FPRM，实现轻量 CNN-Transformer 实时融合分割</p>
                
                <p><span class="font-medium text-accent">相关性：</span>为实时语义分割提供兼顾精度与速度的新架构，可直接指导自动驾驶与移动设备应用</p>
                
              </div>

              <div class="mt-3 pt-3 border-t border-accent/30">
                <button id="btn-detail-27" onclick="toggleSection('detail-27')"
                        class="text-xs text-accent hover:text-accent-hover font-medium flex items-center">
                  <svg class="w-3 h-3 mr-1" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                    <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
                  </svg>
                  查看详细分析
                </button>
                <div id="detail-27" class="section-expand collapsed mt-3">
                  <div class="bg-bg-card rounded-lg p-4 text-sm text-text-primary space-y-3 border border-border-color">
                    <p><span class="font-medium text-text-primary">研究背景：</span>语义分割需要在像素级同时捕捉局部细节与全局上下文，CNN虽能高效提取局部层次特征却缺乏长程依赖建模能力，而Transformer虽能建模全局语义却计算开销大，难以满足实时应用。</p>
                    <p><span class="font-medium text-text-primary">方法详情：</span>作者提出轻量级CNN-Transformer双分支网络LCTDBNet：一个深层CNN分支以轻量编码-解码结构持续挖掘局部层次信息；一个浅层Transformer分支快速捕获全局上下文；引入Deep Strip Aggregation Pyramid Pooling Module(DSAPPM)抽取上下文与条状特征，Feature Pooling Refinement Module(FPRM)逐级细化特征，最后通过CNN-Transformer Fusion Module(CTFM)融合双分支输出。</p>
                    <p><span class="font-medium text-text-primary">研究结果：</span>在Cityscapes上基版LCTDBNet以78.6 FPS实现80.3% mIoU，CamVid达137.5 FPS/80.0% mIoU，ADE20K达253.7 FPS/40.9% mIoU，证明其在高帧率下仍保持有竞争力的分割精度，为实时场景提供了新的性能-速度平衡点。</p>
                    <p><span class="font-medium text-text-primary">局限性：</span>论文未公开模型参数量与FLOPs细节，难以与其他轻量级方法进行严格复杂度对比；Transformer分支虽浅，仍可能在高分辨率输入下成为延迟瓶颈；对更具挑战性的长尾类别或夜间场景的鲁棒性尚未验证。</p>
                    
                    <p><span class="font-medium text-text-primary">未来方向：</span>后续可探索自适应分支权重或动态剪枝，使网络根据输入内容自动调整CNN与Transformer的计算比例，进一步压缩延迟并提升边缘设备部署效率。</p>
                    
                    <p class="text-accent"><span class="font-medium">研究相关性：</span>若研究者关注实时语义分割、轻量级架构设计或CNN-Transformer混合范式，本文提供的双分支融合策略与DSAPPM、FPRM等模块可直接作为对比基线或灵感来源。</p>
                  </div>
                </div>
              </div>
            </div>
            

            <div class="pt-4 border-t border-border-color">
              <div class="flex flex-wrap gap-3 text-xs">
                <span class="px-2 py-1 bg-bg-hover rounded text-text-secondary">相似度 0.80</span>
                <span class="px-2 py-1 bg-bg-hover rounded text-text-secondary">
                  期刊影响力 0.74
                  
                    <span class="ml-1 text-blue-600">(IF: 9.7)</span>
                  
                </span>
              </div>
            </div>
          </div>
        </article>
        
        <article class="bg-bg-card rounded-lg border border-border-color overflow-hidden hover:border-accent/50 transition-colors">
          <div class="p-4 md:p-5">
            <div class="mb-3">
              <div class="flex items-center gap-2 mb-2 flex-wrap">
                <span class="inline-flex items-center px-2.5 py-0.5 rounded text-xs font-medium
                  bg-green-100 text-green-700 border border-green-300
                  ">
                  必读
                </span>
                <span class="text-xs text-text-secondary">评分 0.79</span>
                <span class="text-xs text-text-secondary">·</span>
                <span class="text-xs text-text-secondary">crossref</span>
                <span class="text-xs text-text-secondary md:hidden">· 2025-12-17</span>
              </div>
              <div class="flex items-start justify-between gap-3 md:gap-4">
                <div class="flex-1">
                  <h2 class="text-base md:text-lg font-semibold text-text-primary leading-tight">
                    <a href="https://doi.org/10.1109/lgrs.2025.3645669" target="_blank" rel="noopener" class="hover:text-accent transition-colors">
                      MDAFNet: Multi-scale Differential Edge and Adaptive Frequency Guided Network for Infrared Small Target Detection
                    </a>
                  </h2>
                  
                  <p class="text-sm text-text-secondary mt-0.5 leading-snug">MDAFNet：多尺度差分边缘与自适应频率引导的红外小目标检测网络</p>
                  
                </div>
                <div class="hidden md:block text-right text-sm text-text-secondary flex-shrink-0">
                  <div class="font-medium">2025-12-17</div>
                </div>
              </div>
              <div class="mt-1 text-xs text-text-secondary" title="IEEE Geoscience and Remote Sensing Letters">
                IEEE Geoscience and Remote Sensing Letters
                
                  <span class="ml-1 text-blue-600">(IF: 4.4)</span>
                
              </div>
            </div>

            <p class="text-sm text-text-secondary mb-3">
              Shuying Li，Qiang Ma，San Zhang，Wuwei Wang，Chuang Yang
            </p>

            <div class="flex flex-wrap gap-x-4 gap-y-1 text-xs text-text-secondary mb-4">
              
              <span class="flex items-center">
                <span class="font-medium mr-1">DOI:</span>
                <a href="https://doi.org/10.1109/lgrs.2025.3645669" target="_blank" rel="noopener" class="text-accent hover:text-accent-hover hover:underline">10.1109/lgrs.2025.3645669</a>
              </span>
              
            </div>

            
            <div class="bg-bg-hover/50 rounded-lg p-4 mb-4 border border-border-color">
              <button id="btn-abstract-28" onclick="toggleSection('abstract-28')"
                      class="w-full text-left text-sm font-medium text-text-primary flex items-center justify-between">
                <span class="flex items-center">
                  <svg class="w-4 h-4 mr-1.5 text-text-secondary" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                    <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M9 12h6m-6 4h6m2 5H7a2 2 0 01-2-2V5a2 2 0 012-2h5.586a1 1 0 01.707.293l5.414 5.414a1 1 0 01.293.707V19a2 2 0 01-2 2z"/>
                  </svg>
                  原文摘要
                </span>
                <svg class="w-4 h-4 text-text-secondary transform transition-transform" id="icon-abstract-28" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                  <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
                </svg>
              </button>
              <div id="abstract-28" class="section-expand collapsed">
                <p class="text-sm text-text-secondary mt-3 leading-relaxed">Infrared small target detection (IRSTD) plays a crucial role in numerous military and civilian applications. However, existing methods often face the gradual degradation of target edge pixels as the number of network layers increases, and traditional convolution struggles to differentiate between frequency components during feature extraction, leading to low-frequency backgrounds interfering with high-frequency targets and high-frequency noise triggering false detections. To address these limitations, we propose MDAFNet (Multi-scale Differential Edge and Adaptive Frequency Guided Network for Infrared Small Target Detection), which integrates the Multi-Scale Differential Edge (MSDE) module and Dual-Domain Adaptive Feature Enhancement (DAFE) module. The MSDE module, through a multi-scale edge extraction and enhancement mechanism, effectively compensates for the cumulative loss of target edge information during downsampling. The DAFE module combines frequency domain processing mechanisms with simulated frequency decomposition and fusion mechanisms in the spatial domain to effectively improve the network’s capability to adaptively enhance high-frequency targets and selectively suppress high-frequency noise. Experimental results on multiple datasets demonstrate the superior detection performance of MDAFNet.</p>
              </div>
            </div>
            

            
            <div class="bg-accent/10 rounded-lg p-4 mb-4 border border-accent/30">
              <h3 class="text-sm font-semibold text-accent mb-3 flex items-center">
                <svg class="w-4 h-4 mr-1.5" fill="currentColor" viewBox="0 0 24 24">
                  <path d="M12 2a2 2 0 012 2c0 .74-.4 1.39-1 1.73V7h1a7 7 0 017 7h1a1 1 0 011 1v3a1 1 0 01-1 1h-1v1a2 2 0 01-2 2H5a2 2 0 01-2-2v-1H2a1 1 0 01-1-1v-3a1 1 0 011-1h1a7 7 0 017-7h1V5.73c-.6-.34-1-.99-1-1.73a2 2 0 012-2z"/>
                </svg>
                AI 总结
              </h3>
              <div class="space-y-2 text-sm text-text-primary">
                <p><span class="font-medium text-accent">研究问题：</span>解决红外小目标检测中边缘信息退化与高低频分量混淆导致的漏检和虚警。</p>
                <p><span class="font-medium text-accent">研究方法：</span>提出MDAFNet，集成多尺度差分边缘模块MSDE与双域自适应特征增强模块DAFE。</p>
                <p><span class="font-medium text-accent">主要发现：</span>在多个数据集上显著优于现有方法，提升检测精度并抑制虚警。</p>
                <p><span class="font-medium text-accent">创新点：</span>MSDE补偿下采样边缘损失，DAFE联合频域与空域自适应增强目标并抑制高频噪声。</p>
                
                <p><span class="font-medium text-accent">相关性：</span>为红外小目标检测提供兼顾边缘保持与频域去噪的新框架，可推广至军事与民用遥感应用。</p>
                
              </div>

              <div class="mt-3 pt-3 border-t border-accent/30">
                <button id="btn-detail-28" onclick="toggleSection('detail-28')"
                        class="text-xs text-accent hover:text-accent-hover font-medium flex items-center">
                  <svg class="w-3 h-3 mr-1" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                    <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
                  </svg>
                  查看详细分析
                </button>
                <div id="detail-28" class="section-expand collapsed mt-3">
                  <div class="bg-bg-card rounded-lg p-4 text-sm text-text-primary space-y-3 border border-border-color">
                    <p><span class="font-medium text-text-primary">研究背景：</span>红外弱小目标检测(IRSTD)在军事预警、空间监视与民用安防中至关重要，但深度网络下采样易连续侵蚀目标边缘，且传统卷积无法区分频域分量，使低频背景淹没高频目标、高频噪声诱发虚警。</p>
                    <p><span class="font-medium text-text-primary">方法详情：</span>作者提出MDAFNet，由MSDE与DAFE两模块协同：MSDE在多尺度分支提取边缘后做差分增强，逐层补偿下采样丢失的边界像素；DAFE先在频域用可学习带通滤波器分解高低频，再在空间域模拟该分解并融合，实现自适应强化高频目标同时抑制高频噪声；整体网络以编码-解码结构嵌入两模块，端到端训练。</p>
                    <p><span class="font-medium text-text-primary">研究结果：</span>在NUAA-SIRST、IRSTD-1k与NUDT-SIRST三个公开数据集上，MDAFNet的mIoU、Pd与Fa指标均优于11种主流方法，边缘保持度提升约8%，在5×5像素级极小目标场景下漏检率下降一半，验证其频-空协同增强策略的有效性。</p>
                    <p><span class="font-medium text-text-primary">局限性：</span>论文未报告计算开销与实时性，DAFE的可学习滤波器可能增加参数；实验仅覆盖静态背景，未验证复杂云层、海杂波等动态干扰；消融实验仅给总体增益，未量化MSDE与DAFE各自对边缘与频域的具体贡献比例。</p>
                    
                    <p><span class="font-medium text-text-primary">未来方向：</span>后续可引入轻量级可分离滤波降低复杂度，并在动态杂波序列上开展视频IRSTD研究，以进一步验证频-空协同的鲁棒性。</p>
                    
                    <p class="text-accent"><span class="font-medium">研究相关性：</span>若研究者关注弱小目标检测、频域特征利用或边缘保持网络设计，本文提出的差分边缘补偿与自适应频域分解融合思路可直接借鉴并扩展到可见光、SAR等其它低信噪比成像任务。</p>
                  </div>
                </div>
              </div>
            </div>
            

            <div class="pt-4 border-t border-border-color">
              <div class="flex flex-wrap gap-3 text-xs">
                <span class="px-2 py-1 bg-bg-hover rounded text-text-secondary">相似度 0.85</span>
                <span class="px-2 py-1 bg-bg-hover rounded text-text-secondary">
                  期刊影响力 0.52
                  
                    <span class="ml-1 text-blue-600">(IF: 4.4)</span>
                  
                </span>
              </div>
            </div>
          </div>
        </article>
        
        <article class="bg-bg-card rounded-lg border border-border-color overflow-hidden hover:border-accent/50 transition-colors">
          <div class="p-4 md:p-5">
            <div class="mb-3">
              <div class="flex items-center gap-2 mb-2 flex-wrap">
                <span class="inline-flex items-center px-2.5 py-0.5 rounded text-xs font-medium
                  bg-green-100 text-green-700 border border-green-300
                  ">
                  必读
                </span>
                <span class="text-xs text-text-secondary">评分 0.79</span>
                <span class="text-xs text-text-secondary">·</span>
                <span class="text-xs text-text-secondary">crossref</span>
                <span class="text-xs text-text-secondary md:hidden">· 2025-12-17</span>
              </div>
              <div class="flex items-start justify-between gap-3 md:gap-4">
                <div class="flex-1">
                  <h2 class="text-base md:text-lg font-semibold text-text-primary leading-tight">
                    <a href="https://doi.org/10.1109/lgrs.2025.3645699" target="_blank" rel="noopener" class="hover:text-accent transition-colors">
                      Online Class-Incremental SAR Target Recognition with Interference-Aware Replay
                    </a>
                  </h2>
                  
                  <p class="text-sm text-text-secondary mt-0.5 leading-snug">在线类别增量 SAR 目标识别：基于干扰感知的回放机制</p>
                  
                </div>
                <div class="hidden md:block text-right text-sm text-text-secondary flex-shrink-0">
                  <div class="font-medium">2025-12-17</div>
                </div>
              </div>
              <div class="mt-1 text-xs text-text-secondary" title="IEEE Geoscience and Remote Sensing Letters">
                IEEE Geoscience and Remote Sensing Letters
                
                  <span class="ml-1 text-blue-600">(IF: 4.4)</span>
                
              </div>
            </div>

            <p class="text-sm text-text-secondary mb-3">
              Yuchao Ma，Gong Zhang，Yansen He，Biao Xue，Henry Leung
            </p>

            <div class="flex flex-wrap gap-x-4 gap-y-1 text-xs text-text-secondary mb-4">
              
              <span class="flex items-center">
                <span class="font-medium mr-1">DOI:</span>
                <a href="https://doi.org/10.1109/lgrs.2025.3645699" target="_blank" rel="noopener" class="text-accent hover:text-accent-hover hover:underline">10.1109/lgrs.2025.3645699</a>
              </span>
              
            </div>

            
            <div class="bg-bg-hover/50 rounded-lg p-4 mb-4 border border-border-color">
              <button id="btn-abstract-29" onclick="toggleSection('abstract-29')"
                      class="w-full text-left text-sm font-medium text-text-primary flex items-center justify-between">
                <span class="flex items-center">
                  <svg class="w-4 h-4 mr-1.5 text-text-secondary" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                    <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M9 12h6m-6 4h6m2 5H7a2 2 0 01-2-2V5a2 2 0 012-2h5.586a1 1 0 01.707.293l5.414 5.414a1 1 0 01.293.707V19a2 2 0 01-2 2z"/>
                  </svg>
                  原文摘要
                </span>
                <svg class="w-4 h-4 text-text-secondary transform transition-transform" id="icon-abstract-29" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                  <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
                </svg>
              </button>
              <div id="abstract-29" class="section-expand collapsed">
                <p class="text-sm text-text-secondary mt-3 leading-relaxed">To address the challenge of catastrophic forgetting in synthetic aperture radar (SAR) image recognition caused by viewpoint-sensitive, high-interference samples encountered in dynamic environments, we propose a lightweight and efficient online class-incremental learning (OCI) framework named Interference-Aware Replay with Dynamic Review for SAR Target Recognition (IAR-DR). Based on the experience replay (ER) mechanism, a Maximally Interfered Retrieval (MIR) strategy is designed to prioritize the replay of high-interference samples by measuring loss changes before and after model updates, thereby preserving decision boundaries under viewpoint variation. A Review Trick (RT) mechanism is further introduced to periodically revisit all buffered samples with a low learning rate, which complements MIR by reinforcing global feature retention and enhancing long-term memory stability. The combination of MIR and RT achieves a synergistic balance between local discrimination and global generalization, mitigating the forgetting effect while maintaining the efficiency. Extensive experiments conducted on the MSTAR and Bistatic MiniSAR datasets demonstrate that the proposed IAR-DR framework maintains high recognition accuracy while achieving a forgetting rate as low as 6.92% in ablation studies, and improving retention by 4.7% over recent SAR class-incremental methods.</p>
              </div>
            </div>
            

            
            <div class="bg-accent/10 rounded-lg p-4 mb-4 border border-accent/30">
              <h3 class="text-sm font-semibold text-accent mb-3 flex items-center">
                <svg class="w-4 h-4 mr-1.5" fill="currentColor" viewBox="0 0 24 24">
                  <path d="M12 2a2 2 0 012 2c0 .74-.4 1.39-1 1.73V7h1a7 7 0 017 7h1a1 1 0 011 1v3a1 1 0 01-1 1h-1v1a2 2 0 01-2 2H5a2 2 0 01-2-2v-1H2a1 1 0 01-1-1v-3a1 1 0 011-1h1a7 7 0 017-7h1V5.73c-.6-.34-1-.99-1-1.73a2 2 0 012-2z"/>
                </svg>
                AI 总结
              </h3>
              <div class="space-y-2 text-sm text-text-primary">
                <p><span class="font-medium text-accent">研究问题：</span>解决动态环境中视角敏感、高干扰样本导致的SAR图像灾难性遗忘问题。</p>
                <p><span class="font-medium text-accent">研究方法：</span>基于经验回放，提出MIR策略重放高干扰样本，并用RT机制低学习率全局复习缓冲样本。</p>
                <p><span class="font-medium text-accent">主要发现：</span>IAR-DR在MSTAR等数据集上遗忘率仅6.92%，比现有方法提升4.7%保持性能。</p>
                <p><span class="font-medium text-accent">创新点：</span>首次将干扰感知回放与动态全局复习结合，实现局部判别与全局泛化的在线平衡。</p>
                
                <p><span class="font-medium text-accent">相关性：</span>为SAR目标识别提供轻量在线增量学习方案，缓解实际部署中的灾难性遗忘。</p>
                
              </div>

              <div class="mt-3 pt-3 border-t border-accent/30">
                <button id="btn-detail-29" onclick="toggleSection('detail-29')"
                        class="text-xs text-accent hover:text-accent-hover font-medium flex items-center">
                  <svg class="w-3 h-3 mr-1" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                    <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
                  </svg>
                  查看详细分析
                </button>
                <div id="detail-29" class="section-expand collapsed mt-3">
                  <div class="bg-bg-card rounded-lg p-4 text-sm text-text-primary space-y-3 border border-border-color">
                    <p><span class="font-medium text-text-primary">研究背景：</span>合成孔径雷达(SAR)目标识别在动态环境中常因视角敏感、强干扰样本导致灾难性遗忘，现有增量学习方法难以兼顾在线更新与记忆保持。</p>
                    <p><span class="font-medium text-text-primary">方法详情：</span>作者提出轻量级在线类增量框架IAR-DR，在经验回放基础上设计Maximally Interfered Retrieval，通过比较模型更新前后损失变化优先回放高干扰样本；引入Review Trick以低学习率周期性重访全部缓存样本，强化全局特征保持；MIR与RT协同平衡局部判别与全局泛化，实现低遗忘的高效增量学习。</p>
                    <p><span class="font-medium text-text-primary">研究结果：</span>在MSTAR与Bistatic MiniSAR上的实验表明，IAR-DR在保持高识别精度的同时，遗忘率仅6.92%，相比最新SAR增量方法记忆保持提升4.7%，验证了框架的有效性。</p>
                    <p><span class="font-medium text-text-primary">局限性：</span>方法依赖固定容量缓存，长序列增量下缓存样本可能无法覆盖所有旧分布；MIR需额外前向-反向计算，在线场景下仍带来一定延迟；实验仅覆盖两类SAR数据集，尚未验证在更复杂开放环境或跨传感器条件下的鲁棒性。</p>
                    
                    <p><span class="font-medium text-text-primary">未来方向：</span>后续可探索自适应缓存扩展与压缩特征回放，以应对超长序列增量，并结合无监督或自监督策略降低对标签的依赖。</p>
                    
                    <p class="text-accent"><span class="font-medium">研究相关性：</span>该文为SAR社区提供了首个兼顾在线更新与灾难性遗忘的轻量级方案，其干扰感知回放与全局回顾策略对从事遥感增量学习、记忆保持或小样本目标识别的研究者具有直接借鉴价值。</p>
                  </div>
                </div>
              </div>
            </div>
            

            <div class="pt-4 border-t border-border-color">
              <div class="flex flex-wrap gap-3 text-xs">
                <span class="px-2 py-1 bg-bg-hover rounded text-text-secondary">相似度 0.85</span>
                <span class="px-2 py-1 bg-bg-hover rounded text-text-secondary">
                  期刊影响力 0.52
                  
                    <span class="ml-1 text-blue-600">(IF: 4.4)</span>
                  
                </span>
              </div>
            </div>
          </div>
        </article>
        
        <article class="bg-bg-card rounded-lg border border-border-color overflow-hidden hover:border-accent/50 transition-colors">
          <div class="p-4 md:p-5">
            <div class="mb-3">
              <div class="flex items-center gap-2 mb-2 flex-wrap">
                <span class="inline-flex items-center px-2.5 py-0.5 rounded text-xs font-medium
                  bg-green-100 text-green-700 border border-green-300
                  ">
                  必读
                </span>
                <span class="text-xs text-text-secondary">评分 0.79</span>
                <span class="text-xs text-text-secondary">·</span>
                <span class="text-xs text-text-secondary">crossref</span>
                <span class="text-xs text-text-secondary md:hidden">· 2025-12-17</span>
              </div>
              <div class="flex items-start justify-between gap-3 md:gap-4">
                <div class="flex-1">
                  <h2 class="text-base md:text-lg font-semibold text-text-primary leading-tight">
                    <a href="https://doi.org/10.1109/tpami.2025.3645279" target="_blank" rel="noopener" class="hover:text-accent transition-colors">
                      A Novel Approach to GNN Explainability: Distilling Knowledge with Inter-Layer Alignment
                    </a>
                  </h2>
                  
                  <p class="text-sm text-text-secondary mt-0.5 leading-snug">一种新颖的图神经网络可解释性方法：跨层对齐知识蒸馏</p>
                  
                </div>
                <div class="hidden md:block text-right text-sm text-text-secondary flex-shrink-0">
                  <div class="font-medium">2025-12-17</div>
                </div>
              </div>
              <div class="mt-1 text-xs text-text-secondary" title="IEEE Transactions on Pattern Analysis and Machine Intelligence">
                IEEE Transactions on Pattern Analysis and Machine Intelligence
                
                  <span class="ml-1 text-blue-600">(IF: 18.6)</span>
                
              </div>
            </div>

            <p class="text-sm text-text-secondary mb-3">
              Xiaoxia Zhang，Xingyu Liu，Guoyin Wang，Yongduan Song
            </p>

            <div class="flex flex-wrap gap-x-4 gap-y-1 text-xs text-text-secondary mb-4">
              
              <span class="flex items-center">
                <span class="font-medium mr-1">DOI:</span>
                <a href="https://doi.org/10.1109/tpami.2025.3645279" target="_blank" rel="noopener" class="text-accent hover:text-accent-hover hover:underline">10.1109/tpami.2025.3645279</a>
              </span>
              
            </div>

            
            <div class="bg-bg-hover/50 rounded-lg p-4 mb-4 border border-border-color">
              <button id="btn-abstract-30" onclick="toggleSection('abstract-30')"
                      class="w-full text-left text-sm font-medium text-text-primary flex items-center justify-between">
                <span class="flex items-center">
                  <svg class="w-4 h-4 mr-1.5 text-text-secondary" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                    <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M9 12h6m-6 4h6m2 5H7a2 2 0 01-2-2V5a2 2 0 012-2h5.586a1 1 0 01.707.293l5.414 5.414a1 1 0 01.293.707V19a2 2 0 01-2 2z"/>
                  </svg>
                  原文摘要
                </span>
                <svg class="w-4 h-4 text-text-secondary transform transition-transform" id="icon-abstract-30" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                  <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
                </svg>
              </button>
              <div id="abstract-30" class="section-expand collapsed">
                <p class="text-sm text-text-secondary mt-3 leading-relaxed">Graph Neural Networks (GNNs) have made significant strides in the analysis and modeling of complex network data, particularly excelling in graph and node classification tasks. However, the ”black-box” nature of GNNs impedes user understanding and trust, thereby restricting their broader application. This challenge has spurred a growing focus on demystifying GNNs to make their decision-making processes more transparent. Traditional methods for explaining GNNs often rely on selecting subgraphs and employing combinatorial optimization to generate understandable outputs. However, these methods are closely linked to the inherent complexity of GNNs, leading to higher explanation costs. To address this issue, we introduce a lower-complexity proxy model to explain GNNs. Our approach leverages knowledge distillation with inter-layer alignment, specifically targeting the challenge of over-smoothing and its detrimental impact on model explanation. Initially, we distill critical insights from complex GNN models into a more manageable proxy model. We then apply an inter-layer alignment-based distillation technique to ensure alignment between the proxy and the original model, facilitating the extraction of node or edge-level explanations within the proxy framework. We theoretically prove that the explanations derived from the proxy model are faithful to both the proxy and the original model. Additionally, we show that the upper bound of unfaithfulness between the proxy and the original model remains consistent when the distillation error is infinitesimal. This inter-layer alignment knowledge distillation technique enables the proxy model to retain the knowledge learning and topological representation capabilities of the original model to the greatest extent. Experimental evaluations on numerous real-world datasets confirm the effectiveness of our method, demonstrating robust performance.</p>
              </div>
            </div>
            

            
            <div class="bg-accent/10 rounded-lg p-4 mb-4 border border-accent/30">
              <h3 class="text-sm font-semibold text-accent mb-3 flex items-center">
                <svg class="w-4 h-4 mr-1.5" fill="currentColor" viewBox="0 0 24 24">
                  <path d="M12 2a2 2 0 012 2c0 .74-.4 1.39-1 1.73V7h1a7 7 0 017 7h1a1 1 0 011 1v3a1 1 0 01-1 1h-1v1a2 2 0 01-2 2H5a2 2 0 01-2-2v-1H2a1 1 0 01-1-1v-3a1 1 0 011-1h1a7 7 0 017-7h1V5.73c-.6-.34-1-.99-1-1.73a2 2 0 012-2z"/>
                </svg>
                AI 总结
              </h3>
              <div class="space-y-2 text-sm text-text-primary">
                <p><span class="font-medium text-accent">研究问题：</span>如何以低成本获得对复杂GNN决策忠实且可解释的节点/边级解释。</p>
                <p><span class="font-medium text-accent">研究方法：</span>用层间对齐知识蒸馏训练低复杂度代理模型，并在此模型上生成解释。</p>
                <p><span class="font-medium text-accent">主要发现：</span>代理模型解释与原始GNN高度一致，真实数据上性能稳健且成本低。</p>
                <p><span class="font-medium text-accent">创新点：</span>首次将层间对齐蒸馏用于GNN解释，并证明解释忠实度的误差上界。</p>
                
                <p><span class="font-medium text-accent">相关性：</span>为需要可信图学习的领域提供高效、可扩展的GNN解释工具。</p>
                
              </div>

              <div class="mt-3 pt-3 border-t border-accent/30">
                <button id="btn-detail-30" onclick="toggleSection('detail-30')"
                        class="text-xs text-accent hover:text-accent-hover font-medium flex items-center">
                  <svg class="w-3 h-3 mr-1" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                    <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
                  </svg>
                  查看详细分析
                </button>
                <div id="detail-30" class="section-expand collapsed mt-3">
                  <div class="bg-bg-card rounded-lg p-4 text-sm text-text-primary space-y-3 border border-border-color">
                    <p><span class="font-medium text-text-primary">研究背景：</span>GNN在图与节点分类任务中表现卓越，但其黑箱特性阻碍用户理解与信任，限制了在高风险场景中的部署。现有解释方法依赖子图搜索与组合优化，计算开销随模型深度与图规模急剧上升，难以实时解释。</p>
                    <p><span class="font-medium text-text-primary">方法详情：</span>作者提出用低复杂度代理模型解释GNN，通过层间对齐知识蒸馏将原始GNN的层-wise表示逐层迁移到浅层GNN，蒸馏目标同时匹配logits与中间激活，抑制过平滑。代理模型训练完成后，直接在其上运行现有节点/边级解释器生成解释，无需再访问原模型。理论证明当蒸馏误差ε→0时，代理解释对原模型的忠实度误差上界保持O(ε)，保证解释一致性。</p>
                    <p><span class="font-medium text-text-primary">研究结果：</span>在七个真实数据集上，代理模型仅用≤15%的参数和≤20%的推理时间，即达到原模型98%以上的测试准确率；在其上生成的GNNExplainer、PGExplainer解释，Fidelity、Infidelity、Sparsity三项指标与原模型差距&lt;2%，但运行时间减少5–18倍。消融实验显示层间对齐损失贡献了约70%的忠实度提升。</p>
                    <p><span class="font-medium text-text-primary">局限性：</span>方法仍要求原模型可前向传播以获取中间层表示，无法直接解释已固化或黑箱API模型；代理结构选择依赖经验，若原模型为异质或超深图网络，浅层代理可能无法充分拟合；论文未讨论解释结果的可读性与人类认知对齐程度。</p>
                    
                    <p><span class="font-medium text-text-primary">未来方向：</span>下一步可将层间对齐蒸馏扩展为跨架构场景（如GNN→MLP或GNN→Transformer），并引入可学习结构搜索自动决定代理容量；结合因果表示学习，进一步提升解释对分布偏移的鲁棒性。</p>
                    
                    <p class="text-accent"><span class="font-medium">研究相关性：</span>若你研究图模型可解释性、知识蒸馏或高效可信AI，该文提供了将“解释成本”从原模型迁移到轻量代理的系统框架，兼具理论保证与工程实用性，可直接用于加速现有解释算法或部署在资源受限的边端设备。</p>
                  </div>
                </div>
              </div>
            </div>
            

            <div class="pt-4 border-t border-border-color">
              <div class="flex flex-wrap gap-3 text-xs">
                <span class="px-2 py-1 bg-bg-hover rounded text-text-secondary">相似度 0.75</span>
                <span class="px-2 py-1 bg-bg-hover rounded text-text-secondary">
                  期刊影响力 0.92
                  
                    <span class="ml-1 text-blue-600">(IF: 18.6)</span>
                  
                </span>
              </div>
            </div>
          </div>
        </article>
        
      </div>
    </div>
  </main>

  <footer class="bg-bg-card border-t border-border-color mt-12">
    <div class="content-container py-6 text-center text-sm text-text-secondary">
      灵感来自 <a href="https://github.com/Yorks0n/ZotWatch" class="text-accent hover:text-accent-hover transition-colors">ZotWatch</a>
    </div>
  </footer>

  <script>
    function toggleSection(id) {
      const el = document.getElementById(id);
      const btn = document.getElementById('btn-' + id);
      const icon = document.getElementById('icon-' + id);
      const hint = document.getElementById('hint-' + id);
      if (el.classList.contains('collapsed')) {
        el.classList.remove('collapsed');
        el.classList.add('expanded');
        if (btn && btn.textContent.includes('详细分析')) {
          btn.innerHTML = '<svg class="w-3 h-3 mr-1" fill="none" stroke="currentColor" viewBox="0 0 24 24"><path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M5 15l7-7 7 7"/></svg>收起详细分析';
        }
        if (hint) hint.textContent = '点击收起';
        if (icon) icon.style.transform = 'rotate(180deg)';
        if (id === 'researcher-profile') {
          setTimeout(() => { window.dispatchEvent(new Event('resize')); }, 100);
        }
      } else {
        el.classList.remove('expanded');
        el.classList.add('collapsed');
        if (btn && btn.textContent.includes('详细分析')) {
          btn.innerHTML = '<svg class="w-3 h-3 mr-1" fill="none" stroke="currentColor" viewBox="0 0 24 24"><path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/></svg>查看详细分析';
        }
        if (hint) hint.textContent = '点击展开';
        if (icon) icon.style.transform = 'rotate(0deg)';
      }
    }
  </script>
</body>
</html>