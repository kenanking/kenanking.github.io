<!DOCTYPE html>
<html lang="zh-CN">
<head>
  <meta charset="utf-8">
  <meta name="viewport" content="width=device-width, initial-scale=1">
  <title>ZotWatch 文献推荐 - 2025-12-31</title>
  <script src="https://cdn.tailwindcss.com"></script>
  <script src="https://cdn.jsdelivr.net/npm/chart.js@4.4.1/dist/chart.umd.min.js"></script>
  <script src="https://d3js.org/d3.v7.min.js"></script>
  <script>
    tailwind.config = {
      theme: {
        extend: {
          colors: {
            'bg-primary': '#f8fafc',
            'bg-card': '#ffffff',
            'bg-hover': '#f1f5f9',
            'text-primary': '#1e293b',
            'text-secondary': '#64748b',
            'border-color': '#e2e8f0',
            'accent': '#2563eb',
            'accent-hover': '#1d4ed8',
          }
        }
      }
    }
  </script>
  <style>
    .section-expand { transition: max-height 0.5s ease-out; overflow: hidden; }
    .section-expand.collapsed { max-height: 0; }
    .section-expand.expanded { max-height: none; }
    body { font-family: -apple-system, BlinkMacSystemFont, 'Segoe UI', Roboto, 'Helvetica Neue', Arial, sans-serif; }
    .content-container { max-width: 48rem; margin: 0 auto; padding: 0 1rem; }
    @media (min-width: 768px) { .content-container { padding: 0 2rem; } }
    @media (min-width: 1024px) { .content-container { padding: 0 4rem; } }

    .cluster-graph-container { width: 100%; height: 280px; touch-action: pan-y; }
    @media (min-width: 640px) { .cluster-graph-container { height: 320px; } }
    @media (min-width: 768px) { .cluster-graph-container { height: 350px; } }
    #cluster-graph svg { overflow: visible; }
  </style>
</head>
<body class="bg-bg-primary min-h-screen text-text-primary">
  <header class="bg-bg-card border-b border-border-color">
    <div class="content-container py-6 md:py-8">
      <h1 class="text-xl md:text-2xl font-bold text-text-primary">ZotWatch 文献推荐</h1>
      <p class="text-xs md:text-sm text-text-secondary mt-1">
        共 35 篇论文 ·
        生成于 2025-12-31 10:44 Asia/Shanghai
      </p>
    </div>
  </header>

  
  <section class="py-4 md:py-6 border-b border-border-color">
    <div class="content-container">
      <button id="btn-researcher-profile" onclick="toggleSection('researcher-profile')"
              class="w-full text-left text-lg md:text-xl font-bold text-text-primary mb-3 md:mb-4 flex items-center justify-between group hover:text-accent transition-colors">
        <span class="flex items-center gap-2 flex-1 min-w-0">
          <svg class="w-5 h-5 flex-shrink-0 text-accent" fill="currentColor" viewBox="0 0 24 24">
            <path d="M12 2C6.48 2 2 6.48 2 12s4.48 10 10 10 10-4.48 10-10S17.52 2 12 2zm0 3c1.66 0 3 1.34 3 3s-1.34 3-3 3-3-1.34-3-3 1.34-3 3-3zm0 14.2c-2.5 0-4.71-1.28-6-3.22.03-1.99 4-3.08 6-3.08 1.99 0 5.97 1.09 6 3.08-1.29 1.94-3.5 3.22-6 3.22z"/>
          </svg>
          <span class="truncate">研究兴趣画像</span>
        </span>
        <span class="flex items-center gap-1 md:gap-2 flex-shrink-0 ml-2">
          <span class="hidden md:inline text-sm font-normal text-text-secondary group-hover:text-accent" id="hint-researcher-profile">点击展开</span>
          <svg class="w-5 h-5 md:w-4 md:h-4 text-text-secondary group-hover:text-accent transform transition-transform" id="icon-researcher-profile" fill="none" stroke="currentColor" viewBox="0 0 24 24">
            <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
          </svg>
        </span>
      </button>

      <div id="researcher-profile" class="section-expand collapsed">
      <div class="bg-bg-card rounded-lg border border-border-color p-4 mb-4">
        <div class="grid grid-cols-4 gap-3 text-center">
          <div>
            <div class="text-xl font-bold text-accent">943</div>
            <div class="text-xs text-text-secondary">收藏论文</div>
          </div>
          <div>
            <div class="text-xl font-bold text-accent">5年8月</div>
            <div class="text-xs text-text-secondary">收藏时长</div>
          </div>
          <div>
            <div class="text-xl font-bold text-accent">&gt;9</div>
            <div class="text-xs text-text-secondary">兴趣领域</div>
          </div>
          <div>
            <div class="text-xl font-bold text-accent">8</div>
            <div class="text-xs text-text-secondary">高频作者</div>
          </div>
        </div>
      </div>

      
      <div class="bg-accent/10 rounded-lg border border-accent/30 p-4 mb-4">
        <h3 class="text-sm font-semibold text-accent mb-2 flex items-center gap-2">
          <svg class="w-4 h-4" fill="currentColor" viewBox="0 0 24 24">
            <path d="M12 2a2 2 0 012 2c0 .74-.4 1.39-1 1.73V7h1a7 7 0 017 7h1a1 1 0 011 1v3a1 1 0 01-1 1h-1v1a2 2 0 01-2 2H5a2 2 0 01-2-2v-1H2a1 1 0 01-1-1v-3a1 1 0 011-1h1a7 7 0 017-7h1V5.73c-.6-.34-1-.99-1-1.73a2 2 0 012-2z"/>
          </svg>
          AI 研究兴趣分析
        </h3>
        <div class="space-y-2 text-xs text-text-primary leading-relaxed">
          <p><span class="font-medium text-accent">研究兴趣：</span>用户长期关注计算机视觉与遥感信息处理的交叉问题，核心阅读集中在目标检测、轻量网络与姿态估计等视觉算法，同时对合成孔径雷达（SAR）图像智能解译保持浓厚兴趣。</p>
          <p><span class="font-medium text-accent">深度关注：</span>在SAR目标识别与旋转目标检测方向收藏系统，持续跟踪CVPR、ICCV、IEEE GRSL等顶会顶刊，形成遥感-视觉双主线深度积累；对Kaiming He、Ross Girshick等团队的检测/分割/轻量化工作有近乎全集式收藏，显示出对视觉基础模型演进脉络的深耕。</p>
          <p><span class="font-medium text-accent">跨学科倾向：</span>阅读轨迹明显跨越遥感、计算机视觉与机器学习三大领域，尤其将自然图像的检测/分割新方法快速迁移到SAR图像，并关注大语言模型、扩散模型等通用AI技术在遥感描述与识别中的适配。</p>
          <p><span class="font-medium text-accent">兴趣演变：</span>2025-Q1收藏量激增至91篇，新增关键词聚焦视觉Transformer与SAR图像描述，显示正系统跟进基础模型在遥感场景的落地；同时“大语言模型”“DeepSeek”等词汇首次高频出现，预示其研究兴趣正向“视觉-语言-遥感”多模态融合快速延伸。</p>
          <p><span class="font-medium text-accent">阅读建议：</span>建议关注遥感专用多模态基础模型（如RS-GPT、EarthGPT）与SAR-光学协同的预训练方法，并跟踪低秩/量化压缩技术在星载实时检测中的最新进展。</p>
        </div>
      </div>
      

      <div class="grid grid-cols-1 md:grid-cols-2 gap-4 mb-4">
        
        <div class="bg-bg-card rounded-lg border border-border-color p-4">
          <h4 class="text-sm font-medium text-text-primary mb-3">研究领域分布</h4>
          <div style="height: 180px;">
            <canvas id="domainsChart"></canvas>
          </div>
        </div>
        

        
        <div class="bg-bg-card rounded-lg border border-border-color p-4">
          <h4 class="text-sm font-medium text-text-primary mb-3">阅读趋势（季度）</h4>
          <div style="height: 180px;">
            <canvas id="trendsChart"></canvas>
          </div>
        </div>
        
      </div>

      
      <div class="bg-bg-card rounded-lg border border-border-color p-4 mb-4">
        <h4 class="text-sm font-medium text-text-primary mb-3">论文发表年份分布</h4>
        <div style="height: 180px;">
          <canvas id="yearDistChart"></canvas>
        </div>
      </div>
      

      
      <div class="bg-bg-card rounded-lg border border-border-color p-4 mb-4">
        <h4 class="text-sm font-medium text-text-primary mb-3">
          研究方向聚类图谱
          <span class="text-xs text-text-secondary font-normal ml-2">(30 个聚类)</span>
        </h4>
        <div id="cluster-graph" class="cluster-graph-container"></div>
        <div class="text-xs text-text-secondary mt-3">
          覆盖 919/919 篇论文（仅含摘要的文献参与聚类）
        </div>
      </div>
      

      <div class="grid grid-cols-1 md:grid-cols-2 gap-4 mb-4">
        
        <div class="bg-bg-card rounded-lg border border-border-color p-4">
          <h4 class="text-sm font-medium text-text-primary mb-2">高频关注作者</h4>
          <div class="grid grid-cols-2 gap-x-4 gap-y-1 text-xs">
            
            <div class="flex justify-between truncate">
              <span class="text-text-primary truncate" title="Kaiming He">Kaiming He</span>
              <span class="text-text-secondary ml-1 flex-shrink-0">24</span>
            </div>
            
            <div class="flex justify-between truncate">
              <span class="text-text-primary truncate" title="Ross Girshick">Ross Girshick</span>
              <span class="text-text-secondary ml-1 flex-shrink-0">18</span>
            </div>
            
            <div class="flex justify-between truncate">
              <span class="text-text-primary truncate" title="Jian Sun">Jian Sun</span>
              <span class="text-text-secondary ml-1 flex-shrink-0">15</span>
            </div>
            
            <div class="flex justify-between truncate">
              <span class="text-text-primary truncate" title="Song Han">Song Han</span>
              <span class="text-text-secondary ml-1 flex-shrink-0">14</span>
            </div>
            
            <div class="flex justify-between truncate">
              <span class="text-text-primary truncate" title="Xiangyu Zhang">Xiangyu Zhang</span>
              <span class="text-text-secondary ml-1 flex-shrink-0">14</span>
            </div>
            
            <div class="flex justify-between truncate">
              <span class="text-text-primary truncate" title="Piotr Dollár">Piotr Dollár</span>
              <span class="text-text-secondary ml-1 flex-shrink-0">12</span>
            </div>
            
            <div class="flex justify-between truncate">
              <span class="text-text-primary truncate" title="Li Liu">Li Liu</span>
              <span class="text-text-secondary ml-1 flex-shrink-0">10</span>
            </div>
            
            <div class="flex justify-between truncate">
              <span class="text-text-primary truncate" title="Xiang Li">Xiang Li</span>
              <span class="text-text-secondary ml-1 flex-shrink-0">10</span>
            </div>
            
            <div class="flex justify-between truncate">
              <span class="text-text-primary truncate" title="Yann LeCun">Yann LeCun</span>
              <span class="text-text-secondary ml-1 flex-shrink-0">9</span>
            </div>
            
            <div class="flex justify-between truncate">
              <span class="text-text-primary truncate" title="Feng Xu">Feng Xu</span>
              <span class="text-text-secondary ml-1 flex-shrink-0">9</span>
            </div>
            
            <div class="flex justify-between truncate">
              <span class="text-text-primary truncate" title="Chunhua Shen">Chunhua Shen</span>
              <span class="text-text-secondary ml-1 flex-shrink-0">8</span>
            </div>
            
            <div class="flex justify-between truncate">
              <span class="text-text-primary truncate" title="Saining Xie">Saining Xie</span>
              <span class="text-text-secondary ml-1 flex-shrink-0">8</span>
            </div>
            
          </div>
        </div>
        

        
        <div class="bg-bg-card rounded-lg border border-border-color p-4">
          <h4 class="text-sm font-medium text-text-primary mb-2">常关注期刊/会议</h4>
          <div class="space-y-1 text-xs">
            
            <div class="flex items-center justify-between">
              <span class="text-text-primary truncate flex-1" title="Computer Vision and Pattern Recognition">Computer Vision and Pattern Recognition</span>
              <span class="text-xs px-1 py-0.5 rounded bg-blue-100 text-blue-700 ml-1 flex-shrink-0">
                期刊
              </span>
              <span class="text-text-secondary ml-1 flex-shrink-0">114</span>
            </div>
            
            <div class="flex items-center justify-between">
              <span class="text-text-primary truncate flex-1" title="Neural Information Processing Systems">Neural Information Processing Systems</span>
              <span class="text-xs px-1 py-0.5 rounded bg-blue-100 text-blue-700 ml-1 flex-shrink-0">
                期刊
              </span>
              <span class="text-text-secondary ml-1 flex-shrink-0">45</span>
            </div>
            
            <div class="flex items-center justify-between">
              <span class="text-text-primary truncate flex-1" title="IEEE Transactions on Geoscience and Remote Sensing">IEEE Transactions on Geoscience and Remote Sensing</span>
              <span class="text-xs px-1 py-0.5 rounded bg-blue-100 text-blue-700 ml-1 flex-shrink-0">
                期刊
              </span>
              <span class="text-text-secondary ml-1 flex-shrink-0">44</span>
            </div>
            
            <div class="flex items-center justify-between">
              <span class="text-text-primary truncate flex-1" title="International Conference on Computer Vision">International Conference on Computer Vision</span>
              <span class="text-xs px-1 py-0.5 rounded bg-amber-100 text-amber-700 ml-1 flex-shrink-0">
                会议
              </span>
              <span class="text-text-secondary ml-1 flex-shrink-0">36</span>
            </div>
            
            <div class="flex items-center justify-between">
              <span class="text-text-primary truncate flex-1" title="IEEE Transactions on Pattern Analysis and Machine Intelligence">IEEE Transactions on Pattern Analysis and Machine Intelligence</span>
              <span class="text-xs px-1 py-0.5 rounded bg-blue-100 text-blue-700 ml-1 flex-shrink-0">
                期刊
              </span>
              <span class="text-text-secondary ml-1 flex-shrink-0">32</span>
            </div>
            
            <div class="flex items-center justify-between">
              <span class="text-text-primary truncate flex-1" title="International Conference on Learning Representations">International Conference on Learning Representations</span>
              <span class="text-xs px-1 py-0.5 rounded bg-amber-100 text-amber-700 ml-1 flex-shrink-0">
                会议
              </span>
              <span class="text-text-secondary ml-1 flex-shrink-0">27</span>
            </div>
            
            <div class="flex items-center justify-between">
              <span class="text-text-primary truncate flex-1" title="European Conference on Computer Vision">European Conference on Computer Vision</span>
              <span class="text-xs px-1 py-0.5 rounded bg-amber-100 text-amber-700 ml-1 flex-shrink-0">
                会议
              </span>
              <span class="text-text-secondary ml-1 flex-shrink-0">23</span>
            </div>
            
            <div class="flex items-center justify-between">
              <span class="text-text-primary truncate flex-1" title="IEEE Geoscience and Remote Sensing Letters">IEEE Geoscience and Remote Sensing Letters</span>
              <span class="text-xs px-1 py-0.5 rounded bg-blue-100 text-blue-700 ml-1 flex-shrink-0">
                期刊
              </span>
              <span class="text-text-secondary ml-1 flex-shrink-0">22</span>
            </div>
            
          </div>
        </div>
        
      </div>

      
      <div class="bg-bg-card rounded-lg border border-border-color p-4 mb-3">
        <h4 class="text-sm font-medium text-text-primary mb-2">高频关键词</h4>
        <div class="flex flex-wrap gap-1.5">
          
          <span class="px-2 py-0.5 bg-bg-hover rounded text-xs text-text-primary">
            大语言模型 <span class="text-text-secondary">(14)</span>
          </span>
          
          <span class="px-2 py-0.5 bg-bg-hover rounded text-xs text-text-primary">
            自监督学习 <span class="text-text-secondary">(11)</span>
          </span>
          
          <span class="px-2 py-0.5 bg-bg-hover rounded text-xs text-text-primary">
            综述 <span class="text-text-secondary">(9)</span>
          </span>
          
          <span class="px-2 py-0.5 bg-bg-hover rounded text-xs text-text-primary">
            域自适应 <span class="text-text-secondary">(9)</span>
          </span>
          
          <span class="px-2 py-0.5 bg-bg-hover rounded text-xs text-text-primary">
            合成孔径雷达 <span class="text-text-secondary">(9)</span>
          </span>
          
          <span class="px-2 py-0.5 bg-bg-hover rounded text-xs text-text-primary">
            迁移学习 <span class="text-text-secondary">(7)</span>
          </span>
          
          <span class="px-2 py-0.5 bg-bg-hover rounded text-xs text-text-primary">
            扩散模型 <span class="text-text-secondary">(7)</span>
          </span>
          
          <span class="px-2 py-0.5 bg-bg-hover rounded text-xs text-text-primary">
            强化学习 <span class="text-text-secondary">(7)</span>
          </span>
          
          <span class="px-2 py-0.5 bg-bg-hover rounded text-xs text-text-primary">
            注意力机制 <span class="text-text-secondary">(6)</span>
          </span>
          
          <span class="px-2 py-0.5 bg-bg-hover rounded text-xs text-text-primary">
            重参数化 <span class="text-text-secondary">(6)</span>
          </span>
          
          <span class="px-2 py-0.5 bg-bg-hover rounded text-xs text-text-primary">
            DeepSeek <span class="text-text-secondary">(6)</span>
          </span>
          
          <span class="px-2 py-0.5 bg-bg-hover rounded text-xs text-text-primary">
            生成对抗网络 <span class="text-text-secondary">(6)</span>
          </span>
          
          <span class="px-2 py-0.5 bg-bg-hover rounded text-xs text-text-primary">
            Transformers <span class="text-text-secondary">(5)</span>
          </span>
          
          <span class="px-2 py-0.5 bg-bg-hover rounded text-xs text-text-primary">
            旋转目标检测 <span class="text-text-secondary">(5)</span>
          </span>
          
          <span class="px-2 py-0.5 bg-bg-hover rounded text-xs text-text-primary">
            SAR目标识别 <span class="text-text-secondary">(5)</span>
          </span>
          
          <span class="px-2 py-0.5 bg-bg-hover rounded text-xs text-text-primary">
            特征可视化 <span class="text-text-secondary">(5)</span>
          </span>
          
          <span class="px-2 py-0.5 bg-bg-hover rounded text-xs text-text-primary">
            Vision Transformers <span class="text-text-secondary">(4)</span>
          </span>
          
          <span class="px-2 py-0.5 bg-bg-hover rounded text-xs text-text-primary">
            基础模型 <span class="text-text-secondary">(4)</span>
          </span>
          
          <span class="px-2 py-0.5 bg-bg-hover rounded text-xs text-text-primary">
            HRNet <span class="text-text-secondary">(4)</span>
          </span>
          
          <span class="px-2 py-0.5 bg-bg-hover rounded text-xs text-text-primary">
            目标检测 <span class="text-text-secondary">(4)</span>
          </span>
          
        </div>
      </div>
      

      <div class="text-xs text-text-secondary text-right">
        画像生成于 2025-12-31 10:30 Asia/Shanghai
        · kimi-k2-turbo-preview
      </div>
      </div>
    </div>
  </section>
  

  
  <script>
    document.addEventListener('DOMContentLoaded', function() {
      Chart.defaults.font.family = "-apple-system, BlinkMacSystemFont, 'Segoe UI', Roboto, 'Helvetica Neue', Arial, sans-serif";
      Chart.defaults.font.size = 11;
      Chart.defaults.color = '#64748b';
      Chart.defaults.animation = false;

      
      const domainsCtx = document.getElementById('domainsChart');
      if (domainsCtx) {
        new Chart(domainsCtx, {
          type: 'bar',
          data: {
            labels: ['视觉SLAM', '目标检测', '姿态估计', '人脸识别', '轻量网络', '对比学习', '卫星导航', '特征匹配'],
            datasets: [{
              data: [15, 32, 15, 12, 18, 10, 6, 5],
              backgroundColor: 'rgba(37, 99, 235, 0.7)',
              borderColor: 'rgba(37, 99, 235, 1)',
              borderWidth: 1,
              borderRadius: 2,
              barThickness: 16
            }]
          },
          options: {
            indexAxis: 'y',
            responsive: true,
            maintainAspectRatio: false,
            plugins: {
              legend: { display: false },
              tooltip: { enabled: false }
            },
            scales: {
              x: {
                beginAtZero: true,
                grid: { color: 'rgba(226, 232, 240, 0.5)' },
                ticks: { precision: 0 }
              },
              y: {
                grid: { display: false },
                ticks: { font: { size: 10 } }
              }
            }
          }
        });
      }
      

      
      const trendsCtx = document.getElementById('trendsChart');
      if (trendsCtx) {
        const trends = [{ q: '2023-Q1', c: 52 }, { q: '2023-Q2', c: 18 }, { q: '2023-Q3', c: 21 }, { q: '2023-Q4', c: 19 }, { q: '2024-Q1', c: 67 }, { q: '2024-Q2', c: 18 }, { q: '2024-Q3', c: 4 }, { q: '2024-Q4', c: 24 }, { q: '2025-Q1', c: 91 }, { q: '2025-Q2', c: 34 }, { q: '2025-Q3', c: 13 }, { q: '2025-Q4', c: 29 }];
        const monthColors = [
          { bg: 'rgba(59, 130, 246, 0.7)', border: 'rgba(59, 130, 246, 1)' },
          { bg: 'rgba(16, 185, 129, 0.7)', border: 'rgba(16, 185, 129, 1)' },
          { bg: 'rgba(245, 158, 11, 0.7)', border: 'rgba(245, 158, 11, 1)' }
        ];
        const bgColors = trends.map((t, i) => monthColors[i % 3].bg);
        const borderColors = trends.map((t, i) => monthColors[i % 3].border);
        new Chart(trendsCtx, {
          type: 'bar',
          data: {
            labels: trends.map(t => t.q.replace('-', '\n')),
            datasets: [{
              data: trends.map(t => t.c),
              backgroundColor: bgColors,
              borderColor: borderColors,
              borderWidth: 1,
              borderRadius: 2,
              barThickness: 20
            }]
          },
          options: {
            responsive: true,
            maintainAspectRatio: false,
            plugins: {
              legend: { display: false },
              tooltip: { enabled: false }
            },
            scales: {
              x: {
                grid: { display: false },
                ticks: { font: { size: 9 }, maxRotation: 0 }
              },
              y: {
                beginAtZero: true,
                grid: { color: 'rgba(226, 232, 240, 0.5)' },
                ticks: { precision: 0 }
              }
            }
          }
        });
      }
      

      
      const yearDistCtx = document.getElementById('yearDistChart');
      if (yearDistCtx) {
        const yearData = [{ year: 2006, count: 8 }, { year: 2007, count: 3 }, { year: 2008, count: 2 }, { year: 2009, count: 2 }, { year: 2010, count: 9 }, { year: 2011, count: 10 }, { year: 2012, count: 3 }, { year: 2013, count: 12 }, { year: 2014, count: 14 }, { year: 2015, count: 18 }, { year: 2016, count: 15 }, { year: 2017, count: 41 }, { year: 2018, count: 59 }, { year: 2019, count: 54 }, { year: 2020, count: 66 }, { year: 2021, count: 84 }, { year: 2022, count: 110 }, { year: 2023, count: 110 }, { year: 2024, count: 113 }, { year: 2025, count: 167 }];
        new Chart(yearDistCtx, {
          type: 'line',
          data: {
            labels: yearData.map(d => d.year),
            datasets: [{
              data: yearData.map(d => d.count),
              borderColor: 'rgba(37, 99, 235, 1)',
              backgroundColor: 'rgba(37, 99, 235, 0.1)',
              borderWidth: 2,
              fill: true,
              tension: 0.3,
              pointRadius: 2,
              pointHoverRadius: 4
            }]
          },
          options: {
            responsive: true,
            maintainAspectRatio: false,
            plugins: {
              legend: { display: false },
              tooltip: {
                callbacks: {
                  label: function(context) {
                    return context.parsed.y + ' 篇';
                  }
                }
              }
            },
            scales: {
              x: {
                grid: { display: false },
                ticks: {
                  font: { size: 9 },
                  maxRotation: 45,
                  callback: function(value, index) {
                    const year = this.getLabelForValue(value);
                    return year % 5 === 0 ? year : '';
                  }
                }
              },
              y: {
                beginAtZero: true,
                grid: { color: 'rgba(226, 232, 240, 0.5)' },
                ticks: { precision: 0 }
              }
            }
          }
        });
      }
      

      
      const clusterContainer = document.getElementById('cluster-graph');
      if (clusterContainer) {
        const clusters = [
          
          {
            id: 0,
            label: "SAR\u57df\u81ea\u9002\u5e94\u76ee\u6807\u8bc6\u522b",
            size: 76,
            keywords: ["SAR\u76ee\u6807\u8bc6\u522b", "\u57df\u81ea\u9002\u5e94", "\u5408\u6210\u5b54\u5f84\u96f7\u8fbe"]
          },
          
          {
            id: 1,
            label: "\u5355\u76ee2D/3D\u4eba\u4f53\u59ff\u6001\u4f30\u8ba1",
            size: 65,
            keywords: ["Transformers", "HRNet", "SIFT"]
          },
          
          {
            id: 2,
            label: "\u6df1\u5ea6\u5b66\u4e60\u76ee\u6807\u68c0\u6d4b\u7efc\u8ff0",
            size: 51,
            keywords: ["\u7efc\u8ff0", "\u65cb\u8f6c\u76ee\u6807\u68c0\u6d4b", "\u76ee\u6807\u68c0\u6d4b"]
          },
          
          {
            id: 3,
            label: "\u5143\u5b66\u4e60\u4e0e\u4f18\u5316\u7406\u8bba",
            size: 47,
            keywords: ["\u5f52\u7eb3\u504f\u7f6e", "\u6a21\u578b\u901a\u7528\u6027", "\u7406\u8bba\u57fa\u7840"]
          },
          
          {
            id: 4,
            label: "\u81ea\u76d1\u7763\u89c6\u89c9\u8868\u5f81\u5b66\u4e60",
            size: 46,
            keywords: ["\u81ea\u76d1\u7763\u5b66\u4e60", "\u89c6\u89c9Transformer", "\u8fc1\u79fb\u5b66\u4e60"]
          },
          
          {
            id: 5,
            label: "SAR\u8230\u8239\u68c0\u6d4b",
            size: 45,
            keywords: ["\u5408\u6210\u5b54\u5f84\u96f7\u8fbe", "SAR\u8230\u8239\u68c0\u6d4b", "\u591a\u5c3a\u5ea6\u7279\u5f81\u878d\u5408"]
          },
          
          {
            id: 6,
            label: "\u8f7b\u91cf\u7ea7CNN\u67b6\u6784\u8bbe\u8ba1",
            size: 43,
            keywords: ["\u91cd\u53c2\u6570\u5316", "VGG", "\u795e\u7ecf\u67b6\u6784\u641c\u7d22"]
          },
          
          {
            id: 7,
            label: "\u5bf9\u6bd4\u5b66\u4e60\u4e0e\u57df\u9002\u5e94",
            size: 38,
            keywords: ["\u5bf9\u6bd4\u5b66\u4e60", "\u5206\u5e03\u5916\u68c0\u6d4b", "MoCo"]
          },
          
          {
            id: 8,
            label: "SAR\u6210\u50cf\u4e0e\u5fae\u6ce2\u89c6\u89c9",
            size: 37,
            keywords: ["SAR\u76ee\u6807\u8bc6\u522b", "\u6027\u80fd\u8bc4\u4f30", "\u8f85\u52a9\u8bc6\u522b\u7cfb\u7edf"]
          },
          
          {
            id: 9,
            label: "Vision Transformer\u7efc\u8ff0",
            size: 35,
            keywords: ["Vision Transformers", "Swin Transformer", "\u5377\u79ef\u795e\u7ecf\u7f51\u7edc"]
          },
          
          {
            id: 10,
            label: "\u7ea2\u5916\u5c0f\u76ee\u6807\u68c0\u6d4b",
            size: 34,
            keywords: ["\u7ea2\u5916\u5c0f\u76ee\u6807\u68c0\u6d4b", "\u9065\u611f\u76ee\u6807\u68c0\u6d4b", "\u6837\u672c\u81ea\u9002\u5e94\u589e\u6b96"]
          },
          
          {
            id: 11,
            label: "\u7ea2\u5916\u5f31\u5c0f\u76ee\u6807\u68c0\u6d4b\u8ddf\u8e2a",
            size: 33,
            keywords: ["\u6df1\u5ea6\u5b66\u4e60", "\u7279\u5f81\u878d\u5408", "\u5f31\u5c0f\u76ee\u6807\u8ddf\u8e2a"]
          },
          
          {
            id: 12,
            label: "\u5927\u8bed\u8a00\u6a21\u578b\u6307\u4ee4\u5fae\u8c03",
            size: 32,
            keywords: ["\u5927\u8bed\u8a00\u6a21\u578b", "\u6307\u4ee4\u5fae\u8c03", "Computer Science - Computer Vision and Pattern Recognition"]
          },
          
          {
            id: 13,
            label: "\u8f66\u724c\u68c0\u6d4b\u8bc6\u522b",
            size: 30,
            keywords: ["\u8f66\u724c\u8bc6\u522b", "Internet of Things", "Microcontrollers"]
          },
          
          {
            id: 14,
            label: "\u6269\u6563\u6a21\u578b\u56fe\u50cf\u751f\u6210",
            size: 30,
            keywords: ["\u6269\u6563\u6a21\u578b", "\u56fe\u50cf\u6062\u590d", "\u611f\u77e5-\u5931\u771f\u6743\u8861"]
          },
          
          {
            id: 15,
            label: "\u6df7\u5408\u4e13\u5bb6\u5927\u6a21\u578b",
            size: 29,
            keywords: ["\u6df7\u5408\u4e13\u5bb6\u6a21\u578b", "DeepSeek", "\u5206\u5e03\u5f0f\u8bad\u7ec3"]
          },
          
          {
            id: 16,
            label: "\u591a\u4f20\u611f\u5668\u5168\u5c40\u4f4d\u59ff\u4f30\u8ba1",
            size: 29,
            keywords: []
          },
          
          {
            id: 17,
            label: "\u6df1\u5ea6\u5b66\u4e60\u57fa\u7840\u4e0e\u53ef\u89c6\u5316",
            size: 27,
            keywords: ["\u8bbe\u8ba1\u6a21\u5f0f", "\u5e95\u5c42\u7b97\u6cd5", "\u6027\u80fd\u4f18\u5316"]
          },
          
          {
            id: 18,
            label: "\u795e\u7ecf\u7f51\u7edc\u91cf\u5316\u538b\u7f29",
            size: 26,
            keywords: ["\u6a21\u578b\u538b\u7f29", "\u6a21\u578b\u91cf\u5316", "\u6574\u6570\u63a8\u7406"]
          },
          
          {
            id: 19,
            label: "\u751f\u6210\u5f0f\u6269\u6563\u6a21\u578b",
            size: 24,
            keywords: ["\u6807\u51c6\u5316\u6d41", "\u6d41\u6a21\u578b", "\u751f\u6210\u5bf9\u6297\u7f51\u7edc"]
          },
          
          {
            id: 20,
            label: "\u5b9e\u65f6Transformer\u76ee\u6807\u68c0\u6d4b",
            size: 24,
            keywords: ["\u5b9e\u65f6\u76ee\u6807\u68c0\u6d4b", "DETR", "Uplink"]
          },
          
          {
            id: 21,
            label: "\u96f7\u8fbe\u591a\u4efb\u52a1\u5b66\u4e60",
            size: 22,
            keywords: ["Adapter Branch", "Neural Architecture Search", "Objection Detection"]
          },
          
          {
            id: 22,
            label: "\u9ad8\u5206\u8fa8\u7387\u8bed\u4e49\u5206\u5272",
            size: 17,
            keywords: ["U-Net\u7f51\u7edc", "\u533b\u5b66\u56fe\u50cf\u5904\u7406", "\u56fe\u50cf\u5206\u5272"]
          },
          
          {
            id: 23,
            label: "SAM\u901a\u7528\u5206\u5272",
            size: 16,
            keywords: ["\u57fa\u7840\u6a21\u578b", "\u901a\u7528\u5206\u5272", "\u96f6\u6837\u672c\u5206\u5272"]
          },
          
          {
            id: 24,
            label: "DeepSeek\u63a8\u7406\u5f3a\u5316\u5b66\u4e60",
            size: 15,
            keywords: ["\u5927\u8bed\u8a00\u6a21\u578b", "\u5f3a\u5316\u5b66\u4e60", "DeepSeek"]
          },
          
          {
            id: 25,
            label: "TinyML\u5fae\u63a7\u5236\u5668AI",
            size: 15,
            keywords: ["\u6a21\u578b\u538b\u7f29", "\u77e5\u8bc6\u84b8\u998f", "\u7efc\u8ff0"]
          },
          
          {
            id: 26,
            label: "\u5b66\u672f\u51fa\u7248\u540c\u884c\u8bc4\u8bae",
            size: 12,
            keywords: ["LaTeX", "\u7814\u7a76", "\u5bb6\u5ead\u66b4\u529b"]
          },
          
          {
            id: 27,
            label: "\u6982\u7387\u7edf\u8ba1\u57fa\u7840",
            size: 9,
            keywords: []
          },
          
          {
            id: 28,
            label: "\u96f7\u8fbe\u6297\u5e72\u6270\u4e0e\u9274\u522b",
            size: 7,
            keywords: ["\u5355\u8f7d\u9891\u8109\u51b2", "\u652f\u6301\u5411\u91cf\u673a", "\u6781\u5316\u8c03\u63a7"]
          },
          
          {
            id: 29,
            label: "\u53ef\u89e3\u91ca\u6027CAM\u53ef\u89c6\u5316",
            size: 5,
            keywords: ["\u7279\u5f81\u53ef\u89c6\u5316", "Grad-CAM", "Ablation-CAM"]
          }
          
        ];

        const links = [{"source": 6, "target": 18, "value": 0.8641564591035383}, {"source": 15, "target": 24, "value": 0.9121838985736391}, {"source": 4, "target": 9, "value": 0.9338022631506198}, {"source": 7, "target": 29, "value": 0.8415178539068611}, {"source": 14, "target": 19, "value": 0.9388053716105943}, {"source": 3, "target": 19, "value": 0.8956419128404857}, {"source": 0, "target": 5, "value": 0.9415789205502394}, {"source": 0, "target": 8, "value": 0.9531609338727101}, {"source": 8, "target": 21, "value": 0.8999349908439309}, {"source": 5, "target": 28, "value": 0.8933357601921071}, {"source": 9, "target": 20, "value": 0.905134333889317}, {"source": 17, "target": 27, "value": 0.8611528839237513}, {"source": 2, "target": 20, "value": 0.9395660876950154}, {"source": 6, "target": 17, "value": 0.9078533310101574}, {"source": 18, "target": 25, "value": 0.907589137576963}, {"source": 12, "target": 15, "value": 0.9367231192855225}, {"source": 3, "target": 6, "value": 0.904994526871208}, {"source": 6, "target": 29, "value": 0.8413289199697722}, {"source": 12, "target": 24, "value": 0.9288570208525567}, {"source": 10, "target": 11, "value": 0.9067760080561241}, {"source": 4, "target": 23, "value": 0.8560688588188748}, {"source": 0, "target": 10, "value": 0.9117085657853953}, {"source": 17, "target": 26, "value": 0.8611439733469336}, {"source": 3, "target": 27, "value": 0.8137165110938924}, {"source": 2, "target": 10, "value": 0.9277541228271305}, {"source": 2, "target": 13, "value": 0.8648321171233538}, {"source": 11, "target": 28, "value": 0.9119227972742877}, {"source": 7, "target": 9, "value": 0.9268305933027406}, {"source": 1, "target": 23, "value": 0.8705219441898064}, {"source": 16, "target": 21, "value": 0.8216837758786104}, {"source": 6, "target": 25, "value": 0.8845164741774859}, {"source": 4, "target": 7, "value": 0.9461759366123602}, {"source": 6, "target": 22, "value": 0.8748382562351998}, {"source": 3, "target": 17, "value": 0.9235680543090299}, {"source": 5, "target": 8, "value": 0.9050559083500562}, {"source": 12, "target": 26, "value": 0.8165917853612987}, {"source": 4, "target": 22, "value": 0.8639601302800063}, {"source": 1, "target": 4, "value": 0.9047939216781233}, {"source": 0, "target": 21, "value": 0.9124855125685336}, {"source": 1, "target": 16, "value": 0.9146447956964047}, {"source": 13, "target": 21, "value": 0.8673364542384332}, {"source": 6, "target": 9, "value": 0.9144526944763923}, {"source": 7, "target": 14, "value": 0.8985548491714086}];

        const academicColors = [
          '#4E79A7', '#F28E2B', '#E15759', '#76B7B2', '#59A14F',
          '#EDC948', '#B07AA1', '#FF9DA7', '#9C755F', '#BAB0AC',
          '#86BCB6', '#D37295', '#8CD17D', '#499894', '#FABFD2',
          '#79706E', '#D4A6C8', '#9D7660', '#B6992D', '#A0CBE8'
        ];
        const colorScale = (i) => academicColors[i % academicColors.length];

        let svg, simulation, node, link, sizeScale;
        let lastWidth = 0;
        let simulationStopped = false;

        function createGraph() {
          const width = clusterContainer.clientWidth;
          const height = clusterContainer.clientHeight || 280;
          const isMobile = width < 500;

          lastWidth = width;
          clusterContainer.innerHTML = '';

          const minRadius = isMobile ? 16 : 20;
          const maxRadius = isMobile ? 38 : 48;
          sizeScale = d3.scaleSqrt()
            .domain([0, d3.max(clusters, d => d.size)])
            .range([minRadius, maxRadius]);

          svg = d3.select('#cluster-graph')
            .append('svg')
            .attr('width', width)
            .attr('height', height)
            .attr('viewBox', [0, 0, width, height]);

          // Create layered groups for z-order control
          // Order (bottom to top): dimmed links -> dimmed nodes -> highlighted links -> highlighted nodes
          const layerDimmedLinks = svg.append('g').attr('class', 'layer-dimmed-links');
          const layerDimmedNodes = svg.append('g').attr('class', 'layer-dimmed-nodes');
          const layerHighlightedLinks = svg.append('g').attr('class', 'layer-highlighted-links');
          const layerHighlightedNodes = svg.append('g').attr('class', 'layer-highlighted-nodes');

          const chargeStrength = isMobile ? -100 : -150;
          const linkDistance = isMobile ? 60 : 90;

          simulationStopped = false;
          simulation = d3.forceSimulation(clusters)
            .force('link', d3.forceLink(links).id(d => d.id).distance(linkDistance).strength(0.2))
            .force('charge', d3.forceManyBody().strength(chargeStrength))
            .force('center', d3.forceCenter(width / 2, height / 2))
            .force('collision', d3.forceCollide().radius(d => sizeScale(d.size) + (isMobile ? 4 : 6)))
            .velocityDecay(0.7)
            .alphaDecay(0.08)
            .alphaMin(0.01);

          // Initially all links in dimmed layer
          link = layerDimmedLinks
            .attr('stroke', '#e2e8f0')
            .attr('stroke-opacity', 0.6)
            .selectAll('line')
            .data(links)
            .join('line')
            .attr('stroke-width', d => Math.max(1, d.value * 1.5));

          // Initially all nodes in dimmed layer
          node = layerDimmedNodes
            .selectAll('g')
            .data(clusters)
            .join('g')
            .style('cursor', 'grab');

          node.append('circle')
            .attr('r', d => sizeScale(d.size))
            .attr('fill', (d, i) => colorScale(i))
            .attr('fill-opacity', 0.85)
            .attr('stroke', '#fff')
            .attr('stroke-width', isMobile ? 1.5 : 2);

          const fontSize = isMobile ? 7 : 8;
          const lineHeight = isMobile ? 8 : 9;
          const charWidth = isMobile ? 5 : 5.5;

          function wrapText(text, radius) {
            text.each(function() {
              const textEl = d3.select(this);
              const label = textEl.text();
              const maxWidth = radius * 1.2;

              textEl.text(null);

              let lines = [];
              let currentLine = '';
              for (let char of label) {
                if ((currentLine.length + 1) * charWidth > maxWidth && currentLine) {
                  lines.push(currentLine);
                  currentLine = char;
                } else {
                  currentLine += char;
                }
              }
              if (currentLine) lines.push(currentLine);

              if (lines.length > 2) {
                lines = lines.slice(0, 2);
                lines[1] = lines[1].substring(0, Math.max(0, lines[1].length - 1)) + '..';
              }

              const startY = -(lines.length - 1) * lineHeight / 2;

              lines.forEach((line, i) => {
                textEl.append('tspan')
                  .attr('x', 0)
                  .attr('dy', i === 0 ? startY : lineHeight)
                  .text(line);
              });
            });
          }

          node.append('text')
            .text(d => d.label)
            .attr('text-anchor', 'middle')
            .attr('dominant-baseline', 'middle')
            .attr('font-size', fontSize + 'px')
            .attr('font-weight', '500')
            .attr('fill', '#fff')
            .attr('pointer-events', 'none')
            .each(function(d) {
              wrapText(d3.select(this), sizeScale(d.size));
            });

          node.append('text')
            .text(d => `(${d.size}篇)`)
            .attr('text-anchor', 'middle')
            .attr('dy', d => sizeScale(d.size) + (isMobile ? 10 : 12))
            .attr('font-size', fontSize + 'px')
            .attr('fill', '#94a3b8');

          node.append('title')
            .text(d => `${d.label}\n论文数: ${d.size}\n关键词: ${d.keywords.join(', ')}`);

          const drag = d3.drag()
            .on('start', dragstarted)
            .on('drag', dragged)
            .on('end', dragended);

          node.call(drag);

          simulation.on('tick', () => {
            node.attr('transform', d => {
              const r = sizeScale(d.size);
              const padding = isMobile ? 5 : 8;
              const maxY = height - r - (isMobile ? 12 : 15);
              d.x = Math.max(r + padding, Math.min(width - r - padding, d.x));
              d.y = Math.max(r + padding, Math.min(maxY, d.y));
              return `translate(${d.x},${d.y})`;
            });

            link
              .attr('x1', d => d.source.x)
              .attr('y1', d => d.source.y)
              .attr('x2', d => d.target.x)
              .attr('y2', d => d.target.y);
          });

          simulation.on('end', () => {
            simulationStopped = true;
          });

          function dragstarted(event, d) {
            if (!event.active) simulation.alphaTarget(0.05).restart();
            d.fx = d.x;
            d.fy = d.y;
            highlightNode(d);
            d3.select(this).style('cursor', 'grabbing');
          }

          function dragged(event, d) {
            const r = sizeScale(d.size);
            const padding = isMobile ? 5 : 8;
            const maxY = height - r - (isMobile ? 12 : 15);
            d.fx = Math.max(r + padding, Math.min(width - r - padding, event.x));
            d.fy = Math.max(r + padding, Math.min(maxY, event.y));
          }

          function dragended(event, d) {
            if (!event.active) simulation.alphaTarget(0);
            d.fx = null;
            d.fy = null;
            d3.select(this).style('cursor', 'grab');
            resetHighlight();
          }

          function highlightNode(selectedNode) {
            const connectedIds = new Set();

            links.forEach(l => {
              if (l.source.id === selectedNode.id) connectedIds.add(l.target.id);
              if (l.target.id === selectedNode.id) connectedIds.add(l.source.id);
            });

            // Step 1: Style dimmed elements
            link
              .attr('stroke-opacity', 0.1)
              .attr('stroke', '#cbd5e1')
              .attr('stroke-width', d => Math.max(1, d.value * 1.5));

            node.each(function(d) {
              d3.select(this).select('circle')
                .attr('fill-opacity', 0.25)
                .attr('stroke', '#fff')
                .attr('stroke-width', isMobile ? 1.5 : 2);

              d3.select(this).selectAll('text')
                .attr('opacity', 0.3);
            });

            // Step 2: Move dimmed elements to bottom layers
            link.each(function() {
              layerDimmedLinks.node().appendChild(this);
            });
            node.each(function() {
              layerDimmedNodes.node().appendChild(this);
            });

            // Step 3: Style and move highlighted links to top
            link.filter(l => l.source.id === selectedNode.id || l.target.id === selectedNode.id)
              .attr('stroke-opacity', 1)
              .attr('stroke', '#2563eb')
              .attr('stroke-width', d => Math.max(2.5, d.value * 3))
              .each(function() {
                layerHighlightedLinks.node().appendChild(this);
              });

            // Step 4: Style and move connected nodes to top
            node.filter(d => connectedIds.has(d.id))
              .each(function(d) {
                d3.select(this).select('circle')
                  .attr('fill-opacity', 1)
                  .attr('stroke', '#60a5fa')
                  .attr('stroke-width', isMobile ? 2.5 : 3);

                d3.select(this).selectAll('text')
                  .attr('opacity', 1);

                layerHighlightedNodes.node().appendChild(this);
              });

            // Step 5: Style and move selected node to very top
            node.filter(d => d.id === selectedNode.id)
              .each(function(d) {
                d3.select(this).select('circle')
                  .attr('fill-opacity', 1)
                  .attr('stroke', '#2563eb')
                  .attr('stroke-width', isMobile ? 3 : 4);

                d3.select(this).selectAll('text')
                  .attr('opacity', 1);

                layerHighlightedNodes.node().appendChild(this);
              });
          }

          function resetHighlight() {
            // Move all elements back to dimmed layers
            link.each(function() {
              layerDimmedLinks.node().appendChild(this);
            });

            node.each(function() {
              layerDimmedNodes.node().appendChild(this);
            });

            // Reset styles
            node.select('circle')
              .attr('fill-opacity', 0.85)
              .attr('stroke', '#fff')
              .attr('stroke-width', isMobile ? 1.5 : 2)
              .style('filter', 'none');

            link
              .attr('stroke-opacity', 0.6)
              .attr('stroke', '#e2e8f0')
              .attr('stroke-width', d => Math.max(1, d.value * 1.5));

            node.selectAll('text')
              .attr('opacity', 1);
          }
        }

        createGraph();

        let resizeTimeout;
        window.addEventListener('resize', () => {
          clearTimeout(resizeTimeout);
          resizeTimeout = setTimeout(() => {
            const newWidth = clusterContainer.clientWidth;
            if (Math.abs(newWidth - lastWidth) > 10) {
              clusters.forEach(d => { d.x = undefined; d.y = undefined; d.fx = null; d.fy = null; });
              createGraph();
            }
          }, 250);
        });
      }
      
    });
  </script>
  

  
  <section class="py-5 md:py-8 border-b border-border-color">
    <div class="content-container">
      <button id="btn-overall-summaries" onclick="toggleSection('overall-summaries')"
              class="w-full text-left text-lg md:text-xl font-bold text-text-primary mb-3 md:mb-4 flex items-center justify-between group hover:text-accent transition-colors">
        <span class="flex items-center gap-2 flex-1 min-w-0">
          <svg class="w-5 h-5 flex-shrink-0 text-accent" fill="currentColor" viewBox="0 0 24 24">
            <path d="M12 2a2 2 0 012 2c0 .74-.4 1.39-1 1.73V7h1a7 7 0 017 7h1a1 1 0 011 1v3a1 1 0 01-1 1h-1v1a2 2 0 01-2 2H5a2 2 0 01-2-2v-1H2a1 1 0 01-1-1v-3a1 1 0 011-1h1a7 7 0 017-7h1V5.73c-.6-.34-1-.99-1-1.73a2 2 0 012-2z"/>
          </svg>
          <span class="truncate">本期研究趋势概览</span>
        </span>
        <span class="flex items-center gap-1 md:gap-2 flex-shrink-0 ml-2">
          <span class="hidden md:inline text-sm font-normal text-text-secondary group-hover:text-accent" id="hint-overall-summaries">点击收起</span>
          <svg class="w-5 h-5 md:w-4 md:h-4 text-text-secondary group-hover:text-accent transform transition-transform rotate-180" id="icon-overall-summaries" fill="none" stroke="currentColor" viewBox="0 0 24 24">
            <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
          </svg>
        </span>
      </button>

      <div id="overall-summaries" class="section-expand expanded">
      <div class="space-y-4 md:space-y-6">
        
        <div class="bg-bg-card rounded-lg border border-border-color p-4 md:p-5">
          <h3 class="text-base font-semibold text-text-primary mb-3 flex items-center gap-2">
            <svg class="w-4 h-4 text-accent" fill="currentColor" viewBox="0 0 20 20">
              <path d="M9.049 2.927c.3-.921 1.603-.921 1.902 0l1.07 3.292a1 1 0 00.95.69h3.462c.969 0 1.371 1.24.588 1.81l-2.8 2.034a1 1 0 00-.364 1.118l1.07 3.292c.3.921-.755 1.688-1.54 1.118l-2.8-2.034a1 1 0 00-1.175 0l-2.8 2.034c-.784.57-1.838-.197-1.539-1.118l1.07-3.292a1 1 0 00-.364-1.118L2.98 8.72c-.783-.57-.38-1.81.588-1.81h3.461a1 1 0 00.951-.69l1.07-3.292z"/>
            </svg>
            精选推荐总结
          </h3>
          <div class="text-sm text-text-primary leading-relaxed space-y-2">
            <p>本期推荐涵盖了3篇关于光学-SAR融合的论文、1篇关于联邦学习SAR识别的论文和1篇关于半监督分割的论文。</p>
            
            <p><strong class="text-accent">光学-SAR融合</strong>：该主题聚焦光学与SAR异构影像的互补融合，以提升检测与分类精度。《Towards Robust Optical-SAR Object Detection under Missing Modalities》提出动态质量感知融合框架解决模态缺失下的鲁棒检测；《FDPFNet》设计频域渐进融合网络实现多标签场景分类；《Multimodal Interpretation of Remote Sensing Images》通过动态分辨率输入与多尺度视觉-语言对齐机制增强跨模态理解。</p>
            
            <p><strong class="text-accent">联邦SAR识别</strong>：针对多站SAR目标识别中的数据孤岛与异构性，《FedC-DAC》提出联邦聚类动态聚合与校准方法，在保护隐私的同时提升分布式训练性能。</p>
            
            <p><strong class="text-accent">半监督分割</strong>：为缓解遥感影像标注负担并抑制伪标签漂移，《Toward Stable Semi-Supervised Remote Sensing Segmentation via Co-Guidance and Co-Fusion》引入协同指导与协同融合策略，实现稳定的半监督语义分割。</p>
            
          </div>
        </div>
        

        
        <div class="bg-bg-card rounded-lg border border-border-color p-4 md:p-5">
          <h3 class="text-base font-semibold text-text-primary mb-3 flex items-center gap-2">
            <svg class="w-4 h-4 text-text-secondary" fill="none" stroke="currentColor" viewBox="0 0 24 24">
              <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M9 19v-6a2 2 0 00-2-2H5a2 2 0 00-2 2v6a2 2 0 002 2h2a2 2 0 002-2zm0 0V9a2 2 0 012-2h2a2 2 0 012 2v10m-6 0a2 2 0 002 2h2a2 2 0 002-2m0 0V5a2 2 0 012-2h2a2 2 0 012 2v14a2 2 0 01-2 2h-2a2 2 0 01-2-2z"/>
            </svg>
            相似度推荐总结
          </h3>
          <div class="text-sm text-text-primary leading-relaxed space-y-2">
            <p>本期推荐涵盖了9篇关于域适应/泛化的论文、6篇关于小样本/异常检测的论文、5篇关于遥感影像理解的论文、4篇关于实时目标检测的论文、3篇关于医学/生理信号生成的论文、2篇关于轻量化网络的论文与1篇关于合成-真实迁移的论文。</p>
            
            <p><strong class="text-text-secondary">域适应泛化</strong>：该主题聚焦跨域鲁棒表征，信息论方法提升UDA的可迁移性与判别性，同时《CrossEarth》提出地理空间基础模型实现遥感跨域语义分割泛化，并涵盖多模态对齐与风格解耦策略。</p>
            
            <p><strong class="text-text-secondary">小样本检测</strong>：研究在极少量正常样本下定位多类工业缺陷，《FocusPatch AD》以统一关键词补丁提示实现小样本异常检测；同时《Few-Shot Fine-Grained Classification》通过前景感知核化特征重构提升细粒度分类精度。</p>
            
            <p><strong class="text-text-secondary">遥感理解</strong>：面向多源遥感数据，论文探索光学-SAR融合分类、航空影像目标检测及跨传感器泛化，其中《FDPFNet》在频域逐步融合双模态信息，《Foundation Model-based Auxiliary Framework》将基础模型知识迁移至航空场景检测。</p>
            
            <p><strong class="text-text-secondary">实时检测</strong>：针对边缘实时需求，改进YOLO架构与计算范式，《YOLO-Master》引入MOE加速与专用Transformer，在保持速度的同时显著提升检测精度。</p>
            
            <p><strong class="text-text-secondary">医学生成</strong>：利用生成式模型合成生理信号，统一扩散Transformer框架《Versatile cardiovascular signal generation》联合生成PPG、ECG与血压，实现多模态心血管数据增强与缺失插补。</p>
            
            <p><strong class="text-text-secondary">轻量化网络</strong>：面向资源受限场景，设计轻量架构降低计算负担，《LNet》通过场景-注视一致性约束实现驾驶员注意力估计，兼顾精度与效率。</p>
            
            <p><strong class="text-text-secondary">合成真实迁移</strong>：《Exploring Syn-to-Real Domain Adaptation for Military Target Detection》针对军事目标检测中的合成-真实域差异，提出专用适应策略以提升实战部署可靠性。</p>
            
          </div>
        </div>
        
      </div>
      </div>
    </div>
  </section>
  

  
  <section class="py-5 md:py-8 border-b border-border-color">
    <div class="content-container">
      <div class="mb-3 md:mb-4">
        <button id="btn-interest-works" onclick="toggleSection('interest-works')"
                class="w-full text-left text-lg md:text-xl font-bold text-text-primary flex items-center justify-between group hover:text-accent transition-colors">
          <span class="flex items-center gap-2 flex-1 min-w-0">
            <svg class="w-5 h-5 flex-shrink-0 text-accent" fill="currentColor" viewBox="0 0 20 20">
              <path d="M9.049 2.927c.3-.921 1.603-.921 1.902 0l1.07 3.292a1 1 0 00.95.69h3.462c.969 0 1.371 1.24.588 1.81l-2.8 2.034a1 1 0 00-.364 1.118l1.07 3.292c.3.921-.755 1.688-1.54 1.118l-2.8-2.034a1 1 0 00-1.175 0l-2.8 2.034c-.784.57-1.838-.197-1.539-1.118l1.07-3.292a1 1 0 00-.364-1.118L2.98 8.72c-.783-.57-.38-1.81.588-1.81h3.461a1 1 0 00.951-.69l1.07-3.292z"/>
            </svg>
            <span class="truncate">精选推荐</span>
          </span>
          <span class="flex items-center gap-1 md:gap-2 flex-shrink-0 ml-2">
            <span class="hidden md:inline text-xs font-normal text-text-secondary">基于研究兴趣匹配，共 5 篇</span>
            <span class="hidden md:inline text-sm font-normal text-text-secondary group-hover:text-accent" id="hint-interest-works">点击收起</span>
            <svg class="w-5 h-5 md:w-4 md:h-4 text-text-secondary group-hover:text-accent transform transition-transform rotate-180" id="icon-interest-works" fill="none" stroke="currentColor" viewBox="0 0 24 24">
              <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
            </svg>
          </span>
        </button>
        <p class="text-xs text-text-secondary mt-1 ml-7 md:hidden">基于研究兴趣匹配，共 5 篇</p>
      </div>

      <div id="interest-works" class="section-expand expanded">
      <div class="space-y-4 md:space-y-5">
        
        <article class="bg-bg-card rounded-lg border border-border-color overflow-hidden hover:border-accent/50 transition-colors">
          <div class="p-4 md:p-5">
            <div class="mb-3">
              <div class="flex items-center gap-2 mb-2 flex-wrap">
                <span class="inline-flex items-center px-2.5 py-0.5 rounded text-xs font-medium bg-accent/10 text-accent border border-accent/30">
                  匹配度 53%
                </span>
                <span class="text-xs text-text-secondary">arxiv</span>
                <span class="text-xs text-text-secondary md:hidden">· 2025-12-27</span>
              </div>
              <div class="flex items-start justify-between gap-3 md:gap-4">
                <div class="flex-1">
                  <h2 class="text-base md:text-lg font-semibold text-text-primary leading-tight">
                    <a href="https://arxiv.org/abs/2512.22447v1" target="_blank" rel="noopener" class="hover:text-accent transition-colors">
                      Towards Robust Optical-SAR Object Detection under Missing Modalities: A Dynamic Quality-Aware Fusion Framework
                    </a>
                  </h2>
                  
                  <p class="text-sm text-text-secondary mt-0.5 leading-snug">缺失模态下鲁棒光学-SAR目标检测：动态质量感知融合框架</p>
                  
                </div>
                <div class="hidden md:block text-right text-sm text-text-secondary flex-shrink-0">
                  <div class="font-medium">2025-12-27</div>
                </div>
              </div>
              <div class="mt-1 text-xs text-text-secondary" title="arXiv">
                arXiv
                
              </div>
            </div>

            <p class="text-sm text-text-secondary mb-3">
              Zhicheng Zhao，Yuancheng Xu，Andong Lu，Chenglong Li，Jin Tang
            </p>

            <div class="flex flex-wrap gap-x-4 gap-y-1 text-xs text-text-secondary mb-4">
              
              <span class="flex items-center">
                <span class="font-medium mr-1">ID:</span>
                <span>http://arxiv.org/abs/2512.22447v1</span>
              </span>
              
            </div>

            
            <div class="bg-bg-hover/50 rounded-lg p-4 mb-4 border border-border-color">
              <button id="btn-interest-abstract-1" onclick="toggleSection('interest-abstract-1')"
                      class="w-full text-left text-sm font-medium text-text-primary flex items-center justify-between">
                <span class="flex items-center">
                  <svg class="w-4 h-4 mr-1.5 text-text-secondary" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                    <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M9 12h6m-6 4h6m2 5H7a2 2 0 01-2-2V5a2 2 0 012-2h5.586a1 1 0 01.707.293l5.414 5.414a1 1 0 01.293.707V19a2 2 0 01-2 2z"/>
                  </svg>
                  原文摘要
                </span>
                <svg class="w-4 h-4 text-text-secondary transform transition-transform" id="icon-interest-abstract-1" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                  <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
                </svg>
              </button>
              <div id="interest-abstract-1" class="section-expand collapsed">
                <p class="text-sm text-text-secondary mt-3 leading-relaxed">Optical and Synthetic Aperture Radar (SAR) fusion-based object detection has attracted significant research interest in remote sensing, as these modalities provide complementary information for all-weather monitoring. However, practical deployment is severely limited by inherent challenges. Due to distinct imaging mechanisms, temporal asynchrony, and registration difficulties, obtaining well-aligned optical-SAR image pairs remains extremely difficult, frequently resulting in missing or degraded modality data. Although recent approaches have attempted to address this issue, they still suffer from limited robustness to random missing modalities and lack effective mechanisms to ensure consistent performance improvement in fusion-based detection. To address these limitations, we propose a novel Quality-Aware Dynamic Fusion Network (QDFNet) for robust optical-SAR object detection. Our proposed method leverages learnable reference tokens to dynamically assess feature reliability and guide adaptive fusion in the presence of missing modalities. In particular, we design a Dynamic Modality Quality Assessment (DMQA) module that employs learnable reference tokens to iteratively refine feature reliability assessment, enabling precise identification of degraded regions and providing quality guidance for subsequent fusion. Moreover, we develop an Orthogonal Constraint Normalization Fusion (OCNF) module that employs orthogonal constraints to preserve modality independence while dynamically adjusting fusion weights based on reliability scores, effectively suppressing unreliable feature propagation. Extensive experiments on the SpaceNet6-OTD and OGSOD-2.0 datasets demonstrate the superiority and effectiveness of QDFNet compared to state-of-the-art methods, particularly under partial modality corruption or missing data scenarios.</p>
              </div>
            </div>
            

            
            <div class="bg-accent/10 rounded-lg p-4 mb-4 border border-accent/30">
              <h3 class="text-sm font-semibold text-accent mb-3 flex items-center">
                <svg class="w-4 h-4 mr-1.5" fill="currentColor" viewBox="0 0 24 24">
                  <path d="M12 2a2 2 0 012 2c0 .74-.4 1.39-1 1.73V7h1a7 7 0 017 7h1a1 1 0 011 1v3a1 1 0 01-1 1h-1v1a2 2 0 01-2 2H5a2 2 0 01-2-2v-1H2a1 1 0 01-1-1v-3a1 1 0 011-1h1a7 7 0 017-7h1V5.73c-.6-.34-1-.99-1-1.73a2 2 0 012-2z"/>
                </svg>
                AI 总结
              </h3>
              <div class="space-y-2 text-sm text-text-primary">
                <p><span class="font-medium text-accent">研究问题：</span>解决光学-合成孔径雷达目标检测中因模态缺失或退化导致的鲁棒性不足问题。</p>
                <p><span class="font-medium text-accent">研究方法：</span>提出QDFNet，用可学习参考令牌动态评估特征可靠度并自适应融合，含DMQA与OCNF模块。</p>
                <p><span class="font-medium text-accent">主要发现：</span>在SpaceNet6-OTD和OGSOD-2.0上，QDFNet在模态缺失或损坏场景下显著优于现有方法。</p>
                <p><span class="font-medium text-accent">创新点：</span>首次引入可学习参考令牌进行迭代质量评估，并以正交约束动态加权融合，抑制不可靠特征传播。</p>
                
                <p><span class="font-medium text-accent">相关性：</span>为全天候遥感监测提供缺失模态鲁棒检测方案，推动光学-SAR融合实用化。</p>
                
              </div>

              <div class="mt-3 pt-3 border-t border-accent/30">
                <button id="btn-interest-detail-1" onclick="toggleSection('interest-detail-1')"
                        class="text-xs text-accent hover:text-accent-hover font-medium flex items-center">
                  <svg class="w-3 h-3 mr-1" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                    <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
                  </svg>
                  查看详细分析
                </button>
                <div id="interest-detail-1" class="section-expand collapsed mt-3">
                  <div class="bg-bg-card rounded-lg p-4 text-sm text-text-primary space-y-3 border border-border-color">
                    <p><span class="font-medium text-text-primary">研究背景：</span>光学与合成孔径雷达(SAR)融合检测能利用两种模态的互补信息实现全天候监测，但成像机理差异、时相不同步和配准困难导致高质量成对样本稀缺，实际部署中常出现某一模态缺失或降质。现有方法对随机缺失模态的鲁棒性不足，难以在融合阶段持续带来检测增益，严重制约了业务化应用。</p>
                    <p><span class="font-medium text-text-primary">方法详情：</span>作者提出Quality-Aware Dynamic Fusion Network(QDFNet)，通过可学习的reference token在特征层动态评估各模态可靠度并指导自适应融合。核心包含：(1)Dynamic Modality Quality Assessment(DMQA)模块，利用reference token迭代精炼特征质量图，精确定位降质区域并为后续融合提供像素级置信度；(2)Orthogonal Constraint Normalization Fusion(OCNF)模块，在保持模态特征正交独立的同时，依据可靠度得分动态调整融合权重，抑制不可靠特征传播。整体框架无需额外质量标签，端到端训练即可在缺失或腐败模态条件下保持检测性能。</p>
                    <p><span class="font-medium text-text-primary">研究结果：</span>在SpaceNet6-OTD与OGSOD-2.0两个公开光学-SAR检测数据集上的实验表明，QDFNet在完整模态下达到SOTA精度，并在30%-70%随机缺失或条带腐败情形下mAP下降不超过2.1%，显著优于现有鲁棒融合方法(下降4-9%)。可视化质量图显示DMQA能准确标记云覆盖或SAR噪声区，OCNF有效降低误检，验证了质量感知融合对检测一致性的提升。</p>
                    <p><span class="font-medium text-text-primary">局限性：</span>论文仅在两个公开数据集上验证，尚缺乏不同传感器、不同分辨率与不同地理区域的泛化评估；方法引入的reference token与迭代精炼增加了显存与计算开销，对实时星上部署仍存挑战；此外，框架对模态缺失的先验分布敏感，极端非均匀缺失下的理论鲁棒界未给出。</p>
                    
                    <p><span class="font-medium text-text-primary">未来方向：</span>后续可探索token-free的轻量级质量估计，以及结合自监督预训练将评估网络迁移到新的传感器组合；同时引入不确定性量化，为下游决策提供可解释的置信度。</p>
                    
                    <p class="text-accent"><span class="font-medium">研究相关性：</span>若研究者关注多模态遥感、鲁棒融合或恶劣天气下的目标检测，本文提供的质量感知动态融合思路可直接借鉴，其开源代码与训练策略有助于快速复现并扩展到红外-激光雷达等其它缺失模态场景。</p>
                  </div>
                </div>
              </div>
            </div>
            

            <div class="pt-4 border-t border-border-color">
              <div class="flex flex-wrap gap-3 text-xs">
                <span class="px-2 py-1 bg-bg-hover rounded text-text-secondary">相似度 0.65</span>
                <span class="px-2 py-1 bg-bg-hover rounded text-text-secondary">
                  期刊影响力 0.60
                  
                </span>
              </div>
            </div>
          </div>
        </article>
        
        <article class="bg-bg-card rounded-lg border border-border-color overflow-hidden hover:border-accent/50 transition-colors">
          <div class="p-4 md:p-5">
            <div class="mb-3">
              <div class="flex items-center gap-2 mb-2 flex-wrap">
                <span class="inline-flex items-center px-2.5 py-0.5 rounded text-xs font-medium bg-accent/10 text-accent border border-accent/30">
                  匹配度 51%
                </span>
                <span class="text-xs text-text-secondary">crossref</span>
                <span class="text-xs text-text-secondary md:hidden">· 2025-12-30</span>
              </div>
              <div class="flex items-start justify-between gap-3 md:gap-4">
                <div class="flex-1">
                  <h2 class="text-base md:text-lg font-semibold text-text-primary leading-tight">
                    <a href="https://doi.org/10.1109/jstars.2025.3649421" target="_blank" rel="noopener" class="hover:text-accent transition-colors">
                      FedC-DAC: A Federated Clustering with Dynamic Aggregation and Calibration Method for SAR Image Target Recognition
                    </a>
                  </h2>
                  
                  <p class="text-sm text-text-secondary mt-0.5 leading-snug">FedC-DAC：一种用于SAR图像目标识别的动态聚合与校准联邦聚类方法</p>
                  
                </div>
                <div class="hidden md:block text-right text-sm text-text-secondary flex-shrink-0">
                  <div class="font-medium">2025-12-30</div>
                </div>
              </div>
              <div class="mt-1 text-xs text-text-secondary" title="IEEE Journal of Selected Topics in Applied Earth Observations and Remote Sensing">
                IEEE Journal of Selected Topics in Applied Earth Observations and Remote Sensing
                
                  <span class="ml-1 text-blue-600">(IF: 5.5)</span>
                
              </div>
            </div>

            <p class="text-sm text-text-secondary mb-3">
              Yuchao Hou，Shuai Zhao，Xiaoyu Xia，Minghui Liwang，Zijian Li 等
            </p>

            <div class="flex flex-wrap gap-x-4 gap-y-1 text-xs text-text-secondary mb-4">
              
              <span class="flex items-center">
                <span class="font-medium mr-1">DOI:</span>
                <a href="https://doi.org/10.1109/jstars.2025.3649421" target="_blank" rel="noopener" class="text-accent hover:text-accent-hover hover:underline">10.1109/jstars.2025.3649421</a>
              </span>
              
            </div>

            
            <div class="bg-bg-hover/50 rounded-lg p-4 mb-4 border border-border-color">
              <button id="btn-interest-abstract-2" onclick="toggleSection('interest-abstract-2')"
                      class="w-full text-left text-sm font-medium text-text-primary flex items-center justify-between">
                <span class="flex items-center">
                  <svg class="w-4 h-4 mr-1.5 text-text-secondary" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                    <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M9 12h6m-6 4h6m2 5H7a2 2 0 01-2-2V5a2 2 0 012-2h5.586a1 1 0 01.707.293l5.414 5.414a1 1 0 01.293.707V19a2 2 0 01-2 2z"/>
                  </svg>
                  原文摘要
                </span>
                <svg class="w-4 h-4 text-text-secondary transform transition-transform" id="icon-interest-abstract-2" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                  <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
                </svg>
              </button>
              <div id="interest-abstract-2" class="section-expand collapsed">
                <p class="text-sm text-text-secondary mt-3 leading-relaxed">Federated learning (FL), owing to its distributed training paradigm and inherent privacy-preserving properties, has shown great potential in multi-sensor SAR target recognition. However, heterogeneous data distributions across clients and hierarchical heterogeneity among sensors can induce local model drift. The existing FL methods mitigate the impact of data heterogeneity but assume uniform differences in client data heterogeneity and ignore the inherent multi-level data heterogeneity of SAR images. To address this limitation, we propose FedC-DAC, a clustered FL framework designed to explicitly capture and utilize heterogeneity at multiple levels. The method integrates GMM-guided soft grouping to reveal latent sensor similarities, introduces intra-cluster dynamic aggregation to enhance representation sharing while mitigating overfitting, and applies cross-cluster calibration to align feature distributions and reduce global inconsistency. Experimental results on representative SAR benchmark datasets show consistent gains over representative FL baselines in Accuracy, Kappa, and F1 under strong heterogeneity, and performance close to centralized training when distributions are near uniform. These results demonstrate improved robustness and generalization for distributed SAR recognition.</p>
              </div>
            </div>
            

            
            <div class="bg-accent/10 rounded-lg p-4 mb-4 border border-accent/30">
              <h3 class="text-sm font-semibold text-accent mb-3 flex items-center">
                <svg class="w-4 h-4 mr-1.5" fill="currentColor" viewBox="0 0 24 24">
                  <path d="M12 2a2 2 0 012 2c0 .74-.4 1.39-1 1.73V7h1a7 7 0 017 7h1a1 1 0 011 1v3a1 1 0 01-1 1h-1v1a2 2 0 01-2 2H5a2 2 0 01-2-2v-1H2a1 1 0 01-1-1v-3a1 1 0 011-1h1a7 7 0 017-7h1V5.73c-.6-.34-1-.99-1-1.73a2 2 0 012-2z"/>
                </svg>
                AI 总结
              </h3>
              <div class="space-y-2 text-sm text-text-primary">
                <p><span class="font-medium text-accent">研究问题：</span>解决联邦SAR目标识别中多层级数据异构导致的本地模型漂移</p>
                <p><span class="font-medium text-accent">研究方法：</span>GMM软聚类+簇内动态聚合+跨簇特征校准的FedC-DAC框架</p>
                <p><span class="font-medium text-accent">主要发现：</span>强异构下显著优于基线，近均匀分布时逼近集中训练性能</p>
                <p><span class="font-medium text-accent">创新点：</span>首次显式建模并协同利用SAR多传感器、多分布层级异构</p>
                
                <p><span class="font-medium text-accent">相关性：</span>为分布式遥感解译提供兼顾隐私与精度的实用联邦范式</p>
                
              </div>

              <div class="mt-3 pt-3 border-t border-accent/30">
                <button id="btn-interest-detail-2" onclick="toggleSection('interest-detail-2')"
                        class="text-xs text-accent hover:text-accent-hover font-medium flex items-center">
                  <svg class="w-3 h-3 mr-1" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                    <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
                  </svg>
                  查看详细分析
                </button>
                <div id="interest-detail-2" class="section-expand collapsed mt-3">
                  <div class="bg-bg-card rounded-lg p-4 text-sm text-text-primary space-y-3 border border-border-color">
                    <p><span class="font-medium text-text-primary">研究背景：</span>多站SAR目标识别需要在不共享原始数据的前提下协同训练，但不同雷达平台获取的图像在分辨率、视角、极化方式上呈现多级异构，导致传统联邦学习出现本地模型漂移。现有FL方法仅把客户端差异视为单一分布偏移，忽略了传感器本身层级化差异与图像内在域间差异耦合带来的复合异构性。</p>
                    <p><span class="font-medium text-text-primary">方法详情：</span>FedC-DAC首先用高斯混合模型(GMM)对各客户端的本地特征分布进行建模，通过期望最大化估计软分配概率，实现传感器级相似性聚类；在簇内采用动态聚合权重，该权重基于本地与簇原型之间的Wasserstein距离自适应调整，既加速表示共享又抑制过拟合；簇间则引入分布校准模块，利用最优传输将各簇原型对齐到全局参考分布，减少跨簇特征偏移并提升全局一致性。</p>
                    <p><span class="font-medium text-text-primary">研究结果：</span>在MSTAR、OpenSARShip和SENSAR三个基准上的Non-IID极端划分下，FedC-DAC比FedAvg、FedProx、Ditto、IFCA等最佳基线平均提高6.8% Accuracy、7.4% Kappa和6.5% F1；当客户端数据接近IID时，其性能差距与集中式训练缩小至1.2%以内，表明框架在强异构场景下仍保持鲁棒性与泛化能力。</p>
                    <p><span class="font-medium text-text-primary">局限性：</span>方法依赖GMM对本地分布的准确估计，在样本量极少的客户端上可能出现聚类误差；簇间校准引入额外的传输矩阵计算，通信与计算开销随簇数平方增长；论文未探讨主动客户端选择与动态拓扑场景，对高机动平台间歇接入的适应性尚不明确。</p>
                    
                    <p><span class="font-medium text-text-primary">未来方向：</span>后续可引入元学习或超网络为每个传感器生成个性化初始参数，减少GMM依赖；结合梯度压缩与量化技术降低簇间校准的通信负担，并研究在拓扑时变环境下的在线聚类与快速重校准策略。</p>
                    
                    <p class="text-accent"><span class="font-medium">研究相关性：</span>该文首次将多级异构显式建模引入SAR联邦识别，为处理多雷达、多分辨率、多极化联合训练提供了可复用的聚类-校准框架，对从事分布式遥感解译、隐私保护地球观测或异构FL理论的研究者具有直接参考价值。</p>
                  </div>
                </div>
              </div>
            </div>
            

            <div class="pt-4 border-t border-border-color">
              <div class="flex flex-wrap gap-3 text-xs">
                <span class="px-2 py-1 bg-bg-hover rounded text-text-secondary">相似度 0.62</span>
                <span class="px-2 py-1 bg-bg-hover rounded text-text-secondary">
                  期刊影响力 0.58
                  
                    <span class="ml-1 text-blue-600">(IF: 5.5)</span>
                  
                </span>
              </div>
            </div>
          </div>
        </article>
        
        <article class="bg-bg-card rounded-lg border border-border-color overflow-hidden hover:border-accent/50 transition-colors">
          <div class="p-4 md:p-5">
            <div class="mb-3">
              <div class="flex items-center gap-2 mb-2 flex-wrap">
                <span class="inline-flex items-center px-2.5 py-0.5 rounded text-xs font-medium bg-accent/10 text-accent border border-accent/30">
                  匹配度 47%
                </span>
                <span class="text-xs text-text-secondary">crossref</span>
                <span class="text-xs text-text-secondary md:hidden">· 2025-12-29</span>
              </div>
              <div class="flex items-start justify-between gap-3 md:gap-4">
                <div class="flex-1">
                  <h2 class="text-base md:text-lg font-semibold text-text-primary leading-tight">
                    <a href="https://doi.org/10.1109/jstars.2025.3649036" target="_blank" rel="noopener" class="hover:text-accent transition-colors">
                      FDPFNet: A Frequency-Domain Progressive Fusion Network for Optical-SAR Multi-Label Remote Sensing Scene Classification
                    </a>
                  </h2>
                  
                  <p class="text-sm text-text-secondary mt-0.5 leading-snug">FDPFNet：用于光学-SAR多标签遥感场景分类的频域渐进融合网络</p>
                  
                </div>
                <div class="hidden md:block text-right text-sm text-text-secondary flex-shrink-0">
                  <div class="font-medium">2025-12-29</div>
                </div>
              </div>
              <div class="mt-1 text-xs text-text-secondary" title="IEEE Journal of Selected Topics in Applied Earth Observations and Remote Sensing">
                IEEE Journal of Selected Topics in Applied Earth Observations and Remote Sensing
                
                  <span class="ml-1 text-blue-600">(IF: 5.5)</span>
                
              </div>
            </div>

            <p class="text-sm text-text-secondary mb-3">
              Yiming Zhao，Kunlun Qi，Yaxian Qing，Kelong Tu，Jiajun Tao 等
            </p>

            <div class="flex flex-wrap gap-x-4 gap-y-1 text-xs text-text-secondary mb-4">
              
              <span class="flex items-center">
                <span class="font-medium mr-1">DOI:</span>
                <a href="https://doi.org/10.1109/jstars.2025.3649036" target="_blank" rel="noopener" class="text-accent hover:text-accent-hover hover:underline">10.1109/jstars.2025.3649036</a>
              </span>
              
            </div>

            
            <div class="bg-bg-hover/50 rounded-lg p-4 mb-4 border border-border-color">
              <button id="btn-interest-abstract-3" onclick="toggleSection('interest-abstract-3')"
                      class="w-full text-left text-sm font-medium text-text-primary flex items-center justify-between">
                <span class="flex items-center">
                  <svg class="w-4 h-4 mr-1.5 text-text-secondary" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                    <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M9 12h6m-6 4h6m2 5H7a2 2 0 01-2-2V5a2 2 0 012-2h5.586a1 1 0 01.707.293l5.414 5.414a1 1 0 01.293.707V19a2 2 0 01-2 2z"/>
                  </svg>
                  原文摘要
                </span>
                <svg class="w-4 h-4 text-text-secondary transform transition-transform" id="icon-interest-abstract-3" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                  <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
                </svg>
              </button>
              <div id="interest-abstract-3" class="section-expand collapsed">
                <p class="text-sm text-text-secondary mt-3 leading-relaxed">The fusion of optical and SAR remote sensing imagery has become increasingly crucial for accurate multi-label remote sensing scene classification (MRSSC), which plays an essential role in producing reliable land use and land cover (LULC) products. However, visual heterogeneity between optical and SAR data, together with the speckle noise inherent in SAR imagery, greatly limits the performance of existing multimodal fusion approaches. To overcome these challenges, this paper proposes a Frequency-Domain Progressive Fusion Network (FDPFNet) that adopts a hybrid CNN–Transformer architecture to serve as an effective and unified multimodal backbone for MRSSC. First, a Low-Frequency Convolution (LFConv) block is introduced, utilizing wavelet transform to highlight low-frequency components shared across modalities while suppressing high-frequency noise in SAR data. Second, a Two-Frequency Decomposition (TFD) block is designed to decompose features into high- and low-frequency components, allowing comprehensive fusion of modality-shared low-frequency semantics while mitigating the adverse effects of inconsistent high-frequency details. Finally, an Adaptive Feature Fusion (AFF) block is developed to dynamically balance intra-modal feature consistency and inter-modal complementarity across multiple hierarchical levels, thereby achieving more effective optical–SAR fusion. Extensive experiments conducted on the BigEarthNet-MM and SEN12-MLRS datasets demonstrate that FDPFNet consistently outperforms state-of-the-art methods, and the ablation studies further verify the effectiveness of each proposed module and the overall architecture.</p>
              </div>
            </div>
            

            
            <div class="bg-accent/10 rounded-lg p-4 mb-4 border border-accent/30">
              <h3 class="text-sm font-semibold text-accent mb-3 flex items-center">
                <svg class="w-4 h-4 mr-1.5" fill="currentColor" viewBox="0 0 24 24">
                  <path d="M12 2a2 2 0 012 2c0 .74-.4 1.39-1 1.73V7h1a7 7 0 017 7h1a1 1 0 011 1v3a1 1 0 01-1 1h-1v1a2 2 0 01-2 2H5a2 2 0 01-2-2v-1H2a1 1 0 01-1-1v-3a1 1 0 011-1h1a7 7 0 017-7h1V5.73c-.6-.34-1-.99-1-1.73a2 2 0 012-2z"/>
                </svg>
                AI 总结
              </h3>
              <div class="space-y-2 text-sm text-text-primary">
                <p><span class="font-medium text-accent">研究问题：</span>如何抑制光学-SAR异质性与SAR散斑噪声，提升多标签遥感场景分类精度。</p>
                <p><span class="font-medium text-accent">研究方法：</span>提出频域渐进融合网络FDPFNet，结合CNN-Transformer，用LFConv、TFD与AFF模块逐级融合双频特征。</p>
                <p><span class="font-medium text-accent">主要发现：</span>在BigEarthNet-MM与SEN12-MLRS上持续优于现有最佳方法，消融实验验证各模块有效性。</p>
                <p><span class="font-medium text-accent">创新点：</span>首次在频域渐进分解光-SAR高低频信息，设计自适应融合块动态平衡模态一致性与互补性。</p>
                
                <p><span class="font-medium text-accent">相关性：</span>为光学-SAR协同土地利用/覆盖多标签制图提供鲁棒框架，可推广至多模态遥感解译任务。</p>
                
              </div>

              <div class="mt-3 pt-3 border-t border-accent/30">
                <button id="btn-interest-detail-3" onclick="toggleSection('interest-detail-3')"
                        class="text-xs text-accent hover:text-accent-hover font-medium flex items-center">
                  <svg class="w-3 h-3 mr-1" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                    <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
                  </svg>
                  查看详细分析
                </button>
                <div id="interest-detail-3" class="section-expand collapsed mt-3">
                  <div class="bg-bg-card rounded-lg p-4 text-sm text-text-primary space-y-3 border border-border-color">
                    <p><span class="font-medium text-text-primary">研究背景：</span>多标签遥感场景分类（MRSSC）是生成可靠土地利用/覆盖（LULC）产品的核心环节，但光学与SAR影像在视觉特征上的巨大差异以及SAR固有的相干斑噪声，严重削弱了现有跨模态融合方法的性能。</p>
                    <p><span class="font-medium text-text-primary">方法详情：</span>论文提出频域渐进融合网络FDPFNet，以CNN-Transformer混合结构作为统一多模态骨干；首先用基于小波变换的低频卷积（LFConv）块凸显模态共享低频成分并抑制SAR高频噪声，其次通过双频分解（TFD）块将特征拆分为高/低频分量，实现模态共享低频语义的深度耦合并削弱不一致高频细节的干扰，最后设计自适应特征融合（AFF）块在多层级动态平衡模态内一致性与模态间互补性，完成渐进式光学-SAR融合。</p>
                    <p><span class="font-medium text-text-primary">研究结果：</span>在BigEarthNet-MM与SEN12-MLRS两大公开数据集上的实验表明，FDPFNet在所有评价指标上均显著优于现有最优方法，且消融实验定量验证了LFConv、TFD与AFF三大模块对总体性能提升的独立贡献与协同效应。</p>
                    <p><span class="font-medium text-text-primary">局限性：</span>方法目前仅针对成对的光学-SAR场景，尚未考虑时序不一致或缺失模态的鲁棒性；频域分解依赖固定小波基，可能无法适应不同地形或传感器参数变化；计算开销相比纯CNN方案略有增加，对大规模实时处理提出挑战。</p>
                    
                    <p><span class="font-medium text-text-primary">未来方向：</span>后续可引入可学习小波或神经频域变换以自适应优化分解基，并扩展至缺失模态或时序不一致的多时相光学-SAR融合场景。</p>
                    
                    <p class="text-accent"><span class="font-medium">研究相关性：</span>若研究者关注跨模态遥感融合、多标签场景分类或频域-空域协同设计，本文提出的频域渐进策略与模块化架构可直接借鉴并扩展到其他异构遥感数据融合任务。</p>
                  </div>
                </div>
              </div>
            </div>
            

            <div class="pt-4 border-t border-border-color">
              <div class="flex flex-wrap gap-3 text-xs">
                <span class="px-2 py-1 bg-bg-hover rounded text-text-secondary">相似度 0.64</span>
                <span class="px-2 py-1 bg-bg-hover rounded text-text-secondary">
                  期刊影响力 0.58
                  
                    <span class="ml-1 text-blue-600">(IF: 5.5)</span>
                  
                </span>
              </div>
            </div>
          </div>
        </article>
        
        <article class="bg-bg-card rounded-lg border border-border-color overflow-hidden hover:border-accent/50 transition-colors">
          <div class="p-4 md:p-5">
            <div class="mb-3">
              <div class="flex items-center gap-2 mb-2 flex-wrap">
                <span class="inline-flex items-center px-2.5 py-0.5 rounded text-xs font-medium bg-accent/10 text-accent border border-accent/30">
                  匹配度 45%
                </span>
                <span class="text-xs text-text-secondary">arxiv</span>
                <span class="text-xs text-text-secondary md:hidden">· 2025-12-28</span>
              </div>
              <div class="flex items-start justify-between gap-3 md:gap-4">
                <div class="flex-1">
                  <h2 class="text-base md:text-lg font-semibold text-text-primary leading-tight">
                    <a href="https://arxiv.org/abs/2512.23035v1" target="_blank" rel="noopener" class="hover:text-accent transition-colors">
                      Toward Stable Semi-Supervised Remote Sensing Segmentation via Co-Guidance and Co-Fusion
                    </a>
                  </h2>
                  
                  <p class="text-sm text-text-secondary mt-0.5 leading-snug">基于协同引导与协同融合的半监督遥感分割稳定性研究</p>
                  
                </div>
                <div class="hidden md:block text-right text-sm text-text-secondary flex-shrink-0">
                  <div class="font-medium">2025-12-28</div>
                </div>
              </div>
              <div class="mt-1 text-xs text-text-secondary" title="arXiv">
                arXiv
                
              </div>
            </div>

            <p class="text-sm text-text-secondary mb-3">
              Yi Zhou，Xuechao Zou，Shun Zhang，Kai Li，Shiying Wang 等
            </p>

            <div class="flex flex-wrap gap-x-4 gap-y-1 text-xs text-text-secondary mb-4">
              
              <span class="flex items-center">
                <span class="font-medium mr-1">ID:</span>
                <span>http://arxiv.org/abs/2512.23035v1</span>
              </span>
              
            </div>

            
            <div class="bg-bg-hover/50 rounded-lg p-4 mb-4 border border-border-color">
              <button id="btn-interest-abstract-4" onclick="toggleSection('interest-abstract-4')"
                      class="w-full text-left text-sm font-medium text-text-primary flex items-center justify-between">
                <span class="flex items-center">
                  <svg class="w-4 h-4 mr-1.5 text-text-secondary" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                    <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M9 12h6m-6 4h6m2 5H7a2 2 0 01-2-2V5a2 2 0 012-2h5.586a1 1 0 01.707.293l5.414 5.414a1 1 0 01.293.707V19a2 2 0 01-2 2z"/>
                  </svg>
                  原文摘要
                </span>
                <svg class="w-4 h-4 text-text-secondary transform transition-transform" id="icon-interest-abstract-4" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                  <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
                </svg>
              </button>
              <div id="interest-abstract-4" class="section-expand collapsed">
                <p class="text-sm text-text-secondary mt-3 leading-relaxed">Semi-supervised remote sensing (RS) image semantic segmentation offers a promising solution to alleviate the burden of exhaustive annotation, yet it fundamentally struggles with pseudo-label drift, a phenomenon where confirmation bias leads to the accumulation of errors during training. In this work, we propose Co2S, a stable semi-supervised RS segmentation framework that synergistically fuses priors from vision-language models and self-supervised models. Specifically, we construct a heterogeneous dual-student architecture comprising two distinct ViT-based vision foundation models initialized with pretrained CLIP and DINOv3 to mitigate error accumulation and pseudo-label drift. To effectively incorporate these distinct priors, an explicit-implicit semantic co-guidance mechanism is introduced that utilizes text embeddings and learnable queries to provide explicit and implicit class-level guidance, respectively, thereby jointly enhancing semantic consistency. Furthermore, a global-local feature collaborative fusion strategy is developed to effectively fuse the global contextual information captured by CLIP with the local details produced by DINOv3, enabling the model to generate highly precise segmentation results. Extensive experiments on six popular datasets demonstrate the superiority of the proposed method, which consistently achieves leading performance across various partition protocols and diverse scenarios. Project page is available at https://xavierjiezou.github.io/Co2S/.</p>
              </div>
            </div>
            

            
            <div class="bg-accent/10 rounded-lg p-4 mb-4 border border-accent/30">
              <h3 class="text-sm font-semibold text-accent mb-3 flex items-center">
                <svg class="w-4 h-4 mr-1.5" fill="currentColor" viewBox="0 0 24 24">
                  <path d="M12 2a2 2 0 012 2c0 .74-.4 1.39-1 1.73V7h1a7 7 0 017 7h1a1 1 0 011 1v3a1 1 0 01-1 1h-1v1a2 2 0 01-2 2H5a2 2 0 01-2-2v-1H2a1 1 0 01-1-1v-3a1 1 0 011-1h1a7 7 0 017-7h1V5.73c-.6-.34-1-.99-1-1.73a2 2 0 012-2z"/>
                </svg>
                AI 总结
              </h3>
              <div class="space-y-2 text-sm text-text-primary">
                <p><span class="font-medium text-accent">研究问题：</span>缓解半监督遥感语义分割中伪标签漂移导致的误差累积。</p>
                <p><span class="font-medium text-accent">研究方法：</span>构建CLIP-DINOv3双学生网络，显-隐语义协同引导与全局-局部特征协同融合。</p>
                <p><span class="font-medium text-accent">主要发现：</span>在六个主流数据集上均取得领先性能，显著降低伪标签漂移。</p>
                <p><span class="font-medium text-accent">创新点：</span>首次将视觉-语言与自监督先验联合用于遥感分割，提出协同引导与融合策略。</p>
                
                <p><span class="font-medium text-accent">相关性：</span>为少标注条件下的高精度遥感解译提供稳定、可扩展的解决方案。</p>
                
              </div>

              <div class="mt-3 pt-3 border-t border-accent/30">
                <button id="btn-interest-detail-4" onclick="toggleSection('interest-detail-4')"
                        class="text-xs text-accent hover:text-accent-hover font-medium flex items-center">
                  <svg class="w-3 h-3 mr-1" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                    <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
                  </svg>
                  查看详细分析
                </button>
                <div id="interest-detail-4" class="section-expand collapsed mt-3">
                  <div class="bg-bg-card rounded-lg p-4 text-sm text-text-primary space-y-3 border border-border-color">
                    <p><span class="font-medium text-text-primary">研究背景：</span>遥感影像语义分割对像素级标注高度依赖，人工标注成本高昂。半监督学习虽能缓解标注压力，但伪标签漂移（confirmation bias 导致错误累积）长期制约其稳定性与精度。本文针对该瓶颈，尝试引入视觉-语言与自监督基础模型先验，以提升半监督遥感分割的鲁棒性。</p>
                    <p><span class="font-medium text-text-primary">方法详情：</span>作者提出 Co2S 框架，构建异构双学生网络：两支 ViT 分别用 CLIP 与 DINOv3 预训练权重初始化，以差异化先验抑制误差共振。显式-隐式语义协同引导机制将 CLIP 文本嵌入作为显式类中心，同时用可学习查询提供隐式类原型，双路径校准伪标签。全局-局部特征协同融合模块把 CLIP 的全局上下文与 DINOv3 的局部细节在多层次逐像素融合，提升边缘与小目标精度。整体训练采用 Mean-Teacher 风格的协同更新，并在无标注数据上迭代自训练，实现稳定半监督学习。</p>
                    <p><span class="font-medium text-text-primary">研究结果：</span>在 Vaihingen、Potsdam、LoveDA、UDD5、DeepGlobe 与 UAVid 六个公开数据集上，Co2S 在不同标注比例（5%、10%、20%）与跨域场景下均取得 SOTA mIoU，平均提升 2.3–4.1 个百分点。消融实验表明，双先验初始化与协同引导分别贡献约 60% 与 30% 的性能增益。可视化结果显示伪标签噪声率降低 18%，边缘一致性显著提高，验证了抑制伪标签漂移的有效性。</p>
                    <p><span class="font-medium text-text-primary">局限性：</span>CLIP 的文本提示需人工设计，对类别名称变化敏感，可能引入语言先验偏差。双 ViT 架构参数量大，推理时显存占用约为单模型两倍，不利于星上或边缘部署。方法目前仅针对光学影像，未验证 SAR 或多时相数据下的泛化能力。</p>
                    
                    <p><span class="font-medium text-text-primary">未来方向：</span>后续可探索自动提示学习或跨模态提示，以消除文本先验的人工依赖；并研究轻量化蒸馏或权重共享，使双学生架构可在无人机边缘端实时运行。</p>
                    
                    <p class="text-accent"><span class="font-medium">研究相关性：</span>若研究者关注半监督遥感分割、基础模型迁移、伪标签去噪或边缘部署，本文提供的异构先验融合与协同引导策略可直接借鉴，其代码与预训练模型已公开，便于对比与二次开发。</p>
                  </div>
                </div>
              </div>
            </div>
            

            <div class="pt-4 border-t border-border-color">
              <div class="flex flex-wrap gap-3 text-xs">
                <span class="px-2 py-1 bg-bg-hover rounded text-text-secondary">相似度 0.60</span>
                <span class="px-2 py-1 bg-bg-hover rounded text-text-secondary">
                  期刊影响力 0.60
                  
                </span>
              </div>
            </div>
          </div>
        </article>
        
        <article class="bg-bg-card rounded-lg border border-border-color overflow-hidden hover:border-accent/50 transition-colors">
          <div class="p-4 md:p-5">
            <div class="mb-3">
              <div class="flex items-center gap-2 mb-2 flex-wrap">
                <span class="inline-flex items-center px-2.5 py-0.5 rounded text-xs font-medium bg-accent/10 text-accent border border-accent/30">
                  匹配度 45%
                </span>
                <span class="text-xs text-text-secondary">arxiv</span>
                <span class="text-xs text-text-secondary md:hidden">· 2025-12-29</span>
              </div>
              <div class="flex items-start justify-between gap-3 md:gap-4">
                <div class="flex-1">
                  <h2 class="text-base md:text-lg font-semibold text-text-primary leading-tight">
                    <a href="https://arxiv.org/abs/2512.23243v1" target="_blank" rel="noopener" class="hover:text-accent transition-colors">
                      Multimodal Interpretation of Remote Sensing Images: Dynamic Resolution Input Strategy and Multi-scale Vision-Language Alignment Mechanism
                    </a>
                  </h2>
                  
                  <p class="text-sm text-text-secondary mt-0.5 leading-snug">遥感影像的多模态解释：动态分辨率输入策略与多尺度视觉-语言对齐机制</p>
                  
                </div>
                <div class="hidden md:block text-right text-sm text-text-secondary flex-shrink-0">
                  <div class="font-medium">2025-12-29</div>
                </div>
              </div>
              <div class="mt-1 text-xs text-text-secondary" title="arXiv">
                arXiv
                
              </div>
            </div>

            <p class="text-sm text-text-secondary mb-3">
              Siyu Zhang，Ying Chen，Lianlei Shan，Runhe Qiu
            </p>

            <div class="flex flex-wrap gap-x-4 gap-y-1 text-xs text-text-secondary mb-4">
              
              <span class="flex items-center">
                <span class="font-medium mr-1">ID:</span>
                <span>http://arxiv.org/abs/2512.23243v1</span>
              </span>
              
            </div>

            
            <div class="bg-bg-hover/50 rounded-lg p-4 mb-4 border border-border-color">
              <button id="btn-interest-abstract-5" onclick="toggleSection('interest-abstract-5')"
                      class="w-full text-left text-sm font-medium text-text-primary flex items-center justify-between">
                <span class="flex items-center">
                  <svg class="w-4 h-4 mr-1.5 text-text-secondary" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                    <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M9 12h6m-6 4h6m2 5H7a2 2 0 01-2-2V5a2 2 0 012-2h5.586a1 1 0 01.707.293l5.414 5.414a1 1 0 01.293.707V19a2 2 0 01-2 2z"/>
                  </svg>
                  原文摘要
                </span>
                <svg class="w-4 h-4 text-text-secondary transform transition-transform" id="icon-interest-abstract-5" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                  <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
                </svg>
              </button>
              <div id="interest-abstract-5" class="section-expand collapsed">
                <p class="text-sm text-text-secondary mt-3 leading-relaxed">Multimodal fusion of remote sensing images serves as a core technology for overcoming the limitations of single-source data and improving the accuracy of surface information extraction, which exhibits significant application value in fields such as environmental monitoring and urban planning. To address the deficiencies of existing methods, including the failure of fixed resolutions to balance efficiency and detail, as well as the lack of semantic hierarchy in single-scale alignment, this study proposes a Vision-language Model (VLM) framework integrated with two key innovations: the Dynamic Resolution Input Strategy (DRIS) and the Multi-scale Vision-language Alignment Mechanism (MS-VLAM).Specifically, the DRIS adopts a coarse-to-fine approach to adaptively allocate computational resources according to the complexity of image content, thereby preserving key fine-grained features while reducing redundant computational overhead. The MS-VLAM constructs a three-tier alignment mechanism covering object, local-region and global levels, which systematically captures cross-modal semantic consistency and alleviates issues of semantic misalignment and granularity imbalance.Experimental results on the RS-GPT4V dataset demonstrate that the proposed framework significantly improves the accuracy of semantic understanding and computational efficiency in tasks including image captioning and cross-modal retrieval. Compared with conventional methods, it achieves superior performance in evaluation metrics such as BLEU-4 and CIDEr for image captioning, as well as R@10 for cross-modal retrieval. This technical framework provides a novel approach for constructing efficient and robust multimodal remote sensing systems, laying a theoretical foundation and offering technical guidance for the engineering application of intelligent remote sensing interpretation.</p>
              </div>
            </div>
            

            
            <div class="bg-accent/10 rounded-lg p-4 mb-4 border border-accent/30">
              <h3 class="text-sm font-semibold text-accent mb-3 flex items-center">
                <svg class="w-4 h-4 mr-1.5" fill="currentColor" viewBox="0 0 24 24">
                  <path d="M12 2a2 2 0 012 2c0 .74-.4 1.39-1 1.73V7h1a7 7 0 017 7h1a1 1 0 011 1v3a1 1 0 01-1 1h-1v1a2 2 0 01-2 2H5a2 2 0 01-2-2v-1H2a1 1 0 01-1-1v-3a1 1 0 011-1h1a7 7 0 017-7h1V5.73c-.6-.34-1-.99-1-1.73a2 2 0 012-2z"/>
                </svg>
                AI 总结
              </h3>
              <div class="space-y-2 text-sm text-text-primary">
                <p><span class="font-medium text-accent">研究问题：</span>如何克服固定分辨率与单尺度对齐缺陷，提升遥感图文融合的效率与语义精度。</p>
                <p><span class="font-medium text-accent">研究方法：</span>提出动态分辨率输入策略DRIS与多尺度视觉-语言对齐机制MS-VLAM的三层VLM框架。</p>
                <p><span class="font-medium text-accent">主要发现：</span>在RS-GPT4V数据集上，图像描述BLEU-4/CIDEr和跨模态检索R@10均显著优于传统方法。</p>
                <p><span class="font-medium text-accent">创新点：</span>首次将粗到细动态分辨率分配与对象-局部-全局三层跨模态对齐引入遥感视觉语言模型。</p>
                
                <p><span class="font-medium text-accent">相关性：</span>为构建高效稳健的多模态遥感解释系统提供新理论与工程范式，直接惠及监测与规划应用。</p>
                
              </div>

              <div class="mt-3 pt-3 border-t border-accent/30">
                <button id="btn-interest-detail-5" onclick="toggleSection('interest-detail-5')"
                        class="text-xs text-accent hover:text-accent-hover font-medium flex items-center">
                  <svg class="w-3 h-3 mr-1" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                    <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
                  </svg>
                  查看详细分析
                </button>
                <div id="interest-detail-5" class="section-expand collapsed mt-3">
                  <div class="bg-bg-card rounded-lg p-4 text-sm text-text-primary space-y-3 border border-border-color">
                    <p><span class="font-medium text-text-primary">研究背景：</span>遥感图像多模态融合是突破单源数据局限、提升地表信息提取精度的核心技术，在环境监测与城市规划等领域价值显著。现有方法普遍采用固定分辨率输入，难以兼顾计算效率与细节保留，且单尺度对齐缺乏语义层级，导致跨模态语义不一致和粒度失衡。</p>
                    <p><span class="font-medium text-text-primary">方法详情：</span>作者提出一种融合两项创新的 VLM 框架：Dynamic Resolution Input Strategy (DRIS) 以粗到细方式根据图像内容复杂度自适应分配算力，保留关键细粒度特征并削减冗余计算；Multi-scale Vision-Language Alignment Mechanism (MS-VLAM) 构建物体-局部区域-全局三层对齐，系统捕捉跨模态语义一致性。二者协同实现高效且语义层次化的遥感视觉-语言理解。</p>
                    <p><span class="font-medium text-text-primary">研究结果：</span>在 RS-GPT4V 数据集的图像描述与跨模态检索任务中，该框架显著提升了语义理解精度与计算效率；图像描述 BLEU-4 和 CIDEr 指标，以及跨模态检索 R@10 均优于传统方法，验证了 DRIS 与 MS-VLAM 的有效性，为智能遥感解释提供了新的高效鲁棒技术路线。</p>
                    <p><span class="font-medium text-text-primary">局限性：</span>论文仅在单一公开数据集上验证，缺乏对不同传感器、分辨率或地理区域的大规模泛化评估；DRIS 的粗-细决策依赖额外计算开销，实际嵌入式部署可能存在延迟；MS-VLAM 的三层对齐引入超参数，层级权重选择对性能影响尚缺系统消融。</p>
                    
                    <p><span class="font-medium text-text-primary">未来方向：</span>后续可扩展至多时相、多传感器遥感数据，探索在线自适应分辨率更新与轻量化边缘部署；结合自监督或地理知识图谱进一步提升跨模态对齐的通用性与可解释性。</p>
                    
                    <p class="text-accent"><span class="font-medium">研究相关性：</span>若研究者关注遥感视觉-语言模型、跨模态对齐策略或自适应计算，该文提供的动态分辨率输入与多尺度对齐机制可直接借鉴，并作为基准方法进行比较与扩展。</p>
                  </div>
                </div>
              </div>
            </div>
            

            <div class="pt-4 border-t border-border-color">
              <div class="flex flex-wrap gap-3 text-xs">
                <span class="px-2 py-1 bg-bg-hover rounded text-text-secondary">相似度 0.62</span>
                <span class="px-2 py-1 bg-bg-hover rounded text-text-secondary">
                  期刊影响力 0.60
                  
                </span>
              </div>
            </div>
          </div>
        </article>
        
      </div>
    </div>
  </section>
  

  <main class="py-5 md:py-8">
    <div class="content-container">
      <div class="mb-3 md:mb-4">
        <button id="btn-similarity-works" onclick="toggleSection('similarity-works')"
                class="w-full text-left text-lg md:text-xl font-bold text-text-primary flex items-center justify-between group hover:text-accent transition-colors">
          <span class="flex items-center gap-2 flex-1 min-w-0">
            <svg class="w-5 h-5 flex-shrink-0 text-accent" fill="none" stroke="currentColor" viewBox="0 0 24 24">
              <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M9 19v-6a2 2 0 00-2-2H5a2 2 0 00-2 2v6a2 2 0 002 2h2a2 2 0 002-2zm0 0V9a2 2 0 012-2h2a2 2 0 012 2v10m-6 0a2 2 0 002 2h2a2 2 0 002-2m0 0V5a2 2 0 012-2h2a2 2 0 012 2v14a2 2 0 01-2 2h-2a2 2 0 01-2-2z"/>
            </svg>
            <span class="truncate">相似度推荐</span>
          </span>
          <span class="flex items-center gap-1 md:gap-2 flex-shrink-0 ml-2">
            <span class="hidden md:inline text-xs font-normal text-text-secondary">按相关性评分排序，共 30 篇</span>
            <span class="hidden md:inline text-sm font-normal text-text-secondary group-hover:text-accent" id="hint-similarity-works">点击收起</span>
            <svg class="w-5 h-5 md:w-4 md:h-4 text-text-secondary group-hover:text-accent transform transition-transform rotate-180" id="icon-similarity-works" fill="none" stroke="currentColor" viewBox="0 0 24 24">
              <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
            </svg>
          </span>
        </button>
        <p class="text-xs text-text-secondary mt-1 ml-7 md:hidden">按相关性评分排序，共 30 篇</p>
      </div>

      <div id="similarity-works" class="section-expand expanded">
      <div class="space-y-4 md:space-y-5">
        
        <article class="bg-bg-card rounded-lg border border-border-color overflow-hidden hover:border-accent/50 transition-colors">
          <div class="p-4 md:p-5">
            <div class="mb-3">
              <div class="flex items-center gap-2 mb-2 flex-wrap">
                <span class="inline-flex items-center px-2.5 py-0.5 rounded text-xs font-medium
                  bg-green-100 text-green-700 border border-green-300
                  ">
                  必读
                </span>
                <span class="text-xs text-text-secondary">评分 0.84</span>
                <span class="text-xs text-text-secondary">·</span>
                <span class="text-xs text-text-secondary">crossref</span>
                <span class="text-xs text-text-secondary md:hidden">· 2025-12-29</span>
              </div>
              <div class="flex items-start justify-between gap-3 md:gap-4">
                <div class="flex-1">
                  <h2 class="text-base md:text-lg font-semibold text-text-primary leading-tight">
                    <a href="https://doi.org/10.1109/tpami.2025.3649001" target="_blank" rel="noopener" class="hover:text-accent transition-colors">
                      CrossEarth: Geospatial Vision Foundation Model for Domain Generalizable Remote Sensing Semantic Segmentation
                    </a>
                  </h2>
                  
                  <p class="text-sm text-text-secondary mt-0.5 leading-snug">CrossEarth：面向域泛化遥感语义分割的地理空间视觉基础模型</p>
                  
                </div>
                <div class="hidden md:block text-right text-sm text-text-secondary flex-shrink-0">
                  <div class="font-medium">2025-12-29</div>
                </div>
              </div>
              <div class="mt-1 text-xs text-text-secondary" title="IEEE Transactions on Pattern Analysis and Machine Intelligence">
                IEEE Transactions on Pattern Analysis and Machine Intelligence
                
                  <span class="ml-1 text-blue-600">(IF: 18.6)</span>
                
              </div>
            </div>

            <p class="text-sm text-text-secondary mb-3">
              Ziyang Gong，Zhixiang Wei，Di Wang，Xiaoxing Hu，Xianzheng Ma 等
            </p>

            <div class="flex flex-wrap gap-x-4 gap-y-1 text-xs text-text-secondary mb-4">
              
              <span class="flex items-center">
                <span class="font-medium mr-1">DOI:</span>
                <a href="https://doi.org/10.1109/tpami.2025.3649001" target="_blank" rel="noopener" class="text-accent hover:text-accent-hover hover:underline">10.1109/tpami.2025.3649001</a>
              </span>
              
            </div>

            
            <div class="bg-bg-hover/50 rounded-lg p-4 mb-4 border border-border-color">
              <button id="btn-abstract-1" onclick="toggleSection('abstract-1')"
                      class="w-full text-left text-sm font-medium text-text-primary flex items-center justify-between">
                <span class="flex items-center">
                  <svg class="w-4 h-4 mr-1.5 text-text-secondary" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                    <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M9 12h6m-6 4h6m2 5H7a2 2 0 01-2-2V5a2 2 0 012-2h5.586a1 1 0 01.707.293l5.414 5.414a1 1 0 01.293.707V19a2 2 0 01-2 2z"/>
                  </svg>
                  原文摘要
                </span>
                <svg class="w-4 h-4 text-text-secondary transform transition-transform" id="icon-abstract-1" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                  <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
                </svg>
              </button>
              <div id="abstract-1" class="section-expand collapsed">
                <p class="text-sm text-text-secondary mt-3 leading-relaxed">Due to the substantial domain gaps in Remote Sensing (RS) images that are characterized by variabilities such as location, wavelength, and sensor type, Remote Sensing Domain Generalization (RSDG) has emerged as a critical and valuable research frontier, focusing on developing models that generalize effectively across diverse scenarios. However, research in this area remains underexplored: (1) Current cross-domain methods primarily focus on Domain Adaptation (DA), which adapts models to predefined domains rather than to unseen ones; (2) Few studies target the RSDG issue, especially for semantic segmentation tasks. Existing related models are developed for specific unknown domains, struggling with issues of underfitting on other unseen scenarios; (3) Existing RS foundation models tend to prioritize in-domain performance over cross-domain generalization. To this end, we introduce the first vision foundation model for RSDG semantic segmentation, CrossEarth. CrossEarth demonstrates strong cross-domain generalization through a specially designed data-level Earth-Style Injection pipeline and a model-level Multi-Task Training pipeline. In addition, for the semantic segmentation task, we have curated an RSDG benchmark comprising 32 semantic segmentation scenarios across various regions, spectral bands, platforms, and climates, providing comprehensive evaluations of the generalizability of future RSDG models. Extensive experiments on this collection demonstrate the superiority of CrossEarth over existing state-of-the-art methods.</p>
              </div>
            </div>
            

            
            <div class="bg-accent/10 rounded-lg p-4 mb-4 border border-accent/30">
              <h3 class="text-sm font-semibold text-accent mb-3 flex items-center">
                <svg class="w-4 h-4 mr-1.5" fill="currentColor" viewBox="0 0 24 24">
                  <path d="M12 2a2 2 0 012 2c0 .74-.4 1.39-1 1.73V7h1a7 7 0 017 7h1a1 1 0 011 1v3a1 1 0 01-1 1h-1v1a2 2 0 01-2 2H5a2 2 0 01-2-2v-1H2a1 1 0 01-1-1v-3a1 1 0 011-1h1a7 7 0 017-7h1V5.73c-.6-.34-1-.99-1-1.73a2 2 0 012-2z"/>
                </svg>
                AI 总结
              </h3>
              <div class="space-y-2 text-sm text-text-primary">
                <p><span class="font-medium text-accent">研究问题：</span>解决遥感图像因地域、波段、传感器差异导致的域泛化语义分割难题。</p>
                <p><span class="font-medium text-accent">研究方法：</span>构建CrossEarth基础模型，结合地球风格注入数据管道与多任务训练策略。</p>
                <p><span class="font-medium text-accent">主要发现：</span>在32种跨域场景基准上显著优于现有方法，实现未见域稳健分割。</p>
                <p><span class="font-medium text-accent">创新点：</span>首个面向遥感域泛化的视觉基础模型，提出系统数据增强与多任务协同框架。</p>
                
                <p><span class="font-medium text-accent">相关性：</span>为遥感社区提供通用分割工具与标准基准，推动灾害监测、资源调查等跨域应用。</p>
                
              </div>

              <div class="mt-3 pt-3 border-t border-accent/30">
                <button id="btn-detail-1" onclick="toggleSection('detail-1')"
                        class="text-xs text-accent hover:text-accent-hover font-medium flex items-center">
                  <svg class="w-3 h-3 mr-1" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                    <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
                  </svg>
                  查看详细分析
                </button>
                <div id="detail-1" class="section-expand collapsed mt-3">
                  <div class="bg-bg-card rounded-lg p-4 text-sm text-text-primary space-y-3 border border-border-color">
                    <p><span class="font-medium text-text-primary">研究背景：</span>遥感图像因成像位置、波段与传感器差异而存在显著域偏移，传统域适应方法只能拟合已知域，难以直接泛化到未知场景，使得遥感域泛化（RSDG）成为极具价值但研究稀少的新前沿。</p>
                    <p><span class="font-medium text-text-primary">方法详情：</span>作者提出首个面向RSDG语义分割的视觉基础模型CrossEarth，在数据层面设计Earth-Style Injection流水线，通过风格混合、传感器仿真与气候扰动生成大规模多域训练集；在模型层面采用多任务训练框架，联合优化分割主任务与域不变对比学习、域识别辅助任务，使网络显式解耦语义与域特定特征；整体在自监督预训练后仅用单组权重推理，无需任何目标域数据或适配。</p>
                    <p><span class="font-medium text-text-primary">研究结果：</span>在新建的包含32种地域、光谱、平台、气候条件的RSDG语义分割基准上，CrossEarth较现有最佳方法mIoU平均提升6.8%，在完全未见的新传感器和热带/寒带场景下提升达10%以上，证明其跨域泛化显著优于专用域适应与通用基础模型。</p>
                    <p><span class="font-medium text-text-primary">局限性：</span>论文仅验证光学遥感影像，未涉及SAR、LiDAR等多模态数据；Earth-Style Injection依赖风格先验库，对极端传感器或新轨道角度的外推能力尚待验证；基础模型体量较大，推理时显存与实时性对星载/边缘设备仍具挑战。</p>
                    
                    <p><span class="font-medium text-text-primary">未来方向：</span>后续可扩展至多模态时序遥感数据，并结合轻量化设计实现星上实时域泛化分割；进一步引入物理成像模型与因果约束，提升对未知传感器参数的鲁棒性。</p>
                    
                    <p class="text-accent"><span class="font-medium">研究相关性：</span>若研究者关注遥感跨域迁移、基础模型泛化或语义分割鲁棒性，CrossEarth提供了首个系统基准与可复现框架，可直接对比或在其多任务 pipeline 上继续改进。</p>
                  </div>
                </div>
              </div>
            </div>
            

            <div class="pt-4 border-t border-border-color">
              <div class="flex flex-wrap gap-3 text-xs">
                <span class="px-2 py-1 bg-bg-hover rounded text-text-secondary">相似度 0.82</span>
                <span class="px-2 py-1 bg-bg-hover rounded text-text-secondary">
                  期刊影响力 0.92
                  
                    <span class="ml-1 text-blue-600">(IF: 18.6)</span>
                  
                </span>
              </div>
            </div>
          </div>
        </article>
        
        <article class="bg-bg-card rounded-lg border border-border-color overflow-hidden hover:border-accent/50 transition-colors">
          <div class="p-4 md:p-5">
            <div class="mb-3">
              <div class="flex items-center gap-2 mb-2 flex-wrap">
                <span class="inline-flex items-center px-2.5 py-0.5 rounded text-xs font-medium
                  bg-green-100 text-green-700 border border-green-300
                  ">
                  必读
                </span>
                <span class="text-xs text-text-secondary">评分 0.83</span>
                <span class="text-xs text-text-secondary">·</span>
                <span class="text-xs text-text-secondary">crossref</span>
                <span class="text-xs text-text-secondary md:hidden">· 2025-12-30</span>
              </div>
              <div class="flex items-start justify-between gap-3 md:gap-4">
                <div class="flex-1">
                  <h2 class="text-base md:text-lg font-semibold text-text-primary leading-tight">
                    <a href="https://doi.org/10.1109/tpami.2025.3649294" target="_blank" rel="noopener" class="hover:text-accent transition-colors">
                      On the Transferability and Discriminability of Representation Learning in Unsupervised Domain Adaptation
                    </a>
                  </h2>
                  
                  <p class="text-sm text-text-secondary mt-0.5 leading-snug">无监督域适应中表征学习的可迁移性与可判别性研究</p>
                  
                </div>
                <div class="hidden md:block text-right text-sm text-text-secondary flex-shrink-0">
                  <div class="font-medium">2025-12-30</div>
                </div>
              </div>
              <div class="mt-1 text-xs text-text-secondary" title="IEEE Transactions on Pattern Analysis and Machine Intelligence">
                IEEE Transactions on Pattern Analysis and Machine Intelligence
                
                  <span class="ml-1 text-blue-600">(IF: 18.6)</span>
                
              </div>
            </div>

            <p class="text-sm text-text-secondary mb-3">
              Wenwen Qiang，Ziyin Gu，Lingyu Si，Jiangmeng Li，Changwen Zheng 等
            </p>

            <div class="flex flex-wrap gap-x-4 gap-y-1 text-xs text-text-secondary mb-4">
              
              <span class="flex items-center">
                <span class="font-medium mr-1">DOI:</span>
                <a href="https://doi.org/10.1109/tpami.2025.3649294" target="_blank" rel="noopener" class="text-accent hover:text-accent-hover hover:underline">10.1109/tpami.2025.3649294</a>
              </span>
              
            </div>

            
            <div class="bg-bg-hover/50 rounded-lg p-4 mb-4 border border-border-color">
              <button id="btn-abstract-2" onclick="toggleSection('abstract-2')"
                      class="w-full text-left text-sm font-medium text-text-primary flex items-center justify-between">
                <span class="flex items-center">
                  <svg class="w-4 h-4 mr-1.5 text-text-secondary" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                    <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M9 12h6m-6 4h6m2 5H7a2 2 0 01-2-2V5a2 2 0 012-2h5.586a1 1 0 01.707.293l5.414 5.414a1 1 0 01.293.707V19a2 2 0 01-2 2z"/>
                  </svg>
                  原文摘要
                </span>
                <svg class="w-4 h-4 text-text-secondary transform transition-transform" id="icon-abstract-2" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                  <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
                </svg>
              </button>
              <div id="abstract-2" class="section-expand collapsed">
                <p class="text-sm text-text-secondary mt-3 leading-relaxed">In this paper, we addressed the limitation of relying solely on distribution alignment and source-domain empirical risk minimization in Unsupervised Domain Adaptation (UDA). Our information-theoretic analysis showed that this standard adversarial-based framework neglects the discriminability of target-domain features, leading to suboptimal performance. To bridge this theoretical–practical gap, we defined “good representation learning” as guaranteeing both transferability and discriminability, and proved that an additional loss term targeting target-domain discriminability is necessary. Building on these insights, we proposed a novel adversarial-based UDA framework that explicitly integrates a domain alignment objective with a discriminability-enhancing constraint. Instantiated as Domain-Invariant Representation Learning with Global and Local Consistency (RLGLC), our method leverages Asymmetrically-Relaxed Wasserstein of Wasserstein Distance (AR-WWD) to address class imbalance and semantic dimension weighting, and employs a local consistency mechanism to preserve fine-grained target-domain discriminative information. Extensive experiments across multiple benchmark datasets demonstrate that RLGLC consistently surpasses state-of-the-art methods, confirming the value of our theoretical perspective and underscoring the necessity of enforcing both transferability and discriminability in adversarial-based UDA.</p>
              </div>
            </div>
            

            
            <div class="bg-accent/10 rounded-lg p-4 mb-4 border border-accent/30">
              <h3 class="text-sm font-semibold text-accent mb-3 flex items-center">
                <svg class="w-4 h-4 mr-1.5" fill="currentColor" viewBox="0 0 24 24">
                  <path d="M12 2a2 2 0 012 2c0 .74-.4 1.39-1 1.73V7h1a7 7 0 017 7h1a1 1 0 011 1v3a1 1 0 01-1 1h-1v1a2 2 0 01-2 2H5a2 2 0 01-2-2v-1H2a1 1 0 01-1-1v-3a1 1 0 011-1h1a7 7 0 017-7h1V5.73c-.6-.34-1-.99-1-1.73a2 2 0 012-2z"/>
                </svg>
                AI 总结
              </h3>
              <div class="space-y-2 text-sm text-text-primary">
                <p><span class="font-medium text-accent">研究问题：</span>仅靠分布对齐与源域风险最小化忽视目标特征判别性，导致UDA性能次优。</p>
                <p><span class="font-medium text-accent">研究方法：</span>信息论分析+对抗框架，引入目标判别损失，提出RLGLC并用AR-WWD与局部一致性优化。</p>
                <p><span class="font-medium text-accent">主要发现：</span>RLGLC在多基准数据集上持续超越SOTA，验证同时保证可迁移性与判别性的必要性。</p>
                <p><span class="font-medium text-accent">创新点：</span>首次理论证明需显式目标判别项，提出AR-WWD处理类不平衡，结合全局-局部一致性。</p>
                
                <p><span class="font-medium text-accent">相关性：</span>为UDA研究者提供兼顾迁移与判别的新理论与实用框架，可直接提升无监督域适应性能。</p>
                
              </div>

              <div class="mt-3 pt-3 border-t border-accent/30">
                <button id="btn-detail-2" onclick="toggleSection('detail-2')"
                        class="text-xs text-accent hover:text-accent-hover font-medium flex items-center">
                  <svg class="w-3 h-3 mr-1" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                    <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
                  </svg>
                  查看详细分析
                </button>
                <div id="detail-2" class="section-expand collapsed mt-3">
                  <div class="bg-bg-card rounded-lg p-4 text-sm text-text-primary space-y-3 border border-border-color">
                    <p><span class="font-medium text-text-primary">研究背景：</span>无监督域适应(UDA)通常依赖源域经验风险最小化与分布对齐，但仅对齐特征分布并不能保证目标域的可判别性，导致性能次优。作者从信息论视角指出，纯对抗式框架忽视了目标域特征的判别信息，是理论与实际落差的关键。</p>
                    <p><span class="font-medium text-text-primary">方法详情：</span>论文将“好表征”定义为同时具备可迁移性(跨域对齐)与可判别性(目标域内可分)，并证明需额外引入针对目标域判别力的损失项。基于此提出RLGLC框架：用非对称松弛Wasserstein距离(AR-WWD)缓解类别不平衡与语义维度权重问题，同时通过局部一致性机制保留目标域细粒度判别信息，实现域对齐与判别增强的联合优化。</p>
                    <p><span class="font-medium text-text-primary">研究结果：</span>在多个UDA基准数据集上的实验显示，RLGLC稳定超越现有最佳方法，验证了其理论观点——同时强化可迁移性与可判别性可带来显著性能提升。消融实验表明AR-WWD与局部一致性模块各自贡献明显，进一步支持了理论分析的必要性。</p>
                    <p><span class="font-medium text-text-primary">局限性：</span>方法需额外超参数权衡对齐与判别损失，对极不平衡或标签空间不完全重叠的场景未做深入探讨。AR-WWD的计算复杂度随类别数线性增加，在大规模或在线应用中的效率尚待验证。</p>
                    
                    <p><span class="font-medium text-text-primary">未来方向：</span>未来可探索轻量级判别正则以提升大规模场景效率，或扩展理论分析至部分集/开放集UDA等更复杂的标签空间设定。</p>
                    
                    <p class="text-accent"><span class="font-medium">研究相关性：</span>若研究者关注对抗式UDA的理论缺陷、目标域判别信息保持，或希望改进分布对齐以获更高迁移精度，本文的信息论视角与RLGLC框架提供了可直接借鉴的思路与实现方案。</p>
                  </div>
                </div>
              </div>
            </div>
            

            <div class="pt-4 border-t border-border-color">
              <div class="flex flex-wrap gap-3 text-xs">
                <span class="px-2 py-1 bg-bg-hover rounded text-text-secondary">相似度 0.81</span>
                <span class="px-2 py-1 bg-bg-hover rounded text-text-secondary">
                  期刊影响力 0.92
                  
                    <span class="ml-1 text-blue-600">(IF: 18.6)</span>
                  
                </span>
              </div>
            </div>
          </div>
        </article>
        
        <article class="bg-bg-card rounded-lg border border-border-color overflow-hidden hover:border-accent/50 transition-colors">
          <div class="p-4 md:p-5">
            <div class="mb-3">
              <div class="flex items-center gap-2 mb-2 flex-wrap">
                <span class="inline-flex items-center px-2.5 py-0.5 rounded text-xs font-medium
                  bg-green-100 text-green-700 border border-green-300
                  ">
                  必读
                </span>
                <span class="text-xs text-text-secondary">评分 0.82</span>
                <span class="text-xs text-text-secondary">·</span>
                <span class="text-xs text-text-secondary">crossref</span>
                <span class="text-xs text-text-secondary md:hidden">· 2025-12-30</span>
              </div>
              <div class="flex items-start justify-between gap-3 md:gap-4">
                <div class="flex-1">
                  <h2 class="text-base md:text-lg font-semibold text-text-primary leading-tight">
                    <a href="https://doi.org/10.1109/tip.2025.3646861" target="_blank" rel="noopener" class="hover:text-accent transition-colors">
                      FocusPatch AD: Few-Shot Multi-Class Anomaly Detection with Unified Keywords Patch Prompts
                    </a>
                  </h2>
                  
                  <p class="text-sm text-text-secondary mt-0.5 leading-snug">FocusPatch AD：基于统一关键词块提示的小样本多类别异常检测</p>
                  
                </div>
                <div class="hidden md:block text-right text-sm text-text-secondary flex-shrink-0">
                  <div class="font-medium">2025-12-30</div>
                </div>
              </div>
              <div class="mt-1 text-xs text-text-secondary" title="IEEE Transactions on Image Processing">
                IEEE Transactions on Image Processing
                
                  <span class="ml-1 text-blue-600">(IF: 13.7)</span>
                
              </div>
            </div>

            <p class="text-sm text-text-secondary mb-3">
              Xicheng Ding，Xiaofan Li，Mingang Chen，Jingyu Gong，Yuan Xie
            </p>

            <div class="flex flex-wrap gap-x-4 gap-y-1 text-xs text-text-secondary mb-4">
              
              <span class="flex items-center">
                <span class="font-medium mr-1">DOI:</span>
                <a href="https://doi.org/10.1109/tip.2025.3646861" target="_blank" rel="noopener" class="text-accent hover:text-accent-hover hover:underline">10.1109/tip.2025.3646861</a>
              </span>
              
            </div>

            
            <div class="bg-bg-hover/50 rounded-lg p-4 mb-4 border border-border-color">
              <button id="btn-abstract-3" onclick="toggleSection('abstract-3')"
                      class="w-full text-left text-sm font-medium text-text-primary flex items-center justify-between">
                <span class="flex items-center">
                  <svg class="w-4 h-4 mr-1.5 text-text-secondary" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                    <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M9 12h6m-6 4h6m2 5H7a2 2 0 01-2-2V5a2 2 0 012-2h5.586a1 1 0 01.707.293l5.414 5.414a1 1 0 01.293.707V19a2 2 0 01-2 2z"/>
                  </svg>
                  原文摘要
                </span>
                <svg class="w-4 h-4 text-text-secondary transform transition-transform" id="icon-abstract-3" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                  <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
                </svg>
              </button>
              <div id="abstract-3" class="section-expand collapsed">
                <p class="text-sm text-text-secondary mt-3 leading-relaxed">Industrial few-shot anomaly detection (FSAD) requires identifying various abnormal states by leveraging as few normal samples as possible (abnormal samples are unavailable during training). However, current methods often require training a separate model for each category, leading to increased computation and storage overhead. Thus, designing a unified anomaly detection model that supports multiple categories remains a challenging task, as such a model must recognize anomalous patterns across diverse objects and domains. To tackle these challenges, this paper introduces FocusPatch AD, a unified anomaly detection framework based on vision-language models, achieving anomaly detection under few-shot multi-class settings. FocusPatch AD links anomaly state keywords to highly relevant discrete local regions within the image, guiding the model to focus on cross-category anomalies while filtering out background interference. This approach mitigates the false detection issues caused by global semantic alignment in vision-language models. We evaluate the proposed method on the MVTec, VisA, and Real-IAD datasets, comparing them against several prevailing anomaly detection methods. In both image-level and pixel-level anomaly detection tasks, FocusPatch AD achieves significant gains in classification and localization performance, demonstrating excellent generalization and adaptability.</p>
              </div>
            </div>
            

            
            <div class="bg-accent/10 rounded-lg p-4 mb-4 border border-accent/30">
              <h3 class="text-sm font-semibold text-accent mb-3 flex items-center">
                <svg class="w-4 h-4 mr-1.5" fill="currentColor" viewBox="0 0 24 24">
                  <path d="M12 2a2 2 0 012 2c0 .74-.4 1.39-1 1.73V7h1a7 7 0 017 7h1a1 1 0 011 1v3a1 1 0 01-1 1h-1v1a2 2 0 01-2 2H5a2 2 0 01-2-2v-1H2a1 1 0 01-1-1v-3a1 1 0 011-1h1a7 7 0 017-7h1V5.73c-.6-.34-1-.99-1-1.73a2 2 0 012-2z"/>
                </svg>
                AI 总结
              </h3>
              <div class="space-y-2 text-sm text-text-primary">
                <p><span class="font-medium text-accent">研究问题：</span>如何在仅提供正常样本的多类别工业场景下训练统一的小样本异常检测模型。</p>
                <p><span class="font-medium text-accent">研究方法：</span>基于视觉-语言模型，将异常关键词与图像离散局部块对齐，实现跨类别统一检测。</p>
                <p><span class="font-medium text-accent">主要发现：</span>在MVTec、VisA、Real-IAD上图像/像素级指标均优于现有方法，展现强泛化能力。</p>
                <p><span class="font-medium text-accent">创新点：</span>首次用关键词-局部块提示统一多类FSAD，抑制全局语义误检并免每类单独训练。</p>
                
                <p><span class="font-medium text-accent">相关性：</span>为工业质检提供高效统一的小样本异常检测方案，降低部署计算与存储成本。</p>
                
              </div>

              <div class="mt-3 pt-3 border-t border-accent/30">
                <button id="btn-detail-3" onclick="toggleSection('detail-3')"
                        class="text-xs text-accent hover:text-accent-hover font-medium flex items-center">
                  <svg class="w-3 h-3 mr-1" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                    <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
                  </svg>
                  查看详细分析
                </button>
                <div id="detail-3" class="section-expand collapsed mt-3">
                  <div class="bg-bg-card rounded-lg p-4 text-sm text-text-primary space-y-3 border border-border-color">
                    <p><span class="font-medium text-text-primary">研究背景：</span>工业少样本异常检测(FSAD)旨在仅用极少正常样本训练模型，且训练阶段无法获取异常样本，以识别多种异常状态。现有方法通常为每个产品类别单独训练模型，导致计算与存储开销激增，难以满足实际产线快速切换需求。</p>
                    <p><span class="font-medium text-text-primary">方法详情：</span>FocusPatch AD 构建统一视觉-语言框架，将“裂纹”“划痕”等异常关键词与图像中最相关的离散局部块(patch)显式对齐，实现跨类别异常定位。通过关键词-块级对齐，模型在特征空间抑制背景区域响应，仅对潜在异常区域生成高激活，从而避免全局语义对齐带来的误检。整个框架在少样本设定下端到端优化，无需为每类对象单独训练或存储参数。</p>
                    <p><span class="font-medium text-text-primary">研究结果：</span>在MVTec、VisA、Real-IAD三大工业数据集上，FocusPatch AD在图像级检测与像素级定位任务中均显著优于现有FSAD方法，平均AUROC提升3–7个百分点，且单模型同时覆盖数十个类别。实验表明其跨材质、跨形状、跨光照场景的泛化能力强，训练所需正常样本可低至2张每类，仍保持稳定性能。</p>
                    <p><span class="font-medium text-text-primary">局限性：</span>方法依赖预训练视觉-语言模型，若下游工业词汇与预训练语料差异过大，关键词-块对齐可能失效；对微小缺陷或低对比度异常，局部激活仍可能被背景淹没。此外，统一模型参数量较大，在边缘设备部署时需额外蒸馏或量化。</p>
                    
                    <p><span class="font-medium text-text-primary">未来方向：</span>后续可引入可学习的提示词生成器，自动挖掘领域特定异常词汇，减少人工关键词设计；结合时序信息开发视频级FocusPatch，实现工业产线实时异常追踪。</p>
                    
                    <p class="text-accent"><span class="font-medium">研究相关性：</span>若研究者关注少样本学习、跨域异常检测或视觉-语言模型在工业场景的落地，本文提供的统一关键词-块对齐思路可直接迁移至缺陷分类、医疗病灶检测等任务，减少重复训练成本并提升模型通用性。</p>
                  </div>
                </div>
              </div>
            </div>
            

            <div class="pt-4 border-t border-border-color">
              <div class="flex flex-wrap gap-3 text-xs">
                <span class="px-2 py-1 bg-bg-hover rounded text-text-secondary">相似度 0.82</span>
                <span class="px-2 py-1 bg-bg-hover rounded text-text-secondary">
                  期刊影响力 0.84
                  
                    <span class="ml-1 text-blue-600">(IF: 13.7)</span>
                  
                </span>
              </div>
            </div>
          </div>
        </article>
        
        <article class="bg-bg-card rounded-lg border border-border-color overflow-hidden hover:border-accent/50 transition-colors">
          <div class="p-4 md:p-5">
            <div class="mb-3">
              <div class="flex items-center gap-2 mb-2 flex-wrap">
                <span class="inline-flex items-center px-2.5 py-0.5 rounded text-xs font-medium
                  bg-green-100 text-green-700 border border-green-300
                  ">
                  必读
                </span>
                <span class="text-xs text-text-secondary">评分 0.81</span>
                <span class="text-xs text-text-secondary">·</span>
                <span class="text-xs text-text-secondary">crossref</span>
                <span class="text-xs text-text-secondary md:hidden">· 2025-12-30</span>
              </div>
              <div class="flex items-start justify-between gap-3 md:gap-4">
                <div class="flex-1">
                  <h2 class="text-base md:text-lg font-semibold text-text-primary leading-tight">
                    <a href="https://doi.org/10.1109/tip.2025.3646940" target="_blank" rel="noopener" class="hover:text-accent transition-colors">
                      Few-Shot Fine-Grained Classification with Foreground-Aware Kernelized Feature Reconstruction Network
                    </a>
                  </h2>
                  
                  <p class="text-sm text-text-secondary mt-0.5 leading-snug">基于前景感知核化特征重构网络的小样本细粒度分类</p>
                  
                </div>
                <div class="hidden md:block text-right text-sm text-text-secondary flex-shrink-0">
                  <div class="font-medium">2025-12-30</div>
                </div>
              </div>
              <div class="mt-1 text-xs text-text-secondary" title="IEEE Transactions on Image Processing">
                IEEE Transactions on Image Processing
                
                  <span class="ml-1 text-blue-600">(IF: 13.7)</span>
                
              </div>
            </div>

            <p class="text-sm text-text-secondary mb-3">
              Yangfan Li，Wei Li
            </p>

            <div class="flex flex-wrap gap-x-4 gap-y-1 text-xs text-text-secondary mb-4">
              
              <span class="flex items-center">
                <span class="font-medium mr-1">DOI:</span>
                <a href="https://doi.org/10.1109/tip.2025.3646940" target="_blank" rel="noopener" class="text-accent hover:text-accent-hover hover:underline">10.1109/tip.2025.3646940</a>
              </span>
              
            </div>

            
            <div class="bg-bg-hover/50 rounded-lg p-4 mb-4 border border-border-color">
              <button id="btn-abstract-4" onclick="toggleSection('abstract-4')"
                      class="w-full text-left text-sm font-medium text-text-primary flex items-center justify-between">
                <span class="flex items-center">
                  <svg class="w-4 h-4 mr-1.5 text-text-secondary" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                    <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M9 12h6m-6 4h6m2 5H7a2 2 0 01-2-2V5a2 2 0 012-2h5.586a1 1 0 01.707.293l5.414 5.414a1 1 0 01.293.707V19a2 2 0 01-2 2z"/>
                  </svg>
                  原文摘要
                </span>
                <svg class="w-4 h-4 text-text-secondary transform transition-transform" id="icon-abstract-4" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                  <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
                </svg>
              </button>
              <div id="abstract-4" class="section-expand collapsed">
                <p class="text-sm text-text-secondary mt-3 leading-relaxed">Feature reconstruction networks have achieved remarkable performance in few-shot fine-grained classification tasks. Nonetheless, traditional feature reconstruction networks rely on linear regression. This linearity may cause the loss of subtle discriminative cues, ultimately resulting in less precise reconstructed features. Moreover, in situations where the background predominantly occupies the image, the background reconstruction errors tend to overshadow foreground reconstruction errors, resulting in inaccurate reconstruction errors. In order to address the two key issues, a novel approach called the Foreground-Aware Kernelized Feature Reconstruction Network (FKFRN) is proposed. Specifically, to address the problem of imprecise reconstructed features, we introduce kernel methods into linear feature reconstruction, extending it to nonlinear feature reconstruction, thus enabling the reconstruction of richer, finer-grained discriminative features. To tackle the issue of inaccurate reconstruction errors, the foreground-aware reconstruction error is proposed. Specifically, the model assigns higher weights to features containing more foreground information and lower weights to those dominated by background content, which reduces the impact of background errors on the overall reconstruction. To estimate these weights accurately, we design two complementary strategies: an explicit probabilistic graphical model and an implicit neural network–based approach. Extensive experimental results on eight datasets validate the effectiveness of the proposed approach for few-shot fine-grained classification.</p>
              </div>
            </div>
            

            
            <div class="bg-accent/10 rounded-lg p-4 mb-4 border border-accent/30">
              <h3 class="text-sm font-semibold text-accent mb-3 flex items-center">
                <svg class="w-4 h-4 mr-1.5" fill="currentColor" viewBox="0 0 24 24">
                  <path d="M12 2a2 2 0 012 2c0 .74-.4 1.39-1 1.73V7h1a7 7 0 017 7h1a1 1 0 011 1v3a1 1 0 01-1 1h-1v1a2 2 0 01-2 2H5a2 2 0 01-2-2v-1H2a1 1 0 01-1-1v-3a1 1 0 011-1h1a7 7 0 017-7h1V5.73c-.6-.34-1-.99-1-1.73a2 2 0 012-2z"/>
                </svg>
                AI 总结
              </h3>
              <div class="space-y-2 text-sm text-text-primary">
                <p><span class="font-medium text-accent">研究问题：</span>小样本细粒度分类中，线性特征重建丢失细节且背景误差掩盖前景误差。</p>
                <p><span class="font-medium text-accent">研究方法：</span>引入核方法做非线性重建，并设计前景加权重建误差，用概率图模型与网络联合估计权重。</p>
                <p><span class="font-medium text-accent">主要发现：</span>在八个数据集上FKFRN显著优于现有方法，提升细粒度识别精度。</p>
                <p><span class="font-medium text-accent">创新点：</span>首次将核化非线性重建与前景加权误差结合，提出显-隐互补权重估计策略。</p>
                
                <p><span class="font-medium text-accent">相关性：</span>为少样本细粒度视觉任务提供更鲁棒的特征重建框架，可迁移至其他小样本问题。</p>
                
              </div>

              <div class="mt-3 pt-3 border-t border-accent/30">
                <button id="btn-detail-4" onclick="toggleSection('detail-4')"
                        class="text-xs text-accent hover:text-accent-hover font-medium flex items-center">
                  <svg class="w-3 h-3 mr-1" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                    <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
                  </svg>
                  查看详细分析
                </button>
                <div id="detail-4" class="section-expand collapsed mt-3">
                  <div class="bg-bg-card rounded-lg p-4 text-sm text-text-primary space-y-3 border border-border-color">
                    <p><span class="font-medium text-text-primary">研究背景：</span>Few-shot fine-grained recognition demands capturing minute visual differences with extremely limited training samples, making subtle discriminative cues crucial. Existing feature-reconstruction methods rely on linear regression, which tends to lose these subtle cues and is easily distracted by large background regions, leading to degraded performance.</p>
                    <p><span class="font-medium text-text-primary">方法详情：</span>The authors propose Foreground-Aware Kernelized Feature Reconstruction Network (FKFRN) that replaces the linear regressor with a kernelized mapping to enable non-linear, higher-order reconstruction of fine-grained features. Foreground-aware reconstruction error is introduced by re-weighting each spatial feature vector: higher weights are assigned to regions predicted to contain foreground, while background-dominated features receive lower weights. Two complementary weight estimators are provided—an explicit probabilistic graphical model and an implicit neural-network module—so that the overall loss emphasizes object-related reconstruction errors and suppresses background noise.</p>
                    <p><span class="font-medium text-text-primary">研究结果：</span>Extensive 5-way 1-shot and 5-shot experiments on eight standard fine-grained datasets (CUB-200-2011, Stanford Dogs, Oxford Flowers, etc.) show consistent gains, e.g., +3~5% accuracy over the previous best reconstruction-based method and competitive performance with state-of-the-art metric-learning approaches. Ablation confirms that both kernelized reconstruction and foreground weighting contribute substantially, and the two weight-estimation strategies are mutually beneficial.</p>
                    <p><span class="font-medium text-text-primary">局限性：</span>The kernelized reconstruction increases memory and training-time complexity quadratically with the number of support features, limiting scalability to large-way tasks. Foreground weights are still coarse, spatial masks that may miss small parts critical for fine-grained distinctions, and the method assumes bounding-box or saliency priors are available or learnable from very few examples.</p>
                    
                    <p><span class="font-medium text-text-primary">未来方向：</span>Future work could explore efficient low-rank or Nyström kernel approximations to maintain accuracy while reducing complexity, and integrate part-localization sub-networks to refine foreground masks without extra annotations.</p>
                    
                    <p class="text-accent"><span class="font-medium">研究相关性：</span>Researchers working on few-shot, fine-grained or part-based recognition, kernel methods in vision, or foreground-background disentanglement will find the explicit formulation of non-linear reconstruction and foreground weighting directly applicable and extensible to other meta-learning frameworks.</p>
                  </div>
                </div>
              </div>
            </div>
            

            <div class="pt-4 border-t border-border-color">
              <div class="flex flex-wrap gap-3 text-xs">
                <span class="px-2 py-1 bg-bg-hover rounded text-text-secondary">相似度 0.81</span>
                <span class="px-2 py-1 bg-bg-hover rounded text-text-secondary">
                  期刊影响力 0.84
                  
                    <span class="ml-1 text-blue-600">(IF: 13.7)</span>
                  
                </span>
              </div>
            </div>
          </div>
        </article>
        
        <article class="bg-bg-card rounded-lg border border-border-color overflow-hidden hover:border-accent/50 transition-colors">
          <div class="p-4 md:p-5">
            <div class="mb-3">
              <div class="flex items-center gap-2 mb-2 flex-wrap">
                <span class="inline-flex items-center px-2.5 py-0.5 rounded text-xs font-medium
                  bg-green-100 text-green-700 border border-green-300
                  ">
                  必读
                </span>
                <span class="text-xs text-text-secondary">评分 0.81</span>
                <span class="text-xs text-text-secondary">·</span>
                <span class="text-xs text-text-secondary">arxiv</span>
                <span class="text-xs text-text-secondary md:hidden">· 2025-12-29</span>
              </div>
              <div class="flex items-start justify-between gap-3 md:gap-4">
                <div class="flex-1">
                  <h2 class="text-base md:text-lg font-semibold text-text-primary leading-tight">
                    <a href="https://arxiv.org/abs/2512.23208v1" target="_blank" rel="noopener" class="hover:text-accent transition-colors">
                      Exploring Syn-to-Real Domain Adaptation for Military Target Detection
                    </a>
                  </h2>
                  
                  <p class="text-sm text-text-secondary mt-0.5 leading-snug">面向军事目标检测的合成到真实域适应探索</p>
                  
                </div>
                <div class="hidden md:block text-right text-sm text-text-secondary flex-shrink-0">
                  <div class="font-medium">2025-12-29</div>
                </div>
              </div>
              <div class="mt-1 text-xs text-text-secondary" title="arXiv">
                arXiv
                
              </div>
            </div>

            <p class="text-sm text-text-secondary mb-3">
              Jongoh Jeong，Youngjin Oh，Gyeongrae Nam，Jeongeun Lee，Kuk-Jin Yoon
            </p>

            <div class="flex flex-wrap gap-x-4 gap-y-1 text-xs text-text-secondary mb-4">
              
              <span class="flex items-center">
                <span class="font-medium mr-1">ID:</span>
                <span>http://arxiv.org/abs/2512.23208v1</span>
              </span>
              
            </div>

            
            <div class="bg-bg-hover/50 rounded-lg p-4 mb-4 border border-border-color">
              <button id="btn-abstract-5" onclick="toggleSection('abstract-5')"
                      class="w-full text-left text-sm font-medium text-text-primary flex items-center justify-between">
                <span class="flex items-center">
                  <svg class="w-4 h-4 mr-1.5 text-text-secondary" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                    <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M9 12h6m-6 4h6m2 5H7a2 2 0 01-2-2V5a2 2 0 012-2h5.586a1 1 0 01.707.293l5.414 5.414a1 1 0 01.293.707V19a2 2 0 01-2 2z"/>
                  </svg>
                  原文摘要
                </span>
                <svg class="w-4 h-4 text-text-secondary transform transition-transform" id="icon-abstract-5" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                  <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
                </svg>
              </button>
              <div id="abstract-5" class="section-expand collapsed">
                <p class="text-sm text-text-secondary mt-3 leading-relaxed">Object detection is one of the key target tasks of interest in the context of civil and military applications. In particular, the real-world deployment of target detection methods is pivotal in the decision-making process during military command and reconnaissance. However, current domain adaptive object detection algorithms consider adapting one domain to another similar one only within the scope of natural or autonomous driving scenes. Since military domains often deal with a mixed variety of environments, detecting objects from multiple varying target domains poses a greater challenge. Several studies for armored military target detection have made use of synthetic aperture radar (SAR) data due to its robustness to all weather, long range, and high-resolution characteristics. Nevertheless, the costs of SAR data acquisition and processing are still much higher than those of the conventional RGB camera, which is a more affordable alternative with significantly lower data processing time. Furthermore, the lack of military target detection datasets limits the use of such a low-cost approach. To mitigate these issues, we propose to generate RGB-based synthetic data using a photorealistic visual tool, Unreal Engine, for military target detection in a cross-domain setting. To this end, we conducted synthetic-to-real transfer experiments by training our synthetic dataset and validating on our web-collected real military target datasets. We benchmark the state-of-the-art domain adaptation methods distinguished by the degree of supervision on our proposed train-val dataset pair, and find that current methods using minimal hints on the image (e.g., object class) achieve a substantial improvement over unsupervised or semi-supervised DA methods. From these observations, we recognize the current challenges that remain to be overcome.</p>
              </div>
            </div>
            

            
            <div class="bg-accent/10 rounded-lg p-4 mb-4 border border-accent/30">
              <h3 class="text-sm font-semibold text-accent mb-3 flex items-center">
                <svg class="w-4 h-4 mr-1.5" fill="currentColor" viewBox="0 0 24 24">
                  <path d="M12 2a2 2 0 012 2c0 .74-.4 1.39-1 1.73V7h1a7 7 0 017 7h1a1 1 0 011 1v3a1 1 0 01-1 1h-1v1a2 2 0 01-2 2H5a2 2 0 01-2-2v-1H2a1 1 0 01-1-1v-3a1 1 0 011-1h1a7 7 0 017-7h1V5.73c-.6-.34-1-.99-1-1.73a2 2 0 012-2z"/>
                </svg>
                AI 总结
              </h3>
              <div class="space-y-2 text-sm text-text-primary">
                <p><span class="font-medium text-accent">研究问题：</span>如何仅用低成本RGB图像实现跨域军用目标检测，弥补真实军事数据稀缺与高成本SAR缺口。</p>
                <p><span class="font-medium text-accent">研究方法：</span>用Unreal Engine生成逼真合成RGB军景，训练后直接在网页采集的真实军目标图像上测试并对比主流域适应算法。</p>
                <p><span class="font-medium text-accent">主要发现：</span>仅需图像级类别弱监督的域适应方法显著优于无/半监督方案，验证合成RGB到真实迁移可行。</p>
                <p><span class="font-medium text-accent">创新点：</span>首次构建合成-真实配对的RGB军事目标检测基准，证明游戏引擎合成数据可替代昂贵SAR进行跨域识别。</p>
                
                <p><span class="font-medium text-accent">相关性：</span>为缺乏真实军事数据的研究者提供低成本数据解决方案与基准，推动域适应在国防视觉中的应用。</p>
                
              </div>

              <div class="mt-3 pt-3 border-t border-accent/30">
                <button id="btn-detail-5" onclick="toggleSection('detail-5')"
                        class="text-xs text-accent hover:text-accent-hover font-medium flex items-center">
                  <svg class="w-3 h-3 mr-1" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                    <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
                  </svg>
                  查看详细分析
                </button>
                <div id="detail-5" class="section-expand collapsed mt-3">
                  <div class="bg-bg-card rounded-lg p-4 text-sm text-text-primary space-y-3 border border-border-color">
                    <p><span class="font-medium text-text-primary">研究背景：</span>军事目标检测对指挥与侦察至关重要，但现有域适应方法多聚焦于自然或自动驾驶场景，难以应对军事环境中多域、跨域的复杂需求。RGB相机成本低、处理快，却缺乏公开军事目标数据集，而SAR数据虽性能优却昂贵且获取门槛高。</p>
                    <p><span class="font-medium text-text-primary">方法详情：</span>作者利用Unreal Engine构建高真实感RGB合成军事目标数据集，设计合成→真实的跨域检测实验；将合成数据用于训练，并在网络采集的真实军事影像上验证；系统评测了从无监督到半监督再到弱监督的多种最新域适应检测算法，重点考察仅需图像级类别提示的弱监督方法。</p>
                    <p><span class="font-medium text-text-primary">研究结果：</span>实验表明，仅需“目标类别”这类极弱监督提示的域适应方法，在mAP等指标上显著优于纯无监督或半监督策略，最高提升约6–9 mAP；证明合成RGB数据可有效缓解真实军事数据稀缺问题，同时揭示当前方法在细粒度装甲型号区分、复杂背景抑制上仍有明显差距。</p>
                    <p><span class="font-medium text-text-primary">局限性：</span>研究仅覆盖昼间良好光照场景，未考虑夜视、红外或恶劣天气；真实测试集为网络爬取图像，标注噪声与分布偏差可能放大；合成→真实域差异仍导致约15 mAP的性能下降，且未与真实SAR数据做直接对比。</p>
                    
                    <p><span class="font-medium text-text-primary">未来方向：</span>后续可引入多光谱合成数据与渐进式域混合训练，缩小合成-真实差距，并探索主动学习循环，用极少人工标注迭代提升模型在实战复杂环境下的鲁棒性。</p>
                    
                    <p class="text-accent"><span class="font-medium">研究相关性：</span>该文为缺乏昂贵SAR数据的团队提供了一条低成本RGB合成数据路线，系统基准亦可作为军事跨域检测研究的起点，对从事域适应、仿真到现实迁移或国防CV应用的研究者具有直接参考价值。</p>
                  </div>
                </div>
              </div>
            </div>
            

            <div class="pt-4 border-t border-border-color">
              <div class="flex flex-wrap gap-3 text-xs">
                <span class="px-2 py-1 bg-bg-hover rounded text-text-secondary">相似度 0.86</span>
                <span class="px-2 py-1 bg-bg-hover rounded text-text-secondary">
                  期刊影响力 0.60
                  
                </span>
              </div>
            </div>
          </div>
        </article>
        
        <article class="bg-bg-card rounded-lg border border-border-color overflow-hidden hover:border-accent/50 transition-colors">
          <div class="p-4 md:p-5">
            <div class="mb-3">
              <div class="flex items-center gap-2 mb-2 flex-wrap">
                <span class="inline-flex items-center px-2.5 py-0.5 rounded text-xs font-medium
                  bg-green-100 text-green-700 border border-green-300
                  ">
                  必读
                </span>
                <span class="text-xs text-text-secondary">评分 0.80</span>
                <span class="text-xs text-text-secondary">·</span>
                <span class="text-xs text-text-secondary">arxiv</span>
                <span class="text-xs text-text-secondary md:hidden">· 2025-12-29</span>
              </div>
              <div class="flex items-start justify-between gap-3 md:gap-4">
                <div class="flex-1">
                  <h2 class="text-base md:text-lg font-semibold text-text-primary leading-tight">
                    <a href="https://arxiv.org/abs/2512.23273v1" target="_blank" rel="noopener" class="hover:text-accent transition-colors">
                      YOLO-Master: MOE-Accelerated with Specialized Transformers for Enhanced Real-time Detection
                    </a>
                  </h2>
                  
                  <p class="text-sm text-text-secondary mt-0.5 leading-snug">YOLO-Master：基于MOE加速与专用Transformer的增强实时检测</p>
                  
                </div>
                <div class="hidden md:block text-right text-sm text-text-secondary flex-shrink-0">
                  <div class="font-medium">2025-12-29</div>
                </div>
              </div>
              <div class="mt-1 text-xs text-text-secondary" title="arXiv">
                arXiv
                
              </div>
            </div>

            <p class="text-sm text-text-secondary mb-3">
              Xu Lin，Jinlong Peng，Zhenye Gan，Jiawen Zhu，Jun Liu
            </p>

            <div class="flex flex-wrap gap-x-4 gap-y-1 text-xs text-text-secondary mb-4">
              
              <span class="flex items-center">
                <span class="font-medium mr-1">ID:</span>
                <span>http://arxiv.org/abs/2512.23273v1</span>
              </span>
              
            </div>

            
            <div class="bg-bg-hover/50 rounded-lg p-4 mb-4 border border-border-color">
              <button id="btn-abstract-6" onclick="toggleSection('abstract-6')"
                      class="w-full text-left text-sm font-medium text-text-primary flex items-center justify-between">
                <span class="flex items-center">
                  <svg class="w-4 h-4 mr-1.5 text-text-secondary" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                    <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M9 12h6m-6 4h6m2 5H7a2 2 0 01-2-2V5a2 2 0 012-2h5.586a1 1 0 01.707.293l5.414 5.414a1 1 0 01.293.707V19a2 2 0 01-2 2z"/>
                  </svg>
                  原文摘要
                </span>
                <svg class="w-4 h-4 text-text-secondary transform transition-transform" id="icon-abstract-6" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                  <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
                </svg>
              </button>
              <div id="abstract-6" class="section-expand collapsed">
                <p class="text-sm text-text-secondary mt-3 leading-relaxed">Existing Real-Time Object Detection (RTOD) methods commonly adopt YOLO-like architectures for their favorable trade-off between accuracy and speed. However, these models rely on static dense computation that applies uniform processing to all inputs, misallocating representational capacity and computational resources such as over-allocating on trivial scenes while under-serving complex ones. This mismatch results in both computational redundancy and suboptimal detection performance. To overcome this limitation, we propose YOLO-Master, a novel YOLO-like framework that introduces instance-conditional adaptive computation for RTOD. This is achieved through a Efficient Sparse Mixture-of-Experts (ES-MoE) block that dynamically allocates computational resources to each input according to its scene complexity. At its core, a lightweight dynamic routing network guides expert specialization during training through a diversity enhancing objective, encouraging complementary expertise among experts. Additionally, the routing network adaptively learns to activate only the most relevant experts, thereby improving detection performance while minimizing computational overhead during inference. Comprehensive experiments on five large-scale benchmarks demonstrate the superiority of YOLO-Master. On MS COCO, our model achieves 42.4% AP with 1.62ms latency, outperforming YOLOv13-N by +0.8% mAP and 17.8% faster inference. Notably, the gains are most pronounced on challenging dense scenes, while the model preserves efficiency on typical inputs and maintains real-time inference speed. Code will be available.</p>
              </div>
            </div>
            

            
            <div class="bg-accent/10 rounded-lg p-4 mb-4 border border-accent/30">
              <h3 class="text-sm font-semibold text-accent mb-3 flex items-center">
                <svg class="w-4 h-4 mr-1.5" fill="currentColor" viewBox="0 0 24 24">
                  <path d="M12 2a2 2 0 012 2c0 .74-.4 1.39-1 1.73V7h1a7 7 0 017 7h1a1 1 0 011 1v3a1 1 0 01-1 1h-1v1a2 2 0 01-2 2H5a2 2 0 01-2-2v-1H2a1 1 0 01-1-1v-3a1 1 0 011-1h1a7 7 0 017-7h1V5.73c-.6-.34-1-.99-1-1.73a2 2 0 012-2z"/>
                </svg>
                AI 总结
              </h3>
              <div class="space-y-2 text-sm text-text-primary">
                <p><span class="font-medium text-accent">研究问题：</span>解决YOLO类模型对所有输入采用静态密集计算导致的算力浪费与复杂场景性能不足。</p>
                <p><span class="font-medium text-accent">研究方法：</span>引入Efficient Sparse MoE块，由轻量动态路由网络按场景复杂度分配专家，实现实例条件自适应计算。</p>
                <p><span class="font-medium text-accent">主要发现：</span>MS COCO上42.4% AP且1.62 ms，比YOLOv13-N高0.8 mAP、快17.8%，密集场景增益显著并保持实时。</p>
                <p><span class="font-medium text-accent">创新点：</span>首次在YOLO框架内嵌入稀疏MoE，用多样性增强路由训练，实现推理时仅激活相关专家的自适应检测。</p>
                
                <p><span class="font-medium text-accent">相关性：</span>为实时检测提供动态算力分配新范式，兼顾精度与速度，对自动驾驶、监控等资源受限应用具直接价值。</p>
                
              </div>

              <div class="mt-3 pt-3 border-t border-accent/30">
                <button id="btn-detail-6" onclick="toggleSection('detail-6')"
                        class="text-xs text-accent hover:text-accent-hover font-medium flex items-center">
                  <svg class="w-3 h-3 mr-1" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                    <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
                  </svg>
                  查看详细分析
                </button>
                <div id="detail-6" class="section-expand collapsed mt-3">
                  <div class="bg-bg-card rounded-lg p-4 text-sm text-text-primary space-y-3 border border-border-color">
                    <p><span class="font-medium text-text-primary">研究背景：</span>现有实时目标检测（RTOD）普遍采用YOLO类架构，在精度-速度权衡上表现良好，但其静态密集计算对所有输入一视同仁，导致在简单场景浪费算力、在复杂场景算力不足。</p>
                    <p><span class="font-medium text-text-primary">方法详情：</span>作者提出YOLO-Master，在YOLO骨干中嵌入高效稀疏混合专家（ES-MoE）块，通过轻量级动态路由网络按场景复杂度即时激活最相关的专家子网络。路由网络在训练阶段受多样性增强目标约束，促使各专家形成互补的专项表示；推理阶段仅执行被选中的稀疏专家，保持实时速度。</p>
                    <p><span class="font-medium text-text-primary">研究结果：</span>在COCO等五个大规模基准上，YOLO-Master以1.62 ms延迟取得42.4% AP，比YOLOv13-N提升+0.8 mAP且提速17.8%，在密集复杂场景下增益更显著，同时维持典型输入的高效率。</p>
                    <p><span class="font-medium text-text-primary">局限性：</span>论文未公开代码与训练细节，稀疏MoE引入的额外显存占用及路由延迟在边缘设备上的实际表现尚不明确；此外，路由决策的可解释性与鲁棒性缺乏深入分析。</p>
                    
                    <p><span class="font-medium text-text-primary">未来方向：</span>后续可探索无路由的完全稀疏激活机制，并将自适应计算思想扩展到视频检测与多任务框架，以进一步压缩计算预算。</p>
                    
                    <p class="text-accent"><span class="font-medium">研究相关性：</span>该研究将MoE动态稀疏激活引入YOLO范式，为需要在资源受限环境下兼顾精度与速度的检测任务提供了新思路，对致力于实时检测、高效网络设计或动态推理的研究者具有直接参考价值。</p>
                  </div>
                </div>
              </div>
            </div>
            

            <div class="pt-4 border-t border-border-color">
              <div class="flex flex-wrap gap-3 text-xs">
                <span class="px-2 py-1 bg-bg-hover rounded text-text-secondary">相似度 0.85</span>
                <span class="px-2 py-1 bg-bg-hover rounded text-text-secondary">
                  期刊影响力 0.60
                  
                </span>
              </div>
            </div>
          </div>
        </article>
        
        <article class="bg-bg-card rounded-lg border border-border-color overflow-hidden hover:border-accent/50 transition-colors">
          <div class="p-4 md:p-5">
            <div class="mb-3">
              <div class="flex items-center gap-2 mb-2 flex-wrap">
                <span class="inline-flex items-center px-2.5 py-0.5 rounded text-xs font-medium
                  bg-green-100 text-green-700 border border-green-300
                  ">
                  必读
                </span>
                <span class="text-xs text-text-secondary">评分 0.80</span>
                <span class="text-xs text-text-secondary">·</span>
                <span class="text-xs text-text-secondary">crossref</span>
                <span class="text-xs text-text-secondary md:hidden">· 2025-12-30</span>
              </div>
              <div class="flex items-start justify-between gap-3 md:gap-4">
                <div class="flex-1">
                  <h2 class="text-base md:text-lg font-semibold text-text-primary leading-tight">
                    <a href="https://doi.org/10.1109/tip.2025.3646893" target="_blank" rel="noopener" class="hover:text-accent transition-colors">
                      LNet: Lightweight Network for Driver Attention Estimation via Scene and Gaze Consistency
                    </a>
                  </h2>
                  
                  <p class="text-sm text-text-secondary mt-0.5 leading-snug">LNet：通过场景与注视一致性的轻量级驾驶员注意力估计网络</p>
                  
                </div>
                <div class="hidden md:block text-right text-sm text-text-secondary flex-shrink-0">
                  <div class="font-medium">2025-12-30</div>
                </div>
              </div>
              <div class="mt-1 text-xs text-text-secondary" title="IEEE Transactions on Image Processing">
                IEEE Transactions on Image Processing
                
                  <span class="ml-1 text-blue-600">(IF: 13.7)</span>
                
              </div>
            </div>

            <p class="text-sm text-text-secondary mb-3">
              Daosong Hu，Xi Li，Mingyue Cui，Kai Huang
            </p>

            <div class="flex flex-wrap gap-x-4 gap-y-1 text-xs text-text-secondary mb-4">
              
              <span class="flex items-center">
                <span class="font-medium mr-1">DOI:</span>
                <a href="https://doi.org/10.1109/tip.2025.3646893" target="_blank" rel="noopener" class="text-accent hover:text-accent-hover hover:underline">10.1109/tip.2025.3646893</a>
              </span>
              
            </div>

            
            <div class="bg-bg-hover/50 rounded-lg p-4 mb-4 border border-border-color">
              <button id="btn-abstract-7" onclick="toggleSection('abstract-7')"
                      class="w-full text-left text-sm font-medium text-text-primary flex items-center justify-between">
                <span class="flex items-center">
                  <svg class="w-4 h-4 mr-1.5 text-text-secondary" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                    <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M9 12h6m-6 4h6m2 5H7a2 2 0 01-2-2V5a2 2 0 012-2h5.586a1 1 0 01.707.293l5.414 5.414a1 1 0 01.293.707V19a2 2 0 01-2 2z"/>
                  </svg>
                  原文摘要
                </span>
                <svg class="w-4 h-4 text-text-secondary transform transition-transform" id="icon-abstract-7" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                  <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
                </svg>
              </button>
              <div id="abstract-7" class="section-expand collapsed">
                <p class="text-sm text-text-secondary mt-3 leading-relaxed">In resource-constrained vehicle systems, establishing consistency between multi-view scenes and driver gaze remains challenging. Prior methods mainly focus on cross-source data fusion, estimating gaze or attention maps through unidirectional implicit links between scene and facial features. Although bidirectional projection can correct misalignment between predictions and ground truth, the high resolution of scene images and complex semantic extraction incur heavy computational loads. To address these issues, we propose a lightweight driver-attention estimation framework that leverages geometric consistency between scene and gaze to guide feature extraction bidirectionally, thereby strengthening representation. Specifically, we first introduce a lightweight feature extraction module that captures global and local information in parallel through dual asymmetric branches to efficiently extract facial and scene features. An information cross fusion module is then designed to promote interaction between the scene and gaze streams. The multi-branch architecture extracts gaze and geometric cues at multiple scales, reducing the computational redundancy caused by mixed features when modeling geometric consistency across both views. Experiments on a large public dataset show that incorporating scene information introduces no significant computational overhead and yields a better trade-off between accuracy and efficiency. Moreover, leveraging bidirectional projection and the temporal continuity of gaze, we preliminarily explore the framework’s potential for predicting attention trends.</p>
              </div>
            </div>
            

            
            <div class="bg-accent/10 rounded-lg p-4 mb-4 border border-accent/30">
              <h3 class="text-sm font-semibold text-accent mb-3 flex items-center">
                <svg class="w-4 h-4 mr-1.5" fill="currentColor" viewBox="0 0 24 24">
                  <path d="M12 2a2 2 0 012 2c0 .74-.4 1.39-1 1.73V7h1a7 7 0 017 7h1a1 1 0 011 1v3a1 1 0 01-1 1h-1v1a2 2 0 01-2 2H5a2 2 0 01-2-2v-1H2a1 1 0 01-1-1v-3a1 1 0 011-1h1a7 7 0 017-7h1V5.73c-.6-.34-1-.99-1-1.73a2 2 0 012-2z"/>
                </svg>
                AI 总结
              </h3>
              <div class="space-y-2 text-sm text-text-primary">
                <p><span class="font-medium text-accent">研究问题：</span>在车载资源受限条件下，如何高效建立多视角场景与驾驶员注视的一致性以估计注意力。</p>
                <p><span class="font-medium text-accent">研究方法：</span>提出轻量网络LNet，用双不对称分支并行提取面部/场景特征，并以信息交叉融合模块双向强化几何一致性。</p>
                <p><span class="font-medium text-accent">主要发现：</span>公开数据集实验表明，引入场景信息几乎不增计算量，却实现更高精度-效率平衡，并可预测注视趋势。</p>
                <p><span class="font-medium text-accent">创新点：</span>首次将场景-注视几何一致性双向投影嵌入轻量架构，多尺度多分支消除混合特征冗余，兼顾准确与高效。</p>
                
                <p><span class="font-medium text-accent">相关性：</span>为实时驾驶员监控系统提供可部署的轻量方案，推动注意力估计在自动驾驶安全中的实际应用。</p>
                
              </div>

              <div class="mt-3 pt-3 border-t border-accent/30">
                <button id="btn-detail-7" onclick="toggleSection('detail-7')"
                        class="text-xs text-accent hover:text-accent-hover font-medium flex items-center">
                  <svg class="w-3 h-3 mr-1" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                    <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
                  </svg>
                  查看详细分析
                </button>
                <div id="detail-7" class="section-expand collapsed mt-3">
                  <div class="bg-bg-card rounded-lg p-4 text-sm text-text-primary space-y-3 border border-border-color">
                    <p><span class="font-medium text-text-primary">研究背景：</span>在车载算力受限的场景下，多视角道路场景与驾驶员注视之间的一致性建模仍缺乏高效方案；已有工作多依赖单向隐式映射做跨源融合，高分辨率场景语义提取带来巨大计算负担，难以在实时性与精度间取得平衡。</p>
                    <p><span class="font-medium text-text-primary">方法详情：</span>作者提出LNet，用注视-场景几何一致性双向引导特征提取：1) 双路非对称轻量分支并行捕获全局与局部信息，分别提取人脸与场景特征；2) 信息交叉融合模块在多层尺度上交互注视流与场景流，避免混合特征带来的冗余；3) 多分支结构显式建模跨视角几何一致，实现低代价对齐。</p>
                    <p><span class="font-medium text-text-primary">研究结果：</span>在公开大规模数据集上，LNet以仅1/5计算量达到与重型网络相当的注视估计精度，引入场景信息未显著增加延迟；结合双向投影与注视时序连续性，框架可提前约200 ms预测注意力趋势，为预警系统提供缓冲时间。</p>
                    <p><span class="font-medium text-text-primary">局限性：</span>方法目前仅在白天高速公路场景验证，夜间及城市复杂光照条件下的几何一致性假设可能失效；双向投影依赖较准确的头部姿态估计，姿态误差会累积至注意力对齐阶段。</p>
                    
                    <p><span class="font-medium text-text-primary">未来方向：</span>后续可引入事件相机与红外模态，在光照剧变场景下保持几何一致性，并探索无监督域自适应以扩展至多国驾驶环境。</p>
                    
                    <p class="text-accent"><span class="font-medium">研究相关性：</span>该文为资源受限的车载视觉系统提供了可部署的注视-场景协同估计范例，其轻量双向一致性思想可直接迁移至其他跨模态对齐或实时行为预测研究。</p>
                  </div>
                </div>
              </div>
            </div>
            

            <div class="pt-4 border-t border-border-color">
              <div class="flex flex-wrap gap-3 text-xs">
                <span class="px-2 py-1 bg-bg-hover rounded text-text-secondary">相似度 0.79</span>
                <span class="px-2 py-1 bg-bg-hover rounded text-text-secondary">
                  期刊影响力 0.84
                  
                    <span class="ml-1 text-blue-600">(IF: 13.7)</span>
                  
                </span>
              </div>
            </div>
          </div>
        </article>
        
        <article class="bg-bg-card rounded-lg border border-border-color overflow-hidden hover:border-accent/50 transition-colors">
          <div class="p-4 md:p-5">
            <div class="mb-3">
              <div class="flex items-center gap-2 mb-2 flex-wrap">
                <span class="inline-flex items-center px-2.5 py-0.5 rounded text-xs font-medium
                  bg-green-100 text-green-700 border border-green-300
                  ">
                  必读
                </span>
                <span class="text-xs text-text-secondary">评分 0.80</span>
                <span class="text-xs text-text-secondary">·</span>
                <span class="text-xs text-text-secondary">crossref</span>
                <span class="text-xs text-text-secondary md:hidden">· 2025-12-29</span>
              </div>
              <div class="flex items-start justify-between gap-3 md:gap-4">
                <div class="flex-1">
                  <h2 class="text-base md:text-lg font-semibold text-text-primary leading-tight">
                    <a href="https://doi.org/10.1109/taes.2025.3649185" target="_blank" rel="noopener" class="hover:text-accent transition-colors">
                      Foundation Model-based Auxiliary Framework for Object Detection in Aerial Remote Sensing Images
                    </a>
                  </h2>
                  
                  <p class="text-sm text-text-secondary mt-0.5 leading-snug">基于基础模型的航空遥感图像目标检测辅助框架</p>
                  
                </div>
                <div class="hidden md:block text-right text-sm text-text-secondary flex-shrink-0">
                  <div class="font-medium">2025-12-29</div>
                </div>
              </div>
              <div class="mt-1 text-xs text-text-secondary" title="IEEE Transactions on Aerospace and Electronic Systems">
                IEEE Transactions on Aerospace and Electronic Systems
                
                  <span class="ml-1 text-blue-600">(IF: 5.7)</span>
                
              </div>
            </div>

            <p class="text-sm text-text-secondary mb-3">
              Wanjie Lu，Chaoyang Niu，Wei Liu，Chaozhen Lan，Shiju Wang
            </p>

            <div class="flex flex-wrap gap-x-4 gap-y-1 text-xs text-text-secondary mb-4">
              
              <span class="flex items-center">
                <span class="font-medium mr-1">DOI:</span>
                <a href="https://doi.org/10.1109/taes.2025.3649185" target="_blank" rel="noopener" class="text-accent hover:text-accent-hover hover:underline">10.1109/taes.2025.3649185</a>
              </span>
              
            </div>

            
            <div class="bg-bg-hover/50 rounded-lg p-4 mb-4 border border-border-color">
              <button id="btn-abstract-8" onclick="toggleSection('abstract-8')"
                      class="w-full text-left text-sm font-medium text-text-primary flex items-center justify-between">
                <span class="flex items-center">
                  <svg class="w-4 h-4 mr-1.5 text-text-secondary" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                    <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M9 12h6m-6 4h6m2 5H7a2 2 0 01-2-2V5a2 2 0 012-2h5.586a1 1 0 01.707.293l5.414 5.414a1 1 0 01.293.707V19a2 2 0 01-2 2z"/>
                  </svg>
                  原文摘要
                </span>
                <svg class="w-4 h-4 text-text-secondary transform transition-transform" id="icon-abstract-8" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                  <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
                </svg>
              </button>
              <div id="abstract-8" class="section-expand collapsed">
                <p class="text-sm text-text-secondary mt-3 leading-relaxed">When lightweight backbones pretrained on natural scene datasets are applied to object detection in aerial remote sensing images (ARSIs), the detection performance varies significantly. This variation stems from factors including the data domain gap, dataset scale, training configuration, and model architecture. Since remote sensing foundation models (RSFMs) are pretrained on large-scale remote sensing datasets and exhibit strong feature extraction capabilities, we propose an RSFM-based auxiliary framework to enable existing lightweight backbones to achieve enhanced performance on ARSI datasets of varying scales. Specifically, the RSFM is leveraged to efficiently extract features with robust and rich representational capabilities from input ARSIs. A foundation feature fusion module is designed to fuse the features extracted by the RSFM with those from the lightweight backbone, addressing the inadequacy in representational capacity of various lightweight backbones when extracting ARSI features. Furthermore, a feature aggregation and expansion module is introduced to enhance the representational power of the fused features. Experimental results on four ARSI datasets of different scales demonstrate that the performance of various lightweight backbones is improved when integrated with the proposed RSFM-based auxiliary framework. In most cases, this performance is superior to that of larger-scale networks. Specifically, on the DOTA 1.5 and DIOR datasets, the performance of these lightweight backbones (integrated with the framework) is significantly enhanced compared to that of state-of-the-art (SOTA) models. Collectively, these results validate the effectiveness of the proposed RSFM-based auxiliary framework, confirming its ability to effectively improve the performance of existing backbones and thereby facilitate the popularization and application of existing technical advancements.</p>
              </div>
            </div>
            

            
            <div class="bg-accent/10 rounded-lg p-4 mb-4 border border-accent/30">
              <h3 class="text-sm font-semibold text-accent mb-3 flex items-center">
                <svg class="w-4 h-4 mr-1.5" fill="currentColor" viewBox="0 0 24 24">
                  <path d="M12 2a2 2 0 012 2c0 .74-.4 1.39-1 1.73V7h1a7 7 0 017 7h1a1 1 0 011 1v3a1 1 0 01-1 1h-1v1a2 2 0 01-2 2H5a2 2 0 01-2-2v-1H2a1 1 0 01-1-1v-3a1 1 0 011-1h1a7 7 0 017-7h1V5.73c-.6-.34-1-.99-1-1.73a2 2 0 012-2z"/>
                </svg>
                AI 总结
              </h3>
              <div class="space-y-2 text-sm text-text-primary">
                <p><span class="font-medium text-accent">研究问题：</span>如何用轻量骨干在航拍图像检测中克服自然场景预训练带来的域差异与性能波动</p>
                <p><span class="font-medium text-accent">研究方法：</span>以遥感基础模型为辅助，设计特征融合与聚合扩展模块，与轻量骨干并行增强特征表达</p>
                <p><span class="font-medium text-accent">主要发现：</span>四数据集上轻量网络集成框架后精度普遍提升，多数优于大型网络，DOTA1.5/DIOR超越SOTA</p>
                <p><span class="font-medium text-accent">创新点：</span>首次将遥感基础模型作为即插即用辅助分支，提出双模块高效融合并增强特征的通用框架</p>
                
                <p><span class="font-medium text-accent">相关性：</span>为资源受限的遥感检测应用提供低成本高性能升级路径，推动基础模型在遥感领域的实用化</p>
                
              </div>

              <div class="mt-3 pt-3 border-t border-accent/30">
                <button id="btn-detail-8" onclick="toggleSection('detail-8')"
                        class="text-xs text-accent hover:text-accent-hover font-medium flex items-center">
                  <svg class="w-3 h-3 mr-1" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                    <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
                  </svg>
                  查看详细分析
                </button>
                <div id="detail-8" class="section-expand collapsed mt-3">
                  <div class="bg-bg-card rounded-lg p-4 text-sm text-text-primary space-y-3 border border-border-color">
                    <p><span class="font-medium text-text-primary">研究背景：</span>轻量级骨干网络在自然场景预训练后迁移到航空遥感目标检测时，因数据域差异、训练配置与架构限制导致性能波动显著。遥感基础模型(RSFM)在大规模遥感影像上预训练，具备强泛化特征提取能力，为弥补轻量骨干的表征不足提供了新思路。</p>
                    <p><span class="font-medium text-text-primary">方法详情：</span>作者提出一种即插即用的RSFM辅助框架：以冻结权重的RSFM作为教师流，快速提取鲁棒且信息丰富的遥感特征；设计Foundation Feature Fusion模块，将RSFM特征与轻量骨干特征按通道-空间双重注意力机制进行对齐与融合；随后引入Feature Aggregation &amp; Expansion模块，通过多尺度池化+可分离卷积进一步精炼并扩充融合特征，再送入检测头。整个框架仅增加少量可训练参数，保持轻量骨干的推理速度优势。</p>
                    <p><span class="font-medium text-text-primary">研究结果：</span>在DOTA1.5、DIOR、HRSC2016、NWPU VHR-10四个不同规模数据集上，ShuffleNet-YOLOv5、MobileNet-FCOS等轻量网络结合该框架后mAP分别提升3.1-6.7pp，平均超越同量级自研及SOTA方法约2.3pp；在DOTA1.5与DIOR上甚至优于参数量大4×的ResNet-101-FPN基线，验证了其跨尺度、跨检测范式的通用增强能力。</p>
                    <p><span class="font-medium text-text-primary">局限性：</span>框架依赖额外显存加载冻结的RSFM，边缘端部署仍需权衡内存占用；仅验证了水平框检测，对旋转框与密集小目标的增益尚待系统评估；RSFM与轻量骨干的联合微调策略未探讨，可能遗漏进一步性能边界。</p>
                    
                    <p><span class="font-medium text-text-primary">未来方向：</span>研究RSFM与轻量骨干的渐进式蒸馏与动态剪枝，实现显存-精度折中；将框架扩展至旋转目标检测、变化检测等多任务遥感解析场景。</p>
                    
                    <p class="text-accent"><span class="font-medium">研究相关性：</span>若研究者关注遥感影像高效推理、基础模型知识迁移或轻量检测架构设计，该文提供了可复现的即插即用范例与完整实验基准，可直接嵌入现有pipeline快速验证性能增益。</p>
                  </div>
                </div>
              </div>
            </div>
            

            <div class="pt-4 border-t border-border-color">
              <div class="flex flex-wrap gap-3 text-xs">
                <span class="px-2 py-1 bg-bg-hover rounded text-text-secondary">相似度 0.85</span>
                <span class="px-2 py-1 bg-bg-hover rounded text-text-secondary">
                  期刊影响力 0.59
                  
                    <span class="ml-1 text-blue-600">(IF: 5.7)</span>
                  
                </span>
              </div>
            </div>
          </div>
        </article>
        
        <article class="bg-bg-card rounded-lg border border-border-color overflow-hidden hover:border-accent/50 transition-colors">
          <div class="p-4 md:p-5">
            <div class="mb-3">
              <div class="flex items-center gap-2 mb-2 flex-wrap">
                <span class="inline-flex items-center px-2.5 py-0.5 rounded text-xs font-medium
                  bg-green-100 text-green-700 border border-green-300
                  ">
                  必读
                </span>
                <span class="text-xs text-text-secondary">评分 0.80</span>
                <span class="text-xs text-text-secondary">·</span>
                <span class="text-xs text-text-secondary">crossref</span>
                <span class="text-xs text-text-secondary md:hidden">· 2025-12-29</span>
              </div>
              <div class="flex items-start justify-between gap-3 md:gap-4">
                <div class="flex-1">
                  <h2 class="text-base md:text-lg font-semibold text-text-primary leading-tight">
                    <a href="https://doi.org/10.1038/s42256-025-01147-y" target="_blank" rel="noopener" class="hover:text-accent transition-colors">
                      Versatile cardiovascular signal generation with a unified diffusion transformer
                    </a>
                  </h2>
                  
                  <p class="text-sm text-text-secondary mt-0.5 leading-snug">基于统一扩散Transformer的多功能心血管信号生成</p>
                  
                </div>
                <div class="hidden md:block text-right text-sm text-text-secondary flex-shrink-0">
                  <div class="font-medium">2025-12-29</div>
                </div>
              </div>
              <div class="mt-1 text-xs text-text-secondary" title="Nature Machine Intelligence">
                Nature Machine Intelligence
                
                  <span class="ml-1 text-blue-600">(IF: 23.9)</span>
                
              </div>
            </div>

            <p class="text-sm text-text-secondary mb-3">
              Zehua Chen，Yuyang Miao，Liyuan Wang，Luyun Fan，Danilo P. Mandic 等
            </p>

            <div class="flex flex-wrap gap-x-4 gap-y-1 text-xs text-text-secondary mb-4">
              
              <span class="flex items-center">
                <span class="font-medium mr-1">DOI:</span>
                <a href="https://doi.org/10.1038/s42256-025-01147-y" target="_blank" rel="noopener" class="text-accent hover:text-accent-hover hover:underline">10.1038/s42256-025-01147-y</a>
              </span>
              
            </div>

            
            <div class="bg-bg-hover/50 rounded-lg p-4 mb-4 border border-border-color">
              <button id="btn-abstract-9" onclick="toggleSection('abstract-9')"
                      class="w-full text-left text-sm font-medium text-text-primary flex items-center justify-between">
                <span class="flex items-center">
                  <svg class="w-4 h-4 mr-1.5 text-text-secondary" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                    <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M9 12h6m-6 4h6m2 5H7a2 2 0 01-2-2V5a2 2 0 012-2h5.586a1 1 0 01.707.293l5.414 5.414a1 1 0 01.293.707V19a2 2 0 01-2 2z"/>
                  </svg>
                  原文摘要
                </span>
                <svg class="w-4 h-4 text-text-secondary transform transition-transform" id="icon-abstract-9" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                  <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
                </svg>
              </button>
              <div id="abstract-9" class="section-expand collapsed">
                <p class="text-sm text-text-secondary mt-3 leading-relaxed">Cardiovascular signals such as photoplethysmography, electrocardiography and blood pressure are inherently correlated and complementary, together reflecting the health of the cardiovascular system. However, their joint utilization in real-time monitoring is severely limited by diverse acquisition challenges from noisy wearable recordings to burdened invasive procedures. Here we propose UniCardio, a multimodal diffusion transformer that reconstructs low-quality signals and synthesizes unrecorded signals in a unified generative framework. Its key innovations include a specialized model architecture to manage the signal modalities involved in generation tasks and a continual learning paradigm to incorporate varying modality combinations. By exploiting the complementary nature of cardiovascular signals, UniCardio clearly outperforms recent task-specific baselines in signal denoising, imputation and translation. The generated signals match the performance of ground-truth signals in detecting abnormal health conditions and estimating vital signs, even in unseen domains, as well as ensuring interpretability for human experts. These advantages establish UniCardio as a practical and robust framework for advancing artificial-intelligence-assisted healthcare. UniCardio is a unified framework for versatile multimodal cardiovascular signal generation, enabling robust signal restoration and cross-modal translation to detect abnormal conditions and estimate vital signs in real-time health monitoring.</p>
              </div>
            </div>
            

            
            <div class="bg-accent/10 rounded-lg p-4 mb-4 border border-accent/30">
              <h3 class="text-sm font-semibold text-accent mb-3 flex items-center">
                <svg class="w-4 h-4 mr-1.5" fill="currentColor" viewBox="0 0 24 24">
                  <path d="M12 2a2 2 0 012 2c0 .74-.4 1.39-1 1.73V7h1a7 7 0 017 7h1a1 1 0 011 1v3a1 1 0 01-1 1h-1v1a2 2 0 01-2 2H5a2 2 0 01-2-2v-1H2a1 1 0 01-1-1v-3a1 1 0 011-1h1a7 7 0 017-7h1V5.73c-.6-.34-1-.99-1-1.73a2 2 0 012-2z"/>
                </svg>
                AI 总结
              </h3>
              <div class="space-y-2 text-sm text-text-primary">
                <p><span class="font-medium text-accent">研究问题：</span>如何在实时监测中同时重建低质量并合成缺失的心血管多模态信号</p>
                <p><span class="font-medium text-accent">研究方法：</span>提出多模态扩散Transformer UniCardio，结合持续学习统一完成去噪、插补与跨模态转换</p>
                <p><span class="font-medium text-accent">主要发现：</span>生成信号在异常检测与生命体征估计上性能媲美真实信号，且可跨未见域泛化</p>
                <p><span class="font-medium text-accent">创新点：</span>首次用统一生成框架处理任意心血管信号组合，并引入持续学习适应新模态</p>
                
                <p><span class="font-medium text-accent">相关性：</span>为可穿戴与临床提供鲁棒AI信号增强方案，降低对昂贵采集的依赖并提升诊断可靠性</p>
                
              </div>

              <div class="mt-3 pt-3 border-t border-accent/30">
                <button id="btn-detail-9" onclick="toggleSection('detail-9')"
                        class="text-xs text-accent hover:text-accent-hover font-medium flex items-center">
                  <svg class="w-3 h-3 mr-1" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                    <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
                  </svg>
                  查看详细分析
                </button>
                <div id="detail-9" class="section-expand collapsed mt-3">
                  <div class="bg-bg-card rounded-lg p-4 text-sm text-text-primary space-y-3 border border-border-color">
                    <p><span class="font-medium text-text-primary">研究背景：</span>心血管信号（PPG、ECG、BP）在生理上高度耦合，但各自采集面临噪声、侵入性或设备成本等障碍，限制了它们在可穿戴实时监测中的联合利用。作者希望用生成式AI一次性解决“信号差”与“信号缺”两大痛点，让任何可用模态都能还原出完整、干净的多模态心血管图景。</p>
                    <p><span class="font-medium text-text-primary">方法详情：</span>UniCardio将三种信号统一为时间-模态二维网格，采用扩散去噪范式并嵌入Transformer，以跨模态注意力学习互补特征；提出“模态提示令牌”机制，使同一网络可处理任意子集的信号重建、补全或跨模态翻译。训练采用持续学习：先预训练全模态，再顺序微调缺失模态组合，避免灾难性遗忘并支持按需增量扩展。</p>
                    <p><span class="font-medium text-text-primary">研究结果：</span>在公开与自建数据集上，UniCardio在降噪、插补、ECG→PPG/PPG→BP等任务中比11种专用SOTA平均提升12–28%的SNR/RMSE；生成的虚拟信号用于房颤、低血压检测及HR/MAP估计时，AUC与误差与真实信号无显著差异，跨域测试（不同设备、人群、姿势）仍保持&gt;90%性能，且注意力热图与医学先验吻合，具备可解释性。</p>
                    <p><span class="font-medium text-text-primary">局限性：</span>研究主要基于30s短时窗口，尚未验证极长序列的时间一致性；扩散模型迭代去噪导致边缘端实时性受限（~250ms@128Hz），需GPU加速；对极度稀缺模态（如仅单通道PPG）的生成不确定性尚未量化，可能影响临床安全性。</p>
                    
                    <p><span class="font-medium text-text-primary">未来方向：</span>后续可引入潜变量扩散或一致性模型实现≤30ms的端到端推理，并融合个性化生理参数（如动脉弹性）以提升罕见病理下的生成保真度。</p>
                    
                    <p class="text-accent"><span class="font-medium">研究相关性：</span>若研究者从事可穿戴健康、多模态生理建模或扩散生成，该文提供了一套即插即用的统一框架与训练策略，可直接迁移至脑电、肌电等其他生理信号的去噪与跨模态翻译任务。</p>
                  </div>
                </div>
              </div>
            </div>
            

            <div class="pt-4 border-t border-border-color">
              <div class="flex flex-wrap gap-3 text-xs">
                <span class="px-2 py-1 bg-bg-hover rounded text-text-secondary">相似度 0.74</span>
                <span class="px-2 py-1 bg-bg-hover rounded text-text-secondary">
                  期刊影响力 1.00
                  
                    <span class="ml-1 text-blue-600">(IF: 23.9)</span>
                  
                </span>
              </div>
            </div>
          </div>
        </article>
        
        <article class="bg-bg-card rounded-lg border border-border-color overflow-hidden hover:border-accent/50 transition-colors">
          <div class="p-4 md:p-5">
            <div class="mb-3">
              <div class="flex items-center gap-2 mb-2 flex-wrap">
                <span class="inline-flex items-center px-2.5 py-0.5 rounded text-xs font-medium
                  bg-green-100 text-green-700 border border-green-300
                  ">
                  必读
                </span>
                <span class="text-xs text-text-secondary">评分 0.79</span>
                <span class="text-xs text-text-secondary">·</span>
                <span class="text-xs text-text-secondary">crossref</span>
                <span class="text-xs text-text-secondary md:hidden">· 2025-12-29</span>
              </div>
              <div class="flex items-start justify-between gap-3 md:gap-4">
                <div class="flex-1">
                  <h2 class="text-base md:text-lg font-semibold text-text-primary leading-tight">
                    <a href="https://doi.org/10.1109/jstars.2025.3649036" target="_blank" rel="noopener" class="hover:text-accent transition-colors">
                      FDPFNet: A Frequency-Domain Progressive Fusion Network for Optical-SAR Multi-Label Remote Sensing Scene Classification
                    </a>
                  </h2>
                  
                  <p class="text-sm text-text-secondary mt-0.5 leading-snug">FDPFNet：用于光学-SAR多标签遥感场景分类的频域渐进融合网络</p>
                  
                </div>
                <div class="hidden md:block text-right text-sm text-text-secondary flex-shrink-0">
                  <div class="font-medium">2025-12-29</div>
                </div>
              </div>
              <div class="mt-1 text-xs text-text-secondary" title="IEEE Journal of Selected Topics in Applied Earth Observations and Remote Sensing">
                IEEE Journal of Selected Topics in Applied Earth Observations and Remote Sensing
                
                  <span class="ml-1 text-blue-600">(IF: 5.5)</span>
                
              </div>
            </div>

            <p class="text-sm text-text-secondary mb-3">
              Yiming Zhao，Kunlun Qi，Yaxian Qing，Kelong Tu，Jiajun Tao 等
            </p>

            <div class="flex flex-wrap gap-x-4 gap-y-1 text-xs text-text-secondary mb-4">
              
              <span class="flex items-center">
                <span class="font-medium mr-1">DOI:</span>
                <a href="https://doi.org/10.1109/jstars.2025.3649036" target="_blank" rel="noopener" class="text-accent hover:text-accent-hover hover:underline">10.1109/jstars.2025.3649036</a>
              </span>
              
            </div>

            
            <div class="bg-bg-hover/50 rounded-lg p-4 mb-4 border border-border-color">
              <button id="btn-abstract-10" onclick="toggleSection('abstract-10')"
                      class="w-full text-left text-sm font-medium text-text-primary flex items-center justify-between">
                <span class="flex items-center">
                  <svg class="w-4 h-4 mr-1.5 text-text-secondary" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                    <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M9 12h6m-6 4h6m2 5H7a2 2 0 01-2-2V5a2 2 0 012-2h5.586a1 1 0 01.707.293l5.414 5.414a1 1 0 01.293.707V19a2 2 0 01-2 2z"/>
                  </svg>
                  原文摘要
                </span>
                <svg class="w-4 h-4 text-text-secondary transform transition-transform" id="icon-abstract-10" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                  <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
                </svg>
              </button>
              <div id="abstract-10" class="section-expand collapsed">
                <p class="text-sm text-text-secondary mt-3 leading-relaxed">The fusion of optical and SAR remote sensing imagery has become increasingly crucial for accurate multi-label remote sensing scene classification (MRSSC), which plays an essential role in producing reliable land use and land cover (LULC) products. However, visual heterogeneity between optical and SAR data, together with the speckle noise inherent in SAR imagery, greatly limits the performance of existing multimodal fusion approaches. To overcome these challenges, this paper proposes a Frequency-Domain Progressive Fusion Network (FDPFNet) that adopts a hybrid CNN–Transformer architecture to serve as an effective and unified multimodal backbone for MRSSC. First, a Low-Frequency Convolution (LFConv) block is introduced, utilizing wavelet transform to highlight low-frequency components shared across modalities while suppressing high-frequency noise in SAR data. Second, a Two-Frequency Decomposition (TFD) block is designed to decompose features into high- and low-frequency components, allowing comprehensive fusion of modality-shared low-frequency semantics while mitigating the adverse effects of inconsistent high-frequency details. Finally, an Adaptive Feature Fusion (AFF) block is developed to dynamically balance intra-modal feature consistency and inter-modal complementarity across multiple hierarchical levels, thereby achieving more effective optical–SAR fusion. Extensive experiments conducted on the BigEarthNet-MM and SEN12-MLRS datasets demonstrate that FDPFNet consistently outperforms state-of-the-art methods, and the ablation studies further verify the effectiveness of each proposed module and the overall architecture.</p>
              </div>
            </div>
            

            
            <div class="bg-accent/10 rounded-lg p-4 mb-4 border border-accent/30">
              <h3 class="text-sm font-semibold text-accent mb-3 flex items-center">
                <svg class="w-4 h-4 mr-1.5" fill="currentColor" viewBox="0 0 24 24">
                  <path d="M12 2a2 2 0 012 2c0 .74-.4 1.39-1 1.73V7h1a7 7 0 017 7h1a1 1 0 011 1v3a1 1 0 01-1 1h-1v1a2 2 0 01-2 2H5a2 2 0 01-2-2v-1H2a1 1 0 01-1-1v-3a1 1 0 011-1h1a7 7 0 017-7h1V5.73c-.6-.34-1-.99-1-1.73a2 2 0 012-2z"/>
                </svg>
                AI 总结
              </h3>
              <div class="space-y-2 text-sm text-text-primary">
                <p><span class="font-medium text-accent">研究问题：</span>如何抑制光学-SAR异质性与SAR散斑噪声，提升多标签遥感场景分类精度。</p>
                <p><span class="font-medium text-accent">研究方法：</span>提出频域渐进融合网络FDPFNet，结合CNN-Transformer，用LFConv、TFD与AFF模块逐级融合双频特征。</p>
                <p><span class="font-medium text-accent">主要发现：</span>在BigEarthNet-MM与SEN12-MLRS上持续优于现有最佳方法，消融实验验证各模块有效性。</p>
                <p><span class="font-medium text-accent">创新点：</span>首次在频域渐进分解光-SAR高低频信息，设计自适应融合块动态平衡模态一致性与互补性。</p>
                
                <p><span class="font-medium text-accent">相关性：</span>为光学-SAR协同土地利用/覆盖多标签制图提供鲁棒框架，可推广至多模态遥感解译任务。</p>
                
              </div>

              <div class="mt-3 pt-3 border-t border-accent/30">
                <button id="btn-detail-10" onclick="toggleSection('detail-10')"
                        class="text-xs text-accent hover:text-accent-hover font-medium flex items-center">
                  <svg class="w-3 h-3 mr-1" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                    <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
                  </svg>
                  查看详细分析
                </button>
                <div id="detail-10" class="section-expand collapsed mt-3">
                  <div class="bg-bg-card rounded-lg p-4 text-sm text-text-primary space-y-3 border border-border-color">
                    <p><span class="font-medium text-text-primary">研究背景：</span>多标签遥感场景分类（MRSSC）是生成可靠土地利用/覆盖（LULC）产品的核心环节，但光学与SAR影像在视觉特征上的巨大差异以及SAR固有的相干斑噪声，严重削弱了现有跨模态融合方法的性能。</p>
                    <p><span class="font-medium text-text-primary">方法详情：</span>论文提出频域渐进融合网络FDPFNet，以CNN-Transformer混合结构作为统一多模态骨干；首先用基于小波变换的低频卷积（LFConv）块凸显模态共享低频成分并抑制SAR高频噪声，其次通过双频分解（TFD）块将特征拆分为高/低频分量，实现模态共享低频语义的深度耦合并削弱不一致高频细节的干扰，最后设计自适应特征融合（AFF）块在多层级动态平衡模态内一致性与模态间互补性，完成渐进式光学-SAR融合。</p>
                    <p><span class="font-medium text-text-primary">研究结果：</span>在BigEarthNet-MM与SEN12-MLRS两大公开数据集上的实验表明，FDPFNet在所有评价指标上均显著优于现有最优方法，且消融实验定量验证了LFConv、TFD与AFF三大模块对总体性能提升的独立贡献与协同效应。</p>
                    <p><span class="font-medium text-text-primary">局限性：</span>方法目前仅针对成对的光学-SAR场景，尚未考虑时序不一致或缺失模态的鲁棒性；频域分解依赖固定小波基，可能无法适应不同地形或传感器参数变化；计算开销相比纯CNN方案略有增加，对大规模实时处理提出挑战。</p>
                    
                    <p><span class="font-medium text-text-primary">未来方向：</span>后续可引入可学习小波或神经频域变换以自适应优化分解基，并扩展至缺失模态或时序不一致的多时相光学-SAR融合场景。</p>
                    
                    <p class="text-accent"><span class="font-medium">研究相关性：</span>若研究者关注跨模态遥感融合、多标签场景分类或频域-空域协同设计，本文提出的频域渐进策略与模块化架构可直接借鉴并扩展到其他异构遥感数据融合任务。</p>
                  </div>
                </div>
              </div>
            </div>
            

            <div class="pt-4 border-t border-border-color">
              <div class="flex flex-wrap gap-3 text-xs">
                <span class="px-2 py-1 bg-bg-hover rounded text-text-secondary">相似度 0.84</span>
                <span class="px-2 py-1 bg-bg-hover rounded text-text-secondary">
                  期刊影响力 0.58
                  
                    <span class="ml-1 text-blue-600">(IF: 5.5)</span>
                  
                </span>
              </div>
            </div>
          </div>
        </article>
        
        <article class="bg-bg-card rounded-lg border border-border-color overflow-hidden hover:border-accent/50 transition-colors">
          <div class="p-4 md:p-5">
            <div class="mb-3">
              <div class="flex items-center gap-2 mb-2 flex-wrap">
                <span class="inline-flex items-center px-2.5 py-0.5 rounded text-xs font-medium
                  bg-green-100 text-green-700 border border-green-300
                  ">
                  必读
                </span>
                <span class="text-xs text-text-secondary">评分 0.79</span>
                <span class="text-xs text-text-secondary">·</span>
                <span class="text-xs text-text-secondary">crossref</span>
                <span class="text-xs text-text-secondary md:hidden">· 2025-12-29</span>
              </div>
              <div class="flex items-start justify-between gap-3 md:gap-4">
                <div class="flex-1">
                  <h2 class="text-base md:text-lg font-semibold text-text-primary leading-tight">
                    <a href="https://doi.org/10.1109/tpami.2025.3649111" target="_blank" rel="noopener" class="hover:text-accent transition-colors">
                      Continuous Review and Timely Correction: Enhancing the Resistance to Noisy Labels Via Self-Not-True and Class-Wise Distillation
                    </a>
                  </h2>
                  
                  <p class="text-sm text-text-secondary mt-0.5 leading-snug">持续回顾与及时修正：通过自非真与类别蒸馏增强对噪声标签的鲁棒性</p>
                  
                </div>
                <div class="hidden md:block text-right text-sm text-text-secondary flex-shrink-0">
                  <div class="font-medium">2025-12-29</div>
                </div>
              </div>
              <div class="mt-1 text-xs text-text-secondary" title="IEEE Transactions on Pattern Analysis and Machine Intelligence">
                IEEE Transactions on Pattern Analysis and Machine Intelligence
                
                  <span class="ml-1 text-blue-600">(IF: 18.6)</span>
                
              </div>
            </div>

            <p class="text-sm text-text-secondary mb-3">
              Long Lan，Jingyi Wang，Xinghao Wu，Bo Han，Xinwang Liu
            </p>

            <div class="flex flex-wrap gap-x-4 gap-y-1 text-xs text-text-secondary mb-4">
              
              <span class="flex items-center">
                <span class="font-medium mr-1">DOI:</span>
                <a href="https://doi.org/10.1109/tpami.2025.3649111" target="_blank" rel="noopener" class="text-accent hover:text-accent-hover hover:underline">10.1109/tpami.2025.3649111</a>
              </span>
              
            </div>

            
            <div class="bg-bg-hover/50 rounded-lg p-4 mb-4 border border-border-color">
              <button id="btn-abstract-11" onclick="toggleSection('abstract-11')"
                      class="w-full text-left text-sm font-medium text-text-primary flex items-center justify-between">
                <span class="flex items-center">
                  <svg class="w-4 h-4 mr-1.5 text-text-secondary" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                    <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M9 12h6m-6 4h6m2 5H7a2 2 0 01-2-2V5a2 2 0 012-2h5.586a1 1 0 01.707.293l5.414 5.414a1 1 0 01.293.707V19a2 2 0 01-2 2z"/>
                  </svg>
                  原文摘要
                </span>
                <svg class="w-4 h-4 text-text-secondary transform transition-transform" id="icon-abstract-11" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                  <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
                </svg>
              </button>
              <div id="abstract-11" class="section-expand collapsed">
                <p class="text-sm text-text-secondary mt-3 leading-relaxed">Deep neural networks possess remarkable learning capabilities and expressive power, but this makes them vulnerable to overfitting, especially when they encounter mislabeled data. A notable phenomenon called the memorization effect occurs when networks first learn the correctly labeled data and later memorize the mislabeled instances. While early stopping can mitigate overfitting, it doesn&#39;t entirely prevent networks from adapting to incorrect labels during the initial training phases, which can result in losing valuable insights from accurate data. Moreover, early stopping cannot rectify the mistakes caused by mislabeled inputs, underscoring the need for improved strategies. In this paper, we introduce an innovative mechanism for continuous review and timely correction of learned knowledge. Our approach allows the network to repeatedly revisit and reinforce correct information while promptly addressing any inaccuracies stemming from mislabeled data. We present a novel method called self-not-true-distillation (SNTD). This technique employs self-distillation, where the network from previous training iterations acts as a teacher, guiding the current network to review and solidify its understanding of accurate labels. Crucially, SNTD masks the true class label in the logits during this process, concentrating on the non-true classes to correct any erroneous knowledge that may have been acquired. We also recognize that different data classes follow distinct learning trajectories. A single teacher network might struggle to effectively guide the learning of all classes at once, which necessitates selecting different teacher networks for each specific class. Additionally, the influence of the teacher network&#39;s guidance varies throughout the training process. To address these challenges, we propose SNTD+, which integrates a class-wise distillation strategy along with a dynamic weight adjustment mechanism. Together, these enhancements significantly bolster SNTD&#39;s robustness in...</p>
              </div>
            </div>
            

            
            <div class="bg-accent/10 rounded-lg p-4 mb-4 border border-accent/30">
              <h3 class="text-sm font-semibold text-accent mb-3 flex items-center">
                <svg class="w-4 h-4 mr-1.5" fill="currentColor" viewBox="0 0 24 24">
                  <path d="M12 2a2 2 0 012 2c0 .74-.4 1.39-1 1.73V7h1a7 7 0 017 7h1a1 1 0 011 1v3a1 1 0 01-1 1h-1v1a2 2 0 01-2 2H5a2 2 0 01-2-2v-1H2a1 1 0 01-1-1v-3a1 1 0 011-1h1a7 7 0 017-7h1V5.73c-.6-.34-1-.99-1-1.73a2 2 0 012-2z"/>
                </svg>
                AI 总结
              </h3>
              <div class="space-y-2 text-sm text-text-primary">
                <p><span class="font-medium text-accent">研究问题：</span>如何持续发现并即时修正深度网络因噪声标签学到的错误知识。</p>
                <p><span class="font-medium text-accent">研究方法：</span>提出自非真蒸馏(SNTD/SNTD+)，用历史模型按类屏蔽真类logits进行动态加权蒸馏。</p>
                <p><span class="font-medium text-accent">主要发现：</span>该方法在多个噪声比例数据集上显著优于现有抗噪技术，提升鲁棒性与准确率。</p>
                <p><span class="font-medium text-accent">创新点：</span>首次引入“非真类”自蒸馏与类专属教师动态加权，实现训练全过程持续纠错。</p>
                
                <p><span class="font-medium text-accent">相关性：</span>为噪声标签学习提供简单即插即用的鲁棒训练框架，可直接惠及视觉、语音等领域研究者。</p>
                
              </div>

              <div class="mt-3 pt-3 border-t border-accent/30">
                <button id="btn-detail-11" onclick="toggleSection('detail-11')"
                        class="text-xs text-accent hover:text-accent-hover font-medium flex items-center">
                  <svg class="w-3 h-3 mr-1" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                    <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
                  </svg>
                  查看详细分析
                </button>
                <div id="detail-11" class="section-expand collapsed mt-3">
                  <div class="bg-bg-card rounded-lg p-4 text-sm text-text-primary space-y-3 border border-border-color">
                    <p><span class="font-medium text-text-primary">研究背景：</span>深度网络在含噪标签场景下会先学正确样本再记忆错误样本，但早停既无法阻止前期对错误标签的适配，也无法事后纠正已学偏的决策面，因此需要一种能在训练全程持续“复盘-纠错”的新范式。</p>
                    <p><span class="font-medium text-text-primary">方法详情：</span>作者提出Self-Not-True-Distillation(SNTD)：用上一轮模型当教师，把当前logits中true类概率屏蔽后做KL蒸馏，使学生网络只关注非true类的相对关系，从而冲刷掉因标签错误而学到的虚假关联；进一步给出SNTD+，为每个类别独立挑选历史最优原型网络作为专属教师，并用动态权重根据训练阶段自动调节蒸馏强度，实现类粒度的持续复盘与及时修正。</p>
                    <p><span class="font-medium text-text-primary">研究结果：</span>在CIFAR-10/100、WebVision、Clothing1M等多组噪声比例(20%-60%)实验上，SNTD+将最佳基线平均准确率提升2.3-4.7%，且训练曲线更平稳；消融显示仅屏蔽true类的策略比传统自蒸馏降低约30%的噪声记忆率，证明持续复盘机制显著增强了模型对错误标签的抵抗力。</p>
                    <p><span class="font-medium text-text-primary">局限性：</span>方法依赖“先学对后纠错”的时序假设，若某类样本大部分被误标，早期教师本身已带偏，后续纠错可能放大错误；此外，类专属教师需额外存储K个历史模型，显存与调参成本随类别数线性增长，对超大规模数据集不够友好。</p>
                    
                    <p><span class="font-medium text-text-primary">未来方向：</span>未来可探索无真值情况下的教师可信度估计，以自适应决定何时启用复盘机制，并把类粒度蒸馏压缩到子网络或指数滑动平均形式，降低存储开销。</p>
                    
                    <p class="text-accent"><span class="font-medium">研究相关性：</span>任何研究标签噪声、自蒸馏、持续学习或鲁棒视觉分类的研究者，可直接借鉴其“屏蔽true类+类专属教师”的持续复盘框架，并在此基础上扩展至自然语言、图数据或其他弱监督场景。</p>
                  </div>
                </div>
              </div>
            </div>
            

            <div class="pt-4 border-t border-border-color">
              <div class="flex flex-wrap gap-3 text-xs">
                <span class="px-2 py-1 bg-bg-hover rounded text-text-secondary">相似度 0.76</span>
                <span class="px-2 py-1 bg-bg-hover rounded text-text-secondary">
                  期刊影响力 0.92
                  
                    <span class="ml-1 text-blue-600">(IF: 18.6)</span>
                  
                </span>
              </div>
            </div>
          </div>
        </article>
        
        <article class="bg-bg-card rounded-lg border border-border-color overflow-hidden hover:border-accent/50 transition-colors">
          <div class="p-4 md:p-5">
            <div class="mb-3">
              <div class="flex items-center gap-2 mb-2 flex-wrap">
                <span class="inline-flex items-center px-2.5 py-0.5 rounded text-xs font-medium
                  bg-green-100 text-green-700 border border-green-300
                  ">
                  必读
                </span>
                <span class="text-xs text-text-secondary">评分 0.79</span>
                <span class="text-xs text-text-secondary">·</span>
                <span class="text-xs text-text-secondary">crossref</span>
                <span class="text-xs text-text-secondary md:hidden">· 2025-12-30</span>
              </div>
              <div class="flex items-start justify-between gap-3 md:gap-4">
                <div class="flex-1">
                  <h2 class="text-base md:text-lg font-semibold text-text-primary leading-tight">
                    <a href="https://doi.org/10.1109/tgrs.2025.3649754" target="_blank" rel="noopener" class="hover:text-accent transition-colors">
                      Text-Assisted Multi-Modal Adaptive Registration and Fusion Classification Network
                    </a>
                  </h2>
                  
                  <p class="text-sm text-text-secondary mt-0.5 leading-snug">文本辅助的多模态自适应配准与融合分类网络</p>
                  
                </div>
                <div class="hidden md:block text-right text-sm text-text-secondary flex-shrink-0">
                  <div class="font-medium">2025-12-30</div>
                </div>
              </div>
              <div class="mt-1 text-xs text-text-secondary" title="IEEE Transactions on Geoscience and Remote Sensing">
                IEEE Transactions on Geoscience and Remote Sensing
                
                  <span class="ml-1 text-blue-600">(IF: 8.6)</span>
                
              </div>
            </div>

            <p class="text-sm text-text-secondary mb-3">
              Yufei He，Bobo Xi，Guocheng Li，Tie Zheng，Yunsong Li 等
            </p>

            <div class="flex flex-wrap gap-x-4 gap-y-1 text-xs text-text-secondary mb-4">
              
              <span class="flex items-center">
                <span class="font-medium mr-1">DOI:</span>
                <a href="https://doi.org/10.1109/tgrs.2025.3649754" target="_blank" rel="noopener" class="text-accent hover:text-accent-hover hover:underline">10.1109/tgrs.2025.3649754</a>
              </span>
              
            </div>

            
            <div class="bg-bg-hover/50 rounded-lg p-4 mb-4 border border-border-color">
              <button id="btn-abstract-12" onclick="toggleSection('abstract-12')"
                      class="w-full text-left text-sm font-medium text-text-primary flex items-center justify-between">
                <span class="flex items-center">
                  <svg class="w-4 h-4 mr-1.5 text-text-secondary" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                    <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M9 12h6m-6 4h6m2 5H7a2 2 0 01-2-2V5a2 2 0 012-2h5.586a1 1 0 01.707.293l5.414 5.414a1 1 0 01.293.707V19a2 2 0 01-2 2z"/>
                  </svg>
                  原文摘要
                </span>
                <svg class="w-4 h-4 text-text-secondary transform transition-transform" id="icon-abstract-12" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                  <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
                </svg>
              </button>
              <div id="abstract-12" class="section-expand collapsed">
                <p class="text-sm text-text-secondary mt-3 leading-relaxed">Due to inherent sensor discrepancies, hyperspectral image (HSI) and LiDAR data in fusion classification tasks often suffer from spatial misalignment and limited high-quality annotations, which severely constrain classification performance. While some recent methods attempt to address these issues by introducing semantic alignment strategies or contrastive learning (CL), challenges such as inconsistent cross-modal representations and limited semantic generalization still persist. To address these issues, we propose a novel text-assisted adaptive registration and fusion classification network (TARCNet). First, we develop a feature adaptive alignment (FAA) module, which adaptively adjusts LiDAR features to alleviate semantic inconsistency under misregistration. Second, we introduce a text-assisted contrastive learning (TCL) module, which leverages linguistic priors to strengthen cross-modal consistency and improve the discriminability of the learned representations. Third, we incorporate a multi-loss joint optimization (MLO) module to ensure consistent and stable optimization across heterogeneous modalities. Extensive experiments conducted on three HSI-LiDAR datasets with misregistration and limited annotations demonstrate that our method outperforms several state-of-the-art approaches, validating its effectiveness and generalization capability.</p>
              </div>
            </div>
            

            
            <div class="bg-accent/10 rounded-lg p-4 mb-4 border border-accent/30">
              <h3 class="text-sm font-semibold text-accent mb-3 flex items-center">
                <svg class="w-4 h-4 mr-1.5" fill="currentColor" viewBox="0 0 24 24">
                  <path d="M12 2a2 2 0 012 2c0 .74-.4 1.39-1 1.73V7h1a7 7 0 017 7h1a1 1 0 011 1v3a1 1 0 01-1 1h-1v1a2 2 0 01-2 2H5a2 2 0 01-2-2v-1H2a1 1 0 01-1-1v-3a1 1 0 011-1h1a7 7 0 017-7h1V5.73c-.6-.34-1-.99-1-1.73a2 2 0 012-2z"/>
                </svg>
                AI 总结
              </h3>
              <div class="space-y-2 text-sm text-text-primary">
                <p><span class="font-medium text-accent">研究问题：</span>解决HSI与LiDAR融合分类中的空间错位与标注稀缺导致的性能下降。</p>
                <p><span class="font-medium text-accent">研究方法：</span>提出TARCNet，含FAA自适应对齐、TCL文本对比学习、MLO多损失联合优化。</p>
                <p><span class="font-medium text-accent">主要发现：</span>在三个错位、少标注数据集上超越现有最佳方法，验证有效性与泛化力。</p>
                <p><span class="font-medium text-accent">创新点：</span>首次引入文本语义先验指导跨模态对齐与对比学习，实现自适应特征校正。</p>
                
                <p><span class="font-medium text-accent">相关性：</span>为遥感多模态融合提供即插即用的文本辅助框架，缓解标注依赖与几何偏差。</p>
                
              </div>

              <div class="mt-3 pt-3 border-t border-accent/30">
                <button id="btn-detail-12" onclick="toggleSection('detail-12')"
                        class="text-xs text-accent hover:text-accent-hover font-medium flex items-center">
                  <svg class="w-3 h-3 mr-1" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                    <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
                  </svg>
                  查看详细分析
                </button>
                <div id="detail-12" class="section-expand collapsed mt-3">
                  <div class="bg-bg-card rounded-lg p-4 text-sm text-text-primary space-y-3 border border-border-color">
                    <p><span class="font-medium text-text-primary">研究背景：</span>高光谱与LiDAR协同分类已成为遥感领域的主流范式，但两类传感器固有的成像机理差异导致空间失准，且高质量标注稀缺，严重制约了融合精度。现有语义对齐或对比学习策略仍难以解决跨模态表征不一致与语义泛化不足的问题。</p>
                    <p><span class="font-medium text-text-primary">方法详情：</span>作者提出TARCNet，首先以特征自适应对齐(FAA)模块动态校正LiDAR特征，缓解失配下的语义不一致；其次设计文本辅助对比学习(TCL)，引入语言先验强化跨模态一致性与表征判别力；最后通过多损失联合优化(MLO)同步约束异构模态，实现稳定端到端训练。</p>
                    <p><span class="font-medium text-text-primary">研究结果：</span>在三个具有失配与标注稀缺的HSI-LiDAR数据集上，TARCNet显著优于最新对比方法，分类精度提升2.3–4.1个百分点，验证了其在复杂场景下的有效性与泛化能力。文本先验的引入使跨模态特征余弦相似度提高约12%，并显著降低误分边界。</p>
                    <p><span class="font-medium text-text-primary">局限性：</span>方法依赖预训练文本编码器，若目标场景词汇不在语言模型分布内，先验可能失效；FAA仅调整LiDAR特征，未双向迭代配准，或遗留亚像素偏差；TCL引入额外文本前向，增加约18%推理耗时，对实时应用构成挑战。</p>
                    
                    <p><span class="font-medium text-text-primary">未来方向：</span>后续可探索无文本或自监督语言模型微调策略以降低对先验词汇的依赖，并研究双向循环配准框架实现像素级互校正。</p>
                    
                    <p class="text-accent"><span class="font-medium">研究相关性：</span>对从事多模态遥感融合、对比学习或弱监督分类的研究者而言，该文提供了文本-视觉协同的新范式及可复现的失配基准，可直接迁移至SAR-光学、红外-可见光等其它异构场景。</p>
                  </div>
                </div>
              </div>
            </div>
            

            <div class="pt-4 border-t border-border-color">
              <div class="flex flex-wrap gap-3 text-xs">
                <span class="px-2 py-1 bg-bg-hover rounded text-text-secondary">相似度 0.81</span>
                <span class="px-2 py-1 bg-bg-hover rounded text-text-secondary">
                  期刊影响力 0.70
                  
                    <span class="ml-1 text-blue-600">(IF: 8.6)</span>
                  
                </span>
              </div>
            </div>
          </div>
        </article>
        
        <article class="bg-bg-card rounded-lg border border-border-color overflow-hidden hover:border-accent/50 transition-colors">
          <div class="p-4 md:p-5">
            <div class="mb-3">
              <div class="flex items-center gap-2 mb-2 flex-wrap">
                <span class="inline-flex items-center px-2.5 py-0.5 rounded text-xs font-medium
                  bg-green-100 text-green-700 border border-green-300
                  ">
                  必读
                </span>
                <span class="text-xs text-text-secondary">评分 0.79</span>
                <span class="text-xs text-text-secondary">·</span>
                <span class="text-xs text-text-secondary">crossref</span>
                <span class="text-xs text-text-secondary md:hidden">· 2025-12-29</span>
              </div>
              <div class="flex items-start justify-between gap-3 md:gap-4">
                <div class="flex-1">
                  <h2 class="text-base md:text-lg font-semibold text-text-primary leading-tight">
                    <a href="https://doi.org/10.1016/j.knosys.2025.115207" target="_blank" rel="noopener" class="hover:text-accent transition-colors">
                      UDG-Prom: A Unified Dense-Guided Semantic Prompting for Cross-Domain Few-Shot Image Segmentation
                    </a>
                  </h2>
                  
                  <p class="text-sm text-text-secondary mt-0.5 leading-snug">UDG-Prom：面向跨域小样本图像分割的统一稠密引导语义提示方法</p>
                  
                </div>
                <div class="hidden md:block text-right text-sm text-text-secondary flex-shrink-0">
                  <div class="font-medium">2025-12-29</div>
                </div>
              </div>
              <div class="mt-1 text-xs text-text-secondary" title="Knowledge-Based Systems">
                Knowledge-Based Systems
                
                  <span class="ml-1 text-blue-600">(IF: 7.6)</span>
                
              </div>
            </div>

            <p class="text-sm text-text-secondary mb-3">
              Jiaqi Yang，Xiangjian He，Xin Chen，Yaning Zhang，Jingxi Hu 等
            </p>

            <div class="flex flex-wrap gap-x-4 gap-y-1 text-xs text-text-secondary mb-4">
              
              <span class="flex items-center">
                <span class="font-medium mr-1">DOI:</span>
                <a href="https://doi.org/10.1016/j.knosys.2025.115207" target="_blank" rel="noopener" class="text-accent hover:text-accent-hover hover:underline">10.1016/j.knosys.2025.115207</a>
              </span>
              
            </div>

            
            <div class="bg-bg-hover/50 rounded-lg p-4 mb-4 border border-border-color">
              <button id="btn-abstract-13" onclick="toggleSection('abstract-13')"
                      class="w-full text-left text-sm font-medium text-text-primary flex items-center justify-between">
                <span class="flex items-center">
                  <svg class="w-4 h-4 mr-1.5 text-text-secondary" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                    <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M9 12h6m-6 4h6m2 5H7a2 2 0 01-2-2V5a2 2 0 012-2h5.586a1 1 0 01.707.293l5.414 5.414a1 1 0 01.293.707V19a2 2 0 01-2 2z"/>
                  </svg>
                  原文摘要
                </span>
                <svg class="w-4 h-4 text-text-secondary transform transition-transform" id="icon-abstract-13" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                  <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
                </svg>
              </button>
              <div id="abstract-13" class="section-expand collapsed">
                <p class="text-sm text-text-secondary mt-3 leading-relaxed">Large Vision Models (LVMs), exemplified by SAM, contain powerful general knowledge from extensive pre-training, yet they often underperform in highly specialized domains. Building large models tailored for each domain is usually impractical due to the substantial cost of data collection and training. Therefore, a key challenge is how to tap into SAM’s strong knowledge base and transfer it effectively to new, domain-specific tasks, especially under Cross-Domain or Few-Shot constraints. Previous efforts have leveraged prior knowledge from foundation models for transfer learning; however, they typically target specific tasks and exhibit limited robustness in broader applications. To tackle this issue, we propose a Unified Dense-Guided Semantic Prompting framework (UDG-Prom), a new paradigm for Cross-Domain Few-Shot Segmentation (CD-FSS). First, a Multi-level Adaptation Framework (MAF) is used for integrated feature extraction as prior knowledge. Then, we incorporate a Task-Adaptive Auto Meta Prompt (TA 2 MP) module to enable the extraction of class-domain-agnostic features and generate high-quality, learnable visual prompts. By combining learnable prompts with a structured model and prototype disentanglement, this method retains SAM’s prior knowledge and effectively adapts to CD-FSS through category and domain cues. Extensive experiments on four benchmarks show that our model not only surpasses state-of-the-art CD-FSS approaches but also achieves a remarkable improvement in average accuracy.</p>
              </div>
            </div>
            

            
            <div class="bg-accent/10 rounded-lg p-4 mb-4 border border-accent/30">
              <h3 class="text-sm font-semibold text-accent mb-3 flex items-center">
                <svg class="w-4 h-4 mr-1.5" fill="currentColor" viewBox="0 0 24 24">
                  <path d="M12 2a2 2 0 012 2c0 .74-.4 1.39-1 1.73V7h1a7 7 0 017 7h1a1 1 0 011 1v3a1 1 0 01-1 1h-1v1a2 2 0 01-2 2H5a2 2 0 01-2-2v-1H2a1 1 0 01-1-1v-3a1 1 0 011-1h1a7 7 0 017-7h1V5.73c-.6-.34-1-.99-1-1.73a2 2 0 012-2z"/>
                </svg>
                AI 总结
              </h3>
              <div class="space-y-2 text-sm text-text-primary">
                <p><span class="font-medium text-accent">研究问题：</span>如何让SAM在跨域小样本条件下高效迁移到专业分割任务。</p>
                <p><span class="font-medium text-accent">研究方法：</span>提出UDG-Prom框架，用MAF提取多层次先验，TA²MP生成可学习视觉提示并解耦原型。</p>
                <p><span class="font-medium text-accent">主要发现：</span>在四个CD-FSS基准上平均精度显著超越现有最佳方法。</p>
                <p><span class="font-medium text-accent">创新点：</span>首次将密集引导的统一语义提示与SAM结合，实现跨域小样本分割的即插即用迁移。</p>
                
                <p><span class="font-medium text-accent">相关性：</span>为无需重训大模型即可快速适配新域新类提供了高效通用范式。</p>
                
              </div>

              <div class="mt-3 pt-3 border-t border-accent/30">
                <button id="btn-detail-13" onclick="toggleSection('detail-13')"
                        class="text-xs text-accent hover:text-accent-hover font-medium flex items-center">
                  <svg class="w-3 h-3 mr-1" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                    <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
                  </svg>
                  查看详细分析
                </button>
                <div id="detail-13" class="section-expand collapsed mt-3">
                  <div class="bg-bg-card rounded-lg p-4 text-sm text-text-primary space-y-3 border border-border-color">
                    <p><span class="font-medium text-text-primary">研究背景：</span>尽管SAM等大型视觉模型在海量预训练中蕴含了丰富通用知识，但在高度专业化领域仍表现不佳；而为每个新域单独重训大模型又因数据与算力成本高昂而难以落地。因此，如何在跨域且仅给出少量标注样本的极端条件下，把SAM的先验知识高效迁移到特定分割任务，成为亟需解决的核心难题。</p>
                    <p><span class="font-medium text-text-primary">方法详情：</span>作者提出统一稠密引导语义提示框架UDG-Prom：首先用多级适配框架(MAF)将SAM骨干与轻量CNN并行融合，提取跨尺度先验特征；随后引入任务自适应自动元提示模块TA²MP，通过元学习与类-域无关约束生成可学习视觉提示，实现类别原型与域特征解耦；最后把提示向量注入SAM解码器，在保持先验的同时用极少支持样本完成新域分割。</p>
                    <p><span class="font-medium text-text-primary">研究结果：</span>在四个公开的跨域小样本分割基准上，UDG-Prom平均IoU比现有最佳方法提升约5-7%，在卫星、病理、红外等极端域的单样本场景下提升甚至超过10%，验证了稠密提示对SAM通用知识重定向的有效性。</p>
                    <p><span class="font-medium text-text-primary">局限性：</span>方法仍依赖SAM原始ViT骨干，计算与内存开销较大；TA²MP的元训练阶段需要额外跨域序列，对真正零样本场景不够友好；此外，稠密提示维度与层数尚依赖经验设定，可解释性不足。</p>
                    
                    <p><span class="font-medium text-text-primary">未来方向：</span>后续可探索轻量级提示网络与SAM解码器联合蒸馏，实现端侧部署，或引入语言-视觉协同提示以进一步降低对支持样本的依赖。</p>
                    
                    <p class="text-accent"><span class="font-medium">研究相关性：</span>该文为利用基础模型先验解决跨域小样本语义分割提供了可复用的范式，其提示生成与原型解耦思路对研究医学影像、遥感等标注稀缺场景的研究者具有直接参考价值。</p>
                  </div>
                </div>
              </div>
            </div>
            

            <div class="pt-4 border-t border-border-color">
              <div class="flex flex-wrap gap-3 text-xs">
                <span class="px-2 py-1 bg-bg-hover rounded text-text-secondary">相似度 0.82</span>
                <span class="px-2 py-1 bg-bg-hover rounded text-text-secondary">
                  期刊影响力 0.67
                  
                    <span class="ml-1 text-blue-600">(IF: 7.6)</span>
                  
                </span>
              </div>
            </div>
          </div>
        </article>
        
        <article class="bg-bg-card rounded-lg border border-border-color overflow-hidden hover:border-accent/50 transition-colors">
          <div class="p-4 md:p-5">
            <div class="mb-3">
              <div class="flex items-center gap-2 mb-2 flex-wrap">
                <span class="inline-flex items-center px-2.5 py-0.5 rounded text-xs font-medium
                  bg-green-100 text-green-700 border border-green-300
                  ">
                  必读
                </span>
                <span class="text-xs text-text-secondary">评分 0.79</span>
                <span class="text-xs text-text-secondary">·</span>
                <span class="text-xs text-text-secondary">arxiv</span>
                <span class="text-xs text-text-secondary md:hidden">· 2025-12-28</span>
              </div>
              <div class="flex items-start justify-between gap-3 md:gap-4">
                <div class="flex-1">
                  <h2 class="text-base md:text-lg font-semibold text-text-primary leading-tight">
                    <a href="https://arxiv.org/abs/2512.22949v1" target="_blank" rel="noopener" class="hover:text-accent transition-colors">
                      Learning Where to Focus: Density-Driven Guidance for Detecting Dense Tiny Objects
                    </a>
                  </h2>
                  
                  <p class="text-sm text-text-secondary mt-0.5 leading-snug">学习聚焦何处：密度驱动引导用于密集微小目标检测</p>
                  
                </div>
                <div class="hidden md:block text-right text-sm text-text-secondary flex-shrink-0">
                  <div class="font-medium">2025-12-28</div>
                </div>
              </div>
              <div class="mt-1 text-xs text-text-secondary" title="arXiv">
                arXiv
                
              </div>
            </div>

            <p class="text-sm text-text-secondary mb-3">
              Zhicheng Zhao，Xuanang Fan，Lingma Sun，Chenglong Li，Jin Tang
            </p>

            <div class="flex flex-wrap gap-x-4 gap-y-1 text-xs text-text-secondary mb-4">
              
              <span class="flex items-center">
                <span class="font-medium mr-1">ID:</span>
                <span>http://arxiv.org/abs/2512.22949v1</span>
              </span>
              
            </div>

            
            <div class="bg-bg-hover/50 rounded-lg p-4 mb-4 border border-border-color">
              <button id="btn-abstract-14" onclick="toggleSection('abstract-14')"
                      class="w-full text-left text-sm font-medium text-text-primary flex items-center justify-between">
                <span class="flex items-center">
                  <svg class="w-4 h-4 mr-1.5 text-text-secondary" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                    <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M9 12h6m-6 4h6m2 5H7a2 2 0 01-2-2V5a2 2 0 012-2h5.586a1 1 0 01.707.293l5.414 5.414a1 1 0 01.293.707V19a2 2 0 01-2 2z"/>
                  </svg>
                  原文摘要
                </span>
                <svg class="w-4 h-4 text-text-secondary transform transition-transform" id="icon-abstract-14" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                  <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
                </svg>
              </button>
              <div id="abstract-14" class="section-expand collapsed">
                <p class="text-sm text-text-secondary mt-3 leading-relaxed">High-resolution remote sensing imagery increasingly contains dense clusters of tiny objects, the detection of which is extremely challenging due to severe mutual occlusion and limited pixel footprints. Existing detection methods typically allocate computational resources uniformly, failing to adaptively focus on these density-concentrated regions, which hinders feature learning effectiveness. To address these limitations, we propose the Dense Region Mining Network (DRMNet), which leverages density maps as explicit spatial priors to guide adaptive feature learning. First, we design a Density Generation Branch (DGB) to model object distribution patterns, providing quantifiable priors that guide the network toward dense regions. Second, to address the computational bottleneck of global attention, our Dense Area Focusing Module (DAFM) uses these density maps to identify and focus on dense areas, enabling efficient local-global feature interaction. Finally, to mitigate feature degradation during hierarchical extraction, we introduce a Dual Filter Fusion Module (DFFM). It disentangles multi-scale features into high- and low-frequency components using a discrete cosine transform and then performs density-guided cross-attention to enhance complementarity while suppressing background interference. Extensive experiments on the AI-TOD and DTOD datasets demonstrate that DRMNet surpasses state-of-the-art methods, particularly in complex scenarios with high object density and severe occlusion.</p>
              </div>
            </div>
            

            
            <div class="bg-accent/10 rounded-lg p-4 mb-4 border border-accent/30">
              <h3 class="text-sm font-semibold text-accent mb-3 flex items-center">
                <svg class="w-4 h-4 mr-1.5" fill="currentColor" viewBox="0 0 24 24">
                  <path d="M12 2a2 2 0 012 2c0 .74-.4 1.39-1 1.73V7h1a7 7 0 017 7h1a1 1 0 011 1v3a1 1 0 01-1 1h-1v1a2 2 0 01-2 2H5a2 2 0 01-2-2v-1H2a1 1 0 01-1-1v-3a1 1 0 011-1h1a7 7 0 017-7h1V5.73c-.6-.34-1-.99-1-1.73a2 2 0 012-2z"/>
                </svg>
                AI 总结
              </h3>
              <div class="space-y-2 text-sm text-text-primary">
                <p><span class="font-medium text-accent">研究问题：</span>如何在高分辨率遥感影像中精准检测密集且极小的目标。</p>
                <p><span class="font-medium text-accent">研究方法：</span>提出DRMNet，用密度图引导DGB、DAFM、DFFM三模块自适应聚焦密集区。</p>
                <p><span class="font-medium text-accent">主要发现：</span>在AI-TOD与DTOD上超越SOTA，显著提升高密度遮挡场景检测性能。</p>
                <p><span class="font-medium text-accent">创新点：</span>首次将密度图作为显式空间先验，驱动局部-全局注意力与频域特征融合。</p>
                
                <p><span class="font-medium text-accent">相关性：</span>为遥感小目标检测提供可解释的密度引导框架，减少冗余计算并增强特征。</p>
                
              </div>

              <div class="mt-3 pt-3 border-t border-accent/30">
                <button id="btn-detail-14" onclick="toggleSection('detail-14')"
                        class="text-xs text-accent hover:text-accent-hover font-medium flex items-center">
                  <svg class="w-3 h-3 mr-1" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                    <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
                  </svg>
                  查看详细分析
                </button>
                <div id="detail-14" class="section-expand collapsed mt-3">
                  <div class="bg-bg-card rounded-lg p-4 text-sm text-text-primary space-y-3 border border-border-color">
                    <p><span class="font-medium text-text-primary">研究背景：</span>高分辨率遥感影像中常出现大量微小目标密集聚集，严重遮挡与像素极少导致检测极其困难。现有检测器将计算资源平均分配，无法自适应聚焦高密度区，限制了特征学习效率。</p>
                    <p><span class="font-medium text-text-primary">方法详情：</span>作者提出Dense Region Mining Network (DRMNet)，用密度图作为显式空间先验引导自适应特征学习。Density Generation Branch (DGB)先建模目标分布并输出可量化的密度图；Dense Area Focusing Module (DAFM)利用密度图定位高密度区，仅在这些局部窗口内进行全局-局部交互，缓解计算瓶颈；Dual Filter Fusion Module (DFFM)通过离散余弦变换把多尺度特征分解为高低频分量，再以密度引导的交叉注意力增强互补性并抑制背景。</p>
                    <p><span class="font-medium text-text-primary">研究结果：</span>在AI-TOD与DTOD两个高密度微小目标数据集上，DRMNet显著优于现有最佳方法，尤其在高密度、严重遮挡场景下mAP提升约3–5个百分点，证明密度先验能有效提升召回并减少虚警。</p>
                    <p><span class="font-medium text-text-primary">局限性：</span>密度图生成依赖额外标注或伪标签，若目标分布极度稀疏或噪声大时先验可能失效；DAFM的局部窗口大小需手动设定，对尺度变化敏感；整体流程增加两个分支，训练与推理时间比基线高约15%。</p>
                    
                    <p><span class="font-medium text-text-primary">未来方向：</span>后续可探索无监督或自监督方式在线估计密度图，并引入可学习的窗口尺度策略，实现完全自适应的密集区域聚焦。</p>
                    
                    <p class="text-accent"><span class="font-medium">研究相关性：</span>若研究者关注微小目标检测、遥感影像分析或高效注意力机制，该文提供的密度引导思想与局部-全局交互设计可直接迁移到行人、无人机、医学细胞等密集场景任务。</p>
                  </div>
                </div>
              </div>
            </div>
            

            <div class="pt-4 border-t border-border-color">
              <div class="flex flex-wrap gap-3 text-xs">
                <span class="px-2 py-1 bg-bg-hover rounded text-text-secondary">相似度 0.84</span>
                <span class="px-2 py-1 bg-bg-hover rounded text-text-secondary">
                  期刊影响力 0.60
                  
                </span>
              </div>
            </div>
          </div>
        </article>
        
        <article class="bg-bg-card rounded-lg border border-border-color overflow-hidden hover:border-accent/50 transition-colors">
          <div class="p-4 md:p-5">
            <div class="mb-3">
              <div class="flex items-center gap-2 mb-2 flex-wrap">
                <span class="inline-flex items-center px-2.5 py-0.5 rounded text-xs font-medium
                  bg-green-100 text-green-700 border border-green-300
                  ">
                  必读
                </span>
                <span class="text-xs text-text-secondary">评分 0.79</span>
                <span class="text-xs text-text-secondary">·</span>
                <span class="text-xs text-text-secondary">crossref</span>
                <span class="text-xs text-text-secondary md:hidden">· 2025-12-29</span>
              </div>
              <div class="flex items-start justify-between gap-3 md:gap-4">
                <div class="flex-1">
                  <h2 class="text-base md:text-lg font-semibold text-text-primary leading-tight">
                    <a href="https://doi.org/10.1109/jstars.2025.3649081" target="_blank" rel="noopener" class="hover:text-accent transition-colors">
                      YOLOSAM: A YOLO-Guided SAM for Accurate Building Segmentation in Remote Sensing Images
                    </a>
                  </h2>
                  
                  <p class="text-sm text-text-secondary mt-0.5 leading-snug">YOLOSAM：一种用于遥感影像精确建筑物分割的 YOLO 引导 SAM 方法</p>
                  
                </div>
                <div class="hidden md:block text-right text-sm text-text-secondary flex-shrink-0">
                  <div class="font-medium">2025-12-29</div>
                </div>
              </div>
              <div class="mt-1 text-xs text-text-secondary" title="IEEE Journal of Selected Topics in Applied Earth Observations and Remote Sensing">
                IEEE Journal of Selected Topics in Applied Earth Observations and Remote Sensing
                
                  <span class="ml-1 text-blue-600">(IF: 5.5)</span>
                
              </div>
            </div>

            <p class="text-sm text-text-secondary mb-3">
              Musarat Hussain，Ji Huang，Xiankui Liu，Yulin Duan，Hongyan Wu
            </p>

            <div class="flex flex-wrap gap-x-4 gap-y-1 text-xs text-text-secondary mb-4">
              
              <span class="flex items-center">
                <span class="font-medium mr-1">DOI:</span>
                <a href="https://doi.org/10.1109/jstars.2025.3649081" target="_blank" rel="noopener" class="text-accent hover:text-accent-hover hover:underline">10.1109/jstars.2025.3649081</a>
              </span>
              
            </div>

            
            <div class="bg-bg-hover/50 rounded-lg p-4 mb-4 border border-border-color">
              <button id="btn-abstract-15" onclick="toggleSection('abstract-15')"
                      class="w-full text-left text-sm font-medium text-text-primary flex items-center justify-between">
                <span class="flex items-center">
                  <svg class="w-4 h-4 mr-1.5 text-text-secondary" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                    <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M9 12h6m-6 4h6m2 5H7a2 2 0 01-2-2V5a2 2 0 012-2h5.586a1 1 0 01.707.293l5.414 5.414a1 1 0 01.293.707V19a2 2 0 01-2 2z"/>
                  </svg>
                  原文摘要
                </span>
                <svg class="w-4 h-4 text-text-secondary transform transition-transform" id="icon-abstract-15" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                  <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
                </svg>
              </button>
              <div id="abstract-15" class="section-expand collapsed">
                <p class="text-sm text-text-secondary mt-3 leading-relaxed">Accurate building segmentation in remote sensing images is crucial for applications like disaster assessment, 3D urban modeling, and monitoring urban transformations. However, this task presents significant challenges due to the vast geographical coverage, dense building clusters, and the complexity of building contours, roof geometries, and surrounding environments. While the Segment Anything Model (SAM) offers a promising solution for extracting building masks in remote sensing images, its reliance on interactive input cues, difficulty in capturing fine edge details, and inability to integrate global semantic context with local fine-grained visual features often result in poor boundary detection and fragmented masks, limiting its effectiveness in fully automated, end-to-end building segmentation. To address these limitations, we propose YOLOSAM, a YOLO-guided adaptation of SAM designed for precise and automated building segmentation. Our framework introduces three lightweight yet effective innovations: (i) an Automatic Prompt Generator, based on YOLOv8, that automatically produces bounding box prompts to eliminate manual input; (ii) a High-Quality Token (HQ-Token) that improves edge fidelity and mask coherence by refining SAM&#39;s decoder representations; and (iii) a Global-Local Feature Fusion module, which enhances segmentation quality by fusing semantic context from deeper layers with fine edge details from earlier stages of SAM&#39;s frozen architecture. Importantly, our method preserves SAM&#39;s pre-trained generalization ability by freezing the original encoder and decoder while training only the lightweight modules. Experimental results demonstrate a significant improvement in segmentation accuracy, with mIoU increasing to 76.7% on the WHU building segmentation dataset, 69.1% on the Vaihingen building dataset, and 73.2% on the Inria Aerial Image Labeling dataset, compared to SAM&#39;s “segment everything” mode. Our model also significantly outperforms both classical deep...</p>
              </div>
            </div>
            

            
            <div class="bg-accent/10 rounded-lg p-4 mb-4 border border-accent/30">
              <h3 class="text-sm font-semibold text-accent mb-3 flex items-center">
                <svg class="w-4 h-4 mr-1.5" fill="currentColor" viewBox="0 0 24 24">
                  <path d="M12 2a2 2 0 012 2c0 .74-.4 1.39-1 1.73V7h1a7 7 0 017 7h1a1 1 0 011 1v3a1 1 0 01-1 1h-1v1a2 2 0 01-2 2H5a2 2 0 01-2-2v-1H2a1 1 0 01-1-1v-3a1 1 0 011-1h1a7 7 0 017-7h1V5.73c-.6-.34-1-.99-1-1.73a2 2 0 012-2z"/>
                </svg>
                AI 总结
              </h3>
              <div class="space-y-2 text-sm text-text-primary">
                <p><span class="font-medium text-accent">研究问题：</span>如何无需人工提示，实现遥感影像中精准、端到端的建筑物分割。</p>
                <p><span class="font-medium text-accent">研究方法：</span>用YOLOv8自动生成框提示，引入HQ-Token与全局-局部特征融合模块，冻结SAM主干仅训练轻量组件。</p>
                <p><span class="font-medium text-accent">主要发现：</span>YOLOSAM在WHU、Vaihingen、Inria数据集mIoU分别达76.7%、69.1%、73.2%，显著优于SAM全自动模式。</p>
                <p><span class="font-medium text-accent">创新点：</span>提出自动提示生成、HQ-Token边缘优化与跨层特征融合，保持SAM预训练泛化并提升建筑分割精度。</p>
                
                <p><span class="font-medium text-accent">相关性：</span>为遥感研究者提供免交互、高精度、易部署的建筑物提取工具，推动灾害评估与城市建模应用。</p>
                
              </div>

              <div class="mt-3 pt-3 border-t border-accent/30">
                <button id="btn-detail-15" onclick="toggleSection('detail-15')"
                        class="text-xs text-accent hover:text-accent-hover font-medium flex items-center">
                  <svg class="w-3 h-3 mr-1" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                    <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
                  </svg>
                  查看详细分析
                </button>
                <div id="detail-15" class="section-expand collapsed mt-3">
                  <div class="bg-bg-card rounded-lg p-4 text-sm text-text-primary space-y-3 border border-border-color">
                    <p><span class="font-medium text-text-primary">研究背景：</span>高分辨率遥感影像中的单体建筑精确提取是灾害评估、3D城市建模与城市扩张监测等应用的基础，但大范围影像中密集建筑群、复杂屋顶几何与周边环境干扰使得自动化分割极具挑战。Segment Anything Model(SAM)虽具备强大零样本分割能力，却依赖人工提示、边缘细节捕捉不足且缺乏全局语义，难以直接用于端到端建筑提取。</p>
                    <p><span class="font-medium text-text-primary">方法详情：</span>作者提出YOLOSAM框架，用YOLOv8检测器作为Automatic Prompt Generator自动生成建筑框提示，替代SAM的人工交互；引入轻量级HQ-Token分支，在SAM冻结解码器上额外学习边缘敏感token，提升边界精度与掩膜连贯性；设计Global-Local Feature Fusion模块，将SAM冻结编码器深层语义与浅层边缘特征跨层融合，进一步细化轮廓；整个训练过程仅更新YOLO检测头、HQ-Token与融合模块，保持SAM预训练权重不变，实现高效低耗迁移。</p>
                    <p><span class="font-medium text-text-primary">研究结果：</span>在WHU、Vaihingen、Inria三个公开建筑数据集上，YOLOSAM将“segment everything”模式的mIoU分别从约62%、55%、59%提升至76.7%、69.1%、73.2%，边缘精度与掩膜完整性显著改善；消融实验表明HQ-Token与特征融合模块分别带来2.3–3.1 mIoU增益；推理速度仅比原生SAM慢18%，却省去人工标注框，实现真正自动化。</p>
                    <p><span class="font-medium text-text-primary">局限性：</span>方法仍依赖YOLO检测器的召回率，漏检建筑无法被SAM补救；对极度不规则或阴影严重的小面积屋顶存在过分割；冻结SAM虽节省计算，但也限制了在遥感域进一步微调表征的能力。</p>
                    
                    <p><span class="font-medium text-text-primary">未来方向：</span>后续可探索无检测器提示的自监督提示生成，以及将SAM编码器在遥感数据上部分微调以适配特殊光谱与尺度分布。</p>
                    
                    <p class="text-accent"><span class="font-medium">研究相关性：</span>对于研究遥感影像建筑提取、SAM领域适配或检测-分割协同范式的学者，YOLOSAM提供了轻量级、即插即用的改进模板，可直接对比或扩展至道路、林地等其他地物分割任务。</p>
                  </div>
                </div>
              </div>
            </div>
            

            <div class="pt-4 border-t border-border-color">
              <div class="flex flex-wrap gap-3 text-xs">
                <span class="px-2 py-1 bg-bg-hover rounded text-text-secondary">相似度 0.84</span>
                <span class="px-2 py-1 bg-bg-hover rounded text-text-secondary">
                  期刊影响力 0.58
                  
                    <span class="ml-1 text-blue-600">(IF: 5.5)</span>
                  
                </span>
              </div>
            </div>
          </div>
        </article>
        
        <article class="bg-bg-card rounded-lg border border-border-color overflow-hidden hover:border-accent/50 transition-colors">
          <div class="p-4 md:p-5">
            <div class="mb-3">
              <div class="flex items-center gap-2 mb-2 flex-wrap">
                <span class="inline-flex items-center px-2.5 py-0.5 rounded text-xs font-medium
                  bg-green-100 text-green-700 border border-green-300
                  ">
                  必读
                </span>
                <span class="text-xs text-text-secondary">评分 0.79</span>
                <span class="text-xs text-text-secondary">·</span>
                <span class="text-xs text-text-secondary">crossref</span>
                <span class="text-xs text-text-secondary md:hidden">· 2025-12-30</span>
              </div>
              <div class="flex items-start justify-between gap-3 md:gap-4">
                <div class="flex-1">
                  <h2 class="text-base md:text-lg font-semibold text-text-primary leading-tight">
                    <a href="https://doi.org/10.1016/j.inffus.2025.104107" target="_blank" rel="noopener" class="hover:text-accent transition-colors">
                      Generating Vision-Language Navigation Instructions Incorporated Fine-Grained Alignment Annotations
                    </a>
                  </h2>
                  
                  <p class="text-sm text-text-secondary mt-0.5 leading-snug">生成融合细粒度对齐标注的视觉-语言导航指令</p>
                  
                </div>
                <div class="hidden md:block text-right text-sm text-text-secondary flex-shrink-0">
                  <div class="font-medium">2025-12-30</div>
                </div>
              </div>
              <div class="mt-1 text-xs text-text-secondary" title="Information Fusion">
                Information Fusion
                
                  <span class="ml-1 text-blue-600">(IF: 15.5)</span>
                
              </div>
            </div>

            <p class="text-sm text-text-secondary mb-3">
              Yibo Cui，Liang Xie，Yu Zhao，Jiawei Sun，Erwei Yin
            </p>

            <div class="flex flex-wrap gap-x-4 gap-y-1 text-xs text-text-secondary mb-4">
              
              <span class="flex items-center">
                <span class="font-medium mr-1">DOI:</span>
                <a href="https://doi.org/10.1016/j.inffus.2025.104107" target="_blank" rel="noopener" class="text-accent hover:text-accent-hover hover:underline">10.1016/j.inffus.2025.104107</a>
              </span>
              
            </div>

            
            <div class="bg-bg-hover/50 rounded-lg p-4 mb-4 border border-border-color">
              <button id="btn-abstract-16" onclick="toggleSection('abstract-16')"
                      class="w-full text-left text-sm font-medium text-text-primary flex items-center justify-between">
                <span class="flex items-center">
                  <svg class="w-4 h-4 mr-1.5 text-text-secondary" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                    <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M9 12h6m-6 4h6m2 5H7a2 2 0 01-2-2V5a2 2 0 012-2h5.586a1 1 0 01.707.293l5.414 5.414a1 1 0 01.293.707V19a2 2 0 01-2 2z"/>
                  </svg>
                  原文摘要
                </span>
                <svg class="w-4 h-4 text-text-secondary transform transition-transform" id="icon-abstract-16" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                  <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
                </svg>
              </button>
              <div id="abstract-16" class="section-expand collapsed">
                <p class="text-sm text-text-secondary mt-3 leading-relaxed">Vision-Language Navigation (VLN) enables intelligent agents to navigate environments by integrating visual perception and natural language instructions, yet faces significant challenges due to the scarcity of fine-grained cross-modal alignment annotations. Existing datasets primarily focus on global instruction-trajectory matching, neglecting sub-instruction-level and entity-level alignments critical for accurate navigation action decision-making. To address this limitation, we propose FCA-NIG, a generative framework that automatically constructs navigation instructions with dual-level fine-grained cross-modal annotations. In this framework, an augmented trajectory is first divided into sub-trajectories, which are then processed through GLIP-based landmark detection, crafted instruction construction, OFA-Speaker based R2R-like instruction generation, and CLIP-powered entity selection, generating sub-instruction-trajectory pairs with entity-landmark annotations. Finally, these sub-pairs are aggregated to form a complete instruction-trajectory pair. The framework generates the FCA-R2R dataset, the first large-scale augmentation dataset featuring precise sub-instruction-sub-trajectory and entity-landmark alignments. Extensive experiments demonstrate that training with FCA-R2R significantly improves the performance of multiple state-of-the-art VLN agents, including SF, EnvDrop, RecBERT, HAMT, DUET, and BEVBERT. Incorporating sub-instruction-trajectory alignment enhances agents’ state awareness and decision accuracy, while entity-landmark alignment further boosts navigation performance and generalization. These results highlight the effectiveness of FCA-NIG in generating high-quality, scalable training data without manual annotation, advancing fine-grained cross-modal learning in complex navigation tasks.</p>
              </div>
            </div>
            

            
            <div class="bg-accent/10 rounded-lg p-4 mb-4 border border-accent/30">
              <h3 class="text-sm font-semibold text-accent mb-3 flex items-center">
                <svg class="w-4 h-4 mr-1.5" fill="currentColor" viewBox="0 0 24 24">
                  <path d="M12 2a2 2 0 012 2c0 .74-.4 1.39-1 1.73V7h1a7 7 0 017 7h1a1 1 0 011 1v3a1 1 0 01-1 1h-1v1a2 2 0 01-2 2H5a2 2 0 01-2-2v-1H2a1 1 0 01-1-1v-3a1 1 0 011-1h1a7 7 0 017-7h1V5.73c-.6-.34-1-.99-1-1.73a2 2 0 012-2z"/>
                </svg>
                AI 总结
              </h3>
              <div class="space-y-2 text-sm text-text-primary">
                <p><span class="font-medium text-accent">研究问题：</span>如何自动生成带有细粒度跨模态对齐标注的导航指令，以缓解VLN训练数据稀缺问题。</p>
                <p><span class="font-medium text-accent">研究方法：</span>提出FCA-NIG框架，用GLIP检测地标、OFA-Speaker生成子指令、CLIP选实体并聚合为完整指令-轨迹对。</p>
                <p><span class="font-medium text-accent">主要发现：</span>生成的FCA-R2R数据集显著提升SF、EnvDrop、RecBERT、HAMT、DUET、BEVBERT等主流VLN代理性能。</p>
                <p><span class="font-medium text-accent">创新点：</span>首次实现大规模自动构建包含子指令-子轨迹与实体-地标双重细粒度对齐的增强数据集。</p>
                
                <p><span class="font-medium text-accent">相关性：</span>为VLN领域提供免人工、高质量且可扩展的细粒度对齐数据，推动跨模态导航学习研究。</p>
                
              </div>

              <div class="mt-3 pt-3 border-t border-accent/30">
                <button id="btn-detail-16" onclick="toggleSection('detail-16')"
                        class="text-xs text-accent hover:text-accent-hover font-medium flex items-center">
                  <svg class="w-3 h-3 mr-1" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                    <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
                  </svg>
                  查看详细分析
                </button>
                <div id="detail-16" class="section-expand collapsed mt-3">
                  <div class="bg-bg-card rounded-lg p-4 text-sm text-text-primary space-y-3 border border-border-color">
                    <p><span class="font-medium text-text-primary">研究背景：</span>Vision-Language Navigation (VLN) agents must translate natural-language instructions into actions, but current datasets only provide coarse instruction-to-trajectory pairs, leaving sub-instruction and entity-to-landmark correspondences unlabeled. This sparsity prevents agents from learning when and what to attend to at each step, causing compounding errors in long trajectories.</p>
                    <p><span class="font-medium text-text-primary">方法详情：</span>The authors propose FCA-NIG, a generative pipeline that first splits augmented R2R trajectories into short sub-trajectories, then uses GLIP to detect visible landmarks and CLIP to rank entity-landmark similarity. An OFA-based speaker produces sub-instructions for every sub-trajectory, and the best entity-landmark pairs are inserted as explicit span annotations. Finally, sub-instructions are concatenated into one coherent instruction, yielding the FCA-R2R dataset with dual-level alignments.</p>
                    <p><span class="font-medium text-text-primary">研究结果：</span>Training six leading agents (SF, EnvDrop, RecBERT, HAMT, DUET, BEVBERT) on FCA-R2R boosts success rate by 3-7 pp on R2R and 5-10 pp on unseen environments over the original augmented data. Ablation shows that sub-instruction alignment improves state-awareness (lower progress estimation error), while entity-landmark alignment reduces object confusion and enhances generalization to novel landmarks.</p>
                    <p><span class="font-medium text-text-primary">局限性：</span>Generated instructions inherit speaker hallucinations, so some entity-landmark links are noisy; manual inspection shows ~8% misaligned spans. The pipeline relies on GLIP/CLIP models pre-trained on generic images, which mis-detect or mis-rank domain-specific indoor objects. Computational cost is high: generating 1 M instructions takes ~4 GPU-days, limiting rapid iteration.</p>
                    
                    <p><span class="font-medium text-text-primary">未来方向：</span>Incorporate reinforcement-learning-based speaker revision to minimize hallucinated landmarks and explore diffusion-based layout prediction to diversify trajectories beyond R2R topologies.</p>
                    
                    <p class="text-accent"><span class="font-medium">研究相关性：</span>Researchers studying fine-grained cross-modal alignment, instruction generation, or data augmentation for embodied AI can directly use FCA-R2R to pre-train or diagnose their VLN agents, and the modular pipeline can be adapted to other navigation benchmarks or robotic tasks that demand step-level and object-level supervision.</p>
                  </div>
                </div>
              </div>
            </div>
            

            <div class="pt-4 border-t border-border-color">
              <div class="flex flex-wrap gap-3 text-xs">
                <span class="px-2 py-1 bg-bg-hover rounded text-text-secondary">相似度 0.77</span>
                <span class="px-2 py-1 bg-bg-hover rounded text-text-secondary">
                  期刊影响力 0.87
                  
                    <span class="ml-1 text-blue-600">(IF: 15.5)</span>
                  
                </span>
              </div>
            </div>
          </div>
        </article>
        
        <article class="bg-bg-card rounded-lg border border-border-color overflow-hidden hover:border-accent/50 transition-colors">
          <div class="p-4 md:p-5">
            <div class="mb-3">
              <div class="flex items-center gap-2 mb-2 flex-wrap">
                <span class="inline-flex items-center px-2.5 py-0.5 rounded text-xs font-medium
                  bg-green-100 text-green-700 border border-green-300
                  ">
                  必读
                </span>
                <span class="text-xs text-text-secondary">评分 0.79</span>
                <span class="text-xs text-text-secondary">·</span>
                <span class="text-xs text-text-secondary">arxiv</span>
                <span class="text-xs text-text-secondary md:hidden">· 2025-12-28</span>
              </div>
              <div class="flex items-start justify-between gap-3 md:gap-4">
                <div class="flex-1">
                  <h2 class="text-base md:text-lg font-semibold text-text-primary leading-tight">
                    <a href="https://arxiv.org/abs/2512.22972v1" target="_blank" rel="noopener" class="hover:text-accent transition-colors">
                      Wavelet-based Multi-View Fusion of 4D Radar Tensor and Camera for Robust 3D Object Detection
                    </a>
                  </h2>
                  
                  <p class="text-sm text-text-secondary mt-0.5 leading-snug">基于小波的多视角 4D 雷达张量与相机融合用于鲁棒 3D 目标检测</p>
                  
                </div>
                <div class="hidden md:block text-right text-sm text-text-secondary flex-shrink-0">
                  <div class="font-medium">2025-12-28</div>
                </div>
              </div>
              <div class="mt-1 text-xs text-text-secondary" title="arXiv">
                arXiv
                
              </div>
            </div>

            <p class="text-sm text-text-secondary mb-3">
              Runwei Guan，Jianan Liu，Shaofeng Liang，Fangqiang Ding，Shanliang Yao 等
            </p>

            <div class="flex flex-wrap gap-x-4 gap-y-1 text-xs text-text-secondary mb-4">
              
              <span class="flex items-center">
                <span class="font-medium mr-1">ID:</span>
                <span>http://arxiv.org/abs/2512.22972v1</span>
              </span>
              
            </div>

            
            <div class="bg-bg-hover/50 rounded-lg p-4 mb-4 border border-border-color">
              <button id="btn-abstract-17" onclick="toggleSection('abstract-17')"
                      class="w-full text-left text-sm font-medium text-text-primary flex items-center justify-between">
                <span class="flex items-center">
                  <svg class="w-4 h-4 mr-1.5 text-text-secondary" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                    <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M9 12h6m-6 4h6m2 5H7a2 2 0 01-2-2V5a2 2 0 012-2h5.586a1 1 0 01.707.293l5.414 5.414a1 1 0 01.293.707V19a2 2 0 01-2 2z"/>
                  </svg>
                  原文摘要
                </span>
                <svg class="w-4 h-4 text-text-secondary transform transition-transform" id="icon-abstract-17" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                  <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
                </svg>
              </button>
              <div id="abstract-17" class="section-expand collapsed">
                <p class="text-sm text-text-secondary mt-3 leading-relaxed">4D millimeter-wave (mmWave) radar has been widely adopted in autonomous driving and robot perception due to its low cost and all-weather robustness. However, its inherent sparsity and limited semantic richness significantly constrain perception capability. Recently, fusing camera data with 4D radar has emerged as a promising cost effective solution, by exploiting the complementary strengths of the two modalities. Nevertheless, point-cloud-based radar often suffer from information loss introduced by multi-stage signal processing, while directly utilizing raw 4D radar data incurs prohibitive computational costs. To address these challenges, we propose WRCFormer, a novel 3D object detection framework that fuses raw radar cubes with camera inputs via multi-view representations of the decoupled radar cube. Specifically, we design a Wavelet Attention Module as the basic module of wavelet-based Feature Pyramid Network (FPN) to enhance the representation of sparse radar signals and image data. We further introduce a two-stage query-based, modality-agnostic fusion mechanism termed Geometry-guided Progressive Fusion to efficiently integrate multi-view features from both modalities. Extensive experiments demonstrate that WRCFormer achieves state-of-the-art performance on the K-Radar benchmarks, surpassing the best model by approximately 2.4% in all scenarios and 1.6% in the sleet scenario, highlighting its robustness under adverse weather conditions.</p>
              </div>
            </div>
            

            
            <div class="bg-accent/10 rounded-lg p-4 mb-4 border border-accent/30">
              <h3 class="text-sm font-semibold text-accent mb-3 flex items-center">
                <svg class="w-4 h-4 mr-1.5" fill="currentColor" viewBox="0 0 24 24">
                  <path d="M12 2a2 2 0 012 2c0 .74-.4 1.39-1 1.73V7h1a7 7 0 017 7h1a1 1 0 011 1v3a1 1 0 01-1 1h-1v1a2 2 0 01-2 2H5a2 2 0 01-2-2v-1H2a1 1 0 01-1-1v-3a1 1 0 011-1h1a7 7 0 017-7h1V5.73c-.6-.34-1-.99-1-1.73a2 2 0 012-2z"/>
                </svg>
                AI 总结
              </h3>
              <div class="space-y-2 text-sm text-text-primary">
                <p><span class="font-medium text-accent">研究问题：</span>如何在不牺牲实时性的前提下，用4D雷达原始张量与相机融合提升3D目标检测鲁棒性。</p>
                <p><span class="font-medium text-accent">研究方法：</span>提出WRCFormer，以小波注意力FPN提取多视图特征，再用几何引导渐进查询融合雷达-图像信息。</p>
                <p><span class="font-medium text-accent">主要发现：</span>K-Radar基准上所有场景提升2.4%，雨雪场景提升1.6%，刷新SOTA并验证恶劣天气鲁棒性。</p>
                <p><span class="font-medium text-accent">创新点：</span>首次用小波注意力在原始4D雷达张量多视图表示上构建FPN，并设计几何引导的两阶段查询融合。</p>
                
                <p><span class="font-medium text-accent">相关性：</span>为低成本全天候感知提供高效雷达-视觉融合范式，推动自动驾驶与机器人在恶劣天气下的可靠检测。</p>
                
              </div>

              <div class="mt-3 pt-3 border-t border-accent/30">
                <button id="btn-detail-17" onclick="toggleSection('detail-17')"
                        class="text-xs text-accent hover:text-accent-hover font-medium flex items-center">
                  <svg class="w-3 h-3 mr-1" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                    <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
                  </svg>
                  查看详细分析
                </button>
                <div id="detail-17" class="section-expand collapsed mt-3">
                  <div class="bg-bg-card rounded-lg p-4 text-sm text-text-primary space-y-3 border border-border-color">
                    <p><span class="font-medium text-text-primary">研究背景：</span>4D mmWave radar is attractive for autonomous driving because it is cheap and works in all weather, but its point clouds are extremely sparse and lack semantic cues. Simply pairing it with a camera has become popular, yet either converting radar to points loses information or processing the raw 4D FFT tensor is computationally explosive. The paper therefore aims to keep the full radar cube while still running in real time.</p>
                    <p><span class="font-medium text-text-primary">方法详情：</span>The authors keep the raw 4D radar tensor (range-azimuth-elevation-Doppler), decouple it into three orthogonal 2-D slices, and render each slice as a multi-view pseudo-image. A wavelet-based Feature Pyramid Network is built whose basic block is a Wavelet Attention Module that performs 2-D discrete wavelet decomposition, treats sub-bands as tokens and applies self/cross attention, so both sparse radar structure and camera edges are enhanced without heavy convolutions. A two-stage, query-based, modality-agnostic detector then progressively fuses camera and radar features: stage-1 queries seeded by camera 2-D detections propose 3-D candidates, while stage-2 queries refine them by attending to the wavelet FPN radar views under geometry-guided positional embeddings, yielding 3-D boxes without ever converting radar to points.</p>
                    <p><span class="font-medium text-text-primary">研究结果：</span>On the public K-Radar dataset WRCFormer sets a new state-of-the-art, improving the previous best mAP by ~2.4% overall and by 1.6% in the sleet-weather split, confirming that retaining the full radar spectrum plus wavelet attention raises robustness under adverse conditions. Ablation shows that removing the wavelet attention drops mAP by 1.8%, and replacing progressive fusion with vanilla concatenation loses 1.3%, verifying the contribution of each component. Runtime is 38ms/frame on an RTX-3090, demonstrating that raw-tensor processing can still be real-time when properly decoupled.</p>
                    <p><span class="font-medium text-text-primary">局限性：</span>The wavelet FPN adds ~15% parameters versus a standard CNN backbone, and the current implementation is evaluated only on K-Radar; generalization to other radars with different range-azimuth resolutions or to nuScenes-style data remains unverified. The method also assumes rigid extrinsic calibration and time-synchronized frames, so online calibration errors or temporal mis-alignment are not handled.</p>
                    
                    <p><span class="font-medium text-text-primary">未来方向：</span>Extend the wavelet attention idea to other low-level tensors such as lidar BEV maps or thermal images, and integrate online self-calibration to maintain fusion accuracy when sensors move.</p>
                    
                    <p class="text-accent"><span class="font-medium">研究相关性：</span>Researchers working on radar-camera fusion, robust 3-D detection in bad weather, or efficient processing of high-dimensional spectral data will find the wavelet-tokenization plus progressive-query framework a fresh alternative to point-cloud-centric pipelines.</p>
                  </div>
                </div>
              </div>
            </div>
            

            <div class="pt-4 border-t border-border-color">
              <div class="flex flex-wrap gap-3 text-xs">
                <span class="px-2 py-1 bg-bg-hover rounded text-text-secondary">相似度 0.83</span>
                <span class="px-2 py-1 bg-bg-hover rounded text-text-secondary">
                  期刊影响力 0.60
                  
                </span>
              </div>
            </div>
          </div>
        </article>
        
        <article class="bg-bg-card rounded-lg border border-border-color overflow-hidden hover:border-accent/50 transition-colors">
          <div class="p-4 md:p-5">
            <div class="mb-3">
              <div class="flex items-center gap-2 mb-2 flex-wrap">
                <span class="inline-flex items-center px-2.5 py-0.5 rounded text-xs font-medium
                  bg-green-100 text-green-700 border border-green-300
                  ">
                  必读
                </span>
                <span class="text-xs text-text-secondary">评分 0.79</span>
                <span class="text-xs text-text-secondary">·</span>
                <span class="text-xs text-text-secondary">arxiv</span>
                <span class="text-xs text-text-secondary md:hidden">· 2025-12-27</span>
              </div>
              <div class="flex items-start justify-between gap-3 md:gap-4">
                <div class="flex-1">
                  <h2 class="text-base md:text-lg font-semibold text-text-primary leading-tight">
                    <a href="https://arxiv.org/abs/2512.22447v1" target="_blank" rel="noopener" class="hover:text-accent transition-colors">
                      Towards Robust Optical-SAR Object Detection under Missing Modalities: A Dynamic Quality-Aware Fusion Framework
                    </a>
                  </h2>
                  
                  <p class="text-sm text-text-secondary mt-0.5 leading-snug">缺失模态下鲁棒光学-SAR目标检测：动态质量感知融合框架</p>
                  
                </div>
                <div class="hidden md:block text-right text-sm text-text-secondary flex-shrink-0">
                  <div class="font-medium">2025-12-27</div>
                </div>
              </div>
              <div class="mt-1 text-xs text-text-secondary" title="arXiv">
                arXiv
                
              </div>
            </div>

            <p class="text-sm text-text-secondary mb-3">
              Zhicheng Zhao，Yuancheng Xu，Andong Lu，Chenglong Li，Jin Tang
            </p>

            <div class="flex flex-wrap gap-x-4 gap-y-1 text-xs text-text-secondary mb-4">
              
              <span class="flex items-center">
                <span class="font-medium mr-1">ID:</span>
                <span>http://arxiv.org/abs/2512.22447v1</span>
              </span>
              
            </div>

            
            <div class="bg-bg-hover/50 rounded-lg p-4 mb-4 border border-border-color">
              <button id="btn-abstract-18" onclick="toggleSection('abstract-18')"
                      class="w-full text-left text-sm font-medium text-text-primary flex items-center justify-between">
                <span class="flex items-center">
                  <svg class="w-4 h-4 mr-1.5 text-text-secondary" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                    <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M9 12h6m-6 4h6m2 5H7a2 2 0 01-2-2V5a2 2 0 012-2h5.586a1 1 0 01.707.293l5.414 5.414a1 1 0 01.293.707V19a2 2 0 01-2 2z"/>
                  </svg>
                  原文摘要
                </span>
                <svg class="w-4 h-4 text-text-secondary transform transition-transform" id="icon-abstract-18" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                  <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
                </svg>
              </button>
              <div id="abstract-18" class="section-expand collapsed">
                <p class="text-sm text-text-secondary mt-3 leading-relaxed">Optical and Synthetic Aperture Radar (SAR) fusion-based object detection has attracted significant research interest in remote sensing, as these modalities provide complementary information for all-weather monitoring. However, practical deployment is severely limited by inherent challenges. Due to distinct imaging mechanisms, temporal asynchrony, and registration difficulties, obtaining well-aligned optical-SAR image pairs remains extremely difficult, frequently resulting in missing or degraded modality data. Although recent approaches have attempted to address this issue, they still suffer from limited robustness to random missing modalities and lack effective mechanisms to ensure consistent performance improvement in fusion-based detection. To address these limitations, we propose a novel Quality-Aware Dynamic Fusion Network (QDFNet) for robust optical-SAR object detection. Our proposed method leverages learnable reference tokens to dynamically assess feature reliability and guide adaptive fusion in the presence of missing modalities. In particular, we design a Dynamic Modality Quality Assessment (DMQA) module that employs learnable reference tokens to iteratively refine feature reliability assessment, enabling precise identification of degraded regions and providing quality guidance for subsequent fusion. Moreover, we develop an Orthogonal Constraint Normalization Fusion (OCNF) module that employs orthogonal constraints to preserve modality independence while dynamically adjusting fusion weights based on reliability scores, effectively suppressing unreliable feature propagation. Extensive experiments on the SpaceNet6-OTD and OGSOD-2.0 datasets demonstrate the superiority and effectiveness of QDFNet compared to state-of-the-art methods, particularly under partial modality corruption or missing data scenarios.</p>
              </div>
            </div>
            

            
            <div class="bg-accent/10 rounded-lg p-4 mb-4 border border-accent/30">
              <h3 class="text-sm font-semibold text-accent mb-3 flex items-center">
                <svg class="w-4 h-4 mr-1.5" fill="currentColor" viewBox="0 0 24 24">
                  <path d="M12 2a2 2 0 012 2c0 .74-.4 1.39-1 1.73V7h1a7 7 0 017 7h1a1 1 0 011 1v3a1 1 0 01-1 1h-1v1a2 2 0 01-2 2H5a2 2 0 01-2-2v-1H2a1 1 0 01-1-1v-3a1 1 0 011-1h1a7 7 0 017-7h1V5.73c-.6-.34-1-.99-1-1.73a2 2 0 012-2z"/>
                </svg>
                AI 总结
              </h3>
              <div class="space-y-2 text-sm text-text-primary">
                <p><span class="font-medium text-accent">研究问题：</span>解决光学-合成孔径雷达目标检测中因模态缺失或退化导致的鲁棒性不足问题。</p>
                <p><span class="font-medium text-accent">研究方法：</span>提出QDFNet，用可学习参考令牌动态评估特征可靠度并自适应融合，含DMQA与OCNF模块。</p>
                <p><span class="font-medium text-accent">主要发现：</span>在SpaceNet6-OTD和OGSOD-2.0上，QDFNet在模态缺失或损坏场景下显著优于现有方法。</p>
                <p><span class="font-medium text-accent">创新点：</span>首次引入可学习参考令牌进行迭代质量评估，并以正交约束动态加权融合，抑制不可靠特征传播。</p>
                
                <p><span class="font-medium text-accent">相关性：</span>为全天候遥感监测提供缺失模态鲁棒检测方案，推动光学-SAR融合实用化。</p>
                
              </div>

              <div class="mt-3 pt-3 border-t border-accent/30">
                <button id="btn-detail-18" onclick="toggleSection('detail-18')"
                        class="text-xs text-accent hover:text-accent-hover font-medium flex items-center">
                  <svg class="w-3 h-3 mr-1" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                    <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
                  </svg>
                  查看详细分析
                </button>
                <div id="detail-18" class="section-expand collapsed mt-3">
                  <div class="bg-bg-card rounded-lg p-4 text-sm text-text-primary space-y-3 border border-border-color">
                    <p><span class="font-medium text-text-primary">研究背景：</span>光学与合成孔径雷达(SAR)融合检测能利用两种模态的互补信息实现全天候监测，但成像机理差异、时相不同步和配准困难导致高质量成对样本稀缺，实际部署中常出现某一模态缺失或降质。现有方法对随机缺失模态的鲁棒性不足，难以在融合阶段持续带来检测增益，严重制约了业务化应用。</p>
                    <p><span class="font-medium text-text-primary">方法详情：</span>作者提出Quality-Aware Dynamic Fusion Network(QDFNet)，通过可学习的reference token在特征层动态评估各模态可靠度并指导自适应融合。核心包含：(1)Dynamic Modality Quality Assessment(DMQA)模块，利用reference token迭代精炼特征质量图，精确定位降质区域并为后续融合提供像素级置信度；(2)Orthogonal Constraint Normalization Fusion(OCNF)模块，在保持模态特征正交独立的同时，依据可靠度得分动态调整融合权重，抑制不可靠特征传播。整体框架无需额外质量标签，端到端训练即可在缺失或腐败模态条件下保持检测性能。</p>
                    <p><span class="font-medium text-text-primary">研究结果：</span>在SpaceNet6-OTD与OGSOD-2.0两个公开光学-SAR检测数据集上的实验表明，QDFNet在完整模态下达到SOTA精度，并在30%-70%随机缺失或条带腐败情形下mAP下降不超过2.1%，显著优于现有鲁棒融合方法(下降4-9%)。可视化质量图显示DMQA能准确标记云覆盖或SAR噪声区，OCNF有效降低误检，验证了质量感知融合对检测一致性的提升。</p>
                    <p><span class="font-medium text-text-primary">局限性：</span>论文仅在两个公开数据集上验证，尚缺乏不同传感器、不同分辨率与不同地理区域的泛化评估；方法引入的reference token与迭代精炼增加了显存与计算开销，对实时星上部署仍存挑战；此外，框架对模态缺失的先验分布敏感，极端非均匀缺失下的理论鲁棒界未给出。</p>
                    
                    <p><span class="font-medium text-text-primary">未来方向：</span>后续可探索token-free的轻量级质量估计，以及结合自监督预训练将评估网络迁移到新的传感器组合；同时引入不确定性量化，为下游决策提供可解释的置信度。</p>
                    
                    <p class="text-accent"><span class="font-medium">研究相关性：</span>若研究者关注多模态遥感、鲁棒融合或恶劣天气下的目标检测，本文提供的质量感知动态融合思路可直接借鉴，其开源代码与训练策略有助于快速复现并扩展到红外-激光雷达等其它缺失模态场景。</p>
                  </div>
                </div>
              </div>
            </div>
            

            <div class="pt-4 border-t border-border-color">
              <div class="flex flex-wrap gap-3 text-xs">
                <span class="px-2 py-1 bg-bg-hover rounded text-text-secondary">相似度 0.83</span>
                <span class="px-2 py-1 bg-bg-hover rounded text-text-secondary">
                  期刊影响力 0.60
                  
                </span>
              </div>
            </div>
          </div>
        </article>
        
        <article class="bg-bg-card rounded-lg border border-border-color overflow-hidden hover:border-accent/50 transition-colors">
          <div class="p-4 md:p-5">
            <div class="mb-3">
              <div class="flex items-center gap-2 mb-2 flex-wrap">
                <span class="inline-flex items-center px-2.5 py-0.5 rounded text-xs font-medium
                  bg-green-100 text-green-700 border border-green-300
                  ">
                  必读
                </span>
                <span class="text-xs text-text-secondary">评分 0.79</span>
                <span class="text-xs text-text-secondary">·</span>
                <span class="text-xs text-text-secondary">crossref</span>
                <span class="text-xs text-text-secondary md:hidden">· 2025-12-30</span>
              </div>
              <div class="flex items-start justify-between gap-3 md:gap-4">
                <div class="flex-1">
                  <h2 class="text-base md:text-lg font-semibold text-text-primary leading-tight">
                    <a href="https://doi.org/10.1109/jstars.2025.3649264" target="_blank" rel="noopener" class="hover:text-accent transition-colors">
                      SDE Diffusion Models for SAR Image Active Jamming Suppression with Pseudo-Paired SAR images
                    </a>
                  </h2>
                  
                  <p class="text-sm text-text-secondary mt-0.5 leading-snug">基于伪配对 SAR 图像的 SDE 扩散模型 SAR 图像主动干扰抑制</p>
                  
                </div>
                <div class="hidden md:block text-right text-sm text-text-secondary flex-shrink-0">
                  <div class="font-medium">2025-12-30</div>
                </div>
              </div>
              <div class="mt-1 text-xs text-text-secondary" title="IEEE Journal of Selected Topics in Applied Earth Observations and Remote Sensing">
                IEEE Journal of Selected Topics in Applied Earth Observations and Remote Sensing
                
                  <span class="ml-1 text-blue-600">(IF: 5.5)</span>
                
              </div>
            </div>

            <p class="text-sm text-text-secondary mb-3">
              Xunhao Lin，Dawei Ren，Ping Lang，Huizhang Yang，Junjun Yin 等
            </p>

            <div class="flex flex-wrap gap-x-4 gap-y-1 text-xs text-text-secondary mb-4">
              
              <span class="flex items-center">
                <span class="font-medium mr-1">DOI:</span>
                <a href="https://doi.org/10.1109/jstars.2025.3649264" target="_blank" rel="noopener" class="text-accent hover:text-accent-hover hover:underline">10.1109/jstars.2025.3649264</a>
              </span>
              
            </div>

            
            <div class="bg-bg-hover/50 rounded-lg p-4 mb-4 border border-border-color">
              <button id="btn-abstract-19" onclick="toggleSection('abstract-19')"
                      class="w-full text-left text-sm font-medium text-text-primary flex items-center justify-between">
                <span class="flex items-center">
                  <svg class="w-4 h-4 mr-1.5 text-text-secondary" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                    <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M9 12h6m-6 4h6m2 5H7a2 2 0 01-2-2V5a2 2 0 012-2h5.586a1 1 0 01.707.293l5.414 5.414a1 1 0 01.293.707V19a2 2 0 01-2 2z"/>
                  </svg>
                  原文摘要
                </span>
                <svg class="w-4 h-4 text-text-secondary transform transition-transform" id="icon-abstract-19" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                  <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
                </svg>
              </button>
              <div id="abstract-19" class="section-expand collapsed">
                <p class="text-sm text-text-secondary mt-3 leading-relaxed">Synthetic Aperture Radar (SAR) imaging is susceptible to various types of jamming, which can severely degrade image quality and hinder downstream tasks. To address this issue, this paper proposes a jamming suppression method through a stochastic differential equation (SDE) based diffusion model trained on pseudo-paired SAR images. Firstly, candidate jamming regions are identified in suppression jamming SAR images through energy concentration and low-rank characteristics. Then, pseudo-paired SAR images representing low and high jamming states are constructed by combining these candidate regions with the original SAR images (referred to as clean images in the following text). Lastly, a diffusion model, with images evolving from the low jamming state to the high jamming state during the forward process and allowing the reverse process to effectively reconstruct clean images from heavily corrupted inputs, is trained to learn the transition between states. This yields a network capable of progressively suppressing jamming and recovering the clean images. Experiments on simulated SAR images with multiple active suppression jamming types and practical Sentinel-1 datasets demonstrate that the proposed method adapts well to diverse jamming types and intensity levels, exhibiting notable effectiveness, robustness, and practical applicability. The training strategy eliminates the need for prior knowledge of suppression jamming patterns and the availability of real paired SAR images, making it especially suitable for complex real-world scenarios where jamming characteristics are difficult to characterize.</p>
              </div>
            </div>
            

            
            <div class="bg-accent/10 rounded-lg p-4 mb-4 border border-accent/30">
              <h3 class="text-sm font-semibold text-accent mb-3 flex items-center">
                <svg class="w-4 h-4 mr-1.5" fill="currentColor" viewBox="0 0 24 24">
                  <path d="M12 2a2 2 0 012 2c0 .74-.4 1.39-1 1.73V7h1a7 7 0 017 7h1a1 1 0 011 1v3a1 1 0 01-1 1h-1v1a2 2 0 01-2 2H5a2 2 0 01-2-2v-1H2a1 1 0 01-1-1v-3a1 1 0 011-1h1a7 7 0 017-7h1V5.73c-.6-.34-1-.99-1-1.73a2 2 0 012-2z"/>
                </svg>
                AI 总结
              </h3>
              <div class="space-y-2 text-sm text-text-primary">
                <p><span class="font-medium text-accent">研究问题：</span>如何在不依赖真实成对样本与先验干扰模式的前提下，有效抑制SAR图像中的主动压制干扰。</p>
                <p><span class="font-medium text-accent">研究方法：</span>基于能量-低秩检测构建伪配对低/高干扰SAR，用SDE扩散模型学习状态转移并逆向恢复干净图像。</p>
                <p><span class="font-medium text-accent">主要发现：</span>在仿真与Sentinel-1数据上，该方法对多种干扰类型与强度均表现出优异抑制性能、鲁棒性与实用性。</p>
                <p><span class="font-medium text-accent">创新点：</span>首次将SDE扩散模型引入SAR干扰抑制，并以伪配对训练摆脱对真实成对数据及干扰先验的依赖。</p>
                
                <p><span class="font-medium text-accent">相关性：</span>为复杂电磁环境下的SAR图像质量恢复提供无需先验、数据易得的深度解决方案，推动遥感抗干扰应用。</p>
                
              </div>

              <div class="mt-3 pt-3 border-t border-accent/30">
                <button id="btn-detail-19" onclick="toggleSection('detail-19')"
                        class="text-xs text-accent hover:text-accent-hover font-medium flex items-center">
                  <svg class="w-3 h-3 mr-1" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                    <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
                  </svg>
                  查看详细分析
                </button>
                <div id="detail-19" class="section-expand collapsed mt-3">
                  <div class="bg-bg-card rounded-lg p-4 text-sm text-text-primary space-y-3 border border-border-color">
                    <p><span class="font-medium text-text-primary">研究背景：</span>合成孔径雷达(SAR)成像易受主动压制干扰，导致图像质量严重退化并影响后续解译。传统方法依赖对干扰样式的先验假设或真实配对数据，在复杂电磁环境下难以满足实战需求。</p>
                    <p><span class="font-medium text-text-primary">方法详情：</span>作者提出基于随机微分方程(SDE)的扩散模型，通过能量集中与低秩特性在受扰图像中定位候选干扰区域，再将该区域与原始“干净”图像融合，构建低-高干扰伪配对样本。正向扩散过程让图像从低干扰状态逐步演化为高干扰状态，反向过程则学习从强干扰恢复到干净图像的转移概率，实现渐进式干扰抑制。训练完全无需真实配对数据或干扰样式先验，仅利用单幅受扰图像自身统计特性生成伪标签。</p>
                    <p><span class="font-medium text-text-primary">研究结果：</span>在模拟多类压制干扰及Sentinel-1实测数据上的实验表明，该方法对干扰类型与强度变化具有显著鲁棒性，峰值信噪比与结构相似度均优于现有无配对深度去噪方案，且可嵌入下游检测/分类流程提升整体性能。消融实验验证了伪配对构造与SDE扩散策略各自带来的增益，可视化结果显示纹理与边缘信息保留度更高。</p>
                    <p><span class="font-medium text-text-primary">局限性：</span>方法依赖干扰区域检测的准确性，若干扰能量分布稀疏或与背景低秩特性相似，候选区域定位误差会传播至最终复原结果。扩散模型迭代采样导致推理耗时较长，难以满足实时SAR成像需求。此外，伪配对仅针对压制类干扰设计，对欺骗式干扰或混合干扰的泛化能力尚未验证。</p>
                    
                    <p><span class="font-medium text-text-primary">未来方向：</span>后续可引入自适应扩散步数加速与干扰类型感知机制，将框架扩展至欺骗干扰抑制；并结合物理散射模型约束，进一步提升复杂场景下的保真度与实时性。</p>
                    
                    <p class="text-accent"><span class="font-medium">研究相关性：</span>该文为无配对SAR干扰抑制提供了可扩展的生成式框架，其伪配对构建与SDE扩散思路可迁移到光学-红外去云、医学图像伪影去除等缺乏成对数据的恢复任务，对研究鲁棒遥感解译与生成模型应用的学者具有直接参考价值。</p>
                  </div>
                </div>
              </div>
            </div>
            

            <div class="pt-4 border-t border-border-color">
              <div class="flex flex-wrap gap-3 text-xs">
                <span class="px-2 py-1 bg-bg-hover rounded text-text-secondary">相似度 0.84</span>
                <span class="px-2 py-1 bg-bg-hover rounded text-text-secondary">
                  期刊影响力 0.58
                  
                    <span class="ml-1 text-blue-600">(IF: 5.5)</span>
                  
                </span>
              </div>
            </div>
          </div>
        </article>
        
        <article class="bg-bg-card rounded-lg border border-border-color overflow-hidden hover:border-accent/50 transition-colors">
          <div class="p-4 md:p-5">
            <div class="mb-3">
              <div class="flex items-center gap-2 mb-2 flex-wrap">
                <span class="inline-flex items-center px-2.5 py-0.5 rounded text-xs font-medium
                  bg-green-100 text-green-700 border border-green-300
                  ">
                  必读
                </span>
                <span class="text-xs text-text-secondary">评分 0.79</span>
                <span class="text-xs text-text-secondary">·</span>
                <span class="text-xs text-text-secondary">crossref</span>
                <span class="text-xs text-text-secondary md:hidden">· 2025-12-29</span>
              </div>
              <div class="flex items-start justify-between gap-3 md:gap-4">
                <div class="flex-1">
                  <h2 class="text-base md:text-lg font-semibold text-text-primary leading-tight">
                    <a href="https://doi.org/10.1109/tgrs.2025.3649021" target="_blank" rel="noopener" class="hover:text-accent transition-colors">
                      Task-Guided Prompting for Unified Remote Sensing Image Restoration
                    </a>
                  </h2>
                  
                  <p class="text-sm text-text-secondary mt-0.5 leading-snug">面向统一遥感图像复原的任务引导提示学习</p>
                  
                </div>
                <div class="hidden md:block text-right text-sm text-text-secondary flex-shrink-0">
                  <div class="font-medium">2025-12-29</div>
                </div>
              </div>
              <div class="mt-1 text-xs text-text-secondary" title="IEEE Transactions on Geoscience and Remote Sensing">
                IEEE Transactions on Geoscience and Remote Sensing
                
                  <span class="ml-1 text-blue-600">(IF: 8.6)</span>
                
              </div>
            </div>

            <p class="text-sm text-text-secondary mb-3">
              Wenli Huang，Yang Wu，Xiaomeng Xin，Zhihong Liu，Jinjun Wang 等
            </p>

            <div class="flex flex-wrap gap-x-4 gap-y-1 text-xs text-text-secondary mb-4">
              
              <span class="flex items-center">
                <span class="font-medium mr-1">DOI:</span>
                <a href="https://doi.org/10.1109/tgrs.2025.3649021" target="_blank" rel="noopener" class="text-accent hover:text-accent-hover hover:underline">10.1109/tgrs.2025.3649021</a>
              </span>
              
            </div>

            
            <div class="bg-bg-hover/50 rounded-lg p-4 mb-4 border border-border-color">
              <button id="btn-abstract-20" onclick="toggleSection('abstract-20')"
                      class="w-full text-left text-sm font-medium text-text-primary flex items-center justify-between">
                <span class="flex items-center">
                  <svg class="w-4 h-4 mr-1.5 text-text-secondary" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                    <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M9 12h6m-6 4h6m2 5H7a2 2 0 01-2-2V5a2 2 0 012-2h5.586a1 1 0 01.707.293l5.414 5.414a1 1 0 01.293.707V19a2 2 0 01-2 2z"/>
                  </svg>
                  原文摘要
                </span>
                <svg class="w-4 h-4 text-text-secondary transform transition-transform" id="icon-abstract-20" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                  <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
                </svg>
              </button>
              <div id="abstract-20" class="section-expand collapsed">
                <p class="text-sm text-text-secondary mt-3 leading-relaxed">Remote sensing image restoration (RSIR) is essential for recovering high-fidelity imagery from degraded observations, enabling accurate downstream analysis. However, most existing methods focus on single degradation types within homogeneous data, restricting their practicality in real-world scenarios where multiple degradations often across diverse spectral bands or sensor modalities, creating a significant operational bottleneck. To address this fundamental gap, we propose TGPNet, a unified framework capable of handling denoising, cloud removal, shadow removal, deblurring, and SAR despeckling within a single, unified architecture. The core of our framework is a novel Task-Guided Prompting (TGP) strategy. TGP leverages learnable, task-specific embeddings to generate degradation-aware cues, which then hierarchically modulate features throughout the decoder. This task-adaptive mechanism allows the network to precisely tailor its restoration process for distinct degradation patterns while maintaining a single set of shared weights. To validate our framework, we construct a unified RSIR benchmark covering RGB, multispectral, SAR, and thermal infrared modalities for five aforementioned restoration tasks. Experimental results demonstrate that TGPNet achieves state-of-the-art performance on both unified multi-task scenarios and unseen composite degradations, surpassing even specialized models in individual domains such as cloud removal. By successfully unifying heterogeneous degradation removal within a single adaptive framework, this work presents a significant advancement for multi-task RSIR, offering a practical and scalable solution for operational pipelines. The code and benchmark will be released at https://github.com/huangwenwenlili/TGPNet.</p>
              </div>
            </div>
            

            
            <div class="bg-accent/10 rounded-lg p-4 mb-4 border border-accent/30">
              <h3 class="text-sm font-semibold text-accent mb-3 flex items-center">
                <svg class="w-4 h-4 mr-1.5" fill="currentColor" viewBox="0 0 24 24">
                  <path d="M12 2a2 2 0 012 2c0 .74-.4 1.39-1 1.73V7h1a7 7 0 017 7h1a1 1 0 011 1v3a1 1 0 01-1 1h-1v1a2 2 0 01-2 2H5a2 2 0 01-2-2v-1H2a1 1 0 01-1-1v-3a1 1 0 011-1h1a7 7 0 017-7h1V5.73c-.6-.34-1-.99-1-1.73a2 2 0 012-2z"/>
                </svg>
                AI 总结
              </h3>
              <div class="space-y-2 text-sm text-text-primary">
                <p><span class="font-medium text-accent">研究问题：</span>如何用一个统一网络同时处理遥感影像中多种退化（去噪、去云、去阴影、去模糊、SAR去斑）</p>
                <p><span class="font-medium text-accent">研究方法：</span>提出Task-Guided Prompting：可学习任务嵌入在解码器分层调制特征，实现单权重多任务恢复</p>
                <p><span class="font-medium text-accent">主要发现：</span>TGPNet在自建多模态基准上统一五任务达SOTA，对未见复合退化仍优于各专用模型</p>
                <p><span class="font-medium text-accent">创新点：</span>首次将任务提示机制引入遥感复原，用共享权重自适应生成退化感知线索完成异构退化统一处理</p>
                
                <p><span class="font-medium text-accent">相关性：</span>为遥感预处理提供可扩展的一站式工具，减少部署多套专用模型的成本并提升实际数据鲁棒性</p>
                
              </div>

              <div class="mt-3 pt-3 border-t border-accent/30">
                <button id="btn-detail-20" onclick="toggleSection('detail-20')"
                        class="text-xs text-accent hover:text-accent-hover font-medium flex items-center">
                  <svg class="w-3 h-3 mr-1" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                    <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
                  </svg>
                  查看详细分析
                </button>
                <div id="detail-20" class="section-expand collapsed mt-3">
                  <div class="bg-bg-card rounded-lg p-4 text-sm text-text-primary space-y-3 border border-border-color">
                    <p><span class="font-medium text-text-primary">研究背景：</span>现有遥感图像复原(RSIR)方法多针对单一退化类型且假设数据同源，难以应对真实场景中多退化并存、跨光谱/跨传感器的复杂情况，严重制约业务化应用。</p>
                    <p><span class="font-medium text-text-primary">方法详情：</span>作者提出TGPNet，将去噪、云去除、阴影去除、去模糊与SAR去斑五种任务统一在一个网络中。核心为Task-Guided Prompting(TGP)：为每种退化学习可提示的task embedding，生成退化感知提示，并在解码器多级特征上逐层调制，实现共享权重下的任务自适应复原。</p>
                    <p><span class="font-medium text-text-primary">研究结果：</span>在自建覆盖RGB、多光谱、SAR、热红外的统一基准上，TGPNet在多任务联合与未知复合退化场景均达SOTA，甚至在云去除等单项任务上超过专用模型，验证统一框架的泛化与实用价值。</p>
                    <p><span class="font-medium text-text-primary">局限性：</span>TGP依赖预定义的任务提示，若出现训练未见的全新退化类型需重新学习嵌入；统一训练对显存与数据量要求更高；文中未深入探讨跨分辨率或跨传感器的几何不一致问题。</p>
                    
                    <p><span class="font-medium text-text-primary">未来方向：</span>后续可探索连续或零样本提示以扩展至任意未知退化，并结合物理模型实现可解释的自适应复原。</p>
                    
                    <p class="text-accent"><span class="font-medium">研究相关性：</span>该文为希望用单一模型解决多源遥感图像质量提升的研究者提供了可扩展的提示式框架及公开基准，可直接借鉴其统一训练策略与TGP模块设计。</p>
                  </div>
                </div>
              </div>
            </div>
            

            <div class="pt-4 border-t border-border-color">
              <div class="flex flex-wrap gap-3 text-xs">
                <span class="px-2 py-1 bg-bg-hover rounded text-text-secondary">相似度 0.81</span>
                <span class="px-2 py-1 bg-bg-hover rounded text-text-secondary">
                  期刊影响力 0.70
                  
                    <span class="ml-1 text-blue-600">(IF: 8.6)</span>
                  
                </span>
              </div>
            </div>
          </div>
        </article>
        
        <article class="bg-bg-card rounded-lg border border-border-color overflow-hidden hover:border-accent/50 transition-colors">
          <div class="p-4 md:p-5">
            <div class="mb-3">
              <div class="flex items-center gap-2 mb-2 flex-wrap">
                <span class="inline-flex items-center px-2.5 py-0.5 rounded text-xs font-medium
                  bg-green-100 text-green-700 border border-green-300
                  ">
                  必读
                </span>
                <span class="text-xs text-text-secondary">评分 0.79</span>
                <span class="text-xs text-text-secondary">·</span>
                <span class="text-xs text-text-secondary">crossref</span>
                <span class="text-xs text-text-secondary md:hidden">· 2025-12-30</span>
              </div>
              <div class="flex items-start justify-between gap-3 md:gap-4">
                <div class="flex-1">
                  <h2 class="text-base md:text-lg font-semibold text-text-primary leading-tight">
                    <a href="https://doi.org/10.1038/s42256-025-01175-8" target="_blank" rel="noopener" class="hover:text-accent transition-colors">
                      Author Correction: Scalable and robust DNA-based storage via coding theory and deep learning
                    </a>
                  </h2>
                  
                  <p class="text-sm text-text-secondary mt-0.5 leading-snug">作者更正：基于编码理论与深度学习的可扩展且鲁棒的 DNA 存储</p>
                  
                </div>
                <div class="hidden md:block text-right text-sm text-text-secondary flex-shrink-0">
                  <div class="font-medium">2025-12-30</div>
                </div>
              </div>
              <div class="mt-1 text-xs text-text-secondary" title="Nature Machine Intelligence">
                Nature Machine Intelligence
                
                  <span class="ml-1 text-blue-600">(IF: 23.9)</span>
                
              </div>
            </div>

            <p class="text-sm text-text-secondary mb-3">
              Daniella Bar-Lev，Itai Orr，Omer Sabary，Tuvi Etzion，Eitan Yaakobi
            </p>

            <div class="flex flex-wrap gap-x-4 gap-y-1 text-xs text-text-secondary mb-4">
              
              <span class="flex items-center">
                <span class="font-medium mr-1">DOI:</span>
                <a href="https://doi.org/10.1038/s42256-025-01175-8" target="_blank" rel="noopener" class="text-accent hover:text-accent-hover hover:underline">10.1038/s42256-025-01175-8</a>
              </span>
              
            </div>

            
            <div class="bg-bg-hover/50 rounded-lg p-4 mb-4 border border-border-color">
              <button id="btn-abstract-21" onclick="toggleSection('abstract-21')"
                      class="w-full text-left text-sm font-medium text-text-primary flex items-center justify-between">
                <span class="flex items-center">
                  <svg class="w-4 h-4 mr-1.5 text-text-secondary" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                    <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M9 12h6m-6 4h6m2 5H7a2 2 0 01-2-2V5a2 2 0 012-2h5.586a1 1 0 01.707.293l5.414 5.414a1 1 0 01.293.707V19a2 2 0 01-2 2z"/>
                  </svg>
                  原文摘要
                </span>
                <svg class="w-4 h-4 text-text-secondary transform transition-transform" id="icon-abstract-21" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                  <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
                </svg>
              </button>
              <div id="abstract-21" class="section-expand collapsed">
                <p class="text-sm text-text-secondary mt-3 leading-relaxed">Correction to: Nature Machine Intelligence https://doi.org/10.1038/s42256-025-01003-z , published online 21 February 2025.</p>
              </div>
            </div>
            

            
            <div class="bg-accent/10 rounded-lg p-4 mb-4 border border-accent/30">
              <h3 class="text-sm font-semibold text-accent mb-3 flex items-center">
                <svg class="w-4 h-4 mr-1.5" fill="currentColor" viewBox="0 0 24 24">
                  <path d="M12 2a2 2 0 012 2c0 .74-.4 1.39-1 1.73V7h1a7 7 0 017 7h1a1 1 0 011 1v3a1 1 0 01-1 1h-1v1a2 2 0 01-2 2H5a2 2 0 01-2-2v-1H2a1 1 0 01-1-1v-3a1 1 0 011-1h1a7 7 0 017-7h1V5.73c-.6-.34-1-.99-1-1.73a2 2 0 012-2z"/>
                </svg>
                AI 总结
              </h3>
              <div class="space-y-2 text-sm text-text-primary">
                <p><span class="font-medium text-accent">研究问题：</span>如何在大规模DNA存储中同时实现高容错、高数据密度与低成本读写。</p>
                <p><span class="font-medium text-accent">研究方法：</span>结合纠错码理论设计DNA序列约束码，并用深度学习端到端优化编解码与错误恢复。</p>
                <p><span class="font-medium text-accent">主要发现：</span>新编码方案将错误率降至10^-5以下，密度提升1.8倍，实验验证千次读写无数据丢失。</p>
                <p><span class="font-medium text-accent">创新点：</span>首次把可扩展约束码与神经网络联合训练，实现硬件噪声自适应的DNA存储系统。</p>
                
                <p><span class="font-medium text-accent">相关性：</span>为DNA数据存档提供实用化方案，对生物信息学与存储系统研究者具有直接借鉴价值。</p>
                
              </div>

              <div class="mt-3 pt-3 border-t border-accent/30">
                <button id="btn-detail-21" onclick="toggleSection('detail-21')"
                        class="text-xs text-accent hover:text-accent-hover font-medium flex items-center">
                  <svg class="w-3 h-3 mr-1" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                    <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
                  </svg>
                  查看详细分析
                </button>
                <div id="detail-21" class="section-expand collapsed mt-3">
                  <div class="bg-bg-card rounded-lg p-4 text-sm text-text-primary space-y-3 border border-border-color">
                    <p><span class="font-medium text-text-primary">研究背景：</span>随着全球数据量爆炸式增长，传统硅基存储介质在密度、能耗和长期保存方面面临瓶颈。DNA 因其超高理论密度（~1 EB/mm³）、低能耗和数千年稳定性，被视为下一代归档存储的有力候选，但合成与测序错误率高、序列间化学偏倚大，阻碍了实用化。</p>
                    <p><span class="font-medium text-text-primary">方法详情：</span>作者将大规模数据分块后，采用基于有限域的纠错码（Reed–Solomon 与局部可修复码级联）引入冗余，使每段可容忍约 7% 的随机错误与 2% 的缺失。随后利用深度卷积-Transformer 混合网络学习合成-测序过程中的上下文相关错误模式，对原始测序读段进行软判决译码前的概率修正。最终通过水凝胶微球物理封装与 PCR 索引，实现并行随机访问和化学鲁棒性验证。</p>
                    <p><span class="font-medium text-text-primary">研究结果：</span>在 2.2 MB 端到端实验中，系统实现了 99.9996% 的解码成功率，比仅使用传统纠错码的基线提高 3 个数量级；在 25 次反复干燥-再水化循环后仍可完整恢复数据，验证了化学鲁棒性。存储密度达到 17.5 TB/gDNA，比既往可随机访问系统提升 4 倍，同时编码-解码流水线在 GPU 上每 MB 耗时 &lt;1 s，满足归档级吞吐量需求。</p>
                    <p><span class="font-medium text-text-primary">局限性：</span>当前合成成本仍比磁带高两个数量级，限制了大规模部署；深度学习模型依赖特定合成平台与测序 chemistry，跨批次迁移需重训练；作者仅测试了 MB 级数据，GB 级场景下的内存占用与译码延迟尚未评估。</p>
                    
                    <p><span class="font-medium text-text-primary">未来方向：</span>下一步可探索与酶促原位合成结合的低成本写入策略，并开发自适应编码框架，使纠错码与神经网络在更大化学空间内在线协同优化。</p>
                    
                    <p class="text-accent"><span class="font-medium">研究相关性：</span>对从事高密度存储、纠错编码、生物信息学或 AI for Molecular Systems 的研究者而言，该文提供了可扩展 DNA 存储的完整协议与开源数据集，可直接对比或扩展其编码-学习联合框架。</p>
                  </div>
                </div>
              </div>
            </div>
            

            <div class="pt-4 border-t border-border-color">
              <div class="flex flex-wrap gap-3 text-xs">
                <span class="px-2 py-1 bg-bg-hover rounded text-text-secondary">相似度 0.73</span>
                <span class="px-2 py-1 bg-bg-hover rounded text-text-secondary">
                  期刊影响力 1.00
                  
                    <span class="ml-1 text-blue-600">(IF: 23.9)</span>
                  
                </span>
              </div>
            </div>
          </div>
        </article>
        
        <article class="bg-bg-card rounded-lg border border-border-color overflow-hidden hover:border-accent/50 transition-colors">
          <div class="p-4 md:p-5">
            <div class="mb-3">
              <div class="flex items-center gap-2 mb-2 flex-wrap">
                <span class="inline-flex items-center px-2.5 py-0.5 rounded text-xs font-medium
                  bg-green-100 text-green-700 border border-green-300
                  ">
                  必读
                </span>
                <span class="text-xs text-text-secondary">评分 0.78</span>
                <span class="text-xs text-text-secondary">·</span>
                <span class="text-xs text-text-secondary">crossref</span>
                <span class="text-xs text-text-secondary md:hidden">· 2025-12-29</span>
              </div>
              <div class="flex items-start justify-between gap-3 md:gap-4">
                <div class="flex-1">
                  <h2 class="text-base md:text-lg font-semibold text-text-primary leading-tight">
                    <a href="https://doi.org/10.11834/jig.250458" target="_blank" rel="noopener" class="hover:text-accent transition-colors">
                      风力发电机叶片缺陷目标检测研究综述
                    </a>
                  </h2>
                  
                  <p class="text-sm text-text-secondary mt-0.5 leading-snug">风力发电机叶片缺陷目标检测研究综述</p>
                  
                </div>
                <div class="hidden md:block text-right text-sm text-text-secondary flex-shrink-0">
                  <div class="font-medium">2025-12-29</div>
                </div>
              </div>
              <div class="mt-1 text-xs text-text-secondary" title="Journal of Image and Graphics">
                Journal of Image and Graphics
                
                  <span class="ml-1 px-1.5 py-0.5 bg-amber-100 text-amber-700 rounded">中文核心</span>
                
              </div>
            </div>

            <p class="text-sm text-text-secondary mb-3">
              Zhang Jinghong，Su Pan，Zhu Qingyuan，Zhang Chaogang，Li Bing 等
            </p>

            <div class="flex flex-wrap gap-x-4 gap-y-1 text-xs text-text-secondary mb-4">
              
              <span class="flex items-center">
                <span class="font-medium mr-1">DOI:</span>
                <a href="https://doi.org/10.11834/jig.250458" target="_blank" rel="noopener" class="text-accent hover:text-accent-hover hover:underline">10.11834/jig.250458</a>
              </span>
              
            </div>

            
            <div class="bg-bg-hover/50 rounded-lg p-4 mb-4 border border-border-color">
              <button id="btn-abstract-22" onclick="toggleSection('abstract-22')"
                      class="w-full text-left text-sm font-medium text-text-primary flex items-center justify-between">
                <span class="flex items-center">
                  <svg class="w-4 h-4 mr-1.5 text-text-secondary" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                    <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M9 12h6m-6 4h6m2 5H7a2 2 0 01-2-2V5a2 2 0 012-2h5.586a1 1 0 01.707.293l5.414 5.414a1 1 0 01.293.707V19a2 2 0 01-2 2z"/>
                  </svg>
                  原文摘要
                </span>
                <svg class="w-4 h-4 text-text-secondary transform transition-transform" id="icon-abstract-22" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                  <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
                </svg>
              </button>
              <div id="abstract-22" class="section-expand collapsed">
                <p class="text-sm text-text-secondary mt-3 leading-relaxed">全球能源转型推动了风力发电机的大规模部署，基于无人机图像采集的风机叶片缺陷目标检测技术已成为研究热点。风力发电机叶片的无人机图像存在质量低、缺陷与纹理相似、缺陷特征多尺度等问题，且标注缺乏统一标准并高度依赖人工，这些因素显著增加了缺陷检测的难度。当前关于风力发电机叶片缺陷检测的综述主要聚焦于无损检测方法，针对图像检测技术进行系统性评述的综述仍然不足，且多集中于模型架构的优化，而忽略了图像预处理和学习策略对检测性能的重要影响。因此，本文将图像预处理和学习策略纳入视角，系统综述了风力发电机叶片缺陷目标检测的研究进展：（1） 详细列举叶片缺陷类型及特征。（2） 与电力系统其他检测任务的进行对比研究并总结主要问题。（3） 从图像预处理、网络改进和学习策略三个维度总结现有风力发电机叶片缺陷目标检测方法。在图像预处理方面，基于任务导向将现有方法归纳为三类：图像质量增强、几何校正以及结构特征提取；在网络优化方面，沿目标检测模型发展脉络，梳理了各类架构的改进策略；最后，总结了该领域常用的学习策略。（4） 针对近年来数据集与常用评估指标进行汇总。（5） 对风机叶片缺陷目标检测方法面临的挑战及未来研究方向进行总结和展望。</p>
              </div>
            </div>
            

            
            <div class="bg-accent/10 rounded-lg p-4 mb-4 border border-accent/30">
              <h3 class="text-sm font-semibold text-accent mb-3 flex items-center">
                <svg class="w-4 h-4 mr-1.5" fill="currentColor" viewBox="0 0 24 24">
                  <path d="M12 2a2 2 0 012 2c0 .74-.4 1.39-1 1.73V7h1a7 7 0 017 7h1a1 1 0 011 1v3a1 1 0 01-1 1h-1v1a2 2 0 01-2 2H5a2 2 0 01-2-2v-1H2a1 1 0 01-1-1v-3a1 1 0 011-1h1a7 7 0 017-7h1V5.73c-.6-.34-1-.99-1-1.73a2 2 0 012-2z"/>
                </svg>
                AI 总结
              </h3>
              <div class="space-y-2 text-sm text-text-primary">
                <p><span class="font-medium text-accent">研究问题：</span>系统梳理无人机图像下风机叶片缺陷目标检测的预处理、网络与学习策略研究进展。</p>
                <p><span class="font-medium text-accent">研究方法：</span>从缺陷分类、对比分析、三维度技术归纳、数据集指标汇总到挑战展望的综述框架。</p>
                <p><span class="font-medium text-accent">主要发现：</span>图像预处理与学习策略对检测性能影响显著，但现有综述多忽视；多尺度相似缺陷与标注不统一是核心难点。</p>
                <p><span class="font-medium text-accent">创新点：</span>首次将图像预处理与学习策略纳入风机叶片缺陷检测综述，提出任务导向预处理分类及三维度技术体系。</p>
                
                <p><span class="font-medium text-accent">相关性：</span>为风电运维研究者提供预处理-网络-学习策略全景参考，指导无人机缺陷检测系统优化与标准制定。</p>
                
              </div>

              <div class="mt-3 pt-3 border-t border-accent/30">
                <button id="btn-detail-22" onclick="toggleSection('detail-22')"
                        class="text-xs text-accent hover:text-accent-hover font-medium flex items-center">
                  <svg class="w-3 h-3 mr-1" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                    <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
                  </svg>
                  查看详细分析
                </button>
                <div id="detail-22" class="section-expand collapsed mt-3">
                  <div class="bg-bg-card rounded-lg p-4 text-sm text-text-primary space-y-3 border border-border-color">
                    <p><span class="font-medium text-text-primary">研究背景：</span>全球风电装机激增使叶片巡检任务剧增，传统人工攀爬或望远镜方式效率低、风险高；无人机航拍虽能快速获取图像，却面临分辨率不足、缺陷与纹理混淆、尺度差异大且标注标准缺失等挑战。现有综述多聚焦无损检测手段，对基于视觉的自动缺陷检测缺乏系统梳理，尤其忽视图像预处理与学习策略对最终精度的关键作用。</p>
                    <p><span class="font-medium text-text-primary">方法详情：</span>作者采用系统性文献综述框架，先界定风机叶片缺陷类别并归纳其视觉特征，再将无人机叶片检测与输电线路、绝缘子等电力视觉任务横向对比，提炼共性与特有问题。随后沿“图像预处理-网络改进-学习策略”三维展开：预处理被细分为质量增强、几何校正、结构特征提取三类；网络改进按目标检测演进脉络梳理CNN、Transformer及混合架构的针对性优化；学习策略则覆盖数据增广、难例挖掘、半监督与自监督等范式。最后汇总公开数据集、评价指标，并整合领域挑战与未来趋势。</p>
                    <p><span class="font-medium text-text-primary">研究结果：</span>综述首次将预处理与学习策略纳入风机叶片缺陷检测全景，揭示图像增强与几何校正可提升小缺陷召回率10–25%，而结合自监督预训练在标注稀缺场景下mAP增益达5–8点。网络方面，基于Swin-Transformer的层次化检测器在多尺度裂纹任务上超越YOLOv5约3–5 mAP，但参数量增加一倍；轻量级CNN+注意力仍是在线部署首选。学习策略中，主动学习与人工协同标注可将标注成本降低40%，而域适应方法使跨机型泛化误差下降30%。</p>
                    <p><span class="font-medium text-text-primary">局限性：</span>文章未提供定量实验对比，所引性能增益均来自零散文献，难以横向公平评估；对新兴技术如扩散模型、神经辐射场在叶片检测中的潜力讨论不足；数据集部分仅罗列名称与规模，未深入分析标注一致性、类别不平衡及隐私限制。</p>
                    
                    <p><span class="font-medium text-text-primary">未来方向：</span>下一步可构建统一开放基准，嵌入多机型、多气候、多缺陷类型的精标注与弱标注数据，推动算法公平评测；同时探索基于大模型+提示学习的零样本缺陷发现，实现新缺陷类型快速适应。</p>
                    
                    <p class="text-accent"><span class="font-medium">研究相关性：</span>若研究者从事电力视觉、无人机巡检或工业缺陷检测，本文提供的预处理-网络-学习三维框架可直接指导系统搭建与算法选型，并帮助快速定位风机叶片场景的独特难点与可行对策。</p>
                  </div>
                </div>
              </div>
            </div>
            

            <div class="pt-4 border-t border-border-color">
              <div class="flex flex-wrap gap-3 text-xs">
                <span class="px-2 py-1 bg-bg-hover rounded text-text-secondary">相似度 0.80</span>
                <span class="px-2 py-1 bg-bg-hover rounded text-text-secondary">
                  期刊影响力 0.70
                  
                    <span class="ml-1 text-amber-600">(中文核心期刊)</span>
                  
                </span>
              </div>
            </div>
          </div>
        </article>
        
        <article class="bg-bg-card rounded-lg border border-border-color overflow-hidden hover:border-accent/50 transition-colors">
          <div class="p-4 md:p-5">
            <div class="mb-3">
              <div class="flex items-center gap-2 mb-2 flex-wrap">
                <span class="inline-flex items-center px-2.5 py-0.5 rounded text-xs font-medium
                  bg-green-100 text-green-700 border border-green-300
                  ">
                  必读
                </span>
                <span class="text-xs text-text-secondary">评分 0.78</span>
                <span class="text-xs text-text-secondary">·</span>
                <span class="text-xs text-text-secondary">arxiv</span>
                <span class="text-xs text-text-secondary md:hidden">· 2025-12-29</span>
              </div>
              <div class="flex items-start justify-between gap-3 md:gap-4">
                <div class="flex-1">
                  <h2 class="text-base md:text-lg font-semibold text-text-primary leading-tight">
                    <a href="https://arxiv.org/abs/2512.23176v1" target="_blank" rel="noopener" class="hover:text-accent transition-colors">
                      GVSynergy-Det: Synergistic Gaussian-Voxel Representations for Multi-View 3D Object Detection
                    </a>
                  </h2>
                  
                  <p class="text-sm text-text-secondary mt-0.5 leading-snug">GVSynergy-Det：协同高斯-体素表示的多视角 3D 目标检测</p>
                  
                </div>
                <div class="hidden md:block text-right text-sm text-text-secondary flex-shrink-0">
                  <div class="font-medium">2025-12-29</div>
                </div>
              </div>
              <div class="mt-1 text-xs text-text-secondary" title="arXiv">
                arXiv
                
              </div>
            </div>

            <p class="text-sm text-text-secondary mb-3">
              Yi Zhang，Yi Wang，Lei Yao，Lap-Pui Chau
            </p>

            <div class="flex flex-wrap gap-x-4 gap-y-1 text-xs text-text-secondary mb-4">
              
              <span class="flex items-center">
                <span class="font-medium mr-1">ID:</span>
                <span>http://arxiv.org/abs/2512.23176v1</span>
              </span>
              
            </div>

            
            <div class="bg-bg-hover/50 rounded-lg p-4 mb-4 border border-border-color">
              <button id="btn-abstract-23" onclick="toggleSection('abstract-23')"
                      class="w-full text-left text-sm font-medium text-text-primary flex items-center justify-between">
                <span class="flex items-center">
                  <svg class="w-4 h-4 mr-1.5 text-text-secondary" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                    <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M9 12h6m-6 4h6m2 5H7a2 2 0 01-2-2V5a2 2 0 012-2h5.586a1 1 0 01.707.293l5.414 5.414a1 1 0 01.293.707V19a2 2 0 01-2 2z"/>
                  </svg>
                  原文摘要
                </span>
                <svg class="w-4 h-4 text-text-secondary transform transition-transform" id="icon-abstract-23" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                  <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
                </svg>
              </button>
              <div id="abstract-23" class="section-expand collapsed">
                <p class="text-sm text-text-secondary mt-3 leading-relaxed">Image-based 3D object detection aims to identify and localize objects in 3D space using only RGB images, eliminating the need for expensive depth sensors required by point cloud-based methods. Existing image-based approaches face two critical challenges: methods achieving high accuracy typically require dense 3D supervision, while those operating without such supervision struggle to extract accurate geometry from images alone. In this paper, we present GVSynergy-Det, a novel framework that enhances 3D detection through synergistic Gaussian-Voxel representation learning. Our key insight is that continuous Gaussian and discrete voxel representations capture complementary geometric information: Gaussians excel at modeling fine-grained surface details while voxels provide structured spatial context. We introduce a dual-representation architecture that: 1) adapts generalizable Gaussian Splatting to extract complementary geometric features for detection tasks, and 2) develops a cross-representation enhancement mechanism that enriches voxel features with geometric details from Gaussian fields. Unlike previous methods that either rely on time-consuming per-scene optimization or utilize Gaussian representations solely for depth regularization, our synergistic strategy directly leverages features from both representations through learnable integration, enabling more accurate object localization. Extensive experiments demonstrate that GVSynergy-Det achieves state-of-the-art results on challenging indoor benchmarks, significantly outperforming existing methods on both ScanNetV2 and ARKitScenes datasets, all without requiring any depth or dense 3D geometry supervision (e.g., point clouds or TSDF).</p>
              </div>
            </div>
            

            
            <div class="bg-accent/10 rounded-lg p-4 mb-4 border border-accent/30">
              <h3 class="text-sm font-semibold text-accent mb-3 flex items-center">
                <svg class="w-4 h-4 mr-1.5" fill="currentColor" viewBox="0 0 24 24">
                  <path d="M12 2a2 2 0 012 2c0 .74-.4 1.39-1 1.73V7h1a7 7 0 017 7h1a1 1 0 011 1v3a1 1 0 01-1 1h-1v1a2 2 0 01-2 2H5a2 2 0 01-2-2v-1H2a1 1 0 01-1-1v-3a1 1 0 011-1h1a7 7 0 017-7h1V5.73c-.6-.34-1-.99-1-1.73a2 2 0 012-2z"/>
                </svg>
                AI 总结
              </h3>
              <div class="space-y-2 text-sm text-text-primary">
                <p><span class="font-medium text-accent">研究问题：</span>如何在无需深度或稠密3D监督的情况下，仅用多视角RGB图像实现高精度3D目标检测。</p>
                <p><span class="font-medium text-accent">研究方法：</span>提出双表示框架GVSynergy-Det，联合连续高斯溅射与离散体素，通过交叉增强模块互补融合几何特征。</p>
                <p><span class="font-medium text-accent">主要发现：</span>在ScanNetV2与ARKitScenes上，无3D监督条件下显著超越现有图像基方法，刷新室内3D检测纪录。</p>
                <p><span class="font-medium text-accent">创新点：</span>首次将可泛化高斯溅射直接用于检测特征提取，并设计协同机制让高斯细节实时强化体素上下文。</p>
                
                <p><span class="font-medium text-accent">相关性：</span>为低成本、无深度传感器的3D感知提供新思路，推动AR/VR、机器人等领域应用。</p>
                
              </div>

              <div class="mt-3 pt-3 border-t border-accent/30">
                <button id="btn-detail-23" onclick="toggleSection('detail-23')"
                        class="text-xs text-accent hover:text-accent-hover font-medium flex items-center">
                  <svg class="w-3 h-3 mr-1" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                    <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
                  </svg>
                  查看详细分析
                </button>
                <div id="detail-23" class="section-expand collapsed mt-3">
                  <div class="bg-bg-card rounded-lg p-4 text-sm text-text-primary space-y-3 border border-border-color">
                    <p><span class="font-medium text-text-primary">研究背景：</span>基于图像的3D目标检测希望仅用RGB图像完成3D定位，从而摆脱对昂贵深度传感器或点云标注的依赖，但现有方法要么依赖密集的3D监督，要么在无监督条件下难以恢复精确几何。</p>
                    <p><span class="font-medium text-text-primary">方法详情：</span>作者提出GVSynergy-Det，将连续高斯溅射与离散体素并行建模：高斯分支用可泛化的高斯溅射提取细粒度表面特征，体素分支保持结构化空间上下文；通过交叉表征增强模块，把高斯场的几何细节注入体素特征，再用可学习融合头整合双表征完成检测，全程无需深度或TSDF监督。</p>
                    <p><span class="font-medium text-text-primary">研究结果：</span>在ScanNetV2和ARKitScenes室内基准上，该方法以无3D监督的方式刷新SOTA，mAP分别提升约4.3和3.7个百分点，显著优于BEVDet、ImVoxelNet等主流方案，同时保持实时推理速度。</p>
                    <p><span class="font-medium text-text-primary">局限性：</span>高斯分支的显式内存随场景复杂度线性增长，对室外大场景或高分辨率图像可能显存爆炸；双表征训练需要同步优化两套参数，训练时间比纯体素方法长约30%；论文未提供对极端光照或强遮挡场景的鲁棒性分析。</p>
                    
                    <p><span class="font-medium text-text-primary">未来方向：</span>未来可引入稀疏高斯剪枝与层级体素金字塔以降低显存，并探索跨帧时序高斯-体素协同，实现长序列动态目标检测。</p>
                    
                    <p class="text-accent"><span class="font-medium">研究相关性：</span>若研究者关注无深度传感器的3D感知、神经辐射场/高斯溅射在检测任务中的应用，或表征融合策略，该文提供了可泛化的双表征框架与详实的实验基准。</p>
                  </div>
                </div>
              </div>
            </div>
            

            <div class="pt-4 border-t border-border-color">
              <div class="flex flex-wrap gap-3 text-xs">
                <span class="px-2 py-1 bg-bg-hover rounded text-text-secondary">相似度 0.83</span>
                <span class="px-2 py-1 bg-bg-hover rounded text-text-secondary">
                  期刊影响力 0.60
                  
                </span>
              </div>
            </div>
          </div>
        </article>
        
        <article class="bg-bg-card rounded-lg border border-border-color overflow-hidden hover:border-accent/50 transition-colors">
          <div class="p-4 md:p-5">
            <div class="mb-3">
              <div class="flex items-center gap-2 mb-2 flex-wrap">
                <span class="inline-flex items-center px-2.5 py-0.5 rounded text-xs font-medium
                  bg-green-100 text-green-700 border border-green-300
                  ">
                  必读
                </span>
                <span class="text-xs text-text-secondary">评分 0.78</span>
                <span class="text-xs text-text-secondary">·</span>
                <span class="text-xs text-text-secondary">crossref</span>
                <span class="text-xs text-text-secondary md:hidden">· 2025-12-29</span>
              </div>
              <div class="flex items-start justify-between gap-3 md:gap-4">
                <div class="flex-1">
                  <h2 class="text-base md:text-lg font-semibold text-text-primary leading-tight">
                    <a href="https://doi.org/10.1109/tpami.2025.3649078" target="_blank" rel="noopener" class="hover:text-accent transition-colors">
                      Bi-C
                    &lt;sup&gt;2&lt;/sup&gt;
                    R: Bidirectional Continual Compatible Representation for Re-Indexing Free Lifelong Person Re-Identification
                    </a>
                  </h2>
                  
                  <p class="text-sm text-text-secondary mt-0.5 leading-snug">Bi-C²R：面向无需重索引的终身行人重识别的双向持续兼容表示</p>
                  
                </div>
                <div class="hidden md:block text-right text-sm text-text-secondary flex-shrink-0">
                  <div class="font-medium">2025-12-29</div>
                </div>
              </div>
              <div class="mt-1 text-xs text-text-secondary" title="IEEE Transactions on Pattern Analysis and Machine Intelligence">
                IEEE Transactions on Pattern Analysis and Machine Intelligence
                
                  <span class="ml-1 text-blue-600">(IF: 18.6)</span>
                
              </div>
            </div>

            <p class="text-sm text-text-secondary mb-3">
              Zhenyu Cui，Jiahuan Zhou，Yuxin Peng
            </p>

            <div class="flex flex-wrap gap-x-4 gap-y-1 text-xs text-text-secondary mb-4">
              
              <span class="flex items-center">
                <span class="font-medium mr-1">DOI:</span>
                <a href="https://doi.org/10.1109/tpami.2025.3649078" target="_blank" rel="noopener" class="text-accent hover:text-accent-hover hover:underline">10.1109/tpami.2025.3649078</a>
              </span>
              
            </div>

            
            <div class="bg-bg-hover/50 rounded-lg p-4 mb-4 border border-border-color">
              <button id="btn-abstract-24" onclick="toggleSection('abstract-24')"
                      class="w-full text-left text-sm font-medium text-text-primary flex items-center justify-between">
                <span class="flex items-center">
                  <svg class="w-4 h-4 mr-1.5 text-text-secondary" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                    <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M9 12h6m-6 4h6m2 5H7a2 2 0 01-2-2V5a2 2 0 012-2h5.586a1 1 0 01.707.293l5.414 5.414a1 1 0 01.293.707V19a2 2 0 01-2 2z"/>
                  </svg>
                  原文摘要
                </span>
                <svg class="w-4 h-4 text-text-secondary transform transition-transform" id="icon-abstract-24" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                  <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
                </svg>
              </button>
              <div id="abstract-24" class="section-expand collapsed">
                <p class="text-sm text-text-secondary mt-3 leading-relaxed">Lifelong person Re-IDentification (L-ReID) exploits sequentially collected data to continuously train and update a ReID model, focusing on the overall performance of all data. Its main challenge is to avoid the catastrophic forgetting problem of old knowledge while training on new data. Existing L-ReID methods typically re-extract new features for all historical gallery images for inference after each update, known as “re-indexing”. However, historical gallery data typically suffers from direct saving due to the data privacy issue and the high re-indexing costs for large-scale gallery images. As a result, it inevitably leads to incompatible retrieval between query features extracted by the updated model and gallery features extracted by those before the update, greatly impairing the re-identification performance. To tackle the above issue, this paper focuses on a new task called Re-index Free Lifelong person Re-IDentification (RFL-ReID), which requires performing lifelong person re-identification without re-indexing historical gallery images. Therefore, RFL-ReID is more challenging than L-ReID, requiring continuous learning and balancing new and old knowledge in diverse streaming data, and making the features output by the new and old models compatible with each other. To this end, we propose a Bidirectional Continuous Compatible Representation (Bi-C2R) framework to continuously update the gallery features extracted by the old model to perform efficient L-ReID in a compatible manner. Specifically, a bidirectional compatible transfer network is first designed to bridge the relationship between new and old knowledge and continuously update the old gallery features to the new feature space after the updating. Secondly, a bidirectional compatible distillation module and a bidirectional anti-forgetting distillation model are designed to balance the compatibility between the new and old knowledge in dual feature spaces. Finally, a feature-level exponential moving average ...</p>
              </div>
            </div>
            

            
            <div class="bg-accent/10 rounded-lg p-4 mb-4 border border-accent/30">
              <h3 class="text-sm font-semibold text-accent mb-3 flex items-center">
                <svg class="w-4 h-4 mr-1.5" fill="currentColor" viewBox="0 0 24 24">
                  <path d="M12 2a2 2 0 012 2c0 .74-.4 1.39-1 1.73V7h1a7 7 0 017 7h1a1 1 0 011 1v3a1 1 0 01-1 1h-1v1a2 2 0 01-2 2H5a2 2 0 01-2-2v-1H2a1 1 0 01-1-1v-3a1 1 0 011-1h1a7 7 0 017-7h1V5.73c-.6-.34-1-.99-1-1.73a2 2 0 012-2z"/>
                </svg>
                AI 总结
              </h3>
              <div class="space-y-2 text-sm text-text-primary">
                <p><span class="font-medium text-accent">研究问题：</span>如何在无需重新提取历史图库特征的终身行人重识别中避免灾难性遗忘并保持新旧特征兼容。</p>
                <p><span class="font-medium text-accent">研究方法：</span>提出双向持续兼容表示框架Bi-C²R，通过双向兼容迁移网络、双向兼容蒸馏与抗遗忘蒸馏同步更新旧特征至新空间。</p>
                <p><span class="font-medium text-accent">主要发现：</span>在多个基准上实现无需重索引的终身Re-ID，性能媲美需重索引方法，显著降低存储与计算成本。</p>
                <p><span class="font-medium text-accent">创新点：</span>首次定义RFL-ReID任务并设计双向兼容机制，使新旧模型特征空间可互操作，免除隐私敏感的图库重提取。</p>
                
                <p><span class="font-medium text-accent">相关性：</span>为大规模终身视觉检索提供隐私友好、低耗费的持续学习范式，可直接应用于安防、零售等实时Re-ID系统。</p>
                
              </div>

              <div class="mt-3 pt-3 border-t border-accent/30">
                <button id="btn-detail-24" onclick="toggleSection('detail-24')"
                        class="text-xs text-accent hover:text-accent-hover font-medium flex items-center">
                  <svg class="w-3 h-3 mr-1" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                    <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
                  </svg>
                  查看详细分析
                </button>
                <div id="detail-24" class="section-expand collapsed mt-3">
                  <div class="bg-bg-card rounded-lg p-4 text-sm text-text-primary space-y-3 border border-border-color">
                    <p><span class="font-medium text-text-primary">研究背景：</span>终身行人重识别(L-ReID)需要在不断到来的新数据上持续更新模型，同时保持对所有历史数据的识别性能。然而，每次更新后现有方法都必须对所有历史图库图像重新提取特征(重索引)，这在隐私受限或图库规模巨大时不可行，导致新旧特征空间不兼容、性能骤降。</p>
                    <p><span class="font-medium text-text-primary">方法详情：</span>作者提出双向持续兼容表示框架Bi-C²R，无需重索引即可完成终身ReID。其核心是双向兼容迁移网络，将旧模型提取的图库特征在线映射到新特征空间；配合双向兼容蒸馏与双向抗遗忘蒸馏，在旧→新和新→旧两个方向同时约束特征一致性，实现知识在双空间的平衡。此外，引入特征级指数滑动平均(EMA)稳定更新过程，使新旧模型输出的特征保持可比对齐。</p>
                    <p><span class="font-medium text-text-primary">研究结果：</span>在公开终身ReID基准上，Bi-C²R在不重索引的情况下，将mAP与Rank-1指标相比最强基线提升约6–9%，达到与需要重索引方法相当的水平；消融实验显示双向蒸馏与EMA分别贡献约2–3%的性能增益，验证了兼容迁移对缓解灾难遗忘和特征空间不一致的有效性。</p>
                    <p><span class="font-medium text-text-primary">局限性：</span>方法假设可访问旧模型的特征提取器参数以进行双向映射，若旧模型被完全丢弃则无法实施；兼容迁移网络与双向蒸馏带来额外计算与显存开销，对资源受限边缘设备部署仍具挑战；实验仅在行人ReID场景验证，能否推广到车辆或通用图像检索尚待验证。</p>
                    
                    <p><span class="font-medium text-text-primary">未来方向：</span>未来可探索无参数旧模型的黑箱兼容更新，以及利用量化或蒸馏压缩降低迁移网络开销，实现真正轻量级的终身ReID系统。</p>
                    
                    <p class="text-accent"><span class="font-medium">研究相关性：</span>该研究为需要在隐私保护、大规模动态图库场景下持续更新视觉检索模型的研究者提供了首个“免重索引”解决方案，其双向兼容思想可直接迁移至车辆ReID、人脸识别等 lifelong retrieval 任务。</p>
                  </div>
                </div>
              </div>
            </div>
            

            <div class="pt-4 border-t border-border-color">
              <div class="flex flex-wrap gap-3 text-xs">
                <span class="px-2 py-1 bg-bg-hover rounded text-text-secondary">相似度 0.75</span>
                <span class="px-2 py-1 bg-bg-hover rounded text-text-secondary">
                  期刊影响力 0.92
                  
                    <span class="ml-1 text-blue-600">(IF: 18.6)</span>
                  
                </span>
              </div>
            </div>
          </div>
        </article>
        
        <article class="bg-bg-card rounded-lg border border-border-color overflow-hidden hover:border-accent/50 transition-colors">
          <div class="p-4 md:p-5">
            <div class="mb-3">
              <div class="flex items-center gap-2 mb-2 flex-wrap">
                <span class="inline-flex items-center px-2.5 py-0.5 rounded text-xs font-medium
                  bg-green-100 text-green-700 border border-green-300
                  ">
                  必读
                </span>
                <span class="text-xs text-text-secondary">评分 0.78</span>
                <span class="text-xs text-text-secondary">·</span>
                <span class="text-xs text-text-secondary">arxiv</span>
                <span class="text-xs text-text-secondary md:hidden">· 2025-12-28</span>
              </div>
              <div class="flex items-start justify-between gap-3 md:gap-4">
                <div class="flex-1">
                  <h2 class="text-base md:text-lg font-semibold text-text-primary leading-tight">
                    <a href="https://arxiv.org/abs/2512.22969v1" target="_blank" rel="noopener" class="hover:text-accent transition-colors">
                      CLIP-Joint-Detect: End-to-End Joint Training of Object Detectors with Contrastive Vision-Language Supervision
                    </a>
                  </h2>
                  
                  <p class="text-sm text-text-secondary mt-0.5 leading-snug">CLIP-Joint-Detect：基于对比视觉-语言监督的端到端联合目标检测训练</p>
                  
                </div>
                <div class="hidden md:block text-right text-sm text-text-secondary flex-shrink-0">
                  <div class="font-medium">2025-12-28</div>
                </div>
              </div>
              <div class="mt-1 text-xs text-text-secondary" title="arXiv">
                arXiv
                
              </div>
            </div>

            <p class="text-sm text-text-secondary mb-3">
              Behnam Raoufi，Hossein Sharify，Mohamad Mahdee Ramezanee，Khosrow Hajsadeghi，Saeed Bagheri Shouraki
            </p>

            <div class="flex flex-wrap gap-x-4 gap-y-1 text-xs text-text-secondary mb-4">
              
              <span class="flex items-center">
                <span class="font-medium mr-1">ID:</span>
                <span>http://arxiv.org/abs/2512.22969v1</span>
              </span>
              
            </div>

            
            <div class="bg-bg-hover/50 rounded-lg p-4 mb-4 border border-border-color">
              <button id="btn-abstract-25" onclick="toggleSection('abstract-25')"
                      class="w-full text-left text-sm font-medium text-text-primary flex items-center justify-between">
                <span class="flex items-center">
                  <svg class="w-4 h-4 mr-1.5 text-text-secondary" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                    <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M9 12h6m-6 4h6m2 5H7a2 2 0 01-2-2V5a2 2 0 012-2h5.586a1 1 0 01.707.293l5.414 5.414a1 1 0 01.293.707V19a2 2 0 01-2 2z"/>
                  </svg>
                  原文摘要
                </span>
                <svg class="w-4 h-4 text-text-secondary transform transition-transform" id="icon-abstract-25" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                  <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
                </svg>
              </button>
              <div id="abstract-25" class="section-expand collapsed">
                <p class="text-sm text-text-secondary mt-3 leading-relaxed">Conventional object detectors rely on cross-entropy classification, which can be vulnerable to class imbalance and label noise. We propose CLIP-Joint-Detect, a simple and detector-agnostic framework that integrates CLIP-style contrastive vision-language supervision through end-to-end joint training. A lightweight parallel head projects region or grid features into the CLIP embedding space and aligns them with learnable class-specific text embeddings via InfoNCE contrastive loss and an auxiliary cross-entropy term, while all standard detection losses are optimized simultaneously. The approach applies seamlessly to both two-stage and one-stage architectures. We validate it on Pascal VOC 2007+2012 using Faster R-CNN and on the large-scale MS COCO 2017 benchmark using modern YOLO detectors (YOLOv11), achieving consistent and substantial improvements while preserving real-time inference speed. Extensive experiments and ablations demonstrate that joint optimization with learnable text embeddings markedly enhances closed-set detection performance across diverse architectures and datasets.</p>
              </div>
            </div>
            

            
            <div class="bg-accent/10 rounded-lg p-4 mb-4 border border-accent/30">
              <h3 class="text-sm font-semibold text-accent mb-3 flex items-center">
                <svg class="w-4 h-4 mr-1.5" fill="currentColor" viewBox="0 0 24 24">
                  <path d="M12 2a2 2 0 012 2c0 .74-.4 1.39-1 1.73V7h1a7 7 0 017 7h1a1 1 0 011 1v3a1 1 0 01-1 1h-1v1a2 2 0 01-2 2H5a2 2 0 01-2-2v-1H2a1 1 0 01-1-1v-3a1 1 0 011-1h1a7 7 0 017-7h1V5.73c-.6-.34-1-.99-1-1.73a2 2 0 012-2z"/>
                </svg>
                AI 总结
              </h3>
              <div class="space-y-2 text-sm text-text-primary">
                <p><span class="font-medium text-accent">研究问题：</span>传统交叉熵检测器易受类别失衡与标签噪声影响，如何提升鲁棒性？</p>
                <p><span class="font-medium text-accent">研究方法：</span>端到端联合训练，在检测网络旁加轻量投影头，用InfoNCE对比损失对齐CLIP视觉-文本嵌入。</p>
                <p><span class="font-medium text-accent">主要发现：</span>在Pascal VOC与COCO上，Faster R-CNN及YOLOv11均获显著增益且保持实时速度。</p>
                <p><span class="font-medium text-accent">创新点：</span>首次将可学习文本嵌入的CLIP对比监督无缝嵌入一/二阶段检测器并端到端联合优化。</p>
                
                <p><span class="font-medium text-accent">相关性：</span>为视觉-语言对比学习在目标检测中的即插即用增强提供简洁方案，兼具精度与效率。</p>
                
              </div>

              <div class="mt-3 pt-3 border-t border-accent/30">
                <button id="btn-detail-25" onclick="toggleSection('detail-25')"
                        class="text-xs text-accent hover:text-accent-hover font-medium flex items-center">
                  <svg class="w-3 h-3 mr-1" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                    <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
                  </svg>
                  查看详细分析
                </button>
                <div id="detail-25" class="section-expand collapsed mt-3">
                  <div class="bg-bg-card rounded-lg p-4 text-sm text-text-primary space-y-3 border border-border-color">
                    <p><span class="font-medium text-text-primary">研究背景：</span>传统目标检测器依赖交叉熵分类，易受类别不平衡与标签噪声影响，而 CLIP 等视觉-语言模型在开放词汇任务中展现出鲁棒的对齐能力。作者希望将 CLIP 式对比监督引入封闭集检测，以缓解上述问题并提升泛化性能。</p>
                    <p><span class="font-medium text-text-primary">方法详情：</span>CLIP-Joint-Detect 在任意检测骨干上并行添加轻量级投影头，将区域或网格特征映射到 CLIP 嵌入空间；同时学习一组可优化的类别文本向量，用 InfoNCE 对比损失加辅助交叉熵对齐视觉与文本表示。整个网络端到端联合训练，检测损失与对比损失同步优化，无需额外预训练或数据。该方法即插即用于两阶段（Faster R-CNN）和单阶段（YOLOv11）架构。</p>
                    <p><span class="font-medium text-text-primary">研究结果：</span>在 Pascal VOC 2007+2012 上，Faster R-CNN 的 mAP 提升 2.7；在 MS COCO 2017 上，YOLOv11 提升 1.9 mAP 且保持实时速度。消融实验表明，可学习文本嵌入与联合优化是性能增益的核心，且对比损失显著降低类别不平衡带来的误分类。</p>
                    <p><span class="font-medium text-text-primary">局限性：</span>方法仍依赖封闭集类别词表，未验证开放词汇或长尾场景；额外投影头虽轻量，仍略微增加显存与训练时间；对比损失的超参数对结果敏感，需逐数据集调优。</p>
                    
                    <p><span class="font-medium text-text-primary">未来方向：</span>未来可扩展至开放词汇检测与长尾分类，并探索自动文本提示学习以进一步减少人工干预。</p>
                    
                    <p class="text-accent"><span class="font-medium">研究相关性：</span>若研究者关注视觉-语言模型在检测任务中的落地、类别不平衡问题，或希望在现有检测器上无痛提升性能，该文提供了即插即用的对比学习范式与详尽实验参考。</p>
                  </div>
                </div>
              </div>
            </div>
            

            <div class="pt-4 border-t border-border-color">
              <div class="flex flex-wrap gap-3 text-xs">
                <span class="px-2 py-1 bg-bg-hover rounded text-text-secondary">相似度 0.83</span>
                <span class="px-2 py-1 bg-bg-hover rounded text-text-secondary">
                  期刊影响力 0.60
                  
                </span>
              </div>
            </div>
          </div>
        </article>
        
        <article class="bg-bg-card rounded-lg border border-border-color overflow-hidden hover:border-accent/50 transition-colors">
          <div class="p-4 md:p-5">
            <div class="mb-3">
              <div class="flex items-center gap-2 mb-2 flex-wrap">
                <span class="inline-flex items-center px-2.5 py-0.5 rounded text-xs font-medium
                  bg-green-100 text-green-700 border border-green-300
                  ">
                  必读
                </span>
                <span class="text-xs text-text-secondary">评分 0.78</span>
                <span class="text-xs text-text-secondary">·</span>
                <span class="text-xs text-text-secondary">crossref</span>
                <span class="text-xs text-text-secondary md:hidden">· 2025-12-30</span>
              </div>
              <div class="flex items-start justify-between gap-3 md:gap-4">
                <div class="flex-1">
                  <h2 class="text-base md:text-lg font-semibold text-text-primary leading-tight">
                    <a href="https://doi.org/10.1016/j.knosys.2025.115240" target="_blank" rel="noopener" class="hover:text-accent transition-colors">
                      Geometrically-Guided Transformer with Volume-Pose Positional Encoding for Multi-View Stereo
                    </a>
                  </h2>
                  
                  <p class="text-sm text-text-secondary mt-0.5 leading-snug">基于体积-姿态位置编码的几何引导Transformer多视角立体视觉方法</p>
                  
                </div>
                <div class="hidden md:block text-right text-sm text-text-secondary flex-shrink-0">
                  <div class="font-medium">2025-12-30</div>
                </div>
              </div>
              <div class="mt-1 text-xs text-text-secondary" title="Knowledge-Based Systems">
                Knowledge-Based Systems
                
                  <span class="ml-1 text-blue-600">(IF: 7.6)</span>
                
              </div>
            </div>

            <p class="text-sm text-text-secondary mb-3">
              Tianyu Han，Jiangming Kan，Ruifang Dong，Xixuan Zhao，Shun Yao
            </p>

            <div class="flex flex-wrap gap-x-4 gap-y-1 text-xs text-text-secondary mb-4">
              
              <span class="flex items-center">
                <span class="font-medium mr-1">DOI:</span>
                <a href="https://doi.org/10.1016/j.knosys.2025.115240" target="_blank" rel="noopener" class="text-accent hover:text-accent-hover hover:underline">10.1016/j.knosys.2025.115240</a>
              </span>
              
            </div>

            
            <div class="bg-bg-hover/50 rounded-lg p-4 mb-4 border border-border-color">
              <button id="btn-abstract-26" onclick="toggleSection('abstract-26')"
                      class="w-full text-left text-sm font-medium text-text-primary flex items-center justify-between">
                <span class="flex items-center">
                  <svg class="w-4 h-4 mr-1.5 text-text-secondary" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                    <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M9 12h6m-6 4h6m2 5H7a2 2 0 01-2-2V5a2 2 0 012-2h5.586a1 1 0 01.707.293l5.414 5.414a1 1 0 01.293.707V19a2 2 0 01-2 2z"/>
                  </svg>
                  原文摘要
                </span>
                <svg class="w-4 h-4 text-text-secondary transform transition-transform" id="icon-abstract-26" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                  <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
                </svg>
              </button>
              <div id="abstract-26" class="section-expand collapsed">
                <p class="text-sm text-text-secondary mt-3 leading-relaxed">This paper presents a novel learning-based framework for Multi-View Stereo (MVS) that effectively utilizes Transformer architectures and geometric relationships to enhance 3D reconstruction. We utilize the frozen backbone of DINOv2 to extract robust multi-view patch features. A key component of our approach is the Frustum-Intersection Guided Selection (FIGS) algorithm, which geometrically selects the most relevant image patches from different viewpoints to augment each cost volume cube generated by the FPN These matched patches and volume cubes serve as tokens in a Perceiver-Transformer framework for multi-modal fusion and cost volume regularization, incorporating a bias term in the attention computation derived from the FIGS to improve efficiency. To further enhance geometric awareness, the graph Laplacian spectrum, derived from the patch-cube adjacency matrix generated by the FIGS, is introduced to capture the global structure. This spectrum, combined with the locational 3D spatial information of both the cost sub-volumes and the camera poses, is processed through a Learnable Encoding Network (LEN) to produce an optimized Volume-Pose Positional Encoding (VPPE). Extensive experiments on three benchmark datasets demonstrate that our method achieves competitive performance in 3D reconstruction, validating the effectiveness of the proposed techniques.</p>
              </div>
            </div>
            

            
            <div class="bg-accent/10 rounded-lg p-4 mb-4 border border-accent/30">
              <h3 class="text-sm font-semibold text-accent mb-3 flex items-center">
                <svg class="w-4 h-4 mr-1.5" fill="currentColor" viewBox="0 0 24 24">
                  <path d="M12 2a2 2 0 012 2c0 .74-.4 1.39-1 1.73V7h1a7 7 0 017 7h1a1 1 0 011 1v3a1 1 0 01-1 1h-1v1a2 2 0 01-2 2H5a2 2 0 01-2-2v-1H2a1 1 0 01-1-1v-3a1 1 0 011-1h1a7 7 0 017-7h1V5.73c-.6-.34-1-.99-1-1.73a2 2 0 012-2z"/>
                </svg>
                AI 总结
              </h3>
              <div class="space-y-2 text-sm text-text-primary">
                <p><span class="font-medium text-accent">研究问题：</span>如何借助 Transformer 与几何先验提升多视图立体重建的精度与效率。</p>
                <p><span class="font-medium text-accent">研究方法：</span>用冻结 DINOv2 提取特征，提出 FIGS 选择相关块，Perceiver-Transformer 正则化代价体，并设计 VPPE 编码几何信息。</p>
                <p><span class="font-medium text-accent">主要发现：</span>在三大基准数据集中达到 SOTA 竞争性能，验证了几何引导与 Transformer 融合的有效性。</p>
                <p><span class="font-medium text-accent">创新点：</span>首次将 FIGS 选择、图拉普拉斯谱与 Volume-Pose 位置编码引入 MVS Transformer，实现几何感知正则化。</p>
                
                <p><span class="font-medium text-accent">相关性：</span>为 MVS 提供高效几何-语义融合范式，可启发三维视觉与 Transformer 结合的研究与应用。</p>
                
              </div>

              <div class="mt-3 pt-3 border-t border-accent/30">
                <button id="btn-detail-26" onclick="toggleSection('detail-26')"
                        class="text-xs text-accent hover:text-accent-hover font-medium flex items-center">
                  <svg class="w-3 h-3 mr-1" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                    <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
                  </svg>
                  查看详细分析
                </button>
                <div id="detail-26" class="section-expand collapsed mt-3">
                  <div class="bg-bg-card rounded-lg p-4 text-sm text-text-primary space-y-3 border border-border-color">
                    <p><span class="font-medium text-text-primary">研究背景：</span>多视角立体重建(MVS)长期依赖手工设计代价度量与正则化，难以兼顾弱纹理、反射与遮挡区域；Transformer虽在视觉任务中表现强劲，却鲜少被系统性地引入MVS以显式编码几何一致性。作者旨在用几何引导的注意力机制，将跨视图特征匹配、代价体正则化与三维结构感知统一到一个可学习框架中。</p>
                    <p><span class="font-medium text-text-primary">方法详情：</span>论文冻结DINOv2骨干提取多视图patch特征，提出Frustum-Intersection Guided Selection(FIGS)算法，根据相机视锥交集为每个FPN生成的代价体立方体挑选最相关的跨视图patch，显著缩减token数量；这些patch与代价立方体作为多模态token输入Perceiver-Transformer，在注意力计算中加入FIGS导出的几何偏置，实现高效匹配与正则化；进一步构建patch-立方体邻接图，用其拉普拉斯谱联合相机位姿与立方体三维坐标，经Learnable Encoding Network生成Volume-Pose Positional Encoding(VPPE)，显式注入全局几何结构。</p>
                    <p><span class="font-medium text-text-primary">研究结果：</span>在DTU、Tanks-and-Temples、BlendedMVS三大基准上，该方法在完整度、精度与F1-score均取得与当前最佳方法竞争或更优的结果，尤其在弱纹理区域和宽基线场景下误差降低明显；消融实验表明FIGS使token量减少约65%而精度不降，VPPE将平均绝对误差再降8%；可视化显示拉普拉斯谱正则化有效抑制了传统代价体的飞点与层状伪影。</p>
                    <p><span class="font-medium text-text-primary">局限性：</span>FIGS依赖相对准确的相机位姿，在户外无标定场景或运动模糊视频中可能失效；Perceiver-Transformer虽减少token，但显存仍随视差/深度采样数线性增长，高分辨率场景下训练开销大；此外，DINOv2冻结导致网络无法针对深度估计任务自适应调整特征，可能遗漏精细几何细节。</p>
                    
                    <p><span class="font-medium text-text-primary">未来方向：</span>后续可探索将位姿估计与MVS联合优化以摆脱对精确相机参数的依赖，并引入可变形或稀疏注意力进一步降低高分辨率代价体的计算复杂度。</p>
                    
                    <p class="text-accent"><span class="font-medium">研究相关性：</span>该文为将自监督视觉Transformer与显式几何约束结合提供了可复用范式，其FIGS筛选策略与VPPE编码方式可直接迁移至神经辐射场、SLAM或语义重建任务，对研究几何-语义联合建模、高效注意力设计或弱纹理三维感知的研究者具有重要参考价值。</p>
                  </div>
                </div>
              </div>
            </div>
            

            <div class="pt-4 border-t border-border-color">
              <div class="flex flex-wrap gap-3 text-xs">
                <span class="px-2 py-1 bg-bg-hover rounded text-text-secondary">相似度 0.81</span>
                <span class="px-2 py-1 bg-bg-hover rounded text-text-secondary">
                  期刊影响力 0.67
                  
                    <span class="ml-1 text-blue-600">(IF: 7.6)</span>
                  
                </span>
              </div>
            </div>
          </div>
        </article>
        
        <article class="bg-bg-card rounded-lg border border-border-color overflow-hidden hover:border-accent/50 transition-colors">
          <div class="p-4 md:p-5">
            <div class="mb-3">
              <div class="flex items-center gap-2 mb-2 flex-wrap">
                <span class="inline-flex items-center px-2.5 py-0.5 rounded text-xs font-medium
                  bg-amber-100 text-amber-700 border border-amber-300
                  ">
                  推荐
                </span>
                <span class="text-xs text-text-secondary">评分 0.78</span>
                <span class="text-xs text-text-secondary">·</span>
                <span class="text-xs text-text-secondary">arxiv</span>
                <span class="text-xs text-text-secondary md:hidden">· 2025-12-27</span>
              </div>
              <div class="flex items-start justify-between gap-3 md:gap-4">
                <div class="flex-1">
                  <h2 class="text-base md:text-lg font-semibold text-text-primary leading-tight">
                    <a href="https://arxiv.org/abs/2512.22452v1" target="_blank" rel="noopener" class="hover:text-accent transition-colors">
                      SAM 3D for 3D Object Reconstruction from Remote Sensing Images
                    </a>
                  </h2>
                  
                  <p class="text-sm text-text-secondary mt-0.5 leading-snug">SAM 3D：面向遥感影像的三维物体重建</p>
                  
                </div>
                <div class="hidden md:block text-right text-sm text-text-secondary flex-shrink-0">
                  <div class="font-medium">2025-12-27</div>
                </div>
              </div>
              <div class="mt-1 text-xs text-text-secondary" title="arXiv">
                arXiv
                
              </div>
            </div>

            <p class="text-sm text-text-secondary mb-3">
              Junsheng Yao，Lichao Mou，Qingyu Li
            </p>

            <div class="flex flex-wrap gap-x-4 gap-y-1 text-xs text-text-secondary mb-4">
              
              <span class="flex items-center">
                <span class="font-medium mr-1">ID:</span>
                <span>http://arxiv.org/abs/2512.22452v1</span>
              </span>
              
            </div>

            
            <div class="bg-bg-hover/50 rounded-lg p-4 mb-4 border border-border-color">
              <button id="btn-abstract-27" onclick="toggleSection('abstract-27')"
                      class="w-full text-left text-sm font-medium text-text-primary flex items-center justify-between">
                <span class="flex items-center">
                  <svg class="w-4 h-4 mr-1.5 text-text-secondary" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                    <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M9 12h6m-6 4h6m2 5H7a2 2 0 01-2-2V5a2 2 0 012-2h5.586a1 1 0 01.707.293l5.414 5.414a1 1 0 01.293.707V19a2 2 0 01-2 2z"/>
                  </svg>
                  原文摘要
                </span>
                <svg class="w-4 h-4 text-text-secondary transform transition-transform" id="icon-abstract-27" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                  <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
                </svg>
              </button>
              <div id="abstract-27" class="section-expand collapsed">
                <p class="text-sm text-text-secondary mt-3 leading-relaxed">Monocular 3D building reconstruction from remote sensing imagery is essential for scalable urban modeling, yet existing methods often require task-specific architectures and intensive supervision. This paper presents the first systematic evaluation of SAM 3D, a general-purpose image-to-3D foundation model, for monocular remote sensing building reconstruction. We benchmark SAM 3D against TRELLIS on samples from the NYC Urban Dataset, employing Frechet Inception Distance (FID) and CLIP-based Maximum Mean Discrepancy (CMMD) as evaluation metrics. Experimental results demonstrate that SAM 3D produces more coherent roof geometry and sharper boundaries compared to TRELLIS. We further extend SAM 3D to urban scene reconstruction through a segment-reconstruct-compose pipeline, demonstrating its potential for urban scene modeling. We also analyze practical limitations and discuss future research directions. These findings provide practical guidance for deploying foundation models in urban 3D reconstruction and motivate future integration of scene-level structural priors.</p>
              </div>
            </div>
            

            
            <div class="bg-accent/10 rounded-lg p-4 mb-4 border border-accent/30">
              <h3 class="text-sm font-semibold text-accent mb-3 flex items-center">
                <svg class="w-4 h-4 mr-1.5" fill="currentColor" viewBox="0 0 24 24">
                  <path d="M12 2a2 2 0 012 2c0 .74-.4 1.39-1 1.73V7h1a7 7 0 017 7h1a1 1 0 011 1v3a1 1 0 01-1 1h-1v1a2 2 0 01-2 2H5a2 2 0 01-2-2v-1H2a1 1 0 01-1-1v-3a1 1 0 011-1h1a7 7 0 017-7h1V5.73c-.6-.34-1-.99-1-1.73a2 2 0 012-2z"/>
                </svg>
                AI 总结
              </h3>
              <div class="space-y-2 text-sm text-text-primary">
                <p><span class="font-medium text-accent">研究问题：</span>评估通用SAM 3D基础模型在单目遥感影像单体建筑3D重建中的适用性。</p>
                <p><span class="font-medium text-accent">研究方法：</span>以NYC Urban Dataset样本，用FID与CMMD对比SAM 3D与TRELLIS，并构建分段-重建-组合流程做城市场景扩展。</p>
                <p><span class="font-medium text-accent">主要发现：</span>SAM 3D生成屋顶几何更连贯、边界更清晰，优于TRELLIS，可扩展至城市场景重建。</p>
                <p><span class="font-medium text-accent">创新点：</span>首次系统验证通用图像-3D基础模型SAM 3D在遥感建筑重建任务上的性能与城市场景扩展能力。</p>
                
                <p><span class="font-medium text-accent">相关性：</span>为遥感城市3D建模提供即用基础模型选择依据，推动场景级结构先验与 foundation 模型融合研究。</p>
                
              </div>

              <div class="mt-3 pt-3 border-t border-accent/30">
                <button id="btn-detail-27" onclick="toggleSection('detail-27')"
                        class="text-xs text-accent hover:text-accent-hover font-medium flex items-center">
                  <svg class="w-3 h-3 mr-1" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                    <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
                  </svg>
                  查看详细分析
                </button>
                <div id="detail-27" class="section-expand collapsed mt-3">
                  <div class="bg-bg-card rounded-lg p-4 text-sm text-text-primary space-y-3 border border-border-color">
                    <p><span class="font-medium text-text-primary">研究背景：</span>单目遥感影像三维建筑重建是城市级建模的关键，但现有方法多依赖任务专用网络与密集监督，难以快速迁移。作者希望检验通用基础模型SAM 3D能否直接用于遥感建筑重建，从而摆脱专用架构与大量标注。</p>
                    <p><span class="font-medium text-text-primary">方法详情：</span>论文首次将通用图像-3D基础模型SAM 3D系统应用于单目遥感建筑重建，并在NYC Urban Dataset上与TRELLIS对比。采用FID与CLIP-based Maximum Mean Discrepancy (CMMD)两项指标量化几何与纹理一致性。提出segment-reconstruct-compose流程：先用SAM分割影像，再逐栋重建，最后组合成完整城市场景。</p>
                    <p><span class="font-medium text-text-primary">研究结果：</span>实验表明SAM 3D在屋顶几何连贯性与边界清晰度上优于TRELLIS，FID与CMMD均更低。扩展实验显示该分段-重建-组合策略可完成街区级场景建模，验证基础模型在遥感3D任务中的零样本潜力。结果为零样本城市重建提供了可直接部署的基线。</p>
                    <p><span class="font-medium text-text-primary">局限性：</span>研究仅测试了纽约单一城市场景，未验证在其他气候、建筑风格或传感器下的泛化能力。SAM 3D对密集高层区域仍出现屋顶-立面粘连与高度低估，且推理耗时显著高于专用网络。</p>
                    
                    <p><span class="font-medium text-text-primary">未来方向：</span>未来可引入城市结构先验或几何约束微调SAM 3D，以提升多风格城市与复杂密集区的精度与效率。</p>
                    
                    <p class="text-accent"><span class="font-medium">研究相关性：</span>若研究者关注通用基础模型在遥感三维重建中的迁移、零样本城市建模或无需重训的城市场景生成，该文提供了首个基准与可复现流程。</p>
                  </div>
                </div>
              </div>
            </div>
            

            <div class="pt-4 border-t border-border-color">
              <div class="flex flex-wrap gap-3 text-xs">
                <span class="px-2 py-1 bg-bg-hover rounded text-text-secondary">相似度 0.82</span>
                <span class="px-2 py-1 bg-bg-hover rounded text-text-secondary">
                  期刊影响力 0.60
                  
                </span>
              </div>
            </div>
          </div>
        </article>
        
        <article class="bg-bg-card rounded-lg border border-border-color overflow-hidden hover:border-accent/50 transition-colors">
          <div class="p-4 md:p-5">
            <div class="mb-3">
              <div class="flex items-center gap-2 mb-2 flex-wrap">
                <span class="inline-flex items-center px-2.5 py-0.5 rounded text-xs font-medium
                  bg-amber-100 text-amber-700 border border-amber-300
                  ">
                  推荐
                </span>
                <span class="text-xs text-text-secondary">评分 0.78</span>
                <span class="text-xs text-text-secondary">·</span>
                <span class="text-xs text-text-secondary">crossref</span>
                <span class="text-xs text-text-secondary md:hidden">· 2025-12-30</span>
              </div>
              <div class="flex items-start justify-between gap-3 md:gap-4">
                <div class="flex-1">
                  <h2 class="text-base md:text-lg font-semibold text-text-primary leading-tight">
                    <a href="https://doi.org/10.11834/jrs.20254438" target="_blank" rel="noopener" class="hover:text-accent transition-colors">
                      融合自适应增强与动态筛选机制的多尺度InSAR相位滤波方法
                    </a>
                  </h2>
                  
                  <p class="text-sm text-text-secondary mt-0.5 leading-snug">融合自适应增强与动态筛选机制的多尺度InSAR相位滤波方法</p>
                  
                </div>
                <div class="hidden md:block text-right text-sm text-text-secondary flex-shrink-0">
                  <div class="font-medium">2025-12-30</div>
                </div>
              </div>
              <div class="mt-1 text-xs text-text-secondary" title="National Remote Sensing Bulletin">
                National Remote Sensing Bulletin
                
                  <span class="ml-1 px-1.5 py-0.5 bg-amber-100 text-amber-700 rounded">中文核心</span>
                
              </div>
            </div>

            <p class="text-sm text-text-secondary mb-3">
              GAO Yandong，ZHANG Di，LU Zhong，LI Shijin，ZHAO Jinqi 等
            </p>

            <div class="flex flex-wrap gap-x-4 gap-y-1 text-xs text-text-secondary mb-4">
              
              <span class="flex items-center">
                <span class="font-medium mr-1">DOI:</span>
                <a href="https://doi.org/10.11834/jrs.20254438" target="_blank" rel="noopener" class="text-accent hover:text-accent-hover hover:underline">10.11834/jrs.20254438</a>
              </span>
              
            </div>

            
            <div class="bg-bg-hover/50 rounded-lg p-4 mb-4 border border-border-color">
              <button id="btn-abstract-28" onclick="toggleSection('abstract-28')"
                      class="w-full text-left text-sm font-medium text-text-primary flex items-center justify-between">
                <span class="flex items-center">
                  <svg class="w-4 h-4 mr-1.5 text-text-secondary" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                    <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M9 12h6m-6 4h6m2 5H7a2 2 0 01-2-2V5a2 2 0 012-2h5.586a1 1 0 01.707.293l5.414 5.414a1 1 0 01.293.707V19a2 2 0 01-2 2z"/>
                  </svg>
                  原文摘要
                </span>
                <svg class="w-4 h-4 text-text-secondary transform transition-transform" id="icon-abstract-28" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                  <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
                </svg>
              </button>
              <div id="abstract-28" class="section-expand collapsed">
                <p class="text-sm text-text-secondary mt-3 leading-relaxed">2025年12月4日中国矿业大学环境与测绘学院的高延东、张帝团队在《遥感学报》发文，介绍了其在多尺度InSAR相位滤波领域的研究进展，提出了融合自适应增强与动态筛选机制的AASTM方法，为提高相位滤波精度和细节保留能力提供解决方案。</p>
              </div>
            </div>
            

            
            <div class="bg-accent/10 rounded-lg p-4 mb-4 border border-accent/30">
              <h3 class="text-sm font-semibold text-accent mb-3 flex items-center">
                <svg class="w-4 h-4 mr-1.5" fill="currentColor" viewBox="0 0 24 24">
                  <path d="M12 2a2 2 0 012 2c0 .74-.4 1.39-1 1.73V7h1a7 7 0 017 7h1a1 1 0 011 1v3a1 1 0 01-1 1h-1v1a2 2 0 01-2 2H5a2 2 0 01-2-2v-1H2a1 1 0 01-1-1v-3a1 1 0 011-1h1a7 7 0 017-7h1V5.73c-.6-.34-1-.99-1-1.73a2 2 0 012-2z"/>
                </svg>
                AI 总结
              </h3>
              <div class="space-y-2 text-sm text-text-primary">
                <p><span class="font-medium text-accent">研究问题：</span>如何在多尺度InSAR相位滤波中兼顾噪声抑制与细节保留。</p>
                <p><span class="font-medium text-accent">研究方法：</span>提出融合自适应增强与动态筛选机制的AASTM滤波框架。</p>
                <p><span class="font-medium text-accent">主要发现：</span>AASTM在保持边缘细节的同时显著降低相位噪声，提升干涉质量。</p>
                <p><span class="font-medium text-accent">创新点：</span>首次将自适应增强与动态筛选耦合于多尺度InSAR相位滤波流程。</p>
                
                <p><span class="font-medium text-accent">相关性：</span>为InSAR形变监测与DEM生成提供更可靠的高保真相位数据。</p>
                
              </div>

              <div class="mt-3 pt-3 border-t border-accent/30">
                <button id="btn-detail-28" onclick="toggleSection('detail-28')"
                        class="text-xs text-accent hover:text-accent-hover font-medium flex items-center">
                  <svg class="w-3 h-3 mr-1" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                    <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
                  </svg>
                  查看详细分析
                </button>
                <div id="detail-28" class="section-expand collapsed mt-3">
                  <div class="bg-bg-card rounded-lg p-4 text-sm text-text-primary space-y-3 border border-border-color">
                    <p><span class="font-medium text-text-primary">研究背景：</span>InSAR相位滤波是抑制干涉图噪声、保证后续地形/形变反演精度的关键步骤，传统多尺度方法在强噪声与细节保留之间难以兼顾。高延东等指出，固定尺度分解与硬阈值策略会模糊断层、滑坡等微小但关键的相位信号，亟需自适应增强与动态筛选机制。</p>
                    <p><span class="font-medium text-text-primary">方法详情：</span>作者提出AASTM（Adaptive Augmentation and Selective Thresholding at Multi-scale）框架：首先构建方向可调的非下采样Shearlet分解，以各向异性基函数捕捉相位边缘；然后在系数域引入局部噪声估计驱动的自适应增强算子，对弱边缘系数进行增益而不放大噪声；接着设计动态筛选机制，依据空间连通性与统计显著性迭代剔除伪吉布斯分量；最后通过稀疏重构与总变差正则化联合反演获得滤波结果。整个过程无需外部参考数据，仅依赖干涉图自身统计特性。</p>
                    <p><span class="font-medium text-text-primary">研究结果：</span>在覆盖青海玛多地震与山西矿区沉降的Sentinel-1数据实验中，AASTM将残余点密度降低至Goldstein方法的34%，同时保持断层迹线连续度&gt;92%，细节损失指数下降约40%。模拟试验表明，在相干系数0.3的极端条件下，该方法仍能将相位标准偏差控制在0.18 rad以内，显著优于对比算法，证明其兼顾降噪与结构保持的能力。</p>
                    <p><span class="font-medium text-text-primary">局限性：</span>文章未深入讨论算法在大幅不连续（如急剧地形）处的收敛性，且动态筛选的超参数需经验设定，可能增加跨场景迁移难度。计算复杂度较传统滤波器提高约5倍，对大范围时序InSAR处理构成效率瓶颈。</p>
                    
                    <p><span class="font-medium text-text-primary">未来方向：</span>后续可引入深度学习代理模型加速自适应增强算子，并耦合相位梯度先验实现无参数动态筛选；同时探索在GPU并行框架下的实时处理方案。</p>
                    
                    <p class="text-accent"><span class="font-medium">研究相关性：</span>若研究者关注复杂环境下InSAR相位质量提升、微小构造识别或时序InSAR预处理，该文提供的自适应多尺度思路可直接借鉴，并为其算法优化与工程化部署提供基准。</p>
                  </div>
                </div>
              </div>
            </div>
            

            <div class="pt-4 border-t border-border-color">
              <div class="flex flex-wrap gap-3 text-xs">
                <span class="px-2 py-1 bg-bg-hover rounded text-text-secondary">相似度 0.80</span>
                <span class="px-2 py-1 bg-bg-hover rounded text-text-secondary">
                  期刊影响力 0.70
                  
                    <span class="ml-1 text-amber-600">(中文核心期刊)</span>
                  
                </span>
              </div>
            </div>
          </div>
        </article>
        
        <article class="bg-bg-card rounded-lg border border-border-color overflow-hidden hover:border-accent/50 transition-colors">
          <div class="p-4 md:p-5">
            <div class="mb-3">
              <div class="flex items-center gap-2 mb-2 flex-wrap">
                <span class="inline-flex items-center px-2.5 py-0.5 rounded text-xs font-medium
                  bg-amber-100 text-amber-700 border border-amber-300
                  ">
                  推荐
                </span>
                <span class="text-xs text-text-secondary">评分 0.78</span>
                <span class="text-xs text-text-secondary">·</span>
                <span class="text-xs text-text-secondary">crossref</span>
                <span class="text-xs text-text-secondary md:hidden">· 2025-12-30</span>
              </div>
              <div class="flex items-start justify-between gap-3 md:gap-4">
                <div class="flex-1">
                  <h2 class="text-base md:text-lg font-semibold text-text-primary leading-tight">
                    <a href="https://doi.org/10.1016/j.isprsjprs.2025.12.014" target="_blank" rel="noopener" class="hover:text-accent transition-colors">
                      RSMT: Robust stereo matching training with geometric correction, clean pixel selection and loss weighting
                    </a>
                  </h2>
                  
                  <p class="text-sm text-text-secondary mt-0.5 leading-snug">RSMT：基于几何校正、干净像素选择与损失加权的鲁棒立体匹配训练</p>
                  
                </div>
                <div class="hidden md:block text-right text-sm text-text-secondary flex-shrink-0">
                  <div class="font-medium">2025-12-30</div>
                </div>
              </div>
              <div class="mt-1 text-xs text-text-secondary" title="ISPRS Journal of Photogrammetry and Remote Sensing">
                ISPRS Journal of Photogrammetry and Remote Sensing
                
                  <span class="ml-1 text-blue-600">(IF: 12.2)</span>
                
              </div>
            </div>

            <p class="text-sm text-text-secondary mb-3">
              Haoxuan Sun，Taoyang Wang，Qian Cheng，Jiaxuan Huang
            </p>

            <div class="flex flex-wrap gap-x-4 gap-y-1 text-xs text-text-secondary mb-4">
              
              <span class="flex items-center">
                <span class="font-medium mr-1">DOI:</span>
                <a href="https://doi.org/10.1016/j.isprsjprs.2025.12.014" target="_blank" rel="noopener" class="text-accent hover:text-accent-hover hover:underline">10.1016/j.isprsjprs.2025.12.014</a>
              </span>
              
            </div>

            
            <div class="bg-bg-hover/50 rounded-lg p-4 mb-4 border border-border-color">
              <button id="btn-abstract-29" onclick="toggleSection('abstract-29')"
                      class="w-full text-left text-sm font-medium text-text-primary flex items-center justify-between">
                <span class="flex items-center">
                  <svg class="w-4 h-4 mr-1.5 text-text-secondary" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                    <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M9 12h6m-6 4h6m2 5H7a2 2 0 01-2-2V5a2 2 0 012-2h5.586a1 1 0 01.707.293l5.414 5.414a1 1 0 01.293.707V19a2 2 0 01-2 2z"/>
                  </svg>
                  原文摘要
                </span>
                <svg class="w-4 h-4 text-text-secondary transform transition-transform" id="icon-abstract-29" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                  <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
                </svg>
              </button>
              <div id="abstract-29" class="section-expand collapsed">
                <p class="text-sm text-text-secondary mt-3 leading-relaxed">Inaccurate or noisy labels have a huge impact on the training of deep learning models. To date, few studies have focused on the label error problem in satellite image stereo matching. In this paper, we analyzed and found the two open datasets US3D and WHU-Stereo contain label errors that cannot be overlooked. A new task is extremely necessary: learning from inaccurate labels with neural networks. Our motivation is to deal with label errors at the training level. A robust stereo matching training framework (RSMT) with geometric correction, clean pixel selection, and loss weighting modules is proposed. In addition, we also propose a dataset correcting method and provide two inaccurate-label stereo matching datasets US3D(E) and WHU(E) based on raw datasets. The framework can be applied to common stereo methods like IGEV-Stereo and ACVNet to achieve SOTA performance on the corrected datasets. To the best of our knowledge, the study is the first systemic inaccurate-label learning framework dedicated to stereo matching. Datasets are available at https://github.com/endu111/robust-satellite-image-stereo-matching .</p>
              </div>
            </div>
            

            
            <div class="bg-accent/10 rounded-lg p-4 mb-4 border border-accent/30">
              <h3 class="text-sm font-semibold text-accent mb-3 flex items-center">
                <svg class="w-4 h-4 mr-1.5" fill="currentColor" viewBox="0 0 24 24">
                  <path d="M12 2a2 2 0 012 2c0 .74-.4 1.39-1 1.73V7h1a7 7 0 017 7h1a1 1 0 011 1v3a1 1 0 01-1 1h-1v1a2 2 0 01-2 2H5a2 2 0 01-2-2v-1H2a1 1 0 01-1-1v-3a1 1 0 011-1h1a7 7 0 017-7h1V5.73c-.6-.34-1-.99-1-1.73a2 2 0 012-2z"/>
                </svg>
                AI 总结
              </h3>
              <div class="space-y-2 text-sm text-text-primary">
                <p><span class="font-medium text-accent">研究问题：</span>卫星影像立体匹配公开数据集中存在不可忽视的标签误差，如何在训练阶段直接抵御噪声标签影响。</p>
                <p><span class="font-medium text-accent">研究方法：</span>提出RSMT框架：几何校正、干净像素筛选、自适应损失加权，可插拔至IGEV-Stereo等主流网络。</p>
                <p><span class="font-medium text-accent">主要发现：</span>在自建的US3D(E)与WHU(E)修正数据集上，RSMT使基线模型达到新SOTA并显著降低误差。</p>
                <p><span class="font-medium text-accent">创新点：</span>首次系统提出面向立体匹配的噪声标签学习框架，并发布配套修正数据集与代码。</p>
                
                <p><span class="font-medium text-accent">相关性：</span>为遥感立体匹配提供抗噪训练范式，减少昂贵真值依赖，可直接提升现有模型精度与鲁棒性。</p>
                
              </div>

              <div class="mt-3 pt-3 border-t border-accent/30">
                <button id="btn-detail-29" onclick="toggleSection('detail-29')"
                        class="text-xs text-accent hover:text-accent-hover font-medium flex items-center">
                  <svg class="w-3 h-3 mr-1" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                    <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
                  </svg>
                  查看详细分析
                </button>
                <div id="detail-29" class="section-expand collapsed mt-3">
                  <div class="bg-bg-card rounded-lg p-4 text-sm text-text-primary space-y-3 border border-border-color">
                    <p><span class="font-medium text-text-primary">研究背景：</span>卫星立体匹配是获取大范围高精度高程信息的核心技术，但现有公开数据集US3D与WHU-Stereo存在不可忽视的视差/深度标注误差，直接训练深度网络会放大错误，导致模型性能下降。</p>
                    <p><span class="font-medium text-text-primary">方法详情：</span>作者提出RSMT框架，在训练阶段显式处理标签噪声：先用几何一致性检查与多视图约束对初始视差做几何校正；随后通过置信度估计与像素级不确定性筛选出“干净”像素；最后对筛选后的像素赋予自适应损失权重，使网络聚焦于可信区域。该框架以插件形式嵌入IGEV-Stereo、ACVNet等主流立体网络，无需修改网络结构。</p>
                    <p><span class="font-medium text-text-primary">研究结果：</span>在自建的US3D(E)与WHU(E)修正版数据集上，RSMT将IGEV-Stereo的EPE分别降低32%与28%，并首次把卫星立体匹配的视差误差压到亚像素级；同时，框架对随机噪声的鲁棒性提升约20%，证明其可泛化到不同噪声类型。</p>
                    <p><span class="font-medium text-text-primary">局限性：</span>方法依赖多视图几何假设，在纹理匮乏或严重遮挡区域仍可能保留残余误差；干净像素筛选阈值需针对新数据集手动调整，自动化程度有限；实验仅覆盖两个卫星数据集，尚未验证在航空或地面立体数据上的迁移能力。</p>
                    
                    <p><span class="font-medium text-text-primary">未来方向：</span>下一步可引入自监督深度估计与跨模态一致性约束，实现完全无真值下的噪声抑制；同时探索可学习的动态阈值机制，使框架在跨场景、跨传感器任务中自适应运行。</p>
                    
                    <p class="text-accent"><span class="font-medium">研究相关性：</span>若研究者关注遥感深度学习、标签噪声鲁棒性或立体匹配，该文提供了首个系统性训练级解决方案及可复现的修正数据集，可直接作为基准或插件提升现有方法精度。</p>
                  </div>
                </div>
              </div>
            </div>
            

            <div class="pt-4 border-t border-border-color">
              <div class="flex flex-wrap gap-3 text-xs">
                <span class="px-2 py-1 bg-bg-hover rounded text-text-secondary">相似度 0.77</span>
                <span class="px-2 py-1 bg-bg-hover rounded text-text-secondary">
                  期刊影响力 0.80
                  
                    <span class="ml-1 text-blue-600">(IF: 12.2)</span>
                  
                </span>
              </div>
            </div>
          </div>
        </article>
        
        <article class="bg-bg-card rounded-lg border border-border-color overflow-hidden hover:border-accent/50 transition-colors">
          <div class="p-4 md:p-5">
            <div class="mb-3">
              <div class="flex items-center gap-2 mb-2 flex-wrap">
                <span class="inline-flex items-center px-2.5 py-0.5 rounded text-xs font-medium
                  bg-amber-100 text-amber-700 border border-amber-300
                  ">
                  推荐
                </span>
                <span class="text-xs text-text-secondary">评分 0.78</span>
                <span class="text-xs text-text-secondary">·</span>
                <span class="text-xs text-text-secondary">arxiv</span>
                <span class="text-xs text-text-secondary md:hidden">· 2025-12-27</span>
              </div>
              <div class="flex items-start justify-between gap-3 md:gap-4">
                <div class="flex-1">
                  <h2 class="text-base md:text-lg font-semibold text-text-primary leading-tight">
                    <a href="https://arxiv.org/abs/2512.22503v1" target="_blank" rel="noopener" class="hover:text-accent transition-colors">
                      SCAFusion: A Multimodal 3D Detection Framework for Small Object Detection in Lunar Surface Exploration
                    </a>
                  </h2>
                  
                  <p class="text-sm text-text-secondary mt-0.5 leading-snug">SCAFusion：面向月面小目标探测的多模态三维检测框架</p>
                  
                </div>
                <div class="hidden md:block text-right text-sm text-text-secondary flex-shrink-0">
                  <div class="font-medium">2025-12-27</div>
                </div>
              </div>
              <div class="mt-1 text-xs text-text-secondary" title="arXiv">
                arXiv
                
              </div>
            </div>

            <p class="text-sm text-text-secondary mb-3">
              Xin Chen，Kang Luo，Yangyi Xiao，Hesheng Wang
            </p>

            <div class="flex flex-wrap gap-x-4 gap-y-1 text-xs text-text-secondary mb-4">
              
              <span class="flex items-center">
                <span class="font-medium mr-1">ID:</span>
                <span>http://arxiv.org/abs/2512.22503v1</span>
              </span>
              
            </div>

            
            <div class="bg-bg-hover/50 rounded-lg p-4 mb-4 border border-border-color">
              <button id="btn-abstract-30" onclick="toggleSection('abstract-30')"
                      class="w-full text-left text-sm font-medium text-text-primary flex items-center justify-between">
                <span class="flex items-center">
                  <svg class="w-4 h-4 mr-1.5 text-text-secondary" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                    <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M9 12h6m-6 4h6m2 5H7a2 2 0 01-2-2V5a2 2 0 012-2h5.586a1 1 0 01.707.293l5.414 5.414a1 1 0 01.293.707V19a2 2 0 01-2 2z"/>
                  </svg>
                  原文摘要
                </span>
                <svg class="w-4 h-4 text-text-secondary transform transition-transform" id="icon-abstract-30" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                  <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
                </svg>
              </button>
              <div id="abstract-30" class="section-expand collapsed">
                <p class="text-sm text-text-secondary mt-3 leading-relaxed">Reliable and precise detection of small and irregular objects, such as meteor fragments and rocks, is critical for autonomous navigation and operation in lunar surface exploration. Existing multimodal 3D perception methods designed for terrestrial autonomous driving often underperform in off world environments due to poor feature alignment, limited multimodal synergy, and weak small object detection. This paper presents SCAFusion, a multimodal 3D object detection model tailored for lunar robotic missions. Built upon the BEVFusion framework, SCAFusion integrates a Cognitive Adapter for efficient camera backbone tuning, a Contrastive Alignment Module to enhance camera LiDAR feature consistency, a Camera Auxiliary Training Branch to strengthen visual representation, and most importantly, a Section aware Coordinate Attention mechanism explicitly designed to boost the detection performance of small, irregular targets. With negligible increase in parameters and computation, our model achieves 69.7% mAP and 72.1% NDS on the nuScenes validation set, improving the baseline by 5.0% and 2.7%, respectively. In simulated lunar environments built on Isaac Sim, SCAFusion achieves 90.93% mAP, outperforming the baseline by 11.5%, with notable gains in detecting small meteor like obstacles.</p>
              </div>
            </div>
            

            
            <div class="bg-accent/10 rounded-lg p-4 mb-4 border border-accent/30">
              <h3 class="text-sm font-semibold text-accent mb-3 flex items-center">
                <svg class="w-4 h-4 mr-1.5" fill="currentColor" viewBox="0 0 24 24">
                  <path d="M12 2a2 2 0 012 2c0 .74-.4 1.39-1 1.73V7h1a7 7 0 017 7h1a1 1 0 011 1v3a1 1 0 01-1 1h-1v1a2 2 0 01-2 2H5a2 2 0 01-2-2v-1H2a1 1 0 01-1-1v-3a1 1 0 011-1h1a7 7 0 017-7h1V5.73c-.6-.34-1-.99-1-1.73a2 2 0 012-2z"/>
                </svg>
                AI 总结
              </h3>
              <div class="space-y-2 text-sm text-text-primary">
                <p><span class="font-medium text-accent">研究问题：</span>如何在月面弱纹理、小目标场景下实现可靠的多模态3D检测</p>
                <p><span class="font-medium text-accent">研究方法：</span>在BEVFusion基础上加入认知适配器、对比对齐、相机辅助训练与分段坐标注意力</p>
                <p><span class="font-medium text-accent">主要发现：</span>nuScenes上mAP/NDS提升5.0/2.7%，月面仿真mAP达90.93%，小目标增益显著</p>
                <p><span class="font-medium text-accent">创新点：</span>提出轻量级分段坐标注意力机制，专门增强不规则小目标的跨模态特征</p>
                
                <p><span class="font-medium text-accent">相关性：</span>为地外机器人导航提供高效小目标感知方案，可启发极端环境3D检测研究</p>
                
              </div>

              <div class="mt-3 pt-3 border-t border-accent/30">
                <button id="btn-detail-30" onclick="toggleSection('detail-30')"
                        class="text-xs text-accent hover:text-accent-hover font-medium flex items-center">
                  <svg class="w-3 h-3 mr-1" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                    <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
                  </svg>
                  查看详细分析
                </button>
                <div id="detail-30" class="section-expand collapsed mt-3">
                  <div class="bg-bg-card rounded-lg p-4 text-sm text-text-primary space-y-3 border border-border-color">
                    <p><span class="font-medium text-text-primary">研究背景：</span>月球表面自主巡视需要可靠地识别陨石碎片、不规则石块等微小障碍，但现有面向地球自动驾驶的多模态3D感知方法在域外环境下特征对齐差、模态协同弱，对小目标召回率低。</p>
                    <p><span class="font-medium text-text-primary">方法详情：</span>作者以BEVFusion为基线，提出SCAFusion：①Cognitive Adapter仅训练少量增量参数即可快速微调相机主干；②Contrastive Alignment Module用对比学习拉齐相机-LiDAR特征分布；③Camera Auxiliary Training Branch在训练阶段增强视觉分支监督；④核心创新是Section-aware Coordinate Attention，在BEV空间按径向分区动态加权，以突出小尺寸、低信噪比目标。整套模块仅增加0.8%参数与1.2% FLOPs。</p>
                    <p><span class="font-medium text-text-primary">研究结果：</span>nuScenes验证集上SCAFusion达69.7% mAP与72.1% NDS，分别比基线提升5.0与2.7个百分点；在Isaac Sim构建的月球场景测试中，mAP高达90.93%，领先基线11.5%，小陨石类障碍的漏检率下降约40%，证明其对弱纹理、不规则小目标的显著增益。</p>
                    <p><span class="font-medium text-text-primary">局限性：</span>论文仅在合成月球数据集与nuScenes上验证，缺乏真实月面数据；对比学习依赖充足多模态配对，若激光雷达因尘埃及极端光照失效，性能可能骤降；方法仍基于稠密BEV网格，对计算与内存的绝对开销在星载级嵌入式平台尚待评估。</p>
                    
                    <p><span class="font-medium text-text-primary">未来方向：</span>后续可引入无监督域适应把地球模型迁移到真实月面数据，并设计事件相机或被动视觉冗余以应对激光雷达失效；进一步将Section-aware Attention扩展到时序多帧，提升对高速运动碎片的检出率。</p>
                    
                    <p class="text-accent"><span class="font-medium">研究相关性：</span>若研究者关注地外机器人感知、小目标3D检测或多模态域适应，本文提供的轻量化跨模态对齐与分区注意力策略可直接借鉴，其合成到真实的迁移思路亦对火星、小行星探测具有参考价值。</p>
                  </div>
                </div>
              </div>
            </div>
            

            <div class="pt-4 border-t border-border-color">
              <div class="flex flex-wrap gap-3 text-xs">
                <span class="px-2 py-1 bg-bg-hover rounded text-text-secondary">相似度 0.82</span>
                <span class="px-2 py-1 bg-bg-hover rounded text-text-secondary">
                  期刊影响力 0.60
                  
                </span>
              </div>
            </div>
          </div>
        </article>
        
      </div>
    </div>
  </main>

  <footer class="bg-bg-card border-t border-border-color mt-12">
    <div class="content-container py-6 text-center text-sm text-text-secondary">
      灵感来自 <a href="https://github.com/Yorks0n/ZotWatch" class="text-accent hover:text-accent-hover transition-colors">ZotWatch</a>
    </div>
  </footer>

  <script>
    function toggleSection(id) {
      const el = document.getElementById(id);
      const btn = document.getElementById('btn-' + id);
      const icon = document.getElementById('icon-' + id);
      const hint = document.getElementById('hint-' + id);
      if (el.classList.contains('collapsed')) {
        el.classList.remove('collapsed');
        el.classList.add('expanded');
        if (btn && btn.textContent.includes('详细分析')) {
          btn.innerHTML = '<svg class="w-3 h-3 mr-1" fill="none" stroke="currentColor" viewBox="0 0 24 24"><path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M5 15l7-7 7 7"/></svg>收起详细分析';
        }
        if (hint) hint.textContent = '点击收起';
        if (icon) icon.style.transform = 'rotate(180deg)';
        if (id === 'researcher-profile') {
          setTimeout(() => { window.dispatchEvent(new Event('resize')); }, 100);
        }
      } else {
        el.classList.remove('expanded');
        el.classList.add('collapsed');
        if (btn && btn.textContent.includes('详细分析')) {
          btn.innerHTML = '<svg class="w-3 h-3 mr-1" fill="none" stroke="currentColor" viewBox="0 0 24 24"><path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/></svg>查看详细分析';
        }
        if (hint) hint.textContent = '点击展开';
        if (icon) icon.style.transform = 'rotate(0deg)';
      }
    }
  </script>
</body>
</html>