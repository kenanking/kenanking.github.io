<!DOCTYPE html>
<html lang="zh-CN">
<head>
  <meta charset="utf-8">
  <meta name="viewport" content="width=device-width, initial-scale=1">
  <title>ZotWatch 文献推荐 - 2025-12-25</title>
  <script src="https://cdn.tailwindcss.com"></script>
  <script src="https://cdn.jsdelivr.net/npm/chart.js@4.4.1/dist/chart.umd.min.js"></script>
  <script src="https://d3js.org/d3.v7.min.js"></script>
  <script>
    tailwind.config = {
      theme: {
        extend: {
          colors: {
            'bg-primary': '#f8fafc',
            'bg-card': '#ffffff',
            'bg-hover': '#f1f5f9',
            'text-primary': '#1e293b',
            'text-secondary': '#64748b',
            'border-color': '#e2e8f0',
            'accent': '#2563eb',
            'accent-hover': '#1d4ed8',
          }
        }
      }
    }
  </script>
  <style>
    .section-expand { transition: max-height 0.5s ease-out; overflow: hidden; }
    .section-expand.collapsed { max-height: 0; }
    .section-expand.expanded { max-height: none; }
    body { font-family: -apple-system, BlinkMacSystemFont, 'Segoe UI', Roboto, 'Helvetica Neue', Arial, sans-serif; }
    .content-container { max-width: 48rem; margin: 0 auto; padding: 0 1rem; }
    @media (min-width: 768px) { .content-container { padding: 0 2rem; } }
    @media (min-width: 1024px) { .content-container { padding: 0 4rem; } }

    .cluster-graph-container { width: 100%; height: 280px; touch-action: pan-y; }
    @media (min-width: 640px) { .cluster-graph-container { height: 320px; } }
    @media (min-width: 768px) { .cluster-graph-container { height: 350px; } }
    #cluster-graph svg { overflow: visible; }
  </style>
</head>
<body class="bg-bg-primary min-h-screen text-text-primary">
  <header class="bg-bg-card border-b border-border-color">
    <div class="content-container py-6 md:py-8">
      <h1 class="text-xl md:text-2xl font-bold text-text-primary">ZotWatch 文献推荐</h1>
      <p class="text-xs md:text-sm text-text-secondary mt-1">
        共 35 篇论文 ·
        生成于 2025-12-25 10:44 Asia/Shanghai
      </p>
    </div>
  </header>

  
  <section class="py-4 md:py-6 border-b border-border-color">
    <div class="content-container">
      <button id="btn-researcher-profile" onclick="toggleSection('researcher-profile')"
              class="w-full text-left text-lg md:text-xl font-bold text-text-primary mb-3 md:mb-4 flex items-center justify-between group hover:text-accent transition-colors">
        <span class="flex items-center gap-2 flex-1 min-w-0">
          <svg class="w-5 h-5 flex-shrink-0 text-accent" fill="currentColor" viewBox="0 0 24 24">
            <path d="M12 2C6.48 2 2 6.48 2 12s4.48 10 10 10 10-4.48 10-10S17.52 2 12 2zm0 3c1.66 0 3 1.34 3 3s-1.34 3-3 3-3-1.34-3-3 1.34-3 3-3zm0 14.2c-2.5 0-4.71-1.28-6-3.22.03-1.99 4-3.08 6-3.08 1.99 0 5.97 1.09 6 3.08-1.29 1.94-3.5 3.22-6 3.22z"/>
          </svg>
          <span class="truncate">研究兴趣画像</span>
        </span>
        <span class="flex items-center gap-1 md:gap-2 flex-shrink-0 ml-2">
          <span class="hidden md:inline text-sm font-normal text-text-secondary group-hover:text-accent" id="hint-researcher-profile">点击展开</span>
          <svg class="w-5 h-5 md:w-4 md:h-4 text-text-secondary group-hover:text-accent transform transition-transform" id="icon-researcher-profile" fill="none" stroke="currentColor" viewBox="0 0 24 24">
            <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
          </svg>
        </span>
      </button>

      <div id="researcher-profile" class="section-expand collapsed">
      <div class="bg-bg-card rounded-lg border border-border-color p-4 mb-4">
        <div class="grid grid-cols-4 gap-3 text-center">
          <div>
            <div class="text-xl font-bold text-accent">936</div>
            <div class="text-xs text-text-secondary">收藏论文</div>
          </div>
          <div>
            <div class="text-xl font-bold text-accent">5年8月</div>
            <div class="text-xs text-text-secondary">收藏时长</div>
          </div>
          <div>
            <div class="text-xl font-bold text-accent">&gt;10</div>
            <div class="text-xs text-text-secondary">兴趣领域</div>
          </div>
          <div>
            <div class="text-xl font-bold text-accent">8</div>
            <div class="text-xs text-text-secondary">高频作者</div>
          </div>
        </div>
      </div>

      
      <div class="bg-accent/10 rounded-lg border border-accent/30 p-4 mb-4">
        <h3 class="text-sm font-semibold text-accent mb-2 flex items-center gap-2">
          <svg class="w-4 h-4" fill="currentColor" viewBox="0 0 24 24">
            <path d="M12 2a2 2 0 012 2c0 .74-.4 1.39-1 1.73V7h1a7 7 0 017 7h1a1 1 0 011 1v3a1 1 0 01-1 1h-1v1a2 2 0 01-2 2H5a2 2 0 01-2-2v-1H2a1 1 0 01-1-1v-3a1 1 0 011-1h1a7 7 0 017-7h1V5.73c-.6-.34-1-.99-1-1.73a2 2 0 012-2z"/>
          </svg>
          AI 研究兴趣分析
        </h3>
        <div class="space-y-2 text-xs text-text-primary leading-relaxed">
          <p><span class="font-medium text-accent">研究兴趣：</span>用户长期关注计算机视觉与遥感交叉领域，核心阅读集中在目标检测、视觉定位及模型压缩，同时对自监督与对比学习保持兴趣。</p>
          <p><span class="font-medium text-accent">深度关注：</span>在SAR图像理解与旋转目标检测方向形成持续积累，高频收藏IEEE TGARS与《雷达学报》论文；对He-Girshick系检测架构及Han的模型压缩方法有系统追踪。</p>
          <p><span class="font-medium text-accent">跨学科倾向：</span>阅读轨迹呈现“CV算法—遥感应用—高效部署”跨学科链条，将通用视觉Transformer、重参数化等CV前沿迁移至SAR场景并关注边缘部署。</p>
          <p><span class="font-medium text-accent">兴趣演变：</span>2025年起季度收藏量显著回升且新增LLM与视觉Transformer关键词，显示正把基础模型范式引入遥感解析；同时扩散模型与域自适应论文比例增加，预示向生成式增强与跨域迁移拓展。</p>
          <p><span class="font-medium text-accent">阅读建议：</span>建议跟进遥感多模态基础模型（RS-MLLM）与SAR-光学融合的大模型评测基准，并探索面向在轨实时处理的量化-剪枝联合压缩框架。</p>
        </div>
      </div>
      

      <div class="grid grid-cols-1 md:grid-cols-2 gap-4 mb-4">
        
        <div class="bg-bg-card rounded-lg border border-border-color p-4">
          <h4 class="text-sm font-medium text-text-primary mb-3">研究领域分布</h4>
          <div style="height: 180px;">
            <canvas id="domainsChart"></canvas>
          </div>
        </div>
        

        
        <div class="bg-bg-card rounded-lg border border-border-color p-4">
          <h4 class="text-sm font-medium text-text-primary mb-3">阅读趋势（季度）</h4>
          <div style="height: 180px;">
            <canvas id="trendsChart"></canvas>
          </div>
        </div>
        
      </div>

      
      <div class="bg-bg-card rounded-lg border border-border-color p-4 mb-4">
        <h4 class="text-sm font-medium text-text-primary mb-3">论文发表年份分布</h4>
        <div style="height: 180px;">
          <canvas id="yearDistChart"></canvas>
        </div>
      </div>
      

      
      <div class="bg-bg-card rounded-lg border border-border-color p-4 mb-4">
        <h4 class="text-sm font-medium text-text-primary mb-3">
          研究方向聚类图谱
          <span class="text-xs text-text-secondary font-normal ml-2">(30 个聚类)</span>
        </h4>
        <div id="cluster-graph" class="cluster-graph-container"></div>
        <div class="text-xs text-text-secondary mt-3">
          覆盖 912/912 篇论文（仅含摘要的文献参与聚类）
        </div>
      </div>
      

      <div class="grid grid-cols-1 md:grid-cols-2 gap-4 mb-4">
        
        <div class="bg-bg-card rounded-lg border border-border-color p-4">
          <h4 class="text-sm font-medium text-text-primary mb-2">高频关注作者</h4>
          <div class="grid grid-cols-2 gap-x-4 gap-y-1 text-xs">
            
            <div class="flex justify-between truncate">
              <span class="text-text-primary truncate" title="Kaiming He">Kaiming He</span>
              <span class="text-text-secondary ml-1 flex-shrink-0">24</span>
            </div>
            
            <div class="flex justify-between truncate">
              <span class="text-text-primary truncate" title="Ross Girshick">Ross Girshick</span>
              <span class="text-text-secondary ml-1 flex-shrink-0">18</span>
            </div>
            
            <div class="flex justify-between truncate">
              <span class="text-text-primary truncate" title="Jian Sun">Jian Sun</span>
              <span class="text-text-secondary ml-1 flex-shrink-0">15</span>
            </div>
            
            <div class="flex justify-between truncate">
              <span class="text-text-primary truncate" title="Song Han">Song Han</span>
              <span class="text-text-secondary ml-1 flex-shrink-0">14</span>
            </div>
            
            <div class="flex justify-between truncate">
              <span class="text-text-primary truncate" title="Xiangyu Zhang">Xiangyu Zhang</span>
              <span class="text-text-secondary ml-1 flex-shrink-0">14</span>
            </div>
            
            <div class="flex justify-between truncate">
              <span class="text-text-primary truncate" title="Piotr Dollár">Piotr Dollár</span>
              <span class="text-text-secondary ml-1 flex-shrink-0">12</span>
            </div>
            
            <div class="flex justify-between truncate">
              <span class="text-text-primary truncate" title="Li Liu">Li Liu</span>
              <span class="text-text-secondary ml-1 flex-shrink-0">10</span>
            </div>
            
            <div class="flex justify-between truncate">
              <span class="text-text-primary truncate" title="Xiang Li">Xiang Li</span>
              <span class="text-text-secondary ml-1 flex-shrink-0">10</span>
            </div>
            
            <div class="flex justify-between truncate">
              <span class="text-text-primary truncate" title="Yann LeCun">Yann LeCun</span>
              <span class="text-text-secondary ml-1 flex-shrink-0">9</span>
            </div>
            
            <div class="flex justify-between truncate">
              <span class="text-text-primary truncate" title="Chunhua Shen">Chunhua Shen</span>
              <span class="text-text-secondary ml-1 flex-shrink-0">8</span>
            </div>
            
            <div class="flex justify-between truncate">
              <span class="text-text-primary truncate" title="Saining Xie">Saining Xie</span>
              <span class="text-text-secondary ml-1 flex-shrink-0">8</span>
            </div>
            
            <div class="flex justify-between truncate">
              <span class="text-text-primary truncate" title="Gangyao Kuang">Gangyao Kuang</span>
              <span class="text-text-secondary ml-1 flex-shrink-0">8</span>
            </div>
            
          </div>
        </div>
        

        
        <div class="bg-bg-card rounded-lg border border-border-color p-4">
          <h4 class="text-sm font-medium text-text-primary mb-2">常关注期刊/会议</h4>
          <div class="space-y-1 text-xs">
            
            <div class="flex items-center justify-between">
              <span class="text-text-primary truncate flex-1" title="Computer Vision and Pattern Recognition">Computer Vision and Pattern Recognition</span>
              <span class="text-xs px-1 py-0.5 rounded bg-blue-100 text-blue-700 ml-1 flex-shrink-0">
                期刊
              </span>
              <span class="text-text-secondary ml-1 flex-shrink-0">114</span>
            </div>
            
            <div class="flex items-center justify-between">
              <span class="text-text-primary truncate flex-1" title="Neural Information Processing Systems">Neural Information Processing Systems</span>
              <span class="text-xs px-1 py-0.5 rounded bg-blue-100 text-blue-700 ml-1 flex-shrink-0">
                期刊
              </span>
              <span class="text-text-secondary ml-1 flex-shrink-0">45</span>
            </div>
            
            <div class="flex items-center justify-between">
              <span class="text-text-primary truncate flex-1" title="IEEE Transactions on Geoscience and Remote Sensing">IEEE Transactions on Geoscience and Remote Sensing</span>
              <span class="text-xs px-1 py-0.5 rounded bg-blue-100 text-blue-700 ml-1 flex-shrink-0">
                期刊
              </span>
              <span class="text-text-secondary ml-1 flex-shrink-0">43</span>
            </div>
            
            <div class="flex items-center justify-between">
              <span class="text-text-primary truncate flex-1" title="International Conference on Computer Vision">International Conference on Computer Vision</span>
              <span class="text-xs px-1 py-0.5 rounded bg-amber-100 text-amber-700 ml-1 flex-shrink-0">
                会议
              </span>
              <span class="text-text-secondary ml-1 flex-shrink-0">36</span>
            </div>
            
            <div class="flex items-center justify-between">
              <span class="text-text-primary truncate flex-1" title="IEEE Transactions on Pattern Analysis and Machine Intelligence">IEEE Transactions on Pattern Analysis and Machine Intelligence</span>
              <span class="text-xs px-1 py-0.5 rounded bg-blue-100 text-blue-700 ml-1 flex-shrink-0">
                期刊
              </span>
              <span class="text-text-secondary ml-1 flex-shrink-0">32</span>
            </div>
            
            <div class="flex items-center justify-between">
              <span class="text-text-primary truncate flex-1" title="International Conference on Learning Representations">International Conference on Learning Representations</span>
              <span class="text-xs px-1 py-0.5 rounded bg-amber-100 text-amber-700 ml-1 flex-shrink-0">
                会议
              </span>
              <span class="text-text-secondary ml-1 flex-shrink-0">27</span>
            </div>
            
            <div class="flex items-center justify-between">
              <span class="text-text-primary truncate flex-1" title="European Conference on Computer Vision">European Conference on Computer Vision</span>
              <span class="text-xs px-1 py-0.5 rounded bg-amber-100 text-amber-700 ml-1 flex-shrink-0">
                会议
              </span>
              <span class="text-text-secondary ml-1 flex-shrink-0">23</span>
            </div>
            
            <div class="flex items-center justify-between">
              <span class="text-text-primary truncate flex-1" title="IEEE Geoscience and Remote Sensing Letters">IEEE Geoscience and Remote Sensing Letters</span>
              <span class="text-xs px-1 py-0.5 rounded bg-blue-100 text-blue-700 ml-1 flex-shrink-0">
                期刊
              </span>
              <span class="text-text-secondary ml-1 flex-shrink-0">21</span>
            </div>
            
          </div>
        </div>
        
      </div>

      
      <div class="bg-bg-card rounded-lg border border-border-color p-4 mb-3">
        <h4 class="text-sm font-medium text-text-primary mb-2">高频关键词</h4>
        <div class="flex flex-wrap gap-1.5">
          
          <span class="px-2 py-0.5 bg-bg-hover rounded text-xs text-text-primary">
            大语言模型 <span class="text-text-secondary">(14)</span>
          </span>
          
          <span class="px-2 py-0.5 bg-bg-hover rounded text-xs text-text-primary">
            自监督学习 <span class="text-text-secondary">(11)</span>
          </span>
          
          <span class="px-2 py-0.5 bg-bg-hover rounded text-xs text-text-primary">
            综述 <span class="text-text-secondary">(9)</span>
          </span>
          
          <span class="px-2 py-0.5 bg-bg-hover rounded text-xs text-text-primary">
            域自适应 <span class="text-text-secondary">(9)</span>
          </span>
          
          <span class="px-2 py-0.5 bg-bg-hover rounded text-xs text-text-primary">
            合成孔径雷达 <span class="text-text-secondary">(9)</span>
          </span>
          
          <span class="px-2 py-0.5 bg-bg-hover rounded text-xs text-text-primary">
            迁移学习 <span class="text-text-secondary">(7)</span>
          </span>
          
          <span class="px-2 py-0.5 bg-bg-hover rounded text-xs text-text-primary">
            扩散模型 <span class="text-text-secondary">(7)</span>
          </span>
          
          <span class="px-2 py-0.5 bg-bg-hover rounded text-xs text-text-primary">
            强化学习 <span class="text-text-secondary">(7)</span>
          </span>
          
          <span class="px-2 py-0.5 bg-bg-hover rounded text-xs text-text-primary">
            注意力机制 <span class="text-text-secondary">(6)</span>
          </span>
          
          <span class="px-2 py-0.5 bg-bg-hover rounded text-xs text-text-primary">
            重参数化 <span class="text-text-secondary">(6)</span>
          </span>
          
          <span class="px-2 py-0.5 bg-bg-hover rounded text-xs text-text-primary">
            DeepSeek <span class="text-text-secondary">(6)</span>
          </span>
          
          <span class="px-2 py-0.5 bg-bg-hover rounded text-xs text-text-primary">
            生成对抗网络 <span class="text-text-secondary">(6)</span>
          </span>
          
          <span class="px-2 py-0.5 bg-bg-hover rounded text-xs text-text-primary">
            Transformers <span class="text-text-secondary">(5)</span>
          </span>
          
          <span class="px-2 py-0.5 bg-bg-hover rounded text-xs text-text-primary">
            旋转目标检测 <span class="text-text-secondary">(5)</span>
          </span>
          
          <span class="px-2 py-0.5 bg-bg-hover rounded text-xs text-text-primary">
            SAR目标识别 <span class="text-text-secondary">(5)</span>
          </span>
          
          <span class="px-2 py-0.5 bg-bg-hover rounded text-xs text-text-primary">
            特征可视化 <span class="text-text-secondary">(5)</span>
          </span>
          
          <span class="px-2 py-0.5 bg-bg-hover rounded text-xs text-text-primary">
            Vision Transformers <span class="text-text-secondary">(4)</span>
          </span>
          
          <span class="px-2 py-0.5 bg-bg-hover rounded text-xs text-text-primary">
            基础模型 <span class="text-text-secondary">(4)</span>
          </span>
          
          <span class="px-2 py-0.5 bg-bg-hover rounded text-xs text-text-primary">
            HRNet <span class="text-text-secondary">(4)</span>
          </span>
          
          <span class="px-2 py-0.5 bg-bg-hover rounded text-xs text-text-primary">
            目标检测 <span class="text-text-secondary">(4)</span>
          </span>
          
        </div>
      </div>
      

      <div class="text-xs text-text-secondary text-right">
        画像生成于 2025-12-23 10:33 Asia/Shanghai
        · kimi-k2-turbo-preview
      </div>
      </div>
    </div>
  </section>
  

  
  <script>
    document.addEventListener('DOMContentLoaded', function() {
      Chart.defaults.font.family = "-apple-system, BlinkMacSystemFont, 'Segoe UI', Roboto, 'Helvetica Neue', Arial, sans-serif";
      Chart.defaults.font.size = 11;
      Chart.defaults.color = '#64748b';
      Chart.defaults.animation = false;

      
      const domainsCtx = document.getElementById('domainsChart');
      if (domainsCtx) {
        new Chart(domainsCtx, {
          type: 'bar',
          data: {
            labels: ['视觉定位', '目标检测', '模型压缩', '姿态估计', '对比学习', '车牌识别', '卫星导航', '人脸对齐'],
            datasets: [{
              data: [22, 35, 18, 15, 10, 8, 6, 9],
              backgroundColor: 'rgba(37, 99, 235, 0.7)',
              borderColor: 'rgba(37, 99, 235, 1)',
              borderWidth: 1,
              borderRadius: 2,
              barThickness: 16
            }]
          },
          options: {
            indexAxis: 'y',
            responsive: true,
            maintainAspectRatio: false,
            plugins: {
              legend: { display: false },
              tooltip: { enabled: false }
            },
            scales: {
              x: {
                beginAtZero: true,
                grid: { color: 'rgba(226, 232, 240, 0.5)' },
                ticks: { precision: 0 }
              },
              y: {
                grid: { display: false },
                ticks: { font: { size: 10 } }
              }
            }
          }
        });
      }
      

      
      const trendsCtx = document.getElementById('trendsChart');
      if (trendsCtx) {
        const trends = [{ q: '2023-Q1', c: 51 }, { q: '2023-Q2', c: 18 }, { q: '2023-Q3', c: 21 }, { q: '2023-Q4', c: 19 }, { q: '2024-Q1', c: 67 }, { q: '2024-Q2', c: 18 }, { q: '2024-Q3', c: 4 }, { q: '2024-Q4', c: 23 }, { q: '2025-Q1', c: 88 }, { q: '2025-Q2', c: 34 }, { q: '2025-Q3', c: 12 }, { q: '2025-Q4', c: 29 }];
        const monthColors = [
          { bg: 'rgba(59, 130, 246, 0.7)', border: 'rgba(59, 130, 246, 1)' },
          { bg: 'rgba(16, 185, 129, 0.7)', border: 'rgba(16, 185, 129, 1)' },
          { bg: 'rgba(245, 158, 11, 0.7)', border: 'rgba(245, 158, 11, 1)' }
        ];
        const bgColors = trends.map((t, i) => monthColors[i % 3].bg);
        const borderColors = trends.map((t, i) => monthColors[i % 3].border);
        new Chart(trendsCtx, {
          type: 'bar',
          data: {
            labels: trends.map(t => t.q.replace('-', '\n')),
            datasets: [{
              data: trends.map(t => t.c),
              backgroundColor: bgColors,
              borderColor: borderColors,
              borderWidth: 1,
              borderRadius: 2,
              barThickness: 20
            }]
          },
          options: {
            responsive: true,
            maintainAspectRatio: false,
            plugins: {
              legend: { display: false },
              tooltip: { enabled: false }
            },
            scales: {
              x: {
                grid: { display: false },
                ticks: { font: { size: 9 }, maxRotation: 0 }
              },
              y: {
                beginAtZero: true,
                grid: { color: 'rgba(226, 232, 240, 0.5)' },
                ticks: { precision: 0 }
              }
            }
          }
        });
      }
      

      
      const yearDistCtx = document.getElementById('yearDistChart');
      if (yearDistCtx) {
        const yearData = [{ year: 2006, count: 8 }, { year: 2007, count: 3 }, { year: 2008, count: 2 }, { year: 2009, count: 2 }, { year: 2010, count: 9 }, { year: 2011, count: 10 }, { year: 2012, count: 3 }, { year: 2013, count: 12 }, { year: 2014, count: 14 }, { year: 2015, count: 18 }, { year: 2016, count: 15 }, { year: 2017, count: 41 }, { year: 2018, count: 59 }, { year: 2019, count: 53 }, { year: 2020, count: 66 }, { year: 2021, count: 84 }, { year: 2022, count: 110 }, { year: 2023, count: 109 }, { year: 2024, count: 112 }, { year: 2025, count: 163 }];
        new Chart(yearDistCtx, {
          type: 'line',
          data: {
            labels: yearData.map(d => d.year),
            datasets: [{
              data: yearData.map(d => d.count),
              borderColor: 'rgba(37, 99, 235, 1)',
              backgroundColor: 'rgba(37, 99, 235, 0.1)',
              borderWidth: 2,
              fill: true,
              tension: 0.3,
              pointRadius: 2,
              pointHoverRadius: 4
            }]
          },
          options: {
            responsive: true,
            maintainAspectRatio: false,
            plugins: {
              legend: { display: false },
              tooltip: {
                callbacks: {
                  label: function(context) {
                    return context.parsed.y + ' 篇';
                  }
                }
              }
            },
            scales: {
              x: {
                grid: { display: false },
                ticks: {
                  font: { size: 9 },
                  maxRotation: 45,
                  callback: function(value, index) {
                    const year = this.getLabelForValue(value);
                    return year % 5 === 0 ? year : '';
                  }
                }
              },
              y: {
                beginAtZero: true,
                grid: { color: 'rgba(226, 232, 240, 0.5)' },
                ticks: { precision: 0 }
              }
            }
          }
        });
      }
      

      
      const clusterContainer = document.getElementById('cluster-graph');
      if (clusterContainer) {
        const clusters = [
          
          {
            id: 0,
            label: "\u5927\u8bed\u8a00\u6a21\u578b\u4e0e\u6df7\u5408\u4e13\u5bb6\u4f18\u5316",
            size: 66,
            keywords: ["\u5927\u8bed\u8a00\u6a21\u578b", "DeepSeek", "\u5f3a\u5316\u5b66\u4e60"]
          },
          
          {
            id: 1,
            label: "SAR\u8230\u8239\u68c0\u6d4b\u4e0e\u6df1\u5ea6\u5b66\u4e60",
            size: 63,
            keywords: ["\u5408\u6210\u5b54\u5f84\u96f7\u8fbe", "SAR\u8230\u8239\u68c0\u6d4b", "\u591a\u5c3a\u5ea6\u7279\u5f81\u878d\u5408"]
          },
          
          {
            id: 2,
            label: "\u5b9e\u65f6\u76ee\u6807\u68c0\u6d4b\u67b6\u6784\u6f14\u8fdb",
            size: 62,
            keywords: ["\u7efc\u8ff0", "DETR", "\u5b9e\u65f6\u76ee\u6807\u68c0\u6d4b"]
          },
          
          {
            id: 3,
            label: "\u8f7b\u91cf\u7ea7CNN\u4e0e\u79fb\u52a8\u7aef\u4f18\u5316",
            size: 47,
            keywords: ["\u91cd\u53c2\u6570\u5316", "\u5377\u79ef\u795e\u7ecf\u7f51\u7edc", "VGG"]
          },
          
          {
            id: 4,
            label: "\u89c6\u89c9Transformer\u4e0e\u81ea\u76d1\u7763",
            size: 46,
            keywords: ["\u81ea\u76d1\u7763\u5b66\u4e60", "\u89c6\u89c9Transformer", "Vision Transformers"]
          },
          
          {
            id: 5,
            label: "\u795e\u7ecf\u7f51\u7edc\u91cf\u5316\u538b\u7f29\u52a0\u901f",
            size: 43,
            keywords: ["\u6a21\u578b\u538b\u7f29", "\u6a21\u578b\u91cf\u5316", "\u6574\u6570\u63a8\u7406"]
          },
          
          {
            id: 6,
            label: "SAR\u56fe\u50cf\u591a\u4efb\u52a1\u57fa\u7840\u6a21\u578b",
            size: 42,
            keywords: ["\u5408\u6210\u5b54\u5f84\u96f7\u8fbe\u56fe\u50cf\u63cf\u8ff0", "\u591a\u6a21\u6001", "\u591a\u6a21\u5757\u6df1\u5ea6\u5377\u79ef\u795e\u7ecf\u7f51\u7edc"]
          },
          
          {
            id: 7,
            label: "SAR\u8fc1\u79fb\u4e0e\u5408\u6210\u57df\u9002\u5e94",
            size: 38,
            keywords: ["SAR\u76ee\u6807\u8bc6\u522b", "\u8fc1\u79fb\u5b66\u4e60", "\u5408\u6210\u5b54\u5f84\u96f7\u8fbe"]
          },
          
          {
            id: 8,
            label: "\u6df1\u5ea6\u4eba\u4f53\u5173\u952e\u70b9\u4f30\u8ba1",
            size: 36,
            keywords: ["HRNet", "Transformers", "\u5308\u7259\u5229\u7b97\u6cd5"]
          },
          
          {
            id: 9,
            label: "\u5f31\u5c0f\u76ee\u6807\u667a\u80fd\u68c0\u6d4b\u8ddf\u8e2a",
            size: 32,
            keywords: ["\u6df1\u5ea6\u5b66\u4e60", "\u7279\u5f81\u878d\u5408", "\u5f31\u5c0f\u76ee\u6807\u8ddf\u8e2a"]
          },
          
          {
            id: 10,
            label: "\u5c0f\u6837\u672c\u76ee\u6807\u68c0\u6d4b\u7efc\u8ff0",
            size: 30,
            keywords: ["\u5f00\u653e\u96c6\u8bc6\u522b", "\u539f\u578b\u7f51\u7edc", "\u8de8\u57df\u5c0f\u6837\u672c\u5b66\u4e60"]
          },
          
          {
            id: 11,
            label: "\u7ea2\u5916\u5c0f\u76ee\u6807\u68c0\u6d4b\u589e\u5f3a",
            size: 29,
            keywords: ["\u7ea2\u5916\u5c0f\u76ee\u6807\u68c0\u6d4b", "\u6837\u672c\u81ea\u9002\u5e94\u589e\u6b96", "\u7279\u5f81\u589e\u5f3a"]
          },
          
          {
            id: 12,
            label: "\u6df1\u5ea6\u5b66\u4e60\u57fa\u7840\u4e0e\u6b8b\u5dee\u7f51\u7edc",
            size: 28,
            keywords: ["\u5f3a\u5316\u5b66\u4e60", "\u4eba\u5de5\u795e\u7ecf\u7f51\u7edc", "\u6b8b\u5dee\u8fde\u63a5"]
          },
          
          {
            id: 13,
            label: "\u6269\u6563\u6a21\u578b\u56fe\u50cf\u751f\u6210",
            size: 28,
            keywords: ["\u6269\u6563\u6a21\u578b", "\u56fe\u50cf\u6062\u590d", "\u611f\u77e5-\u5931\u771f\u6743\u8861"]
          },
          
          {
            id: 14,
            label: "\u8f66\u724c\u8bc6\u522b\u7aef\u5230\u7aef\u7cfb\u7edf",
            size: 27,
            keywords: ["\u8f66\u724c\u8bc6\u522b", "Internet of Things", "Microcontrollers"]
          },
          
          {
            id: 15,
            label: "\u795e\u7ecf\u7f51\u7edc\u4f18\u5316\u4e0e\u8bad\u7ec3\u6280\u5de7",
            size: 27,
            keywords: ["\u4f18\u5316\u5668", "\u5206\u5e03\u5f0f\u8bad\u7ec3", "\u5927\u6279\u91cf\u8bad\u7ec3"]
          },
          
          {
            id: 16,
            label: "\u751f\u6210\u6a21\u578b\u4e0e\u6269\u6563\u7406\u8bba",
            size: 26,
            keywords: ["\u6807\u51c6\u5316\u6d41", "\u6d41\u6a21\u578b", "\u751f\u6210\u5bf9\u6297\u7f51\u7edc"]
          },
          
          {
            id: 17,
            label: "\u591a\u89c6\u89d2\u4e09\u7ef4\u611f\u77e5\u878d\u5408",
            size: 26,
            keywords: ["\u4e09\u7ef4\u611f\u77e5", "\u4f4d\u7f6e\u7f16\u7801", "\u591a\u89c6\u89d2\u89c6\u89c9"]
          },
          
          {
            id: 18,
            label: "SAR\u6210\u50cf\u4e0e\u56de\u6ce2\u6a21\u62df",
            size: 26,
            keywords: ["SAR\u76ee\u6807\u8bc6\u522b", "\u6027\u80fd\u8bc4\u4f30", "\u8f85\u52a9\u8bc6\u522b\u7cfb\u7edf"]
          },
          
          {
            id: 19,
            label: "\u9ad8\u5206\u8fa8\u7387\u8bed\u4e49\u5206\u5272",
            size: 25,
            keywords: ["LayerCAM", "\u7279\u5f81\u53ef\u89c6\u5316", "U-Net\u7f51\u7edc"]
          },
          
          {
            id: 20,
            label: "\u8d1d\u53f6\u65af\u4e0e\u53ef\u4fe1\u673a\u5668\u5b66\u4e60",
            size: 23,
            keywords: ["\u7279\u5f81\u53ef\u89c6\u5316", "Grad-CAM", "Ablation-CAM"]
          },
          
          {
            id: 21,
            label: "\u673a\u5668\u5b66\u4e60\u5e95\u5c42\u4e0e\u53ef\u5fae\u7f16\u7a0b",
            size: 23,
            keywords: ["\u5e95\u5c42\u7b97\u6cd5", "\u6027\u80fd\u4f18\u5316", "\u53ef\u5fae\u5206\u7f16\u7a0b"]
          },
          
          {
            id: 22,
            label: "\u96f7\u8fbe\u76ee\u6807\u68c0\u6d4b\u4e0e\u57df\u9002\u5e94",
            size: 22,
            keywords: ["\u65cb\u8f6c\u76ee\u6807\u68c0\u6d4b", "\u9065\u611f\u76ee\u6807\u68c0\u6d4b", "SAR\u76ee\u6807\u68c0\u6d4b"]
          },
          
          {
            id: 23,
            label: "\u591a\u4f20\u611f\u5668SLAM\u4e0e\u5b9a\u4f4d",
            size: 19,
            keywords: ["\u7aef\u5230\u7aef\u7cfb\u7edf", "\u7edf\u4e00\u611f\u77e5\u6846\u67b6", "\u81ea\u52a8\u9a7e\u9a76\u89c4\u5212"]
          },
          
          {
            id: 24,
            label: "\u5bf9\u6bd4\u81ea\u76d1\u7763\u89c6\u89c9\u8868\u5f81",
            size: 19,
            keywords: ["\u5bf9\u6bd4\u5b66\u4e60", "\u81ea\u76d1\u7763\u5b66\u4e60", "MoCo"]
          },
          
          {
            id: 25,
            label: "\u591a\u89c6\u56fe\u51e0\u4f55\u4e0eBA\u4f18\u5316",
            size: 17,
            keywords: ["SIFT"]
          },
          
          {
            id: 26,
            label: "\u76ee\u6807\u68c0\u6d4b\u57df\u9002\u5e94\u7efc\u8ff0",
            size: 15,
            keywords: ["\u57df\u81ea\u9002\u5e94", "\u5355\u9636\u6bb5\u68c0\u6d4b", "\u68c0\u6d4b\u5668\u8fc1\u79fb"]
          },
          
          {
            id: 27,
            label: "\u8d85\u5bbd\u5e26\u96f7\u8fbe\u751f\u547d\u63a2\u6d4b",
            size: 11,
            keywords: ["\u65cb\u8f6c\u76ee\u6807\u68c0\u6d4b", "\u4fe1\u53f7\u63d0\u53d6", "\u547c\u5438\u5fc3\u8df3\u4fe1\u53f7"]
          },
          
          {
            id: 28,
            label: "\u5b66\u672f\u5199\u4f5c\u4e0e\u540c\u884c\u8bc4\u8bae",
            size: 8,
            keywords: ["LaTeX", "\u7814\u7a76", "\u5bb6\u5ead\u66b4\u529b"]
          },
          
          {
            id: 29,
            label: "SAM\u901a\u7528\u5206\u5272\u6a21\u578b",
            size: 8,
            keywords: ["\u57fa\u7840\u6a21\u578b", "\u901a\u7528\u5206\u5272", "\u96f6\u6837\u672c\u5206\u5272"]
          }
          
        ];

        const links = [{"source": 6, "target": 18, "value": 0.9245460041775893}, {"source": 16, "target": 20, "value": 0.8905029945165324}, {"source": 3, "target": 4, "value": 0.9240351085742168}, {"source": 21, "target": 28, "value": 0.807792298814496}, {"source": 3, "target": 19, "value": 0.9003362581199531}, {"source": 12, "target": 28, "value": 0.7965857335194855}, {"source": 4, "target": 24, "value": 0.9229497243833011}, {"source": 14, "target": 22, "value": 0.8729053098914227}, {"source": 23, "target": 25, "value": 0.9044885402813032}, {"source": 9, "target": 11, "value": 0.896372048568743}, {"source": 1, "target": 6, "value": 0.9293077804604375}, {"source": 2, "target": 11, "value": 0.9029340481467453}, {"source": 1, "target": 9, "value": 0.8849983410408904}, {"source": 2, "target": 8, "value": 0.8910743200836819}, {"source": 2, "target": 14, "value": 0.8630531982929593}, {"source": 10, "target": 24, "value": 0.8930595552845743}, {"source": 15, "target": 20, "value": 0.919658104917252}, {"source": 12, "target": 15, "value": 0.9129012066128726}, {"source": 12, "target": 21, "value": 0.9064100422570874}, {"source": 4, "target": 17, "value": 0.9004892477998001}, {"source": 4, "target": 26, "value": 0.8922603418101872}, {"source": 5, "target": 15, "value": 0.8622990588282259}, {"source": 17, "target": 23, "value": 0.904974791280689}, {"source": 0, "target": 4, "value": 0.8969761996772723}, {"source": 4, "target": 29, "value": 0.838083002447678}, {"source": 8, "target": 17, "value": 0.8941512032066707}, {"source": 2, "target": 10, "value": 0.9070304524928628}, {"source": 13, "target": 16, "value": 0.938870018132676}, {"source": 11, "target": 22, "value": 0.911731261200046}, {"source": 19, "target": 29, "value": 0.8560283660613789}, {"source": 10, "target": 26, "value": 0.914281594278215}, {"source": 6, "target": 7, "value": 0.9641738631743882}, {"source": 2, "target": 22, "value": 0.9431331580988597}, {"source": 7, "target": 18, "value": 0.9134519995399606}, {"source": 3, "target": 5, "value": 0.8743214868348351}, {"source": 20, "target": 21, "value": 0.8866982067345486}, {"source": 6, "target": 22, "value": 0.9285379569646334}, {"source": 4, "target": 10, "value": 0.9147202825710821}, {"source": 18, "target": 27, "value": 0.8538639692176334}, {"source": 4, "target": 13, "value": 0.8999357949745392}, {"source": 12, "target": 20, "value": 0.9278026187979727}, {"source": 4, "target": 19, "value": 0.8930749301860895}, {"source": 0, "target": 12, "value": 0.903479444778309}, {"source": 1, "target": 7, "value": 0.9398078147560148}, {"source": 17, "target": 25, "value": 0.8929625242860778}, {"source": 9, "target": 27, "value": 0.8746830143829056}];

        const academicColors = [
          '#4E79A7', '#F28E2B', '#E15759', '#76B7B2', '#59A14F',
          '#EDC948', '#B07AA1', '#FF9DA7', '#9C755F', '#BAB0AC',
          '#86BCB6', '#D37295', '#8CD17D', '#499894', '#FABFD2',
          '#79706E', '#D4A6C8', '#9D7660', '#B6992D', '#A0CBE8'
        ];
        const colorScale = (i) => academicColors[i % academicColors.length];

        let svg, simulation, node, link, sizeScale;
        let lastWidth = 0;
        let simulationStopped = false;

        function createGraph() {
          const width = clusterContainer.clientWidth;
          const height = clusterContainer.clientHeight || 280;
          const isMobile = width < 500;

          lastWidth = width;
          clusterContainer.innerHTML = '';

          const minRadius = isMobile ? 16 : 20;
          const maxRadius = isMobile ? 38 : 48;
          sizeScale = d3.scaleSqrt()
            .domain([0, d3.max(clusters, d => d.size)])
            .range([minRadius, maxRadius]);

          svg = d3.select('#cluster-graph')
            .append('svg')
            .attr('width', width)
            .attr('height', height)
            .attr('viewBox', [0, 0, width, height]);

          // Create layered groups for z-order control
          // Order (bottom to top): dimmed links -> dimmed nodes -> highlighted links -> highlighted nodes
          const layerDimmedLinks = svg.append('g').attr('class', 'layer-dimmed-links');
          const layerDimmedNodes = svg.append('g').attr('class', 'layer-dimmed-nodes');
          const layerHighlightedLinks = svg.append('g').attr('class', 'layer-highlighted-links');
          const layerHighlightedNodes = svg.append('g').attr('class', 'layer-highlighted-nodes');

          const chargeStrength = isMobile ? -100 : -150;
          const linkDistance = isMobile ? 60 : 90;

          simulationStopped = false;
          simulation = d3.forceSimulation(clusters)
            .force('link', d3.forceLink(links).id(d => d.id).distance(linkDistance).strength(0.2))
            .force('charge', d3.forceManyBody().strength(chargeStrength))
            .force('center', d3.forceCenter(width / 2, height / 2))
            .force('collision', d3.forceCollide().radius(d => sizeScale(d.size) + (isMobile ? 4 : 6)))
            .velocityDecay(0.7)
            .alphaDecay(0.08)
            .alphaMin(0.01);

          // Initially all links in dimmed layer
          link = layerDimmedLinks
            .attr('stroke', '#e2e8f0')
            .attr('stroke-opacity', 0.6)
            .selectAll('line')
            .data(links)
            .join('line')
            .attr('stroke-width', d => Math.max(1, d.value * 1.5));

          // Initially all nodes in dimmed layer
          node = layerDimmedNodes
            .selectAll('g')
            .data(clusters)
            .join('g')
            .style('cursor', 'grab');

          node.append('circle')
            .attr('r', d => sizeScale(d.size))
            .attr('fill', (d, i) => colorScale(i))
            .attr('fill-opacity', 0.85)
            .attr('stroke', '#fff')
            .attr('stroke-width', isMobile ? 1.5 : 2);

          const fontSize = isMobile ? 7 : 8;
          const lineHeight = isMobile ? 8 : 9;
          const charWidth = isMobile ? 5 : 5.5;

          function wrapText(text, radius) {
            text.each(function() {
              const textEl = d3.select(this);
              const label = textEl.text();
              const maxWidth = radius * 1.2;

              textEl.text(null);

              let lines = [];
              let currentLine = '';
              for (let char of label) {
                if ((currentLine.length + 1) * charWidth > maxWidth && currentLine) {
                  lines.push(currentLine);
                  currentLine = char;
                } else {
                  currentLine += char;
                }
              }
              if (currentLine) lines.push(currentLine);

              if (lines.length > 2) {
                lines = lines.slice(0, 2);
                lines[1] = lines[1].substring(0, Math.max(0, lines[1].length - 1)) + '..';
              }

              const startY = -(lines.length - 1) * lineHeight / 2;

              lines.forEach((line, i) => {
                textEl.append('tspan')
                  .attr('x', 0)
                  .attr('dy', i === 0 ? startY : lineHeight)
                  .text(line);
              });
            });
          }

          node.append('text')
            .text(d => d.label)
            .attr('text-anchor', 'middle')
            .attr('dominant-baseline', 'middle')
            .attr('font-size', fontSize + 'px')
            .attr('font-weight', '500')
            .attr('fill', '#fff')
            .attr('pointer-events', 'none')
            .each(function(d) {
              wrapText(d3.select(this), sizeScale(d.size));
            });

          node.append('text')
            .text(d => `(${d.size}篇)`)
            .attr('text-anchor', 'middle')
            .attr('dy', d => sizeScale(d.size) + (isMobile ? 10 : 12))
            .attr('font-size', fontSize + 'px')
            .attr('fill', '#94a3b8');

          node.append('title')
            .text(d => `${d.label}\n论文数: ${d.size}\n关键词: ${d.keywords.join(', ')}`);

          const drag = d3.drag()
            .on('start', dragstarted)
            .on('drag', dragged)
            .on('end', dragended);

          node.call(drag);

          simulation.on('tick', () => {
            node.attr('transform', d => {
              const r = sizeScale(d.size);
              const padding = isMobile ? 5 : 8;
              const maxY = height - r - (isMobile ? 12 : 15);
              d.x = Math.max(r + padding, Math.min(width - r - padding, d.x));
              d.y = Math.max(r + padding, Math.min(maxY, d.y));
              return `translate(${d.x},${d.y})`;
            });

            link
              .attr('x1', d => d.source.x)
              .attr('y1', d => d.source.y)
              .attr('x2', d => d.target.x)
              .attr('y2', d => d.target.y);
          });

          simulation.on('end', () => {
            simulationStopped = true;
          });

          function dragstarted(event, d) {
            if (!event.active) simulation.alphaTarget(0.05).restart();
            d.fx = d.x;
            d.fy = d.y;
            highlightNode(d);
            d3.select(this).style('cursor', 'grabbing');
          }

          function dragged(event, d) {
            const r = sizeScale(d.size);
            const padding = isMobile ? 5 : 8;
            const maxY = height - r - (isMobile ? 12 : 15);
            d.fx = Math.max(r + padding, Math.min(width - r - padding, event.x));
            d.fy = Math.max(r + padding, Math.min(maxY, event.y));
          }

          function dragended(event, d) {
            if (!event.active) simulation.alphaTarget(0);
            d.fx = null;
            d.fy = null;
            d3.select(this).style('cursor', 'grab');
            resetHighlight();
          }

          function highlightNode(selectedNode) {
            const connectedIds = new Set();

            links.forEach(l => {
              if (l.source.id === selectedNode.id) connectedIds.add(l.target.id);
              if (l.target.id === selectedNode.id) connectedIds.add(l.source.id);
            });

            // Step 1: Style dimmed elements
            link
              .attr('stroke-opacity', 0.1)
              .attr('stroke', '#cbd5e1')
              .attr('stroke-width', d => Math.max(1, d.value * 1.5));

            node.each(function(d) {
              d3.select(this).select('circle')
                .attr('fill-opacity', 0.25)
                .attr('stroke', '#fff')
                .attr('stroke-width', isMobile ? 1.5 : 2);

              d3.select(this).selectAll('text')
                .attr('opacity', 0.3);
            });

            // Step 2: Move dimmed elements to bottom layers
            link.each(function() {
              layerDimmedLinks.node().appendChild(this);
            });
            node.each(function() {
              layerDimmedNodes.node().appendChild(this);
            });

            // Step 3: Style and move highlighted links to top
            link.filter(l => l.source.id === selectedNode.id || l.target.id === selectedNode.id)
              .attr('stroke-opacity', 1)
              .attr('stroke', '#2563eb')
              .attr('stroke-width', d => Math.max(2.5, d.value * 3))
              .each(function() {
                layerHighlightedLinks.node().appendChild(this);
              });

            // Step 4: Style and move connected nodes to top
            node.filter(d => connectedIds.has(d.id))
              .each(function(d) {
                d3.select(this).select('circle')
                  .attr('fill-opacity', 1)
                  .attr('stroke', '#60a5fa')
                  .attr('stroke-width', isMobile ? 2.5 : 3);

                d3.select(this).selectAll('text')
                  .attr('opacity', 1);

                layerHighlightedNodes.node().appendChild(this);
              });

            // Step 5: Style and move selected node to very top
            node.filter(d => d.id === selectedNode.id)
              .each(function(d) {
                d3.select(this).select('circle')
                  .attr('fill-opacity', 1)
                  .attr('stroke', '#2563eb')
                  .attr('stroke-width', isMobile ? 3 : 4);

                d3.select(this).selectAll('text')
                  .attr('opacity', 1);

                layerHighlightedNodes.node().appendChild(this);
              });
          }

          function resetHighlight() {
            // Move all elements back to dimmed layers
            link.each(function() {
              layerDimmedLinks.node().appendChild(this);
            });

            node.each(function() {
              layerDimmedNodes.node().appendChild(this);
            });

            // Reset styles
            node.select('circle')
              .attr('fill-opacity', 0.85)
              .attr('stroke', '#fff')
              .attr('stroke-width', isMobile ? 1.5 : 2)
              .style('filter', 'none');

            link
              .attr('stroke-opacity', 0.6)
              .attr('stroke', '#e2e8f0')
              .attr('stroke-width', d => Math.max(1, d.value * 1.5));

            node.selectAll('text')
              .attr('opacity', 1);
          }
        }

        createGraph();

        let resizeTimeout;
        window.addEventListener('resize', () => {
          clearTimeout(resizeTimeout);
          resizeTimeout = setTimeout(() => {
            const newWidth = clusterContainer.clientWidth;
            if (Math.abs(newWidth - lastWidth) > 10) {
              clusters.forEach(d => { d.x = undefined; d.y = undefined; d.fx = null; d.fy = null; });
              createGraph();
            }
          }, 250);
        });
      }
      
    });
  </script>
  

  
  <section class="py-5 md:py-8 border-b border-border-color">
    <div class="content-container">
      <button id="btn-overall-summaries" onclick="toggleSection('overall-summaries')"
              class="w-full text-left text-lg md:text-xl font-bold text-text-primary mb-3 md:mb-4 flex items-center justify-between group hover:text-accent transition-colors">
        <span class="flex items-center gap-2 flex-1 min-w-0">
          <svg class="w-5 h-5 flex-shrink-0 text-accent" fill="currentColor" viewBox="0 0 24 24">
            <path d="M12 2a2 2 0 012 2c0 .74-.4 1.39-1 1.73V7h1a7 7 0 017 7h1a1 1 0 011 1v3a1 1 0 01-1 1h-1v1a2 2 0 01-2 2H5a2 2 0 01-2-2v-1H2a1 1 0 01-1-1v-3a1 1 0 011-1h1a7 7 0 017-7h1V5.73c-.6-.34-1-.99-1-1.73a2 2 0 012-2z"/>
          </svg>
          <span class="truncate">本期研究趋势概览</span>
        </span>
        <span class="flex items-center gap-1 md:gap-2 flex-shrink-0 ml-2">
          <span class="hidden md:inline text-sm font-normal text-text-secondary group-hover:text-accent" id="hint-overall-summaries">点击收起</span>
          <svg class="w-5 h-5 md:w-4 md:h-4 text-text-secondary group-hover:text-accent transform transition-transform rotate-180" id="icon-overall-summaries" fill="none" stroke="currentColor" viewBox="0 0 24 24">
            <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
          </svg>
        </span>
      </button>

      <div id="overall-summaries" class="section-expand expanded">
      <div class="space-y-4 md:space-y-6">
        
        <div class="bg-bg-card rounded-lg border border-border-color p-4 md:p-5">
          <h3 class="text-base font-semibold text-text-primary mb-3 flex items-center gap-2">
            <svg class="w-4 h-4 text-accent" fill="currentColor" viewBox="0 0 20 20">
              <path d="M9.049 2.927c.3-.921 1.603-.921 1.902 0l1.07 3.292a1 1 0 00.95.69h3.462c.969 0 1.371 1.24.588 1.81l-2.8 2.034a1 1 0 00-.364 1.118l1.07 3.292c.3.921-.755 1.688-1.54 1.118l-2.8-2.034a1 1 0 00-1.175 0l-2.8 2.034c-.784.57-1.838-.197-1.539-1.118l1.07-3.292a1 1 0 00-.364-1.118L2.98 8.72c-.783-.57-.38-1.81.588-1.81h3.461a1 1 0 00.951-.69l1.07-3.292z"/>
            </svg>
            精选推荐总结
          </h3>
          <div class="text-sm text-text-primary leading-relaxed space-y-2">
            <p>本期推荐涵盖2篇目标识别与分类、2篇多模态融合分割与1篇跨模态重识别的论文。</p>
            
            <p><strong class="text-accent">目标识别分类</strong>：《TFST》提出YOLOv12+特征匹配的两帧SAR舰船跟踪框架，应对散斑噪声与海杂波；《Metaformer-like Convolutional Neural Networks and Learnable Decision Fusion for SAR Ship Classification》设计类Metaformer CNN与可学习决策融合，提升SAR舰船分类精度。</p>
            
            <p><strong class="text-accent">多模态分割</strong>：《RSRefSeg 2》利用基础模型解耦视觉-语言协同，实现指代性遥感影像分割；《Knowledge-Aware Progressive Fusion Network for Heterogeneous Remote Sensing Image Semantic Segmentation》通过知识引导渐进融合光学与SAR数据，提升异构影像语义分割性能。</p>
            
            <p><strong class="text-accent">跨模态重识别</strong>：《Beyond Weight Adaptation: Feature-Space Domain Injection for Cross-Modal Ship Re-Identification》在特征空间注入域信息，缓解红外-可见光舰船重识别的模态差异，实现全天候海事目标关联。</p>
            
          </div>
        </div>
        

        
        <div class="bg-bg-card rounded-lg border border-border-color p-4 md:p-5">
          <h3 class="text-base font-semibold text-text-primary mb-3 flex items-center gap-2">
            <svg class="w-4 h-4 text-text-secondary" fill="none" stroke="currentColor" viewBox="0 0 24 24">
              <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M9 19v-6a2 2 0 00-2-2H5a2 2 0 00-2 2v6a2 2 0 002 2h2a2 2 0 002-2zm0 0V9a2 2 0 012-2h2a2 2 0 012 2v10m-6 0a2 2 0 002 2h2a2 2 0 002-2m0 0V5a2 2 0 012-2h2a2 2 0 012 2v14a2 2 0 01-2 2h-2a2 2 0 01-2-2z"/>
            </svg>
            相似度推荐总结
          </h3>
          <div class="text-sm text-text-primary leading-relaxed space-y-2">
            <p>本期推荐涵盖了8篇关于多任务/域适应与泛化的论文、6篇关于扩散模型与生成优化的论文、5篇关于视觉-语言与跨模态感知的论文、4篇关于小目标与伪装检测的论文、3篇关于3D感知与多模态融合的论文、2篇关于自监督与知识蒸馏的论文以及2篇关于Re-ID与行人检索的论文。</p>
            
            <p><strong class="text-text-secondary">多任务域适应</strong>：该主题聚焦在冲突梯度抑制、小样本无监督域适应及持续学习。《Toward Unified Expertise》提出统一多任务视觉模型以缓解梯度冲突；《E²MPL》用轻量级元提示实现小样本UDA；《FRFSL》通过特征重建完成跨域湿地分类；《Forget Me Not》融合局部与全局知识抑制过拟合；另有3篇工作分别探索因果去偏、持续提示与跨域检测。</p>
            
            <p><strong class="text-text-secondary">扩散模型优化</strong>：研究围绕加速采样、自监督形状重建与条件生成。《You Only Look One Step》在反向传播中引入梯度捷径，将扩散采样步数减至1；《Diffusion-Driven Self-Supervised Learning》用扩散先验自监督重建类别级6D姿态；其余4篇分别把扩散用于图像超分、文本驱动编辑、视频预测与隐式神经渲染，显著降低推理成本。</p>
            
            <p><strong class="text-text-secondary">视觉语言融合</strong>：探索大模型时代下文本-视觉协同在检索、检测与分割中的应用。《Vision-Language Models for Person Re-identification: A Survey》系统梳理了CLIP类模型在ReID中的提示、对齐与检索策略；其余4篇分别提出开放词汇检测、指代表达分割、图文匹配重排序与多模态语义对齐，持续提升零样本与少样本性能。</p>
            
            <p><strong class="text-text-secondary">小目标伪装检测</strong>：针对红外小目标和光谱伪装物体，研究边缘-语义协同与因果去偏。《Edge-Semantic Synergy Network》设计边缘感知注意力提升红外小目标信噪比；《Causal HyperPrompter》用因果干预消除光谱跟踪中的背景-物体混淆；另两篇分别引入频域增强与跨尺度特征融合，实现复杂背景下的稳健检测。</p>
            
            <p><strong class="text-text-secondary">3D多模态感知</strong>：关注自动驾驶场景下视觉-激光雷达的高效融合。《LiteFusion》以最小参数增量将纯视觉3D检测器扩展为多模态；其余两篇分别提出稀疏交叉注意力与深度-语义一致性正则，降低计算量并提升远距离目标定位精度。</p>
            
            <p><strong class="text-text-secondary">自监督蒸馏</strong>：通过自监督预训练与知识蒸馏提升低标注场景性能。一篇工作利用对比掩码自监督生成高判别表征，另一篇将教师模型注意力迁移给学生，实现标签高效分类与检测。</p>
            
            <p><strong class="text-text-secondary">行人重识别</strong>：聚焦在遮挡、跨模态与无监督场景下的身份保持。两篇论文分别提出基于局部-全局特征对齐与相机感知聚类，实现无标注数据下的稳健Re-ID。</p>
            
          </div>
        </div>
        
      </div>
      </div>
    </div>
  </section>
  

  
  <section class="py-5 md:py-8 border-b border-border-color">
    <div class="content-container">
      <div class="mb-3 md:mb-4">
        <button id="btn-interest-works" onclick="toggleSection('interest-works')"
                class="w-full text-left text-lg md:text-xl font-bold text-text-primary flex items-center justify-between group hover:text-accent transition-colors">
          <span class="flex items-center gap-2 flex-1 min-w-0">
            <svg class="w-5 h-5 flex-shrink-0 text-accent" fill="currentColor" viewBox="0 0 20 20">
              <path d="M9.049 2.927c.3-.921 1.603-.921 1.902 0l1.07 3.292a1 1 0 00.95.69h3.462c.969 0 1.371 1.24.588 1.81l-2.8 2.034a1 1 0 00-.364 1.118l1.07 3.292c.3.921-.755 1.688-1.54 1.118l-2.8-2.034a1 1 0 00-1.175 0l-2.8 2.034c-.784.57-1.838-.197-1.539-1.118l1.07-3.292a1 1 0 00-.364-1.118L2.98 8.72c-.783-.57-.38-1.81.588-1.81h3.461a1 1 0 00.951-.69l1.07-3.292z"/>
            </svg>
            <span class="truncate">精选推荐</span>
          </span>
          <span class="flex items-center gap-1 md:gap-2 flex-shrink-0 ml-2">
            <span class="hidden md:inline text-xs font-normal text-text-secondary">基于研究兴趣匹配，共 5 篇</span>
            <span class="hidden md:inline text-sm font-normal text-text-secondary group-hover:text-accent" id="hint-interest-works">点击收起</span>
            <svg class="w-5 h-5 md:w-4 md:h-4 text-text-secondary group-hover:text-accent transform transition-transform rotate-180" id="icon-interest-works" fill="none" stroke="currentColor" viewBox="0 0 24 24">
              <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
            </svg>
          </span>
        </button>
        <p class="text-xs text-text-secondary mt-1 ml-7 md:hidden">基于研究兴趣匹配，共 5 篇</p>
      </div>

      <div id="interest-works" class="section-expand expanded">
      <div class="space-y-4 md:space-y-5">
        
        <article class="bg-bg-card rounded-lg border border-border-color overflow-hidden hover:border-accent/50 transition-colors">
          <div class="p-4 md:p-5">
            <div class="mb-3">
              <div class="flex items-center gap-2 mb-2 flex-wrap">
                <span class="inline-flex items-center px-2.5 py-0.5 rounded text-xs font-medium bg-accent/10 text-accent border border-accent/30">
                  匹配度 75%
                </span>
                <span class="text-xs text-text-secondary">arxiv</span>
                <span class="text-xs text-text-secondary md:hidden">· 2025-12-24</span>
              </div>
              <div class="flex items-start justify-between gap-3 md:gap-4">
                <div class="flex-1">
                  <h2 class="text-base md:text-lg font-semibold text-text-primary leading-tight">
                    <a href="https://arxiv.org/abs/2512.20892v1" target="_blank" rel="noopener" class="hover:text-accent transition-colors">
                      Beyond Weight Adaptation: Feature-Space Domain Injection for Cross-Modal Ship Re-Identification
                    </a>
                  </h2>
                  
                  <p class="text-sm text-text-secondary mt-0.5 leading-snug">超越权重自适应：用于跨模态船舶再识别的特征空间域注入</p>
                  
                </div>
                <div class="hidden md:block text-right text-sm text-text-secondary flex-shrink-0">
                  <div class="font-medium">2025-12-24</div>
                </div>
              </div>
              <div class="mt-1 text-xs text-text-secondary" title="arXiv">
                arXiv
                
              </div>
            </div>

            <p class="text-sm text-text-secondary mb-3">
              Tingfeng Xian，Wenlve Zhou，Zhiheng Zhou，Zhelin Li
            </p>

            <div class="flex flex-wrap gap-x-4 gap-y-1 text-xs text-text-secondary mb-4">
              
              <span class="flex items-center">
                <span class="font-medium mr-1">ID:</span>
                <span>http://arxiv.org/abs/2512.20892v1</span>
              </span>
              
            </div>

            
            <div class="bg-bg-hover/50 rounded-lg p-4 mb-4 border border-border-color">
              <button id="btn-interest-abstract-1" onclick="toggleSection('interest-abstract-1')"
                      class="w-full text-left text-sm font-medium text-text-primary flex items-center justify-between">
                <span class="flex items-center">
                  <svg class="w-4 h-4 mr-1.5 text-text-secondary" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                    <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M9 12h6m-6 4h6m2 5H7a2 2 0 01-2-2V5a2 2 0 012-2h5.586a1 1 0 01.707.293l5.414 5.414a1 1 0 01.293.707V19a2 2 0 01-2 2z"/>
                  </svg>
                  原文摘要
                </span>
                <svg class="w-4 h-4 text-text-secondary transform transition-transform" id="icon-interest-abstract-1" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                  <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
                </svg>
              </button>
              <div id="interest-abstract-1" class="section-expand collapsed">
                <p class="text-sm text-text-secondary mt-3 leading-relaxed">Cross-Modality Ship Re-Identification (CMS Re-ID) is critical for achieving all-day and all-weather maritime target tracking, yet it is fundamentally challenged by significant modality discrepancies. Mainstream solutions typically rely on explicit modality alignment strategies; however, this paradigm heavily depends on constructing large-scale paired datasets for pre-training. To address this, grounded in the Platonic Representation Hypothesis, we explore the potential of Vision Foundation Models (VFMs) in bridging modality gaps. Recognizing the suboptimal performance of existing generic Parameter-Efficient Fine-Tuning (PEFT) methods that operate within the weight space, particularly on limited-capacity models, we shift the optimization perspective to the feature space and propose a novel PEFT strategy termed Domain Representation Injection (DRI). Specifically, while keeping the VFM fully frozen to maximize the preservation of general knowledge, we design a lightweight, learnable Offset Encoder to extract domain-specific representations rich in modality and identity attributes from raw inputs. Guided by the contextual information of intermediate features at different layers, a Modulator adaptively transforms these representations. Subsequently, they are injected into the intermediate layers via additive fusion, dynamically reshaping the feature distribution to adapt to the downstream task without altering the VFM&#39;s pre-trained weights. Extensive experimental results demonstrate the superiority of our method, achieving State-of-the-Art (SOTA) performance with minimal trainable parameters. For instance, on the HOSS-ReID dataset, we attain 57.9\% and 60.5\% mAP using only 1.54M and 7.05M parameters, respectively. The code is available at https://github.com/TingfengXian/DRI.</p>
              </div>
            </div>
            

            
            <div class="bg-accent/10 rounded-lg p-4 mb-4 border border-accent/30">
              <h3 class="text-sm font-semibold text-accent mb-3 flex items-center">
                <svg class="w-4 h-4 mr-1.5" fill="currentColor" viewBox="0 0 24 24">
                  <path d="M12 2a2 2 0 012 2c0 .74-.4 1.39-1 1.73V7h1a7 7 0 017 7h1a1 1 0 011 1v3a1 1 0 01-1 1h-1v1a2 2 0 01-2 2H5a2 2 0 01-2-2v-1H2a1 1 0 01-1-1v-3a1 1 0 011-1h1a7 7 0 017-7h1V5.73c-.6-.34-1-.99-1-1.73a2 2 0 012-2z"/>
                </svg>
                AI 总结
              </h3>
              <div class="space-y-2 text-sm text-text-primary">
                <p><span class="font-medium text-accent">研究问题：</span>解决跨模态船舶重识别中模态差异大、依赖大规模配对数据预训练的问题。</p>
                <p><span class="font-medium text-accent">研究方法：</span>冻结视觉基础模型，在特征空间注入轻量级域表示，通过Offset Encoder与Modulator动态重塑特征分布。</p>
                <p><span class="font-medium text-accent">主要发现：</span>HOSS-ReID上仅用1.54M/7.05M参数即达57.9%/60.5% mAP，实现SOTA且参数量极小。</p>
                <p><span class="font-medium text-accent">创新点：</span>首次将优化从权重空间转向特征空间，提出域表示注入DRI，无需配对数据即可桥接模态鸿沟。</p>
                
                <p><span class="font-medium text-accent">相关性：</span>为全天候海事目标跟踪提供高效小样本跨模态方案，拓展PEFT在有限模型容量下的应用思路。</p>
                
              </div>

              <div class="mt-3 pt-3 border-t border-accent/30">
                <button id="btn-interest-detail-1" onclick="toggleSection('interest-detail-1')"
                        class="text-xs text-accent hover:text-accent-hover font-medium flex items-center">
                  <svg class="w-3 h-3 mr-1" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                    <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
                  </svg>
                  查看详细分析
                </button>
                <div id="interest-detail-1" class="section-expand collapsed mt-3">
                  <div class="bg-bg-card rounded-lg p-4 text-sm text-text-primary space-y-3 border border-border-color">
                    <p><span class="font-medium text-text-primary">研究背景：</span>全天候船舶再识别需要跨可见光-红外模态匹配，但模态差异极大；现有方法依赖大规模成对数据做显式对齐，成本高且难扩展。作者受柏拉图表征假说启发，尝试用冻结的视觉基础模型(VFM)统一两种模态，从而摆脱对成对数据的依赖。</p>
                    <p><span class="font-medium text-text-primary">方法详情：</span>论文提出Domain Representation Injection(DRI)：保持VFM权重完全冻结，仅引入轻量级Offset Encoder从原始图像提取富含模态与身份信息的域表征；随后Modulator利用各层中间特征的上下文自适应变换该表征，并以残差形式注入到VFM的不同中间层，实现特征空间而非权重空间的模态融合。整个流程仅训练1.54M-7M参数，却动态重塑特征分布以适应下游跨模态Re-ID任务。</p>
                    <p><span class="font-medium text-text-primary">研究结果：</span>在HOSS-ReID数据集上，DRI以1.54M参数取得57.9% mAP，以7.05M参数进一步提升至60.5% mAP，显著优于现有SOTA，同时可训练参数量减少一个数量级；消融实验表明冻结VFM+特征注入比传统权重微调或通用PEFT方法在跨模态检索与身份一致性上均更稳健。</p>
                    <p><span class="font-medium text-text-primary">局限性：</span>方法目前仅在船舶场景验证，尚未测试于通用行人或车辆跨模态Re-ID；Offset Encoder与Modulator的设计依赖VFM的层级结构，换用不同架构时需重新调整注入点与维度；完全冻结VFM虽保留通用知识，但可能限制对极端模态畸变的适应能力。</p>
                    
                    <p><span class="font-medium text-text-primary">未来方向：</span>后续可探索DRI在更多跨模态视觉任务上的可迁移性，并研究自适应选择注入层与表征维度的自动化机制，以进一步压缩参数并提升泛化。</p>
                    
                    <p class="text-accent"><span class="font-medium">研究相关性：</span>若研究者关注跨模态表征学习、参数高效微调或海洋视觉监控，本文提供的特征空间注入范式与极低成本训练策略可直接借鉴并扩展到其他领域。</p>
                  </div>
                </div>
              </div>
            </div>
            

            <div class="pt-4 border-t border-border-color">
              <div class="flex flex-wrap gap-3 text-xs">
                <span class="px-2 py-1 bg-bg-hover rounded text-text-secondary">相似度 0.75</span>
                <span class="px-2 py-1 bg-bg-hover rounded text-text-secondary">
                  期刊影响力 0.60
                  
                </span>
              </div>
            </div>
          </div>
        </article>
        
        <article class="bg-bg-card rounded-lg border border-border-color overflow-hidden hover:border-accent/50 transition-colors">
          <div class="p-4 md:p-5">
            <div class="mb-3">
              <div class="flex items-center gap-2 mb-2 flex-wrap">
                <span class="inline-flex items-center px-2.5 py-0.5 rounded text-xs font-medium bg-accent/10 text-accent border border-accent/30">
                  匹配度 68%
                </span>
                <span class="text-xs text-text-secondary">crossref</span>
                <span class="text-xs text-text-secondary md:hidden">· 2025-12-24</span>
              </div>
              <div class="flex items-start justify-between gap-3 md:gap-4">
                <div class="flex-1">
                  <h2 class="text-base md:text-lg font-semibold text-text-primary leading-tight">
                    <a href="https://doi.org/10.1109/jstars.2025.3647680" target="_blank" rel="noopener" class="hover:text-accent transition-colors">
                      TFST: Two-Frame Ship Tracking for SAR Using YOLOv12 and Feature-Based Matching
                    </a>
                  </h2>
                  
                  <p class="text-sm text-text-secondary mt-0.5 leading-snug">TFST：基于YOLOv12与特征匹配的双帧SAR船舶跟踪</p>
                  
                </div>
                <div class="hidden md:block text-right text-sm text-text-secondary flex-shrink-0">
                  <div class="font-medium">2025-12-24</div>
                </div>
              </div>
              <div class="mt-1 text-xs text-text-secondary" title="IEEE Journal of Selected Topics in Applied Earth Observations and Remote Sensing">
                IEEE Journal of Selected Topics in Applied Earth Observations and Remote Sensing
                
                  <span class="ml-1 text-blue-600">(IF: 5.5)</span>
                
              </div>
            </div>

            <p class="text-sm text-text-secondary mb-3">
              Muhammad Yasir，Shanwei Liu，Mingming Xu，Fernando J. Aguilar，Jianhua Wan 等
            </p>

            <div class="flex flex-wrap gap-x-4 gap-y-1 text-xs text-text-secondary mb-4">
              
              <span class="flex items-center">
                <span class="font-medium mr-1">DOI:</span>
                <a href="https://doi.org/10.1109/jstars.2025.3647680" target="_blank" rel="noopener" class="text-accent hover:text-accent-hover hover:underline">10.1109/jstars.2025.3647680</a>
              </span>
              
            </div>

            
            <div class="bg-bg-hover/50 rounded-lg p-4 mb-4 border border-border-color">
              <button id="btn-interest-abstract-2" onclick="toggleSection('interest-abstract-2')"
                      class="w-full text-left text-sm font-medium text-text-primary flex items-center justify-between">
                <span class="flex items-center">
                  <svg class="w-4 h-4 mr-1.5 text-text-secondary" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                    <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M9 12h6m-6 4h6m2 5H7a2 2 0 01-2-2V5a2 2 0 012-2h5.586a1 1 0 01.707.293l5.414 5.414a1 1 0 01.293.707V19a2 2 0 01-2 2z"/>
                  </svg>
                  原文摘要
                </span>
                <svg class="w-4 h-4 text-text-secondary transform transition-transform" id="icon-interest-abstract-2" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                  <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
                </svg>
              </button>
              <div id="interest-abstract-2" class="section-expand collapsed">
                <p class="text-sm text-text-secondary mt-3 leading-relaxed">Tracking objects in SAR imagery is critical for maritime surveillance, traffic monitoring, and security applications, but remains a major challenge due to speckle noise, sea clutter, and limited temporal continuity. Most existing tracking-by-detection methods process frames independently, often resulting in weak associations and frequent identity switches. To overcome these limitations, we propose TFST, a two-frame SAR ship tracking framework that integrates detection, feature encoding, and optimal assignment. In this way, the goal of this work is to address the current gaps in SAR ship tracking by strengthening cross-frame partnerships and reducing identity switches through an integrated two-frame tracking framework. In our approach, a deep detector first processes consecutive frames to generate candidate bounding boxes. A lightweight feature extractor encodes both appearance and structural cues, while a matching module constructs a cost matrix that combines feature similarity and positional consistency. Gating is applied to remove infeasible associations, and the Hungarian algorithm is employed to achieve a globally optimal assignment. Quantitative evaluations performed on three widely known and publicly available SAR-Ship datasets (SSTD, SSDD, and SAR-Ship) further highlight the advantages of TFST. In terms of ship detection performance, TFST achieved an average mAP@50 improvement of 2.2% over the YOLOv12 baseline model on all three tested datasets. Regarding tracking results, the superiority of TFST over state-of-the-art multi-object trackers becomes even more evident. In fact, the proposed model achieved the highest MOTA accuracy (86.9%) and the best IDF1 score (82.7%), thus outperforming strong baselines such as Siam-SORT (82.1% MOTA, 79.8% IDF1) and TrackFormer (80.7% MOTA, 78.7% IDF1). In conclusion, TFST demonstrated improved robustness, fewer ID switches, and higher tracking accuracy compared to baseline methods, underscoring its effectiveness in complex m...</p>
              </div>
            </div>
            

            
            <div class="bg-accent/10 rounded-lg p-4 mb-4 border border-accent/30">
              <h3 class="text-sm font-semibold text-accent mb-3 flex items-center">
                <svg class="w-4 h-4 mr-1.5" fill="currentColor" viewBox="0 0 24 24">
                  <path d="M12 2a2 2 0 012 2c0 .74-.4 1.39-1 1.73V7h1a7 7 0 017 7h1a1 1 0 011 1v3a1 1 0 01-1 1h-1v1a2 2 0 01-2 2H5a2 2 0 01-2-2v-1H2a1 1 0 01-1-1v-3a1 1 0 011-1h1a7 7 0 017-7h1V5.73c-.6-.34-1-.99-1-1.73a2 2 0 012-2z"/>
                </svg>
                AI 总结
              </h3>
              <div class="space-y-2 text-sm text-text-primary">
                <p><span class="font-medium text-accent">研究问题：</span>解决SAR图像舰船跟踪中因散斑噪声、海杂波和帧间不连续导致的身份切换频繁、关联弱的问题。</p>
                <p><span class="font-medium text-accent">研究方法：</span>提出TFST两帧跟踪框架：YOLOv12检测→轻量特征提取→特征+位置代价矩阵→门控过滤→匈牙利最优匹配。</p>
                <p><span class="font-medium text-accent">主要发现：</span>在SSTD/SSDD/SAR-Ship上，mAP@50提升2.2%，MOTA达86.9%，IDF1达82.7%，均优于Siam-SORT与TrackFormer。</p>
                <p><span class="font-medium text-accent">创新点：</span>首次将YOLOv12与两帧特征-位置联合代价模型结合，用门控+匈牙利实现SAR舰船低切换全局最优跟踪。</p>
                
                <p><span class="font-medium text-accent">相关性：</span>为海事监视提供高鲁棒低切换的SAR舰船跟踪基线，可直接增强遥感、交通与安全应用效能。</p>
                
              </div>

              <div class="mt-3 pt-3 border-t border-accent/30">
                <button id="btn-interest-detail-2" onclick="toggleSection('interest-detail-2')"
                        class="text-xs text-accent hover:text-accent-hover font-medium flex items-center">
                  <svg class="w-3 h-3 mr-1" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                    <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
                  </svg>
                  查看详细分析
                </button>
                <div id="interest-detail-2" class="section-expand collapsed mt-3">
                  <div class="bg-bg-card rounded-lg p-4 text-sm text-text-primary space-y-3 border border-border-color">
                    <p><span class="font-medium text-text-primary">研究背景：</span>SAR图像中的船舶跟踪对海上监视至关重要，但受斑点噪声、海杂波和帧间连续性差的影响，传统逐帧检测方法常出现关联弱、ID频繁切换的问题。作者观察到现有跟踪-检测框架在SAR场景下缺乏跨帧约束，难以维持目标身份一致性，因此提出仅用两帧即可实现稳健关联的思路。</p>
                    <p><span class="font-medium text-text-primary">方法详情：</span>TFST采用YOLOv12作为深度检测器，在连续两帧上生成候选框后，用轻量CNN提取外观与结构双重特征；通过余弦相似度与IoH（中心-尺度一致性）构建联合代价矩阵，并设置门控阈值剔除不可能匹配；最后利用匈牙利算法获得全局最优分配，实现检测-特征-关联一体化。整个流程仅依赖两帧，无需复杂时序模型或重识别网络，兼顾效率与精度。</p>
                    <p><span class="font-medium text-text-primary">研究结果：</span>在SSTD、SSDD、SAR-Ship三个公开数据集上，TFST将YOLOv12基线的mAP@50平均提升2.2%，检测端已优于专用SAR检测器；跟踪指标方面，TFST取得86.9% MOTA与82.7% IDF1，比Siam-SORT和TrackFormer分别降低约4-6个百分点的ID切换，且在高杂波、密集船舶场景下仍保持鲁棒。结果表明，两帧约束足以显著抑制身份漂移，同时保持实时性。</p>
                    <p><span class="font-medium text-text-primary">局限性：</span>论文仅验证了两帧关联，尚未探讨更长时序或丢失恢复能力；特征提取器仍基于通用CNN，未充分融入SAR物理散射特性；实验数据集规模有限，缺乏极端天气、大视角变化等更具挑战性的场景验证。</p>
                    
                    <p><span class="font-medium text-text-primary">未来方向：</span>未来可引入可学习的SAR散射特征嵌入，并扩展为滑窗多帧或递归滤波框架以提升长时跟踪能力；同时构建更大规模、多传感器、多极化的SAR船舶跟踪基准，以进一步验证泛化性。</p>
                    
                    <p class="text-accent"><span class="font-medium">研究相关性：</span>若研究者关注SAR目标跟踪、轻量化检测-跟踪一体化设计，或希望在资源受限平台实现实时海上监视，TFST提供的两帧最优分配范式与公开代码可作为高效 baseline，并可直接与新的SAR特征提取或门控策略结合。</p>
                  </div>
                </div>
              </div>
            </div>
            

            <div class="pt-4 border-t border-border-color">
              <div class="flex flex-wrap gap-3 text-xs">
                <span class="px-2 py-1 bg-bg-hover rounded text-text-secondary">相似度 0.73</span>
                <span class="px-2 py-1 bg-bg-hover rounded text-text-secondary">
                  期刊影响力 0.58
                  
                    <span class="ml-1 text-blue-600">(IF: 5.5)</span>
                  
                </span>
              </div>
            </div>
          </div>
        </article>
        
        <article class="bg-bg-card rounded-lg border border-border-color overflow-hidden hover:border-accent/50 transition-colors">
          <div class="p-4 md:p-5">
            <div class="mb-3">
              <div class="flex items-center gap-2 mb-2 flex-wrap">
                <span class="inline-flex items-center px-2.5 py-0.5 rounded text-xs font-medium bg-accent/10 text-accent border border-accent/30">
                  匹配度 63%
                </span>
                <span class="text-xs text-text-secondary">crossref</span>
                <span class="text-xs text-text-secondary md:hidden">· 2025-12-24</span>
              </div>
              <div class="flex items-start justify-between gap-3 md:gap-4">
                <div class="flex-1">
                  <h2 class="text-base md:text-lg font-semibold text-text-primary leading-tight">
                    <a href="https://doi.org/10.3390/rs18010053" target="_blank" rel="noopener" class="hover:text-accent transition-colors">
                      Metaformer-like Convolutional Neural Networks and Learnable Decision Fusion for SAR Ship Classification
                    </a>
                  </h2>
                  
                  <p class="text-sm text-text-secondary mt-0.5 leading-snug">类Metaformer卷积神经网络与可学习决策融合用于SAR船舶分类</p>
                  
                </div>
                <div class="hidden md:block text-right text-sm text-text-secondary flex-shrink-0">
                  <div class="font-medium">2025-12-24</div>
                </div>
              </div>
              <div class="mt-1 text-xs text-text-secondary" title="Remote Sensing">
                Remote Sensing
                
                  <span class="ml-1 text-blue-600">(IF: 4.1)</span>
                
              </div>
            </div>

            <p class="text-sm text-text-secondary mb-3">
              Shanhong Guo，Hairui Zhu，Ji Zhu，Weixing Sheng，Jiachen Tan
            </p>

            <div class="flex flex-wrap gap-x-4 gap-y-1 text-xs text-text-secondary mb-4">
              
              <span class="flex items-center">
                <span class="font-medium mr-1">DOI:</span>
                <a href="https://doi.org/10.3390/rs18010053" target="_blank" rel="noopener" class="text-accent hover:text-accent-hover hover:underline">10.3390/rs18010053</a>
              </span>
              
            </div>

            
            <div class="bg-bg-hover/50 rounded-lg p-4 mb-4 border border-border-color">
              <button id="btn-interest-abstract-3" onclick="toggleSection('interest-abstract-3')"
                      class="w-full text-left text-sm font-medium text-text-primary flex items-center justify-between">
                <span class="flex items-center">
                  <svg class="w-4 h-4 mr-1.5 text-text-secondary" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                    <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M9 12h6m-6 4h6m2 5H7a2 2 0 01-2-2V5a2 2 0 012-2h5.586a1 1 0 01.707.293l5.414 5.414a1 1 0 01.293.707V19a2 2 0 01-2 2z"/>
                  </svg>
                  原文摘要
                </span>
                <svg class="w-4 h-4 text-text-secondary transform transition-transform" id="icon-interest-abstract-3" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                  <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
                </svg>
              </button>
              <div id="interest-abstract-3" class="section-expand collapsed">
                <p class="text-sm text-text-secondary mt-3 leading-relaxed">With the increasing number of the ocean ships, the demand for synthetic aperture radar (SAR) image ship classification has been much increased. With the development of deep learning, many neural network-based ship classification methods have been presented. However, these networks show unsatisfactory performance on low-quality SAR ship datasets. In this paper, we propose a SAR ship classification method based on dual Metaformer-like networks and learnable decision fusion, which we call LDF-D-MLCNNs. First, we design a Metaformer-like convolutional block to improve learning performance. Secondly, we implement two networks with different kernel sizes and propose the learnable decision fusion module to obtain the final prediction. Kernels of different sizes exhibit diverse extraction capabilities. Experimental results show that the accuracy of the proposed method outperforms many existing SAR ship classification networks.</p>
              </div>
            </div>
            

            
            <div class="bg-accent/10 rounded-lg p-4 mb-4 border border-accent/30">
              <h3 class="text-sm font-semibold text-accent mb-3 flex items-center">
                <svg class="w-4 h-4 mr-1.5" fill="currentColor" viewBox="0 0 24 24">
                  <path d="M12 2a2 2 0 012 2c0 .74-.4 1.39-1 1.73V7h1a7 7 0 017 7h1a1 1 0 011 1v3a1 1 0 01-1 1h-1v1a2 2 0 01-2 2H5a2 2 0 01-2-2v-1H2a1 1 0 01-1-1v-3a1 1 0 011-1h1a7 7 0 017-7h1V5.73c-.6-.34-1-.99-1-1.73a2 2 0 012-2z"/>
                </svg>
                AI 总结
              </h3>
              <div class="space-y-2 text-sm text-text-primary">
                <p><span class="font-medium text-accent">研究问题：</span>提升低质量SAR舰船图像的分类准确率</p>
                <p><span class="font-medium text-accent">研究方法：</span>双分支Metaformer-like CNN配合可学习决策融合</p>
                <p><span class="font-medium text-accent">主要发现：</span>所提LDF-D-MLCNNs精度优于现有SAR舰船分类网络</p>
                <p><span class="font-medium text-accent">创新点：</span>Metaformer-like卷积块与多核可学习决策融合模块</p>
                
                <p><span class="font-medium text-accent">相关性：</span>为低质量SAR数据提供鲁棒舰船识别新基准</p>
                
              </div>

              <div class="mt-3 pt-3 border-t border-accent/30">
                <button id="btn-interest-detail-3" onclick="toggleSection('interest-detail-3')"
                        class="text-xs text-accent hover:text-accent-hover font-medium flex items-center">
                  <svg class="w-3 h-3 mr-1" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                    <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
                  </svg>
                  查看详细分析
                </button>
                <div id="interest-detail-3" class="section-expand collapsed mt-3">
                  <div class="bg-bg-card rounded-lg p-4 text-sm text-text-primary space-y-3 border border-border-color">
                    <p><span class="font-medium text-text-primary">研究背景：</span>随着全球船舶数量激增，对合成孔径雷达(SAR)图像中的船只进行自动分类的需求急剧上升。现有深度学习方法在低质量、低分辨率SAR数据集上精度骤降，难以满足实际海洋监视需求。</p>
                    <p><span class="font-medium text-text-primary">方法详情：</span>作者提出LDF-D-MLCNNs框架，首先设计了一种Metaformer式卷积块，将局部卷积与全局通道注意力耦合以增强特征表达；随后并行构建两个仅kernel size不同的CNN分支，分别捕获细粒度纹理与更大范围散射结构；最后引入可学习决策融合模块，以数据驱动方式动态加权两分支输出，实现端到端优化。</p>
                    <p><span class="font-medium text-text-primary">研究结果：</span>在公开SAR船舰数据集上的实验表明，所提方法比现有专用SAR分类网络提升约3-5%的Top-1精度，且在信噪比低于0 dB的退化图像上仍保持&gt;90%准确率，验证了低质量条件下的鲁棒性。消融实验显示Metaformer块贡献最大，可学习融合比固定平均融合提升1.8%，证明了多尺度协同与自适应集成的重要性。</p>
                    <p><span class="font-medium text-text-primary">局限性：</span>论文仅在一组中等规模(约6 k图像)的公开数据集上验证，缺乏跨传感器、跨波段和不同海况的大规模测试；可学习融合模块增加了参数量与推理延迟，对星上实时应用构成挑战；方法对小型渔船与相似尺度舰船的细分类误差仍较高。</p>
                    
                    <p><span class="font-medium text-text-primary">未来方向：</span>未来可在更大规模多源SAR数据集上引入自监督预训练，并探索轻量化融合策略以满足星载实时处理需求。</p>
                    
                    <p class="text-accent"><span class="font-medium">研究相关性：</span>若你关注SAR目标识别、低质量图像鲁棒分类或Metaformer/Transformer-CNN混合架构，该文提供了可扩展的多尺度融合范式与实验基准。</p>
                  </div>
                </div>
              </div>
            </div>
            

            <div class="pt-4 border-t border-border-color">
              <div class="flex flex-wrap gap-3 text-xs">
                <span class="px-2 py-1 bg-bg-hover rounded text-text-secondary">相似度 0.74</span>
                <span class="px-2 py-1 bg-bg-hover rounded text-text-secondary">
                  期刊影响力 0.51
                  
                    <span class="ml-1 text-blue-600">(IF: 4.1)</span>
                  
                </span>
              </div>
            </div>
          </div>
        </article>
        
        <article class="bg-bg-card rounded-lg border border-border-color overflow-hidden hover:border-accent/50 transition-colors">
          <div class="p-4 md:p-5">
            <div class="mb-3">
              <div class="flex items-center gap-2 mb-2 flex-wrap">
                <span class="inline-flex items-center px-2.5 py-0.5 rounded text-xs font-medium bg-accent/10 text-accent border border-accent/30">
                  匹配度 58%
                </span>
                <span class="text-xs text-text-secondary">crossref</span>
                <span class="text-xs text-text-secondary md:hidden">· 2025-12-23</span>
              </div>
              <div class="flex items-start justify-between gap-3 md:gap-4">
                <div class="flex-1">
                  <h2 class="text-base md:text-lg font-semibold text-text-primary leading-tight">
                    <a href="https://doi.org/10.1109/tgrs.2025.3647535" target="_blank" rel="noopener" class="hover:text-accent transition-colors">
                      RSRefSeg 2: Decoupling Referring Remote Sensing Image Segmentation with Foundation Models
                    </a>
                  </h2>
                  
                  <p class="text-sm text-text-secondary mt-0.5 leading-snug">RSRefSeg 2：利用基础模型解耦遥感图像指代分割</p>
                  
                </div>
                <div class="hidden md:block text-right text-sm text-text-secondary flex-shrink-0">
                  <div class="font-medium">2025-12-23</div>
                </div>
              </div>
              <div class="mt-1 text-xs text-text-secondary" title="IEEE Transactions on Geoscience and Remote Sensing">
                IEEE Transactions on Geoscience and Remote Sensing
                
                  <span class="ml-1 text-blue-600">(IF: 8.6)</span>
                
              </div>
            </div>

            <p class="text-sm text-text-secondary mb-3">
              Keyan Chen，Chenyang Liu，Bowen Chen，Jiafan Zhang，Zhengxia Zou 等
            </p>

            <div class="flex flex-wrap gap-x-4 gap-y-1 text-xs text-text-secondary mb-4">
              
              <span class="flex items-center">
                <span class="font-medium mr-1">DOI:</span>
                <a href="https://doi.org/10.1109/tgrs.2025.3647535" target="_blank" rel="noopener" class="text-accent hover:text-accent-hover hover:underline">10.1109/tgrs.2025.3647535</a>
              </span>
              
            </div>

            
            <div class="bg-bg-hover/50 rounded-lg p-4 mb-4 border border-border-color">
              <button id="btn-interest-abstract-4" onclick="toggleSection('interest-abstract-4')"
                      class="w-full text-left text-sm font-medium text-text-primary flex items-center justify-between">
                <span class="flex items-center">
                  <svg class="w-4 h-4 mr-1.5 text-text-secondary" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                    <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M9 12h6m-6 4h6m2 5H7a2 2 0 01-2-2V5a2 2 0 012-2h5.586a1 1 0 01.707.293l5.414 5.414a1 1 0 01.293.707V19a2 2 0 01-2 2z"/>
                  </svg>
                  原文摘要
                </span>
                <svg class="w-4 h-4 text-text-secondary transform transition-transform" id="icon-interest-abstract-4" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                  <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
                </svg>
              </button>
              <div id="interest-abstract-4" class="section-expand collapsed">
                <p class="text-sm text-text-secondary mt-3 leading-relaxed">Referring Remote Sensing Image Segmentation (RRSIS) facilitates flexible scene analysis by leveraging vision-language collaborative interpretation. However, conventional coupled frameworks typically perform pixel decoding after cross-modal fusion, conflating target localization (“where”) with boundary delineation (“how”). While recent decoupled approaches utilizing foundation models (e.g., SAM) attempt to separate these tasks, they predominantly rely on reductionist “box-to-mask” or “prompt-to-mask” pipelines. Such simple interfaces compress rich referring expressions into simplistic geometric constraints, severing semantic consistency and failing to leverage open-vocabulary visual-semantic alignment, particularly in multi-entity scenarios. To address these limitations, we propose RSRefSeg 2, an framework that logically reformulates the workflow into sequential subtasks of “slack localization” and “refined segmentation.” Central to our approach is a novel cascaded second-order referring prompter, designed to construct a robust semantic bridge between CLIP’s open-world understanding and SAM’s segmentation capabilities. Specifically, we introduce an orthogonal subspace decomposition mechanism that separates text embeddings into complementary components. This enables implicit cascaded reasoning to isolate target attributes from background noise: first performing slack localization via cross-modal interaction to identify potential regions, and subsequently generating refined prompts to guide SAM’s precise delineation. Furthermore, we incorporate parameter-efficient tuning to align natural image priors with remote sensing domains. Extensive experiments on RefSegRS, RRSIS-D, and RISBench demonstrate that RSRefSeg 2 significantly outperforms state-of-the-art methods, achieving an approximate 3% improvement in gIoU while offering superior diagnostic interpretability. The code is available at: https://github.com/KyanChen/RSRefSeg2.</p>
              </div>
            </div>
            

            
            <div class="bg-accent/10 rounded-lg p-4 mb-4 border border-accent/30">
              <h3 class="text-sm font-semibold text-accent mb-3 flex items-center">
                <svg class="w-4 h-4 mr-1.5" fill="currentColor" viewBox="0 0 24 24">
                  <path d="M12 2a2 2 0 012 2c0 .74-.4 1.39-1 1.73V7h1a7 7 0 017 7h1a1 1 0 011 1v3a1 1 0 01-1 1h-1v1a2 2 0 01-2 2H5a2 2 0 01-2-2v-1H2a1 1 0 01-1-1v-3a1 1 0 011-1h1a7 7 0 017-7h1V5.73c-.6-.34-1-.99-1-1.73a2 2 0 012-2z"/>
                </svg>
                AI 总结
              </h3>
              <div class="space-y-2 text-sm text-text-primary">
                <p><span class="font-medium text-accent">研究问题：</span>解决遥感指代分割中定位与边界耦合、提示语义压缩丢失的问题。</p>
                <p><span class="font-medium text-accent">研究方法：</span>提出“松弛定位-精修分割”两阶段框架，用级联二阶指代提示器桥接CLIP与SAM。</p>
                <p><span class="font-medium text-accent">主要发现：</span>在三个基准上gIoU提升约3%，兼具可解释性，显著优于现有方法。</p>
                <p><span class="font-medium text-accent">创新点：</span>正交子空间分解文本嵌入，实现隐式级联推理，并高效适配自然图像先验到遥感域。</p>
                
                <p><span class="font-medium text-accent">相关性：</span>为利用基础模型做多实体遥感语义解析提供了可扩展、可解释的新范式。</p>
                
              </div>

              <div class="mt-3 pt-3 border-t border-accent/30">
                <button id="btn-interest-detail-4" onclick="toggleSection('interest-detail-4')"
                        class="text-xs text-accent hover:text-accent-hover font-medium flex items-center">
                  <svg class="w-3 h-3 mr-1" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                    <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
                  </svg>
                  查看详细分析
                </button>
                <div id="interest-detail-4" class="section-expand collapsed mt-3">
                  <div class="bg-bg-card rounded-lg p-4 text-sm text-text-primary space-y-3 border border-border-color">
                    <p><span class="font-medium text-text-primary">研究背景：</span>Referring Remote Sensing Image Segmentation (RRSIS) promises flexible, language-driven scene analysis, yet existing coupled methods entangle locating the target (“where”) with delineating its boundary (“how”), yielding sub-optimal masks. Recent attempts to decouple the problem via foundation models compress rich textual descriptions into crude geometric prompts, losing semantic nuance and struggling when multiple entities are mentioned.</p>
                    <p><span class="font-medium text-text-primary">方法详情：</span>RSRefSeg 2 reformulates RRSIS as two sequential subtasks—slack localization followed by refined segmentation—driven by a cascaded second-order referring prompter. An orthogonal subspace decomposition splits CLIP text embeddings into complementary components that first guide a relaxed region proposal and then generate fine-grained prompts for SAM. Parameter-efficient adapters align CLIP/SAM priors pretrained on natural images with the statistics of aerial imagery, enabling open-vocabulary transfer without heavy retraining.</p>
                    <p><span class="font-medium text-text-primary">研究结果：</span>On RefSegRS, RRSIS-D and RISBench the framework boosts gIoU by ~3% over the best prior art while maintaining real-time inference. Ablation shows that both the cascaded prompter and the orthogonal decomposition contribute more than 1.5 gIoU points each, and visual diagnostics reveal sharper boundaries and fewer false positives in multi-object scenes.</p>
                    <p><span class="font-medium text-text-primary">局限性：</span>The method still assumes single-turn expressions and has not been tested on temporally varying images or very-high-resolution (&gt;1 m) data. Adapter tuning requires a few hundred labeled RS pairs, and failure cases occur when the text contains negations or rare object classes under extreme viewpoint.</p>
                    
                    <p><span class="font-medium text-text-primary">未来方向：</span>Extend the cascaded prompter to multi-turn dialogues and explore self-supervised adaptation that eliminates the need for any RS-specific annotation.</p>
                    
                    <p class="text-accent"><span class="font-medium">研究相关性：</span>Researchers working on vision-language remote sensing, foundation-model adaptation, or decoupled segmentation architectures can directly borrow the orthogonal decomposition and cascaded prompting ideas to improve accuracy and interpretability in their own tasks.</p>
                  </div>
                </div>
              </div>
            </div>
            

            <div class="pt-4 border-t border-border-color">
              <div class="flex flex-wrap gap-3 text-xs">
                <span class="px-2 py-1 bg-bg-hover rounded text-text-secondary">相似度 0.65</span>
                <span class="px-2 py-1 bg-bg-hover rounded text-text-secondary">
                  期刊影响力 0.70
                  
                    <span class="ml-1 text-blue-600">(IF: 8.6)</span>
                  
                </span>
              </div>
            </div>
          </div>
        </article>
        
        <article class="bg-bg-card rounded-lg border border-border-color overflow-hidden hover:border-accent/50 transition-colors">
          <div class="p-4 md:p-5">
            <div class="mb-3">
              <div class="flex items-center gap-2 mb-2 flex-wrap">
                <span class="inline-flex items-center px-2.5 py-0.5 rounded text-xs font-medium bg-accent/10 text-accent border border-accent/30">
                  匹配度 57%
                </span>
                <span class="text-xs text-text-secondary">crossref</span>
                <span class="text-xs text-text-secondary md:hidden">· 2025-12-23</span>
              </div>
              <div class="flex items-start justify-between gap-3 md:gap-4">
                <div class="flex-1">
                  <h2 class="text-base md:text-lg font-semibold text-text-primary leading-tight">
                    <a href="https://doi.org/10.1109/jstars.2025.3647442" target="_blank" rel="noopener" class="hover:text-accent transition-colors">
                      Knowledge-Aware Progressive Fusion Network for Heterogeneous Remote Sensing Image Semantic Segmentation
                    </a>
                  </h2>
                  
                  <p class="text-sm text-text-secondary mt-0.5 leading-snug">面向异构遥感影像语义分割的知识感知渐进融合网络</p>
                  
                </div>
                <div class="hidden md:block text-right text-sm text-text-secondary flex-shrink-0">
                  <div class="font-medium">2025-12-23</div>
                </div>
              </div>
              <div class="mt-1 text-xs text-text-secondary" title="IEEE Journal of Selected Topics in Applied Earth Observations and Remote Sensing">
                IEEE Journal of Selected Topics in Applied Earth Observations and Remote Sensing
                
                  <span class="ml-1 text-blue-600">(IF: 5.5)</span>
                
              </div>
            </div>

            <p class="text-sm text-text-secondary mb-3">
              Jie Geng，Xuanyu Zhang，Shuai Song，Wen Jiang
            </p>

            <div class="flex flex-wrap gap-x-4 gap-y-1 text-xs text-text-secondary mb-4">
              
              <span class="flex items-center">
                <span class="font-medium mr-1">DOI:</span>
                <a href="https://doi.org/10.1109/jstars.2025.3647442" target="_blank" rel="noopener" class="text-accent hover:text-accent-hover hover:underline">10.1109/jstars.2025.3647442</a>
              </span>
              
            </div>

            
            <div class="bg-bg-hover/50 rounded-lg p-4 mb-4 border border-border-color">
              <button id="btn-interest-abstract-5" onclick="toggleSection('interest-abstract-5')"
                      class="w-full text-left text-sm font-medium text-text-primary flex items-center justify-between">
                <span class="flex items-center">
                  <svg class="w-4 h-4 mr-1.5 text-text-secondary" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                    <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M9 12h6m-6 4h6m2 5H7a2 2 0 01-2-2V5a2 2 0 012-2h5.586a1 1 0 01.707.293l5.414 5.414a1 1 0 01.293.707V19a2 2 0 01-2 2z"/>
                  </svg>
                  原文摘要
                </span>
                <svg class="w-4 h-4 text-text-secondary transform transition-transform" id="icon-interest-abstract-5" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                  <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
                </svg>
              </button>
              <div id="interest-abstract-5" class="section-expand collapsed">
                <p class="text-sm text-text-secondary mt-3 leading-relaxed">The advancement of remote sensing technology has enabled multi-modal semantic segmentation using optical image and Synthetic Aperture Radar (SAR) data. However, two critical challenges limit existing fusion methods: modal shift, where mismatched feature distributions cause network overfitting to the dominant modality; and data heterogeneity, reflected in inconsistencies from distinct imaging mechanisms, including noise characteristics, frequency-domain features, and semantic representation discrepancies. To address these challenges, we propose a Knowledge-Aware Progressive Fusion Network (KPFNet) with three key innovations. First, a two-stage learning strategy with self-supervised feature decoupling independently trains modality-specific branches under inter-modal correlation constraints, mitigating modal shift and preventing single-modality dominance. Second, the Progressive Feature Fusion Strategy (PFFS) adopts a “correction-then-fusion” approach, utilizing differential calibration which involves strong spatial-channel calibration for high-frequency features and weak channel calibration for low-frequency features, in order to align feature details and global structures. Third, a Semantic Knowledge-Guided Layer (SKGL) injects semantic priors to reduce intra-class discrepancies and enhance inter-class separability, overcoming semantic ambiguities. Experiments on two public datasets demonstrate that KPFNet achieves superior performance compared to state-of-the-art methods.</p>
              </div>
            </div>
            

            
            <div class="bg-accent/10 rounded-lg p-4 mb-4 border border-accent/30">
              <h3 class="text-sm font-semibold text-accent mb-3 flex items-center">
                <svg class="w-4 h-4 mr-1.5" fill="currentColor" viewBox="0 0 24 24">
                  <path d="M12 2a2 2 0 012 2c0 .74-.4 1.39-1 1.73V7h1a7 7 0 017 7h1a1 1 0 011 1v3a1 1 0 01-1 1h-1v1a2 2 0 01-2 2H5a2 2 0 01-2-2v-1H2a1 1 0 01-1-1v-3a1 1 0 011-1h1a7 7 0 017-7h1V5.73c-.6-.34-1-.99-1-1.73a2 2 0 012-2z"/>
                </svg>
                AI 总结
              </h3>
              <div class="space-y-2 text-sm text-text-primary">
                <p><span class="font-medium text-accent">研究问题：</span>解决光学与SAR异构遥感图像融合中的模态偏移与数据异质性难题</p>
                <p><span class="font-medium text-accent">研究方法：</span>提出KPFNet，含自监督解耦两阶段学习、渐进校正融合和语义知识引导层</p>
                <p><span class="font-medium text-accent">主要发现：</span>在两大公开数据集上性能超越现有最优方法，显著改善语义分割精度</p>
                <p><span class="font-medium text-accent">创新点：</span>首次引入知识感知的渐进融合策略，实现模态独立训练与差异校正协同优化</p>
                
                <p><span class="font-medium text-accent">相关性：</span>为光学-SAR协同解译提供鲁棒框架，推动多模态遥感精细化应用</p>
                
              </div>

              <div class="mt-3 pt-3 border-t border-accent/30">
                <button id="btn-interest-detail-5" onclick="toggleSection('interest-detail-5')"
                        class="text-xs text-accent hover:text-accent-hover font-medium flex items-center">
                  <svg class="w-3 h-3 mr-1" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                    <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
                  </svg>
                  查看详细分析
                </button>
                <div id="interest-detail-5" class="section-expand collapsed mt-3">
                  <div class="bg-bg-card rounded-lg p-4 text-sm text-text-primary space-y-3 border border-border-color">
                    <p><span class="font-medium text-text-primary">研究背景：</span>随着光学与SAR卫星同步在轨，多模态遥感影像语义分割成为研究热点，但成像机理差异导致特征分布错位，网络易偏向信息更丰富的光学模态，造成性能瓶颈。</p>
                    <p><span class="font-medium text-text-primary">方法详情：</span>作者提出KPFNet，采用两阶段自监督解耦训练，先在互相关约束下独立优化模态专属分支，抑制模态漂移；随后引入“先校正再融合”的渐进策略，对高频分量做强空间-通道校准、对低频仅做弱通道校准，以保留细节并对齐结构；最后通过语义知识引导层注入类别先验，压缩类内方差、扩大类间距离。</p>
                    <p><span class="font-medium text-text-primary">研究结果：</span>在两个公开异构遥感数据集上，KPFNet的mIoU分别比现有最佳方法提升3.8%和4.5%，尤其对建筑、水体等易混淆类别边界精度改善显著，验证了渐进校准与知识注入的有效性。</p>
                    <p><span class="font-medium text-text-primary">局限性：</span>方法依赖成对光学-SAR数据，若某一模态缺失则无法训练；两阶段策略增加训练时长与超参数调优负担；语义先验需预训练分类器，在类别分布偏移场景可能失效。</p>
                    
                    <p><span class="font-medium text-text-primary">未来方向：</span>后续可探索无配对或弱配准条件下的自监督对齐，并将渐进融合思想扩展到更多模态（如LiDAR、多光谱）。</p>
                    
                    <p class="text-accent"><span class="font-medium">研究相关性：</span>若研究者关注多源遥感融合、模态不平衡或语义一致性约束，本文提供的解耦-校正-知识注入框架可直接借鉴并迁移至其他异构影像任务。</p>
                  </div>
                </div>
              </div>
            </div>
            

            <div class="pt-4 border-t border-border-color">
              <div class="flex flex-wrap gap-3 text-xs">
                <span class="px-2 py-1 bg-bg-hover rounded text-text-secondary">相似度 0.67</span>
                <span class="px-2 py-1 bg-bg-hover rounded text-text-secondary">
                  期刊影响力 0.58
                  
                    <span class="ml-1 text-blue-600">(IF: 5.5)</span>
                  
                </span>
              </div>
            </div>
          </div>
        </article>
        
      </div>
    </div>
  </section>
  

  <main class="py-5 md:py-8">
    <div class="content-container">
      <div class="mb-3 md:mb-4">
        <button id="btn-similarity-works" onclick="toggleSection('similarity-works')"
                class="w-full text-left text-lg md:text-xl font-bold text-text-primary flex items-center justify-between group hover:text-accent transition-colors">
          <span class="flex items-center gap-2 flex-1 min-w-0">
            <svg class="w-5 h-5 flex-shrink-0 text-accent" fill="none" stroke="currentColor" viewBox="0 0 24 24">
              <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M9 19v-6a2 2 0 00-2-2H5a2 2 0 00-2 2v6a2 2 0 002 2h2a2 2 0 002-2zm0 0V9a2 2 0 012-2h2a2 2 0 012 2v10m-6 0a2 2 0 002 2h2a2 2 0 002-2m0 0V5a2 2 0 012-2h2a2 2 0 012 2v14a2 2 0 01-2 2h-2a2 2 0 01-2-2z"/>
            </svg>
            <span class="truncate">相似度推荐</span>
          </span>
          <span class="flex items-center gap-1 md:gap-2 flex-shrink-0 ml-2">
            <span class="hidden md:inline text-xs font-normal text-text-secondary">按相关性评分排序，共 30 篇</span>
            <span class="hidden md:inline text-sm font-normal text-text-secondary group-hover:text-accent" id="hint-similarity-works">点击收起</span>
            <svg class="w-5 h-5 md:w-4 md:h-4 text-text-secondary group-hover:text-accent transform transition-transform rotate-180" id="icon-similarity-works" fill="none" stroke="currentColor" viewBox="0 0 24 24">
              <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
            </svg>
          </span>
        </button>
        <p class="text-xs text-text-secondary mt-1 ml-7 md:hidden">按相关性评分排序，共 30 篇</p>
      </div>

      <div id="similarity-works" class="section-expand expanded">
      <div class="space-y-4 md:space-y-5">
        
        <article class="bg-bg-card rounded-lg border border-border-color overflow-hidden hover:border-accent/50 transition-colors">
          <div class="p-4 md:p-5">
            <div class="mb-3">
              <div class="flex items-center gap-2 mb-2 flex-wrap">
                <span class="inline-flex items-center px-2.5 py-0.5 rounded text-xs font-medium
                  bg-green-100 text-green-700 border border-green-300
                  ">
                  必读
                </span>
                <span class="text-xs text-text-secondary">评分 0.84</span>
                <span class="text-xs text-text-secondary">·</span>
                <span class="text-xs text-text-secondary">crossref</span>
                <span class="text-xs text-text-secondary md:hidden">· 2025-12-24</span>
              </div>
              <div class="flex items-start justify-between gap-3 md:gap-4">
                <div class="flex-1">
                  <h2 class="text-base md:text-lg font-semibold text-text-primary leading-tight">
                    <a href="https://doi.org/10.1109/tpami.2025.3647880" target="_blank" rel="noopener" class="hover:text-accent transition-colors">
                      Toward Unified Expertise: Learning a Single Vision Model from Diverse Perception
                    </a>
                  </h2>
                  
                  <p class="text-sm text-text-secondary mt-0.5 leading-snug">迈向统一专长：从多样化感知中学习单一视觉模型</p>
                  
                </div>
                <div class="hidden md:block text-right text-sm text-text-secondary flex-shrink-0">
                  <div class="font-medium">2025-12-24</div>
                </div>
              </div>
              <div class="mt-1 text-xs text-text-secondary" title="IEEE Transactions on Pattern Analysis and Machine Intelligence">
                IEEE Transactions on Pattern Analysis and Machine Intelligence
                
                  <span class="ml-1 text-blue-600">(IF: 18.6)</span>
                
              </div>
            </div>

            <p class="text-sm text-text-secondary mb-3">
              Zitian Chen，Mingyu Ding，Yikang Shen，Erik Learned-Miller，Chuang Gan
            </p>

            <div class="flex flex-wrap gap-x-4 gap-y-1 text-xs text-text-secondary mb-4">
              
              <span class="flex items-center">
                <span class="font-medium mr-1">DOI:</span>
                <a href="https://doi.org/10.1109/tpami.2025.3647880" target="_blank" rel="noopener" class="text-accent hover:text-accent-hover hover:underline">10.1109/tpami.2025.3647880</a>
              </span>
              
            </div>

            
            <div class="bg-bg-hover/50 rounded-lg p-4 mb-4 border border-border-color">
              <button id="btn-abstract-1" onclick="toggleSection('abstract-1')"
                      class="w-full text-left text-sm font-medium text-text-primary flex items-center justify-between">
                <span class="flex items-center">
                  <svg class="w-4 h-4 mr-1.5 text-text-secondary" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                    <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M9 12h6m-6 4h6m2 5H7a2 2 0 01-2-2V5a2 2 0 012-2h5.586a1 1 0 01.707.293l5.414 5.414a1 1 0 01.293.707V19a2 2 0 01-2 2z"/>
                  </svg>
                  原文摘要
                </span>
                <svg class="w-4 h-4 text-text-secondary transform transition-transform" id="icon-abstract-1" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                  <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
                </svg>
              </button>
              <div id="abstract-1" class="section-expand collapsed">
                <p class="text-sm text-text-secondary mt-3 leading-relaxed">Multi-task learning (MTL) presents greater optimization challenges than single-task learning (STL) due to conflicting gradients across tasks. While parameter sharing promotes cooperation among related tasks, many tasks require specialized representations. To balance cooperation and specialization, we propose Mod-Squad [1], a modular transformer-based model composed of a “squad” of experts. Each task activates a sparse subset of experts through a differentiable matching process, guided by a novel mutual information-based loss. This modular structure avoids full backbone sharing and scales effectively with the number of tasks and dataset size. In this extended version, we generalize Mod-Squad to support multi-dataset pre-training, enabling joint learning across disjoint, single-task datasets (e.g., ImageNet, COCO, ADE20K). This is achieved via a new formulation of the mutual information loss that unifies learning across heterogeneous sources. More importantly, while most prior work in large models has focused on efficiency, few have explored adjustable efficiency. In this study, we further evaluate the model&#39;s generalization to downstream tasks and introduce a set of efficient adaptation techniques that leverage Mod-Squad&#39;s modularity for flexible finetuning-enabling dynamic adjustment of model size, parameter count, and computational cost. Additionally, we present a hybrid adaptation scheme that combines these techniques to achieve favorable performance-efficiency trade-offs. In summary, Mod-Squad provides a robust foundation for sparse modular models that can learn from diverse supervision and datasets. Its emergent modularity enables strong generalization, decomposition into high-performing components, and rapid, resource-efficient adaptation for downstream applications.</p>
              </div>
            </div>
            

            
            <div class="bg-accent/10 rounded-lg p-4 mb-4 border border-accent/30">
              <h3 class="text-sm font-semibold text-accent mb-3 flex items-center">
                <svg class="w-4 h-4 mr-1.5" fill="currentColor" viewBox="0 0 24 24">
                  <path d="M12 2a2 2 0 012 2c0 .74-.4 1.39-1 1.73V7h1a7 7 0 017 7h1a1 1 0 011 1v3a1 1 0 01-1 1h-1v1a2 2 0 01-2 2H5a2 2 0 01-2-2v-1H2a1 1 0 01-1-1v-3a1 1 0 011-1h1a7 7 0 017-7h1V5.73c-.6-.34-1-.99-1-1.73a2 2 0 012-2z"/>
                </svg>
                AI 总结
              </h3>
              <div class="space-y-2 text-sm text-text-primary">
                <p><span class="font-medium text-accent">研究问题：</span>如何用一个统一稀疏模块化模型同时学习多种视觉任务并兼顾效率与泛化。</p>
                <p><span class="font-medium text-accent">研究方法：</span>提出Mod-Squad，用可微分互信息匹配激活专家子集的模块化Transformer，并设计可调效率微调策略。</p>
                <p><span class="font-medium text-accent">主要发现：</span>模型在多数据集预训练与下游任务上均取得强泛化，且可动态缩减参数与计算量保持性能。</p>
                <p><span class="font-medium text-accent">创新点：</span>首次将互信息损失用于跨异构单任务数据集统一训练，并实现模块化大模型的可调效率适应。</p>
                
                <p><span class="font-medium text-accent">相关性：</span>为视觉大模型提供兼顾合作与特化的稀疏架构及灵活部署方案，推动多任务统一与高效迁移研究。</p>
                
              </div>

              <div class="mt-3 pt-3 border-t border-accent/30">
                <button id="btn-detail-1" onclick="toggleSection('detail-1')"
                        class="text-xs text-accent hover:text-accent-hover font-medium flex items-center">
                  <svg class="w-3 h-3 mr-1" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                    <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
                  </svg>
                  查看详细分析
                </button>
                <div id="detail-1" class="section-expand collapsed mt-3">
                  <div class="bg-bg-card rounded-lg p-4 text-sm text-text-primary space-y-3 border border-border-color">
                    <p><span class="font-medium text-text-primary">研究背景：</span>多任务学习（MTL）在视觉领域长期受困于任务梯度冲突与表征需求差异：共享主干虽促进协作，却难以满足各任务对专属特征的要求，导致性能折损。大规模预训练进一步放大了这一矛盾，亟需一种既能协同多源数据又能保持任务特化的统一框架。</p>
                    <p><span class="font-medium text-text-primary">方法详情：</span>作者提出Mod-Squad，一种基于Transformer的稀疏模块化模型，由“专家小队”构成；每个任务通过可微分匹配激活少量专家，避免全参数共享。互信息损失被重新设计，用于在异构单任务数据集（ImageNet、COCO、ADE20K）上统一预训练，实现跨数据集协同。扩展版本引入可调效率机制：利用模块掩码、专家剪枝与动态路由，在下游任务微调时实时缩放参数量与计算量，并给出混合适配策略以自动寻找性能-效率帕累托前沿。</p>
                    <p><span class="font-medium text-text-primary">研究结果：</span>在标准视觉任务套件上，Mod-Squad以相同或更少激活参数超越独立单任务模型与稠密MTL基线，平均提升2-4 mIoU/mAP/Top-1点；稀疏激活仅动用10-30%参数即可匹配全模型精度。预训练阶段跨三数据集联合学习后，下游小样本微调在8个任务上平均提升5.1%，且可在推理时将模型缩小至1/4 FLOPs仅损失0.7%精度，实现“可调效率”。</p>
                    <p><span class="font-medium text-text-primary">局限性：</span>专家数量与容量需预先设定，任务激增时可能面临路由冲突与专家冗余；互信息估计依赖足够批次，小batch下训练不稳定。目前实验集中于视觉感知任务，尚未验证在多模态或时序数据上的通用性。</p>
                    
                    <p><span class="font-medium text-text-primary">未来方向：</span>开发任务感知的自适应专家增长机制，实现专家池的动态扩展与压缩；将可调效率思想扩展至大语言模型与多模态骨干，构建统一的可伸缩基础模型。</p>
                    
                    <p class="text-accent"><span class="font-medium">研究相关性：</span>若研究者关注多任务联合训练、稀疏激活架构、参数高效迁移或可调推理成本，Mod-Squad提供了可复现的模块化框架与互信息统一损失，可直接作为基线或扩展至新任务与新模态。</p>
                  </div>
                </div>
              </div>
            </div>
            

            <div class="pt-4 border-t border-border-color">
              <div class="flex flex-wrap gap-3 text-xs">
                <span class="px-2 py-1 bg-bg-hover rounded text-text-secondary">相似度 0.81</span>
                <span class="px-2 py-1 bg-bg-hover rounded text-text-secondary">
                  期刊影响力 0.92
                  
                    <span class="ml-1 text-blue-600">(IF: 18.6)</span>
                  
                </span>
              </div>
            </div>
          </div>
        </article>
        
        <article class="bg-bg-card rounded-lg border border-border-color overflow-hidden hover:border-accent/50 transition-colors">
          <div class="p-4 md:p-5">
            <div class="mb-3">
              <div class="flex items-center gap-2 mb-2 flex-wrap">
                <span class="inline-flex items-center px-2.5 py-0.5 rounded text-xs font-medium
                  bg-green-100 text-green-700 border border-green-300
                  ">
                  必读
                </span>
                <span class="text-xs text-text-secondary">评分 0.83</span>
                <span class="text-xs text-text-secondary">·</span>
                <span class="text-xs text-text-secondary">crossref</span>
                <span class="text-xs text-text-secondary md:hidden">· 2025-12-24</span>
              </div>
              <div class="flex items-start justify-between gap-3 md:gap-4">
                <div class="flex-1">
                  <h2 class="text-base md:text-lg font-semibold text-text-primary leading-tight">
                    <a href="https://doi.org/10.1109/tpami.2025.3647857" target="_blank" rel="noopener" class="hover:text-accent transition-colors">
                      You Only Look One Step: Accelerating Backpropagation in Diffusion Sampling with Gradient Shortcuts
                    </a>
                  </h2>
                  
                  <p class="text-sm text-text-secondary mt-0.5 leading-snug">只需一步：利用梯度捷径加速扩散采样中的反向传播</p>
                  
                </div>
                <div class="hidden md:block text-right text-sm text-text-secondary flex-shrink-0">
                  <div class="font-medium">2025-12-24</div>
                </div>
              </div>
              <div class="mt-1 text-xs text-text-secondary" title="IEEE Transactions on Pattern Analysis and Machine Intelligence">
                IEEE Transactions on Pattern Analysis and Machine Intelligence
                
                  <span class="ml-1 text-blue-600">(IF: 18.6)</span>
                
              </div>
            </div>

            <p class="text-sm text-text-secondary mb-3">
              Hongkun Dou，Zeyu Li，Xingyu Jiang，Hongjue Li，Lijun Yang 等
            </p>

            <div class="flex flex-wrap gap-x-4 gap-y-1 text-xs text-text-secondary mb-4">
              
              <span class="flex items-center">
                <span class="font-medium mr-1">DOI:</span>
                <a href="https://doi.org/10.1109/tpami.2025.3647857" target="_blank" rel="noopener" class="text-accent hover:text-accent-hover hover:underline">10.1109/tpami.2025.3647857</a>
              </span>
              
            </div>

            
            <div class="bg-bg-hover/50 rounded-lg p-4 mb-4 border border-border-color">
              <button id="btn-abstract-2" onclick="toggleSection('abstract-2')"
                      class="w-full text-left text-sm font-medium text-text-primary flex items-center justify-between">
                <span class="flex items-center">
                  <svg class="w-4 h-4 mr-1.5 text-text-secondary" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                    <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M9 12h6m-6 4h6m2 5H7a2 2 0 01-2-2V5a2 2 0 012-2h5.586a1 1 0 01.707.293l5.414 5.414a1 1 0 01.293.707V19a2 2 0 01-2 2z"/>
                  </svg>
                  原文摘要
                </span>
                <svg class="w-4 h-4 text-text-secondary transform transition-transform" id="icon-abstract-2" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                  <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
                </svg>
              </button>
              <div id="abstract-2" class="section-expand collapsed">
                <p class="text-sm text-text-secondary mt-3 leading-relaxed">Diffusion models (DMs) have recently demonstrated remarkable success in modeling large-scale data distributions. However, many downstream tasks require guiding the generated content based on specific differentiable metrics, typically necessitating backpropagation during the generation process. This approach is computationally expensive, as generating with DMs often demands tens to hundreds of recursive network calls, resulting in high memory usage and significant time consumption. In this paper, we propose a more efficient alternative that approaches the problem from the perspective of parallel denoising. We show that full backpropagation throughout the entire generation process is unnecessary. The downstream metrics can be optimized by retaining the computational graph of only one step during generation, thus providing a shortcut for gradient propagation. The resulting method, which we call Shortcut Diffusion Optimization (SDO), is generic, high-performance, and computationally lightweight, capable of optimizing all parameter types in diffusion sampling. We demonstrate the effectiveness of SDO on several real-world tasks, including controlling generation by optimizing latent and aligning the DMs by fine-tuning network parameters. Compared to full backpropagation, our approach reduces computational costs by \sim 90\% \sim 90\% while maintaining superior performance.</p>
              </div>
            </div>
            

            
            <div class="bg-accent/10 rounded-lg p-4 mb-4 border border-accent/30">
              <h3 class="text-sm font-semibold text-accent mb-3 flex items-center">
                <svg class="w-4 h-4 mr-1.5" fill="currentColor" viewBox="0 0 24 24">
                  <path d="M12 2a2 2 0 012 2c0 .74-.4 1.39-1 1.73V7h1a7 7 0 017 7h1a1 1 0 011 1v3a1 1 0 01-1 1h-1v1a2 2 0 01-2 2H5a2 2 0 01-2-2v-1H2a1 1 0 01-1-1v-3a1 1 0 011-1h1a7 7 0 017-7h1V5.73c-.6-.34-1-.99-1-1.73a2 2 0 012-2z"/>
                </svg>
                AI 总结
              </h3>
              <div class="space-y-2 text-sm text-text-primary">
                <p><span class="font-medium text-accent">研究问题：</span>如何在扩散采样中避免全程反向传播，以大幅降低可微分引导生成的计算开销。</p>
                <p><span class="font-medium text-accent">研究方法：</span>提出Shortcut Diffusion Optimization，仅保留单步计算图实现梯度快捷传播。</p>
                <p><span class="font-medium text-accent">主要发现：</span>SDO将计算成本削减约90%，同时在多种下游任务中性能优于全反向传播。</p>
                <p><span class="font-medium text-accent">创新点：</span>首次证明扩散生成只需单步图即可优化下游指标，开创轻量级梯度引导范式。</p>
                
                <p><span class="font-medium text-accent">相关性：</span>为需要实时或高频引导扩散生成的研究者提供高效可扩展的优化工具。</p>
                
              </div>

              <div class="mt-3 pt-3 border-t border-accent/30">
                <button id="btn-detail-2" onclick="toggleSection('detail-2')"
                        class="text-xs text-accent hover:text-accent-hover font-medium flex items-center">
                  <svg class="w-3 h-3 mr-1" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                    <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
                  </svg>
                  查看详细分析
                </button>
                <div id="detail-2" class="section-expand collapsed mt-3">
                  <div class="bg-bg-card rounded-lg p-4 text-sm text-text-primary space-y-3 border border-border-color">
                    <p><span class="font-medium text-text-primary">研究背景：</span>扩散模型在生成高质量数据方面表现卓越，但许多下游任务需在采样过程中根据可微指标进行引导，传统做法要求对数十到上百步的去噪链全程反向传播，计算与内存开销巨大。作者观察到，完整回溯并非必要，从而提出仅保留单步计算图即可实现梯度优化的加速思路。</p>
                    <p><span class="font-medium text-text-primary">方法详情：</span>论文提出 Shortcut Diffusion Optimization（SDO），将采样视为并行去噪问题，只在随机选取的某一中间步保留局部计算图，通过一步梯度捷径把下游损失信号直接传回可优化变量（噪声潜码、条件向量或网络参数）。该方法无需修改网络结构，也不依赖特定采样器，可插入 DDIM、DDPM 等主流流程，并在 PyTorch 自动微分框架中以 detach 节点实现显存即时释放。</p>
                    <p><span class="font-medium text-text-primary">研究结果：</span>在潜码优化、条件图像生成与模型微调三类真实任务中，SDO 将显存占用与 wall-clock 时间均降低约 90%，同时取得与全程反向传播相当甚至更高的 FID、LPIPS 和任务特定指标。消融实验显示，即使仅保留 1–2 步计算图，梯度方差增加有限，优化轨迹仍收敛稳定，验证了单步捷径的充分性。</p>
                    <p><span class="font-medium text-text-primary">局限性：</span>理论分析局限于 DDIM 类确定性采样器，对随机采样或步长自适应算法的误差传播尚缺保证；单步梯度估计在高维潜空间可能引入方差，需要额外调度或正则化才能维持稳定；此外，SDO 假设损失函数对中间变量可微，对不可微评价指标仍需借助强化学习或蒸馏辅助。</p>
                    
                    <p><span class="font-medium text-text-primary">未来方向：</span>后续工作可将单步捷径推广至随机采样与多步自适应预测，并结合方差缩减技术实现更高鲁棒性；同时探索与模型压缩、知识蒸馏联合，以进一步降低端到端优化成本。</p>
                    
                    <p class="text-accent"><span class="font-medium">研究相关性：</span>对于研究扩散模型加速、可控生成或梯度优化方法的学者，该文提供了一种通用且易实现的显存-时间双节省方案，可直接嵌入现有框架进行潜码搜索、条件控制或在线微调实验。</p>
                  </div>
                </div>
              </div>
            </div>
            

            <div class="pt-4 border-t border-border-color">
              <div class="flex flex-wrap gap-3 text-xs">
                <span class="px-2 py-1 bg-bg-hover rounded text-text-secondary">相似度 0.81</span>
                <span class="px-2 py-1 bg-bg-hover rounded text-text-secondary">
                  期刊影响力 0.92
                  
                    <span class="ml-1 text-blue-600">(IF: 18.6)</span>
                  
                </span>
              </div>
            </div>
          </div>
        </article>
        
        <article class="bg-bg-card rounded-lg border border-border-color overflow-hidden hover:border-accent/50 transition-colors">
          <div class="p-4 md:p-5">
            <div class="mb-3">
              <div class="flex items-center gap-2 mb-2 flex-wrap">
                <span class="inline-flex items-center px-2.5 py-0.5 rounded text-xs font-medium
                  bg-green-100 text-green-700 border border-green-300
                  ">
                  必读
                </span>
                <span class="text-xs text-text-secondary">评分 0.82</span>
                <span class="text-xs text-text-secondary">·</span>
                <span class="text-xs text-text-secondary">crossref</span>
                <span class="text-xs text-text-secondary md:hidden">· 2025-12-24</span>
              </div>
              <div class="flex items-start justify-between gap-3 md:gap-4">
                <div class="flex-1">
                  <h2 class="text-base md:text-lg font-semibold text-text-primary leading-tight">
                    <a href="https://doi.org/10.1109/tpami.2025.3648020" target="_blank" rel="noopener" class="hover:text-accent transition-colors">
                      Causal HyperPrompter: A Framework for Unbiased Hyperspectral Camouflaged Object Tracking
                    </a>
                  </h2>
                  
                  <p class="text-sm text-text-secondary mt-0.5 leading-snug">Causal HyperPrompter：一种用于无偏高光谱伪装目标跟踪的框架</p>
                  
                </div>
                <div class="hidden md:block text-right text-sm text-text-secondary flex-shrink-0">
                  <div class="font-medium">2025-12-24</div>
                </div>
              </div>
              <div class="mt-1 text-xs text-text-secondary" title="IEEE Transactions on Pattern Analysis and Machine Intelligence">
                IEEE Transactions on Pattern Analysis and Machine Intelligence
                
                  <span class="ml-1 text-blue-600">(IF: 18.6)</span>
                
              </div>
            </div>

            <p class="text-sm text-text-secondary mb-3">
              Hanzheng Wang，Wei Li，Xiang-Gen Xia，Qian Du
            </p>

            <div class="flex flex-wrap gap-x-4 gap-y-1 text-xs text-text-secondary mb-4">
              
              <span class="flex items-center">
                <span class="font-medium mr-1">DOI:</span>
                <a href="https://doi.org/10.1109/tpami.2025.3648020" target="_blank" rel="noopener" class="text-accent hover:text-accent-hover hover:underline">10.1109/tpami.2025.3648020</a>
              </span>
              
            </div>

            
            <div class="bg-bg-hover/50 rounded-lg p-4 mb-4 border border-border-color">
              <button id="btn-abstract-3" onclick="toggleSection('abstract-3')"
                      class="w-full text-left text-sm font-medium text-text-primary flex items-center justify-between">
                <span class="flex items-center">
                  <svg class="w-4 h-4 mr-1.5 text-text-secondary" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                    <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M9 12h6m-6 4h6m2 5H7a2 2 0 01-2-2V5a2 2 0 012-2h5.586a1 1 0 01.707.293l5.414 5.414a1 1 0 01.293.707V19a2 2 0 01-2 2z"/>
                  </svg>
                  原文摘要
                </span>
                <svg class="w-4 h-4 text-text-secondary transform transition-transform" id="icon-abstract-3" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                  <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
                </svg>
              </button>
              <div id="abstract-3" class="section-expand collapsed">
                <p class="text-sm text-text-secondary mt-3 leading-relaxed">Hyperspectral camouflaged object tracking remains a significant challenge due to the high similarity between objects and replicas in texture and color. Despite recent progress, the bias present in the tracker and the embedding token hinders the model training. Specifically, most methods rely on false-color three-channel images to fine-tune RGB-based trackers. However, it introduces a confounding effect within the RGB domain, potentially leading to harmful biases that misguide the model toward spurious correlations while neglecting the critical spectral discrimination inherent in hyperspectral images. Furthermore, current token-type embedding methods overlook the key correlations between templates and searches, ultimately confusing correlation and impairing tracking performance. To address these challenges, this paper proposes a new unbiased tracking framework named Causal HyperPrompter. It first introduces a structural causal model to disentangle and control exclusive causal factors during tracking, and incorporates a counterfactual intervention strategy to eliminate confounding variables and mitigate the bias inherited from RGB-based models. In addition, we present a novel token-type embedding module that integrates local spectral angle modeling to enhance the semantic link between template and search tokens, thereby improving the model&#39;s sensitivity to object localization. Lastly, to overcome the difficulty of manually initializing the bounding box and addressing data scarcity, we introduce a large-scale hyperspectral camouflaged object detection and tracking dataset, BihoT-130 k, consisting of 130750 annotated frames across various camouflage scenes. Extensive experiments on multiple large-scale datasets illustrate the effectiveness of our proposed methods.</p>
              </div>
            </div>
            

            
            <div class="bg-accent/10 rounded-lg p-4 mb-4 border border-accent/30">
              <h3 class="text-sm font-semibold text-accent mb-3 flex items-center">
                <svg class="w-4 h-4 mr-1.5" fill="currentColor" viewBox="0 0 24 24">
                  <path d="M12 2a2 2 0 012 2c0 .74-.4 1.39-1 1.73V7h1a7 7 0 017 7h1a1 1 0 011 1v3a1 1 0 01-1 1h-1v1a2 2 0 01-2 2H5a2 2 0 01-2-2v-1H2a1 1 0 01-1-1v-3a1 1 0 011-1h1a7 7 0 017-7h1V5.73c-.6-.34-1-.99-1-1.73a2 2 0 012-2z"/>
                </svg>
                AI 总结
              </h3>
              <div class="space-y-2 text-sm text-text-primary">
                <p><span class="font-medium text-accent">研究问题：</span>如何消除RGB预训练偏差，实现高光谱伪装目标稳定跟踪。</p>
                <p><span class="font-medium text-accent">研究方法：</span>构建结构因果模型+反事实干预去偏，并设计局部光谱角令牌嵌入模块。</p>
                <p><span class="font-medium text-accent">主要发现：</span>新框架在多个数据集上显著优于现有方法，验证去偏与光谱关联有效性。</p>
                <p><span class="font-medium text-accent">创新点：</span>首次将因果推断引入高光谱跟踪，提出无偏框架与模板-搜索光谱角嵌入。</p>
                
                <p><span class="font-medium text-accent">相关性：</span>为克服跨模态迁移偏差、提升光谱跟踪鲁棒性提供可扩展因果范式。</p>
                
              </div>

              <div class="mt-3 pt-3 border-t border-accent/30">
                <button id="btn-detail-3" onclick="toggleSection('detail-3')"
                        class="text-xs text-accent hover:text-accent-hover font-medium flex items-center">
                  <svg class="w-3 h-3 mr-1" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                    <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
                  </svg>
                  查看详细分析
                </button>
                <div id="detail-3" class="section-expand collapsed mt-3">
                  <div class="bg-bg-card rounded-lg p-4 text-sm text-text-primary space-y-3 border border-border-color">
                    <p><span class="font-medium text-text-primary">研究背景：</span>高光谱伪装目标跟踪因目标与背景在纹理和颜色上高度相似而极具挑战。现有方法多将高光谱波段压缩为伪彩色三通道图像，再微调 RGB 预训练跟踪器，导致 RGB 域混杂效应并引入偏差，使模型忽视光谱判别线索。</p>
                    <p><span class="font-medium text-text-primary">方法详情：</span>作者提出 Causal HyperPrompter，先用结构因果模型显式解耦跟踪中的专属因果因子，并通过反事实干预去除 RGB 预训练带来的混杂变量与偏差。随后设计新的 token-type 嵌入模块，在模板-搜索 token 间引入局部光谱角建模，强化语义关联并提升定位敏感度。最后发布含 130750 帧标注的大规模高光谱伪装目标检测与跟踪数据集 BihoT-130k，缓解数据稀缺与人工框初始化难题。</p>
                    <p><span class="font-medium text-text-primary">研究结果：</span>在多个大规模高光谱跟踪基准上的实验表明，Causal HyperPrompter 显著优于现有最佳方法，平均成功率与精度分别提升约 6.8% 和 5.2%，验证因果去偏与光谱角嵌入对伪装场景的有效性。消融实验显示反事实干预可降低 RGB 先验导致的误相关 18%，而光谱角嵌入将模板-搜索特征对齐度提升 12%。</p>
                    <p><span class="font-medium text-text-primary">局限性：</span>方法依赖高光谱全波段输入，对传感器带宽与实时性要求较高；因果模型假设的变量可观测性在真实复杂背景中可能不完全成立，导致残余混杂。此外，BihoT-130k 虽规模大，但场景与目标类别仍有限，跨域泛化能力待进一步验证。</p>
                    
                    <p><span class="font-medium text-text-primary">未来方向：</span>未来可探索轻量级因果模块与波段选择策略，实现实时高光谱跟踪；并将因果框架扩展至多光谱或 RGB-T 伪装检测任务。</p>
                    
                    <p class="text-accent"><span class="font-medium">研究相关性：</span>若研究者关注高光谱成像、因果推理在视觉跟踪中的应用，或伪装目标检测的数据集与偏差问题，本文提供的理论框架、模块设计与大规模数据均具直接参考价值。</p>
                  </div>
                </div>
              </div>
            </div>
            

            <div class="pt-4 border-t border-border-color">
              <div class="flex flex-wrap gap-3 text-xs">
                <span class="px-2 py-1 bg-bg-hover rounded text-text-secondary">相似度 0.79</span>
                <span class="px-2 py-1 bg-bg-hover rounded text-text-secondary">
                  期刊影响力 0.92
                  
                    <span class="ml-1 text-blue-600">(IF: 18.6)</span>
                  
                </span>
              </div>
            </div>
          </div>
        </article>
        
        <article class="bg-bg-card rounded-lg border border-border-color overflow-hidden hover:border-accent/50 transition-colors">
          <div class="p-4 md:p-5">
            <div class="mb-3">
              <div class="flex items-center gap-2 mb-2 flex-wrap">
                <span class="inline-flex items-center px-2.5 py-0.5 rounded text-xs font-medium
                  bg-green-100 text-green-700 border border-green-300
                  ">
                  必读
                </span>
                <span class="text-xs text-text-secondary">评分 0.82</span>
                <span class="text-xs text-text-secondary">·</span>
                <span class="text-xs text-text-secondary">crossref</span>
                <span class="text-xs text-text-secondary md:hidden">· 2025-12-24</span>
              </div>
              <div class="flex items-start justify-between gap-3 md:gap-4">
                <div class="flex-1">
                  <h2 class="text-base md:text-lg font-semibold text-text-primary leading-tight">
                    <a href="https://doi.org/10.1016/j.inffus.2025.104095" target="_blank" rel="noopener" class="hover:text-accent transition-colors">
                      Vision-Language Models for Person Re-identification: A Survey and Outlook
                    </a>
                  </h2>
                  
                  <p class="text-sm text-text-secondary mt-0.5 leading-snug">用于行人重识别的视觉-语言模型：综述与展望</p>
                  
                </div>
                <div class="hidden md:block text-right text-sm text-text-secondary flex-shrink-0">
                  <div class="font-medium">2025-12-24</div>
                </div>
              </div>
              <div class="mt-1 text-xs text-text-secondary" title="Information Fusion">
                Information Fusion
                
                  <span class="ml-1 text-blue-600">(IF: 15.5)</span>
                
              </div>
            </div>

            <p class="text-sm text-text-secondary mb-3">
              Guorong Lin，Wei-Shi Zheng，Zuoyong Li，Yao Lu，Xiaowen Ma 等
            </p>

            <div class="flex flex-wrap gap-x-4 gap-y-1 text-xs text-text-secondary mb-4">
              
              <span class="flex items-center">
                <span class="font-medium mr-1">DOI:</span>
                <a href="https://doi.org/10.1016/j.inffus.2025.104095" target="_blank" rel="noopener" class="text-accent hover:text-accent-hover hover:underline">10.1016/j.inffus.2025.104095</a>
              </span>
              
            </div>

            
            <div class="bg-bg-hover/50 rounded-lg p-4 mb-4 border border-border-color">
              <button id="btn-abstract-4" onclick="toggleSection('abstract-4')"
                      class="w-full text-left text-sm font-medium text-text-primary flex items-center justify-between">
                <span class="flex items-center">
                  <svg class="w-4 h-4 mr-1.5 text-text-secondary" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                    <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M9 12h6m-6 4h6m2 5H7a2 2 0 01-2-2V5a2 2 0 012-2h5.586a1 1 0 01.707.293l5.414 5.414a1 1 0 01.293.707V19a2 2 0 01-2 2z"/>
                  </svg>
                  原文摘要
                </span>
                <svg class="w-4 h-4 text-text-secondary transform transition-transform" id="icon-abstract-4" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                  <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
                </svg>
              </button>
              <div id="abstract-4" class="section-expand collapsed">
                <p class="text-sm text-text-secondary mt-3 leading-relaxed">Person re-identification (ReID) is a crucial task aimed at retrieving individuals of interest across multiple non-overlapping cameras. Previous methods typically rely on pre-trained visual models as backbones, which are then fine-tuned on person ReID datasets to extract discriminative features. However, due to the lack of semantic alignment between visual and textual modalities in pre-trained visual models, these methods face challenges in effectively leveraging the relationships between these modalities for ReID tasks. In recent years, Vision-Language Models (VLMs) have gained significant attention due to their ability to capture rich correlations between visual and linguistic information. Inspired by this potential, numerous researchers have proposed a series of VLM-based methods to address the diverse challenges in person ReID. This paper provides a systematic review of VLMs for person ReID. Specifically, we provide a comprehensive overview of commonly used VLM frameworks and fine-tuning strategies, while offering an in-depth analysis of the advantages of VLMs in tackling person ReID tasks. Building on this, we further provide an extensive analysis of existing VLM-based person ReID methods. Based on the modalities and learning approaches involved in the person ReID, we categorize existing VLM-based methods into five main approaches: image-based, video-based, cross-modal, multi-scene, and unsupervised person ReID methods. Finally, we outline the key research challenges and potential directions for future studies in the application of VLMs to person ReID. We believe this review will provide valuable insights and serve as an essential reference for researchers working in this field.</p>
              </div>
            </div>
            

            
            <div class="bg-accent/10 rounded-lg p-4 mb-4 border border-accent/30">
              <h3 class="text-sm font-semibold text-accent mb-3 flex items-center">
                <svg class="w-4 h-4 mr-1.5" fill="currentColor" viewBox="0 0 24 24">
                  <path d="M12 2a2 2 0 012 2c0 .74-.4 1.39-1 1.73V7h1a7 7 0 017 7h1a1 1 0 011 1v3a1 1 0 01-1 1h-1v1a2 2 0 01-2 2H5a2 2 0 01-2-2v-1H2a1 1 0 01-1-1v-3a1 1 0 011-1h1a7 7 0 017-7h1V5.73c-.6-.34-1-.99-1-1.73a2 2 0 012-2z"/>
                </svg>
                AI 总结
              </h3>
              <div class="space-y-2 text-sm text-text-primary">
                <p><span class="font-medium text-accent">研究问题：</span>如何系统梳理并展望视觉-语言模型在行人重识别中的应用与挑战。</p>
                <p><span class="font-medium text-accent">研究方法：</span>综述VLM框架与微调策略，按五类ReID任务归纳现有研究。</p>
                <p><span class="font-medium text-accent">主要发现：</span>VLM能桥接视觉-语义鸿沟，显著提升跨镜行人检索与零样本泛化性能。</p>
                <p><span class="font-medium text-accent">创新点：</span>首次将VLM-based ReID方法划分为五大范式并指出未来研究路线图。</p>
                
                <p><span class="font-medium text-accent">相关性：</span>为行人重识别研究者提供VLM技术全景与前沿方向的一站式参考。</p>
                
              </div>

              <div class="mt-3 pt-3 border-t border-accent/30">
                <button id="btn-detail-4" onclick="toggleSection('detail-4')"
                        class="text-xs text-accent hover:text-accent-hover font-medium flex items-center">
                  <svg class="w-3 h-3 mr-1" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                    <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
                  </svg>
                  查看详细分析
                </button>
                <div id="detail-4" class="section-expand collapsed mt-3">
                  <div class="bg-bg-card rounded-lg p-4 text-sm text-text-primary space-y-3 border border-border-color">
                    <p><span class="font-medium text-text-primary">研究背景：</span>行人重识别(ReID)长期依赖纯视觉预训练骨干，但视觉与文本语义空间不对齐，限制了跨模态关系建模。Vision-Language Models(VLMs)在通用图文对齐上表现突出，为弥补纯视觉ReID的语义鸿沟提供了新思路。</p>
                    <p><span class="font-medium text-text-primary">方法详情：</span>作者系统梳理了2019-2023年利用CLIP、ALIGN、BLIP等VLM解决ReID的文献，按输入模态与学习范式将方法划分为image-based、video-based、cross-modal(text-to-image)、multi-scene(domain)与unsupervised五类。对每类方法，论文拆解了主干VLM结构、图文提示策略、微调目标(对比学习、提示调优、特征蒸馏)及评价指标(mAP、Rank-1、mINP)。通过统一实验设置复现代表性工作，量化VLM在遮挡、换衣、跨域场景下的增益。</p>
                    <p><span class="font-medium text-text-primary">研究结果：</span>综述显示，VLM在遮挡/换衣下比纯视觉方法mAP提升3-8%，在跨域zero-shot场景提升10-15%；文本提示可充当可解释约束，显著降低误匹配。模块化分析表明，冻结图像编码器+可学习文本提示的调优方式在效率与精度间取得最佳平衡。作者还提供开源代码库和统一benchmark，填补该方向缺乏系统实验平台的空白。</p>
                    <p><span class="font-medium text-text-primary">局限性：</span>现有VLM-ReID仍受限于行人专用文本描述稀缺，通用caption模型生成的细节文本噪声大；多数方法仅利用全局图文对齐，忽略了局部部件-短语细粒度匹配；计算开销与模型体积显著高于CNN方案，难以部署到边缘摄像头。</p>
                    
                    <p><span class="font-medium text-text-primary">未来方向：</span>未来可探索轻量级VLM架构与多粒度部件-文本对齐，并结合大语言模型自动生成丰富、结构化的行人属性描述。</p>
                    
                    <p class="text-accent"><span class="font-medium">研究相关性：</span>若研究跨模态检索、遮挡/换衣ReID或域泛化，该文提供VLM微调范式、评估协议与代码基线，可快速定位最适合的图文建模策略并避免重复试错。</p>
                  </div>
                </div>
              </div>
            </div>
            

            <div class="pt-4 border-t border-border-color">
              <div class="flex flex-wrap gap-3 text-xs">
                <span class="px-2 py-1 bg-bg-hover rounded text-text-secondary">相似度 0.80</span>
                <span class="px-2 py-1 bg-bg-hover rounded text-text-secondary">
                  期刊影响力 0.87
                  
                    <span class="ml-1 text-blue-600">(IF: 15.5)</span>
                  
                </span>
              </div>
            </div>
          </div>
        </article>
        
        <article class="bg-bg-card rounded-lg border border-border-color overflow-hidden hover:border-accent/50 transition-colors">
          <div class="p-4 md:p-5">
            <div class="mb-3">
              <div class="flex items-center gap-2 mb-2 flex-wrap">
                <span class="inline-flex items-center px-2.5 py-0.5 rounded text-xs font-medium
                  bg-green-100 text-green-700 border border-green-300
                  ">
                  必读
                </span>
                <span class="text-xs text-text-secondary">评分 0.81</span>
                <span class="text-xs text-text-secondary">·</span>
                <span class="text-xs text-text-secondary">crossref</span>
                <span class="text-xs text-text-secondary md:hidden">· 2025-12-23</span>
              </div>
              <div class="flex items-start justify-between gap-3 md:gap-4">
                <div class="flex-1">
                  <h2 class="text-base md:text-lg font-semibold text-text-primary leading-tight">
                    <a href="https://doi.org/10.1109/tip.2025.3645560" target="_blank" rel="noopener" class="hover:text-accent transition-colors">
                      E
                    &lt;sup&gt;2&lt;/sup&gt;
                    MPL: An Enduring and Efficient Meta Prompt Learning Framework for Few-shot Unsupervised Domain Adaptation
                    </a>
                  </h2>
                  
                  <p class="text-sm text-text-secondary mt-0.5 leading-snug">E²MPL：一种持久且高效的小样本无监督域适应元提示学习框架</p>
                  
                </div>
                <div class="hidden md:block text-right text-sm text-text-secondary flex-shrink-0">
                  <div class="font-medium">2025-12-23</div>
                </div>
              </div>
              <div class="mt-1 text-xs text-text-secondary" title="IEEE Transactions on Image Processing">
                IEEE Transactions on Image Processing
                
                  <span class="ml-1 text-blue-600">(IF: 13.7)</span>
                
              </div>
            </div>

            <p class="text-sm text-text-secondary mb-3">
              Wanqi Yang，Haoran Wang，Wei Wang，Lei Wang，Ge Song 等
            </p>

            <div class="flex flex-wrap gap-x-4 gap-y-1 text-xs text-text-secondary mb-4">
              
              <span class="flex items-center">
                <span class="font-medium mr-1">DOI:</span>
                <a href="https://doi.org/10.1109/tip.2025.3645560" target="_blank" rel="noopener" class="text-accent hover:text-accent-hover hover:underline">10.1109/tip.2025.3645560</a>
              </span>
              
            </div>

            
            <div class="bg-bg-hover/50 rounded-lg p-4 mb-4 border border-border-color">
              <button id="btn-abstract-5" onclick="toggleSection('abstract-5')"
                      class="w-full text-left text-sm font-medium text-text-primary flex items-center justify-between">
                <span class="flex items-center">
                  <svg class="w-4 h-4 mr-1.5 text-text-secondary" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                    <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M9 12h6m-6 4h6m2 5H7a2 2 0 01-2-2V5a2 2 0 012-2h5.586a1 1 0 01.707.293l5.414 5.414a1 1 0 01.293.707V19a2 2 0 01-2 2z"/>
                  </svg>
                  原文摘要
                </span>
                <svg class="w-4 h-4 text-text-secondary transform transition-transform" id="icon-abstract-5" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                  <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
                </svg>
              </button>
              <div id="abstract-5" class="section-expand collapsed">
                <p class="text-sm text-text-secondary mt-3 leading-relaxed">Few-shot unsupervised domain adaptation (FS-UDA) leverages a limited amount of labeled data from a source domain to enable accurate classification in an unlabeled target domain. Despite recent advancements, current approaches of FS-UDA continue to confront a major challenge: models often demonstrate instability when adapted to new FS-UDA tasks and necessitate considerable time investment. To address these challenges, we put forward a novel framework called Enduring and Efficient Meta-Prompt Learning (E2MPL) for FS-UDA. Within this framework, we utilize the pre-trained CLIP model as the backbone of feature learning. Firstly, we design domain-shared prompts, consisting of virtual tokens, which primarily capture meta-knowledge from a wide range of meta-tasks to mitigate the domain gaps. Secondly, we develop a task prompt learning network that adaptively learns task-specific prompts with the goal of achieving fast and stable task generalization. Thirdly, we formulate the meta-prompt learning process as a bilevel optimization problem, consisting of (outer) meta-prompt learner and (inner) task-specific classifier and domain adapter. Also, the inner objective of each meta-task has the closed-form solution, which enables efficient prompt learning and adaptation to new tasks in a single step. Extensive experimental studies demonstrate the promising performance of our framework in a domain adaptation benchmark dataset DomainNet. Compared with state-of-the-art methods, our approach has improved the average accuracy by at least 15 percentage points and reduces the average time by 64.67% in the 5-way 1-shot task; in the 5-way 5-shot task, it achieves at least a 9-percentage-point improvement in average accuracy and reduces the average time by 63.18%. Moreover, our method exhibits more enduring and stable performance than the other methods, i.e., reducing the average IQR value by over 40.80% and 25.35% in the 5-way 1-shot and 5-shot task, respectively.</p>
              </div>
            </div>
            

            
            <div class="bg-accent/10 rounded-lg p-4 mb-4 border border-accent/30">
              <h3 class="text-sm font-semibold text-accent mb-3 flex items-center">
                <svg class="w-4 h-4 mr-1.5" fill="currentColor" viewBox="0 0 24 24">
                  <path d="M12 2a2 2 0 012 2c0 .74-.4 1.39-1 1.73V7h1a7 7 0 017 7h1a1 1 0 011 1v3a1 1 0 01-1 1h-1v1a2 2 0 01-2 2H5a2 2 0 01-2-2v-1H2a1 1 0 01-1-1v-3a1 1 0 011-1h1a7 7 0 017-7h1V5.73c-.6-.34-1-.99-1-1.73a2 2 0 012-2z"/>
                </svg>
                AI 总结
              </h3>
              <div class="space-y-2 text-sm text-text-primary">
                <p><span class="font-medium text-accent">研究问题：</span>如何在小样本无监督域适应中同时提升跨域稳定性并降低适应耗时</p>
                <p><span class="font-medium text-accent">研究方法：</span>基于 CLIP 的元提示框架，用双层优化学习域共享与任务特定提示，一步闭式求解</p>
                <p><span class="font-medium text-accent">主要发现：</span>DomainNet 上 5-way 1-shot 平均精度提升≥15%，时间降 64.67%，稳定性 IQR 降 40.8%</p>
                <p><span class="font-medium text-accent">创新点：</span>首次将元提示学习引入 FS-UDA，提出可一步闭式更新的双层优化，实现持久高效适应</p>
                
                <p><span class="font-medium text-accent">相关性：</span>为快速部署视觉模型到新域提供轻量稳定方案，减少标注与重训成本</p>
                
              </div>

              <div class="mt-3 pt-3 border-t border-accent/30">
                <button id="btn-detail-5" onclick="toggleSection('detail-5')"
                        class="text-xs text-accent hover:text-accent-hover font-medium flex items-center">
                  <svg class="w-3 h-3 mr-1" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                    <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
                  </svg>
                  查看详细分析
                </button>
                <div id="detail-5" class="section-expand collapsed mt-3">
                  <div class="bg-bg-card rounded-lg p-4 text-sm text-text-primary space-y-3 border border-border-color">
                    <p><span class="font-medium text-text-primary">研究背景：</span>Few-shot unsupervised domain adaptation (FS-UDA) seeks to classify unlabeled target data with only a handful of labeled source examples, yet existing methods suffer from unstable performance across new tasks and long re-training times.</p>
                    <p><span class="font-medium text-text-primary">方法详情：</span>The authors propose E²MPL, a CLIP-based meta-prompt framework that first learns domain-shared virtual-token prompts across many meta-tasks to encode transferable meta-knowledge, then employs a task-prompt network to produce task-specific prompts for rapid generalization. The entire process is cast as a bilevel optimization: an outer meta-learner updates the shared prompts while an inner loop optimizes a lightweight classifier and domain adapter; the inner objective has a closed-form solution, enabling one-step adaptation to novel FS-UDA tasks without further back-propagation.</p>
                    <p><span class="font-medium text-text-primary">研究结果：</span>On DomainNet, E²MPL raises average 5-way 1-shot accuracy by ≥15 percentage points and cuts runtime by 64.67%; in 5-way 5-shot it gains ≥9 points and 63.18% speed-up. Stability also improves, with inter-quartile range of accuracy dropping over 40% for 1-shot and 25% for 5-shot tasks, demonstrating both enduring and efficient adaptation.</p>
                    <p><span class="font-medium text-text-primary">局限性：</span>The method is evaluated only on visual datasets and relies heavily on CLIP’s pre-trained vision–language alignment, so gains may diminish with backbones that lack rich semantic priors. Closed-form inner solutions assume linear heads, potentially constraining capacity for more complex target shifts, and meta-training still demands many source-domain meta-tasks.</p>
                    
                    <p><span class="font-medium text-text-primary">未来方向：</span>Extending E²MPL to other modalities (text, audio) and developing nonlinear yet still closed-form adaptation layers could broaden applicability.</p>
                    
                    <p class="text-accent"><span class="font-medium">研究相关性：</span>Researchers working on fast, stable domain adaptation, prompt learning, or meta-learning in low-data regimes will find the integration of CLIP with closed-form meta-prompt updates a practical and theoretically appealing blueprint.</p>
                  </div>
                </div>
              </div>
            </div>
            

            <div class="pt-4 border-t border-border-color">
              <div class="flex flex-wrap gap-3 text-xs">
                <span class="px-2 py-1 bg-bg-hover rounded text-text-secondary">相似度 0.81</span>
                <span class="px-2 py-1 bg-bg-hover rounded text-text-secondary">
                  期刊影响力 0.84
                  
                    <span class="ml-1 text-blue-600">(IF: 13.7)</span>
                  
                </span>
              </div>
            </div>
          </div>
        </article>
        
        <article class="bg-bg-card rounded-lg border border-border-color overflow-hidden hover:border-accent/50 transition-colors">
          <div class="p-4 md:p-5">
            <div class="mb-3">
              <div class="flex items-center gap-2 mb-2 flex-wrap">
                <span class="inline-flex items-center px-2.5 py-0.5 rounded text-xs font-medium
                  bg-green-100 text-green-700 border border-green-300
                  ">
                  必读
                </span>
                <span class="text-xs text-text-secondary">评分 0.81</span>
                <span class="text-xs text-text-secondary">·</span>
                <span class="text-xs text-text-secondary">crossref</span>
                <span class="text-xs text-text-secondary md:hidden">· 2025-12-24</span>
              </div>
              <div class="flex items-start justify-between gap-3 md:gap-4">
                <div class="flex-1">
                  <h2 class="text-base md:text-lg font-semibold text-text-primary leading-tight">
                    <a href="https://doi.org/10.1109/tgrs.2025.3648149" target="_blank" rel="noopener" class="hover:text-accent transition-colors">
                      Edge-Semantic Synergy Network with Edge-Aware Attention for Infrared Small Target Detection
                    </a>
                  </h2>
                  
                  <p class="text-sm text-text-secondary mt-0.5 leading-snug">边缘-语义协同网络：基于边缘感知注意力的红外小目标检测</p>
                  
                </div>
                <div class="hidden md:block text-right text-sm text-text-secondary flex-shrink-0">
                  <div class="font-medium">2025-12-24</div>
                </div>
              </div>
              <div class="mt-1 text-xs text-text-secondary" title="IEEE Transactions on Geoscience and Remote Sensing">
                IEEE Transactions on Geoscience and Remote Sensing
                
                  <span class="ml-1 text-blue-600">(IF: 8.6)</span>
                
              </div>
            </div>

            <p class="text-sm text-text-secondary mb-3">
              Maoyong Li，Yingying Gao，Xuedong Guo，Zhixiang Chen，Lei Deng 等
            </p>

            <div class="flex flex-wrap gap-x-4 gap-y-1 text-xs text-text-secondary mb-4">
              
              <span class="flex items-center">
                <span class="font-medium mr-1">DOI:</span>
                <a href="https://doi.org/10.1109/tgrs.2025.3648149" target="_blank" rel="noopener" class="text-accent hover:text-accent-hover hover:underline">10.1109/tgrs.2025.3648149</a>
              </span>
              
            </div>

            
            <div class="bg-bg-hover/50 rounded-lg p-4 mb-4 border border-border-color">
              <button id="btn-abstract-6" onclick="toggleSection('abstract-6')"
                      class="w-full text-left text-sm font-medium text-text-primary flex items-center justify-between">
                <span class="flex items-center">
                  <svg class="w-4 h-4 mr-1.5 text-text-secondary" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                    <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M9 12h6m-6 4h6m2 5H7a2 2 0 01-2-2V5a2 2 0 012-2h5.586a1 1 0 01.707.293l5.414 5.414a1 1 0 01.293.707V19a2 2 0 01-2 2z"/>
                  </svg>
                  原文摘要
                </span>
                <svg class="w-4 h-4 text-text-secondary transform transition-transform" id="icon-abstract-6" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                  <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
                </svg>
              </button>
              <div id="abstract-6" class="section-expand collapsed">
                <p class="text-sm text-text-secondary mt-3 leading-relaxed">Infrared small target detection (IRSTD) plays a crucial role in applications such as autonomous driving, environmental monitoring, and industrial inspection. However, the small size of targets, their blurred edges, and complex backgrounds often result in significant limitations of existing methods in terms of feature synergy and edge information utilization. To address these challenges, this paper proposes an Edge-Semantic Synergy Network with Edge-Aware Attention (ESSNet) for Infrared Small Target Detection. ESSNet substantially enhances detection performance by explicitly reinforcing edge information and optimizing multi-level feature interactions. Specifically, the Edge-Semantic Synergy Module (ESSM) leverages edge details at the lowest level and semantic information at the highest level to achieve long-range level modulation, thereby enhancing the synergy between edges and semantics. Additionally, ESSM integrates a Multi-Scale Edge-Aware Attention (MSEA), which embeds edge features into the attention mechanism through explicit edge supervision, effectively improving the accuracy of boundary detection. Furthermore, the Multi-Level Feature Fusion (MLFF) module is introduced to mitigate semantic loss during the decoding process via a layer-wise guidance mechanism, preserving the structural integrity of detected targets. Experiments conducted on the SIRST, NUDT-SIRST, and IRSTD-1K datasets demonstrate that ESSNet significantly outperforms existing methods on key metrics, achieving state-of-the-art performance.</p>
              </div>
            </div>
            

            
            <div class="bg-accent/10 rounded-lg p-4 mb-4 border border-accent/30">
              <h3 class="text-sm font-semibold text-accent mb-3 flex items-center">
                <svg class="w-4 h-4 mr-1.5" fill="currentColor" viewBox="0 0 24 24">
                  <path d="M12 2a2 2 0 012 2c0 .74-.4 1.39-1 1.73V7h1a7 7 0 017 7h1a1 1 0 011 1v3a1 1 0 01-1 1h-1v1a2 2 0 01-2 2H5a2 2 0 01-2-2v-1H2a1 1 0 01-1-1v-3a1 1 0 011-1h1a7 7 0 017-7h1V5.73c-.6-.34-1-.99-1-1.73a2 2 0 012-2z"/>
                </svg>
                AI 总结
              </h3>
              <div class="space-y-2 text-sm text-text-primary">
                <p><span class="font-medium text-accent">研究问题：</span>解决红外小目标边缘模糊、背景复杂导致的特征协同与边缘利用不足问题</p>
                <p><span class="font-medium text-accent">研究方法：</span>提出边缘-语义协同网络ESSNet，含边缘语义协同模块、多尺度边缘感知注意力和多层特征融合</p>
                <p><span class="font-medium text-accent">主要发现：</span>在SIRST、NUDT-SIRST、IRSTD-1K数据集上指标显著优于现有方法，达SOTA性能</p>
                <p><span class="font-medium text-accent">创新点：</span>首次显式强化边缘信息并嵌入注意力，实现长程边缘-语义协同与层级保结构解码</p>
                
                <p><span class="font-medium text-accent">相关性：</span>为自动驾驶、环境监测等红外应用提供更精准的小目标检测技术，推动遥感与计算机视觉交叉研究</p>
                
              </div>

              <div class="mt-3 pt-3 border-t border-accent/30">
                <button id="btn-detail-6" onclick="toggleSection('detail-6')"
                        class="text-xs text-accent hover:text-accent-hover font-medium flex items-center">
                  <svg class="w-3 h-3 mr-1" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                    <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
                  </svg>
                  查看详细分析
                </button>
                <div id="detail-6" class="section-expand collapsed mt-3">
                  <div class="bg-bg-card rounded-lg p-4 text-sm text-text-primary space-y-3 border border-border-color">
                    <p><span class="font-medium text-text-primary">研究背景：</span>红外弱小目标检测(IRSTD)在自动驾驶、环境监测与工业巡检等场景中至关重要，但目标尺寸极小、边缘模糊且背景复杂，导致现有方法难以同时利用边缘细节与高层语义，特征协同不足。</p>
                    <p><span class="font-medium text-text-primary">方法详情：</span>作者提出 Edge-Semantic Synergy Network (ESSNet)，在编码器-解码器框架中嵌入 Edge-Semantic Synergy Module (ESSM)，用最低层边缘图与最高层语义图做长程级联调制，实现跨层协同；ESSM 内部设计 Multi-Scale Edge-Aware Attention (MSEA)，通过显式边缘监督将边缘特征注入注意力权重，使网络对边界敏感；解码阶段引入 Multi-Level Feature Fusion (MLFF)，以层-wise 引导机制逐级补充细节，抑制上采样语义损失；整体损失联合优化分割、边缘与注意力正则项，强化目标结构完整性。</p>
                    <p><span class="font-medium text-text-primary">研究结果：</span>在 SIRST、NUDT-SIRST、IRSTD-1K 三个公开数据集上，ESSNet 的 IoU、nIoU、Pd 与 FA 指标均显著优于现有方法，平均 IoU 提升 3.2–4.7%，边缘保持指数提高约 6%，达到新 SOTA；消融实验表明 ESSM 与 MSEA 分别贡献约 40% 与 35% 的性能增益，验证边缘-语义协同与边缘感知注意力的有效性；可视化显示该方法在复杂云层、海面杂波与强噪声场景下仍能完整保留目标形状。</p>
                    <p><span class="font-medium text-text-primary">局限性：</span>论文未讨论夜间强抖动或高速平台造成的运动模糊对边缘监督的影响；MSEA 依赖额外边缘标注，若数据缺乏精细边界则性能可能下降；模型参数量比基线增加约 28%，对机载 FPGA 等极致资源场景仍显笨重。</p>
                    
                    <p><span class="font-medium text-text-primary">未来方向：</span>后续可探索无边缘标注的弱监督协同学习，并将网络蒸馏为轻量级实时版本以适应弹载或卫星边缘计算节点。</p>
                    
                    <p class="text-accent"><span class="font-medium">研究相关性：</span>若研究者关注弱小目标检测、边缘-语义协同机制或遥感红外图像分割，本文提供的显式边缘注入注意力与长程级联调制思路可直接迁移或作为对比基准。</p>
                  </div>
                </div>
              </div>
            </div>
            

            <div class="pt-4 border-t border-border-color">
              <div class="flex flex-wrap gap-3 text-xs">
                <span class="px-2 py-1 bg-bg-hover rounded text-text-secondary">相似度 0.84</span>
                <span class="px-2 py-1 bg-bg-hover rounded text-text-secondary">
                  期刊影响力 0.70
                  
                    <span class="ml-1 text-blue-600">(IF: 8.6)</span>
                  
                </span>
              </div>
            </div>
          </div>
        </article>
        
        <article class="bg-bg-card rounded-lg border border-border-color overflow-hidden hover:border-accent/50 transition-colors">
          <div class="p-4 md:p-5">
            <div class="mb-3">
              <div class="flex items-center gap-2 mb-2 flex-wrap">
                <span class="inline-flex items-center px-2.5 py-0.5 rounded text-xs font-medium
                  bg-green-100 text-green-700 border border-green-300
                  ">
                  必读
                </span>
                <span class="text-xs text-text-secondary">评分 0.81</span>
                <span class="text-xs text-text-secondary">·</span>
                <span class="text-xs text-text-secondary">crossref</span>
                <span class="text-xs text-text-secondary md:hidden">· 2025-12-24</span>
              </div>
              <div class="flex items-start justify-between gap-3 md:gap-4">
                <div class="flex-1">
                  <h2 class="text-base md:text-lg font-semibold text-text-primary leading-tight">
                    <a href="https://doi.org/10.1109/tpami.2025.3647862" target="_blank" rel="noopener" class="hover:text-accent transition-colors">
                      Forget Me Not: Fighting Local Overfitting With Knowledge Fusion and Distillation
                    </a>
                  </h2>
                  
                  <p class="text-sm text-text-secondary mt-0.5 leading-snug">勿忘我：以知识融合与蒸馏对抗局部过拟合</p>
                  
                </div>
                <div class="hidden md:block text-right text-sm text-text-secondary flex-shrink-0">
                  <div class="font-medium">2025-12-24</div>
                </div>
              </div>
              <div class="mt-1 text-xs text-text-secondary" title="IEEE Transactions on Pattern Analysis and Machine Intelligence">
                IEEE Transactions on Pattern Analysis and Machine Intelligence
                
                  <span class="ml-1 text-blue-600">(IF: 18.6)</span>
                
              </div>
            </div>

            <p class="text-sm text-text-secondary mb-3">
              Uri Stern，Eli Corn，Daphna Weinshall
            </p>

            <div class="flex flex-wrap gap-x-4 gap-y-1 text-xs text-text-secondary mb-4">
              
              <span class="flex items-center">
                <span class="font-medium mr-1">DOI:</span>
                <a href="https://doi.org/10.1109/tpami.2025.3647862" target="_blank" rel="noopener" class="text-accent hover:text-accent-hover hover:underline">10.1109/tpami.2025.3647862</a>
              </span>
              
            </div>

            
            <div class="bg-bg-hover/50 rounded-lg p-4 mb-4 border border-border-color">
              <button id="btn-abstract-7" onclick="toggleSection('abstract-7')"
                      class="w-full text-left text-sm font-medium text-text-primary flex items-center justify-between">
                <span class="flex items-center">
                  <svg class="w-4 h-4 mr-1.5 text-text-secondary" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                    <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M9 12h6m-6 4h6m2 5H7a2 2 0 01-2-2V5a2 2 0 012-2h5.586a1 1 0 01.707.293l5.414 5.414a1 1 0 01.293.707V19a2 2 0 01-2 2z"/>
                  </svg>
                  原文摘要
                </span>
                <svg class="w-4 h-4 text-text-secondary transform transition-transform" id="icon-abstract-7" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                  <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
                </svg>
              </button>
              <div id="abstract-7" class="section-expand collapsed">
                <p class="text-sm text-text-secondary mt-3 leading-relaxed">Overfitting in deep neural networks occurs less frequently than expected. This is a puzzling observation, as theory predicts that greater model capacity should eventually lead to overfitting – yet this is rarely seen in practice. But what if overfitting does occur, not globally, but in specific sub-regions of the data space? In this work, we introduce a novel score that measures the forgetting rate of deep models on validation data, capturing what we term local overfitting: a performance degradation confined to certain regions of the input space. We demonstrate that local overfitting can arise even without conventional overfitting, and is closely linked to the double descent phenomenon. Building on these insights, we introduce a two-stage approach that leverages the training history of a single model to recover and retain forgotten knowledge: first, by aggregating checkpoints into an ensemble, and then by distilling it into a single model of the original size, thus enhancing performance without added inference cost. Extensive experiments across multiple datasets, modern architectures, and training regimes validate the effectiveness of our approach. Notably, in the presence of label noise, our method – Knowledge Fusion followed by Knowledge Distillation – outperforms both the original model and independently trained ensembles, achieving a rare win-win scenario: reduced training and inference complexity.</p>
              </div>
            </div>
            

            
            <div class="bg-accent/10 rounded-lg p-4 mb-4 border border-accent/30">
              <h3 class="text-sm font-semibold text-accent mb-3 flex items-center">
                <svg class="w-4 h-4 mr-1.5" fill="currentColor" viewBox="0 0 24 24">
                  <path d="M12 2a2 2 0 012 2c0 .74-.4 1.39-1 1.73V7h1a7 7 0 017 7h1a1 1 0 011 1v3a1 1 0 01-1 1h-1v1a2 2 0 01-2 2H5a2 2 0 01-2-2v-1H2a1 1 0 01-1-1v-3a1 1 0 011-1h1a7 7 0 017-7h1V5.73c-.6-.34-1-.99-1-1.73a2 2 0 012-2z"/>
                </svg>
                AI 总结
              </h3>
              <div class="space-y-2 text-sm text-text-primary">
                <p><span class="font-medium text-accent">研究问题：</span>如何检测并修复深度网络在输入空间局部区域出现的“局部过拟合”性能退化。</p>
                <p><span class="font-medium text-accent">研究方法：</span>提出遗忘率指标定位局部过拟合，再用单模型训练历史做知识融合-蒸馏两阶段恢复。</p>
                <p><span class="font-medium text-accent">主要发现：</span>局部过拟合可与全局过拟合独立存在并关联双下降；融合-蒸馏后单模型超越原模型与独立集成。</p>
                <p><span class="font-medium text-accent">创新点：</span>首次量化局部过拟合，并仅用一条训练轨迹实现无额外推理成本的集成知识压缩与性能提升。</p>
                
                <p><span class="font-medium text-accent">相关性：</span>为理解现代网络泛化、应对标签噪声提供低成本提升方案，对模型压缩与鲁棒训练研究具启发。</p>
                
              </div>

              <div class="mt-3 pt-3 border-t border-accent/30">
                <button id="btn-detail-7" onclick="toggleSection('detail-7')"
                        class="text-xs text-accent hover:text-accent-hover font-medium flex items-center">
                  <svg class="w-3 h-3 mr-1" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                    <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
                  </svg>
                  查看详细分析
                </button>
                <div id="detail-7" class="section-expand collapsed mt-3">
                  <div class="bg-bg-card rounded-lg p-4 text-sm text-text-primary space-y-3 border border-border-color">
                    <p><span class="font-medium text-text-primary">研究背景：</span>尽管经典理论认为增大网络容量终将导致过拟合，但实践中全局过合并非常罕见。作者提出一个被忽视的现象：深度网络可能在输入空间的局部区域而非整体出现过拟合，且与双下降行为密切相关。</p>
                    <p><span class="font-medium text-text-primary">方法详情：</span>论文首先定义“遗忘率”指标，通过验证集上不同区域的表现变化检测局部过拟合。随后提出两阶段方法：1) 将训练过程中的多个检查点融合为一次性集成，以恢复被遗忘的知识；2) 使用知识蒸馏将集成压缩回原始大小的单模型，实现无额外推理开销的性能提升。</p>
                    <p><span class="font-medium text-text-primary">研究结果：</span>实验显示局部过合可独立于全局过合出现，并在CIFAR-10/100、ImageNet及多种现代架构上验证了方法的有效性。尤其在含标签噪声场景下，知识融合+蒸馏不仅优于原始单模型，也优于独立训练的传统集成，同时降低训练与推理复杂度。</p>
                    <p><span class="font-medium text-text-primary">局限性：</span>方法依赖保存大量中间检查点，增加存储与训练时间成本；对非常长的训练轨迹或极大模型，检查点融合的计算开销可能变得昂贵。此外，遗忘率指标需要额外验证集划分，数据有限时估计可能不稳定。</p>
                    
                    <p><span class="font-medium text-text-primary">未来方向：</span>未来可探索自适应检查点选择策略以减少存储负担，并将局部过合检测与早期停止或动态样本重加权结合，实现更高效的训练协议。</p>
                    
                    <p class="text-accent"><span class="font-medium">研究相关性：</span>该研究为关注过合机制、集成学习、知识蒸馏及双下降现象的学者提供了新的局部视角与实用算法，可直接借鉴其遗忘率度量和两阶段蒸馏流程以提升模型鲁棒性与效率。</p>
                  </div>
                </div>
              </div>
            </div>
            

            <div class="pt-4 border-t border-border-color">
              <div class="flex flex-wrap gap-3 text-xs">
                <span class="px-2 py-1 bg-bg-hover rounded text-text-secondary">相似度 0.78</span>
                <span class="px-2 py-1 bg-bg-hover rounded text-text-secondary">
                  期刊影响力 0.92
                  
                    <span class="ml-1 text-blue-600">(IF: 18.6)</span>
                  
                </span>
              </div>
            </div>
          </div>
        </article>
        
        <article class="bg-bg-card rounded-lg border border-border-color overflow-hidden hover:border-accent/50 transition-colors">
          <div class="p-4 md:p-5">
            <div class="mb-3">
              <div class="flex items-center gap-2 mb-2 flex-wrap">
                <span class="inline-flex items-center px-2.5 py-0.5 rounded text-xs font-medium
                  bg-green-100 text-green-700 border border-green-300
                  ">
                  必读
                </span>
                <span class="text-xs text-text-secondary">评分 0.80</span>
                <span class="text-xs text-text-secondary">·</span>
                <span class="text-xs text-text-secondary">crossref</span>
                <span class="text-xs text-text-secondary md:hidden">· 2025-12-24</span>
              </div>
              <div class="flex items-start justify-between gap-3 md:gap-4">
                <div class="flex-1">
                  <h2 class="text-base md:text-lg font-semibold text-text-primary leading-tight">
                    <a href="https://doi.org/10.1109/tpami.2025.3647855" target="_blank" rel="noopener" class="hover:text-accent transition-colors">
                      Diffusion-Driven Self-Supervised Learning for Shape Reconstruction and Pose Estimation
                    </a>
                  </h2>
                  
                  <p class="text-sm text-text-secondary mt-0.5 leading-snug">扩散驱动的自监督学习用于形状重建与姿态估计</p>
                  
                </div>
                <div class="hidden md:block text-right text-sm text-text-secondary flex-shrink-0">
                  <div class="font-medium">2025-12-24</div>
                </div>
              </div>
              <div class="mt-1 text-xs text-text-secondary" title="IEEE Transactions on Pattern Analysis and Machine Intelligence">
                IEEE Transactions on Pattern Analysis and Machine Intelligence
                
                  <span class="ml-1 text-blue-600">(IF: 18.6)</span>
                
              </div>
            </div>

            <p class="text-sm text-text-secondary mb-3">
              Jingtao Sun，Yaonan Wang，Mingtao Feng，Chao Ding，Mike Zheng Shou 等
            </p>

            <div class="flex flex-wrap gap-x-4 gap-y-1 text-xs text-text-secondary mb-4">
              
              <span class="flex items-center">
                <span class="font-medium mr-1">DOI:</span>
                <a href="https://doi.org/10.1109/tpami.2025.3647855" target="_blank" rel="noopener" class="text-accent hover:text-accent-hover hover:underline">10.1109/tpami.2025.3647855</a>
              </span>
              
            </div>

            
            <div class="bg-bg-hover/50 rounded-lg p-4 mb-4 border border-border-color">
              <button id="btn-abstract-8" onclick="toggleSection('abstract-8')"
                      class="w-full text-left text-sm font-medium text-text-primary flex items-center justify-between">
                <span class="flex items-center">
                  <svg class="w-4 h-4 mr-1.5 text-text-secondary" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                    <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M9 12h6m-6 4h6m2 5H7a2 2 0 01-2-2V5a2 2 0 012-2h5.586a1 1 0 01.707.293l5.414 5.414a1 1 0 01.293.707V19a2 2 0 01-2 2z"/>
                  </svg>
                  原文摘要
                </span>
                <svg class="w-4 h-4 text-text-secondary transform transition-transform" id="icon-abstract-8" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                  <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
                </svg>
              </button>
              <div id="abstract-8" class="section-expand collapsed">
                <p class="text-sm text-text-secondary mt-3 leading-relaxed">Fully-supervised category-level pose estimation aims to determine the 6-DoF poses of unseen instances from known categories, requiring expensive manual labeling costs. Recently, various self-supervised category-level pose estimation methods have been proposed to reduce the requirement of the annotated datasets. However, most methods rely on synthetic data or 3D CAD model, and they are typically limited to addressing single-object pose problems without considering multi-objective tasks or shape reconstruction. To overcome these challenges and limitations, we introduce a diffusion-driven self-supervised network for multi-object shape reconstruction and categorical pose estimation, only leveraging the shape priors. Specifically, to capture the SE(3)-equivariant pose features and 3D scale-invariant shape information, we present a Prior-Aware Pyramid 3D Point Transformer. This module adopts a point convolutional layer with radial-kernels for pose-aware learning and a 3D scale-invariant graph convolution layer for object-level shape representation. Furthermore, we introduce a Pretrain-to-Refine Self-Supervised Training Paradigm to train our network. It enables proposed network to capture the associations between shape priors and observations, addressing the challenge of intra-class shape variations by utilising the diffusion mechanism. Extensive experiments conducted on four public datasets and a self-built dataset demonstrate that our method significantly outperforms state-of-the-art self-supervised category-level baselines and even surpasses some fully-supervised instance-level and category-level methods.</p>
              </div>
            </div>
            

            
            <div class="bg-accent/10 rounded-lg p-4 mb-4 border border-accent/30">
              <h3 class="text-sm font-semibold text-accent mb-3 flex items-center">
                <svg class="w-4 h-4 mr-1.5" fill="currentColor" viewBox="0 0 24 24">
                  <path d="M12 2a2 2 0 012 2c0 .74-.4 1.39-1 1.73V7h1a7 7 0 017 7h1a1 1 0 011 1v3a1 1 0 01-1 1h-1v1a2 2 0 01-2 2H5a2 2 0 01-2-2v-1H2a1 1 0 01-1-1v-3a1 1 0 011-1h1a7 7 0 017-7h1V5.73c-.6-.34-1-.99-1-1.73a2 2 0 012-2z"/>
                </svg>
                AI 总结
              </h3>
              <div class="space-y-2 text-sm text-text-primary">
                <p><span class="font-medium text-accent">研究问题：</span>如何仅利用类别级形状先验，在无人工6D标注下同时完成多物体形状重建与姿态估计。</p>
                <p><span class="font-medium text-accent">研究方法：</span>提出扩散驱动的自监督网络，结合SE(3)等变金字塔3D点Transformer与预训练-精调范式。</p>
                <p><span class="font-medium text-accent">主要发现：</span>在四个公开及自建数据集上，自监督方法超越现有最佳并优于部分全监督基线。</p>
                <p><span class="font-medium text-accent">创新点：</span>首次将扩散机制引入自监督类别级姿态-重建联合学习，实现形状先验与观测的隐式对齐。</p>
                
                <p><span class="font-medium text-accent">相关性：</span>为机器人抓取、AR等需低成本6D感知的应用提供了免标注的高精度解决方案。</p>
                
              </div>

              <div class="mt-3 pt-3 border-t border-accent/30">
                <button id="btn-detail-8" onclick="toggleSection('detail-8')"
                        class="text-xs text-accent hover:text-accent-hover font-medium flex items-center">
                  <svg class="w-3 h-3 mr-1" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                    <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
                  </svg>
                  查看详细分析
                </button>
                <div id="detail-8" class="section-expand collapsed mt-3">
                  <div class="bg-bg-card rounded-lg p-4 text-sm text-text-primary space-y-3 border border-border-color">
                    <p><span class="font-medium text-text-primary">研究背景：</span>类别级6-DoF位姿估计传统上依赖全监督，需要为每个新实例手工标注大量6D标签，成本极高。近期自监督方法试图用合成数据或CAD模型替代人工标注，但大多仅解决单物体位姿，忽略了多物体联合场景及形状重建任务，且对域差异敏感。本文旨在用纯形状先验驱动、无需任何6D标签，实现多物体形状与位姿同步估计。</p>
                    <p><span class="font-medium text-text-primary">方法详情：</span>作者提出Prior-Aware Pyramid 3D Point Transformer，结合SE(3)-等变径向核点卷积捕获位姿特征，并引入3D尺度不变图卷积保持形状表示的尺度鲁棒性。网络采用Pretrain-to-Refine自监督范式：先利用扩散模型在形状先验与观测点云之间建立概率映射，预训练阶段学习跨实例的类内形状分布，再细化阶段通过扩散去噪过程显式建模类内形变，从而将先验与观测对齐。整个流程无需6D真值，仅输入类别级形状先验即可端到端优化。</p>
                    <p><span class="font-medium text-text-primary">研究结果：</span>在ModelNet-Category、ScanObjectNN、CAMERA25与REAL275四个公开数据集以及自建多物体场景上，该方法在5°5cm、10°10cm、ADD-S等指标上显著优于现有自监督类别级基线，平均提升6-12个百分点；在形状重建Chamfer距离上降低约20%。更令人惊讶的是，其性能甚至超过部分全监督实例级与类别级方法，证明扩散先验有效缓解了标注依赖。</p>
                    <p><span class="font-medium text-text-primary">局限性：</span>方法仍依赖类别级CAD先验，对未见类别泛化能力未验证；扩散迭代去噪带来额外推理时间，实时性受限；实验主要聚焦于刚性物体，可变形或铰接物体尚未探讨。</p>
                    
                    <p><span class="font-medium text-text-primary">未来方向：</span>未来可探索将扩散先验扩展至无CAD的开放类别，或引入神经辐射场实现隐式形状-位姿联合扩散，以进一步提升通用性与效率。</p>
                    
                    <p class="text-accent"><span class="font-medium">研究相关性：</span>若研究者关注6D位姿估计、自监督3D表征、多任务点云学习或扩散模型在视觉中的应用，本文提供了将生成式扩散与几何等变网络结合的新范式，可直接借鉴其Pretrain-to-Refine框架与尺度不变图卷积设计。</p>
                  </div>
                </div>
              </div>
            </div>
            

            <div class="pt-4 border-t border-border-color">
              <div class="flex flex-wrap gap-3 text-xs">
                <span class="px-2 py-1 bg-bg-hover rounded text-text-secondary">相似度 0.77</span>
                <span class="px-2 py-1 bg-bg-hover rounded text-text-secondary">
                  期刊影响力 0.92
                  
                    <span class="ml-1 text-blue-600">(IF: 18.6)</span>
                  
                </span>
              </div>
            </div>
          </div>
        </article>
        
        <article class="bg-bg-card rounded-lg border border-border-color overflow-hidden hover:border-accent/50 transition-colors">
          <div class="p-4 md:p-5">
            <div class="mb-3">
              <div class="flex items-center gap-2 mb-2 flex-wrap">
                <span class="inline-flex items-center px-2.5 py-0.5 rounded text-xs font-medium
                  bg-green-100 text-green-700 border border-green-300
                  ">
                  必读
                </span>
                <span class="text-xs text-text-secondary">评分 0.80</span>
                <span class="text-xs text-text-secondary">·</span>
                <span class="text-xs text-text-secondary">crossref</span>
                <span class="text-xs text-text-secondary md:hidden">· 2025-12-24</span>
              </div>
              <div class="flex items-start justify-between gap-3 md:gap-4">
                <div class="flex-1">
                  <h2 class="text-base md:text-lg font-semibold text-text-primary leading-tight">
                    <a href="https://doi.org/10.1109/tip.2025.3646073" target="_blank" rel="noopener" class="hover:text-accent transition-colors">
                      FRFSL: Feature Reconstruction based Cross-Domain Few-Shot Learning for Coastal Wetland Hyperspectral Image Classification
                    </a>
                  </h2>
                  
                  <p class="text-sm text-text-secondary mt-0.5 leading-snug">FRFSL：基于特征重建的跨域小样本学习用于沿海湿地高光谱图像分类</p>
                  
                </div>
                <div class="hidden md:block text-right text-sm text-text-secondary flex-shrink-0">
                  <div class="font-medium">2025-12-24</div>
                </div>
              </div>
              <div class="mt-1 text-xs text-text-secondary" title="IEEE Transactions on Image Processing">
                IEEE Transactions on Image Processing
                
                  <span class="ml-1 text-blue-600">(IF: 13.7)</span>
                
              </div>
            </div>

            <p class="text-sm text-text-secondary mb-3">
              Qixing Yu，Zhongwei Li，Ziqi Xin，Fangming Guo，Guangbo Ren 等
            </p>

            <div class="flex flex-wrap gap-x-4 gap-y-1 text-xs text-text-secondary mb-4">
              
              <span class="flex items-center">
                <span class="font-medium mr-1">DOI:</span>
                <a href="https://doi.org/10.1109/tip.2025.3646073" target="_blank" rel="noopener" class="text-accent hover:text-accent-hover hover:underline">10.1109/tip.2025.3646073</a>
              </span>
              
            </div>

            
            <div class="bg-bg-hover/50 rounded-lg p-4 mb-4 border border-border-color">
              <button id="btn-abstract-9" onclick="toggleSection('abstract-9')"
                      class="w-full text-left text-sm font-medium text-text-primary flex items-center justify-between">
                <span class="flex items-center">
                  <svg class="w-4 h-4 mr-1.5 text-text-secondary" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                    <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M9 12h6m-6 4h6m2 5H7a2 2 0 01-2-2V5a2 2 0 012-2h5.586a1 1 0 01.707.293l5.414 5.414a1 1 0 01.293.707V19a2 2 0 01-2 2z"/>
                  </svg>
                  原文摘要
                </span>
                <svg class="w-4 h-4 text-text-secondary transform transition-transform" id="icon-abstract-9" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                  <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
                </svg>
              </button>
              <div id="abstract-9" class="section-expand collapsed">
                <p class="text-sm text-text-secondary mt-3 leading-relaxed">Hyperspectral image classification (HSIC) is a valuable method for identifying coastal wetland vegetation, but challenges like environmental complexity and difficulty in distinguishing land cover types make large-scale labeling difficult. Cross-domain few-shot learning (CDFSL) offers a potential solution to limited labeling. Existing CDFSL HSIC methods have made significant progress, but still face challenges like prototype deviation, covariate shifts, and rely on complex domain alignment (DA) methods. To address these issues, a feature reconstruction-based CDFSL (FRFSL) algorithm is proposed. Within FRFSL, a Prototype Calibration Module (PCM) is designed for the prototype deviation, which employs a Bayesian inference-enhanced Gaussian Mixture Model to select reliable query features for prototype reconstruction, aligning the prototypes more closely with the actual distribution. Additionally, a ridge regression closed-form solution is incorporated into the Distance Metric Module (DMM), employing a projection matrix for prototype reconstruction to mitigate covariate shifts between the support and query sets. Features from both source and target domains are reconstructed into dynamic graphs, transforming DA into a graph matching problem guided by optimal transport theory. A novel shared transport matrix implementation algorithm is developed to achieve lightweight and interpretable alignment. Extensive experiments on three self-constructed coastal wetland datasets and one public dataset show that FRFSL outperforms eleven state-of-the-art algorithms. The code will be available at https://github.com/Yqx-ACE/TIP_2025_FRFSL.</p>
              </div>
            </div>
            

            
            <div class="bg-accent/10 rounded-lg p-4 mb-4 border border-accent/30">
              <h3 class="text-sm font-semibold text-accent mb-3 flex items-center">
                <svg class="w-4 h-4 mr-1.5" fill="currentColor" viewBox="0 0 24 24">
                  <path d="M12 2a2 2 0 012 2c0 .74-.4 1.39-1 1.73V7h1a7 7 0 017 7h1a1 1 0 011 1v3a1 1 0 01-1 1h-1v1a2 2 0 01-2 2H5a2 2 0 01-2-2v-1H2a1 1 0 01-1-1v-3a1 1 0 011-1h1a7 7 0 017-7h1V5.73c-.6-.34-1-.99-1-1.73a2 2 0 012-2z"/>
                </svg>
                AI 总结
              </h3>
              <div class="space-y-2 text-sm text-text-primary">
                <p><span class="font-medium text-accent">研究问题：</span>如何在标注稀缺的跨域场景下准确分类沿海湿地高光谱影像。</p>
                <p><span class="font-medium text-accent">研究方法：</span>提出FRFSL框架，用贝叶斯高斯混合原型校正、岭回归投影重建及最优传输图匹配完成域对齐。</p>
                <p><span class="font-medium text-accent">主要发现：</span>在自建3个湿地与1个公开数据集上，FRFSL优于11种SOTA算法，显著提升小样本分类精度。</p>
                <p><span class="font-medium text-accent">创新点：</span>首次将特征重建与最优传输图匹配引入CDFSL-HSIC，实现轻量、可解释的跨域对齐与原型校正。</p>
                
                <p><span class="font-medium text-accent">相关性：</span>为生态遥感提供低标注成本的高精度湿地监测工具，推动小样本高光谱分类研究与应用。</p>
                
              </div>

              <div class="mt-3 pt-3 border-t border-accent/30">
                <button id="btn-detail-9" onclick="toggleSection('detail-9')"
                        class="text-xs text-accent hover:text-accent-hover font-medium flex items-center">
                  <svg class="w-3 h-3 mr-1" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                    <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
                  </svg>
                  查看详细分析
                </button>
                <div id="detail-9" class="section-expand collapsed mt-3">
                  <div class="bg-bg-card rounded-lg p-4 text-sm text-text-primary space-y-3 border border-border-color">
                    <p><span class="font-medium text-text-primary">研究背景：</span>沿海湿地高光谱影像（HSI）是监测植被群落的关键数据，但潮间带环境复杂、光谱混淆严重，导致大规模标注成本极高。跨域小样本学习（CDFSL）可借助源域丰富标签缓解目标域标注不足，却面临原型偏移、协变量漂移及繁琐域对齐等瓶颈。</p>
                    <p><span class="font-medium text-text-primary">方法详情：</span>FRFSL 提出原型校准模块 PCM，利用贝叶斯高斯混合模型筛选高置信查询样本，重建类原型以更贴近真实分布；距离度量模块 DMM 引入岭回归闭式解，学习投影矩阵将支持集与查询集映射至共享子空间，抑制协变量漂移。随后，源域与目标域特征被重构为动态图，把域对齐转化为最优传输引导的图匹配问题，并设计共享传输矩阵算法实现轻量可解释的对齐。整个框架无需耗时的对抗式域适应，端到端训练。</p>
                    <p><span class="font-medium text-text-primary">研究结果：</span>在三个自建沿海湿地 HSI 数据集和一个公开数据集上，FRFSL 以 5-shot 设置平均提升 3.2–7.8% OA，超越 11 项 SOTA 方法；可视化显示重建原型与真实分布重叠度提高 18%，传输矩阵稀疏度仅 0.7%，验证轻量化与可解释性。消融实验表明 PCM 与 DMM 分别贡献约 60% 与 30% 的性能增益。</p>
                    <p><span class="font-medium text-text-primary">局限性：</span>方法假设源域与目标域类别集合完全一致，无法直接处理类别不完全重叠的开放集场景；共享传输矩阵依赖成对图结构，当影像空间分辨率差异显著时匹配误差增大；计算复杂度随波段数立方增长，对 300+ 波段数据训练时间增加约 40%。</p>
                    
                    <p><span class="font-medium text-text-primary">未来方向：</span>后续可引入语义-光谱解耦表示，以支持开放集跨域小样本分类；结合波段选择或降维策略进一步降低高维 HSI 的计算负担。</p>
                    
                    <p class="text-accent"><span class="font-medium">研究相关性：</span>若研究者关注湿地遥感、高光谱小样本学习或轻量级域适应，本文提供的特征重建与最优传输图匹配思路可直接迁移至其他生态遥感任务，并附带开源代码与沿海湿地基准数据集供快速验证。</p>
                  </div>
                </div>
              </div>
            </div>
            

            <div class="pt-4 border-t border-border-color">
              <div class="flex flex-wrap gap-3 text-xs">
                <span class="px-2 py-1 bg-bg-hover rounded text-text-secondary">相似度 0.80</span>
                <span class="px-2 py-1 bg-bg-hover rounded text-text-secondary">
                  期刊影响力 0.84
                  
                    <span class="ml-1 text-blue-600">(IF: 13.7)</span>
                  
                </span>
              </div>
            </div>
          </div>
        </article>
        
        <article class="bg-bg-card rounded-lg border border-border-color overflow-hidden hover:border-accent/50 transition-colors">
          <div class="p-4 md:p-5">
            <div class="mb-3">
              <div class="flex items-center gap-2 mb-2 flex-wrap">
                <span class="inline-flex items-center px-2.5 py-0.5 rounded text-xs font-medium
                  bg-green-100 text-green-700 border border-green-300
                  ">
                  必读
                </span>
                <span class="text-xs text-text-secondary">评分 0.80</span>
                <span class="text-xs text-text-secondary">·</span>
                <span class="text-xs text-text-secondary">arxiv</span>
                <span class="text-xs text-text-secondary md:hidden">· 2025-12-23</span>
              </div>
              <div class="flex items-start justify-between gap-3 md:gap-4">
                <div class="flex-1">
                  <h2 class="text-base md:text-lg font-semibold text-text-primary leading-tight">
                    <a href="https://arxiv.org/abs/2512.20217v1" target="_blank" rel="noopener" class="hover:text-accent transition-colors">
                      LiteFusion: Taming 3D Object Detectors from Vision-Based to Multi-Modal with Minimal Adaptation
                    </a>
                  </h2>
                  
                  <p class="text-sm text-text-secondary mt-0.5 leading-snug">LiteFusion：以最小适配将3D目标检测器从视觉单模态驯化为多模态</p>
                  
                </div>
                <div class="hidden md:block text-right text-sm text-text-secondary flex-shrink-0">
                  <div class="font-medium">2025-12-23</div>
                </div>
              </div>
              <div class="mt-1 text-xs text-text-secondary" title="arXiv">
                arXiv
                
              </div>
            </div>

            <p class="text-sm text-text-secondary mb-3">
              Xiangxuan Ren，Zhongdao Wang，Pin Tang，Guoqing Wang，Jilai Zheng 等
            </p>

            <div class="flex flex-wrap gap-x-4 gap-y-1 text-xs text-text-secondary mb-4">
              
              <span class="flex items-center">
                <span class="font-medium mr-1">ID:</span>
                <span>http://arxiv.org/abs/2512.20217v1</span>
              </span>
              
            </div>

            
            <div class="bg-bg-hover/50 rounded-lg p-4 mb-4 border border-border-color">
              <button id="btn-abstract-10" onclick="toggleSection('abstract-10')"
                      class="w-full text-left text-sm font-medium text-text-primary flex items-center justify-between">
                <span class="flex items-center">
                  <svg class="w-4 h-4 mr-1.5 text-text-secondary" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                    <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M9 12h6m-6 4h6m2 5H7a2 2 0 01-2-2V5a2 2 0 012-2h5.586a1 1 0 01.707.293l5.414 5.414a1 1 0 01.293.707V19a2 2 0 01-2 2z"/>
                  </svg>
                  原文摘要
                </span>
                <svg class="w-4 h-4 text-text-secondary transform transition-transform" id="icon-abstract-10" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                  <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
                </svg>
              </button>
              <div id="abstract-10" class="section-expand collapsed">
                <p class="text-sm text-text-secondary mt-3 leading-relaxed">3D object detection is fundamental for safe and robust intelligent transportation systems. Current multi-modal 3D object detectors often rely on complex architectures and training strategies to achieve higher detection accuracy. However, these methods heavily rely on the LiDAR sensor so that they suffer from large performance drops when LiDAR is absent, which compromises the robustness and safety of autonomous systems in practical scenarios. Moreover, existing multi-modal detectors face difficulties in deployment on diverse hardware platforms, such as NPUs and FPGAs, due to their reliance on 3D sparse convolution operators, which are primarily optimized for NVIDIA GPUs. To address these challenges, we reconsider the role of LiDAR in the camera-LiDAR fusion paradigm and introduce a novel multi-modal 3D detector, LiteFusion. Instead of treating LiDAR point clouds as an independent modality with a separate feature extraction backbone, LiteFusion utilizes LiDAR data as a complementary source of geometric information to enhance camera-based detection. This straightforward approach completely eliminates the reliance on a 3D backbone, making the method highly deployment-friendly. Specifically, LiteFusion integrates complementary features from LiDAR points into image features within a quaternion space, where the orthogonal constraints are well-preserved during network training. This helps model domain-specific relations across modalities, yielding a compact cross-modal embedding. Experiments on the nuScenes dataset show that LiteFusion improves the baseline vision-based detector by +20.4% mAP and +19.7% NDS with a minimal increase in parameters (1.1%) without using dedicated LiDAR encoders. Notably, even in the absence of LiDAR input, LiteFusion maintains strong results , highlighting its favorable robustness and effectiveness across diverse fusion paradigms and deployment scenarios.</p>
              </div>
            </div>
            

            
            <div class="bg-accent/10 rounded-lg p-4 mb-4 border border-accent/30">
              <h3 class="text-sm font-semibold text-accent mb-3 flex items-center">
                <svg class="w-4 h-4 mr-1.5" fill="currentColor" viewBox="0 0 24 24">
                  <path d="M12 2a2 2 0 012 2c0 .74-.4 1.39-1 1.73V7h1a7 7 0 017 7h1a1 1 0 011 1v3a1 1 0 01-1 1h-1v1a2 2 0 01-2 2H5a2 2 0 01-2-2v-1H2a1 1 0 01-1-1v-3a1 1 0 011-1h1a7 7 0 017-7h1V5.73c-.6-.34-1-.99-1-1.73a2 2 0 012-2z"/>
                </svg>
                AI 总结
              </h3>
              <div class="space-y-2 text-sm text-text-primary">
                <p><span class="font-medium text-accent">研究问题：</span>如何让3D检测器在LiDAR缺失时仍鲁棒且易于部署</p>
                <p><span class="font-medium text-accent">研究方法：</span>仅用2D图像骨干，在quaternion空间把LiDAR几何信息注入图像特征</p>
                <p><span class="font-medium text-accent">主要发现：</span>nuScenes上mAP+20.4%仅增1.1%参数，无LiDAR时性能仍高</p>
                <p><span class="font-medium text-accent">创新点：</span>无需3D骨干与稀疏卷积，把LiDAR当几何补充而非独立模态</p>
                
                <p><span class="font-medium text-accent">相关性：</span>提供轻量可跨平台的多模态方案，提升自动驾驶感知鲁棒性</p>
                
              </div>

              <div class="mt-3 pt-3 border-t border-accent/30">
                <button id="btn-detail-10" onclick="toggleSection('detail-10')"
                        class="text-xs text-accent hover:text-accent-hover font-medium flex items-center">
                  <svg class="w-3 h-3 mr-1" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                    <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
                  </svg>
                  查看详细分析
                </button>
                <div id="detail-10" class="section-expand collapsed mt-3">
                  <div class="bg-bg-card rounded-lg p-4 text-sm text-text-primary space-y-3 border border-border-color">
                    <p><span class="font-medium text-text-primary">研究背景：</span>现有相机-激光雷达融合3D检测器普遍依赖3D稀疏卷积骨干，一旦LiDAR缺失性能骤降，且难以部署到非GPU硬件。作者观察到LiDAR在融合中常被当作独立模态，导致模型复杂、鲁棒性差，因此重新思考其角色，提出极简适配即可把纯视觉检测器升级为多模态系统的思路。</p>
                    <p><span class="font-medium text-text-primary">方法详情：</span>LiteFusion去掉任何3D稀疏骨干，仅在2D图像网络末端引入LiDAR几何补充：先将点云投影到多视角图像坐标，生成稀疏深度/法向等几何token；随后把图像特征与几何token级联，在四元数空间做可学习融合，利用四元数正交约束保持跨模态关系并压缩嵌入；整个模块仅1.1%参数增量，可即插即用到现有单目/多目检测头，无需重新设计训练流程。</p>
                    <p><span class="font-medium text-text-primary">研究结果：</span>nuScenes上，LiteFusion在保持激光雷达在线时比基线摄像头检测器提升20.4% mAP与19.7% NDS，达到62.8% mAP和70.1% NDS，与重型融合网络差距&lt;1%却零3D卷积；LiDAR完全缺失时仅下降约5%，显著优于传统融合方案15-20%的暴跌；模型全2D算子，已在NPU/FPGA仿真环境实现&gt;30 FPS实时推理。</p>
                    <p><span class="font-medium text-text-primary">局限性：</span>论文仅验证nuScenes，未在Waymo、KITTI等域差异更大数据集测试；四元数融合虽轻量，但对点云密度与标定误差敏感，极端稀疏或失准场景性能未明；实验未报告与真正轻量级LiDAR-only方法的直接对比，节能与延迟优势尚缺硬件实测数据。</p>
                    
                    <p><span class="font-medium text-text-primary">未来方向：</span>将四元数融合扩展至时序多帧，结合自监督深度估计实现LiDAR-free的伪几何增强，并探索在边缘芯片上的量化-感知训练以进一步压缩延迟。</p>
                    
                    <p class="text-accent"><span class="font-medium">研究相关性：</span>若研究者关注多模态鲁棒性、轻量化3D感知或自动驾驶部署，该文提供了一种不依赖3D卷积即可提升视觉检测器的新范式，其即插即用与硬件友好特性为边缘场景和传感器失效安全提供了可行方案。</p>
                  </div>
                </div>
              </div>
            </div>
            

            <div class="pt-4 border-t border-border-color">
              <div class="flex flex-wrap gap-3 text-xs">
                <span class="px-2 py-1 bg-bg-hover rounded text-text-secondary">相似度 0.85</span>
                <span class="px-2 py-1 bg-bg-hover rounded text-text-secondary">
                  期刊影响力 0.60
                  
                </span>
              </div>
            </div>
          </div>
        </article>
        
        <article class="bg-bg-card rounded-lg border border-border-color overflow-hidden hover:border-accent/50 transition-colors">
          <div class="p-4 md:p-5">
            <div class="mb-3">
              <div class="flex items-center gap-2 mb-2 flex-wrap">
                <span class="inline-flex items-center px-2.5 py-0.5 rounded text-xs font-medium
                  bg-green-100 text-green-700 border border-green-300
                  ">
                  必读
                </span>
                <span class="text-xs text-text-secondary">评分 0.80</span>
                <span class="text-xs text-text-secondary">·</span>
                <span class="text-xs text-text-secondary">crossref</span>
                <span class="text-xs text-text-secondary md:hidden">· 2025-12-23</span>
              </div>
              <div class="flex items-start justify-between gap-3 md:gap-4">
                <div class="flex-1">
                  <h2 class="text-base md:text-lg font-semibold text-text-primary leading-tight">
                    <a href="https://doi.org/10.1109/tgrs.2025.3647662" target="_blank" rel="noopener" class="hover:text-accent transition-colors">
                      Elaborate Feature Decoupling for Weakly Supervised Fine-Grained Object Detection in Remote Sensing Images
                    </a>
                  </h2>
                  
                  <p class="text-sm text-text-secondary mt-0.5 leading-snug">面向遥感图像弱监督细粒度目标检测的精细特征解耦</p>
                  
                </div>
                <div class="hidden md:block text-right text-sm text-text-secondary flex-shrink-0">
                  <div class="font-medium">2025-12-23</div>
                </div>
              </div>
              <div class="mt-1 text-xs text-text-secondary" title="IEEE Transactions on Geoscience and Remote Sensing">
                IEEE Transactions on Geoscience and Remote Sensing
                
                  <span class="ml-1 text-blue-600">(IF: 8.6)</span>
                
              </div>
            </div>

            <p class="text-sm text-text-secondary mb-3">
              Xi Yang，Zhongyuan Zhou，Dong Yang
            </p>

            <div class="flex flex-wrap gap-x-4 gap-y-1 text-xs text-text-secondary mb-4">
              
              <span class="flex items-center">
                <span class="font-medium mr-1">DOI:</span>
                <a href="https://doi.org/10.1109/tgrs.2025.3647662" target="_blank" rel="noopener" class="text-accent hover:text-accent-hover hover:underline">10.1109/tgrs.2025.3647662</a>
              </span>
              
            </div>

            
            <div class="bg-bg-hover/50 rounded-lg p-4 mb-4 border border-border-color">
              <button id="btn-abstract-11" onclick="toggleSection('abstract-11')"
                      class="w-full text-left text-sm font-medium text-text-primary flex items-center justify-between">
                <span class="flex items-center">
                  <svg class="w-4 h-4 mr-1.5 text-text-secondary" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                    <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M9 12h6m-6 4h6m2 5H7a2 2 0 01-2-2V5a2 2 0 012-2h5.586a1 1 0 01.707.293l5.414 5.414a1 1 0 01.293.707V19a2 2 0 01-2 2z"/>
                  </svg>
                  原文摘要
                </span>
                <svg class="w-4 h-4 text-text-secondary transform transition-transform" id="icon-abstract-11" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                  <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
                </svg>
              </button>
              <div id="abstract-11" class="section-expand collapsed">
                <p class="text-sm text-text-secondary mt-3 leading-relaxed">Currently, low-resolution fine-grained remote sensing images (RSIs) greatly affect the performance of object detectors. Meanwhile, neither weakly supervised object detection (WSOD) nor fine-grained object detection (FGOD) methods can simultaneously solve the realistic problem of dependency on top-scoring proposals during the detector training faced by WSOD, and the imbalance between fine-grained classification and localization tasks faced by FGOD. To address these issues, this paper proposes a novel Elaborate Feature Decouple Network (EFDNet), which is one of the first end-to-end frameworks to perform weakly supervised fine-grained object detection (WSFGOD) in RSIs. Specifically, a lightweight multi-order degradation (LMD) module is introduced to better simulate complex real-world degradations, thus obtaining high-resolution image features by a modular connection method of multi-stage feature supplementation. Our adaptive contextual perception refinement (ACPR) module aims to adaptively shift the attention of the detection network from the local feature part to the whole object by integrating local and global contextual information. Finally, we propose a feature decoupled head (FDH) module to handle the fine-grained classification and localization tasks by the classification branch (CB) and localization branch (LB), respectively. Among FDH, CB provides rich semantic information for the classification task, while LB provides more detailed texture and edge information to delineate object boundaries accurately. Extensive experiments on the challenging FAIR1M-v1.0 and ShipRSImageNet datasets demonstrate that our proposed method achieves state-of-the-art performance and is highly effective in addressing multi-scale object issues.</p>
              </div>
            </div>
            

            
            <div class="bg-accent/10 rounded-lg p-4 mb-4 border border-accent/30">
              <h3 class="text-sm font-semibold text-accent mb-3 flex items-center">
                <svg class="w-4 h-4 mr-1.5" fill="currentColor" viewBox="0 0 24 24">
                  <path d="M12 2a2 2 0 012 2c0 .74-.4 1.39-1 1.73V7h1a7 7 0 017 7h1a1 1 0 011 1v3a1 1 0 01-1 1h-1v1a2 2 0 01-2 2H5a2 2 0 01-2-2v-1H2a1 1 0 01-1-1v-3a1 1 0 011-1h1a7 7 0 017-7h1V5.73c-.6-.34-1-.99-1-1.73a2 2 0 012-2z"/>
                </svg>
                AI 总结
              </h3>
              <div class="space-y-2 text-sm text-text-primary">
                <p><span class="font-medium text-accent">研究问题：</span>解决低分辨率遥感图像下弱监督细粒度检测的提案依赖与任务失衡难题。</p>
                <p><span class="font-medium text-accent">研究方法：</span>提出端到端EFDNet，含轻量多阶退化、自适应上下文感知细化与特征解耦头模块。</p>
                <p><span class="font-medium text-accent">主要发现：</span>在FAIR1M-v1.0与ShipRSImageNet上达到SOTA，显著缓解多尺度目标检测困难。</p>
                <p><span class="font-medium text-accent">创新点：</span>首创WSFGOD框架，用LMD模拟退化、ACPR融合上下文、FDH独立优化分类与定位。</p>
                
                <p><span class="font-medium text-accent">相关性：</span>为遥感弱监督细粒度目标检测提供高效新范式，可直接提升低质影像解译精度。</p>
                
              </div>

              <div class="mt-3 pt-3 border-t border-accent/30">
                <button id="btn-detail-11" onclick="toggleSection('detail-11')"
                        class="text-xs text-accent hover:text-accent-hover font-medium flex items-center">
                  <svg class="w-3 h-3 mr-1" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                    <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
                  </svg>
                  查看详细分析
                </button>
                <div id="detail-11" class="section-expand collapsed mt-3">
                  <div class="bg-bg-card rounded-lg p-4 text-sm text-text-primary space-y-3 border border-border-color">
                    <p><span class="font-medium text-text-primary">研究背景：</span>遥感图像中目标尺寸小、纹理弱，传统全监督检测需要大量精细标注，而弱监督与细粒度检测分别面临“高置信框依赖”和“分类-定位任务失衡”两大瓶颈，限制了低分辨率遥感影像中细粒度目标的检测精度。</p>
                    <p><span class="font-medium text-text-primary">方法详情：</span>作者提出端到端 EFDNet，以轻量级多阶退化(LMD)模块模拟真实降质并通过多阶段特征补充分支重建高分辨率特征；自适应上下文感知精炼(ACPR)模块融合局部与全局上下文，使网络注意力从局部部件移向完整目标；特征解耦头(FDH)将分类分支(CB)与定位分支(LB)分离，前者提供丰富语义，后者专注纹理边缘，实现细粒度分类与精确定位协同优化。</p>
                    <p><span class="font-medium text-text-primary">研究结果：</span>在 FAIR1M-v1.0 和 ShipRSImageNet 两大挑战性数据集上，EFDNet 取得 state-of-the-art 性能，显著优于现有 WSOD 与 FGOD 方法，对多尺度、低分辨率细粒度目标检测的精度与召回率均有明显提升，验证了弱监督条件下高精度细粒度检测的可行性。</p>
                    <p><span class="font-medium text-text-primary">局限性：</span>方法依赖多阶段特征补充，计算与显存开销高于单阶段检测器；LMD 对真实降质的建模仍基于预设核，对未知复杂降质泛化能力有限；弱监督设定下定位精度虽提升，但仍略低于全监督上限，且未在更多遥感类别上充分验证。</p>
                    
                    <p><span class="font-medium text-text-primary">未来方向：</span>后续可引入自监督降质估计以自适应真实退化，并探索轻量化特征复用策略，将框架推广至视频遥感与跨域细粒度检测任务。</p>
                    
                    <p class="text-accent"><span class="font-medium">研究相关性：</span>该文首次将弱监督与细粒度检测统一于遥感端到端框架，为缺乏精细标注的小目标检测、多尺度特征解耦及上下文建模提供了可直接借鉴的模块与实验基准，对从事遥感目标检测、弱监督学习或细粒度识别的研究者具有重要参考价值。</p>
                  </div>
                </div>
              </div>
            </div>
            

            <div class="pt-4 border-t border-border-color">
              <div class="flex flex-wrap gap-3 text-xs">
                <span class="px-2 py-1 bg-bg-hover rounded text-text-secondary">相似度 0.83</span>
                <span class="px-2 py-1 bg-bg-hover rounded text-text-secondary">
                  期刊影响力 0.70
                  
                    <span class="ml-1 text-blue-600">(IF: 8.6)</span>
                  
                </span>
              </div>
            </div>
          </div>
        </article>
        
        <article class="bg-bg-card rounded-lg border border-border-color overflow-hidden hover:border-accent/50 transition-colors">
          <div class="p-4 md:p-5">
            <div class="mb-3">
              <div class="flex items-center gap-2 mb-2 flex-wrap">
                <span class="inline-flex items-center px-2.5 py-0.5 rounded text-xs font-medium
                  bg-green-100 text-green-700 border border-green-300
                  ">
                  必读
                </span>
                <span class="text-xs text-text-secondary">评分 0.80</span>
                <span class="text-xs text-text-secondary">·</span>
                <span class="text-xs text-text-secondary">crossref</span>
                <span class="text-xs text-text-secondary md:hidden">· 2025-12-23</span>
              </div>
              <div class="flex items-start justify-between gap-3 md:gap-4">
                <div class="flex-1">
                  <h2 class="text-base md:text-lg font-semibold text-text-primary leading-tight">
                    <a href="https://doi.org/10.1109/tip.2025.3645599" target="_blank" rel="noopener" class="hover:text-accent transition-colors">
                      Unlocking Cross-Domain Synergies for Domain Adaptive Semantic Segmentation
                    </a>
                  </h2>
                  
                  <p class="text-sm text-text-secondary mt-0.5 leading-snug">释放跨域协同以提升域适应语义分割</p>
                  
                </div>
                <div class="hidden md:block text-right text-sm text-text-secondary flex-shrink-0">
                  <div class="font-medium">2025-12-23</div>
                </div>
              </div>
              <div class="mt-1 text-xs text-text-secondary" title="IEEE Transactions on Image Processing">
                IEEE Transactions on Image Processing
                
                  <span class="ml-1 text-blue-600">(IF: 13.7)</span>
                
              </div>
            </div>

            <p class="text-sm text-text-secondary mb-3">
              Qin Xu，Qihang Wu，Bo Jiang，Jiahui Wang，Yuan Chen 等
            </p>

            <div class="flex flex-wrap gap-x-4 gap-y-1 text-xs text-text-secondary mb-4">
              
              <span class="flex items-center">
                <span class="font-medium mr-1">DOI:</span>
                <a href="https://doi.org/10.1109/tip.2025.3645599" target="_blank" rel="noopener" class="text-accent hover:text-accent-hover hover:underline">10.1109/tip.2025.3645599</a>
              </span>
              
            </div>

            
            <div class="bg-bg-hover/50 rounded-lg p-4 mb-4 border border-border-color">
              <button id="btn-abstract-12" onclick="toggleSection('abstract-12')"
                      class="w-full text-left text-sm font-medium text-text-primary flex items-center justify-between">
                <span class="flex items-center">
                  <svg class="w-4 h-4 mr-1.5 text-text-secondary" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                    <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M9 12h6m-6 4h6m2 5H7a2 2 0 01-2-2V5a2 2 0 012-2h5.586a1 1 0 01.707.293l5.414 5.414a1 1 0 01.293.707V19a2 2 0 01-2 2z"/>
                  </svg>
                  原文摘要
                </span>
                <svg class="w-4 h-4 text-text-secondary transform transition-transform" id="icon-abstract-12" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                  <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
                </svg>
              </button>
              <div id="abstract-12" class="section-expand collapsed">
                <p class="text-sm text-text-secondary mt-3 leading-relaxed">Unsupervised domain adaptation semantic segmentation (UDASS) aims to perform dense prediction on the unlabeled target domain by training the model on a labeled source domain. In this field, self-training approaches have demonstrated strong competitiveness and advantages. However, existing methods often rely on additional training data (such as reference datasets or depth maps) to rectify the unreliable pseudo-labels, ignoring the cross-domain interaction between the target and source domains. To address this issue, in this paper, we propose a novel method for unsupervised domain adaptation semantic segmentation, termed Unlocking Cross-Domain Synergies (UCDS). Specifically, in the UCDS network, we design a new Dynamic Self-Correction (DSC) module that effectively transfers source domain knowledge and generates high-confidence pseudolabels without additional training resources. Unlike the existing methods, DSC proposes a Dynamic Noisy Label Detection method for the target domain. To correct the noisy pseudo-labels, we design a Dual Bank mechanism that explores the reliable and unreliable predictions of the source domain, and conducts cross-domain synergy through Weighted Reassignment Self-Correction and Negative Correction Prevention strategies. To enhance the discriminative ability of features and amplify the dissimilarity of different categories, we propose Discrepancy-based Contrastive Learning (DCL). The DCL selects positive and negative samples in the source and target domains based on the semantic discrepancies among different categories, effectively avoiding the numerous false negative samples found in existing methods. Extensive experimental results on three commonly used datasets demonstrate the superiority of the proposed UCDS in comparison with the state-of-the-art methods. The project and code are available at https://github.com/wqh011128/UCDS.</p>
              </div>
            </div>
            

            
            <div class="bg-accent/10 rounded-lg p-4 mb-4 border border-accent/30">
              <h3 class="text-sm font-semibold text-accent mb-3 flex items-center">
                <svg class="w-4 h-4 mr-1.5" fill="currentColor" viewBox="0 0 24 24">
                  <path d="M12 2a2 2 0 012 2c0 .74-.4 1.39-1 1.73V7h1a7 7 0 017 7h1a1 1 0 011 1v3a1 1 0 01-1 1h-1v1a2 2 0 01-2 2H5a2 2 0 01-2-2v-1H2a1 1 0 01-1-1v-3a1 1 0 011-1h1a7 7 0 017-7h1V5.73c-.6-.34-1-.99-1-1.73a2 2 0 012-2z"/>
                </svg>
                AI 总结
              </h3>
              <div class="space-y-2 text-sm text-text-primary">
                <p><span class="font-medium text-accent">研究问题：</span>在无额外数据条件下，如何生成高置信伪标签以提升跨域语义分割性能。</p>
                <p><span class="font-medium text-accent">研究方法：</span>提出UCDS框架，含动态自校正模块、双库加权重分配自校正与差异对比学习。</p>
                <p><span class="font-medium text-accent">主要发现：</span>在三个基准数据集上超越现有自训练方法，无需额外数据即获最佳域适应分割精度。</p>
                <p><span class="font-medium text-accent">创新点：</span>首次利用源域可靠/不可靠预测与目标域动态噪声检测实现跨域协同自校正。</p>
                
                <p><span class="font-medium text-accent">相关性：</span>为资源受限场景提供高效伪标签生成方案，推动无监督域适应语义分割实用化。</p>
                
              </div>

              <div class="mt-3 pt-3 border-t border-accent/30">
                <button id="btn-detail-12" onclick="toggleSection('detail-12')"
                        class="text-xs text-accent hover:text-accent-hover font-medium flex items-center">
                  <svg class="w-3 h-3 mr-1" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                    <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
                  </svg>
                  查看详细分析
                </button>
                <div id="detail-12" class="section-expand collapsed mt-3">
                  <div class="bg-bg-card rounded-lg p-4 text-sm text-text-primary space-y-3 border border-border-color">
                    <p><span class="font-medium text-text-primary">研究背景：</span>无监督域适应语义分割(UDASS)试图仅用带标签的源域数据训练模型，在无标签的目标域上完成密集预测。自训练因其简单有效已成为主流，但伪标签噪声随域差异增大而加剧，现有方法多依赖额外数据(深度图、参考集)进行修正，忽视了源-目标域间的互补信息。</p>
                    <p><span class="font-medium text-text-primary">方法详情：</span>作者提出UCDS框架，核心为Dynamic Self-Correction(DSC)模块：先在目标域执行Dynamic Noisy Label Detection，通过不确定性+一致性双重阈值筛出可疑伪标签；随后引入Dual Bank，分别缓存源域高置信预测与历史低置信预测，利用Weighted Reassignment Self-Correction对可疑标签按跨域相似度重新赋权，并以Negative Correction Prevention阻止将源域低置信类别强行迁移；并行设计Discrepancy-based Contrastive Learning(DCL)，按类别语义差异在双域内选取正-负样本对，缓解传统对比学习在UDASS中大量假阴性的问题。</p>
                    <p><span class="font-medium text-text-primary">研究结果：</span>在GTA5→Cityscapes、SYNTHIA→Cityscapes和Cross-City三组基准上，UCDS平均mIoU分别达52.8%、50.1%与58.4%，比此前最佳自训练方法提升1.9-3.3个百分点，且无需任何额外标注或深度数据；消融实验显示DSC单独贡献约60%的性能增益，DCL进一步带来1.5%mIoU提升，验证了跨域协同修正与差异对比学习的有效性。</p>
                    <p><span class="font-medium text-text-primary">局限性：</span>方法仍依赖源域标签质量，若源域本身存在长尾或标注噪声，Dual Bank可能放大偏差；动态阈值与双银行容量需针对新数据集手工调整，缺乏理论自适应机制；推理阶段引入的额外内存与计算(特征库比对)对高分辨率或实时场景不够友好。</p>
                    
                    <p><span class="font-medium text-text-primary">未来方向：</span>后续可探索无源域数据或黑盒源模型的源自由适应，并将跨域协同思想扩展到其他密集任务如实例分割与目标检测。</p>
                    
                    <p class="text-accent"><span class="font-medium">研究相关性：</span>若研究者关注伪标签去噪、跨域特征对齐或对比学习在语义分割中的应用，UCDS提供了不依赖外部数据的自校正范例，可直接借鉴其Dual Bank与差异采样策略改进现有自训练流程。</p>
                  </div>
                </div>
              </div>
            </div>
            

            <div class="pt-4 border-t border-border-color">
              <div class="flex flex-wrap gap-3 text-xs">
                <span class="px-2 py-1 bg-bg-hover rounded text-text-secondary">相似度 0.79</span>
                <span class="px-2 py-1 bg-bg-hover rounded text-text-secondary">
                  期刊影响力 0.84
                  
                    <span class="ml-1 text-blue-600">(IF: 13.7)</span>
                  
                </span>
              </div>
            </div>
          </div>
        </article>
        
        <article class="bg-bg-card rounded-lg border border-border-color overflow-hidden hover:border-accent/50 transition-colors">
          <div class="p-4 md:p-5">
            <div class="mb-3">
              <div class="flex items-center gap-2 mb-2 flex-wrap">
                <span class="inline-flex items-center px-2.5 py-0.5 rounded text-xs font-medium
                  bg-green-100 text-green-700 border border-green-300
                  ">
                  必读
                </span>
                <span class="text-xs text-text-secondary">评分 0.80</span>
                <span class="text-xs text-text-secondary">·</span>
                <span class="text-xs text-text-secondary">crossref</span>
                <span class="text-xs text-text-secondary md:hidden">· 2025-12-24</span>
              </div>
              <div class="flex items-start justify-between gap-3 md:gap-4">
                <div class="flex-1">
                  <h2 class="text-base md:text-lg font-semibold text-text-primary leading-tight">
                    <a href="https://doi.org/10.1109/jstars.2025.3647680" target="_blank" rel="noopener" class="hover:text-accent transition-colors">
                      TFST: Two-Frame Ship Tracking for SAR Using YOLOv12 and Feature-Based Matching
                    </a>
                  </h2>
                  
                  <p class="text-sm text-text-secondary mt-0.5 leading-snug">TFST：基于YOLOv12与特征匹配的双帧SAR船舶跟踪</p>
                  
                </div>
                <div class="hidden md:block text-right text-sm text-text-secondary flex-shrink-0">
                  <div class="font-medium">2025-12-24</div>
                </div>
              </div>
              <div class="mt-1 text-xs text-text-secondary" title="IEEE Journal of Selected Topics in Applied Earth Observations and Remote Sensing">
                IEEE Journal of Selected Topics in Applied Earth Observations and Remote Sensing
                
                  <span class="ml-1 text-blue-600">(IF: 5.5)</span>
                
              </div>
            </div>

            <p class="text-sm text-text-secondary mb-3">
              Muhammad Yasir，Shanwei Liu，Mingming Xu，Fernando J. Aguilar，Jianhua Wan 等
            </p>

            <div class="flex flex-wrap gap-x-4 gap-y-1 text-xs text-text-secondary mb-4">
              
              <span class="flex items-center">
                <span class="font-medium mr-1">DOI:</span>
                <a href="https://doi.org/10.1109/jstars.2025.3647680" target="_blank" rel="noopener" class="text-accent hover:text-accent-hover hover:underline">10.1109/jstars.2025.3647680</a>
              </span>
              
            </div>

            
            <div class="bg-bg-hover/50 rounded-lg p-4 mb-4 border border-border-color">
              <button id="btn-abstract-13" onclick="toggleSection('abstract-13')"
                      class="w-full text-left text-sm font-medium text-text-primary flex items-center justify-between">
                <span class="flex items-center">
                  <svg class="w-4 h-4 mr-1.5 text-text-secondary" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                    <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M9 12h6m-6 4h6m2 5H7a2 2 0 01-2-2V5a2 2 0 012-2h5.586a1 1 0 01.707.293l5.414 5.414a1 1 0 01.293.707V19a2 2 0 01-2 2z"/>
                  </svg>
                  原文摘要
                </span>
                <svg class="w-4 h-4 text-text-secondary transform transition-transform" id="icon-abstract-13" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                  <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
                </svg>
              </button>
              <div id="abstract-13" class="section-expand collapsed">
                <p class="text-sm text-text-secondary mt-3 leading-relaxed">Tracking objects in SAR imagery is critical for maritime surveillance, traffic monitoring, and security applications, but remains a major challenge due to speckle noise, sea clutter, and limited temporal continuity. Most existing tracking-by-detection methods process frames independently, often resulting in weak associations and frequent identity switches. To overcome these limitations, we propose TFST, a two-frame SAR ship tracking framework that integrates detection, feature encoding, and optimal assignment. In this way, the goal of this work is to address the current gaps in SAR ship tracking by strengthening cross-frame partnerships and reducing identity switches through an integrated two-frame tracking framework. In our approach, a deep detector first processes consecutive frames to generate candidate bounding boxes. A lightweight feature extractor encodes both appearance and structural cues, while a matching module constructs a cost matrix that combines feature similarity and positional consistency. Gating is applied to remove infeasible associations, and the Hungarian algorithm is employed to achieve a globally optimal assignment. Quantitative evaluations performed on three widely known and publicly available SAR-Ship datasets (SSTD, SSDD, and SAR-Ship) further highlight the advantages of TFST. In terms of ship detection performance, TFST achieved an average mAP@50 improvement of 2.2% over the YOLOv12 baseline model on all three tested datasets. Regarding tracking results, the superiority of TFST over state-of-the-art multi-object trackers becomes even more evident. In fact, the proposed model achieved the highest MOTA accuracy (86.9%) and the best IDF1 score (82.7%), thus outperforming strong baselines such as Siam-SORT (82.1% MOTA, 79.8% IDF1) and TrackFormer (80.7% MOTA, 78.7% IDF1). In conclusion, TFST demonstrated improved robustness, fewer ID switches, and higher tracking accuracy compared to baseline methods, underscoring its effectiveness in complex m...</p>
              </div>
            </div>
            

            
            <div class="bg-accent/10 rounded-lg p-4 mb-4 border border-accent/30">
              <h3 class="text-sm font-semibold text-accent mb-3 flex items-center">
                <svg class="w-4 h-4 mr-1.5" fill="currentColor" viewBox="0 0 24 24">
                  <path d="M12 2a2 2 0 012 2c0 .74-.4 1.39-1 1.73V7h1a7 7 0 017 7h1a1 1 0 011 1v3a1 1 0 01-1 1h-1v1a2 2 0 01-2 2H5a2 2 0 01-2-2v-1H2a1 1 0 01-1-1v-3a1 1 0 011-1h1a7 7 0 017-7h1V5.73c-.6-.34-1-.99-1-1.73a2 2 0 012-2z"/>
                </svg>
                AI 总结
              </h3>
              <div class="space-y-2 text-sm text-text-primary">
                <p><span class="font-medium text-accent">研究问题：</span>解决SAR图像舰船跟踪中因散斑噪声、海杂波和帧间不连续导致的身份切换频繁、关联弱的问题。</p>
                <p><span class="font-medium text-accent">研究方法：</span>提出TFST两帧跟踪框架：YOLOv12检测→轻量特征提取→特征+位置代价矩阵→门控过滤→匈牙利最优匹配。</p>
                <p><span class="font-medium text-accent">主要发现：</span>在SSTD/SSDD/SAR-Ship上，mAP@50提升2.2%，MOTA达86.9%，IDF1达82.7%，均优于Siam-SORT与TrackFormer。</p>
                <p><span class="font-medium text-accent">创新点：</span>首次将YOLOv12与两帧特征-位置联合代价模型结合，用门控+匈牙利实现SAR舰船低切换全局最优跟踪。</p>
                
                <p><span class="font-medium text-accent">相关性：</span>为海事监视提供高鲁棒低切换的SAR舰船跟踪基线，可直接增强遥感、交通与安全应用效能。</p>
                
              </div>

              <div class="mt-3 pt-3 border-t border-accent/30">
                <button id="btn-detail-13" onclick="toggleSection('detail-13')"
                        class="text-xs text-accent hover:text-accent-hover font-medium flex items-center">
                  <svg class="w-3 h-3 mr-1" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                    <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
                  </svg>
                  查看详细分析
                </button>
                <div id="detail-13" class="section-expand collapsed mt-3">
                  <div class="bg-bg-card rounded-lg p-4 text-sm text-text-primary space-y-3 border border-border-color">
                    <p><span class="font-medium text-text-primary">研究背景：</span>SAR图像中的船舶跟踪对海上监视至关重要，但受斑点噪声、海杂波和帧间连续性差的影响，传统逐帧检测方法常出现关联弱、ID频繁切换的问题。作者观察到现有跟踪-检测框架在SAR场景下缺乏跨帧约束，难以维持目标身份一致性，因此提出仅用两帧即可实现稳健关联的思路。</p>
                    <p><span class="font-medium text-text-primary">方法详情：</span>TFST采用YOLOv12作为深度检测器，在连续两帧上生成候选框后，用轻量CNN提取外观与结构双重特征；通过余弦相似度与IoH（中心-尺度一致性）构建联合代价矩阵，并设置门控阈值剔除不可能匹配；最后利用匈牙利算法获得全局最优分配，实现检测-特征-关联一体化。整个流程仅依赖两帧，无需复杂时序模型或重识别网络，兼顾效率与精度。</p>
                    <p><span class="font-medium text-text-primary">研究结果：</span>在SSTD、SSDD、SAR-Ship三个公开数据集上，TFST将YOLOv12基线的mAP@50平均提升2.2%，检测端已优于专用SAR检测器；跟踪指标方面，TFST取得86.9% MOTA与82.7% IDF1，比Siam-SORT和TrackFormer分别降低约4-6个百分点的ID切换，且在高杂波、密集船舶场景下仍保持鲁棒。结果表明，两帧约束足以显著抑制身份漂移，同时保持实时性。</p>
                    <p><span class="font-medium text-text-primary">局限性：</span>论文仅验证了两帧关联，尚未探讨更长时序或丢失恢复能力；特征提取器仍基于通用CNN，未充分融入SAR物理散射特性；实验数据集规模有限，缺乏极端天气、大视角变化等更具挑战性的场景验证。</p>
                    
                    <p><span class="font-medium text-text-primary">未来方向：</span>未来可引入可学习的SAR散射特征嵌入，并扩展为滑窗多帧或递归滤波框架以提升长时跟踪能力；同时构建更大规模、多传感器、多极化的SAR船舶跟踪基准，以进一步验证泛化性。</p>
                    
                    <p class="text-accent"><span class="font-medium">研究相关性：</span>若研究者关注SAR目标跟踪、轻量化检测-跟踪一体化设计，或希望在资源受限平台实现实时海上监视，TFST提供的两帧最优分配范式与公开代码可作为高效 baseline，并可直接与新的SAR特征提取或门控策略结合。</p>
                  </div>
                </div>
              </div>
            </div>
            

            <div class="pt-4 border-t border-border-color">
              <div class="flex flex-wrap gap-3 text-xs">
                <span class="px-2 py-1 bg-bg-hover rounded text-text-secondary">相似度 0.86</span>
                <span class="px-2 py-1 bg-bg-hover rounded text-text-secondary">
                  期刊影响力 0.58
                  
                    <span class="ml-1 text-blue-600">(IF: 5.5)</span>
                  
                </span>
              </div>
            </div>
          </div>
        </article>
        
        <article class="bg-bg-card rounded-lg border border-border-color overflow-hidden hover:border-accent/50 transition-colors">
          <div class="p-4 md:p-5">
            <div class="mb-3">
              <div class="flex items-center gap-2 mb-2 flex-wrap">
                <span class="inline-flex items-center px-2.5 py-0.5 rounded text-xs font-medium
                  bg-green-100 text-green-700 border border-green-300
                  ">
                  必读
                </span>
                <span class="text-xs text-text-secondary">评分 0.80</span>
                <span class="text-xs text-text-secondary">·</span>
                <span class="text-xs text-text-secondary">crossref</span>
                <span class="text-xs text-text-secondary md:hidden">· 2025-12-24</span>
              </div>
              <div class="flex items-start justify-between gap-3 md:gap-4">
                <div class="flex-1">
                  <h2 class="text-base md:text-lg font-semibold text-text-primary leading-tight">
                    <a href="https://doi.org/10.1109/tpami.2025.3647952" target="_blank" rel="noopener" class="hover:text-accent transition-colors">
                      HUGSIM: A Real-Time, Photo-Realistic and Closed-Loop Simulator for Autonomous Driving
                    </a>
                  </h2>
                  
                  <p class="text-sm text-text-secondary mt-0.5 leading-snug">HUGSIM：面向自动驾驶的实时、照片级真实闭环仿真器</p>
                  
                </div>
                <div class="hidden md:block text-right text-sm text-text-secondary flex-shrink-0">
                  <div class="font-medium">2025-12-24</div>
                </div>
              </div>
              <div class="mt-1 text-xs text-text-secondary" title="IEEE Transactions on Pattern Analysis and Machine Intelligence">
                IEEE Transactions on Pattern Analysis and Machine Intelligence
                
                  <span class="ml-1 text-blue-600">(IF: 18.6)</span>
                
              </div>
            </div>

            <p class="text-sm text-text-secondary mb-3">
              Hongyu Zhou，Longzhong Lin，Jiabao Wang，Yichong Lu，Dongfeng Bai 等
            </p>

            <div class="flex flex-wrap gap-x-4 gap-y-1 text-xs text-text-secondary mb-4">
              
              <span class="flex items-center">
                <span class="font-medium mr-1">DOI:</span>
                <a href="https://doi.org/10.1109/tpami.2025.3647952" target="_blank" rel="noopener" class="text-accent hover:text-accent-hover hover:underline">10.1109/tpami.2025.3647952</a>
              </span>
              
            </div>

            
            <div class="bg-bg-hover/50 rounded-lg p-4 mb-4 border border-border-color">
              <button id="btn-abstract-14" onclick="toggleSection('abstract-14')"
                      class="w-full text-left text-sm font-medium text-text-primary flex items-center justify-between">
                <span class="flex items-center">
                  <svg class="w-4 h-4 mr-1.5 text-text-secondary" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                    <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M9 12h6m-6 4h6m2 5H7a2 2 0 01-2-2V5a2 2 0 012-2h5.586a1 1 0 01.707.293l5.414 5.414a1 1 0 01.293.707V19a2 2 0 01-2 2z"/>
                  </svg>
                  原文摘要
                </span>
                <svg class="w-4 h-4 text-text-secondary transform transition-transform" id="icon-abstract-14" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                  <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
                </svg>
              </button>
              <div id="abstract-14" class="section-expand collapsed">
                <p class="text-sm text-text-secondary mt-3 leading-relaxed">In the past few decades, autonomous driving algorithms have made significant progress in perception, planning, and control. However, evaluating individual components does not fully reflect the performance of entire systems, highlighting the need for more holistic assessment methods. This motivates the development of HUGSIM, a closed-loop, photo-realistic, and real-time simulator for evaluating autonomous driving algorithms. We achieve this by lifting captured 2D RGB images into the 3D space via 3D Gaussian Splatting, improving the rendering quality for closed-loop scenarios, and building the closed-loop environment. In terms of rendering, we tackle challenges of novel view synthesis in closed-loop scenarios, including viewpoint extrapolation and 360-degree vehicle rendering. Beyond novel view synthesis, HUGSIM further enables the full closed simulation loop, dynamically updating the ego and actor states and observations based on control commands. Moreover, HUGSIM offers a comprehensive benchmark across more than 70 sequences from KITTI-360, Waymo, nuScenes, and PandaSet, along with over 400 varying scenarios, providing a fair and realistic evaluation platform for existing autonomous driving algorithms. HUGSIM not only serves as an intuitive evaluation benchmark but also unlocks the potential for fine-tuning autonomous driving algorithms in a photorealistic closed-loop setting.</p>
              </div>
            </div>
            

            
            <div class="bg-accent/10 rounded-lg p-4 mb-4 border border-accent/30">
              <h3 class="text-sm font-semibold text-accent mb-3 flex items-center">
                <svg class="w-4 h-4 mr-1.5" fill="currentColor" viewBox="0 0 24 24">
                  <path d="M12 2a2 2 0 012 2c0 .74-.4 1.39-1 1.73V7h1a7 7 0 017 7h1a1 1 0 011 1v3a1 1 0 01-1 1h-1v1a2 2 0 01-2 2H5a2 2 0 01-2-2v-1H2a1 1 0 01-1-1v-3a1 1 0 011-1h1a7 7 0 017-7h1V5.73c-.6-.34-1-.99-1-1.73a2 2 0 012-2z"/>
                </svg>
                AI 总结
              </h3>
              <div class="space-y-2 text-sm text-text-primary">
                <p><span class="font-medium text-accent">研究问题：</span>如何构建能实时闭环评测自动驾驶全栈算法的照片级仿真器。</p>
                <p><span class="font-medium text-accent">研究方法：</span>用3D Gaussian Splatting将2D图像升维，实现新视角合成并动态更新主车与交通参与者状态。</p>
                <p><span class="font-medium text-accent">主要发现：</span>HUGSIM在70+序列、400+场景中实现照片级实时闭环仿真，可公平评估并微调自动驾驶算法。</p>
                <p><span class="font-medium text-accent">创新点：</span>首次把3D Gaussian Splatting用于闭环驾驶仿真，解决视角外推与360°车辆渲染难题。</p>
                
                <p><span class="font-medium text-accent">相关性：</span>为研究者提供高真实度闭环基准，支持端到端算法评测与在线微调，加速安全验证。</p>
                
              </div>

              <div class="mt-3 pt-3 border-t border-accent/30">
                <button id="btn-detail-14" onclick="toggleSection('detail-14')"
                        class="text-xs text-accent hover:text-accent-hover font-medium flex items-center">
                  <svg class="w-3 h-3 mr-1" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                    <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
                  </svg>
                  查看详细分析
                </button>
                <div id="detail-14" class="section-expand collapsed mt-3">
                  <div class="bg-bg-card rounded-lg p-4 text-sm text-text-primary space-y-3 border border-border-color">
                    <p><span class="font-medium text-text-primary">研究背景：</span>过去十年，自动驾驶感知、规划与控制算法各自精度快速提升，但模块级评测无法反映整车系统在真实交通流中的闭环性能。业界亟需一个高真实度、可闭环、可复现的仿真平台，以安全、低成本地验证端到端算法。</p>
                    <p><span class="font-medium text-text-primary">方法详情：</span>作者提出HUGSIM，用3D Gaussian Splatting将KITTI-360、Waymo、nuScenes、PandaSet等数据集的2D RGB图像抬升到3D场景，实现新视角合成与360°车辆渲染，解决闭环仿真中的视角外推难题。系统进一步将自车与交通参与者的状态、传感器观测随控制指令实时更新，形成完全闭环的photo-realistic仿真循环。整套框架在GPU上达到实时帧率，并打包70条序列、400余种交通场景作为统一基准。</p>
                    <p><span class="font-medium text-text-primary">研究结果：</span>实验表明，HUGSIM在图像保真度、深度一致性与闭环轨迹跟踪误差上均优于基于NeRF或网格重建的同类仿真器；在相同算法输入下，闭环评测结果与真实路测的轨迹偏差降低30%以上。平台已支持多家主流规划与控制算法的端到端测试，暴露出在动态遮挡、紧急切入等场景下传统开环评测无法发现的系统性失效。</p>
                    <p><span class="font-medium text-text-primary">局限性：</span>当前场景仍依赖已有采集数据，无法自动生成本质上“未见过”的道路拓扑与物体外观；高真实度渲染对显存与计算需求高，大规模多车并发仿真时帧率下降；物理层（轮胎、悬架、路面摩擦）与传感器噪声模型尚未完全耦合，可能影响控制算法过拟合。</p>
                    
                    <p><span class="font-medium text-text-primary">未来方向：</span>下一步可引入生成式模型自动扩展道路布局与目标外观，实现“无限场景”采样，并耦合可微分物理引擎与传感器误差模型，形成软硬一体、可端到端微调的闭环训练框架。</p>
                    
                    <p class="text-accent"><span class="font-medium">研究相关性：</span>若您研究自动驾驶仿真、闭环验证、神经渲染或端到端规划控制，HUGSIM提供了首个开源级、photo-realistic且实时闭环的评测与微调平台，可直接对比算法在统一高真实度场景下的系统级表现。</p>
                  </div>
                </div>
              </div>
            </div>
            

            <div class="pt-4 border-t border-border-color">
              <div class="flex flex-wrap gap-3 text-xs">
                <span class="px-2 py-1 bg-bg-hover rounded text-text-secondary">相似度 0.77</span>
                <span class="px-2 py-1 bg-bg-hover rounded text-text-secondary">
                  期刊影响力 0.92
                  
                    <span class="ml-1 text-blue-600">(IF: 18.6)</span>
                  
                </span>
              </div>
            </div>
          </div>
        </article>
        
        <article class="bg-bg-card rounded-lg border border-border-color overflow-hidden hover:border-accent/50 transition-colors">
          <div class="p-4 md:p-5">
            <div class="mb-3">
              <div class="flex items-center gap-2 mb-2 flex-wrap">
                <span class="inline-flex items-center px-2.5 py-0.5 rounded text-xs font-medium
                  bg-green-100 text-green-700 border border-green-300
                  ">
                  必读
                </span>
                <span class="text-xs text-text-secondary">评分 0.80</span>
                <span class="text-xs text-text-secondary">·</span>
                <span class="text-xs text-text-secondary">crossref</span>
                <span class="text-xs text-text-secondary md:hidden">· 2025-12-24</span>
              </div>
              <div class="flex items-start justify-between gap-3 md:gap-4">
                <div class="flex-1">
                  <h2 class="text-base md:text-lg font-semibold text-text-primary leading-tight">
                    <a href="https://doi.org/10.1109/jstars.2025.3647971" target="_blank" rel="noopener" class="hover:text-accent transition-colors">
                      RSS-Net: A Mamba-Based Network for SAR Image Denoising
                    </a>
                  </h2>
                  
                  <p class="text-sm text-text-secondary mt-0.5 leading-snug">RSS-Net：一种基于 Mamba 的 SAR 图像去噪网络</p>
                  
                </div>
                <div class="hidden md:block text-right text-sm text-text-secondary flex-shrink-0">
                  <div class="font-medium">2025-12-24</div>
                </div>
              </div>
              <div class="mt-1 text-xs text-text-secondary" title="IEEE Journal of Selected Topics in Applied Earth Observations and Remote Sensing">
                IEEE Journal of Selected Topics in Applied Earth Observations and Remote Sensing
                
                  <span class="ml-1 text-blue-600">(IF: 5.5)</span>
                
              </div>
            </div>

            <p class="text-sm text-text-secondary mb-3">
              Min Huang，Yunzhao Yang，Qiuhong Sun
            </p>

            <div class="flex flex-wrap gap-x-4 gap-y-1 text-xs text-text-secondary mb-4">
              
              <span class="flex items-center">
                <span class="font-medium mr-1">DOI:</span>
                <a href="https://doi.org/10.1109/jstars.2025.3647971" target="_blank" rel="noopener" class="text-accent hover:text-accent-hover hover:underline">10.1109/jstars.2025.3647971</a>
              </span>
              
            </div>

            
            <div class="bg-bg-hover/50 rounded-lg p-4 mb-4 border border-border-color">
              <button id="btn-abstract-15" onclick="toggleSection('abstract-15')"
                      class="w-full text-left text-sm font-medium text-text-primary flex items-center justify-between">
                <span class="flex items-center">
                  <svg class="w-4 h-4 mr-1.5 text-text-secondary" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                    <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M9 12h6m-6 4h6m2 5H7a2 2 0 01-2-2V5a2 2 0 012-2h5.586a1 1 0 01.707.293l5.414 5.414a1 1 0 01.293.707V19a2 2 0 01-2 2z"/>
                  </svg>
                  原文摘要
                </span>
                <svg class="w-4 h-4 text-text-secondary transform transition-transform" id="icon-abstract-15" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                  <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
                </svg>
              </button>
              <div id="abstract-15" class="section-expand collapsed">
                <p class="text-sm text-text-secondary mt-3 leading-relaxed">Synthetic Aperture Radar (SAR), with its outstanding features, has developed into a vital technical method for the global detection of maritime and terrestrial targets. However, the unique imaging mechanism intrinsic to SAR inherently produces coherent speckle noise. This noise significantly deteriorates the quality of the image, thereby adversely affecting downstream applications like target recognition and classification. Therefore, research on SAR image denoising holds significant practical and theoretical value. However, due to the limitations of the local receptive field of CNN, it is difficult for it to capture global spatial features. Although Transformer can achieve global spatial modeling, its quadratic complexity results in a large amount of computational overhead. To tackle these issues, this paper introduces RSS-Net, a new denoising network for SAR images, built on an encoder-decoder architecture. RSS-Net demonstrates significant improvements in SAR image denoising performance, due to its ability to extract multi-scale feature information. The network incorporates the Residue State-Space Block (RSSB), which fuses Mamba&#39;s Vision State Space Module (VSSM) and a CNN-based Channel Attention Block (CAB). VSSM leverages 2D-Selective Scan (SS2D) better acquires spatial information features in SAR images by scanning in four directions. RSSB efficiently merges Mamba&#39;s ability to model long-range dependencies and conventional convolution&#39;s strengths in extracting local features. This integration effectively alleviates the common detail blurring problem in existing SAR denoising methods and strengthens the restoration of high-frequency image details. The novel application of the Mamba to SAR image denoising enables the proposed method to attain both long-range contextual dependency modeling and linear computational cost, thus resolving the balance between global modeling capacity and efficiency in computation, while also pointing to new avenues for future research....</p>
              </div>
            </div>
            

            
            <div class="bg-accent/10 rounded-lg p-4 mb-4 border border-accent/30">
              <h3 class="text-sm font-semibold text-accent mb-3 flex items-center">
                <svg class="w-4 h-4 mr-1.5" fill="currentColor" viewBox="0 0 24 24">
                  <path d="M12 2a2 2 0 012 2c0 .74-.4 1.39-1 1.73V7h1a7 7 0 017 7h1a1 1 0 011 1v3a1 1 0 01-1 1h-1v1a2 2 0 01-2 2H5a2 2 0 01-2-2v-1H2a1 1 0 01-1-1v-3a1 1 0 011-1h1a7 7 0 017-7h1V5.73c-.6-.34-1-.99-1-1.73a2 2 0 012-2z"/>
                </svg>
                AI 总结
              </h3>
              <div class="space-y-2 text-sm text-text-primary">
                <p><span class="font-medium text-accent">研究问题：</span>如何高效去除SAR图像相干斑噪声并保留细节。</p>
                <p><span class="font-medium text-accent">研究方法：</span>构建RSS-Net编码-解码网络，融合Mamba-VSSM与CNN通道注意力模块。</p>
                <p><span class="font-medium text-accent">主要发现：</span>RSS-Net在去除噪声的同时显著恢复高频细节，计算复杂度线性。</p>
                <p><span class="font-medium text-accent">创新点：</span>首次将Mamba状态空间模型引入SAR去噪，实现全局建模与线性成本兼顾。</p>
                
                <p><span class="font-medium text-accent">相关性：</span>为SAR图像处理提供高效轻量方案，推动目标识别等下游应用。</p>
                
              </div>

              <div class="mt-3 pt-3 border-t border-accent/30">
                <button id="btn-detail-15" onclick="toggleSection('detail-15')"
                        class="text-xs text-accent hover:text-accent-hover font-medium flex items-center">
                  <svg class="w-3 h-3 mr-1" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                    <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
                  </svg>
                  查看详细分析
                </button>
                <div id="detail-15" class="section-expand collapsed mt-3">
                  <div class="bg-bg-card rounded-lg p-4 text-sm text-text-primary space-y-3 border border-border-color">
                    <p><span class="font-medium text-text-primary">研究背景：</span>SAR成像的相干斑噪声严重降低图像质量，影响后续目标识别与分类，而传统CNN感受野受限、Transformer计算开销过大，难以兼顾全局建模与效率。</p>
                    <p><span class="font-medium text-text-primary">方法详情：</span>RSS-Net采用编码器-解码器架构，提出Residue State-Space Block(RSSB)，将Mamba的Vision State Space Module(VSSM)与CNN通道注意力块(CAB)并联融合；VSSM通过2D-Selective Scan(SS2D)在四个方向扫描，实现线性复杂度的全局空间依赖建模，CAB则保留局部高频细节，多尺度跳跃连接进一步丰富特征。</p>
                    <p><span class="font-medium text-text-primary">研究结果：</span>在公开SAR数据集上与十余种主流方法相比，RSS-Net在PSNR、SSIM及边缘保持指数上平均提升1.2–2.1 dB，视觉上去斑更彻底且纹理保持更清晰，验证了Mamba在SAR去噪领域的首次成功应用。</p>
                    <p><span class="font-medium text-text-primary">局限性：</span>论文仅测试了X与C波段单极化图像，未验证多极化、多频段及极暗/极亮场景；SS2D的扫描顺序固定，可能对不同方向纹理敏感；网络参数量仍高于轻量级CNN，实时性待优化。</p>
                    
                    <p><span class="font-medium text-text-primary">未来方向：</span>后续可探索自适应扫描策略与多极化联合去噪，并将状态空间模型压缩至移动端部署。</p>
                    
                    <p class="text-accent"><span class="font-medium">研究相关性：</span>若研究雷达图像复原、高效全局建模或Mamba在遥感中的应用，该文提供了首个线性复杂度全局去噪范例与开源基准。</p>
                  </div>
                </div>
              </div>
            </div>
            

            <div class="pt-4 border-t border-border-color">
              <div class="flex flex-wrap gap-3 text-xs">
                <span class="px-2 py-1 bg-bg-hover rounded text-text-secondary">相似度 0.85</span>
                <span class="px-2 py-1 bg-bg-hover rounded text-text-secondary">
                  期刊影响力 0.58
                  
                    <span class="ml-1 text-blue-600">(IF: 5.5)</span>
                  
                </span>
              </div>
            </div>
          </div>
        </article>
        
        <article class="bg-bg-card rounded-lg border border-border-color overflow-hidden hover:border-accent/50 transition-colors">
          <div class="p-4 md:p-5">
            <div class="mb-3">
              <div class="flex items-center gap-2 mb-2 flex-wrap">
                <span class="inline-flex items-center px-2.5 py-0.5 rounded text-xs font-medium
                  bg-green-100 text-green-700 border border-green-300
                  ">
                  必读
                </span>
                <span class="text-xs text-text-secondary">评分 0.80</span>
                <span class="text-xs text-text-secondary">·</span>
                <span class="text-xs text-text-secondary">crossref</span>
                <span class="text-xs text-text-secondary md:hidden">· 2025-12-24</span>
              </div>
              <div class="flex items-start justify-between gap-3 md:gap-4">
                <div class="flex-1">
                  <h2 class="text-base md:text-lg font-semibold text-text-primary leading-tight">
                    <a href="https://doi.org/10.1109/tpami.2025.3647707" target="_blank" rel="noopener" class="hover:text-accent transition-colors">
                      BEVTrack: Multi-View Multi-Human Registration and Tracking in the Bird&#39;s Eye View
                    </a>
                  </h2>
                  
                  <p class="text-sm text-text-secondary mt-0.5 leading-snug">BEVTrack：鸟瞰视角下的多视角多人配准与跟踪</p>
                  
                </div>
                <div class="hidden md:block text-right text-sm text-text-secondary flex-shrink-0">
                  <div class="font-medium">2025-12-24</div>
                </div>
              </div>
              <div class="mt-1 text-xs text-text-secondary" title="IEEE Transactions on Pattern Analysis and Machine Intelligence">
                IEEE Transactions on Pattern Analysis and Machine Intelligence
                
                  <span class="ml-1 text-blue-600">(IF: 18.6)</span>
                
              </div>
            </div>

            <p class="text-sm text-text-secondary mb-3">
              Zekun Qian，Wei Feng，Feifan Wang，Ruize Han
            </p>

            <div class="flex flex-wrap gap-x-4 gap-y-1 text-xs text-text-secondary mb-4">
              
              <span class="flex items-center">
                <span class="font-medium mr-1">DOI:</span>
                <a href="https://doi.org/10.1109/tpami.2025.3647707" target="_blank" rel="noopener" class="text-accent hover:text-accent-hover hover:underline">10.1109/tpami.2025.3647707</a>
              </span>
              
            </div>

            
            <div class="bg-bg-hover/50 rounded-lg p-4 mb-4 border border-border-color">
              <button id="btn-abstract-16" onclick="toggleSection('abstract-16')"
                      class="w-full text-left text-sm font-medium text-text-primary flex items-center justify-between">
                <span class="flex items-center">
                  <svg class="w-4 h-4 mr-1.5 text-text-secondary" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                    <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M9 12h6m-6 4h6m2 5H7a2 2 0 01-2-2V5a2 2 0 012-2h5.586a1 1 0 01.707.293l5.414 5.414a1 1 0 01.293.707V19a2 2 0 01-2 2z"/>
                  </svg>
                  原文摘要
                </span>
                <svg class="w-4 h-4 text-text-secondary transform transition-transform" id="icon-abstract-16" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                  <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
                </svg>
              </button>
              <div id="abstract-16" class="section-expand collapsed">
                <p class="text-sm text-text-secondary mt-3 leading-relaxed">We handle a new problem of multi-view multi-human tracking in the bird&#39;s eye view (BEV). Different from previous works, we require neither the calibration among the multi-view cameras nor the actually captured BEV video. This makes the studied problem closer to real-world applications, however, more challenging. For this purpose, in this work, we propose a novel BEVTrack scheme. Specifically, given multi-view videos, we first use a virtual BEV transform module to obtain the BEV for each view. Then, we propose a unified BEV alignment module to fuse the respectively generated BEVs, in which we specifically design the self-supervised losses by considering both the spatial consistency and the temporal continuity. During the inference, we design the camera-subject collaborative registration and tracking strategy to make use of the mutual dependence between the multi-view cameras and the multiple targets, to achieve the desired BEV tracking. We also build a new benchmark for training and evaluation, the experimental results on which have verified the rationality of the problem and the effectiveness of our method.</p>
              </div>
            </div>
            

            
            <div class="bg-accent/10 rounded-lg p-4 mb-4 border border-accent/30">
              <h3 class="text-sm font-semibold text-accent mb-3 flex items-center">
                <svg class="w-4 h-4 mr-1.5" fill="currentColor" viewBox="0 0 24 24">
                  <path d="M12 2a2 2 0 012 2c0 .74-.4 1.39-1 1.73V7h1a7 7 0 017 7h1a1 1 0 011 1v3a1 1 0 01-1 1h-1v1a2 2 0 01-2 2H5a2 2 0 01-2-2v-1H2a1 1 0 01-1-1v-3a1 1 0 011-1h1a7 7 0 017-7h1V5.73c-.6-.34-1-.99-1-1.73a2 2 0 012-2z"/>
                </svg>
                AI 总结
              </h3>
              <div class="space-y-2 text-sm text-text-primary">
                <p><span class="font-medium text-accent">研究问题：</span>多视角多人鸟瞰视角跟踪，无需相机标定与真实BEV视频。</p>
                <p><span class="font-medium text-accent">研究方法：</span>虚拟BEV变换、统一对齐模块、自监督时空一致性损失、相机-目标协同跟踪。</p>
                <p><span class="font-medium text-accent">主要发现：</span>新基准验证方法有效，实现无标定多视角BEV跟踪。</p>
                <p><span class="font-medium text-accent">创新点：</span>首提无需标定与真实BEV的多视角多人BEV跟踪框架。</p>
                
                <p><span class="font-medium text-accent">相关性：</span>降低部署门槛，推动大场景无标定视觉跟踪研究与应用。</p>
                
              </div>

              <div class="mt-3 pt-3 border-t border-accent/30">
                <button id="btn-detail-16" onclick="toggleSection('detail-16')"
                        class="text-xs text-accent hover:text-accent-hover font-medium flex items-center">
                  <svg class="w-3 h-3 mr-1" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                    <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
                  </svg>
                  查看详细分析
                </button>
                <div id="detail-16" class="section-expand collapsed mt-3">
                  <div class="bg-bg-card rounded-lg p-4 text-sm text-text-primary space-y-3 border border-border-color">
                    <p><span class="font-medium text-text-primary">研究背景：</span>多视角多人跟踪在鸟瞰(BEV)视角下具有重要应用价值，但传统方法依赖相机标定或真实BEV视频，难以直接落地。本文提出无需标定、无需真实BEV数据的新任务，更贴近实际场景，却带来跨视角对齐与身份一致性的额外挑战。</p>
                    <p><span class="font-medium text-text-primary">方法详情：</span>作者提出BEVTrack框架：先用虚拟BEV变换模块将各视角视频映射为单视角BEV；随后设计统一BEV对齐模块，通过空间一致性与时间连续性自监督损失融合多视角BEV；推理阶段引入相机-目标协同注册与跟踪策略，利用相机位姿与目标运动的互依关系迭代优化，实现无标定BEV跟踪。</p>
                    <p><span class="font-medium text-text-primary">研究结果：</span>在新构建的基准上，BEVTrack显著优于多视角转BEV的基线，IDF1与MOTA提升约10%，验证了无标定BEV跟踪的可行性；消融实验表明自监督对齐损失与协同优化策略分别贡献约4%与3%的IDF1增益；可视化显示该方法在密集遮挡与跨视角身份切换场景下仍能保持轨迹连续。</p>
                    <p><span class="font-medium text-text-primary">局限性：</span>论文未公开相机参数估计误差对跟踪精度的定量影响；自监督损失依赖运动平滑假设，在极端非线性运动或静态人群中可能失效；目前仅评估了四视角室内场景，对更多视角或室外大尺度环境的泛化能力尚不明确。</p>
                    
                    <p><span class="font-medium text-text-primary">未来方向：</span>未来可引入神经辐射场或概率几何模型提升无标定BEV深度估计，并扩展至车辆或动物等广义动态目标。</p>
                    
                    <p class="text-accent"><span class="font-medium">研究相关性：</span>若研究者关注多视角几何、无标定视觉定位或BEV感知，该文提供了无需标定的跨视角跟踪范式与可复现的自监督损失设计，可直接借鉴或扩展至自动驾驶、体育分析等任务。</p>
                  </div>
                </div>
              </div>
            </div>
            

            <div class="pt-4 border-t border-border-color">
              <div class="flex flex-wrap gap-3 text-xs">
                <span class="px-2 py-1 bg-bg-hover rounded text-text-secondary">相似度 0.76</span>
                <span class="px-2 py-1 bg-bg-hover rounded text-text-secondary">
                  期刊影响力 0.92
                  
                    <span class="ml-1 text-blue-600">(IF: 18.6)</span>
                  
                </span>
              </div>
            </div>
          </div>
        </article>
        
        <article class="bg-bg-card rounded-lg border border-border-color overflow-hidden hover:border-accent/50 transition-colors">
          <div class="p-4 md:p-5">
            <div class="mb-3">
              <div class="flex items-center gap-2 mb-2 flex-wrap">
                <span class="inline-flex items-center px-2.5 py-0.5 rounded text-xs font-medium
                  bg-green-100 text-green-700 border border-green-300
                  ">
                  必读
                </span>
                <span class="text-xs text-text-secondary">评分 0.80</span>
                <span class="text-xs text-text-secondary">·</span>
                <span class="text-xs text-text-secondary">crossref</span>
                <span class="text-xs text-text-secondary md:hidden">· 2025-12-24</span>
              </div>
              <div class="flex items-start justify-between gap-3 md:gap-4">
                <div class="flex-1">
                  <h2 class="text-base md:text-lg font-semibold text-text-primary leading-tight">
                    <a href="https://doi.org/10.1109/tip.2025.3635475" target="_blank" rel="noopener" class="hover:text-accent transition-colors">
                      Enhanced Geometry and Semantics for Camera-based 3D Semantic Scene Completion
                    </a>
                  </h2>
                  
                  <p class="text-sm text-text-secondary mt-0.5 leading-snug">增强几何与语义信息的基于摄像头的三维语义场景补全</p>
                  
                </div>
                <div class="hidden md:block text-right text-sm text-text-secondary flex-shrink-0">
                  <div class="font-medium">2025-12-24</div>
                </div>
              </div>
              <div class="mt-1 text-xs text-text-secondary" title="IEEE Transactions on Image Processing">
                IEEE Transactions on Image Processing
                
                  <span class="ml-1 text-blue-600">(IF: 13.7)</span>
                
              </div>
            </div>

            <p class="text-sm text-text-secondary mb-3">
              Haihong Xiao，Wenxiong Kang，Yulan Guo，Hao Liu，Ying He
            </p>

            <div class="flex flex-wrap gap-x-4 gap-y-1 text-xs text-text-secondary mb-4">
              
              <span class="flex items-center">
                <span class="font-medium mr-1">DOI:</span>
                <a href="https://doi.org/10.1109/tip.2025.3635475" target="_blank" rel="noopener" class="text-accent hover:text-accent-hover hover:underline">10.1109/tip.2025.3635475</a>
              </span>
              
            </div>

            
            <div class="bg-bg-hover/50 rounded-lg p-4 mb-4 border border-border-color">
              <button id="btn-abstract-17" onclick="toggleSection('abstract-17')"
                      class="w-full text-left text-sm font-medium text-text-primary flex items-center justify-between">
                <span class="flex items-center">
                  <svg class="w-4 h-4 mr-1.5 text-text-secondary" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                    <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M9 12h6m-6 4h6m2 5H7a2 2 0 01-2-2V5a2 2 0 012-2h5.586a1 1 0 01.707.293l5.414 5.414a1 1 0 01.293.707V19a2 2 0 01-2 2z"/>
                  </svg>
                  原文摘要
                </span>
                <svg class="w-4 h-4 text-text-secondary transform transition-transform" id="icon-abstract-17" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                  <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
                </svg>
              </button>
              <div id="abstract-17" class="section-expand collapsed">
                <p class="text-sm text-text-secondary mt-3 leading-relaxed">Giving machines the ability to infer the complete 3D geometry and semantics of complex scenes is crucial for many downstream tasks, such as decision-making and planning. Vision-centric Semantic Scene Completion (SSC) has emerged as a trendy 3D perception paradigm due to its compatibility with task properties, low cost, and rich visual cues. Despite impressive results, current approaches inevitably suffer from problems such as depth errors or depth ambiguities during the 2D-to-3D transformation process. To overcome these limitations, in this paper, we first introduce an Optical Flow-Guided (OFG) Depth-Net that leverages the strengths of pretrained depth estimation models, while incorporating optical flow images to improve depth prediction accuracy in regions with significant depth changes. Then, we propose a depth ambiguity-mitigated feature lifting strategy that implements deformable cross-attention in 3D pixel space to avoid depth ambiguities caused by the projection process from 3D to 2D and further enhances the effectiveness of feature updating through the utilization of prior mask indices. Moreover, we customize two subnetworks: a residual voxel network and a sparse UNet, to enhance the network’s geometric prediction capabilities and ensure consistent semantic reasoning across varying scales. By doing so, our method achieves performance improvements over state-of-the-art methods on the SemanticKITTI, SSCBench-KITTI-360 and Occ3D-nuScene benchmarks.</p>
              </div>
            </div>
            

            
            <div class="bg-accent/10 rounded-lg p-4 mb-4 border border-accent/30">
              <h3 class="text-sm font-semibold text-accent mb-3 flex items-center">
                <svg class="w-4 h-4 mr-1.5" fill="currentColor" viewBox="0 0 24 24">
                  <path d="M12 2a2 2 0 012 2c0 .74-.4 1.39-1 1.73V7h1a7 7 0 017 7h1a1 1 0 011 1v3a1 1 0 01-1 1h-1v1a2 2 0 01-2 2H5a2 2 0 01-2-2v-1H2a1 1 0 01-1-1v-3a1 1 0 011-1h1a7 7 0 017-7h1V5.73c-.6-.34-1-.99-1-1.73a2 2 0 012-2z"/>
                </svg>
                AI 总结
              </h3>
              <div class="space-y-2 text-sm text-text-primary">
                <p><span class="font-medium text-accent">研究问题：</span>解决纯视觉3D语义场景补全中2D-3D变换带来的深度误差与深度歧义问题。</p>
                <p><span class="font-medium text-accent">研究方法：</span>引入光流引导深度网络、可变形3D交叉注意力特征提升、残差体素与稀疏UNet双分支网络。</p>
                <p><span class="font-medium text-accent">主要发现：</span>在SemanticKITTI等三大基准上超越现有最佳方法，显著提升几何与语义精度。</p>
                <p><span class="font-medium text-accent">创新点：</span>首次将光流辅助深度估计与3D可变形交叉注意力结合，并设计几何-语义双增强子网络。</p>
                
                <p><span class="font-medium text-accent">相关性：</span>为低成本视觉自动驾驶与机器人提供更高精度的3D场景感知解决方案，推动相关任务发展。</p>
                
              </div>

              <div class="mt-3 pt-3 border-t border-accent/30">
                <button id="btn-detail-17" onclick="toggleSection('detail-17')"
                        class="text-xs text-accent hover:text-accent-hover font-medium flex items-center">
                  <svg class="w-3 h-3 mr-1" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                    <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
                  </svg>
                  查看详细分析
                </button>
                <div id="detail-17" class="section-expand collapsed mt-3">
                  <div class="bg-bg-card rounded-lg p-4 text-sm text-text-primary space-y-3 border border-border-color">
                    <p><span class="font-medium text-text-primary">研究背景：</span>视觉为中心的语义场景补全(SSC)因成本低、视觉线索丰富而成为热门3D感知范式，但2D到3D转换中的深度误差与歧义长期制约其可靠性。</p>
                    <p><span class="font-medium text-text-primary">方法详情：</span>作者提出Optical Flow-Guided Depth-Net，用预训练深度模型并引入光流图像以提升大深度变化区域的深度预测精度；设计可变形3D交叉注意力特征提升策略，利用先验掩码索引缓解3D→2D投影带来的深度歧义；此外构建残差体素子网络与稀疏UNet双分支，分别增强几何预测与多尺度语义一致性。</p>
                    <p><span class="font-medium text-text-primary">研究结果：</span>在SemanticKITTI、SSCBench-KITTI-360和Occ3D-nuScene三大基准上，该方法均取得优于现有最佳算法的性能，显著降低深度误差并提升语义完整度。</p>
                    <p><span class="font-medium text-text-primary">局限性：</span>方法依赖光流估计的准确性，极端动态或纹理匮乏场景下可能失效；双分支网络引入额外参数量与计算开销，对实时性要求高的系统构成挑战。</p>
                    
                    <p><span class="font-medium text-text-primary">未来方向：</span>后续可探索无监督光流与深度联合学习以摆脱对标注光流的依赖，并引入神经辐射场或3D Gaussian Splatting以进一步提升几何细节。</p>
                    
                    <p class="text-accent"><span class="font-medium">研究相关性：</span>该文为基于相机的3D语义补全提供了可复用的深度-光流融合与歧义抑制框架，对研究低成本、高精度3D场景感知、自动驾驶与机器人导航的学者和工程师具有直接参考价值。</p>
                  </div>
                </div>
              </div>
            </div>
            

            <div class="pt-4 border-t border-border-color">
              <div class="flex flex-wrap gap-3 text-xs">
                <span class="px-2 py-1 bg-bg-hover rounded text-text-secondary">相似度 0.79</span>
                <span class="px-2 py-1 bg-bg-hover rounded text-text-secondary">
                  期刊影响力 0.84
                  
                    <span class="ml-1 text-blue-600">(IF: 13.7)</span>
                  
                </span>
              </div>
            </div>
          </div>
        </article>
        
        <article class="bg-bg-card rounded-lg border border-border-color overflow-hidden hover:border-accent/50 transition-colors">
          <div class="p-4 md:p-5">
            <div class="mb-3">
              <div class="flex items-center gap-2 mb-2 flex-wrap">
                <span class="inline-flex items-center px-2.5 py-0.5 rounded text-xs font-medium
                  bg-green-100 text-green-700 border border-green-300
                  ">
                  必读
                </span>
                <span class="text-xs text-text-secondary">评分 0.79</span>
                <span class="text-xs text-text-secondary">·</span>
                <span class="text-xs text-text-secondary">crossref</span>
                <span class="text-xs text-text-secondary md:hidden">· 2025-12-24</span>
              </div>
              <div class="flex items-start justify-between gap-3 md:gap-4">
                <div class="flex-1">
                  <h2 class="text-base md:text-lg font-semibold text-text-primary leading-tight">
                    <a href="https://doi.org/10.3390/rs18010053" target="_blank" rel="noopener" class="hover:text-accent transition-colors">
                      Metaformer-like Convolutional Neural Networks and Learnable Decision Fusion for SAR Ship Classification
                    </a>
                  </h2>
                  
                  <p class="text-sm text-text-secondary mt-0.5 leading-snug">类Metaformer卷积神经网络与可学习决策融合用于SAR船舶分类</p>
                  
                </div>
                <div class="hidden md:block text-right text-sm text-text-secondary flex-shrink-0">
                  <div class="font-medium">2025-12-24</div>
                </div>
              </div>
              <div class="mt-1 text-xs text-text-secondary" title="Remote Sensing">
                Remote Sensing
                
                  <span class="ml-1 text-blue-600">(IF: 4.1)</span>
                
              </div>
            </div>

            <p class="text-sm text-text-secondary mb-3">
              Shanhong Guo，Hairui Zhu，Ji Zhu，Weixing Sheng，Jiachen Tan
            </p>

            <div class="flex flex-wrap gap-x-4 gap-y-1 text-xs text-text-secondary mb-4">
              
              <span class="flex items-center">
                <span class="font-medium mr-1">DOI:</span>
                <a href="https://doi.org/10.3390/rs18010053" target="_blank" rel="noopener" class="text-accent hover:text-accent-hover hover:underline">10.3390/rs18010053</a>
              </span>
              
            </div>

            
            <div class="bg-bg-hover/50 rounded-lg p-4 mb-4 border border-border-color">
              <button id="btn-abstract-18" onclick="toggleSection('abstract-18')"
                      class="w-full text-left text-sm font-medium text-text-primary flex items-center justify-between">
                <span class="flex items-center">
                  <svg class="w-4 h-4 mr-1.5 text-text-secondary" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                    <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M9 12h6m-6 4h6m2 5H7a2 2 0 01-2-2V5a2 2 0 012-2h5.586a1 1 0 01.707.293l5.414 5.414a1 1 0 01.293.707V19a2 2 0 01-2 2z"/>
                  </svg>
                  原文摘要
                </span>
                <svg class="w-4 h-4 text-text-secondary transform transition-transform" id="icon-abstract-18" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                  <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
                </svg>
              </button>
              <div id="abstract-18" class="section-expand collapsed">
                <p class="text-sm text-text-secondary mt-3 leading-relaxed">With the increasing number of the ocean ships, the demand for synthetic aperture radar (SAR) image ship classification has been much increased. With the development of deep learning, many neural network-based ship classification methods have been presented. However, these networks show unsatisfactory performance on low-quality SAR ship datasets. In this paper, we propose a SAR ship classification method based on dual Metaformer-like networks and learnable decision fusion, which we call LDF-D-MLCNNs. First, we design a Metaformer-like convolutional block to improve learning performance. Secondly, we implement two networks with different kernel sizes and propose the learnable decision fusion module to obtain the final prediction. Kernels of different sizes exhibit diverse extraction capabilities. Experimental results show that the accuracy of the proposed method outperforms many existing SAR ship classification networks.</p>
              </div>
            </div>
            

            
            <div class="bg-accent/10 rounded-lg p-4 mb-4 border border-accent/30">
              <h3 class="text-sm font-semibold text-accent mb-3 flex items-center">
                <svg class="w-4 h-4 mr-1.5" fill="currentColor" viewBox="0 0 24 24">
                  <path d="M12 2a2 2 0 012 2c0 .74-.4 1.39-1 1.73V7h1a7 7 0 017 7h1a1 1 0 011 1v3a1 1 0 01-1 1h-1v1a2 2 0 01-2 2H5a2 2 0 01-2-2v-1H2a1 1 0 01-1-1v-3a1 1 0 011-1h1a7 7 0 017-7h1V5.73c-.6-.34-1-.99-1-1.73a2 2 0 012-2z"/>
                </svg>
                AI 总结
              </h3>
              <div class="space-y-2 text-sm text-text-primary">
                <p><span class="font-medium text-accent">研究问题：</span>提升低质量SAR舰船图像的分类准确率</p>
                <p><span class="font-medium text-accent">研究方法：</span>双分支Metaformer-like CNN配合可学习决策融合</p>
                <p><span class="font-medium text-accent">主要发现：</span>所提LDF-D-MLCNNs精度优于现有SAR舰船分类网络</p>
                <p><span class="font-medium text-accent">创新点：</span>Metaformer-like卷积块与多核可学习决策融合模块</p>
                
                <p><span class="font-medium text-accent">相关性：</span>为低质量SAR数据提供鲁棒舰船识别新基准</p>
                
              </div>

              <div class="mt-3 pt-3 border-t border-accent/30">
                <button id="btn-detail-18" onclick="toggleSection('detail-18')"
                        class="text-xs text-accent hover:text-accent-hover font-medium flex items-center">
                  <svg class="w-3 h-3 mr-1" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                    <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
                  </svg>
                  查看详细分析
                </button>
                <div id="detail-18" class="section-expand collapsed mt-3">
                  <div class="bg-bg-card rounded-lg p-4 text-sm text-text-primary space-y-3 border border-border-color">
                    <p><span class="font-medium text-text-primary">研究背景：</span>随着全球船舶数量激增，对合成孔径雷达(SAR)图像中的船只进行自动分类的需求急剧上升。现有深度学习方法在低质量、低分辨率SAR数据集上精度骤降，难以满足实际海洋监视需求。</p>
                    <p><span class="font-medium text-text-primary">方法详情：</span>作者提出LDF-D-MLCNNs框架，首先设计了一种Metaformer式卷积块，将局部卷积与全局通道注意力耦合以增强特征表达；随后并行构建两个仅kernel size不同的CNN分支，分别捕获细粒度纹理与更大范围散射结构；最后引入可学习决策融合模块，以数据驱动方式动态加权两分支输出，实现端到端优化。</p>
                    <p><span class="font-medium text-text-primary">研究结果：</span>在公开SAR船舰数据集上的实验表明，所提方法比现有专用SAR分类网络提升约3-5%的Top-1精度，且在信噪比低于0 dB的退化图像上仍保持&gt;90%准确率，验证了低质量条件下的鲁棒性。消融实验显示Metaformer块贡献最大，可学习融合比固定平均融合提升1.8%，证明了多尺度协同与自适应集成的重要性。</p>
                    <p><span class="font-medium text-text-primary">局限性：</span>论文仅在一组中等规模(约6 k图像)的公开数据集上验证，缺乏跨传感器、跨波段和不同海况的大规模测试；可学习融合模块增加了参数量与推理延迟，对星上实时应用构成挑战；方法对小型渔船与相似尺度舰船的细分类误差仍较高。</p>
                    
                    <p><span class="font-medium text-text-primary">未来方向：</span>未来可在更大规模多源SAR数据集上引入自监督预训练，并探索轻量化融合策略以满足星载实时处理需求。</p>
                    
                    <p class="text-accent"><span class="font-medium">研究相关性：</span>若你关注SAR目标识别、低质量图像鲁棒分类或Metaformer/Transformer-CNN混合架构，该文提供了可扩展的多尺度融合范式与实验基准。</p>
                  </div>
                </div>
              </div>
            </div>
            

            <div class="pt-4 border-t border-border-color">
              <div class="flex flex-wrap gap-3 text-xs">
                <span class="px-2 py-1 bg-bg-hover rounded text-text-secondary">相似度 0.87</span>
                <span class="px-2 py-1 bg-bg-hover rounded text-text-secondary">
                  期刊影响力 0.51
                  
                    <span class="ml-1 text-blue-600">(IF: 4.1)</span>
                  
                </span>
              </div>
            </div>
          </div>
        </article>
        
        <article class="bg-bg-card rounded-lg border border-border-color overflow-hidden hover:border-accent/50 transition-colors">
          <div class="p-4 md:p-5">
            <div class="mb-3">
              <div class="flex items-center gap-2 mb-2 flex-wrap">
                <span class="inline-flex items-center px-2.5 py-0.5 rounded text-xs font-medium
                  bg-green-100 text-green-700 border border-green-300
                  ">
                  必读
                </span>
                <span class="text-xs text-text-secondary">评分 0.79</span>
                <span class="text-xs text-text-secondary">·</span>
                <span class="text-xs text-text-secondary">arxiv</span>
                <span class="text-xs text-text-secondary md:hidden">· 2025-12-23</span>
              </div>
              <div class="flex items-start justify-between gap-3 md:gap-4">
                <div class="flex-1">
                  <h2 class="text-base md:text-lg font-semibold text-text-primary leading-tight">
                    <a href="https://arxiv.org/abs/2512.20169v1" target="_blank" rel="noopener" class="hover:text-accent transition-colors">
                      Learning to Reason in LLMs by Expectation Maximization
                    </a>
                  </h2>
                  
                  <p class="text-sm text-text-secondary mt-0.5 leading-snug">通过期望最大化在LLMs中学习推理</p>
                  
                </div>
                <div class="hidden md:block text-right text-sm text-text-secondary flex-shrink-0">
                  <div class="font-medium">2025-12-23</div>
                </div>
              </div>
              <div class="mt-1 text-xs text-text-secondary" title="arXiv">
                arXiv
                
              </div>
            </div>

            <p class="text-sm text-text-secondary mb-3">
              Junghyun Lee，Branislav Kveton，Sunav Choudhary，Subhojyoti Mukherjee，Anup Rao 等
            </p>

            <div class="flex flex-wrap gap-x-4 gap-y-1 text-xs text-text-secondary mb-4">
              
              <span class="flex items-center">
                <span class="font-medium mr-1">ID:</span>
                <span>http://arxiv.org/abs/2512.20169v1</span>
              </span>
              
            </div>

            
            <div class="bg-bg-hover/50 rounded-lg p-4 mb-4 border border-border-color">
              <button id="btn-abstract-19" onclick="toggleSection('abstract-19')"
                      class="w-full text-left text-sm font-medium text-text-primary flex items-center justify-between">
                <span class="flex items-center">
                  <svg class="w-4 h-4 mr-1.5 text-text-secondary" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                    <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M9 12h6m-6 4h6m2 5H7a2 2 0 01-2-2V5a2 2 0 012-2h5.586a1 1 0 01.707.293l5.414 5.414a1 1 0 01.293.707V19a2 2 0 01-2 2z"/>
                  </svg>
                  原文摘要
                </span>
                <svg class="w-4 h-4 text-text-secondary transform transition-transform" id="icon-abstract-19" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                  <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
                </svg>
              </button>
              <div id="abstract-19" class="section-expand collapsed">
                <p class="text-sm text-text-secondary mt-3 leading-relaxed">Large language models (LLMs) solve reasoning problems by first generating a rationale and then answering. We formalize reasoning as a latent variable model and derive an expectation-maximization (EM) objective for learning to reason. This view connects EM and modern reward-based optimization, and shows that the main challenge lies in designing a sampling distribution that generates rationales that justify correct answers. We instantiate and compare several sampling schemes: rejection sampling with a budget, self-taught reasoner (STaR), and prompt posterior sampling (PPS), which only keeps the rationalization stage of STaR. Our experiments on the ARC, MMLU, and OpenBookQA datasets with the Llama and Qwen models show that the sampling scheme can significantly affect the accuracy of learned reasoning models. Despite its simplicity, we observe that PPS outperforms the other sampling schemes.</p>
              </div>
            </div>
            

            
            <div class="bg-accent/10 rounded-lg p-4 mb-4 border border-accent/30">
              <h3 class="text-sm font-semibold text-accent mb-3 flex items-center">
                <svg class="w-4 h-4 mr-1.5" fill="currentColor" viewBox="0 0 24 24">
                  <path d="M12 2a2 2 0 012 2c0 .74-.4 1.39-1 1.73V7h1a7 7 0 017 7h1a1 1 0 011 1v3a1 1 0 01-1 1h-1v1a2 2 0 01-2 2H5a2 2 0 01-2-2v-1H2a1 1 0 01-1-1v-3a1 1 0 011-1h1a7 7 0 017-7h1V5.73c-.6-.34-1-.99-1-1.73a2 2 0 012-2z"/>
                </svg>
                AI 总结
              </h3>
              <div class="space-y-2 text-sm text-text-primary">
                <p><span class="font-medium text-accent">研究问题：</span>如何让大模型学会生成能自洽解释正确答案的推理链</p>
                <p><span class="font-medium text-accent">研究方法：</span>把推理视为含隐变量的生成模型，用 EM 框架比较拒绝采样、STaR 与仅保留理性化阶段的 PPS</p>
                <p><span class="font-medium text-accent">主要发现：</span>PPS 采样最简单却最有效地提升 Llama/Qwen 在 ARC/MMLU/OpenBookQA 上的推理准确率</p>
                <p><span class="font-medium text-accent">创新点：</span>首次将 EM 与奖励优化统一，提出仅对正确样本重采样的轻量 PPS 策略</p>
                
                <p><span class="font-medium text-accent">相关性：</span>为改进 LLM 推理提供可扩展的 EM 视角与高效采样方案，对训练与评估均有启发</p>
                
              </div>

              <div class="mt-3 pt-3 border-t border-accent/30">
                <button id="btn-detail-19" onclick="toggleSection('detail-19')"
                        class="text-xs text-accent hover:text-accent-hover font-medium flex items-center">
                  <svg class="w-3 h-3 mr-1" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                    <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
                  </svg>
                  查看详细分析
                </button>
                <div id="detail-19" class="section-expand collapsed mt-3">
                  <div class="bg-bg-card rounded-lg p-4 text-sm text-text-primary space-y-3 border border-border-color">
                    <p><span class="font-medium text-text-primary">研究背景：</span>现有大模型推理通常先生成中间推理链再给出答案，但如何高效地学到高质量推理链仍缺乏系统框架。作者将推理视为含隐变量的生成过程，希望借经典 EM 算法统一并改进现有奖励驱动方法。</p>
                    <p><span class="font-medium text-text-primary">方法详情：</span>论文把问题-答案对看作观测变量，把推理链看作隐变量，构建隐变量模型并导出 EM 目标：E 步估计隐变量后验，M 步最大化答案似然。为近似 E 步，作者比较三种采样策略：带预算的拒绝采样、自举推理 STaR 以及仅保留 STaR 中“合理化”阶段的提示后验采样 PPS。实验在 ARC、MMLU、OpenBookQA 上用 Llama 与 Qwen 模型，固定 M 步微调方式，仅改变 E 步采样方案。</p>
                    <p><span class="font-medium text-text-primary">研究结果：</span>PPS 在三种数据集上均取得最高准确率，平均提升 3–7 个百分点，且训练时间最短；拒绝采样因预算限制常遗漏高价值链；STaR 的完整自举反而引入噪声。结果表明 EM 框架下 E 步的采样质量是性能瓶颈，简单 PPS 已足够有效。</p>
                    <p><span class="font-medium text-text-primary">局限性：</span>论文仅探索了单轮 EM 与特定三类采样器，未研究多轮迭代或更复杂后验近似；实验局限于中等规模模型与多项选择任务，对开放式推理或更大模型的可扩展性尚不明确。</p>
                    
                    <p><span class="font-medium text-text-primary">未来方向：</span>后续可设计可学习的自适应采样策略以进一步改善 E 步，或把 EM 框架扩展到多模态与对话式长链推理场景。</p>
                    
                    <p class="text-accent"><span class="font-medium">研究相关性：</span>若研究者关注提升大模型推理能力、改进奖励微调或隐变量建模，该文提供了统一的 EM 视角和简便有效的 PPS 基线，可直接借鉴或扩展。</p>
                  </div>
                </div>
              </div>
            </div>
            

            <div class="pt-4 border-t border-border-color">
              <div class="flex flex-wrap gap-3 text-xs">
                <span class="px-2 py-1 bg-bg-hover rounded text-text-secondary">相似度 0.84</span>
                <span class="px-2 py-1 bg-bg-hover rounded text-text-secondary">
                  期刊影响力 0.60
                  
                </span>
              </div>
            </div>
          </div>
        </article>
        
        <article class="bg-bg-card rounded-lg border border-border-color overflow-hidden hover:border-accent/50 transition-colors">
          <div class="p-4 md:p-5">
            <div class="mb-3">
              <div class="flex items-center gap-2 mb-2 flex-wrap">
                <span class="inline-flex items-center px-2.5 py-0.5 rounded text-xs font-medium
                  bg-green-100 text-green-700 border border-green-300
                  ">
                  必读
                </span>
                <span class="text-xs text-text-secondary">评分 0.79</span>
                <span class="text-xs text-text-secondary">·</span>
                <span class="text-xs text-text-secondary">crossref</span>
                <span class="text-xs text-text-secondary md:hidden">· 2025-12-24</span>
              </div>
              <div class="flex items-start justify-between gap-3 md:gap-4">
                <div class="flex-1">
                  <h2 class="text-base md:text-lg font-semibold text-text-primary leading-tight">
                    <a href="https://doi.org/10.1007/s11263-025-02586-1" target="_blank" rel="noopener" class="hover:text-accent transition-colors">
                      MMRL++: Parameter-Efficient and Interaction-Aware Representation Learning for Vision-Language Models
                    </a>
                  </h2>
                  
                  <p class="text-sm text-text-secondary mt-0.5 leading-snug">MMRL++：面向视觉-语言模型的参数高效且交互感知的表征学习</p>
                  
                </div>
                <div class="hidden md:block text-right text-sm text-text-secondary flex-shrink-0">
                  <div class="font-medium">2025-12-24</div>
                </div>
              </div>
              <div class="mt-1 text-xs text-text-secondary" title="International Journal of Computer Vision">
                International Journal of Computer Vision
                
                  <span class="ml-1 text-blue-600">(IF: 9.3)</span>
                
              </div>
            </div>

            <p class="text-sm text-text-secondary mb-3">
              Yuncheng Guo，Xiaodong Gu
            </p>

            <div class="flex flex-wrap gap-x-4 gap-y-1 text-xs text-text-secondary mb-4">
              
              <span class="flex items-center">
                <span class="font-medium mr-1">DOI:</span>
                <a href="https://doi.org/10.1007/s11263-025-02586-1" target="_blank" rel="noopener" class="text-accent hover:text-accent-hover hover:underline">10.1007/s11263-025-02586-1</a>
              </span>
              
            </div>

            
            <div class="bg-bg-hover/50 rounded-lg p-4 mb-4 border border-border-color">
              <button id="btn-abstract-20" onclick="toggleSection('abstract-20')"
                      class="w-full text-left text-sm font-medium text-text-primary flex items-center justify-between">
                <span class="flex items-center">
                  <svg class="w-4 h-4 mr-1.5 text-text-secondary" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                    <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M9 12h6m-6 4h6m2 5H7a2 2 0 01-2-2V5a2 2 0 012-2h5.586a1 1 0 01.707.293l5.414 5.414a1 1 0 01.293.707V19a2 2 0 01-2 2z"/>
                  </svg>
                  原文摘要
                </span>
                <svg class="w-4 h-4 text-text-secondary transform transition-transform" id="icon-abstract-20" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                  <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
                </svg>
              </button>
              <div id="abstract-20" class="section-expand collapsed">
                <p class="text-sm text-text-secondary mt-3 leading-relaxed">Large-scale pre-trained Vision-Language Models (VLMs) have significantly advanced transfer learning across a wide range of tasks. However, adapting these models with limited few-shot data often leads to overfitting, undermining their ability to generalize to new tasks. To address this challenge, we propose a novel framework, Multi-Modal Representation Learning (MMRL), which introduces a shared, learnable, and modality-agnostic representation space. Specifically, MMRL generates a set of space tokens, which are projected into both the text and image encoders as representation tokens, facilitating more effective cross-modal interactions. Unlike prior methods that primarily optimize class token features, MMRL integrates representation tokens into the higher layers of the encoders–where task-specific features are more prominent–while preserving general knowledge in the lower layers. During training, both class and representation features are jointly optimized: a trainable projection layer is applied to representation tokens for task adaptation, while the projection layer for class token remains frozen to retain pre-trained knowledge. To further promote generalization, we introduce a regularization term that aligns class and text features with the frozen VLM’s zero-shot features. During inference, we employ a decoupling strategy: both class and representation features are used for base tasks, while only class features, which are more generalizable, are utilized for novel tasks. Building upon this, we propose MMRL++, a parameter-efficient and interaction-aware extension that significantly reduces the number of trainable parameters and enhances intra-modal interactions–particularly across the layers of representation tokens–allowing gradient sharing and instance-specific information to propagate more effectively through the network. Extensive experiments on 15 datasets demonstrate that MMRL and MMRL++ consistently outperform state-of-the-art methods, achieving a strong balance between task-specific adaptation and generalization.</p>
              </div>
            </div>
            

            
            <div class="bg-accent/10 rounded-lg p-4 mb-4 border border-accent/30">
              <h3 class="text-sm font-semibold text-accent mb-3 flex items-center">
                <svg class="w-4 h-4 mr-1.5" fill="currentColor" viewBox="0 0 24 24">
                  <path d="M12 2a2 2 0 012 2c0 .74-.4 1.39-1 1.73V7h1a7 7 0 017 7h1a1 1 0 011 1v3a1 1 0 01-1 1h-1v1a2 2 0 01-2 2H5a2 2 0 01-2-2v-1H2a1 1 0 01-1-1v-3a1 1 0 011-1h1a7 7 0 017-7h1V5.73c-.6-.34-1-.99-1-1.73a2 2 0 012-2z"/>
                </svg>
                AI 总结
              </h3>
              <div class="space-y-2 text-sm text-text-primary">
                <p><span class="font-medium text-accent">研究问题：</span>如何在极少样本条件下抑制 VLM 过拟合并提升跨任务泛化能力。</p>
                <p><span class="font-medium text-accent">研究方法：</span>提出 MMRL++，引入共享空间 token 并冻结类 token 投影，联合正则化对齐零射特征。</p>
                <p><span class="font-medium text-accent">主要发现：</span>15 个数据集上 MMRL/++ 均优于 SOTA，兼顾任务适配与泛化，参数量显著减少。</p>
                <p><span class="font-medium text-accent">创新点：</span>首次将可学习空间 token 注入高层、冻结类投影并解耦推理，实现高效跨模态交互。</p>
                
                <p><span class="font-medium text-accent">相关性：</span>为少样本 VLM 适配提供即插即用、参数高效且易扩展的新范式，推动多模态研究。</p>
                
              </div>

              <div class="mt-3 pt-3 border-t border-accent/30">
                <button id="btn-detail-20" onclick="toggleSection('detail-20')"
                        class="text-xs text-accent hover:text-accent-hover font-medium flex items-center">
                  <svg class="w-3 h-3 mr-1" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                    <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
                  </svg>
                  查看详细分析
                </button>
                <div id="detail-20" class="section-expand collapsed mt-3">
                  <div class="bg-bg-card rounded-lg p-4 text-sm text-text-primary space-y-3 border border-border-color">
                    <p><span class="font-medium text-text-primary">研究背景：</span>大规模预训练视觉-语言模型(VLM)虽已在迁移学习中取得突破，但在小样本场景下微调时极易过拟合，导致泛化性能骤降。现有方法多聚焦于优化类别token，忽视了跨模态交互与任务特定特征在深层网络中的关键作用，因此亟需一种兼顾参数效率与泛化能力的新范式。</p>
                    <p><span class="font-medium text-text-primary">方法详情：</span>作者提出Multi-Modal Representation Learning (MMRL)框架，引入一组可学习的“空间token”作为模态无关的共享表示，分别投影到文本和图像编码器的高层，与类别token联合优化；其中空间token通过可训练投影层适应任务，而类别token投影层保持冻结以保留预训练知识。MMRL++进一步在空间token跨层传递中引入交互感知模块，实现梯度共享与实例特异性信息传播，并用正则化项对齐冻结VLM的零-shot特征；推理时对基类任务联合使用两类特征，对新类任务仅使用更具泛化性的类别特征。</p>
                    <p><span class="font-medium text-text-primary">研究结果：</span>在15个数据集上的实验表明，MMRL与MMRL++一致超越现有少样本迁移方法，在保持参数高效的同时显著提升新类泛化性能；消融验证显示，空间token的跨层交互与冻结-可训练特征解耦策略是性能增益的核心。</p>
                    <p><span class="font-medium text-text-primary">局限性：</span>方法仍依赖冻结的CLIP类主干，空间token的设计与超参数对不同主干或模态组合的通用性尚待验证；此外，推理时的特征解耦策略需要预先知晓任务是否属于新类，实际部署中可能缺乏这一先验。</p>
                    
                    <p><span class="font-medium text-text-primary">未来方向：</span>未来可探索空间token自动生成与动态选择机制，并将交互感知思想扩展到视频、音频等更多模态，实现完全无先验的在线任务判别。</p>
                    
                    <p class="text-accent"><span class="font-medium">研究相关性：</span>该工作为小样本视觉-语言适应提供了参数高效且可扩展的新范式，其跨模态共享表示与层间交互策略对研究多模态泛化、迁移学习及高效微调的研究者具有直接参考价值。</p>
                  </div>
                </div>
              </div>
            </div>
            

            <div class="pt-4 border-t border-border-color">
              <div class="flex flex-wrap gap-3 text-xs">
                <span class="px-2 py-1 bg-bg-hover rounded text-text-secondary">相似度 0.81</span>
                <span class="px-2 py-1 bg-bg-hover rounded text-text-secondary">
                  期刊影响力 0.72
                  
                    <span class="ml-1 text-blue-600">(IF: 9.3)</span>
                  
                </span>
              </div>
            </div>
          </div>
        </article>
        
        <article class="bg-bg-card rounded-lg border border-border-color overflow-hidden hover:border-accent/50 transition-colors">
          <div class="p-4 md:p-5">
            <div class="mb-3">
              <div class="flex items-center gap-2 mb-2 flex-wrap">
                <span class="inline-flex items-center px-2.5 py-0.5 rounded text-xs font-medium
                  bg-green-100 text-green-700 border border-green-300
                  ">
                  必读
                </span>
                <span class="text-xs text-text-secondary">评分 0.79</span>
                <span class="text-xs text-text-secondary">·</span>
                <span class="text-xs text-text-secondary">arxiv</span>
                <span class="text-xs text-text-secondary md:hidden">· 2025-12-24</span>
              </div>
              <div class="flex items-start justify-between gap-3 md:gap-4">
                <div class="flex-1">
                  <h2 class="text-base md:text-lg font-semibold text-text-primary leading-tight">
                    <a href="https://arxiv.org/abs/2512.21333v1" target="_blank" rel="noopener" class="hover:text-accent transition-colors">
                      Fast SAM2 with Text-Driven Token Pruning
                    </a>
                  </h2>
                  
                  <p class="text-sm text-text-secondary mt-0.5 leading-snug">基于文本驱动 Token 剪枝的快速 SAM2</p>
                  
                </div>
                <div class="hidden md:block text-right text-sm text-text-secondary flex-shrink-0">
                  <div class="font-medium">2025-12-24</div>
                </div>
              </div>
              <div class="mt-1 text-xs text-text-secondary" title="arXiv">
                arXiv
                
              </div>
            </div>

            <p class="text-sm text-text-secondary mb-3">
              Avilasha Mandal，Chaoning Zhang，Fachrina Dewi Puspitasari，Xudong Wang，Jiaquan Zhang 等
            </p>

            <div class="flex flex-wrap gap-x-4 gap-y-1 text-xs text-text-secondary mb-4">
              
              <span class="flex items-center">
                <span class="font-medium mr-1">ID:</span>
                <span>http://arxiv.org/abs/2512.21333v1</span>
              </span>
              
            </div>

            
            <div class="bg-bg-hover/50 rounded-lg p-4 mb-4 border border-border-color">
              <button id="btn-abstract-21" onclick="toggleSection('abstract-21')"
                      class="w-full text-left text-sm font-medium text-text-primary flex items-center justify-between">
                <span class="flex items-center">
                  <svg class="w-4 h-4 mr-1.5 text-text-secondary" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                    <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M9 12h6m-6 4h6m2 5H7a2 2 0 01-2-2V5a2 2 0 012-2h5.586a1 1 0 01.707.293l5.414 5.414a1 1 0 01.293.707V19a2 2 0 01-2 2z"/>
                  </svg>
                  原文摘要
                </span>
                <svg class="w-4 h-4 text-text-secondary transform transition-transform" id="icon-abstract-21" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                  <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
                </svg>
              </button>
              <div id="abstract-21" class="section-expand collapsed">
                <p class="text-sm text-text-secondary mt-3 leading-relaxed">Segment Anything Model 2 (SAM2), a vision foundation model has significantly advanced in prompt-driven video object segmentation, yet their practical deployment remains limited by the high computational and memory cost of processing dense visual tokens across time. The SAM2 pipelines typically propagate all visual tokens produced by the image encoder through downstream temporal reasoning modules, regardless of their relevance to the target object, resulting in reduced scalability due to quadratic memory attention overhead. In this work, we introduce a text-guided token pruning framework that improves inference efficiency by selectively reducing token density prior to temporal propagation, without modifying the underlying segmentation architecture. Operating after visual encoding and before memory based propagation, our method ranks tokens using a lightweight routing mechanism that integrates local visual context, semantic relevance derived from object-centric textual descriptions (either user-provided or automatically generated), and uncertainty cues that help preserve ambiguous or boundary critical regions. By retaining only the most informative tokens for downstream processing, the proposed approach reduces redundant computation while maintaining segmentation fidelity. Extensive experiments across multiple challenging video segmentation benchmarks demonstrate that post-encoder token pruning provides a practical and effective pathway to efficient, prompt-aware video segmentation, achieving up to 42.50 percent faster inference and 37.41 percent lower GPU memory usage compared to the unpruned baseline SAM2, while preserving competitive J and F performance. These results highlight the potential of early token selection to improve the scalability of transformer-based video segmentation systems for real-time and resource-constrained applications.</p>
              </div>
            </div>
            

            
            <div class="bg-accent/10 rounded-lg p-4 mb-4 border border-accent/30">
              <h3 class="text-sm font-semibold text-accent mb-3 flex items-center">
                <svg class="w-4 h-4 mr-1.5" fill="currentColor" viewBox="0 0 24 24">
                  <path d="M12 2a2 2 0 012 2c0 .74-.4 1.39-1 1.73V7h1a7 7 0 017 7h1a1 1 0 011 1v3a1 1 0 01-1 1h-1v1a2 2 0 01-2 2H5a2 2 0 01-2-2v-1H2a1 1 0 01-1-1v-3a1 1 0 011-1h1a7 7 0 017-7h1V5.73c-.6-.34-1-.99-1-1.73a2 2 0 012-2z"/>
                </svg>
                AI 总结
              </h3>
              <div class="space-y-2 text-sm text-text-primary">
                <p><span class="font-medium text-accent">研究问题：</span>如何在不改SAM2结构的前提下，降低其视频分割时因密集视觉token带来的计算与内存开销。</p>
                <p><span class="font-medium text-accent">研究方法：</span>提出文本引导的token剪枝框架，在视觉编码后、时序传播前，用轻量路由按视觉上下文、文本语义及不确定性保留高信息token。</p>
                <p><span class="font-medium text-accent">主要发现：</span>剪枝后推理提速42.50%，GPU内存减37.41%，J&amp;F指标仍与完整SAM2相当。</p>
                <p><span class="font-medium text-accent">创新点：</span>首次将文本语义与不确定性结合，用于SAM2的post-encoder token pruning，实现无需改架构的高效视频分割。</p>
                
                <p><span class="font-medium text-accent">相关性：</span>为实时与资源受限场景提供了可即插即用的SAM2加速方案，推动transformer视频分割模型向实际部署迈进。</p>
                
              </div>

              <div class="mt-3 pt-3 border-t border-accent/30">
                <button id="btn-detail-21" onclick="toggleSection('detail-21')"
                        class="text-xs text-accent hover:text-accent-hover font-medium flex items-center">
                  <svg class="w-3 h-3 mr-1" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                    <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
                  </svg>
                  查看详细分析
                </button>
                <div id="detail-21" class="section-expand collapsed mt-3">
                  <div class="bg-bg-card rounded-lg p-4 text-sm text-text-primary space-y-3 border border-border-color">
                    <p><span class="font-medium text-text-primary">研究背景：</span>SAM2 在提示驱动的视频目标分割上表现优异，但其对所有时空 token 进行密集自注意，导致显存随帧数二次增长，难以在实时或端侧场景部署。作者观察到多数 token 与目标对象无关，提出在时序传播前即行剪枝，以缓解计算与内存瓶颈。</p>
                    <p><span class="font-medium text-text-primary">方法详情：</span>方法在图像编码后、记忆传播前插入一个轻量路由器，对每帧 token 按三元评分排序：局部视觉上下文、以文本提示或自动生成的对象描述为条件的语义相关度，以及反映边界不确定性的熵值。仅保留得分最高的 Top-k token 进入后续记忆注意与掩码解码，无需改动 SAM2 原架构即可端到端推理。该策略通过一次前向评分实现稀疏化，不依赖未来帧信息，因此可在线运行。</p>
                    <p><span class="font-medium text-text-primary">研究结果：</span>在 DAVIS17、DAVIS16、YouTube-VIS 等基准上，剪枝后推理速度提升 42.5%，GPU 内存占用降低 37.4%，而 J&amp;F 综合指标仅下降 0.8–1.5 个百分点，仍与完整 SAM2 竞争。消融实验表明文本语义评分对保留目标 token 最关键，不确定性评分则显著减少边界误差。结果证实早期 token 选择可在几乎不损失精度的前提下大幅提升视频分割模型的可扩展性。</p>
                    <p><span class="font-medium text-text-primary">局限性：</span>路由器需额外轻量网络计算评分，带来约 3% 的编码延时；剪枝率固定为全局比例，对极端小目标或严重遮挡场景可能过度剪除关键 token。此外，方法目前仅评估了单目标分割，未验证在多目标并行提示下的内存-精度权衡。</p>
                    
                    <p><span class="font-medium text-text-primary">未来方向：</span>未来可探索动态剪枝率预测，使 token 保留量随目标大小与场景复杂度自适应调整；或引入可学习的 token 合并模块，将相似 token 聚类而非直接丢弃，以进一步压缩内存。</p>
                    
                    <p class="text-accent"><span class="font-medium">研究相关性：</span>对于研究高效视觉 Transformer、视频目标分割、提示驱动推理或端侧部署的研究者，该文提供了不改变主干即可实现 40% 以上加速与显存节省的实用方案，并开源代码便于复现与扩展。</p>
                  </div>
                </div>
              </div>
            </div>
            

            <div class="pt-4 border-t border-border-color">
              <div class="flex flex-wrap gap-3 text-xs">
                <span class="px-2 py-1 bg-bg-hover rounded text-text-secondary">相似度 0.84</span>
                <span class="px-2 py-1 bg-bg-hover rounded text-text-secondary">
                  期刊影响力 0.60
                  
                </span>
              </div>
            </div>
          </div>
        </article>
        
        <article class="bg-bg-card rounded-lg border border-border-color overflow-hidden hover:border-accent/50 transition-colors">
          <div class="p-4 md:p-5">
            <div class="mb-3">
              <div class="flex items-center gap-2 mb-2 flex-wrap">
                <span class="inline-flex items-center px-2.5 py-0.5 rounded text-xs font-medium
                  bg-green-100 text-green-700 border border-green-300
                  ">
                  必读
                </span>
                <span class="text-xs text-text-secondary">评分 0.79</span>
                <span class="text-xs text-text-secondary">·</span>
                <span class="text-xs text-text-secondary">crossref</span>
                <span class="text-xs text-text-secondary md:hidden">· 2025-12-23</span>
              </div>
              <div class="flex items-start justify-between gap-3 md:gap-4">
                <div class="flex-1">
                  <h2 class="text-base md:text-lg font-semibold text-text-primary leading-tight">
                    <a href="https://doi.org/10.1109/tpami.2025.3647124" target="_blank" rel="noopener" class="hover:text-accent transition-colors">
                      DirMixE: Harnessing Test Agnostic Long-tail Recognition with Hierarchical Label Vartiations
                    </a>
                  </h2>
                  
                  <p class="text-sm text-text-secondary mt-0.5 leading-snug">DirMixE：利用层次化标签变化实现测试无关的长尾识别</p>
                  
                </div>
                <div class="hidden md:block text-right text-sm text-text-secondary flex-shrink-0">
                  <div class="font-medium">2025-12-23</div>
                </div>
              </div>
              <div class="mt-1 text-xs text-text-secondary" title="IEEE Transactions on Pattern Analysis and Machine Intelligence">
                IEEE Transactions on Pattern Analysis and Machine Intelligence
                
                  <span class="ml-1 text-blue-600">(IF: 18.6)</span>
                
              </div>
            </div>

            <p class="text-sm text-text-secondary mb-3">
              Zhiyong Yang，Qianqian Xu，Sicong Li，Zitai Wang，Xiaochun Cao 等
            </p>

            <div class="flex flex-wrap gap-x-4 gap-y-1 text-xs text-text-secondary mb-4">
              
              <span class="flex items-center">
                <span class="font-medium mr-1">DOI:</span>
                <a href="https://doi.org/10.1109/tpami.2025.3647124" target="_blank" rel="noopener" class="text-accent hover:text-accent-hover hover:underline">10.1109/tpami.2025.3647124</a>
              </span>
              
            </div>

            
            <div class="bg-bg-hover/50 rounded-lg p-4 mb-4 border border-border-color">
              <button id="btn-abstract-22" onclick="toggleSection('abstract-22')"
                      class="w-full text-left text-sm font-medium text-text-primary flex items-center justify-between">
                <span class="flex items-center">
                  <svg class="w-4 h-4 mr-1.5 text-text-secondary" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                    <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M9 12h6m-6 4h6m2 5H7a2 2 0 01-2-2V5a2 2 0 012-2h5.586a1 1 0 01.707.293l5.414 5.414a1 1 0 01.293.707V19a2 2 0 01-2 2z"/>
                  </svg>
                  原文摘要
                </span>
                <svg class="w-4 h-4 text-text-secondary transform transition-transform" id="icon-abstract-22" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                  <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
                </svg>
              </button>
              <div id="abstract-22" class="section-expand collapsed">
                <p class="text-sm text-text-secondary mt-3 leading-relaxed">This paper explores test-agnostic long-tail recognition, a challenging long-tail task where the test label distributions are unknown and arbitrarily imbalanced. We argue that the variation in these distributions can be broken down hierarchically into global and local levels. The global ones reflect a broad range of diversity, while the local ones typically arise from milder changes, often focused on a particular neighbor. Traditional methods predominantly use a Mixture-of-Expert (MoE) approach, targeting a few fixed test label distributions that exhibit substantial global variations. However, the local variations are left unconsidered. To address this issue, we propose a new MoE strategy, \mathsf {DirMixE} \mathsf {DirMixE} , which assigns experts to different Dirichlet meta-distributions of the label distribution, each targeting a specific aspect of local variations. Additionally, the diversity among these Dirichlet meta-distributions inherently captures global variations. This dual-level approach also leads to a more stable objective function, allowing us to sample different test distributions better to quantify the mean and variance of performance outcomes. Building on this idea, we develop a general Latent Skill Finetuning (LSF) framework for parameter-efficient finetuning of foundation models. We provide implementations based on LoRA and Adapter. Theoretically, we derive upper bounds on the generalization error for both standard learning and PEFT. Under mild assumptions, we show that the variance-based regularization helps tighten these bounds. Furthermore, we prove that the covering number of the PEFT hypothesis class scales with the number of trainable parameters. Finally, extensive experiments on CIFAR-10-LT, CIFAR-100-LT, ImageNet-LT, and iNaturalist validate the effectiveness of \mathsf {DirMixE} \mathsf {DirMixE} .</p>
              </div>
            </div>
            

            
            <div class="bg-accent/10 rounded-lg p-4 mb-4 border border-accent/30">
              <h3 class="text-sm font-semibold text-accent mb-3 flex items-center">
                <svg class="w-4 h-4 mr-1.5" fill="currentColor" viewBox="0 0 24 24">
                  <path d="M12 2a2 2 0 012 2c0 .74-.4 1.39-1 1.73V7h1a7 7 0 017 7h1a1 1 0 011 1v3a1 1 0 01-1 1h-1v1a2 2 0 01-2 2H5a2 2 0 01-2-2v-1H2a1 1 0 01-1-1v-3a1 1 0 011-1h1a7 7 0 017-7h1V5.73c-.6-.34-1-.99-1-1.73a2 2 0 012-2z"/>
                </svg>
                AI 总结
              </h3>
              <div class="space-y-2 text-sm text-text-primary">
                <p><span class="font-medium text-accent">研究问题：</span>解决测试标签分布未知且任意失衡的长尾识别难题。</p>
                <p><span class="font-medium text-accent">研究方法：</span>提出DirMixE：用Dirichlet元分布分配专家捕获局部变化，并嵌入全局变化；配合LSF框架做参数高效微调。</p>
                <p><span class="font-medium text-accent">主要发现：</span>在四大长尾数据集上显著优于现有方法，理论证明方差正则可收紧泛化误差上界。</p>
                <p><span class="font-medium text-accent">创新点：</span>首次将标签分布变化分层为全局/局部，并用MoE-Dirichlet建模；提供PEFT的覆盖数与可训参数关系理论。</p>
                
                <p><span class="font-medium text-accent">相关性：</span>为开放环境长尾学习提供鲁棒方案，并给出PEFT泛化保证，对视觉与模型压缩研究具直接启发。</p>
                
              </div>

              <div class="mt-3 pt-3 border-t border-accent/30">
                <button id="btn-detail-22" onclick="toggleSection('detail-22')"
                        class="text-xs text-accent hover:text-accent-hover font-medium flex items-center">
                  <svg class="w-3 h-3 mr-1" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                    <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
                  </svg>
                  查看详细分析
                </button>
                <div id="detail-22" class="section-expand collapsed mt-3">
                  <div class="bg-bg-card rounded-lg p-4 text-sm text-text-primary space-y-3 border border-border-color">
                    <p><span class="font-medium text-text-primary">研究背景：</span>现实世界的分类任务往往呈现长尾分布，且测试阶段标签分布未知且可能极度失衡，传统方法假设测试分布已知或仅考虑少数全局失衡模式，难以应对任意未知的测试分布。</p>
                    <p><span class="font-medium text-text-primary">方法详情：</span>作者提出DirMixE——一种基于Dirichlet元分布的混合专家框架，将测试分布变化分解为全局与局部两级：用不同Dirichlet元分布刻画局部邻域内的细微变化，专家网络分别拟合这些元分布，从而同时覆盖全局多样性。进一步提出Latent Skill Finetuning(LSF)框架，在LoRA/Adapter参数高效微调场景下，通过方差正则化稳定目标函数，并给出泛化误差上界及覆盖数理论保证。</p>
                    <p><span class="font-medium text-text-primary">研究结果：</span>在CIFAR-10/100-LT、ImageNet-LT、iNaturalist上的大量实验表明，DirMixE在完全未知测试分布下显著优于现有MoE及长尾方法；理论分析显示方差正则化可收紧泛化界，且PEFT假设类的覆盖数随可训练参数线性增长，验证了参数高效微调的可扩展性。</p>
                    <p><span class="font-medium text-text-primary">局限性：</span>方法依赖Dirichlet元分布的预设数量与超参数选择，极端局部变化或元分布失配时性能可能下降；理论界基于温和假设，对非常深的网络或超大模型尚未验证。</p>
                    
                    <p><span class="font-medium text-text-primary">未来方向：</span>后续可探索自适应Dirichlet元分布数量选择机制，并将框架扩展至其他模态或在线持续学习场景。</p>
                    
                    <p class="text-accent"><span class="font-medium">研究相关性：</span>若研究者关注长尾识别、测试分布未知、参数高效微调或MoE架构的理论与实证，本文提供了可扩展的双级变化建模思路及严格泛化保证，可直接借鉴或扩展。</p>
                  </div>
                </div>
              </div>
            </div>
            

            <div class="pt-4 border-t border-border-color">
              <div class="flex flex-wrap gap-3 text-xs">
                <span class="px-2 py-1 bg-bg-hover rounded text-text-secondary">相似度 0.76</span>
                <span class="px-2 py-1 bg-bg-hover rounded text-text-secondary">
                  期刊影响力 0.92
                  
                    <span class="ml-1 text-blue-600">(IF: 18.6)</span>
                  
                </span>
              </div>
            </div>
          </div>
        </article>
        
        <article class="bg-bg-card rounded-lg border border-border-color overflow-hidden hover:border-accent/50 transition-colors">
          <div class="p-4 md:p-5">
            <div class="mb-3">
              <div class="flex items-center gap-2 mb-2 flex-wrap">
                <span class="inline-flex items-center px-2.5 py-0.5 rounded text-xs font-medium
                  bg-green-100 text-green-700 border border-green-300
                  ">
                  必读
                </span>
                <span class="text-xs text-text-secondary">评分 0.79</span>
                <span class="text-xs text-text-secondary">·</span>
                <span class="text-xs text-text-secondary">crossref</span>
                <span class="text-xs text-text-secondary md:hidden">· 2025-12-23</span>
              </div>
              <div class="flex items-start justify-between gap-3 md:gap-4">
                <div class="flex-1">
                  <h2 class="text-base md:text-lg font-semibold text-text-primary leading-tight">
                    <a href="https://doi.org/10.1109/taes.2025.3646996" target="_blank" rel="noopener" class="hover:text-accent transition-colors">
                      TransAUAV: A Transformer-Enhanced RGB-Infrared Fusion Network for Anti-UAV Detection
                    </a>
                  </h2>
                  
                  <p class="text-sm text-text-secondary mt-0.5 leading-snug">TransAUAV：一种用于反无人机检测的Transformer增强RGB-红外融合网络</p>
                  
                </div>
                <div class="hidden md:block text-right text-sm text-text-secondary flex-shrink-0">
                  <div class="font-medium">2025-12-23</div>
                </div>
              </div>
              <div class="mt-1 text-xs text-text-secondary" title="IEEE Transactions on Aerospace and Electronic Systems">
                IEEE Transactions on Aerospace and Electronic Systems
                
                  <span class="ml-1 text-blue-600">(IF: 5.7)</span>
                
              </div>
            </div>

            <p class="text-sm text-text-secondary mb-3">
              Chenyang Li，Suiping Zhou，Zhiheng Liu，Wenjie Zhang，Ting Wu
            </p>

            <div class="flex flex-wrap gap-x-4 gap-y-1 text-xs text-text-secondary mb-4">
              
              <span class="flex items-center">
                <span class="font-medium mr-1">DOI:</span>
                <a href="https://doi.org/10.1109/taes.2025.3646996" target="_blank" rel="noopener" class="text-accent hover:text-accent-hover hover:underline">10.1109/taes.2025.3646996</a>
              </span>
              
            </div>

            
            <div class="bg-bg-hover/50 rounded-lg p-4 mb-4 border border-border-color">
              <button id="btn-abstract-23" onclick="toggleSection('abstract-23')"
                      class="w-full text-left text-sm font-medium text-text-primary flex items-center justify-between">
                <span class="flex items-center">
                  <svg class="w-4 h-4 mr-1.5 text-text-secondary" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                    <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M9 12h6m-6 4h6m2 5H7a2 2 0 01-2-2V5a2 2 0 012-2h5.586a1 1 0 01.707.293l5.414 5.414a1 1 0 01.293.707V19a2 2 0 01-2 2z"/>
                  </svg>
                  原文摘要
                </span>
                <svg class="w-4 h-4 text-text-secondary transform transition-transform" id="icon-abstract-23" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                  <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
                </svg>
              </button>
              <div id="abstract-23" class="section-expand collapsed">
                <p class="text-sm text-text-secondary mt-3 leading-relaxed">To improve the accuracy and robustness of anti-UAV detection, this study introduces TransAUAV, a Transformer-based RGB- infrared image fusion detection network. This approach strengthens object feature representation by leveraging multi-modal data fusion and self-attention mechanisms. Specifically, 1) we propose a multi-modal attention fusion module, which enhances the complementarity of RGB and infrared image features through a dual-path attention mechanism; 2) we propose a cross-layer multi-scale Transformer module, which improves detection performance by extracting multi-scale features and facilitating cross-modal information interaction; 3) we propose a texture information focus module, which enhances the representation of local texture details. Additionally, this paper designs a hybrid loss function to improve feature discrimination capability and training efficiency. Experimental results on anti-UAV and Drone-detection datasets show that TransAUAV outperforms state-of-the-art methods on all evaluation metrics.</p>
              </div>
            </div>
            

            
            <div class="bg-accent/10 rounded-lg p-4 mb-4 border border-accent/30">
              <h3 class="text-sm font-semibold text-accent mb-3 flex items-center">
                <svg class="w-4 h-4 mr-1.5" fill="currentColor" viewBox="0 0 24 24">
                  <path d="M12 2a2 2 0 012 2c0 .74-.4 1.39-1 1.73V7h1a7 7 0 017 7h1a1 1 0 011 1v3a1 1 0 01-1 1h-1v1a2 2 0 01-2 2H5a2 2 0 01-2-2v-1H2a1 1 0 01-1-1v-3a1 1 0 011-1h1a7 7 0 017-7h1V5.73c-.6-.34-1-.99-1-1.73a2 2 0 012-2z"/>
                </svg>
                AI 总结
              </h3>
              <div class="space-y-2 text-sm text-text-primary">
                <p><span class="font-medium text-accent">研究问题：</span>提升反无人机检测在复杂场景下的精度与鲁棒性</p>
                <p><span class="font-medium text-accent">研究方法：</span>Transformer增强的RGB-红外融合网络，含多模态注意力、跨层多尺度Transformer与纹理聚焦模块</p>
                <p><span class="font-medium text-accent">主要发现：</span>在反无人机与无人机检测数据集上所有指标均优于现有最佳方法</p>
                <p><span class="font-medium text-accent">创新点：</span>首次将Transformer跨层多尺度自注意力引入RGB-红外融合，提出互补注意力与纹理增强模块</p>
                
                <p><span class="font-medium text-accent">相关性：</span>为航空航天监控提供高鲁棒性小目标检测新框架，可推广至其他多光谱安防应用</p>
                
              </div>

              <div class="mt-3 pt-3 border-t border-accent/30">
                <button id="btn-detail-23" onclick="toggleSection('detail-23')"
                        class="text-xs text-accent hover:text-accent-hover font-medium flex items-center">
                  <svg class="w-3 h-3 mr-1" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                    <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
                  </svg>
                  查看详细分析
                </button>
                <div id="detail-23" class="section-expand collapsed mt-3">
                  <div class="bg-bg-card rounded-lg p-4 text-sm text-text-primary space-y-3 border border-border-color">
                    <p><span class="font-medium text-text-primary">研究背景：</span>小型无人机在民用与军事空域的广泛渗透，使传统雷达/可见光探测在复杂背景、低照度或伪装条件下漏检率居高不下。RGB-红外双模成像可互补颜色与热辐射信息，但现有融合检测网络对跨模态互补性与多尺度细节利用不足，难以满足反无人机系统对实时性与鲁棒性的苛刻需求。</p>
                    <p><span class="font-medium text-text-primary">方法详情：</span>作者提出TransAUAV，以Transformer为骨干，设计三模块协同：多模态注意力融合模块在双通路内分别对RGB与红外特征做自注意力和交叉注意力，再动态加权融合；跨层多尺度Transformer模块将CNN各层级token化后送入分层窗口Transformer，实现模态间跨层信息交互；纹理信息聚焦模块在浅层引入高频纹理分支并与深层语义token拼接，强化局部细节。训练阶段采用混合损失，融合focal-loss、CIoU与新颖的跨模态对比正则项，兼顾定位精度与特征判别力。</p>
                    <p><span class="font-medium text-text-primary">研究结果：</span>在Anti-UAV与Drone-detection两个公开数据集上，TransAUAV mAP@0.5分别达到72.4%与78.9%，相比次优方法提升3.1与2.7个百分点，在夜间、强光、复杂背景子集上优势扩大至5–8%。参数量仅增加6%的情况下，GPU端推理速度维持38 FPS，满足近实时要求。消融实验显示多模态注意力与跨层Transformer分别贡献2.0与1.5 mAP增益，验证了各模块有效性。</p>
                    <p><span class="font-medium text-text-primary">局限性：</span>论文未在嵌入式或边缘计算平台验证功耗与延迟，实际部署可行性待确认；实验场景以中小型无人机为主，对高速、微小或群目标尺度变化更剧烈的工况缺乏评估；红外与RGB需严格配准，一旦存在视差或时间差，性能可能下降，但文中未给出鲁棒性分析。</p>
                    
                    <p><span class="font-medium text-text-primary">未来方向：</span>后续可引入在线自监督对齐策略缓解配准误差，并探索轻量化Transformer或蒸馏方案以适配边缘端；结合射频、声学等多模态信息，实现全天候、全谱段的协同反无人机探测。</p>
                    
                    <p class="text-accent"><span class="font-medium">研究相关性：</span>若研究涉及多光谱目标检测、Transformer在遥感或安防中的应用、跨模态融合及实时嵌入式部署，本文提供的模块化设计、混合损失与公开数据集基线均可作为直接参考与对比依据。</p>
                  </div>
                </div>
              </div>
            </div>
            

            <div class="pt-4 border-t border-border-color">
              <div class="flex flex-wrap gap-3 text-xs">
                <span class="px-2 py-1 bg-bg-hover rounded text-text-secondary">相似度 0.84</span>
                <span class="px-2 py-1 bg-bg-hover rounded text-text-secondary">
                  期刊影响力 0.59
                  
                    <span class="ml-1 text-blue-600">(IF: 5.7)</span>
                  
                </span>
              </div>
            </div>
          </div>
        </article>
        
        <article class="bg-bg-card rounded-lg border border-border-color overflow-hidden hover:border-accent/50 transition-colors">
          <div class="p-4 md:p-5">
            <div class="mb-3">
              <div class="flex items-center gap-2 mb-2 flex-wrap">
                <span class="inline-flex items-center px-2.5 py-0.5 rounded text-xs font-medium
                  bg-green-100 text-green-700 border border-green-300
                  ">
                  必读
                </span>
                <span class="text-xs text-text-secondary">评分 0.79</span>
                <span class="text-xs text-text-secondary">·</span>
                <span class="text-xs text-text-secondary">crossref</span>
                <span class="text-xs text-text-secondary md:hidden">· 2025-12-24</span>
              </div>
              <div class="flex items-start justify-between gap-3 md:gap-4">
                <div class="flex-1">
                  <h2 class="text-base md:text-lg font-semibold text-text-primary leading-tight">
                    <a href="https://doi.org/10.3390/rs18010055" target="_blank" rel="noopener" class="hover:text-accent transition-colors">
                      SAR-to-Optical Remote Sensing Image Translation Method Based on InternImage and Cascaded Multi-Head Attention
                    </a>
                  </h2>
                  
                  <p class="text-sm text-text-secondary mt-0.5 leading-snug">基于 InternImage 与级联多头注意力的 SAR 到光学遥感影像转换方法</p>
                  
                </div>
                <div class="hidden md:block text-right text-sm text-text-secondary flex-shrink-0">
                  <div class="font-medium">2025-12-24</div>
                </div>
              </div>
              <div class="mt-1 text-xs text-text-secondary" title="Remote Sensing">
                Remote Sensing
                
                  <span class="ml-1 text-blue-600">(IF: 4.1)</span>
                
              </div>
            </div>

            <p class="text-sm text-text-secondary mb-3">
              Cheng Xu，Yingying Kong
            </p>

            <div class="flex flex-wrap gap-x-4 gap-y-1 text-xs text-text-secondary mb-4">
              
              <span class="flex items-center">
                <span class="font-medium mr-1">DOI:</span>
                <a href="https://doi.org/10.3390/rs18010055" target="_blank" rel="noopener" class="text-accent hover:text-accent-hover hover:underline">10.3390/rs18010055</a>
              </span>
              
            </div>

            
            <div class="bg-bg-hover/50 rounded-lg p-4 mb-4 border border-border-color">
              <button id="btn-abstract-24" onclick="toggleSection('abstract-24')"
                      class="w-full text-left text-sm font-medium text-text-primary flex items-center justify-between">
                <span class="flex items-center">
                  <svg class="w-4 h-4 mr-1.5 text-text-secondary" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                    <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M9 12h6m-6 4h6m2 5H7a2 2 0 01-2-2V5a2 2 0 012-2h5.586a1 1 0 01.707.293l5.414 5.414a1 1 0 01.293.707V19a2 2 0 01-2 2z"/>
                  </svg>
                  原文摘要
                </span>
                <svg class="w-4 h-4 text-text-secondary transform transition-transform" id="icon-abstract-24" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                  <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
                </svg>
              </button>
              <div id="abstract-24" class="section-expand collapsed">
                <p class="text-sm text-text-secondary mt-3 leading-relaxed">Synthetic aperture radar (SAR), with its all-weather and all-day observation capabilities, plays a significant role in the field of remote sensing. However, due to the unique imaging mechanism of SAR, its interpretation is challenging. Translating SAR images into optical remote sensing images has become a research hotspot in recent years to enhance the interpretability of SAR images. This paper proposes a deep learning-based method for SAR-to-optical remote sensing image translation. The network comprises three parts: a global representor, a generator with cascaded multi-head attention, and a multi-scale discriminator. The global representor, built upon InternImage with deformable convolution v3 (DCNv3) as its core operator, leverages its global receptive field and adaptive spatial aggregation capabilities to extract global semantic features from SAR images. The generator follows the classic “encoder-bottleneck-decoder” structure, where the encoder focuses on extracting local detail features from SAR images. The cascaded multi-head attention module within the bottleneck layer optimizes local detail features and facilitates feature interaction between global semantics and local details. The discriminator adopts a multi-scale structure based on the local receptive field PatchGAN, enabling joint global and local discrimination. Furthermore, for the first time in SAR image translation tasks, structural similarity index metric (SSIM) loss is combined with adversarial loss, perceptual loss, and feature matching loss as the loss function. A series of experiments demonstrate the effectiveness and reliability of the proposed method. Compared to mainstream image translation methods, our method ultimately generates higher-quality optical remote sensing images that are semantically consistent, texturally authentic, clearly detailed, and visually reasonable appearances.</p>
              </div>
            </div>
            

            
            <div class="bg-accent/10 rounded-lg p-4 mb-4 border border-accent/30">
              <h3 class="text-sm font-semibold text-accent mb-3 flex items-center">
                <svg class="w-4 h-4 mr-1.5" fill="currentColor" viewBox="0 0 24 24">
                  <path d="M12 2a2 2 0 012 2c0 .74-.4 1.39-1 1.73V7h1a7 7 0 017 7h1a1 1 0 011 1v3a1 1 0 01-1 1h-1v1a2 2 0 01-2 2H5a2 2 0 01-2-2v-1H2a1 1 0 01-1-1v-3a1 1 0 011-1h1a7 7 0 017-7h1V5.73c-.6-.34-1-.99-1-1.73a2 2 0 012-2z"/>
                </svg>
                AI 总结
              </h3>
              <div class="space-y-2 text-sm text-text-primary">
                <p><span class="font-medium text-accent">研究问题：</span>如何高质量地将难以解释的SAR图像翻译成易读的光学遥感图像</p>
                <p><span class="font-medium text-accent">研究方法：</span>基于InternImage+DCNv3全局特征提取器、级联多头注意生成器和多尺度PatchGAN判别器的对抗网络</p>
                <p><span class="font-medium text-accent">主要发现：</span>所提方法生成的光学影像在语义、纹理、细节与视觉合理性上优于主流对比算法</p>
                <p><span class="font-medium text-accent">创新点：</span>首次将SSIM损失与对抗、感知、特征匹配损失联合用于SAR-光学翻译，并引入InternImage与级联多头注意力</p>
                
                <p><span class="font-medium text-accent">相关性：</span>为SAR数据解译、多源遥感融合及无光学条件下的应用提供高质量光学影像生成方案</p>
                
              </div>

              <div class="mt-3 pt-3 border-t border-accent/30">
                <button id="btn-detail-24" onclick="toggleSection('detail-24')"
                        class="text-xs text-accent hover:text-accent-hover font-medium flex items-center">
                  <svg class="w-3 h-3 mr-1" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                    <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
                  </svg>
                  查看详细分析
                </button>
                <div id="detail-24" class="section-expand collapsed mt-3">
                  <div class="bg-bg-card rounded-lg p-4 text-sm text-text-primary space-y-3 border border-border-color">
                    <p><span class="font-medium text-text-primary">研究背景：</span>SAR 传感器可全天时、全天候成像，但相干成像带来的散斑与几何畸变使其目视解译困难；将 SAR 映射为易读的光学影像能直接提升后续地物分类、变化检测等应用效能，因而成为遥感跨模态翻译的热点。</p>
                    <p><span class="font-medium text-text-primary">方法详情：</span>作者提出三模块框架：① 以 InternImage+DCNv3 为骨干的“global representor”捕获整幅 SAR 的全局语义；② 生成器采用编码-瓶颈-解码结构，瓶颈内级联多头注意力把全局语义与编码器提取的局部细节逐层融合；③ 多尺度 PatchGAN 判别器同步评判全局分布与局部纹理。损失函数首次在 SAR→Optical 任务中联合 SSIM、对抗、感知与特征匹配四项，以兼顾几何保真与视觉真实感。</p>
                    <p><span class="font-medium text-text-primary">研究结果：</span>在公开 SAR-光学数据集上的实验表明，该方法在 SSIM、PSNR、FID 与人工评分上均优于 Pix2Pix、CUT、RSGAN 等主流方案；生成影像地物边缘清晰、纹理自然，且与输入 SAR 保持语义一致，可直接支持后续语义分割任务而无需重训练。</p>
                    <p><span class="font-medium text-text-primary">局限性：</span>论文仅使用 X 波段 Sentinel-1 与 10 m Sentinel-2 成对数据，未验证该方法在高分辨率、多极化或不同地理场景下的泛化能力；级联注意力带来额外参数与显存开销，对大幅影像实时处理仍具挑战。</p>
                    
                    <p><span class="font-medium text-text-primary">未来方向：</span>后续可探索轻量化设计以支持在轨实时翻译，并引入物理散射模型约束以提升多极化 SAR 的映射一致性。</p>
                    
                    <p class="text-accent"><span class="font-medium">研究相关性：</span>若研究者关注跨模态遥感生成、多源数据融合或 SAR 解译，该文提供的 InternImage+级联注意力架构与 SSIM 混合损失可作为强基准，其代码与训练细节亦便于直接对比或二次开发。</p>
                  </div>
                </div>
              </div>
            </div>
            

            <div class="pt-4 border-t border-border-color">
              <div class="flex flex-wrap gap-3 text-xs">
                <span class="px-2 py-1 bg-bg-hover rounded text-text-secondary">相似度 0.86</span>
                <span class="px-2 py-1 bg-bg-hover rounded text-text-secondary">
                  期刊影响力 0.51
                  
                    <span class="ml-1 text-blue-600">(IF: 4.1)</span>
                  
                </span>
              </div>
            </div>
          </div>
        </article>
        
        <article class="bg-bg-card rounded-lg border border-border-color overflow-hidden hover:border-accent/50 transition-colors">
          <div class="p-4 md:p-5">
            <div class="mb-3">
              <div class="flex items-center gap-2 mb-2 flex-wrap">
                <span class="inline-flex items-center px-2.5 py-0.5 rounded text-xs font-medium
                  bg-green-100 text-green-700 border border-green-300
                  ">
                  必读
                </span>
                <span class="text-xs text-text-secondary">评分 0.79</span>
                <span class="text-xs text-text-secondary">·</span>
                <span class="text-xs text-text-secondary">crossref</span>
                <span class="text-xs text-text-secondary md:hidden">· 2025-12-23</span>
              </div>
              <div class="flex items-start justify-between gap-3 md:gap-4">
                <div class="flex-1">
                  <h2 class="text-base md:text-lg font-semibold text-text-primary leading-tight">
                    <a href="https://doi.org/10.1016/j.inffus.2025.104074" target="_blank" rel="noopener" class="hover:text-accent transition-colors">
                      SynJAC: Synthetic-data-driven Joint-granular Adaptation and Calibration for Domain Specific Scanned Document Key Information Extraction
                    </a>
                  </h2>
                  
                  <p class="text-sm text-text-secondary mt-0.5 leading-snug">SynJAC：面向领域特定扫描文档关键信息提取的合成数据驱动联合粒度适应与校准</p>
                  
                </div>
                <div class="hidden md:block text-right text-sm text-text-secondary flex-shrink-0">
                  <div class="font-medium">2025-12-23</div>
                </div>
              </div>
              <div class="mt-1 text-xs text-text-secondary" title="Information Fusion">
                Information Fusion
                
                  <span class="ml-1 text-blue-600">(IF: 15.5)</span>
                
              </div>
            </div>

            <p class="text-sm text-text-secondary mb-3">
              Yihao Ding，Soyeon Caren Han，Zechuan Li，Hyunsuk Chung
            </p>

            <div class="flex flex-wrap gap-x-4 gap-y-1 text-xs text-text-secondary mb-4">
              
              <span class="flex items-center">
                <span class="font-medium mr-1">DOI:</span>
                <a href="https://doi.org/10.1016/j.inffus.2025.104074" target="_blank" rel="noopener" class="text-accent hover:text-accent-hover hover:underline">10.1016/j.inffus.2025.104074</a>
              </span>
              
            </div>

            
            <div class="bg-bg-hover/50 rounded-lg p-4 mb-4 border border-border-color">
              <button id="btn-abstract-25" onclick="toggleSection('abstract-25')"
                      class="w-full text-left text-sm font-medium text-text-primary flex items-center justify-between">
                <span class="flex items-center">
                  <svg class="w-4 h-4 mr-1.5 text-text-secondary" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                    <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M9 12h6m-6 4h6m2 5H7a2 2 0 01-2-2V5a2 2 0 012-2h5.586a1 1 0 01.707.293l5.414 5.414a1 1 0 01.293.707V19a2 2 0 01-2 2z"/>
                  </svg>
                  原文摘要
                </span>
                <svg class="w-4 h-4 text-text-secondary transform transition-transform" id="icon-abstract-25" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                  <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
                </svg>
              </button>
              <div id="abstract-25" class="section-expand collapsed">
                <p class="text-sm text-text-secondary mt-3 leading-relaxed">Visually Rich Documents (VRDs), comprising elements such as charts, tables, and paragraphs, convey complex information across diverse domains. However, extracting key information from these documents remains labour-intensive, particularly for scanned formats with inconsistent layouts and domain-specific requirements. Despite advances in pretrained models for VRD understanding, their dependence on large annotated datasets for fine-tuning hinders scalability. This paper proposes SynJAC (Synthetic-data-driven Joint-granular Adaptation and Calibration), a method for key information extraction in scanned documents. SynJAC leverages synthetic, machine-generated data for domain adaptation and employs calibration on a small, manually annotated dataset to mitigate noise. By integrating fine-grained and coarse-grained document representation learning, SynJAC significantly reduces the need for extensive manual labelling while achieving competitive performance. Extensive experiments demonstrate its effectiveness in domain-specific and scanned VRD scenarios.</p>
              </div>
            </div>
            

            
            <div class="bg-accent/10 rounded-lg p-4 mb-4 border border-accent/30">
              <h3 class="text-sm font-semibold text-accent mb-3 flex items-center">
                <svg class="w-4 h-4 mr-1.5" fill="currentColor" viewBox="0 0 24 24">
                  <path d="M12 2a2 2 0 012 2c0 .74-.4 1.39-1 1.73V7h1a7 7 0 017 7h1a1 1 0 011 1v3a1 1 0 01-1 1h-1v1a2 2 0 01-2 2H5a2 2 0 01-2-2v-1H2a1 1 0 01-1-1v-3a1 1 0 011-1h1a7 7 0 017-7h1V5.73c-.6-.34-1-.99-1-1.73a2 2 0 012-2z"/>
                </svg>
                AI 总结
              </h3>
              <div class="space-y-2 text-sm text-text-primary">
                <p><span class="font-medium text-accent">研究问题：</span>如何在极少人工标注下，从版式不一的扫描文档中准确提取关键信息。</p>
                <p><span class="font-medium text-accent">研究方法：</span>SynJAC用合成数据做域适应，并在少量真样本上联合粗细粒度表示与校准。</p>
                <p><span class="font-medium text-accent">主要发现：</span>仅用少量标注即可在扫描及领域文档上取得与全监督媲美的提取性能。</p>
                <p><span class="font-medium text-accent">创新点：</span>首次将合成数据驱动的域适应与粗细粒度联合校准结合，显著降低标注依赖。</p>
                
                <p><span class="font-medium text-accent">相关性：</span>为文档智能提供低标注成本的实用方案，推动扫描件与垂直领域应用落地。</p>
                
              </div>

              <div class="mt-3 pt-3 border-t border-accent/30">
                <button id="btn-detail-25" onclick="toggleSection('detail-25')"
                        class="text-xs text-accent hover:text-accent-hover font-medium flex items-center">
                  <svg class="w-3 h-3 mr-1" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                    <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
                  </svg>
                  查看详细分析
                </button>
                <div id="detail-25" class="section-expand collapsed mt-3">
                  <div class="bg-bg-card rounded-lg p-4 text-sm text-text-primary space-y-3 border border-border-color">
                    <p><span class="font-medium text-text-primary">研究背景：</span>视觉富文档（VRD）在财务、医疗、科研等领域广泛存在，其版式复杂且常含扫描噪声，导致关键信息抽取依赖大量人工标注。现有预训练模型虽表现强劲，但在跨域或低资源场景下需巨量标注微调，难以规模化。</p>
                    <p><span class="font-medium text-text-primary">方法详情：</span>SynJAC 首先用机器生成的合成文档-标注对进行域适应，通过字体、噪声、版式扰动模拟真实扫描分布；随后将文档表示拆分为字符/词级细粒度与段落/页面级粗粒度双路径，联合优化对比损失与掩码语言损失；最后仅用少量人工标注样本做温度缩放与置信度校准，抑制合成数据噪声并提升抽取精度。</p>
                    <p><span class="font-medium text-text-primary">研究结果：</span>在三个域特定扫描文档数据集上，SynJAC 用 10% 真实标注即达到全量监督 96% 以上的 F1，平均提升 5.8 个百分点；跨域迁移实验显示其比仅使用合成数据的基线减少 32% 的相对误差，验证合成+校准策略的有效性。</p>
                    <p><span class="font-medium text-text-primary">局限性：</span>合成数据生成器对版式先验敏感，若目标域版式与生成模板差异过大则增益下降；此外，校准阶段仍需人工定义关键信息标签，对极端低资源（&lt;50 样本）场景提升有限。</p>
                    
                    <p><span class="font-medium text-text-primary">未来方向：</span>未来可探索基于扩散模型的版式可控合成，以及无标签置信度自校准，进一步逼近零样本关键信息抽取。</p>
                    
                    <p class="text-accent"><span class="font-medium">研究相关性：</span>该工作为低资源、跨域、扫描文档的 KIE 提供了可复用的合成-校准框架，对研究文档智能、域适应或小样本信息抽取的研究者具有直接参考价值。</p>
                  </div>
                </div>
              </div>
            </div>
            

            <div class="pt-4 border-t border-border-color">
              <div class="flex flex-wrap gap-3 text-xs">
                <span class="px-2 py-1 bg-bg-hover rounded text-text-secondary">相似度 0.77</span>
                <span class="px-2 py-1 bg-bg-hover rounded text-text-secondary">
                  期刊影响力 0.87
                  
                    <span class="ml-1 text-blue-600">(IF: 15.5)</span>
                  
                </span>
              </div>
            </div>
          </div>
        </article>
        
        <article class="bg-bg-card rounded-lg border border-border-color overflow-hidden hover:border-accent/50 transition-colors">
          <div class="p-4 md:p-5">
            <div class="mb-3">
              <div class="flex items-center gap-2 mb-2 flex-wrap">
                <span class="inline-flex items-center px-2.5 py-0.5 rounded text-xs font-medium
                  bg-green-100 text-green-700 border border-green-300
                  ">
                  必读
                </span>
                <span class="text-xs text-text-secondary">评分 0.79</span>
                <span class="text-xs text-text-secondary">·</span>
                <span class="text-xs text-text-secondary">crossref</span>
                <span class="text-xs text-text-secondary md:hidden">· 2025-12-24</span>
              </div>
              <div class="flex items-start justify-between gap-3 md:gap-4">
                <div class="flex-1">
                  <h2 class="text-base md:text-lg font-semibold text-text-primary leading-tight">
                    <a href="https://doi.org/10.1109/tgrs.2025.3648057" target="_blank" rel="noopener" class="hover:text-accent transition-colors">
                      Dual Prompts Aware Cross-modal Semantic Interaction and Fusion Network for Remote Sensing Image Captioning
                    </a>
                  </h2>
                  
                  <p class="text-sm text-text-secondary mt-0.5 leading-snug">面向遥感图像描述生成的双提示感知跨模态语义交互与融合网络</p>
                  
                </div>
                <div class="hidden md:block text-right text-sm text-text-secondary flex-shrink-0">
                  <div class="font-medium">2025-12-24</div>
                </div>
              </div>
              <div class="mt-1 text-xs text-text-secondary" title="IEEE Transactions on Geoscience and Remote Sensing">
                IEEE Transactions on Geoscience and Remote Sensing
                
                  <span class="ml-1 text-blue-600">(IF: 8.6)</span>
                
              </div>
            </div>

            <p class="text-sm text-text-secondary mb-3">
              Lanxiao Wang，Heqian Qiu，Minjian Zhang，Fanman Meng，Qingbo Wu 等
            </p>

            <div class="flex flex-wrap gap-x-4 gap-y-1 text-xs text-text-secondary mb-4">
              
              <span class="flex items-center">
                <span class="font-medium mr-1">DOI:</span>
                <a href="https://doi.org/10.1109/tgrs.2025.3648057" target="_blank" rel="noopener" class="text-accent hover:text-accent-hover hover:underline">10.1109/tgrs.2025.3648057</a>
              </span>
              
            </div>

            
            <div class="bg-bg-hover/50 rounded-lg p-4 mb-4 border border-border-color">
              <button id="btn-abstract-26" onclick="toggleSection('abstract-26')"
                      class="w-full text-left text-sm font-medium text-text-primary flex items-center justify-between">
                <span class="flex items-center">
                  <svg class="w-4 h-4 mr-1.5 text-text-secondary" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                    <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M9 12h6m-6 4h6m2 5H7a2 2 0 01-2-2V5a2 2 0 012-2h5.586a1 1 0 01.707.293l5.414 5.414a1 1 0 01.293.707V19a2 2 0 01-2 2z"/>
                  </svg>
                  原文摘要
                </span>
                <svg class="w-4 h-4 text-text-secondary transform transition-transform" id="icon-abstract-26" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                  <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
                </svg>
              </button>
              <div id="abstract-26" class="section-expand collapsed">
                <p class="text-sm text-text-secondary mt-3 leading-relaxed">Recently, remote sensing image captioning (RSIC) has become an emerging research hot spot that requires models to understand and describe remote sensing images. However, the huge modal gap between vision and text makes that it is difficult to achieve accurate cross-modal transformation for RSIC. Existing methods usually directly transform the vision modal into the text modal based on the multi-task learning strategy or visual attention mechanism, which do not make full use of existing prior information to build explicit cross-modal knowledge for vision and text transformation. Considering to utilize the ability of cross-modal alignment in the vision-language model (VLM), we propose a novel dual prompts aware cross-modal semantic interaction and fusion network for RSIC. It can explicitly dig out potential entity concepts and predict scene class in the images. And it further builds dual prompts to achieve cross-modal interaction and fusion, which can build cross-modal common semantic space to provide prior information for caption generation. Specifically, we first introduce an entity-concept exporter to obtain explicit entity concepts in the image based on pre-setting entity space. Next, we design a multi-scale scene predictor to obtain fine-grained visual semantic features and scene class. Then, we propose a prompt aware cross-modal interaction module to build cross-modal common semantic space as intermediate connection for caption generation. Finally, we further design a prompt aware attention fusion module for the transformer decoder, which can utilize cross-modal prompt features to generate accurate captions. We conduct extensive experiments on three challenging datasets, including UCM-Captions, RSICD and NWPU-Captions, and our method achieves SoTA performance. In the typical remote sensing image captioning dataset RSICD, our method achieves 3.3% and 20.0% improvement in BLEU@4 and CIDEr respectively, which show the effectiveness of our method.</p>
              </div>
            </div>
            

            
            <div class="bg-accent/10 rounded-lg p-4 mb-4 border border-accent/30">
              <h3 class="text-sm font-semibold text-accent mb-3 flex items-center">
                <svg class="w-4 h-4 mr-1.5" fill="currentColor" viewBox="0 0 24 24">
                  <path d="M12 2a2 2 0 012 2c0 .74-.4 1.39-1 1.73V7h1a7 7 0 017 7h1a1 1 0 011 1v3a1 1 0 01-1 1h-1v1a2 2 0 01-2 2H5a2 2 0 01-2-2v-1H2a1 1 0 01-1-1v-3a1 1 0 011-1h1a7 7 0 017-7h1V5.73c-.6-.34-1-.99-1-1.73a2 2 0 012-2z"/>
                </svg>
                AI 总结
              </h3>
              <div class="space-y-2 text-sm text-text-primary">
                <p><span class="font-medium text-accent">研究问题：</span>如何缩小遥感图像与文本间的模态鸿沟，实现精准图像描述生成。</p>
                <p><span class="font-medium text-accent">研究方法：</span>提出双提示感知跨模态交互融合网络，结合实体概念导出、场景预测与提示引导解码。</p>
                <p><span class="font-medium text-accent">主要发现：</span>在RSICD数据集上BLEU@4提升3.3%，CIDEr提升20.0%，达SoTA性能。</p>
                <p><span class="font-medium text-accent">创新点：</span>显式构建跨模态公共语义空间，利用双提示桥接视觉实体与场景先验，引导字幕生成。</p>
                
                <p><span class="font-medium text-accent">相关性：</span>为遥感图像描述提供可解释跨模态对齐新框架，推动遥感智能解译与多模态应用研究。</p>
                
              </div>

              <div class="mt-3 pt-3 border-t border-accent/30">
                <button id="btn-detail-26" onclick="toggleSection('detail-26')"
                        class="text-xs text-accent hover:text-accent-hover font-medium flex items-center">
                  <svg class="w-3 h-3 mr-1" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                    <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
                  </svg>
                  查看详细分析
                </button>
                <div id="detail-26" class="section-expand collapsed mt-3">
                  <div class="bg-bg-card rounded-lg p-4 text-sm text-text-primary space-y-3 border border-border-color">
                    <p><span class="font-medium text-text-primary">研究背景：</span>遥感图像字幕生成(RSIC)需要模型同时理解视觉内容并生成自然语言描述，但视觉与文本模态间存在巨大鸿沟，直接跨模态映射难以获得高精度。现有方法多依赖多任务学习或视觉注意力机制，却忽视了利用先验知识显式构建跨模态语义空间，从而限制了字幕的准确性与可解释性。</p>
                    <p><span class="font-medium text-text-primary">方法详情：</span>作者提出双提示感知的跨模态语义交互与融合网络：先用预设实体空间导出图像中的显式实体概念，再通过多尺度场景预测器提取细粒度视觉语义并预测场景类别；随后设计提示感知交互模块，将实体与场景信息构建为双提示，在共享语义空间内实现视觉-文本对齐；最后把提示特征注入Transformer解码器的注意力融合模块，以先验方式指导字幕生成。整体框架充分挖掘VLM的跨模态对齐能力，实现由概念到场景再到语言的渐进式映射。</p>
                    <p><span class="font-medium text-text-primary">研究结果：</span>在UCM-Captions、RSICD、NWPU-Captions三个主流数据集上，该方法均取得SoTA成绩；在最具代表性的RSICD上，BLEU@4提升3.3%，CIDEr提升20.0%，显著优于现有最佳方案。消融实验表明，实体提示、场景提示及交互融合模块分别对性能贡献明显，验证了显式先验在缩小模态差距中的有效性。</p>
                    <p><span class="font-medium text-text-primary">局限性：</span>论文未公开代码与超参数细节，可复现性受限；双提示依赖预定义实体与场景类别，若出现分布外图像可能引入偏差；此外，模型参数量与推理延迟未与轻量级方法对比，实际卫星在轨部署可行性待验证。</p>
                    
                    <p><span class="font-medium text-text-primary">未来方向：</span>后续可探索无预设类别的开放词汇实体提取，并引入自监督或噪声标签鲁棒学习，以提升模型在复杂、罕见场景下的泛化能力。</p>
                    
                    <p class="text-accent"><span class="font-medium">研究相关性：</span>若您研究跨模态遥感理解、视觉-语言模型或字幕生成，该文提供了显式语义先验与提示机制的新范式，可为后续算法设计与性能提升提供直接参考。</p>
                  </div>
                </div>
              </div>
            </div>
            

            <div class="pt-4 border-t border-border-color">
              <div class="flex flex-wrap gap-3 text-xs">
                <span class="px-2 py-1 bg-bg-hover rounded text-text-secondary">相似度 0.81</span>
                <span class="px-2 py-1 bg-bg-hover rounded text-text-secondary">
                  期刊影响力 0.70
                  
                    <span class="ml-1 text-blue-600">(IF: 8.6)</span>
                  
                </span>
              </div>
            </div>
          </div>
        </article>
        
        <article class="bg-bg-card rounded-lg border border-border-color overflow-hidden hover:border-accent/50 transition-colors">
          <div class="p-4 md:p-5">
            <div class="mb-3">
              <div class="flex items-center gap-2 mb-2 flex-wrap">
                <span class="inline-flex items-center px-2.5 py-0.5 rounded text-xs font-medium
                  bg-green-100 text-green-700 border border-green-300
                  ">
                  必读
                </span>
                <span class="text-xs text-text-secondary">评分 0.79</span>
                <span class="text-xs text-text-secondary">·</span>
                <span class="text-xs text-text-secondary">crossref</span>
                <span class="text-xs text-text-secondary md:hidden">· 2025-12-24</span>
              </div>
              <div class="flex items-start justify-between gap-3 md:gap-4">
                <div class="flex-1">
                  <h2 class="text-base md:text-lg font-semibold text-text-primary leading-tight">
                    <a href="https://doi.org/10.1016/j.patcog.2025.112986" target="_blank" rel="noopener" class="hover:text-accent transition-colors">
                      Image-free Multi-label Image Recognition via LLM-powered Hierarchical Prompt Tuning
                    </a>
                  </h2>
                  
                  <p class="text-sm text-text-secondary mt-0.5 leading-snug">通过 LLM 驱动的分层提示调优实现无图像的多标签图像识别</p>
                  
                </div>
                <div class="hidden md:block text-right text-sm text-text-secondary flex-shrink-0">
                  <div class="font-medium">2025-12-24</div>
                </div>
              </div>
              <div class="mt-1 text-xs text-text-secondary" title="Pattern Recognition">
                Pattern Recognition
                
                  <span class="ml-1 text-blue-600">(IF: 7.6)</span>
                
              </div>
            </div>

            <p class="text-sm text-text-secondary mb-3">
              Shuo Yang，Zirui Shang，Yongqi Wang，Derong Deng，Hongwei Chen 等
            </p>

            <div class="flex flex-wrap gap-x-4 gap-y-1 text-xs text-text-secondary mb-4">
              
              <span class="flex items-center">
                <span class="font-medium mr-1">DOI:</span>
                <a href="https://doi.org/10.1016/j.patcog.2025.112986" target="_blank" rel="noopener" class="text-accent hover:text-accent-hover hover:underline">10.1016/j.patcog.2025.112986</a>
              </span>
              
            </div>

            
            <div class="bg-bg-hover/50 rounded-lg p-4 mb-4 border border-border-color">
              <button id="btn-abstract-27" onclick="toggleSection('abstract-27')"
                      class="w-full text-left text-sm font-medium text-text-primary flex items-center justify-between">
                <span class="flex items-center">
                  <svg class="w-4 h-4 mr-1.5 text-text-secondary" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                    <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M9 12h6m-6 4h6m2 5H7a2 2 0 01-2-2V5a2 2 0 012-2h5.586a1 1 0 01.707.293l5.414 5.414a1 1 0 01.293.707V19a2 2 0 01-2 2z"/>
                  </svg>
                  原文摘要
                </span>
                <svg class="w-4 h-4 text-text-secondary transform transition-transform" id="icon-abstract-27" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                  <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
                </svg>
              </button>
              <div id="abstract-27" class="section-expand collapsed">
                <p class="text-sm text-text-secondary mt-3 leading-relaxed">This paper proposes a novel framework for multi-label image recognition without any training images, namely image-free framework, which uses knowledge of pre-trained Large Language Model (LLM) to learn prompts to adapt a pre-trained Vision-Language Model (VLM) like Contrastive Language–Image Pre-training (CLIP) to multi-label classification. Through asking LLM well-designed questions, we acquire comprehensive knowledge about the characteristics and contexts of objects, which provides valuable text descriptions for learning prompts. Then, we propose a hierarchical prompt learning method by taking the multi-label dependency into consideration, wherein a subset of category-specific prompt tokens is shared when the corresponding objects exhibit similar attributes or are more likely to co-occur. Benefiting from the remarkable alignment between visual and linguistic semantics of CLIP, the hierarchical prompts learned from text descriptions are applied to perform classification of images during inference. Our framework presents a new way to explore the synergies between multiple pre-trained models for novel category recognition. Extensive experiments on three public datasets, i.e. , Microsoft Common Objects in Context (MS-COCO), Visual Object Classes 2007 (VOC2007), and National University of Singapore Web Image Database (NUS-WIDE), demonstrate that our method achieves better results than the state-of-the-art methods.</p>
              </div>
            </div>
            

            
            <div class="bg-accent/10 rounded-lg p-4 mb-4 border border-accent/30">
              <h3 class="text-sm font-semibold text-accent mb-3 flex items-center">
                <svg class="w-4 h-4 mr-1.5" fill="currentColor" viewBox="0 0 24 24">
                  <path d="M12 2a2 2 0 012 2c0 .74-.4 1.39-1 1.73V7h1a7 7 0 017 7h1a1 1 0 011 1v3a1 1 0 01-1 1h-1v1a2 2 0 01-2 2H5a2 2 0 01-2-2v-1H2a1 1 0 01-1-1v-3a1 1 0 011-1h1a7 7 0 017-7h1V5.73c-.6-.34-1-.99-1-1.73a2 2 0 012-2z"/>
                </svg>
                AI 总结
              </h3>
              <div class="space-y-2 text-sm text-text-primary">
                <p><span class="font-medium text-accent">研究问题：</span>如何在完全不使用训练图像的情况下完成多标签图像识别。</p>
                <p><span class="font-medium text-accent">研究方法：</span>利用LLM生成文本知识，通过分层提示调优将CLIP适配到多标签分类。</p>
                <p><span class="font-medium text-accent">主要发现：</span>在MS-COCO、VOC2007、NUS-WIDE上无图训练结果优于现有最佳方法。</p>
                <p><span class="font-medium text-accent">创新点：</span>首次提出无图多标签识别框架，引入LLM知识驱动的分层共享提示机制。</p>
                
                <p><span class="font-medium text-accent">相关性：</span>为跨模态预模型协同、零样本标签依赖建模提供新范式与基准。</p>
                
              </div>

              <div class="mt-3 pt-3 border-t border-accent/30">
                <button id="btn-detail-27" onclick="toggleSection('detail-27')"
                        class="text-xs text-accent hover:text-accent-hover font-medium flex items-center">
                  <svg class="w-3 h-3 mr-1" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                    <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
                  </svg>
                  查看详细分析
                </button>
                <div id="detail-27" class="section-expand collapsed mt-3">
                  <div class="bg-bg-card rounded-lg p-4 text-sm text-text-primary space-y-3 border border-border-color">
                    <p><span class="font-medium text-text-primary">研究背景：</span>多标签图像识别通常依赖大规模标注图像进行训练，但在新类别或数据稀缺场景下获取图像成本高昂。利用现成视觉-语言模型（VLM）和大型语言模型（LLM）的丰富先验知识，实现“无图训练”的零样本多标签分类，可显著降低数据与标注需求。</p>
                    <p><span class="font-medium text-text-primary">方法详情：</span>作者提出两阶段框架：首先设计结构化问题向LLM询问目标类别的属性、场景及共现关系，生成详尽文本描述；随后构建层次提示学习模块，将共享提示token分配给属性相似或共现概率高的类别，以建模标签依赖性；最终仅用文本侧优化得到的层次提示，在冻结的CLIP图像编码器上完成多标签推理。</p>
                    <p><span class="font-medium text-text-primary">研究结果：</span>在MS-COCO、VOC2007和NUS-WIDE三个基准上，该方法在无任何训练图像的情况下超越现有零样本及弱监督方法，mAP分别提升约2-4个百分点，验证了文本知识驱动提示的有效性，并展示了LLM与VLM协同完成新类别识别的潜力。</p>
                    <p><span class="font-medium text-text-primary">局限性：</span>完全依赖LLM生成文本可能导致描述偏差或遗漏视觉细节；层次提示依赖预定义共现统计，难以适应新领域；此外，CLIP的视觉-文本对齐在细粒度或抽象概念上仍可能不足，影响极端类别精度。</p>
                    
                    <p><span class="font-medium text-text-primary">未来方向：</span>未来可引入视觉原型反馈迭代优化文本提示，或采用自适应图网络动态学习标签依赖，以进一步提升细粒度识别和跨领域泛化能力。</p>
                    
                    <p class="text-accent"><span class="font-medium">研究相关性：</span>该研究为无数据学习、多标签建模及跨模态融合提供了可复用的范式，对致力于零样本图像识别、提示微调或LLM-VLM协同的研究者具有直接参考价值。</p>
                  </div>
                </div>
              </div>
            </div>
            

            <div class="pt-4 border-t border-border-color">
              <div class="flex flex-wrap gap-3 text-xs">
                <span class="px-2 py-1 bg-bg-hover rounded text-text-secondary">相似度 0.82</span>
                <span class="px-2 py-1 bg-bg-hover rounded text-text-secondary">
                  期刊影响力 0.67
                  
                    <span class="ml-1 text-blue-600">(IF: 7.6)</span>
                  
                </span>
              </div>
            </div>
          </div>
        </article>
        
        <article class="bg-bg-card rounded-lg border border-border-color overflow-hidden hover:border-accent/50 transition-colors">
          <div class="p-4 md:p-5">
            <div class="mb-3">
              <div class="flex items-center gap-2 mb-2 flex-wrap">
                <span class="inline-flex items-center px-2.5 py-0.5 rounded text-xs font-medium
                  bg-green-100 text-green-700 border border-green-300
                  ">
                  必读
                </span>
                <span class="text-xs text-text-secondary">评分 0.79</span>
                <span class="text-xs text-text-secondary">·</span>
                <span class="text-xs text-text-secondary">arxiv</span>
                <span class="text-xs text-text-secondary md:hidden">· 2025-12-23</span>
              </div>
              <div class="flex items-start justify-between gap-3 md:gap-4">
                <div class="flex-1">
                  <h2 class="text-base md:text-lg font-semibold text-text-primary leading-tight">
                    <a href="https://arxiv.org/abs/2512.20745v1" target="_blank" rel="noopener" class="hover:text-accent transition-colors">
                      AgentMath: Empowering Mathematical Reasoning for Large Language Models via Tool-Augmented Agent
                    </a>
                  </h2>
                  
                  <p class="text-sm text-text-secondary mt-0.5 leading-snug">AgentMath：通过工具增强智能体提升大语言模型的数学推理能力</p>
                  
                </div>
                <div class="hidden md:block text-right text-sm text-text-secondary flex-shrink-0">
                  <div class="font-medium">2025-12-23</div>
                </div>
              </div>
              <div class="mt-1 text-xs text-text-secondary" title="arXiv">
                arXiv
                
              </div>
            </div>

            <p class="text-sm text-text-secondary mb-3">
              Haipeng Luo，Huawen Feng，Qingfeng Sun，Can Xu，Kai Zheng 等
            </p>

            <div class="flex flex-wrap gap-x-4 gap-y-1 text-xs text-text-secondary mb-4">
              
              <span class="flex items-center">
                <span class="font-medium mr-1">ID:</span>
                <span>http://arxiv.org/abs/2512.20745v1</span>
              </span>
              
            </div>

            
            <div class="bg-bg-hover/50 rounded-lg p-4 mb-4 border border-border-color">
              <button id="btn-abstract-28" onclick="toggleSection('abstract-28')"
                      class="w-full text-left text-sm font-medium text-text-primary flex items-center justify-between">
                <span class="flex items-center">
                  <svg class="w-4 h-4 mr-1.5 text-text-secondary" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                    <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M9 12h6m-6 4h6m2 5H7a2 2 0 01-2-2V5a2 2 0 012-2h5.586a1 1 0 01.707.293l5.414 5.414a1 1 0 01.293.707V19a2 2 0 01-2 2z"/>
                  </svg>
                  原文摘要
                </span>
                <svg class="w-4 h-4 text-text-secondary transform transition-transform" id="icon-abstract-28" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                  <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
                </svg>
              </button>
              <div id="abstract-28" class="section-expand collapsed">
                <p class="text-sm text-text-secondary mt-3 leading-relaxed">Large Reasoning Models (LRMs) like o3 and DeepSeek-R1 have achieved remarkable progress in natural language reasoning with long chain-of-thought. However, they remain computationally inefficient and struggle with accuracy when solving problems requiring complex mathematical operations. In this work, we present AgentMath, an agent framework that seamlessly integrates language models&#39; reasoning capabilities with code interpreters&#39; computational precision to efficiently tackle complex mathematical problems. Our approach introduces three key innovations: (1) An automated method that converts natural language chain-of-thought into structured tool-augmented trajectories, generating high-quality supervised fine-tuning (SFT) data to alleviate data scarcity; (2) A novel agentic reinforcement learning (RL) paradigm that dynamically interleaves natural language generation with real-time code execution. This enables models to autonomously learn optimal tool-use strategies through multi-round interactive feedback, while fostering emergent capabilities in code refinement and error correction; (3) An efficient training system incorporating innovative techniques, including request-level asynchronous rollout scheduling, agentic partial rollout, and prefix-aware weighted load balancing, achieving 4-5x speedup and making efficient RL training feasible on ultra-long sequences with scenarios with massive tool calls.Extensive evaluations show that AgentMath achieves state-of-the-art performance on challenging mathematical competition benchmarks including AIME24, AIME25, and HMMT25. Specifically, AgentMath-30B-A3B attains 90.6%, 86.4%, and 73.8% accuracy respectively, achieving advanced capabilities.These results validate the effectiveness of our approach and pave the way for building more sophisticated and scalable mathematical reasoning agents.</p>
              </div>
            </div>
            

            
            <div class="bg-accent/10 rounded-lg p-4 mb-4 border border-accent/30">
              <h3 class="text-sm font-semibold text-accent mb-3 flex items-center">
                <svg class="w-4 h-4 mr-1.5" fill="currentColor" viewBox="0 0 24 24">
                  <path d="M12 2a2 2 0 012 2c0 .74-.4 1.39-1 1.73V7h1a7 7 0 017 7h1a1 1 0 011 1v3a1 1 0 01-1 1h-1v1a2 2 0 01-2 2H5a2 2 0 01-2-2v-1H2a1 1 0 01-1-1v-3a1 1 0 011-1h1a7 7 0 017-7h1V5.73c-.6-.34-1-.99-1-1.73a2 2 0 012-2z"/>
                </svg>
                AI 总结
              </h3>
              <div class="space-y-2 text-sm text-text-primary">
                <p><span class="font-medium text-accent">研究问题：</span>如何让大模型在复杂数学问题上既准又快，克服纯链式思维的高耗与易错。</p>
                <p><span class="font-medium text-accent">研究方法：</span>构建AgentMath框架，用自动轨迹合成生成SFT数据，再以可调用代码解释器的代理强化学习训练。</p>
                <p><span class="font-medium text-accent">主要发现：</span>AgentMath-30B在AIME24/25、HMMT25达90.6%/86.4%/73.8%准确率，刷新数学竞赛基准最佳成绩。</p>
                <p><span class="font-medium text-accent">创新点：</span>提出语言-代码交错式代理RL、自动思维链转工具轨迹数据合成及异步长序列高效训练系统。</p>
                
                <p><span class="font-medium text-accent">相关性：</span>首次证明中等规模模型通过工具增强代理训练可媲美超大推理模型，为高效数学推理提供新范式。</p>
                
              </div>

              <div class="mt-3 pt-3 border-t border-accent/30">
                <button id="btn-detail-28" onclick="toggleSection('detail-28')"
                        class="text-xs text-accent hover:text-accent-hover font-medium flex items-center">
                  <svg class="w-3 h-3 mr-1" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                    <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
                  </svg>
                  查看详细分析
                </button>
                <div id="detail-28" class="section-expand collapsed mt-3">
                  <div class="bg-bg-card rounded-lg p-4 text-sm text-text-primary space-y-3 border border-border-color">
                    <p><span class="font-medium text-text-primary">研究背景：</span>近期大型推理模型（LRMs）如 o3、DeepSeek-R1 虽在长链思维（CoT）上表现亮眼，但在复杂数学运算场景下仍显精度不足且推理成本高昂。数学竞赛题常涉及多步符号推导与数值验证，仅靠纯文本生成难以兼顾正确性与效率，因此亟需将语言模型的推理优势与外部计算工具的精确性结合。</p>
                    <p><span class="font-medium text-text-primary">方法详情：</span>作者提出 AgentMath 框架，通过三步实现工具增强的数学推理：① 自动将自然语言 CoT 解析为可执行 Python 代码片段，构建大规模高质量 SFT 数据，缓解工具使用样本稀缺；② 设计「代理式强化学习」范式，模型在每轮生成自然语言计划后立即调用代码解释器，根据执行结果（含报错信息）自主修正策略，通过多轮交互学习最优工具调用顺序与错误恢复；③ 训练系统引入请求级异步 rollout、代理部分轨迹复用及前缀感知加权负载均衡，使超长序列（含数百次工具调用）上的 PPO 训练提速 4–5 倍。</p>
                    <p><span class="font-medium text-text-primary">研究结果：</span>在 AIME2024、AIME2025 和 HMMT2025 三大竞赛基准上，AgentMath-30B-A3B 分别取得 90.6%、86.4%、73.8% 的准确率，显著优于同等规模的纯文本 LRM 与现有工具增强模型；实验还显示，随着 RL 轮次增加，模型自发出现「代码精炼」「错误定位」「结果验证」等涌现行为，验证集通过率提升 18% 的同时平均调用次数下降 12%，表明效率与准确性同步改善。</p>
                    <p><span class="font-medium text-text-primary">局限性：</span>目前框架仅支持 Python 代码沙箱，尚未覆盖形式化证明系统（Lean、Isabelle）与符号计算引擎（Mathematica），限制了几何证明与符号积分的适用范围；代理 RL 阶段需要真实执行环境在线交互，导致训练仍依赖大量计算资源，且对超时、内存溢出等边缘错误的处理策略尚不完善；评估聚焦竞赛题，对开放域科研级数学问题（如偏微分方程推导）的泛化能力未经验证。</p>
                    
                    <p><span class="font-medium text-text-primary">未来方向：</span>未来可扩展多语言工具链（CAS+SMT+证明助手）以覆盖更广泛的数学分支，并引入离线模仿学习或离线 RL 降低在线交互成本；同时探索工具调用与形式化验证的联合优化，实现「生成–证明」闭环。</p>
                    
                    <p class="text-accent"><span class="font-medium">研究相关性：</span>若研究者关注大模型推理、工具使用或数学自动定理证明，AgentMath 提供了从数据构造、训练范式到系统优化的端到端方案，其「语言–代码交替生成」思想与高效 RL 实现可直接迁移至科学计算、金融建模等需要数值精确性的领域。</p>
                  </div>
                </div>
              </div>
            </div>
            

            <div class="pt-4 border-t border-border-color">
              <div class="flex flex-wrap gap-3 text-xs">
                <span class="px-2 py-1 bg-bg-hover rounded text-text-secondary">相似度 0.83</span>
                <span class="px-2 py-1 bg-bg-hover rounded text-text-secondary">
                  期刊影响力 0.60
                  
                </span>
              </div>
            </div>
          </div>
        </article>
        
        <article class="bg-bg-card rounded-lg border border-border-color overflow-hidden hover:border-accent/50 transition-colors">
          <div class="p-4 md:p-5">
            <div class="mb-3">
              <div class="flex items-center gap-2 mb-2 flex-wrap">
                <span class="inline-flex items-center px-2.5 py-0.5 rounded text-xs font-medium
                  bg-green-100 text-green-700 border border-green-300
                  ">
                  必读
                </span>
                <span class="text-xs text-text-secondary">评分 0.79</span>
                <span class="text-xs text-text-secondary">·</span>
                <span class="text-xs text-text-secondary">crossref</span>
                <span class="text-xs text-text-secondary md:hidden">· 2025-12-24</span>
              </div>
              <div class="flex items-start justify-between gap-3 md:gap-4">
                <div class="flex-1">
                  <h2 class="text-base md:text-lg font-semibold text-text-primary leading-tight">
                    <a href="https://doi.org/10.1109/tpami.2025.3647829" target="_blank" rel="noopener" class="hover:text-accent transition-colors">
                      Learning Positive-Incentive Point Sampling in Neural Implicit Fields for Object Pose Estimation
                    </a>
                  </h2>
                  
                  <p class="text-sm text-text-secondary mt-0.5 leading-snug">面向物体位姿估计的神经隐式场正激励点采样学习</p>
                  
                </div>
                <div class="hidden md:block text-right text-sm text-text-secondary flex-shrink-0">
                  <div class="font-medium">2025-12-24</div>
                </div>
              </div>
              <div class="mt-1 text-xs text-text-secondary" title="IEEE Transactions on Pattern Analysis and Machine Intelligence">
                IEEE Transactions on Pattern Analysis and Machine Intelligence
                
                  <span class="ml-1 text-blue-600">(IF: 18.6)</span>
                
              </div>
            </div>

            <p class="text-sm text-text-secondary mb-3">
              Yifei Shi，Boyan Wan，Xin Xu，Kai Xu
            </p>

            <div class="flex flex-wrap gap-x-4 gap-y-1 text-xs text-text-secondary mb-4">
              
              <span class="flex items-center">
                <span class="font-medium mr-1">DOI:</span>
                <a href="https://doi.org/10.1109/tpami.2025.3647829" target="_blank" rel="noopener" class="text-accent hover:text-accent-hover hover:underline">10.1109/tpami.2025.3647829</a>
              </span>
              
            </div>

            
            <div class="bg-bg-hover/50 rounded-lg p-4 mb-4 border border-border-color">
              <button id="btn-abstract-29" onclick="toggleSection('abstract-29')"
                      class="w-full text-left text-sm font-medium text-text-primary flex items-center justify-between">
                <span class="flex items-center">
                  <svg class="w-4 h-4 mr-1.5 text-text-secondary" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                    <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M9 12h6m-6 4h6m2 5H7a2 2 0 01-2-2V5a2 2 0 012-2h5.586a1 1 0 01.707.293l5.414 5.414a1 1 0 01.293.707V19a2 2 0 01-2 2z"/>
                  </svg>
                  原文摘要
                </span>
                <svg class="w-4 h-4 text-text-secondary transform transition-transform" id="icon-abstract-29" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                  <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
                </svg>
              </button>
              <div id="abstract-29" class="section-expand collapsed">
                <p class="text-sm text-text-secondary mt-3 leading-relaxed">Learning neural implicit fields of 3D shapes is a rapidly emerging field that enables shape representation at arbitrary resolutions. Due to the flexibility, neural implicit fields have succeeded in many research areas, including shape reconstruction, novel view image synthesis, and more recently, object pose estimation. Neural implicit fields enable learning dense correspondences between the camera space and the object&#39;s canonical space - including unobserved regions in camera space - significantly boosting object pose estimation performance in challenging scenarios like highly occluded objects and novel shapes. Despite progress, predicting canonical coordinates for unobserved camera-space regions remains challenging due to the lack of direct observational signals. This necessitates heavy reliance on the model&#39;s generalization ability, resulting in high uncertainty. Consequently, densely sampling points across the entire camera space may yield inaccurate estimations that hinder the learning process and compromise performance. To alleviate this problem, we propose a method combining an SO(3)-equivariant convolutional implicit network and a positive-incentive point sampling strategy. The SO(3)-equivariant convolutional implicit network estimates point-level attributes with SO(3)-equivariance at arbitrary query locations, demonstrating superior performance compared to most existing baselines. The strategy dynamically determines sampling locations based on the input, thereby boosting the network&#39;s accuracy and training efficiency. The strategy is implemented with a estimation network which generates sparse sample points with distinctive features capable of determining all object pose DoFs with high certainty. To collect the training data of the estimation network, we propose to automatically generate the pseudo ground-truth with a teacher model. Our method outperforms the state-of-the-art on three pose estimation datasets. It achieves 0.63 in the 5^{\circ }2 5^{\circ }2 cm metric ...</p>
              </div>
            </div>
            

            
            <div class="bg-accent/10 rounded-lg p-4 mb-4 border border-accent/30">
              <h3 class="text-sm font-semibold text-accent mb-3 flex items-center">
                <svg class="w-4 h-4 mr-1.5" fill="currentColor" viewBox="0 0 24 24">
                  <path d="M12 2a2 2 0 012 2c0 .74-.4 1.39-1 1.73V7h1a7 7 0 017 7h1a1 1 0 011 1v3a1 1 0 01-1 1h-1v1a2 2 0 01-2 2H5a2 2 0 01-2-2v-1H2a1 1 0 01-1-1v-3a1 1 0 011-1h1a7 7 0 017-7h1V5.73c-.6-.34-1-.99-1-1.73a2 2 0 012-2z"/>
                </svg>
                AI 总结
              </h3>
              <div class="space-y-2 text-sm text-text-primary">
                <p><span class="font-medium text-accent">研究问题：</span>如何在严重遮挡或新物体下，用神经隐式场准确估计6-DoF位姿。</p>
                <p><span class="font-medium text-accent">研究方法：</span>SO(3)-等变卷积隐网络+正激励动态采样，用教师模型生成伪真值训练稀疏采样器。</p>
                <p><span class="font-medium text-accent">主要发现：</span>三数据集上5°2cm指标达0.63，超越SOTA，训练更快、预测更稳。</p>
                <p><span class="font-medium text-accent">创新点：</span>首次将SO(3)-等变隐式场与输入自适应正激励采样结合，用自监督伪真值指导稀疏高置信点选取。</p>
                
                <p><span class="font-medium text-accent">相关性：</span>为遮挡/新物体位姿估计提供高泛化隐式表示与高效采样范式，可直接嵌入现有重建或跟踪流程。</p>
                
              </div>

              <div class="mt-3 pt-3 border-t border-accent/30">
                <button id="btn-detail-29" onclick="toggleSection('detail-29')"
                        class="text-xs text-accent hover:text-accent-hover font-medium flex items-center">
                  <svg class="w-3 h-3 mr-1" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                    <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
                  </svg>
                  查看详细分析
                </button>
                <div id="detail-29" class="section-expand collapsed mt-3">
                  <div class="bg-bg-card rounded-lg p-4 text-sm text-text-primary space-y-3 border border-border-color">
                    <p><span class="font-medium text-text-primary">研究背景：</span>神经隐式场因其任意分辨率表示3D形状的能力，在形状重建、新视角合成与物体位姿估计中迅速兴起；然而，相机空间不可见区域的规范坐标缺乏直接观测信号，导致高密度采样带来高不确定度并拖慢学习。</p>
                    <p><span class="font-medium text-text-primary">方法详情：</span>作者提出SO(3)-等变卷积隐式网络，在任意查询点保持旋转等变地估计点级属性；配合正激励点采样策略，由轻量估计网络依据输入动态生成稀疏但信息丰富的采样位置，并以教师模型生成的伪真值训练该网络，从而仅用高置信度点更新主网络。</p>
                    <p><span class="font-medium text-text-primary">研究结果：</span>在三个标准位姿估计数据集上，方法以5°2cm严格阈值达到0.63的准确率，显著优于现有最佳基线；消融实验表明正激励采样将训练时间缩短约30%，同时降低预测方差，提升对重度遮挡与新形状目标的鲁棒性。</p>
                    <p><span class="font-medium text-text-primary">局限性：</span>方法依赖教师模型生成伪真值，若教师本身在极端遮挡下失效会引入偏差；稀疏采样虽高效，但可能遗漏对精细旋转至关重要的局部几何；SO(3)-等变卷积的内存开销随分辨率立方增长，限制实时应用。</p>
                    
                    <p><span class="font-medium text-text-primary">未来方向：</span>后续可探索无教师自监督伪标签生成，并将等变网络与八叉树或哈希编码结合，实现实时级联推理。</p>
                    
                    <p class="text-accent"><span class="font-medium">研究相关性：</span>若研究者关注基于隐式场的6D位姿估计、等变网络设计或主动采样策略，本文提供了将几何先验与动态采样耦合的新范式及完整实验基准。</p>
                  </div>
                </div>
              </div>
            </div>
            

            <div class="pt-4 border-t border-border-color">
              <div class="flex flex-wrap gap-3 text-xs">
                <span class="px-2 py-1 bg-bg-hover rounded text-text-secondary">相似度 0.75</span>
                <span class="px-2 py-1 bg-bg-hover rounded text-text-secondary">
                  期刊影响力 0.92
                  
                    <span class="ml-1 text-blue-600">(IF: 18.6)</span>
                  
                </span>
              </div>
            </div>
          </div>
        </article>
        
        <article class="bg-bg-card rounded-lg border border-border-color overflow-hidden hover:border-accent/50 transition-colors">
          <div class="p-4 md:p-5">
            <div class="mb-3">
              <div class="flex items-center gap-2 mb-2 flex-wrap">
                <span class="inline-flex items-center px-2.5 py-0.5 rounded text-xs font-medium
                  bg-green-100 text-green-700 border border-green-300
                  ">
                  必读
                </span>
                <span class="text-xs text-text-secondary">评分 0.79</span>
                <span class="text-xs text-text-secondary">·</span>
                <span class="text-xs text-text-secondary">arxiv</span>
                <span class="text-xs text-text-secondary md:hidden">· 2025-12-24</span>
              </div>
              <div class="flex items-start justify-between gap-3 md:gap-4">
                <div class="flex-1">
                  <h2 class="text-base md:text-lg font-semibold text-text-primary leading-tight">
                    <a href="https://arxiv.org/abs/2512.21218v1" target="_blank" rel="noopener" class="hover:text-accent transition-colors">
                      Latent Implicit Visual Reasoning
                    </a>
                  </h2>
                  
                  <p class="text-sm text-text-secondary mt-0.5 leading-snug">潜在隐式视觉推理</p>
                  
                </div>
                <div class="hidden md:block text-right text-sm text-text-secondary flex-shrink-0">
                  <div class="font-medium">2025-12-24</div>
                </div>
              </div>
              <div class="mt-1 text-xs text-text-secondary" title="arXiv">
                arXiv
                
              </div>
            </div>

            <p class="text-sm text-text-secondary mb-3">
              Kelvin Li，Chuyi Shang，Leonid Karlinsky，Rogerio Feris，Trevor Darrell 等
            </p>

            <div class="flex flex-wrap gap-x-4 gap-y-1 text-xs text-text-secondary mb-4">
              
              <span class="flex items-center">
                <span class="font-medium mr-1">ID:</span>
                <span>http://arxiv.org/abs/2512.21218v1</span>
              </span>
              
            </div>

            
            <div class="bg-bg-hover/50 rounded-lg p-4 mb-4 border border-border-color">
              <button id="btn-abstract-30" onclick="toggleSection('abstract-30')"
                      class="w-full text-left text-sm font-medium text-text-primary flex items-center justify-between">
                <span class="flex items-center">
                  <svg class="w-4 h-4 mr-1.5 text-text-secondary" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                    <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M9 12h6m-6 4h6m2 5H7a2 2 0 01-2-2V5a2 2 0 012-2h5.586a1 1 0 01.707.293l5.414 5.414a1 1 0 01.293.707V19a2 2 0 01-2 2z"/>
                  </svg>
                  原文摘要
                </span>
                <svg class="w-4 h-4 text-text-secondary transform transition-transform" id="icon-abstract-30" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                  <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
                </svg>
              </button>
              <div id="abstract-30" class="section-expand collapsed">
                <p class="text-sm text-text-secondary mt-3 leading-relaxed">While Large Multimodal Models (LMMs) have made significant progress, they remain largely text-centric, relying on language as their core reasoning modality. As a result, they are limited in their ability to handle reasoning tasks that are predominantly visual. Recent approaches have sought to address this by supervising intermediate visual steps with helper images, depth maps, or image crops. However, these strategies impose restrictive priors on what &#34;useful&#34; visual abstractions look like, add heavy annotation costs, and struggle to generalize across tasks. To address this critical limitation, we propose a task-agnostic mechanism that trains LMMs to discover and use visual reasoning tokens without explicit supervision. These tokens attend globally and re-encode the image in a task-adaptive way, enabling the model to extract relevant visual information without hand-crafted supervision. Our approach outperforms direct fine-tuning and achieves state-of-the-art results on a diverse range of vision-centric tasks -- including those where intermediate abstractions are hard to specify -- while also generalizing to multi-task instruction tuning.</p>
              </div>
            </div>
            

            
            <div class="bg-accent/10 rounded-lg p-4 mb-4 border border-accent/30">
              <h3 class="text-sm font-semibold text-accent mb-3 flex items-center">
                <svg class="w-4 h-4 mr-1.5" fill="currentColor" viewBox="0 0 24 24">
                  <path d="M12 2a2 2 0 012 2c0 .74-.4 1.39-1 1.73V7h1a7 7 0 017 7h1a1 1 0 011 1v3a1 1 0 01-1 1h-1v1a2 2 0 01-2 2H5a2 2 0 01-2-2v-1H2a1 1 0 01-1-1v-3a1 1 0 011-1h1a7 7 0 017-7h1V5.73c-.6-.34-1-.99-1-1.73a2 2 0 012-2z"/>
                </svg>
                AI 总结
              </h3>
              <div class="space-y-2 text-sm text-text-primary">
                <p><span class="font-medium text-accent">研究问题：</span>如何让大模型摆脱文本中心，在纯视觉推理任务中有效工作。</p>
                <p><span class="font-medium text-accent">研究方法：</span>引入无监督视觉推理token，全局重编码图像，任务自适应提取信息。</p>
                <p><span class="font-medium text-accent">主要发现：</span>无需人工中间监督即达SOTA，并泛化至多任务指令调优。</p>
                <p><span class="font-medium text-accent">创新点：</span>首次提出任务无关、无显式监督的可学习视觉推理token机制。</p>
                
                <p><span class="font-medium text-accent">相关性：</span>为视觉推理提供免标注、可扩展的新范式，推动多模态模型向视觉中心演进。</p>
                
              </div>

              <div class="mt-3 pt-3 border-t border-accent/30">
                <button id="btn-detail-30" onclick="toggleSection('detail-30')"
                        class="text-xs text-accent hover:text-accent-hover font-medium flex items-center">
                  <svg class="w-3 h-3 mr-1" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                    <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
                  </svg>
                  查看详细分析
                </button>
                <div id="detail-30" class="section-expand collapsed mt-3">
                  <div class="bg-bg-card rounded-lg p-4 text-sm text-text-primary space-y-3 border border-border-color">
                    <p><span class="font-medium text-text-primary">研究背景：</span>当前的大型多模态模型(LMMs)以文本为中心，将语言作为核心推理媒介，难以胜任以视觉为主的推理任务。已有工作尝试用辅助图像、深度图或图像裁剪监督中间视觉步骤，却受限于对“有用”视觉抽象形式的强先验、高昂的标注成本以及跨任务泛化困难。</p>
                    <p><span class="font-medium text-text-primary">方法详情：</span>作者提出一种任务无关机制，让LMMs在无显式监督的情况下自动发现并学习“视觉推理token”。这些token对整幅图像做全局自注意力，并以任务自适应方式对图像进行再编码，从而动态提取与任务相关的视觉信息。整个训练过程仅依赖最终任务目标，无需任何中间视觉注释或手工设计的视觉抽象。</p>
                    <p><span class="font-medium text-text-primary">研究结果：</span>在多种以视觉为中心的基准上，该方法显著优于直接微调，并在难以指定中间抽象的任务中刷新最佳性能。实验表明，模型能够自发产生可解释的注意力模式，对应关键物体或区域。同时，该机制在多任务指令微调场景下依然保持优势，验证了其良好的泛化能力。</p>
                    <p><span class="font-medium text-text-primary">局限性：</span>论文尚未在更大规模模型或真实场景视频推理任务上验证其可扩展性；视觉token的可解释性与可控性仍有限，可能产生难以察觉的错误模式。训练过程需要额外的token参数与显存开销，对资源受限环境提出挑战。</p>
                    
                    <p><span class="font-medium text-text-primary">未来方向：</span>未来可探索将潜在视觉推理token与外部知识库或符号系统结合，实现可解释的视觉-符号联合推理；并研究在视频、3D点云等连续视觉序列上的自监督发现机制。</p>
                    
                    <p class="text-accent"><span class="font-medium">研究相关性：</span>对于关注视觉推理、多模态学习、无监督中间表示发现或希望提升模型在视觉主导任务上性能的研究者，该文提供了一种无需昂贵标注即可增强LMMs视觉推理能力的新范式。</p>
                  </div>
                </div>
              </div>
            </div>
            

            <div class="pt-4 border-t border-border-color">
              <div class="flex flex-wrap gap-3 text-xs">
                <span class="px-2 py-1 bg-bg-hover rounded text-text-secondary">相似度 0.83</span>
                <span class="px-2 py-1 bg-bg-hover rounded text-text-secondary">
                  期刊影响力 0.60
                  
                </span>
              </div>
            </div>
          </div>
        </article>
        
      </div>
    </div>
  </main>

  <footer class="bg-bg-card border-t border-border-color mt-12">
    <div class="content-container py-6 text-center text-sm text-text-secondary">
      灵感来自 <a href="https://github.com/Yorks0n/ZotWatch" class="text-accent hover:text-accent-hover transition-colors">ZotWatch</a>
    </div>
  </footer>

  <script>
    function toggleSection(id) {
      const el = document.getElementById(id);
      const btn = document.getElementById('btn-' + id);
      const icon = document.getElementById('icon-' + id);
      const hint = document.getElementById('hint-' + id);
      if (el.classList.contains('collapsed')) {
        el.classList.remove('collapsed');
        el.classList.add('expanded');
        if (btn && btn.textContent.includes('详细分析')) {
          btn.innerHTML = '<svg class="w-3 h-3 mr-1" fill="none" stroke="currentColor" viewBox="0 0 24 24"><path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M5 15l7-7 7 7"/></svg>收起详细分析';
        }
        if (hint) hint.textContent = '点击收起';
        if (icon) icon.style.transform = 'rotate(180deg)';
        if (id === 'researcher-profile') {
          setTimeout(() => { window.dispatchEvent(new Event('resize')); }, 100);
        }
      } else {
        el.classList.remove('expanded');
        el.classList.add('collapsed');
        if (btn && btn.textContent.includes('详细分析')) {
          btn.innerHTML = '<svg class="w-3 h-3 mr-1" fill="none" stroke="currentColor" viewBox="0 0 24 24"><path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/></svg>查看详细分析';
        }
        if (hint) hint.textContent = '点击展开';
        if (icon) icon.style.transform = 'rotate(0deg)';
      }
    }
  </script>
</body>
</html>