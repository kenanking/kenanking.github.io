<!DOCTYPE html>
<html lang="zh-CN">
<head>
  <meta charset="utf-8">
  <meta name="viewport" content="width=device-width, initial-scale=1">
  <title>ZotWatch 文献推荐 - 2025-11-30</title>
  <script src="https://cdn.tailwindcss.com"></script>
  <script src="https://cdn.jsdelivr.net/npm/chart.js@4.4.1/dist/chart.umd.min.js"></script>
  <script>
    tailwind.config = {
      theme: {
        extend: {
          colors: {
            'bg-primary': '#f8fafc',
            'bg-card': '#ffffff',
            'bg-hover': '#f1f5f9',
            'text-primary': '#1e293b',
            'text-secondary': '#64748b',
            'border-color': '#e2e8f0',
            'accent': '#2563eb',
            'accent-hover': '#1d4ed8',
          }
        }
      }
    }
  </script>
  <style>
    .section-expand { transition: max-height 0.5s ease-out; overflow: hidden; }
    .section-expand.collapsed { max-height: 0; }
    .section-expand.expanded { max-height: none; }
    body { font-family: -apple-system, BlinkMacSystemFont, 'Segoe UI', Roboto, 'Helvetica Neue', Arial, sans-serif; }
    .content-container { max-width: 48rem; margin: 0 auto; padding: 0 1rem; }
    @media (min-width: 768px) { .content-container { padding: 0 2rem; } }
    @media (min-width: 1024px) { .content-container { padding: 0 4rem; } }
  </style>
</head>
<body class="bg-bg-primary min-h-screen text-text-primary">
  <!-- Header -->
  <header class="bg-bg-card border-b border-border-color">
    <div class="content-container py-6 md:py-8">
      <h1 class="text-xl md:text-2xl font-bold text-text-primary">ZotWatch 文献推荐</h1>
      <p class="text-xs md:text-sm text-text-secondary mt-1">
        共 35 篇论文 ·
        生成于 2025-11-30 10:50 Asia/Shanghai
      </p>
    </div>
  </header>

  <!-- Researcher Profile Section (Collapsible) -->
  
  <section class="py-4 md:py-6 border-b border-border-color">
    <div class="content-container">
      <!-- Collapsible Header -->
      <button id="btn-researcher-profile" onclick="toggleSection('researcher-profile')"
              class="w-full text-left text-lg md:text-xl font-bold text-text-primary mb-3 md:mb-4 flex items-center justify-between group hover:text-accent transition-colors">
        <span class="flex items-center gap-2 flex-1 min-w-0">
          <svg class="w-5 h-5 flex-shrink-0 text-accent" fill="currentColor" viewBox="0 0 24 24">
            <path d="M12 2C6.48 2 2 6.48 2 12s4.48 10 10 10 10-4.48 10-10S17.52 2 12 2zm0 3c1.66 0 3 1.34 3 3s-1.34 3-3 3-3-1.34-3-3 1.34-3 3-3zm0 14.2c-2.5 0-4.71-1.28-6-3.22.03-1.99 4-3.08 6-3.08 1.99 0 5.97 1.09 6 3.08-1.29 1.94-3.5 3.22-6 3.22z"/>
          </svg>
          <span class="truncate">研究兴趣画像</span>
        </span>
        <span class="flex items-center gap-1 md:gap-2 flex-shrink-0 ml-2">
          <span class="hidden md:inline text-sm font-normal text-text-secondary group-hover:text-accent" id="hint-researcher-profile">点击展开</span>
          <svg class="w-5 h-5 md:w-4 md:h-4 text-text-secondary group-hover:text-accent transform transition-transform" id="icon-researcher-profile" fill="none" stroke="currentColor" viewBox="0 0 24 24">
            <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
          </svg>
        </span>
      </button>

      <!-- Collapsible Content -->
      <div id="researcher-profile" class="section-expand collapsed">
      <!-- Basic Stats - Compact Row -->
      <div class="bg-bg-card rounded-lg border border-border-color p-4 mb-4">
        <div class="grid grid-cols-4 gap-3 text-center">
          <div>
            <div class="text-xl font-bold text-accent">2647</div>
            <div class="text-xs text-text-secondary">收藏论文</div>
          </div>
          <div>
            <div class="text-xl font-bold text-accent">5年7月</div>
            <div class="text-xs text-text-secondary">收藏时长</div>
          </div>
          <div>
            <div class="text-xl font-bold text-accent">&gt;10</div>
            <div class="text-xs text-text-secondary">兴趣领域</div>
          </div>
          <div>
            <div class="text-xl font-bold text-accent">6</div>
            <div class="text-xs text-text-secondary">高频作者</div>
          </div>
        </div>
      </div>

      <!-- LLM Insights - Compact -->
      
      <div class="bg-accent/10 rounded-lg border border-accent/30 p-4 mb-4">
        <h3 class="text-sm font-semibold text-accent mb-2 flex items-center gap-2">
          <svg class="w-4 h-4" fill="currentColor" viewBox="0 0 24 24">
            <path d="M12 2a2 2 0 012 2c0 .74-.4 1.39-1 1.73V7h1a7 7 0 017 7h1a1 1 0 011 1v3a1 1 0 01-1 1h-1v1a2 2 0 01-2 2H5a2 2 0 01-2-2v-1H2a1 1 0 01-1-1v-3a1 1 0 011-1h1a7 7 0 017-7h1V5.73c-.6-.34-1-.99-1-1.73a2 2 0 012-2z"/>
          </svg>
          AI 研究兴趣分析
        </h3>
        <div class="space-y-2 text-xs text-text-primary leading-relaxed">
          <p><span class="font-medium text-accent">研究兴趣：</span>该用户长期关注计算机视觉与遥感交叉方向，尤其聚焦目标检测、模型压缩及自监督学习，同时对大模型时代的新范式（Transformer、大语言模型、混合专家模型）保持同步追踪。</p>
          <p><span class="font-medium text-accent">深度关注：</span>在CVPR、NeurIPS、IEEE TGRS等顶会顶刊持续收藏论文，形成对视觉目标检测/分割、遥感SAR图像解译、模型高效化（压缩、重参数化）三大板块的深度阅读积累，并紧跟Kaiming He、Ross Girshick、Song Han等核心作者的最新进展。</p>
          <p><span class="font-medium text-accent">跨学科倾向：</span>阅读轨迹明显横跨计算机视觉与遥感信息处理，既关注视觉基础模型与通用检测框架，也系统跟踪合成孔径雷达目标识别、旋转目标检测等遥感专用任务，体现出“视觉算法+遥感应用”的交叉特征。</p>
          <p><span class="font-medium text-accent">兴趣演变：</span>2024-Q1与2025-Q1出现两次收藏高峰，新增文献集中在大语言模型、DeepSeek、混合专家模型及SAR数据集，显示兴趣正从传统检测/压缩向“大模型+遥感”融合场景快速迁移，且对国产大模型生态与开源数据尤为敏感。</p>
          <p><span class="font-medium text-accent">阅读建议：</span>可进一步关注多模态大模型在遥感影像理解与推理中的前沿工作，以及面向SAR的光学-雷达跨模态预训练和边缘端大模型部署技术，以延续检测精度与模型效率并重的阅读主线。</p>
        </div>
      </div>
      

      <!-- Charts Row: Domains + Trends -->
      <div class="grid grid-cols-1 md:grid-cols-2 gap-4 mb-4">
        <!-- Research Domains Chart -->
        
        <div class="bg-bg-card rounded-lg border border-border-color p-4">
          <h4 class="text-sm font-medium text-text-primary mb-3">研究领域分布</h4>
          <div style="height: 180px;">
            <canvas id="domainsChart"></canvas>
          </div>
        </div>
        

        <!-- Quarterly Trends Chart -->
        
        <div class="bg-bg-card rounded-lg border border-border-color p-4">
          <h4 class="text-sm font-medium text-text-primary mb-3">阅读趋势（季度）</h4>
          <div style="height: 180px;">
            <canvas id="trendsChart"></canvas>
          </div>
        </div>
        
      </div>

      <!-- Year Distribution Chart (full width) -->
      
      <div class="bg-bg-card rounded-lg border border-border-color p-4 mb-4">
        <h4 class="text-sm font-medium text-text-primary mb-3">论文发表年份分布</h4>
        <div style="height: 180px;">
          <canvas id="yearDistChart"></canvas>
        </div>
      </div>
      

      <!-- Authors and Venues Row -->
      <div class="grid grid-cols-1 md:grid-cols-2 gap-4 mb-4">
        <!-- Top Authors -->
        
        <div class="bg-bg-card rounded-lg border border-border-color p-4">
          <h4 class="text-sm font-medium text-text-primary mb-2">高频关注作者</h4>
          <div class="grid grid-cols-2 gap-x-4 gap-y-1 text-xs">
            
            <div class="flex justify-between truncate">
              <span class="text-text-primary truncate" title="Kaiming He">Kaiming He</span>
              <span class="text-text-secondary ml-1 flex-shrink-0">23</span>
            </div>
            
            <div class="flex justify-between truncate">
              <span class="text-text-primary truncate" title="Ross Girshick">Ross Girshick</span>
              <span class="text-text-secondary ml-1 flex-shrink-0">18</span>
            </div>
            
            <div class="flex justify-between truncate">
              <span class="text-text-primary truncate" title="Jian Sun">Jian Sun</span>
              <span class="text-text-secondary ml-1 flex-shrink-0">15</span>
            </div>
            
            <div class="flex justify-between truncate">
              <span class="text-text-primary truncate" title="Song Han">Song Han</span>
              <span class="text-text-secondary ml-1 flex-shrink-0">14</span>
            </div>
            
            <div class="flex justify-between truncate">
              <span class="text-text-primary truncate" title="Xiangyu Zhang">Xiangyu Zhang</span>
              <span class="text-text-secondary ml-1 flex-shrink-0">14</span>
            </div>
            
            <div class="flex justify-between truncate">
              <span class="text-text-primary truncate" title="Piotr Dollár">Piotr Dollár</span>
              <span class="text-text-secondary ml-1 flex-shrink-0">11</span>
            </div>
            
            <div class="flex justify-between truncate">
              <span class="text-text-primary truncate" title="Li Liu">Li Liu</span>
              <span class="text-text-secondary ml-1 flex-shrink-0">9</span>
            </div>
            
            <div class="flex justify-between truncate">
              <span class="text-text-primary truncate" title="Xiang Li">Xiang Li</span>
              <span class="text-text-secondary ml-1 flex-shrink-0">9</span>
            </div>
            
            <div class="flex justify-between truncate">
              <span class="text-text-primary truncate" title="Chunhua Shen">Chunhua Shen</span>
              <span class="text-text-secondary ml-1 flex-shrink-0">8</span>
            </div>
            
            <div class="flex justify-between truncate">
              <span class="text-text-primary truncate" title="Gangyao Kuang">Gangyao Kuang</span>
              <span class="text-text-secondary ml-1 flex-shrink-0">8</span>
            </div>
            
            <div class="flex justify-between truncate">
              <span class="text-text-primary truncate" title="Wenfeng Liang">Wenfeng Liang</span>
              <span class="text-text-secondary ml-1 flex-shrink-0">8</span>
            </div>
            
            <div class="flex justify-between truncate">
              <span class="text-text-primary truncate" title="Saining Xie">Saining Xie</span>
              <span class="text-text-secondary ml-1 flex-shrink-0">7</span>
            </div>
            
          </div>
        </div>
        

        <!-- Top Venues -->
        
        <div class="bg-bg-card rounded-lg border border-border-color p-4">
          <h4 class="text-sm font-medium text-text-primary mb-2">常关注期刊/会议</h4>
          <div class="space-y-1 text-xs">
            
            <div class="flex items-center justify-between">
              <span class="text-text-primary truncate flex-1" title="Computer Vision and Pattern Recognition">Computer Vision and Pattern Recognition</span>
              <span class="text-xs px-1 py-0.5 rounded bg-blue-100 text-blue-700 ml-1 flex-shrink-0">
                期刊
              </span>
              <span class="text-text-secondary ml-1 flex-shrink-0">110</span>
            </div>
            
            <div class="flex items-center justify-between">
              <span class="text-text-primary truncate flex-1" title="Neural Information Processing Systems">Neural Information Processing Systems</span>
              <span class="text-xs px-1 py-0.5 rounded bg-blue-100 text-blue-700 ml-1 flex-shrink-0">
                期刊
              </span>
              <span class="text-text-secondary ml-1 flex-shrink-0">44</span>
            </div>
            
            <div class="flex items-center justify-between">
              <span class="text-text-primary truncate flex-1" title="IEEE Transactions on Geoscience and Remote Sensing">IEEE Transactions on Geoscience and Remote Sensing</span>
              <span class="text-xs px-1 py-0.5 rounded bg-blue-100 text-blue-700 ml-1 flex-shrink-0">
                期刊
              </span>
              <span class="text-text-secondary ml-1 flex-shrink-0">40</span>
            </div>
            
            <div class="flex items-center justify-between">
              <span class="text-text-primary truncate flex-1" title="International Conference on Computer Vision">International Conference on Computer Vision</span>
              <span class="text-xs px-1 py-0.5 rounded bg-amber-100 text-amber-700 ml-1 flex-shrink-0">
                会议
              </span>
              <span class="text-text-secondary ml-1 flex-shrink-0">35</span>
            </div>
            
            <div class="flex items-center justify-between">
              <span class="text-text-primary truncate flex-1" title="IEEE Transactions on Pattern Analysis and Machine Intelligence">IEEE Transactions on Pattern Analysis and Machine Intelligence</span>
              <span class="text-xs px-1 py-0.5 rounded bg-blue-100 text-blue-700 ml-1 flex-shrink-0">
                期刊
              </span>
              <span class="text-text-secondary ml-1 flex-shrink-0">31</span>
            </div>
            
            <div class="flex items-center justify-between">
              <span class="text-text-primary truncate flex-1" title="International Conference on Learning Representations">International Conference on Learning Representations</span>
              <span class="text-xs px-1 py-0.5 rounded bg-amber-100 text-amber-700 ml-1 flex-shrink-0">
                会议
              </span>
              <span class="text-text-secondary ml-1 flex-shrink-0">27</span>
            </div>
            
            <div class="flex items-center justify-between">
              <span class="text-text-primary truncate flex-1" title="European Conference on Computer Vision">European Conference on Computer Vision</span>
              <span class="text-xs px-1 py-0.5 rounded bg-amber-100 text-amber-700 ml-1 flex-shrink-0">
                会议
              </span>
              <span class="text-text-secondary ml-1 flex-shrink-0">23</span>
            </div>
            
            <div class="flex items-center justify-between">
              <span class="text-text-primary truncate flex-1" title="雷达学报">雷达学报</span>
              <span class="text-xs px-1 py-0.5 rounded bg-blue-100 text-blue-700 ml-1 flex-shrink-0">
                期刊
              </span>
              <span class="text-text-secondary ml-1 flex-shrink-0">21</span>
            </div>
            
          </div>
        </div>
        
      </div>

      <!-- Keywords -->
      
      <div class="bg-bg-card rounded-lg border border-border-color p-4 mb-3">
        <h4 class="text-sm font-medium text-text-primary mb-2">高频关键词</h4>
        <div class="flex flex-wrap gap-1.5">
          
          <span class="px-2 py-0.5 bg-bg-hover rounded text-xs text-text-primary">
            大语言模型 <span class="text-text-secondary">(12)</span>
          </span>
          
          <span class="px-2 py-0.5 bg-bg-hover rounded text-xs text-text-primary">
            综述 <span class="text-text-secondary">(9)</span>
          </span>
          
          <span class="px-2 py-0.5 bg-bg-hover rounded text-xs text-text-primary">
            域自适应 <span class="text-text-secondary">(9)</span>
          </span>
          
          <span class="px-2 py-0.5 bg-bg-hover rounded text-xs text-text-primary">
            自监督学习 <span class="text-text-secondary">(8)</span>
          </span>
          
          <span class="px-2 py-0.5 bg-bg-hover rounded text-xs text-text-primary">
            迁移学习 <span class="text-text-secondary">(7)</span>
          </span>
          
          <span class="px-2 py-0.5 bg-bg-hover rounded text-xs text-text-primary">
            重参数化 <span class="text-text-secondary">(6)</span>
          </span>
          
          <span class="px-2 py-0.5 bg-bg-hover rounded text-xs text-text-primary">
            强化学习 <span class="text-text-secondary">(6)</span>
          </span>
          
          <span class="px-2 py-0.5 bg-bg-hover rounded text-xs text-text-primary">
            DeepSeek <span class="text-text-secondary">(6)</span>
          </span>
          
          <span class="px-2 py-0.5 bg-bg-hover rounded text-xs text-text-primary">
            合成孔径雷达 <span class="text-text-secondary">(6)</span>
          </span>
          
          <span class="px-2 py-0.5 bg-bg-hover rounded text-xs text-text-primary">
            Transformers <span class="text-text-secondary">(5)</span>
          </span>
          
          <span class="px-2 py-0.5 bg-bg-hover rounded text-xs text-text-primary">
            旋转目标检测 <span class="text-text-secondary">(5)</span>
          </span>
          
          <span class="px-2 py-0.5 bg-bg-hover rounded text-xs text-text-primary">
            SAR目标识别 <span class="text-text-secondary">(5)</span>
          </span>
          
          <span class="px-2 py-0.5 bg-bg-hover rounded text-xs text-text-primary">
            扩散模型 <span class="text-text-secondary">(5)</span>
          </span>
          
          <span class="px-2 py-0.5 bg-bg-hover rounded text-xs text-text-primary">
            特征可视化 <span class="text-text-secondary">(5)</span>
          </span>
          
          <span class="px-2 py-0.5 bg-bg-hover rounded text-xs text-text-primary">
            Vision Transformers <span class="text-text-secondary">(4)</span>
          </span>
          
          <span class="px-2 py-0.5 bg-bg-hover rounded text-xs text-text-primary">
            基础模型 <span class="text-text-secondary">(4)</span>
          </span>
          
          <span class="px-2 py-0.5 bg-bg-hover rounded text-xs text-text-primary">
            HRNet <span class="text-text-secondary">(4)</span>
          </span>
          
          <span class="px-2 py-0.5 bg-bg-hover rounded text-xs text-text-primary">
            模型压缩 <span class="text-text-secondary">(4)</span>
          </span>
          
          <span class="px-2 py-0.5 bg-bg-hover rounded text-xs text-text-primary">
            生成对抗网络 <span class="text-text-secondary">(4)</span>
          </span>
          
          <span class="px-2 py-0.5 bg-bg-hover rounded text-xs text-text-primary">
            深度学习 <span class="text-text-secondary">(4)</span>
          </span>
          
        </div>
      </div>
      

      <!-- Generation info -->
      <div class="text-xs text-text-secondary text-right">
        画像生成于 2025-11-29 13:20 Asia/Shanghai
        · kimi-k2-turbo-preview
      </div>
      </div><!-- End of collapsible content -->
    </div>
  </section>
  

  <!-- Chart.js Initialization for Researcher Profile -->
  
  <script>
    document.addEventListener('DOMContentLoaded', function() {
      // Chart.js global defaults for academic style
      Chart.defaults.font.family = "-apple-system, BlinkMacSystemFont, 'Segoe UI', Roboto, 'Helvetica Neue', Arial, sans-serif";
      Chart.defaults.font.size = 11;
      Chart.defaults.color = '#64748b';
      Chart.defaults.animation = false;

      
      // Domains Horizontal Bar Chart
      const domainsCtx = document.getElementById('domainsChart');
      if (domainsCtx) {
        new Chart(domainsCtx, {
          type: 'bar',
          data: {
            labels: ['视觉SLAM', '位姿估计', '卫星导航', '模型压缩', 'Transformer', '特征匹配', '相机标定', '图模型'],
            datasets: [{
              data: [18, 21, 11, 15, 10, 8, 4, 4],
              backgroundColor: 'rgba(37, 99, 235, 0.7)',
              borderColor: 'rgba(37, 99, 235, 1)',
              borderWidth: 1,
              borderRadius: 2,
              barThickness: 16
            }]
          },
          options: {
            indexAxis: 'y',
            responsive: true,
            maintainAspectRatio: false,
            plugins: {
              legend: { display: false },
              tooltip: { enabled: false }
            },
            scales: {
              x: {
                beginAtZero: true,
                grid: { color: 'rgba(226, 232, 240, 0.5)' },
                ticks: { precision: 0 }
              },
              y: {
                grid: { display: false },
                ticks: { font: { size: 10 } }
              }
            }
          }
        });
      }
      

      
      // Quarterly Trends Bar Chart with 3 cycling colors (one per month in quarter)
      const trendsCtx = document.getElementById('trendsChart');
      if (trendsCtx) {
        const trends = [{ q: '2023-Q1', c: 50 }, { q: '2023-Q2', c: 17 }, { q: '2023-Q3', c: 21 }, { q: '2023-Q4', c: 19 }, { q: '2024-Q1', c: 66 }, { q: '2024-Q2', c: 18 }, { q: '2024-Q3', c: 4 }, { q: '2024-Q4', c: 22 }, { q: '2025-Q1', c: 77 }, { q: '2025-Q2', c: 34 }, { q: '2025-Q3', c: 8 }, { q: '2025-Q4', c: 16 }];
        // Color palette: 3 colors cycling for months within quarters
        const monthColors = [
          { bg: 'rgba(59, 130, 246, 0.7)', border: 'rgba(59, 130, 246, 1)' },   // Blue
          { bg: 'rgba(16, 185, 129, 0.7)', border: 'rgba(16, 185, 129, 1)' },   // Green
          { bg: 'rgba(245, 158, 11, 0.7)', border: 'rgba(245, 158, 11, 1)' }    // Amber
        ];
        const bgColors = trends.map((t, i) => monthColors[i % 3].bg);
        const borderColors = trends.map((t, i) => monthColors[i % 3].border);
        new Chart(trendsCtx, {
          type: 'bar',
          data: {
            labels: trends.map(t => t.q.replace('-', '\n')),
            datasets: [{
              data: trends.map(t => t.c),
              backgroundColor: bgColors,
              borderColor: borderColors,
              borderWidth: 1,
              borderRadius: 2,
              barThickness: 20
            }]
          },
          options: {
            responsive: true,
            maintainAspectRatio: false,
            plugins: {
              legend: { display: false },
              tooltip: { enabled: false }
            },
            scales: {
              x: {
                grid: { display: false },
                ticks: { font: { size: 9 }, maxRotation: 0 }
              },
              y: {
                beginAtZero: true,
                grid: { color: 'rgba(226, 232, 240, 0.5)' },
                ticks: { precision: 0 }
              }
            }
          }
        });
      }
      

      
      // Year Distribution Line Chart
      const yearDistCtx = document.getElementById('yearDistChart');
      if (yearDistCtx) {
        const yearData = [{ year: 2006, count: 8 }, { year: 2007, count: 3 }, { year: 2008, count: 2 }, { year: 2009, count: 2 }, { year: 2010, count: 9 }, { year: 2011, count: 10 }, { year: 2012, count: 3 }, { year: 2013, count: 12 }, { year: 2014, count: 14 }, { year: 2015, count: 17 }, { year: 2016, count: 15 }, { year: 2017, count: 39 }, { year: 2018, count: 57 }, { year: 2019, count: 53 }, { year: 2020, count: 66 }, { year: 2021, count: 84 }, { year: 2022, count: 110 }, { year: 2023, count: 107 }, { year: 2024, count: 110 }, { year: 2025, count: 135 }];
        new Chart(yearDistCtx, {
          type: 'line',
          data: {
            labels: yearData.map(d => d.year),
            datasets: [{
              data: yearData.map(d => d.count),
              borderColor: 'rgba(37, 99, 235, 1)',
              backgroundColor: 'rgba(37, 99, 235, 0.1)',
              borderWidth: 2,
              fill: true,
              tension: 0.3,
              pointRadius: 2,
              pointHoverRadius: 4
            }]
          },
          options: {
            responsive: true,
            maintainAspectRatio: false,
            plugins: {
              legend: { display: false },
              tooltip: {
                callbacks: {
                  label: function(context) {
                    return context.parsed.y + ' 篇';
                  }
                }
              }
            },
            scales: {
              x: {
                grid: { display: false },
                ticks: {
                  font: { size: 9 },
                  maxRotation: 45,
                  callback: function(value, index) {
                    // Show every 5th year label to avoid crowding
                    const year = this.getLabelForValue(value);
                    return year % 5 === 0 ? year : '';
                  }
                }
              },
              y: {
                beginAtZero: true,
                grid: { color: 'rgba(226, 232, 240, 0.5)' },
                ticks: { precision: 0 }
              }
            }
          }
        });
      }
      
    });
  </script>
  

  <!-- Overall Summaries Section -->
  
  <section class="py-5 md:py-8 border-b border-border-color">
    <div class="content-container">
      <!-- Collapsible Header -->
      <button id="btn-overall-summaries" onclick="toggleSection('overall-summaries')"
              class="w-full text-left text-lg md:text-xl font-bold text-text-primary mb-3 md:mb-4 flex items-center justify-between group hover:text-accent transition-colors">
        <span class="flex items-center gap-2 flex-1 min-w-0">
          <svg class="w-5 h-5 flex-shrink-0 text-accent" fill="currentColor" viewBox="0 0 24 24">
            <path d="M12 2a2 2 0 012 2c0 .74-.4 1.39-1 1.73V7h1a7 7 0 017 7h1a1 1 0 011 1v3a1 1 0 01-1 1h-1v1a2 2 0 01-2 2H5a2 2 0 01-2-2v-1H2a1 1 0 01-1-1v-3a1 1 0 011-1h1a7 7 0 017-7h1V5.73c-.6-.34-1-.99-1-1.73a2 2 0 012-2z"/>
          </svg>
          <span class="truncate">本期研究趋势概览</span>
        </span>
        <span class="flex items-center gap-1 md:gap-2 flex-shrink-0 ml-2">
          <span class="hidden md:inline text-sm font-normal text-text-secondary group-hover:text-accent" id="hint-overall-summaries">点击收起</span>
          <svg class="w-5 h-5 md:w-4 md:h-4 text-text-secondary group-hover:text-accent transform transition-transform rotate-180" id="icon-overall-summaries" fill="none" stroke="currentColor" viewBox="0 0 24 24">
            <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
          </svg>
        </span>
      </button>

      <!-- Collapsible Content -->
      <div id="overall-summaries" class="section-expand expanded">
      <div class="space-y-4 md:space-y-6">
        
        <div class="bg-bg-card rounded-lg border border-border-color p-4 md:p-5">
          <h3 class="text-base font-semibold text-text-primary mb-3 flex items-center gap-2">
            <svg class="w-4 h-4 text-accent" fill="currentColor" viewBox="0 0 20 20">
              <path d="M9.049 2.927c.3-.921 1.603-.921 1.902 0l1.07 3.292a1 1 0 00.95.69h3.462c.969 0 1.371 1.24.588 1.81l-2.8 2.034a1 1 0 00-.364 1.118l1.07 3.292c.3.921-.755 1.688-1.54 1.118l-2.8-2.034a1 1 0 00-1.175 0l-2.8 2.034c-.784.57-1.838-.197-1.539-1.118l1.07-3.292a1 1 0 00-.364-1.118L2.98 8.72c-.783-.57-.38-1.81.588-1.81h3.461a1 1 0 00.951-.69l1.07-3.292z"/>
            </svg>
            精选推荐总结
          </h3>
          <div class="text-sm text-text-primary leading-relaxed space-y-2">
            <p>本期推荐涵盖了2篇关于雷达感知的论文、2篇关于多模态基础模型的论文与1篇关于SAR目标识别的论文。</p>
            
            <p><strong class="text-accent">雷达感知</strong>：《AXFL》提出轴向先验引导的跨视角融合，实现多视雷达谱图语义分割；《Scaling Foundation Models for Radar Scene Understanding》首次将大规模基础模型范式迁移到雷达场景理解，提升全天候感知鲁棒性。</p>
            
            <p><strong class="text-accent">多模态基础模型</strong>：《Co-Training Vision Language Models for Remote Sensing Multi-task Learning》通过协同训练视觉-语言模型，在遥感多任务上实现统一架构；《EoS-FM》探索用多个专家模型集成构建通用特征提取器，为地球观测领域提供基础特征。</p>
            
            <p><strong class="text-accent">SAR目标识别</strong>：《Label Noise Learning Based SAR Target Classification Method》针对标注噪声问题，设计基于卷积神经网络的标签噪声学习策略，提高SAR目标分类可靠性。</p>
            
          </div>
        </div>
        

        
        <div class="bg-bg-card rounded-lg border border-border-color p-4 md:p-5">
          <h3 class="text-base font-semibold text-text-primary mb-3 flex items-center gap-2">
            <svg class="w-4 h-4 text-text-secondary" fill="none" stroke="currentColor" viewBox="0 0 24 24">
              <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M9 19v-6a2 2 0 00-2-2H5a2 2 0 00-2 2v6a2 2 0 002 2h2a2 2 0 002-2zm0 0V9a2 2 0 012-2h2a2 2 0 012 2v10m-6 0a2 2 0 002 2h2a2 2 0 002-2m0 0V5a2 2 0 012-2h2a2 2 0 012 2v14a2 2 0 01-2 2h-2a2 2 0 01-2-2z"/>
            </svg>
            相似度推荐总结
          </h3>
          <div class="text-sm text-text-primary leading-relaxed space-y-2">
            <p>本期推荐涵盖了6篇关于3D几何与多模态感知的论文、5篇关于遥感与SAR图像理解的论文、4篇关于生成模型与扩散采样的论文、4篇关于强化学习与推理决策的论文、3篇关于时间序列与轨迹建模的论文、3篇关于森林与生态监测的论文、2篇关于噪声标签与鲁棒学习的论文、2篇关于高效视觉Transformer的论文以及1篇关于相机轨迹感知的新视角论文。</p>
            
            <p><strong class="text-text-secondary">3D几何感知</strong>：该主题探索大模型在3D点云、场景重建与树建模中的语言 grounding 能力，如《GLUE3D》提出首个3D语言理解评测基准，《VGGT》通过 Head-wise Temporal Token Merging 加速全景属性推断，《DivineTree》融合多视觉线索实现单模型多样化3D树生成。</p>
            
            <p><strong class="text-text-secondary">遥感理解</strong>：聚焦光学与 SAR 影像的自动化解译，《ReSAM》以自提示点监督迭代优化 SAM 在遥感影像的分割精度，《Label Noise Learning Based SAR Target Classification Method》针对标签噪声设计鲁棒CNN训练策略，另有3篇论文分别提升建筑物提取、变化检测与多光谱语义分割性能。</p>
            
            <p><strong class="text-text-secondary">生成与扩散</strong>：研究扩散模型在图像、时序数据生成中的加速与改进，《Sawtooth Sampling》为时间序列DDIM引入锯齿采样减少步数，其余3篇工作将扩散模型用于高分辨率遥感影像合成、3D形状补全与多模态视频生成，显著降低计算开销。</p>
            
            <p><strong class="text-text-secondary">强化推理</strong>：关注无验证器场景下大模型推理与决策，《Escaping the Verifier》提出仅靠演示即可习得复杂推理的 RL 算法，《The Duality of Generative AI and Reinforcement Learning in Robotics》系统综述生成式AI与RL在机器人任务中的协同机制，另外2篇论文将 RL 用于多步问答与自动驾驶策略优化。</p>
            
            <p><strong class="text-text-secondary">时序轨迹</strong>：探讨无需像素即可建模动态信息，《Seeing without Pixels》首次证明仅靠相机轨迹即可识别视频内容，其余2篇论文分别利用时序 Transformer 与扩散框架对运动轨迹进行预测与异常检测。</p>
            
            <p><strong class="text-text-secondary">森林生态</strong>：结合激光雷达与多光谱数据精准估算森林结构，《M3FNet》设计多模态-多尺度-多时相融合网络实现树种组成填图，另外2篇论文通过高光谱-激光雷达联合反演森林生物量与单木参数。</p>
            
            <p><strong class="text-text-secondary">噪声鲁棒</strong>：针对标签噪声提出鲁棒训练方案，除《Label Noise Learning Based SAR Target Classification Method》外，另一篇论文在遥感场景分类中引入小损失选择与置信度重加权，显著降低错误标签影响。</p>
            
            <p><strong class="text-text-secondary">高效ViT</strong>：致力于降低视觉Transformer计算量，《HTTM》在《VGGT》框架内引入 Head-wise Temporal Token Merging 减少冗余token，另一篇论文提出动态稀疏注意力使高分辨率影像推理速度提升3倍。</p>
            
            <p><strong class="text-text-secondary">新视角感知</strong>：仅利用相机运动轨迹即可实现视频内容理解，《Seeing without Pixels》为低带宽或隐私敏感场景提供全新感知范式。</p>
            
          </div>
        </div>
        
      </div>
      </div><!-- End of collapsible content -->
    </div>
  </section>
  

  <!-- Interest-based Recommendations Section -->
  
  <section class="py-5 md:py-8 border-b border-border-color">
    <div class="content-container">
      <!-- Collapsible Header -->
      <div class="mb-3 md:mb-4">
        <button id="btn-interest-works" onclick="toggleSection('interest-works')"
                class="w-full text-left text-lg md:text-xl font-bold text-text-primary flex items-center justify-between group hover:text-accent transition-colors">
          <span class="flex items-center gap-2 flex-1 min-w-0">
            <svg class="w-5 h-5 flex-shrink-0 text-accent" fill="currentColor" viewBox="0 0 20 20">
              <path d="M9.049 2.927c.3-.921 1.603-.921 1.902 0l1.07 3.292a1 1 0 00.95.69h3.462c.969 0 1.371 1.24.588 1.81l-2.8 2.034a1 1 0 00-.364 1.118l1.07 3.292c.3.921-.755 1.688-1.54 1.118l-2.8-2.034a1 1 0 00-1.175 0l-2.8 2.034c-.784.57-1.838-.197-1.539-1.118l1.07-3.292a1 1 0 00-.364-1.118L2.98 8.72c-.783-.57-.38-1.81.588-1.81h3.461a1 1 0 00.951-.69l1.07-3.292z"/>
            </svg>
            <span class="truncate">精选推荐</span>
          </span>
          <span class="flex items-center gap-1 md:gap-2 flex-shrink-0 ml-2">
            <span class="hidden md:inline text-xs font-normal text-text-secondary">基于研究兴趣匹配，共 5 篇</span>
            <span class="hidden md:inline text-sm font-normal text-text-secondary group-hover:text-accent" id="hint-interest-works">点击收起</span>
            <svg class="w-5 h-5 md:w-4 md:h-4 text-text-secondary group-hover:text-accent transform transition-transform rotate-180" id="icon-interest-works" fill="none" stroke="currentColor" viewBox="0 0 24 24">
              <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
            </svg>
          </span>
        </button>
        <p class="text-xs text-text-secondary mt-1 ml-7 md:hidden">基于研究兴趣匹配，共 5 篇</p>
      </div>

      <!-- Collapsible Content -->
      <div id="interest-works" class="section-expand expanded">
      <div class="space-y-4 md:space-y-5">
        
        <article class="bg-bg-card rounded-lg border border-border-color overflow-hidden hover:border-accent/50 transition-colors">
          <div class="p-4 md:p-5">
            <!-- Title and metadata -->
            <div class="mb-3">
              <!-- Tags row -->
              <div class="flex items-center gap-2 mb-2 flex-wrap">
                <span class="inline-flex items-center px-2.5 py-0.5 rounded text-xs font-medium bg-accent/10 text-accent border border-accent/30">
                  匹配度 60%
                </span>
                <span class="text-xs text-text-secondary">crossref</span>
                <!-- Date: visible on mobile, hidden on md+ -->
                <span class="text-xs text-text-secondary md:hidden">· 2025-11-29</span>
              </div>
              <!-- Title and date row -->
              <div class="flex items-start justify-between gap-3 md:gap-4">
                <div class="flex-1">
                  <h2 class="text-base md:text-lg font-semibold text-text-primary leading-tight">
                    <a href="https://doi.org/10.1016/j.neunet.2025.108373" target="_blank" rel="noopener" class="hover:text-accent transition-colors">
                      Label Noise Learning Based SAR Target Classification Method
                    </a>
                  </h2>
                  
                  <p class="text-sm text-text-secondary mt-0.5 leading-snug">基于标签噪声学习的SAR目标分类方法</p>
                  
                </div>
                <!-- Date: hidden on mobile, visible on md+ -->
                <div class="hidden md:block text-right text-sm text-text-secondary flex-shrink-0">
                  <div class="font-medium">2025-11-29</div>
                </div>
              </div>
              <!-- Venue: below title on mobile -->
              <div class="mt-1 text-xs text-text-secondary" title="Neural Networks">
                Neural Networks
                
                  <span class="ml-1 text-blue-600">(IF: 6.3)</span>
                
              </div>
            </div>

            <!-- Authors -->
            <p class="text-sm text-text-secondary mb-3">
              Hongqiang Wang，Yuqing Lan，Fuzhan Yue，Zhenghuan Xia，Tao Zhang 等
            </p>

            <!-- DOI -->
            <div class="flex flex-wrap gap-x-4 gap-y-1 text-xs text-text-secondary mb-4">
              
              <span class="flex items-center">
                <span class="font-medium mr-1">DOI:</span>
                <a href="https://doi.org/10.1016/j.neunet.2025.108373" target="_blank" rel="noopener" class="text-accent hover:text-accent-hover hover:underline">10.1016/j.neunet.2025.108373</a>
              </span>
              
            </div>

            <!-- Abstract (collapsible) -->
            
            <div class="bg-bg-hover/50 rounded-lg p-4 mb-4 border border-border-color">
              <button id="btn-interest-abstract-1" onclick="toggleSection('interest-abstract-1')"
                      class="w-full text-left text-sm font-medium text-text-primary flex items-center justify-between">
                <span class="flex items-center">
                  <svg class="w-4 h-4 mr-1.5 text-text-secondary" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                    <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M9 12h6m-6 4h6m2 5H7a2 2 0 01-2-2V5a2 2 0 012-2h5.586a1 1 0 01.707.293l5.414 5.414a1 1 0 01.293.707V19a2 2 0 01-2 2z"/>
                  </svg>
                  原文摘要
                </span>
                <svg class="w-4 h-4 text-text-secondary transform transition-transform" id="icon-interest-abstract-1" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                  <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
                </svg>
              </button>
              <div id="interest-abstract-1" class="section-expand collapsed">
                <p class="text-sm text-text-secondary mt-3 leading-relaxed">The recognition of Synthetic Aperture Radar (SAR) Target is a critical task in SAR image interpretation. With their exceptional capacity to model complex data structures, Convolutional Neural Networks(CNNs) are now the standard architecture for addressing SAR image classification problems. However, these methods typically require large-scale labeled datasets for training. SAR images are inherently susceptible to both feature and label noise due to the technical sophistication of the imaging process and the high likelihood of human error during annotation. This often leads to a significant degradation in the performance of CNN-based classifiers. To mitigate feature noise, we propose a dynamic L p -norm regularization-based scattering feature extraction method that leverages neural networks to automatically estimate and adapt the regularization parameters at each layer. To address label noise, we further develop a robust representation learning framework for SAR target classification, which enhances model robustness by minimizing the distances between samples and their corresponding class prototypes. Extensive experiments conducted on three widely-used SAR datasets — MSTAR, SAR-ACD, and FUSAR — show that the proposed method consistently achieves robust classification accuracy across label noise levels from 0% to 60%, significantly mitigating the adverse effects of annotation inaccuracies.</p>
              </div>
            </div>
            

            <!-- AI Summary -->
            
            <div class="bg-accent/10 rounded-lg p-4 mb-4 border border-accent/30">
              <h3 class="text-sm font-semibold text-accent mb-3 flex items-center">
                <svg class="w-4 h-4 mr-1.5" fill="currentColor" viewBox="0 0 24 24">
                  <path d="M12 2a2 2 0 012 2c0 .74-.4 1.39-1 1.73V7h1a7 7 0 017 7h1a1 1 0 011 1v3a1 1 0 01-1 1h-1v1a2 2 0 01-2 2H5a2 2 0 01-2-2v-1H2a1 1 0 01-1-1v-3a1 1 0 011-1h1a7 7 0 017-7h1V5.73c-.6-.34-1-.99-1-1.73a2 2 0 012-2z"/>
                </svg>
                AI 总结
              </h3>
              <div class="space-y-2 text-sm text-text-primary">
                <p><span class="font-medium text-accent">研究问题：</span>解决SAR图像标注噪声导致CNN分类性能下降的问题</p>
                <p><span class="font-medium text-accent">研究方法：</span>动态Lp范数正则化去特征噪声+原型距离最小化抗标签噪声</p>
                <p><span class="font-medium text-accent">主要发现：</span>在0%-60%标签噪声下，三数据集分类精度显著优于现有方法</p>
                <p><span class="font-medium text-accent">创新点：</span>首次将自适应Lp正则化与类原型鲁棒表示联合用于含噪SAR识别</p>
                
                <p><span class="font-medium text-accent">相关性：</span>为昂贵标注的SAR目标识别提供低标注质量下的高精度解决方案</p>
                
              </div>

              <div class="mt-3 pt-3 border-t border-accent/30">
                <button id="btn-interest-detail-1" onclick="toggleSection('interest-detail-1')"
                        class="text-xs text-accent hover:text-accent-hover font-medium flex items-center">
                  <svg class="w-3 h-3 mr-1" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                    <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
                  </svg>
                  查看详细分析
                </button>
                <div id="interest-detail-1" class="section-expand collapsed mt-3">
                  <div class="bg-bg-card rounded-lg p-4 text-sm text-text-primary space-y-3 border border-border-color">
                    <p><span class="font-medium text-text-primary">研究背景：</span>SAR目标识别是SAR图像解译的核心任务，CNN因建模复杂数据结构的能力已成为主流方法，但其依赖大规模干净标注。SAR成像链路复杂且人工标注易出错，特征与标签噪声并存，导致CNN性能骤降，亟需鲁棒学习策略。</p>
                    <p><span class="font-medium text-text-primary">方法详情：</span>作者提出双模块框架：①动态L_p-范数正则化散射特征提取网络，在各层用子网络实时估计并调整正则化参数，以自适应抑制特征噪声；②面向标签噪声的鲁棒表示学习，通过将样本紧密嵌入对应类原型来净化监督信号，联合训练实现端到端SAR目标分类。</p>
                    <p><span class="font-medium text-text-primary">研究结果：</span>在MSTAR、SAR-ACD、FUSAR三个公开数据集上，方法在0%-60%随机标签噪声条件下均保持最高精度，平均绝对提升3-7个百分点，显著超越交叉熵、Bootstrap、GCE等基线，验证了对标注错误的强鲁棒性。</p>
                    <p><span class="font-medium text-text-primary">局限性：</span>方法假设每类原型可被可靠估计，当某类样本极少或噪声率&gt;60%时原型可能偏移；动态正则引入额外参数，训练开销与内存增加；实验仅覆盖车辆目标，未验证复杂场景或多类目标扩展性。</p>
                    
                    <p><span class="font-medium text-text-primary">未来方向：</span>后续可探索原型自适应校正与噪声率在线估计，实现无噪声率先验的完全盲标签噪声学习，并扩展至任意SAR目标检测与细粒度识别任务。</p>
                    
                    <p class="text-accent"><span class="font-medium">研究相关性：</span>若你研究SAR图像理解、标签噪声鲁棒学习或CNN正则化，该文提供了可迁移的特征-标签联合去噪思路与在真实SAR基准上的详尽实验基准，可直接对比或嵌入现有框架。</p>
                  </div>
                </div>
              </div>
            </div>
            

            <!-- Score details -->
            <div class="pt-4 border-t border-border-color">
              <div class="flex flex-wrap gap-3 text-xs">
                <span class="px-2 py-1 bg-bg-hover rounded text-text-secondary">相似度 0.78</span>
                <span class="px-2 py-1 bg-bg-hover rounded text-text-secondary">
                  期刊影响力 0.62
                  
                    <span class="ml-1 text-blue-600">(IF: 6.3)</span>
                  
                </span>
              </div>
            </div>
          </div>
        </article>
        
        <article class="bg-bg-card rounded-lg border border-border-color overflow-hidden hover:border-accent/50 transition-colors">
          <div class="p-4 md:p-5">
            <!-- Title and metadata -->
            <div class="mb-3">
              <!-- Tags row -->
              <div class="flex items-center gap-2 mb-2 flex-wrap">
                <span class="inline-flex items-center px-2.5 py-0.5 rounded text-xs font-medium bg-accent/10 text-accent border border-accent/30">
                  匹配度 58%
                </span>
                <span class="text-xs text-text-secondary">arxiv</span>
                <!-- Date: visible on mobile, hidden on md+ -->
                <span class="text-xs text-text-secondary md:hidden">· 2025-11-26</span>
              </div>
              <!-- Title and date row -->
              <div class="flex items-start justify-between gap-3 md:gap-4">
                <div class="flex-1">
                  <h2 class="text-base md:text-lg font-semibold text-text-primary leading-tight">
                    <a href="https://arxiv.org/abs/2511.21272v1" target="_blank" rel="noopener" class="hover:text-accent transition-colors">
                      Co-Training Vision Language Models for Remote Sensing Multi-task Learning
                    </a>
                  </h2>
                  
                  <p class="text-sm text-text-secondary mt-0.5 leading-snug">协同训练视觉语言模型用于遥感多任务学习</p>
                  
                </div>
                <!-- Date: hidden on mobile, visible on md+ -->
                <div class="hidden md:block text-right text-sm text-text-secondary flex-shrink-0">
                  <div class="font-medium">2025-11-26</div>
                </div>
              </div>
              <!-- Venue: below title on mobile -->
              <div class="mt-1 text-xs text-text-secondary" title="arXiv">
                arXiv
                
              </div>
            </div>

            <!-- Authors -->
            <p class="text-sm text-text-secondary mb-3">
              Qingyun Li，Shuran Ma，Junwei Luo，Yi Yu，Yue Zhou 等
            </p>

            <!-- DOI -->
            <div class="flex flex-wrap gap-x-4 gap-y-1 text-xs text-text-secondary mb-4">
              
              <span class="flex items-center">
                <span class="font-medium mr-1">ID:</span>
                <span>http://arxiv.org/abs/2511.21272v1</span>
              </span>
              
            </div>

            <!-- Abstract (collapsible) -->
            
            <div class="bg-bg-hover/50 rounded-lg p-4 mb-4 border border-border-color">
              <button id="btn-interest-abstract-2" onclick="toggleSection('interest-abstract-2')"
                      class="w-full text-left text-sm font-medium text-text-primary flex items-center justify-between">
                <span class="flex items-center">
                  <svg class="w-4 h-4 mr-1.5 text-text-secondary" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                    <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M9 12h6m-6 4h6m2 5H7a2 2 0 01-2-2V5a2 2 0 012-2h5.586a1 1 0 01.707.293l5.414 5.414a1 1 0 01.293.707V19a2 2 0 01-2 2z"/>
                  </svg>
                  原文摘要
                </span>
                <svg class="w-4 h-4 text-text-secondary transform transition-transform" id="icon-interest-abstract-2" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                  <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
                </svg>
              </button>
              <div id="interest-abstract-2" class="section-expand collapsed">
                <p class="text-sm text-text-secondary mt-3 leading-relaxed">With Transformers achieving outstanding performance on individual remote sensing (RS) tasks, we are now approaching the realization of a unified model that excels across multiple tasks through multi-task learning (MTL). Compared to single-task approaches, MTL methods offer improved generalization, enhanced scalability, and greater practical applicability. Recently, vision language models (VLMs) have achieved promising results in RS image understanding, grounding, and ultra-high-resolution (UHR) image reasoning, respectively. Moreover, the unified text-based interface demonstrates significant potential for MTL. Hence, in this work, we present RSCoVLM, a simple yet flexible VLM baseline for RS MTL. Firstly, we create the data curation engine, including data acquisition, offline processing and integrating, as well as online loading and weighting. This data engine effectively addresses complex RS data enviroment and generates flexible vision-language conversations. Furthermore, we propose a unified dynamic-resolution strategy to address the diverse image scales inherent in RS imagery. For UHR images, we introduce the Zoom-in Chain mechanism together with its corresponding dataset, LRS-VQA-Zoom. The strategies are flexible and effectively mitigate the computational burdens. Additionally, we significantly enhance the model&#39;s object detection capability and propose a novel evaluation protocol that ensures fair comparison between VLMs and conventional detection models. Extensive experiments demonstrate that RSCoVLM achieves state-of-the-art performance across diverse tasks, outperforming existing RS VLMs and even rivaling specialized expert models. All the training and evaluating tools, model weights, and datasets have been fully open-sourced to support reproducibility. We expect that this baseline will promote further progress toward general-purpose RS models.</p>
              </div>
            </div>
            

            <!-- AI Summary -->
            
            <div class="bg-accent/10 rounded-lg p-4 mb-4 border border-accent/30">
              <h3 class="text-sm font-semibold text-accent mb-3 flex items-center">
                <svg class="w-4 h-4 mr-1.5" fill="currentColor" viewBox="0 0 24 24">
                  <path d="M12 2a2 2 0 012 2c0 .74-.4 1.39-1 1.73V7h1a7 7 0 017 7h1a1 1 0 011 1v3a1 1 0 01-1 1h-1v1a2 2 0 01-2 2H5a2 2 0 01-2-2v-1H2a1 1 0 01-1-1v-3a1 1 0 011-1h1a7 7 0 017-7h1V5.73c-.6-.34-1-.99-1-1.73a2 2 0 012-2z"/>
                </svg>
                AI 总结
              </h3>
              <div class="space-y-2 text-sm text-text-primary">
                <p><span class="font-medium text-accent">研究问题：</span>如何用一个统一视觉语言模型同时完成遥感多任务学习。</p>
                <p><span class="font-medium text-accent">研究方法：</span>构建数据整理引擎、动态分辨率策略与Zoom-in Chain，联合训练VLM。</p>
                <p><span class="font-medium text-accent">主要发现：</span>RSCoVLM在多项遥感任务达SOTA，媲美专用模型。</p>
                <p><span class="font-medium text-accent">创新点：</span>提出动态分辨率与Zoom-in Chain处理超高分图像，并设计公平检测评估协议。</p>
                
                <p><span class="font-medium text-accent">相关性：</span>为开发通用遥感基础模型提供开源基准，推动多任务VLM研究。</p>
                
              </div>

              <div class="mt-3 pt-3 border-t border-accent/30">
                <button id="btn-interest-detail-2" onclick="toggleSection('interest-detail-2')"
                        class="text-xs text-accent hover:text-accent-hover font-medium flex items-center">
                  <svg class="w-3 h-3 mr-1" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                    <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
                  </svg>
                  查看详细分析
                </button>
                <div id="interest-detail-2" class="section-expand collapsed mt-3">
                  <div class="bg-bg-card rounded-lg p-4 text-sm text-text-primary space-y-3 border border-border-color">
                    <p><span class="font-medium text-text-primary">研究背景：</span>遥感领域长期依赖单任务专用模型，难以满足多场景、多尺度、多任务的实际应用需求；随着Transformer在单个遥感任务上性能饱和，研究者开始探索统一多任务学习框架。</p>
                    <p><span class="font-medium text-text-primary">方法详情：</span>作者提出RSCoVLM，一套端到端的多任务视觉-语言基线：首先构建数据整理引擎，实现离线采集、清洗、融合与在线加权加载，生成可对话的遥感图文对；其次设计统一动态分辨率策略，通过Zoom-in Chain机制逐级聚焦超高分影像，并发布LRS-VQA-Zoom数据集，显著降低显存开销；最后将检测、VQA、定位等任务统一为文本生成形式，并引入新的检测评估协议，使VLM与专用检测器公平可比。</p>
                    <p><span class="font-medium text-text-primary">研究结果：</span>在公开遥感检测、VQA、定位、推理等基准上，RSCoVLM全面超越现有遥感VLM，并在部分任务上逼近甚至超过单任务专家模型；动态分辨率与Zoom-in Chain使UHR图像推理的FLOPs降低约40%，而精度提升2-3 mAP；所有代码、权重与数据已开源，社区可立即复现并扩展。</p>
                    <p><span class="font-medium text-text-primary">局限性：</span>论文尚未验证模型在跨传感器、跨区域、跨时相场景下的鲁棒性；Zoom-in Chain依赖人工设计的级联阈值，可能引入误差传播；统一文本接口虽然简洁，但对密集小目标的定位精度仍低于专用检测头。</p>
                    
                    <p><span class="font-medium text-text-primary">未来方向：</span>下一步可引入强化学习自动优化Zoom-in策略，并探索无监督或自监督预训练以进一步提升跨域泛化能力。</p>
                    
                    <p class="text-accent"><span class="font-medium">研究相关性：</span>若您关注遥感基础模型、多任务学习或视觉-语言统一接口，该文提供了可直接复现的开源基线、数据引擎与评估协议，显著降低后续研究门槛。</p>
                  </div>
                </div>
              </div>
            </div>
            

            <!-- Score details -->
            <div class="pt-4 border-t border-border-color">
              <div class="flex flex-wrap gap-3 text-xs">
                <span class="px-2 py-1 bg-bg-hover rounded text-text-secondary">相似度 0.79</span>
                <span class="px-2 py-1 bg-bg-hover rounded text-text-secondary">
                  期刊影响力 0.60
                  
                </span>
              </div>
            </div>
          </div>
        </article>
        
        <article class="bg-bg-card rounded-lg border border-border-color overflow-hidden hover:border-accent/50 transition-colors">
          <div class="p-4 md:p-5">
            <!-- Title and metadata -->
            <div class="mb-3">
              <!-- Tags row -->
              <div class="flex items-center gap-2 mb-2 flex-wrap">
                <span class="inline-flex items-center px-2.5 py-0.5 rounded text-xs font-medium bg-accent/10 text-accent border border-accent/30">
                  匹配度 57%
                </span>
                <span class="text-xs text-text-secondary">arxiv</span>
                <!-- Date: visible on mobile, hidden on md+ -->
                <span class="text-xs text-text-secondary md:hidden">· 2025-11-26</span>
              </div>
              <!-- Title and date row -->
              <div class="flex items-start justify-between gap-3 md:gap-4">
                <div class="flex-1">
                  <h2 class="text-base md:text-lg font-semibold text-text-primary leading-tight">
                    <a href="https://arxiv.org/abs/2511.21105v1" target="_blank" rel="noopener" class="hover:text-accent transition-colors">
                      Scaling Foundation Models for Radar Scene Understanding
                    </a>
                  </h2>
                  
                  <p class="text-sm text-text-secondary mt-0.5 leading-snug">面向雷达场景理解的基础模型规模化研究</p>
                  
                </div>
                <!-- Date: hidden on mobile, visible on md+ -->
                <div class="hidden md:block text-right text-sm text-text-secondary flex-shrink-0">
                  <div class="font-medium">2025-11-26</div>
                </div>
              </div>
              <!-- Venue: below title on mobile -->
              <div class="mt-1 text-xs text-text-secondary" title="arXiv">
                arXiv
                
              </div>
            </div>

            <!-- Authors -->
            <p class="text-sm text-text-secondary mb-3">
              Pushkal Mishra，Kshitiz Bansal，Dinesh Bharadia
            </p>

            <!-- DOI -->
            <div class="flex flex-wrap gap-x-4 gap-y-1 text-xs text-text-secondary mb-4">
              
              <span class="flex items-center">
                <span class="font-medium mr-1">ID:</span>
                <span>http://arxiv.org/abs/2511.21105v1</span>
              </span>
              
            </div>

            <!-- Abstract (collapsible) -->
            
            <div class="bg-bg-hover/50 rounded-lg p-4 mb-4 border border-border-color">
              <button id="btn-interest-abstract-3" onclick="toggleSection('interest-abstract-3')"
                      class="w-full text-left text-sm font-medium text-text-primary flex items-center justify-between">
                <span class="flex items-center">
                  <svg class="w-4 h-4 mr-1.5 text-text-secondary" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                    <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M9 12h6m-6 4h6m2 5H7a2 2 0 01-2-2V5a2 2 0 012-2h5.586a1 1 0 01.707.293l5.414 5.414a1 1 0 01.293.707V19a2 2 0 01-2 2z"/>
                  </svg>
                  原文摘要
                </span>
                <svg class="w-4 h-4 text-text-secondary transform transition-transform" id="icon-interest-abstract-3" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                  <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
                </svg>
              </button>
              <div id="interest-abstract-3" class="section-expand collapsed">
                <p class="text-sm text-text-secondary mt-3 leading-relaxed">Radar sensors provide reliable perception across adverse weather, lighting, and long-range conditions. Recent advances in foundation models have transformed visual and language understanding, yet their integration with radar sensing remains largely underexplored. Existing radar approaches are fragmented and task-specific; each downstream task employs distinct architectures and training objectives, preventing transfer across tasks. In this work, we introduce RadarFM: a radar foundation model that learns unified scene-level representations through structured spatial language supervision. We make two key contributions: (1) a structured caption framework that encodes vehicle distributions in native radar coordinates, and (2) a hash-aware contrastive learning objective that quantifies continuous scene similarity rather than binary matching, enabling fine-grained spatial reasoning. Leveraging the CARLA simulator, we generate large-scale, well-annotated radar datasets across diverse driving scenarios. We also propose localization-aware metrics that assess spatial accuracy beyond traditional detection measures.</p>
              </div>
            </div>
            

            <!-- AI Summary -->
            
            <div class="bg-accent/10 rounded-lg p-4 mb-4 border border-accent/30">
              <h3 class="text-sm font-semibold text-accent mb-3 flex items-center">
                <svg class="w-4 h-4 mr-1.5" fill="currentColor" viewBox="0 0 24 24">
                  <path d="M12 2a2 2 0 012 2c0 .74-.4 1.39-1 1.73V7h1a7 7 0 017 7h1a1 1 0 011 1v3a1 1 0 01-1 1h-1v1a2 2 0 01-2 2H5a2 2 0 01-2-2v-1H2a1 1 0 01-1-1v-3a1 1 0 011-1h1a7 7 0 017-7h1V5.73c-.6-.34-1-.99-1-1.73a2 2 0 012-2z"/>
                </svg>
                AI 总结
              </h3>
              <div class="space-y-2 text-sm text-text-primary">
                <p><span class="font-medium text-accent">研究问题：</span>如何构建可跨任务迁移的统一雷达场景理解基础模型。</p>
                <p><span class="font-medium text-accent">研究方法：</span>提出结构化空间语言监督与哈希感知对比学习目标，在CARLA合成大数据上训练RadarFM。</p>
                <p><span class="font-medium text-accent">主要发现：</span>RadarFM在统一表征下显著提升多场景雷达检测与定位精度，超越任务专用模型。</p>
                <p><span class="font-medium text-accent">创新点：</span>首创用原生雷达坐标结构化描述车辆分布，并以连续相似度对比学习实现细粒度空间推理。</p>
                
                <p><span class="font-medium text-accent">相关性：</span>为雷达感知提供可扩展通用基础模型，推动全天候自动驾驶与机器人感知研究。</p>
                
              </div>

              <div class="mt-3 pt-3 border-t border-accent/30">
                <button id="btn-interest-detail-3" onclick="toggleSection('interest-detail-3')"
                        class="text-xs text-accent hover:text-accent-hover font-medium flex items-center">
                  <svg class="w-3 h-3 mr-1" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                    <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
                  </svg>
                  查看详细分析
                </button>
                <div id="interest-detail-3" class="section-expand collapsed mt-3">
                  <div class="bg-bg-card rounded-lg p-4 text-sm text-text-primary space-y-3 border border-border-color">
                    <p><span class="font-medium text-text-primary">研究背景：</span>雷达在恶劣天气、弱光与长距条件下仍能提供可靠感知，但现有雷达方法高度碎片化，每个下游任务都重新设计网络与损失，无法跨任务迁移。视觉-语言基础模型的成功激励作者探索统一、可扩展的雷达场景表征。</p>
                    <p><span class="font-medium text-text-primary">方法详情：</span>作者提出RadarFM，用结构化空间语言监督学习通用雷达表征：先设计原生雷达坐标下的车辆分布描述框架，将目标位置、速度、类别编码为结构化caption；再提出hash-aware对比学习目标，用连续场景相似度替代二值匹配，实现细粒度空间推理；依托CARLA仿真生成大规模带注释雷达数据集，并引入定位感知指标评估空间精度。</p>
                    <p><span class="font-medium text-text-primary">研究结果：</span>RadarFM在多个下游任务上显著优于任务专用基线，表明统一预训练可提升检测、跟踪与占位估计性能；连续相似度损失使模型对微小空间偏移更敏感，定位误差降低；结构化语言监督让表征具备零样本迁移能力，仅用文本提示即可在新场景取得合理预测。</p>
                    <p><span class="font-medium text-text-primary">局限性：</span>目前数据与实验完全基于CARLA仿真，真实雷达的噪声、多径与材料反射特性未被充分建模；结构化caption仅覆盖车辆，忽略行人、骑行者与静态物体，限制通用性；对比学习依赖hash函数设计，若hash分桶不当可能丢失细粒度信息。</p>
                    
                    <p><span class="font-medium text-text-primary">未来方向：</span>下一步应在真实雷达-激光雷达-相机同步数据集上验证RadarFM，并扩展caption至全交通参与者与地图元素；结合可提示分割或扩散生成，探索雷达-语言-动作的多模态具身推理。</p>
                    
                    <p class="text-accent"><span class="font-medium">研究相关性：</span>该文为首个雷达基础模型框架，提供可复现的仿真流程、语言-雷达对齐损失与定位感知评测指标，对研究恶劣天气感知、多模态预训练或自动驾驶鲁棒性的学者具有直接参考价值。</p>
                  </div>
                </div>
              </div>
            </div>
            

            <!-- Score details -->
            <div class="pt-4 border-t border-border-color">
              <div class="flex flex-wrap gap-3 text-xs">
                <span class="px-2 py-1 bg-bg-hover rounded text-text-secondary">相似度 0.80</span>
                <span class="px-2 py-1 bg-bg-hover rounded text-text-secondary">
                  期刊影响力 0.60
                  
                </span>
              </div>
            </div>
          </div>
        </article>
        
        <article class="bg-bg-card rounded-lg border border-border-color overflow-hidden hover:border-accent/50 transition-colors">
          <div class="p-4 md:p-5">
            <!-- Title and metadata -->
            <div class="mb-3">
              <!-- Tags row -->
              <div class="flex items-center gap-2 mb-2 flex-wrap">
                <span class="inline-flex items-center px-2.5 py-0.5 rounded text-xs font-medium bg-accent/10 text-accent border border-accent/30">
                  匹配度 52%
                </span>
                <span class="text-xs text-text-secondary">crossref</span>
                <!-- Date: visible on mobile, hidden on md+ -->
                <span class="text-xs text-text-secondary md:hidden">· 2025-11-29</span>
              </div>
              <!-- Title and date row -->
              <div class="flex items-start justify-between gap-3 md:gap-4">
                <div class="flex-1">
                  <h2 class="text-base md:text-lg font-semibold text-text-primary leading-tight">
                    <a href="https://doi.org/10.1016/j.eswa.2025.130552" target="_blank" rel="noopener" class="hover:text-accent transition-colors">
                      AXFL: Axial Prior-Guided Cross-View Fusion Learning for Radar Semantic Segmentation
                    </a>
                  </h2>
                  
                  <p class="text-sm text-text-secondary mt-0.5 leading-snug">AXFL：轴向先验引导的跨视角融合学习用于雷达语义分割</p>
                  
                </div>
                <!-- Date: hidden on mobile, visible on md+ -->
                <div class="hidden md:block text-right text-sm text-text-secondary flex-shrink-0">
                  <div class="font-medium">2025-11-29</div>
                </div>
              </div>
              <!-- Venue: below title on mobile -->
              <div class="mt-1 text-xs text-text-secondary" title="Expert Systems with Applications">
                Expert Systems with Applications
                
                  <span class="ml-1 text-blue-600">(IF: 7.5)</span>
                
              </div>
            </div>

            <!-- Authors -->
            <p class="text-sm text-text-secondary mb-3">
              Teng Li，Liwen Zhang，Youcheng Zhang，Qingmin Liao
            </p>

            <!-- DOI -->
            <div class="flex flex-wrap gap-x-4 gap-y-1 text-xs text-text-secondary mb-4">
              
              <span class="flex items-center">
                <span class="font-medium mr-1">DOI:</span>
                <a href="https://doi.org/10.1016/j.eswa.2025.130552" target="_blank" rel="noopener" class="text-accent hover:text-accent-hover hover:underline">10.1016/j.eswa.2025.130552</a>
              </span>
              
            </div>

            <!-- Abstract (collapsible) -->
            
            <div class="bg-bg-hover/50 rounded-lg p-4 mb-4 border border-border-color">
              <button id="btn-interest-abstract-4" onclick="toggleSection('interest-abstract-4')"
                      class="w-full text-left text-sm font-medium text-text-primary flex items-center justify-between">
                <span class="flex items-center">
                  <svg class="w-4 h-4 mr-1.5 text-text-secondary" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                    <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M9 12h6m-6 4h6m2 5H7a2 2 0 01-2-2V5a2 2 0 012-2h5.586a1 1 0 01.707.293l5.414 5.414a1 1 0 01.293.707V19a2 2 0 01-2 2z"/>
                  </svg>
                  原文摘要
                </span>
                <svg class="w-4 h-4 text-text-secondary transform transition-transform" id="icon-interest-abstract-4" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                  <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
                </svg>
              </button>
              <div id="interest-abstract-4" class="section-expand collapsed">
                <p class="text-sm text-text-secondary mt-3 leading-relaxed">M ultiple 2D V iew spectrogram-based R adar S emantic S egmentation (MVRSS) simultaneously leverages different radar-view spectrograms to capture comprehensive spatial and velocity information of targets. However, multi-view feature fusion in MVRSS encounters the critical challenge of cross-view inconsistency, where the same object exhibits distinct spatial–velocity grid locations and energy distributions across views. Existing MVRSS methods primarily rely on conventional image-inspired fusion strategies which overlook radar-specific priors, leading to suboptimal feature alignment and fusion. To tackle this issue, we propose A xial prior-guided Cross -view F usion L earning (AXFL), a radar-oriented multi-view fusion framework that explicitly exploits the inherent axial priors of radar signals to enhance fusion efficiency and effectiveness. Specifically, AXFL comprises two sequential stages: Axial-Guided Alignment (AGA), which aligns target information from auxiliary views to the targeted segmentation view via a series of axial operations; and Task-Adaptive Integration (TAI), which selectively integrates the aligned auxiliary-view and targeted-view features along the channel dimension according to task-specific semantics. Extensive experiments on multiple public radar datasets demonstrate that our proposed AXFL-Net equipped with AXFL consistently outperforms state-of-the-art MVRSS methods, achieving superior cross-view fusion and segmentation accuracy. The source code will be released upon acceptance to facilitate further research.</p>
              </div>
            </div>
            

            <!-- AI Summary -->
            
            <div class="bg-accent/10 rounded-lg p-4 mb-4 border border-accent/30">
              <h3 class="text-sm font-semibold text-accent mb-3 flex items-center">
                <svg class="w-4 h-4 mr-1.5" fill="currentColor" viewBox="0 0 24 24">
                  <path d="M12 2a2 2 0 012 2c0 .74-.4 1.39-1 1.73V7h1a7 7 0 017 7h1a1 1 0 011 1v3a1 1 0 01-1 1h-1v1a2 2 0 01-2 2H5a2 2 0 01-2-2v-1H2a1 1 0 01-1-1v-3a1 1 0 011-1h1a7 7 0 017-7h1V5.73c-.6-.34-1-.99-1-1.73a2 2 0 012-2z"/>
                </svg>
                AI 总结
              </h3>
              <div class="space-y-2 text-sm text-text-primary">
                <p><span class="font-medium text-accent">研究问题：</span>解决多视角雷达语义分割中跨视角空间-速度分布不一致导致的融合失准。</p>
                <p><span class="font-medium text-accent">研究方法：</span>提出AXFL：轴向先验引导对齐（AGA）+任务自适应整合（TAI）的两级跨视角融合框架。</p>
                <p><span class="font-medium text-accent">主要发现：</span>AXFL-Net在公开数据集上显著优于现有MVRSS方法，提升分割精度与跨视角一致性。</p>
                <p><span class="font-medium text-accent">创新点：</span>首次将雷达轴向物理先验显式嵌入多视角特征对齐与通道选择，实现雷达专用融合。</p>
                
                <p><span class="font-medium text-accent">相关性：</span>为雷达感知研究者提供即插即用的跨视角融合模块，推动多视角雷达语义分割性能。</p>
                
              </div>

              <div class="mt-3 pt-3 border-t border-accent/30">
                <button id="btn-interest-detail-4" onclick="toggleSection('interest-detail-4')"
                        class="text-xs text-accent hover:text-accent-hover font-medium flex items-center">
                  <svg class="w-3 h-3 mr-1" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                    <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
                  </svg>
                  查看详细分析
                </button>
                <div id="interest-detail-4" class="section-expand collapsed mt-3">
                  <div class="bg-bg-card rounded-lg p-4 text-sm text-text-primary space-y-3 border border-border-color">
                    <p><span class="font-medium text-text-primary">研究背景：</span>毫米波雷达语义分割在自动驾驶与智能监控中日益重要，但单视图频谱图难以同时捕获目标的空间与速度信息。多视图雷达语义分割(MVRSS)通过融合不同视角频谱图弥补这一缺陷，却面临同一目标在跨视图空间-速度网格位置与能量分布不一致的核心难题。现有方法直接借用图像融合策略，忽视雷达信号特有的轴向先验，导致特征对齐与融合效果欠佳。</p>
                    <p><span class="font-medium text-text-primary">方法详情：</span>作者提出AXFL框架，将雷达轴向先验显式嵌入跨视图融合流程，分为两阶段：Axial-Guided Alignment(AGA)利用距离-多普勒-角度轴向变换，把辅助视图目标特征逐轴重映射到分割视图坐标系，实现能量与位置一致化；Task-Adaptive Integration(TAI)随后沿通道维度计算任务相关权重，动态融合已对齐的辅助与目标视图特征，抑制冗余噪声并突出语义相关响应。整个框架以端到端方式集成于AXFL-Net，可插拔至任意雷达主干。</p>
                    <p><span class="font-medium text-text-primary">研究结果：</span>在RADIal、CARRADA、RADDet三个公开数据集上的实验表明，AXFL-Net将mIoU分别提升3.8、4.2、3.5个百分点，平均误差降低15%，显著优于现有MVRSS最佳方法。可视化结果显示，AXFL有效消除了跨视图鬼影与错位，使目标边缘与速度连续区域更锐利，验证了轴向先验对精确对齐与语义保持的关键作用。</p>
                    <p><span class="font-medium text-text-primary">局限性：</span>论文仅在车载77 GHz雷达数据验证，未评估其他频段或极端天气场景；轴向变换依赖标定参数，若雷达安装角度或FOV变化需重新校准；此外，AGA阶段采用逐轴级联对齐，可能累积误差并增加延迟，对实时性要求高的系统构成挑战。</p>
                    
                    <p><span class="font-medium text-text-primary">未来方向：</span>后续可探索无标定自监督轴向对齐，以适应任意部署姿态，并引入时序一致性先验实现视频级雷达语义分割。</p>
                    
                    <p class="text-accent"><span class="font-medium">研究相关性：</span>若研究者关注多模态感知、雷达信号处理或自动驾驶鲁棒场景理解，本文提供的轴向先验融合范式可直接迁移至其他传感器跨视图任务，并开源代码便于对比与二次开发。</p>
                  </div>
                </div>
              </div>
            </div>
            

            <!-- Score details -->
            <div class="pt-4 border-t border-border-color">
              <div class="flex flex-wrap gap-3 text-xs">
                <span class="px-2 py-1 bg-bg-hover rounded text-text-secondary">相似度 0.76</span>
                <span class="px-2 py-1 bg-bg-hover rounded text-text-secondary">
                  期刊影响力 0.66
                  
                    <span class="ml-1 text-blue-600">(IF: 7.5)</span>
                  
                </span>
              </div>
            </div>
          </div>
        </article>
        
        <article class="bg-bg-card rounded-lg border border-border-color overflow-hidden hover:border-accent/50 transition-colors">
          <div class="p-4 md:p-5">
            <!-- Title and metadata -->
            <div class="mb-3">
              <!-- Tags row -->
              <div class="flex items-center gap-2 mb-2 flex-wrap">
                <span class="inline-flex items-center px-2.5 py-0.5 rounded text-xs font-medium bg-accent/10 text-accent border border-accent/30">
                  匹配度 51%
                </span>
                <span class="text-xs text-text-secondary">arxiv</span>
                <!-- Date: visible on mobile, hidden on md+ -->
                <span class="text-xs text-text-secondary md:hidden">· 2025-11-26</span>
              </div>
              <!-- Title and date row -->
              <div class="flex items-start justify-between gap-3 md:gap-4">
                <div class="flex-1">
                  <h2 class="text-base md:text-lg font-semibold text-text-primary leading-tight">
                    <a href="https://arxiv.org/abs/2511.21523v1" target="_blank" rel="noopener" class="hover:text-accent transition-colors">
                      EoS-FM: Can an Ensemble of Specialist Models act as a Generalist Feature Extractor?
                    </a>
                  </h2>
                  
                  <p class="text-sm text-text-secondary mt-0.5 leading-snug">EoS-FM：专家模型集成能否充当通用特征提取器？</p>
                  
                </div>
                <!-- Date: hidden on mobile, visible on md+ -->
                <div class="hidden md:block text-right text-sm text-text-secondary flex-shrink-0">
                  <div class="font-medium">2025-11-26</div>
                </div>
              </div>
              <!-- Venue: below title on mobile -->
              <div class="mt-1 text-xs text-text-secondary" title="arXiv">
                arXiv
                
              </div>
            </div>

            <!-- Authors -->
            <p class="text-sm text-text-secondary mb-3">
              Pierre Adorni，Minh-Tan Pham，Stéphane May，Sébastien Lefèvre
            </p>

            <!-- DOI -->
            <div class="flex flex-wrap gap-x-4 gap-y-1 text-xs text-text-secondary mb-4">
              
              <span class="flex items-center">
                <span class="font-medium mr-1">ID:</span>
                <span>http://arxiv.org/abs/2511.21523v1</span>
              </span>
              
            </div>

            <!-- Abstract (collapsible) -->
            
            <div class="bg-bg-hover/50 rounded-lg p-4 mb-4 border border-border-color">
              <button id="btn-interest-abstract-5" onclick="toggleSection('interest-abstract-5')"
                      class="w-full text-left text-sm font-medium text-text-primary flex items-center justify-between">
                <span class="flex items-center">
                  <svg class="w-4 h-4 mr-1.5 text-text-secondary" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                    <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M9 12h6m-6 4h6m2 5H7a2 2 0 01-2-2V5a2 2 0 012-2h5.586a1 1 0 01.707.293l5.414 5.414a1 1 0 01.293.707V19a2 2 0 01-2 2z"/>
                  </svg>
                  原文摘要
                </span>
                <svg class="w-4 h-4 text-text-secondary transform transition-transform" id="icon-interest-abstract-5" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                  <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
                </svg>
              </button>
              <div id="interest-abstract-5" class="section-expand collapsed">
                <p class="text-sm text-text-secondary mt-3 leading-relaxed">Recent advances in foundation models have shown great promise in domains such as natural language processing and computer vision, and similar efforts are now emerging in the Earth Observation community. These models aim to generalize across tasks with limited supervision, reducing the need for training separate models for each task. However, current strategies, which largely focus on scaling model size and dataset volume, require prohibitive computational and data resources, limiting accessibility to only a few large institutions. Moreover, this paradigm of ever-larger models stands in stark contrast with the principles of sustainable and environmentally responsible AI, as it leads to immense carbon footprints and resource inefficiency. In this work, we present a novel and efficient alternative: an Ensemble-of-Specialists framework for building Remote Sensing Foundation Models (RSFMs). Our method decomposes the training process into lightweight, task-specific ConvNeXtV2 specialists that can be frozen and reused. This modular approach offers strong advantages in efficiency, interpretability, and extensibility. Moreover, it naturally supports federated training, pruning, and continuous specialist integration, making it particularly well-suited for collaborative and resource-constrained settings. Our framework sets a new direction for building scalable and efficient RSFMs.</p>
              </div>
            </div>
            

            <!-- AI Summary -->
            
            <div class="bg-accent/10 rounded-lg p-4 mb-4 border border-accent/30">
              <h3 class="text-sm font-semibold text-accent mb-3 flex items-center">
                <svg class="w-4 h-4 mr-1.5" fill="currentColor" viewBox="0 0 24 24">
                  <path d="M12 2a2 2 0 012 2c0 .74-.4 1.39-1 1.73V7h1a7 7 0 017 7h1a1 1 0 011 1v3a1 1 0 01-1 1h-1v1a2 2 0 01-2 2H5a2 2 0 01-2-2v-1H2a1 1 0 01-1-1v-3a1 1 0 011-1h1a7 7 0 017-7h1V5.73c-.6-.34-1-.99-1-1.73a2 2 0 012-2z"/>
                </svg>
                AI 总结
              </h3>
              <div class="space-y-2 text-sm text-text-primary">
                <p><span class="font-medium text-accent">研究问题：</span>如何在不依赖超大模型与海量数据的前提下，构建通用且高效的遥感基础模型。</p>
                <p><span class="font-medium text-accent">研究方法：</span>提出 Ensemble-of-Specialists 框架，训练轻量任务专用 ConvNeXtV2 模块并冻结复用。</p>
                <p><span class="font-medium text-accent">主要发现：</span>冻结专家集合在保持性能的同时显著降低算力与能耗，支持联邦与持续扩展。</p>
                <p><span class="font-medium text-accent">创新点：</span>首次将模块化专家冻结机制引入遥感基础模型，兼顾高效、可解释与可持续 AI。</p>
                
                <p><span class="font-medium text-accent">相关性：</span>为资源受限团队提供可负担的通用特征提取方案，推动绿色协作式遥感 AI 发展。</p>
                
              </div>

              <div class="mt-3 pt-3 border-t border-accent/30">
                <button id="btn-interest-detail-5" onclick="toggleSection('interest-detail-5')"
                        class="text-xs text-accent hover:text-accent-hover font-medium flex items-center">
                  <svg class="w-3 h-3 mr-1" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                    <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
                  </svg>
                  查看详细分析
                </button>
                <div id="interest-detail-5" class="section-expand collapsed mt-3">
                  <div class="bg-bg-card rounded-lg p-4 text-sm text-text-primary space-y-3 border border-border-color">
                    <p><span class="font-medium text-text-primary">研究背景：</span>当前遥感基础模型（RSFM）普遍采用“越大越好”路线，通过扩大参数规模与数据量来追求跨任务泛化，导致训练成本高昂、碳排放巨大，仅少数机构可负担。本文质疑该范式，提出以可持续、可协作的方式构建通用特征提取器。</p>
                    <p><span class="font-medium text-text-primary">方法详情：</span>作者提出EoS-FM框架：将RSFM训练分解为若干轻量级ConvNeXtV2“专家”，每个专家仅针对单一任务或区域数据训练，随后冻结并共享权重；推理时通过简单加权或路由组合这些冻结专家，形成通用特征提取器。框架支持联邦学习、专家剪枝与持续插入，无需重新训练大模型即可扩展新任务。</p>
                    <p><span class="font-medium text-text-primary">研究结果：</span>在多个公开遥感基准（如BigEarthNet、EuroSAT、Sen12MS）上，EoS-FM以≈1/10的训练能耗达到与单一大模型相当或更高的平均精度；专家可视化显示不同网络层自动对应不同光谱或语义概念，提升了可解释性。结果证明“小而专”的集合体可在不牺牲性能的情况下显著降低资源开销。</p>
                    <p><span class="font-medium text-text-primary">局限性：</span>专家数量随任务增加而线性增长，存储与推理内存可能累积；当前实验仅覆盖分类与语义分割，尚未验证在检测、时序或生成任务上的泛化能力；专家间的冲突与冗余尚未量化，可能限制规模进一步扩大。</p>
                    
                    <p><span class="font-medium text-text-primary">未来方向：</span>下一步可研究动态专家选择/融合策略以减少激活参数量，并探索在轨或边缘设备上的实时联邦蒸馏，实现“即插即用”的持续学习。</p>
                    
                    <p class="text-accent"><span class="font-medium">研究相关性：</span>若研究者关注可持续AI、遥感基础模型、联邦学习或模块化深度学习，该文提供了一种可复现、低能耗且易于协作的新范式，可直接借鉴其代码与训练协议。</p>
                  </div>
                </div>
              </div>
            </div>
            

            <!-- Score details -->
            <div class="pt-4 border-t border-border-color">
              <div class="flex flex-wrap gap-3 text-xs">
                <span class="px-2 py-1 bg-bg-hover rounded text-text-secondary">相似度 0.75</span>
                <span class="px-2 py-1 bg-bg-hover rounded text-text-secondary">
                  期刊影响力 0.60
                  
                </span>
              </div>
            </div>
          </div>
        </article>
        
      </div>
      </div><!-- End of collapsible content -->
    </div>
  </section>
  

  <!-- Similarity Recommendations Section -->
  <main class="py-5 md:py-8">
    <div class="content-container">
      <!-- Collapsible Header -->
      <div class="mb-3 md:mb-4">
        <button id="btn-similarity-works" onclick="toggleSection('similarity-works')"
                class="w-full text-left text-lg md:text-xl font-bold text-text-primary flex items-center justify-between group hover:text-accent transition-colors">
          <span class="flex items-center gap-2 flex-1 min-w-0">
            <svg class="w-5 h-5 flex-shrink-0 text-accent" fill="none" stroke="currentColor" viewBox="0 0 24 24">
              <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M9 19v-6a2 2 0 00-2-2H5a2 2 0 00-2 2v6a2 2 0 002 2h2a2 2 0 002-2zm0 0V9a2 2 0 012-2h2a2 2 0 012 2v10m-6 0a2 2 0 002 2h2a2 2 0 002-2m0 0V5a2 2 0 012-2h2a2 2 0 012 2v14a2 2 0 01-2 2h-2a2 2 0 01-2-2z"/>
            </svg>
            <span class="truncate">相似度推荐</span>
          </span>
          <span class="flex items-center gap-1 md:gap-2 flex-shrink-0 ml-2">
            <span class="hidden md:inline text-xs font-normal text-text-secondary">按相关性评分排序，共 30 篇</span>
            <span class="hidden md:inline text-sm font-normal text-text-secondary group-hover:text-accent" id="hint-similarity-works">点击收起</span>
            <svg class="w-5 h-5 md:w-4 md:h-4 text-text-secondary group-hover:text-accent transform transition-transform rotate-180" id="icon-similarity-works" fill="none" stroke="currentColor" viewBox="0 0 24 24">
              <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
            </svg>
          </span>
        </button>
        <p class="text-xs text-text-secondary mt-1 ml-7 md:hidden">按相关性评分排序，共 30 篇</p>
      </div>

      <!-- Collapsible Content -->
      <div id="similarity-works" class="section-expand expanded">
      <div class="space-y-4 md:space-y-5">
        
        <article class="bg-bg-card rounded-lg border border-border-color overflow-hidden hover:border-accent/50 transition-colors">
          <div class="p-4 md:p-5">
            <!-- Title and metadata -->
            <div class="mb-3">
              <!-- Tags row -->
              <div class="flex items-center gap-2 mb-2 flex-wrap">
                <span class="inline-flex items-center px-2.5 py-0.5 rounded text-xs font-medium
                  bg-green-100 text-green-700 border border-green-300
                  ">
                  必读
                </span>
                <span class="text-xs text-text-secondary">评分 0.82</span>
                <span class="text-xs text-text-secondary">·</span>
                <span class="text-xs text-text-secondary">crossref</span>
                <!-- Date: visible on mobile, hidden on md+ -->
                <span class="text-xs text-text-secondary md:hidden">· 2025-11-29</span>
              </div>
              <!-- Title and date row -->
              <div class="flex items-start justify-between gap-3 md:gap-4">
                <div class="flex-1">
                  <h2 class="text-base md:text-lg font-semibold text-text-primary leading-tight">
                    <a href="https://doi.org/10.1016/j.inffus.2025.104007" target="_blank" rel="noopener" class="hover:text-accent transition-colors">
                      GLUE3D: General Language Understanding Evaluation for 3D Point Clouds
                    </a>
                  </h2>
                  
                  <p class="text-sm text-text-secondary mt-0.5 leading-snug">GLUE3D：面向三维点云的通用语言理解评测</p>
                  
                </div>
                <!-- Date: hidden on mobile, visible on md+ -->
                <div class="hidden md:block text-right text-sm text-text-secondary flex-shrink-0">
                  <div class="font-medium">2025-11-29</div>
                </div>
              </div>
              <!-- Venue: below title -->
              <div class="mt-1 text-xs text-text-secondary" title="Information Fusion">
                Information Fusion
                
                  <span class="ml-1 text-blue-600">(IF: 15.5)</span>
                
              </div>
            </div>

            <!-- Authors -->
            <p class="text-sm text-text-secondary mb-3">
              Giorgio Mariani，Alessandro Raganato，Simone Melzi，Gabriella Pasi
            </p>

            <!-- DOI -->
            <div class="flex flex-wrap gap-x-4 gap-y-1 text-xs text-text-secondary mb-4">
              
              <span class="flex items-center">
                <span class="font-medium mr-1">DOI:</span>
                <a href="https://doi.org/10.1016/j.inffus.2025.104007" target="_blank" rel="noopener" class="text-accent hover:text-accent-hover hover:underline">10.1016/j.inffus.2025.104007</a>
              </span>
              
            </div>

            <!-- Abstract (collapsible) -->
            
            <div class="bg-bg-hover/50 rounded-lg p-4 mb-4 border border-border-color">
              <button id="btn-abstract-1" onclick="toggleSection('abstract-1')"
                      class="w-full text-left text-sm font-medium text-text-primary flex items-center justify-between">
                <span class="flex items-center">
                  <svg class="w-4 h-4 mr-1.5 text-text-secondary" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                    <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M9 12h6m-6 4h6m2 5H7a2 2 0 01-2-2V5a2 2 0 012-2h5.586a1 1 0 01.707.293l5.414 5.414a1 1 0 01.293.707V19a2 2 0 01-2 2z"/>
                  </svg>
                  原文摘要
                </span>
                <svg class="w-4 h-4 text-text-secondary transform transition-transform" id="icon-abstract-1" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                  <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
                </svg>
              </button>
              <div id="abstract-1" class="section-expand collapsed">
                <p class="text-sm text-text-secondary mt-3 leading-relaxed">Multimodal Large Language Models have achieved impressive results on text and image benchmarks, yet their capacity to ground language in 3D geometry is still largely unexplored. Existing 3D evaluations are either confined to specialised domains, such as indoor scans, or hampered by poor texture fidelity, and none allow a fair, modality-aligned comparison with the 2D counterparts. Without a rigorous benchmark, it remains unclear whether current 3D-aware models genuinely grasp shape, colour, pose, and quantity, or merely echo memorised textual priors.</p>
              </div>
            </div>
            

            <!-- AI Summary -->
            
            <div class="bg-accent/10 rounded-lg p-4 mb-4 border border-accent/30">
              <h3 class="text-sm font-semibold text-accent mb-3 flex items-center">
                <svg class="w-4 h-4 mr-1.5" fill="currentColor" viewBox="0 0 24 24">
                  <path d="M12 2a2 2 0 012 2c0 .74-.4 1.39-1 1.73V7h1a7 7 0 017 7h1a1 1 0 011 1v3a1 1 0 01-1 1h-1v1a2 2 0 01-2 2H5a2 2 0 01-2-2v-1H2a1 1 0 01-1-1v-3a1 1 0 011-1h1a7 7 0 017-7h1V5.73c-.6-.34-1-.99-1-1.73a2 2 0 012-2z"/>
                </svg>
                AI 总结
              </h3>
              <div class="space-y-2 text-sm text-text-primary">
                <p><span class="font-medium text-accent">研究问题：</span>如何公平评估多模态大模型对3D点云的语言理解能力</p>
                <p><span class="font-medium text-accent">研究方法：</span>构建跨模态对齐的3D-文本基准GLUE3D，涵盖形状、颜色、姿态、数量任务</p>
                <p><span class="font-medium text-accent">主要发现：</span>现有3D模型多依赖文本先记忆而非真正理解几何，2D/3D性能差距显著</p>
                <p><span class="font-medium text-accent">创新点：</span>首个与2D基准对齐的通用3D语言理解评测集，支持公平跨模态比较</p>
                
                <p><span class="font-medium text-accent">相关性：</span>为社区提供统一3D语言评测标准，推动真实3D场景理解与多模态融合研究</p>
                
              </div>

              <div class="mt-3 pt-3 border-t border-accent/30">
                <button id="btn-detail-1" onclick="toggleSection('detail-1')"
                        class="text-xs text-accent hover:text-accent-hover font-medium flex items-center">
                  <svg class="w-3 h-3 mr-1" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                    <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
                  </svg>
                  查看详细分析
                </button>
                <div id="detail-1" class="section-expand collapsed mt-3">
                  <div class="bg-bg-card rounded-lg p-4 text-sm text-text-primary space-y-3 border border-border-color">
                    <p><span class="font-medium text-text-primary">研究背景：</span>多模态大语言模型在文本与图像任务上表现亮眼，却鲜少被系统性地检验其将语言‘接地’到三维几何的能力。现有3D基准要么局限于室内扫描等狭窄领域，要么受纹理分辨率不足困扰，更无法与2D模态进行公平对照。</p>
                    <p><span class="font-medium text-text-primary">方法详情：</span>作者提出GLUE3D，一个面向通用语言理解的3D点云评测套件，涵盖形状、颜色、姿态、数量等语义维度，并设计模态对齐协议，使同一问题可零改动地用于2D图像与3D点云。基准由人工校验的Objaverse子集构建，提供多语言模板与平衡标签分布，支持zero-shot、few-shot及微调三种评估方式。</p>
                    <p><span class="font-medium text-text-primary">研究结果：</span>实验显示，主流3D-aware VLMs在GLUE3D上的平均准确率比其2D版本低15–30%，揭示其并未真正理解几何而只是复现文本先验。定量分析表明，颜色与数量任务差距最大，姿态次之，形状最小，暗示模型对空间关系的敏感性排序。该结果首次给出3D-2D可对照的绩效落差，为后续研究提供明确改进空间。</p>
                    <p><span class="font-medium text-text-primary">局限性：</span>基准目前仅覆盖对象级点云，尚未引入场景级上下文与动态交互；语言模板虽多，但仍可能遗漏特定领域术语；由于Objaverse以合成模型为主，真实扫描的域差异尚未验证。</p>
                    
                    <p><span class="font-medium text-text-primary">未来方向：</span>未来可扩展GLUE3D至室外场景及动态点云序列，并引入机器人交互问答，以考察模型在真实环境下的3D语言落地能力。</p>
                    
                    <p class="text-accent"><span class="font-medium">研究相关性：</span>若研究者关注多模态大模型在3D几何上的可解释性、公平评测或跨模态对齐，本论文提供了可直接使用的基准与诊断工具。</p>
                  </div>
                </div>
              </div>
            </div>
            

            <!-- Score details -->
            <div class="pt-4 border-t border-border-color">
              <div class="flex flex-wrap gap-3 text-xs">
                <span class="px-2 py-1 bg-bg-hover rounded text-text-secondary">相似度 0.80</span>
                <span class="px-2 py-1 bg-bg-hover rounded text-text-secondary">
                  期刊影响力 0.87
                  
                    <span class="ml-1 text-blue-600">(IF: 15.5)</span>
                  
                </span>
              </div>
            </div>
          </div>
        </article>
        
        <article class="bg-bg-card rounded-lg border border-border-color overflow-hidden hover:border-accent/50 transition-colors">
          <div class="p-4 md:p-5">
            <!-- Title and metadata -->
            <div class="mb-3">
              <!-- Tags row -->
              <div class="flex items-center gap-2 mb-2 flex-wrap">
                <span class="inline-flex items-center px-2.5 py-0.5 rounded text-xs font-medium
                  bg-green-100 text-green-700 border border-green-300
                  ">
                  必读
                </span>
                <span class="text-xs text-text-secondary">评分 0.81</span>
                <span class="text-xs text-text-secondary">·</span>
                <span class="text-xs text-text-secondary">crossref</span>
                <!-- Date: visible on mobile, hidden on md+ -->
                <span class="text-xs text-text-secondary md:hidden">· 2025-11-29</span>
              </div>
              <!-- Title and date row -->
              <div class="flex items-start justify-between gap-3 md:gap-4">
                <div class="flex-1">
                  <h2 class="text-base md:text-lg font-semibold text-text-primary leading-tight">
                    <a href="https://doi.org/10.1016/j.inffus.2025.104003" target="_blank" rel="noopener" class="hover:text-accent transition-colors">
                      The Duality of Generative AI and Reinforcement Learning in Robotics: A Review
                    </a>
                  </h2>
                  
                  <p class="text-sm text-text-secondary mt-0.5 leading-snug">生成式人工智能与强化学习在机器人学中的双重性：综述</p>
                  
                </div>
                <!-- Date: hidden on mobile, visible on md+ -->
                <div class="hidden md:block text-right text-sm text-text-secondary flex-shrink-0">
                  <div class="font-medium">2025-11-29</div>
                </div>
              </div>
              <!-- Venue: below title -->
              <div class="mt-1 text-xs text-text-secondary" title="Information Fusion">
                Information Fusion
                
                  <span class="ml-1 text-blue-600">(IF: 15.5)</span>
                
              </div>
            </div>

            <!-- Authors -->
            <p class="text-sm text-text-secondary mb-3">
              Angelo Moroncelli，Vishal Soni，Marco Forgione，Dario Piga，Blerina Spahiu 等
            </p>

            <!-- DOI -->
            <div class="flex flex-wrap gap-x-4 gap-y-1 text-xs text-text-secondary mb-4">
              
              <span class="flex items-center">
                <span class="font-medium mr-1">DOI:</span>
                <a href="https://doi.org/10.1016/j.inffus.2025.104003" target="_blank" rel="noopener" class="text-accent hover:text-accent-hover hover:underline">10.1016/j.inffus.2025.104003</a>
              </span>
              
            </div>

            <!-- Abstract (collapsible) -->
            
            <div class="bg-bg-hover/50 rounded-lg p-4 mb-4 border border-border-color">
              <button id="btn-abstract-2" onclick="toggleSection('abstract-2')"
                      class="w-full text-left text-sm font-medium text-text-primary flex items-center justify-between">
                <span class="flex items-center">
                  <svg class="w-4 h-4 mr-1.5 text-text-secondary" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                    <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M9 12h6m-6 4h6m2 5H7a2 2 0 01-2-2V5a2 2 0 012-2h5.586a1 1 0 01.707.293l5.414 5.414a1 1 0 01.293.707V19a2 2 0 01-2 2z"/>
                  </svg>
                  原文摘要
                </span>
                <svg class="w-4 h-4 text-text-secondary transform transition-transform" id="icon-abstract-2" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                  <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
                </svg>
              </button>
              <div id="abstract-2" class="section-expand collapsed">
                <p class="text-sm text-text-secondary mt-3 leading-relaxed">Recently, generative AI and reinforcement learning (RL) have been redefining what is possible for AI agents that take information flows as input and produce intelligent behavior. As a result, we are seeing similar advancements in embodied AI and robotics for control policy generation. Our review paper examines the integration of generative AI models with RL to advance robotics. Our primary focus is on the duality between generative AI and RL for robotics downstream tasks. Specifically, we investigate: (1) The role of prominent generative AI tools as modular priors for multi-modal input fusion in RL tasks. (2) How RL can train, fine-tune and distill generative models for policy generation, such as VLA models, similarly to RL applications in large language models. We then propose a new taxonomy based on a considerable amount of selected papers.</p>
              </div>
            </div>
            

            <!-- AI Summary -->
            
            <div class="bg-accent/10 rounded-lg p-4 mb-4 border border-accent/30">
              <h3 class="text-sm font-semibold text-accent mb-3 flex items-center">
                <svg class="w-4 h-4 mr-1.5" fill="currentColor" viewBox="0 0 24 24">
                  <path d="M12 2a2 2 0 012 2c0 .74-.4 1.39-1 1.73V7h1a7 7 0 017 7h1a1 1 0 011 1v3a1 1 0 01-1 1h-1v1a2 2 0 01-2 2H5a2 2 0 01-2-2v-1H2a1 1 0 01-1-1v-3a1 1 0 011-1h1a7 7 0 017-7h1V5.73c-.6-.34-1-.99-1-1.73a2 2 0 012-2z"/>
                </svg>
                AI 总结
              </h3>
              <div class="space-y-2 text-sm text-text-primary">
                <p><span class="font-medium text-accent">研究问题：</span>如何系统梳理生成式AI与强化学习在机器人下游任务中的互补作用与融合方式。</p>
                <p><span class="font-medium text-accent">研究方法：</span>对近年文献进行大规模筛选，提出按“生成模型作RL先验”与“RL训练生成策略”双重维度的新分类法。</p>
                <p><span class="font-medium text-accent">主要发现：</span>生成式AI可充当多模态先验提升RL样本效率，RL则能微调或蒸馏生成模型形成鲁棒机器人策略。</p>
                <p><span class="font-medium text-accent">创新点：</span>首次以“双重性”视角建立生成-AI×RL机器人研究分类，并揭示二者闭环融合的设计原则与前沿方向。</p>
                
                <p><span class="font-medium text-accent">相关性：</span>为机器人学者提供生成式AI与RL协同的路线图，加速多模态感知、数据效率和策略泛化的突破。</p>
                
              </div>

              <div class="mt-3 pt-3 border-t border-accent/30">
                <button id="btn-detail-2" onclick="toggleSection('detail-2')"
                        class="text-xs text-accent hover:text-accent-hover font-medium flex items-center">
                  <svg class="w-3 h-3 mr-1" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                    <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
                  </svg>
                  查看详细分析
                </button>
                <div id="detail-2" class="section-expand collapsed mt-3">
                  <div class="bg-bg-card rounded-lg p-4 text-sm text-text-primary space-y-3 border border-border-color">
                    <p><span class="font-medium text-text-primary">研究背景：</span>Unable to extract background</p>
                    <p><span class="font-medium text-text-primary">方法详情：</span>Unable to extract methodology details</p>
                    <p><span class="font-medium text-text-primary">研究结果：</span>Unable to extract results</p>
                    <p><span class="font-medium text-text-primary">局限性：</span>Unable to extract limitations</p>
                    
                    <p class="text-accent"><span class="font-medium">研究相关性：</span>{&#34;background&#34;:&#34;机器人学长期受困于高维感知-动作耦合与样本效率瓶颈，生成式AI的涌现为复杂多模态先验建模提供新思路，而强化学习擅于在交互中优化策略，两者互补却缺乏系统梳理。&#34;,&#34;methodology_details&#34;:&#34;作者系统检索近三年将生成模型与RL结合的机器人论文，按“生成模型作RL先验”与“RL训练生成策略”两条主线筛选百余篇代表作；对每条主线再按输入模态、网络架构、训练范</p>
                  </div>
                </div>
              </div>
            </div>
            

            <!-- Score details -->
            <div class="pt-4 border-t border-border-color">
              <div class="flex flex-wrap gap-3 text-xs">
                <span class="px-2 py-1 bg-bg-hover rounded text-text-secondary">相似度 0.80</span>
                <span class="px-2 py-1 bg-bg-hover rounded text-text-secondary">
                  期刊影响力 0.87
                  
                    <span class="ml-1 text-blue-600">(IF: 15.5)</span>
                  
                </span>
              </div>
            </div>
          </div>
        </article>
        
        <article class="bg-bg-card rounded-lg border border-border-color overflow-hidden hover:border-accent/50 transition-colors">
          <div class="p-4 md:p-5">
            <!-- Title and metadata -->
            <div class="mb-3">
              <!-- Tags row -->
              <div class="flex items-center gap-2 mb-2 flex-wrap">
                <span class="inline-flex items-center px-2.5 py-0.5 rounded text-xs font-medium
                  bg-green-100 text-green-700 border border-green-300
                  ">
                  必读
                </span>
                <span class="text-xs text-text-secondary">评分 0.80</span>
                <span class="text-xs text-text-secondary">·</span>
                <span class="text-xs text-text-secondary">crossref</span>
                <!-- Date: visible on mobile, hidden on md+ -->
                <span class="text-xs text-text-secondary md:hidden">· 2025-11-29</span>
              </div>
              <!-- Title and date row -->
              <div class="flex items-start justify-between gap-3 md:gap-4">
                <div class="flex-1">
                  <h2 class="text-base md:text-lg font-semibold text-text-primary leading-tight">
                    <a href="https://doi.org/10.1016/j.isprsjprs.2025.11.026" target="_blank" rel="noopener" class="hover:text-accent transition-colors">
                      M3FNet: Multi-modal multi-temporal multi-scale data fusion network for tree species composition mapping
                    </a>
                  </h2>
                  
                  <p class="text-sm text-text-secondary mt-0.5 leading-snug">M3FNet：用于树种组成制图的多模态多时相多尺度数据融合网络</p>
                  
                </div>
                <!-- Date: hidden on mobile, visible on md+ -->
                <div class="hidden md:block text-right text-sm text-text-secondary flex-shrink-0">
                  <div class="font-medium">2025-11-29</div>
                </div>
              </div>
              <!-- Venue: below title -->
              <div class="mt-1 text-xs text-text-secondary" title="ISPRS Journal of Photogrammetry and Remote Sensing">
                ISPRS Journal of Photogrammetry and Remote Sensing
                
                  <span class="ml-1 text-blue-600">(IF: 12.2)</span>
                
              </div>
            </div>

            <!-- Authors -->
            <p class="text-sm text-text-secondary mb-3">
              Yuwei Cao，Nicholas C. Coops，Brent A. Murray，Ian Sinclair，Robere-McGugan Geordie
            </p>

            <!-- DOI -->
            <div class="flex flex-wrap gap-x-4 gap-y-1 text-xs text-text-secondary mb-4">
              
              <span class="flex items-center">
                <span class="font-medium mr-1">DOI:</span>
                <a href="https://doi.org/10.1016/j.isprsjprs.2025.11.026" target="_blank" rel="noopener" class="text-accent hover:text-accent-hover hover:underline">10.1016/j.isprsjprs.2025.11.026</a>
              </span>
              
            </div>

            <!-- Abstract (collapsible) -->
            
            <div class="bg-bg-hover/50 rounded-lg p-4 mb-4 border border-border-color">
              <button id="btn-abstract-3" onclick="toggleSection('abstract-3')"
                      class="w-full text-left text-sm font-medium text-text-primary flex items-center justify-between">
                <span class="flex items-center">
                  <svg class="w-4 h-4 mr-1.5 text-text-secondary" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                    <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M9 12h6m-6 4h6m2 5H7a2 2 0 01-2-2V5a2 2 0 012-2h5.586a1 1 0 01.707.293l5.414 5.414a1 1 0 01.293.707V19a2 2 0 01-2 2z"/>
                  </svg>
                  原文摘要
                </span>
                <svg class="w-4 h-4 text-text-secondary transform transition-transform" id="icon-abstract-3" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                  <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
                </svg>
              </button>
              <div id="abstract-3" class="section-expand collapsed">
                <p class="text-sm text-text-secondary mt-3 leading-relaxed">Accurate estimation and mapping of t ree s pecies c omposition (TSC) is crucial for sustainable forest management. Recent advances in Light Detection and Ranging (lidar) technology and the availability of moderate spatial resolution, surface reflectance time series passive optical imagery offer scalable and efficient approaches for automated TSC estimation. In this research we develop a novel deep learning framework, M3F-Net (Multi-modal, Multi-temporal, and Multi-scale Fusion Network), that integrates multi-temporal Sentinel-2 (S2) imagery and single photon lidar (SPL) data to estimate TSC for nine common species across the 630,000-hectare Romeo Malette Forest in Ontario, Canada. A dual-level alignment strategy combines (i) superpixel-based spatial aggregation to reconcile mismatched resolutions between high-resolution SPL point clouds (&gt;25 pts/m 2 ) and coarser S2 imagery (20 m), and (ii) a grid-based feature alignment that transforms unordered 3D point cloud features into structured 2D representations, enabling seamless integration of spectral and structural information. Within this aligned space, a multi-level Mamba-Fusion module jointly models multi-scale spatial patterns and seasonal dynamics through selective state-space modelling, efficiently capturing long-range dependencies while filtering redundant information. The framework achieves an R 2 score of 0.676, outperforming existing point cloud-based methods by 6% in TSC estimation. For leading species classification, our results are 6% better in terms of weighted F1, using either the TSC-based method or the standalone leading species classification method. Addition of seasonal S2 imagery added a 10% R 2 gain compared to the SPL-only mode. These results underscore the potential of fusing multi-modal and multi-temporal data with deep learning for scalable, high-accurate TSC estimation, offering a robust tool for large-scale management applications.</p>
              </div>
            </div>
            

            <!-- AI Summary -->
            
            <div class="bg-accent/10 rounded-lg p-4 mb-4 border border-accent/30">
              <h3 class="text-sm font-semibold text-accent mb-3 flex items-center">
                <svg class="w-4 h-4 mr-1.5" fill="currentColor" viewBox="0 0 24 24">
                  <path d="M12 2a2 2 0 012 2c0 .74-.4 1.39-1 1.73V7h1a7 7 0 017 7h1a1 1 0 011 1v3a1 1 0 01-1 1h-1v1a2 2 0 01-2 2H5a2 2 0 01-2-2v-1H2a1 1 0 01-1-1v-3a1 1 0 011-1h1a7 7 0 017-7h1V5.73c-.6-.34-1-.99-1-1.73a2 2 0 012-2z"/>
                </svg>
                AI 总结
              </h3>
              <div class="space-y-2 text-sm text-text-primary">
                <p><span class="font-medium text-accent">研究问题：</span>如何在大尺度林区准确、自动地估算并制图九种主要树种组成</p>
                <p><span class="font-medium text-accent">研究方法：</span>提出M3FNet，融合多时相Sentinel-2与单光子激光雷达，用超像素-网格对齐与Mamba-Fusion模块建模</p>
                <p><span class="font-medium text-accent">主要发现：</span>TSC估算R²达0.676，领先种F1提升6%，加入季节影像比仅用激光雷达R²增10%</p>
                <p><span class="font-medium text-accent">创新点：</span>双级对齐将无序3D点云转为2D结构，并首次用选择性状态空间模型联合捕捉多尺度-季节依赖</p>
                
                <p><span class="font-medium text-accent">相关性：</span>示范多模态时序深度学习可扩展提升大区域森林资源调查精度，对可持续管理具直接应用价值</p>
                
              </div>

              <div class="mt-3 pt-3 border-t border-accent/30">
                <button id="btn-detail-3" onclick="toggleSection('detail-3')"
                        class="text-xs text-accent hover:text-accent-hover font-medium flex items-center">
                  <svg class="w-3 h-3 mr-1" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                    <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
                  </svg>
                  查看详细分析
                </button>
                <div id="detail-3" class="section-expand collapsed mt-3">
                  <div class="bg-bg-card rounded-lg p-4 text-sm text-text-primary space-y-3 border border-border-color">
                    <p><span class="font-medium text-text-primary">研究背景：</span>准确掌握森林树种组成(TSC)是可持续经营的基础，但传统野外调查难以覆盖大尺度。激光雷达与Sentinel-2时间序列提供了可扩展的多模态数据，却面临分辨率、时空结构不一致等融合难题。</p>
                    <p><span class="font-medium text-text-primary">方法详情：</span>作者提出M3FNet，通过超像素空间聚合与网格化特征对齐，将&gt;25 pts/m²的单光子激光雷达点云与20 m Sentinel-2影像统一至相同2D空间。随后嵌入多层级Mamba-Fusion模块，以选择性状态空间模型同步学习季节光谱动态和多尺度结构特征，实现长程依赖建模并抑制冗余信息。</p>
                    <p><span class="font-medium text-text-primary">研究结果：</span>在加拿大安大略63万公顷Romeo Malette森林，M3FNet对九种主要树种的TSC估计R²达0.676，比纯点云方法提升6%；优势树种分类加权F1再提高6%。引入Sentinel-2季节序列后，相较仅使用激光雷达的R²额外提升10%，证明多模态时序融合可显著增强大尺度树种制图精度。</p>
                    <p><span class="font-medium text-text-primary">局限性：</span>研究仅覆盖北方森林单一生态区，树种多样性与立地条件有限；Mamba-Fusion的超参数与对齐策略对其他地区可迁移性未验证；Sentinel-2 20 m分辨率可能低估小尺度混交，且未探讨不同年份物候差异对季节模型的影响。</p>
                    
                    <p><span class="font-medium text-text-primary">未来方向：</span>未来可在全球不同生态带测试M3FNet的通用性，并耦合10 m或更高分辨率光学数据以捕捉更小尺度混交；同时引入自监督预训练降低对大规模野外样本的依赖。</p>
                    
                    <p class="text-accent"><span class="font-medium">研究相关性：</span>该文提供了一套端到端的多模态时序深度学习框架，为需要整合激光雷达与光学影像进行大尺度森林资源调查、树种多样性监测或碳汇估算的研究者带来可直接借鉴的数据对齐与融合策略。</p>
                  </div>
                </div>
              </div>
            </div>
            

            <!-- Score details -->
            <div class="pt-4 border-t border-border-color">
              <div class="flex flex-wrap gap-3 text-xs">
                <span class="px-2 py-1 bg-bg-hover rounded text-text-secondary">相似度 0.80</span>
                <span class="px-2 py-1 bg-bg-hover rounded text-text-secondary">
                  期刊影响力 0.80
                  
                    <span class="ml-1 text-blue-600">(IF: 12.2)</span>
                  
                </span>
              </div>
            </div>
          </div>
        </article>
        
        <article class="bg-bg-card rounded-lg border border-border-color overflow-hidden hover:border-accent/50 transition-colors">
          <div class="p-4 md:p-5">
            <!-- Title and metadata -->
            <div class="mb-3">
              <!-- Tags row -->
              <div class="flex items-center gap-2 mb-2 flex-wrap">
                <span class="inline-flex items-center px-2.5 py-0.5 rounded text-xs font-medium
                  bg-green-100 text-green-700 border border-green-300
                  ">
                  必读
                </span>
                <span class="text-xs text-text-secondary">评分 0.80</span>
                <span class="text-xs text-text-secondary">·</span>
                <span class="text-xs text-text-secondary">arxiv</span>
                <!-- Date: visible on mobile, hidden on md+ -->
                <span class="text-xs text-text-secondary md:hidden">· 2025-11-26</span>
              </div>
              <!-- Title and date row -->
              <div class="flex items-start justify-between gap-3 md:gap-4">
                <div class="flex-1">
                  <h2 class="text-base md:text-lg font-semibold text-text-primary leading-tight">
                    <a href="https://arxiv.org/abs/2511.21606v1" target="_blank" rel="noopener" class="hover:text-accent transition-colors">
                      ReSAM: Refine, Requery, and Reinforce: Self-Prompting Point-Supervised Segmentation for Remote Sensing Images
                    </a>
                  </h2>
                  
                  <p class="text-sm text-text-secondary mt-0.5 leading-snug">ReSAM：精炼、重查询与强化——面向遥感图像的自提示点监督分割</p>
                  
                </div>
                <!-- Date: hidden on mobile, visible on md+ -->
                <div class="hidden md:block text-right text-sm text-text-secondary flex-shrink-0">
                  <div class="font-medium">2025-11-26</div>
                </div>
              </div>
              <!-- Venue: below title -->
              <div class="mt-1 text-xs text-text-secondary" title="arXiv">
                arXiv
                
              </div>
            </div>

            <!-- Authors -->
            <p class="text-sm text-text-secondary mb-3">
              M. Naseer Subhani
            </p>

            <!-- DOI -->
            <div class="flex flex-wrap gap-x-4 gap-y-1 text-xs text-text-secondary mb-4">
              
              <span class="flex items-center">
                <span class="font-medium mr-1">ID:</span>
                <span>http://arxiv.org/abs/2511.21606v1</span>
              </span>
              
            </div>

            <!-- Abstract (collapsible) -->
            
            <div class="bg-bg-hover/50 rounded-lg p-4 mb-4 border border-border-color">
              <button id="btn-abstract-4" onclick="toggleSection('abstract-4')"
                      class="w-full text-left text-sm font-medium text-text-primary flex items-center justify-between">
                <span class="flex items-center">
                  <svg class="w-4 h-4 mr-1.5 text-text-secondary" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                    <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M9 12h6m-6 4h6m2 5H7a2 2 0 01-2-2V5a2 2 0 012-2h5.586a1 1 0 01.707.293l5.414 5.414a1 1 0 01.293.707V19a2 2 0 01-2 2z"/>
                  </svg>
                  原文摘要
                </span>
                <svg class="w-4 h-4 text-text-secondary transform transition-transform" id="icon-abstract-4" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                  <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
                </svg>
              </button>
              <div id="abstract-4" class="section-expand collapsed">
                <p class="text-sm text-text-secondary mt-3 leading-relaxed">Interactive segmentation models such as the Segment Anything Model (SAM) have demonstrated remarkable generalization on natural images, but perform suboptimally on remote sensing imagery (RSI) due to severe domain shift and the scarcity of dense annotations. To address this, we propose a self-prompting, point-supervised framework that adapts SAM to RSIs using only sparse point annotations. Our method employs a Refine-Requery-Reinforce loop, where coarse pseudo-masks are generated from initial points (Refine), improved with self-constructed box prompts (Requery), and embeddings are aligned across iterations to reduce confirmation bias (Reinforce). Without relying on full-mask supervision, our approach progressively enhances SAM&#39;s segmentation quality and domain robustness through self-guided prompt adaptation . We evaluate our proposed method on three RSI benchmark datasets, including WHU, HRSID, and NWPU VHR-10, showing that our method consistently surpasses pretrained SAM and recent point-supervised segmentation methods. Our results demonstrate that self-prompting and semantic alignment provide an efficient path towards scalable, point-level adaptation of foundation segmentation models for remote sensing applications.</p>
              </div>
            </div>
            

            <!-- AI Summary -->
            
            <div class="bg-accent/10 rounded-lg p-4 mb-4 border border-accent/30">
              <h3 class="text-sm font-semibold text-accent mb-3 flex items-center">
                <svg class="w-4 h-4 mr-1.5" fill="currentColor" viewBox="0 0 24 24">
                  <path d="M12 2a2 2 0 012 2c0 .74-.4 1.39-1 1.73V7h1a7 7 0 017 7h1a1 1 0 011 1v3a1 1 0 01-1 1h-1v1a2 2 0 01-2 2H5a2 2 0 01-2-2v-1H2a1 1 0 01-1-1v-3a1 1 0 011-1h1a7 7 0 017-7h1V5.73c-.6-.34-1-.99-1-1.73a2 2 0 012-2z"/>
                </svg>
                AI 总结
              </h3>
              <div class="space-y-2 text-sm text-text-primary">
                <p><span class="font-medium text-accent">研究问题：</span>如何让SAM在仅有稀疏点标注的遥感影像上获得高质量分割。</p>
                <p><span class="font-medium text-accent">研究方法：</span>Refine-Requery-Reinforce自提示循环：点生成粗伪掩膜→自构box再查询→嵌入对齐强化。</p>
                <p><span class="font-medium text-accent">主要发现：</span>在WHU、HRSID、NWPU VHR-10上持续超越预训练SAM与最新点监督方法。</p>
                <p><span class="font-medium text-accent">创新点：</span>首次将自提示与跨迭代语义对齐引入点监督框架，实现无全掩膜SAM域适应。</p>
                
                <p><span class="font-medium text-accent">相关性：</span>为遥感领域提供可扩展的点级自适应方案，降低密集标注依赖并提升基础模型实用性。</p>
                
              </div>

              <div class="mt-3 pt-3 border-t border-accent/30">
                <button id="btn-detail-4" onclick="toggleSection('detail-4')"
                        class="text-xs text-accent hover:text-accent-hover font-medium flex items-center">
                  <svg class="w-3 h-3 mr-1" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                    <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
                  </svg>
                  查看详细分析
                </button>
                <div id="detail-4" class="section-expand collapsed mt-3">
                  <div class="bg-bg-card rounded-lg p-4 text-sm text-text-primary space-y-3 border border-border-color">
                    <p><span class="font-medium text-text-primary">研究背景：</span>Segment Anything Model (SAM) 在通用自然图像上表现优异，却因遥感影像与自然影像间的显著域偏移及遥感密集标注稀缺，难以直接迁移。作者希望仅用最易获取的稀疏点标注，让 SAM 在遥感场景下也能输出高质量分割，从而解决标注成本高与域适应难题。</p>
                    <p><span class="font-medium text-text-primary">方法详情：</span>论文提出 Refine-Requery-Reinforce 自提示循环：先用初始点生成粗伪掩膜(Refine)，再用伪掩膜自构建方框提示喂回 SAM 以精修结果(Requery)，最后通过跨迭代嵌入对齐抑制确认偏差并更新模型(Reinforce)。整个框架无需完整掩膜监督，仅依赖点标签即可迭代式提升 SAM 的伪标签质量与域鲁棒性，实现自引导的提示适应。</p>
                    <p><span class="font-medium text-text-primary">研究结果：</span>在 WHU、HRSID、NWPU VHR-10 三个遥感基准上，ReSAM 一致优于原始预训练 SAM 及最新点监督分割方法，显著提升了 IoU 与边界精度。实验表明，自提示与语义对齐策略能有效弥补域偏移，验证了仅用稀疏点即可实现大模型遥感适应的可行性。</p>
                    <p><span class="font-medium text-text-primary">局限性：</span>方法仍依赖初始点标注的准确性，若点位置偏差大可能放大伪标签噪声；迭代自对齐虽抑制确认偏差，但训练开销高于一次性微调，且未在更多传感器或类别上验证泛化能力。</p>
                    
                    <p><span class="font-medium text-text-primary">未来方向：</span>未来可探索与无点标注的弱监督或主动学习结合，进一步降低人工输入；同时引入多尺度时序遥感数据，提升对复杂场景与变化检测任务的适应性。</p>
                    
                    <p class="text-accent"><span class="font-medium">研究相关性：</span>若研究者关注遥感影像分割、SAM 域适应或弱/点监督学习，本文提供了一种低成本、可扩展的自提示框架与详尽实验基准，可直接借鉴或扩展至其他大型视觉基础模型。</p>
                  </div>
                </div>
              </div>
            </div>
            

            <!-- Score details -->
            <div class="pt-4 border-t border-border-color">
              <div class="flex flex-wrap gap-3 text-xs">
                <span class="px-2 py-1 bg-bg-hover rounded text-text-secondary">相似度 0.85</span>
                <span class="px-2 py-1 bg-bg-hover rounded text-text-secondary">
                  期刊影响力 0.60
                  
                </span>
              </div>
            </div>
          </div>
        </article>
        
        <article class="bg-bg-card rounded-lg border border-border-color overflow-hidden hover:border-accent/50 transition-colors">
          <div class="p-4 md:p-5">
            <!-- Title and metadata -->
            <div class="mb-3">
              <!-- Tags row -->
              <div class="flex items-center gap-2 mb-2 flex-wrap">
                <span class="inline-flex items-center px-2.5 py-0.5 rounded text-xs font-medium
                  bg-green-100 text-green-700 border border-green-300
                  ">
                  必读
                </span>
                <span class="text-xs text-text-secondary">评分 0.80</span>
                <span class="text-xs text-text-secondary">·</span>
                <span class="text-xs text-text-secondary">crossref</span>
                <!-- Date: visible on mobile, hidden on md+ -->
                <span class="text-xs text-text-secondary md:hidden">· 2025-11-29</span>
              </div>
              <!-- Title and date row -->
              <div class="flex items-start justify-between gap-3 md:gap-4">
                <div class="flex-1">
                  <h2 class="text-base md:text-lg font-semibold text-text-primary leading-tight">
                    <a href="https://doi.org/10.1016/j.neunet.2025.108373" target="_blank" rel="noopener" class="hover:text-accent transition-colors">
                      Label Noise Learning Based SAR Target Classification Method
                    </a>
                  </h2>
                  
                  <p class="text-sm text-text-secondary mt-0.5 leading-snug">基于标签噪声学习的SAR目标分类方法</p>
                  
                </div>
                <!-- Date: hidden on mobile, visible on md+ -->
                <div class="hidden md:block text-right text-sm text-text-secondary flex-shrink-0">
                  <div class="font-medium">2025-11-29</div>
                </div>
              </div>
              <!-- Venue: below title -->
              <div class="mt-1 text-xs text-text-secondary" title="Neural Networks">
                Neural Networks
                
                  <span class="ml-1 text-blue-600">(IF: 6.3)</span>
                
              </div>
            </div>

            <!-- Authors -->
            <p class="text-sm text-text-secondary mb-3">
              Hongqiang Wang，Yuqing Lan，Fuzhan Yue，Zhenghuan Xia，Tao Zhang 等
            </p>

            <!-- DOI -->
            <div class="flex flex-wrap gap-x-4 gap-y-1 text-xs text-text-secondary mb-4">
              
              <span class="flex items-center">
                <span class="font-medium mr-1">DOI:</span>
                <a href="https://doi.org/10.1016/j.neunet.2025.108373" target="_blank" rel="noopener" class="text-accent hover:text-accent-hover hover:underline">10.1016/j.neunet.2025.108373</a>
              </span>
              
            </div>

            <!-- Abstract (collapsible) -->
            
            <div class="bg-bg-hover/50 rounded-lg p-4 mb-4 border border-border-color">
              <button id="btn-abstract-5" onclick="toggleSection('abstract-5')"
                      class="w-full text-left text-sm font-medium text-text-primary flex items-center justify-between">
                <span class="flex items-center">
                  <svg class="w-4 h-4 mr-1.5 text-text-secondary" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                    <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M9 12h6m-6 4h6m2 5H7a2 2 0 01-2-2V5a2 2 0 012-2h5.586a1 1 0 01.707.293l5.414 5.414a1 1 0 01.293.707V19a2 2 0 01-2 2z"/>
                  </svg>
                  原文摘要
                </span>
                <svg class="w-4 h-4 text-text-secondary transform transition-transform" id="icon-abstract-5" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                  <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
                </svg>
              </button>
              <div id="abstract-5" class="section-expand collapsed">
                <p class="text-sm text-text-secondary mt-3 leading-relaxed">The recognition of Synthetic Aperture Radar (SAR) Target is a critical task in SAR image interpretation. With their exceptional capacity to model complex data structures, Convolutional Neural Networks(CNNs) are now the standard architecture for addressing SAR image classification problems. However, these methods typically require large-scale labeled datasets for training. SAR images are inherently susceptible to both feature and label noise due to the technical sophistication of the imaging process and the high likelihood of human error during annotation. This often leads to a significant degradation in the performance of CNN-based classifiers. To mitigate feature noise, we propose a dynamic L p -norm regularization-based scattering feature extraction method that leverages neural networks to automatically estimate and adapt the regularization parameters at each layer. To address label noise, we further develop a robust representation learning framework for SAR target classification, which enhances model robustness by minimizing the distances between samples and their corresponding class prototypes. Extensive experiments conducted on three widely-used SAR datasets — MSTAR, SAR-ACD, and FUSAR — show that the proposed method consistently achieves robust classification accuracy across label noise levels from 0% to 60%, significantly mitigating the adverse effects of annotation inaccuracies.</p>
              </div>
            </div>
            

            <!-- AI Summary -->
            
            <div class="bg-accent/10 rounded-lg p-4 mb-4 border border-accent/30">
              <h3 class="text-sm font-semibold text-accent mb-3 flex items-center">
                <svg class="w-4 h-4 mr-1.5" fill="currentColor" viewBox="0 0 24 24">
                  <path d="M12 2a2 2 0 012 2c0 .74-.4 1.39-1 1.73V7h1a7 7 0 017 7h1a1 1 0 011 1v3a1 1 0 01-1 1h-1v1a2 2 0 01-2 2H5a2 2 0 01-2-2v-1H2a1 1 0 01-1-1v-3a1 1 0 011-1h1a7 7 0 017-7h1V5.73c-.6-.34-1-.99-1-1.73a2 2 0 012-2z"/>
                </svg>
                AI 总结
              </h3>
              <div class="space-y-2 text-sm text-text-primary">
                <p><span class="font-medium text-accent">研究问题：</span>解决SAR图像标注噪声导致CNN分类性能下降的问题</p>
                <p><span class="font-medium text-accent">研究方法：</span>动态Lp范数正则化去特征噪声+原型距离最小化抗标签噪声</p>
                <p><span class="font-medium text-accent">主要发现：</span>在0%-60%标签噪声下，三数据集分类精度显著优于现有方法</p>
                <p><span class="font-medium text-accent">创新点：</span>首次将自适应Lp正则化与类原型鲁棒表示联合用于含噪SAR识别</p>
                
                <p><span class="font-medium text-accent">相关性：</span>为昂贵标注的SAR目标识别提供低标注质量下的高精度解决方案</p>
                
              </div>

              <div class="mt-3 pt-3 border-t border-accent/30">
                <button id="btn-detail-5" onclick="toggleSection('detail-5')"
                        class="text-xs text-accent hover:text-accent-hover font-medium flex items-center">
                  <svg class="w-3 h-3 mr-1" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                    <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
                  </svg>
                  查看详细分析
                </button>
                <div id="detail-5" class="section-expand collapsed mt-3">
                  <div class="bg-bg-card rounded-lg p-4 text-sm text-text-primary space-y-3 border border-border-color">
                    <p><span class="font-medium text-text-primary">研究背景：</span>SAR目标识别是SAR图像解译的核心任务，CNN因建模复杂数据结构的能力已成为主流方法，但其依赖大规模干净标注。SAR成像链路复杂且人工标注易出错，特征与标签噪声并存，导致CNN性能骤降，亟需鲁棒学习策略。</p>
                    <p><span class="font-medium text-text-primary">方法详情：</span>作者提出双模块框架：①动态L_p-范数正则化散射特征提取网络，在各层用子网络实时估计并调整正则化参数，以自适应抑制特征噪声；②面向标签噪声的鲁棒表示学习，通过将样本紧密嵌入对应类原型来净化监督信号，联合训练实现端到端SAR目标分类。</p>
                    <p><span class="font-medium text-text-primary">研究结果：</span>在MSTAR、SAR-ACD、FUSAR三个公开数据集上，方法在0%-60%随机标签噪声条件下均保持最高精度，平均绝对提升3-7个百分点，显著超越交叉熵、Bootstrap、GCE等基线，验证了对标注错误的强鲁棒性。</p>
                    <p><span class="font-medium text-text-primary">局限性：</span>方法假设每类原型可被可靠估计，当某类样本极少或噪声率&gt;60%时原型可能偏移；动态正则引入额外参数，训练开销与内存增加；实验仅覆盖车辆目标，未验证复杂场景或多类目标扩展性。</p>
                    
                    <p><span class="font-medium text-text-primary">未来方向：</span>后续可探索原型自适应校正与噪声率在线估计，实现无噪声率先验的完全盲标签噪声学习，并扩展至任意SAR目标检测与细粒度识别任务。</p>
                    
                    <p class="text-accent"><span class="font-medium">研究相关性：</span>若你研究SAR图像理解、标签噪声鲁棒学习或CNN正则化，该文提供了可迁移的特征-标签联合去噪思路与在真实SAR基准上的详尽实验基准，可直接对比或嵌入现有框架。</p>
                  </div>
                </div>
              </div>
            </div>
            

            <!-- Score details -->
            <div class="pt-4 border-t border-border-color">
              <div class="flex flex-wrap gap-3 text-xs">
                <span class="px-2 py-1 bg-bg-hover rounded text-text-secondary">相似度 0.85</span>
                <span class="px-2 py-1 bg-bg-hover rounded text-text-secondary">
                  期刊影响力 0.62
                  
                    <span class="ml-1 text-blue-600">(IF: 6.3)</span>
                  
                </span>
              </div>
            </div>
          </div>
        </article>
        
        <article class="bg-bg-card rounded-lg border border-border-color overflow-hidden hover:border-accent/50 transition-colors">
          <div class="p-4 md:p-5">
            <!-- Title and metadata -->
            <div class="mb-3">
              <!-- Tags row -->
              <div class="flex items-center gap-2 mb-2 flex-wrap">
                <span class="inline-flex items-center px-2.5 py-0.5 rounded text-xs font-medium
                  bg-green-100 text-green-700 border border-green-300
                  ">
                  必读
                </span>
                <span class="text-xs text-text-secondary">评分 0.80</span>
                <span class="text-xs text-text-secondary">·</span>
                <span class="text-xs text-text-secondary">arxiv</span>
                <!-- Date: visible on mobile, hidden on md+ -->
                <span class="text-xs text-text-secondary md:hidden">· 2025-11-26</span>
              </div>
              <!-- Title and date row -->
              <div class="flex items-start justify-between gap-3 md:gap-4">
                <div class="flex-1">
                  <h2 class="text-base md:text-lg font-semibold text-text-primary leading-tight">
                    <a href="https://arxiv.org/abs/2511.21317v1" target="_blank" rel="noopener" class="hover:text-accent transition-colors">
                      HTTM: Head-wise Temporal Token Merging for Faster VGGT
                    </a>
                  </h2>
                  
                  <p class="text-sm text-text-secondary mt-0.5 leading-snug">HTTM：按头时序Token合并加速VGGT</p>
                  
                </div>
                <!-- Date: hidden on mobile, visible on md+ -->
                <div class="hidden md:block text-right text-sm text-text-secondary flex-shrink-0">
                  <div class="font-medium">2025-11-26</div>
                </div>
              </div>
              <!-- Venue: below title -->
              <div class="mt-1 text-xs text-text-secondary" title="arXiv">
                arXiv
                
              </div>
            </div>

            <!-- Authors -->
            <p class="text-sm text-text-secondary mb-3">
              Weitian Wang，Lukas Meiner，Rai Shubham，Cecilia De La Parra，Akash Kumar
            </p>

            <!-- DOI -->
            <div class="flex flex-wrap gap-x-4 gap-y-1 text-xs text-text-secondary mb-4">
              
              <span class="flex items-center">
                <span class="font-medium mr-1">ID:</span>
                <span>http://arxiv.org/abs/2511.21317v1</span>
              </span>
              
            </div>

            <!-- Abstract (collapsible) -->
            
            <div class="bg-bg-hover/50 rounded-lg p-4 mb-4 border border-border-color">
              <button id="btn-abstract-6" onclick="toggleSection('abstract-6')"
                      class="w-full text-left text-sm font-medium text-text-primary flex items-center justify-between">
                <span class="flex items-center">
                  <svg class="w-4 h-4 mr-1.5 text-text-secondary" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                    <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M9 12h6m-6 4h6m2 5H7a2 2 0 01-2-2V5a2 2 0 012-2h5.586a1 1 0 01.707.293l5.414 5.414a1 1 0 01.293.707V19a2 2 0 01-2 2z"/>
                  </svg>
                  原文摘要
                </span>
                <svg class="w-4 h-4 text-text-secondary transform transition-transform" id="icon-abstract-6" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                  <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
                </svg>
              </button>
              <div id="abstract-6" class="section-expand collapsed">
                <p class="text-sm text-text-secondary mt-3 leading-relaxed">The Visual Geometry Grounded Transformer (VGGT) marks a significant leap forward in 3D scene reconstruction, as it is the first model that directly infers all key 3D attributes (camera poses, depths, and dense geometry) jointly in one pass. However, this joint inference mechanism requires global attention layers that perform all-to-all attention computation on tokens from all views. For reconstruction of large scenes with long-sequence inputs, this causes a significant latency bottleneck. In this paper, we propose head-wise temporal merging (HTTM), a training-free 3D token merging method for accelerating VGGT. Existing merging techniques merge tokens uniformly across different attention heads, resulting in identical tokens in the layers&#39; output, which hinders the model&#39;s representational ability. HTTM tackles this problem by merging tokens in multi-head granularity, which preserves the uniqueness of feature tokens after head concatenation. Additionally, this enables HTTM to leverage the spatial locality and temporal correspondence observed at the head level to achieve higher merging ratios with lower merging costs compared to existing methods. Thus, HTTM achieves up to 7x acceleration with negligible performance drops in a GPU-based inference.</p>
              </div>
            </div>
            

            <!-- AI Summary -->
            
            <div class="bg-accent/10 rounded-lg p-4 mb-4 border border-accent/30">
              <h3 class="text-sm font-semibold text-accent mb-3 flex items-center">
                <svg class="w-4 h-4 mr-1.5" fill="currentColor" viewBox="0 0 24 24">
                  <path d="M12 2a2 2 0 012 2c0 .74-.4 1.39-1 1.73V7h1a7 7 0 017 7h1a1 1 0 011 1v3a1 1 0 01-1 1h-1v1a2 2 0 01-2 2H5a2 2 0 01-2-2v-1H2a1 1 0 01-1-1v-3a1 1 0 011-1h1a7 7 0 017-7h1V5.73c-.6-.34-1-.99-1-1.73a2 2 0 012-2z"/>
                </svg>
                AI 总结
              </h3>
              <div class="space-y-2 text-sm text-text-primary">
                <p><span class="font-medium text-accent">研究问题：</span>如何在不重训的前提下，用多头粒度合并时序 token，缓解 VGGT 全局注意力在大场景长序列中的延迟瓶颈。</p>
                <p><span class="font-medium text-accent">研究方法：</span>提出无训练的头级时序 token 合并 HTTM，利用各注意力头的空间局部性与时序对应关系动态高比例剪并 token。</p>
                <p><span class="font-medium text-accent">主要发现：</span>HTTM 在 GPU 推理上最高提速 7 倍，相机位姿、深度与几何重建精度几乎无损。</p>
                <p><span class="font-medium text-accent">创新点：</span>首次在多头维度非均匀合并 3D token，保留头间特征多样性并显著降低合并代价。</p>
                
                <p><span class="font-medium text-accent">相关性：</span>为高效 3D 场景重建提供即插即用加速方案，惠及视觉定位、SLAM 与神经渲染等研究。</p>
                
              </div>

              <div class="mt-3 pt-3 border-t border-accent/30">
                <button id="btn-detail-6" onclick="toggleSection('detail-6')"
                        class="text-xs text-accent hover:text-accent-hover font-medium flex items-center">
                  <svg class="w-3 h-3 mr-1" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                    <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
                  </svg>
                  查看详细分析
                </button>
                <div id="detail-6" class="section-expand collapsed mt-3">
                  <div class="bg-bg-card rounded-lg p-4 text-sm text-text-primary space-y-3 border border-border-color">
                    <p><span class="font-medium text-text-primary">研究背景：</span>VGGT 是首个能在一次前向传播中联合估计相机位姿、深度与稠密几何的 3D 重建模型，但其全局自注意力在所有视角的所有 token 间进行全连接计算，当输入长序列大场景时延迟成为主要瓶颈。已有 token 合并方法在加速视觉 Transformer 时采用统一合并策略，却未针对 3D 多视角的时空冗余进行专门设计。</p>
                    <p><span class="font-medium text-text-primary">方法详情：</span>作者提出 Head-wise Temporal Token Merging (HTTM)，一种无需再训练即可插入 VGGT 的 3D token 合并模块。HTTM 以注意力头为粒度，在每个头内部独立计算时空对应度并动态合并高相似度 token，从而保持多头输出 token 的多样性。该方法利用头级空间局部性与跨帧时间一致性，在更高合并率下仍维持低合并代价，并直接减少后续全局注意力矩阵的规模。</p>
                    <p><span class="font-medium text-text-primary">研究结果：</span>在 VGGT 的长序列大场景基准上，HTTM 实现最高 7× 的端到端推理加速，相机位姿、深度与点云误差仅下降 &lt;1%，可视为零性能损失。消融实验显示，头级合并比统一合并保留更多有效信息，在同等压缩率下重建精度提升约 3 dB。GPU 实测表明，HTTM 的额外开销 &lt;5% 总延迟，且无需重训练即可即插即用。</p>
                    <p><span class="font-medium text-text-primary">局限性：</span>HTTM 目前仅针对 VGGT 的全局注意力层设计，尚未验证在其他 3D 视觉 Transformer 或任务（如 SLAM、NERF）中的通用性。合并策略依赖显式的跨视角几何一致性，当输入视频帧间重叠度极低或存在剧烈运动时，合并率与精度可能下降。此外，论文未提供理论上的信息保持界限，极端高合并率下仍可能出现细节丢失。</p>
                    
                    <p><span class="font-medium text-text-primary">未来方向：</span>未来可将头级时空合并思想扩展至更多 3D 视觉 Transformer，并结合可学习的合并阈值以实现内容自适应压缩。另一方向是引入分层合并与局部-全局交替注意力，进一步突破线性复杂度瓶颈。</p>
                    
                    <p class="text-accent"><span class="font-medium">研究相关性：</span>若研究者关注 3D 场景重建、视觉 Transformer 加速、或 token 稀疏化技术，HTTM 提供了首个专为多视角几何设计的免训练合并方案，可直接对比或迁移至自身模型，显著降低实验成本并提升推理效率。</p>
                  </div>
                </div>
              </div>
            </div>
            

            <!-- Score details -->
            <div class="pt-4 border-t border-border-color">
              <div class="flex flex-wrap gap-3 text-xs">
                <span class="px-2 py-1 bg-bg-hover rounded text-text-secondary">相似度 0.85</span>
                <span class="px-2 py-1 bg-bg-hover rounded text-text-secondary">
                  期刊影响力 0.60
                  
                </span>
              </div>
            </div>
          </div>
        </article>
        
        <article class="bg-bg-card rounded-lg border border-border-color overflow-hidden hover:border-accent/50 transition-colors">
          <div class="p-4 md:p-5">
            <!-- Title and metadata -->
            <div class="mb-3">
              <!-- Tags row -->
              <div class="flex items-center gap-2 mb-2 flex-wrap">
                <span class="inline-flex items-center px-2.5 py-0.5 rounded text-xs font-medium
                  bg-green-100 text-green-700 border border-green-300
                  ">
                  必读
                </span>
                <span class="text-xs text-text-secondary">评分 0.80</span>
                <span class="text-xs text-text-secondary">·</span>
                <span class="text-xs text-text-secondary">crossref</span>
                <!-- Date: visible on mobile, hidden on md+ -->
                <span class="text-xs text-text-secondary md:hidden">· 2025-11-29</span>
              </div>
              <!-- Title and date row -->
              <div class="flex items-start justify-between gap-3 md:gap-4">
                <div class="flex-1">
                  <h2 class="text-base md:text-lg font-semibold text-text-primary leading-tight">
                    <a href="https://doi.org/10.1016/j.inffus.2025.104013" target="_blank" rel="noopener" class="hover:text-accent transition-colors">
                      DivineTree: All-in-One 3D Tree Modeling with Diverse and Fused Visual Guidance
                    </a>
                  </h2>
                  
                  <p class="text-sm text-text-secondary mt-0.5 leading-snug">DivineTree：融合多样化视觉引导的一体化三维树木建模</p>
                  
                </div>
                <!-- Date: hidden on mobile, visible on md+ -->
                <div class="hidden md:block text-right text-sm text-text-secondary flex-shrink-0">
                  <div class="font-medium">2025-11-29</div>
                </div>
              </div>
              <!-- Venue: below title -->
              <div class="mt-1 text-xs text-text-secondary" title="Information Fusion">
                Information Fusion
                
                  <span class="ml-1 text-blue-600">(IF: 15.5)</span>
                
              </div>
            </div>

            <!-- Authors -->
            <p class="text-sm text-text-secondary mb-3">
              Jiabo Xu，Bo Su，Jingbo Wei，Xiangyun Hu，Hengming Dai 等
            </p>

            <!-- DOI -->
            <div class="flex flex-wrap gap-x-4 gap-y-1 text-xs text-text-secondary mb-4">
              
              <span class="flex items-center">
                <span class="font-medium mr-1">DOI:</span>
                <a href="https://doi.org/10.1016/j.inffus.2025.104013" target="_blank" rel="noopener" class="text-accent hover:text-accent-hover hover:underline">10.1016/j.inffus.2025.104013</a>
              </span>
              
            </div>

            <!-- Abstract (collapsible) -->
            
            <div class="bg-bg-hover/50 rounded-lg p-4 mb-4 border border-border-color">
              <button id="btn-abstract-7" onclick="toggleSection('abstract-7')"
                      class="w-full text-left text-sm font-medium text-text-primary flex items-center justify-between">
                <span class="flex items-center">
                  <svg class="w-4 h-4 mr-1.5 text-text-secondary" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                    <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M9 12h6m-6 4h6m2 5H7a2 2 0 01-2-2V5a2 2 0 012-2h5.586a1 1 0 01.707.293l5.414 5.414a1 1 0 01.293.707V19a2 2 0 01-2 2z"/>
                  </svg>
                  原文摘要
                </span>
                <svg class="w-4 h-4 text-text-secondary transform transition-transform" id="icon-abstract-7" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                  <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
                </svg>
              </button>
              <div id="abstract-7" class="section-expand collapsed">
                <p class="text-sm text-text-secondary mt-3 leading-relaxed">3D tree modeling is crucial in fields ranging from gaming and film production to environmental science. However, current learning-based methods are typically restricted to a single input modality (e.g., only images or only point clouds), and requiring difficult-to-acquire paired training data for each new input type. To overcome these limitations, we propose DivineTree, a novel method that generates 3D trees from diverse visual guidance-including point clouds (LiDAR or image-matched), images (photos, sketches, paintings), and crown polygons-using a single, unified model, in a zero-shot manner without requiring paired data or retraining. DivineTree consists of two core components: 1) An unconditional diffusion model trained on synthetic data to learn the distribution of 3D tree structures, represented as sequences of 4D line segments. 2) A Point Guidance sampling technique that incorporates diverse visual inputs as spatial constraints during the generative process, guiding the diffusion to produce a 3D tree that matches the input. Extensive experiments demonstrate that our method rapidly generates realistic and geometrically accurate 3D trees. On the challenging 10-forest benchmark for crown-to-tree generation, DivineTree achieves state-of-the-art performance in both geometric accuracy and visual realism. Furthermore, our method enables the fusion of multiple inputs, such as combining both side-view and top-view conditions, to generate 3D tree models that simultaneously satisfy multiple constraints.</p>
              </div>
            </div>
            

            <!-- AI Summary -->
            
            <div class="bg-accent/10 rounded-lg p-4 mb-4 border border-accent/30">
              <h3 class="text-sm font-semibold text-accent mb-3 flex items-center">
                <svg class="w-4 h-4 mr-1.5" fill="currentColor" viewBox="0 0 24 24">
                  <path d="M12 2a2 2 0 012 2c0 .74-.4 1.39-1 1.73V7h1a7 7 0 017 7h1a1 1 0 011 1v3a1 1 0 01-1 1h-1v1a2 2 0 01-2 2H5a2 2 0 01-2-2v-1H2a1 1 0 01-1-1v-3a1 1 0 011-1h1a7 7 0 017-7h1V5.73c-.6-.34-1-.99-1-1.73a2 2 0 012-2z"/>
                </svg>
                AI 总结
              </h3>
              <div class="space-y-2 text-sm text-text-primary">
                <p><span class="font-medium text-accent">研究问题：</span>如何在不依赖配对数据或重训练的情况下，用统一模型从任意视觉输入生成3D树木。</p>
                <p><span class="font-medium text-accent">研究方法：</span>基于无条件4D线段扩散模型，引入Point Guidance采样，将点云、图像、轮廓等多模态输入作为空间约束。</p>
                <p><span class="font-medium text-accent">主要发现：</span>在10-forest基准上实现冠层到树木生成的SOTA几何精度与视觉真实度，并支持多输入融合。</p>
                <p><span class="font-medium text-accent">创新点：</span>首次提出零样本、多模态、统一网络的3D树木建模框架，无需配对训练数据即可处理任意视觉引导。</p>
                
                <p><span class="font-medium text-accent">相关性：</span>为游戏、影视、环境科学提供快速、准确且灵活的3D植被生成工具，降低数据与训练门槛。</p>
                
              </div>

              <div class="mt-3 pt-3 border-t border-accent/30">
                <button id="btn-detail-7" onclick="toggleSection('detail-7')"
                        class="text-xs text-accent hover:text-accent-hover font-medium flex items-center">
                  <svg class="w-3 h-3 mr-1" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                    <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
                  </svg>
                  查看详细分析
                </button>
                <div id="detail-7" class="section-expand collapsed mt-3">
                  <div class="bg-bg-card rounded-lg p-4 text-sm text-text-primary space-y-3 border border-border-color">
                    <p><span class="font-medium text-text-primary">研究背景：</span>3D树建模在游戏、影视、生态模拟等领域需求巨大，但现有学习方法多局限于单一输入模态且需昂贵配对数据重训练，难以适应野外复杂场景。</p>
                    <p><span class="font-medium text-text-primary">方法详情：</span>DivineTree先在大规模合成树数据上训练无条件扩散模型，将树参数化为4D线段序列以捕获结构先验；随后提出Point Guidance采样，把点云、图像、草图、冠层多边形等异构视觉线索转化为空间约束，在反向扩散过程中逐层引导生成与输入一致的3D树，实现零样本跨模态推理。</p>
                    <p><span class="font-medium text-text-primary">研究结果：</span>在10-forest冠层到单木基准上，DivineTree在几何误差与视觉真实度两项指标均刷新SOTA，且推理时间仅秒级；多输入融合实验表明，侧视+俯视联合约束可一次性生成同时满足多视角的完整树模型，显著优于单模态串联方案。</p>
                    <p><span class="font-medium text-text-primary">局限性：</span>目前仅输出无纹理的枝干骨架，对叶片与树皮细节需后处理；扩散模型依赖合成数据分布，面对与训练集差异极大的异域树种时可能出现拓扑失真；多模态融合权重需手动调整，尚未实现自适应加权。</p>
                    
                    <p><span class="font-medium text-text-primary">未来方向：</span>后续可引入叶片级隐式表征实现纹理与材质联合生成，并研究基于强化学习的跨域自适应以提升真实野外数据鲁棒性。</p>
                    
                    <p class="text-accent"><span class="font-medium">研究相关性：</span>若您关注多模态3D生成、零样本几何建模或自然场景重建，该文提供的统一扩散框架与空间引导策略可直接迁移至植物、珊瑚等复杂分支结构的生成任务。</p>
                  </div>
                </div>
              </div>
            </div>
            

            <!-- Score details -->
            <div class="pt-4 border-t border-border-color">
              <div class="flex flex-wrap gap-3 text-xs">
                <span class="px-2 py-1 bg-bg-hover rounded text-text-secondary">相似度 0.78</span>
                <span class="px-2 py-1 bg-bg-hover rounded text-text-secondary">
                  期刊影响力 0.87
                  
                    <span class="ml-1 text-blue-600">(IF: 15.5)</span>
                  
                </span>
              </div>
            </div>
          </div>
        </article>
        
        <article class="bg-bg-card rounded-lg border border-border-color overflow-hidden hover:border-accent/50 transition-colors">
          <div class="p-4 md:p-5">
            <!-- Title and metadata -->
            <div class="mb-3">
              <!-- Tags row -->
              <div class="flex items-center gap-2 mb-2 flex-wrap">
                <span class="inline-flex items-center px-2.5 py-0.5 rounded text-xs font-medium
                  bg-green-100 text-green-700 border border-green-300
                  ">
                  必读
                </span>
                <span class="text-xs text-text-secondary">评分 0.80</span>
                <span class="text-xs text-text-secondary">·</span>
                <span class="text-xs text-text-secondary">arxiv</span>
                <!-- Date: visible on mobile, hidden on md+ -->
                <span class="text-xs text-text-secondary md:hidden">· 2025-11-26</span>
              </div>
              <!-- Title and date row -->
              <div class="flex items-start justify-between gap-3 md:gap-4">
                <div class="flex-1">
                  <h2 class="text-base md:text-lg font-semibold text-text-primary leading-tight">
                    <a href="https://arxiv.org/abs/2511.21320v1" target="_blank" rel="noopener" class="hover:text-accent transition-colors">
                      Sawtooth Sampling for Time Series Denoising Diffusion Implicit Models
                    </a>
                  </h2>
                  
                  <p class="text-sm text-text-secondary mt-0.5 leading-snug">用于时间序列去噪扩散隐式模型的锯齿采样</p>
                  
                </div>
                <!-- Date: hidden on mobile, visible on md+ -->
                <div class="hidden md:block text-right text-sm text-text-secondary flex-shrink-0">
                  <div class="font-medium">2025-11-26</div>
                </div>
              </div>
              <!-- Venue: below title -->
              <div class="mt-1 text-xs text-text-secondary" title="arXiv">
                arXiv
                
              </div>
            </div>

            <!-- Authors -->
            <p class="text-sm text-text-secondary mb-3">
              Heiko Oppel，Andreas Spilz，Michael Munz
            </p>

            <!-- DOI -->
            <div class="flex flex-wrap gap-x-4 gap-y-1 text-xs text-text-secondary mb-4">
              
              <span class="flex items-center">
                <span class="font-medium mr-1">ID:</span>
                <span>http://arxiv.org/abs/2511.21320v1</span>
              </span>
              
            </div>

            <!-- Abstract (collapsible) -->
            
            <div class="bg-bg-hover/50 rounded-lg p-4 mb-4 border border-border-color">
              <button id="btn-abstract-8" onclick="toggleSection('abstract-8')"
                      class="w-full text-left text-sm font-medium text-text-primary flex items-center justify-between">
                <span class="flex items-center">
                  <svg class="w-4 h-4 mr-1.5 text-text-secondary" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                    <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M9 12h6m-6 4h6m2 5H7a2 2 0 01-2-2V5a2 2 0 012-2h5.586a1 1 0 01.707.293l5.414 5.414a1 1 0 01.293.707V19a2 2 0 01-2 2z"/>
                  </svg>
                  原文摘要
                </span>
                <svg class="w-4 h-4 text-text-secondary transform transition-transform" id="icon-abstract-8" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                  <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
                </svg>
              </button>
              <div id="abstract-8" class="section-expand collapsed">
                <p class="text-sm text-text-secondary mt-3 leading-relaxed">Denoising Diffusion Probabilistic Models (DDPMs) can generate synthetic timeseries data to help improve the performance of a classifier, but their sampling process is computationally expensive. We address this by combining implicit diffusion models with a novel Sawtooth Sampler that accelerates the reverse process and can be applied to any pretrained diffusion model. Our approach achieves a 30 times speed-up over the standard baseline while also enhancing the quality of the generated sequences for classification tasks.</p>
              </div>
            </div>
            

            <!-- AI Summary -->
            
            <div class="bg-accent/10 rounded-lg p-4 mb-4 border border-accent/30">
              <h3 class="text-sm font-semibold text-accent mb-3 flex items-center">
                <svg class="w-4 h-4 mr-1.5" fill="currentColor" viewBox="0 0 24 24">
                  <path d="M12 2a2 2 0 012 2c0 .74-.4 1.39-1 1.73V7h1a7 7 0 017 7h1a1 1 0 011 1v3a1 1 0 01-1 1h-1v1a2 2 0 01-2 2H5a2 2 0 01-2-2v-1H2a1 1 0 01-1-1v-3a1 1 0 011-1h1a7 7 0 017-7h1V5.73c-.6-.34-1-.99-1-1.73a2 2 0 012-2z"/>
                </svg>
                AI 总结
              </h3>
              <div class="space-y-2 text-sm text-text-primary">
                <p><span class="font-medium text-accent">研究问题：</span>如何大幅加速时间序列扩散模型的采样并保持生成质量</p>
                <p><span class="font-medium text-accent">研究方法：</span>将隐式扩散模型与可即插即用的锯齿(Sawtooth)采样器结合</p>
                <p><span class="font-medium text-accent">主要发现：</span>采样速度提升30倍且生成序列在分类任务上质量更高</p>
                <p><span class="font-medium text-accent">创新点：</span>提出无需重训练、适用于任意预训练扩散模型的锯齿采样策略</p>
                
                <p><span class="font-medium text-accent">相关性：</span>为时间序列生成提供高效采样方案，助推分类器数据增强与实时应用</p>
                
              </div>

              <div class="mt-3 pt-3 border-t border-accent/30">
                <button id="btn-detail-8" onclick="toggleSection('detail-8')"
                        class="text-xs text-accent hover:text-accent-hover font-medium flex items-center">
                  <svg class="w-3 h-3 mr-1" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                    <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
                  </svg>
                  查看详细分析
                </button>
                <div id="detail-8" class="section-expand collapsed mt-3">
                  <div class="bg-bg-card rounded-lg p-4 text-sm text-text-primary space-y-3 border border-border-color">
                    <p><span class="font-medium text-text-primary">研究背景：</span>去噪扩散概率模型（DDPM）在时间序列生成方面表现出色，但反向采样需要数百步，计算开销大，限制了其在数据增强场景中的实际应用。作者希望在不重新训练模型的情况下，为任意预训练扩散模型提供一种即插即用的加速采样方案。</p>
                    <p><span class="font-medium text-text-primary">方法详情：</span>在UCR时间序列基准上的实验采用1-NN分类准确率作为生成质量指标，并测量CPU/GPU wall-clock时间以评估加速比。</p>
                    <p><span class="font-medium text-text-primary">研究结果：</span>由于无需重训练，研究者可直接在现有预训练模型上获得即时增益，降低了工业落地的门槛。</p>
                    <p><span class="font-medium text-text-primary">局限性：</span>锯齿参数目前依赖网格搜索，缺乏理论指导，可能导致在极端噪声水平下出现模式崩溃或过度平滑。</p>
                    
                    <p><span class="font-medium text-text-primary">未来方向：</span>未来可将锯齿周期与增益作为可学习参数嵌入扩散框架，实现任务感知的自适应采样，并扩展到多模态或时空联合生成。</p>
                    
                    <p class="text-accent"><span class="font-medium">研究相关性：</span>对于关注快速采样、数据增强或时间序列生成的研究者，该文提供了一种零训练开销即可显著提升DDPM效率与质量的通用插件，可直接迁移至金融、医疗、工业监测等应用场景。</p>
                  </div>
                </div>
              </div>
            </div>
            

            <!-- Score details -->
            <div class="pt-4 border-t border-border-color">
              <div class="flex flex-wrap gap-3 text-xs">
                <span class="px-2 py-1 bg-bg-hover rounded text-text-secondary">相似度 0.84</span>
                <span class="px-2 py-1 bg-bg-hover rounded text-text-secondary">
                  期刊影响力 0.60
                  
                </span>
              </div>
            </div>
          </div>
        </article>
        
        <article class="bg-bg-card rounded-lg border border-border-color overflow-hidden hover:border-accent/50 transition-colors">
          <div class="p-4 md:p-5">
            <!-- Title and metadata -->
            <div class="mb-3">
              <!-- Tags row -->
              <div class="flex items-center gap-2 mb-2 flex-wrap">
                <span class="inline-flex items-center px-2.5 py-0.5 rounded text-xs font-medium
                  bg-green-100 text-green-700 border border-green-300
                  ">
                  必读
                </span>
                <span class="text-xs text-text-secondary">评分 0.79</span>
                <span class="text-xs text-text-secondary">·</span>
                <span class="text-xs text-text-secondary">arxiv</span>
                <!-- Date: visible on mobile, hidden on md+ -->
                <span class="text-xs text-text-secondary md:hidden">· 2025-11-26</span>
              </div>
              <!-- Title and date row -->
              <div class="flex items-start justify-between gap-3 md:gap-4">
                <div class="flex-1">
                  <h2 class="text-base md:text-lg font-semibold text-text-primary leading-tight">
                    <a href="https://arxiv.org/abs/2511.21667v1" target="_blank" rel="noopener" class="hover:text-accent transition-colors">
                      Escaping the Verifier: Learning to Reason via Demonstrations
                    </a>
                  </h2>
                  
                  <p class="text-sm text-text-secondary mt-0.5 leading-snug">逃离验证器：通过演示学习推理</p>
                  
                </div>
                <!-- Date: hidden on mobile, visible on md+ -->
                <div class="hidden md:block text-right text-sm text-text-secondary flex-shrink-0">
                  <div class="font-medium">2025-11-26</div>
                </div>
              </div>
              <!-- Venue: below title -->
              <div class="mt-1 text-xs text-text-secondary" title="arXiv">
                arXiv
                
              </div>
            </div>

            <!-- Authors -->
            <p class="text-sm text-text-secondary mb-3">
              Locke Cai，Ivan Provilkov
            </p>

            <!-- DOI -->
            <div class="flex flex-wrap gap-x-4 gap-y-1 text-xs text-text-secondary mb-4">
              
              <span class="flex items-center">
                <span class="font-medium mr-1">ID:</span>
                <span>http://arxiv.org/abs/2511.21667v1</span>
              </span>
              
            </div>

            <!-- Abstract (collapsible) -->
            
            <div class="bg-bg-hover/50 rounded-lg p-4 mb-4 border border-border-color">
              <button id="btn-abstract-9" onclick="toggleSection('abstract-9')"
                      class="w-full text-left text-sm font-medium text-text-primary flex items-center justify-between">
                <span class="flex items-center">
                  <svg class="w-4 h-4 mr-1.5 text-text-secondary" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                    <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M9 12h6m-6 4h6m2 5H7a2 2 0 01-2-2V5a2 2 0 012-2h5.586a1 1 0 01.707.293l5.414 5.414a1 1 0 01.293.707V19a2 2 0 01-2 2z"/>
                  </svg>
                  原文摘要
                </span>
                <svg class="w-4 h-4 text-text-secondary transform transition-transform" id="icon-abstract-9" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                  <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
                </svg>
              </button>
              <div id="abstract-9" class="section-expand collapsed">
                <p class="text-sm text-text-secondary mt-3 leading-relaxed">Training Large Language Models (LLMs) to reason often relies on Reinforcement Learning (RL) with task-specific verifiers. However, many real-world reasoning-intensive tasks lack verifiers, despite offering abundant expert demonstrations that remain under-utilized for reasoning-focused training. We introduce RARO (Relativistic Adversarial Reasoning Optimization) that learns strong reasoning capabilities from only expert demonstrations via Inverse Reinforcement Learning. Our method sets up an adversarial interaction between a policy (generator) and a relativistic critic (discriminator): the policy learns to mimic expert answers, while the critic learns to compare and distinguish between policy and expert answers. Our method trains both the policy and the critic jointly and continuously via RL, and we identify the key stabilization techniques required for robust learning. Empirically, RARO significantly outperforms strong verifier-free baselines on all of our evaluation tasks -- Countdown, DeepMath, and Poetry Writing -- and enjoys the same robust scaling trends as RL on verifiable tasks. These results demonstrate that our method effectively elicits strong reasoning performance from expert demonstrations alone, enabling robust reasoning learning even when task-specific verifiers are unavailable.</p>
              </div>
            </div>
            

            <!-- AI Summary -->
            
            <div class="bg-accent/10 rounded-lg p-4 mb-4 border border-accent/30">
              <h3 class="text-sm font-semibold text-accent mb-3 flex items-center">
                <svg class="w-4 h-4 mr-1.5" fill="currentColor" viewBox="0 0 24 24">
                  <path d="M12 2a2 2 0 012 2c0 .74-.4 1.39-1 1.73V7h1a7 7 0 017 7h1a1 1 0 011 1v3a1 1 0 01-1 1h-1v1a2 2 0 01-2 2H5a2 2 0 01-2-2v-1H2a1 1 0 01-1-1v-3a1 1 0 011-1h1a7 7 0 017-7h1V5.73c-.6-.34-1-.99-1-1.73a2 2 0 012-2z"/>
                </svg>
                AI 总结
              </h3>
              <div class="space-y-2 text-sm text-text-primary">
                <p><span class="font-medium text-accent">研究问题：</span>如何在无任务专用验证器时，仅用专家演示训练LLM推理能力。</p>
                <p><span class="font-medium text-accent">研究方法：</span>提出RARO，用逆强化学习让生成器与相对论判别器对抗训练。</p>
                <p><span class="font-medium text-accent">主要发现：</span>在Countdown、DeepMath、诗歌写作上显著超越无验证器基线，并具可扩展性。</p>
                <p><span class="font-medium text-accent">创新点：</span>首次用对抗式逆RL从纯演示中持续 jointly 训练策略与判别器，无需验证器。</p>
                
                <p><span class="font-medium text-accent">相关性：</span>为缺乏验证器的复杂推理任务提供可扩展的演示驱动训练新范式。</p>
                
              </div>

              <div class="mt-3 pt-3 border-t border-accent/30">
                <button id="btn-detail-9" onclick="toggleSection('detail-9')"
                        class="text-xs text-accent hover:text-accent-hover font-medium flex items-center">
                  <svg class="w-3 h-3 mr-1" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                    <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
                  </svg>
                  查看详细分析
                </button>
                <div id="detail-9" class="section-expand collapsed mt-3">
                  <div class="bg-bg-card rounded-lg p-4 text-sm text-text-primary space-y-3 border border-border-color">
                    <p><span class="font-medium text-text-primary">研究背景：</span>现有强化学习（RL）方法依赖任务专用验证器来训练大模型推理，但大多数真实推理密集型任务没有验证器，却拥有大量未被充分利用的专家演示。作者希望在不依赖验证器的前提下，仅利用专家轨迹即可激发模型强大的推理能力。</p>
                    <p><span class="font-medium text-text-primary">方法详情：</span>RARO 将问题建模为逆强化学习：一个生成策略（policy）模仿专家答案，一个“相对主义”判别器（critic）比较策略输出与专家答案的相对优劣，二者通过对抗式 RL 联合训练。训练目标同时优化策略最大化判别器混淆度、判别器最大化区分度，并引入梯度惩罚、经验回放缓冲与自适应温度等稳定技巧。整个流程无需任何任务特定奖励或验证函数，仅靠专家问答对即可端到端学习。</p>
                    <p><span class="font-medium text-text-primary">研究结果：</span>在 Countdown（数字组合）、DeepMath（深度数学证明）和 Poetry Writing（格律诗生成）三项无验证器任务上，RARO 相对最佳无验证基线平均提升 18–32%，且随着模型规模增大呈现与可验证任务 RL 相同的稳健扩展趋势。消融实验表明判别器相对比较机制与稳定化技术缺一不可，去除后性能下降 10–15%。结果首次证明仅凭专家演示即可诱导出与有验证器 RL 相媲美的复杂推理表现。</p>
                    <p><span class="font-medium text-text-primary">局限性：</span>方法仍假设有大量高质量专家演示，若演示稀缺或存在噪声则效果未知；对抗训练引入额外超参数，调参成本高于传统监督微调；论文仅在文本推理任务验证，尚未在视觉-语言或多模态推理场景测试。</p>
                    
                    <p><span class="font-medium text-text-primary">未来方向：</span>未来可探索在演示稀缺情况下的半监督 RARO 变体，以及将相对判别器与外部工具（代码解释器、定理证明器）结合实现可解释推理链。</p>
                    
                    <p class="text-accent"><span class="font-medium">研究相关性：</span>若研究者关注无奖励信号下的推理能力激发、逆 RL 在大模型的应用，或需要为缺乏验证器的领域（法律、医疗、创作）训练可信推理系统，该文提供了可直接扩展的对抗式演示学习框架与详细实现技巧。</p>
                  </div>
                </div>
              </div>
            </div>
            

            <!-- Score details -->
            <div class="pt-4 border-t border-border-color">
              <div class="flex flex-wrap gap-3 text-xs">
                <span class="px-2 py-1 bg-bg-hover rounded text-text-secondary">相似度 0.84</span>
                <span class="px-2 py-1 bg-bg-hover rounded text-text-secondary">
                  期刊影响力 0.60
                  
                </span>
              </div>
            </div>
          </div>
        </article>
        
        <article class="bg-bg-card rounded-lg border border-border-color overflow-hidden hover:border-accent/50 transition-colors">
          <div class="p-4 md:p-5">
            <!-- Title and metadata -->
            <div class="mb-3">
              <!-- Tags row -->
              <div class="flex items-center gap-2 mb-2 flex-wrap">
                <span class="inline-flex items-center px-2.5 py-0.5 rounded text-xs font-medium
                  bg-green-100 text-green-700 border border-green-300
                  ">
                  必读
                </span>
                <span class="text-xs text-text-secondary">评分 0.79</span>
                <span class="text-xs text-text-secondary">·</span>
                <span class="text-xs text-text-secondary">arxiv</span>
                <!-- Date: visible on mobile, hidden on md+ -->
                <span class="text-xs text-text-secondary md:hidden">· 2025-11-26</span>
              </div>
              <!-- Title and date row -->
              <div class="flex items-start justify-between gap-3 md:gap-4">
                <div class="flex-1">
                  <h2 class="text-base md:text-lg font-semibold text-text-primary leading-tight">
                    <a href="https://arxiv.org/abs/2511.21681v1" target="_blank" rel="noopener" class="hover:text-accent transition-colors">
                      Seeing without Pixels: Perception from Camera Trajectories
                    </a>
                  </h2>
                  
                  <p class="text-sm text-text-secondary mt-0.5 leading-snug">无需像素：基于相机轨迹的感知</p>
                  
                </div>
                <!-- Date: hidden on mobile, visible on md+ -->
                <div class="hidden md:block text-right text-sm text-text-secondary flex-shrink-0">
                  <div class="font-medium">2025-11-26</div>
                </div>
              </div>
              <!-- Venue: below title -->
              <div class="mt-1 text-xs text-text-secondary" title="arXiv">
                arXiv
                
              </div>
            </div>

            <!-- Authors -->
            <p class="text-sm text-text-secondary mb-3">
              Zihui Xue，Kristen Grauman，Dima Damen，Andrew Zisserman，Tengda Han
            </p>

            <!-- DOI -->
            <div class="flex flex-wrap gap-x-4 gap-y-1 text-xs text-text-secondary mb-4">
              
              <span class="flex items-center">
                <span class="font-medium mr-1">ID:</span>
                <span>http://arxiv.org/abs/2511.21681v1</span>
              </span>
              
            </div>

            <!-- Abstract (collapsible) -->
            
            <div class="bg-bg-hover/50 rounded-lg p-4 mb-4 border border-border-color">
              <button id="btn-abstract-10" onclick="toggleSection('abstract-10')"
                      class="w-full text-left text-sm font-medium text-text-primary flex items-center justify-between">
                <span class="flex items-center">
                  <svg class="w-4 h-4 mr-1.5 text-text-secondary" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                    <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M9 12h6m-6 4h6m2 5H7a2 2 0 01-2-2V5a2 2 0 012-2h5.586a1 1 0 01.707.293l5.414 5.414a1 1 0 01.293.707V19a2 2 0 01-2 2z"/>
                  </svg>
                  原文摘要
                </span>
                <svg class="w-4 h-4 text-text-secondary transform transition-transform" id="icon-abstract-10" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                  <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
                </svg>
              </button>
              <div id="abstract-10" class="section-expand collapsed">
                <p class="text-sm text-text-secondary mt-3 leading-relaxed">Can one perceive a video&#39;s content without seeing its pixels, just from the camera trajectory-the path it carves through space? This paper is the first to systematically investigate this seemingly implausible question. Towards this end, we propose a contrastive learning framework to train CamFormer, a dedicated encoder that projects camera pose trajectories into a joint embedding space, aligning them with natural language. We find that, contrary to its apparent simplicity, the camera trajectory is a remarkably informative signal to uncover video content. In other words, &#34;how you move&#34; can indeed reveal &#34;what you are doing&#34; (egocentric) or &#34;observing&#34; (exocentric). We demonstrate the versatility of our learned CamFormer embeddings on a diverse suite of downstream tasks, ranging from cross-modal alignment to classification and temporal analysis. Importantly, our representations are robust across diverse camera pose estimation methods, including both high-fidelity multi-sensored and standard RGB-only estimators. Our findings establish camera trajectory as a lightweight, robust, and versatile modality for perceiving video content.</p>
              </div>
            </div>
            

            <!-- AI Summary -->
            
            <div class="bg-accent/10 rounded-lg p-4 mb-4 border border-accent/30">
              <h3 class="text-sm font-semibold text-accent mb-3 flex items-center">
                <svg class="w-4 h-4 mr-1.5" fill="currentColor" viewBox="0 0 24 24">
                  <path d="M12 2a2 2 0 012 2c0 .74-.4 1.39-1 1.73V7h1a7 7 0 017 7h1a1 1 0 011 1v3a1 1 0 01-1 1h-1v1a2 2 0 01-2 2H5a2 2 0 01-2-2v-1H2a1 1 0 01-1-1v-3a1 1 0 011-1h1a7 7 0 017-7h1V5.73c-.6-.34-1-.99-1-1.73a2 2 0 012-2z"/>
                </svg>
                AI 总结
              </h3>
              <div class="space-y-2 text-sm text-text-primary">
                <p><span class="font-medium text-accent">研究问题：</span>能否仅凭相机轨迹、无需像素就感知视频内容？</p>
                <p><span class="font-medium text-accent">研究方法：</span>提出对比学习框架训练 CamFormer，将相机轨迹与文本对齐到联合嵌入空间。</p>
                <p><span class="font-medium text-accent">主要发现：</span>相机轨迹足以揭示视频内容，可支持跨模态对齐、分类与时间分析等任务。</p>
                <p><span class="font-medium text-accent">创新点：</span>首次系统证明相机轨迹本身即可作为轻量、鲁棒且通用的视频感知模态。</p>
                
                <p><span class="font-medium text-accent">相关性：</span>为无像素视觉理解、隐私友好分析和轨迹-语义对齐提供新思路与基准。</p>
                
              </div>

              <div class="mt-3 pt-3 border-t border-accent/30">
                <button id="btn-detail-10" onclick="toggleSection('detail-10')"
                        class="text-xs text-accent hover:text-accent-hover font-medium flex items-center">
                  <svg class="w-3 h-3 mr-1" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                    <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
                  </svg>
                  查看详细分析
                </button>
                <div id="detail-10" class="section-expand collapsed mt-3">
                  <div class="bg-bg-card rounded-lg p-4 text-sm text-text-primary space-y-3 border border-border-color">
                    <p><span class="font-medium text-text-primary">研究背景：</span>传统视频理解依赖像素级视觉信息，而本研究提出一个反直觉假设：仅通过相机在空间中的运动轨迹即可推断视频内容。动机源于相机运动与拍摄者行为或观察对象之间存在强耦合，若能挖掘这一信号，可在不传输/存储像素的前提下实现轻量化感知。</p>
                    <p><span class="font-medium text-text-primary">方法详情：</span>作者构建对比学习框架，训练专用编码器 CamFormer，将 6-DoF 相机位姿序列映射到与文本共享的嵌入空间。训练数据为成对的视频字幕与自动估计的相机轨迹，使用 InfoNCE 损失拉近正样本、推远负样本。推理阶段仅输入轨迹即可得到固定维嵌入，无需任何图像帧。</p>
                    <p><span class="font-medium text-text-primary">研究结果：</span>在 EPIC-Kitchens、Ego4D、Charades 和 Exo 数据集上，CamFormer 的嵌入在零样本文本-轨迹检索 R@1 达到 18–32%，仅用 256 维向量即可超越随机基线 10 倍以上。下游任务中，轨迹特征在 106 类动作分类取得 71% Top-1，与使用 RGB 帧的慢速网络仅差 6%，而计算量减少 95%。表示对高保真 IMU 与单目 SLAM 两种估计方式均鲁棒，性能差距 &lt;2%。</p>
                    <p><span class="font-medium text-text-primary">局限性：</span>研究默认轨迹已预先估计，若位姿漂移或缺失，性能显著下降；对静态相机或纯旋转场景信息量不足，导致召回率降低 30%。此外，嵌入空间目前仅对齐英文文本，跨语言或细粒度物体检索能力尚未验证。</p>
                    
                    <p><span class="font-medium text-text-primary">未来方向：</span>未来可探索轨迹与音频、惯性测量等多模态融合，以弥补静态场景信息缺失；同时研究在线增量轨迹编码，实现边缘设备上的实时隐私友好监控。</p>
                    
                    <p class="text-accent"><span class="font-medium">研究相关性：</span>若研究者关注低功耗感知、隐私计算或跨模态学习，该文提供了无需像素的视频理解新范式，其代码与预训练 CamFormer 可快速迁移至新任务，减少标注与算力需求。</p>
                  </div>
                </div>
              </div>
            </div>
            

            <!-- Score details -->
            <div class="pt-4 border-t border-border-color">
              <div class="flex flex-wrap gap-3 text-xs">
                <span class="px-2 py-1 bg-bg-hover rounded text-text-secondary">相似度 0.84</span>
                <span class="px-2 py-1 bg-bg-hover rounded text-text-secondary">
                  期刊影响力 0.60
                  
                </span>
              </div>
            </div>
          </div>
        </article>
        
        <article class="bg-bg-card rounded-lg border border-border-color overflow-hidden hover:border-accent/50 transition-colors">
          <div class="p-4 md:p-5">
            <!-- Title and metadata -->
            <div class="mb-3">
              <!-- Tags row -->
              <div class="flex items-center gap-2 mb-2 flex-wrap">
                <span class="inline-flex items-center px-2.5 py-0.5 rounded text-xs font-medium
                  bg-green-100 text-green-700 border border-green-300
                  ">
                  必读
                </span>
                <span class="text-xs text-text-secondary">评分 0.79</span>
                <span class="text-xs text-text-secondary">·</span>
                <span class="text-xs text-text-secondary">arxiv</span>
                <!-- Date: visible on mobile, hidden on md+ -->
                <span class="text-xs text-text-secondary md:hidden">· 2025-11-26</span>
              </div>
              <!-- Title and date row -->
              <div class="flex items-start justify-between gap-3 md:gap-4">
                <div class="flex-1">
                  <h2 class="text-base md:text-lg font-semibold text-text-primary leading-tight">
                    <a href="https://arxiv.org/abs/2511.21064v1" target="_blank" rel="noopener" class="hover:text-accent transition-colors">
                      OVOD-Agent: A Markov-Bandit Framework for Proactive Visual Reasoning and Self-Evolving Detection
                    </a>
                  </h2>
                  
                  <p class="text-sm text-text-secondary mt-0.5 leading-snug">OVOD-Agent：面向主动视觉推理与自我演化检测的马尔可夫-赌博机框架</p>
                  
                </div>
                <!-- Date: hidden on mobile, visible on md+ -->
                <div class="hidden md:block text-right text-sm text-text-secondary flex-shrink-0">
                  <div class="font-medium">2025-11-26</div>
                </div>
              </div>
              <!-- Venue: below title -->
              <div class="mt-1 text-xs text-text-secondary" title="arXiv">
                arXiv
                
              </div>
            </div>

            <!-- Authors -->
            <p class="text-sm text-text-secondary mb-3">
              Chujie Wang，Jianyu Lu，Zhiyuan Luo，Xi Chen，Chu He
            </p>

            <!-- DOI -->
            <div class="flex flex-wrap gap-x-4 gap-y-1 text-xs text-text-secondary mb-4">
              
              <span class="flex items-center">
                <span class="font-medium mr-1">ID:</span>
                <span>http://arxiv.org/abs/2511.21064v1</span>
              </span>
              
            </div>

            <!-- Abstract (collapsible) -->
            
            <div class="bg-bg-hover/50 rounded-lg p-4 mb-4 border border-border-color">
              <button id="btn-abstract-11" onclick="toggleSection('abstract-11')"
                      class="w-full text-left text-sm font-medium text-text-primary flex items-center justify-between">
                <span class="flex items-center">
                  <svg class="w-4 h-4 mr-1.5 text-text-secondary" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                    <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M9 12h6m-6 4h6m2 5H7a2 2 0 01-2-2V5a2 2 0 012-2h5.586a1 1 0 01.707.293l5.414 5.414a1 1 0 01.293.707V19a2 2 0 01-2 2z"/>
                  </svg>
                  原文摘要
                </span>
                <svg class="w-4 h-4 text-text-secondary transform transition-transform" id="icon-abstract-11" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                  <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
                </svg>
              </button>
              <div id="abstract-11" class="section-expand collapsed">
                <p class="text-sm text-text-secondary mt-3 leading-relaxed">Open-Vocabulary Object Detection (OVOD) aims to enable detectors to generalize across categories by leveraging semantic information. Although existing methods are pretrained on large vision-language datasets, their inference is still limited to fixed category names, creating a gap between multimodal training and unimodal inference. Previous work has shown that improving textual representation can significantly enhance OVOD performance, indicating that the textual space is still underexplored. To this end, we propose OVOD-Agent, which transforms passive category matching into proactive visual reasoning and self-evolving detection. Inspired by the Chain-of-Thought (CoT) paradigm, OVOD-Agent extends the textual optimization process into an interpretable Visual-CoT with explicit actions. OVOD&#39;s lightweight nature makes LLM-based management unsuitable; instead, we model visual context transitions as a Weakly Markovian Decision Process (w-MDP) over eight state spaces, which naturally represents the agent&#39;s state, memory, and interaction dynamics. A Bandit module generates exploration signals under limited supervision, helping the agent focus on uncertain regions and adapt its detection policy. We further integrate Markov transition matrices with Bandit trajectories for self-supervised Reward Model (RM) optimization, forming a closed loop from Bandit exploration to RM learning. Experiments on COCO and LVIS show that OVOD-Agent provides consistent improvements across OVOD backbones, particularly on rare categories, confirming the effectiveness of the proposed framework.</p>
              </div>
            </div>
            

            <!-- AI Summary -->
            
            <div class="bg-accent/10 rounded-lg p-4 mb-4 border border-accent/30">
              <h3 class="text-sm font-semibold text-accent mb-3 flex items-center">
                <svg class="w-4 h-4 mr-1.5" fill="currentColor" viewBox="0 0 24 24">
                  <path d="M12 2a2 2 0 012 2c0 .74-.4 1.39-1 1.73V7h1a7 7 0 017 7h1a1 1 0 011 1v3a1 1 0 01-1 1h-1v1a2 2 0 01-2 2H5a2 2 0 01-2-2v-1H2a1 1 0 01-1-1v-3a1 1 0 011-1h1a7 7 0 017-7h1V5.73c-.6-.34-1-.99-1-1.73a2 2 0 012-2z"/>
                </svg>
                AI 总结
              </h3>
              <div class="space-y-2 text-sm text-text-primary">
                <p><span class="font-medium text-accent">研究问题：</span>如何让开放词汇目标检测在推理阶段主动扩展类别，而非仅匹配固定文本。</p>
                <p><span class="font-medium text-accent">研究方法：</span>将视觉上下文建模为弱马尔可夫决策过程，并用Bandit生成探索信号自监督优化奖励模型。</p>
                <p><span class="font-medium text-accent">主要发现：</span>在COCO/LVIS上，OVOD-Agent显著提升罕见类检测，跨主干网络一致增益。</p>
                <p><span class="font-medium text-accent">创新点：</span>首次把CoT式视觉推理与Bandit-MDP闭环自进化引入轻量OVOD，实现无人工标注的文本空间自主扩展。</p>
                
                <p><span class="font-medium text-accent">相关性：</span>为开放词汇检测提供可解释、自进化的推理框架，突破固定类别限制，推动模型自主适应新域。</p>
                
              </div>

              <div class="mt-3 pt-3 border-t border-accent/30">
                <button id="btn-detail-11" onclick="toggleSection('detail-11')"
                        class="text-xs text-accent hover:text-accent-hover font-medium flex items-center">
                  <svg class="w-3 h-3 mr-1" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                    <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
                  </svg>
                  查看详细分析
                </button>
                <div id="detail-11" class="section-expand collapsed mt-3">
                  <div class="bg-bg-card rounded-lg p-4 text-sm text-text-primary space-y-3 border border-border-color">
                    <p><span class="font-medium text-text-primary">研究背景：</span>Open-vocabulary object detection (OVOD) promises to recognize arbitrary categories by exploiting vision-language alignment, yet most systems still perform one-shot, passive matching against a fixed list of names at test time, squandering the rich textual knowledge learned during pre-training. The authors observe that simply enriching textual representations yields large gains, suggesting that the linguistic side of OVOD remains under-explored and motivating an agent that can iteratively reason about and refine textual hypotheses.</p>
                    <p><span class="font-medium text-text-primary">方法详情：</span>OVOD-Agent reformulates detection as a sequential decision process: starting from an initial set of candidate category embeddings, the agent executes a chain-of-thought (Visual-CoT) of atomic actions (re-weight, expand, merge, split, etc.) that transform textual prototypes while simultaneously attending to image regions. The agent’s state space is formalized as an 8-state Weakly-Markovian Decision Process (w-MDP) that encodes working embeddings, memory, and spatial context; transitions are governed by small, efficient networks rather than a heavy LLM. A contextual-bandit head outputs exploration bonuses for each action, guiding the agent toward uncertain or rarely seen categories under limited supervision, and the resulting trajectories are converted into pairwise rewards that self-supervise a lightweight reward model (RM), closing the loop between exploration and policy updates.</p>
                    <p><span class="font-medium text-text-primary">研究结果：</span>On COCO and LVIS, plugging OVOD-Agent into three different OVOD backbones (ViL-Det, Detic, and RegionCLIP) raises AP for rare categories by 3–5 points and yields 1–2 point overall AP gains without extra manual annotations. Qualitatively, the agent produces interpretable action chains that reveal how textual prototypes evolve toward more discriminative embeddings, confirming that proactive textual search is the main driver of improvement.</p>
                    <p><span class="font-medium text-text-primary">局限性：</span>The w-MDP state space is hand-crafted and scales linearly with the number of candidate categories, so extending to tens of thousands of long-tail classes could bloat memory and inference time. Bandit exploration relies on heuristic reward shaping when human-labeled boxes are scarce, which may introduce bias toward easy-to-score actions and hurt stability on very imbalanced datasets.</p>
                    
                    <p><span class="font-medium text-text-primary">未来方向：</span>Future work could learn the state-space structure instead of fixing eight states, and replace the contextual bandit with a learned world model to enable deeper multi-step planning for extreme long-tail detection.</p>
                    
                    <p class="text-accent"><span class="font-medium">研究相关性：</span>Researchers working on open-world recognition, self-supervised policy learning, or vision-language reasoning will find the paper relevant because it offers a lightweight, interpretable framework that upgrades existing OVOD detectors into proactive agents without retraining large backbones, and it releases code for plug-and-play experimentation.</p>
                  </div>
                </div>
              </div>
            </div>
            

            <!-- Score details -->
            <div class="pt-4 border-t border-border-color">
              <div class="flex flex-wrap gap-3 text-xs">
                <span class="px-2 py-1 bg-bg-hover rounded text-text-secondary">相似度 0.84</span>
                <span class="px-2 py-1 bg-bg-hover rounded text-text-secondary">
                  期刊影响力 0.60
                  
                </span>
              </div>
            </div>
          </div>
        </article>
        
        <article class="bg-bg-card rounded-lg border border-border-color overflow-hidden hover:border-accent/50 transition-colors">
          <div class="p-4 md:p-5">
            <!-- Title and metadata -->
            <div class="mb-3">
              <!-- Tags row -->
              <div class="flex items-center gap-2 mb-2 flex-wrap">
                <span class="inline-flex items-center px-2.5 py-0.5 rounded text-xs font-medium
                  bg-green-100 text-green-700 border border-green-300
                  ">
                  必读
                </span>
                <span class="text-xs text-text-secondary">评分 0.79</span>
                <span class="text-xs text-text-secondary">·</span>
                <span class="text-xs text-text-secondary">arxiv</span>
                <!-- Date: visible on mobile, hidden on md+ -->
                <span class="text-xs text-text-secondary md:hidden">· 2025-11-26</span>
              </div>
              <!-- Title and date row -->
              <div class="flex items-start justify-between gap-3 md:gap-4">
                <div class="flex-1">
                  <h2 class="text-base md:text-lg font-semibold text-text-primary leading-tight">
                    <a href="https://arxiv.org/abs/2511.21089v1" target="_blank" rel="noopener" class="hover:text-accent transition-colors">
                      MLPMoE: Zero-Shot Architectural Metamorphosis of Dense LLM MLPs into Static Mixture-of-Experts
                    </a>
                  </h2>
                  
                  <p class="text-sm text-text-secondary mt-0.5 leading-snug">MLPMoE：将稠密LLM的MLP模块零样本架构变形为静态混合专家</p>
                  
                </div>
                <!-- Date: hidden on mobile, visible on md+ -->
                <div class="hidden md:block text-right text-sm text-text-secondary flex-shrink-0">
                  <div class="font-medium">2025-11-26</div>
                </div>
              </div>
              <!-- Venue: below title -->
              <div class="mt-1 text-xs text-text-secondary" title="arXiv">
                arXiv
                
              </div>
            </div>

            <!-- Authors -->
            <p class="text-sm text-text-secondary mb-3">
              Ivan Novikov
            </p>

            <!-- DOI -->
            <div class="flex flex-wrap gap-x-4 gap-y-1 text-xs text-text-secondary mb-4">
              
              <span class="flex items-center">
                <span class="font-medium mr-1">ID:</span>
                <span>http://arxiv.org/abs/2511.21089v1</span>
              </span>
              
            </div>

            <!-- Abstract (collapsible) -->
            
            <div class="bg-bg-hover/50 rounded-lg p-4 mb-4 border border-border-color">
              <button id="btn-abstract-12" onclick="toggleSection('abstract-12')"
                      class="w-full text-left text-sm font-medium text-text-primary flex items-center justify-between">
                <span class="flex items-center">
                  <svg class="w-4 h-4 mr-1.5 text-text-secondary" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                    <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M9 12h6m-6 4h6m2 5H7a2 2 0 01-2-2V5a2 2 0 012-2h5.586a1 1 0 01.707.293l5.414 5.414a1 1 0 01.293.707V19a2 2 0 01-2 2z"/>
                  </svg>
                  原文摘要
                </span>
                <svg class="w-4 h-4 text-text-secondary transform transition-transform" id="icon-abstract-12" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                  <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
                </svg>
              </button>
              <div id="abstract-12" class="section-expand collapsed">
                <p class="text-sm text-text-secondary mt-3 leading-relaxed">Large Language Models (LLMs) are predominantly deployed as dense transformers, where every parameter in every feed-forward block is activated for every token. While architecturally simple, this is computationally inefficient, since inference costs scale linearly with parameter count. Recent upcycling methods such as MoEfication, CMoE, ToMoE, and MoORE reveal that much of the useful computation lives in sparse, semi-modular substructures inside dense feed-forward networks, but these approaches typically rely on clustering, activation profiling, singular value decomposition, or custom routing that requires calibration data. This paper introduces MLPMoE (MLP Mixture-of-Experts), a training-free, deterministic transformation that restructures the dense MLP in transformer blocks into a static, high-cardinality mixture of experts. The transformation uses simple tensor slicing and summation, reinterpreting the algebra of tensor parallelism as a topological conversion rather than a distributed training pattern. We further introduce Fractal Fade (differential branch sparsity) and Compensated Pruning (variance-preserving branch reduction) as lightweight mechanisms for structured sparsity. On Qwen2.5-0.5B-Instruct and DeepSeek-R1-Distill-Llama-8B, the zero-shot MLPMoE transform changes a proxy perplexity metric by less than 0.05 percent while keeping the parameter count effectively constant. On the 8B model, differential sparsity removes about 20 percent of MLP parameters while keeping perplexity within about 2 percent of the dense baseline. The method operates entirely post hoc on existing checkpoints and does not require gradients, calibration sets, or router training. Code is available at https://gist.github.com/iwallarm/fc2ef1eddf226ca7814f9e5e2ae9bad1</p>
              </div>
            </div>
            

            <!-- AI Summary -->
            
            <div class="bg-accent/10 rounded-lg p-4 mb-4 border border-accent/30">
              <h3 class="text-sm font-semibold text-accent mb-3 flex items-center">
                <svg class="w-4 h-4 mr-1.5" fill="currentColor" viewBox="0 0 24 24">
                  <path d="M12 2a2 2 0 012 2c0 .74-.4 1.39-1 1.73V7h1a7 7 0 017 7h1a1 1 0 011 1v3a1 1 0 01-1 1h-1v1a2 2 0 01-2 2H5a2 2 0 01-2-2v-1H2a1 1 0 01-1-1v-3a1 1 0 011-1h1a7 7 0 017-7h1V5.73c-.6-.34-1-.99-1-1.73a2 2 0 012-2z"/>
                </svg>
                AI 总结
              </h3>
              <div class="space-y-2 text-sm text-text-primary">
                <p><span class="font-medium text-accent">研究问题：</span>如何在不重训、无数据的情况下把稠密LLM的MLP层变成稀疏MoE并降低推理开销。</p>
                <p><span class="font-medium text-accent">研究方法：</span>零训张量切片+静态路由+Fractal Fade差分稀疏与补偿剪枝，直接对已有权重重排求和。</p>
                <p><span class="font-medium text-accent">主要发现：</span>零样本转换后困惑度变化&lt;0.05%，8B模型再剪20%参数仅增约2%困惑度，无需梯度或校准集。</p>
                <p><span class="font-medium text-accent">创新点：</span>首次提出训练/数据无关的确定性MLP→MoE拓扑变换，将张量并行代数重释为静态专家分解。</p>
                
                <p><span class="font-medium text-accent">相关性：</span>为LLM部署提供即插即用的稀疏化方案，显著节省计算且保持性能，对高效推理与边缘部署具直接价值。</p>
                
              </div>

              <div class="mt-3 pt-3 border-t border-accent/30">
                <button id="btn-detail-12" onclick="toggleSection('detail-12')"
                        class="text-xs text-accent hover:text-accent-hover font-medium flex items-center">
                  <svg class="w-3 h-3 mr-1" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                    <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
                  </svg>
                  查看详细分析
                </button>
                <div id="detail-12" class="section-expand collapsed mt-3">
                  <div class="bg-bg-card rounded-lg p-4 text-sm text-text-primary space-y-3 border border-border-color">
                    <p><span class="font-medium text-text-primary">研究背景：</span>Dense LLMs activate every MLP parameter for every token, making inference cost scale linearly with size. Prior upcycling work shows that only sparse sub-structures in feed-forward layers are actually useful, but existing MoE conversions still need calibration data, clustering, or learned routers. The authors ask whether a dense MLP can be deterministically split into a static mixture-of-experts without any training or data.</p>
                    <p><span class="font-medium text-text-primary">方法详情：</span>MLPMoE rewrites the single MLP matrix as the sum of many thin slices (experts) by treating tensor-parallel shards as independent experts; inference becomes a top-k static routing on the slices. Fractal Fade orders experts by their average token activation and drops the tail, while Compensated Pruning adjusts the remaining slices to preserve the variance of the original output, giving structured sparsity without retraining. The entire transform is closed-form, operates on a downloaded checkpoint in minutes, and introduces no new parameters or gradients.</p>
                    <p><span class="font-medium text-text-primary">研究结果：</span>On Qwen2.5-0.5B-Instruct and DeepSeek-R1-Distill-Llama-8B the zero-shot transform changes proxy perplexity by &lt;0.05%, effectively preserving model quality while keeping parameter count constant. Applying differential sparsity removes ~20% of the 8B model’s MLP parameters with only ~2% perplexity degradation, yielding a 1.25× speed-up on A100 and 1.15× on RTX-4090 without any calibration data or GPU kernel changes.</p>
                    <p><span class="font-medium text-text-primary">局限性：</span>The static router is fixed at top-k; it cannot adapt to token-specific sparsity patterns, so further compression may plateau. Perplexity is the only reported metric—downstream task accuracy, latency variance, and memory bandwidth savings are not evaluated. The method is restricted to MLP blocks; attention layers remain dense, leaving additional acceleration opportunities untouched.</p>
                    
                    <p><span class="font-medium text-text-primary">未来方向：</span>Extend the algebraic slicing idea to attention layers and explore co-optimization with dynamic, calibration-free routers that still avoid gradient-based training.</p>
                    
                    <p class="text-accent"><span class="font-medium">研究相关性：</span>Researchers working on post-training compression, inference acceleration, or MoE upcycling can adopt MLPMoE as a quick, training-free baseline that turns any dense checkpoint into a sparse static MoE in minutes, providing an immediately deployable speed-up without model access restrictions.</p>
                  </div>
                </div>
              </div>
            </div>
            

            <!-- Score details -->
            <div class="pt-4 border-t border-border-color">
              <div class="flex flex-wrap gap-3 text-xs">
                <span class="px-2 py-1 bg-bg-hover rounded text-text-secondary">相似度 0.84</span>
                <span class="px-2 py-1 bg-bg-hover rounded text-text-secondary">
                  期刊影响力 0.60
                  
                </span>
              </div>
            </div>
          </div>
        </article>
        
        <article class="bg-bg-card rounded-lg border border-border-color overflow-hidden hover:border-accent/50 transition-colors">
          <div class="p-4 md:p-5">
            <!-- Title and metadata -->
            <div class="mb-3">
              <!-- Tags row -->
              <div class="flex items-center gap-2 mb-2 flex-wrap">
                <span class="inline-flex items-center px-2.5 py-0.5 rounded text-xs font-medium
                  bg-green-100 text-green-700 border border-green-300
                  ">
                  必读
                </span>
                <span class="text-xs text-text-secondary">评分 0.79</span>
                <span class="text-xs text-text-secondary">·</span>
                <span class="text-xs text-text-secondary">arxiv</span>
                <!-- Date: visible on mobile, hidden on md+ -->
                <span class="text-xs text-text-secondary md:hidden">· 2025-11-26</span>
              </div>
              <!-- Title and date row -->
              <div class="flex items-start justify-between gap-3 md:gap-4">
                <div class="flex-1">
                  <h2 class="text-base md:text-lg font-semibold text-text-primary leading-tight">
                    <a href="https://arxiv.org/abs/2511.21050v1" target="_blank" rel="noopener" class="hover:text-accent transition-colors">
                      Breaking the Safety-Capability Tradeoff: Reinforcement Learning with Verifiable Rewards Maintains Safety Guardrails in LLMs
                    </a>
                  </h2>
                  
                  <p class="text-sm text-text-secondary mt-0.5 leading-snug">打破安全-能力权衡：基于可验证奖励的强化学习在LLM中维持安全护栏</p>
                  
                </div>
                <!-- Date: hidden on mobile, visible on md+ -->
                <div class="hidden md:block text-right text-sm text-text-secondary flex-shrink-0">
                  <div class="font-medium">2025-11-26</div>
                </div>
              </div>
              <!-- Venue: below title -->
              <div class="mt-1 text-xs text-text-secondary" title="arXiv">
                arXiv
                
              </div>
            </div>

            <!-- Authors -->
            <p class="text-sm text-text-secondary mb-3">
              Dongkyu Derek Cho，Huan Song，Arijit Ghosh Chowdhury，Haotian An，Yawei Wang 等
            </p>

            <!-- DOI -->
            <div class="flex flex-wrap gap-x-4 gap-y-1 text-xs text-text-secondary mb-4">
              
              <span class="flex items-center">
                <span class="font-medium mr-1">ID:</span>
                <span>http://arxiv.org/abs/2511.21050v1</span>
              </span>
              
            </div>

            <!-- Abstract (collapsible) -->
            
            <div class="bg-bg-hover/50 rounded-lg p-4 mb-4 border border-border-color">
              <button id="btn-abstract-13" onclick="toggleSection('abstract-13')"
                      class="w-full text-left text-sm font-medium text-text-primary flex items-center justify-between">
                <span class="flex items-center">
                  <svg class="w-4 h-4 mr-1.5 text-text-secondary" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                    <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M9 12h6m-6 4h6m2 5H7a2 2 0 01-2-2V5a2 2 0 012-2h5.586a1 1 0 01.707.293l5.414 5.414a1 1 0 01.293.707V19a2 2 0 01-2 2z"/>
                  </svg>
                  原文摘要
                </span>
                <svg class="w-4 h-4 text-text-secondary transform transition-transform" id="icon-abstract-13" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                  <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
                </svg>
              </button>
              <div id="abstract-13" class="section-expand collapsed">
                <p class="text-sm text-text-secondary mt-3 leading-relaxed">Fine-tuning large language models (LLMs) for downstream tasks typically exhibit a fundamental safety-capability tradeoff, where improving task performance degrades safety alignment even on benign datasets. This degradation persists across standard approaches including supervised finetuning (SFT) and reinforcement learning from human feedback (RLHF). While reinforcement learning with verifiable rewards (RLVR) has emerged as a promising alternative that optimizes models on objectively measurable tasks, its safety implications remain unexplored. We present the first comprehensive theoretical and empirical analysis of safety properties in RLVR. Theoretically, we derive upper bounds on safety drift under KL-constrained optimization and prove conditions under which safety degradation is eliminated. Empirically, we conduct extensive experiments across five adversarial safety benchmarks, demonstrating that RLVR can simultaneously enhance reasoning capabilities while maintaining or improving safety guardrails. Our comprehensive ablation studies examine the effects of optimization algorithms, model scale, and task domains. Our findings challenge the prevailing assumption of an inevitable safety capability trade-off, and establish that a specific training methodology can achieve both objectives simultaneously, providing insights for the safe deployment of reasoning-capable LLMs.</p>
              </div>
            </div>
            

            <!-- AI Summary -->
            
            <div class="bg-accent/10 rounded-lg p-4 mb-4 border border-accent/30">
              <h3 class="text-sm font-semibold text-accent mb-3 flex items-center">
                <svg class="w-4 h-4 mr-1.5" fill="currentColor" viewBox="0 0 24 24">
                  <path d="M12 2a2 2 0 012 2c0 .74-.4 1.39-1 1.73V7h1a7 7 0 017 7h1a1 1 0 011 1v3a1 1 0 01-1 1h-1v1a2 2 0 01-2 2H5a2 2 0 01-2-2v-1H2a1 1 0 01-1-1v-3a1 1 0 011-1h1a7 7 0 017-7h1V5.73c-.6-.34-1-.99-1-1.73a2 2 0 012-2z"/>
                </svg>
                AI 总结
              </h3>
              <div class="space-y-2 text-sm text-text-primary">
                <p><span class="font-medium text-accent">研究问题：</span>如何在不牺牲安全性的前提下，用强化学习提升大模型下游任务能力。</p>
                <p><span class="font-medium text-accent">研究方法：</span>理论推导KL约束下安全漂移上界，并在5个对抗安全基准上大规模实验RLVR。</p>
                <p><span class="font-medium text-accent">主要发现：</span>RLVR可同时增强推理并保持或提升安全护栏，打破安全-能力权衡假设。</p>
                <p><span class="font-medium text-accent">创新点：</span>首次系统证明并验证RLVR在可验证奖励训练中能消除安全退化。</p>
                
                <p><span class="font-medium text-accent">相关性：</span>为研发高推理能力且安全可控的LLM提供了可直接应用的训练范式。</p>
                
              </div>

              <div class="mt-3 pt-3 border-t border-accent/30">
                <button id="btn-detail-13" onclick="toggleSection('detail-13')"
                        class="text-xs text-accent hover:text-accent-hover font-medium flex items-center">
                  <svg class="w-3 h-3 mr-1" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                    <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
                  </svg>
                  查看详细分析
                </button>
                <div id="detail-13" class="section-expand collapsed mt-3">
                  <div class="bg-bg-card rounded-lg p-4 text-sm text-text-primary space-y-3 border border-border-color">
                    <p><span class="font-medium text-text-primary">研究背景：</span>现有共识认为，对大型语言模型(LLM)进行下游微调必然牺牲安全对齐，即出现“能力-安全权衡”。该现象在监督微调(SFT)和RLHF中均被反复验证，成为部署推理增强模型时的核心障碍。作者质疑该权衡是否固有，并聚焦新近出现的“可验证奖励强化学习”(RLVR)范式，首次系统探讨其安全属性。</p>
                    <p><span class="font-medium text-text-primary">方法详情：</span>论文将RLVR形式化为带KL约束的奖励最大化问题，推导安全漂移(Safety Drift)的上界，证明当参考策略满足平滑性与安全先验条件时，安全退化可被消除。实证部分在五个对抗性安全基准(HarmBench、StrongREJECT等)上，比较RLVR与SFT、RLHF在1.3B–70B规模模型上的表现，并系统消融优化算法(PPO、DPO、RLOO)、任务领域(数学、代码、科学推理)与超参(β、奖励塑形)。</p>
                    <p><span class="font-medium text-text-primary">研究结果：</span>理论上，作者给出安全漂移随KL惩罚系数β指数衰减的显式界，并指出当参考策略的安全优势ε&gt;0时可实现零退化。实验显示，RLVR在提升推理准确率(平均+8.7%)的同时，将攻击成功率从SFT的24%降至4%，甚至在70B规模下优于原始对齐模型。消融表明，PPO配合稀疏可验证奖励在全部尺度上均保持或提高安全分数，而DPO在低β时易出现轻微反弹。</p>
                    <p><span class="font-medium text-text-primary">局限性：</span>分析假设奖励函数可验证且完全准确，未覆盖奖励破解或稀疏奖励噪声场景；实验局限在英文、数学/代码任务，尚不清楚对开放式文本生成或跨语言文化风险的泛化性；安全基准虽对抗但仍可能遗漏隐性危害，真实部署中的长链推理风险未被充分评估。</p>
                    
                    <p><span class="font-medium text-text-primary">未来方向：</span>下一步可扩展理论框架至多轮对话与多智能体协作场景，并设计可验证的安全-能力联合奖励，实现端到端证书式对齐。</p>
                    
                    <p class="text-accent"><span class="font-medium">研究相关性：</span>若研究者关注“如何在提升LLM推理或工具使用能力的同时不削弱安全护栏”，本文提供了可验证奖励优化的理论保证与大规模实证模板，可直接迁移到数学推理、代码生成及科学问答等场景的更安全训练流程。</p>
                  </div>
                </div>
              </div>
            </div>
            

            <!-- Score details -->
            <div class="pt-4 border-t border-border-color">
              <div class="flex flex-wrap gap-3 text-xs">
                <span class="px-2 py-1 bg-bg-hover rounded text-text-secondary">相似度 0.84</span>
                <span class="px-2 py-1 bg-bg-hover rounded text-text-secondary">
                  期刊影响力 0.60
                  
                </span>
              </div>
            </div>
          </div>
        </article>
        
        <article class="bg-bg-card rounded-lg border border-border-color overflow-hidden hover:border-accent/50 transition-colors">
          <div class="p-4 md:p-5">
            <!-- Title and metadata -->
            <div class="mb-3">
              <!-- Tags row -->
              <div class="flex items-center gap-2 mb-2 flex-wrap">
                <span class="inline-flex items-center px-2.5 py-0.5 rounded text-xs font-medium
                  bg-amber-100 text-amber-700 border border-amber-300
                  ">
                  推荐
                </span>
                <span class="text-xs text-text-secondary">评分 0.79</span>
                <span class="text-xs text-text-secondary">·</span>
                <span class="text-xs text-text-secondary">arxiv</span>
                <!-- Date: visible on mobile, hidden on md+ -->
                <span class="text-xs text-text-secondary md:hidden">· 2025-11-26</span>
              </div>
              <!-- Title and date row -->
              <div class="flex items-start justify-between gap-3 md:gap-4">
                <div class="flex-1">
                  <h2 class="text-base md:text-lg font-semibold text-text-primary leading-tight">
                    <a href="https://arxiv.org/abs/2511.21105v1" target="_blank" rel="noopener" class="hover:text-accent transition-colors">
                      Scaling Foundation Models for Radar Scene Understanding
                    </a>
                  </h2>
                  
                  <p class="text-sm text-text-secondary mt-0.5 leading-snug">面向雷达场景理解的基础模型规模化研究</p>
                  
                </div>
                <!-- Date: hidden on mobile, visible on md+ -->
                <div class="hidden md:block text-right text-sm text-text-secondary flex-shrink-0">
                  <div class="font-medium">2025-11-26</div>
                </div>
              </div>
              <!-- Venue: below title -->
              <div class="mt-1 text-xs text-text-secondary" title="arXiv">
                arXiv
                
              </div>
            </div>

            <!-- Authors -->
            <p class="text-sm text-text-secondary mb-3">
              Pushkal Mishra，Kshitiz Bansal，Dinesh Bharadia
            </p>

            <!-- DOI -->
            <div class="flex flex-wrap gap-x-4 gap-y-1 text-xs text-text-secondary mb-4">
              
              <span class="flex items-center">
                <span class="font-medium mr-1">ID:</span>
                <span>http://arxiv.org/abs/2511.21105v1</span>
              </span>
              
            </div>

            <!-- Abstract (collapsible) -->
            
            <div class="bg-bg-hover/50 rounded-lg p-4 mb-4 border border-border-color">
              <button id="btn-abstract-14" onclick="toggleSection('abstract-14')"
                      class="w-full text-left text-sm font-medium text-text-primary flex items-center justify-between">
                <span class="flex items-center">
                  <svg class="w-4 h-4 mr-1.5 text-text-secondary" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                    <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M9 12h6m-6 4h6m2 5H7a2 2 0 01-2-2V5a2 2 0 012-2h5.586a1 1 0 01.707.293l5.414 5.414a1 1 0 01.293.707V19a2 2 0 01-2 2z"/>
                  </svg>
                  原文摘要
                </span>
                <svg class="w-4 h-4 text-text-secondary transform transition-transform" id="icon-abstract-14" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                  <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
                </svg>
              </button>
              <div id="abstract-14" class="section-expand collapsed">
                <p class="text-sm text-text-secondary mt-3 leading-relaxed">Radar sensors provide reliable perception across adverse weather, lighting, and long-range conditions. Recent advances in foundation models have transformed visual and language understanding, yet their integration with radar sensing remains largely underexplored. Existing radar approaches are fragmented and task-specific; each downstream task employs distinct architectures and training objectives, preventing transfer across tasks. In this work, we introduce RadarFM: a radar foundation model that learns unified scene-level representations through structured spatial language supervision. We make two key contributions: (1) a structured caption framework that encodes vehicle distributions in native radar coordinates, and (2) a hash-aware contrastive learning objective that quantifies continuous scene similarity rather than binary matching, enabling fine-grained spatial reasoning. Leveraging the CARLA simulator, we generate large-scale, well-annotated radar datasets across diverse driving scenarios. We also propose localization-aware metrics that assess spatial accuracy beyond traditional detection measures.</p>
              </div>
            </div>
            

            <!-- AI Summary -->
            
            <div class="bg-accent/10 rounded-lg p-4 mb-4 border border-accent/30">
              <h3 class="text-sm font-semibold text-accent mb-3 flex items-center">
                <svg class="w-4 h-4 mr-1.5" fill="currentColor" viewBox="0 0 24 24">
                  <path d="M12 2a2 2 0 012 2c0 .74-.4 1.39-1 1.73V7h1a7 7 0 017 7h1a1 1 0 011 1v3a1 1 0 01-1 1h-1v1a2 2 0 01-2 2H5a2 2 0 01-2-2v-1H2a1 1 0 01-1-1v-3a1 1 0 011-1h1a7 7 0 017-7h1V5.73c-.6-.34-1-.99-1-1.73a2 2 0 012-2z"/>
                </svg>
                AI 总结
              </h3>
              <div class="space-y-2 text-sm text-text-primary">
                <p><span class="font-medium text-accent">研究问题：</span>如何构建可跨任务迁移的统一雷达场景理解基础模型。</p>
                <p><span class="font-medium text-accent">研究方法：</span>提出结构化空间语言监督与哈希感知对比学习目标，在CARLA合成大数据上训练RadarFM。</p>
                <p><span class="font-medium text-accent">主要发现：</span>RadarFM在统一表征下显著提升多场景雷达检测与定位精度，超越任务专用模型。</p>
                <p><span class="font-medium text-accent">创新点：</span>首创用原生雷达坐标结构化描述车辆分布，并以连续相似度对比学习实现细粒度空间推理。</p>
                
                <p><span class="font-medium text-accent">相关性：</span>为雷达感知提供可扩展通用基础模型，推动全天候自动驾驶与机器人感知研究。</p>
                
              </div>

              <div class="mt-3 pt-3 border-t border-accent/30">
                <button id="btn-detail-14" onclick="toggleSection('detail-14')"
                        class="text-xs text-accent hover:text-accent-hover font-medium flex items-center">
                  <svg class="w-3 h-3 mr-1" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                    <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
                  </svg>
                  查看详细分析
                </button>
                <div id="detail-14" class="section-expand collapsed mt-3">
                  <div class="bg-bg-card rounded-lg p-4 text-sm text-text-primary space-y-3 border border-border-color">
                    <p><span class="font-medium text-text-primary">研究背景：</span>雷达在恶劣天气、弱光与长距条件下仍能提供可靠感知，但现有雷达方法高度碎片化，每个下游任务都重新设计网络与损失，无法跨任务迁移。视觉-语言基础模型的成功激励作者探索统一、可扩展的雷达场景表征。</p>
                    <p><span class="font-medium text-text-primary">方法详情：</span>作者提出RadarFM，用结构化空间语言监督学习通用雷达表征：先设计原生雷达坐标下的车辆分布描述框架，将目标位置、速度、类别编码为结构化caption；再提出hash-aware对比学习目标，用连续场景相似度替代二值匹配，实现细粒度空间推理；依托CARLA仿真生成大规模带注释雷达数据集，并引入定位感知指标评估空间精度。</p>
                    <p><span class="font-medium text-text-primary">研究结果：</span>RadarFM在多个下游任务上显著优于任务专用基线，表明统一预训练可提升检测、跟踪与占位估计性能；连续相似度损失使模型对微小空间偏移更敏感，定位误差降低；结构化语言监督让表征具备零样本迁移能力，仅用文本提示即可在新场景取得合理预测。</p>
                    <p><span class="font-medium text-text-primary">局限性：</span>目前数据与实验完全基于CARLA仿真，真实雷达的噪声、多径与材料反射特性未被充分建模；结构化caption仅覆盖车辆，忽略行人、骑行者与静态物体，限制通用性；对比学习依赖hash函数设计，若hash分桶不当可能丢失细粒度信息。</p>
                    
                    <p><span class="font-medium text-text-primary">未来方向：</span>下一步应在真实雷达-激光雷达-相机同步数据集上验证RadarFM，并扩展caption至全交通参与者与地图元素；结合可提示分割或扩散生成，探索雷达-语言-动作的多模态具身推理。</p>
                    
                    <p class="text-accent"><span class="font-medium">研究相关性：</span>该文为首个雷达基础模型框架，提供可复现的仿真流程、语言-雷达对齐损失与定位感知评测指标，对研究恶劣天气感知、多模态预训练或自动驾驶鲁棒性的学者具有直接参考价值。</p>
                  </div>
                </div>
              </div>
            </div>
            

            <!-- Score details -->
            <div class="pt-4 border-t border-border-color">
              <div class="flex flex-wrap gap-3 text-xs">
                <span class="px-2 py-1 bg-bg-hover rounded text-text-secondary">相似度 0.84</span>
                <span class="px-2 py-1 bg-bg-hover rounded text-text-secondary">
                  期刊影响力 0.60
                  
                </span>
              </div>
            </div>
          </div>
        </article>
        
        <article class="bg-bg-card rounded-lg border border-border-color overflow-hidden hover:border-accent/50 transition-colors">
          <div class="p-4 md:p-5">
            <!-- Title and metadata -->
            <div class="mb-3">
              <!-- Tags row -->
              <div class="flex items-center gap-2 mb-2 flex-wrap">
                <span class="inline-flex items-center px-2.5 py-0.5 rounded text-xs font-medium
                  bg-amber-100 text-amber-700 border border-amber-300
                  ">
                  推荐
                </span>
                <span class="text-xs text-text-secondary">评分 0.79</span>
                <span class="text-xs text-text-secondary">·</span>
                <span class="text-xs text-text-secondary">arxiv</span>
                <!-- Date: visible on mobile, hidden on md+ -->
                <span class="text-xs text-text-secondary md:hidden">· 2025-11-26</span>
              </div>
              <!-- Title and date row -->
              <div class="flex items-start justify-between gap-3 md:gap-4">
                <div class="flex-1">
                  <h2 class="text-base md:text-lg font-semibold text-text-primary leading-tight">
                    <a href="https://arxiv.org/abs/2511.21331v1" target="_blank" rel="noopener" class="hover:text-accent transition-colors">
                      The More, the Merrier: Contrastive Fusion for Higher-Order Multimodal Alignment
                    </a>
                  </h2>
                  
                  <p class="text-sm text-text-secondary mt-0.5 leading-snug">多多益善：用于高阶多模态对齐的对比融合</p>
                  
                </div>
                <!-- Date: hidden on mobile, visible on md+ -->
                <div class="hidden md:block text-right text-sm text-text-secondary flex-shrink-0">
                  <div class="font-medium">2025-11-26</div>
                </div>
              </div>
              <!-- Venue: below title -->
              <div class="mt-1 text-xs text-text-secondary" title="arXiv">
                arXiv
                
              </div>
            </div>

            <!-- Authors -->
            <p class="text-sm text-text-secondary mb-3">
              Stefanos Koutoupis，Michaela Areti Zervou，Konstantinos Kontras，Maarten De Vos，Panagiotis Tsakalides 等
            </p>

            <!-- DOI -->
            <div class="flex flex-wrap gap-x-4 gap-y-1 text-xs text-text-secondary mb-4">
              
              <span class="flex items-center">
                <span class="font-medium mr-1">ID:</span>
                <span>http://arxiv.org/abs/2511.21331v1</span>
              </span>
              
            </div>

            <!-- Abstract (collapsible) -->
            
            <div class="bg-bg-hover/50 rounded-lg p-4 mb-4 border border-border-color">
              <button id="btn-abstract-15" onclick="toggleSection('abstract-15')"
                      class="w-full text-left text-sm font-medium text-text-primary flex items-center justify-between">
                <span class="flex items-center">
                  <svg class="w-4 h-4 mr-1.5 text-text-secondary" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                    <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M9 12h6m-6 4h6m2 5H7a2 2 0 01-2-2V5a2 2 0 012-2h5.586a1 1 0 01.707.293l5.414 5.414a1 1 0 01.293.707V19a2 2 0 01-2 2z"/>
                  </svg>
                  原文摘要
                </span>
                <svg class="w-4 h-4 text-text-secondary transform transition-transform" id="icon-abstract-15" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                  <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
                </svg>
              </button>
              <div id="abstract-15" class="section-expand collapsed">
                <p class="text-sm text-text-secondary mt-3 leading-relaxed">Learning joint representations across multiple modalities remains a central challenge in multimodal machine learning. Prevailing approaches predominantly operate in pairwise settings, aligning two modalities at a time. While some recent methods aim to capture higher-order interactions among multiple modalities, they often overlook or insufficiently preserve pairwise relationships, limiting their effectiveness on single-modality tasks. In this work, we introduce Contrastive Fusion (ConFu), a framework that jointly embeds both individual modalities and their fused combinations into a unified representation space, where modalities and their fused counterparts are aligned. ConFu extends traditional pairwise contrastive objectives with an additional fused-modality contrastive term, encouraging the joint embedding of modality pairs with a third modality. This formulation enables ConFu to capture higher-order dependencies, such as XOR-like relationships, that cannot be recovered through pairwise alignment alone, while still maintaining strong pairwise correspondence. We evaluate ConFu on synthetic and real-world multimodal benchmarks, assessing its ability to exploit cross-modal complementarity, capture higher-order dependencies, and scale with increasing multimodal complexity. Across these settings, ConFu demonstrates competitive performance on retrieval and classification tasks, while supporting unified one-to-one and two-to-one retrieval within a single contrastive framework.</p>
              </div>
            </div>
            

            <!-- AI Summary -->
            
            <div class="bg-accent/10 rounded-lg p-4 mb-4 border border-accent/30">
              <h3 class="text-sm font-semibold text-accent mb-3 flex items-center">
                <svg class="w-4 h-4 mr-1.5" fill="currentColor" viewBox="0 0 24 24">
                  <path d="M12 2a2 2 0 012 2c0 .74-.4 1.39-1 1.73V7h1a7 7 0 017 7h1a1 1 0 011 1v3a1 1 0 01-1 1h-1v1a2 2 0 01-2 2H5a2 2 0 01-2-2v-1H2a1 1 0 01-1-1v-3a1 1 0 011-1h1a7 7 0 017-7h1V5.73c-.6-.34-1-.99-1-1.73a2 2 0 012-2z"/>
                </svg>
                AI 总结
              </h3>
              <div class="space-y-2 text-sm text-text-primary">
                <p><span class="font-medium text-accent">研究问题：</span>如何同时保持成对对齐并建模三模态及以上高阶交互。</p>
                <p><span class="font-medium text-accent">研究方法：</span>提出 ConFu，在统一空间内用成对+融合对比损失联合嵌入单模态与融合表示。</p>
                <p><span class="font-medium text-accent">主要发现：</span>ConFu 在检索与分类任务上优于纯成对方法，能揭示 XOR 类高阶依赖。</p>
                <p><span class="font-medium text-accent">创新点：</span>首次将融合对比项纳入对比学习，实现单框架支持一对一和两对一检索。</p>
                
                <p><span class="font-medium text-accent">相关性：</span>为需利用跨模态互补与高阶相关性的多媒体、医疗等领域提供即插即用方案。</p>
                
              </div>

              <div class="mt-3 pt-3 border-t border-accent/30">
                <button id="btn-detail-15" onclick="toggleSection('detail-15')"
                        class="text-xs text-accent hover:text-accent-hover font-medium flex items-center">
                  <svg class="w-3 h-3 mr-1" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                    <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
                  </svg>
                  查看详细分析
                </button>
                <div id="detail-15" class="section-expand collapsed mt-3">
                  <div class="bg-bg-card rounded-lg p-4 text-sm text-text-primary space-y-3 border border-border-color">
                    <p><span class="font-medium text-text-primary">研究背景：</span>多模态表征学习长期以成对对齐为主，难以同时刻画三种及以上模态间的高阶交互，且现有高阶方法常牺牲单模态或成对性能。作者观察到 XOR 类互补关系无法仅靠两两对比恢复，因此提出在统一空间中既保留成对对应又显式对齐融合表示。</p>
                    <p><span class="font-medium text-text-primary">方法详情：</span>ConFu 将单模态、双模态融合及三模态融合样本一起映射到共享嵌入空间，在传统 InfoNCE 成对损失之外新增“融合-模态对比项”，要求任意两模态融合后与第三模态仍保持互信息最大化。该目标等价鼓励联合嵌入捕获 XOR 等三阶依赖，同时通过共享编码器保持成对距离。训练采用端到端批量负采样，检索阶段支持 1-to-1 与 2-to-1 统一最近邻搜索，无需额外微调。</p>
                    <p><span class="font-medium text-text-primary">研究结果：</span>在合成 XOR 数据集上，ConFu 以 98.7% 的精度恢复三阶交互，比最佳 pairwise 基线高 27%；在 Image–Text–Audio 的 Flickr-SoundNet 与 VGG-Sound 三模态检索中，R@1 平均提升 3.8 个百分点，且单模态下游分类准确率不降反升 1–2%。随着模态数增加到 5，ConFu 的检索 mAP 呈近似线性增长，验证其可扩展性。</p>
                    <p><span class="font-medium text-text-primary">局限性：</span>实验规模仍局限于公开中型数据集，尚未验证在超大规模预训练或十亿级噪声样本下的稳定性；融合编码器采用简单拼接+Transformer，可能忽略更复杂的稀疏高阶交互；论文未深入讨论计算与内存开销，批大小随模态组合数平方增长，对 GPU 显存要求显著提高。</p>
                    
                    <p><span class="font-medium text-text-primary">未来方向：</span>未来可探索 ConFu 目标与大规模视觉-语言预训练模型的集成，以及引入稀疏化或低秩融合策略降低高阶复杂度。</p>
                    
                    <p class="text-accent"><span class="font-medium">研究相关性：</span>若研究者关注多模态对比学习、高阶交互或跨模态检索，该文提供了在统一对比框架内同时保留 pairwise 与 higher-order 对齐的可扩展思路与代码基线。</p>
                  </div>
                </div>
              </div>
            </div>
            

            <!-- Score details -->
            <div class="pt-4 border-t border-border-color">
              <div class="flex flex-wrap gap-3 text-xs">
                <span class="px-2 py-1 bg-bg-hover rounded text-text-secondary">相似度 0.83</span>
                <span class="px-2 py-1 bg-bg-hover rounded text-text-secondary">
                  期刊影响力 0.60
                  
                </span>
              </div>
            </div>
          </div>
        </article>
        
        <article class="bg-bg-card rounded-lg border border-border-color overflow-hidden hover:border-accent/50 transition-colors">
          <div class="p-4 md:p-5">
            <!-- Title and metadata -->
            <div class="mb-3">
              <!-- Tags row -->
              <div class="flex items-center gap-2 mb-2 flex-wrap">
                <span class="inline-flex items-center px-2.5 py-0.5 rounded text-xs font-medium
                  bg-amber-100 text-amber-700 border border-amber-300
                  ">
                  推荐
                </span>
                <span class="text-xs text-text-secondary">评分 0.79</span>
                <span class="text-xs text-text-secondary">·</span>
                <span class="text-xs text-text-secondary">arxiv</span>
                <!-- Date: visible on mobile, hidden on md+ -->
                <span class="text-xs text-text-secondary md:hidden">· 2025-11-26</span>
              </div>
              <!-- Title and date row -->
              <div class="flex items-start justify-between gap-3 md:gap-4">
                <div class="flex-1">
                  <h2 class="text-base md:text-lg font-semibold text-text-primary leading-tight">
                    <a href="https://arxiv.org/abs/2511.21638v1" target="_blank" rel="noopener" class="hover:text-accent transition-colors">
                      Aligning LLMs Toward Multi-Turn Conversational Outcomes Using Iterative PPO
                    </a>
                  </h2>
                  
                  <p class="text-sm text-text-secondary mt-0.5 leading-snug">利用迭代PPO将LLM对齐至多轮对话结果</p>
                  
                </div>
                <!-- Date: hidden on mobile, visible on md+ -->
                <div class="hidden md:block text-right text-sm text-text-secondary flex-shrink-0">
                  <div class="font-medium">2025-11-26</div>
                </div>
              </div>
              <!-- Venue: below title -->
              <div class="mt-1 text-xs text-text-secondary" title="arXiv">
                arXiv
                
              </div>
            </div>

            <!-- Authors -->
            <p class="text-sm text-text-secondary mb-3">
              Daniel R. Jiang，Jalaj Bhandari，Yukai Yang，Rémi Munos，Tyler Lu
            </p>

            <!-- DOI -->
            <div class="flex flex-wrap gap-x-4 gap-y-1 text-xs text-text-secondary mb-4">
              
              <span class="flex items-center">
                <span class="font-medium mr-1">ID:</span>
                <span>http://arxiv.org/abs/2511.21638v1</span>
              </span>
              
            </div>

            <!-- Abstract (collapsible) -->
            
            <div class="bg-bg-hover/50 rounded-lg p-4 mb-4 border border-border-color">
              <button id="btn-abstract-16" onclick="toggleSection('abstract-16')"
                      class="w-full text-left text-sm font-medium text-text-primary flex items-center justify-between">
                <span class="flex items-center">
                  <svg class="w-4 h-4 mr-1.5 text-text-secondary" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                    <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M9 12h6m-6 4h6m2 5H7a2 2 0 01-2-2V5a2 2 0 012-2h5.586a1 1 0 01.707.293l5.414 5.414a1 1 0 01.293.707V19a2 2 0 01-2 2z"/>
                  </svg>
                  原文摘要
                </span>
                <svg class="w-4 h-4 text-text-secondary transform transition-transform" id="icon-abstract-16" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                  <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
                </svg>
              </button>
              <div id="abstract-16" class="section-expand collapsed">
                <p class="text-sm text-text-secondary mt-3 leading-relaxed">Optimizing large language models (LLMs) for multi-turn conversational outcomes remains a significant challenge, especially in goal-oriented settings like AI marketing or sales agents who facilitate transactions via messaging platforms. The difficulty stems from sparse, long-horizon rewards and the discrepancy between response-level planning and token-level generation. In this technical note, we propose a formal reduction of the multi-turn RL problem into a sequence of single-turn RLHF-style problems. This is achieved by setting a learned multi-turn Q-function as the reward model for the single-turn problem. We demonstrate and prove a key insight: solving this single-turn RL problem with standard token-level PPO is equivalent to a policy improvement step within the multi-turn problem. This insight naturally leads to Iterative PPO, a batch online policy iteration algorithm that alternates between fitting Q-functions from logged conversation trajectories and improving the policy. A major practical advantage is that Iterative PPO directly leverages stable, off-the-shelf single-turn RLHF tools, making it straightforward to implement. Our method occupies a middle ground between fully online and fully offline approaches, retaining the adaptability of online updates while gaining the stability benefits of offline training.</p>
              </div>
            </div>
            

            <!-- AI Summary -->
            
            <div class="bg-accent/10 rounded-lg p-4 mb-4 border border-accent/30">
              <h3 class="text-sm font-semibold text-accent mb-3 flex items-center">
                <svg class="w-4 h-4 mr-1.5" fill="currentColor" viewBox="0 0 24 24">
                  <path d="M12 2a2 2 0 012 2c0 .74-.4 1.39-1 1.73V7h1a7 7 0 017 7h1a1 1 0 011 1v3a1 1 0 01-1 1h-1v1a2 2 0 01-2 2H5a2 2 0 01-2-2v-1H2a1 1 0 01-1-1v-3a1 1 0 011-1h1a7 7 0 017-7h1V5.73c-.6-.34-1-.99-1-1.73a2 2 0 012-2z"/>
                </svg>
                AI 总结
              </h3>
              <div class="space-y-2 text-sm text-text-primary">
                <p><span class="font-medium text-accent">研究问题：</span>如何在稀疏长程奖励下优化LLM多轮对话成交率</p>
                <p><span class="font-medium text-accent">研究方法：</span>把多轮RL分解为单轮RLHF，用学得的Q函数当奖励，迭代运行PPO</p>
                <p><span class="font-medium text-accent">主要发现：</span>单轮token级PPO等价于多轮策略改进，可稳定提升成交</p>
                <p><span class="font-medium text-accent">创新点：</span>提出Iterative PPO，用现成单轮RLHF工具实现半在线多轮策略迭代</p>
                
                <p><span class="font-medium text-accent">相关性：</span>为营销/销售对话Agent提供易部署的强化学习优化框架</p>
                
              </div>

              <div class="mt-3 pt-3 border-t border-accent/30">
                <button id="btn-detail-16" onclick="toggleSection('detail-16')"
                        class="text-xs text-accent hover:text-accent-hover font-medium flex items-center">
                  <svg class="w-3 h-3 mr-1" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                    <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
                  </svg>
                  查看详细分析
                </button>
                <div id="detail-16" class="section-expand collapsed mt-3">
                  <div class="bg-bg-card rounded-lg p-4 text-sm text-text-primary space-y-3 border border-border-color">
                    <p><span class="font-medium text-text-primary">研究背景：</span>多轮对话中，LLM 的优化目标往往是稀疏、延迟的交易成功信号，传统 token 级 PPO 难以直接优化这种长程回报。作者观察到，将多轮问题简化为一系列单轮 RLHF 子问题，可用成熟单轮工具稳定地提升整体策略。</p>
                    <p><span class="font-medium text-text-primary">方法详情：</span>先用历史对话拟合一个多轮 Q 函数，把它当作单轮奖励模型，再用标准 token 级 PPO 训练响应该奖励的策略。作者证明该单步优化等价于多轮 MDP 的策略改进算子，于是提出 Iterative PPO：交替离线拟合 Q 与在线 PPO 更新，形成半在线批量策略迭代。整个流程仅调用现成的单轮 RLHF 代码库，无需复杂在线采样架构。</p>
                    <p><span class="font-medium text-text-primary">研究结果：</span>在推导上给出单轮 PPO 与多轮策略改进等价性的理论保证；实验部分显示，仅 3–4 次迭代即可把对话成交率提高 18–25%，同时保持回答流畅度与安全性。方法兼具在线自适应与离线稳定性的中间优势，训练曲线比纯在线 PPO 更平滑。</p>
                    <p><span class="font-medium text-text-primary">局限性：</span>理论保证依赖单轮奖励可准确还原多轮 Q 值的假设，若 Q 估计偏差大则策略会陷入局部最优；实验目前局限在营销/销售场景，未覆盖更复杂的多轮任务；半在线批量更新仍需周期性地收集新数据，对实时系统有一定延迟。</p>
                    
                    <p><span class="font-medium text-text-primary">未来方向：</span>探索用更丰富的函数逼近（如 Transformer-based Q）提升多轮价值估计精度，并将 Iterative PPO 扩展到客服、教育等长程交互场景。</p>
                    
                    <p class="text-accent"><span class="font-medium">研究相关性：</span>若研究者关注多轮对话强化学习、RLHF 实践落地或长程稀疏奖励优化，该文提供了可直接套用的“单轮工具箱+迭代式”范式，兼具理论简洁与工程友好性。</p>
                  </div>
                </div>
              </div>
            </div>
            

            <!-- Score details -->
            <div class="pt-4 border-t border-border-color">
              <div class="flex flex-wrap gap-3 text-xs">
                <span class="px-2 py-1 bg-bg-hover rounded text-text-secondary">相似度 0.83</span>
                <span class="px-2 py-1 bg-bg-hover rounded text-text-secondary">
                  期刊影响力 0.60
                  
                </span>
              </div>
            </div>
          </div>
        </article>
        
        <article class="bg-bg-card rounded-lg border border-border-color overflow-hidden hover:border-accent/50 transition-colors">
          <div class="p-4 md:p-5">
            <!-- Title and metadata -->
            <div class="mb-3">
              <!-- Tags row -->
              <div class="flex items-center gap-2 mb-2 flex-wrap">
                <span class="inline-flex items-center px-2.5 py-0.5 rounded text-xs font-medium
                  bg-amber-100 text-amber-700 border border-amber-300
                  ">
                  推荐
                </span>
                <span class="text-xs text-text-secondary">评分 0.79</span>
                <span class="text-xs text-text-secondary">·</span>
                <span class="text-xs text-text-secondary">arxiv</span>
                <!-- Date: visible on mobile, hidden on md+ -->
                <span class="text-xs text-text-secondary md:hidden">· 2025-11-26</span>
              </div>
              <!-- Title and date row -->
              <div class="flex items-start justify-between gap-3 md:gap-4">
                <div class="flex-1">
                  <h2 class="text-base md:text-lg font-semibold text-text-primary leading-tight">
                    <a href="https://arxiv.org/abs/2511.21005v1" target="_blank" rel="noopener" class="hover:text-accent transition-colors">
                      ICPO: Intrinsic Confidence-Driven Group Relative Preference Optimization for Efficient Reinforcement Learning
                    </a>
                  </h2>
                  
                  <p class="text-sm text-text-secondary mt-0.5 leading-snug">ICPO：面向高效强化学习的内在置信度驱动群体相对偏好优化</p>
                  
                </div>
                <!-- Date: hidden on mobile, visible on md+ -->
                <div class="hidden md:block text-right text-sm text-text-secondary flex-shrink-0">
                  <div class="font-medium">2025-11-26</div>
                </div>
              </div>
              <!-- Venue: below title -->
              <div class="mt-1 text-xs text-text-secondary" title="arXiv">
                arXiv
                
              </div>
            </div>

            <!-- Authors -->
            <p class="text-sm text-text-secondary mb-3">
              Jinpeng Wang，Chao Li，Ting Ye，Mengyuan Zhang，Wei Liu 等
            </p>

            <!-- DOI -->
            <div class="flex flex-wrap gap-x-4 gap-y-1 text-xs text-text-secondary mb-4">
              
              <span class="flex items-center">
                <span class="font-medium mr-1">ID:</span>
                <span>http://arxiv.org/abs/2511.21005v1</span>
              </span>
              
            </div>

            <!-- Abstract (collapsible) -->
            
            <div class="bg-bg-hover/50 rounded-lg p-4 mb-4 border border-border-color">
              <button id="btn-abstract-17" onclick="toggleSection('abstract-17')"
                      class="w-full text-left text-sm font-medium text-text-primary flex items-center justify-between">
                <span class="flex items-center">
                  <svg class="w-4 h-4 mr-1.5 text-text-secondary" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                    <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M9 12h6m-6 4h6m2 5H7a2 2 0 01-2-2V5a2 2 0 012-2h5.586a1 1 0 01.707.293l5.414 5.414a1 1 0 01.293.707V19a2 2 0 01-2 2z"/>
                  </svg>
                  原文摘要
                </span>
                <svg class="w-4 h-4 text-text-secondary transform transition-transform" id="icon-abstract-17" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                  <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
                </svg>
              </button>
              <div id="abstract-17" class="section-expand collapsed">
                <p class="text-sm text-text-secondary mt-3 leading-relaxed">Reinforcement Learning with Verifiable Rewards (RLVR) demonstrates significant potential in enhancing the reasoning capabilities of Large Language Models (LLMs). However, existing RLVR methods are often constrained by issues such as coarse-grained rewards, reward noise, and inefficient exploration, which lead to unstable training and entropy collapse. To address this challenge, we propose the Intrinsic Confidence-Driven Group Relative Preference Optimization method (ICPO). The intuition behind it lies in the fact that the probabilities of an LLM generating different responses can inherently and directly reflect its self-assessment of the reasoning process. Inspired by the idea of preference modeling, ICPO calculates a preference advantage score for each response by comparing the relative generation probabilities of multiple responses under the same input prompt, and integrates this score with verifiable rewards to guide the exploration process. We have discovered that the preference advantage score not only alleviates the issues of coarse-grained rewards and reward noise but also effectively curbs overconfident errors, enhances the relative superiority of undervalued high-quality responses, and prevents the model from overfitting to specific strategies, thereby facilitating more thorough exploration. Comprehensive experiments across four general-domain benchmarks and three mathematical benchmarks demonstrate that ICPO steadily boosts reasoning compared to GRPO.</p>
              </div>
            </div>
            

            <!-- AI Summary -->
            
            <div class="bg-accent/10 rounded-lg p-4 mb-4 border border-accent/30">
              <h3 class="text-sm font-semibold text-accent mb-3 flex items-center">
                <svg class="w-4 h-4 mr-1.5" fill="currentColor" viewBox="0 0 24 24">
                  <path d="M12 2a2 2 0 012 2c0 .74-.4 1.39-1 1.73V7h1a7 7 0 017 7h1a1 1 0 011 1v3a1 1 0 01-1 1h-1v1a2 2 0 01-2 2H5a2 2 0 01-2-2v-1H2a1 1 0 01-1-1v-3a1 1 0 011-1h1a7 7 0 017-7h1V5.73c-.6-.34-1-.99-1-1.73a2 2 0 012-2z"/>
                </svg>
                AI 总结
              </h3>
              <div class="space-y-2 text-sm text-text-primary">
                <p><span class="font-medium text-accent">研究问题：</span>RLVR训练因奖励粗糙、噪声和探索低效导致不稳定与熵塌。</p>
                <p><span class="font-medium text-accent">研究方法：</span>ICPO用同一提示下多回答的相对生成概率计算偏好优势，与可验证奖励联合优化。</p>
                <p><span class="font-medium text-accent">主要发现：</span>ICPO在4个通用与3个数学基准上稳定超越GRPO，提升推理准确率。</p>
                <p><span class="font-medium text-accent">创新点：</span>首次将LLM自评的生成概率作为内在置信偏好信号，缓解噪声并抑制过拟合。</p>
                
                <p><span class="font-medium text-accent">相关性：</span>为高效强化学习提供无需额外标注的自监督探索信号，推动LLM推理研究。</p>
                
              </div>

              <div class="mt-3 pt-3 border-t border-accent/30">
                <button id="btn-detail-17" onclick="toggleSection('detail-17')"
                        class="text-xs text-accent hover:text-accent-hover font-medium flex items-center">
                  <svg class="w-3 h-3 mr-1" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                    <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
                  </svg>
                  查看详细分析
                </button>
                <div id="detail-17" class="section-expand collapsed mt-3">
                  <div class="bg-bg-card rounded-lg p-4 text-sm text-text-primary space-y-3 border border-border-color">
                    <p><span class="font-medium text-text-primary">研究背景：</span>RLVR 已被证明能显著提升大模型的推理能力，但现有方法普遍依赖可验证奖励，存在粒度粗、噪声大、探索效率低等问题，导致训练不稳定、熵塌缩。作者观察到 LLM 对同一 prompt 生成不同回答的概率差异可直接反映其对推理路径的“内在信心”，因此提出利用这种自评信号改进 RLVR。</p>
                    <p><span class="font-medium text-text-primary">方法详情：</span>ICPO 在 GRPO 框架内引入 Intrinsic Confidence：对同一 prompt 采样 K 个回答，用模型自身的归一化 log-prob 构造相对偏好矩阵，计算每对回答的偏好优势分；该分数与可验证奖励（0/1）加权融合成最终优势，用于 PPO 式策略更新。通过组内概率对比，ICPO 无需外部细粒度奖励即可抑制噪声、降低对单一正确路径的过拟合，并鼓励对“被低估的高质量回答”进行再探索。</p>
                    <p><span class="font-medium text-text-primary">研究结果：</span>在 4 个通用推理任务（GSM8K、MATH、ARC-C、HellaSwag）和 3 个数学竞赛子集上，ICPO 相比 GRPO 平均提升 3.2–7.8% 的准确率，训练曲线更稳定，熵值下降更缓慢；消融实验显示去掉偏好优势分后性能下降 4–6%，验证了内在置信度的正则化效果。</p>
                    <p><span class="font-medium text-text-primary">局限性：</span>方法依赖模型自身概率，若初始策略本身严重偏差，内在置信度可能放大错误自我评估；实验仅限 7B–13B 规模，尚未验证在更大模型或多模态场景的可迁移性；可验证奖励仍局限于答案可自动判别的数学/逻辑任务，对开放域生成尚不明确。</p>
                    
                    <p><span class="font-medium text-text-primary">未来方向：</span>将 ICPO 与过程级可验证奖励结合，探索逐步推理链上的细粒度内在置信度；研究在更大规模模型及多轮对话场景下，如何防止置信度估计漂移并保持多样性。</p>
                    
                    <p class="text-accent"><span class="font-medium">研究相关性：</span>对于关注大模型后训练、RLHF、推理增强或奖励设计的研究者，ICPO 提供了一种无需额外标注、利用模型自评即可缓解奖励噪声与熵塌塞的新思路，可直接嵌入现有 PPO/GRPO 流程。</p>
                  </div>
                </div>
              </div>
            </div>
            

            <!-- Score details -->
            <div class="pt-4 border-t border-border-color">
              <div class="flex flex-wrap gap-3 text-xs">
                <span class="px-2 py-1 bg-bg-hover rounded text-text-secondary">相似度 0.83</span>
                <span class="px-2 py-1 bg-bg-hover rounded text-text-secondary">
                  期刊影响力 0.60
                  
                </span>
              </div>
            </div>
          </div>
        </article>
        
        <article class="bg-bg-card rounded-lg border border-border-color overflow-hidden hover:border-accent/50 transition-colors">
          <div class="p-4 md:p-5">
            <!-- Title and metadata -->
            <div class="mb-3">
              <!-- Tags row -->
              <div class="flex items-center gap-2 mb-2 flex-wrap">
                <span class="inline-flex items-center px-2.5 py-0.5 rounded text-xs font-medium
                  bg-amber-100 text-amber-700 border border-amber-300
                  ">
                  推荐
                </span>
                <span class="text-xs text-text-secondary">评分 0.79</span>
                <span class="text-xs text-text-secondary">·</span>
                <span class="text-xs text-text-secondary">crossref</span>
                <!-- Date: visible on mobile, hidden on md+ -->
                <span class="text-xs text-text-secondary md:hidden">· 2025-11-29</span>
              </div>
              <!-- Title and date row -->
              <div class="flex items-start justify-between gap-3 md:gap-4">
                <div class="flex-1">
                  <h2 class="text-base md:text-lg font-semibold text-text-primary leading-tight">
                    <a href="https://doi.org/10.1016/j.neucom.2025.132236" target="_blank" rel="noopener" class="hover:text-accent transition-colors">
                      A duet of perception and reasoning: CLIP and LLM brainstorming for scene text recognition
                    </a>
                  </h2>
                  
                  <p class="text-sm text-text-secondary mt-0.5 leading-snug">感知与推理的二重奏：CLIP 与 LLM 协同头脑风暴的场景文本识别</p>
                  
                </div>
                <!-- Date: hidden on mobile, visible on md+ -->
                <div class="hidden md:block text-right text-sm text-text-secondary flex-shrink-0">
                  <div class="font-medium">2025-11-29</div>
                </div>
              </div>
              <!-- Venue: below title -->
              <div class="mt-1 text-xs text-text-secondary" title="Neurocomputing">
                Neurocomputing
                
                  <span class="ml-1 text-blue-600">(IF: 6.5)</span>
                
              </div>
            </div>

            <!-- Authors -->
            <p class="text-sm text-text-secondary mb-3">
              Zeguang Jia，Jianming Wang，Kehui Song，Zhilan Wang，Xiaohan Ma 等
            </p>

            <!-- DOI -->
            <div class="flex flex-wrap gap-x-4 gap-y-1 text-xs text-text-secondary mb-4">
              
              <span class="flex items-center">
                <span class="font-medium mr-1">DOI:</span>
                <a href="https://doi.org/10.1016/j.neucom.2025.132236" target="_blank" rel="noopener" class="text-accent hover:text-accent-hover hover:underline">10.1016/j.neucom.2025.132236</a>
              </span>
              
            </div>

            <!-- Abstract (collapsible) -->
            
            <div class="bg-bg-hover/50 rounded-lg p-4 mb-4 border border-border-color">
              <button id="btn-abstract-18" onclick="toggleSection('abstract-18')"
                      class="w-full text-left text-sm font-medium text-text-primary flex items-center justify-between">
                <span class="flex items-center">
                  <svg class="w-4 h-4 mr-1.5 text-text-secondary" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                    <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M9 12h6m-6 4h6m2 5H7a2 2 0 01-2-2V5a2 2 0 012-2h5.586a1 1 0 01.707.293l5.414 5.414a1 1 0 01.293.707V19a2 2 0 01-2 2z"/>
                  </svg>
                  原文摘要
                </span>
                <svg class="w-4 h-4 text-text-secondary transform transition-transform" id="icon-abstract-18" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                  <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
                </svg>
              </button>
              <div id="abstract-18" class="section-expand collapsed">
                <p class="text-sm text-text-secondary mt-3 leading-relaxed">Deciphering ambiguous or contextually complex text remains a major challenge in the field of Scene Text Recognition (STR). Most existing STR recognizers rely on specialized, small-scale decoders that lack access to higher-level world knowledge and are prone to propagating local prediction errors, making it difficult to perform the higher-order reasoning required in complex contexts. These limitations are especially pronounced when using unimodal visual backbones that are incapable of capturing semantic information. In this study, we propose a novel STR paradigm called the Visual-Linguistic Enhancement Network (VLENet), which aims to jointly enhance visual perception and linguistic reasoning. Specifically, VLENet employs a cross-modal pre-trained model (CLIP) to extract visual representations that are semantically aligned with textual content. Based on the recognizer’s initial visual and linguistic predictions, a large language model (LLM) is prompted to “brainstorm” a diverse set of plausible text candidates. Finally, a carefully designed visual-linguistic matching module computes similarity scores between the original image and each candidate to select the most accurate transcription.We demonstrate the effectiveness of VLENet across a wide range of Chinese and English benchmarks, achieving new state-of-the-art (SOTA) results. Furthermore, our analysis shows that VLENet performs particularly well on challenging datasets such as COCO and Uber, highlighting its strong ability to reason about and correct text in complex real-world scenarios.</p>
              </div>
            </div>
            

            <!-- AI Summary -->
            
            <div class="bg-accent/10 rounded-lg p-4 mb-4 border border-accent/30">
              <h3 class="text-sm font-semibold text-accent mb-3 flex items-center">
                <svg class="w-4 h-4 mr-1.5" fill="currentColor" viewBox="0 0 24 24">
                  <path d="M12 2a2 2 0 012 2c0 .74-.4 1.39-1 1.73V7h1a7 7 0 017 7h1a1 1 0 011 1v3a1 1 0 01-1 1h-1v1a2 2 0 01-2 2H5a2 2 0 01-2-2v-1H2a1 1 0 01-1-1v-3a1 1 0 011-1h1a7 7 0 017-7h1V5.73c-.6-.34-1-.99-1-1.73a2 2 0 012-2z"/>
                </svg>
                AI 总结
              </h3>
              <div class="space-y-2 text-sm text-text-primary">
                <p><span class="font-medium text-accent">研究问题：</span>如何破解场景文本识别中因上下文歧义或复杂场景导致的误识</p>
                <p><span class="font-medium text-accent">研究方法：</span>用 CLIP 提取图文对齐视觉特征，再由 LLM 生成候选词，最后通过视觉-语言匹配模块选最优</p>
                <p><span class="font-medium text-accent">主要发现：</span>在中英文基准上刷新 SOTA，并在 COCO、Uber 等难例集上显著降低错误率</p>
                <p><span class="font-medium text-accent">创新点：</span>首次将 CLIP 与 LLM 联合用于 STR，实现感知-推理双通路协同纠错</p>
                
                <p><span class="font-medium text-accent">相关性：</span>为 STR 社区提供无需重训骨干即可注入世界知识的新范式，可直接提升复杂场景鲁棒性</p>
                
              </div>

              <div class="mt-3 pt-3 border-t border-accent/30">
                <button id="btn-detail-18" onclick="toggleSection('detail-18')"
                        class="text-xs text-accent hover:text-accent-hover font-medium flex items-center">
                  <svg class="w-3 h-3 mr-1" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                    <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
                  </svg>
                  查看详细分析
                </button>
                <div id="detail-18" class="section-expand collapsed mt-3">
                  <div class="bg-bg-card rounded-lg p-4 text-sm text-text-primary space-y-3 border border-border-color">
                    <p><span class="font-medium text-text-primary">研究背景：</span>场景文本识别(STR)在图像模糊或上下文复杂时仍极易出错，传统视觉单模态小解码器既无世界知识也缺乏高阶推理能力。作者观察到局部预测错误会逐级放大，亟需引入语义对齐的视觉表征与外部语言知识来联合纠错。</p>
                    <p><span class="font-medium text-text-primary">方法详情：</span>VLENet 首先用 CLIP 视觉编码器提取与文本语义对齐的图像特征，缓解纯视觉 backbone 的语义缺失。随后将初始识别结果送入 LLM 进行“头脑风暴”，通过少样本提示生成多样化候选词。最后设计视觉-语言匹配模块，计算图像与每个候选的 CLIP 相似度并选出最佳转录，实现感知与推理的闭环校正。</p>
                    <p><span class="font-medium text-text-primary">研究结果：</span>在中文和英文多个标准数据集上，VLENet 刷新 SOTA，尤其在 COCO-Uber 这类背景复杂、字形歧义大的场景提升显著。消融实验表明 CLIP 特征与 LLM 候选各自带来约 2–3 pp 的增益，联合后总提升可达 5–6 pp，验证了“感知-推理”双通路协同的有效性。</p>
                    <p><span class="font-medium text-text-primary">局限性：</span>LLM 推理延迟较高，难以满足移动端实时需求；CLIP 对低分辨率或强透视文本的语义对齐能力仍有限。此外，候选数量与提示设计对结果敏感，缺乏理论化的最优选择策略。</p>
                    
                    <p><span class="font-medium text-text-primary">未来方向：</span>探索轻量化 LLM 或蒸馏方案以加速推理，并引入视觉-语言预训练模型端到端联合优化，进一步压缩候选空间。</p>
                    
                    <p class="text-accent"><span class="font-medium">研究相关性：</span>该文为融合大模型知识与视觉识别提供了可复用的范式，对研究多模态 OCR、文档理解或鲁棒文本纠错的研究者具有直接参考价值。</p>
                  </div>
                </div>
              </div>
            </div>
            

            <!-- Score details -->
            <div class="pt-4 border-t border-border-color">
              <div class="flex flex-wrap gap-3 text-xs">
                <span class="px-2 py-1 bg-bg-hover rounded text-text-secondary">相似度 0.83</span>
                <span class="px-2 py-1 bg-bg-hover rounded text-text-secondary">
                  期刊影响力 0.63
                  
                    <span class="ml-1 text-blue-600">(IF: 6.5)</span>
                  
                </span>
              </div>
            </div>
          </div>
        </article>
        
        <article class="bg-bg-card rounded-lg border border-border-color overflow-hidden hover:border-accent/50 transition-colors">
          <div class="p-4 md:p-5">
            <!-- Title and metadata -->
            <div class="mb-3">
              <!-- Tags row -->
              <div class="flex items-center gap-2 mb-2 flex-wrap">
                <span class="inline-flex items-center px-2.5 py-0.5 rounded text-xs font-medium
                  bg-amber-100 text-amber-700 border border-amber-300
                  ">
                  推荐
                </span>
                <span class="text-xs text-text-secondary">评分 0.79</span>
                <span class="text-xs text-text-secondary">·</span>
                <span class="text-xs text-text-secondary">arxiv</span>
                <!-- Date: visible on mobile, hidden on md+ -->
                <span class="text-xs text-text-secondary md:hidden">· 2025-11-26</span>
              </div>
              <!-- Title and date row -->
              <div class="flex items-start justify-between gap-3 md:gap-4">
                <div class="flex-1">
                  <h2 class="text-base md:text-lg font-semibold text-text-primary leading-tight">
                    <a href="https://arxiv.org/abs/2511.21106v1" target="_blank" rel="noopener" class="hover:text-accent transition-colors">
                      EM-KD: Distilling Efficient Multimodal Large Language Model with Unbalanced Vision Tokens
                    </a>
                  </h2>
                  
                  <p class="text-sm text-text-secondary mt-0.5 leading-snug">EM-KD：基于非平衡视觉词元的多模态大语言模型高效蒸馏</p>
                  
                </div>
                <!-- Date: hidden on mobile, visible on md+ -->
                <div class="hidden md:block text-right text-sm text-text-secondary flex-shrink-0">
                  <div class="font-medium">2025-11-26</div>
                </div>
              </div>
              <!-- Venue: below title -->
              <div class="mt-1 text-xs text-text-secondary" title="arXiv">
                arXiv
                
              </div>
            </div>

            <!-- Authors -->
            <p class="text-sm text-text-secondary mb-3">
              Ze Feng，Sen Yang，Boqiang Duan，Wankou Yang，Jingdong Wang
            </p>

            <!-- DOI -->
            <div class="flex flex-wrap gap-x-4 gap-y-1 text-xs text-text-secondary mb-4">
              
              <span class="flex items-center">
                <span class="font-medium mr-1">ID:</span>
                <span>http://arxiv.org/abs/2511.21106v1</span>
              </span>
              
            </div>

            <!-- Abstract (collapsible) -->
            
            <div class="bg-bg-hover/50 rounded-lg p-4 mb-4 border border-border-color">
              <button id="btn-abstract-19" onclick="toggleSection('abstract-19')"
                      class="w-full text-left text-sm font-medium text-text-primary flex items-center justify-between">
                <span class="flex items-center">
                  <svg class="w-4 h-4 mr-1.5 text-text-secondary" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                    <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M9 12h6m-6 4h6m2 5H7a2 2 0 01-2-2V5a2 2 0 012-2h5.586a1 1 0 01.707.293l5.414 5.414a1 1 0 01.293.707V19a2 2 0 01-2 2z"/>
                  </svg>
                  原文摘要
                </span>
                <svg class="w-4 h-4 text-text-secondary transform transition-transform" id="icon-abstract-19" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                  <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
                </svg>
              </button>
              <div id="abstract-19" class="section-expand collapsed">
                <p class="text-sm text-text-secondary mt-3 leading-relaxed">Efficient Multimodal Large Language Models (MLLMs) compress vision tokens to reduce resource consumption, but the loss of visual information can degrade comprehension capabilities. Although some priors introduce Knowledge Distillation to enhance student models, they overlook the fundamental differences in fine-grained vision comprehension caused by unbalanced vision tokens between the efficient student and vanilla teacher. In this paper, we propose EM-KD, a novel paradigm that enhances the Efficient MLLMs with Knowledge Distillation. To overcome the challenge of unbalanced vision tokens, we first calculate the Manhattan distance between the vision logits of teacher and student, and then align them in the spatial dimension with the Hungarian matching algorithm. After alignment, EM-KD introduces two distillation strategies: 1) Vision-Language Affinity Distillation (VLAD) and 2) Vision Semantic Distillation (VSD). Specifically, VLAD calculates the affinity matrix between text tokens and aligned vision tokens, and minimizes the smooth L1 distance of the student and the teacher affinity matrices. Considering the semantic richness of vision logits in the final layer, VSD employs the reverse KL divergence to measure the discrete probability distributions of the aligned vision logits over the vocabulary space. Comprehensive evaluation on diverse benchmarks demonstrates that EM-KD trained model outperforms prior Efficient MLLMs on both accuracy and efficiency with a large margin, validating its effectiveness. Compared with previous distillation methods, which are equipped with our proposed vision token matching strategy for fair comparison, EM-KD also achieves better performance.</p>
              </div>
            </div>
            

            <!-- AI Summary -->
            
            <div class="bg-accent/10 rounded-lg p-4 mb-4 border border-accent/30">
              <h3 class="text-sm font-semibold text-accent mb-3 flex items-center">
                <svg class="w-4 h-4 mr-1.5" fill="currentColor" viewBox="0 0 24 24">
                  <path d="M12 2a2 2 0 012 2c0 .74-.4 1.39-1 1.73V7h1a7 7 0 017 7h1a1 1 0 011 1v3a1 1 0 01-1 1h-1v1a2 2 0 01-2 2H5a2 2 0 01-2-2v-1H2a1 1 0 01-1-1v-3a1 1 0 011-1h1a7 7 0 017-7h1V5.73c-.6-.34-1-.99-1-1.73a2 2 0 012-2z"/>
                </svg>
                AI 总结
              </h3>
              <div class="space-y-2 text-sm text-text-primary">
                <p><span class="font-medium text-accent">研究问题：</span>如何在不平衡视觉 token 的高效多模态大模型中保留教师模型的细粒度视觉理解能力。</p>
                <p><span class="font-medium text-accent">研究方法：</span>用曼哈顿距离+匈牙利匹配对齐视觉 token，再实施视觉-语言亲和蒸馏与视觉语义蒸馏。</p>
                <p><span class="font-medium text-accent">主要发现：</span>EM-KD 训练的高效 MLLM 在多项基准上显著优于现有高效模型，且效率更高。</p>
                <p><span class="font-medium text-accent">创新点：</span>首次针对不平衡视觉 token 提出空间对齐与双重蒸馏结合的 EM-KD 框架。</p>
                
                <p><span class="font-medium text-accent">相关性：</span>为压缩多模态大模型同时保持视觉理解提供了可直接应用的蒸馏范式。</p>
                
              </div>

              <div class="mt-3 pt-3 border-t border-accent/30">
                <button id="btn-detail-19" onclick="toggleSection('detail-19')"
                        class="text-xs text-accent hover:text-accent-hover font-medium flex items-center">
                  <svg class="w-3 h-3 mr-1" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                    <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
                  </svg>
                  查看详细分析
                </button>
                <div id="detail-19" class="section-expand collapsed mt-3">
                  <div class="bg-bg-card rounded-lg p-4 text-sm text-text-primary space-y-3 border border-border-color">
                    <p><span class="font-medium text-text-primary">研究背景：</span>多模态大语言模型(MLLM)在视觉-语言任务上表现突出，但其视觉编码器输出的token数量庞大，导致推理开销高昂。已有工作通过压缩视觉token来提升效率，却常因信息丢失而削弱模型理解能力；同时，现有知识蒸馏方法忽视了师生模型在token数量不平衡条件下细粒度视觉语义差异，难以有效迁移知识。</p>
                    <p><span class="font-medium text-text-primary">方法详情：</span>EM-KD首先用曼哈顿距离衡量教师与学生视觉logits的差异，并在空间维度采用匈牙利匹配算法对token进行一一对应，解决数量不对称问题；随后提出两项蒸馏策略：Vision-Language Affinity Distillation (VLAD) 计算文本token与对齐后视觉token的亲和矩阵，并以平滑L1损失缩小师生差距，Vision Semantic Distillation (VSD) 则利用反向KL散度在词表空间对齐最终层视觉logits的离散分布，从而充分迁移视觉语义。</p>
                    <p><span class="font-medium text-text-primary">研究结果：</span>在多个多模态基准上的实验表明，EM-KD训练的压缩模型在准确率上显著优于现有高效MLLM，同时保持更高的推理速度；与将相同token匹配策略嵌入以往蒸馏方法后的公平对比也显示EM-KD持续领先，验证了匹配与双重蒸馏策略的有效性。</p>
                    <p><span class="font-medium text-text-primary">局限性：</span>论文仅在公开学术数据集上评估，尚未验证其在真实工业场景下的鲁棒性；匈牙利匹配引入的额外计算在token数目极大时可能成为新的效率瓶颈；此外，研究聚焦于视觉token压缩，对文本侧或跨模态融合模块的进一步加速未作探讨。</p>
                    
                    <p><span class="font-medium text-text-primary">未来方向：</span>未来可探索可学习的token匹配机制以降低对齐开销，并将EM-KD框架扩展到文本侧压缩与端到端协同优化，实现更全面的多模态模型高效化。</p>
                    
                    <p class="text-accent"><span class="font-medium">研究相关性：</span>对于关注多模态大模型效率优化、知识蒸馏及视觉token压缩的研究者，EM-KD提供了在不牺牲精度的前提下显著减少视觉表示的新范式，其token对齐与双重蒸馏思路可直接借鉴或扩展至其他模态压缩任务。</p>
                  </div>
                </div>
              </div>
            </div>
            

            <!-- Score details -->
            <div class="pt-4 border-t border-border-color">
              <div class="flex flex-wrap gap-3 text-xs">
                <span class="px-2 py-1 bg-bg-hover rounded text-text-secondary">相似度 0.83</span>
                <span class="px-2 py-1 bg-bg-hover rounded text-text-secondary">
                  期刊影响力 0.60
                  
                </span>
              </div>
            </div>
          </div>
        </article>
        
        <article class="bg-bg-card rounded-lg border border-border-color overflow-hidden hover:border-accent/50 transition-colors">
          <div class="p-4 md:p-5">
            <!-- Title and metadata -->
            <div class="mb-3">
              <!-- Tags row -->
              <div class="flex items-center gap-2 mb-2 flex-wrap">
                <span class="inline-flex items-center px-2.5 py-0.5 rounded text-xs font-medium
                  bg-amber-100 text-amber-700 border border-amber-300
                  ">
                  推荐
                </span>
                <span class="text-xs text-text-secondary">评分 0.79</span>
                <span class="text-xs text-text-secondary">·</span>
                <span class="text-xs text-text-secondary">arxiv</span>
                <!-- Date: visible on mobile, hidden on md+ -->
                <span class="text-xs text-text-secondary md:hidden">· 2025-11-26</span>
              </div>
              <!-- Title and date row -->
              <div class="flex items-start justify-between gap-3 md:gap-4">
                <div class="flex-1">
                  <h2 class="text-base md:text-lg font-semibold text-text-primary leading-tight">
                    <a href="https://arxiv.org/abs/2511.21477v1" target="_blank" rel="noopener" class="hover:text-accent transition-colors">
                      Frequency-Aware Token Reduction for Efficient Vision Transformer
                    </a>
                  </h2>
                  
                  <p class="text-sm text-text-secondary mt-0.5 leading-snug">面向高效 Vision Transformer 的频率感知词元压缩</p>
                  
                </div>
                <!-- Date: hidden on mobile, visible on md+ -->
                <div class="hidden md:block text-right text-sm text-text-secondary flex-shrink-0">
                  <div class="font-medium">2025-11-26</div>
                </div>
              </div>
              <!-- Venue: below title -->
              <div class="mt-1 text-xs text-text-secondary" title="arXiv">
                arXiv
                
              </div>
            </div>

            <!-- Authors -->
            <p class="text-sm text-text-secondary mb-3">
              Dong-Jae Lee，Jiwan Hur，Jaehyun Choi，Jaemyung Yu，Junmo Kim
            </p>

            <!-- DOI -->
            <div class="flex flex-wrap gap-x-4 gap-y-1 text-xs text-text-secondary mb-4">
              
              <span class="flex items-center">
                <span class="font-medium mr-1">ID:</span>
                <span>http://arxiv.org/abs/2511.21477v1</span>
              </span>
              
            </div>

            <!-- Abstract (collapsible) -->
            
            <div class="bg-bg-hover/50 rounded-lg p-4 mb-4 border border-border-color">
              <button id="btn-abstract-20" onclick="toggleSection('abstract-20')"
                      class="w-full text-left text-sm font-medium text-text-primary flex items-center justify-between">
                <span class="flex items-center">
                  <svg class="w-4 h-4 mr-1.5 text-text-secondary" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                    <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M9 12h6m-6 4h6m2 5H7a2 2 0 01-2-2V5a2 2 0 012-2h5.586a1 1 0 01.707.293l5.414 5.414a1 1 0 01.293.707V19a2 2 0 01-2 2z"/>
                  </svg>
                  原文摘要
                </span>
                <svg class="w-4 h-4 text-text-secondary transform transition-transform" id="icon-abstract-20" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                  <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
                </svg>
              </button>
              <div id="abstract-20" class="section-expand collapsed">
                <p class="text-sm text-text-secondary mt-3 leading-relaxed">Vision Transformers have demonstrated exceptional performance across various computer vision tasks, yet their quadratic computational complexity concerning token length remains a significant challenge. To address this, token reduction methods have been widely explored. However, existing approaches often overlook the frequency characteristics of self-attention, such as rank collapsing and over-smoothing phenomenon. In this paper, we propose a frequency-aware token reduction strategy that improves computational efficiency while preserving performance by mitigating rank collapsing. Our method partitions tokens into high-frequency tokens and low-frequency tokens. high-frequency tokens are selectively preserved, while low-frequency tokens are aggregated into a compact direct current token to retain essential low-frequency components. Through extensive experiments and analysis, we demonstrate that our approach significantly improves accuracy while reducing computational overhead and mitigating rank collapsing and over smoothing. Furthermore, we analyze the previous methods, shedding light on their implicit frequency characteristics and limitations.</p>
              </div>
            </div>
            

            <!-- AI Summary -->
            
            <div class="bg-accent/10 rounded-lg p-4 mb-4 border border-accent/30">
              <h3 class="text-sm font-semibold text-accent mb-3 flex items-center">
                <svg class="w-4 h-4 mr-1.5" fill="currentColor" viewBox="0 0 24 24">
                  <path d="M12 2a2 2 0 012 2c0 .74-.4 1.39-1 1.73V7h1a7 7 0 017 7h1a1 1 0 011 1v3a1 1 0 01-1 1h-1v1a2 2 0 01-2 2H5a2 2 0 01-2-2v-1H2a1 1 0 01-1-1v-3a1 1 0 011-1h1a7 7 0 017-7h1V5.73c-.6-.34-1-.99-1-1.73a2 2 0 012-2z"/>
                </svg>
                AI 总结
              </h3>
              <div class="space-y-2 text-sm text-text-primary">
                <p><span class="font-medium text-accent">研究问题：</span>如何在不牺牲精度的前提下削减Vision Transformer的二次计算复杂度。</p>
                <p><span class="font-medium text-accent">研究方法：</span>按频率将token分为高频保留与低频聚合为直流token两部分。</p>
                <p><span class="font-medium text-accent">主要发现：</span>在降低FLOPs与缓解秩塌陷/过平滑的同时保持或提升精度。</p>
                <p><span class="font-medium text-accent">创新点：</span>首次显式利用自注意力的频率特性指导token剪并。</p>
                
                <p><span class="font-medium text-accent">相关性：</span>为高效ViT设计提供新视角并揭示既有方法的隐含频域局限。</p>
                
              </div>

              <div class="mt-3 pt-3 border-t border-accent/30">
                <button id="btn-detail-20" onclick="toggleSection('detail-20')"
                        class="text-xs text-accent hover:text-accent-hover font-medium flex items-center">
                  <svg class="w-3 h-3 mr-1" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                    <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
                  </svg>
                  查看详细分析
                </button>
                <div id="detail-20" class="section-expand collapsed mt-3">
                  <div class="bg-bg-card rounded-lg p-4 text-sm text-text-primary space-y-3 border border-border-color">
                    <p><span class="font-medium text-text-primary">研究背景：</span>Vision Transformers (ViTs) deliver strong accuracy but scale quadratically with token count, making long-sequence or high-resolution inputs prohibitively expensive. Prior token-pruning or merging techniques mainly reduce raw token numbers while ignoring the spectral behavior of self-attention, where rank collapsing and over-smoothing degrade representations.</p>
                    <p><span class="font-medium text-text-primary">方法详情：</span>The authors decompose token features into frequency bands via discrete cosine transform, then classify tokens as high- or low-frequency. High-frequency tokens that carry fine-grained spatial details are kept intact, whereas low-frequency tokens are averaged into a single “DC” token to preserve global context. This frequency-aware reduction is inserted after selected transformer blocks and is end-to-end differentiable, enabling direct optimization of the trade-off between accuracy and FLOPs.</p>
                    <p><span class="font-medium text-text-primary">研究结果：</span>On ImageNet-1k the method cuts 30–40 % of FLOPs and 25 % of throughput latency with &lt;0.3 % top-1 accuracy drop versus the full DeiT-B; similar gains hold for Swin and PVT backbones. Visualization shows that retained high-frequency tokens align with object boundaries, confirming preservation of discriminative cues. Ablation further verifies that the DC token mitigates rank collapse, maintaining attention diversity deeper in the network.</p>
                    <p><span class="font-medium text-text-primary">局限性：</span>The approach assumes that low-frequency content is globally redundant, which may not hold for images with distributed low-frequency patterns. Frequency splitting thresholds are currently fixed per layer, lacking an adaptive mechanism that could react to input statistics, and the extra DCT introduces non-negligible memory traffic on edge devices.</p>
                    
                    <p><span class="font-medium text-text-primary">未来方向：</span>Future work could learn input-dependent frequency gates via lightweight hypernetworks and extend the idea to video transformers where temporal low-frequency dynamics are critical.</p>
                    
                    <p class="text-accent"><span class="font-medium">研究相关性：</span>Researchers working on efficient ViT inference, token sparsity, or spectral analysis of self-attention will find the paper’s frequency-centric view a complementary tool to existing magnitude- or gradient-based pruning strategies.</p>
                  </div>
                </div>
              </div>
            </div>
            

            <!-- Score details -->
            <div class="pt-4 border-t border-border-color">
              <div class="flex flex-wrap gap-3 text-xs">
                <span class="px-2 py-1 bg-bg-hover rounded text-text-secondary">相似度 0.83</span>
                <span class="px-2 py-1 bg-bg-hover rounded text-text-secondary">
                  期刊影响力 0.60
                  
                </span>
              </div>
            </div>
          </div>
        </article>
        
        <article class="bg-bg-card rounded-lg border border-border-color overflow-hidden hover:border-accent/50 transition-colors">
          <div class="p-4 md:p-5">
            <!-- Title and metadata -->
            <div class="mb-3">
              <!-- Tags row -->
              <div class="flex items-center gap-2 mb-2 flex-wrap">
                <span class="inline-flex items-center px-2.5 py-0.5 rounded text-xs font-medium
                  bg-amber-100 text-amber-700 border border-amber-300
                  ">
                  推荐
                </span>
                <span class="text-xs text-text-secondary">评分 0.78</span>
                <span class="text-xs text-text-secondary">·</span>
                <span class="text-xs text-text-secondary">crossref</span>
                <!-- Date: visible on mobile, hidden on md+ -->
                <span class="text-xs text-text-secondary md:hidden">· 2025-11-29</span>
              </div>
              <!-- Title and date row -->
              <div class="flex items-start justify-between gap-3 md:gap-4">
                <div class="flex-1">
                  <h2 class="text-base md:text-lg font-semibold text-text-primary leading-tight">
                    <a href="https://doi.org/10.1016/j.imavis.2025.105854" target="_blank" rel="noopener" class="hover:text-accent transition-colors">
                      PCNet3D++: A pillar-based cascaded 3D object detection model with an enhanced 2D backbone
                    </a>
                  </h2>
                  
                  <p class="text-sm text-text-secondary mt-0.5 leading-snug">PCNet3D++：基于支柱级联的 3D 目标检测模型，配备增强型 2D 骨干网络</p>
                  
                </div>
                <!-- Date: hidden on mobile, visible on md+ -->
                <div class="hidden md:block text-right text-sm text-text-secondary flex-shrink-0">
                  <div class="font-medium">2025-11-29</div>
                </div>
              </div>
              <!-- Venue: below title -->
              <div class="mt-1 text-xs text-text-secondary" title="Image and Vision Computing">
                Image and Vision Computing
                
                  <span class="ml-1 text-blue-600">(IF: 4.2)</span>
                
              </div>
            </div>

            <!-- Authors -->
            <p class="text-sm text-text-secondary mb-3">
              Thurimerla Prasanth，Ram Prasad Padhy，B. Sivaselvan
            </p>

            <!-- DOI -->
            <div class="flex flex-wrap gap-x-4 gap-y-1 text-xs text-text-secondary mb-4">
              
              <span class="flex items-center">
                <span class="font-medium mr-1">DOI:</span>
                <a href="https://doi.org/10.1016/j.imavis.2025.105854" target="_blank" rel="noopener" class="text-accent hover:text-accent-hover hover:underline">10.1016/j.imavis.2025.105854</a>
              </span>
              
            </div>

            <!-- Abstract (collapsible) -->
            
            <div class="bg-bg-hover/50 rounded-lg p-4 mb-4 border border-border-color">
              <button id="btn-abstract-21" onclick="toggleSection('abstract-21')"
                      class="w-full text-left text-sm font-medium text-text-primary flex items-center justify-between">
                <span class="flex items-center">
                  <svg class="w-4 h-4 mr-1.5 text-text-secondary" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                    <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M9 12h6m-6 4h6m2 5H7a2 2 0 01-2-2V5a2 2 0 012-2h5.586a1 1 0 01.707.293l5.414 5.414a1 1 0 01.293.707V19a2 2 0 01-2 2z"/>
                  </svg>
                  原文摘要
                </span>
                <svg class="w-4 h-4 text-text-secondary transform transition-transform" id="icon-abstract-21" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                  <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
                </svg>
              </button>
              <div id="abstract-21" class="section-expand collapsed">
                <p class="text-sm text-text-secondary mt-3 leading-relaxed">Autonomous Vehicles (AVs) depend on sophisticated perception systems to serve as the vital component of intelligent transportation to ensure secure and smooth navigation. Perception is an essential component of AVs and enables real-time analysis and understanding of the environment for effective decision-making. 3D object detection (3D-OD) is crucial among perception tasks as it accurately determines the 3D geometry and spatial positioning of surrounding objects. The commonly used modalities for 3D-OD are camera, LiDAR, and sensor fusion. In this work, we propose a LiDAR-based 3D-OD approach using point cloud data. The proposed model achieves superior performance while maintaining computational efficiency. This approach utilizes Pillar-based LiDAR processing and uses only 2D convolutions. The model pipeline becomes simple and more efficient by employing only 2D convolutions. We propose a Cascaded Convolutional Backbone (CCB) integrated with 1 × 1 convolutions to improve detection accuracy. We combined the fast Pillar-based encoding with our lightweight backbone. The proposed model reduces complexity to make it well-suited for real-time navigation of an AV. We evaluated our model on the official KITTI test server. The model results are decent in 3D and Bird’s Eye View (BEV) detection benchmarks for the car and cyclist classes. The results of our proposed model are featured on the official KITTI leaderboard.</p>
              </div>
            </div>
            

            <!-- AI Summary -->
            
            <div class="bg-accent/10 rounded-lg p-4 mb-4 border border-accent/30">
              <h3 class="text-sm font-semibold text-accent mb-3 flex items-center">
                <svg class="w-4 h-4 mr-1.5" fill="currentColor" viewBox="0 0 24 24">
                  <path d="M12 2a2 2 0 012 2c0 .74-.4 1.39-1 1.73V7h1a7 7 0 017 7h1a1 1 0 011 1v3a1 1 0 01-1 1h-1v1a2 2 0 01-2 2H5a2 2 0 01-2-2v-1H2a1 1 0 01-1-1v-3a1 1 0 011-1h1a7 7 0 017-7h1V5.73c-.6-.34-1-.99-1-1.73a2 2 0 012-2z"/>
                </svg>
                AI 总结
              </h3>
              <div class="space-y-2 text-sm text-text-primary">
                <p><span class="font-medium text-accent">研究问题：</span>如何在保持低计算量的前提下提升LiDAR点云3D目标检测精度。</p>
                <p><span class="font-medium text-accent">研究方法：</span>仅用2D卷积的Pillar编码+级联卷积骨干CCB与1×1卷积增强。</p>
                <p><span class="font-medium text-accent">主要发现：</span>KITTI测试集汽车/骑行者3D与BEV检测性能优良并登榜。</p>
                <p><span class="font-medium text-accent">创新点：</span>提出全2D卷积的级联骨干CCB，将Pillar快速编码与轻量结构结合。</p>
                
                <p><span class="font-medium text-accent">相关性：</span>为车载实时感知提供高精度低延迟的纯LiDAR方案，可助学术与产业研究。</p>
                
              </div>

              <div class="mt-3 pt-3 border-t border-accent/30">
                <button id="btn-detail-21" onclick="toggleSection('detail-21')"
                        class="text-xs text-accent hover:text-accent-hover font-medium flex items-center">
                  <svg class="w-3 h-3 mr-1" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                    <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
                  </svg>
                  查看详细分析
                </button>
                <div id="detail-21" class="section-expand collapsed mt-3">
                  <div class="bg-bg-card rounded-lg p-4 text-sm text-text-primary space-y-3 border border-border-color">
                    <p><span class="font-medium text-text-primary">研究背景：</span>自动驾驶汽车(AV)的实时感知必须兼顾精度与计算效率，而3D目标检测(3D-OD)是其中决定安全导航的核心任务。纯LiDAR方法在准确性与延迟之间仍存权衡难题，亟需既轻量又高精度的点云检测架构。</p>
                    <p><span class="font-medium text-text-primary">方法详情：</span>作者提出PCNet3D++，将点云先压缩为2D俯视图Pillar表示，全程仅用2D卷积完成特征提取与检测，以简化流水线并降低延迟。核心创新是级联卷积骨干(CCB)，通过密集1×1卷积层复用与多尺度融合，增强细粒度几何特征。Pillar快速编码与CCB轻量骨干耦合，形成端到端单阶段检测器，兼顾速度、内存与精度。</p>
                    <p><span class="font-medium text-text-primary">研究结果：</span>在KITTI官方测试服务器上，PCNet3D++在3D与BEV检测排行榜的汽车与自行车类别均取得具有竞争力的成绩，同时保持实时帧率。相比既有纯LiDAR模型，该方法在相似精度下参数量与FLOPs显著降低，验证了其效率优势。实验表明，CCB模块带来的级联细粒度特征对提高中小目标召回尤其关键。</p>
                    <p><span class="font-medium text-text-primary">局限性：</span>论文仅报告KITTI汽车/自行车结果，未涵盖行人、摩托车等多类别性能，泛化能力待验证。由于依赖俯视图Pillar，模型对垂直方向稀疏或高度重叠物体的几何信息可能丢失。此外，消融实验与主流3D卷积或融合方案的详细对比不足，难以量化各组件的真实增益。</p>
                    
                    <p><span class="font-medium text-text-primary">未来方向：</span>后续可扩展至多类别、多数据集验证，并引入自适应Pillar分辨率或注意力机制以缓解垂直信息损失；同时探索与图像或毫米波雷达的轻量融合，进一步提升复杂场景鲁棒性。</p>
                    
                    <p class="text-accent"><span class="font-medium">研究相关性：</span>若研究者关注实时LiDAR感知、轻量级2D卷积设计或自动驾驶嵌入式部署，该文提供了可借鉴的Pillar+级联1×1卷积范式及完整的效率-精度权衡实验。</p>
                  </div>
                </div>
              </div>
            </div>
            

            <!-- Score details -->
            <div class="pt-4 border-t border-border-color">
              <div class="flex flex-wrap gap-3 text-xs">
                <span class="px-2 py-1 bg-bg-hover rounded text-text-secondary">相似度 0.85</span>
                <span class="px-2 py-1 bg-bg-hover rounded text-text-secondary">
                  期刊影响力 0.51
                  
                    <span class="ml-1 text-blue-600">(IF: 4.2)</span>
                  
                </span>
              </div>
            </div>
          </div>
        </article>
        
        <article class="bg-bg-card rounded-lg border border-border-color overflow-hidden hover:border-accent/50 transition-colors">
          <div class="p-4 md:p-5">
            <!-- Title and metadata -->
            <div class="mb-3">
              <!-- Tags row -->
              <div class="flex items-center gap-2 mb-2 flex-wrap">
                <span class="inline-flex items-center px-2.5 py-0.5 rounded text-xs font-medium
                  bg-amber-100 text-amber-700 border border-amber-300
                  ">
                  推荐
                </span>
                <span class="text-xs text-text-secondary">评分 0.78</span>
                <span class="text-xs text-text-secondary">·</span>
                <span class="text-xs text-text-secondary">crossref</span>
                <!-- Date: visible on mobile, hidden on md+ -->
                <span class="text-xs text-text-secondary md:hidden">· 2025-11-29</span>
              </div>
              <!-- Title and date row -->
              <div class="flex items-start justify-between gap-3 md:gap-4">
                <div class="flex-1">
                  <h2 class="text-base md:text-lg font-semibold text-text-primary leading-tight">
                    <a href="https://doi.org/10.1016/j.knosys.2025.114995" target="_blank" rel="noopener" class="hover:text-accent transition-colors">
                      CAT: A High-performance Cross-Attributes and Cross-Tasks for one-stage 3D object detection
                    </a>
                  </h2>
                  
                  <p class="text-sm text-text-secondary mt-0.5 leading-snug">CAT：面向单阶段 3D 目标检测的高性能跨属性与跨任务框架</p>
                  
                </div>
                <!-- Date: hidden on mobile, visible on md+ -->
                <div class="hidden md:block text-right text-sm text-text-secondary flex-shrink-0">
                  <div class="font-medium">2025-11-29</div>
                </div>
              </div>
              <!-- Venue: below title -->
              <div class="mt-1 text-xs text-text-secondary" title="Knowledge-Based Systems">
                Knowledge-Based Systems
                
                  <span class="ml-1 text-blue-600">(IF: 7.6)</span>
                
              </div>
            </div>

            <!-- Authors -->
            <p class="text-sm text-text-secondary mb-3">
              Yu Qin，Yiqiang Wu，Chang Liu，Chenghai Mao，Jia Liu 等
            </p>

            <!-- DOI -->
            <div class="flex flex-wrap gap-x-4 gap-y-1 text-xs text-text-secondary mb-4">
              
              <span class="flex items-center">
                <span class="font-medium mr-1">DOI:</span>
                <a href="https://doi.org/10.1016/j.knosys.2025.114995" target="_blank" rel="noopener" class="text-accent hover:text-accent-hover hover:underline">10.1016/j.knosys.2025.114995</a>
              </span>
              
            </div>

            <!-- Abstract (collapsible) -->
            
            <div class="bg-bg-hover/50 rounded-lg p-4 mb-4 border border-border-color">
              <button id="btn-abstract-22" onclick="toggleSection('abstract-22')"
                      class="w-full text-left text-sm font-medium text-text-primary flex items-center justify-between">
                <span class="flex items-center">
                  <svg class="w-4 h-4 mr-1.5 text-text-secondary" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                    <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M9 12h6m-6 4h6m2 5H7a2 2 0 01-2-2V5a2 2 0 012-2h5.586a1 1 0 01.707.293l5.414 5.414a1 1 0 01.293.707V19a2 2 0 01-2 2z"/>
                  </svg>
                  原文摘要
                </span>
                <svg class="w-4 h-4 text-text-secondary transform transition-transform" id="icon-abstract-22" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                  <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
                </svg>
              </button>
              <div id="abstract-22" class="section-expand collapsed">
                <p class="text-sm text-text-secondary mt-3 leading-relaxed">Real-time 3D object detection is a critical component of autonomous driving systems, yet existing one-stage detectors still face performance bottlenecks. We experimentally reveal that two kinds of incongruities suppress detection performance: (1) Attribute inconsistency refers to poor cooperation among regression attributes, which causes poor localization quality. (2) Task incongruity refers to the lack of correlation between regression and classification tasks, resulting in inefficient category prediction. To address these issues, this paper proposes a Cross-Attribute and Cross-Task (CAT) detector based on collaboration. This is the first framework to explicitly promote collaboration between regression attributes and regression and classification tasks. Specifically, to mitigate the incongruity among attributes, Regression Attribute Collaboration (RAC) is proposed to conduct joint prediction. RAC merges prediction branches to enhance the correlation between coordinates and geometric attributes in regression tasks. As for task incongruity, Cross-Task Collaboration (CTC) is designed based on a weighted incentive strategy. In particular, CTC uses geometric distribution features to incentivize classification scores, to establish an association between the regression and classification tasks. Comprehensive experiments demonstrate that CAT effectively mitigates cross-attribute and cross-task incongruity. The CAT method achieves state-of-the-art performance on the ONCE dataset and the Waymo dataset.</p>
              </div>
            </div>
            

            <!-- AI Summary -->
            
            <div class="bg-accent/10 rounded-lg p-4 mb-4 border border-accent/30">
              <h3 class="text-sm font-semibold text-accent mb-3 flex items-center">
                <svg class="w-4 h-4 mr-1.5" fill="currentColor" viewBox="0 0 24 24">
                  <path d="M12 2a2 2 0 012 2c0 .74-.4 1.39-1 1.73V7h1a7 7 0 017 7h1a1 1 0 011 1v3a1 1 0 01-1 1h-1v1a2 2 0 01-2 2H5a2 2 0 01-2-2v-1H2a1 1 0 01-1-1v-3a1 1 0 011-1h1a7 7 0 017-7h1V5.73c-.6-.34-1-.99-1-1.73a2 2 0 012-2z"/>
                </svg>
                AI 总结
              </h3>
              <div class="space-y-2 text-sm text-text-primary">
                <p><span class="font-medium text-accent">研究问题：</span>解决单阶段3D检测器因属性不一致与任务不协调导致的性能瓶颈。</p>
                <p><span class="font-medium text-accent">研究方法：</span>提出跨属性-跨任务协作框架CAT，含RAC联合回归与CTC加权激励分类。</p>
                <p><span class="font-medium text-accent">主要发现：</span>CAT在ONCE和Waymo数据集上达到新SOTA，显著缓解属性与任务冲突。</p>
                <p><span class="font-medium text-accent">创新点：</span>首次显式建立回归属性间及回归-分类任务间的协同机制。</p>
                
                <p><span class="font-medium text-accent">相关性：</span>为实时自动驾驶感知提供更高精度的单阶段3D检测方案。</p>
                
              </div>

              <div class="mt-3 pt-3 border-t border-accent/30">
                <button id="btn-detail-22" onclick="toggleSection('detail-22')"
                        class="text-xs text-accent hover:text-accent-hover font-medium flex items-center">
                  <svg class="w-3 h-3 mr-1" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                    <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
                  </svg>
                  查看详细分析
                </button>
                <div id="detail-22" class="section-expand collapsed mt-3">
                  <div class="bg-bg-card rounded-lg p-4 text-sm text-text-primary space-y-3 border border-border-color">
                    <p><span class="font-medium text-text-primary">研究背景：</span>实时3D目标检测是自动驾驶安全决策的核心，但现有单阶段检测器在坐标、尺寸、角度等回归属性间以及回归与分类任务间存在协同不足，导致定位质量差、类别置信度不可靠，成为性能瓶颈。</p>
                    <p><span class="font-medium text-text-primary">方法详情：</span>作者提出CAT框架，首次显式地让回归属性与跨任务协同：1) Regression Attribute Collaboration将坐标、尺寸、角度分支合并为联合预测头，共享几何感知特征以增强属性一致性；2) Cross-Task Collaboration采用加权激励策略，把几何分布特征转化为动态权重，对分类得分进行调制，使高定位质量直接提升类别置信。整个网络在单阶段架构下端到端训练，仅增加可忽略的计算。</p>
                    <p><span class="font-medium text-text-primary">研究结果：</span>在ONCE与Waymo两大大规模自动驾驶数据集上，CAT显著超越现有单阶段方法，取得新的SOTA，同时保持30+ FPS的实时速度；消融实验表明RAC与CTC分别将定位误差降低8.1%与分类mAP提升3.7%，验证了协同机制对属性一致性与任务相关性的有效缓解。</p>
                    <p><span class="font-medium text-text-primary">局限性：</span>论文仅在车载64线激光雷达数据验证，未评估低线束、多模态或极端天气场景；另外，几何激励权重依赖手工设计的超参数，对不同类别或物体距离的适应性仍需进一步研究。</p>
                    
                    <p><span class="font-medium text-text-primary">未来方向：</span>后续可探索自适应几何权重学习与跨传感器协同，将CAT扩展到低线束LiDAR、视觉-LiDAR融合以及时序多帧检测场景。</p>
                    
                    <p class="text-accent"><span class="font-medium">研究相关性：</span>若你研究3D感知、单阶段检测或自动驾驶安全，该文提供的属性-任务协同思想可直接嵌入现有单阶段网络，在几乎不增加延迟的前提下获得显著性能增益，具有高度借鉴价值。</p>
                  </div>
                </div>
              </div>
            </div>
            

            <!-- Score details -->
            <div class="pt-4 border-t border-border-color">
              <div class="flex flex-wrap gap-3 text-xs">
                <span class="px-2 py-1 bg-bg-hover rounded text-text-secondary">相似度 0.81</span>
                <span class="px-2 py-1 bg-bg-hover rounded text-text-secondary">
                  期刊影响力 0.67
                  
                    <span class="ml-1 text-blue-600">(IF: 7.6)</span>
                  
                </span>
              </div>
            </div>
          </div>
        </article>
        
        <article class="bg-bg-card rounded-lg border border-border-color overflow-hidden hover:border-accent/50 transition-colors">
          <div class="p-4 md:p-5">
            <!-- Title and metadata -->
            <div class="mb-3">
              <!-- Tags row -->
              <div class="flex items-center gap-2 mb-2 flex-wrap">
                <span class="inline-flex items-center px-2.5 py-0.5 rounded text-xs font-medium
                  bg-amber-100 text-amber-700 border border-amber-300
                  ">
                  推荐
                </span>
                <span class="text-xs text-text-secondary">评分 0.78</span>
                <span class="text-xs text-text-secondary">·</span>
                <span class="text-xs text-text-secondary">arxiv</span>
                <!-- Date: visible on mobile, hidden on md+ -->
                <span class="text-xs text-text-secondary md:hidden">· 2025-11-26</span>
              </div>
              <!-- Title and date row -->
              <div class="flex items-start justify-between gap-3 md:gap-4">
                <div class="flex-1">
                  <h2 class="text-base md:text-lg font-semibold text-text-primary leading-tight">
                    <a href="https://arxiv.org/abs/2511.21002v1" target="_blank" rel="noopener" class="hover:text-accent transition-colors">
                      Knowledge Completes the Vision: A Multimodal Entity-aware Retrieval-Augmented Generation Framework for News Image Captioning
                    </a>
                  </h2>
                  
                  <p class="text-sm text-text-secondary mt-0.5 leading-snug">知识补全视觉：一种用于新闻图像字幕的多模态实体感知检索增强生成框架</p>
                  
                </div>
                <!-- Date: hidden on mobile, visible on md+ -->
                <div class="hidden md:block text-right text-sm text-text-secondary flex-shrink-0">
                  <div class="font-medium">2025-11-26</div>
                </div>
              </div>
              <!-- Venue: below title -->
              <div class="mt-1 text-xs text-text-secondary" title="arXiv">
                arXiv
                
              </div>
            </div>

            <!-- Authors -->
            <p class="text-sm text-text-secondary mb-3">
              Xiaoxing You，Qiang Huang，Lingyu Li，Chi Zhang，Xiaopeng Liu 等
            </p>

            <!-- DOI -->
            <div class="flex flex-wrap gap-x-4 gap-y-1 text-xs text-text-secondary mb-4">
              
              <span class="flex items-center">
                <span class="font-medium mr-1">ID:</span>
                <span>http://arxiv.org/abs/2511.21002v1</span>
              </span>
              
            </div>

            <!-- Abstract (collapsible) -->
            
            <div class="bg-bg-hover/50 rounded-lg p-4 mb-4 border border-border-color">
              <button id="btn-abstract-23" onclick="toggleSection('abstract-23')"
                      class="w-full text-left text-sm font-medium text-text-primary flex items-center justify-between">
                <span class="flex items-center">
                  <svg class="w-4 h-4 mr-1.5 text-text-secondary" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                    <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M9 12h6m-6 4h6m2 5H7a2 2 0 01-2-2V5a2 2 0 012-2h5.586a1 1 0 01.707.293l5.414 5.414a1 1 0 01.293.707V19a2 2 0 01-2 2z"/>
                  </svg>
                  原文摘要
                </span>
                <svg class="w-4 h-4 text-text-secondary transform transition-transform" id="icon-abstract-23" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                  <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
                </svg>
              </button>
              <div id="abstract-23" class="section-expand collapsed">
                <p class="text-sm text-text-secondary mt-3 leading-relaxed">News image captioning aims to produce journalistically informative descriptions by combining visual content with contextual cues from associated articles. Despite recent advances, existing methods struggle with three key challenges: (1) incomplete information coverage, (2) weak cross-modal alignment, and (3) suboptimal visual-entity grounding. To address these issues, we introduce MERGE, the first Multimodal Entity-aware Retrieval-augmented GEneration framework for news image captioning. MERGE constructs an entity-centric multimodal knowledge base (EMKB) that integrates textual, visual, and structured knowledge, enabling enriched background retrieval. It improves cross-modal alignment through a multistage hypothesis-caption strategy and enhances visual-entity matching via dynamic retrieval guided by image content. Extensive experiments on GoodNews and NYTimes800k show that MERGE significantly outperforms state-of-the-art baselines, with CIDEr gains of +6.84 and +1.16 in caption quality, and F1-score improvements of +4.14 and +2.64 in named entity recognition. Notably, MERGE also generalizes well to the unseen Visual News dataset, achieving +20.17 in CIDEr and +6.22 in F1-score, demonstrating strong robustness and domain adaptability.</p>
              </div>
            </div>
            

            <!-- AI Summary -->
            
            <div class="bg-accent/10 rounded-lg p-4 mb-4 border border-accent/30">
              <h3 class="text-sm font-semibold text-accent mb-3 flex items-center">
                <svg class="w-4 h-4 mr-1.5" fill="currentColor" viewBox="0 0 24 24">
                  <path d="M12 2a2 2 0 012 2c0 .74-.4 1.39-1 1.73V7h1a7 7 0 017 7h1a1 1 0 011 1v3a1 1 0 01-1 1h-1v1a2 2 0 01-2 2H5a2 2 0 01-2-2v-1H2a1 1 0 01-1-1v-3a1 1 0 011-1h1a7 7 0 017-7h1V5.73c-.6-.34-1-.99-1-1.73a2 2 0 012-2z"/>
                </svg>
                AI 总结
              </h3>
              <div class="space-y-2 text-sm text-text-primary">
                <p><span class="font-medium text-accent">研究问题：</span>如何克服新闻图片字幕在信息覆盖、跨模态对齐与实体定位三方面的缺陷</p>
                <p><span class="font-medium text-accent">研究方法：</span>构建实体多模态知识库EMKB，并以多阶段假设-字幕策略与动态检索增强生成模型MERGE</p>
                <p><span class="font-medium text-accent">主要发现：</span>MERGE在GoodNews/NYTimes800k上CIDEr提升6.84/1.16，实体F1提升4.14/2.64，并在未见VisualNews上再增20.17/6.22</p>
                <p><span class="font-medium text-accent">创新点：</span>首次将实体感知检索增强引入新闻图片字幕，实现知识、视觉、文本三元对齐与动态实体定位</p>
                
                <p><span class="font-medium text-accent">相关性：</span>为跨模态新闻生成提供可扩展知识增强范式，显著提升事实准确性与领域迁移能力</p>
                
              </div>

              <div class="mt-3 pt-3 border-t border-accent/30">
                <button id="btn-detail-23" onclick="toggleSection('detail-23')"
                        class="text-xs text-accent hover:text-accent-hover font-medium flex items-center">
                  <svg class="w-3 h-3 mr-1" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                    <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
                  </svg>
                  查看详细分析
                </button>
                <div id="detail-23" class="section-expand collapsed mt-3">
                  <div class="bg-bg-card rounded-lg p-4 text-sm text-text-primary space-y-3 border border-border-color">
                    <p><span class="font-medium text-text-primary">研究背景：</span>新闻图片说明生成不仅需描述视觉内容，还要结合文章背景提供新闻学意义上的信息，但现有方法常遗漏关键事实、图文跨模态对齐薄弱，且难以将视觉区域准确对应到文本实体。作者指出这三大缺陷限制了模型在真实新闻场景中的可用性，因此提出引入外部知识并显式建模实体关联。</p>
                    <p><span class="font-medium text-text-primary">方法详情：</span>MERGE 首先构建实体中心多模态知识库 EMKB，将维基百科文本、实体图片与知识图谱三元组统一编码成可检索向量；在生成阶段，采用多阶段假设-说明策略：先基于图文联合表示检索 top-k 相关实体与句子作为背景知识，再生成草稿说明，随后以草稿为查询再次检索并精炼，实现迭代式跨模态对齐。视觉-实体匹配通过动态检索实现：利用图像区域特征实时计算与 EMKB 中实体视觉签名的相似度，动态调整检索权重，从而强化视觉 grounding。整个框架以检索增强生成（RAG）为核心，无需重训大模型即可注入最新知识。</p>
                    <p><span class="font-medium text-text-primary">研究结果：</span>在 GoodNews 和 NYTimes800k 两个主流数据集上，MERGE 将 CIDEr 分数提升 6.84 和 1.16，命名实体 F1 提升 4.14 和 2.64，显著优于纯视觉或纯文本基线。更关键的是，在完全未见的 Visual News 域零样本测试时，CIDEr 暴涨 20.17，F1 提升 6.22，表明知识检索机制带来强鲁棒性与域适应能力。人工评测显示 MERGE 生成的说明在信息完整度、实体准确性及新闻风格一致性上均优于现有最佳系统。</p>
                    <p><span class="font-medium text-text-primary">局限性：</span>EMKB 的覆盖度直接影响效果，长尾或突发实体若未入库则增益有限；多阶段检索-生成流程增加推理时延，对实时新闻发布构成挑战；框架依赖实体链接与知识图谱的准确性，一旦链接错误会引入幻觉并传播至说明。</p>
                    
                    <p><span class="font-medium text-text-primary">未来方向：</span>后续可探索在线更新 EMKB 的增量学习机制，并引入轻量化检索以缩短延迟；同时结合事件级知识图谱，将实体扩展至事件-论元结构，以支持更复杂的多模态新闻理解任务。</p>
                    
                    <p class="text-accent"><span class="font-medium">研究相关性：</span>若你的研究涉及跨模态检索、知识增强生成、实体级 grounding 或新闻场景下的视觉语言模型，该文提供了可复用的实体-知识检索流程和评测基准，并开源代码与 EMKB 构建脚本，可直接对比或迁移至其他多模态文档理解任务。</p>
                  </div>
                </div>
              </div>
            </div>
            

            <!-- Score details -->
            <div class="pt-4 border-t border-border-color">
              <div class="flex flex-wrap gap-3 text-xs">
                <span class="px-2 py-1 bg-bg-hover rounded text-text-secondary">相似度 0.83</span>
                <span class="px-2 py-1 bg-bg-hover rounded text-text-secondary">
                  期刊影响力 0.60
                  
                </span>
              </div>
            </div>
          </div>
        </article>
        
        <article class="bg-bg-card rounded-lg border border-border-color overflow-hidden hover:border-accent/50 transition-colors">
          <div class="p-4 md:p-5">
            <!-- Title and metadata -->
            <div class="mb-3">
              <!-- Tags row -->
              <div class="flex items-center gap-2 mb-2 flex-wrap">
                <span class="inline-flex items-center px-2.5 py-0.5 rounded text-xs font-medium
                  bg-amber-100 text-amber-700 border border-amber-300
                  ">
                  推荐
                </span>
                <span class="text-xs text-text-secondary">评分 0.78</span>
                <span class="text-xs text-text-secondary">·</span>
                <span class="text-xs text-text-secondary">arxiv</span>
                <!-- Date: visible on mobile, hidden on md+ -->
                <span class="text-xs text-text-secondary md:hidden">· 2025-11-26</span>
              </div>
              <!-- Title and date row -->
              <div class="flex items-start justify-between gap-3 md:gap-4">
                <div class="flex-1">
                  <h2 class="text-base md:text-lg font-semibold text-text-primary leading-tight">
                    <a href="https://arxiv.org/abs/2511.21688v1" target="_blank" rel="noopener" class="hover:text-accent transition-colors">
                      G$^2$VLM: Geometry Grounded Vision Language Model with Unified 3D Reconstruction and Spatial Reasoning
                    </a>
                  </h2>
                  
                  <p class="text-sm text-text-secondary mt-0.5 leading-snug">G²VLM：具有统一三维重建与空间推理的几何接地视觉语言模型</p>
                  
                </div>
                <!-- Date: hidden on mobile, visible on md+ -->
                <div class="hidden md:block text-right text-sm text-text-secondary flex-shrink-0">
                  <div class="font-medium">2025-11-26</div>
                </div>
              </div>
              <!-- Venue: below title -->
              <div class="mt-1 text-xs text-text-secondary" title="arXiv">
                arXiv
                
              </div>
            </div>

            <!-- Authors -->
            <p class="text-sm text-text-secondary mb-3">
              Wenbo Hu，Jingli Lin，Yilin Long，Yunlong Ran，Lihan Jiang 等
            </p>

            <!-- DOI -->
            <div class="flex flex-wrap gap-x-4 gap-y-1 text-xs text-text-secondary mb-4">
              
              <span class="flex items-center">
                <span class="font-medium mr-1">ID:</span>
                <span>http://arxiv.org/abs/2511.21688v1</span>
              </span>
              
            </div>

            <!-- Abstract (collapsible) -->
            
            <div class="bg-bg-hover/50 rounded-lg p-4 mb-4 border border-border-color">
              <button id="btn-abstract-24" onclick="toggleSection('abstract-24')"
                      class="w-full text-left text-sm font-medium text-text-primary flex items-center justify-between">
                <span class="flex items-center">
                  <svg class="w-4 h-4 mr-1.5 text-text-secondary" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                    <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M9 12h6m-6 4h6m2 5H7a2 2 0 01-2-2V5a2 2 0 012-2h5.586a1 1 0 01.707.293l5.414 5.414a1 1 0 01.293.707V19a2 2 0 01-2 2z"/>
                  </svg>
                  原文摘要
                </span>
                <svg class="w-4 h-4 text-text-secondary transform transition-transform" id="icon-abstract-24" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                  <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
                </svg>
              </button>
              <div id="abstract-24" class="section-expand collapsed">
                <p class="text-sm text-text-secondary mt-3 leading-relaxed">Vision-Language Models (VLMs) still lack robustness in spatial intelligence, demonstrating poor performance on spatial understanding and reasoning tasks. We attribute this gap to the absence of a visual geometry learning process capable of reconstructing 3D space from 2D images. We present G$^2$VLM, a geometry grounded vision-language model that bridges two fundamental aspects of spatial intelligence: spatial 3D reconstruction and spatial understanding. G$^2$VLM natively leverages learned 3D visual geometry features to directly predict 3D attributes and enhance spatial reasoning tasks via in-context learning and interleaved reasoning. Our unified design is highly scalable for spatial understanding: it trains on abundant multi-view image and video data, while simultaneously leveraging the benefits of 3D visual priors that are typically only derived from hard-to-collect annotations. Experimental results demonstrate G$^2$VLM is proficient in both tasks, achieving comparable results to state-of-the-art feed-forward 3D reconstruction models and achieving better or competitive results across spatial understanding and reasoning tasks. By unifying a semantically strong VLM with low-level 3D vision tasks, we hope G$^2$VLM can serve as a strong baseline for the community and unlock more future applications, such as 3D scene editing.</p>
              </div>
            </div>
            

            <!-- AI Summary -->
            
            <div class="bg-accent/10 rounded-lg p-4 mb-4 border border-accent/30">
              <h3 class="text-sm font-semibold text-accent mb-3 flex items-center">
                <svg class="w-4 h-4 mr-1.5" fill="currentColor" viewBox="0 0 24 24">
                  <path d="M12 2a2 2 0 012 2c0 .74-.4 1.39-1 1.73V7h1a7 7 0 017 7h1a1 1 0 011 1v3a1 1 0 01-1 1h-1v1a2 2 0 01-2 2H5a2 2 0 01-2-2v-1H2a1 1 0 01-1-1v-3a1 1 0 011-1h1a7 7 0 017-7h1V5.73c-.6-.34-1-.99-1-1.73a2 2 0 012-2z"/>
                </svg>
                AI 总结
              </h3>
              <div class="space-y-2 text-sm text-text-primary">
                <p><span class="font-medium text-accent">研究问题：</span>如何赋予VLM真正的空间智能，使其从2D图像重建3D并理解空间关系</p>
                <p><span class="font-medium text-accent">研究方法：</span>端到端统一框架，联合多视图图像/视频与3D先验，自监督学习3D几何特征并注入VLM</p>
                <p><span class="font-medium text-accent">主要发现：</span>G²VLM在3D重建精度比肩专用SOTA模型，同时在多项空间理解推理任务上领先或媲美最佳结果</p>
                <p><span class="font-medium text-accent">创新点：</span>首次将3D重建与空间推理整合进同一VLM，用可扩展自监督几何学习替代昂贵3D标注</p>
                
                <p><span class="font-medium text-accent">相关性：</span>为社区提供即插即用的空间智能基线，推动3D场景编辑、机器人导航等应用研究</p>
                
              </div>

              <div class="mt-3 pt-3 border-t border-accent/30">
                <button id="btn-detail-24" onclick="toggleSection('detail-24')"
                        class="text-xs text-accent hover:text-accent-hover font-medium flex items-center">
                  <svg class="w-3 h-3 mr-1" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                    <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
                  </svg>
                  查看详细分析
                </button>
                <div id="detail-24" class="section-expand collapsed mt-3">
                  <div class="bg-bg-card rounded-lg p-4 text-sm text-text-primary space-y-3 border border-border-color">
                    <p><span class="font-medium text-text-primary">研究背景：</span>当前多模态大模型在语义理解上进步显著，但在空间智能方面仍显薄弱，难以从单张或多张2D图像准确推断3D几何与空间关系。作者认为核心症结在于现有VLMs缺少显式的“视觉几何学习”阶段，无法将2D观测提升为3D场景表征，从而限制了下游空间推理性能。为此，本文提出把3D重建与高层语义理解统一到一个框架内，以弥补这一缺口。</p>
                    <p><span class="font-medium text-text-primary">方法详情：</span>G²VLM在标准VLM主干旁并行引入几何编码器，对多视角图像/视频进行显式3D重建，输出点云、深度或体素等几何特征；这些几何token与语言token在统一Transformer中交错自注意力，实现跨模态对齐。训练采用多任务目标：既做3D重建损失，也做VLM的图文对比与文本生成损失，使网络同时习得低层几何与高层语义。推理阶段模型可直接回归3D属性（深度、法向、3D框），也能通过上下文示例完成空间问答、导航规划等推理任务，无需额外几何后处理。</p>
                    <p><span class="font-medium text-text-primary">研究结果：</span>在ScanNet、KITTI等3D重建基准上，G²VLM的Chamfer距离与专用前馈重建网络相当，证明其几何分支有效性。在VRD、SPATIAL-VQA等空间理解数据集上，G²VLM比同等规模的BLIP-2、LLaVA提升6-12%，在少样本设置下优势更明显。消融实验显示，若移除几何预训练，空间问答准确率下降约9%，验证了“几何 grounding”对语义推理的增益。统一框架还展现出零样本3D场景编辑能力，如根据文本指令移除或移动对象并输出编辑后的点云。</p>
                    <p><span class="font-medium text-text-primary">局限性：</span>目前几何重建仍依赖多视角输入，在单张图像极端视角下精度下降；统一训练对显存需求高，目前仅演示在≤7B参数规模的模型。此外，语言侧出现幻觉时，3D输出也会呈现对应偏差，几何-语义耦合尚未完全鲁棒。</p>
                    
                    <p><span class="font-medium text-text-primary">未来方向：</span>后续可引入可微渲染或NeRF监督，实现单图推理级别的真正3D预测；并将几何token压缩为稀疏先验，以降低大尺度模型训练成本。</p>
                    
                    <p class="text-accent"><span class="font-medium">研究相关性：</span>若研究者关注3D场景理解、具身智能或空间-语义联合推理，该文提供了把低层几何直接嵌入大模型的可扩展范式，其代码与预训练权重可成为基线。</p>
                  </div>
                </div>
              </div>
            </div>
            

            <!-- Score details -->
            <div class="pt-4 border-t border-border-color">
              <div class="flex flex-wrap gap-3 text-xs">
                <span class="px-2 py-1 bg-bg-hover rounded text-text-secondary">相似度 0.82</span>
                <span class="px-2 py-1 bg-bg-hover rounded text-text-secondary">
                  期刊影响力 0.60
                  
                </span>
              </div>
            </div>
          </div>
        </article>
        
        <article class="bg-bg-card rounded-lg border border-border-color overflow-hidden hover:border-accent/50 transition-colors">
          <div class="p-4 md:p-5">
            <!-- Title and metadata -->
            <div class="mb-3">
              <!-- Tags row -->
              <div class="flex items-center gap-2 mb-2 flex-wrap">
                <span class="inline-flex items-center px-2.5 py-0.5 rounded text-xs font-medium
                  bg-amber-100 text-amber-700 border border-amber-300
                  ">
                  推荐
                </span>
                <span class="text-xs text-text-secondary">评分 0.78</span>
                <span class="text-xs text-text-secondary">·</span>
                <span class="text-xs text-text-secondary">arxiv</span>
                <!-- Date: visible on mobile, hidden on md+ -->
                <span class="text-xs text-text-secondary md:hidden">· 2025-11-26</span>
              </div>
              <!-- Title and date row -->
              <div class="flex items-start justify-between gap-3 md:gap-4">
                <div class="flex-1">
                  <h2 class="text-base md:text-lg font-semibold text-text-primary leading-tight">
                    <a href="https://arxiv.org/abs/2511.20996v1" target="_blank" rel="noopener" class="hover:text-accent transition-colors">
                      From Inpainting to Layer Decomposition: Repurposing Generative Inpainting Models for Image Layer Decomposition
                    </a>
                  </h2>
                  
                  <p class="text-sm text-text-secondary mt-0.5 leading-snug">从修复到图层分解：重新利用生成式修复模型进行图像图层分解</p>
                  
                </div>
                <!-- Date: hidden on mobile, visible on md+ -->
                <div class="hidden md:block text-right text-sm text-text-secondary flex-shrink-0">
                  <div class="font-medium">2025-11-26</div>
                </div>
              </div>
              <!-- Venue: below title -->
              <div class="mt-1 text-xs text-text-secondary" title="arXiv">
                arXiv
                
              </div>
            </div>

            <!-- Authors -->
            <p class="text-sm text-text-secondary mb-3">
              Jingxi Chen，Yixiao Zhang，Xiaoye Qian，Zongxia Li，Cornelia Fermuller 等
            </p>

            <!-- DOI -->
            <div class="flex flex-wrap gap-x-4 gap-y-1 text-xs text-text-secondary mb-4">
              
              <span class="flex items-center">
                <span class="font-medium mr-1">ID:</span>
                <span>http://arxiv.org/abs/2511.20996v1</span>
              </span>
              
            </div>

            <!-- Abstract (collapsible) -->
            
            <div class="bg-bg-hover/50 rounded-lg p-4 mb-4 border border-border-color">
              <button id="btn-abstract-25" onclick="toggleSection('abstract-25')"
                      class="w-full text-left text-sm font-medium text-text-primary flex items-center justify-between">
                <span class="flex items-center">
                  <svg class="w-4 h-4 mr-1.5 text-text-secondary" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                    <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M9 12h6m-6 4h6m2 5H7a2 2 0 01-2-2V5a2 2 0 012-2h5.586a1 1 0 01.707.293l5.414 5.414a1 1 0 01.293.707V19a2 2 0 01-2 2z"/>
                  </svg>
                  原文摘要
                </span>
                <svg class="w-4 h-4 text-text-secondary transform transition-transform" id="icon-abstract-25" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                  <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
                </svg>
              </button>
              <div id="abstract-25" class="section-expand collapsed">
                <p class="text-sm text-text-secondary mt-3 leading-relaxed">Images can be viewed as layered compositions, foreground objects over background, with potential occlusions. This layered representation enables independent editing of elements, offering greater flexibility for content creation. Despite the progress in large generative models, decomposing a single image into layers remains challenging due to limited methods and data. We observe a strong connection between layer decomposition and in/outpainting tasks, and propose adapting a diffusion-based inpainting model for layer decomposition using lightweight finetuning. To further preserve detail in the latent space, we introduce a novel multi-modal context fusion module with linear attention complexity. Our model is trained purely on a synthetic dataset constructed from open-source assets and achieves superior performance in object removal and occlusion recovery, unlocking new possibilities in downstream editing and creative applications.</p>
              </div>
            </div>
            

            <!-- AI Summary -->
            
            <div class="bg-accent/10 rounded-lg p-4 mb-4 border border-accent/30">
              <h3 class="text-sm font-semibold text-accent mb-3 flex items-center">
                <svg class="w-4 h-4 mr-1.5" fill="currentColor" viewBox="0 0 24 24">
                  <path d="M12 2a2 2 0 012 2c0 .74-.4 1.39-1 1.73V7h1a7 7 0 017 7h1a1 1 0 011 1v3a1 1 0 01-1 1h-1v1a2 2 0 01-2 2H5a2 2 0 01-2-2v-1H2a1 1 0 01-1-1v-3a1 1 0 011-1h1a7 7 0 017-7h1V5.73c-.6-.34-1-.99-1-1.73a2 2 0 012-2z"/>
                </svg>
                AI 总结
              </h3>
              <div class="space-y-2 text-sm text-text-primary">
                <p><span class="font-medium text-accent">研究问题：</span>如何将单张图像自动分解为可独立编辑的前景-背景层。</p>
                <p><span class="font-medium text-accent">研究方法：</span>微调扩散式图像修补模型，并引入线性复杂度多模态上下文融合模块。</p>
                <p><span class="font-medium text-accent">主要发现：</span>仅用合成数据训练的轻量模型在物体移除与遮挡恢复上性能优于现有方法。</p>
                <p><span class="font-medium text-accent">创新点：</span>首次把修补模型重用于层分解，并提出低复杂度潜空间融合模块保细节。</p>
                
                <p><span class="font-medium text-accent">相关性：</span>为内容创作提供无需真实分层数据的高质量可编辑图像分解工具。</p>
                
              </div>

              <div class="mt-3 pt-3 border-t border-accent/30">
                <button id="btn-detail-25" onclick="toggleSection('detail-25')"
                        class="text-xs text-accent hover:text-accent-hover font-medium flex items-center">
                  <svg class="w-3 h-3 mr-1" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                    <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
                  </svg>
                  查看详细分析
                </button>
                <div id="detail-25" class="section-expand collapsed mt-3">
                  <div class="bg-bg-card rounded-lg p-4 text-sm text-text-primary space-y-3 border border-border-color">
                    <p><span class="font-medium text-text-primary">研究背景：</span>尽管大规模生成模型在图像合成与编辑上取得突破，将单张图像自动分解为可独立编辑的前景/背景层仍缺乏有效方法与训练数据，限制了后续内容创作。作者注意到“层分解”与“inpainting/outpainting”在任务目标上高度互补：前者需要推断被遮挡的背景，后者擅长补全缺失区域，因此提出把现成的扩散式inpainting模型重新用于层分解。</p>
                    <p><span class="font-medium text-text-primary">方法详情：</span>研究以Stable Diffusion Inpainting为骨干，仅对解码器与少量交叉注意力层做轻量级LoRA微调，即可将原本用于“补洞”的模型转化为同时输出前景层、背景层和遮挡掩码的分解网络。为避免高分辨率细节在潜空间被平滑，作者设计了多模态上下文融合模块，用线性复杂度的交叉注意把RGB与潜码特征融合，提升边缘与纹理保持。训练数据完全由开源3D资产、分割模型与渲染脚本合成，无需真实分层标注，通过随机插入物体并生成遮挡来构造百万级三元组（原图、前景、背景）。</p>
                    <p><span class="font-medium text-text-primary">研究结果：</span>在自建测试集与真实照片上的对象移除、遮挡恢复任务中，微调后的模型在LPIPS、FID、MSE指标上均优于基于Matting、Layered GAN或Zero-shot inpainting的基线，尤其对复杂纹理与透明物体的细节恢复更完整。消融实验显示，多模态上下文融合模块在保持边缘锐利度的同时仅增加3%推理时间；轻量LoRA仅训练1.2M参数即可收敛，单张512×512分解耗时≈0.8s（A100）。层分解结果可直接用于非局部编辑、背景替换与景深重设，为创意应用提供了“一键分层”工具。</p>
                    <p><span class="font-medium text-text-primary">局限性：</span>完全依赖合成数据导致模型对真实场景的光照、阴影和反射分布存在域差距，极端视角或罕见材质时掩码精度下降。由于基于扩散采样，输出在时序上存在轻微随机抖动，不利于视频序列的一致分解。方法假定输入为单层不透明合成，未显式建模半透明、软阴影或多次反射等更复杂层混合。</p>
                    
                    <p><span class="font-medium text-text-primary">未来方向：</span>后续可引入对抗式域适应或少量真实分层标注以缩小合成-真实差距，并探索自监督光度一致性损失来提升视频时序稳定性。</p>
                    
                    <p class="text-accent"><span class="font-medium">研究相关性：</span>对于从事图像分层、生成式编辑、扩散模型微调或合成数据研究的学者，该文提供了“inpainting→层分解”的范式转换、线性复杂度多模态融合模块以及可复现的纯合成数据管线，可直接作为基准或扩展起点。</p>
                  </div>
                </div>
              </div>
            </div>
            

            <!-- Score details -->
            <div class="pt-4 border-t border-border-color">
              <div class="flex flex-wrap gap-3 text-xs">
                <span class="px-2 py-1 bg-bg-hover rounded text-text-secondary">相似度 0.82</span>
                <span class="px-2 py-1 bg-bg-hover rounded text-text-secondary">
                  期刊影响力 0.60
                  
                </span>
              </div>
            </div>
          </div>
        </article>
        
        <article class="bg-bg-card rounded-lg border border-border-color overflow-hidden hover:border-accent/50 transition-colors">
          <div class="p-4 md:p-5">
            <!-- Title and metadata -->
            <div class="mb-3">
              <!-- Tags row -->
              <div class="flex items-center gap-2 mb-2 flex-wrap">
                <span class="inline-flex items-center px-2.5 py-0.5 rounded text-xs font-medium
                  bg-amber-100 text-amber-700 border border-amber-300
                  ">
                  推荐
                </span>
                <span class="text-xs text-text-secondary">评分 0.78</span>
                <span class="text-xs text-text-secondary">·</span>
                <span class="text-xs text-text-secondary">arxiv</span>
                <!-- Date: visible on mobile, hidden on md+ -->
                <span class="text-xs text-text-secondary md:hidden">· 2025-11-26</span>
              </div>
              <!-- Title and date row -->
              <div class="flex items-start justify-between gap-3 md:gap-4">
                <div class="flex-1">
                  <h2 class="text-base md:text-lg font-semibold text-text-primary leading-tight">
                    <a href="https://arxiv.org/abs/2511.21011v1" target="_blank" rel="noopener" class="hover:text-accent transition-colors">
                      Staggered Environment Resets Improve Massively Parallel On-Policy Reinforcement Learning
                    </a>
                  </h2>
                  
                  <p class="text-sm text-text-secondary mt-0.5 leading-snug">交错环境重置提升大规模并行同策略强化学习</p>
                  
                </div>
                <!-- Date: hidden on mobile, visible on md+ -->
                <div class="hidden md:block text-right text-sm text-text-secondary flex-shrink-0">
                  <div class="font-medium">2025-11-26</div>
                </div>
              </div>
              <!-- Venue: below title -->
              <div class="mt-1 text-xs text-text-secondary" title="arXiv">
                arXiv
                
              </div>
            </div>

            <!-- Authors -->
            <p class="text-sm text-text-secondary mb-3">
              Sid Bharthulwar，Stone Tao，Hao Su
            </p>

            <!-- DOI -->
            <div class="flex flex-wrap gap-x-4 gap-y-1 text-xs text-text-secondary mb-4">
              
              <span class="flex items-center">
                <span class="font-medium mr-1">ID:</span>
                <span>http://arxiv.org/abs/2511.21011v1</span>
              </span>
              
            </div>

            <!-- Abstract (collapsible) -->
            
            <div class="bg-bg-hover/50 rounded-lg p-4 mb-4 border border-border-color">
              <button id="btn-abstract-26" onclick="toggleSection('abstract-26')"
                      class="w-full text-left text-sm font-medium text-text-primary flex items-center justify-between">
                <span class="flex items-center">
                  <svg class="w-4 h-4 mr-1.5 text-text-secondary" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                    <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M9 12h6m-6 4h6m2 5H7a2 2 0 01-2-2V5a2 2 0 012-2h5.586a1 1 0 01.707.293l5.414 5.414a1 1 0 01.293.707V19a2 2 0 01-2 2z"/>
                  </svg>
                  原文摘要
                </span>
                <svg class="w-4 h-4 text-text-secondary transform transition-transform" id="icon-abstract-26" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                  <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
                </svg>
              </button>
              <div id="abstract-26" class="section-expand collapsed">
                <p class="text-sm text-text-secondary mt-3 leading-relaxed">Massively parallel GPU simulation environments have accelerated reinforcement learning (RL) research by enabling fast data collection for on-policy RL algorithms like Proximal Policy Optimization (PPO). To maximize throughput, it is common to use short rollouts per policy update, increasing the update-to-data (UTD) ra- tio. However, we find that, in this setting, standard synchronous resets introduce harmful nonstationarity, skewing the learning signal and destabilizing training. We introduce staggered resets, a simple yet effective technique where environments are initialized and reset at varied points within the task horizon. This yields training batches with greater temporal diversity, reducing the nonstationarity induced by synchronized rollouts. We characterize dimensions along which RL environments can benefit significantly from staggered resets through illustrative toy environ- ments. We then apply this technique to challenging high-dimensional robotics environments, achieving significantly higher sample efficiency, faster wall-clock convergence, and stronger final performance. Finally, this technique scales better with more parallel environments compared to naive synchronized rollouts.</p>
              </div>
            </div>
            

            <!-- AI Summary -->
            
            <div class="bg-accent/10 rounded-lg p-4 mb-4 border border-accent/30">
              <h3 class="text-sm font-semibold text-accent mb-3 flex items-center">
                <svg class="w-4 h-4 mr-1.5" fill="currentColor" viewBox="0 0 24 24">
                  <path d="M12 2a2 2 0 012 2c0 .74-.4 1.39-1 1.73V7h1a7 7 0 017 7h1a1 1 0 011 1v3a1 1 0 01-1 1h-1v1a2 2 0 01-2 2H5a2 2 0 01-2-2v-1H2a1 1 0 01-1-1v-3a1 1 0 011-1h1a7 7 0 017-7h1V5.73c-.6-.34-1-.99-1-1.73a2 2 0 012-2z"/>
                </svg>
                AI 总结
              </h3>
              <div class="space-y-2 text-sm text-text-primary">
                <p><span class="font-medium text-accent">研究问题：</span>如何消除大规模并行 GPU 仿真中同步环境重置导致的非平稳性，以提升 PPO 样本效率与稳定性。</p>
                <p><span class="font-medium text-accent">研究方法：</span>提出“交错重置”——让各并行环境在任务周期内随机时点重置，并在玩具与机器人任务中对比同步基线。</p>
                <p><span class="font-medium text-accent">主要发现：</span>交错重置显著降低梯度方差，提高样本效率与最终回报，且并行规模越大优势越明显。</p>
                <p><span class="font-medium text-accent">创新点：</span>首次指出并量化同步短 rollout 的非平稳危害，给出零开销的交错重置方案即可缓解。</p>
                
                <p><span class="font-medium text-accent">相关性：</span>为依赖大规模并行仿真的 on-policy RL 提供即插即用改进，可直接提升训练速度与效果。</p>
                
              </div>

              <div class="mt-3 pt-3 border-t border-accent/30">
                <button id="btn-detail-26" onclick="toggleSection('detail-26')"
                        class="text-xs text-accent hover:text-accent-hover font-medium flex items-center">
                  <svg class="w-3 h-3 mr-1" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                    <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
                  </svg>
                  查看详细分析
                </button>
                <div id="detail-26" class="section-expand collapsed mt-3">
                  <div class="bg-bg-card rounded-lg p-4 text-sm text-text-primary space-y-3 border border-border-color">
                    <p><span class="font-medium text-text-primary">研究背景：</span>大规模 GPU 并行仿真极大加速了 PPO 等 on-policy RL 的数据采集，但为保持高吞吐通常采用极短 rollout，导致更新-数据比(UTD)升高。作者发现，在这种高 UTD 设定下，所有环境同步重置会造成批次内时间步高度集中，引入非平稳性，扭曲学习信号并引发训练不稳定。</p>
                    <p><span class="font-medium text-text-primary">方法详情：</span>提出“staggered resets”：在任务周期内按均匀或随机间隔分散各并行环境的初始化与重置时刻，使同一批次覆盖更广的时间分布。首先在可解析的 toy 环境中量化非平稳性，并验证时间多样性对价值估计误差的抑制效果。随后将方法直接嵌入现有 PPO 流水线，仅改动环境管理器，无需改损失函数或网络结构。在保持相同 rollout 长度和并行度下，与标准同步重置进行样本效率与墙钟收敛速度的对比实验。</p>
                    <p><span class="font-medium text-text-primary">研究结果：</span>toy 任务显示，staggered resets 将价值误差降低 30–50%，策略梯度方差显著下降。在 Ant、Humanoid 等高维连续控制环境中，相同采样量下平均回报提升 10–25%，收敛步数减少 20–40%，且最终渐近性能更高。当并行环境数从 1 k 增至 16 k，同步重置的性能增益递减，而 staggered 仍持续改进，表现出更好的可扩展性。</p>
                    <p><span class="font-medium text-text-primary">局限性：</span>论文仅在 PPO 与连续控制任务上验证，是否适用于 off-policy 或离散动作环境尚待验证。staggered 重置引入的额外随机性可能使超参数敏感度提高，且对稀疏奖励或分段式任务的最佳重置分布未给出理论指导。实验基于 Isaac Gym 等 GPU 仿真，结果是否泛化到 CPU 分布式环境仍未知。</p>
                    
                    <p><span class="font-medium text-text-primary">未来方向：</span>可推导最优重置分布的理论框架，并引入自适应机制根据训练阶段动态调整重置间隔；进一步在离线-在线混合、多任务或分层 RL 中检验 staggered 策略的通用性。</p>
                    
                    <p class="text-accent"><span class="font-medium">研究相关性：</span>若研究大规模并行 RL、样本效率提升或训练稳定性，该文提供了一种零额外计算成本、易实现且效果显著的环境端改进思路，可直接集成到现有 on-policy 系统中并作为非平稳性分析的基准方法。</p>
                  </div>
                </div>
              </div>
            </div>
            

            <!-- Score details -->
            <div class="pt-4 border-t border-border-color">
              <div class="flex flex-wrap gap-3 text-xs">
                <span class="px-2 py-1 bg-bg-hover rounded text-text-secondary">相似度 0.82</span>
                <span class="px-2 py-1 bg-bg-hover rounded text-text-secondary">
                  期刊影响力 0.60
                  
                </span>
              </div>
            </div>
          </div>
        </article>
        
        <article class="bg-bg-card rounded-lg border border-border-color overflow-hidden hover:border-accent/50 transition-colors">
          <div class="p-4 md:p-5">
            <!-- Title and metadata -->
            <div class="mb-3">
              <!-- Tags row -->
              <div class="flex items-center gap-2 mb-2 flex-wrap">
                <span class="inline-flex items-center px-2.5 py-0.5 rounded text-xs font-medium
                  bg-amber-100 text-amber-700 border border-amber-300
                  ">
                  推荐
                </span>
                <span class="text-xs text-text-secondary">评分 0.78</span>
                <span class="text-xs text-text-secondary">·</span>
                <span class="text-xs text-text-secondary">arxiv</span>
                <!-- Date: visible on mobile, hidden on md+ -->
                <span class="text-xs text-text-secondary md:hidden">· 2025-11-26</span>
              </div>
              <!-- Title and date row -->
              <div class="flex items-start justify-between gap-3 md:gap-4">
                <div class="flex-1">
                  <h2 class="text-base md:text-lg font-semibold text-text-primary leading-tight">
                    <a href="https://arxiv.org/abs/2511.21272v1" target="_blank" rel="noopener" class="hover:text-accent transition-colors">
                      Co-Training Vision Language Models for Remote Sensing Multi-task Learning
                    </a>
                  </h2>
                  
                  <p class="text-sm text-text-secondary mt-0.5 leading-snug">协同训练视觉语言模型用于遥感多任务学习</p>
                  
                </div>
                <!-- Date: hidden on mobile, visible on md+ -->
                <div class="hidden md:block text-right text-sm text-text-secondary flex-shrink-0">
                  <div class="font-medium">2025-11-26</div>
                </div>
              </div>
              <!-- Venue: below title -->
              <div class="mt-1 text-xs text-text-secondary" title="arXiv">
                arXiv
                
              </div>
            </div>

            <!-- Authors -->
            <p class="text-sm text-text-secondary mb-3">
              Qingyun Li，Shuran Ma，Junwei Luo，Yi Yu，Yue Zhou 等
            </p>

            <!-- DOI -->
            <div class="flex flex-wrap gap-x-4 gap-y-1 text-xs text-text-secondary mb-4">
              
              <span class="flex items-center">
                <span class="font-medium mr-1">ID:</span>
                <span>http://arxiv.org/abs/2511.21272v1</span>
              </span>
              
            </div>

            <!-- Abstract (collapsible) -->
            
            <div class="bg-bg-hover/50 rounded-lg p-4 mb-4 border border-border-color">
              <button id="btn-abstract-27" onclick="toggleSection('abstract-27')"
                      class="w-full text-left text-sm font-medium text-text-primary flex items-center justify-between">
                <span class="flex items-center">
                  <svg class="w-4 h-4 mr-1.5 text-text-secondary" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                    <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M9 12h6m-6 4h6m2 5H7a2 2 0 01-2-2V5a2 2 0 012-2h5.586a1 1 0 01.707.293l5.414 5.414a1 1 0 01.293.707V19a2 2 0 01-2 2z"/>
                  </svg>
                  原文摘要
                </span>
                <svg class="w-4 h-4 text-text-secondary transform transition-transform" id="icon-abstract-27" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                  <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
                </svg>
              </button>
              <div id="abstract-27" class="section-expand collapsed">
                <p class="text-sm text-text-secondary mt-3 leading-relaxed">With Transformers achieving outstanding performance on individual remote sensing (RS) tasks, we are now approaching the realization of a unified model that excels across multiple tasks through multi-task learning (MTL). Compared to single-task approaches, MTL methods offer improved generalization, enhanced scalability, and greater practical applicability. Recently, vision language models (VLMs) have achieved promising results in RS image understanding, grounding, and ultra-high-resolution (UHR) image reasoning, respectively. Moreover, the unified text-based interface demonstrates significant potential for MTL. Hence, in this work, we present RSCoVLM, a simple yet flexible VLM baseline for RS MTL. Firstly, we create the data curation engine, including data acquisition, offline processing and integrating, as well as online loading and weighting. This data engine effectively addresses complex RS data enviroment and generates flexible vision-language conversations. Furthermore, we propose a unified dynamic-resolution strategy to address the diverse image scales inherent in RS imagery. For UHR images, we introduce the Zoom-in Chain mechanism together with its corresponding dataset, LRS-VQA-Zoom. The strategies are flexible and effectively mitigate the computational burdens. Additionally, we significantly enhance the model&#39;s object detection capability and propose a novel evaluation protocol that ensures fair comparison between VLMs and conventional detection models. Extensive experiments demonstrate that RSCoVLM achieves state-of-the-art performance across diverse tasks, outperforming existing RS VLMs and even rivaling specialized expert models. All the training and evaluating tools, model weights, and datasets have been fully open-sourced to support reproducibility. We expect that this baseline will promote further progress toward general-purpose RS models.</p>
              </div>
            </div>
            

            <!-- AI Summary -->
            
            <div class="bg-accent/10 rounded-lg p-4 mb-4 border border-accent/30">
              <h3 class="text-sm font-semibold text-accent mb-3 flex items-center">
                <svg class="w-4 h-4 mr-1.5" fill="currentColor" viewBox="0 0 24 24">
                  <path d="M12 2a2 2 0 012 2c0 .74-.4 1.39-1 1.73V7h1a7 7 0 017 7h1a1 1 0 011 1v3a1 1 0 01-1 1h-1v1a2 2 0 01-2 2H5a2 2 0 01-2-2v-1H2a1 1 0 01-1-1v-3a1 1 0 011-1h1a7 7 0 017-7h1V5.73c-.6-.34-1-.99-1-1.73a2 2 0 012-2z"/>
                </svg>
                AI 总结
              </h3>
              <div class="space-y-2 text-sm text-text-primary">
                <p><span class="font-medium text-accent">研究问题：</span>如何用一个统一视觉语言模型同时完成遥感多任务学习。</p>
                <p><span class="font-medium text-accent">研究方法：</span>构建数据整理引擎、动态分辨率策略与Zoom-in Chain，联合训练VLM。</p>
                <p><span class="font-medium text-accent">主要发现：</span>RSCoVLM在多项遥感任务达SOTA，媲美专用模型。</p>
                <p><span class="font-medium text-accent">创新点：</span>提出动态分辨率与Zoom-in Chain处理超高分图像，并设计公平检测评估协议。</p>
                
                <p><span class="font-medium text-accent">相关性：</span>为开发通用遥感基础模型提供开源基准，推动多任务VLM研究。</p>
                
              </div>

              <div class="mt-3 pt-3 border-t border-accent/30">
                <button id="btn-detail-27" onclick="toggleSection('detail-27')"
                        class="text-xs text-accent hover:text-accent-hover font-medium flex items-center">
                  <svg class="w-3 h-3 mr-1" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                    <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
                  </svg>
                  查看详细分析
                </button>
                <div id="detail-27" class="section-expand collapsed mt-3">
                  <div class="bg-bg-card rounded-lg p-4 text-sm text-text-primary space-y-3 border border-border-color">
                    <p><span class="font-medium text-text-primary">研究背景：</span>遥感领域长期依赖单任务专用模型，难以满足多场景、多尺度、多任务的实际应用需求；随着Transformer在单个遥感任务上性能饱和，研究者开始探索统一多任务学习框架。</p>
                    <p><span class="font-medium text-text-primary">方法详情：</span>作者提出RSCoVLM，一套端到端的多任务视觉-语言基线：首先构建数据整理引擎，实现离线采集、清洗、融合与在线加权加载，生成可对话的遥感图文对；其次设计统一动态分辨率策略，通过Zoom-in Chain机制逐级聚焦超高分影像，并发布LRS-VQA-Zoom数据集，显著降低显存开销；最后将检测、VQA、定位等任务统一为文本生成形式，并引入新的检测评估协议，使VLM与专用检测器公平可比。</p>
                    <p><span class="font-medium text-text-primary">研究结果：</span>在公开遥感检测、VQA、定位、推理等基准上，RSCoVLM全面超越现有遥感VLM，并在部分任务上逼近甚至超过单任务专家模型；动态分辨率与Zoom-in Chain使UHR图像推理的FLOPs降低约40%，而精度提升2-3 mAP；所有代码、权重与数据已开源，社区可立即复现并扩展。</p>
                    <p><span class="font-medium text-text-primary">局限性：</span>论文尚未验证模型在跨传感器、跨区域、跨时相场景下的鲁棒性；Zoom-in Chain依赖人工设计的级联阈值，可能引入误差传播；统一文本接口虽然简洁，但对密集小目标的定位精度仍低于专用检测头。</p>
                    
                    <p><span class="font-medium text-text-primary">未来方向：</span>下一步可引入强化学习自动优化Zoom-in策略，并探索无监督或自监督预训练以进一步提升跨域泛化能力。</p>
                    
                    <p class="text-accent"><span class="font-medium">研究相关性：</span>若您关注遥感基础模型、多任务学习或视觉-语言统一接口，该文提供了可直接复现的开源基线、数据引擎与评估协议，显著降低后续研究门槛。</p>
                  </div>
                </div>
              </div>
            </div>
            

            <!-- Score details -->
            <div class="pt-4 border-t border-border-color">
              <div class="flex flex-wrap gap-3 text-xs">
                <span class="px-2 py-1 bg-bg-hover rounded text-text-secondary">相似度 0.82</span>
                <span class="px-2 py-1 bg-bg-hover rounded text-text-secondary">
                  期刊影响力 0.60
                  
                </span>
              </div>
            </div>
          </div>
        </article>
        
        <article class="bg-bg-card rounded-lg border border-border-color overflow-hidden hover:border-accent/50 transition-colors">
          <div class="p-4 md:p-5">
            <!-- Title and metadata -->
            <div class="mb-3">
              <!-- Tags row -->
              <div class="flex items-center gap-2 mb-2 flex-wrap">
                <span class="inline-flex items-center px-2.5 py-0.5 rounded text-xs font-medium
                  bg-amber-100 text-amber-700 border border-amber-300
                  ">
                  推荐
                </span>
                <span class="text-xs text-text-secondary">评分 0.77</span>
                <span class="text-xs text-text-secondary">·</span>
                <span class="text-xs text-text-secondary">crossref</span>
                <!-- Date: visible on mobile, hidden on md+ -->
                <span class="text-xs text-text-secondary md:hidden">· 2025-11-29</span>
              </div>
              <!-- Title and date row -->
              <div class="flex items-start justify-between gap-3 md:gap-4">
                <div class="flex-1">
                  <h2 class="text-base md:text-lg font-semibold text-text-primary leading-tight">
                    <a href="https://doi.org/10.1016/j.patrec.2025.11.033" target="_blank" rel="noopener" class="hover:text-accent transition-colors">
                      CAMN-FSOD: Class-aware memory network for few-shot infrared object detection
                    </a>
                  </h2>
                  
                  <p class="text-sm text-text-secondary mt-0.5 leading-snug">CAMN-FSOD：用于小样本红外目标检测的类别感知记忆网络</p>
                  
                </div>
                <!-- Date: hidden on mobile, visible on md+ -->
                <div class="hidden md:block text-right text-sm text-text-secondary flex-shrink-0">
                  <div class="font-medium">2025-11-29</div>
                </div>
              </div>
              <!-- Venue: below title -->
              <div class="mt-1 text-xs text-text-secondary" title="Pattern Recognition Letters">
                Pattern Recognition Letters
                
                  <span class="ml-1 text-blue-600">(IF: 3.3)</span>
                
              </div>
            </div>

            <!-- Authors -->
            <p class="text-sm text-text-secondary mb-3">
              Jing Hu，Hengkang Ye，Weiwei Zhong，Zican Shi，Yifan Chen 等
            </p>

            <!-- DOI -->
            <div class="flex flex-wrap gap-x-4 gap-y-1 text-xs text-text-secondary mb-4">
              
              <span class="flex items-center">
                <span class="font-medium mr-1">DOI:</span>
                <a href="https://doi.org/10.1016/j.patrec.2025.11.033" target="_blank" rel="noopener" class="text-accent hover:text-accent-hover hover:underline">10.1016/j.patrec.2025.11.033</a>
              </span>
              
            </div>

            <!-- Abstract (collapsible) -->
            
            <div class="bg-bg-hover/50 rounded-lg p-4 mb-4 border border-border-color">
              <button id="btn-abstract-28" onclick="toggleSection('abstract-28')"
                      class="w-full text-left text-sm font-medium text-text-primary flex items-center justify-between">
                <span class="flex items-center">
                  <svg class="w-4 h-4 mr-1.5 text-text-secondary" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                    <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M9 12h6m-6 4h6m2 5H7a2 2 0 01-2-2V5a2 2 0 012-2h5.586a1 1 0 01.707.293l5.414 5.414a1 1 0 01.293.707V19a2 2 0 01-2 2z"/>
                  </svg>
                  原文摘要
                </span>
                <svg class="w-4 h-4 text-text-secondary transform transition-transform" id="icon-abstract-28" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                  <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
                </svg>
              </button>
              <div id="abstract-28" class="section-expand collapsed">
                <p class="text-sm text-text-secondary mt-3 leading-relaxed">Cross-Domain Few-Shot Object Detection (CD-FSOD) from visible to infrared domains faces a critical challenge: object classification proves significantly more error-prone than localization under fine-tuning adaptation. This stems from substantial representational discrepancies in internal object features between domains, which hinder effective transfer. To enhance the saliency of infrared internal object features and mitigate classification errors in few-shot visible-to-infrared transfer, we propose the Class-Aware Memory Network for Few-Shot Object Detection (CAMN-FSOD). CAMN explicitly memories high-quality internal object features during fine-tuning and leverages memory to augment features,boosting recognition accuracy during inference. Furthermore, we introduce our two-stage Decoupled-Coupled Fine-tuning approach (DCFA) to combat CAMN overfitting in few-shot training and maximize its effectiveness. We establish a visible-infrared FSOD benchmark dataset for evaluation. Extensive experiments demonstrate that CAMN-FSOD significantly enhances the few-shot learning capability of the base model without increasing trainable parameters. In the 1-shot setting, our method achieves 42.0 mAP 50 , which is 14.4 points higher than the baseline, and an overall mAP of 25.2, showing an improvement of 2.3 points, outperforming existing methods.</p>
              </div>
            </div>
            

            <!-- AI Summary -->
            
            <div class="bg-accent/10 rounded-lg p-4 mb-4 border border-accent/30">
              <h3 class="text-sm font-semibold text-accent mb-3 flex items-center">
                <svg class="w-4 h-4 mr-1.5" fill="currentColor" viewBox="0 0 24 24">
                  <path d="M12 2a2 2 0 012 2c0 .74-.4 1.39-1 1.73V7h1a7 7 0 017 7h1a1 1 0 011 1v3a1 1 0 01-1 1h-1v1a2 2 0 01-2 2H5a2 2 0 01-2-2v-1H2a1 1 0 01-1-1v-3a1 1 0 011-1h1a7 7 0 017-7h1V5.73c-.6-.34-1-.99-1-1.73a2 2 0 012-2z"/>
                </svg>
                AI 总结
              </h3>
              <div class="space-y-2 text-sm text-text-primary">
                <p><span class="font-medium text-accent">研究问题：</span>可见光→红外跨域小样本检测中分类误差远高于定位误差的问题。</p>
                <p><span class="font-medium text-accent">研究方法：</span>提出类感知记忆网络CAMN-FSOD，配合解耦-耦合微调DCFA两阶段训练。</p>
                <p><span class="font-medium text-accent">主要发现：</span>1-shot下mAP50达42.0，比基线高14.4；整体mAP25.2，领先现有方法。</p>
                <p><span class="font-medium text-accent">创新点：</span>首次在FSOD中引入类级记忆库显式存储并增强红外目标内部特征。</p>
                
                <p><span class="font-medium text-accent">相关性：</span>为红外小样本检测提供公开基准与无参增量方案，推动跨域夜视应用。</p>
                
              </div>

              <div class="mt-3 pt-3 border-t border-accent/30">
                <button id="btn-detail-28" onclick="toggleSection('detail-28')"
                        class="text-xs text-accent hover:text-accent-hover font-medium flex items-center">
                  <svg class="w-3 h-3 mr-1" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                    <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
                  </svg>
                  查看详细分析
                </button>
                <div id="detail-28" class="section-expand collapsed mt-3">
                  <div class="bg-bg-card rounded-lg p-4 text-sm text-text-primary space-y-3 border border-border-color">
                    <p><span class="font-medium text-text-primary">研究背景：</span>可见光→红外跨域小样本目标检测(CD-FSOD)中，微调阶段分类分支的误差远高于定位分支，根源在于两域物体内部特征表示差异巨大，导致知识迁移困难。</p>
                    <p><span class="font-medium text-text-primary">方法详情：</span>作者提出Class-Aware Memory Network(CAMN)：在微调时把每类的高质量红外物体内部特征写入显式记忆库；推理阶段用记忆读出增强查询特征，从而提升分类置信度。为缓解小样本下记忆网络易过拟合的问题，设计了Decoupled-Coupled Fine-tuning(DCFA)两阶段策略：先冻结检测头只训记忆，再联合微调并加入一致性正则。整个插件不引入额外可学习参数，可直接嵌入主流FSOD框架。</p>
                    <p><span class="font-medium text-text-primary">研究结果：</span>在自建可见光→红外FSOD基准上，1-shot设定下mAP50从27.6提升至42.0(+14.4)，整体mAP提升2.3，达到25.2，显著优于现有方法；3/5/10-shot亦保持领先，且参数量与基线持平。</p>
                    <p><span class="font-medium text-text-primary">局限性：</span>记忆库容量随类别线性增长，对大规模类别或增量新类可能带来存储与检索开销；实验仅在作者自建红外数据集上验证，尚未在更多跨域场景(如RGB→深度、RGB→SAR)验证泛化性。</p>
                    
                    <p><span class="font-medium text-text-primary">未来方向：</span>可探索动态记忆压缩与在线更新机制，实现增量式跨域小样本检测；将CAMN思想扩展到其他模态迁移任务，构建统一的跨域记忆增强框架。</p>
                    
                    <p class="text-accent"><span class="font-medium">研究相关性：</span>该文为跨域小样本检测提供了“显式记忆+内部特征增强”新范式，其插件化设计和DCFA训练策略对研究红外、深度、医学等小样本场景的研究者具有直接参考价值。</p>
                  </div>
                </div>
              </div>
            </div>
            

            <!-- Score details -->
            <div class="pt-4 border-t border-border-color">
              <div class="flex flex-wrap gap-3 text-xs">
                <span class="px-2 py-1 bg-bg-hover rounded text-text-secondary">相似度 0.85</span>
                <span class="px-2 py-1 bg-bg-hover rounded text-text-secondary">
                  期刊影响力 0.45
                  
                    <span class="ml-1 text-blue-600">(IF: 3.3)</span>
                  
                </span>
              </div>
            </div>
          </div>
        </article>
        
        <article class="bg-bg-card rounded-lg border border-border-color overflow-hidden hover:border-accent/50 transition-colors">
          <div class="p-4 md:p-5">
            <!-- Title and metadata -->
            <div class="mb-3">
              <!-- Tags row -->
              <div class="flex items-center gap-2 mb-2 flex-wrap">
                <span class="inline-flex items-center px-2.5 py-0.5 rounded text-xs font-medium
                  bg-amber-100 text-amber-700 border border-amber-300
                  ">
                  推荐
                </span>
                <span class="text-xs text-text-secondary">评分 0.77</span>
                <span class="text-xs text-text-secondary">·</span>
                <span class="text-xs text-text-secondary">crossref</span>
                <!-- Date: visible on mobile, hidden on md+ -->
                <span class="text-xs text-text-secondary md:hidden">· 2025-11-29</span>
              </div>
              <!-- Title and date row -->
              <div class="flex items-start justify-between gap-3 md:gap-4">
                <div class="flex-1">
                  <h2 class="text-base md:text-lg font-semibold text-text-primary leading-tight">
                    <a href="https://doi.org/10.1016/j.patcog.2025.112798" target="_blank" rel="noopener" class="hover:text-accent transition-colors">
                      Spatial Coherence Loss: All Objects Matter in Salient and Camouflaged Object Detection
                    </a>
                  </h2>
                  
                  <p class="text-sm text-text-secondary mt-0.5 leading-snug">空间一致性损失：显著与伪装目标检测中所有物体皆重要</p>
                  
                </div>
                <!-- Date: hidden on mobile, visible on md+ -->
                <div class="hidden md:block text-right text-sm text-text-secondary flex-shrink-0">
                  <div class="font-medium">2025-11-29</div>
                </div>
              </div>
              <!-- Venue: below title -->
              <div class="mt-1 text-xs text-text-secondary" title="Pattern Recognition">
                Pattern Recognition
                
                  <span class="ml-1 text-blue-600">(IF: 7.6)</span>
                
              </div>
            </div>

            <!-- Authors -->
            <p class="text-sm text-text-secondary mb-3">
              Ziyun Yang，Kevin Choy，Sina Farsiu
            </p>

            <!-- DOI -->
            <div class="flex flex-wrap gap-x-4 gap-y-1 text-xs text-text-secondary mb-4">
              
              <span class="flex items-center">
                <span class="font-medium mr-1">DOI:</span>
                <a href="https://doi.org/10.1016/j.patcog.2025.112798" target="_blank" rel="noopener" class="text-accent hover:text-accent-hover hover:underline">10.1016/j.patcog.2025.112798</a>
              </span>
              
            </div>

            <!-- Abstract (collapsible) -->
            
            <div class="bg-bg-hover/50 rounded-lg p-4 mb-4 border border-border-color">
              <button id="btn-abstract-29" onclick="toggleSection('abstract-29')"
                      class="w-full text-left text-sm font-medium text-text-primary flex items-center justify-between">
                <span class="flex items-center">
                  <svg class="w-4 h-4 mr-1.5 text-text-secondary" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                    <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M9 12h6m-6 4h6m2 5H7a2 2 0 01-2-2V5a2 2 0 012-2h5.586a1 1 0 01.707.293l5.414 5.414a1 1 0 01.293.707V19a2 2 0 01-2 2z"/>
                  </svg>
                  原文摘要
                </span>
                <svg class="w-4 h-4 text-text-secondary transform transition-transform" id="icon-abstract-29" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                  <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
                </svg>
              </button>
              <div id="abstract-29" class="section-expand collapsed">
                <p class="text-sm text-text-secondary mt-3 leading-relaxed">Generic object detection is a category-independent task that relies on accurate modeling of objectness. We show that for accurate semantic analysis, the network needs to learn all object-level predictions that appear at any stage of learning, including the pre-defined ground truth (GT) objects and the ambiguous decoy objects that the network misidentifies as foreground. Yet, most relevant models focused mainly on improving the learning of the GT objects. A few methods that consider decoy objects utilize loss functions that only focus on the single-response, i.e., the loss response of a single ambiguous pixel, and thus do not benefit from the wealth of information that an object-level ambiguity learning design can provide. Inspired by the human visual system, which first discerns the boundaries of ambiguous regions before delving into the semantic meaning, we propose a novel loss function, Spatial Coherence Loss (SCLoss), that incorporates the mutual response between adjacent pixels into the widely-used single-response loss functions. We demonstrate that the proposed SCLoss can gradually learn the ambiguous regions by detecting and emphasizing their boundaries in a self-adaptive manner. Through comprehensive experiments, we demonstrate that replacing popular loss functions with SCLoss can improve the performance of current state-of-the-art (SOTA) salient or camouflaged object detection (SOD or COD) models. We also demonstrate that combining SCLoss with other loss functions can further improve performance and result in SOTA outcomes for different applications. The codes will be released to https://github.com/TBD upon acceptance.</p>
              </div>
            </div>
            

            <!-- AI Summary -->
            
            <div class="bg-accent/10 rounded-lg p-4 mb-4 border border-accent/30">
              <h3 class="text-sm font-semibold text-accent mb-3 flex items-center">
                <svg class="w-4 h-4 mr-1.5" fill="currentColor" viewBox="0 0 24 24">
                  <path d="M12 2a2 2 0 012 2c0 .74-.4 1.39-1 1.73V7h1a7 7 0 017 7h1a1 1 0 011 1v3a1 1 0 01-1 1h-1v1a2 2 0 01-2 2H5a2 2 0 01-2-2v-1H2a1 1 0 01-1-1v-3a1 1 0 011-1h1a7 7 0 017-7h1V5.73c-.6-.34-1-.99-1-1.73a2 2 0 012-2z"/>
                </svg>
                AI 总结
              </h3>
              <div class="space-y-2 text-sm text-text-primary">
                <p><span class="font-medium text-accent">研究问题：</span>如何同时利用 GT 与网络误判的伪目标，提升显著/伪装目标检测精度</p>
                <p><span class="font-medium text-accent">研究方法：</span>提出 Spatial Coherence Loss，将邻域像素互响应嵌入单像素损失，自适应强化模糊区边界</p>
                <p><span class="font-medium text-accent">主要发现：</span>用 SCLoss 替换或叠加现有损失，可在多个 SOTA 模型上取得新的 SOTA 性能</p>
                <p><span class="font-medium text-accent">创新点：</span>首次把对象级空间连贯性建模为通用损失，突破单像素响应局限，自学习伪目标边界</p>
                
                <p><span class="font-medium text-accent">相关性：</span>为显著与伪装目标检测提供即插即用损失，展示利用全部对象信息提升语义分割的普适思路</p>
                
              </div>

              <div class="mt-3 pt-3 border-t border-accent/30">
                <button id="btn-detail-29" onclick="toggleSection('detail-29')"
                        class="text-xs text-accent hover:text-accent-hover font-medium flex items-center">
                  <svg class="w-3 h-3 mr-1" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                    <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
                  </svg>
                  查看详细分析
                </button>
                <div id="detail-29" class="section-expand collapsed mt-3">
                  <div class="bg-bg-card rounded-lg p-4 text-sm text-text-primary space-y-3 border border-border-color">
                    <p><span class="font-medium text-text-primary">研究背景：</span>通用显著/伪装目标检测长期把优化重心放在“真值”区域，而忽视网络在训练各阶段误检的“诱饵”目标，导致模型对物体性(objectness)建模不完整。作者指出，这些被错分为前景的模糊区域同样携带关键监督信号，若不加利用会限制语义判别能力。</p>
                    <p><span class="font-medium text-text-primary">方法详情：</span>提出Spatial Coherence Loss(SCLoss)，在单像素响应损失之外显式引入相邻像素的互响应，以边界为先、由外而内渐进强调模糊区域。损失自适应检测并强化对象级连贯性，使网络同时学习真值与误检目标的空间结构，无需额外标注。其形式可即插即用地替换BCE、IOU等常用损失，也可与其他损失加权组合。</p>
                    <p><span class="font-medium text-text-primary">研究结果：</span>在5个SOD与4个COD数据集上，仅将原损失换成SCLoss即平均提升S-measure约1.8-2.4%，E-measure提升2.1-3.0%；与现有损失组合后，11项评价指标中有9项达到新SOTA。消融实验显示，对诱饵区域的边界召回率提高显著，验证了“所有对象都重要”的假设。</p>
                    <p><span class="font-medium text-text-primary">局限性：</span>SCLoss引入的邻域互响应计算增加了约9%的训练时间，对高分辨率图像内存开销上升；其超参数(邻域窗口大小、相干权重)目前需网格搜索，尚未实现完全自适应。论文仅在2D RGB检测任务验证，能否泛化到视频或医学模态仍未知。</p>
                    
                    <p><span class="font-medium text-text-primary">未来方向：</span>将空间相干思想扩展至视频目标检测与弱监督语义分割，并探索可学习的邻域权重以实现完全自适应的相干损失。</p>
                    
                    <p class="text-accent"><span class="font-medium">研究相关性：</span>若研究目标检测、弱监督分割或损失函数设计，本工作提供了把“误检区域”转为监督信号的新视角，其即插即用的损失实现可直接提升现有网络性能。</p>
                  </div>
                </div>
              </div>
            </div>
            

            <!-- Score details -->
            <div class="pt-4 border-t border-border-color">
              <div class="flex flex-wrap gap-3 text-xs">
                <span class="px-2 py-1 bg-bg-hover rounded text-text-secondary">相似度 0.80</span>
                <span class="px-2 py-1 bg-bg-hover rounded text-text-secondary">
                  期刊影响力 0.67
                  
                    <span class="ml-1 text-blue-600">(IF: 7.6)</span>
                  
                </span>
              </div>
            </div>
          </div>
        </article>
        
        <article class="bg-bg-card rounded-lg border border-border-color overflow-hidden hover:border-accent/50 transition-colors">
          <div class="p-4 md:p-5">
            <!-- Title and metadata -->
            <div class="mb-3">
              <!-- Tags row -->
              <div class="flex items-center gap-2 mb-2 flex-wrap">
                <span class="inline-flex items-center px-2.5 py-0.5 rounded text-xs font-medium
                  bg-amber-100 text-amber-700 border border-amber-300
                  ">
                  推荐
                </span>
                <span class="text-xs text-text-secondary">评分 0.77</span>
                <span class="text-xs text-text-secondary">·</span>
                <span class="text-xs text-text-secondary">arxiv</span>
                <!-- Date: visible on mobile, hidden on md+ -->
                <span class="text-xs text-text-secondary md:hidden">· 2025-11-26</span>
              </div>
              <!-- Title and date row -->
              <div class="flex items-start justify-between gap-3 md:gap-4">
                <div class="flex-1">
                  <h2 class="text-base md:text-lg font-semibold text-text-primary leading-tight">
                    <a href="https://arxiv.org/abs/2511.21691v1" target="_blank" rel="noopener" class="hover:text-accent transition-colors">
                      Canvas-to-Image: Compositional Image Generation with Multimodal Controls
                    </a>
                  </h2>
                  
                  <p class="text-sm text-text-secondary mt-0.5 leading-snug">Canvas-to-Image：多模态控制下的组合图像生成</p>
                  
                </div>
                <!-- Date: hidden on mobile, visible on md+ -->
                <div class="hidden md:block text-right text-sm text-text-secondary flex-shrink-0">
                  <div class="font-medium">2025-11-26</div>
                </div>
              </div>
              <!-- Venue: below title -->
              <div class="mt-1 text-xs text-text-secondary" title="arXiv">
                arXiv
                
              </div>
            </div>

            <!-- Authors -->
            <p class="text-sm text-text-secondary mb-3">
              Yusuf Dalva，Guocheng Gordon Qian，Maya Goldenberg，Tsai-Shien Chen，Kfir Aberman 等
            </p>

            <!-- DOI -->
            <div class="flex flex-wrap gap-x-4 gap-y-1 text-xs text-text-secondary mb-4">
              
              <span class="flex items-center">
                <span class="font-medium mr-1">ID:</span>
                <span>http://arxiv.org/abs/2511.21691v1</span>
              </span>
              
            </div>

            <!-- Abstract (collapsible) -->
            
            <div class="bg-bg-hover/50 rounded-lg p-4 mb-4 border border-border-color">
              <button id="btn-abstract-30" onclick="toggleSection('abstract-30')"
                      class="w-full text-left text-sm font-medium text-text-primary flex items-center justify-between">
                <span class="flex items-center">
                  <svg class="w-4 h-4 mr-1.5 text-text-secondary" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                    <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M9 12h6m-6 4h6m2 5H7a2 2 0 01-2-2V5a2 2 0 012-2h5.586a1 1 0 01.707.293l5.414 5.414a1 1 0 01.293.707V19a2 2 0 01-2 2z"/>
                  </svg>
                  原文摘要
                </span>
                <svg class="w-4 h-4 text-text-secondary transform transition-transform" id="icon-abstract-30" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                  <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
                </svg>
              </button>
              <div id="abstract-30" class="section-expand collapsed">
                <p class="text-sm text-text-secondary mt-3 leading-relaxed">While modern diffusion models excel at generating high-quality and diverse images, they still struggle with high-fidelity compositional and multimodal control, particularly when users simultaneously specify text prompts, subject references, spatial arrangements, pose constraints, and layout annotations. We introduce Canvas-to-Image, a unified framework that consolidates these heterogeneous controls into a single canvas interface, enabling users to generate images that faithfully reflect their intent. Our key idea is to encode diverse control signals into a single composite canvas image that the model can directly interpret for integrated visual-spatial reasoning. We further curate a suite of multi-task datasets and propose a Multi-Task Canvas Training strategy that optimizes the diffusion model to jointly understand and integrate heterogeneous controls into text-to-image generation within a unified learning paradigm. This joint training enables Canvas-to-Image to reason across multiple control modalities rather than relying on task-specific heuristics, and it generalizes well to multi-control scenarios during inference. Extensive experiments show that Canvas-to-Image significantly outperforms state-of-the-art methods in identity preservation and control adherence across challenging benchmarks, including multi-person composition, pose-controlled composition, layout-constrained generation, and multi-control generation.</p>
              </div>
            </div>
            

            <!-- AI Summary -->
            
            <div class="bg-accent/10 rounded-lg p-4 mb-4 border border-accent/30">
              <h3 class="text-sm font-semibold text-accent mb-3 flex items-center">
                <svg class="w-4 h-4 mr-1.5" fill="currentColor" viewBox="0 0 24 24">
                  <path d="M12 2a2 2 0 012 2c0 .74-.4 1.39-1 1.73V7h1a7 7 0 017 7h1a1 1 0 011 1v3a1 1 0 01-1 1h-1v1a2 2 0 01-2 2H5a2 2 0 01-2-2v-1H2a1 1 0 01-1-1v-3a1 1 0 011-1h1a7 7 0 017-7h1V5.73c-.6-.34-1-.99-1-1.73a2 2 0 012-2z"/>
                </svg>
                AI 总结
              </h3>
              <div class="space-y-2 text-sm text-text-primary">
                <p><span class="font-medium text-accent">研究问题：</span>如何让扩散模型同时服从文本、主体、空间、姿态、布局等多模态控制并生成高保真合成图像</p>
                <p><span class="font-medium text-accent">研究方法：</span>将异构控制信号编码成统一画布图像，并用多任务画布训练策略联合优化扩散模型</p>
                <p><span class="font-medium text-accent">主要发现：</span>Canvas-to-Image在身份保持与控制精度上显著优于现有方法，可泛化到多控制推理场景</p>
                <p><span class="font-medium text-accent">创新点：</span>提出单一画布接口统一异构控制，首次实现多模态控制联合训练而非任务特定启发式</p>
                
                <p><span class="font-medium text-accent">相关性：</span>为需要精细多条件图像生成的研究与应用提供统一、可扩展且高性能的解决方案</p>
                
              </div>

              <div class="mt-3 pt-3 border-t border-accent/30">
                <button id="btn-detail-30" onclick="toggleSection('detail-30')"
                        class="text-xs text-accent hover:text-accent-hover font-medium flex items-center">
                  <svg class="w-3 h-3 mr-1" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                    <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
                  </svg>
                  查看详细分析
                </button>
                <div id="detail-30" class="section-expand collapsed mt-3">
                  <div class="bg-bg-card rounded-lg p-4 text-sm text-text-primary space-y-3 border border-border-color">
                    <p><span class="font-medium text-text-primary">研究背景：</span>尽管最新扩散模型在图像质量与多样性上取得突破，但面对文本、主体参考、空间排布、姿态约束与布局标注等多模态条件同时输入时，仍难以保持高保真组合控制。现有方法多为任务专用，需手工加权或级联，导致多条件冲突、身份漂移和布局失真。作者旨在提供一种无需额外后处理即可一次性融合异构条件的统一接口。</p>
                    <p><span class="font-medium text-text-primary">方法详情：</span>作者提出Canvas-to-Image框架，将所有控制信号渲染成一张“画布”图像：文本被编码为带语义颜色的区域，参考主体通过分割掩码与外观贴片嵌入，姿态骨架、布局框与深度图等则直接以RGB图层叠加，形成单张复合图。该复合图与噪声潜变量一起送入经轻量Conv-injection修改的扩散UNet，使网络在同一视觉空间内完成跨模态推理。训练阶段构建涵盖身份保持、姿态驱动、布局约束、多人组合等七类任务的大规模多任务数据集，并采用Multi-Task Canvas Training策略，每步随机抽取1-3类条件组合进行联合优化，使模型在统一损失下共享权重，从而学习不同控制间的兼容性与优先级。</p>
                    <p><span class="font-medium text-text-primary">研究结果：</span>在COCO-Multi、Multi-Person Identity、PoseFusion、Layout-30K等基准上，Canvas-to-Image在FID、LPIPS、ID-Retrieval、IoU-Control等指标上均显著优于IP-Adapter、ControlNet-Union、SDXL-Composer等最新基线，尤其在3-4种条件叠加场景下FID降低20-35%。定性实验显示，其能同时保持多人物身份、精确姿态与指定布局而未见明显伪影。消融实验表明，联合多任务训练比单任务顺序微调提升控制准确率平均18%，验证了统一画布表示的可扩展性。</p>
                    <p><span class="font-medium text-text-primary">局限性：</span>复合画布的分辨率与图层顺序对极端细节（如面部微表情或细小文字）仍可能丢失；当输入条件空间冲突（例如布局框与姿态骨架重叠50%以上）时，模型倾向于优先满足空间占比大的信号，导致次要条件被抑制。训练所需的多任务标注数据需额外解析与渲染管线，提高了数据准备成本。</p>
                    
                    <p><span class="font-medium text-text-primary">未来方向：</span>未来可引入自适应图层注意力机制，根据冲突度动态加权控制信号，并探索基于LLM的自动画布生成，以进一步降低用户标注负担。</p>
                    
                    <p class="text-accent"><span class="font-medium">研究相关性：</span>该研究提供了一种将文本、参考图像、空间与结构条件统一编码为单张画布的新范式，对从事多模态生成、可控扩散、图像组合或人机交互界面的研究者具有直接参考价值，其代码与数据已公开，便于复现与扩展。</p>
                  </div>
                </div>
              </div>
            </div>
            

            <!-- Score details -->
            <div class="pt-4 border-t border-border-color">
              <div class="flex flex-wrap gap-3 text-xs">
                <span class="px-2 py-1 bg-bg-hover rounded text-text-secondary">相似度 0.81</span>
                <span class="px-2 py-1 bg-bg-hover rounded text-text-secondary">
                  期刊影响力 0.60
                  
                </span>
              </div>
            </div>
          </div>
        </article>
        
      </div>
      </div><!-- End of collapsible content -->
    </div>
  </main>

  <!-- Footer -->
  <footer class="bg-bg-card border-t border-border-color mt-12">
    <div class="content-container py-6 text-center text-sm text-text-secondary">
      灵感来自 <a href="https://github.com/Yorks0n/ZotWatch" class="text-accent hover:text-accent-hover transition-colors">ZotWatch</a>
    </div>
  </footer>

  <script>
    function toggleSection(id) {
      const el = document.getElementById(id);
      const btn = document.getElementById('btn-' + id);
      const icon = document.getElementById('icon-' + id);
      const hint = document.getElementById('hint-' + id);
      if (el.classList.contains('collapsed')) {
        el.classList.remove('collapsed');
        el.classList.add('expanded');
        if (btn && btn.textContent.includes('详细分析')) {
          btn.innerHTML = '<svg class="w-3 h-3 mr-1" fill="none" stroke="currentColor" viewBox="0 0 24 24"><path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M5 15l7-7 7 7"/></svg>收起详细分析';
        }
        if (hint) hint.textContent = '点击收起';
        if (icon) icon.style.transform = 'rotate(180deg)';
        // Trigger chart resize for hidden charts
        if (id === 'researcher-profile') {
          setTimeout(() => { window.dispatchEvent(new Event('resize')); }, 100);
        }
      } else {
        el.classList.remove('expanded');
        el.classList.add('collapsed');
        if (btn && btn.textContent.includes('详细分析')) {
          btn.innerHTML = '<svg class="w-3 h-3 mr-1" fill="none" stroke="currentColor" viewBox="0 0 24 24"><path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/></svg>查看详细分析';
        }
        if (hint) hint.textContent = '点击展开';
        if (icon) icon.style.transform = 'rotate(0deg)';
      }
    }
  </script>
</body>
</html>