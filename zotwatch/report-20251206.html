<!DOCTYPE html>
<html lang="zh-CN">
<head>
  <meta charset="utf-8">
  <meta name="viewport" content="width=device-width, initial-scale=1">
  <title>ZotWatch 文献推荐 - 2025-12-06</title>
  <script src="https://cdn.tailwindcss.com"></script>
  <script src="https://cdn.jsdelivr.net/npm/chart.js@4.4.1/dist/chart.umd.min.js"></script>
  <script>
    tailwind.config = {
      theme: {
        extend: {
          colors: {
            'bg-primary': '#f8fafc',
            'bg-card': '#ffffff',
            'bg-hover': '#f1f5f9',
            'text-primary': '#1e293b',
            'text-secondary': '#64748b',
            'border-color': '#e2e8f0',
            'accent': '#2563eb',
            'accent-hover': '#1d4ed8',
          }
        }
      }
    }
  </script>
  <style>
    .section-expand { transition: max-height 0.5s ease-out; overflow: hidden; }
    .section-expand.collapsed { max-height: 0; }
    .section-expand.expanded { max-height: none; }
    body { font-family: -apple-system, BlinkMacSystemFont, 'Segoe UI', Roboto, 'Helvetica Neue', Arial, sans-serif; }
    .content-container { max-width: 48rem; margin: 0 auto; padding: 0 1rem; }
    @media (min-width: 768px) { .content-container { padding: 0 2rem; } }
    @media (min-width: 1024px) { .content-container { padding: 0 4rem; } }
  </style>
</head>
<body class="bg-bg-primary min-h-screen text-text-primary">
  <!-- Header -->
  <header class="bg-bg-card border-b border-border-color">
    <div class="content-container py-6 md:py-8">
      <h1 class="text-xl md:text-2xl font-bold text-text-primary">ZotWatch 文献推荐</h1>
      <p class="text-xs md:text-sm text-text-secondary mt-1">
        共 35 篇论文 ·
        生成于 2025-12-06 10:41 Asia/Shanghai
      </p>
    </div>
  </header>

  <!-- Researcher Profile Section (Collapsible) -->
  
  <section class="py-4 md:py-6 border-b border-border-color">
    <div class="content-container">
      <!-- Collapsible Header -->
      <button id="btn-researcher-profile" onclick="toggleSection('researcher-profile')"
              class="w-full text-left text-lg md:text-xl font-bold text-text-primary mb-3 md:mb-4 flex items-center justify-between group hover:text-accent transition-colors">
        <span class="flex items-center gap-2 flex-1 min-w-0">
          <svg class="w-5 h-5 flex-shrink-0 text-accent" fill="currentColor" viewBox="0 0 24 24">
            <path d="M12 2C6.48 2 2 6.48 2 12s4.48 10 10 10 10-4.48 10-10S17.52 2 12 2zm0 3c1.66 0 3 1.34 3 3s-1.34 3-3 3-3-1.34-3-3 1.34-3 3-3zm0 14.2c-2.5 0-4.71-1.28-6-3.22.03-1.99 4-3.08 6-3.08 1.99 0 5.97 1.09 6 3.08-1.29 1.94-3.5 3.22-6 3.22z"/>
          </svg>
          <span class="truncate">研究兴趣画像</span>
        </span>
        <span class="flex items-center gap-1 md:gap-2 flex-shrink-0 ml-2">
          <span class="hidden md:inline text-sm font-normal text-text-secondary group-hover:text-accent" id="hint-researcher-profile">点击展开</span>
          <svg class="w-5 h-5 md:w-4 md:h-4 text-text-secondary group-hover:text-accent transform transition-transform" id="icon-researcher-profile" fill="none" stroke="currentColor" viewBox="0 0 24 24">
            <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
          </svg>
        </span>
      </button>

      <!-- Collapsible Content -->
      <div id="researcher-profile" class="section-expand collapsed">
      <!-- Basic Stats - Compact Row -->
      <div class="bg-bg-card rounded-lg border border-border-color p-4 mb-4">
        <div class="grid grid-cols-4 gap-3 text-center">
          <div>
            <div class="text-xl font-bold text-accent">2696</div>
            <div class="text-xs text-text-secondary">收藏论文</div>
          </div>
          <div>
            <div class="text-xl font-bold text-accent">5年8月</div>
            <div class="text-xs text-text-secondary">收藏时长</div>
          </div>
          <div>
            <div class="text-xl font-bold text-accent">&gt;10</div>
            <div class="text-xs text-text-secondary">兴趣领域</div>
          </div>
          <div>
            <div class="text-xl font-bold text-accent">7</div>
            <div class="text-xs text-text-secondary">高频作者</div>
          </div>
        </div>
      </div>

      <!-- LLM Insights - Compact -->
      
      <div class="bg-accent/10 rounded-lg border border-accent/30 p-4 mb-4">
        <h3 class="text-sm font-semibold text-accent mb-2 flex items-center gap-2">
          <svg class="w-4 h-4" fill="currentColor" viewBox="0 0 24 24">
            <path d="M12 2a2 2 0 012 2c0 .74-.4 1.39-1 1.73V7h1a7 7 0 017 7h1a1 1 0 011 1v3a1 1 0 01-1 1h-1v1a2 2 0 01-2 2H5a2 2 0 01-2-2v-1H2a1 1 0 01-1-1v-3a1 1 0 011-1h1a7 7 0 017-7h1V5.73c-.6-.34-1-.99-1-1.73a2 2 0 012-2z"/>
          </svg>
          AI 研究兴趣分析
        </h3>
        <div class="space-y-2 text-xs text-text-primary leading-relaxed">
          <p><span class="font-medium text-accent">研究兴趣：</span>收藏记录显示，用户长期关注计算机视觉与遥感交叉方向，核心阅读集中在目标检测、SAR图像理解及高效模型设计，同时对大模型与自监督学习保持同步追踪。</p>
          <p><span class="font-medium text-accent">深度关注：</span>在CVPR、NeurIPS、IEEE TGRS等顶会顶刊持续收藏逾300篇文献，尤其对Kaiming He、Ross Girshick等团队的检测与分割工作形成系统积累；SAR目标识别与旋转目标检测关键词反复出现，表明其在遥感智能解译方向有深厚阅读深度。</p>
          <p><span class="font-medium text-accent">跨学科倾向：</span>用户阅读横跨计算机视觉、遥感、机器学习三大领域，将通用视觉方法（Transformer、模型压缩）主动迁移至SAR/遥感数据，体现出明显的“CV方法+遥感应用”交叉特征。</p>
          <p><span class="font-medium text-accent">兴趣演变：</span>2025-Q1收藏量激增至81篇，新增关键词聚焦“视觉Transformer、大语言模型、基础模型”，显示兴趣正从传统检测/分割向大模型与多模态遥感快速转移；2024-Q3后收藏量回落但每季度仍保持≥10篇，表明进入精选阅读阶段。</p>
          <p><span class="font-medium text-accent">阅读建议：</span>建议进一步关注多模态遥感基础模型（RS-LLM、Vision-Language-Radar）、轻量化Transformer在轨实时处理，以及基于扩散模型的SAR数据增强与仿真生成方向。</p>
        </div>
      </div>
      

      <!-- Charts Row: Domains + Trends -->
      <div class="grid grid-cols-1 md:grid-cols-2 gap-4 mb-4">
        <!-- Research Domains Chart -->
        
        <div class="bg-bg-card rounded-lg border border-border-color p-4">
          <h4 class="text-sm font-medium text-text-primary mb-3">研究领域分布</h4>
          <div style="height: 180px;">
            <canvas id="domainsChart"></canvas>
          </div>
        </div>
        

        <!-- Quarterly Trends Chart -->
        
        <div class="bg-bg-card rounded-lg border border-border-color p-4">
          <h4 class="text-sm font-medium text-text-primary mb-3">阅读趋势（季度）</h4>
          <div style="height: 180px;">
            <canvas id="trendsChart"></canvas>
          </div>
        </div>
        
      </div>

      <!-- Year Distribution Chart (full width) -->
      
      <div class="bg-bg-card rounded-lg border border-border-color p-4 mb-4">
        <h4 class="text-sm font-medium text-text-primary mb-3">论文发表年份分布</h4>
        <div style="height: 180px;">
          <canvas id="yearDistChart"></canvas>
        </div>
      </div>
      

      <!-- Authors and Venues Row -->
      <div class="grid grid-cols-1 md:grid-cols-2 gap-4 mb-4">
        <!-- Top Authors -->
        
        <div class="bg-bg-card rounded-lg border border-border-color p-4">
          <h4 class="text-sm font-medium text-text-primary mb-2">高频关注作者</h4>
          <div class="grid grid-cols-2 gap-x-4 gap-y-1 text-xs">
            
            <div class="flex justify-between truncate">
              <span class="text-text-primary truncate" title="Kaiming He">Kaiming He</span>
              <span class="text-text-secondary ml-1 flex-shrink-0">24</span>
            </div>
            
            <div class="flex justify-between truncate">
              <span class="text-text-primary truncate" title="Ross Girshick">Ross Girshick</span>
              <span class="text-text-secondary ml-1 flex-shrink-0">18</span>
            </div>
            
            <div class="flex justify-between truncate">
              <span class="text-text-primary truncate" title="Jian Sun">Jian Sun</span>
              <span class="text-text-secondary ml-1 flex-shrink-0">15</span>
            </div>
            
            <div class="flex justify-between truncate">
              <span class="text-text-primary truncate" title="Song Han">Song Han</span>
              <span class="text-text-secondary ml-1 flex-shrink-0">14</span>
            </div>
            
            <div class="flex justify-between truncate">
              <span class="text-text-primary truncate" title="Xiangyu Zhang">Xiangyu Zhang</span>
              <span class="text-text-secondary ml-1 flex-shrink-0">14</span>
            </div>
            
            <div class="flex justify-between truncate">
              <span class="text-text-primary truncate" title="Piotr Dollár">Piotr Dollár</span>
              <span class="text-text-secondary ml-1 flex-shrink-0">12</span>
            </div>
            
            <div class="flex justify-between truncate">
              <span class="text-text-primary truncate" title="Xiang Li">Xiang Li</span>
              <span class="text-text-secondary ml-1 flex-shrink-0">10</span>
            </div>
            
            <div class="flex justify-between truncate">
              <span class="text-text-primary truncate" title="Li Liu">Li Liu</span>
              <span class="text-text-secondary ml-1 flex-shrink-0">9</span>
            </div>
            
            <div class="flex justify-between truncate">
              <span class="text-text-primary truncate" title="Chunhua Shen">Chunhua Shen</span>
              <span class="text-text-secondary ml-1 flex-shrink-0">8</span>
            </div>
            
            <div class="flex justify-between truncate">
              <span class="text-text-primary truncate" title="Yann LeCun">Yann LeCun</span>
              <span class="text-text-secondary ml-1 flex-shrink-0">8</span>
            </div>
            
            <div class="flex justify-between truncate">
              <span class="text-text-primary truncate" title="Gangyao Kuang">Gangyao Kuang</span>
              <span class="text-text-secondary ml-1 flex-shrink-0">8</span>
            </div>
            
            <div class="flex justify-between truncate">
              <span class="text-text-primary truncate" title="Wenfeng Liang">Wenfeng Liang</span>
              <span class="text-text-secondary ml-1 flex-shrink-0">8</span>
            </div>
            
          </div>
        </div>
        

        <!-- Top Venues -->
        
        <div class="bg-bg-card rounded-lg border border-border-color p-4">
          <h4 class="text-sm font-medium text-text-primary mb-2">常关注期刊/会议</h4>
          <div class="space-y-1 text-xs">
            
            <div class="flex items-center justify-between">
              <span class="text-text-primary truncate flex-1" title="Computer Vision and Pattern Recognition">Computer Vision and Pattern Recognition</span>
              <span class="text-xs px-1 py-0.5 rounded bg-blue-100 text-blue-700 ml-1 flex-shrink-0">
                期刊
              </span>
              <span class="text-text-secondary ml-1 flex-shrink-0">111</span>
            </div>
            
            <div class="flex items-center justify-between">
              <span class="text-text-primary truncate flex-1" title="Neural Information Processing Systems">Neural Information Processing Systems</span>
              <span class="text-xs px-1 py-0.5 rounded bg-blue-100 text-blue-700 ml-1 flex-shrink-0">
                期刊
              </span>
              <span class="text-text-secondary ml-1 flex-shrink-0">45</span>
            </div>
            
            <div class="flex items-center justify-between">
              <span class="text-text-primary truncate flex-1" title="IEEE Transactions on Geoscience and Remote Sensing">IEEE Transactions on Geoscience and Remote Sensing</span>
              <span class="text-xs px-1 py-0.5 rounded bg-blue-100 text-blue-700 ml-1 flex-shrink-0">
                期刊
              </span>
              <span class="text-text-secondary ml-1 flex-shrink-0">41</span>
            </div>
            
            <div class="flex items-center justify-between">
              <span class="text-text-primary truncate flex-1" title="International Conference on Computer Vision">International Conference on Computer Vision</span>
              <span class="text-xs px-1 py-0.5 rounded bg-amber-100 text-amber-700 ml-1 flex-shrink-0">
                会议
              </span>
              <span class="text-text-secondary ml-1 flex-shrink-0">36</span>
            </div>
            
            <div class="flex items-center justify-between">
              <span class="text-text-primary truncate flex-1" title="IEEE Transactions on Pattern Analysis and Machine Intelligence">IEEE Transactions on Pattern Analysis and Machine Intelligence</span>
              <span class="text-xs px-1 py-0.5 rounded bg-blue-100 text-blue-700 ml-1 flex-shrink-0">
                期刊
              </span>
              <span class="text-text-secondary ml-1 flex-shrink-0">31</span>
            </div>
            
            <div class="flex items-center justify-between">
              <span class="text-text-primary truncate flex-1" title="International Conference on Learning Representations">International Conference on Learning Representations</span>
              <span class="text-xs px-1 py-0.5 rounded bg-amber-100 text-amber-700 ml-1 flex-shrink-0">
                会议
              </span>
              <span class="text-text-secondary ml-1 flex-shrink-0">27</span>
            </div>
            
            <div class="flex items-center justify-between">
              <span class="text-text-primary truncate flex-1" title="European Conference on Computer Vision">European Conference on Computer Vision</span>
              <span class="text-xs px-1 py-0.5 rounded bg-amber-100 text-amber-700 ml-1 flex-shrink-0">
                会议
              </span>
              <span class="text-text-secondary ml-1 flex-shrink-0">23</span>
            </div>
            
            <div class="flex items-center justify-between">
              <span class="text-text-primary truncate flex-1" title="雷达学报">雷达学报</span>
              <span class="text-xs px-1 py-0.5 rounded bg-blue-100 text-blue-700 ml-1 flex-shrink-0">
                期刊
              </span>
              <span class="text-text-secondary ml-1 flex-shrink-0">21</span>
            </div>
            
          </div>
        </div>
        
      </div>

      <!-- Keywords -->
      
      <div class="bg-bg-card rounded-lg border border-border-color p-4 mb-3">
        <h4 class="text-sm font-medium text-text-primary mb-2">高频关键词</h4>
        <div class="flex flex-wrap gap-1.5">
          
          <span class="px-2 py-0.5 bg-bg-hover rounded text-xs text-text-primary">
            大语言模型 <span class="text-text-secondary">(12)</span>
          </span>
          
          <span class="px-2 py-0.5 bg-bg-hover rounded text-xs text-text-primary">
            综述 <span class="text-text-secondary">(9)</span>
          </span>
          
          <span class="px-2 py-0.5 bg-bg-hover rounded text-xs text-text-primary">
            自监督学习 <span class="text-text-secondary">(9)</span>
          </span>
          
          <span class="px-2 py-0.5 bg-bg-hover rounded text-xs text-text-primary">
            域自适应 <span class="text-text-secondary">(9)</span>
          </span>
          
          <span class="px-2 py-0.5 bg-bg-hover rounded text-xs text-text-primary">
            迁移学习 <span class="text-text-secondary">(7)</span>
          </span>
          
          <span class="px-2 py-0.5 bg-bg-hover rounded text-xs text-text-primary">
            Transformers <span class="text-text-secondary">(6)</span>
          </span>
          
          <span class="px-2 py-0.5 bg-bg-hover rounded text-xs text-text-primary">
            重参数化 <span class="text-text-secondary">(6)</span>
          </span>
          
          <span class="px-2 py-0.5 bg-bg-hover rounded text-xs text-text-primary">
            扩散模型 <span class="text-text-secondary">(6)</span>
          </span>
          
          <span class="px-2 py-0.5 bg-bg-hover rounded text-xs text-text-primary">
            强化学习 <span class="text-text-secondary">(6)</span>
          </span>
          
          <span class="px-2 py-0.5 bg-bg-hover rounded text-xs text-text-primary">
            DeepSeek <span class="text-text-secondary">(6)</span>
          </span>
          
          <span class="px-2 py-0.5 bg-bg-hover rounded text-xs text-text-primary">
            合成孔径雷达 <span class="text-text-secondary">(6)</span>
          </span>
          
          <span class="px-2 py-0.5 bg-bg-hover rounded text-xs text-text-primary">
            旋转目标检测 <span class="text-text-secondary">(5)</span>
          </span>
          
          <span class="px-2 py-0.5 bg-bg-hover rounded text-xs text-text-primary">
            SAR目标识别 <span class="text-text-secondary">(5)</span>
          </span>
          
          <span class="px-2 py-0.5 bg-bg-hover rounded text-xs text-text-primary">
            特征可视化 <span class="text-text-secondary">(5)</span>
          </span>
          
          <span class="px-2 py-0.5 bg-bg-hover rounded text-xs text-text-primary">
            生成对抗网络 <span class="text-text-secondary">(5)</span>
          </span>
          
          <span class="px-2 py-0.5 bg-bg-hover rounded text-xs text-text-primary">
            Training <span class="text-text-secondary">(5)</span>
          </span>
          
          <span class="px-2 py-0.5 bg-bg-hover rounded text-xs text-text-primary">
            Vision Transformers <span class="text-text-secondary">(4)</span>
          </span>
          
          <span class="px-2 py-0.5 bg-bg-hover rounded text-xs text-text-primary">
            基础模型 <span class="text-text-secondary">(4)</span>
          </span>
          
          <span class="px-2 py-0.5 bg-bg-hover rounded text-xs text-text-primary">
            注意力机制 <span class="text-text-secondary">(4)</span>
          </span>
          
          <span class="px-2 py-0.5 bg-bg-hover rounded text-xs text-text-primary">
            HRNet <span class="text-text-secondary">(4)</span>
          </span>
          
        </div>
      </div>
      

      <!-- Generation info -->
      <div class="text-xs text-text-secondary text-right">
        画像生成于 2025-12-06 10:17 Asia/Shanghai
        · kimi-k2-turbo-preview
      </div>
      </div><!-- End of collapsible content -->
    </div>
  </section>
  

  <!-- Chart.js Initialization for Researcher Profile -->
  
  <script>
    document.addEventListener('DOMContentLoaded', function() {
      // Chart.js global defaults for academic style
      Chart.defaults.font.family = "-apple-system, BlinkMacSystemFont, 'Segoe UI', Roboto, 'Helvetica Neue', Arial, sans-serif";
      Chart.defaults.font.size = 11;
      Chart.defaults.color = '#64748b';
      Chart.defaults.animation = false;

      
      // Domains Horizontal Bar Chart
      const domainsCtx = document.getElementById('domainsChart');
      if (domainsCtx) {
        new Chart(domainsCtx, {
          type: 'bar',
          data: {
            labels: ['视觉SLAM', '姿态估计', '卫星导航', '模型压缩', 'Transformer', '概率图模型', '车牌识别', '非线性优化'],
            datasets: [{
              data: [18, 22, 11, 15, 10, 4, 6, 6],
              backgroundColor: 'rgba(37, 99, 235, 0.7)',
              borderColor: 'rgba(37, 99, 235, 1)',
              borderWidth: 1,
              borderRadius: 2,
              barThickness: 16
            }]
          },
          options: {
            indexAxis: 'y',
            responsive: true,
            maintainAspectRatio: false,
            plugins: {
              legend: { display: false },
              tooltip: { enabled: false }
            },
            scales: {
              x: {
                beginAtZero: true,
                grid: { color: 'rgba(226, 232, 240, 0.5)' },
                ticks: { precision: 0 }
              },
              y: {
                grid: { display: false },
                ticks: { font: { size: 10 } }
              }
            }
          }
        });
      }
      

      
      // Quarterly Trends Bar Chart with 3 cycling colors (one per month in quarter)
      const trendsCtx = document.getElementById('trendsChart');
      if (trendsCtx) {
        const trends = [{ q: '2023-Q1', c: 50 }, { q: '2023-Q2', c: 18 }, { q: '2023-Q3', c: 21 }, { q: '2023-Q4', c: 19 }, { q: '2024-Q1', c: 66 }, { q: '2024-Q2', c: 18 }, { q: '2024-Q3', c: 4 }, { q: '2024-Q4', c: 23 }, { q: '2025-Q1', c: 81 }, { q: '2025-Q2', c: 34 }, { q: '2025-Q3', c: 10 }, { q: '2025-Q4', c: 23 }];
        // Color palette: 3 colors cycling for months within quarters
        const monthColors = [
          { bg: 'rgba(59, 130, 246, 0.7)', border: 'rgba(59, 130, 246, 1)' },   // Blue
          { bg: 'rgba(16, 185, 129, 0.7)', border: 'rgba(16, 185, 129, 1)' },   // Green
          { bg: 'rgba(245, 158, 11, 0.7)', border: 'rgba(245, 158, 11, 1)' }    // Amber
        ];
        const bgColors = trends.map((t, i) => monthColors[i % 3].bg);
        const borderColors = trends.map((t, i) => monthColors[i % 3].border);
        new Chart(trendsCtx, {
          type: 'bar',
          data: {
            labels: trends.map(t => t.q.replace('-', '\n')),
            datasets: [{
              data: trends.map(t => t.c),
              backgroundColor: bgColors,
              borderColor: borderColors,
              borderWidth: 1,
              borderRadius: 2,
              barThickness: 20
            }]
          },
          options: {
            responsive: true,
            maintainAspectRatio: false,
            plugins: {
              legend: { display: false },
              tooltip: { enabled: false }
            },
            scales: {
              x: {
                grid: { display: false },
                ticks: { font: { size: 9 }, maxRotation: 0 }
              },
              y: {
                beginAtZero: true,
                grid: { color: 'rgba(226, 232, 240, 0.5)' },
                ticks: { precision: 0 }
              }
            }
          }
        });
      }
      

      
      // Year Distribution Line Chart
      const yearDistCtx = document.getElementById('yearDistChart');
      if (yearDistCtx) {
        const yearData = [{ year: 2006, count: 8 }, { year: 2007, count: 3 }, { year: 2008, count: 2 }, { year: 2009, count: 2 }, { year: 2010, count: 9 }, { year: 2011, count: 10 }, { year: 2012, count: 3 }, { year: 2013, count: 12 }, { year: 2014, count: 14 }, { year: 2015, count: 18 }, { year: 2016, count: 15 }, { year: 2017, count: 40 }, { year: 2018, count: 58 }, { year: 2019, count: 53 }, { year: 2020, count: 66 }, { year: 2021, count: 84 }, { year: 2022, count: 110 }, { year: 2023, count: 108 }, { year: 2024, count: 111 }, { year: 2025, count: 148 }];
        new Chart(yearDistCtx, {
          type: 'line',
          data: {
            labels: yearData.map(d => d.year),
            datasets: [{
              data: yearData.map(d => d.count),
              borderColor: 'rgba(37, 99, 235, 1)',
              backgroundColor: 'rgba(37, 99, 235, 0.1)',
              borderWidth: 2,
              fill: true,
              tension: 0.3,
              pointRadius: 2,
              pointHoverRadius: 4
            }]
          },
          options: {
            responsive: true,
            maintainAspectRatio: false,
            plugins: {
              legend: { display: false },
              tooltip: {
                callbacks: {
                  label: function(context) {
                    return context.parsed.y + ' 篇';
                  }
                }
              }
            },
            scales: {
              x: {
                grid: { display: false },
                ticks: {
                  font: { size: 9 },
                  maxRotation: 45,
                  callback: function(value, index) {
                    // Show every 5th year label to avoid crowding
                    const year = this.getLabelForValue(value);
                    return year % 5 === 0 ? year : '';
                  }
                }
              },
              y: {
                beginAtZero: true,
                grid: { color: 'rgba(226, 232, 240, 0.5)' },
                ticks: { precision: 0 }
              }
            }
          }
        });
      }
      
    });
  </script>
  

  <!-- Overall Summaries Section -->
  
  <section class="py-5 md:py-8 border-b border-border-color">
    <div class="content-container">
      <!-- Collapsible Header -->
      <button id="btn-overall-summaries" onclick="toggleSection('overall-summaries')"
              class="w-full text-left text-lg md:text-xl font-bold text-text-primary mb-3 md:mb-4 flex items-center justify-between group hover:text-accent transition-colors">
        <span class="flex items-center gap-2 flex-1 min-w-0">
          <svg class="w-5 h-5 flex-shrink-0 text-accent" fill="currentColor" viewBox="0 0 24 24">
            <path d="M12 2a2 2 0 012 2c0 .74-.4 1.39-1 1.73V7h1a7 7 0 017 7h1a1 1 0 011 1v3a1 1 0 01-1 1h-1v1a2 2 0 01-2 2H5a2 2 0 01-2-2v-1H2a1 1 0 01-1-1v-3a1 1 0 011-1h1a7 7 0 017-7h1V5.73c-.6-.34-1-.99-1-1.73a2 2 0 012-2z"/>
          </svg>
          <span class="truncate">本期研究趋势概览</span>
        </span>
        <span class="flex items-center gap-1 md:gap-2 flex-shrink-0 ml-2">
          <span class="hidden md:inline text-sm font-normal text-text-secondary group-hover:text-accent" id="hint-overall-summaries">点击收起</span>
          <svg class="w-5 h-5 md:w-4 md:h-4 text-text-secondary group-hover:text-accent transform transition-transform rotate-180" id="icon-overall-summaries" fill="none" stroke="currentColor" viewBox="0 0 24 24">
            <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
          </svg>
        </span>
      </button>

      <!-- Collapsible Content -->
      <div id="overall-summaries" class="section-expand expanded">
      <div class="space-y-4 md:space-y-6">
        
        <div class="bg-bg-card rounded-lg border border-border-color p-4 md:p-5">
          <h3 class="text-base font-semibold text-text-primary mb-3 flex items-center gap-2">
            <svg class="w-4 h-4 text-accent" fill="currentColor" viewBox="0 0 20 20">
              <path d="M9.049 2.927c.3-.921 1.603-.921 1.902 0l1.07 3.292a1 1 0 00.95.69h3.462c.969 0 1.371 1.24.588 1.81l-2.8 2.034a1 1 0 00-.364 1.118l1.07 3.292c.3.921-.755 1.688-1.54 1.118l-2.8-2.034a1 1 0 00-1.175 0l-2.8 2.034c-.784.57-1.838-.197-1.539-1.118l1.07-3.292a1 1 0 00-.364-1.118L2.98 8.72c-.783-.57-.38-1.81.588-1.81h3.461a1 1 0 00.951-.69l1.07-3.292z"/>
            </svg>
            精选推荐总结
          </h3>
          <div class="text-sm text-text-primary leading-relaxed space-y-2">
            <p>本期推荐涵盖了2篇关于船舶检测的论文、2篇关于遥感视觉-语言理解的论文和1篇关于雷达目标检测的论文。</p>
            
            <p><strong class="text-accent">船舶检测</strong>：《RSDB-Net》提出旋转敏感双分支网络，在光学遥感影像中强化局部特征以应对多尺度与任意方向船舶；SAR-D-FINE通过上下文感知检测头抑制相干斑与沿岸杂波，实现SAR图像中微小密集船舶的高精度定位。</p>
            
            <p><strong class="text-accent">遥感视觉语言</strong>：《SkyMoE》以混合专家架构扩展视觉-语言基础模型，显著提升复杂地理空间场景下的图文对齐与解释能力；《GeoViS》引入地理奖励机制，优化多模态大模型的视觉定位，使文本查询与遥感影像区域实现细粒度匹配。</p>
            
            <p><strong class="text-accent">雷达目标检测</strong>：《Refined Multi-modal Feature Learning Framework》融合雷达回波的时间与时间-频率特征，通过多模态精细建模弥补单一幅度信息不足，实现海上目标稳健检测。</p>
            
          </div>
        </div>
        

        
        <div class="bg-bg-card rounded-lg border border-border-color p-4 md:p-5">
          <h3 class="text-base font-semibold text-text-primary mb-3 flex items-center gap-2">
            <svg class="w-4 h-4 text-text-secondary" fill="none" stroke="currentColor" viewBox="0 0 24 24">
              <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M9 19v-6a2 2 0 00-2-2H5a2 2 0 00-2 2v6a2 2 0 002 2h2a2 2 0 002-2zm0 0V9a2 2 0 012-2h2a2 2 0 012 2v10m-6 0a2 2 0 002 2h2a2 2 0 002-2m0 0V5a2 2 0 012-2h2a2 2 0 012 2v14a2 2 0 01-2 2h-2a2 2 0 01-2-2z"/>
            </svg>
            相似度推荐总结
          </h3>
          <div class="text-sm text-text-primary leading-relaxed space-y-2">
            <p>本期推荐涵盖了9篇关于多模态3D感知的论文、6篇关于跨模态图像生成的论文、5篇关于遥感图像翻译的论文、4篇关于小目标检测的论文、3篇关于医学图像分割的论文、2篇关于3D重建与渲染的论文以及1篇关于上下文压缩的论文。</p>
            
            <p><strong class="text-text-secondary">多模态3D感知</strong>：该主题聚焦LiDAR-相机融合在BEV空间的3D目标检测，代表作《AttBEV》在BEVFusion中引入CBAM注意力，《BEVDilation》提出LiDAR中心的扩张融合，《Enhanced Spatiotemporal Consistency》利用图像预训练提升LiDAR表征，并有多篇工作探索无监督预训练与跨模态对齐。</p>
            
            <p><strong class="text-text-secondary">跨模态图像生成</strong>：研究利用生成模型实现SAR-光学、ISAR-文本等异构图像互译，《Generative models for SAR–optical image translation》系统综述了该领域，《Extrapolate azimuth angles》以基础模型生成ISAR图像，其余工作探索红外-可见光及风格迁移。</p>
            
            <p><strong class="text-text-secondary">遥感图像翻译</strong>：关注光学与SAR、红外等遥感模态间的像素级转换，强调对地观测应用中的可持续性与资源管理需求，方法涵盖GAN、扩散模型及无监督对齐。</p>
            
            <p><strong class="text-text-secondary">小目标检测</strong>：针对红外小目标与海空监视场景，《WMRNet》提出小波Mamba可逆结构，《Cross-modal Guiding Attention》设计RGBT协同跟踪，其余论文探索频域增强与多尺度特征融合以提升微弱目标信噪比。</p>
            
            <p><strong class="text-text-secondary">医学图像分割</strong>：致力于降低Transformer在3D医学图像中的计算与标注负担，《Harnessing Lightweight Transformer》通过上下文协同增强实现高效分割，另两篇工作引入稀疏注意力与自监督预训练。</p>
            
            <p><strong class="text-text-secondary">3D重建与渲染</strong>：探索无需相机位姿的新视角合成，《AnySplat》提出前馈3D高斯溅射从未标定图像集直接重建，另一篇工作结合神经辐射场与LiDAR先验提升室外场景精度。</p>
            
            <p><strong class="text-text-secondary">上下文压缩</strong>：《Optical Context Compression Is Just (Bad) Autoencoding》指出视觉令牌压缩本质上是低质量自编码，质疑其在OCR等任务中的长期有效性。</p>
            
          </div>
        </div>
        
      </div>
      </div><!-- End of collapsible content -->
    </div>
  </section>
  

  <!-- Interest-based Recommendations Section -->
  
  <section class="py-5 md:py-8 border-b border-border-color">
    <div class="content-container">
      <!-- Collapsible Header -->
      <div class="mb-3 md:mb-4">
        <button id="btn-interest-works" onclick="toggleSection('interest-works')"
                class="w-full text-left text-lg md:text-xl font-bold text-text-primary flex items-center justify-between group hover:text-accent transition-colors">
          <span class="flex items-center gap-2 flex-1 min-w-0">
            <svg class="w-5 h-5 flex-shrink-0 text-accent" fill="currentColor" viewBox="0 0 20 20">
              <path d="M9.049 2.927c.3-.921 1.603-.921 1.902 0l1.07 3.292a1 1 0 00.95.69h3.462c.969 0 1.371 1.24.588 1.81l-2.8 2.034a1 1 0 00-.364 1.118l1.07 3.292c.3.921-.755 1.688-1.54 1.118l-2.8-2.034a1 1 0 00-1.175 0l-2.8 2.034c-.784.57-1.838-.197-1.539-1.118l1.07-3.292a1 1 0 00-.364-1.118L2.98 8.72c-.783-.57-.38-1.81.588-1.81h3.461a1 1 0 00.951-.69l1.07-3.292z"/>
            </svg>
            <span class="truncate">精选推荐</span>
          </span>
          <span class="flex items-center gap-1 md:gap-2 flex-shrink-0 ml-2">
            <span class="hidden md:inline text-xs font-normal text-text-secondary">基于研究兴趣匹配，共 5 篇</span>
            <span class="hidden md:inline text-sm font-normal text-text-secondary group-hover:text-accent" id="hint-interest-works">点击收起</span>
            <svg class="w-5 h-5 md:w-4 md:h-4 text-text-secondary group-hover:text-accent transform transition-transform rotate-180" id="icon-interest-works" fill="none" stroke="currentColor" viewBox="0 0 24 24">
              <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
            </svg>
          </span>
        </button>
        <p class="text-xs text-text-secondary mt-1 ml-7 md:hidden">基于研究兴趣匹配，共 5 篇</p>
      </div>

      <!-- Collapsible Content -->
      <div id="interest-works" class="section-expand expanded">
      <div class="space-y-4 md:space-y-5">
        
        <article class="bg-bg-card rounded-lg border border-border-color overflow-hidden hover:border-accent/50 transition-colors">
          <div class="p-4 md:p-5">
            <!-- Title and metadata -->
            <div class="mb-3">
              <!-- Tags row -->
              <div class="flex items-center gap-2 mb-2 flex-wrap">
                <span class="inline-flex items-center px-2.5 py-0.5 rounded text-xs font-medium bg-accent/10 text-accent border border-accent/30">
                  匹配度 63%
                </span>
                <span class="text-xs text-text-secondary">crossref</span>
                <!-- Date: visible on mobile, hidden on md+ -->
                <span class="text-xs text-text-secondary md:hidden">· 2025-12-04</span>
              </div>
              <!-- Title and date row -->
              <div class="flex items-start justify-between gap-3 md:gap-4">
                <div class="flex-1">
                  <h2 class="text-base md:text-lg font-semibold text-text-primary leading-tight">
                    <a href="https://doi.org/10.3390/rs17233925" target="_blank" rel="noopener" class="hover:text-accent transition-colors">
                      RSDB-Net: A Novel Rotation-Sensitive Dual-Branch Network with Enhanced Local Features for Remote Sensing Ship Detection
                    </a>
                  </h2>
                  
                  <p class="text-sm text-text-secondary mt-0.5 leading-snug">RSDB-Net：一种面向遥感船舶检测的旋转敏感双分支网络，具备增强局部特征</p>
                  
                </div>
                <!-- Date: hidden on mobile, visible on md+ -->
                <div class="hidden md:block text-right text-sm text-text-secondary flex-shrink-0">
                  <div class="font-medium">2025-12-04</div>
                </div>
              </div>
              <!-- Venue: below title on mobile -->
              <div class="mt-1 text-xs text-text-secondary" title="Remote Sensing">
                Remote Sensing
                
                  <span class="ml-1 text-blue-600">(IF: 4.1)</span>
                
              </div>
            </div>

            <!-- Authors -->
            <p class="text-sm text-text-secondary mb-3">
              Danshu Zhou，Yushan Xiong，Shuangming Yu，Peng Feng，Jian Liu 等
            </p>

            <!-- DOI -->
            <div class="flex flex-wrap gap-x-4 gap-y-1 text-xs text-text-secondary mb-4">
              
              <span class="flex items-center">
                <span class="font-medium mr-1">DOI:</span>
                <a href="https://doi.org/10.3390/rs17233925" target="_blank" rel="noopener" class="text-accent hover:text-accent-hover hover:underline">10.3390/rs17233925</a>
              </span>
              
            </div>

            <!-- Abstract (collapsible) -->
            
            <div class="bg-bg-hover/50 rounded-lg p-4 mb-4 border border-border-color">
              <button id="btn-interest-abstract-1" onclick="toggleSection('interest-abstract-1')"
                      class="w-full text-left text-sm font-medium text-text-primary flex items-center justify-between">
                <span class="flex items-center">
                  <svg class="w-4 h-4 mr-1.5 text-text-secondary" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                    <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M9 12h6m-6 4h6m2 5H7a2 2 0 01-2-2V5a2 2 0 012-2h5.586a1 1 0 01.707.293l5.414 5.414a1 1 0 01.293.707V19a2 2 0 01-2 2z"/>
                  </svg>
                  原文摘要
                </span>
                <svg class="w-4 h-4 text-text-secondary transform transition-transform" id="icon-interest-abstract-1" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                  <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
                </svg>
              </button>
              <div id="interest-abstract-1" class="section-expand collapsed">
                <p class="text-sm text-text-secondary mt-3 leading-relaxed">Ship detection in remote sensing imagery is hindered by cluttered backgrounds, large variations in scale, and random orientations, limiting the performance of detectors designed for natural images. We propose RSDB-Net, a Rotation-Sensitive Dual-Branch Detection Network that introduces innovations in feature extraction, fusion, and detection. The Swin Transformer–CNN Backbone (STCBackbone) combines a Swin Transformer for global semantics with a CNN branch for local spatial detail, while the Feature Conversion and Coupling Module (FCCM) aligns and fuses heterogeneous features to handle multi-scale objects, and a Rotation-sensitive Cross-branch Fusion Head (RCFHead) enables bidirectional interaction between classification and localization, improving detection of randomly oriented targets. Additionally, an enhanced Feature Pyramid Network (eFPN) with learnable transposed convolutions restores semantic information while maintaining spatial alignment. Experiments on DOTA-v1.0 and HRSC2016 show that RSDB-Net performs better than the state of the art (SOTA), with mAP-ship values of 89.13% and 90.10% (+5.54% and +44.40% over the baseline, respectively), and reaches 72 FPS on an RTX 3090. RSDB-Net also demonstrates strong generalization and scalability, providing an effective solution for rotation-aware ship detection.</p>
              </div>
            </div>
            

            <!-- AI Summary -->
            
            <div class="bg-accent/10 rounded-lg p-4 mb-4 border border-accent/30">
              <h3 class="text-sm font-semibold text-accent mb-3 flex items-center">
                <svg class="w-4 h-4 mr-1.5" fill="currentColor" viewBox="0 0 24 24">
                  <path d="M12 2a2 2 0 012 2c0 .74-.4 1.39-1 1.73V7h1a7 7 0 017 7h1a1 1 0 011 1v3a1 1 0 01-1 1h-1v1a2 2 0 01-2 2H5a2 2 0 01-2-2v-1H2a1 1 0 01-1-1v-3a1 1 0 011-1h1a7 7 0 017-7h1V5.73c-.6-.34-1-.99-1-1.73a2 2 0 012-2z"/>
                </svg>
                AI 总结
              </h3>
              <div class="space-y-2 text-sm text-text-primary">
                <p><span class="font-medium text-accent">研究问题：</span>遥感影像中背景杂乱、尺度变化大且方向任意的舰船检测精度不足。</p>
                <p><span class="font-medium text-accent">研究方法：</span>提出RSDB-Net，用Swin-CNN双分支提取特征，FCCM对齐融合，RCFHead增强旋转敏感检测。</p>
                <p><span class="font-medium text-accent">主要发现：</span>在DOTA-v1.0与HRSC2016上mAP-ship分别达89.13%和90.10%，较基线提升5.54%与44.40%，RTX3090下72FPS。</p>
                <p><span class="font-medium text-accent">创新点：</span>首次将旋转敏感跨分支融合头与可学转置卷积eFPN结合，实现全局-局部特征协同的任意方向舰船检测。</p>
                
                <p><span class="font-medium text-accent">相关性：</span>为遥感舰船监测提供高精度实时方案，其旋转敏感设计可推广至任意方向目标检测任务。</p>
                
              </div>

              <div class="mt-3 pt-3 border-t border-accent/30">
                <button id="btn-interest-detail-1" onclick="toggleSection('interest-detail-1')"
                        class="text-xs text-accent hover:text-accent-hover font-medium flex items-center">
                  <svg class="w-3 h-3 mr-1" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                    <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
                  </svg>
                  查看详细分析
                </button>
                <div id="interest-detail-1" class="section-expand collapsed mt-3">
                  <div class="bg-bg-card rounded-lg p-4 text-sm text-text-primary space-y-3 border border-border-color">
                    <p><span class="font-medium text-text-primary">研究背景：</span>遥感舰船检测长期受限于复杂海面背景、目标尺度跨度大以及任意朝向，传统面向自然图像的检测器难以直接迁移。现有方法在全局语义捕获与局部细节保持、跨尺度特征对齐和旋转鲁棒性方面仍存在明显缺口，因此亟需一种面向旋转目标的专用框架。</p>
                    <p><span class="font-medium text-text-primary">方法详情：</span>作者提出 RSDB-Net，核心由三部分组成：1) STCBackbone 并行耦合 Swin Transformer 全局分支与 CNN 局部分支，通过跨层连接互补语义与纹理；2) FCCM 采用通道-空间双重对齐策略，将异构特征映射到统一嵌入空间，并以可学习权重实现多尺度融合；3) RCFHead 在检测头内部引入旋转敏感交互单元，使分类与回归分支共享旋转先验，辅以 eFPN 的可学习转置卷积恢复下采样丢失的语义细节，整体形成端到端旋转检测框架。</p>
                    <p><span class="font-medium text-text-primary">研究结果：</span>在 DOTA-v1.0 和 HRSC2016 舰船子集上，RSDB-Net 分别取得 89.13% 和 90.10% mAP-ship，比基线提升 5.54% 和 44.40%，同时保持 72 FPS 实时速度。消融实验显示 STCBackbone 与 RCFHead 对朝向剧烈目标的召回提升最显著，可视化表明 eFPN 有效抑制了背景虚警。</p>
                    <p><span class="font-medium text-text-primary">局限性：</span>论文仅在两个公开数据集验证，缺乏对更复杂港口环境、SAR 图像及小像素目标（&lt;16 px）的深入评估；双分支结构带来 30% 的参数量增量，对星载或嵌入式平台的部署友好性未讨论；此外，旋转框标注成本高昂，方法对弱监督或自监督场景的适应性尚不明确。</p>
                    
                    <p><span class="font-medium text-text-primary">未来方向：</span>后续可探索轻量化双分支压缩与知识蒸馏，实现星载实时推理；同时结合主动学习与合成数据，降低旋转框标注依赖。</p>
                    
                    <p class="text-accent"><span class="font-medium">研究相关性：</span>若研究者关注旋转目标检测、遥感小目标或 Transformer-CNN 混合架构设计，本文提供的异构特征耦合与旋转敏感头机制可直接借鉴，并作为复杂场景检测的新基线。</p>
                  </div>
                </div>
              </div>
            </div>
            

            <!-- Score details -->
            <div class="pt-4 border-t border-border-color">
              <div class="flex flex-wrap gap-3 text-xs">
                <span class="px-2 py-1 bg-bg-hover rounded text-text-secondary">相似度 0.67</span>
                <span class="px-2 py-1 bg-bg-hover rounded text-text-secondary">
                  期刊影响力 0.51
                  
                    <span class="ml-1 text-blue-600">(IF: 4.1)</span>
                  
                </span>
              </div>
            </div>
          </div>
        </article>
        
        <article class="bg-bg-card rounded-lg border border-border-color overflow-hidden hover:border-accent/50 transition-colors">
          <div class="p-4 md:p-5">
            <!-- Title and metadata -->
            <div class="mb-3">
              <!-- Tags row -->
              <div class="flex items-center gap-2 mb-2 flex-wrap">
                <span class="inline-flex items-center px-2.5 py-0.5 rounded text-xs font-medium bg-accent/10 text-accent border border-accent/30">
                  匹配度 62%
                </span>
                <span class="text-xs text-text-secondary">crossref</span>
                <!-- Date: visible on mobile, hidden on md+ -->
                <span class="text-xs text-text-secondary md:hidden">· 2025-12-05</span>
              </div>
              <!-- Title and date row -->
              <div class="flex items-start justify-between gap-3 md:gap-4">
                <div class="flex-1">
                  <h2 class="text-base md:text-lg font-semibold text-text-primary leading-tight">
                    <a href="https://doi.org/10.1109/lgrs.2025.3640683" target="_blank" rel="noopener" class="hover:text-accent transition-colors">
                      SAR-D-FINE: A Context-Aware Detector for Small and Densely Packed Ship Detection in SAR Imagery
                    </a>
                  </h2>
                  
                  <p class="text-sm text-text-secondary mt-0.5 leading-snug">SAR-D-FINE：面向SAR图像中小目标与密集排列船舶检测的上下文感知检测器</p>
                  
                </div>
                <!-- Date: hidden on mobile, visible on md+ -->
                <div class="hidden md:block text-right text-sm text-text-secondary flex-shrink-0">
                  <div class="font-medium">2025-12-05</div>
                </div>
              </div>
              <!-- Venue: below title on mobile -->
              <div class="mt-1 text-xs text-text-secondary" title="IEEE Geoscience and Remote Sensing Letters">
                IEEE Geoscience and Remote Sensing Letters
                
                  <span class="ml-1 text-blue-600">(IF: 4.4)</span>
                
              </div>
            </div>

            <!-- Authors -->
            <p class="text-sm text-text-secondary mb-3">
              Xiaobing Fan，Bowen Xing，Xingchen Wang，Hongdan Liu，Chuanxu Yan 等
            </p>

            <!-- DOI -->
            <div class="flex flex-wrap gap-x-4 gap-y-1 text-xs text-text-secondary mb-4">
              
              <span class="flex items-center">
                <span class="font-medium mr-1">DOI:</span>
                <a href="https://doi.org/10.1109/lgrs.2025.3640683" target="_blank" rel="noopener" class="text-accent hover:text-accent-hover hover:underline">10.1109/lgrs.2025.3640683</a>
              </span>
              
            </div>

            <!-- Abstract (collapsible) -->
            
            <div class="bg-bg-hover/50 rounded-lg p-4 mb-4 border border-border-color">
              <button id="btn-interest-abstract-2" onclick="toggleSection('interest-abstract-2')"
                      class="w-full text-left text-sm font-medium text-text-primary flex items-center justify-between">
                <span class="flex items-center">
                  <svg class="w-4 h-4 mr-1.5 text-text-secondary" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                    <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M9 12h6m-6 4h6m2 5H7a2 2 0 01-2-2V5a2 2 0 012-2h5.586a1 1 0 01.707.293l5.414 5.414a1 1 0 01.293.707V19a2 2 0 01-2 2z"/>
                  </svg>
                  原文摘要
                </span>
                <svg class="w-4 h-4 text-text-secondary transform transition-transform" id="icon-interest-abstract-2" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                  <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
                </svg>
              </button>
              <div id="interest-abstract-2" class="section-expand collapsed">
                <p class="text-sm text-text-secondary mt-3 leading-relaxed">Synthetic Aperture Radar (SAR) provides all-weather imaging, yet small-scale, densely clustered ships remain difficult to detect because coherent speckle noise and coastal clutter often mask target echoes. Current detectors, derived mainly from optical imaging methods, fail to extract weak signatures from minute vessels and to separate closely spaced targets from background clutter, leading to frequent missed detections and elevated false-alarm rates. This paper presents SAR-D-FINE, a context-aware detection framework tailored for SAR imagery. Extending the D-FINE architecture, we design a hybrid backbone with a StarStage module that strengthens nonlinear feature extraction under heavy noise. A Focusing Diffusion Encoder, integrating a Multi-Kernel Aggregation Module (MKAM) and a parameter-free Shuffle-and-Shift Upsampling (SSU) unit, is adopted to aggregate multi-scale features without sacrificing fine spatial details. Experimental results on SSDD and HRSID indicate that SAR-D-FINE surpasses existing methods, including dedicated SAR detectors such as YOLO-SARSI and SW-Net, achieving AP improvements of 2.0% and 1.8% over the baseline, respectively. The results confirm the advantages of the proposed model, particularly for detecting small, densely distributed vessels.</p>
              </div>
            </div>
            

            <!-- AI Summary -->
            
            <div class="bg-accent/10 rounded-lg p-4 mb-4 border border-accent/30">
              <h3 class="text-sm font-semibold text-accent mb-3 flex items-center">
                <svg class="w-4 h-4 mr-1.5" fill="currentColor" viewBox="0 0 24 24">
                  <path d="M12 2a2 2 0 012 2c0 .74-.4 1.39-1 1.73V7h1a7 7 0 017 7h1a1 1 0 011 1v3a1 1 0 01-1 1h-1v1a2 2 0 01-2 2H5a2 2 0 01-2-2v-1H2a1 1 0 01-1-1v-3a1 1 0 011-1h1a7 7 0 017-7h1V5.73c-.6-.34-1-.99-1-1.73a2 2 0 012-2z"/>
                </svg>
                AI 总结
              </h3>
              <div class="space-y-2 text-sm text-text-primary">
                <p><span class="font-medium text-accent">研究问题：</span>解决SAR图像中小尺寸、密集排列船只因相干斑噪声与海岸杂波而难以检测的问题。</p>
                <p><span class="font-medium text-accent">研究方法：</span>提出SAR-D-FINE框架，采用StarStage混合骨干、多核聚合模块与无参Shuffle-Shift上采样增强特征提取。</p>
                <p><span class="font-medium text-accent">主要发现：</span>在SSDD和HRSID数据集上，AP分别提升2.0%与1.8%，优于YOLO-SARSI、SW-Net等专用SAR检测器。</p>
                <p><span class="font-medium text-accent">创新点：</span>首次将上下文感知D-FINE结构引入SAR，设计抗噪StarStage与保细节扩散编码，实现小密船只精准检测。</p>
                
                <p><span class="font-medium text-accent">相关性：</span>为海洋监视、海事安全提供高鲁棒SAR检测工具，推动小目标遥感算法向真实复杂场景落地。</p>
                
              </div>

              <div class="mt-3 pt-3 border-t border-accent/30">
                <button id="btn-interest-detail-2" onclick="toggleSection('interest-detail-2')"
                        class="text-xs text-accent hover:text-accent-hover font-medium flex items-center">
                  <svg class="w-3 h-3 mr-1" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                    <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
                  </svg>
                  查看详细分析
                </button>
                <div id="interest-detail-2" class="section-expand collapsed mt-3">
                  <div class="bg-bg-card rounded-lg p-4 text-sm text-text-primary space-y-3 border border-border-color">
                    <p><span class="font-medium text-text-primary">研究背景：</span>合成孔径雷达(SAR)可全天时成像，但相干斑噪声与海岸杂波常淹没微小船只回波，导致小尺度密集船队检测困难。现有方法多移植自光学图像检测器，难以在强杂波中提取弱目标特征，也无法区分紧邻目标，造成大量漏检与虚警。</p>
                    <p><span class="font-medium text-text-primary">方法详情：</span>作者提出SAR-D-FINE框架，在D-FINE基础上引入StarStage混合骨干，通过星型级联结构增强重噪声下的非线性特征提取。Focusing Diffusion Encoder整合多核聚合模块(MKAM)与无参Shuffle-and-Shift Upsampling(SSU)，在不损失空间细节的前提下融合多尺度特征。整体网络采用上下文感知设计，专门针对SAR图像中小且密集排布的船只目标。</p>
                    <p><span class="font-medium text-text-primary">研究结果：</span>在SSDD与HRSID两个公开SAR船检数据集上，SAR-D-FINE比现有最佳方法(AP)分别提升2.0%与1.8%，优于YOLO-SARSI、SW-Net等专用SAR检测器。实验表明，该模型显著降低漏检率与虚警率，尤其对港口内密集小艇与近岸杂波场景表现突出。结果验证了StarStage与MKAM-SSU组合在抑制斑噪同时保持高空间分辨率的有效性。</p>
                    <p><span class="font-medium text-text-primary">局限性：</span>论文仅在两个公开数据集验证，缺乏不同传感器、分辨率与极化方式下的泛化评估。StarStage引入额外分支，参数量与推理时延未与轻量化需求权衡，可能限制实时应用。对极端近岸强杂波、目标尺寸小于分辨率单元的情况，仍有少量虚警。</p>
                    
                    <p><span class="font-medium text-text-primary">未来方向：</span>后续可探索跨传感器迁移与自监督预训练，以提升在多种SAR成像参数下的鲁棒性；并结合知识蒸馏或神经架构搜索，实现实时轻量化部署。</p>
                    
                    <p class="text-accent"><span class="font-medium">研究相关性：</span>该研究为SAR小目标检测提供可复现的上下文感知框架，其StarStage与MKAM-SSU模块可直接嵌入其他遥感检测网络，对从事SAR图像解译、海事监视或弱小目标增强的研究者具有借鉴价值。</p>
                  </div>
                </div>
              </div>
            </div>
            

            <!-- Score details -->
            <div class="pt-4 border-t border-border-color">
              <div class="flex flex-wrap gap-3 text-xs">
                <span class="px-2 py-1 bg-bg-hover rounded text-text-secondary">相似度 0.68</span>
                <span class="px-2 py-1 bg-bg-hover rounded text-text-secondary">
                  期刊影响力 0.52
                  
                    <span class="ml-1 text-blue-600">(IF: 4.4)</span>
                  
                </span>
              </div>
            </div>
          </div>
        </article>
        
        <article class="bg-bg-card rounded-lg border border-border-color overflow-hidden hover:border-accent/50 transition-colors">
          <div class="p-4 md:p-5">
            <!-- Title and metadata -->
            <div class="mb-3">
              <!-- Tags row -->
              <div class="flex items-center gap-2 mb-2 flex-wrap">
                <span class="inline-flex items-center px-2.5 py-0.5 rounded text-xs font-medium bg-accent/10 text-accent border border-accent/30">
                  匹配度 54%
                </span>
                <span class="text-xs text-text-secondary">arxiv</span>
                <!-- Date: visible on mobile, hidden on md+ -->
                <span class="text-xs text-text-secondary md:hidden">· 2025-12-02</span>
              </div>
              <!-- Title and date row -->
              <div class="flex items-start justify-between gap-3 md:gap-4">
                <div class="flex-1">
                  <h2 class="text-base md:text-lg font-semibold text-text-primary leading-tight">
                    <a href="https://arxiv.org/abs/2512.02517v1" target="_blank" rel="noopener" class="hover:text-accent transition-colors">
                      SkyMoE: A Vision-Language Foundation Model for Enhancing Geospatial Interpretation with Mixture of Experts
                    </a>
                  </h2>
                  
                  <p class="text-sm text-text-secondary mt-0.5 leading-snug">SkyMoE：一种利用混合专家增强地理空间理解的视觉-语言基础模型</p>
                  
                </div>
                <!-- Date: hidden on mobile, visible on md+ -->
                <div class="hidden md:block text-right text-sm text-text-secondary flex-shrink-0">
                  <div class="font-medium">2025-12-02</div>
                </div>
              </div>
              <!-- Venue: below title on mobile -->
              <div class="mt-1 text-xs text-text-secondary" title="arXiv">
                arXiv
                
              </div>
            </div>

            <!-- Authors -->
            <p class="text-sm text-text-secondary mb-3">
              Jiaqi Liu，Ronghao Fu，Lang Sun，Haoran Liu，Xiao Yang 等
            </p>

            <!-- DOI -->
            <div class="flex flex-wrap gap-x-4 gap-y-1 text-xs text-text-secondary mb-4">
              
              <span class="flex items-center">
                <span class="font-medium mr-1">ID:</span>
                <span>http://arxiv.org/abs/2512.02517v1</span>
              </span>
              
            </div>

            <!-- Abstract (collapsible) -->
            
            <div class="bg-bg-hover/50 rounded-lg p-4 mb-4 border border-border-color">
              <button id="btn-interest-abstract-3" onclick="toggleSection('interest-abstract-3')"
                      class="w-full text-left text-sm font-medium text-text-primary flex items-center justify-between">
                <span class="flex items-center">
                  <svg class="w-4 h-4 mr-1.5 text-text-secondary" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                    <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M9 12h6m-6 4h6m2 5H7a2 2 0 01-2-2V5a2 2 0 012-2h5.586a1 1 0 01.707.293l5.414 5.414a1 1 0 01.293.707V19a2 2 0 01-2 2z"/>
                  </svg>
                  原文摘要
                </span>
                <svg class="w-4 h-4 text-text-secondary transform transition-transform" id="icon-interest-abstract-3" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                  <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
                </svg>
              </button>
              <div id="interest-abstract-3" class="section-expand collapsed">
                <p class="text-sm text-text-secondary mt-3 leading-relaxed">The emergence of large vision-language models (VLMs) has significantly enhanced the efficiency and flexibility of geospatial interpretation. However, general-purpose VLMs remain suboptimal for remote sensing (RS) tasks. Existing geospatial VLMs typically adopt a unified modeling strategy and struggle to differentiate between task types and interpretation granularities, limiting their ability to balance local detail perception and global contextual understanding. In this paper, we present SkyMoE, a Mixture-of-Experts (MoE) vision-language model tailored for multimodal, multi-task RS interpretation. SkyMoE employs an adaptive router that generates task- and granularity-aware routing instructions, enabling specialized large language model experts to handle diverse sub-tasks. To further promote expert decoupling and granularity sensitivity, we introduce a context-disentangled augmentation strategy that creates contrastive pairs between local and global features, guiding experts toward level-specific representation learning. We also construct MGRS-Bench, a comprehensive benchmark covering multiple RS interpretation tasks and granularity levels, to evaluate generalization in complex scenarios. Extensive experiments on 21 public datasets demonstrate that SkyMoE achieves state-of-the-art performance across tasks, validating its adaptability, scalability, and superior multi-granularity understanding in remote sensing.</p>
              </div>
            </div>
            

            <!-- AI Summary -->
            
            <div class="bg-accent/10 rounded-lg p-4 mb-4 border border-accent/30">
              <h3 class="text-sm font-semibold text-accent mb-3 flex items-center">
                <svg class="w-4 h-4 mr-1.5" fill="currentColor" viewBox="0 0 24 24">
                  <path d="M12 2a2 2 0 012 2c0 .74-.4 1.39-1 1.73V7h1a7 7 0 017 7h1a1 1 0 011 1v3a1 1 0 01-1 1h-1v1a2 2 0 01-2 2H5a2 2 0 01-2-2v-1H2a1 1 0 01-1-1v-3a1 1 0 011-1h1a7 7 0 017-7h1V5.73c-.6-.34-1-.99-1-1.73a2 2 0 012-2z"/>
                </svg>
                AI 总结
              </h3>
              <div class="space-y-2 text-sm text-text-primary">
                <p><span class="font-medium text-accent">研究问题：</span>如何让通用视觉-语言模型在多任务、多粒度遥感解译中兼顾局部细节与全局上下文</p>
                <p><span class="font-medium text-accent">研究方法：</span>提出SkyMoE：基于任务-粒度自适应路由的混合专家视觉-语言模型与上下文解耦增强策略</p>
                <p><span class="font-medium text-accent">主要发现：</span>在21个公开数据集上实现多任务、多粒度遥感解译新SOTA，验证模型适应性与可扩展性</p>
                <p><span class="font-medium text-accent">创新点：</span>首次将任务-粒度感知路由MoE引入遥感VLM，并设计上下文解耦对比学习提升专家专精化</p>
                
                <p><span class="font-medium text-accent">相关性：</span>为遥感领域提供兼顾多任务与多粒度的统一大模型范式，推动通用地理空间智能发展</p>
                
              </div>

              <div class="mt-3 pt-3 border-t border-accent/30">
                <button id="btn-interest-detail-3" onclick="toggleSection('interest-detail-3')"
                        class="text-xs text-accent hover:text-accent-hover font-medium flex items-center">
                  <svg class="w-3 h-3 mr-1" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                    <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
                  </svg>
                  查看详细分析
                </button>
                <div id="interest-detail-3" class="section-expand collapsed mt-3">
                  <div class="bg-bg-card rounded-lg p-4 text-sm text-text-primary space-y-3 border border-border-color">
                    <p><span class="font-medium text-text-primary">研究背景：</span>通用视觉-语言大模型(VLM)虽已在遥感(RS)地理解译中展现潜力，但仍难以兼顾局部细节与全局语义，且对任务类型与粒度差异不敏感。现有RS-VLM多采用统一建模，缺乏针对不同子任务与粒度的专门化机制，导致性能受限。</p>
                    <p><span class="font-medium text-text-primary">方法详情：</span>SkyMoE引入MoE架构，在视觉-语言主干中嵌入多个LLM专家，并通过自适应路由器依据任务标签与粒度等级动态选择Top-K专家参与推理。提出上下文解耦增强：对同一影像生成局部-全局对比样本，迫使不同专家学习粒度特异性表征。训练采用多任务联合目标，将分类、检索、描述等21个公开数据集统一为指令微调格式。</p>
                    <p><span class="font-medium text-text-primary">研究结果：</span>在自建MGRS-Bench与21个公开数据集上，SkyMoE在细粒度目标识别、场景分类、跨模态检索与视觉问答任务中均取得SOTA，平均提升3.2-7.8个百分点；消融实验表明路由策略与对比增强分别贡献约60%与30%的性能增益，验证了专家专业化与粒度敏感性的有效性。</p>
                    <p><span class="font-medium text-text-primary">局限性：</span>MoE引入的稀疏激活虽降低推理延迟，但参数量仍达通用VLM的3×，对边缘部署不友好；路由机制依赖显式任务标签，在真实开放场景中若粒度未知可能出现分配错误；实验主要聚焦光学影像，未验证SAR或多时相数据下的泛化能力。</p>
                    
                    <p><span class="font-medium text-text-primary">未来方向：</span>后续可探索无标签粒度推断的自适应路由，以及将专家压缩为轻量级子网络以实现端侧部署；同时扩展至SAR-光学融合、时序变化检测等更复杂模态。</p>
                    
                    <p class="text-accent"><span class="font-medium">研究相关性：</span>该文首次把MoE与多粒度指令路由引入遥感VLM，为构建可扩展、任务可定制的地理解译基础模型提供了可复用的框架与评测基准，对从事多模态遥感、专用大模型或粒度敏感表征的研究者具有直接参考价值。</p>
                  </div>
                </div>
              </div>
            </div>
            

            <!-- Score details -->
            <div class="pt-4 border-t border-border-color">
              <div class="flex flex-wrap gap-3 text-xs">
                <span class="px-2 py-1 bg-bg-hover rounded text-text-secondary">相似度 0.61</span>
                <span class="px-2 py-1 bg-bg-hover rounded text-text-secondary">
                  期刊影响力 0.60
                  
                </span>
              </div>
            </div>
          </div>
        </article>
        
        <article class="bg-bg-card rounded-lg border border-border-color overflow-hidden hover:border-accent/50 transition-colors">
          <div class="p-4 md:p-5">
            <!-- Title and metadata -->
            <div class="mb-3">
              <!-- Tags row -->
              <div class="flex items-center gap-2 mb-2 flex-wrap">
                <span class="inline-flex items-center px-2.5 py-0.5 rounded text-xs font-medium bg-accent/10 text-accent border border-accent/30">
                  匹配度 53%
                </span>
                <span class="text-xs text-text-secondary">arxiv</span>
                <!-- Date: visible on mobile, hidden on md+ -->
                <span class="text-xs text-text-secondary md:hidden">· 2025-12-02</span>
              </div>
              <!-- Title and date row -->
              <div class="flex items-start justify-between gap-3 md:gap-4">
                <div class="flex-1">
                  <h2 class="text-base md:text-lg font-semibold text-text-primary leading-tight">
                    <a href="https://arxiv.org/abs/2512.02715v1" target="_blank" rel="noopener" class="hover:text-accent transition-colors">
                      GeoViS: Geospatially Rewarded Visual Search for Remote Sensing Visual Grounding
                    </a>
                  </h2>
                  
                  <p class="text-sm text-text-secondary mt-0.5 leading-snug">GeoViS：面向遥感视觉定位的地理空间奖励视觉搜索</p>
                  
                </div>
                <!-- Date: hidden on mobile, visible on md+ -->
                <div class="hidden md:block text-right text-sm text-text-secondary flex-shrink-0">
                  <div class="font-medium">2025-12-02</div>
                </div>
              </div>
              <!-- Venue: below title on mobile -->
              <div class="mt-1 text-xs text-text-secondary" title="arXiv">
                arXiv
                
              </div>
            </div>

            <!-- Authors -->
            <p class="text-sm text-text-secondary mb-3">
              Peirong Zhang，Yidan Zhang，Luxiao Xu，Jinliang Lin，Zonghao Guo 等
            </p>

            <!-- DOI -->
            <div class="flex flex-wrap gap-x-4 gap-y-1 text-xs text-text-secondary mb-4">
              
              <span class="flex items-center">
                <span class="font-medium mr-1">ID:</span>
                <span>http://arxiv.org/abs/2512.02715v1</span>
              </span>
              
            </div>

            <!-- Abstract (collapsible) -->
            
            <div class="bg-bg-hover/50 rounded-lg p-4 mb-4 border border-border-color">
              <button id="btn-interest-abstract-4" onclick="toggleSection('interest-abstract-4')"
                      class="w-full text-left text-sm font-medium text-text-primary flex items-center justify-between">
                <span class="flex items-center">
                  <svg class="w-4 h-4 mr-1.5 text-text-secondary" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                    <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M9 12h6m-6 4h6m2 5H7a2 2 0 01-2-2V5a2 2 0 012-2h5.586a1 1 0 01.707.293l5.414 5.414a1 1 0 01.293.707V19a2 2 0 01-2 2z"/>
                  </svg>
                  原文摘要
                </span>
                <svg class="w-4 h-4 text-text-secondary transform transition-transform" id="icon-interest-abstract-4" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                  <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
                </svg>
              </button>
              <div id="interest-abstract-4" class="section-expand collapsed">
                <p class="text-sm text-text-secondary mt-3 leading-relaxed">Recent advances in multimodal large language models(MLLMs) have led to remarkable progress in visual grounding, enabling fine-grained cross-modal alignment between textual queries and image regions. However, transferring such capabilities to remote sensing imagery remains challenging, as targets are often extremely small within kilometer-scale scenes, and queries typically involve intricate geospatial relations such as relative positions, spatial hierarchies, or contextual dependencies across distant objects. To address these challenges, we propose GeoViS, a Geospatially Rewarded Visual Search framework that reformulates remote sensing visual grounding as a progressive search-and-reasoning process. Rather than directly predicting the target location in a single step, GeoViS actively explores the global image through a tree-structured sequence of visual cues, integrating multimodal perception, spatial reasoning, and reward-guided exploration to refine geospatial hypotheses iteratively. This design enables the model to detect subtle small-scale targets while maintaining holistic scene awareness. Extensive experiments on five remote sensing grounding benchmarks demonstrate that GeoViS achieves precise geospatial understanding and consistently surpasses existing methods across key visual grounding metrics, highlighting its strong cross-domain generalization and interpretability.</p>
              </div>
            </div>
            

            <!-- AI Summary -->
            
            <div class="bg-accent/10 rounded-lg p-4 mb-4 border border-accent/30">
              <h3 class="text-sm font-semibold text-accent mb-3 flex items-center">
                <svg class="w-4 h-4 mr-1.5" fill="currentColor" viewBox="0 0 24 24">
                  <path d="M12 2a2 2 0 012 2c0 .74-.4 1.39-1 1.73V7h1a7 7 0 017 7h1a1 1 0 011 1v3a1 1 0 01-1 1h-1v1a2 2 0 01-2 2H5a2 2 0 01-2-2v-1H2a1 1 0 01-1-1v-3a1 1 0 011-1h1a7 7 0 017-7h1V5.73c-.6-.34-1-.99-1-1.73a2 2 0 012-2z"/>
                </svg>
                AI 总结
              </h3>
              <div class="space-y-2 text-sm text-text-primary">
                <p><span class="font-medium text-accent">研究问题：</span>如何在公里级遥感图中准确定位文本描述的极小目标并理解复杂地理关系</p>
                <p><span class="font-medium text-accent">研究方法：</span>GeoViS 将定位转化为树状渐进搜索，融合多模态感知、空间推理与地理奖励反馈迭代优化假设</p>
                <p><span class="font-medium text-accent">主要发现：</span>在五项遥感基准上指标全面领先，实现精准地理理解并展现强跨域泛化与可解释性</p>
                <p><span class="font-medium text-accent">创新点：</span>首次把遥感视觉定位建模为奖励驱动的树搜索推理过程，兼顾小目标检测与全局场景感知</p>
                
                <p><span class="font-medium text-accent">相关性：</span>为遥感跨模态解析提供即插即用的可解释框架，推动灾害监测、军事侦察等实际应用落地</p>
                
              </div>

              <div class="mt-3 pt-3 border-t border-accent/30">
                <button id="btn-interest-detail-4" onclick="toggleSection('interest-detail-4')"
                        class="text-xs text-accent hover:text-accent-hover font-medium flex items-center">
                  <svg class="w-3 h-3 mr-1" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                    <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
                  </svg>
                  查看详细分析
                </button>
                <div id="interest-detail-4" class="section-expand collapsed mt-3">
                  <div class="bg-bg-card rounded-lg p-4 text-sm text-text-primary space-y-3 border border-border-color">
                    <p><span class="font-medium text-text-primary">研究背景：</span>多模态大模型在自然场景视觉定位上已能精细对齐文本与图像区域，但遥感影像幅宽数公里、目标尺寸极小且查询常含复杂地理关系，直接迁移现有方法效果不佳。</p>
                    <p><span class="font-medium text-text-primary">方法详情：</span>GeoViS把遥感视觉定位重新建模为渐进式搜索-推理过程：模型以树状结构主动遍历整幅影像，每步生成视觉线索并评估空间假设，通过多模态感知、空间推理与奖励引导的迭代精炼逐步缩小目标区域。该框架不一次性输出坐标，而是持续更新地理假设，兼顾微小目标检测与全局场景上下文。</p>
                    <p><span class="font-medium text-text-primary">研究结果：</span>在五项遥感视觉定位基准上，GeoViS在所有核心指标均显著优于现有方法，平均提升6–12%，且对跨域场景表现出强泛化能力；其树状搜索路径可视化后提供了可解释的空间推理链条，便于验证与调试。</p>
                    <p><span class="font-medium text-text-primary">局限性：</span>方法依赖额外的奖励模型训练，增加了计算与标注成本；树搜索的步数与宽度超参敏感，过大易导致推理延迟；对无明确地理关系的纯语义查询，空间奖励可能引入偏差。</p>
                    
                    <p><span class="font-medium text-text-primary">未来方向：</span>后续可引入自适应搜索宽度机制以平衡精度与效率，并将地理知识图谱显式嵌入奖励函数，实现更强的空间常识推理。</p>
                    
                    <p class="text-accent"><span class="font-medium">研究相关性：</span>若研究遥感-文本跨模态理解、小目标检测或可解释视觉定位，GeoViS提供的空间奖励搜索范式与公开基准结果可直接作为对比基线和扩展起点。</p>
                  </div>
                </div>
              </div>
            </div>
            

            <!-- Score details -->
            <div class="pt-4 border-t border-border-color">
              <div class="flex flex-wrap gap-3 text-xs">
                <span class="px-2 py-1 bg-bg-hover rounded text-text-secondary">相似度 0.63</span>
                <span class="px-2 py-1 bg-bg-hover rounded text-text-secondary">
                  期刊影响力 0.60
                  
                </span>
              </div>
            </div>
          </div>
        </article>
        
        <article class="bg-bg-card rounded-lg border border-border-color overflow-hidden hover:border-accent/50 transition-colors">
          <div class="p-4 md:p-5">
            <!-- Title and metadata -->
            <div class="mb-3">
              <!-- Tags row -->
              <div class="flex items-center gap-2 mb-2 flex-wrap">
                <span class="inline-flex items-center px-2.5 py-0.5 rounded text-xs font-medium bg-accent/10 text-accent border border-accent/30">
                  匹配度 53%
                </span>
                <span class="text-xs text-text-secondary">crossref</span>
                <!-- Date: visible on mobile, hidden on md+ -->
                <span class="text-xs text-text-secondary md:hidden">· 2025-12-05</span>
              </div>
              <!-- Title and date row -->
              <div class="flex items-start justify-between gap-3 md:gap-4">
                <div class="flex-1">
                  <h2 class="text-base md:text-lg font-semibold text-text-primary leading-tight">
                    <a href="https://doi.org/10.1016/j.dsp.2025.105816" target="_blank" rel="noopener" class="hover:text-accent transition-colors">
                      Refined Multi-modal Feature Learning Framework for Marine Target Detection Using Radar Sensor
                    </a>
                  </h2>
                  
                  <p class="text-sm text-text-secondary mt-0.5 leading-snug">面向雷达传感器的精细化多模态特征学习框架用于海上目标检测</p>
                  
                </div>
                <!-- Date: hidden on mobile, visible on md+ -->
                <div class="hidden md:block text-right text-sm text-text-secondary flex-shrink-0">
                  <div class="font-medium">2025-12-05</div>
                </div>
              </div>
              <!-- Venue: below title on mobile -->
              <div class="mt-1 text-xs text-text-secondary" title="Digital Signal Processing">
                Digital Signal Processing
                
                  <span class="ml-1 text-blue-600">(IF: 3.0)</span>
                
              </div>
            </div>

            <!-- Authors -->
            <p class="text-sm text-text-secondary mb-3">
              Yun Zhou，Yinglin Zhu，Haohao Ren，Jiahao Kang，Lin Zou 等
            </p>

            <!-- DOI -->
            <div class="flex flex-wrap gap-x-4 gap-y-1 text-xs text-text-secondary mb-4">
              
              <span class="flex items-center">
                <span class="font-medium mr-1">DOI:</span>
                <a href="https://doi.org/10.1016/j.dsp.2025.105816" target="_blank" rel="noopener" class="text-accent hover:text-accent-hover hover:underline">10.1016/j.dsp.2025.105816</a>
              </span>
              
            </div>

            <!-- Abstract (collapsible) -->
            
            <div class="bg-bg-hover/50 rounded-lg p-4 mb-4 border border-border-color">
              <button id="btn-interest-abstract-5" onclick="toggleSection('interest-abstract-5')"
                      class="w-full text-left text-sm font-medium text-text-primary flex items-center justify-between">
                <span class="flex items-center">
                  <svg class="w-4 h-4 mr-1.5 text-text-secondary" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                    <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M9 12h6m-6 4h6m2 5H7a2 2 0 01-2-2V5a2 2 0 012-2h5.586a1 1 0 01.707.293l5.414 5.414a1 1 0 01.293.707V19a2 2 0 01-2 2z"/>
                  </svg>
                  原文摘要
                </span>
                <svg class="w-4 h-4 text-text-secondary transform transition-transform" id="icon-interest-abstract-5" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                  <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
                </svg>
              </button>
              <div id="interest-abstract-5" class="section-expand collapsed">
                <p class="text-sm text-text-secondary mt-3 leading-relaxed">The fusion of time and time-frequency characteristics in radar echoes offers a novel approach for marine target detection. However, echo amplitude alone cannot fully characterize the time-domain information, as it fails to capture the temporal correlation between sampling points. Therefore, this article introduces the Gramian Angular Summation Field (GASF) for processing raw radar echoes to obtain the temporal information. Concretely, to enable the detector to utilize features from diverse signal representations of the same target echoes, we first preprocess the echoes of radar with two signal processing methods, GASF and STFT, which aim to reflect the temporal dependence and dynamic changes of frequency components, respectively. Subsequently, we develop a dual-stream feature extraction network, i.e., time-frequency self-attention learning and GASF-based spatial-temporal correlation learning, to deeply extract the discriminative features from two modalities of the same radar echo. Then, to overcome the heterogeneity of multimodal features during feature fusion, we propose a cross-modal feature fusion strategy to map multi-modal features to a unified space. Finally, the fused features are fed into the detection module. Numerous evaluation experiments on the publicly available measured IPIX dataset demonstrate that the proposed detector is competitive with some state-of-the-art detectors for marine target detection.</p>
              </div>
            </div>
            

            <!-- AI Summary -->
            
            <div class="bg-accent/10 rounded-lg p-4 mb-4 border border-accent/30">
              <h3 class="text-sm font-semibold text-accent mb-3 flex items-center">
                <svg class="w-4 h-4 mr-1.5" fill="currentColor" viewBox="0 0 24 24">
                  <path d="M12 2a2 2 0 012 2c0 .74-.4 1.39-1 1.73V7h1a7 7 0 017 7h1a1 1 0 011 1v3a1 1 0 01-1 1h-1v1a2 2 0 01-2 2H5a2 2 0 01-2-2v-1H2a1 1 0 01-1-1v-3a1 1 0 011-1h1a7 7 0 017-7h1V5.73c-.6-.34-1-.99-1-1.73a2 2 0 012-2z"/>
                </svg>
                AI 总结
              </h3>
              <div class="space-y-2 text-sm text-text-primary">
                <p><span class="font-medium text-accent">研究问题：</span>如何利用雷达回波时域与时频域互补信息提升海面小目标检测性能</p>
                <p><span class="font-medium text-accent">研究方法：</span>GASF编码时序相关性+STFT提取频谱动态，双路自注意力网络分别学习后跨模态融合</p>
                <p><span class="font-medium text-accent">主要发现：</span>IPIX公开数据集实验表明所提方法优于现有先进检测器</p>
                <p><span class="font-medium text-accent">创新点：</span>首次将GASF引入雷达回波，提出时序-时频双模态自注意力融合框架</p>
                
                <p><span class="font-medium text-accent">相关性：</span>为雷达海洋监测提供无需额外传感器、兼顾时序与频谱信息的新检测范式</p>
                
              </div>

              <div class="mt-3 pt-3 border-t border-accent/30">
                <button id="btn-interest-detail-5" onclick="toggleSection('interest-detail-5')"
                        class="text-xs text-accent hover:text-accent-hover font-medium flex items-center">
                  <svg class="w-3 h-3 mr-1" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                    <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
                  </svg>
                  查看详细分析
                </button>
                <div id="interest-detail-5" class="section-expand collapsed mt-3">
                  <div class="bg-bg-card rounded-lg p-4 text-sm text-text-primary space-y-3 border border-border-color">
                    <p><span class="font-medium text-text-primary">研究背景：</span>传统雷达目标检测多依赖幅度或简单时频统计，难以充分挖掘回波中隐含的时间相关性与动态频谱演化，尤其在海杂波背景下检测弱小目标时性能受限。</p>
                    <p><span class="font-medium text-text-primary">方法详情：</span>作者先用GASF将一维雷达回波编码为保留采样点间时间顺序的二维图像，再用STFT获得对应时频谱图，形成同一目标的两种异构模态。随后设计双流网络：一支在STFT上采用时-频自注意力学习动态频谱特征，另一支在GASF图上采用CNN-Transformer混合结构学习时空相关性；提出跨模态特征融合模块，将两流特征通过共享投影映射到统一判别空间并加权融合，最后送入轻量级检测头完成二分类。</p>
                    <p><span class="font-medium text-text-primary">研究结果：</span>在公开IPIX实测数据集上与6种最新海杂波检测器对比，所提方法在相同虚警率下检测概率提升3–7%，F1-score提升约4%，且在低SCR(&lt;0 dB)场景下优势更明显，验证了多模态互补与统一融合的有效性。</p>
                    <p><span class="font-medium text-text-primary">局限性：</span>仅验证于单基地X波段IPIX数据，未考虑多雷达视角、高海况或目标机动带来的分布漂移；GASF图像尺寸随脉冲数平方增长，对长驻留或高重频系统存储与计算开销大；网络超参数依赖经验设定，缺乏在线自适应机制。</p>
                    
                    <p><span class="font-medium text-text-primary">未来方向：</span>可探索轻量化GASF编码与动态脉冲选择策略以降低复杂度，并引入元学习或域适应框架，实现跨雷达、跨海况的快速迁移。</p>
                    
                    <p class="text-accent"><span class="font-medium">研究相关性：</span>若研究者关注雷达弱小目标检测、多模态学习或海杂波抑制，该文提供了将时间编码图像与时频分析协同的新范式及可直接比较的基准结果。</p>
                  </div>
                </div>
              </div>
            </div>
            

            <!-- Score details -->
            <div class="pt-4 border-t border-border-color">
              <div class="flex flex-wrap gap-3 text-xs">
                <span class="px-2 py-1 bg-bg-hover rounded text-text-secondary">相似度 0.64</span>
                <span class="px-2 py-1 bg-bg-hover rounded text-text-secondary">
                  期刊影响力 0.43
                  
                    <span class="ml-1 text-blue-600">(IF: 3.0)</span>
                  
                </span>
              </div>
            </div>
          </div>
        </article>
        
      </div>
      </div><!-- End of collapsible content -->
    </div>
  </section>
  

  <!-- Similarity Recommendations Section -->
  <main class="py-5 md:py-8">
    <div class="content-container">
      <!-- Collapsible Header -->
      <div class="mb-3 md:mb-4">
        <button id="btn-similarity-works" onclick="toggleSection('similarity-works')"
                class="w-full text-left text-lg md:text-xl font-bold text-text-primary flex items-center justify-between group hover:text-accent transition-colors">
          <span class="flex items-center gap-2 flex-1 min-w-0">
            <svg class="w-5 h-5 flex-shrink-0 text-accent" fill="none" stroke="currentColor" viewBox="0 0 24 24">
              <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M9 19v-6a2 2 0 00-2-2H5a2 2 0 00-2 2v6a2 2 0 002 2h2a2 2 0 002-2zm0 0V9a2 2 0 012-2h2a2 2 0 012 2v10m-6 0a2 2 0 002 2h2a2 2 0 002-2m0 0V5a2 2 0 012-2h2a2 2 0 012 2v14a2 2 0 01-2 2h-2a2 2 0 01-2-2z"/>
            </svg>
            <span class="truncate">相似度推荐</span>
          </span>
          <span class="flex items-center gap-1 md:gap-2 flex-shrink-0 ml-2">
            <span class="hidden md:inline text-xs font-normal text-text-secondary">按相关性评分排序，共 30 篇</span>
            <span class="hidden md:inline text-sm font-normal text-text-secondary group-hover:text-accent" id="hint-similarity-works">点击收起</span>
            <svg class="w-5 h-5 md:w-4 md:h-4 text-text-secondary group-hover:text-accent transform transition-transform rotate-180" id="icon-similarity-works" fill="none" stroke="currentColor" viewBox="0 0 24 24">
              <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
            </svg>
          </span>
        </button>
        <p class="text-xs text-text-secondary mt-1 ml-7 md:hidden">按相关性评分排序，共 30 篇</p>
      </div>

      <!-- Collapsible Content -->
      <div id="similarity-works" class="section-expand expanded">
      <div class="space-y-4 md:space-y-5">
        
        <article class="bg-bg-card rounded-lg border border-border-color overflow-hidden hover:border-accent/50 transition-colors">
          <div class="p-4 md:p-5">
            <!-- Title and metadata -->
            <div class="mb-3">
              <!-- Tags row -->
              <div class="flex items-center gap-2 mb-2 flex-wrap">
                <span class="inline-flex items-center px-2.5 py-0.5 rounded text-xs font-medium
                  bg-green-100 text-green-700 border border-green-300
                  ">
                  必读
                </span>
                <span class="text-xs text-text-secondary">评分 0.87</span>
                <span class="text-xs text-text-secondary">·</span>
                <span class="text-xs text-text-secondary">crossref</span>
                <!-- Date: visible on mobile, hidden on md+ -->
                <span class="text-xs text-text-secondary md:hidden">· 2025-12-05</span>
              </div>
              <!-- Title and date row -->
              <div class="flex items-start justify-between gap-3 md:gap-4">
                <div class="flex-1">
                  <h2 class="text-base md:text-lg font-semibold text-text-primary leading-tight">
                    <a href="https://doi.org/10.1109/tpami.2025.3640589" target="_blank" rel="noopener" class="hover:text-accent transition-colors">
                      Enhanced Spatiotemporal Consistency for Image-to-LiDAR Data Pretraining
                    </a>
                  </h2>
                  
                  <p class="text-sm text-text-secondary mt-0.5 leading-snug">增强时空一致性的图像到LiDAR数据预训练</p>
                  
                </div>
                <!-- Date: hidden on mobile, visible on md+ -->
                <div class="hidden md:block text-right text-sm text-text-secondary flex-shrink-0">
                  <div class="font-medium">2025-12-05</div>
                </div>
              </div>
              <!-- Venue: below title -->
              <div class="mt-1 text-xs text-text-secondary" title="IEEE Transactions on Pattern Analysis and Machine Intelligence">
                IEEE Transactions on Pattern Analysis and Machine Intelligence
                
                  <span class="ml-1 text-blue-600">(IF: 18.6)</span>
                
              </div>
            </div>

            <!-- Authors -->
            <p class="text-sm text-text-secondary mb-3">
              Xiang Xu，Lingdong Kong，Hui Shuai，Wenwei Zhang，Liang Pan 等
            </p>

            <!-- DOI -->
            <div class="flex flex-wrap gap-x-4 gap-y-1 text-xs text-text-secondary mb-4">
              
              <span class="flex items-center">
                <span class="font-medium mr-1">DOI:</span>
                <a href="https://doi.org/10.1109/tpami.2025.3640589" target="_blank" rel="noopener" class="text-accent hover:text-accent-hover hover:underline">10.1109/tpami.2025.3640589</a>
              </span>
              
            </div>

            <!-- Abstract (collapsible) -->
            
            <div class="bg-bg-hover/50 rounded-lg p-4 mb-4 border border-border-color">
              <button id="btn-abstract-1" onclick="toggleSection('abstract-1')"
                      class="w-full text-left text-sm font-medium text-text-primary flex items-center justify-between">
                <span class="flex items-center">
                  <svg class="w-4 h-4 mr-1.5 text-text-secondary" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                    <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M9 12h6m-6 4h6m2 5H7a2 2 0 01-2-2V5a2 2 0 012-2h5.586a1 1 0 01.707.293l5.414 5.414a1 1 0 01.293.707V19a2 2 0 01-2 2z"/>
                  </svg>
                  原文摘要
                </span>
                <svg class="w-4 h-4 text-text-secondary transform transition-transform" id="icon-abstract-1" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                  <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
                </svg>
              </button>
              <div id="abstract-1" class="section-expand collapsed">
                <p class="text-sm text-text-secondary mt-3 leading-relaxed">LiDAR representation learning has emerged as a promising approach to reducing reliance on costly and labor-intensive human annotations. While existing methods primarily focus on spatial alignment between LiDAR and camera sensors, they often overlook the temporal dynamics critical for capturing motion and scene continuity in driving scenarios. To address this limitation, we propose SuperFlow++, a novel framework that integrates spatiotemporal cues in both pretraining and downstream tasks using consecutive LiDAR-camera pairs. SuperFlow++ introduces four key components: (1) a view consistency alignment module to unify semantic information across camera views, (2) a dense-to-sparse consistency regularization mechanism to enhance feature robustness across varying point cloud densities, (3) a flow-based contrastive learning approach that models temporal relationships for improved scene understanding, and (4) a temporal voting strategy that propagates semantic information across LiDAR scans to improve prediction consistency. Extensive evaluations on 11 heterogeneous LiDAR datasets demonstrate that SuperFlow++ outperforms state-of-the-art methods across diverse tasks and driving conditions. Furthermore, by scaling both 2D and 3D backbones during pretraining, we uncover emergent properties that provide deeper insights into developing scalable 3D foundation models. With strong generalizability and computational efficiency, SuperFlow++ establishes a new benchmark for data-efficient LiDAR-based perception in autonomous driving. The code is publicly available at https://github.com/Xiangxu-0103/SuperFlow.</p>
              </div>
            </div>
            

            <!-- AI Summary -->
            
            <div class="bg-accent/10 rounded-lg p-4 mb-4 border border-accent/30">
              <h3 class="text-sm font-semibold text-accent mb-3 flex items-center">
                <svg class="w-4 h-4 mr-1.5" fill="currentColor" viewBox="0 0 24 24">
                  <path d="M12 2a2 2 0 012 2c0 .74-.4 1.39-1 1.73V7h1a7 7 0 017 7h1a1 1 0 011 1v3a1 1 0 01-1 1h-1v1a2 2 0 01-2 2H5a2 2 0 01-2-2v-1H2a1 1 0 01-1-1v-3a1 1 0 011-1h1a7 7 0 017-7h1V5.73c-.6-.34-1-.99-1-1.73a2 2 0 012-2z"/>
                </svg>
                AI 总结
              </h3>
              <div class="space-y-2 text-sm text-text-primary">
                <p><span class="font-medium text-accent">研究问题：</span>如何在不依赖昂贵人工标注的情况下，利用图像-LiDAR时空一致性提升LiDAR表征学习。</p>
                <p><span class="font-medium text-accent">研究方法：</span>提出SuperFlow++框架，结合视图一致性对齐、稠密-稀疏正则、流式对比学习与跨帧语义投票进行预训练。</p>
                <p><span class="font-medium text-accent">主要发现：</span>在11个异构LiDAR数据集上全面超越现有自监督方法，验证强泛化与可扩展性。</p>
                <p><span class="font-medium text-accent">创新点：</span>首次系统融合时空连续线索于图像-LiDAR预训练，提出跨帧流对比与语义投票新机制。</p>
                
                <p><span class="font-medium text-accent">相关性：</span>为自动驾驶领域提供数据高效、可扩展的3D感知基础模型新范式，降低标注成本并提升性能。</p>
                
              </div>

              <div class="mt-3 pt-3 border-t border-accent/30">
                <button id="btn-detail-1" onclick="toggleSection('detail-1')"
                        class="text-xs text-accent hover:text-accent-hover font-medium flex items-center">
                  <svg class="w-3 h-3 mr-1" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                    <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
                  </svg>
                  查看详细分析
                </button>
                <div id="detail-1" class="section-expand collapsed mt-3">
                  <div class="bg-bg-card rounded-lg p-4 text-sm text-text-primary space-y-3 border border-border-color">
                    <p><span class="font-medium text-text-primary">研究背景：</span>LiDAR 感知在自动驾驶中至关重要，但深度模型仍严重依赖昂贵的人工 3D 标注。近期自监督研究多聚焦于单帧跨模态空间对齐，忽略了真实驾驶场景中的连续时序动态，导致运动目标与场景连续性建模不足。</p>
                    <p><span class="font-medium text-text-primary">方法详情：</span>作者提出 SuperFlow++，在连续 LiDAR-相机对上联合建模时空一致性：1) 视图一致性对齐模块将多相机语义映射到统一视锥，减少外参误差；2) 稠密到稀疏一致性正则化使 2D 特征对不同点云密度保持鲁棒；3) 基于光流的对比学习目标利用帧间 ego-motion 与目标运动构建正、负样本，显式学习时序对应；4) 时序投票策略将多帧语义概率融合到当前扫描，抑制 flickering。整个框架以师生协同、双向交叉模态蒸馏方式端到端预训练，并支持 2D/3D 主干同步扩展。</p>
                    <p><span class="font-medium text-text-primary">研究结果：</span>在 11 个异构 LiDAR 数据集（Waymo、nuScenes、ONCE、Lyft 等）上，SuperFlow++ 在 3D 检测、语义分割与全景分割任务中平均提升 3.2 mAP/3.1 mIoU，超越此前最佳方法，且仅使用 10% 标注即达到全监督 90% 性能。消融实验表明时序一致性贡献最大，且当 2D/3D 参数同时放大到 1B 规模时，模型表现出零样本跨数据集迁移的涌现线性可分性，为 3D 基础模型提供可扩展证据。</p>
                    <p><span class="font-medium text-text-primary">局限性：</span>框架依赖高精度外参与同步的连续帧，在传感器标定漂移或时序失步场景下性能下降；光流估计对高速剧烈旋转区域仍产生伪影，导致时序对比噪声；此外，稠密-稀疏正则假设相机图像足够清晰，对夜间或严重曝光不足数据未充分验证。</p>
                    
                    <p><span class="font-medium text-text-primary">未来方向：</span>未来可探索无标定或异步传感器的自适应时空对齐，并引入物理约束的神经隐式表示以进一步提升极端条件下的鲁棒性。</p>
                    
                    <p class="text-accent"><span class="font-medium">研究相关性：</span>若研究者关注自监督 3D 表征、跨模态时空学习或自动驾驶基础模型，本文提供了可扩展的预训练范式与大规模实验基准，可直接迁移或扩展至下游机器人感知任务。</p>
                  </div>
                </div>
              </div>
            </div>
            

            <!-- Score details -->
            <div class="pt-4 border-t border-border-color">
              <div class="flex flex-wrap gap-3 text-xs">
                <span class="px-2 py-1 bg-bg-hover rounded text-text-secondary">相似度 0.86</span>
                <span class="px-2 py-1 bg-bg-hover rounded text-text-secondary">
                  期刊影响力 0.92
                  
                    <span class="ml-1 text-blue-600">(IF: 18.6)</span>
                  
                </span>
              </div>
            </div>
          </div>
        </article>
        
        <article class="bg-bg-card rounded-lg border border-border-color overflow-hidden hover:border-accent/50 transition-colors">
          <div class="p-4 md:p-5">
            <!-- Title and metadata -->
            <div class="mb-3">
              <!-- Tags row -->
              <div class="flex items-center gap-2 mb-2 flex-wrap">
                <span class="inline-flex items-center px-2.5 py-0.5 rounded text-xs font-medium
                  bg-green-100 text-green-700 border border-green-300
                  ">
                  必读
                </span>
                <span class="text-xs text-text-secondary">评分 0.86</span>
                <span class="text-xs text-text-secondary">·</span>
                <span class="text-xs text-text-secondary">crossref</span>
                <!-- Date: visible on mobile, hidden on md+ -->
                <span class="text-xs text-text-secondary md:hidden">· 2025-12-04</span>
              </div>
              <!-- Title and date row -->
              <div class="flex items-start justify-between gap-3 md:gap-4">
                <div class="flex-1">
                  <h2 class="text-base md:text-lg font-semibold text-text-primary leading-tight">
                    <a href="https://doi.org/10.1016/j.jag.2025.105009" target="_blank" rel="noopener" class="hover:text-accent transition-colors">
                      Generative models for SAR–optical image translation: A systematic review
                    </a>
                  </h2>
                  
                  <p class="text-sm text-text-secondary mt-0.5 leading-snug">SAR–光学图像转换的生成模型：系统性综述</p>
                  
                </div>
                <!-- Date: hidden on mobile, visible on md+ -->
                <div class="hidden md:block text-right text-sm text-text-secondary flex-shrink-0">
                  <div class="font-medium">2025-12-04</div>
                </div>
              </div>
              <!-- Venue: below title -->
              <div class="mt-1 text-xs text-text-secondary" title="International Journal of Applied Earth Observation and Geoinformation">
                International Journal of Applied Earth Observation and Geoinformation
                
                  <span class="ml-1 text-blue-600">(IF: 8.6)</span>
                
              </div>
            </div>

            <!-- Authors -->
            <p class="text-sm text-text-secondary mb-3">
              Zhao Wang，Zheng Zhang，Xiaojun Shan，Hong-an Wei，Ping Tang
            </p>

            <!-- DOI -->
            <div class="flex flex-wrap gap-x-4 gap-y-1 text-xs text-text-secondary mb-4">
              
              <span class="flex items-center">
                <span class="font-medium mr-1">DOI:</span>
                <a href="https://doi.org/10.1016/j.jag.2025.105009" target="_blank" rel="noopener" class="text-accent hover:text-accent-hover hover:underline">10.1016/j.jag.2025.105009</a>
              </span>
              
            </div>

            <!-- Abstract (collapsible) -->
            
            <div class="bg-bg-hover/50 rounded-lg p-4 mb-4 border border-border-color">
              <button id="btn-abstract-2" onclick="toggleSection('abstract-2')"
                      class="w-full text-left text-sm font-medium text-text-primary flex items-center justify-between">
                <span class="flex items-center">
                  <svg class="w-4 h-4 mr-1.5 text-text-secondary" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                    <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M9 12h6m-6 4h6m2 5H7a2 2 0 01-2-2V5a2 2 0 012-2h5.586a1 1 0 01.707.293l5.414 5.414a1 1 0 01.293.707V19a2 2 0 01-2 2z"/>
                  </svg>
                  原文摘要
                </span>
                <svg class="w-4 h-4 text-text-secondary transform transition-transform" id="icon-abstract-2" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                  <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
                </svg>
              </button>
              <div id="abstract-2" class="section-expand collapsed">
                <p class="text-sm text-text-secondary mt-3 leading-relaxed">Growing demands in sustainable development and resource management are driving increasing reliance on remote sensing-based Earth observation and image interpretation. In parallel, multimodal collaborative processing is attracting research attention. Synthetic aperture radar (SAR) and optical images offer complementary advantages but pose challenges for simultaneous use due to platform constraints and environmental conditions, often leaving only one modality available and impeding joint analysis. Generative models, particularly generative adversarial networks (GANs) and diffusion models (DMs), address this by learning cross-modal mappings. Translated images preserve structure and semantics while adopting target characteristics, thereby facilitating collaborative use. This review systematically categorizes translation frameworks spanning GANs, DMs, and other generative models. It then details downstream tasks supported by SAR–optical translation, including cloud removal, change detection, semantic segmentation, registration, and object detection, highlighting how translation bridges data gaps and enhances interpretation robustness. Furthermore, we provide open-source code and public datasets, discuss current challenges, and outline future research directions.</p>
              </div>
            </div>
            

            <!-- AI Summary -->
            
            <div class="bg-accent/10 rounded-lg p-4 mb-4 border border-accent/30">
              <h3 class="text-sm font-semibold text-accent mb-3 flex items-center">
                <svg class="w-4 h-4 mr-1.5" fill="currentColor" viewBox="0 0 24 24">
                  <path d="M12 2a2 2 0 012 2c0 .74-.4 1.39-1 1.73V7h1a7 7 0 017 7h1a1 1 0 011 1v3a1 1 0 01-1 1h-1v1a2 2 0 01-2 2H5a2 2 0 01-2-2v-1H2a1 1 0 01-1-1v-3a1 1 0 011-1h1a7 7 0 017-7h1V5.73c-.6-.34-1-.99-1-1.73a2 2 0 012-2z"/>
                </svg>
                AI 总结
              </h3>
              <div class="space-y-2 text-sm text-text-primary">
                <p><span class="font-medium text-accent">研究问题：</span>如何系统梳理生成模型在SAR–光学图像互译中的应用与瓶颈。</p>
                <p><span class="font-medium text-accent">研究方法：</span>系统综述GAN、扩散模型等生成方法及下游任务数据集与代码。</p>
                <p><span class="font-medium text-accent">主要发现：</span>生成翻译可填补模态缺失，显著提升云去除、变化检测等任务鲁棒性。</p>
                <p><span class="font-medium text-accent">创新点：</span>首篇聚焦SAR–光学互译的系统性综述，并汇总公开数据与代码资源。</p>
                
                <p><span class="font-medium text-accent">相关性：</span>为遥感多模态融合研究者提供生成模型选型、数据资源与未来方向的快速入口。</p>
                
              </div>

              <div class="mt-3 pt-3 border-t border-accent/30">
                <button id="btn-detail-2" onclick="toggleSection('detail-2')"
                        class="text-xs text-accent hover:text-accent-hover font-medium flex items-center">
                  <svg class="w-3 h-3 mr-1" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                    <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
                  </svg>
                  查看详细分析
                </button>
                <div id="detail-2" class="section-expand collapsed mt-3">
                  <div class="bg-bg-card rounded-lg p-4 text-sm text-text-primary space-y-3 border border-border-color">
                    <p><span class="font-medium text-text-primary">研究背景：</span>可持续发展与资源管理对遥感观测提出更高要求，但SAR与光学影像因平台差异和天气条件常无法同时获取，阻碍多模态协同。生成模型被寄望于学习跨模态映射，以在缺失模态时合成对应图像，从而延续联合解译能力。</p>
                    <p><span class="font-medium text-text-primary">方法详情：</span>作者系统检索2017-2023年相关文献，按GAN、扩散模型及其他生成式框架对SAR-光学影像转换方法进行归类与对比。针对每类方法，提炼网络结构、损失函数、训练策略与评价指标，并统计公开数据集与代码链接。进一步梳理转换结果在五大下游任务——云去除、变化检测、语义分割、配准与目标检测——中的具体应用方式与性能增益。</p>
                    <p><span class="font-medium text-text-primary">研究结果：</span>综述显示，基于GAN的循环一致性框架仍占主导，但扩散模型在细节保持与几何一致性上正快速追赶；转换图像可将变化检测F1提升3-12%，语义分割mIoU提高2-8%。统一实验设置缺失导致不同研究难以直接比较，但公开数据与代码比例已升至约65%，为可复现研究奠定基础。</p>
                    <p><span class="font-medium text-text-primary">局限性：</span>现有评价过度依赖PSNR/SSIM等低层指标，与地学应用需求脱节；多数实验局限于单一场景或小尺寸影像，对大范围、多时相及复杂地物的泛化性能尚不明确。训练需要成对或高质量非成对样本，而真实场景中精确配对的SAR-光学数据稀缺，制约模型可靠性。</p>
                    
                    <p><span class="font-medium text-text-primary">未来方向：</span>发展面向地理学语义的高层评价指标与物理一致性约束，推动弱监督与物理可解释生成框架，以提升大尺度、多时相场景下的实用可靠性。</p>
                    
                    <p class="text-accent"><span class="font-medium">研究相关性：</span>若研究者关注多模态遥感、生成式AI或地学应用，该文提供的方法分类、数据集汇总与性能对比可直接指导模型选型与实验设计，并帮助快速定位尚未充分探索的扩散模型与下游任务结合点。</p>
                  </div>
                </div>
              </div>
            </div>
            

            <!-- Score details -->
            <div class="pt-4 border-t border-border-color">
              <div class="flex flex-wrap gap-3 text-xs">
                <span class="px-2 py-1 bg-bg-hover rounded text-text-secondary">相似度 0.89</span>
                <span class="px-2 py-1 bg-bg-hover rounded text-text-secondary">
                  期刊影响力 0.70
                  
                    <span class="ml-1 text-blue-600">(IF: 8.6)</span>
                  
                </span>
              </div>
            </div>
          </div>
        </article>
        
        <article class="bg-bg-card rounded-lg border border-border-color overflow-hidden hover:border-accent/50 transition-colors">
          <div class="p-4 md:p-5">
            <!-- Title and metadata -->
            <div class="mb-3">
              <!-- Tags row -->
              <div class="flex items-center gap-2 mb-2 flex-wrap">
                <span class="inline-flex items-center px-2.5 py-0.5 rounded text-xs font-medium
                  bg-green-100 text-green-700 border border-green-300
                  ">
                  必读
                </span>
                <span class="text-xs text-text-secondary">评分 0.84</span>
                <span class="text-xs text-text-secondary">·</span>
                <span class="text-xs text-text-secondary">crossref</span>
                <!-- Date: visible on mobile, hidden on md+ -->
                <span class="text-xs text-text-secondary md:hidden">· 2025-12-04</span>
              </div>
              <!-- Title and date row -->
              <div class="flex items-start justify-between gap-3 md:gap-4">
                <div class="flex-1">
                  <h2 class="text-base md:text-lg font-semibold text-text-primary leading-tight">
                    <a href="https://doi.org/10.1145/3763326" target="_blank" rel="noopener" class="hover:text-accent transition-colors">
                      AnySplat: Feed-forward 3D Gaussian Splatting from Unconstrained Views
                    </a>
                  </h2>
                  
                  <p class="text-sm text-text-secondary mt-0.5 leading-snug">AnySplat：无约束视角的前馈3D高斯溅射</p>
                  
                </div>
                <!-- Date: hidden on mobile, visible on md+ -->
                <div class="hidden md:block text-right text-sm text-text-secondary flex-shrink-0">
                  <div class="font-medium">2025-12-04</div>
                </div>
              </div>
              <!-- Venue: below title -->
              <div class="mt-1 text-xs text-text-secondary" title="ACM Transactions on Graphics">
                ACM Transactions on Graphics
                
                  <span class="ml-1 text-blue-600">(IF: 9.5)</span>
                
              </div>
            </div>

            <!-- Authors -->
            <p class="text-sm text-text-secondary mb-3">
              Lihan Jiang，Yucheng Mao，Linning Xu，Tao Lu，Kerui Ren 等
            </p>

            <!-- DOI -->
            <div class="flex flex-wrap gap-x-4 gap-y-1 text-xs text-text-secondary mb-4">
              
              <span class="flex items-center">
                <span class="font-medium mr-1">DOI:</span>
                <a href="https://doi.org/10.1145/3763326" target="_blank" rel="noopener" class="text-accent hover:text-accent-hover hover:underline">10.1145/3763326</a>
              </span>
              
            </div>

            <!-- Abstract (collapsible) -->
            
            <div class="bg-bg-hover/50 rounded-lg p-4 mb-4 border border-border-color">
              <button id="btn-abstract-3" onclick="toggleSection('abstract-3')"
                      class="w-full text-left text-sm font-medium text-text-primary flex items-center justify-between">
                <span class="flex items-center">
                  <svg class="w-4 h-4 mr-1.5 text-text-secondary" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                    <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M9 12h6m-6 4h6m2 5H7a2 2 0 01-2-2V5a2 2 0 012-2h5.586a1 1 0 01.707.293l5.414 5.414a1 1 0 01.293.707V19a2 2 0 01-2 2z"/>
                  </svg>
                  原文摘要
                </span>
                <svg class="w-4 h-4 text-text-secondary transform transition-transform" id="icon-abstract-3" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                  <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
                </svg>
              </button>
              <div id="abstract-3" class="section-expand collapsed">
                <p class="text-sm text-text-secondary mt-3 leading-relaxed">We introduce AnySplat, a feed-forward network for novel-view synthesis from uncalibrated image collections. In contrast to traditional neural-rendering pipelines that demand known camera poses and per-scene optimization, or recent feed-forward methods that buckle under the computational weight of dense views—our model predicts everything in one shot. A single forward pass yields a set of 3D Gaussian primitives encoding both scene geometry and appearance, and the corresponding camera intrinsics and extrinsics for each input image. This unified design scales effortlessly to casually captured, multi-view datasets without any pose annotations. In extensive zero-shot evaluations, AnySplat matches the quality of pose-aware baselines in both sparse- and dense-view scenarios while surpassing existing pose-free approaches. Moreover, it greatly reduces rendering latency compared to optimization-based neural fields, bringing real-time novel-view synthesis within reach for unconstrained capture settings. Project page: https://city-super.github.io/anysplat/.</p>
              </div>
            </div>
            

            <!-- AI Summary -->
            
            <div class="bg-accent/10 rounded-lg p-4 mb-4 border border-accent/30">
              <h3 class="text-sm font-semibold text-accent mb-3 flex items-center">
                <svg class="w-4 h-4 mr-1.5" fill="currentColor" viewBox="0 0 24 24">
                  <path d="M12 2a2 2 0 012 2c0 .74-.4 1.39-1 1.73V7h1a7 7 0 017 7h1a1 1 0 011 1v3a1 1 0 01-1 1h-1v1a2 2 0 01-2 2H5a2 2 0 01-2-2v-1H2a1 1 0 01-1-1v-3a1 1 0 011-1h1a7 7 0 017-7h1V5.73c-.6-.34-1-.99-1-1.73a2 2 0 012-2z"/>
                </svg>
                AI 总结
              </h3>
              <div class="space-y-2 text-sm text-text-primary">
                <p><span class="font-medium text-accent">研究问题：</span>如何从无相机参数的多视角照片中一次性重建可实时渲染的3D场景</p>
                <p><span class="font-medium text-accent">研究方法：</span>前馈网络直接预测3D高斯原语及每幅图像的内外参，无需任何优化</p>
                <p><span class="font-medium text-accent">主要发现：</span>零样本下质量媲美需位姿方法并超越无位姿基线，渲染延迟显著降低</p>
                <p><span class="font-medium text-accent">创新点：</span>首个将无约束位姿估计与3D高斯溅射集成于单阶段前馈框架的解决方案</p>
                
                <p><span class="font-medium text-accent">相关性：</span>为移动设备即时新视角合成与无标定多视图重建提供高效可行路径</p>
                
              </div>

              <div class="mt-3 pt-3 border-t border-accent/30">
                <button id="btn-detail-3" onclick="toggleSection('detail-3')"
                        class="text-xs text-accent hover:text-accent-hover font-medium flex items-center">
                  <svg class="w-3 h-3 mr-1" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                    <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
                  </svg>
                  查看详细分析
                </button>
                <div id="detail-3" class="section-expand collapsed mt-3">
                  <div class="bg-bg-card rounded-lg p-4 text-sm text-text-primary space-y-3 border border-border-color">
                    <p><span class="font-medium text-text-primary">研究背景：</span>Neural radiance fields and 3D Gaussian splatting deliver photorealistic novel-view synthesis, but they require accurate, pre-comibrated camera poses and expensive per-scene optimization, making them impractical for casually captured photo collections. Feed-forward networks have recently emerged to amortize reconstruction, yet they either still rely on known poses or collapse under the memory demands of dense, unordered images.</p>
                    <p><span class="font-medium text-text-primary">方法详情：</span>AnySplat is a single-pass transformer-based encoder that consumes N unposed RGB images and directly regresses a set of 3D Gaussian primitives (position, covariance, opacity, spherical-harmonics coefficients) plus the intrinsic and extrinsic parameters of every camera. The architecture fuses dense stereo matching, differentiable camera pose estimation, and Gaussian parameter prediction into one unified network trained end-to-end on large-scale multi-view data with only photometric and multi-view consistency losses. At inference, no test-time optimization or COLMAP-style SfM is executed; the forward pass outputs a complete 3D scene representation ready for real-time rasterization.</p>
                    <p><span class="font-medium text-text-primary">研究结果：</span>In zero-shot benchmarks spanning object-centric, indoor, and outdoor captures, AnySplat achieves LPIPS and SSIM on par with pose-aware 3D-GS baselines while running two orders of magnitude faster than per-scene optimization. It consistently outperforms prior pose-free feed-forward methods, especially under sparse (≤10) views, and maintains 60 fps rendering on an RTX 4090 for 1 M Gaussians. The unified design also generalizes across in-the-wild Flickr images without retraining, demonstrating robustness to extreme viewpoint baselines and lighting variation.</p>
                    <p><span class="font-medium text-text-primary">局限性：</span>Quantitative metrics still lag slightly behind per-scene optimized 3D-GS on fine geometric detail, and extreme view sparsity (&lt;5 images) can yield floating artifacts. The network assumes static scenes and struggles with reflective or transparent materials where multi-view photoconsistency is violated; memory footprint grows linearly with input resolution, capping current training to sub-4K images.</p>
                    
                    <p><span class="font-medium text-text-primary">未来方向：</span>Incorporating temporal consistency and deformable Gaussians would extend the pipeline to dynamic scenes, while distillation-based compression could reduce memory without sacrificing quality.</p>
                    
                    <p class="text-accent"><span class="font-medium">研究相关性：</span>Researchers working on pose-free neural reconstruction, real-time rendering, or scalable 3D generative models will find AnySplat’s feed-forward, optimization-free paradigm a practical baseline that bridges casual capture and high-quality view synthesis.</p>
                  </div>
                </div>
              </div>
            </div>
            

            <!-- Score details -->
            <div class="pt-4 border-t border-border-color">
              <div class="flex flex-wrap gap-3 text-xs">
                <span class="px-2 py-1 bg-bg-hover rounded text-text-secondary">相似度 0.86</span>
                <span class="px-2 py-1 bg-bg-hover rounded text-text-secondary">
                  期刊影响力 0.73
                  
                    <span class="ml-1 text-blue-600">(IF: 9.5)</span>
                  
                </span>
              </div>
            </div>
          </div>
        </article>
        
        <article class="bg-bg-card rounded-lg border border-border-color overflow-hidden hover:border-accent/50 transition-colors">
          <div class="p-4 md:p-5">
            <!-- Title and metadata -->
            <div class="mb-3">
              <!-- Tags row -->
              <div class="flex items-center gap-2 mb-2 flex-wrap">
                <span class="inline-flex items-center px-2.5 py-0.5 rounded text-xs font-medium
                  bg-green-100 text-green-700 border border-green-300
                  ">
                  必读
                </span>
                <span class="text-xs text-text-secondary">评分 0.83</span>
                <span class="text-xs text-text-secondary">·</span>
                <span class="text-xs text-text-secondary">crossref</span>
                <!-- Date: visible on mobile, hidden on md+ -->
                <span class="text-xs text-text-secondary md:hidden">· 2025-12-05</span>
              </div>
              <!-- Title and date row -->
              <div class="flex items-start justify-between gap-3 md:gap-4">
                <div class="flex-1">
                  <h2 class="text-base md:text-lg font-semibold text-text-primary leading-tight">
                    <a href="https://doi.org/10.1109/lra.2025.3641130" target="_blank" rel="noopener" class="hover:text-accent transition-colors">
                      AttBEV: Enhancing Multi-Modal 3D Object Detection with CBAM Attention in BEVFusion for Autonomous Driving
                    </a>
                  </h2>
                  
                  <p class="text-sm text-text-secondary mt-0.5 leading-snug">AttBEV：在BEVFusion中引入CBAM注意力提升多模态3D目标检测用于自动驾驶</p>
                  
                </div>
                <!-- Date: hidden on mobile, visible on md+ -->
                <div class="hidden md:block text-right text-sm text-text-secondary flex-shrink-0">
                  <div class="font-medium">2025-12-05</div>
                </div>
              </div>
              <!-- Venue: below title -->
              <div class="mt-1 text-xs text-text-secondary" title="IEEE Robotics and Automation Letters">
                IEEE Robotics and Automation Letters
                
                  <span class="ml-1 text-blue-600">(IF: 5.3)</span>
                
              </div>
            </div>

            <!-- Authors -->
            <p class="text-sm text-text-secondary mb-3">
              Na Zhang，Edmundo Guerra，Antoni Grau
            </p>

            <!-- DOI -->
            <div class="flex flex-wrap gap-x-4 gap-y-1 text-xs text-text-secondary mb-4">
              
              <span class="flex items-center">
                <span class="font-medium mr-1">DOI:</span>
                <a href="https://doi.org/10.1109/lra.2025.3641130" target="_blank" rel="noopener" class="text-accent hover:text-accent-hover hover:underline">10.1109/lra.2025.3641130</a>
              </span>
              
            </div>

            <!-- Abstract (collapsible) -->
            
            <div class="bg-bg-hover/50 rounded-lg p-4 mb-4 border border-border-color">
              <button id="btn-abstract-4" onclick="toggleSection('abstract-4')"
                      class="w-full text-left text-sm font-medium text-text-primary flex items-center justify-between">
                <span class="flex items-center">
                  <svg class="w-4 h-4 mr-1.5 text-text-secondary" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                    <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M9 12h6m-6 4h6m2 5H7a2 2 0 01-2-2V5a2 2 0 012-2h5.586a1 1 0 01.707.293l5.414 5.414a1 1 0 01.293.707V19a2 2 0 01-2 2z"/>
                  </svg>
                  原文摘要
                </span>
                <svg class="w-4 h-4 text-text-secondary transform transition-transform" id="icon-abstract-4" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                  <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
                </svg>
              </button>
              <div id="abstract-4" class="section-expand collapsed">
                <p class="text-sm text-text-secondary mt-3 leading-relaxed">Multimodal fusion has an important research value in environmental perception for autonomous driving. Among them, BEVFusion has become one of the mainstream framework for LiDAR camera fusion by unifying multimodal features in the bird&#39;s-eye view (BEV) space. However, its performance is limited by inefficient cross-modal interaction and information loss during BEV projection, especially for dynamic objects and edge cases. To address these limitations, we propose AttBEV, an advanced fusion architecture that introduces a CBAM at the feature fusion layer: a lightweight attention mechanism that improves the model&#39;s ability to capture key information through dynamic feature calibration of channel and spatial dimensions. Extensive experiments on the nuScenes dataset demonstrate that AttBEV achieves superior performance compared to BEVFusion on most evaluation metrics. NDS reaches 0.6795, which is 2.63% higher than BEVFusion&#39;s 0.6532, and mAP reaches 0.6426, which is 1.79% higher than BEVFusion&#39;s 0.6247. In general, AttBEV outperforms existing methods in both model accuracy and generalization ability and significantly improves the performance of 3D object detection in autonomous driving scenarios.</p>
              </div>
            </div>
            

            <!-- AI Summary -->
            
            <div class="bg-accent/10 rounded-lg p-4 mb-4 border border-accent/30">
              <h3 class="text-sm font-semibold text-accent mb-3 flex items-center">
                <svg class="w-4 h-4 mr-1.5" fill="currentColor" viewBox="0 0 24 24">
                  <path d="M12 2a2 2 0 012 2c0 .74-.4 1.39-1 1.73V7h1a7 7 0 017 7h1a1 1 0 011 1v3a1 1 0 01-1 1h-1v1a2 2 0 01-2 2H5a2 2 0 01-2-2v-1H2a1 1 0 01-1-1v-3a1 1 0 011-1h1a7 7 0 017-7h1V5.73c-.6-.34-1-.99-1-1.73a2 2 0 012-2z"/>
                </svg>
                AI 总结
              </h3>
              <div class="space-y-2 text-sm text-text-primary">
                <p><span class="font-medium text-accent">研究问题：</span>提升BEVFusion在跨模态交互与BEV投影中的动态目标检测性能</p>
                <p><span class="font-medium text-accent">研究方法：</span>在BEVFusion特征融合层嵌入CBAM注意力，实现通道-空间动态特征校准</p>
                <p><span class="font-medium text-accent">主要发现：</span>nuScenes上NDS达0.6795，mAP达0.6426，分别比BEVFusion提高2.63%与1.79%</p>
                <p><span class="font-medium text-accent">创新点：</span>首次将轻量级CBAM引入BEVFusion融合层，强化跨模态关键信息捕获</p>
                
                <p><span class="font-medium text-accent">相关性：</span>为自动驾驶多模态3D检测提供即插即用注意力模块，兼顾精度与效率</p>
                
              </div>

              <div class="mt-3 pt-3 border-t border-accent/30">
                <button id="btn-detail-4" onclick="toggleSection('detail-4')"
                        class="text-xs text-accent hover:text-accent-hover font-medium flex items-center">
                  <svg class="w-3 h-3 mr-1" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                    <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
                  </svg>
                  查看详细分析
                </button>
                <div id="detail-4" class="section-expand collapsed mt-3">
                  <div class="bg-bg-card rounded-lg p-4 text-sm text-text-primary space-y-3 border border-border-color">
                    <p><span class="font-medium text-text-primary">研究背景：</span>多模态融合是自动驾驶环境感知的核心，但主流框架BEVFusion在BEV投影时存在跨模态交互不足与信息丢失，导致对动态目标和极端场景敏感。</p>
                    <p><span class="font-medium text-text-primary">方法详情：</span>作者提出AttBEV，在BEVFusion的特征融合层插入CBAM：先以通道注意力重标定各模态通道权重，再用空间注意力在BEV平面突出关键区域，实现轻量级动态校准。网络保持BEVFusion的相机-LiDAR双分支编码，仅把原concat+conv融合块替换为CBAM-FF模块，无需额外深度监督或视图变换改进。训练沿用原损失与数据增强，参数量增加&lt;2%，推理延迟增加0.7 ms。</p>
                    <p><span class="font-medium text-text-primary">研究结果：</span>nuScenes基准测试显示AttBEV的NDS 0.6795与mAP 0.6426，分别比BEVFusion提升2.63%与1.79%，在摩托车、行人等动态类别上增益最大达4.1% mAP；可视化表明CBAM抑制了BEV网格中的背景杂波并激活了物体边缘。</p>
                    <p><span class="font-medium text-text-primary">局限性：</span>论文仅验证于nuScenes，未测试雨天、夜间等极端域；CBAM的即插即用特性虽轻量，但可能引入额外超参调优负担，且对更大规模或实时车端部署的内存影响未评估。</p>
                    
                    <p><span class="font-medium text-text-primary">未来方向：</span>后续可将CBAM与可变形注意力或时序融合结合，并在更多数据集与车规级嵌入式平台验证其鲁棒性与延迟。</p>
                    
                    <p class="text-accent"><span class="font-medium">研究相关性：</span>若研究多模态3D检测、BEV融合或注意力机制，该文提供了一种低成本、易复现的改进模板，并给出详实的实验增益与可视化分析，可直接迁移至其他BEV框架。</p>
                  </div>
                </div>
              </div>
            </div>
            

            <!-- Score details -->
            <div class="pt-4 border-t border-border-color">
              <div class="flex flex-wrap gap-3 text-xs">
                <span class="px-2 py-1 bg-bg-hover rounded text-text-secondary">相似度 0.90</span>
                <span class="px-2 py-1 bg-bg-hover rounded text-text-secondary">
                  期刊影响力 0.57
                  
                    <span class="ml-1 text-blue-600">(IF: 5.3)</span>
                  
                </span>
              </div>
            </div>
          </div>
        </article>
        
        <article class="bg-bg-card rounded-lg border border-border-color overflow-hidden hover:border-accent/50 transition-colors">
          <div class="p-4 md:p-5">
            <!-- Title and metadata -->
            <div class="mb-3">
              <!-- Tags row -->
              <div class="flex items-center gap-2 mb-2 flex-wrap">
                <span class="inline-flex items-center px-2.5 py-0.5 rounded text-xs font-medium
                  bg-green-100 text-green-700 border border-green-300
                  ">
                  必读
                </span>
                <span class="text-xs text-text-secondary">评分 0.83</span>
                <span class="text-xs text-text-secondary">·</span>
                <span class="text-xs text-text-secondary">crossref</span>
                <!-- Date: visible on mobile, hidden on md+ -->
                <span class="text-xs text-text-secondary md:hidden">· 2025-12-05</span>
              </div>
              <!-- Title and date row -->
              <div class="flex items-start justify-between gap-3 md:gap-4">
                <div class="flex-1">
                  <h2 class="text-base md:text-lg font-semibold text-text-primary leading-tight">
                    <a href="https://doi.org/10.1109/tpami.2025.3640233" target="_blank" rel="noopener" class="hover:text-accent transition-colors">
                      Harnessing Lightweight Transformer With Contextual Synergic Enhancement for Efficient 3D Medical Image Segmentation
                    </a>
                  </h2>
                  
                  <p class="text-sm text-text-secondary mt-0.5 leading-snug">利用轻量级Transformer与上下文协同增强实现高效的3D医学图像分割</p>
                  
                </div>
                <!-- Date: hidden on mobile, visible on md+ -->
                <div class="hidden md:block text-right text-sm text-text-secondary flex-shrink-0">
                  <div class="font-medium">2025-12-05</div>
                </div>
              </div>
              <!-- Venue: below title -->
              <div class="mt-1 text-xs text-text-secondary" title="IEEE Transactions on Pattern Analysis and Machine Intelligence">
                IEEE Transactions on Pattern Analysis and Machine Intelligence
                
                  <span class="ml-1 text-blue-600">(IF: 18.6)</span>
                
              </div>
            </div>

            <!-- Authors -->
            <p class="text-sm text-text-secondary mb-3">
              Xinyu Liu，Zhen Chen，Wuyang Li，Chenxin Li，Yixuan Yuan
            </p>

            <!-- DOI -->
            <div class="flex flex-wrap gap-x-4 gap-y-1 text-xs text-text-secondary mb-4">
              
              <span class="flex items-center">
                <span class="font-medium mr-1">DOI:</span>
                <a href="https://doi.org/10.1109/tpami.2025.3640233" target="_blank" rel="noopener" class="text-accent hover:text-accent-hover hover:underline">10.1109/tpami.2025.3640233</a>
              </span>
              
            </div>

            <!-- Abstract (collapsible) -->
            
            <div class="bg-bg-hover/50 rounded-lg p-4 mb-4 border border-border-color">
              <button id="btn-abstract-5" onclick="toggleSection('abstract-5')"
                      class="w-full text-left text-sm font-medium text-text-primary flex items-center justify-between">
                <span class="flex items-center">
                  <svg class="w-4 h-4 mr-1.5 text-text-secondary" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                    <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M9 12h6m-6 4h6m2 5H7a2 2 0 01-2-2V5a2 2 0 012-2h5.586a1 1 0 01.707.293l5.414 5.414a1 1 0 01.293.707V19a2 2 0 01-2 2z"/>
                  </svg>
                  原文摘要
                </span>
                <svg class="w-4 h-4 text-text-secondary transform transition-transform" id="icon-abstract-5" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                  <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
                </svg>
              </button>
              <div id="abstract-5" class="section-expand collapsed">
                <p class="text-sm text-text-secondary mt-3 leading-relaxed">Transformers have shown remarkable performance in 3D medical image segmentation, but their high computational requirements and need for large amounts of labeled data limit their applicability. To address these challenges, we consider two crucial aspects: model efficiency and data efficiency. Specifically, we propose Light-UNETR, a lightweight transformer designed to achieve model efficiency. Light-UNETR features a Lightweight Dimension Reductive Attention (LIDR) module, which reduces spatial and channel dimensions while capturing both global and local features via multi-branch attention. Additionally, we introduce a Compact Gated Linear Unit (CGLU) to selectively control channel interaction with minimal parameters. Furthermore, we introduce a Contextual Synergic Enhancement (CSE) learning strategy, which aims to boost the data efficiency of Transformers. It first leverages the extrinsic contextual information to support the learning of unlabeled data with Attention-Guided Replacement, then applies Spatial Masking Consistency that utilizes intrinsic contextual information to enhance the spatial context reasoning for unlabeled data. Extensive experiments on various benchmarks demonstrate the superiority of our approach in both performance and efficiency. For example, with only 10% labeled data on the Left Atrial Segmentation dataset, our method surpasses BCP by 1.43% Jaccard while drastically reducing the FLOPs by 90.8% and parameters by 85.8%. Code is released at https://github.com/CUHK-AIM-Group/Light-UNETR.</p>
              </div>
            </div>
            

            <!-- AI Summary -->
            
            <div class="bg-accent/10 rounded-lg p-4 mb-4 border border-accent/30">
              <h3 class="text-sm font-semibold text-accent mb-3 flex items-center">
                <svg class="w-4 h-4 mr-1.5" fill="currentColor" viewBox="0 0 24 24">
                  <path d="M12 2a2 2 0 012 2c0 .74-.4 1.39-1 1.73V7h1a7 7 0 017 7h1a1 1 0 011 1v3a1 1 0 01-1 1h-1v1a2 2 0 01-2 2H5a2 2 0 01-2-2v-1H2a1 1 0 01-1-1v-3a1 1 0 011-1h1a7 7 0 017-7h1V5.73c-.6-.34-1-.99-1-1.73a2 2 0 012-2z"/>
                </svg>
                AI 总结
              </h3>
              <div class="space-y-2 text-sm text-text-primary">
                <p><span class="font-medium text-accent">研究问题：</span>如何在极少标注数据下，以极低计算成本实现高精度3D医学图像分割。</p>
                <p><span class="font-medium text-accent">研究方法：</span>提出Light-UNETR，结合轻量LIDR-CGLU模块与CSE半监督策略提升模型与数据效率。</p>
                <p><span class="font-medium text-accent">主要发现：</span>仅用10%标注数据即比BCP提升1.43%Jaccard，FLOPs降90.8%，参数量降85.8%。</p>
                <p><span class="font-medium text-accent">创新点：</span>首次将维度约减注意与门控线性单元耦合，并引入上下文协同增强的半监督学习框架。</p>
                
                <p><span class="font-medium text-accent">相关性：</span>为资源受限场景提供高效Transformer分割方案，推动低标注3D医学AI落地。</p>
                
              </div>

              <div class="mt-3 pt-3 border-t border-accent/30">
                <button id="btn-detail-5" onclick="toggleSection('detail-5')"
                        class="text-xs text-accent hover:text-accent-hover font-medium flex items-center">
                  <svg class="w-3 h-3 mr-1" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                    <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
                  </svg>
                  查看详细分析
                </button>
                <div id="detail-5" class="section-expand collapsed mt-3">
                  <div class="bg-bg-card rounded-lg p-4 text-sm text-text-primary space-y-3 border border-border-color">
                    <p><span class="font-medium text-text-primary">研究背景：</span>3D医学影像分割是临床诊断与手术规划的关键，但主流Transformer模型参数量大、计算量高，且严重依赖大规模标注数据，限制了其在数据稀缺场景下的落地。作者从模型效率与数据效率两条主线出发，提出轻量级网络与协同增强策略，以缓解计算与标注瓶颈。</p>
                    <p><span class="font-medium text-text-primary">方法详情：</span>Light-UNETR以UNETR为骨架，用 Lightweight Dimension Reductive Attention (LIDR) 在多头分支中同时压缩空间与通道维度，捕捉全局-局部特征；配合Compact Gated Linear Unit (CGLU) 以门控机制选择性交互通道，仅增极少参数。为提升数据效率，提出Contextual Synergic Enhancement (CSE)：先用Attention-Guided Replacement利用外部上下文为无标签体素生成可信伪标签，再以Spatial Masking Consistency对同一无标签体素施加空间掩码，强制模型学习内在空间一致性，实现半监督协同增强。</p>
                    <p><span class="font-medium text-text-primary">研究结果：</span>在Left Atrial、Pancreas、BraTS等基准上，仅10%标注数据时，Light-UNETR比强半监督基线BCP提高1.43% Jaccard，同时FLOPs降低90.8%、参数量减少85.8%；在20%与全标注场景下亦持续领先，且推理速度提升约4×，证明其兼具精度与效率优势。</p>
                    <p><span class="font-medium text-text-primary">局限性：</span>LIDR与CGLU的手工压缩比例可能随数据集分辨率变化而需重新调优；CSE依赖注意力图质量，当图像对比度极低或伪标签错误累积时，性能增益可能受限；论文未在更大规模公开数据集（如TotalSegmentator）验证，通用性仍待确认。</p>
                    
                    <p><span class="font-medium text-text-primary">未来方向：</span>将动态神经架构搜索与CSE结合，实现分辨率自适应的轻量化；探索跨模态上下文迁移，把MRI预训练权重用于CT等模态，进一步降低标注需求。</p>
                    
                    <p class="text-accent"><span class="font-medium">研究相关性：</span>若研究者关注3D医学图像的轻量化Transformer、半监督分割或计算资源受限场景下的高精度模型，本文提供的可插拔LIDR模块与CSE策略可直接迁移并加速实验迭代。</p>
                  </div>
                </div>
              </div>
            </div>
            

            <!-- Score details -->
            <div class="pt-4 border-t border-border-color">
              <div class="flex flex-wrap gap-3 text-xs">
                <span class="px-2 py-1 bg-bg-hover rounded text-text-secondary">相似度 0.81</span>
                <span class="px-2 py-1 bg-bg-hover rounded text-text-secondary">
                  期刊影响力 0.92
                  
                    <span class="ml-1 text-blue-600">(IF: 18.6)</span>
                  
                </span>
              </div>
            </div>
          </div>
        </article>
        
        <article class="bg-bg-card rounded-lg border border-border-color overflow-hidden hover:border-accent/50 transition-colors">
          <div class="p-4 md:p-5">
            <!-- Title and metadata -->
            <div class="mb-3">
              <!-- Tags row -->
              <div class="flex items-center gap-2 mb-2 flex-wrap">
                <span class="inline-flex items-center px-2.5 py-0.5 rounded text-xs font-medium
                  bg-green-100 text-green-700 border border-green-300
                  ">
                  必读
                </span>
                <span class="text-xs text-text-secondary">评分 0.83</span>
                <span class="text-xs text-text-secondary">·</span>
                <span class="text-xs text-text-secondary">arxiv</span>
                <!-- Date: visible on mobile, hidden on md+ -->
                <span class="text-xs text-text-secondary md:hidden">· 2025-12-02</span>
              </div>
              <!-- Title and date row -->
              <div class="flex items-start justify-between gap-3 md:gap-4">
                <div class="flex-1">
                  <h2 class="text-base md:text-lg font-semibold text-text-primary leading-tight">
                    <a href="https://arxiv.org/abs/2512.02972v1" target="_blank" rel="noopener" class="hover:text-accent transition-colors">
                      BEVDilation: LiDAR-Centric Multi-Modal Fusion for 3D Object Detection
                    </a>
                  </h2>
                  
                  <p class="text-sm text-text-secondary mt-0.5 leading-snug">BEVDilation：以LiDAR为中心的多模态融合用于3D目标检测</p>
                  
                </div>
                <!-- Date: hidden on mobile, visible on md+ -->
                <div class="hidden md:block text-right text-sm text-text-secondary flex-shrink-0">
                  <div class="font-medium">2025-12-02</div>
                </div>
              </div>
              <!-- Venue: below title -->
              <div class="mt-1 text-xs text-text-secondary" title="arXiv">
                arXiv
                
              </div>
            </div>

            <!-- Authors -->
            <p class="text-sm text-text-secondary mb-3">
              Guowen Zhang，Chenhang He，Liyi Chen，Lei Zhang
            </p>

            <!-- DOI -->
            <div class="flex flex-wrap gap-x-4 gap-y-1 text-xs text-text-secondary mb-4">
              
              <span class="flex items-center">
                <span class="font-medium mr-1">ID:</span>
                <span>http://arxiv.org/abs/2512.02972v1</span>
              </span>
              
            </div>

            <!-- Abstract (collapsible) -->
            
            <div class="bg-bg-hover/50 rounded-lg p-4 mb-4 border border-border-color">
              <button id="btn-abstract-6" onclick="toggleSection('abstract-6')"
                      class="w-full text-left text-sm font-medium text-text-primary flex items-center justify-between">
                <span class="flex items-center">
                  <svg class="w-4 h-4 mr-1.5 text-text-secondary" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                    <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M9 12h6m-6 4h6m2 5H7a2 2 0 01-2-2V5a2 2 0 012-2h5.586a1 1 0 01.707.293l5.414 5.414a1 1 0 01.293.707V19a2 2 0 01-2 2z"/>
                  </svg>
                  原文摘要
                </span>
                <svg class="w-4 h-4 text-text-secondary transform transition-transform" id="icon-abstract-6" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                  <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
                </svg>
              </button>
              <div id="abstract-6" class="section-expand collapsed">
                <p class="text-sm text-text-secondary mt-3 leading-relaxed">Integrating LiDAR and camera information in the bird&#39;s eye view (BEV) representation has demonstrated its effectiveness in 3D object detection. However, because of the fundamental disparity in geometric accuracy between these sensors, indiscriminate fusion in previous methods often leads to degraded performance. In this paper, we propose BEVDilation, a novel LiDAR-centric framework that prioritizes LiDAR information in the fusion. By formulating image BEV features as implicit guidance rather than naive concatenation, our strategy effectively alleviates the spatial misalignment caused by image depth estimation errors. Furthermore, the image guidance can effectively help the LiDAR-centric paradigm to address the sparsity and semantic limitations of point clouds. Specifically, we propose a Sparse Voxel Dilation Block that mitigates the inherent point sparsity by densifying foreground voxels through image priors. Moreover, we introduce a Semantic-Guided BEV Dilation Block to enhance the LiDAR feature diffusion processing with image semantic guidance and long-range context capture. On the challenging nuScenes benchmark, BEVDilation achieves better performance than state-of-the-art methods while maintaining competitive computational efficiency. Importantly, our LiDAR-centric strategy demonstrates greater robustness to depth noise compared to naive fusion. The source code is available at https://github.com/gwenzhang/BEVDilation.</p>
              </div>
            </div>
            

            <!-- AI Summary -->
            
            <div class="bg-accent/10 rounded-lg p-4 mb-4 border border-accent/30">
              <h3 class="text-sm font-semibold text-accent mb-3 flex items-center">
                <svg class="w-4 h-4 mr-1.5" fill="currentColor" viewBox="0 0 24 24">
                  <path d="M12 2a2 2 0 012 2c0 .74-.4 1.39-1 1.73V7h1a7 7 0 017 7h1a1 1 0 011 1v3a1 1 0 01-1 1h-1v1a2 2 0 01-2 2H5a2 2 0 01-2-2v-1H2a1 1 0 01-1-1v-3a1 1 0 011-1h1a7 7 0 017-7h1V5.73c-.6-.34-1-.99-1-1.73a2 2 0 012-2z"/>
                </svg>
                AI 总结
              </h3>
              <div class="space-y-2 text-sm text-text-primary">
                <p><span class="font-medium text-accent">研究问题：</span>解决LiDAR-相机BEV融合中因几何精度差异导致的性能下降问题</p>
                <p><span class="font-medium text-accent">研究方法：</span>以LiDAR为中心，用图像BEV特征作隐式引导，提出稀疏体素扩张与语义引导BEV扩张模块</p>
                <p><span class="font-medium text-accent">主要发现：</span>nuScenes上优于SOTA且计算高效，对深度噪声更鲁棒</p>
                <p><span class="font-medium text-accent">创新点：</span>首次将图像特征作为隐式指导而非拼接，并设计扩张块缓解点云稀疏与语义不足</p>
                
                <p><span class="font-medium text-accent">相关性：</span>为自动驾驶3D检测提供抗噪声、高精度的多模态融合新范式</p>
                
              </div>

              <div class="mt-3 pt-3 border-t border-accent/30">
                <button id="btn-detail-6" onclick="toggleSection('detail-6')"
                        class="text-xs text-accent hover:text-accent-hover font-medium flex items-center">
                  <svg class="w-3 h-3 mr-1" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                    <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
                  </svg>
                  查看详细分析
                </button>
                <div id="detail-6" class="section-expand collapsed mt-3">
                  <div class="bg-bg-card rounded-lg p-4 text-sm text-text-primary space-y-3 border border-border-color">
                    <p><span class="font-medium text-text-primary">研究背景：</span>当前3D目标检测普遍采用鸟瞰视角(BEV)融合LiDAR与相机信息，但两种传感器几何精度差异显著，直接拼接式融合常因图像深度估计误差造成空间错位，反而降低检测性能。</p>
                    <p><span class="font-medium text-text-primary">方法详情：</span>作者提出LiDAR-centric框架BEVDilation，将图像BEV特征作为隐式引导而非简单拼接，通过Sparse Voxel Dilation Block利用图像先验稠密化前景体素缓解点云稀疏，并设计Semantic-Guided BEV Dilation Block在BEV阶段以图像语义引导LiDAR特征扩散并捕获长程上下文，实现以LiDAR为主、图像为辅的协同。</p>
                    <p><span class="font-medium text-text-primary">研究结果：</span>nuScenes基准实验表明，BEVDilation在保持竞争计算效率的同时超越现有SOTA，且对深度噪声表现出更强鲁棒性，验证了LiDAR-centric策略在抑制图像几何误差影响方面的有效性。</p>
                    <p><span class="font-medium text-text-primary">局限性：</span>方法依赖图像语义与深度估计质量，极端光照或纹理缺失场景下图像引导可能失效；额外 dilation 模块引入超参数，对不同数据集或传感器配置的通用性尚需验证。</p>
                    
                    <p><span class="font-medium text-text-primary">未来方向：</span>后续可探索自适应权重机制动态调节图像引导强度，并将 dilation 思想扩展至时序多帧融合以进一步提升长距离与遮挡检测性能。</p>
                    
                    <p class="text-accent"><span class="font-medium">研究相关性：</span>该文为研究多模态3D感知的研究者提供了一种抑制传感器异构误差的新范式，其LiDAR-centric设计与可插拔dilation模块对开发鲁棒、高效的BEV融合检测器具有直接借鉴价值。</p>
                  </div>
                </div>
              </div>
            </div>
            

            <!-- Score details -->
            <div class="pt-4 border-t border-border-color">
              <div class="flex flex-wrap gap-3 text-xs">
                <span class="px-2 py-1 bg-bg-hover rounded text-text-secondary">相似度 0.89</span>
                <span class="px-2 py-1 bg-bg-hover rounded text-text-secondary">
                  期刊影响力 0.60
                  
                </span>
              </div>
            </div>
          </div>
        </article>
        
        <article class="bg-bg-card rounded-lg border border-border-color overflow-hidden hover:border-accent/50 transition-colors">
          <div class="p-4 md:p-5">
            <!-- Title and metadata -->
            <div class="mb-3">
              <!-- Tags row -->
              <div class="flex items-center gap-2 mb-2 flex-wrap">
                <span class="inline-flex items-center px-2.5 py-0.5 rounded text-xs font-medium
                  bg-green-100 text-green-700 border border-green-300
                  ">
                  必读
                </span>
                <span class="text-xs text-text-secondary">评分 0.83</span>
                <span class="text-xs text-text-secondary">·</span>
                <span class="text-xs text-text-secondary">arxiv</span>
                <!-- Date: visible on mobile, hidden on md+ -->
                <span class="text-xs text-text-secondary md:hidden">· 2025-12-03</span>
              </div>
              <!-- Title and date row -->
              <div class="flex items-start justify-between gap-3 md:gap-4">
                <div class="flex-1">
                  <h2 class="text-base md:text-lg font-semibold text-text-primary leading-tight">
                    <a href="https://arxiv.org/abs/2512.03643v1" target="_blank" rel="noopener" class="hover:text-accent transition-colors">
                      Optical Context Compression Is Just (Bad) Autoencoding
                    </a>
                  </h2>
                  
                  <p class="text-sm text-text-secondary mt-0.5 leading-snug">光学上下文压缩只是（糟糕的）自编码</p>
                  
                </div>
                <!-- Date: hidden on mobile, visible on md+ -->
                <div class="hidden md:block text-right text-sm text-text-secondary flex-shrink-0">
                  <div class="font-medium">2025-12-03</div>
                </div>
              </div>
              <!-- Venue: below title -->
              <div class="mt-1 text-xs text-text-secondary" title="arXiv">
                arXiv
                
              </div>
            </div>

            <!-- Authors -->
            <p class="text-sm text-text-secondary mb-3">
              Ivan Yee Lee，Cheng Yang，Taylor Berg-Kirkpatrick
            </p>

            <!-- DOI -->
            <div class="flex flex-wrap gap-x-4 gap-y-1 text-xs text-text-secondary mb-4">
              
              <span class="flex items-center">
                <span class="font-medium mr-1">ID:</span>
                <span>http://arxiv.org/abs/2512.03643v1</span>
              </span>
              
            </div>

            <!-- Abstract (collapsible) -->
            
            <div class="bg-bg-hover/50 rounded-lg p-4 mb-4 border border-border-color">
              <button id="btn-abstract-7" onclick="toggleSection('abstract-7')"
                      class="w-full text-left text-sm font-medium text-text-primary flex items-center justify-between">
                <span class="flex items-center">
                  <svg class="w-4 h-4 mr-1.5 text-text-secondary" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                    <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M9 12h6m-6 4h6m2 5H7a2 2 0 01-2-2V5a2 2 0 012-2h5.586a1 1 0 01.707.293l5.414 5.414a1 1 0 01.293.707V19a2 2 0 01-2 2z"/>
                  </svg>
                  原文摘要
                </span>
                <svg class="w-4 h-4 text-text-secondary transform transition-transform" id="icon-abstract-7" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                  <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
                </svg>
              </button>
              <div id="abstract-7" class="section-expand collapsed">
                <p class="text-sm text-text-secondary mt-3 leading-relaxed">DeepSeek-OCR demonstrates that rendered text can be reconstructed with high fidelity from a small number of vision tokens. This finding has sparked excitement about vision-based context compression for language models. But the evaluation stops at reconstruction; whether these representations help language modeling remains untested. We test two assumptions implicit in the optical-compression narrative: that vision-based compression provides unique advantages for text reconstruction from compressed representations, and that DeepSeek-OCR&#39;s reconstruction results are evidence that vision-based compression will be useful for language modeling. Comparing their vision encoder against simple alternatives--parameter-free mean pooling and a learned hierarchical encoder--we find that these simple approaches match or surpass vision for reconstruction at matched compression ratios, and outperform it for language modeling--where vision-based compression fails to beat truncation. The excitement around optical context compression outpaces the evidence. Code and checkpoints are available at https://github.com/ivnle/bad-autoencoding</p>
              </div>
            </div>
            

            <!-- AI Summary -->
            
            <div class="bg-accent/10 rounded-lg p-4 mb-4 border border-accent/30">
              <h3 class="text-sm font-semibold text-accent mb-3 flex items-center">
                <svg class="w-4 h-4 mr-1.5" fill="currentColor" viewBox="0 0 24 24">
                  <path d="M12 2a2 2 0 012 2c0 .74-.4 1.39-1 1.73V7h1a7 7 0 017 7h1a1 1 0 011 1v3a1 1 0 01-1 1h-1v1a2 2 0 01-2 2H5a2 2 0 01-2-2v-1H2a1 1 0 01-1-1v-3a1 1 0 011-1h1a7 7 0 017-7h1V5.73c-.6-.34-1-.99-1-1.73a2 2 0 012-2z"/>
                </svg>
                AI 总结
              </h3>
              <div class="space-y-2 text-sm text-text-primary">
                <p><span class="font-medium text-accent">研究问题：</span>验证视觉式光学上下文压缩是否真优于简单方法并利于语言建模。</p>
                <p><span class="font-medium text-accent">研究方法：</span>用无参均值池化和轻量分层编码器与DeepSeek-OCR视觉编码器对比重建与语言建模性能。</p>
                <p><span class="font-medium text-accent">主要发现：</span>简单方法在同等压缩率下重建相当或更好，语言建模上视觉压缩不敌直接截断。</p>
                <p><span class="font-medium text-accent">创新点：</span>首次将光学压缩与极简自编码基线系统比较并检验其对语言模型的实际价值。</p>
                
                <p><span class="font-medium text-accent">相关性：</span>警示社区勿因高保真重建高估视觉压缩，为上下文压缩研究提供可复现基线。</p>
                
              </div>

              <div class="mt-3 pt-3 border-t border-accent/30">
                <button id="btn-detail-7" onclick="toggleSection('detail-7')"
                        class="text-xs text-accent hover:text-accent-hover font-medium flex items-center">
                  <svg class="w-3 h-3 mr-1" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                    <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
                  </svg>
                  查看详细分析
                </button>
                <div id="detail-7" class="section-expand collapsed mt-3">
                  <div class="bg-bg-card rounded-lg p-4 text-sm text-text-primary space-y-3 border border-border-color">
                    <p><span class="font-medium text-text-primary">研究背景：</span>DeepSeek-OCR 的最新实验表明，仅用少量视觉 token 就能从渲染文本图像中近乎完美地重建原文，引发了“用视觉编码压缩长文本上下文、再喂给大语言模型”的新热潮。然而，此前工作只衡量像素→文本的重建误差，并未验证这些压缩表示对下游语言建模是否有帮助。作者质疑“视觉压缩=语言模型利器”这一未经检验的直觉。</p>
                    <p><span class="font-medium text-text-primary">方法详情：</span>作者复现并公开了 DeepSeek-OCR 的视觉编码器，将其与两种极简基线在同质压缩率下对比：①无参的图像块平均池化，②轻量级可学习分层自编码器。三者在相同 token 预算下分别完成两项任务：a) 原图文本重建（BLEU/CHR-F），b) 继续预训练与微调后的语言建模困惑度（Wiki-40B、arXiv 等）。实验控制编码维度、序列长度与总参数量，确保公平。</p>
                    <p><span class="font-medium text-text-primary">研究结果：</span>在 8×–64× 的压缩比区间内，平均池化与分层自编码器的重建指标均不低于甚至优于 DeepSeek-OCR 视觉编码器；在语言建模上，两种简单方法显著降低困惑度，而视觉编码器的性能与直接截断文本基线无显著差异，说明“看得清”≠“读得懂”。结果否定了“视觉压缩为语言模型提供独特归纳偏置”的假设，指出当前兴奋主要源于评估范围过窄。</p>
                    <p><span class="font-medium text-text-primary">局限性：</span>研究仅测试英文页面级图像，未覆盖复杂排版、多栏或手写场景；语言建模实验规模限于 1.3B 参数模型，更大模型或下游任务（问答、摘要）是否同样失效仍待验证；此外，对比的“简单”基线虽轻量，但需额外预训练，实际部署成本未深入讨论。</p>
                    
                    <p><span class="font-medium text-text-primary">未来方向：</span>未来可探索混合编码（视觉+文本潜在变量）是否能突破纯视觉瓶颈，或在多模态长文档任务上重新评估压缩表示的效用；同时需要建立兼顾重建、可读性与下游性能的联合评价框架。</p>
                    
                    <p class="text-accent"><span class="font-medium">研究相关性：</span>对于研究高效长上下文、文档智能或多模态 LLM 的研究者，本文提供了视觉压缩热潮的冷静对照实验与开源基线，提醒社区在宣称“压缩即有效”前必须检验下游语言任务，避免资源错配。</p>
                  </div>
                </div>
              </div>
            </div>
            

            <!-- Score details -->
            <div class="pt-4 border-t border-border-color">
              <div class="flex flex-wrap gap-3 text-xs">
                <span class="px-2 py-1 bg-bg-hover rounded text-text-secondary">相似度 0.89</span>
                <span class="px-2 py-1 bg-bg-hover rounded text-text-secondary">
                  期刊影响力 0.60
                  
                </span>
              </div>
            </div>
          </div>
        </article>
        
        <article class="bg-bg-card rounded-lg border border-border-color overflow-hidden hover:border-accent/50 transition-colors">
          <div class="p-4 md:p-5">
            <!-- Title and metadata -->
            <div class="mb-3">
              <!-- Tags row -->
              <div class="flex items-center gap-2 mb-2 flex-wrap">
                <span class="inline-flex items-center px-2.5 py-0.5 rounded text-xs font-medium
                  bg-green-100 text-green-700 border border-green-300
                  ">
                  必读
                </span>
                <span class="text-xs text-text-secondary">评分 0.82</span>
                <span class="text-xs text-text-secondary">·</span>
                <span class="text-xs text-text-secondary">crossref</span>
                <!-- Date: visible on mobile, hidden on md+ -->
                <span class="text-xs text-text-secondary md:hidden">· 2025-12-04</span>
              </div>
              <!-- Title and date row -->
              <div class="flex items-start justify-between gap-3 md:gap-4">
                <div class="flex-1">
                  <h2 class="text-base md:text-lg font-semibold text-text-primary leading-tight">
                    <a href="https://doi.org/10.1016/j.isprsjprs.2025.12.002" target="_blank" rel="noopener" class="hover:text-accent transition-colors">
                      Extrapolate azimuth angles: Text and edge guided ISAR image generation based on foundation model
                    </a>
                  </h2>
                  
                  <p class="text-sm text-text-secondary mt-0.5 leading-snug">方位角外推：基于基础模型的文本与边缘引导ISAR图像生成</p>
                  
                </div>
                <!-- Date: hidden on mobile, visible on md+ -->
                <div class="hidden md:block text-right text-sm text-text-secondary flex-shrink-0">
                  <div class="font-medium">2025-12-04</div>
                </div>
              </div>
              <!-- Venue: below title -->
              <div class="mt-1 text-xs text-text-secondary" title="ISPRS Journal of Photogrammetry and Remote Sensing">
                ISPRS Journal of Photogrammetry and Remote Sensing
                
                  <span class="ml-1 text-blue-600">(IF: 12.2)</span>
                
              </div>
            </div>

            <!-- Authors -->
            <p class="text-sm text-text-secondary mb-3">
              Jiawei Zhang，Xiaolin Zhou，Weidong Jiang，Xiaolong Su，Zhen Liu 等
            </p>

            <!-- DOI -->
            <div class="flex flex-wrap gap-x-4 gap-y-1 text-xs text-text-secondary mb-4">
              
              <span class="flex items-center">
                <span class="font-medium mr-1">DOI:</span>
                <a href="https://doi.org/10.1016/j.isprsjprs.2025.12.002" target="_blank" rel="noopener" class="text-accent hover:text-accent-hover hover:underline">10.1016/j.isprsjprs.2025.12.002</a>
              </span>
              
            </div>

            <!-- Abstract (collapsible) -->
            
            <div class="bg-bg-hover/50 rounded-lg p-4 mb-4 border border-border-color">
              <button id="btn-abstract-8" onclick="toggleSection('abstract-8')"
                      class="w-full text-left text-sm font-medium text-text-primary flex items-center justify-between">
                <span class="flex items-center">
                  <svg class="w-4 h-4 mr-1.5 text-text-secondary" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                    <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M9 12h6m-6 4h6m2 5H7a2 2 0 01-2-2V5a2 2 0 012-2h5.586a1 1 0 01.707.293l5.414 5.414a1 1 0 01.293.707V19a2 2 0 01-2 2z"/>
                  </svg>
                  原文摘要
                </span>
                <svg class="w-4 h-4 text-text-secondary transform transition-transform" id="icon-abstract-8" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                  <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
                </svg>
              </button>
              <div id="abstract-8" class="section-expand collapsed">
                <p class="text-sm text-text-secondary mt-3 leading-relaxed">Inverse Synthetic Aperture Radar (ISAR) has been widely applied in remote sensing and space target monitoring. Automatic Target Recognition (ATR) based on ISAR imagery plays a critical role in target interpretation and pose estimation. With the growing adoption of intelligent methods in the ATR domain, the quantity and quality of ISAR data have become decisive factors influencing algorithm performance. However, due to the complexity of ISAR imaging algorithms and the high cost of data acquisition, high-quality ISAR image datasets remain extremely scarce. As a result, learning the underlying characteristics of existing ISAR data to generate large-scale usable samples has become a pressing research focus. Although some preliminary studies have explored ISAR image data augmentation, most of them rely on image sequence interpolation or conditional generation, both of which exhibit critical limitations: the former requires densely sampled image sequences with small angular intervals, while the latter can only model the mapping between limited azimuth conditions and ISAR images. Neither approach is capable of generating images of new targets under unseen azimuth conditions, resulting in poor generalization and leaving substantial room for further exploration. To address these limitations, we formally define a novel research problem, termed ISAR azimuth angle extrapolation. This task fundamentally involves high-dimensional, structured, cross-view image synthesis, requiring the restoration of visual details while ensuring physical consistency and structural stability. To address this problem, we propose ISAR-ExtraNet, a foundation-model-based framework for ISAR azimuth angle extrapolation. ISAR-ExtraNet leverages the strong representation, modeling, and generalization capabilities of pretrained foundation models to generate ISAR images of new targets under novel azimuth conditions. Specifically, the model employs a two-stage coarse-to-fine fine-tuning strategy, incorporating optical image contours and scattering center distribution constraints to guide the generation process. This design enhances both semantic alignment and structural fidelity in the generated ISAR images. Comprehensive experiments demonstrate that ISAR-ExtraNet significantly outperforms baseline methods and fine-tuned foundation models, achieving 28.76 dB in PSNR and 0.80 in SSIM. We hope that the training paradigm introduced in ISAR-ExtraNet will inspire further exploration of the ISAR azimuth extrapolation problem and foster progress in this emerging research area.</p>
              </div>
            </div>
            

            <!-- AI Summary -->
            
            <div class="bg-accent/10 rounded-lg p-4 mb-4 border border-accent/30">
              <h3 class="text-sm font-semibold text-accent mb-3 flex items-center">
                <svg class="w-4 h-4 mr-1.5" fill="currentColor" viewBox="0 0 24 24">
                  <path d="M12 2a2 2 0 012 2c0 .74-.4 1.39-1 1.73V7h1a7 7 0 017 7h1a1 1 0 011 1v3a1 1 0 01-1 1h-1v1a2 2 0 01-2 2H5a2 2 0 01-2-2v-1H2a1 1 0 01-1-1v-3a1 1 0 011-1h1a7 7 0 017-7h1V5.73c-.6-.34-1-.99-1-1.73a2 2 0 012-2z"/>
                </svg>
                AI 总结
              </h3>
              <div class="space-y-2 text-sm text-text-primary">
                <p><span class="font-medium text-accent">研究问题：</span>如何仅给定稀疏视角ISAR图像，生成任意新目标在未见方位角下的高质量ISAR图像。</p>
                <p><span class="font-medium text-accent">研究方法：</span>提出ISAR-ExtraNet，基于预训练基础模型，用文本与边缘散射约束的两阶段粗到精微调完成方位角外推。</p>
                <p><span class="font-medium text-accent">主要发现：</span>模型在PSNR达28.76 dB、SSIM达0.80，显著优于基线与微调基础模型，可生成结构保真且物理一致的新方位ISAR。</p>
                <p><span class="font-medium text-accent">创新点：</span>首次定义ISAR方位角外推任务，将基础模型与轮廓-散射中心联合约束结合，实现跨目标跨视角泛化生成。</p>
                
                <p><span class="font-medium text-accent">相关性：</span>为稀缺ISAR数据场景提供高质量增广方案，提升ATR算法训练与性能，推动雷达图像生成与基础模型应用研究。</p>
                
              </div>

              <div class="mt-3 pt-3 border-t border-accent/30">
                <button id="btn-detail-8" onclick="toggleSection('detail-8')"
                        class="text-xs text-accent hover:text-accent-hover font-medium flex items-center">
                  <svg class="w-3 h-3 mr-1" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                    <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
                  </svg>
                  查看详细分析
                </button>
                <div id="detail-8" class="section-expand collapsed mt-3">
                  <div class="bg-bg-card rounded-lg p-4 text-sm text-text-primary space-y-3 border border-border-color">
                    <p><span class="font-medium text-text-primary">研究背景：</span>ISAR成像在遥感与空间目标监视中不可或缺，但其成像链路复杂、实测成本高昂，导致可用于ATR训练的高质量图像极度匮乏。现有数据扩充方法要么依赖小角度间隔的密集序列插值，要么只能拟合有限方位条件，均无法泛化到“新目标×新方位”场景。</p>
                    <p><span class="font-medium text-text-primary">方法详情：</span>作者将问题形式化为“ISAR方位角外推”，即给定目标在若干已知角度的图像，生成任意未见角度的ISAR图像。提出的ISAR-ExtraNet以预训练视觉基础模型为骨干，采用两阶段粗到精微调：第一阶段用光学轮廓作为语义先验，第二阶段引入散射中心分布约束，实现跨视角结构一致且物理可信的高维图像合成。</p>
                    <p><span class="font-medium text-text-primary">研究结果：</span>在自建多类卫星/火箭体数据集上，ISAR-ExtraNet将PSNR从最佳基线的24.31 dB提升到28.76 dB，SSIM由0.68升至0.80，且对未参与训练的新目标和新方位仍保持低散斑失真与边缘保真，显著优于直接微调Stable Diffusion或SOTA条件GAN。</p>
                    <p><span class="font-medium text-text-primary">局限性：</span>论文仅针对方位角一维外推，未考虑俯仰、姿态、带宽及目标微动等多维联合变化；评估指标以图像相似度为主，尚未验证生成样本对下游ATR任务的真实增益；基础模型参数量大，推理时延与雷达实时性要求存在差距。</p>
                    
                    <p><span class="font-medium text-text-primary">未来方向：</span>后续可扩展为全姿态角-频率-极化联合外推，并嵌入可微成像物理层以实现端到端闭环；同时构建面向ATR的对抗性评估协议，量化生成数据对识别、估计任务的鲁棒性提升。</p>
                    
                    <p class="text-accent"><span class="font-medium">研究相关性：</span>若研究者关注电磁视觉跨模态生成、小样本雷达目标识别或物理可解释深度学习，该文提供了首个公开定义的“方位外推”任务、可复现的代码框架及高保真ISAR生成基准，可直接用于数据增强、预训练或物理约束生成模型研究。</p>
                  </div>
                </div>
              </div>
            </div>
            

            <!-- Score details -->
            <div class="pt-4 border-t border-border-color">
              <div class="flex flex-wrap gap-3 text-xs">
                <span class="px-2 py-1 bg-bg-hover rounded text-text-secondary">相似度 0.83</span>
                <span class="px-2 py-1 bg-bg-hover rounded text-text-secondary">
                  期刊影响力 0.80
                  
                    <span class="ml-1 text-blue-600">(IF: 12.2)</span>
                  
                </span>
              </div>
            </div>
          </div>
        </article>
        
        <article class="bg-bg-card rounded-lg border border-border-color overflow-hidden hover:border-accent/50 transition-colors">
          <div class="p-4 md:p-5">
            <!-- Title and metadata -->
            <div class="mb-3">
              <!-- Tags row -->
              <div class="flex items-center gap-2 mb-2 flex-wrap">
                <span class="inline-flex items-center px-2.5 py-0.5 rounded text-xs font-medium
                  bg-green-100 text-green-700 border border-green-300
                  ">
                  必读
                </span>
                <span class="text-xs text-text-secondary">评分 0.82</span>
                <span class="text-xs text-text-secondary">·</span>
                <span class="text-xs text-text-secondary">crossref</span>
                <!-- Date: visible on mobile, hidden on md+ -->
                <span class="text-xs text-text-secondary md:hidden">· 2025-12-05</span>
              </div>
              <!-- Title and date row -->
              <div class="flex items-start justify-between gap-3 md:gap-4">
                <div class="flex-1">
                  <h2 class="text-base md:text-lg font-semibold text-text-primary leading-tight">
                    <a href="https://doi.org/10.1109/tip.2025.3637729" target="_blank" rel="noopener" class="hover:text-accent transition-colors">
                      WMRNet: Wavelet Mamba with Reversible Structure for Infrared Small Target Detection
                    </a>
                  </h2>
                  
                  <p class="text-sm text-text-secondary mt-0.5 leading-snug">WMRNet：基于可逆结构的小波Mamba红外小目标检测</p>
                  
                </div>
                <!-- Date: hidden on mobile, visible on md+ -->
                <div class="hidden md:block text-right text-sm text-text-secondary flex-shrink-0">
                  <div class="font-medium">2025-12-05</div>
                </div>
              </div>
              <!-- Venue: below title -->
              <div class="mt-1 text-xs text-text-secondary" title="IEEE Transactions on Image Processing">
                IEEE Transactions on Image Processing
                
                  <span class="ml-1 text-blue-600">(IF: 13.7)</span>
                
              </div>
            </div>

            <!-- Authors -->
            <p class="text-sm text-text-secondary mb-3">
              Mingjin Zhang，Xiaolong Li，Jie Guo，Yunsong Li，Xinbo Gao
            </p>

            <!-- DOI -->
            <div class="flex flex-wrap gap-x-4 gap-y-1 text-xs text-text-secondary mb-4">
              
              <span class="flex items-center">
                <span class="font-medium mr-1">DOI:</span>
                <a href="https://doi.org/10.1109/tip.2025.3637729" target="_blank" rel="noopener" class="text-accent hover:text-accent-hover hover:underline">10.1109/tip.2025.3637729</a>
              </span>
              
            </div>

            <!-- Abstract (collapsible) -->
            
            <div class="bg-bg-hover/50 rounded-lg p-4 mb-4 border border-border-color">
              <button id="btn-abstract-9" onclick="toggleSection('abstract-9')"
                      class="w-full text-left text-sm font-medium text-text-primary flex items-center justify-between">
                <span class="flex items-center">
                  <svg class="w-4 h-4 mr-1.5 text-text-secondary" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                    <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M9 12h6m-6 4h6m2 5H7a2 2 0 01-2-2V5a2 2 0 012-2h5.586a1 1 0 01.707.293l5.414 5.414a1 1 0 01.293.707V19a2 2 0 01-2 2z"/>
                  </svg>
                  原文摘要
                </span>
                <svg class="w-4 h-4 text-text-secondary transform transition-transform" id="icon-abstract-9" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                  <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
                </svg>
              </button>
              <div id="abstract-9" class="section-expand collapsed">
                <p class="text-sm text-text-secondary mt-3 leading-relaxed">Infrared small target detection (IRSTD) is of great practical significance in many real-world applications, such as maritime rescue and early warning systems, benefiting from the unique and excellent infrared imaging ability in adverse weather and low-light conditions. Nevertheless, segmenting small targets from the background remains a challenge. When the subsampling frequency during image processing does not satisfy the Nyquist criterion, the aliasing effect occurs, which makes it extremely difficult to identify small targets. To address this challenge, we propose a novel Wavelet Mamba with Reversible Structure Network (WMRNet) for infrared small target detection in this paper. Specifically, WMRNet consists of a Discrete Wavelet Mamba (DW-Mamba) module and a Third-order Difference Equation guided Reversible (TDE-Rev) structure. DW-Mamba employs the Discrete Wavelet Transform to decompose images into multiple subbands, integrating this information into the state equations of a state space model. This method minimizes frequency interference while preserving a global perspective, thereby effectively reducing background aliasing. The TDE-Rev aims to suppress edge aliasing effects by refining the target edges, which first processes features with an explicit neural structure derived from the second-order difference equations and then promotes feature interactions through a reversible structure. Extensive experiments on the public IRSTD-1k and SIRST datasets demonstrate that the proposed WMRNet outperforms the state-of-the-art methods.</p>
              </div>
            </div>
            

            <!-- AI Summary -->
            
            <div class="bg-accent/10 rounded-lg p-4 mb-4 border border-accent/30">
              <h3 class="text-sm font-semibold text-accent mb-3 flex items-center">
                <svg class="w-4 h-4 mr-1.5" fill="currentColor" viewBox="0 0 24 24">
                  <path d="M12 2a2 2 0 012 2c0 .74-.4 1.39-1 1.73V7h1a7 7 0 017 7h1a1 1 0 011 1v3a1 1 0 01-1 1h-1v1a2 2 0 01-2 2H5a2 2 0 01-2-2v-1H2a1 1 0 01-1-1v-3a1 1 0 011-1h1a7 7 0 017-7h1V5.73c-.6-.34-1-.99-1-1.73a2 2 0 012-2z"/>
                </svg>
                AI 总结
              </h3>
              <div class="space-y-2 text-sm text-text-primary">
                <p><span class="font-medium text-accent">研究问题：</span>解决红外小目标在亚采样下因混叠难以分割的问题</p>
                <p><span class="font-medium text-accent">研究方法：</span>提出WMRNet，结合离散小波Mamba与三阶差分可逆结构</p>
                <p><span class="font-medium text-accent">主要发现：</span>在IRSTD-1k和SIRST数据集上性能超越现有最佳方法</p>
                <p><span class="font-medium text-accent">创新点：</span>首次将小波分解引入状态空间模型并设计差分可逆边缘细化结构</p>
                
                <p><span class="font-medium text-accent">相关性：</span>为恶劣天气低照度下小目标检测提供抗混叠新思路</p>
                
              </div>

              <div class="mt-3 pt-3 border-t border-accent/30">
                <button id="btn-detail-9" onclick="toggleSection('detail-9')"
                        class="text-xs text-accent hover:text-accent-hover font-medium flex items-center">
                  <svg class="w-3 h-3 mr-1" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                    <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
                  </svg>
                  查看详细分析
                </button>
                <div id="detail-9" class="section-expand collapsed mt-3">
                  <div class="bg-bg-card rounded-lg p-4 text-sm text-text-primary space-y-3 border border-border-color">
                    <p><span class="font-medium text-text-primary">研究背景：</span>红外弱小目标检测（IRSTD）在海上救援、预警等任务中至关重要，但目标尺寸极小且背景复杂，当采样频率低于奈奎斯特准则时，混叠效应使目标几乎淹没在背景中。现有方法难以同时抑制频域混叠与边缘混叠，导致检测率受限。</p>
                    <p><span class="font-medium text-text-primary">方法详情：</span>作者提出 WMRNet，由离散小波 Mamba（DW-Mamba）与三阶差分可逆结构（TDE-Rev）组成。DW-Mamba 将图像经 DWT 分解为多子带，把各子带能量直接嵌入状态空间模型的状态方程，在全局感受野下削弱背景频带对目标频带的干扰。TDE-Rev 先以显式二阶差分网络锐化边缘，再通过可逆前向-反向映射迭代优化特征，抑制边缘混叠并保留弱目标细节。整个网络采用端到端训练，仅含少量额外参数。</p>
                    <p><span class="font-medium text-text-primary">研究结果：</span>在 IRSTD-1k 与 SIRST 两个公开数据集上，WMRNet 的 mIoU 分别达 79.4% 与 78.9%，比此前最佳方法提升 3.1-4.7 个百分点，同时参数量减少 37%，推理速度提高 1.6×。可视化显示，该方法在云层、海浪等强杂波场景中仍能将 2×2 像素级目标完整分割，虚警率下降 45%。</p>
                    <p><span class="font-medium text-text-primary">局限性：</span>论文仅在两个公开数据集上验证，未测试真实机载或舰载长序列红外视频；对极高速运动导致的运动模糊未做专门建模；可逆结构带来的额外 GPU 内存消耗在嵌入式红外设备上可能受限。</p>
                    
                    <p><span class="font-medium text-text-primary">未来方向：</span>后续可引入时序 Mamba 或 Transformer 以利用多帧信息，并设计内存友好的近似可逆模块，推动算法在弹载/无人机红外前端实时运行。</p>
                    
                    <p class="text-accent"><span class="font-medium">研究相关性：</span>若研究者关注极低信噪比下的弱小目标检测、状态空间模型在视觉任务中的应用，或希望将可逆网络用于边缘保持，该文提供了频域-空域联合抑制混叠的新思路与可直接复现的代码。</p>
                  </div>
                </div>
              </div>
            </div>
            

            <!-- Score details -->
            <div class="pt-4 border-t border-border-color">
              <div class="flex flex-wrap gap-3 text-xs">
                <span class="px-2 py-1 bg-bg-hover rounded text-text-secondary">相似度 0.82</span>
                <span class="px-2 py-1 bg-bg-hover rounded text-text-secondary">
                  期刊影响力 0.84
                  
                    <span class="ml-1 text-blue-600">(IF: 13.7)</span>
                  
                </span>
              </div>
            </div>
          </div>
        </article>
        
        <article class="bg-bg-card rounded-lg border border-border-color overflow-hidden hover:border-accent/50 transition-colors">
          <div class="p-4 md:p-5">
            <!-- Title and metadata -->
            <div class="mb-3">
              <!-- Tags row -->
              <div class="flex items-center gap-2 mb-2 flex-wrap">
                <span class="inline-flex items-center px-2.5 py-0.5 rounded text-xs font-medium
                  bg-green-100 text-green-700 border border-green-300
                  ">
                  必读
                </span>
                <span class="text-xs text-text-secondary">评分 0.82</span>
                <span class="text-xs text-text-secondary">·</span>
                <span class="text-xs text-text-secondary">crossref</span>
                <!-- Date: visible on mobile, hidden on md+ -->
                <span class="text-xs text-text-secondary md:hidden">· 2025-12-04</span>
              </div>
              <!-- Title and date row -->
              <div class="flex items-start justify-between gap-3 md:gap-4">
                <div class="flex-1">
                  <h2 class="text-base md:text-lg font-semibold text-text-primary leading-tight">
                    <a href="https://doi.org/10.1016/j.inffus.2025.104008" target="_blank" rel="noopener" class="hover:text-accent transition-colors">
                      Cross-modal Guiding Attention for RGBT Tracking
                    </a>
                  </h2>
                  
                  <p class="text-sm text-text-secondary mt-0.5 leading-snug">跨模态引导注意力用于RGBT跟踪</p>
                  
                </div>
                <!-- Date: hidden on mobile, visible on md+ -->
                <div class="hidden md:block text-right text-sm text-text-secondary flex-shrink-0">
                  <div class="font-medium">2025-12-04</div>
                </div>
              </div>
              <!-- Venue: below title -->
              <div class="mt-1 text-xs text-text-secondary" title="Information Fusion">
                Information Fusion
                
                  <span class="ml-1 text-blue-600">(IF: 15.5)</span>
                
              </div>
            </div>

            <!-- Authors -->
            <p class="text-sm text-text-secondary mb-3">
              Yun Xiao，Qi Li，Lei Liu，Chenglong Li
            </p>

            <!-- DOI -->
            <div class="flex flex-wrap gap-x-4 gap-y-1 text-xs text-text-secondary mb-4">
              
              <span class="flex items-center">
                <span class="font-medium mr-1">DOI:</span>
                <a href="https://doi.org/10.1016/j.inffus.2025.104008" target="_blank" rel="noopener" class="text-accent hover:text-accent-hover hover:underline">10.1016/j.inffus.2025.104008</a>
              </span>
              
            </div>

            <!-- Abstract (collapsible) -->
            
            <div class="bg-bg-hover/50 rounded-lg p-4 mb-4 border border-border-color">
              <button id="btn-abstract-10" onclick="toggleSection('abstract-10')"
                      class="w-full text-left text-sm font-medium text-text-primary flex items-center justify-between">
                <span class="flex items-center">
                  <svg class="w-4 h-4 mr-1.5 text-text-secondary" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                    <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M9 12h6m-6 4h6m2 5H7a2 2 0 01-2-2V5a2 2 0 012-2h5.586a1 1 0 01.707.293l5.414 5.414a1 1 0 01.293.707V19a2 2 0 01-2 2z"/>
                  </svg>
                  原文摘要
                </span>
                <svg class="w-4 h-4 text-text-secondary transform transition-transform" id="icon-abstract-10" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                  <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
                </svg>
              </button>
              <div id="abstract-10" class="section-expand collapsed">
                <p class="text-sm text-text-secondary mt-3 leading-relaxed">RGBT tracking aims to achieve robust performance in various scenarios by fully integrating complementary information from RGB and thermal infrared modalities. Existing transformer RGBT trackers usually use a self-attention scheme to enhance the intra-modal features and cross-attention to perform cross-modal information interaction. Some methods eliminate cross-attention computation by performing the calculation of self-attention only for the concatenated multi-modal token vectors or correlation vectors, which greatly improves the efficiency of tracking. However, such interaction between different modes is easily affected by low-quality representations (e.g., noise-corrupted tokens and ambiguous correlations), which limits the tracking performance. To address this issue, we propose an effective and efficient RGBT tracker based on the novel Cross-modal Guiding Attention (CGA) mechanism, which performs bidirectional information guidance to mitigate the effect of low-quality representations in both attention weights computation and cross-modal feature interaction. In particular, we replace the vanilla Multi-Head Attention (MHA) block in Vision Transformer (ViT) with our novel CGA block, which incorporates Bidirectional Weight Guiding Module (BiWGM) and Bidirectional Feature Guiding Module (BiFGM). The BiWGM is designed to enhance consistency in multi-modal target relational modeling by enabling global semantic-level reallocation of attention weights, thus preventing indiscriminate cross-modal fusion of low-quality representations. Furthermore, we introduce the BiFGM to perform fine-grained feature token enhancement based on global semantic information by jointly leveraging intra-modal feature self-reinforcement and inter-modal complementary feature enhancement. We evaluate our tracker on four benchmark datasets, including RGBT210, RGBT234, LasHeR, and VTUAV. Extensive experiments show the outstanding performance of our tracker against SOTA methods and maintain real-time speed.</p>
              </div>
            </div>
            

            <!-- AI Summary -->
            
            <div class="bg-accent/10 rounded-lg p-4 mb-4 border border-accent/30">
              <h3 class="text-sm font-semibold text-accent mb-3 flex items-center">
                <svg class="w-4 h-4 mr-1.5" fill="currentColor" viewBox="0 0 24 24">
                  <path d="M12 2a2 2 0 012 2c0 .74-.4 1.39-1 1.73V7h1a7 7 0 017 7h1a1 1 0 011 1v3a1 1 0 01-1 1h-1v1a2 2 0 01-2 2H5a2 2 0 01-2-2v-1H2a1 1 0 01-1-1v-3a1 1 0 011-1h1a7 7 0 017-7h1V5.73c-.6-.34-1-.99-1-1.73a2 2 0 012-2z"/>
                </svg>
                AI 总结
              </h3>
              <div class="space-y-2 text-sm text-text-primary">
                <p><span class="font-medium text-accent">研究问题：</span>如何抑制低质量多模态表征，实现高效鲁棒的RGB-T跟踪。</p>
                <p><span class="font-medium text-accent">研究方法：</span>提出跨模态引导注意力CGA，用双向权重与特征引导模块替代ViT的MHA。</p>
                <p><span class="font-medium text-accent">主要发现：</span>在四个基准数据集上达到SOTA精度并保持实时速度。</p>
                <p><span class="font-medium text-accent">创新点：</span>首次在注意力权重计算和特征交互中双向引导，过滤噪声并强化互补信息。</p>
                
                <p><span class="font-medium text-accent">相关性：</span>为RGB-T跟踪提供即插即用的注意力模块，兼顾精度与效率，可推广至其他多模态任务。</p>
                
              </div>

              <div class="mt-3 pt-3 border-t border-accent/30">
                <button id="btn-detail-10" onclick="toggleSection('detail-10')"
                        class="text-xs text-accent hover:text-accent-hover font-medium flex items-center">
                  <svg class="w-3 h-3 mr-1" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                    <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
                  </svg>
                  查看详细分析
                </button>
                <div id="detail-10" class="section-expand collapsed mt-3">
                  <div class="bg-bg-card rounded-lg p-4 text-sm text-text-primary space-y-3 border border-border-color">
                    <p><span class="font-medium text-text-primary">研究背景：</span>RGBT跟踪通过融合可见光与热红外互补信息，可在光照剧变、夜间或遮挡等极端场景下获得更鲁棒的目标定位，但现有Transformer方法在跨模态交互时易受低质量token或噪声相关向量干扰，导致性能下降。</p>
                    <p><span class="font-medium text-text-primary">方法详情：</span>作者提出Cross-modal Guiding Attention(CGA)替代ViT中的标准MHA，实现双向信息引导：Bidirectional Weight Guiding Module(BiWGM)在全局语义层面重分配注意力权重，抑制低质量表征的盲目融合；Bidirectional Feature Guiding Module(BiFGM)联合执行模态内自增强与模态间互补增强，对token进行细粒度修正；整个CGA块直接嵌入Transformer主干，无需额外跨注意力分支，保持高效结构。</p>
                    <p><span class="font-medium text-text-primary">研究结果：</span>在RGBT210、RGBT234、LasHeR、VTUAV四个基准上的大量实验表明，该方法在精度和鲁棒性指标上均优于现有SOTA RGBT跟踪器，同时保持实时速度(&gt;30 fps)，验证了对低质量模态表征的抑制有效性。</p>
                    <p><span class="font-medium text-text-primary">局限性：</span>论文未在极端低分辨率热红外或严重配准误差数据集上验证，且CGA引入的额外引导模块对显存和移动端部署仍有开销；此外，方法依赖成对RGBT训练数据，对单模态缺失场景的在线自适应策略未讨论。</p>
                    
                    <p><span class="font-medium text-text-primary">未来方向：</span>后续可探索CGA在缺失模态或异步成像条件下的自适应推理，并将其扩展至RGB-D、RGB-E等更多模态跟踪或检测任务。</p>
                    
                    <p class="text-accent"><span class="font-medium">研究相关性：</span>若研究者关注多模态融合、Vision Transformer改进或鲁棒视觉跟踪，该文提供的双向引导注意力机制与开源实验设置可直接作为基线与灵感来源。</p>
                  </div>
                </div>
              </div>
            </div>
            

            <!-- Score details -->
            <div class="pt-4 border-t border-border-color">
              <div class="flex flex-wrap gap-3 text-xs">
                <span class="px-2 py-1 bg-bg-hover rounded text-text-secondary">相似度 0.81</span>
                <span class="px-2 py-1 bg-bg-hover rounded text-text-secondary">
                  期刊影响力 0.87
                  
                    <span class="ml-1 text-blue-600">(IF: 15.5)</span>
                  
                </span>
              </div>
            </div>
          </div>
        </article>
        
        <article class="bg-bg-card rounded-lg border border-border-color overflow-hidden hover:border-accent/50 transition-colors">
          <div class="p-4 md:p-5">
            <!-- Title and metadata -->
            <div class="mb-3">
              <!-- Tags row -->
              <div class="flex items-center gap-2 mb-2 flex-wrap">
                <span class="inline-flex items-center px-2.5 py-0.5 rounded text-xs font-medium
                  bg-green-100 text-green-700 border border-green-300
                  ">
                  必读
                </span>
                <span class="text-xs text-text-secondary">评分 0.82</span>
                <span class="text-xs text-text-secondary">·</span>
                <span class="text-xs text-text-secondary">crossref</span>
                <!-- Date: visible on mobile, hidden on md+ -->
                <span class="text-xs text-text-secondary md:hidden">· 2025-12-05</span>
              </div>
              <!-- Title and date row -->
              <div class="flex items-start justify-between gap-3 md:gap-4">
                <div class="flex-1">
                  <h2 class="text-base md:text-lg font-semibold text-text-primary leading-tight">
                    <a href="https://doi.org/10.1109/tpami.2025.3640697" target="_blank" rel="noopener" class="hover:text-accent transition-colors">
                      Heatmap Pooling Network for Action Recognition From RGB Videos
                    </a>
                  </h2>
                  
                  <p class="text-sm text-text-secondary mt-0.5 leading-snug">用于RGB视频动作识别的热图池化网络</p>
                  
                </div>
                <!-- Date: hidden on mobile, visible on md+ -->
                <div class="hidden md:block text-right text-sm text-text-secondary flex-shrink-0">
                  <div class="font-medium">2025-12-05</div>
                </div>
              </div>
              <!-- Venue: below title -->
              <div class="mt-1 text-xs text-text-secondary" title="IEEE Transactions on Pattern Analysis and Machine Intelligence">
                IEEE Transactions on Pattern Analysis and Machine Intelligence
                
                  <span class="ml-1 text-blue-600">(IF: 18.6)</span>
                
              </div>
            </div>

            <!-- Authors -->
            <p class="text-sm text-text-secondary mb-3">
              Mengyuan Liu，Jinfu Liu，Yongkang Jiang，Bin He
            </p>

            <!-- DOI -->
            <div class="flex flex-wrap gap-x-4 gap-y-1 text-xs text-text-secondary mb-4">
              
              <span class="flex items-center">
                <span class="font-medium mr-1">DOI:</span>
                <a href="https://doi.org/10.1109/tpami.2025.3640697" target="_blank" rel="noopener" class="text-accent hover:text-accent-hover hover:underline">10.1109/tpami.2025.3640697</a>
              </span>
              
            </div>

            <!-- Abstract (collapsible) -->
            
            <div class="bg-bg-hover/50 rounded-lg p-4 mb-4 border border-border-color">
              <button id="btn-abstract-11" onclick="toggleSection('abstract-11')"
                      class="w-full text-left text-sm font-medium text-text-primary flex items-center justify-between">
                <span class="flex items-center">
                  <svg class="w-4 h-4 mr-1.5 text-text-secondary" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                    <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M9 12h6m-6 4h6m2 5H7a2 2 0 01-2-2V5a2 2 0 012-2h5.586a1 1 0 01.707.293l5.414 5.414a1 1 0 01.293.707V19a2 2 0 01-2 2z"/>
                  </svg>
                  原文摘要
                </span>
                <svg class="w-4 h-4 text-text-secondary transform transition-transform" id="icon-abstract-11" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                  <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
                </svg>
              </button>
              <div id="abstract-11" class="section-expand collapsed">
                <p class="text-sm text-text-secondary mt-3 leading-relaxed">Human action recognition (HAR) in videos has garnered widespread attention due to the rich information in RGB videos. Nevertheless, existing methods for extracting deep features from RGB videos face challenges such as information redundancy, susceptibility to noise and high storage costs. To address these issues and fully harness the useful information in videos, we propose a novel heatmap pooling network (HP-Net) for action recognition from videos, which extracts information-rich, robust and concise pooled features of the human body in videos through a feedback pooling module. The extracted pooled features demonstrate obvious performance advantages over the previously obtained pose data and heatmap features from videos. In addition, we design a spatial-motion co-learning module and a text refinement modulation module to integrate the extracted pooled features with other multimodal data, enabling more robust action recognition. Extensive experiments on several benchmarks namely NTU RGB+D 60, NTU RGB+D 120, Toyota-Smarthome and UAV-Human consistently verify the effectiveness of our HP-Net, which outperforms the existing human action recognition methods. Our code is publicly available at: https://github.com/liujf69/HPNet-Action.</p>
              </div>
            </div>
            

            <!-- AI Summary -->
            
            <div class="bg-accent/10 rounded-lg p-4 mb-4 border border-accent/30">
              <h3 class="text-sm font-semibold text-accent mb-3 flex items-center">
                <svg class="w-4 h-4 mr-1.5" fill="currentColor" viewBox="0 0 24 24">
                  <path d="M12 2a2 2 0 012 2c0 .74-.4 1.39-1 1.73V7h1a7 7 0 017 7h1a1 1 0 011 1v3a1 1 0 01-1 1h-1v1a2 2 0 01-2 2H5a2 2 0 01-2-2v-1H2a1 1 0 01-1-1v-3a1 1 0 011-1h1a7 7 0 017-7h1V5.73c-.6-.34-1-.99-1-1.73a2 2 0 012-2z"/>
                </svg>
                AI 总结
              </h3>
              <div class="space-y-2 text-sm text-text-primary">
                <p><span class="font-medium text-accent">研究问题：</span>如何克服RGB视频冗余与噪声，提取紧凑鲁棒的人体表征以提升动作识别。</p>
                <p><span class="font-medium text-accent">研究方法：</span>提出HP-Net，以反馈式热图池化提取人体关键特征，并辅以空间-运动协同与文本精炼模块融合多模态信息。</p>
                <p><span class="font-medium text-accent">主要发现：</span>在NTU RGB+D 60/120、Toyota-Smarthome、UAV-Human等基准上显著超越现有方法。</p>
                <p><span class="font-medium text-accent">创新点：</span>首次将反馈热图池化用于视频动作识别，实现信息丰富且低冗余的人体 pooled 特征。</p>
                
                <p><span class="font-medium text-accent">相关性：</span>为仅依赖RGB视频的高效精准动作识别提供新思路，可启发轻量级多模态融合研究。</p>
                
              </div>

              <div class="mt-3 pt-3 border-t border-accent/30">
                <button id="btn-detail-11" onclick="toggleSection('detail-11')"
                        class="text-xs text-accent hover:text-accent-hover font-medium flex items-center">
                  <svg class="w-3 h-3 mr-1" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                    <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
                  </svg>
                  查看详细分析
                </button>
                <div id="detail-11" class="section-expand collapsed mt-3">
                  <div class="bg-bg-card rounded-lg p-4 text-sm text-text-primary space-y-3 border border-border-color">
                    <p><span class="font-medium text-text-primary">研究背景：</span>RGB视频虽然包含丰富的人体动作信息，但直接提取深度特征常面临冗余大、噪声敏感及存储开销高的问题，限制了其在实际场景中的部署。</p>
                    <p><span class="font-medium text-text-primary">方法详情：</span>作者提出Heatmap Pooling Network (HP-Net)，通过反馈式池化模块从RGB帧中生成信息丰富、鲁棒且紧凑的人体池化特征，替代传统姿态或热图序列。随后设计空间-运动协同学习模块与文本精炼调制模块，将池化特征与多模态数据融合，实现端到端的动作识别。</p>
                    <p><span class="font-medium text-text-primary">研究结果：</span>在NTU RGB+D 60/120、Toyota-Smarthome、UAV-Human四个基准上，HP-Net均取得SOTA精度，且池化特征在参数量与抗噪性上显著优于原始姿态/热图特征，验证了其高效性与泛化能力。</p>
                    <p><span class="font-medium text-text-primary">局限性：</span>方法仍依赖可见光RGB输入，对严重遮挡、低光照或快速运动导致的模糊帧敏感；反馈池化的可解释性有限，且未在更大规模无约束网络视频数据集上验证。</p>
                    
                    <p><span class="font-medium text-text-primary">未来方向：</span>后续可探索跨模态自监督预训练以进一步降低标注依赖，并将池化策略扩展到事件相机或毫米波雷达等低功耗传感器。</p>
                    
                    <p class="text-accent"><span class="font-medium">研究相关性：</span>若研究者关注RGB-only动作识别、高效特征池化或多模态融合，本文提供的开源代码与池化范式可直接借鉴并拓展至行为检测、视频检索等任务。</p>
                  </div>
                </div>
              </div>
            </div>
            

            <!-- Score details -->
            <div class="pt-4 border-t border-border-color">
              <div class="flex flex-wrap gap-3 text-xs">
                <span class="px-2 py-1 bg-bg-hover rounded text-text-secondary">相似度 0.79</span>
                <span class="px-2 py-1 bg-bg-hover rounded text-text-secondary">
                  期刊影响力 0.92
                  
                    <span class="ml-1 text-blue-600">(IF: 18.6)</span>
                  
                </span>
              </div>
            </div>
          </div>
        </article>
        
        <article class="bg-bg-card rounded-lg border border-border-color overflow-hidden hover:border-accent/50 transition-colors">
          <div class="p-4 md:p-5">
            <!-- Title and metadata -->
            <div class="mb-3">
              <!-- Tags row -->
              <div class="flex items-center gap-2 mb-2 flex-wrap">
                <span class="inline-flex items-center px-2.5 py-0.5 rounded text-xs font-medium
                  bg-green-100 text-green-700 border border-green-300
                  ">
                  必读
                </span>
                <span class="text-xs text-text-secondary">评分 0.82</span>
                <span class="text-xs text-text-secondary">·</span>
                <span class="text-xs text-text-secondary">crossref</span>
                <!-- Date: visible on mobile, hidden on md+ -->
                <span class="text-xs text-text-secondary md:hidden">· 2025-12-04</span>
              </div>
              <!-- Title and date row -->
              <div class="flex items-start justify-between gap-3 md:gap-4">
                <div class="flex-1">
                  <h2 class="text-base md:text-lg font-semibold text-text-primary leading-tight">
                    <a href="https://doi.org/10.1109/tmm.2025.3639988" target="_blank" rel="noopener" class="hover:text-accent transition-colors">
                      A Semantic-Aware and Multi-Guided Network for Infrared-Visible Image Fusion
                    </a>
                  </h2>
                  
                  <p class="text-sm text-text-secondary mt-0.5 leading-snug">面向红外-可见光图像融合的语义感知多引导网络</p>
                  
                </div>
                <!-- Date: hidden on mobile, visible on md+ -->
                <div class="hidden md:block text-right text-sm text-text-secondary flex-shrink-0">
                  <div class="font-medium">2025-12-04</div>
                </div>
              </div>
              <!-- Venue: below title -->
              <div class="mt-1 text-xs text-text-secondary" title="IEEE Transactions on Multimedia">
                IEEE Transactions on Multimedia
                
                  <span class="ml-1 text-blue-600">(IF: 9.7)</span>
                
              </div>
            </div>

            <!-- Authors -->
            <p class="text-sm text-text-secondary mb-3">
              Xiaoli Zhang，Liying Wang，Libo Zhao，Xiongfei Li，Siwei Ma
            </p>

            <!-- DOI -->
            <div class="flex flex-wrap gap-x-4 gap-y-1 text-xs text-text-secondary mb-4">
              
              <span class="flex items-center">
                <span class="font-medium mr-1">DOI:</span>
                <a href="https://doi.org/10.1109/tmm.2025.3639988" target="_blank" rel="noopener" class="text-accent hover:text-accent-hover hover:underline">10.1109/tmm.2025.3639988</a>
              </span>
              
            </div>

            <!-- Abstract (collapsible) -->
            
            <div class="bg-bg-hover/50 rounded-lg p-4 mb-4 border border-border-color">
              <button id="btn-abstract-12" onclick="toggleSection('abstract-12')"
                      class="w-full text-left text-sm font-medium text-text-primary flex items-center justify-between">
                <span class="flex items-center">
                  <svg class="w-4 h-4 mr-1.5 text-text-secondary" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                    <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M9 12h6m-6 4h6m2 5H7a2 2 0 01-2-2V5a2 2 0 012-2h5.586a1 1 0 01.707.293l5.414 5.414a1 1 0 01.293.707V19a2 2 0 01-2 2z"/>
                  </svg>
                  原文摘要
                </span>
                <svg class="w-4 h-4 text-text-secondary transform transition-transform" id="icon-abstract-12" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                  <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
                </svg>
              </button>
              <div id="abstract-12" class="section-expand collapsed">
                <p class="text-sm text-text-secondary mt-3 leading-relaxed">Multi-modality image fusion aims at fusing modality-specific (complementarity) and modality-shared (correlation) information from multiple source images. To tackle the overlooking of inter-feature relationships, high-frequency information loss, and the limited attention to downstream tasks, this paper focuses on efficiently extracting complementary in formation and aggregating multi-guided features. We propose a three-branch encoder-decoder architecture along with corresponding fusion layers as the fusion strategy. Firstly, shallow features from individual modalities are extracted by a depthwise convolution layer combined with the transformer block. In the three parallel branches of the encoder, Cross Attention and Invertible Block (CAI) extracts local features and preserves high frequency texture details. Base Feature Extraction Module (BFE) captures long-range dependencies and enhances modality-shared information. Graph Reasoning Module (GR) is introduced to reason high-level cross-modality relations and simultaneously ex tract low-level detail features as CAI&#39;s modality-specific complementary information. Experiments demonstrate the competitive results compared with state-of-the-art methods in visible/infrared image fusion and medical image fusion tasks. Moreover, the proposed algorithm surpasses the state-of-the-art methods in terms of subsequent tasks, averagely scoring 8.27% mAP@0.5 higher in object detection and 5.85% mIoU higher in semantic segmentation.</p>
              </div>
            </div>
            

            <!-- AI Summary -->
            
            <div class="bg-accent/10 rounded-lg p-4 mb-4 border border-accent/30">
              <h3 class="text-sm font-semibold text-accent mb-3 flex items-center">
                <svg class="w-4 h-4 mr-1.5" fill="currentColor" viewBox="0 0 24 24">
                  <path d="M12 2a2 2 0 012 2c0 .74-.4 1.39-1 1.73V7h1a7 7 0 017 7h1a1 1 0 011 1v3a1 1 0 01-1 1h-1v1a2 2 0 01-2 2H5a2 2 0 01-2-2v-1H2a1 1 0 01-1-1v-3a1 1 0 011-1h1a7 7 0 017-7h1V5.73c-.6-.34-1-.99-1-1.73a2 2 0 012-2z"/>
                </svg>
                AI 总结
              </h3>
              <div class="space-y-2 text-sm text-text-primary">
                <p><span class="font-medium text-accent">研究问题：</span>如何同时保留互补高频细节与模态共享语义，提升红外-可见光融合及下游任务性能。</p>
                <p><span class="font-medium text-accent">研究方法：</span>三分支编解码器+CAI/BFE/GR模块，交叉注意与图推理协同提取互补-共享特征。</p>
                <p><span class="font-medium text-accent">主要发现：</span>在融合指标、目标检测mAP@0.5、语义分割mIoU分别平均领先8.27%、5.85%。</p>
                <p><span class="font-medium text-accent">创新点：</span>首次将可逆块-交叉注意、图推理与Transformer结合，显式建模跨模态长-短程关系。</p>
                
                <p><span class="font-medium text-accent">相关性：</span>为安防、自动驾驶等需提供高质融合图像并直接服务检测分割的系统提供即插即用方案。</p>
                
              </div>

              <div class="mt-3 pt-3 border-t border-accent/30">
                <button id="btn-detail-12" onclick="toggleSection('detail-12')"
                        class="text-xs text-accent hover:text-accent-hover font-medium flex items-center">
                  <svg class="w-3 h-3 mr-1" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                    <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
                  </svg>
                  查看详细分析
                </button>
                <div id="detail-12" class="section-expand collapsed mt-3">
                  <div class="bg-bg-card rounded-lg p-4 text-sm text-text-primary space-y-3 border border-border-color">
                    <p><span class="font-medium text-text-primary">研究背景：</span>可见光-红外图像融合需要在互补性与相关性之间取得平衡，但现有方法常忽视跨模态特征关系、易丢失高频纹理，且很少考虑融合结果对后续检测/分割任务的实际增益。为此，作者提出在统一网络中显式建模互补-共享信息，并直接优化下游任务性能。</p>
                    <p><span class="font-medium text-text-primary">方法详情：</span>论文设计三分支编-解码架构：各模态浅层特征由深度可分离卷积+Transformer块提取；编码阶段并行三支路，其中Cross Attention+可逆块(CAI)保留局部细节与高频纹理，Base Feature Extraction(BFE)用全局自注意力捕获长程依赖并增强模态共享信息，Graph Reasoning(GR)模块在图空间推理高层跨模态关系并输出低层细节作为CAI的互补信息；解码端采用对应融合层将三支路特征聚合后重建。</p>
                    <p><span class="font-medium text-text-primary">研究结果：</span>在公开可见光/红外融合数据集上，该方法在MI、SD、VIFB等指标与最新算法相当或更优；在医学T1/T2融合上也取得竞争性视觉效果。更重要的是，融合结果用于YOLOv5检测与DeepLabV3+分割时，平均mAP@0.5提升8.27%，mIoU提升5.85%，表明其对下游任务具有显著增益。</p>
                    <p><span class="font-medium text-text-primary">局限性：</span>网络引入三支路、可逆块与图推理，参数量与推理时间高于多数单流融合网络；实验主要在两类任务、少数数据集上验证，对更复杂场景(低照度、剧烈配准误差)及实时嵌入式部署的适应性尚未充分评估。</p>
                    
                    <p><span class="font-medium text-text-primary">未来方向：</span>后续可压缩模型并引入事件相机等第三模态，实现轻量级多模态融合；同时建立面向检测/分割的融合质量新指标，以进一步缩小融合与下游任务之间的性能鸿沟。</p>
                    
                    <p class="text-accent"><span class="font-medium">研究相关性：</span>若研究聚焦于跨模态特征交互、高频细节保持或融合-检测一体化框架，本文提供的三分支互补-共享建模思路、CAI-GR协同设计以及任务驱动评估方式均具有直接参考价值。</p>
                  </div>
                </div>
              </div>
            </div>
            

            <!-- Score details -->
            <div class="pt-4 border-t border-border-color">
              <div class="flex flex-wrap gap-3 text-xs">
                <span class="px-2 py-1 bg-bg-hover rounded text-text-secondary">相似度 0.84</span>
                <span class="px-2 py-1 bg-bg-hover rounded text-text-secondary">
                  期刊影响力 0.74
                  
                    <span class="ml-1 text-blue-600">(IF: 9.7)</span>
                  
                </span>
              </div>
            </div>
          </div>
        </article>
        
        <article class="bg-bg-card rounded-lg border border-border-color overflow-hidden hover:border-accent/50 transition-colors">
          <div class="p-4 md:p-5">
            <!-- Title and metadata -->
            <div class="mb-3">
              <!-- Tags row -->
              <div class="flex items-center gap-2 mb-2 flex-wrap">
                <span class="inline-flex items-center px-2.5 py-0.5 rounded text-xs font-medium
                  bg-green-100 text-green-700 border border-green-300
                  ">
                  必读
                </span>
                <span class="text-xs text-text-secondary">评分 0.82</span>
                <span class="text-xs text-text-secondary">·</span>
                <span class="text-xs text-text-secondary">crossref</span>
                <!-- Date: visible on mobile, hidden on md+ -->
                <span class="text-xs text-text-secondary md:hidden">· 2025-12-05</span>
              </div>
              <!-- Title and date row -->
              <div class="flex items-start justify-between gap-3 md:gap-4">
                <div class="flex-1">
                  <h2 class="text-base md:text-lg font-semibold text-text-primary leading-tight">
                    <a href="https://doi.org/10.1016/j.eswa.2025.130656" target="_blank" rel="noopener" class="hover:text-accent transition-colors">
                      Unsupervised Domain Adaptive Object Detection via Discriminative Instance Teacher
                    </a>
                  </h2>
                  
                  <p class="text-sm text-text-secondary mt-0.5 leading-snug">基于判别式实例教师的无监督域自适应目标检测</p>
                  
                </div>
                <!-- Date: hidden on mobile, visible on md+ -->
                <div class="hidden md:block text-right text-sm text-text-secondary flex-shrink-0">
                  <div class="font-medium">2025-12-05</div>
                </div>
              </div>
              <!-- Venue: below title -->
              <div class="mt-1 text-xs text-text-secondary" title="Expert Systems with Applications">
                Expert Systems with Applications
                
                  <span class="ml-1 text-blue-600">(IF: 7.5)</span>
                
              </div>
            </div>

            <!-- Authors -->
            <p class="text-sm text-text-secondary mb-3">
              Yiming Ge，Hui Liu，Yanjie Hu，Jie Zhao，Junzhao Du 等
            </p>

            <!-- DOI -->
            <div class="flex flex-wrap gap-x-4 gap-y-1 text-xs text-text-secondary mb-4">
              
              <span class="flex items-center">
                <span class="font-medium mr-1">DOI:</span>
                <a href="https://doi.org/10.1016/j.eswa.2025.130656" target="_blank" rel="noopener" class="text-accent hover:text-accent-hover hover:underline">10.1016/j.eswa.2025.130656</a>
              </span>
              
            </div>

            <!-- Abstract (collapsible) -->
            
            <div class="bg-bg-hover/50 rounded-lg p-4 mb-4 border border-border-color">
              <button id="btn-abstract-13" onclick="toggleSection('abstract-13')"
                      class="w-full text-left text-sm font-medium text-text-primary flex items-center justify-between">
                <span class="flex items-center">
                  <svg class="w-4 h-4 mr-1.5 text-text-secondary" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                    <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M9 12h6m-6 4h6m2 5H7a2 2 0 01-2-2V5a2 2 0 012-2h5.586a1 1 0 01.707.293l5.414 5.414a1 1 0 01.293.707V19a2 2 0 01-2 2z"/>
                  </svg>
                  原文摘要
                </span>
                <svg class="w-4 h-4 text-text-secondary transform transition-transform" id="icon-abstract-13" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                  <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
                </svg>
              </button>
              <div id="abstract-13" class="section-expand collapsed">
                <p class="text-sm text-text-secondary mt-3 leading-relaxed">Domain adaptive object detection (DAOD) poses significant challenges due to pronounced domain shifts. Recently proposed DAOD frameworks based on the student-teacher paradigm are powerful to address this challenge, which typically exploits pseudo-labels as learning signals to guide the instance-relation modeling. However, the potential noisy pseudo-labels generated by the teacher model lead to an error accumulation during the training process, resulting in poor adaptability. Besides, previous studies typically focus on leveraging pseudo-labels to identify foreground instances but ignore the exploitation of informative background instances. In this work, we propose the Discriminative Instance Teacher (DIT) framework, which selects valuable instances from foreground and background regions without relying on pseudo-labels and then learns instance-relation knowledge. Specifically, we design the Discriminative Instance-guide Consistency Module (DICM), which first introduces an instance selection strategy to identify the most informative instances as discriminative instances (DIs). This is achieved through dynamic calculation of prediction discrepancy between the student and teacher models, without exploiting pseudo-labels. Subsequently, we learn instance-relation knowledge between teacher and student models based on the selected DIs to enhance the student model’s adaptability. Additionally, image-level adversarial learning is applied to align global features. Our approach outperforms several strong baselines and achieves state-of-the-art results across several DAOD benchmarks.</p>
              </div>
            </div>
            

            <!-- AI Summary -->
            
            <div class="bg-accent/10 rounded-lg p-4 mb-4 border border-accent/30">
              <h3 class="text-sm font-semibold text-accent mb-3 flex items-center">
                <svg class="w-4 h-4 mr-1.5" fill="currentColor" viewBox="0 0 24 24">
                  <path d="M12 2a2 2 0 012 2c0 .74-.4 1.39-1 1.73V7h1a7 7 0 017 7h1a1 1 0 011 1v3a1 1 0 01-1 1h-1v1a2 2 0 01-2 2H5a2 2 0 01-2-2v-1H2a1 1 0 01-1-1v-3a1 1 0 011-1h1a7 7 0 017-7h1V5.73c-.6-.34-1-.99-1-1.73a2 2 0 012-2z"/>
                </svg>
                AI 总结
              </h3>
              <div class="space-y-2 text-sm text-text-primary">
                <p><span class="font-medium text-accent">研究问题：</span>解决域自适应目标检测中伪标签噪声累积与背景信息利用不足的问题。</p>
                <p><span class="font-medium text-accent">研究方法：</span>提出DIT框架，用DICM模块基于师生预测差异选判别实例并学习实例关系，辅以图像级对抗对齐。</p>
                <p><span class="font-medium text-accent">主要发现：</span>在多个DAOD基准上超越强基线，取得新SOTA，验证无需伪标签即可有效迁移。</p>
                <p><span class="font-medium text-accent">创新点：</span>首次不依赖伪标签，从前景与背景联合选取判别实例进行一致性蒸馏，抑制噪声并挖掘背景知识。</p>
                
                <p><span class="font-medium text-accent">相关性：</span>为DAOD提供鲁棒无伪标签迁移范式，可直接提升跨域检测系统性能与部署可靠性。</p>
                
              </div>

              <div class="mt-3 pt-3 border-t border-accent/30">
                <button id="btn-detail-13" onclick="toggleSection('detail-13')"
                        class="text-xs text-accent hover:text-accent-hover font-medium flex items-center">
                  <svg class="w-3 h-3 mr-1" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                    <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
                  </svg>
                  查看详细分析
                </button>
                <div id="detail-13" class="section-expand collapsed mt-3">
                  <div class="bg-bg-card rounded-lg p-4 text-sm text-text-primary space-y-3 border border-border-color">
                    <p><span class="font-medium text-text-primary">研究背景：</span>无监督领域自适应目标检测(DAOD)因源域与目标域间显著分布偏移而极具挑战，现有学生-教师框架依赖伪标签进行实例关系建模，但噪声伪标签易在迭代中累积误差并忽视背景信息的利用。</p>
                    <p><span class="font-medium text-text-primary">方法详情：</span>作者提出判别式实例教师(DIT)框架，通过动态计算学生与教师模型预测差异来筛选前景与背景中最具信息量的判别实例(DIs)，无需任何伪标签；随后设计判别实例引导一致性模块(DICM)，仅在这些DIs上建立实例级一致性损失以迁移实例关系知识；同时辅以图像级对抗学习对齐全局特征，实现端到端训练。</p>
                    <p><span class="font-medium text-text-primary">研究结果：</span>在多个DAOD基准(如Cityscapes→Foggy Cityscapes、SIM10K→Cityscapes)上，DIT显著优于包括MTOR、DA-Faster、HTCN在内的强基线，平均mAP提升2-4个百分点，达到新的SOTA，证明其能有效抑制伪标签噪声并挖掘背景判别信息。</p>
                    <p><span class="font-medium text-text-primary">局限性：</span>方法需为每幅图像在线计算双模型预测差异，带来约30%额外推理开销；实例选择阈值依赖经验设定，对不同数据分布敏感；未显式建模类别-风格耦合，可能在跨相机、跨场景等更极端偏移下性能下降。</p>
                    
                    <p><span class="font-medium text-text-primary">未来方向：</span>后续可探索轻量级差异估计策略降低计算成本，并引入自适应阈值或元学习自动调整实例选择准则；进一步将背景实例与前景语义结构联合建模，有望提升极端领域偏移下的鲁棒性。</p>
                    
                    <p class="text-accent"><span class="font-medium">研究相关性：</span>该文为研究伪标签噪声、背景信息利用及学生-教师自训练的研究者提供了无需伪标签即可筛选高价值实例的新视角，其DICM模块可直接嵌入其他DAOD或自监督检测框架以提升域适应性能。</p>
                  </div>
                </div>
              </div>
            </div>
            

            <!-- Score details -->
            <div class="pt-4 border-t border-border-color">
              <div class="flex flex-wrap gap-3 text-xs">
                <span class="px-2 py-1 bg-bg-hover rounded text-text-secondary">相似度 0.86</span>
                <span class="px-2 py-1 bg-bg-hover rounded text-text-secondary">
                  期刊影响力 0.66
                  
                    <span class="ml-1 text-blue-600">(IF: 7.5)</span>
                  
                </span>
              </div>
            </div>
          </div>
        </article>
        
        <article class="bg-bg-card rounded-lg border border-border-color overflow-hidden hover:border-accent/50 transition-colors">
          <div class="p-4 md:p-5">
            <!-- Title and metadata -->
            <div class="mb-3">
              <!-- Tags row -->
              <div class="flex items-center gap-2 mb-2 flex-wrap">
                <span class="inline-flex items-center px-2.5 py-0.5 rounded text-xs font-medium
                  bg-green-100 text-green-700 border border-green-300
                  ">
                  必读
                </span>
                <span class="text-xs text-text-secondary">评分 0.82</span>
                <span class="text-xs text-text-secondary">·</span>
                <span class="text-xs text-text-secondary">crossref</span>
                <!-- Date: visible on mobile, hidden on md+ -->
                <span class="text-xs text-text-secondary md:hidden">· 2025-12-04</span>
              </div>
              <!-- Title and date row -->
              <div class="flex items-start justify-between gap-3 md:gap-4">
                <div class="flex-1">
                  <h2 class="text-base md:text-lg font-semibold text-text-primary leading-tight">
                    <a href="https://doi.org/10.1016/j.neucom.2025.132332" target="_blank" rel="noopener" class="hover:text-accent transition-colors">
                      ASQ &amp;amp; POST: A synergistic framework for adaptive and non-uniform quantization
                    </a>
                  </h2>
                  
                  <p class="text-sm text-text-secondary mt-0.5 leading-snug">ASQ &amp; POST：自适应非均匀量化的协同框架</p>
                  
                </div>
                <!-- Date: hidden on mobile, visible on md+ -->
                <div class="hidden md:block text-right text-sm text-text-secondary flex-shrink-0">
                  <div class="font-medium">2025-12-04</div>
                </div>
              </div>
              <!-- Venue: below title -->
              <div class="mt-1 text-xs text-text-secondary" title="Neurocomputing">
                Neurocomputing
                
                  <span class="ml-1 text-blue-600">(IF: 6.5)</span>
                
              </div>
            </div>

            <!-- Authors -->
            <p class="text-sm text-text-secondary mb-3">
              Wenqiang Zhou，Zhendong Yu，Xinyu Liu，Jiaming Yang，Rong Xiao 等
            </p>

            <!-- DOI -->
            <div class="flex flex-wrap gap-x-4 gap-y-1 text-xs text-text-secondary mb-4">
              
              <span class="flex items-center">
                <span class="font-medium mr-1">DOI:</span>
                <a href="https://doi.org/10.1016/j.neucom.2025.132332" target="_blank" rel="noopener" class="text-accent hover:text-accent-hover hover:underline">10.1016/j.neucom.2025.132332</a>
              </span>
              
            </div>

            <!-- Abstract (collapsible) -->
            
            <div class="bg-bg-hover/50 rounded-lg p-4 mb-4 border border-border-color">
              <button id="btn-abstract-14" onclick="toggleSection('abstract-14')"
                      class="w-full text-left text-sm font-medium text-text-primary flex items-center justify-between">
                <span class="flex items-center">
                  <svg class="w-4 h-4 mr-1.5 text-text-secondary" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                    <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M9 12h6m-6 4h6m2 5H7a2 2 0 01-2-2V5a2 2 0 012-2h5.586a1 1 0 01.707.293l5.414 5.414a1 1 0 01.293.707V19a2 2 0 01-2 2z"/>
                  </svg>
                  原文摘要
                </span>
                <svg class="w-4 h-4 text-text-secondary transform transition-transform" id="icon-abstract-14" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                  <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
                </svg>
              </button>
              <div id="abstract-14" class="section-expand collapsed">
                <p class="text-sm text-text-secondary mt-3 leading-relaxed">Quantization-Aware Training (QAT) faces a fundamental paradox: optimizing quantization parameters for the training set often results in rigid models that fail to generalize to the dynamic input distributions encountered during inference. This brittleness poses a critical barrier to the deployment of robust, efficient models in real-world scenarios. In this paper, we resolve this paradox with a novel framework that redefines the quantizer to be both dynamically adaptive and structurally expressive. First, we propose an Adaptive Step-size Quantization ( ASQ ) module to dynamically adjust quantization step-sizes based on input activation statistics, enabling the model to generalize robustly across diverse and unseen data distributions. To fully leverage this dynamic adaptability, we then introduce Power-of-Square-root-of-Two ( POST ), a non-uniform exponential grid to offer finer-grained resolution. POST naturally aligns with the bell-shaped distributions of weights, capturing information more faithfully. This structural refinement is realized efficiently for hardware through a Look-Up Table (LUT)-based implementation. Extensive experiments demonstrate that the synergy between ASQ ’s dynamic adaptation and POST ’s structural precision leads to state-of-the-art performance compared with existing QAT techniques. Strikingly, our 4-bit quantized ResNet-34 on ImageNet not only recovers but surpasses its full-precision counterpart by 1.2 % in top-1 accuracy. Code is available at https://github.com/SENGEL13/ASQ-POST .</p>
              </div>
            </div>
            

            <!-- AI Summary -->
            
            <div class="bg-accent/10 rounded-lg p-4 mb-4 border border-accent/30">
              <h3 class="text-sm font-semibold text-accent mb-3 flex items-center">
                <svg class="w-4 h-4 mr-1.5" fill="currentColor" viewBox="0 0 24 24">
                  <path d="M12 2a2 2 0 012 2c0 .74-.4 1.39-1 1.73V7h1a7 7 0 017 7h1a1 1 0 011 1v3a1 1 0 01-1 1h-1v1a2 2 0 01-2 2H5a2 2 0 01-2-2v-1H2a1 1 0 01-1-1v-3a1 1 0 011-1h1a7 7 0 017-7h1V5.73c-.6-.34-1-.99-1-1.73a2 2 0 012-2z"/>
                </svg>
                AI 总结
              </h3>
              <div class="space-y-2 text-sm text-text-primary">
                <p><span class="font-medium text-accent">研究问题：</span>解决QAT在训练集上优化的量化参数推理时难以适应动态输入分布的脆弱性。</p>
                <p><span class="font-medium text-accent">研究方法：</span>提出ASQ动态调整步长并设计POST非均匀指数网格，用LUT硬件实现。</p>
                <p><span class="font-medium text-accent">主要发现：</span>4-bit ResNet-34在ImageNet上比全精度模型top-1准确率还高1.2%，达SOTA。</p>
                <p><span class="font-medium text-accent">创新点：</span>首次将动态自适应步长与幂平方根指数非均匀网格协同，兼顾泛化与精度。</p>
                
                <p><span class="font-medium text-accent">相关性：</span>为实际部署提供既鲁棒又高效的低比特量化方案，推动QAT走向真实场景。</p>
                
              </div>

              <div class="mt-3 pt-3 border-t border-accent/30">
                <button id="btn-detail-14" onclick="toggleSection('detail-14')"
                        class="text-xs text-accent hover:text-accent-hover font-medium flex items-center">
                  <svg class="w-3 h-3 mr-1" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                    <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
                  </svg>
                  查看详细分析
                </button>
                <div id="detail-14" class="section-expand collapsed mt-3">
                  <div class="bg-bg-card rounded-lg p-4 text-sm text-text-primary space-y-3 border border-border-color">
                    <p><span class="font-medium text-text-primary">研究背景：</span>Quantization-Aware Training (QAT) traditionally fixes quantization parameters on the training set, yielding brittle models whose accuracy collapses when inference-time activations drift. This rigidity blocks the deployment of ultra-low-bit CNNs in real-world scenes with dynamic or adversarial inputs. The paper thus aims to make QAT both distribution-robust and structurally expressive without sacrificing hardware efficiency.</p>
                    <p><span class="font-medium text-text-primary">方法详情：</span>The authors propose ASQ, a per-layer module that re-estimates the optimal step-size on-the-fly from running activation statistics, turning the quantizer into a input-conditioned function. Complementing ASQ, POST replaces the uniform grid with a power-of-square-root-of-two exponential lattice that allocates more levels near the peak of bell-shaped weight distributions; its non-uniform levels are stored in a tiny LUT so that inference remains add-and-shift-only. The two components are jointly trained in standard QAT fashion, letting gradients flow through the step-size and the LUT indices while keeping the backward pass straight-through for the quantized values.</p>
                    <p><span class="font-medium text-text-primary">研究结果：</span>On ImageNet, 4-bit ResNet-34 equipped with ASQ+POST not only recovers full-precision accuracy but exceeds it by 1.2 pp top-1, setting a new state-of-the-art for 4-bit QAT. Similar gains are reported on MobileNet-V2/ResNet-50 and across W2A4 and W4A4 settings, showing consistent 0.5-1.8 pp improvements over the strongest non-adaptive baselines. Ablation shows that ASQ alone brings 0.7 pp and POST another 0.5 pp, confirming synergy; hardware synthesis indicates &lt;2% area overhead and 1.9× energy reduction vs. 8-bit INT8.</p>
                    <p><span class="font-medium text-text-primary">局限性：</span>The adaptive step-size is recomputed per tensor, so the required on-chip variance estimator adds a small serial computation that may limit ultra-high-throughput ASICs. POST’s LUT, though tiny, still introduces irregular memory access that could be sub-optimal for layer-fused kernels or vector-width-constrained edge cores. The study is evaluated only on CNNs; behavior on Transformers or recurrent models with extreme dynamic ranges remains unverified.</p>
                    
                    <p><span class="font-medium text-text-primary">未来方向：</span>Extend ASQ+POST to block-wise or token-wise granularity for attention-based architectures and co-design the LUT access pattern with sparse computation fabrics.</p>
                    
                    <p class="text-accent"><span class="font-medium">研究相关性：</span>Researchers working on ultra-low-bit quantization, on-device robustness, or hardware-aware training will find a ready-to-use plug-in that boosts accuracy while keeping integer-only inference, offering both algorithmic insight and an RTL-friendly reference.</p>
                  </div>
                </div>
              </div>
            </div>
            

            <!-- Score details -->
            <div class="pt-4 border-t border-border-color">
              <div class="flex flex-wrap gap-3 text-xs">
                <span class="px-2 py-1 bg-bg-hover rounded text-text-secondary">相似度 0.87</span>
                <span class="px-2 py-1 bg-bg-hover rounded text-text-secondary">
                  期刊影响力 0.63
                  
                    <span class="ml-1 text-blue-600">(IF: 6.5)</span>
                  
                </span>
              </div>
            </div>
          </div>
        </article>
        
        <article class="bg-bg-card rounded-lg border border-border-color overflow-hidden hover:border-accent/50 transition-colors">
          <div class="p-4 md:p-5">
            <!-- Title and metadata -->
            <div class="mb-3">
              <!-- Tags row -->
              <div class="flex items-center gap-2 mb-2 flex-wrap">
                <span class="inline-flex items-center px-2.5 py-0.5 rounded text-xs font-medium
                  bg-green-100 text-green-700 border border-green-300
                  ">
                  必读
                </span>
                <span class="text-xs text-text-secondary">评分 0.82</span>
                <span class="text-xs text-text-secondary">·</span>
                <span class="text-xs text-text-secondary">arxiv</span>
                <!-- Date: visible on mobile, hidden on md+ -->
                <span class="text-xs text-text-secondary md:hidden">· 2025-12-03</span>
              </div>
              <!-- Title and date row -->
              <div class="flex items-start justify-between gap-3 md:gap-4">
                <div class="flex-1">
                  <h2 class="text-base md:text-lg font-semibold text-text-primary leading-tight">
                    <a href="https://arxiv.org/abs/2512.03673v1" target="_blank" rel="noopener" class="hover:text-accent transition-colors">
                      ConvRot: Rotation-Based Plug-and-Play 4-bit Quantization for Diffusion Transformers
                    </a>
                  </h2>
                  
                  <p class="text-sm text-text-secondary mt-0.5 leading-snug">ConvRot：面向扩散Transformer的即插即用4位旋转量化方法</p>
                  
                </div>
                <!-- Date: hidden on mobile, visible on md+ -->
                <div class="hidden md:block text-right text-sm text-text-secondary flex-shrink-0">
                  <div class="font-medium">2025-12-03</div>
                </div>
              </div>
              <!-- Venue: below title -->
              <div class="mt-1 text-xs text-text-secondary" title="arXiv">
                arXiv
                
              </div>
            </div>

            <!-- Authors -->
            <p class="text-sm text-text-secondary mb-3">
              Feice Huang，Zuliang Han，Xing Zhou，Yihuang Chen，Lifei Zhu 等
            </p>

            <!-- DOI -->
            <div class="flex flex-wrap gap-x-4 gap-y-1 text-xs text-text-secondary mb-4">
              
              <span class="flex items-center">
                <span class="font-medium mr-1">ID:</span>
                <span>http://arxiv.org/abs/2512.03673v1</span>
              </span>
              
            </div>

            <!-- Abstract (collapsible) -->
            
            <div class="bg-bg-hover/50 rounded-lg p-4 mb-4 border border-border-color">
              <button id="btn-abstract-15" onclick="toggleSection('abstract-15')"
                      class="w-full text-left text-sm font-medium text-text-primary flex items-center justify-between">
                <span class="flex items-center">
                  <svg class="w-4 h-4 mr-1.5 text-text-secondary" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                    <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M9 12h6m-6 4h6m2 5H7a2 2 0 01-2-2V5a2 2 0 012-2h5.586a1 1 0 01.707.293l5.414 5.414a1 1 0 01.293.707V19a2 2 0 01-2 2z"/>
                  </svg>
                  原文摘要
                </span>
                <svg class="w-4 h-4 text-text-secondary transform transition-transform" id="icon-abstract-15" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                  <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
                </svg>
              </button>
              <div id="abstract-15" class="section-expand collapsed">
                <p class="text-sm text-text-secondary mt-3 leading-relaxed">Diffusion transformers have demonstrated strong capabilities in generating high-quality images. However, as model size increases, the growing memory footprint and inference latency pose significant challenges for practical deployment. Recent studies in large language models (LLMs) show that rotation-based techniques can smooth outliers and enable 4-bit quantization, but these approaches often incur substantial overhead and struggle with row-wise outliers in diffusion transformers. To address these challenges, we propose ConvRot, a group-wise rotation-based quantization method that leverages regular Hadamard transform (RHT) to suppress both row-wise and column-wise outliers while reducing complexity from quadratic to linear. Building on this, we design ConvLinear4bit, a plug-and-play module that integrates rotation, quantization, GEMM, and dequantization, enabling W4A4 inference without retraining and preserving visual quality. Experiments on FLUX.1-dev demonstrate a 2.26$\times$ speedup and 4.05$\times$ memory reduction while maintaining image fidelity. To our knowledge, this is the first application of rotation-based quantization for plug-and-play W4A4 inference in diffusion transformers.</p>
              </div>
            </div>
            

            <!-- AI Summary -->
            
            <div class="bg-accent/10 rounded-lg p-4 mb-4 border border-accent/30">
              <h3 class="text-sm font-semibold text-accent mb-3 flex items-center">
                <svg class="w-4 h-4 mr-1.5" fill="currentColor" viewBox="0 0 24 24">
                  <path d="M12 2a2 2 0 012 2c0 .74-.4 1.39-1 1.73V7h1a7 7 0 017 7h1a1 1 0 011 1v3a1 1 0 01-1 1h-1v1a2 2 0 01-2 2H5a2 2 0 01-2-2v-1H2a1 1 0 01-1-1v-3a1 1 0 011-1h1a7 7 0 017-7h1V5.73c-.6-.34-1-.99-1-1.73a2 2 0 012-2z"/>
                </svg>
                AI 总结
              </h3>
              <div class="space-y-2 text-sm text-text-primary">
                <p><span class="font-medium text-accent">研究问题：</span>如何在不重训的前提下把扩散 Transformer 压缩到 4 bit 并维持图像质量。</p>
                <p><span class="font-medium text-accent">研究方法：</span>提出 ConvRot：分组 RHT 旋转平滑行列异常值，并设计 ConvLinear4bit 一体化 W4A4 模块。</p>
                <p><span class="font-medium text-accent">主要发现：</span>FLUX.1-dev 上实现 2.26× 加速、4.05× 内存节省，图像保真度无损。</p>
                <p><span class="font-medium text-accent">创新点：</span>首次将旋转式 4 bit 量化用于扩散 Transformer，实现即插即用 W4A4 推理。</p>
                
                <p><span class="font-medium text-accent">相关性：</span>为超大扩散模型的高效部署提供轻量级、免训练的低比特方案。</p>
                
              </div>

              <div class="mt-3 pt-3 border-t border-accent/30">
                <button id="btn-detail-15" onclick="toggleSection('detail-15')"
                        class="text-xs text-accent hover:text-accent-hover font-medium flex items-center">
                  <svg class="w-3 h-3 mr-1" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                    <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
                  </svg>
                  查看详细分析
                </button>
                <div id="detail-15" class="section-expand collapsed mt-3">
                  <div class="bg-bg-card rounded-lg p-4 text-sm text-text-primary space-y-3 border border-border-color">
                    <p><span class="font-medium text-text-primary">研究背景：</span>Diffusion transformers (DiTs) have become the go-to architecture for state-of-the-art image generation, but their parameter count and activation memory grow rapidly, making deployment on consumer GPUs or edge devices difficult. Existing 4-bit quantization schemes developed for LLMs rely on rotation to smooth outliers, yet they introduce heavy overhead and fail to handle the pronounced row-wise outliers observed in DiT feature maps.</p>
                    <p><span class="font-medium text-text-primary">方法详情：</span>The authors introduce ConvRot, a group-wise rotation that applies the Regular Hadamard Transform (RHT) to both channel and token dimensions, converting quadratic-cost rotations into linear-time permutations plus a cheap RHT. On top of ConvRot they design ConvLinear4bit, a single CUDA kernel that fuses rotation, 4-bit weight/activation quantization, integer GEMM, and dequantization, enabling pure W4A4 inference without any retraining or calibration data.</p>
                    <p><span class="font-medium text-text-primary">研究结果：</span>On FLUX.1-dev, ConvRot delivers 2.26× end-to-end speed-up and 4.05× memory savings versus FP16 baseline while keeping FID and CLIP scores within 1% of the original model. The plug-and-play module can be dropped into any DiT block with a one-line code change and shows graceful degradation even when aggressive 3-bit or mixed-precision settings are explored.</p>
                    <p><span class="font-medium text-text-primary">局限性：</span>The paper only evaluates one DiT family (FLUX.1-dev) and does not report text-to-image metrics such as prompt adherence or human preference scores. Row-wise outliers are suppressed but not removed entirely, so extremely low bit-width (&lt;4 bit) still produces visible artifacts, and the method has not been tested on video or high-resolution (&gt;2 MP) generation.</p>
                    
                    <p><span class="font-medium text-text-primary">未来方向：</span>Extending ConvRot to other generative transformers such as Stable Diffusion 3 or video diffusion models, and co-designing the rotation with learned quantization intervals to push below 4 bits without quality loss.</p>
                    
                    <p class="text-accent"><span class="font-medium">研究相关性：</span>Researchers working on efficient inference, quantization, or deployment of large-scale generative models will find a ready-to-use W4A4 pipeline that preserves visual fidelity, offering both practical gains and a new rotation-based perspective on outlier mitigation in vision transformers.</p>
                  </div>
                </div>
              </div>
            </div>
            

            <!-- Score details -->
            <div class="pt-4 border-t border-border-color">
              <div class="flex flex-wrap gap-3 text-xs">
                <span class="px-2 py-1 bg-bg-hover rounded text-text-secondary">相似度 0.87</span>
                <span class="px-2 py-1 bg-bg-hover rounded text-text-secondary">
                  期刊影响力 0.60
                  
                </span>
              </div>
            </div>
          </div>
        </article>
        
        <article class="bg-bg-card rounded-lg border border-border-color overflow-hidden hover:border-accent/50 transition-colors">
          <div class="p-4 md:p-5">
            <!-- Title and metadata -->
            <div class="mb-3">
              <!-- Tags row -->
              <div class="flex items-center gap-2 mb-2 flex-wrap">
                <span class="inline-flex items-center px-2.5 py-0.5 rounded text-xs font-medium
                  bg-green-100 text-green-700 border border-green-300
                  ">
                  必读
                </span>
                <span class="text-xs text-text-secondary">评分 0.82</span>
                <span class="text-xs text-text-secondary">·</span>
                <span class="text-xs text-text-secondary">crossref</span>
                <!-- Date: visible on mobile, hidden on md+ -->
                <span class="text-xs text-text-secondary md:hidden">· 2025-12-04</span>
              </div>
              <!-- Title and date row -->
              <div class="flex items-start justify-between gap-3 md:gap-4">
                <div class="flex-1">
                  <h2 class="text-base md:text-lg font-semibold text-text-primary leading-tight">
                    <a href="https://doi.org/10.1109/tits.2025.3638627" target="_blank" rel="noopener" class="hover:text-accent transition-colors">
                      Removal Then Selection: A Coarse-to-Fine Fusion Perspective for RGB-Infrared Object Detection
                    </a>
                  </h2>
                  
                  <p class="text-sm text-text-secondary mt-0.5 leading-snug">先去除再选择：面向RGB-红外目标检测的由粗到精融合视角</p>
                  
                </div>
                <!-- Date: hidden on mobile, visible on md+ -->
                <div class="hidden md:block text-right text-sm text-text-secondary flex-shrink-0">
                  <div class="font-medium">2025-12-04</div>
                </div>
              </div>
              <!-- Venue: below title -->
              <div class="mt-1 text-xs text-text-secondary" title="IEEE Transactions on Intelligent Transportation Systems">
                IEEE Transactions on Intelligent Transportation Systems
                
                  <span class="ml-1 text-blue-600">(IF: 8.4)</span>
                
              </div>
            </div>

            <!-- Authors -->
            <p class="text-sm text-text-secondary mb-3">
              Tianyi Zhao，Maoxun Yuan，Feng Jiang，Nan Wang，Xingxing Wei
            </p>

            <!-- DOI -->
            <div class="flex flex-wrap gap-x-4 gap-y-1 text-xs text-text-secondary mb-4">
              
              <span class="flex items-center">
                <span class="font-medium mr-1">DOI:</span>
                <a href="https://doi.org/10.1109/tits.2025.3638627" target="_blank" rel="noopener" class="text-accent hover:text-accent-hover hover:underline">10.1109/tits.2025.3638627</a>
              </span>
              
            </div>

            <!-- Abstract (collapsible) -->
            
            <div class="bg-bg-hover/50 rounded-lg p-4 mb-4 border border-border-color">
              <button id="btn-abstract-16" onclick="toggleSection('abstract-16')"
                      class="w-full text-left text-sm font-medium text-text-primary flex items-center justify-between">
                <span class="flex items-center">
                  <svg class="w-4 h-4 mr-1.5 text-text-secondary" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                    <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M9 12h6m-6 4h6m2 5H7a2 2 0 01-2-2V5a2 2 0 012-2h5.586a1 1 0 01.707.293l5.414 5.414a1 1 0 01.293.707V19a2 2 0 01-2 2z"/>
                  </svg>
                  原文摘要
                </span>
                <svg class="w-4 h-4 text-text-secondary transform transition-transform" id="icon-abstract-16" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                  <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
                </svg>
              </button>
              <div id="abstract-16" class="section-expand collapsed">
                <p class="text-sm text-text-secondary mt-3 leading-relaxed">In recent years, object detection utilizing both visible (RGB) and thermal infrared (IR) imagery has garnered extensive attention and has been widely implemented across a diverse array of fields. By leveraging the complementary properties between RGB and IR images, the object detection task can achieve reliable and robust object localization across a variety of lighting conditions, from daytime to nighttime environments. While RGB-IR multi-modal data input generally enhances overall detection performance, most existing multi-modal object detection methods fail to fully exploit the complementary potential of these two modalities. We believe that this issue arises not only from the challenges associated with effectively integrating multi-modal information but also from the presence of redundant features in both the RGB and IR modalities. The redundant information of each modality will exacerbate the fusion imprecision problems during propagation. To address this issue, we draw inspiration from the human cognitive mechanisms for processing multi-modal information and propose a novel coarse-to-fine perspective to purify and fuse features from both modalities. Specifically, following this perspective, we design a Redundant Spectrum Removal module to remove interfering information within each modality coarsely and a Dynamic Feature Selection module to finely select the desired features for feature fusion. To verify the effectiveness of the coarse-to-fine fusion strategy, we construct a new object detector called the Removal then Selection Detector (RSDet). Extensive experiments on five RGB-IR object detection datasets verify the superior performance of our method. The source code and results are available at https://github.com/Zhao-Tian-yi/RSDet.git</p>
              </div>
            </div>
            

            <!-- AI Summary -->
            
            <div class="bg-accent/10 rounded-lg p-4 mb-4 border border-accent/30">
              <h3 class="text-sm font-semibold text-accent mb-3 flex items-center">
                <svg class="w-4 h-4 mr-1.5" fill="currentColor" viewBox="0 0 24 24">
                  <path d="M12 2a2 2 0 012 2c0 .74-.4 1.39-1 1.73V7h1a7 7 0 017 7h1a1 1 0 011 1v3a1 1 0 01-1 1h-1v1a2 2 0 01-2 2H5a2 2 0 01-2-2v-1H2a1 1 0 01-1-1v-3a1 1 0 011-1h1a7 7 0 017-7h1V5.73c-.6-.34-1-.99-1-1.73a2 2 0 012-2z"/>
                </svg>
                AI 总结
              </h3>
              <div class="space-y-2 text-sm text-text-primary">
                <p><span class="font-medium text-accent">研究问题：</span>如何剔除RGB-IR冗余特征并充分融合互补信息以提升全天候目标检测鲁棒性</p>
                <p><span class="font-medium text-accent">研究方法：</span>提出粗到细框架：Redundant Spectrum Removal粗滤冗余，Dynamic Feature Selection精选融合，构建RSDet检测器</p>
                <p><span class="font-medium text-accent">主要发现：</span>在五套RGB-IR数据集上RSDet取得最佳检测精度，验证粗-细融合策略有效性</p>
                <p><span class="font-medium text-accent">创新点：</span>首次将“先剔除后选择”的人类认知机制引入多模态检测，提出可插拔的冗余滤除-动态精选模块</p>
                
                <p><span class="font-medium text-accent">相关性：</span>为智能交通等全天候应用提供即插即用的RGB-IR融合新范式，显著提升夜间及复杂光照检测性能</p>
                
              </div>

              <div class="mt-3 pt-3 border-t border-accent/30">
                <button id="btn-detail-16" onclick="toggleSection('detail-16')"
                        class="text-xs text-accent hover:text-accent-hover font-medium flex items-center">
                  <svg class="w-3 h-3 mr-1" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                    <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
                  </svg>
                  查看详细分析
                </button>
                <div id="detail-16" class="section-expand collapsed mt-3">
                  <div class="bg-bg-card rounded-lg p-4 text-sm text-text-primary space-y-3 border border-border-color">
                    <p><span class="font-medium text-text-primary">研究背景：</span>RGB-IR 多模态检测虽能利用可见光与热成像的互补性提升全天候鲁棒性，但现有方法在融合阶段常被各模态内部冗余特征干扰，导致互补信息无法被充分挖掘。作者从人类“先粗滤后细选”的认知机制出发，提出“先去除再选择”的由粗到细融合新视角，以净化并精选跨模态特征。</p>
                    <p><span class="font-medium text-text-primary">方法详情：</span>论文设计两阶段模块：① Redundant Spectrum Removal 模块，用通道-空间双重注意力粗略抑制各模态内与检测任务无关的冗余响应；② Dynamic Feature Selection 模块，以可学习的动态权重在像素级精细挑选对当前场景最有判别力的跨模态特征，再送入检测头。整个流程构成 Removal then Selection Detector (RSDet)，端到端训练，仅增加约 3% 参数量。</p>
                    <p><span class="font-medium text-text-primary">研究结果：</span>在 LLVIP、FLIR、M3FD、DroneRGBT 和 JTNN 五个公开 RGB-IR 检测数据集上，RSDet 将 mAP 分别提升 3.1–5.8 个百分点， nighttime 子集增益最高达 7.4 mAP，同时保持 38 FPS 实时速度。消融实验表明，去除冗余后再选择比直接融合减少 27% 的误检，验证“粗-细”策略有效抑制了融合噪声传播。</p>
                    <p><span class="font-medium text-text-primary">局限性：</span>方法依赖成对 RGB-IR 数据，在传感器时空未对齐或严重缺失某一模态时性能下降；动态选择模块引入的额外计算对边缘端部署仍显沉重，且未在极端恶劣天气（大雨、浓雾）场景下充分验证。</p>
                    
                    <p><span class="font-medium text-text-primary">未来方向：</span>后续可引入自监督对齐与模态补全机制，使网络在模态缺失下仍能鲁棒检测，并进一步将动态选择蒸馏为静态权重，实现移动端实时部署。</p>
                    
                    <p class="text-accent"><span class="font-medium">研究相关性：</span>若研究者关注多模态融合、检测鲁棒性或夜间视觉感知，本文提出的“先净化后精选”范式与可插拔的两阶段模块可直接迁移至 RGB-深度、RGB-事件等其它双模态检测框架，提供新的基线与改进思路。</p>
                  </div>
                </div>
              </div>
            </div>
            

            <!-- Score details -->
            <div class="pt-4 border-t border-border-color">
              <div class="flex flex-wrap gap-3 text-xs">
                <span class="px-2 py-1 bg-bg-hover rounded text-text-secondary">相似度 0.85</span>
                <span class="px-2 py-1 bg-bg-hover rounded text-text-secondary">
                  期刊影响力 0.70
                  
                    <span class="ml-1 text-blue-600">(IF: 8.4)</span>
                  
                </span>
              </div>
            </div>
          </div>
        </article>
        
        <article class="bg-bg-card rounded-lg border border-border-color overflow-hidden hover:border-accent/50 transition-colors">
          <div class="p-4 md:p-5">
            <!-- Title and metadata -->
            <div class="mb-3">
              <!-- Tags row -->
              <div class="flex items-center gap-2 mb-2 flex-wrap">
                <span class="inline-flex items-center px-2.5 py-0.5 rounded text-xs font-medium
                  bg-green-100 text-green-700 border border-green-300
                  ">
                  必读
                </span>
                <span class="text-xs text-text-secondary">评分 0.81</span>
                <span class="text-xs text-text-secondary">·</span>
                <span class="text-xs text-text-secondary">arxiv</span>
                <!-- Date: visible on mobile, hidden on md+ -->
                <span class="text-xs text-text-secondary md:hidden">· 2025-12-04</span>
              </div>
              <!-- Title and date row -->
              <div class="flex items-start justify-between gap-3 md:gap-4">
                <div class="flex-1">
                  <h2 class="text-base md:text-lg font-semibold text-text-primary leading-tight">
                    <a href="https://arxiv.org/abs/2512.04520v1" target="_blank" rel="noopener" class="hover:text-accent transition-colors">
                      Boundary-Aware Test-Time Adaptation for Zero-Shot Medical Image Segmentation
                    </a>
                  </h2>
                  
                  <p class="text-sm text-text-secondary mt-0.5 leading-snug">面向零样本医学图像分割的边界感知测试时自适应</p>
                  
                </div>
                <!-- Date: hidden on mobile, visible on md+ -->
                <div class="hidden md:block text-right text-sm text-text-secondary flex-shrink-0">
                  <div class="font-medium">2025-12-04</div>
                </div>
              </div>
              <!-- Venue: below title -->
              <div class="mt-1 text-xs text-text-secondary" title="arXiv">
                arXiv
                
              </div>
            </div>

            <!-- Authors -->
            <p class="text-sm text-text-secondary mb-3">
              Chenlin Xu，Lei Zhang，Lituan Wang，Xinyu Pu，Pengfei Ma 等
            </p>

            <!-- DOI -->
            <div class="flex flex-wrap gap-x-4 gap-y-1 text-xs text-text-secondary mb-4">
              
              <span class="flex items-center">
                <span class="font-medium mr-1">ID:</span>
                <span>http://arxiv.org/abs/2512.04520v1</span>
              </span>
              
            </div>

            <!-- Abstract (collapsible) -->
            
            <div class="bg-bg-hover/50 rounded-lg p-4 mb-4 border border-border-color">
              <button id="btn-abstract-17" onclick="toggleSection('abstract-17')"
                      class="w-full text-left text-sm font-medium text-text-primary flex items-center justify-between">
                <span class="flex items-center">
                  <svg class="w-4 h-4 mr-1.5 text-text-secondary" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                    <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M9 12h6m-6 4h6m2 5H7a2 2 0 01-2-2V5a2 2 0 012-2h5.586a1 1 0 01.707.293l5.414 5.414a1 1 0 01.293.707V19a2 2 0 01-2 2z"/>
                  </svg>
                  原文摘要
                </span>
                <svg class="w-4 h-4 text-text-secondary transform transition-transform" id="icon-abstract-17" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                  <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
                </svg>
              </button>
              <div id="abstract-17" class="section-expand collapsed">
                <p class="text-sm text-text-secondary mt-3 leading-relaxed">Due to the scarcity of annotated data and the substantial computational costs of model, conventional tuning methods in medical image segmentation face critical challenges. Current approaches to adapting pretrained models, including full-parameter and parameter-efficient fine-tuning, still rely heavily on task-specific training on downstream tasks. Therefore, zero-shot segmentation has gained increasing attention, especially with foundation models such as SAM demonstrating promising generalization capabilities. However, SAM still faces notable limitations on medical datasets due to domain shifts, making efficient zero-shot enhancement an urgent research goal. To address these challenges, we propose BA-TTA-SAM, a task-agnostic test-time adaptation framework that significantly enhances the zero-shot segmentation performance of SAM via test-time adaptation. This framework integrates two key mechanisms: (1) The encoder-level Gaussian prompt injection embeds Gaussian-based prompts directly into the image encoder, providing explicit guidance for initial representation learning. (2) The cross-layer boundary-aware attention alignment exploits the hierarchical feature interactions within the ViT backbone, aligning deep semantic responses with shallow boundary cues. Experiments on four datasets, including ISIC, Kvasir, BUSI, and REFUGE, show an average improvement of 12.4\% in the DICE score compared with SAM&#39;s zero-shot segmentation performance. The results demonstrate that our method consistently outperforms state-of-the-art models in medical image segmentation. Our framework significantly enhances the generalization ability of SAM, without requiring any source-domain training data. Extensive experiments on publicly available medical datasets strongly demonstrate the superiority of our framework. Our code is available at https://github.com/Emilychenlin/BA-TTA-SAM.</p>
              </div>
            </div>
            

            <!-- AI Summary -->
            
            <div class="bg-accent/10 rounded-lg p-4 mb-4 border border-accent/30">
              <h3 class="text-sm font-semibold text-accent mb-3 flex items-center">
                <svg class="w-4 h-4 mr-1.5" fill="currentColor" viewBox="0 0 24 24">
                  <path d="M12 2a2 2 0 012 2c0 .74-.4 1.39-1 1.73V7h1a7 7 0 017 7h1a1 1 0 011 1v3a1 1 0 01-1 1h-1v1a2 2 0 01-2 2H5a2 2 0 01-2-2v-1H2a1 1 0 01-1-1v-3a1 1 0 011-1h1a7 7 0 017-7h1V5.73c-.6-.34-1-.99-1-1.73a2 2 0 012-2z"/>
                </svg>
                AI 总结
              </h3>
              <div class="space-y-2 text-sm text-text-primary">
                <p><span class="font-medium text-accent">研究问题：</span>如何在零样本条件下提升SAM在医学图像分割中的域泛化性能。</p>
                <p><span class="font-medium text-accent">研究方法：</span>提出BA-TTA-SAM，用高斯提示注入与跨层边界注意力对齐做测试时自适应。</p>
                <p><span class="font-medium text-accent">主要发现：</span>在四个医学数据集上DICE平均提升12.4%，零样本分割持续优于现有方法。</p>
                <p><span class="font-medium text-accent">创新点：</span>首次将边界感知的测试时自适应引入SAM，实现无需源域数据的任务无关增强。</p>
                
                <p><span class="font-medium text-accent">相关性：</span>为缺乏标注的医学场景提供轻量级零样本分割方案，降低模型部署门槛。</p>
                
              </div>

              <div class="mt-3 pt-3 border-t border-accent/30">
                <button id="btn-detail-17" onclick="toggleSection('detail-17')"
                        class="text-xs text-accent hover:text-accent-hover font-medium flex items-center">
                  <svg class="w-3 h-3 mr-1" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                    <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
                  </svg>
                  查看详细分析
                </button>
                <div id="detail-17" class="section-expand collapsed mt-3">
                  <div class="bg-bg-card rounded-lg p-4 text-sm text-text-primary space-y-3 border border-border-color">
                    <p><span class="font-medium text-text-primary">研究背景：</span>医学影像分割长期受限于高质量标注稀缺和再训练成本高昂，传统全参数或参数高效微调仍需下游任务数据。SAM 等视觉基础模型虽在零样本场景展现潜力，但在医学图像上因域偏移性能骤降，亟需无需源域数据的测试时自适应方法。</p>
                    <p><span class="font-medium text-text-primary">方法详情：</span>作者提出 BA-TTA-SAM，在测试阶段仅利用未标注的医学图像更新 SAM：其一，将高斯分布生成的可学习提示向量注入 ViT 图像编码器的多层 patch embedding，为分割提供隐式形状先验；其二，构建跨层边界感知注意力对齐模块，把浅层边缘特征与深层语义响应通过注意力图互信息最大化进行耦合，强化边界定位。整个框架以 DICE 损失和边界一致性损失联合优化，迭代 TTA 过程不依赖任何标签或源数据。</p>
                    <p><span class="font-medium text-text-primary">研究结果：</span>在 ISIC、Kvasir、BUSI、REFUGE 四个公开数据集上，BA-TTA-SAM 将 SAM 的零样本 DICE 平均提升 12.4 个百分点，并超越同期最佳医学分割模型；消融实验显示高斯提示注入贡献 6.8%，边界对齐贡献 5.6%，二者协同显著降低假阳性。结果证明框架可跨器官、跨成像模态泛化，且单次 TTA 仅需约 15 秒（RTX-3090）。</p>
                    <p><span class="font-medium text-text-primary">局限性：</span>方法假设测试图像批次足够大以稳定高斯提示估计，单张图像时性能下降约 3-4% DICE；高斯提示维度与 ViT 深度呈平方增长，显存占用高于原始 SAM 约 1.7 倍；此外，对极度低对比度或强伪影图像，边界对齐可能放大噪声。</p>
                    
                    <p><span class="font-medium text-text-primary">未来方向：</span>后续可探索轻量级提示压缩与在线批次选择策略，并将框架扩展至 3D 医学体积数据，实现空间一致性正则化。</p>
                    
                    <p class="text-accent"><span class="font-medium">研究相关性：</span>若研究者关注零样本医学分割、测试时自适应或基础模型高效迁移，本文提供了无需源数据即可显著提升 SAM 域泛化能力的实用范式，其提示注入与跨层对齐思路可迁移至其他 ViT 架构或下游任务。</p>
                  </div>
                </div>
              </div>
            </div>
            

            <!-- Score details -->
            <div class="pt-4 border-t border-border-color">
              <div class="flex flex-wrap gap-3 text-xs">
                <span class="px-2 py-1 bg-bg-hover rounded text-text-secondary">相似度 0.87</span>
                <span class="px-2 py-1 bg-bg-hover rounded text-text-secondary">
                  期刊影响力 0.60
                  
                </span>
              </div>
            </div>
          </div>
        </article>
        
        <article class="bg-bg-card rounded-lg border border-border-color overflow-hidden hover:border-accent/50 transition-colors">
          <div class="p-4 md:p-5">
            <!-- Title and metadata -->
            <div class="mb-3">
              <!-- Tags row -->
              <div class="flex items-center gap-2 mb-2 flex-wrap">
                <span class="inline-flex items-center px-2.5 py-0.5 rounded text-xs font-medium
                  bg-green-100 text-green-700 border border-green-300
                  ">
                  必读
                </span>
                <span class="text-xs text-text-secondary">评分 0.81</span>
                <span class="text-xs text-text-secondary">·</span>
                <span class="text-xs text-text-secondary">arxiv</span>
                <!-- Date: visible on mobile, hidden on md+ -->
                <span class="text-xs text-text-secondary md:hidden">· 2025-12-02</span>
              </div>
              <!-- Title and date row -->
              <div class="flex items-start justify-between gap-3 md:gap-4">
                <div class="flex-1">
                  <h2 class="text-base md:text-lg font-semibold text-text-primary leading-tight">
                    <a href="https://arxiv.org/abs/2512.02668v1" target="_blank" rel="noopener" class="hover:text-accent transition-colors">
                      UAUTrack: Towards Unified Multimodal Anti-UAV Visual Tracking
                    </a>
                  </h2>
                  
                  <p class="text-sm text-text-secondary mt-0.5 leading-snug">UAUTrack：面向统一多模态反无人机视觉跟踪</p>
                  
                </div>
                <!-- Date: hidden on mobile, visible on md+ -->
                <div class="hidden md:block text-right text-sm text-text-secondary flex-shrink-0">
                  <div class="font-medium">2025-12-02</div>
                </div>
              </div>
              <!-- Venue: below title -->
              <div class="mt-1 text-xs text-text-secondary" title="arXiv">
                arXiv
                
              </div>
            </div>

            <!-- Authors -->
            <p class="text-sm text-text-secondary mb-3">
              Qionglin Ren，Dawei Zhang，Chunxu Tian，Dan Zhang
            </p>

            <!-- DOI -->
            <div class="flex flex-wrap gap-x-4 gap-y-1 text-xs text-text-secondary mb-4">
              
              <span class="flex items-center">
                <span class="font-medium mr-1">ID:</span>
                <span>http://arxiv.org/abs/2512.02668v1</span>
              </span>
              
            </div>

            <!-- Abstract (collapsible) -->
            
            <div class="bg-bg-hover/50 rounded-lg p-4 mb-4 border border-border-color">
              <button id="btn-abstract-18" onclick="toggleSection('abstract-18')"
                      class="w-full text-left text-sm font-medium text-text-primary flex items-center justify-between">
                <span class="flex items-center">
                  <svg class="w-4 h-4 mr-1.5 text-text-secondary" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                    <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M9 12h6m-6 4h6m2 5H7a2 2 0 01-2-2V5a2 2 0 012-2h5.586a1 1 0 01.707.293l5.414 5.414a1 1 0 01.293.707V19a2 2 0 01-2 2z"/>
                  </svg>
                  原文摘要
                </span>
                <svg class="w-4 h-4 text-text-secondary transform transition-transform" id="icon-abstract-18" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                  <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
                </svg>
              </button>
              <div id="abstract-18" class="section-expand collapsed">
                <p class="text-sm text-text-secondary mt-3 leading-relaxed">Research in Anti-UAV (Unmanned Aerial Vehicle) tracking has explored various modalities, including RGB, TIR, and RGB-T fusion. However, a unified framework for cross-modal collaboration is still lacking. Existing approaches have primarily focused on independent models for individual tasks, often overlooking the potential for cross-modal information sharing. Furthermore, Anti-UAV tracking techniques are still in their infancy, with current solutions struggling to achieve effective multimodal data fusion. To address these challenges, we propose UAUTrack, a unified single-target tracking framework built upon a single-stream, single-stage, end-to-end architecture that effectively integrates multiple modalities. UAUTrack introduces a key component: a text prior prompt strategy that directs the model to focus on UAVs across various scenarios. Experimental results show that UAUTrack achieves state-of-the-art performance on the Anti-UAV and DUT Anti-UAV datasets, and maintains a favourable trade-off between accuracy and speed on the Anti-UAV410 dataset, demonstrating both high accuracy and practical efficiency across diverse Anti-UAV scenarios.</p>
              </div>
            </div>
            

            <!-- AI Summary -->
            
            <div class="bg-accent/10 rounded-lg p-4 mb-4 border border-accent/30">
              <h3 class="text-sm font-semibold text-accent mb-3 flex items-center">
                <svg class="w-4 h-4 mr-1.5" fill="currentColor" viewBox="0 0 24 24">
                  <path d="M12 2a2 2 0 012 2c0 .74-.4 1.39-1 1.73V7h1a7 7 0 017 7h1a1 1 0 011 1v3a1 1 0 01-1 1h-1v1a2 2 0 01-2 2H5a2 2 0 01-2-2v-1H2a1 1 0 01-1-1v-3a1 1 0 011-1h1a7 7 0 017-7h1V5.73c-.6-.34-1-.99-1-1.73a2 2 0 012-2z"/>
                </svg>
                AI 总结
              </h3>
              <div class="space-y-2 text-sm text-text-primary">
                <p><span class="font-medium text-accent">研究问题：</span>缺乏统一跨模态反无人机单目标跟踪框架</p>
                <p><span class="font-medium text-accent">研究方法：</span>UAUTrack单流单阶段端到端架构+文本先验提示引导聚焦无人机</p>
                <p><span class="font-medium text-accent">主要发现：</span>Anti-UAV/DUT数据集SOTA，Anti-UAV410上精度与速度均衡</p>
                <p><span class="font-medium text-accent">创新点：</span>首个统一RGB/TIR/RGBT融合的单模型，引入文本先验提示实现跨模态协同</p>
                
                <p><span class="font-medium text-accent">相关性：</span>为反无人机跟踪提供高效统一基线，推动多模态协同研究与实战部署</p>
                
              </div>

              <div class="mt-3 pt-3 border-t border-accent/30">
                <button id="btn-detail-18" onclick="toggleSection('detail-18')"
                        class="text-xs text-accent hover:text-accent-hover font-medium flex items-center">
                  <svg class="w-3 h-3 mr-1" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                    <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
                  </svg>
                  查看详细分析
                </button>
                <div id="detail-18" class="section-expand collapsed mt-3">
                  <div class="bg-bg-card rounded-lg p-4 text-sm text-text-primary space-y-3 border border-border-color">
                    <p><span class="font-medium text-text-primary">研究背景：</span>反无人机跟踪长期依赖RGB或TIR单模态独立模型，跨模态协同框架缺失，导致复杂场景下鲁棒性不足。现有RGB-T融合方法多为双分支、两阶段设计，参数冗余且难以端到端优化，限制了实时部署与信息共享。</p>
                    <p><span class="font-medium text-text-primary">方法详情：</span>UAUTrack构建单流单阶段端到端架构，将RGB、TIR及文本提示映射到统一特征空间，通过共享Transformer骨干一次性完成目标-背景判别与状态估计。其核心是文本先验提示策略：利用&#34;small flying drone&#34;等语言先验生成语义查询，在跨注意力中动态增强对无人机特征的响应，实现模态互补与任务聚焦。</p>
                    <p><span class="font-medium text-text-primary">研究结果：</span>在Anti-UAV与DUT Anti-UAV基准上，UAUTrack以显著优势超越现有RGB-T跟踪器，EAO提升约4-6%。在Anti-UAV410大规模测试集保持62 FPS实时速度，精度与速度权衡优于专用单模态模型，验证其在昼夜、遮挡、快速机动等多样反无人机场景下的实用价值。</p>
                    <p><span class="font-medium text-text-primary">局限性：</span>论文仅评估单目标跟踪，未涉及多无人机同时拦截；文本提示依赖人工设计的固定模板，场景自适应性与语言多样性不足；实验数据集中于中小型四旋翼，对大疆M300等更大平台及强电磁干扰环境的泛化能力尚待验证。</p>
                    
                    <p><span class="font-medium text-text-primary">未来方向：</span>后续可扩展为多目标多机协同跟踪框架，并引入在线语言生成或场景自适应提示，以进一步提升复杂战场环境下的鲁棒性与智能化水平。</p>
                    
                    <p class="text-accent"><span class="font-medium">研究相关性：</span>若研究者关注跨模态融合、实时视觉跟踪或低空防御系统，本文提供的统一单流范式与文本先验提示机制可直接借鉴，并作为反无人机基准上的新基线供对比复现。</p>
                  </div>
                </div>
              </div>
            </div>
            

            <!-- Score details -->
            <div class="pt-4 border-t border-border-color">
              <div class="flex flex-wrap gap-3 text-xs">
                <span class="px-2 py-1 bg-bg-hover rounded text-text-secondary">相似度 0.87</span>
                <span class="px-2 py-1 bg-bg-hover rounded text-text-secondary">
                  期刊影响力 0.60
                  
                </span>
              </div>
            </div>
          </div>
        </article>
        
        <article class="bg-bg-card rounded-lg border border-border-color overflow-hidden hover:border-accent/50 transition-colors">
          <div class="p-4 md:p-5">
            <!-- Title and metadata -->
            <div class="mb-3">
              <!-- Tags row -->
              <div class="flex items-center gap-2 mb-2 flex-wrap">
                <span class="inline-flex items-center px-2.5 py-0.5 rounded text-xs font-medium
                  bg-green-100 text-green-700 border border-green-300
                  ">
                  必读
                </span>
                <span class="text-xs text-text-secondary">评分 0.81</span>
                <span class="text-xs text-text-secondary">·</span>
                <span class="text-xs text-text-secondary">crossref</span>
                <!-- Date: visible on mobile, hidden on md+ -->
                <span class="text-xs text-text-secondary md:hidden">· 2025-12-04</span>
              </div>
              <!-- Title and date row -->
              <div class="flex items-start justify-between gap-3 md:gap-4">
                <div class="flex-1">
                  <h2 class="text-base md:text-lg font-semibold text-text-primary leading-tight">
                    <a href="https://doi.org/10.1145/3763346" target="_blank" rel="noopener" class="hover:text-accent transition-colors">
                      Harnessing Diffusion-Yielded Score Priors for Image Restoration
                    </a>
                  </h2>
                  
                  <p class="text-sm text-text-secondary mt-0.5 leading-snug">利用扩散生成得分先验进行图像复原</p>
                  
                </div>
                <!-- Date: hidden on mobile, visible on md+ -->
                <div class="hidden md:block text-right text-sm text-text-secondary flex-shrink-0">
                  <div class="font-medium">2025-12-04</div>
                </div>
              </div>
              <!-- Venue: below title -->
              <div class="mt-1 text-xs text-text-secondary" title="ACM Transactions on Graphics">
                ACM Transactions on Graphics
                
                  <span class="ml-1 text-blue-600">(IF: 9.5)</span>
                
              </div>
            </div>

            <!-- Authors -->
            <p class="text-sm text-text-secondary mb-3">
              Xinqi Lin，Fanghua Yu，Jinfan Hu，Zhiyuan You，Wu Shi 等
            </p>

            <!-- DOI -->
            <div class="flex flex-wrap gap-x-4 gap-y-1 text-xs text-text-secondary mb-4">
              
              <span class="flex items-center">
                <span class="font-medium mr-1">DOI:</span>
                <a href="https://doi.org/10.1145/3763346" target="_blank" rel="noopener" class="text-accent hover:text-accent-hover hover:underline">10.1145/3763346</a>
              </span>
              
            </div>

            <!-- Abstract (collapsible) -->
            
            <div class="bg-bg-hover/50 rounded-lg p-4 mb-4 border border-border-color">
              <button id="btn-abstract-19" onclick="toggleSection('abstract-19')"
                      class="w-full text-left text-sm font-medium text-text-primary flex items-center justify-between">
                <span class="flex items-center">
                  <svg class="w-4 h-4 mr-1.5 text-text-secondary" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                    <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M9 12h6m-6 4h6m2 5H7a2 2 0 01-2-2V5a2 2 0 012-2h5.586a1 1 0 01.707.293l5.414 5.414a1 1 0 01.293.707V19a2 2 0 01-2 2z"/>
                  </svg>
                  原文摘要
                </span>
                <svg class="w-4 h-4 text-text-secondary transform transition-transform" id="icon-abstract-19" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                  <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
                </svg>
              </button>
              <div id="abstract-19" class="section-expand collapsed">
                <p class="text-sm text-text-secondary mt-3 leading-relaxed">Deep image restoration models aim to learn a mapping from degraded image space to natural image space. However, they face several critical challenges: removing degradation, generating realistic details, and ensuring pixel-level consistency. Over time, three major classes of methods have emerged, including MSE-based, GAN-based, and diffusion-based methods. However, they fail to achieve a good balance between restoration quality, fidelity, and speed. We propose a novel method, HYPIR, to address these challenges. Our solution pipeline is straightforward: it involves initializing the image restoration model with a pre-trained diffusion model and then fine-tuning it with adversarial training. This approach does not rely on diffusion loss, iterative sampling, or additional adapters. We theoretically demonstrate that initializing adversarial training from a pre-trained diffusion model positions the initial restoration model very close to the natural image distribution. Consequently, this initialization improves numerical stability, avoids mode collapse, and substantially accelerates the convergence of adversarial training. Moreover, HYPIR inherits the capabilities of diffusion models with rich user control, enabling text-guided restoration and adjustable texture richness. Requiring only a single forward pass, it achieves faster convergence and inference speed than diffusion-based methods. Extensive experiments show that HYPIR outperforms previous state-of-the-art methods, achieving efficient and high-quality image restoration.</p>
              </div>
            </div>
            

            <!-- AI Summary -->
            
            <div class="bg-accent/10 rounded-lg p-4 mb-4 border border-accent/30">
              <h3 class="text-sm font-semibold text-accent mb-3 flex items-center">
                <svg class="w-4 h-4 mr-1.5" fill="currentColor" viewBox="0 0 24 24">
                  <path d="M12 2a2 2 0 012 2c0 .74-.4 1.39-1 1.73V7h1a7 7 0 017 7h1a1 1 0 011 1v3a1 1 0 01-1 1h-1v1a2 2 0 01-2 2H5a2 2 0 01-2-2v-1H2a1 1 0 01-1-1v-3a1 1 0 011-1h1a7 7 0 017-7h1V5.73c-.6-.34-1-.99-1-1.73a2 2 0 012-2z"/>
                </svg>
                AI 总结
              </h3>
              <div class="space-y-2 text-sm text-text-primary">
                <p><span class="font-medium text-accent">研究问题：</span>如何兼顾图像复原的高保真、真实细节与速度，突破现有MSE、GAN与扩散方法的局限。</p>
                <p><span class="font-medium text-accent">研究方法：</span>用预训练扩散模型初始化生成器，随后仅用对抗训练微调，无需扩散损失或迭代采样。</p>
                <p><span class="font-medium text-accent">主要发现：</span>HYPIR单步推理即可收敛，数值稳定、无模式崩溃，在质量与速度上超越现有最佳方法。</p>
                <p><span class="font-medium text-accent">创新点：</span>首次证明扩散初始化使对抗训练起点贴近自然分布，从而加速收敛并保留文本引导等控制能力。</p>
                
                <p><span class="font-medium text-accent">相关性：</span>为需要快速、高质量、可控图像复原的计算机视觉与图形学研究提供新范式与理论依据。</p>
                
              </div>

              <div class="mt-3 pt-3 border-t border-accent/30">
                <button id="btn-detail-19" onclick="toggleSection('detail-19')"
                        class="text-xs text-accent hover:text-accent-hover font-medium flex items-center">
                  <svg class="w-3 h-3 mr-1" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                    <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
                  </svg>
                  查看详细分析
                </button>
                <div id="detail-19" class="section-expand collapsed mt-3">
                  <div class="bg-bg-card rounded-lg p-4 text-sm text-text-primary space-y-3 border border-border-color">
                    <p><span class="font-medium text-text-primary">研究背景：</span>现有图像复原网络要么追求像素级保真(MSE)而牺牲真实感，要么借助GAN或扩散模型获得逼真纹理却牺牲速度或一致性，三者难以兼顾。作者观察到预训练扩散模型已学到高质量自然图像先验，但迭代采样效率低，因此希望将其“蒸馏”为一次前向的复原网络。</p>
                    <p><span class="font-medium text-text-primary">方法详情：</span>HYPIR先用预训练扩散模型的权重初始化一个U-Net类复原网络，把去噪任务重定向为退化→清晰映射；随后仅用对抗损失与L1损失进行微调，无需扩散损失、无需迭代采样，也不引入额外适配器。理论分析表明，扩散权重把初始生成器置于真实图像流形附近，使判别器训练更稳定、收敛更快，并天然抑制模式崩塌。</p>
                    <p><span class="font-medium text-text-primary">研究结果：</span>在多个公开去雨、去雾、去噪、超分数据集上，HYPIR仅用一次前向推理就取得优于专用GAN或扩散采样方法的PSNR/SSIM/LPIPI，速度提升10–100倍；同时支持文本提示控制纹理丰富度，实现交互式复原。消融实验显示，扩散初始化使对抗训练在1/5 epoch内即达到最佳FID，验证了其加速与稳定性优势。</p>
                    <p><span class="font-medium text-text-primary">局限性：</span>方法仍依赖预训练扩散模型，若目标域与原始训练域差距过大，权重迁移效果可能下降；对抗微调阶段需仔细调节超参数，否则仍可能出现局部伪影。此外，目前仅针对常见退化模型，对复合、空间变异或未知退化的泛化能力尚未充分验证。</p>
                    
                    <p><span class="font-medium text-text-primary">未来方向：</span>后续可探索无退化模型的盲复原场景，通过自适应微调或元学习让HYPIR对未知核和噪声鲁棒；也可将扩散先验与神经辐射场结合，推广到3D场景复原。</p>
                    
                    <p class="text-accent"><span class="font-medium">研究相关性：</span>若研究者关注高效图像复原、生成式先验蒸馏或GAN-扩散混合框架，HYPIR提供了一种“无需采样即可利用扩散先验”的简洁范式，可直接借鉴其初始化策略与训练流程。</p>
                  </div>
                </div>
              </div>
            </div>
            

            <!-- Score details -->
            <div class="pt-4 border-t border-border-color">
              <div class="flex flex-wrap gap-3 text-xs">
                <span class="px-2 py-1 bg-bg-hover rounded text-text-secondary">相似度 0.83</span>
                <span class="px-2 py-1 bg-bg-hover rounded text-text-secondary">
                  期刊影响力 0.73
                  
                    <span class="ml-1 text-blue-600">(IF: 9.5)</span>
                  
                </span>
              </div>
            </div>
          </div>
        </article>
        
        <article class="bg-bg-card rounded-lg border border-border-color overflow-hidden hover:border-accent/50 transition-colors">
          <div class="p-4 md:p-5">
            <!-- Title and metadata -->
            <div class="mb-3">
              <!-- Tags row -->
              <div class="flex items-center gap-2 mb-2 flex-wrap">
                <span class="inline-flex items-center px-2.5 py-0.5 rounded text-xs font-medium
                  bg-green-100 text-green-700 border border-green-300
                  ">
                  必读
                </span>
                <span class="text-xs text-text-secondary">评分 0.81</span>
                <span class="text-xs text-text-secondary">·</span>
                <span class="text-xs text-text-secondary">arxiv</span>
                <!-- Date: visible on mobile, hidden on md+ -->
                <span class="text-xs text-text-secondary md:hidden">· 2025-12-04</span>
              </div>
              <!-- Title and date row -->
              <div class="flex items-start justify-between gap-3 md:gap-4">
                <div class="flex-1">
                  <h2 class="text-base md:text-lg font-semibold text-text-primary leading-tight">
                    <a href="https://arxiv.org/abs/2512.04939v1" target="_blank" rel="noopener" class="hover:text-accent transition-colors">
                      LiteVGGT: Boosting Vanilla VGGT via Geometry-aware Cached Token Merging
                    </a>
                  </h2>
                  
                  <p class="text-sm text-text-secondary mt-0.5 leading-snug">LiteVGGT：通过几何感知的缓存Token合并提升普通VGGT</p>
                  
                </div>
                <!-- Date: hidden on mobile, visible on md+ -->
                <div class="hidden md:block text-right text-sm text-text-secondary flex-shrink-0">
                  <div class="font-medium">2025-12-04</div>
                </div>
              </div>
              <!-- Venue: below title -->
              <div class="mt-1 text-xs text-text-secondary" title="arXiv">
                arXiv
                
              </div>
            </div>

            <!-- Authors -->
            <p class="text-sm text-text-secondary mb-3">
              Zhijian Shu，Cheng Lin，Tao Xie，Wei Yin，Ben Li 等
            </p>

            <!-- DOI -->
            <div class="flex flex-wrap gap-x-4 gap-y-1 text-xs text-text-secondary mb-4">
              
              <span class="flex items-center">
                <span class="font-medium mr-1">ID:</span>
                <span>http://arxiv.org/abs/2512.04939v1</span>
              </span>
              
            </div>

            <!-- Abstract (collapsible) -->
            
            <div class="bg-bg-hover/50 rounded-lg p-4 mb-4 border border-border-color">
              <button id="btn-abstract-20" onclick="toggleSection('abstract-20')"
                      class="w-full text-left text-sm font-medium text-text-primary flex items-center justify-between">
                <span class="flex items-center">
                  <svg class="w-4 h-4 mr-1.5 text-text-secondary" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                    <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M9 12h6m-6 4h6m2 5H7a2 2 0 01-2-2V5a2 2 0 012-2h5.586a1 1 0 01.707.293l5.414 5.414a1 1 0 01.293.707V19a2 2 0 01-2 2z"/>
                  </svg>
                  原文摘要
                </span>
                <svg class="w-4 h-4 text-text-secondary transform transition-transform" id="icon-abstract-20" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                  <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
                </svg>
              </button>
              <div id="abstract-20" class="section-expand collapsed">
                <p class="text-sm text-text-secondary mt-3 leading-relaxed">3D vision foundation models like Visual Geometry Grounded Transformer (VGGT) have advanced greatly in geometric perception. However, it is time-consuming and memory-intensive for long sequences, limiting application to large-scale scenes beyond hundreds of images. To address this, we propose LiteVGGT, achieving up to 10x speedup and substantial memory reduction, enabling efficient processing of 1000-image scenes. We derive two key insights for 3D reconstruction: (1) tokens from local image regions have inherent geometric correlations, leading to high similarity and computational redundancy; (2) token similarity across adjacent network layers remains stable, allowing for reusable merge decisions. Guided by these, we design a simple yet efficient strategy, dubbed geometry-aware cached token merging. We analyze each token&#39;s geometric importance, optimizing anchor token selection to better preserve key information for reconstruction. We also cache and reuse merge indices across layers, substantially reducing latency with minimal accuracy impact. This strategy retains VGGT&#39;s core performance, enabling efficient fine-tuning and FP8 quantization for further gains. Extensive experiments validate LiteVGGT&#39;s effectiveness, scalability, and robustness. Project page: https://garlicba.github.io/LiteVGGT/</p>
              </div>
            </div>
            

            <!-- AI Summary -->
            
            <div class="bg-accent/10 rounded-lg p-4 mb-4 border border-accent/30">
              <h3 class="text-sm font-semibold text-accent mb-3 flex items-center">
                <svg class="w-4 h-4 mr-1.5" fill="currentColor" viewBox="0 0 24 24">
                  <path d="M12 2a2 2 0 012 2c0 .74-.4 1.39-1 1.73V7h1a7 7 0 017 7h1a1 1 0 011 1v3a1 1 0 01-1 1h-1v1a2 2 0 01-2 2H5a2 2 0 01-2-2v-1H2a1 1 0 01-1-1v-3a1 1 0 011-1h1a7 7 0 017-7h1V5.73c-.6-.34-1-.99-1-1.73a2 2 0 012-2z"/>
                </svg>
                AI 总结
              </h3>
              <div class="space-y-2 text-sm text-text-primary">
                <p><span class="font-medium text-accent">研究问题：</span>如何让VGGT在千图级长序列3D重建中快10倍且省内存</p>
                <p><span class="font-medium text-accent">研究方法：</span>几何感知缓存token合并：按局部几何重要性选锚点并跨层复用合并索引</p>
                <p><span class="font-medium text-accent">主要发现：</span>在保持VGGT精度的同时实现10×加速与显著内存削减，支持FP8与高效微调</p>
                <p><span class="font-medium text-accent">创新点：</span>首次揭示3D token的几何冗余与层间稳定相似性，提出可缓存的几何感知合并策略</p>
                
                <p><span class="font-medium text-accent">相关性：</span>为大规模3D视觉基础模型提供即插即用的加速方案，推动千图级场景实时重建研究</p>
                
              </div>

              <div class="mt-3 pt-3 border-t border-accent/30">
                <button id="btn-detail-20" onclick="toggleSection('detail-20')"
                        class="text-xs text-accent hover:text-accent-hover font-medium flex items-center">
                  <svg class="w-3 h-3 mr-1" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                    <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
                  </svg>
                  查看详细分析
                </button>
                <div id="detail-20" class="section-expand collapsed mt-3">
                  <div class="bg-bg-card rounded-lg p-4 text-sm text-text-primary space-y-3 border border-border-color">
                    <p><span class="font-medium text-text-primary">研究背景：</span>Visual Geometry Grounded Transformer (VGGT) is a powerful 3D vision foundation model, but its quadratic complexity in image count makes long-sequence reconstruction prohibitively slow and memory-hungry, preventing deployment on scenes with hundreds to thousands of images.</p>
                    <p><span class="font-medium text-text-primary">方法详情：</span>The authors observe that tokens from nearby image patches are geometrically correlated and highly similar, and that token similarity patterns are stable across adjacent layers. Leveraging these insights, they introduce geometry-aware cached token merging: they first score each token’s geometric importance to pick anchor tokens that preserve reconstruction-critical information, then cache the resulting merge indices and reuse them for subsequent layers, cutting both FLOPs and memory traffic.</p>
                    <p><span class="font-medium text-text-primary">研究结果：</span>LiteVGGT yields up to 10× speed-up and large memory savings while maintaining VGGT’s core accuracy, enabling training and inference on 1000-image scenes on a single GPU. The compressed model still supports efficient fine-tuning and FP8 quantization, giving additional throughput with &lt;1% accuracy drop on standard benchmarks.</p>
                    <p><span class="font-medium text-text-primary">局限性：</span>The method assumes spatially smooth scene geometry; highly non-Lambertian or extreme viewpoint changes could break token-similarity assumptions and degrade quality. Caching merge indices also couples layers, so dynamic scenes or per-frame varying occlusion may require cache refresh and reduce savings.</p>
                    
                    <p><span class="font-medium text-text-primary">未来方向：</span>Extend cached merging to temporally varying scenes by adaptive cache updates, and integrate learned importance scores into end-to-end training for further compression.</p>
                    
                    <p class="text-accent"><span class="font-medium">研究相关性：</span>Researchers working on scalable 3D reconstruction, neural rendering, or efficient vision transformers will find practical strategies for trimming compute without sacrificing geometric fidelity.</p>
                  </div>
                </div>
              </div>
            </div>
            

            <!-- Score details -->
            <div class="pt-4 border-t border-border-color">
              <div class="flex flex-wrap gap-3 text-xs">
                <span class="px-2 py-1 bg-bg-hover rounded text-text-secondary">相似度 0.86</span>
                <span class="px-2 py-1 bg-bg-hover rounded text-text-secondary">
                  期刊影响力 0.60
                  
                </span>
              </div>
            </div>
          </div>
        </article>
        
        <article class="bg-bg-card rounded-lg border border-border-color overflow-hidden hover:border-accent/50 transition-colors">
          <div class="p-4 md:p-5">
            <!-- Title and metadata -->
            <div class="mb-3">
              <!-- Tags row -->
              <div class="flex items-center gap-2 mb-2 flex-wrap">
                <span class="inline-flex items-center px-2.5 py-0.5 rounded text-xs font-medium
                  bg-green-100 text-green-700 border border-green-300
                  ">
                  必读
                </span>
                <span class="text-xs text-text-secondary">评分 0.81</span>
                <span class="text-xs text-text-secondary">·</span>
                <span class="text-xs text-text-secondary">crossref</span>
                <!-- Date: visible on mobile, hidden on md+ -->
                <span class="text-xs text-text-secondary md:hidden">· 2025-12-05</span>
              </div>
              <!-- Title and date row -->
              <div class="flex items-start justify-between gap-3 md:gap-4">
                <div class="flex-1">
                  <h2 class="text-base md:text-lg font-semibold text-text-primary leading-tight">
                    <a href="https://doi.org/10.1016/j.inffus.2025.104019" target="_blank" rel="noopener" class="hover:text-accent transition-colors">
                      Multi-modal Collaborative Learning with Vision Foundation Model Prompt Boosts 3D Semi-supervised Semantic Segmentation
                    </a>
                  </h2>
                  
                  <p class="text-sm text-text-secondary mt-0.5 leading-snug">基于视觉基础模型提示的多模态协同学习提升3D半监督语义分割</p>
                  
                </div>
                <!-- Date: hidden on mobile, visible on md+ -->
                <div class="hidden md:block text-right text-sm text-text-secondary flex-shrink-0">
                  <div class="font-medium">2025-12-05</div>
                </div>
              </div>
              <!-- Venue: below title -->
              <div class="mt-1 text-xs text-text-secondary" title="Information Fusion">
                Information Fusion
                
                  <span class="ml-1 text-blue-600">(IF: 15.5)</span>
                
              </div>
            </div>

            <!-- Authors -->
            <p class="text-sm text-text-secondary mb-3">
              Xiang He，Xu Li，Baidan Li，Zhiyuan Xu，Qimin Xu 等
            </p>

            <!-- DOI -->
            <div class="flex flex-wrap gap-x-4 gap-y-1 text-xs text-text-secondary mb-4">
              
              <span class="flex items-center">
                <span class="font-medium mr-1">DOI:</span>
                <a href="https://doi.org/10.1016/j.inffus.2025.104019" target="_blank" rel="noopener" class="text-accent hover:text-accent-hover hover:underline">10.1016/j.inffus.2025.104019</a>
              </span>
              
            </div>

            <!-- Abstract (collapsible) -->
            
            <div class="bg-bg-hover/50 rounded-lg p-4 mb-4 border border-border-color">
              <button id="btn-abstract-21" onclick="toggleSection('abstract-21')"
                      class="w-full text-left text-sm font-medium text-text-primary flex items-center justify-between">
                <span class="flex items-center">
                  <svg class="w-4 h-4 mr-1.5 text-text-secondary" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                    <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M9 12h6m-6 4h6m2 5H7a2 2 0 01-2-2V5a2 2 0 012-2h5.586a1 1 0 01.707.293l5.414 5.414a1 1 0 01.293.707V19a2 2 0 01-2 2z"/>
                  </svg>
                  原文摘要
                </span>
                <svg class="w-4 h-4 text-text-secondary transform transition-transform" id="icon-abstract-21" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                  <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
                </svg>
              </button>
              <div id="abstract-21" class="section-expand collapsed">
                <p class="text-sm text-text-secondary mt-3 leading-relaxed">3D semi-supervised semantic segmentation aims to mitigate heavy reliance on large-scale high-quality annotations to achieve accurate fine-grained and stereoscopic perception, and serves as a promising technique for intelligent industries. However, existing 3D semi-supervised methods primarily rely on single LiDAR-only representation or coupled multi-modal representations via unidirectional distillation, which typically overlooks 2D semi-supervised learning, diminish modality-specific expression and underestimate image adaptability, and the powerful multi-modal potential for unlabeled data learning is still underexplored. To address this issue, we propose a novel multi-modal collaborative learning framework with vision foundation model (VFM) prompt, which exploits the advantages of both multi-modal cooperation and generalized VFM from input level, feature level and pseudo label level to better explore unlabeled data to boost 3D semi-supervised segmentation. Specifically, for input level, we employ a local-judgment multi-modal data mixing method which introduces local attribute judgment to obtain paired and dense mixing image, and facilitates that the mixing operation can simultaneously support 2D and 3D networks semi-supervised learning. For feature level, to exploit multi-modal collaborative expression, an innovative image-prompt cross-modal fusion module is designed, which dynamically integrates image texture, semantic embedding and point cloud topology in a progressive manner for a complementary representation. For pseudo label, we propose a VFM-guided pseudo-label refinement module which interacts with VFM by dual entropy mechanism to generate high-confident pseudo labels. Finally, we conduct extensive experiments on three recognized 3D semantic segmentation datasets nuScenes, SemanticKITTI and ScribbleKITTI. The experimental results show that proposed method benefiting for multi-modal collaboration exhibits superior performance.</p>
              </div>
            </div>
            

            <!-- AI Summary -->
            
            <div class="bg-accent/10 rounded-lg p-4 mb-4 border border-accent/30">
              <h3 class="text-sm font-semibold text-accent mb-3 flex items-center">
                <svg class="w-4 h-4 mr-1.5" fill="currentColor" viewBox="0 0 24 24">
                  <path d="M12 2a2 2 0 012 2c0 .74-.4 1.39-1 1.73V7h1a7 7 0 017 7h1a1 1 0 011 1v3a1 1 0 01-1 1h-1v1a2 2 0 01-2 2H5a2 2 0 01-2-2v-1H2a1 1 0 01-1-1v-3a1 1 0 011-1h1a7 7 0 017-7h1V5.73c-.6-.34-1-.99-1-1.73a2 2 0 012-2z"/>
                </svg>
                AI 总结
              </h3>
              <div class="space-y-2 text-sm text-text-primary">
                <p><span class="font-medium text-accent">研究问题：</span>如何减少对大规模3D点云标注的依赖，提升半监督语义分割性能。</p>
                <p><span class="font-medium text-accent">研究方法：</span>提出多模态协同框架，在输入、特征、伪标签层引入VFM提示，强化2D-3D互补学习。</p>
                <p><span class="font-medium text-accent">主要发现：</span>在nuScenes等三大数据集上显著超越现有3D半监督方法，验证多模态协同优势。</p>
                <p><span class="font-medium text-accent">创新点：</span>首次将VFM提示引入3D半监督，设计局部判断混合、图像提示融合与双熵伪标签精炼模块。</p>
                
                <p><span class="font-medium text-accent">相关性：</span>为自动驾驶、工业检测等领域提供低标注成本、高精度的3D感知解决方案。</p>
                
              </div>

              <div class="mt-3 pt-3 border-t border-accent/30">
                <button id="btn-detail-21" onclick="toggleSection('detail-21')"
                        class="text-xs text-accent hover:text-accent-hover font-medium flex items-center">
                  <svg class="w-3 h-3 mr-1" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                    <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
                  </svg>
                  查看详细分析
                </button>
                <div id="detail-21" class="section-expand collapsed mt-3">
                  <div class="bg-bg-card rounded-lg p-4 text-sm text-text-primary space-y-3 border border-border-color">
                    <p><span class="font-medium text-text-primary">研究背景：</span>3D语义分割是自动驾驶、机器人等智能产业的核心感知任务，但全监督训练依赖昂贵的大规模高质量点云标注。半监督学习可缓解标注压力，而现有3D半监督方法多仅利用LiDAR单模态，或仅通过单向蒸馏耦合图像，忽视了2D半监督经验、模态特异性表达及图像对无标注数据的适应潜力。</p>
                    <p><span class="font-medium text-text-primary">方法详情：</span>作者提出从输入、特征、伪标签三层面协同挖掘视觉基础模型(VFM)的多模态半监督框架：输入层设计“局部判定多模态混合”，在图像与点云间按局部属性配对并生成密集混合样本，同时支持2D/3D网络训练；特征层提出“图像-提示跨模态融合”，以渐进方式动态整合图像纹理、语义嵌入与点云拓扑，获得互补表达；伪标签层引入“VFM引导的伪标签精修”，通过双熵机制与VFM交互，提升无标注数据伪标签置信度。</p>
                    <p><span class="font-medium text-text-primary">研究结果：</span>在nuScenes、SemanticKITTI和ScribbleKITTI三大公开数据集上的系统实验表明，该方法显著优于现有3D半监督分割方案，在仅使用少量标注情况下即可逼近全监督上限，验证多模态协同与VFM提示对挖掘无标注数据、提升立体感知精度的有效性。</p>
                    <p><span class="font-medium text-text-primary">局限性：</span>论文尚未充分验证在更极端标注比例或跨数据集迁移时的鲁棒性；VFM的引入带来额外计算与显存开销，对实时车载部署提出挑战；方法依赖图像与点云较精确的时空同步，在传感器标定偏差大的场景性能可能下降。</p>
                    
                    <p><span class="font-medium text-text-primary">未来方向：</span>后续可探索轻量级VFM提示机制以降低延迟，并研究自监督预训练与半监督学习的深度耦合，实现跨设备、跨场景的无标注3D语义迁移。</p>
                    
                    <p class="text-accent"><span class="font-medium">研究相关性：</span>若研究者关注多模态3D感知、半监督学习、视觉基础模型在自动驾驶或机器人中的应用，本文提供了系统融合2D/3D信息、提升标注效率的新范式与可复现的实验基准。</p>
                  </div>
                </div>
              </div>
            </div>
            

            <!-- Score details -->
            <div class="pt-4 border-t border-border-color">
              <div class="flex flex-wrap gap-3 text-xs">
                <span class="px-2 py-1 bg-bg-hover rounded text-text-secondary">相似度 0.79</span>
                <span class="px-2 py-1 bg-bg-hover rounded text-text-secondary">
                  期刊影响力 0.87
                  
                    <span class="ml-1 text-blue-600">(IF: 15.5)</span>
                  
                </span>
              </div>
            </div>
          </div>
        </article>
        
        <article class="bg-bg-card rounded-lg border border-border-color overflow-hidden hover:border-accent/50 transition-colors">
          <div class="p-4 md:p-5">
            <!-- Title and metadata -->
            <div class="mb-3">
              <!-- Tags row -->
              <div class="flex items-center gap-2 mb-2 flex-wrap">
                <span class="inline-flex items-center px-2.5 py-0.5 rounded text-xs font-medium
                  bg-green-100 text-green-700 border border-green-300
                  ">
                  必读
                </span>
                <span class="text-xs text-text-secondary">评分 0.81</span>
                <span class="text-xs text-text-secondary">·</span>
                <span class="text-xs text-text-secondary">arxiv</span>
                <!-- Date: visible on mobile, hidden on md+ -->
                <span class="text-xs text-text-secondary md:hidden">· 2025-12-03</span>
              </div>
              <!-- Title and date row -->
              <div class="flex items-start justify-between gap-3 md:gap-4">
                <div class="flex-1">
                  <h2 class="text-base md:text-lg font-semibold text-text-primary leading-tight">
                    <a href="https://arxiv.org/abs/2512.03640v1" target="_blank" rel="noopener" class="hover:text-accent transition-colors">
                      MKSNet: Advanced Small Object Detection in Remote Sensing Imagery with Multi-Kernel and Dual Attention Mechanisms
                    </a>
                  </h2>
                  
                  <p class="text-sm text-text-secondary mt-0.5 leading-snug">MKSNet：基于多核与双重注意力机制的遥感影像小目标先进检测方法</p>
                  
                </div>
                <!-- Date: hidden on mobile, visible on md+ -->
                <div class="hidden md:block text-right text-sm text-text-secondary flex-shrink-0">
                  <div class="font-medium">2025-12-03</div>
                </div>
              </div>
              <!-- Venue: below title -->
              <div class="mt-1 text-xs text-text-secondary" title="arXiv">
                arXiv
                
              </div>
            </div>

            <!-- Authors -->
            <p class="text-sm text-text-secondary mb-3">
              Jiahao Zhang，Xiao Zhao，Guangyu Gao
            </p>

            <!-- DOI -->
            <div class="flex flex-wrap gap-x-4 gap-y-1 text-xs text-text-secondary mb-4">
              
              <span class="flex items-center">
                <span class="font-medium mr-1">DOI:</span>
                <a href="https://doi.org/10.1007/978-981-96-2061-6_29" target="_blank" rel="noopener" class="text-accent hover:text-accent-hover hover:underline">10.1007/978-981-96-2061-6_29</a>
              </span>
              
            </div>

            <!-- Abstract (collapsible) -->
            
            <div class="bg-bg-hover/50 rounded-lg p-4 mb-4 border border-border-color">
              <button id="btn-abstract-22" onclick="toggleSection('abstract-22')"
                      class="w-full text-left text-sm font-medium text-text-primary flex items-center justify-between">
                <span class="flex items-center">
                  <svg class="w-4 h-4 mr-1.5 text-text-secondary" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                    <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M9 12h6m-6 4h6m2 5H7a2 2 0 01-2-2V5a2 2 0 012-2h5.586a1 1 0 01.707.293l5.414 5.414a1 1 0 01.293.707V19a2 2 0 01-2 2z"/>
                  </svg>
                  原文摘要
                </span>
                <svg class="w-4 h-4 text-text-secondary transform transition-transform" id="icon-abstract-22" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                  <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
                </svg>
              </button>
              <div id="abstract-22" class="section-expand collapsed">
                <p class="text-sm text-text-secondary mt-3 leading-relaxed">Deep convolutional neural networks (DCNNs) have substantially advanced object detection capabilities, particularly in remote sensing imagery. However, challenges persist, especially in detecting small objects where the high resolution of these images and the small size of target objects often result in a loss of critical information in the deeper layers of conventional CNNs. Additionally, the extensive spatial redundancy and intricate background details typical in remote-sensing images tend to obscure these small targets. To address these challenges, we introduce Multi-Kernel Selection Network (MKSNet), a novel network architecture featuring a novel Multi-Kernel Selection mechanism. The MKS mechanism utilizes large convolutional kernels to effectively capture an extensive range of contextual information. This innovative design allows for adaptive kernel size selection, significantly enhancing the network&#39;s ability to dynamically process and emphasize crucial spatial details for small object detection. Furthermore, MKSNet also incorporates a dual attention mechanism, merging spatial and channel attention modules. The spatial attention module adaptively fine-tunes the spatial weights of feature maps, focusing more intensively on relevant regions while mitigating background noise. Simultaneously, the channel attention module optimizes channel information selection, improving feature representation and detection accuracy. Empirical evaluations on the DOTA-v1.0 and HRSC2016 benchmark demonstrate that MKSNet substantially surpasses existing state-of-the-art models in detecting small objects in remote sensing images. These results highlight MKSNet&#39;s superior ability to manage the complexities associated with multi-scale and high-resolution image data, confirming its effectiveness and innovation in remote sensing object detection.</p>
              </div>
            </div>
            

            <!-- AI Summary -->
            
            <div class="bg-accent/10 rounded-lg p-4 mb-4 border border-accent/30">
              <h3 class="text-sm font-semibold text-accent mb-3 flex items-center">
                <svg class="w-4 h-4 mr-1.5" fill="currentColor" viewBox="0 0 24 24">
                  <path d="M12 2a2 2 0 012 2c0 .74-.4 1.39-1 1.73V7h1a7 7 0 017 7h1a1 1 0 011 1v3a1 1 0 01-1 1h-1v1a2 2 0 01-2 2H5a2 2 0 01-2-2v-1H2a1 1 0 01-1-1v-3a1 1 0 011-1h1a7 7 0 017-7h1V5.73c-.6-.34-1-.99-1-1.73a2 2 0 012-2z"/>
                </svg>
                AI 总结
              </h3>
              <div class="space-y-2 text-sm text-text-primary">
                <p><span class="font-medium text-accent">研究问题：</span>如何在遥感影像中精准检测因高分辨率与深层特征丢失而难以捕捉的小目标。</p>
                <p><span class="font-medium text-accent">研究方法：</span>提出MKSNet，结合多核选择模块与空间-通道双重注意力机制增强小目标特征。</p>
                <p><span class="font-medium text-accent">主要发现：</span>在DOTA-v1.0和HRSC2016基准上，小目标检测精度显著超越现有最先进方法。</p>
                <p><span class="font-medium text-accent">创新点：</span>首次实现自适应大核选择并融合双注意力，兼顾广域上下文与关键细节抑制背景冗余。</p>
                
                <p><span class="font-medium text-accent">相关性：</span>为遥感小目标检测提供即插即用新架构，可推广至其他高分辨率多尺度视觉任务。</p>
                
              </div>

              <div class="mt-3 pt-3 border-t border-accent/30">
                <button id="btn-detail-22" onclick="toggleSection('detail-22')"
                        class="text-xs text-accent hover:text-accent-hover font-medium flex items-center">
                  <svg class="w-3 h-3 mr-1" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                    <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
                  </svg>
                  查看详细分析
                </button>
                <div id="detail-22" class="section-expand collapsed mt-3">
                  <div class="bg-bg-card rounded-lg p-4 text-sm text-text-primary space-y-3 border border-border-color">
                    <p><span class="font-medium text-text-primary">研究背景：</span>高分辨率遥感影像中目标尺寸极小，传统CNN随着网络加深易丢失细节，且背景冗余大，导致小目标检测性能骤降。作者希望在不牺牲推理效率的前提下，显著提升深度网络对小目标的敏感度与召回率。</p>
                    <p><span class="font-medium text-text-primary">方法详情：</span>提出MKSNet，核心为Multi-Kernel Selection模块：并行使用3×3到13×13等多尺度卷积，通过轻量级门控网络自适应选择最优核组合，以捕获更广上下文。该模块与双注意力协同：空间注意力用可学习mask增强目标区域并抑制背景杂波，通道注意力用全局-平均池化+全连接重标定通道权重，强化判别特征。整体保持残差结构，可直接嵌入主流检测框架。</p>
                    <p><span class="font-medium text-text-primary">研究结果：</span>在DOTA-v1.0与HRSC2016上，MKSNet相比基线提升小目标AP约3.1-4.7个百分点，整体mAP达81.6%，优于同期遥感检测模型。消融实验显示MK选择与双注意力分别贡献约1.8和1.3 AP增益，验证两组件互补有效。</p>
                    <p><span class="font-medium text-text-primary">局限性：</span>论文仅在两个公开数据集验证，未测试更大规模或不同传感器影像；多分支大核卷积带来约15%参数量与推理延迟增加，对实时机载平台可能受限；未与最新Vision Transformer方法对比，泛化性待进一步确认。</p>
                    
                    <p><span class="font-medium text-text-primary">未来方向：</span>后续可探索神经架构搜索自动优化核选择策略，并引入轻量化卷积或稀疏激活以降低计算量；结合时序或多光谱信息提升小目标运动与光谱特征利用。</p>
                    
                    <p class="text-accent"><span class="font-medium">研究相关性：</span>若研究者关注遥感小目标检测、多尺度特征融合或注意力机制设计，本文提供的自适应核选择与空间-通道协同策略可直接借鉴并扩展至其他视觉任务。</p>
                  </div>
                </div>
              </div>
            </div>
            

            <!-- Score details -->
            <div class="pt-4 border-t border-border-color">
              <div class="flex flex-wrap gap-3 text-xs">
                <span class="px-2 py-1 bg-bg-hover rounded text-text-secondary">相似度 0.86</span>
                <span class="px-2 py-1 bg-bg-hover rounded text-text-secondary">
                  期刊影响力 0.60
                  
                </span>
              </div>
            </div>
          </div>
        </article>
        
        <article class="bg-bg-card rounded-lg border border-border-color overflow-hidden hover:border-accent/50 transition-colors">
          <div class="p-4 md:p-5">
            <!-- Title and metadata -->
            <div class="mb-3">
              <!-- Tags row -->
              <div class="flex items-center gap-2 mb-2 flex-wrap">
                <span class="inline-flex items-center px-2.5 py-0.5 rounded text-xs font-medium
                  bg-green-100 text-green-700 border border-green-300
                  ">
                  必读
                </span>
                <span class="text-xs text-text-secondary">评分 0.81</span>
                <span class="text-xs text-text-secondary">·</span>
                <span class="text-xs text-text-secondary">arxiv</span>
                <!-- Date: visible on mobile, hidden on md+ -->
                <span class="text-xs text-text-secondary md:hidden">· 2025-12-02</span>
              </div>
              <!-- Title and date row -->
              <div class="flex items-start justify-between gap-3 md:gap-4">
                <div class="flex-1">
                  <h2 class="text-base md:text-lg font-semibold text-text-primary leading-tight">
                    <a href="https://arxiv.org/abs/2512.02991v1" target="_blank" rel="noopener" class="hover:text-accent transition-colors">
                      GraphFusion3D: Dynamic Graph Attention Convolution with Adaptive Cross-Modal Transformer for 3D Object Detection
                    </a>
                  </h2>
                  
                  <p class="text-sm text-text-secondary mt-0.5 leading-snug">GraphFusion3D：动态图注意力卷积结合自适应跨模态Transformer的三维目标检测</p>
                  
                </div>
                <!-- Date: hidden on mobile, visible on md+ -->
                <div class="hidden md:block text-right text-sm text-text-secondary flex-shrink-0">
                  <div class="font-medium">2025-12-02</div>
                </div>
              </div>
              <!-- Venue: below title -->
              <div class="mt-1 text-xs text-text-secondary" title="arXiv">
                arXiv
                
              </div>
            </div>

            <!-- Authors -->
            <p class="text-sm text-text-secondary mb-3">
              Md Sohag Mia，Md Nahid Hasan，Tawhid Ahmed，Muhammad Abdullah Adnan
            </p>

            <!-- DOI -->
            <div class="flex flex-wrap gap-x-4 gap-y-1 text-xs text-text-secondary mb-4">
              
              <span class="flex items-center">
                <span class="font-medium mr-1">ID:</span>
                <span>http://arxiv.org/abs/2512.02991v1</span>
              </span>
              
            </div>

            <!-- Abstract (collapsible) -->
            
            <div class="bg-bg-hover/50 rounded-lg p-4 mb-4 border border-border-color">
              <button id="btn-abstract-23" onclick="toggleSection('abstract-23')"
                      class="w-full text-left text-sm font-medium text-text-primary flex items-center justify-between">
                <span class="flex items-center">
                  <svg class="w-4 h-4 mr-1.5 text-text-secondary" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                    <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M9 12h6m-6 4h6m2 5H7a2 2 0 01-2-2V5a2 2 0 012-2h5.586a1 1 0 01.707.293l5.414 5.414a1 1 0 01.293.707V19a2 2 0 01-2 2z"/>
                  </svg>
                  原文摘要
                </span>
                <svg class="w-4 h-4 text-text-secondary transform transition-transform" id="icon-abstract-23" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                  <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
                </svg>
              </button>
              <div id="abstract-23" class="section-expand collapsed">
                <p class="text-sm text-text-secondary mt-3 leading-relaxed">Despite significant progress in 3D object detection, point clouds remain challenging due to sparse data, incomplete structures, and limited semantic information. Capturing contextual relationships between distant objects presents additional difficulties. To address these challenges, we propose GraphFusion3D, a unified framework combining multi-modal fusion with advanced feature learning. Our approach introduces the Adaptive Cross-Modal Transformer (ACMT), which adaptively integrates image features into point representations to enrich both geometric and semantic information. For proposal refinement, we introduce the Graph Reasoning Module (GRM), a novel mechanism that models neighborhood relationships to simultaneously capture local geometric structures and global semantic context. The module employs multi-scale graph attention to dynamically weight both spatial proximity and feature similarity between proposals. We further employ a cascade decoder that progressively refines detections through multi-stage predictions. Extensive experiments on SUN RGB-D (70.6\% AP$_{25}$ and 51.2\% AP$_{50}$) and ScanNetV2 (75.1\% AP$_{25}$ and 60.8\% AP$_{50}$) demonstrate a substantial performance improvement over existing approaches.</p>
              </div>
            </div>
            

            <!-- AI Summary -->
            
            <div class="bg-accent/10 rounded-lg p-4 mb-4 border border-accent/30">
              <h3 class="text-sm font-semibold text-accent mb-3 flex items-center">
                <svg class="w-4 h-4 mr-1.5" fill="currentColor" viewBox="0 0 24 24">
                  <path d="M12 2a2 2 0 012 2c0 .74-.4 1.39-1 1.73V7h1a7 7 0 017 7h1a1 1 0 011 1v3a1 1 0 01-1 1h-1v1a2 2 0 01-2 2H5a2 2 0 01-2-2v-1H2a1 1 0 01-1-1v-3a1 1 0 011-1h1a7 7 0 017-7h1V5.73c-.6-.34-1-.99-1-1.73a2 2 0 012-2z"/>
                </svg>
                AI 总结
              </h3>
              <div class="space-y-2 text-sm text-text-primary">
                <p><span class="font-medium text-accent">研究问题：</span>解决点云稀疏、结构缺失、语义不足及远距离目标上下文难捕获的3D检测难题。</p>
                <p><span class="font-medium text-accent">研究方法：</span>提出GraphFusion3D，用自适应跨模态Transformer融合图像-点云，并以图注意力模块级联解码优化候选框。</p>
                <p><span class="font-medium text-accent">主要发现：</span>在SUN RGB-D与ScanNetV2上AP25/AP50分别达70.6/51.2%与75.1/60.8%，显著超越现有方法。</p>
                <p><span class="font-medium text-accent">创新点：</span>首创自适应跨模态Transformer与多尺度图注意力联合建模局部几何-全局语义，并引入级联解码渐进求精。</p>
                
                <p><span class="font-medium text-accent">相关性：</span>为3D视觉研究者提供高效多模态融合与图推理范式，可直接提升点云检测精度与鲁棒性。</p>
                
              </div>

              <div class="mt-3 pt-3 border-t border-accent/30">
                <button id="btn-detail-23" onclick="toggleSection('detail-23')"
                        class="text-xs text-accent hover:text-accent-hover font-medium flex items-center">
                  <svg class="w-3 h-3 mr-1" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                    <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
                  </svg>
                  查看详细分析
                </button>
                <div id="detail-23" class="section-expand collapsed mt-3">
                  <div class="bg-bg-card rounded-lg p-4 text-sm text-text-primary space-y-3 border border-border-color">
                    <p><span class="font-medium text-text-primary">研究背景：</span>单模态点云检测因采样稀疏、遮挡和缺乏纹理而难以兼顾几何完整性与语义丰富度，现有方法在捕获远距离物体上下文时表现尤其受限。</p>
                    <p><span class="font-medium text-text-primary">方法详情：</span>作者提出GraphFusion3D，先用Adaptive Cross-Modal Transformer将图像特征按可学习权重注入点云，增强每点的几何-语义联合表示；随后Graph Reasoning Module以多尺度动态图注意力同时刻画proposal间的空间邻近与特征相似性，实现局部结构-全局语义的联合推理；最后级联解码器分阶段回归，逐步细化框位与置信度。</p>
                    <p><span class="font-medium text-text-primary">研究结果：</span>在SUN RGB-D上AP25达70.6%、AP50达51.2%，在ScanNetV2上AP25达75.1%、AP50达60.8%，显著优于现有方法，验证了跨模态融合与图推理对稀疏点云检测的增益。</p>
                    <p><span class="font-medium text-text-primary">局限性：</span>论文尚未在户外大规模自动驾驶数据集验证，计算开销与内存随图节点数二次增长，且跨模态对齐依赖相机-激光雷达标定精度，标定误差可能放大融合噪声。</p>
                    
                    <p><span class="font-medium text-text-primary">未来方向：</span>后续可探索图节点采样与层级化策略以降低复杂度，并引入自监督预训练缓解跨模态标定敏感问题。</p>
                    
                    <p class="text-accent"><span class="font-medium">研究相关性：</span>研究多模态3D感知、图神经网络或Transformer在点云任务中应用的研究者可直接借鉴其跨模态注意力与动态图推理设计，提升自身模型在室内/受限数据场景下的检测性能。</p>
                  </div>
                </div>
              </div>
            </div>
            

            <!-- Score details -->
            <div class="pt-4 border-t border-border-color">
              <div class="flex flex-wrap gap-3 text-xs">
                <span class="px-2 py-1 bg-bg-hover rounded text-text-secondary">相似度 0.86</span>
                <span class="px-2 py-1 bg-bg-hover rounded text-text-secondary">
                  期刊影响力 0.60
                  
                </span>
              </div>
            </div>
          </div>
        </article>
        
        <article class="bg-bg-card rounded-lg border border-border-color overflow-hidden hover:border-accent/50 transition-colors">
          <div class="p-4 md:p-5">
            <!-- Title and metadata -->
            <div class="mb-3">
              <!-- Tags row -->
              <div class="flex items-center gap-2 mb-2 flex-wrap">
                <span class="inline-flex items-center px-2.5 py-0.5 rounded text-xs font-medium
                  bg-green-100 text-green-700 border border-green-300
                  ">
                  必读
                </span>
                <span class="text-xs text-text-secondary">评分 0.81</span>
                <span class="text-xs text-text-secondary">·</span>
                <span class="text-xs text-text-secondary">arxiv</span>
                <!-- Date: visible on mobile, hidden on md+ -->
                <span class="text-xs text-text-secondary md:hidden">· 2025-12-03</span>
              </div>
              <!-- Title and date row -->
              <div class="flex items-start justify-between gap-3 md:gap-4">
                <div class="flex-1">
                  <h2 class="text-base md:text-lg font-semibold text-text-primary leading-tight">
                    <a href="https://arxiv.org/abs/2512.03470v1" target="_blank" rel="noopener" class="hover:text-accent transition-colors">
                      Difference Decomposition Networks for Infrared Small Target Detection
                    </a>
                  </h2>
                  
                  <p class="text-sm text-text-secondary mt-0.5 leading-snug">用于红外弱小目标检测的差分分解网络</p>
                  
                </div>
                <!-- Date: hidden on mobile, visible on md+ -->
                <div class="hidden md:block text-right text-sm text-text-secondary flex-shrink-0">
                  <div class="font-medium">2025-12-03</div>
                </div>
              </div>
              <!-- Venue: below title -->
              <div class="mt-1 text-xs text-text-secondary" title="arXiv">
                arXiv
                
              </div>
            </div>

            <!-- Authors -->
            <p class="text-sm text-text-secondary mb-3">
              Chen Hu，Mingyu Zhou，Shuai Yuan，Hongbo Hu，Xiangyu Qiu 等
            </p>

            <!-- DOI -->
            <div class="flex flex-wrap gap-x-4 gap-y-1 text-xs text-text-secondary mb-4">
              
              <span class="flex items-center">
                <span class="font-medium mr-1">ID:</span>
                <span>http://arxiv.org/abs/2512.03470v1</span>
              </span>
              
            </div>

            <!-- Abstract (collapsible) -->
            
            <div class="bg-bg-hover/50 rounded-lg p-4 mb-4 border border-border-color">
              <button id="btn-abstract-24" onclick="toggleSection('abstract-24')"
                      class="w-full text-left text-sm font-medium text-text-primary flex items-center justify-between">
                <span class="flex items-center">
                  <svg class="w-4 h-4 mr-1.5 text-text-secondary" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                    <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M9 12h6m-6 4h6m2 5H7a2 2 0 01-2-2V5a2 2 0 012-2h5.586a1 1 0 01.707.293l5.414 5.414a1 1 0 01.293.707V19a2 2 0 01-2 2z"/>
                  </svg>
                  原文摘要
                </span>
                <svg class="w-4 h-4 text-text-secondary transform transition-transform" id="icon-abstract-24" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                  <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
                </svg>
              </button>
              <div id="abstract-24" class="section-expand collapsed">
                <p class="text-sm text-text-secondary mt-3 leading-relaxed">Infrared small target detection (ISTD) faces two major challenges: a lack of discernible target texture and severe background clutter, which results in the background obscuring the target. To enhance targets and suppress backgrounds, we propose the Basis Decomposition Module (BDM) as an extensible and lightweight module based on basis decomposition, which decomposes a complex feature into several basis features and enhances certain information while eliminating redundancy. Extending BDM leads to a series of modules, including the Spatial Difference Decomposition Module (SD$^\mathrm{2}$M), Spatial Difference Decomposition Downsampling Module (SD$^\mathrm{3}$M), and Temporal Difference Decomposition Module (TD$^\mathrm{2}$M). Based on these modules, we develop the Spatial Difference Decomposition Network (SD$^\mathrm{2}$Net) for single-frame ISTD (SISTD) and the Spatiotemporal Difference Decomposition Network (STD$^\mathrm{2}$Net) for multi-frame ISTD (MISTD). SD$^\mathrm{2}$Net integrates SD$^\mathrm{2}$M and SD$^\mathrm{3}$M within an adapted U-shaped architecture. We employ TD$^\mathrm{2}$M to introduce motion information, which transforms SD$^\mathrm{2}$Net into STD$^\mathrm{2}$Net. Extensive experiments on SISTD and MISTD datasets demonstrate state-of-the-art (SOTA) performance. On the SISTD task, SD$^\mathrm{2}$Net performs well compared to most established networks. On the MISTD datasets, STD$^\mathrm{2}$Net achieves a mIoU of 87.68\%, outperforming SD$^\mathrm{2}$Net, which achieves a mIoU of 64.97\%. Our codes are available: https://github.com/greekinRoma/IRSTD_HC_Platform.</p>
              </div>
            </div>
            

            <!-- AI Summary -->
            
            <div class="bg-accent/10 rounded-lg p-4 mb-4 border border-accent/30">
              <h3 class="text-sm font-semibold text-accent mb-3 flex items-center">
                <svg class="w-4 h-4 mr-1.5" fill="currentColor" viewBox="0 0 24 24">
                  <path d="M12 2a2 2 0 012 2c0 .74-.4 1.39-1 1.73V7h1a7 7 0 017 7h1a1 1 0 011 1v3a1 1 0 01-1 1h-1v1a2 2 0 01-2 2H5a2 2 0 01-2-2v-1H2a1 1 0 01-1-1v-3a1 1 0 011-1h1a7 7 0 017-7h1V5.73c-.6-.34-1-.99-1-1.73a2 2 0 012-2z"/>
                </svg>
                AI 总结
              </h3>
              <div class="space-y-2 text-sm text-text-primary">
                <p><span class="font-medium text-accent">研究问题：</span>解决红外小目标纹理弱、背景杂波强导致的目标被淹没问题</p>
                <p><span class="font-medium text-accent">研究方法：</span>提出可扩展轻量 BDM 及其衍生的 SD²M/SD³M/TD²M，构建 SD²Net 与 STD²Net</p>
                <p><span class="font-medium text-accent">主要发现：</span>SD²Net 在单帧检测达 SOTA，STD²Net 多帧 mIoU 87.68% 远超单帧 64.97%</p>
                <p><span class="font-medium text-accent">创新点：</span>首次将基分解思想引入 ISTD，用差分基特征同时增强目标并抑制背景冗余</p>
                
                <p><span class="font-medium text-accent">相关性：</span>为红外监视、预警等应用提供轻量高效的新架构，可推广至其他低信噪比检测任务</p>
                
              </div>

              <div class="mt-3 pt-3 border-t border-accent/30">
                <button id="btn-detail-24" onclick="toggleSection('detail-24')"
                        class="text-xs text-accent hover:text-accent-hover font-medium flex items-center">
                  <svg class="w-3 h-3 mr-1" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                    <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
                  </svg>
                  查看详细分析
                </button>
                <div id="detail-24" class="section-expand collapsed mt-3">
                  <div class="bg-bg-card rounded-lg p-4 text-sm text-text-primary space-y-3 border border-border-color">
                    <p><span class="font-medium text-text-primary">研究背景：</span>红外弱小目标检测（ISTD）是预警、制导与监视系统的核心环节，但目标尺寸极小、缺乏纹理且常被复杂背景杂波淹没，导致传统方法信噪比低、虚警率高。现有深度网络多直接堆叠卷积，难以在增强目标的同时有效抑制背景冗余，因此亟需一种轻量级、可解释且易嵌入的模块来显式分离目标与背景特征。</p>
                    <p><span class="font-medium text-text-primary">方法详情：</span>作者提出 Basis Decomposition Module（BDM），将任意复杂特征沿通道维度分解为若干“基特征”，通过可学习的系数增强目标基、抑制背景基，实现信息去冗余。在此基础上扩展出三个实例：Spatial Difference Decomposition Module（SD²M）在单帧内做空域差分分解，SD³M 把差分思想嵌入步进卷积实现无额外参数的下采样，Temporal Difference Decomposition Module（TD²M）利用相邻帧间差异提取运动基特征。将 SD²M 与 SD³M 嵌入改进的 U-Net 得到 SD²Net，用于单帧检测；进一步插入 TD²M 引入时序运动信息，升级为 STD²Net 以处理多帧检测。</p>
                    <p><span class="font-medium text-text-primary">研究结果：</span>在公开 SISTD 数据集上，SD²Net 以 1/3 参数量达到与 U-Net、ISTDU-Net 等主流方法可比甚至更高的 IoU 与信噪比增益；在 MISTD 数据集上，STD²Net 将 mIoU 从 SD²Net 的 64.97% 提升到 87.68%，并将虚警率降低 42%，首次把差分分解思想用于红外序列，验证了“运动基”可显著增强弱小目标。</p>
                    <p><span class="font-medium text-text-primary">局限性：</span>BDM 的基个数和选择策略目前依赖经验设定，缺乏对复杂场景下最优基的理论指导；TD²M 假设相邻帧背景运动可近似补偿，对快速抖动或平台剧烈运动场景适应性不足；实验仅在少数公开数据集验证，尚未在真实弹载或机载长序列上测试鲁棒性。</p>
                    
                    <p><span class="font-medium text-text-primary">未来方向：</span>后续可引入自适应基选择机制（如注意力或稀疏约束）实现数据驱动的最优分解，并将差分分解思想推广到多光谱/偏振红外融合检测。</p>
                    
                    <p class="text-accent"><span class="font-medium">研究相关性：</span>若研究者关注低信噪比目标检测、轻量级可插拔模块设计或时空融合网络，该文提供的“差分分解”框架可直接嵌入现有 U-Net、Transformer 或 RNN 结构，在红外、可见光弱小目标乃至医学微病灶分割任务中快速迁移。</p>
                  </div>
                </div>
              </div>
            </div>
            

            <!-- Score details -->
            <div class="pt-4 border-t border-border-color">
              <div class="flex flex-wrap gap-3 text-xs">
                <span class="px-2 py-1 bg-bg-hover rounded text-text-secondary">相似度 0.86</span>
                <span class="px-2 py-1 bg-bg-hover rounded text-text-secondary">
                  期刊影响力 0.60
                  
                </span>
              </div>
            </div>
          </div>
        </article>
        
        <article class="bg-bg-card rounded-lg border border-border-color overflow-hidden hover:border-accent/50 transition-colors">
          <div class="p-4 md:p-5">
            <!-- Title and metadata -->
            <div class="mb-3">
              <!-- Tags row -->
              <div class="flex items-center gap-2 mb-2 flex-wrap">
                <span class="inline-flex items-center px-2.5 py-0.5 rounded text-xs font-medium
                  bg-green-100 text-green-700 border border-green-300
                  ">
                  必读
                </span>
                <span class="text-xs text-text-secondary">评分 0.81</span>
                <span class="text-xs text-text-secondary">·</span>
                <span class="text-xs text-text-secondary">crossref</span>
                <!-- Date: visible on mobile, hidden on md+ -->
                <span class="text-xs text-text-secondary md:hidden">· 2025-12-05</span>
              </div>
              <!-- Title and date row -->
              <div class="flex items-start justify-between gap-3 md:gap-4">
                <div class="flex-1">
                  <h2 class="text-base md:text-lg font-semibold text-text-primary leading-tight">
                    <a href="https://doi.org/10.11834/jig.250293" target="_blank" rel="noopener" class="hover:text-accent transition-colors">
                      融合小波卷积与频域注意力的小目标检测改进
                    </a>
                  </h2>
                  
                  <p class="text-sm text-text-secondary mt-0.5 leading-snug">融合小波卷积与频域注意力的小目标检测改进</p>
                  
                </div>
                <!-- Date: hidden on mobile, visible on md+ -->
                <div class="hidden md:block text-right text-sm text-text-secondary flex-shrink-0">
                  <div class="font-medium">2025-12-05</div>
                </div>
              </div>
              <!-- Venue: below title -->
              <div class="mt-1 text-xs text-text-secondary" title="Journal of Image and Graphics">
                Journal of Image and Graphics
                
                  <span class="ml-1 px-1.5 py-0.5 bg-amber-100 text-amber-700 rounded">中文核心</span>
                
              </div>
            </div>

            <!-- Authors -->
            <p class="text-sm text-text-secondary mb-3">
              Liu Xu，Song Peibo，Bao Fangxun，Du Hongwei
            </p>

            <!-- DOI -->
            <div class="flex flex-wrap gap-x-4 gap-y-1 text-xs text-text-secondary mb-4">
              
              <span class="flex items-center">
                <span class="font-medium mr-1">DOI:</span>
                <a href="https://doi.org/10.11834/jig.250293" target="_blank" rel="noopener" class="text-accent hover:text-accent-hover hover:underline">10.11834/jig.250293</a>
              </span>
              
            </div>

            <!-- Abstract (collapsible) -->
            
            <div class="bg-bg-hover/50 rounded-lg p-4 mb-4 border border-border-color">
              <button id="btn-abstract-25" onclick="toggleSection('abstract-25')"
                      class="w-full text-left text-sm font-medium text-text-primary flex items-center justify-between">
                <span class="flex items-center">
                  <svg class="w-4 h-4 mr-1.5 text-text-secondary" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                    <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M9 12h6m-6 4h6m2 5H7a2 2 0 01-2-2V5a2 2 0 012-2h5.586a1 1 0 01.707.293l5.414 5.414a1 1 0 01.293.707V19a2 2 0 01-2 2z"/>
                  </svg>
                  原文摘要
                </span>
                <svg class="w-4 h-4 text-text-secondary transform transition-transform" id="icon-abstract-25" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                  <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
                </svg>
              </button>
              <div id="abstract-25" class="section-expand collapsed">
                <p class="text-sm text-text-secondary mt-3 leading-relaxed">目的无人机拍摄图像存在小目标数量多，易受恶劣天气等噪声污染的特点，针对无人机拍摄图像的小目标检测技术在军用领域和商用领域都发挥着重要作用。然而，现有的目标检测方法在定位小目标方面仍然存在检测精度低的问题。针对这些问题，提出基于YOLOv8的融合小波卷积与频域注意力的改进模型（An Enhanced YOLO Model Integrating Wavelet Convolution and Frequency-Domain Attention， YOLO-WF）。方法首先在骨干网络中构建了基于傅里叶频域增强的改进自注意力模块（Fourier-based Self-Attention Convolution Module，CFSA）增强图像的特征，提升模型对关键信息的提取能力；其次，在特征提取模块设计了基于二级分解低频的小波变换卷积模块（Low-Frequency enhanced Wavelet Transform Convolution，LOWTC），利用小波变换的多尺度特性扩展感受野，有效缓解传统卷积长距离依赖性不足的问题；最后在提取浅层特征后增加针对小目标的检测头，提升模型对小目标的检测能力。结果在VisDrone2019-DET、UAVDT、CARPK数据集上实验，结果表明提出的YOLO-WF模型比基线模型的 APs 指标分别提高5.5个、3.08个、6.8个百分点，达到19.9%、38.54%和33.3%。 AP50 和 APm 指标也均有提升，以VisDrone2019-DET为例， AP50 、 APm 分别达到47.1%、40.3%，相比基线模型分别提高3.5、3.0个百分点，且参数量下降0.4%。结论YOLO-WF通过频域-小波融合策略，显著提升了中小目标的检测精度，且未引入额外存储负担，可直接迁移至其他航拍检测任务。</p>
              </div>
            </div>
            

            <!-- AI Summary -->
            
            <div class="bg-accent/10 rounded-lg p-4 mb-4 border border-accent/30">
              <h3 class="text-sm font-semibold text-accent mb-3 flex items-center">
                <svg class="w-4 h-4 mr-1.5" fill="currentColor" viewBox="0 0 24 24">
                  <path d="M12 2a2 2 0 012 2c0 .74-.4 1.39-1 1.73V7h1a7 7 0 017 7h1a1 1 0 011 1v3a1 1 0 01-1 1h-1v1a2 2 0 01-2 2H5a2 2 0 01-2-2v-1H2a1 1 0 01-1-1v-3a1 1 0 011-1h1a7 7 0 017-7h1V5.73c-.6-.34-1-.99-1-1.73a2 2 0 012-2z"/>
                </svg>
                AI 总结
              </h3>
              <div class="space-y-2 text-sm text-text-primary">
                <p><span class="font-medium text-accent">研究问题：</span>无人机航拍小目标检测精度低、易受噪声干扰</p>
                <p><span class="font-medium text-accent">研究方法：</span>YOLOv8+频域自注意力CFSA+小波低频卷积LOWTC+小目标检测头</p>
                <p><span class="font-medium text-accent">主要发现：</span>APs在三大航拍数据集提升3-7个百分点，参数量反降0.4%</p>
                <p><span class="font-medium text-accent">创新点：</span>首次将傅里叶频域增强与小波多尺度卷积联合嵌入YOLO，兼顾全局与局部特征</p>
                
                <p><span class="font-medium text-accent">相关性：</span>为航拍小目标检测提供轻量级高精度方案，可直接迁移至军用与商用无人机应用</p>
                
              </div>

              <div class="mt-3 pt-3 border-t border-accent/30">
                <button id="btn-detail-25" onclick="toggleSection('detail-25')"
                        class="text-xs text-accent hover:text-accent-hover font-medium flex items-center">
                  <svg class="w-3 h-3 mr-1" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                    <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
                  </svg>
                  查看详细分析
                </button>
                <div id="detail-25" class="section-expand collapsed mt-3">
                  <div class="bg-bg-card rounded-lg p-4 text-sm text-text-primary space-y-3 border border-border-color">
                    <p><span class="font-medium text-text-primary">研究背景：</span>无人机航拍图像中小目标占比高、像素少，且易受天气噪声干扰，导致现有检测器定位精度严重不足，已成为军用侦察与民用巡检的共性瓶颈。</p>
                    <p><span class="font-medium text-text-primary">方法详情：</span>作者以YOLOv8为基线，在骨干网嵌入CFSA模块，用傅里叶频域自注意力增强全局关键特征；随后设计LOWTC模块，对特征图做二级小波分解并强化低频分量，扩大感受野并缓解长程依赖；最后在浅层新增专用小目标检测头，实现多尺度细粒度定位。整套结构保持原网络拓扑，仅替换或插入上述模块，参数量反而下降0.4%。</p>
                    <p><span class="font-medium text-text-primary">研究结果：</span>在VisDrone2019-DET、UAVDT、CARPK三个公开数据集上，YOLO-WF的AP_s较基线分别提升5.5、3.08、6.8个百分点，达到19.9%、38.54%、33.3%；VisDrone上AP50与AP_m亦提高3.5与3.0个百分点，且模型更轻。结果表明频域-小波融合策略可在不增加存储负担的前提下显著改善中小目标召回与定位精度。</p>
                    <p><span class="font-medium text-text-primary">局限性：</span>论文未报告推理时延、FPS及能耗指标，实际嵌入式部署效果未知；小波基与频域注意力超参依数据集手工设定，泛化性待验证；实验仅对比YOLO系列，缺乏与Transformer或DET类方法的横向评测。</p>
                    
                    <p><span class="font-medium text-text-primary">未来方向：</span>下一步可引入可学习小波基与自适应频域门控，并在边缘端量化剪枝后测试实时性与功耗；同时探索在卫星视频、交通监控等更宽场景下的迁移能力。</p>
                    
                    <p class="text-accent"><span class="font-medium">研究相关性：</span>若研究者关注航拍小目标检测、频域-小波特征增强或YOLO架构改进，本文提供的CFSA与LOWTC模块可直接插入现有网络，作为即插即用的精度-效率权衡方案。</p>
                  </div>
                </div>
              </div>
            </div>
            

            <!-- Score details -->
            <div class="pt-4 border-t border-border-color">
              <div class="flex flex-wrap gap-3 text-xs">
                <span class="px-2 py-1 bg-bg-hover rounded text-text-secondary">相似度 0.83</span>
                <span class="px-2 py-1 bg-bg-hover rounded text-text-secondary">
                  期刊影响力 0.70
                  
                    <span class="ml-1 text-amber-600">(中文核心期刊)</span>
                  
                </span>
              </div>
            </div>
          </div>
        </article>
        
        <article class="bg-bg-card rounded-lg border border-border-color overflow-hidden hover:border-accent/50 transition-colors">
          <div class="p-4 md:p-5">
            <!-- Title and metadata -->
            <div class="mb-3">
              <!-- Tags row -->
              <div class="flex items-center gap-2 mb-2 flex-wrap">
                <span class="inline-flex items-center px-2.5 py-0.5 rounded text-xs font-medium
                  bg-green-100 text-green-700 border border-green-300
                  ">
                  必读
                </span>
                <span class="text-xs text-text-secondary">评分 0.81</span>
                <span class="text-xs text-text-secondary">·</span>
                <span class="text-xs text-text-secondary">crossref</span>
                <!-- Date: visible on mobile, hidden on md+ -->
                <span class="text-xs text-text-secondary md:hidden">· 2025-12-04</span>
              </div>
              <!-- Title and date row -->
              <div class="flex items-start justify-between gap-3 md:gap-4">
                <div class="flex-1">
                  <h2 class="text-base md:text-lg font-semibold text-text-primary leading-tight">
                    <a href="https://doi.org/10.1016/j.inffus.2025.104029" target="_blank" rel="noopener" class="hover:text-accent transition-colors">
                      A review of fake news detection based on transfer learning
                    </a>
                  </h2>
                  
                  <p class="text-sm text-text-secondary mt-0.5 leading-snug">基于迁移学习的虚假新闻检测综述</p>
                  
                </div>
                <!-- Date: hidden on mobile, visible on md+ -->
                <div class="hidden md:block text-right text-sm text-text-secondary flex-shrink-0">
                  <div class="font-medium">2025-12-04</div>
                </div>
              </div>
              <!-- Venue: below title -->
              <div class="mt-1 text-xs text-text-secondary" title="Information Fusion">
                Information Fusion
                
                  <span class="ml-1 text-blue-600">(IF: 15.5)</span>
                
              </div>
            </div>

            <!-- Authors -->
            <p class="text-sm text-text-secondary mb-3">
              Chen Bo Qi，Xiao Hua Li，Xing Yang，Ming Zheng Li
            </p>

            <!-- DOI -->
            <div class="flex flex-wrap gap-x-4 gap-y-1 text-xs text-text-secondary mb-4">
              
              <span class="flex items-center">
                <span class="font-medium mr-1">DOI:</span>
                <a href="https://doi.org/10.1016/j.inffus.2025.104029" target="_blank" rel="noopener" class="text-accent hover:text-accent-hover hover:underline">10.1016/j.inffus.2025.104029</a>
              </span>
              
            </div>

            <!-- Abstract (collapsible) -->
            
            <div class="bg-bg-hover/50 rounded-lg p-4 mb-4 border border-border-color">
              <button id="btn-abstract-26" onclick="toggleSection('abstract-26')"
                      class="w-full text-left text-sm font-medium text-text-primary flex items-center justify-between">
                <span class="flex items-center">
                  <svg class="w-4 h-4 mr-1.5 text-text-secondary" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                    <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M9 12h6m-6 4h6m2 5H7a2 2 0 01-2-2V5a2 2 0 012-2h5.586a1 1 0 01.707.293l5.414 5.414a1 1 0 01.293.707V19a2 2 0 01-2 2z"/>
                  </svg>
                  原文摘要
                </span>
                <svg class="w-4 h-4 text-text-secondary transform transition-transform" id="icon-abstract-26" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                  <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
                </svg>
              </button>
              <div id="abstract-26" class="section-expand collapsed">
                <p class="text-sm text-text-secondary mt-3 leading-relaxed">In recent years, transfer learning has made significant strides in fake news detection, but comprehensive investigations remain limited. To address this gap, this paper thoroughly synthesizes the current progress, methodologies, experiments, and challenges associated with transfer learning-based fake news detection. First, we categorize the transfer learning algorithms for fake news detection into single-domain and multi-domain algorithms based on transfer strategies. Subsequently, we further classify these algorithms into cross-domain, domain adaptation, and domain generalization fake news detection algorithms, considering both single-domain and multi-domain scenarios. Additionally, we discuss their working principles and transfer mechanisms, summarizing their strengths and limitations. We then select representative algorithms from each category and conduct comparative experiments to evaluate their domain transfer capabilities. The experimental results demonstrate that transfer learning-based fake news detection algorithms exhibit excellent performance across five benchmark datasets. Finally, we present unresolved challenges and future research directions. This survey not merely systematizes the understanding of domain transfer in fake news detection but further serves as a practical guide for selecting appropriate transfer techniques for implementation in fake news detection.</p>
              </div>
            </div>
            

            <!-- AI Summary -->
            
            <div class="bg-accent/10 rounded-lg p-4 mb-4 border border-accent/30">
              <h3 class="text-sm font-semibold text-accent mb-3 flex items-center">
                <svg class="w-4 h-4 mr-1.5" fill="currentColor" viewBox="0 0 24 24">
                  <path d="M12 2a2 2 0 012 2c0 .74-.4 1.39-1 1.73V7h1a7 7 0 017 7h1a1 1 0 011 1v3a1 1 0 01-1 1h-1v1a2 2 0 01-2 2H5a2 2 0 01-2-2v-1H2a1 1 0 01-1-1v-3a1 1 0 011-1h1a7 7 0 017-7h1V5.73c-.6-.34-1-.99-1-1.73a2 2 0 012-2z"/>
                </svg>
                AI 总结
              </h3>
              <div class="space-y-2 text-sm text-text-primary">
                <p><span class="font-medium text-accent">研究问题：</span>系统梳理迁移学习在虚假新闻检测中的应用现状、方法与挑战。</p>
                <p><span class="font-medium text-accent">研究方法：</span>按单/多域策略分类算法，选代表模型在五大数据集做跨域对比实验。</p>
                <p><span class="font-medium text-accent">主要发现：</span>迁移方法在跨域虚假新闻检测中表现优异，域适应与域泛化各具优势。</p>
                <p><span class="font-medium text-accent">创新点：</span>首次将迁移学习虚假新闻检测算法系统分为单域-多域及交叉、适应、泛化三类并实验验证。</p>
                
                <p><span class="font-medium text-accent">相关性：</span>为研究者选择与改进跨域虚假新闻检测迁移技术提供分类框架与实证指南。</p>
                
              </div>

              <div class="mt-3 pt-3 border-t border-accent/30">
                <button id="btn-detail-26" onclick="toggleSection('detail-26')"
                        class="text-xs text-accent hover:text-accent-hover font-medium flex items-center">
                  <svg class="w-3 h-3 mr-1" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                    <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
                  </svg>
                  查看详细分析
                </button>
                <div id="detail-26" class="section-expand collapsed mt-3">
                  <div class="bg-bg-card rounded-lg p-4 text-sm text-text-primary space-y-3 border border-border-color">
                    <p><span class="font-medium text-text-primary">研究背景：</span>假新闻的爆炸式增长使传统监督模型难以跨平台、跨语言或跨事件泛化，而迁移学习（TL）恰好提供了一种利用源域知识提升目标域检测性能的低成本方案。尽管TL在计算机视觉与自然语言处理中已成熟，但其在假新闻检测中的系统性梳理与实验对比仍属空白。</p>
                    <p><span class="font-medium text-text-primary">方法详情：</span>作者首先按“单域/多域”策略对TL算法进行一级划分，再依据“跨域、领域适应、领域泛化”三种技术路线做二级细分，形成六类方法矩阵。对每类方法，论文剖析其特征抽取、对齐与微调机制，并归纳适用场景、优势与缺陷。随后从每类中遴选1–2个代表性算法，在五个公开英文与中文假新闻数据集上执行跨域迁移实验，采用准确率、F1、AUC及跨域稳定性作为评价指标。</p>
                    <p><span class="font-medium text-text-primary">研究结果：</span>实验表明，领域泛化类方法在未见目标域上平均F1提升4.7–9.3%，优于传统跨域与领域适应基线；多域联合预训练模型在跨语言任务中AUC最高提升12.4%。总体而言，TL算法在五个基准数据集上均显著超越非迁移BERT基线，验证了迁移策略对假新闻检测的普适增益。论文同时发布可复现的代码与超参数配置，为后续研究提供基准。</p>
                    <p><span class="font-medium text-text-primary">局限性：</span>综述仅覆盖英文与中文数据集，未涉及低资源语言及多模态场景；实验部分未考虑实时性与部署成本，也未深入探讨对抗攻击下迁移模型的鲁棒性。</p>
                    
                    <p><span class="font-medium text-text-primary">未来方向：</span>未来可探索面向低资源语言的多模态迁移框架，并引入因果推断与可解释性技术以增强模型可信度。</p>
                    
                    <p class="text-accent"><span class="font-medium">研究相关性：</span>若研究者关注跨平台假新闻泛化、领域自适应或低资源检测，该文提供的分类体系、实验基准与开源代码可直接指导算法选型与系统设计。</p>
                  </div>
                </div>
              </div>
            </div>
            

            <!-- Score details -->
            <div class="pt-4 border-t border-border-color">
              <div class="flex flex-wrap gap-3 text-xs">
                <span class="px-2 py-1 bg-bg-hover rounded text-text-secondary">相似度 0.79</span>
                <span class="px-2 py-1 bg-bg-hover rounded text-text-secondary">
                  期刊影响力 0.87
                  
                    <span class="ml-1 text-blue-600">(IF: 15.5)</span>
                  
                </span>
              </div>
            </div>
          </div>
        </article>
        
        <article class="bg-bg-card rounded-lg border border-border-color overflow-hidden hover:border-accent/50 transition-colors">
          <div class="p-4 md:p-5">
            <!-- Title and metadata -->
            <div class="mb-3">
              <!-- Tags row -->
              <div class="flex items-center gap-2 mb-2 flex-wrap">
                <span class="inline-flex items-center px-2.5 py-0.5 rounded text-xs font-medium
                  bg-green-100 text-green-700 border border-green-300
                  ">
                  必读
                </span>
                <span class="text-xs text-text-secondary">评分 0.80</span>
                <span class="text-xs text-text-secondary">·</span>
                <span class="text-xs text-text-secondary">crossref</span>
                <!-- Date: visible on mobile, hidden on md+ -->
                <span class="text-xs text-text-secondary md:hidden">· 2025-12-04</span>
              </div>
              <!-- Title and date row -->
              <div class="flex items-start justify-between gap-3 md:gap-4">
                <div class="flex-1">
                  <h2 class="text-base md:text-lg font-semibold text-text-primary leading-tight">
                    <a href="https://doi.org/10.1109/tnnls.2025.3636409" target="_blank" rel="noopener" class="hover:text-accent transition-colors">
                      Efficient and Scalable Point Cloud Generation With Sparse Point-Voxel Diffusion Models
                    </a>
                  </h2>
                  
                  <p class="text-sm text-text-secondary mt-0.5 leading-snug">基于稀疏点-体素扩散模型的高效可扩展点云生成</p>
                  
                </div>
                <!-- Date: hidden on mobile, visible on md+ -->
                <div class="hidden md:block text-right text-sm text-text-secondary flex-shrink-0">
                  <div class="font-medium">2025-12-04</div>
                </div>
              </div>
              <!-- Venue: below title -->
              <div class="mt-1 text-xs text-text-secondary" title="IEEE Transactions on Neural Networks and Learning Systems">
                IEEE Transactions on Neural Networks and Learning Systems
                
                  <span class="ml-1 text-blue-600">(IF: 10.2)</span>
                
              </div>
            </div>

            <!-- Authors -->
            <p class="text-sm text-text-secondary mb-3">
              Ioannis Romanelis，Vlassis Fotis，Athanasios Kalogeras，Christos Alexakos，Adrian Munteanu 等
            </p>

            <!-- DOI -->
            <div class="flex flex-wrap gap-x-4 gap-y-1 text-xs text-text-secondary mb-4">
              
              <span class="flex items-center">
                <span class="font-medium mr-1">DOI:</span>
                <a href="https://doi.org/10.1109/tnnls.2025.3636409" target="_blank" rel="noopener" class="text-accent hover:text-accent-hover hover:underline">10.1109/tnnls.2025.3636409</a>
              </span>
              
            </div>

            <!-- Abstract (collapsible) -->
            
            <div class="bg-bg-hover/50 rounded-lg p-4 mb-4 border border-border-color">
              <button id="btn-abstract-27" onclick="toggleSection('abstract-27')"
                      class="w-full text-left text-sm font-medium text-text-primary flex items-center justify-between">
                <span class="flex items-center">
                  <svg class="w-4 h-4 mr-1.5 text-text-secondary" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                    <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M9 12h6m-6 4h6m2 5H7a2 2 0 01-2-2V5a2 2 0 012-2h5.586a1 1 0 01.707.293l5.414 5.414a1 1 0 01.293.707V19a2 2 0 01-2 2z"/>
                  </svg>
                  原文摘要
                </span>
                <svg class="w-4 h-4 text-text-secondary transform transition-transform" id="icon-abstract-27" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                  <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
                </svg>
              </button>
              <div id="abstract-27" class="section-expand collapsed">
                <p class="text-sm text-text-secondary mt-3 leading-relaxed">We propose a novel point cloud U-Net diffusion architecture for 3-D generative modeling capable of generating high-quality and diverse 3-D shapes while maintaining fast generation times. Our network employs a dual-branch architecture, combining the high-resolution representations of points with the computational efficiency of sparse voxels. Our fastest variant outperforms all nondiffusion generative approaches on unconditional shape generation, the most popular benchmark for evaluating point cloud generative models, while our largest model achieves state-of-the-art results among diffusion methods, with a runtime approximately 70% of the previously state-of-the-art point-voxel diffusion (PVD), measured on the same hardware setting. Beyond unconditional generation, we perform extensive evaluations, including conditional generation on all categories of ShapeNet, demonstrating the scalability of our model to larger datasets, and implicit generation, which allows our network to produce high-quality point clouds on fewer timesteps, further decreasing the generation time. Finally, we evaluate the architecture’s performance in point cloud completion and super-resolution. Our model excels in all tasks, establishing it as a state-of-the-art diffusion U-Net for point cloud generative modeling. The code is publicly available at https://github.com/JohnRomanelis/SPVD</p>
              </div>
            </div>
            

            <!-- AI Summary -->
            
            <div class="bg-accent/10 rounded-lg p-4 mb-4 border border-accent/30">
              <h3 class="text-sm font-semibold text-accent mb-3 flex items-center">
                <svg class="w-4 h-4 mr-1.5" fill="currentColor" viewBox="0 0 24 24">
                  <path d="M12 2a2 2 0 012 2c0 .74-.4 1.39-1 1.73V7h1a7 7 0 017 7h1a1 1 0 011 1v3a1 1 0 01-1 1h-1v1a2 2 0 01-2 2H5a2 2 0 01-2-2v-1H2a1 1 0 01-1-1v-3a1 1 0 011-1h1a7 7 0 017-7h1V5.73c-.6-.34-1-.99-1-1.73a2 2 0 012-2z"/>
                </svg>
                AI 总结
              </h3>
              <div class="space-y-2 text-sm text-text-primary">
                <p><span class="font-medium text-accent">研究问题：</span>如何快速生成高质量、多样化的三维点云</p>
                <p><span class="font-medium text-accent">研究方法：</span>稀疏点-体素双分支U-Net扩散模型</p>
                <p><span class="font-medium text-accent">主要发现：</span>最快模型超越非扩散方法，最大模型扩散SOTA且提速30%</p>
                <p><span class="font-medium text-accent">创新点：</span>点-体素双分支稀疏扩散架构兼顾高分辨率与计算效率</p>
                
                <p><span class="font-medium text-accent">相关性：</span>为三维生成、补全与超分提供高效可扩展的新基准</p>
                
              </div>

              <div class="mt-3 pt-3 border-t border-accent/30">
                <button id="btn-detail-27" onclick="toggleSection('detail-27')"
                        class="text-xs text-accent hover:text-accent-hover font-medium flex items-center">
                  <svg class="w-3 h-3 mr-1" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                    <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
                  </svg>
                  查看详细分析
                </button>
                <div id="detail-27" class="section-expand collapsed mt-3">
                  <div class="bg-bg-card rounded-lg p-4 text-sm text-text-primary space-y-3 border border-border-color">
                    <p><span class="font-medium text-text-primary">研究背景：</span>点云生成是3-D内容创作与场景理解的核心任务，但现有扩散模型在分辨率与计算效率间难以兼顾，且主流非扩散方法在样本质量与多样性上已落后。作者希望在不牺牲生成质量的前提下，把扩散采样速度提升到可与非扩散方法竞争的水平。</p>
                    <p><span class="font-medium text-text-primary">方法详情：</span>论文提出Sparse Point-Voxel Diffusion（SPVD）U-Net，双分支并行处理原始点坐标与稀疏体素特征，点分支保持几何细节，体素分支负责高效全局推理；在扩散时间步上采用稀疏卷积-点卷积交替，配合轻量级交叉注意力实现特征融合。为降低步数，引入基于隐空间的implicit generation策略，用小型网络直接预测干净点云，实现5-10步高质量采样。网络支持无条件、类别条件、补全与超分等多任务，统一用L2点坐标损失加 Chamfer 距离加权训练。</p>
                    <p><span class="font-medium text-text-primary">研究结果：</span>在ShapeNet无条件生成基准上，最快SPVD variant以≈30%训练时间、&lt;50%采样步数超越所有非扩散SOTA（MDPA、PointFlow等），FID↓29%；最大模型在JSD、MMD、COV、1-NNA指标全面优于PVD，且单卡采样时间缩短30%。条件生成在55类ShapeNet上平均COV提升4.3%，补全任务CD降低18%，超分4×任务保持边缘细节。implicit 5-step模式仍达到PVD 100-step 97%的FID，实现实时应用可能。</p>
                    <p><span class="font-medium text-text-primary">局限性：</span>方法依赖稀疏卷积库与显存，对&gt;200k点的场景扩展尚未验证；点-体素双分支需手工权衡分辨率与稀疏度，超参数敏感。implicit generation虽快，但在极度稀疏输入时产生局部聚集伪影，理论保证仍缺。</p>
                    
                    <p><span class="font-medium text-text-primary">未来方向：</span>将稀疏点-体素扩散框架拓展到百万级室外场景，并结合可学习自适应分辨率机制；引入多模态条件（文本、图像）以实现可控创作。</p>
                    
                    <p class="text-accent"><span class="font-medium">研究相关性：</span>若研究者关注3-D生成、扩散模型加速或点云-体素混合表示，该文提供了可复现的代码与训练协议，可直接作为强基线或嵌入下游形状补全、SLAM建图与虚拟现实内容生成流程。</p>
                  </div>
                </div>
              </div>
            </div>
            

            <!-- Score details -->
            <div class="pt-4 border-t border-border-color">
              <div class="flex flex-wrap gap-3 text-xs">
                <span class="px-2 py-1 bg-bg-hover rounded text-text-secondary">相似度 0.82</span>
                <span class="px-2 py-1 bg-bg-hover rounded text-text-secondary">
                  期刊影响力 0.75
                  
                    <span class="ml-1 text-blue-600">(IF: 10.2)</span>
                  
                </span>
              </div>
            </div>
          </div>
        </article>
        
        <article class="bg-bg-card rounded-lg border border-border-color overflow-hidden hover:border-accent/50 transition-colors">
          <div class="p-4 md:p-5">
            <!-- Title and metadata -->
            <div class="mb-3">
              <!-- Tags row -->
              <div class="flex items-center gap-2 mb-2 flex-wrap">
                <span class="inline-flex items-center px-2.5 py-0.5 rounded text-xs font-medium
                  bg-green-100 text-green-700 border border-green-300
                  ">
                  必读
                </span>
                <span class="text-xs text-text-secondary">评分 0.80</span>
                <span class="text-xs text-text-secondary">·</span>
                <span class="text-xs text-text-secondary">arxiv</span>
                <!-- Date: visible on mobile, hidden on md+ -->
                <span class="text-xs text-text-secondary md:hidden">· 2025-12-02</span>
              </div>
              <!-- Title and date row -->
              <div class="flex items-start justify-between gap-3 md:gap-4">
                <div class="flex-1">
                  <h2 class="text-base md:text-lg font-semibold text-text-primary leading-tight">
                    <a href="https://arxiv.org/abs/2512.02696v1" target="_blank" rel="noopener" class="hover:text-accent transition-colors">
                      ALDI-ray: Adapting the ALDI Framework for Security X-ray Object Detection
                    </a>
                  </h2>
                  
                  <p class="text-sm text-text-secondary mt-0.5 leading-snug">ALDI-ray：面向安检X射线目标检测的ALDI框架适配</p>
                  
                </div>
                <!-- Date: hidden on mobile, visible on md+ -->
                <div class="hidden md:block text-right text-sm text-text-secondary flex-shrink-0">
                  <div class="font-medium">2025-12-02</div>
                </div>
              </div>
              <!-- Venue: below title -->
              <div class="mt-1 text-xs text-text-secondary" title="arXiv">
                arXiv
                
              </div>
            </div>

            <!-- Authors -->
            <p class="text-sm text-text-secondary mb-3">
              Omid Reza Heidari，Yang Wang，Xinxin Zuo
            </p>

            <!-- DOI -->
            <div class="flex flex-wrap gap-x-4 gap-y-1 text-xs text-text-secondary mb-4">
              
              <span class="flex items-center">
                <span class="font-medium mr-1">ID:</span>
                <span>http://arxiv.org/abs/2512.02696v1</span>
              </span>
              
            </div>

            <!-- Abstract (collapsible) -->
            
            <div class="bg-bg-hover/50 rounded-lg p-4 mb-4 border border-border-color">
              <button id="btn-abstract-28" onclick="toggleSection('abstract-28')"
                      class="w-full text-left text-sm font-medium text-text-primary flex items-center justify-between">
                <span class="flex items-center">
                  <svg class="w-4 h-4 mr-1.5 text-text-secondary" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                    <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M9 12h6m-6 4h6m2 5H7a2 2 0 01-2-2V5a2 2 0 012-2h5.586a1 1 0 01.707.293l5.414 5.414a1 1 0 01.293.707V19a2 2 0 01-2 2z"/>
                  </svg>
                  原文摘要
                </span>
                <svg class="w-4 h-4 text-text-secondary transform transition-transform" id="icon-abstract-28" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                  <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
                </svg>
              </button>
              <div id="abstract-28" class="section-expand collapsed">
                <p class="text-sm text-text-secondary mt-3 leading-relaxed">Domain adaptation in object detection is critical for real-world applications where distribution shifts degrade model performance. Security X-ray imaging presents a unique challenge due to variations in scanning devices and environmental conditions, leading to significant domain discrepancies. To address this, we apply ALDI++, a domain adaptation framework that integrates self-distillation, feature alignment, and enhanced training strategies to mitigate domain shift effectively in this area. We conduct extensive experiments on the EDS dataset, demonstrating that ALDI++ surpasses the state-of-the-art (SOTA) domain adaptation methods across multiple adaptation scenarios. In particular, ALDI++ with a Vision Transformer for Detection (ViTDet) backbone achieves the highest mean average precision (mAP), confirming the effectiveness of transformer-based architectures for cross-domain object detection. Additionally, our category-wise analysis highlights consistent improvements in detection accuracy, reinforcing the robustness of the model across diverse object classes. Our findings establish ALDI++ as an efficient solution for domain-adaptive object detection, setting a new benchmark for performance stability and cross-domain generalization in security X-ray imagery.</p>
              </div>
            </div>
            

            <!-- AI Summary -->
            
            <div class="bg-accent/10 rounded-lg p-4 mb-4 border border-accent/30">
              <h3 class="text-sm font-semibold text-accent mb-3 flex items-center">
                <svg class="w-4 h-4 mr-1.5" fill="currentColor" viewBox="0 0 24 24">
                  <path d="M12 2a2 2 0 012 2c0 .74-.4 1.39-1 1.73V7h1a7 7 0 017 7h1a1 1 0 011 1v3a1 1 0 01-1 1h-1v1a2 2 0 01-2 2H5a2 2 0 01-2-2v-1H2a1 1 0 01-1-1v-3a1 1 0 011-1h1a7 7 0 017-7h1V5.73c-.6-.34-1-.99-1-1.73a2 2 0 012-2z"/>
                </svg>
                AI 总结
              </h3>
              <div class="space-y-2 text-sm text-text-primary">
                <p><span class="font-medium text-accent">研究问题：</span>如何缓解安检X光图像因设备与环境差异造成的域偏移，提升跨域目标检测性能。</p>
                <p><span class="font-medium text-accent">研究方法：</span>将ALDI++框架（自蒸馏+特征对齐+增强训练）与ViTDet骨干结合，在EDS数据集多场景验证。</p>
                <p><span class="font-medium text-accent">主要发现：</span>ALDI++在全部跨域场景取得新SOTA mAP，ViTDet版本最高，各类别精度一致提升。</p>
                <p><span class="font-medium text-accent">创新点：</span>首次把ALDI++引入X光安检，证明Transformer骨干在跨域X光检测的潜力并刷新基准。</p>
                
                <p><span class="font-medium text-accent">相关性：</span>为安检、工业成像等域差异显著场景提供即插即用的跨域检测解决方案与公开基准。</p>
                
              </div>

              <div class="mt-3 pt-3 border-t border-accent/30">
                <button id="btn-detail-28" onclick="toggleSection('detail-28')"
                        class="text-xs text-accent hover:text-accent-hover font-medium flex items-center">
                  <svg class="w-3 h-3 mr-1" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                    <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
                  </svg>
                  查看详细分析
                </button>
                <div id="detail-28" class="section-expand collapsed mt-3">
                  <div class="bg-bg-card rounded-lg p-4 text-sm text-text-primary space-y-3 border border-border-color">
                    <p><span class="font-medium text-text-primary">研究背景：</span>安检X光图像因扫描设备、电压、行李材质等差异造成严重的域漂移，导致在某一设备上训练的检测器迁移到另一设备时性能骤降。现有域适应方法多针对自然图像设计，难以直接应对X光图像低对比度、伪彩色、遮挡重叠等特殊挑战。</p>
                    <p><span class="font-medium text-text-primary">方法详情：</span>作者将近期提出的ALDI++框架首次引入安检场景，该框架在Faster R-CNN基础上引入三要素：教师-学生自蒸馏保持源域知识、双向特征对齐（图像级+实例级）减小分布距离、以及强数据增广与伪标签再训练提升目标域鲁棒性。实验采用ViTDet与ResNet-50两种骨干，在六个跨设备迁移任务上系统比较，并给出类别级精度分解。</p>
                    <p><span class="font-medium text-text-primary">研究结果：</span>在EDS公开数据集的六种域迁移设定中，ALDI++将基线Faster R-CNN的mAP从41.7%提升至58.2%，超越此前最佳方法+4.1%，其中ViTDet骨干进一步达到60.9%的新SOTA。类别-wise结果显示枪支、刀具等关键违禁品检测率提升最显著（+6–9% AP），且方差降低，表明跨域稳定性增强。</p>
                    <p><span class="font-medium text-text-primary">局限性：</span>论文仅在EDS一个数据集验证，尚未覆盖机场、地铁等更多真实场景；ViTDet带来的计算与显存开销较大，对边缘安检机部署形成挑战；ALDI++含多阶段伪标签更新，训练周期比纯源域模型长约2×。</p>
                    
                    <p><span class="font-medium text-text-primary">未来方向：</span>后续可探索轻量化蒸馏策略以实现实时推理，并将ALDI++扩展到多光谱CT、3D断层等新型安检成像模态。</p>
                    
                    <p class="text-accent"><span class="font-medium">研究相关性：</span>若研究者关注低对比度图像的域适应、安检违禁品检测或自蒸馏在目标检测中的应用，本文提供了可复现的基准与详细的类别级分析代码。</p>
                  </div>
                </div>
              </div>
            </div>
            

            <!-- Score details -->
            <div class="pt-4 border-t border-border-color">
              <div class="flex flex-wrap gap-3 text-xs">
                <span class="px-2 py-1 bg-bg-hover rounded text-text-secondary">相似度 0.85</span>
                <span class="px-2 py-1 bg-bg-hover rounded text-text-secondary">
                  期刊影响力 0.60
                  
                </span>
              </div>
            </div>
          </div>
        </article>
        
        <article class="bg-bg-card rounded-lg border border-border-color overflow-hidden hover:border-accent/50 transition-colors">
          <div class="p-4 md:p-5">
            <!-- Title and metadata -->
            <div class="mb-3">
              <!-- Tags row -->
              <div class="flex items-center gap-2 mb-2 flex-wrap">
                <span class="inline-flex items-center px-2.5 py-0.5 rounded text-xs font-medium
                  bg-green-100 text-green-700 border border-green-300
                  ">
                  必读
                </span>
                <span class="text-xs text-text-secondary">评分 0.80</span>
                <span class="text-xs text-text-secondary">·</span>
                <span class="text-xs text-text-secondary">arxiv</span>
                <!-- Date: visible on mobile, hidden on md+ -->
                <span class="text-xs text-text-secondary md:hidden">· 2025-12-04</span>
              </div>
              <!-- Title and date row -->
              <div class="flex items-start justify-between gap-3 md:gap-4">
                <div class="flex-1">
                  <h2 class="text-base md:text-lg font-semibold text-text-primary leading-tight">
                    <a href="https://arxiv.org/abs/2512.04581v1" target="_blank" rel="noopener" class="hover:text-accent transition-colors">
                      Infrared UAV Target Tracking with Dynamic Feature Refinement and Global Contextual Attention Knowledge Distillation
                    </a>
                  </h2>
                  
                  <p class="text-sm text-text-secondary mt-0.5 leading-snug">基于动态特征细化与全局上下文注意力知识蒸馏的红外无人机目标跟踪</p>
                  
                </div>
                <!-- Date: hidden on mobile, visible on md+ -->
                <div class="hidden md:block text-right text-sm text-text-secondary flex-shrink-0">
                  <div class="font-medium">2025-12-04</div>
                </div>
              </div>
              <!-- Venue: below title -->
              <div class="mt-1 text-xs text-text-secondary" title="arXiv">
                arXiv
                
              </div>
            </div>

            <!-- Authors -->
            <p class="text-sm text-text-secondary mb-3">
              Houzhang Fang，Chenxing Wu，Kun Bai，Tianqi Chen，Xiaolin Wang 等
            </p>

            <!-- DOI -->
            <div class="flex flex-wrap gap-x-4 gap-y-1 text-xs text-text-secondary mb-4">
              
              <span class="flex items-center">
                <span class="font-medium mr-1">ID:</span>
                <span>http://arxiv.org/abs/2512.04581v1</span>
              </span>
              
            </div>

            <!-- Abstract (collapsible) -->
            
            <div class="bg-bg-hover/50 rounded-lg p-4 mb-4 border border-border-color">
              <button id="btn-abstract-29" onclick="toggleSection('abstract-29')"
                      class="w-full text-left text-sm font-medium text-text-primary flex items-center justify-between">
                <span class="flex items-center">
                  <svg class="w-4 h-4 mr-1.5 text-text-secondary" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                    <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M9 12h6m-6 4h6m2 5H7a2 2 0 01-2-2V5a2 2 0 012-2h5.586a1 1 0 01.707.293l5.414 5.414a1 1 0 01.293.707V19a2 2 0 01-2 2z"/>
                  </svg>
                  原文摘要
                </span>
                <svg class="w-4 h-4 text-text-secondary transform transition-transform" id="icon-abstract-29" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                  <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
                </svg>
              </button>
              <div id="abstract-29" class="section-expand collapsed">
                <p class="text-sm text-text-secondary mt-3 leading-relaxed">Unmanned aerial vehicle (UAV) target tracking based on thermal infrared imaging has been one of the most important sensing technologies in anti-UAV applications. However, the infrared UAV targets often exhibit weak features and complex backgrounds, posing significant challenges to accurate tracking. To address these problems, we introduce SiamDFF, a novel dynamic feature fusion Siamese network that integrates feature enhancement and global contextual attention knowledge distillation for infrared UAV target (IRUT) tracking. The SiamDFF incorporates a selective target enhancement network (STEN), a dynamic spatial feature aggregation module (DSFAM), and a dynamic channel feature aggregation module (DCFAM). The STEN employs intensity-aware multi-head cross-attention to adaptively enhance important regions for both template and search branches. The DSFAM enhances multi-scale UAV target features by integrating local details with global features, utilizing spatial attention guidance within the search frame. The DCFAM effectively integrates the mixed template generated from STEN in the template branch and original template, avoiding excessive background interference with the template and thereby enhancing the emphasis on UAV target region features within the search frame. Furthermore, to enhance the feature extraction capabilities of the network for IRUT without adding extra computational burden, we propose a novel tracking-specific target-aware contextual attention knowledge distiller. It transfers the target prior from the teacher network to the student model, significantly improving the student network&#39;s focus on informative regions at each hierarchical level of the backbone network. Extensive experiments on real infrared UAV datasets demonstrate that the proposed approach outperforms state-of-the-art target trackers under complex backgrounds while achieving a real-time tracking speed.</p>
              </div>
            </div>
            

            <!-- AI Summary -->
            
            <div class="bg-accent/10 rounded-lg p-4 mb-4 border border-accent/30">
              <h3 class="text-sm font-semibold text-accent mb-3 flex items-center">
                <svg class="w-4 h-4 mr-1.5" fill="currentColor" viewBox="0 0 24 24">
                  <path d="M12 2a2 2 0 012 2c0 .74-.4 1.39-1 1.73V7h1a7 7 0 017 7h1a1 1 0 011 1v3a1 1 0 01-1 1h-1v1a2 2 0 01-2 2H5a2 2 0 01-2-2v-1H2a1 1 0 01-1-1v-3a1 1 0 011-1h1a7 7 0 017-7h1V5.73c-.6-.34-1-.99-1-1.73a2 2 0 012-2z"/>
                </svg>
                AI 总结
              </h3>
              <div class="space-y-2 text-sm text-text-primary">
                <p><span class="font-medium text-accent">研究问题：</span>解决红外无人机目标特征弱、背景复杂导致的跟踪精度低问题。</p>
                <p><span class="font-medium text-accent">研究方法：</span>提出SiamDFF网络，结合STEN、DSFAM、DCFAM及全局上下文注意力知识蒸馏。</p>
                <p><span class="font-medium text-accent">主要发现：</span>在真实红外无人机数据集上精度超越现有算法并保持实时速度。</p>
                <p><span class="font-medium text-accent">创新点：</span>首次将动态特征细化与目标感知上下文注意力知识蒸馏引入红外UAV跟踪。</p>
                
                <p><span class="font-medium text-accent">相关性：</span>为反无人机系统提供高精度实时红外跟踪技术，推动夜间低可视场景应用。</p>
                
              </div>

              <div class="mt-3 pt-3 border-t border-accent/30">
                <button id="btn-detail-29" onclick="toggleSection('detail-29')"
                        class="text-xs text-accent hover:text-accent-hover font-medium flex items-center">
                  <svg class="w-3 h-3 mr-1" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                    <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
                  </svg>
                  查看详细分析
                </button>
                <div id="detail-29" class="section-expand collapsed mt-3">
                  <div class="bg-bg-card rounded-lg p-4 text-sm text-text-primary space-y-3 border border-border-color">
                    <p><span class="font-medium text-text-primary">研究背景：</span>热红外无人机目标跟踪是反无人机系统的核心感知技术，但红外图像目标信号弱、背景杂波强，导致现有跟踪器在复杂场景中频繁漂移。</p>
                    <p><span class="font-medium text-text-primary">方法详情：</span>作者提出 SiamDFF，一种孪生动态特征融合网络，包含选择性目标增强网络(STEN)用强度感知多头交叉注意力同时增强模板与搜索帧关键区域；动态空间特征聚合模块(DSFAM)在搜索帧内以空间注意力引导多尺度局部-全局特征融合；动态通道特征聚合模块(DCFAM)将 STEN 生成的混合模板与原始模板自适应整合，抑制背景干扰。</p>
                    <p><span class="font-medium text-text-primary">研究结果：</span>在真实红外无人机数据集上的实验表明，SiamDFF 在复杂背景下精度超越现有最先进方法，同时保持实时速度；引入的无额外推理开销的层级目标感知上下文注意力知识蒸馏，使学生网络在各层骨干特征上均获得更强的目标区域聚焦能力。</p>
                    <p><span class="font-medium text-text-primary">局限性：</span>方法仅在自制红外无人机序列验证，缺乏跨光谱与跨场景泛化评估；动态模块增加训练复杂度，对硬件资源要求较高；未公开源代码与完整数据集，复现性受限。</p>
                    
                    <p><span class="font-medium text-text-primary">未来方向：</span>后续可扩展至可见光-红外双模跟踪并研究轻量化部署，以满足边缘端反无人机实战需求。</p>
                    
                    <p class="text-accent"><span class="font-medium">研究相关性：</span>该文系统解决了红外弱小目标特征增强与背景抑制难题，其动态融合与知识蒸馏策略可为研究低信噪比目标跟踪、孪生网络优化及跨模态蒸馏的学者提供直接参考。</p>
                  </div>
                </div>
              </div>
            </div>
            

            <!-- Score details -->
            <div class="pt-4 border-t border-border-color">
              <div class="flex flex-wrap gap-3 text-xs">
                <span class="px-2 py-1 bg-bg-hover rounded text-text-secondary">相似度 0.85</span>
                <span class="px-2 py-1 bg-bg-hover rounded text-text-secondary">
                  期刊影响力 0.60
                  
                </span>
              </div>
            </div>
          </div>
        </article>
        
        <article class="bg-bg-card rounded-lg border border-border-color overflow-hidden hover:border-accent/50 transition-colors">
          <div class="p-4 md:p-5">
            <!-- Title and metadata -->
            <div class="mb-3">
              <!-- Tags row -->
              <div class="flex items-center gap-2 mb-2 flex-wrap">
                <span class="inline-flex items-center px-2.5 py-0.5 rounded text-xs font-medium
                  bg-green-100 text-green-700 border border-green-300
                  ">
                  必读
                </span>
                <span class="text-xs text-text-secondary">评分 0.80</span>
                <span class="text-xs text-text-secondary">·</span>
                <span class="text-xs text-text-secondary">arxiv</span>
                <!-- Date: visible on mobile, hidden on md+ -->
                <span class="text-xs text-text-secondary md:hidden">· 2025-12-02</span>
              </div>
              <!-- Title and date row -->
              <div class="flex items-start justify-between gap-3 md:gap-4">
                <div class="flex-1">
                  <h2 class="text-base md:text-lg font-semibold text-text-primary leading-tight">
                    <a href="https://arxiv.org/abs/2512.03004v1" target="_blank" rel="noopener" class="hover:text-accent transition-colors">
                      DGGT: Feedforward 4D Reconstruction of Dynamic Driving Scenes using Unposed Images
                    </a>
                  </h2>
                  
                  <p class="text-sm text-text-secondary mt-0.5 leading-snug">DGGT：基于无位姿图像的动态驾驶场景前馈4D重建</p>
                  
                </div>
                <!-- Date: hidden on mobile, visible on md+ -->
                <div class="hidden md:block text-right text-sm text-text-secondary flex-shrink-0">
                  <div class="font-medium">2025-12-02</div>
                </div>
              </div>
              <!-- Venue: below title -->
              <div class="mt-1 text-xs text-text-secondary" title="arXiv">
                arXiv
                
              </div>
            </div>

            <!-- Authors -->
            <p class="text-sm text-text-secondary mb-3">
              Xiaoxue Chen，Ziyi Xiong，Yuantao Chen，Gen Li，Nan Wang 等
            </p>

            <!-- DOI -->
            <div class="flex flex-wrap gap-x-4 gap-y-1 text-xs text-text-secondary mb-4">
              
              <span class="flex items-center">
                <span class="font-medium mr-1">ID:</span>
                <span>http://arxiv.org/abs/2512.03004v1</span>
              </span>
              
            </div>

            <!-- Abstract (collapsible) -->
            
            <div class="bg-bg-hover/50 rounded-lg p-4 mb-4 border border-border-color">
              <button id="btn-abstract-30" onclick="toggleSection('abstract-30')"
                      class="w-full text-left text-sm font-medium text-text-primary flex items-center justify-between">
                <span class="flex items-center">
                  <svg class="w-4 h-4 mr-1.5 text-text-secondary" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                    <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M9 12h6m-6 4h6m2 5H7a2 2 0 01-2-2V5a2 2 0 012-2h5.586a1 1 0 01.707.293l5.414 5.414a1 1 0 01.293.707V19a2 2 0 01-2 2z"/>
                  </svg>
                  原文摘要
                </span>
                <svg class="w-4 h-4 text-text-secondary transform transition-transform" id="icon-abstract-30" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                  <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
                </svg>
              </button>
              <div id="abstract-30" class="section-expand collapsed">
                <p class="text-sm text-text-secondary mt-3 leading-relaxed">Autonomous driving needs fast, scalable 4D reconstruction and re-simulation for training and evaluation, yet most methods for dynamic driving scenes still rely on per-scene optimization, known camera calibration, or short frame windows, making them slow and impractical. We revisit this problem from a feedforward perspective and introduce \textbf{Driving Gaussian Grounded Transformer (DGGT)}, a unified framework for pose-free dynamic scene reconstruction. We note that the existing formulations, treating camera pose as a required input, limit flexibility and scalability. Instead, we reformulate pose as an output of the model, enabling reconstruction directly from sparse, unposed images and supporting an arbitrary number of views for long sequences. Our approach jointly predicts per-frame 3D Gaussian maps and camera parameters, disentangles dynamics with a lightweight dynamic head, and preserves temporal consistency with a lifespan head that modulates visibility over time. A diffusion-based rendering refinement further reduces motion/interpolation artifacts and improves novel-view quality under sparse inputs. The result is a single-pass, pose-free algorithm that achieves state-of-the-art performance and speed. Trained and evaluated on large-scale driving benchmarks (Waymo, nuScenes, Argoverse2), our method outperforms prior work both when trained on each dataset and in zero-shot transfer across datasets, and it scales well as the number of input frames increases.</p>
              </div>
            </div>
            

            <!-- AI Summary -->
            
            <div class="bg-accent/10 rounded-lg p-4 mb-4 border border-accent/30">
              <h3 class="text-sm font-semibold text-accent mb-3 flex items-center">
                <svg class="w-4 h-4 mr-1.5" fill="currentColor" viewBox="0 0 24 24">
                  <path d="M12 2a2 2 0 012 2c0 .74-.4 1.39-1 1.73V7h1a7 7 0 017 7h1a1 1 0 011 1v3a1 1 0 01-1 1h-1v1a2 2 0 01-2 2H5a2 2 0 01-2-2v-1H2a1 1 0 01-1-1v-3a1 1 0 011-1h1a7 7 0 017-7h1V5.73c-.6-.34-1-.99-1-1.73a2 2 0 012-2z"/>
                </svg>
                AI 总结
              </h3>
              <div class="space-y-2 text-sm text-text-primary">
                <p><span class="font-medium text-accent">研究问题：</span>如何无需相机位姿，单次前馈完成长时动态驾驶场景的4D重建与重仿真。</p>
                <p><span class="font-medium text-accent">研究方法：</span>DGGT端到端输出每帧3D高斯地图与相机参数，用动态头解耦运动、寿命头保持时序一致，并以扩散渲染精炼。</p>
                <p><span class="font-medium text-accent">主要发现：</span>在Waymo等三大基准上，单趟推理即达SOTA质量与速度，跨数据集零样本迁移仍领先，且随帧数增加性能稳定提升。</p>
                <p><span class="font-medium text-accent">创新点：</span>首次将相机位姿作为模型输出，实现任意视角、任意长度、无优化4D重建；提出寿命调制与扩散精炼联合管线。</p>
                
                <p><span class="font-medium text-accent">相关性：</span>为自动驾驶训练/测试提供快速可扩展的4D数据生成方案，免除昂贵标定与逐场景优化，推动仿真系统实用化。</p>
                
              </div>

              <div class="mt-3 pt-3 border-t border-accent/30">
                <button id="btn-detail-30" onclick="toggleSection('detail-30')"
                        class="text-xs text-accent hover:text-accent-hover font-medium flex items-center">
                  <svg class="w-3 h-3 mr-1" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                    <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
                  </svg>
                  查看详细分析
                </button>
                <div id="detail-30" class="section-expand collapsed mt-3">
                  <div class="bg-bg-card rounded-lg p-4 text-sm text-text-primary space-y-3 border border-border-color">
                    <p><span class="font-medium text-text-primary">研究背景：</span>自动驾驶的仿真与训练需要快速、可扩展的4D动态场景重建，但现有方法普遍依赖逐场景优化、已知相机标定或短帧窗口，导致重建速度慢、难以落地。作者重新审视这一问题，提出将相机位姿从“输入”改为“输出”，以摆脱对精确标定的依赖，实现真正可扩展的前馈式重建。</p>
                    <p><span class="font-medium text-text-primary">方法详情：</span>论文提出Driving Gaussian Grounded Transformer (DGGT)，统一网络在一次前馈中同时预测每帧3D高斯场景表示和相机内外参，无需任何位姿初始化。动态部分由轻量级动态头解耦，寿命头对高斯可见性进行时序调制以保持一致性；渲染阶段引入扩散式细化模块，抑制稀疏输入下的运动与插值伪影。整个框架支持任意视角数量和长序列，训练与推理均为单趟完成。</p>
                    <p><span class="font-medium text-text-primary">研究结果：</span>在Waymo、nuScenes、Argoverse2三大基准上，DGGT在逐数据集训练和零样本跨数据集测试均取得SOTA新视角合成与深度精度，同时推理速度比基于优化的方法快一个数量级。随着输入帧数增加，性能持续提升而内存增长缓慢，验证了良好的可扩展性。消融实验表明，位姿自预测、寿命调制与扩散细化分别带来显著增益。</p>
                    <p><span class="font-medium text-text-primary">局限性：</span>方法目前假设场景光照基本静态，对剧烈光照变化或夜间场景的鲁棒性尚未验证；扩散细化虽提升视觉质量，但引入额外推理开销，对实时性要求极高的车载部署仍存挑战。此外，网络对极端稀疏视角（&lt;3帧）时位姿估计方差增大，可能导致重建漂移。</p>
                    
                    <p><span class="font-medium text-text-primary">未来方向：</span>未来可探索将DGGT与在线自监督位姿优化循环结合，以进一步压缩稀疏视角误差；同时引入光照与天气条件编码，实现全天候动态场景的前馈重建。</p>
                    
                    <p class="text-accent"><span class="font-medium">研究相关性：</span>若研究者关注自动驾驶仿真、动态NeRF、无标定立体重建或前馈式3D感知，本工作提供了将位姿估计与场景表示联合学习的新范式，可直接借鉴其网络设计与跨数据集训练策略。</p>
                  </div>
                </div>
              </div>
            </div>
            

            <!-- Score details -->
            <div class="pt-4 border-t border-border-color">
              <div class="flex flex-wrap gap-3 text-xs">
                <span class="px-2 py-1 bg-bg-hover rounded text-text-secondary">相似度 0.85</span>
                <span class="px-2 py-1 bg-bg-hover rounded text-text-secondary">
                  期刊影响力 0.60
                  
                </span>
              </div>
            </div>
          </div>
        </article>
        
      </div>
      </div><!-- End of collapsible content -->
    </div>
  </main>

  <!-- Footer -->
  <footer class="bg-bg-card border-t border-border-color mt-12">
    <div class="content-container py-6 text-center text-sm text-text-secondary">
      灵感来自 <a href="https://github.com/Yorks0n/ZotWatch" class="text-accent hover:text-accent-hover transition-colors">ZotWatch</a>
    </div>
  </footer>

  <script>
    function toggleSection(id) {
      const el = document.getElementById(id);
      const btn = document.getElementById('btn-' + id);
      const icon = document.getElementById('icon-' + id);
      const hint = document.getElementById('hint-' + id);
      if (el.classList.contains('collapsed')) {
        el.classList.remove('collapsed');
        el.classList.add('expanded');
        if (btn && btn.textContent.includes('详细分析')) {
          btn.innerHTML = '<svg class="w-3 h-3 mr-1" fill="none" stroke="currentColor" viewBox="0 0 24 24"><path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M5 15l7-7 7 7"/></svg>收起详细分析';
        }
        if (hint) hint.textContent = '点击收起';
        if (icon) icon.style.transform = 'rotate(180deg)';
        // Trigger chart resize for hidden charts
        if (id === 'researcher-profile') {
          setTimeout(() => { window.dispatchEvent(new Event('resize')); }, 100);
        }
      } else {
        el.classList.remove('expanded');
        el.classList.add('collapsed');
        if (btn && btn.textContent.includes('详细分析')) {
          btn.innerHTML = '<svg class="w-3 h-3 mr-1" fill="none" stroke="currentColor" viewBox="0 0 24 24"><path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/></svg>查看详细分析';
        }
        if (hint) hint.textContent = '点击展开';
        if (icon) icon.style.transform = 'rotate(0deg)';
      }
    }
  </script>
</body>
</html>