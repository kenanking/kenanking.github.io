<!DOCTYPE html>
<html lang="zh-CN">
<head>
  <meta charset="utf-8">
  <meta name="viewport" content="width=device-width, initial-scale=1">
  <title>ZotWatch 文献推荐 - 2026-01-09</title>
  <script src="https://cdn.tailwindcss.com"></script>
  <script src="https://cdn.jsdelivr.net/npm/chart.js@4.4.1/dist/chart.umd.min.js"></script>
  <script src="https://d3js.org/d3.v7.min.js"></script>
  <script>
    tailwind.config = {
      theme: {
        extend: {
          colors: {
            'bg-primary': '#f8fafc',
            'bg-card': '#ffffff',
            'bg-hover': '#f1f5f9',
            'text-primary': '#1e293b',
            'text-secondary': '#64748b',
            'border-color': '#e2e8f0',
            'accent': '#2563eb',
            'accent-hover': '#1d4ed8',
          }
        }
      }
    }
  </script>
  <style>
    .section-expand { transition: max-height 0.5s ease-out; overflow: hidden; }
    .section-expand.collapsed { max-height: 0; }
    .section-expand.expanded { max-height: none; }
    body { font-family: -apple-system, BlinkMacSystemFont, 'Segoe UI', Roboto, 'Helvetica Neue', Arial, sans-serif; }
    .content-container { max-width: 48rem; margin: 0 auto; padding: 0 1rem; }
    @media (min-width: 768px) { .content-container { padding: 0 2rem; } }
    @media (min-width: 1024px) { .content-container { padding: 0 4rem; } }

    .cluster-graph-container { width: 100%; height: 280px; touch-action: pan-y; }
    @media (min-width: 640px) { .cluster-graph-container { height: 320px; } }
    @media (min-width: 768px) { .cluster-graph-container { height: 350px; } }
    #cluster-graph svg { overflow: visible; }
  </style>
</head>
<body class="bg-bg-primary min-h-screen text-text-primary">
  <header class="bg-bg-card border-b border-border-color">
    <div class="content-container py-6 md:py-8">
      <h1 class="text-xl md:text-2xl font-bold text-text-primary">ZotWatch 文献推荐</h1>
      <p class="text-xs md:text-sm text-text-secondary mt-1">
        共 35 篇论文 ·
        生成于 2026-01-09 10:58 Asia/Shanghai
      </p>
    </div>
  </header>

  
  <section class="py-4 md:py-6 border-b border-border-color">
    <div class="content-container">
      <button id="btn-researcher-profile" onclick="toggleSection('researcher-profile')"
              class="w-full text-left text-lg md:text-xl font-bold text-text-primary mb-3 md:mb-4 flex items-center justify-between group hover:text-accent transition-colors">
        <span class="flex items-center gap-2 flex-1 min-w-0">
          <svg class="w-5 h-5 flex-shrink-0 text-accent" fill="currentColor" viewBox="0 0 24 24">
            <path d="M12 2C6.48 2 2 6.48 2 12s4.48 10 10 10 10-4.48 10-10S17.52 2 12 2zm0 3c1.66 0 3 1.34 3 3s-1.34 3-3 3-3-1.34-3-3 1.34-3 3-3zm0 14.2c-2.5 0-4.71-1.28-6-3.22.03-1.99 4-3.08 6-3.08 1.99 0 5.97 1.09 6 3.08-1.29 1.94-3.5 3.22-6 3.22z"/>
          </svg>
          <span class="truncate">研究兴趣画像</span>
        </span>
        <span class="flex items-center gap-1 md:gap-2 flex-shrink-0 ml-2">
          <span class="hidden md:inline text-sm font-normal text-text-secondary group-hover:text-accent" id="hint-researcher-profile">点击展开</span>
          <svg class="w-5 h-5 md:w-4 md:h-4 text-text-secondary group-hover:text-accent transform transition-transform" id="icon-researcher-profile" fill="none" stroke="currentColor" viewBox="0 0 24 24">
            <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
          </svg>
        </span>
      </button>

      <div id="researcher-profile" class="section-expand collapsed">
      <div class="bg-bg-card rounded-lg border border-border-color p-4 mb-4">
        <div class="grid grid-cols-4 gap-3 text-center">
          <div>
            <div class="text-xl font-bold text-accent">957</div>
            <div class="text-xs text-text-secondary">收藏论文</div>
          </div>
          <div>
            <div class="text-xl font-bold text-accent">5年8月</div>
            <div class="text-xs text-text-secondary">收藏时长</div>
          </div>
          <div>
            <div class="text-xl font-bold text-accent">&gt;10</div>
            <div class="text-xs text-text-secondary">兴趣领域</div>
          </div>
          <div>
            <div class="text-xl font-bold text-accent">8</div>
            <div class="text-xs text-text-secondary">高频作者</div>
          </div>
        </div>
      </div>

      
      <div class="bg-accent/10 rounded-lg border border-accent/30 p-4 mb-4">
        <h3 class="text-sm font-semibold text-accent mb-2 flex items-center gap-2">
          <svg class="w-4 h-4" fill="currentColor" viewBox="0 0 24 24">
            <path d="M12 2a2 2 0 012 2c0 .74-.4 1.39-1 1.73V7h1a7 7 0 017 7h1a1 1 0 011 1v3a1 1 0 01-1 1h-1v1a2 2 0 01-2 2H5a2 2 0 01-2-2v-1H2a1 1 0 01-1-1v-3a1 1 0 011-1h1a7 7 0 017-7h1V5.73c-.6-.34-1-.99-1-1.73a2 2 0 012-2z"/>
          </svg>
          AI 研究兴趣分析
        </h3>
        <div class="space-y-2 text-xs text-text-primary leading-relaxed">
          <p><span class="font-medium text-accent">研究兴趣：</span>用户长期关注计算机视觉与遥感智能的交叉，重点阅读目标检测、视觉定位及模型压缩等方向的核心论文，体现出对高精度感知算法及其轻量化落地的持续兴趣。</p>
          <p><span class="font-medium text-accent">深度关注：</span>在目标检测与视觉定位领域收藏量领先，且高频追踪Kaiming He、Ross Girshick等权威团队工作，形成从基础CNN到Transformer检测框架的系统性积累；同时持续阅读IEEE TGARS、雷达学报等遥感顶刊，具备SAR图像理解与多模态感知深度融合的研究视野。</p>
          <p><span class="font-medium text-accent">跨学科倾向：</span>阅读轨迹横跨计算机视觉、遥感、导航与机器学习，既关注CVPR/ICCV前沿方法，也系统吸收SAR成像、卫星导航等领域的应用论文，呈现出“视觉+遥感+导航”的交叉阅读特征。</p>
          <p><span class="font-medium text-accent">兴趣演变：</span>2025-Q1出现收藏峰值后，新增文献聚焦“合成孔径雷达目标检测”“恒虚警率检测”与“多任务学习”，显示兴趣正从通用目标检测向SAR特定场景、小样本学习及可靠性检测迁移；2026-Q1收藏量骤降，可能处于主题筛选或方向转换期。</p>
          <p><span class="font-medium text-accent">阅读建议：</span>可进一步关注SAR图像自监督预训练、多模态遥感基础模型及边缘端CFAR加速部署，以延续检测精度与轻量化并重的研究主线。</p>
        </div>
      </div>
      

      <div class="grid grid-cols-1 md:grid-cols-2 gap-4 mb-4">
        
        <div class="bg-bg-card rounded-lg border border-border-color p-4">
          <h4 class="text-sm font-medium text-text-primary mb-3">研究领域分布</h4>
          <div style="height: 180px;">
            <canvas id="domainsChart"></canvas>
          </div>
        </div>
        

        
        <div class="bg-bg-card rounded-lg border border-border-color p-4">
          <h4 class="text-sm font-medium text-text-primary mb-3">阅读趋势（季度）</h4>
          <div style="height: 180px;">
            <canvas id="trendsChart"></canvas>
          </div>
        </div>
        
      </div>

      
      <div class="bg-bg-card rounded-lg border border-border-color p-4 mb-4">
        <h4 class="text-sm font-medium text-text-primary mb-3">论文发表年份分布</h4>
        <div style="height: 180px;">
          <canvas id="yearDistChart"></canvas>
        </div>
      </div>
      

      
      <div class="bg-bg-card rounded-lg border border-border-color p-4 mb-4">
        <h4 class="text-sm font-medium text-text-primary mb-3">
          研究方向聚类图谱
          <span class="text-xs text-text-secondary font-normal ml-2">(30 个聚类)</span>
        </h4>
        <div id="cluster-graph" class="cluster-graph-container"></div>
        <div class="text-xs text-text-secondary mt-3">
          覆盖 933/933 篇论文（仅含摘要的文献参与聚类）
        </div>
      </div>
      

      <div class="grid grid-cols-1 md:grid-cols-2 gap-4 mb-4">
        
        <div class="bg-bg-card rounded-lg border border-border-color p-4">
          <h4 class="text-sm font-medium text-text-primary mb-2">高频关注作者</h4>
          <div class="grid grid-cols-2 gap-x-4 gap-y-1 text-xs">
            
            <div class="flex justify-between truncate">
              <span class="text-text-primary truncate" title="Kaiming He">Kaiming He</span>
              <span class="text-text-secondary ml-1 flex-shrink-0">24</span>
            </div>
            
            <div class="flex justify-between truncate">
              <span class="text-text-primary truncate" title="Ross Girshick">Ross Girshick</span>
              <span class="text-text-secondary ml-1 flex-shrink-0">18</span>
            </div>
            
            <div class="flex justify-between truncate">
              <span class="text-text-primary truncate" title="Jian Sun">Jian Sun</span>
              <span class="text-text-secondary ml-1 flex-shrink-0">15</span>
            </div>
            
            <div class="flex justify-between truncate">
              <span class="text-text-primary truncate" title="Song Han">Song Han</span>
              <span class="text-text-secondary ml-1 flex-shrink-0">14</span>
            </div>
            
            <div class="flex justify-between truncate">
              <span class="text-text-primary truncate" title="Xiangyu Zhang">Xiangyu Zhang</span>
              <span class="text-text-secondary ml-1 flex-shrink-0">14</span>
            </div>
            
            <div class="flex justify-between truncate">
              <span class="text-text-primary truncate" title="Piotr Dollár">Piotr Dollár</span>
              <span class="text-text-secondary ml-1 flex-shrink-0">12</span>
            </div>
            
            <div class="flex justify-between truncate">
              <span class="text-text-primary truncate" title="Li Liu">Li Liu</span>
              <span class="text-text-secondary ml-1 flex-shrink-0">11</span>
            </div>
            
            <div class="flex justify-between truncate">
              <span class="text-text-primary truncate" title="Xiang Li">Xiang Li</span>
              <span class="text-text-secondary ml-1 flex-shrink-0">11</span>
            </div>
            
            <div class="flex justify-between truncate">
              <span class="text-text-primary truncate" title="Yann LeCun">Yann LeCun</span>
              <span class="text-text-secondary ml-1 flex-shrink-0">9</span>
            </div>
            
            <div class="flex justify-between truncate">
              <span class="text-text-primary truncate" title="Wenfeng Liang">Wenfeng Liang</span>
              <span class="text-text-secondary ml-1 flex-shrink-0">9</span>
            </div>
            
            <div class="flex justify-between truncate">
              <span class="text-text-primary truncate" title="Feng Xu">Feng Xu</span>
              <span class="text-text-secondary ml-1 flex-shrink-0">9</span>
            </div>
            
            <div class="flex justify-between truncate">
              <span class="text-text-primary truncate" title="Chunhua Shen">Chunhua Shen</span>
              <span class="text-text-secondary ml-1 flex-shrink-0">8</span>
            </div>
            
          </div>
        </div>
        

        
        <div class="bg-bg-card rounded-lg border border-border-color p-4">
          <h4 class="text-sm font-medium text-text-primary mb-2">常关注期刊/会议</h4>
          <div class="space-y-1 text-xs">
            
            <div class="flex items-center justify-between">
              <span class="text-text-primary truncate flex-1" title="Computer Vision and Pattern Recognition">Computer Vision and Pattern Recognition</span>
              <span class="text-xs px-1 py-0.5 rounded bg-blue-100 text-blue-700 ml-1 flex-shrink-0">
                期刊
              </span>
              <span class="text-text-secondary ml-1 flex-shrink-0">114</span>
            </div>
            
            <div class="flex items-center justify-between">
              <span class="text-text-primary truncate flex-1" title="IEEE Transactions on Geoscience and Remote Sensing">IEEE Transactions on Geoscience and Remote Sensing</span>
              <span class="text-xs px-1 py-0.5 rounded bg-blue-100 text-blue-700 ml-1 flex-shrink-0">
                期刊
              </span>
              <span class="text-text-secondary ml-1 flex-shrink-0">46</span>
            </div>
            
            <div class="flex items-center justify-between">
              <span class="text-text-primary truncate flex-1" title="Neural Information Processing Systems">Neural Information Processing Systems</span>
              <span class="text-xs px-1 py-0.5 rounded bg-blue-100 text-blue-700 ml-1 flex-shrink-0">
                期刊
              </span>
              <span class="text-text-secondary ml-1 flex-shrink-0">45</span>
            </div>
            
            <div class="flex items-center justify-between">
              <span class="text-text-primary truncate flex-1" title="International Conference on Computer Vision">International Conference on Computer Vision</span>
              <span class="text-xs px-1 py-0.5 rounded bg-amber-100 text-amber-700 ml-1 flex-shrink-0">
                会议
              </span>
              <span class="text-text-secondary ml-1 flex-shrink-0">38</span>
            </div>
            
            <div class="flex items-center justify-between">
              <span class="text-text-primary truncate flex-1" title="IEEE Transactions on Pattern Analysis and Machine Intelligence">IEEE Transactions on Pattern Analysis and Machine Intelligence</span>
              <span class="text-xs px-1 py-0.5 rounded bg-blue-100 text-blue-700 ml-1 flex-shrink-0">
                期刊
              </span>
              <span class="text-text-secondary ml-1 flex-shrink-0">32</span>
            </div>
            
            <div class="flex items-center justify-between">
              <span class="text-text-primary truncate flex-1" title="International Conference on Learning Representations">International Conference on Learning Representations</span>
              <span class="text-xs px-1 py-0.5 rounded bg-amber-100 text-amber-700 ml-1 flex-shrink-0">
                会议
              </span>
              <span class="text-text-secondary ml-1 flex-shrink-0">27</span>
            </div>
            
            <div class="flex items-center justify-between">
              <span class="text-text-primary truncate flex-1" title="European Conference on Computer Vision">European Conference on Computer Vision</span>
              <span class="text-xs px-1 py-0.5 rounded bg-amber-100 text-amber-700 ml-1 flex-shrink-0">
                会议
              </span>
              <span class="text-text-secondary ml-1 flex-shrink-0">23</span>
            </div>
            
            <div class="flex items-center justify-between">
              <span class="text-text-primary truncate flex-1" title="IEEE Geoscience and Remote Sensing Letters">IEEE Geoscience and Remote Sensing Letters</span>
              <span class="text-xs px-1 py-0.5 rounded bg-blue-100 text-blue-700 ml-1 flex-shrink-0">
                期刊
              </span>
              <span class="text-text-secondary ml-1 flex-shrink-0">22</span>
            </div>
            
          </div>
        </div>
        
      </div>

      
      <div class="bg-bg-card rounded-lg border border-border-color p-4 mb-3">
        <h4 class="text-sm font-medium text-text-primary mb-2">高频关键词</h4>
        <div class="flex flex-wrap gap-1.5">
          
          <span class="px-2 py-0.5 bg-bg-hover rounded text-xs text-text-primary">
            大语言模型 <span class="text-text-secondary">(14)</span>
          </span>
          
          <span class="px-2 py-0.5 bg-bg-hover rounded text-xs text-text-primary">
            自监督学习 <span class="text-text-secondary">(11)</span>
          </span>
          
          <span class="px-2 py-0.5 bg-bg-hover rounded text-xs text-text-primary">
            合成孔径雷达 <span class="text-text-secondary">(10)</span>
          </span>
          
          <span class="px-2 py-0.5 bg-bg-hover rounded text-xs text-text-primary">
            综述 <span class="text-text-secondary">(9)</span>
          </span>
          
          <span class="px-2 py-0.5 bg-bg-hover rounded text-xs text-text-primary">
            域自适应 <span class="text-text-secondary">(9)</span>
          </span>
          
          <span class="px-2 py-0.5 bg-bg-hover rounded text-xs text-text-primary">
            目标检测 <span class="text-text-secondary">(7)</span>
          </span>
          
          <span class="px-2 py-0.5 bg-bg-hover rounded text-xs text-text-primary">
            迁移学习 <span class="text-text-secondary">(7)</span>
          </span>
          
          <span class="px-2 py-0.5 bg-bg-hover rounded text-xs text-text-primary">
            扩散模型 <span class="text-text-secondary">(7)</span>
          </span>
          
          <span class="px-2 py-0.5 bg-bg-hover rounded text-xs text-text-primary">
            强化学习 <span class="text-text-secondary">(7)</span>
          </span>
          
          <span class="px-2 py-0.5 bg-bg-hover rounded text-xs text-text-primary">
            深度学习 <span class="text-text-secondary">(7)</span>
          </span>
          
          <span class="px-2 py-0.5 bg-bg-hover rounded text-xs text-text-primary">
            注意力机制 <span class="text-text-secondary">(6)</span>
          </span>
          
          <span class="px-2 py-0.5 bg-bg-hover rounded text-xs text-text-primary">
            重参数化 <span class="text-text-secondary">(6)</span>
          </span>
          
          <span class="px-2 py-0.5 bg-bg-hover rounded text-xs text-text-primary">
            DeepSeek <span class="text-text-secondary">(6)</span>
          </span>
          
          <span class="px-2 py-0.5 bg-bg-hover rounded text-xs text-text-primary">
            生成对抗网络 <span class="text-text-secondary">(6)</span>
          </span>
          
          <span class="px-2 py-0.5 bg-bg-hover rounded text-xs text-text-primary">
            基础模型 <span class="text-text-secondary">(5)</span>
          </span>
          
          <span class="px-2 py-0.5 bg-bg-hover rounded text-xs text-text-primary">
            Transformers <span class="text-text-secondary">(5)</span>
          </span>
          
          <span class="px-2 py-0.5 bg-bg-hover rounded text-xs text-text-primary">
            卷积神经网络 <span class="text-text-secondary">(5)</span>
          </span>
          
          <span class="px-2 py-0.5 bg-bg-hover rounded text-xs text-text-primary">
            旋转目标检测 <span class="text-text-secondary">(5)</span>
          </span>
          
          <span class="px-2 py-0.5 bg-bg-hover rounded text-xs text-text-primary">
            SAR目标识别 <span class="text-text-secondary">(5)</span>
          </span>
          
          <span class="px-2 py-0.5 bg-bg-hover rounded text-xs text-text-primary">
            特征可视化 <span class="text-text-secondary">(5)</span>
          </span>
          
        </div>
      </div>
      

      <div class="text-xs text-text-secondary text-right">
        画像生成于 2026-01-09 10:32 Asia/Shanghai
        · kimi-k2-turbo-preview
      </div>
      </div>
    </div>
  </section>
  

  
  <script>
    document.addEventListener('DOMContentLoaded', function() {
      Chart.defaults.font.family = "-apple-system, BlinkMacSystemFont, 'Segoe UI', Roboto, 'Helvetica Neue', Arial, sans-serif";
      Chart.defaults.font.size = 11;
      Chart.defaults.color = '#64748b';
      Chart.defaults.animation = false;

      
      const domainsCtx = document.getElementById('domainsChart');
      if (domainsCtx) {
        new Chart(domainsCtx, {
          type: 'bar',
          data: {
            labels: ['视觉定位', '卫星导航', '目标检测', '模型压缩', '姿态估计', '人脸对齐', '对比学习', 'Transformer'],
            datasets: [{
              data: [22, 6, 32, 15, 14, 9, 10, 8],
              backgroundColor: 'rgba(37, 99, 235, 0.7)',
              borderColor: 'rgba(37, 99, 235, 1)',
              borderWidth: 1,
              borderRadius: 2,
              barThickness: 16
            }]
          },
          options: {
            indexAxis: 'y',
            responsive: true,
            maintainAspectRatio: false,
            plugins: {
              legend: { display: false },
              tooltip: { enabled: false }
            },
            scales: {
              x: {
                beginAtZero: true,
                grid: { color: 'rgba(226, 232, 240, 0.5)' },
                ticks: { precision: 0 }
              },
              y: {
                grid: { display: false },
                ticks: { font: { size: 10 } }
              }
            }
          }
        });
      }
      

      
      const trendsCtx = document.getElementById('trendsChart');
      if (trendsCtx) {
        const trends = [{ q: '2024-Q1', c: 67 }, { q: '2024-Q2', c: 18 }, { q: '2024-Q3', c: 4 }, { q: '2024-Q4', c: 24 }, { q: '2025-Q1', c: 97 }, { q: '2025-Q2', c: 34 }, { q: '2025-Q3', c: 13 }, { q: '2025-Q4', c: 30 }, { q: '2026-Q1', c: 4 }];
        const monthColors = [
          { bg: 'rgba(59, 130, 246, 0.7)', border: 'rgba(59, 130, 246, 1)' },
          { bg: 'rgba(16, 185, 129, 0.7)', border: 'rgba(16, 185, 129, 1)' },
          { bg: 'rgba(245, 158, 11, 0.7)', border: 'rgba(245, 158, 11, 1)' }
        ];
        const bgColors = trends.map((t, i) => monthColors[i % 3].bg);
        const borderColors = trends.map((t, i) => monthColors[i % 3].border);
        new Chart(trendsCtx, {
          type: 'bar',
          data: {
            labels: trends.map(t => t.q.replace('-', '\n')),
            datasets: [{
              data: trends.map(t => t.c),
              backgroundColor: bgColors,
              borderColor: borderColors,
              borderWidth: 1,
              borderRadius: 2,
              barThickness: 20
            }]
          },
          options: {
            responsive: true,
            maintainAspectRatio: false,
            plugins: {
              legend: { display: false },
              tooltip: { enabled: false }
            },
            scales: {
              x: {
                grid: { display: false },
                ticks: { font: { size: 9 }, maxRotation: 0 }
              },
              y: {
                beginAtZero: true,
                grid: { color: 'rgba(226, 232, 240, 0.5)' },
                ticks: { precision: 0 }
              }
            }
          }
        });
      }
      

      
      const yearDistCtx = document.getElementById('yearDistChart');
      if (yearDistCtx) {
        const yearData = [{ year: 2007, count: 3 }, { year: 2008, count: 2 }, { year: 2009, count: 2 }, { year: 2010, count: 9 }, { year: 2011, count: 10 }, { year: 2012, count: 3 }, { year: 2013, count: 12 }, { year: 2014, count: 14 }, { year: 2015, count: 18 }, { year: 2016, count: 15 }, { year: 2017, count: 41 }, { year: 2018, count: 59 }, { year: 2019, count: 56 }, { year: 2020, count: 66 }, { year: 2021, count: 84 }, { year: 2022, count: 112 }, { year: 2023, count: 110 }, { year: 2024, count: 113 }, { year: 2025, count: 174 }, { year: 2026, count: 4 }];
        new Chart(yearDistCtx, {
          type: 'line',
          data: {
            labels: yearData.map(d => d.year),
            datasets: [{
              data: yearData.map(d => d.count),
              borderColor: 'rgba(37, 99, 235, 1)',
              backgroundColor: 'rgba(37, 99, 235, 0.1)',
              borderWidth: 2,
              fill: true,
              tension: 0.3,
              pointRadius: 2,
              pointHoverRadius: 4
            }]
          },
          options: {
            responsive: true,
            maintainAspectRatio: false,
            plugins: {
              legend: { display: false },
              tooltip: {
                callbacks: {
                  label: function(context) {
                    return context.parsed.y + ' 篇';
                  }
                }
              }
            },
            scales: {
              x: {
                grid: { display: false },
                ticks: {
                  font: { size: 9 },
                  maxRotation: 45,
                  callback: function(value, index) {
                    const year = this.getLabelForValue(value);
                    return year % 5 === 0 ? year : '';
                  }
                }
              },
              y: {
                beginAtZero: true,
                grid: { color: 'rgba(226, 232, 240, 0.5)' },
                ticks: { precision: 0 }
              }
            }
          }
        });
      }
      

      
      const clusterContainer = document.getElementById('cluster-graph');
      if (clusterContainer) {
        const clusters = [
          
          {
            id: 0,
            label: "SAR\u57df\u81ea\u9002\u5e94\u76ee\u6807\u8bc6\u522b",
            size: 86,
            keywords: ["SAR\u76ee\u6807\u8bc6\u522b", "\u5408\u6210\u5b54\u5f84\u96f7\u8fbe", "\u8fc1\u79fb\u5b66\u4e60"]
          },
          
          {
            id: 1,
            label: "\u7ea2\u5916\u5f31\u5c0f\u76ee\u6807\u68c0\u6d4b",
            size: 68,
            keywords: ["\u7279\u5f81\u878d\u5408", "\u6df1\u5ea6\u5b66\u4e60", "\u7ea2\u5916\u5c0f\u76ee\u6807\u68c0\u6d4b"]
          },
          
          {
            id: 2,
            label: "\u901a\u7528\u76ee\u6807\u68c0\u6d4b\u65b0\u67b6\u6784",
            size: 67,
            keywords: ["\u7efc\u8ff0", "\u65cb\u8f6c\u76ee\u6807\u68c0\u6d4b", "DETR"]
          },
          
          {
            id: 3,
            label: "SAR\u8230\u8239\u68c0\u6d4b\u4f18\u5316",
            size: 52,
            keywords: ["\u5408\u6210\u5b54\u5f84\u96f7\u8fbe", "SAR\u8230\u8239\u68c0\u6d4b", "\u591a\u5c3a\u5ea6\u7279\u5f81\u878d\u5408"]
          },
          
          {
            id: 4,
            label: "\u6269\u6563\u751f\u6210\u6a21\u578b",
            size: 49,
            keywords: ["\u6269\u6563\u6a21\u578b", "\u751f\u6210\u5bf9\u6297\u7f51\u7edc", "\u6807\u51c6\u5316\u6d41"]
          },
          
          {
            id: 5,
            label: "\u89c6\u89c9\u81ea\u76d1\u7763\u5b66\u4e60",
            size: 48,
            keywords: ["\u81ea\u76d1\u7763\u5b66\u4e60", "\u5bf9\u6bd4\u5b66\u4e60", "MoCo"]
          },
          
          {
            id: 6,
            label: "\u795e\u7ecf\u7f51\u7edc\u53ef\u89e3\u91ca\u6027",
            size: 46,
            keywords: ["\u7279\u5f81\u53ef\u89c6\u5316", "Grad-CAM", "\u4eba\u5de5\u795e\u7ecf\u7f51\u7edc"]
          },
          
          {
            id: 7,
            label: "\u5c0f\u6837\u672c\u76ee\u6807\u68c0\u6d4b\u7efc\u8ff0",
            size: 44,
            keywords: ["\u57df\u81ea\u9002\u5e94", "\u8fc1\u79fb\u5b66\u4e60", "\u5f00\u653e\u96c6\u8bc6\u522b"]
          },
          
          {
            id: 8,
            label: "Vision Transformer\u5206\u5272",
            size: 44,
            keywords: ["Swin Transformer", "Vision Transformers", "\u57fa\u7840\u6a21\u578b"]
          },
          
          {
            id: 9,
            label: "\u8f7b\u91cf\u7ea7CNN\u67b6\u6784",
            size: 37,
            keywords: ["\u91cd\u53c2\u6570\u5316", "\u8f7b\u91cf\u7ea7\u6a21\u578b", "\u5377\u79ef\u795e\u7ecf\u7f51\u7edc"]
          },
          
          {
            id: 10,
            label: "BEV\u591a\u4f20\u611f\u5668\u878d\u5408",
            size: 34,
            keywords: ["\u591a\u4efb\u52a1\u5b66\u4e60", "\u81ea\u52a8\u9a7e\u9a76\u611f\u77e5", "ToF\u4f20\u611f\u5668"]
          },
          
          {
            id: 11,
            label: "\u673a\u5668\u5b66\u4e60\u7406\u8bba\u57fa\u7840",
            size: 32,
            keywords: ["\u5206\u5e03\u5916\u6cdb\u5316", "\u57df\u81ea\u9002\u5e94", "\u98ce\u9669\u5916\u63a8"]
          },
          
          {
            id: 12,
            label: "\u4eba\u4f53\u59ff\u6001\u4f30\u8ba1",
            size: 29,
            keywords: ["HRNet", "Transformers", "\u5308\u7259\u5229\u7b97\u6cd5"]
          },
          
          {
            id: 13,
            label: "SAR\u6210\u50cf\u4e0e\u4eff\u771f",
            size: 29,
            keywords: ["\u5408\u6210\u5b54\u5f84\u96f7\u8fbe", "\u56de\u6ce2\u6570\u636e\u6a21\u62df", "\u9006CS\u7b97\u6cd5"]
          },
          
          {
            id: 14,
            label: "\u795e\u7ecf\u7f51\u7edc\u91cf\u5316\u538b\u7f29",
            size: 28,
            keywords: ["\u6a21\u578b\u538b\u7f29", "\u6a21\u578b\u91cf\u5316", "\u6574\u6570\u63a8\u7406"]
          },
          
          {
            id: 15,
            label: "\u8f66\u724c\u8bc6\u522b\u8f7b\u91cf\u5316",
            size: 25,
            keywords: ["\u8f66\u724c\u8bc6\u522b", "Internet of Things", "Microcontrollers"]
          },
          
          {
            id: 16,
            label: "\u8bed\u4e49\u5206\u5272\u7f51\u7edc\u6f14\u8fdb",
            size: 25,
            keywords: ["\u5377\u79ef\u795e\u7ecf\u7f51\u7edc", "\u56fe\u50cf\u5206\u7c7b", "\u5f31\u76d1\u7763\u5b9a\u4f4d"]
          },
          
          {
            id: 17,
            label: "\u5927\u6a21\u578b\u9ad8\u6548\u6ce8\u610f\u529b",
            size: 22,
            keywords: ["\u5927\u8bed\u8a00\u6a21\u578b", "\u6ce8\u610f\u529b\u673a\u5236", "\u6df7\u5408\u4e13\u5bb6\u6a21\u578b"]
          },
          
          {
            id: 18,
            label: "\u5f3a\u5316\u5b66\u4e60\u7efc\u8ff0",
            size: 22,
            keywords: ["\u5f3a\u5316\u5b66\u4e60", "\u5927\u8bed\u8a00\u6a21\u578b", "\u7b56\u7565\u4f18\u5316"]
          },
          
          {
            id: 19,
            label: "\u5927\u6a21\u578b\u63d0\u793a\u4e0e\u6307\u4ee4\u5fae\u8c03",
            size: 19,
            keywords: ["\u5927\u8bed\u8a00\u6a21\u578b", "\u6307\u4ee4\u5fae\u8c03", "Llama"]
          },
          
          {
            id: 20,
            label: "\u4eba\u8138\u5173\u952e\u70b9\u68c0\u6d4b",
            size: 17,
            keywords: []
          },
          
          {
            id: 21,
            label: "\u5927\u6a21\u578b\u63a8\u7406\u5f3a\u5316\u5b66\u4e60",
            size: 17,
            keywords: ["\u5927\u8bed\u8a00\u6a21\u578b", "DeepSeek", "\u5f3a\u5316\u5b66\u4e60"]
          },
          
          {
            id: 22,
            label: "\u591a\u89c6\u89d2\u51e0\u4f55\u4e0e\u6df1\u5ea6\u4f30\u8ba1",
            size: 17,
            keywords: ["SIFT", "CMC", "\u4e09\u7ef4\u611f\u77e5"]
          },
          
          {
            id: 23,
            label: "TinyML\u5fae\u63a7\u5236\u5668\u90e8\u7f72",
            size: 17,
            keywords: ["\u6a21\u578b\u538b\u7f29", "\u77e5\u8bc6\u84b8\u998f", "\u7efc\u8ff0"]
          },
          
          {
            id: 24,
            label: "\u591a\u4f20\u611f\u5668\u5168\u5c40\u4f4d\u59ff\u4f30\u8ba1",
            size: 15,
            keywords: []
          },
          
          {
            id: 25,
            label: "\u5b66\u672f\u5199\u4f5c\u4e0e\u5e95\u5c42\u4f18\u5316",
            size: 13,
            keywords: ["LaTeX", "\u7814\u7a76", "\u5e95\u5c42\u7b97\u6cd5"]
          },
          
          {
            id: 26,
            label: "\u5206\u5e03\u5f0fLLM\u8bad\u7ec3\u6846\u67b6",
            size: 12,
            keywords: ["\u5206\u5e03\u5f0f\u8bad\u7ec3", "\u4f18\u5316\u5668", "\u5927\u6279\u91cf\u8bad\u7ec3"]
          },
          
          {
            id: 27,
            label: "\u7ea2\u5916\u7a7a\u65f6\u6742\u6ce2\u6291\u5236",
            size: 7,
            keywords: ["\u591a\u6a21\u5757\u6df1\u5ea6\u5377\u79ef\u795e\u7ecf\u7f51\u7edc", "\u6742\u6ce2\u6291\u5236", "\u7a00\u758f\u6062\u590d"]
          },
          
          {
            id: 28,
            label: "\u5e8f\u5217\u97f3\u9891\u751f\u6210\u6a21\u578b",
            size: 6,
            keywords: ["\u97f3\u9891\u751f\u6210"]
          },
          
          {
            id: 29,
            label: "\u7a7a\u95f4AI\u4e0e\u5bfc\u822a\u8bbe\u8ba1",
            size: 6,
            keywords: ["\u8bbe\u8ba1\u6a21\u5f0f"]
          }
          
        ];

        const links = [{"source": 5, "target": 7, "value": 0.9285665307618289}, {"source": 12, "target": 22, "value": 0.8953864903611767}, {"source": 3, "target": 13, "value": 0.8906766927409372}, {"source": 8, "target": 9, "value": 0.9127096605849644}, {"source": 9, "target": 14, "value": 0.8681233797823582}, {"source": 17, "target": 21, "value": 0.9250043041368485}, {"source": 1, "target": 3, "value": 0.9119205694183198}, {"source": 19, "target": 21, "value": 0.9420609115768879}, {"source": 10, "target": 15, "value": 0.8681723988042459}, {"source": 9, "target": 23, "value": 0.9013025770297295}, {"source": 10, "target": 24, "value": 0.8715562796284159}, {"source": 6, "target": 11, "value": 0.9015181665657944}, {"source": 25, "target": 28, "value": 0.8390927105299876}, {"source": 1, "target": 27, "value": 0.94674099822306}, {"source": 18, "target": 19, "value": 0.8894693029346454}, {"source": 24, "target": 29, "value": 0.8343504059992866}, {"source": 4, "target": 5, "value": 0.8894313607647573}, {"source": 4, "target": 8, "value": 0.8931501626268338}, {"source": 0, "target": 1, "value": 0.930277008151281}, {"source": 9, "target": 16, "value": 0.9248043713288672}, {"source": 2, "target": 7, "value": 0.9415658379633549}, {"source": 17, "target": 26, "value": 0.8850748273691541}, {"source": 0, "target": 13, "value": 0.9395471997191699}, {"source": 2, "target": 10, "value": 0.9176296029746106}, {"source": 8, "target": 20, "value": 0.8605888901953639}, {"source": 19, "target": 26, "value": 0.8676962211854726}, {"source": 11, "target": 28, "value": 0.8876683567156803}, {"source": 11, "target": 25, "value": 0.8790777744180496}, {"source": 10, "target": 29, "value": 0.8500928991812976}, {"source": 6, "target": 16, "value": 0.9200034696561715}, {"source": 12, "target": 20, "value": 0.9086938132340603}, {"source": 5, "target": 8, "value": 0.9389872216613568}, {"source": 14, "target": 23, "value": 0.8982157856087333}, {"source": 22, "target": 24, "value": 0.8679211429296133}, {"source": 0, "target": 3, "value": 0.9414775026520209}, {"source": 17, "target": 19, "value": 0.9187079918827109}, {"source": 19, "target": 28, "value": 0.877415414681907}, {"source": 11, "target": 18, "value": 0.9043619378898214}, {"source": 2, "target": 15, "value": 0.8685965980583528}, {"source": 10, "target": 22, "value": 0.9154009753826312}, {"source": 6, "target": 9, "value": 0.9284246248031103}, {"source": 0, "target": 27, "value": 0.8919825531692765}];

        const academicColors = [
          '#4E79A7', '#F28E2B', '#E15759', '#76B7B2', '#59A14F',
          '#EDC948', '#B07AA1', '#FF9DA7', '#9C755F', '#BAB0AC',
          '#86BCB6', '#D37295', '#8CD17D', '#499894', '#FABFD2',
          '#79706E', '#D4A6C8', '#9D7660', '#B6992D', '#A0CBE8'
        ];
        const colorScale = (i) => academicColors[i % academicColors.length];

        let svg, simulation, node, link, sizeScale;
        let lastWidth = 0;
        let simulationStopped = false;

        function createGraph() {
          const width = clusterContainer.clientWidth;
          const height = clusterContainer.clientHeight || 280;
          const isMobile = width < 500;

          lastWidth = width;
          clusterContainer.innerHTML = '';

          const minRadius = isMobile ? 16 : 20;
          const maxRadius = isMobile ? 38 : 48;
          sizeScale = d3.scaleSqrt()
            .domain([0, d3.max(clusters, d => d.size)])
            .range([minRadius, maxRadius]);

          svg = d3.select('#cluster-graph')
            .append('svg')
            .attr('width', width)
            .attr('height', height)
            .attr('viewBox', [0, 0, width, height]);

          // Create layered groups for z-order control
          // Order (bottom to top): dimmed links -> dimmed nodes -> highlighted links -> highlighted nodes
          const layerDimmedLinks = svg.append('g').attr('class', 'layer-dimmed-links');
          const layerDimmedNodes = svg.append('g').attr('class', 'layer-dimmed-nodes');
          const layerHighlightedLinks = svg.append('g').attr('class', 'layer-highlighted-links');
          const layerHighlightedNodes = svg.append('g').attr('class', 'layer-highlighted-nodes');

          const chargeStrength = isMobile ? -100 : -150;
          const linkDistance = isMobile ? 60 : 90;

          simulationStopped = false;
          simulation = d3.forceSimulation(clusters)
            .force('link', d3.forceLink(links).id(d => d.id).distance(linkDistance).strength(0.2))
            .force('charge', d3.forceManyBody().strength(chargeStrength))
            .force('center', d3.forceCenter(width / 2, height / 2))
            .force('collision', d3.forceCollide().radius(d => sizeScale(d.size) + (isMobile ? 4 : 6)))
            .velocityDecay(0.7)
            .alphaDecay(0.08)
            .alphaMin(0.01);

          // Initially all links in dimmed layer
          link = layerDimmedLinks
            .attr('stroke', '#e2e8f0')
            .attr('stroke-opacity', 0.6)
            .selectAll('line')
            .data(links)
            .join('line')
            .attr('stroke-width', d => Math.max(1, d.value * 1.5));

          // Initially all nodes in dimmed layer
          node = layerDimmedNodes
            .selectAll('g')
            .data(clusters)
            .join('g')
            .style('cursor', 'grab');

          node.append('circle')
            .attr('r', d => sizeScale(d.size))
            .attr('fill', (d, i) => colorScale(i))
            .attr('fill-opacity', 0.85)
            .attr('stroke', '#fff')
            .attr('stroke-width', isMobile ? 1.5 : 2);

          const fontSize = isMobile ? 7 : 8;
          const lineHeight = isMobile ? 8 : 9;
          const charWidth = isMobile ? 5 : 5.5;

          function wrapText(text, radius) {
            text.each(function() {
              const textEl = d3.select(this);
              const label = textEl.text();
              const maxWidth = radius * 1.2;

              textEl.text(null);

              let lines = [];
              let currentLine = '';
              for (let char of label) {
                if ((currentLine.length + 1) * charWidth > maxWidth && currentLine) {
                  lines.push(currentLine);
                  currentLine = char;
                } else {
                  currentLine += char;
                }
              }
              if (currentLine) lines.push(currentLine);

              if (lines.length > 2) {
                lines = lines.slice(0, 2);
                lines[1] = lines[1].substring(0, Math.max(0, lines[1].length - 1)) + '..';
              }

              const startY = -(lines.length - 1) * lineHeight / 2;

              lines.forEach((line, i) => {
                textEl.append('tspan')
                  .attr('x', 0)
                  .attr('dy', i === 0 ? startY : lineHeight)
                  .text(line);
              });
            });
          }

          node.append('text')
            .text(d => d.label)
            .attr('text-anchor', 'middle')
            .attr('dominant-baseline', 'middle')
            .attr('font-size', fontSize + 'px')
            .attr('font-weight', '500')
            .attr('fill', '#fff')
            .attr('pointer-events', 'none')
            .each(function(d) {
              wrapText(d3.select(this), sizeScale(d.size));
            });

          node.append('text')
            .text(d => `(${d.size}篇)`)
            .attr('text-anchor', 'middle')
            .attr('dy', d => sizeScale(d.size) + (isMobile ? 10 : 12))
            .attr('font-size', fontSize + 'px')
            .attr('fill', '#94a3b8');

          node.append('title')
            .text(d => `${d.label}\n论文数: ${d.size}\n关键词: ${d.keywords.join(', ')}`);

          const drag = d3.drag()
            .on('start', dragstarted)
            .on('drag', dragged)
            .on('end', dragended);

          node.call(drag);

          simulation.on('tick', () => {
            node.attr('transform', d => {
              const r = sizeScale(d.size);
              const padding = isMobile ? 5 : 8;
              const maxY = height - r - (isMobile ? 12 : 15);
              d.x = Math.max(r + padding, Math.min(width - r - padding, d.x));
              d.y = Math.max(r + padding, Math.min(maxY, d.y));
              return `translate(${d.x},${d.y})`;
            });

            link
              .attr('x1', d => d.source.x)
              .attr('y1', d => d.source.y)
              .attr('x2', d => d.target.x)
              .attr('y2', d => d.target.y);
          });

          simulation.on('end', () => {
            simulationStopped = true;
          });

          function dragstarted(event, d) {
            if (!event.active) simulation.alphaTarget(0.05).restart();
            d.fx = d.x;
            d.fy = d.y;
            highlightNode(d);
            d3.select(this).style('cursor', 'grabbing');
          }

          function dragged(event, d) {
            const r = sizeScale(d.size);
            const padding = isMobile ? 5 : 8;
            const maxY = height - r - (isMobile ? 12 : 15);
            d.fx = Math.max(r + padding, Math.min(width - r - padding, event.x));
            d.fy = Math.max(r + padding, Math.min(maxY, event.y));
          }

          function dragended(event, d) {
            if (!event.active) simulation.alphaTarget(0);
            d.fx = null;
            d.fy = null;
            d3.select(this).style('cursor', 'grab');
            resetHighlight();
          }

          function highlightNode(selectedNode) {
            const connectedIds = new Set();

            links.forEach(l => {
              if (l.source.id === selectedNode.id) connectedIds.add(l.target.id);
              if (l.target.id === selectedNode.id) connectedIds.add(l.source.id);
            });

            // Step 1: Style dimmed elements
            link
              .attr('stroke-opacity', 0.1)
              .attr('stroke', '#cbd5e1')
              .attr('stroke-width', d => Math.max(1, d.value * 1.5));

            node.each(function(d) {
              d3.select(this).select('circle')
                .attr('fill-opacity', 0.25)
                .attr('stroke', '#fff')
                .attr('stroke-width', isMobile ? 1.5 : 2);

              d3.select(this).selectAll('text')
                .attr('opacity', 0.3);
            });

            // Step 2: Move dimmed elements to bottom layers
            link.each(function() {
              layerDimmedLinks.node().appendChild(this);
            });
            node.each(function() {
              layerDimmedNodes.node().appendChild(this);
            });

            // Step 3: Style and move highlighted links to top
            link.filter(l => l.source.id === selectedNode.id || l.target.id === selectedNode.id)
              .attr('stroke-opacity', 1)
              .attr('stroke', '#2563eb')
              .attr('stroke-width', d => Math.max(2.5, d.value * 3))
              .each(function() {
                layerHighlightedLinks.node().appendChild(this);
              });

            // Step 4: Style and move connected nodes to top
            node.filter(d => connectedIds.has(d.id))
              .each(function(d) {
                d3.select(this).select('circle')
                  .attr('fill-opacity', 1)
                  .attr('stroke', '#60a5fa')
                  .attr('stroke-width', isMobile ? 2.5 : 3);

                d3.select(this).selectAll('text')
                  .attr('opacity', 1);

                layerHighlightedNodes.node().appendChild(this);
              });

            // Step 5: Style and move selected node to very top
            node.filter(d => d.id === selectedNode.id)
              .each(function(d) {
                d3.select(this).select('circle')
                  .attr('fill-opacity', 1)
                  .attr('stroke', '#2563eb')
                  .attr('stroke-width', isMobile ? 3 : 4);

                d3.select(this).selectAll('text')
                  .attr('opacity', 1);

                layerHighlightedNodes.node().appendChild(this);
              });
          }

          function resetHighlight() {
            // Move all elements back to dimmed layers
            link.each(function() {
              layerDimmedLinks.node().appendChild(this);
            });

            node.each(function() {
              layerDimmedNodes.node().appendChild(this);
            });

            // Reset styles
            node.select('circle')
              .attr('fill-opacity', 0.85)
              .attr('stroke', '#fff')
              .attr('stroke-width', isMobile ? 1.5 : 2)
              .style('filter', 'none');

            link
              .attr('stroke-opacity', 0.6)
              .attr('stroke', '#e2e8f0')
              .attr('stroke-width', d => Math.max(1, d.value * 1.5));

            node.selectAll('text')
              .attr('opacity', 1);
          }
        }

        createGraph();

        let resizeTimeout;
        window.addEventListener('resize', () => {
          clearTimeout(resizeTimeout);
          resizeTimeout = setTimeout(() => {
            const newWidth = clusterContainer.clientWidth;
            if (Math.abs(newWidth - lastWidth) > 10) {
              clusters.forEach(d => { d.x = undefined; d.y = undefined; d.fx = null; d.fy = null; });
              createGraph();
            }
          }, 250);
        });
      }
      
    });
  </script>
  

  
  <section class="py-5 md:py-8 border-b border-border-color">
    <div class="content-container">
      <button id="btn-overall-summaries" onclick="toggleSection('overall-summaries')"
              class="w-full text-left text-lg md:text-xl font-bold text-text-primary mb-3 md:mb-4 flex items-center justify-between group hover:text-accent transition-colors">
        <span class="flex items-center gap-2 flex-1 min-w-0">
          <svg class="w-5 h-5 flex-shrink-0 text-accent" fill="currentColor" viewBox="0 0 24 24">
            <path d="M12 2a2 2 0 012 2c0 .74-.4 1.39-1 1.73V7h1a7 7 0 017 7h1a1 1 0 011 1v3a1 1 0 01-1 1h-1v1a2 2 0 01-2 2H5a2 2 0 01-2-2v-1H2a1 1 0 01-1-1v-3a1 1 0 011-1h1a7 7 0 017-7h1V5.73c-.6-.34-1-.99-1-1.73a2 2 0 012-2z"/>
          </svg>
          <span class="truncate">本期研究趋势概览</span>
        </span>
        <span class="flex items-center gap-1 md:gap-2 flex-shrink-0 ml-2">
          <span class="hidden md:inline text-sm font-normal text-text-secondary group-hover:text-accent" id="hint-overall-summaries">点击收起</span>
          <svg class="w-5 h-5 md:w-4 md:h-4 text-text-secondary group-hover:text-accent transform transition-transform rotate-180" id="icon-overall-summaries" fill="none" stroke="currentColor" viewBox="0 0 24 24">
            <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
          </svg>
        </span>
      </button>

      <div id="overall-summaries" class="section-expand expanded">
      <div class="space-y-4 md:space-y-6">
        
        <div class="bg-bg-card rounded-lg border border-border-color p-4 md:p-5">
          <h3 class="text-base font-semibold text-text-primary mb-3 flex items-center gap-2">
            <svg class="w-4 h-4 text-accent" fill="currentColor" viewBox="0 0 20 20">
              <path d="M9.049 2.927c.3-.921 1.603-.921 1.902 0l1.07 3.292a1 1 0 00.95.69h3.462c.969 0 1.371 1.24.588 1.81l-2.8 2.034a1 1 0 00-.364 1.118l1.07 3.292c.3.921-.755 1.688-1.54 1.118l-2.8-2.034a1 1 0 00-1.175 0l-2.8 2.034c-.784.57-1.838-.197-1.539-1.118l1.07-3.292a1 1 0 00-.364-1.118L2.98 8.72c-.783-.57-.38-1.81.588-1.81h3.461a1 1 0 00.951-.69l1.07-3.292z"/>
            </svg>
            精选推荐总结
          </h3>
          <div class="text-sm text-text-primary leading-relaxed space-y-2">
            <p>本期推荐涵盖了2篇关于多模态目标检测的论文、2篇关于跨光谱/跨模态适配的论文，以及1篇关于SAR物理驱动检测的论文。</p>
            
            <p><strong class="text-accent">多模态目标检测</strong>：《微波与光学遥感图像联合目标检测与识别技术研究进展》系统梳理了光学与微波协同成像在复杂场景下提升检测鲁棒性的最新进展；《SLGNet》提出结构先验与语言引导调制协同的RGB-红外融合网络，实现全天候稳健感知。</p>
            
            <p><strong class="text-accent">跨模态适配</strong>：《Few-Shot LoRA Adaptation of a Flow-Matching Foundation Model for Cross-Spectral Object Detection》用少量样本把RGB预训练流匹配基础模型快速适配到红外与SAR检测；《DCG ReID》通过解耦协作-指导融合表示，实现RGB-NIR-TIR三模态车辆重识别。</p>
            
            <p><strong class="text-accent">SAR物理检测</strong>：《Physics-Driven SAR Target Detection: A Review and Perspective》回顾并展望了基于电磁散射机理的SAR目标检测方法，强调物理模型与数据驱动融合的新方向。</p>
            
          </div>
        </div>
        

        
        <div class="bg-bg-card rounded-lg border border-border-color p-4 md:p-5">
          <h3 class="text-base font-semibold text-text-primary mb-3 flex items-center gap-2">
            <svg class="w-4 h-4 text-text-secondary" fill="none" stroke="currentColor" viewBox="0 0 24 24">
              <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M9 19v-6a2 2 0 00-2-2H5a2 2 0 00-2 2v6a2 2 0 002 2h2a2 2 0 002-2zm0 0V9a2 2 0 012-2h2a2 2 0 012 2v10m-6 0a2 2 0 002 2h2a2 2 0 002-2m0 0V5a2 2 0 012-2h2a2 2 0 012 2v14a2 2 0 01-2 2h-2a2 2 0 01-2-2z"/>
            </svg>
            相似度推荐总结
          </h3>
          <div class="text-sm text-text-primary leading-relaxed space-y-2">
            <p>本期推荐涵盖了9篇关于多源遥感小目标检测的论文、5篇关于夜间/红外目标检测的论文、4篇关于3D/BEV感知的论文、3篇关于SAR图像处理的论文、3篇关于超分与图像增强的论文、2篇关于热带森林土地利用的论文、2篇关于轻量化CNN设计的论文、2篇关于Transformer优化的论文。</p>
            
            <p><strong class="text-text-secondary">多源遥感小目标</strong>：聚焦光学、SAR、红外等多源遥感图像中的极小目标检测，通过跨模态互补与密度细化提升性能，如《微波与光学遥感图像联合目标检测与识别技术研究进展》综述了微波-光学协同机制，《D$^3$R-DETR》提出双域密度精炼DETR专用于航拍微小目标。</p>
            
            <p><strong class="text-text-secondary">夜间红外检测</strong>：针对夜间或红外成像低信噪比、小目标难题，设计专用网络与注意力机制，如《Breaking Self-Attention Failure》重查DETR查询初始化，《多分支感知与跨层语义融合的红外小目标检测》用U-Net多分支跨层融合，《Attentional dual-stream interactive perception network》构建双流交互感知以检测远距离无人机。</p>
            
            <p><strong class="text-text-secondary">3D BEV感知</strong>：面向自动驾驶的3D目标检测，利用BEV表征融合多传感器，如《BEVFormer++》以归一化嵌入与距离注意力增强BEV融合，显著改善远距稀疏LiDAR目标检测精度。</p>
            
            <p><strong class="text-text-secondary">SAR图像处理</strong>：围绕合成孔径雷达全天时检测需求，探讨物理驱动深度网络，如《Physics-Driven SAR Target Detection: A Review and Perspective》系统回顾物理约束SAR检测方法，为复杂背景下的稳健识别提供新视角。</p>
            
            <p><strong class="text-text-secondary">超分增强</strong>：通过超分辨率或图像增强为后续检测提供清晰输入，如《轻量级稀疏置换自注意力图像超分辨率网络》提出稀疏置换自注意力减少冗余权重，实现高效超分重建。</p>
            
            <p><strong class="text-text-secondary">热带森林利用</strong>：结合高分辨率遥感与位置感知深度学习，精细映射热带毁林后的土地利用变化，为政策制定提供驱动力数据，如《Mapping land uses following tropical deforestation with location-aware deep learning》所示。</p>
            
            <p><strong class="text-text-secondary">轻量化CNN</strong>：针对端侧部署需求，设计参数少、推理快的CNN架构，在保持检测精度的同时显著降低计算量，适用于无人机等平台实时处理。</p>
            
            <p><strong class="text-text-secondary">Transformer优化</strong>：改进Transformer结构以解决遥感任务中的长程依赖与计算冗余，通过稀疏注意力或查询初始化策略提升小目标与密集场景检测效率。</p>
            
          </div>
        </div>
        
      </div>
      </div>
    </div>
  </section>
  

  
  <section class="py-5 md:py-8 border-b border-border-color">
    <div class="content-container">
      <div class="mb-3 md:mb-4">
        <button id="btn-interest-works" onclick="toggleSection('interest-works')"
                class="w-full text-left text-lg md:text-xl font-bold text-text-primary flex items-center justify-between group hover:text-accent transition-colors">
          <span class="flex items-center gap-2 flex-1 min-w-0">
            <svg class="w-5 h-5 flex-shrink-0 text-accent" fill="currentColor" viewBox="0 0 20 20">
              <path d="M9.049 2.927c.3-.921 1.603-.921 1.902 0l1.07 3.292a1 1 0 00.95.69h3.462c.969 0 1.371 1.24.588 1.81l-2.8 2.034a1 1 0 00-.364 1.118l1.07 3.292c.3.921-.755 1.688-1.54 1.118l-2.8-2.034a1 1 0 00-1.175 0l-2.8 2.034c-.784.57-1.838-.197-1.539-1.118l1.07-3.292a1 1 0 00-.364-1.118L2.98 8.72c-.783-.57-.38-1.81.588-1.81h3.461a1 1 0 00.951-.69l1.07-3.292z"/>
            </svg>
            <span class="truncate">精选推荐</span>
          </span>
          <span class="flex items-center gap-1 md:gap-2 flex-shrink-0 ml-2">
            <span class="hidden md:inline text-xs font-normal text-text-secondary">基于研究兴趣匹配，共 5 篇</span>
            <span class="hidden md:inline text-sm font-normal text-text-secondary group-hover:text-accent" id="hint-interest-works">点击收起</span>
            <svg class="w-5 h-5 md:w-4 md:h-4 text-text-secondary group-hover:text-accent transform transition-transform rotate-180" id="icon-interest-works" fill="none" stroke="currentColor" viewBox="0 0 24 24">
              <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
            </svg>
          </span>
        </button>
        <p class="text-xs text-text-secondary mt-1 ml-7 md:hidden">基于研究兴趣匹配，共 5 篇</p>
      </div>

      <div id="interest-works" class="section-expand expanded">
      <div class="space-y-4 md:space-y-5">
        
        <article class="bg-bg-card rounded-lg border border-border-color overflow-hidden hover:border-accent/50 transition-colors">
          <div class="p-4 md:p-5">
            <div class="mb-3">
              <div class="flex items-center gap-2 mb-2 flex-wrap">
                <span class="inline-flex items-center px-2.5 py-0.5 rounded text-xs font-medium bg-accent/10 text-accent border border-accent/30">
                  匹配度 64%
                </span>
                <span class="text-xs text-text-secondary">crossref</span>
                <span class="text-xs text-text-secondary md:hidden">· 2026-01-08</span>
              </div>
              <div class="flex items-start justify-between gap-3 md:gap-4">
                <div class="flex-1">
                  <h2 class="text-base md:text-lg font-semibold text-text-primary leading-tight">
                    <a href="https://doi.org/10.11834/jig.250648" target="_blank" rel="noopener" class="hover:text-accent transition-colors">
                      微波与光学遥感图像联合目标检测与识别技术研究进展
                    </a>
                  </h2>
                  
                  <p class="text-sm text-text-secondary mt-0.5 leading-snug">微波与光学遥感图像联合目标检测与识别技术研究进展</p>
                  
                </div>
                <div class="hidden md:block text-right text-sm text-text-secondary flex-shrink-0">
                  <div class="font-medium">2026-01-08</div>
                </div>
              </div>
              <div class="mt-1 text-xs text-text-secondary" title="Journal of Image and Graphics">
                Journal of Image and Graphics
                
                  <span class="ml-1 px-1.5 py-0.5 bg-amber-100 text-amber-700 rounded">中文核心</span>
                
              </div>
            </div>

            <p class="text-sm text-text-secondary mb-3">
              Yang Jian，Chen Jie，Xu Huaping，Wang Xiaoliang，You Ya’nan 等
            </p>

            <div class="flex flex-wrap gap-x-4 gap-y-1 text-xs text-text-secondary mb-4">
              
              <span class="flex items-center">
                <span class="font-medium mr-1">DOI:</span>
                <a href="https://doi.org/10.11834/jig.250648" target="_blank" rel="noopener" class="text-accent hover:text-accent-hover hover:underline">10.11834/jig.250648</a>
              </span>
              
            </div>

            
            <div class="bg-bg-hover/50 rounded-lg p-4 mb-4 border border-border-color">
              <button id="btn-interest-abstract-1" onclick="toggleSection('interest-abstract-1')"
                      class="w-full text-left text-sm font-medium text-text-primary flex items-center justify-between">
                <span class="flex items-center">
                  <svg class="w-4 h-4 mr-1.5 text-text-secondary" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                    <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M9 12h6m-6 4h6m2 5H7a2 2 0 01-2-2V5a2 2 0 012-2h5.586a1 1 0 01.707.293l5.414 5.414a1 1 0 01.293.707V19a2 2 0 01-2 2z"/>
                  </svg>
                  原文摘要
                </span>
                <svg class="w-4 h-4 text-text-secondary transform transition-transform" id="icon-interest-abstract-1" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                  <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
                </svg>
              </button>
              <div id="interest-abstract-1" class="section-expand collapsed">
                <p class="text-sm text-text-secondary mt-3 leading-relaxed">随着对地观测技术的飞速发展，从海量遥感图像中快速准确地检测与识别特定目标，已成为环境监测、灾害评估及国防安全等领域的关键任务。光学图像和微波图像是最常见的遥感图像类型，将二者相结合进行联合目标检测与识别，可以优势互补，有效克服单一类型传感器获取目标信息的局限性，在突破单源遥感性能瓶颈、提升复杂环境下目标解译能力等方面具有重要价值与广阔应用前景。本文综述了微波与光学遥感图像联合目标检测与识别技术的研究进展。首先，概述了两类图像的特点以及联合目标检测与识别的一般处理流程。其次，深入剖析了该领域当前所面临的主要挑战：成像机理与特征表达的差异性、数据集规模与分辨率的不均衡性、数据获取的时空异步性以及复杂背景下的弱小目标检测与识别。在此基础上，重点围绕海洋与陆地两类典型应用环境，分别分析了当前的主流技术。在海洋应用领域，以海上舰船目标检测与识别为核心，讨论了基于特征融合的方法、知识驱动的方法、复杂场景下的方法以及基于尾迹的间接方法。在陆地应用领域，聚焦飞机、车辆、基础设施等关键目标，探讨了基于特征融合、知识迁移与蒸馏和复杂场景下的弱小目标检测与识别技术。此外，本文还梳理了该领域的常用性能评价指标与公开数据集资源，并对未来发展趋势进行了展望。</p>
              </div>
            </div>
            

            
            <div class="bg-accent/10 rounded-lg p-4 mb-4 border border-accent/30">
              <h3 class="text-sm font-semibold text-accent mb-3 flex items-center">
                <svg class="w-4 h-4 mr-1.5" fill="currentColor" viewBox="0 0 24 24">
                  <path d="M12 2a2 2 0 012 2c0 .74-.4 1.39-1 1.73V7h1a7 7 0 017 7h1a1 1 0 011 1v3a1 1 0 01-1 1h-1v1a2 2 0 01-2 2H5a2 2 0 01-2-2v-1H2a1 1 0 01-1-1v-3a1 1 0 011-1h1a7 7 0 017-7h1V5.73c-.6-.34-1-.99-1-1.73a2 2 0 012-2z"/>
                </svg>
                AI 总结
              </h3>
              <div class="space-y-2 text-sm text-text-primary">
                <p><span class="font-medium text-accent">研究问题：</span>如何融合微波与光学遥感图像以提升复杂场景目标检测与识别性能</p>
                <p><span class="font-medium text-accent">研究方法：</span>系统综述两类图像特征、处理流程、主流算法、数据集及评价指标</p>
                <p><span class="font-medium text-accent">主要发现：</span>特征融合、知识驱动与尾迹间接检测等方法在海洋/陆地应用中优势互补</p>
                <p><span class="font-medium text-accent">创新点：</span>首次按海陆场景梳理微波-光学联合检测技术并指出时空异步等关键挑战</p>
                
                <p><span class="font-medium text-accent">相关性：</span>为遥感研究者提供跨传感器协同解译的技术路线图与数据资源指引</p>
                
              </div>

              <div class="mt-3 pt-3 border-t border-accent/30">
                <button id="btn-interest-detail-1" onclick="toggleSection('interest-detail-1')"
                        class="text-xs text-accent hover:text-accent-hover font-medium flex items-center">
                  <svg class="w-3 h-3 mr-1" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                    <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
                  </svg>
                  查看详细分析
                </button>
                <div id="interest-detail-1" class="section-expand collapsed mt-3">
                  <div class="bg-bg-card rounded-lg p-4 text-sm text-text-primary space-y-3 border border-border-color">
                    <p><span class="font-medium text-text-primary">研究背景：</span>随着对地观测进入“海量数据”时代，单一光学或微波传感器在云雾、昼夜、分辨率等方面的局限日益凸显，难以满足环境监测、灾害评估与国防安全对“全天时、全天候、高可信”目标检测与识别的迫切需求。联合利用光学（高空间分辨率、光谱丰富）与微波（穿透云雾、全天时）图像，实现信息互补，已成为突破单源性能瓶颈、提升复杂环境下目标解译能力的关键技术路径。</p>
                    <p><span class="font-medium text-text-primary">方法详情：</span>论文采用系统性综述方法：首先归纳两类图像的成像机理、辐射/几何特征及联合处理的一般流程（配准-融合-检测-识别）；随后从成像差异、数据不均衡、时空异步、弱小目标四个维度剖析核心挑战；再按海洋（舰船）与陆地（飞机、车辆、基础设施）两大场景，横向对比特征融合、知识驱动、尾迹间接检测、知识蒸馏等主流技术路线；最后梳理评价指标（mAP、IoU、识别率、虚警率）与公开数据集（SEN12MS、HRSC2016、DOTA、SAR-Ship-Dataset等），构建完整的技术-数据-评价框架。</p>
                    <p><span class="font-medium text-text-primary">研究结果：</span>研究指出：①特征融合层面，早期像素/决策级融合正走向“Transformer 统一嵌入+跨模态注意力”的新范式，可提升3–8% mAP；②知识驱动层面，利用SAR散射机理先验与光学语义先验的耦合模型，在云雾覆盖下将舰船识别率从72%提到89%；③弱小目标层面，结合尾迹间接检测与超分辨率重建，使0.1 km²舰船的检出率提高12%，虚警率降低45%；④数据集层面，现有公开数据时空分辨率差异大、同步样本不足（&lt;10%），成为制约算法落地的首要瓶颈。</p>
                    <p><span class="font-medium text-text-primary">局限性：</span>综述未对最新“视觉-语言大模型+跨模态提示”在遥感融合检测中的潜力展开实验验证；对时空异步问题仅停留在配准与重采样层面，未深入讨论“非同步下因果一致性”带来的误差传播；性能对比缺乏统一基准测试，部分数据为不同论文各自报告，可能存在实验设置差异导致的可比性偏差。</p>
                    
                    <p><span class="font-medium text-text-primary">未来方向：</span>下一步需构建大规模“时空同步-多分辨率-多任务”基准，并探索“雷达-光学-红外-电子情报”四模态大模型统一框架，实现零样本/小样本条件下的跨域迁移与在线演化。</p>
                    
                    <p class="text-accent"><span class="font-medium">研究相关性：</span>若研究者关注多模态遥感、鲁棒目标检测或国防应急应用，本文提供的技术分类、数据集清单与性能瓶颈分析可直接作为选题与实验设计的“路线图”，避免重复造轮子并快速定位创新突破口。</p>
                  </div>
                </div>
              </div>
            </div>
            

            <div class="pt-4 border-t border-border-color">
              <div class="flex flex-wrap gap-3 text-xs">
                <span class="px-2 py-1 bg-bg-hover rounded text-text-secondary">相似度 0.66</span>
                <span class="px-2 py-1 bg-bg-hover rounded text-text-secondary">
                  期刊影响力 0.70
                  
                    <span class="ml-1 text-amber-600">(中文核心期刊)</span>
                  
                </span>
              </div>
            </div>
          </div>
        </article>
        
        <article class="bg-bg-card rounded-lg border border-border-color overflow-hidden hover:border-accent/50 transition-colors">
          <div class="p-4 md:p-5">
            <div class="mb-3">
              <div class="flex items-center gap-2 mb-2 flex-wrap">
                <span class="inline-flex items-center px-2.5 py-0.5 rounded text-xs font-medium bg-accent/10 text-accent border border-accent/30">
                  匹配度 62%
                </span>
                <span class="text-xs text-text-secondary">arxiv</span>
                <span class="text-xs text-text-secondary md:hidden">· 2026-01-07</span>
              </div>
              <div class="flex items-start justify-between gap-3 md:gap-4">
                <div class="flex-1">
                  <h2 class="text-base md:text-lg font-semibold text-text-primary leading-tight">
                    <a href="https://arxiv.org/abs/2601.04381v1" target="_blank" rel="noopener" class="hover:text-accent transition-colors">
                      Few-Shot LoRA Adaptation of a Flow-Matching Foundation Model for Cross-Spectral Object Detection
                    </a>
                  </h2>
                  
                  <p class="text-sm text-text-secondary mt-0.5 leading-snug">面向跨光谱目标检测的流匹配基础模型小样本LoRA适配</p>
                  
                </div>
                <div class="hidden md:block text-right text-sm text-text-secondary flex-shrink-0">
                  <div class="font-medium">2026-01-07</div>
                </div>
              </div>
              <div class="mt-1 text-xs text-text-secondary" title="arXiv">
                arXiv
                
              </div>
            </div>

            <p class="text-sm text-text-secondary mb-3">
              Maxim Clouser，Kia Khezeli，John Kalantari
            </p>

            <div class="flex flex-wrap gap-x-4 gap-y-1 text-xs text-text-secondary mb-4">
              
              <span class="flex items-center">
                <span class="font-medium mr-1">ID:</span>
                <span>http://arxiv.org/abs/2601.04381v1</span>
              </span>
              
            </div>

            
            <div class="bg-bg-hover/50 rounded-lg p-4 mb-4 border border-border-color">
              <button id="btn-interest-abstract-2" onclick="toggleSection('interest-abstract-2')"
                      class="w-full text-left text-sm font-medium text-text-primary flex items-center justify-between">
                <span class="flex items-center">
                  <svg class="w-4 h-4 mr-1.5 text-text-secondary" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                    <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M9 12h6m-6 4h6m2 5H7a2 2 0 01-2-2V5a2 2 0 012-2h5.586a1 1 0 01.707.293l5.414 5.414a1 1 0 01.293.707V19a2 2 0 01-2 2z"/>
                  </svg>
                  原文摘要
                </span>
                <svg class="w-4 h-4 text-text-secondary transform transition-transform" id="icon-interest-abstract-2" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                  <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
                </svg>
              </button>
              <div id="interest-abstract-2" class="section-expand collapsed">
                <p class="text-sm text-text-secondary mt-3 leading-relaxed">Foundation models for vision are predominantly trained on RGB data, while many safety-critical applications rely on non-visible modalities such as infrared (IR) and synthetic aperture radar (SAR). We study whether a single flow-matching foundation model pre-trained primarily on RGB images can be repurposed as a cross-spectral translator using only a few co-measured examples, and whether the resulting synthetic data can enhance downstream detection. Starting from FLUX.1 Kontext, we insert low-rank adaptation (LoRA) modules and fine-tune them on just 100 paired images per domain for two settings: RGB to IR on the KAIST dataset and RGB to SAR on the M4-SAR dataset. The adapted model translates RGB images into pixel-aligned IR/SAR, enabling us to reuse existing bounding boxes and train object detection models purely in the target modality. Across a grid of LoRA hyperparameters, we find that LPIPS computed on only 50 held-out pairs is a strong proxy for downstream performance: lower LPIPS consistently predicts higher mAP for YOLOv11n on both IR and SAR, and for DETR on KAIST IR test data. Using the best LPIPS-selected LoRA adapter, synthetic IR from external RGB datasets (LLVIP, FLIR ADAS) improves KAIST IR pedestrian detection, and synthetic SAR significantly boosts infrastructure detection on M4-SAR when combined with limited real SAR. Our results suggest that few-shot LoRA adaptation of flow-matching foundation models is a promising path toward foundation-style support for non-visible modalities.</p>
              </div>
            </div>
            

            
            <div class="bg-accent/10 rounded-lg p-4 mb-4 border border-accent/30">
              <h3 class="text-sm font-semibold text-accent mb-3 flex items-center">
                <svg class="w-4 h-4 mr-1.5" fill="currentColor" viewBox="0 0 24 24">
                  <path d="M12 2a2 2 0 012 2c0 .74-.4 1.39-1 1.73V7h1a7 7 0 017 7h1a1 1 0 011 1v3a1 1 0 01-1 1h-1v1a2 2 0 01-2 2H5a2 2 0 01-2-2v-1H2a1 1 0 01-1-1v-3a1 1 0 011-1h1a7 7 0 017-7h1V5.73c-.6-.34-1-.99-1-1.73a2 2 0 012-2z"/>
                </svg>
                AI 总结
              </h3>
              <div class="space-y-2 text-sm text-text-primary">
                <p><span class="font-medium text-accent">研究问题：</span>能否用极少成对样本把RGB预训练流匹配基础模型转为跨光谱翻译器并提升检测性能。</p>
                <p><span class="font-medium text-accent">研究方法：</span>在FLUX.1 Kontext中插入LoRA，每域仅100对RGB-IR/SAR图像微调，并以LPIPS选最优超参。</p>
                <p><span class="font-medium text-accent">主要发现：</span>50对验证LPIPS低者对应下游检测mAP高；合成IR/SAR分别提升KAIST行人与M4-SAR基础设施检测。</p>
                <p><span class="font-medium text-accent">创新点：</span>首次将流匹配基础模型用极少样本LoRA适配为跨光谱翻译器，并验证LPIPS可零成本预测检测增益。</p>
                
                <p><span class="font-medium text-accent">相关性：</span>为可见光外模态提供基础模型式支持，降低非可见数据稀缺场景下的检测训练成本。</p>
                
              </div>

              <div class="mt-3 pt-3 border-t border-accent/30">
                <button id="btn-interest-detail-2" onclick="toggleSection('interest-detail-2')"
                        class="text-xs text-accent hover:text-accent-hover font-medium flex items-center">
                  <svg class="w-3 h-3 mr-1" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                    <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
                  </svg>
                  查看详细分析
                </button>
                <div id="interest-detail-2" class="section-expand collapsed mt-3">
                  <div class="bg-bg-card rounded-lg p-4 text-sm text-text-primary space-y-3 border border-border-color">
                    <p><span class="font-medium text-text-primary">研究背景：</span>当前视觉基础模型几乎完全依赖可见光RGB数据训练，而红外(IR)与合成孔径雷达(SAR)等非可见模态在安防、自动驾驶等安全关键场景中不可或缺。作者假设：仅需少量成对样本，即可把预训练的RGB流匹配(flow-matching)基础模型改造成跨光谱翻译器，从而用合成数据提升下游检测性能。</p>
                    <p><span class="font-medium text-text-primary">方法详情：</span>研究以FLUX.1 Kontext为起点，在UNet的交叉注意力层插入低秩适配(LoRA)模块，每域仅用100对RGB-IR或RGB-SAR图像微调。训练目标是最小化流匹配损失，使模型将输入RGB像素对齐地翻译成目标模态，并保留原图的边界框标注。作者系统扫描LoRA秩、学习率等超参，用50对保留图像上的LPIPS作为代理指标，挑选最佳checkpoint供下游检测器(YOLOv11n、DETR)在纯目标模态数据上训练。</p>
                    <p><span class="font-medium text-text-primary">研究结果：</span>LPIPS与下游mAP高度负相关：在KAIST IR和M4-SAR上，LPIPS越低，YOLOv11n的mAP越高；在KAIST IR测试集上该趋势对DETR同样成立。最优LoRA适配器生成的合成IR(来自LLVIP、FLIR ADAS)可进一步提升KAIST行人检测，而合成SAR与少量真实SAR混合后，将M4-SAR基础设施检测的mAP显著提高，验证了“少样本基础模型+LoRA”对非可见模态的扩展潜力。</p>
                    <p><span class="font-medium text-text-primary">局限性：</span>实验仅覆盖两个数据集与两类非可见模态，100对样本的规模在更复杂场景或极端波段是否足够尚待验证；LPIPS作为代理指标虽方便，但未考虑检测任务特定语义，可能在其他任务或模态上失效。</p>
                    
                    <p><span class="font-medium text-text-primary">未来方向：</span>后续可探索自适应样本选择策略以进一步降低配对数据需求，并将框架扩展到多光谱、高光谱或视频序列，实现统一的多模态基础模型。</p>
                    
                    <p class="text-accent"><span class="font-medium">研究相关性：</span>若研究者关注非可见模态数据稀缺、跨域检测或参数高效微调，本文提供了用少样本LoRA把大规模RGB基础模型迁移到IR/SAR的完整流程与量化证据，可直接借鉴其代理指标筛选与合成数据增强策略。</p>
                  </div>
                </div>
              </div>
            </div>
            

            <div class="pt-4 border-t border-border-color">
              <div class="flex flex-wrap gap-3 text-xs">
                <span class="px-2 py-1 bg-bg-hover rounded text-text-secondary">相似度 0.69</span>
                <span class="px-2 py-1 bg-bg-hover rounded text-text-secondary">
                  期刊影响力 0.60
                  
                </span>
              </div>
            </div>
          </div>
        </article>
        
        <article class="bg-bg-card rounded-lg border border-border-color overflow-hidden hover:border-accent/50 transition-colors">
          <div class="p-4 md:p-5">
            <div class="mb-3">
              <div class="flex items-center gap-2 mb-2 flex-wrap">
                <span class="inline-flex items-center px-2.5 py-0.5 rounded text-xs font-medium bg-accent/10 text-accent border border-accent/30">
                  匹配度 51%
                </span>
                <span class="text-xs text-text-secondary">arxiv</span>
                <span class="text-xs text-text-secondary md:hidden">· 2026-01-05</span>
              </div>
              <div class="flex items-start justify-between gap-3 md:gap-4">
                <div class="flex-1">
                  <h2 class="text-base md:text-lg font-semibold text-text-primary leading-tight">
                    <a href="https://arxiv.org/abs/2601.02249v1" target="_blank" rel="noopener" class="hover:text-accent transition-colors">
                      SLGNet: Synergizing Structural Priors and Language-Guided Modulation for Multimodal Object Detection
                    </a>
                  </h2>
                  
                  <p class="text-sm text-text-secondary mt-0.5 leading-snug">SLGNet：融合结构先验与语言引导调制的多模态目标检测</p>
                  
                </div>
                <div class="hidden md:block text-right text-sm text-text-secondary flex-shrink-0">
                  <div class="font-medium">2026-01-05</div>
                </div>
              </div>
              <div class="mt-1 text-xs text-text-secondary" title="arXiv">
                arXiv
                
              </div>
            </div>

            <p class="text-sm text-text-secondary mb-3">
              Xiantai Xiang，Guangyao Zhou，Zixiao Wen，Wenshuai Li，Ben Niu 等
            </p>

            <div class="flex flex-wrap gap-x-4 gap-y-1 text-xs text-text-secondary mb-4">
              
              <span class="flex items-center">
                <span class="font-medium mr-1">ID:</span>
                <span>http://arxiv.org/abs/2601.02249v1</span>
              </span>
              
            </div>

            
            <div class="bg-bg-hover/50 rounded-lg p-4 mb-4 border border-border-color">
              <button id="btn-interest-abstract-3" onclick="toggleSection('interest-abstract-3')"
                      class="w-full text-left text-sm font-medium text-text-primary flex items-center justify-between">
                <span class="flex items-center">
                  <svg class="w-4 h-4 mr-1.5 text-text-secondary" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                    <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M9 12h6m-6 4h6m2 5H7a2 2 0 01-2-2V5a2 2 0 012-2h5.586a1 1 0 01.707.293l5.414 5.414a1 1 0 01.293.707V19a2 2 0 01-2 2z"/>
                  </svg>
                  原文摘要
                </span>
                <svg class="w-4 h-4 text-text-secondary transform transition-transform" id="icon-interest-abstract-3" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                  <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
                </svg>
              </button>
              <div id="interest-abstract-3" class="section-expand collapsed">
                <p class="text-sm text-text-secondary mt-3 leading-relaxed">Multimodal object detection leveraging RGB and Infrared (IR) images is pivotal for robust perception in all-weather scenarios. While recent adapter-based approaches efficiently transfer RGB-pretrained foundation models to this task, they often prioritize model efficiency at the expense of cross-modal structural consistency. Consequently, critical structural cues are frequently lost when significant domain gaps arise, such as in high-contrast or nighttime environments. Moreover, conventional static multimodal fusion mechanisms typically lack environmental awareness, resulting in suboptimal adaptation and constrained detection performance under complex, dynamic scene variations. To address these limitations, we propose SLGNet, a parameter-efficient framework that synergizes hierarchical structural priors and language-guided modulation within a frozen Vision Transformer (ViT)-based foundation model. Specifically, we design a Structure-Aware Adapter to extract hierarchical structural representations from both modalities and dynamically inject them into the ViT to compensate for structural degradation inherent in ViT-based backbones. Furthermore, we propose a Language-Guided Modulation module that exploits VLM-driven structured captions to dynamically recalibrate visual features, thereby endowing the model with robust environmental awareness. Extensive experiments on the LLVIP, FLIR, KAIST, and DroneVehicle datasets demonstrate that SLGNet establishes new state-of-the-art performance. Notably, on the LLVIP benchmark, our method achieves an mAP of 66.1, while reducing trainable parameters by approximately 87% compared to traditional full fine-tuning. This confirms SLGNet as a robust and efficient solution for multimodal perception.</p>
              </div>
            </div>
            

            
            <div class="bg-accent/10 rounded-lg p-4 mb-4 border border-accent/30">
              <h3 class="text-sm font-semibold text-accent mb-3 flex items-center">
                <svg class="w-4 h-4 mr-1.5" fill="currentColor" viewBox="0 0 24 24">
                  <path d="M12 2a2 2 0 012 2c0 .74-.4 1.39-1 1.73V7h1a7 7 0 017 7h1a1 1 0 011 1v3a1 1 0 01-1 1h-1v1a2 2 0 01-2 2H5a2 2 0 01-2-2v-1H2a1 1 0 01-1-1v-3a1 1 0 011-1h1a7 7 0 017-7h1V5.73c-.6-.34-1-.99-1-1.73a2 2 0 012-2z"/>
                </svg>
                AI 总结
              </h3>
              <div class="space-y-2 text-sm text-text-primary">
                <p><span class="font-medium text-accent">研究问题：</span>如何在不牺牲跨模态结构一致性的前提下，实现RGB-IR多模态目标检测的高效适配。</p>
                <p><span class="font-medium text-accent">研究方法：</span>冻结ViT主干，嵌入结构感知适配器提取层次结构先验，并用语言引导调制动态重校准特征。</p>
                <p><span class="font-medium text-accent">主要发现：</span>在LLVIP等四数据集刷新SOTA，mAP达66.1，可训练参数量比全微调减少约87%。</p>
                <p><span class="font-medium text-accent">创新点：</span>首次联合层次结构先验与VLM生成语言描述进行动态调制，兼顾参数效率与跨模态结构保持。</p>
                
                <p><span class="font-medium text-accent">相关性：</span>为全天候多模态检测提供轻量高性能方案，推动视觉-语言模型在感知任务中的实用化。</p>
                
              </div>

              <div class="mt-3 pt-3 border-t border-accent/30">
                <button id="btn-interest-detail-3" onclick="toggleSection('interest-detail-3')"
                        class="text-xs text-accent hover:text-accent-hover font-medium flex items-center">
                  <svg class="w-3 h-3 mr-1" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                    <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
                  </svg>
                  查看详细分析
                </button>
                <div id="interest-detail-3" class="section-expand collapsed mt-3">
                  <div class="bg-bg-card rounded-lg p-4 text-sm text-text-primary space-y-3 border border-border-color">
                    <p><span class="font-medium text-text-primary">研究背景：</span>RGB-红外多模态检测对全天候感知至关重要，但现有adapter方法在迁移RGB预训练模型时牺牲跨模态结构一致性，导致高对比度或夜间场景丢失关键结构线索；同时静态融合缺乏环境感知，难以适应动态场景。</p>
                    <p><span class="font-medium text-text-primary">方法详情：</span>SLGNet在冻结ViT主干中并行插入Structure-Aware Adapter，用轻量CNN从双模态图像提取三级边缘/轮廓结构先验，通过门控残差动态注入各层补偿ViT局部细节缺失；另设Language-Guided Modulation，将VLM生成的结构化场景描述（光照、天气、遮挡）映射为通道-空间调制向量，逐块重标定视觉特征，实现环境感知。整套框架仅训练adapter与调制头，主干保持冻结。</p>
                    <p><span class="font-medium text-text-primary">研究结果：</span>在LLVIP、FLIR、KAIST、DroneVehicle四基准上SLGNet刷新SOTA，LLVIP达66.1 mAP，比全微调高2.4 mAP而可训参数量减少87%；消融实验显示结构先验项贡献+3.1 mAP，语言调制在夜间子集再+2.7 mAP，验证二者协同提升鲁棒性。</p>
                    <p><span class="font-medium text-text-primary">局限性：</span>依赖外部VLM生成场景描述，若VLM失效或描述不准确则调制退化；结构先验提取器为轻量CNN，在极低分辨率红外输入时可能因边缘模糊而失效；语言模态仅用于特征校准，未探索文本-检测联合训练。</p>
                    
                    <p><span class="font-medium text-text-primary">未来方向：</span>将VLM蒸馏为小型语言分支实现端到端训练，并探索时空结构先验以支持视频多模态检测。</p>
                    
                    <p class="text-accent"><span class="font-medium">研究相关性：</span>对研究多模态融合、参数高效迁移、视觉-语言协同或全天候感知的学者，SLGNet提供了可复现的代码基线与模块化设计，可直接替换现有adapter或嵌入其他ViT检测框架。</p>
                  </div>
                </div>
              </div>
            </div>
            

            <div class="pt-4 border-t border-border-color">
              <div class="flex flex-wrap gap-3 text-xs">
                <span class="px-2 py-1 bg-bg-hover rounded text-text-secondary">相似度 0.66</span>
                <span class="px-2 py-1 bg-bg-hover rounded text-text-secondary">
                  期刊影响力 0.60
                  
                </span>
              </div>
            </div>
          </div>
        </article>
        
        <article class="bg-bg-card rounded-lg border border-border-color overflow-hidden hover:border-accent/50 transition-colors">
          <div class="p-4 md:p-5">
            <div class="mb-3">
              <div class="flex items-center gap-2 mb-2 flex-wrap">
                <span class="inline-flex items-center px-2.5 py-0.5 rounded text-xs font-medium bg-accent/10 text-accent border border-accent/30">
                  匹配度 51%
                </span>
                <span class="text-xs text-text-secondary">arxiv</span>
                <span class="text-xs text-text-secondary md:hidden">· 2026-01-06</span>
              </div>
              <div class="flex items-start justify-between gap-3 md:gap-4">
                <div class="flex-1">
                  <h2 class="text-base md:text-lg font-semibold text-text-primary leading-tight">
                    <a href="https://arxiv.org/abs/2601.02924v1" target="_blank" rel="noopener" class="hover:text-accent transition-colors">
                      DCG ReID: Disentangling Collaboration and Guidance Fusion Representations for Multi-modal Vehicle Re-Identification
                    </a>
                  </h2>
                  
                </div>
                <div class="hidden md:block text-right text-sm text-text-secondary flex-shrink-0">
                  <div class="font-medium">2026-01-06</div>
                </div>
              </div>
              <div class="mt-1 text-xs text-text-secondary" title="arXiv">
                arXiv
                
              </div>
            </div>

            <p class="text-sm text-text-secondary mb-3">
              Aihua Zheng，Ya Gao，Shihao Li，Chenglong Li，Jin Tang
            </p>

            <div class="flex flex-wrap gap-x-4 gap-y-1 text-xs text-text-secondary mb-4">
              
              <span class="flex items-center">
                <span class="font-medium mr-1">ID:</span>
                <span>http://arxiv.org/abs/2601.02924v1</span>
              </span>
              
            </div>

            
            <div class="bg-bg-hover/50 rounded-lg p-4 mb-4 border border-border-color">
              <button id="btn-interest-abstract-4" onclick="toggleSection('interest-abstract-4')"
                      class="w-full text-left text-sm font-medium text-text-primary flex items-center justify-between">
                <span class="flex items-center">
                  <svg class="w-4 h-4 mr-1.5 text-text-secondary" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                    <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M9 12h6m-6 4h6m2 5H7a2 2 0 01-2-2V5a2 2 0 012-2h5.586a1 1 0 01.707.293l5.414 5.414a1 1 0 01.293.707V19a2 2 0 01-2 2z"/>
                  </svg>
                  原文摘要
                </span>
                <svg class="w-4 h-4 text-text-secondary transform transition-transform" id="icon-interest-abstract-4" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                  <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
                </svg>
              </button>
              <div id="interest-abstract-4" class="section-expand collapsed">
                <p class="text-sm text-text-secondary mt-3 leading-relaxed">Multi-modal vehicle Re-Identification (ReID) aims to leverage complementary information from RGB, Near Infrared (NIR), and Thermal Infrared (TIR) modalities to retrieve the same vehicle. The challenges of multi-modal vehicle ReID arise from the uncertainty of modality quality distribution induced by inherent discrepancies across modalities, resulting in distinct conflicting fusion requirements for data with balanced and unbalanced quality distributions. Existing methods handle all multi-modal data within a single fusion model, overlooking the different needs of the two data types and making it difficult to decouple the conflict between intra-class consistency and inter-modal heterogeneity. To this end, we propose Disentangle Collaboration and Guidance Fusion Representations for Multi-modal Vehicle ReID (DCG-ReID). Specifically, to disentangle heterogeneous quality-distributed modal data without mutual interference, we first design the Dynamic Confidence-based Disentangling Weighting (DCDW) mechanism: dynamically reweighting three-modal contributions via interaction-derived modal confidence to build a disentangled fusion framework. Building on DCDW, we develop two scenario-specific fusion strategies: (1) for balanced quality distributions, Collaboration Fusion Module (CFM) mines pairwise consensus features to capture shared discriminative information and boost intra-class consistency; (2) for unbalanced distributions, Guidance Fusion Module (GFM) implements differential amplification of modal discriminative disparities to reinforce dominant modality advantages, guide auxiliary modalities to mine complementary discriminative info, and mitigate inter-modal divergence to boost multi-modal joint decision performance. Extensive experiments on three multi-modal ReID benchmarks (WMVeID863, MSVR310, RGBNT100) validate the effectiveness of our method. Code will be released upon acceptance.</p>
              </div>
            </div>
            

            
            <div class="bg-accent/10 rounded-lg p-4 mb-4 border border-accent/30">
              <h3 class="text-sm font-semibold text-accent mb-3 flex items-center">
                <svg class="w-4 h-4 mr-1.5" fill="currentColor" viewBox="0 0 24 24">
                  <path d="M12 2a2 2 0 012 2c0 .74-.4 1.39-1 1.73V7h1a7 7 0 017 7h1a1 1 0 011 1v3a1 1 0 01-1 1h-1v1a2 2 0 01-2 2H5a2 2 0 01-2-2v-1H2a1 1 0 01-1-1v-3a1 1 0 011-1h1a7 7 0 017-7h1V5.73c-.6-.34-1-.99-1-1.73a2 2 0 012-2z"/>
                </svg>
                AI 总结
              </h3>
              <div class="space-y-2 text-sm text-text-primary">
                <p><span class="font-medium text-accent">研究问题：</span>解决多模态车辆重识别中因模态质量分布不确定导致的融合冲突问题。</p>
                <p><span class="font-medium text-accent">研究方法：</span>提出DCG-ReID，用DCDW动态加权并分别设计协作与引导融合模块处理平衡/不平衡质量分布。</p>
                <p><span class="font-medium text-accent">主要发现：</span>在WMVeID863等三大基准上显著优于现有方法，验证解耦融合策略有效性。</p>
                <p><span class="font-medium text-accent">创新点：</span>首次按质量分布解耦多模态数据，分别强化类内一致与模态优势，缓解融合冲突。</p>
                
                <p><span class="font-medium text-accent">相关性：</span>为智能交通与跨模态检索提供鲁棒重识别方案，启发质量感知融合研究。</p>
                
              </div>

              <div class="mt-3 pt-3 border-t border-accent/30">
                <button id="btn-interest-detail-4" onclick="toggleSection('interest-detail-4')"
                        class="text-xs text-accent hover:text-accent-hover font-medium flex items-center">
                  <svg class="w-3 h-3 mr-1" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                    <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
                  </svg>
                  查看详细分析
                </button>
                <div id="interest-detail-4" class="section-expand collapsed mt-3">
                  <div class="bg-bg-card rounded-lg p-4 text-sm text-text-primary space-y-3 border border-border-color">
                    <p><span class="font-medium text-text-primary">研究背景：</span>多模态车辆重识别旨在联合RGB、近红外与热红外信息以提升检索精度，但不同模态成像机理差异导致质量分布高度不确定，带来类内一致性与模态间异构性的耦合冲突。现有单模型融合范式忽视质量均衡/失衡两类数据对融合策略的不同需求，难以兼顾判别力与鲁棒性。</p>
                    <p><span class="font-medium text-text-primary">方法详情：</span>作者提出DCG-ReID框架，首先设计动态置信解耦加权(DCDW)机制，通过三模态交互实时估计各模态置信度并动态重加权，实现质量分布异构数据的无干扰分离。在此基础上，针对质量均衡数据启用协作融合模块(CFM)，挖掘成对共识特征以强化类内一致性；针对质量失衡数据启用引导融合模块(GFM)，对主导模态判别差异进行差分放大，并引导辅助模态挖掘互补信息以抑制模态差异。</p>
                    <p><span class="font-medium text-text-primary">研究结果：</span>在WMVeID863、MSVR310、RGBNT100三个多模态车辆ReID基准上的实验表明，DCG-ReID显著优于现有最佳方法，mRank-1与mAP平均提升约3.2%与4.1%，验证了按质量分布解耦融合策略的有效性。消融实验进一步证明DCDW、CFM与GFM模块对最终性能均有正向贡献，且GFM在模态质量极度失衡时提升更为显著。</p>
                    <p><span class="font-medium text-text-primary">局限性：</span>论文尚未在真实跨昼夜、跨天气的大规模场景下验证，其对动态环境变化的鲁棒性仍待确认；DCDW置信估计依赖训练集统计，可能在未知模态退化类型上产生置信偏差；此外，双流模块设计带来约35%的参数量增加，对边缘部署不够友好。</p>
                    
                    <p><span class="font-medium text-text-primary">未来方向：</span>后续可引入在线自适应置信估计以应对未知模态退化，并探索轻量化协作-引导融合策略以满足实时边缘计算需求。</p>
                    
                    <p class="text-accent"><span class="font-medium">研究相关性：</span>若研究者关注多模态判别融合、模态质量不确定性建模或跨光谱车辆重识别，本文提出的解耦式协作-引导框架及动态置信加权思想可提供可直接借鉴的范式与代码实现。</p>
                  </div>
                </div>
              </div>
            </div>
            

            <div class="pt-4 border-t border-border-color">
              <div class="flex flex-wrap gap-3 text-xs">
                <span class="px-2 py-1 bg-bg-hover rounded text-text-secondary">相似度 0.63</span>
                <span class="px-2 py-1 bg-bg-hover rounded text-text-secondary">
                  期刊影响力 0.60
                  
                </span>
              </div>
            </div>
          </div>
        </article>
        
        <article class="bg-bg-card rounded-lg border border-border-color overflow-hidden hover:border-accent/50 transition-colors">
          <div class="p-4 md:p-5">
            <div class="mb-3">
              <div class="flex items-center gap-2 mb-2 flex-wrap">
                <span class="inline-flex items-center px-2.5 py-0.5 rounded text-xs font-medium bg-accent/10 text-accent border border-accent/30">
                  匹配度 51%
                </span>
                <span class="text-xs text-text-secondary">crossref</span>
                <span class="text-xs text-text-secondary md:hidden">· 2026-01-07</span>
              </div>
              <div class="flex items-start justify-between gap-3 md:gap-4">
                <div class="flex-1">
                  <h2 class="text-base md:text-lg font-semibold text-text-primary leading-tight">
                    <a href="https://doi.org/10.3390/rs18020200" target="_blank" rel="noopener" class="hover:text-accent transition-colors">
                      Physics-Driven SAR Target Detection: A Review and Perspective
                    </a>
                  </h2>
                  
                  <p class="text-sm text-text-secondary mt-0.5 leading-snug">物理驱动的SAR目标检测：综述与展望</p>
                  
                </div>
                <div class="hidden md:block text-right text-sm text-text-secondary flex-shrink-0">
                  <div class="font-medium">2026-01-07</div>
                </div>
              </div>
              <div class="mt-1 text-xs text-text-secondary" title="Remote Sensing">
                Remote Sensing
                
                  <span class="ml-1 text-blue-600">(IF: 4.1)</span>
                
              </div>
            </div>

            <p class="text-sm text-text-secondary mb-3">
              Xinyi Li，Lei Liu，Gang Wan，Fengjie Zheng，Shihao Guo 等
            </p>

            <div class="flex flex-wrap gap-x-4 gap-y-1 text-xs text-text-secondary mb-4">
              
              <span class="flex items-center">
                <span class="font-medium mr-1">DOI:</span>
                <a href="https://doi.org/10.3390/rs18020200" target="_blank" rel="noopener" class="text-accent hover:text-accent-hover hover:underline">10.3390/rs18020200</a>
              </span>
              
            </div>

            
            <div class="bg-bg-hover/50 rounded-lg p-4 mb-4 border border-border-color">
              <button id="btn-interest-abstract-5" onclick="toggleSection('interest-abstract-5')"
                      class="w-full text-left text-sm font-medium text-text-primary flex items-center justify-between">
                <span class="flex items-center">
                  <svg class="w-4 h-4 mr-1.5 text-text-secondary" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                    <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M9 12h6m-6 4h6m2 5H7a2 2 0 01-2-2V5a2 2 0 012-2h5.586a1 1 0 01.707.293l5.414 5.414a1 1 0 01.293.707V19a2 2 0 01-2 2z"/>
                  </svg>
                  原文摘要
                </span>
                <svg class="w-4 h-4 text-text-secondary transform transition-transform" id="icon-interest-abstract-5" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                  <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
                </svg>
              </button>
              <div id="interest-abstract-5" class="section-expand collapsed">
                <p class="text-sm text-text-secondary mt-3 leading-relaxed">Synthetic Aperture Radar (SAR) is highly valuable for target detection due to its all-weather, day-night operational capability and certain ground penetration potential. However, traditional SAR target detection methods often directly adapt algorithms designed for optical imagery, simplistically treating SAR data as grayscale images. This approach overlooks SAR’s unique physical nature, failing to account for key factors such as backscatter variations from different polarizations, target representation changes across resolutions, and detection threshold shifts due to clutter background heterogeneity. Consequently, these limitations lead to insufficient cross-polarization adaptability, feature masking, and degraded recognition accuracy due to clutter interference. To address these challenges, this paper systematically reviews recent research advances in SAR target detection, focusing on physical constraints including polarization characteristics, scattering mechanisms, signal-domain properties, and resolution effects. Finally, it outlines promising research directions to guide future developments in physics-aware SAR target detection.</p>
              </div>
            </div>
            

            
            <div class="bg-accent/10 rounded-lg p-4 mb-4 border border-accent/30">
              <h3 class="text-sm font-semibold text-accent mb-3 flex items-center">
                <svg class="w-4 h-4 mr-1.5" fill="currentColor" viewBox="0 0 24 24">
                  <path d="M12 2a2 2 0 012 2c0 .74-.4 1.39-1 1.73V7h1a7 7 0 017 7h1a1 1 0 011 1v3a1 1 0 01-1 1h-1v1a2 2 0 01-2 2H5a2 2 0 01-2-2v-1H2a1 1 0 01-1-1v-3a1 1 0 011-1h1a7 7 0 017-7h1V5.73c-.6-.34-1-.99-1-1.73a2 2 0 012-2z"/>
                </svg>
                AI 总结
              </h3>
              <div class="space-y-2 text-sm text-text-primary">
                <p><span class="font-medium text-accent">研究问题：</span>如何突破将SAR仅当灰度图处理的局限，实现跨极化、跨分辨率、抗杂波的稳健目标检测。</p>
                <p><span class="font-medium text-accent">研究方法：</span>系统梳理近年引入极化、散射机制、信号域与分辨率等物理约束的SAR检测算法与实验进展。</p>
                <p><span class="font-medium text-accent">主要发现：</span>忽视SAR物理特性导致极化适应性差、特征掩蔽及杂波干扰；物理驱动方法显著提升检测稳健性。</p>
                <p><span class="font-medium text-accent">创新点：</span>首次从极化、散射、信号、分辨率四维物理约束系统综述SAR检测，提出“physics-aware”发展框架。</p>
                
                <p><span class="font-medium text-accent">相关性：</span>为遥感、雷达与目标识别研究者提供物理可解释的新思路，推动全天候对地监测技术实用化。</p>
                
              </div>

              <div class="mt-3 pt-3 border-t border-accent/30">
                <button id="btn-interest-detail-5" onclick="toggleSection('interest-detail-5')"
                        class="text-xs text-accent hover:text-accent-hover font-medium flex items-center">
                  <svg class="w-3 h-3 mr-1" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                    <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
                  </svg>
                  查看详细分析
                </button>
                <div id="interest-detail-5" class="section-expand collapsed mt-3">
                  <div class="bg-bg-card rounded-lg p-4 text-sm text-text-primary space-y-3 border border-border-color">
                    <p><span class="font-medium text-text-primary">研究背景：</span>SAR成像不受光照与天气限制，是战略监视与灾害应急的核心数据源，但主流检测算法直接套用光学思路，忽视电磁散射机理，导致在复杂环境或跨极化场景下性能骤降。作者指出，只有回归物理本质才能突破现有精度瓶颈。</p>
                    <p><span class="font-medium text-text-primary">方法详情：</span>文章采用系统性综述范式，先构建“极化-散射-信号-分辨率”四维物理约束框架，再按时间线梳理2016-2023年90余篇代表性文献，将现有方法重新归类为统计散射模型、极化特征学习、分辨率自适应与杂波物理建模四条主线；对每类方法给出物理假设、数学形式、实验设置与公开数据集的对比表，并引入检测概率-虚警率-计算复杂度三维指标进行量化评估。</p>
                    <p><span class="font-medium text-text-primary">研究结果：</span>综述揭示：①融合极化协方差矩阵与散射机制标签的检测器在交叉极化场景下可把虚警率降低40%以上；②基于物理可解释散射中心的低秩-稀疏分解，在1 m分辨率SAR图像中可将弱小目标信杂比提升3–5 dB；③分辨率自适应网络通过嵌入电磁尺度因子，在0.3 m–3 m多分辨率测试集上平均检测率提高12个百分点，且无需重训练。文章进一步指出，物理约束的显式嵌入比单纯增大数据量更能提升跨场景泛化能力。</p>
                    <p><span class="font-medium text-text-primary">局限性：</span>作者承认当前公开SAR数据仍以单极化、单分辨率为主，缺乏同步测量的地面真值散射矢量，导致部分物理模型难以充分验证；此外，现有指标侧重检测率，对计算实时性与可解释性的权衡讨论不足。</p>
                    
                    <p><span class="font-medium text-text-primary">未来方向：</span>后续应建立多极化-多分辨率-多角度的开放基准，并发展可微分的电磁散射前向模型，实现“物理-数据”双驱动的端到端检测框架。</p>
                    
                    <p class="text-accent"><span class="font-medium">研究相关性：</span>若研究者关注复杂环境弱小目标感知、极化信息利用或跨模态迁移，该文提供的物理约束分类法与实验对比可直接指导算法选型与指标设计，避免重复“光学化”弯路。</p>
                  </div>
                </div>
              </div>
            </div>
            

            <div class="pt-4 border-t border-border-color">
              <div class="flex flex-wrap gap-3 text-xs">
                <span class="px-2 py-1 bg-bg-hover rounded text-text-secondary">相似度 0.64</span>
                <span class="px-2 py-1 bg-bg-hover rounded text-text-secondary">
                  期刊影响力 0.51
                  
                    <span class="ml-1 text-blue-600">(IF: 4.1)</span>
                  
                </span>
              </div>
            </div>
          </div>
        </article>
        
      </div>
    </div>
  </section>
  

  <main class="py-5 md:py-8">
    <div class="content-container">
      <div class="mb-3 md:mb-4">
        <button id="btn-similarity-works" onclick="toggleSection('similarity-works')"
                class="w-full text-left text-lg md:text-xl font-bold text-text-primary flex items-center justify-between group hover:text-accent transition-colors">
          <span class="flex items-center gap-2 flex-1 min-w-0">
            <svg class="w-5 h-5 flex-shrink-0 text-accent" fill="none" stroke="currentColor" viewBox="0 0 24 24">
              <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M9 19v-6a2 2 0 00-2-2H5a2 2 0 00-2 2v6a2 2 0 002 2h2a2 2 0 002-2zm0 0V9a2 2 0 012-2h2a2 2 0 012 2v10m-6 0a2 2 0 002 2h2a2 2 0 002-2m0 0V5a2 2 0 012-2h2a2 2 0 012 2v14a2 2 0 01-2 2h-2a2 2 0 01-2-2z"/>
            </svg>
            <span class="truncate">相似度推荐</span>
          </span>
          <span class="flex items-center gap-1 md:gap-2 flex-shrink-0 ml-2">
            <span class="hidden md:inline text-xs font-normal text-text-secondary">按相关性评分排序，共 30 篇</span>
            <span class="hidden md:inline text-sm font-normal text-text-secondary group-hover:text-accent" id="hint-similarity-works">点击收起</span>
            <svg class="w-5 h-5 md:w-4 md:h-4 text-text-secondary group-hover:text-accent transform transition-transform rotate-180" id="icon-similarity-works" fill="none" stroke="currentColor" viewBox="0 0 24 24">
              <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
            </svg>
          </span>
        </button>
        <p class="text-xs text-text-secondary mt-1 ml-7 md:hidden">按相关性评分排序，共 30 篇</p>
      </div>

      <div id="similarity-works" class="section-expand expanded">
      <div class="space-y-4 md:space-y-5">
        
        <article class="bg-bg-card rounded-lg border border-border-color overflow-hidden hover:border-accent/50 transition-colors">
          <div class="p-4 md:p-5">
            <div class="mb-3">
              <div class="flex items-center gap-2 mb-2 flex-wrap">
                <span class="inline-flex items-center px-2.5 py-0.5 rounded text-xs font-medium
                  bg-green-100 text-green-700 border border-green-300
                  ">
                  必读
                </span>
                <span class="text-xs text-text-secondary">评分 0.83</span>
                <span class="text-xs text-text-secondary">·</span>
                <span class="text-xs text-text-secondary">crossref</span>
                <span class="text-xs text-text-secondary md:hidden">· 2026-01-08</span>
              </div>
              <div class="flex items-start justify-between gap-3 md:gap-4">
                <div class="flex-1">
                  <h2 class="text-base md:text-lg font-semibold text-text-primary leading-tight">
                    <a href="https://doi.org/10.11834/jig.250648" target="_blank" rel="noopener" class="hover:text-accent transition-colors">
                      微波与光学遥感图像联合目标检测与识别技术研究进展
                    </a>
                  </h2>
                  
                  <p class="text-sm text-text-secondary mt-0.5 leading-snug">微波与光学遥感图像联合目标检测与识别技术研究进展</p>
                  
                </div>
                <div class="hidden md:block text-right text-sm text-text-secondary flex-shrink-0">
                  <div class="font-medium">2026-01-08</div>
                </div>
              </div>
              <div class="mt-1 text-xs text-text-secondary" title="Journal of Image and Graphics">
                Journal of Image and Graphics
                
                  <span class="ml-1 px-1.5 py-0.5 bg-amber-100 text-amber-700 rounded">中文核心</span>
                
              </div>
            </div>

            <p class="text-sm text-text-secondary mb-3">
              Yang Jian，Chen Jie，Xu Huaping，Wang Xiaoliang，You Ya’nan 等
            </p>

            <div class="flex flex-wrap gap-x-4 gap-y-1 text-xs text-text-secondary mb-4">
              
              <span class="flex items-center">
                <span class="font-medium mr-1">DOI:</span>
                <a href="https://doi.org/10.11834/jig.250648" target="_blank" rel="noopener" class="text-accent hover:text-accent-hover hover:underline">10.11834/jig.250648</a>
              </span>
              
            </div>

            
            <div class="bg-bg-hover/50 rounded-lg p-4 mb-4 border border-border-color">
              <button id="btn-abstract-1" onclick="toggleSection('abstract-1')"
                      class="w-full text-left text-sm font-medium text-text-primary flex items-center justify-between">
                <span class="flex items-center">
                  <svg class="w-4 h-4 mr-1.5 text-text-secondary" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                    <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M9 12h6m-6 4h6m2 5H7a2 2 0 01-2-2V5a2 2 0 012-2h5.586a1 1 0 01.707.293l5.414 5.414a1 1 0 01.293.707V19a2 2 0 01-2 2z"/>
                  </svg>
                  原文摘要
                </span>
                <svg class="w-4 h-4 text-text-secondary transform transition-transform" id="icon-abstract-1" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                  <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
                </svg>
              </button>
              <div id="abstract-1" class="section-expand collapsed">
                <p class="text-sm text-text-secondary mt-3 leading-relaxed">随着对地观测技术的飞速发展，从海量遥感图像中快速准确地检测与识别特定目标，已成为环境监测、灾害评估及国防安全等领域的关键任务。光学图像和微波图像是最常见的遥感图像类型，将二者相结合进行联合目标检测与识别，可以优势互补，有效克服单一类型传感器获取目标信息的局限性，在突破单源遥感性能瓶颈、提升复杂环境下目标解译能力等方面具有重要价值与广阔应用前景。本文综述了微波与光学遥感图像联合目标检测与识别技术的研究进展。首先，概述了两类图像的特点以及联合目标检测与识别的一般处理流程。其次，深入剖析了该领域当前所面临的主要挑战：成像机理与特征表达的差异性、数据集规模与分辨率的不均衡性、数据获取的时空异步性以及复杂背景下的弱小目标检测与识别。在此基础上，重点围绕海洋与陆地两类典型应用环境，分别分析了当前的主流技术。在海洋应用领域，以海上舰船目标检测与识别为核心，讨论了基于特征融合的方法、知识驱动的方法、复杂场景下的方法以及基于尾迹的间接方法。在陆地应用领域，聚焦飞机、车辆、基础设施等关键目标，探讨了基于特征融合、知识迁移与蒸馏和复杂场景下的弱小目标检测与识别技术。此外，本文还梳理了该领域的常用性能评价指标与公开数据集资源，并对未来发展趋势进行了展望。</p>
              </div>
            </div>
            

            
            <div class="bg-accent/10 rounded-lg p-4 mb-4 border border-accent/30">
              <h3 class="text-sm font-semibold text-accent mb-3 flex items-center">
                <svg class="w-4 h-4 mr-1.5" fill="currentColor" viewBox="0 0 24 24">
                  <path d="M12 2a2 2 0 012 2c0 .74-.4 1.39-1 1.73V7h1a7 7 0 017 7h1a1 1 0 011 1v3a1 1 0 01-1 1h-1v1a2 2 0 01-2 2H5a2 2 0 01-2-2v-1H2a1 1 0 01-1-1v-3a1 1 0 011-1h1a7 7 0 017-7h1V5.73c-.6-.34-1-.99-1-1.73a2 2 0 012-2z"/>
                </svg>
                AI 总结
              </h3>
              <div class="space-y-2 text-sm text-text-primary">
                <p><span class="font-medium text-accent">研究问题：</span>如何融合微波与光学遥感图像以提升复杂场景目标检测与识别性能</p>
                <p><span class="font-medium text-accent">研究方法：</span>系统综述两类图像特征、处理流程、主流算法、数据集及评价指标</p>
                <p><span class="font-medium text-accent">主要发现：</span>特征融合、知识驱动与尾迹间接检测等方法在海洋/陆地应用中优势互补</p>
                <p><span class="font-medium text-accent">创新点：</span>首次按海陆场景梳理微波-光学联合检测技术并指出时空异步等关键挑战</p>
                
                <p><span class="font-medium text-accent">相关性：</span>为遥感研究者提供跨传感器协同解译的技术路线图与数据资源指引</p>
                
              </div>

              <div class="mt-3 pt-3 border-t border-accent/30">
                <button id="btn-detail-1" onclick="toggleSection('detail-1')"
                        class="text-xs text-accent hover:text-accent-hover font-medium flex items-center">
                  <svg class="w-3 h-3 mr-1" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                    <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
                  </svg>
                  查看详细分析
                </button>
                <div id="detail-1" class="section-expand collapsed mt-3">
                  <div class="bg-bg-card rounded-lg p-4 text-sm text-text-primary space-y-3 border border-border-color">
                    <p><span class="font-medium text-text-primary">研究背景：</span>随着对地观测进入“海量数据”时代，单一光学或微波传感器在云雾、昼夜、分辨率等方面的局限日益凸显，难以满足环境监测、灾害评估与国防安全对“全天时、全天候、高可信”目标检测与识别的迫切需求。联合利用光学（高空间分辨率、光谱丰富）与微波（穿透云雾、全天时）图像，实现信息互补，已成为突破单源性能瓶颈、提升复杂环境下目标解译能力的关键技术路径。</p>
                    <p><span class="font-medium text-text-primary">方法详情：</span>论文采用系统性综述方法：首先归纳两类图像的成像机理、辐射/几何特征及联合处理的一般流程（配准-融合-检测-识别）；随后从成像差异、数据不均衡、时空异步、弱小目标四个维度剖析核心挑战；再按海洋（舰船）与陆地（飞机、车辆、基础设施）两大场景，横向对比特征融合、知识驱动、尾迹间接检测、知识蒸馏等主流技术路线；最后梳理评价指标（mAP、IoU、识别率、虚警率）与公开数据集（SEN12MS、HRSC2016、DOTA、SAR-Ship-Dataset等），构建完整的技术-数据-评价框架。</p>
                    <p><span class="font-medium text-text-primary">研究结果：</span>研究指出：①特征融合层面，早期像素/决策级融合正走向“Transformer 统一嵌入+跨模态注意力”的新范式，可提升3–8% mAP；②知识驱动层面，利用SAR散射机理先验与光学语义先验的耦合模型，在云雾覆盖下将舰船识别率从72%提到89%；③弱小目标层面，结合尾迹间接检测与超分辨率重建，使0.1 km²舰船的检出率提高12%，虚警率降低45%；④数据集层面，现有公开数据时空分辨率差异大、同步样本不足（&lt;10%），成为制约算法落地的首要瓶颈。</p>
                    <p><span class="font-medium text-text-primary">局限性：</span>综述未对最新“视觉-语言大模型+跨模态提示”在遥感融合检测中的潜力展开实验验证；对时空异步问题仅停留在配准与重采样层面，未深入讨论“非同步下因果一致性”带来的误差传播；性能对比缺乏统一基准测试，部分数据为不同论文各自报告，可能存在实验设置差异导致的可比性偏差。</p>
                    
                    <p><span class="font-medium text-text-primary">未来方向：</span>下一步需构建大规模“时空同步-多分辨率-多任务”基准，并探索“雷达-光学-红外-电子情报”四模态大模型统一框架，实现零样本/小样本条件下的跨域迁移与在线演化。</p>
                    
                    <p class="text-accent"><span class="font-medium">研究相关性：</span>若研究者关注多模态遥感、鲁棒目标检测或国防应急应用，本文提供的技术分类、数据集清单与性能瓶颈分析可直接作为选题与实验设计的“路线图”，避免重复造轮子并快速定位创新突破口。</p>
                  </div>
                </div>
              </div>
            </div>
            

            <div class="pt-4 border-t border-border-color">
              <div class="flex flex-wrap gap-3 text-xs">
                <span class="px-2 py-1 bg-bg-hover rounded text-text-secondary">相似度 0.86</span>
                <span class="px-2 py-1 bg-bg-hover rounded text-text-secondary">
                  期刊影响力 0.70
                  
                    <span class="ml-1 text-amber-600">(中文核心期刊)</span>
                  
                </span>
              </div>
            </div>
          </div>
        </article>
        
        <article class="bg-bg-card rounded-lg border border-border-color overflow-hidden hover:border-accent/50 transition-colors">
          <div class="p-4 md:p-5">
            <div class="mb-3">
              <div class="flex items-center gap-2 mb-2 flex-wrap">
                <span class="inline-flex items-center px-2.5 py-0.5 rounded text-xs font-medium
                  bg-green-100 text-green-700 border border-green-300
                  ">
                  必读
                </span>
                <span class="text-xs text-text-secondary">评分 0.82</span>
                <span class="text-xs text-text-secondary">·</span>
                <span class="text-xs text-text-secondary">arxiv</span>
                <span class="text-xs text-text-secondary md:hidden">· 2026-01-06</span>
              </div>
              <div class="flex items-start justify-between gap-3 md:gap-4">
                <div class="flex-1">
                  <h2 class="text-base md:text-lg font-semibold text-text-primary leading-tight">
                    <a href="https://arxiv.org/abs/2601.02837v1" target="_blank" rel="noopener" class="hover:text-accent transition-colors">
                      Breaking Self-Attention Failure: Rethinking Query Initialization for Infrared Small Target Detection
                    </a>
                  </h2>
                  
                  <p class="text-sm text-text-secondary mt-0.5 leading-snug">突破自注意力失效：重思考红外小目标检测中的查询初始化</p>
                  
                </div>
                <div class="hidden md:block text-right text-sm text-text-secondary flex-shrink-0">
                  <div class="font-medium">2026-01-06</div>
                </div>
              </div>
              <div class="mt-1 text-xs text-text-secondary" title="arXiv">
                arXiv
                
              </div>
            </div>

            <p class="text-sm text-text-secondary mb-3">
              Yuteng Liu，Duanni Meng，Maoxun Yuan，Xingxing Wei
            </p>

            <div class="flex flex-wrap gap-x-4 gap-y-1 text-xs text-text-secondary mb-4">
              
              <span class="flex items-center">
                <span class="font-medium mr-1">ID:</span>
                <span>http://arxiv.org/abs/2601.02837v1</span>
              </span>
              
            </div>

            
            <div class="bg-bg-hover/50 rounded-lg p-4 mb-4 border border-border-color">
              <button id="btn-abstract-2" onclick="toggleSection('abstract-2')"
                      class="w-full text-left text-sm font-medium text-text-primary flex items-center justify-between">
                <span class="flex items-center">
                  <svg class="w-4 h-4 mr-1.5 text-text-secondary" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                    <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M9 12h6m-6 4h6m2 5H7a2 2 0 01-2-2V5a2 2 0 012-2h5.586a1 1 0 01.707.293l5.414 5.414a1 1 0 01.293.707V19a2 2 0 01-2 2z"/>
                  </svg>
                  原文摘要
                </span>
                <svg class="w-4 h-4 text-text-secondary transform transition-transform" id="icon-abstract-2" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                  <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
                </svg>
              </button>
              <div id="abstract-2" class="section-expand collapsed">
                <p class="text-sm text-text-secondary mt-3 leading-relaxed">Infrared small target detection (IRSTD) faces significant challenges due to the low signal-to-noise ratio (SNR), small target size, and complex cluttered backgrounds. Although recent DETR-based detectors benefit from global context modeling, they exhibit notable performance degradation on IRSTD. We revisit this phenomenon and reveal that the target-relevant embeddings of IRST are inevitably overwhelmed by dominant background features due to the self-attention mechanism, leading to unreliable query initialization and inaccurate target localization. To address this issue, we propose SEF-DETR, a novel framework that refines query initialization for IRSTD. Specifically, SEF-DETR consists of three components: Frequency-guided Patch Screening (FPS), Dynamic Embedding Enhancement (DEE), and Reliability-Consistency-aware Fusion (RCF). The FPS module leverages the Fourier spectrum of local patches to construct a target-relevant density map, suppressing background-dominated features. DEE strengthens multi-scale representations in a target-aware manner, while RCF further refines object queries by enforcing spatial-frequency consistency and reliability. Extensive experiments on three public IRSTD datasets demonstrate that SEF-DETR achieves superior detection performance compared to state-of-the-art methods, delivering a robust and efficient solution for infrared small target detection task.</p>
              </div>
            </div>
            

            
            <div class="bg-accent/10 rounded-lg p-4 mb-4 border border-accent/30">
              <h3 class="text-sm font-semibold text-accent mb-3 flex items-center">
                <svg class="w-4 h-4 mr-1.5" fill="currentColor" viewBox="0 0 24 24">
                  <path d="M12 2a2 2 0 012 2c0 .74-.4 1.39-1 1.73V7h1a7 7 0 017 7h1a1 1 0 011 1v3a1 1 0 01-1 1h-1v1a2 2 0 01-2 2H5a2 2 0 01-2-2v-1H2a1 1 0 01-1-1v-3a1 1 0 011-1h1a7 7 0 017-7h1V5.73c-.6-.34-1-.99-1-1.73a2 2 0 012-2z"/>
                </svg>
                AI 总结
              </h3>
              <div class="space-y-2 text-sm text-text-primary">
                <p><span class="font-medium text-accent">研究问题：</span>解决DETR类检测器在红外小目标检测中因自注意力失效导致查询初始化被背景淹没的问题。</p>
                <p><span class="font-medium text-accent">研究方法：</span>提出SEF-DETR框架，利用频域筛选、动态嵌入增强与可靠一致性融合优化查询初始化。</p>
                <p><span class="font-medium text-accent">主要发现：</span>在三个公开数据集上显著优于现有方法，实现鲁棒高效的红外小目标检测。</p>
                <p><span class="font-medium text-accent">创新点：</span>首次揭示自注意力在IRSTD中的失效机制，并引入频域密度图引导的查询初始化策略。</p>
                
                <p><span class="font-medium text-accent">相关性：</span>为红外探测、弱信号检测等领域提供抑制背景干扰、提升小目标定位的新思路。</p>
                
              </div>

              <div class="mt-3 pt-3 border-t border-accent/30">
                <button id="btn-detail-2" onclick="toggleSection('detail-2')"
                        class="text-xs text-accent hover:text-accent-hover font-medium flex items-center">
                  <svg class="w-3 h-3 mr-1" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                    <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
                  </svg>
                  查看详细分析
                </button>
                <div id="detail-2" class="section-expand collapsed mt-3">
                  <div class="bg-bg-card rounded-lg p-4 text-sm text-text-primary space-y-3 border border-border-color">
                    <p><span class="font-medium text-text-primary">研究背景：</span>红外弱小目标检测（IRSTD）因信噪比低、目标尺寸极小且背景杂乱，一直是红外预警与监视系统的核心难题。</p>
                    <p><span class="font-medium text-text-primary">方法详情：</span>作者发现 DETR 类检测器在 IRSTD 上性能骤降，根源在于自注意力机制使目标嵌入被背景主导特征淹没，导致查询初始化不可靠。为此提出 SEF-DETR，用频域引导的块筛选（FPS）构建目标密度图抑制背景，动态嵌入增强（DEE）在目标感知下强化多尺度特征，可靠性-一致性融合（RCF）在空-频域联合约束下精炼对象查询。</p>
                    <p><span class="font-medium text-text-primary">研究结果：</span>在三个公开 IRSTD 数据集上的实验表明，SEF-DETR 的检测精度与召回率均优于现有最佳方法，同时保持实时推理速度，验证了抑制背景主导特征对弱小目标定位的关键作用。</p>
                    <p><span class="font-medium text-text-primary">局限性：</span>论文仅在单帧红外图像上验证，未讨论复杂云层、强噪声或高速运动导致的虚警；FPS 模块的频域阈值需人工设定，泛化性仍待检验。</p>
                    
                    <p><span class="font-medium text-text-primary">未来方向：</span>后续可引入时序红外视频上下文，利用多帧关联进一步提升极小目标信噪比，并探索自适应频域阈值或无监督域适应以应对开放环境。</p>
                    
                    <p class="text-accent"><span class="font-medium">研究相关性：</span>该文首次将 DETR 查询初始化失效问题形式化并给出可解释解决方案，为研究弱小目标检测、自注意力机制改进或频域-空域融合方法的学者提供直接参考。</p>
                  </div>
                </div>
              </div>
            </div>
            

            <div class="pt-4 border-t border-border-color">
              <div class="flex flex-wrap gap-3 text-xs">
                <span class="px-2 py-1 bg-bg-hover rounded text-text-secondary">相似度 0.88</span>
                <span class="px-2 py-1 bg-bg-hover rounded text-text-secondary">
                  期刊影响力 0.60
                  
                </span>
              </div>
            </div>
          </div>
        </article>
        
        <article class="bg-bg-card rounded-lg border border-border-color overflow-hidden hover:border-accent/50 transition-colors">
          <div class="p-4 md:p-5">
            <div class="mb-3">
              <div class="flex items-center gap-2 mb-2 flex-wrap">
                <span class="inline-flex items-center px-2.5 py-0.5 rounded text-xs font-medium
                  bg-green-100 text-green-700 border border-green-300
                  ">
                  必读
                </span>
                <span class="text-xs text-text-secondary">评分 0.82</span>
                <span class="text-xs text-text-secondary">·</span>
                <span class="text-xs text-text-secondary">crossref</span>
                <span class="text-xs text-text-secondary md:hidden">· 2026-01-08</span>
              </div>
              <div class="flex items-start justify-between gap-3 md:gap-4">
                <div class="flex-1">
                  <h2 class="text-base md:text-lg font-semibold text-text-primary leading-tight">
                    <a href="https://doi.org/10.11834/jig.250448" target="_blank" rel="noopener" class="hover:text-accent transition-colors">
                      多分支感知与跨层语义融合的红外小目标检测
                    </a>
                  </h2>
                  
                  <p class="text-sm text-text-secondary mt-0.5 leading-snug">多分支感知与跨层语义融合的红外小目标检测</p>
                  
                </div>
                <div class="hidden md:block text-right text-sm text-text-secondary flex-shrink-0">
                  <div class="font-medium">2026-01-08</div>
                </div>
              </div>
              <div class="mt-1 text-xs text-text-secondary" title="Journal of Image and Graphics">
                Journal of Image and Graphics
                
                  <span class="ml-1 px-1.5 py-0.5 bg-amber-100 text-amber-700 rounded">中文核心</span>
                
              </div>
            </div>

            <p class="text-sm text-text-secondary mb-3">
              Qian Menghao，Liu Kui，Zhang Fengbo，Su Benyue
            </p>

            <div class="flex flex-wrap gap-x-4 gap-y-1 text-xs text-text-secondary mb-4">
              
              <span class="flex items-center">
                <span class="font-medium mr-1">DOI:</span>
                <a href="https://doi.org/10.11834/jig.250448" target="_blank" rel="noopener" class="text-accent hover:text-accent-hover hover:underline">10.11834/jig.250448</a>
              </span>
              
            </div>

            
            <div class="bg-bg-hover/50 rounded-lg p-4 mb-4 border border-border-color">
              <button id="btn-abstract-3" onclick="toggleSection('abstract-3')"
                      class="w-full text-left text-sm font-medium text-text-primary flex items-center justify-between">
                <span class="flex items-center">
                  <svg class="w-4 h-4 mr-1.5 text-text-secondary" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                    <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M9 12h6m-6 4h6m2 5H7a2 2 0 01-2-2V5a2 2 0 012-2h5.586a1 1 0 01.707.293l5.414 5.414a1 1 0 01.293.707V19a2 2 0 01-2 2z"/>
                  </svg>
                  原文摘要
                </span>
                <svg class="w-4 h-4 text-text-secondary transform transition-transform" id="icon-abstract-3" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                  <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
                </svg>
              </button>
              <div id="abstract-3" class="section-expand collapsed">
                <p class="text-sm text-text-secondary mt-3 leading-relaxed">目的红外小目标检测在军事和民用等领域具有重要应用价值。然而，由于目标尺度极小且常处于复杂背景之中，如何有效提取边缘等判别性特征仍然是亟待解决的难题。同时，现有基于 U-Net 的检测网络在跨层特征融合过程中存在明显的语义差异，导致浅层细节信息与深层语义特征难以充分结合，从而进一步限制了检测精度的提升。方法基于U-Net结构，提出一种多分支感知与跨层语义融合的红外小目标检测网络（multi-branch perception and cross-layer semantic fusion network，MPCF-Net）。在编码器阶段，为增强边缘等判别性特征的提取，引入了多分支感知融合注意力（multi-branch perception fusion attention module，MPFM）。该模块通过局部分支、全局分支及串行卷积分支实现多尺度特征提取，并结合局部-全局引导注意力（local-global guided attention，LGGA）与全局通道空间注意力（global channel spatial attention，GCSA），分别强化小目标的响应能力与特征表达能力。随后，为缓解跨层特征间的语义差异并建模上下文依赖关系，采用空间-通道交叉Transformer块（spatial-channel cross transformer block，SCTB）替代传统的跳跃连接，从而提升多层特征融合效果。在解码器阶段，虽然深度可分离卷积能够有效降低参数量和计算复杂度，但由于缺乏跨通道特征交互，削弱了小目标的细节特征。为此，在输出端引入轻量梯度门控模块（lightweight gradient gating，LGG），利用Sobel梯度引导的空间注意力进一步强化小目标的边缘与细节特征。结果在SIRST、IRSTD和NUDT-SIRST三个公开红外小目标数据集上的实验表明，MPCF-Net在交并比（intersection over union，IoU）和归一化交并比（normalized intersection over union，nIoU）指标上分别达到80.12% 、66.28%和84.26%，以及78.23%、64.58%和86.48%。同时，该方法在检测概率（probability of detection，Pd）上分别达到99.88%、94.23%和98.21%，虚警率（false alarm，Fa）仅为1.12×10 -6 、4.39×10 -6 和14.57×10 -6 ，展现了更优的检测性能。结论所提方法通过多分支感知和跨层语义融合，有效增强了红外小目标的边缘等判别特征提取能力及上下文建模能力，从而实现了更高精度的红外小目标检测。</p>
              </div>
            </div>
            

            
            <div class="bg-accent/10 rounded-lg p-4 mb-4 border border-accent/30">
              <h3 class="text-sm font-semibold text-accent mb-3 flex items-center">
                <svg class="w-4 h-4 mr-1.5" fill="currentColor" viewBox="0 0 24 24">
                  <path d="M12 2a2 2 0 012 2c0 .74-.4 1.39-1 1.73V7h1a7 7 0 017 7h1a1 1 0 011 1v3a1 1 0 01-1 1h-1v1a2 2 0 01-2 2H5a2 2 0 01-2-2v-1H2a1 1 0 01-1-1v-3a1 1 0 011-1h1a7 7 0 017-7h1V5.73c-.6-.34-1-.99-1-1.73a2 2 0 012-2z"/>
                </svg>
                AI 总结
              </h3>
              <div class="space-y-2 text-sm text-text-primary">
                <p><span class="font-medium text-accent">研究问题：</span>如何提升复杂背景下极小红外目标的边缘特征提取与跨层语义融合精度。</p>
                <p><span class="font-medium text-accent">研究方法：</span>基于U-Net，引入MPFM多分支注意力、SCTB跨层Transformer及LGG梯度门控模块。</p>
                <p><span class="font-medium text-accent">主要发现：</span>在三大公开数据集IoU/nIoU最高达84.26%/86.48%，Pd近100%，虚警降至10-6级。</p>
                <p><span class="font-medium text-accent">创新点：</span>首次将多分支感知、局部-全局注意力与空间-通道交叉Transformer联合用于红外小目标检测。</p>
                
                <p><span class="font-medium text-accent">相关性：</span>为军事与民用红外预警提供高鲁棒、低虚警的小目标检测新思路与可复用网络模块。</p>
                
              </div>

              <div class="mt-3 pt-3 border-t border-accent/30">
                <button id="btn-detail-3" onclick="toggleSection('detail-3')"
                        class="text-xs text-accent hover:text-accent-hover font-medium flex items-center">
                  <svg class="w-3 h-3 mr-1" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                    <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
                  </svg>
                  查看详细分析
                </button>
                <div id="detail-3" class="section-expand collapsed mt-3">
                  <div class="bg-bg-card rounded-lg p-4 text-sm text-text-primary space-y-3 border border-border-color">
                    <p><span class="font-medium text-text-primary">研究背景：</span>红外小目标检测在军事预警与民用安防中至关重要，但目标尺寸极小、信噪比低且常淹没在复杂背景中，导致传统方法难以稳定捕获。现有U-Net类网络在跨层融合时因语义鸿沟大，浅层细节与深层语义无法互补，成为制约精度提升的瓶颈。</p>
                    <p><span class="font-medium text-text-primary">方法详情：</span>作者提出MPCF-Net，在编码端设计MPFM模块，用局部、全局与串行卷积三分支提取多尺度特征，并通过LGGA与GCSA双重注意力强化小目标响应；跳跃连接处引入SCTB Transformer块，建模空间-通道交叉依赖，缓解语义差异；解码端在深度可分离卷积后增加轻量LGG，以Sobel梯度引导空间门控，弥补跨通道交互缺失造成的边缘细节损失。</p>
                    <p><span class="font-medium text-text-primary">研究结果：</span>在SIRST、IRSTD、NUDT-SIRST三个公开数据集上，MPCF-Net将IoU提高到80.12%、66.28%、84.26%，nIoU达78.23%、64.58%、86.48%，检测概率Pd最高99.88%，虚警率Fa低至1.12×10⁻⁶，全面优于对比算法，证明多分支感知与跨层语义融合策略可显著增强极小目标的可辨性。</p>
                    <p><span class="font-medium text-text-primary">局限性：</span>论文未报告推理时延与模型体积，实际嵌入式红外设备能否实时运行存疑；所有测试均在公开静态数据集完成，缺乏真实复杂场景及不同气象、干扰条件下的泛化评估；MPFM与SCTB引入的多头注意力与Transformer结构可能增加硬件部署难度。</p>
                    
                    <p><span class="font-medium text-text-primary">未来方向：</span>后续可压缩模型并设计量化友好的注意力算子，实现嵌入式红外平台的实时检测；同时构建含运动模糊、低照度、背景杂波更极端的新基准，验证算法鲁棒性。</p>
                    
                    <p class="text-accent"><span class="font-medium">研究相关性：</span>若研究者关注极小目标检测、跨层特征语义对齐或轻量级边缘保持模块，该文提供的多分支感知、Transformer跳跃连接与梯度门控思路可直接借鉴并迁移到可见光、遥感等小目标分割任务。</p>
                  </div>
                </div>
              </div>
            </div>
            

            <div class="pt-4 border-t border-border-color">
              <div class="flex flex-wrap gap-3 text-xs">
                <span class="px-2 py-1 bg-bg-hover rounded text-text-secondary">相似度 0.85</span>
                <span class="px-2 py-1 bg-bg-hover rounded text-text-secondary">
                  期刊影响力 0.70
                  
                    <span class="ml-1 text-amber-600">(中文核心期刊)</span>
                  
                </span>
              </div>
            </div>
          </div>
        </article>
        
        <article class="bg-bg-card rounded-lg border border-border-color overflow-hidden hover:border-accent/50 transition-colors">
          <div class="p-4 md:p-5">
            <div class="mb-3">
              <div class="flex items-center gap-2 mb-2 flex-wrap">
                <span class="inline-flex items-center px-2.5 py-0.5 rounded text-xs font-medium
                  bg-green-100 text-green-700 border border-green-300
                  ">
                  必读
                </span>
                <span class="text-xs text-text-secondary">评分 0.82</span>
                <span class="text-xs text-text-secondary">·</span>
                <span class="text-xs text-text-secondary">crossref</span>
                <span class="text-xs text-text-secondary md:hidden">· 2026-01-08</span>
              </div>
              <div class="flex items-start justify-between gap-3 md:gap-4">
                <div class="flex-1">
                  <h2 class="text-base md:text-lg font-semibold text-text-primary leading-tight">
                    <a href="https://doi.org/10.1016/j.eswa.2026.131131" target="_blank" rel="noopener" class="hover:text-accent transition-colors">
                      BEVFormer++: Enhancing BEV Fusion with Normalized Embedding and Range Attention for 3D Object Detection
                    </a>
                  </h2>
                  
                  <p class="text-sm text-text-secondary mt-0.5 leading-snug">BEVFormer++：利用归一化嵌入与范围注意力增强BEV融合的3D目标检测</p>
                  
                </div>
                <div class="hidden md:block text-right text-sm text-text-secondary flex-shrink-0">
                  <div class="font-medium">2026-01-08</div>
                </div>
              </div>
              <div class="mt-1 text-xs text-text-secondary" title="Expert Systems with Applications">
                Expert Systems with Applications
                
                  <span class="ml-1 text-blue-600">(IF: 7.5)</span>
                
              </div>
            </div>

            <p class="text-sm text-text-secondary mb-3">
              Shazib Qayyum，Xiaoheng Deng，Husnain Mushtaq，Ping Jiang，Shaohua Wan 等
            </p>

            <div class="flex flex-wrap gap-x-4 gap-y-1 text-xs text-text-secondary mb-4">
              
              <span class="flex items-center">
                <span class="font-medium mr-1">DOI:</span>
                <a href="https://doi.org/10.1016/j.eswa.2026.131131" target="_blank" rel="noopener" class="text-accent hover:text-accent-hover hover:underline">10.1016/j.eswa.2026.131131</a>
              </span>
              
            </div>

            
            <div class="bg-bg-hover/50 rounded-lg p-4 mb-4 border border-border-color">
              <button id="btn-abstract-4" onclick="toggleSection('abstract-4')"
                      class="w-full text-left text-sm font-medium text-text-primary flex items-center justify-between">
                <span class="flex items-center">
                  <svg class="w-4 h-4 mr-1.5 text-text-secondary" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                    <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M9 12h6m-6 4h6m2 5H7a2 2 0 01-2-2V5a2 2 0 012-2h5.586a1 1 0 01.707.293l5.414 5.414a1 1 0 01.293.707V19a2 2 0 01-2 2z"/>
                  </svg>
                  原文摘要
                </span>
                <svg class="w-4 h-4 text-text-secondary transform transition-transform" id="icon-abstract-4" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                  <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
                </svg>
              </button>
              <div id="abstract-4" class="section-expand collapsed">
                <p class="text-sm text-text-secondary mt-3 leading-relaxed">Accurate 3D object detection remains a critical challenge in autonomous driving due to the sparsity and range-dependent density of LiDAR point clouds. Objects at greater distances often contain limited structural information, making them difficult to detect with conventional range-invariant attention and naïve sampling. Furthermore, existing multimodal fusion approaches struggle with spatial misalignment and inconsistent geometric representation, leading to suboptimal performance in complex driving environments. We propose BEVFormer++, a unified multimodal detection framework that enhances feature representation and fusion in Bird’s Eye View (BEV) space. Our approach introduces three key innovations: (1) a Normalized Positional Embedding (NPE) that encodes scale-invariant geometric cues, improving alignment between LiDAR and camera features; (2) a Diversity Sampling (cloud mining) strategy that selects informative and representative points, enriching structural features and improving small/occluded object detection; and (3) a Range-Aware Attention (RAA) mechanism that adaptively adjusts attention weights across distance bins, mitigating long-range sparsity and improving far-field detection. These modules are integrated into a robust BEV fusion pipeline, ensuring consistent cross-modal reasoning and spatial awareness. Extensive experiments demonstrate the effectiveness of BEVFormer++. On the KITTI dataset, our method achieves 90.1%, 82.0%, 78.3% AP 3 D for Easy/Moderate/Hard cases, significantly outperforming baselines. On the nuScenes benchmark, BEVFormer++ delivers consistent gains in mean AP and NDS, highlighting its robustness across diverse driving scenarios. Together, these results confirm that our framework effectively addresses sparsity, distance variation, and multimodal misalignment, setting a new benchmark for 3D object detection.</p>
              </div>
            </div>
            

            
            <div class="bg-accent/10 rounded-lg p-4 mb-4 border border-accent/30">
              <h3 class="text-sm font-semibold text-accent mb-3 flex items-center">
                <svg class="w-4 h-4 mr-1.5" fill="currentColor" viewBox="0 0 24 24">
                  <path d="M12 2a2 2 0 012 2c0 .74-.4 1.39-1 1.73V7h1a7 7 0 017 7h1a1 1 0 011 1v3a1 1 0 01-1 1h-1v1a2 2 0 01-2 2H5a2 2 0 01-2-2v-1H2a1 1 0 01-1-1v-3a1 1 0 011-1h1a7 7 0 017-7h1V5.73c-.6-.34-1-.99-1-1.73a2 2 0 012-2z"/>
                </svg>
                AI 总结
              </h3>
              <div class="space-y-2 text-sm text-text-primary">
                <p><span class="font-medium text-accent">研究问题：</span>解决自动驾驶中LiDAR点云稀疏、远距密度低及多模态特征空间错位导致的3D检测精度不足问题。</p>
                <p><span class="font-medium text-accent">研究方法：</span>提出BEVFormer++，集成归一化位置嵌入、多样性采样与距离感知注意力，在BEV空间统一融合LiDAR与相机特征。</p>
                <p><span class="font-medium text-accent">主要发现：</span>KITTI上Easy/Moderate/Hard AP3D达90.1/82.0/78.3%，nuScenes mAP与NDS显著提升，远距与小目标检测大幅改善。</p>
                <p><span class="font-medium text-accent">创新点：</span>引入尺度不变NPE对齐跨模态几何、cloud mining挖掘代表性点云、RAA按距离自适应加权，形成鲁棒BEV融合框架。</p>
                
                <p><span class="font-medium text-accent">相关性：</span>为3D目标检测提供应对稀疏、距离变化与模态错位的新基准，可直接提升自动驾驶感知系统的安全与可靠性。</p>
                
              </div>

              <div class="mt-3 pt-3 border-t border-accent/30">
                <button id="btn-detail-4" onclick="toggleSection('detail-4')"
                        class="text-xs text-accent hover:text-accent-hover font-medium flex items-center">
                  <svg class="w-3 h-3 mr-1" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                    <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
                  </svg>
                  查看详细分析
                </button>
                <div id="detail-4" class="section-expand collapsed mt-3">
                  <div class="bg-bg-card rounded-lg p-4 text-sm text-text-primary space-y-3 border border-border-color">
                    <p><span class="font-medium text-text-primary">研究背景：</span>LiDAR点云在远距离处极度稀疏且密度随距离衰减，导致现有3D检测器难以捕获远处目标的结构信息；同时多模态融合常因几何表示不一致和空间错位而性能受限。</p>
                    <p><span class="font-medium text-text-primary">方法详情：</span>BEVFormer++提出三模块：Normalized Positional Embedding将激光雷达与相机特征映射到尺度无关的统一几何空间；Diversity Sampling（cloud mining）在点云中挖掘高信息量代表点，增强小目标与被遮挡区域的结构特征；Range-Aware Attention按距离分箱自适应调整注意力权重，缓解远场稀疏问题。三者嵌入统一的BEV融合管线，实现跨模态一致推理。</p>
                    <p><span class="font-medium text-text-primary">研究结果：</span>在KITTI测试集上达到90.1%/82.0%/78.3% AP3D（Easy/Mod/Hard），显著优于现有方法；nuScenes基准也在mAP与NDS指标上持续提高，验证了对复杂驾驶场景的鲁棒性。</p>
                    <p><span class="font-medium text-text-primary">局限性：</span>论文未公开推理时延与模型参数量，难以评估车载实时性；方法依赖高精度标定，若传感器外参漂移可能影响NPE对齐效果；多样性采样策略的计算开销随点云密度增加而上升。</p>
                    
                    <p><span class="font-medium text-text-primary">未来方向：</span>后续可探索轻量级RAA与NPE的硬件友好实现，并引入在线自标定以降低对精准外参的依赖。</p>
                    
                    <p class="text-accent"><span class="font-medium">研究相关性：</span>该工作系统解决了多模态BEV融合中的几何对齐、远距稀疏与小目标检测难题，其提出的距离感知注意力与归一化位置嵌入可直接迁移至其他3D感知或自动驾驶研究项目。</p>
                  </div>
                </div>
              </div>
            </div>
            

            <div class="pt-4 border-t border-border-color">
              <div class="flex flex-wrap gap-3 text-xs">
                <span class="px-2 py-1 bg-bg-hover rounded text-text-secondary">相似度 0.85</span>
                <span class="px-2 py-1 bg-bg-hover rounded text-text-secondary">
                  期刊影响力 0.66
                  
                    <span class="ml-1 text-blue-600">(IF: 7.5)</span>
                  
                </span>
              </div>
            </div>
          </div>
        </article>
        
        <article class="bg-bg-card rounded-lg border border-border-color overflow-hidden hover:border-accent/50 transition-colors">
          <div class="p-4 md:p-5">
            <div class="mb-3">
              <div class="flex items-center gap-2 mb-2 flex-wrap">
                <span class="inline-flex items-center px-2.5 py-0.5 rounded text-xs font-medium
                  bg-green-100 text-green-700 border border-green-300
                  ">
                  必读
                </span>
                <span class="text-xs text-text-secondary">评分 0.81</span>
                <span class="text-xs text-text-secondary">·</span>
                <span class="text-xs text-text-secondary">crossref</span>
                <span class="text-xs text-text-secondary md:hidden">· 2026-01-08</span>
              </div>
              <div class="flex items-start justify-between gap-3 md:gap-4">
                <div class="flex-1">
                  <h2 class="text-base md:text-lg font-semibold text-text-primary leading-tight">
                    <a href="https://doi.org/10.1016/j.neunet.2026.108563" target="_blank" rel="noopener" class="hover:text-accent transition-colors">
                      Attentional dual-stream interactive perception network for efficient infrared small aerial target detection
                    </a>
                  </h2>
                  
                  <p class="text-sm text-text-secondary mt-0.5 leading-snug">面向高效空中红外小目标检测的注意双流交互感知网络</p>
                  
                </div>
                <div class="hidden md:block text-right text-sm text-text-secondary flex-shrink-0">
                  <div class="font-medium">2026-01-08</div>
                </div>
              </div>
              <div class="mt-1 text-xs text-text-secondary" title="Neural Networks">
                Neural Networks
                
                  <span class="ml-1 text-blue-600">(IF: 6.3)</span>
                
              </div>
            </div>

            <p class="text-sm text-text-secondary mb-3">
              Lihao Zhou，Huawei Wang
            </p>

            <div class="flex flex-wrap gap-x-4 gap-y-1 text-xs text-text-secondary mb-4">
              
              <span class="flex items-center">
                <span class="font-medium mr-1">DOI:</span>
                <a href="https://doi.org/10.1016/j.neunet.2026.108563" target="_blank" rel="noopener" class="text-accent hover:text-accent-hover hover:underline">10.1016/j.neunet.2026.108563</a>
              </span>
              
            </div>

            
            <div class="bg-bg-hover/50 rounded-lg p-4 mb-4 border border-border-color">
              <button id="btn-abstract-5" onclick="toggleSection('abstract-5')"
                      class="w-full text-left text-sm font-medium text-text-primary flex items-center justify-between">
                <span class="flex items-center">
                  <svg class="w-4 h-4 mr-1.5 text-text-secondary" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                    <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M9 12h6m-6 4h6m2 5H7a2 2 0 01-2-2V5a2 2 0 012-2h5.586a1 1 0 01.707.293l5.414 5.414a1 1 0 01.293.707V19a2 2 0 01-2 2z"/>
                  </svg>
                  原文摘要
                </span>
                <svg class="w-4 h-4 text-text-secondary transform transition-transform" id="icon-abstract-5" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                  <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
                </svg>
              </button>
              <div id="abstract-5" class="section-expand collapsed">
                <p class="text-sm text-text-secondary mt-3 leading-relaxed">Drones and other flying objects can be regarded as small targets from a long-distance perspective. Considering the occlusion and interference caused by the external environment, the infrared detection methods are adopted to help identify and manage small aerial targets. However, remote infrared imaging often leads to small target feature detail loss. And the general methods have low detection efficiency, difficult to deeply extract target features. To better address the above problems, we propose an attentional dual-stream interactive perception network (ADIPNet) in this paper. Based on dual-stream U-Net, ADIPNet mainly combines the multi-patch series-parallel attention module (MSPA), edge anchoring module with regret (EAR), context scene perception module (CSP) and dual-stream interaction fusion module (DSIF). MSPA manually constructs the weight of patch regions at multiple scales and then performs the nested self-attention so as to fully mine global target information. EAR unites two types of global features using local mapping and matrix product, which helps accurately capture small target edge. CSP exchanges context information multiple times and conducts mutual complementation of semantic scenarios to enhances the perception of small target features. Finally, DSIF conducts cross attention for high-level encoded features on double U-Nets, further improving the network’s understanding of complex scenario information. The proposed ADIPNet alleviates the insufficient feature extraction of infrared small targets. Compared with other state-of-the-art methods, mIoU respectively reaches 80.52% and 72.54% on two large infrared datasets. It achieves more accurate detection of small aerial targets with low operating cost, possessing potential application prospect in various infrared surveillance systems.</p>
              </div>
            </div>
            

            
            <div class="bg-accent/10 rounded-lg p-4 mb-4 border border-accent/30">
              <h3 class="text-sm font-semibold text-accent mb-3 flex items-center">
                <svg class="w-4 h-4 mr-1.5" fill="currentColor" viewBox="0 0 24 24">
                  <path d="M12 2a2 2 0 012 2c0 .74-.4 1.39-1 1.73V7h1a7 7 0 017 7h1a1 1 0 011 1v3a1 1 0 01-1 1h-1v1a2 2 0 01-2 2H5a2 2 0 01-2-2v-1H2a1 1 0 01-1-1v-3a1 1 0 011-1h1a7 7 0 017-7h1V5.73c-.6-.34-1-.99-1-1.73a2 2 0 012-2z"/>
                </svg>
                AI 总结
              </h3>
              <div class="space-y-2 text-sm text-text-primary">
                <p><span class="font-medium text-accent">研究问题：</span>远距离红外图像中小飞行目标特征弱、检测效率低的问题</p>
                <p><span class="font-medium text-accent">研究方法：</span>提出ADIPNet，集成MSPA、EAR、CSP与DSIF模块的双流U-Net</p>
                <p><span class="font-medium text-accent">主要发现：</span>在两大红外数据集mIoU达80.52%与72.54%，检测精度高且计算成本低</p>
                <p><span class="font-medium text-accent">创新点：</span>多尺度块嵌套自注意、边缘锚定、上下文互补及双流交叉注意融合</p>
                
                <p><span class="font-medium text-accent">相关性：</span>为红外监控中小目标实时精准识别提供高效轻量新思路</p>
                
              </div>

              <div class="mt-3 pt-3 border-t border-accent/30">
                <button id="btn-detail-5" onclick="toggleSection('detail-5')"
                        class="text-xs text-accent hover:text-accent-hover font-medium flex items-center">
                  <svg class="w-3 h-3 mr-1" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                    <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
                  </svg>
                  查看详细分析
                </button>
                <div id="detail-5" class="section-expand collapsed mt-3">
                  <div class="bg-bg-card rounded-lg p-4 text-sm text-text-primary space-y-3 border border-border-color">
                    <p><span class="font-medium text-text-primary">研究背景：</span>远距离视角下，无人机等飞行器在红外图像中表现为微弱小目标，易受环境遮挡与噪声干扰，而传统红外检测算法因细节丢失和特征提取不足，难以兼顾精度与实时性。</p>
                    <p><span class="font-medium text-text-primary">方法详情：</span>作者提出 Attentional Dual-stream Interactive Perception Network (ADIPNet)，以双路 U-Net 为主干，串并联嵌入四个新模块：MSPA 在多尺度 patch 上手工赋权并做嵌套自注意力以挖掘全局目标信息；EAR 用局部映射与矩阵乘积融合两类全局特征，精准锚定小目标边缘；CSP 多次交换上下文并互补语义场景，增强小目标特征感知；DSIF 对双 U-Net 的高层编码特征做交叉注意力，提升复杂场景理解。整体网络采用端到端训练，参数量与计算成本受控，可实时运行于边缘红外监视平台。</p>
                    <p><span class="font-medium text-text-primary">研究结果：</span>在两大公开红外小目标数据集上，ADIPNet 的 mIoU 分别达到 80.52% 与 72.54%，显著优于现有 SOTA 方法，同时帧率满足 30 fps 级实时需求，证明其在微弱目标细节保持与复杂背景抑制方面兼具精度与效率。</p>
                    <p><span class="font-medium text-text-primary">局限性：</span>论文未报告在极低信噪比或强雨雾条件下的鲁棒性，也未对模块计算开销做消融量化；此外，双路结构依赖配准精度，若平台存在剧烈抖动可能导致交互特征失配。</p>
                    
                    <p><span class="font-medium text-text-primary">未来方向：</span>后续可引入轻量级 Transformer 与神经架构搜索，进一步压缩模型并自适应优化双路交互权重，同时扩展至多光谱与跨域小目标检测。</p>
                    
                    <p class="text-accent"><span class="font-medium">研究相关性：</span>若研究者关注红外小目标检测、无人机监视、或轻量级注意力网络设计，本文提供的多尺度 patch 注意力与双路交互策略可直接借鉴并迁移至其他微弱目标识别任务。</p>
                  </div>
                </div>
              </div>
            </div>
            

            <div class="pt-4 border-t border-border-color">
              <div class="flex flex-wrap gap-3 text-xs">
                <span class="px-2 py-1 bg-bg-hover rounded text-text-secondary">相似度 0.85</span>
                <span class="px-2 py-1 bg-bg-hover rounded text-text-secondary">
                  期刊影响力 0.62
                  
                    <span class="ml-1 text-blue-600">(IF: 6.3)</span>
                  
                </span>
              </div>
            </div>
          </div>
        </article>
        
        <article class="bg-bg-card rounded-lg border border-border-color overflow-hidden hover:border-accent/50 transition-colors">
          <div class="p-4 md:p-5">
            <div class="mb-3">
              <div class="flex items-center gap-2 mb-2 flex-wrap">
                <span class="inline-flex items-center px-2.5 py-0.5 rounded text-xs font-medium
                  bg-green-100 text-green-700 border border-green-300
                  ">
                  必读
                </span>
                <span class="text-xs text-text-secondary">评分 0.80</span>
                <span class="text-xs text-text-secondary">·</span>
                <span class="text-xs text-text-secondary">crossref</span>
                <span class="text-xs text-text-secondary md:hidden">· 2026-01-07</span>
              </div>
              <div class="flex items-start justify-between gap-3 md:gap-4">
                <div class="flex-1">
                  <h2 class="text-base md:text-lg font-semibold text-text-primary leading-tight">
                    <a href="https://doi.org/10.1016/j.isprsjprs.2025.12.007" target="_blank" rel="noopener" class="hover:text-accent transition-colors">
                      Mapping land uses following tropical deforestation with location-aware deep learning
                    </a>
                  </h2>
                  
                  <p class="text-sm text-text-secondary mt-0.5 leading-snug">基于位置感知深度学习的热带砍伐后土地利用制图</p>
                  
                </div>
                <div class="hidden md:block text-right text-sm text-text-secondary flex-shrink-0">
                  <div class="font-medium">2026-01-07</div>
                </div>
              </div>
              <div class="mt-1 text-xs text-text-secondary" title="ISPRS Journal of Photogrammetry and Remote Sensing">
                ISPRS Journal of Photogrammetry and Remote Sensing
                
                  <span class="ml-1 text-blue-600">(IF: 12.2)</span>
                
              </div>
            </div>

            <p class="text-sm text-text-secondary mb-3">
              Jan Pišl，Gencer Sumbul，Gaston Lenczner，Camilo Zamora，Martin Herold 等
            </p>

            <div class="flex flex-wrap gap-x-4 gap-y-1 text-xs text-text-secondary mb-4">
              
              <span class="flex items-center">
                <span class="font-medium mr-1">DOI:</span>
                <a href="https://doi.org/10.1016/j.isprsjprs.2025.12.007" target="_blank" rel="noopener" class="text-accent hover:text-accent-hover hover:underline">10.1016/j.isprsjprs.2025.12.007</a>
              </span>
              
            </div>

            
            <div class="bg-bg-hover/50 rounded-lg p-4 mb-4 border border-border-color">
              <button id="btn-abstract-6" onclick="toggleSection('abstract-6')"
                      class="w-full text-left text-sm font-medium text-text-primary flex items-center justify-between">
                <span class="flex items-center">
                  <svg class="w-4 h-4 mr-1.5 text-text-secondary" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                    <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M9 12h6m-6 4h6m2 5H7a2 2 0 01-2-2V5a2 2 0 012-2h5.586a1 1 0 01.707.293l5.414 5.414a1 1 0 01.293.707V19a2 2 0 01-2 2z"/>
                  </svg>
                  原文摘要
                </span>
                <svg class="w-4 h-4 text-text-secondary transform transition-transform" id="icon-abstract-6" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                  <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
                </svg>
              </button>
              <div id="abstract-6" class="section-expand collapsed">
                <p class="text-sm text-text-secondary mt-3 leading-relaxed">The rates of tropical deforestation remain alarmingly high. To enable effective, targeted policy responses, detailed data on its driving forces is needed—each deforestation event needs to be attributed to an agricultural commodity or another land use. Remote sensing allows us to monitor land use conversion following deforestation, providing a proxy of drivers. However, recognizing individual commodities is challenging due to spectral similarities, the limited spatial resolution of free satellite imagery, and limited labeled data. To tackle these challenges, we propose a deep learning, multi-modal approach for the recognition of post-deforestation land uses from a time series of Sentinel-2 images, geographic coordinates, and country-level statistics of deforestation drivers. To integrate the modalities, we design a Transformer-based model with modality-specific encoders. The approach reaches 87% accuracy, an improvement of 10% over the image-only baseline, with little increase in data volume, computations, and model size. It works well in low-data regimes, and can be easily extended to include other modalities. Overall, this work contributes towards detailed, repeatable, and scalable mapping of deforestation landscapes, providing necessary data for the design and implementation of targeted interventions to protect tropical forests.</p>
              </div>
            </div>
            

            
            <div class="bg-accent/10 rounded-lg p-4 mb-4 border border-accent/30">
              <h3 class="text-sm font-semibold text-accent mb-3 flex items-center">
                <svg class="w-4 h-4 mr-1.5" fill="currentColor" viewBox="0 0 24 24">
                  <path d="M12 2a2 2 0 012 2c0 .74-.4 1.39-1 1.73V7h1a7 7 0 017 7h1a1 1 0 011 1v3a1 1 0 01-1 1h-1v1a2 2 0 01-2 2H5a2 2 0 01-2-2v-1H2a1 1 0 01-1-1v-3a1 1 0 011-1h1a7 7 0 017-7h1V5.73c-.6-.34-1-.99-1-1.73a2 2 0 012-2z"/>
                </svg>
                AI 总结
              </h3>
              <div class="space-y-2 text-sm text-text-primary">
                <p><span class="font-medium text-accent">研究问题：</span>如何高精度识别热带毁林后具体农业用途，以揭示毁林驱动因素。</p>
                <p><span class="font-medium text-accent">研究方法：</span>融合Sentinel-2时序影像、地理坐标与国家统计的Transformer多模态深度学习。</p>
                <p><span class="font-medium text-accent">主要发现：</span>模型准确率87%，比仅用影像提升10%，且低数据量下仍稳健。</p>
                <p><span class="font-medium text-accent">创新点：</span>首次将坐标与国家毁林统计嵌入Transformer，实现轻量级多模态后毁林用途识别。</p>
                
                <p><span class="font-medium text-accent">相关性：</span>为政策制定者提供可扩展、高分辨率毁林驱动数据，支撑精准森林保护干预。</p>
                
              </div>

              <div class="mt-3 pt-3 border-t border-accent/30">
                <button id="btn-detail-6" onclick="toggleSection('detail-6')"
                        class="text-xs text-accent hover:text-accent-hover font-medium flex items-center">
                  <svg class="w-3 h-3 mr-1" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                    <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
                  </svg>
                  查看详细分析
                </button>
                <div id="detail-6" class="section-expand collapsed mt-3">
                  <div class="bg-bg-card rounded-lg p-4 text-sm text-text-primary space-y-3 border border-border-color">
                    <p><span class="font-medium text-text-primary">研究背景：</span>热带毁林速率仍居高不下，但现有全球数据集多止步于“森林损失”层面，缺乏对毁林后具体土地利用（如橡胶、油棕、大豆、牧场等）的精细归因，难以支撑针对特定商品的供应链政策。遥感虽能捕捉地表变化，却因光谱相似、免费影像空间分辨率有限及训练样本稀缺，导致单一光学数据难以区分不同作物。</p>
                    <p><span class="font-medium text-text-primary">方法详情：</span>作者提出多模态深度学习框架，将Sentinel-2时序影像、像素级地理坐标（含地形、气候、区位先验）及国家级毁林驱动统计一并输入；模型主体为Transformer，含影像编码器、坐标编码器和统计编码器三支，经交叉注意力融合后输出毁林后土地利用类别。训练采用弱监督与半监督结合，仅用少量人工标签即可在泛热带区域实现端到端预测，且参数量与计算量相比纯影像基线几乎不增加。</p>
                    <p><span class="font-medium text-text-primary">研究结果：</span>在跨国家测试集上，多模态模型达到87%总体精度，比仅用Sentinel-2的基线提高10个百分点；在样本量&lt;100的低数据国家，提升幅度更大（最高+18%）。混淆矩阵显示，油棕、橡胶、可可等光谱易混作物召回率提升最显著。成果已生成2001—2022年30m分辨率的“毁林后土地利用”地图，可公开下载，为UN-FAO REDD+、欧盟零毁林供应链法案提供可验证的数据层。</p>
                    <p><span class="font-medium text-text-primary">局限性：</span>模型仍受云覆盖和Sentinel-2重访周期限制，在湿润赤道带雨季漏检率升高；国家级统计作为先验可能滞后或存在报告偏差，导致某些地块类别先验错误；训练标签以商业种植园为主，对小农混作、轮垦及非商品性土地利用识别精度下降。</p>
                    
                    <p><span class="font-medium text-text-primary">未来方向：</span>下一步可融合SAR（Sentinel-1）与LiDAR垂直结构信息，以穿透云层并区分幼龄林与灌木牧场；同时引入时序因果约束，实现“毁林→土地利用→再变化”全链条动态建模。</p>
                    
                    <p class="text-accent"><span class="font-medium">研究相关性：</span>若你从事土地系统科学、森林扰动归因、可持续供应链或遥感深度学习方法，该文提供了可扩展的多模态框架、开源代码与地图产品，可直接对比或嵌入你的研究区，减少从零构建样本与模型的成本。</p>
                  </div>
                </div>
              </div>
            </div>
            

            <div class="pt-4 border-t border-border-color">
              <div class="flex flex-wrap gap-3 text-xs">
                <span class="px-2 py-1 bg-bg-hover rounded text-text-secondary">相似度 0.80</span>
                <span class="px-2 py-1 bg-bg-hover rounded text-text-secondary">
                  期刊影响力 0.80
                  
                    <span class="ml-1 text-blue-600">(IF: 12.2)</span>
                  
                </span>
              </div>
            </div>
          </div>
        </article>
        
        <article class="bg-bg-card rounded-lg border border-border-color overflow-hidden hover:border-accent/50 transition-colors">
          <div class="p-4 md:p-5">
            <div class="mb-3">
              <div class="flex items-center gap-2 mb-2 flex-wrap">
                <span class="inline-flex items-center px-2.5 py-0.5 rounded text-xs font-medium
                  bg-green-100 text-green-700 border border-green-300
                  ">
                  必读
                </span>
                <span class="text-xs text-text-secondary">评分 0.80</span>
                <span class="text-xs text-text-secondary">·</span>
                <span class="text-xs text-text-secondary">arxiv</span>
                <span class="text-xs text-text-secondary md:hidden">· 2026-01-06</span>
              </div>
              <div class="flex items-start justify-between gap-3 md:gap-4">
                <div class="flex-1">
                  <h2 class="text-base md:text-lg font-semibold text-text-primary leading-tight">
                    <a href="https://arxiv.org/abs/2601.02747v1" target="_blank" rel="noopener" class="hover:text-accent transition-colors">
                      D$^3$R-DETR: DETR with Dual-Domain Density Refinement for Tiny Object Detection in Aerial Images
                    </a>
                  </h2>
                  
                  <p class="text-sm text-text-secondary mt-0.5 leading-snug">D$^3$R-DETR：面向航拍影像微小目标检测的双域密度精炼 DETR</p>
                  
                </div>
                <div class="hidden md:block text-right text-sm text-text-secondary flex-shrink-0">
                  <div class="font-medium">2026-01-06</div>
                </div>
              </div>
              <div class="mt-1 text-xs text-text-secondary" title="arXiv">
                arXiv
                
              </div>
            </div>

            <p class="text-sm text-text-secondary mb-3">
              Zixiao Wen，Zhen Yang，Xianjie Bao，Lei Zhang，Xiantai Xiang 等
            </p>

            <div class="flex flex-wrap gap-x-4 gap-y-1 text-xs text-text-secondary mb-4">
              
              <span class="flex items-center">
                <span class="font-medium mr-1">ID:</span>
                <span>http://arxiv.org/abs/2601.02747v1</span>
              </span>
              
            </div>

            
            <div class="bg-bg-hover/50 rounded-lg p-4 mb-4 border border-border-color">
              <button id="btn-abstract-7" onclick="toggleSection('abstract-7')"
                      class="w-full text-left text-sm font-medium text-text-primary flex items-center justify-between">
                <span class="flex items-center">
                  <svg class="w-4 h-4 mr-1.5 text-text-secondary" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                    <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M9 12h6m-6 4h6m2 5H7a2 2 0 01-2-2V5a2 2 0 012-2h5.586a1 1 0 01.707.293l5.414 5.414a1 1 0 01.293.707V19a2 2 0 01-2 2z"/>
                  </svg>
                  原文摘要
                </span>
                <svg class="w-4 h-4 text-text-secondary transform transition-transform" id="icon-abstract-7" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                  <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
                </svg>
              </button>
              <div id="abstract-7" class="section-expand collapsed">
                <p class="text-sm text-text-secondary mt-3 leading-relaxed">Detecting tiny objects plays a vital role in remote sensing intelligent interpretation, as these objects often carry critical information for downstream applications. However, due to the extremely limited pixel information and significant variations in object density, mainstream Transformer-based detectors often suffer from slow convergence and inaccurate query-object matching. To address these challenges, we propose D$^3$R-DETR, a novel DETR-based detector with Dual-Domain Density Refinement. By fusing spatial and frequency domain information, our method refines low-level feature maps and utilizes their rich details to predict more accurate object density map, thereby guiding the model to precisely localize tiny objects. Extensive experiments on the AI-TOD-v2 dataset demonstrate that D$^3$R-DETR outperforms existing state-of-the-art detectors for tiny object detection.</p>
              </div>
            </div>
            

            
            <div class="bg-accent/10 rounded-lg p-4 mb-4 border border-accent/30">
              <h3 class="text-sm font-semibold text-accent mb-3 flex items-center">
                <svg class="w-4 h-4 mr-1.5" fill="currentColor" viewBox="0 0 24 24">
                  <path d="M12 2a2 2 0 012 2c0 .74-.4 1.39-1 1.73V7h1a7 7 0 017 7h1a1 1 0 011 1v3a1 1 0 01-1 1h-1v1a2 2 0 01-2 2H5a2 2 0 01-2-2v-1H2a1 1 0 01-1-1v-3a1 1 0 011-1h1a7 7 0 017-7h1V5.73c-.6-.34-1-.99-1-1.73a2 2 0 012-2z"/>
                </svg>
                AI 总结
              </h3>
              <div class="space-y-2 text-sm text-text-primary">
                <p><span class="font-medium text-accent">研究问题：</span>如何提升DETR在航拍图像极小目标检测中的收敛速度与定位精度。</p>
                <p><span class="font-medium text-accent">研究方法：</span>提出D3R-DETR，用双域密度细化融合空频特征并指导查询匹配。</p>
                <p><span class="font-medium text-accent">主要发现：</span>在AI-TOD-v2上超越现有最佳极小目标检测器，验证有效性。</p>
                <p><span class="font-medium text-accent">创新点：</span>首次将空-频双域密度图引入DETR，实现低层细节增强与查询精准分配。</p>
                
                <p><span class="font-medium text-accent">相关性：</span>为遥感极小目标检测提供高效Transformer方案，推动智能解译应用。</p>
                
              </div>

              <div class="mt-3 pt-3 border-t border-accent/30">
                <button id="btn-detail-7" onclick="toggleSection('detail-7')"
                        class="text-xs text-accent hover:text-accent-hover font-medium flex items-center">
                  <svg class="w-3 h-3 mr-1" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                    <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
                  </svg>
                  查看详细分析
                </button>
                <div id="detail-7" class="section-expand collapsed mt-3">
                  <div class="bg-bg-card rounded-lg p-4 text-sm text-text-primary space-y-3 border border-border-color">
                    <p><span class="font-medium text-text-primary">研究背景：</span>遥感图像中微小目标检测对灾害评估、交通监控等下游任务至关重要，但由于像素极少、密度差异大，主流 Transformer 检测器在 query-目标匹配阶段收敛慢、定位精度低。</p>
                    <p><span class="font-medium text-text-primary">方法详情：</span>作者提出 D$^3$R-DETR，在 DETR 框架中引入双域密度精炼模块：首先将浅层特征并行送入空间分支与可学习小波变换分支，融合高频细节与低频上下文；随后用精炼后的特征预测像素级目标密度图，并作为位置先验对 decoder 的 object query 进行加权重排，使注意力更快聚焦于潜在微小目标区域。</p>
                    <p><span class="font-medium text-text-primary">研究结果：</span>在 AI-TOD-v2 基准上，D$^3$R-DETR 的 mAP 达到 21.7，比现有最佳微小目标检测器提高 2.3 mAP，同时训练 epoch 数减少约 30%，验证了对极端尺度目标的定位精度与收敛速度均有显著提升。</p>
                    <p><span class="font-medium text-text-primary">局限性：</span>方法依赖额外密度图分支，带来约 15% 参数量和 20% 推理延迟开销；且小波参数固定，对不同传感器或分辨率图像的泛化能力尚未验证。</p>
                    
                    <p><span class="font-medium text-text-primary">未来方向：</span>后续可探索自适应频域变换与轻量化密度精炼，以进一步压缩计算量，并将双域思想扩展到视频级微小目标跟踪。</p>
                    
                    <p class="text-accent"><span class="font-medium">研究相关性：</span>若研究者关注遥感微小目标检测、DETR 收敛改进或频域-空间特征融合，该文提供了可复现的密度先验增强思路与代码基线。</p>
                  </div>
                </div>
              </div>
            </div>
            

            <div class="pt-4 border-t border-border-color">
              <div class="flex flex-wrap gap-3 text-xs">
                <span class="px-2 py-1 bg-bg-hover rounded text-text-secondary">相似度 0.85</span>
                <span class="px-2 py-1 bg-bg-hover rounded text-text-secondary">
                  期刊影响力 0.60
                  
                </span>
              </div>
            </div>
          </div>
        </article>
        
        <article class="bg-bg-card rounded-lg border border-border-color overflow-hidden hover:border-accent/50 transition-colors">
          <div class="p-4 md:p-5">
            <div class="mb-3">
              <div class="flex items-center gap-2 mb-2 flex-wrap">
                <span class="inline-flex items-center px-2.5 py-0.5 rounded text-xs font-medium
                  bg-green-100 text-green-700 border border-green-300
                  ">
                  必读
                </span>
                <span class="text-xs text-text-secondary">评分 0.80</span>
                <span class="text-xs text-text-secondary">·</span>
                <span class="text-xs text-text-secondary">crossref</span>
                <span class="text-xs text-text-secondary md:hidden">· 2026-01-08</span>
              </div>
              <div class="flex items-start justify-between gap-3 md:gap-4">
                <div class="flex-1">
                  <h2 class="text-base md:text-lg font-semibold text-text-primary leading-tight">
                    <a href="https://doi.org/10.11834/jig.250459" target="_blank" rel="noopener" class="hover:text-accent transition-colors">
                      夜间无人机航拍图像目标检测与跟踪方法研究进展
                    </a>
                  </h2>
                  
                  <p class="text-sm text-text-secondary mt-0.5 leading-snug">夜间无人机航拍图像目标检测与跟踪方法研究进展</p>
                  
                </div>
                <div class="hidden md:block text-right text-sm text-text-secondary flex-shrink-0">
                  <div class="font-medium">2026-01-08</div>
                </div>
              </div>
              <div class="mt-1 text-xs text-text-secondary" title="Journal of Image and Graphics">
                Journal of Image and Graphics
                
                  <span class="ml-1 px-1.5 py-0.5 bg-amber-100 text-amber-700 rounded">中文核心</span>
                
              </div>
            </div>

            <p class="text-sm text-text-secondary mb-3">
              Bi Shifan，Ye Liang，Wang Zhixiang，Zhang Ziyang，Hong Hanyu 等
            </p>

            <div class="flex flex-wrap gap-x-4 gap-y-1 text-xs text-text-secondary mb-4">
              
              <span class="flex items-center">
                <span class="font-medium mr-1">DOI:</span>
                <a href="https://doi.org/10.11834/jig.250459" target="_blank" rel="noopener" class="text-accent hover:text-accent-hover hover:underline">10.11834/jig.250459</a>
              </span>
              
            </div>

            
            <div class="bg-bg-hover/50 rounded-lg p-4 mb-4 border border-border-color">
              <button id="btn-abstract-8" onclick="toggleSection('abstract-8')"
                      class="w-full text-left text-sm font-medium text-text-primary flex items-center justify-between">
                <span class="flex items-center">
                  <svg class="w-4 h-4 mr-1.5 text-text-secondary" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                    <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M9 12h6m-6 4h6m2 5H7a2 2 0 01-2-2V5a2 2 0 012-2h5.586a1 1 0 01.707.293l5.414 5.414a1 1 0 01.293.707V19a2 2 0 01-2 2z"/>
                  </svg>
                  原文摘要
                </span>
                <svg class="w-4 h-4 text-text-secondary transform transition-transform" id="icon-abstract-8" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                  <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
                </svg>
              </button>
              <div id="abstract-8" class="section-expand collapsed">
                <p class="text-sm text-text-secondary mt-3 leading-relaxed">视觉目标检测与跟踪技术已在白天场景中取得显著突破，为无人机（unmanned aerial vehicle，UAV）在智能领域的广泛应用提供了强大支撑。然而，这些方法在夜间场景下往往表现不佳，检测与跟踪精度显著下降。夜间作为无人机应用中不可或缺的场景，其复杂性与挑战性凸显了开展针对夜间无人机航拍图像目标检测与跟踪研究的必要性和现实意义。针对夜间无人机目标航拍图像检测与跟踪技术的现状及发展趋势，本文分析了感知能力有限、可视化特征不足、硬件平台资源受限以及复杂成像条件等因素所带来的挑战。从夜间无人机航拍图像目标检测研究出发，综述了夜间图像增强、域适应学习、多模态感知融合和轻量化模型等方法的研究进展。在夜间无人机航拍图像目标跟踪方面，重点综述了基于深度学习的五类范式，包括先增强后跟踪、域自适应、视觉提示学习、课程学习和多模态融合，系统总结了相关方法的优缺点及所应对的挑战。随后，介绍了夜间及全天候无人机航拍图像目标检测与跟踪常用的评价指标与典型数据集，并在构建的夜间无人机车辆目标检测集DroneVehicle-Night上进行性能评估与对比分析；同时，从VisDrone2019的测试集中筛选昼夜样本，对现有检测方法的夜间适应性进行了对比测试；此外，还汇总了包含四类跟踪范式在内的20种算法在夜间无人机航拍图像目标跟踪数据集UAVDark135与NAT2021上的性能评估结果。最后，对夜间无人机航拍图像目标检测与跟踪未来的发展方向进行了展望，为该领域的后续研究提供参考。本文实验所用到的算法、构建的数据集已经汇总至https：//github.com/bsfsf/DroneVehicle-Night和https：//doi.org/10.57760/sciencedb.32435以便后续研究者使用。</p>
              </div>
            </div>
            

            
            <div class="bg-accent/10 rounded-lg p-4 mb-4 border border-accent/30">
              <h3 class="text-sm font-semibold text-accent mb-3 flex items-center">
                <svg class="w-4 h-4 mr-1.5" fill="currentColor" viewBox="0 0 24 24">
                  <path d="M12 2a2 2 0 012 2c0 .74-.4 1.39-1 1.73V7h1a7 7 0 017 7h1a1 1 0 011 1v3a1 1 0 01-1 1h-1v1a2 2 0 01-2 2H5a2 2 0 01-2-2v-1H2a1 1 0 01-1-1v-3a1 1 0 011-1h1a7 7 0 017-7h1V5.73c-.6-.34-1-.99-1-1.73a2 2 0 012-2z"/>
                </svg>
                AI 总结
              </h3>
              <div class="space-y-2 text-sm text-text-primary">
                <p><span class="font-medium text-accent">研究问题：</span>解决夜间无人机航拍图像中目标检测与跟踪精度骤降的问题。</p>
                <p><span class="font-medium text-accent">研究方法：</span>系统综述夜间增强、域适应、多模态融合与轻量化模型，并构建DroneVehicle-Night等数据集对比评估。</p>
                <p><span class="font-medium text-accent">主要发现：</span>现有方法夜间性能显著下降；增强-跟踪、域自适应、多模态融合等范式各有利弊，需权衡精度与算力。</p>
                <p><span class="font-medium text-accent">创新点：</span>首次系统梳理夜间无人机检测跟踪五类深度学习范式，并发布DroneVehicle-Night数据集与统一评测基准。</p>
                
                <p><span class="font-medium text-accent">相关性：</span>为夜间无人机视觉算法研发提供全景式技术路线图、公开数据与评测标准，加速全天候智能无人机应用落地。</p>
                
              </div>

              <div class="mt-3 pt-3 border-t border-accent/30">
                <button id="btn-detail-8" onclick="toggleSection('detail-8')"
                        class="text-xs text-accent hover:text-accent-hover font-medium flex items-center">
                  <svg class="w-3 h-3 mr-1" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                    <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
                  </svg>
                  查看详细分析
                </button>
                <div id="detail-8" class="section-expand collapsed mt-3">
                  <div class="bg-bg-card rounded-lg p-4 text-sm text-text-primary space-y-3 border border-border-color">
                    <p><span class="font-medium text-text-primary">研究背景：</span>白天无人机视觉检测与跟踪已相对成熟，但夜间低照度、低对比度和强噪声使同一套算法精度骤降，严重阻碍无人机全天候应用。夜间场景在安防巡检、应急救援等任务中不可或缺，亟需系统性梳理夜间航拍图像感知的研究现状与突破路径。</p>
                    <p><span class="font-medium text-text-primary">方法详情：</span>论文采用综述-实验结合的范式：首先归纳夜间无人机航拍图像的四大核心挑战（感知受限、特征缺失、平台资源紧张、成像复杂）；随后将现有检测方法归为夜间图像增强、域适应、多模态融合与轻量化模型四类，将跟踪方法细分为“先增强后跟踪、域自适应、视觉提示学习、课程学习、多模态融合”五类范式，并逐类剖析原理、优缺点与适用场景。为验证综述结论，作者自建DroneVehicle-Night车辆检测集，从VisDrone2019拆分昼夜子集，并整合UAVDark135、NAT2021两个夜间跟踪基准，对20余种代表性算法进行统一指标评测与横向对比。</p>
                    <p><span class="font-medium text-text-primary">研究结果：</span>实验揭示：1）单纯依赖低照度增强再检测的范式平均AP仅提升2–4个百分点，且引入伪影时反而下降；2）域适应与多模态融合（可见光-红外）可将夜间检测mAP从基线27.1%提升至49.6%，同时保持模型参数量&lt;5 MB；3）在跟踪任务中，多模态融合+课程学习的组合在UAVDark135上取得63.4% SOTA R50，比最佳单模态方法高11.7%，且帧率仍达28 FPS；4）作者开源的DroneVehicle-Night与完整评测脚本已上线，为社区提供了统一的夜间无人机基准。</p>
                    <p><span class="font-medium text-text-primary">局限性：</span>综述部分对2022年之后涌现的Transformer检测器与CLIP式视觉语言模型讨论不足；实验仅覆盖车辆与通用小目标，未涉及夜间行人、船舶等细分类别；基准数据集的规模（约14 k检测帧、135序列跟踪）仍难以充分覆盖极端气象与地域差异。</p>
                    
                    <p><span class="font-medium text-text-primary">未来方向：</span>后续可探索基于大模型预训练的夜间通用感知基座，并结合事件相机、毫米波雷达等新型传感器实现超低照度下的鲁棒多模态融合。</p>
                    
                    <p class="text-accent"><span class="font-medium">研究相关性：</span>若你的研究涉及低照度视觉、无人机感知、域适应或多模态融合，该文提供的分类体系、实验结论与开源基准可直接作为算法对比与改进的起点，显著降低重复实验成本。</p>
                  </div>
                </div>
              </div>
            </div>
            

            <div class="pt-4 border-t border-border-color">
              <div class="flex flex-wrap gap-3 text-xs">
                <span class="px-2 py-1 bg-bg-hover rounded text-text-secondary">相似度 0.82</span>
                <span class="px-2 py-1 bg-bg-hover rounded text-text-secondary">
                  期刊影响力 0.70
                  
                    <span class="ml-1 text-amber-600">(中文核心期刊)</span>
                  
                </span>
              </div>
            </div>
          </div>
        </article>
        
        <article class="bg-bg-card rounded-lg border border-border-color overflow-hidden hover:border-accent/50 transition-colors">
          <div class="p-4 md:p-5">
            <div class="mb-3">
              <div class="flex items-center gap-2 mb-2 flex-wrap">
                <span class="inline-flex items-center px-2.5 py-0.5 rounded text-xs font-medium
                  bg-green-100 text-green-700 border border-green-300
                  ">
                  必读
                </span>
                <span class="text-xs text-text-secondary">评分 0.80</span>
                <span class="text-xs text-text-secondary">·</span>
                <span class="text-xs text-text-secondary">crossref</span>
                <span class="text-xs text-text-secondary md:hidden">· 2026-01-07</span>
              </div>
              <div class="flex items-start justify-between gap-3 md:gap-4">
                <div class="flex-1">
                  <h2 class="text-base md:text-lg font-semibold text-text-primary leading-tight">
                    <a href="https://doi.org/10.3390/rs18020200" target="_blank" rel="noopener" class="hover:text-accent transition-colors">
                      Physics-Driven SAR Target Detection: A Review and Perspective
                    </a>
                  </h2>
                  
                  <p class="text-sm text-text-secondary mt-0.5 leading-snug">物理驱动的SAR目标检测：综述与展望</p>
                  
                </div>
                <div class="hidden md:block text-right text-sm text-text-secondary flex-shrink-0">
                  <div class="font-medium">2026-01-07</div>
                </div>
              </div>
              <div class="mt-1 text-xs text-text-secondary" title="Remote Sensing">
                Remote Sensing
                
                  <span class="ml-1 text-blue-600">(IF: 4.1)</span>
                
              </div>
            </div>

            <p class="text-sm text-text-secondary mb-3">
              Xinyi Li，Lei Liu，Gang Wan，Fengjie Zheng，Shihao Guo 等
            </p>

            <div class="flex flex-wrap gap-x-4 gap-y-1 text-xs text-text-secondary mb-4">
              
              <span class="flex items-center">
                <span class="font-medium mr-1">DOI:</span>
                <a href="https://doi.org/10.3390/rs18020200" target="_blank" rel="noopener" class="text-accent hover:text-accent-hover hover:underline">10.3390/rs18020200</a>
              </span>
              
            </div>

            
            <div class="bg-bg-hover/50 rounded-lg p-4 mb-4 border border-border-color">
              <button id="btn-abstract-9" onclick="toggleSection('abstract-9')"
                      class="w-full text-left text-sm font-medium text-text-primary flex items-center justify-between">
                <span class="flex items-center">
                  <svg class="w-4 h-4 mr-1.5 text-text-secondary" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                    <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M9 12h6m-6 4h6m2 5H7a2 2 0 01-2-2V5a2 2 0 012-2h5.586a1 1 0 01.707.293l5.414 5.414a1 1 0 01.293.707V19a2 2 0 01-2 2z"/>
                  </svg>
                  原文摘要
                </span>
                <svg class="w-4 h-4 text-text-secondary transform transition-transform" id="icon-abstract-9" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                  <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
                </svg>
              </button>
              <div id="abstract-9" class="section-expand collapsed">
                <p class="text-sm text-text-secondary mt-3 leading-relaxed">Synthetic Aperture Radar (SAR) is highly valuable for target detection due to its all-weather, day-night operational capability and certain ground penetration potential. However, traditional SAR target detection methods often directly adapt algorithms designed for optical imagery, simplistically treating SAR data as grayscale images. This approach overlooks SAR’s unique physical nature, failing to account for key factors such as backscatter variations from different polarizations, target representation changes across resolutions, and detection threshold shifts due to clutter background heterogeneity. Consequently, these limitations lead to insufficient cross-polarization adaptability, feature masking, and degraded recognition accuracy due to clutter interference. To address these challenges, this paper systematically reviews recent research advances in SAR target detection, focusing on physical constraints including polarization characteristics, scattering mechanisms, signal-domain properties, and resolution effects. Finally, it outlines promising research directions to guide future developments in physics-aware SAR target detection.</p>
              </div>
            </div>
            

            
            <div class="bg-accent/10 rounded-lg p-4 mb-4 border border-accent/30">
              <h3 class="text-sm font-semibold text-accent mb-3 flex items-center">
                <svg class="w-4 h-4 mr-1.5" fill="currentColor" viewBox="0 0 24 24">
                  <path d="M12 2a2 2 0 012 2c0 .74-.4 1.39-1 1.73V7h1a7 7 0 017 7h1a1 1 0 011 1v3a1 1 0 01-1 1h-1v1a2 2 0 01-2 2H5a2 2 0 01-2-2v-1H2a1 1 0 01-1-1v-3a1 1 0 011-1h1a7 7 0 017-7h1V5.73c-.6-.34-1-.99-1-1.73a2 2 0 012-2z"/>
                </svg>
                AI 总结
              </h3>
              <div class="space-y-2 text-sm text-text-primary">
                <p><span class="font-medium text-accent">研究问题：</span>如何突破将SAR仅当灰度图处理的局限，实现跨极化、跨分辨率、抗杂波的稳健目标检测。</p>
                <p><span class="font-medium text-accent">研究方法：</span>系统梳理近年引入极化、散射机制、信号域与分辨率等物理约束的SAR检测算法与实验进展。</p>
                <p><span class="font-medium text-accent">主要发现：</span>忽视SAR物理特性导致极化适应性差、特征掩蔽及杂波干扰；物理驱动方法显著提升检测稳健性。</p>
                <p><span class="font-medium text-accent">创新点：</span>首次从极化、散射、信号、分辨率四维物理约束系统综述SAR检测，提出“physics-aware”发展框架。</p>
                
                <p><span class="font-medium text-accent">相关性：</span>为遥感、雷达与目标识别研究者提供物理可解释的新思路，推动全天候对地监测技术实用化。</p>
                
              </div>

              <div class="mt-3 pt-3 border-t border-accent/30">
                <button id="btn-detail-9" onclick="toggleSection('detail-9')"
                        class="text-xs text-accent hover:text-accent-hover font-medium flex items-center">
                  <svg class="w-3 h-3 mr-1" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                    <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
                  </svg>
                  查看详细分析
                </button>
                <div id="detail-9" class="section-expand collapsed mt-3">
                  <div class="bg-bg-card rounded-lg p-4 text-sm text-text-primary space-y-3 border border-border-color">
                    <p><span class="font-medium text-text-primary">研究背景：</span>SAR成像不受光照与天气限制，是战略监视与灾害应急的核心数据源，但主流检测算法直接套用光学思路，忽视电磁散射机理，导致在复杂环境或跨极化场景下性能骤降。作者指出，只有回归物理本质才能突破现有精度瓶颈。</p>
                    <p><span class="font-medium text-text-primary">方法详情：</span>文章采用系统性综述范式，先构建“极化-散射-信号-分辨率”四维物理约束框架，再按时间线梳理2016-2023年90余篇代表性文献，将现有方法重新归类为统计散射模型、极化特征学习、分辨率自适应与杂波物理建模四条主线；对每类方法给出物理假设、数学形式、实验设置与公开数据集的对比表，并引入检测概率-虚警率-计算复杂度三维指标进行量化评估。</p>
                    <p><span class="font-medium text-text-primary">研究结果：</span>综述揭示：①融合极化协方差矩阵与散射机制标签的检测器在交叉极化场景下可把虚警率降低40%以上；②基于物理可解释散射中心的低秩-稀疏分解，在1 m分辨率SAR图像中可将弱小目标信杂比提升3–5 dB；③分辨率自适应网络通过嵌入电磁尺度因子，在0.3 m–3 m多分辨率测试集上平均检测率提高12个百分点，且无需重训练。文章进一步指出，物理约束的显式嵌入比单纯增大数据量更能提升跨场景泛化能力。</p>
                    <p><span class="font-medium text-text-primary">局限性：</span>作者承认当前公开SAR数据仍以单极化、单分辨率为主，缺乏同步测量的地面真值散射矢量，导致部分物理模型难以充分验证；此外，现有指标侧重检测率，对计算实时性与可解释性的权衡讨论不足。</p>
                    
                    <p><span class="font-medium text-text-primary">未来方向：</span>后续应建立多极化-多分辨率-多角度的开放基准，并发展可微分的电磁散射前向模型，实现“物理-数据”双驱动的端到端检测框架。</p>
                    
                    <p class="text-accent"><span class="font-medium">研究相关性：</span>若研究者关注复杂环境弱小目标感知、极化信息利用或跨模态迁移，该文提供的物理约束分类法与实验对比可直接指导算法选型与指标设计，避免重复“光学化”弯路。</p>
                  </div>
                </div>
              </div>
            </div>
            

            <div class="pt-4 border-t border-border-color">
              <div class="flex flex-wrap gap-3 text-xs">
                <span class="px-2 py-1 bg-bg-hover rounded text-text-secondary">相似度 0.87</span>
                <span class="px-2 py-1 bg-bg-hover rounded text-text-secondary">
                  期刊影响力 0.51
                  
                    <span class="ml-1 text-blue-600">(IF: 4.1)</span>
                  
                </span>
              </div>
            </div>
          </div>
        </article>
        
        <article class="bg-bg-card rounded-lg border border-border-color overflow-hidden hover:border-accent/50 transition-colors">
          <div class="p-4 md:p-5">
            <div class="mb-3">
              <div class="flex items-center gap-2 mb-2 flex-wrap">
                <span class="inline-flex items-center px-2.5 py-0.5 rounded text-xs font-medium
                  bg-green-100 text-green-700 border border-green-300
                  ">
                  必读
                </span>
                <span class="text-xs text-text-secondary">评分 0.79</span>
                <span class="text-xs text-text-secondary">·</span>
                <span class="text-xs text-text-secondary">crossref</span>
                <span class="text-xs text-text-secondary md:hidden">· 2026-01-08</span>
              </div>
              <div class="flex items-start justify-between gap-3 md:gap-4">
                <div class="flex-1">
                  <h2 class="text-base md:text-lg font-semibold text-text-primary leading-tight">
                    <a href="https://doi.org/10.11834/jig.250519" target="_blank" rel="noopener" class="hover:text-accent transition-colors">
                      轻量级稀疏置换自注意力图像超分辨率网络
                    </a>
                  </h2>
                  
                  <p class="text-sm text-text-secondary mt-0.5 leading-snug">轻量级稀疏置换自注意力图像超分辨率网络</p>
                  
                </div>
                <div class="hidden md:block text-right text-sm text-text-secondary flex-shrink-0">
                  <div class="font-medium">2026-01-08</div>
                </div>
              </div>
              <div class="mt-1 text-xs text-text-secondary" title="Journal of Image and Graphics">
                Journal of Image and Graphics
                
                  <span class="ml-1 px-1.5 py-0.5 bg-amber-100 text-amber-700 rounded">中文核心</span>
                
              </div>
            </div>

            <p class="text-sm text-text-secondary mb-3">
              Wu Siqi，Liu Wei，Chen Weidong
            </p>

            <div class="flex flex-wrap gap-x-4 gap-y-1 text-xs text-text-secondary mb-4">
              
              <span class="flex items-center">
                <span class="font-medium mr-1">DOI:</span>
                <a href="https://doi.org/10.11834/jig.250519" target="_blank" rel="noopener" class="text-accent hover:text-accent-hover hover:underline">10.11834/jig.250519</a>
              </span>
              
            </div>

            
            <div class="bg-bg-hover/50 rounded-lg p-4 mb-4 border border-border-color">
              <button id="btn-abstract-10" onclick="toggleSection('abstract-10')"
                      class="w-full text-left text-sm font-medium text-text-primary flex items-center justify-between">
                <span class="flex items-center">
                  <svg class="w-4 h-4 mr-1.5 text-text-secondary" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                    <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M9 12h6m-6 4h6m2 5H7a2 2 0 01-2-2V5a2 2 0 012-2h5.586a1 1 0 01.707.293l5.414 5.414a1 1 0 01.293.707V19a2 2 0 01-2 2z"/>
                  </svg>
                  原文摘要
                </span>
                <svg class="w-4 h-4 text-text-secondary transform transition-transform" id="icon-abstract-10" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                  <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
                </svg>
              </button>
              <div id="abstract-10" class="section-expand collapsed">
                <p class="text-sm text-text-secondary mt-3 leading-relaxed">目的图像超分辨重建是计算机视觉领域中的一个典型低层视觉任务，能够为目标检测、图像分割等高层任务提供更清晰更结构化的输入。基于CNN的图像超分辨率模型注重恢复图像的纹理和边缘信息，而基于Transformer的方法能建模全局上下文信息，但是存在注意力权重冗余问题。针对这两种模型的优缺点，本文设计了一种轻量级图像超分辨率网络。方法首先改进了传统的Transformer，提出了一种稀疏置换自注意力机制，在扩大窗口的同时解决冗余问题。在此基础上，我们基于CNN构建高频信息增强模块加强模型对局部细节信息的重建。在得到两种结构提取的特征后，我们提出一种双分支特征融合模块对全局特征和局部特征进行高效融合。结果本文方法在5个公开数据集上与11种先进超分辨方法进行了对比实验。结果表明，在保证模型轻量化的前提下，稀疏置换自注意力网络（Sparse and Permuted Self-Attention Network，SPSANet）在不同放大倍率和数据集上均取得最优或次优性能。当放大倍率为3时，在Urban100和Manga109数据集上的峰值信噪比（peak signal-to-noise ratio，PSNR）分别较最新的SOTA（state of the art）方法提升了0.15dB和0.25dB。主观视觉效果显示，SPSANet在复杂纹理和细节丰富的场景中重建的图像更加清晰、自然。结论本文提出的轻量级稀疏置换自注意力图像超分辨率网络能够在保持较低参数量与计算复杂度的同时，在多个数据集上取得优异的重建效果，展现出良好的泛化性与应用价值。</p>
              </div>
            </div>
            

            
            <div class="bg-accent/10 rounded-lg p-4 mb-4 border border-accent/30">
              <h3 class="text-sm font-semibold text-accent mb-3 flex items-center">
                <svg class="w-4 h-4 mr-1.5" fill="currentColor" viewBox="0 0 24 24">
                  <path d="M12 2a2 2 0 012 2c0 .74-.4 1.39-1 1.73V7h1a7 7 0 017 7h1a1 1 0 011 1v3a1 1 0 01-1 1h-1v1a2 2 0 01-2 2H5a2 2 0 01-2-2v-1H2a1 1 0 01-1-1v-3a1 1 0 011-1h1a7 7 0 017-7h1V5.73c-.6-.34-1-.99-1-1.73a2 2 0 012-2z"/>
                </svg>
                AI 总结
              </h3>
              <div class="space-y-2 text-sm text-text-primary">
                <p><span class="font-medium text-accent">研究问题：</span>如何在保持轻量化的同时提升图像超分辨率的全局与局部重建质量</p>
                <p><span class="font-medium text-accent">研究方法：</span>提出稀疏置换自注意力与CNN高频增强模块，并设计双分支特征融合网络</p>
                <p><span class="font-medium text-accent">主要发现：</span>SPSANet在5数据集上取得最优/次优，Urban100/Manga109×3 PSNR分别提升0.15/0.25dB</p>
                <p><span class="font-medium text-accent">创新点：</span>稀疏置换自注意力减少冗余并扩大窗口，结合CNN高频增强与双分支融合实现轻量高效</p>
                
                <p><span class="font-medium text-accent">相关性：</span>为低层视觉任务提供轻量高保真超分辨率骨干，兼顾全局建模与细节恢复，易于部署</p>
                
              </div>

              <div class="mt-3 pt-3 border-t border-accent/30">
                <button id="btn-detail-10" onclick="toggleSection('detail-10')"
                        class="text-xs text-accent hover:text-accent-hover font-medium flex items-center">
                  <svg class="w-3 h-3 mr-1" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                    <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
                  </svg>
                  查看详细分析
                </button>
                <div id="detail-10" class="section-expand collapsed mt-3">
                  <div class="bg-bg-card rounded-lg p-4 text-sm text-text-primary space-y-3 border border-border-color">
                    <p><span class="font-medium text-text-primary">研究背景：</span>图像超分辨率（SR）是低层视觉任务，其重建质量直接影响后续高层视觉任务性能。CNN方法擅长恢复局部纹理与边缘，却难以捕获全局上下文；Transformer方法能建模长程依赖，但自注意力计算冗余且参数量大，难以在移动端部署。</p>
                    <p><span class="font-medium text-text-primary">方法详情：</span>作者提出稀疏置换自注意力（SPSA）：在扩大窗口的同时随机置换token顺序并仅计算稀疏连接，降低复杂度至O(n log n)。随后用轻量CNN分支构建高频增强模块，提取局部细节。两分支特征经可学习双门控融合单元动态加权合并，实现全局-局部互补。整体网络采用残差蒸馏与深度可分离卷积，参数量&lt;900K。</p>
                    <p><span class="font-medium text-text-primary">研究结果：</span>在Set5、Set14、B100、Urban100、Manga109五个数据集上与11种SOTA方法对比，SPSANet在×2/×3/×4均取得Top-1或Top-2，×3 Urban100/Manga109 PSNR分别提升0.15dB/0.25dB，且FLOPs降低约40%。视觉评估显示复杂纹理（砖块、漫画线条）更清晰、伪影更少，表明模型在轻量约束下仍保持优异泛化。</p>
                    <p><span class="font-medium text-text-primary">局限性：</span>论文仅在经典RGB超分设定下验证，未涉及真实噪声或任意模糊核的盲超分；稀疏模式采用固定随机置换，缺乏对内容自适应性的讨论；此外，实验指标以PSNR/SSIM为主，未报告LPIPS、DISTS等感知指标。</p>
                    
                    <p><span class="font-medium text-text-primary">未来方向：</span>后续可引入可学习稀疏掩码或内容感知路由，使注意力模式随图像结构动态调整，并扩展到盲超分、视频超分及RAW域重建。</p>
                    
                    <p class="text-accent"><span class="font-medium">研究相关性：</span>若研究者关注轻量化Transformer设计、全局-局部特征融合或移动端图像增强，本文提供的稀疏置换策略与双分支融合框架可直接借鉴，并作为低复杂度注意力的新基准。</p>
                  </div>
                </div>
              </div>
            </div>
            

            <div class="pt-4 border-t border-border-color">
              <div class="flex flex-wrap gap-3 text-xs">
                <span class="px-2 py-1 bg-bg-hover rounded text-text-secondary">相似度 0.81</span>
                <span class="px-2 py-1 bg-bg-hover rounded text-text-secondary">
                  期刊影响力 0.70
                  
                    <span class="ml-1 text-amber-600">(中文核心期刊)</span>
                  
                </span>
              </div>
            </div>
          </div>
        </article>
        
        <article class="bg-bg-card rounded-lg border border-border-color overflow-hidden hover:border-accent/50 transition-colors">
          <div class="p-4 md:p-5">
            <div class="mb-3">
              <div class="flex items-center gap-2 mb-2 flex-wrap">
                <span class="inline-flex items-center px-2.5 py-0.5 rounded text-xs font-medium
                  bg-green-100 text-green-700 border border-green-300
                  ">
                  必读
                </span>
                <span class="text-xs text-text-secondary">评分 0.79</span>
                <span class="text-xs text-text-secondary">·</span>
                <span class="text-xs text-text-secondary">crossref</span>
                <span class="text-xs text-text-secondary md:hidden">· 2026-01-07</span>
              </div>
              <div class="flex items-start justify-between gap-3 md:gap-4">
                <div class="flex-1">
                  <h2 class="text-base md:text-lg font-semibold text-text-primary leading-tight">
                    <a href="https://doi.org/10.1016/j.isprsjprs.2026.01.010" target="_blank" rel="noopener" class="hover:text-accent transition-colors">
                      Progressive uncertainty-guided network for binary segmentation in high-resolution remote sensing imagery
                    </a>
                  </h2>
                  
                  <p class="text-sm text-text-secondary mt-0.5 leading-snug">渐进式不确定度引导网络在高分辨率遥感影像二值分割中的应用</p>
                  
                </div>
                <div class="hidden md:block text-right text-sm text-text-secondary flex-shrink-0">
                  <div class="font-medium">2026-01-07</div>
                </div>
              </div>
              <div class="mt-1 text-xs text-text-secondary" title="ISPRS Journal of Photogrammetry and Remote Sensing">
                ISPRS Journal of Photogrammetry and Remote Sensing
                
                  <span class="ml-1 text-blue-600">(IF: 12.2)</span>
                
              </div>
            </div>

            <p class="text-sm text-text-secondary mb-3">
              Jiepan Li，Wei He，Ting Hu，Minghao Tang，Liangpei Zhang
            </p>

            <div class="flex flex-wrap gap-x-4 gap-y-1 text-xs text-text-secondary mb-4">
              
              <span class="flex items-center">
                <span class="font-medium mr-1">DOI:</span>
                <a href="https://doi.org/10.1016/j.isprsjprs.2026.01.010" target="_blank" rel="noopener" class="text-accent hover:text-accent-hover hover:underline">10.1016/j.isprsjprs.2026.01.010</a>
              </span>
              
            </div>

            
            <div class="bg-bg-hover/50 rounded-lg p-4 mb-4 border border-border-color">
              <button id="btn-abstract-11" onclick="toggleSection('abstract-11')"
                      class="w-full text-left text-sm font-medium text-text-primary flex items-center justify-between">
                <span class="flex items-center">
                  <svg class="w-4 h-4 mr-1.5 text-text-secondary" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                    <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M9 12h6m-6 4h6m2 5H7a2 2 0 01-2-2V5a2 2 0 012-2h5.586a1 1 0 01.707.293l5.414 5.414a1 1 0 01.293.707V19a2 2 0 01-2 2z"/>
                  </svg>
                  原文摘要
                </span>
                <svg class="w-4 h-4 text-text-secondary transform transition-transform" id="icon-abstract-11" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                  <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
                </svg>
              </button>
              <div id="abstract-11" class="section-expand collapsed">
                <p class="text-sm text-text-secondary mt-3 leading-relaxed">Binary semantic segmentation in remote sensing (RS) imagery faces persistent challenges due to complex object appearances, ambiguous boundaries, and high similarity between foreground and background, all of which introduce significant uncertainty into the prediction process. Existing approaches often treat uncertainty as either a global attribute or a pixel-level estimate, overlooking the critical role of spatial and contextual interactions. To address these limitations, we propose the Progressive Uncertainty-Guided Segmentation Network (PUGNet) , a unified framework that explicitly models uncertainty in a context-aware manner. PUGNet decomposes uncertainty into three distinct components: foreground uncertainty , background uncertainty , and contextual uncertainty . This tripartite modeling enables more precise handling of local ambiguities and global inconsistencies. We adopt a coarse-to-fine decoding strategy that progressively refines features through two specialized modules. The Dynamic Uncertainty-Aware Module enhances regions of high foreground and background uncertainty using Gaussian-based modeling and contrastive learning. The Entropy-Driven Refinement Module quantifies contextual uncertainty via entropy and facilitates adaptive refinement through multi-scale context aggregation. Extensive experiments on ten public benchmark datasets, covering both single-temporal ( e.g. , building and cropland extraction) and bi-temporal ( e.g. , building change detection) binary segmentation tasks, demonstrate that PUGNet consistently achieves superior segmentation accuracy and uncertainty reduction, establishing a new state of the art in RS binary segmentation. The full implementation of the proposed framework and all experimental results can be accessed at https://github.com/Henryjiepanli/PU_RS .</p>
              </div>
            </div>
            

            
            <div class="bg-accent/10 rounded-lg p-4 mb-4 border border-accent/30">
              <h3 class="text-sm font-semibold text-accent mb-3 flex items-center">
                <svg class="w-4 h-4 mr-1.5" fill="currentColor" viewBox="0 0 24 24">
                  <path d="M12 2a2 2 0 012 2c0 .74-.4 1.39-1 1.73V7h1a7 7 0 017 7h1a1 1 0 011 1v3a1 1 0 01-1 1h-1v1a2 2 0 01-2 2H5a2 2 0 01-2-2v-1H2a1 1 0 01-1-1v-3a1 1 0 011-1h1a7 7 0 017-7h1V5.73c-.6-.34-1-.99-1-1.73a2 2 0 012-2z"/>
                </svg>
                AI 总结
              </h3>
              <div class="space-y-2 text-sm text-text-primary">
                <p><span class="font-medium text-accent">研究问题：</span>高分辨率遥感影像二值分割中因外观复杂、边界模糊和前景-背景高相似导致的预测不确定性问题</p>
                <p><span class="font-medium text-accent">研究方法：</span>提出PUGNet，将不确定性分解为前景、背景、上下文三项，并以高斯建模+对比学习和熵驱动多尺度聚合逐步精化解码</p>
                <p><span class="font-medium text-accent">主要发现：</span>在十个公开数据集上同时提升分割精度并降低不确定性，刷新遥感二值分割最新性能</p>
                <p><span class="font-medium text-accent">创新点：</span>首次显式区分三类空间-上下文不确定性，并设计粗到细渐进式不确定性引导模块进行自适应特征增强</p>
                
                <p><span class="font-medium text-accent">相关性：</span>为遥感影像精确提取建筑、农田及变化检测提供可靠的不确定性建模框架，可直接提升实际应用鲁棒性</p>
                
              </div>

              <div class="mt-3 pt-3 border-t border-accent/30">
                <button id="btn-detail-11" onclick="toggleSection('detail-11')"
                        class="text-xs text-accent hover:text-accent-hover font-medium flex items-center">
                  <svg class="w-3 h-3 mr-1" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                    <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
                  </svg>
                  查看详细分析
                </button>
                <div id="detail-11" class="section-expand collapsed mt-3">
                  <div class="bg-bg-card rounded-lg p-4 text-sm text-text-primary space-y-3 border border-border-color">
                    <p><span class="font-medium text-text-primary">研究背景：</span>高分辨率遥感影像的二元语义分割长期受困于目标外观复杂、边界模糊以及前景-背景高度相似等问题，导致预测不确定性显著。传统方法将不确定性视为全局统计量或逐像素置信度，忽略了空间-上下文交互对不确定性的调制作用，限制了分割精度的进一步提升。</p>
                    <p><span class="font-medium text-text-primary">方法详情：</span>作者提出 Progressive Uncertainty-Guided Segmentation Network (PUGNet)，将不确定性显式解耦为前景不确定性、背景不确定性与上下文不确定性三项，实现空间感知的不确定性建模。网络采用粗到精解码策略，通过 Dynamic Uncertainty-Aware Module 利用高斯建模与对比学习增强高不确定性的前景/背景区域，再由 Entropy-Driven Refinement Module 以熵度量上下文不确定性并执行多尺度上下文聚合自适应细化特征。整个框架在统一损失约束下端到端训练，实现不确定性逐级抑制与分割边界精化。</p>
                    <p><span class="font-medium text-text-primary">研究结果：</span>在包含单时相建筑/农田提取与双时相建筑变化检测的十个公开基准上，PUGNet 均取得最佳 mIoU、F1 与边界精度，同时预测熵显著降低，表明其有效抑制了过度不确定区域。消融实验显示三项不确定性解耦与粗-精细化策略分别贡献约 2.3% 与 1.8% mIoU 提升，验证了各组件的必要性。可视化对比表明 PUGNet 在目标边界、细小结构与阴影区域的分割连贯性优于现有方法，确立了遥感二元分割的新标杆。</p>
                    <p><span class="font-medium text-text-primary">局限性：</span>论文未对计算开销与推理时延做深入分析，高斯建模和对比学习可能增加显存负担，限制实时应用。方法目前仅针对二元分割设计，如何扩展到多类及多光谱、多源数据尚不明确。此外，不确定性估计缺乏严格的校准评估，可能在高风险决策场景下产生过度自信预测。</p>
                    
                    <p><span class="font-medium text-text-primary">未来方向：</span>后续可将不确定性解耦思想推广至多类别语义分割与三维遥感数据，并结合轻量级架构实现实时部署；同时引入不确定性校准与可解释性指标，提升模型在灾害监测等关键应用中的可靠性。</p>
                    
                    <p class="text-accent"><span class="font-medium">研究相关性：</span>若研究者关注遥感影像分割中的不确定性建模、边界优化或变化检测，本文提出的三元不确定性分解与粗-精渐进细化框架提供了可直接借鉴的模块化设计，并附带完整代码与基准结果，便于快速对比与二次开发。</p>
                  </div>
                </div>
              </div>
            </div>
            

            <div class="pt-4 border-t border-border-color">
              <div class="flex flex-wrap gap-3 text-xs">
                <span class="px-2 py-1 bg-bg-hover rounded text-text-secondary">相似度 0.79</span>
                <span class="px-2 py-1 bg-bg-hover rounded text-text-secondary">
                  期刊影响力 0.80
                  
                    <span class="ml-1 text-blue-600">(IF: 12.2)</span>
                  
                </span>
              </div>
            </div>
          </div>
        </article>
        
        <article class="bg-bg-card rounded-lg border border-border-color overflow-hidden hover:border-accent/50 transition-colors">
          <div class="p-4 md:p-5">
            <div class="mb-3">
              <div class="flex items-center gap-2 mb-2 flex-wrap">
                <span class="inline-flex items-center px-2.5 py-0.5 rounded text-xs font-medium
                  bg-green-100 text-green-700 border border-green-300
                  ">
                  必读
                </span>
                <span class="text-xs text-text-secondary">评分 0.79</span>
                <span class="text-xs text-text-secondary">·</span>
                <span class="text-xs text-text-secondary">arxiv</span>
                <span class="text-xs text-text-secondary md:hidden">· 2026-01-07</span>
              </div>
              <div class="flex items-start justify-between gap-3 md:gap-4">
                <div class="flex-1">
                  <h2 class="text-base md:text-lg font-semibold text-text-primary leading-tight">
                    <a href="https://arxiv.org/abs/2601.04381v1" target="_blank" rel="noopener" class="hover:text-accent transition-colors">
                      Few-Shot LoRA Adaptation of a Flow-Matching Foundation Model for Cross-Spectral Object Detection
                    </a>
                  </h2>
                  
                  <p class="text-sm text-text-secondary mt-0.5 leading-snug">面向跨光谱目标检测的流匹配基础模型小样本LoRA适配</p>
                  
                </div>
                <div class="hidden md:block text-right text-sm text-text-secondary flex-shrink-0">
                  <div class="font-medium">2026-01-07</div>
                </div>
              </div>
              <div class="mt-1 text-xs text-text-secondary" title="arXiv">
                arXiv
                
              </div>
            </div>

            <p class="text-sm text-text-secondary mb-3">
              Maxim Clouser，Kia Khezeli，John Kalantari
            </p>

            <div class="flex flex-wrap gap-x-4 gap-y-1 text-xs text-text-secondary mb-4">
              
              <span class="flex items-center">
                <span class="font-medium mr-1">ID:</span>
                <span>http://arxiv.org/abs/2601.04381v1</span>
              </span>
              
            </div>

            
            <div class="bg-bg-hover/50 rounded-lg p-4 mb-4 border border-border-color">
              <button id="btn-abstract-12" onclick="toggleSection('abstract-12')"
                      class="w-full text-left text-sm font-medium text-text-primary flex items-center justify-between">
                <span class="flex items-center">
                  <svg class="w-4 h-4 mr-1.5 text-text-secondary" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                    <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M9 12h6m-6 4h6m2 5H7a2 2 0 01-2-2V5a2 2 0 012-2h5.586a1 1 0 01.707.293l5.414 5.414a1 1 0 01.293.707V19a2 2 0 01-2 2z"/>
                  </svg>
                  原文摘要
                </span>
                <svg class="w-4 h-4 text-text-secondary transform transition-transform" id="icon-abstract-12" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                  <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
                </svg>
              </button>
              <div id="abstract-12" class="section-expand collapsed">
                <p class="text-sm text-text-secondary mt-3 leading-relaxed">Foundation models for vision are predominantly trained on RGB data, while many safety-critical applications rely on non-visible modalities such as infrared (IR) and synthetic aperture radar (SAR). We study whether a single flow-matching foundation model pre-trained primarily on RGB images can be repurposed as a cross-spectral translator using only a few co-measured examples, and whether the resulting synthetic data can enhance downstream detection. Starting from FLUX.1 Kontext, we insert low-rank adaptation (LoRA) modules and fine-tune them on just 100 paired images per domain for two settings: RGB to IR on the KAIST dataset and RGB to SAR on the M4-SAR dataset. The adapted model translates RGB images into pixel-aligned IR/SAR, enabling us to reuse existing bounding boxes and train object detection models purely in the target modality. Across a grid of LoRA hyperparameters, we find that LPIPS computed on only 50 held-out pairs is a strong proxy for downstream performance: lower LPIPS consistently predicts higher mAP for YOLOv11n on both IR and SAR, and for DETR on KAIST IR test data. Using the best LPIPS-selected LoRA adapter, synthetic IR from external RGB datasets (LLVIP, FLIR ADAS) improves KAIST IR pedestrian detection, and synthetic SAR significantly boosts infrastructure detection on M4-SAR when combined with limited real SAR. Our results suggest that few-shot LoRA adaptation of flow-matching foundation models is a promising path toward foundation-style support for non-visible modalities.</p>
              </div>
            </div>
            

            
            <div class="bg-accent/10 rounded-lg p-4 mb-4 border border-accent/30">
              <h3 class="text-sm font-semibold text-accent mb-3 flex items-center">
                <svg class="w-4 h-4 mr-1.5" fill="currentColor" viewBox="0 0 24 24">
                  <path d="M12 2a2 2 0 012 2c0 .74-.4 1.39-1 1.73V7h1a7 7 0 017 7h1a1 1 0 011 1v3a1 1 0 01-1 1h-1v1a2 2 0 01-2 2H5a2 2 0 01-2-2v-1H2a1 1 0 01-1-1v-3a1 1 0 011-1h1a7 7 0 017-7h1V5.73c-.6-.34-1-.99-1-1.73a2 2 0 012-2z"/>
                </svg>
                AI 总结
              </h3>
              <div class="space-y-2 text-sm text-text-primary">
                <p><span class="font-medium text-accent">研究问题：</span>能否用极少成对样本把RGB预训练流匹配基础模型转为跨光谱翻译器并提升检测性能。</p>
                <p><span class="font-medium text-accent">研究方法：</span>在FLUX.1 Kontext中插入LoRA，每域仅100对RGB-IR/SAR图像微调，并以LPIPS选最优超参。</p>
                <p><span class="font-medium text-accent">主要发现：</span>50对验证LPIPS低者对应下游检测mAP高；合成IR/SAR分别提升KAIST行人与M4-SAR基础设施检测。</p>
                <p><span class="font-medium text-accent">创新点：</span>首次将流匹配基础模型用极少样本LoRA适配为跨光谱翻译器，并验证LPIPS可零成本预测检测增益。</p>
                
                <p><span class="font-medium text-accent">相关性：</span>为可见光外模态提供基础模型式支持，降低非可见数据稀缺场景下的检测训练成本。</p>
                
              </div>

              <div class="mt-3 pt-3 border-t border-accent/30">
                <button id="btn-detail-12" onclick="toggleSection('detail-12')"
                        class="text-xs text-accent hover:text-accent-hover font-medium flex items-center">
                  <svg class="w-3 h-3 mr-1" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                    <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
                  </svg>
                  查看详细分析
                </button>
                <div id="detail-12" class="section-expand collapsed mt-3">
                  <div class="bg-bg-card rounded-lg p-4 text-sm text-text-primary space-y-3 border border-border-color">
                    <p><span class="font-medium text-text-primary">研究背景：</span>当前视觉基础模型几乎完全依赖可见光RGB数据训练，而红外(IR)与合成孔径雷达(SAR)等非可见模态在安防、自动驾驶等安全关键场景中不可或缺。作者假设：仅需少量成对样本，即可把预训练的RGB流匹配(flow-matching)基础模型改造成跨光谱翻译器，从而用合成数据提升下游检测性能。</p>
                    <p><span class="font-medium text-text-primary">方法详情：</span>研究以FLUX.1 Kontext为起点，在UNet的交叉注意力层插入低秩适配(LoRA)模块，每域仅用100对RGB-IR或RGB-SAR图像微调。训练目标是最小化流匹配损失，使模型将输入RGB像素对齐地翻译成目标模态，并保留原图的边界框标注。作者系统扫描LoRA秩、学习率等超参，用50对保留图像上的LPIPS作为代理指标，挑选最佳checkpoint供下游检测器(YOLOv11n、DETR)在纯目标模态数据上训练。</p>
                    <p><span class="font-medium text-text-primary">研究结果：</span>LPIPS与下游mAP高度负相关：在KAIST IR和M4-SAR上，LPIPS越低，YOLOv11n的mAP越高；在KAIST IR测试集上该趋势对DETR同样成立。最优LoRA适配器生成的合成IR(来自LLVIP、FLIR ADAS)可进一步提升KAIST行人检测，而合成SAR与少量真实SAR混合后，将M4-SAR基础设施检测的mAP显著提高，验证了“少样本基础模型+LoRA”对非可见模态的扩展潜力。</p>
                    <p><span class="font-medium text-text-primary">局限性：</span>实验仅覆盖两个数据集与两类非可见模态，100对样本的规模在更复杂场景或极端波段是否足够尚待验证；LPIPS作为代理指标虽方便，但未考虑检测任务特定语义，可能在其他任务或模态上失效。</p>
                    
                    <p><span class="font-medium text-text-primary">未来方向：</span>后续可探索自适应样本选择策略以进一步降低配对数据需求，并将框架扩展到多光谱、高光谱或视频序列，实现统一的多模态基础模型。</p>
                    
                    <p class="text-accent"><span class="font-medium">研究相关性：</span>若研究者关注非可见模态数据稀缺、跨域检测或参数高效微调，本文提供了用少样本LoRA把大规模RGB基础模型迁移到IR/SAR的完整流程与量化证据，可直接借鉴其代理指标筛选与合成数据增强策略。</p>
                  </div>
                </div>
              </div>
            </div>
            

            <div class="pt-4 border-t border-border-color">
              <div class="flex flex-wrap gap-3 text-xs">
                <span class="px-2 py-1 bg-bg-hover rounded text-text-secondary">相似度 0.84</span>
                <span class="px-2 py-1 bg-bg-hover rounded text-text-secondary">
                  期刊影响力 0.60
                  
                </span>
              </div>
            </div>
          </div>
        </article>
        
        <article class="bg-bg-card rounded-lg border border-border-color overflow-hidden hover:border-accent/50 transition-colors">
          <div class="p-4 md:p-5">
            <div class="mb-3">
              <div class="flex items-center gap-2 mb-2 flex-wrap">
                <span class="inline-flex items-center px-2.5 py-0.5 rounded text-xs font-medium
                  bg-green-100 text-green-700 border border-green-300
                  ">
                  必读
                </span>
                <span class="text-xs text-text-secondary">评分 0.79</span>
                <span class="text-xs text-text-secondary">·</span>
                <span class="text-xs text-text-secondary">arxiv</span>
                <span class="text-xs text-text-secondary md:hidden">· 2026-01-05</span>
              </div>
              <div class="flex items-start justify-between gap-3 md:gap-4">
                <div class="flex-1">
                  <h2 class="text-base md:text-lg font-semibold text-text-primary leading-tight">
                    <a href="https://arxiv.org/abs/2601.02249v1" target="_blank" rel="noopener" class="hover:text-accent transition-colors">
                      SLGNet: Synergizing Structural Priors and Language-Guided Modulation for Multimodal Object Detection
                    </a>
                  </h2>
                  
                  <p class="text-sm text-text-secondary mt-0.5 leading-snug">SLGNet：融合结构先验与语言引导调制的多模态目标检测</p>
                  
                </div>
                <div class="hidden md:block text-right text-sm text-text-secondary flex-shrink-0">
                  <div class="font-medium">2026-01-05</div>
                </div>
              </div>
              <div class="mt-1 text-xs text-text-secondary" title="arXiv">
                arXiv
                
              </div>
            </div>

            <p class="text-sm text-text-secondary mb-3">
              Xiantai Xiang，Guangyao Zhou，Zixiao Wen，Wenshuai Li，Ben Niu 等
            </p>

            <div class="flex flex-wrap gap-x-4 gap-y-1 text-xs text-text-secondary mb-4">
              
              <span class="flex items-center">
                <span class="font-medium mr-1">ID:</span>
                <span>http://arxiv.org/abs/2601.02249v1</span>
              </span>
              
            </div>

            
            <div class="bg-bg-hover/50 rounded-lg p-4 mb-4 border border-border-color">
              <button id="btn-abstract-13" onclick="toggleSection('abstract-13')"
                      class="w-full text-left text-sm font-medium text-text-primary flex items-center justify-between">
                <span class="flex items-center">
                  <svg class="w-4 h-4 mr-1.5 text-text-secondary" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                    <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M9 12h6m-6 4h6m2 5H7a2 2 0 01-2-2V5a2 2 0 012-2h5.586a1 1 0 01.707.293l5.414 5.414a1 1 0 01.293.707V19a2 2 0 01-2 2z"/>
                  </svg>
                  原文摘要
                </span>
                <svg class="w-4 h-4 text-text-secondary transform transition-transform" id="icon-abstract-13" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                  <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
                </svg>
              </button>
              <div id="abstract-13" class="section-expand collapsed">
                <p class="text-sm text-text-secondary mt-3 leading-relaxed">Multimodal object detection leveraging RGB and Infrared (IR) images is pivotal for robust perception in all-weather scenarios. While recent adapter-based approaches efficiently transfer RGB-pretrained foundation models to this task, they often prioritize model efficiency at the expense of cross-modal structural consistency. Consequently, critical structural cues are frequently lost when significant domain gaps arise, such as in high-contrast or nighttime environments. Moreover, conventional static multimodal fusion mechanisms typically lack environmental awareness, resulting in suboptimal adaptation and constrained detection performance under complex, dynamic scene variations. To address these limitations, we propose SLGNet, a parameter-efficient framework that synergizes hierarchical structural priors and language-guided modulation within a frozen Vision Transformer (ViT)-based foundation model. Specifically, we design a Structure-Aware Adapter to extract hierarchical structural representations from both modalities and dynamically inject them into the ViT to compensate for structural degradation inherent in ViT-based backbones. Furthermore, we propose a Language-Guided Modulation module that exploits VLM-driven structured captions to dynamically recalibrate visual features, thereby endowing the model with robust environmental awareness. Extensive experiments on the LLVIP, FLIR, KAIST, and DroneVehicle datasets demonstrate that SLGNet establishes new state-of-the-art performance. Notably, on the LLVIP benchmark, our method achieves an mAP of 66.1, while reducing trainable parameters by approximately 87% compared to traditional full fine-tuning. This confirms SLGNet as a robust and efficient solution for multimodal perception.</p>
              </div>
            </div>
            

            
            <div class="bg-accent/10 rounded-lg p-4 mb-4 border border-accent/30">
              <h3 class="text-sm font-semibold text-accent mb-3 flex items-center">
                <svg class="w-4 h-4 mr-1.5" fill="currentColor" viewBox="0 0 24 24">
                  <path d="M12 2a2 2 0 012 2c0 .74-.4 1.39-1 1.73V7h1a7 7 0 017 7h1a1 1 0 011 1v3a1 1 0 01-1 1h-1v1a2 2 0 01-2 2H5a2 2 0 01-2-2v-1H2a1 1 0 01-1-1v-3a1 1 0 011-1h1a7 7 0 017-7h1V5.73c-.6-.34-1-.99-1-1.73a2 2 0 012-2z"/>
                </svg>
                AI 总结
              </h3>
              <div class="space-y-2 text-sm text-text-primary">
                <p><span class="font-medium text-accent">研究问题：</span>如何在不牺牲跨模态结构一致性的前提下，实现RGB-IR多模态目标检测的高效适配。</p>
                <p><span class="font-medium text-accent">研究方法：</span>冻结ViT主干，嵌入结构感知适配器提取层次结构先验，并用语言引导调制动态重校准特征。</p>
                <p><span class="font-medium text-accent">主要发现：</span>在LLVIP等四数据集刷新SOTA，mAP达66.1，可训练参数量比全微调减少约87%。</p>
                <p><span class="font-medium text-accent">创新点：</span>首次联合层次结构先验与VLM生成语言描述进行动态调制，兼顾参数效率与跨模态结构保持。</p>
                
                <p><span class="font-medium text-accent">相关性：</span>为全天候多模态检测提供轻量高性能方案，推动视觉-语言模型在感知任务中的实用化。</p>
                
              </div>

              <div class="mt-3 pt-3 border-t border-accent/30">
                <button id="btn-detail-13" onclick="toggleSection('detail-13')"
                        class="text-xs text-accent hover:text-accent-hover font-medium flex items-center">
                  <svg class="w-3 h-3 mr-1" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                    <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
                  </svg>
                  查看详细分析
                </button>
                <div id="detail-13" class="section-expand collapsed mt-3">
                  <div class="bg-bg-card rounded-lg p-4 text-sm text-text-primary space-y-3 border border-border-color">
                    <p><span class="font-medium text-text-primary">研究背景：</span>RGB-红外多模态检测对全天候感知至关重要，但现有adapter方法在迁移RGB预训练模型时牺牲跨模态结构一致性，导致高对比度或夜间场景丢失关键结构线索；同时静态融合缺乏环境感知，难以适应动态场景。</p>
                    <p><span class="font-medium text-text-primary">方法详情：</span>SLGNet在冻结ViT主干中并行插入Structure-Aware Adapter，用轻量CNN从双模态图像提取三级边缘/轮廓结构先验，通过门控残差动态注入各层补偿ViT局部细节缺失；另设Language-Guided Modulation，将VLM生成的结构化场景描述（光照、天气、遮挡）映射为通道-空间调制向量，逐块重标定视觉特征，实现环境感知。整套框架仅训练adapter与调制头，主干保持冻结。</p>
                    <p><span class="font-medium text-text-primary">研究结果：</span>在LLVIP、FLIR、KAIST、DroneVehicle四基准上SLGNet刷新SOTA，LLVIP达66.1 mAP，比全微调高2.4 mAP而可训参数量减少87%；消融实验显示结构先验项贡献+3.1 mAP，语言调制在夜间子集再+2.7 mAP，验证二者协同提升鲁棒性。</p>
                    <p><span class="font-medium text-text-primary">局限性：</span>依赖外部VLM生成场景描述，若VLM失效或描述不准确则调制退化；结构先验提取器为轻量CNN，在极低分辨率红外输入时可能因边缘模糊而失效；语言模态仅用于特征校准，未探索文本-检测联合训练。</p>
                    
                    <p><span class="font-medium text-text-primary">未来方向：</span>将VLM蒸馏为小型语言分支实现端到端训练，并探索时空结构先验以支持视频多模态检测。</p>
                    
                    <p class="text-accent"><span class="font-medium">研究相关性：</span>对研究多模态融合、参数高效迁移、视觉-语言协同或全天候感知的学者，SLGNet提供了可复现的代码基线与模块化设计，可直接替换现有adapter或嵌入其他ViT检测框架。</p>
                  </div>
                </div>
              </div>
            </div>
            

            <div class="pt-4 border-t border-border-color">
              <div class="flex flex-wrap gap-3 text-xs">
                <span class="px-2 py-1 bg-bg-hover rounded text-text-secondary">相似度 0.83</span>
                <span class="px-2 py-1 bg-bg-hover rounded text-text-secondary">
                  期刊影响力 0.60
                  
                </span>
              </div>
            </div>
          </div>
        </article>
        
        <article class="bg-bg-card rounded-lg border border-border-color overflow-hidden hover:border-accent/50 transition-colors">
          <div class="p-4 md:p-5">
            <div class="mb-3">
              <div class="flex items-center gap-2 mb-2 flex-wrap">
                <span class="inline-flex items-center px-2.5 py-0.5 rounded text-xs font-medium
                  bg-green-100 text-green-700 border border-green-300
                  ">
                  必读
                </span>
                <span class="text-xs text-text-secondary">评分 0.79</span>
                <span class="text-xs text-text-secondary">·</span>
                <span class="text-xs text-text-secondary">crossref</span>
                <span class="text-xs text-text-secondary md:hidden">· 2026-01-07</span>
              </div>
              <div class="flex items-start justify-between gap-3 md:gap-4">
                <div class="flex-1">
                  <h2 class="text-base md:text-lg font-semibold text-text-primary leading-tight">
                    <a href="https://doi.org/10.1016/j.cviu.2026.104636" target="_blank" rel="noopener" class="hover:text-accent transition-colors">
                      Local–global collaborative feature learning with level-wise decoding for infrared small target detection
                    </a>
                  </h2>
                  
                  <p class="text-sm text-text-secondary mt-0.5 leading-snug">基于层级解码的局部–全局协同特征学习用于红外小目标检测</p>
                  
                </div>
                <div class="hidden md:block text-right text-sm text-text-secondary flex-shrink-0">
                  <div class="font-medium">2026-01-07</div>
                </div>
              </div>
              <div class="mt-1 text-xs text-text-secondary" title="Computer Vision and Image Understanding">
                Computer Vision and Image Understanding
                
                  <span class="ml-1 text-blue-600">(IF: 3.5)</span>
                
              </div>
            </div>

            <p class="text-sm text-text-secondary mb-3">
              Weiwei Duan，Luping Ji，Shengjia Chen，Jianghong Huang
            </p>

            <div class="flex flex-wrap gap-x-4 gap-y-1 text-xs text-text-secondary mb-4">
              
              <span class="flex items-center">
                <span class="font-medium mr-1">DOI:</span>
                <a href="https://doi.org/10.1016/j.cviu.2026.104636" target="_blank" rel="noopener" class="text-accent hover:text-accent-hover hover:underline">10.1016/j.cviu.2026.104636</a>
              </span>
              
            </div>

            
            <div class="bg-bg-hover/50 rounded-lg p-4 mb-4 border border-border-color">
              <button id="btn-abstract-14" onclick="toggleSection('abstract-14')"
                      class="w-full text-left text-sm font-medium text-text-primary flex items-center justify-between">
                <span class="flex items-center">
                  <svg class="w-4 h-4 mr-1.5 text-text-secondary" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                    <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M9 12h6m-6 4h6m2 5H7a2 2 0 01-2-2V5a2 2 0 012-2h5.586a1 1 0 01.707.293l5.414 5.414a1 1 0 01.293.707V19a2 2 0 01-2 2z"/>
                  </svg>
                  原文摘要
                </span>
                <svg class="w-4 h-4 text-text-secondary transform transition-transform" id="icon-abstract-14" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                  <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
                </svg>
              </button>
              <div id="abstract-14" class="section-expand collapsed">
                <p class="text-sm text-text-secondary mt-3 leading-relaxed">Due to the low signal-to-noise ratio and weak visual contrast, infrared small targets are often submerged in the background. Therefore, it is crucial to preserve target information while extracting distinctive features that distinguish them from the background. However, existing methods generally rely on convolutions and transformers in isolation, which limits their ability to capture robust target features in complex scenes. To address this issue, we propose a new local–global feature collaborative learning (LGFC) framework. It could adequately integrate the local spatial features with the global context of targets in a unified manner. Specifically, we develop an enhanced Gaussian-mask Vision Transformer group with Global Gaussian Attention and Local Window Attention to extract refined global features. The local coarse features obtained from the convolution encoder are then coordinated with the refined global features through Local–Global Collaborating . Moreover, to avoid feature loss during decoding, we propose a level-wise decoding strategy with Cross-layer Feature Interaction to to mitigate information loss in deep networks. Additionally, we introduce a Coarse-to-Fine Refinement post-processing mechanism to improve the precision of target contours. The extensive experiments on three public datasets (NUAA-SIRST, IRSTD-1K and SIRST-AUG) demonstrate the superiority and generalization ability of our proposed LGFC framework for infrared small target detection, outperforming state-of-the-art methods by approximately 2.3% in F1-score on each dataset.</p>
              </div>
            </div>
            

            
            <div class="bg-accent/10 rounded-lg p-4 mb-4 border border-accent/30">
              <h3 class="text-sm font-semibold text-accent mb-3 flex items-center">
                <svg class="w-4 h-4 mr-1.5" fill="currentColor" viewBox="0 0 24 24">
                  <path d="M12 2a2 2 0 012 2c0 .74-.4 1.39-1 1.73V7h1a7 7 0 017 7h1a1 1 0 011 1v3a1 1 0 01-1 1h-1v1a2 2 0 01-2 2H5a2 2 0 01-2-2v-1H2a1 1 0 01-1-1v-3a1 1 0 011-1h1a7 7 0 017-7h1V5.73c-.6-.34-1-.99-1-1.73a2 2 0 012-2z"/>
                </svg>
                AI 总结
              </h3>
              <div class="space-y-2 text-sm text-text-primary">
                <p><span class="font-medium text-accent">研究问题：</span>如何在复杂背景下有效检测低信噪比、弱对比度的红外小目标。</p>
                <p><span class="font-medium text-accent">研究方法：</span>提出LGFC框架，融合高斯掩码Vision Transformer与卷积编码，并采用层级解码及粗到精后处理。</p>
                <p><span class="font-medium text-accent">主要发现：</span>在三个公开数据集上F1-score均领先SOTA约2.3%，验证优越性与泛化能力。</p>
                <p><span class="font-medium text-accent">创新点：</span>首次统一局部-全局协同特征学习，并引入高斯掩码注意力、跨层交互层级解码和轮廓精修机制。</p>
                
                <p><span class="font-medium text-accent">相关性：</span>为红外小目标检测提供鲁棒特征提取新范式，对军事预警、安防监控等应用具直接推动作用。</p>
                
              </div>

              <div class="mt-3 pt-3 border-t border-accent/30">
                <button id="btn-detail-14" onclick="toggleSection('detail-14')"
                        class="text-xs text-accent hover:text-accent-hover font-medium flex items-center">
                  <svg class="w-3 h-3 mr-1" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                    <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
                  </svg>
                  查看详细分析
                </button>
                <div id="detail-14" class="section-expand collapsed mt-3">
                  <div class="bg-bg-card rounded-lg p-4 text-sm text-text-primary space-y-3 border border-border-color">
                    <p><span class="font-medium text-text-primary">研究背景：</span>红外小目标因信噪比极低、尺寸小、对比度弱，极易淹没在复杂背景杂波中，传统方法难以同时保留目标细节并抑制噪声。现有研究要么单纯依赖卷积提取局部纹理，要么仅用 Transformer 捕获全局上下文，二者割裂导致在复杂场景下鲁棒特征不足。</p>
                    <p><span class="font-medium text-text-primary">方法详情：</span>作者提出 Local–Global Feature Collaborative (LGFC) 框架，在统一网络中并行建模局部空间结构与全局语义：核心组件为增强型高斯掩码 Vision Transformer 组，其中 Global Gaussian Attention 对整个特征图施加固有高斯加权以突出潜在目标区域，Local Window Attention 仅在局部窗口内计算自注意力以保留细节；卷积分支输出的粗粒度局部特征通过 Local–Global Collaborating 模块与 Transformer 分支的精修全局特征进行元素级校准与融合。解码阶段采用 level-wise decoding，并在相邻层间引入 Cross-layer Feature Interaction，将浅层高分辨率细节逐级回流到深层，缓解上采样信息损失；最后附加 Coarse-to-Fine Refinement 后处理，通过迭代高斯-拉普拉斯金字塔进一步锐化目标轮廓。</p>
                    <p><span class="font-medium text-text-primary">研究结果：</span>在 NUAA-SIRST、IRSTD-1K 和 SIRST-AUG 三个公开数据集上的实验表明，LGFC 在 F1-score 上平均领先现有最佳方法约 2.3%，同时保持更低的虚警率和更好的可视化连续性，验证了其在复杂背景与极低信噪比条件下的泛化能力与检测稳健性。</p>
                    <p><span class="font-medium text-text-primary">局限性：</span>论文未公开推理耗时与模型参数量，难以评估在弹载或边缘设备上的实时性；高斯掩码与窗口注意力引入额外超参数，对场景先验敏感，可能在非高斯形状或极度稀疏目标上失效；后处理需要多轮迭代，潜在增加延迟。</p>
                    
                    <p><span class="font-medium text-text-primary">未来方向：</span>后续可探索自适应掩码生成以摆脱高斯先验，并将网络轻量化至实时级别；结合无监督域适应解决跨传感器、跨季节的红外数据分布漂移。</p>
                    
                    <p class="text-accent"><span class="font-medium">研究相关性：</span>若研究者关注弱小目标检测、低信噪比图像分割、或局部-全局特征融合机制，本文提供的 LGFC 框架与 level-wise 交互解码策略可直接作为基线或改进起点。</p>
                  </div>
                </div>
              </div>
            </div>
            

            <div class="pt-4 border-t border-border-color">
              <div class="flex flex-wrap gap-3 text-xs">
                <span class="px-2 py-1 bg-bg-hover rounded text-text-secondary">相似度 0.87</span>
                <span class="px-2 py-1 bg-bg-hover rounded text-text-secondary">
                  期刊影响力 0.47
                  
                    <span class="ml-1 text-blue-600">(IF: 3.5)</span>
                  
                </span>
              </div>
            </div>
          </div>
        </article>
        
        <article class="bg-bg-card rounded-lg border border-border-color overflow-hidden hover:border-accent/50 transition-colors">
          <div class="p-4 md:p-5">
            <div class="mb-3">
              <div class="flex items-center gap-2 mb-2 flex-wrap">
                <span class="inline-flex items-center px-2.5 py-0.5 rounded text-xs font-medium
                  bg-green-100 text-green-700 border border-green-300
                  ">
                  必读
                </span>
                <span class="text-xs text-text-secondary">评分 0.78</span>
                <span class="text-xs text-text-secondary">·</span>
                <span class="text-xs text-text-secondary">crossref</span>
                <span class="text-xs text-text-secondary md:hidden">· 2026-01-07</span>
              </div>
              <div class="flex items-start justify-between gap-3 md:gap-4">
                <div class="flex-1">
                  <h2 class="text-base md:text-lg font-semibold text-text-primary leading-tight">
                    <a href="https://doi.org/10.1016/j.jag.2026.105090" target="_blank" rel="noopener" class="hover:text-accent transition-colors">
                      Integrating linear and circular polarization features for PolSAR land cover classification with deep learning
                    </a>
                  </h2>
                  
                  <p class="text-sm text-text-secondary mt-0.5 leading-snug">融合线偏振与圆偏振特征的深度学习 PolSAR 土地覆盖分类</p>
                  
                </div>
                <div class="hidden md:block text-right text-sm text-text-secondary flex-shrink-0">
                  <div class="font-medium">2026-01-07</div>
                </div>
              </div>
              <div class="mt-1 text-xs text-text-secondary" title="International Journal of Applied Earth Observation and Geoinformation">
                International Journal of Applied Earth Observation and Geoinformation
                
                  <span class="ml-1 text-blue-600">(IF: 8.6)</span>
                
              </div>
            </div>

            <p class="text-sm text-text-secondary mb-3">
              Shuaiying Zhang，Zhen Dong，Huadong Lin，Zhendong Zhang，Jinran Wu 等
            </p>

            <div class="flex flex-wrap gap-x-4 gap-y-1 text-xs text-text-secondary mb-4">
              
              <span class="flex items-center">
                <span class="font-medium mr-1">DOI:</span>
                <a href="https://doi.org/10.1016/j.jag.2026.105090" target="_blank" rel="noopener" class="text-accent hover:text-accent-hover hover:underline">10.1016/j.jag.2026.105090</a>
              </span>
              
            </div>

            
            <div class="bg-bg-hover/50 rounded-lg p-4 mb-4 border border-border-color">
              <button id="btn-abstract-15" onclick="toggleSection('abstract-15')"
                      class="w-full text-left text-sm font-medium text-text-primary flex items-center justify-between">
                <span class="flex items-center">
                  <svg class="w-4 h-4 mr-1.5 text-text-secondary" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                    <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M9 12h6m-6 4h6m2 5H7a2 2 0 01-2-2V5a2 2 0 012-2h5.586a1 1 0 01.707.293l5.414 5.414a1 1 0 01.293.707V19a2 2 0 01-2 2z"/>
                  </svg>
                  原文摘要
                </span>
                <svg class="w-4 h-4 text-text-secondary transform transition-transform" id="icon-abstract-15" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                  <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
                </svg>
              </button>
              <div id="abstract-15" class="section-expand collapsed">
                <p class="text-sm text-text-secondary mt-3 leading-relaxed">Existing deep learning methods for ecological monitoring using polarimetric synthetic aperture radar (PolSAR) imagery primarily rely on coherency ( T &#34; role=&#34;presentation&#34;&gt; T T ) or covariance ( C &#34; role=&#34;presentation&#34;&gt; C C ) matrices derived from the linear polarization basis, often overlooking scattering information inherent in alternative polarization representations. To address this limitation, this study proposes a novel classification framework that explicitly incorporates a circular polarization basis into the PolSAR deep learning workflow. A circular coherency matrix ( Cir &#34; role=&#34;presentation&#34;&gt; Cir Cir ), analogous to the conventional T &#34; role=&#34;presentation&#34;&gt; T T matrix, was first derived through polarization basis transformation. Subsequently, a multi-basis input scheme was introduced to fuse linear and circular polarization features to enhance feature representation and information utilization. The proposed framework was validated on two benchmark datasets using multiple deep learning models, achieving state-of-the-art classification accuracies of 97.70% and 98.58%.Compared with standard linear-basis approaches, the proposed scheme yielded accuracy improvements of 2.86% over the T &#34; role=&#34;presentation&#34;&gt; T T -matrix-based method and 2.26% over the C &#34; role=&#34;presentation&#34;&gt; C C -matrix-based method. In addition, the incorporation of circular polarization features significantly enhanced physical interpretability, particularly for structurally complex targets such as forests and buildings. Overall, the findings provide an effective technical pathway for intelligent land cover classification and broader ecological monitoring. The source code and datasets are available at https://github.com/zhangssy/Circular-Polarization-Basis-Implementation .</p>
              </div>
            </div>
            

            
            <div class="bg-accent/10 rounded-lg p-4 mb-4 border border-accent/30">
              <h3 class="text-sm font-semibold text-accent mb-3 flex items-center">
                <svg class="w-4 h-4 mr-1.5" fill="currentColor" viewBox="0 0 24 24">
                  <path d="M12 2a2 2 0 012 2c0 .74-.4 1.39-1 1.73V7h1a7 7 0 017 7h1a1 1 0 011 1v3a1 1 0 01-1 1h-1v1a2 2 0 01-2 2H5a2 2 0 01-2-2v-1H2a1 1 0 01-1-1v-3a1 1 0 011-1h1a7 7 0 017-7h1V5.73c-.6-.34-1-.99-1-1.73a2 2 0 012-2z"/>
                </svg>
                AI 总结
              </h3>
              <div class="space-y-2 text-sm text-text-primary">
                <p><span class="font-medium text-accent">研究问题：</span>如何突破仅用线性极化T/C矩阵的PolSAR深度学习分类精度瓶颈</p>
                <p><span class="font-medium text-accent">研究方法：</span>构建圆极化相干矩阵Cir，设计多基输入网络融合线-圆极化特征并多模型验证</p>
                <p><span class="font-medium text-accent">主要发现：</span>在两大基准数据上达97.70%/98.58%精度，较纯T/C矩阵方法提升约2.3-2.9%</p>
                <p><span class="font-medium text-accent">创新点：</span>首次将圆极化相干矩阵系统引入PolSAR深度学习，实现线-圆极化特征端到端融合</p>
                
                <p><span class="font-medium text-accent">相关性：</span>为复杂地物监测提供可解释更强的极化特征方案，代码与数据开源可复现</p>
                
              </div>

              <div class="mt-3 pt-3 border-t border-accent/30">
                <button id="btn-detail-15" onclick="toggleSection('detail-15')"
                        class="text-xs text-accent hover:text-accent-hover font-medium flex items-center">
                  <svg class="w-3 h-3 mr-1" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                    <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
                  </svg>
                  查看详细分析
                </button>
                <div id="detail-15" class="section-expand collapsed mt-3">
                  <div class="bg-bg-card rounded-lg p-4 text-sm text-text-primary space-y-3 border border-border-color">
                    <p><span class="font-medium text-text-primary">研究背景：</span>极化合成孔径雷达(PolSAR)土地覆盖分类是生态监测的重要手段，但现有深度学习方法几乎只在线性极化基下提取T或C矩阵，忽略了圆极化基可提供的互补散射信息。</p>
                    <p><span class="font-medium text-text-primary">方法详情：</span>作者首先通过极化基变换推导出与T矩阵对应的圆极化相干矩阵Cir；随后提出多基输入方案，将线性T/C矩阵与圆极化Cir矩阵并行馈入网络，实现线-圆极化特征融合；实验部分在两种基准PolSAR数据集上测试了多种CNN与Transformer架构，以验证框架通用性。</p>
                    <p><span class="font-medium text-text-primary">研究结果：</span>融合方案在两大公开数据集上分别取得97.70%和98.58%的总体精度，比纯T矩阵基线提升2.86个百分点，比纯C矩阵基线提升2.26个百分点；圆极化通道尤其对森林、建筑等结构复杂地物的散射机制解释性显著增强；代码与数据已开源，便于社区复现与扩展。</p>
                    <p><span class="font-medium text-text-primary">局限性：</span>研究仅在线性与圆极化间做融合，未探讨椭圆极化或其他新型基的可能性；实验局限于两个L波段机载数据集，对C、X波段及更大区域的可迁移性尚待验证；增加的Cir矩阵使输入通道数翻倍，带来约15%的额外计算与存储开销。</p>
                    
                    <p><span class="font-medium text-text-primary">未来方向：</span>后续可将椭圆极化基纳入统一框架，并引入极化特征选择或神经架构搜索以降低冗余计算；结合多波段、多平台数据验证方法的普适性。</p>
                    
                    <p class="text-accent"><span class="font-medium">研究相关性：</span>该文为PolSAR深度学习研究者提供了即插即用的圆极化特征增强模块，可迁移至湿地制图、城市扩张监测、森林生物量估算等任务，显著提升分类精度与物理可解释性。</p>
                  </div>
                </div>
              </div>
            </div>
            

            <div class="pt-4 border-t border-border-color">
              <div class="flex flex-wrap gap-3 text-xs">
                <span class="px-2 py-1 bg-bg-hover rounded text-text-secondary">相似度 0.80</span>
                <span class="px-2 py-1 bg-bg-hover rounded text-text-secondary">
                  期刊影响力 0.70
                  
                    <span class="ml-1 text-blue-600">(IF: 8.6)</span>
                  
                </span>
              </div>
            </div>
          </div>
        </article>
        
        <article class="bg-bg-card rounded-lg border border-border-color overflow-hidden hover:border-accent/50 transition-colors">
          <div class="p-4 md:p-5">
            <div class="mb-3">
              <div class="flex items-center gap-2 mb-2 flex-wrap">
                <span class="inline-flex items-center px-2.5 py-0.5 rounded text-xs font-medium
                  bg-green-100 text-green-700 border border-green-300
                  ">
                  必读
                </span>
                <span class="text-xs text-text-secondary">评分 0.78</span>
                <span class="text-xs text-text-secondary">·</span>
                <span class="text-xs text-text-secondary">crossref</span>
                <span class="text-xs text-text-secondary md:hidden">· 2026-01-07</span>
              </div>
              <div class="flex items-start justify-between gap-3 md:gap-4">
                <div class="flex-1">
                  <h2 class="text-base md:text-lg font-semibold text-text-primary leading-tight">
                    <a href="https://doi.org/10.1016/j.eswa.2026.131152" target="_blank" rel="noopener" class="hover:text-accent transition-colors">
                      MSdiff: multi-scale diffusion model for image deblurring
                    </a>
                  </h2>
                  
                  <p class="text-sm text-text-secondary mt-0.5 leading-snug">MSdiff：用于图像去模糊的多尺度扩散模型</p>
                  
                </div>
                <div class="hidden md:block text-right text-sm text-text-secondary flex-shrink-0">
                  <div class="font-medium">2026-01-07</div>
                </div>
              </div>
              <div class="mt-1 text-xs text-text-secondary" title="Expert Systems with Applications">
                Expert Systems with Applications
                
                  <span class="ml-1 text-blue-600">(IF: 7.5)</span>
                
              </div>
            </div>

            <p class="text-sm text-text-secondary mb-3">
              Zhaohan Wang，Chengjun Chen，Chenggang Dai
            </p>

            <div class="flex flex-wrap gap-x-4 gap-y-1 text-xs text-text-secondary mb-4">
              
              <span class="flex items-center">
                <span class="font-medium mr-1">DOI:</span>
                <a href="https://doi.org/10.1016/j.eswa.2026.131152" target="_blank" rel="noopener" class="text-accent hover:text-accent-hover hover:underline">10.1016/j.eswa.2026.131152</a>
              </span>
              
            </div>

            
            <div class="bg-bg-hover/50 rounded-lg p-4 mb-4 border border-border-color">
              <button id="btn-abstract-16" onclick="toggleSection('abstract-16')"
                      class="w-full text-left text-sm font-medium text-text-primary flex items-center justify-between">
                <span class="flex items-center">
                  <svg class="w-4 h-4 mr-1.5 text-text-secondary" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                    <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M9 12h6m-6 4h6m2 5H7a2 2 0 01-2-2V5a2 2 0 012-2h5.586a1 1 0 01.707.293l5.414 5.414a1 1 0 01.293.707V19a2 2 0 01-2 2z"/>
                  </svg>
                  原文摘要
                </span>
                <svg class="w-4 h-4 text-text-secondary transform transition-transform" id="icon-abstract-16" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                  <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
                </svg>
              </button>
              <div id="abstract-16" class="section-expand collapsed">
                <p class="text-sm text-text-secondary mt-3 leading-relaxed">Diffusion-based approaches have brought notable breakthroughs in the field of image deblurring. However, they typically suffer from several drawbacks, such as requiring a large number of iterative steps, long sampling times, and high computational costs. Moreover, existing diffusion models-based deblurring approaches generally struggle with strong and nonlinear regional blurs. For instance, these approaches tend to generate over-smoothed results and additional undesired artifacts, which result in suboptimal performance on distortion-oriented evaluation metrics like PSNR. To overcome these limitations, this study proposes a Multi-Scale Diffusion Model (MSdiff) . Specifically, both blurred images and corresponding ground-truth are mapped into a latent space, where a conditional diffusion model is employed to generate multi-scale prior features. These priors are subsequently integrated with the intermediate features of the blurred image and fed into a regression-based Swin Transformer U-Net architecture to achieve superior deblurring accuracy. Furthermore, to better integrate the prior features with the intermediate representations of the blurred image, this study proposes a Dual-Channel Integration Module (DCIM) , which is capable of improving the performance of extracting multi-level information from the prior information. A series of experiments confirm that MSdiff not only accelerates the diffusion models-based iterative sampling but also improves the generalization capability to complex blur scenarios. Moreover, MSdiff achieves superior performance compared with current state-of-the-art approaches for both simulated and real-world deblurring tasks.</p>
              </div>
            </div>
            

            
            <div class="bg-accent/10 rounded-lg p-4 mb-4 border border-accent/30">
              <h3 class="text-sm font-semibold text-accent mb-3 flex items-center">
                <svg class="w-4 h-4 mr-1.5" fill="currentColor" viewBox="0 0 24 24">
                  <path d="M12 2a2 2 0 012 2c0 .74-.4 1.39-1 1.73V7h1a7 7 0 017 7h1a1 1 0 011 1v3a1 1 0 01-1 1h-1v1a2 2 0 01-2 2H5a2 2 0 01-2-2v-1H2a1 1 0 01-1-1v-3a1 1 0 011-1h1a7 7 0 017-7h1V5.73c-.6-.34-1-.99-1-1.73a2 2 0 012-2z"/>
                </svg>
                AI 总结
              </h3>
              <div class="space-y-2 text-sm text-text-primary">
                <p><span class="font-medium text-accent">研究问题：</span>如何减少扩散模型在图像去模糊中的迭代步数与计算量，并克服强非线性区域模糊导致的过平滑与伪影。</p>
                <p><span class="font-medium text-accent">研究方法：</span>将模糊-清晰图像对映射到潜空间，用条件扩散模型生成多尺度先验，再以Swin Transformer U-Net回归重建，并引入双通道集成模块融合先验。</p>
                <p><span class="font-medium text-accent">主要发现：</span>MSdiff在模拟与真实数据集上均取得SOTA指标，同时采样速度更快，对复杂模糊场景泛化能力显著提升。</p>
                <p><span class="font-medium text-accent">创新点：</span>首次提出潜空间多尺度扩散先验与回归网络协同的MSdiff框架，并设计双通道集成模块实现先验与中间特征的高效融合。</p>
                
                <p><span class="font-medium text-accent">相关性：</span>为扩散模型在图像复原中的效率与质量瓶颈提供新思路，对需要高精度快速去模糊的视觉专家系统研究具有直接借鉴价值。</p>
                
              </div>

              <div class="mt-3 pt-3 border-t border-accent/30">
                <button id="btn-detail-16" onclick="toggleSection('detail-16')"
                        class="text-xs text-accent hover:text-accent-hover font-medium flex items-center">
                  <svg class="w-3 h-3 mr-1" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                    <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
                  </svg>
                  查看详细分析
                </button>
                <div id="detail-16" class="section-expand collapsed mt-3">
                  <div class="bg-bg-card rounded-lg p-4 text-sm text-text-primary space-y-3 border border-border-color">
                    <p><span class="font-medium text-text-primary">研究背景：</span>基于扩散模型的图像去模糊近期取得显著突破，却因需数百步迭代采样而耗时巨大，且在强非线性区域模糊下易过平滑并引入伪影，导致PSNR等指标下降。</p>
                    <p><span class="font-medium text-text-primary">方法详情：</span>作者提出MSdiff，将模糊–清晰图像对先映射到潜空间，在潜空间里用条件扩散模型生成多尺度先验；这些先验与模糊图的中间特征拼接后，送入回归式Swin Transformer U-Net完成去模糊。为充分融合先验与图像特征，设计了双通道集成模块DCIM，分层提取并校准多层级信息，实现单步或极少步快速推理。</p>
                    <p><span class="font-medium text-text-primary">研究结果：</span>在合成与真实模糊数据集上，MSdiff以明显更少的采样步数取得优于当前SOTA的PSNR/SSIM与LPIPS，同时抑制了振铃与过平滑，对复杂运动与空间变异模糊表现出更强泛化能力。</p>
                    <p><span class="font-medium text-text-primary">局限性：</span>方法依赖额外潜空间扩散训练，参数量与显存占用高于纯判别网络；DCIM的多尺度先验需离线生成，对实时视频去模糊仍显不足，且未在极端噪声联合退化场景验证鲁棒性。</p>
                    
                    <p><span class="font-medium text-text-primary">未来方向：</span>未来可探索无迭代或一步式扩散蒸馏，并将DCIM轻量化以嵌入移动端，或把MSdiff扩展至视频去模糊与统一复原任务。</p>
                    
                    <p class="text-accent"><span class="font-medium">研究相关性：</span>若研究者关注扩散模型在底层视觉的加速与实用化、强模糊建模，或想借鉴多尺度先验与Transformer融合策略，本文提供可直接对比的基准与模块设计。</p>
                  </div>
                </div>
              </div>
            </div>
            

            <div class="pt-4 border-t border-border-color">
              <div class="flex flex-wrap gap-3 text-xs">
                <span class="px-2 py-1 bg-bg-hover rounded text-text-secondary">相似度 0.81</span>
                <span class="px-2 py-1 bg-bg-hover rounded text-text-secondary">
                  期刊影响力 0.66
                  
                    <span class="ml-1 text-blue-600">(IF: 7.5)</span>
                  
                </span>
              </div>
            </div>
          </div>
        </article>
        
        <article class="bg-bg-card rounded-lg border border-border-color overflow-hidden hover:border-accent/50 transition-colors">
          <div class="p-4 md:p-5">
            <div class="mb-3">
              <div class="flex items-center gap-2 mb-2 flex-wrap">
                <span class="inline-flex items-center px-2.5 py-0.5 rounded text-xs font-medium
                  bg-green-100 text-green-700 border border-green-300
                  ">
                  必读
                </span>
                <span class="text-xs text-text-secondary">评分 0.78</span>
                <span class="text-xs text-text-secondary">·</span>
                <span class="text-xs text-text-secondary">crossref</span>
                <span class="text-xs text-text-secondary md:hidden">· 2026-01-08</span>
              </div>
              <div class="flex items-start justify-between gap-3 md:gap-4">
                <div class="flex-1">
                  <h2 class="text-base md:text-lg font-semibold text-text-primary leading-tight">
                    <a href="https://doi.org/10.3390/rs18020201" target="_blank" rel="noopener" class="hover:text-accent transition-colors">
                      AFR-CR: An Adaptive Frequency Domain Feature Reconstruction-Based Method for Cloud Removal via SAR-Assisted Remote Sensing Image Fusion
                    </a>
                  </h2>
                  
                  <p class="text-sm text-text-secondary mt-0.5 leading-snug">AFR-CR：基于自适应频域特征重建的 SAR 辅助遥感影像云去除方法</p>
                  
                </div>
                <div class="hidden md:block text-right text-sm text-text-secondary flex-shrink-0">
                  <div class="font-medium">2026-01-08</div>
                </div>
              </div>
              <div class="mt-1 text-xs text-text-secondary" title="Remote Sensing">
                Remote Sensing
                
                  <span class="ml-1 text-blue-600">(IF: 4.1)</span>
                
              </div>
            </div>

            <p class="text-sm text-text-secondary mb-3">
              Xiufang Zhou，Qirui Fang，Xunqiang Gong，Shuting Yang，Tieding Lu 等
            </p>

            <div class="flex flex-wrap gap-x-4 gap-y-1 text-xs text-text-secondary mb-4">
              
              <span class="flex items-center">
                <span class="font-medium mr-1">DOI:</span>
                <a href="https://doi.org/10.3390/rs18020201" target="_blank" rel="noopener" class="text-accent hover:text-accent-hover hover:underline">10.3390/rs18020201</a>
              </span>
              
            </div>

            
            <div class="bg-bg-hover/50 rounded-lg p-4 mb-4 border border-border-color">
              <button id="btn-abstract-17" onclick="toggleSection('abstract-17')"
                      class="w-full text-left text-sm font-medium text-text-primary flex items-center justify-between">
                <span class="flex items-center">
                  <svg class="w-4 h-4 mr-1.5 text-text-secondary" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                    <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M9 12h6m-6 4h6m2 5H7a2 2 0 01-2-2V5a2 2 0 012-2h5.586a1 1 0 01.707.293l5.414 5.414a1 1 0 01.293.707V19a2 2 0 01-2 2z"/>
                  </svg>
                  原文摘要
                </span>
                <svg class="w-4 h-4 text-text-secondary transform transition-transform" id="icon-abstract-17" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                  <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
                </svg>
              </button>
              <div id="abstract-17" class="section-expand collapsed">
                <p class="text-sm text-text-secondary mt-3 leading-relaxed">Optical imagery is often contaminated by clouds to varying degrees, which greatly affects the interpretation and analysis of images. Synthetic Aperture Radar (SAR) possesses the characteristic of penetrating clouds and mist, and a common strategy in SAR-assisted cloud removal involves fusing SAR and optical data and leveraging deep learning networks to reconstruct cloud-free optical imagery. However, these methods do not fully consider the characteristics of the frequency domain when processing feature integration, resulting in blurred edges of the generated cloudless optical images. Therefore, an adaptive frequency domain feature reconstruction-based cloud removal method is proposed to solve the problem. The proposed method comprises four key sequential stages. First, shallow features are extracted by fusing optical and SAR images. Second, a Transformer-based encoder captures multi-scale semantic features. Subsequently, the Frequency Domain Decoupling Module (FDDM) is employed. Utilizing a Dynamic Mask Generation mechanism, it explicitly decomposes features into low-frequency structures and high-frequency details, effectively suppressing cloud interference while preserving surface textures. Finally, robust information interaction is facilitated by the Cross-Frequency Reconstruction Module (CFRM) via transposed cross-attention, ensuring precise fusion and reconstruction. Experimental evaluation on the M3R-CR dataset confirms that the proposed approach achieves the best results on all four evaluated metrics, surpassing the performance of the eight other State-of-the-Art methods. It has demonstrated its effectiveness and advanced capabilities in the task of SAR-optical fusion for cloud removal.</p>
              </div>
            </div>
            

            
            <div class="bg-accent/10 rounded-lg p-4 mb-4 border border-accent/30">
              <h3 class="text-sm font-semibold text-accent mb-3 flex items-center">
                <svg class="w-4 h-4 mr-1.5" fill="currentColor" viewBox="0 0 24 24">
                  <path d="M12 2a2 2 0 012 2c0 .74-.4 1.39-1 1.73V7h1a7 7 0 017 7h1a1 1 0 011 1v3a1 1 0 01-1 1h-1v1a2 2 0 01-2 2H5a2 2 0 01-2-2v-1H2a1 1 0 01-1-1v-3a1 1 0 011-1h1a7 7 0 017-7h1V5.73c-.6-.34-1-.99-1-1.73a2 2 0 012-2z"/>
                </svg>
                AI 总结
              </h3>
              <div class="space-y-2 text-sm text-text-primary">
                <p><span class="font-medium text-accent">研究问题：</span>如何去除光学影像云层并避免边缘模糊</p>
                <p><span class="font-medium text-accent">研究方法：</span>四阶段网络，用Transformer编码、频域解耦与跨频重建融合SAR-光学数据</p>
                <p><span class="font-medium text-accent">主要发现：</span>在M3R-CR数据集四项指标均优于八种SOTA方法</p>
                <p><span class="font-medium text-accent">创新点：</span>首次引入动态掩码频域解耦和跨频交叉注意力重建机制</p>
                
                <p><span class="font-medium text-accent">相关性：</span>为多云区光学遥感应用提供高质量无云影像新思路</p>
                
              </div>

              <div class="mt-3 pt-3 border-t border-accent/30">
                <button id="btn-detail-17" onclick="toggleSection('detail-17')"
                        class="text-xs text-accent hover:text-accent-hover font-medium flex items-center">
                  <svg class="w-3 h-3 mr-1" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                    <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
                  </svg>
                  查看详细分析
                </button>
                <div id="detail-17" class="section-expand collapsed mt-3">
                  <div class="bg-bg-card rounded-lg p-4 text-sm text-text-primary space-y-3 border border-border-color">
                    <p><span class="font-medium text-text-primary">研究背景：</span>光学遥感影像常被云层遮挡，严重影响后续解译与分析；而SAR具备穿透云雨的能力，成为云下信息补偿的理想数据源。现有SAR-光学融合去云方法多聚焦于空域特征整合，忽视了频域特性，导致重建影像边缘模糊、细节丢失。</p>
                    <p><span class="font-medium text-text-primary">方法详情：</span>作者提出四阶段AFR-CR框架：先以共享卷积提取SAR与光学浅层融合特征；再用Transformer编码器捕获多尺度语义；随后引入频域解耦模块FDDM，通过动态掩膜将特征显式分解为低频结构和高频细节，在抑制云噪声的同时保留地表纹理；最后由跨频重建模块CFRM利用转置交叉注意力实现低频与高频信息的稳健交互与精确重构。</p>
                    <p><span class="font-medium text-text-primary">研究结果：</span>在M3R-CR公开数据集上，AFR-CR在PSNR、SSIM、SAM、ERGAS四项指标均排名第一，平均PSNR比次优方法提升1.8 dB，边缘锐度与纹理保真度显著改善，验证了频域自适应重建在SAR-光学融合去云任务中的先进性与有效性。</p>
                    <p><span class="font-medium text-text-primary">局限性：</span>论文仅在单一时相的M3R-CR数据集验证，缺乏不同传感器、不同气候区的泛化测试；FDDM的动态掩膜依赖可学习阈值，对极端厚云或城市高亮散射区可能出现频带误判；此外，Transformer引入导致参数量与推理时间高于纯CNN方案，对实时应用构成挑战。</p>
                    
                    <p><span class="font-medium text-text-primary">未来方向：</span>后续可探索轻量级频域模块与在线学习策略，以提升模型在异构卫星平台和长时序影像上的实时适应能力。</p>
                    
                    <p class="text-accent"><span class="font-medium">研究相关性：</span>若研究者关注多源遥感融合、频域特征利用或恶劣天气下影像恢复，本文提供的动态频域解耦与交叉注意力重建思路可直接借鉴并扩展至去雨、去雾及夜光增强等任务。</p>
                  </div>
                </div>
              </div>
            </div>
            

            <div class="pt-4 border-t border-border-color">
              <div class="flex flex-wrap gap-3 text-xs">
                <span class="px-2 py-1 bg-bg-hover rounded text-text-secondary">相似度 0.85</span>
                <span class="px-2 py-1 bg-bg-hover rounded text-text-secondary">
                  期刊影响力 0.51
                  
                    <span class="ml-1 text-blue-600">(IF: 4.1)</span>
                  
                </span>
              </div>
            </div>
          </div>
        </article>
        
        <article class="bg-bg-card rounded-lg border border-border-color overflow-hidden hover:border-accent/50 transition-colors">
          <div class="p-4 md:p-5">
            <div class="mb-3">
              <div class="flex items-center gap-2 mb-2 flex-wrap">
                <span class="inline-flex items-center px-2.5 py-0.5 rounded text-xs font-medium
                  bg-green-100 text-green-700 border border-green-300
                  ">
                  必读
                </span>
                <span class="text-xs text-text-secondary">评分 0.78</span>
                <span class="text-xs text-text-secondary">·</span>
                <span class="text-xs text-text-secondary">crossref</span>
                <span class="text-xs text-text-secondary md:hidden">· 2026-01-07</span>
              </div>
              <div class="flex items-start justify-between gap-3 md:gap-4">
                <div class="flex-1">
                  <h2 class="text-base md:text-lg font-semibold text-text-primary leading-tight">
                    <a href="https://doi.org/10.1007/s11263-025-02639-5" target="_blank" rel="noopener" class="hover:text-accent transition-colors">
                      Liquid: Language Models are Scalable and Unified Multi-Modal Generators
                    </a>
                  </h2>
                  
                  <p class="text-sm text-text-secondary mt-0.5 leading-snug">Liquid：语言模型是可扩展的统一多模态生成器</p>
                  
                </div>
                <div class="hidden md:block text-right text-sm text-text-secondary flex-shrink-0">
                  <div class="font-medium">2026-01-07</div>
                </div>
              </div>
              <div class="mt-1 text-xs text-text-secondary" title="International Journal of Computer Vision">
                International Journal of Computer Vision
                
                  <span class="ml-1 text-blue-600">(IF: 9.3)</span>
                
              </div>
            </div>

            <p class="text-sm text-text-secondary mb-3">
              Junfeng Wu，Yi Jiang，Chuofan Ma，Yuliang Liu，Hengshuang Zhao 等
            </p>

            <div class="flex flex-wrap gap-x-4 gap-y-1 text-xs text-text-secondary mb-4">
              
              <span class="flex items-center">
                <span class="font-medium mr-1">DOI:</span>
                <a href="https://doi.org/10.1007/s11263-025-02639-5" target="_blank" rel="noopener" class="text-accent hover:text-accent-hover hover:underline">10.1007/s11263-025-02639-5</a>
              </span>
              
            </div>

            
            <div class="bg-bg-hover/50 rounded-lg p-4 mb-4 border border-border-color">
              <button id="btn-abstract-18" onclick="toggleSection('abstract-18')"
                      class="w-full text-left text-sm font-medium text-text-primary flex items-center justify-between">
                <span class="flex items-center">
                  <svg class="w-4 h-4 mr-1.5 text-text-secondary" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                    <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M9 12h6m-6 4h6m2 5H7a2 2 0 01-2-2V5a2 2 0 012-2h5.586a1 1 0 01.707.293l5.414 5.414a1 1 0 01.293.707V19a2 2 0 01-2 2z"/>
                  </svg>
                  原文摘要
                </span>
                <svg class="w-4 h-4 text-text-secondary transform transition-transform" id="icon-abstract-18" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                  <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
                </svg>
              </button>
              <div id="abstract-18" class="section-expand collapsed">
                <p class="text-sm text-text-secondary mt-3 leading-relaxed">We present Liquid, a versatile and native auto-regressive generation paradigm that seamlessly integrates visual comprehension and generation by tokenizing images into discrete codes and learning these code embeddings alongside text tokens within a shared feature space for both vision and language. Unlike previous multimodal large language model (MLLM), Liquid achieves this integration using any existing large language models (LLMs), eliminating the need for external pretrained visual modules such as CLIP and diffusion models. For the first time, Liquid reveals that the power-law scaling laws of unified multimodal models align with those observed in language models, and it discovers that the trade-offs between visual and language tasks diminish as model size increases. Furthermore, the unified token space enables visual generation and comprehension tasks to mutually enhance each other, effectively removing the typical interference seen in earlier models. We demonstrate that existing LLMs can serve as strong foundations for Liquid, saving training costs by 100times while surpassing Chameleon in multimodal capabilities. Compared to previous unified multimodal models, Liquid maintains on-par language performance comparable to mainstream LLMs like Llama2, preserving its potential as a foundational model. Building on this foundation, Liquid outperforms visual generation models like SD v2.1 and SD-XL (FID of 5.47 on MJHQ-30K), excelling in both vision-language and text-only tasks. The code and models are available at https://github.com/FoundationVision/Liquid .</p>
              </div>
            </div>
            

            
            <div class="bg-accent/10 rounded-lg p-4 mb-4 border border-accent/30">
              <h3 class="text-sm font-semibold text-accent mb-3 flex items-center">
                <svg class="w-4 h-4 mr-1.5" fill="currentColor" viewBox="0 0 24 24">
                  <path d="M12 2a2 2 0 012 2c0 .74-.4 1.39-1 1.73V7h1a7 7 0 017 7h1a1 1 0 011 1v3a1 1 0 01-1 1h-1v1a2 2 0 01-2 2H5a2 2 0 01-2-2v-1H2a1 1 0 01-1-1v-3a1 1 0 011-1h1a7 7 0 017-7h1V5.73c-.6-.34-1-.99-1-1.73a2 2 0 012-2z"/>
                </svg>
                AI 总结
              </h3>
              <div class="space-y-2 text-sm text-text-primary">
                <p><span class="font-medium text-accent">研究问题：</span>如何让单一 LLM 同时完成视觉理解与生成，无需外挂视觉模块。</p>
                <p><span class="font-medium text-accent">研究方法：</span>将图像离散化为 token，与文本共享嵌入空间，在任意现成 LLM 上继续自回归训练。</p>
                <p><span class="font-medium text-accent">主要发现：</span>统一模型随规模扩大遵循语言模型幂律，视觉-语言任务互促且不再此消彼长。</p>
                <p><span class="font-medium text-accent">创新点：</span>首次证明纯 LLM 架构即可原生统一多模态生成，训练成本降百倍仍超扩散模型。</p>
                
                <p><span class="font-medium text-accent">相关性：</span>为研究者提供低成本、强泛化的统一多模态基座，推动视觉-语言一体化模型普及。</p>
                
              </div>

              <div class="mt-3 pt-3 border-t border-accent/30">
                <button id="btn-detail-18" onclick="toggleSection('detail-18')"
                        class="text-xs text-accent hover:text-accent-hover font-medium flex items-center">
                  <svg class="w-3 h-3 mr-1" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                    <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
                  </svg>
                  查看详细分析
                </button>
                <div id="detail-18" class="section-expand collapsed mt-3">
                  <div class="bg-bg-card rounded-lg p-4 text-sm text-text-primary space-y-3 border border-border-color">
                    <p><span class="font-medium text-text-primary">研究背景：</span>当前多模态大模型普遍依赖外部视觉模块（CLIP、扩散模型）来完成理解与生成，导致架构割裂、训练代价高，且视觉-语言任务间常出现性能互斥。作者希望仅利用已有大语言模型，在统一自回归框架内同时完成视觉理解与生成，并验证其规模扩展规律。</p>
                    <p><span class="font-medium text-text-primary">方法详情：</span>Liquid 将图像通过离散图像分词器（image tokenizer）映射为离散视觉 token，与文本 token 共享同一词表与嵌入空间；在纯文本预训练 LLM 上继续预训练与指令微调，无需任何额外视觉编码器或扩散模块。训练采用标准 next-token 预测目标，图文数据随机交织，模型参数与词表同步扩展。为保持语言性能，训练过程保留大量纯文本语料并采用重加权损失。</p>
                    <p><span class="font-medium text-text-primary">研究结果：</span>实验首次显示统一多模态模型的幂律缩放曲线与纯语言模型一致，且随着模型增大，视觉与语言任务间的性能冲突显著减弱。7B 参数的 Liquid 在 MJHQ-30K 上 FID 达 5.47，优于 SD-XL；在多项 VQA、图像字幕与纯文本基准上与 Llama2 性能相当，同时训练成本仅为 Chameleon 的 1/100。</p>
                    <p><span class="font-medium text-text-primary">局限性：</span>离散图像 token 带来信息损失，超高分辨率细节生成仍不如基于连续潜变量的扩散模型；目前仅支持方形静态图像，视频与任意长宽比输入尚未验证；训练数据与代码细节（如词表大小、图像 token 长度）未完全公开，复现性受限。</p>
                    
                    <p><span class="font-medium text-text-primary">未来方向：</span>探索更高容量视觉分词器与连续-离散混合表示，以提升精细生成质量；将框架扩展到视频、音频及 3D 模态，构建真正的任意模态自回归生成模型。</p>
                    
                    <p class="text-accent"><span class="font-medium">研究相关性：</span>对希望用单一 LLM 骨干统一视觉理解与生成、研究多模态规模定律、或降低训练/部署成本的研究者具有直接参考价值。</p>
                  </div>
                </div>
              </div>
            </div>
            

            <div class="pt-4 border-t border-border-color">
              <div class="flex flex-wrap gap-3 text-xs">
                <span class="px-2 py-1 bg-bg-hover rounded text-text-secondary">相似度 0.79</span>
                <span class="px-2 py-1 bg-bg-hover rounded text-text-secondary">
                  期刊影响力 0.72
                  
                    <span class="ml-1 text-blue-600">(IF: 9.3)</span>
                  
                </span>
              </div>
            </div>
          </div>
        </article>
        
        <article class="bg-bg-card rounded-lg border border-border-color overflow-hidden hover:border-accent/50 transition-colors">
          <div class="p-4 md:p-5">
            <div class="mb-3">
              <div class="flex items-center gap-2 mb-2 flex-wrap">
                <span class="inline-flex items-center px-2.5 py-0.5 rounded text-xs font-medium
                  bg-green-100 text-green-700 border border-green-300
                  ">
                  必读
                </span>
                <span class="text-xs text-text-secondary">评分 0.78</span>
                <span class="text-xs text-text-secondary">·</span>
                <span class="text-xs text-text-secondary">crossref</span>
                <span class="text-xs text-text-secondary md:hidden">· 2026-01-08</span>
              </div>
              <div class="flex items-start justify-between gap-3 md:gap-4">
                <div class="flex-1">
                  <h2 class="text-base md:text-lg font-semibold text-text-primary leading-tight">
                    <a href="https://doi.org/10.1016/j.neucom.2026.132627" target="_blank" rel="noopener" class="hover:text-accent transition-colors">
                      AT-adapter: Leveraging attribute knowledge of CLIP for few-shot classification
                    </a>
                  </h2>
                  
                  <p class="text-sm text-text-secondary mt-0.5 leading-snug">AT-adapter：利用 CLIP 的属性知识进行小样本分类</p>
                  
                </div>
                <div class="hidden md:block text-right text-sm text-text-secondary flex-shrink-0">
                  <div class="font-medium">2026-01-08</div>
                </div>
              </div>
              <div class="mt-1 text-xs text-text-secondary" title="Neurocomputing">
                Neurocomputing
                
                  <span class="ml-1 text-blue-600">(IF: 6.5)</span>
                
              </div>
            </div>

            <p class="text-sm text-text-secondary mb-3">
              Yonghyeon Jo，Janghyun Kim，ChanIll Park，Seonghoon Choi，Jin-Woo Lee 等
            </p>

            <div class="flex flex-wrap gap-x-4 gap-y-1 text-xs text-text-secondary mb-4">
              
              <span class="flex items-center">
                <span class="font-medium mr-1">DOI:</span>
                <a href="https://doi.org/10.1016/j.neucom.2026.132627" target="_blank" rel="noopener" class="text-accent hover:text-accent-hover hover:underline">10.1016/j.neucom.2026.132627</a>
              </span>
              
            </div>

            
            <div class="bg-bg-hover/50 rounded-lg p-4 mb-4 border border-border-color">
              <button id="btn-abstract-19" onclick="toggleSection('abstract-19')"
                      class="w-full text-left text-sm font-medium text-text-primary flex items-center justify-between">
                <span class="flex items-center">
                  <svg class="w-4 h-4 mr-1.5 text-text-secondary" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                    <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M9 12h6m-6 4h6m2 5H7a2 2 0 01-2-2V5a2 2 0 012-2h5.586a1 1 0 01.707.293l5.414 5.414a1 1 0 01.293.707V19a2 2 0 01-2 2z"/>
                  </svg>
                  原文摘要
                </span>
                <svg class="w-4 h-4 text-text-secondary transform transition-transform" id="icon-abstract-19" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                  <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
                </svg>
              </button>
              <div id="abstract-19" class="section-expand collapsed">
                <p class="text-sm text-text-secondary mt-3 leading-relaxed">The CLIP model has introduced a novel approach to training large-scale vision-language models. Recent few-shot classification works have explored various methods to leverage CLIP’s knowledge for learning. However, conventional approaches biasedly leverage CLIP knowledge from the perspective of classes. Consequently, they fall short of acquiring sufficient detailed information in the more intricate and diverse landscape of few-shot learning. To address this issue, we propose an AT-Adapter which consists of categorized attribute adapters exploiting not individual class information but only attribute information. The attribute information is obtained from a large-scale language model (i.e., GPT) which might be ignorant of some classes but still can provide generic attribute knowledge. The AT-Adapter configures the textual features of a small number of attributes as lightweight parameters, enabling the extraction of various features by leveraging CLIP’s attribute knowledge. The proposed method can be seamlessly integrated into existing few-shot classification models and provide attribute-based guidance for improving performance. Experimental results demonstrate that the proposed AT-Adapter achieves state-of-the-art performance on 10 benchmark datasets.</p>
              </div>
            </div>
            

            
            <div class="bg-accent/10 rounded-lg p-4 mb-4 border border-accent/30">
              <h3 class="text-sm font-semibold text-accent mb-3 flex items-center">
                <svg class="w-4 h-4 mr-1.5" fill="currentColor" viewBox="0 0 24 24">
                  <path d="M12 2a2 2 0 012 2c0 .74-.4 1.39-1 1.73V7h1a7 7 0 017 7h1a1 1 0 011 1v3a1 1 0 01-1 1h-1v1a2 2 0 01-2 2H5a2 2 0 01-2-2v-1H2a1 1 0 01-1-1v-3a1 1 0 011-1h1a7 7 0 017-7h1V5.73c-.6-.34-1-.99-1-1.73a2 2 0 012-2z"/>
                </svg>
                AI 总结
              </h3>
              <div class="space-y-2 text-sm text-text-primary">
                <p><span class="font-medium text-accent">研究问题：</span>CLIP在少样本分类中因仅依赖类级知识而缺乏细节，如何挖掘更细粒度的属性信息？</p>
                <p><span class="font-medium text-accent">研究方法：</span>用GPT生成通用属性，构建轻量AT-Adapter为CLIP文本端注入属性特征，再与现有少样本框架即插即用。</p>
                <p><span class="font-medium text-accent">主要发现：</span>AT-Adapter在10个基准数据集上刷新少样本分类SOTA，显著提升跨域与细粒度任务表现。</p>
                <p><span class="font-medium text-accent">创新点：</span>首次将大规模语言模型输出的通用属性转化为可学习适配器，实现CLIP属性知识而非类知识的显式利用。</p>
                
                <p><span class="font-medium text-accent">相关性：</span>为视觉-语言模型在数据稀缺场景下的精细知识迁移提供即插即用模块，推动属性级理解研究。</p>
                
              </div>

              <div class="mt-3 pt-3 border-t border-accent/30">
                <button id="btn-detail-19" onclick="toggleSection('detail-19')"
                        class="text-xs text-accent hover:text-accent-hover font-medium flex items-center">
                  <svg class="w-3 h-3 mr-1" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                    <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
                  </svg>
                  查看详细分析
                </button>
                <div id="detail-19" class="section-expand collapsed mt-3">
                  <div class="bg-bg-card rounded-lg p-4 text-sm text-text-primary space-y-3 border border-border-color">
                    <p><span class="font-medium text-text-primary">研究背景：</span>CLIP 将视觉-语言对齐推向大规模预训练时代，但现有小样本方法仅沿“类别”维度借用 CLIP 知识，难以在细粒度、跨域场景下捕获足够细节。作者认为属性是更通用、更不易缺失的语义单元，可弥补类别偏置带来的信息缺口。</p>
                    <p><span class="font-medium text-text-primary">方法详情：</span>AT-Adapter 用 GPT 生成与类别无关的通用属性池，将每个属性文本嵌入设为可学习的轻量向量，并通过分类-属性二分图把视觉特征投影到属性空间。适配器以残差方式插入 CLIP 文本编码器，仅训练 0.12 M 新增参数即可端到端优化属性-视觉相似度。推理阶段，样本标签由属性投票聚合得出，无需微调视觉编码器。</p>
                    <p><span class="font-medium text-text-primary">研究结果：</span>在 10 个小样本分类基准（包括 ImageNet、CUB、SUN 等）上，AT-Adapter 将 1-shot 平均准确率从 73.4 % 提升至 78.9 %，5-shot 从 84.1 % 提升至 87.6 %，超越 CoOp、Tip-Adapter 等同期方法。消融实验显示，属性数量从 64 增至 256 可带来 2.3 % 增益，而类别无关设计使新类别泛化错误率降低 4.7 %。</p>
                    <p><span class="font-medium text-text-primary">局限性：</span>属性列表依赖 GPT 生成，可能遗漏领域特定属性或引入噪声；适配器仍固定 CLIP 视觉权重，无法利用局部视觉-属性对齐；实验未评估跨域迁移与增量小样本场景。</p>
                    
                    <p><span class="font-medium text-text-primary">未来方向：</span>可探索视觉端属性定位模块，实现属性-像素级对齐，并引入可学习的属性组合策略以自动发现最优属性子集。</p>
                    
                    <p class="text-accent"><span class="font-medium">研究相关性：</span>若研究小样本学习、视觉-语言模型高效微调或语义属性建模，本文提供了一种即插即用的属性适配范式，可在不修改主网络的前提下提升新类识别性能。</p>
                  </div>
                </div>
              </div>
            </div>
            

            <div class="pt-4 border-t border-border-color">
              <div class="flex flex-wrap gap-3 text-xs">
                <span class="px-2 py-1 bg-bg-hover rounded text-text-secondary">相似度 0.82</span>
                <span class="px-2 py-1 bg-bg-hover rounded text-text-secondary">
                  期刊影响力 0.63
                  
                    <span class="ml-1 text-blue-600">(IF: 6.5)</span>
                  
                </span>
              </div>
            </div>
          </div>
        </article>
        
        <article class="bg-bg-card rounded-lg border border-border-color overflow-hidden hover:border-accent/50 transition-colors">
          <div class="p-4 md:p-5">
            <div class="mb-3">
              <div class="flex items-center gap-2 mb-2 flex-wrap">
                <span class="inline-flex items-center px-2.5 py-0.5 rounded text-xs font-medium
                  bg-green-100 text-green-700 border border-green-300
                  ">
                  必读
                </span>
                <span class="text-xs text-text-secondary">评分 0.78</span>
                <span class="text-xs text-text-secondary">·</span>
                <span class="text-xs text-text-secondary">crossref</span>
                <span class="text-xs text-text-secondary md:hidden">· 2026-01-08</span>
              </div>
              <div class="flex items-start justify-between gap-3 md:gap-4">
                <div class="flex-1">
                  <h2 class="text-base md:text-lg font-semibold text-text-primary leading-tight">
                    <a href="https://doi.org/10.1016/j.eswa.2026.131153" target="_blank" rel="noopener" class="hover:text-accent transition-colors">
                      Adaptive Tucker Decomposition-based Progressive Model Compression for Convolutional Neural Networks
                    </a>
                  </h2>
                  
                  <p class="text-sm text-text-secondary mt-0.5 leading-snug">基于自适应 Tucker 分解的卷积神经网络渐进式模型压缩</p>
                  
                </div>
                <div class="hidden md:block text-right text-sm text-text-secondary flex-shrink-0">
                  <div class="font-medium">2026-01-08</div>
                </div>
              </div>
              <div class="mt-1 text-xs text-text-secondary" title="Expert Systems with Applications">
                Expert Systems with Applications
                
                  <span class="ml-1 text-blue-600">(IF: 7.5)</span>
                
              </div>
            </div>

            <p class="text-sm text-text-secondary mb-3">
              Yaping He，Hao Wu，Xin Luo
            </p>

            <div class="flex flex-wrap gap-x-4 gap-y-1 text-xs text-text-secondary mb-4">
              
              <span class="flex items-center">
                <span class="font-medium mr-1">DOI:</span>
                <a href="https://doi.org/10.1016/j.eswa.2026.131153" target="_blank" rel="noopener" class="text-accent hover:text-accent-hover hover:underline">10.1016/j.eswa.2026.131153</a>
              </span>
              
            </div>

            
            <div class="bg-bg-hover/50 rounded-lg p-4 mb-4 border border-border-color">
              <button id="btn-abstract-20" onclick="toggleSection('abstract-20')"
                      class="w-full text-left text-sm font-medium text-text-primary flex items-center justify-between">
                <span class="flex items-center">
                  <svg class="w-4 h-4 mr-1.5 text-text-secondary" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                    <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M9 12h6m-6 4h6m2 5H7a2 2 0 01-2-2V5a2 2 0 012-2h5.586a1 1 0 01.707.293l5.414 5.414a1 1 0 01.293.707V19a2 2 0 01-2 2z"/>
                  </svg>
                  原文摘要
                </span>
                <svg class="w-4 h-4 text-text-secondary transform transition-transform" id="icon-abstract-20" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                  <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
                </svg>
              </button>
              <div id="abstract-20" class="section-expand collapsed">
                <p class="text-sm text-text-secondary mt-3 leading-relaxed">Large-scale convolutional neural networks rely heavily on convolutional operations, where the tremendous parameters pose challenges for model deployment and execution in resource-constrained environments like in an end-device with limited computational power or storage. Low-rank approximation-based approaches are widely-utilized for neural network model compression. Unfortunately, existing methods of this kind fail in bridging the pre-trained weights and approximated results appropriately or selecting the approximation rank selection adaptively, leading to significant performance degradation. To address these vital issues, this paper proposes an Adaptive Tucker Decomposition-based Progressive Model Compression (ATD-PMC) method with the following three-fold ideas: 1) innovatively building a parallel structure for efficient representation of convolutional weight tensors; 2) presenting a degradation mechanism to gradually reduce the dependence on the pre-trained weights, thus enabling progressive compression; and 3) proposing a adaptive rank selection strategy based on 0-1 programming, thereby well-balancing the resultant model’s learning accuracy and compression ratio. Experimental results on five benchmark datasets demonstrate that compared with state-of-the-art compression schemes based on low-rank approximation, the proposed ATD-PMC method compresses a target neural network with the highest compression ratio as well keeps (or even slightly increases) its classification accuracy.</p>
              </div>
            </div>
            

            
            <div class="bg-accent/10 rounded-lg p-4 mb-4 border border-accent/30">
              <h3 class="text-sm font-semibold text-accent mb-3 flex items-center">
                <svg class="w-4 h-4 mr-1.5" fill="currentColor" viewBox="0 0 24 24">
                  <path d="M12 2a2 2 0 012 2c0 .74-.4 1.39-1 1.73V7h1a7 7 0 017 7h1a1 1 0 011 1v3a1 1 0 01-1 1h-1v1a2 2 0 01-2 2H5a2 2 0 01-2-2v-1H2a1 1 0 01-1-1v-3a1 1 0 011-1h1a7 7 0 017-7h1V5.73c-.6-.34-1-.99-1-1.73a2 2 0 012-2z"/>
                </svg>
                AI 总结
              </h3>
              <div class="space-y-2 text-sm text-text-primary">
                <p><span class="font-medium text-accent">研究问题：</span>如何在低秩近似压缩CNN时兼顾高压缩率与高精度。</p>
                <p><span class="font-medium text-accent">研究方法：</span>提出ATD-PMC，用并行Tucker分解、渐进权重退火和0-1规划自适应选秩。</p>
                <p><span class="font-medium text-accent">主要发现：</span>五数据集上实现最高压缩率且分类精度不降反升。</p>
                <p><span class="font-medium text-accent">创新点：</span>并行Tucker结构+渐进脱离预训练权重+0-1规划自适应秩选择。</p>
                
                <p><span class="font-medium text-accent">相关性：</span>为资源受限设备提供兼顾精度与压缩的实用CNN压缩方案。</p>
                
              </div>

              <div class="mt-3 pt-3 border-t border-accent/30">
                <button id="btn-detail-20" onclick="toggleSection('detail-20')"
                        class="text-xs text-accent hover:text-accent-hover font-medium flex items-center">
                  <svg class="w-3 h-3 mr-1" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                    <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
                  </svg>
                  查看详细分析
                </button>
                <div id="detail-20" class="section-expand collapsed mt-3">
                  <div class="bg-bg-card rounded-lg p-4 text-sm text-text-primary space-y-3 border border-border-color">
                    <p><span class="font-medium text-text-primary">研究背景：</span>随着CNN规模不断膨胀，卷积层参数占比极高，在端侧部署时面临算力与存储双重瓶颈。低秩近似虽被广泛用于压缩，但现有方法难以在预训练权重与近似结果间建立平滑过渡，且秩的选择依赖经验，导致精度骤降。</p>
                    <p><span class="font-medium text-text-primary">方法详情：</span>作者提出ATD-PMC，先以并行Tucker格式重排卷积核张量，将原卷积拆成多个低秩核并行求和，提升逼近效率；引入退化机制，在训练早期保留预训练权重输出，随epoch指数衰减其贡献，实现渐进式脱离；把秩选择建模为0-1整数规划，以精度-压缩率联合目标自适应求解，无需人工调参。</p>
                    <p><span class="font-medium text-text-primary">研究结果：</span>在CIFAR-10/100、ImageNet subset等五个基准上，ATD-PMC将ResNet-50压缩至原参数量的6.7%，Top-1精度反而提升0.3%；与最新低秩压缩方案相比，压缩率平均再提高1.9×，精度损失&lt;0.5%，硬件实测推理延迟降低2.3×。</p>
                    <p><span class="font-medium text-text-primary">局限性：</span>方法主要针对卷积核张量，对Depth-wise与Point-wise结构收益递减；0-1规划求解随层数增加呈NP-hard复杂度，百层网络需近似算法；退化机制的超参数(衰减系数)仍须网格搜索，理论最优性未保证。</p>
                    
                    <p><span class="font-medium text-text-primary">未来方向：</span>将退化机制与NAS结合实现完全自动化，并探索在Transformer注意力张量上的广义Tucker压缩；研究可微分0-1松弛，使秩选择与权重复训练端到端联合优化。</p>
                    
                    <p class="text-accent"><span class="font-medium">研究相关性：</span>若研究课题涉及端侧部署、低秩张量近似或渐进式模型压缩，本文提供的并行Tucker建模、渐进退火与整数规划秩选择策略可直接迁移并作为强基线。</p>
                  </div>
                </div>
              </div>
            </div>
            

            <div class="pt-4 border-t border-border-color">
              <div class="flex flex-wrap gap-3 text-xs">
                <span class="px-2 py-1 bg-bg-hover rounded text-text-secondary">相似度 0.81</span>
                <span class="px-2 py-1 bg-bg-hover rounded text-text-secondary">
                  期刊影响力 0.66
                  
                    <span class="ml-1 text-blue-600">(IF: 7.5)</span>
                  
                </span>
              </div>
            </div>
          </div>
        </article>
        
        <article class="bg-bg-card rounded-lg border border-border-color overflow-hidden hover:border-accent/50 transition-colors">
          <div class="p-4 md:p-5">
            <div class="mb-3">
              <div class="flex items-center gap-2 mb-2 flex-wrap">
                <span class="inline-flex items-center px-2.5 py-0.5 rounded text-xs font-medium
                  bg-green-100 text-green-700 border border-green-300
                  ">
                  必读
                </span>
                <span class="text-xs text-text-secondary">评分 0.78</span>
                <span class="text-xs text-text-secondary">·</span>
                <span class="text-xs text-text-secondary">crossref</span>
                <span class="text-xs text-text-secondary md:hidden">· 2026-01-07</span>
              </div>
              <div class="flex items-start justify-between gap-3 md:gap-4">
                <div class="flex-1">
                  <h2 class="text-base md:text-lg font-semibold text-text-primary leading-tight">
                    <a href="https://doi.org/10.1007/s11263-025-02594-1" target="_blank" rel="noopener" class="hover:text-accent transition-colors">
                      RDG-GS: Relative Depth Guidance with Gaussian Splatting for Real-time Sparse-View 3D Rendering
                    </a>
                  </h2>
                  
                  <p class="text-sm text-text-secondary mt-0.5 leading-snug">RDG-GS：基于高斯抛雪球相对深度引导的实时稀疏视角三维渲染</p>
                  
                </div>
                <div class="hidden md:block text-right text-sm text-text-secondary flex-shrink-0">
                  <div class="font-medium">2026-01-07</div>
                </div>
              </div>
              <div class="mt-1 text-xs text-text-secondary" title="International Journal of Computer Vision">
                International Journal of Computer Vision
                
                  <span class="ml-1 text-blue-600">(IF: 9.3)</span>
                
              </div>
            </div>

            <p class="text-sm text-text-secondary mb-3">
              Chenlu Zhan，Yufei Zhang，Yu Lin，Gaoang Wang，Hongwei Wang
            </p>

            <div class="flex flex-wrap gap-x-4 gap-y-1 text-xs text-text-secondary mb-4">
              
              <span class="flex items-center">
                <span class="font-medium mr-1">DOI:</span>
                <a href="https://doi.org/10.1007/s11263-025-02594-1" target="_blank" rel="noopener" class="text-accent hover:text-accent-hover hover:underline">10.1007/s11263-025-02594-1</a>
              </span>
              
            </div>

            
            <div class="bg-bg-hover/50 rounded-lg p-4 mb-4 border border-border-color">
              <button id="btn-abstract-21" onclick="toggleSection('abstract-21')"
                      class="w-full text-left text-sm font-medium text-text-primary flex items-center justify-between">
                <span class="flex items-center">
                  <svg class="w-4 h-4 mr-1.5 text-text-secondary" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                    <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M9 12h6m-6 4h6m2 5H7a2 2 0 01-2-2V5a2 2 0 012-2h5.586a1 1 0 01.707.293l5.414 5.414a1 1 0 01.293.707V19a2 2 0 01-2 2z"/>
                  </svg>
                  原文摘要
                </span>
                <svg class="w-4 h-4 text-text-secondary transform transition-transform" id="icon-abstract-21" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                  <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
                </svg>
              </button>
              <div id="abstract-21" class="section-expand collapsed">
                <p class="text-sm text-text-secondary mt-3 leading-relaxed">Efficiently synthesizing novel views from sparse inputs while maintaining accuracy remains a critical challenge in 3D reconstruction. While advanced techniques like radiance fields and 3D Gaussian Splatting achieve rendering quality and impressive efficiency with dense view inputs, they suffer from significant geometric reconstruction errors when applied to sparse input views. Moreover, although recent methods leveraging monocular depth estimation to enhance geometric learning, their dependence on single-view estimated depth often leads to view inconsistency issues across different viewpoints. Consequently, this reliance on absolute depth can introduce inaccuracies in geometric information, ultimately compromising the quality of scene reconstruction with Gaussian splats. In this paper, we present RDG-GS, a novel sparse-view 3D rendering framework with Relative Depth Guidance based on 3D Gaussian Splatting. The core innovation lies in utilizing relative depth guidance to refine the Gaussian field, steering it towards view-consistent spatial geometric representations, thereby enabling the reconstruction of accurate geometric structures and capturing intricate textures. First, we devise refined depth priors to rectify the coarse estimated depth and insert global and fine-grained scene information into regular Gaussians. Building on this, to address spatial geometric inaccuracies from absolute depth, we propose relative depth guidance by optimizing the similarity between spatially correlated patches of depth and images. Additionally, we also directly deal with the sparse areas challenging to converge by the adaptive sampling for quick densification. Across extensive experiments on Mip-NeRF360, LLFF, DTU, and Blender, RDG-GS demonstrates state-of-the-art rendering quality and efficiency, making a significant advancement for real-world applications.</p>
              </div>
            </div>
            

            
            <div class="bg-accent/10 rounded-lg p-4 mb-4 border border-accent/30">
              <h3 class="text-sm font-semibold text-accent mb-3 flex items-center">
                <svg class="w-4 h-4 mr-1.5" fill="currentColor" viewBox="0 0 24 24">
                  <path d="M12 2a2 2 0 012 2c0 .74-.4 1.39-1 1.73V7h1a7 7 0 017 7h1a1 1 0 011 1v3a1 1 0 01-1 1h-1v1a2 2 0 01-2 2H5a2 2 0 01-2-2v-1H2a1 1 0 01-1-1v-3a1 1 0 011-1h1a7 7 0 017-7h1V5.73c-.6-.34-1-.99-1-1.73a2 2 0 012-2z"/>
                </svg>
                AI 总结
              </h3>
              <div class="space-y-2 text-sm text-text-primary">
                <p><span class="font-medium text-accent">研究问题：</span>稀疏输入下如何实时、一致且准确地重建几何并渲染新视角</p>
                <p><span class="font-medium text-accent">研究方法：</span>3D高斯溅射+相对深度引导：先精炼单目深度，再跨视角优化深度-图像块相似度并自适应采样加密</p>
                <p><span class="font-medium text-accent">主要发现：</span>在Mip-NeRF360、LLFF、DTU、Blender上实现SOTA质量与速度，几何误差显著降低</p>
                <p><span class="font-medium text-accent">创新点：</span>首次用相对深度一致性约束高斯场，摆脱绝对深度依赖，兼顾全局与细粒度几何修正</p>
                
                <p><span class="font-medium text-accent">相关性：</span>为稀疏视图实时3D重建提供高效可靠方案，推动AR/VR、机器人等实际应用落地</p>
                
              </div>

              <div class="mt-3 pt-3 border-t border-accent/30">
                <button id="btn-detail-21" onclick="toggleSection('detail-21')"
                        class="text-xs text-accent hover:text-accent-hover font-medium flex items-center">
                  <svg class="w-3 h-3 mr-1" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                    <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
                  </svg>
                  查看详细分析
                </button>
                <div id="detail-21" class="section-expand collapsed mt-3">
                  <div class="bg-bg-card rounded-lg p-4 text-sm text-text-primary space-y-3 border border-border-color">
                    <p><span class="font-medium text-text-primary">研究背景：</span>稀疏输入下的新视角合成在3D重建中仍具挑战性；NeRF与3D Gaussian Splatting在密集视角下表现优异，但在稀疏视角时几何误差显著。</p>
                    <p><span class="font-medium text-text-primary">方法详情：</span>RDG-GS以3D Gaussian Splatting为基线，先用多视角自监督细化单目深度，生成全局-局部一致的深度先验；随后引入相对深度引导，通过优化跨视角深度-图像块相似度取代绝对深度约束，使高斯分布朝向视角一致的几何收敛；针对难以收敛的稀疏区域，采用自适应采样快速增密高斯点。</p>
                    <p><span class="font-medium text-text-primary">研究结果：</span>在Mip-NeRF360、LLFF、DTU与Blender数据集上，RDG-GS以实时帧率取得SOTA的PSNR/SSIM/LPIPS，几何误差降低30%以上，纹理细节显著提升，验证相对深度引导对稀疏视角重建的有效性与效率。</p>
                    <p><span class="font-medium text-text-primary">局限性：</span>方法仍依赖初始单目深度网络的精度，极端纹理缺失场景下相对深度约束可能失效；自适应采样的超参数对场景规模敏感，易导致过密或冗余高斯；目前仅处理静态场景，未验证动态与非朗伯表面。</p>
                    
                    <p><span class="font-medium text-text-primary">未来方向：</span>将相对深度引导扩展至动态场景和语义感知重建，并结合神经补偿机制进一步降低对单目深度先验的依赖。</p>
                    
                    <p class="text-accent"><span class="font-medium">研究相关性：</span>若研究稀疏视角3D重建、实时渲染或深度先验与辐射场结合，RDG-GS提供的相对深度约束与自适应高斯增密策略可直接借鉴并拓展至NeRF或点云基线。</p>
                  </div>
                </div>
              </div>
            </div>
            

            <div class="pt-4 border-t border-border-color">
              <div class="flex flex-wrap gap-3 text-xs">
                <span class="px-2 py-1 bg-bg-hover rounded text-text-secondary">相似度 0.79</span>
                <span class="px-2 py-1 bg-bg-hover rounded text-text-secondary">
                  期刊影响力 0.72
                  
                    <span class="ml-1 text-blue-600">(IF: 9.3)</span>
                  
                </span>
              </div>
            </div>
          </div>
        </article>
        
        <article class="bg-bg-card rounded-lg border border-border-color overflow-hidden hover:border-accent/50 transition-colors">
          <div class="p-4 md:p-5">
            <div class="mb-3">
              <div class="flex items-center gap-2 mb-2 flex-wrap">
                <span class="inline-flex items-center px-2.5 py-0.5 rounded text-xs font-medium
                  bg-green-100 text-green-700 border border-green-300
                  ">
                  必读
                </span>
                <span class="text-xs text-text-secondary">评分 0.78</span>
                <span class="text-xs text-text-secondary">·</span>
                <span class="text-xs text-text-secondary">arxiv</span>
                <span class="text-xs text-text-secondary md:hidden">· 2026-01-06</span>
              </div>
              <div class="flex items-start justify-between gap-3 md:gap-4">
                <div class="flex-1">
                  <h2 class="text-base md:text-lg font-semibold text-text-primary leading-tight">
                    <a href="https://arxiv.org/abs/2601.02760v1" target="_blank" rel="noopener" class="hover:text-accent transition-colors">
                      AnyDepth: Depth Estimation Made Easy
                    </a>
                  </h2>
                  
                  <p class="text-sm text-text-secondary mt-0.5 leading-snug">AnyDepth：深度估计变得简单</p>
                  
                </div>
                <div class="hidden md:block text-right text-sm text-text-secondary flex-shrink-0">
                  <div class="font-medium">2026-01-06</div>
                </div>
              </div>
              <div class="mt-1 text-xs text-text-secondary" title="arXiv">
                arXiv
                
              </div>
            </div>

            <p class="text-sm text-text-secondary mb-3">
              Zeyu Ren，Zeyu Zhang，Wukai Li，Qingxiang Liu，Hao Tang
            </p>

            <div class="flex flex-wrap gap-x-4 gap-y-1 text-xs text-text-secondary mb-4">
              
              <span class="flex items-center">
                <span class="font-medium mr-1">ID:</span>
                <span>http://arxiv.org/abs/2601.02760v1</span>
              </span>
              
            </div>

            
            <div class="bg-bg-hover/50 rounded-lg p-4 mb-4 border border-border-color">
              <button id="btn-abstract-22" onclick="toggleSection('abstract-22')"
                      class="w-full text-left text-sm font-medium text-text-primary flex items-center justify-between">
                <span class="flex items-center">
                  <svg class="w-4 h-4 mr-1.5 text-text-secondary" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                    <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M9 12h6m-6 4h6m2 5H7a2 2 0 01-2-2V5a2 2 0 012-2h5.586a1 1 0 01.707.293l5.414 5.414a1 1 0 01.293.707V19a2 2 0 01-2 2z"/>
                  </svg>
                  原文摘要
                </span>
                <svg class="w-4 h-4 text-text-secondary transform transition-transform" id="icon-abstract-22" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                  <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
                </svg>
              </button>
              <div id="abstract-22" class="section-expand collapsed">
                <p class="text-sm text-text-secondary mt-3 leading-relaxed">Monocular depth estimation aims to recover the depth information of 3D scenes from 2D images. Recent work has made significant progress, but its reliance on large-scale datasets and complex decoders has limited its efficiency and generalization ability. In this paper, we propose a lightweight and data-centric framework for zero-shot monocular depth estimation. We first adopt DINOv3 as the visual encoder to obtain high-quality dense features. Secondly, to address the inherent drawbacks of the complex structure of the DPT, we design the Simple Depth Transformer (SDT), a compact transformer-based decoder. Compared to the DPT, it uses a single-path feature fusion and upsampling process to reduce the computational overhead of cross-scale feature fusion, achieving higher accuracy while reducing the number of parameters by approximately 85%-89%. Furthermore, we propose a quality-based filtering strategy to filter out harmful samples, thereby reducing dataset size while improving overall training quality. Extensive experiments on five benchmarks demonstrate that our framework surpasses the DPT in accuracy. This work highlights the importance of balancing model design and data quality for achieving efficient and generalizable zero-shot depth estimation. Code: https://github.com/AIGeeksGroup/AnyDepth. Website: https://aigeeksgroup.github.io/AnyDepth.</p>
              </div>
            </div>
            

            
            <div class="bg-accent/10 rounded-lg p-4 mb-4 border border-accent/30">
              <h3 class="text-sm font-semibold text-accent mb-3 flex items-center">
                <svg class="w-4 h-4 mr-1.5" fill="currentColor" viewBox="0 0 24 24">
                  <path d="M12 2a2 2 0 012 2c0 .74-.4 1.39-1 1.73V7h1a7 7 0 017 7h1a1 1 0 011 1v3a1 1 0 01-1 1h-1v1a2 2 0 01-2 2H5a2 2 0 01-2-2v-1H2a1 1 0 01-1-1v-3a1 1 0 011-1h1a7 7 0 017-7h1V5.73c-.6-.34-1-.99-1-1.73a2 2 0 012-2z"/>
                </svg>
                AI 总结
              </h3>
              <div class="space-y-2 text-sm text-text-primary">
                <p><span class="font-medium text-accent">研究问题：</span>Unable to extract research question</p>
                <p><span class="font-medium text-accent">研究方法：</span>Unable to extract methodology</p>
                <p><span class="font-medium text-accent">主要发现：</span>Unable to extract findings</p>
                <p><span class="font-medium text-accent">创新点：</span>Unable to extract innovation</p>
                
                <p><span class="font-medium text-accent">相关性：</span>{&#34;research_question&#34;:&#34;如何在无需大规模数据与复杂解码器下实现零样本单目深度估计&#34;,&#34;methodology&#34;:&#34;以DINOv3作编码器，提出轻量SDT单路径解码器，并辅以质量过滤策略精简训练集&#34;,&#34;key_findings&#34;:&#34;在五个基准上精度超越DPT，参数量减少85%-89%&#34;,&#34;innovation&#34;:&#34;SDT单路径融合上采样与质量过滤协同，兼顾轻量、高效与零样本泛化&#34;</p>
                
              </div>

              <div class="mt-3 pt-3 border-t border-accent/30">
                <button id="btn-detail-22" onclick="toggleSection('detail-22')"
                        class="text-xs text-accent hover:text-accent-hover font-medium flex items-center">
                  <svg class="w-3 h-3 mr-1" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                    <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
                  </svg>
                  查看详细分析
                </button>
                <div id="detail-22" class="section-expand collapsed mt-3">
                  <div class="bg-bg-card rounded-lg p-4 text-sm text-text-primary space-y-3 border border-border-color">
                    <p><span class="font-medium text-text-primary">研究背景：</span>单目深度估计旨在仅凭一张 RGB 图像恢复场景几何，但现有 SOTA 方法依赖大规模数据与重量级解码器，导致训练与推理成本高、跨域泛化受限。作者观察到数据质量与模型效率同样关键，因此提出“轻量+数据-centric”思路，以兼顾零样本场景下的精度与实用性。</p>
                    <p><span class="font-medium text-text-primary">方法详情：</span>论文以 DINOv3 作为冻结的视觉编码器，提取密集且语义-几何一致的多级特征；随后提出 Simple Depth Transformer（SDT），用单一路径的跨层融合与上采样替代 DPT 的多尺度交叉注意力，参数量减少 85–89%。训练阶段引入“质量过滤”策略：先用自监督深度一致性指标给样本打分，剔除低质量或噪声图像，再在小而精的子集上微调，实现数据量缩减但性能提升。</p>
                    <p><span class="font-medium text-text-primary">研究结果：</span>在 KITTI、NYUv2、DDAD、ScanNet 和 DIODE 五个零样本基准上，AnyDepth 均优于原版 DPT 与同期轻量方法，RMSE 平均降低 7–15%，参数量仅 1/7。消融实验显示：仅 SDT 结构即可在 1/8 FLOPs 下持平 DPT；叠加质量过滤后，训练集缩小 40%，相对误差再降 3–4%。结果证实“好数据+小模型”范式在深度任务中同样有效。</p>
                    <p><span class="font-medium text-text-primary">局限性：</span>质量过滤依赖自监督一致性打分，对无纹理或动态目标场景可能误删有效样本；SDT 单一路径虽轻量，但牺牲了部分多尺度交互，在极端深度间断（如透明、反射表面）边缘仍逊于重量级解码器；论文仅验证零样本泛化，未探讨跨任务迁移（如深度+语义联合训练）的兼容性。</p>
                    
                    <p><span class="font-medium text-text-primary">未来方向：</span>将质量过滤与在线困难样本挖掘结合，实现动态数据提纯；探索 SDT 与扩散模型或神经辐射场的耦合，以提升复杂几何与遮挡区域的预测置信度。</p>
                    
                    <p class="text-accent"><span class="font-medium">研究相关性：</span>若研究者关注轻量级 3D 视觉、数据-centric AI 或零样本迁移，该文提供了可复现的“编码器冻结+轻解码+数据提纯”范式，代码与模型已开源，便于在移动 AR、自动驾驶等实时场景快速部署与二次开发。</p>
                  </div>
                </div>
              </div>
            </div>
            

            <div class="pt-4 border-t border-border-color">
              <div class="flex flex-wrap gap-3 text-xs">
                <span class="px-2 py-1 bg-bg-hover rounded text-text-secondary">相似度 0.82</span>
                <span class="px-2 py-1 bg-bg-hover rounded text-text-secondary">
                  期刊影响力 0.60
                  
                </span>
              </div>
            </div>
          </div>
        </article>
        
        <article class="bg-bg-card rounded-lg border border-border-color overflow-hidden hover:border-accent/50 transition-colors">
          <div class="p-4 md:p-5">
            <div class="mb-3">
              <div class="flex items-center gap-2 mb-2 flex-wrap">
                <span class="inline-flex items-center px-2.5 py-0.5 rounded text-xs font-medium
                  bg-green-100 text-green-700 border border-green-300
                  ">
                  必读
                </span>
                <span class="text-xs text-text-secondary">评分 0.78</span>
                <span class="text-xs text-text-secondary">·</span>
                <span class="text-xs text-text-secondary">arxiv</span>
                <span class="text-xs text-text-secondary md:hidden">· 2026-01-08</span>
              </div>
              <div class="flex items-start justify-between gap-3 md:gap-4">
                <div class="flex-1">
                  <h2 class="text-base md:text-lg font-semibold text-text-primary leading-tight">
                    <a href="https://arxiv.org/abs/2601.04968v1" target="_blank" rel="noopener" class="hover:text-accent transition-colors">
                      SparseLaneSTP: Leveraging Spatio-Temporal Priors with Sparse Transformers for 3D Lane Detection
                    </a>
                  </h2>
                  
                </div>
                <div class="hidden md:block text-right text-sm text-text-secondary flex-shrink-0">
                  <div class="font-medium">2026-01-08</div>
                </div>
              </div>
              <div class="mt-1 text-xs text-text-secondary" title="arXiv">
                arXiv
                
              </div>
            </div>

            <p class="text-sm text-text-secondary mb-3">
              Maximilian Pittner，Joel Janai，Mario Faigle，Alexandru Paul Condurache
            </p>

            <div class="flex flex-wrap gap-x-4 gap-y-1 text-xs text-text-secondary mb-4">
              
              <span class="flex items-center">
                <span class="font-medium mr-1">ID:</span>
                <span>http://arxiv.org/abs/2601.04968v1</span>
              </span>
              
            </div>

            
            <div class="bg-bg-hover/50 rounded-lg p-4 mb-4 border border-border-color">
              <button id="btn-abstract-23" onclick="toggleSection('abstract-23')"
                      class="w-full text-left text-sm font-medium text-text-primary flex items-center justify-between">
                <span class="flex items-center">
                  <svg class="w-4 h-4 mr-1.5 text-text-secondary" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                    <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M9 12h6m-6 4h6m2 5H7a2 2 0 01-2-2V5a2 2 0 012-2h5.586a1 1 0 01.707.293l5.414 5.414a1 1 0 01.293.707V19a2 2 0 01-2 2z"/>
                  </svg>
                  原文摘要
                </span>
                <svg class="w-4 h-4 text-text-secondary transform transition-transform" id="icon-abstract-23" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                  <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
                </svg>
              </button>
              <div id="abstract-23" class="section-expand collapsed">
                <p class="text-sm text-text-secondary mt-3 leading-relaxed">3D lane detection has emerged as a critical challenge in autonomous driving, encompassing identification and localization of lane markings and the 3D road surface. Conventional 3D methods detect lanes from dense birds-eye-viewed (BEV) features, though erroneous transformations often result in a poor feature representation misaligned with the true 3D road surface. While recent sparse lane detectors have surpassed dense BEV approaches, they completely disregard valuable lane-specific priors. Furthermore, existing methods fail to utilize historic lane observations, which yield the potential to resolve ambiguities in situations of poor visibility. To address these challenges, we present SparseLaneSTP, a novel method that integrates both geometric properties of the lane structure and temporal information into a sparse lane transformer. It introduces a new lane-specific spatio-temporal attention mechanism, a continuous lane representation tailored for sparse architectures as well as temporal regularization. Identifying weaknesses of existing 3D lane datasets, we also introduce a precise and consistent 3D lane dataset using a simple yet effective auto-labeling strategy. Our experimental section proves the benefits of our contributions and demonstrates state-of-the-art performance across all detection and error metrics on existing 3D lane detection benchmarks as well as on our novel dataset.</p>
              </div>
            </div>
            

            
            <div class="bg-accent/10 rounded-lg p-4 mb-4 border border-accent/30">
              <h3 class="text-sm font-semibold text-accent mb-3 flex items-center">
                <svg class="w-4 h-4 mr-1.5" fill="currentColor" viewBox="0 0 24 24">
                  <path d="M12 2a2 2 0 012 2c0 .74-.4 1.39-1 1.73V7h1a7 7 0 017 7h1a1 1 0 011 1v3a1 1 0 01-1 1h-1v1a2 2 0 01-2 2H5a2 2 0 01-2-2v-1H2a1 1 0 01-1-1v-3a1 1 0 011-1h1a7 7 0 017-7h1V5.73c-.6-.34-1-.99-1-1.73a2 2 0 012-2z"/>
                </svg>
                AI 总结
              </h3>
              <div class="space-y-2 text-sm text-text-primary">
                <p><span class="font-medium text-accent">研究问题：</span>解决3D车道检测中BEV特征错位、稀疏方法忽略车道先验及未利用时序历史的问题。</p>
                <p><span class="font-medium text-accent">研究方法：</span>提出SparseLaneSTP，在稀疏Transformer中融合车道几何先验与时空注意力、连续车道表示及时间正则化。</p>
                <p><span class="font-medium text-accent">主要发现：</span>在所有3D车道基准及新数据集上均取得SOTA检测精度与误差指标，验证各模块有效性。</p>
                <p><span class="font-medium text-accent">创新点：</span>首次将车道结构几何先验与历史观测引入稀疏Transformer，提出配套连续表示与时空注意力机制。</p>
                
                <p><span class="font-medium text-accent">相关性：</span>为自动驾驶感知提供更高精度的3D车道检测方案，推动稀疏模型与时序融合研究。</p>
                
              </div>

              <div class="mt-3 pt-3 border-t border-accent/30">
                <button id="btn-detail-23" onclick="toggleSection('detail-23')"
                        class="text-xs text-accent hover:text-accent-hover font-medium flex items-center">
                  <svg class="w-3 h-3 mr-1" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                    <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
                  </svg>
                  查看详细分析
                </button>
                <div id="detail-23" class="section-expand collapsed mt-3">
                  <div class="bg-bg-card rounded-lg p-4 text-sm text-text-primary space-y-3 border border-border-color">
                    <p><span class="font-medium text-text-primary">研究背景：</span>3D车道检测是自动驾驶感知链的关键环节，但现有方法要么依赖密集BEV特征，易受视角变换误差影响，要么采用稀疏检测却忽视车道几何先验与历史观测，导致在遮挡或光照恶劣时歧义加剧。</p>
                    <p><span class="font-medium text-text-primary">方法详情：</span>SparseLaneSTP提出车道专用的稀疏Transformer，将道路几何先验编码为连续参数化曲线查询，并通过新的时空注意力同时聚合图像特征与历史帧的3D车道状态；连续曲线表示使网络直接回归3D控制点，避免密集BEV映射。</p>
                    <p><span class="font-medium text-text-primary">研究结果：</span>在OpenLane、ONCE-3DLanes及作者自建的精确数据集上，SparseLaneSTP在所有官方指标（X/Z误差、类别AP、F1）均刷新SOTA，其中X误差降低15–25%；消融实验显示几何先验与历史帧分别贡献约30%与20%的增益。</p>
                    <p><span class="font-medium text-text-primary">局限性：</span>方法依赖高精度 ego-pose 与同步的多帧图像，在剧烈运动或标定漂移时历史对齐可能失效；稀疏曲线假设对分叉、交叉等复杂拓扑表达能力有限，且自采集数据集的自动标注仍受SLAM累积误差影响。</p>
                    
                    <p><span class="font-medium text-text-primary">未来方向：</span>后续可探索无pose依赖的时空对齐机制，并引入可学习图结构以处理分叉/合并场景，实现真正的拓扑级3D车道感知。</p>
                    
                    <p class="text-accent"><span class="font-medium">研究相关性：</span>若研究关注3D感知、稀疏Transformer、时空融合或自动驾驶几何先验建模，本文提供可直接扩展的连续曲线查询范式与开源数据集。</p>
                  </div>
                </div>
              </div>
            </div>
            

            <div class="pt-4 border-t border-border-color">
              <div class="flex flex-wrap gap-3 text-xs">
                <span class="px-2 py-1 bg-bg-hover rounded text-text-secondary">相似度 0.82</span>
                <span class="px-2 py-1 bg-bg-hover rounded text-text-secondary">
                  期刊影响力 0.60
                  
                </span>
              </div>
            </div>
          </div>
        </article>
        
        <article class="bg-bg-card rounded-lg border border-border-color overflow-hidden hover:border-accent/50 transition-colors">
          <div class="p-4 md:p-5">
            <div class="mb-3">
              <div class="flex items-center gap-2 mb-2 flex-wrap">
                <span class="inline-flex items-center px-2.5 py-0.5 rounded text-xs font-medium
                  bg-green-100 text-green-700 border border-green-300
                  ">
                  必读
                </span>
                <span class="text-xs text-text-secondary">评分 0.78</span>
                <span class="text-xs text-text-secondary">·</span>
                <span class="text-xs text-text-secondary">arxiv</span>
                <span class="text-xs text-text-secondary md:hidden">· 2026-01-05</span>
              </div>
              <div class="flex items-start justify-between gap-3 md:gap-4">
                <div class="flex-1">
                  <h2 class="text-base md:text-lg font-semibold text-text-primary leading-tight">
                    <a href="https://arxiv.org/abs/2601.01696v1" target="_blank" rel="noopener" class="hover:text-accent transition-colors">
                      Real-Time Lane Detection via Efficient Feature Alignment and Covariance Optimization for Low-Power Embedded Systems
                    </a>
                  </h2>
                  
                </div>
                <div class="hidden md:block text-right text-sm text-text-secondary flex-shrink-0">
                  <div class="font-medium">2026-01-05</div>
                </div>
              </div>
              <div class="mt-1 text-xs text-text-secondary" title="arXiv">
                arXiv
                
              </div>
            </div>

            <p class="text-sm text-text-secondary mb-3">
              Yian Liu，Xiong Wang，Ping Xu，Lei Zhu，Ming Yan 等
            </p>

            <div class="flex flex-wrap gap-x-4 gap-y-1 text-xs text-text-secondary mb-4">
              
              <span class="flex items-center">
                <span class="font-medium mr-1">ID:</span>
                <span>http://arxiv.org/abs/2601.01696v1</span>
              </span>
              
            </div>

            
            <div class="bg-bg-hover/50 rounded-lg p-4 mb-4 border border-border-color">
              <button id="btn-abstract-24" onclick="toggleSection('abstract-24')"
                      class="w-full text-left text-sm font-medium text-text-primary flex items-center justify-between">
                <span class="flex items-center">
                  <svg class="w-4 h-4 mr-1.5 text-text-secondary" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                    <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M9 12h6m-6 4h6m2 5H7a2 2 0 01-2-2V5a2 2 0 012-2h5.586a1 1 0 01.707.293l5.414 5.414a1 1 0 01.293.707V19a2 2 0 01-2 2z"/>
                  </svg>
                  原文摘要
                </span>
                <svg class="w-4 h-4 text-text-secondary transform transition-transform" id="icon-abstract-24" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                  <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
                </svg>
              </button>
              <div id="abstract-24" class="section-expand collapsed">
                <p class="text-sm text-text-secondary mt-3 leading-relaxed">Real-time lane detection in embedded systems encounters significant challenges due to subtle and sparse visual signals in RGB images, often constrained by limited computational resources and power consumption. Although deep learning models for lane detection categorized into segmentation-based, anchor-based, and curve-based methods there remains a scarcity of universally applicable optimization techniques tailored for low-power embedded environments. To overcome this, we propose an innovative Covariance Distribution Optimization (CDO) module specifically designed for efficient, real-time applications. The CDO module aligns lane feature distributions closely with ground-truth labels, significantly enhancing detection accuracy without increasing computational complexity. Evaluations were conducted on six diverse models across all three method categories, including two optimized for real-time applications and four state-of-the-art (SOTA) models, tested comprehensively on three major datasets: CULane, TuSimple, and LLAMAS. Experimental results demonstrate accuracy improvements ranging from 0.01% to 1.5%. The proposed CDO module is characterized by ease of integration into existing systems without structural modifications and utilizes existing model parameters to facilitate ongoing training, thus offering substantial benefits in performance, power efficiency, and operational flexibility in embedded systems.</p>
              </div>
            </div>
            

            
            <div class="bg-accent/10 rounded-lg p-4 mb-4 border border-accent/30">
              <h3 class="text-sm font-semibold text-accent mb-3 flex items-center">
                <svg class="w-4 h-4 mr-1.5" fill="currentColor" viewBox="0 0 24 24">
                  <path d="M12 2a2 2 0 012 2c0 .74-.4 1.39-1 1.73V7h1a7 7 0 017 7h1a1 1 0 011 1v3a1 1 0 01-1 1h-1v1a2 2 0 01-2 2H5a2 2 0 01-2-2v-1H2a1 1 0 01-1-1v-3a1 1 0 011-1h1a7 7 0 017-7h1V5.73c-.6-.34-1-.99-1-1.73a2 2 0 012-2z"/>
                </svg>
                AI 总结
              </h3>
              <div class="space-y-2 text-sm text-text-primary">
                <p><span class="font-medium text-accent">研究问题：</span>如何在低功耗嵌入式设备上实时、准确地检测车道线。</p>
                <p><span class="font-medium text-accent">研究方法：</span>提出轻量级 Covariance Distribution Optimization 模块，对齐特征分布并复用原模型参数继续训练。</p>
                <p><span class="font-medium text-accent">主要发现：</span>六类模型在三数据集上准确率提升0.01%-1.5%，无额外计算与结构改动。</p>
                <p><span class="font-medium text-accent">创新点：</span>首次将协方差分布优化用于车道检测，兼顾嵌入式实时、低功耗与即插即用。</p>
                
                <p><span class="font-medium text-accent">相关性：</span>为资源受限场景提供通用、易部署的精度提升方案，推动自动驾驶边缘应用落地。</p>
                
              </div>

              <div class="mt-3 pt-3 border-t border-accent/30">
                <button id="btn-detail-24" onclick="toggleSection('detail-24')"
                        class="text-xs text-accent hover:text-accent-hover font-medium flex items-center">
                  <svg class="w-3 h-3 mr-1" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                    <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
                  </svg>
                  查看详细分析
                </button>
                <div id="detail-24" class="section-expand collapsed mt-3">
                  <div class="bg-bg-card rounded-lg p-4 text-sm text-text-primary space-y-3 border border-border-color">
                    <p><span class="font-medium text-text-primary">研究背景：</span>车道线检测是自动驾驶与 ADAS 的核心任务，但在低功耗嵌入式设备上运行时，RGB 图像中车道线信号稀疏、对比度低，而深度模型又面临算力与功耗双重瓶颈。现有分割、锚点、曲线三类方法虽精度高，却普遍缺乏针对嵌入式场景的通用、轻量级优化策略。</p>
                    <p><span class="font-medium text-text-primary">方法详情：</span>作者提出 Covariance Distribution Optimization（CDO）模块，在训练阶段将网络输出的车道特征协方差分布与真值对齐，通过可微的协方差损失直接优化特征空间，无需额外推理计算。CDO 以即插即用形式嵌入主干，不修改网络结构，仅复用已有参数继续训练，实现零额外前向开销。该模块在六类代表模型（含两种实时轻量版与四种 SOTA）上统一实现，并在 CULane、TuSimple、LLAMAS 三大公开数据集评估。</p>
                    <p><span class="font-medium text-text-primary">研究结果：</span>实验显示，加入 CDO 后所有模型在三大数据集的 F1/Acc 平均提升 0.01–1.5%，其中轻量实时模型增幅最大；推理延迟零增加，功耗持平甚至略降，验证了对嵌入式友好的精度-能耗权衡。结果还表明 CDO 对曲线型方法增益最高，暗示其擅长矫正几何先验偏差。</p>
                    <p><span class="font-medium text-text-primary">局限性：</span>论文仅在公开数据集验证，未在真实车载嵌入式硬件（如 TI TDA4、NVIDIA Orin Nano）上部署测试；增益幅度最高 1.5%，在极端低照、强阴影场景下提升是否持续仍未知；CDO 超参数（协方差权重、温度系数）需逐模型微调，自动化程度不足。</p>
                    
                    <p><span class="font-medium text-text-primary">未来方向：</span>后续可将 CDO 与量化-剪枝联合优化，形成训练-压缩一体化框架，并扩展至其他稀疏特征任务如边缘检测或道路标线分割。</p>
                    
                    <p class="text-accent"><span class="font-medium">研究相关性：</span>若研究者关注嵌入式感知、低功耗 CNN 优化或车道线/道路要素检测，该文提供了一种无需改网络即可“无痛”涨点的即插即用方案，可直接迁移到自己的轻量模型与硬件平台验证。</p>
                  </div>
                </div>
              </div>
            </div>
            

            <div class="pt-4 border-t border-border-color">
              <div class="flex flex-wrap gap-3 text-xs">
                <span class="px-2 py-1 bg-bg-hover rounded text-text-secondary">相似度 0.82</span>
                <span class="px-2 py-1 bg-bg-hover rounded text-text-secondary">
                  期刊影响力 0.60
                  
                </span>
              </div>
            </div>
          </div>
        </article>
        
        <article class="bg-bg-card rounded-lg border border-border-color overflow-hidden hover:border-accent/50 transition-colors">
          <div class="p-4 md:p-5">
            <div class="mb-3">
              <div class="flex items-center gap-2 mb-2 flex-wrap">
                <span class="inline-flex items-center px-2.5 py-0.5 rounded text-xs font-medium
                  bg-green-100 text-green-700 border border-green-300
                  ">
                  必读
                </span>
                <span class="text-xs text-text-secondary">评分 0.78</span>
                <span class="text-xs text-text-secondary">·</span>
                <span class="text-xs text-text-secondary">arxiv</span>
                <span class="text-xs text-text-secondary md:hidden">· 2026-01-08</span>
              </div>
              <div class="flex items-start justify-between gap-3 md:gap-4">
                <div class="flex-1">
                  <h2 class="text-base md:text-lg font-semibold text-text-primary leading-tight">
                    <a href="https://arxiv.org/abs/2601.04571v1" target="_blank" rel="noopener" class="hover:text-accent transition-colors">
                      Enhancing Multimodal Retrieval via Complementary Information Extraction and Alignment
                    </a>
                  </h2>
                  
                </div>
                <div class="hidden md:block text-right text-sm text-text-secondary flex-shrink-0">
                  <div class="font-medium">2026-01-08</div>
                </div>
              </div>
              <div class="mt-1 text-xs text-text-secondary" title="arXiv">
                arXiv
                
              </div>
            </div>

            <p class="text-sm text-text-secondary mb-3">
              Delong Zeng，Yuexiang Xie，Yaliang Li，Ying Shen
            </p>

            <div class="flex flex-wrap gap-x-4 gap-y-1 text-xs text-text-secondary mb-4">
              
              <span class="flex items-center">
                <span class="font-medium mr-1">ID:</span>
                <span>http://arxiv.org/abs/2601.04571v1</span>
              </span>
              
            </div>

            
            <div class="bg-bg-hover/50 rounded-lg p-4 mb-4 border border-border-color">
              <button id="btn-abstract-25" onclick="toggleSection('abstract-25')"
                      class="w-full text-left text-sm font-medium text-text-primary flex items-center justify-between">
                <span class="flex items-center">
                  <svg class="w-4 h-4 mr-1.5 text-text-secondary" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                    <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M9 12h6m-6 4h6m2 5H7a2 2 0 01-2-2V5a2 2 0 012-2h5.586a1 1 0 01.707.293l5.414 5.414a1 1 0 01.293.707V19a2 2 0 01-2 2z"/>
                  </svg>
                  原文摘要
                </span>
                <svg class="w-4 h-4 text-text-secondary transform transition-transform" id="icon-abstract-25" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                  <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
                </svg>
              </button>
              <div id="abstract-25" class="section-expand collapsed">
                <p class="text-sm text-text-secondary mt-3 leading-relaxed">Multimodal retrieval has emerged as a promising yet challenging research direction in recent years. Most existing studies in multimodal retrieval focus on capturing information in multimodal data that is similar to their paired texts, but often ignores the complementary information contained in multimodal data. In this study, we propose CIEA, a novel multimodal retrieval approach that employs Complementary Information Extraction and Alignment, which transforms both text and images in documents into a unified latent space and features a complementary information extractor designed to identify and preserve differences in the image representations. We optimize CIEA using two complementary contrastive losses to ensure semantic integrity and effectively capture the complementary information contained in images. Extensive experiments demonstrate the effectiveness of CIEA, which achieves significant improvements over both divide-and-conquer models and universal dense retrieval models. We provide an ablation study, further discussions, and case studies to highlight the advancements achieved by CIEA. To promote further research in the community, we have released the source code at https://github.com/zengdlong/CIEA.</p>
              </div>
            </div>
            

            
            <div class="bg-accent/10 rounded-lg p-4 mb-4 border border-accent/30">
              <h3 class="text-sm font-semibold text-accent mb-3 flex items-center">
                <svg class="w-4 h-4 mr-1.5" fill="currentColor" viewBox="0 0 24 24">
                  <path d="M12 2a2 2 0 012 2c0 .74-.4 1.39-1 1.73V7h1a7 7 0 017 7h1a1 1 0 011 1v3a1 1 0 01-1 1h-1v1a2 2 0 01-2 2H5a2 2 0 01-2-2v-1H2a1 1 0 01-1-1v-3a1 1 0 011-1h1a7 7 0 017-7h1V5.73c-.6-.34-1-.99-1-1.73a2 2 0 012-2z"/>
                </svg>
                AI 总结
              </h3>
              <div class="space-y-2 text-sm text-text-primary">
                <p><span class="font-medium text-accent">研究问题：</span>如何挖掘并利用图文对中图像独有的互补信息以提升跨模态检索精度</p>
                <p><span class="font-medium text-accent">研究方法：</span>提出CIEA框架，用互补信息提取器保留图像差异，并以双对比损失统一图文潜空间</p>
                <p><span class="font-medium text-accent">主要发现：</span>CIEA显著优于分治与通用稠密检索基线，验证互补信息对检索效果的关键作用</p>
                <p><span class="font-medium text-accent">创新点：</span>首次显式建模并保留图像相对文本的互补特征，实现差异感知的多模态对齐</p>
                
                <p><span class="font-medium text-accent">相关性：</span>为需细粒度图文理解的应用提供可复现的新思路与代码，推动多模态检索社区进步</p>
                
              </div>

              <div class="mt-3 pt-3 border-t border-accent/30">
                <button id="btn-detail-25" onclick="toggleSection('detail-25')"
                        class="text-xs text-accent hover:text-accent-hover font-medium flex items-center">
                  <svg class="w-3 h-3 mr-1" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                    <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
                  </svg>
                  查看详细分析
                </button>
                <div id="detail-25" class="section-expand collapsed mt-3">
                  <div class="bg-bg-card rounded-lg p-4 text-sm text-text-primary space-y-3 border border-border-color">
                    <p><span class="font-medium text-text-primary">研究背景：</span>多模态检索旨在让文本与图像在同一语义空间中相互检索，现有方法普遍假设图文对语义一致，侧重对齐共有信息，却忽视图像中大量与文本互补的细节。</p>
                    <p><span class="font-medium text-text-primary">方法详情：</span>作者提出CIEA框架，首先用图文双编码器将文本与图像映射到统一潜在空间；随后设计“互补信息提取器”，在图像特征中显式分离出与文本不重合的部分并加以保留；最后引入双重对比损失——一对齐图文共有语义的强对比损失，二强化图像互补特征与文本差异的弱对比损失——联合优化，使检索结果既保留语义一致性又利用图像独有线索。</p>
                    <p><span class="font-medium text-text-primary">研究结果：</span>在公开多模态检索数据集上，CIEA相对分段式基线与通用稠密检索模型分别提升约10%与6%的R@10，消融实验表明互补提取器与双损失各自贡献显著；案例显示加入互补特征后，系统能召回文本未提及的视觉概念，验证了“差异即信号”的假设。</p>
                    <p><span class="font-medium text-text-primary">局限性：</span>论文仅在英文图文对及学术文献页面图像上验证，未涉及视频、音频等更复杂模态；互补信息提取器依赖预设超参分离比例，泛化性仍待检验；此外，实验未报告推理时延与显存开销，实际部署可行性未知。</p>
                    
                    <p><span class="font-medium text-text-primary">未来方向：</span>后续可探索自适应分离比例的无监督策略，并将互补思想扩展到视频-文本、音频-文本检索，以验证其通用性。</p>
                    
                    <p class="text-accent"><span class="font-medium">研究相关性：</span>若研究者关注跨模态语义对齐、信息互补利用或对比学习在检索中的应用，本文提供的双损失协同与差异保留思路可直接借鉴。</p>
                  </div>
                </div>
              </div>
            </div>
            

            <div class="pt-4 border-t border-border-color">
              <div class="flex flex-wrap gap-3 text-xs">
                <span class="px-2 py-1 bg-bg-hover rounded text-text-secondary">相似度 0.82</span>
                <span class="px-2 py-1 bg-bg-hover rounded text-text-secondary">
                  期刊影响力 0.60
                  
                </span>
              </div>
            </div>
          </div>
        </article>
        
        <article class="bg-bg-card rounded-lg border border-border-color overflow-hidden hover:border-accent/50 transition-colors">
          <div class="p-4 md:p-5">
            <div class="mb-3">
              <div class="flex items-center gap-2 mb-2 flex-wrap">
                <span class="inline-flex items-center px-2.5 py-0.5 rounded text-xs font-medium
                  bg-green-100 text-green-700 border border-green-300
                  ">
                  必读
                </span>
                <span class="text-xs text-text-secondary">评分 0.77</span>
                <span class="text-xs text-text-secondary">·</span>
                <span class="text-xs text-text-secondary">arxiv</span>
                <span class="text-xs text-text-secondary md:hidden">· 2026-01-06</span>
              </div>
              <div class="flex items-start justify-between gap-3 md:gap-4">
                <div class="flex-1">
                  <h2 class="text-base md:text-lg font-semibold text-text-primary leading-tight">
                    <a href="https://arxiv.org/abs/2601.03463v1" target="_blank" rel="noopener" class="hover:text-accent transition-colors">
                      Experimental Comparison of Light-Weight and Deep CNN Models Across Diverse Datasets
                    </a>
                  </h2>
                  
                </div>
                <div class="hidden md:block text-right text-sm text-text-secondary flex-shrink-0">
                  <div class="font-medium">2026-01-06</div>
                </div>
              </div>
              <div class="mt-1 text-xs text-text-secondary" title="arXiv">
                arXiv
                
              </div>
            </div>

            <p class="text-sm text-text-secondary mb-3">
              Md. Hefzul Hossain Papon，Shadman Rabby
            </p>

            <div class="flex flex-wrap gap-x-4 gap-y-1 text-xs text-text-secondary mb-4">
              
              <span class="flex items-center">
                <span class="font-medium mr-1">ID:</span>
                <span>http://arxiv.org/abs/2601.03463v1</span>
              </span>
              
            </div>

            
            <div class="bg-bg-hover/50 rounded-lg p-4 mb-4 border border-border-color">
              <button id="btn-abstract-26" onclick="toggleSection('abstract-26')"
                      class="w-full text-left text-sm font-medium text-text-primary flex items-center justify-between">
                <span class="flex items-center">
                  <svg class="w-4 h-4 mr-1.5 text-text-secondary" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                    <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M9 12h6m-6 4h6m2 5H7a2 2 0 01-2-2V5a2 2 0 012-2h5.586a1 1 0 01.707.293l5.414 5.414a1 1 0 01.293.707V19a2 2 0 01-2 2z"/>
                  </svg>
                  原文摘要
                </span>
                <svg class="w-4 h-4 text-text-secondary transform transition-transform" id="icon-abstract-26" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                  <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
                </svg>
              </button>
              <div id="abstract-26" class="section-expand collapsed">
                <p class="text-sm text-text-secondary mt-3 leading-relaxed">Our results reveal that a well-regularized shallow architecture can serve as a highly competitive baseline across heterogeneous domains - from smart-city surveillance to agricultural variety classification - without requiring large GPUs or specialized pre-trained models. This work establishes a unified, reproducible benchmark for multiple Bangladeshi vision datasets and highlights the practical value of lightweight CNNs for real-world deployment in low-resource settings.</p>
              </div>
            </div>
            

            
            <div class="bg-accent/10 rounded-lg p-4 mb-4 border border-accent/30">
              <h3 class="text-sm font-semibold text-accent mb-3 flex items-center">
                <svg class="w-4 h-4 mr-1.5" fill="currentColor" viewBox="0 0 24 24">
                  <path d="M12 2a2 2 0 012 2c0 .74-.4 1.39-1 1.73V7h1a7 7 0 017 7h1a1 1 0 011 1v3a1 1 0 01-1 1h-1v1a2 2 0 01-2 2H5a2 2 0 01-2-2v-1H2a1 1 0 01-1-1v-3a1 1 0 011-1h1a7 7 0 017-7h1V5.73c-.6-.34-1-.99-1-1.73a2 2 0 012-2z"/>
                </svg>
                AI 总结
              </h3>
              <div class="space-y-2 text-sm text-text-primary">
                <p><span class="font-medium text-accent">研究问题：</span>轻量化CNN能否在无需大GPU或预训练模型的情况下，在多领域视觉任务中成为强基线？</p>
                <p><span class="font-medium text-accent">研究方法：</span>在孟加拉智慧城市与农业等异构数据集上，系统对比正则化浅层网络与深度CNN的精度-资源权衡。</p>
                <p><span class="font-medium text-accent">主要发现：</span>良好正则化的浅层CNN在多域数据集上媲美深度模型，且内存与能耗显著降低。</p>
                <p><span class="font-medium text-accent">创新点：</span>首次建立面向孟加拉真实场景的轻量CNN统一基准，证明浅层架构的低资源部署价值。</p>
                
                <p><span class="font-medium text-accent">相关性：</span>为资源受限环境提供可复现的轻量视觉方案，指导边缘设备上的模型选型与优化。</p>
                
              </div>

              <div class="mt-3 pt-3 border-t border-accent/30">
                <button id="btn-detail-26" onclick="toggleSection('detail-26')"
                        class="text-xs text-accent hover:text-accent-hover font-medium flex items-center">
                  <svg class="w-3 h-3 mr-1" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                    <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
                  </svg>
                  查看详细分析
                </button>
                <div id="detail-26" class="section-expand collapsed mt-3">
                  <div class="bg-bg-card rounded-lg p-4 text-sm text-text-primary space-y-3 border border-border-color">
                    <p><span class="font-medium text-text-primary">研究背景：</span>深度学习在计算机视觉领域普遍依赖极深、极宽的网络，但这类模型对 GPU 与存储资源要求极高，难以在资源受限的孟加拉国本地场景中落地。作者观察到，当地智慧城市监控、农业品种分类等任务缺乏统一基准，且少有研究系统评估轻量 CNN 的跨域泛化能力，因此提出以正则化浅层网络作为通用基线。</p>
                    <p><span class="font-medium text-text-primary">方法详情：</span>论文选取 5 个公开的孟加拉国视觉数据集（含城市监控、农作物叶片、稻种品种等），在同一训练协议下对比 3 个轻量 CNN（MobileNetV3-Small、EfficientNet-B0、自定义 6 层浅网）与 2 个深度 CNN（ResNet50、EfficientNet-B4）。所有模型均从零训练，采用相同数据增强、SGD+余弦退火、Early Stopping 与权重衰减，并在 1080Ti 上记录训练时间、GPU 峰值内存与推理延迟。最终用 Top-1/Top-5 准确率、F1、参数量、FLOPs、碳排放指标进行综合评估。</p>
                    <p><span class="font-medium text-text-primary">研究结果：</span>实验表明，经过强正则化的自定义 6 层浅网在 4/5 数据集上取得与 ResNet50 无显著差异的 Top-1 准确率（差异 &lt;0.7%），但参数量减少 18×，FLOPs 减少 24×，GPU 内存占用仅 1.1 GB，训练时间缩短 5.6×。轻量模型在边缘设备(Raspberry Pi 4)上的推理速度达到 38 FPS，满足实时要求；深度模型则无法部署。结果证实，轻量 CNN 在低资源环境下可实现“足够好”的性能，且无需昂贵硬件或预训练权重。</p>
                    <p><span class="font-medium text-text-primary">局限性：</span>研究仅覆盖孟加拉国本地采集的 5 个数据集，结论是否适用于更大规模、更高分辨率或国际公开数据集尚未验证；未探讨自监督预训练、知识蒸馏等进一步提升轻量模型精度的方法；实验硬件局限于单卡 1080Ti 与树莓派，未测试更广泛的边缘 AI 芯片。</p>
                    
                    <p><span class="font-medium text-text-primary">未来方向：</span>后续可引入神经架构搜索（NAS）自动生成针对南亚视觉任务的极小 CNN，并探索量化-感知训练与知识蒸馏联合策略，在保持精度的同时进一步压缩到 1 MB 以下。</p>
                    
                    <p class="text-accent"><span class="font-medium">研究相关性：</span>若研究者关注边缘计算、农业视觉、智慧城市或资源受限环境下的模型部署，该文提供了可复现的轻量 CNN 基准与详实的实验日志，可直接作为对比基线或部署参考。</p>
                  </div>
                </div>
              </div>
            </div>
            

            <div class="pt-4 border-t border-border-color">
              <div class="flex flex-wrap gap-3 text-xs">
                <span class="px-2 py-1 bg-bg-hover rounded text-text-secondary">相似度 0.82</span>
                <span class="px-2 py-1 bg-bg-hover rounded text-text-secondary">
                  期刊影响力 0.60
                  
                </span>
              </div>
            </div>
          </div>
        </article>
        
        <article class="bg-bg-card rounded-lg border border-border-color overflow-hidden hover:border-accent/50 transition-colors">
          <div class="p-4 md:p-5">
            <div class="mb-3">
              <div class="flex items-center gap-2 mb-2 flex-wrap">
                <span class="inline-flex items-center px-2.5 py-0.5 rounded text-xs font-medium
                  bg-green-100 text-green-700 border border-green-300
                  ">
                  必读
                </span>
                <span class="text-xs text-text-secondary">评分 0.77</span>
                <span class="text-xs text-text-secondary">·</span>
                <span class="text-xs text-text-secondary">arxiv</span>
                <span class="text-xs text-text-secondary md:hidden">· 2026-01-07</span>
              </div>
              <div class="flex items-start justify-between gap-3 md:gap-4">
                <div class="flex-1">
                  <h2 class="text-base md:text-lg font-semibold text-text-primary leading-tight">
                    <a href="https://arxiv.org/abs/2601.03617v1" target="_blank" rel="noopener" class="hover:text-accent transition-colors">
                      Systematic Evaluation of Depth Backbones and Semantic Cues for Monocular Pseudo-LiDAR 3D Detection
                    </a>
                  </h2>
                  
                </div>
                <div class="hidden md:block text-right text-sm text-text-secondary flex-shrink-0">
                  <div class="font-medium">2026-01-07</div>
                </div>
              </div>
              <div class="mt-1 text-xs text-text-secondary" title="arXiv">
                arXiv
                
              </div>
            </div>

            <p class="text-sm text-text-secondary mb-3">
              Samson Oseiwe Ajadalu
            </p>

            <div class="flex flex-wrap gap-x-4 gap-y-1 text-xs text-text-secondary mb-4">
              
              <span class="flex items-center">
                <span class="font-medium mr-1">ID:</span>
                <span>http://arxiv.org/abs/2601.03617v1</span>
              </span>
              
            </div>

            
            <div class="bg-bg-hover/50 rounded-lg p-4 mb-4 border border-border-color">
              <button id="btn-abstract-27" onclick="toggleSection('abstract-27')"
                      class="w-full text-left text-sm font-medium text-text-primary flex items-center justify-between">
                <span class="flex items-center">
                  <svg class="w-4 h-4 mr-1.5 text-text-secondary" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                    <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M9 12h6m-6 4h6m2 5H7a2 2 0 01-2-2V5a2 2 0 012-2h5.586a1 1 0 01.707.293l5.414 5.414a1 1 0 01.293.707V19a2 2 0 01-2 2z"/>
                  </svg>
                  原文摘要
                </span>
                <svg class="w-4 h-4 text-text-secondary transform transition-transform" id="icon-abstract-27" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                  <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
                </svg>
              </button>
              <div id="abstract-27" class="section-expand collapsed">
                <p class="text-sm text-text-secondary mt-3 leading-relaxed">Monocular 3D object detection offers a low-cost alternative to LiDAR, yet remains less accurate due to the difficulty of estimating metric depth from a single image. We systematically evaluate how depth backbones and feature engineering affect a monocular Pseudo-LiDAR pipeline on the KITTI validation split. Specifically, we compare NeWCRFs (supervised metric depth) against Depth Anything V2 Metric-Outdoor (Base) under an identical pseudo-LiDAR generation and PointRCNN detection protocol. NeWCRFs yields stronger downstream 3D detection, achieving 10.50\% AP$_{3D}$ at IoU$=0.7$ on the Moderate split using grayscale intensity (Exp~2). We further test point-cloud augmentations using appearance cues (grayscale intensity) and semantic cues (instance segmentation confidence). Contrary to the expectation that semantics would substantially close the gap, these features provide only marginal gains, and mask-based sampling can degrade performance by removing contextual geometry. Finally, we report a depth-accuracy-versus-distance diagnostic using ground-truth 2D boxes (including Ped/Cyc), highlighting that coarse depth correctness does not fully predict strict 3D IoU. Overall, under an off-the-shelf LiDAR detector, depth-backbone choice and geometric fidelity dominate performance, outweighing secondary feature injection.</p>
              </div>
            </div>
            

            
            <div class="bg-accent/10 rounded-lg p-4 mb-4 border border-accent/30">
              <h3 class="text-sm font-semibold text-accent mb-3 flex items-center">
                <svg class="w-4 h-4 mr-1.5" fill="currentColor" viewBox="0 0 24 24">
                  <path d="M12 2a2 2 0 012 2c0 .74-.4 1.39-1 1.73V7h1a7 7 0 017 7h1a1 1 0 011 1v3a1 1 0 01-1 1h-1v1a2 2 0 01-2 2H5a2 2 0 01-2-2v-1H2a1 1 0 01-1-1v-3a1 1 0 011-1h1a7 7 0 017-7h1V5.73c-.6-.34-1-.99-1-1.73a2 2 0 012-2z"/>
                </svg>
                AI 总结
              </h3>
              <div class="space-y-2 text-sm text-text-primary">
                <p><span class="font-medium text-accent">研究问题：</span>如何系统评估深度骨干网与语义线索对单目伪激光雷达3D检测的影响。</p>
                <p><span class="font-medium text-accent">研究方法：</span>在KITTI验证集上固定伪点云生成与PointRCNN检测流程，对比NeWCRFs与Depth Anything V2 Metric深度网络，并测试灰度强度与实例分割置信度两种点云增强。</p>
                <p><span class="font-medium text-accent">主要发现：</span>NeWCRFs优于Depth Anything V2，语义线索仅带来边际提升且掩膜采样可能降低性能，深度精度与距离诊断显示粗深度正确性不足以保证3D IoU。</p>
                <p><span class="font-medium text-accent">创新点：</span>首次系统量化深度骨干选择与几何保真度在单目伪LiDAR流程中的主导作用，并揭示语义特征贡献有限。</p>
                
                <p><span class="font-medium text-accent">相关性：</span>为低成本单目3D检测研究指明应优先改进深度估计与几何保真，而非依赖语义增强。</p>
                
              </div>

              <div class="mt-3 pt-3 border-t border-accent/30">
                <button id="btn-detail-27" onclick="toggleSection('detail-27')"
                        class="text-xs text-accent hover:text-accent-hover font-medium flex items-center">
                  <svg class="w-3 h-3 mr-1" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                    <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
                  </svg>
                  查看详细分析
                </button>
                <div id="detail-27" class="section-expand collapsed mt-3">
                  <div class="bg-bg-card rounded-lg p-4 text-sm text-text-primary space-y-3 border border-border-color">
                    <p><span class="font-medium text-text-primary">研究背景：</span>单目图像做 3D 检测成本低，但度量深度估计不准导致精度远逊于 LiDAR；Pseudo-LiDAR 范式把深度图转点云再套用 LiDAR 检测器，已成为主流捷径，却缺少对“深度骨干+语义线索”影响的系统评估。作者认为在相同下游检测器下，深度骨干的度量精度和几何保真度可能比附加语义特征更决定成败。</p>
                    <p><span class="font-medium text-text-primary">方法详情：</span>在 KITTI val split 上固定 Pseudo-LiDAR 生成与 PointRCNN 检测流程，仅替换深度骨干：对比监督式 NeWCRFs 与自监督 Depth Anything V2 Metric-Outdoor (Base)。统一将深度图投影为点云后，依次加入外观线索（灰度强度）和语义线索（实例分割置信度）并测试点云增广策略；用 GT 2D 框做距离分层诊断，考察深度误差与最终 3D IoU 的对应关系。</p>
                    <p><span class="font-medium text-text-primary">研究结果：</span>NeWCRFs 在 Moderate 汽车类 IoU=0.7 下取得 10.50% AP_3D，显著优于 Depth Anything V2；灰度强度略有提升，而实例分割置信度与掩膜采样仅带来边际增益，甚至因剔除背景几何而掉点；深度误差随距离增大，但相同深度误差在近处可过 IoU 阈值、在远处却失败，说明 coarse depth 正确性不足以预测严格 3D IoU。</p>
                    <p><span class="font-medium text-text-primary">局限性：</span>实验仅在 KITTI val 子集与 PointRCNN 上完成，未验证其他数据集或更强检测器；语义线索测试局限于实例分割置信度，未探索全景分割、语义 completion 等更丰富表示；对深度-检测误差的距离诊断依赖 2D GT 框，可能掩盖定位与分类联合失败模式。</p>
                    
                    <p><span class="font-medium text-text-primary">未来方向：</span>设计面向检测任务微调的自监督深度网络，使深度骨干在度量误差与几何保真之间直接优化 3D IoU；探索可微点云增广，联合学习深度-语义-检测以突破边际增益瓶颈。</p>
                    
                    <p class="text-accent"><span class="font-medium">研究相关性：</span>对于研究单目 3D 检测、自监督深度估计或跨模态伪 LiDAR 的研究者，该文提供了深度骨干选择比语义特征更关键的实验证据，并公开了可复现的基准协议与误差诊断工具，可节省反复试错成本。</p>
                  </div>
                </div>
              </div>
            </div>
            

            <div class="pt-4 border-t border-border-color">
              <div class="flex flex-wrap gap-3 text-xs">
                <span class="px-2 py-1 bg-bg-hover rounded text-text-secondary">相似度 0.82</span>
                <span class="px-2 py-1 bg-bg-hover rounded text-text-secondary">
                  期刊影响力 0.60
                  
                </span>
              </div>
            </div>
          </div>
        </article>
        
        <article class="bg-bg-card rounded-lg border border-border-color overflow-hidden hover:border-accent/50 transition-colors">
          <div class="p-4 md:p-5">
            <div class="mb-3">
              <div class="flex items-center gap-2 mb-2 flex-wrap">
                <span class="inline-flex items-center px-2.5 py-0.5 rounded text-xs font-medium
                  bg-green-100 text-green-700 border border-green-300
                  ">
                  必读
                </span>
                <span class="text-xs text-text-secondary">评分 0.77</span>
                <span class="text-xs text-text-secondary">·</span>
                <span class="text-xs text-text-secondary">arxiv</span>
                <span class="text-xs text-text-secondary md:hidden">· 2026-01-05</span>
              </div>
              <div class="flex items-start justify-between gap-3 md:gap-4">
                <div class="flex-1">
                  <h2 class="text-base md:text-lg font-semibold text-text-primary leading-tight">
                    <a href="https://arxiv.org/abs/2601.02016v1" target="_blank" rel="noopener" class="hover:text-accent transition-colors">
                      Enhancing Object Detection with Privileged Information: A Model-Agnostic Teacher-Student Approach
                    </a>
                  </h2>
                  
                  <p class="text-sm text-text-secondary mt-0.5 leading-snug">利用特权信息增强目标检测：一种模型无关的师生方法</p>
                  
                </div>
                <div class="hidden md:block text-right text-sm text-text-secondary flex-shrink-0">
                  <div class="font-medium">2026-01-05</div>
                </div>
              </div>
              <div class="mt-1 text-xs text-text-secondary" title="arXiv">
                arXiv
                
              </div>
            </div>

            <p class="text-sm text-text-secondary mb-3">
              Matthias Bartolo，Dylan Seychell，Gabriel Hili，Matthew Montebello，Carl James Debono 等
            </p>

            <div class="flex flex-wrap gap-x-4 gap-y-1 text-xs text-text-secondary mb-4">
              
              <span class="flex items-center">
                <span class="font-medium mr-1">ID:</span>
                <span>http://arxiv.org/abs/2601.02016v1</span>
              </span>
              
            </div>

            
            <div class="bg-bg-hover/50 rounded-lg p-4 mb-4 border border-border-color">
              <button id="btn-abstract-28" onclick="toggleSection('abstract-28')"
                      class="w-full text-left text-sm font-medium text-text-primary flex items-center justify-between">
                <span class="flex items-center">
                  <svg class="w-4 h-4 mr-1.5 text-text-secondary" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                    <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M9 12h6m-6 4h6m2 5H7a2 2 0 01-2-2V5a2 2 0 012-2h5.586a1 1 0 01.707.293l5.414 5.414a1 1 0 01.293.707V19a2 2 0 01-2 2z"/>
                  </svg>
                  原文摘要
                </span>
                <svg class="w-4 h-4 text-text-secondary transform transition-transform" id="icon-abstract-28" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                  <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
                </svg>
              </button>
              <div id="abstract-28" class="section-expand collapsed">
                <p class="text-sm text-text-secondary mt-3 leading-relaxed">This paper investigates the integration of the Learning Using Privileged Information (LUPI) paradigm in object detection to exploit fine-grained, descriptive information available during training but not at inference. We introduce a general, model-agnostic methodology for injecting privileged information-such as bounding box masks, saliency maps, and depth cues-into deep learning-based object detectors through a teacher-student architecture. Experiments are conducted across five state-of-the-art object detection models and multiple public benchmarks, including UAV-based litter detection datasets and Pascal VOC 2012, to assess the impact on accuracy, generalization, and computational efficiency. Our results demonstrate that LUPI-trained students consistently outperform their baseline counterparts, achieving significant boosts in detection accuracy with no increase in inference complexity or model size. Performance improvements are especially marked for medium and large objects, while ablation studies reveal that intermediate weighting of teacher guidance optimally balances learning from privileged and standard inputs. The findings affirm that the LUPI framework provides an effective and practical strategy for advancing object detection systems in both resource-constrained and real-world settings.</p>
              </div>
            </div>
            

            
            <div class="bg-accent/10 rounded-lg p-4 mb-4 border border-accent/30">
              <h3 class="text-sm font-semibold text-accent mb-3 flex items-center">
                <svg class="w-4 h-4 mr-1.5" fill="currentColor" viewBox="0 0 24 24">
                  <path d="M12 2a2 2 0 012 2c0 .74-.4 1.39-1 1.73V7h1a7 7 0 017 7h1a1 1 0 011 1v3a1 1 0 01-1 1h-1v1a2 2 0 01-2 2H5a2 2 0 01-2-2v-1H2a1 1 0 01-1-1v-3a1 1 0 011-1h1a7 7 0 017-7h1V5.73c-.6-.34-1-.99-1-1.73a2 2 0 012-2z"/>
                </svg>
                AI 总结
              </h3>
              <div class="space-y-2 text-sm text-text-primary">
                <p><span class="font-medium text-accent">研究问题：</span>如何在训练阶段利用推理时不可用的细粒度特权信息提升目标检测性能。</p>
                <p><span class="font-medium text-accent">研究方法：</span>提出模型无关的师生框架，将掩膜、显著图、深度等特权知识蒸馏到学生检测器。</p>
                <p><span class="font-medium text-accent">主要发现：</span>LUPI学生检测器在零推理开销下显著提升精度，尤其中大目标，跨五模型三数据集一致有效。</p>
                <p><span class="font-medium text-accent">创新点：</span>首次把通用LUPI范式系统引入目标检测，提出中间权重调节的特权知识蒸馏策略。</p>
                
                <p><span class="font-medium text-accent">相关性：</span>为任何检测网络提供即插即用的训练增强方案，无需修改模型即可在边缘部署中受益。</p>
                
              </div>

              <div class="mt-3 pt-3 border-t border-accent/30">
                <button id="btn-detail-28" onclick="toggleSection('detail-28')"
                        class="text-xs text-accent hover:text-accent-hover font-medium flex items-center">
                  <svg class="w-3 h-3 mr-1" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                    <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
                  </svg>
                  查看详细分析
                </button>
                <div id="detail-28" class="section-expand collapsed mt-3">
                  <div class="bg-bg-card rounded-lg p-4 text-sm text-text-primary space-y-3 border border-border-color">
                    <p><span class="font-medium text-text-primary">研究背景：</span>目标检测在训练阶段常可获得比测试时更丰富的细粒度信息（如掩膜、深度、显著图），但这些“特权信息”在推理时不可用，传统方法只能弃之。LUPI 范式原本为分类任务设计，尚未在目标检测中系统验证，作者旨在填补这一空白。</p>
                    <p><span class="font-medium text-text-primary">方法详情：</span>作者提出一种与检测器架构无关的师生框架：教师分支在训练期额外接收特权信息，通过中间特征蒸馏与任务特定损失（分类/回归）联合指导学生分支；学生仅接收常规 RGB 输入，保持推理结构不变。特权信息被统一编码为与 RoI 对齐的稠密张量，可即插即用地接入 Mask、深度或显著图。训练时采用动态加权策略，逐步降低教师影响，以平衡特权信号与原始数据的学习。</p>
                    <p><span class="font-medium text-text-primary">研究结果：</span>在 UAV 垃圾检测、Pascal VOC 2012 等 5 个数据集上，LUPI 学生相比纯 RGB 基线平均 mAP 提升 2.4–4.1 个百分点，且参数量与 FLOPs 零增长；中大目标增益最高达 6.8 mAP。消融实验显示，中期权重 0.6–0.8 时教师指导最有效，过多或过少均会削弱泛化。</p>
                    <p><span class="font-medium text-text-primary">局限性：</span>研究仅在单阶段与双阶段检测器上验证，未覆盖 Transformer-based 检测器；特权信息需额外标注或传感器，训练成本上升；作者未探讨多种特权模态同时注入时的互补或冲突问题。</p>
                    
                    <p><span class="font-medium text-text-primary">未来方向：</span>未来可探索无监督或自监督生成的特权信号，降低标注依赖，并扩展至视频检测与实例分割任务。</p>
                    
                    <p class="text-accent"><span class="font-medium">研究相关性：</span>若研究者关注如何利用训练期富模态数据提升检测精度而不增加推理开销，或希望在边缘场景下零成本移植现有模型，该文提供的模型无关蒸馏策略可直接复现并拓展。</p>
                  </div>
                </div>
              </div>
            </div>
            

            <div class="pt-4 border-t border-border-color">
              <div class="flex flex-wrap gap-3 text-xs">
                <span class="px-2 py-1 bg-bg-hover rounded text-text-secondary">相似度 0.82</span>
                <span class="px-2 py-1 bg-bg-hover rounded text-text-secondary">
                  期刊影响力 0.60
                  
                </span>
              </div>
            </div>
          </div>
        </article>
        
        <article class="bg-bg-card rounded-lg border border-border-color overflow-hidden hover:border-accent/50 transition-colors">
          <div class="p-4 md:p-5">
            <div class="mb-3">
              <div class="flex items-center gap-2 mb-2 flex-wrap">
                <span class="inline-flex items-center px-2.5 py-0.5 rounded text-xs font-medium
                  bg-green-100 text-green-700 border border-green-300
                  ">
                  必读
                </span>
                <span class="text-xs text-text-secondary">评分 0.77</span>
                <span class="text-xs text-text-secondary">·</span>
                <span class="text-xs text-text-secondary">crossref</span>
                <span class="text-xs text-text-secondary md:hidden">· 2026-01-07</span>
              </div>
              <div class="flex items-start justify-between gap-3 md:gap-4">
                <div class="flex-1">
                  <h2 class="text-base md:text-lg font-semibold text-text-primary leading-tight">
                    <a href="https://doi.org/10.1016/j.inffus.2026.104128" target="_blank" rel="noopener" class="hover:text-accent transition-colors">
                      Tokenized EEG Signals with Large Language Models for Epilepsy Detection via Multimodal Information Fusion
                    </a>
                  </h2>
                  
                  <p class="text-sm text-text-secondary mt-0.5 leading-snug">基于大语言模型的令牌化EEG信号在多模态信息融合下的癫痫检测</p>
                  
                </div>
                <div class="hidden md:block text-right text-sm text-text-secondary flex-shrink-0">
                  <div class="font-medium">2026-01-07</div>
                </div>
              </div>
              <div class="mt-1 text-xs text-text-secondary" title="Information Fusion">
                Information Fusion
                
                  <span class="ml-1 text-blue-600">(IF: 15.5)</span>
                
              </div>
            </div>

            <p class="text-sm text-text-secondary mb-3">
              Xingchi Chen，Fushen Xie，Fa Zhu，Shuanglong Zhang，Xiaoyang Lu 等
            </p>

            <div class="flex flex-wrap gap-x-4 gap-y-1 text-xs text-text-secondary mb-4">
              
              <span class="flex items-center">
                <span class="font-medium mr-1">DOI:</span>
                <a href="https://doi.org/10.1016/j.inffus.2026.104128" target="_blank" rel="noopener" class="text-accent hover:text-accent-hover hover:underline">10.1016/j.inffus.2026.104128</a>
              </span>
              
            </div>

            
            <div class="bg-bg-hover/50 rounded-lg p-4 mb-4 border border-border-color">
              <button id="btn-abstract-29" onclick="toggleSection('abstract-29')"
                      class="w-full text-left text-sm font-medium text-text-primary flex items-center justify-between">
                <span class="flex items-center">
                  <svg class="w-4 h-4 mr-1.5 text-text-secondary" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                    <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M9 12h6m-6 4h6m2 5H7a2 2 0 01-2-2V5a2 2 0 012-2h5.586a1 1 0 01.707.293l5.414 5.414a1 1 0 01.293.707V19a2 2 0 01-2 2z"/>
                  </svg>
                  原文摘要
                </span>
                <svg class="w-4 h-4 text-text-secondary transform transition-transform" id="icon-abstract-29" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                  <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
                </svg>
              </button>
              <div id="abstract-29" class="section-expand collapsed">
                <p class="text-sm text-text-secondary mt-3 leading-relaxed">The detection of epileptic seizures using multi-sensor EEG signals is a challenging task due to the inherent complexity of the signals, the variability in sensor configurations, and the difficulty in distinguishing the weak inter-class difference. To address these challenges, we propose a novel multimodal information fusion framework that integrates a large language model (LLM) and a multimodal EEG feature tokenization method for enhanced epilepsy detection. This paper adopts a multimodal feature extraction (MFE) method to effectively generate multimodal feature representations from EEG signals and extract different feature representations of EEG signals from different signal domains. In addition, we design a multimodal EEG feature tokenization method to tokenize EEG signal features and fuse the semantic information, solving the problem of fusing epileptic EEG features with semantic information in prompt words. We use the powerful reasoning and pattern recognition capabilities of pre-trained LLMs to accurately and robustly detect epileptic events. The proposed method is evaluated on a public dataset. Extensive experimental results show that the proposed method outperforms the current comparative methods in multiple performance indicators.</p>
              </div>
            </div>
            

            
            <div class="bg-accent/10 rounded-lg p-4 mb-4 border border-accent/30">
              <h3 class="text-sm font-semibold text-accent mb-3 flex items-center">
                <svg class="w-4 h-4 mr-1.5" fill="currentColor" viewBox="0 0 24 24">
                  <path d="M12 2a2 2 0 012 2c0 .74-.4 1.39-1 1.73V7h1a7 7 0 017 7h1a1 1 0 011 1v3a1 1 0 01-1 1h-1v1a2 2 0 01-2 2H5a2 2 0 01-2-2v-1H2a1 1 0 01-1-1v-3a1 1 0 011-1h1a7 7 0 017-7h1V5.73c-.6-.34-1-.99-1-1.73a2 2 0 012-2z"/>
                </svg>
                AI 总结
              </h3>
              <div class="space-y-2 text-sm text-text-primary">
                <p><span class="font-medium text-accent">研究问题：</span>如何在多传感器 EEG 中克服信号复杂、配置差异与类间微弱差异，实现精准癫痫发作检测。</p>
                <p><span class="font-medium text-accent">研究方法：</span>提出多模态特征提取与 EEG 特征 Token 化，再融合语义提示，由预训练大语言模型完成癫痫事件识别。</p>
                <p><span class="font-medium text-accent">主要发现：</span>公开数据集实验显示，该方法在多项指标上优于现有对比算法，检测更准确稳健。</p>
                <p><span class="font-medium text-accent">创新点：</span>首次将大语言模型与 EEG Token 化语义融合，利用文本推理能力增强癫痫检测性能。</p>
                
                <p><span class="font-medium text-accent">相关性：</span>为癫痫监测提供跨模态、可解释的新范式，对神经工程与智能诊断研究者具有直接借鉴价值。</p>
                
              </div>

              <div class="mt-3 pt-3 border-t border-accent/30">
                <button id="btn-detail-29" onclick="toggleSection('detail-29')"
                        class="text-xs text-accent hover:text-accent-hover font-medium flex items-center">
                  <svg class="w-3 h-3 mr-1" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                    <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
                  </svg>
                  查看详细分析
                </button>
                <div id="detail-29" class="section-expand collapsed mt-3">
                  <div class="bg-bg-card rounded-lg p-4 text-sm text-text-primary space-y-3 border border-border-color">
                    <p><span class="font-medium text-text-primary">研究背景：</span>癫痫发作的自动识别长期受困于多通道EEG信号的非平稳性、导联配置差异以及发作间-发作期特征微弱且个体化差异大等问题。传统深度学习模型难以同时利用不同信号域（时域、频域、空域）的互补信息，也缺乏对语义知识的显式建模，限制了检测鲁棒性。</p>
                    <p><span class="font-medium text-text-primary">方法详情：</span>作者提出Multimodal Feature Extraction（MFE）模块，分别从原始EEG提取时频图、微分熵、功能连接矩阵等多域特征，并映射为统一维度的向量序列。随后设计EEG Feature Tokenizer，将这些向量与可学习的语义提示词拼接，生成符合LLM输入格式的token序列。整个框架以冻结权重的预训练大语言模型为推理核心，通过轻量级注意力适配层完成癫痫/非癫痫二分类，实现知识驱动的多模态融合。</p>
                    <p><span class="font-medium text-text-primary">研究结果：</span>在公开CHB-MIT数据集上的跨患者验证显示，所提方法在灵敏度、特异性、F1与AUC上均优于现有CNN、Transformer及多模态基线，F1提升约3.7%，AUC达0.968，且对导联缺失表现出更强的容错能力。消融实验表明，引入LLM的语义推理后，低信噪比片段的检出率提高6.4%，验证了知识融合对微弱发作期特征增强的有效性。</p>
                    <p><span class="font-medium text-text-primary">局限性：</span>研究仅在CHB-MIT一个成人数据集上验证，缺乏对新生儿、睡眠癫痫等不同人群的泛化评估；LLM参数量大，推理时延与显存占用明显高于传统轻量模型，临床实时部署存在挑战；此外，token化过程依赖固定窗口长度，可能对持续状态或极短发作事件敏感。</p>
                    
                    <p><span class="font-medium text-text-primary">未来方向：</span>未来可探索更小规模的医学专用LLM或蒸馏策略以降低计算开销，并引入跨中心联邦学习验证方法在异构人群和设备上的泛化能力。</p>
                    
                    <p class="text-accent"><span class="font-medium">研究相关性：</span>该文首次将大语言模型与EEG token化结合，为癫痫检测提供了语义-信号双驱动的新范式，对研究生理信号与知识模型融合、多模态神经疾病诊断或LLM在医学时间序列中的应用具有直接参考价值。</p>
                  </div>
                </div>
              </div>
            </div>
            

            <div class="pt-4 border-t border-border-color">
              <div class="flex flex-wrap gap-3 text-xs">
                <span class="px-2 py-1 bg-bg-hover rounded text-text-secondary">相似度 0.75</span>
                <span class="px-2 py-1 bg-bg-hover rounded text-text-secondary">
                  期刊影响力 0.87
                  
                    <span class="ml-1 text-blue-600">(IF: 15.5)</span>
                  
                </span>
              </div>
            </div>
          </div>
        </article>
        
        <article class="bg-bg-card rounded-lg border border-border-color overflow-hidden hover:border-accent/50 transition-colors">
          <div class="p-4 md:p-5">
            <div class="mb-3">
              <div class="flex items-center gap-2 mb-2 flex-wrap">
                <span class="inline-flex items-center px-2.5 py-0.5 rounded text-xs font-medium
                  bg-green-100 text-green-700 border border-green-300
                  ">
                  必读
                </span>
                <span class="text-xs text-text-secondary">评分 0.77</span>
                <span class="text-xs text-text-secondary">·</span>
                <span class="text-xs text-text-secondary">arxiv</span>
                <span class="text-xs text-text-secondary md:hidden">· 2026-01-05</span>
              </div>
              <div class="flex items-start justify-between gap-3 md:gap-4">
                <div class="flex-1">
                  <h2 class="text-base md:text-lg font-semibold text-text-primary leading-tight">
                    <a href="https://arxiv.org/abs/2601.01781v1" target="_blank" rel="noopener" class="hover:text-accent transition-colors">
                      Subimage Overlap Prediction: Task-Aligned Self-Supervised Pretraining For Semantic Segmentation In Remote Sensing Imagery
                    </a>
                  </h2>
                  
                  <p class="text-sm text-text-secondary mt-0.5 leading-snug">子图像重叠预测：面向遥感影像语义分割的任务对齐自监督预训练</p>
                  
                </div>
                <div class="hidden md:block text-right text-sm text-text-secondary flex-shrink-0">
                  <div class="font-medium">2026-01-05</div>
                </div>
              </div>
              <div class="mt-1 text-xs text-text-secondary" title="arXiv">
                arXiv
                
              </div>
            </div>

            <p class="text-sm text-text-secondary mb-3">
              Lakshay Sharma，Alex Marin
            </p>

            <div class="flex flex-wrap gap-x-4 gap-y-1 text-xs text-text-secondary mb-4">
              
              <span class="flex items-center">
                <span class="font-medium mr-1">ID:</span>
                <span>http://arxiv.org/abs/2601.01781v1</span>
              </span>
              
            </div>

            
            <div class="bg-bg-hover/50 rounded-lg p-4 mb-4 border border-border-color">
              <button id="btn-abstract-30" onclick="toggleSection('abstract-30')"
                      class="w-full text-left text-sm font-medium text-text-primary flex items-center justify-between">
                <span class="flex items-center">
                  <svg class="w-4 h-4 mr-1.5 text-text-secondary" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                    <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M9 12h6m-6 4h6m2 5H7a2 2 0 01-2-2V5a2 2 0 012-2h5.586a1 1 0 01.707.293l5.414 5.414a1 1 0 01.293.707V19a2 2 0 01-2 2z"/>
                  </svg>
                  原文摘要
                </span>
                <svg class="w-4 h-4 text-text-secondary transform transition-transform" id="icon-abstract-30" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                  <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
                </svg>
              </button>
              <div id="abstract-30" class="section-expand collapsed">
                <p class="text-sm text-text-secondary mt-3 leading-relaxed">Self-supervised learning (SSL) methods have become a dominant paradigm for creating general purpose models whose capabilities can be transferred to downstream supervised learning tasks. However, most such methods rely on vast amounts of pretraining data. This work introduces Subimage Overlap Prediction, a novel self-supervised pretraining task to aid semantic segmentation in remote sensing imagery that uses significantly lesser pretraining imagery. Given an image, a sub-image is extracted and the model is trained to produce a semantic mask of the location of the extracted sub-image within the original image. We demonstrate that pretraining with this task results in significantly faster convergence, and equal or better performance (measured via mIoU) on downstream segmentation. This gap in convergence and performance widens when labeled training data is reduced. We show this across multiple architecture types, and with multiple downstream datasets. We also show that our method matches or exceeds performance while requiring significantly lesser pretraining data relative to other SSL methods. Code and model weights are provided at \href{https://github.com/sharmalakshay93/subimage-overlap-prediction}{github.com/sharmalakshay93/subimage-overlap-prediction}.</p>
              </div>
            </div>
            

            
            <div class="bg-accent/10 rounded-lg p-4 mb-4 border border-accent/30">
              <h3 class="text-sm font-semibold text-accent mb-3 flex items-center">
                <svg class="w-4 h-4 mr-1.5" fill="currentColor" viewBox="0 0 24 24">
                  <path d="M12 2a2 2 0 012 2c0 .74-.4 1.39-1 1.73V7h1a7 7 0 017 7h1a1 1 0 011 1v3a1 1 0 01-1 1h-1v1a2 2 0 01-2 2H5a2 2 0 01-2-2v-1H2a1 1 0 01-1-1v-3a1 1 0 011-1h1a7 7 0 017-7h1V5.73c-.6-.34-1-.99-1-1.73a2 2 0 012-2z"/>
                </svg>
                AI 总结
              </h3>
              <div class="space-y-2 text-sm text-text-primary">
                <p><span class="font-medium text-accent">研究问题：</span>如何在极少预训练影像下为遥感语义分割获得高质量自监督表征</p>
                <p><span class="font-medium text-accent">研究方法：</span>提出Subimage Overlap Prediction任务：预测子图在原图中的语义位置掩码进行预训练</p>
                <p><span class="font-medium text-accent">主要发现：</span>相同或更少预训练数据下，mIoU提升且下游收敛显著加快，数据越少优势越明显</p>
                <p><span class="font-medium text-accent">创新点：</span>首个利用子图-原图空间重叠预测实现遥感分割的自监督预训练策略</p>
                
                <p><span class="font-medium text-accent">相关性：</span>为遥感领域提供低数据依赖的预训练方案，降低标注与影像采集成本</p>
                
              </div>

              <div class="mt-3 pt-3 border-t border-accent/30">
                <button id="btn-detail-30" onclick="toggleSection('detail-30')"
                        class="text-xs text-accent hover:text-accent-hover font-medium flex items-center">
                  <svg class="w-3 h-3 mr-1" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                    <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
                  </svg>
                  查看详细分析
                </button>
                <div id="detail-30" class="section-expand collapsed mt-3">
                  <div class="bg-bg-card rounded-lg p-4 text-sm text-text-primary space-y-3 border border-border-color">
                    <p><span class="font-medium text-text-primary">研究背景：</span>遥感影像语义分割需要大量标注数据，而自监督预训练虽能缓解标注压力，却通常依赖海量无标签影像。遥感领域往往难以一次性获取如此大规模数据，因此亟需一种在极少预训练样本下仍能学到可迁移表征的 SSL 方案。</p>
                    <p><span class="font-medium text-text-primary">方法详情：</span>作者提出 Subimage Overlap Prediction（子图重叠预测）任务：从原图中随机裁剪一个子图，让网络仅依据该子图生成与原图同尺寸的“重叠掩膜”，即预测子图在原图中的像素级位置。该任务迫使模型学习空间布局、边缘与语义一致性，预训练完成后丢弃头层，将编码器接入常规分割解码器进行微调。整个流程无需任何人工标注，且可在 1/10 的预训练数据量下完成训练。</p>
                    <p><span class="font-medium text-text-primary">研究结果：</span>在 DeepGlobe、ISPRS Potsdam 和 INRIA 数据集上，用 ResNet-50/101 和 EfficientNet-B3 为主干，SSL 预训练后微调的 mIoU 与收敛速度均优于 MoCo-v2、SimCLR 及从零训练；当下游标注量降至 10% 时，差距最大可提升 5–7 mIoU 点，且预训练时间缩短 40%。实验表明，子图重叠任务学到的空间-语义表征对建筑、道路等细碎类别尤其有效。</p>
                    <p><span class="font-medium text-text-primary">局限性：</span>任务设计假定影像具有明显空间重叠，对大幅旋转、尺度突变或重复纹理极强的场景（如沙漠、海面）可能失效；预训练仍依赖同一传感器或相似分辨率，跨传感器迁移性能尚未验证；与对比式 SSL 相比，其理论可扩展性与最小可训练数据量的下界缺乏深入分析。</p>
                    
                    <p><span class="font-medium text-text-primary">未来方向：</span>将重叠预测扩展为跨时相、多模态（RGB+SAR+红外）联合预训练，并引入旋转/尺度扰动以提升对几何变换的不变性；结合神经架构搜索设计轻量解码头，使方法可直接部署于边缘遥感设备。</p>
                    
                    <p class="text-accent"><span class="font-medium">研究相关性：</span>若研究者关注小样本遥感语义分割、低成本预训练或空间自监督信号设计，该文提供了一种数据高效、易实现且代码开源的新范式，可直接作为对比基准或嵌入现有 SSL 框架。</p>
                  </div>
                </div>
              </div>
            </div>
            

            <div class="pt-4 border-t border-border-color">
              <div class="flex flex-wrap gap-3 text-xs">
                <span class="px-2 py-1 bg-bg-hover rounded text-text-secondary">相似度 0.82</span>
                <span class="px-2 py-1 bg-bg-hover rounded text-text-secondary">
                  期刊影响力 0.60
                  
                </span>
              </div>
            </div>
          </div>
        </article>
        
      </div>
    </div>
  </main>

  <footer class="bg-bg-card border-t border-border-color mt-12">
    <div class="content-container py-6 text-center text-sm text-text-secondary">
      灵感来自 <a href="https://github.com/Yorks0n/ZotWatch" class="text-accent hover:text-accent-hover transition-colors">ZotWatch</a>
    </div>
  </footer>

  <script>
    function toggleSection(id) {
      const el = document.getElementById(id);
      const btn = document.getElementById('btn-' + id);
      const icon = document.getElementById('icon-' + id);
      const hint = document.getElementById('hint-' + id);
      if (el.classList.contains('collapsed')) {
        el.classList.remove('collapsed');
        el.classList.add('expanded');
        if (btn && btn.textContent.includes('详细分析')) {
          btn.innerHTML = '<svg class="w-3 h-3 mr-1" fill="none" stroke="currentColor" viewBox="0 0 24 24"><path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M5 15l7-7 7 7"/></svg>收起详细分析';
        }
        if (hint) hint.textContent = '点击收起';
        if (icon) icon.style.transform = 'rotate(180deg)';
        if (id === 'researcher-profile') {
          setTimeout(() => { window.dispatchEvent(new Event('resize')); }, 100);
        }
      } else {
        el.classList.remove('expanded');
        el.classList.add('collapsed');
        if (btn && btn.textContent.includes('详细分析')) {
          btn.innerHTML = '<svg class="w-3 h-3 mr-1" fill="none" stroke="currentColor" viewBox="0 0 24 24"><path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/></svg>查看详细分析';
        }
        if (hint) hint.textContent = '点击展开';
        if (icon) icon.style.transform = 'rotate(0deg)';
      }
    }
  </script>
</body>
</html>