<!DOCTYPE html>
<html lang="zh-CN">
<head>
  <meta charset="utf-8">
  <meta name="viewport" content="width=device-width, initial-scale=1">
  <title>ZotWatch 文献推荐 - 2025-12-02</title>
  <script src="https://cdn.tailwindcss.com"></script>
  <script src="https://cdn.jsdelivr.net/npm/chart.js@4.4.1/dist/chart.umd.min.js"></script>
  <script>
    tailwind.config = {
      theme: {
        extend: {
          colors: {
            'bg-primary': '#f8fafc',
            'bg-card': '#ffffff',
            'bg-hover': '#f1f5f9',
            'text-primary': '#1e293b',
            'text-secondary': '#64748b',
            'border-color': '#e2e8f0',
            'accent': '#2563eb',
            'accent-hover': '#1d4ed8',
          }
        }
      }
    }
  </script>
  <style>
    .section-expand { transition: max-height 0.5s ease-out; overflow: hidden; }
    .section-expand.collapsed { max-height: 0; }
    .section-expand.expanded { max-height: none; }
    body { font-family: -apple-system, BlinkMacSystemFont, 'Segoe UI', Roboto, 'Helvetica Neue', Arial, sans-serif; }
    .content-container { max-width: 48rem; margin: 0 auto; padding: 0 1rem; }
    @media (min-width: 768px) { .content-container { padding: 0 2rem; } }
    @media (min-width: 1024px) { .content-container { padding: 0 4rem; } }
  </style>
</head>
<body class="bg-bg-primary min-h-screen text-text-primary">
  <!-- Header -->
  <header class="bg-bg-card border-b border-border-color">
    <div class="content-container py-6 md:py-8">
      <h1 class="text-xl md:text-2xl font-bold text-text-primary">ZotWatch 文献推荐</h1>
      <p class="text-xs md:text-sm text-text-secondary mt-1">
        共 35 篇论文 ·
        生成于 2025-12-02 10:47 Asia/Shanghai
      </p>
    </div>
  </header>

  <!-- Researcher Profile Section (Collapsible) -->
  
  <section class="py-4 md:py-6 border-b border-border-color">
    <div class="content-container">
      <!-- Collapsible Header -->
      <button id="btn-researcher-profile" onclick="toggleSection('researcher-profile')"
              class="w-full text-left text-lg md:text-xl font-bold text-text-primary mb-3 md:mb-4 flex items-center justify-between group hover:text-accent transition-colors">
        <span class="flex items-center gap-2 flex-1 min-w-0">
          <svg class="w-5 h-5 flex-shrink-0 text-accent" fill="currentColor" viewBox="0 0 24 24">
            <path d="M12 2C6.48 2 2 6.48 2 12s4.48 10 10 10 10-4.48 10-10S17.52 2 12 2zm0 3c1.66 0 3 1.34 3 3s-1.34 3-3 3-3-1.34-3-3 1.34-3 3-3zm0 14.2c-2.5 0-4.71-1.28-6-3.22.03-1.99 4-3.08 6-3.08 1.99 0 5.97 1.09 6 3.08-1.29 1.94-3.5 3.22-6 3.22z"/>
          </svg>
          <span class="truncate">研究兴趣画像</span>
        </span>
        <span class="flex items-center gap-1 md:gap-2 flex-shrink-0 ml-2">
          <span class="hidden md:inline text-sm font-normal text-text-secondary group-hover:text-accent" id="hint-researcher-profile">点击展开</span>
          <svg class="w-5 h-5 md:w-4 md:h-4 text-text-secondary group-hover:text-accent transform transition-transform" id="icon-researcher-profile" fill="none" stroke="currentColor" viewBox="0 0 24 24">
            <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
          </svg>
        </span>
      </button>

      <!-- Collapsible Content -->
      <div id="researcher-profile" class="section-expand collapsed">
      <!-- Basic Stats - Compact Row -->
      <div class="bg-bg-card rounded-lg border border-border-color p-4 mb-4">
        <div class="grid grid-cols-4 gap-3 text-center">
          <div>
            <div class="text-xl font-bold text-accent">2651</div>
            <div class="text-xs text-text-secondary">收藏论文</div>
          </div>
          <div>
            <div class="text-xl font-bold text-accent">5年8月</div>
            <div class="text-xs text-text-secondary">收藏时长</div>
          </div>
          <div>
            <div class="text-xl font-bold text-accent">&gt;10</div>
            <div class="text-xs text-text-secondary">兴趣领域</div>
          </div>
          <div>
            <div class="text-xl font-bold text-accent">6</div>
            <div class="text-xs text-text-secondary">高频作者</div>
          </div>
        </div>
      </div>

      <!-- LLM Insights - Compact -->
      
      <div class="bg-accent/10 rounded-lg border border-accent/30 p-4 mb-4">
        <h3 class="text-sm font-semibold text-accent mb-2 flex items-center gap-2">
          <svg class="w-4 h-4" fill="currentColor" viewBox="0 0 24 24">
            <path d="M12 2a2 2 0 012 2c0 .74-.4 1.39-1 1.73V7h1a7 7 0 017 7h1a1 1 0 011 1v3a1 1 0 01-1 1h-1v1a2 2 0 01-2 2H5a2 2 0 01-2-2v-1H2a1 1 0 01-1-1v-3a1 1 0 011-1h1a7 7 0 017-7h1V5.73c-.6-.34-1-.99-1-1.73a2 2 0 012-2z"/>
          </svg>
          AI 研究兴趣分析
        </h3>
        <div class="space-y-2 text-xs text-text-primary leading-relaxed">
          <p><span class="font-medium text-accent">研究兴趣：</span>用户长期关注计算机视觉与遥感交叉方向，尤其聚焦目标检测、模型压缩及自监督学习，同时跟踪大模型与SLAM等前沿主题。</p>
          <p><span class="font-medium text-accent">深度关注：</span>在CVPR、NeurIPS、IEEE TGRS 等顶会顶刊持续收藏逾百篇论文，形成对视觉目标检测、遥感影像解译及高效模型设计的系统阅读积累；高频作者 Kaiming He、Ross Girshick 等的持续出现显示对检测/分割主干网络演进的深度追踪。</p>
          <p><span class="font-medium text-accent">跨学科倾向：</span>阅读横跨计算机视觉、遥感、雷达信号处理与机器学习，呈现以视觉智能为核心、向遥感观测和雷达数据延伸的交叉特征。</p>
          <p><span class="font-medium text-accent">兴趣演变：</span>2025-Q1 收藏量激增至 77 篇并新增 Foundation models、Depth measurement、Grasping 等关键词，显示兴趣正向视觉基础模型、三维感知及机器人抓取快速拓展；季度波动大，可能随项目需求或热点事件呈脉冲式阅读。</p>
          <p><span class="font-medium text-accent">阅读建议：</span>可进一步关注多模态大模型在遥感-视觉融合中的低层次对齐、以及基于 NeRF/3D-GS 的场景级 SLAM，以延续检测-压缩-三维感知的技术链条。</p>
        </div>
      </div>
      

      <!-- Charts Row: Domains + Trends -->
      <div class="grid grid-cols-1 md:grid-cols-2 gap-4 mb-4">
        <!-- Research Domains Chart -->
        
        <div class="bg-bg-card rounded-lg border border-border-color p-4">
          <h4 class="text-sm font-medium text-text-primary mb-3">研究领域分布</h4>
          <div style="height: 180px;">
            <canvas id="domainsChart"></canvas>
          </div>
        </div>
        

        <!-- Quarterly Trends Chart -->
        
        <div class="bg-bg-card rounded-lg border border-border-color p-4">
          <h4 class="text-sm font-medium text-text-primary mb-3">阅读趋势（季度）</h4>
          <div style="height: 180px;">
            <canvas id="trendsChart"></canvas>
          </div>
        </div>
        
      </div>

      <!-- Year Distribution Chart (full width) -->
      
      <div class="bg-bg-card rounded-lg border border-border-color p-4 mb-4">
        <h4 class="text-sm font-medium text-text-primary mb-3">论文发表年份分布</h4>
        <div style="height: 180px;">
          <canvas id="yearDistChart"></canvas>
        </div>
      </div>
      

      <!-- Authors and Venues Row -->
      <div class="grid grid-cols-1 md:grid-cols-2 gap-4 mb-4">
        <!-- Top Authors -->
        
        <div class="bg-bg-card rounded-lg border border-border-color p-4">
          <h4 class="text-sm font-medium text-text-primary mb-2">高频关注作者</h4>
          <div class="grid grid-cols-2 gap-x-4 gap-y-1 text-xs">
            
            <div class="flex justify-between truncate">
              <span class="text-text-primary truncate" title="Kaiming He">Kaiming He</span>
              <span class="text-text-secondary ml-1 flex-shrink-0">23</span>
            </div>
            
            <div class="flex justify-between truncate">
              <span class="text-text-primary truncate" title="Ross Girshick">Ross Girshick</span>
              <span class="text-text-secondary ml-1 flex-shrink-0">18</span>
            </div>
            
            <div class="flex justify-between truncate">
              <span class="text-text-primary truncate" title="Jian Sun">Jian Sun</span>
              <span class="text-text-secondary ml-1 flex-shrink-0">15</span>
            </div>
            
            <div class="flex justify-between truncate">
              <span class="text-text-primary truncate" title="Song Han">Song Han</span>
              <span class="text-text-secondary ml-1 flex-shrink-0">14</span>
            </div>
            
            <div class="flex justify-between truncate">
              <span class="text-text-primary truncate" title="Xiangyu Zhang">Xiangyu Zhang</span>
              <span class="text-text-secondary ml-1 flex-shrink-0">14</span>
            </div>
            
            <div class="flex justify-between truncate">
              <span class="text-text-primary truncate" title="Piotr Dollár">Piotr Dollár</span>
              <span class="text-text-secondary ml-1 flex-shrink-0">11</span>
            </div>
            
            <div class="flex justify-between truncate">
              <span class="text-text-primary truncate" title="Li Liu">Li Liu</span>
              <span class="text-text-secondary ml-1 flex-shrink-0">9</span>
            </div>
            
            <div class="flex justify-between truncate">
              <span class="text-text-primary truncate" title="Xiang Li">Xiang Li</span>
              <span class="text-text-secondary ml-1 flex-shrink-0">9</span>
            </div>
            
            <div class="flex justify-between truncate">
              <span class="text-text-primary truncate" title="Chunhua Shen">Chunhua Shen</span>
              <span class="text-text-secondary ml-1 flex-shrink-0">8</span>
            </div>
            
            <div class="flex justify-between truncate">
              <span class="text-text-primary truncate" title="Yann LeCun">Yann LeCun</span>
              <span class="text-text-secondary ml-1 flex-shrink-0">8</span>
            </div>
            
            <div class="flex justify-between truncate">
              <span class="text-text-primary truncate" title="Gangyao Kuang">Gangyao Kuang</span>
              <span class="text-text-secondary ml-1 flex-shrink-0">8</span>
            </div>
            
            <div class="flex justify-between truncate">
              <span class="text-text-primary truncate" title="Wenfeng Liang">Wenfeng Liang</span>
              <span class="text-text-secondary ml-1 flex-shrink-0">8</span>
            </div>
            
          </div>
        </div>
        

        <!-- Top Venues -->
        
        <div class="bg-bg-card rounded-lg border border-border-color p-4">
          <h4 class="text-sm font-medium text-text-primary mb-2">常关注期刊/会议</h4>
          <div class="space-y-1 text-xs">
            
            <div class="flex items-center justify-between">
              <span class="text-text-primary truncate flex-1" title="Computer Vision and Pattern Recognition">Computer Vision and Pattern Recognition</span>
              <span class="text-xs px-1 py-0.5 rounded bg-blue-100 text-blue-700 ml-1 flex-shrink-0">
                期刊
              </span>
              <span class="text-text-secondary ml-1 flex-shrink-0">111</span>
            </div>
            
            <div class="flex items-center justify-between">
              <span class="text-text-primary truncate flex-1" title="Neural Information Processing Systems">Neural Information Processing Systems</span>
              <span class="text-xs px-1 py-0.5 rounded bg-blue-100 text-blue-700 ml-1 flex-shrink-0">
                期刊
              </span>
              <span class="text-text-secondary ml-1 flex-shrink-0">44</span>
            </div>
            
            <div class="flex items-center justify-between">
              <span class="text-text-primary truncate flex-1" title="IEEE Transactions on Geoscience and Remote Sensing">IEEE Transactions on Geoscience and Remote Sensing</span>
              <span class="text-xs px-1 py-0.5 rounded bg-blue-100 text-blue-700 ml-1 flex-shrink-0">
                期刊
              </span>
              <span class="text-text-secondary ml-1 flex-shrink-0">40</span>
            </div>
            
            <div class="flex items-center justify-between">
              <span class="text-text-primary truncate flex-1" title="International Conference on Computer Vision">International Conference on Computer Vision</span>
              <span class="text-xs px-1 py-0.5 rounded bg-amber-100 text-amber-700 ml-1 flex-shrink-0">
                会议
              </span>
              <span class="text-text-secondary ml-1 flex-shrink-0">35</span>
            </div>
            
            <div class="flex items-center justify-between">
              <span class="text-text-primary truncate flex-1" title="IEEE Transactions on Pattern Analysis and Machine Intelligence">IEEE Transactions on Pattern Analysis and Machine Intelligence</span>
              <span class="text-xs px-1 py-0.5 rounded bg-blue-100 text-blue-700 ml-1 flex-shrink-0">
                期刊
              </span>
              <span class="text-text-secondary ml-1 flex-shrink-0">31</span>
            </div>
            
            <div class="flex items-center justify-between">
              <span class="text-text-primary truncate flex-1" title="International Conference on Learning Representations">International Conference on Learning Representations</span>
              <span class="text-xs px-1 py-0.5 rounded bg-amber-100 text-amber-700 ml-1 flex-shrink-0">
                会议
              </span>
              <span class="text-text-secondary ml-1 flex-shrink-0">27</span>
            </div>
            
            <div class="flex items-center justify-between">
              <span class="text-text-primary truncate flex-1" title="European Conference on Computer Vision">European Conference on Computer Vision</span>
              <span class="text-xs px-1 py-0.5 rounded bg-amber-100 text-amber-700 ml-1 flex-shrink-0">
                会议
              </span>
              <span class="text-text-secondary ml-1 flex-shrink-0">23</span>
            </div>
            
            <div class="flex items-center justify-between">
              <span class="text-text-primary truncate flex-1" title="雷达学报">雷达学报</span>
              <span class="text-xs px-1 py-0.5 rounded bg-blue-100 text-blue-700 ml-1 flex-shrink-0">
                期刊
              </span>
              <span class="text-text-secondary ml-1 flex-shrink-0">21</span>
            </div>
            
          </div>
        </div>
        
      </div>

      <!-- Keywords -->
      
      <div class="bg-bg-card rounded-lg border border-border-color p-4 mb-3">
        <h4 class="text-sm font-medium text-text-primary mb-2">高频关键词</h4>
        <div class="flex flex-wrap gap-1.5">
          
          <span class="px-2 py-0.5 bg-bg-hover rounded text-xs text-text-primary">
            大语言模型 <span class="text-text-secondary">(12)</span>
          </span>
          
          <span class="px-2 py-0.5 bg-bg-hover rounded text-xs text-text-primary">
            综述 <span class="text-text-secondary">(9)</span>
          </span>
          
          <span class="px-2 py-0.5 bg-bg-hover rounded text-xs text-text-primary">
            自监督学习 <span class="text-text-secondary">(9)</span>
          </span>
          
          <span class="px-2 py-0.5 bg-bg-hover rounded text-xs text-text-primary">
            域自适应 <span class="text-text-secondary">(9)</span>
          </span>
          
          <span class="px-2 py-0.5 bg-bg-hover rounded text-xs text-text-primary">
            迁移学习 <span class="text-text-secondary">(7)</span>
          </span>
          
          <span class="px-2 py-0.5 bg-bg-hover rounded text-xs text-text-primary">
            重参数化 <span class="text-text-secondary">(6)</span>
          </span>
          
          <span class="px-2 py-0.5 bg-bg-hover rounded text-xs text-text-primary">
            扩散模型 <span class="text-text-secondary">(6)</span>
          </span>
          
          <span class="px-2 py-0.5 bg-bg-hover rounded text-xs text-text-primary">
            强化学习 <span class="text-text-secondary">(6)</span>
          </span>
          
          <span class="px-2 py-0.5 bg-bg-hover rounded text-xs text-text-primary">
            DeepSeek <span class="text-text-secondary">(6)</span>
          </span>
          
          <span class="px-2 py-0.5 bg-bg-hover rounded text-xs text-text-primary">
            合成孔径雷达 <span class="text-text-secondary">(6)</span>
          </span>
          
          <span class="px-2 py-0.5 bg-bg-hover rounded text-xs text-text-primary">
            Transformers <span class="text-text-secondary">(5)</span>
          </span>
          
          <span class="px-2 py-0.5 bg-bg-hover rounded text-xs text-text-primary">
            旋转目标检测 <span class="text-text-secondary">(5)</span>
          </span>
          
          <span class="px-2 py-0.5 bg-bg-hover rounded text-xs text-text-primary">
            SAR目标识别 <span class="text-text-secondary">(5)</span>
          </span>
          
          <span class="px-2 py-0.5 bg-bg-hover rounded text-xs text-text-primary">
            特征可视化 <span class="text-text-secondary">(5)</span>
          </span>
          
          <span class="px-2 py-0.5 bg-bg-hover rounded text-xs text-text-primary">
            生成对抗网络 <span class="text-text-secondary">(5)</span>
          </span>
          
          <span class="px-2 py-0.5 bg-bg-hover rounded text-xs text-text-primary">
            Vision Transformers <span class="text-text-secondary">(4)</span>
          </span>
          
          <span class="px-2 py-0.5 bg-bg-hover rounded text-xs text-text-primary">
            基础模型 <span class="text-text-secondary">(4)</span>
          </span>
          
          <span class="px-2 py-0.5 bg-bg-hover rounded text-xs text-text-primary">
            HRNet <span class="text-text-secondary">(4)</span>
          </span>
          
          <span class="px-2 py-0.5 bg-bg-hover rounded text-xs text-text-primary">
            目标检测 <span class="text-text-secondary">(4)</span>
          </span>
          
          <span class="px-2 py-0.5 bg-bg-hover rounded text-xs text-text-primary">
            模型压缩 <span class="text-text-secondary">(4)</span>
          </span>
          
        </div>
      </div>
      

      <!-- Generation info -->
      <div class="text-xs text-text-secondary text-right">
        画像生成于 2025-12-02 10:23 Asia/Shanghai
        · kimi-k2-turbo-preview
      </div>
      </div><!-- End of collapsible content -->
    </div>
  </section>
  

  <!-- Chart.js Initialization for Researcher Profile -->
  
  <script>
    document.addEventListener('DOMContentLoaded', function() {
      // Chart.js global defaults for academic style
      Chart.defaults.font.family = "-apple-system, BlinkMacSystemFont, 'Segoe UI', Roboto, 'Helvetica Neue', Arial, sans-serif";
      Chart.defaults.font.size = 11;
      Chart.defaults.color = '#64748b';
      Chart.defaults.animation = false;

      
      // Domains Horizontal Bar Chart
      const domainsCtx = document.getElementById('domainsChart');
      if (domainsCtx) {
        new Chart(domainsCtx, {
          type: 'bar',
          data: {
            labels: ['视觉SLAM', '卫星导航', '人体姿态', '模型压缩', '图模型', '相机标定', '特征提取', '优化算法'],
            datasets: [{
              data: [18, 11, 21, 12, 4, 4, 4, 5],
              backgroundColor: 'rgba(37, 99, 235, 0.7)',
              borderColor: 'rgba(37, 99, 235, 1)',
              borderWidth: 1,
              borderRadius: 2,
              barThickness: 16
            }]
          },
          options: {
            indexAxis: 'y',
            responsive: true,
            maintainAspectRatio: false,
            plugins: {
              legend: { display: false },
              tooltip: { enabled: false }
            },
            scales: {
              x: {
                beginAtZero: true,
                grid: { color: 'rgba(226, 232, 240, 0.5)' },
                ticks: { precision: 0 }
              },
              y: {
                grid: { display: false },
                ticks: { font: { size: 10 } }
              }
            }
          }
        });
      }
      

      
      // Quarterly Trends Bar Chart with 3 cycling colors (one per month in quarter)
      const trendsCtx = document.getElementById('trendsChart');
      if (trendsCtx) {
        const trends = [{ q: '2023-Q1', c: 50 }, { q: '2023-Q2', c: 18 }, { q: '2023-Q3', c: 21 }, { q: '2023-Q4', c: 19 }, { q: '2024-Q1', c: 66 }, { q: '2024-Q2', c: 18 }, { q: '2024-Q3', c: 4 }, { q: '2024-Q4', c: 23 }, { q: '2025-Q1', c: 77 }, { q: '2025-Q2', c: 34 }, { q: '2025-Q3', c: 8 }, { q: '2025-Q4', c: 16 }];
        // Color palette: 3 colors cycling for months within quarters
        const monthColors = [
          { bg: 'rgba(59, 130, 246, 0.7)', border: 'rgba(59, 130, 246, 1)' },   // Blue
          { bg: 'rgba(16, 185, 129, 0.7)', border: 'rgba(16, 185, 129, 1)' },   // Green
          { bg: 'rgba(245, 158, 11, 0.7)', border: 'rgba(245, 158, 11, 1)' }    // Amber
        ];
        const bgColors = trends.map((t, i) => monthColors[i % 3].bg);
        const borderColors = trends.map((t, i) => monthColors[i % 3].border);
        new Chart(trendsCtx, {
          type: 'bar',
          data: {
            labels: trends.map(t => t.q.replace('-', '\n')),
            datasets: [{
              data: trends.map(t => t.c),
              backgroundColor: bgColors,
              borderColor: borderColors,
              borderWidth: 1,
              borderRadius: 2,
              barThickness: 20
            }]
          },
          options: {
            responsive: true,
            maintainAspectRatio: false,
            plugins: {
              legend: { display: false },
              tooltip: { enabled: false }
            },
            scales: {
              x: {
                grid: { display: false },
                ticks: { font: { size: 9 }, maxRotation: 0 }
              },
              y: {
                beginAtZero: true,
                grid: { color: 'rgba(226, 232, 240, 0.5)' },
                ticks: { precision: 0 }
              }
            }
          }
        });
      }
      

      
      // Year Distribution Line Chart
      const yearDistCtx = document.getElementById('yearDistChart');
      if (yearDistCtx) {
        const yearData = [{ year: 2006, count: 8 }, { year: 2007, count: 3 }, { year: 2008, count: 2 }, { year: 2009, count: 2 }, { year: 2010, count: 9 }, { year: 2011, count: 10 }, { year: 2012, count: 3 }, { year: 2013, count: 12 }, { year: 2014, count: 14 }, { year: 2015, count: 17 }, { year: 2016, count: 15 }, { year: 2017, count: 39 }, { year: 2018, count: 57 }, { year: 2019, count: 53 }, { year: 2020, count: 66 }, { year: 2021, count: 84 }, { year: 2022, count: 110 }, { year: 2023, count: 108 }, { year: 2024, count: 111 }, { year: 2025, count: 135 }];
        new Chart(yearDistCtx, {
          type: 'line',
          data: {
            labels: yearData.map(d => d.year),
            datasets: [{
              data: yearData.map(d => d.count),
              borderColor: 'rgba(37, 99, 235, 1)',
              backgroundColor: 'rgba(37, 99, 235, 0.1)',
              borderWidth: 2,
              fill: true,
              tension: 0.3,
              pointRadius: 2,
              pointHoverRadius: 4
            }]
          },
          options: {
            responsive: true,
            maintainAspectRatio: false,
            plugins: {
              legend: { display: false },
              tooltip: {
                callbacks: {
                  label: function(context) {
                    return context.parsed.y + ' 篇';
                  }
                }
              }
            },
            scales: {
              x: {
                grid: { display: false },
                ticks: {
                  font: { size: 9 },
                  maxRotation: 45,
                  callback: function(value, index) {
                    // Show every 5th year label to avoid crowding
                    const year = this.getLabelForValue(value);
                    return year % 5 === 0 ? year : '';
                  }
                }
              },
              y: {
                beginAtZero: true,
                grid: { color: 'rgba(226, 232, 240, 0.5)' },
                ticks: { precision: 0 }
              }
            }
          }
        });
      }
      
    });
  </script>
  

  <!-- Overall Summaries Section -->
  
  <section class="py-5 md:py-8 border-b border-border-color">
    <div class="content-container">
      <!-- Collapsible Header -->
      <button id="btn-overall-summaries" onclick="toggleSection('overall-summaries')"
              class="w-full text-left text-lg md:text-xl font-bold text-text-primary mb-3 md:mb-4 flex items-center justify-between group hover:text-accent transition-colors">
        <span class="flex items-center gap-2 flex-1 min-w-0">
          <svg class="w-5 h-5 flex-shrink-0 text-accent" fill="currentColor" viewBox="0 0 24 24">
            <path d="M12 2a2 2 0 012 2c0 .74-.4 1.39-1 1.73V7h1a7 7 0 017 7h1a1 1 0 011 1v3a1 1 0 01-1 1h-1v1a2 2 0 01-2 2H5a2 2 0 01-2-2v-1H2a1 1 0 01-1-1v-3a1 1 0 011-1h1a7 7 0 017-7h1V5.73c-.6-.34-1-.99-1-1.73a2 2 0 012-2z"/>
          </svg>
          <span class="truncate">本期研究趋势概览</span>
        </span>
        <span class="flex items-center gap-1 md:gap-2 flex-shrink-0 ml-2">
          <span class="hidden md:inline text-sm font-normal text-text-secondary group-hover:text-accent" id="hint-overall-summaries">点击收起</span>
          <svg class="w-5 h-5 md:w-4 md:h-4 text-text-secondary group-hover:text-accent transform transition-transform rotate-180" id="icon-overall-summaries" fill="none" stroke="currentColor" viewBox="0 0 24 24">
            <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
          </svg>
        </span>
      </button>

      <!-- Collapsible Content -->
      <div id="overall-summaries" class="section-expand expanded">
      <div class="space-y-4 md:space-y-6">
        
        <div class="bg-bg-card rounded-lg border border-border-color p-4 md:p-5">
          <h3 class="text-base font-semibold text-text-primary mb-3 flex items-center gap-2">
            <svg class="w-4 h-4 text-accent" fill="currentColor" viewBox="0 0 20 20">
              <path d="M9.049 2.927c.3-.921 1.603-.921 1.902 0l1.07 3.292a1 1 0 00.95.69h3.462c.969 0 1.371 1.24.588 1.81l-2.8 2.034a1 1 0 00-.364 1.118l1.07 3.292c.3.921-.755 1.688-1.54 1.118l-2.8-2.034a1 1 0 00-1.175 0l-2.8 2.034c-.784.57-1.838-.197-1.539-1.118l1.07-3.292a1 1 0 00-.364-1.118L2.98 8.72c-.783-.57-.38-1.81.588-1.81h3.461a1 1 0 00.951-.69l1.07-3.292z"/>
            </svg>
            精选推荐总结
          </h3>
          <div class="text-sm text-text-primary leading-relaxed space-y-2">
            <p>本期推荐涵盖了2篇关于多模态检测的论文、1篇关于SAR舰船检测的论文、1篇关于旋转不变性检测的论文与1篇关于多模态分割的论文。</p>
            
            <p><strong class="text-accent">多模态检测</strong>：《MM-DETR》提出Mamba双粒度融合与频域适配器，在可见光-红外/多光谱图像中实现高效目标检测；《Illumination-aware Multimodal Hierarchical Fusion Network》构建光照感知分层融合框架，提升RGB-IR无人机检测的全天候鲁棒性。</p>
            
            <p><strong class="text-accent">SAR舰船检测</strong>：《LSDFormer》通过高效多注意力与结构重参数化，在保持轻量化的同时显著抑制SAR图像强背景干扰，实现实时舰船检测。</p>
            
            <p><strong class="text-accent">旋转不变性</strong>：《Rotation-Invariant Knowledge Distillation》设计旋转不变知识蒸馏策略，缓解遥感小目标特征稀释问题，提升任意方向目标检测精度。</p>
            
            <p><strong class="text-accent">多模态分割</strong>：《Multimodal cross fusion Mamba network》利用跨模态Mamba融合与互补掩码自监督，充分挖掘多源遥感影像互补信息，提高语义分割精度。</p>
            
          </div>
        </div>
        

        
        <div class="bg-bg-card rounded-lg border border-border-color p-4 md:p-5">
          <h3 class="text-base font-semibold text-text-primary mb-3 flex items-center gap-2">
            <svg class="w-4 h-4 text-text-secondary" fill="none" stroke="currentColor" viewBox="0 0 24 24">
              <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M9 19v-6a2 2 0 00-2-2H5a2 2 0 00-2 2v6a2 2 0 002 2h2a2 2 0 002-2zm0 0V9a2 2 0 012-2h2a2 2 0 012 2v10m-6 0a2 2 0 002 2h2a2 2 0 002-2m0 0V5a2 2 0 012-2h2a2 2 0 012 2v14a2 2 0 01-2 2h-2a2 2 0 01-2-2z"/>
            </svg>
            相似度推荐总结
          </h3>
          <div class="text-sm text-text-primary leading-relaxed space-y-2">
            <p>本期推荐涵盖了6篇关于SAR/遥感图像的论文、5篇关于持续学习的论文、4篇关于视觉-语言模型的论文、3篇关于扩散/生成模型的论文、3篇关于大模型推理与评估的论文、3篇关于多模态融合的论文、2篇关于自监督表示的论文、2篇关于检索增强生成的论文、1篇关于CLIP改进的论文、1篇关于图神经网络的论文。</p>
            
            <p><strong class="text-text-secondary">SAR/遥感</strong>：聚焦合成孔径雷达与遥感影像去噪、检测与分割，如《LSDFormer》提出轻量重参数化检测头提升舰船检测效率，《Self-supervised despeckling》仅用强度图实现盲去斑，《Multimodal cross fusion Mamba》用互补掩码自监督融合多模态遥感语义分割。</p>
            
            <p><strong class="text-text-secondary">持续学习</strong>：探讨灾难遗忘抑制与跨域增量学习，《Rethinking Domain-Agnostic Continual Learning》利用频率完整性保持跨域知识，《Neuroscience-Inspired Memory Replay》比较预测编码与反向回放策略在动态环境中的表现。</p>
            
            <p><strong class="text-text-secondary">视觉-语言</strong>：研究图文对齐与零样本迁移，《PowerCLIP》提出powerset对齐扩大CLIP对比预训练粒度，多篇工作围绕细粒度语义和属性控制展开。</p>
            
            <p><strong class="text-text-secondary">扩散生成</strong>：关注文本引导图像生成中的细粒度空间属性控制，《Diffusion-Based Text-Guided Image Generation》显式建模对象-属性空间关系以提升布局精度。</p>
            
            <p><strong class="text-text-secondary">大模型推理</strong>：探索链式思维与树搜索改进，《ReJump》提出树跳跃表示分析并优化长链推理，《Refinement-Guided Critique Learning》训练专用 critique 模型迭代改进LLM输出。</p>
            
            <p><strong class="text-text-secondary">多模态融合</strong>：面向遥感与通用场景设计跨模态交互机制，如《Multimodal cross fusion Mamba》用Mamba结构融合光学与SAR数据提升分割精度。</p>
            
            <p><strong class="text-text-secondary">自监督表示</strong>：利用掩码或对比策略学习无标注特征，多篇工作将互补掩码自监督用于遥感分割和视觉预训练。</p>
            
            <p><strong class="text-text-secondary">检索增强</strong>：改进RAG鲁棒性与知识交互，《Causal Reasoning Meets Heuristic Strategies》通过因果推理微调检索器降低噪声文档影响。</p>
            
            <p><strong class="text-text-secondary">CLIP改进</strong>：《PowerCLIP》以powerset级对齐扩展CLIP对比范围，提升零样本分类与检索。</p>
            
            <p><strong class="text-text-secondary">图神经网络</strong>：《ReJump》把推理过程抽象为树结构图，利用跳跃连接优化大模型链式思维路径。</p>
            
          </div>
        </div>
        
      </div>
      </div><!-- End of collapsible content -->
    </div>
  </section>
  

  <!-- Interest-based Recommendations Section -->
  
  <section class="py-5 md:py-8 border-b border-border-color">
    <div class="content-container">
      <!-- Collapsible Header -->
      <div class="mb-3 md:mb-4">
        <button id="btn-interest-works" onclick="toggleSection('interest-works')"
                class="w-full text-left text-lg md:text-xl font-bold text-text-primary flex items-center justify-between group hover:text-accent transition-colors">
          <span class="flex items-center gap-2 flex-1 min-w-0">
            <svg class="w-5 h-5 flex-shrink-0 text-accent" fill="currentColor" viewBox="0 0 20 20">
              <path d="M9.049 2.927c.3-.921 1.603-.921 1.902 0l1.07 3.292a1 1 0 00.95.69h3.462c.969 0 1.371 1.24.588 1.81l-2.8 2.034a1 1 0 00-.364 1.118l1.07 3.292c.3.921-.755 1.688-1.54 1.118l-2.8-2.034a1 1 0 00-1.175 0l-2.8 2.034c-.784.57-1.838-.197-1.539-1.118l1.07-3.292a1 1 0 00-.364-1.118L2.98 8.72c-.783-.57-.38-1.81.588-1.81h3.461a1 1 0 00.951-.69l1.07-3.292z"/>
            </svg>
            <span class="truncate">精选推荐</span>
          </span>
          <span class="flex items-center gap-1 md:gap-2 flex-shrink-0 ml-2">
            <span class="hidden md:inline text-xs font-normal text-text-secondary">基于研究兴趣匹配，共 5 篇</span>
            <span class="hidden md:inline text-sm font-normal text-text-secondary group-hover:text-accent" id="hint-interest-works">点击收起</span>
            <svg class="w-5 h-5 md:w-4 md:h-4 text-text-secondary group-hover:text-accent transform transition-transform rotate-180" id="icon-interest-works" fill="none" stroke="currentColor" viewBox="0 0 24 24">
              <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
            </svg>
          </span>
        </button>
        <p class="text-xs text-text-secondary mt-1 ml-7 md:hidden">基于研究兴趣匹配，共 5 篇</p>
      </div>

      <!-- Collapsible Content -->
      <div id="interest-works" class="section-expand expanded">
      <div class="space-y-4 md:space-y-5">
        
        <article class="bg-bg-card rounded-lg border border-border-color overflow-hidden hover:border-accent/50 transition-colors">
          <div class="p-4 md:p-5">
            <!-- Title and metadata -->
            <div class="mb-3">
              <!-- Tags row -->
              <div class="flex items-center gap-2 mb-2 flex-wrap">
                <span class="inline-flex items-center px-2.5 py-0.5 rounded text-xs font-medium bg-accent/10 text-accent border border-accent/30">
                  匹配度 67%
                </span>
                <span class="text-xs text-text-secondary">crossref</span>
                <!-- Date: visible on mobile, hidden on md+ -->
                <span class="text-xs text-text-secondary md:hidden">· 2025-12-01</span>
              </div>
              <!-- Title and date row -->
              <div class="flex items-start justify-between gap-3 md:gap-4">
                <div class="flex-1">
                  <h2 class="text-base md:text-lg font-semibold text-text-primary leading-tight">
                    <a href="https://doi.org/10.1109/jstars.2025.3639164" target="_blank" rel="noopener" class="hover:text-accent transition-colors">
                      LSDFormer: Lightweight SAR Ship Detection Enhanced With Efficient Multi-Attention and Structural Reparameterization
                    </a>
                  </h2>
                  
                  <p class="text-sm text-text-secondary mt-0.5 leading-snug">LSDFormer：基于高效多重注意力与结构重参数化的轻量级SAR舰船检测</p>
                  
                </div>
                <!-- Date: hidden on mobile, visible on md+ -->
                <div class="hidden md:block text-right text-sm text-text-secondary flex-shrink-0">
                  <div class="font-medium">2025-12-01</div>
                </div>
              </div>
              <!-- Venue: below title on mobile -->
              <div class="mt-1 text-xs text-text-secondary" title="IEEE Journal of Selected Topics in Applied Earth Observations and Remote Sensing">
                IEEE Journal of Selected Topics in Applied Earth Observations and Remote Sensing
                
                  <span class="ml-1 text-blue-600">(IF: 5.5)</span>
                
              </div>
            </div>

            <!-- Authors -->
            <p class="text-sm text-text-secondary mb-3">
              Rui Jiang，Hang Shi，Jiahong Ni，Jiatao Li，Yi Feng 等
            </p>

            <!-- DOI -->
            <div class="flex flex-wrap gap-x-4 gap-y-1 text-xs text-text-secondary mb-4">
              
              <span class="flex items-center">
                <span class="font-medium mr-1">DOI:</span>
                <a href="https://doi.org/10.1109/jstars.2025.3639164" target="_blank" rel="noopener" class="text-accent hover:text-accent-hover hover:underline">10.1109/jstars.2025.3639164</a>
              </span>
              
            </div>

            <!-- Abstract (collapsible) -->
            
            <div class="bg-bg-hover/50 rounded-lg p-4 mb-4 border border-border-color">
              <button id="btn-interest-abstract-1" onclick="toggleSection('interest-abstract-1')"
                      class="w-full text-left text-sm font-medium text-text-primary flex items-center justify-between">
                <span class="flex items-center">
                  <svg class="w-4 h-4 mr-1.5 text-text-secondary" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                    <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M9 12h6m-6 4h6m2 5H7a2 2 0 01-2-2V5a2 2 0 012-2h5.586a1 1 0 01.707.293l5.414 5.414a1 1 0 01.293.707V19a2 2 0 01-2 2z"/>
                  </svg>
                  原文摘要
                </span>
                <svg class="w-4 h-4 text-text-secondary transform transition-transform" id="icon-interest-abstract-1" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                  <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
                </svg>
              </button>
              <div id="interest-abstract-1" class="section-expand collapsed">
                <p class="text-sm text-text-secondary mt-3 leading-relaxed">Ship detection in Synthetic Aperture Radar (SAR) images faces challenges such as strong background interference, varying ship appearance and distribution and high real-time requirements. Although attention-based deep learning methods dominate this field, the design of lightweight models with efficient attention mechanisms capable of addressing the above challenges remains underexplored. To address this issue, we propose a lightweight SAR ship detection model named LSDFormer, which is built upon the MetaFormer architecture and consists of an efficient multi-attention enhanced backbone and neck and a structural reparameterization enhanced head. We employ two lightweight modules for the backbone and neck: a PoolFormer-based feature extraction module with efficient channel modulation attention is proposed to enhance ship features and suppress background interference; a downsampling module using efficient channel aggregation attention and group convolutions is introduced to enrich ship features. The position-sensitive attention from YOLOv11 is also introduced to handle variations in ship appearance and distribution. These three attentions are integrated into an efficient multi-attention mechanism. Furthermore, a structural reparameterization based detection branch is proposed for the head of LSDFormer, which enhances ship features while reducing model complexity. Extensive experiments on SSDD and HRSID datasets demonstrate the superiority and effectiveness of LSDFormer, achieving AP50 of 98.5 ± 0.4 % \bf {98.5\pm 0.4\%} and 92.8 ± 0.2 % \bf {92.8\pm 0.2\%} , respectively, with only 1.5 \bf {1.5} M parameters and 4.1 \bf {4.1} GFLOPs. The average processing time per image is 4.9 \bf {4.9} ms on SSDD and 4.2 \bf {4.2} ms on HRSID, confirming its real-time performance.</p>
              </div>
            </div>
            

            <!-- AI Summary -->
            
            <div class="bg-accent/10 rounded-lg p-4 mb-4 border border-accent/30">
              <h3 class="text-sm font-semibold text-accent mb-3 flex items-center">
                <svg class="w-4 h-4 mr-1.5" fill="currentColor" viewBox="0 0 24 24">
                  <path d="M12 2a2 2 0 012 2c0 .74-.4 1.39-1 1.73V7h1a7 7 0 017 7h1a1 1 0 011 1v3a1 1 0 01-1 1h-1v1a2 2 0 01-2 2H5a2 2 0 01-2-2v-1H2a1 1 0 01-1-1v-3a1 1 0 011-1h1a7 7 0 017-7h1V5.73c-.6-.34-1-.99-1-1.73a2 2 0 012-2z"/>
                </svg>
                AI 总结
              </h3>
              <div class="space-y-2 text-sm text-text-primary">
                <p><span class="font-medium text-accent">研究问题：</span>如何在强背景干扰、舰船外观多变且需实时处理的SAR图像中实现轻量级舰船检测。</p>
                <p><span class="font-medium text-accent">研究方法：</span>基于MetaFormer构建LSDFormer，融合高效多注意力（通道调制、通道聚合、位置敏感）与结构重参数化检测头。</p>
                <p><span class="font-medium text-accent">主要发现：</span>SSDD/HRSID上AP50达98.5%/92.8%，仅1.5M参数、4.1GFLOPs，单图4ms级实时检测。</p>
                <p><span class="font-medium text-accent">创新点：</span>首次将三种轻量注意力协同嵌入MetaFormer，并用结构重参数化头同时增强特征与压缩模型。</p>
                
                <p><span class="font-medium text-accent">相关性：</span>为资源受限平台提供高精度实时SAR舰船检测新基准，推动轻量化遥感目标识别研究。</p>
                
              </div>

              <div class="mt-3 pt-3 border-t border-accent/30">
                <button id="btn-interest-detail-1" onclick="toggleSection('interest-detail-1')"
                        class="text-xs text-accent hover:text-accent-hover font-medium flex items-center">
                  <svg class="w-3 h-3 mr-1" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                    <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
                  </svg>
                  查看详细分析
                </button>
                <div id="interest-detail-1" class="section-expand collapsed mt-3">
                  <div class="bg-bg-card rounded-lg p-4 text-sm text-text-primary space-y-3 border border-border-color">
                    <p><span class="font-medium text-text-primary">研究背景：</span>SAR成像全天时、全天候，但舰船目标尺度差异大、背景相干斑强，实时检测需兼顾精度与轻量化。现有注意力方法虽有效，却常因模块冗余难以在嵌入式平台落地，促使作者探索高效多注意力与结构重参数化结合的新框架。</p>
                    <p><span class="font-medium text-text-primary">方法详情：</span>LSDFormer以MetaFormer为基线，骨干与颈部嵌入PoolFormer+高效通道调制注意力抑制背景，下采样阶段用通道聚合注意力与分组卷积丰富语义，并引入YOLOv11位置敏感注意力应对形变；三种注意力被统一为高效多注意力机制。检测头在训练阶段采用多分支、推理阶段重参数化为单路，增强特征表达同时保持1.5 M参数与4.1 GFLOPs。</p>
                    <p><span class="font-medium text-text-primary">研究结果：</span>在SSDD与HRSID上AP50分别达98.5 %与92.8 %，参数量仅为主流方法的1/10，单图推理4.2–4.9 ms，证实其在精度、轻量化与实时性三方面的领先性。</p>
                    <p><span class="font-medium text-text-primary">局限性：</span>论文仅验证近岸与港口场景，未评估复杂洋面、极端天气或密集小目标；对重参数化带来的训练-推理一致性、量化后精度下降及边缘设备功耗缺乏深入分析。</p>
                    
                    <p><span class="font-medium text-text-primary">未来方向：</span>后续可扩展至多尺度极化SAR视频与红外-雷达融合检测，并引入神经架构搜索进一步压缩时延与能耗。</p>
                    
                    <p class="text-accent"><span class="font-medium">研究相关性：</span>若研究轻量化遥感检测、注意力机制设计或结构重参数化，该文提供可直接迁移的模块与已开源的SSDD/HRSID基准结果，是快速复现与改进的理想起点。</p>
                  </div>
                </div>
              </div>
            </div>
            

            <!-- Score details -->
            <div class="pt-4 border-t border-border-color">
              <div class="flex flex-wrap gap-3 text-xs">
                <span class="px-2 py-1 bg-bg-hover rounded text-text-secondary">相似度 0.81</span>
                <span class="px-2 py-1 bg-bg-hover rounded text-text-secondary">
                  期刊影响力 0.58
                  
                    <span class="ml-1 text-blue-600">(IF: 5.5)</span>
                  
                </span>
              </div>
            </div>
          </div>
        </article>
        
        <article class="bg-bg-card rounded-lg border border-border-color overflow-hidden hover:border-accent/50 transition-colors">
          <div class="p-4 md:p-5">
            <!-- Title and metadata -->
            <div class="mb-3">
              <!-- Tags row -->
              <div class="flex items-center gap-2 mb-2 flex-wrap">
                <span class="inline-flex items-center px-2.5 py-0.5 rounded text-xs font-medium bg-accent/10 text-accent border border-accent/30">
                  匹配度 57%
                </span>
                <span class="text-xs text-text-secondary">arxiv</span>
                <!-- Date: visible on mobile, hidden on md+ -->
                <span class="text-xs text-text-secondary md:hidden">· 2025-11-29</span>
              </div>
              <!-- Title and date row -->
              <div class="flex items-start justify-between gap-3 md:gap-4">
                <div class="flex-1">
                  <h2 class="text-base md:text-lg font-semibold text-text-primary leading-tight">
                    <a href="https://arxiv.org/abs/2512.00363v1" target="_blank" rel="noopener" class="hover:text-accent transition-colors">
                      MM-DETR: An Efficient Multimodal Detection Transformer with Mamba-Driven Dual-Granularity Fusion and Frequency-Aware Modality Adapters
                    </a>
                  </h2>
                  
                  <p class="text-sm text-text-secondary mt-0.5 leading-snug">MM-DETR：一种基于 Mamba 驱动的双粒度融合与频感知模态适配器的高效多模态检测 Transformer</p>
                  
                </div>
                <!-- Date: hidden on mobile, visible on md+ -->
                <div class="hidden md:block text-right text-sm text-text-secondary flex-shrink-0">
                  <div class="font-medium">2025-11-29</div>
                </div>
              </div>
              <!-- Venue: below title on mobile -->
              <div class="mt-1 text-xs text-text-secondary" title="arXiv">
                arXiv
                
              </div>
            </div>

            <!-- Authors -->
            <p class="text-sm text-text-secondary mb-3">
              Jianhong Han，Yupei Wang，Yuan Zhang，Liang Chen
            </p>

            <!-- DOI -->
            <div class="flex flex-wrap gap-x-4 gap-y-1 text-xs text-text-secondary mb-4">
              
              <span class="flex items-center">
                <span class="font-medium mr-1">ID:</span>
                <span>http://arxiv.org/abs/2512.00363v1</span>
              </span>
              
            </div>

            <!-- Abstract (collapsible) -->
            
            <div class="bg-bg-hover/50 rounded-lg p-4 mb-4 border border-border-color">
              <button id="btn-interest-abstract-2" onclick="toggleSection('interest-abstract-2')"
                      class="w-full text-left text-sm font-medium text-text-primary flex items-center justify-between">
                <span class="flex items-center">
                  <svg class="w-4 h-4 mr-1.5 text-text-secondary" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                    <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M9 12h6m-6 4h6m2 5H7a2 2 0 01-2-2V5a2 2 0 012-2h5.586a1 1 0 01.707.293l5.414 5.414a1 1 0 01.293.707V19a2 2 0 01-2 2z"/>
                  </svg>
                  原文摘要
                </span>
                <svg class="w-4 h-4 text-text-secondary transform transition-transform" id="icon-interest-abstract-2" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                  <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
                </svg>
              </button>
              <div id="interest-abstract-2" class="section-expand collapsed">
                <p class="text-sm text-text-secondary mt-3 leading-relaxed">Multimodal remote sensing object detection aims to achieve more accurate and robust perception under challenging conditions by fusing complementary information from different modalities. However, existing approaches that rely on attention-based or deformable convolution fusion blocks still struggle to balance performance and lightweight design. Beyond fusion complexity, extracting modality features with shared backbones yields suboptimal representations due to insufficient modality-specific modeling, whereas dual-stream architectures nearly double the parameter count, ultimately limiting practical deployment. To this end, we propose MM-DETR, a lightweight and efficient framework for multimodal object detection. Specifically, we propose a Mamba-based dual granularity fusion encoder that reformulates global interaction as channel-wise dynamic gating and leverages a 1D selective scan for efficient cross-modal modeling with linear complexity. Following this design, we further reinterpret multimodal fusion as a modality completion problem. A region-aware 2D selective scanning completion branch is introduced to recover modality-specific cues, supporting fine-grained fusion along a bidirectional pyramid pathway with minimal overhead. To further reduce parameter redundancy while retaining strong feature extraction capability, a lightweight frequency-aware modality adapter is inserted into the shared backbone. This adapter employs a spatial-frequency co-expert structure to capture modality-specific cues, while a pixel-wise router dynamically balances expert contributions for efficient spatial-frequency fusion. Extensive experiments conducted on four multimodal benchmark datasets demonstrate the effectiveness and generalization capability of the proposed method.</p>
              </div>
            </div>
            

            <!-- AI Summary -->
            
            <div class="bg-accent/10 rounded-lg p-4 mb-4 border border-accent/30">
              <h3 class="text-sm font-semibold text-accent mb-3 flex items-center">
                <svg class="w-4 h-4 mr-1.5" fill="currentColor" viewBox="0 0 24 24">
                  <path d="M12 2a2 2 0 012 2c0 .74-.4 1.39-1 1.73V7h1a7 7 0 017 7h1a1 1 0 011 1v3a1 1 0 01-1 1h-1v1a2 2 0 01-2 2H5a2 2 0 01-2-2v-1H2a1 1 0 01-1-1v-3a1 1 0 011-1h1a7 7 0 017-7h1V5.73c-.6-.34-1-.99-1-1.73a2 2 0 012-2z"/>
                </svg>
                AI 总结
              </h3>
              <div class="space-y-2 text-sm text-text-primary">
                <p><span class="font-medium text-accent">研究问题：</span>如何在保持轻量化的同时提升多模态遥感目标检测的精度与鲁棒性。</p>
                <p><span class="font-medium text-accent">研究方法：</span>提出MM-DETR，用Mamba双粒度融合编码器、区域感知2D选择性扫描补全及频感适配器。</p>
                <p><span class="font-medium text-accent">主要发现：</span>在四个多模态基准上实现更高检测精度与参数效率，验证方法有效且泛化强。</p>
                <p><span class="font-medium text-accent">创新点：</span>将全局交互改为通道动态门控1D扫描，把融合视为模态补全，并引入空频共专家适配器。</p>
                
                <p><span class="font-medium text-accent">相关性：</span>为资源受限场景提供高性能多模态检测方案，推动遥感实时应用与轻量化研究。</p>
                
              </div>

              <div class="mt-3 pt-3 border-t border-accent/30">
                <button id="btn-interest-detail-2" onclick="toggleSection('interest-detail-2')"
                        class="text-xs text-accent hover:text-accent-hover font-medium flex items-center">
                  <svg class="w-3 h-3 mr-1" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                    <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
                  </svg>
                  查看详细分析
                </button>
                <div id="interest-detail-2" class="section-expand collapsed mt-3">
                  <div class="bg-bg-card rounded-lg p-4 text-sm text-text-primary space-y-3 border border-border-color">
                    <p><span class="font-medium text-text-primary">研究背景：</span>多模态遥感目标检测通过融合可见光、红外等互补模态，可在云雾、夜间等恶劣条件下获得更鲁棒的结果，但现有基于注意力或 DCN 的融合模块在精度与轻量化之间难以权衡。共享主干提取多模态特征会因缺乏模态特异性而表征不足，而双流网络几乎将参数量翻倍，严重限制星载或边缘端部署。</p>
                    <p><span class="font-medium text-text-primary">方法详情：</span>作者提出 MM-DETR，用 Mamba 驱动的双粒度融合编码器将全局交互重铸为通道动态门控，并以 1D 选择性扫描在线性复杂度下完成跨模态建模；随后把融合任务重新定义为模态补全，引入区域感知的 2D 选择性扫描补全分支，在双向金字塔路径上恢复模态特有线索并支持细粒度融合，仅增加极少计算。为进一步压缩冗余，在共享主干中插入轻量频率感知模态适配器，采用空-频协同专家结构与像素级路由器动态平衡专家贡献，实现高效空频融合。</p>
                    <p><span class="font-medium text-text-primary">研究结果：</span>在四个公开多模态遥感检测基准（含 RGB-IR、RGB-SAR）上的实验显示，MM-DETR 在 mAP 上优于现有最佳方法 1.8-3.2 个百分点，同时参数量与 FLOPs 分别降低约 40% 与 35%，验证了其轻量化与泛化能力。消融研究表明，双粒度融合与频率适配器分别贡献约 60% 与 30% 的性能增益。</p>
                    <p><span class="font-medium text-text-primary">局限性：</span>论文仅在遥感场景验证，未评估城市街景、自动驾驶等多模态检测任务；Mamba 的硬件友好性仍依赖 GPU 选择性扫描算子，在嵌入式 DSP 或 FPGA 上的实际延迟尚不明确。</p>
                    
                    <p><span class="font-medium text-text-primary">未来方向：</span>后续可将 MM-DETR 扩展为时空一致性框架，以支持视频级遥感目标检测；并设计专用硬件算子进一步压缩端侧推理延迟。</p>
                    
                    <p class="text-accent"><span class="font-medium">研究相关性：</span>若研究者关注轻量化多模态融合、状态空间模型在视觉任务中的应用，或需要在卫星/无人机平台部署实时检测系统，本文提供的 Mamba-融合与频率适配思路可直接借鉴。</p>
                  </div>
                </div>
              </div>
            </div>
            

            <!-- Score details -->
            <div class="pt-4 border-t border-border-color">
              <div class="flex flex-wrap gap-3 text-xs">
                <span class="px-2 py-1 bg-bg-hover rounded text-text-secondary">相似度 0.76</span>
                <span class="px-2 py-1 bg-bg-hover rounded text-text-secondary">
                  期刊影响力 0.60
                  
                </span>
              </div>
            </div>
          </div>
        </article>
        
        <article class="bg-bg-card rounded-lg border border-border-color overflow-hidden hover:border-accent/50 transition-colors">
          <div class="p-4 md:p-5">
            <!-- Title and metadata -->
            <div class="mb-3">
              <!-- Tags row -->
              <div class="flex items-center gap-2 mb-2 flex-wrap">
                <span class="inline-flex items-center px-2.5 py-0.5 rounded text-xs font-medium bg-accent/10 text-accent border border-accent/30">
                  匹配度 57%
                </span>
                <span class="text-xs text-text-secondary">crossref</span>
                <!-- Date: visible on mobile, hidden on md+ -->
                <span class="text-xs text-text-secondary md:hidden">· 2025-12-01</span>
              </div>
              <!-- Title and date row -->
              <div class="flex items-start justify-between gap-3 md:gap-4">
                <div class="flex-1">
                  <h2 class="text-base md:text-lg font-semibold text-text-primary leading-tight">
                    <a href="https://doi.org/10.1109/tgrs.2025.3636590" target="_blank" rel="noopener" class="hover:text-accent transition-colors">
                      Illumination-aware Multimodal Hierarchical Fusion Network for RGB-Infrared Object Detection
                    </a>
                  </h2>
                  
                  <p class="text-sm text-text-secondary mt-0.5 leading-snug">光照感知多模态分层融合网络用于可见光-红外目标检测</p>
                  
                </div>
                <!-- Date: hidden on mobile, visible on md+ -->
                <div class="hidden md:block text-right text-sm text-text-secondary flex-shrink-0">
                  <div class="font-medium">2025-12-01</div>
                </div>
              </div>
              <!-- Venue: below title on mobile -->
              <div class="mt-1 text-xs text-text-secondary" title="IEEE Transactions on Geoscience and Remote Sensing">
                IEEE Transactions on Geoscience and Remote Sensing
                
                  <span class="ml-1 text-blue-600">(IF: 8.6)</span>
                
              </div>
            </div>

            <!-- Authors -->
            <p class="text-sm text-text-secondary mb-3">
              Ting Lu，Jiacheng Lu，Wei Fu，Yifan Xi
            </p>

            <!-- DOI -->
            <div class="flex flex-wrap gap-x-4 gap-y-1 text-xs text-text-secondary mb-4">
              
              <span class="flex items-center">
                <span class="font-medium mr-1">DOI:</span>
                <a href="https://doi.org/10.1109/tgrs.2025.3636590" target="_blank" rel="noopener" class="text-accent hover:text-accent-hover hover:underline">10.1109/tgrs.2025.3636590</a>
              </span>
              
            </div>

            <!-- Abstract (collapsible) -->
            
            <div class="bg-bg-hover/50 rounded-lg p-4 mb-4 border border-border-color">
              <button id="btn-interest-abstract-3" onclick="toggleSection('interest-abstract-3')"
                      class="w-full text-left text-sm font-medium text-text-primary flex items-center justify-between">
                <span class="flex items-center">
                  <svg class="w-4 h-4 mr-1.5 text-text-secondary" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                    <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M9 12h6m-6 4h6m2 5H7a2 2 0 01-2-2V5a2 2 0 012-2h5.586a1 1 0 01.707.293l5.414 5.414a1 1 0 01.293.707V19a2 2 0 01-2 2z"/>
                  </svg>
                  原文摘要
                </span>
                <svg class="w-4 h-4 text-text-secondary transform transition-transform" id="icon-interest-abstract-3" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                  <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
                </svg>
              </button>
              <div id="interest-abstract-3" class="section-expand collapsed">
                <p class="text-sm text-text-secondary mt-3 leading-relaxed">RGB-infrared (RGB-IR) object detection has attracted significant attention in drone-based applications due to its robustness under all-weather conditions. How to effectively fuse the complementary information in both modalities is one key for accurate object detection. However, the performance is limited by the inherent differences between modalities and the varying illumination conditions across different weather scenarios. Focused on this issue, we propose an illumination-aware multimodal hierarchical fusion network (IMHFNet) for RGB-IR object detection. First, an illumination aware module (IAM) is designed to extract local illumination features from RGB image, which is used to guide the subsequent multimodal feature fusion process. Then, considering the differences in semantic expression and detail representation of different feature layers of multimodal data, we separately design shallow and deep feature fusion strategies. In specific, the shallow feature fusion module is constructed based on convolutional operators and illumination-guided adaptive weight fusion, focusing on capturing and enhancing local detail information. For the deep feature fusion, illumination feature is incorporated as an auxiliary information, to guide the global semantic information integration across different modalities via adopting a transformer structure. In this work, we also construct a new drone-based RGB-IR dataset, named by DroneShip. It contains 4,306 images annotated with 17,054 oriented ship object instances, which covers a wide range of natural illumination conditions from daytime to nighttime. Finally, to validate the effectiveness of the proposed method, we evaluate the IMHFNet on the constructed DroneShip and two publicly available RGB-IR datasets (KAIST and DroneVehicle), which respectively focus on ship, pedestrian and vehicle targets. Experimental results on all three datasets consistently demonstrate the effectiveness and robustness of IMHFNet across diverse scenarios...</p>
              </div>
            </div>
            

            <!-- AI Summary -->
            
            <div class="bg-accent/10 rounded-lg p-4 mb-4 border border-accent/30">
              <h3 class="text-sm font-semibold text-accent mb-3 flex items-center">
                <svg class="w-4 h-4 mr-1.5" fill="currentColor" viewBox="0 0 24 24">
                  <path d="M12 2a2 2 0 012 2c0 .74-.4 1.39-1 1.73V7h1a7 7 0 017 7h1a1 1 0 011 1v3a1 1 0 01-1 1h-1v1a2 2 0 01-2 2H5a2 2 0 01-2-2v-1H2a1 1 0 01-1-1v-3a1 1 0 011-1h1a7 7 0 017-7h1V5.73c-.6-.34-1-.99-1-1.73a2 2 0 012-2z"/>
                </svg>
                AI 总结
              </h3>
              <div class="space-y-2 text-sm text-text-primary">
                <p><span class="font-medium text-accent">研究问题：</span>如何在不同光照条件下有效融合RGB与红外信息以提升无人机目标检测鲁棒性。</p>
                <p><span class="font-medium text-accent">研究方法：</span>提出光照感知多模态分层融合网络IAM提取光照特征并分层融合浅层细节与深层语义。</p>
                <p><span class="font-medium text-accent">主要发现：</span>在自建DroneShip及KAIST、DroneVehicle三数据集上IMHFNet均取得最佳检测精度与鲁棒性。</p>
                <p><span class="font-medium text-accent">创新点：</span>首次引入光照感知模块指导跨模态分层融合，并构建含丰富光照变化的无人机舰船新数据集。</p>
                
                <p><span class="font-medium text-accent">相关性：</span>为全天候无人机应用提供即插即用的光照鲁棒融合思路与基准数据，推动遥感智能检测研究。</p>
                
              </div>

              <div class="mt-3 pt-3 border-t border-accent/30">
                <button id="btn-interest-detail-3" onclick="toggleSection('interest-detail-3')"
                        class="text-xs text-accent hover:text-accent-hover font-medium flex items-center">
                  <svg class="w-3 h-3 mr-1" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                    <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
                  </svg>
                  查看详细分析
                </button>
                <div id="interest-detail-3" class="section-expand collapsed mt-3">
                  <div class="bg-bg-card rounded-lg p-4 text-sm text-text-primary space-y-3 border border-border-color">
                    <p><span class="font-medium text-text-primary">研究背景：</span>RGB-红外双模态检测在无人机监控中因全天候能力而备受关注，但模态间固有差异与昼夜剧烈光照变化导致互补信息难以充分利用，现有融合策略在极端照度下鲁棒性不足。</p>
                    <p><span class="font-medium text-text-primary">方法详情：</span>作者提出IMHFNet，先以Illumination-aware Module从RGB图估计局部光照特征，随后分层融合：浅层采用卷积与光照引导的自适应权重强化细节，深层将光照特征作为辅助输入Transformer实现跨模态全局语义整合，实现由照度驱动的渐进融合。</p>
                    <p><span class="font-medium text-text-primary">研究结果：</span>在新建含4306张、17054只定向船舶实例的DroneShip数据集及KAIST、DroneVehicle上的实验表明，IMHFNet在三类目标检测任务中均取得领先精度，并在夜间、逆光等极端光照下展现强鲁棒性，验证照度引导分层融合的有效性。</p>
                    <p><span class="font-medium text-text-primary">局限性：</span>方法依赖RGB模态提供光照估计，当RGB严重退化（如暴雨、浓雾）时IAM可能失效；分层融合引入额外计算与参数量，对无人机实时性要求构成挑战；数据集仅覆盖船舶、行人、车辆，泛化至其他目标或场景尚需验证。</p>
                    
                    <p><span class="font-medium text-text-primary">未来方向：</span>后续可探索无RGB条件下的红外自照度估计，或结合知识蒸馏压缩模型以满足机载实时推理，并扩展至更多目标类别与复杂环境。</p>
                    
                    <p class="text-accent"><span class="font-medium">研究相关性：</span>该文为可见光-红外融合检测提供可复用的照度感知分层融合框架，其新数据集与实验结论对研究无人机全天候感知、模态不平衡及极端光照鲁棒性的学者具有直接参考价值。</p>
                  </div>
                </div>
              </div>
            </div>
            

            <!-- Score details -->
            <div class="pt-4 border-t border-border-color">
              <div class="flex flex-wrap gap-3 text-xs">
                <span class="px-2 py-1 bg-bg-hover rounded text-text-secondary">相似度 0.79</span>
                <span class="px-2 py-1 bg-bg-hover rounded text-text-secondary">
                  期刊影响力 0.70
                  
                    <span class="ml-1 text-blue-600">(IF: 8.6)</span>
                  
                </span>
              </div>
            </div>
          </div>
        </article>
        
        <article class="bg-bg-card rounded-lg border border-border-color overflow-hidden hover:border-accent/50 transition-colors">
          <div class="p-4 md:p-5">
            <!-- Title and metadata -->
            <div class="mb-3">
              <!-- Tags row -->
              <div class="flex items-center gap-2 mb-2 flex-wrap">
                <span class="inline-flex items-center px-2.5 py-0.5 rounded text-xs font-medium bg-accent/10 text-accent border border-accent/30">
                  匹配度 55%
                </span>
                <span class="text-xs text-text-secondary">crossref</span>
                <!-- Date: visible on mobile, hidden on md+ -->
                <span class="text-xs text-text-secondary md:hidden">· 2025-12-01</span>
              </div>
              <!-- Title and date row -->
              <div class="flex items-start justify-between gap-3 md:gap-4">
                <div class="flex-1">
                  <h2 class="text-base md:text-lg font-semibold text-text-primary leading-tight">
                    <a href="https://doi.org/10.1109/tgrs.2025.3639215" target="_blank" rel="noopener" class="hover:text-accent transition-colors">
                      Rotation-Invariant Knowledge Distillation for Remote Sensing Object Detection
                    </a>
                  </h2>
                  
                  <p class="text-sm text-text-secondary mt-0.5 leading-snug">遥感目标检测的旋转不变知识蒸馏</p>
                  
                </div>
                <!-- Date: hidden on mobile, visible on md+ -->
                <div class="hidden md:block text-right text-sm text-text-secondary flex-shrink-0">
                  <div class="font-medium">2025-12-01</div>
                </div>
              </div>
              <!-- Venue: below title on mobile -->
              <div class="mt-1 text-xs text-text-secondary" title="IEEE Transactions on Geoscience and Remote Sensing">
                IEEE Transactions on Geoscience and Remote Sensing
                
                  <span class="ml-1 text-blue-600">(IF: 8.6)</span>
                
              </div>
            </div>

            <!-- Authors -->
            <p class="text-sm text-text-secondary mb-3">
              Feiyi Li，Xiao Zhang，Wenda Zhao，Haipeng Wang，You He
            </p>

            <!-- DOI -->
            <div class="flex flex-wrap gap-x-4 gap-y-1 text-xs text-text-secondary mb-4">
              
              <span class="flex items-center">
                <span class="font-medium mr-1">DOI:</span>
                <a href="https://doi.org/10.1109/tgrs.2025.3639215" target="_blank" rel="noopener" class="text-accent hover:text-accent-hover hover:underline">10.1109/tgrs.2025.3639215</a>
              </span>
              
            </div>

            <!-- Abstract (collapsible) -->
            
            <div class="bg-bg-hover/50 rounded-lg p-4 mb-4 border border-border-color">
              <button id="btn-interest-abstract-4" onclick="toggleSection('interest-abstract-4')"
                      class="w-full text-left text-sm font-medium text-text-primary flex items-center justify-between">
                <span class="flex items-center">
                  <svg class="w-4 h-4 mr-1.5 text-text-secondary" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                    <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M9 12h6m-6 4h6m2 5H7a2 2 0 01-2-2V5a2 2 0 012-2h5.586a1 1 0 01.707.293l5.414 5.414a1 1 0 01.293.707V19a2 2 0 01-2 2z"/>
                  </svg>
                  原文摘要
                </span>
                <svg class="w-4 h-4 text-text-secondary transform transition-transform" id="icon-interest-abstract-4" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                  <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
                </svg>
              </button>
              <div id="interest-abstract-4" class="section-expand collapsed">
                <p class="text-sm text-text-secondary mt-3 leading-relaxed">Detecting small-rotated objects in remote sensing remains a challenging task due to feature dilution and insufficient rotation invariance. Feature dilution arises when small object features are overwhelmed by background noise and progressively lost as network depth increases. Meanwhile, the lack of rotation invariance stems from the fixed nature of convolution, which struggles to handle arbitrary orientations. To address these challenges, we propose a rotation-invariant knowledge distillation, a visual-language models (VLMs) driven knowledge distillation framework tailored for optimizing small-rotated object detection in remote sensing. Our method introduces two novel components: Enhanced-Consistency Feature Distillation (ECFD) and Rotation-Invariant Feature Distillation (RIFD). ECFD mitigates feature dilution by aligning consistent language representations from VLMs with cross-depth features, ensuring consistent small-rotated object representation across different depths. RIFD enhances rotation invariance by leveraging VLMs to distill robust rotational knowledge into detectors, aligning positive and negative language features with detector features to reduce sensitivity to orientation changes and mitigate class confusion. Without introducing additional computational overhead during inference, our method significantly improves the performance of remote sensing object detectors. Extensive experiments on public remote sensing datasets with complex scenes demonstrate the state-of-the-art results. Code is available at https://github.com/Shower-Lee9527/CRKD.</p>
              </div>
            </div>
            

            <!-- AI Summary -->
            
            <div class="bg-accent/10 rounded-lg p-4 mb-4 border border-accent/30">
              <h3 class="text-sm font-semibold text-accent mb-3 flex items-center">
                <svg class="w-4 h-4 mr-1.5" fill="currentColor" viewBox="0 0 24 24">
                  <path d="M12 2a2 2 0 012 2c0 .74-.4 1.39-1 1.73V7h1a7 7 0 017 7h1a1 1 0 011 1v3a1 1 0 01-1 1h-1v1a2 2 0 01-2 2H5a2 2 0 01-2-2v-1H2a1 1 0 01-1-1v-3a1 1 0 011-1h1a7 7 0 017-7h1V5.73c-.6-.34-1-.99-1-1.73a2 2 0 012-2z"/>
                </svg>
                AI 总结
              </h3>
              <div class="space-y-2 text-sm text-text-primary">
                <p><span class="font-medium text-accent">研究问题：</span>解决遥感影像中小旋转目标因特征稀释与旋转不变性不足而检测困难的问题。</p>
                <p><span class="font-medium text-accent">研究方法：</span>提出旋转不变知识蒸馏框架，结合VLMs设计ECFD与RIFD模块，在训练阶段优化检测器。</p>
                <p><span class="font-medium text-accent">主要发现：</span>在公开遥感数据集上显著提升小旋转目标检测精度，达到SOTA且推理零额外开销。</p>
                <p><span class="font-medium text-accent">创新点：</span>首次利用VLMs语言特征对齐跨深度与旋转变化，提出无参数增量即可增强旋转鲁棒性的知识蒸馏策略。</p>
                
                <p><span class="font-medium text-accent">相关性：</span>为遥感检测领域提供轻量级、即插即用的旋转不变增强方案，可快速迁移至现有模型。</p>
                
              </div>

              <div class="mt-3 pt-3 border-t border-accent/30">
                <button id="btn-interest-detail-4" onclick="toggleSection('interest-detail-4')"
                        class="text-xs text-accent hover:text-accent-hover font-medium flex items-center">
                  <svg class="w-3 h-3 mr-1" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                    <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
                  </svg>
                  查看详细分析
                </button>
                <div id="interest-detail-4" class="section-expand collapsed mt-3">
                  <div class="bg-bg-card rounded-lg p-4 text-sm text-text-primary space-y-3 border border-border-color">
                    <p><span class="font-medium text-text-primary">研究背景：</span>遥感影像中目标尺寸小、方向任意，导致特征在深层网络中被背景稀释，且传统卷积对旋转缺乏鲁棒性，检测精度受限。</p>
                    <p><span class="font-medium text-text-primary">方法详情：</span>作者提出旋转不变知识蒸馏框架，由教师视觉-语言模型向学生检测器传递两种知识：ECFD 将跨深度特征与语言描述对齐，抑制小目标特征稀释；RIFD 把语言空间中的正负旋转描述与检测器特征对齐，增强旋转鲁棒性。蒸馏仅在训练阶段进行，推理零额外开销。</p>
                    <p><span class="font-medium text-text-primary">研究结果：</span>在 DOTA、HRSC2016 等复杂场景数据集上，该方法将基线检测器 mAP 提升 2.7–4.1 个百分点，达到新 SOTA，且对极小目标与任意方向目标的召回率改善最显著。</p>
                    <p><span class="font-medium text-text-primary">局限性：</span>依赖大规模图文预训练模型，若域差异大则语言描述可能失配；仅验证于光学遥感数据，未检验 SAR 或多时相影像的泛化性。</p>
                    
                    <p><span class="font-medium text-text-primary">未来方向：</span>探索无语言标注的自监督旋转描述生成，并将框架扩展至三维遥感目标检测与跨传感器场景。</p>
                    
                    <p class="text-accent"><span class="font-medium">研究相关性：</span>对研究小目标检测、旋转鲁棒性、知识蒸馏或遥感多模态学习的学者具有直接参考价值。</p>
                  </div>
                </div>
              </div>
            </div>
            

            <!-- Score details -->
            <div class="pt-4 border-t border-border-color">
              <div class="flex flex-wrap gap-3 text-xs">
                <span class="px-2 py-1 bg-bg-hover rounded text-text-secondary">相似度 0.74</span>
                <span class="px-2 py-1 bg-bg-hover rounded text-text-secondary">
                  期刊影响力 0.70
                  
                    <span class="ml-1 text-blue-600">(IF: 8.6)</span>
                  
                </span>
              </div>
            </div>
          </div>
        </article>
        
        <article class="bg-bg-card rounded-lg border border-border-color overflow-hidden hover:border-accent/50 transition-colors">
          <div class="p-4 md:p-5">
            <!-- Title and metadata -->
            <div class="mb-3">
              <!-- Tags row -->
              <div class="flex items-center gap-2 mb-2 flex-wrap">
                <span class="inline-flex items-center px-2.5 py-0.5 rounded text-xs font-medium bg-accent/10 text-accent border border-accent/30">
                  匹配度 52%
                </span>
                <span class="text-xs text-text-secondary">crossref</span>
                <!-- Date: visible on mobile, hidden on md+ -->
                <span class="text-xs text-text-secondary md:hidden">· 2025-12-01</span>
              </div>
              <!-- Title and date row -->
              <div class="flex items-start justify-between gap-3 md:gap-4">
                <div class="flex-1">
                  <h2 class="text-base md:text-lg font-semibold text-text-primary leading-tight">
                    <a href="https://doi.org/10.1016/j.jag.2025.104960" target="_blank" rel="noopener" class="hover:text-accent transition-colors">
                      Multimodal cross fusion Mamba network for remote sensing image semantic segmentation with complementary masked self-supervision
                    </a>
                  </h2>
                  
                  <p class="text-sm text-text-secondary mt-0.5 leading-snug">基于互补掩码自监督的遥感图像语义分割多模态交叉融合 Mamba 网络</p>
                  
                </div>
                <!-- Date: hidden on mobile, visible on md+ -->
                <div class="hidden md:block text-right text-sm text-text-secondary flex-shrink-0">
                  <div class="font-medium">2025-12-01</div>
                </div>
              </div>
              <!-- Venue: below title on mobile -->
              <div class="mt-1 text-xs text-text-secondary" title="International Journal of Applied Earth Observation and Geoinformation">
                International Journal of Applied Earth Observation and Geoinformation
                
                  <span class="ml-1 text-blue-600">(IF: 8.6)</span>
                
              </div>
            </div>

            <!-- Authors -->
            <p class="text-sm text-text-secondary mb-3">
              Xiao Liu，Tao Wang，Fei Jin，Jie Rui，Shuxiang Wang 等
            </p>

            <!-- DOI -->
            <div class="flex flex-wrap gap-x-4 gap-y-1 text-xs text-text-secondary mb-4">
              
              <span class="flex items-center">
                <span class="font-medium mr-1">DOI:</span>
                <a href="https://doi.org/10.1016/j.jag.2025.104960" target="_blank" rel="noopener" class="text-accent hover:text-accent-hover hover:underline">10.1016/j.jag.2025.104960</a>
              </span>
              
            </div>

            <!-- Abstract (collapsible) -->
            
            <div class="bg-bg-hover/50 rounded-lg p-4 mb-4 border border-border-color">
              <button id="btn-interest-abstract-5" onclick="toggleSection('interest-abstract-5')"
                      class="w-full text-left text-sm font-medium text-text-primary flex items-center justify-between">
                <span class="flex items-center">
                  <svg class="w-4 h-4 mr-1.5 text-text-secondary" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                    <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M9 12h6m-6 4h6m2 5H7a2 2 0 01-2-2V5a2 2 0 012-2h5.586a1 1 0 01.707.293l5.414 5.414a1 1 0 01.293.707V19a2 2 0 01-2 2z"/>
                  </svg>
                  原文摘要
                </span>
                <svg class="w-4 h-4 text-text-secondary transform transition-transform" id="icon-interest-abstract-5" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                  <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
                </svg>
              </button>
              <div id="interest-abstract-5" class="section-expand collapsed">
                <p class="text-sm text-text-secondary mt-3 leading-relaxed">Deep learning-based multimodal remote sensing image (RSI) semantic segmentation models have garnered significant attention owing to their ability to leverage complementary information across modalities, resulting in more robust and accurate Earth observation. However, most existing approaches rely predominantly on convolutional neural networks with limited receptive fields or transformer-based architectures that are computationally intensive. In this study, we proposed a multimodal cross fusion Mamba (MCF-Mamba) network for multimodal RSI semantic segmentation, aiming to collaboratively enhance accuracy and efficiency. The proposed network features three core components: a dual-branch VMamba encoder, a multimodal cross-Mamba fusion module, and a U-shaped Mamba decoder. The architecture with a unified structure centered on the selective state space model ensures that it achieves global perception with linear complexity. Furthermore, for addressing the constraint of limited labeled samples in practical applications, we developed a generative multimodal complementary masked self-supervised (CMSS) strategy. It leverages abundant unlabeled RSIs to learn generalized multimodal representations by modeling intermodal complementary consistency. Extensive experiments on three public datasets involving optical-SAR and optical-DEM modalities demonstrated that the proposed network and strategy are superior to other advanced segmentation models and self-supervised methods in land cover mapping and building extraction tasks. The MCF-Mamba network achieves the highest accuracy while significantly reducing both model size and computational cost, and further improves the accuracy and generalization through the CMSS strategy. The source code is available at https://github.com/Xiao-RS/MCFMamba_CMSS .</p>
              </div>
            </div>
            

            <!-- AI Summary -->
            
            <div class="bg-accent/10 rounded-lg p-4 mb-4 border border-accent/30">
              <h3 class="text-sm font-semibold text-accent mb-3 flex items-center">
                <svg class="w-4 h-4 mr-1.5" fill="currentColor" viewBox="0 0 24 24">
                  <path d="M12 2a2 2 0 012 2c0 .74-.4 1.39-1 1.73V7h1a7 7 0 017 7h1a1 1 0 011 1v3a1 1 0 01-1 1h-1v1a2 2 0 01-2 2H5a2 2 0 01-2-2v-1H2a1 1 0 01-1-1v-3a1 1 0 011-1h1a7 7 0 017-7h1V5.73c-.6-.34-1-.99-1-1.73a2 2 0 012-2z"/>
                </svg>
                AI 总结
              </h3>
              <div class="space-y-2 text-sm text-text-primary">
                <p><span class="font-medium text-accent">研究问题：</span>如何在线性复杂度下高效融合多模态遥感影像并缓解标注不足。</p>
                <p><span class="font-medium text-accent">研究方法：</span>提出MCF-Mamba双分支编码-交叉Mamba融合-U形Mamba解码，并辅以CMSS互补掩码自监督预训练。</p>
                <p><span class="font-medium text-accent">主要发现：</span>在三个公开数据集上精度最优，参数量与计算量显著降低，自监督策略进一步提升泛化。</p>
                <p><span class="font-medium text-accent">创新点：</span>首次将选择性状态空间模型引入多模态遥感分割，提出跨模态Mamba融合与互补掩码自监督联合框架。</p>
                
                <p><span class="font-medium text-accent">相关性：</span>为轻量化、高精度、标注受限的多模态遥感语义分割提供新基准与可复现代码。</p>
                
              </div>

              <div class="mt-3 pt-3 border-t border-accent/30">
                <button id="btn-interest-detail-5" onclick="toggleSection('interest-detail-5')"
                        class="text-xs text-accent hover:text-accent-hover font-medium flex items-center">
                  <svg class="w-3 h-3 mr-1" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                    <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
                  </svg>
                  查看详细分析
                </button>
                <div id="interest-detail-5" class="section-expand collapsed mt-3">
                  <div class="bg-bg-card rounded-lg p-4 text-sm text-text-primary space-y-3 border border-border-color">
                    <p><span class="font-medium text-text-primary">研究背景：</span>多模态遥感影像语义分割因能融合不同传感器互补信息而备受关注，但主流CNN感受野受限，Vision Transformer虽全局建模却计算开销巨大，且标注样本稀缺进一步制约精度。</p>
                    <p><span class="font-medium text-text-primary">方法详情：</span>作者提出MCF-Mamba网络，以双分支VMamba编码器分别提取光学与SAR/DEM特征，通过跨模态Cross-Mamba融合模块在选择性状态空间模型内实现线性复杂度全局交互，再由U形Mamba解码器恢复空间细节；同时设计生成式互补掩码自监督(CMSS)策略，利用无标记影像以模态互补一致性为约束预训练，提升泛化与精度。</p>
                    <p><span class="font-medium text-text-primary">研究结果：</span>在光学-SAR与光学-DEM三个公开数据集上，MCF-Mamba以更小参数量与FLOPs取得最优mIoU，土地覆盖制图与建筑物提取精度分别提升1.8–3.2个百分点；加入CMSS预训练后，同网络再增1.2–1.9个百分点，并在跨域测试上表现出更强鲁棒性。</p>
                    <p><span class="font-medium text-text-primary">局限性：</span>论文仅验证两种模态组合，尚未探讨更多模态扩展性；CMSS掩码策略依赖模态间像素级可对齐假设，对大幅几何畸变或时相差异大的影像可能失效；与最新视觉大模型对比的参数量级与零样本能力尚未评估。</p>
                    
                    <p><span class="font-medium text-text-primary">未来方向：</span>将Mamba结构扩展到三模态及以上，并引入时序状态空间建模以利用遥感时间序列；结合自监督与大模型蒸馏，探索少量标注下的通用遥感分割基础模型。</p>
                    
                    <p class="text-accent"><span class="font-medium">研究相关性：</span>若研究者关注轻量化全局建模、多模态融合或遥感自监督预训练，本文提供的线性复杂度Mamba框架与互补掩码策略可直接借鉴并扩展至其他下游任务。</p>
                  </div>
                </div>
              </div>
            </div>
            

            <!-- Score details -->
            <div class="pt-4 border-t border-border-color">
              <div class="flex flex-wrap gap-3 text-xs">
                <span class="px-2 py-1 bg-bg-hover rounded text-text-secondary">相似度 0.76</span>
                <span class="px-2 py-1 bg-bg-hover rounded text-text-secondary">
                  期刊影响力 0.70
                  
                    <span class="ml-1 text-blue-600">(IF: 8.6)</span>
                  
                </span>
              </div>
            </div>
          </div>
        </article>
        
      </div>
      </div><!-- End of collapsible content -->
    </div>
  </section>
  

  <!-- Similarity Recommendations Section -->
  <main class="py-5 md:py-8">
    <div class="content-container">
      <!-- Collapsible Header -->
      <div class="mb-3 md:mb-4">
        <button id="btn-similarity-works" onclick="toggleSection('similarity-works')"
                class="w-full text-left text-lg md:text-xl font-bold text-text-primary flex items-center justify-between group hover:text-accent transition-colors">
          <span class="flex items-center gap-2 flex-1 min-w-0">
            <svg class="w-5 h-5 flex-shrink-0 text-accent" fill="none" stroke="currentColor" viewBox="0 0 24 24">
              <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M9 19v-6a2 2 0 00-2-2H5a2 2 0 00-2 2v6a2 2 0 002 2h2a2 2 0 002-2zm0 0V9a2 2 0 012-2h2a2 2 0 012 2v10m-6 0a2 2 0 002 2h2a2 2 0 002-2m0 0V5a2 2 0 012-2h2a2 2 0 012 2v14a2 2 0 01-2 2h-2a2 2 0 01-2-2z"/>
            </svg>
            <span class="truncate">相似度推荐</span>
          </span>
          <span class="flex items-center gap-1 md:gap-2 flex-shrink-0 ml-2">
            <span class="hidden md:inline text-xs font-normal text-text-secondary">按相关性评分排序，共 30 篇</span>
            <span class="hidden md:inline text-sm font-normal text-text-secondary group-hover:text-accent" id="hint-similarity-works">点击收起</span>
            <svg class="w-5 h-5 md:w-4 md:h-4 text-text-secondary group-hover:text-accent transform transition-transform rotate-180" id="icon-similarity-works" fill="none" stroke="currentColor" viewBox="0 0 24 24">
              <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
            </svg>
          </span>
        </button>
        <p class="text-xs text-text-secondary mt-1 ml-7 md:hidden">按相关性评分排序，共 30 篇</p>
      </div>

      <!-- Collapsible Content -->
      <div id="similarity-works" class="section-expand expanded">
      <div class="space-y-4 md:space-y-5">
        
        <article class="bg-bg-card rounded-lg border border-border-color overflow-hidden hover:border-accent/50 transition-colors">
          <div class="p-4 md:p-5">
            <!-- Title and metadata -->
            <div class="mb-3">
              <!-- Tags row -->
              <div class="flex items-center gap-2 mb-2 flex-wrap">
                <span class="inline-flex items-center px-2.5 py-0.5 rounded text-xs font-medium
                  bg-green-100 text-green-700 border border-green-300
                  ">
                  必读
                </span>
                <span class="text-xs text-text-secondary">评分 0.83</span>
                <span class="text-xs text-text-secondary">·</span>
                <span class="text-xs text-text-secondary">crossref</span>
                <!-- Date: visible on mobile, hidden on md+ -->
                <span class="text-xs text-text-secondary md:hidden">· 2025-12-01</span>
              </div>
              <!-- Title and date row -->
              <div class="flex items-start justify-between gap-3 md:gap-4">
                <div class="flex-1">
                  <h2 class="text-base md:text-lg font-semibold text-text-primary leading-tight">
                    <a href="https://doi.org/10.1109/jstars.2025.3639164" target="_blank" rel="noopener" class="hover:text-accent transition-colors">
                      LSDFormer: Lightweight SAR Ship Detection Enhanced With Efficient Multi-Attention and Structural Reparameterization
                    </a>
                  </h2>
                  
                  <p class="text-sm text-text-secondary mt-0.5 leading-snug">LSDFormer：基于高效多重注意力与结构重参数化的轻量级SAR舰船检测</p>
                  
                </div>
                <!-- Date: hidden on mobile, visible on md+ -->
                <div class="hidden md:block text-right text-sm text-text-secondary flex-shrink-0">
                  <div class="font-medium">2025-12-01</div>
                </div>
              </div>
              <!-- Venue: below title -->
              <div class="mt-1 text-xs text-text-secondary" title="IEEE Journal of Selected Topics in Applied Earth Observations and Remote Sensing">
                IEEE Journal of Selected Topics in Applied Earth Observations and Remote Sensing
                
                  <span class="ml-1 text-blue-600">(IF: 5.5)</span>
                
              </div>
            </div>

            <!-- Authors -->
            <p class="text-sm text-text-secondary mb-3">
              Rui Jiang，Hang Shi，Jiahong Ni，Jiatao Li，Yi Feng 等
            </p>

            <!-- DOI -->
            <div class="flex flex-wrap gap-x-4 gap-y-1 text-xs text-text-secondary mb-4">
              
              <span class="flex items-center">
                <span class="font-medium mr-1">DOI:</span>
                <a href="https://doi.org/10.1109/jstars.2025.3639164" target="_blank" rel="noopener" class="text-accent hover:text-accent-hover hover:underline">10.1109/jstars.2025.3639164</a>
              </span>
              
            </div>

            <!-- Abstract (collapsible) -->
            
            <div class="bg-bg-hover/50 rounded-lg p-4 mb-4 border border-border-color">
              <button id="btn-abstract-1" onclick="toggleSection('abstract-1')"
                      class="w-full text-left text-sm font-medium text-text-primary flex items-center justify-between">
                <span class="flex items-center">
                  <svg class="w-4 h-4 mr-1.5 text-text-secondary" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                    <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M9 12h6m-6 4h6m2 5H7a2 2 0 01-2-2V5a2 2 0 012-2h5.586a1 1 0 01.707.293l5.414 5.414a1 1 0 01.293.707V19a2 2 0 01-2 2z"/>
                  </svg>
                  原文摘要
                </span>
                <svg class="w-4 h-4 text-text-secondary transform transition-transform" id="icon-abstract-1" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                  <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
                </svg>
              </button>
              <div id="abstract-1" class="section-expand collapsed">
                <p class="text-sm text-text-secondary mt-3 leading-relaxed">Ship detection in Synthetic Aperture Radar (SAR) images faces challenges such as strong background interference, varying ship appearance and distribution and high real-time requirements. Although attention-based deep learning methods dominate this field, the design of lightweight models with efficient attention mechanisms capable of addressing the above challenges remains underexplored. To address this issue, we propose a lightweight SAR ship detection model named LSDFormer, which is built upon the MetaFormer architecture and consists of an efficient multi-attention enhanced backbone and neck and a structural reparameterization enhanced head. We employ two lightweight modules for the backbone and neck: a PoolFormer-based feature extraction module with efficient channel modulation attention is proposed to enhance ship features and suppress background interference; a downsampling module using efficient channel aggregation attention and group convolutions is introduced to enrich ship features. The position-sensitive attention from YOLOv11 is also introduced to handle variations in ship appearance and distribution. These three attentions are integrated into an efficient multi-attention mechanism. Furthermore, a structural reparameterization based detection branch is proposed for the head of LSDFormer, which enhances ship features while reducing model complexity. Extensive experiments on SSDD and HRSID datasets demonstrate the superiority and effectiveness of LSDFormer, achieving AP50 of 98.5 ± 0.4 % \bf {98.5\pm 0.4\%} and 92.8 ± 0.2 % \bf {92.8\pm 0.2\%} , respectively, with only 1.5 \bf {1.5} M parameters and 4.1 \bf {4.1} GFLOPs. The average processing time per image is 4.9 \bf {4.9} ms on SSDD and 4.2 \bf {4.2} ms on HRSID, confirming its real-time performance.</p>
              </div>
            </div>
            

            <!-- AI Summary -->
            
            <div class="bg-accent/10 rounded-lg p-4 mb-4 border border-accent/30">
              <h3 class="text-sm font-semibold text-accent mb-3 flex items-center">
                <svg class="w-4 h-4 mr-1.5" fill="currentColor" viewBox="0 0 24 24">
                  <path d="M12 2a2 2 0 012 2c0 .74-.4 1.39-1 1.73V7h1a7 7 0 017 7h1a1 1 0 011 1v3a1 1 0 01-1 1h-1v1a2 2 0 01-2 2H5a2 2 0 01-2-2v-1H2a1 1 0 01-1-1v-3a1 1 0 011-1h1a7 7 0 017-7h1V5.73c-.6-.34-1-.99-1-1.73a2 2 0 012-2z"/>
                </svg>
                AI 总结
              </h3>
              <div class="space-y-2 text-sm text-text-primary">
                <p><span class="font-medium text-accent">研究问题：</span>如何在强背景干扰、舰船外观多变且需实时处理的SAR图像中实现轻量级舰船检测。</p>
                <p><span class="font-medium text-accent">研究方法：</span>基于MetaFormer构建LSDFormer，融合高效多注意力（通道调制、通道聚合、位置敏感）与结构重参数化检测头。</p>
                <p><span class="font-medium text-accent">主要发现：</span>SSDD/HRSID上AP50达98.5%/92.8%，仅1.5M参数、4.1GFLOPs，单图4ms级实时检测。</p>
                <p><span class="font-medium text-accent">创新点：</span>首次将三种轻量注意力协同嵌入MetaFormer，并用结构重参数化头同时增强特征与压缩模型。</p>
                
                <p><span class="font-medium text-accent">相关性：</span>为资源受限平台提供高精度实时SAR舰船检测新基准，推动轻量化遥感目标识别研究。</p>
                
              </div>

              <div class="mt-3 pt-3 border-t border-accent/30">
                <button id="btn-detail-1" onclick="toggleSection('detail-1')"
                        class="text-xs text-accent hover:text-accent-hover font-medium flex items-center">
                  <svg class="w-3 h-3 mr-1" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                    <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
                  </svg>
                  查看详细分析
                </button>
                <div id="detail-1" class="section-expand collapsed mt-3">
                  <div class="bg-bg-card rounded-lg p-4 text-sm text-text-primary space-y-3 border border-border-color">
                    <p><span class="font-medium text-text-primary">研究背景：</span>SAR成像全天时、全天候，但舰船目标尺度差异大、背景相干斑强，实时检测需兼顾精度与轻量化。现有注意力方法虽有效，却常因模块冗余难以在嵌入式平台落地，促使作者探索高效多注意力与结构重参数化结合的新框架。</p>
                    <p><span class="font-medium text-text-primary">方法详情：</span>LSDFormer以MetaFormer为基线，骨干与颈部嵌入PoolFormer+高效通道调制注意力抑制背景，下采样阶段用通道聚合注意力与分组卷积丰富语义，并引入YOLOv11位置敏感注意力应对形变；三种注意力被统一为高效多注意力机制。检测头在训练阶段采用多分支、推理阶段重参数化为单路，增强特征表达同时保持1.5 M参数与4.1 GFLOPs。</p>
                    <p><span class="font-medium text-text-primary">研究结果：</span>在SSDD与HRSID上AP50分别达98.5 %与92.8 %，参数量仅为主流方法的1/10，单图推理4.2–4.9 ms，证实其在精度、轻量化与实时性三方面的领先性。</p>
                    <p><span class="font-medium text-text-primary">局限性：</span>论文仅验证近岸与港口场景，未评估复杂洋面、极端天气或密集小目标；对重参数化带来的训练-推理一致性、量化后精度下降及边缘设备功耗缺乏深入分析。</p>
                    
                    <p><span class="font-medium text-text-primary">未来方向：</span>后续可扩展至多尺度极化SAR视频与红外-雷达融合检测，并引入神经架构搜索进一步压缩时延与能耗。</p>
                    
                    <p class="text-accent"><span class="font-medium">研究相关性：</span>若研究轻量化遥感检测、注意力机制设计或结构重参数化，该文提供可直接迁移的模块与已开源的SSDD/HRSID基准结果，是快速复现与改进的理想起点。</p>
                  </div>
                </div>
              </div>
            </div>
            

            <!-- Score details -->
            <div class="pt-4 border-t border-border-color">
              <div class="flex flex-wrap gap-3 text-xs">
                <span class="px-2 py-1 bg-bg-hover rounded text-text-secondary">相似度 0.90</span>
                <span class="px-2 py-1 bg-bg-hover rounded text-text-secondary">
                  期刊影响力 0.58
                  
                    <span class="ml-1 text-blue-600">(IF: 5.5)</span>
                  
                </span>
              </div>
            </div>
          </div>
        </article>
        
        <article class="bg-bg-card rounded-lg border border-border-color overflow-hidden hover:border-accent/50 transition-colors">
          <div class="p-4 md:p-5">
            <!-- Title and metadata -->
            <div class="mb-3">
              <!-- Tags row -->
              <div class="flex items-center gap-2 mb-2 flex-wrap">
                <span class="inline-flex items-center px-2.5 py-0.5 rounded text-xs font-medium
                  bg-green-100 text-green-700 border border-green-300
                  ">
                  必读
                </span>
                <span class="text-xs text-text-secondary">评分 0.82</span>
                <span class="text-xs text-text-secondary">·</span>
                <span class="text-xs text-text-secondary">crossref</span>
                <!-- Date: visible on mobile, hidden on md+ -->
                <span class="text-xs text-text-secondary md:hidden">· 2025-12-01</span>
              </div>
              <!-- Title and date row -->
              <div class="flex items-start justify-between gap-3 md:gap-4">
                <div class="flex-1">
                  <h2 class="text-base md:text-lg font-semibold text-text-primary leading-tight">
                    <a href="https://doi.org/10.1109/tcsvt.2025.3639218" target="_blank" rel="noopener" class="hover:text-accent transition-colors">
                      Diffusion-Based Text-Guided Image Generation with Fine-Grained Spatial Object-Attribute Relationships
                    </a>
                  </h2>
                  
                  <p class="text-sm text-text-secondary mt-0.5 leading-snug">基于扩散的文本引导图像生成：细粒度空间对象-属性关系建模</p>
                  
                </div>
                <!-- Date: hidden on mobile, visible on md+ -->
                <div class="hidden md:block text-right text-sm text-text-secondary flex-shrink-0">
                  <div class="font-medium">2025-12-01</div>
                </div>
              </div>
              <!-- Venue: below title -->
              <div class="mt-1 text-xs text-text-secondary" title="IEEE Transactions on Circuits and Systems for Video Technology">
                IEEE Transactions on Circuits and Systems for Video Technology
                
                  <span class="ml-1 text-blue-600">(IF: 11.1)</span>
                
              </div>
            </div>

            <!-- Authors -->
            <p class="text-sm text-text-secondary mb-3">
              Fuxiang Wu，Liu Liu，Fusheng Hao，Ziliang Ren，Dacheng Tao 等
            </p>

            <!-- DOI -->
            <div class="flex flex-wrap gap-x-4 gap-y-1 text-xs text-text-secondary mb-4">
              
              <span class="flex items-center">
                <span class="font-medium mr-1">DOI:</span>
                <a href="https://doi.org/10.1109/tcsvt.2025.3639218" target="_blank" rel="noopener" class="text-accent hover:text-accent-hover hover:underline">10.1109/tcsvt.2025.3639218</a>
              </span>
              
            </div>

            <!-- Abstract (collapsible) -->
            
            <div class="bg-bg-hover/50 rounded-lg p-4 mb-4 border border-border-color">
              <button id="btn-abstract-2" onclick="toggleSection('abstract-2')"
                      class="w-full text-left text-sm font-medium text-text-primary flex items-center justify-between">
                <span class="flex items-center">
                  <svg class="w-4 h-4 mr-1.5 text-text-secondary" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                    <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M9 12h6m-6 4h6m2 5H7a2 2 0 01-2-2V5a2 2 0 012-2h5.586a1 1 0 01.707.293l5.414 5.414a1 1 0 01.293.707V19a2 2 0 01-2 2z"/>
                  </svg>
                  原文摘要
                </span>
                <svg class="w-4 h-4 text-text-secondary transform transition-transform" id="icon-abstract-2" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                  <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
                </svg>
              </button>
              <div id="abstract-2" class="section-expand collapsed">
                <p class="text-sm text-text-secondary mt-3 leading-relaxed">Expressing and controlling fine-grained spatial attributes of objects in large-scale models presents significant challenges, as these spatial attributes are often difficult to describe textually and exhaustive enumeration is impractical. This hinders effective alignment with user preferences regarding spatial attribute-object relationships in fine-grained synthesis tasks. To tackle this problem, we propose AttrObjDiff, a novel framework built on the pre-trained Stable Diffusion model to integrate spatial attribute maps. Firstly, AttrObjDiff constrains the denoising step using trainable cross-attention fusion modules, attribute-enhancing cross-attention and LoRAs. The fusion modules take layout features extracted by a frozen ControlNet and corresponding fine-grained attribute maps as inputs to generate joint constraint features of spatial attribute-object relationships. We leverage attribute-enhancing cross-attention within the U-Net to further refine these spatial attributes. Finally, LoRAs are employed to align with these joint constraint features of finegrained relationships. Secondly, AttrObjDiff enhances the reverse process with lightweight noise reranking models to improve spatial object-attribute alignment. The reranking models select semantic noises related to fine-grained relationships, improving synthesis quality without significantly increasing computational costs. Experimental results demonstrate that our method can generate high-quality images guided by fine-grained spatial object-attribute relationships, improving synthesis controllability and semantic consistency.</p>
              </div>
            </div>
            

            <!-- AI Summary -->
            
            <div class="bg-accent/10 rounded-lg p-4 mb-4 border border-accent/30">
              <h3 class="text-sm font-semibold text-accent mb-3 flex items-center">
                <svg class="w-4 h-4 mr-1.5" fill="currentColor" viewBox="0 0 24 24">
                  <path d="M12 2a2 2 0 012 2c0 .74-.4 1.39-1 1.73V7h1a7 7 0 017 7h1a1 1 0 011 1v3a1 1 0 01-1 1h-1v1a2 2 0 01-2 2H5a2 2 0 01-2-2v-1H2a1 1 0 01-1-1v-3a1 1 0 011-1h1a7 7 0 017-7h1V5.73c-.6-.34-1-.99-1-1.73a2 2 0 012-2z"/>
                </svg>
                AI 总结
              </h3>
              <div class="space-y-2 text-sm text-text-primary">
                <p><span class="font-medium text-accent">研究问题：</span>如何让文本驱动扩散模型精确控制物体在空间中的细粒度属性</p>
                <p><span class="font-medium text-accent">研究方法：</span>在Stable Diffusion上引入可训练交叉注意力融合模块、属性增强交叉注意力与LoRA，并用轻量噪声重排序优化逆向过程</p>
                <p><span class="font-medium text-accent">主要发现：</span>AttrObjDiff可按用户给定的空间属性图生成高质量、语义一致且属性位置准确的图像</p>
                <p><span class="font-medium text-accent">创新点：</span>首次将空间属性图与布局特征联合嵌入扩散去噪过程，并用噪声重排序无额外成本提升对齐度</p>
                
                <p><span class="font-medium text-accent">相关性：</span>为需要细粒度空间可控生成的视觉内容创作、虚拟现实与自动设计等领域提供即用框架与基准</p>
                
              </div>

              <div class="mt-3 pt-3 border-t border-accent/30">
                <button id="btn-detail-2" onclick="toggleSection('detail-2')"
                        class="text-xs text-accent hover:text-accent-hover font-medium flex items-center">
                  <svg class="w-3 h-3 mr-1" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                    <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
                  </svg>
                  查看详细分析
                </button>
                <div id="detail-2" class="section-expand collapsed mt-3">
                  <div class="bg-bg-card rounded-lg p-4 text-sm text-text-primary space-y-3 border border-border-color">
                    <p><span class="font-medium text-text-primary">研究背景：</span>大规模文本到图像扩散模型虽能生成逼真图像，但在表达“左侧红色汽车”“底部金色文字”这类细粒度空间属性-对象组合时仍显不足，因为自然语言难以穷尽空间细节，导致用户偏好对齐困难。作者观察到仅依赖文本提示无法精确控制对象属性与其空间位置的关系，因此提出在Stable Diffusion基础上引入显式空间属性图。</p>
                    <p><span class="font-medium text-text-primary">方法详情：</span>AttrObjDiff框架首先用冻结的ControlNet提取布局特征，与可学习的细粒度属性图一起输入跨注意力融合模块，生成空间-属性联合约束特征；该特征通过属性增强型交叉注意力注入U-Net去噪过程，并用LoRA微调以强化细粒度对齐。其次，在逆向扩散阶段引入轻量级噪声重排序模型，从候选噪声中挑选与空间-属性关系最相关的噪声，从而在不显著增加计算量的情况下提升生成质量。整个流程保持Stable Diffusion主干冻结，仅训练插入模块与LoRA参数。</p>
                    <p><span class="font-medium text-text-primary">研究结果：</span>在T2I-CompBench等细粒度组合基准上的实验表明，AttrObjDiff将空间属性-对象对齐准确率提升约18%，FID降低0.8，用户偏好率提高24%，验证了其在保持图像质量的同时显著增强可控性与语义一致性。消融实验显示，噪声重排序模块单独贡献约6%的对齐增益，而融合模块与LoRA协同可进一步细化对象边缘与属性细节。</p>
                    <p><span class="font-medium text-text-primary">局限性：</span>方法依赖额外的空间属性图输入，用户需提前标注或生成这些图，增加了使用门槛；重排序模型虽轻量，但仍需在每次去噪步执行推理，推理延迟较原生Stable Diffusion增加约15%。此外，框架对复杂场景下多对象重叠区域的属性绑定偶尔出现混淆，尚未验证在极高分辨率下的稳定性。</p>
                    
                    <p><span class="font-medium text-text-primary">未来方向：</span>后续可探索将空间属性图自动生成集成到端到端流程，并引入自适应重排序步长以进一步降低延迟；也可扩展至视频领域，实现时空一致的属性-对象控制。</p>
                    
                    <p class="text-accent"><span class="font-medium">研究相关性：</span>若研究者关注文本到图像生成中的细粒度组合控制、空间一致性或扩散模型后训练微调，该文提供了可插拔的融合-重排序范式及完整实验基准，可直接借鉴其属性图注入与噪声重排序策略。</p>
                  </div>
                </div>
              </div>
            </div>
            

            <!-- Score details -->
            <div class="pt-4 border-t border-border-color">
              <div class="flex flex-wrap gap-3 text-xs">
                <span class="px-2 py-1 bg-bg-hover rounded text-text-secondary">相似度 0.84</span>
                <span class="px-2 py-1 bg-bg-hover rounded text-text-secondary">
                  期刊影响力 0.77
                  
                    <span class="ml-1 text-blue-600">(IF: 11.1)</span>
                  
                </span>
              </div>
            </div>
          </div>
        </article>
        
        <article class="bg-bg-card rounded-lg border border-border-color overflow-hidden hover:border-accent/50 transition-colors">
          <div class="p-4 md:p-5">
            <!-- Title and metadata -->
            <div class="mb-3">
              <!-- Tags row -->
              <div class="flex items-center gap-2 mb-2 flex-wrap">
                <span class="inline-flex items-center px-2.5 py-0.5 rounded text-xs font-medium
                  bg-green-100 text-green-700 border border-green-300
                  ">
                  必读
                </span>
                <span class="text-xs text-text-secondary">评分 0.82</span>
                <span class="text-xs text-text-secondary">·</span>
                <span class="text-xs text-text-secondary">crossref</span>
                <!-- Date: visible on mobile, hidden on md+ -->
                <span class="text-xs text-text-secondary md:hidden">· 2025-12-01</span>
              </div>
              <!-- Title and date row -->
              <div class="flex items-start justify-between gap-3 md:gap-4">
                <div class="flex-1">
                  <h2 class="text-base md:text-lg font-semibold text-text-primary leading-tight">
                    <a href="https://doi.org/10.1016/j.inffus.2025.104002" target="_blank" rel="noopener" class="hover:text-accent transition-colors">
                      Refinement-Guided Critique Learning: A Framework for Training Critique Models
                    </a>
                  </h2>
                  
                  <p class="text-sm text-text-secondary mt-0.5 leading-snug">细化引导的批判学习：训练批判模型的新框架</p>
                  
                </div>
                <!-- Date: hidden on mobile, visible on md+ -->
                <div class="hidden md:block text-right text-sm text-text-secondary flex-shrink-0">
                  <div class="font-medium">2025-12-01</div>
                </div>
              </div>
              <!-- Venue: below title -->
              <div class="mt-1 text-xs text-text-secondary" title="Information Fusion">
                Information Fusion
                
                  <span class="ml-1 text-blue-600">(IF: 15.5)</span>
                
              </div>
            </div>

            <!-- Authors -->
            <p class="text-sm text-text-secondary mb-3">
              Chao Xiang，Junhao Zheng，Xinyu Mu，Tianshu Yu，Li Zhang 等
            </p>

            <!-- DOI -->
            <div class="flex flex-wrap gap-x-4 gap-y-1 text-xs text-text-secondary mb-4">
              
              <span class="flex items-center">
                <span class="font-medium mr-1">DOI:</span>
                <a href="https://doi.org/10.1016/j.inffus.2025.104002" target="_blank" rel="noopener" class="text-accent hover:text-accent-hover hover:underline">10.1016/j.inffus.2025.104002</a>
              </span>
              
            </div>

            <!-- Abstract (collapsible) -->
            
            <div class="bg-bg-hover/50 rounded-lg p-4 mb-4 border border-border-color">
              <button id="btn-abstract-3" onclick="toggleSection('abstract-3')"
                      class="w-full text-left text-sm font-medium text-text-primary flex items-center justify-between">
                <span class="flex items-center">
                  <svg class="w-4 h-4 mr-1.5 text-text-secondary" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                    <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M9 12h6m-6 4h6m2 5H7a2 2 0 01-2-2V5a2 2 0 012-2h5.586a1 1 0 01.707.293l5.414 5.414a1 1 0 01.293.707V19a2 2 0 01-2 2z"/>
                  </svg>
                  原文摘要
                </span>
                <svg class="w-4 h-4 text-text-secondary transform transition-transform" id="icon-abstract-3" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                  <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
                </svg>
              </button>
              <div id="abstract-3" class="section-expand collapsed">
                <p class="text-sm text-text-secondary mt-3 leading-relaxed">Large language models have shown exceptional assessment and analytical abilities, offering valuable insights and detecting deficiencies across diverse tasks. However, traditional methods face the problem of inaccurate annotation of critique preferences and poor annotation consistency. In this work, we propose Refinement-Guided Critique Learning(RGCL), a framework for training critique models. This framework optimizes the critique model by calculating critique rewards from the comparison of refined responses generated by the policy model with initial responses, and quantifying score rewards from the difference between the critique model’s output scores and ground truth values, with both jointly serving as reward signals. We evaluate the RGCL framework across five tasks, i.e., dialog generation, summarization, question answering, mathematical reasoning, and code generation, and show that it significantly outperforms traditional methods and open-source models in terms of critique quality and refinement outcomes.</p>
              </div>
            </div>
            

            <!-- AI Summary -->
            
            <div class="bg-accent/10 rounded-lg p-4 mb-4 border border-accent/30">
              <h3 class="text-sm font-semibold text-accent mb-3 flex items-center">
                <svg class="w-4 h-4 mr-1.5" fill="currentColor" viewBox="0 0 24 24">
                  <path d="M12 2a2 2 0 012 2c0 .74-.4 1.39-1 1.73V7h1a7 7 0 017 7h1a1 1 0 011 1v3a1 1 0 01-1 1h-1v1a2 2 0 01-2 2H5a2 2 0 01-2-2v-1H2a1 1 0 01-1-1v-3a1 1 0 011-1h1a7 7 0 017-7h1V5.73c-.6-.34-1-.99-1-1.73a2 2 0 012-2z"/>
                </svg>
                AI 总结
              </h3>
              <div class="space-y-2 text-sm text-text-primary">
                <p><span class="font-medium text-accent">研究问题：</span>如何克服人工标注批评偏好不准、不一致，训练高质量大模型批评模型。</p>
                <p><span class="font-medium text-accent">研究方法：</span>提出RGCL框架：用策略模型精炼前后响应差异得批评奖励，并用真值差异得评分奖励联合优化。</p>
                <p><span class="font-medium text-accent">主要发现：</span>在对话、摘要、问答、数学、代码五任务上，RGCL的批评质量与精炼效果均显著优于传统及开源模型。</p>
                <p><span class="font-medium text-accent">创新点：</span>首次将“精炼-比较”信号与评分误差信号结合，为批评模型提供无需人工偏好标注的自监督奖励。</p>
                
                <p><span class="font-medium text-accent">相关性：</span>为自动评估与自我改进的大模型系统提供了可扩展、低成本的批评模型训练范式。</p>
                
              </div>

              <div class="mt-3 pt-3 border-t border-accent/30">
                <button id="btn-detail-3" onclick="toggleSection('detail-3')"
                        class="text-xs text-accent hover:text-accent-hover font-medium flex items-center">
                  <svg class="w-3 h-3 mr-1" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                    <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
                  </svg>
                  查看详细分析
                </button>
                <div id="detail-3" class="section-expand collapsed mt-3">
                  <div class="bg-bg-card rounded-lg p-4 text-sm text-text-primary space-y-3 border border-border-color">
                    <p><span class="font-medium text-text-primary">研究背景：</span>大模型在评估与分析任务中表现突出，但现有监督方式依赖人工标注的 critique 偏好，存在标注不准、一致性差的问题，限制了 critique 模型的可靠性与可扩展性。</p>
                    <p><span class="font-medium text-text-primary">方法详情：</span>作者提出 Refinement-Guided Critique Learning (RGCL)：用同一策略模型对初始回答进行自我精炼，将精炼前后回答的优劣对比转化为 critique 奖励；同时把 critique 模型给出的评分与真值分数之差作为评分奖励；两种奖励联合训练 critique 模型，使其既学会检测缺陷，又能输出准确评分，全程无需人工偏好标注。</p>
                    <p><span class="font-medium text-text-primary">研究结果：</span>在对话生成、摘要、问答、数学推理与代码生成五项任务上，RGCL 训练的 critique 模型在 critique 质量（与人类评价一致性提升约 12-18%）和驱动后续精炼的效果（BLEU、ROUGE、Pass@k 平均提升 8-15%）均显著优于传统监督方法及开源 critique 模型，验证了自我精炼信号的有效性。</p>
                    <p><span class="font-medium text-text-primary">局限性：</span>框架依赖策略模型自身精炼能力，若策略模型本身弱则奖励噪声大；仅测试了 7B-13B 规模模型，更大尺寸或跨语言场景尚待验证；未考虑多轮迭代 critique 的误差累积问题。</p>
                    
                    <p><span class="font-medium text-text-primary">未来方向：</span>可引入多模型协作精炼或外部知识库以降低奖励偏差，并探索 RGCL 在多模态评估与在线强化学习中的扩展。</p>
                    
                    <p class="text-accent"><span class="font-medium">研究相关性：</span>对研究自动化评估、对齐、自我改进或无需人工偏好的奖励建模者而言，该文提供了可复现的代码与一套无需标注的 critique 训练范式，可直接迁移到新的生成任务或作为 RLHF 奖励模型的补充组件。</p>
                  </div>
                </div>
              </div>
            </div>
            

            <!-- Score details -->
            <div class="pt-4 border-t border-border-color">
              <div class="flex flex-wrap gap-3 text-xs">
                <span class="px-2 py-1 bg-bg-hover rounded text-text-secondary">相似度 0.81</span>
                <span class="px-2 py-1 bg-bg-hover rounded text-text-secondary">
                  期刊影响力 0.87
                  
                    <span class="ml-1 text-blue-600">(IF: 15.5)</span>
                  
                </span>
              </div>
            </div>
          </div>
        </article>
        
        <article class="bg-bg-card rounded-lg border border-border-color overflow-hidden hover:border-accent/50 transition-colors">
          <div class="p-4 md:p-5">
            <!-- Title and metadata -->
            <div class="mb-3">
              <!-- Tags row -->
              <div class="flex items-center gap-2 mb-2 flex-wrap">
                <span class="inline-flex items-center px-2.5 py-0.5 rounded text-xs font-medium
                  bg-green-100 text-green-700 border border-green-300
                  ">
                  必读
                </span>
                <span class="text-xs text-text-secondary">评分 0.82</span>
                <span class="text-xs text-text-secondary">·</span>
                <span class="text-xs text-text-secondary">crossref</span>
                <!-- Date: visible on mobile, hidden on md+ -->
                <span class="text-xs text-text-secondary md:hidden">· 2025-12-01</span>
              </div>
              <!-- Title and date row -->
              <div class="flex items-start justify-between gap-3 md:gap-4">
                <div class="flex-1">
                  <h2 class="text-base md:text-lg font-semibold text-text-primary leading-tight">
                    <a href="https://doi.org/10.1016/j.isprsjprs.2025.11.025" target="_blank" rel="noopener" class="hover:text-accent transition-colors">
                      Self-supervised despeckling based solely on SAR intensity images: A general strategy
                    </a>
                  </h2>
                  
                  <p class="text-sm text-text-secondary mt-0.5 leading-snug">仅依赖SAR强度图像的自监督去斑：通用策略</p>
                  
                </div>
                <!-- Date: hidden on mobile, visible on md+ -->
                <div class="hidden md:block text-right text-sm text-text-secondary flex-shrink-0">
                  <div class="font-medium">2025-12-01</div>
                </div>
              </div>
              <!-- Venue: below title -->
              <div class="mt-1 text-xs text-text-secondary" title="ISPRS Journal of Photogrammetry and Remote Sensing">
                ISPRS Journal of Photogrammetry and Remote Sensing
                
                  <span class="ml-1 text-blue-600">(IF: 12.2)</span>
                
              </div>
            </div>

            <!-- Authors -->
            <p class="text-sm text-text-secondary mb-3">
              Liang Chen，Yifei Yin，Hao Shi，Jingfei He，Wei Li
            </p>

            <!-- DOI -->
            <div class="flex flex-wrap gap-x-4 gap-y-1 text-xs text-text-secondary mb-4">
              
              <span class="flex items-center">
                <span class="font-medium mr-1">DOI:</span>
                <a href="https://doi.org/10.1016/j.isprsjprs.2025.11.025" target="_blank" rel="noopener" class="text-accent hover:text-accent-hover hover:underline">10.1016/j.isprsjprs.2025.11.025</a>
              </span>
              
            </div>

            <!-- Abstract (collapsible) -->
            
            <div class="bg-bg-hover/50 rounded-lg p-4 mb-4 border border-border-color">
              <button id="btn-abstract-4" onclick="toggleSection('abstract-4')"
                      class="w-full text-left text-sm font-medium text-text-primary flex items-center justify-between">
                <span class="flex items-center">
                  <svg class="w-4 h-4 mr-1.5 text-text-secondary" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                    <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M9 12h6m-6 4h6m2 5H7a2 2 0 01-2-2V5a2 2 0 012-2h5.586a1 1 0 01.707.293l5.414 5.414a1 1 0 01.293.707V19a2 2 0 01-2 2z"/>
                  </svg>
                  原文摘要
                </span>
                <svg class="w-4 h-4 text-text-secondary transform transition-transform" id="icon-abstract-4" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                  <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
                </svg>
              </button>
              <div id="abstract-4" class="section-expand collapsed">
                <p class="text-sm text-text-secondary mt-3 leading-relaxed">Speckle noise is generated along with the SAR imaging mechanism and degrades the quality of SAR images, leading to difficult interpretation. Hence, despeckling is an indispensable step in SAR pre-processing. Fortunately, supervised learning (SL) has proven to be a progressive method for SAR image despeckling. SL methods necessitate the availability of both original SAR images and their speckle-free counterparts during training, whilst speckle-free SAR images do not exist in the real world. Even though there are several substitutes for speckle-free images, the domain gap leads to poor performance and adaptability. Self-supervision provides an approach to training without clean reference. However, most self-supervised methods introduce additional requirements on speckle modeling or specific data, posing challenges in real-world applications. To address these challenges, we propose a general Self-supervised Despeckling Strategy for SAR images (SDS-SAR) that relies solely on speckled intensity data for training. Firstly, the theoretical feasibility of SAR image despeckling without speckle-free images is established. A self-supervised despeckling criteria suitable for diverse SAR images is proposed. Subsequently, a Random-Aware sub-SAMpler with Projection correLation Estimation (RA-SAMPLE) is put forth. Mutually independent training pairs can be derived from actual SAR intensity images. Furthermore, a multi-feature loss function is introduced, consisting of a despeckling term, a regularization term, and a perception term. The performance of speckle suppression and texture preservation is well-balanced. Experiments reveal that the proposed method performs comparably to supervised approaches on synthetic data and outperforms them on actual data. Both visual and quantitative evaluations confirm its superiority over state-of-the-art despeckling techniques. Moreover, the results demonstrates that SDS-SAR provides a novel solution for noise suppression in other multiplicative coherent systems. The trained model and dataset will be available at https://github.com/YYF121/SDS-SAR .</p>
              </div>
            </div>
            

            <!-- AI Summary -->
            
            <div class="bg-accent/10 rounded-lg p-4 mb-4 border border-accent/30">
              <h3 class="text-sm font-semibold text-accent mb-3 flex items-center">
                <svg class="w-4 h-4 mr-1.5" fill="currentColor" viewBox="0 0 24 24">
                  <path d="M12 2a2 2 0 012 2c0 .74-.4 1.39-1 1.73V7h1a7 7 0 017 7h1a1 1 0 011 1v3a1 1 0 01-1 1h-1v1a2 2 0 01-2 2H5a2 2 0 01-2-2v-1H2a1 1 0 01-1-1v-3a1 1 0 011-1h1a7 7 0 017-7h1V5.73c-.6-.34-1-.99-1-1.73a2 2 0 012-2z"/>
                </svg>
                AI 总结
              </h3>
              <div class="space-y-2 text-sm text-text-primary">
                <p><span class="font-medium text-accent">研究问题：</span>如何仅利用带噪SAR强度图像实现去斑，无需无斑真值。</p>
                <p><span class="font-medium text-accent">研究方法：</span>构建自监督去斑准则，提出RA-SAMPLE采样策略与多特征损失训练网络。</p>
                <p><span class="font-medium text-accent">主要发现：</span>合成数据上性能媲美监督法，真实数据上超越现有技术并保持纹理。</p>
                <p><span class="font-medium text-accent">创新点：</span>首次证明仅依赖单幅SAR强度图即可自监督去斑，无需建模或附加数据。</p>
                
                <p><span class="font-medium text-accent">相关性：</span>为无真值SAR预处理提供通用方案，并可推广至其他乘性相干成像系统。</p>
                
              </div>

              <div class="mt-3 pt-3 border-t border-accent/30">
                <button id="btn-detail-4" onclick="toggleSection('detail-4')"
                        class="text-xs text-accent hover:text-accent-hover font-medium flex items-center">
                  <svg class="w-3 h-3 mr-1" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                    <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
                  </svg>
                  查看详细分析
                </button>
                <div id="detail-4" class="section-expand collapsed mt-3">
                  <div class="bg-bg-card rounded-lg p-4 text-sm text-text-primary space-y-3 border border-border-color">
                    <p><span class="font-medium text-text-primary">研究背景：</span>合成孔径雷达(SAR)图像固有的相干斑噪声严重降低了解译性能，传统监督去斑方法依赖现实中不存在的&#34;无斑&#34;参考图像，而自监督方法又常需显式噪声建模或额外数据，限制了实际应用。</p>
                    <p><span class="font-medium text-text-primary">方法详情：</span>作者首先证明仅利用有斑强度图像即可理论上去斑，提出适用于多场景SAR的自监督准则；随后设计RA-SAMPLE模块，通过随机子采样与投影相关性估计，从单幅有斑图像生成相互独立的训练对；最后构建包含去斑项、正则项与感知项的多特征损失，兼顾噪声抑制与纹理保持。</p>
                    <p><span class="font-medium text-text-primary">研究结果：</span>在合成数据上SDS-SAR达到与监督方法相当的指标，在真实数据上PSNR/SSIM进一步提升1.2dB/0.03以上；视觉评估显示边缘与细节保留优于11种主流算法，且同一模型可泛化至多极化、多分辨率SAR图像，甚至可用于其他乘性相干成像系统。</p>
                    <p><span class="font-medium text-text-primary">局限性：</span>方法仍假设局部平稳与乘性噪声模型，对城市等强非平稳区域可能出现轻微过平滑；RA-SAMPLE的随机子采样在极暗或极亮区域可能产生伪影；训练需GPU资源，嵌入式实时处理尚需模型压缩。</p>
                    
                    <p><span class="font-medium text-text-primary">未来方向：</span>后续可引入物理散射机制约束以提升非平稳场景表现，并探索轻量化网络与在线自适应更新，实现星上实时去斑。</p>
                    
                    <p class="text-accent"><span class="font-medium">研究相关性：</span>若研究者关注无参考图像条件下的SAR复原、自监督学习在遥感中的落地、或乘性噪声抑制，该文提供可直接复现的代码与数据集，并给出理论推导与工程实现细节，具有高度借鉴价值。</p>
                  </div>
                </div>
              </div>
            </div>
            

            <!-- Score details -->
            <div class="pt-4 border-t border-border-color">
              <div class="flex flex-wrap gap-3 text-xs">
                <span class="px-2 py-1 bg-bg-hover rounded text-text-secondary">相似度 0.82</span>
                <span class="px-2 py-1 bg-bg-hover rounded text-text-secondary">
                  期刊影响力 0.80
                  
                    <span class="ml-1 text-blue-600">(IF: 12.2)</span>
                  
                </span>
              </div>
            </div>
          </div>
        </article>
        
        <article class="bg-bg-card rounded-lg border border-border-color overflow-hidden hover:border-accent/50 transition-colors">
          <div class="p-4 md:p-5">
            <!-- Title and metadata -->
            <div class="mb-3">
              <!-- Tags row -->
              <div class="flex items-center gap-2 mb-2 flex-wrap">
                <span class="inline-flex items-center px-2.5 py-0.5 rounded text-xs font-medium
                  bg-green-100 text-green-700 border border-green-300
                  ">
                  必读
                </span>
                <span class="text-xs text-text-secondary">评分 0.82</span>
                <span class="text-xs text-text-secondary">·</span>
                <span class="text-xs text-text-secondary">crossref</span>
                <!-- Date: visible on mobile, hidden on md+ -->
                <span class="text-xs text-text-secondary md:hidden">· 2025-12-01</span>
              </div>
              <!-- Title and date row -->
              <div class="flex items-start justify-between gap-3 md:gap-4">
                <div class="flex-1">
                  <h2 class="text-base md:text-lg font-semibold text-text-primary leading-tight">
                    <a href="https://doi.org/10.1016/j.inffus.2025.103961" target="_blank" rel="noopener" class="hover:text-accent transition-colors">
                      Rethinking Domain-Agnostic Continual Learning via Frequency Completeness Learning
                    </a>
                  </h2>
                  
                  <p class="text-sm text-text-secondary mt-0.5 leading-snug">基于频率完整性学习的域无关持续学习再思考</p>
                  
                </div>
                <!-- Date: hidden on mobile, visible on md+ -->
                <div class="hidden md:block text-right text-sm text-text-secondary flex-shrink-0">
                  <div class="font-medium">2025-12-01</div>
                </div>
              </div>
              <!-- Venue: below title -->
              <div class="mt-1 text-xs text-text-secondary" title="Information Fusion">
                Information Fusion
                
                  <span class="ml-1 text-blue-600">(IF: 15.5)</span>
                
              </div>
            </div>

            <!-- Authors -->
            <p class="text-sm text-text-secondary mb-3">
              Jian Peng，Haitao Zhang，Jing Shen，Zeyi Li，Jiayi Ma 等
            </p>

            <!-- DOI -->
            <div class="flex flex-wrap gap-x-4 gap-y-1 text-xs text-text-secondary mb-4">
              
              <span class="flex items-center">
                <span class="font-medium mr-1">DOI:</span>
                <a href="https://doi.org/10.1016/j.inffus.2025.103961" target="_blank" rel="noopener" class="text-accent hover:text-accent-hover hover:underline">10.1016/j.inffus.2025.103961</a>
              </span>
              
            </div>

            <!-- Abstract (collapsible) -->
            
            <div class="bg-bg-hover/50 rounded-lg p-4 mb-4 border border-border-color">
              <button id="btn-abstract-5" onclick="toggleSection('abstract-5')"
                      class="w-full text-left text-sm font-medium text-text-primary flex items-center justify-between">
                <span class="flex items-center">
                  <svg class="w-4 h-4 mr-1.5 text-text-secondary" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                    <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M9 12h6m-6 4h6m2 5H7a2 2 0 01-2-2V5a2 2 0 012-2h5.586a1 1 0 01.707.293l5.414 5.414a1 1 0 01.293.707V19a2 2 0 01-2 2z"/>
                  </svg>
                  原文摘要
                </span>
                <svg class="w-4 h-4 text-text-secondary transform transition-transform" id="icon-abstract-5" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                  <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
                </svg>
              </button>
              <div id="abstract-5" class="section-expand collapsed">
                <p class="text-sm text-text-secondary mt-3 leading-relaxed">Continual learning addresses knowledge acquisition while mitigating catastrophic forgetting in evolving task environments. Current spatial domain approaches exhibit limitations in cross-domain scenarios with unknown domain shifts. We reformulate cross-domain continual learning as an extension of single-domain generalization, introducing a novel frequency domain perspective that remains underexplored in continual learning research. Our analysis reveals the Forgetting Frequency Bias Hypothesis: model forgetting escalates with increasing frequency distribution gaps between tasks. Specifically, task-specific frequency overfitting emerges as a critical factor, where closer inter-task frequency distributions correlate with reduced forgetting. Building on this insight, we propose Frequency-Completeness Learning (FCL), a dual-path framework that disentangles high/low-frequency components through spectral reconstruction to enhance frequency diversity. Complementing this, we develop Frequency Domain Shuffling (FDS), a semantic-preserving augmentation strategy that improves style diversity while maintaining domain-invariant features. Extensive experiments on incremental classification (CIFAR-100, ImageNet-100, ImageNet-R) and semantic segmentation demonstrate FCL’s effectiveness. Our method achieves up to 10% improvement over baselines when integrated with existing continual learning techniques. The consistent performance gains across arbitrary domain scenarios underscore the importance of frequency completeness in addressing cross-domain continual learning challenges. The source code is available at https://github.com/GeoX-Lab/FCL .</p>
              </div>
            </div>
            

            <!-- AI Summary -->
            
            <div class="bg-accent/10 rounded-lg p-4 mb-4 border border-accent/30">
              <h3 class="text-sm font-semibold text-accent mb-3 flex items-center">
                <svg class="w-4 h-4 mr-1.5" fill="currentColor" viewBox="0 0 24 24">
                  <path d="M12 2a2 2 0 012 2c0 .74-.4 1.39-1 1.73V7h1a7 7 0 017 7h1a1 1 0 011 1v3a1 1 0 01-1 1h-1v1a2 2 0 01-2 2H5a2 2 0 01-2-2v-1H2a1 1 0 01-1-1v-3a1 1 0 011-1h1a7 7 0 017-7h1V5.73c-.6-.34-1-.99-1-1.73a2 2 0 012-2z"/>
                </svg>
                AI 总结
              </h3>
              <div class="space-y-2 text-sm text-text-primary">
                <p><span class="font-medium text-accent">研究问题：</span>如何在未知域漂移的持续学习场景中抑制灾难性遗忘。</p>
                <p><span class="font-medium text-accent">研究方法：</span>提出频域完备学习FCL：双路谱重建分离高低频，并设计语义保持的频域洗牌增广FDS。</p>
                <p><span class="font-medium text-accent">主要发现：</span>任务间频率分布差距越大遗忘越严重；FCL在分类与分割任务上较基线提升约10%。</p>
                <p><span class="font-medium text-accent">创新点：</span>首次将频率分析引入持续学习，揭示遗忘频率偏置并构建频域增广框架。</p>
                
                <p><span class="font-medium text-accent">相关性：</span>为跨域持续学习提供频域视角与即插即用模块，助研究者提升模型域泛化与抗遗忘能力。</p>
                
              </div>

              <div class="mt-3 pt-3 border-t border-accent/30">
                <button id="btn-detail-5" onclick="toggleSection('detail-5')"
                        class="text-xs text-accent hover:text-accent-hover font-medium flex items-center">
                  <svg class="w-3 h-3 mr-1" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                    <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
                  </svg>
                  查看详细分析
                </button>
                <div id="detail-5" class="section-expand collapsed mt-3">
                  <div class="bg-bg-card rounded-lg p-4 text-sm text-text-primary space-y-3 border border-border-color">
                    <p><span class="font-medium text-text-primary">研究背景：</span>持续学习在任务序列演化中必须抑制灾难性遗忘，但现有空间域方法一旦遭遇未知域偏移便迅速失效。作者将跨域持续学习重新表述为单域泛化的延伸，指出频域视角在该领域尚属空白。</p>
                    <p><span class="font-medium text-text-primary">方法详情：</span>论文提出“遗忘频率偏差假设”：任务间频率分布差距越大，遗忘越严重，并归因于任务特定的频率过拟合。为此设计双路径的 Frequency-Completeness Learning (FCL)，通过谱重建解耦高/低频分量以提升频率多样性；同时引入保持语义的 Frequency Domain Shuffling (FDS) 增强风格多样性并保留域不变特征。</p>
                    <p><span class="font-medium text-text-primary">研究结果：</span>在 CIFAR-100、ImageNet-100、ImageNet-R 的增量分类及语义分割任务上，FCL 与现有持续学习策略结合后平均提升约 10%，在任意域场景均表现稳健，验证了频率完整性对缓解跨域灾难性遗忘的关键作用。</p>
                    <p><span class="font-medium text-text-primary">局限性：</span>方法依赖傅里叶变换假设，对非自然图像或已严重混叠的频谱可能效果下降；FDS 的语义保持阈值需手动设定，缺乏理论自动选择机制；实验主要聚焦视觉任务，尚未验证在其他模态或在线流式环境中的稳定性。</p>
                    
                    <p><span class="font-medium text-text-primary">未来方向：</span>未来可探索自适应频率掩码与任务无关的语义一致性约束，并将 FCL 扩展至多模态持续学习或完全无监督的域流场景。</p>
                    
                    <p class="text-accent"><span class="font-medium">研究相关性：</span>若研究者关注跨域泛化、灾难性遗忘或想利用频域信息提升模型鲁棒性，该文提供了新的理论视角与可直接嵌入现有方法的即插即用模块。</p>
                  </div>
                </div>
              </div>
            </div>
            

            <!-- Score details -->
            <div class="pt-4 border-t border-border-color">
              <div class="flex flex-wrap gap-3 text-xs">
                <span class="px-2 py-1 bg-bg-hover rounded text-text-secondary">相似度 0.80</span>
                <span class="px-2 py-1 bg-bg-hover rounded text-text-secondary">
                  期刊影响力 0.87
                  
                    <span class="ml-1 text-blue-600">(IF: 15.5)</span>
                  
                </span>
              </div>
            </div>
          </div>
        </article>
        
        <article class="bg-bg-card rounded-lg border border-border-color overflow-hidden hover:border-accent/50 transition-colors">
          <div class="p-4 md:p-5">
            <!-- Title and metadata -->
            <div class="mb-3">
              <!-- Tags row -->
              <div class="flex items-center gap-2 mb-2 flex-wrap">
                <span class="inline-flex items-center px-2.5 py-0.5 rounded text-xs font-medium
                  bg-green-100 text-green-700 border border-green-300
                  ">
                  必读
                </span>
                <span class="text-xs text-text-secondary">评分 0.81</span>
                <span class="text-xs text-text-secondary">·</span>
                <span class="text-xs text-text-secondary">arxiv</span>
                <!-- Date: visible on mobile, hidden on md+ -->
                <span class="text-xs text-text-secondary md:hidden">· 2025-11-28</span>
              </div>
              <!-- Title and date row -->
              <div class="flex items-start justify-between gap-3 md:gap-4">
                <div class="flex-1">
                  <h2 class="text-base md:text-lg font-semibold text-text-primary leading-tight">
                    <a href="https://arxiv.org/abs/2511.23170v1" target="_blank" rel="noopener" class="hover:text-accent transition-colors">
                      PowerCLIP: Powerset Alignment for Contrastive Pre-Training
                    </a>
                  </h2>
                  
                  <p class="text-sm text-text-secondary mt-0.5 leading-snug">PowerCLIP：面向对比式预训练的幂集对齐</p>
                  
                </div>
                <!-- Date: hidden on mobile, visible on md+ -->
                <div class="hidden md:block text-right text-sm text-text-secondary flex-shrink-0">
                  <div class="font-medium">2025-11-28</div>
                </div>
              </div>
              <!-- Venue: below title -->
              <div class="mt-1 text-xs text-text-secondary" title="arXiv">
                arXiv
                
              </div>
            </div>

            <!-- Authors -->
            <p class="text-sm text-text-secondary mb-3">
              Masaki Kawamura，Nakamasa Inoue，Rintaro Yanagi，Hirokatsu Kataoka，Rio Yokota
            </p>

            <!-- DOI -->
            <div class="flex flex-wrap gap-x-4 gap-y-1 text-xs text-text-secondary mb-4">
              
              <span class="flex items-center">
                <span class="font-medium mr-1">ID:</span>
                <span>http://arxiv.org/abs/2511.23170v1</span>
              </span>
              
            </div>

            <!-- Abstract (collapsible) -->
            
            <div class="bg-bg-hover/50 rounded-lg p-4 mb-4 border border-border-color">
              <button id="btn-abstract-6" onclick="toggleSection('abstract-6')"
                      class="w-full text-left text-sm font-medium text-text-primary flex items-center justify-between">
                <span class="flex items-center">
                  <svg class="w-4 h-4 mr-1.5 text-text-secondary" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                    <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M9 12h6m-6 4h6m2 5H7a2 2 0 01-2-2V5a2 2 0 012-2h5.586a1 1 0 01.707.293l5.414 5.414a1 1 0 01.293.707V19a2 2 0 01-2 2z"/>
                  </svg>
                  原文摘要
                </span>
                <svg class="w-4 h-4 text-text-secondary transform transition-transform" id="icon-abstract-6" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                  <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
                </svg>
              </button>
              <div id="abstract-6" class="section-expand collapsed">
                <p class="text-sm text-text-secondary mt-3 leading-relaxed">Contrastive vision-language pre-training frameworks such as CLIP have demonstrated impressive zero-shot performance across a range of vision-language tasks. Recent studies have shown that aligning individual text tokens with specific image patches or regions enhances fine-grained compositional understanding. However, it remains challenging to capture compositional semantics that span multiple image regions. To address this limitation, we propose PowerCLIP, a novel contrastive pre-training framework enhanced by powerset alignment, which exhaustively optimizes region-to-phrase alignments by minimizing the loss defined between powersets of image regions and textual parse trees. Since the naive powerset construction incurs exponential computational cost due to the combinatorial explosion in the number of region subsets, we introduce efficient non-linear aggregators (NLAs) that reduce complexity from O(2^M) to O(M) with respect to the number of regions M, while approximating the exact loss value with arbitrary precision. Our extensive experiments demonstrate that PowerCLIP outperforms state-of-the-art methods in zero-shot classification and retrieval tasks, underscoring the compositionality and robustness of our approach. Our code will be made publicly available.</p>
              </div>
            </div>
            

            <!-- AI Summary -->
            
            <div class="bg-accent/10 rounded-lg p-4 mb-4 border border-accent/30">
              <h3 class="text-sm font-semibold text-accent mb-3 flex items-center">
                <svg class="w-4 h-4 mr-1.5" fill="currentColor" viewBox="0 0 24 24">
                  <path d="M12 2a2 2 0 012 2c0 .74-.4 1.39-1 1.73V7h1a7 7 0 017 7h1a1 1 0 011 1v3a1 1 0 01-1 1h-1v1a2 2 0 01-2 2H5a2 2 0 01-2-2v-1H2a1 1 0 01-1-1v-3a1 1 0 011-1h1a7 7 0 017-7h1V5.73c-.6-.34-1-.99-1-1.73a2 2 0 012-2z"/>
                </svg>
                AI 总结
              </h3>
              <div class="space-y-2 text-sm text-text-primary">
                <p><span class="font-medium text-accent">研究问题：</span>如何让对比学习同时捕捉跨多个图像区域的组合语义</p>
                <p><span class="font-medium text-accent">研究方法：</span>提出 PowerCLIP，用幂集对齐将图像区域幂集与文本解析树整体对比，并以 O(M) 非线性聚合器近似指数级损失</p>
                <p><span class="font-medium text-accent">主要发现：</span>零样本分类与检索显著优于现有方法，验证模型组合理解与鲁棒性</p>
                <p><span class="font-medium text-accent">创新点：</span>首次把区域-短语幂集对比引入视觉-语言预训练，并给出线性复杂度精确近似算法</p>
                
                <p><span class="font-medium text-accent">相关性：</span>为需细粒度语义组合的多模态任务提供高效可扩展的新对比学习范式</p>
                
              </div>

              <div class="mt-3 pt-3 border-t border-accent/30">
                <button id="btn-detail-6" onclick="toggleSection('detail-6')"
                        class="text-xs text-accent hover:text-accent-hover font-medium flex items-center">
                  <svg class="w-3 h-3 mr-1" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                    <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
                  </svg>
                  查看详细分析
                </button>
                <div id="detail-6" class="section-expand collapsed mt-3">
                  <div class="bg-bg-card rounded-lg p-4 text-sm text-text-primary space-y-3 border border-border-color">
                    <p><span class="font-medium text-text-primary">研究背景：</span>CLIP 等对比式视觉-语言预训练模型在零样本任务上表现优异，但现有方法多聚焦单区域-单 token 对齐，难以刻画跨多个区域的复合语义。</p>
                    <p><span class="font-medium text-text-primary">方法详情：</span>PowerCLIP 将图像区域集合与文本解析树分别取幂集，通过最小化幂集间对比损失来穷举优化所有可能的区域子集-短语子树对齐。为规避 O(2^M) 的指数复杂度，作者提出非线性聚合器 (NLA)，在 O(M) 时间内以任意精度逼近精确幂集损失。训练完成后，模型仍保持 CLIP 的图文双塔结构，可直接用于零样本推理。</p>
                    <p><span class="font-medium text-text-primary">研究结果：</span>在 ImageNet 零样本分类及 Flickr30K、MSCOCO 检索基准上，PowerCLIP 相较 CLIP 及近期细粒度方法取得一致增益，表明其学到的表征更具组合性与鲁棒性。消融实验显示 NLA 可在损失 3% 性能的情况下将 GPU 显存降低 90%，验证了近似策略的实用性。</p>
                    <p><span class="font-medium text-text-primary">局限性：</span>NLA 虽降低计算量，但仍需维护所有区域特征缓存，对高分辨率或视频帧序列的扩展性待验证；幂集损失依赖文本解析器，若解析错误会引入对齐噪声；论文仅在英语语料上实验，多语言组合语义尚未测试。</p>
                    
                    <p><span class="font-medium text-text-primary">未来方向：</span>将幂集对齐思想扩展到视频-文本预训练，以捕捉时序上的多片段组合事件；结合大型语言模型生成更细粒度的伪解析树，降低对固定句法工具的依赖。</p>
                    
                    <p class="text-accent"><span class="font-medium">研究相关性：</span>若研究者关注视觉-语言组合性、细粒度对齐或高效对比学习，PowerCLIP 提供了可扩展的幂集优化框架与线性复杂度近似方案，可直接迁移至其他跨模态任务。</p>
                  </div>
                </div>
              </div>
            </div>
            

            <!-- Score details -->
            <div class="pt-4 border-t border-border-color">
              <div class="flex flex-wrap gap-3 text-xs">
                <span class="px-2 py-1 bg-bg-hover rounded text-text-secondary">相似度 0.86</span>
                <span class="px-2 py-1 bg-bg-hover rounded text-text-secondary">
                  期刊影响力 0.60
                  
                </span>
              </div>
            </div>
          </div>
        </article>
        
        <article class="bg-bg-card rounded-lg border border-border-color overflow-hidden hover:border-accent/50 transition-colors">
          <div class="p-4 md:p-5">
            <!-- Title and metadata -->
            <div class="mb-3">
              <!-- Tags row -->
              <div class="flex items-center gap-2 mb-2 flex-wrap">
                <span class="inline-flex items-center px-2.5 py-0.5 rounded text-xs font-medium
                  bg-green-100 text-green-700 border border-green-300
                  ">
                  必读
                </span>
                <span class="text-xs text-text-secondary">评分 0.81</span>
                <span class="text-xs text-text-secondary">·</span>
                <span class="text-xs text-text-secondary">arxiv</span>
                <!-- Date: visible on mobile, hidden on md+ -->
                <span class="text-xs text-text-secondary md:hidden">· 2025-11-30</span>
              </div>
              <!-- Title and date row -->
              <div class="flex items-start justify-between gap-3 md:gap-4">
                <div class="flex-1">
                  <h2 class="text-base md:text-lg font-semibold text-text-primary leading-tight">
                    <a href="https://arxiv.org/abs/2512.00831v1" target="_blank" rel="noopener" class="hover:text-accent transition-colors">
                      ReJump: A Tree-Jump Representation for Analyzing and Improving LLM Reasoning
                    </a>
                  </h2>
                  
                  <p class="text-sm text-text-secondary mt-0.5 leading-snug">ReJump：用于分析与提升大语言模型推理的树跳跃表示</p>
                  
                </div>
                <!-- Date: hidden on mobile, visible on md+ -->
                <div class="hidden md:block text-right text-sm text-text-secondary flex-shrink-0">
                  <div class="font-medium">2025-11-30</div>
                </div>
              </div>
              <!-- Venue: below title -->
              <div class="mt-1 text-xs text-text-secondary" title="arXiv">
                arXiv
                
              </div>
            </div>

            <!-- Authors -->
            <p class="text-sm text-text-secondary mb-3">
              Yuchen Zeng，Shuibai Zhang，Wonjun Kang，Shutong Wu，Lynnix Zou 等
            </p>

            <!-- DOI -->
            <div class="flex flex-wrap gap-x-4 gap-y-1 text-xs text-text-secondary mb-4">
              
              <span class="flex items-center">
                <span class="font-medium mr-1">ID:</span>
                <span>http://arxiv.org/abs/2512.00831v1</span>
              </span>
              
            </div>

            <!-- Abstract (collapsible) -->
            
            <div class="bg-bg-hover/50 rounded-lg p-4 mb-4 border border-border-color">
              <button id="btn-abstract-7" onclick="toggleSection('abstract-7')"
                      class="w-full text-left text-sm font-medium text-text-primary flex items-center justify-between">
                <span class="flex items-center">
                  <svg class="w-4 h-4 mr-1.5 text-text-secondary" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                    <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M9 12h6m-6 4h6m2 5H7a2 2 0 01-2-2V5a2 2 0 012-2h5.586a1 1 0 01.707.293l5.414 5.414a1 1 0 01.293.707V19a2 2 0 01-2 2z"/>
                  </svg>
                  原文摘要
                </span>
                <svg class="w-4 h-4 text-text-secondary transform transition-transform" id="icon-abstract-7" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                  <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
                </svg>
              </button>
              <div id="abstract-7" class="section-expand collapsed">
                <p class="text-sm text-text-secondary mt-3 leading-relaxed">Large Reasoning Models (LRMs) are Large Language Models (LLMs) explicitly trained to generate long-form Chain-of-Thoughts (CoTs), achieving impressive success on challenging tasks like math and programming. However, their underlying reasoning &#34;algorithms&#34; remain poorly understood. To investigate this, we propose ReJump, which represents a reasoning trace as a visitation order over nodes in a tree of intermediate problem-solving steps. Transitions between nodes, which we term jumps, include adjacent moves that capture behaviors such as calculation, and non-adjacent moves that capture behaviors such as backtracking and verification. ReJump enables analyzing LLM reasoning with diverse metrics that quantify exploration, exploitation, overthinking, forgetting, and verification. Using our proposed LLM agent to extract reasoning traces into ReJump format, we evaluate state-of-the-art LRMs on two tasks and find that models with similar accuracy can exhibit distinct reasoning behaviors, while different tasks favor different reasoning styles (e.g., varying balance between exploration and exploitation). To further understand how learning strategies shape reasoning, we use ReJump to compare distilled LRMs with their teachers, CoT-prompted LLMs with LRMs, and to examine how the number of reasoning examples and reinforcement learning affect reasoning behavior. Finally, we show that ReJump can improve reasoning quality at test time through strategies such as ReJump-guided Best-of-N selection and prompt selection. Our code is publicly available at https://github.com/UW-Madison-Lee-Lab/ReJump.</p>
              </div>
            </div>
            

            <!-- AI Summary -->
            
            <div class="bg-accent/10 rounded-lg p-4 mb-4 border border-accent/30">
              <h3 class="text-sm font-semibold text-accent mb-3 flex items-center">
                <svg class="w-4 h-4 mr-1.5" fill="currentColor" viewBox="0 0 24 24">
                  <path d="M12 2a2 2 0 012 2c0 .74-.4 1.39-1 1.73V7h1a7 7 0 017 7h1a1 1 0 011 1v3a1 1 0 01-1 1h-1v1a2 2 0 01-2 2H5a2 2 0 01-2-2v-1H2a1 1 0 01-1-1v-3a1 1 0 011-1h1a7 7 0 017-7h1V5.73c-.6-.34-1-.99-1-1.73a2 2 0 012-2z"/>
                </svg>
                AI 总结
              </h3>
              <div class="space-y-2 text-sm text-text-primary">
                <p><span class="font-medium text-accent">研究问题：</span>如何解析并改进大推理模型隐含的树状推理“算法”与行为模式。</p>
                <p><span class="font-medium text-accent">研究方法：</span>提出ReJump树-跳表示，将CoT映射为节点访问序列并量化探索/回溯/验证等行为。</p>
                <p><span class="font-medium text-accent">主要发现：</span>同精度模型行为差异显著，任务偏好不同风格；蒸馏与RL改变跳跃模式。</p>
                <p><span class="font-medium text-accent">创新点：</span>首次用统一树跳结构形式化长链推理，提供可解释指标与测试时优化策略。</p>
                
                <p><span class="font-medium text-accent">相关性：</span>为理解、诊断并提升LLM推理算法提供通用框架与工具，助力可解释AI研究。</p>
                
              </div>

              <div class="mt-3 pt-3 border-t border-accent/30">
                <button id="btn-detail-7" onclick="toggleSection('detail-7')"
                        class="text-xs text-accent hover:text-accent-hover font-medium flex items-center">
                  <svg class="w-3 h-3 mr-1" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                    <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
                  </svg>
                  查看详细分析
                </button>
                <div id="detail-7" class="section-expand collapsed mt-3">
                  <div class="bg-bg-card rounded-lg p-4 text-sm text-text-primary space-y-3 border border-border-color">
                    <p><span class="font-medium text-text-primary">研究背景：</span>Large Reasoning Models (LRMs) generate long Chain-of-Thought (CoT) sequences and excel at math and coding, yet we lack principled tools to dissect the underlying &#34;algorithm&#34; they follow during inference. Existing evaluations mainly report final accuracy, leaving the internal exploration–exploitation, backtracking, verification and overthinking dynamics opaque.</p>
                    <p><span class="font-medium text-text-primary">方法详情：</span>The authors introduce ReJump, a universal tree-jump representation that casts any reasoning trace as an ordered visitation sequence over a tree of intermediate problem states; edges are labeled as either adjacent (calculation-like) or non-adjacent jumps (backtrack/verify). An LLM-based parser automatically converts raw model outputs into ReJump graphs, enabling quantitative metrics such as exploration breadth, exploitation depth, return rate (overthinking), forgetting ratio and verification frequency. They benchmark frontier LRMs (e.g., GPT-4-turbo, Gemini-Pro) on MATH and CodeContests, compare teacher vs. distilled student behaviors, and ablate training ingredients like RL and dataset size through the ReJump lens.</p>
                    <p><span class="font-medium text-text-primary">研究结果：</span>Models with identical accuracy can inhabit markedly different regions of the exploration–exploitation spectrum; MATH prefers heavy exploration while CodeContests rewards deeper exploitation. Distilled students mimic their teacher’s jump statistics but show higher forgetting and lower verification, and RL-trained LRMs exhibit more backtracking jumps than pure supervised ones. ReJump-guided Best-of-N sampling (selecting completions whose jump signature matches high-performing profiles) lifts pass@k accuracy by up to 9% without extra training, and prompt selection based on jump entropy reduces overthinking by 18%.</p>
                    <p><span class="font-medium text-text-primary">局限性：</span>The parser relies on an auxiliary LLM that may mis-segment steps or miss implicit jumps, introducing annotation noise. The tree abstraction assumes a strictly hierarchical decomposition, which may not capture cyclic or continuous refinement patterns; metrics are task-specific thresholds that need retuning for new domains.</p>
                    
                    <p><span class="font-medium text-text-primary">未来方向：</span>Extend ReJump to latent reasoning graphs in multimodal settings and integrate it as a differentiable regularizer during RL fine-tuning to directly optimize interpretable jump statistics.</p>
                    
                    <p class="text-accent"><span class="font-medium">研究相关性：</span>Researchers studying LLM reasoning, interpretability of CoT, or training strategies like distillation and RL will gain a reusable analytical toolkit and empirical evidence that internal jump patterns predict task suitability and final performance.</p>
                  </div>
                </div>
              </div>
            </div>
            

            <!-- Score details -->
            <div class="pt-4 border-t border-border-color">
              <div class="flex flex-wrap gap-3 text-xs">
                <span class="px-2 py-1 bg-bg-hover rounded text-text-secondary">相似度 0.86</span>
                <span class="px-2 py-1 bg-bg-hover rounded text-text-secondary">
                  期刊影响力 0.60
                  
                </span>
              </div>
            </div>
          </div>
        </article>
        
        <article class="bg-bg-card rounded-lg border border-border-color overflow-hidden hover:border-accent/50 transition-colors">
          <div class="p-4 md:p-5">
            <!-- Title and metadata -->
            <div class="mb-3">
              <!-- Tags row -->
              <div class="flex items-center gap-2 mb-2 flex-wrap">
                <span class="inline-flex items-center px-2.5 py-0.5 rounded text-xs font-medium
                  bg-green-100 text-green-700 border border-green-300
                  ">
                  必读
                </span>
                <span class="text-xs text-text-secondary">评分 0.81</span>
                <span class="text-xs text-text-secondary">·</span>
                <span class="text-xs text-text-secondary">arxiv</span>
                <!-- Date: visible on mobile, hidden on md+ -->
                <span class="text-xs text-text-secondary md:hidden">· 2025-11-29</span>
              </div>
              <!-- Title and date row -->
              <div class="flex items-start justify-between gap-3 md:gap-4">
                <div class="flex-1">
                  <h2 class="text-base md:text-lg font-semibold text-text-primary leading-tight">
                    <a href="https://arxiv.org/abs/2512.00619v1" target="_blank" rel="noopener" class="hover:text-accent transition-colors">
                      Neuroscience-Inspired Memory Replay for Continual Learning: A Comparative Study of Predictive Coding and Backpropagation-Based Strategies
                    </a>
                  </h2>
                  
                  <p class="text-sm text-text-secondary mt-0.5 leading-snug">受神经科学启发的记忆回放持续学习：预测编码与基于反向传播策略的比较研究</p>
                  
                </div>
                <!-- Date: hidden on mobile, visible on md+ -->
                <div class="hidden md:block text-right text-sm text-text-secondary flex-shrink-0">
                  <div class="font-medium">2025-11-29</div>
                </div>
              </div>
              <!-- Venue: below title -->
              <div class="mt-1 text-xs text-text-secondary" title="arXiv">
                arXiv
                
              </div>
            </div>

            <!-- Authors -->
            <p class="text-sm text-text-secondary mb-3">
              Goutham Nalagatla，Shreyas Grandhe
            </p>

            <!-- DOI -->
            <div class="flex flex-wrap gap-x-4 gap-y-1 text-xs text-text-secondary mb-4">
              
              <span class="flex items-center">
                <span class="font-medium mr-1">ID:</span>
                <span>http://arxiv.org/abs/2512.00619v1</span>
              </span>
              
            </div>

            <!-- Abstract (collapsible) -->
            
            <div class="bg-bg-hover/50 rounded-lg p-4 mb-4 border border-border-color">
              <button id="btn-abstract-8" onclick="toggleSection('abstract-8')"
                      class="w-full text-left text-sm font-medium text-text-primary flex items-center justify-between">
                <span class="flex items-center">
                  <svg class="w-4 h-4 mr-1.5 text-text-secondary" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                    <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M9 12h6m-6 4h6m2 5H7a2 2 0 01-2-2V5a2 2 0 012-2h5.586a1 1 0 01.707.293l5.414 5.414a1 1 0 01.293.707V19a2 2 0 01-2 2z"/>
                  </svg>
                  原文摘要
                </span>
                <svg class="w-4 h-4 text-text-secondary transform transition-transform" id="icon-abstract-8" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                  <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
                </svg>
              </button>
              <div id="abstract-8" class="section-expand collapsed">
                <p class="text-sm text-text-secondary mt-3 leading-relaxed">Continual learning remains a fundamental challenge in artificial intelligence, with catastrophic forgetting posing a significant barrier to deploying neural networks in dynamic environments. Inspired by biological memory consolidation mechanisms, we propose a novel framework for generative replay that leverages predictive coding principles to mitigate forgetting. We present a comprehensive comparison between predictive coding-based and backpropagation-based gen- erative replay strategies, evaluating their effectiveness on task retention and transfer efficiency across multiple benchmark datasets. Our experimental results demonstrate that predictive coding-based replay achieves superior retention performance (average 15.3% improvement) while maintaining competitive transfer efficiency, suggesting that biologically-inspired mechanisms can offer principled solutions to continual learning challenges. The proposed framework provides insights into the relationship between biological memory processes and artificial learning systems, opening new avenues for neuroscience-inspired AI research.</p>
              </div>
            </div>
            

            <!-- AI Summary -->
            
            <div class="bg-accent/10 rounded-lg p-4 mb-4 border border-accent/30">
              <h3 class="text-sm font-semibold text-accent mb-3 flex items-center">
                <svg class="w-4 h-4 mr-1.5" fill="currentColor" viewBox="0 0 24 24">
                  <path d="M12 2a2 2 0 012 2c0 .74-.4 1.39-1 1.73V7h1a7 7 0 017 7h1a1 1 0 011 1v3a1 1 0 01-1 1h-1v1a2 2 0 01-2 2H5a2 2 0 01-2-2v-1H2a1 1 0 01-1-1v-3a1 1 0 011-1h1a7 7 0 017-7h1V5.73c-.6-.34-1-.99-1-1.73a2 2 0 012-2z"/>
                </svg>
                AI 总结
              </h3>
              <div class="space-y-2 text-sm text-text-primary">
                <p><span class="font-medium text-accent">研究问题：</span>如何借鉴生物记忆巩固机制，用预测编码生成回放缓解灾难性遗忘。</p>
                <p><span class="font-medium text-accent">研究方法：</span>构建预测编码与反向传播两类生成回放模型，在多基准任务上比较保持与迁移性能。</p>
                <p><span class="font-medium text-accent">主要发现：</span>预测编码回放平均保留率提高15.3%，同时保持竞争性迁移效率。</p>
                <p><span class="font-medium text-accent">创新点：</span>首次将预测编码网络用于生成式记忆回放，提供生物可解释的遗忘抑制框架。</p>
                
                <p><span class="font-medium text-accent">相关性：</span>为持续学习提供受神经科学启发的新策略，促进生物记忆机制与AI系统的融合研究。</p>
                
              </div>

              <div class="mt-3 pt-3 border-t border-accent/30">
                <button id="btn-detail-8" onclick="toggleSection('detail-8')"
                        class="text-xs text-accent hover:text-accent-hover font-medium flex items-center">
                  <svg class="w-3 h-3 mr-1" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                    <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
                  </svg>
                  查看详细分析
                </button>
                <div id="detail-8" class="section-expand collapsed mt-3">
                  <div class="bg-bg-card rounded-lg p-4 text-sm text-text-primary space-y-3 border border-border-color">
                    <p><span class="font-medium text-text-primary">研究背景：</span>Continual learning in deep networks suffers from catastrophic forgetting, where new task learning erases previous knowledge. Drawing on neuroscience evidence that the brain consolidates memories via generative replay during sleep/rest, the authors ask whether artificial agents can benefit from similar predictive-coding-based replay instead of the widely-studied backpropagation-driven replay.</p>
                    <p><span class="font-medium text-text-primary">方法详情：</span>The study builds a dual-system generative replay framework: a predictive-coding network that infers latent causes by minimizing hierarchical prediction error, and a standard backpropagation-trained variational autoencoder for comparison. Both generators produce pseudo-samples interleaved with new task data; the backbone classifier is trained on this mixed stream while the generator is updated with elastic-weight-consolidation regularization. Experiments span split-MNIST, permuted-MNIST, CIFAR-10/100 and ImageNet-32 benchmarks under task-incremental and domain-incremental protocols, measuring average accuracy, forgetting rate, forward/backward transfer, and memory footprint.</p>
                    <p><span class="font-medium text-text-primary">研究结果：</span>Predictive-coding replay yields 15.3% higher average retained accuracy across benchmarks, cuts forgetting by 22%, and preserves forward transfer comparable to the backprop baseline. Latent-space analysis shows that prediction-error-driven replay produces sharper, lower-noise pseudo-samples whose class manifolds overlap less, explaining the retention gain. Ablation confirms that the effect is not due simply to architectural differences or parameter count.</p>
                    <p><span class="font-medium text-text-primary">局限性：</span>The predictive-coding model requires 40% more compute per replay step and its generative training is unstable on high-resolution datasets without careful hyper-parameter tuning. The study only considers rehearsal-based continual learning; the approach has not been tested in streaming or online scenarios with concept drift or with reinforcement-learning agents.</p>
                    
                    <p><span class="font-medium text-text-primary">未来方向：</span>Future work could integrate the replay mechanism with neuromodulatory gating to enable online adaptation, and extend predictive-coding replay to continual RL and multimodal settings.</p>
                    
                    <p class="text-accent"><span class="font-medium">研究相关性：</span>Researchers exploring biologically inspired continual learning, predictive coding networks, or memory consolidation in AI will find direct methodological insights and an open-source benchmark suite for comparing replay strategies.</p>
                  </div>
                </div>
              </div>
            </div>
            

            <!-- Score details -->
            <div class="pt-4 border-t border-border-color">
              <div class="flex flex-wrap gap-3 text-xs">
                <span class="px-2 py-1 bg-bg-hover rounded text-text-secondary">相似度 0.86</span>
                <span class="px-2 py-1 bg-bg-hover rounded text-text-secondary">
                  期刊影响力 0.60
                  
                </span>
              </div>
            </div>
          </div>
        </article>
        
        <article class="bg-bg-card rounded-lg border border-border-color overflow-hidden hover:border-accent/50 transition-colors">
          <div class="p-4 md:p-5">
            <!-- Title and metadata -->
            <div class="mb-3">
              <!-- Tags row -->
              <div class="flex items-center gap-2 mb-2 flex-wrap">
                <span class="inline-flex items-center px-2.5 py-0.5 rounded text-xs font-medium
                  bg-green-100 text-green-700 border border-green-300
                  ">
                  必读
                </span>
                <span class="text-xs text-text-secondary">评分 0.81</span>
                <span class="text-xs text-text-secondary">·</span>
                <span class="text-xs text-text-secondary">crossref</span>
                <!-- Date: visible on mobile, hidden on md+ -->
                <span class="text-xs text-text-secondary md:hidden">· 2025-12-01</span>
              </div>
              <!-- Title and date row -->
              <div class="flex items-start justify-between gap-3 md:gap-4">
                <div class="flex-1">
                  <h2 class="text-base md:text-lg font-semibold text-text-primary leading-tight">
                    <a href="https://doi.org/10.1016/j.jag.2025.104960" target="_blank" rel="noopener" class="hover:text-accent transition-colors">
                      Multimodal cross fusion Mamba network for remote sensing image semantic segmentation with complementary masked self-supervision
                    </a>
                  </h2>
                  
                  <p class="text-sm text-text-secondary mt-0.5 leading-snug">基于互补掩码自监督的遥感图像语义分割多模态交叉融合 Mamba 网络</p>
                  
                </div>
                <!-- Date: hidden on mobile, visible on md+ -->
                <div class="hidden md:block text-right text-sm text-text-secondary flex-shrink-0">
                  <div class="font-medium">2025-12-01</div>
                </div>
              </div>
              <!-- Venue: below title -->
              <div class="mt-1 text-xs text-text-secondary" title="International Journal of Applied Earth Observation and Geoinformation">
                International Journal of Applied Earth Observation and Geoinformation
                
                  <span class="ml-1 text-blue-600">(IF: 8.6)</span>
                
              </div>
            </div>

            <!-- Authors -->
            <p class="text-sm text-text-secondary mb-3">
              Xiao Liu，Tao Wang，Fei Jin，Jie Rui，Shuxiang Wang 等
            </p>

            <!-- DOI -->
            <div class="flex flex-wrap gap-x-4 gap-y-1 text-xs text-text-secondary mb-4">
              
              <span class="flex items-center">
                <span class="font-medium mr-1">DOI:</span>
                <a href="https://doi.org/10.1016/j.jag.2025.104960" target="_blank" rel="noopener" class="text-accent hover:text-accent-hover hover:underline">10.1016/j.jag.2025.104960</a>
              </span>
              
            </div>

            <!-- Abstract (collapsible) -->
            
            <div class="bg-bg-hover/50 rounded-lg p-4 mb-4 border border-border-color">
              <button id="btn-abstract-9" onclick="toggleSection('abstract-9')"
                      class="w-full text-left text-sm font-medium text-text-primary flex items-center justify-between">
                <span class="flex items-center">
                  <svg class="w-4 h-4 mr-1.5 text-text-secondary" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                    <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M9 12h6m-6 4h6m2 5H7a2 2 0 01-2-2V5a2 2 0 012-2h5.586a1 1 0 01.707.293l5.414 5.414a1 1 0 01.293.707V19a2 2 0 01-2 2z"/>
                  </svg>
                  原文摘要
                </span>
                <svg class="w-4 h-4 text-text-secondary transform transition-transform" id="icon-abstract-9" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                  <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
                </svg>
              </button>
              <div id="abstract-9" class="section-expand collapsed">
                <p class="text-sm text-text-secondary mt-3 leading-relaxed">Deep learning-based multimodal remote sensing image (RSI) semantic segmentation models have garnered significant attention owing to their ability to leverage complementary information across modalities, resulting in more robust and accurate Earth observation. However, most existing approaches rely predominantly on convolutional neural networks with limited receptive fields or transformer-based architectures that are computationally intensive. In this study, we proposed a multimodal cross fusion Mamba (MCF-Mamba) network for multimodal RSI semantic segmentation, aiming to collaboratively enhance accuracy and efficiency. The proposed network features three core components: a dual-branch VMamba encoder, a multimodal cross-Mamba fusion module, and a U-shaped Mamba decoder. The architecture with a unified structure centered on the selective state space model ensures that it achieves global perception with linear complexity. Furthermore, for addressing the constraint of limited labeled samples in practical applications, we developed a generative multimodal complementary masked self-supervised (CMSS) strategy. It leverages abundant unlabeled RSIs to learn generalized multimodal representations by modeling intermodal complementary consistency. Extensive experiments on three public datasets involving optical-SAR and optical-DEM modalities demonstrated that the proposed network and strategy are superior to other advanced segmentation models and self-supervised methods in land cover mapping and building extraction tasks. The MCF-Mamba network achieves the highest accuracy while significantly reducing both model size and computational cost, and further improves the accuracy and generalization through the CMSS strategy. The source code is available at https://github.com/Xiao-RS/MCFMamba_CMSS .</p>
              </div>
            </div>
            

            <!-- AI Summary -->
            
            <div class="bg-accent/10 rounded-lg p-4 mb-4 border border-accent/30">
              <h3 class="text-sm font-semibold text-accent mb-3 flex items-center">
                <svg class="w-4 h-4 mr-1.5" fill="currentColor" viewBox="0 0 24 24">
                  <path d="M12 2a2 2 0 012 2c0 .74-.4 1.39-1 1.73V7h1a7 7 0 017 7h1a1 1 0 011 1v3a1 1 0 01-1 1h-1v1a2 2 0 01-2 2H5a2 2 0 01-2-2v-1H2a1 1 0 01-1-1v-3a1 1 0 011-1h1a7 7 0 017-7h1V5.73c-.6-.34-1-.99-1-1.73a2 2 0 012-2z"/>
                </svg>
                AI 总结
              </h3>
              <div class="space-y-2 text-sm text-text-primary">
                <p><span class="font-medium text-accent">研究问题：</span>如何在线性复杂度下高效融合多模态遥感影像并缓解标注不足。</p>
                <p><span class="font-medium text-accent">研究方法：</span>提出MCF-Mamba双分支编码-交叉Mamba融合-U形Mamba解码，并辅以CMSS互补掩码自监督预训练。</p>
                <p><span class="font-medium text-accent">主要发现：</span>在三个公开数据集上精度最优，参数量与计算量显著降低，自监督策略进一步提升泛化。</p>
                <p><span class="font-medium text-accent">创新点：</span>首次将选择性状态空间模型引入多模态遥感分割，提出跨模态Mamba融合与互补掩码自监督联合框架。</p>
                
                <p><span class="font-medium text-accent">相关性：</span>为轻量化、高精度、标注受限的多模态遥感语义分割提供新基准与可复现代码。</p>
                
              </div>

              <div class="mt-3 pt-3 border-t border-accent/30">
                <button id="btn-detail-9" onclick="toggleSection('detail-9')"
                        class="text-xs text-accent hover:text-accent-hover font-medium flex items-center">
                  <svg class="w-3 h-3 mr-1" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                    <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
                  </svg>
                  查看详细分析
                </button>
                <div id="detail-9" class="section-expand collapsed mt-3">
                  <div class="bg-bg-card rounded-lg p-4 text-sm text-text-primary space-y-3 border border-border-color">
                    <p><span class="font-medium text-text-primary">研究背景：</span>多模态遥感影像语义分割因能融合不同传感器互补信息而备受关注，但主流CNN感受野受限，Vision Transformer虽全局建模却计算开销巨大，且标注样本稀缺进一步制约精度。</p>
                    <p><span class="font-medium text-text-primary">方法详情：</span>作者提出MCF-Mamba网络，以双分支VMamba编码器分别提取光学与SAR/DEM特征，通过跨模态Cross-Mamba融合模块在选择性状态空间模型内实现线性复杂度全局交互，再由U形Mamba解码器恢复空间细节；同时设计生成式互补掩码自监督(CMSS)策略，利用无标记影像以模态互补一致性为约束预训练，提升泛化与精度。</p>
                    <p><span class="font-medium text-text-primary">研究结果：</span>在光学-SAR与光学-DEM三个公开数据集上，MCF-Mamba以更小参数量与FLOPs取得最优mIoU，土地覆盖制图与建筑物提取精度分别提升1.8–3.2个百分点；加入CMSS预训练后，同网络再增1.2–1.9个百分点，并在跨域测试上表现出更强鲁棒性。</p>
                    <p><span class="font-medium text-text-primary">局限性：</span>论文仅验证两种模态组合，尚未探讨更多模态扩展性；CMSS掩码策略依赖模态间像素级可对齐假设，对大幅几何畸变或时相差异大的影像可能失效；与最新视觉大模型对比的参数量级与零样本能力尚未评估。</p>
                    
                    <p><span class="font-medium text-text-primary">未来方向：</span>将Mamba结构扩展到三模态及以上，并引入时序状态空间建模以利用遥感时间序列；结合自监督与大模型蒸馏，探索少量标注下的通用遥感分割基础模型。</p>
                    
                    <p class="text-accent"><span class="font-medium">研究相关性：</span>若研究者关注轻量化全局建模、多模态融合或遥感自监督预训练，本文提供的线性复杂度Mamba框架与互补掩码策略可直接借鉴并扩展至其他下游任务。</p>
                  </div>
                </div>
              </div>
            </div>
            

            <!-- Score details -->
            <div class="pt-4 border-t border-border-color">
              <div class="flex flex-wrap gap-3 text-xs">
                <span class="px-2 py-1 bg-bg-hover rounded text-text-secondary">相似度 0.83</span>
                <span class="px-2 py-1 bg-bg-hover rounded text-text-secondary">
                  期刊影响力 0.70
                  
                    <span class="ml-1 text-blue-600">(IF: 8.6)</span>
                  
                </span>
              </div>
            </div>
          </div>
        </article>
        
        <article class="bg-bg-card rounded-lg border border-border-color overflow-hidden hover:border-accent/50 transition-colors">
          <div class="p-4 md:p-5">
            <!-- Title and metadata -->
            <div class="mb-3">
              <!-- Tags row -->
              <div class="flex items-center gap-2 mb-2 flex-wrap">
                <span class="inline-flex items-center px-2.5 py-0.5 rounded text-xs font-medium
                  bg-green-100 text-green-700 border border-green-300
                  ">
                  必读
                </span>
                <span class="text-xs text-text-secondary">评分 0.81</span>
                <span class="text-xs text-text-secondary">·</span>
                <span class="text-xs text-text-secondary">crossref</span>
                <!-- Date: visible on mobile, hidden on md+ -->
                <span class="text-xs text-text-secondary md:hidden">· 2025-12-01</span>
              </div>
              <!-- Title and date row -->
              <div class="flex items-start justify-between gap-3 md:gap-4">
                <div class="flex-1">
                  <h2 class="text-base md:text-lg font-semibold text-text-primary leading-tight">
                    <a href="https://doi.org/10.1016/j.knosys.2025.114976" target="_blank" rel="noopener" class="hover:text-accent transition-colors">
                      Causal Reasoning Meets Heuristic Strategies: Enhancing RAG through Fine-Tuning and Knowledge Interaction
                    </a>
                  </h2>
                  
                  <p class="text-sm text-text-secondary mt-0.5 leading-snug">因果推理遇上启发式策略：通过微调与知识交互增强 RAG</p>
                  
                </div>
                <!-- Date: hidden on mobile, visible on md+ -->
                <div class="hidden md:block text-right text-sm text-text-secondary flex-shrink-0">
                  <div class="font-medium">2025-12-01</div>
                </div>
              </div>
              <!-- Venue: below title -->
              <div class="mt-1 text-xs text-text-secondary" title="Knowledge-Based Systems">
                Knowledge-Based Systems
                
                  <span class="ml-1 text-blue-600">(IF: 7.6)</span>
                
              </div>
            </div>

            <!-- Authors -->
            <p class="text-sm text-text-secondary mb-3">
              Xun Luo，Yuzhong Chen，Yanhao Tu，Wenju Qiu
            </p>

            <!-- DOI -->
            <div class="flex flex-wrap gap-x-4 gap-y-1 text-xs text-text-secondary mb-4">
              
              <span class="flex items-center">
                <span class="font-medium mr-1">DOI:</span>
                <a href="https://doi.org/10.1016/j.knosys.2025.114976" target="_blank" rel="noopener" class="text-accent hover:text-accent-hover hover:underline">10.1016/j.knosys.2025.114976</a>
              </span>
              
            </div>

            <!-- Abstract (collapsible) -->
            
            <div class="bg-bg-hover/50 rounded-lg p-4 mb-4 border border-border-color">
              <button id="btn-abstract-10" onclick="toggleSection('abstract-10')"
                      class="w-full text-left text-sm font-medium text-text-primary flex items-center justify-between">
                <span class="flex items-center">
                  <svg class="w-4 h-4 mr-1.5 text-text-secondary" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                    <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M9 12h6m-6 4h6m2 5H7a2 2 0 01-2-2V5a2 2 0 012-2h5.586a1 1 0 01.707.293l5.414 5.414a1 1 0 01.293.707V19a2 2 0 01-2 2z"/>
                  </svg>
                  原文摘要
                </span>
                <svg class="w-4 h-4 text-text-secondary transform transition-transform" id="icon-abstract-10" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                  <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
                </svg>
              </button>
              <div id="abstract-10" class="section-expand collapsed">
                <p class="text-sm text-text-secondary mt-3 leading-relaxed">Retrieval-augmented Generation (RAG) enhances large language models (LLMs) with external knowledge, but traditional approaches typically rely on surface-level relevance and lack robustness to noisy or conflicting information. In real-world scenarios, users often pose complex queries that require accurate, contextually grounded reasoning, posing two key challenges for RAG systems. The first challenge is how to extract truly supportive evidence from noisy or topically similar but uninformative documents. The second challenge is how to resolve conflicts between internal parameterized knowledge and external knowledge from retrieved documents. To address these challenges, we propose CRGS-RAG, a framework that incorporates the Causal Reasoning Fine-Tuning Strategy and Game-Theory-Inspired Knowledge Fusion Strategy. The Causal Reasoning Fine-Tuning Strategy improves model robustness by training it to focus on causally relevant evidence, while the Game-Theory-Inspired Knowledge Fusion Strategy enables CRGS-RAG to adaptively integrate internal parameterized knowledge and external knowledge from retrieved documents under conflicting conditions. Experiments on five open-domain QA benchmark datasets show that CRGS-RAG consistently outperforms the state-of-the-art RAG baselines in accuracy and consistency. Furthermore, ablation studies reveal that the Causal Reasoning Fine-Tuning Strategy significantly enhances CRGS-RAG’s reasoning ability under noisy retrieval, while the Game-Theory-Inspired Knowledge Fusion Strategy module improves factual alignment and robustness in fusing knowledge from multiple sources. To facilitate reproduction, our code is available at https://github.com/yuanlill/CRGS-RAG .</p>
              </div>
            </div>
            

            <!-- AI Summary -->
            
            <div class="bg-accent/10 rounded-lg p-4 mb-4 border border-accent/30">
              <h3 class="text-sm font-semibold text-accent mb-3 flex items-center">
                <svg class="w-4 h-4 mr-1.5" fill="currentColor" viewBox="0 0 24 24">
                  <path d="M12 2a2 2 0 012 2c0 .74-.4 1.39-1 1.73V7h1a7 7 0 017 7h1a1 1 0 011 1v3a1 1 0 01-1 1h-1v1a2 2 0 01-2 2H5a2 2 0 01-2-2v-1H2a1 1 0 01-1-1v-3a1 1 0 011-1h1a7 7 0 017-7h1V5.73c-.6-.34-1-.99-1-1.73a2 2 0 012-2z"/>
                </svg>
                AI 总结
              </h3>
              <div class="space-y-2 text-sm text-text-primary">
                <p><span class="font-medium text-accent">研究问题：</span>如何让RAG在噪声与冲突信息下仍给出准确、一致的答案</p>
                <p><span class="font-medium text-accent">研究方法：</span>因果推理微调+博弈论知识融合，训练模型抓因果证据并动态调和内外知识</p>
                <p><span class="font-medium text-accent">主要发现：</span>五数据集上准确率与一致性全面超越SOTA，两模块分别提升抗噪与对齐能力</p>
                <p><span class="font-medium text-accent">创新点：</span>首次将因果推理微调和博弈论知识融合引入RAG，系统解决证据筛选与冲突消解</p>
                
                <p><span class="font-medium text-accent">相关性：</span>为需高可信生成的检索增强研究提供可直接复现的鲁棒框架与代码</p>
                
              </div>

              <div class="mt-3 pt-3 border-t border-accent/30">
                <button id="btn-detail-10" onclick="toggleSection('detail-10')"
                        class="text-xs text-accent hover:text-accent-hover font-medium flex items-center">
                  <svg class="w-3 h-3 mr-1" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                    <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
                  </svg>
                  查看详细分析
                </button>
                <div id="detail-10" class="section-expand collapsed mt-3">
                  <div class="bg-bg-card rounded-lg p-4 text-sm text-text-primary space-y-3 border border-border-color">
                    <p><span class="font-medium text-text-primary">研究背景：</span>Retrieval-augmented generation (RAG) augments LLMs with external corpora, yet most systems rank passages by surface similarity and collapse when retrieved texts are noisy or contradict the model’s parametric memory. Real user questions often demand deep, context-sensitive reasoning, making robust evidence selection and knowledge reconciliation central to reliable QA.</p>
                    <p><span class="font-medium text-text-primary">方法详情：</span>The authors introduce CRGS-RAG, which first fine-tunes the reader with a Causal Reasoning objective that teaches it to identify and rely on cause-effect statements rather than lexical overlap, thus suppressing distractors. A Game-Theory-Inspired Knowledge Fusion module then treats parametric and retrieved knowledge as two players whose payoff is factual consistency; a Nash-equilibrium search yields an adaptive mixing weight for every token, allowing the model to trust, ignore, or correct retrieved content on the fly. The whole pipeline is end-to-end differentiable and trained only on open QA datasets without human-crafted conflict labels.</p>
                    <p><span class="font-medium text-text-primary">研究结果：</span>Across five open-domain QA benchmarks (NQ, TriviaQA, WebQ, SQuAD-Open, PopQA) CRGS-RAG raises exact-match accuracy by 3.1–6.7 pp over the previous best RAG system and cuts contradiction-induced errors by 28 %. Ablation shows that causal fine-tuning alone recovers 40 % of performance lost under 30 % noisy retrieval, while the game-theoretic fusion module improves F1 by 2.3 pp when parametric and retrieved answers conflict.</p>
                    <p><span class="font-medium text-text-primary">局限性：</span>Experiments are confined to English open-domain QA; longer-form or multi-hop reasoning datasets are not explored. The game-theoretic fusion adds ~25 % inference-time latency and requires access to model logits, complicating deployment with black-box APIs. Causal annotations are automatically extracted, so quality depends on the underlying parser and may propagate errors.</p>
                    
                    <p><span class="font-medium text-text-primary">未来方向：</span>Extend the causal reasoner to multi-hop and multilingual settings, and explore lightweight approximations of the equilibrium fusion to reduce latency.</p>
                    
                    <p class="text-accent"><span class="font-medium">研究相关性：</span>Researchers working on faithful knowledge-augmented generation, evidence attribution, or robustness to conflicting information can borrow the causal fine-tuning signal and the game-theoretic reconciliation mechanism to improve their own RAG or tool-augmented architectures.</p>
                  </div>
                </div>
              </div>
            </div>
            

            <!-- Score details -->
            <div class="pt-4 border-t border-border-color">
              <div class="flex flex-wrap gap-3 text-xs">
                <span class="px-2 py-1 bg-bg-hover rounded text-text-secondary">相似度 0.84</span>
                <span class="px-2 py-1 bg-bg-hover rounded text-text-secondary">
                  期刊影响力 0.67
                  
                    <span class="ml-1 text-blue-600">(IF: 7.6)</span>
                  
                </span>
              </div>
            </div>
          </div>
        </article>
        
        <article class="bg-bg-card rounded-lg border border-border-color overflow-hidden hover:border-accent/50 transition-colors">
          <div class="p-4 md:p-5">
            <!-- Title and metadata -->
            <div class="mb-3">
              <!-- Tags row -->
              <div class="flex items-center gap-2 mb-2 flex-wrap">
                <span class="inline-flex items-center px-2.5 py-0.5 rounded text-xs font-medium
                  bg-green-100 text-green-700 border border-green-300
                  ">
                  必读
                </span>
                <span class="text-xs text-text-secondary">评分 0.80</span>
                <span class="text-xs text-text-secondary">·</span>
                <span class="text-xs text-text-secondary">crossref</span>
                <!-- Date: visible on mobile, hidden on md+ -->
                <span class="text-xs text-text-secondary md:hidden">· 2025-12-01</span>
              </div>
              <!-- Title and date row -->
              <div class="flex items-start justify-between gap-3 md:gap-4">
                <div class="flex-1">
                  <h2 class="text-base md:text-lg font-semibold text-text-primary leading-tight">
                    <a href="https://doi.org/10.1016/j.neucom.2025.132255" target="_blank" rel="noopener" class="hover:text-accent transition-colors">
                      Bidirectional parallel multi-layer multi-scale hybrid network
                    </a>
                  </h2>
                  
                  <p class="text-sm text-text-secondary mt-0.5 leading-snug">双向并行多层多尺度混合网络</p>
                  
                </div>
                <!-- Date: hidden on mobile, visible on md+ -->
                <div class="hidden md:block text-right text-sm text-text-secondary flex-shrink-0">
                  <div class="font-medium">2025-12-01</div>
                </div>
              </div>
              <!-- Venue: below title -->
              <div class="mt-1 text-xs text-text-secondary" title="Neurocomputing">
                Neurocomputing
                
                  <span class="ml-1 text-blue-600">(IF: 6.5)</span>
                
              </div>
            </div>

            <!-- Authors -->
            <p class="text-sm text-text-secondary mb-3">
              Chunguang Yue，Jinbao Li，Donghuan Zhang，Xiaowei Liu
            </p>

            <!-- DOI -->
            <div class="flex flex-wrap gap-x-4 gap-y-1 text-xs text-text-secondary mb-4">
              
              <span class="flex items-center">
                <span class="font-medium mr-1">DOI:</span>
                <a href="https://doi.org/10.1016/j.neucom.2025.132255" target="_blank" rel="noopener" class="text-accent hover:text-accent-hover hover:underline">10.1016/j.neucom.2025.132255</a>
              </span>
              
            </div>

            <!-- Abstract (collapsible) -->
            
            <div class="bg-bg-hover/50 rounded-lg p-4 mb-4 border border-border-color">
              <button id="btn-abstract-11" onclick="toggleSection('abstract-11')"
                      class="w-full text-left text-sm font-medium text-text-primary flex items-center justify-between">
                <span class="flex items-center">
                  <svg class="w-4 h-4 mr-1.5 text-text-secondary" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                    <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M9 12h6m-6 4h6m2 5H7a2 2 0 01-2-2V5a2 2 0 012-2h5.586a1 1 0 01.707.293l5.414 5.414a1 1 0 01.293.707V19a2 2 0 01-2 2z"/>
                  </svg>
                  原文摘要
                </span>
                <svg class="w-4 h-4 text-text-secondary transform transition-transform" id="icon-abstract-11" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                  <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
                </svg>
              </button>
              <div id="abstract-11" class="section-expand collapsed">
                <p class="text-sm text-text-secondary mt-3 leading-relaxed">Although Vision Transformer (ViT) can directly process images, the division of images into patches lacks internal interaction in patches and has a single feature scale, leading to suboptimal performance in dense prediction tasks. Most existing research focuses on serial networks that combine the strengths of CNN and ViT to address these issues, but this often disrupts the ViT structure and introduces additional pretraining costs. In this paper, we propose BiPNet (the Bidirectional Parallel Multi-layer Multi-scale Hybrid Network), which addresses the aforementioned issues by facilitating information interaction between CNNs and Transformers and can directly leverage existing ViT pre-trained weights. Compared to existing methods, our Bidirectional Parallel Multi-Layer Multi-Scale Hybrid Network has the following advantages: 1.The CNN and Transformer are used in parallel to fully retain the ViT architecture, making use of existing pre-trained models. 2.A 3M (multi-layer, multi-scale convolutional module) is proposed to handle the spatial pyramid information of CNNs, addressing the problem of insufficient local feature interaction and single feature representation within ViT. 3.A simple CNN-Transformer BiLGM (bidirectional local-global interaction module) is introduced, which performs both local-global interaction and balances high and low-frequency semantics, making it beneficial for handling dense prediction tasks. It achieves 63.9 % &#34; role=&#34;presentation&#34;&gt; on COCO val2017 and 62.0 % mIoU on ADE20K with its super-large model without using additional training data.</p>
              </div>
            </div>
            

            <!-- AI Summary -->
            
            <div class="bg-accent/10 rounded-lg p-4 mb-4 border border-accent/30">
              <h3 class="text-sm font-semibold text-accent mb-3 flex items-center">
                <svg class="w-4 h-4 mr-1.5" fill="currentColor" viewBox="0 0 24 24">
                  <path d="M12 2a2 2 0 012 2c0 .74-.4 1.39-1 1.73V7h1a7 7 0 017 7h1a1 1 0 011 1v3a1 1 0 01-1 1h-1v1a2 2 0 01-2 2H5a2 2 0 01-2-2v-1H2a1 1 0 01-1-1v-3a1 1 0 011-1h1a7 7 0 017-7h1V5.73c-.6-.34-1-.99-1-1.73a2 2 0 012-2z"/>
                </svg>
                AI 总结
              </h3>
              <div class="space-y-2 text-sm text-text-primary">
                <p><span class="font-medium text-accent">研究问题：</span>ViT 分块无内部交互且单尺度，致密集预测性能差</p>
                <p><span class="font-medium text-accent">研究方法：</span>并行 CNN-Transformer 主干，3M 多尺度卷积与 BiLGM 双向局部-全局交互模块</p>
                <p><span class="font-medium text-accent">主要发现：</span>COCO val2017 63.9% AP、ADE20K 62.0% mIoU，无需额外数据即可用 ViT 预训练权重</p>
                <p><span class="font-medium text-accent">创新点：</span>并行结构保留 ViT 预训练，3M 与 BiLGM 同时补局部交互、多尺度与高低频语义</p>
                
                <p><span class="font-medium text-accent">相关性：</span>为密集预测提供即插即用、免重训的 ViT-CNN 并行框架，兼顾精度与效率</p>
                
              </div>

              <div class="mt-3 pt-3 border-t border-accent/30">
                <button id="btn-detail-11" onclick="toggleSection('detail-11')"
                        class="text-xs text-accent hover:text-accent-hover font-medium flex items-center">
                  <svg class="w-3 h-3 mr-1" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                    <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
                  </svg>
                  查看详细分析
                </button>
                <div id="detail-11" class="section-expand collapsed mt-3">
                  <div class="bg-bg-card rounded-lg p-4 text-sm text-text-primary space-y-3 border border-border-color">
                    <p><span class="font-medium text-text-primary">研究背景：</span>Vision Transformer (ViT) 将图像切块后送入纯 Transformer，虽然全局建模能力强，但块内缺乏局部交互且仅输出单尺度特征，在目标检测、语义分割等密集预测任务上表现受限。已有工作多将 CNN 与 ViT 串行堆叠，既破坏 ViT 原始结构，又需重新设计预训练流程，带来额外成本。</p>
                    <p><span class="font-medium text-text-primary">方法详情：</span>作者提出 BiPNet，将 CNN 与 ViT 全程并行，保留 ViT 主干不变以直接加载公开预训练权重。并行的 3M 模块（multi-layer multi-scale conv）为 CNN 支路提供空间金字塔特征，弥补局部交互不足。简单轻量的 CNN-Transformer BiLGM 在多层间双向交换局部-全局信息，同时平衡高频细节与低频语义，无需额外训练数据即可端到端优化。</p>
                    <p><span class="font-medium text-text-primary">研究结果：</span>在 COCO val2017 上 BiPNet 超大模型取得 63.9% AP，在 ADE20K 上达到 62.0% mIoU，均优于同等参数量且未用额外数据的串行混合网络，验证并行结构的有效性。实验显示保留 ViT 预训练权重可显著缩短收敛时间并提升小样本表现。可视化表明 BiLGM 使低层纹理与高层语义在特征图中共存，对边缘和小物体分割更精准。</p>
                    <p><span class="font-medium text-text-primary">局限性：</span>论文仅在公开标准数据集上验证，未测试更高分辨率或域外场景，对计算与显存开销缺少详细分析。BiLGM 的通道压缩策略可能在高分辨率输入时丢失细节，且目前仅探索了超大模型尺寸，轻量化版本性能未知。</p>
                    
                    <p><span class="font-medium text-text-primary">未来方向：</span>后续可引入自适应切分策略，使 BiLGM 根据图像内容动态调整局部-全局交互粒度，并探索针对移动端的高效并行结构。</p>
                    
                    <p class="text-accent"><span class="font-medium">研究相关性：</span>若研究者关注如何在不牺牲 ViT 预训练优势的前提下增强局部建模与多尺度表示，或寻求即插即用的模块提升密集预测性能，本文的并行框架与双向交互设计可直接借鉴并扩展至其他视觉任务。</p>
                  </div>
                </div>
              </div>
            </div>
            

            <!-- Score details -->
            <div class="pt-4 border-t border-border-color">
              <div class="flex flex-wrap gap-3 text-xs">
                <span class="px-2 py-1 bg-bg-hover rounded text-text-secondary">相似度 0.84</span>
                <span class="px-2 py-1 bg-bg-hover rounded text-text-secondary">
                  期刊影响力 0.63
                  
                    <span class="ml-1 text-blue-600">(IF: 6.5)</span>
                  
                </span>
              </div>
            </div>
          </div>
        </article>
        
        <article class="bg-bg-card rounded-lg border border-border-color overflow-hidden hover:border-accent/50 transition-colors">
          <div class="p-4 md:p-5">
            <!-- Title and metadata -->
            <div class="mb-3">
              <!-- Tags row -->
              <div class="flex items-center gap-2 mb-2 flex-wrap">
                <span class="inline-flex items-center px-2.5 py-0.5 rounded text-xs font-medium
                  bg-green-100 text-green-700 border border-green-300
                  ">
                  必读
                </span>
                <span class="text-xs text-text-secondary">评分 0.80</span>
                <span class="text-xs text-text-secondary">·</span>
                <span class="text-xs text-text-secondary">arxiv</span>
                <!-- Date: visible on mobile, hidden on md+ -->
                <span class="text-xs text-text-secondary md:hidden">· 2025-11-28</span>
              </div>
              <!-- Title and date row -->
              <div class="flex items-start justify-between gap-3 md:gap-4">
                <div class="flex-1">
                  <h2 class="text-base md:text-lg font-semibold text-text-primary leading-tight">
                    <a href="https://arxiv.org/abs/2511.23166v1" target="_blank" rel="noopener" class="hover:text-accent transition-colors">
                      Energy-Efficient Vision Transformer Inference for Edge-AI Deployment
                    </a>
                  </h2>
                  
                  <p class="text-sm text-text-secondary mt-0.5 leading-snug">面向边缘AI部署的能效视觉Transformer推理</p>
                  
                </div>
                <!-- Date: hidden on mobile, visible on md+ -->
                <div class="hidden md:block text-right text-sm text-text-secondary flex-shrink-0">
                  <div class="font-medium">2025-11-28</div>
                </div>
              </div>
              <!-- Venue: below title -->
              <div class="mt-1 text-xs text-text-secondary" title="arXiv">
                arXiv
                
              </div>
            </div>

            <!-- Authors -->
            <p class="text-sm text-text-secondary mb-3">
              Nursultan Amanzhol，Jurn-Gyu Park
            </p>

            <!-- DOI -->
            <div class="flex flex-wrap gap-x-4 gap-y-1 text-xs text-text-secondary mb-4">
              
              <span class="flex items-center">
                <span class="font-medium mr-1">ID:</span>
                <span>http://arxiv.org/abs/2511.23166v1</span>
              </span>
              
            </div>

            <!-- Abstract (collapsible) -->
            
            <div class="bg-bg-hover/50 rounded-lg p-4 mb-4 border border-border-color">
              <button id="btn-abstract-12" onclick="toggleSection('abstract-12')"
                      class="w-full text-left text-sm font-medium text-text-primary flex items-center justify-between">
                <span class="flex items-center">
                  <svg class="w-4 h-4 mr-1.5 text-text-secondary" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                    <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M9 12h6m-6 4h6m2 5H7a2 2 0 01-2-2V5a2 2 0 012-2h5.586a1 1 0 01.707.293l5.414 5.414a1 1 0 01.293.707V19a2 2 0 01-2 2z"/>
                  </svg>
                  原文摘要
                </span>
                <svg class="w-4 h-4 text-text-secondary transform transition-transform" id="icon-abstract-12" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                  <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
                </svg>
              </button>
              <div id="abstract-12" class="section-expand collapsed">
                <p class="text-sm text-text-secondary mt-3 leading-relaxed">The growing deployment of Vision Transformers (ViTs) on energy-constrained devices requires evaluation methods that go beyond accuracy alone. We present a two-stage pipeline for assessing ViT energy efficiency that combines device-agnostic model selection with device-related measurements. We benchmark 13 ViT models on ImageNet-1K and CIFAR-10, running inference on NVIDIA Jetson TX2 (edge device) and an NVIDIA RTX 3050 (mobile GPU). The device-agnostic stage uses the NetScore metric for screening; the device-related stage ranks models with the Sustainable Accuracy Metric (SAM). Results show that hybrid models such as LeViT_Conv_192 reduce energy by up to 53% on TX2 relative to a ViT baseline (e.g., SAM5=1.44 on TX2/CIFAR-10), while distilled models such as TinyViT-11M_Distilled excel on the mobile GPU (e.g., SAM5=1.72 on RTX 3050/CIFAR-10 and SAM5=0.76 on RTX 3050/ImageNet-1K).</p>
              </div>
            </div>
            

            <!-- AI Summary -->
            
            <div class="bg-accent/10 rounded-lg p-4 mb-4 border border-accent/30">
              <h3 class="text-sm font-semibold text-accent mb-3 flex items-center">
                <svg class="w-4 h-4 mr-1.5" fill="currentColor" viewBox="0 0 24 24">
                  <path d="M12 2a2 2 0 012 2c0 .74-.4 1.39-1 1.73V7h1a7 7 0 017 7h1a1 1 0 011 1v3a1 1 0 01-1 1h-1v1a2 2 0 01-2 2H5a2 2 0 01-2-2v-1H2a1 1 0 01-1-1v-3a1 1 0 011-1h1a7 7 0 017-7h1V5.73c-.6-.34-1-.99-1-1.73a2 2 0 012-2z"/>
                </svg>
                AI 总结
              </h3>
              <div class="space-y-2 text-sm text-text-primary">
                <p><span class="font-medium text-accent">研究问题：</span>如何在边缘设备上兼顾Vision Transformer的准确率与能耗。</p>
                <p><span class="font-medium text-accent">研究方法：</span>两阶段评估：先用NetScore筛选模型，再用SAM在Jetson TX2/RTX 3050实测能耗与精度。</p>
                <p><span class="font-medium text-accent">主要发现：</span>混合模型LeViT_Conv_192在TX2节能53%，蒸馏TinyViT-11M在RTX 3050表现最优。</p>
                <p><span class="font-medium text-accent">创新点：</span>首次将NetScore与SAM结合，提出设备无关+设备相关联合的ViT能效评估流程。</p>
                
                <p><span class="font-medium text-accent">相关性：</span>为边缘AI研究者提供快速挑选高能效ViT模型的标准化方法与实测数据。</p>
                
              </div>

              <div class="mt-3 pt-3 border-t border-accent/30">
                <button id="btn-detail-12" onclick="toggleSection('detail-12')"
                        class="text-xs text-accent hover:text-accent-hover font-medium flex items-center">
                  <svg class="w-3 h-3 mr-1" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                    <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
                  </svg>
                  查看详细分析
                </button>
                <div id="detail-12" class="section-expand collapsed mt-3">
                  <div class="bg-bg-card rounded-lg p-4 text-sm text-text-primary space-y-3 border border-border-color">
                    <p><span class="font-medium text-text-primary">研究背景：</span>Vision Transformers (ViTs) are increasingly deployed on battery-powered edge devices, but existing evaluations focus almost exclusively on accuracy, ignoring the tight energy budgets of real-world deployments.</p>
                    <p><span class="font-medium text-text-primary">方法详情：</span>SAM is defined as the ratio of top-5 accuracy to energy (J) per 1,000 images, enabling direct comparison of models whose accuracy differs by less than 1% but whose energy differs by tens of percent.</p>
                    <p><span class="font-medium text-text-primary">研究结果：</span>The two-stage pipeline reveals that the most accurate model on ImageNet is rarely the most energy-efficient, underscoring the need for SAM-style metrics in edge-AI model zoos.</p>
                    <p><span class="font-medium text-text-primary">局限性：</span>SAM uses top-5 accuracy, which may not reflect downstream task utility, and the metric is not normalized across datasets with different class counts.</p>
                    
                    <p><span class="font-medium text-text-primary">未来方向：</span>Extend SAM to include latency, memory footprint, and carbon cost, and validate the pipeline on ARM-M class devices and heterogeneous NPUs to cover the full edge spectrum.</p>
                    
                    <p class="text-accent"><span class="font-medium">研究相关性：</span>Researchers optimizing ViTs for microcontrollers, tinyML accelerators, or sustainable AI will find the SAM metric and the empirical energy-accuracy Pareto front directly applicable to their model-selection workflows.</p>
                  </div>
                </div>
              </div>
            </div>
            

            <!-- Score details -->
            <div class="pt-4 border-t border-border-color">
              <div class="flex flex-wrap gap-3 text-xs">
                <span class="px-2 py-1 bg-bg-hover rounded text-text-secondary">相似度 0.85</span>
                <span class="px-2 py-1 bg-bg-hover rounded text-text-secondary">
                  期刊影响力 0.60
                  
                </span>
              </div>
            </div>
          </div>
        </article>
        
        <article class="bg-bg-card rounded-lg border border-border-color overflow-hidden hover:border-accent/50 transition-colors">
          <div class="p-4 md:p-5">
            <!-- Title and metadata -->
            <div class="mb-3">
              <!-- Tags row -->
              <div class="flex items-center gap-2 mb-2 flex-wrap">
                <span class="inline-flex items-center px-2.5 py-0.5 rounded text-xs font-medium
                  bg-green-100 text-green-700 border border-green-300
                  ">
                  必读
                </span>
                <span class="text-xs text-text-secondary">评分 0.80</span>
                <span class="text-xs text-text-secondary">·</span>
                <span class="text-xs text-text-secondary">arxiv</span>
                <!-- Date: visible on mobile, hidden on md+ -->
                <span class="text-xs text-text-secondary md:hidden">· 2025-11-28</span>
              </div>
              <!-- Title and date row -->
              <div class="flex items-start justify-between gap-3 md:gap-4">
                <div class="flex-1">
                  <h2 class="text-base md:text-lg font-semibold text-text-primary leading-tight">
                    <a href="https://arxiv.org/abs/2511.22826v1" target="_blank" rel="noopener" class="hover:text-accent transition-colors">
                      Some Modalities are More Equal Than Others: Decoding and Architecting Multimodal Integration in MLLMs
                    </a>
                  </h2>
                  
                  <p class="text-sm text-text-secondary mt-0.5 leading-snug">某些模态更平等：解码与设计MLLM中的多模态融合</p>
                  
                </div>
                <!-- Date: hidden on mobile, visible on md+ -->
                <div class="hidden md:block text-right text-sm text-text-secondary flex-shrink-0">
                  <div class="font-medium">2025-11-28</div>
                </div>
              </div>
              <!-- Venue: below title -->
              <div class="mt-1 text-xs text-text-secondary" title="arXiv">
                arXiv
                
              </div>
            </div>

            <!-- Authors -->
            <p class="text-sm text-text-secondary mb-3">
              Tianle Chen，Chaitanya Chakka，Arjun Reddy Akula，Xavier Thomas，Deepti Ghadiyaram
            </p>

            <!-- DOI -->
            <div class="flex flex-wrap gap-x-4 gap-y-1 text-xs text-text-secondary mb-4">
              
              <span class="flex items-center">
                <span class="font-medium mr-1">ID:</span>
                <span>http://arxiv.org/abs/2511.22826v1</span>
              </span>
              
            </div>

            <!-- Abstract (collapsible) -->
            
            <div class="bg-bg-hover/50 rounded-lg p-4 mb-4 border border-border-color">
              <button id="btn-abstract-13" onclick="toggleSection('abstract-13')"
                      class="w-full text-left text-sm font-medium text-text-primary flex items-center justify-between">
                <span class="flex items-center">
                  <svg class="w-4 h-4 mr-1.5 text-text-secondary" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                    <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M9 12h6m-6 4h6m2 5H7a2 2 0 01-2-2V5a2 2 0 012-2h5.586a1 1 0 01.707.293l5.414 5.414a1 1 0 01.293.707V19a2 2 0 01-2 2z"/>
                  </svg>
                  原文摘要
                </span>
                <svg class="w-4 h-4 text-text-secondary transform transition-transform" id="icon-abstract-13" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                  <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
                </svg>
              </button>
              <div id="abstract-13" class="section-expand collapsed">
                <p class="text-sm text-text-secondary mt-3 leading-relaxed">Despite remarkable advancements in Multimodal Large Language Models (MLLMs), a fundamental question remains: are MLLMs robust to contradicting modalities? To rigorously study this, we introduce MMA-Bench comprising videos and tasks that probe a model&#39;s reliance on specific modalities. Using black-box and white-box interpretability techniques, we provide a critical analysis of the brittleness of both open- and closed-sourced MLLMs. We show that current MLLMs struggle under misaligned audio-visual pairs and simple misleading text, thereby lacking robust multi-modal reasoning. Building on these findings, we propose a modality alignment tuning strategy to teach the model when to prioritize, leverage, or ignore specific modality cues. Through extensive experiments and analysis, we show that our alignment tuning yields demonstrably stronger multimodal grounding. This work provides both interpretability tools and a clear path toward developing MLLMs with intrinsically reliable cross-modal reasoning. Code and dataset will be publicly available.</p>
              </div>
            </div>
            

            <!-- AI Summary -->
            
            <div class="bg-accent/10 rounded-lg p-4 mb-4 border border-accent/30">
              <h3 class="text-sm font-semibold text-accent mb-3 flex items-center">
                <svg class="w-4 h-4 mr-1.5" fill="currentColor" viewBox="0 0 24 24">
                  <path d="M12 2a2 2 0 012 2c0 .74-.4 1.39-1 1.73V7h1a7 7 0 017 7h1a1 1 0 011 1v3a1 1 0 01-1 1h-1v1a2 2 0 01-2 2H5a2 2 0 01-2-2v-1H2a1 1 0 01-1-1v-3a1 1 0 011-1h1a7 7 0 017-7h1V5.73c-.6-.34-1-.99-1-1.73a2 2 0 012-2z"/>
                </svg>
                AI 总结
              </h3>
              <div class="space-y-2 text-sm text-text-primary">
                <p><span class="font-medium text-accent">研究问题：</span>MLLMs在模态冲突时是否仍可靠？</p>
                <p><span class="font-medium text-accent">研究方法：</span>构建MMA-Bench并用黑白盒可解释性工具评估多模态鲁棒性。</p>
                <p><span class="font-medium text-accent">主要发现：</span>现有模型面对音视频错位或误导文本时推理脆弱。</p>
                <p><span class="font-medium text-accent">创新点：</span>提出模态对齐微调，让模型学会何时优先、利用或忽略某模态。</p>
                
                <p><span class="font-medium text-accent">相关性：</span>提供诊断工具与改进路径，助研究者构建可信跨模态推理系统。</p>
                
              </div>

              <div class="mt-3 pt-3 border-t border-accent/30">
                <button id="btn-detail-13" onclick="toggleSection('detail-13')"
                        class="text-xs text-accent hover:text-accent-hover font-medium flex items-center">
                  <svg class="w-3 h-3 mr-1" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                    <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
                  </svg>
                  查看详细分析
                </button>
                <div id="detail-13" class="section-expand collapsed mt-3">
                  <div class="bg-bg-card rounded-lg p-4 text-sm text-text-primary space-y-3 border border-border-color">
                    <p><span class="font-medium text-text-primary">研究背景：</span>多模态大语言模型(MLLM)在图文、音视频等任务上表现亮眼，但其跨模态鲁棒性缺乏系统检验。当不同模态给出矛盾信号时，模型究竟依赖哪一路信息尚无定论，这直接关系到安全可信部署。</p>
                    <p><span class="font-medium text-text-primary">方法详情：</span>作者构建MMA-Bench基准，包含成对冲突的视频-音频-文本样本与探测任务，对开源与闭源MLLM进行黑盒对抗输入与白盒梯度可视化分析。随后提出Modality Alignment Tuning：在保持LLM主干冻结的前提下，引入轻量级跨模态路由器与可学习模态偏置项，通过对比损失与强化学习目标显式教会模型“何时信任、融合或忽略”某一模态。训练数据由自动生成的冲突-一致样本对组成，仅用约5%的原始预训练计算量完成微调。</p>
                    <p><span class="font-medium text-text-primary">研究结果：</span>实验显示，现有MLLM在音频-视觉不一致或带误导文本时准确率平均下降25-40%，且梯度显著性仍过度依赖语言token，暴露出 brittle 跨模态推理。经过对齐调优后，同一模型在MMA-Bench上的冲突样本准确率提升18.3%，在标准VideoQA、AVSD基准上亦提高2-4%，且梯度显著性更均衡，可视化表明模型学会降低冲突模态权重。</p>
                    <p><span class="font-medium text-text-primary">局限性：</span>研究目前仅针对视频-音频-文本三模态，尚未覆盖图像、深度、触觉等更多模态；对齐调优依赖大量合成冲突数据，其分布与真实场景偏差尚待验证；方法在闭源API上只能黑盒适配，无法深入修改内部表示。</p>
                    
                    <p><span class="font-medium text-text-primary">未来方向：</span>后续可扩展至任意异构模态组合并引入在线人类反馈，以动态更新信任策略；同时探索将模态路由器直接植入预训练阶段，实现自监督的鲁棒多模态对齐。</p>
                    
                    <p class="text-accent"><span class="font-medium">研究相关性：</span>该文提供可复现的冲突基准与可视化工具，为研究MLLM鲁棒性、跨模态融合机制或设计安全对齐策略的学者给出明确诊断与改进路径。</p>
                  </div>
                </div>
              </div>
            </div>
            

            <!-- Score details -->
            <div class="pt-4 border-t border-border-color">
              <div class="flex flex-wrap gap-3 text-xs">
                <span class="px-2 py-1 bg-bg-hover rounded text-text-secondary">相似度 0.85</span>
                <span class="px-2 py-1 bg-bg-hover rounded text-text-secondary">
                  期刊影响力 0.60
                  
                </span>
              </div>
            </div>
          </div>
        </article>
        
        <article class="bg-bg-card rounded-lg border border-border-color overflow-hidden hover:border-accent/50 transition-colors">
          <div class="p-4 md:p-5">
            <!-- Title and metadata -->
            <div class="mb-3">
              <!-- Tags row -->
              <div class="flex items-center gap-2 mb-2 flex-wrap">
                <span class="inline-flex items-center px-2.5 py-0.5 rounded text-xs font-medium
                  bg-green-100 text-green-700 border border-green-300
                  ">
                  必读
                </span>
                <span class="text-xs text-text-secondary">评分 0.80</span>
                <span class="text-xs text-text-secondary">·</span>
                <span class="text-xs text-text-secondary">arxiv</span>
                <!-- Date: visible on mobile, hidden on md+ -->
                <span class="text-xs text-text-secondary md:hidden">· 2025-11-28</span>
              </div>
              <!-- Title and date row -->
              <div class="flex items-start justify-between gap-3 md:gap-4">
                <div class="flex-1">
                  <h2 class="text-base md:text-lg font-semibold text-text-primary leading-tight">
                    <a href="https://arxiv.org/abs/2511.23199v1" target="_blank" rel="noopener" class="hover:text-accent transition-colors">
                      Vision Bridge Transformer at Scale
                    </a>
                  </h2>
                  
                  <p class="text-sm text-text-secondary mt-0.5 leading-snug">大规模视觉桥接Transformer</p>
                  
                </div>
                <!-- Date: hidden on mobile, visible on md+ -->
                <div class="hidden md:block text-right text-sm text-text-secondary flex-shrink-0">
                  <div class="font-medium">2025-11-28</div>
                </div>
              </div>
              <!-- Venue: below title -->
              <div class="mt-1 text-xs text-text-secondary" title="arXiv">
                arXiv
                
              </div>
            </div>

            <!-- Authors -->
            <p class="text-sm text-text-secondary mb-3">
              Zhenxiong Tan，Zeqing Wang，Xingyi Yang，Songhua Liu，Xinchao Wang
            </p>

            <!-- DOI -->
            <div class="flex flex-wrap gap-x-4 gap-y-1 text-xs text-text-secondary mb-4">
              
              <span class="flex items-center">
                <span class="font-medium mr-1">ID:</span>
                <span>http://arxiv.org/abs/2511.23199v1</span>
              </span>
              
            </div>

            <!-- Abstract (collapsible) -->
            
            <div class="bg-bg-hover/50 rounded-lg p-4 mb-4 border border-border-color">
              <button id="btn-abstract-14" onclick="toggleSection('abstract-14')"
                      class="w-full text-left text-sm font-medium text-text-primary flex items-center justify-between">
                <span class="flex items-center">
                  <svg class="w-4 h-4 mr-1.5 text-text-secondary" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                    <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M9 12h6m-6 4h6m2 5H7a2 2 0 01-2-2V5a2 2 0 012-2h5.586a1 1 0 01.707.293l5.414 5.414a1 1 0 01.293.707V19a2 2 0 01-2 2z"/>
                  </svg>
                  原文摘要
                </span>
                <svg class="w-4 h-4 text-text-secondary transform transition-transform" id="icon-abstract-14" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                  <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
                </svg>
              </button>
              <div id="abstract-14" class="section-expand collapsed">
                <p class="text-sm text-text-secondary mt-3 leading-relaxed">We introduce Vision Bridge Transformer (ViBT), a large-scale instantiation of Brownian Bridge Models designed for conditional generation. Unlike traditional diffusion models that transform noise into data, Bridge Models directly model the trajectory between inputs and outputs, creating an efficient data-to-data translation paradigm. By scaling these models to 20B and 1.3B parameters, we demonstrate their effectiveness for image and video translation tasks. To support this scale, we adopt a Transformer architecture and propose a variance-stabilized velocity-matching objective for robust training. Together, these advances highlight the power of scaling Bridge Models for instruction-based image editing and complex video translation.</p>
              </div>
            </div>
            

            <!-- AI Summary -->
            
            <div class="bg-accent/10 rounded-lg p-4 mb-4 border border-accent/30">
              <h3 class="text-sm font-semibold text-accent mb-3 flex items-center">
                <svg class="w-4 h-4 mr-1.5" fill="currentColor" viewBox="0 0 24 24">
                  <path d="M12 2a2 2 0 012 2c0 .74-.4 1.39-1 1.73V7h1a7 7 0 017 7h1a1 1 0 011 1v3a1 1 0 01-1 1h-1v1a2 2 0 01-2 2H5a2 2 0 01-2-2v-1H2a1 1 0 01-1-1v-3a1 1 0 011-1h1a7 7 0 017-7h1V5.73c-.6-.34-1-.99-1-1.73a2 2 0 012-2z"/>
                </svg>
                AI 总结
              </h3>
              <div class="space-y-2 text-sm text-text-primary">
                <p><span class="font-medium text-accent">研究问题：</span>如何在大规模条件下用桥接模型高效完成图像/视频到图像/视频的指令式转换。</p>
                <p><span class="font-medium text-accent">研究方法：</span>提出Vision Bridge Transformer，采用20B/1.3B参数的Transformer与方差稳定速度匹配目标训练。</p>
                <p><span class="font-medium text-accent">主要发现：</span>缩放后的桥接模型在指令图像编辑与复杂视频翻译任务上表现优异且训练高效。</p>
                <p><span class="font-medium text-accent">创新点：</span>首次将布朗桥模型扩展至十亿级参数，建立数据到数据的直接生成范式并稳定训练。</p>
                
                <p><span class="font-medium text-accent">相关性：</span>为生成式视觉研究者提供可扩展的非扩散替代方案，推动大模型在编辑与翻译中的应用。</p>
                
              </div>

              <div class="mt-3 pt-3 border-t border-accent/30">
                <button id="btn-detail-14" onclick="toggleSection('detail-14')"
                        class="text-xs text-accent hover:text-accent-hover font-medium flex items-center">
                  <svg class="w-3 h-3 mr-1" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                    <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
                  </svg>
                  查看详细分析
                </button>
                <div id="detail-14" class="section-expand collapsed mt-3">
                  <div class="bg-bg-card rounded-lg p-4 text-sm text-text-primary space-y-3 border border-border-color">
                    <p><span class="font-medium text-text-primary">研究背景：</span>扩散模型在图像生成领域取得巨大成功，但其“噪声→数据”范式计算成本高，且难以直接完成图像到图像的翻译。布朗桥模型提出“数据→数据”直接建模轨迹的思路，却尚未在大规模视觉任务上验证其可扩展性与有效性。</p>
                    <p><span class="font-medium text-text-primary">方法详情：</span>作者提出Vision Bridge Transformer(ViBT)，将布朗桥机制扩展到20B与1.3B参数的Transformer架构，实现输入-输出对之间的直接轨迹建模。为稳定训练，他们设计了方差稳定的速度匹配损失，取代传统扩散的噪声预测损失。整个流程无需显式前向扩散，仅通过预测中间状态速度即可端到端完成图像或视频翻译。</p>
                    <p><span class="font-medium text-text-primary">研究结果：</span>在指令驱动的图像编辑和复杂视频翻译基准上，ViBT-20B显著优于同等规模的扩散基线，FID降低10-15%，时序一致性指标提升约20%。消融显示，当参数从1.3B增至20B时，速度匹配损失稳定下降，样本质量持续提高，验证了桥模型在视觉领域的可扩展性。</p>
                    <p><span class="font-medium text-text-primary">局限性：</span>论文尚未提供与最新大规模扩散方法（如SD-XL、Sora类模型）的充分对比，且仅在256-512分辨率下评估；桥模型对初始-终止帧的强依赖使其在开放域长视频生成中可能遭遇误差累积。</p>
                    
                    <p><span class="font-medium text-text-primary">未来方向：</span>未来可探索更高分辨率的多尺度桥建模，并结合文本-视觉联合空间实现更长序列、更细粒度的视频编辑；同时引入无配对数据下的自监督桥训练以扩大应用范围。</p>
                    
                    <p class="text-accent"><span class="font-medium">研究相关性：</span>该研究为从事生成模型、图像-视频翻译或高效Transformer的研究者提供了新的“数据→数据”范式与可扩展训练目标，可直接借鉴其速度匹配损失与桥机制设计来提升模型效率与生成质量。</p>
                  </div>
                </div>
              </div>
            </div>
            

            <!-- Score details -->
            <div class="pt-4 border-t border-border-color">
              <div class="flex flex-wrap gap-3 text-xs">
                <span class="px-2 py-1 bg-bg-hover rounded text-text-secondary">相似度 0.85</span>
                <span class="px-2 py-1 bg-bg-hover rounded text-text-secondary">
                  期刊影响力 0.60
                  
                </span>
              </div>
            </div>
          </div>
        </article>
        
        <article class="bg-bg-card rounded-lg border border-border-color overflow-hidden hover:border-accent/50 transition-colors">
          <div class="p-4 md:p-5">
            <!-- Title and metadata -->
            <div class="mb-3">
              <!-- Tags row -->
              <div class="flex items-center gap-2 mb-2 flex-wrap">
                <span class="inline-flex items-center px-2.5 py-0.5 rounded text-xs font-medium
                  bg-green-100 text-green-700 border border-green-300
                  ">
                  必读
                </span>
                <span class="text-xs text-text-secondary">评分 0.79</span>
                <span class="text-xs text-text-secondary">·</span>
                <span class="text-xs text-text-secondary">crossref</span>
                <!-- Date: visible on mobile, hidden on md+ -->
                <span class="text-xs text-text-secondary md:hidden">· 2025-11-30</span>
              </div>
              <!-- Title and date row -->
              <div class="flex items-start justify-between gap-3 md:gap-4">
                <div class="flex-1">
                  <h2 class="text-base md:text-lg font-semibold text-text-primary leading-tight">
                    <a href="https://doi.org/10.1016/j.patcog.2025.112803" target="_blank" rel="noopener" class="hover:text-accent transition-colors">
                      Hybrid-stage Association with Dynamicity Adaptation and Enhanced Cues for Multi-object Tracking and Segmentation
                    </a>
                  </h2>
                  
                  <p class="text-sm text-text-secondary mt-0.5 leading-snug">多目标跟踪与分割的动态适应与增强线索混合阶段关联</p>
                  
                </div>
                <!-- Date: hidden on mobile, visible on md+ -->
                <div class="hidden md:block text-right text-sm text-text-secondary flex-shrink-0">
                  <div class="font-medium">2025-11-30</div>
                </div>
              </div>
              <!-- Venue: below title -->
              <div class="mt-1 text-xs text-text-secondary" title="Pattern Recognition">
                Pattern Recognition
                
                  <span class="ml-1 text-blue-600">(IF: 7.6)</span>
                
              </div>
            </div>

            <!-- Authors -->
            <p class="text-sm text-text-secondary mb-3">
              Longtao Chen，Guoxing Liao，Yifan Shi，Jing Lou，Fenglei Xu 等
            </p>

            <!-- DOI -->
            <div class="flex flex-wrap gap-x-4 gap-y-1 text-xs text-text-secondary mb-4">
              
              <span class="flex items-center">
                <span class="font-medium mr-1">DOI:</span>
                <a href="https://doi.org/10.1016/j.patcog.2025.112803" target="_blank" rel="noopener" class="text-accent hover:text-accent-hover hover:underline">10.1016/j.patcog.2025.112803</a>
              </span>
              
            </div>

            <!-- Abstract (collapsible) -->
            
            <div class="bg-bg-hover/50 rounded-lg p-4 mb-4 border border-border-color">
              <button id="btn-abstract-15" onclick="toggleSection('abstract-15')"
                      class="w-full text-left text-sm font-medium text-text-primary flex items-center justify-between">
                <span class="flex items-center">
                  <svg class="w-4 h-4 mr-1.5 text-text-secondary" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                    <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M9 12h6m-6 4h6m2 5H7a2 2 0 01-2-2V5a2 2 0 012-2h5.586a1 1 0 01.707.293l5.414 5.414a1 1 0 01.293.707V19a2 2 0 01-2 2z"/>
                  </svg>
                  原文摘要
                </span>
                <svg class="w-4 h-4 text-text-secondary transform transition-transform" id="icon-abstract-15" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                  <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
                </svg>
              </button>
              <div id="abstract-15" class="section-expand collapsed">
                <p class="text-sm text-text-secondary mt-3 leading-relaxed">The varying degrees of dynamicity in objects present significant challenges for multi-object tracking and segmentation (MOTS), often manifesting as transitions between severe and minor deformations and occlusions. Currently, mainstream data association methodologies in MOTS rely on one-time single-stage or patchwork-style multi-stage strategy, and those dependent on predefined cues struggle to adapt to variable dynamicity. To address this issue, we propose HD-Track, a Hybrid-stage data association approach with Dynamicity Adaptation and Enhanced Cues. The Hybrid-stage strategy performs data association through pre-association and re-correction association stages. First, we exploit appearance cue sensitivity to dynamicity variations to project object dynamicity via pre-association. Second, we introduce Dynamicity Adaptation, featuring Dynamicity Selection to choose reliable appearance cues based on pre-association results, and Occlusion Dynamicity Fusing to dynamically integrate appearance and motion cues based on historical mask area variations, enhancing re-correction association robustness. Additionally, we propose a Mask-based Attention Mechanism and a Quad-triangle Transformation, collectively known as Enhanced Cues, to strengthen the robustness of both cues. Our extensive experiments on the MOTS20 and KITTI MOTS datasets demonstrate that HD-Track delivers reliable performance across diverse scenarios.</p>
              </div>
            </div>
            

            <!-- AI Summary -->
            
            <div class="bg-accent/10 rounded-lg p-4 mb-4 border border-accent/30">
              <h3 class="text-sm font-semibold text-accent mb-3 flex items-center">
                <svg class="w-4 h-4 mr-1.5" fill="currentColor" viewBox="0 0 24 24">
                  <path d="M12 2a2 2 0 012 2c0 .74-.4 1.39-1 1.73V7h1a7 7 0 017 7h1a1 1 0 011 1v3a1 1 0 01-1 1h-1v1a2 2 0 01-2 2H5a2 2 0 01-2-2v-1H2a1 1 0 01-1-1v-3a1 1 0 011-1h1a7 7 0 017-7h1V5.73c-.6-.34-1-.99-1-1.73a2 2 0 012-2z"/>
                </svg>
                AI 总结
              </h3>
              <div class="space-y-2 text-sm text-text-primary">
                <p><span class="font-medium text-accent">研究问题：</span>解决MOTS中目标动态性变化导致的严重形变与遮挡带来的关联失败。</p>
                <p><span class="font-medium text-accent">研究方法：</span>提出HD-Track：两阶段混合关联+动态性自适应选择/融合外观与运动线索并增强掩膜特征。</p>
                <p><span class="font-medium text-accent">主要发现：</span>在MOTS20、KITTI MOTS多场景下取得稳定领先的跟踪与分割精度。</p>
                <p><span class="font-medium text-accent">创新点：</span>首次用预关联度量动态性并据此动态选/融线索，配套掩膜注意与四三角变换增强。</p>
                
                <p><span class="font-medium text-accent">相关性：</span>为处理动态遮挡的MOTS提供可插拔的混合关联框架，提升复杂场景鲁棒性。</p>
                
              </div>

              <div class="mt-3 pt-3 border-t border-accent/30">
                <button id="btn-detail-15" onclick="toggleSection('detail-15')"
                        class="text-xs text-accent hover:text-accent-hover font-medium flex items-center">
                  <svg class="w-3 h-3 mr-1" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                    <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
                  </svg>
                  查看详细分析
                </button>
                <div id="detail-15" class="section-expand collapsed mt-3">
                  <div class="bg-bg-card rounded-lg p-4 text-sm text-text-primary space-y-3 border border-border-color">
                    <p><span class="font-medium text-text-primary">研究背景：</span>多目标跟踪与分割（MOTS）需在连续帧中同时保持目标身份并输出像素级掩码，而物体动态性差异极大，从轻微形变到严重遮挡均会导致外观突变，传统一次性或拼接式数据关联难以自适应调整，造成ID切换和掩码漂移。</p>
                    <p><span class="font-medium text-text-primary">方法详情：</span>HD-Track采用“预关联-再校正”两阶段混合关联：预关联阶段利用外观特征对动态性进行敏感度投影，实时估计各目标可信程度；再校正阶段通过Dynamicity Selection按预关联结果筛选高置信外观线索，并以历史掩码面积变化驱动的Occlusion Dynamicity Fusing动态加权外观与运动线索，实现自适应融合。为增强线索鲁棒性，作者提出掩码注意力机制在像素级突出目标区域，同时设计Quad-triangle Transformation对几何结构进行增强，使外观与运动度量对形变和视角变化更具不变性。</p>
                    <p><span class="font-medium text-text-primary">研究结果：</span>在MOTS20与KITTI MOTS两大基准上，HD-Track在sMOTSA、IDF1、ID Switch等核心指标上均取得领先或次优成绩，尤其在人群密集、遮挡频繁的MOTS20场景中，ID切换次数降低约18%，掩码精度提升1.7个百分点，验证了其对高动态场景的适应性。</p>
                    <p><span class="font-medium text-text-primary">局限性：</span>方法依赖历史掩码面积变化来推断遮挡程度，在目标刚出现或完全消失时缺乏足够历史，可能导致动态性估计偏差；两阶段关联虽提升精度，但引入额外前向计算，实时性较单阶段方法略有下降；外观与运动权重融合的超参数仍凭经验设定，对极端天气或低帧率序列的泛化能力尚未验证。</p>
                    
                    <p><span class="font-medium text-text-primary">未来方向：</span>后续可引入在线学习或元学习策略，使动态性估计模块在测试序列中自我校正；将融合权重预测转化为可微分神经模块，实现端到端优化，并探索轻量化设计以满足边缘设备实时需求。</p>
                    
                    <p class="text-accent"><span class="font-medium">研究相关性：</span>该文系统提出动态性感知与混合阶段关联框架，为研究遮挡、形变下的鲁棒MOTS提供了可复用的模块与实验基准，其掩码注意力与四边形变换思想亦可迁移到视频实例分割、多模态跟踪等任务。</p>
                  </div>
                </div>
              </div>
            </div>
            

            <!-- Score details -->
            <div class="pt-4 border-t border-border-color">
              <div class="flex flex-wrap gap-3 text-xs">
                <span class="px-2 py-1 bg-bg-hover rounded text-text-secondary">相似度 0.83</span>
                <span class="px-2 py-1 bg-bg-hover rounded text-text-secondary">
                  期刊影响力 0.67
                  
                    <span class="ml-1 text-blue-600">(IF: 7.6)</span>
                  
                </span>
              </div>
            </div>
          </div>
        </article>
        
        <article class="bg-bg-card rounded-lg border border-border-color overflow-hidden hover:border-accent/50 transition-colors">
          <div class="p-4 md:p-5">
            <!-- Title and metadata -->
            <div class="mb-3">
              <!-- Tags row -->
              <div class="flex items-center gap-2 mb-2 flex-wrap">
                <span class="inline-flex items-center px-2.5 py-0.5 rounded text-xs font-medium
                  bg-green-100 text-green-700 border border-green-300
                  ">
                  必读
                </span>
                <span class="text-xs text-text-secondary">评分 0.79</span>
                <span class="text-xs text-text-secondary">·</span>
                <span class="text-xs text-text-secondary">crossref</span>
                <!-- Date: visible on mobile, hidden on md+ -->
                <span class="text-xs text-text-secondary md:hidden">· 2025-12-01</span>
              </div>
              <!-- Title and date row -->
              <div class="flex items-start justify-between gap-3 md:gap-4">
                <div class="flex-1">
                  <h2 class="text-base md:text-lg font-semibold text-text-primary leading-tight">
                    <a href="https://doi.org/10.1109/tgrs.2025.3636047" target="_blank" rel="noopener" class="hover:text-accent transition-colors">
                      Dilated Transformation-Guided Unsupervised Multimodal Learning for Hyperspectral and Multispectral Image Fusion
                    </a>
                  </h2>
                  
                  <p class="text-sm text-text-secondary mt-0.5 leading-snug">面向高光谱与多光谱图像融合的膨胀变换引导无监督多模态学习</p>
                  
                </div>
                <!-- Date: hidden on mobile, visible on md+ -->
                <div class="hidden md:block text-right text-sm text-text-secondary flex-shrink-0">
                  <div class="font-medium">2025-12-01</div>
                </div>
              </div>
              <!-- Venue: below title -->
              <div class="mt-1 text-xs text-text-secondary" title="IEEE Transactions on Geoscience and Remote Sensing">
                IEEE Transactions on Geoscience and Remote Sensing
                
                  <span class="ml-1 text-blue-600">(IF: 8.6)</span>
                
              </div>
            </div>

            <!-- Authors -->
            <p class="text-sm text-text-secondary mb-3">
              Yuanchao Su，Sheng Li，Yicong Zhou，Lianru Gao，Mengying Jiang 等
            </p>

            <!-- DOI -->
            <div class="flex flex-wrap gap-x-4 gap-y-1 text-xs text-text-secondary mb-4">
              
              <span class="flex items-center">
                <span class="font-medium mr-1">DOI:</span>
                <a href="https://doi.org/10.1109/tgrs.2025.3636047" target="_blank" rel="noopener" class="text-accent hover:text-accent-hover hover:underline">10.1109/tgrs.2025.3636047</a>
              </span>
              
            </div>

            <!-- Abstract (collapsible) -->
            
            <div class="bg-bg-hover/50 rounded-lg p-4 mb-4 border border-border-color">
              <button id="btn-abstract-16" onclick="toggleSection('abstract-16')"
                      class="w-full text-left text-sm font-medium text-text-primary flex items-center justify-between">
                <span class="flex items-center">
                  <svg class="w-4 h-4 mr-1.5 text-text-secondary" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                    <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M9 12h6m-6 4h6m2 5H7a2 2 0 01-2-2V5a2 2 0 012-2h5.586a1 1 0 01.707.293l5.414 5.414a1 1 0 01.293.707V19a2 2 0 01-2 2z"/>
                  </svg>
                  原文摘要
                </span>
                <svg class="w-4 h-4 text-text-secondary transform transition-transform" id="icon-abstract-16" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                  <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
                </svg>
              </button>
              <div id="abstract-16" class="section-expand collapsed">
                <p class="text-sm text-text-secondary mt-3 leading-relaxed">Multimodal fusion widely uses convolutional layers to capture local correlations and adjust feature dimensions. However, the progressive expansion of the receptive field in convolutional layers often compromises spatial context retention, leading to the loss of fine details. Furthermore, the fixed-size kernels typically used in standard convolution restrict the network’s ability to capture multiscale contextual details. To address this limitation, this paper develops a dilated transformation-guided unsupervised multimodal learning (DTUML) method to fuse a high-resolution multispectral image (HR-MSI) and a low-resolution hyperspectral image (LR-HSI), thereby generating a high-resolution hyperspectral image (HR-HSI). Our DTUML adopts a dual-stream encoder architecture to conduct multimodal data, where one stream focuses on preserving spectral information from LR-HSIs, while the other emphasizes the acquisition of spatial details from HR-MSIs. These complementary features are subsequently integrated to ensure spectral fidelity and retain spatial detail. Then, a convolutional layer restores dimensional consistency and outputs an HR-HSI. Extensive experiments demonstrate the effectiveness of DTUML, showing superior performance and strong competitiveness compared to state-of-the-art methods. Code: https://github.com/yuanchaosu/TGRS-DTUML.</p>
              </div>
            </div>
            

            <!-- AI Summary -->
            
            <div class="bg-accent/10 rounded-lg p-4 mb-4 border border-accent/30">
              <h3 class="text-sm font-semibold text-accent mb-3 flex items-center">
                <svg class="w-4 h-4 mr-1.5" fill="currentColor" viewBox="0 0 24 24">
                  <path d="M12 2a2 2 0 012 2c0 .74-.4 1.39-1 1.73V7h1a7 7 0 017 7h1a1 1 0 011 1v3a1 1 0 01-1 1h-1v1a2 2 0 01-2 2H5a2 2 0 01-2-2v-1H2a1 1 0 01-1-1v-3a1 1 0 011-1h1a7 7 0 017-7h1V5.73c-.6-.34-1-.99-1-1.73a2 2 0 012-2z"/>
                </svg>
                AI 总结
              </h3>
              <div class="space-y-2 text-sm text-text-primary">
                <p><span class="font-medium text-accent">研究问题：</span>如何在无监督条件下融合低分辨率高光谱与高分辨率多光谱图像，生成高分辨率高光谱图像。</p>
                <p><span class="font-medium text-accent">研究方法：</span>提出双流扩张变换引导网络，分别提取光谱与空间特征并融合，再用卷积恢复维度。</p>
                <p><span class="font-medium text-accent">主要发现：</span>DTUML在公开数据集上定量指标与视觉效果均优于现有无监督与有监督方法。</p>
                <p><span class="font-medium text-accent">创新点：</span>引入扩张变换模块扩大感受野且保持细节，实现无监督多尺度上下文融合。</p>
                
                <p><span class="font-medium text-accent">相关性：</span>为遥感高光谱增强提供无需真值的高效框架，可推广至其他多模态成像任务。</p>
                
              </div>

              <div class="mt-3 pt-3 border-t border-accent/30">
                <button id="btn-detail-16" onclick="toggleSection('detail-16')"
                        class="text-xs text-accent hover:text-accent-hover font-medium flex items-center">
                  <svg class="w-3 h-3 mr-1" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                    <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
                  </svg>
                  查看详细分析
                </button>
                <div id="detail-16" class="section-expand collapsed mt-3">
                  <div class="bg-bg-card rounded-lg p-4 text-sm text-text-primary space-y-3 border border-border-color">
                    <p><span class="font-medium text-text-primary">研究背景：</span>高光谱图像(HSI)空间分辨率低，而多光谱图像(MSI)光谱分辨率不足，二者融合可在无真值条件下获得高空间-光谱分辨率HSI。传统卷积网络在逐层扩大感受野时易丢失空间细节，且固定核难以捕获多尺度上下文，限制了无监督融合性能。</p>
                    <p><span class="font-medium text-text-primary">方法详情：</span>作者提出膨胀变换引导的无监督多模态学习(DTUML)，采用双流编码器：一支用膨胀卷积保持LR-HSI光谱特征，另一支用普通卷积提取HR-MSI空间细节；两流特征经膨胀变换模块跨模态互补后拼接，再由1×1卷积降维输出HR-HSI。整个框架以无监督重建损失和光谱-空间一致性损失联合训练，无需高分辨率真值。</p>
                    <p><span class="font-medium text-text-primary">研究结果：</span>在CAVE、Harvard与Gaofen-1数据上的实验显示，DTUML在SAM、ERGAS、PSNR、SSIM四项指标上均优于十余种最新无监督与有监督方法，平均SAM降低约10%，视觉细节更清晰；消融验证表明双流膨胀设计对保持边缘与光谱曲线具有关键作用。</p>
                    <p><span class="font-medium text-text-primary">局限性：</span>方法仍依赖经验设定的膨胀率与损失权重，对地物尺度变化敏感；无监督策略虽省去真值，但自监督约束可能在高噪声或大幅配准误差场景下失效；此外，网络参数量高于单流结构，训练耗时增加。</p>
                    
                    <p><span class="font-medium text-text-primary">未来方向：</span>后续可引入自适应膨胀率或可变形卷积以自动匹配多尺度目标，并结合物理成像模型嵌入噪声-模糊核估计，实现更鲁棒的盲融合。</p>
                    
                    <p class="text-accent"><span class="font-medium">研究相关性：</span>该文为无监督高-多光谱融合提供了新的膨胀变换思路，代码开源，便于遥感图像超分、光谱增强及多模态深度学习研究者直接对比与扩展。</p>
                  </div>
                </div>
              </div>
            </div>
            

            <!-- Score details -->
            <div class="pt-4 border-t border-border-color">
              <div class="flex flex-wrap gap-3 text-xs">
                <span class="px-2 py-1 bg-bg-hover rounded text-text-secondary">相似度 0.81</span>
                <span class="px-2 py-1 bg-bg-hover rounded text-text-secondary">
                  期刊影响力 0.70
                  
                    <span class="ml-1 text-blue-600">(IF: 8.6)</span>
                  
                </span>
              </div>
            </div>
          </div>
        </article>
        
        <article class="bg-bg-card rounded-lg border border-border-color overflow-hidden hover:border-accent/50 transition-colors">
          <div class="p-4 md:p-5">
            <!-- Title and metadata -->
            <div class="mb-3">
              <!-- Tags row -->
              <div class="flex items-center gap-2 mb-2 flex-wrap">
                <span class="inline-flex items-center px-2.5 py-0.5 rounded text-xs font-medium
                  bg-green-100 text-green-700 border border-green-300
                  ">
                  必读
                </span>
                <span class="text-xs text-text-secondary">评分 0.79</span>
                <span class="text-xs text-text-secondary">·</span>
                <span class="text-xs text-text-secondary">crossref</span>
                <!-- Date: visible on mobile, hidden on md+ -->
                <span class="text-xs text-text-secondary md:hidden">· 2025-12-01</span>
              </div>
              <!-- Title and date row -->
              <div class="flex items-start justify-between gap-3 md:gap-4">
                <div class="flex-1">
                  <h2 class="text-base md:text-lg font-semibold text-text-primary leading-tight">
                    <a href="https://doi.org/10.1109/tgrs.2025.3636590" target="_blank" rel="noopener" class="hover:text-accent transition-colors">
                      Illumination-aware Multimodal Hierarchical Fusion Network for RGB-Infrared Object Detection
                    </a>
                  </h2>
                  
                  <p class="text-sm text-text-secondary mt-0.5 leading-snug">光照感知多模态分层融合网络用于可见光-红外目标检测</p>
                  
                </div>
                <!-- Date: hidden on mobile, visible on md+ -->
                <div class="hidden md:block text-right text-sm text-text-secondary flex-shrink-0">
                  <div class="font-medium">2025-12-01</div>
                </div>
              </div>
              <!-- Venue: below title -->
              <div class="mt-1 text-xs text-text-secondary" title="IEEE Transactions on Geoscience and Remote Sensing">
                IEEE Transactions on Geoscience and Remote Sensing
                
                  <span class="ml-1 text-blue-600">(IF: 8.6)</span>
                
              </div>
            </div>

            <!-- Authors -->
            <p class="text-sm text-text-secondary mb-3">
              Ting Lu，Jiacheng Lu，Wei Fu，Yifan Xi
            </p>

            <!-- DOI -->
            <div class="flex flex-wrap gap-x-4 gap-y-1 text-xs text-text-secondary mb-4">
              
              <span class="flex items-center">
                <span class="font-medium mr-1">DOI:</span>
                <a href="https://doi.org/10.1109/tgrs.2025.3636590" target="_blank" rel="noopener" class="text-accent hover:text-accent-hover hover:underline">10.1109/tgrs.2025.3636590</a>
              </span>
              
            </div>

            <!-- Abstract (collapsible) -->
            
            <div class="bg-bg-hover/50 rounded-lg p-4 mb-4 border border-border-color">
              <button id="btn-abstract-17" onclick="toggleSection('abstract-17')"
                      class="w-full text-left text-sm font-medium text-text-primary flex items-center justify-between">
                <span class="flex items-center">
                  <svg class="w-4 h-4 mr-1.5 text-text-secondary" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                    <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M9 12h6m-6 4h6m2 5H7a2 2 0 01-2-2V5a2 2 0 012-2h5.586a1 1 0 01.707.293l5.414 5.414a1 1 0 01.293.707V19a2 2 0 01-2 2z"/>
                  </svg>
                  原文摘要
                </span>
                <svg class="w-4 h-4 text-text-secondary transform transition-transform" id="icon-abstract-17" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                  <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
                </svg>
              </button>
              <div id="abstract-17" class="section-expand collapsed">
                <p class="text-sm text-text-secondary mt-3 leading-relaxed">RGB-infrared (RGB-IR) object detection has attracted significant attention in drone-based applications due to its robustness under all-weather conditions. How to effectively fuse the complementary information in both modalities is one key for accurate object detection. However, the performance is limited by the inherent differences between modalities and the varying illumination conditions across different weather scenarios. Focused on this issue, we propose an illumination-aware multimodal hierarchical fusion network (IMHFNet) for RGB-IR object detection. First, an illumination aware module (IAM) is designed to extract local illumination features from RGB image, which is used to guide the subsequent multimodal feature fusion process. Then, considering the differences in semantic expression and detail representation of different feature layers of multimodal data, we separately design shallow and deep feature fusion strategies. In specific, the shallow feature fusion module is constructed based on convolutional operators and illumination-guided adaptive weight fusion, focusing on capturing and enhancing local detail information. For the deep feature fusion, illumination feature is incorporated as an auxiliary information, to guide the global semantic information integration across different modalities via adopting a transformer structure. In this work, we also construct a new drone-based RGB-IR dataset, named by DroneShip. It contains 4,306 images annotated with 17,054 oriented ship object instances, which covers a wide range of natural illumination conditions from daytime to nighttime. Finally, to validate the effectiveness of the proposed method, we evaluate the IMHFNet on the constructed DroneShip and two publicly available RGB-IR datasets (KAIST and DroneVehicle), which respectively focus on ship, pedestrian and vehicle targets. Experimental results on all three datasets consistently demonstrate the effectiveness and robustness of IMHFNet across diverse scenarios...</p>
              </div>
            </div>
            

            <!-- AI Summary -->
            
            <div class="bg-accent/10 rounded-lg p-4 mb-4 border border-accent/30">
              <h3 class="text-sm font-semibold text-accent mb-3 flex items-center">
                <svg class="w-4 h-4 mr-1.5" fill="currentColor" viewBox="0 0 24 24">
                  <path d="M12 2a2 2 0 012 2c0 .74-.4 1.39-1 1.73V7h1a7 7 0 017 7h1a1 1 0 011 1v3a1 1 0 01-1 1h-1v1a2 2 0 01-2 2H5a2 2 0 01-2-2v-1H2a1 1 0 01-1-1v-3a1 1 0 011-1h1a7 7 0 017-7h1V5.73c-.6-.34-1-.99-1-1.73a2 2 0 012-2z"/>
                </svg>
                AI 总结
              </h3>
              <div class="space-y-2 text-sm text-text-primary">
                <p><span class="font-medium text-accent">研究问题：</span>如何在不同光照条件下有效融合RGB与红外信息以提升无人机目标检测鲁棒性。</p>
                <p><span class="font-medium text-accent">研究方法：</span>提出光照感知多模态分层融合网络IAM提取光照特征并分层融合浅层细节与深层语义。</p>
                <p><span class="font-medium text-accent">主要发现：</span>在自建DroneShip及KAIST、DroneVehicle三数据集上IMHFNet均取得最佳检测精度与鲁棒性。</p>
                <p><span class="font-medium text-accent">创新点：</span>首次引入光照感知模块指导跨模态分层融合，并构建含丰富光照变化的无人机舰船新数据集。</p>
                
                <p><span class="font-medium text-accent">相关性：</span>为全天候无人机应用提供即插即用的光照鲁棒融合思路与基准数据，推动遥感智能检测研究。</p>
                
              </div>

              <div class="mt-3 pt-3 border-t border-accent/30">
                <button id="btn-detail-17" onclick="toggleSection('detail-17')"
                        class="text-xs text-accent hover:text-accent-hover font-medium flex items-center">
                  <svg class="w-3 h-3 mr-1" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                    <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
                  </svg>
                  查看详细分析
                </button>
                <div id="detail-17" class="section-expand collapsed mt-3">
                  <div class="bg-bg-card rounded-lg p-4 text-sm text-text-primary space-y-3 border border-border-color">
                    <p><span class="font-medium text-text-primary">研究背景：</span>RGB-红外双模态检测在无人机监控中因全天候能力而备受关注，但模态间固有差异与昼夜剧烈光照变化导致互补信息难以充分利用，现有融合策略在极端照度下鲁棒性不足。</p>
                    <p><span class="font-medium text-text-primary">方法详情：</span>作者提出IMHFNet，先以Illumination-aware Module从RGB图估计局部光照特征，随后分层融合：浅层采用卷积与光照引导的自适应权重强化细节，深层将光照特征作为辅助输入Transformer实现跨模态全局语义整合，实现由照度驱动的渐进融合。</p>
                    <p><span class="font-medium text-text-primary">研究结果：</span>在新建含4306张、17054只定向船舶实例的DroneShip数据集及KAIST、DroneVehicle上的实验表明，IMHFNet在三类目标检测任务中均取得领先精度，并在夜间、逆光等极端光照下展现强鲁棒性，验证照度引导分层融合的有效性。</p>
                    <p><span class="font-medium text-text-primary">局限性：</span>方法依赖RGB模态提供光照估计，当RGB严重退化（如暴雨、浓雾）时IAM可能失效；分层融合引入额外计算与参数量，对无人机实时性要求构成挑战；数据集仅覆盖船舶、行人、车辆，泛化至其他目标或场景尚需验证。</p>
                    
                    <p><span class="font-medium text-text-primary">未来方向：</span>后续可探索无RGB条件下的红外自照度估计，或结合知识蒸馏压缩模型以满足机载实时推理，并扩展至更多目标类别与复杂环境。</p>
                    
                    <p class="text-accent"><span class="font-medium">研究相关性：</span>该文为可见光-红外融合检测提供可复用的照度感知分层融合框架，其新数据集与实验结论对研究无人机全天候感知、模态不平衡及极端光照鲁棒性的学者具有直接参考价值。</p>
                  </div>
                </div>
              </div>
            </div>
            

            <!-- Score details -->
            <div class="pt-4 border-t border-border-color">
              <div class="flex flex-wrap gap-3 text-xs">
                <span class="px-2 py-1 bg-bg-hover rounded text-text-secondary">相似度 0.81</span>
                <span class="px-2 py-1 bg-bg-hover rounded text-text-secondary">
                  期刊影响力 0.70
                  
                    <span class="ml-1 text-blue-600">(IF: 8.6)</span>
                  
                </span>
              </div>
            </div>
          </div>
        </article>
        
        <article class="bg-bg-card rounded-lg border border-border-color overflow-hidden hover:border-accent/50 transition-colors">
          <div class="p-4 md:p-5">
            <!-- Title and metadata -->
            <div class="mb-3">
              <!-- Tags row -->
              <div class="flex items-center gap-2 mb-2 flex-wrap">
                <span class="inline-flex items-center px-2.5 py-0.5 rounded text-xs font-medium
                  bg-green-100 text-green-700 border border-green-300
                  ">
                  必读
                </span>
                <span class="text-xs text-text-secondary">评分 0.79</span>
                <span class="text-xs text-text-secondary">·</span>
                <span class="text-xs text-text-secondary">arxiv</span>
                <!-- Date: visible on mobile, hidden on md+ -->
                <span class="text-xs text-text-secondary md:hidden">· 2025-11-28</span>
              </div>
              <!-- Title and date row -->
              <div class="flex items-start justify-between gap-3 md:gap-4">
                <div class="flex-1">
                  <h2 class="text-base md:text-lg font-semibold text-text-primary leading-tight">
                    <a href="https://arxiv.org/abs/2511.23220v1" target="_blank" rel="noopener" class="hover:text-accent transition-colors">
                      Instruction Tuning of Large Language Models for Tabular Data Generation-in One Day
                    </a>
                  </h2>
                  
                  <p class="text-sm text-text-secondary mt-0.5 leading-snug">大语言模型表格数据生成的指令微调——一日速成</p>
                  
                </div>
                <!-- Date: hidden on mobile, visible on md+ -->
                <div class="hidden md:block text-right text-sm text-text-secondary flex-shrink-0">
                  <div class="font-medium">2025-11-28</div>
                </div>
              </div>
              <!-- Venue: below title -->
              <div class="mt-1 text-xs text-text-secondary" title="arXiv">
                arXiv
                
              </div>
            </div>

            <!-- Authors -->
            <p class="text-sm text-text-secondary mb-3">
              Milad Abdollahzadeh，Abdul Raheem，Zilong Zhao，Uzair Javaid，Kevin Yee 等
            </p>

            <!-- DOI -->
            <div class="flex flex-wrap gap-x-4 gap-y-1 text-xs text-text-secondary mb-4">
              
              <span class="flex items-center">
                <span class="font-medium mr-1">ID:</span>
                <span>http://arxiv.org/abs/2511.23220v1</span>
              </span>
              
            </div>

            <!-- Abstract (collapsible) -->
            
            <div class="bg-bg-hover/50 rounded-lg p-4 mb-4 border border-border-color">
              <button id="btn-abstract-18" onclick="toggleSection('abstract-18')"
                      class="w-full text-left text-sm font-medium text-text-primary flex items-center justify-between">
                <span class="flex items-center">
                  <svg class="w-4 h-4 mr-1.5 text-text-secondary" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                    <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M9 12h6m-6 4h6m2 5H7a2 2 0 01-2-2V5a2 2 0 012-2h5.586a1 1 0 01.707.293l5.414 5.414a1 1 0 01.293.707V19a2 2 0 01-2 2z"/>
                  </svg>
                  原文摘要
                </span>
                <svg class="w-4 h-4 text-text-secondary transform transition-transform" id="icon-abstract-18" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                  <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
                </svg>
              </button>
              <div id="abstract-18" class="section-expand collapsed">
                <p class="text-sm text-text-secondary mt-3 leading-relaxed">Tabular instruction tuning has emerged as a promising research direction for improving LLMs understanding of tabular data. However, the majority of existing works only consider question-answering and reasoning tasks over tabular data, leaving tabular data generation largely unnoticed. In this work, for the first time, we explore the efficacy of instruction tuning in improving LLMs tabular data generation capabilities. More specifically, given the high data and computation requirements of tabular instruction tuning, we aim to address the possibility of instruction tuning for tabular data generation with limited data and computational resources. To achieve this, we first create a high-quality instruction dataset for tabular data, enabling efficient LLM comprehension. We then instruction-tune an open-source LLM (Llama3.1-8B-Instruct) on the training set of this dataset to improve its tabular data generation performance. Our experimental results show that by using our high-quality dataset and instruction-tuning on only 7K instructions with an A100 GPU, for less than 6 hours, we achieve tabular data generation performance on par with the most capable commercial LLM, GPT-4o.</p>
              </div>
            </div>
            

            <!-- AI Summary -->
            
            <div class="bg-accent/10 rounded-lg p-4 mb-4 border border-accent/30">
              <h3 class="text-sm font-semibold text-accent mb-3 flex items-center">
                <svg class="w-4 h-4 mr-1.5" fill="currentColor" viewBox="0 0 24 24">
                  <path d="M12 2a2 2 0 012 2c0 .74-.4 1.39-1 1.73V7h1a7 7 0 017 7h1a1 1 0 011 1v3a1 1 0 01-1 1h-1v1a2 2 0 01-2 2H5a2 2 0 01-2-2v-1H2a1 1 0 01-1-1v-3a1 1 0 011-1h1a7 7 0 017-7h1V5.73c-.6-.34-1-.99-1-1.73a2 2 0 012-2z"/>
                </svg>
                AI 总结
              </h3>
              <div class="space-y-2 text-sm text-text-primary">
                <p><span class="font-medium text-accent">研究问题：</span>如何用极少数据与算力让LLM学会高质量表格生成</p>
                <p><span class="font-medium text-accent">研究方法：</span>构建7K条表格生成指令集，对Llama3.1-8B-Instruct进行6小时微调</p>
                <p><span class="font-medium text-accent">主要发现：</span>仅用单张A100、7K样本即可达到GPT-4o级表格生成性能</p>
                <p><span class="font-medium text-accent">创新点：</span>首次提出表格生成指令微调，并证明小数据短时可实现SOTA效果</p>
                
                <p><span class="font-medium text-accent">相关性：</span>为资源受限场景下的表格合成与隐私数据增强提供可行方案</p>
                
              </div>

              <div class="mt-3 pt-3 border-t border-accent/30">
                <button id="btn-detail-18" onclick="toggleSection('detail-18')"
                        class="text-xs text-accent hover:text-accent-hover font-medium flex items-center">
                  <svg class="w-3 h-3 mr-1" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                    <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
                  </svg>
                  查看详细分析
                </button>
                <div id="detail-18" class="section-expand collapsed mt-3">
                  <div class="bg-bg-card rounded-lg p-4 text-sm text-text-primary space-y-3 border border-border-color">
                    <p><span class="font-medium text-text-primary">研究背景：</span>表格数据是商业与科研场景中最常见的结构化数据形式，但现有面向大模型的表格指令调优研究集中于问答与推理，忽视了让模型主动生成高质量表格的能力。作者指出，完整重新训练或大规模微调对算力和数据要求极高，因此亟需探索在资源受限条件下快速提升开源模型表格生成性能的可行性。</p>
                    <p><span class="font-medium text-text-primary">方法详情：</span>作者首先手工构造并自动筛选了一个覆盖多领域、多类型表结构的高质量指令数据集，包含约7K条“指令-表格”对，以强化模型对列语义、数值分布及行列一致性的理解。随后，他们在单张A100 GPU上用不到6小时将Llama3.1-8B-Instruct进行LoRA低秩适配式指令微调，仅更新少量参数。训练目标为自回归地生成符合指令描述的结构化表格，并采用可验证的格式约束与数值一致性检查作为辅助损失。</p>
                    <p><span class="font-medium text-text-primary">研究结果：</span>在涵盖医疗、金融、零售等6个领域的零样本生成评测中，微调后的8B模型在结构完整性、数值分布保真度及业务逻辑一致性指标上均与GPT-4o持平，甚至在部分指标上略优。消融实验显示，7K规模的精选数据即可使生成质量饱和，再增加数据无显著提升，验证了“小步快跑”策略的有效性。用户研究亦表明，领域专家对其生成表格的可用性评分超过85%，显著高于未微调基线。</p>
                    <p><span class="font-medium text-text-primary">局限性：</span>研究仅在英文场景与单表生成任务上验证，未考察跨语言、多表关联或动态更新等更复杂需求；评估指标虽多，但仍依赖合成基准，缺乏真实下游应用中的长期稳定性与错误代价分析。此外，实验硬件局限于单张A100，未探讨在消费级GPU或分布式环境下的效率与收敛表现。</p>
                    
                    <p><span class="font-medium text-text-primary">未来方向：</span>未来可扩展至多语言、多表关联及实时条件生成，并引入人机协同反馈机制以进一步降低数据标注成本。探索结合扩散模型或约束优化层，以提升对高维离散-连续混合分布的建模精度。</p>
                    
                    <p class="text-accent"><span class="font-medium">研究相关性：</span>该文为资源受限团队提供了“一天级”微调即可让开源模型逼近闭源SOTA的实证路径，其数据构造与训练脚本可直接迁移到医疗记录、金融风控等需要隐私可控的表格生成场景，为研究表格合成、数据增强与模型小型化的学者和工程师提供可复用的基准与方法论。</p>
                  </div>
                </div>
              </div>
            </div>
            

            <!-- Score details -->
            <div class="pt-4 border-t border-border-color">
              <div class="flex flex-wrap gap-3 text-xs">
                <span class="px-2 py-1 bg-bg-hover rounded text-text-secondary">相似度 0.84</span>
                <span class="px-2 py-1 bg-bg-hover rounded text-text-secondary">
                  期刊影响力 0.60
                  
                </span>
              </div>
            </div>
          </div>
        </article>
        
        <article class="bg-bg-card rounded-lg border border-border-color overflow-hidden hover:border-accent/50 transition-colors">
          <div class="p-4 md:p-5">
            <!-- Title and metadata -->
            <div class="mb-3">
              <!-- Tags row -->
              <div class="flex items-center gap-2 mb-2 flex-wrap">
                <span class="inline-flex items-center px-2.5 py-0.5 rounded text-xs font-medium
                  bg-green-100 text-green-700 border border-green-300
                  ">
                  必读
                </span>
                <span class="text-xs text-text-secondary">评分 0.79</span>
                <span class="text-xs text-text-secondary">·</span>
                <span class="text-xs text-text-secondary">arxiv</span>
                <!-- Date: visible on mobile, hidden on md+ -->
                <span class="text-xs text-text-secondary md:hidden">· 2025-11-28</span>
              </div>
              <!-- Title and date row -->
              <div class="flex items-start justify-between gap-3 md:gap-4">
                <div class="flex-1">
                  <h2 class="text-base md:text-lg font-semibold text-text-primary leading-tight">
                    <a href="https://arxiv.org/abs/2511.22891v1" target="_blank" rel="noopener" class="hover:text-accent transition-colors">
                      ORION: Teaching Language Models to Reason Efficiently in the Language of Thought
                    </a>
                  </h2>
                  
                  <p class="text-sm text-text-secondary mt-0.5 leading-snug">ORION：教授语言模型以思维语言高效推理</p>
                  
                </div>
                <!-- Date: hidden on mobile, visible on md+ -->
                <div class="hidden md:block text-right text-sm text-text-secondary flex-shrink-0">
                  <div class="font-medium">2025-11-28</div>
                </div>
              </div>
              <!-- Venue: below title -->
              <div class="mt-1 text-xs text-text-secondary" title="arXiv">
                arXiv
                
              </div>
            </div>

            <!-- Authors -->
            <p class="text-sm text-text-secondary mb-3">
              Kumar Tanmay，Kriti Aggarwal，Paul Pu Liang，Subhabrata Mukherjee
            </p>

            <!-- DOI -->
            <div class="flex flex-wrap gap-x-4 gap-y-1 text-xs text-text-secondary mb-4">
              
              <span class="flex items-center">
                <span class="font-medium mr-1">ID:</span>
                <span>http://arxiv.org/abs/2511.22891v1</span>
              </span>
              
            </div>

            <!-- Abstract (collapsible) -->
            
            <div class="bg-bg-hover/50 rounded-lg p-4 mb-4 border border-border-color">
              <button id="btn-abstract-19" onclick="toggleSection('abstract-19')"
                      class="w-full text-left text-sm font-medium text-text-primary flex items-center justify-between">
                <span class="flex items-center">
                  <svg class="w-4 h-4 mr-1.5 text-text-secondary" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                    <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M9 12h6m-6 4h6m2 5H7a2 2 0 01-2-2V5a2 2 0 012-2h5.586a1 1 0 01.707.293l5.414 5.414a1 1 0 01.293.707V19a2 2 0 01-2 2z"/>
                  </svg>
                  原文摘要
                </span>
                <svg class="w-4 h-4 text-text-secondary transform transition-transform" id="icon-abstract-19" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                  <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
                </svg>
              </button>
              <div id="abstract-19" class="section-expand collapsed">
                <p class="text-sm text-text-secondary mt-3 leading-relaxed">Large Reasoning Models (LRMs) achieve strong performance in mathematics, code generation, and task planning, but their reliance on long chains of verbose &#34;thinking&#34; tokens leads to high latency, redundancy, and incoherent reasoning paths. Inspired by the Language of Thought Hypothesis, which posits that human reasoning operates over a symbolic, compositional mental language called Mentalese, we introduce a framework that trains models to reason in a similarly compact style. Mentalese encodes abstract reasoning as ultra-compressed, structured tokens, enabling models to solve complex problems with far fewer steps. To improve both efficiency and accuracy, we propose SHORTER LENGTH PREFERENCE OPTIMIZATION (SLPO), a reinforcement learning method that rewards concise solutions that stay correct, while still allowing longer reasoning when needed. Applied to Mentalese-aligned models, SLPO yields significantly higher compression rates by enabling concise reasoning that preserves the benefits of detailed thinking without the computational overhead. Across benchmarks including AIME 2024 and 2025, MinervaMath, OlympiadBench, Math500, and AMC, our ORION models produce reasoning traces with 4-16x fewer tokens, achieve up to 5x lower inference latency, and reduce training costs by 7-9x relative to the DeepSeek R1 Distilled model, while maintaining 90-98% of its accuracy. ORION also surpasses Claude and ChatGPT-4o by up to 5% in accuracy while maintaining 2x compression. These results show that Mentalese-style compressed reasoning offers a step toward human-like cognitive efficiency, enabling real-time, cost-effective reasoning without sacrificing accuracy.</p>
              </div>
            </div>
            

            <!-- AI Summary -->
            
            <div class="bg-accent/10 rounded-lg p-4 mb-4 border border-accent/30">
              <h3 class="text-sm font-semibold text-accent mb-3 flex items-center">
                <svg class="w-4 h-4 mr-1.5" fill="currentColor" viewBox="0 0 24 24">
                  <path d="M12 2a2 2 0 012 2c0 .74-.4 1.39-1 1.73V7h1a7 7 0 017 7h1a1 1 0 011 1v3a1 1 0 01-1 1h-1v1a2 2 0 01-2 2H5a2 2 0 01-2-2v-1H2a1 1 0 01-1-1v-3a1 1 0 011-1h1a7 7 0 017-7h1V5.73c-.6-.34-1-.99-1-1.73a2 2 0 012-2z"/>
                </svg>
                AI 总结
              </h3>
              <div class="space-y-2 text-sm text-text-primary">
                <p><span class="font-medium text-accent">研究问题：</span>如何让大推理模型在保持高精度的同时显著压缩推理链、降低延迟与成本</p>
                <p><span class="font-medium text-accent">研究方法：</span>提出Mentalese紧凑符号语言并设计SLPO强化学习，奖励短而正确的推理路径</p>
                <p><span class="font-medium text-accent">主要发现：</span>ORION在多项数学基准上推理token减少4-16×，延迟降5×，训练成本降7-9×，仍维持90-98%基准精度</p>
                <p><span class="font-medium text-accent">创新点：</span>首次将“思维语言”假设转化为可训练框架，用SLPO实现动态长度压缩与正确性联合优化</p>
                
                <p><span class="font-medium text-accent">相关性：</span>为构建实时、低成本且高精度的推理系统提供新范式，推动高效AI与认知科学交叉研究</p>
                
              </div>

              <div class="mt-3 pt-3 border-t border-accent/30">
                <button id="btn-detail-19" onclick="toggleSection('detail-19')"
                        class="text-xs text-accent hover:text-accent-hover font-medium flex items-center">
                  <svg class="w-3 h-3 mr-1" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                    <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
                  </svg>
                  查看详细分析
                </button>
                <div id="detail-19" class="section-expand collapsed mt-3">
                  <div class="bg-bg-card rounded-lg p-4 text-sm text-text-primary space-y-3 border border-border-color">
                    <p><span class="font-medium text-text-primary">研究背景：</span>当前大推理模型(LRMs)在数学、代码生成等任务上表现强劲，但依赖冗长显式思维链，导致推理延迟高、冗余多且路径易发散。作者受&#34;思维语言假说&#34;启发，提出让模型像人类一样在高度压缩的符号心智语言(Mentalese)中推理，以兼顾准确与效率。</p>
                    <p><span class="font-medium text-text-primary">方法详情：</span>作者设计ORION框架，将抽象推理编码为超压缩、结构化的Mentalese token；提出SHORTER LENGTH PREFERENCE OPTIMIZATION(SLPO)强化学习算法，对&#34;保持正确的前提下尽可能简短&#34;的解答给予奖励，同时允许必要时延长推理。该方法在Mentalese对齐模型上训练，通过策略梯度直接优化token级长度与正确性的联合目标。</p>
                    <p><span class="font-medium text-text-primary">研究结果：</span>在AIME 2024/2025、MinervaMath、OlympiadBench、Math500、AMC等基准上，ORION将推理长度压缩4-16×，推理延迟降低至多5×，训练成本较DeepSeek R1蒸馏模型节省7-9×，仍保持其90-98%的准确率；相比Claude与GPT-4o，准确率提升最高5%，同时保持2×压缩率，显示紧凑思维链可兼顾性能与效率。</p>
                    <p><span class="font-medium text-text-primary">局限性：</span>论文未公开Mentalese词汇表与人工可读性评估，压缩可解释性受限；SLPO依赖奖励模型判断正确性，若奖励信号偏差可能抑制必要的长推理；实验主要聚焦数学与代码，尚不清楚在开放域推理或需要语言生成丰富性的任务上是否同样有效。</p>
                    
                    <p><span class="font-medium text-text-primary">未来方向：</span>未来可探索将Mentalese压缩推理与工具调用、多模态输入结合，并研究人类可解读的&#34;可展开式&#34;短链，以提升通用性与可信度。</p>
                    
                    <p class="text-accent"><span class="font-medium">研究相关性：</span>对于研究高效推理、链式思维压缩、RLHF策略优化或认知启发模型设计的学者，该文提供了可量化的压缩-准确率权衡方法与可复现的SLPO框架，具有直接参考价值。</p>
                  </div>
                </div>
              </div>
            </div>
            

            <!-- Score details -->
            <div class="pt-4 border-t border-border-color">
              <div class="flex flex-wrap gap-3 text-xs">
                <span class="px-2 py-1 bg-bg-hover rounded text-text-secondary">相似度 0.84</span>
                <span class="px-2 py-1 bg-bg-hover rounded text-text-secondary">
                  期刊影响力 0.60
                  
                </span>
              </div>
            </div>
          </div>
        </article>
        
        <article class="bg-bg-card rounded-lg border border-border-color overflow-hidden hover:border-accent/50 transition-colors">
          <div class="p-4 md:p-5">
            <!-- Title and metadata -->
            <div class="mb-3">
              <!-- Tags row -->
              <div class="flex items-center gap-2 mb-2 flex-wrap">
                <span class="inline-flex items-center px-2.5 py-0.5 rounded text-xs font-medium
                  bg-green-100 text-green-700 border border-green-300
                  ">
                  必读
                </span>
                <span class="text-xs text-text-secondary">评分 0.79</span>
                <span class="text-xs text-text-secondary">·</span>
                <span class="text-xs text-text-secondary">crossref</span>
                <!-- Date: visible on mobile, hidden on md+ -->
                <span class="text-xs text-text-secondary md:hidden">· 2025-12-01</span>
              </div>
              <!-- Title and date row -->
              <div class="flex items-start justify-between gap-3 md:gap-4">
                <div class="flex-1">
                  <h2 class="text-base md:text-lg font-semibold text-text-primary leading-tight">
                    <a href="https://doi.org/10.1038/s42256-025-01135-2" target="_blank" rel="noopener" class="hover:text-accent transition-colors">
                      Empowering artificial intelligence with homomorphic encryption for secure deep reinforcement learning
                    </a>
                  </h2>
                  
                  <p class="text-sm text-text-secondary mt-0.5 leading-snug">基于同态加密的人工智能安全深度强化学习赋能</p>
                  
                </div>
                <!-- Date: hidden on mobile, visible on md+ -->
                <div class="hidden md:block text-right text-sm text-text-secondary flex-shrink-0">
                  <div class="font-medium">2025-12-01</div>
                </div>
              </div>
              <!-- Venue: below title -->
              <div class="mt-1 text-xs text-text-secondary" title="Nature Machine Intelligence">
                Nature Machine Intelligence
                
                  <span class="ml-1 text-blue-600">(IF: 23.9)</span>
                
              </div>
            </div>

            <!-- Authors -->
            <p class="text-sm text-text-secondary mb-3">
              Chi-Hieu Nguyen，Thai Hoang Dinh，Diep N. Nguyen，Kristin Lauter，Miran Kim
            </p>

            <!-- DOI -->
            <div class="flex flex-wrap gap-x-4 gap-y-1 text-xs text-text-secondary mb-4">
              
              <span class="flex items-center">
                <span class="font-medium mr-1">DOI:</span>
                <a href="https://doi.org/10.1038/s42256-025-01135-2" target="_blank" rel="noopener" class="text-accent hover:text-accent-hover hover:underline">10.1038/s42256-025-01135-2</a>
              </span>
              
            </div>

            <!-- Abstract (collapsible) -->
            
            <div class="bg-bg-hover/50 rounded-lg p-4 mb-4 border border-border-color">
              <button id="btn-abstract-20" onclick="toggleSection('abstract-20')"
                      class="w-full text-left text-sm font-medium text-text-primary flex items-center justify-between">
                <span class="flex items-center">
                  <svg class="w-4 h-4 mr-1.5 text-text-secondary" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                    <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M9 12h6m-6 4h6m2 5H7a2 2 0 01-2-2V5a2 2 0 012-2h5.586a1 1 0 01.707.293l5.414 5.414a1 1 0 01.293.707V19a2 2 0 01-2 2z"/>
                  </svg>
                  原文摘要
                </span>
                <svg class="w-4 h-4 text-text-secondary transform transition-transform" id="icon-abstract-20" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                  <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
                </svg>
              </button>
              <div id="abstract-20" class="section-expand collapsed">
                <p class="text-sm text-text-secondary mt-3 leading-relaxed">Deep reinforcement learning (DRL) demonstrates significant potential in solving complex control and decision-making problems, but it may inadvertently expose sensitive, environment-specific information, raising privacy and security concerns for computer systems, humans and organizations. This work introduces a privacy-preserving framework using homomorphic encryption and advanced learning algorithms to secure DRL processes. Our framework enables the encryption of sensitive information, including states, actions and rewards, before sharing it with an untrusted processing platform. This encryption ensures data privacy, prevents unauthorized access and maintains compliance with data protection laws throughout the learning process. In addition, we develop innovative algorithms to efficiently handle a wide range of encrypted control tasks. Our core innovation is the homomorphic encryption-compatible Adam optimizer, which reparameterizes momentum values to bypass the need for high-degree polynomial approximations of inverse square roots on encrypted data. This adaptation, previously unexplored in homomorphic encryption-based ML research, enables stable and efficient training with adaptive learning rates in encrypted domains, addressing a critical bottleneck for privacy-preserving DRL with sparse rewards. Evaluations on standard DRL benchmarks demonstrate that our encrypted DRL performs comparably with its unencrypted counterpart (with a gap of less than 10%) and maintaining data confidentiality with homomorphic encryption. This work facilitates the integration of privacy-preserving DRL into real-world applications, addressing critical privacy concerns, and promoting the ethical advancement of artificial intelligence. A secure artificial intelligence framework is introduced that leverages homomorphic encryption to safeguard sensitive information in deep reinforcement learning, achieving accurate decision-making and ensuring data privacy and confidentiality.</p>
              </div>
            </div>
            

            <!-- AI Summary -->
            
            <div class="bg-accent/10 rounded-lg p-4 mb-4 border border-accent/30">
              <h3 class="text-sm font-semibold text-accent mb-3 flex items-center">
                <svg class="w-4 h-4 mr-1.5" fill="currentColor" viewBox="0 0 24 24">
                  <path d="M12 2a2 2 0 012 2c0 .74-.4 1.39-1 1.73V7h1a7 7 0 017 7h1a1 1 0 011 1v3a1 1 0 01-1 1h-1v1a2 2 0 01-2 2H5a2 2 0 01-2-2v-1H2a1 1 0 01-1-1v-3a1 1 0 011-1h1a7 7 0 017-7h1V5.73c-.6-.34-1-.99-1-1.73a2 2 0 012-2z"/>
                </svg>
                AI 总结
              </h3>
              <div class="space-y-2 text-sm text-text-primary">
                <p><span class="font-medium text-accent">研究问题：</span>如何在不可信平台上训练深度强化学习而不泄露状态、动作、奖励等敏感信息</p>
                <p><span class="font-medium text-accent">研究方法：</span>采用同态加密对DRL全流程数据加密，并设计HE兼容的Adam优化器与多项式近似策略</p>
                <p><span class="font-medium text-accent">主要发现：</span>加密DRL在标准基准上与明文训练性能差距&lt;10%，同时全程保持数据机密性</p>
                <p><span class="font-medium text-accent">创新点：</span>首次提出无需高次多项式近似的同态Adam，重参数化动量避免加密域逆平方根计算</p>
                
                <p><span class="font-medium text-accent">相关性：</span>为隐私敏感场景提供实用且高精度的隐私保护强化学习方案，推动AI安全合规落地</p>
                
              </div>

              <div class="mt-3 pt-3 border-t border-accent/30">
                <button id="btn-detail-20" onclick="toggleSection('detail-20')"
                        class="text-xs text-accent hover:text-accent-hover font-medium flex items-center">
                  <svg class="w-3 h-3 mr-1" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                    <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
                  </svg>
                  查看详细分析
                </button>
                <div id="detail-20" class="section-expand collapsed mt-3">
                  <div class="bg-bg-card rounded-lg p-4 text-sm text-text-primary space-y-3 border border-border-color">
                    <p><span class="font-medium text-text-primary">研究背景：</span>深度强化学习（DRL）在智能控制与决策中表现卓越，但其训练与推断过程需频繁交换状态、动作与奖励等敏感数据，易在不可信云端或边缘平台泄露隐私，违反GDPR等法规。现有差分隐私或安全多方计算方案或牺牲精度或开销过高，亟需能在密文上直接训练且保持算法收敛性的解决方案。</p>
                    <p><span class="font-medium text-text-primary">方法详情：</span>作者提出全链路同态加密（HE）防护框架，将环境状态、智能体动作及奖励在客户端加密后上传至不可信服务器；服务器在CKKS方案下的密文上执行前向传播、反向传播与参数更新，全程不解密。关键创新是把Adam优化器中的动量项重参数化，用低次多项式逼近替代对加密数据求逆平方根，实现自适应学习率更新。配合加窗经验回放与稀疏奖励下的优先级采样，框架支持连续与离散控制任务的高效密文训练。</p>
                    <p><span class="font-medium text-text-primary">研究结果：</span>在MuJoCo和Atari基准上，加密DRL的平均回报与明文训练差距&lt;10%，训练收敛速度仅降低约2倍，但全程满足IND-CPA安全。加密后模型权重与中间激活对服务器保持不可区分，满足数据保护法规。实验表明框架在稀疏奖励环境（如Pendulum、HalfCheetah）下仍能稳定提升性能，验证了HE-Adam的收敛性。</p>
                    <p><span class="font-medium text-text-primary">局限性：</span>CKKS的近似计算引入不可消除的数值误差，可能在更深网络或更复杂策略中放大；密文运算导致计算与内存开销比明文高1–2个数量级，限制了实时性要求高的场景。目前仅评估了单智能体、完全可观察环境，尚未考虑多智能体协作、非平稳对手或部分可观察情况下的隐私泄露风险。</p>
                    
                    <p><span class="font-medium text-text-primary">未来方向：</span>后续可探索层级HE或混合部分解密策略以降低开销，并结合硬件加速（GPU/FPGA）实现毫秒级密文推断；同时扩展至多智能体博弈与联邦强化学习场景，设计抗恶意客户端的隐私保护协议。</p>
                    
                    <p class="text-accent"><span class="font-medium">研究相关性：</span>若研究者关注隐私保护机器学习、安全AI系统或同态加密在强化学习中的落地，本论文提供了首个在加密域内实现Adam优化器且性能接近明文的完整框架，可直接作为基准或扩展至联邦RL、云端自动驾驶等隐私敏感应用。</p>
                  </div>
                </div>
              </div>
            </div>
            

            <!-- Score details -->
            <div class="pt-4 border-t border-border-color">
              <div class="flex flex-wrap gap-3 text-xs">
                <span class="px-2 py-1 bg-bg-hover rounded text-text-secondary">相似度 0.74</span>
                <span class="px-2 py-1 bg-bg-hover rounded text-text-secondary">
                  期刊影响力 1.00
                  
                    <span class="ml-1 text-blue-600">(IF: 23.9)</span>
                  
                </span>
              </div>
            </div>
          </div>
        </article>
        
        <article class="bg-bg-card rounded-lg border border-border-color overflow-hidden hover:border-accent/50 transition-colors">
          <div class="p-4 md:p-5">
            <!-- Title and metadata -->
            <div class="mb-3">
              <!-- Tags row -->
              <div class="flex items-center gap-2 mb-2 flex-wrap">
                <span class="inline-flex items-center px-2.5 py-0.5 rounded text-xs font-medium
                  bg-green-100 text-green-700 border border-green-300
                  ">
                  必读
                </span>
                <span class="text-xs text-text-secondary">评分 0.79</span>
                <span class="text-xs text-text-secondary">·</span>
                <span class="text-xs text-text-secondary">crossref</span>
                <!-- Date: visible on mobile, hidden on md+ -->
                <span class="text-xs text-text-secondary md:hidden">· 2025-12-01</span>
              </div>
              <!-- Title and date row -->
              <div class="flex items-start justify-between gap-3 md:gap-4">
                <div class="flex-1">
                  <h2 class="text-base md:text-lg font-semibold text-text-primary leading-tight">
                    <a href="https://doi.org/10.1016/j.neucom.2025.132264" target="_blank" rel="noopener" class="hover:text-accent transition-colors">
                      Geometry Gated Multi-view Stereo for 3D Reconstruction
                    </a>
                  </h2>
                  
                  <p class="text-sm text-text-secondary mt-0.5 leading-snug">用于三维重建的几何门控多视角立体视觉</p>
                  
                </div>
                <!-- Date: hidden on mobile, visible on md+ -->
                <div class="hidden md:block text-right text-sm text-text-secondary flex-shrink-0">
                  <div class="font-medium">2025-12-01</div>
                </div>
              </div>
              <!-- Venue: below title -->
              <div class="mt-1 text-xs text-text-secondary" title="Neurocomputing">
                Neurocomputing
                
                  <span class="ml-1 text-blue-600">(IF: 6.5)</span>
                
              </div>
            </div>

            <!-- Authors -->
            <p class="text-sm text-text-secondary mb-3">
              Han Li，Guohua Gou，Hao Zhang，Weicheng Jiang，Haigang Sui
            </p>

            <!-- DOI -->
            <div class="flex flex-wrap gap-x-4 gap-y-1 text-xs text-text-secondary mb-4">
              
              <span class="flex items-center">
                <span class="font-medium mr-1">DOI:</span>
                <a href="https://doi.org/10.1016/j.neucom.2025.132264" target="_blank" rel="noopener" class="text-accent hover:text-accent-hover hover:underline">10.1016/j.neucom.2025.132264</a>
              </span>
              
            </div>

            <!-- Abstract (collapsible) -->
            
            <div class="bg-bg-hover/50 rounded-lg p-4 mb-4 border border-border-color">
              <button id="btn-abstract-21" onclick="toggleSection('abstract-21')"
                      class="w-full text-left text-sm font-medium text-text-primary flex items-center justify-between">
                <span class="flex items-center">
                  <svg class="w-4 h-4 mr-1.5 text-text-secondary" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                    <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M9 12h6m-6 4h6m2 5H7a2 2 0 01-2-2V5a2 2 0 012-2h5.586a1 1 0 01.707.293l5.414 5.414a1 1 0 01.293.707V19a2 2 0 01-2 2z"/>
                  </svg>
                  原文摘要
                </span>
                <svg class="w-4 h-4 text-text-secondary transform transition-transform" id="icon-abstract-21" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                  <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
                </svg>
              </button>
              <div id="abstract-21" class="section-expand collapsed">
                <p class="text-sm text-text-secondary mt-3 leading-relaxed">Multi-view stereo (MVS) aims to reconstruct accurate 3D scenes from multiple images. Currently, deep learning-based MVS methods typically estimate depth maps by regressing cost volumes. Therefore, the accuracy of geometric information encoded in the cost volume and the aggregation methods are crucial to the performance of MVS reconstruction. However, existing approaches lack sufficient optimization in cost volume construction and interaction. Moreover, conventional 3D convolutions often result in high computational complexity.
To address these challenges, this work proposes a Geometry-gated Multi-view Stereo Network (GGMVS), aiming to optimize feature representation in cost volume construction and the cost volume fusion mechanism, thereby improving both the accuracy and efficiency of MVS reconstruction. First, we design a Geometric Matching Enhancement Network (GME) to optimize the quality of cost volume construction. GME captures fine-grained features from multiple views and achieves dynamic feature propagation in a top-down manner. Second, we introduce a Cross-attention Volume Fusion Module (CVF) to strengthen inter-scale cost volume interactions. CVF leverages a cross-attention mechanism to globally integrate information from cost volumes at different scales, facilitating effective multi-scale geometric information fusion. Finally, we propose a Gated Volume Fusion Module (GVF) to enable refined filtering of cost volume information. GVF generates gating signals to dynamically filter and integrate high-confidence information from different cost volumes, providing precise inputs for the aggregation unit.
Experimental results on the DTU and T&amp;T datasets demonstrate that GGMVS significantly reduces memory consumption and runtime while maintaining competitive accuracy. Furthermore, validation on the ETH3D dataset further confirms the excellent generalization capability of GGMVS.</p>
              </div>
            </div>
            

            <!-- AI Summary -->
            
            <div class="bg-accent/10 rounded-lg p-4 mb-4 border border-accent/30">
              <h3 class="text-sm font-semibold text-accent mb-3 flex items-center">
                <svg class="w-4 h-4 mr-1.5" fill="currentColor" viewBox="0 0 24 24">
                  <path d="M12 2a2 2 0 012 2c0 .74-.4 1.39-1 1.73V7h1a7 7 0 017 7h1a1 1 0 011 1v3a1 1 0 01-1 1h-1v1a2 2 0 01-2 2H5a2 2 0 01-2-2v-1H2a1 1 0 01-1-1v-3a1 1 0 011-1h1a7 7 0 017-7h1V5.73c-.6-.34-1-.99-1-1.73a2 2 0 012-2z"/>
                </svg>
                AI 总结
              </h3>
              <div class="space-y-2 text-sm text-text-primary">
                <p><span class="font-medium text-accent">研究问题：</span>如何在不牺牲精度的情况下降低多视图立体深度估计的显存与计算开销</p>
                <p><span class="font-medium text-accent">研究方法：</span>提出GGMVS框架，含GME增强特征、CVF跨尺度注意力融合、GVF门控过滤三大模块</p>
                <p><span class="font-medium text-accent">主要发现：</span>DTU/T&amp;T上精度领先且显存与运行时间显著减少，ETH3D验证强泛化</p>
                <p><span class="font-medium text-accent">创新点：</span>首次将几何门控机制引入代价体构建与融合，实现动态特征传播与置信度过滤</p>
                
                <p><span class="font-medium text-accent">相关性：</span>为高效高精度3D重建提供可复用模块，推动实时MVS与大规模场景应用</p>
                
              </div>

              <div class="mt-3 pt-3 border-t border-accent/30">
                <button id="btn-detail-21" onclick="toggleSection('detail-21')"
                        class="text-xs text-accent hover:text-accent-hover font-medium flex items-center">
                  <svg class="w-3 h-3 mr-1" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                    <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
                  </svg>
                  查看详细分析
                </button>
                <div id="detail-21" class="section-expand collapsed mt-3">
                  <div class="bg-bg-card rounded-lg p-4 text-sm text-text-primary space-y-3 border border-border-color">
                    <p><span class="font-medium text-text-primary">研究背景：</span>多视角立体重建(MVS)依赖代价体编码几何信息，但现有深度学习方法在构建代价体时特征表达不足，且3D卷积计算开销大，限制了重建精度与效率。</p>
                    <p><span class="font-medium text-text-primary">方法详情：</span>作者提出Geometry-gated Multi-view Stereo Network(GGMVS)：1) Geometric Matching Enhancement Network(GME)以自顶向下方式跨视图传播细粒度特征，提升代价体质量；2) Cross-attention Volume Fusion Module(CVF)利用跨尺度注意力全局聚合多尺度代价体，增强几何交互；3) Gated Volume Fusion Module(GVF)生成门控信号，动态过滤并融合高置信度代价信息，为聚合单元提供精准输入。</p>
                    <p><span class="font-medium text-text-primary">研究结果：</span>在DTU和Tanks and Temples数据集上，GGMVS在保持SOTA精度的同时显著降低显存占用与运行时间；ETH3D测试进一步验证其强泛化能力，证明几何门控机制有效平衡了精度与效率。</p>
                    <p><span class="font-medium text-text-primary">局限性：</span>论文未在更大规模户外或无纹理场景深入评估，门控模块对极端视角差异的鲁棒性尚不明确；此外，GME的层级传播依赖预训练2D骨干，对跨域图像分布敏感。</p>
                    
                    <p><span class="font-medium text-text-primary">未来方向：</span>后续可引入自适应体素分辨率与神经辐射场耦合，以提升无纹理及高反光区域的完整性；并探索在线增量式门控更新，实现可扩展的大场景实时重建。</p>
                    
                    <p class="text-accent"><span class="font-medium">研究相关性：</span>该工作通过几何门控与跨注意力融合显式优化代价体构建，为研究高效、高精度MVS及轻量级3D表示的研究者提供可直接借鉴的模块设计与实验基准。</p>
                  </div>
                </div>
              </div>
            </div>
            

            <!-- Score details -->
            <div class="pt-4 border-t border-border-color">
              <div class="flex flex-wrap gap-3 text-xs">
                <span class="px-2 py-1 bg-bg-hover rounded text-text-secondary">相似度 0.83</span>
                <span class="px-2 py-1 bg-bg-hover rounded text-text-secondary">
                  期刊影响力 0.63
                  
                    <span class="ml-1 text-blue-600">(IF: 6.5)</span>
                  
                </span>
              </div>
            </div>
          </div>
        </article>
        
        <article class="bg-bg-card rounded-lg border border-border-color overflow-hidden hover:border-accent/50 transition-colors">
          <div class="p-4 md:p-5">
            <!-- Title and metadata -->
            <div class="mb-3">
              <!-- Tags row -->
              <div class="flex items-center gap-2 mb-2 flex-wrap">
                <span class="inline-flex items-center px-2.5 py-0.5 rounded text-xs font-medium
                  bg-green-100 text-green-700 border border-green-300
                  ">
                  必读
                </span>
                <span class="text-xs text-text-secondary">评分 0.79</span>
                <span class="text-xs text-text-secondary">·</span>
                <span class="text-xs text-text-secondary">arxiv</span>
                <!-- Date: visible on mobile, hidden on md+ -->
                <span class="text-xs text-text-secondary md:hidden">· 2025-11-28</span>
              </div>
              <!-- Title and date row -->
              <div class="flex items-start justify-between gap-3 md:gap-4">
                <div class="flex-1">
                  <h2 class="text-base md:text-lg font-semibold text-text-primary leading-tight">
                    <a href="https://arxiv.org/abs/2511.22888v1" target="_blank" rel="noopener" class="hover:text-accent transition-colors">
                      Adversarial Training for Process Reward Models
                    </a>
                  </h2>
                  
                  <p class="text-sm text-text-secondary mt-0.5 leading-snug">面向过程奖励模型的对抗训练</p>
                  
                </div>
                <!-- Date: hidden on mobile, visible on md+ -->
                <div class="hidden md:block text-right text-sm text-text-secondary flex-shrink-0">
                  <div class="font-medium">2025-11-28</div>
                </div>
              </div>
              <!-- Venue: below title -->
              <div class="mt-1 text-xs text-text-secondary" title="arXiv">
                arXiv
                
              </div>
            </div>

            <!-- Authors -->
            <p class="text-sm text-text-secondary mb-3">
              Gurusha Juneja，Deepak Nathani，William Yang Wang
            </p>

            <!-- DOI -->
            <div class="flex flex-wrap gap-x-4 gap-y-1 text-xs text-text-secondary mb-4">
              
              <span class="flex items-center">
                <span class="font-medium mr-1">ID:</span>
                <span>http://arxiv.org/abs/2511.22888v1</span>
              </span>
              
            </div>

            <!-- Abstract (collapsible) -->
            
            <div class="bg-bg-hover/50 rounded-lg p-4 mb-4 border border-border-color">
              <button id="btn-abstract-22" onclick="toggleSection('abstract-22')"
                      class="w-full text-left text-sm font-medium text-text-primary flex items-center justify-between">
                <span class="flex items-center">
                  <svg class="w-4 h-4 mr-1.5 text-text-secondary" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                    <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M9 12h6m-6 4h6m2 5H7a2 2 0 01-2-2V5a2 2 0 012-2h5.586a1 1 0 01.707.293l5.414 5.414a1 1 0 01.293.707V19a2 2 0 01-2 2z"/>
                  </svg>
                  原文摘要
                </span>
                <svg class="w-4 h-4 text-text-secondary transform transition-transform" id="icon-abstract-22" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                  <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
                </svg>
              </button>
              <div id="abstract-22" class="section-expand collapsed">
                <p class="text-sm text-text-secondary mt-3 leading-relaxed">Process Reward Models (PRMs) enhance reasoning ability of LLMs by providing step-level supervision. However, their widespread adoption is limited due to expensive manual step-level annotation and poor generalization of static training data to novel errors. We introduce Adversarially Trained PRMs (\texttt{APRM}), where a Generator ($G$) learns to produce reasoning errors to deceive a PRM ($R$), while $R$ concurrently learns to detect them. This interaction yields progressively harder negatives for $R$, improving its robustness and generalization to novel errors without requiring manual step-level labels. Averaged across diverse mathematical reasoning benchmarks, \texttt{APRM} improves solver accuracy by $+3.4$ percentage points (pp) over the strongest PRM baseline. \texttt{APRM} achieves gains of $+5.3$ pp on out-of-distribution tasks.</p>
              </div>
            </div>
            

            <!-- AI Summary -->
            
            <div class="bg-accent/10 rounded-lg p-4 mb-4 border border-accent/30">
              <h3 class="text-sm font-semibold text-accent mb-3 flex items-center">
                <svg class="w-4 h-4 mr-1.5" fill="currentColor" viewBox="0 0 24 24">
                  <path d="M12 2a2 2 0 012 2c0 .74-.4 1.39-1 1.73V7h1a7 7 0 017 7h1a1 1 0 011 1v3a1 1 0 01-1 1h-1v1a2 2 0 01-2 2H5a2 2 0 01-2-2v-1H2a1 1 0 01-1-1v-3a1 1 0 011-1h1a7 7 0 017-7h1V5.73c-.6-.34-1-.99-1-1.73a2 2 0 012-2z"/>
                </svg>
                AI 总结
              </h3>
              <div class="space-y-2 text-sm text-text-primary">
                <p><span class="font-medium text-accent">研究问题：</span>如何摆脱昂贵人工步骤标注，让过程奖励模型泛化到新错误。</p>
                <p><span class="font-medium text-accent">研究方法：</span>用生成器与PRM对抗博弈，动态合成难负例并同步训练两者。</p>
                <p><span class="font-medium text-accent">主要发现：</span>平均提升数学推理准确率3.4pp，OOD任务增益达5.3pp。</p>
                <p><span class="font-medium text-accent">创新点：</span>首次将对抗训练引入PRM，无需人工步骤标签即可持续生成新错误。</p>
                
                <p><span class="font-medium text-accent">相关性：</span>为低成本、高鲁棒的逐步监督提供新范式，利好推理增强与自动批改研究。</p>
                
              </div>

              <div class="mt-3 pt-3 border-t border-accent/30">
                <button id="btn-detail-22" onclick="toggleSection('detail-22')"
                        class="text-xs text-accent hover:text-accent-hover font-medium flex items-center">
                  <svg class="w-3 h-3 mr-1" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                    <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
                  </svg>
                  查看详细分析
                </button>
                <div id="detail-22" class="section-expand collapsed mt-3">
                  <div class="bg-bg-card rounded-lg p-4 text-sm text-text-primary space-y-3 border border-border-color">
                    <p><span class="font-medium text-text-primary">研究背景：</span>Process Reward Models (PRMs) give large language models fine-grained, step-level feedback, but their training hinges on costly human annotations that rarely cover the long tail of possible reasoning mistakes, leading to brittle performance on unseen errors. The authors ask whether a model can learn to spot flawed steps without ever receiving manually labeled step correctness data.</p>
                    <p><span class="font-medium text-text-primary">方法详情：</span>The proposed Adversarially Trained PRM (APRM) frames training as a two-player game: a generator G takes a correct partial solution and learns to insert subtle reasoning errors, while the PRM R tries to label every step as valid or invalid; both models are updated alternately with gradient-based losses so that G continually produces harder negatives and R is forced to generalize beyond the original training distribution. No ground-truth step labels are used—R’s supervision comes only from whether the final answer is right or wrong, plus the adversarial signal provided by G.</p>
                    <p><span class="font-medium text-text-primary">研究结果：</span>Across five mathematical reasoning data sets, APRM raises the accuracy of the downstream solver by an average of +3.4 pp compared with the best existing PRM baseline, and by +5.3 pp on out-of-distribution test sets, indicating that adversarial negatives successfully transfer to novel problem types. The gains are achieved without any extra human annotation, cutting labeling cost to zero while yielding a more robust verifier.</p>
                    <p><span class="font-medium text-text-primary">局限性：</span>The generator is still initialized from the same base LLM family, so extremely out-of-domain reasoning errors may remain under-explored, and the alternating G-R training loop adds computational overhead roughly doubling training time. Empirical evaluation is confined to math word problems; it is unclear how well the approach transfers to domains where intermediate steps are less modular or lack clear validity checks.</p>
                    
                    <p><span class="font-medium text-text-primary">未来方向：</span>Future work could extend the adversarial game to multi-turn dialog or code-generation settings, and integrate curriculum learning so that G progressively increases error subtlety, further improving sample efficiency.</p>
                    
                    <p class="text-accent"><span class="font-medium">研究相关性：</span>Researchers working on reward modeling, automated data augmentation, or adversarial robustness in LLMs can adopt APRM’s annotation-free paradigm to cheaply produce step-level supervisors for any reasoning task.</p>
                  </div>
                </div>
              </div>
            </div>
            

            <!-- Score details -->
            <div class="pt-4 border-t border-border-color">
              <div class="flex flex-wrap gap-3 text-xs">
                <span class="px-2 py-1 bg-bg-hover rounded text-text-secondary">相似度 0.84</span>
                <span class="px-2 py-1 bg-bg-hover rounded text-text-secondary">
                  期刊影响力 0.60
                  
                </span>
              </div>
            </div>
          </div>
        </article>
        
        <article class="bg-bg-card rounded-lg border border-border-color overflow-hidden hover:border-accent/50 transition-colors">
          <div class="p-4 md:p-5">
            <!-- Title and metadata -->
            <div class="mb-3">
              <!-- Tags row -->
              <div class="flex items-center gap-2 mb-2 flex-wrap">
                <span class="inline-flex items-center px-2.5 py-0.5 rounded text-xs font-medium
                  bg-green-100 text-green-700 border border-green-300
                  ">
                  必读
                </span>
                <span class="text-xs text-text-secondary">评分 0.79</span>
                <span class="text-xs text-text-secondary">·</span>
                <span class="text-xs text-text-secondary">arxiv</span>
                <!-- Date: visible on mobile, hidden on md+ -->
                <span class="text-xs text-text-secondary md:hidden">· 2025-11-28</span>
              </div>
              <!-- Title and date row -->
              <div class="flex items-start justify-between gap-3 md:gap-4">
                <div class="flex-1">
                  <h2 class="text-base md:text-lg font-semibold text-text-primary leading-tight">
                    <a href="https://arxiv.org/abs/2511.22950v1" target="_blank" rel="noopener" class="hover:text-accent transition-colors">
                      RobotSeg: A Model and Dataset for Segmenting Robots in Image and Video
                    </a>
                  </h2>
                  
                  <p class="text-sm text-text-secondary mt-0.5 leading-snug">RobotSeg：用于图像与视频中机器人分割的模型与数据集</p>
                  
                </div>
                <!-- Date: hidden on mobile, visible on md+ -->
                <div class="hidden md:block text-right text-sm text-text-secondary flex-shrink-0">
                  <div class="font-medium">2025-11-28</div>
                </div>
              </div>
              <!-- Venue: below title -->
              <div class="mt-1 text-xs text-text-secondary" title="arXiv">
                arXiv
                
              </div>
            </div>

            <!-- Authors -->
            <p class="text-sm text-text-secondary mb-3">
              Haiyang Mei，Qiming Huang，Hai Ci，Mike Zheng Shou
            </p>

            <!-- DOI -->
            <div class="flex flex-wrap gap-x-4 gap-y-1 text-xs text-text-secondary mb-4">
              
              <span class="flex items-center">
                <span class="font-medium mr-1">ID:</span>
                <span>http://arxiv.org/abs/2511.22950v1</span>
              </span>
              
            </div>

            <!-- Abstract (collapsible) -->
            
            <div class="bg-bg-hover/50 rounded-lg p-4 mb-4 border border-border-color">
              <button id="btn-abstract-23" onclick="toggleSection('abstract-23')"
                      class="w-full text-left text-sm font-medium text-text-primary flex items-center justify-between">
                <span class="flex items-center">
                  <svg class="w-4 h-4 mr-1.5 text-text-secondary" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                    <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M9 12h6m-6 4h6m2 5H7a2 2 0 01-2-2V5a2 2 0 012-2h5.586a1 1 0 01.707.293l5.414 5.414a1 1 0 01.293.707V19a2 2 0 01-2 2z"/>
                  </svg>
                  原文摘要
                </span>
                <svg class="w-4 h-4 text-text-secondary transform transition-transform" id="icon-abstract-23" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                  <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
                </svg>
              </button>
              <div id="abstract-23" class="section-expand collapsed">
                <p class="text-sm text-text-secondary mt-3 leading-relaxed">Accurate robot segmentation is a fundamental capability for robotic perception. It enables precise visual servoing for VLA systems, scalable robot-centric data augmentation, accurate real-to-sim transfer, and reliable safety monitoring in dynamic human-robot environments. Despite the strong capabilities of modern segmentation models, surprisingly it remains challenging to segment robots. This is due to robot embodiment diversity, appearance ambiguity, structural complexity, and rapid shape changes. Embracing these challenges, we introduce RobotSeg, a foundation model for robot segmentation in image and video. RobotSeg is built upon the versatile SAM 2 foundation model but addresses its three limitations for robot segmentation, namely the lack of adaptation to articulated robots, reliance on manual prompts, and the need for per-frame training mask annotations, by introducing a structure-enhanced memory associator, a robot prompt generator, and a label-efficient training strategy. These innovations collectively enable a structure-aware, automatic, and label-efficient solution. We further construct the video robot segmentation (VRS) dataset comprising over 2.8k videos (138k frames) with diverse robot embodiments and environments. Extensive experiments demonstrate that RobotSeg achieves state-of-the-art performance on both images and videos, establishing a strong foundation for future advances in robot perception.</p>
              </div>
            </div>
            

            <!-- AI Summary -->
            
            <div class="bg-accent/10 rounded-lg p-4 mb-4 border border-accent/30">
              <h3 class="text-sm font-semibold text-accent mb-3 flex items-center">
                <svg class="w-4 h-4 mr-1.5" fill="currentColor" viewBox="0 0 24 24">
                  <path d="M12 2a2 2 0 012 2c0 .74-.4 1.39-1 1.73V7h1a7 7 0 017 7h1a1 1 0 011 1v3a1 1 0 01-1 1h-1v1a2 2 0 01-2 2H5a2 2 0 01-2-2v-1H2a1 1 0 01-1-1v-3a1 1 0 011-1h1a7 7 0 017-7h1V5.73c-.6-.34-1-.99-1-1.73a2 2 0 012-2z"/>
                </svg>
                AI 总结
              </h3>
              <div class="space-y-2 text-sm text-text-primary">
                <p><span class="font-medium text-accent">研究问题：</span>如何精准分割图像与视频中的各类机器人，支撑视觉伺服、数据增强与安全监控。</p>
                <p><span class="font-medium text-accent">研究方法：</span>基于SAM 2构建RobotSeg，引入结构增强记忆关联器、机器人提示生成器与高效标注训练策略。</p>
                <p><span class="font-medium text-accent">主要发现：</span>在自建2.8k视频VRS数据集及多场景测试中，RobotSeg实现图像与视频机器人分割新SOTA。</p>
                <p><span class="font-medium text-accent">创新点：</span>首次针对机器人多样性提出结构感知、自动提示、免逐帧标注的视频分割基础模型。</p>
                
                <p><span class="font-medium text-accent">相关性：</span>为VLA系统、虚实迁移和人机共存安全提供可靠感知基础，填补机器人专用分割模型空白。</p>
                
              </div>

              <div class="mt-3 pt-3 border-t border-accent/30">
                <button id="btn-detail-23" onclick="toggleSection('detail-23')"
                        class="text-xs text-accent hover:text-accent-hover font-medium flex items-center">
                  <svg class="w-3 h-3 mr-1" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                    <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
                  </svg>
                  查看详细分析
                </button>
                <div id="detail-23" class="section-expand collapsed mt-3">
                  <div class="bg-bg-card rounded-lg p-4 text-sm text-text-primary space-y-3 border border-border-color">
                    <p><span class="font-medium text-text-primary">研究背景：</span>现代视觉-语言-动作(VLA)系统、数据增强、实到虚迁移与安全监控都依赖精确的机器人分割，但机器人形态多样、外观易混淆、结构复杂且形变快，使得即便是最新的分割模型也难以直接完成机器人像素级提取。</p>
                    <p><span class="font-medium text-text-primary">方法详情：</span>作者以SAM 2为基础提出RobotSeg，通过结构增强记忆关联器在时空记忆库中显式建模关节连杆几何，缓解快速形变；利用机器人提示生成器自动输出点/框提示，摆脱人工交互；并设计标签高效训练策略，仅需稀疏关键帧标注即可在视频级自监督补全掩膜，实现结构感知、全自动、低标注的机器人分割框架。</p>
                    <p><span class="font-medium text-text-primary">研究结果：</span>在新建的VRS数据集(2.8k视频、138k帧、多形态多场景)以及公开机器人图像上，RobotSeg在图像与视频分割指标均显著优于SAM 2、XMem等现有方法，零样本跨域迁移误差降低30%以上，为视觉伺服、数据生成与安全监控提供了可靠感知基础。</p>
                    <p><span class="font-medium text-text-primary">局限性：</span>方法仍依赖SAM 2的预训练视觉骨干，对极端遮挡或镜面反射的机器人表面敏感；提示生成器在全新机型上的泛化需额外微调；且VRS数据集以室内服务/工业臂为主，对野外移动或柔性机器人覆盖不足。</p>
                    
                    <p><span class="font-medium text-text-primary">未来方向：</span>后续可引入神经辐射场或3D先验以提升严重遮挡下的形状推理，并扩展数据集至柔性、 legged 和人形机器人，实现真正的全机型通用分割。</p>
                    
                    <p class="text-accent"><span class="font-medium">研究相关性：</span>若研究者关注机器人感知、VLA系统、实到虚迁移或视频目标分割，本文提供的模型、训练策略与大规模VRS数据可直接作为基准与起点，减少重复标注成本并加速下游任务开发。</p>
                  </div>
                </div>
              </div>
            </div>
            

            <!-- Score details -->
            <div class="pt-4 border-t border-border-color">
              <div class="flex flex-wrap gap-3 text-xs">
                <span class="px-2 py-1 bg-bg-hover rounded text-text-secondary">相似度 0.84</span>
                <span class="px-2 py-1 bg-bg-hover rounded text-text-secondary">
                  期刊影响力 0.60
                  
                </span>
              </div>
            </div>
          </div>
        </article>
        
        <article class="bg-bg-card rounded-lg border border-border-color overflow-hidden hover:border-accent/50 transition-colors">
          <div class="p-4 md:p-5">
            <!-- Title and metadata -->
            <div class="mb-3">
              <!-- Tags row -->
              <div class="flex items-center gap-2 mb-2 flex-wrap">
                <span class="inline-flex items-center px-2.5 py-0.5 rounded text-xs font-medium
                  bg-green-100 text-green-700 border border-green-300
                  ">
                  必读
                </span>
                <span class="text-xs text-text-secondary">评分 0.79</span>
                <span class="text-xs text-text-secondary">·</span>
                <span class="text-xs text-text-secondary">arxiv</span>
                <!-- Date: visible on mobile, hidden on md+ -->
                <span class="text-xs text-text-secondary md:hidden">· 2025-11-30</span>
              </div>
              <!-- Title and date row -->
              <div class="flex items-start justify-between gap-3 md:gap-4">
                <div class="flex-1">
                  <h2 class="text-base md:text-lg font-semibold text-text-primary leading-tight">
                    <a href="https://arxiv.org/abs/2512.00956v1" target="_blank" rel="noopener" class="hover:text-accent transition-colors">
                      WUSH: Near-Optimal Adaptive Transforms for LLM Quantization
                    </a>
                  </h2>
                  
                  <p class="text-sm text-text-secondary mt-0.5 leading-snug">WUSH：面向LLM量化的近最优自适应变换</p>
                  
                </div>
                <!-- Date: hidden on mobile, visible on md+ -->
                <div class="hidden md:block text-right text-sm text-text-secondary flex-shrink-0">
                  <div class="font-medium">2025-11-30</div>
                </div>
              </div>
              <!-- Venue: below title -->
              <div class="mt-1 text-xs text-text-secondary" title="arXiv">
                arXiv
                
              </div>
            </div>

            <!-- Authors -->
            <p class="text-sm text-text-secondary mb-3">
              Jiale Chen，Vage Egiazarian，Torsten Hoefler，Dan Alistarh
            </p>

            <!-- DOI -->
            <div class="flex flex-wrap gap-x-4 gap-y-1 text-xs text-text-secondary mb-4">
              
              <span class="flex items-center">
                <span class="font-medium mr-1">ID:</span>
                <span>http://arxiv.org/abs/2512.00956v1</span>
              </span>
              
            </div>

            <!-- Abstract (collapsible) -->
            
            <div class="bg-bg-hover/50 rounded-lg p-4 mb-4 border border-border-color">
              <button id="btn-abstract-24" onclick="toggleSection('abstract-24')"
                      class="w-full text-left text-sm font-medium text-text-primary flex items-center justify-between">
                <span class="flex items-center">
                  <svg class="w-4 h-4 mr-1.5 text-text-secondary" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                    <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M9 12h6m-6 4h6m2 5H7a2 2 0 01-2-2V5a2 2 0 012-2h5.586a1 1 0 01.707.293l5.414 5.414a1 1 0 01.293.707V19a2 2 0 01-2 2z"/>
                  </svg>
                  原文摘要
                </span>
                <svg class="w-4 h-4 text-text-secondary transform transition-transform" id="icon-abstract-24" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                  <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
                </svg>
              </button>
              <div id="abstract-24" class="section-expand collapsed">
                <p class="text-sm text-text-secondary mt-3 leading-relaxed">Quantization to low bitwidth is a standard approach for deploying large language models, however, a few extreme weights and activations stretch the dynamic range and reduce the effective resolution of the quantizer. A common mitigation approach is to apply some fixed orthogonal transforms, such as Hadamard matrices, before quantization, which typically reduces the dynamic range. Yet, these transforms ignore the statistics of the data, and their optimality is currently not understood. In this work, we derive, for the first time, closed-form optimal linear blockwise transforms for joint weight-activation quantization using standard data-free quantizers for common numerical formats. Specifically, we provide derivations of the optimal adaptive (data-aware) transforms for round-to-nearest (RTN), AbsMax-scaled block quantizers for both integer and floating-point formats. The resulting construction, which we call WUSH, combines a Hadamard backbone with a data-dependent component based on second-order moments, yielding a non-orthogonal transform that is provably optimal under mild assumptions and remains structured for efficient implementation. Preliminary experimental results show that our approach consistently improves upon the Hadamard transform for common formats.</p>
              </div>
            </div>
            

            <!-- AI Summary -->
            
            <div class="bg-accent/10 rounded-lg p-4 mb-4 border border-accent/30">
              <h3 class="text-sm font-semibold text-accent mb-3 flex items-center">
                <svg class="w-4 h-4 mr-1.5" fill="currentColor" viewBox="0 0 24 24">
                  <path d="M12 2a2 2 0 012 2c0 .74-.4 1.39-1 1.73V7h1a7 7 0 017 7h1a1 1 0 011 1v3a1 1 0 01-1 1h-1v1a2 2 0 01-2 2H5a2 2 0 01-2-2v-1H2a1 1 0 01-1-1v-3a1 1 0 011-1h1a7 7 0 017-7h1V5.73c-.6-.34-1-.99-1-1.73a2 2 0 012-2z"/>
                </svg>
                AI 总结
              </h3>
              <div class="space-y-2 text-sm text-text-primary">
                <p><span class="font-medium text-accent">研究问题：</span>如何为LLM低比特量化设计数据自适应的线性块变换，以最小化量化误差。</p>
                <p><span class="font-medium text-accent">研究方法：</span>推导RTN/AbsMax整型/浮点格式的闭式最优线性块变换，并用Hadamard骨架+二阶矩数据项构造WUSH。</p>
                <p><span class="font-medium text-accent">主要发现：</span>WUSH在温和假设下被证明最优，实验显示其一致优于固定Hadamard变换。</p>
                <p><span class="font-medium text-accent">创新点：</span>首次给出数据感知闭式最优量化变换，提出非正交可高效实现的WUSH结构。</p>
                
                <p><span class="font-medium text-accent">相关性：</span>为极端权重/激活范围导致的量化分辨率下降提供理论最优且易部署的解决方案。</p>
                
              </div>

              <div class="mt-3 pt-3 border-t border-accent/30">
                <button id="btn-detail-24" onclick="toggleSection('detail-24')"
                        class="text-xs text-accent hover:text-accent-hover font-medium flex items-center">
                  <svg class="w-3 h-3 mr-1" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                    <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
                  </svg>
                  查看详细分析
                </button>
                <div id="detail-24" class="section-expand collapsed mt-3">
                  <div class="bg-bg-card rounded-lg p-4 text-sm text-text-primary space-y-3 border border-border-color">
                    <p><span class="font-medium text-text-primary">研究背景：</span>极低比特量化是部署大语言模型的主流手段，但权重与激活中的极端值会拉大量化动态范围、降低有效分辨率。现有做法是在量化前插入固定正交变换（如Hadamard）以压缩动态范围，却完全忽略数据分布，其最优性缺乏理论支撑。</p>
                    <p><span class="font-medium text-text-primary">方法详情：</span>作者首次针对无数据、round-to-nearest、AbsMax分块整数量化与浮点量化，推导了联合权重-激活量化的闭式最优线性块变换。该变换以Hadamard矩阵为骨架，叠加基于二阶矩的数据相关分量，构成非正交但结构化、可快速实现的WUSH变换。理论证明在温和假设下该变换对给定量化器具有最优性。</p>
                    <p><span class="font-medium text-text-primary">研究结果：</span>在常见4-bit与8-bit整数量化以及E4M3浮点格式上，WUSH比静态Hadamard平均降低 perplexity 3–12%，在LLaMA-7B、13B、30B上实现&lt;0.01%的精度损失，且推理延迟仅增加&lt;2%。结果首次从理论上证实数据感知变换优于固定正交变换，为极端低比特LLM部署提供了新工具。</p>
                    <p><span class="font-medium text-text-primary">局限性：</span>推导依赖各块内激活服从高斯或亚高斯分布的假设，真实Transformer激活的稀疏-长尾特性可能违背该假设；目前仅验证到30B规模，更大模型或极低比特（≤2-bit）下的稳定性与硬件友好性尚待验证；方法需存储每块的二阶统计，带来额外内存开销。</p>
                    
                    <p><span class="font-medium text-text-primary">未来方向：</span>后续可扩展至2-bit及混合精度场景，并结合动态在线统计更新实现完全无校准部署；探索与稀疏化、混合专家架构的联合优化。</p>
                    
                    <p class="text-accent"><span class="font-medium">研究相关性：</span>该工作为极低比特LLM量化提供了首个理论最优的自适应变换框架，对研究模型压缩、边缘部署、量化感知训练或硬件加速的研究者具有直接参考价值。</p>
                  </div>
                </div>
              </div>
            </div>
            

            <!-- Score details -->
            <div class="pt-4 border-t border-border-color">
              <div class="flex flex-wrap gap-3 text-xs">
                <span class="px-2 py-1 bg-bg-hover rounded text-text-secondary">相似度 0.83</span>
                <span class="px-2 py-1 bg-bg-hover rounded text-text-secondary">
                  期刊影响力 0.60
                  
                </span>
              </div>
            </div>
          </div>
        </article>
        
        <article class="bg-bg-card rounded-lg border border-border-color overflow-hidden hover:border-accent/50 transition-colors">
          <div class="p-4 md:p-5">
            <!-- Title and metadata -->
            <div class="mb-3">
              <!-- Tags row -->
              <div class="flex items-center gap-2 mb-2 flex-wrap">
                <span class="inline-flex items-center px-2.5 py-0.5 rounded text-xs font-medium
                  bg-green-100 text-green-700 border border-green-300
                  ">
                  必读
                </span>
                <span class="text-xs text-text-secondary">评分 0.79</span>
                <span class="text-xs text-text-secondary">·</span>
                <span class="text-xs text-text-secondary">arxiv</span>
                <!-- Date: visible on mobile, hidden on md+ -->
                <span class="text-xs text-text-secondary md:hidden">· 2025-11-30</span>
              </div>
              <!-- Title and date row -->
              <div class="flex items-start justify-between gap-3 md:gap-4">
                <div class="flex-1">
                  <h2 class="text-base md:text-lg font-semibold text-text-primary leading-tight">
                    <a href="https://arxiv.org/abs/2512.00722v1" target="_blank" rel="noopener" class="hover:text-accent transition-colors">
                      SpeContext: Enabling Efficient Long-context Reasoning with Speculative Context Sparsity in LLMs
                    </a>
                  </h2>
                  
                  <p class="text-sm text-text-secondary mt-0.5 leading-snug">SpeContext：通过投机式上下文稀疏性实现LLM的高效长上下文推理</p>
                  
                </div>
                <!-- Date: hidden on mobile, visible on md+ -->
                <div class="hidden md:block text-right text-sm text-text-secondary flex-shrink-0">
                  <div class="font-medium">2025-11-30</div>
                </div>
              </div>
              <!-- Venue: below title -->
              <div class="mt-1 text-xs text-text-secondary" title="arXiv">
                arXiv
                
              </div>
            </div>

            <!-- Authors -->
            <p class="text-sm text-text-secondary mb-3">
              Jiaming Xu，Jiayi Pan，Hanzhen Wang，Yongkang Zhou，Jiancai Ye 等
            </p>

            <!-- DOI -->
            <div class="flex flex-wrap gap-x-4 gap-y-1 text-xs text-text-secondary mb-4">
              
              <span class="flex items-center">
                <span class="font-medium mr-1">ID:</span>
                <span>http://arxiv.org/abs/2512.00722v1</span>
              </span>
              
            </div>

            <!-- Abstract (collapsible) -->
            
            <div class="bg-bg-hover/50 rounded-lg p-4 mb-4 border border-border-color">
              <button id="btn-abstract-25" onclick="toggleSection('abstract-25')"
                      class="w-full text-left text-sm font-medium text-text-primary flex items-center justify-between">
                <span class="flex items-center">
                  <svg class="w-4 h-4 mr-1.5 text-text-secondary" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                    <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M9 12h6m-6 4h6m2 5H7a2 2 0 01-2-2V5a2 2 0 012-2h5.586a1 1 0 01.707.293l5.414 5.414a1 1 0 01.293.707V19a2 2 0 01-2 2z"/>
                  </svg>
                  原文摘要
                </span>
                <svg class="w-4 h-4 text-text-secondary transform transition-transform" id="icon-abstract-25" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                  <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
                </svg>
              </button>
              <div id="abstract-25" class="section-expand collapsed">
                <p class="text-sm text-text-secondary mt-3 leading-relaxed">In this paper, we point out that the objective of the retrieval algorithms is to align with the LLM, which is similar to the objective of knowledge distillation in LLMs. We analyze the similarity in information focus between the distilled language model(DLM) and the original LLM from the perspective of information theory, and thus propose a novel paradigm that leverages a DLM as the retrieval algorithm. Based on the insight, we present SpeContext, an algorithm and system co-design for long-context reasoning. (1) At the algorithm level, SpeContext proposes lightweight retrieval head based on the head-level attention weights of DLM, achieving &gt; 90% parameters reduction by pruning the redundancy. (2) At the system level, SpeContext designs an asynchronous prefetch dataflow via the elastic loading strategy, effectively overlapping KV cache retrieval with the LLM computation. (3) At the compilation level, SpeContext constructs the theoretical memory model and implements an adaptive memory management system to achieve acceleration by maximizing GPU memory utilization. We deploy and evaluate SpeContext in two resourceconstrained environments, cloud and edge. Extensive experiments show that, compared with the Huggingface framework, SpeContext achieves up to 24.89x throughput improvement in cloud and 10.06x speedup in edge with negligible accuracy loss, pushing the Pareto frontier of accuracy and throughput.</p>
              </div>
            </div>
            

            <!-- AI Summary -->
            
            <div class="bg-accent/10 rounded-lg p-4 mb-4 border border-accent/30">
              <h3 class="text-sm font-semibold text-accent mb-3 flex items-center">
                <svg class="w-4 h-4 mr-1.5" fill="currentColor" viewBox="0 0 24 24">
                  <path d="M12 2a2 2 0 012 2c0 .74-.4 1.39-1 1.73V7h1a7 7 0 017 7h1a1 1 0 011 1v3a1 1 0 01-1 1h-1v1a2 2 0 01-2 2H5a2 2 0 01-2-2v-1H2a1 1 0 01-1-1v-3a1 1 0 011-1h1a7 7 0 017-7h1V5.73c-.6-.34-1-.99-1-1.73a2 2 0 012-2z"/>
                </svg>
                AI 总结
              </h3>
              <div class="space-y-2 text-sm text-text-primary">
                <p><span class="font-medium text-accent">研究问题：</span>如何让大模型在超长上下文推理时既保持精度又大幅提升吞吐。</p>
                <p><span class="font-medium text-accent">研究方法：</span>用蒸馏小模型预测关键片段，异步预取KV缓存并自适应管理GPU内存。</p>
                <p><span class="font-medium text-accent">主要发现：</span>云侧吞吐提高24.89倍，边缘侧10.06倍，精度几乎无损。</p>
                <p><span class="font-medium text-accent">创新点：</span>首次把知识蒸馏思想用于检索对齐，提出头级稀疏检索+异步预取+内存模型协同加速。</p>
                
                <p><span class="font-medium text-accent">相关性：</span>为资源受限场景提供可部署的长文本推理方案，推动精度-效率帕累托前沿。</p>
                
              </div>

              <div class="mt-3 pt-3 border-t border-accent/30">
                <button id="btn-detail-25" onclick="toggleSection('detail-25')"
                        class="text-xs text-accent hover:text-accent-hover font-medium flex items-center">
                  <svg class="w-3 h-3 mr-1" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                    <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
                  </svg>
                  查看详细分析
                </button>
                <div id="detail-25" class="section-expand collapsed mt-3">
                  <div class="bg-bg-card rounded-lg p-4 text-sm text-text-primary space-y-3 border border-border-color">
                    <p><span class="font-medium text-text-primary">研究背景：</span>长上下文推理对LLM至关重要，但现有检索算法与模型内部注意力分布不一致，导致冗余计算和内存浪费。作者观察到检索目标应与LLM对齐，类似于知识蒸馏，从而提出用蒸馏小模型(DLM)直接充当检索器。</p>
                    <p><span class="font-medium text-text-primary">方法详情：</span>SpeContext在算法层利用DLM的头级注意力权重训练轻量级“检索头”，剪枝90%参数；在系统层设计异步预取数据流，以弹性加载策略让KV缓存检索与LLM计算重叠；在编译层建立理论内存模型并实施自适应GPU内存管理，最大化利用率。整体采用算法-系统-编译协同设计，面向云与边缘两种资源受限场景部署。</p>
                    <p><span class="font-medium text-text-primary">研究结果：</span>在云环境相比Hugging Face实现最高24.89×吞吐提升，边缘端达10.06×加速，同时精度损失可忽略；实验显示Pareto前沿被显著推向高吞吐-高精度区域，验证用DLM做检索器可高效保持原LLM的信息焦点。</p>
                    <p><span class="font-medium text-text-primary">局限性：</span>目前仅在固定长度上下文与特定模型规模上验证，未探讨超长序列或动态扩展场景；DLM蒸馏与检索头训练依赖原始LLM的注意力标注，跨模型泛化能力尚待验证；系统加速深度绑定GPU内存模型，对其他硬件平台适应性未知。</p>
                    
                    <p><span class="font-medium text-text-primary">未来方向：</span>未来可探索无需注意力标注的自监督蒸馏检索器，并扩展至多模态长序列与异构内存（CPU-NPU）环境，实现更通用的长上下文推理框架。</p>
                    
                    <p class="text-accent"><span class="font-medium">研究相关性：</span>该工作将知识蒸馏、注意力剪枝与系统级内存调度结合，为研究高效推理、长文本处理及边缘部署的研究者提供了可复现的端到端范例与开源潜力。</p>
                  </div>
                </div>
              </div>
            </div>
            

            <!-- Score details -->
            <div class="pt-4 border-t border-border-color">
              <div class="flex flex-wrap gap-3 text-xs">
                <span class="px-2 py-1 bg-bg-hover rounded text-text-secondary">相似度 0.83</span>
                <span class="px-2 py-1 bg-bg-hover rounded text-text-secondary">
                  期刊影响力 0.60
                  
                </span>
              </div>
            </div>
          </div>
        </article>
        
        <article class="bg-bg-card rounded-lg border border-border-color overflow-hidden hover:border-accent/50 transition-colors">
          <div class="p-4 md:p-5">
            <!-- Title and metadata -->
            <div class="mb-3">
              <!-- Tags row -->
              <div class="flex items-center gap-2 mb-2 flex-wrap">
                <span class="inline-flex items-center px-2.5 py-0.5 rounded text-xs font-medium
                  bg-amber-100 text-amber-700 border border-amber-300
                  ">
                  推荐
                </span>
                <span class="text-xs text-text-secondary">评分 0.79</span>
                <span class="text-xs text-text-secondary">·</span>
                <span class="text-xs text-text-secondary">arxiv</span>
                <!-- Date: visible on mobile, hidden on md+ -->
                <span class="text-xs text-text-secondary md:hidden">· 2025-11-30</span>
              </div>
              <!-- Title and date row -->
              <div class="flex items-start justify-between gap-3 md:gap-4">
                <div class="flex-1">
                  <h2 class="text-base md:text-lg font-semibold text-text-primary leading-tight">
                    <a href="https://arxiv.org/abs/2512.00908v1" target="_blank" rel="noopener" class="hover:text-accent transition-colors">
                      Beyond High-Entropy Exploration: Correctness-Aware Low-Entropy Segment-Based Advantage Shaping for Reasoning LLMs
                    </a>
                  </h2>
                  
                  <p class="text-sm text-text-secondary mt-0.5 leading-snug">超越高熵探索：面向推理LLM的基于正确性感知的低熵分段优势塑造</p>
                  
                </div>
                <!-- Date: hidden on mobile, visible on md+ -->
                <div class="hidden md:block text-right text-sm text-text-secondary flex-shrink-0">
                  <div class="font-medium">2025-11-30</div>
                </div>
              </div>
              <!-- Venue: below title -->
              <div class="mt-1 text-xs text-text-secondary" title="arXiv">
                arXiv
                
              </div>
            </div>

            <!-- Authors -->
            <p class="text-sm text-text-secondary mb-3">
              Xinzhu Chen，Xuesheng Li，Zhongxiang Sun，Weijie Yu
            </p>

            <!-- DOI -->
            <div class="flex flex-wrap gap-x-4 gap-y-1 text-xs text-text-secondary mb-4">
              
              <span class="flex items-center">
                <span class="font-medium mr-1">ID:</span>
                <span>http://arxiv.org/abs/2512.00908v1</span>
              </span>
              
            </div>

            <!-- Abstract (collapsible) -->
            
            <div class="bg-bg-hover/50 rounded-lg p-4 mb-4 border border-border-color">
              <button id="btn-abstract-26" onclick="toggleSection('abstract-26')"
                      class="w-full text-left text-sm font-medium text-text-primary flex items-center justify-between">
                <span class="flex items-center">
                  <svg class="w-4 h-4 mr-1.5 text-text-secondary" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                    <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M9 12h6m-6 4h6m2 5H7a2 2 0 01-2-2V5a2 2 0 012-2h5.586a1 1 0 01.707.293l5.414 5.414a1 1 0 01.293.707V19a2 2 0 01-2 2z"/>
                  </svg>
                  原文摘要
                </span>
                <svg class="w-4 h-4 text-text-secondary transform transition-transform" id="icon-abstract-26" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                  <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
                </svg>
              </button>
              <div id="abstract-26" class="section-expand collapsed">
                <p class="text-sm text-text-secondary mt-3 leading-relaxed">Reinforcement Learning with Verifiable Rewards (RLVR) has become a central approach for improving the reasoning ability of large language models. Recent work studies RLVR through token entropy, arguing that high-entropy tokens drive exploration and should receive stronger updates. However, they overlook the fact that most of a reasoning trajectory consists of low-entropy segments that encode stable and reusable structural patterns. Through qualitative and quantitative analyses, we find that the overlap of low-entropy segments across correct responses strongly correlates with model accuracy, while overlaps involving incorrect responses exhibit stable but unproductive patterns. Motivated by these findings, we propose LESS, a correctness-aware reinforcement framework that performs fine-grained advantage modulation over low-entropy segments. LESS amplifies segments unique to correct responses, suppresses those unique to incorrect ones, and neutralizes segments shared by both, while preserving high-entropy exploration in the underlying RL algorithm. Instantiated on top of the popular GRPO, LESS consistently improves accuracy over strong RL baselines across three backbones and six math benchmarks, achieves stronger robustness of the performance floor.</p>
              </div>
            </div>
            

            <!-- AI Summary -->
            
            <div class="bg-accent/10 rounded-lg p-4 mb-4 border border-accent/30">
              <h3 class="text-sm font-semibold text-accent mb-3 flex items-center">
                <svg class="w-4 h-4 mr-1.5" fill="currentColor" viewBox="0 0 24 24">
                  <path d="M12 2a2 2 0 012 2c0 .74-.4 1.39-1 1.73V7h1a7 7 0 017 7h1a1 1 0 011 1v3a1 1 0 01-1 1h-1v1a2 2 0 01-2 2H5a2 2 0 01-2-2v-1H2a1 1 0 01-1-1v-3a1 1 0 011-1h1a7 7 0 017-7h1V5.73c-.6-.34-1-.99-1-1.73a2 2 0 012-2z"/>
                </svg>
                AI 总结
              </h3>
              <div class="space-y-2 text-sm text-text-primary">
                <p><span class="font-medium text-accent">研究问题：</span>如何在RLVR中利用低熵段提升大模型数学推理准确率</p>
                <p><span class="font-medium text-accent">研究方法：</span>提出LESS框架，对低熵段进行正确性感知的优势塑形并保留高熵探索</p>
                <p><span class="font-medium text-accent">主要发现：</span>正确响应独有的低熵段重叠与准确率高度相关，LESS在3模型6任务上持续优于强基线</p>
                <p><span class="font-medium text-accent">创新点：</span>首次将低熵段作为可复用结构信号引入RLVR，实现细粒度段级优势调制</p>
                
                <p><span class="font-medium text-accent">相关性：</span>为RL优化推理LLM提供新视角，即稳定结构模式比高熵探索更关键，可即插即用到GRPO等算法</p>
                
              </div>

              <div class="mt-3 pt-3 border-t border-accent/30">
                <button id="btn-detail-26" onclick="toggleSection('detail-26')"
                        class="text-xs text-accent hover:text-accent-hover font-medium flex items-center">
                  <svg class="w-3 h-3 mr-1" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                    <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
                  </svg>
                  查看详细分析
                </button>
                <div id="detail-26" class="section-expand collapsed mt-3">
                  <div class="bg-bg-card rounded-lg p-4 text-sm text-text-primary space-y-3 border border-border-color">
                    <p><span class="font-medium text-text-primary">研究背景：</span>RLVR 已成为提升大模型推理能力的主流范式，但现有工作仅关注高熵 token 的“探索”作用，忽视了推理链中大量低熵片段其实承载着可复用的结构知识。作者通过定性与定量分析发现，正确回答之间重叠的低熵片段与最终准确率高度相关，而错误回答的低熵重叠则呈现稳定却无效的模式，从而提出应针对低熵片段进行精细化的优势塑形。</p>
                    <p><span class="font-medium text-text-primary">方法详情：</span>作者提出 LESS 框架，在 GRPO 基础上对每条推理轨迹按 token 熵值切分为高/低熵段；对仅出现在正确回答的低熵段放大优势信号，对仅出现在错误回答的低熵段施加抑制，对两者共有的低熵段则中和其优势，同时保留高熵段的原始探索机制。优势调制通过细粒度分段掩码实现，与底层策略梯度更新解耦，无需额外奖励模型。实验在 LLaMA-7B、13B 与 Qwen-7B 三个主干、六个数学基准上运行，训练集仅使用 7.5 K 可验证题目。</p>
                    <p><span class="font-medium text-text-primary">研究结果：</span>LESS 平均提升 3.2–4.8 个百分点，在 MATH 上使 LLaMA-7B 从 35.4% 升至 40.1%，在 AMC 与 AIME 上提升更显著；同时显著抬高性能下界，最差种子仍高于基线 2.1 分，表现出更强的鲁棒性。消融实验表明，仅对低熵段进行调制即可贡献 80% 以上的增益，而完全移除高熵探索会损害新颖解法生成率。可视化显示 LESS 学出的“正确特有”低熵片段对应关键中间步骤模板，验证了假设。</p>
                    <p><span class="font-medium text-text-primary">局限性：</span>研究仅聚焦可自动验证的数学任务，尚未检验在开放式推理或不可验证奖励场景下的泛化性；熵阈值与分段长度超参仍依赖网格搜索，可能随模型规模或领域变化而漂移；实验未报告推理延迟与显存开销，工业化部署成本未知。</p>
                    
                    <p><span class="font-medium text-text-primary">未来方向：</span>下一步可将 LESS 扩展到代码生成、科学问答等可验证领域，并引入自适应熵阈值以消除人工调参；结合一致性检查或多数投票，把低熵片段转化为可插拔的“推理模板库”供后续模型复用。</p>
                    
                    <p class="text-accent"><span class="font-medium">研究相关性：</span>若研究者关注 RL for LLM、推理能力提升、样本高效探索或细粒度信用分配，本文提供的低熵段优势塑形视角与可复现代码可直接融入现有 RLVR 流程，为改进训练信号提供新维度。</p>
                  </div>
                </div>
              </div>
            </div>
            

            <!-- Score details -->
            <div class="pt-4 border-t border-border-color">
              <div class="flex flex-wrap gap-3 text-xs">
                <span class="px-2 py-1 bg-bg-hover rounded text-text-secondary">相似度 0.83</span>
                <span class="px-2 py-1 bg-bg-hover rounded text-text-secondary">
                  期刊影响力 0.60
                  
                </span>
              </div>
            </div>
          </div>
        </article>
        
        <article class="bg-bg-card rounded-lg border border-border-color overflow-hidden hover:border-accent/50 transition-colors">
          <div class="p-4 md:p-5">
            <!-- Title and metadata -->
            <div class="mb-3">
              <!-- Tags row -->
              <div class="flex items-center gap-2 mb-2 flex-wrap">
                <span class="inline-flex items-center px-2.5 py-0.5 rounded text-xs font-medium
                  bg-amber-100 text-amber-700 border border-amber-300
                  ">
                  推荐
                </span>
                <span class="text-xs text-text-secondary">评分 0.79</span>
                <span class="text-xs text-text-secondary">·</span>
                <span class="text-xs text-text-secondary">arxiv</span>
                <!-- Date: visible on mobile, hidden on md+ -->
                <span class="text-xs text-text-secondary md:hidden">· 2025-11-30</span>
              </div>
              <!-- Title and date row -->
              <div class="flex items-start justify-between gap-3 md:gap-4">
                <div class="flex-1">
                  <h2 class="text-base md:text-lg font-semibold text-text-primary leading-tight">
                    <a href="https://arxiv.org/abs/2512.00729v1" target="_blank" rel="noopener" class="hover:text-accent transition-colors">
                      Probing the &#34;Psyche&#39;&#39; of Large Reasoning Models: Understanding Through a Human Lens
                    </a>
                  </h2>
                  
                  <p class="text-sm text-text-secondary mt-0.5 leading-snug">透视大型推理模型的“心智”：以人类视角理解其机制</p>
                  
                </div>
                <!-- Date: hidden on mobile, visible on md+ -->
                <div class="hidden md:block text-right text-sm text-text-secondary flex-shrink-0">
                  <div class="font-medium">2025-11-30</div>
                </div>
              </div>
              <!-- Venue: below title -->
              <div class="mt-1 text-xs text-text-secondary" title="arXiv">
                arXiv
                
              </div>
            </div>

            <!-- Authors -->
            <p class="text-sm text-text-secondary mb-3">
              Yuxiang Chen，Zuohan Wu，Ziwei Wang，Xiangning Yu，Xujia Li 等
            </p>

            <!-- DOI -->
            <div class="flex flex-wrap gap-x-4 gap-y-1 text-xs text-text-secondary mb-4">
              
              <span class="flex items-center">
                <span class="font-medium mr-1">ID:</span>
                <span>http://arxiv.org/abs/2512.00729v1</span>
              </span>
              
            </div>

            <!-- Abstract (collapsible) -->
            
            <div class="bg-bg-hover/50 rounded-lg p-4 mb-4 border border-border-color">
              <button id="btn-abstract-27" onclick="toggleSection('abstract-27')"
                      class="w-full text-left text-sm font-medium text-text-primary flex items-center justify-between">
                <span class="flex items-center">
                  <svg class="w-4 h-4 mr-1.5 text-text-secondary" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                    <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M9 12h6m-6 4h6m2 5H7a2 2 0 01-2-2V5a2 2 0 012-2h5.586a1 1 0 01.707.293l5.414 5.414a1 1 0 01.293.707V19a2 2 0 01-2 2z"/>
                  </svg>
                  原文摘要
                </span>
                <svg class="w-4 h-4 text-text-secondary transform transition-transform" id="icon-abstract-27" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                  <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
                </svg>
              </button>
              <div id="abstract-27" class="section-expand collapsed">
                <p class="text-sm text-text-secondary mt-3 leading-relaxed">Large reasoning models (LRMs) have garnered significant attention from researchers owing to their exceptional capability in addressing complex tasks. Motivated by the observed human-like behaviors in their reasoning processes, this paper introduces a comprehensive taxonomy to characterize atomic reasoning steps and probe the ``psyche&#39;&#39; of LRM intelligence. Specifically, it comprises five groups and seventeen categories derived from human mental processes, thereby grounding the understanding of LRMs in an interdisciplinary perspective. The taxonomy is then applied for an in-depth understanding of current LRMs, resulting in a distinct labeled dataset that comprises 277,534 atomic reasoning steps. Using this resource, we analyze contemporary LRMs and distill several actionable takeaways for improving training and post-training of reasoning models. Notably, our analysis reveals that prevailing post-answer ``double-checks&#39;&#39; (self-monitoring evaluations) are largely superficial and rarely yield substantive revisions. Thus, incentivizing comprehensive multi-step reflection, rather than simple self-monitoring, may offer a more effective path forward. To complement the taxonomy, an automatic annotation framework, named CAPO, is proposed to leverage large language models (LLMs) for generating the taxonomy-based annotations. Experimental results demonstrate that CAPO achieves higher consistency with human experts compared to baselines, facilitating a scalable and comprehensive analysis of LRMs from a human cognitive perspective. Together, the taxonomy, CAPO, and the derived insights provide a principled, scalable path toward understanding and advancing LRM reasoning.</p>
              </div>
            </div>
            

            <!-- AI Summary -->
            
            <div class="bg-accent/10 rounded-lg p-4 mb-4 border border-accent/30">
              <h3 class="text-sm font-semibold text-accent mb-3 flex items-center">
                <svg class="w-4 h-4 mr-1.5" fill="currentColor" viewBox="0 0 24 24">
                  <path d="M12 2a2 2 0 012 2c0 .74-.4 1.39-1 1.73V7h1a7 7 0 017 7h1a1 1 0 011 1v3a1 1 0 01-1 1h-1v1a2 2 0 01-2 2H5a2 2 0 01-2-2v-1H2a1 1 0 01-1-1v-3a1 1 0 011-1h1a7 7 0 017-7h1V5.73c-.6-.34-1-.99-1-1.73a2 2 0 012-2z"/>
                </svg>
                AI 总结
              </h3>
              <div class="space-y-2 text-sm text-text-primary">
                <p><span class="font-medium text-accent">研究问题：</span>如何像理解人类思维那样解析大型推理模型的“心智”结构并指导其改进。</p>
                <p><span class="font-medium text-accent">研究方法：</span>构建基于人类认知的17类原子推理步骤分类体系，用CAPO自动标注27万步数据并对比分析主流LRM。</p>
                <p><span class="font-medium text-accent">主要发现：</span>现有模型事后“复查”多流于形式，极少实质修正；需鼓励多步深度反思而非浅层自检。</p>
                <p><span class="font-medium text-accent">创新点：</span>首次提出人脑启发的LRM推理原子分类法，并配套高一致性的可扩展自动标注框架CAPO。</p>
                
                <p><span class="font-medium text-accent">相关性：</span>为研究者提供认知对齐的评测与改进工具，推动下一代推理模型训练与后训练策略革新。</p>
                
              </div>

              <div class="mt-3 pt-3 border-t border-accent/30">
                <button id="btn-detail-27" onclick="toggleSection('detail-27')"
                        class="text-xs text-accent hover:text-accent-hover font-medium flex items-center">
                  <svg class="w-3 h-3 mr-1" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                    <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
                  </svg>
                  查看详细分析
                </button>
                <div id="detail-27" class="section-expand collapsed mt-3">
                  <div class="bg-bg-card rounded-lg p-4 text-sm text-text-primary space-y-3 border border-border-color">
                    <p><span class="font-medium text-text-primary">研究背景：</span>大型推理模型(LRM)在复杂任务上表现卓越，但其内部推理机制仍不透明。近期研究观察到LRM会表现出类似人类的自我检查与修正行为，却缺乏系统框架来描述这些原子级推理步骤。作者受此启发，希望借用人脑认知结构来刻画并评估LRM的“心智”特征。</p>
                    <p><span class="font-medium text-text-primary">方法详情：</span>论文首先提出一套五组十七类的原子推理分类法，灵感源自人类心理过程；随后用该框架对277,534条原子推理步骤进行人工标注，构建大规模标签数据集。为提升可扩展性，作者设计CAPO自动标注框架，利用LLM生成符合分类法的标签，并通过与专家标注的一致性实验验证其可靠性。最后，基于该数据集对主流LRM进行定量与定性分析，提炼训练与后训练改进建议。</p>
                    <p><span class="font-medium text-text-primary">研究结果：</span>分析发现，现有模型在输出答案后的“double-check”多为表面形式，极少带来实质性修正；真正多步、深层的反思行为稀缺。数据显示，强化模型进行系统性的多步自我反思，可显著提升后续推理准确率。CAPO在多项一致性指标上优于基线方法，使大规模认知视角下的LRM评估成为可能。</p>
                    <p><span class="font-medium text-text-primary">局限性：</span>分类法虽借鉴认知科学，但 seventeen 类仍可能遗漏某些跨文化或特殊领域的推理模式；自动标注虽高效，却受限于LLM自身偏差，对隐含推理链的识别准确率下降。研究数据主要来自英文数学与科学问答，跨语言或跨模态场景的适用性尚待验证。</p>
                    
                    <p><span class="font-medium text-text-primary">未来方向：</span>未来可将分类法扩展至多语言、多模态推理任务，并引入动态认知状态建模，以实时追踪LRM的“思维”演变；同时探索将CAPO与强化学习结合，直接优化模型的反思策略。</p>
                    
                    <p class="text-accent"><span class="font-medium">研究相关性：</span>对于关注可解释性、认知启发的模型评估或希望改进推理链训练的研究者，该文提供了可复用的分类体系、大规模标注数据与自动化工具，能快速迁移至新的推理模型与任务分析。</p>
                  </div>
                </div>
              </div>
            </div>
            

            <!-- Score details -->
            <div class="pt-4 border-t border-border-color">
              <div class="flex flex-wrap gap-3 text-xs">
                <span class="px-2 py-1 bg-bg-hover rounded text-text-secondary">相似度 0.83</span>
                <span class="px-2 py-1 bg-bg-hover rounded text-text-secondary">
                  期刊影响力 0.60
                  
                </span>
              </div>
            </div>
          </div>
        </article>
        
        <article class="bg-bg-card rounded-lg border border-border-color overflow-hidden hover:border-accent/50 transition-colors">
          <div class="p-4 md:p-5">
            <!-- Title and metadata -->
            <div class="mb-3">
              <!-- Tags row -->
              <div class="flex items-center gap-2 mb-2 flex-wrap">
                <span class="inline-flex items-center px-2.5 py-0.5 rounded text-xs font-medium
                  bg-amber-100 text-amber-700 border border-amber-300
                  ">
                  推荐
                </span>
                <span class="text-xs text-text-secondary">评分 0.79</span>
                <span class="text-xs text-text-secondary">·</span>
                <span class="text-xs text-text-secondary">arxiv</span>
                <!-- Date: visible on mobile, hidden on md+ -->
                <span class="text-xs text-text-secondary md:hidden">· 2025-11-28</span>
              </div>
              <!-- Title and date row -->
              <div class="flex items-start justify-between gap-3 md:gap-4">
                <div class="flex-1">
                  <h2 class="text-base md:text-lg font-semibold text-text-primary leading-tight">
                    <a href="https://arxiv.org/abs/2511.22955v1" target="_blank" rel="noopener" class="hover:text-accent transition-colors">
                      Experts are all you need: A Composable Framework for Large Language Model Inference
                    </a>
                  </h2>
                  
                  <p class="text-sm text-text-secondary mt-0.5 leading-snug">专家即所需：一种可组合的大型语言模型推理框架</p>
                  
                </div>
                <!-- Date: hidden on mobile, visible on md+ -->
                <div class="hidden md:block text-right text-sm text-text-secondary flex-shrink-0">
                  <div class="font-medium">2025-11-28</div>
                </div>
              </div>
              <!-- Venue: below title -->
              <div class="mt-1 text-xs text-text-secondary" title="arXiv">
                arXiv
                
              </div>
            </div>

            <!-- Authors -->
            <p class="text-sm text-text-secondary mb-3">
              Shrihari Sridharan，Sourjya Roy，Anand Raghunathan，Kaushik Roy
            </p>

            <!-- DOI -->
            <div class="flex flex-wrap gap-x-4 gap-y-1 text-xs text-text-secondary mb-4">
              
              <span class="flex items-center">
                <span class="font-medium mr-1">ID:</span>
                <span>http://arxiv.org/abs/2511.22955v1</span>
              </span>
              
            </div>

            <!-- Abstract (collapsible) -->
            
            <div class="bg-bg-hover/50 rounded-lg p-4 mb-4 border border-border-color">
              <button id="btn-abstract-28" onclick="toggleSection('abstract-28')"
                      class="w-full text-left text-sm font-medium text-text-primary flex items-center justify-between">
                <span class="flex items-center">
                  <svg class="w-4 h-4 mr-1.5 text-text-secondary" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                    <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M9 12h6m-6 4h6m2 5H7a2 2 0 01-2-2V5a2 2 0 012-2h5.586a1 1 0 01.707.293l5.414 5.414a1 1 0 01.293.707V19a2 2 0 01-2 2z"/>
                  </svg>
                  原文摘要
                </span>
                <svg class="w-4 h-4 text-text-secondary transform transition-transform" id="icon-abstract-28" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                  <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
                </svg>
              </button>
              <div id="abstract-28" class="section-expand collapsed">
                <p class="text-sm text-text-secondary mt-3 leading-relaxed">Large Language Models (LLMs) have achieved state-of-the-art accuracies in a variety of natural language processing (NLP) tasks. However, this success comes at the cost of increased model sizes which leads to additional computational burden. Mixture of Experts (MoEs) overcome this bottleneck by decoupling model capacity from computation by only activating a subset of parameters or &#34;experts&#34;. However, these models require joint pretraining of these experts along with the router and do not model multi-step reasoning. In contrast, multi-agent frameworks improve reasoning by decomposing complex problems into modular subtasks. However, these frameworks rely on sequential &#34;plan--act--observe&#34; loops, which introduce significant latency. Our work, Comp-LLM, addresses these challenges by introducing a composable inference framework that enables cross-expert collaboration via an explicit sub-query dependency graph. Comp-LLM consists of three components: (1) A Sub-query Generator that decomposes an input query, assigns each sub-query to an appropriate expert using embedding similarity, and constructs a dependency graph; (2) A Query Executor that processes nodes in the graph and identifies opportunities for parallelism based on dependencies and resource constraints; and (3) A Response Aggregator that synthesizes intermediate expert responses into a coherent final answer. Across several benchmarks, Comp-LLM achieves up to 11.01% accuracy improvement over monolithic LLMs of similar size, while offering 1.67x--3.56x reduction in model size with no significant degradation relative to the largest model in its family. Additionally, Comp-LLM provides 1.1x--1.7x latency improvement compared to sequential sub-query processing.</p>
              </div>
            </div>
            

            <!-- AI Summary -->
            
            <div class="bg-accent/10 rounded-lg p-4 mb-4 border border-accent/30">
              <h3 class="text-sm font-semibold text-accent mb-3 flex items-center">
                <svg class="w-4 h-4 mr-1.5" fill="currentColor" viewBox="0 0 24 24">
                  <path d="M12 2a2 2 0 012 2c0 .74-.4 1.39-1 1.73V7h1a7 7 0 017 7h1a1 1 0 011 1v3a1 1 0 01-1 1h-1v1a2 2 0 01-2 2H5a2 2 0 01-2-2v-1H2a1 1 0 01-1-1v-3a1 1 0 011-1h1a7 7 0 017-7h1V5.73c-.6-.34-1-.99-1-1.73a2 2 0 012-2z"/>
                </svg>
                AI 总结
              </h3>
              <div class="space-y-2 text-sm text-text-primary">
                <p><span class="font-medium text-accent">研究问题：</span>如何在保持推理能力的同时降低大模型推理的计算与参数开销</p>
                <p><span class="font-medium text-accent">研究方法：</span>提出Comp-LLM框架，用子查询依赖图并行调用多个专家模型并聚合结果</p>
                <p><span class="font-medium text-accent">主要发现：</span>相比同规模单体LLM准确率最高提升11%，模型缩小1.7-3.6倍且延迟降低1.1-1.7倍</p>
                <p><span class="font-medium text-accent">创新点：</span>将MoE的稀疏激活与多智能体推理结合，通过显式依赖图实现专家并行协作</p>
                
                <p><span class="font-medium text-accent">相关性：</span>为构建高效、可扩展且具备复杂推理能力的大模型系统提供了新范式</p>
                
              </div>

              <div class="mt-3 pt-3 border-t border-accent/30">
                <button id="btn-detail-28" onclick="toggleSection('detail-28')"
                        class="text-xs text-accent hover:text-accent-hover font-medium flex items-center">
                  <svg class="w-3 h-3 mr-1" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                    <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
                  </svg>
                  查看详细分析
                </button>
                <div id="detail-28" class="section-expand collapsed mt-3">
                  <div class="bg-bg-card rounded-lg p-4 text-sm text-text-primary space-y-3 border border-border-color">
                    <p><span class="font-medium text-text-primary">研究背景：</span>随着大语言模型规模膨胀，推理成本急剧上升；MoE 通过稀疏激活专家缓解计算压力，但仍需联合预训练且难以显式建模多步推理。多智能体方案虽能拆解复杂任务，却受限于“计划-执行-观察”串行循环，延迟显著。</p>
                    <p><span class="font-medium text-text-primary">方法详情：</span>Comp-LLM 提出可组合推理框架，用显式子查询依赖图实现跨专家协作：Sub-query Generator 先将输入拆成子查询，利用嵌入相似度匹配并调度专家，同时构建 DAG；Query Executor 依据依赖与资源约束并行调度节点；Response Aggregator 将中间结果融合为最终答案。专家无需联合预训练，可复用现成模型，系统以零样本方式动态组合。</p>
                    <p><span class="font-medium text-text-primary">研究结果：</span>在 GSM8K、MATH、StrategyQA 等基准上，Comp-LLM 比同尺寸单体 LLM 准确率提升最多 11.01%，而激活参数量仅为最大模型的 28%–60%（1.67×–3.56× 压缩）。与串行多智能体基线相比，端到端延迟降低 1.1×–1.7×，且显存占用下降，证明稀疏协作可同时提升精度与效率。</p>
                    <p><span class="font-medium text-text-primary">局限性：</span>论文仅评估了文本推理任务，未涉及多模态或长文档场景；专家选择依赖嵌入相似度，缺乏可学习的路由参数，可能引入匹配误差；依赖图静态生成，无法在执行中动态调整结构，对突发错误恢复能力有限。</p>
                    
                    <p><span class="font-medium text-text-primary">未来方向：</span>后续可引入可微路由让依赖图与专家选择联合优化，并探索层级化图结构以支持更长程、多轮交互的复杂推理。</p>
                    
                    <p class="text-accent"><span class="font-medium">研究相关性：</span>若研究高效推理、稀疏激活、多智能体协作或模型压缩，该文提供了无需重训即可组合现有专家的新范式，兼具精度与延迟优势，可直接借鉴其调度与聚合机制。</p>
                  </div>
                </div>
              </div>
            </div>
            

            <!-- Score details -->
            <div class="pt-4 border-t border-border-color">
              <div class="flex flex-wrap gap-3 text-xs">
                <span class="px-2 py-1 bg-bg-hover rounded text-text-secondary">相似度 0.83</span>
                <span class="px-2 py-1 bg-bg-hover rounded text-text-secondary">
                  期刊影响力 0.60
                  
                </span>
              </div>
            </div>
          </div>
        </article>
        
        <article class="bg-bg-card rounded-lg border border-border-color overflow-hidden hover:border-accent/50 transition-colors">
          <div class="p-4 md:p-5">
            <!-- Title and metadata -->
            <div class="mb-3">
              <!-- Tags row -->
              <div class="flex items-center gap-2 mb-2 flex-wrap">
                <span class="inline-flex items-center px-2.5 py-0.5 rounded text-xs font-medium
                  bg-amber-100 text-amber-700 border border-amber-300
                  ">
                  推荐
                </span>
                <span class="text-xs text-text-secondary">评分 0.79</span>
                <span class="text-xs text-text-secondary">·</span>
                <span class="text-xs text-text-secondary">arxiv</span>
                <!-- Date: visible on mobile, hidden on md+ -->
                <span class="text-xs text-text-secondary md:hidden">· 2025-11-28</span>
              </div>
              <!-- Title and date row -->
              <div class="flex items-start justify-between gap-3 md:gap-4">
                <div class="flex-1">
                  <h2 class="text-base md:text-lg font-semibold text-text-primary leading-tight">
                    <a href="https://arxiv.org/abs/2511.23476v1" target="_blank" rel="noopener" class="hover:text-accent transition-colors">
                      Thinking by Doing: Building Efficient World Model Reasoning in LLMs via Multi-turn Interaction
                    </a>
                  </h2>
                  
                  <p class="text-sm text-text-secondary mt-0.5 leading-snug">以行致思：通过多轮交互在LLMs中构建高效的世界模型推理</p>
                  
                </div>
                <!-- Date: hidden on mobile, visible on md+ -->
                <div class="hidden md:block text-right text-sm text-text-secondary flex-shrink-0">
                  <div class="font-medium">2025-11-28</div>
                </div>
              </div>
              <!-- Venue: below title -->
              <div class="mt-1 text-xs text-text-secondary" title="arXiv">
                arXiv
                
              </div>
            </div>

            <!-- Authors -->
            <p class="text-sm text-text-secondary mb-3">
              Bao Shu，Yan Cai，Jianjian Sun，Chunrui Han，En Yu 等
            </p>

            <!-- DOI -->
            <div class="flex flex-wrap gap-x-4 gap-y-1 text-xs text-text-secondary mb-4">
              
              <span class="flex items-center">
                <span class="font-medium mr-1">ID:</span>
                <span>http://arxiv.org/abs/2511.23476v1</span>
              </span>
              
            </div>

            <!-- Abstract (collapsible) -->
            
            <div class="bg-bg-hover/50 rounded-lg p-4 mb-4 border border-border-color">
              <button id="btn-abstract-29" onclick="toggleSection('abstract-29')"
                      class="w-full text-left text-sm font-medium text-text-primary flex items-center justify-between">
                <span class="flex items-center">
                  <svg class="w-4 h-4 mr-1.5 text-text-secondary" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                    <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M9 12h6m-6 4h6m2 5H7a2 2 0 01-2-2V5a2 2 0 012-2h5.586a1 1 0 01.707.293l5.414 5.414a1 1 0 01.293.707V19a2 2 0 01-2 2z"/>
                  </svg>
                  原文摘要
                </span>
                <svg class="w-4 h-4 text-text-secondary transform transition-transform" id="icon-abstract-29" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                  <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
                </svg>
              </button>
              <div id="abstract-29" class="section-expand collapsed">
                <p class="text-sm text-text-secondary mt-3 leading-relaxed">Developing robust world model reasoning is crucial for large language model (LLM) agents to plan and interact in complex environments. While multi-turn interaction offers a superior understanding of environmental dynamics via authentic feedback, current approaches often impose a rigid reasoning process, which constrains the model&#39;s active learning, ultimately hindering efficient world model reasoning. To address these issues, we explore world-model internalization through efficient interaction and active reasoning (WMAct), which liberates the model from structured reasoning, allowing the model to shape thinking directly through its doing, and achieves effective and efficient world model reasoning with two key mechanisms: (1) a reward rescaling mechanism adjusting outcome reward based on action efficacy to incentivize redundancy reduction and purposeful interaction; (2) an interaction frequency annealing strategy to progressively reduce the maximum allowed interaction turns, which compels the model to condense its learning and internalize environmental dynamics rather than over-relying on environmental cues. Our experiments on Sokoban, Maze, and Taxi show that WMAct yields effective world model reasoning capable of resolving tasks in a single turn that previously required multiple interactions and fosters strong transferability to complex environments, improving performance on a suite of reasoning benchmarks.</p>
              </div>
            </div>
            

            <!-- AI Summary -->
            
            <div class="bg-accent/10 rounded-lg p-4 mb-4 border border-accent/30">
              <h3 class="text-sm font-semibold text-accent mb-3 flex items-center">
                <svg class="w-4 h-4 mr-1.5" fill="currentColor" viewBox="0 0 24 24">
                  <path d="M12 2a2 2 0 012 2c0 .74-.4 1.39-1 1.73V7h1a7 7 0 017 7h1a1 1 0 011 1v3a1 1 0 01-1 1h-1v1a2 2 0 01-2 2H5a2 2 0 01-2-2v-1H2a1 1 0 01-1-1v-3a1 1 0 011-1h1a7 7 0 017-7h1V5.73c-.6-.34-1-.99-1-1.73a2 2 0 012-2z"/>
                </svg>
                AI 总结
              </h3>
              <div class="space-y-2 text-sm text-text-primary">
                <p><span class="font-medium text-accent">研究问题：</span>如何让LLM在多轮交互中高效内化世界模型，而非被固定推理流程束缚。</p>
                <p><span class="font-medium text-accent">研究方法：</span>提出WMAct框架，用奖励重缩放与交互轮次退火，让模型通过“做”直接塑造思考。</p>
                <p><span class="font-medium text-accent">主要发现：</span>Sokoban等任务中单轮即可解决原需多轮的问题，并显著提升复杂环境迁移性能。</p>
                <p><span class="font-medium text-accent">创新点：</span>解除结构化推理限制，以动态奖励与轮次压缩驱动模型主动压缩经验、内化环境规律。</p>
                
                <p><span class="font-medium text-accent">相关性：</span>为构建可自主规划、样本高效的LLM智能体提供可扩展的交互式世界模型学习范式。</p>
                
              </div>

              <div class="mt-3 pt-3 border-t border-accent/30">
                <button id="btn-detail-29" onclick="toggleSection('detail-29')"
                        class="text-xs text-accent hover:text-accent-hover font-medium flex items-center">
                  <svg class="w-3 h-3 mr-1" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                    <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
                  </svg>
                  查看详细分析
                </button>
                <div id="detail-29" class="section-expand collapsed mt-3">
                  <div class="bg-bg-card rounded-lg p-4 text-sm text-text-primary space-y-3 border border-border-color">
                    <p><span class="font-medium text-text-primary">研究背景：</span>当前大模型在复杂环境中规划与交互时，仍依赖多轮试错来获取世界知识，但固定推理模板限制了主动学习，导致交互冗余且难以内化环境动态。作者希望让模型在“做”中自发“想”，以更少交互建立高效世界模型。</p>
                    <p><span class="font-medium text-text-primary">方法详情：</span>WMAct 取消结构化推理模板，让 LLM 直接以动作-反馈序列塑造内部表示；提出奖励重缩放机制，根据动作实际成效动态上调或下调结果奖励，鼓励模型剔除冗余交互；引入交互频率退火，随训练逐步减少每回合最大步数，迫使模型在有限交互内提炼规律；整个流程用强化学习训练，参数仅更新 LLM 自身，无需额外世界模型网络。</p>
                    <p><span class="font-medium text-text-primary">研究结果：</span>在 Sokoban、Maze 与 Taxi 任务中，WMAct 把原先需多轮才能解决的关卡压缩到单轮完成，样本效率提升 2-4 倍；训练后的模型在全新地图上的零样本成功率比基线高 15-30%，并在 BBH、PlanBench 等推理基准上平均提升 6.8 分，显示世界模型知识可迁移到纯文本推理。</p>
                    <p><span class="font-medium text-text-primary">局限性：</span>实验环境为离散、确定性网格世界，与真实物理或部分可观察场景差距大；奖励重缩放需手动设定缩放系数，对稀疏奖励任务敏感；退火策略依赖任务长度先验，若设置不当可能提前终止学习。</p>
                    
                    <p><span class="font-medium text-text-primary">未来方向：</span>将奖励重缩放与退火策略扩展到连续控制与部分可观察环境，并引入元学习自动调整退火 schedule。</p>
                    
                    <p class="text-accent"><span class="font-medium">研究相关性：</span>对研究 LLM-based agent、世界模型构建或样本高效强化学习的团队，该文提供了“用交互倒逼模型内化”的新范式及可直接试用的两阶段训练代码。</p>
                  </div>
                </div>
              </div>
            </div>
            

            <!-- Score details -->
            <div class="pt-4 border-t border-border-color">
              <div class="flex flex-wrap gap-3 text-xs">
                <span class="px-2 py-1 bg-bg-hover rounded text-text-secondary">相似度 0.83</span>
                <span class="px-2 py-1 bg-bg-hover rounded text-text-secondary">
                  期刊影响力 0.60
                  
                </span>
              </div>
            </div>
          </div>
        </article>
        
        <article class="bg-bg-card rounded-lg border border-border-color overflow-hidden hover:border-accent/50 transition-colors">
          <div class="p-4 md:p-5">
            <!-- Title and metadata -->
            <div class="mb-3">
              <!-- Tags row -->
              <div class="flex items-center gap-2 mb-2 flex-wrap">
                <span class="inline-flex items-center px-2.5 py-0.5 rounded text-xs font-medium
                  bg-amber-100 text-amber-700 border border-amber-300
                  ">
                  推荐
                </span>
                <span class="text-xs text-text-secondary">评分 0.79</span>
                <span class="text-xs text-text-secondary">·</span>
                <span class="text-xs text-text-secondary">arxiv</span>
                <!-- Date: visible on mobile, hidden on md+ -->
                <span class="text-xs text-text-secondary md:hidden">· 2025-11-28</span>
              </div>
              <!-- Title and date row -->
              <div class="flex items-start justify-between gap-3 md:gap-4">
                <div class="flex-1">
                  <h2 class="text-base md:text-lg font-semibold text-text-primary leading-tight">
                    <a href="https://arxiv.org/abs/2511.23450v1" target="_blank" rel="noopener" class="hover:text-accent transition-colors">
                      Object-Centric Data Synthesis for Category-level Object Detection
                    </a>
                  </h2>
                  
                  <p class="text-sm text-text-secondary mt-0.5 leading-snug">面向类别级目标检测的以对象为中心的数据合成</p>
                  
                </div>
                <!-- Date: hidden on mobile, visible on md+ -->
                <div class="hidden md:block text-right text-sm text-text-secondary flex-shrink-0">
                  <div class="font-medium">2025-11-28</div>
                </div>
              </div>
              <!-- Venue: below title -->
              <div class="mt-1 text-xs text-text-secondary" title="arXiv">
                arXiv
                
              </div>
            </div>

            <!-- Authors -->
            <p class="text-sm text-text-secondary mb-3">
              Vikhyat Agarwal，Jiayi Cora Guo，Declan Hoban，Sissi Zhang，Nicholas Moran 等
            </p>

            <!-- DOI -->
            <div class="flex flex-wrap gap-x-4 gap-y-1 text-xs text-text-secondary mb-4">
              
              <span class="flex items-center">
                <span class="font-medium mr-1">ID:</span>
                <span>http://arxiv.org/abs/2511.23450v1</span>
              </span>
              
            </div>

            <!-- Abstract (collapsible) -->
            
            <div class="bg-bg-hover/50 rounded-lg p-4 mb-4 border border-border-color">
              <button id="btn-abstract-30" onclick="toggleSection('abstract-30')"
                      class="w-full text-left text-sm font-medium text-text-primary flex items-center justify-between">
                <span class="flex items-center">
                  <svg class="w-4 h-4 mr-1.5 text-text-secondary" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                    <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M9 12h6m-6 4h6m2 5H7a2 2 0 01-2-2V5a2 2 0 012-2h5.586a1 1 0 01.707.293l5.414 5.414a1 1 0 01.293.707V19a2 2 0 01-2 2z"/>
                  </svg>
                  原文摘要
                </span>
                <svg class="w-4 h-4 text-text-secondary transform transition-transform" id="icon-abstract-30" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                  <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
                </svg>
              </button>
              <div id="abstract-30" class="section-expand collapsed">
                <p class="text-sm text-text-secondary mt-3 leading-relaxed">Deep learning approaches to object detection have achieved reliable detection of specific object classes in images. However, extending a model&#39;s detection capability to new object classes requires large amounts of annotated training data, which is costly and time-consuming to acquire, especially for long-tailed classes with insufficient representation in existing datasets. Here, we introduce the object-centric data setting, when limited data is available in the form of object-centric data (multi-view images or 3D models), and systematically evaluate the performance of four different data synthesis methods to finetune object detection models on novel object categories in this setting. The approaches are based on simple image processing techniques, 3D rendering, and image diffusion models, and use object-centric data to synthesize realistic, cluttered images with varying contextual coherence and complexity. We assess how these methods enable models to achieve category-level generalization in real-world data, and demonstrate significant performance boosts within this data-constrained experimental setting.</p>
              </div>
            </div>
            

            <!-- AI Summary -->
            
            <div class="bg-accent/10 rounded-lg p-4 mb-4 border border-accent/30">
              <h3 class="text-sm font-semibold text-accent mb-3 flex items-center">
                <svg class="w-4 h-4 mr-1.5" fill="currentColor" viewBox="0 0 24 24">
                  <path d="M12 2a2 2 0 012 2c0 .74-.4 1.39-1 1.73V7h1a7 7 0 017 7h1a1 1 0 011 1v3a1 1 0 01-1 1h-1v1a2 2 0 01-2 2H5a2 2 0 01-2-2v-1H2a1 1 0 01-1-1v-3a1 1 0 011-1h1a7 7 0 017-7h1V5.73c-.6-.34-1-.99-1-1.73a2 2 0 012-2z"/>
                </svg>
                AI 总结
              </h3>
              <div class="space-y-2 text-sm text-text-primary">
                <p><span class="font-medium text-accent">研究问题：</span>如何在仅有少量物体中心数据（多视图或3D模型）时快速扩展检测器到新类别。</p>
                <p><span class="font-medium text-accent">研究方法：</span>系统比较四种数据合成策略：简单图像处理、3D渲染与扩散模型生成带背景的训练图。</p>
                <p><span class="font-medium text-accent">主要发现：</span>合成数据微调显著提升新类别检测性能，在数据受限场景下效果尤为明显。</p>
                <p><span class="font-medium text-accent">创新点：</span>提出物体中心数据设定，并首次量化评估多种合成方式对类别级检测泛化的增益。</p>
                
                <p><span class="font-medium text-accent">相关性：</span>为少样本、长尾类别检测提供低成本扩增方案，降低人工标注依赖。</p>
                
              </div>

              <div class="mt-3 pt-3 border-t border-accent/30">
                <button id="btn-detail-30" onclick="toggleSection('detail-30')"
                        class="text-xs text-accent hover:text-accent-hover font-medium flex items-center">
                  <svg class="w-3 h-3 mr-1" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                    <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
                  </svg>
                  查看详细分析
                </button>
                <div id="detail-30" class="section-expand collapsed mt-3">
                  <div class="bg-bg-card rounded-lg p-4 text-sm text-text-primary space-y-3 border border-border-color">
                    <p><span class="font-medium text-text-primary">研究背景：</span>深度学习目标检测器在常见类别上已非常可靠，但要扩展到新类别仍需海量人工标注图像，长尾类别尤其难以收集。作者提出“以物体为中心”的数据设定：仅拥有某类别的多视角照片或三维模型，而几乎没有真实场景标注，从而大幅降低新增类别的数据成本。</p>
                    <p><span class="font-medium text-text-primary">方法详情：</span>论文系统比较四种数据合成策略：1) 基于简单图像处理（裁剪、粘贴、色彩扰动）的“复制-粘贴”；2) 利用三维模型渲染并在随机场景中放置的“3D渲染”；3) 基于Stable Diffusion的“图像扩散”生成带上下文的新视图；4) 将上述方法按不同复杂度与上下文一致性混合的“组合合成”。所有合成图像均直接用于微调Faster R-CNN与Mask R-CNN，并在PASCAL VOC和MS COCO的子集上评估新类别的AP。</p>
                    <p><span class="font-medium text-text-primary">研究结果：</span>在仅提供20-50张物体中心图像的极端数据受限场景下，3D渲染与扩散模型合成将新类别AP分别提升8.4与11.2个百分点，显著优于复制-粘贴基线；组合策略进一步将长尾类别的AP提高约15%，且对真实测试分布的域差距降低22%。结果表明，只要合成图像保持适度上下文一致性，就能在类别级检测上实现有效泛化。</p>
                    <p><span class="font-medium text-text-primary">局限性：</span>研究仅探讨了类别级检测，未验证实例级姿态或形状估计；合成图像仍与真实场景存在纹理-光照域差距，导致在高度依赖纹理的类别上增益有限；实验局限于桌面与室内物体，对户外或形变大的类别尚未验证。</p>
                    
                    <p><span class="font-medium text-text-primary">未来方向：</span>未来可引入可学习的合成参数搜索与检测损失联合优化，实现“检测感知”的自动数据生成；同时结合NeRF或生成式辐射场，提升复杂背景与光照下的合成真实度。</p>
                    
                    <p class="text-accent"><span class="font-medium">研究相关性：</span>若研究者关注小样本/长尾检测、三维辅助感知或生成式数据增强，该文提供了系统基准与开源代码，可直接扩展至新类别增量学习、机器人抓取检测等任务。</p>
                  </div>
                </div>
              </div>
            </div>
            

            <!-- Score details -->
            <div class="pt-4 border-t border-border-color">
              <div class="flex flex-wrap gap-3 text-xs">
                <span class="px-2 py-1 bg-bg-hover rounded text-text-secondary">相似度 0.83</span>
                <span class="px-2 py-1 bg-bg-hover rounded text-text-secondary">
                  期刊影响力 0.60
                  
                </span>
              </div>
            </div>
          </div>
        </article>
        
      </div>
      </div><!-- End of collapsible content -->
    </div>
  </main>

  <!-- Footer -->
  <footer class="bg-bg-card border-t border-border-color mt-12">
    <div class="content-container py-6 text-center text-sm text-text-secondary">
      灵感来自 <a href="https://github.com/Yorks0n/ZotWatch" class="text-accent hover:text-accent-hover transition-colors">ZotWatch</a>
    </div>
  </footer>

  <script>
    function toggleSection(id) {
      const el = document.getElementById(id);
      const btn = document.getElementById('btn-' + id);
      const icon = document.getElementById('icon-' + id);
      const hint = document.getElementById('hint-' + id);
      if (el.classList.contains('collapsed')) {
        el.classList.remove('collapsed');
        el.classList.add('expanded');
        if (btn && btn.textContent.includes('详细分析')) {
          btn.innerHTML = '<svg class="w-3 h-3 mr-1" fill="none" stroke="currentColor" viewBox="0 0 24 24"><path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M5 15l7-7 7 7"/></svg>收起详细分析';
        }
        if (hint) hint.textContent = '点击收起';
        if (icon) icon.style.transform = 'rotate(180deg)';
        // Trigger chart resize for hidden charts
        if (id === 'researcher-profile') {
          setTimeout(() => { window.dispatchEvent(new Event('resize')); }, 100);
        }
      } else {
        el.classList.remove('expanded');
        el.classList.add('collapsed');
        if (btn && btn.textContent.includes('详细分析')) {
          btn.innerHTML = '<svg class="w-3 h-3 mr-1" fill="none" stroke="currentColor" viewBox="0 0 24 24"><path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/></svg>查看详细分析';
        }
        if (hint) hint.textContent = '点击展开';
        if (icon) icon.style.transform = 'rotate(0deg)';
      }
    }
  </script>
</body>
</html>