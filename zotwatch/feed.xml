<?xml version='1.0' encoding='utf-8'?>
<rss xmlns:content="http://purl.org/rss/1.0/modules/content/" xmlns:dc="http://purl.org/dc/elements/1.1/" xmlns:prism="http://prismstandard.org/namespaces/basic/3.0/" version="2.0"><channel><title>ZotWatch Feed</title><link>https://ehehe.cn/zotwatch/</link><description>AI-assisted literature watch</description><lastBuildDate>Sat, 20 Dec 2025 01:39:41 +0000</lastBuildDate><item><title>SAR-NanoShipNet: A scale-adaptive network for robust small ship detection in SAR imagery</title><link>https://doi.org/10.1016/j.isprsjprs.2025.12.006</link><guid>10.1016/j.isprsjprs.2025.12.006</guid><pubDate>Fri, 19 Dec 2025 12:04:42 +0000</pubDate><dc:creator>Yuhao Zhang</dc:creator><dc:creator>Jieru Chi</dc:creator><dc:creator>Guowei Yang</dc:creator><dc:creator>Chenglizhao Chen</dc:creator><dc:creator>Teng Yu</dc:creator><prism:publicationName>ISPRS Journal of Photogrammetry and Remote Sensing</prism:publicationName><prism:doi>10.1016/j.isprsjprs.2025.12.006</prism:doi><description>Currently, notable progress has been attained in small ship target detection for synthetic aperture radar (SAR) imagery, with such advancements being driven by three key methodological innovations within the deep learning framework: self-supervision combined with knowledge distillation, rotated bounding box detection, and multi-scale feature fusion. However, it still faces challenges such as high speckle noise in SAR images, difficulty in extracting small target features, geometric distortion of ship shapes and heading dependence. Therefore, this article proposes a new SAR-NanoShipNet model. To enhance the targeting of ship objects, the proposed method employs a specialized convolution (DABConv) that exhibits greater suitability for ship targets, replacing the conventional standard convolution. As opposed to traditional approaches for SAR target detection, which typically lack the capability to adaptively capture the irregular boundaries and low-contrast features of small ship targets in SAR images, this method pioneers the adaptive capture of these features through deformable convolutions and boundary attention mechanisms, leading to enhanced target localization accuracy. In addition, we introduce the VerticalCompSPPF module (VC-SPPF), which incorporates longitudinal multi-scale convolution alongside a channel attention mechanism. Finally, the design of D-CLEM is linked with DABConv to enhance directional feature extraction while also fusing, improving the accuracy of small object detection. We have validated the superiority of our method on five datasets, particularly for high precision detection of small targets (AP s " role="presentation"&gt; s s &amp;#x2191; " role="presentation"&gt; ↑ ↑ 2.66%). Our code can be found at https://github.com/Z-Yuhao/1.git .
Published: 2025-12-19T12:04:42+00:00
Venue: ISPRS Journal of Photogrammetry and Remote Sensing
Score: 0.875 (must_read)</description><content:encoded>&lt;p&gt;&lt;strong&gt;Authors:&lt;/strong&gt; Yuhao Zhang; Jieru Chi; Guowei Yang; Chenglizhao Chen; Teng Yu&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Journal:&lt;/strong&gt; ISPRS Journal of Photogrammetry and Remote Sensing&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;a href="https://doi.org/10.1016/j.isprsjprs.2025.12.006"&gt;10.1016/j.isprsjprs.2025.12.006&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Relevance:&lt;/strong&gt; 0.875 (must_read)&lt;/p&gt;
&lt;p&gt;Currently, notable progress has been attained in small ship target detection for synthetic aperture radar (SAR) imagery, with such advancements being driven by three key methodological innovations within the deep learning framework: self-supervision combined with knowledge distillation, rotated bounding box detection, and multi-scale feature fusion. However, it still faces challenges such as high speckle noise in SAR images, difficulty in extracting small target features, geometric distortion of ship shapes and heading dependence. Therefore, this article proposes a new SAR-NanoShipNet model. To enhance the targeting of ship objects, the proposed method employs a specialized convolution (DABConv) that exhibits greater suitability for ship targets, replacing the conventional standard convolution. As opposed to traditional approaches for SAR target detection, which typically lack the capability to adaptively capture the irregular boundaries and low-contrast features of small ship targets in SAR images, this method pioneers the adaptive capture of these features through deformable convolutions and boundary attention mechanisms, leading to enhanced target localization accuracy. In addition, we introduce the VerticalCompSPPF module (VC-SPPF), which incorporates longitudinal multi-scale convolution alongside a channel attention mechanism. Finally, the design of D-CLEM is linked with DABConv to enhance directional feature extraction while also fusing, improving the accuracy of small object detection. We have validated the superiority of our method on five datasets, particularly for high precision detection of small targets (AP s &amp;quot; role=&amp;quot;presentation&amp;quot;&amp;gt; s s &amp;amp;#x2191; &amp;quot; role=&amp;quot;presentation&amp;quot;&amp;gt; ↑ ↑ 2.66%). Our code can be found at https://github.com/Z-Yuhao/1.git .&lt;/p&gt;</content:encoded></item><item><title>The CUR Decomposition of Self-Attention Matrices in Vision Transformers</title><link>https://doi.org/10.1109/tpami.2025.3646452</link><guid>10.1109/tpami.2025.3646452</guid><pubDate>Fri, 19 Dec 2025 18:58:30 +0000</pubDate><dc:creator>Chong Wu</dc:creator><dc:creator>Maolin Che</dc:creator><dc:creator>Hong Yan</dc:creator><prism:publicationName>IEEE Transactions on Pattern Analysis and Machine Intelligence</prism:publicationName><prism:doi>10.1109/tpami.2025.3646452</prism:doi><description>Transformers have achieved great success in natural language processing and computer vision. The core and basic technique of transformers is the self-attention mechanism. The vanilla self-attention mechanism has quadratic complexity, which limits its applications to vision tasks. Most of the existing linear self-attention mechanisms will sacrifice performance to some extent to reduce complexity. In this paper, we propose a novel linear approximation of the vanilla self-attention mechanism named CURSA to achieve both high performance and low complexity at the same time. CURSA is based on the CUR decomposition to decompose the multiplication of large matrices into the multiplication of several small matrices to achieve almost linear complexity. Experiment results of CURSA in image classification tasks, semantic segmentation tasks, object detection tasks, and long-range arena show that it outperforms state-of-the-art self-attention mechanisms with better data efficiency, faster speed, and higher accuracy.
Published: 2025-12-19T18:58:30+00:00
Venue: IEEE Transactions on Pattern Analysis and Machine Intelligence
Score: 0.834 (must_read)</description><content:encoded>&lt;p&gt;&lt;strong&gt;Authors:&lt;/strong&gt; Chong Wu; Maolin Che; Hong Yan&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Journal:&lt;/strong&gt; IEEE Transactions on Pattern Analysis and Machine Intelligence&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;a href="https://doi.org/10.1109/tpami.2025.3646452"&gt;10.1109/tpami.2025.3646452&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Relevance:&lt;/strong&gt; 0.834 (must_read)&lt;/p&gt;
&lt;p&gt;Transformers have achieved great success in natural language processing and computer vision. The core and basic technique of transformers is the self-attention mechanism. The vanilla self-attention mechanism has quadratic complexity, which limits its applications to vision tasks. Most of the existing linear self-attention mechanisms will sacrifice performance to some extent to reduce complexity. In this paper, we propose a novel linear approximation of the vanilla self-attention mechanism named CURSA to achieve both high performance and low complexity at the same time. CURSA is based on the CUR decomposition to decompose the multiplication of large matrices into the multiplication of several small matrices to achieve almost linear complexity. Experiment results of CURSA in image classification tasks, semantic segmentation tasks, object detection tasks, and long-range arena show that it outperforms state-of-the-art self-attention mechanisms with better data efficiency, faster speed, and higher accuracy.&lt;/p&gt;</content:encoded></item><item><title>DCCS-Det: Directional Context and Cross-Scale Aware Detector for Infrared Small Target</title><link>https://doi.org/10.1109/tgrs.2025.3646345</link><guid>10.1109/tgrs.2025.3646345</guid><pubDate>Fri, 19 Dec 2025 18:58:35 +0000</pubDate><dc:creator>Shuying Li</dc:creator><dc:creator>Qiang Ma</dc:creator><dc:creator>San Zhang</dc:creator><dc:creator>Chuang Yang</dc:creator><prism:publicationName>IEEE Transactions on Geoscience and Remote Sensing</prism:publicationName><prism:doi>10.1109/tgrs.2025.3646345</prism:doi><description>Infrared small target detection (IRSTD) is critical for applications like remote sensing and surveillance, which aims to identify small, low-contrast targets against complex backgrounds. However, existing methods often struggle with inadequate joint modeling of local-global features (harming target-background discrimination) or feature redundancy and semantic dilution (degrading target representation quality). To tackle these issues, we propose DCCS-Det (Directional Context and Cross-Scale Aware Detector for Infrared Small Target), a novel detector that incorporates a Dual-stream Saliency Enhancement (DSE) block and a Latent-aware Semantic Extraction and Aggregation (LaSEA) module. The DSE block integrates localized perception with direction-aware context aggregation to help capture long-range spatial dependencies and local details. On this basis, the LaSEA module mitigates feature degradation via cross-scale feature extraction and random pooling sampling strategies, enhancing discriminative features and suppressing noise. Extensive experiments show that DCCS-Det achieves state-of-the-art detection accuracy with competitive efficiency across multiple datasets. Ablation studies further validate the contributions of DSE and LaSEA in improving target perception and feature representation under complex scenarios. Code is available at https://github.com/ML202010/DCCS-Det.
Published: 2025-12-19T18:58:35+00:00
Venue: IEEE Transactions on Geoscience and Remote Sensing
Score: 0.826 (must_read)</description><content:encoded>&lt;p&gt;&lt;strong&gt;Authors:&lt;/strong&gt; Shuying Li; Qiang Ma; San Zhang; Chuang Yang&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Journal:&lt;/strong&gt; IEEE Transactions on Geoscience and Remote Sensing&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;a href="https://doi.org/10.1109/tgrs.2025.3646345"&gt;10.1109/tgrs.2025.3646345&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Relevance:&lt;/strong&gt; 0.826 (must_read)&lt;/p&gt;
&lt;p&gt;Infrared small target detection (IRSTD) is critical for applications like remote sensing and surveillance, which aims to identify small, low-contrast targets against complex backgrounds. However, existing methods often struggle with inadequate joint modeling of local-global features (harming target-background discrimination) or feature redundancy and semantic dilution (degrading target representation quality). To tackle these issues, we propose DCCS-Det (Directional Context and Cross-Scale Aware Detector for Infrared Small Target), a novel detector that incorporates a Dual-stream Saliency Enhancement (DSE) block and a Latent-aware Semantic Extraction and Aggregation (LaSEA) module. The DSE block integrates localized perception with direction-aware context aggregation to help capture long-range spatial dependencies and local details. On this basis, the LaSEA module mitigates feature degradation via cross-scale feature extraction and random pooling sampling strategies, enhancing discriminative features and suppressing noise. Extensive experiments show that DCCS-Det achieves state-of-the-art detection accuracy with competitive efficiency across multiple datasets. Ablation studies further validate the contributions of DSE and LaSEA in improving target perception and feature representation under complex scenarios. Code is available at https://github.com/ML202010/DCCS-Det.&lt;/p&gt;</content:encoded></item><item><title>Two-Stage SAR Image Generation Based on Attribute Feature Decoupling</title><link>https://doi.org/10.1109/lgrs.2025.3645620</link><guid>10.1109/lgrs.2025.3645620</guid><pubDate>Thu, 18 Dec 2025 18:36:05 +0000</pubDate><dc:creator>Rubo Jin</dc:creator><dc:creator>Wei Wang</dc:creator><dc:creator>Jianda Cheng</dc:creator><dc:creator>Hui Fan</dc:creator><dc:creator>Jiyuan Liu</dc:creator><dc:creator>Hongqi Fan</dc:creator><prism:publicationName>IEEE Geoscience and Remote Sensing Letters</prism:publicationName><prism:doi>10.1109/lgrs.2025.3645620</prism:doi><description>Training of synthetic aperture radar (SAR) target detection and recognition methods based on deep learning heavily relies on a large amount of data. As one of the significant approaches to address the scarcity of SAR data, SAR image intelligent generation methods have witnessed rapid development. However, these methods often require many data samples for learning and are prone to deviating from the physical scattering characteristics. To address these issues, this paper proposes a two-stage SAR image generation method based on attribute feature decoupling within a generative adversarial network (GAN) architecture. In the first stage, the original SAR target image undergoes feature extraction and reconstruction, yielding generated images highly similar to real images. The attribute features decoupled during this process correlate with the scattering characteristics of SAR target, providing guiding information for generating target images in the second stage. In the second stage, by applying perturbations to specific dimensions of the decoupled features, we can reconstruct target images with altered attributes, achieving diverse data augmentation. Multi-task discrimination based on pixel intensity, authenticity, and feature distance differences enhances the quality of generated images across multiple levels. The decoupled representation-driven generation paradigm simplifies the network’s mapping learning task through task decomposition, diminishing the dependency on the volume of data. The experimental results demonstrate that the generated images possess higher quality and superior application performance, with an improvement of 5.23% in recognition accuracy.
Published: 2025-12-18T18:36:05+00:00
Venue: IEEE Geoscience and Remote Sensing Letters
Score: 0.822 (must_read)</description><content:encoded>&lt;p&gt;&lt;strong&gt;Authors:&lt;/strong&gt; Rubo Jin; Wei Wang; Jianda Cheng; Hui Fan; Jiyuan Liu; Hongqi Fan&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Journal:&lt;/strong&gt; IEEE Geoscience and Remote Sensing Letters&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;a href="https://doi.org/10.1109/lgrs.2025.3645620"&gt;10.1109/lgrs.2025.3645620&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Relevance:&lt;/strong&gt; 0.822 (must_read)&lt;/p&gt;
&lt;p&gt;Training of synthetic aperture radar (SAR) target detection and recognition methods based on deep learning heavily relies on a large amount of data. As one of the significant approaches to address the scarcity of SAR data, SAR image intelligent generation methods have witnessed rapid development. However, these methods often require many data samples for learning and are prone to deviating from the physical scattering characteristics. To address these issues, this paper proposes a two-stage SAR image generation method based on attribute feature decoupling within a generative adversarial network (GAN) architecture. In the first stage, the original SAR target image undergoes feature extraction and reconstruction, yielding generated images highly similar to real images. The attribute features decoupled during this process correlate with the scattering characteristics of SAR target, providing guiding information for generating target images in the second stage. In the second stage, by applying perturbations to specific dimensions of the decoupled features, we can reconstruct target images with altered attributes, achieving diverse data augmentation. Multi-task discrimination based on pixel intensity, authenticity, and feature distance differences enhances the quality of generated images across multiple levels. The decoupled representation-driven generation paradigm simplifies the network’s mapping learning task through task decomposition, diminishing the dependency on the volume of data. The experimental results demonstrate that the generated images possess higher quality and superior application performance, with an improvement of 5.23% in recognition accuracy.&lt;/p&gt;</content:encoded></item><item><title>Context-Aware and Semantic-Guided Adaptive Filtering Network for Infrared Small Target Detection</title><link>https://doi.org/10.1109/tgrs.2025.3646255</link><guid>10.1109/tgrs.2025.3646255</guid><pubDate>Fri, 19 Dec 2025 18:58:35 +0000</pubDate><dc:creator>Lingchuan Kong</dc:creator><dc:creator>Bo Yang</dc:creator><dc:creator>Rui Chang</dc:creator><dc:creator>Jun Luo</dc:creator><dc:creator>Huayan Pu</dc:creator><dc:creator>Yangjun Pi</dc:creator><prism:publicationName>IEEE Transactions on Geoscience and Remote Sensing</prism:publicationName><prism:doi>10.1109/tgrs.2025.3646255</prism:doi><description>Infrared small target detection (ISTD) is a crucial task to identify tiny targets from infrared images. Although existing hybrid CNN-Transformer methods achieve excellent segmentation performance, they still face some challenges. First, the self-attention mechanism is insensitive to subtle local variations and incurs high computational cost; second, during feature fusion these methods fail to fully exploit the key information contained in shallow features. Consequently, they struggle to distinguish targets from backgrounds efficiently and accurately in scenes where the two are highly similar. To address these issues, this paper proposes CSAFNet to enhances the discriminability of targets and backgrounds. Specifically, we introduce Parallel Self-Awareness Attention (PSAA), which leverages physical priors to capture global context and incorporates wavelet transforms to strengthen local detail, achieving efficient fusion of local and global features. Considering the importance of shallow features for precise localization and fine segmentation, we design cross-semantic adaptive filtering module (CAFM) in feature fusion, which deeply explores key information from shallow features and enhances the relative saliency of target representations. Moreover, we propose the dynamic multi-scale spatial pyramid (DMSSP) module to improve edge precision and enhance segmentation accuracy. Extensive experiments on the two most widely used ISTD datasets, NUAA-SIRST and IRSTD-1K, show that CSAFNet outperforms other state-of-the-art methods. The code is available at https://github.com/LingchuanK/CSAFNet.
Published: 2025-12-19T18:58:35+00:00
Venue: IEEE Transactions on Geoscience and Remote Sensing
Score: 0.822 (must_read)</description><content:encoded>&lt;p&gt;&lt;strong&gt;Authors:&lt;/strong&gt; Lingchuan Kong; Bo Yang; Rui Chang; Jun Luo; Huayan Pu; Yangjun Pi&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Journal:&lt;/strong&gt; IEEE Transactions on Geoscience and Remote Sensing&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;a href="https://doi.org/10.1109/tgrs.2025.3646255"&gt;10.1109/tgrs.2025.3646255&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Relevance:&lt;/strong&gt; 0.822 (must_read)&lt;/p&gt;
&lt;p&gt;Infrared small target detection (ISTD) is a crucial task to identify tiny targets from infrared images. Although existing hybrid CNN-Transformer methods achieve excellent segmentation performance, they still face some challenges. First, the self-attention mechanism is insensitive to subtle local variations and incurs high computational cost; second, during feature fusion these methods fail to fully exploit the key information contained in shallow features. Consequently, they struggle to distinguish targets from backgrounds efficiently and accurately in scenes where the two are highly similar. To address these issues, this paper proposes CSAFNet to enhances the discriminability of targets and backgrounds. Specifically, we introduce Parallel Self-Awareness Attention (PSAA), which leverages physical priors to capture global context and incorporates wavelet transforms to strengthen local detail, achieving efficient fusion of local and global features. Considering the importance of shallow features for precise localization and fine segmentation, we design cross-semantic adaptive filtering module (CAFM) in feature fusion, which deeply explores key information from shallow features and enhances the relative saliency of target representations. Moreover, we propose the dynamic multi-scale spatial pyramid (DMSSP) module to improve edge precision and enhance segmentation accuracy. Extensive experiments on the two most widely used ISTD datasets, NUAA-SIRST and IRSTD-1K, show that CSAFNet outperforms other state-of-the-art methods. The code is available at https://github.com/LingchuanK/CSAFNet.&lt;/p&gt;</content:encoded></item><item><title>Controllable Generation with Text-to-Image Diffusion Models: a Survey</title><link>https://doi.org/10.1109/tpami.2025.3646548</link><guid>10.1109/tpami.2025.3646548</guid><pubDate>Fri, 19 Dec 2025 18:58:30 +0000</pubDate><dc:creator>Pu Cao</dc:creator><dc:creator>Feng Zhou</dc:creator><dc:creator>Qing Song</dc:creator><dc:creator>Lu Yang</dc:creator><prism:publicationName>IEEE Transactions on Pattern Analysis and Machine Intelligence</prism:publicationName><prism:doi>10.1109/tpami.2025.3646548</prism:doi><description>In the rapidly advancing realm of visual generation, diffusion models have revolutionized the landscape, marking a significant shift in capabilities with their impressive text-guided generative functions. However, relying solely on text for conditioning these models does not fully cater to the varied and complex requirements of different applications and scenarios. Acknowledging this shortfall, a variety of studies aim to control pre-trained text-to-image (T2I) models to support novel conditions. In this survey, we undertake a thorough review of the literature on controllable generation with T2I diffusion models, covering both the theoretical foundations and practical advancements in this domain. Our review begins with a brief introduction to the basics of denoising diffusion probabilistic models (DDPMs) and widely used T2I diffusion models. Additionally, we provide a detailed overview of research in this area, categorizing it from the condition perspective into three directions: generation with specific conditions, generation with multiple conditions, and universal controllable generation. For each category, we analyze the underlying control mechanisms and review representative methods based on their core techniques. For an exhaustive list of the controllable generation literature surveyed, please refer to our curated repository at https://github.com/PRIV-Creation/Awesome-Controllable-T2I-Diffusion-Models.
Published: 2025-12-19T18:58:30+00:00
Venue: IEEE Transactions on Pattern Analysis and Machine Intelligence
Score: 0.821 (must_read)</description><content:encoded>&lt;p&gt;&lt;strong&gt;Authors:&lt;/strong&gt; Pu Cao; Feng Zhou; Qing Song; Lu Yang&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Journal:&lt;/strong&gt; IEEE Transactions on Pattern Analysis and Machine Intelligence&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;a href="https://doi.org/10.1109/tpami.2025.3646548"&gt;10.1109/tpami.2025.3646548&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Relevance:&lt;/strong&gt; 0.821 (must_read)&lt;/p&gt;
&lt;p&gt;In the rapidly advancing realm of visual generation, diffusion models have revolutionized the landscape, marking a significant shift in capabilities with their impressive text-guided generative functions. However, relying solely on text for conditioning these models does not fully cater to the varied and complex requirements of different applications and scenarios. Acknowledging this shortfall, a variety of studies aim to control pre-trained text-to-image (T2I) models to support novel conditions. In this survey, we undertake a thorough review of the literature on controllable generation with T2I diffusion models, covering both the theoretical foundations and practical advancements in this domain. Our review begins with a brief introduction to the basics of denoising diffusion probabilistic models (DDPMs) and widely used T2I diffusion models. Additionally, we provide a detailed overview of research in this area, categorizing it from the condition perspective into three directions: generation with specific conditions, generation with multiple conditions, and universal controllable generation. For each category, we analyze the underlying control mechanisms and review representative methods based on their core techniques. For an exhaustive list of the controllable generation literature surveyed, please refer to our curated repository at https://github.com/PRIV-Creation/Awesome-Controllable-T2I-Diffusion-Models.&lt;/p&gt;</content:encoded></item><item><title>DSwinIR: Rethinking Window-Based Attention for Image Restoration</title><link>https://doi.org/10.1109/tpami.2025.3646016</link><guid>10.1109/tpami.2025.3646016</guid><pubDate>Fri, 19 Dec 2025 18:58:30 +0000</pubDate><dc:creator>Gang Wu</dc:creator><dc:creator>Junjun Jiang</dc:creator><dc:creator>Kui Jiang</dc:creator><dc:creator>Xianming Liu</dc:creator><dc:creator>Liqiang Nie</dc:creator><prism:publicationName>IEEE Transactions on Pattern Analysis and Machine Intelligence</prism:publicationName><prism:doi>10.1109/tpami.2025.3646016</prism:doi><description>Image restoration has witnessed significant advancements with the development of deep learning models. Transformer-based models, particularly those using window-based self-attention, have become a dominant force. However, their performance is constrained by the rigid, non-overlapping window partitioning scheme, which leads to insufficient feature interaction across windows and limited receptive fields. This highlights the need for more adaptive and flexible attention mechanisms. In this paper, we propose the Deformable Sliding Window Transformer for Image Restoration (DSwinIR), a new attention mechanism: the Deformable Sliding Window (DSwin) Attention. This mechanism introduces a token-centric and content-aware paradigm that moves beyond the grid and fixed window partition. It comprises two complementary components. First, it replaces the rigid partitioning with a token-centric sliding window paradigm, making it effective at eliminating boundary artifacts. Second, it incorporates a content-aware deformable sampling strategy, which allows the attention mechanism to learn data-dependent offsets and actively shape its receptive field to focus on the most informative image regions. Extensive experiments show that DSwinIR achieves strong results, including stateoftheart performance on several evaluated benchmarks. For instance, in all-in-one image restoration, our DSwinIR surpasses the most recent backbone GridFormer by 0.53 dB on the three-task benchmark and 0.87 dB on the five-task benchmark. The code and pre-trained models are available at https://github.com/Aitical/DSwinIR.
Published: 2025-12-19T18:58:30+00:00
Venue: IEEE Transactions on Pattern Analysis and Machine Intelligence
Score: 0.817 (must_read)</description><content:encoded>&lt;p&gt;&lt;strong&gt;Authors:&lt;/strong&gt; Gang Wu; Junjun Jiang; Kui Jiang; Xianming Liu; Liqiang Nie&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Journal:&lt;/strong&gt; IEEE Transactions on Pattern Analysis and Machine Intelligence&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;a href="https://doi.org/10.1109/tpami.2025.3646016"&gt;10.1109/tpami.2025.3646016&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Relevance:&lt;/strong&gt; 0.817 (must_read)&lt;/p&gt;
&lt;p&gt;Image restoration has witnessed significant advancements with the development of deep learning models. Transformer-based models, particularly those using window-based self-attention, have become a dominant force. However, their performance is constrained by the rigid, non-overlapping window partitioning scheme, which leads to insufficient feature interaction across windows and limited receptive fields. This highlights the need for more adaptive and flexible attention mechanisms. In this paper, we propose the Deformable Sliding Window Transformer for Image Restoration (DSwinIR), a new attention mechanism: the Deformable Sliding Window (DSwin) Attention. This mechanism introduces a token-centric and content-aware paradigm that moves beyond the grid and fixed window partition. It comprises two complementary components. First, it replaces the rigid partitioning with a token-centric sliding window paradigm, making it effective at eliminating boundary artifacts. Second, it incorporates a content-aware deformable sampling strategy, which allows the attention mechanism to learn data-dependent offsets and actively shape its receptive field to focus on the most informative image regions. Extensive experiments show that DSwinIR achieves strong results, including stateoftheart performance on several evaluated benchmarks. For instance, in all-in-one image restoration, our DSwinIR surpasses the most recent backbone GridFormer by 0.53 dB on the three-task benchmark and 0.87 dB on the five-task benchmark. The code and pre-trained models are available at https://github.com/Aitical/DSwinIR.&lt;/p&gt;</content:encoded></item><item><title>Efficient Scene Modeling Via Structure-Aware and Region-Prioritized 3D Gaussians</title><link>https://doi.org/10.1109/tpami.2025.3646473</link><guid>10.1109/tpami.2025.3646473</guid><pubDate>Fri, 19 Dec 2025 18:58:30 +0000</pubDate><dc:creator>Guangchi Fang</dc:creator><dc:creator>Bing Wang</dc:creator><prism:publicationName>IEEE Transactions on Pattern Analysis and Machine Intelligence</prism:publicationName><prism:doi>10.1109/tpami.2025.3646473</prism:doi><description>Reconstructing 3D scenes with high fidelity and efficiency remains a central pursuit in computer vision and graphics. Recent advances in 3D Gaussian Splatting (3DGS) enable photorealistic rendering with Gaussian primitives, yet the modeling process remains governed predominantly by photometric supervision. This reliance often leads to irregular spatial distribution and indiscriminate primitive adjustments that largely ignore underlying geometric context. In this work, we rethink Gaussian modeling from a geometric standpoint and introduce Mini-Splatting2, an efficient scene modeling framework that couples structure-aware distribution and region-prioritized optimization, driving 3DGS into a geometry-regulated paradigm. The structure-aware distribution enforces spatial regularity through structured reorganization and representation sparsity, ensuring balanced structural coverage for compact organization. The region-prioritized optimization improves training discrimination through geometric saliency and computational selectivity, fostering appropriate structural emergence for fast convergence. These mechanisms alleviate the long-standing tension among representation compactness, convergence acceleration, and rendering fidelity. Extensive experiments demonstrate that Mini-Splatting2 achieves up to 4× fewer Gaussians and 3× faster optimization while maintaining state-of-the-art visual quality, paving the way towards structured and efficient 3D Gaussian modeling.
Published: 2025-12-19T18:58:30+00:00
Venue: IEEE Transactions on Pattern Analysis and Machine Intelligence
Score: 0.812 (must_read)</description><content:encoded>&lt;p&gt;&lt;strong&gt;Authors:&lt;/strong&gt; Guangchi Fang; Bing Wang&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Journal:&lt;/strong&gt; IEEE Transactions on Pattern Analysis and Machine Intelligence&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;a href="https://doi.org/10.1109/tpami.2025.3646473"&gt;10.1109/tpami.2025.3646473&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Relevance:&lt;/strong&gt; 0.812 (must_read)&lt;/p&gt;
&lt;p&gt;Reconstructing 3D scenes with high fidelity and efficiency remains a central pursuit in computer vision and graphics. Recent advances in 3D Gaussian Splatting (3DGS) enable photorealistic rendering with Gaussian primitives, yet the modeling process remains governed predominantly by photometric supervision. This reliance often leads to irregular spatial distribution and indiscriminate primitive adjustments that largely ignore underlying geometric context. In this work, we rethink Gaussian modeling from a geometric standpoint and introduce Mini-Splatting2, an efficient scene modeling framework that couples structure-aware distribution and region-prioritized optimization, driving 3DGS into a geometry-regulated paradigm. The structure-aware distribution enforces spatial regularity through structured reorganization and representation sparsity, ensuring balanced structural coverage for compact organization. The region-prioritized optimization improves training discrimination through geometric saliency and computational selectivity, fostering appropriate structural emergence for fast convergence. These mechanisms alleviate the long-standing tension among representation compactness, convergence acceleration, and rendering fidelity. Extensive experiments demonstrate that Mini-Splatting2 achieves up to 4× fewer Gaussians and 3× faster optimization while maintaining state-of-the-art visual quality, paving the way towards structured and efficient 3D Gaussian modeling.&lt;/p&gt;</content:encoded></item><item><title>Few-shot object detection via semantic prompts and classifier decoupling</title><link>https://doi.org/10.1016/j.neunet.2025.108488</link><guid>10.1016/j.neunet.2025.108488</guid><pubDate>Fri, 19 Dec 2025 07:49:43 +0000</pubDate><dc:creator>Baifan Chen</dc:creator><dc:creator>Ruyi Zhu</dc:creator><dc:creator>Yilan Li</dc:creator><prism:publicationName>Neural Networks</prism:publicationName><prism:doi>10.1016/j.neunet.2025.108488</prism:doi><description>Many existing few-shot object detection methods employ two-stage detectors to achieve higher accuracy. Given the limited feature information and the challenges of adapting two-stage object detectors to few-shot learning, this paper proposes a few-shot object detection method based on semantic prompts and classifier decoupling. The key to incorporating textual information into object detectors lies in the effective fusion and alignment of image and text features. This paper introduces a Semantic Prompts module, enhancing the features of few-shot learning while aiding the model in better understanding image content. Leveraging the functionalities of the components of two-stage object detectors and their inter-component interactions, Gradient Scaling is employed to attenuate parameter updates, mitigating negative inter-module influences. To address the inconsistent feature demands between classification and regression branches, a Classifier Decoupling module is utilized to achieve more accurate classification and localization effects. Experimental evaluations on benchmark datasets demonstrate that the proposed method outperforms strong baselines such as DeFRCN by up to 3.15% mAP under 1-shot settings on PASCAL VOC. These improvements indicate enhanced generalization and robustness in low-data regimes.
Published: 2025-12-19T07:49:43+00:00
Venue: Neural Networks
Score: 0.810 (must_read)</description><content:encoded>&lt;p&gt;&lt;strong&gt;Authors:&lt;/strong&gt; Baifan Chen; Ruyi Zhu; Yilan Li&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Journal:&lt;/strong&gt; Neural Networks&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;a href="https://doi.org/10.1016/j.neunet.2025.108488"&gt;10.1016/j.neunet.2025.108488&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Relevance:&lt;/strong&gt; 0.810 (must_read)&lt;/p&gt;
&lt;p&gt;Many existing few-shot object detection methods employ two-stage detectors to achieve higher accuracy. Given the limited feature information and the challenges of adapting two-stage object detectors to few-shot learning, this paper proposes a few-shot object detection method based on semantic prompts and classifier decoupling. The key to incorporating textual information into object detectors lies in the effective fusion and alignment of image and text features. This paper introduces a Semantic Prompts module, enhancing the features of few-shot learning while aiding the model in better understanding image content. Leveraging the functionalities of the components of two-stage object detectors and their inter-component interactions, Gradient Scaling is employed to attenuate parameter updates, mitigating negative inter-module influences. To address the inconsistent feature demands between classification and regression branches, a Classifier Decoupling module is utilized to achieve more accurate classification and localization effects. Experimental evaluations on benchmark datasets demonstrate that the proposed method outperforms strong baselines such as DeFRCN by up to 3.15% mAP under 1-shot settings on PASCAL VOC. These improvements indicate enhanced generalization and robustness in low-data regimes.&lt;/p&gt;</content:encoded></item><item><title>Noisy Correspondence Rectification in Multimodal Clustering Space for Cross-Modal Matching</title><link>https://doi.org/10.1109/tpami.2025.3646184</link><guid>10.1109/tpami.2025.3646184</guid><pubDate>Fri, 19 Dec 2025 18:58:30 +0000</pubDate><dc:creator>Shuo Yang</dc:creator><dc:creator>Yancheng Long</dc:creator><dc:creator>Yujie Wei</dc:creator><dc:creator>Zeke Xie</dc:creator><dc:creator>Hongxun Yao</dc:creator><dc:creator>Min Xu</dc:creator><dc:creator>Liqiang Nie</dc:creator><prism:publicationName>IEEE Transactions on Pattern Analysis and Machine Intelligence</prism:publicationName><prism:doi>10.1109/tpami.2025.3646184</prism:doi><description>As one of the most fundamental techniques in multimodal learning, cross-modal matching aims to project various sensory modalities into a shared feature space. To achieve this, massive and correctly aligned data pairs are required for model training. However, unlike unimodal datasets, multimodal datasets are extremely harder to collect and annotate precisely. As an alternative, the co-occurred data pairs (e.g., image-text pairs) collected from the Internet have been widely exploited in the area. Unfortunately, the cheaply collected dataset unavoidably contains many mismatched data pairs, which have been proven to be harmful to the model's performance. To address this, we propose BiCro++ (Improved Bidirectional Cross-modal Similarity Consistency). This module can be integrated into existing cross-modal matching models, enhancing their robustness against noisy data through self-adaptive soft labels that dynamically reflect the true correspondence of data pairs. The basic idea of BiCro++ is motivated by that taking image-text matching as an example similar images should have similar textual descriptions and vice versa. This bidirectional similarity consistency can be directly translated into soft labels as a self-supervision signal to train the matching model. To further refine soft label quality, BiCro++ first introduces a Diagonal-Dominance Purification process to identify reliable anchor points from noisy dataset as the reference for soft label estimation. Then it employs a Hybrid-level Codebook Alignment mechanism that establishes enhanced consistency in bidirectional cross-modal similarity. The experiments on three popular cross-modal matching datasets show that our method significantly improves the noise-robustness of various matching models, and surpasses the state-of-the-art method by an average of 5.3%, 3.1% and 6.4% in terms of recall, respectively.
Published: 2025-12-19T18:58:30+00:00
Venue: IEEE Transactions on Pattern Analysis and Machine Intelligence
Score: 0.808 (must_read)</description><content:encoded>&lt;p&gt;&lt;strong&gt;Authors:&lt;/strong&gt; Shuo Yang; Yancheng Long; Yujie Wei; Zeke Xie; Hongxun Yao; Min Xu; Liqiang Nie&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Journal:&lt;/strong&gt; IEEE Transactions on Pattern Analysis and Machine Intelligence&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;a href="https://doi.org/10.1109/tpami.2025.3646184"&gt;10.1109/tpami.2025.3646184&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Relevance:&lt;/strong&gt; 0.808 (must_read)&lt;/p&gt;
&lt;p&gt;As one of the most fundamental techniques in multimodal learning, cross-modal matching aims to project various sensory modalities into a shared feature space. To achieve this, massive and correctly aligned data pairs are required for model training. However, unlike unimodal datasets, multimodal datasets are extremely harder to collect and annotate precisely. As an alternative, the co-occurred data pairs (e.g., image-text pairs) collected from the Internet have been widely exploited in the area. Unfortunately, the cheaply collected dataset unavoidably contains many mismatched data pairs, which have been proven to be harmful to the model&amp;#x27;s performance. To address this, we propose BiCro++ (Improved Bidirectional Cross-modal Similarity Consistency). This module can be integrated into existing cross-modal matching models, enhancing their robustness against noisy data through self-adaptive soft labels that dynamically reflect the true correspondence of data pairs. The basic idea of BiCro++ is motivated by that taking image-text matching as an example similar images should have similar textual descriptions and vice versa. This bidirectional similarity consistency can be directly translated into soft labels as a self-supervision signal to train the matching model. To further refine soft label quality, BiCro++ first introduces a Diagonal-Dominance Purification process to identify reliable anchor points from noisy dataset as the reference for soft label estimation. Then it employs a Hybrid-level Codebook Alignment mechanism that establishes enhanced consistency in bidirectional cross-modal similarity. The experiments on three popular cross-modal matching datasets show that our method significantly improves the noise-robustness of various matching models, and surpasses the state-of-the-art method by an average of 5.3%, 3.1% and 6.4% in terms of recall, respectively.&lt;/p&gt;</content:encoded></item><item><title>Hi-Mamba: Hierarchical Mamba for Efficient Image Super-Resolution</title><link>https://doi.org/10.1109/tip.2025.3643146</link><guid>10.1109/tip.2025.3643146</guid><pubDate>Thu, 18 Dec 2025 18:35:49 +0000</pubDate><dc:creator>Junbo Qiao</dc:creator><dc:creator>Jincheng Liao</dc:creator><dc:creator>Wei Li</dc:creator><dc:creator>Yulun Zhang</dc:creator><dc:creator>Yong Guo</dc:creator><dc:creator>Jiao Xie</dc:creator><dc:creator>Jie Hu</dc:creator><dc:creator>Shaohui Lin</dc:creator><prism:publicationName>IEEE Transactions on Image Processing</prism:publicationName><prism:doi>10.1109/tip.2025.3643146</prism:doi><description>Despite Transformers have achieved significant success in low-level vision tasks, they are constrained by computing self-attention with a quadratic complexity and limited-size windows. This limitation results in a lack of global receptive field across the entire image. Recently, State Space Models (SSMs) have gained widespread attention due to their global receptive field and linear complexity with respect to input length. However, integrating SSMs into low-level vision tasks presents two major challenges: (1) Relationship degradation of long-range tokens with a long-range forgetting problem by encoding pixel-by-pixel high-resolution images. (2) Significant redundancy in the existing multi-direction scanning strategy. To this end, we propose Hi-Mamba for image super-resolution (SR) to address these challenges, which unfolds the image with only a single scan. Specifically, the Global Hierarchical Mamba Block (GHMB) enables token interactions across the entire image, providing a global receptive field while leveraging a multi-scale structure to facilitate long-range dependency learning. Additionally, the Direction Alternation Module (DAM) adjusts the scanning patterns of GHMB across different layers to enhance spatial relationship modeling. Extensive experiments demonstrate that our Hi-Mamba achieves 0.2-0.27dB PSNR gains on the Urban100 dataset across different scaling factors compared to the state-of-the-art MambaIRv2 for SR. Moreover, our lightweight Hi-Mamba also outperforms lightweight SRFormer by 0.39dB PSNR for ×2 SR.
Published: 2025-12-18T18:35:49+00:00
Venue: IEEE Transactions on Image Processing
Score: 0.807 (must_read)</description><content:encoded>&lt;p&gt;&lt;strong&gt;Authors:&lt;/strong&gt; Junbo Qiao; Jincheng Liao; Wei Li; Yulun Zhang; Yong Guo; Jiao Xie; Jie Hu; Shaohui Lin&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Journal:&lt;/strong&gt; IEEE Transactions on Image Processing&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;a href="https://doi.org/10.1109/tip.2025.3643146"&gt;10.1109/tip.2025.3643146&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Relevance:&lt;/strong&gt; 0.807 (must_read)&lt;/p&gt;
&lt;p&gt;Despite Transformers have achieved significant success in low-level vision tasks, they are constrained by computing self-attention with a quadratic complexity and limited-size windows. This limitation results in a lack of global receptive field across the entire image. Recently, State Space Models (SSMs) have gained widespread attention due to their global receptive field and linear complexity with respect to input length. However, integrating SSMs into low-level vision tasks presents two major challenges: (1) Relationship degradation of long-range tokens with a long-range forgetting problem by encoding pixel-by-pixel high-resolution images. (2) Significant redundancy in the existing multi-direction scanning strategy. To this end, we propose Hi-Mamba for image super-resolution (SR) to address these challenges, which unfolds the image with only a single scan. Specifically, the Global Hierarchical Mamba Block (GHMB) enables token interactions across the entire image, providing a global receptive field while leveraging a multi-scale structure to facilitate long-range dependency learning. Additionally, the Direction Alternation Module (DAM) adjusts the scanning patterns of GHMB across different layers to enhance spatial relationship modeling. Extensive experiments demonstrate that our Hi-Mamba achieves 0.2-0.27dB PSNR gains on the Urban100 dataset across different scaling factors compared to the state-of-the-art MambaIRv2 for SR. Moreover, our lightweight Hi-Mamba also outperforms lightweight SRFormer by 0.39dB PSNR for ×2 SR.&lt;/p&gt;</content:encoded></item><item><title>Towards High spatial resolution and fine-grained fidelity depth reconstruction of single-photon LiDAR with context-aware spatiotemporal modeling</title><link>https://doi.org/10.1016/j.isprsjprs.2025.11.034</link><guid>10.1016/j.isprsjprs.2025.11.034</guid><pubDate>Thu, 18 Dec 2025 16:01:21 +0000</pubDate><dc:creator>Zhenyu Zhang</dc:creator><dc:creator>Yuan Li</dc:creator><dc:creator>Feihu Zhu</dc:creator><dc:creator>Yuechao Ma</dc:creator><dc:creator>Junying Lv</dc:creator><dc:creator>Qian Sun</dc:creator><dc:creator>Lin Li</dc:creator><dc:creator>Wuming Zhang</dc:creator><prism:publicationName>ISPRS Journal of Photogrammetry and Remote Sensing</prism:publicationName><prism:doi>10.1016/j.isprsjprs.2025.11.034</prism:doi><description>While single-photon LiDAR promises revolutionary depth sensing capabilities, existing deep learning frameworks fundamentally fail to overcome the challenge of high spatial resolution (SR) data processing. To address the amplification of fine geometric details and complex spatiotemporal dependencies in high-SR single-photon data, we adopt a U-Net++ backbone with dense skip connections to preserve high-frequency features. Our encoder cascades two novel modules, integrating attention-driven modulation and convolution to adaptively model intricate patterns without sacrificing detail. We propose a 3D triple local-attention fusion module (3D-TriLAF) to suppress incoherent responses across temporal, spatial, and channel axes. In parallel, an opposite continuous dilation spatial–temporal convolution module (OCDSConv) is designed to extract structured context while preserving transient cues. To alleviate the misalignment and semantic drift between low and high-level features—problems exacerbated by increased resolution—we design a multi-scale fusion mechanism that facilitates consistent geometric modeling across scales. Finally, we propose a hybrid loss combining ordinal regression (OR) loss, structural similarity index measure (SSIM) loss, and bilateral total variation (BTV) loss to jointly enhances peak localization, structural fidelity, and edge-aware smoothness. Extensive experiments on two 128 × 128 SR simulated datasets show that, compared with the best baseline, our framework reduces RMSE and Abs Rel by up to 60.00 % and 31.58 %. On two (200 + )×(200 + ) SR real-world datasets, RMSE and Abs Rel drop by 42.31 % and 39.44 %. These quantitative gains and visual improvements in geometric continuity under complex lighting confirm its suitability for fine-grained high-SR single-photon depth reconstruction.
Published: 2025-12-18T16:01:21+00:00
Venue: ISPRS Journal of Photogrammetry and Remote Sensing
Score: 0.803 (must_read)</description><content:encoded>&lt;p&gt;&lt;strong&gt;Authors:&lt;/strong&gt; Zhenyu Zhang; Yuan Li; Feihu Zhu; Yuechao Ma; Junying Lv; Qian Sun; Lin Li; Wuming Zhang&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Journal:&lt;/strong&gt; ISPRS Journal of Photogrammetry and Remote Sensing&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;a href="https://doi.org/10.1016/j.isprsjprs.2025.11.034"&gt;10.1016/j.isprsjprs.2025.11.034&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Relevance:&lt;/strong&gt; 0.803 (must_read)&lt;/p&gt;
&lt;p&gt;While single-photon LiDAR promises revolutionary depth sensing capabilities, existing deep learning frameworks fundamentally fail to overcome the challenge of high spatial resolution (SR) data processing. To address the amplification of fine geometric details and complex spatiotemporal dependencies in high-SR single-photon data, we adopt a U-Net++ backbone with dense skip connections to preserve high-frequency features. Our encoder cascades two novel modules, integrating attention-driven modulation and convolution to adaptively model intricate patterns without sacrificing detail. We propose a 3D triple local-attention fusion module (3D-TriLAF) to suppress incoherent responses across temporal, spatial, and channel axes. In parallel, an opposite continuous dilation spatial–temporal convolution module (OCDSConv) is designed to extract structured context while preserving transient cues. To alleviate the misalignment and semantic drift between low and high-level features—problems exacerbated by increased resolution—we design a multi-scale fusion mechanism that facilitates consistent geometric modeling across scales. Finally, we propose a hybrid loss combining ordinal regression (OR) loss, structural similarity index measure (SSIM) loss, and bilateral total variation (BTV) loss to jointly enhances peak localization, structural fidelity, and edge-aware smoothness. Extensive experiments on two 128 × 128 SR simulated datasets show that, compared with the best baseline, our framework reduces RMSE and Abs Rel by up to 60.00 % and 31.58 %. On two (200 + )×(200 + ) SR real-world datasets, RMSE and Abs Rel drop by 42.31 % and 39.44 %. These quantitative gains and visual improvements in geometric continuity under complex lighting confirm its suitability for fine-grained high-SR single-photon depth reconstruction.&lt;/p&gt;</content:encoded></item><item><title>SD-Fuse: An Image Structure-Driven Model for Multi-Focus Image Fusion</title><link>https://doi.org/10.1016/j.inffus.2025.104058</link><guid>10.1016/j.inffus.2025.104058</guid><pubDate>Fri, 19 Dec 2025 05:59:44 +0000</pubDate><dc:creator>Zeyu Wang</dc:creator><dc:creator>Jiayu Wang</dc:creator><dc:creator>Haiyu Song</dc:creator><dc:creator>Pengjie Wang</dc:creator><dc:creator>Kedi Lyu</dc:creator><dc:creator>Wei Li</dc:creator><dc:creator>Libo Zhao</dc:creator><prism:publicationName>Information Fusion</prism:publicationName><prism:doi>10.1016/j.inffus.2025.104058</prism:doi><description>Multi-focus image fusion (MFIF) aims to generate a fully focused composite from multiple partially focused images. Existing methods often employ complex loss functions or customized network architectures to refine decision map boundaries, overlooking intrinsic structural information. In this study, we empirically uncover an image structure-boundary prior through comprehensive statistical analysis, explicitly demonstrating that boundaries between focused and defocused regions naturally align with prominent structural features of images. Motivated by this structural prior, we propose a structure-driven fusion framework termed SD-Fuse. This framework consists of three complementary components: a global structure-aware branch, a local focus detection branch, and a novel structure-guided filter (SGF). The structure-aware branch first extracts essential structural cues and employs a Transformer module to capture global structural dependencies. Concurrently, the focus detection branch leverages a CNN architecture to generate initial decision maps based on spatial inputs. Crucially, we introduce SGF, inspired by traditional guided filtering methods, to facilitate effective interaction between global and local features. Through optimization within SGF, the refined global structure provided by the Transformer progressively guides the local spatial features, ensuring precise alignment of boundaries and artifact-free decision maps. Extensive qualitative and quantitative experiments demonstrate that our SD-Fuse significantly outperforms existing methods, achieving state-of-the-art performance. We will release code and pretrained weights.
Published: 2025-12-19T05:59:44+00:00
Venue: Information Fusion
Score: 0.800 (must_read)</description><content:encoded>&lt;p&gt;&lt;strong&gt;Authors:&lt;/strong&gt; Zeyu Wang; Jiayu Wang; Haiyu Song; Pengjie Wang; Kedi Lyu; Wei Li; Libo Zhao&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Journal:&lt;/strong&gt; Information Fusion&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;a href="https://doi.org/10.1016/j.inffus.2025.104058"&gt;10.1016/j.inffus.2025.104058&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Relevance:&lt;/strong&gt; 0.800 (must_read)&lt;/p&gt;
&lt;p&gt;Multi-focus image fusion (MFIF) aims to generate a fully focused composite from multiple partially focused images. Existing methods often employ complex loss functions or customized network architectures to refine decision map boundaries, overlooking intrinsic structural information. In this study, we empirically uncover an image structure-boundary prior through comprehensive statistical analysis, explicitly demonstrating that boundaries between focused and defocused regions naturally align with prominent structural features of images. Motivated by this structural prior, we propose a structure-driven fusion framework termed SD-Fuse. This framework consists of three complementary components: a global structure-aware branch, a local focus detection branch, and a novel structure-guided filter (SGF). The structure-aware branch first extracts essential structural cues and employs a Transformer module to capture global structural dependencies. Concurrently, the focus detection branch leverages a CNN architecture to generate initial decision maps based on spatial inputs. Crucially, we introduce SGF, inspired by traditional guided filtering methods, to facilitate effective interaction between global and local features. Through optimization within SGF, the refined global structure provided by the Transformer progressively guides the local spatial features, ensuring precise alignment of boundaries and artifact-free decision maps. Extensive qualitative and quantitative experiments demonstrate that our SD-Fuse significantly outperforms existing methods, achieving state-of-the-art performance. We will release code and pretrained weights.&lt;/p&gt;</content:encoded></item><item><title>RA-MD: An RKHS-based Adaptive Mahalanobis Distance to Enhance Counterfactual Explanations for Neural Networks</title><link>https://doi.org/10.1016/j.inffus.2025.104067</link><guid>10.1016/j.inffus.2025.104067</guid><pubDate>Fri, 19 Dec 2025 05:59:50 +0000</pubDate><dc:creator>Ao Xu</dc:creator><dc:creator>Yukai Zhang</dc:creator><dc:creator>Tieru Wu</dc:creator><prism:publicationName>Information Fusion</prism:publicationName><prism:doi>10.1016/j.inffus.2025.104067</prism:doi><description>With the rapid advancement of deep neural networks, their integration into critical decision-making systems has become a significant driver of societal progress, necessitating robust methods for interpretability. Counterfactual explanations (CEs) play a pivotal role in enhancing the transparency of neural network models within eXplainable Artificial Intelligence (XAI). Although extensive research has explored counterfactual explanation generation, efficiently producing minimal and human-interpretable CEs for complex neural architectures remains a persistent challenge. In this paper, we propose a unified RKHS-based Adaptive Mahalanobis Distance (RA-MD) framework for generating CEs in neural networks. The framework first selects the most informative layers using a Kernel Feature Disagreement (KFD) criterion, then captures feature relevance through a Wasserstein-based Divergence Vector Representation (WDVR), and finally employs a two-stage optimization strategy that refines counterfactuals in feature space and reconstructs realistic instances via a generative model. This formulation unifies distributional modeling and interpretability under a single framework, leading to more robust and semantically consistent counterfactual explanations. Extensive experiments on multiple datasets and architectures demonstrate that the proposed RA-MD approach produces counterfactuals with smaller perturbations, higher fidelity, and improved interpretability compared to existing methods.
Published: 2025-12-19T05:59:50+00:00
Venue: Information Fusion
Score: 0.799 (must_read)</description><content:encoded>&lt;p&gt;&lt;strong&gt;Authors:&lt;/strong&gt; Ao Xu; Yukai Zhang; Tieru Wu&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Journal:&lt;/strong&gt; Information Fusion&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;a href="https://doi.org/10.1016/j.inffus.2025.104067"&gt;10.1016/j.inffus.2025.104067&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Relevance:&lt;/strong&gt; 0.799 (must_read)&lt;/p&gt;
&lt;p&gt;With the rapid advancement of deep neural networks, their integration into critical decision-making systems has become a significant driver of societal progress, necessitating robust methods for interpretability. Counterfactual explanations (CEs) play a pivotal role in enhancing the transparency of neural network models within eXplainable Artificial Intelligence (XAI). Although extensive research has explored counterfactual explanation generation, efficiently producing minimal and human-interpretable CEs for complex neural architectures remains a persistent challenge. In this paper, we propose a unified RKHS-based Adaptive Mahalanobis Distance (RA-MD) framework for generating CEs in neural networks. The framework first selects the most informative layers using a Kernel Feature Disagreement (KFD) criterion, then captures feature relevance through a Wasserstein-based Divergence Vector Representation (WDVR), and finally employs a two-stage optimization strategy that refines counterfactuals in feature space and reconstructs realistic instances via a generative model. This formulation unifies distributional modeling and interpretability under a single framework, leading to more robust and semantically consistent counterfactual explanations. Extensive experiments on multiple datasets and architectures demonstrate that the proposed RA-MD approach produces counterfactuals with smaller perturbations, higher fidelity, and improved interpretability compared to existing methods.&lt;/p&gt;</content:encoded></item><item><title>Real-Time DEtection TRansformer Enhanced by WaveFormer and WS-GD Neck</title><link>https://doi.org/10.1109/lgrs.2025.3646494</link><guid>10.1109/lgrs.2025.3646494</guid><pubDate>Fri, 19 Dec 2025 19:03:04 +0000</pubDate><dc:creator>Litao Kang</dc:creator><dc:creator>Chaoyue Liu</dc:creator><dc:creator>Huaitao Fan</dc:creator><dc:creator>Zhimin Zhang</dc:creator><dc:creator>Zhen Chen</dc:creator><prism:publicationName>IEEE Geoscience and Remote Sensing Letters</prism:publicationName><prism:doi>10.1109/lgrs.2025.3646494</prism:doi><description>Deep learning-based methods hold significant potential for synthetic aperture radar (SAR) target detection, but they still face numerous challenges, including difficulty extracting global contextual features for large-scale targets, significant multi-scale issues, and the problem of feature extraction of SAR targets with large aspect ratios, which hinder further performance improvement. To this end, this paper proposes a WaveFormer module, which decomposes the image through wavelet convolution and uses convolution and Transformer to process the frequency domain components they are good at, respectively, to expand the receptive field with low parameter overhead and enhance the target feature extraction ability. To address cross-layer information attenuation during feature fusion, a Gather-and-Distribute(GD) mechanism is introduced to reconstruct the Neck network, enhancing multi-scale feature fusion and detection capabilities. Furthermore, given the large aspect ratio and distinct principal axis orientation of SAR targets, a Weighted Strip-Convolution(WSConv) is proposed to effectively improve detection performance. Experiments on the largest multi-class SAR target detection dataset, SARDet-100K, demonstrate that our method achieves a mean average precision (mAP) of 61.5%, reaching state-of-the-art performance and validating its effectiveness.
Published: 2025-12-19T19:03:04+00:00
Venue: IEEE Geoscience and Remote Sensing Letters
Score: 0.799 (must_read)</description><content:encoded>&lt;p&gt;&lt;strong&gt;Authors:&lt;/strong&gt; Litao Kang; Chaoyue Liu; Huaitao Fan; Zhimin Zhang; Zhen Chen&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Journal:&lt;/strong&gt; IEEE Geoscience and Remote Sensing Letters&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;a href="https://doi.org/10.1109/lgrs.2025.3646494"&gt;10.1109/lgrs.2025.3646494&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Relevance:&lt;/strong&gt; 0.799 (must_read)&lt;/p&gt;
&lt;p&gt;Deep learning-based methods hold significant potential for synthetic aperture radar (SAR) target detection, but they still face numerous challenges, including difficulty extracting global contextual features for large-scale targets, significant multi-scale issues, and the problem of feature extraction of SAR targets with large aspect ratios, which hinder further performance improvement. To this end, this paper proposes a WaveFormer module, which decomposes the image through wavelet convolution and uses convolution and Transformer to process the frequency domain components they are good at, respectively, to expand the receptive field with low parameter overhead and enhance the target feature extraction ability. To address cross-layer information attenuation during feature fusion, a Gather-and-Distribute(GD) mechanism is introduced to reconstruct the Neck network, enhancing multi-scale feature fusion and detection capabilities. Furthermore, given the large aspect ratio and distinct principal axis orientation of SAR targets, a Weighted Strip-Convolution(WSConv) is proposed to effectively improve detection performance. Experiments on the largest multi-class SAR target detection dataset, SARDet-100K, demonstrate that our method achieves a mean average precision (mAP) of 61.5%, reaching state-of-the-art performance and validating its effectiveness.&lt;/p&gt;</content:encoded></item><item><title>Complexity Control Facilitates Reasoning-Based Compositional Generalization in Transformers</title><link>https://doi.org/10.1109/tpami.2025.3646483</link><guid>10.1109/tpami.2025.3646483</guid><pubDate>Fri, 19 Dec 2025 18:58:30 +0000</pubDate><dc:creator>Zhongwang Zhang</dc:creator><dc:creator>Pengxiao Lin</dc:creator><dc:creator>Zhiwei Wang</dc:creator><dc:creator>Yaoyu Zhang</dc:creator><dc:creator>Zhi-Qin John Xu</dc:creator><prism:publicationName>IEEE Transactions on Pattern Analysis and Machine Intelligence</prism:publicationName><prism:doi>10.1109/tpami.2025.3646483</prism:doi><description>Transformers have demonstrated impressive capabilities across various tasks, yet their performance on compositional problems remains a subject of debate. In this study, we investigate the internal mechanisms underlying Transformers' behavior in compositional tasks. We find that complexity control strategies—particularly the choice of parameter initialization scale and weight decay—significantly influence whether the model learns primitive-level rules that generalize out-of-distribution (reasoning-based solutions) or relies solely on memorized map pings (memory-based solutions). By applying masking strategies to the model's information circuits and employing multiple complexity metrics, we reveal distinct internal working mechanisms associated with different solution types. Further analysis reveals that reasoning-based solutions exhibit a lower complexity bias, which aligns with the well-studied neuron condensation phenomenon. This lower complexity bias is hypothesized to be the key factor enabling these solutions to learn reasoning rules. We validate these conclusions across multiple real-world datasets, including image generation and natural language processing tasks, confirming the broad applicability of our findings.
Published: 2025-12-19T18:58:30+00:00
Venue: IEEE Transactions on Pattern Analysis and Machine Intelligence
Score: 0.797 (must_read)</description><content:encoded>&lt;p&gt;&lt;strong&gt;Authors:&lt;/strong&gt; Zhongwang Zhang; Pengxiao Lin; Zhiwei Wang; Yaoyu Zhang; Zhi-Qin John Xu&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Journal:&lt;/strong&gt; IEEE Transactions on Pattern Analysis and Machine Intelligence&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;a href="https://doi.org/10.1109/tpami.2025.3646483"&gt;10.1109/tpami.2025.3646483&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Relevance:&lt;/strong&gt; 0.797 (must_read)&lt;/p&gt;
&lt;p&gt;Transformers have demonstrated impressive capabilities across various tasks, yet their performance on compositional problems remains a subject of debate. In this study, we investigate the internal mechanisms underlying Transformers&amp;#x27; behavior in compositional tasks. We find that complexity control strategies—particularly the choice of parameter initialization scale and weight decay—significantly influence whether the model learns primitive-level rules that generalize out-of-distribution (reasoning-based solutions) or relies solely on memorized map pings (memory-based solutions). By applying masking strategies to the model&amp;#x27;s information circuits and employing multiple complexity metrics, we reveal distinct internal working mechanisms associated with different solution types. Further analysis reveals that reasoning-based solutions exhibit a lower complexity bias, which aligns with the well-studied neuron condensation phenomenon. This lower complexity bias is hypothesized to be the key factor enabling these solutions to learn reasoning rules. We validate these conclusions across multiple real-world datasets, including image generation and natural language processing tasks, confirming the broad applicability of our findings.&lt;/p&gt;</content:encoded></item><item><title>Breaking the Multi-Enhancement Bottleneck: Domain-Consistent Quality Enhancement for Compressed Images</title><link>https://doi.org/10.1109/tpami.2025.3646223</link><guid>10.1109/tpami.2025.3646223</guid><pubDate>Fri, 19 Dec 2025 18:58:30 +0000</pubDate><dc:creator>Qunliang Xing</dc:creator><dc:creator>Ce Zheng</dc:creator><dc:creator>Mai Xu</dc:creator><dc:creator>Jing Yang</dc:creator><dc:creator>Shengxi Li</dc:creator><prism:publicationName>IEEE Transactions on Pattern Analysis and Machine Intelligence</prism:publicationName><prism:doi>10.1109/tpami.2025.3646223</prism:doi><description>Quality enhancement methods have been widely integrated into visual communication pipelines to mitigate artifacts in compressed images. Ideally, these quality enhancement methods should perform robustly when applied to images that have already undergone prior enhancement during transmission. We refer to this scenario as multi-enhancement, which generalizes the well-known multi-generation scenario of image compression. Unfortunately, current quality enhancement methods suffer from severe degradation when applied in multi-enhancement. To address this challenge, we propose a novel adaptation method that transforms existing quality enhancement models into domain-consistent ones. Specifically, our method enhances a low-quality compressed image into a high-quality image within the natural domain during the first enhancement, and ensures that subsequent enhancements preserve this quality without further degradation. Extensive experiments validate the effectiveness of our method and show that various existing models can be successfully adapted to maintain both fidelity and perceptual quality in multi-enhancement scenarios.
Published: 2025-12-19T18:58:30+00:00
Venue: IEEE Transactions on Pattern Analysis and Machine Intelligence
Score: 0.796 (must_read)</description><content:encoded>&lt;p&gt;&lt;strong&gt;Authors:&lt;/strong&gt; Qunliang Xing; Ce Zheng; Mai Xu; Jing Yang; Shengxi Li&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Journal:&lt;/strong&gt; IEEE Transactions on Pattern Analysis and Machine Intelligence&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;a href="https://doi.org/10.1109/tpami.2025.3646223"&gt;10.1109/tpami.2025.3646223&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Relevance:&lt;/strong&gt; 0.796 (must_read)&lt;/p&gt;
&lt;p&gt;Quality enhancement methods have been widely integrated into visual communication pipelines to mitigate artifacts in compressed images. Ideally, these quality enhancement methods should perform robustly when applied to images that have already undergone prior enhancement during transmission. We refer to this scenario as multi-enhancement, which generalizes the well-known multi-generation scenario of image compression. Unfortunately, current quality enhancement methods suffer from severe degradation when applied in multi-enhancement. To address this challenge, we propose a novel adaptation method that transforms existing quality enhancement models into domain-consistent ones. Specifically, our method enhances a low-quality compressed image into a high-quality image within the natural domain during the first enhancement, and ensures that subsequent enhancements preserve this quality without further degradation. Extensive experiments validate the effectiveness of our method and show that various existing models can be successfully adapted to maintain both fidelity and perceptual quality in multi-enhancement scenarios.&lt;/p&gt;</content:encoded></item><item><title>ViV-ReID: Bidirectional Structural-Aware Spatial-Temporal Graph Networks on Large-Scale Video-Based Vessel Re-Identification Dataset</title><link>https://doi.org/10.1109/tip.2025.3643156</link><guid>10.1109/tip.2025.3643156</guid><pubDate>Thu, 18 Dec 2025 18:35:49 +0000</pubDate><dc:creator>Mingxin Zhang</dc:creator><dc:creator>Fuxiang Feng</dc:creator><dc:creator>Xing Fang</dc:creator><dc:creator>Lin Zhang</dc:creator><dc:creator>Youmei Zhang</dc:creator><dc:creator>Xiaolei Li</dc:creator><dc:creator>Wei Zhang</dc:creator><prism:publicationName>IEEE Transactions on Image Processing</prism:publicationName><prism:doi>10.1109/tip.2025.3643156</prism:doi><description>Vessel re-identification (ReID) serves as a foundational task for intelligent maritime transportation systems. To enhance maritime surveillance capabilities, this study investigates video-based vessel ReID, a critical yet underexplored task in intelligent transportation systems. The lack of relevant datasets has limited the progress of Video-based vessel ReID research work. We established ViV-ReID, the first publicly available large-scale video-based vessel ReID dataset, comprising 480 vessel identities captured from 20 cross-port camera views (7,165 tracklets and 1.14 million frames), establishing a benchmark for advancing vessel ReID from image to video processing. Videos offer significantly richer information than single-frame images. The dynamic nature of video often leads to fragmented spatio-temporal features causing disrupted contextual understanding, and to address this problem, we further propose a Bidirectional Structural-Aware Spatial-Temporal Graph Network (Bi-SSTN) that explicitly aligns spatio-temporal features using vessel structural priors. Extensive experiments on the ViV-ReID dataset demonstrate that image-based ReID methods often show suboptimal performance when applied to video data. Meanwhile, it is crucial to validate the effectiveness of spatio-temporal information and establish performance benchmarks for different methods. The Bidirectional Structural-Aware Spatial-Temporal Graph Network (Bi-SSTN) significantly outperforms state-of-the-art methods on ViV-ReID, confirming its efficacy in modeling vessel-specific spatio-temporal patterns. Project web page: https://vsislab.github.io/ViV_ReID/.
Published: 2025-12-18T18:35:49+00:00
Venue: IEEE Transactions on Image Processing
Score: 0.795 (must_read)</description><content:encoded>&lt;p&gt;&lt;strong&gt;Authors:&lt;/strong&gt; Mingxin Zhang; Fuxiang Feng; Xing Fang; Lin Zhang; Youmei Zhang; Xiaolei Li; Wei Zhang&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Journal:&lt;/strong&gt; IEEE Transactions on Image Processing&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;a href="https://doi.org/10.1109/tip.2025.3643156"&gt;10.1109/tip.2025.3643156&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Relevance:&lt;/strong&gt; 0.795 (must_read)&lt;/p&gt;
&lt;p&gt;Vessel re-identification (ReID) serves as a foundational task for intelligent maritime transportation systems. To enhance maritime surveillance capabilities, this study investigates video-based vessel ReID, a critical yet underexplored task in intelligent transportation systems. The lack of relevant datasets has limited the progress of Video-based vessel ReID research work. We established ViV-ReID, the first publicly available large-scale video-based vessel ReID dataset, comprising 480 vessel identities captured from 20 cross-port camera views (7,165 tracklets and 1.14 million frames), establishing a benchmark for advancing vessel ReID from image to video processing. Videos offer significantly richer information than single-frame images. The dynamic nature of video often leads to fragmented spatio-temporal features causing disrupted contextual understanding, and to address this problem, we further propose a Bidirectional Structural-Aware Spatial-Temporal Graph Network (Bi-SSTN) that explicitly aligns spatio-temporal features using vessel structural priors. Extensive experiments on the ViV-ReID dataset demonstrate that image-based ReID methods often show suboptimal performance when applied to video data. Meanwhile, it is crucial to validate the effectiveness of spatio-temporal information and establish performance benchmarks for different methods. The Bidirectional Structural-Aware Spatial-Temporal Graph Network (Bi-SSTN) significantly outperforms state-of-the-art methods on ViV-ReID, confirming its efficacy in modeling vessel-specific spatio-temporal patterns. Project web page: https://vsislab.github.io/ViV_ReID/.&lt;/p&gt;</content:encoded></item><item><title>4D-RaDiff: Latent Diffusion for 4D Radar Point Cloud Generation</title><link>https://arxiv.org/abs/2512.14235v1</link><guid>http://arxiv.org/abs/2512.14235v1</guid><pubDate>Tue, 16 Dec 2025 09:43:05 +0000</pubDate><dc:creator>Jimmie Kwok</dc:creator><dc:creator>Holger Caesar</dc:creator><dc:creator>Andras Palffy</dc:creator><dc:type>preprint</dc:type><prism:publicationName>arXiv</prism:publicationName><description>Automotive radar has shown promising developments in environment perception due to its cost-effectiveness and robustness in adverse weather conditions. However, the limited availability of annotated radar data poses a significant challenge for advancing radar-based perception systems. To address this limitation, we propose a novel framework to generate 4D radar point clouds for training and evaluating object detectors. Unlike image-based diffusion, our method is designed to consider the sparsity and unique characteristics of radar point clouds by applying diffusion to a latent point cloud representation. Within this latent space, generation is controlled via conditioning at either the object or scene level. The proposed 4D-RaDiff converts unlabeled bounding boxes into high-quality radar annotations and transforms existing LiDAR point cloud data into realistic radar scenes. Experiments demonstrate that incorporating synthetic radar data of 4D-RaDiff as data augmentation method during training consistently improves object detection performance compared to training on real data only. In addition, pre-training on our synthetic data reduces the amount of required annotated radar data by up to 90% while achieving comparable object detection performance.
Published: 2025-12-16T09:43:05+00:00
Venue: arXiv
Score: 0.791 (must_read)</description><content:encoded>&lt;p&gt;&lt;strong&gt;Authors:&lt;/strong&gt; Jimmie Kwok; Holger Caesar; Andras Palffy&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Journal:&lt;/strong&gt; arXiv&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Relevance:&lt;/strong&gt; 0.791 (must_read)&lt;/p&gt;
&lt;p&gt;Automotive radar has shown promising developments in environment perception due to its cost-effectiveness and robustness in adverse weather conditions. However, the limited availability of annotated radar data poses a significant challenge for advancing radar-based perception systems. To address this limitation, we propose a novel framework to generate 4D radar point clouds for training and evaluating object detectors. Unlike image-based diffusion, our method is designed to consider the sparsity and unique characteristics of radar point clouds by applying diffusion to a latent point cloud representation. Within this latent space, generation is controlled via conditioning at either the object or scene level. The proposed 4D-RaDiff converts unlabeled bounding boxes into high-quality radar annotations and transforms existing LiDAR point cloud data into realistic radar scenes. Experiments demonstrate that incorporating synthetic radar data of 4D-RaDiff as data augmentation method during training consistently improves object detection performance compared to training on real data only. In addition, pre-training on our synthetic data reduces the amount of required annotated radar data by up to 90% while achieving comparable object detection performance.&lt;/p&gt;</content:encoded></item><item><title>Light CNN-Transformer Dual-Branch Network for Real-Time Semantic Segmentation</title><link>https://doi.org/10.1109/tmm.2025.3645624</link><guid>10.1109/tmm.2025.3645624</guid><pubDate>Thu, 18 Dec 2025 18:34:14 +0000</pubDate><dc:creator>Yongsheng Dong</dc:creator><dc:creator>Siming Jia</dc:creator><dc:creator>Xuelong Li</dc:creator><prism:publicationName>IEEE Transactions on Multimedia</prism:publicationName><prism:doi>10.1109/tmm.2025.3645624</prism:doi><description>Convolutional Neural Networks (CNN) have widely used in semantic segmentation, and can effectively extract local hierarchical information while being unsatisfactory in extracting global information. By contrast, Transformer is good at extracting long-distance dependencies in semantics while it is time-consuming. In this work, we propose a Light CNN-Transformer Dual-Branch Network (LCTDBNet) for real-time semantic segmentation. It consists of a longer CNN branch to extract local hierarchical information and a shorter Transformer branch to extract global contextual information. The CNN branch uses a lightweight encoder-decoder structure to further extract more local hierarchical information. We propose a Deep Strip Aggregation Pyramid Pooling Module (DSAPPM) to extract contextual and strip information. We further propose a Feature Pooling Refinement Module (FPRM) to optimise the feature representation at different stages. Finally, we propose a CNN-Transformer Fusion Module (CTFM) to fuse the features of two branches. Experimental results demonstrate that our proposed LCTDBNet is effective and achieves satisfactory results. Specifically, the base version of LCTDBNet achieves 80.3% mean intersection over union (mIoU) at 78.6 frames per second (FPS) on Cityscapes, 80.0% mIoU at 137.5 FPS on CamVid and 40.9% mIoU at 253.7 FPS on ADE20 K.
Published: 2025-12-18T18:34:14+00:00
Venue: IEEE Transactions on Multimedia
Score: 0.790 (must_read)</description><content:encoded>&lt;p&gt;&lt;strong&gt;Authors:&lt;/strong&gt; Yongsheng Dong; Siming Jia; Xuelong Li&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Journal:&lt;/strong&gt; IEEE Transactions on Multimedia&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;a href="https://doi.org/10.1109/tmm.2025.3645624"&gt;10.1109/tmm.2025.3645624&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Relevance:&lt;/strong&gt; 0.790 (must_read)&lt;/p&gt;
&lt;p&gt;Convolutional Neural Networks (CNN) have widely used in semantic segmentation, and can effectively extract local hierarchical information while being unsatisfactory in extracting global information. By contrast, Transformer is good at extracting long-distance dependencies in semantics while it is time-consuming. In this work, we propose a Light CNN-Transformer Dual-Branch Network (LCTDBNet) for real-time semantic segmentation. It consists of a longer CNN branch to extract local hierarchical information and a shorter Transformer branch to extract global contextual information. The CNN branch uses a lightweight encoder-decoder structure to further extract more local hierarchical information. We propose a Deep Strip Aggregation Pyramid Pooling Module (DSAPPM) to extract contextual and strip information. We further propose a Feature Pooling Refinement Module (FPRM) to optimise the feature representation at different stages. Finally, we propose a CNN-Transformer Fusion Module (CTFM) to fuse the features of two branches. Experimental results demonstrate that our proposed LCTDBNet is effective and achieves satisfactory results. Specifically, the base version of LCTDBNet achieves 80.3% mean intersection over union (mIoU) at 78.6 frames per second (FPS) on Cityscapes, 80.0% mIoU at 137.5 FPS on CamVid and 40.9% mIoU at 253.7 FPS on ADE20 K.&lt;/p&gt;</content:encoded></item><item><title>Next-Generation License Plate Detection and Recognition System using YOLOv8</title><link>https://arxiv.org/abs/2512.16826v1</link><guid>http://arxiv.org/abs/2512.16826v1</guid><pubDate>Thu, 18 Dec 2025 18:06:29 +0000</pubDate><dc:creator>Arslan Amin</dc:creator><dc:creator>Rafia Mumtaz</dc:creator><dc:creator>Muhammad Jawad Bashir</dc:creator><dc:creator>Syed Mohammad Hassan Zaidi</dc:creator><dc:type>preprint</dc:type><prism:publicationName>arXiv</prism:publicationName><prism:doi>10.1109/HONET59747.2023.10374756</prism:doi><description>In the evolving landscape of traffic management and vehicle surveillance, efficient license plate detection and recognition are indispensable. Historically, many methodologies have tackled this challenge, but consistent real-time accuracy, especially in diverse environments, remains elusive. This study examines the performance of YOLOv8 variants on License Plate Recognition (LPR) and Character Recognition tasks, crucial for advancing Intelligent Transportation Systems. Two distinct datasets were employed for training and evaluation, yielding notable findings. The YOLOv8 Nano variant demonstrated a precision of 0.964 and mAP50 of 0.918 on the LPR task, while the YOLOv8 Small variant exhibited a precision of 0.92 and mAP50 of 0.91 on the Character Recognition task. A custom method for character sequencing was introduced, effectively sequencing the detected characters based on their x-axis positions. An optimized pipeline, utilizing YOLOv8 Nano for LPR and YOLOv8 Small for Character Recognition, is proposed. This configuration not only maintains computational efficiency but also ensures high accuracy, establishing a robust foundation for future real-world deployments on edge devices within Intelligent Transportation Systems. This effort marks a significant stride towards the development of smarter and more efficient urban infrastructures.
Published: 2025-12-18T18:06:29+00:00
Venue: arXiv
Score: 0.789 (must_read)</description><content:encoded>&lt;p&gt;&lt;strong&gt;Authors:&lt;/strong&gt; Arslan Amin; Rafia Mumtaz; Muhammad Jawad Bashir; Syed Mohammad Hassan Zaidi&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Journal:&lt;/strong&gt; arXiv&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;a href="https://doi.org/10.1109/HONET59747.2023.10374756"&gt;10.1109/HONET59747.2023.10374756&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Relevance:&lt;/strong&gt; 0.789 (must_read)&lt;/p&gt;
&lt;p&gt;In the evolving landscape of traffic management and vehicle surveillance, efficient license plate detection and recognition are indispensable. Historically, many methodologies have tackled this challenge, but consistent real-time accuracy, especially in diverse environments, remains elusive. This study examines the performance of YOLOv8 variants on License Plate Recognition (LPR) and Character Recognition tasks, crucial for advancing Intelligent Transportation Systems. Two distinct datasets were employed for training and evaluation, yielding notable findings. The YOLOv8 Nano variant demonstrated a precision of 0.964 and mAP50 of 0.918 on the LPR task, while the YOLOv8 Small variant exhibited a precision of 0.92 and mAP50 of 0.91 on the Character Recognition task. A custom method for character sequencing was introduced, effectively sequencing the detected characters based on their x-axis positions. An optimized pipeline, utilizing YOLOv8 Nano for LPR and YOLOv8 Small for Character Recognition, is proposed. This configuration not only maintains computational efficiency but also ensures high accuracy, establishing a robust foundation for future real-world deployments on edge devices within Intelligent Transportation Systems. This effort marks a significant stride towards the development of smarter and more efficient urban infrastructures.&lt;/p&gt;</content:encoded></item><item><title>Lightweight Semantic Feature Extraction Model With Direction Awareness for Aerial Traffic Object Detection</title><link>https://doi.org/10.1109/tits.2025.3642410</link><guid>10.1109/tits.2025.3642410</guid><pubDate>Fri, 19 Dec 2025 19:00:48 +0000</pubDate><dc:creator>Jiaquan Shen</dc:creator><dc:creator>Ningzhong Liu</dc:creator><dc:creator>Han Sun</dc:creator><dc:creator>Shang Wu</dc:creator><dc:creator>Zongzheng Liang</dc:creator><dc:creator>Lulu Han</dc:creator><dc:creator>Yongxin Zhang</dc:creator><dc:creator>Deguang Li</dc:creator><prism:publicationName>IEEE Transactions on Intelligent Transportation Systems</prism:publicationName><prism:doi>10.1109/tits.2025.3642410</prism:doi><description>The detection of traffic objects in aerial scenes holds significant application potential in both military and civilian sectors. However, current aerial traffic object detection techniques based on computer vision face challenges including limited awareness of object direction, a heavy computational burden on the feature extraction backbone network, and inadequate capacity to learn crucial semantic information. In this paper, our focus is on investigating the mechanisms for predicting the directional perception of traffic objects in aerial scenes, achieving backbone network lightness, and exploring methods for extracting key semantic information from objects. Firstly, to tackle the challenge of poor perception of traffic object direction and angle in aerial scenes, we utilize techniques like equivariant vector field convolution, multi-task anchor-free prediction, and adaptive loss to develop a precise mechanism for recognizing and predicting object directions. Secondly, given the presence of small-sized and numerous objects in aerial scenes, we propose the adoption of a lightweight backbone network employing channel stacking to decrease the model’s computational burden. Additionally, we establish a theoretical framework and methodology for optimizing and compressing this backbone network, aimed at enhancing feature extraction and propagation for aerial traffic objects. Furthermore, to address the issue of inadequate learning of key semantic information features, we incorporate saliency attention and multi-scale contextual information to capture the essential semantic characteristics of the objects. We also establish a method for extracting semantic features specifically for aerial traffic objects. The approach presented in this paper broadens the applicability of aerial object detection algorithms and offers novel methodologies and theoretical foundations for object detection in intricate scenarios.
Published: 2025-12-19T19:00:48+00:00
Venue: IEEE Transactions on Intelligent Transportation Systems
Score: 0.789 (must_read)</description><content:encoded>&lt;p&gt;&lt;strong&gt;Authors:&lt;/strong&gt; Jiaquan Shen; Ningzhong Liu; Han Sun; Shang Wu; Zongzheng Liang; Lulu Han; Yongxin Zhang; Deguang Li&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Journal:&lt;/strong&gt; IEEE Transactions on Intelligent Transportation Systems&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;a href="https://doi.org/10.1109/tits.2025.3642410"&gt;10.1109/tits.2025.3642410&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Relevance:&lt;/strong&gt; 0.789 (must_read)&lt;/p&gt;
&lt;p&gt;The detection of traffic objects in aerial scenes holds significant application potential in both military and civilian sectors. However, current aerial traffic object detection techniques based on computer vision face challenges including limited awareness of object direction, a heavy computational burden on the feature extraction backbone network, and inadequate capacity to learn crucial semantic information. In this paper, our focus is on investigating the mechanisms for predicting the directional perception of traffic objects in aerial scenes, achieving backbone network lightness, and exploring methods for extracting key semantic information from objects. Firstly, to tackle the challenge of poor perception of traffic object direction and angle in aerial scenes, we utilize techniques like equivariant vector field convolution, multi-task anchor-free prediction, and adaptive loss to develop a precise mechanism for recognizing and predicting object directions. Secondly, given the presence of small-sized and numerous objects in aerial scenes, we propose the adoption of a lightweight backbone network employing channel stacking to decrease the model’s computational burden. Additionally, we establish a theoretical framework and methodology for optimizing and compressing this backbone network, aimed at enhancing feature extraction and propagation for aerial traffic objects. Furthermore, to address the issue of inadequate learning of key semantic information features, we incorporate saliency attention and multi-scale contextual information to capture the essential semantic characteristics of the objects. We also establish a method for extracting semantic features specifically for aerial traffic objects. The approach presented in this paper broadens the applicability of aerial object detection algorithms and offers novel methodologies and theoretical foundations for object detection in intricate scenarios.&lt;/p&gt;</content:encoded></item><item><title>Hunting for the Unknown: Open World Object Detection from a Class-Agnostic Perspective</title><link>https://doi.org/10.1016/j.neunet.2025.108501</link><guid>10.1016/j.neunet.2025.108501</guid><pubDate>Fri, 19 Dec 2025 04:24:19 +0000</pubDate><dc:creator>Jing Wang</dc:creator><dc:creator>Yonghua Cao</dc:creator><dc:creator>Zhanqiang Huo</dc:creator><dc:creator>Yingxu Qiao</dc:creator><prism:publicationName>Neural Networks</prism:publicationName><prism:doi>10.1016/j.neunet.2025.108501</prism:doi><description>The existing Open World Object Detection models rely on pseudo-labeling to annotate unknown objects during training. However, this approach leads to an over-dependence on known objects, thereby weakening the model’s capability to detect unknown objects. To tackle this issue, this paper presents a novel class-agnostic object detection model based on dynamic foreground perception and localization. The model leverages a dynamic foreground perception and localization algorithm that adeptly distinguishes foreground and background regions within images using dynamic detection heads. Additionally, by employing class-agnostic detection that does not rely on specific class information, the model mitigates excessive dependence on known categories and demonstrates improved performance in the detection of unknown objects. The key innovation of the model revolves around three main aspects: the refinement of spatial perception features, the disentanglement of attention features, and dynamic foreground perception and localization. Experimental findings across PASCAL VOC, COCO2017, LVISv1.0, and Objects365 datasets demonstrate that our model maintains high-level detection performance on known objects while surpassing most existing methods in the detection of unknown objects, exhibiting +11 points improvement in U-Recall performance. These results affirm the efficacy and superiority of the proposed detection method detailed in this paper.
Published: 2025-12-19T04:24:19+00:00
Venue: Neural Networks
Score: 0.789 (must_read)</description><content:encoded>&lt;p&gt;&lt;strong&gt;Authors:&lt;/strong&gt; Jing Wang; Yonghua Cao; Zhanqiang Huo; Yingxu Qiao&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Journal:&lt;/strong&gt; Neural Networks&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;a href="https://doi.org/10.1016/j.neunet.2025.108501"&gt;10.1016/j.neunet.2025.108501&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Relevance:&lt;/strong&gt; 0.789 (must_read)&lt;/p&gt;
&lt;p&gt;The existing Open World Object Detection models rely on pseudo-labeling to annotate unknown objects during training. However, this approach leads to an over-dependence on known objects, thereby weakening the model’s capability to detect unknown objects. To tackle this issue, this paper presents a novel class-agnostic object detection model based on dynamic foreground perception and localization. The model leverages a dynamic foreground perception and localization algorithm that adeptly distinguishes foreground and background regions within images using dynamic detection heads. Additionally, by employing class-agnostic detection that does not rely on specific class information, the model mitigates excessive dependence on known categories and demonstrates improved performance in the detection of unknown objects. The key innovation of the model revolves around three main aspects: the refinement of spatial perception features, the disentanglement of attention features, and dynamic foreground perception and localization. Experimental findings across PASCAL VOC, COCO2017, LVISv1.0, and Objects365 datasets demonstrate that our model maintains high-level detection performance on known objects while surpassing most existing methods in the detection of unknown objects, exhibiting +11 points improvement in U-Recall performance. These results affirm the efficacy and superiority of the proposed detection method detailed in this paper.&lt;/p&gt;</content:encoded></item><item><title>Multitask Reinforcement Learning with Metadata-Guided Adaptive Routing</title><link>https://doi.org/10.1016/j.inffus.2025.104068</link><guid>10.1016/j.inffus.2025.104068</guid><pubDate>Fri, 19 Dec 2025 05:59:38 +0000</pubDate><dc:creator>Rui Pan</dc:creator><dc:creator>Haoran Luo</dc:creator><dc:creator>Quan Yuan</dc:creator><dc:creator>Guiyang Luo</dc:creator><dc:creator>Jinglin Li</dc:creator><dc:creator>Tiesunlong Shen</dc:creator><dc:creator>Rui Mao</dc:creator><dc:creator>Erik Cambria</dc:creator><prism:publicationName>Information Fusion</prism:publicationName><prism:doi>10.1016/j.inffus.2025.104068</prism:doi><description>Multitask reinforcement learning aims to train a unified policy that generalizes across multiple related tasks, improving sample efficiency and promoting knowledge transfer. However, existing methods often suffer from negative knowledge transfer due to task interference, especially when using hard parameter sharing across tasks with diverse dynamics or goals. Conventional solutions typically adopt shared backbones with task-specific heads, gradient projection methods, or routing-based networks to mitigate conflict. However, many of these methods rely on simplistic task identifiers (e.g., one-hot vectors), lack expressive representations of task semantics, or fail to modulate shared components in a fine-grained, task-specific manner. To overcome these challenges, we propose Meta data-guided A daptive R outing ( MetaAR ), a novel framework that incorporates rich task metadata such as natural language descriptions to generate expressive and interpretable task representations. These representations are injected into a dynamic routing network, which adaptively reconfigures layer-wise computation paths in a shared modular policy network. To enable robust task-specific adaptation, we further introduce a noise-injected Top-K routing mechanism that dynamically selects the most relevant computation paths for each task. By injecting stochasticity during routing, this mechanism promotes exploration and mitigates interference between tasks through sparse, selective information flow. We evaluate MetaAR on the Meta-World benchmark with up to 50 robotic manipulation tasks, where it consistently outperforms strong baselines, achieving 4–8% higher mean success rates than the best-performing methods across the MT10 and MT50 variants.
Published: 2025-12-19T05:59:38+00:00
Venue: Information Fusion
Score: 0.789 (must_read)</description><content:encoded>&lt;p&gt;&lt;strong&gt;Authors:&lt;/strong&gt; Rui Pan; Haoran Luo; Quan Yuan; Guiyang Luo; Jinglin Li; Tiesunlong Shen; Rui Mao; Erik Cambria&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Journal:&lt;/strong&gt; Information Fusion&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;a href="https://doi.org/10.1016/j.inffus.2025.104068"&gt;10.1016/j.inffus.2025.104068&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Relevance:&lt;/strong&gt; 0.789 (must_read)&lt;/p&gt;
&lt;p&gt;Multitask reinforcement learning aims to train a unified policy that generalizes across multiple related tasks, improving sample efficiency and promoting knowledge transfer. However, existing methods often suffer from negative knowledge transfer due to task interference, especially when using hard parameter sharing across tasks with diverse dynamics or goals. Conventional solutions typically adopt shared backbones with task-specific heads, gradient projection methods, or routing-based networks to mitigate conflict. However, many of these methods rely on simplistic task identifiers (e.g., one-hot vectors), lack expressive representations of task semantics, or fail to modulate shared components in a fine-grained, task-specific manner. To overcome these challenges, we propose Meta data-guided A daptive R outing ( MetaAR ), a novel framework that incorporates rich task metadata such as natural language descriptions to generate expressive and interpretable task representations. These representations are injected into a dynamic routing network, which adaptively reconfigures layer-wise computation paths in a shared modular policy network. To enable robust task-specific adaptation, we further introduce a noise-injected Top-K routing mechanism that dynamically selects the most relevant computation paths for each task. By injecting stochasticity during routing, this mechanism promotes exploration and mitigates interference between tasks through sparse, selective information flow. We evaluate MetaAR on the Meta-World benchmark with up to 50 robotic manipulation tasks, where it consistently outperforms strong baselines, achieving 4–8% higher mean success rates than the best-performing methods across the MT10 and MT50 variants.&lt;/p&gt;</content:encoded></item><item><title>UAV-DETR: Few-parameter DETR for Small Object Detection in High-Altitude UAV Images</title><link>https://doi.org/10.1109/jstars.2025.3645731</link><guid>10.1109/jstars.2025.3645731</guid><pubDate>Thu, 18 Dec 2025 18:33:57 +0000</pubDate><dc:creator>Ningsheng Liao</dc:creator><dc:creator>Yuning Zhang</dc:creator><dc:creator>Zhongliang Yu</dc:creator><dc:creator>Jiangshuai Huang</dc:creator><dc:creator>Mi Zhu</dc:creator><dc:creator>Bo Peng</dc:creator><prism:publicationName>IEEE Journal of Selected Topics in Applied Earth Observations and Remote Sensing</prism:publicationName><prism:doi>10.1109/jstars.2025.3645731</prism:doi><description>In the field of computer vision, DEtection TRansformer (DETR) has received significant recognition for its ability to streamline the design process of object detectors through the concept of set prediction. However, its exceptional performance comes at the cost of a high parameter count and significant computational requirements. Moreover, its ability to detect small objects is compromised, making it less suitable for analyzing high-altitude Unmanned Aerial Vehicle (UAV) images. This paper proposes UAV-DETR, a DETR architecture specifically designed for detecting UAV images captured at high altitudes, which achieves a trade-off between parameter count and precision. UAV-DETR is built in two steps: first, inverted residual structures are used to preserve low-dimensional image features, followed by a carefully designed cascaded linear attention mechanism to mitigate parameter redundancy. Through observation and analysis of the attention diffusion issue in the encoder, a cross-channel dynamic sampling mechanism is proposed, which effectively expands the model's receptive field while maintaining accuracy. In addition, the loss function is redesigned by incorporating the Wasserstein distance, which is insensitive to bounding boxes, in order to significantly enhance the convergence speed of the model. Extensive experimental results on two major benchmarks, i.e. VisDrone and UAVDT, validate the simplicity and efficiency of our model. Specifically, on the VisDrone2021 public test set, UAV-DETR exhibits superior performance with only 14 million parameters compared to YOLOv8 _{m} _{m} , reducing the model's parameter count and complexity by 44% and 10% respectively, while achieving a 16.6% improvement in accuracy, without any data augmentation or post-processing procedures.
Published: 2025-12-18T18:33:57+00:00
Venue: IEEE Journal of Selected Topics in Applied Earth Observations and Remote Sensing
Score: 0.788 (must_read)</description><content:encoded>&lt;p&gt;&lt;strong&gt;Authors:&lt;/strong&gt; Ningsheng Liao; Yuning Zhang; Zhongliang Yu; Jiangshuai Huang; Mi Zhu; Bo Peng&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Journal:&lt;/strong&gt; IEEE Journal of Selected Topics in Applied Earth Observations and Remote Sensing&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;a href="https://doi.org/10.1109/jstars.2025.3645731"&gt;10.1109/jstars.2025.3645731&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Relevance:&lt;/strong&gt; 0.788 (must_read)&lt;/p&gt;
&lt;p&gt;In the field of computer vision, DEtection TRansformer (DETR) has received significant recognition for its ability to streamline the design process of object detectors through the concept of set prediction. However, its exceptional performance comes at the cost of a high parameter count and significant computational requirements. Moreover, its ability to detect small objects is compromised, making it less suitable for analyzing high-altitude Unmanned Aerial Vehicle (UAV) images. This paper proposes UAV-DETR, a DETR architecture specifically designed for detecting UAV images captured at high altitudes, which achieves a trade-off between parameter count and precision. UAV-DETR is built in two steps: first, inverted residual structures are used to preserve low-dimensional image features, followed by a carefully designed cascaded linear attention mechanism to mitigate parameter redundancy. Through observation and analysis of the attention diffusion issue in the encoder, a cross-channel dynamic sampling mechanism is proposed, which effectively expands the model&amp;#x27;s receptive field while maintaining accuracy. In addition, the loss function is redesigned by incorporating the Wasserstein distance, which is insensitive to bounding boxes, in order to significantly enhance the convergence speed of the model. Extensive experimental results on two major benchmarks, i.e. VisDrone and UAVDT, validate the simplicity and efficiency of our model. Specifically, on the VisDrone2021 public test set, UAV-DETR exhibits superior performance with only 14 million parameters compared to YOLOv8 _{m} _{m} , reducing the model&amp;#x27;s parameter count and complexity by 44% and 10% respectively, while achieving a 16.6% improvement in accuracy, without any data augmentation or post-processing procedures.&lt;/p&gt;</content:encoded></item><item><title>Collection-driven and Resolution-aware Prompt Learning for Few-Shot Remote Sensing Scene Classification</title><link>https://doi.org/10.1016/j.knosys.2025.115144</link><guid>10.1016/j.knosys.2025.115144</guid><pubDate>Fri, 19 Dec 2025 04:40:49 +0000</pubDate><dc:creator>Yufei Zheng</dc:creator><dc:creator>Shengsheng Wang</dc:creator><dc:creator>Yansheng Gao</dc:creator><prism:publicationName>Knowledge-Based Systems</prism:publicationName><prism:doi>10.1016/j.knosys.2025.115144</prism:doi><description>Remote sensing image datasets exhibit high interclass similarity and large intraclass diversity, limiting the generalization performance of Visual-Language Models (VLMs) in scene classification tasks. Existing approaches mainly adjust feature embedding or optimize prompts for prompt learning, but these methods based on embedding space correction do not fundamentally resolve the issue. In this paper, we propose Collection-driven and Resolution-aware Prompt Learning for Few-Shot Remote Sensing Scene Classification (CRNet), which guides the model to better address both intraclass and interclass challenges and thus improves the generalization performance of the model by leveraging class collection commonalities and the resolution-aware image information. Specifically, we introduce the Collection Commonality Generation (CCG) module, which utilizes Large Language Models to split all classes into collections and embeds their commonalities into the original prompts to reduce interclass similarity. In addition, we develop the Resolution-aware Visual Prompt (RVP) module, which weakens the differences within classes by introducing more resolution prompts, thus reducing intraclass diversity. Finally, we align pre-trained and learnable text and image features and design a shared layer to facilitate knowledge interaction between the collection commonalities introduced in the text prompts and visual prompts with different resolutions introduced in the images. Experiments across four public datasets demonstrate that CRNet outperforms existing methods in remote sensing, while numerous ablation studies also confirm the effectiveness of each component.
Published: 2025-12-19T04:40:49+00:00
Venue: Knowledge-Based Systems
Score: 0.784 (must_read)</description><content:encoded>&lt;p&gt;&lt;strong&gt;Authors:&lt;/strong&gt; Yufei Zheng; Shengsheng Wang; Yansheng Gao&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Journal:&lt;/strong&gt; Knowledge-Based Systems&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;a href="https://doi.org/10.1016/j.knosys.2025.115144"&gt;10.1016/j.knosys.2025.115144&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Relevance:&lt;/strong&gt; 0.784 (must_read)&lt;/p&gt;
&lt;p&gt;Remote sensing image datasets exhibit high interclass similarity and large intraclass diversity, limiting the generalization performance of Visual-Language Models (VLMs) in scene classification tasks. Existing approaches mainly adjust feature embedding or optimize prompts for prompt learning, but these methods based on embedding space correction do not fundamentally resolve the issue. In this paper, we propose Collection-driven and Resolution-aware Prompt Learning for Few-Shot Remote Sensing Scene Classification (CRNet), which guides the model to better address both intraclass and interclass challenges and thus improves the generalization performance of the model by leveraging class collection commonalities and the resolution-aware image information. Specifically, we introduce the Collection Commonality Generation (CCG) module, which utilizes Large Language Models to split all classes into collections and embeds their commonalities into the original prompts to reduce interclass similarity. In addition, we develop the Resolution-aware Visual Prompt (RVP) module, which weakens the differences within classes by introducing more resolution prompts, thus reducing intraclass diversity. Finally, we align pre-trained and learnable text and image features and design a shared layer to facilitate knowledge interaction between the collection commonalities introduced in the text prompts and visual prompts with different resolutions introduced in the images. Experiments across four public datasets demonstrate that CRNet outperforms existing methods in remote sensing, while numerous ablation studies also confirm the effectiveness of each component.&lt;/p&gt;</content:encoded></item><item><title>Zero-shot domain adaptation for remote sensing image classification with vision-language models</title><link>https://doi.org/10.1016/j.neucom.2025.132470</link><guid>10.1016/j.neucom.2025.132470</guid><pubDate>Fri, 19 Dec 2025 07:50:15 +0000</pubDate><dc:creator>Ziyao Wang</dc:creator><dc:creator>Chengxuan Pei</dc:creator><dc:creator>Xianping Ma</dc:creator><dc:creator>Man-On Pun</dc:creator><prism:publicationName>Neurocomputing</prism:publicationName><prism:doi>10.1016/j.neucom.2025.132470</prism:doi><description>While vision-language models (VLMs) excel with natural images, their application to remote sensing (RS) is hindered by closed-source designs and massive computational demands. To overcome this, we introduce VL-ZSDA-RS, an open-source VLM adapted for zero-shot domain adaptation (ZSDA) in remote sensing. Our model integrates a Mixture-of-Experts (MoE) language model with a Vision Transformer (ViT) encoder, creating a flexible pipeline that can dynamically adjust to new classification tasks via natural language prompts. Through efficient fine-tuning techniques like Low-Rank Adaptation (LoRA), VL-ZSDA-RS achieves strong performance across ten diverse remote sensing datasets, proving the viability of powerful, adaptable VLMs even with limited data and computational resources. The project code will be released at https://github.com/wzy6055/VL-ZSDA-RS .
Published: 2025-12-19T07:50:15+00:00
Venue: Neurocomputing
Score: 0.783 (must_read)</description><content:encoded>&lt;p&gt;&lt;strong&gt;Authors:&lt;/strong&gt; Ziyao Wang; Chengxuan Pei; Xianping Ma; Man-On Pun&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Journal:&lt;/strong&gt; Neurocomputing&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;a href="https://doi.org/10.1016/j.neucom.2025.132470"&gt;10.1016/j.neucom.2025.132470&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Relevance:&lt;/strong&gt; 0.783 (must_read)&lt;/p&gt;
&lt;p&gt;While vision-language models (VLMs) excel with natural images, their application to remote sensing (RS) is hindered by closed-source designs and massive computational demands. To overcome this, we introduce VL-ZSDA-RS, an open-source VLM adapted for zero-shot domain adaptation (ZSDA) in remote sensing. Our model integrates a Mixture-of-Experts (MoE) language model with a Vision Transformer (ViT) encoder, creating a flexible pipeline that can dynamically adjust to new classification tasks via natural language prompts. Through efficient fine-tuning techniques like Low-Rank Adaptation (LoRA), VL-ZSDA-RS achieves strong performance across ten diverse remote sensing datasets, proving the viability of powerful, adaptable VLMs even with limited data and computational resources. The project code will be released at https://github.com/wzy6055/VL-ZSDA-RS .&lt;/p&gt;</content:encoded></item><item><title>From Words to Wavelengths: VLMs for Few-Shot Multispectral Object Detection</title><link>https://arxiv.org/abs/2512.15971v1</link><guid>http://arxiv.org/abs/2512.15971v1</guid><pubDate>Wed, 17 Dec 2025 21:06:36 +0000</pubDate><dc:creator>Manuel Nkegoum</dc:creator><dc:creator>Minh-Tan Pham</dc:creator><dc:creator>Élisa Fromont</dc:creator><dc:creator>Bruno Avignon</dc:creator><dc:creator>Sébastien Lefèvre</dc:creator><dc:type>preprint</dc:type><prism:publicationName>arXiv</prism:publicationName><description>Multispectral object detection is critical for safety-sensitive applications such as autonomous driving and surveillance, where robust perception under diverse illumination conditions is essential. However, the limited availability of annotated multispectral data severely restricts the training of deep detectors. In such data-scarce scenarios, textual class information can serve as a valuable source of semantic supervision. Motivated by the recent success of Vision-Language Models (VLMs) in computer vision, we explore their potential for few-shot multispectral object detection. Specifically, we adapt two representative VLM-based detectors, Grounding DINO and YOLO-World, to handle multispectral inputs and propose an effective mechanism to integrate text, visual and thermal modalities. Through extensive experiments on two popular multispectral image benchmarks, FLIR and M3FD, we demonstrate that VLM-based detectors not only excel in few-shot regimes, significantly outperforming specialized multispectral models trained with comparable data, but also achieve competitive or superior results under fully supervised settings. Our findings reveal that the semantic priors learned by large-scale VLMs effectively transfer to unseen spectral modalities, ofFering a powerful pathway toward data-efficient multispectral perception.
Published: 2025-12-17T21:06:36+00:00
Venue: arXiv
Score: 0.782 (must_read)</description><content:encoded>&lt;p&gt;&lt;strong&gt;Authors:&lt;/strong&gt; Manuel Nkegoum; Minh-Tan Pham; Élisa Fromont; Bruno Avignon; Sébastien Lefèvre&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Journal:&lt;/strong&gt; arXiv&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Relevance:&lt;/strong&gt; 0.782 (must_read)&lt;/p&gt;
&lt;p&gt;Multispectral object detection is critical for safety-sensitive applications such as autonomous driving and surveillance, where robust perception under diverse illumination conditions is essential. However, the limited availability of annotated multispectral data severely restricts the training of deep detectors. In such data-scarce scenarios, textual class information can serve as a valuable source of semantic supervision. Motivated by the recent success of Vision-Language Models (VLMs) in computer vision, we explore their potential for few-shot multispectral object detection. Specifically, we adapt two representative VLM-based detectors, Grounding DINO and YOLO-World, to handle multispectral inputs and propose an effective mechanism to integrate text, visual and thermal modalities. Through extensive experiments on two popular multispectral image benchmarks, FLIR and M3FD, we demonstrate that VLM-based detectors not only excel in few-shot regimes, significantly outperforming specialized multispectral models trained with comparable data, but also achieve competitive or superior results under fully supervised settings. Our findings reveal that the semantic priors learned by large-scale VLMs effectively transfer to unseen spectral modalities, ofFering a powerful pathway toward data-efficient multispectral perception.&lt;/p&gt;</content:encoded></item><item><title>Feature Disentanglement Based on Dual-Mask-Guided Slot Attention for SAR ATR Across Backgrounds</title><link>https://doi.org/10.3390/rs18010003</link><guid>10.3390/rs18010003</guid><pubDate>Fri, 19 Dec 2025 11:03:46 +0000</pubDate><dc:creator>Ruiqiu Wang</dc:creator><dc:creator>Tao Su</dc:creator><dc:creator>Yuan Liang</dc:creator><dc:creator>Jiangtao Liu</dc:creator><prism:publicationName>Remote Sensing</prism:publicationName><prism:doi>10.3390/rs18010003</prism:doi><description>Due to the limited number of SAR samples in the dataset, current networks for SAR automatic target recognition (SAR ATR) are prone to overfitting the environmental information, which diminishes their generalization ability under cross-background conditions. However, acquiring sufficient measured data to cover the entire environmental space remains a significant challenge. This paper proposes a novel feature disentanglement network, named FDSANet. The network is designed to decouple and distinguish the features of the target from the background before classification, thereby improving its adaptability to background changes. Specifically, the network consists of two sub-networks. The first is an autoencoder sub-network based on dual-mask-guided slot attention. This sub-network utilizes target mask to guide the encoder to distinguish between target and background features. It then outputs these features as independent representations, respectively, achieving feature disentanglement. The second is a classification sub-network. It includes an encoder and a classifier, which work together to perform the classification based on the extracted target features. This network enhances the causal relationship between the target and the classification result, while mitigating the background’s interference on the classification. Moreover, the network, trained under a fixed background, demonstrates strong adaptability when applied to a new background. Experiments conducted on the Moving and Stationary Target Acquisition and Recognition (MSTAR) dataset, as well as the OpenSARShip dataset, demonstrate the superior performance of FDSANet.
Published: 2025-12-19T11:03:46+00:00
Venue: Remote Sensing
Score: 0.782 (must_read)</description><content:encoded>&lt;p&gt;&lt;strong&gt;Authors:&lt;/strong&gt; Ruiqiu Wang; Tao Su; Yuan Liang; Jiangtao Liu&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Journal:&lt;/strong&gt; Remote Sensing&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;a href="https://doi.org/10.3390/rs18010003"&gt;10.3390/rs18010003&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Relevance:&lt;/strong&gt; 0.782 (must_read)&lt;/p&gt;
&lt;p&gt;Due to the limited number of SAR samples in the dataset, current networks for SAR automatic target recognition (SAR ATR) are prone to overfitting the environmental information, which diminishes their generalization ability under cross-background conditions. However, acquiring sufficient measured data to cover the entire environmental space remains a significant challenge. This paper proposes a novel feature disentanglement network, named FDSANet. The network is designed to decouple and distinguish the features of the target from the background before classification, thereby improving its adaptability to background changes. Specifically, the network consists of two sub-networks. The first is an autoencoder sub-network based on dual-mask-guided slot attention. This sub-network utilizes target mask to guide the encoder to distinguish between target and background features. It then outputs these features as independent representations, respectively, achieving feature disentanglement. The second is a classification sub-network. It includes an encoder and a classifier, which work together to perform the classification based on the extracted target features. This network enhances the causal relationship between the target and the classification result, while mitigating the background’s interference on the classification. Moreover, the network, trained under a fixed background, demonstrates strong adaptability when applied to a new background. Experiments conducted on the Moving and Stationary Target Acquisition and Recognition (MSTAR) dataset, as well as the OpenSARShip dataset, demonstrate the superior performance of FDSANet.&lt;/p&gt;</content:encoded></item><item><title>Coastal Aquaculture Pond Recognition via Scale and Boundary Dynamic Awareness Network in Remote Sensing Imagery</title><link>https://doi.org/10.1109/tgrs.2025.3646224</link><guid>10.1109/tgrs.2025.3646224</guid><pubDate>Fri, 19 Dec 2025 18:58:35 +0000</pubDate><dc:creator>Zhanchao Huang</dc:creator><dc:creator>Junchao Cai</dc:creator><dc:creator>Wenjun Hong</dc:creator><dc:creator>Weiwang Guan</dc:creator><dc:creator>Jiajun Zhou</dc:creator><dc:creator>Hua Su</dc:creator><dc:creator>Shou Feng</dc:creator><dc:creator>Ran Tao</dc:creator><prism:publicationName>IEEE Transactions on Geoscience and Remote Sensing</prism:publicationName><prism:doi>10.1109/tgrs.2025.3646224</prism:doi><description>Coastal aquaculture ponds, vital to marine aquaculture, are widely distributed along coastlines and hold significant economic and ecological value. Accurately mapping their distribution and types is essential for optimizing marine spatial resources and protecting coastal ecosystems. While deep learning has improved detection from remote sensing imagery, precise recognition still faces challenges such as large scale and shape variations, blurred boundaries in dense areas, and a lack of high-resolution fine-grained datasets. To this end, a Scale and Boundary Dynamic Awareness Network (BDA-Net) is proposed in this paper. Specifically, A Taylor-optimized Scale-adaptive Feature Interaction (TSFI) structure is designed to achieve efficient multi-scale feature adaptation for diverse pond sizes and shapes, overcoming fixed-scale Transformer limitations while preserving accuracy with lower computation via Taylor approximation. Moreover, a Frequency-guided Adaptive Boundary Convolution (FABC) is developed to resolve boundary confusion and detail loss by adaptively fusing multi-directional frequency features, enhancing contour-aligned sampling for improved fine-grained discrimination and recognition performance. Additionally, a high-resolution fine-grained dataset is constructed to fill the gap in low spatial resolution and coarse category subdivision of existing aquaculture pond datasets. Extensive experiments on this dataset and public benchmarks demonstrate that the proposed BDA-Net achieves an mIoU of 85.71%, outperforming the existing methods by over 5.45%.
Published: 2025-12-19T18:58:35+00:00
Venue: IEEE Transactions on Geoscience and Remote Sensing
Score: 0.782 (must_read)</description><content:encoded>&lt;p&gt;&lt;strong&gt;Authors:&lt;/strong&gt; Zhanchao Huang; Junchao Cai; Wenjun Hong; Weiwang Guan; Jiajun Zhou; Hua Su; Shou Feng; Ran Tao&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Journal:&lt;/strong&gt; IEEE Transactions on Geoscience and Remote Sensing&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;a href="https://doi.org/10.1109/tgrs.2025.3646224"&gt;10.1109/tgrs.2025.3646224&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Relevance:&lt;/strong&gt; 0.782 (must_read)&lt;/p&gt;
&lt;p&gt;Coastal aquaculture ponds, vital to marine aquaculture, are widely distributed along coastlines and hold significant economic and ecological value. Accurately mapping their distribution and types is essential for optimizing marine spatial resources and protecting coastal ecosystems. While deep learning has improved detection from remote sensing imagery, precise recognition still faces challenges such as large scale and shape variations, blurred boundaries in dense areas, and a lack of high-resolution fine-grained datasets. To this end, a Scale and Boundary Dynamic Awareness Network (BDA-Net) is proposed in this paper. Specifically, A Taylor-optimized Scale-adaptive Feature Interaction (TSFI) structure is designed to achieve efficient multi-scale feature adaptation for diverse pond sizes and shapes, overcoming fixed-scale Transformer limitations while preserving accuracy with lower computation via Taylor approximation. Moreover, a Frequency-guided Adaptive Boundary Convolution (FABC) is developed to resolve boundary confusion and detail loss by adaptively fusing multi-directional frequency features, enhancing contour-aligned sampling for improved fine-grained discrimination and recognition performance. Additionally, a high-resolution fine-grained dataset is constructed to fill the gap in low spatial resolution and coarse category subdivision of existing aquaculture pond datasets. Extensive experiments on this dataset and public benchmarks demonstrate that the proposed BDA-Net achieves an mIoU of 85.71%, outperforming the existing methods by over 5.45%.&lt;/p&gt;</content:encoded></item></channel></rss>