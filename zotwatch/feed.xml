<?xml version='1.0' encoding='utf-8'?>
<rss xmlns:content="http://purl.org/rss/1.0/modules/content/" xmlns:dc="http://purl.org/dc/elements/1.1/" xmlns:prism="http://prismstandard.org/namespaces/basic/3.0/" version="2.0"><channel><title>ZotWatch Feed</title><link>https://ehehe.cn/zotwatch/</link><description>AI-assisted literature watch</description><lastBuildDate>Sat, 03 Jan 2026 02:37:54 +0000</lastBuildDate><item><title>Regional Defeats Global: An Efficient Regional Feature Fusion via Convolutional Architecture for Multispectral Object Detection</title><link>https://doi.org/10.1016/j.inffus.2025.104110</link><guid>10.1016/j.inffus.2025.104110</guid><pubDate>Fri, 02 Jan 2026 07:42:39 +0000</pubDate><dc:creator>Zhenhao Wang</dc:creator><dc:creator>Tian Tian</dc:creator><prism:publicationName>Information Fusion</prism:publicationName><prism:doi>10.1016/j.inffus.2025.104110</prism:doi><description>Multispectral object detection continues to face significant challenges in achieving a balanced optimization between accuracy and efficiency. Most existing approaches rely heavily on global modeling, which, although capable of integrating multi-band information, incurs substantial computational overhead and fails to fully exploit the spatial correlations across spectral bands. To address this issue, this paper introduces a convolutional architecture-based region feature computation mechanism that leverages the inherent advantage of convolutional operations in preserving spatial structure, enabling spatial cues to be fully retained during feature representation learning and explicitly incorporated into multispectral feature interaction. Meanwhile, by reconstructing global attention computation into localized regional modeling, the proposed method markedly reduces computational cost while maintaining effective feature fusion, thereby facilitating a lightweight architectural design. Experimental results demonstrate that the proposed module achieves the lowest computational overhead while improving mAP@50 by 1.97% and 1.66% on the DroneVehicle and VEDAI remote-sensing datasets, respectively, compared with state-of-the-art methods. Moreover, it exhibits strong applicability on the pedestrian detection datasets FLIR and LLVIP. The code is available https://github.com/wzh326/LMFFM_CARFCOM.git .
Published: 2026-01-02T07:42:39+00:00
Venue: Information Fusion
Score: 0.838 (must_read)</description><content:encoded>&lt;p&gt;&lt;strong&gt;Authors:&lt;/strong&gt; Zhenhao Wang; Tian Tian&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Journal:&lt;/strong&gt; Information Fusion&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;a href="https://doi.org/10.1016/j.inffus.2025.104110"&gt;10.1016/j.inffus.2025.104110&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Relevance:&lt;/strong&gt; 0.838 (must_read)&lt;/p&gt;
&lt;p&gt;Multispectral object detection continues to face significant challenges in achieving a balanced optimization between accuracy and efficiency. Most existing approaches rely heavily on global modeling, which, although capable of integrating multi-band information, incurs substantial computational overhead and fails to fully exploit the spatial correlations across spectral bands. To address this issue, this paper introduces a convolutional architecture-based region feature computation mechanism that leverages the inherent advantage of convolutional operations in preserving spatial structure, enabling spatial cues to be fully retained during feature representation learning and explicitly incorporated into multispectral feature interaction. Meanwhile, by reconstructing global attention computation into localized regional modeling, the proposed method markedly reduces computational cost while maintaining effective feature fusion, thereby facilitating a lightweight architectural design. Experimental results demonstrate that the proposed module achieves the lowest computational overhead while improving mAP@50 by 1.97% and 1.66% on the DroneVehicle and VEDAI remote-sensing datasets, respectively, compared with state-of-the-art methods. Moreover, it exhibits strong applicability on the pedestrian detection datasets FLIR and LLVIP. The code is available https://github.com/wzh326/LMFFM_CARFCOM.git .&lt;/p&gt;</content:encoded></item><item><title>CCAI-YOLO: A High-Precision Synthetic Aperture Radar Ship Detection Model Based on YOLOv8n Algorithm</title><link>https://doi.org/10.3390/rs18010145</link><guid>10.3390/rs18010145</guid><pubDate>Fri, 02 Jan 2026 09:18:56 +0000</pubDate><dc:creator>Hui Liu</dc:creator><dc:creator>Haoyu Dong</dc:creator><dc:creator>Hongyin Shi</dc:creator><dc:creator>Fang Li</dc:creator><prism:publicationName>Remote Sensing</prism:publicationName><prism:doi>10.3390/rs18010145</prism:doi><description>To tackle core challenges in detecting ship targets within synthetic aperture radar (SAR) images—including coherent speckle noise interference, complex background clutter, and multi-scale target distribution—this paper proposes a high-accuracy detection model, CCAI-YOLO. This model is based on the YOLOv8n framework, achieving systematic enhancements through the collaborative optimisation of key components: within the backbone network, the original C2f structure is replaced with the dynamic convolution module C2f-ODConv, improving the model’s extraction capabilities under noisy interference; the C2f-ACmix module is integrated into the neck network, introducing a self-attention mechanism to strengthen global context information modelling, thereby better distinguishing targets from structured backgrounds; the ASFF detection head optimises multi-scale feature fusion, enhancing detection consistency across different-sized targets. Concurrently, the Inner-SIoU loss function further improves bounding box regression accuracy and accelerates convergence. Experimental results demonstrate that on the public datasets SSDD and SAR-Ship-Dataset, CCAI-YOLO achieves consistent improvements over the baseline model YOLOv8n across key metrics including F1 score, mAP50, and mAP50-95. Its overall performance surpasses current mainstream SAR ship detection methods, providing an effective solution for robust and efficient ship detection in complex scenarios.
Published: 2026-01-02T09:18:56+00:00
Venue: Remote Sensing
Score: 0.821 (must_read)</description><content:encoded>&lt;p&gt;&lt;strong&gt;Authors:&lt;/strong&gt; Hui Liu; Haoyu Dong; Hongyin Shi; Fang Li&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Journal:&lt;/strong&gt; Remote Sensing&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;a href="https://doi.org/10.3390/rs18010145"&gt;10.3390/rs18010145&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Relevance:&lt;/strong&gt; 0.821 (must_read)&lt;/p&gt;
&lt;p&gt;To tackle core challenges in detecting ship targets within synthetic aperture radar (SAR) images—including coherent speckle noise interference, complex background clutter, and multi-scale target distribution—this paper proposes a high-accuracy detection model, CCAI-YOLO. This model is based on the YOLOv8n framework, achieving systematic enhancements through the collaborative optimisation of key components: within the backbone network, the original C2f structure is replaced with the dynamic convolution module C2f-ODConv, improving the model’s extraction capabilities under noisy interference; the C2f-ACmix module is integrated into the neck network, introducing a self-attention mechanism to strengthen global context information modelling, thereby better distinguishing targets from structured backgrounds; the ASFF detection head optimises multi-scale feature fusion, enhancing detection consistency across different-sized targets. Concurrently, the Inner-SIoU loss function further improves bounding box regression accuracy and accelerates convergence. Experimental results demonstrate that on the public datasets SSDD and SAR-Ship-Dataset, CCAI-YOLO achieves consistent improvements over the baseline model YOLOv8n across key metrics including F1 score, mAP50, and mAP50-95. Its overall performance surpasses current mainstream SAR ship detection methods, providing an effective solution for robust and efficient ship detection in complex scenarios.&lt;/p&gt;</content:encoded></item><item><title>Token Calibration for Transformer-based Domain Adaptation</title><link>https://doi.org/10.1109/tip.2025.3647367</link><guid>10.1109/tip.2025.3647367</guid><pubDate>Thu, 01 Jan 2026 18:39:24 +0000</pubDate><dc:creator>Xiaowei Fu</dc:creator><dc:creator>Shiyu Ye</dc:creator><dc:creator>Chenxu Zhang</dc:creator><dc:creator>Fuxiang Huang</dc:creator><dc:creator>Xin Xu</dc:creator><dc:creator>Lei Zhang</dc:creator><prism:publicationName>IEEE Transactions on Image Processing</prism:publicationName><prism:doi>10.1109/tip.2025.3647367</prism:doi><description>Unsupervised Domain Adaptation (UDA) aims to transfer knowledge from a labeled source domain to an unlabeled target domain by learning domain-invariant representations. Motivated by the recent success of Vision Transformers (ViTs), several UDA approaches have adopted ViT architectures to exploit fine-grained patch-level representations, which are unified as Transformer-based Domain Adaptation (TransDA) independent of CNN-based. However, we have a key observation in TransDA: due to inherent domain shifts, patches (tokens) from different semantic categories across domains may exhibit abnormally high similarities, which can mislead the self-attention mechanism and degrade adaptation performance. To solve that, we propose a novel Patch-Adaptation Transformer (PATrans), which first identifies similarity-anomalous patches and then adaptively suppresses their negative impact to domain alignment, i.e. token calibration. Specifically, we introduce a Patch-Adaptation Attention (PAA) mechanism to replace the standard self-attention mechanism, which consists of a weight-shared triple-branch mixed attention mechanism and a patch-level domain discriminator. The mixed attention integrates self-attention and cross-attention to enhance intra-domain feature modeling and inter-domain similarity estimation. Meanwhile, the patch-level domain discriminator quantifies the anomaly probability of each patch, enabling dynamic reweighting to mitigate the impact of unreliable patch correspondences. Furthermore, we introduce a contrastive attention regularization strategy, which leverages category-level information in a contrastive learning framework to promote class-consistent attention distributions. Extensive experiments on four benchmark datasets demonstrate that PATrans attains significant improvements over existing state-of-the-art UDA methods (e.g., 89.2% on the VisDA-2017). Code is available at: https://github.com/YSY145/PATrans.
Published: 2026-01-01T18:39:24+00:00
Venue: IEEE Transactions on Image Processing
Score: 0.814 (must_read)</description><content:encoded>&lt;p&gt;&lt;strong&gt;Authors:&lt;/strong&gt; Xiaowei Fu; Shiyu Ye; Chenxu Zhang; Fuxiang Huang; Xin Xu; Lei Zhang&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Journal:&lt;/strong&gt; IEEE Transactions on Image Processing&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;a href="https://doi.org/10.1109/tip.2025.3647367"&gt;10.1109/tip.2025.3647367&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Relevance:&lt;/strong&gt; 0.814 (must_read)&lt;/p&gt;
&lt;p&gt;Unsupervised Domain Adaptation (UDA) aims to transfer knowledge from a labeled source domain to an unlabeled target domain by learning domain-invariant representations. Motivated by the recent success of Vision Transformers (ViTs), several UDA approaches have adopted ViT architectures to exploit fine-grained patch-level representations, which are unified as Transformer-based Domain Adaptation (TransDA) independent of CNN-based. However, we have a key observation in TransDA: due to inherent domain shifts, patches (tokens) from different semantic categories across domains may exhibit abnormally high similarities, which can mislead the self-attention mechanism and degrade adaptation performance. To solve that, we propose a novel Patch-Adaptation Transformer (PATrans), which first identifies similarity-anomalous patches and then adaptively suppresses their negative impact to domain alignment, i.e. token calibration. Specifically, we introduce a Patch-Adaptation Attention (PAA) mechanism to replace the standard self-attention mechanism, which consists of a weight-shared triple-branch mixed attention mechanism and a patch-level domain discriminator. The mixed attention integrates self-attention and cross-attention to enhance intra-domain feature modeling and inter-domain similarity estimation. Meanwhile, the patch-level domain discriminator quantifies the anomaly probability of each patch, enabling dynamic reweighting to mitigate the impact of unreliable patch correspondences. Furthermore, we introduce a contrastive attention regularization strategy, which leverages category-level information in a contrastive learning framework to promote class-consistent attention distributions. Extensive experiments on four benchmark datasets demonstrate that PATrans attains significant improvements over existing state-of-the-art UDA methods (e.g., 89.2% on the VisDA-2017). Code is available at: https://github.com/YSY145/PATrans.&lt;/p&gt;</content:encoded></item><item><title>Few-Shot Object Detection on Remote Sensing Images Based on Decoupled Training, Contrastive Learning and Self-Training</title><link>https://doi.org/10.1109/jstars.2025.3650394</link><guid>10.1109/jstars.2025.3650394</guid><pubDate>Thu, 01 Jan 2026 18:37:35 +0000</pubDate><dc:creator>Shun Zhang</dc:creator><dc:creator>Xuebin Zhang</dc:creator><dc:creator>Yaohui Xu</dc:creator><dc:creator>Ke Wang</dc:creator><prism:publicationName>IEEE Journal of Selected Topics in Applied Earth Observations and Remote Sensing</prism:publicationName><prism:doi>10.1109/jstars.2025.3650394</prism:doi><description>Few-shot object detection (FSOD) in remote sensing imagery faces two critical challenges compared to general methods trained on large datasets: (1) Only a few labeled instances leveraged as the training set significantly limit the feature representation learning of deep neural networks; (2) Remote sensing image data contains complicated background and multiple objects with greatly different sizes in the same image, which leads the detector to large numbers of false alarms and miss detections. This paper proposes a FSOD framework (called DeCL-Det) that applies self-training to generate high-quality pseudo-annotations from unlabeled target domain data. These refined pseudo-labels are iteratively integrated into the training set to expand supervision for novel classes. An auxiliary network is introduced to mitigate label noise by rectifying misclassifications in pseudo-labeled regions, ensuring robust learning. For multi-scale feature learning, we propose a gradient-decoupled framework, GCFPN, combining Feature Pyramid Networks (FPN) with a Gradient Decoupled Layer (GDL). FPN is to extract multi-scale feature representations, and GDL is to decouple the modules between the Region Proposal Network (RPN) and RCNN head into two stages or tasks through gradients. The two modules, FPN and GDL, train Faster R-CNN in a decoupled way to facilitate the multi-scale feature learning of novel objects. To further enhance the classification ability, we introduce a supervised contrastive learning head to enhance feature discrimination, reinforcing robustness in few-shot object detection. Experiments on the DIOR dataset indicate that our method performs better than several existing approaches and achieves competitive results.
Published: 2026-01-01T18:37:35+00:00
Venue: IEEE Journal of Selected Topics in Applied Earth Observations and Remote Sensing
Score: 0.798 (must_read)</description><content:encoded>&lt;p&gt;&lt;strong&gt;Authors:&lt;/strong&gt; Shun Zhang; Xuebin Zhang; Yaohui Xu; Ke Wang&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Journal:&lt;/strong&gt; IEEE Journal of Selected Topics in Applied Earth Observations and Remote Sensing&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;a href="https://doi.org/10.1109/jstars.2025.3650394"&gt;10.1109/jstars.2025.3650394&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Relevance:&lt;/strong&gt; 0.798 (must_read)&lt;/p&gt;
&lt;p&gt;Few-shot object detection (FSOD) in remote sensing imagery faces two critical challenges compared to general methods trained on large datasets: (1) Only a few labeled instances leveraged as the training set significantly limit the feature representation learning of deep neural networks; (2) Remote sensing image data contains complicated background and multiple objects with greatly different sizes in the same image, which leads the detector to large numbers of false alarms and miss detections. This paper proposes a FSOD framework (called DeCL-Det) that applies self-training to generate high-quality pseudo-annotations from unlabeled target domain data. These refined pseudo-labels are iteratively integrated into the training set to expand supervision for novel classes. An auxiliary network is introduced to mitigate label noise by rectifying misclassifications in pseudo-labeled regions, ensuring robust learning. For multi-scale feature learning, we propose a gradient-decoupled framework, GCFPN, combining Feature Pyramid Networks (FPN) with a Gradient Decoupled Layer (GDL). FPN is to extract multi-scale feature representations, and GDL is to decouple the modules between the Region Proposal Network (RPN) and RCNN head into two stages or tasks through gradients. The two modules, FPN and GDL, train Faster R-CNN in a decoupled way to facilitate the multi-scale feature learning of novel objects. To further enhance the classification ability, we introduce a supervised contrastive learning head to enhance feature discrimination, reinforcing robustness in few-shot object detection. Experiments on the DIOR dataset indicate that our method performs better than several existing approaches and achieves competitive results.&lt;/p&gt;</content:encoded></item><item><title>Coupled Diffusion Posterior Sampling for Unsupervised Hyperspectral and Multispectral Images Fusion</title><link>https://doi.org/10.1109/tip.2025.3647207</link><guid>10.1109/tip.2025.3647207</guid><pubDate>Thu, 01 Jan 2026 18:39:24 +0000</pubDate><dc:creator>Yang Xu</dc:creator><dc:creator>Jian Zhu</dc:creator><dc:creator>Danfeng Hong</dc:creator><dc:creator>Zhihui Wei</dc:creator><dc:creator>Zebin Wu</dc:creator><prism:publicationName>IEEE Transactions on Image Processing</prism:publicationName><prism:doi>10.1109/tip.2025.3647207</prism:doi><description>Hyperspectral images (HSIs) and multispectral images (MSIs) fusion is a hot topic in the remote sensing society. A high-resolution HSI (HR-HSI) can be obtained by fusing a low-resolution HSI (LR-HSI) and a high-resolution MSI (HR-MSI) or RGB image. However, most deep learning-based methods require a large amount of HR-HSIs for supervised training, which is very rare in practice. In this paper, we propose a coupled diffusion posterior sampling (CDPS) method for HSI and MSI fusion in which the HR-HSIs are no longer required in the training process. Because the LR-HSI contains the spectral information and HR-MSI contains the spatial information of the captured scene, we design an unsupervised strategy that learns the required diffusion priors directly and solely from the input test image pair (the LR-HSI and HR-MSI themselves). Then, a coupled diffusion posterior sampling method is proposed to introduce the two priors in the diffusion posterior sampling which leverages the observed LR-HSI and HR-MSI as fidelity terms. Experimental results demonstrate that the proposed method outperforms other state-of-the-art unsupervised HSI and MSI fusion methods. Additionally, this method utilizes smaller networks that are simpler and easier to train without other data.
Published: 2026-01-01T18:39:24+00:00
Venue: IEEE Transactions on Image Processing
Score: 0.794 (must_read)</description><content:encoded>&lt;p&gt;&lt;strong&gt;Authors:&lt;/strong&gt; Yang Xu; Jian Zhu; Danfeng Hong; Zhihui Wei; Zebin Wu&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Journal:&lt;/strong&gt; IEEE Transactions on Image Processing&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;a href="https://doi.org/10.1109/tip.2025.3647207"&gt;10.1109/tip.2025.3647207&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Relevance:&lt;/strong&gt; 0.794 (must_read)&lt;/p&gt;
&lt;p&gt;Hyperspectral images (HSIs) and multispectral images (MSIs) fusion is a hot topic in the remote sensing society. A high-resolution HSI (HR-HSI) can be obtained by fusing a low-resolution HSI (LR-HSI) and a high-resolution MSI (HR-MSI) or RGB image. However, most deep learning-based methods require a large amount of HR-HSIs for supervised training, which is very rare in practice. In this paper, we propose a coupled diffusion posterior sampling (CDPS) method for HSI and MSI fusion in which the HR-HSIs are no longer required in the training process. Because the LR-HSI contains the spectral information and HR-MSI contains the spatial information of the captured scene, we design an unsupervised strategy that learns the required diffusion priors directly and solely from the input test image pair (the LR-HSI and HR-MSI themselves). Then, a coupled diffusion posterior sampling method is proposed to introduce the two priors in the diffusion posterior sampling which leverages the observed LR-HSI and HR-MSI as fidelity terms. Experimental results demonstrate that the proposed method outperforms other state-of-the-art unsupervised HSI and MSI fusion methods. Additionally, this method utilizes smaller networks that are simpler and easier to train without other data.&lt;/p&gt;</content:encoded></item><item><title>Semi-Supervised Diversity-Aware Domain Adaptation for 3D Object detection</title><link>https://arxiv.org/abs/2512.24922v1</link><guid>http://arxiv.org/abs/2512.24922v1</guid><pubDate>Wed, 31 Dec 2025 15:26:09 +0000</pubDate><dc:creator>Bartłomiej Olber</dc:creator><dc:creator>Jakub Winter</dc:creator><dc:creator>Paweł Wawrzyński</dc:creator><dc:creator>Andrii Gamalii</dc:creator><dc:creator>Daniel Górniak</dc:creator><dc:creator>Marcin Łojek</dc:creator><dc:creator>Robert Nowak</dc:creator><dc:creator>Krystian Radlak</dc:creator><dc:type>preprint</dc:type><prism:publicationName>arXiv</prism:publicationName><description>3D object detectors are fundamental components of perception systems in autonomous vehicles. While these detectors achieve remarkable performance on standard autonomous driving benchmarks, they often struggle to generalize across different domains - for instance, a model trained in the U.S. may perform poorly in regions like Asia or Europe. This paper presents a novel lidar domain adaptation method based on neuron activation patterns, demonstrating that state-of-the-art performance can be achieved by annotating only a small, representative, and diverse subset of samples from the target domain if they are correctly selected. The proposed approach requires very small annotation budget and, when combined with post-training techniques inspired by continual learning prevent weight drift from the original model. Empirical evaluation shows that the proposed domain adaptation approach outperforms both linear probing and state-of-the-art domain adaptation techniques.
Published: 2025-12-31T15:26:09+00:00
Venue: arXiv
Score: 0.794 (must_read)</description><content:encoded>&lt;p&gt;&lt;strong&gt;Authors:&lt;/strong&gt; Bartłomiej Olber; Jakub Winter; Paweł Wawrzyński; Andrii Gamalii; Daniel Górniak; Marcin Łojek; Robert Nowak; Krystian Radlak&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Journal:&lt;/strong&gt; arXiv&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Relevance:&lt;/strong&gt; 0.794 (must_read)&lt;/p&gt;
&lt;p&gt;3D object detectors are fundamental components of perception systems in autonomous vehicles. While these detectors achieve remarkable performance on standard autonomous driving benchmarks, they often struggle to generalize across different domains - for instance, a model trained in the U.S. may perform poorly in regions like Asia or Europe. This paper presents a novel lidar domain adaptation method based on neuron activation patterns, demonstrating that state-of-the-art performance can be achieved by annotating only a small, representative, and diverse subset of samples from the target domain if they are correctly selected. The proposed approach requires very small annotation budget and, when combined with post-training techniques inspired by continual learning prevent weight drift from the original model. Empirical evaluation shows that the proposed domain adaptation approach outperforms both linear probing and state-of-the-art domain adaptation techniques.&lt;/p&gt;</content:encoded></item><item><title>GrowSP++: Growing Superpoints and Primitives for Unsupervised 3D Semantic Segmentation</title><link>https://doi.org/10.1109/tpami.2025.3650165</link><guid>10.1109/tpami.2025.3650165</guid><pubDate>Fri, 02 Jan 2026 18:16:22 +0000</pubDate><dc:creator>Zihui Zhang</dc:creator><dc:creator>Weisheng Dai</dc:creator><dc:creator>Bing Wang</dc:creator><dc:creator>Bo Li</dc:creator><dc:creator>Bo Yang</dc:creator><prism:publicationName>IEEE Transactions on Pattern Analysis and Machine Intelligence</prism:publicationName><prism:doi>10.1109/tpami.2025.3650165</prism:doi><description>We study the problem of 3D semantic segmentation from raw point clouds. Unlike existing methods which primarily rely on a large amount of human annotations for training neural networks, we proposes GrowSP++, an unsupervised method to successfully identify complex semantic classes for every point in 3D scenes, without needing any type of human labels. Our method is composed of three major components: 1) a feature extractor incorporating 2D-3D feature distillation, 2) a superpoint constructor featuring progressively growing superpoints, and 3) a semantic primitive constructor with an additional growing strategy. The key to our method is the superpoint constructor together with the progressive growing strategy on both super points and semantic primitives, driving the feature extractor to progressively learn similar features for 3D points belonging to the same semantic class. We extensively evaluate our method on five challenging indoor and outdoor datasets, demonstrating state of-the-art performance over all unsupervised baselines. We hope our work could inspire more advanced methods for unsupervised 3D semantic learning.
Published: 2026-01-02T18:16:22+00:00
Venue: IEEE Transactions on Pattern Analysis and Machine Intelligence
Score: 0.793 (must_read)</description><content:encoded>&lt;p&gt;&lt;strong&gt;Authors:&lt;/strong&gt; Zihui Zhang; Weisheng Dai; Bing Wang; Bo Li; Bo Yang&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Journal:&lt;/strong&gt; IEEE Transactions on Pattern Analysis and Machine Intelligence&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;a href="https://doi.org/10.1109/tpami.2025.3650165"&gt;10.1109/tpami.2025.3650165&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Relevance:&lt;/strong&gt; 0.793 (must_read)&lt;/p&gt;
&lt;p&gt;We study the problem of 3D semantic segmentation from raw point clouds. Unlike existing methods which primarily rely on a large amount of human annotations for training neural networks, we proposes GrowSP++, an unsupervised method to successfully identify complex semantic classes for every point in 3D scenes, without needing any type of human labels. Our method is composed of three major components: 1) a feature extractor incorporating 2D-3D feature distillation, 2) a superpoint constructor featuring progressively growing superpoints, and 3) a semantic primitive constructor with an additional growing strategy. The key to our method is the superpoint constructor together with the progressive growing strategy on both super points and semantic primitives, driving the feature extractor to progressively learn similar features for 3D points belonging to the same semantic class. We extensively evaluate our method on five challenging indoor and outdoor datasets, demonstrating state of-the-art performance over all unsupervised baselines. We hope our work could inspire more advanced methods for unsupervised 3D semantic learning.&lt;/p&gt;</content:encoded></item><item><title>Reflectance Prediction-based Knowledge Distillation for Robust 3D Object Detection in Compressed Point Clouds</title><link>https://doi.org/10.1109/tip.2025.3648203</link><guid>10.1109/tip.2025.3648203</guid><pubDate>Fri, 02 Jan 2026 18:17:51 +0000</pubDate><dc:creator>Hao Jing</dc:creator><dc:creator>Anhong Wang</dc:creator><dc:creator>Yifan Zhang</dc:creator><dc:creator>Donghan Bu</dc:creator><dc:creator>Junhui Hou</dc:creator><prism:publicationName>IEEE Transactions on Image Processing</prism:publicationName><prism:doi>10.1109/tip.2025.3648203</prism:doi><description>Regarding intelligent transportation systems, low-bitrate transmission via lossy point cloud compression is vital for facilitating real-time collaborative perception among connected agents, such as vehicles and infrastructures, under restricted bandwidth. In existing compression transmission systems, the sender lossily compresses point coordinates and reflectance to generate a transmission code stream, which faces transmission burdens from reflectance encoding and limited detection robustness due to information loss. To address these issues, this paper proposes a 3D object detection framework with reflectance prediction-based knowledge distillation (RPKD). We compress point coordinates while discarding reflectance during low-bitrate transmission, and feed the decoded non-reflectance compressed point clouds into a student detector. The discarded reflectance is then reconstructed by a geometry-based reflectance prediction (RP) module within the student detector for precise detection. A teacher detector with the same structure as the student detector is designed for performing reflectance knowledge distillation (RKD) and detection knowledge distillation (DKD) from raw to compressed point clouds. Our cross-source distillation training strategy (CDTS) equips the student detector with robustness to low-quality compressed data while preserving the accuracy benefits of raw data through transferred distillation knowledge. Experimental results on the KITTI and DAIR-V2X-V datasets demonstrate that our method can boost detection accuracy for compressed point clouds across multiple code rates. We will release the code publicly at https://github.com/HaoJing-SX/RPKD.
Published: 2026-01-02T18:17:51+00:00
Venue: IEEE Transactions on Image Processing
Score: 0.791 (must_read)</description><content:encoded>&lt;p&gt;&lt;strong&gt;Authors:&lt;/strong&gt; Hao Jing; Anhong Wang; Yifan Zhang; Donghan Bu; Junhui Hou&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Journal:&lt;/strong&gt; IEEE Transactions on Image Processing&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;a href="https://doi.org/10.1109/tip.2025.3648203"&gt;10.1109/tip.2025.3648203&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Relevance:&lt;/strong&gt; 0.791 (must_read)&lt;/p&gt;
&lt;p&gt;Regarding intelligent transportation systems, low-bitrate transmission via lossy point cloud compression is vital for facilitating real-time collaborative perception among connected agents, such as vehicles and infrastructures, under restricted bandwidth. In existing compression transmission systems, the sender lossily compresses point coordinates and reflectance to generate a transmission code stream, which faces transmission burdens from reflectance encoding and limited detection robustness due to information loss. To address these issues, this paper proposes a 3D object detection framework with reflectance prediction-based knowledge distillation (RPKD). We compress point coordinates while discarding reflectance during low-bitrate transmission, and feed the decoded non-reflectance compressed point clouds into a student detector. The discarded reflectance is then reconstructed by a geometry-based reflectance prediction (RP) module within the student detector for precise detection. A teacher detector with the same structure as the student detector is designed for performing reflectance knowledge distillation (RKD) and detection knowledge distillation (DKD) from raw to compressed point clouds. Our cross-source distillation training strategy (CDTS) equips the student detector with robustness to low-quality compressed data while preserving the accuracy benefits of raw data through transferred distillation knowledge. Experimental results on the KITTI and DAIR-V2X-V datasets demonstrate that our method can boost detection accuracy for compressed point clouds across multiple code rates. We will release the code publicly at https://github.com/HaoJing-SX/RPKD.&lt;/p&gt;</content:encoded></item><item><title>HVTC-GAN: A High-level Vision Task Cooperative GAN for SAR-to-Optical translation via Semantic Segmentation</title><link>https://doi.org/10.1109/jstars.2025.3650182</link><guid>10.1109/jstars.2025.3650182</guid><pubDate>Fri, 02 Jan 2026 18:16:44 +0000</pubDate><dc:creator>Hang Liu</dc:creator><dc:creator>Jiarui Lin</dc:creator><dc:creator>Cang Gu</dc:creator><dc:creator>Yujie Zhang</dc:creator><dc:creator>Huihui Li</dc:creator><prism:publicationName>IEEE Journal of Selected Topics in Applied Earth Observations and Remote Sensing</prism:publicationName><prism:doi>10.1109/jstars.2025.3650182</prism:doi><description>Synthetic Aperture Radar (SAR), an active remote sensing technology that can be deployed on various aerial plat forms, generates images by emitting microwaves and analyzing the intensity of backscattered signals. The penetration capability of microwaves enables observation of Earth at all weather, day and night, broadening the application of SAR. However, SAR image interpretation remains challenging for non-experts. In contrast, optical images provide intuitive visual features but are weather-sensitive. To synergize these modalities, SAR-to-Optical Translation (S2OT) has gained research attention, but most studies prioritize visual quality or similarity metrics over practical applicability to downstream tasks. This study proposes a high level vision task-coordinated S2OT framework to address this gap. Semantic segmentation, emulating land cover classification, is integrated as the downstream task. Semantic segmentation loss guides the network to generate optical images that enhance task-relevant features. To preserve structural information in SAR images, we introduce SSIM loss and incorporate SAR derived semantic segmentation maps as auxiliary inputs. An identity loss further aligns the distributions of generated and real optical images, mitigating domain discrepancies. Extensive experiments confirm that S2OT improves downstream land cover classification. The inclusion of task-specific losses elevates translation quality: our HVTC-GAN surpasses the baseline methods in SSIM and PSNR metrics. Ablation studies validate the effectiveness of co-training S2OT with high-level vision tasks, demonstrating that task-oriented constraints enhance both translation fidelity and downstream utility. Code will be available at https://github.com/NWPU-LHH/HVTC-GAN
Published: 2026-01-02T18:16:44+00:00
Venue: IEEE Journal of Selected Topics in Applied Earth Observations and Remote Sensing
Score: 0.790 (must_read)</description><content:encoded>&lt;p&gt;&lt;strong&gt;Authors:&lt;/strong&gt; Hang Liu; Jiarui Lin; Cang Gu; Yujie Zhang; Huihui Li&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Journal:&lt;/strong&gt; IEEE Journal of Selected Topics in Applied Earth Observations and Remote Sensing&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;a href="https://doi.org/10.1109/jstars.2025.3650182"&gt;10.1109/jstars.2025.3650182&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Relevance:&lt;/strong&gt; 0.790 (must_read)&lt;/p&gt;
&lt;p&gt;Synthetic Aperture Radar (SAR), an active remote sensing technology that can be deployed on various aerial plat forms, generates images by emitting microwaves and analyzing the intensity of backscattered signals. The penetration capability of microwaves enables observation of Earth at all weather, day and night, broadening the application of SAR. However, SAR image interpretation remains challenging for non-experts. In contrast, optical images provide intuitive visual features but are weather-sensitive. To synergize these modalities, SAR-to-Optical Translation (S2OT) has gained research attention, but most studies prioritize visual quality or similarity metrics over practical applicability to downstream tasks. This study proposes a high level vision task-coordinated S2OT framework to address this gap. Semantic segmentation, emulating land cover classification, is integrated as the downstream task. Semantic segmentation loss guides the network to generate optical images that enhance task-relevant features. To preserve structural information in SAR images, we introduce SSIM loss and incorporate SAR derived semantic segmentation maps as auxiliary inputs. An identity loss further aligns the distributions of generated and real optical images, mitigating domain discrepancies. Extensive experiments confirm that S2OT improves downstream land cover classification. The inclusion of task-specific losses elevates translation quality: our HVTC-GAN surpasses the baseline methods in SSIM and PSNR metrics. Ablation studies validate the effectiveness of co-training S2OT with high-level vision tasks, demonstrating that task-oriented constraints enhance both translation fidelity and downstream utility. Code will be available at https://github.com/NWPU-LHH/HVTC-GAN&lt;/p&gt;</content:encoded></item><item><title>An Empirical Analysis of Deep Learning Methods for Small Object Detection from Satellite Imagery</title><link>https://doi.org/10.1016/j.eswa.2025.131061</link><guid>10.1016/j.eswa.2025.131061</guid><pubDate>Fri, 02 Jan 2026 23:48:17 +0000</pubDate><dc:creator>Xiaohui Yuan</dc:creator><dc:creator>Aniv Chakravarty</dc:creator><dc:creator>Elinor M. Lichtenberg</dc:creator><dc:creator>Lichuan Gu</dc:creator><dc:creator>Zhenchun Wei</dc:creator><dc:creator>Tian Chen</dc:creator><prism:publicationName>Expert Systems with Applications</prism:publicationName><prism:doi>10.1016/j.eswa.2025.131061</prism:doi><description>Despite a substantial body of literature on object detection, there is a notable lack of empirical studies on detecting small objects. Additionally, the definition of a small object remains unclear. This paper presents a thorough evaluation of six state-of-the-art deep learning methods for small object detection from satellite imagery. Three public high-resolution datasets are used to understand various influential aspects and the generalization ability. Among the six methods, YOLOv11 achieves a balanced performance for localization and adaptability, while Faster R-CNN maintains consistent detection coverage. Anchor box-based methods require extensive fine-tuning, whereas transformer-based methods demand greater computational resources to achieve competitive results. In addition, anchor-based methods, including SSD, Faster R-CNN, and Cascade R-CNN, are sensitive to the anchor box size, and, for small object detection, a small to moderate size is preferred. Both deformable and RT-DETR methods are susceptible to overfitting. RT-DETR exhibits superior detection in partial occlusion scenarios, particularly through vegetation and shadows, whereas deformable DETR struggles to identify individual small objects in dense clusters. Comparing computational efficiency with a batch size of one reveals that RT-DETR and YOLOv11 are more training-intensive, with optimizations focused on inference. Methods such as Faster R-CNN have a larger memory footprint but lower computational costs and time requirements.
Published: 2026-01-02T23:48:17+00:00
Venue: Expert Systems with Applications
Score: 0.790 (must_read)</description><content:encoded>&lt;p&gt;&lt;strong&gt;Authors:&lt;/strong&gt; Xiaohui Yuan; Aniv Chakravarty; Elinor M. Lichtenberg; Lichuan Gu; Zhenchun Wei; Tian Chen&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Journal:&lt;/strong&gt; Expert Systems with Applications&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;a href="https://doi.org/10.1016/j.eswa.2025.131061"&gt;10.1016/j.eswa.2025.131061&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Relevance:&lt;/strong&gt; 0.790 (must_read)&lt;/p&gt;
&lt;p&gt;Despite a substantial body of literature on object detection, there is a notable lack of empirical studies on detecting small objects. Additionally, the definition of a small object remains unclear. This paper presents a thorough evaluation of six state-of-the-art deep learning methods for small object detection from satellite imagery. Three public high-resolution datasets are used to understand various influential aspects and the generalization ability. Among the six methods, YOLOv11 achieves a balanced performance for localization and adaptability, while Faster R-CNN maintains consistent detection coverage. Anchor box-based methods require extensive fine-tuning, whereas transformer-based methods demand greater computational resources to achieve competitive results. In addition, anchor-based methods, including SSD, Faster R-CNN, and Cascade R-CNN, are sensitive to the anchor box size, and, for small object detection, a small to moderate size is preferred. Both deformable and RT-DETR methods are susceptible to overfitting. RT-DETR exhibits superior detection in partial occlusion scenarios, particularly through vegetation and shadows, whereas deformable DETR struggles to identify individual small objects in dense clusters. Comparing computational efficiency with a batch size of one reveals that RT-DETR and YOLOv11 are more training-intensive, with optimizations focused on inference. Methods such as Faster R-CNN have a larger memory footprint but lower computational costs and time requirements.&lt;/p&gt;</content:encoded></item><item><title>FUSE-RSVLM: Feature Fusion Vision-Language Model for Remote Sensing</title><link>https://arxiv.org/abs/2512.24022v1</link><guid>http://arxiv.org/abs/2512.24022v1</guid><pubDate>Tue, 30 Dec 2025 06:48:07 +0000</pubDate><dc:creator>Yunkai Dang</dc:creator><dc:creator>Donghao Wang</dc:creator><dc:creator>Jiacheng Yang</dc:creator><dc:creator>Yifan Jiang</dc:creator><dc:creator>Meiyi Zhu</dc:creator><dc:creator>Yuekun Yang</dc:creator><dc:creator>Cong Wang</dc:creator><dc:creator>Qi Fan</dc:creator><dc:creator>Wenbin Li</dc:creator><dc:creator>Yang Gao</dc:creator><dc:type>preprint</dc:type><prism:publicationName>arXiv</prism:publicationName><description>Large vision-language models (VLMs) exhibit strong performance across various tasks. However, these VLMs encounter significant challenges when applied to the remote sensing domain due to the inherent differences between remote sensing images and natural images. Existing remote sensing VLMs often fail to extract fine-grained visual features and suffer from visual forgetting during deep language processing. To address this, we introduce MF-RSVLM, a Multi-Feature Fusion Remote Sensing Vision--Language Model that effectively extracts and fuses visual features for RS understanding. MF-RSVLM learns multi-scale visual representations and combines global context with local details, improving the capture of small and complex structures in RS scenes. A recurrent visual feature injection scheme ensures the language model remains grounded in visual evidence and reduces visual forgetting during generation. Extensive experiments on diverse RS benchmarks show that MF-RSVLM achieves state-of-the-art or highly competitive performance across remote sensing classification, image captioning, and VQA tasks. Our code is publicly available at https://github.com/Yunkaidang/RSVLM.
Published: 2025-12-30T06:48:07+00:00
Venue: arXiv
Score: 0.785 (must_read)</description><content:encoded>&lt;p&gt;&lt;strong&gt;Authors:&lt;/strong&gt; Yunkai Dang; Donghao Wang; Jiacheng Yang; Yifan Jiang; Meiyi Zhu; Yuekun Yang; Cong Wang; Qi Fan; Wenbin Li; Yang Gao&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Journal:&lt;/strong&gt; arXiv&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Relevance:&lt;/strong&gt; 0.785 (must_read)&lt;/p&gt;
&lt;p&gt;Large vision-language models (VLMs) exhibit strong performance across various tasks. However, these VLMs encounter significant challenges when applied to the remote sensing domain due to the inherent differences between remote sensing images and natural images. Existing remote sensing VLMs often fail to extract fine-grained visual features and suffer from visual forgetting during deep language processing. To address this, we introduce MF-RSVLM, a Multi-Feature Fusion Remote Sensing Vision--Language Model that effectively extracts and fuses visual features for RS understanding. MF-RSVLM learns multi-scale visual representations and combines global context with local details, improving the capture of small and complex structures in RS scenes. A recurrent visual feature injection scheme ensures the language model remains grounded in visual evidence and reduces visual forgetting during generation. Extensive experiments on diverse RS benchmarks show that MF-RSVLM achieves state-of-the-art or highly competitive performance across remote sensing classification, image captioning, and VQA tasks. Our code is publicly available at https://github.com/Yunkaidang/RSVLM.&lt;/p&gt;</content:encoded></item><item><title>IGCDet: Independence Guided Co-Training for Sparsely Annotated Object Detection</title><link>https://doi.org/10.1016/j.knosys.2025.115217</link><guid>10.1016/j.knosys.2025.115217</guid><pubDate>Fri, 02 Jan 2026 07:39:05 +0000</pubDate><dc:creator>Jian-Xun Mi</dc:creator><dc:creator>Jiahui Feng</dc:creator><dc:creator>Haiyang Wang</dc:creator><dc:creator>Yanjun Wu</dc:creator><dc:creator>Ranzhi Zhao</dc:creator><dc:creator>Chang Liu</dc:creator><prism:publicationName>Knowledge-Based Systems</prism:publicationName><prism:doi>10.1016/j.knosys.2025.115217</prism:doi><description>Object detection models can achieve excellent detection performance with fully annotated instances. However, requiring complete annotations for every dataset is impractical due to high labor and time costs, as well as the inevitable occurrence of missing annotations. As a result, the absence of annotations can potentially provide misleading supervision and harm the training process. Recent methodologies have achieved remarkable effectiveness through the application of Co-Mining. However, the independence of each branch in Co-Mining cannot be guaranteed, overlooking valuable information during multi-perspective training. To address this issue, we introduce an Independence Guided Co-Training Model (IGCDet) that leverages Image Independence Decomposition to ensure the independence of each co-training branch. This model aims to capture diverse perspectives from images as extensively as possible, identifying missing annotations and incorporating them as positive supervision in the training process. Additionally, we propose the use of Joint-Confidence, derived from the combination of classification and regression, as pseudo-label scores, effectively mitigating issues associated with pseudo-label bias. Extensive experiments have verified the effectiveness of the proposed method.
Published: 2026-01-02T07:39:05+00:00
Venue: Knowledge-Based Systems
Score: 0.783 (must_read)</description><content:encoded>&lt;p&gt;&lt;strong&gt;Authors:&lt;/strong&gt; Jian-Xun Mi; Jiahui Feng; Haiyang Wang; Yanjun Wu; Ranzhi Zhao; Chang Liu&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Journal:&lt;/strong&gt; Knowledge-Based Systems&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;a href="https://doi.org/10.1016/j.knosys.2025.115217"&gt;10.1016/j.knosys.2025.115217&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Relevance:&lt;/strong&gt; 0.783 (must_read)&lt;/p&gt;
&lt;p&gt;Object detection models can achieve excellent detection performance with fully annotated instances. However, requiring complete annotations for every dataset is impractical due to high labor and time costs, as well as the inevitable occurrence of missing annotations. As a result, the absence of annotations can potentially provide misleading supervision and harm the training process. Recent methodologies have achieved remarkable effectiveness through the application of Co-Mining. However, the independence of each branch in Co-Mining cannot be guaranteed, overlooking valuable information during multi-perspective training. To address this issue, we introduce an Independence Guided Co-Training Model (IGCDet) that leverages Image Independence Decomposition to ensure the independence of each co-training branch. This model aims to capture diverse perspectives from images as extensively as possible, identifying missing annotations and incorporating them as positive supervision in the training process. Additionally, we propose the use of Joint-Confidence, derived from the combination of classification and regression, as pseudo-label scores, effectively mitigating issues associated with pseudo-label bias. Extensive experiments have verified the effectiveness of the proposed method.&lt;/p&gt;</content:encoded></item><item><title>MEFPNet: A multi-scale enhanced feature pyramid network for similarity-confounded substation surface-defect detection</title><link>https://doi.org/10.1016/j.neucom.2025.132598</link><guid>10.1016/j.neucom.2025.132598</guid><pubDate>Fri, 02 Jan 2026 16:45:38 +0000</pubDate><dc:creator>Yunfei Zhou</dc:creator><dc:creator>Quanbo Ge</dc:creator><dc:creator>Mingchuan Zhang</dc:creator><dc:creator>Xinliang He</dc:creator><dc:creator>Kuan Wang</dc:creator><prism:publicationName>Neurocomputing</prism:publicationName><prism:doi>10.1016/j.neucom.2025.132598</prism:doi><description>Substation surface-defect detection remains a challenging task due to the coexistence of domain and illumination shifts, large scale variation, and visually similar backgrounds. To address these issues, this paper proposes a Multi-attention Enhanced Feature Pyramid Network (MEFPNet), a detection framework tailored for similarity-confounded substation equipment inspection. First, a Context-Aware Feature Aggregation Module (CAFAM) is designed to enhance the perception of large-scale structures while preserving fine-grained local cues in complex backgrounds. Second, a Learnable Weighted Feature Pyramid Network (LWFPN) is introduced to adaptively select and reweight hierarchical features, thereby improving cross-scale interaction. Third, the original AIFI module is replaced with a lightweight Simplified Spatial Pyramid Pooling (SimSPPF) block to capture rich spatial context at low computational cost. Experiments are conducted on two datasets—the Substation Dataset and the YOLO Annotated 15-class Ground Truth Dataset for Substation Equipment. Results show that MEFPNet achieves superior accuracy and robustness compared with baseline detectors. Specifically, on the Substation dataset, MEFPNet improves mAP@50 by 6.64% and mAP@50:95 by 3.65%, demonstrating its effectiveness and applicability in real-world substation defect detection scenarios.
Published: 2026-01-02T16:45:38+00:00
Venue: Neurocomputing
Score: 0.782 (must_read)</description><content:encoded>&lt;p&gt;&lt;strong&gt;Authors:&lt;/strong&gt; Yunfei Zhou; Quanbo Ge; Mingchuan Zhang; Xinliang He; Kuan Wang&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Journal:&lt;/strong&gt; Neurocomputing&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;a href="https://doi.org/10.1016/j.neucom.2025.132598"&gt;10.1016/j.neucom.2025.132598&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Relevance:&lt;/strong&gt; 0.782 (must_read)&lt;/p&gt;
&lt;p&gt;Substation surface-defect detection remains a challenging task due to the coexistence of domain and illumination shifts, large scale variation, and visually similar backgrounds. To address these issues, this paper proposes a Multi-attention Enhanced Feature Pyramid Network (MEFPNet), a detection framework tailored for similarity-confounded substation equipment inspection. First, a Context-Aware Feature Aggregation Module (CAFAM) is designed to enhance the perception of large-scale structures while preserving fine-grained local cues in complex backgrounds. Second, a Learnable Weighted Feature Pyramid Network (LWFPN) is introduced to adaptively select and reweight hierarchical features, thereby improving cross-scale interaction. Third, the original AIFI module is replaced with a lightweight Simplified Spatial Pyramid Pooling (SimSPPF) block to capture rich spatial context at low computational cost. Experiments are conducted on two datasets—the Substation Dataset and the YOLO Annotated 15-class Ground Truth Dataset for Substation Equipment. Results show that MEFPNet achieves superior accuracy and robustness compared with baseline detectors. Specifically, on the Substation dataset, MEFPNet improves mAP@50 by 6.64% and mAP@50:95 by 3.65%, demonstrating its effectiveness and applicability in real-world substation defect detection scenarios.&lt;/p&gt;</content:encoded></item><item><title>HyPyraMamba: A Pyramid Spectral Attention and Mamba-Based Architecture for Robust Hyperspectral Image Classification</title><link>https://doi.org/10.1109/tgrs.2025.3650350</link><guid>10.1109/tgrs.2025.3650350</guid><pubDate>Thu, 01 Jan 2026 18:37:16 +0000</pubDate><dc:creator>Dekai Li</dc:creator><dc:creator>Uzair Aslam Bhatti</dc:creator><dc:creator>Mengxing Huang</dc:creator><dc:creator>Lorenzo Bruzzone</dc:creator><dc:creator>Jiaxin Li</dc:creator><prism:publicationName>IEEE Transactions on Geoscience and Remote Sensing</prism:publicationName><prism:doi>10.1109/tgrs.2025.3650350</prism:doi><description>In hyperspectral image (HSI) classification, the high-dimensionality and the complex coupling of spatial-spectral features present severe challenges to existing deep learning methods in terms of accuracy, generalization, and computational efficiency. Researchers have recently explored CNN and Transformer-based methods to overcome these limitations, but CNN's limited receptive field prevents effective modeling of long-range dependencies, while Transformers suffer from high computational cost and inefficiency in high-dimensional data. Motivated by these limitations, the state-space model (SSM) Mamba shows great potential as an efficient alternative for sequence and dependency modeling. Building on this foundation, we propose HyPyraMamba, a novel architecture designed to effectively overcome the above challenges. It integrates the Pyramid Spectral Attention (PSA) module to capture multi-scale key spectral features, thereby reducing interference caused by spectral redundancy. We developed an Adaptive Expert Depthwise Convolution (AEDC) module that enhances the model's ability to express multi-scale spatial-spectral features, and a sequence modeling module, Mamba. In the Mamba module, we utilize the spatial Mamba and spectral Mamba branches to enhance spatial structure and spectral correlation modeling. Extensive experiments on four benchmark HSI datasets demonstrate that HyPyraMamba significantly outperforms several recent state-of-the-art methods and provides a favorable accuracy–efficiency trade-off. In particular, class-wise analyses on spectrally similar land-cover categories (e.g., different soybean and bareland types) show that HyPyraMamba markedly reduces mutual confusion compared with CNN-, Transformer-, and Mamba-based baselines. The code will be available at https://github.com/dekai-li/HyPyraMamba.
Published: 2026-01-01T18:37:16+00:00
Venue: IEEE Transactions on Geoscience and Remote Sensing
Score: 0.781 (must_read)</description><content:encoded>&lt;p&gt;&lt;strong&gt;Authors:&lt;/strong&gt; Dekai Li; Uzair Aslam Bhatti; Mengxing Huang; Lorenzo Bruzzone; Jiaxin Li&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Journal:&lt;/strong&gt; IEEE Transactions on Geoscience and Remote Sensing&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;a href="https://doi.org/10.1109/tgrs.2025.3650350"&gt;10.1109/tgrs.2025.3650350&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Relevance:&lt;/strong&gt; 0.781 (must_read)&lt;/p&gt;
&lt;p&gt;In hyperspectral image (HSI) classification, the high-dimensionality and the complex coupling of spatial-spectral features present severe challenges to existing deep learning methods in terms of accuracy, generalization, and computational efficiency. Researchers have recently explored CNN and Transformer-based methods to overcome these limitations, but CNN&amp;#x27;s limited receptive field prevents effective modeling of long-range dependencies, while Transformers suffer from high computational cost and inefficiency in high-dimensional data. Motivated by these limitations, the state-space model (SSM) Mamba shows great potential as an efficient alternative for sequence and dependency modeling. Building on this foundation, we propose HyPyraMamba, a novel architecture designed to effectively overcome the above challenges. It integrates the Pyramid Spectral Attention (PSA) module to capture multi-scale key spectral features, thereby reducing interference caused by spectral redundancy. We developed an Adaptive Expert Depthwise Convolution (AEDC) module that enhances the model&amp;#x27;s ability to express multi-scale spatial-spectral features, and a sequence modeling module, Mamba. In the Mamba module, we utilize the spatial Mamba and spectral Mamba branches to enhance spatial structure and spectral correlation modeling. Extensive experiments on four benchmark HSI datasets demonstrate that HyPyraMamba significantly outperforms several recent state-of-the-art methods and provides a favorable accuracy–efficiency trade-off. In particular, class-wise analyses on spectrally similar land-cover categories (e.g., different soybean and bareland types) show that HyPyraMamba markedly reduces mutual confusion compared with CNN-, Transformer-, and Mamba-based baselines. The code will be available at https://github.com/dekai-li/HyPyraMamba.&lt;/p&gt;</content:encoded></item><item><title>RankSAM: Lightweight adapters and prompt generation in zero-shot semantic segmentation</title><link>https://doi.org/10.1016/j.neucom.2025.132594</link><guid>10.1016/j.neucom.2025.132594</guid><pubDate>Fri, 02 Jan 2026 16:07:19 +0000</pubDate><dc:creator>Yue Zhuo</dc:creator><dc:creator>Zhaocheng Xu</dc:creator><dc:creator>Di Zhou</dc:creator><dc:creator>Pengpeng Xu</dc:creator><dc:creator>Yan Tian</dc:creator><prism:publicationName>Neurocomputing</prism:publicationName><prism:doi>10.1016/j.neucom.2025.132594</prism:doi><description>Zero-shot segmentation plays a crucial role in neurocomputing, such as embodied intelligence systems and autonomous driving technologies. However, current approaches struggle to preserve the intrinsic generalization ability of SAM as input quality declines. In addition, prompt generation still faces an embarrassment in the balance between effectiveness and efficiency. Motivated by low-rank adaptation (LoRA), we design RankSAM, which integrates slim, adaptable modules into the middle layers of the frozen SAM framework. These modules dynamically fine-tune the operational rank of their weight matrices in response to input data, leveraging a trainable gating mechanism to selectively activate specific (rank-1) matrix components as needed. In addition, a learnable prompt predictor is designed to learn and generate prompt confidence maps and point prompts, and any remaining prompts that would produce the same mask are filtered out to enhance efficiency in prompt generation. The experimental results on multiple datasets indicate that our approach improves the mean intersection over union (mIoU) by a margin of 2.5%–2.8% compared to the prevailing approaches. Project page: https://messeyamumu.github.io/RankSAM .
Published: 2026-01-02T16:07:19+00:00
Venue: Neurocomputing
Score: 0.781 (must_read)</description><content:encoded>&lt;p&gt;&lt;strong&gt;Authors:&lt;/strong&gt; Yue Zhuo; Zhaocheng Xu; Di Zhou; Pengpeng Xu; Yan Tian&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Journal:&lt;/strong&gt; Neurocomputing&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;a href="https://doi.org/10.1016/j.neucom.2025.132594"&gt;10.1016/j.neucom.2025.132594&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Relevance:&lt;/strong&gt; 0.781 (must_read)&lt;/p&gt;
&lt;p&gt;Zero-shot segmentation plays a crucial role in neurocomputing, such as embodied intelligence systems and autonomous driving technologies. However, current approaches struggle to preserve the intrinsic generalization ability of SAM as input quality declines. In addition, prompt generation still faces an embarrassment in the balance between effectiveness and efficiency. Motivated by low-rank adaptation (LoRA), we design RankSAM, which integrates slim, adaptable modules into the middle layers of the frozen SAM framework. These modules dynamically fine-tune the operational rank of their weight matrices in response to input data, leveraging a trainable gating mechanism to selectively activate specific (rank-1) matrix components as needed. In addition, a learnable prompt predictor is designed to learn and generate prompt confidence maps and point prompts, and any remaining prompts that would produce the same mask are filtered out to enhance efficiency in prompt generation. The experimental results on multiple datasets indicate that our approach improves the mean intersection over union (mIoU) by a margin of 2.5%–2.8% compared to the prevailing approaches. Project page: https://messeyamumu.github.io/RankSAM .&lt;/p&gt;</content:encoded></item><item><title>OFL-SAM2: Prompt SAM2 with Online Few-shot Learner for Efficient Medical Image Segmentation</title><link>https://arxiv.org/abs/2512.24861v1</link><guid>http://arxiv.org/abs/2512.24861v1</guid><pubDate>Wed, 31 Dec 2025 13:41:16 +0000</pubDate><dc:creator>Meng Lan</dc:creator><dc:creator>Lefei Zhang</dc:creator><dc:creator>Xiaomeng Li</dc:creator><dc:type>preprint</dc:type><prism:publicationName>arXiv</prism:publicationName><description>The Segment Anything Model 2 (SAM2) has demonstrated remarkable promptable visual segmentation capabilities in video data, showing potential for extension to medical image segmentation (MIS) tasks involving 3D volumes and temporally correlated 2D image sequences. However, adapting SAM2 to MIS presents several challenges, including the need for extensive annotated medical data for fine-tuning and high-quality manual prompts, which are both labor-intensive and require intervention from medical experts. To address these challenges, we introduce OFL-SAM2, a prompt-free SAM2 framework for label-efficient MIS. Our core idea is to leverage limited annotated samples to train a lightweight mapping network that captures medical knowledge and transforms generic image features into target features, thereby providing additional discriminative target representations for each frame and eliminating the need for manual prompts. Crucially, the mapping network supports online parameter update during inference, enhancing the model's generalization across test sequences. Technically, we introduce two key components: (1) an online few-shot learner that trains the mapping network to generate target features using limited data, and (2) an adaptive fusion module that dynamically integrates the target features with the memory-attention features generated by frozen SAM2, leading to accurate and robust target representation. Extensive experiments on three diverse MIS datasets demonstrate that OFL-SAM2 achieves state-of-the-art performance with limited training data.
Published: 2025-12-31T13:41:16+00:00
Venue: arXiv
Score: 0.780 (must_read)</description><content:encoded>&lt;p&gt;&lt;strong&gt;Authors:&lt;/strong&gt; Meng Lan; Lefei Zhang; Xiaomeng Li&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Journal:&lt;/strong&gt; arXiv&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Relevance:&lt;/strong&gt; 0.780 (must_read)&lt;/p&gt;
&lt;p&gt;The Segment Anything Model 2 (SAM2) has demonstrated remarkable promptable visual segmentation capabilities in video data, showing potential for extension to medical image segmentation (MIS) tasks involving 3D volumes and temporally correlated 2D image sequences. However, adapting SAM2 to MIS presents several challenges, including the need for extensive annotated medical data for fine-tuning and high-quality manual prompts, which are both labor-intensive and require intervention from medical experts. To address these challenges, we introduce OFL-SAM2, a prompt-free SAM2 framework for label-efficient MIS. Our core idea is to leverage limited annotated samples to train a lightweight mapping network that captures medical knowledge and transforms generic image features into target features, thereby providing additional discriminative target representations for each frame and eliminating the need for manual prompts. Crucially, the mapping network supports online parameter update during inference, enhancing the model&amp;#x27;s generalization across test sequences. Technically, we introduce two key components: (1) an online few-shot learner that trains the mapping network to generate target features using limited data, and (2) an adaptive fusion module that dynamically integrates the target features with the memory-attention features generated by frozen SAM2, leading to accurate and robust target representation. Extensive experiments on three diverse MIS datasets demonstrate that OFL-SAM2 achieves state-of-the-art performance with limited training data.&lt;/p&gt;</content:encoded></item><item><title>DAPU: Distribution-aware patch upsampling for point cloud based 3D object detection</title><link>https://doi.org/10.1016/j.neucom.2025.132579</link><guid>10.1016/j.neucom.2025.132579</guid><pubDate>Fri, 02 Jan 2026 16:45:38 +0000</pubDate><dc:creator>Yinghao Hu</dc:creator><dc:creator>Yan Wu</dc:creator><dc:creator>Yujian Mo</dc:creator><dc:creator>Jijun Wang</dc:creator><dc:creator>Yuwei Zhang</dc:creator><prism:publicationName>Neurocomputing</prism:publicationName><prism:doi>10.1016/j.neucom.2025.132579</prism:doi><description>The sparsity and quality of point clouds significantly constrain the development of LiDAR-based 3D object detectors. Previous approaches supplemented point clouds through depth completion or upsampling. However, the former suffers from the inconsistency caused by differences in multimodal data, resulting in uneven point cloud quality. Meanwhile, previous upsampling methods convert point clouds into range images which results in a loss of point accuracy. In this paper, we present DAPU, a novel real-time point cloud upsampling method designed to address these challenges. This method consists of three key components: (1) GPR (Ground Points Recognizer), which analyzes the height difference distribution between coplanar and non-coplanar points within patches to identify ground points. GPR establishes a sparse-to-dense index matrix for fast large-scale point cloud queries. (2) DAPKNN (Distribution-Aware Patch KNN), which dynamically adjusts the sampling radius threshold based on distribution to reduce computation and ensure enough neighbors sampling for distant points. (3) Neighbors Upsampling, which linearly upsamples between each pair of neighbors to preserve all point features. KITTI experiments show gains of up to +1.2% AP 3 D " role="presentation"&gt; 3 D 3 D and +1.4% AP B E V " role="presentation"&gt; B E V B E V . Additional evaluations on mini-nuScenes and Waymo further demonstrate consistent improvements across Vehicle, Pedestrian, and Cyclist detection, confirming DAPU’s robustness under diverse LiDAR settings and real-time suitability.
Published: 2026-01-02T16:45:38+00:00
Venue: Neurocomputing
Score: 0.779 (must_read)</description><content:encoded>&lt;p&gt;&lt;strong&gt;Authors:&lt;/strong&gt; Yinghao Hu; Yan Wu; Yujian Mo; Jijun Wang; Yuwei Zhang&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Journal:&lt;/strong&gt; Neurocomputing&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;a href="https://doi.org/10.1016/j.neucom.2025.132579"&gt;10.1016/j.neucom.2025.132579&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Relevance:&lt;/strong&gt; 0.779 (must_read)&lt;/p&gt;
&lt;p&gt;The sparsity and quality of point clouds significantly constrain the development of LiDAR-based 3D object detectors. Previous approaches supplemented point clouds through depth completion or upsampling. However, the former suffers from the inconsistency caused by differences in multimodal data, resulting in uneven point cloud quality. Meanwhile, previous upsampling methods convert point clouds into range images which results in a loss of point accuracy. In this paper, we present DAPU, a novel real-time point cloud upsampling method designed to address these challenges. This method consists of three key components: (1) GPR (Ground Points Recognizer), which analyzes the height difference distribution between coplanar and non-coplanar points within patches to identify ground points. GPR establishes a sparse-to-dense index matrix for fast large-scale point cloud queries. (2) DAPKNN (Distribution-Aware Patch KNN), which dynamically adjusts the sampling radius threshold based on distribution to reduce computation and ensure enough neighbors sampling for distant points. (3) Neighbors Upsampling, which linearly upsamples between each pair of neighbors to preserve all point features. KITTI experiments show gains of up to +1.2% AP 3 D &amp;quot; role=&amp;quot;presentation&amp;quot;&amp;gt; 3 D 3 D and +1.4% AP B E V &amp;quot; role=&amp;quot;presentation&amp;quot;&amp;gt; B E V B E V . Additional evaluations on mini-nuScenes and Waymo further demonstrate consistent improvements across Vehicle, Pedestrian, and Cyclist detection, confirming DAPU’s robustness under diverse LiDAR settings and real-time suitability.&lt;/p&gt;</content:encoded></item><item><title>Implicit Neural Compression of Point Clouds</title><link>https://doi.org/10.1109/tip.2025.3648141</link><guid>10.1109/tip.2025.3648141</guid><pubDate>Thu, 01 Jan 2026 18:39:24 +0000</pubDate><dc:creator>Hongning Ruan</dc:creator><dc:creator>Yulin Shao</dc:creator><dc:creator>Qianqian Yang</dc:creator><dc:creator>Liang Zhao</dc:creator><dc:creator>Zhaoyang Zhang</dc:creator><dc:creator>Dusit Niyato</dc:creator><prism:publicationName>IEEE Transactions on Image Processing</prism:publicationName><prism:doi>10.1109/tip.2025.3648141</prism:doi><description>Point clouds have gained prominence across numerous applications due to their ability to accurately represent 3D objects and scenes. However, efficiently compressing unstructured, high-precision point cloud data remains a significant challenge. In this paper, we propose NeRC3, a novel point cloud compression framework that leverages implicit neural representations (INRs) to encode both geometry and attributes of dense point clouds. Our approach employs two coordinate-based neural networks: one maps spatial coordinates to voxel occupancy, while the other maps occupied voxels to their attributes, thereby implicitly representing the geometry and attributes of a voxelized point cloud. The encoder quantizes and compresses network parameters alongside auxiliary information required for reconstruction, while the decoder reconstructs the original point cloud by inputting voxel coordinates into the neural networks. Furthermore, we extend our method to dynamic point cloud compression through techniques that reduce temporal redundancy, including a 4D spatio-temporal representation termed 4D-NeRC3. Experimental results validate the effectiveness of our approach: For static point clouds, NeRC3 outperforms octree-based G-PCC standard and existing INR-based methods. For dynamic point clouds, 4D-NeRC3 achieves superior geometry compression performance compared to the latest G-PCC and V-PCC standards, while matching state-of-the-art learning-based methods. It also demonstrates competitive performance in joint geometry and attribute compression.
Published: 2026-01-01T18:39:24+00:00
Venue: IEEE Transactions on Image Processing
Score: 0.777 (must_read)</description><content:encoded>&lt;p&gt;&lt;strong&gt;Authors:&lt;/strong&gt; Hongning Ruan; Yulin Shao; Qianqian Yang; Liang Zhao; Zhaoyang Zhang; Dusit Niyato&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Journal:&lt;/strong&gt; IEEE Transactions on Image Processing&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;a href="https://doi.org/10.1109/tip.2025.3648141"&gt;10.1109/tip.2025.3648141&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Relevance:&lt;/strong&gt; 0.777 (must_read)&lt;/p&gt;
&lt;p&gt;Point clouds have gained prominence across numerous applications due to their ability to accurately represent 3D objects and scenes. However, efficiently compressing unstructured, high-precision point cloud data remains a significant challenge. In this paper, we propose NeRC3, a novel point cloud compression framework that leverages implicit neural representations (INRs) to encode both geometry and attributes of dense point clouds. Our approach employs two coordinate-based neural networks: one maps spatial coordinates to voxel occupancy, while the other maps occupied voxels to their attributes, thereby implicitly representing the geometry and attributes of a voxelized point cloud. The encoder quantizes and compresses network parameters alongside auxiliary information required for reconstruction, while the decoder reconstructs the original point cloud by inputting voxel coordinates into the neural networks. Furthermore, we extend our method to dynamic point cloud compression through techniques that reduce temporal redundancy, including a 4D spatio-temporal representation termed 4D-NeRC3. Experimental results validate the effectiveness of our approach: For static point clouds, NeRC3 outperforms octree-based G-PCC standard and existing INR-based methods. For dynamic point clouds, 4D-NeRC3 achieves superior geometry compression performance compared to the latest G-PCC and V-PCC standards, while matching state-of-the-art learning-based methods. It also demonstrates competitive performance in joint geometry and attribute compression.&lt;/p&gt;</content:encoded></item><item><title>Guiding a Diffusion Transformer with the Internal Dynamics of Itself</title><link>https://arxiv.org/abs/2512.24176v1</link><guid>http://arxiv.org/abs/2512.24176v1</guid><pubDate>Tue, 30 Dec 2025 12:16:46 +0000</pubDate><dc:creator>Xingyu Zhou</dc:creator><dc:creator>Qifan Li</dc:creator><dc:creator>Xiaobin Hu</dc:creator><dc:creator>Hai Chen</dc:creator><dc:creator>Shuhang Gu</dc:creator><dc:type>preprint</dc:type><prism:publicationName>arXiv</prism:publicationName><description>The diffusion model presents a powerful ability to capture the entire (conditional) data distribution. However, due to the lack of sufficient training and data to learn to cover low-probability areas, the model will be penalized for failing to generate high-quality images corresponding to these areas. To achieve better generation quality, guidance strategies such as classifier free guidance (CFG) can guide the samples to the high-probability areas during the sampling stage. However, the standard CFG often leads to over-simplified or distorted samples. On the other hand, the alternative line of guiding diffusion model with its bad version is limited by carefully designed degradation strategies, extra training and additional sampling steps. In this paper, we proposed a simple yet effective strategy Internal Guidance (IG), which introduces an auxiliary supervision on the intermediate layer during training process and extrapolates the intermediate and deep layer's outputs to obtain generative results during sampling process. This simple strategy yields significant improvements in both training efficiency and generation quality on various baselines. On ImageNet 256x256, SiT-XL/2+IG achieves FID=5.31 and FID=1.75 at 80 and 800 epochs. More impressively, LightningDiT-XL/1+IG achieves FID=1.34 which achieves a large margin between all of these methods. Combined with CFG, LightningDiT-XL/1+IG achieves the current state-of-the-art FID of 1.19.
Published: 2025-12-30T12:16:46+00:00
Venue: arXiv
Score: 0.777 (consider)</description><content:encoded>&lt;p&gt;&lt;strong&gt;Authors:&lt;/strong&gt; Xingyu Zhou; Qifan Li; Xiaobin Hu; Hai Chen; Shuhang Gu&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Journal:&lt;/strong&gt; arXiv&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Relevance:&lt;/strong&gt; 0.777 (consider)&lt;/p&gt;
&lt;p&gt;The diffusion model presents a powerful ability to capture the entire (conditional) data distribution. However, due to the lack of sufficient training and data to learn to cover low-probability areas, the model will be penalized for failing to generate high-quality images corresponding to these areas. To achieve better generation quality, guidance strategies such as classifier free guidance (CFG) can guide the samples to the high-probability areas during the sampling stage. However, the standard CFG often leads to over-simplified or distorted samples. On the other hand, the alternative line of guiding diffusion model with its bad version is limited by carefully designed degradation strategies, extra training and additional sampling steps. In this paper, we proposed a simple yet effective strategy Internal Guidance (IG), which introduces an auxiliary supervision on the intermediate layer during training process and extrapolates the intermediate and deep layer&amp;#x27;s outputs to obtain generative results during sampling process. This simple strategy yields significant improvements in both training efficiency and generation quality on various baselines. On ImageNet 256x256, SiT-XL/2+IG achieves FID=5.31 and FID=1.75 at 80 and 800 epochs. More impressively, LightningDiT-XL/1+IG achieves FID=1.34 which achieves a large margin between all of these methods. Combined with CFG, LightningDiT-XL/1+IG achieves the current state-of-the-art FID of 1.19.&lt;/p&gt;</content:encoded></item><item><title>Adaptive Adversarial Cross-Domain Segmentation Network for High-Resolution Remote Sensing Images</title><link>https://doi.org/10.1109/jstars.2025.3650193</link><guid>10.1109/jstars.2025.3650193</guid><pubDate>Thu, 01 Jan 2026 18:37:35 +0000</pubDate><dc:creator>Jianfen Wei</dc:creator><dc:creator>Ping Yang</dc:creator><dc:creator>Chang Wang</dc:creator><dc:creator>Chunxiang Shi</dc:creator><dc:creator>Renlong Hang</dc:creator><prism:publicationName>IEEE Journal of Selected Topics in Applied Earth Observations and Remote Sensing</prism:publicationName><prism:doi>10.1109/jstars.2025.3650193</prism:doi><description>Convolutional neural networks (CNNs) have dominated the field of semantic segmentation of high-resolution remote sensing images (HRSIs). These models often assume that training and test data follow the same distribution. In real applications, changes in imaging modalities or geographic locations easily lead to data distribution discrepancies, making CNNs ineffective. Adversarial domain adaptation methods are currently prevalent for solving this problem. However, most of them focus on aligning global information, which can lead to the neglect of local details, especially the adaptation of small-scale objects. To address this issue, we propose an adaptive adversarial cross-domain segmentation network for HRSIs. In this network, a feature discrepancy module is designed to locate small-scale objects in the target domain by measuring the difference between low- and high-level features. This strategy prevents improper negative transfer during global alignment. Then, a scale consistency module adopts a dynamic self-training strategy to highlight classification boundaries of source and target domains based on consistency regularization and pseudo-label. Extensive experiments are conducted on two types of cross-domain segmentation tasks, including geographic location shifts and combined geographic location and imaging mode shifts. Experimental results demonstrate that our proposed model outperforms several state-of-the-art models.
Published: 2026-01-01T18:37:35+00:00
Venue: IEEE Journal of Selected Topics in Applied Earth Observations and Remote Sensing
Score: 0.775 (consider)</description><content:encoded>&lt;p&gt;&lt;strong&gt;Authors:&lt;/strong&gt; Jianfen Wei; Ping Yang; Chang Wang; Chunxiang Shi; Renlong Hang&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Journal:&lt;/strong&gt; IEEE Journal of Selected Topics in Applied Earth Observations and Remote Sensing&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;a href="https://doi.org/10.1109/jstars.2025.3650193"&gt;10.1109/jstars.2025.3650193&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Relevance:&lt;/strong&gt; 0.775 (consider)&lt;/p&gt;
&lt;p&gt;Convolutional neural networks (CNNs) have dominated the field of semantic segmentation of high-resolution remote sensing images (HRSIs). These models often assume that training and test data follow the same distribution. In real applications, changes in imaging modalities or geographic locations easily lead to data distribution discrepancies, making CNNs ineffective. Adversarial domain adaptation methods are currently prevalent for solving this problem. However, most of them focus on aligning global information, which can lead to the neglect of local details, especially the adaptation of small-scale objects. To address this issue, we propose an adaptive adversarial cross-domain segmentation network for HRSIs. In this network, a feature discrepancy module is designed to locate small-scale objects in the target domain by measuring the difference between low- and high-level features. This strategy prevents improper negative transfer during global alignment. Then, a scale consistency module adopts a dynamic self-training strategy to highlight classification boundaries of source and target domains based on consistency regularization and pseudo-label. Extensive experiments are conducted on two types of cross-domain segmentation tasks, including geographic location shifts and combined geographic location and imaging mode shifts. Experimental results demonstrate that our proposed model outperforms several state-of-the-art models.&lt;/p&gt;</content:encoded></item><item><title>Forging Spatial Intelligence: A Roadmap of Multi-Modal Data Pre-Training for Autonomous Systems</title><link>https://arxiv.org/abs/2512.24385v1</link><guid>http://arxiv.org/abs/2512.24385v1</guid><pubDate>Tue, 30 Dec 2025 17:58:01 +0000</pubDate><dc:creator>Song Wang</dc:creator><dc:creator>Lingdong Kong</dc:creator><dc:creator>Xiaolu Liu</dc:creator><dc:creator>Hao Shi</dc:creator><dc:creator>Wentong Li</dc:creator><dc:creator>Jianke Zhu</dc:creator><dc:creator>Steven C. H. Hoi</dc:creator><dc:type>preprint</dc:type><prism:publicationName>arXiv</prism:publicationName><description>The rapid advancement of autonomous systems, including self-driving vehicles and drones, has intensified the need to forge true Spatial Intelligence from multi-modal onboard sensor data. While foundation models excel in single-modal contexts, integrating their capabilities across diverse sensors like cameras and LiDAR to create a unified understanding remains a formidable challenge. This paper presents a comprehensive framework for multi-modal pre-training, identifying the core set of techniques driving progress toward this goal. We dissect the interplay between foundational sensor characteristics and learning strategies, evaluating the role of platform-specific datasets in enabling these advancements. Our central contribution is the formulation of a unified taxonomy for pre-training paradigms: ranging from single-modality baselines to sophisticated unified frameworks that learn holistic representations for advanced tasks like 3D object detection and semantic occupancy prediction. Furthermore, we investigate the integration of textual inputs and occupancy representations to facilitate open-world perception and planning. Finally, we identify critical bottlenecks, such as computational efficiency and model scalability, and propose a roadmap toward general-purpose multi-modal foundation models capable of achieving robust Spatial Intelligence for real-world deployment.
Published: 2025-12-30T17:58:01+00:00
Venue: arXiv
Score: 0.775 (consider)</description><content:encoded>&lt;p&gt;&lt;strong&gt;Authors:&lt;/strong&gt; Song Wang; Lingdong Kong; Xiaolu Liu; Hao Shi; Wentong Li; Jianke Zhu; Steven C. H. Hoi&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Journal:&lt;/strong&gt; arXiv&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Relevance:&lt;/strong&gt; 0.775 (consider)&lt;/p&gt;
&lt;p&gt;The rapid advancement of autonomous systems, including self-driving vehicles and drones, has intensified the need to forge true Spatial Intelligence from multi-modal onboard sensor data. While foundation models excel in single-modal contexts, integrating their capabilities across diverse sensors like cameras and LiDAR to create a unified understanding remains a formidable challenge. This paper presents a comprehensive framework for multi-modal pre-training, identifying the core set of techniques driving progress toward this goal. We dissect the interplay between foundational sensor characteristics and learning strategies, evaluating the role of platform-specific datasets in enabling these advancements. Our central contribution is the formulation of a unified taxonomy for pre-training paradigms: ranging from single-modality baselines to sophisticated unified frameworks that learn holistic representations for advanced tasks like 3D object detection and semantic occupancy prediction. Furthermore, we investigate the integration of textual inputs and occupancy representations to facilitate open-world perception and planning. Finally, we identify critical bottlenecks, such as computational efficiency and model scalability, and propose a roadmap toward general-purpose multi-modal foundation models capable of achieving robust Spatial Intelligence for real-world deployment.&lt;/p&gt;</content:encoded></item><item><title>A Ship Incremental Recognition Framework via Unknown Extraction and Joint Optimization Learning</title><link>https://doi.org/10.3390/rs18010149</link><guid>10.3390/rs18010149</guid><pubDate>Fri, 02 Jan 2026 09:18:56 +0000</pubDate><dc:creator>Yugao Li</dc:creator><dc:creator>Guangzhen Bao</dc:creator><dc:creator>Jianming Hu</dc:creator><dc:creator>Xiyang Zhi</dc:creator><dc:creator>Tianyi Hu</dc:creator><dc:creator>Junjie Wang</dc:creator><dc:creator>Wenbo Wu</dc:creator><prism:publicationName>Remote Sensing</prism:publicationName><prism:doi>10.3390/rs18010149</prism:doi><description>With the rapid growth of the marine economy and the increasing demand for maritime security, ship target detection has become critically important in both military and civilian applications. However, in complex remote sensing scenarios, challenges such as visual similarity among ships, subtle inter-class differences, and the continual emergence of new categories make traditional closed-world detection methods inadequate. To address these issues, this paper proposes an open-world detection framework for remote sensing ships. The framework integrates two key modules: (1) a Fine-Grained Feature and Extreme Value-based Unknown Recognition (FEUR) module, which leverages tail distribution modeling and adaptive thresholding to achieve precise detection and effective differentiation of unknown ship targets; and (2) a Joint Optimization-based Incremental Learning (JOIL) module, which employs hierarchical elastic weight constraints to differentially update the backbone and detection head, thereby alleviating catastrophic forgetting while incorporating new categories with only a few labeled samples. Extensive experiments on the FGSRCS dataset demonstrate that the proposed method not only maintains high accuracy on known categories but also significantly outperforms mainstream open-world detection approaches in unknown recognition and incremental learning. This work provides both theoretical value and practical potential for continuous ship detection and recognition in complex open environments.
Published: 2026-01-02T09:18:56+00:00
Venue: Remote Sensing
Score: 0.774 (consider)</description><content:encoded>&lt;p&gt;&lt;strong&gt;Authors:&lt;/strong&gt; Yugao Li; Guangzhen Bao; Jianming Hu; Xiyang Zhi; Tianyi Hu; Junjie Wang; Wenbo Wu&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Journal:&lt;/strong&gt; Remote Sensing&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;a href="https://doi.org/10.3390/rs18010149"&gt;10.3390/rs18010149&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Relevance:&lt;/strong&gt; 0.774 (consider)&lt;/p&gt;
&lt;p&gt;With the rapid growth of the marine economy and the increasing demand for maritime security, ship target detection has become critically important in both military and civilian applications. However, in complex remote sensing scenarios, challenges such as visual similarity among ships, subtle inter-class differences, and the continual emergence of new categories make traditional closed-world detection methods inadequate. To address these issues, this paper proposes an open-world detection framework for remote sensing ships. The framework integrates two key modules: (1) a Fine-Grained Feature and Extreme Value-based Unknown Recognition (FEUR) module, which leverages tail distribution modeling and adaptive thresholding to achieve precise detection and effective differentiation of unknown ship targets; and (2) a Joint Optimization-based Incremental Learning (JOIL) module, which employs hierarchical elastic weight constraints to differentially update the backbone and detection head, thereby alleviating catastrophic forgetting while incorporating new categories with only a few labeled samples. Extensive experiments on the FGSRCS dataset demonstrate that the proposed method not only maintains high accuracy on known categories but also significantly outperforms mainstream open-world detection approaches in unknown recognition and incremental learning. This work provides both theoretical value and practical potential for continuous ship detection and recognition in complex open environments.&lt;/p&gt;</content:encoded></item><item><title>Balanced Hierarchical Contrastive Learning with Decoupled Queries for Fine-grained Object Detection in Remote Sensing Images</title><link>https://arxiv.org/abs/2512.24074v1</link><guid>http://arxiv.org/abs/2512.24074v1</guid><pubDate>Tue, 30 Dec 2025 08:35:54 +0000</pubDate><dc:creator>Jingzhou Chen</dc:creator><dc:creator>Dexin Chen</dc:creator><dc:creator>Fengchao Xiong</dc:creator><dc:creator>Yuntao Qian</dc:creator><dc:creator>Liang Xiao</dc:creator><dc:type>preprint</dc:type><prism:publicationName>arXiv</prism:publicationName><description>Fine-grained remote sensing datasets often use hierarchical label structures to differentiate objects in a coarse-to-fine manner, with each object annotated across multiple levels. However, embedding this semantic hierarchy into the representation learning space to improve fine-grained detection performance remains challenging. Previous studies have applied supervised contrastive learning at different hierarchical levels to group objects under the same parent class while distinguishing sibling subcategories. Nevertheless, they overlook two critical issues: (1) imbalanced data distribution across the label hierarchy causes high-frequency classes to dominate the learning process, and (2) learning semantic relationships among categories interferes with class-agnostic localization. To address these issues, we propose a balanced hierarchical contrastive loss combined with a decoupled learning strategy within the detection transformer (DETR) framework. The proposed loss introduces learnable class prototypes and equilibrates gradients contributed by different classes at each hierarchical level, ensuring that each hierarchical class contributes equally to the loss computation in every mini-batch. The decoupled strategy separates DETR's object queries into classification and localization sets, enabling task-specific feature extraction and optimization. Experiments on three fine-grained datasets with hierarchical annotations demonstrate that our method outperforms state-of-the-art approaches.
Published: 2025-12-30T08:35:54+00:00
Venue: arXiv
Score: 0.773 (consider)</description><content:encoded>&lt;p&gt;&lt;strong&gt;Authors:&lt;/strong&gt; Jingzhou Chen; Dexin Chen; Fengchao Xiong; Yuntao Qian; Liang Xiao&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Journal:&lt;/strong&gt; arXiv&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Relevance:&lt;/strong&gt; 0.773 (consider)&lt;/p&gt;
&lt;p&gt;Fine-grained remote sensing datasets often use hierarchical label structures to differentiate objects in a coarse-to-fine manner, with each object annotated across multiple levels. However, embedding this semantic hierarchy into the representation learning space to improve fine-grained detection performance remains challenging. Previous studies have applied supervised contrastive learning at different hierarchical levels to group objects under the same parent class while distinguishing sibling subcategories. Nevertheless, they overlook two critical issues: (1) imbalanced data distribution across the label hierarchy causes high-frequency classes to dominate the learning process, and (2) learning semantic relationships among categories interferes with class-agnostic localization. To address these issues, we propose a balanced hierarchical contrastive loss combined with a decoupled learning strategy within the detection transformer (DETR) framework. The proposed loss introduces learnable class prototypes and equilibrates gradients contributed by different classes at each hierarchical level, ensuring that each hierarchical class contributes equally to the loss computation in every mini-batch. The decoupled strategy separates DETR&amp;#x27;s object queries into classification and localization sets, enabling task-specific feature extraction and optimization. Experiments on three fine-grained datasets with hierarchical annotations demonstrate that our method outperforms state-of-the-art approaches.&lt;/p&gt;</content:encoded></item><item><title>Efficiently Estimating Data Efficiency for Language Model Fine-tuning</title><link>https://arxiv.org/abs/2512.24991v1</link><guid>http://arxiv.org/abs/2512.24991v1</guid><pubDate>Wed, 31 Dec 2025 17:37:29 +0000</pubDate><dc:creator>Gyung Hyun Je</dc:creator><dc:creator>Colin Raffel</dc:creator><dc:type>preprint</dc:type><prism:publicationName>arXiv</prism:publicationName><description>While large language models (LLMs) demonstrate reasonable zero-shot capability across many downstream tasks, fine-tuning is a common practice to improve their performance. However, a task's data efficiency--i.e., the number of fine-tuning examples needed to achieve a desired level of performance--is often unknown, resulting in costly cycles of incremental annotation and retraining. Indeed, we demonstrate across a curated set of 30 specialized tasks that performant LLMs may struggle zero-shot but can attain stronger performance after fine-tuning. This motivates the need for methods to predict a task's data efficiency without requiring incremental annotation. After introducing a concrete metric that quantifies a task's data efficiency, we propose using the gradient cosine similarity of low-confidence examples to predict data efficiency based on a small number of labeled samples. We validate our approach on a diverse set of tasks with varying data efficiencies, attaining 8.6% error in overall data efficiency prediction and typically eliminating hundreds of unnecessary annotations on each task. Our experiment results and implementation code are available on GitHub.
Published: 2025-12-31T17:37:29+00:00
Venue: arXiv
Score: 0.771 (consider)</description><content:encoded>&lt;p&gt;&lt;strong&gt;Authors:&lt;/strong&gt; Gyung Hyun Je; Colin Raffel&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Journal:&lt;/strong&gt; arXiv&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Relevance:&lt;/strong&gt; 0.771 (consider)&lt;/p&gt;
&lt;p&gt;While large language models (LLMs) demonstrate reasonable zero-shot capability across many downstream tasks, fine-tuning is a common practice to improve their performance. However, a task&amp;#x27;s data efficiency--i.e., the number of fine-tuning examples needed to achieve a desired level of performance--is often unknown, resulting in costly cycles of incremental annotation and retraining. Indeed, we demonstrate across a curated set of 30 specialized tasks that performant LLMs may struggle zero-shot but can attain stronger performance after fine-tuning. This motivates the need for methods to predict a task&amp;#x27;s data efficiency without requiring incremental annotation. After introducing a concrete metric that quantifies a task&amp;#x27;s data efficiency, we propose using the gradient cosine similarity of low-confidence examples to predict data efficiency based on a small number of labeled samples. We validate our approach on a diverse set of tasks with varying data efficiencies, attaining 8.6% error in overall data efficiency prediction and typically eliminating hundreds of unnecessary annotations on each task. Our experiment results and implementation code are available on GitHub.&lt;/p&gt;</content:encoded></item><item><title>Spatial-aware Vision Language Model for Autonomous Driving</title><link>https://arxiv.org/abs/2512.24331v1</link><guid>http://arxiv.org/abs/2512.24331v1</guid><pubDate>Tue, 30 Dec 2025 16:35:00 +0000</pubDate><dc:creator>Weijie Wei</dc:creator><dc:creator>Zhipeng Luo</dc:creator><dc:creator>Ling Feng</dc:creator><dc:creator>Venice Erin Liong</dc:creator><dc:type>preprint</dc:type><prism:publicationName>arXiv</prism:publicationName><description>While Vision-Language Models (VLMs) show significant promise for end-to-end autonomous driving by leveraging the common sense embedded in language models, their reliance on 2D image cues for complex scene understanding and decision-making presents a critical bottleneck for safety and reliability. Current image-based methods struggle with accurate metric spatial reasoning and geometric inference, leading to unreliable driving policies. To bridge this gap, we propose LVLDrive (LiDAR-Vision-Language), a novel framework specifically designed to upgrade existing VLMs with robust 3D metric spatial understanding for autonomous driving by incoperating LiDAR point cloud as an extra input modality. A key challenge lies in mitigating the catastrophic disturbance introduced by disparate 3D data to the pre-trained VLMs. To this end, we introduce a Gradual Fusion Q-Former that incrementally injects LiDAR features, ensuring the stability and preservation of the VLM's existing knowledge base. Furthermore, we develop a spatial-aware question-answering (SA-QA) dataset to explicitly teach the model advanced 3D perception and reasoning capabilities. Extensive experiments on driving benchmarks demonstrate that LVLDrive achieves superior performance compared to vision-only counterparts across scene understanding, metric spatial perception, and reliable driving decision-making. Our work highlights the necessity of explicit 3D metric data for building trustworthy VLM-based autonomous systems.
Published: 2025-12-30T16:35:00+00:00
Venue: arXiv
Score: 0.770 (consider)</description><content:encoded>&lt;p&gt;&lt;strong&gt;Authors:&lt;/strong&gt; Weijie Wei; Zhipeng Luo; Ling Feng; Venice Erin Liong&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Journal:&lt;/strong&gt; arXiv&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Relevance:&lt;/strong&gt; 0.770 (consider)&lt;/p&gt;
&lt;p&gt;While Vision-Language Models (VLMs) show significant promise for end-to-end autonomous driving by leveraging the common sense embedded in language models, their reliance on 2D image cues for complex scene understanding and decision-making presents a critical bottleneck for safety and reliability. Current image-based methods struggle with accurate metric spatial reasoning and geometric inference, leading to unreliable driving policies. To bridge this gap, we propose LVLDrive (LiDAR-Vision-Language), a novel framework specifically designed to upgrade existing VLMs with robust 3D metric spatial understanding for autonomous driving by incoperating LiDAR point cloud as an extra input modality. A key challenge lies in mitigating the catastrophic disturbance introduced by disparate 3D data to the pre-trained VLMs. To this end, we introduce a Gradual Fusion Q-Former that incrementally injects LiDAR features, ensuring the stability and preservation of the VLM&amp;#x27;s existing knowledge base. Furthermore, we develop a spatial-aware question-answering (SA-QA) dataset to explicitly teach the model advanced 3D perception and reasoning capabilities. Extensive experiments on driving benchmarks demonstrate that LVLDrive achieves superior performance compared to vision-only counterparts across scene understanding, metric spatial perception, and reliable driving decision-making. Our work highlights the necessity of explicit 3D metric data for building trustworthy VLM-based autonomous systems.&lt;/p&gt;</content:encoded></item><item><title>Recursive Language Models</title><link>https://arxiv.org/abs/2512.24601v1</link><guid>http://arxiv.org/abs/2512.24601v1</guid><pubDate>Wed, 31 Dec 2025 03:43:41 +0000</pubDate><dc:creator>Alex L. Zhang</dc:creator><dc:creator>Tim Kraska</dc:creator><dc:creator>Omar Khattab</dc:creator><dc:type>preprint</dc:type><prism:publicationName>arXiv</prism:publicationName><description>We study allowing large language models (LLMs) to process arbitrarily long prompts through the lens of inference-time scaling. We propose Recursive Language Models (RLMs), a general inference strategy that treats long prompts as part of an external environment and allows the LLM to programmatically examine, decompose, and recursively call itself over snippets of the prompt. We find that RLMs successfully handle inputs up to two orders of magnitude beyond model context windows and, even for shorter prompts, dramatically outperform the quality of base LLMs and common long-context scaffolds across four diverse long-context tasks, while having comparable (or cheaper) cost per query.
Published: 2025-12-31T03:43:41+00:00
Venue: arXiv
Score: 0.770 (consider)</description><content:encoded>&lt;p&gt;&lt;strong&gt;Authors:&lt;/strong&gt; Alex L. Zhang; Tim Kraska; Omar Khattab&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Journal:&lt;/strong&gt; arXiv&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Relevance:&lt;/strong&gt; 0.770 (consider)&lt;/p&gt;
&lt;p&gt;We study allowing large language models (LLMs) to process arbitrarily long prompts through the lens of inference-time scaling. We propose Recursive Language Models (RLMs), a general inference strategy that treats long prompts as part of an external environment and allows the LLM to programmatically examine, decompose, and recursively call itself over snippets of the prompt. We find that RLMs successfully handle inputs up to two orders of magnitude beyond model context windows and, even for shorter prompts, dramatically outperform the quality of base LLMs and common long-context scaffolds across four diverse long-context tasks, while having comparable (or cheaper) cost per query.&lt;/p&gt;</content:encoded></item><item><title>UAGLNet: Uncertainty-Aggregated Global-Local Fusion Network with Cooperative CNN-Transformer for Building Extraction</title><link>https://doi.org/10.1109/tgrs.2025.3645320</link><guid>10.1109/tgrs.2025.3645320</guid><pubDate>Thu, 01 Jan 2026 18:37:16 +0000</pubDate><dc:creator>Siyuan Yao</dc:creator><dc:creator>Dongxiu Liu</dc:creator><dc:creator>Taotao Li</dc:creator><dc:creator>Shengjie Li</dc:creator><dc:creator>Wenqi Ren</dc:creator><dc:creator>Xiaochun Cao</dc:creator><prism:publicationName>IEEE Transactions on Geoscience and Remote Sensing</prism:publicationName><prism:doi>10.1109/tgrs.2025.3645320</prism:doi><description>Building extraction from remote sensing images is a challenging task due to the complex structure variations of the buildings. Existing methods employ convolutional or self-attention blocks to capture the multi-scale features in the segmentation models, while the inherent gap of the feature pyramids and insufficient global-local feature integration leads to inaccurate, ambiguous extraction results. To address this issue, in this paper, we present an Uncertainty-Aggregated Global-Local Fusion Network (UAGLNet), which is capable to exploit high-quality global-local visual semantics under the guidance of uncertainty modeling. Specifically, we propose a novel cooperative encoder, which adopts hybrid CNN and transformer layers at different stages to capture the local and global visual semantics, respectively. An intermediate cooperative interaction block (CIB) is designed to narrow the gap between the local and global features when the network becomes deeper. Afterwards, we propose a Global-Local Fusion (GLF) module to complementarily fuse the global and local representations. Moreover, to mitigate the segmentation ambiguity in uncertain regions, we propose an Uncertainty-Aggregated Decoder (UAD) to explicitly estimate the pixel-wise uncertainty to enhance the segmentation accuracy. Extensive experiments demonstrate that our method achieves superior performance to other state-of-the-art methods. Our code is available at https://github.com/Dstate/UAGLNet.
Published: 2026-01-01T18:37:16+00:00
Venue: IEEE Transactions on Geoscience and Remote Sensing
Score: 0.768 (consider)</description><content:encoded>&lt;p&gt;&lt;strong&gt;Authors:&lt;/strong&gt; Siyuan Yao; Dongxiu Liu; Taotao Li; Shengjie Li; Wenqi Ren; Xiaochun Cao&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Journal:&lt;/strong&gt; IEEE Transactions on Geoscience and Remote Sensing&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;a href="https://doi.org/10.1109/tgrs.2025.3645320"&gt;10.1109/tgrs.2025.3645320&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Relevance:&lt;/strong&gt; 0.768 (consider)&lt;/p&gt;
&lt;p&gt;Building extraction from remote sensing images is a challenging task due to the complex structure variations of the buildings. Existing methods employ convolutional or self-attention blocks to capture the multi-scale features in the segmentation models, while the inherent gap of the feature pyramids and insufficient global-local feature integration leads to inaccurate, ambiguous extraction results. To address this issue, in this paper, we present an Uncertainty-Aggregated Global-Local Fusion Network (UAGLNet), which is capable to exploit high-quality global-local visual semantics under the guidance of uncertainty modeling. Specifically, we propose a novel cooperative encoder, which adopts hybrid CNN and transformer layers at different stages to capture the local and global visual semantics, respectively. An intermediate cooperative interaction block (CIB) is designed to narrow the gap between the local and global features when the network becomes deeper. Afterwards, we propose a Global-Local Fusion (GLF) module to complementarily fuse the global and local representations. Moreover, to mitigate the segmentation ambiguity in uncertain regions, we propose an Uncertainty-Aggregated Decoder (UAD) to explicitly estimate the pixel-wise uncertainty to enhance the segmentation accuracy. Extensive experiments demonstrate that our method achieves superior performance to other state-of-the-art methods. Our code is available at https://github.com/Dstate/UAGLNet.&lt;/p&gt;</content:encoded></item><item><title>EchoNet: A Hierarchical Collaborative Network for Point Cloud-based 3D Action Recognition</title><link>https://doi.org/10.1016/j.knosys.2025.115257</link><guid>10.1016/j.knosys.2025.115257</guid><pubDate>Fri, 02 Jan 2026 16:07:28 +0000</pubDate><dc:creator>Guojia Huang</dc:creator><dc:creator>Zhenjie Hou</dc:creator><dc:creator>Xing Li</dc:creator><dc:creator>Jiuzhen Liang</dc:creator><dc:creator>Xinwen Zhou</dc:creator><prism:publicationName>Knowledge-Based Systems</prism:publicationName><prism:doi>10.1016/j.knosys.2025.115257</prism:doi><description>Dynamic point clouds provide inherent geometric fidelity for 3D action recognition, yet their unstructured nature makes it challenging to capture complex spatiotemporal patterns. Existing approaches either rely on local neighborhood aggregation, employ explicit spatiotemporal decoupling, or adopt parallel global modeling. However, they often suffer from limited spatiotemporal awareness, fragmented short-term motion continuity, and a lack of hierarchical progression. To address these issues, we propose the hierarchical collaboration hypothesis: effective representations of dynamic point clouds should follow a progressive abstraction from points to regions to the global level, while maintaining semantic consistency across layers. Building on this hypothesis, we introduce EchoNet, a hierarchical collaborative network composed of three complementary modules: the Point Feature Constructor (PFC) for capturing fine-grained geometric details, the Layered Abstraction Synthesizer (LAS) for hierarchical structural abstraction, and the Temporal Context Refiner (TCR) for enhancing cross-frame temporal dependencies. Furthermore, we design a Multi-Scale Regional Channel Attention (MSRCA) module, which adaptively emphasizes critical action regions by integrating positional encoding with multi-regional context. Experiments on NTU RGB+D 60/120, UTD-MHAD, and MSR Action3D demonstrate that EchoNet achieves state-of-the-art or highly competitive performance, exemplified by a top-tier accuracy of 97.07% on MSR Action3D for complex action recognition. The model also proves effective in large-scale scenarios, attaining 84.3% on the challenging NTU-120 Cross-Subject benchmark. While performance on the large-scale NTU-120 dataset shows the potential for further improvement, our analysis underscores the promise of hierarchical models for building scalable and efficient dynamic point cloud representations.
Published: 2026-01-02T16:07:28+00:00
Venue: Knowledge-Based Systems
Score: 0.768 (consider)</description><content:encoded>&lt;p&gt;&lt;strong&gt;Authors:&lt;/strong&gt; Guojia Huang; Zhenjie Hou; Xing Li; Jiuzhen Liang; Xinwen Zhou&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Journal:&lt;/strong&gt; Knowledge-Based Systems&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;a href="https://doi.org/10.1016/j.knosys.2025.115257"&gt;10.1016/j.knosys.2025.115257&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Relevance:&lt;/strong&gt; 0.768 (consider)&lt;/p&gt;
&lt;p&gt;Dynamic point clouds provide inherent geometric fidelity for 3D action recognition, yet their unstructured nature makes it challenging to capture complex spatiotemporal patterns. Existing approaches either rely on local neighborhood aggregation, employ explicit spatiotemporal decoupling, or adopt parallel global modeling. However, they often suffer from limited spatiotemporal awareness, fragmented short-term motion continuity, and a lack of hierarchical progression. To address these issues, we propose the hierarchical collaboration hypothesis: effective representations of dynamic point clouds should follow a progressive abstraction from points to regions to the global level, while maintaining semantic consistency across layers. Building on this hypothesis, we introduce EchoNet, a hierarchical collaborative network composed of three complementary modules: the Point Feature Constructor (PFC) for capturing fine-grained geometric details, the Layered Abstraction Synthesizer (LAS) for hierarchical structural abstraction, and the Temporal Context Refiner (TCR) for enhancing cross-frame temporal dependencies. Furthermore, we design a Multi-Scale Regional Channel Attention (MSRCA) module, which adaptively emphasizes critical action regions by integrating positional encoding with multi-regional context. Experiments on NTU RGB+D 60/120, UTD-MHAD, and MSR Action3D demonstrate that EchoNet achieves state-of-the-art or highly competitive performance, exemplified by a top-tier accuracy of 97.07% on MSR Action3D for complex action recognition. The model also proves effective in large-scale scenarios, attaining 84.3% on the challenging NTU-120 Cross-Subject benchmark. While performance on the large-scale NTU-120 dataset shows the potential for further improvement, our analysis underscores the promise of hierarchical models for building scalable and efficient dynamic point cloud representations.&lt;/p&gt;</content:encoded></item><item><title>How and Why LLMs Generalize: A Fine-Grained Analysis of LLM Reasoning from Cognitive Behaviors to Low-Level Patterns</title><link>https://arxiv.org/abs/2512.24063v1</link><guid>http://arxiv.org/abs/2512.24063v1</guid><pubDate>Tue, 30 Dec 2025 08:16:20 +0000</pubDate><dc:creator>Haoyue Bai</dc:creator><dc:creator>Yiyou Sun</dc:creator><dc:creator>Wenjie Hu</dc:creator><dc:creator>Shi Qiu</dc:creator><dc:creator>Maggie Ziyu Huan</dc:creator><dc:creator>Peiyang Song</dc:creator><dc:creator>Robert Nowak</dc:creator><dc:creator>Dawn Song</dc:creator><dc:type>preprint</dc:type><prism:publicationName>arXiv</prism:publicationName><description>Large Language Models (LLMs) display strikingly different generalization behaviors: supervised fine-tuning (SFT) often narrows capability, whereas reinforcement-learning (RL) tuning tends to preserve it. The reasons behind this divergence remain unclear, as prior studies have largely relied on coarse accuracy metrics. We address this gap by introducing a novel benchmark that decomposes reasoning into atomic core skills such as calculation, fact retrieval, simulation, enumeration, and diagnostic, providing a concrete framework for addressing the fundamental question of what constitutes reasoning in LLMs. By isolating and measuring these core skills, the benchmark offers a more granular view of how specific cognitive abilities emerge, transfer, and sometimes collapse during post-training. Combined with analyses of low-level statistical patterns such as distributional divergence and parameter statistics, it enables a fine-grained study of how generalization evolves under SFT and RL across mathematical, scientific reasoning, and non-reasoning tasks. Our meta-probing framework tracks model behavior at different training stages and reveals that RL-tuned models maintain more stable behavioral profiles and resist collapse in reasoning skills, whereas SFT models exhibit sharper drift and overfit to surface patterns. This work provides new insights into the nature of reasoning in LLMs and points toward principles for designing training strategies that foster broad, robust generalization.
Published: 2025-12-30T08:16:20+00:00
Venue: arXiv
Score: 0.768 (consider)</description><content:encoded>&lt;p&gt;&lt;strong&gt;Authors:&lt;/strong&gt; Haoyue Bai; Yiyou Sun; Wenjie Hu; Shi Qiu; Maggie Ziyu Huan; Peiyang Song; Robert Nowak; Dawn Song&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Journal:&lt;/strong&gt; arXiv&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Relevance:&lt;/strong&gt; 0.768 (consider)&lt;/p&gt;
&lt;p&gt;Large Language Models (LLMs) display strikingly different generalization behaviors: supervised fine-tuning (SFT) often narrows capability, whereas reinforcement-learning (RL) tuning tends to preserve it. The reasons behind this divergence remain unclear, as prior studies have largely relied on coarse accuracy metrics. We address this gap by introducing a novel benchmark that decomposes reasoning into atomic core skills such as calculation, fact retrieval, simulation, enumeration, and diagnostic, providing a concrete framework for addressing the fundamental question of what constitutes reasoning in LLMs. By isolating and measuring these core skills, the benchmark offers a more granular view of how specific cognitive abilities emerge, transfer, and sometimes collapse during post-training. Combined with analyses of low-level statistical patterns such as distributional divergence and parameter statistics, it enables a fine-grained study of how generalization evolves under SFT and RL across mathematical, scientific reasoning, and non-reasoning tasks. Our meta-probing framework tracks model behavior at different training stages and reveals that RL-tuned models maintain more stable behavioral profiles and resist collapse in reasoning skills, whereas SFT models exhibit sharper drift and overfit to surface patterns. This work provides new insights into the nature of reasoning in LLMs and points toward principles for designing training strategies that foster broad, robust generalization.&lt;/p&gt;</content:encoded></item><item><title>Towards Comprehensive Interactive Change Understanding in Remote Sensing: A Large-scale Dataset and Dual-granularity Enhanced VLM</title><link>https://doi.org/10.1109/tgrs.2025.3650151</link><guid>10.1109/tgrs.2025.3650151</guid><pubDate>Thu, 01 Jan 2026 18:37:16 +0000</pubDate><dc:creator>Junxiao Xue</dc:creator><dc:creator>Quan Deng</dc:creator><dc:creator>Xuecheng Wu</dc:creator><dc:creator>Kelu Yao</dc:creator><dc:creator>Xinyi Yin</dc:creator><dc:creator>Fei Yu</dc:creator><dc:creator>Wei Zhou</dc:creator><dc:creator>Yanfei Zhong</dc:creator><dc:creator>Yang Liu</dc:creator><dc:creator>Dingkang Yang</dc:creator><prism:publicationName>IEEE Transactions on Geoscience and Remote Sensing</prism:publicationName><prism:doi>10.1109/tgrs.2025.3650151</prism:doi><description>Remote sensing change understanding (RSCU) is essential for analyzing remote sensing images and understanding how human activities affect the environment. However, existing datasets lack deep understanding and interactions in the diverse change captioning, counting, and localization tasks. To tackle these gaps, we construct ChangeIMTI, a new large-scale interactive multi-task instruction dataset that encompasses four complementary tasks including change captioning, binary change classification, change counting, and change localization. Building upon this new dataset, we further design a novel vision-guided vision-language model (ChangeVG) with dual-granularity awareness for bi-temporal remote sensing images (i.e., two remote sensing images of the same area at different times). The introduced visionguided module is a dual-branch architecture that synergistically combines fine-grained spatial feature extraction with high-level semantic summarization. These enriched representations further serve as the auxiliary prompts to guide large vision-language models (VLMs) (e.g., Qwen2.5-VL-7B) during instruction tuning, thereby facilitating the hierarchical cross-modal learning. We extensively conduct experiments across four tasks to demonstrate the superiority of our approach. Remarkably, on the change captioning task, our method outperforms the strongest method Semantic-CC by 1.39 points on the comprehensive S∗m metric, which integrates the semantic similarity and descriptive accuracy to provide an overall evaluation of change caption. Moreover, we also perform a series of ablation studies to examine the critical components of our method. The source code and associated data for this work are publicly available at Github.
Published: 2026-01-01T18:37:16+00:00
Venue: IEEE Transactions on Geoscience and Remote Sensing
Score: 0.767 (consider)</description><content:encoded>&lt;p&gt;&lt;strong&gt;Authors:&lt;/strong&gt; Junxiao Xue; Quan Deng; Xuecheng Wu; Kelu Yao; Xinyi Yin; Fei Yu; Wei Zhou; Yanfei Zhong; Yang Liu; Dingkang Yang&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Journal:&lt;/strong&gt; IEEE Transactions on Geoscience and Remote Sensing&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;a href="https://doi.org/10.1109/tgrs.2025.3650151"&gt;10.1109/tgrs.2025.3650151&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Relevance:&lt;/strong&gt; 0.767 (consider)&lt;/p&gt;
&lt;p&gt;Remote sensing change understanding (RSCU) is essential for analyzing remote sensing images and understanding how human activities affect the environment. However, existing datasets lack deep understanding and interactions in the diverse change captioning, counting, and localization tasks. To tackle these gaps, we construct ChangeIMTI, a new large-scale interactive multi-task instruction dataset that encompasses four complementary tasks including change captioning, binary change classification, change counting, and change localization. Building upon this new dataset, we further design a novel vision-guided vision-language model (ChangeVG) with dual-granularity awareness for bi-temporal remote sensing images (i.e., two remote sensing images of the same area at different times). The introduced visionguided module is a dual-branch architecture that synergistically combines fine-grained spatial feature extraction with high-level semantic summarization. These enriched representations further serve as the auxiliary prompts to guide large vision-language models (VLMs) (e.g., Qwen2.5-VL-7B) during instruction tuning, thereby facilitating the hierarchical cross-modal learning. We extensively conduct experiments across four tasks to demonstrate the superiority of our approach. Remarkably, on the change captioning task, our method outperforms the strongest method Semantic-CC by 1.39 points on the comprehensive S∗m metric, which integrates the semantic similarity and descriptive accuracy to provide an overall evaluation of change caption. Moreover, we also perform a series of ablation studies to examine the critical components of our method. The source code and associated data for this work are publicly available at Github.&lt;/p&gt;</content:encoded></item></channel></rss>