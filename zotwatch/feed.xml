<?xml version='1.0' encoding='utf-8'?>
<rss xmlns:content="http://purl.org/rss/1.0/modules/content/" xmlns:dc="http://purl.org/dc/elements/1.1/" xmlns:prism="http://prismstandard.org/namespaces/basic/3.0/" version="2.0"><channel><title>ZotWatch Feed</title><link>https://ehehe.cn/zotwatch/</link><description>AI-assisted literature watch</description><lastBuildDate>Wed, 03 Dec 2025 02:41:35 +0000</lastBuildDate><item><title>LSDFormer: Lightweight SAR Ship Detection Enhanced With Efficient Multi-Attention and Structural Reparameterization</title><link>https://doi.org/10.1109/jstars.2025.3639164</link><guid>10.1109/jstars.2025.3639164</guid><pubDate>Mon, 01 Dec 2025 18:24:44 +0000</pubDate><dc:creator>Rui Jiang</dc:creator><dc:creator>Hang Shi</dc:creator><dc:creator>Jiahong Ni</dc:creator><dc:creator>Jiatao Li</dc:creator><dc:creator>Yi Feng</dc:creator><dc:creator>Xinqiang Chen</dc:creator><dc:creator>Yinlin Li</dc:creator><prism:publicationName>IEEE Journal of Selected Topics in Applied Earth Observations and Remote Sensing</prism:publicationName><prism:doi>10.1109/jstars.2025.3639164</prism:doi><description>Ship detection in Synthetic Aperture Radar (SAR) images faces challenges such as strong background interference, varying ship appearance and distribution and high real-time requirements. Although attention-based deep learning methods dominate this field, the design of lightweight models with efficient attention mechanisms capable of addressing the above challenges remains underexplored. To address this issue, we propose a lightweight SAR ship detection model named LSDFormer, which is built upon the MetaFormer architecture and consists of an efficient multi-attention enhanced backbone and neck and a structural reparameterization enhanced head. We employ two lightweight modules for the backbone and neck: a PoolFormer-based feature extraction module with efficient channel modulation attention is proposed to enhance ship features and suppress background interference; a downsampling module using efficient channel aggregation attention and group convolutions is introduced to enrich ship features. The position-sensitive attention from YOLOv11 is also introduced to handle variations in ship appearance and distribution. These three attentions are integrated into an efficient multi-attention mechanism. Furthermore, a structural reparameterization based detection branch is proposed for the head of LSDFormer, which enhances ship features while reducing model complexity. Extensive experiments on SSDD and HRSID datasets demonstrate the superiority and effectiveness of LSDFormer, achieving AP50 of 98.5 ± 0.4 % \bf {98.5\pm 0.4\%} and 92.8 ± 0.2 % \bf {92.8\pm 0.2\%} , respectively, with only 1.5 \bf {1.5} M parameters and 4.1 \bf {4.1} GFLOPs. The average processing time per image is 4.9 \bf {4.9} ms on SSDD and 4.2 \bf {4.2} ms on HRSID, confirming its real-time performance.
Published: 2025-12-01T18:24:44+00:00
Venue: IEEE Journal of Selected Topics in Applied Earth Observations and Remote Sensing
Score: 0.834 (must_read)</description><content:encoded>&lt;p&gt;&lt;strong&gt;Authors:&lt;/strong&gt; Rui Jiang; Hang Shi; Jiahong Ni; Jiatao Li; Yi Feng; Xinqiang Chen; Yinlin Li&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Journal:&lt;/strong&gt; IEEE Journal of Selected Topics in Applied Earth Observations and Remote Sensing&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;a href="https://doi.org/10.1109/jstars.2025.3639164"&gt;10.1109/jstars.2025.3639164&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Relevance:&lt;/strong&gt; 0.834 (must_read)&lt;/p&gt;
&lt;p&gt;Ship detection in Synthetic Aperture Radar (SAR) images faces challenges such as strong background interference, varying ship appearance and distribution and high real-time requirements. Although attention-based deep learning methods dominate this field, the design of lightweight models with efficient attention mechanisms capable of addressing the above challenges remains underexplored. To address this issue, we propose a lightweight SAR ship detection model named LSDFormer, which is built upon the MetaFormer architecture and consists of an efficient multi-attention enhanced backbone and neck and a structural reparameterization enhanced head. We employ two lightweight modules for the backbone and neck: a PoolFormer-based feature extraction module with efficient channel modulation attention is proposed to enhance ship features and suppress background interference; a downsampling module using efficient channel aggregation attention and group convolutions is introduced to enrich ship features. The position-sensitive attention from YOLOv11 is also introduced to handle variations in ship appearance and distribution. These three attentions are integrated into an efficient multi-attention mechanism. Furthermore, a structural reparameterization based detection branch is proposed for the head of LSDFormer, which enhances ship features while reducing model complexity. Extensive experiments on SSDD and HRSID datasets demonstrate the superiority and effectiveness of LSDFormer, achieving AP50 of 98.5 ± 0.4 % \bf {98.5\pm 0.4\%} and 92.8 ± 0.2 % \bf {92.8\pm 0.2\%} , respectively, with only 1.5 \bf {1.5} M parameters and 4.1 \bf {4.1} GFLOPs. The average processing time per image is 4.9 \bf {4.9} ms on SSDD and 4.2 \bf {4.2} ms on HRSID, confirming its real-time performance.&lt;/p&gt;</content:encoded></item><item><title>Unlocking Pseudolabel Potential and Alignment for Unpaired Cross-Modality Adaptation in Remote Sensing Image Segmentation</title><link>https://doi.org/10.1109/tnnls.2025.3635883</link><guid>10.1109/tnnls.2025.3635883</guid><pubDate>Tue, 02 Dec 2025 18:49:00 +0000</pubDate><dc:creator>Zhengyi Xu</dc:creator><dc:creator>Jie Geng</dc:creator><dc:creator>Wen Jiang</dc:creator><dc:creator>Shuai Song</dc:creator><prism:publicationName>IEEE Transactions on Neural Networks and Learning Systems</prism:publicationName><prism:doi>10.1109/tnnls.2025.3635883</prism:doi><description>With the growth of multisource sensor technology, multimodal learning has become pivotal in remote sensing (RS) image segmentation. Despite its potential, current methods face challenges in acquiring large-scale paired samples. When annotated optical images are available, but synthetic aperture radar (SAR) images lack annotations, learning discriminative features for SAR images from optical images becomes difficult. Unsupervised domain adaptation (UDA) offers a potential solution to this challenge, which we refer to as unpaired cross-modality UDA. In this article, we propose unlocking pseudolabel potential and alignment (ULPA) for unpaired cross-modality adaptation in RS image segmentation, a novel one-stage adaptation framework designed to enhance cross-modality knowledge transfer. Our approach employs a prototypical multidomain alignment (PMDA) strategy, which reduces the modality gap through contrastive learning between features and prototypes of identical classes across different modalities. In addition, we introduce the unreliable-sample-guided feature contrast (UFC) loss to address the underutilization of unreliable pixels during training. This strategy separates reliable and unreliable pixels based on prediction confidence, assigning unreliable pixels to a category-wise queue of negative samples, thus ensuring all candidate pixels contribute to the training process. Extensive experiments show that the integration of PMDA and UFC loss can lead to more effective cross-modality domain alignment and substantially boost the model’s generalization capability.
Published: 2025-12-02T18:49:00+00:00
Venue: IEEE Transactions on Neural Networks and Learning Systems
Score: 0.828 (must_read)</description><content:encoded>&lt;p&gt;&lt;strong&gt;Authors:&lt;/strong&gt; Zhengyi Xu; Jie Geng; Wen Jiang; Shuai Song&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Journal:&lt;/strong&gt; IEEE Transactions on Neural Networks and Learning Systems&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;a href="https://doi.org/10.1109/tnnls.2025.3635883"&gt;10.1109/tnnls.2025.3635883&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Relevance:&lt;/strong&gt; 0.828 (must_read)&lt;/p&gt;
&lt;p&gt;With the growth of multisource sensor technology, multimodal learning has become pivotal in remote sensing (RS) image segmentation. Despite its potential, current methods face challenges in acquiring large-scale paired samples. When annotated optical images are available, but synthetic aperture radar (SAR) images lack annotations, learning discriminative features for SAR images from optical images becomes difficult. Unsupervised domain adaptation (UDA) offers a potential solution to this challenge, which we refer to as unpaired cross-modality UDA. In this article, we propose unlocking pseudolabel potential and alignment (ULPA) for unpaired cross-modality adaptation in RS image segmentation, a novel one-stage adaptation framework designed to enhance cross-modality knowledge transfer. Our approach employs a prototypical multidomain alignment (PMDA) strategy, which reduces the modality gap through contrastive learning between features and prototypes of identical classes across different modalities. In addition, we introduce the unreliable-sample-guided feature contrast (UFC) loss to address the underutilization of unreliable pixels during training. This strategy separates reliable and unreliable pixels based on prediction confidence, assigning unreliable pixels to a category-wise queue of negative samples, thus ensuring all candidate pixels contribute to the training process. Extensive experiments show that the integration of PMDA and UFC loss can lead to more effective cross-modality domain alignment and substantially boost the model’s generalization capability.&lt;/p&gt;</content:encoded></item><item><title>MambaFusion: State-Space Model-Driven Object-Scene Fusion for Multi-Modal 3D Object Detection</title><link>https://doi.org/10.1016/j.patcog.2025.112820</link><guid>10.1016/j.patcog.2025.112820</guid><pubDate>Tue, 02 Dec 2025 00:17:20 +0000</pubDate><dc:creator>Tong Ning</dc:creator><dc:creator>Ke Lu</dc:creator><dc:creator>Xirui Jiang</dc:creator><dc:creator>Jian Xue</dc:creator><prism:publicationName>Pattern Recognition</prism:publicationName><prism:doi>10.1016/j.patcog.2025.112820</prism:doi><description>MambaFusion provides a hierarchical framework for highly efficient multi-modal 3D object detection. The method first achieves object-level fusion by integrating cross-modal object features. These fused features are then projected into the scene-level feature space to enable object-scene interaction, yielding the accurate 3D bounding boxes.
Existing multi-modal 3D detection struggles with geometric discrepancies between LiDAR/camera data and imbalanced feature alignment in Bird’s Eye View (BEV) space, where sparse foreground objects and scene-context gaps degrade performance. We propose MambaFusion, a novel framework unifying object-level fusion and scene-object interaction for robust 3D perception. Unlike scene-centric BEV fusion methods, MambaFusion introduces two modules: Object-Mamba, aligning 2D and 3D object candidates via grid-sorting and state-space models (SSM) to resolve modality inconsistencies, and Scene-Mamba, integrating image patches with object features and bidirectional SSM to model scene-object topological relationships. This dual-branch approach mitigates foreground-background imbalance and geometric misalignment while capturing holistic context. MambaFusion has achieved promising performance on both nuScenes and Waymo benchmarks.
Published: 2025-12-02T00:17:20+00:00
Venue: Pattern Recognition
Score: 0.824 (must_read)</description><content:encoded>&lt;p&gt;&lt;strong&gt;Authors:&lt;/strong&gt; Tong Ning; Ke Lu; Xirui Jiang; Jian Xue&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Journal:&lt;/strong&gt; Pattern Recognition&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;a href="https://doi.org/10.1016/j.patcog.2025.112820"&gt;10.1016/j.patcog.2025.112820&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Relevance:&lt;/strong&gt; 0.824 (must_read)&lt;/p&gt;
&lt;p&gt;MambaFusion provides a hierarchical framework for highly efficient multi-modal 3D object detection. The method first achieves object-level fusion by integrating cross-modal object features. These fused features are then projected into the scene-level feature space to enable object-scene interaction, yielding the accurate 3D bounding boxes.
Existing multi-modal 3D detection struggles with geometric discrepancies between LiDAR/camera data and imbalanced feature alignment in Bird’s Eye View (BEV) space, where sparse foreground objects and scene-context gaps degrade performance. We propose MambaFusion, a novel framework unifying object-level fusion and scene-object interaction for robust 3D perception. Unlike scene-centric BEV fusion methods, MambaFusion introduces two modules: Object-Mamba, aligning 2D and 3D object candidates via grid-sorting and state-space models (SSM) to resolve modality inconsistencies, and Scene-Mamba, integrating image patches with object features and bidirectional SSM to model scene-object topological relationships. This dual-branch approach mitigates foreground-background imbalance and geometric misalignment while capturing holistic context. MambaFusion has achieved promising performance on both nuScenes and Waymo benchmarks.&lt;/p&gt;</content:encoded></item><item><title>Diffusion-Based Text-Guided Image Generation with Fine-Grained Spatial Object-Attribute Relationships</title><link>https://doi.org/10.1109/tcsvt.2025.3639218</link><guid>10.1109/tcsvt.2025.3639218</guid><pubDate>Mon, 01 Dec 2025 18:25:50 +0000</pubDate><dc:creator>Fuxiang Wu</dc:creator><dc:creator>Liu Liu</dc:creator><dc:creator>Fusheng Hao</dc:creator><dc:creator>Ziliang Ren</dc:creator><dc:creator>Dacheng Tao</dc:creator><dc:creator>Xinyu Wu</dc:creator><dc:creator>Jun Cheng</dc:creator><prism:publicationName>IEEE Transactions on Circuits and Systems for Video Technology</prism:publicationName><prism:doi>10.1109/tcsvt.2025.3639218</prism:doi><description>Expressing and controlling fine-grained spatial attributes of objects in large-scale models presents significant challenges, as these spatial attributes are often difficult to describe textually and exhaustive enumeration is impractical. This hinders effective alignment with user preferences regarding spatial attribute-object relationships in fine-grained synthesis tasks. To tackle this problem, we propose AttrObjDiff, a novel framework built on the pre-trained Stable Diffusion model to integrate spatial attribute maps. Firstly, AttrObjDiff constrains the denoising step using trainable cross-attention fusion modules, attribute-enhancing cross-attention and LoRAs. The fusion modules take layout features extracted by a frozen ControlNet and corresponding fine-grained attribute maps as inputs to generate joint constraint features of spatial attribute-object relationships. We leverage attribute-enhancing cross-attention within the U-Net to further refine these spatial attributes. Finally, LoRAs are employed to align with these joint constraint features of finegrained relationships. Secondly, AttrObjDiff enhances the reverse process with lightweight noise reranking models to improve spatial object-attribute alignment. The reranking models select semantic noises related to fine-grained relationships, improving synthesis quality without significantly increasing computational costs. Experimental results demonstrate that our method can generate high-quality images guided by fine-grained spatial object-attribute relationships, improving synthesis controllability and semantic consistency.
Published: 2025-12-01T18:25:50+00:00
Venue: IEEE Transactions on Circuits and Systems for Video Technology
Score: 0.823 (must_read)</description><content:encoded>&lt;p&gt;&lt;strong&gt;Authors:&lt;/strong&gt; Fuxiang Wu; Liu Liu; Fusheng Hao; Ziliang Ren; Dacheng Tao; Xinyu Wu; Jun Cheng&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Journal:&lt;/strong&gt; IEEE Transactions on Circuits and Systems for Video Technology&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;a href="https://doi.org/10.1109/tcsvt.2025.3639218"&gt;10.1109/tcsvt.2025.3639218&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Relevance:&lt;/strong&gt; 0.823 (must_read)&lt;/p&gt;
&lt;p&gt;Expressing and controlling fine-grained spatial attributes of objects in large-scale models presents significant challenges, as these spatial attributes are often difficult to describe textually and exhaustive enumeration is impractical. This hinders effective alignment with user preferences regarding spatial attribute-object relationships in fine-grained synthesis tasks. To tackle this problem, we propose AttrObjDiff, a novel framework built on the pre-trained Stable Diffusion model to integrate spatial attribute maps. Firstly, AttrObjDiff constrains the denoising step using trainable cross-attention fusion modules, attribute-enhancing cross-attention and LoRAs. The fusion modules take layout features extracted by a frozen ControlNet and corresponding fine-grained attribute maps as inputs to generate joint constraint features of spatial attribute-object relationships. We leverage attribute-enhancing cross-attention within the U-Net to further refine these spatial attributes. Finally, LoRAs are employed to align with these joint constraint features of finegrained relationships. Secondly, AttrObjDiff enhances the reverse process with lightweight noise reranking models to improve spatial object-attribute alignment. The reranking models select semantic noises related to fine-grained relationships, improving synthesis quality without significantly increasing computational costs. Experimental results demonstrate that our method can generate high-quality images guided by fine-grained spatial object-attribute relationships, improving synthesis controllability and semantic consistency.&lt;/p&gt;</content:encoded></item><item><title>Refinement-Guided Critique Learning: A Framework for Training Critique Models</title><link>https://doi.org/10.1016/j.inffus.2025.104002</link><guid>10.1016/j.inffus.2025.104002</guid><pubDate>Mon, 01 Dec 2025 16:11:19 +0000</pubDate><dc:creator>Chao Xiang</dc:creator><dc:creator>Junhao Zheng</dc:creator><dc:creator>Xinyu Mu</dc:creator><dc:creator>Tianshu Yu</dc:creator><dc:creator>Li Zhang</dc:creator><dc:creator>Chuxiong Sun</dc:creator><dc:creator>Lijuan Shi</dc:creator><dc:creator>Feng Wang</dc:creator><dc:creator>Mingchuan Yang</dc:creator><prism:publicationName>Information Fusion</prism:publicationName><prism:doi>10.1016/j.inffus.2025.104002</prism:doi><description>Large language models have shown exceptional assessment and analytical abilities, offering valuable insights and detecting deficiencies across diverse tasks. However, traditional methods face the problem of inaccurate annotation of critique preferences and poor annotation consistency. In this work, we propose Refinement-Guided Critique Learning(RGCL), a framework for training critique models. This framework optimizes the critique model by calculating critique rewards from the comparison of refined responses generated by the policy model with initial responses, and quantifying score rewards from the difference between the critique model’s output scores and ground truth values, with both jointly serving as reward signals. We evaluate the RGCL framework across five tasks, i.e., dialog generation, summarization, question answering, mathematical reasoning, and code generation, and show that it significantly outperforms traditional methods and open-source models in terms of critique quality and refinement outcomes.
Published: 2025-12-01T16:11:19+00:00
Venue: Information Fusion
Score: 0.821 (must_read)</description><content:encoded>&lt;p&gt;&lt;strong&gt;Authors:&lt;/strong&gt; Chao Xiang; Junhao Zheng; Xinyu Mu; Tianshu Yu; Li Zhang; Chuxiong Sun; Lijuan Shi; Feng Wang; Mingchuan Yang&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Journal:&lt;/strong&gt; Information Fusion&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;a href="https://doi.org/10.1016/j.inffus.2025.104002"&gt;10.1016/j.inffus.2025.104002&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Relevance:&lt;/strong&gt; 0.821 (must_read)&lt;/p&gt;
&lt;p&gt;Large language models have shown exceptional assessment and analytical abilities, offering valuable insights and detecting deficiencies across diverse tasks. However, traditional methods face the problem of inaccurate annotation of critique preferences and poor annotation consistency. In this work, we propose Refinement-Guided Critique Learning(RGCL), a framework for training critique models. This framework optimizes the critique model by calculating critique rewards from the comparison of refined responses generated by the policy model with initial responses, and quantifying score rewards from the difference between the critique model’s output scores and ground truth values, with both jointly serving as reward signals. We evaluate the RGCL framework across five tasks, i.e., dialog generation, summarization, question answering, mathematical reasoning, and code generation, and show that it significantly outperforms traditional methods and open-source models in terms of critique quality and refinement outcomes.&lt;/p&gt;</content:encoded></item><item><title>Self-supervised despeckling based solely on SAR intensity images: A general strategy</title><link>https://doi.org/10.1016/j.isprsjprs.2025.11.025</link><guid>10.1016/j.isprsjprs.2025.11.025</guid><pubDate>Mon, 01 Dec 2025 22:06:47 +0000</pubDate><dc:creator>Liang Chen</dc:creator><dc:creator>Yifei Yin</dc:creator><dc:creator>Hao Shi</dc:creator><dc:creator>Jingfei He</dc:creator><dc:creator>Wei Li</dc:creator><prism:publicationName>ISPRS Journal of Photogrammetry and Remote Sensing</prism:publicationName><prism:doi>10.1016/j.isprsjprs.2025.11.025</prism:doi><description>Speckle noise is generated along with the SAR imaging mechanism and degrades the quality of SAR images, leading to difficult interpretation. Hence, despeckling is an indispensable step in SAR pre-processing. Fortunately, supervised learning (SL) has proven to be a progressive method for SAR image despeckling. SL methods necessitate the availability of both original SAR images and their speckle-free counterparts during training, whilst speckle-free SAR images do not exist in the real world. Even though there are several substitutes for speckle-free images, the domain gap leads to poor performance and adaptability. Self-supervision provides an approach to training without clean reference. However, most self-supervised methods introduce additional requirements on speckle modeling or specific data, posing challenges in real-world applications. To address these challenges, we propose a general Self-supervised Despeckling Strategy for SAR images (SDS-SAR) that relies solely on speckled intensity data for training. Firstly, the theoretical feasibility of SAR image despeckling without speckle-free images is established. A self-supervised despeckling criteria suitable for diverse SAR images is proposed. Subsequently, a Random-Aware sub-SAMpler with Projection correLation Estimation (RA-SAMPLE) is put forth. Mutually independent training pairs can be derived from actual SAR intensity images. Furthermore, a multi-feature loss function is introduced, consisting of a despeckling term, a regularization term, and a perception term. The performance of speckle suppression and texture preservation is well-balanced. Experiments reveal that the proposed method performs comparably to supervised approaches on synthetic data and outperforms them on actual data. Both visual and quantitative evaluations confirm its superiority over state-of-the-art despeckling techniques. Moreover, the results demonstrates that SDS-SAR provides a novel solution for noise suppression in other multiplicative coherent systems. The trained model and dataset will be available at https://github.com/YYF121/SDS-SAR .
Published: 2025-12-01T22:06:47+00:00
Venue: ISPRS Journal of Photogrammetry and Remote Sensing
Score: 0.820 (must_read)</description><content:encoded>&lt;p&gt;&lt;strong&gt;Authors:&lt;/strong&gt; Liang Chen; Yifei Yin; Hao Shi; Jingfei He; Wei Li&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Journal:&lt;/strong&gt; ISPRS Journal of Photogrammetry and Remote Sensing&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;a href="https://doi.org/10.1016/j.isprsjprs.2025.11.025"&gt;10.1016/j.isprsjprs.2025.11.025&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Relevance:&lt;/strong&gt; 0.820 (must_read)&lt;/p&gt;
&lt;p&gt;Speckle noise is generated along with the SAR imaging mechanism and degrades the quality of SAR images, leading to difficult interpretation. Hence, despeckling is an indispensable step in SAR pre-processing. Fortunately, supervised learning (SL) has proven to be a progressive method for SAR image despeckling. SL methods necessitate the availability of both original SAR images and their speckle-free counterparts during training, whilst speckle-free SAR images do not exist in the real world. Even though there are several substitutes for speckle-free images, the domain gap leads to poor performance and adaptability. Self-supervision provides an approach to training without clean reference. However, most self-supervised methods introduce additional requirements on speckle modeling or specific data, posing challenges in real-world applications. To address these challenges, we propose a general Self-supervised Despeckling Strategy for SAR images (SDS-SAR) that relies solely on speckled intensity data for training. Firstly, the theoretical feasibility of SAR image despeckling without speckle-free images is established. A self-supervised despeckling criteria suitable for diverse SAR images is proposed. Subsequently, a Random-Aware sub-SAMpler with Projection correLation Estimation (RA-SAMPLE) is put forth. Mutually independent training pairs can be derived from actual SAR intensity images. Furthermore, a multi-feature loss function is introduced, consisting of a despeckling term, a regularization term, and a perception term. The performance of speckle suppression and texture preservation is well-balanced. Experiments reveal that the proposed method performs comparably to supervised approaches on synthetic data and outperforms them on actual data. Both visual and quantitative evaluations confirm its superiority over state-of-the-art despeckling techniques. Moreover, the results demonstrates that SDS-SAR provides a novel solution for noise suppression in other multiplicative coherent systems. The trained model and dataset will be available at https://github.com/YYF121/SDS-SAR .&lt;/p&gt;</content:encoded></item><item><title>Adaptive iterative retrieval for enhanced retrieval-augmented generation</title><link>https://doi.org/10.1016/j.neucom.2025.132272</link><guid>10.1016/j.neucom.2025.132272</guid><pubDate>Tue, 02 Dec 2025 07:52:56 +0000</pubDate><dc:creator>Wenhan Han</dc:creator><dc:creator>Xiao Xiao</dc:creator><dc:creator>Yaohang Li</dc:creator><dc:creator>Jun Wang</dc:creator><dc:creator>Mykola Pechenizkiy</dc:creator><dc:creator>Meng Fang</dc:creator><prism:publicationName>Neurocomputing</prism:publicationName><prism:doi>10.1016/j.neucom.2025.132272</prism:doi><description>Existing retrieval-augmented generation (RAG) methods often treat retrieval as a one-off operation, yet recent work suggests that iteratively refining the retrieval step can yield substantial gains in relevance and downstream generation quality. However, prior iterative-retrieval approaches typically optimize only the retriever’s ranking function or only post-hoc document refinement, and they require expensive retriever retraining or complex multi-stage pipelines. To address these challenges, we propose Adaptive Iterative Retrieval for Retrieval-Augmented Generation (AIR-RAG), an adaptive, iterative retrieval framework designed to optimize both document relevance and LLM alignment within the RAG pipeline. By leveraging adaptive feedback, AIR-RAG simultaneously enhances retrieval ranking and document refinement across multiple iterations, eliminating the need for complex retraining pipelines and enabling seamless integration with existing systems. In extensive evaluations against state-of-the-art RAG methods across several benchmark datasets including TriviaQA, PopQA, HotpotQA, WikiMultiHop, PubHealth, and StrategyQA, AIR-RAG consistently demonstrates superior performance, underscoring its effectiveness in enhancing retrieval-augmented generation systems. Our code and data are available anonymously at https://github.com/aialt/AIR-RAG .
Published: 2025-12-02T07:52:56+00:00
Venue: Neurocomputing
Score: 0.818 (must_read)</description><content:encoded>&lt;p&gt;&lt;strong&gt;Authors:&lt;/strong&gt; Wenhan Han; Xiao Xiao; Yaohang Li; Jun Wang; Mykola Pechenizkiy; Meng Fang&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Journal:&lt;/strong&gt; Neurocomputing&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;a href="https://doi.org/10.1016/j.neucom.2025.132272"&gt;10.1016/j.neucom.2025.132272&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Relevance:&lt;/strong&gt; 0.818 (must_read)&lt;/p&gt;
&lt;p&gt;Existing retrieval-augmented generation (RAG) methods often treat retrieval as a one-off operation, yet recent work suggests that iteratively refining the retrieval step can yield substantial gains in relevance and downstream generation quality. However, prior iterative-retrieval approaches typically optimize only the retriever’s ranking function or only post-hoc document refinement, and they require expensive retriever retraining or complex multi-stage pipelines. To address these challenges, we propose Adaptive Iterative Retrieval for Retrieval-Augmented Generation (AIR-RAG), an adaptive, iterative retrieval framework designed to optimize both document relevance and LLM alignment within the RAG pipeline. By leveraging adaptive feedback, AIR-RAG simultaneously enhances retrieval ranking and document refinement across multiple iterations, eliminating the need for complex retraining pipelines and enabling seamless integration with existing systems. In extensive evaluations against state-of-the-art RAG methods across several benchmark datasets including TriviaQA, PopQA, HotpotQA, WikiMultiHop, PubHealth, and StrategyQA, AIR-RAG consistently demonstrates superior performance, underscoring its effectiveness in enhancing retrieval-augmented generation systems. Our code and data are available anonymously at https://github.com/aialt/AIR-RAG .&lt;/p&gt;</content:encoded></item><item><title>Balanced Multi-modality Knowledge Mining for RGB-Infrared Object Detection</title><link>https://doi.org/10.1016/j.neunet.2025.108421</link><guid>10.1016/j.neunet.2025.108421</guid><pubDate>Tue, 02 Dec 2025 07:52:46 +0000</pubDate><dc:creator>You Ma</dc:creator><dc:creator>Yucheng Zhang</dc:creator><dc:creator>Shihan Mao</dc:creator><dc:creator>Lin Chai</dc:creator><dc:creator>Qingling Wang</dc:creator><prism:publicationName>Neural Networks</prism:publicationName><prism:doi>10.1016/j.neunet.2025.108421</prism:doi><description>RGB-Infrared object detection aims to fuse the complementary information of two modalities to improve the accuracy and robustness of the detector. Given the advantages of transformer in modeling long-range dependencies, transformer-based cross-modality fusion methods have been continuously proposed and achieved satisfactory results. However, existing methods face two major challenges: 1) it is difficult to balance the mining of intra-modality specific knowledge and inter-modality complementary knowledge; 2) a single attention layer only models the relationship between token features of the same receptive field, thus failing to capture the intrinsic relationship between objects at different scales and lacking the ability to focus on both local and global information. To this end, we propose a balanced multi-modality knowledge mining method. Specifically, we design a dual attention knowledge mining (DAKM) module, which explicitly mines intra- and inter-modality key knowledge through self-attention and cross-attention, respectively. In addition, we introduce multi-scale information into the attention layer of DAKM, which not only extracts multi-scale object features but also retains both local and global information. Then, we fuse the intra- and inter-modality features obtained by DAKM using the scene-aware adaptive interaction module. The module employs differential and scene information to focus on object-related feature fusion. Finally, the cross-layer feature refinement module is utilized to aggregate different fusion layers to further enhance the feature representation. Extensive experiments in multiple scenes demonstrate that our method outperforms existing state-of-the-art RGB-Infrared object detection methods.
Published: 2025-12-02T07:52:46+00:00
Venue: Neural Networks
Score: 0.818 (must_read)</description><content:encoded>&lt;p&gt;&lt;strong&gt;Authors:&lt;/strong&gt; You Ma; Yucheng Zhang; Shihan Mao; Lin Chai; Qingling Wang&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Journal:&lt;/strong&gt; Neural Networks&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;a href="https://doi.org/10.1016/j.neunet.2025.108421"&gt;10.1016/j.neunet.2025.108421&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Relevance:&lt;/strong&gt; 0.818 (must_read)&lt;/p&gt;
&lt;p&gt;RGB-Infrared object detection aims to fuse the complementary information of two modalities to improve the accuracy and robustness of the detector. Given the advantages of transformer in modeling long-range dependencies, transformer-based cross-modality fusion methods have been continuously proposed and achieved satisfactory results. However, existing methods face two major challenges: 1) it is difficult to balance the mining of intra-modality specific knowledge and inter-modality complementary knowledge; 2) a single attention layer only models the relationship between token features of the same receptive field, thus failing to capture the intrinsic relationship between objects at different scales and lacking the ability to focus on both local and global information. To this end, we propose a balanced multi-modality knowledge mining method. Specifically, we design a dual attention knowledge mining (DAKM) module, which explicitly mines intra- and inter-modality key knowledge through self-attention and cross-attention, respectively. In addition, we introduce multi-scale information into the attention layer of DAKM, which not only extracts multi-scale object features but also retains both local and global information. Then, we fuse the intra- and inter-modality features obtained by DAKM using the scene-aware adaptive interaction module. The module employs differential and scene information to focus on object-related feature fusion. Finally, the cross-layer feature refinement module is utilized to aggregate different fusion layers to further enhance the feature representation. Extensive experiments in multiple scenes demonstrate that our method outperforms existing state-of-the-art RGB-Infrared object detection methods.&lt;/p&gt;</content:encoded></item><item><title>Rethinking Domain-Agnostic Continual Learning via Frequency Completeness Learning</title><link>https://doi.org/10.1016/j.inffus.2025.103961</link><guid>10.1016/j.inffus.2025.103961</guid><pubDate>Mon, 01 Dec 2025 16:11:22 +0000</pubDate><dc:creator>Jian Peng</dc:creator><dc:creator>Haitao Zhang</dc:creator><dc:creator>Jing Shen</dc:creator><dc:creator>Zeyi Li</dc:creator><dc:creator>Jiayi Ma</dc:creator><dc:creator>Haifeng Li</dc:creator><prism:publicationName>Information Fusion</prism:publicationName><prism:doi>10.1016/j.inffus.2025.103961</prism:doi><description>Continual learning addresses knowledge acquisition while mitigating catastrophic forgetting in evolving task environments. Current spatial domain approaches exhibit limitations in cross-domain scenarios with unknown domain shifts. We reformulate cross-domain continual learning as an extension of single-domain generalization, introducing a novel frequency domain perspective that remains underexplored in continual learning research. Our analysis reveals the Forgetting Frequency Bias Hypothesis: model forgetting escalates with increasing frequency distribution gaps between tasks. Specifically, task-specific frequency overfitting emerges as a critical factor, where closer inter-task frequency distributions correlate with reduced forgetting. Building on this insight, we propose Frequency-Completeness Learning (FCL), a dual-path framework that disentangles high/low-frequency components through spectral reconstruction to enhance frequency diversity. Complementing this, we develop Frequency Domain Shuffling (FDS), a semantic-preserving augmentation strategy that improves style diversity while maintaining domain-invariant features. Extensive experiments on incremental classification (CIFAR-100, ImageNet-100, ImageNet-R) and semantic segmentation demonstrate FCL’s effectiveness. Our method achieves up to 10% improvement over baselines when integrated with existing continual learning techniques. The consistent performance gains across arbitrary domain scenarios underscore the importance of frequency completeness in addressing cross-domain continual learning challenges. The source code is available at https://github.com/GeoX-Lab/FCL .
Published: 2025-12-01T16:11:22+00:00
Venue: Information Fusion
Score: 0.816 (must_read)</description><content:encoded>&lt;p&gt;&lt;strong&gt;Authors:&lt;/strong&gt; Jian Peng; Haitao Zhang; Jing Shen; Zeyi Li; Jiayi Ma; Haifeng Li&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Journal:&lt;/strong&gt; Information Fusion&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;a href="https://doi.org/10.1016/j.inffus.2025.103961"&gt;10.1016/j.inffus.2025.103961&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Relevance:&lt;/strong&gt; 0.816 (must_read)&lt;/p&gt;
&lt;p&gt;Continual learning addresses knowledge acquisition while mitigating catastrophic forgetting in evolving task environments. Current spatial domain approaches exhibit limitations in cross-domain scenarios with unknown domain shifts. We reformulate cross-domain continual learning as an extension of single-domain generalization, introducing a novel frequency domain perspective that remains underexplored in continual learning research. Our analysis reveals the Forgetting Frequency Bias Hypothesis: model forgetting escalates with increasing frequency distribution gaps between tasks. Specifically, task-specific frequency overfitting emerges as a critical factor, where closer inter-task frequency distributions correlate with reduced forgetting. Building on this insight, we propose Frequency-Completeness Learning (FCL), a dual-path framework that disentangles high/low-frequency components through spectral reconstruction to enhance frequency diversity. Complementing this, we develop Frequency Domain Shuffling (FDS), a semantic-preserving augmentation strategy that improves style diversity while maintaining domain-invariant features. Extensive experiments on incremental classification (CIFAR-100, ImageNet-100, ImageNet-R) and semantic segmentation demonstrate FCL’s effectiveness. Our method achieves up to 10% improvement over baselines when integrated with existing continual learning techniques. The consistent performance gains across arbitrary domain scenarios underscore the importance of frequency completeness in addressing cross-domain continual learning challenges. The source code is available at https://github.com/GeoX-Lab/FCL .&lt;/p&gt;</content:encoded></item><item><title>Multimodal cross fusion Mamba network for remote sensing image semantic segmentation with complementary masked self-supervision</title><link>https://doi.org/10.1016/j.jag.2025.104960</link><guid>10.1016/j.jag.2025.104960</guid><pubDate>Mon, 01 Dec 2025 08:22:58 +0000</pubDate><dc:creator>Xiao Liu</dc:creator><dc:creator>Tao Wang</dc:creator><dc:creator>Fei Jin</dc:creator><dc:creator>Jie Rui</dc:creator><dc:creator>Shuxiang Wang</dc:creator><dc:creator>Ziheng Huang</dc:creator><dc:creator>Yujie Zou</dc:creator><dc:creator>Xiaowei Yu</dc:creator><prism:publicationName>International Journal of Applied Earth Observation and Geoinformation</prism:publicationName><prism:doi>10.1016/j.jag.2025.104960</prism:doi><description>Deep learning-based multimodal remote sensing image (RSI) semantic segmentation models have garnered significant attention owing to their ability to leverage complementary information across modalities, resulting in more robust and accurate Earth observation. However, most existing approaches rely predominantly on convolutional neural networks with limited receptive fields or transformer-based architectures that are computationally intensive. In this study, we proposed a multimodal cross fusion Mamba (MCF-Mamba) network for multimodal RSI semantic segmentation, aiming to collaboratively enhance accuracy and efficiency. The proposed network features three core components: a dual-branch VMamba encoder, a multimodal cross-Mamba fusion module, and a U-shaped Mamba decoder. The architecture with a unified structure centered on the selective state space model ensures that it achieves global perception with linear complexity. Furthermore, for addressing the constraint of limited labeled samples in practical applications, we developed a generative multimodal complementary masked self-supervised (CMSS) strategy. It leverages abundant unlabeled RSIs to learn generalized multimodal representations by modeling intermodal complementary consistency. Extensive experiments on three public datasets involving optical-SAR and optical-DEM modalities demonstrated that the proposed network and strategy are superior to other advanced segmentation models and self-supervised methods in land cover mapping and building extraction tasks. The MCF-Mamba network achieves the highest accuracy while significantly reducing both model size and computational cost, and further improves the accuracy and generalization through the CMSS strategy. The source code is available at https://github.com/Xiao-RS/MCFMamba_CMSS .
Published: 2025-12-01T08:22:58+00:00
Venue: International Journal of Applied Earth Observation and Geoinformation
Score: 0.807 (must_read)</description><content:encoded>&lt;p&gt;&lt;strong&gt;Authors:&lt;/strong&gt; Xiao Liu; Tao Wang; Fei Jin; Jie Rui; Shuxiang Wang; Ziheng Huang; Yujie Zou; Xiaowei Yu&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Journal:&lt;/strong&gt; International Journal of Applied Earth Observation and Geoinformation&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;a href="https://doi.org/10.1016/j.jag.2025.104960"&gt;10.1016/j.jag.2025.104960&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Relevance:&lt;/strong&gt; 0.807 (must_read)&lt;/p&gt;
&lt;p&gt;Deep learning-based multimodal remote sensing image (RSI) semantic segmentation models have garnered significant attention owing to their ability to leverage complementary information across modalities, resulting in more robust and accurate Earth observation. However, most existing approaches rely predominantly on convolutional neural networks with limited receptive fields or transformer-based architectures that are computationally intensive. In this study, we proposed a multimodal cross fusion Mamba (MCF-Mamba) network for multimodal RSI semantic segmentation, aiming to collaboratively enhance accuracy and efficiency. The proposed network features three core components: a dual-branch VMamba encoder, a multimodal cross-Mamba fusion module, and a U-shaped Mamba decoder. The architecture with a unified structure centered on the selective state space model ensures that it achieves global perception with linear complexity. Furthermore, for addressing the constraint of limited labeled samples in practical applications, we developed a generative multimodal complementary masked self-supervised (CMSS) strategy. It leverages abundant unlabeled RSIs to learn generalized multimodal representations by modeling intermodal complementary consistency. Extensive experiments on three public datasets involving optical-SAR and optical-DEM modalities demonstrated that the proposed network and strategy are superior to other advanced segmentation models and self-supervised methods in land cover mapping and building extraction tasks. The MCF-Mamba network achieves the highest accuracy while significantly reducing both model size and computational cost, and further improves the accuracy and generalization through the CMSS strategy. The source code is available at https://github.com/Xiao-RS/MCFMamba_CMSS .&lt;/p&gt;</content:encoded></item><item><title>Causal Reasoning Meets Heuristic Strategies: Enhancing RAG through Fine-Tuning and Knowledge Interaction</title><link>https://doi.org/10.1016/j.knosys.2025.114976</link><guid>10.1016/j.knosys.2025.114976</guid><pubDate>Mon, 01 Dec 2025 16:47:57 +0000</pubDate><dc:creator>Xun Luo</dc:creator><dc:creator>Yuzhong Chen</dc:creator><dc:creator>Yanhao Tu</dc:creator><dc:creator>Wenju Qiu</dc:creator><prism:publicationName>Knowledge-Based Systems</prism:publicationName><prism:doi>10.1016/j.knosys.2025.114976</prism:doi><description>Retrieval-augmented Generation (RAG) enhances large language models (LLMs) with external knowledge, but traditional approaches typically rely on surface-level relevance and lack robustness to noisy or conflicting information. In real-world scenarios, users often pose complex queries that require accurate, contextually grounded reasoning, posing two key challenges for RAG systems. The first challenge is how to extract truly supportive evidence from noisy or topically similar but uninformative documents. The second challenge is how to resolve conflicts between internal parameterized knowledge and external knowledge from retrieved documents. To address these challenges, we propose CRGS-RAG, a framework that incorporates the Causal Reasoning Fine-Tuning Strategy and Game-Theory-Inspired Knowledge Fusion Strategy. The Causal Reasoning Fine-Tuning Strategy improves model robustness by training it to focus on causally relevant evidence, while the Game-Theory-Inspired Knowledge Fusion Strategy enables CRGS-RAG to adaptively integrate internal parameterized knowledge and external knowledge from retrieved documents under conflicting conditions. Experiments on five open-domain QA benchmark datasets show that CRGS-RAG consistently outperforms the state-of-the-art RAG baselines in accuracy and consistency. Furthermore, ablation studies reveal that the Causal Reasoning Fine-Tuning Strategy significantly enhances CRGS-RAG’s reasoning ability under noisy retrieval, while the Game-Theory-Inspired Knowledge Fusion Strategy module improves factual alignment and robustness in fusing knowledge from multiple sources. To facilitate reproduction, our code is available at https://github.com/yuanlill/CRGS-RAG .
Published: 2025-12-01T16:47:57+00:00
Venue: Knowledge-Based Systems
Score: 0.805 (must_read)</description><content:encoded>&lt;p&gt;&lt;strong&gt;Authors:&lt;/strong&gt; Xun Luo; Yuzhong Chen; Yanhao Tu; Wenju Qiu&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Journal:&lt;/strong&gt; Knowledge-Based Systems&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;a href="https://doi.org/10.1016/j.knosys.2025.114976"&gt;10.1016/j.knosys.2025.114976&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Relevance:&lt;/strong&gt; 0.805 (must_read)&lt;/p&gt;
&lt;p&gt;Retrieval-augmented Generation (RAG) enhances large language models (LLMs) with external knowledge, but traditional approaches typically rely on surface-level relevance and lack robustness to noisy or conflicting information. In real-world scenarios, users often pose complex queries that require accurate, contextually grounded reasoning, posing two key challenges for RAG systems. The first challenge is how to extract truly supportive evidence from noisy or topically similar but uninformative documents. The second challenge is how to resolve conflicts between internal parameterized knowledge and external knowledge from retrieved documents. To address these challenges, we propose CRGS-RAG, a framework that incorporates the Causal Reasoning Fine-Tuning Strategy and Game-Theory-Inspired Knowledge Fusion Strategy. The Causal Reasoning Fine-Tuning Strategy improves model robustness by training it to focus on causally relevant evidence, while the Game-Theory-Inspired Knowledge Fusion Strategy enables CRGS-RAG to adaptively integrate internal parameterized knowledge and external knowledge from retrieved documents under conflicting conditions. Experiments on five open-domain QA benchmark datasets show that CRGS-RAG consistently outperforms the state-of-the-art RAG baselines in accuracy and consistency. Furthermore, ablation studies reveal that the Causal Reasoning Fine-Tuning Strategy significantly enhances CRGS-RAG’s reasoning ability under noisy retrieval, while the Game-Theory-Inspired Knowledge Fusion Strategy module improves factual alignment and robustness in fusing knowledge from multiple sources. To facilitate reproduction, our code is available at https://github.com/yuanlill/CRGS-RAG .&lt;/p&gt;</content:encoded></item><item><title>Enhancing multi-label zero-shot learning with dual-contrastive image-text alignment</title><link>https://doi.org/10.1016/j.neucom.2025.132251</link><guid>10.1016/j.neucom.2025.132251</guid><pubDate>Tue, 02 Dec 2025 00:22:55 +0000</pubDate><dc:creator>Zhongchen Ma</dc:creator><dc:creator>Junjie Yang</dc:creator><dc:creator>Ahmed Belloul</dc:creator><prism:publicationName>Neurocomputing</prism:publicationName><prism:doi>10.1016/j.neucom.2025.132251</prism:doi><description>Prompt learning has emerged as a prevalent strategy for adapting vision-language models like CLIP to multi-label zero-shot learning (ML-ZSL). However, these methods primarily rely on global image-text alignment, lacking the fine-grained mechanisms necessary to link specific image regions with their textual counterparts, which is crucial for complex multi-label scenes. To address these issues, we propose a unified framework that integrates three key components: a Dual Contrastive Alignment (DCA) regularization, a Multi-Granularity Data Augmentation (MGDA) strategy, and a Cross-Attention Alignment Module (CAM). The DCA regularization introduces two complementary constraints—Contrastive Image Content (CIC) and Contrastive Text Content (CTC)—to enhance both image-to-text and text-to-image alignment through mutual contrastive learning. The MGDA strategy synthesizes composite images and unified label sets to enrich supervisory signals and improve feature discriminability. The CAM module leverages cross-modal attention to dynamically focus on relevant image regions guided by text embeddings, ensuring precise local alignment. Extensive experiments on NUS-WIDE and MS-COCO datasets demonstrate that our approach achieves state-of-the-art performance, with mAP improvements of 3.1 % and 6.8 %, respectively, over previous best results. These advancements underscore the effectiveness of our method in enhancing fine-grained visual-textual alignment and facilitating robust multi-label zero-shot recognition.
Published: 2025-12-02T00:22:55+00:00
Venue: Neurocomputing
Score: 0.804 (must_read)</description><content:encoded>&lt;p&gt;&lt;strong&gt;Authors:&lt;/strong&gt; Zhongchen Ma; Junjie Yang; Ahmed Belloul&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Journal:&lt;/strong&gt; Neurocomputing&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;a href="https://doi.org/10.1016/j.neucom.2025.132251"&gt;10.1016/j.neucom.2025.132251&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Relevance:&lt;/strong&gt; 0.804 (must_read)&lt;/p&gt;
&lt;p&gt;Prompt learning has emerged as a prevalent strategy for adapting vision-language models like CLIP to multi-label zero-shot learning (ML-ZSL). However, these methods primarily rely on global image-text alignment, lacking the fine-grained mechanisms necessary to link specific image regions with their textual counterparts, which is crucial for complex multi-label scenes. To address these issues, we propose a unified framework that integrates three key components: a Dual Contrastive Alignment (DCA) regularization, a Multi-Granularity Data Augmentation (MGDA) strategy, and a Cross-Attention Alignment Module (CAM). The DCA regularization introduces two complementary constraints—Contrastive Image Content (CIC) and Contrastive Text Content (CTC)—to enhance both image-to-text and text-to-image alignment through mutual contrastive learning. The MGDA strategy synthesizes composite images and unified label sets to enrich supervisory signals and improve feature discriminability. The CAM module leverages cross-modal attention to dynamically focus on relevant image regions guided by text embeddings, ensuring precise local alignment. Extensive experiments on NUS-WIDE and MS-COCO datasets demonstrate that our approach achieves state-of-the-art performance, with mAP improvements of 3.1 % and 6.8 %, respectively, over previous best results. These advancements underscore the effectiveness of our method in enhancing fine-grained visual-textual alignment and facilitating robust multi-label zero-shot recognition.&lt;/p&gt;</content:encoded></item><item><title>Bidirectional parallel multi-layer multi-scale hybrid network</title><link>https://doi.org/10.1016/j.neucom.2025.132255</link><guid>10.1016/j.neucom.2025.132255</guid><pubDate>Mon, 01 Dec 2025 16:08:14 +0000</pubDate><dc:creator>Chunguang Yue</dc:creator><dc:creator>Jinbao Li</dc:creator><dc:creator>Donghuan Zhang</dc:creator><dc:creator>Xiaowei Liu</dc:creator><prism:publicationName>Neurocomputing</prism:publicationName><prism:doi>10.1016/j.neucom.2025.132255</prism:doi><description>Although Vision Transformer (ViT) can directly process images, the division of images into patches lacks internal interaction in patches and has a single feature scale, leading to suboptimal performance in dense prediction tasks. Most existing research focuses on serial networks that combine the strengths of CNN and ViT to address these issues, but this often disrupts the ViT structure and introduces additional pretraining costs. In this paper, we propose BiPNet (the Bidirectional Parallel Multi-layer Multi-scale Hybrid Network), which addresses the aforementioned issues by facilitating information interaction between CNNs and Transformers and can directly leverage existing ViT pre-trained weights. Compared to existing methods, our Bidirectional Parallel Multi-Layer Multi-Scale Hybrid Network has the following advantages: 1.The CNN and Transformer are used in parallel to fully retain the ViT architecture, making use of existing pre-trained models. 2.A 3M (multi-layer, multi-scale convolutional module) is proposed to handle the spatial pyramid information of CNNs, addressing the problem of insufficient local feature interaction and single feature representation within ViT. 3.A simple CNN-Transformer BiLGM (bidirectional local-global interaction module) is introduced, which performs both local-global interaction and balances high and low-frequency semantics, making it beneficial for handling dense prediction tasks. It achieves 63.9 % " role="presentation"&gt; on COCO val2017 and 62.0 % mIoU on ADE20K with its super-large model without using additional training data.
Published: 2025-12-01T16:08:14+00:00
Venue: Neurocomputing
Score: 0.800 (consider)</description><content:encoded>&lt;p&gt;&lt;strong&gt;Authors:&lt;/strong&gt; Chunguang Yue; Jinbao Li; Donghuan Zhang; Xiaowei Liu&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Journal:&lt;/strong&gt; Neurocomputing&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;a href="https://doi.org/10.1016/j.neucom.2025.132255"&gt;10.1016/j.neucom.2025.132255&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Relevance:&lt;/strong&gt; 0.800 (consider)&lt;/p&gt;
&lt;p&gt;Although Vision Transformer (ViT) can directly process images, the division of images into patches lacks internal interaction in patches and has a single feature scale, leading to suboptimal performance in dense prediction tasks. Most existing research focuses on serial networks that combine the strengths of CNN and ViT to address these issues, but this often disrupts the ViT structure and introduces additional pretraining costs. In this paper, we propose BiPNet (the Bidirectional Parallel Multi-layer Multi-scale Hybrid Network), which addresses the aforementioned issues by facilitating information interaction between CNNs and Transformers and can directly leverage existing ViT pre-trained weights. Compared to existing methods, our Bidirectional Parallel Multi-Layer Multi-Scale Hybrid Network has the following advantages: 1.The CNN and Transformer are used in parallel to fully retain the ViT architecture, making use of existing pre-trained models. 2.A 3M (multi-layer, multi-scale convolutional module) is proposed to handle the spatial pyramid information of CNNs, addressing the problem of insufficient local feature interaction and single feature representation within ViT. 3.A simple CNN-Transformer BiLGM (bidirectional local-global interaction module) is introduced, which performs both local-global interaction and balances high and low-frequency semantics, making it beneficial for handling dense prediction tasks. It achieves 63.9 % &amp;quot; role=&amp;quot;presentation&amp;quot;&amp;gt; on COCO val2017 and 62.0 % mIoU on ADE20K with its super-large model without using additional training data.&lt;/p&gt;</content:encoded></item><item><title>Cross-Scale Adaptive Transformer with Hierarchical Feature Synergy for Aerial Small Object Detection</title><link>https://doi.org/10.1016/j.patcog.2025.112822</link><guid>10.1016/j.patcog.2025.112822</guid><pubDate>Tue, 02 Dec 2025 16:07:21 +0000</pubDate><dc:creator>Wenke Zhang</dc:creator><dc:creator>Mengmeng Liao</dc:creator><prism:publicationName>Pattern Recognition</prism:publicationName><prism:doi>10.1016/j.patcog.2025.112822</prism:doi><description>Small object detection has long been a challenging task in computer vision due to limited pixel representation, extremely small scales, and complex scene variations. These challenges are particularly pronounced in high-resolution aerial imagery, where small objects refer to targets that appear as small-scale in images due to long shooting distances, characterized by limited pixel coverage and sparse feature representation. To address these issues, this paper proposes a novel object detection framework based on a Cross-Scale Adaptive Transformer and Hierarchical Feature Synergy. The framework introduces a Cross-Scale Adaptive Transformer module (CST) to dynamically capture multi-scale features in horizontal and vertical directions. Simultaneously, a Hierarchical Feature Synergy module (HFS) is designed to integrate low-, mid-, and high-level features, thereby enhancing semantic consistency and spatial detail preservation. Furthermore, we develop a novel loss function optimized for small object detection in aerial scenes, where small objects are caused by long shooting distances, effectively improving classification and localization accuracy. Extensive experiments on public datasets, including AI-TOD, VisDrone2019, and NWPU-VHR10, demonstrate that the proposed method significantly outperforms existing approaches in accuracy and efficiency. This work provides a new solution for practical aerial image analysis applications.
Published: 2025-12-02T16:07:21+00:00
Venue: Pattern Recognition
Score: 0.793 (consider)</description><content:encoded>&lt;p&gt;&lt;strong&gt;Authors:&lt;/strong&gt; Wenke Zhang; Mengmeng Liao&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Journal:&lt;/strong&gt; Pattern Recognition&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;a href="https://doi.org/10.1016/j.patcog.2025.112822"&gt;10.1016/j.patcog.2025.112822&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Relevance:&lt;/strong&gt; 0.793 (consider)&lt;/p&gt;
&lt;p&gt;Small object detection has long been a challenging task in computer vision due to limited pixel representation, extremely small scales, and complex scene variations. These challenges are particularly pronounced in high-resolution aerial imagery, where small objects refer to targets that appear as small-scale in images due to long shooting distances, characterized by limited pixel coverage and sparse feature representation. To address these issues, this paper proposes a novel object detection framework based on a Cross-Scale Adaptive Transformer and Hierarchical Feature Synergy. The framework introduces a Cross-Scale Adaptive Transformer module (CST) to dynamically capture multi-scale features in horizontal and vertical directions. Simultaneously, a Hierarchical Feature Synergy module (HFS) is designed to integrate low-, mid-, and high-level features, thereby enhancing semantic consistency and spatial detail preservation. Furthermore, we develop a novel loss function optimized for small object detection in aerial scenes, where small objects are caused by long shooting distances, effectively improving classification and localization accuracy. Extensive experiments on public datasets, including AI-TOD, VisDrone2019, and NWPU-VHR10, demonstrate that the proposed method significantly outperforms existing approaches in accuracy and efficiency. This work provides a new solution for practical aerial image analysis applications.&lt;/p&gt;</content:encoded></item><item><title>Dilated Transformation-Guided Unsupervised Multimodal Learning for Hyperspectral and Multispectral Image Fusion</title><link>https://doi.org/10.1109/tgrs.2025.3636047</link><guid>10.1109/tgrs.2025.3636047</guid><pubDate>Mon, 01 Dec 2025 18:24:33 +0000</pubDate><dc:creator>Yuanchao Su</dc:creator><dc:creator>Sheng Li</dc:creator><dc:creator>Yicong Zhou</dc:creator><dc:creator>Lianru Gao</dc:creator><dc:creator>Mengying Jiang</dc:creator><dc:creator>Xu Sun</dc:creator><dc:creator>Haiwei Li</dc:creator><dc:creator>Enke Hou</dc:creator><prism:publicationName>IEEE Transactions on Geoscience and Remote Sensing</prism:publicationName><prism:doi>10.1109/tgrs.2025.3636047</prism:doi><description>Multimodal fusion widely uses convolutional layers to capture local correlations and adjust feature dimensions. However, the progressive expansion of the receptive field in convolutional layers often compromises spatial context retention, leading to the loss of fine details. Furthermore, the fixed-size kernels typically used in standard convolution restrict the network’s ability to capture multiscale contextual details. To address this limitation, this paper develops a dilated transformation-guided unsupervised multimodal learning (DTUML) method to fuse a high-resolution multispectral image (HR-MSI) and a low-resolution hyperspectral image (LR-HSI), thereby generating a high-resolution hyperspectral image (HR-HSI). Our DTUML adopts a dual-stream encoder architecture to conduct multimodal data, where one stream focuses on preserving spectral information from LR-HSIs, while the other emphasizes the acquisition of spatial details from HR-MSIs. These complementary features are subsequently integrated to ensure spectral fidelity and retain spatial detail. Then, a convolutional layer restores dimensional consistency and outputs an HR-HSI. Extensive experiments demonstrate the effectiveness of DTUML, showing superior performance and strong competitiveness compared to state-of-the-art methods. Code: https://github.com/yuanchaosu/TGRS-DTUML.
Published: 2025-12-01T18:24:33+00:00
Venue: IEEE Transactions on Geoscience and Remote Sensing
Score: 0.792 (consider)</description><content:encoded>&lt;p&gt;&lt;strong&gt;Authors:&lt;/strong&gt; Yuanchao Su; Sheng Li; Yicong Zhou; Lianru Gao; Mengying Jiang; Xu Sun; Haiwei Li; Enke Hou&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Journal:&lt;/strong&gt; IEEE Transactions on Geoscience and Remote Sensing&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;a href="https://doi.org/10.1109/tgrs.2025.3636047"&gt;10.1109/tgrs.2025.3636047&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Relevance:&lt;/strong&gt; 0.792 (consider)&lt;/p&gt;
&lt;p&gt;Multimodal fusion widely uses convolutional layers to capture local correlations and adjust feature dimensions. However, the progressive expansion of the receptive field in convolutional layers often compromises spatial context retention, leading to the loss of fine details. Furthermore, the fixed-size kernels typically used in standard convolution restrict the network’s ability to capture multiscale contextual details. To address this limitation, this paper develops a dilated transformation-guided unsupervised multimodal learning (DTUML) method to fuse a high-resolution multispectral image (HR-MSI) and a low-resolution hyperspectral image (LR-HSI), thereby generating a high-resolution hyperspectral image (HR-HSI). Our DTUML adopts a dual-stream encoder architecture to conduct multimodal data, where one stream focuses on preserving spectral information from LR-HSIs, while the other emphasizes the acquisition of spatial details from HR-MSIs. These complementary features are subsequently integrated to ensure spectral fidelity and retain spatial detail. Then, a convolutional layer restores dimensional consistency and outputs an HR-HSI. Extensive experiments demonstrate the effectiveness of DTUML, showing superior performance and strong competitiveness compared to state-of-the-art methods. Code: https://github.com/yuanchaosu/TGRS-DTUML.&lt;/p&gt;</content:encoded></item><item><title>Illumination-aware Multimodal Hierarchical Fusion Network for RGB-Infrared Object Detection</title><link>https://doi.org/10.1109/tgrs.2025.3636590</link><guid>10.1109/tgrs.2025.3636590</guid><pubDate>Mon, 01 Dec 2025 18:24:33 +0000</pubDate><dc:creator>Ting Lu</dc:creator><dc:creator>Jiacheng Lu</dc:creator><dc:creator>Wei Fu</dc:creator><dc:creator>Yifan Xi</dc:creator><prism:publicationName>IEEE Transactions on Geoscience and Remote Sensing</prism:publicationName><prism:doi>10.1109/tgrs.2025.3636590</prism:doi><description>RGB-infrared (RGB-IR) object detection has attracted significant attention in drone-based applications due to its robustness under all-weather conditions. How to effectively fuse the complementary information in both modalities is one key for accurate object detection. However, the performance is limited by the inherent differences between modalities and the varying illumination conditions across different weather scenarios. Focused on this issue, we propose an illumination-aware multimodal hierarchical fusion network (IMHFNet) for RGB-IR object detection. First, an illumination aware module (IAM) is designed to extract local illumination features from RGB image, which is used to guide the subsequent multimodal feature fusion process. Then, considering the differences in semantic expression and detail representation of different feature layers of multimodal data, we separately design shallow and deep feature fusion strategies. In specific, the shallow feature fusion module is constructed based on convolutional operators and illumination-guided adaptive weight fusion, focusing on capturing and enhancing local detail information. For the deep feature fusion, illumination feature is incorporated as an auxiliary information, to guide the global semantic information integration across different modalities via adopting a transformer structure. In this work, we also construct a new drone-based RGB-IR dataset, named by DroneShip. It contains 4,306 images annotated with 17,054 oriented ship object instances, which covers a wide range of natural illumination conditions from daytime to nighttime. Finally, to validate the effectiveness of the proposed method, we evaluate the IMHFNet on the constructed DroneShip and two publicly available RGB-IR datasets (KAIST and DroneVehicle), which respectively focus on ship, pedestrian and vehicle targets. Experimental results on all three datasets consistently demonstrate the effectiveness and robustness of IMHFNet across diverse scenarios...
Published: 2025-12-01T18:24:33+00:00
Venue: IEEE Transactions on Geoscience and Remote Sensing
Score: 0.792 (consider)</description><content:encoded>&lt;p&gt;&lt;strong&gt;Authors:&lt;/strong&gt; Ting Lu; Jiacheng Lu; Wei Fu; Yifan Xi&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Journal:&lt;/strong&gt; IEEE Transactions on Geoscience and Remote Sensing&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;a href="https://doi.org/10.1109/tgrs.2025.3636590"&gt;10.1109/tgrs.2025.3636590&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Relevance:&lt;/strong&gt; 0.792 (consider)&lt;/p&gt;
&lt;p&gt;RGB-infrared (RGB-IR) object detection has attracted significant attention in drone-based applications due to its robustness under all-weather conditions. How to effectively fuse the complementary information in both modalities is one key for accurate object detection. However, the performance is limited by the inherent differences between modalities and the varying illumination conditions across different weather scenarios. Focused on this issue, we propose an illumination-aware multimodal hierarchical fusion network (IMHFNet) for RGB-IR object detection. First, an illumination aware module (IAM) is designed to extract local illumination features from RGB image, which is used to guide the subsequent multimodal feature fusion process. Then, considering the differences in semantic expression and detail representation of different feature layers of multimodal data, we separately design shallow and deep feature fusion strategies. In specific, the shallow feature fusion module is constructed based on convolutional operators and illumination-guided adaptive weight fusion, focusing on capturing and enhancing local detail information. For the deep feature fusion, illumination feature is incorporated as an auxiliary information, to guide the global semantic information integration across different modalities via adopting a transformer structure. In this work, we also construct a new drone-based RGB-IR dataset, named by DroneShip. It contains 4,306 images annotated with 17,054 oriented ship object instances, which covers a wide range of natural illumination conditions from daytime to nighttime. Finally, to validate the effectiveness of the proposed method, we evaluate the IMHFNet on the constructed DroneShip and two publicly available RGB-IR datasets (KAIST and DroneVehicle), which respectively focus on ship, pedestrian and vehicle targets. Experimental results on all three datasets consistently demonstrate the effectiveness and robustness of IMHFNet across diverse scenarios...&lt;/p&gt;</content:encoded></item><item><title>Empowering artificial intelligence with homomorphic encryption for secure deep reinforcement learning</title><link>https://doi.org/10.1038/s42256-025-01135-2</link><guid>10.1038/s42256-025-01135-2</guid><pubDate>Mon, 01 Dec 2025 10:02:38 +0000</pubDate><dc:creator>Chi-Hieu Nguyen</dc:creator><dc:creator>Thai Hoang Dinh</dc:creator><dc:creator>Diep N. Nguyen</dc:creator><dc:creator>Kristin Lauter</dc:creator><dc:creator>Miran Kim</dc:creator><prism:publicationName>Nature Machine Intelligence</prism:publicationName><prism:doi>10.1038/s42256-025-01135-2</prism:doi><description>Deep reinforcement learning (DRL) demonstrates significant potential in solving complex control and decision-making problems, but it may inadvertently expose sensitive, environment-specific information, raising privacy and security concerns for computer systems, humans and organizations. This work introduces a privacy-preserving framework using homomorphic encryption and advanced learning algorithms to secure DRL processes. Our framework enables the encryption of sensitive information, including states, actions and rewards, before sharing it with an untrusted processing platform. This encryption ensures data privacy, prevents unauthorized access and maintains compliance with data protection laws throughout the learning process. In addition, we develop innovative algorithms to efficiently handle a wide range of encrypted control tasks. Our core innovation is the homomorphic encryption-compatible Adam optimizer, which reparameterizes momentum values to bypass the need for high-degree polynomial approximations of inverse square roots on encrypted data. This adaptation, previously unexplored in homomorphic encryption-based ML research, enables stable and efficient training with adaptive learning rates in encrypted domains, addressing a critical bottleneck for privacy-preserving DRL with sparse rewards. Evaluations on standard DRL benchmarks demonstrate that our encrypted DRL performs comparably with its unencrypted counterpart (with a gap of less than 10%) and maintaining data confidentiality with homomorphic encryption. This work facilitates the integration of privacy-preserving DRL into real-world applications, addressing critical privacy concerns, and promoting the ethical advancement of artificial intelligence. A secure artificial intelligence framework is introduced that leverages homomorphic encryption to safeguard sensitive information in deep reinforcement learning, achieving accurate decision-making and ensuring data privacy and confidentiality.
Published: 2025-12-01T10:02:38+00:00
Venue: Nature Machine Intelligence
Score: 0.791 (consider)</description><content:encoded>&lt;p&gt;&lt;strong&gt;Authors:&lt;/strong&gt; Chi-Hieu Nguyen; Thai Hoang Dinh; Diep N. Nguyen; Kristin Lauter; Miran Kim&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Journal:&lt;/strong&gt; Nature Machine Intelligence&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;a href="https://doi.org/10.1038/s42256-025-01135-2"&gt;10.1038/s42256-025-01135-2&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Relevance:&lt;/strong&gt; 0.791 (consider)&lt;/p&gt;
&lt;p&gt;Deep reinforcement learning (DRL) demonstrates significant potential in solving complex control and decision-making problems, but it may inadvertently expose sensitive, environment-specific information, raising privacy and security concerns for computer systems, humans and organizations. This work introduces a privacy-preserving framework using homomorphic encryption and advanced learning algorithms to secure DRL processes. Our framework enables the encryption of sensitive information, including states, actions and rewards, before sharing it with an untrusted processing platform. This encryption ensures data privacy, prevents unauthorized access and maintains compliance with data protection laws throughout the learning process. In addition, we develop innovative algorithms to efficiently handle a wide range of encrypted control tasks. Our core innovation is the homomorphic encryption-compatible Adam optimizer, which reparameterizes momentum values to bypass the need for high-degree polynomial approximations of inverse square roots on encrypted data. This adaptation, previously unexplored in homomorphic encryption-based ML research, enables stable and efficient training with adaptive learning rates in encrypted domains, addressing a critical bottleneck for privacy-preserving DRL with sparse rewards. Evaluations on standard DRL benchmarks demonstrate that our encrypted DRL performs comparably with its unencrypted counterpart (with a gap of less than 10%) and maintaining data confidentiality with homomorphic encryption. This work facilitates the integration of privacy-preserving DRL into real-world applications, addressing critical privacy concerns, and promoting the ethical advancement of artificial intelligence. A secure artificial intelligence framework is introduced that leverages homomorphic encryption to safeguard sensitive information in deep reinforcement learning, achieving accurate decision-making and ensuring data privacy and confidentiality.&lt;/p&gt;</content:encoded></item><item><title>Geometry Gated Multi-view Stereo for 3D Reconstruction</title><link>https://doi.org/10.1016/j.neucom.2025.132264</link><guid>10.1016/j.neucom.2025.132264</guid><pubDate>Mon, 01 Dec 2025 16:08:04 +0000</pubDate><dc:creator>Han Li</dc:creator><dc:creator>Guohua Gou</dc:creator><dc:creator>Hao Zhang</dc:creator><dc:creator>Weicheng Jiang</dc:creator><dc:creator>Haigang Sui</dc:creator><prism:publicationName>Neurocomputing</prism:publicationName><prism:doi>10.1016/j.neucom.2025.132264</prism:doi><description>Multi-view stereo (MVS) aims to reconstruct accurate 3D scenes from multiple images. Currently, deep learning-based MVS methods typically estimate depth maps by regressing cost volumes. Therefore, the accuracy of geometric information encoded in the cost volume and the aggregation methods are crucial to the performance of MVS reconstruction. However, existing approaches lack sufficient optimization in cost volume construction and interaction. Moreover, conventional 3D convolutions often result in high computational complexity.
To address these challenges, this work proposes a Geometry-gated Multi-view Stereo Network (GGMVS), aiming to optimize feature representation in cost volume construction and the cost volume fusion mechanism, thereby improving both the accuracy and efficiency of MVS reconstruction. First, we design a Geometric Matching Enhancement Network (GME) to optimize the quality of cost volume construction. GME captures fine-grained features from multiple views and achieves dynamic feature propagation in a top-down manner. Second, we introduce a Cross-attention Volume Fusion Module (CVF) to strengthen inter-scale cost volume interactions. CVF leverages a cross-attention mechanism to globally integrate information from cost volumes at different scales, facilitating effective multi-scale geometric information fusion. Finally, we propose a Gated Volume Fusion Module (GVF) to enable refined filtering of cost volume information. GVF generates gating signals to dynamically filter and integrate high-confidence information from different cost volumes, providing precise inputs for the aggregation unit.
Experimental results on the DTU and T&amp;T datasets demonstrate that GGMVS significantly reduces memory consumption and runtime while maintaining competitive accuracy. Furthermore, validation on the ETH3D dataset further confirms the excellent generalization capability of GGMVS.
Published: 2025-12-01T16:08:04+00:00
Venue: Neurocomputing
Score: 0.790 (consider)</description><content:encoded>&lt;p&gt;&lt;strong&gt;Authors:&lt;/strong&gt; Han Li; Guohua Gou; Hao Zhang; Weicheng Jiang; Haigang Sui&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Journal:&lt;/strong&gt; Neurocomputing&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;a href="https://doi.org/10.1016/j.neucom.2025.132264"&gt;10.1016/j.neucom.2025.132264&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Relevance:&lt;/strong&gt; 0.790 (consider)&lt;/p&gt;
&lt;p&gt;Multi-view stereo (MVS) aims to reconstruct accurate 3D scenes from multiple images. Currently, deep learning-based MVS methods typically estimate depth maps by regressing cost volumes. Therefore, the accuracy of geometric information encoded in the cost volume and the aggregation methods are crucial to the performance of MVS reconstruction. However, existing approaches lack sufficient optimization in cost volume construction and interaction. Moreover, conventional 3D convolutions often result in high computational complexity.
To address these challenges, this work proposes a Geometry-gated Multi-view Stereo Network (GGMVS), aiming to optimize feature representation in cost volume construction and the cost volume fusion mechanism, thereby improving both the accuracy and efficiency of MVS reconstruction. First, we design a Geometric Matching Enhancement Network (GME) to optimize the quality of cost volume construction. GME captures fine-grained features from multiple views and achieves dynamic feature propagation in a top-down manner. Second, we introduce a Cross-attention Volume Fusion Module (CVF) to strengthen inter-scale cost volume interactions. CVF leverages a cross-attention mechanism to globally integrate information from cost volumes at different scales, facilitating effective multi-scale geometric information fusion. Finally, we propose a Gated Volume Fusion Module (GVF) to enable refined filtering of cost volume information. GVF generates gating signals to dynamically filter and integrate high-confidence information from different cost volumes, providing precise inputs for the aggregation unit.
Experimental results on the DTU and T&amp;amp;T datasets demonstrate that GGMVS significantly reduces memory consumption and runtime while maintaining competitive accuracy. Furthermore, validation on the ETH3D dataset further confirms the excellent generalization capability of GGMVS.&lt;/p&gt;</content:encoded></item><item><title>Structure and Sensitivity in 3D Human Pose Similarity Quantification and Estimation</title><link>https://doi.org/10.1016/j.patcog.2025.112805</link><guid>10.1016/j.patcog.2025.112805</guid><pubDate>Tue, 02 Dec 2025 16:45:04 +0000</pubDate><dc:creator>Kyoungoh Lee</dc:creator><dc:creator>Jungwoo Huh</dc:creator><dc:creator>Jiwoo Kang</dc:creator><dc:creator>Sanghoon Lee</dc:creator><prism:publicationName>Pattern Recognition</prism:publicationName><prism:doi>10.1016/j.patcog.2025.112805</prism:doi><description>Recent advancements in deep learning have improved quantitative accuracy in 3D human pose estimation, but the estimated poses occasionally suffer from visual defects such as joint tremors and protrusions. While existing 3D pose similarity metrics and estimation models managed to reduce visual defects by addressing the structure of human poses, they still struggle in scenarios where visually sensitive joints are prevalent, particularly in cases of self-occlusion. In this paper, we identify these visually sensitive joints and demonstrate the significance of explicitly considering structure and sensitivity in the problem of 3D human pose estimation. Building upon the successful consideration of human pose structure, we first propose a new enhanced pose similarity metric PSIM + " role="presentation"&gt; + + , which models sensitivity similarity to further capture human perception and focus on visual defects. Furthermore, we introduce a new 3D pose estimation model Dual Graph-based Convolutional Neural Networks (DG-CNN), which reconstructs 3D poses by focusing on the spatio-temporal correlation of the skeletal structure and actively controlling visually sensitive joints. By incorporating a novel similarity loss function, our model can implicitly model the structure and sensitivity of human poses through its architecture and explicitly through direct supervision. Our model not only improves the accuracy of the estimated pose but also increases the perceptual quality as evaluated by PSIM + " role="presentation"&gt; + + , verifying the significance of structure and sensitivity awareness. Through rigorous benchmarking, we demonstrate that our metric and estimation model achieve the highest correlation with user scores and perform best in situations where visually sensitive joints are prevalent.
Published: 2025-12-02T16:45:04+00:00
Venue: Pattern Recognition
Score: 0.789 (consider)</description><content:encoded>&lt;p&gt;&lt;strong&gt;Authors:&lt;/strong&gt; Kyoungoh Lee; Jungwoo Huh; Jiwoo Kang; Sanghoon Lee&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Journal:&lt;/strong&gt; Pattern Recognition&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;a href="https://doi.org/10.1016/j.patcog.2025.112805"&gt;10.1016/j.patcog.2025.112805&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Relevance:&lt;/strong&gt; 0.789 (consider)&lt;/p&gt;
&lt;p&gt;Recent advancements in deep learning have improved quantitative accuracy in 3D human pose estimation, but the estimated poses occasionally suffer from visual defects such as joint tremors and protrusions. While existing 3D pose similarity metrics and estimation models managed to reduce visual defects by addressing the structure of human poses, they still struggle in scenarios where visually sensitive joints are prevalent, particularly in cases of self-occlusion. In this paper, we identify these visually sensitive joints and demonstrate the significance of explicitly considering structure and sensitivity in the problem of 3D human pose estimation. Building upon the successful consideration of human pose structure, we first propose a new enhanced pose similarity metric PSIM + &amp;quot; role=&amp;quot;presentation&amp;quot;&amp;gt; + + , which models sensitivity similarity to further capture human perception and focus on visual defects. Furthermore, we introduce a new 3D pose estimation model Dual Graph-based Convolutional Neural Networks (DG-CNN), which reconstructs 3D poses by focusing on the spatio-temporal correlation of the skeletal structure and actively controlling visually sensitive joints. By incorporating a novel similarity loss function, our model can implicitly model the structure and sensitivity of human poses through its architecture and explicitly through direct supervision. Our model not only improves the accuracy of the estimated pose but also increases the perceptual quality as evaluated by PSIM + &amp;quot; role=&amp;quot;presentation&amp;quot;&amp;gt; + + , verifying the significance of structure and sensitivity awareness. Through rigorous benchmarking, we demonstrate that our metric and estimation model achieve the highest correlation with user scores and perform best in situations where visually sensitive joints are prevalent.&lt;/p&gt;</content:encoded></item><item><title>DiR-Net: A Diagnostic and Iterative Rectification Network for Cross-Modal 3D Object Detection</title><link>https://doi.org/10.1016/j.knosys.2025.115023</link><guid>10.1016/j.knosys.2025.115023</guid><pubDate>Tue, 02 Dec 2025 07:54:22 +0000</pubDate><dc:creator>Miaohui Zhang</dc:creator><dc:creator>Shuang Wang</dc:creator><dc:creator>Kunpeng Bi</dc:creator><dc:creator>Ming Xin</dc:creator><prism:publicationName>Knowledge-Based Systems</prism:publicationName><prism:doi>10.1016/j.knosys.2025.115023</prism:doi><description>Accurate perception of indoor scenes is a cornerstone of embodied intelligence. The core challenge in multi-modal 3D detection lies in achieving precise cross-modal feature alignment. However, existing methods typically rely on static projection, which prevents them from iteratively correcting alignment errors caused by occlusions or calibration inaccuracies, and they lack a mechanism to dynamically allocate computational resources based on fusion quality. To address these limitations, we propose a Diagnostic and Iterative Rectification Network (DiR-Net), which redefines the fusion process from static matching to a dynamic, confidence-based error correction procedure. Our core insight is that fusion quality is quantifiable: a Diagnostic Decision Module (DDM) analyzes the vector differences among initial 3D, 2D, and post-fusion features to compute an alignment confidence score that acts as an intelligent gate, adaptively balancing performance and efficiency. If optimization is deemed necessary, an Iterative Rectification Framework (IRF) module is activated to perform K rounds of refinement. In each iteration, unlike approaches that rely on implicit attention, our Rectification Regression Module (RRM) leverages the current fusion state and 3D geometric context to explicitly regress correction vectors for the 2D sampling coordinates, optimizing alignment with sub-pixel precision. Subsequently, an Internal Fusion Module (IFM) facilitates deep informational complementarity by generating cross-modal context to modulate the 2D and 3D feature streams. Experiments on the SUN RGB-D dataset demonstrate DiR-Net achieves a state-of-the-art performance of 70.68 mAP@0.25, establishing a new record on the benchmark. Our work pioneers a paradigm shift in multi-modal fusion, from one-shot fusion to adaptive, iterative error correction.
Published: 2025-12-02T07:54:22+00:00
Venue: Knowledge-Based Systems
Score: 0.787 (consider)</description><content:encoded>&lt;p&gt;&lt;strong&gt;Authors:&lt;/strong&gt; Miaohui Zhang; Shuang Wang; Kunpeng Bi; Ming Xin&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Journal:&lt;/strong&gt; Knowledge-Based Systems&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;a href="https://doi.org/10.1016/j.knosys.2025.115023"&gt;10.1016/j.knosys.2025.115023&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Relevance:&lt;/strong&gt; 0.787 (consider)&lt;/p&gt;
&lt;p&gt;Accurate perception of indoor scenes is a cornerstone of embodied intelligence. The core challenge in multi-modal 3D detection lies in achieving precise cross-modal feature alignment. However, existing methods typically rely on static projection, which prevents them from iteratively correcting alignment errors caused by occlusions or calibration inaccuracies, and they lack a mechanism to dynamically allocate computational resources based on fusion quality. To address these limitations, we propose a Diagnostic and Iterative Rectification Network (DiR-Net), which redefines the fusion process from static matching to a dynamic, confidence-based error correction procedure. Our core insight is that fusion quality is quantifiable: a Diagnostic Decision Module (DDM) analyzes the vector differences among initial 3D, 2D, and post-fusion features to compute an alignment confidence score that acts as an intelligent gate, adaptively balancing performance and efficiency. If optimization is deemed necessary, an Iterative Rectification Framework (IRF) module is activated to perform K rounds of refinement. In each iteration, unlike approaches that rely on implicit attention, our Rectification Regression Module (RRM) leverages the current fusion state and 3D geometric context to explicitly regress correction vectors for the 2D sampling coordinates, optimizing alignment with sub-pixel precision. Subsequently, an Internal Fusion Module (IFM) facilitates deep informational complementarity by generating cross-modal context to modulate the 2D and 3D feature streams. Experiments on the SUN RGB-D dataset demonstrate DiR-Net achieves a state-of-the-art performance of 70.68 mAP@0.25, establishing a new record on the benchmark. Our work pioneers a paradigm shift in multi-modal fusion, from one-shot fusion to adaptive, iterative error correction.&lt;/p&gt;</content:encoded></item><item><title>Heterogeneous Environment-aware Multimodal Recommendation with Modality Alignment</title><link>https://doi.org/10.1016/j.inffus.2025.103989</link><guid>10.1016/j.inffus.2025.103989</guid><pubDate>Mon, 01 Dec 2025 08:18:37 +0000</pubDate><dc:creator>Ke Shi</dc:creator><dc:creator>Yan Zhang</dc:creator><dc:creator>Miao Zhang</dc:creator><dc:creator>Kui Xiao</dc:creator><dc:creator>Dunhui Yu</dc:creator><dc:creator>Yahui Zhou</dc:creator><dc:creator>Wenxin Huang</dc:creator><dc:creator>Zhifei Li</dc:creator><prism:publicationName>Information Fusion</prism:publicationName><prism:doi>10.1016/j.inffus.2025.103989</prism:doi><description>Multimodal recommendation systems enhance accuracy by integrating information from different modalities. To address the issue of modality missing in real-world data, various efforts have attempted to restore missing data through data completion. Despite notable improvements, these methods still fail to fully resolve uncertainty from missing information. For different missing scenarios, distinct strategies are required for completion. Therefore, building an environment capable of handling any missing-modality case remains a challenge. To address this, we propose HEARec, a framework that simulates diverse missing-modality cases by generating heterogeneous environments. To construct missing scenarios applicable to various cases, we employ a tailored distribution combined with cyclic shifts to generate multiple environments with different weight groups. Moreover, to avoid directly merging multimodal features into item embeddings, we design independent processors to separately handle neighborhood information. For potential cross-modal inconsistencies, we map each modality embedding into a shared hypergraph space with MSE regularization. Finally, interaction-based modeling and aggregation strategies capture user interests from collaborative signals. Experiments demonstrate that HEARec consistently outperforms state-of-the-art models, achieving up to 4.53% and 6.02% improvements on the Baby and Sports datasets, respectively. Our code is available at https://github.com/HubuKG/HEARec .
Published: 2025-12-01T08:18:37+00:00
Venue: Information Fusion
Score: 0.783 (consider)</description><content:encoded>&lt;p&gt;&lt;strong&gt;Authors:&lt;/strong&gt; Ke Shi; Yan Zhang; Miao Zhang; Kui Xiao; Dunhui Yu; Yahui Zhou; Wenxin Huang; Zhifei Li&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Journal:&lt;/strong&gt; Information Fusion&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;a href="https://doi.org/10.1016/j.inffus.2025.103989"&gt;10.1016/j.inffus.2025.103989&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Relevance:&lt;/strong&gt; 0.783 (consider)&lt;/p&gt;
&lt;p&gt;Multimodal recommendation systems enhance accuracy by integrating information from different modalities. To address the issue of modality missing in real-world data, various efforts have attempted to restore missing data through data completion. Despite notable improvements, these methods still fail to fully resolve uncertainty from missing information. For different missing scenarios, distinct strategies are required for completion. Therefore, building an environment capable of handling any missing-modality case remains a challenge. To address this, we propose HEARec, a framework that simulates diverse missing-modality cases by generating heterogeneous environments. To construct missing scenarios applicable to various cases, we employ a tailored distribution combined with cyclic shifts to generate multiple environments with different weight groups. Moreover, to avoid directly merging multimodal features into item embeddings, we design independent processors to separately handle neighborhood information. For potential cross-modal inconsistencies, we map each modality embedding into a shared hypergraph space with MSE regularization. Finally, interaction-based modeling and aggregation strategies capture user interests from collaborative signals. Experiments demonstrate that HEARec consistently outperforms state-of-the-art models, achieving up to 4.53% and 6.02% improvements on the Baby and Sports datasets, respectively. Our code is available at https://github.com/HubuKG/HEARec .&lt;/p&gt;</content:encoded></item><item><title>Inverse++: Vision-centric 3D semantic occupancy prediction assisted with 3D object detection</title><link>https://doi.org/10.1016/j.neucom.2025.132162</link><guid>10.1016/j.neucom.2025.132162</guid><pubDate>Mon, 01 Dec 2025 16:43:18 +0000</pubDate><dc:creator>Zhenxing Ming</dc:creator><dc:creator>Julie Stephany Berrio-Perez</dc:creator><dc:creator>Mao Shan</dc:creator><dc:creator>Stewart Worrall</dc:creator><prism:publicationName>Neurocomputing</prism:publicationName><prism:doi>10.1016/j.neucom.2025.132162</prism:doi><description>3D semantic occupancy prediction aims to forecast detailed geometric and semantic information of the surrounding environment for autonomous vehicles (AVs) using onboard surround-view cameras. Existing methods primarily focus on intricate inner structure module designs to improve model performance, such as efficient feature sampling and aggregation processes or intermediate feature representation formats. In this paper, we explore multitask learning by introducing an additional 3D supervision signal by incorporating an additional 3D object detection auxiliary branch. This extra 3D supervision signal enhances the model’s overall performance by strengthening the capability of the intermediate features to capture small dynamic objects in the scene, and these small dynamic objects often include vulnerable road users, i.e., bicycles, motorcycles, and pedestrians, whose detection is crucial for ensuring driving safety in autonomous vehicles. Extensive experiments conducted on the nuScenes datasets, including challenging rainy and night scenarios, show that our approach delivers state-of-the-art results, achieving an IoU score of 31.73 % and a mIoU score of 20.91 % and excels at detecting vulnerable road users (VRU). The code will be available at: https://github.com/DanielMing123/Inverse++
Published: 2025-12-01T16:43:18+00:00
Venue: Neurocomputing
Score: 0.780 (consider)</description><content:encoded>&lt;p&gt;&lt;strong&gt;Authors:&lt;/strong&gt; Zhenxing Ming; Julie Stephany Berrio-Perez; Mao Shan; Stewart Worrall&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Journal:&lt;/strong&gt; Neurocomputing&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;a href="https://doi.org/10.1016/j.neucom.2025.132162"&gt;10.1016/j.neucom.2025.132162&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Relevance:&lt;/strong&gt; 0.780 (consider)&lt;/p&gt;
&lt;p&gt;3D semantic occupancy prediction aims to forecast detailed geometric and semantic information of the surrounding environment for autonomous vehicles (AVs) using onboard surround-view cameras. Existing methods primarily focus on intricate inner structure module designs to improve model performance, such as efficient feature sampling and aggregation processes or intermediate feature representation formats. In this paper, we explore multitask learning by introducing an additional 3D supervision signal by incorporating an additional 3D object detection auxiliary branch. This extra 3D supervision signal enhances the model’s overall performance by strengthening the capability of the intermediate features to capture small dynamic objects in the scene, and these small dynamic objects often include vulnerable road users, i.e., bicycles, motorcycles, and pedestrians, whose detection is crucial for ensuring driving safety in autonomous vehicles. Extensive experiments conducted on the nuScenes datasets, including challenging rainy and night scenarios, show that our approach delivers state-of-the-art results, achieving an IoU score of 31.73 % and a mIoU score of 20.91 % and excels at detecting vulnerable road users (VRU). The code will be available at: https://github.com/DanielMing123/Inverse++&lt;/p&gt;</content:encoded></item><item><title>CL-WTAL:Weakly-supervised temporal complex action localization based on multi-scale contrast learning</title><link>https://doi.org/10.1109/tcsvt.2025.3639310</link><guid>10.1109/tcsvt.2025.3639310</guid><pubDate>Tue, 02 Dec 2025 18:50:15 +0000</pubDate><dc:creator>Weili Ding</dc:creator><dc:creator>Yu Zhang</dc:creator><dc:creator>Lingyun Yang</dc:creator><dc:creator>Shuo Hu</dc:creator><prism:publicationName>IEEE Transactions on Circuits and Systems for Video Technology</prism:publicationName><prism:doi>10.1109/tcsvt.2025.3639310</prism:doi><description>Temporal action localization in long-term untrimmed videos remains a critical yet challenging task in video understanding, with existing methods often relying on anchor-based or fully-supervised frameworks that incur heavy computation and require labor-intensive frame-level annotations. This paper presents a novel weakly-supervised approach, CL-WTAL, which leverages multi-scale contrast learning and graph convolution for accurate action localization and recognition. The method comprises three key components: (i) A multi-scale sliding window mechanism (long/normal/short sequences) to segment sub-actions from complex videos, adapting to diverse action durations; (ii) A spatio-temporal graph convolution network (ST-RGCN) to extract skeletal feature vectors, integrating human motion dynamics and environmental context; (iii) A contrastive learning-based similarity evaluation framework that combines cosine similarity and Dynamic Time Warping (DTW) distance to measure feature vector relationships, enabling precise action boundary detection without extensive fine-tuning. Experiments on daily-life video datasets demonstrate that CL-WTAL effectively localizes action intervals and classifies actions with high accuracy, outperforming state-of-the-art weakly-supervised methods.
Published: 2025-12-02T18:50:15+00:00
Venue: IEEE Transactions on Circuits and Systems for Video Technology
Score: 0.779 (consider)</description><content:encoded>&lt;p&gt;&lt;strong&gt;Authors:&lt;/strong&gt; Weili Ding; Yu Zhang; Lingyun Yang; Shuo Hu&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Journal:&lt;/strong&gt; IEEE Transactions on Circuits and Systems for Video Technology&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;a href="https://doi.org/10.1109/tcsvt.2025.3639310"&gt;10.1109/tcsvt.2025.3639310&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Relevance:&lt;/strong&gt; 0.779 (consider)&lt;/p&gt;
&lt;p&gt;Temporal action localization in long-term untrimmed videos remains a critical yet challenging task in video understanding, with existing methods often relying on anchor-based or fully-supervised frameworks that incur heavy computation and require labor-intensive frame-level annotations. This paper presents a novel weakly-supervised approach, CL-WTAL, which leverages multi-scale contrast learning and graph convolution for accurate action localization and recognition. The method comprises three key components: (i) A multi-scale sliding window mechanism (long/normal/short sequences) to segment sub-actions from complex videos, adapting to diverse action durations; (ii) A spatio-temporal graph convolution network (ST-RGCN) to extract skeletal feature vectors, integrating human motion dynamics and environmental context; (iii) A contrastive learning-based similarity evaluation framework that combines cosine similarity and Dynamic Time Warping (DTW) distance to measure feature vector relationships, enabling precise action boundary detection without extensive fine-tuning. Experiments on daily-life video datasets demonstrate that CL-WTAL effectively localizes action intervals and classifies actions with high accuracy, outperforming state-of-the-art weakly-supervised methods.&lt;/p&gt;</content:encoded></item><item><title>SpatialFuse3D: Relational-aware multimodal framework for comprehensive 3D scene understanding</title><link>https://doi.org/10.1016/j.neucom.2025.132267</link><guid>10.1016/j.neucom.2025.132267</guid><pubDate>Tue, 02 Dec 2025 00:23:16 +0000</pubDate><dc:creator>Lei Fu</dc:creator><dc:creator>Nannan Li</dc:creator><dc:creator>Huanqiang Hu</dc:creator><dc:creator>Zhaowen Chen</dc:creator><dc:creator>Yiqing Cao</dc:creator><prism:publicationName>Neurocomputing</prism:publicationName><prism:doi>10.1016/j.neucom.2025.132267</prism:doi><description>Despite recent progress in Multimodal Large Language Models (MLLMs) for 3D scene understanding, fundamental challenges persist in integrating multiple modalities and accurately representing spatial relationships in complex 3D environments. We present SpatialFuse3D, a novel framework that combines hierarchical multimodal fusion with explicit spatial reasoning capabilities. At its core, SpatialFuse3D features the 3D Scene and Text Network (3DST-Net), a transformer-based architecture that effectively aligns and integrates local, global, and textual features through cross-modal attention mechanisms, facilitating detailed object recognition while maintaining comprehensive scene understanding. Additionally, SpatialFuse3D implements a Relational-Aware Enhancement (RAE) module that processes object features, positional information, and geometric relationships to systematically model spatial configurations, precisely determining object arrangements in complex scenes. Extensive evaluation on ScanRefer, Multi3DRefer, Scan2Cap, ScanQA, and SQA3D benchmarks demonstrates that SpatialFuse3D achieves superior performance compared to existing methods in both multimodal integration and spatial reasoning tasks within complex 3D environments, effectively leveraging pre-trained language models for advanced multimodal learning.
Published: 2025-12-02T00:23:16+00:00
Venue: Neurocomputing
Score: 0.779 (consider)</description><content:encoded>&lt;p&gt;&lt;strong&gt;Authors:&lt;/strong&gt; Lei Fu; Nannan Li; Huanqiang Hu; Zhaowen Chen; Yiqing Cao&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Journal:&lt;/strong&gt; Neurocomputing&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;a href="https://doi.org/10.1016/j.neucom.2025.132267"&gt;10.1016/j.neucom.2025.132267&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Relevance:&lt;/strong&gt; 0.779 (consider)&lt;/p&gt;
&lt;p&gt;Despite recent progress in Multimodal Large Language Models (MLLMs) for 3D scene understanding, fundamental challenges persist in integrating multiple modalities and accurately representing spatial relationships in complex 3D environments. We present SpatialFuse3D, a novel framework that combines hierarchical multimodal fusion with explicit spatial reasoning capabilities. At its core, SpatialFuse3D features the 3D Scene and Text Network (3DST-Net), a transformer-based architecture that effectively aligns and integrates local, global, and textual features through cross-modal attention mechanisms, facilitating detailed object recognition while maintaining comprehensive scene understanding. Additionally, SpatialFuse3D implements a Relational-Aware Enhancement (RAE) module that processes object features, positional information, and geometric relationships to systematically model spatial configurations, precisely determining object arrangements in complex scenes. Extensive evaluation on ScanRefer, Multi3DRefer, Scan2Cap, ScanQA, and SQA3D benchmarks demonstrates that SpatialFuse3D achieves superior performance compared to existing methods in both multimodal integration and spatial reasoning tasks within complex 3D environments, effectively leveraging pre-trained language models for advanced multimodal learning.&lt;/p&gt;</content:encoded></item><item><title>2Player: A general framework for self-supervised change detection via cooperative learning</title><link>https://doi.org/10.1016/j.isprsjprs.2025.11.024</link><guid>10.1016/j.isprsjprs.2025.11.024</guid><pubDate>Tue, 02 Dec 2025 17:21:36 +0000</pubDate><dc:creator>Manon Béchaz</dc:creator><dc:creator>Emanuele Dalsasso</dc:creator><dc:creator>Ciprian Tomoiagă</dc:creator><dc:creator>Marcin Detyniecki</dc:creator><dc:creator>Devis Tuia</dc:creator><prism:publicationName>ISPRS Journal of Photogrammetry and Remote Sensing</prism:publicationName><prism:doi>10.1016/j.isprsjprs.2025.11.024</prism:doi><description>While recent progress in deep learning have improved change detection (CD) in remote sensing, achieving high performance without labeled data remains an open challenge. Current models remain strongly reliant on large-scale annotated datasets, limiting their scalability to new regions and applications. Unsupervised CD methods offer a promising alternative but often suffer from limited representational ability and vulnerability to irrelevant changes in appearance such as seasonal variations. In this work, we propose the 2Player framework, a novel self-supervised method that transforms any existing CD architecture into an unsupervised model by leveraging a cooperation between a change detector and a reconstruction-based model. The two models, or players, guide each other during training: reconstruction errors provide supervision to the change detector, while change predictions guide the reconstruction process. To further improve robustness, we introduce a Geographical Correspondence Module that provides high-frequency structural information, effectively reducing false positives stemming from irrelevant changes in appearance. Furthermore, we propose a simple filtering strategy to mitigate the impact of label noise in CD datasets, contributing in an improved evaluation. We test 2Player on four very high-resolution datasets: HRSCD, for which we improve and release a new, cleaner set of labels, LEVIR-CD, WHU-CD, as well as a new dataset, ValaisCD. Our approach achieves state-of-the-art performance among unsupervised methods on these datasets, and with its architecture-agnostic design, provides a promising direction for bridging the gap between supervised and unsupervised change detection.
Published: 2025-12-02T17:21:36+00:00
Venue: ISPRS Journal of Photogrammetry and Remote Sensing
Score: 0.778 (consider)</description><content:encoded>&lt;p&gt;&lt;strong&gt;Authors:&lt;/strong&gt; Manon Béchaz; Emanuele Dalsasso; Ciprian Tomoiagă; Marcin Detyniecki; Devis Tuia&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Journal:&lt;/strong&gt; ISPRS Journal of Photogrammetry and Remote Sensing&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;a href="https://doi.org/10.1016/j.isprsjprs.2025.11.024"&gt;10.1016/j.isprsjprs.2025.11.024&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Relevance:&lt;/strong&gt; 0.778 (consider)&lt;/p&gt;
&lt;p&gt;While recent progress in deep learning have improved change detection (CD) in remote sensing, achieving high performance without labeled data remains an open challenge. Current models remain strongly reliant on large-scale annotated datasets, limiting their scalability to new regions and applications. Unsupervised CD methods offer a promising alternative but often suffer from limited representational ability and vulnerability to irrelevant changes in appearance such as seasonal variations. In this work, we propose the 2Player framework, a novel self-supervised method that transforms any existing CD architecture into an unsupervised model by leveraging a cooperation between a change detector and a reconstruction-based model. The two models, or players, guide each other during training: reconstruction errors provide supervision to the change detector, while change predictions guide the reconstruction process. To further improve robustness, we introduce a Geographical Correspondence Module that provides high-frequency structural information, effectively reducing false positives stemming from irrelevant changes in appearance. Furthermore, we propose a simple filtering strategy to mitigate the impact of label noise in CD datasets, contributing in an improved evaluation. We test 2Player on four very high-resolution datasets: HRSCD, for which we improve and release a new, cleaner set of labels, LEVIR-CD, WHU-CD, as well as a new dataset, ValaisCD. Our approach achieves state-of-the-art performance among unsupervised methods on these datasets, and with its architecture-agnostic design, provides a promising direction for bridging the gap between supervised and unsupervised change detection.&lt;/p&gt;</content:encoded></item><item><title>Applying ViT Masked Autoencoders to Seismic Data for Feature Extraction and Few-Shot Learning</title><link>https://doi.org/10.1109/lgrs.2025.3639172</link><guid>10.1109/lgrs.2025.3639172</guid><pubDate>Mon, 01 Dec 2025 18:26:26 +0000</pubDate><dc:creator>Fernando G. Marques</dc:creator><dc:creator>Carlos A. Astudillo</dc:creator><dc:creator>Alan Souza</dc:creator><dc:creator>Daniel Miranda</dc:creator><dc:creator>Edson Borin</dc:creator><prism:publicationName>IEEE Geoscience and Remote Sensing Letters</prism:publicationName><prism:doi>10.1109/lgrs.2025.3639172</prism:doi><description>We apply the self-supervised learning technique of vision transformers masked autoencoder (ViT MAE) models with the goal of to producing a feature extractor vision transformer (ViT) backbone for neural networks that receive seismic data as input. We then evaluate the quality of these backbones by coupling them to a simple linear prediction head and fine-tuning these models in a seismic semantic segmentation task. We compare domain-specific ViT MAE against cross-domain pretrained and randomly initialized ViTs, and show that it yields superior performance in low-data regimes. Furthermore, we also demonstrate that pretraining loss correlates with downstream performance, supporting its use as a proxy for feature quality.
Published: 2025-12-01T18:26:26+00:00
Venue: IEEE Geoscience and Remote Sensing Letters
Score: 0.778 (consider)</description><content:encoded>&lt;p&gt;&lt;strong&gt;Authors:&lt;/strong&gt; Fernando G. Marques; Carlos A. Astudillo; Alan Souza; Daniel Miranda; Edson Borin&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Journal:&lt;/strong&gt; IEEE Geoscience and Remote Sensing Letters&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;a href="https://doi.org/10.1109/lgrs.2025.3639172"&gt;10.1109/lgrs.2025.3639172&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Relevance:&lt;/strong&gt; 0.778 (consider)&lt;/p&gt;
&lt;p&gt;We apply the self-supervised learning technique of vision transformers masked autoencoder (ViT MAE) models with the goal of to producing a feature extractor vision transformer (ViT) backbone for neural networks that receive seismic data as input. We then evaluate the quality of these backbones by coupling them to a simple linear prediction head and fine-tuning these models in a seismic semantic segmentation task. We compare domain-specific ViT MAE against cross-domain pretrained and randomly initialized ViTs, and show that it yields superior performance in low-data regimes. Furthermore, we also demonstrate that pretraining loss correlates with downstream performance, supporting its use as a proxy for feature quality.&lt;/p&gt;</content:encoded></item><item><title>A comparative study of deep learning methods for super-resolution of NPP-VIIRS nighttime light images</title><link>https://doi.org/10.1016/j.jag.2025.104995</link><guid>10.1016/j.jag.2025.104995</guid><pubDate>Mon, 01 Dec 2025 19:50:03 +0000</pubDate><dc:creator>Chaolong Zhang</dc:creator><dc:creator>Zhihui Mao</dc:creator><dc:creator>Juan Nie</dc:creator><dc:creator>Yushi Lai</dc:creator><dc:creator>Lei Deng</dc:creator><prism:publicationName>International Journal of Applied Earth Observation and Geoinformation</prism:publicationName><prism:doi>10.1016/j.jag.2025.104995</prism:doi><description>Nighttime light imagery plays a crucial role in diverse applications such as urban planning, environmental monitoring, and economic analysis. Although NPP-VIIRS provides a long and continuous Nighttime light time series, its relatively low spatial resolution limits detailed spatial analysis. Achieving Nighttime light data with both high spatial and temporal resolution remains a key challenge. This study investigates the effectiveness of several deep learning–based super-resolution (SR) models for enhancing NPP-VIIRS Nighttime light imagery using Luojia1-01 data as high-resolution reference imagery. Five representative models—ESPCN, RDN, SRFBN, SwinIR, and RealESRGAN—were selected to cover a range of network architectures including CNN, RNN, ResNet, DenseNet, Transformer, and GAN. A paired SR dataset was constructed from Luojia1-01 and NPP-VIIRS images, and the selected models were trained and evaluated on this dataset. Model performance was assessed across different urban scales and lighting conditions (e.g., dense urban cores, road networks) using PSNR, SSIM, FSIM, and the 95th percentile metrics. Results indicate that model performance varies substantially across scene types, with RealESRGAN showing superior detail recovery and overall image quality (PSNR = 31.96, SSIM = 0.85, FSIM = 0.85). The 95th percentile distribution of the RealESRGAN-enhanced images closely matches that of high-resolution reference data. These findings demonstrate that deep learning–based SR methods can substantially improve the spatial resolution and visual quality of NPP-VIIRS Nighttime light imagery, enabling finer-scale analysis of urban structures and temporal dynamics. This work provides an effective technical framework for reconstructing historical high-resolution Nighttime light data and expanding their applicability in urban, environmental, and socioeconomic studies.
Published: 2025-12-01T19:50:03+00:00
Venue: International Journal of Applied Earth Observation and Geoinformation
Score: 0.777 (consider)</description><content:encoded>&lt;p&gt;&lt;strong&gt;Authors:&lt;/strong&gt; Chaolong Zhang; Zhihui Mao; Juan Nie; Yushi Lai; Lei Deng&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Journal:&lt;/strong&gt; International Journal of Applied Earth Observation and Geoinformation&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;a href="https://doi.org/10.1016/j.jag.2025.104995"&gt;10.1016/j.jag.2025.104995&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Relevance:&lt;/strong&gt; 0.777 (consider)&lt;/p&gt;
&lt;p&gt;Nighttime light imagery plays a crucial role in diverse applications such as urban planning, environmental monitoring, and economic analysis. Although NPP-VIIRS provides a long and continuous Nighttime light time series, its relatively low spatial resolution limits detailed spatial analysis. Achieving Nighttime light data with both high spatial and temporal resolution remains a key challenge. This study investigates the effectiveness of several deep learning–based super-resolution (SR) models for enhancing NPP-VIIRS Nighttime light imagery using Luojia1-01 data as high-resolution reference imagery. Five representative models—ESPCN, RDN, SRFBN, SwinIR, and RealESRGAN—were selected to cover a range of network architectures including CNN, RNN, ResNet, DenseNet, Transformer, and GAN. A paired SR dataset was constructed from Luojia1-01 and NPP-VIIRS images, and the selected models were trained and evaluated on this dataset. Model performance was assessed across different urban scales and lighting conditions (e.g., dense urban cores, road networks) using PSNR, SSIM, FSIM, and the 95th percentile metrics. Results indicate that model performance varies substantially across scene types, with RealESRGAN showing superior detail recovery and overall image quality (PSNR = 31.96, SSIM = 0.85, FSIM = 0.85). The 95th percentile distribution of the RealESRGAN-enhanced images closely matches that of high-resolution reference data. These findings demonstrate that deep learning–based SR methods can substantially improve the spatial resolution and visual quality of NPP-VIIRS Nighttime light imagery, enabling finer-scale analysis of urban structures and temporal dynamics. This work provides an effective technical framework for reconstructing historical high-resolution Nighttime light data and expanding their applicability in urban, environmental, and socioeconomic studies.&lt;/p&gt;</content:encoded></item><item><title>Self-Attention-Enhanced Dual-Branch Network for Cloud Detection in Panchromatic Satellite Imagery</title><link>https://doi.org/10.1109/jstars.2025.3639193</link><guid>10.1109/jstars.2025.3639193</guid><pubDate>Mon, 01 Dec 2025 18:24:44 +0000</pubDate><dc:creator>Kinga Karwowska</dc:creator><dc:creator>Jolanta Siewert</dc:creator><dc:creator>Aleksandra Sekrecka</dc:creator><prism:publicationName>IEEE Journal of Selected Topics in Applied Earth Observations and Remote Sensing</prism:publicationName><prism:doi>10.1109/jstars.2025.3639193</prism:doi><description>Cloud cover remains a major challenge for the analysis of optical satellite imagery, particularly in panchromatic data, which lack the spectral information required to distinguish clouds from bright land surfaces. Most existing cloud-masking algorithms were developed for multispectral sensors and show limited performance on single-band images. We propose an attention-guided dual-branch deep network for cloud detection in panchromatic satellite imagery. The architecture combines a dual-branch generator, capturing both local texture and global context, with a self-attention mechanism that improves the recognition of thin and irregular cloud structures. The integration of Wasserstein and VGG-based perceptual losses stabilises training and produces sharper cloud masks. Furthermore, XAI techniques (Grad-CAM) were applied to interpret the model's decision-making process. Evaluated on panchromatic tiles derived from Sentinel-2 data, the proposed method outperformed classical ML classifiers as well as CNN-based segmenters (UNet, DeepLabV3+) and the ViT-based SAM 2 model, achieving Accuracy = 0.924, Recall = 0.911 and IoU = 0.702. The results demonstrate its high accuracy and operational applicability for cloud masking in high-resolution nanosatellite imagery. The code is available at: https://github.com/KK-MUT/cGANWM_SA.
Published: 2025-12-01T18:24:44+00:00
Venue: IEEE Journal of Selected Topics in Applied Earth Observations and Remote Sensing
Score: 0.777 (consider)</description><content:encoded>&lt;p&gt;&lt;strong&gt;Authors:&lt;/strong&gt; Kinga Karwowska; Jolanta Siewert; Aleksandra Sekrecka&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Journal:&lt;/strong&gt; IEEE Journal of Selected Topics in Applied Earth Observations and Remote Sensing&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;a href="https://doi.org/10.1109/jstars.2025.3639193"&gt;10.1109/jstars.2025.3639193&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Relevance:&lt;/strong&gt; 0.777 (consider)&lt;/p&gt;
&lt;p&gt;Cloud cover remains a major challenge for the analysis of optical satellite imagery, particularly in panchromatic data, which lack the spectral information required to distinguish clouds from bright land surfaces. Most existing cloud-masking algorithms were developed for multispectral sensors and show limited performance on single-band images. We propose an attention-guided dual-branch deep network for cloud detection in panchromatic satellite imagery. The architecture combines a dual-branch generator, capturing both local texture and global context, with a self-attention mechanism that improves the recognition of thin and irregular cloud structures. The integration of Wasserstein and VGG-based perceptual losses stabilises training and produces sharper cloud masks. Furthermore, XAI techniques (Grad-CAM) were applied to interpret the model&amp;#x27;s decision-making process. Evaluated on panchromatic tiles derived from Sentinel-2 data, the proposed method outperformed classical ML classifiers as well as CNN-based segmenters (UNet, DeepLabV3+) and the ViT-based SAM 2 model, achieving Accuracy = 0.924, Recall = 0.911 and IoU = 0.702. The results demonstrate its high accuracy and operational applicability for cloud masking in high-resolution nanosatellite imagery. The code is available at: https://github.com/KK-MUT/cGANWM_SA.&lt;/p&gt;</content:encoded></item><item><title>Efficient Small Object Detection Based on Multi-level Implicit Feature Enhancement and Presence Region Mask Guidance</title><link>https://doi.org/10.1016/j.knosys.2025.115020</link><guid>10.1016/j.knosys.2025.115020</guid><pubDate>Tue, 02 Dec 2025 07:54:16 +0000</pubDate><dc:creator>Runshi Wang</dc:creator><dc:creator>Jinfu Yang</dc:creator><dc:creator>Mingai Li</dc:creator><dc:creator>Dechen Hao</dc:creator><prism:publicationName>Knowledge-Based Systems</prism:publicationName><prism:doi>10.1016/j.knosys.2025.115020</prism:doi><description>Small object detection has wide application value in aerospace, urban planning, automatic driving and other fields. Limited by the problems of small object size and little feature information, existing detection methods are difficult to achieve excellent performance. In this paper, an efficient small object detection method based on multi-level implicit feature enhancement and presence region mask guidance is proposed. First, a multi-level implicit relation learning module is designed to accomplish image feature enhancement by mining the implicit mapping between low-resolution and high-resolution images. Secondly, to address the problem of increased computational burden caused by feature enhancement, a presence region mask guidance strategy(PRMG) that balances accuracy and efficiency is proposed. The presence regions of small objects are first quickly localized, and then sparse convolution is utilized to accomplish efficient object detection in a query-based approach. Extensive experimental results on Visdrone and AI-TOD datasets prove that the proposed method can effectively improve the detection accuracy and speed up the inference process.
Published: 2025-12-02T07:54:16+00:00
Venue: Knowledge-Based Systems
Score: 0.777 (consider)</description><content:encoded>&lt;p&gt;&lt;strong&gt;Authors:&lt;/strong&gt; Runshi Wang; Jinfu Yang; Mingai Li; Dechen Hao&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Journal:&lt;/strong&gt; Knowledge-Based Systems&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;a href="https://doi.org/10.1016/j.knosys.2025.115020"&gt;10.1016/j.knosys.2025.115020&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Relevance:&lt;/strong&gt; 0.777 (consider)&lt;/p&gt;
&lt;p&gt;Small object detection has wide application value in aerospace, urban planning, automatic driving and other fields. Limited by the problems of small object size and little feature information, existing detection methods are difficult to achieve excellent performance. In this paper, an efficient small object detection method based on multi-level implicit feature enhancement and presence region mask guidance is proposed. First, a multi-level implicit relation learning module is designed to accomplish image feature enhancement by mining the implicit mapping between low-resolution and high-resolution images. Secondly, to address the problem of increased computational burden caused by feature enhancement, a presence region mask guidance strategy(PRMG) that balances accuracy and efficiency is proposed. The presence regions of small objects are first quickly localized, and then sparse convolution is utilized to accomplish efficient object detection in a query-based approach. Extensive experimental results on Visdrone and AI-TOD datasets prove that the proposed method can effectively improve the detection accuracy and speed up the inference process.&lt;/p&gt;</content:encoded></item><item><title>UAV vision-based object detection network with lightweight and multi-scale fusion</title><link>https://doi.org/10.1016/j.neucom.2025.132211</link><guid>10.1016/j.neucom.2025.132211</guid><pubDate>Mon, 01 Dec 2025 16:08:23 +0000</pubDate><dc:creator>Xinxin Luo</dc:creator><dc:creator>Fuzeng Zhang</dc:creator><dc:creator>Eksan Firkat</dc:creator><dc:creator>Askar Hamdulla</dc:creator><dc:creator>Bin Zhu</dc:creator><dc:creator>Abdusalam Dawut</dc:creator><prism:publicationName>Neurocomputing</prism:publicationName><prism:doi>10.1016/j.neucom.2025.132211</prism:doi><description>Object detection technology has significant potential across various domains but faces challenges in identifying small targets, severely blurred and occluded objects, and meeting real-time processing requirements. This paper presents a real-time object detection system tailored for UAV imagery. The system employs a lightweight feature extraction module, DSGConv, to build the backbone network, balancing computation, parameters, and accuracy. It improves the recognition of occluded objects by extracting contextual information and linking feature maps across stages, enhancing information flow and fusion. Experimental results on the VisDrone2019 UAV imagery dataset show that the proposed algorithm achieves a mAP50 of 41.5 % with a precision of 52.1 %, demonstrating advancements in both accuracy and speed. The system’s efficacy and progress in object detection for UAV imagery are affirmed. The source code is available at https://github.com/luoxinxinfdi/FSAL .
Published: 2025-12-01T16:08:23+00:00
Venue: Neurocomputing
Score: 0.776 (consider)</description><content:encoded>&lt;p&gt;&lt;strong&gt;Authors:&lt;/strong&gt; Xinxin Luo; Fuzeng Zhang; Eksan Firkat; Askar Hamdulla; Bin Zhu; Abdusalam Dawut&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Journal:&lt;/strong&gt; Neurocomputing&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;a href="https://doi.org/10.1016/j.neucom.2025.132211"&gt;10.1016/j.neucom.2025.132211&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Relevance:&lt;/strong&gt; 0.776 (consider)&lt;/p&gt;
&lt;p&gt;Object detection technology has significant potential across various domains but faces challenges in identifying small targets, severely blurred and occluded objects, and meeting real-time processing requirements. This paper presents a real-time object detection system tailored for UAV imagery. The system employs a lightweight feature extraction module, DSGConv, to build the backbone network, balancing computation, parameters, and accuracy. It improves the recognition of occluded objects by extracting contextual information and linking feature maps across stages, enhancing information flow and fusion. Experimental results on the VisDrone2019 UAV imagery dataset show that the proposed algorithm achieves a mAP50 of 41.5 % with a precision of 52.1 %, demonstrating advancements in both accuracy and speed. The system’s efficacy and progress in object detection for UAV imagery are affirmed. The source code is available at https://github.com/luoxinxinfdi/FSAL .&lt;/p&gt;</content:encoded></item></channel></rss>