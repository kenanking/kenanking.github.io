<?xml version='1.0' encoding='utf-8'?>
<rss xmlns:content="http://purl.org/rss/1.0/modules/content/" xmlns:dc="http://purl.org/dc/elements/1.1/" xmlns:prism="http://prismstandard.org/namespaces/basic/3.0/" version="2.0"><channel><title>ZotWatch Feed</title><link>https://ehehe.cn/zotwatch/</link><description>AI-assisted literature watch</description><lastBuildDate>Thu, 22 Jan 2026 02:58:29 +0000</lastBuildDate><item><title>Momentor++: Advancing Video Large Language Models With Fine-Grained Long Video Reasoning</title><link>https://doi.org/10.1109/tpami.2026.3656169</link><guid>10.1109/tpami.2026.3656169</guid><pubDate>Tue, 20 Jan 2026 20:40:09 +0000</pubDate><dc:creator>Juncheng Li</dc:creator><dc:creator>Minghe Gao</dc:creator><dc:creator>Xiangnan He</dc:creator><dc:creator>Siliang Tang</dc:creator><dc:creator>Weishi Zheng</dc:creator><dc:creator>Jun Xiao</dc:creator><dc:creator>Meng Wang</dc:creator><dc:creator>Tat-Seng Chua</dc:creator><dc:creator>Yueting Zhuang</dc:creator><prism:publicationName>IEEE Transactions on Pattern Analysis and Machine Intelligence</prism:publicationName><prism:doi>10.1109/tpami.2026.3656169</prism:doi><description>Large Language Models (LLMs) exhibit remarkable proficiency in understanding and managing text-based tasks. Many works try to transfer these capabilities to the video domain, which are referred to as Video-LLMs. However, current Video-LLMs can only grasp the coarse-grained semantics and are unable to efficiently handle tasks involving the comprehension or localization of specific video segments. To address these challenges, we propose Momentor, a Video-LLM designed to perform fine-grained temporal understanding tasks. To facilitate the training of Momentor, we develop an automatic data generation engine to build Moment-10 M, a large-scale video instruction dataset with segment-level instruction data. Building upon the foundation of the previously published Momentor and the Moment-10 M dataset, we further extend this work by introducing a Spatio-Temporal Token Consolidation (STTC) method, which can merge redundant visual tokens spatio-temporally in a parameter-free manner, thereby significantly promoting computational efficiency while preserving fine-grained visual details. We integrate STTC with Momentor to develop Momentor++ and validate its performance on various benchmarks. Momentor demonstrates robust capabilities in fine-grained temporal understanding and localization. Further, Momentor++ excels in efficiently processing and analyzing extended videos with complex events, showcasing marked advancements in handling extensive temporal contexts.
Published: 2026-01-20T20:40:09+00:00
Venue: IEEE Transactions on Pattern Analysis and Machine Intelligence
Score: 0.838 (must_read)</description><content:encoded>&lt;p&gt;&lt;strong&gt;Authors:&lt;/strong&gt; Juncheng Li; Minghe Gao; Xiangnan He; Siliang Tang; Weishi Zheng; Jun Xiao; Meng Wang; Tat-Seng Chua; Yueting Zhuang&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Journal:&lt;/strong&gt; IEEE Transactions on Pattern Analysis and Machine Intelligence&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;a href="https://doi.org/10.1109/tpami.2026.3656169"&gt;10.1109/tpami.2026.3656169&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Relevance:&lt;/strong&gt; 0.838 (must_read)&lt;/p&gt;
&lt;p&gt;Large Language Models (LLMs) exhibit remarkable proficiency in understanding and managing text-based tasks. Many works try to transfer these capabilities to the video domain, which are referred to as Video-LLMs. However, current Video-LLMs can only grasp the coarse-grained semantics and are unable to efficiently handle tasks involving the comprehension or localization of specific video segments. To address these challenges, we propose Momentor, a Video-LLM designed to perform fine-grained temporal understanding tasks. To facilitate the training of Momentor, we develop an automatic data generation engine to build Moment-10 M, a large-scale video instruction dataset with segment-level instruction data. Building upon the foundation of the previously published Momentor and the Moment-10 M dataset, we further extend this work by introducing a Spatio-Temporal Token Consolidation (STTC) method, which can merge redundant visual tokens spatio-temporally in a parameter-free manner, thereby significantly promoting computational efficiency while preserving fine-grained visual details. We integrate STTC with Momentor to develop Momentor++ and validate its performance on various benchmarks. Momentor demonstrates robust capabilities in fine-grained temporal understanding and localization. Further, Momentor++ excels in efficiently processing and analyzing extended videos with complex events, showcasing marked advancements in handling extensive temporal contexts.&lt;/p&gt;</content:encoded></item><item><title>PCL-Reasoner-V1.5: Advancing Math Reasoning with Offline Reinforcement Learning</title><link>https://arxiv.org/abs/2601.14716v1</link><guid>http://arxiv.org/abs/2601.14716v1</guid><pubDate>Wed, 21 Jan 2026 07:11:40 +0000</pubDate><dc:creator>Yao Lu</dc:creator><dc:creator>Dengdong Fan</dc:creator><dc:creator>Jianzheng Nie</dc:creator><dc:creator>Fan Xu</dc:creator><dc:creator>Jie Chen</dc:creator><dc:creator>Bin Zhou</dc:creator><dc:creator>Yonghong Tian</dc:creator><dc:type>preprint</dc:type><prism:publicationName>arXiv</prism:publicationName><description>We present PCL-Reasoner-V1.5, a 32-billion-parameter large language model (LLM) for mathematical reasoning. The model is built upon Qwen2.5-32B and refined via supervised fine-tuning (SFT) followed by reinforcement learning (RL). A central innovation is our proposed offline RL method, which provides superior training stability and efficiency over standard online RL methods such as GRPO. Our model achieves state-of-the-art performance among models post-trained on Qwen2.5-32B, attaining average accuracies of 90.9% on AIME 2024 and 85.6% on AIME 2025. Our work demonstrates offline RL as a stable and efficient paradigm for advancing reasoning in LLMs. All experiments were conducted on Huawei Ascend 910C NPUs.
Published: 2026-01-21T07:11:40+00:00
Venue: arXiv
Score: 0.812 (must_read)</description><content:encoded>&lt;p&gt;&lt;strong&gt;Authors:&lt;/strong&gt; Yao Lu; Dengdong Fan; Jianzheng Nie; Fan Xu; Jie Chen; Bin Zhou; Yonghong Tian&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Journal:&lt;/strong&gt; arXiv&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Relevance:&lt;/strong&gt; 0.812 (must_read)&lt;/p&gt;
&lt;p&gt;We present PCL-Reasoner-V1.5, a 32-billion-parameter large language model (LLM) for mathematical reasoning. The model is built upon Qwen2.5-32B and refined via supervised fine-tuning (SFT) followed by reinforcement learning (RL). A central innovation is our proposed offline RL method, which provides superior training stability and efficiency over standard online RL methods such as GRPO. Our model achieves state-of-the-art performance among models post-trained on Qwen2.5-32B, attaining average accuracies of 90.9% on AIME 2024 and 85.6% on AIME 2025. Our work demonstrates offline RL as a stable and efficient paradigm for advancing reasoning in LLMs. All experiments were conducted on Huawei Ascend 910C NPUs.&lt;/p&gt;</content:encoded></item><item><title>Cross-scale Channel Attention and Feature Fusion-aware Aggregation for Sonar Images Object Detection</title><link>https://doi.org/10.1016/j.knosys.2026.115371</link><guid>10.1016/j.knosys.2026.115371</guid><pubDate>Tue, 20 Jan 2026 07:43:28 +0000</pubDate><dc:creator>Pengfei Shi</dc:creator><dc:creator>Hanren Wang</dc:creator><dc:creator>Qianqian Zhang</dc:creator><dc:creator>Yuanxue Xin</dc:creator><prism:publicationName>Knowledge-Based Systems</prism:publicationName><prism:doi>10.1016/j.knosys.2026.115371</prism:doi><description>Feature extraction and feature fusion are crucial for sonar image target detection. In terms of feature extraction, due to device limitations and the complexity of the underwater environment, sonar images often exhibit high levels of noise, which results in high similarity between targets and background, thus affecting feature extraction. In terms of feature fusion, transformer-based models rely on self-attention mechanisms, but this leads to a lack of local prior information. The interference from noise and the similarity between targets and background disrupt the computation of global relationships, confusing noisy features with useful ones, leading to insufficient geometric information and ultimately affecting detection accuracy. To address these issues, we propose an advanced detection framework that combines effective feature extraction and multi-scale feature fusion. We introduce a cross-scale channel attention module that dynamically adjusts channel weights by integrating the advantages of the squeeze-and-excitation (SE) module and the efficient multi-scale attention (EMA) module, capturing multi-scale dependencies, suppressing background noise, and enhancing global feature representation. Moreover, to further improve the effectiveness of feature fusion and better leverage geometric information, we design a CNN-based feature fusion perception aggregation network. This network promotes interaction between low-level geometric details and high-level semantic information through skip connections, enhancing feature representation and improving detection accuracy. Experimental results show that our method outperforms some advanced detection models in terms of detection performance.
Published: 2026-01-20T07:43:28+00:00
Venue: Knowledge-Based Systems
Score: 0.807 (must_read)</description><content:encoded>&lt;p&gt;&lt;strong&gt;Authors:&lt;/strong&gt; Pengfei Shi; Hanren Wang; Qianqian Zhang; Yuanxue Xin&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Journal:&lt;/strong&gt; Knowledge-Based Systems&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;a href="https://doi.org/10.1016/j.knosys.2026.115371"&gt;10.1016/j.knosys.2026.115371&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Relevance:&lt;/strong&gt; 0.807 (must_read)&lt;/p&gt;
&lt;p&gt;Feature extraction and feature fusion are crucial for sonar image target detection. In terms of feature extraction, due to device limitations and the complexity of the underwater environment, sonar images often exhibit high levels of noise, which results in high similarity between targets and background, thus affecting feature extraction. In terms of feature fusion, transformer-based models rely on self-attention mechanisms, but this leads to a lack of local prior information. The interference from noise and the similarity between targets and background disrupt the computation of global relationships, confusing noisy features with useful ones, leading to insufficient geometric information and ultimately affecting detection accuracy. To address these issues, we propose an advanced detection framework that combines effective feature extraction and multi-scale feature fusion. We introduce a cross-scale channel attention module that dynamically adjusts channel weights by integrating the advantages of the squeeze-and-excitation (SE) module and the efficient multi-scale attention (EMA) module, capturing multi-scale dependencies, suppressing background noise, and enhancing global feature representation. Moreover, to further improve the effectiveness of feature fusion and better leverage geometric information, we design a CNN-based feature fusion perception aggregation network. This network promotes interaction between low-level geometric details and high-level semantic information through skip connections, enhancing feature representation and improving detection accuracy. Experimental results show that our method outperforms some advanced detection models in terms of detection performance.&lt;/p&gt;</content:encoded></item><item><title>Roadside lidar-based scene understanding toward intelligent traffic perception: A comprehensive review</title><link>https://doi.org/10.1016/j.isprsjprs.2026.01.012</link><guid>10.1016/j.isprsjprs.2026.01.012</guid><pubDate>Tue, 20 Jan 2026 16:22:22 +0000</pubDate><dc:creator>Jiaxing Zhang</dc:creator><dc:creator>Chengjun Ge</dc:creator><dc:creator>Wen Xiao</dc:creator><dc:creator>Miao Tang</dc:creator><dc:creator>Jon Mills</dc:creator><dc:creator>Benjamin Coifman</dc:creator><dc:creator>Nengcheng Chen</dc:creator><prism:publicationName>ISPRS Journal of Photogrammetry and Remote Sensing</prism:publicationName><prism:doi>10.1016/j.isprsjprs.2026.01.012</prism:doi><description>Urban transportation systems are undergoing a paradigm shift with the integration of high-precision sensing technologies and intelligent perception frameworks. Roadside lidar, as a key enabler of infrastructure-based sensing technology, offers robust and precise 3D spatial understanding of dynamic urban scenes. This paper presents a comprehensive review of roadside lidar-based traffic perception, structured around five key modules: sensor placement strategies; multi-lidar point cloud fusion; dynamic traffic information extraction;subsequent applications including trajectory prediction, collision risk assessment, and behavioral analysis; representative roadside perception benchmark datasets. Despite notable progress, challenges remain in deployment optimization, robust registration under occlusion and dynamic conditions, generalizable object detection and tracking, and effective utilization of heterogeneous multi-modal data. Emerging trends point toward perception-driven infrastructure design, edge-cloud-terminal collaboration, and generalizable models enabled by domain adaptation, self-supervised learning, and foundation-scale datasets. This review aims to serve as a technical reference for researchers and practitioners, providing insights into current advances, open problems, and future directions in roadside lidar-based traffic perception and digital twin applications.
Published: 2026-01-20T16:22:22+00:00
Venue: ISPRS Journal of Photogrammetry and Remote Sensing
Score: 0.804 (must_read)</description><content:encoded>&lt;p&gt;&lt;strong&gt;Authors:&lt;/strong&gt; Jiaxing Zhang; Chengjun Ge; Wen Xiao; Miao Tang; Jon Mills; Benjamin Coifman; Nengcheng Chen&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Journal:&lt;/strong&gt; ISPRS Journal of Photogrammetry and Remote Sensing&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;a href="https://doi.org/10.1016/j.isprsjprs.2026.01.012"&gt;10.1016/j.isprsjprs.2026.01.012&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Relevance:&lt;/strong&gt; 0.804 (must_read)&lt;/p&gt;
&lt;p&gt;Urban transportation systems are undergoing a paradigm shift with the integration of high-precision sensing technologies and intelligent perception frameworks. Roadside lidar, as a key enabler of infrastructure-based sensing technology, offers robust and precise 3D spatial understanding of dynamic urban scenes. This paper presents a comprehensive review of roadside lidar-based traffic perception, structured around five key modules: sensor placement strategies; multi-lidar point cloud fusion; dynamic traffic information extraction;subsequent applications including trajectory prediction, collision risk assessment, and behavioral analysis; representative roadside perception benchmark datasets. Despite notable progress, challenges remain in deployment optimization, robust registration under occlusion and dynamic conditions, generalizable object detection and tracking, and effective utilization of heterogeneous multi-modal data. Emerging trends point toward perception-driven infrastructure design, edge-cloud-terminal collaboration, and generalizable models enabled by domain adaptation, self-supervised learning, and foundation-scale datasets. This review aims to serve as a technical reference for researchers and practitioners, providing insights into current advances, open problems, and future directions in roadside lidar-based traffic perception and digital twin applications.&lt;/p&gt;</content:encoded></item><item><title>CoScale-RL: Efficient Post-Training by Co-Scaling Data and Computation</title><link>https://arxiv.org/abs/2601.14695v1</link><guid>http://arxiv.org/abs/2601.14695v1</guid><pubDate>Wed, 21 Jan 2026 06:17:52 +0000</pubDate><dc:creator>Yutong Chen</dc:creator><dc:creator>Jiandong Gao</dc:creator><dc:creator>Ji Wu</dc:creator><dc:type>preprint</dc:type><prism:publicationName>arXiv</prism:publicationName><description>Training Large Reasoning Model (LRM) is usually unstable and unpredictable, especially on hard problems or weak foundation models. We found that the current post-training scaling strategy can still improve on these cases. We propose CoScale-RL, a novel scaling strategy with better data and computational efficiency. We first scale up solutions to make problems solvable. The core idea is to collect multiple solutions for each problem, rather than simply enlarging the dataset. Then, we scale up rollout computation to stabilize Reinforcement Learning. We further leverage a model merge technique called Re-distillation to sustain or even improve computational efficiency when scaling up. Our method significantly improves data and computational efficiency, with an average 3.76$\times$ accuracy improvement on four benchmarks. CoScale-RL is able to improve an LRM's ability boundary without an extensive SFT dataset. Our method provides a new scaling direction to further improve LRM's reasoning ability.
Published: 2026-01-21T06:17:52+00:00
Venue: arXiv
Score: 0.803 (must_read)</description><content:encoded>&lt;p&gt;&lt;strong&gt;Authors:&lt;/strong&gt; Yutong Chen; Jiandong Gao; Ji Wu&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Journal:&lt;/strong&gt; arXiv&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Relevance:&lt;/strong&gt; 0.803 (must_read)&lt;/p&gt;
&lt;p&gt;Training Large Reasoning Model (LRM) is usually unstable and unpredictable, especially on hard problems or weak foundation models. We found that the current post-training scaling strategy can still improve on these cases. We propose CoScale-RL, a novel scaling strategy with better data and computational efficiency. We first scale up solutions to make problems solvable. The core idea is to collect multiple solutions for each problem, rather than simply enlarging the dataset. Then, we scale up rollout computation to stabilize Reinforcement Learning. We further leverage a model merge technique called Re-distillation to sustain or even improve computational efficiency when scaling up. Our method significantly improves data and computational efficiency, with an average 3.76$\times$ accuracy improvement on four benchmarks. CoScale-RL is able to improve an LRM&amp;#x27;s ability boundary without an extensive SFT dataset. Our method provides a new scaling direction to further improve LRM&amp;#x27;s reasoning ability.&lt;/p&gt;</content:encoded></item><item><title>Layer-adaptive Expert Pruning for Pre-Training of Mixture-of-Experts Large Language Models</title><link>https://arxiv.org/abs/2601.14327v1</link><guid>http://arxiv.org/abs/2601.14327v1</guid><pubDate>Tue, 20 Jan 2026 08:39:04 +0000</pubDate><dc:creator>YuanLab. ai</dc:creator><dc:creator>Shawn Wu</dc:creator><dc:creator>Jiangang Luo</dc:creator><dc:creator>Tong Yu</dc:creator><dc:creator>Darcy Chen</dc:creator><dc:creator>Sean Wang</dc:creator><dc:creator>Xudong Zhao</dc:creator><dc:creator>Louie Li</dc:creator><dc:creator>Claire Wang</dc:creator><dc:creator>Hunter He</dc:creator><dc:creator>Carol Wang</dc:creator><dc:creator>Allen Wang</dc:creator><dc:type>preprint</dc:type><prism:publicationName>arXiv</prism:publicationName><description>Although Mixture-of-Experts (MoE) Large Language Models (LLMs) deliver superior accuracy with a reduced number of active parameters, their pre-training represents a significant computationally bottleneck due to underutilized experts and limited training efficiency. This work introduces a Layer-Adaptive Expert Pruning (LAEP) algorithm designed for the pre-training stage of MoE LLMs. In contrast to previous expert pruning approaches that operate primarily in the post-training phase, the proposed algorithm enhances training efficiency by selectively pruning underutilized experts and reorganizing experts across computing devices according to token distribution statistics. Comprehensive experiments demonstrate that LAEP effectively reduces model size and substantially improves pre-training efficiency. In particular, when pre-training the 1010B Base model from scratch, LAEP achieves a 48.3\% improvement in training efficiency alongside a 33.3% parameter reduction, while still delivering excellent performance across multiple domains.
Published: 2026-01-20T08:39:04+00:00
Venue: arXiv
Score: 0.801 (must_read)</description><content:encoded>&lt;p&gt;&lt;strong&gt;Authors:&lt;/strong&gt; YuanLab. ai; Shawn Wu; Jiangang Luo; Tong Yu; Darcy Chen; Sean Wang; Xudong Zhao; Louie Li; Claire Wang; Hunter He; Carol Wang; Allen Wang&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Journal:&lt;/strong&gt; arXiv&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Relevance:&lt;/strong&gt; 0.801 (must_read)&lt;/p&gt;
&lt;p&gt;Although Mixture-of-Experts (MoE) Large Language Models (LLMs) deliver superior accuracy with a reduced number of active parameters, their pre-training represents a significant computationally bottleneck due to underutilized experts and limited training efficiency. This work introduces a Layer-Adaptive Expert Pruning (LAEP) algorithm designed for the pre-training stage of MoE LLMs. In contrast to previous expert pruning approaches that operate primarily in the post-training phase, the proposed algorithm enhances training efficiency by selectively pruning underutilized experts and reorganizing experts across computing devices according to token distribution statistics. Comprehensive experiments demonstrate that LAEP effectively reduces model size and substantially improves pre-training efficiency. In particular, when pre-training the 1010B Base model from scratch, LAEP achieves a 48.3\% improvement in training efficiency alongside a 33.3% parameter reduction, while still delivering excellent performance across multiple domains.&lt;/p&gt;</content:encoded></item><item><title>Modulation and Perturbation in Frequency Domain for SAR Ship Detection</title><link>https://doi.org/10.3390/rs18020338</link><guid>10.3390/rs18020338</guid><pubDate>Tue, 20 Jan 2026 11:27:58 +0000</pubDate><dc:creator>Mengqin Fu</dc:creator><dc:creator>Wencong Zhang</dc:creator><dc:creator>Xiaochen Quan</dc:creator><dc:creator>Dahu Shi</dc:creator><dc:creator>Luowei Tan</dc:creator><dc:creator>Jia Zhang</dc:creator><dc:creator>Yinghui Xing</dc:creator><dc:creator>Shizhou Zhang</dc:creator><prism:publicationName>Remote Sensing</prism:publicationName><prism:doi>10.3390/rs18020338</prism:doi><description>Synthetic Aperture Radar (SAR) has unique advantages in ship monitoring at sea due to its all-weather imaging capability. However, its unique imaging mechanism presents two major challenges. First, speckle noise in the frequency domain reduces the contrast between the target and the background. Second, side-lobe scattering blurs the ship outline, especially in nearshore complex scenes, and strong scattering characteristics make it difficult to separate the target from the background. The above two challenges significantly limit the performance of tailored CNN-based detection models in optical images when applied directly to SAR images. To address these challenges, this paper proposes a modulation and perturbation mechanism in the frequency domain based on a lightweight CNN detector. Specifically, the wavelet transform is firstly used to extract high-frequency features in different directions, and feature expression is dynamically adjusted according to the global statistical information to realize the selective enhancement of the ship edge and detail information. In terms of frequency-domain perturbation, a perturbation mechanism guided by frequency-domain weight is introduced to effectively suppress background interference while maintaining key target characteristics, which improves the robustness of the model in complex scenes. Extensive experiments on four widely adopted benchmark datasets, namely LS-SSDD-v1.0, SSDD, SAR-Ship-Dataset, and AIR-SARShip-2.0, demonstrate that our FMP-Net significantly outperforms 18 existing state-of-the-art methods, especially in complex nearshore scenes and sea surface interference scenes.
Published: 2026-01-20T11:27:58+00:00
Venue: Remote Sensing
Score: 0.801 (must_read)</description><content:encoded>&lt;p&gt;&lt;strong&gt;Authors:&lt;/strong&gt; Mengqin Fu; Wencong Zhang; Xiaochen Quan; Dahu Shi; Luowei Tan; Jia Zhang; Yinghui Xing; Shizhou Zhang&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Journal:&lt;/strong&gt; Remote Sensing&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;a href="https://doi.org/10.3390/rs18020338"&gt;10.3390/rs18020338&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Relevance:&lt;/strong&gt; 0.801 (must_read)&lt;/p&gt;
&lt;p&gt;Synthetic Aperture Radar (SAR) has unique advantages in ship monitoring at sea due to its all-weather imaging capability. However, its unique imaging mechanism presents two major challenges. First, speckle noise in the frequency domain reduces the contrast between the target and the background. Second, side-lobe scattering blurs the ship outline, especially in nearshore complex scenes, and strong scattering characteristics make it difficult to separate the target from the background. The above two challenges significantly limit the performance of tailored CNN-based detection models in optical images when applied directly to SAR images. To address these challenges, this paper proposes a modulation and perturbation mechanism in the frequency domain based on a lightweight CNN detector. Specifically, the wavelet transform is firstly used to extract high-frequency features in different directions, and feature expression is dynamically adjusted according to the global statistical information to realize the selective enhancement of the ship edge and detail information. In terms of frequency-domain perturbation, a perturbation mechanism guided by frequency-domain weight is introduced to effectively suppress background interference while maintaining key target characteristics, which improves the robustness of the model in complex scenes. Extensive experiments on four widely adopted benchmark datasets, namely LS-SSDD-v1.0, SSDD, SAR-Ship-Dataset, and AIR-SARShip-2.0, demonstrate that our FMP-Net significantly outperforms 18 existing state-of-the-art methods, especially in complex nearshore scenes and sea surface interference scenes.&lt;/p&gt;</content:encoded></item><item><title>Jointly modeling cardiovascular biomarkers</title><link>https://doi.org/10.1038/s42256-025-01172-x</link><guid>10.1038/s42256-025-01172-x</guid><pubDate>Wed, 21 Jan 2026 10:03:02 +0000</pubDate><dc:creator>Sully F. Chen</dc:creator><prism:publicationName>Nature Machine Intelligence</prism:publicationName><prism:doi>10.1038/s42256-025-01172-x</prism:doi><description>Capturing the complexity of cardiovascular dynamics demands multiple monitoring modalities, each with inherent trade-offs. Diffusion-based modeling offers a promising route for synthesizing and generating cross-modal data.
Published: 2026-01-21T10:03:02+00:00
Venue: Nature Machine Intelligence
Score: 0.800 (must_read)</description><content:encoded>&lt;p&gt;&lt;strong&gt;Authors:&lt;/strong&gt; Sully F. Chen&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Journal:&lt;/strong&gt; Nature Machine Intelligence&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;a href="https://doi.org/10.1038/s42256-025-01172-x"&gt;10.1038/s42256-025-01172-x&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Relevance:&lt;/strong&gt; 0.800 (must_read)&lt;/p&gt;
&lt;p&gt;Capturing the complexity of cardiovascular dynamics demands multiple monitoring modalities, each with inherent trade-offs. Diffusion-based modeling offers a promising route for synthesizing and generating cross-modal data.&lt;/p&gt;</content:encoded></item><item><title>Visual Position Prompt for MLLM Based Visual Grounding</title><link>https://doi.org/10.1109/tmm.2026.3654372</link><guid>10.1109/tmm.2026.3654372</guid><pubDate>Tue, 20 Jan 2026 20:40:43 +0000</pubDate><dc:creator>Wei Tang</dc:creator><dc:creator>Yanpeng Sun</dc:creator><dc:creator>Qinying Gu</dc:creator><dc:creator>Zechao Li</dc:creator><prism:publicationName>IEEE Transactions on Multimedia</prism:publicationName><prism:doi>10.1109/tmm.2026.3654372</prism:doi><description>Although Multimodal Large Language Models (MLLMs) excel at various image-related tasks, they encounter challenges in precisely aligning coordinates with spatial information within images, particularly in position-aware tasks such as visual grounding. This limitation arises from two key factors. First, MLLMs lack explicit spatial references, making it difficult to associate textual descriptions with precise image locations. Second, their feature extraction processes prioritize global context over fine-grained spatial details, leading to weak localization capability. To address these issues, we introduce VPP-LLaVA, an MLLM enhanced with Visual Position Prompt (VPP) to improve its grounding capability. VPP-LLaVA integrates two complementary mechanisms: the global VPP overlays a learnable, axis-like tensor onto the input image to provide structured spatial cues, while the local VPP incorporates position-aware queries to support fine-grained localization. To effectively train our model with spatial guidance, we further introduce VPP-SFT, a curated dataset of 0.6 M high-quality visual grounding samples. Designed in a compact format, it enables efficient training and is significantly smaller than datasets used by other MLLMs (e.g., 21 M samples in MiniGPT-v2), yet still provides a strong performance boost. The resulting model, VPP-LLaVA, not only achieves state-of-the-art results on standard visual grounding benchmarks but also demonstrates strong zero-shot generalization to challenging unseen datasets.
Published: 2026-01-20T20:40:43+00:00
Venue: IEEE Transactions on Multimedia
Score: 0.799 (must_read)</description><content:encoded>&lt;p&gt;&lt;strong&gt;Authors:&lt;/strong&gt; Wei Tang; Yanpeng Sun; Qinying Gu; Zechao Li&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Journal:&lt;/strong&gt; IEEE Transactions on Multimedia&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;a href="https://doi.org/10.1109/tmm.2026.3654372"&gt;10.1109/tmm.2026.3654372&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Relevance:&lt;/strong&gt; 0.799 (must_read)&lt;/p&gt;
&lt;p&gt;Although Multimodal Large Language Models (MLLMs) excel at various image-related tasks, they encounter challenges in precisely aligning coordinates with spatial information within images, particularly in position-aware tasks such as visual grounding. This limitation arises from two key factors. First, MLLMs lack explicit spatial references, making it difficult to associate textual descriptions with precise image locations. Second, their feature extraction processes prioritize global context over fine-grained spatial details, leading to weak localization capability. To address these issues, we introduce VPP-LLaVA, an MLLM enhanced with Visual Position Prompt (VPP) to improve its grounding capability. VPP-LLaVA integrates two complementary mechanisms: the global VPP overlays a learnable, axis-like tensor onto the input image to provide structured spatial cues, while the local VPP incorporates position-aware queries to support fine-grained localization. To effectively train our model with spatial guidance, we further introduce VPP-SFT, a curated dataset of 0.6 M high-quality visual grounding samples. Designed in a compact format, it enables efficient training and is significantly smaller than datasets used by other MLLMs (e.g., 21 M samples in MiniGPT-v2), yet still provides a strong performance boost. The resulting model, VPP-LLaVA, not only achieves state-of-the-art results on standard visual grounding benchmarks but also demonstrates strong zero-shot generalization to challenging unseen datasets.&lt;/p&gt;</content:encoded></item><item><title>语义引导对比学习的SAR与光学图像转换</title><link>https://doi.org/10.11834/jig.250526</link><guid>10.11834/jig.250526</guid><pubDate>Tue, 20 Jan 2026 06:30:29 +0000</pubDate><dc:creator>Du Wenliang</dc:creator><dc:creator>Guo Bo</dc:creator><dc:creator>Zhao Jiaqi</dc:creator><dc:creator>Yao Rui</dc:creator><dc:creator>Zhou Yong</dc:creator><prism:publicationName>Journal of Image and Graphics</prism:publicationName><prism:doi>10.11834/jig.250526</prism:doi><description>目的合成孔径雷达（synthetic aperture radar， SAR）与光学图像转换能够融合两种模态数据的优势，提供全天时、全天候与高分辨率的观测能力。然而，当前基于循环一致性生成对抗网络的方法主要侧重于图像结构的宏观重建，未能充分利用跨模态间的深层语义信息来指导图像生成，限制了生成图像的语义保真度和在下游任务中的性能。同时，现有基于对比学习的转换方法在处理遥感图像时，因同类地物特征高度自相关导致正负样本难以区分，造成对比机制失效。针对上述问题，提出了一种语义引导对比学习的SAR与光学图像转换方法。方法提出了基于语义分割的特征提取模块，利用预训练的SAR与光学语义分割模型提取像素级语义信息；提出了语义引导的对比学习模块，利用先验的语义分割信息，在对比学习空间中显式构建基于类别一致性的正负样本筛选机制，有效解决了遥感图像特征同质化导致的传统对比学习失效问题；设计了融合循环生成结构与对比学习的联合优化框架，通过引入循环语义分割损失与生成对抗损失，约束生成图像在结构、纹理和语义层面的一致性。结果实验在WHU-OPT-SAR和DDHRNet两个公开数据集上进行。实验结果表明，与当前最优方法相比，在SAR到光学及光学到SAR的图像转换任务中，生成质量指标分别最高提升了11.9%和3.8%；在下游任务中，语义分割准确率分别提升了16.29%和10.19%，特征匹配的正确内点比例最高提升了1%。消融实验研究表明，语义引导对比学习模块与循环语义分割损失对提升模型性能均起到关键作用。结论本文提出的语义引导对比学习的SAR与光学图像转换方法，能够有效解决传统对比学习在遥感图像转换中的失效问题，显著提升了生成图像的语义保真度与跨模态特征对齐能力，在下游语义分割和图像匹配任务中取得了最优的综合性能，为无监督SAR与光学图像转换提供了新的解决思路。本文代码开源在链接：https：//www.scidb.cn/s/VVVBnu。
Published: 2026-01-20T06:30:29+00:00
Venue: Journal of Image and Graphics
Score: 0.798 (must_read)</description><content:encoded>&lt;p&gt;&lt;strong&gt;Authors:&lt;/strong&gt; Du Wenliang; Guo Bo; Zhao Jiaqi; Yao Rui; Zhou Yong&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Journal:&lt;/strong&gt; Journal of Image and Graphics&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;a href="https://doi.org/10.11834/jig.250526"&gt;10.11834/jig.250526&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Relevance:&lt;/strong&gt; 0.798 (must_read)&lt;/p&gt;
&lt;p&gt;目的合成孔径雷达（synthetic aperture radar， SAR）与光学图像转换能够融合两种模态数据的优势，提供全天时、全天候与高分辨率的观测能力。然而，当前基于循环一致性生成对抗网络的方法主要侧重于图像结构的宏观重建，未能充分利用跨模态间的深层语义信息来指导图像生成，限制了生成图像的语义保真度和在下游任务中的性能。同时，现有基于对比学习的转换方法在处理遥感图像时，因同类地物特征高度自相关导致正负样本难以区分，造成对比机制失效。针对上述问题，提出了一种语义引导对比学习的SAR与光学图像转换方法。方法提出了基于语义分割的特征提取模块，利用预训练的SAR与光学语义分割模型提取像素级语义信息；提出了语义引导的对比学习模块，利用先验的语义分割信息，在对比学习空间中显式构建基于类别一致性的正负样本筛选机制，有效解决了遥感图像特征同质化导致的传统对比学习失效问题；设计了融合循环生成结构与对比学习的联合优化框架，通过引入循环语义分割损失与生成对抗损失，约束生成图像在结构、纹理和语义层面的一致性。结果实验在WHU-OPT-SAR和DDHRNet两个公开数据集上进行。实验结果表明，与当前最优方法相比，在SAR到光学及光学到SAR的图像转换任务中，生成质量指标分别最高提升了11.9%和3.8%；在下游任务中，语义分割准确率分别提升了16.29%和10.19%，特征匹配的正确内点比例最高提升了1%。消融实验研究表明，语义引导对比学习模块与循环语义分割损失对提升模型性能均起到关键作用。结论本文提出的语义引导对比学习的SAR与光学图像转换方法，能够有效解决传统对比学习在遥感图像转换中的失效问题，显著提升了生成图像的语义保真度与跨模态特征对齐能力，在下游语义分割和图像匹配任务中取得了最优的综合性能，为无监督SAR与光学图像转换提供了新的解决思路。本文代码开源在链接：https：//www.scidb.cn/s/VVVBnu。&lt;/p&gt;</content:encoded></item><item><title>FeedbackSTS-Det: Sparse Frames-Based Spatio-Temporal Semantic Feedback Network for Infrared Small Target Detection</title><link>https://arxiv.org/abs/2601.14690v1</link><guid>http://arxiv.org/abs/2601.14690v1</guid><pubDate>Wed, 21 Jan 2026 06:06:36 +0000</pubDate><dc:creator>Yian Huang</dc:creator><dc:creator>Qing Qin</dc:creator><dc:creator>Aji Mao</dc:creator><dc:creator>Xiangyu Qiu</dc:creator><dc:creator>Liang Xu</dc:creator><dc:creator>Xian Zhang</dc:creator><dc:creator>Zhenming Peng</dc:creator><dc:type>preprint</dc:type><prism:publicationName>arXiv</prism:publicationName><description>Infrared small target detection (ISTD) under complex backgrounds remains a critical yet challenging task, primarily due to the extremely low signal-to-clutter ratio, persistent dynamic interference, and the lack of distinct target features. While multi-frame detection methods leverages temporal cues to improve upon single-frame approaches, existing methods still struggle with inefficient long-range dependency modeling and insufficient robustness. To overcome these issues, we propose a novel scheme for ISTD, realized through a sparse frames-based spatio-temporal semantic feedback network named FeedbackSTS-Det. The core of our approach is a novel spatio-temporal semantic feedback strategy with a closed-loop semantic association mechanism, which consists of paired forward and backward refinement modules that work cooperatively across the encoder and decoder. Moreover, both modules incorporate an embedded sparse semantic module (SSM), which performs structured sparse temporal modeling to capture long-range dependencies with low computational cost. This integrated design facilitates robust implicit inter-frame registration and continuous semantic refinement, effectively suppressing false alarms. Furthermore, our overall procedure maintains a consistent training-inference pipeline, which ensures reliable performance transfer and increases model robustness. Extensive experiments on multiple benchmark datasets confirm the effectiveness of FeedbackSTS-Det. Code and models are available at: https://github.com/IDIP-Lab/FeedbackSTS-Det.
Published: 2026-01-21T06:06:36+00:00
Venue: arXiv
Score: 0.797 (must_read)</description><content:encoded>&lt;p&gt;&lt;strong&gt;Authors:&lt;/strong&gt; Yian Huang; Qing Qin; Aji Mao; Xiangyu Qiu; Liang Xu; Xian Zhang; Zhenming Peng&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Journal:&lt;/strong&gt; arXiv&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Relevance:&lt;/strong&gt; 0.797 (must_read)&lt;/p&gt;
&lt;p&gt;Infrared small target detection (ISTD) under complex backgrounds remains a critical yet challenging task, primarily due to the extremely low signal-to-clutter ratio, persistent dynamic interference, and the lack of distinct target features. While multi-frame detection methods leverages temporal cues to improve upon single-frame approaches, existing methods still struggle with inefficient long-range dependency modeling and insufficient robustness. To overcome these issues, we propose a novel scheme for ISTD, realized through a sparse frames-based spatio-temporal semantic feedback network named FeedbackSTS-Det. The core of our approach is a novel spatio-temporal semantic feedback strategy with a closed-loop semantic association mechanism, which consists of paired forward and backward refinement modules that work cooperatively across the encoder and decoder. Moreover, both modules incorporate an embedded sparse semantic module (SSM), which performs structured sparse temporal modeling to capture long-range dependencies with low computational cost. This integrated design facilitates robust implicit inter-frame registration and continuous semantic refinement, effectively suppressing false alarms. Furthermore, our overall procedure maintains a consistent training-inference pipeline, which ensures reliable performance transfer and increases model robustness. Extensive experiments on multiple benchmark datasets confirm the effectiveness of FeedbackSTS-Det. Code and models are available at: https://github.com/IDIP-Lab/FeedbackSTS-Det.&lt;/p&gt;</content:encoded></item><item><title>A Novel Knowledge Distillation Method for Graph Neural Networks with Gradient Mapping and Fusion</title><link>https://doi.org/10.1016/j.inffus.2026.104163</link><guid>10.1016/j.inffus.2026.104163</guid><pubDate>Tue, 20 Jan 2026 07:45:44 +0000</pubDate><dc:creator>Kang Liu</dc:creator><dc:creator>Shunzhi Yang</dc:creator><dc:creator>Chang-Dong Wang</dc:creator><dc:creator>Yunwen Chen</dc:creator><dc:creator>Zhenhua Huang</dc:creator><prism:publicationName>Information Fusion</prism:publicationName><prism:doi>10.1016/j.inffus.2026.104163</prism:doi><description>The primary goal of graph knowledge distillation (GKD) is to transfer knowledge from a complex graph neural network (GNN) teacher to a smaller, yet more efficient GNN or multi-layer perceptron student. Although existing methods address network scalability, they rely on a frozen teacher that fails to explain how to derive results, thus limiting performance and hindering the improvement of a student. Therefore, we propose a novel GKD method, termed Dynamic Gradient Distillation (DGD), consisting of Generative Adversarial Imitation Learning (GAIL)-based Gradient Mapping and Two-Stage Gradient Fusion modules. The former builds the teacher’s learning process to understand knowledge by drawing on the principle of GAIL. The latter consists of attention fusion and weighted bias operations. Through the attentional fusion operation, it captures and fuses the responses of the teacher to change the gradient of the student at each layer. The fused gradients are then updated by combining them with the student’s backpropagated gradients using the weighted bias operation. DGD allows the student to inherit and extend the teacher’s learning process efficiently. Extensive experiments conducted with seven publicly available datasets show that DGD could significantly outperform some existing methods in node classification tasks. Our code and data are released at https://github.com/KangL-G/Dynamic-Gradient-Distillation .
Published: 2026-01-20T07:45:44+00:00
Venue: Information Fusion
Score: 0.796 (must_read)</description><content:encoded>&lt;p&gt;&lt;strong&gt;Authors:&lt;/strong&gt; Kang Liu; Shunzhi Yang; Chang-Dong Wang; Yunwen Chen; Zhenhua Huang&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Journal:&lt;/strong&gt; Information Fusion&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;a href="https://doi.org/10.1016/j.inffus.2026.104163"&gt;10.1016/j.inffus.2026.104163&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Relevance:&lt;/strong&gt; 0.796 (must_read)&lt;/p&gt;
&lt;p&gt;The primary goal of graph knowledge distillation (GKD) is to transfer knowledge from a complex graph neural network (GNN) teacher to a smaller, yet more efficient GNN or multi-layer perceptron student. Although existing methods address network scalability, they rely on a frozen teacher that fails to explain how to derive results, thus limiting performance and hindering the improvement of a student. Therefore, we propose a novel GKD method, termed Dynamic Gradient Distillation (DGD), consisting of Generative Adversarial Imitation Learning (GAIL)-based Gradient Mapping and Two-Stage Gradient Fusion modules. The former builds the teacher’s learning process to understand knowledge by drawing on the principle of GAIL. The latter consists of attention fusion and weighted bias operations. Through the attentional fusion operation, it captures and fuses the responses of the teacher to change the gradient of the student at each layer. The fused gradients are then updated by combining them with the student’s backpropagated gradients using the weighted bias operation. DGD allows the student to inherit and extend the teacher’s learning process efficiently. Extensive experiments conducted with seven publicly available datasets show that DGD could significantly outperform some existing methods in node classification tasks. Our code and data are released at https://github.com/KangL-G/Dynamic-Gradient-Distillation .&lt;/p&gt;</content:encoded></item><item><title>Revisiting Multi-Task Visual Representation Learning</title><link>https://arxiv.org/abs/2601.13886v1</link><guid>http://arxiv.org/abs/2601.13886v1</guid><pubDate>Tue, 20 Jan 2026 11:59:19 +0000</pubDate><dc:creator>Shangzhe Di</dc:creator><dc:creator>Zhonghua Zhai</dc:creator><dc:creator>Weidi Xie</dc:creator><dc:type>preprint</dc:type><prism:publicationName>arXiv</prism:publicationName><description>Current visual representation learning remains bifurcated: vision-language models (e.g., CLIP) excel at global semantic alignment but lack spatial precision, while self-supervised methods (e.g., MAE, DINO) capture intricate local structures yet struggle with high-level semantic context. We argue that these paradigms are fundamentally complementary and can be integrated into a principled multi-task framework, further enhanced by dense spatial supervision. We introduce MTV, a multi-task visual pretraining framework that jointly optimizes a shared backbone across vision-language contrastive, self-supervised, and dense spatial objectives. To mitigate the need for manual annotations, we leverage high-capacity "expert" models -- such as Depth Anything V2 and OWLv2 -- to synthesize dense, structured pseudo-labels at scale. Beyond the framework, we provide a systematic investigation into the mechanics of multi-task visual learning, analyzing: (i) the marginal gain of each objective, (ii) task synergies versus interference, and (iii) scaling behavior across varying data and model scales. Our results demonstrate that MTV achieves "best-of-both-worlds" performance, significantly enhancing fine-grained spatial reasoning without compromising global semantic understanding. Our findings suggest that multi-task learning, fueled by high-quality pseudo-supervision, is a scalable path toward more general visual encoders.
Published: 2026-01-20T11:59:19+00:00
Venue: arXiv
Score: 0.796 (must_read)</description><content:encoded>&lt;p&gt;&lt;strong&gt;Authors:&lt;/strong&gt; Shangzhe Di; Zhonghua Zhai; Weidi Xie&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Journal:&lt;/strong&gt; arXiv&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Relevance:&lt;/strong&gt; 0.796 (must_read)&lt;/p&gt;
&lt;p&gt;Current visual representation learning remains bifurcated: vision-language models (e.g., CLIP) excel at global semantic alignment but lack spatial precision, while self-supervised methods (e.g., MAE, DINO) capture intricate local structures yet struggle with high-level semantic context. We argue that these paradigms are fundamentally complementary and can be integrated into a principled multi-task framework, further enhanced by dense spatial supervision. We introduce MTV, a multi-task visual pretraining framework that jointly optimizes a shared backbone across vision-language contrastive, self-supervised, and dense spatial objectives. To mitigate the need for manual annotations, we leverage high-capacity &amp;quot;expert&amp;quot; models -- such as Depth Anything V2 and OWLv2 -- to synthesize dense, structured pseudo-labels at scale. Beyond the framework, we provide a systematic investigation into the mechanics of multi-task visual learning, analyzing: (i) the marginal gain of each objective, (ii) task synergies versus interference, and (iii) scaling behavior across varying data and model scales. Our results demonstrate that MTV achieves &amp;quot;best-of-both-worlds&amp;quot; performance, significantly enhancing fine-grained spatial reasoning without compromising global semantic understanding. Our findings suggest that multi-task learning, fueled by high-quality pseudo-supervision, is a scalable path toward more general visual encoders.&lt;/p&gt;</content:encoded></item><item><title>FlexiMo: A Flexible Remote Sensing Foundation Model</title><link>https://doi.org/10.1109/tgrs.2026.3656362</link><guid>10.1109/tgrs.2026.3656362</guid><pubDate>Tue, 20 Jan 2026 20:40:12 +0000</pubDate><dc:creator>Xuyang Li</dc:creator><dc:creator>Chenyu Li</dc:creator><dc:creator>Pedram Ghamisi</dc:creator><dc:creator>Danfeng Hong</dc:creator><dc:creator>Jon Atli Benediktsson</dc:creator><dc:creator>Jocelyn Chanussot</dc:creator><prism:publicationName>IEEE Transactions on Geoscience and Remote Sensing</prism:publicationName><prism:doi>10.1109/tgrs.2026.3656362</prism:doi><description>The rapid expansion of multi-source satellite imagery is driving innovation in Earth observation, opening unprecedented opportunities for Remote Sensing Foundation Models to harness diverse data. However, many existing models remain constrained by fixed spatial resolutions and patch sizes, limiting their ability to fully exploit the heterogeneous spatial characteristics inherent in satellite imagery. To address these challenges, we propose FlexiMo, a flexible remote sensing foundation model that endows the pre-trained model with the flexibility to adapt to arbitrary spatial resolutions. Central to FlexiMo is a spatial resolution-aware module that employs a parameter-free alignment embedding mechanism to dynamically recalibrate patch embeddings based on the resolution and dimensions of the input images. This design not only preserves the geometric fidelity of tokenization under varying image sizes, resolutions, and patch granularities, but also enables efficient feature extraction without requiring modifications to the underlying network architecture. In addition, FlexiMo incorporates a lightweight channel adaptation module that leverages prior spectral information from sensors. This mechanism allows the model to process images with varying numbers of channels while maintaining the data’s intrinsic physical properties. Extensive experiments on diverse multimodal, multi-resolution, and multi-scale datasets demonstrate that FlexiMo significantly enhances model generalization and robustness. In particular, the proposed method achieves outstanding performance across a range of downstream tasks, including scene classification, land cover classification, urban building segmentation, and cloud detection. We also explicitly validate physical consistency through wavelength-channel permutation and wavelength-perturbation tests, showing that FlexiMo is sensitive to physically incorrect spectral metadata while remaining robust to small wavelength deviations. By enabling paramete...
Published: 2026-01-20T20:40:12+00:00
Venue: IEEE Transactions on Geoscience and Remote Sensing
Score: 0.795 (must_read)</description><content:encoded>&lt;p&gt;&lt;strong&gt;Authors:&lt;/strong&gt; Xuyang Li; Chenyu Li; Pedram Ghamisi; Danfeng Hong; Jon Atli Benediktsson; Jocelyn Chanussot&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Journal:&lt;/strong&gt; IEEE Transactions on Geoscience and Remote Sensing&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;a href="https://doi.org/10.1109/tgrs.2026.3656362"&gt;10.1109/tgrs.2026.3656362&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Relevance:&lt;/strong&gt; 0.795 (must_read)&lt;/p&gt;
&lt;p&gt;The rapid expansion of multi-source satellite imagery is driving innovation in Earth observation, opening unprecedented opportunities for Remote Sensing Foundation Models to harness diverse data. However, many existing models remain constrained by fixed spatial resolutions and patch sizes, limiting their ability to fully exploit the heterogeneous spatial characteristics inherent in satellite imagery. To address these challenges, we propose FlexiMo, a flexible remote sensing foundation model that endows the pre-trained model with the flexibility to adapt to arbitrary spatial resolutions. Central to FlexiMo is a spatial resolution-aware module that employs a parameter-free alignment embedding mechanism to dynamically recalibrate patch embeddings based on the resolution and dimensions of the input images. This design not only preserves the geometric fidelity of tokenization under varying image sizes, resolutions, and patch granularities, but also enables efficient feature extraction without requiring modifications to the underlying network architecture. In addition, FlexiMo incorporates a lightweight channel adaptation module that leverages prior spectral information from sensors. This mechanism allows the model to process images with varying numbers of channels while maintaining the data’s intrinsic physical properties. Extensive experiments on diverse multimodal, multi-resolution, and multi-scale datasets demonstrate that FlexiMo significantly enhances model generalization and robustness. In particular, the proposed method achieves outstanding performance across a range of downstream tasks, including scene classification, land cover classification, urban building segmentation, and cloud detection. We also explicitly validate physical consistency through wavelength-channel permutation and wavelength-perturbation tests, showing that FlexiMo is sensitive to physically incorrect spectral metadata while remaining robust to small wavelength deviations. By enabling paramete...&lt;/p&gt;</content:encoded></item><item><title>Agentic Reasoning for Large Language Models</title><link>https://arxiv.org/abs/2601.12538v1</link><guid>http://arxiv.org/abs/2601.12538v1</guid><pubDate>Sun, 18 Jan 2026 18:58:23 +0000</pubDate><dc:creator>Tianxin Wei</dc:creator><dc:creator>Ting-Wei Li</dc:creator><dc:creator>Zhining Liu</dc:creator><dc:creator>Xuying Ning</dc:creator><dc:creator>Ze Yang</dc:creator><dc:creator>Jiaru Zou</dc:creator><dc:creator>Zhichen Zeng</dc:creator><dc:creator>Ruizhong Qiu</dc:creator><dc:creator>Xiao Lin</dc:creator><dc:creator>Dongqi Fu</dc:creator><dc:creator>Zihao Li</dc:creator><dc:creator>Mengting Ai</dc:creator><dc:creator>Duo Zhou</dc:creator><dc:creator>Wenxuan Bao</dc:creator><dc:creator>Yunzhe Li</dc:creator><dc:creator>Gaotang Li</dc:creator><dc:creator>Cheng Qian</dc:creator><dc:creator>Yu Wang</dc:creator><dc:creator>Xiangru Tang</dc:creator><dc:creator>Yin Xiao</dc:creator><dc:creator>Liri Fang</dc:creator><dc:creator>Hui Liu</dc:creator><dc:creator>Xianfeng Tang</dc:creator><dc:creator>Yuji Zhang</dc:creator><dc:creator>Chi Wang</dc:creator><dc:creator>Jiaxuan You</dc:creator><dc:creator>Heng Ji</dc:creator><dc:creator>Hanghang Tong</dc:creator><dc:creator>Jingrui He</dc:creator><dc:type>preprint</dc:type><prism:publicationName>arXiv</prism:publicationName><description>Reasoning is a fundamental cognitive process underlying inference, problem-solving, and decision-making. While large language models (LLMs) demonstrate strong reasoning capabilities in closed-world settings, they struggle in open-ended and dynamic environments. Agentic reasoning marks a paradigm shift by reframing LLMs as autonomous agents that plan, act, and learn through continual interaction. In this survey, we organize agentic reasoning along three complementary dimensions. First, we characterize environmental dynamics through three layers: foundational agentic reasoning, which establishes core single-agent capabilities including planning, tool use, and search in stable environments; self-evolving agentic reasoning, which studies how agents refine these capabilities through feedback, memory, and adaptation; and collective multi-agent reasoning, which extends intelligence to collaborative settings involving coordination, knowledge sharing, and shared goals. Across these layers, we distinguish in-context reasoning, which scales test-time interaction through structured orchestration, from post-training reasoning, which optimizes behaviors via reinforcement learning and supervised fine-tuning. We further review representative agentic reasoning frameworks across real-world applications and benchmarks, including science, robotics, healthcare, autonomous research, and mathematics. This survey synthesizes agentic reasoning methods into a unified roadmap bridging thought and action, and outlines open challenges and future directions, including personalization, long-horizon interaction, world modeling, scalable multi-agent training, and governance for real-world deployment.
Published: 2026-01-18T18:58:23+00:00
Venue: arXiv
Score: 0.795 (must_read)</description><content:encoded>&lt;p&gt;&lt;strong&gt;Authors:&lt;/strong&gt; Tianxin Wei; Ting-Wei Li; Zhining Liu; Xuying Ning; Ze Yang; Jiaru Zou; Zhichen Zeng; Ruizhong Qiu; Xiao Lin; Dongqi Fu; Zihao Li; Mengting Ai; Duo Zhou; Wenxuan Bao; Yunzhe Li; Gaotang Li; Cheng Qian; Yu Wang; Xiangru Tang; Yin Xiao; Liri Fang; Hui Liu; Xianfeng Tang; Yuji Zhang; Chi Wang; Jiaxuan You; Heng Ji; Hanghang Tong; Jingrui He&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Journal:&lt;/strong&gt; arXiv&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Relevance:&lt;/strong&gt; 0.795 (must_read)&lt;/p&gt;
&lt;p&gt;Reasoning is a fundamental cognitive process underlying inference, problem-solving, and decision-making. While large language models (LLMs) demonstrate strong reasoning capabilities in closed-world settings, they struggle in open-ended and dynamic environments. Agentic reasoning marks a paradigm shift by reframing LLMs as autonomous agents that plan, act, and learn through continual interaction. In this survey, we organize agentic reasoning along three complementary dimensions. First, we characterize environmental dynamics through three layers: foundational agentic reasoning, which establishes core single-agent capabilities including planning, tool use, and search in stable environments; self-evolving agentic reasoning, which studies how agents refine these capabilities through feedback, memory, and adaptation; and collective multi-agent reasoning, which extends intelligence to collaborative settings involving coordination, knowledge sharing, and shared goals. Across these layers, we distinguish in-context reasoning, which scales test-time interaction through structured orchestration, from post-training reasoning, which optimizes behaviors via reinforcement learning and supervised fine-tuning. We further review representative agentic reasoning frameworks across real-world applications and benchmarks, including science, robotics, healthcare, autonomous research, and mathematics. This survey synthesizes agentic reasoning methods into a unified roadmap bridging thought and action, and outlines open challenges and future directions, including personalization, long-horizon interaction, world modeling, scalable multi-agent training, and governance for real-world deployment.&lt;/p&gt;</content:encoded></item><item><title>Knowledge Graphs are Implicit Reward Models: Path-Derived Signals Enable Compositional Reasoning</title><link>https://arxiv.org/abs/2601.15160v1</link><guid>http://arxiv.org/abs/2601.15160v1</guid><pubDate>Wed, 21 Jan 2026 16:38:59 +0000</pubDate><dc:creator>Yuval Kansal</dc:creator><dc:creator>Niraj K. Jha</dc:creator><dc:type>preprint</dc:type><prism:publicationName>arXiv</prism:publicationName><description>Large language models have achieved near-expert performance in structured reasoning domains like mathematics and programming, yet their ability to perform compositional multi-hop reasoning in specialized scientific fields remains limited. We propose a bottom-up learning paradigm in which models are grounded in axiomatic domain facts and compose them to solve complex, unseen tasks. To this end, we present a post-training pipeline, based on a combination of supervised fine-tuning and reinforcement learning (RL), in which knowledge graphs act as implicit reward models. By deriving novel reward signals from knowledge graph paths, we provide verifiable, scalable, and grounded supervision that encourages models to compose intermediate axioms rather than optimize only final answers during RL. We validate this approach in the medical domain, training a 14B model on short-hop reasoning paths (1-3 hops) and evaluating its zero-shot generalization to complex multi-hop queries (4-5 hops). Our experiments show that path-derived rewards act as a "compositional bridge", enabling our model to significantly outperform much larger models and frontier systems like GPT-5.2 and Gemini 3 Pro, on the most difficult reasoning tasks. Furthermore, we demonstrate the robustness of our approach to adversarial perturbations against option-shuffling stress tests. This work suggests that grounding the reasoning process in structured knowledge is a scalable and efficient path toward intelligent reasoning.
Published: 2026-01-21T16:38:59+00:00
Venue: arXiv
Score: 0.794 (must_read)</description><content:encoded>&lt;p&gt;&lt;strong&gt;Authors:&lt;/strong&gt; Yuval Kansal; Niraj K. Jha&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Journal:&lt;/strong&gt; arXiv&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Relevance:&lt;/strong&gt; 0.794 (must_read)&lt;/p&gt;
&lt;p&gt;Large language models have achieved near-expert performance in structured reasoning domains like mathematics and programming, yet their ability to perform compositional multi-hop reasoning in specialized scientific fields remains limited. We propose a bottom-up learning paradigm in which models are grounded in axiomatic domain facts and compose them to solve complex, unseen tasks. To this end, we present a post-training pipeline, based on a combination of supervised fine-tuning and reinforcement learning (RL), in which knowledge graphs act as implicit reward models. By deriving novel reward signals from knowledge graph paths, we provide verifiable, scalable, and grounded supervision that encourages models to compose intermediate axioms rather than optimize only final answers during RL. We validate this approach in the medical domain, training a 14B model on short-hop reasoning paths (1-3 hops) and evaluating its zero-shot generalization to complex multi-hop queries (4-5 hops). Our experiments show that path-derived rewards act as a &amp;quot;compositional bridge&amp;quot;, enabling our model to significantly outperform much larger models and frontier systems like GPT-5.2 and Gemini 3 Pro, on the most difficult reasoning tasks. Furthermore, we demonstrate the robustness of our approach to adversarial perturbations against option-shuffling stress tests. This work suggests that grounding the reasoning process in structured knowledge is a scalable and efficient path toward intelligent reasoning.&lt;/p&gt;</content:encoded></item><item><title>What Makes Low-Bit Quantization-Aware Training Work for Reasoning LLMs? A Systematic Study</title><link>https://arxiv.org/abs/2601.14888v1</link><guid>http://arxiv.org/abs/2601.14888v1</guid><pubDate>Wed, 21 Jan 2026 11:22:29 +0000</pubDate><dc:creator>Keyu Lv</dc:creator><dc:creator>Manyi Zhang</dc:creator><dc:creator>Xiaobo Xia</dc:creator><dc:creator>Jingchen Ni</dc:creator><dc:creator>Shannan Yan</dc:creator><dc:creator>Xianzhi Yu</dc:creator><dc:creator>Lu Hou</dc:creator><dc:creator>Chun Yuan</dc:creator><dc:creator>Haoli Bai</dc:creator><dc:type>preprint</dc:type><prism:publicationName>arXiv</prism:publicationName><description>Reasoning models excel at complex tasks such as coding and mathematics, yet their inference is often slow and token-inefficient. To improve the inference efficiency, post-training quantization (PTQ) usually comes with the cost of large accuracy drops, especially for reasoning tasks under low-bit settings. In this study, we present a systematic empirical study of quantization-aware training (QAT) for reasoning models. Our key findings include: (1) Knowledge distillation is a robust objective for reasoning models trained via either supervised fine-tuning or reinforcement learning; (2) PTQ provides a strong initialization for QAT, improving accuracy while reducing training cost; (3) Reinforcement learning remains feasible for quantized models given a viable cold start and yields additional gains; and (4) Aligning the PTQ calibration domain with the QAT training domain accelerates convergence and often improves the final accuracy. Finally, we consolidate these findings into an optimized workflow (Reasoning-QAT), and show that it consistently outperforms state-of-the-art PTQ methods across multiple LLM backbones and reasoning datasets. For instance, on Qwen3-0.6B, it surpasses GPTQ by 44.53% on MATH-500 and consistently recovers performance in the 2-bit regime.
Published: 2026-01-21T11:22:29+00:00
Venue: arXiv
Score: 0.792 (must_read)</description><content:encoded>&lt;p&gt;&lt;strong&gt;Authors:&lt;/strong&gt; Keyu Lv; Manyi Zhang; Xiaobo Xia; Jingchen Ni; Shannan Yan; Xianzhi Yu; Lu Hou; Chun Yuan; Haoli Bai&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Journal:&lt;/strong&gt; arXiv&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Relevance:&lt;/strong&gt; 0.792 (must_read)&lt;/p&gt;
&lt;p&gt;Reasoning models excel at complex tasks such as coding and mathematics, yet their inference is often slow and token-inefficient. To improve the inference efficiency, post-training quantization (PTQ) usually comes with the cost of large accuracy drops, especially for reasoning tasks under low-bit settings. In this study, we present a systematic empirical study of quantization-aware training (QAT) for reasoning models. Our key findings include: (1) Knowledge distillation is a robust objective for reasoning models trained via either supervised fine-tuning or reinforcement learning; (2) PTQ provides a strong initialization for QAT, improving accuracy while reducing training cost; (3) Reinforcement learning remains feasible for quantized models given a viable cold start and yields additional gains; and (4) Aligning the PTQ calibration domain with the QAT training domain accelerates convergence and often improves the final accuracy. Finally, we consolidate these findings into an optimized workflow (Reasoning-QAT), and show that it consistently outperforms state-of-the-art PTQ methods across multiple LLM backbones and reasoning datasets. For instance, on Qwen3-0.6B, it surpasses GPTQ by 44.53% on MATH-500 and consistently recovers performance in the 2-bit regime.&lt;/p&gt;</content:encoded></item><item><title>Code-Driven Programming Prediction Enhanced by LLM with a Feature Fusion Approach</title><link>https://doi.org/10.1016/j.inffus.2026.104165</link><guid>10.1016/j.inffus.2026.104165</guid><pubDate>Tue, 20 Jan 2026 17:30:01 +0000</pubDate><dc:creator>Shengyingjie Liu</dc:creator><dc:creator>Jianxin Li</dc:creator><dc:creator>Qian Wan</dc:creator><dc:creator>Bo He</dc:creator><dc:creator>Zhijun Huang</dc:creator><dc:creator>Qing Li</dc:creator><prism:publicationName>Information Fusion</prism:publicationName><prism:doi>10.1016/j.inffus.2026.104165</prism:doi><description>Programming education is essential for equipping individuals with digital literacy skills and developing the problem-solving abilities necessary for success in the modern workforce. In online programming tutoring systems, knowledge tracing (KT) techniques are crucial for programming prediction, as they monitor user performance and model user cognition. However, both universal and programming-specific knowledge transfer methods depend on traditional state-driven paradigms that indirectly predict programming outcomes based on users’ knowledge states. It does not align with the core objective of programming prediction, which is to determine whether submitted code can solve the question. To address this, we present the code-driven feature fusion KT (CFKT), which integrates large language models (LLM) and encoders for both individualized and common code features. It consists of two modules: pass prediction and code prediction. The pass prediction module leverages LLM to incorporate semantic information from the question and code through embedding, extracting key features that determine code correctness through proxy tasks and effectively narrowing the solution space with vectorization. The code prediction module integrates user historical data and data from other users through feature fusion blocks, allowing for accurate predictions of submitted code and effectively mitigating the cold start problem. Experiments on multiple real-world public programming datasets demonstrate that CFKT significantly outperforms existing baseline methods.
Published: 2026-01-20T17:30:01+00:00
Venue: Information Fusion
Score: 0.790 (must_read)</description><content:encoded>&lt;p&gt;&lt;strong&gt;Authors:&lt;/strong&gt; Shengyingjie Liu; Jianxin Li; Qian Wan; Bo He; Zhijun Huang; Qing Li&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Journal:&lt;/strong&gt; Information Fusion&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;a href="https://doi.org/10.1016/j.inffus.2026.104165"&gt;10.1016/j.inffus.2026.104165&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Relevance:&lt;/strong&gt; 0.790 (must_read)&lt;/p&gt;
&lt;p&gt;Programming education is essential for equipping individuals with digital literacy skills and developing the problem-solving abilities necessary for success in the modern workforce. In online programming tutoring systems, knowledge tracing (KT) techniques are crucial for programming prediction, as they monitor user performance and model user cognition. However, both universal and programming-specific knowledge transfer methods depend on traditional state-driven paradigms that indirectly predict programming outcomes based on users’ knowledge states. It does not align with the core objective of programming prediction, which is to determine whether submitted code can solve the question. To address this, we present the code-driven feature fusion KT (CFKT), which integrates large language models (LLM) and encoders for both individualized and common code features. It consists of two modules: pass prediction and code prediction. The pass prediction module leverages LLM to incorporate semantic information from the question and code through embedding, extracting key features that determine code correctness through proxy tasks and effectively narrowing the solution space with vectorization. The code prediction module integrates user historical data and data from other users through feature fusion blocks, allowing for accurate predictions of submitted code and effectively mitigating the cold start problem. Experiments on multiple real-world public programming datasets demonstrate that CFKT significantly outperforms existing baseline methods.&lt;/p&gt;</content:encoded></item><item><title>A Comprehensive Evaluation of LLM Reasoning: From Single-Model to Multi-Agent Paradigms</title><link>https://arxiv.org/abs/2601.13243v1</link><guid>http://arxiv.org/abs/2601.13243v1</guid><pubDate>Mon, 19 Jan 2026 17:23:45 +0000</pubDate><dc:creator>Yapeng Li</dc:creator><dc:creator>Jiakuo Yu</dc:creator><dc:creator>Zhixin Liu</dc:creator><dc:creator>Xinnan Liu</dc:creator><dc:creator>Jing Yu</dc:creator><dc:creator>Songze Li</dc:creator><dc:creator>Tonghua Su</dc:creator><dc:type>preprint</dc:type><prism:publicationName>arXiv</prism:publicationName><description>Large Language Models (LLMs) are increasingly deployed as reasoning systems, where reasoning paradigms - such as Chain-of-Thought (CoT) and multi-agent systems (MAS) - play a critical role, yet their relative effectiveness and cost-accuracy trade-offs remain poorly understood. In this work, we conduct a comprehensive and unified evaluation of reasoning paradigms, spanning direct single-model generation, CoT-augmented single-model reasoning, and representative MAS workflows, characterizing their reasoning performance across a diverse suite of closed-form benchmarks. Beyond overall performance, we probe role-specific capability demands in MAS using targeted role isolation analyses, and analyze cost-accuracy trade-offs to identify which MAS workflows offer a favorable balance between cost and accuracy, and which incur prohibitive overhead for marginal gains. We further introduce MIMeBench, a new open-ended benchmark that targets two foundational yet underexplored semantic capabilities - semantic abstraction and contrastive discrimination - thereby providing an alternative evaluation axis beyond closed-form accuracy and enabling fine-grained assessment of semantic competence that is difficult to capture with existing benchmarks. Our results show that increased structural complexity does not consistently lead to improved reasoning performance, with its benefits being highly dependent on the properties and suitability of the reasoning paradigm itself. The codes are released at https://gitcode.com/HIT1920/OpenLLMBench.
Published: 2026-01-19T17:23:45+00:00
Venue: arXiv
Score: 0.789 (must_read)</description><content:encoded>&lt;p&gt;&lt;strong&gt;Authors:&lt;/strong&gt; Yapeng Li; Jiakuo Yu; Zhixin Liu; Xinnan Liu; Jing Yu; Songze Li; Tonghua Su&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Journal:&lt;/strong&gt; arXiv&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Relevance:&lt;/strong&gt; 0.789 (must_read)&lt;/p&gt;
&lt;p&gt;Large Language Models (LLMs) are increasingly deployed as reasoning systems, where reasoning paradigms - such as Chain-of-Thought (CoT) and multi-agent systems (MAS) - play a critical role, yet their relative effectiveness and cost-accuracy trade-offs remain poorly understood. In this work, we conduct a comprehensive and unified evaluation of reasoning paradigms, spanning direct single-model generation, CoT-augmented single-model reasoning, and representative MAS workflows, characterizing their reasoning performance across a diverse suite of closed-form benchmarks. Beyond overall performance, we probe role-specific capability demands in MAS using targeted role isolation analyses, and analyze cost-accuracy trade-offs to identify which MAS workflows offer a favorable balance between cost and accuracy, and which incur prohibitive overhead for marginal gains. We further introduce MIMeBench, a new open-ended benchmark that targets two foundational yet underexplored semantic capabilities - semantic abstraction and contrastive discrimination - thereby providing an alternative evaluation axis beyond closed-form accuracy and enabling fine-grained assessment of semantic competence that is difficult to capture with existing benchmarks. Our results show that increased structural complexity does not consistently lead to improved reasoning performance, with its benefits being highly dependent on the properties and suitability of the reasoning paradigm itself. The codes are released at https://gitcode.com/HIT1920/OpenLLMBench.&lt;/p&gt;</content:encoded></item><item><title>YOLO26: An Analysis of NMS-Free End to End Framework for Real-Time Object Detection</title><link>https://arxiv.org/abs/2601.12882v1</link><guid>http://arxiv.org/abs/2601.12882v1</guid><pubDate>Mon, 19 Jan 2026 09:36:08 +0000</pubDate><dc:creator>Sudip Chakrabarty</dc:creator><dc:type>preprint</dc:type><prism:publicationName>arXiv</prism:publicationName><description>The "You Only Look Once" (YOLO) framework has long served as the benchmark for real-time object detection, yet traditional iterations (YOLOv1 through YOLO11) remain constrained by the latency and hyperparameter sensitivity of Non-Maximum Suppression (NMS) post-processing. This paper analyzes a comprehensive analysis of YOLO26, an architecture that fundamentally redefines this paradigm by eliminating NMS in favor of a native end-to-end learning strategy. This study examines the critical innovations that enable this transition, specifically the introduction of the MuSGD optimizer for stabilizing lightweight backbones, STAL for small-target-aware assignment, and ProgLoss for dynamic supervision. Through a systematic review of official performance benchmarks, the results demonstrate that YOLO26 establishes a new Pareto front, outperforming a comprehensive suite of predecessors and state-of-the-art competitors (including RTMDet and DAMO-YOLO) in both inference speed and detection accuracy. The analysis confirms that by decoupling representation learning from heuristic post-processing, YOLOv26 successfully resolves the historical trade-off between latency and precision, signaling the next evolutionary step in edge-based computer vision.
Published: 2026-01-19T09:36:08+00:00
Venue: arXiv
Score: 0.789 (must_read)</description><content:encoded>&lt;p&gt;&lt;strong&gt;Authors:&lt;/strong&gt; Sudip Chakrabarty&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Journal:&lt;/strong&gt; arXiv&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Relevance:&lt;/strong&gt; 0.789 (must_read)&lt;/p&gt;
&lt;p&gt;The &amp;quot;You Only Look Once&amp;quot; (YOLO) framework has long served as the benchmark for real-time object detection, yet traditional iterations (YOLOv1 through YOLO11) remain constrained by the latency and hyperparameter sensitivity of Non-Maximum Suppression (NMS) post-processing. This paper analyzes a comprehensive analysis of YOLO26, an architecture that fundamentally redefines this paradigm by eliminating NMS in favor of a native end-to-end learning strategy. This study examines the critical innovations that enable this transition, specifically the introduction of the MuSGD optimizer for stabilizing lightweight backbones, STAL for small-target-aware assignment, and ProgLoss for dynamic supervision. Through a systematic review of official performance benchmarks, the results demonstrate that YOLO26 establishes a new Pareto front, outperforming a comprehensive suite of predecessors and state-of-the-art competitors (including RTMDet and DAMO-YOLO) in both inference speed and detection accuracy. The analysis confirms that by decoupling representation learning from heuristic post-processing, YOLOv26 successfully resolves the historical trade-off between latency and precision, signaling the next evolutionary step in edge-based computer vision.&lt;/p&gt;</content:encoded></item><item><title>MARO: Learning Stronger Reasoning from Social Interaction</title><link>https://arxiv.org/abs/2601.12323v1</link><guid>http://arxiv.org/abs/2601.12323v1</guid><pubDate>Sun, 18 Jan 2026 09:10:08 +0000</pubDate><dc:creator>Yin Cai</dc:creator><dc:creator>Zhouhong Gu</dc:creator><dc:creator>Juntao Zhang</dc:creator><dc:creator>Ping Chen</dc:creator><dc:type>preprint</dc:type><prism:publicationName>arXiv</prism:publicationName><description>Humans face countless scenarios that require reasoning and judgment in daily life. However, existing large language model training methods primarily allow models to learn from existing textual content or solve predetermined problems, lacking experience in real scenarios involving interaction, negotiation, and competition with others. To address this, this paper proposes Multi-Agent Reward Optimization (MARO), a method that enables large language models (LLMs) to acquire stronger reasoning abilities by learning and practicing in multi-agent social environments. Specifically, MARO first addresses the sparse learning signal problem by decomposing final success or failure outcomes into each specific behavior during the interaction process; second, it handles the uneven role distribution problem by balancing the training sample weights of different roles; finally, it addresses environmental instability issues by directly evaluating the utility of each behavior. Experimental results demonstrate that MARO not only achieves significant improvements in social reasoning capabilities, but also that the abilities acquired through social simulation learning can effectively transfer to other tasks such as mathematical reasoning and instruction following. This reveals the tremendous potential of multi-agent social learning in enhancing the general reasoning capabilities of LLMs.
Published: 2026-01-18T09:10:08+00:00
Venue: arXiv
Score: 0.789 (must_read)</description><content:encoded>&lt;p&gt;&lt;strong&gt;Authors:&lt;/strong&gt; Yin Cai; Zhouhong Gu; Juntao Zhang; Ping Chen&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Journal:&lt;/strong&gt; arXiv&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Relevance:&lt;/strong&gt; 0.789 (must_read)&lt;/p&gt;
&lt;p&gt;Humans face countless scenarios that require reasoning and judgment in daily life. However, existing large language model training methods primarily allow models to learn from existing textual content or solve predetermined problems, lacking experience in real scenarios involving interaction, negotiation, and competition with others. To address this, this paper proposes Multi-Agent Reward Optimization (MARO), a method that enables large language models (LLMs) to acquire stronger reasoning abilities by learning and practicing in multi-agent social environments. Specifically, MARO first addresses the sparse learning signal problem by decomposing final success or failure outcomes into each specific behavior during the interaction process; second, it handles the uneven role distribution problem by balancing the training sample weights of different roles; finally, it addresses environmental instability issues by directly evaluating the utility of each behavior. Experimental results demonstrate that MARO not only achieves significant improvements in social reasoning capabilities, but also that the abilities acquired through social simulation learning can effectively transfer to other tasks such as mathematical reasoning and instruction following. This reveals the tremendous potential of multi-agent social learning in enhancing the general reasoning capabilities of LLMs.&lt;/p&gt;</content:encoded></item><item><title>Practical Insights into Semi-Supervised Object Detection Approaches</title><link>https://arxiv.org/abs/2601.13380v1</link><guid>http://arxiv.org/abs/2601.13380v1</guid><pubDate>Mon, 19 Jan 2026 20:31:15 +0000</pubDate><dc:creator>Chaoxin Wang</dc:creator><dc:creator>Bharaneeshwar Balasubramaniyam</dc:creator><dc:creator>Anurag Sangem</dc:creator><dc:creator>Nicolais Guevara</dc:creator><dc:creator>Doina Caragea</dc:creator><dc:type>preprint</dc:type><prism:publicationName>arXiv</prism:publicationName><description>Learning in data-scarce settings has recently gained significant attention in the research community. Semi-supervised object detection(SSOD) aims to improve detection performance by leveraging a large number of unlabeled images alongside a limited number of labeled images(a.k.a.,few-shot learning). In this paper, we present a comprehensive comparison of three state-of-the-art SSOD approaches, including MixPL, Semi-DETR and Consistent-Teacher, with the goal of understanding how performance varies with the number of labeled images. We conduct experiments using the MS-COCO and Pascal VOC datasets, two popular object detection benchmarks which allow for standardized evaluation. In addition, we evaluate the SSOD approaches on a custom Beetle dataset which enables us to gain insights into their performance on specialized datasets with a smaller number of object categories. Our findings highlight the trade-offs between accuracy, model size, and latency, providing insights into which methods are best suited for low-data regimes.
Published: 2026-01-19T20:31:15+00:00
Venue: arXiv
Score: 0.789 (must_read)</description><content:encoded>&lt;p&gt;&lt;strong&gt;Authors:&lt;/strong&gt; Chaoxin Wang; Bharaneeshwar Balasubramaniyam; Anurag Sangem; Nicolais Guevara; Doina Caragea&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Journal:&lt;/strong&gt; arXiv&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Relevance:&lt;/strong&gt; 0.789 (must_read)&lt;/p&gt;
&lt;p&gt;Learning in data-scarce settings has recently gained significant attention in the research community. Semi-supervised object detection(SSOD) aims to improve detection performance by leveraging a large number of unlabeled images alongside a limited number of labeled images(a.k.a.,few-shot learning). In this paper, we present a comprehensive comparison of three state-of-the-art SSOD approaches, including MixPL, Semi-DETR and Consistent-Teacher, with the goal of understanding how performance varies with the number of labeled images. We conduct experiments using the MS-COCO and Pascal VOC datasets, two popular object detection benchmarks which allow for standardized evaluation. In addition, we evaluate the SSOD approaches on a custom Beetle dataset which enables us to gain insights into their performance on specialized datasets with a smaller number of object categories. Our findings highlight the trade-offs between accuracy, model size, and latency, providing insights into which methods are best suited for low-data regimes.&lt;/p&gt;</content:encoded></item><item><title>InT: Self-Proposed Interventions Enable Credit Assignment in LLM Reasoning</title><link>https://arxiv.org/abs/2601.14209v1</link><guid>http://arxiv.org/abs/2601.14209v1</guid><pubDate>Tue, 20 Jan 2026 18:15:38 +0000</pubDate><dc:creator>Matthew Y. R. Yang</dc:creator><dc:creator>Hao Bai</dc:creator><dc:creator>Ian Wu</dc:creator><dc:creator>Gene Yang</dc:creator><dc:creator>Amrith Setlur</dc:creator><dc:creator>Aviral Kumar</dc:creator><dc:type>preprint</dc:type><prism:publicationName>arXiv</prism:publicationName><description>Outcome-reward reinforcement learning (RL) has proven effective at improving the reasoning capabilities of large language models (LLMs). However, standard RL assigns credit only at the level of the final answer, penalizing entire reasoning traces when the outcome is incorrect and uniformly reinforcing all steps when it is correct. As a result, correct intermediate steps may be discouraged in failed traces, while spurious steps may be reinforced in successful ones. We refer to this failure mode as the problem of credit assignment. While a natural remedy is to train a process reward model, accurately optimizing such models to identify corrective reasoning steps remains challenging. We introduce Intervention Training (InT), a training paradigm in which the model performs fine-grained credit assignment on its own reasoning traces by proposing short, targeted corrections that steer trajectories toward higher reward. Using reference solutions commonly available in mathematical reasoning datasets and exploiting the fact that verifying a model-generated solution is easier than generating a correct one from scratch, the model identifies the first error in its reasoning and proposes a single-step intervention to redirect the trajectory toward the correct solution. We then apply supervised fine-tuning (SFT) to the on-policy rollout up to the point of error concatenated with the intervention, localizing error to the specific step that caused failure. We show that the resulting model serves as a far better initialization for RL training. After running InT and subsequent fine-tuning with RL, we improve accuracy by nearly 14% over a 4B-parameter base model on IMO-AnswerBench, outperforming larger open-source models such as gpt-oss-20b.
Published: 2026-01-20T18:15:38+00:00
Venue: arXiv
Score: 0.788 (must_read)</description><content:encoded>&lt;p&gt;&lt;strong&gt;Authors:&lt;/strong&gt; Matthew Y. R. Yang; Hao Bai; Ian Wu; Gene Yang; Amrith Setlur; Aviral Kumar&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Journal:&lt;/strong&gt; arXiv&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Relevance:&lt;/strong&gt; 0.788 (must_read)&lt;/p&gt;
&lt;p&gt;Outcome-reward reinforcement learning (RL) has proven effective at improving the reasoning capabilities of large language models (LLMs). However, standard RL assigns credit only at the level of the final answer, penalizing entire reasoning traces when the outcome is incorrect and uniformly reinforcing all steps when it is correct. As a result, correct intermediate steps may be discouraged in failed traces, while spurious steps may be reinforced in successful ones. We refer to this failure mode as the problem of credit assignment. While a natural remedy is to train a process reward model, accurately optimizing such models to identify corrective reasoning steps remains challenging. We introduce Intervention Training (InT), a training paradigm in which the model performs fine-grained credit assignment on its own reasoning traces by proposing short, targeted corrections that steer trajectories toward higher reward. Using reference solutions commonly available in mathematical reasoning datasets and exploiting the fact that verifying a model-generated solution is easier than generating a correct one from scratch, the model identifies the first error in its reasoning and proposes a single-step intervention to redirect the trajectory toward the correct solution. We then apply supervised fine-tuning (SFT) to the on-policy rollout up to the point of error concatenated with the intervention, localizing error to the specific step that caused failure. We show that the resulting model serves as a far better initialization for RL training. After running InT and subsequent fine-tuning with RL, we improve accuracy by nearly 14% over a 4B-parameter base model on IMO-AnswerBench, outperforming larger open-source models such as gpt-oss-20b.&lt;/p&gt;</content:encoded></item><item><title>SuperMapNet for long-range and high-accuracy vectorized HD map construction</title><link>https://doi.org/10.1016/j.isprsjprs.2026.01.023</link><guid>10.1016/j.isprsjprs.2026.01.023</guid><pubDate>Tue, 20 Jan 2026 16:22:22 +0000</pubDate><dc:creator>Ruqin Zhou</dc:creator><dc:creator>Chenguang Dai</dc:creator><dc:creator>Wanshou Jiang</dc:creator><dc:creator>Yongsheng Zhang</dc:creator><dc:creator>Zhenchao Zhang</dc:creator><dc:creator>San Jiang</dc:creator><prism:publicationName>ISPRS Journal of Photogrammetry and Remote Sensing</prism:publicationName><prism:doi>10.1016/j.isprsjprs.2026.01.023</prism:doi><description>Vectorized high-definition (HD) map construction is formulated as the task of classifying and localizing typical map elements based on features in a bird’s-eye view (BEV). This is essential for autonomous driving systems, providing interpretable environmental structured representations for decision and planning. Remarkable work has been achieved in recent years, but several major issues remain: (1) in the generation of the BEV features, single modality methods suffer from limited perception capability and range, while existing multi-modal fusion approaches underutilize cross-modal synergies and fail to resolve spatial disparities between modalities, resulting in misaligned BEV features with holes; (2) in the classification and localization of map elements, existing methods heavily rely on point-level modeling information while neglecting the information between elements and between point and element, leading to low accuracy with erroneous shapes and element entanglement. To address these limitations, we propose SuperMapNet, a multi-modal framework designed for long-range and high-accuracy vectorized HD map construction. This framework uses both camera images and LiDAR point clouds as input. It first tightly couples semantic information from camera images and geometric information from LiDAR point clouds by a cross-attention based synergy enhancement module and a flow-based disparity alignment module for long-range BEV feature generation. Subsequently, local information acquired by point queries and global information acquired by element queries are tightly coupled by three-level interactions for high-accuracy classification and localization, where Point2Point interaction captures local geometric consistency between points of the same element, Element2Element interaction learns global semantic relationships between elements, and Point2Element interaction complement element information for its constituent points. Experiments on the nuScenes and Argoverse2 datasets demonstrate high accuracy, surpassing previous state-of-the-art methods (SOTAs) by 14.9%/8.8% and 18.5%/3.1% mAP under the hard/easy settings, respectively, even over the double perception ranges (up to 120 m " role="presentation"&gt; m m in the X-axis and 60 m " role="presentation"&gt; m m in the Y-axis). The code is made publicly available at https://github.com/zhouruqin/SuperMapNet .
Published: 2026-01-20T16:22:22+00:00
Venue: ISPRS Journal of Photogrammetry and Remote Sensing
Score: 0.788 (must_read)</description><content:encoded>&lt;p&gt;&lt;strong&gt;Authors:&lt;/strong&gt; Ruqin Zhou; Chenguang Dai; Wanshou Jiang; Yongsheng Zhang; Zhenchao Zhang; San Jiang&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Journal:&lt;/strong&gt; ISPRS Journal of Photogrammetry and Remote Sensing&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;a href="https://doi.org/10.1016/j.isprsjprs.2026.01.023"&gt;10.1016/j.isprsjprs.2026.01.023&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Relevance:&lt;/strong&gt; 0.788 (must_read)&lt;/p&gt;
&lt;p&gt;Vectorized high-definition (HD) map construction is formulated as the task of classifying and localizing typical map elements based on features in a bird’s-eye view (BEV). This is essential for autonomous driving systems, providing interpretable environmental structured representations for decision and planning. Remarkable work has been achieved in recent years, but several major issues remain: (1) in the generation of the BEV features, single modality methods suffer from limited perception capability and range, while existing multi-modal fusion approaches underutilize cross-modal synergies and fail to resolve spatial disparities between modalities, resulting in misaligned BEV features with holes; (2) in the classification and localization of map elements, existing methods heavily rely on point-level modeling information while neglecting the information between elements and between point and element, leading to low accuracy with erroneous shapes and element entanglement. To address these limitations, we propose SuperMapNet, a multi-modal framework designed for long-range and high-accuracy vectorized HD map construction. This framework uses both camera images and LiDAR point clouds as input. It first tightly couples semantic information from camera images and geometric information from LiDAR point clouds by a cross-attention based synergy enhancement module and a flow-based disparity alignment module for long-range BEV feature generation. Subsequently, local information acquired by point queries and global information acquired by element queries are tightly coupled by three-level interactions for high-accuracy classification and localization, where Point2Point interaction captures local geometric consistency between points of the same element, Element2Element interaction learns global semantic relationships between elements, and Point2Element interaction complement element information for its constituent points. Experiments on the nuScenes and Argoverse2 datasets demonstrate high accuracy, surpassing previous state-of-the-art methods (SOTAs) by 14.9%/8.8% and 18.5%/3.1% mAP under the hard/easy settings, respectively, even over the double perception ranges (up to 120 m &amp;quot; role=&amp;quot;presentation&amp;quot;&amp;gt; m m in the X-axis and 60 m &amp;quot; role=&amp;quot;presentation&amp;quot;&amp;gt; m m in the Y-axis). The code is made publicly available at https://github.com/zhouruqin/SuperMapNet .&lt;/p&gt;</content:encoded></item><item><title>AEMC: Aggregator-efficient model compression for streamlining LLMs</title><link>https://doi.org/10.1016/j.neucom.2026.132779</link><guid>10.1016/j.neucom.2026.132779</guid><pubDate>Tue, 20 Jan 2026 07:42:26 +0000</pubDate><dc:creator>Mingrui Zhou</dc:creator><dc:creator>Rui Mao</dc:creator><dc:creator>Somayajulu Sripada</dc:creator><dc:creator>Xiao Li</dc:creator><prism:publicationName>Neurocomputing</prism:publicationName><prism:doi>10.1016/j.neucom.2026.132779</prism:doi><description>Large language models (LLMs) are increasingly used in practical applications via simple prompt-based interfaces, but their high inference cost limits deployment. Structured pruning offers a promising solution, yet most effective methods require post-training, which poses barriers in real-world scenarios. We identify amplitude mismatch, a hidden state norm discrepancy caused by residual connections, as a key reason for pruning-induced degradation. To address this, we propose Aggregator-Efficient Model Compression (AEMC), a training-free pruning framework that inserts lightweight Aggregator layers to restore residual amplitude and preserve information flow. Without any fine-tuning, AEMC consistently outperforms all existing training-free pruning methods and achieves state-of-the-art performance even compared to post-trained baselines. AEMC offers a practical path toward efficient and scalable LLM deployment in resource-constrained settings.
Published: 2026-01-20T07:42:26+00:00
Venue: Neurocomputing
Score: 0.787 (must_read)</description><content:encoded>&lt;p&gt;&lt;strong&gt;Authors:&lt;/strong&gt; Mingrui Zhou; Rui Mao; Somayajulu Sripada; Xiao Li&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Journal:&lt;/strong&gt; Neurocomputing&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;a href="https://doi.org/10.1016/j.neucom.2026.132779"&gt;10.1016/j.neucom.2026.132779&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Relevance:&lt;/strong&gt; 0.787 (must_read)&lt;/p&gt;
&lt;p&gt;Large language models (LLMs) are increasingly used in practical applications via simple prompt-based interfaces, but their high inference cost limits deployment. Structured pruning offers a promising solution, yet most effective methods require post-training, which poses barriers in real-world scenarios. We identify amplitude mismatch, a hidden state norm discrepancy caused by residual connections, as a key reason for pruning-induced degradation. To address this, we propose Aggregator-Efficient Model Compression (AEMC), a training-free pruning framework that inserts lightweight Aggregator layers to restore residual amplitude and preserve information flow. Without any fine-tuning, AEMC consistently outperforms all existing training-free pruning methods and achieves state-of-the-art performance even compared to post-trained baselines. AEMC offers a practical path toward efficient and scalable LLM deployment in resource-constrained settings.&lt;/p&gt;</content:encoded></item><item><title>LLMOrbit: A Circular Taxonomy of Large Language Models -From Scaling Walls to Agentic AI Systems</title><link>https://arxiv.org/abs/2601.14053v1</link><guid>http://arxiv.org/abs/2601.14053v1</guid><pubDate>Tue, 20 Jan 2026 15:06:19 +0000</pubDate><dc:creator>Badri N. Patro</dc:creator><dc:creator>Vijay S. Agneeswaran</dc:creator><dc:type>preprint</dc:type><prism:publicationName>arXiv</prism:publicationName><description>The field of artificial intelligence has undergone a revolution from foundational Transformer architectures to reasoning-capable systems approaching human-level performance. We present LLMOrbit, a comprehensive circular taxonomy navigating the landscape of large language models spanning 2019-2025. This survey examines over 50 models across 15 organizations through eight interconnected orbital dimensions, documenting architectural innovations, training methodologies, and efficiency patterns defining modern LLMs, generative AI, and agentic systems. We identify three critical crises: (1) data scarcity (9-27T tokens depleted by 2026-2028), (2) exponential cost growth ($3M to $300M+ in 5 years), and (3) unsustainable energy consumption (22x increase), establishing the scaling wall limiting brute-force approaches. Our analysis reveals six paradigms breaking this wall: (1) test-time compute (o1, DeepSeek-R1 achieve GPT-4 performance with 10x inference compute), (2) quantization (4-8x compression), (3) distributed edge computing (10x cost reduction), (4) model merging, (5) efficient training (ORPO reduces memory 50%), and (6) small specialized models (Phi-4 14B matches larger models). Three paradigm shifts emerge: (1) post-training gains (RLHF, GRPO, pure RL contribute substantially, DeepSeek-R1 achieving 79.8% MATH), (2) efficiency revolution (MoE routing 18x efficiency, Multi-head Latent Attention 8x KV cache compression enables GPT-4-level performance at &lt;$0.30/M tokens), and (3) democratization (open-source Llama 3 88.6% MMLU surpasses GPT-4 86.4%). We provide insights into techniques (RLHF, PPO, DPO, GRPO, ORPO), trace evolution from passive generation to tool-using agents (ReAct, RAG, multi-agent systems), and analyze post-training innovations.
Published: 2026-01-20T15:06:19+00:00
Venue: arXiv
Score: 0.787 (must_read)</description><content:encoded>&lt;p&gt;&lt;strong&gt;Authors:&lt;/strong&gt; Badri N. Patro; Vijay S. Agneeswaran&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Journal:&lt;/strong&gt; arXiv&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Relevance:&lt;/strong&gt; 0.787 (must_read)&lt;/p&gt;
&lt;p&gt;The field of artificial intelligence has undergone a revolution from foundational Transformer architectures to reasoning-capable systems approaching human-level performance. We present LLMOrbit, a comprehensive circular taxonomy navigating the landscape of large language models spanning 2019-2025. This survey examines over 50 models across 15 organizations through eight interconnected orbital dimensions, documenting architectural innovations, training methodologies, and efficiency patterns defining modern LLMs, generative AI, and agentic systems. We identify three critical crises: (1) data scarcity (9-27T tokens depleted by 2026-2028), (2) exponential cost growth ($3M to $300M+ in 5 years), and (3) unsustainable energy consumption (22x increase), establishing the scaling wall limiting brute-force approaches. Our analysis reveals six paradigms breaking this wall: (1) test-time compute (o1, DeepSeek-R1 achieve GPT-4 performance with 10x inference compute), (2) quantization (4-8x compression), (3) distributed edge computing (10x cost reduction), (4) model merging, (5) efficient training (ORPO reduces memory 50%), and (6) small specialized models (Phi-4 14B matches larger models). Three paradigm shifts emerge: (1) post-training gains (RLHF, GRPO, pure RL contribute substantially, DeepSeek-R1 achieving 79.8% MATH), (2) efficiency revolution (MoE routing 18x efficiency, Multi-head Latent Attention 8x KV cache compression enables GPT-4-level performance at &amp;lt;$0.30/M tokens), and (3) democratization (open-source Llama 3 88.6% MMLU surpasses GPT-4 86.4%). We provide insights into techniques (RLHF, PPO, DPO, GRPO, ORPO), trace evolution from passive generation to tool-using agents (ReAct, RAG, multi-agent systems), and analyze post-training innovations.&lt;/p&gt;</content:encoded></item><item><title>RayRoPE: Projective Ray Positional Encoding for Multi-view Attention</title><link>https://arxiv.org/abs/2601.15275v1</link><guid>http://arxiv.org/abs/2601.15275v1</guid><pubDate>Wed, 21 Jan 2026 18:55:51 +0000</pubDate><dc:creator>Yu Wu</dc:creator><dc:creator>Minsik Jeon</dc:creator><dc:creator>Jen-Hao Rick Chang</dc:creator><dc:creator>Oncel Tuzel</dc:creator><dc:creator>Shubham Tulsiani</dc:creator><dc:type>preprint</dc:type><prism:publicationName>arXiv</prism:publicationName><description>We study positional encodings for multi-view transformers that process tokens from a set of posed input images, and seek a mechanism that encodes patches uniquely, allows SE(3)-invariant attention with multi-frequency similarity, and can be adaptive to the geometry of the underlying scene. We find that prior (absolute or relative) encoding schemes for multi-view attention do not meet the above desiderata, and present RayRoPE to address this gap. RayRoPE represents patch positions based on associated rays but leverages a predicted point along the ray instead of the direction for a geometry-aware encoding. To achieve SE(3) invariance, RayRoPE computes query-frame projective coordinates for computing multi-frequency similarity. Lastly, as the 'predicted' 3D point along a ray may not be precise, RayRoPE presents a mechanism to analytically compute the expected position encoding under uncertainty. We validate RayRoPE on the tasks of novel-view synthesis and stereo depth estimation and show that it consistently improves over alternate position encoding schemes (e.g. 15% relative improvement on LPIPS in CO3D). We also show that RayRoPE can seamlessly incorporate RGB-D input, resulting in even larger gains over alternatives that cannot positionally encode this information.
Published: 2026-01-21T18:55:51+00:00
Venue: arXiv
Score: 0.786 (must_read)</description><content:encoded>&lt;p&gt;&lt;strong&gt;Authors:&lt;/strong&gt; Yu Wu; Minsik Jeon; Jen-Hao Rick Chang; Oncel Tuzel; Shubham Tulsiani&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Journal:&lt;/strong&gt; arXiv&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Relevance:&lt;/strong&gt; 0.786 (must_read)&lt;/p&gt;
&lt;p&gt;We study positional encodings for multi-view transformers that process tokens from a set of posed input images, and seek a mechanism that encodes patches uniquely, allows SE(3)-invariant attention with multi-frequency similarity, and can be adaptive to the geometry of the underlying scene. We find that prior (absolute or relative) encoding schemes for multi-view attention do not meet the above desiderata, and present RayRoPE to address this gap. RayRoPE represents patch positions based on associated rays but leverages a predicted point along the ray instead of the direction for a geometry-aware encoding. To achieve SE(3) invariance, RayRoPE computes query-frame projective coordinates for computing multi-frequency similarity. Lastly, as the &amp;#x27;predicted&amp;#x27; 3D point along a ray may not be precise, RayRoPE presents a mechanism to analytically compute the expected position encoding under uncertainty. We validate RayRoPE on the tasks of novel-view synthesis and stereo depth estimation and show that it consistently improves over alternate position encoding schemes (e.g. 15% relative improvement on LPIPS in CO3D). We also show that RayRoPE can seamlessly incorporate RGB-D input, resulting in even larger gains over alternatives that cannot positionally encode this information.&lt;/p&gt;</content:encoded></item><item><title>UrbanMFM: Spatial Graph-Based Multiscale Foundation Models for Learning Generalized Urban Representation</title><link>https://doi.org/10.1109/tkde.2026.3656202</link><guid>10.1109/tkde.2026.3656202</guid><pubDate>Tue, 20 Jan 2026 20:41:02 +0000</pubDate><dc:creator>Zhaoqi Zhang</dc:creator><dc:creator>Miao Xie</dc:creator><dc:creator>Pasquale Balsebre</dc:creator><dc:creator>Weiming Huang</dc:creator><dc:creator>Siqiang Luo</dc:creator><dc:creator>Gao Cong</dc:creator><prism:publicationName>IEEE Transactions on Knowledge and Data Engineering</prism:publicationName><prism:doi>10.1109/tkde.2026.3656202</prism:doi><description>As geospatial data from web platforms becomes increasingly accessible and regularly updated, urban representation learning has emerged as a critical research area for advancing urban planning. Recent studies have developed foundation model-based algorithms to leverage this data for various urban-related downstream tasks. However, current research has inadequately explored deep integration strategies for multiscale, multimodal urban data in the context of urban foundation models. This gap arises primarily because the relationships between micro-scale (e.g., individual points of interest and street view imagery) and macro-scale (e.g., region-wide satellite imagery) urban features are inherently implicit and highly complex, making traditional interaction modeling insufficient. This paper introduces a novel research problem – how to learn multiscale urban representations by integrating diverse geographic data modalities and modeling complex multimodal relationships across different spatial scales. To address this significant challenge, we propose UrbanMFM, a spatial graph-based multiscale foundation model framework explicitly designed to capture and leverage these intricate relationships. UrbanMFM utilizes a self-supervised learning paradigm that integrates diverse geographic data modalities, including POI data and urban imagery, through novel contrastive learning objectives and advanced sampling techniques. By explicitly modeling spatial graphs to represent complex multiscale urban relationships, UrbanMFM effectively facilitates deep interactions between multimodal data sources. Extensive experiments on datasets from Singapore, New York, and Beijing demonstrate that UrbanMFM outperforms the strongest baselines significantly in four representative downstream tasks. By effectively modelling spatial hierarchies with diverse data, UrbanMFM provides a more comprehensive and adaptable representation of urban environments.
Published: 2026-01-20T20:41:02+00:00
Venue: IEEE Transactions on Knowledge and Data Engineering
Score: 0.786 (must_read)</description><content:encoded>&lt;p&gt;&lt;strong&gt;Authors:&lt;/strong&gt; Zhaoqi Zhang; Miao Xie; Pasquale Balsebre; Weiming Huang; Siqiang Luo; Gao Cong&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Journal:&lt;/strong&gt; IEEE Transactions on Knowledge and Data Engineering&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;a href="https://doi.org/10.1109/tkde.2026.3656202"&gt;10.1109/tkde.2026.3656202&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Relevance:&lt;/strong&gt; 0.786 (must_read)&lt;/p&gt;
&lt;p&gt;As geospatial data from web platforms becomes increasingly accessible and regularly updated, urban representation learning has emerged as a critical research area for advancing urban planning. Recent studies have developed foundation model-based algorithms to leverage this data for various urban-related downstream tasks. However, current research has inadequately explored deep integration strategies for multiscale, multimodal urban data in the context of urban foundation models. This gap arises primarily because the relationships between micro-scale (e.g., individual points of interest and street view imagery) and macro-scale (e.g., region-wide satellite imagery) urban features are inherently implicit and highly complex, making traditional interaction modeling insufficient. This paper introduces a novel research problem – how to learn multiscale urban representations by integrating diverse geographic data modalities and modeling complex multimodal relationships across different spatial scales. To address this significant challenge, we propose UrbanMFM, a spatial graph-based multiscale foundation model framework explicitly designed to capture and leverage these intricate relationships. UrbanMFM utilizes a self-supervised learning paradigm that integrates diverse geographic data modalities, including POI data and urban imagery, through novel contrastive learning objectives and advanced sampling techniques. By explicitly modeling spatial graphs to represent complex multiscale urban relationships, UrbanMFM effectively facilitates deep interactions between multimodal data sources. Extensive experiments on datasets from Singapore, New York, and Beijing demonstrate that UrbanMFM outperforms the strongest baselines significantly in four representative downstream tasks. By effectively modelling spatial hierarchies with diverse data, UrbanMFM provides a more comprehensive and adaptable representation of urban environments.&lt;/p&gt;</content:encoded></item><item><title>Finding RELIEF: Shaping Reasoning Behavior without Reasoning Supervision via Belief Engineering</title><link>https://arxiv.org/abs/2601.13752v1</link><guid>http://arxiv.org/abs/2601.13752v1</guid><pubDate>Tue, 20 Jan 2026 09:07:01 +0000</pubDate><dc:creator>Chak Tou Leong</dc:creator><dc:creator>Dingwei Chen</dc:creator><dc:creator>Heming Xia</dc:creator><dc:creator>Qingyu Yin</dc:creator><dc:creator>Sunbowen Lee</dc:creator><dc:creator>Jian Wang</dc:creator><dc:creator>Wenjie Li</dc:creator><dc:type>preprint</dc:type><prism:publicationName>arXiv</prism:publicationName><description>Large reasoning models (LRMs) have achieved remarkable success in complex problem-solving, yet they often suffer from computational redundancy or reasoning unfaithfulness. Current methods for shaping LRM behavior typically rely on reinforcement learning or fine-tuning with gold-standard reasoning traces, a paradigm that is both computationally expensive and difficult to scale. In this paper, we reveal that LRMs possess latent \textit{reasoning beliefs} that internally track their own reasoning traits, which can be captured through simple logit probing. Building upon this insight, we propose Reasoning Belief Engineering (RELIEF), a simple yet effective framework that shapes LRM behavior by aligning the model's self-concept with a target belief blueprint. Crucially, RELIEF completely bypasses the need for reasoning-trace supervision. It internalizes desired traits by fine-tuning on synthesized, self-reflective question-answering pairs that affirm the target belief. Extensive experiments on efficiency and faithfulness tasks demonstrate that RELIEF matches or outperforms behavior-supervised and preference-based baselines while requiring lower training costs. Further analysis validates that shifting a model's reasoning belief effectively shapes its actual behavior.
Published: 2026-01-20T09:07:01+00:00
Venue: arXiv
Score: 0.785 (must_read)</description><content:encoded>&lt;p&gt;&lt;strong&gt;Authors:&lt;/strong&gt; Chak Tou Leong; Dingwei Chen; Heming Xia; Qingyu Yin; Sunbowen Lee; Jian Wang; Wenjie Li&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Journal:&lt;/strong&gt; arXiv&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Relevance:&lt;/strong&gt; 0.785 (must_read)&lt;/p&gt;
&lt;p&gt;Large reasoning models (LRMs) have achieved remarkable success in complex problem-solving, yet they often suffer from computational redundancy or reasoning unfaithfulness. Current methods for shaping LRM behavior typically rely on reinforcement learning or fine-tuning with gold-standard reasoning traces, a paradigm that is both computationally expensive and difficult to scale. In this paper, we reveal that LRMs possess latent \textit{reasoning beliefs} that internally track their own reasoning traits, which can be captured through simple logit probing. Building upon this insight, we propose Reasoning Belief Engineering (RELIEF), a simple yet effective framework that shapes LRM behavior by aligning the model&amp;#x27;s self-concept with a target belief blueprint. Crucially, RELIEF completely bypasses the need for reasoning-trace supervision. It internalizes desired traits by fine-tuning on synthesized, self-reflective question-answering pairs that affirm the target belief. Extensive experiments on efficiency and faithfulness tasks demonstrate that RELIEF matches or outperforms behavior-supervised and preference-based baselines while requiring lower training costs. Further analysis validates that shifting a model&amp;#x27;s reasoning belief effectively shapes its actual behavior.&lt;/p&gt;</content:encoded></item><item><title>Weakly Supervised Salient Object Detection with Text Supervision</title><link>https://doi.org/10.1007/s11263-025-02728-5</link><guid>10.1007/s11263-025-02728-5</guid><pubDate>Tue, 20 Jan 2026 09:30:03 +0000</pubDate><dc:creator>Zhihao Wu</dc:creator><dc:creator>Jie Wen</dc:creator><dc:creator>Linlin Shen</dc:creator><dc:creator>Xiaopeng Fan</dc:creator><dc:creator>Yong Xu</dc:creator><dc:creator>Jian Yang</dc:creator><dc:creator>David Zhang</dc:creator><prism:publicationName>International Journal of Computer Vision</prism:publicationName><prism:doi>10.1007/s11263-025-02728-5</prism:doi><description>Weakly supervised salient object detection using image-category supervision offers a cost-effective alternative to dense annotations, yet suffers from significant performance degradation. This is primarily attributed to the limitations of existing pseudo-label generation methods, which tend to either under- or over-activate object regions and indiscriminately label all non-activated pixels as background, introducing considerable label noise. Furthermore, these methods are restricted in the ability to capture objects beyond the pre-trained category set. To overcome these challenges, we propose a CLIP-based pseudo-label generation that exploits text prompts to jointly activate generic background and salient objects, breaking the dependency on specific categories. However, we find that this paradigm faces three challenges: optimal prompt uncertainty, background redundancy, and object-background conflict. To mitigate these, we propose three key modules. First, spatial distribution-guided prompt selection evaluates the spatial distribution of activation regions to identify the optimal prompt. Second, center and scale prior-guided activation refinement integrates self-attention and superpixel cues to suppress background noise. Third, learning feedback-guided pseudo-label update learns saliency knowledge from other pseudo-labels to resolve conflicting regions and iteratively refine supervision. Extensive experiments demonstrate that our method surpasses previous weakly supervised methods with image-category supervision and unsupervised approaches.
Published: 2026-01-20T09:30:03+00:00
Venue: International Journal of Computer Vision
Score: 0.785 (must_read)</description><content:encoded>&lt;p&gt;&lt;strong&gt;Authors:&lt;/strong&gt; Zhihao Wu; Jie Wen; Linlin Shen; Xiaopeng Fan; Yong Xu; Jian Yang; David Zhang&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Journal:&lt;/strong&gt; International Journal of Computer Vision&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;a href="https://doi.org/10.1007/s11263-025-02728-5"&gt;10.1007/s11263-025-02728-5&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Relevance:&lt;/strong&gt; 0.785 (must_read)&lt;/p&gt;
&lt;p&gt;Weakly supervised salient object detection using image-category supervision offers a cost-effective alternative to dense annotations, yet suffers from significant performance degradation. This is primarily attributed to the limitations of existing pseudo-label generation methods, which tend to either under- or over-activate object regions and indiscriminately label all non-activated pixels as background, introducing considerable label noise. Furthermore, these methods are restricted in the ability to capture objects beyond the pre-trained category set. To overcome these challenges, we propose a CLIP-based pseudo-label generation that exploits text prompts to jointly activate generic background and salient objects, breaking the dependency on specific categories. However, we find that this paradigm faces three challenges: optimal prompt uncertainty, background redundancy, and object-background conflict. To mitigate these, we propose three key modules. First, spatial distribution-guided prompt selection evaluates the spatial distribution of activation regions to identify the optimal prompt. Second, center and scale prior-guided activation refinement integrates self-attention and superpixel cues to suppress background noise. Third, learning feedback-guided pseudo-label update learns saliency knowledge from other pseudo-labels to resolve conflicting regions and iteratively refine supervision. Extensive experiments demonstrate that our method surpasses previous weakly supervised methods with image-category supervision and unsupervised approaches.&lt;/p&gt;</content:encoded></item></channel></rss>