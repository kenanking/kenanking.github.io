<?xml version='1.0' encoding='utf-8'?>
<rss xmlns:content="http://purl.org/rss/1.0/modules/content/" xmlns:dc="http://purl.org/dc/elements/1.1/" xmlns:prism="http://prismstandard.org/namespaces/basic/3.0/" version="2.0"><channel><title>ZotWatch Feed</title><link>https://ehehe.cn/zotwatch/</link><description>AI-assisted literature watch</description><lastBuildDate>Sat, 31 Jan 2026 03:22:37 +0000</lastBuildDate><item><title>Diffusion Models and Representation Learning: A Survey</title><link>https://doi.org/10.1109/tpami.2026.3658965</link><guid>10.1109/tpami.2026.3658965</guid><pubDate>Thu, 29 Jan 2026 21:23:36 +0000</pubDate><dc:creator>Michael Fuest</dc:creator><dc:creator>Pingchuan Ma</dc:creator><dc:creator>Ming Gui</dc:creator><dc:creator>Johannes Schusterbauer</dc:creator><dc:creator>Vincent Tao Hu</dc:creator><dc:creator>Björn Ommer</dc:creator><prism:publicationName>IEEE Transactions on Pattern Analysis and Machine Intelligence</prism:publicationName><prism:doi>10.1109/tpami.2026.3658965</prism:doi><description>Diffusion Models are popular generative modeling methods in various vision tasks, attracting significant attention. They can be considered a unique instance of self-supervised learning methods due to their independence from label annotation. This survey explores the interplay between diffusion models and representation learning. It provides an overview of diffusion models' essential aspects, including mathematical foundations, popular denoising network architectures, and guidance methods. Various approaches related to diffusion models and representation learning are detailed. These include frameworks that leverage representations learned from pre-trained diffusion models for subsequent recognition tasks and methods that utilize advancements in representation and self-supervised learning to enhance diffusion models. This survey aims to offer a comprehensive overview of the taxonomy between diffusion models and representation learning, identifying key areas of existing concerns and potential exploration. Github link: https://github.com/dongzhuoyao/Diffusion-Representation-Learning-Survey-Taxonomy.
Published: 2026-01-29T21:23:36+00:00
Venue: IEEE Transactions on Pattern Analysis and Machine Intelligence
Score: 0.834 (must_read)</description><content:encoded>&lt;p&gt;&lt;strong&gt;Authors:&lt;/strong&gt; Michael Fuest; Pingchuan Ma; Ming Gui; Johannes Schusterbauer; Vincent Tao Hu; Björn Ommer&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Journal:&lt;/strong&gt; IEEE Transactions on Pattern Analysis and Machine Intelligence&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;a href="https://doi.org/10.1109/tpami.2026.3658965"&gt;10.1109/tpami.2026.3658965&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Relevance:&lt;/strong&gt; 0.834 (must_read)&lt;/p&gt;
&lt;p&gt;Diffusion Models are popular generative modeling methods in various vision tasks, attracting significant attention. They can be considered a unique instance of self-supervised learning methods due to their independence from label annotation. This survey explores the interplay between diffusion models and representation learning. It provides an overview of diffusion models&amp;#x27; essential aspects, including mathematical foundations, popular denoising network architectures, and guidance methods. Various approaches related to diffusion models and representation learning are detailed. These include frameworks that leverage representations learned from pre-trained diffusion models for subsequent recognition tasks and methods that utilize advancements in representation and self-supervised learning to enhance diffusion models. This survey aims to offer a comprehensive overview of the taxonomy between diffusion models and representation learning, identifying key areas of existing concerns and potential exploration. Github link: https://github.com/dongzhuoyao/Diffusion-Representation-Learning-Survey-Taxonomy.&lt;/p&gt;</content:encoded></item><item><title>ZUMA: Training-free Zero-shot Unified Multimodal Anomaly Detection</title><link>https://doi.org/10.1109/tpami.2026.3658856</link><guid>10.1109/tpami.2026.3658856</guid><pubDate>Thu, 29 Jan 2026 21:23:36 +0000</pubDate><dc:creator>Yunfeng Ma</dc:creator><dc:creator>Min Liu</dc:creator><dc:creator>Shuai Jiang</dc:creator><dc:creator>Jingyu Zhou</dc:creator><dc:creator>Yuan Bian</dc:creator><dc:creator>Xueping Wang</dc:creator><dc:creator>Yaonan Wang</dc:creator><prism:publicationName>IEEE Transactions on Pattern Analysis and Machine Intelligence</prism:publicationName><prism:doi>10.1109/tpami.2026.3658856</prism:doi><description>Multimodal anomaly detection (MAD) aims to exploit both texture and spatial attributes to identify deviations from normal patterns in complex scenarios. However, zero-shot (ZS) settings arising from privacy concerns or confidentiality constraints present significant challenges to existing MAD methods. To address this issue, we introduce ZUMA, a training-free, Zero-shot Unified Multimodal Anomaly detection framework that unleashes CLIP's cross-modal potential to perform ZS MAD. To mitigate the domain gap between CLIP's pretraining space and point clouds, we propose cross-domain calibration (CDC), which efficiently bridges the manifold misalignment through source-domain semantic transfer and establishes a hybrid semantic space, enabling a joint embedding of 2D and 3D representations. Subsequently, ZUMA performs dynamic semantic interaction (DSI) to enable structural decoupling of anomaly regions in the high-dimensional embedding space constructed by CDC, where natural languages serve as semantic anchors to help DSI establish discriminative hyperplanes within hybrid modality representations. Within this framework, ZUMA enables plug-and-play detection of 2D, 3D or multimodal anomalies, without training or fine-tuning even for cross-dataset or incomplete-modality scenarios. Additionally, to further investigate the potential of the training-free ZUMA within the training-based paradigm, we develop ZUMA-FT, a fine-tuned variant that achieves notable improvements with minimal parameter trade-off. Extensive experiments are conducted on two MAD benchmarks, MVTec 3D-AD and Eyecandies. Notably, the training-free ZUMA achieves state-of-the-art (SOTA) performance on both datasets, outperforming existing ZS MAD methods, including training-based approaches. Moreover, ZUMA-FT further extends the performance boundary of ZUMA with only 6.75 M learnable parameters. Code is available at: https://github.com/yif-ma/ZUMA
Published: 2026-01-29T21:23:36+00:00
Venue: IEEE Transactions on Pattern Analysis and Machine Intelligence
Score: 0.813 (must_read)</description><content:encoded>&lt;p&gt;&lt;strong&gt;Authors:&lt;/strong&gt; Yunfeng Ma; Min Liu; Shuai Jiang; Jingyu Zhou; Yuan Bian; Xueping Wang; Yaonan Wang&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Journal:&lt;/strong&gt; IEEE Transactions on Pattern Analysis and Machine Intelligence&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;a href="https://doi.org/10.1109/tpami.2026.3658856"&gt;10.1109/tpami.2026.3658856&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Relevance:&lt;/strong&gt; 0.813 (must_read)&lt;/p&gt;
&lt;p&gt;Multimodal anomaly detection (MAD) aims to exploit both texture and spatial attributes to identify deviations from normal patterns in complex scenarios. However, zero-shot (ZS) settings arising from privacy concerns or confidentiality constraints present significant challenges to existing MAD methods. To address this issue, we introduce ZUMA, a training-free, Zero-shot Unified Multimodal Anomaly detection framework that unleashes CLIP&amp;#x27;s cross-modal potential to perform ZS MAD. To mitigate the domain gap between CLIP&amp;#x27;s pretraining space and point clouds, we propose cross-domain calibration (CDC), which efficiently bridges the manifold misalignment through source-domain semantic transfer and establishes a hybrid semantic space, enabling a joint embedding of 2D and 3D representations. Subsequently, ZUMA performs dynamic semantic interaction (DSI) to enable structural decoupling of anomaly regions in the high-dimensional embedding space constructed by CDC, where natural languages serve as semantic anchors to help DSI establish discriminative hyperplanes within hybrid modality representations. Within this framework, ZUMA enables plug-and-play detection of 2D, 3D or multimodal anomalies, without training or fine-tuning even for cross-dataset or incomplete-modality scenarios. Additionally, to further investigate the potential of the training-free ZUMA within the training-based paradigm, we develop ZUMA-FT, a fine-tuned variant that achieves notable improvements with minimal parameter trade-off. Extensive experiments are conducted on two MAD benchmarks, MVTec 3D-AD and Eyecandies. Notably, the training-free ZUMA achieves state-of-the-art (SOTA) performance on both datasets, outperforming existing ZS MAD methods, including training-based approaches. Moreover, ZUMA-FT further extends the performance boundary of ZUMA with only 6.75 M learnable parameters. Code is available at: https://github.com/yif-ma/ZUMA&lt;/p&gt;</content:encoded></item><item><title>Broadcast-Gated Attention with Identity Adaptive Integration for Efficient Image Super-Resolution</title><link>https://doi.org/10.1109/tip.2026.3657640</link><guid>10.1109/tip.2026.3657640</guid><pubDate>Thu, 29 Jan 2026 21:27:16 +0000</pubDate><dc:creator>Qian Wang</dc:creator><dc:creator>Yanyu Mao</dc:creator><dc:creator>Ruilong Guo</dc:creator><dc:creator>Mengyang Wang</dc:creator><dc:creator>Jing Wei</dc:creator><dc:creator>Han Pan</dc:creator><prism:publicationName>IEEE Transactions on Image Processing</prism:publicationName><prism:doi>10.1109/tip.2026.3657640</prism:doi><description>Efficient image super-resolution (SR) models are essential for achieving high-quality image reconstruction with reduced computational complexity, particularly in resource-constrained environments. In this paper, we introduce a novel self-attention mechanism, Broadcast-Gated Attention with Identity Adaptive Integration (BGAI). Then, based on this mechanism, we design a lightweight super-resolution network that achieves state-of-the-art performance with minimal computational cost. By observing the sparsity and convergence properties of self-attention, BGAI optimizes computational resource utilization through the effective broadcasting of meaningful features across attention heads and network layers. A key innovation in BGAI is the Broadcast-Gated Multi-head Self-Attention (BGMSA) mechanism, which employs a dedicated head to capture and integrate long-range dependencies, broadcasting this broader contextual information to local attention heads. This design enhances long-range interaction modeling while minimizing redundant computations. Additionally, the Identity Attention Adaptive Integration (IAAI) mechanism facilitates efficient feature propagation by leveraging the continuity in dependencies across layers, with a focus on dynamic variations to improve representational efficiency and accelerate convergence. Comprehensive experiments on standard benchmarks demonstrate that BGAI achieves high-fidelity super-resolution while reducing the number of parameters and FLOPs by up to 35% compared with existing lightweight methods. These results establish BGAI as a robust and scalable solution for resource-efficient SR, with significant potential for deployment in real-world, high-resolution image processing applications. The code and trained models are publicly available at https://github.com/bbbolt/BGAI.
Published: 2026-01-29T21:27:16+00:00
Venue: IEEE Transactions on Image Processing
Score: 0.810 (must_read)</description><content:encoded>&lt;p&gt;&lt;strong&gt;Authors:&lt;/strong&gt; Qian Wang; Yanyu Mao; Ruilong Guo; Mengyang Wang; Jing Wei; Han Pan&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Journal:&lt;/strong&gt; IEEE Transactions on Image Processing&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;a href="https://doi.org/10.1109/tip.2026.3657640"&gt;10.1109/tip.2026.3657640&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Relevance:&lt;/strong&gt; 0.810 (must_read)&lt;/p&gt;
&lt;p&gt;Efficient image super-resolution (SR) models are essential for achieving high-quality image reconstruction with reduced computational complexity, particularly in resource-constrained environments. In this paper, we introduce a novel self-attention mechanism, Broadcast-Gated Attention with Identity Adaptive Integration (BGAI). Then, based on this mechanism, we design a lightweight super-resolution network that achieves state-of-the-art performance with minimal computational cost. By observing the sparsity and convergence properties of self-attention, BGAI optimizes computational resource utilization through the effective broadcasting of meaningful features across attention heads and network layers. A key innovation in BGAI is the Broadcast-Gated Multi-head Self-Attention (BGMSA) mechanism, which employs a dedicated head to capture and integrate long-range dependencies, broadcasting this broader contextual information to local attention heads. This design enhances long-range interaction modeling while minimizing redundant computations. Additionally, the Identity Attention Adaptive Integration (IAAI) mechanism facilitates efficient feature propagation by leveraging the continuity in dependencies across layers, with a focus on dynamic variations to improve representational efficiency and accelerate convergence. Comprehensive experiments on standard benchmarks demonstrate that BGAI achieves high-fidelity super-resolution while reducing the number of parameters and FLOPs by up to 35% compared with existing lightweight methods. These results establish BGAI as a robust and scalable solution for resource-efficient SR, with significant potential for deployment in real-world, high-resolution image processing applications. The code and trained models are publicly available at https://github.com/bbbolt/BGAI.&lt;/p&gt;</content:encoded></item><item><title>Principal Component Maximization: A Novel Method for SAR Image Recovery from Raw Data without System Parameters</title><link>https://doi.org/10.1109/tip.2026.3657165</link><guid>10.1109/tip.2026.3657165</guid><pubDate>Thu, 29 Jan 2026 21:27:16 +0000</pubDate><dc:creator>Huizhang Yang</dc:creator><dc:creator>Liyuan Chen</dc:creator><dc:creator>Shao-Shan Zuo</dc:creator><dc:creator>Zhong Liu</dc:creator><dc:creator>Jian Yang</dc:creator><prism:publicationName>IEEE Transactions on Image Processing</prism:publicationName><prism:doi>10.1109/tip.2026.3657165</prism:doi><description>Synthetic Aperture Radar (SAR) imaging relies on using focusing algorithms to transform raw measurement data into radar images. These algorithms require knowledge of SAR system parameters, such as wavelength, center slant range, fast time sampling rate, pulse repetition interval, waveform, and platform speed. However, in non-cooperative scenarios or when metadata is corrupted, these parameters are unavailable, rendering traditional algorithms ineffective. To address this challenge, this paper presents a novel parameter-free method for recovering SAR images from raw data without the requirement of any SAR system parameters. Firstly, we introduce an approximated matched filtering model that leverages the shift-invariance properties of SAR echoes, enabling image formation via convolving the raw data with an unknown reference echo. Secondly, we develop a Principal Component Maximization (PCM) method that exploits the low-dimensional structure of SAR signals to estimate the reference echo. The PCM method employs a three-stage procedure: 1) segment raw data into blocks, 2) normalize the energy of each block, and 3) maximize the principal component’s energy across all blocks, enabling robust estimation of the reference echo under non-stationary clutter. Experimental results on various SAR datasets demonstrate that our method can effectively recover SAR images from raw data without any system parameters. To facilitate reproducibility, the matlab program is available at https://github.com/huizhangyang/pcm.
Published: 2026-01-29T21:27:16+00:00
Venue: IEEE Transactions on Image Processing
Score: 0.806 (must_read)</description><content:encoded>&lt;p&gt;&lt;strong&gt;Authors:&lt;/strong&gt; Huizhang Yang; Liyuan Chen; Shao-Shan Zuo; Zhong Liu; Jian Yang&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Journal:&lt;/strong&gt; IEEE Transactions on Image Processing&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;a href="https://doi.org/10.1109/tip.2026.3657165"&gt;10.1109/tip.2026.3657165&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Relevance:&lt;/strong&gt; 0.806 (must_read)&lt;/p&gt;
&lt;p&gt;Synthetic Aperture Radar (SAR) imaging relies on using focusing algorithms to transform raw measurement data into radar images. These algorithms require knowledge of SAR system parameters, such as wavelength, center slant range, fast time sampling rate, pulse repetition interval, waveform, and platform speed. However, in non-cooperative scenarios or when metadata is corrupted, these parameters are unavailable, rendering traditional algorithms ineffective. To address this challenge, this paper presents a novel parameter-free method for recovering SAR images from raw data without the requirement of any SAR system parameters. Firstly, we introduce an approximated matched filtering model that leverages the shift-invariance properties of SAR echoes, enabling image formation via convolving the raw data with an unknown reference echo. Secondly, we develop a Principal Component Maximization (PCM) method that exploits the low-dimensional structure of SAR signals to estimate the reference echo. The PCM method employs a three-stage procedure: 1) segment raw data into blocks, 2) normalize the energy of each block, and 3) maximize the principal component’s energy across all blocks, enabling robust estimation of the reference echo under non-stationary clutter. Experimental results on various SAR datasets demonstrate that our method can effectively recover SAR images from raw data without any system parameters. To facilitate reproducibility, the matlab program is available at https://github.com/huizhangyang/pcm.&lt;/p&gt;</content:encoded></item><item><title>Diffusion Model-Based Data Augmentation for Land Cover Segmentation in Pol-SAR Imagery</title><link>https://doi.org/10.1016/j.patcog.2026.113171</link><guid>10.1016/j.patcog.2026.113171</guid><pubDate>Fri, 30 Jan 2026 00:19:12 +0000</pubDate><dc:creator>Keunhoon Choi</dc:creator><dc:creator>Sunok Kim</dc:creator><dc:creator>Kwanghoon Sohn</dc:creator><prism:publicationName>Pattern Recognition</prism:publicationName><prism:doi>10.1016/j.patcog.2026.113171</prism:doi><description>Polarimetric Synthetic Aperture Radar (Pol-SAR) provides representations encapsulating physical texture information of land surfaces, useful for land cover segmentation. However, Pol-SAR images and precise segmentation maps are difficult to obtain, limiting public access to large datasets and hindering deep learning methods from achieving optimal performance. To address this, we propose two methods. First, we transform the channel axis to polar coordinates to better exploit surface information in Pol-SAR data. This allows deep learning models to directly learn polarization angles, which improves segmentation performance and resolves the channel imbalance problem in diffusion models. Second, we introduce a diffusion model-based data augmentation framework to generate Pol-SAR imagery with paired land cover maps. By representing land cover maps in a 2-channel format using the Gaussian distribution’s symmetry, we reduce GPU memory compared to one-hot encoding. We also propose a Guided Sampling strategy to generate paired Pol-SAR images when only land cover maps are available. Experimental results validate the effectiveness of our methods on the Pol-SAR dataset.
Published: 2026-01-30T00:19:12+00:00
Venue: Pattern Recognition
Score: 0.798 (must_read)</description><content:encoded>&lt;p&gt;&lt;strong&gt;Authors:&lt;/strong&gt; Keunhoon Choi; Sunok Kim; Kwanghoon Sohn&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Journal:&lt;/strong&gt; Pattern Recognition&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;a href="https://doi.org/10.1016/j.patcog.2026.113171"&gt;10.1016/j.patcog.2026.113171&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Relevance:&lt;/strong&gt; 0.798 (must_read)&lt;/p&gt;
&lt;p&gt;Polarimetric Synthetic Aperture Radar (Pol-SAR) provides representations encapsulating physical texture information of land surfaces, useful for land cover segmentation. However, Pol-SAR images and precise segmentation maps are difficult to obtain, limiting public access to large datasets and hindering deep learning methods from achieving optimal performance. To address this, we propose two methods. First, we transform the channel axis to polar coordinates to better exploit surface information in Pol-SAR data. This allows deep learning models to directly learn polarization angles, which improves segmentation performance and resolves the channel imbalance problem in diffusion models. Second, we introduce a diffusion model-based data augmentation framework to generate Pol-SAR imagery with paired land cover maps. By representing land cover maps in a 2-channel format using the Gaussian distribution’s symmetry, we reduce GPU memory compared to one-hot encoding. We also propose a Guided Sampling strategy to generate paired Pol-SAR images when only land cover maps are available. Experimental results validate the effectiveness of our methods on the Pol-SAR dataset.&lt;/p&gt;</content:encoded></item><item><title>Task-Specific Directions: Definition, Exploration, and Utilization in Parameter Efficient Fine-Tuning</title><link>https://doi.org/10.1109/tpami.2026.3659168</link><guid>10.1109/tpami.2026.3659168</guid><pubDate>Thu, 29 Jan 2026 21:23:36 +0000</pubDate><dc:creator>Chongjie Si</dc:creator><dc:creator>Zhiyi Shi</dc:creator><dc:creator>Shifan Zhang</dc:creator><dc:creator>Xiaokang Yang</dc:creator><dc:creator>Hanspeter Pfister</dc:creator><dc:creator>Wei Shen</dc:creator><prism:publicationName>IEEE Transactions on Pattern Analysis and Machine Intelligence</prism:publicationName><prism:doi>10.1109/tpami.2026.3659168</prism:doi><description>Large language models demonstrate impressive performance on downstream tasks, yet requiring extensive resource consumption when fully fine-tuning all parameters. To mitigate this, Parameter Efficient Fine-Tuning (PEFT) strategies, such as LoRA, have been developed. In this paper, we delve into the concept of task-specific directions (TSDs)—critical for transitioning large models from pretrained states to task-specific enhancements in PEFT. We propose a framework to clearly define these directions and explore their properties, and practical utilization challenges. We then introduce a novel approach, LoRA-Dash, which aims to maximize the impact of TSDs during the fine-tuning process, thereby enhancing model performance on targeted tasks. Additionally, based on our exploration of TSD, we focus on an important issue in PEFT: the initialization of LoRA. While some works have pointed out the significance of initialization for LoRA's performance and proposed various strategies, these methods are often empirical and not task-specific. To address this issue, we propose LoRA-Init. Starting from TSD, we identify the directions that require the most adjustment during fine-tuning for downstream tasks. By initializing the matrices in LoRA with these directions, LoRA-Init significantly enhances LoRA's performance. Moreover, we can combine LoRA-Dash and LoRA-Init to create the final version of LoRA based on TSDs, which we refer to as LoRA-TSD. Extensive experiments have conclusively demonstrated the effectiveness of these methods, and in-depth analyses further reveal the underlying mechanisms of these methods. The codes are available athttps://github.com/Chongjie-Si/Subspace-Tuning.
Published: 2026-01-29T21:23:36+00:00
Venue: IEEE Transactions on Pattern Analysis and Machine Intelligence
Score: 0.796 (must_read)</description><content:encoded>&lt;p&gt;&lt;strong&gt;Authors:&lt;/strong&gt; Chongjie Si; Zhiyi Shi; Shifan Zhang; Xiaokang Yang; Hanspeter Pfister; Wei Shen&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Journal:&lt;/strong&gt; IEEE Transactions on Pattern Analysis and Machine Intelligence&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;a href="https://doi.org/10.1109/tpami.2026.3659168"&gt;10.1109/tpami.2026.3659168&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Relevance:&lt;/strong&gt; 0.796 (must_read)&lt;/p&gt;
&lt;p&gt;Large language models demonstrate impressive performance on downstream tasks, yet requiring extensive resource consumption when fully fine-tuning all parameters. To mitigate this, Parameter Efficient Fine-Tuning (PEFT) strategies, such as LoRA, have been developed. In this paper, we delve into the concept of task-specific directions (TSDs)—critical for transitioning large models from pretrained states to task-specific enhancements in PEFT. We propose a framework to clearly define these directions and explore their properties, and practical utilization challenges. We then introduce a novel approach, LoRA-Dash, which aims to maximize the impact of TSDs during the fine-tuning process, thereby enhancing model performance on targeted tasks. Additionally, based on our exploration of TSD, we focus on an important issue in PEFT: the initialization of LoRA. While some works have pointed out the significance of initialization for LoRA&amp;#x27;s performance and proposed various strategies, these methods are often empirical and not task-specific. To address this issue, we propose LoRA-Init. Starting from TSD, we identify the directions that require the most adjustment during fine-tuning for downstream tasks. By initializing the matrices in LoRA with these directions, LoRA-Init significantly enhances LoRA&amp;#x27;s performance. Moreover, we can combine LoRA-Dash and LoRA-Init to create the final version of LoRA based on TSDs, which we refer to as LoRA-TSD. Extensive experiments have conclusively demonstrated the effectiveness of these methods, and in-depth analyses further reveal the underlying mechanisms of these methods. The codes are available athttps://github.com/Chongjie-Si/Subspace-Tuning.&lt;/p&gt;</content:encoded></item><item><title>MCDF-Net: Dynamic Adaptive Network Based on Modal Competition and Dual Encoder Feature Fusion for Remote Sensing Image Target Detection</title><link>https://doi.org/10.1109/jstars.2026.3659193</link><guid>10.1109/jstars.2026.3659193</guid><pubDate>Thu, 29 Jan 2026 21:24:12 +0000</pubDate><dc:creator>Yuanjie Zhi</dc:creator><dc:creator>Yushuo Qi</dc:creator><dc:creator>Zhi Yang</dc:creator><dc:creator>Wenkui Hao</dc:creator><dc:creator>Mingyang Ma</dc:creator><dc:creator>Shaohui Mei</dc:creator><prism:publicationName>IEEE Journal of Selected Topics in Applied Earth Observations and Remote Sensing</prism:publicationName><prism:doi>10.1109/jstars.2026.3659193</prism:doi><description>Remote sensing image object detection, leveraging the complementary characteristics of infrared and RGB imaging, represents an effective approach for achieving all-weather detection. However, in complex environments, the quality of information provided by different modalities can undergo dynamic variations, necessitating dynamic adjustment of the weights assigned to each modality. Therefore, a Dynamic Adaptive Network based on Modal Competition and Dual-Encoder Feature Fusion (MCDF-Net) is proposed to implement precise modeling of dynamic complementary relationships and adaptive extraction of discriminative features through hierarchical feature dynamic interaction and an adaptive salient modal competition mechanism. Specifically, a Hierarchical Feature Attention Fusion Module (HFAM) is designed under dual parallel feature encoding branches to enable the fusion of global context and local details, in which the Cross-Channel Attention Module (CCAM) is adopted to enhance channel responses through reconstruction via channel feature correlation matrices, and the Difference Fusion Attention Module (DFAM) concurrently calibrates spatial biases through pixel-level difference modeling. Moreover, an Information Entropy-Guided Adaptive Modal Competition Mechanism (IEAMC) is proposed to filter high-confidence queries by quantifying feature point uncertainty, thereby providing useful prior information for the decoder and adaptively determining the salient modality for targets to balance modal contributions. Experimental results over two benchmark datasets, i.e., DroneVehicle and VEDAI datasets, demonstrate that the proposed method clearly outperform state-of-the-art algorithms by effectively handling highly dynamic feature variations.
Published: 2026-01-29T21:24:12+00:00
Venue: IEEE Journal of Selected Topics in Applied Earth Observations and Remote Sensing
Score: 0.795 (must_read)</description><content:encoded>&lt;p&gt;&lt;strong&gt;Authors:&lt;/strong&gt; Yuanjie Zhi; Yushuo Qi; Zhi Yang; Wenkui Hao; Mingyang Ma; Shaohui Mei&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Journal:&lt;/strong&gt; IEEE Journal of Selected Topics in Applied Earth Observations and Remote Sensing&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;a href="https://doi.org/10.1109/jstars.2026.3659193"&gt;10.1109/jstars.2026.3659193&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Relevance:&lt;/strong&gt; 0.795 (must_read)&lt;/p&gt;
&lt;p&gt;Remote sensing image object detection, leveraging the complementary characteristics of infrared and RGB imaging, represents an effective approach for achieving all-weather detection. However, in complex environments, the quality of information provided by different modalities can undergo dynamic variations, necessitating dynamic adjustment of the weights assigned to each modality. Therefore, a Dynamic Adaptive Network based on Modal Competition and Dual-Encoder Feature Fusion (MCDF-Net) is proposed to implement precise modeling of dynamic complementary relationships and adaptive extraction of discriminative features through hierarchical feature dynamic interaction and an adaptive salient modal competition mechanism. Specifically, a Hierarchical Feature Attention Fusion Module (HFAM) is designed under dual parallel feature encoding branches to enable the fusion of global context and local details, in which the Cross-Channel Attention Module (CCAM) is adopted to enhance channel responses through reconstruction via channel feature correlation matrices, and the Difference Fusion Attention Module (DFAM) concurrently calibrates spatial biases through pixel-level difference modeling. Moreover, an Information Entropy-Guided Adaptive Modal Competition Mechanism (IEAMC) is proposed to filter high-confidence queries by quantifying feature point uncertainty, thereby providing useful prior information for the decoder and adaptively determining the salient modality for targets to balance modal contributions. Experimental results over two benchmark datasets, i.e., DroneVehicle and VEDAI datasets, demonstrate that the proposed method clearly outperform state-of-the-art algorithms by effectively handling highly dynamic feature variations.&lt;/p&gt;</content:encoded></item><item><title>TMT: Tri-Modal Translation Between Speech, Image, and Text by Processing Different Modalities as Different Languages</title><link>https://doi.org/10.1109/tmm.2026.3659297</link><guid>10.1109/tmm.2026.3659297</guid><pubDate>Thu, 29 Jan 2026 21:25:04 +0000</pubDate><dc:creator>Minsu Kim</dc:creator><dc:creator>Jee-weon Jung</dc:creator><dc:creator>Hyeongseop Rha</dc:creator><dc:creator>Soumi Maiti</dc:creator><dc:creator>Siddhant Arora</dc:creator><dc:creator>Xuankai Chang</dc:creator><dc:creator>Shinji Watanabe</dc:creator><dc:creator>Yong Man Ro</dc:creator><prism:publicationName>IEEE Transactions on Multimedia</prism:publicationName><prism:doi>10.1109/tmm.2026.3659297</prism:doi><description>The capability to jointly process multi-modal information is becoming essential. However, the development of multi-modal learning is hindered by the substantial computational requirements and the limited availability of paired multi-modal data. We propose a novel Tri-Modal Translation (TMT) model that translates between arbitrary modalities spanning speech, image, and text. We introduce a simple yet efficient and effective approach, treating speech and image modalities as discrete text modality and approaching multi-modal translation as a well-established machine translation problem. To this end, we tokenize speech and image data into discrete tokens, resulting in a significant reduction in computational cost. Furthermore, by incorporating back translation into multi-modal translation, unpaired data can also be utilized for training. TMT can perform six modality translation tasks and consistently outperforms its single-model counterparts. TMT significantly reduces the required data size (in bits) for training, to approximately 0.2% for speech data and 0.04% for image data, respectively.
Published: 2026-01-29T21:25:04+00:00
Venue: IEEE Transactions on Multimedia
Score: 0.794 (must_read)</description><content:encoded>&lt;p&gt;&lt;strong&gt;Authors:&lt;/strong&gt; Minsu Kim; Jee-weon Jung; Hyeongseop Rha; Soumi Maiti; Siddhant Arora; Xuankai Chang; Shinji Watanabe; Yong Man Ro&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Journal:&lt;/strong&gt; IEEE Transactions on Multimedia&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;a href="https://doi.org/10.1109/tmm.2026.3659297"&gt;10.1109/tmm.2026.3659297&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Relevance:&lt;/strong&gt; 0.794 (must_read)&lt;/p&gt;
&lt;p&gt;The capability to jointly process multi-modal information is becoming essential. However, the development of multi-modal learning is hindered by the substantial computational requirements and the limited availability of paired multi-modal data. We propose a novel Tri-Modal Translation (TMT) model that translates between arbitrary modalities spanning speech, image, and text. We introduce a simple yet efficient and effective approach, treating speech and image modalities as discrete text modality and approaching multi-modal translation as a well-established machine translation problem. To this end, we tokenize speech and image data into discrete tokens, resulting in a significant reduction in computational cost. Furthermore, by incorporating back translation into multi-modal translation, unpaired data can also be utilized for training. TMT can perform six modality translation tasks and consistently outperforms its single-model counterparts. TMT significantly reduces the required data size (in bits) for training, to approximately 0.2% for speech data and 0.04% for image data, respectively.&lt;/p&gt;</content:encoded></item><item><title>Urban Neural Surface Reconstruction from Constrained Sparse Aerial Imagery with 3D SAR Fusion</title><link>https://arxiv.org/abs/2601.22045v1</link><guid>http://arxiv.org/abs/2601.22045v1</guid><pubDate>Thu, 29 Jan 2026 17:47:07 +0000</pubDate><dc:creator>Da Li</dc:creator><dc:creator>Chen Yao</dc:creator><dc:creator>Tong Mao</dc:creator><dc:creator>Jiacheng Bao</dc:creator><dc:creator>Houjun Sun</dc:creator><dc:type>preprint</dc:type><prism:publicationName>arXiv</prism:publicationName><description>Neural surface reconstruction (NSR) has recently shown strong potential for urban 3D reconstruction from multi-view aerial imagery. However, existing NSR methods often suffer from geometric ambiguity and instability, particularly under sparse-view conditions. This issue is critical in large-scale urban remote sensing, where aerial image acquisition is limited by flight paths, terrain, and cost. To address this challenge, we present the first urban NSR framework that fuses 3D synthetic aperture radar (SAR) point clouds with aerial imagery for high-fidelity reconstruction under constrained, sparse-view settings. 3D SAR can efficiently capture large-scale geometry even from a single side-looking flight path, providing robust priors that complement photometric cues from images. Our framework integrates radar-derived spatial constraints into an SDF-based NSR backbone, guiding structure-aware ray selection and adaptive sampling for stable and efficient optimization. We also construct the first benchmark dataset with co-registered 3D SAR point clouds and aerial imagery, facilitating systematic evaluation of cross-modal 3D reconstruction. Extensive experiments show that incorporating 3D SAR markedly enhances reconstruction accuracy, completeness, and robustness compared with single-modality baselines under highly sparse and oblique-view conditions, highlighting a viable route toward scalable high-fidelity urban reconstruction with advanced airborne and spaceborne optical-SAR sensing.
Published: 2026-01-29T17:47:07+00:00
Venue: arXiv
Score: 0.788 (must_read)</description><content:encoded>&lt;p&gt;&lt;strong&gt;Authors:&lt;/strong&gt; Da Li; Chen Yao; Tong Mao; Jiacheng Bao; Houjun Sun&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Journal:&lt;/strong&gt; arXiv&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Relevance:&lt;/strong&gt; 0.788 (must_read)&lt;/p&gt;
&lt;p&gt;Neural surface reconstruction (NSR) has recently shown strong potential for urban 3D reconstruction from multi-view aerial imagery. However, existing NSR methods often suffer from geometric ambiguity and instability, particularly under sparse-view conditions. This issue is critical in large-scale urban remote sensing, where aerial image acquisition is limited by flight paths, terrain, and cost. To address this challenge, we present the first urban NSR framework that fuses 3D synthetic aperture radar (SAR) point clouds with aerial imagery for high-fidelity reconstruction under constrained, sparse-view settings. 3D SAR can efficiently capture large-scale geometry even from a single side-looking flight path, providing robust priors that complement photometric cues from images. Our framework integrates radar-derived spatial constraints into an SDF-based NSR backbone, guiding structure-aware ray selection and adaptive sampling for stable and efficient optimization. We also construct the first benchmark dataset with co-registered 3D SAR point clouds and aerial imagery, facilitating systematic evaluation of cross-modal 3D reconstruction. Extensive experiments show that incorporating 3D SAR markedly enhances reconstruction accuracy, completeness, and robustness compared with single-modality baselines under highly sparse and oblique-view conditions, highlighting a viable route toward scalable high-fidelity urban reconstruction with advanced airborne and spaceborne optical-SAR sensing.&lt;/p&gt;</content:encoded></item><item><title>R^3: Replay, Reflection, and Ranking Rewards for LLM Reinforcement Learning</title><link>https://arxiv.org/abs/2601.19620v2</link><guid>http://arxiv.org/abs/2601.19620v2</guid><pubDate>Tue, 27 Jan 2026 13:55:34 +0000</pubDate><dc:creator>Zhizheng Jiang</dc:creator><dc:creator>Kang Zhao</dc:creator><dc:creator>Weikai Xu</dc:creator><dc:creator>Xinkui Lin</dc:creator><dc:creator>Wei Liu</dc:creator><dc:creator>Jian Luan</dc:creator><dc:creator>Shuo Shang</dc:creator><dc:creator>Peng Han</dc:creator><dc:type>preprint</dc:type><prism:publicationName>arXiv</prism:publicationName><description>Large reasoning models (LRMs) aim to solve diverse and complex problems through structured reasoning. Recent advances in group-based policy optimization methods have shown promise in enabling stable advantage estimation without reliance on process-level annotations. However, these methods rely on advantage gaps induced by high-quality samples within the same batch, which makes the training process fragile and inefficient when intra-group advantages collapse under challenging tasks. To address these problems, we propose a reinforcement learning mechanism named \emph{\textbf{R^3}} that along three directions: (1) a \emph{cross-context \underline{\textbf{R}}eplay} strategy that maintains the intra-group advantage by recalling valuable examples from historical trajectories of the same query, (2) an \emph{in-context self-\underline{\textbf{R}}eflection} mechanism enabling models to refine outputs by leveraging past failures, and (3) a \emph{structural entropy \underline{\textbf{R}}anking reward}, which assigns relative rewards to truncated or failed samples by ranking responses based on token-level entropy patterns, capturing both local exploration and global stability. We implement our method on Deepseek-R1-Distill-Qwen-1.5B and train it on the DeepscaleR-40k in the math domain. Experiments demonstrate our method achieves SoTA performance on several math benchmarks, representing significant improvements and fewer reasoning tokens over the base models. Code and model will be released.
Published: 2026-01-27T13:55:34+00:00
Venue: arXiv
Score: 0.786 (must_read)</description><content:encoded>&lt;p&gt;&lt;strong&gt;Authors:&lt;/strong&gt; Zhizheng Jiang; Kang Zhao; Weikai Xu; Xinkui Lin; Wei Liu; Jian Luan; Shuo Shang; Peng Han&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Journal:&lt;/strong&gt; arXiv&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Relevance:&lt;/strong&gt; 0.786 (must_read)&lt;/p&gt;
&lt;p&gt;Large reasoning models (LRMs) aim to solve diverse and complex problems through structured reasoning. Recent advances in group-based policy optimization methods have shown promise in enabling stable advantage estimation without reliance on process-level annotations. However, these methods rely on advantage gaps induced by high-quality samples within the same batch, which makes the training process fragile and inefficient when intra-group advantages collapse under challenging tasks. To address these problems, we propose a reinforcement learning mechanism named \emph{\textbf{R^3}} that along three directions: (1) a \emph{cross-context \underline{\textbf{R}}eplay} strategy that maintains the intra-group advantage by recalling valuable examples from historical trajectories of the same query, (2) an \emph{in-context self-\underline{\textbf{R}}eflection} mechanism enabling models to refine outputs by leveraging past failures, and (3) a \emph{structural entropy \underline{\textbf{R}}anking reward}, which assigns relative rewards to truncated or failed samples by ranking responses based on token-level entropy patterns, capturing both local exploration and global stability. We implement our method on Deepseek-R1-Distill-Qwen-1.5B and train it on the DeepscaleR-40k in the math domain. Experiments demonstrate our method achieves SoTA performance on several math benchmarks, representing significant improvements and fewer reasoning tokens over the base models. Code and model will be released.&lt;/p&gt;</content:encoded></item><item><title>Learning discriminative prototypes: adaptive relation-aware refinement and patch-level contextual feature reweighting for few-shot classification</title><link>https://doi.org/10.1016/j.neunet.2026.108649</link><guid>10.1016/j.neunet.2026.108649</guid><pubDate>Thu, 29 Jan 2026 18:59:59 +0000</pubDate><dc:creator>Mengjuan Jiang</dc:creator><dc:creator>Fanzhang Li</dc:creator><prism:publicationName>Neural Networks</prism:publicationName><prism:doi>10.1016/j.neunet.2026.108649</prism:doi><description>Few-shot learning (FSL) aims to achieve efficient classification with limited labeled samples, providing an important research paradigm for addressing the model generalization issue in data-scarce scenarios. In the metric-based FSL framework, class prototypes serve as the core transferable representation of classes, and their discriminative power directly impacts the model’s classification performance. However, existing methods face two major bottlenecks: first, traditional feature selection mechanisms use static modeling approaches that are susceptible to background noise and struggle to capture dynamic relationships between classes; second, due to limitations in the quantity and quality of labeled samples, prototype representations based on global features lack fine-grained expression of local discriminative features, limiting the prototype’s representational power. To overcome these limitations, we propose a novel framework: Learning Discriminative Prototypes (LDP). LDP includes two modules: (1) Adaptive relation-aware refinement, which dynamically models the relationships between class prototypes, highlighting the key features of each class and effectively enhancing the robustness of feature representations; (2) Patch-level contextual feature reweighting, which performs a reweighting operation on the samples through patch-level feature interactions thereby obtaining a more discriminative prototype. Experimental results demonstrate that LDP shows strong competitiveness on five datasets covering both standard and cross-domain datasets, validating its effectiveness in FSL tasks. For example, in the 1-shot setting on miniImageNet and tieredImageNet, LDP achieves over 12% accuracy improvement compared with the baseline methods; on the cross-domain dataset CUB200, the improvement reaches 6.45% in the 1-shot case. Our code is available on GitHub at https://github.com/fewshot-learner/LDP .
Published: 2026-01-29T18:59:59+00:00
Venue: Neural Networks
Score: 0.786 (must_read)</description><content:encoded>&lt;p&gt;&lt;strong&gt;Authors:&lt;/strong&gt; Mengjuan Jiang; Fanzhang Li&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Journal:&lt;/strong&gt; Neural Networks&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;a href="https://doi.org/10.1016/j.neunet.2026.108649"&gt;10.1016/j.neunet.2026.108649&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Relevance:&lt;/strong&gt; 0.786 (must_read)&lt;/p&gt;
&lt;p&gt;Few-shot learning (FSL) aims to achieve efficient classification with limited labeled samples, providing an important research paradigm for addressing the model generalization issue in data-scarce scenarios. In the metric-based FSL framework, class prototypes serve as the core transferable representation of classes, and their discriminative power directly impacts the model’s classification performance. However, existing methods face two major bottlenecks: first, traditional feature selection mechanisms use static modeling approaches that are susceptible to background noise and struggle to capture dynamic relationships between classes; second, due to limitations in the quantity and quality of labeled samples, prototype representations based on global features lack fine-grained expression of local discriminative features, limiting the prototype’s representational power. To overcome these limitations, we propose a novel framework: Learning Discriminative Prototypes (LDP). LDP includes two modules: (1) Adaptive relation-aware refinement, which dynamically models the relationships between class prototypes, highlighting the key features of each class and effectively enhancing the robustness of feature representations; (2) Patch-level contextual feature reweighting, which performs a reweighting operation on the samples through patch-level feature interactions thereby obtaining a more discriminative prototype. Experimental results demonstrate that LDP shows strong competitiveness on five datasets covering both standard and cross-domain datasets, validating its effectiveness in FSL tasks. For example, in the 1-shot setting on miniImageNet and tieredImageNet, LDP achieves over 12% accuracy improvement compared with the baseline methods; on the cross-domain dataset CUB200, the improvement reaches 6.45% in the 1-shot case. Our code is available on GitHub at https://github.com/fewshot-learner/LDP .&lt;/p&gt;</content:encoded></item><item><title>Instance-Guided Radar Depth Estimation for 3D Object Detection</title><link>https://arxiv.org/abs/2601.19314v1</link><guid>http://arxiv.org/abs/2601.19314v1</guid><pubDate>Tue, 27 Jan 2026 07:53:24 +0000</pubDate><dc:creator>Chen-Chou Lo</dc:creator><dc:creator>Patrick Vandewalle</dc:creator><dc:type>preprint</dc:type><prism:publicationName>arXiv</prism:publicationName><description>Accurate depth estimation is fundamental to 3D perception in autonomous driving, supporting tasks such as detection, tracking, and motion planning. However, monocular camera-based 3D detection suffers from depth ambiguity and reduced robustness under challenging conditions. Radar provides complementary advantages such as resilience to poor lighting and adverse weather, but its sparsity and low resolution limit its direct use in detection frameworks. This motivates the need for effective Radar-camera fusion with improved preprocessing and depth estimation strategies. We propose an end-to-end framework that enhances monocular 3D object detection through two key components. First, we introduce InstaRadar, an instance segmentation-guided expansion method that leverages pre-trained segmentation masks to enhance Radar density and semantic alignment, producing a more structured representation. InstaRadar achieves state-of-the-art results in Radar-guided depth estimation, showing its effectiveness in generating high-quality depth features. Second, we integrate the pre-trained RCDPT into the BEVDepth framework as a replacement for its depth module. With InstaRadar-enhanced inputs, the RCDPT integration consistently improves 3D detection performance. Overall, these components yield steady gains over the baseline BEVDepth model, demonstrating the effectiveness of InstaRadar and the advantage of explicit depth supervision in 3D object detection. Although the framework lags behind Radar-camera fusion models that directly extract BEV features, since Radar serves only as guidance rather than an independent feature stream, this limitation highlights potential for improvement. Future work will extend InstaRadar to point cloud-like representations and integrate a dedicated Radar branch with temporal cues for enhanced BEV fusion.
Published: 2026-01-27T07:53:24+00:00
Venue: arXiv
Score: 0.786 (must_read)</description><content:encoded>&lt;p&gt;&lt;strong&gt;Authors:&lt;/strong&gt; Chen-Chou Lo; Patrick Vandewalle&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Journal:&lt;/strong&gt; arXiv&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Relevance:&lt;/strong&gt; 0.786 (must_read)&lt;/p&gt;
&lt;p&gt;Accurate depth estimation is fundamental to 3D perception in autonomous driving, supporting tasks such as detection, tracking, and motion planning. However, monocular camera-based 3D detection suffers from depth ambiguity and reduced robustness under challenging conditions. Radar provides complementary advantages such as resilience to poor lighting and adverse weather, but its sparsity and low resolution limit its direct use in detection frameworks. This motivates the need for effective Radar-camera fusion with improved preprocessing and depth estimation strategies. We propose an end-to-end framework that enhances monocular 3D object detection through two key components. First, we introduce InstaRadar, an instance segmentation-guided expansion method that leverages pre-trained segmentation masks to enhance Radar density and semantic alignment, producing a more structured representation. InstaRadar achieves state-of-the-art results in Radar-guided depth estimation, showing its effectiveness in generating high-quality depth features. Second, we integrate the pre-trained RCDPT into the BEVDepth framework as a replacement for its depth module. With InstaRadar-enhanced inputs, the RCDPT integration consistently improves 3D detection performance. Overall, these components yield steady gains over the baseline BEVDepth model, demonstrating the effectiveness of InstaRadar and the advantage of explicit depth supervision in 3D object detection. Although the framework lags behind Radar-camera fusion models that directly extract BEV features, since Radar serves only as guidance rather than an independent feature stream, this limitation highlights potential for improvement. Future work will extend InstaRadar to point cloud-like representations and integrate a dedicated Radar branch with temporal cues for enhanced BEV fusion.&lt;/p&gt;</content:encoded></item><item><title>SONIC: Spectral Oriented Neural Invariant Convolutions</title><link>https://arxiv.org/abs/2601.19884v1</link><guid>http://arxiv.org/abs/2601.19884v1</guid><pubDate>Tue, 27 Jan 2026 18:51:11 +0000</pubDate><dc:creator>Gijs Joppe Moens</dc:creator><dc:creator>Regina Beets-Tan</dc:creator><dc:creator>Eduardo H. P. Pooch</dc:creator><dc:type>preprint</dc:type><prism:publicationName>arXiv</prism:publicationName><description>Convolutional Neural Networks (CNNs) rely on fixed-size kernels scanning local patches, which limits their ability to capture global context or long-range dependencies without very deep architectures. Vision Transformers (ViTs), in turn, provide global connectivity but lack spatial inductive bias, depend on explicit positional encodings, and remain tied to the initial patch size. Bridging these limitations requires a representation that is both structured and global. We introduce SONIC (Spectral Oriented Neural Invariant Convolutions), a continuous spectral parameterisation that models convolutional operators using a small set of shared, orientation-selective components. These components define smooth responses across the full frequency domain, yielding global receptive fields and filters that adapt naturally across resolutions. Across synthetic benchmarks, large-scale image classification, and 3D medical datasets, SONIC shows improved robustness to geometric transformations, noise, and resolution shifts, and matches or exceeds convolutional, attention-based, and prior spectral architectures with an order of magnitude fewer parameters. These results demonstrate that continuous, orientation-aware spectral parameterisations provide a principled and scalable alternative to conventional spatial and spectral operators.
Published: 2026-01-27T18:51:11+00:00
Venue: arXiv
Score: 0.783 (must_read)</description><content:encoded>&lt;p&gt;&lt;strong&gt;Authors:&lt;/strong&gt; Gijs Joppe Moens; Regina Beets-Tan; Eduardo H. P. Pooch&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Journal:&lt;/strong&gt; arXiv&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Relevance:&lt;/strong&gt; 0.783 (must_read)&lt;/p&gt;
&lt;p&gt;Convolutional Neural Networks (CNNs) rely on fixed-size kernels scanning local patches, which limits their ability to capture global context or long-range dependencies without very deep architectures. Vision Transformers (ViTs), in turn, provide global connectivity but lack spatial inductive bias, depend on explicit positional encodings, and remain tied to the initial patch size. Bridging these limitations requires a representation that is both structured and global. We introduce SONIC (Spectral Oriented Neural Invariant Convolutions), a continuous spectral parameterisation that models convolutional operators using a small set of shared, orientation-selective components. These components define smooth responses across the full frequency domain, yielding global receptive fields and filters that adapt naturally across resolutions. Across synthetic benchmarks, large-scale image classification, and 3D medical datasets, SONIC shows improved robustness to geometric transformations, noise, and resolution shifts, and matches or exceeds convolutional, attention-based, and prior spectral architectures with an order of magnitude fewer parameters. These results demonstrate that continuous, orientation-aware spectral parameterisations provide a principled and scalable alternative to conventional spatial and spectral operators.&lt;/p&gt;</content:encoded></item><item><title>Unpaired optical-to-SAR image translation with coordinate attention and differentiable histogram loss</title><link>https://doi.org/10.1080/01431161.2026.2621178</link><guid>10.1080/01431161.2026.2621178</guid><pubDate>Thu, 29 Jan 2026 19:39:48 +0000</pubDate><dc:creator>Wenbo Yu</dc:creator><dc:creator>Jiamu Li</dc:creator><dc:creator>Tian Tian</dc:creator><dc:creator>Feng Zhou</dc:creator><prism:publicationName>International Journal of Remote Sensing</prism:publicationName><prism:doi>10.1080/01431161.2026.2621178</prism:doi><description>The high acquisition cost of synthetic aperture radar (SAR) imagery has been a persistent obstacle to advanced deep learning researches. Recently, image translation techniques have emerged as promising solutions for augmenting SAR datasets by translating readily available optical images into SAR-like representations. However, the substantial stylistic differences between optical and SAR images pose significant challenges in accurately extracting optical image semantics and replicating SAR image styles, especially when co-registered data is unavailable. To address this challenge, we propose an unpaired optical-to-SAR image translation (O2SIT) method, named extract-and-transform generative adversarial network (ET-GAN). First, we introduce cascaded coordinate attention (CA) bottleneck blocks that enhance the positional information of feature maps, thereby precisely extracting optical image semantics. Second, to better capture SAR style characteristics, we employ histograms as auxiliary supervision by constructing a differentiable histogram using kernel density estimation and global average pooling. On this basis, the squared earth mover distance is adopted as an additional loss to guide the generator in producing synthetic images with pixel distributions similar to real SAR images. Experimental results on SEN12, WHU-SEN-City, and GaoFen aircraft detection (GF-AD) dataset demonstrate that ET-GAN achieves competitive SAR image generation performance compared to other state-of-the-art methods, with PSNR of 17.11 on SEN12 and FID of 168.16 on GF-AD. Transfer learning results demonstrate that the images generated by ET-GAN can bring about 3% accuracy improvement to SAR aircraft detection.
Published: 2026-01-29T19:39:48+00:00
Venue: International Journal of Remote Sensing
Score: 0.783 (must_read)</description><content:encoded>&lt;p&gt;&lt;strong&gt;Authors:&lt;/strong&gt; Wenbo Yu; Jiamu Li; Tian Tian; Feng Zhou&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Journal:&lt;/strong&gt; International Journal of Remote Sensing&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;a href="https://doi.org/10.1080/01431161.2026.2621178"&gt;10.1080/01431161.2026.2621178&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Relevance:&lt;/strong&gt; 0.783 (must_read)&lt;/p&gt;
&lt;p&gt;The high acquisition cost of synthetic aperture radar (SAR) imagery has been a persistent obstacle to advanced deep learning researches. Recently, image translation techniques have emerged as promising solutions for augmenting SAR datasets by translating readily available optical images into SAR-like representations. However, the substantial stylistic differences between optical and SAR images pose significant challenges in accurately extracting optical image semantics and replicating SAR image styles, especially when co-registered data is unavailable. To address this challenge, we propose an unpaired optical-to-SAR image translation (O2SIT) method, named extract-and-transform generative adversarial network (ET-GAN). First, we introduce cascaded coordinate attention (CA) bottleneck blocks that enhance the positional information of feature maps, thereby precisely extracting optical image semantics. Second, to better capture SAR style characteristics, we employ histograms as auxiliary supervision by constructing a differentiable histogram using kernel density estimation and global average pooling. On this basis, the squared earth mover distance is adopted as an additional loss to guide the generator in producing synthetic images with pixel distributions similar to real SAR images. Experimental results on SEN12, WHU-SEN-City, and GaoFen aircraft detection (GF-AD) dataset demonstrate that ET-GAN achieves competitive SAR image generation performance compared to other state-of-the-art methods, with PSNR of 17.11 on SEN12 and FID of 168.16 on GF-AD. Transfer learning results demonstrate that the images generated by ET-GAN can bring about 3% accuracy improvement to SAR aircraft detection.&lt;/p&gt;</content:encoded></item><item><title>Distributionally Robust Classification for Multi-source Unsupervised Domain Adaptation</title><link>https://arxiv.org/abs/2601.21315v1</link><guid>http://arxiv.org/abs/2601.21315v1</guid><pubDate>Thu, 29 Jan 2026 06:23:14 +0000</pubDate><dc:creator>Seonghwi Kim</dc:creator><dc:creator>Sung Ho Jo</dc:creator><dc:creator>Wooseok Ha</dc:creator><dc:creator>Minwoo Chae</dc:creator><dc:type>preprint</dc:type><prism:publicationName>arXiv</prism:publicationName><description>Unsupervised domain adaptation (UDA) is a statistical learning problem when the distribution of training (source) data is different from that of test (target) data. In this setting, one has access to labeled data only from the source domain and unlabeled data from the target domain. The central objective is to leverage the source data and the unlabeled target data to build models that generalize to the target domain. Despite its potential, existing UDA approaches often struggle in practice, particularly in scenarios where the target domain offers only limited unlabeled data or spurious correlations dominate the source domain. To address these challenges, we propose a novel distributionally robust learning framework that models uncertainty in both the covariate distribution and the conditional label distribution. Our approach is motivated by the multi-source domain adaptation setting but is also directly applicable to the single-source scenario, making it versatile in practice. We develop an efficient learning algorithm that can be seamlessly integrated with existing UDA methods. Extensive experiments under various distribution shift scenarios show that our method consistently outperforms strong baselines, especially when target data are extremely scarce.
Published: 2026-01-29T06:23:14+00:00
Venue: arXiv
Score: 0.783 (must_read)</description><content:encoded>&lt;p&gt;&lt;strong&gt;Authors:&lt;/strong&gt; Seonghwi Kim; Sung Ho Jo; Wooseok Ha; Minwoo Chae&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Journal:&lt;/strong&gt; arXiv&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Relevance:&lt;/strong&gt; 0.783 (must_read)&lt;/p&gt;
&lt;p&gt;Unsupervised domain adaptation (UDA) is a statistical learning problem when the distribution of training (source) data is different from that of test (target) data. In this setting, one has access to labeled data only from the source domain and unlabeled data from the target domain. The central objective is to leverage the source data and the unlabeled target data to build models that generalize to the target domain. Despite its potential, existing UDA approaches often struggle in practice, particularly in scenarios where the target domain offers only limited unlabeled data or spurious correlations dominate the source domain. To address these challenges, we propose a novel distributionally robust learning framework that models uncertainty in both the covariate distribution and the conditional label distribution. Our approach is motivated by the multi-source domain adaptation setting but is also directly applicable to the single-source scenario, making it versatile in practice. We develop an efficient learning algorithm that can be seamlessly integrated with existing UDA methods. Extensive experiments under various distribution shift scenarios show that our method consistently outperforms strong baselines, especially when target data are extremely scarce.&lt;/p&gt;</content:encoded></item><item><title>L$^3$: Large Lookup Layers</title><link>https://arxiv.org/abs/2601.21461v1</link><guid>http://arxiv.org/abs/2601.21461v1</guid><pubDate>Thu, 29 Jan 2026 09:37:31 +0000</pubDate><dc:creator>Albert Tseng</dc:creator><dc:creator>Christopher De Sa</dc:creator><dc:type>preprint</dc:type><prism:publicationName>arXiv</prism:publicationName><description>Modern sparse language models typically achieve sparsity through Mixture-of-Experts (MoE) layers, which dynamically route tokens to dense MLP "experts." However, dynamic hard routing has a number of drawbacks, such as potentially poor hardware efficiency and needing auxiliary losses for stable training. In contrast, the tokenizer embedding table, which is natively sparse, largely avoids these issues by selecting a single embedding per token at the cost of not having contextual information. In this work, we introduce the Large Lookup Layer (L$^3$), which unlocks a new axis of sparsity by generalizing embedding tables to model decoder layers. L$^3$ layers use static token-based routing to aggregate a set of learned embeddings per token in a context-dependent way, allowing the model to efficiently balance memory and compute by caching information in embeddings. L$^3$ has two main components: (1) a systems-friendly architecture that allows for fast training and CPU-offloaded inference with no overhead, and (2) an information-theoretic embedding allocation algorithm that effectively balances speed and quality. We empirically test L$^3$ by training transformers with up to 2.6B active parameters and find that L$^3$ strongly outperforms both dense models and iso-sparse MoEs in both language modeling and downstream tasks.
Published: 2026-01-29T09:37:31+00:00
Venue: arXiv
Score: 0.783 (must_read)</description><content:encoded>&lt;p&gt;&lt;strong&gt;Authors:&lt;/strong&gt; Albert Tseng; Christopher De Sa&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Journal:&lt;/strong&gt; arXiv&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Relevance:&lt;/strong&gt; 0.783 (must_read)&lt;/p&gt;
&lt;p&gt;Modern sparse language models typically achieve sparsity through Mixture-of-Experts (MoE) layers, which dynamically route tokens to dense MLP &amp;quot;experts.&amp;quot; However, dynamic hard routing has a number of drawbacks, such as potentially poor hardware efficiency and needing auxiliary losses for stable training. In contrast, the tokenizer embedding table, which is natively sparse, largely avoids these issues by selecting a single embedding per token at the cost of not having contextual information. In this work, we introduce the Large Lookup Layer (L$^3$), which unlocks a new axis of sparsity by generalizing embedding tables to model decoder layers. L$^3$ layers use static token-based routing to aggregate a set of learned embeddings per token in a context-dependent way, allowing the model to efficiently balance memory and compute by caching information in embeddings. L$^3$ has two main components: (1) a systems-friendly architecture that allows for fast training and CPU-offloaded inference with no overhead, and (2) an information-theoretic embedding allocation algorithm that effectively balances speed and quality. We empirically test L$^3$ by training transformers with up to 2.6B active parameters and find that L$^3$ strongly outperforms both dense models and iso-sparse MoEs in both language modeling and downstream tasks.&lt;/p&gt;</content:encoded></item><item><title>Mitigating Overthinking in Large Reasoning Models via Difficulty-aware Reinforcement Learning</title><link>https://arxiv.org/abs/2601.21418v1</link><guid>http://arxiv.org/abs/2601.21418v1</guid><pubDate>Thu, 29 Jan 2026 08:56:45 +0000</pubDate><dc:creator>Qian Wan</dc:creator><dc:creator>Ziao Xu</dc:creator><dc:creator>Luona Wei</dc:creator><dc:creator>Xiaoxuan Shen</dc:creator><dc:creator>Jianwen Sun</dc:creator><dc:type>preprint</dc:type><prism:publicationName>arXiv</prism:publicationName><description>Large Reasoning Models (LRMs) achieve explicit chain-of-thought expansion by imitating deep thinking behaviors of humans, demonstrating excellent performance in complex task scenarios. However, the deep-thinking mode often leads to unnecessarily lengthy reasoning and resource inefficiency when handling simple tasks. This overthinking phenomenon may arise from the generation preference triggered by the reward function during post-training. Existing research attempts to mitigate overthinking from the perspective of prompt design or model training, but generally underestimates the importance of task difficulty awareness, which makes it difficult for LRMs to effectively allocate reasoning resources. In this paper, we propose Difficulty-aware Policy Optimization (DiPO), a reinforcement learning-based LRM training framework. DiPO encourages LRM to spontaneously model task complexity, and integrates them into reinforcement learning framework to adjust the generation preferences introduced by post-training. A difficulty modeling method based on model self-reasoning is proposed, which significantly reduces the dependence on manual annotation and formalize task complexity. We further develop a difficulty-signal-enhanced reward function that incorporates a penalty for lengthy reasoning while considering reasoning performance and output format. Experimental results indicate that DiPO enables the model to spontaneously adjust inference overhead, significantly reducing redundant tokens without losing performance due to thought compression.
Published: 2026-01-29T08:56:45+00:00
Venue: arXiv
Score: 0.783 (must_read)</description><content:encoded>&lt;p&gt;&lt;strong&gt;Authors:&lt;/strong&gt; Qian Wan; Ziao Xu; Luona Wei; Xiaoxuan Shen; Jianwen Sun&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Journal:&lt;/strong&gt; arXiv&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Relevance:&lt;/strong&gt; 0.783 (must_read)&lt;/p&gt;
&lt;p&gt;Large Reasoning Models (LRMs) achieve explicit chain-of-thought expansion by imitating deep thinking behaviors of humans, demonstrating excellent performance in complex task scenarios. However, the deep-thinking mode often leads to unnecessarily lengthy reasoning and resource inefficiency when handling simple tasks. This overthinking phenomenon may arise from the generation preference triggered by the reward function during post-training. Existing research attempts to mitigate overthinking from the perspective of prompt design or model training, but generally underestimates the importance of task difficulty awareness, which makes it difficult for LRMs to effectively allocate reasoning resources. In this paper, we propose Difficulty-aware Policy Optimization (DiPO), a reinforcement learning-based LRM training framework. DiPO encourages LRM to spontaneously model task complexity, and integrates them into reinforcement learning framework to adjust the generation preferences introduced by post-training. A difficulty modeling method based on model self-reasoning is proposed, which significantly reduces the dependence on manual annotation and formalize task complexity. We further develop a difficulty-signal-enhanced reward function that incorporates a penalty for lengthy reasoning while considering reasoning performance and output format. Experimental results indicate that DiPO enables the model to spontaneously adjust inference overhead, significantly reducing redundant tokens without losing performance due to thought compression.&lt;/p&gt;</content:encoded></item><item><title>Implicit Non-Causal Factors are Out via Dataset Splitting for Domain Generalization Object Detection</title><link>https://arxiv.org/abs/2601.19127v1</link><guid>http://arxiv.org/abs/2601.19127v1</guid><pubDate>Tue, 27 Jan 2026 02:52:13 +0000</pubDate><dc:creator>Zhilong Zhang</dc:creator><dc:creator>Lei Zhang</dc:creator><dc:creator>Qing He</dc:creator><dc:creator>Shuyin Xia</dc:creator><dc:creator>Guoyin Wang</dc:creator><dc:creator>Fuxiang Huang</dc:creator><dc:type>preprint</dc:type><prism:publicationName>arXiv</prism:publicationName><description>Open world object detection faces a significant challenge in domain-invariant representation, i.e., implicit non-causal factors. Most domain generalization (DG) methods based on domain adversarial learning (DAL) pay much attention to learn domain-invariant information, but often overlook the potential non-causal factors. We unveil two critical causes: 1) The domain discriminator-based DAL method is subject to the extremely sparse domain label, i.e., assigning only one domain label to each dataset, thus can only associate explicit non-causal factor, which is incredibly limited. 2) The non-causal factors, induced by unidentified data bias, are excessively implicit and cannot be solely discerned by conventional DAL paradigm. Based on these key findings, inspired by the Granular-Ball perspective, we propose an improved DAL method, i.e., GB-DAL. The proposed GB-DAL utilizes Prototype-based Granular Ball Splitting (PGBS) module to generate more dense domains from limited datasets, akin to more fine-grained granular balls, indicating more potential non-causal factors. Inspired by adversarial perturbations akin to non-causal factors, we propose a Simulated Non-causal Factors (SNF) module as a means of data augmentation to reduce the implicitness of non-causal factors, and facilitate the training of GB-DAL. Comparative experiments on numerous benchmarks demonstrate that our method achieves better generalization performance in novel circumstances.
Published: 2026-01-27T02:52:13+00:00
Venue: arXiv
Score: 0.783 (must_read)</description><content:encoded>&lt;p&gt;&lt;strong&gt;Authors:&lt;/strong&gt; Zhilong Zhang; Lei Zhang; Qing He; Shuyin Xia; Guoyin Wang; Fuxiang Huang&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Journal:&lt;/strong&gt; arXiv&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Relevance:&lt;/strong&gt; 0.783 (must_read)&lt;/p&gt;
&lt;p&gt;Open world object detection faces a significant challenge in domain-invariant representation, i.e., implicit non-causal factors. Most domain generalization (DG) methods based on domain adversarial learning (DAL) pay much attention to learn domain-invariant information, but often overlook the potential non-causal factors. We unveil two critical causes: 1) The domain discriminator-based DAL method is subject to the extremely sparse domain label, i.e., assigning only one domain label to each dataset, thus can only associate explicit non-causal factor, which is incredibly limited. 2) The non-causal factors, induced by unidentified data bias, are excessively implicit and cannot be solely discerned by conventional DAL paradigm. Based on these key findings, inspired by the Granular-Ball perspective, we propose an improved DAL method, i.e., GB-DAL. The proposed GB-DAL utilizes Prototype-based Granular Ball Splitting (PGBS) module to generate more dense domains from limited datasets, akin to more fine-grained granular balls, indicating more potential non-causal factors. Inspired by adversarial perturbations akin to non-causal factors, we propose a Simulated Non-causal Factors (SNF) module as a means of data augmentation to reduce the implicitness of non-causal factors, and facilitate the training of GB-DAL. Comparative experiments on numerous benchmarks demonstrate that our method achieves better generalization performance in novel circumstances.&lt;/p&gt;</content:encoded></item><item><title>A Source-Free Approach for Domain Adaptation via Multiview Image Transformation and Latent Space Consistency</title><link>https://arxiv.org/abs/2601.20284v1</link><guid>http://arxiv.org/abs/2601.20284v1</guid><pubDate>Wed, 28 Jan 2026 05:59:20 +0000</pubDate><dc:creator>Debopom Sutradhar</dc:creator><dc:creator>Md. Abdur Rahman</dc:creator><dc:creator>Mohaimenul Azam Khan Raiaan</dc:creator><dc:creator>Reem E. Mohamed</dc:creator><dc:creator>Sami Azam</dc:creator><dc:type>preprint</dc:type><prism:publicationName>arXiv</prism:publicationName><description>Domain adaptation (DA) addresses the challenge of transferring knowledge from a source domain to a target domain where image data distributions may differ. Existing DA methods often require access to source domain data, adversarial training, or complex pseudo-labeling techniques, which are computationally expensive. To address these challenges, this paper introduces a novel source-free domain adaptation method. It is the first approach to use multiview augmentation and latent space consistency techniques to learn domain-invariant features directly from the target domain. Our method eliminates the need for source-target alignment or pseudo-label refinement by learning transferable representations solely from the target domain by enforcing consistency between multiple augmented views in the latent space. Additionally, the method ensures consistency in the learned features by generating multiple augmented views of target domain data and minimizing the distance between their feature representations in the latent space. We also introduce a ConvNeXt-based encoder and design a loss function that combines classification and consistency objectives to drive effective adaptation directly from the target domain. The proposed model achieves an average classification accuracy of 90. 72\%, 84\%, and 97. 12\% in Office-31, Office-Home and Office-Caltech datasets, respectively. Further evaluations confirm that our study improves existing methods by an average classification accuracy increment of +1.23\%, +7.26\%, and +1.77\% on the respective datasets.
Published: 2026-01-28T05:59:20+00:00
Venue: arXiv
Score: 0.782 (must_read)</description><content:encoded>&lt;p&gt;&lt;strong&gt;Authors:&lt;/strong&gt; Debopom Sutradhar; Md. Abdur Rahman; Mohaimenul Azam Khan Raiaan; Reem E. Mohamed; Sami Azam&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Journal:&lt;/strong&gt; arXiv&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Relevance:&lt;/strong&gt; 0.782 (must_read)&lt;/p&gt;
&lt;p&gt;Domain adaptation (DA) addresses the challenge of transferring knowledge from a source domain to a target domain where image data distributions may differ. Existing DA methods often require access to source domain data, adversarial training, or complex pseudo-labeling techniques, which are computationally expensive. To address these challenges, this paper introduces a novel source-free domain adaptation method. It is the first approach to use multiview augmentation and latent space consistency techniques to learn domain-invariant features directly from the target domain. Our method eliminates the need for source-target alignment or pseudo-label refinement by learning transferable representations solely from the target domain by enforcing consistency between multiple augmented views in the latent space. Additionally, the method ensures consistency in the learned features by generating multiple augmented views of target domain data and minimizing the distance between their feature representations in the latent space. We also introduce a ConvNeXt-based encoder and design a loss function that combines classification and consistency objectives to drive effective adaptation directly from the target domain. The proposed model achieves an average classification accuracy of 90. 72\%, 84\%, and 97. 12\% in Office-31, Office-Home and Office-Caltech datasets, respectively. Further evaluations confirm that our study improves existing methods by an average classification accuracy increment of +1.23\%, +7.26\%, and +1.77\% on the respective datasets.&lt;/p&gt;</content:encoded></item><item><title>MetricAnything: Scaling Metric Depth Pretraining with Noisy Heterogeneous Sources</title><link>https://arxiv.org/abs/2601.22054v1</link><guid>http://arxiv.org/abs/2601.22054v1</guid><pubDate>Thu, 29 Jan 2026 17:52:41 +0000</pubDate><dc:creator>Baorui Ma</dc:creator><dc:creator>Jiahui Yang</dc:creator><dc:creator>Donglin Di</dc:creator><dc:creator>Xuancheng Zhang</dc:creator><dc:creator>Jianxun Cui</dc:creator><dc:creator>Hao Li</dc:creator><dc:creator>Yan Xie</dc:creator><dc:creator>Wei Chen</dc:creator><dc:type>preprint</dc:type><prism:publicationName>arXiv</prism:publicationName><description>Scaling has powered recent advances in vision foundation models, yet extending this paradigm to metric depth estimation remains challenging due to heterogeneous sensor noise, camera-dependent biases, and metric ambiguity in noisy cross-source 3D data. We introduce Metric Anything, a simple and scalable pretraining framework that learns metric depth from noisy, diverse 3D sources without manually engineered prompts, camera-specific modeling, or task-specific architectures. Central to our approach is the Sparse Metric Prompt, created by randomly masking depth maps, which serves as a universal interface that decouples spatial reasoning from sensor and camera biases. Using about 20M image-depth pairs spanning reconstructed, captured, and rendered 3D data across 10000 camera models, we demonstrate-for the first time-a clear scaling trend in the metric depth track. The pretrained model excels at prompt-driven tasks such as depth completion, super-resolution and Radar-camera fusion, while its distilled prompt-free student achieves state-of-the-art results on monocular depth estimation, camera intrinsics recovery, single/multi-view metric 3D reconstruction, and VLA planning. We also show that using pretrained ViT of Metric Anything as a visual encoder significantly boosts Multimodal Large Language Model capabilities in spatial intelligence. These results show that metric depth estimation can benefit from the same scaling laws that drive modern foundation models, establishing a new path toward scalable and efficient real-world metric perception. We open-source MetricAnything at http://metric-anything.github.io/metric-anything-io/ to support community research.
Published: 2026-01-29T17:52:41+00:00
Venue: arXiv
Score: 0.782 (must_read)</description><content:encoded>&lt;p&gt;&lt;strong&gt;Authors:&lt;/strong&gt; Baorui Ma; Jiahui Yang; Donglin Di; Xuancheng Zhang; Jianxun Cui; Hao Li; Yan Xie; Wei Chen&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Journal:&lt;/strong&gt; arXiv&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Relevance:&lt;/strong&gt; 0.782 (must_read)&lt;/p&gt;
&lt;p&gt;Scaling has powered recent advances in vision foundation models, yet extending this paradigm to metric depth estimation remains challenging due to heterogeneous sensor noise, camera-dependent biases, and metric ambiguity in noisy cross-source 3D data. We introduce Metric Anything, a simple and scalable pretraining framework that learns metric depth from noisy, diverse 3D sources without manually engineered prompts, camera-specific modeling, or task-specific architectures. Central to our approach is the Sparse Metric Prompt, created by randomly masking depth maps, which serves as a universal interface that decouples spatial reasoning from sensor and camera biases. Using about 20M image-depth pairs spanning reconstructed, captured, and rendered 3D data across 10000 camera models, we demonstrate-for the first time-a clear scaling trend in the metric depth track. The pretrained model excels at prompt-driven tasks such as depth completion, super-resolution and Radar-camera fusion, while its distilled prompt-free student achieves state-of-the-art results on monocular depth estimation, camera intrinsics recovery, single/multi-view metric 3D reconstruction, and VLA planning. We also show that using pretrained ViT of Metric Anything as a visual encoder significantly boosts Multimodal Large Language Model capabilities in spatial intelligence. These results show that metric depth estimation can benefit from the same scaling laws that drive modern foundation models, establishing a new path toward scalable and efficient real-world metric perception. We open-source MetricAnything at http://metric-anything.github.io/metric-anything-io/ to support community research.&lt;/p&gt;</content:encoded></item><item><title>Designing Extremely Memory-Efficient CNNs for On-device Vision and Audio Tasks</title><link>https://doi.org/10.1007/s11263-025-02688-w</link><guid>10.1007/s11263-025-02688-w</guid><pubDate>Thu, 29 Jan 2026 08:48:59 +0000</pubDate><dc:creator>Yoel Park</dc:creator><dc:creator>Jaewook Lee</dc:creator><dc:creator>Seulki Lee</dc:creator><prism:publicationName>International Journal of Computer Vision</prism:publicationName><prism:doi>10.1007/s11263-025-02688-w</prism:doi><description>Abstract In this paper, we introduce a memory-efficient CNN (convolutional neural network), which enables resource-constrained low-end embedded and IoT devices to perform on-device vision and audio tasks, such as image classification, object detection, and audio classification, using extremely low memory, i.e ., only 63 KB on ImageNet classification. Based on the bottleneck block of MobileNet, we propose three design principles that significantly curtail the peak memory usage of a CNN so that it can fit the limited KB memory of the low-end device. First, ‘input segmentation’ divides an input image into a set of patches, including the central patch overlapped with the others, reducing the size (and memory requirement) of a large input image. Second, ‘patch tunneling’ builds independent tunnel-like paths consisting of multiple bottleneck blocks per patch, penetrating through the entire model from an input patch to the last layer of the network, maintaining lightweight memory usage throughout the whole network. Lastly, ‘bottleneck reordering’ rearranges the execution order of convolution operations inside the bottleneck block such that the memory usage remains constant regardless of the size of the convolution output channels. We also present ‘peak memory aware quantization’, enabling desired peak memory reduction in actual deployment of quantized network. The experiment result shows that the proposed network classifies ImageNet with extremely low memory ( i.e ., 63 KB) while achieving competitive top-1 accuracy ( i.e ., 61.58%). To the best of our knowledge, the memory usage of the proposed network is far smaller than state-of-the-art memory-efficient networks, i.e ., up to 89x and 3.1x smaller than MobileNet ( i.e ., 5.6 MB) and MCUNet ( i.e ., 196 KB), respectively.
Published: 2026-01-29T08:48:59+00:00
Venue: International Journal of Computer Vision
Score: 0.782 (must_read)</description><content:encoded>&lt;p&gt;&lt;strong&gt;Authors:&lt;/strong&gt; Yoel Park; Jaewook Lee; Seulki Lee&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Journal:&lt;/strong&gt; International Journal of Computer Vision&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;a href="https://doi.org/10.1007/s11263-025-02688-w"&gt;10.1007/s11263-025-02688-w&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Relevance:&lt;/strong&gt; 0.782 (must_read)&lt;/p&gt;
&lt;p&gt;Abstract In this paper, we introduce a memory-efficient CNN (convolutional neural network), which enables resource-constrained low-end embedded and IoT devices to perform on-device vision and audio tasks, such as image classification, object detection, and audio classification, using extremely low memory, i.e ., only 63 KB on ImageNet classification. Based on the bottleneck block of MobileNet, we propose three design principles that significantly curtail the peak memory usage of a CNN so that it can fit the limited KB memory of the low-end device. First, ‘input segmentation’ divides an input image into a set of patches, including the central patch overlapped with the others, reducing the size (and memory requirement) of a large input image. Second, ‘patch tunneling’ builds independent tunnel-like paths consisting of multiple bottleneck blocks per patch, penetrating through the entire model from an input patch to the last layer of the network, maintaining lightweight memory usage throughout the whole network. Lastly, ‘bottleneck reordering’ rearranges the execution order of convolution operations inside the bottleneck block such that the memory usage remains constant regardless of the size of the convolution output channels. We also present ‘peak memory aware quantization’, enabling desired peak memory reduction in actual deployment of quantized network. The experiment result shows that the proposed network classifies ImageNet with extremely low memory ( i.e ., 63 KB) while achieving competitive top-1 accuracy ( i.e ., 61.58%). To the best of our knowledge, the memory usage of the proposed network is far smaller than state-of-the-art memory-efficient networks, i.e ., up to 89x and 3.1x smaller than MobileNet ( i.e ., 5.6 MB) and MCUNet ( i.e ., 196 KB), respectively.&lt;/p&gt;</content:encoded></item><item><title>Practical Video Object Detection via Feature Selection and Aggregation</title><link>https://doi.org/10.1007/s11263-025-02700-3</link><guid>10.1007/s11263-025-02700-3</guid><pubDate>Fri, 30 Jan 2026 05:07:41 +0000</pubDate><dc:creator>Yuheng Shi</dc:creator><dc:creator>Tong Zhang</dc:creator><dc:creator>Xiaojie Guo</dc:creator><prism:publicationName>International Journal of Computer Vision</prism:publicationName><prism:doi>10.1007/s11263-025-02700-3</prism:doi><description>Compared with still image object detection, video object detection (VOD) needs to particularly concern the high across-frame variation in object appearance, and the diverse deterioration in some frames. In principle, the detection in a certain frame of a video can benefit from information in other frames. Thus, how to effectively aggregate features across different frames is key to the target problem. Most of contemporary aggregation methods are tailored for two-stage detectors, suffering from high computational costs due to the dual-stage nature. On the other hand, although one-stage detectors have made continuous progress in handling static images, their applicability to VOD lacks sufficient exploration. To tackle the above issues, this study invents a very simple yet potent strategy of feature selection and aggregation, gaining significant accuracy at marginal computational expense. Concretely, for cutting the massive computation and memory consumption from the dense prediction characteristic of one-stage object detectors, we first condense candidate features from dense prediction maps. Then, the relationship between a target frame and its reference frames is evaluated to guide the aggregation. Comprehensive experiments and ablation studies are conducted to validate the efficacy of our design, and showcase its advantage over other cutting-edge VOD methods in both effectiveness and efficiency. Notably, our model reaches a new record performance, i.e., 93.0% AP50 at over 30 FPS on the ImageNet VID dataset on a single 3090 GPU, making it a compelling option for large-scale or real-time applications. The implementation is simple, and accessible at https://github.com/YuHengsss/YOLOV .
Published: 2026-01-30T05:07:41+00:00
Venue: International Journal of Computer Vision
Score: 0.781 (must_read)</description><content:encoded>&lt;p&gt;&lt;strong&gt;Authors:&lt;/strong&gt; Yuheng Shi; Tong Zhang; Xiaojie Guo&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Journal:&lt;/strong&gt; International Journal of Computer Vision&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;a href="https://doi.org/10.1007/s11263-025-02700-3"&gt;10.1007/s11263-025-02700-3&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Relevance:&lt;/strong&gt; 0.781 (must_read)&lt;/p&gt;
&lt;p&gt;Compared with still image object detection, video object detection (VOD) needs to particularly concern the high across-frame variation in object appearance, and the diverse deterioration in some frames. In principle, the detection in a certain frame of a video can benefit from information in other frames. Thus, how to effectively aggregate features across different frames is key to the target problem. Most of contemporary aggregation methods are tailored for two-stage detectors, suffering from high computational costs due to the dual-stage nature. On the other hand, although one-stage detectors have made continuous progress in handling static images, their applicability to VOD lacks sufficient exploration. To tackle the above issues, this study invents a very simple yet potent strategy of feature selection and aggregation, gaining significant accuracy at marginal computational expense. Concretely, for cutting the massive computation and memory consumption from the dense prediction characteristic of one-stage object detectors, we first condense candidate features from dense prediction maps. Then, the relationship between a target frame and its reference frames is evaluated to guide the aggregation. Comprehensive experiments and ablation studies are conducted to validate the efficacy of our design, and showcase its advantage over other cutting-edge VOD methods in both effectiveness and efficiency. Notably, our model reaches a new record performance, i.e., 93.0% AP50 at over 30 FPS on the ImageNet VID dataset on a single 3090 GPU, making it a compelling option for large-scale or real-time applications. The implementation is simple, and accessible at https://github.com/YuHengsss/YOLOV .&lt;/p&gt;</content:encoded></item><item><title>Top-Down Coarse-to-Fine Cascade Network for High-Precision Cluster Infrared Small Target Detection</title><link>https://doi.org/10.1109/jstars.2026.3659652</link><guid>10.1109/jstars.2026.3659652</guid><pubDate>Thu, 29 Jan 2026 21:24:12 +0000</pubDate><dc:creator>Tuntun Wang</dc:creator><dc:creator>Jincheng Zhou</dc:creator><dc:creator>Lang Wu</dc:creator><dc:creator>Shuai Yuan</dc:creator><dc:creator>Yuxin Jing</dc:creator><prism:publicationName>IEEE Journal of Selected Topics in Applied Earth Observations and Remote Sensing</prism:publicationName><prism:doi>10.1109/jstars.2026.3659652</prism:doi><description>Infrared small-target detection (IRSTD) holds a critical role in low-visibility and long-distance imaging scenarios, such as UAV tracking and maritime surveillance. However, cluster-IRSTD (CIRSTD) faces more prominent challenges: adjacent targets are prone to feature coupling, dim targets are easily submerged by background clutter, and cluster shapes vary dynamically. Owing to the constraint of independent single-target modeling, current deep-learning methods struggle to effectively handle dense cluster scenarios. Inspired by the human top-down visual attention mechanism, this paper proposes a coarse-to-fine cascaded detection network. First, an Adaptive Regional Attention (ARA) mechanism is tailored specifically for clusters, and a Coarse Cluster Extraction (CCE) module is further designed to extract the overall features of clusters. Subsequently, the Inner Fine Distinction (IFD) module seamlessly integrates the Gaussian and Scharr filters from model-driven approaches into the deep-learning framework, aiming to amplify the saliency of dim targets. It effectively solves the problems of dim target missed detection and adjacent target coupling in clusters. By synergistically integrating holistic cluster information and enhancing target saliency, the proposed Coarse-to-Fine Cascade IRSTD (C2IRSTD) significantly mitigates missed detections within clusters and reduces false alarms outside clusters. The experiments conducted on the DenseSIRST dataset have strongly demonstrated the superior performance of C2IRSTD in highly challenging dense-cluster scenarios. Meanwhile, its leading performance on the SIRST3 dataset in sparse scenarios fully highlights its excellent generalization ability. The code will be public at https://github.com/wangtuntun/C2IRSTD.
Published: 2026-01-29T21:24:12+00:00
Venue: IEEE Journal of Selected Topics in Applied Earth Observations and Remote Sensing
Score: 0.780 (must_read)</description><content:encoded>&lt;p&gt;&lt;strong&gt;Authors:&lt;/strong&gt; Tuntun Wang; Jincheng Zhou; Lang Wu; Shuai Yuan; Yuxin Jing&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Journal:&lt;/strong&gt; IEEE Journal of Selected Topics in Applied Earth Observations and Remote Sensing&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;a href="https://doi.org/10.1109/jstars.2026.3659652"&gt;10.1109/jstars.2026.3659652&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Relevance:&lt;/strong&gt; 0.780 (must_read)&lt;/p&gt;
&lt;p&gt;Infrared small-target detection (IRSTD) holds a critical role in low-visibility and long-distance imaging scenarios, such as UAV tracking and maritime surveillance. However, cluster-IRSTD (CIRSTD) faces more prominent challenges: adjacent targets are prone to feature coupling, dim targets are easily submerged by background clutter, and cluster shapes vary dynamically. Owing to the constraint of independent single-target modeling, current deep-learning methods struggle to effectively handle dense cluster scenarios. Inspired by the human top-down visual attention mechanism, this paper proposes a coarse-to-fine cascaded detection network. First, an Adaptive Regional Attention (ARA) mechanism is tailored specifically for clusters, and a Coarse Cluster Extraction (CCE) module is further designed to extract the overall features of clusters. Subsequently, the Inner Fine Distinction (IFD) module seamlessly integrates the Gaussian and Scharr filters from model-driven approaches into the deep-learning framework, aiming to amplify the saliency of dim targets. It effectively solves the problems of dim target missed detection and adjacent target coupling in clusters. By synergistically integrating holistic cluster information and enhancing target saliency, the proposed Coarse-to-Fine Cascade IRSTD (C2IRSTD) significantly mitigates missed detections within clusters and reduces false alarms outside clusters. The experiments conducted on the DenseSIRST dataset have strongly demonstrated the superior performance of C2IRSTD in highly challenging dense-cluster scenarios. Meanwhile, its leading performance on the SIRST3 dataset in sparse scenarios fully highlights its excellent generalization ability. The code will be public at https://github.com/wangtuntun/C2IRSTD.&lt;/p&gt;</content:encoded></item><item><title>IHDCP: Single Image Dehazing Using Inverted Haze Density Correction Prior</title><link>https://doi.org/10.1109/tip.2026.3657636</link><guid>10.1109/tip.2026.3657636</guid><pubDate>Thu, 29 Jan 2026 21:27:16 +0000</pubDate><dc:creator>Yun Liu</dc:creator><dc:creator>Tao Li</dc:creator><dc:creator>Chunping Tan</dc:creator><dc:creator>Wenqi Ren</dc:creator><dc:creator>Cosmin Ancuti</dc:creator><dc:creator>Weisi Lin</dc:creator><prism:publicationName>IEEE Transactions on Image Processing</prism:publicationName><prism:doi>10.1109/tip.2026.3657636</prism:doi><description>Image dehazing, a crucial task in low-level vision, supports numerous practical applications, such as autonomous driving, remote sensing, and surveillance. This paper proposes IHDCP, a novel Inverted Haze Density Correction Prior for efficient single image dehazing. It is observed that the medium transmission can be effectively modeled from the inverted haze density map using correction functions with various gamma coefficients. Based on this observation, a pixel-wise gamma correction coefficient is introduced to formulate the transmission as a function of the inverted haze density map. To estimate the transmission, IHDCP is first incorporated into the classic atmospheric scattering model (ASM), leading to a transcendental equation that is subsequently simplified to a quadratic form with a single unknown parameter using the Taylor expansion. Then, boundary constraints are designed to estimate this model parameter, and the gamma correction coefficient map is derived via the Vieta theorem. Finally, the haze-free result is recovered through ASM inversion. Experimental results on diverse synthetic and real-world datasets verify that our algorithm not only provides visually appealing dehazing performance with high computational efficiency, but also outperforms several state-of-the-art dehazing approaches in both subjective and objective evaluations. Moreover, our IHDCP generalizes well to various types of degraded scenes. Our code is available at https://github.com/TaoLi-TL/IHDCP.
Published: 2026-01-29T21:27:16+00:00
Venue: IEEE Transactions on Image Processing
Score: 0.780 (must_read)</description><content:encoded>&lt;p&gt;&lt;strong&gt;Authors:&lt;/strong&gt; Yun Liu; Tao Li; Chunping Tan; Wenqi Ren; Cosmin Ancuti; Weisi Lin&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Journal:&lt;/strong&gt; IEEE Transactions on Image Processing&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;a href="https://doi.org/10.1109/tip.2026.3657636"&gt;10.1109/tip.2026.3657636&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Relevance:&lt;/strong&gt; 0.780 (must_read)&lt;/p&gt;
&lt;p&gt;Image dehazing, a crucial task in low-level vision, supports numerous practical applications, such as autonomous driving, remote sensing, and surveillance. This paper proposes IHDCP, a novel Inverted Haze Density Correction Prior for efficient single image dehazing. It is observed that the medium transmission can be effectively modeled from the inverted haze density map using correction functions with various gamma coefficients. Based on this observation, a pixel-wise gamma correction coefficient is introduced to formulate the transmission as a function of the inverted haze density map. To estimate the transmission, IHDCP is first incorporated into the classic atmospheric scattering model (ASM), leading to a transcendental equation that is subsequently simplified to a quadratic form with a single unknown parameter using the Taylor expansion. Then, boundary constraints are designed to estimate this model parameter, and the gamma correction coefficient map is derived via the Vieta theorem. Finally, the haze-free result is recovered through ASM inversion. Experimental results on diverse synthetic and real-world datasets verify that our algorithm not only provides visually appealing dehazing performance with high computational efficiency, but also outperforms several state-of-the-art dehazing approaches in both subjective and objective evaluations. Moreover, our IHDCP generalizes well to various types of degraded scenes. Our code is available at https://github.com/TaoLi-TL/IHDCP.&lt;/p&gt;</content:encoded></item><item><title>Deeply Learned Robust Matrix Completion for Large-scale Low-rank Data Recovery</title><link>https://doi.org/10.1109/tpami.2026.3659041</link><guid>10.1109/tpami.2026.3659041</guid><pubDate>Thu, 29 Jan 2026 21:23:36 +0000</pubDate><dc:creator>HanQin Cai</dc:creator><dc:creator>Chandra Kundu</dc:creator><dc:creator>Jialin Liu</dc:creator><dc:creator>Wotao Yin</dc:creator><prism:publicationName>IEEE Transactions on Pattern Analysis and Machine Intelligence</prism:publicationName><prism:doi>10.1109/tpami.2026.3659041</prism:doi><description>Robust matrix completion (RMC) is a widely used machine learning tool that simultaneously tackles two critical issues in low-rank data analysis: missing data entries and extreme outliers. This paper proposes a novel scalable and learnable non-convex approach, coined Learned Robust Matrix Completion (LRMC), for large-scale RMC problems. LRMC enjoys low computational complexity with linear convergence. Motivated by the proposed theorem, the free parameters of LRMC can be effectively learned via deep unfolding to achieve optimum performance. Furthermore, this paper proposes a flexible feedforward-recurrent-mixed neural network framework that extends deep unfolding from fixed-number iterations to infinite iterations. The superior empirical performance of LRMC is verified with extensive experiments against state-of-the-art on synthetic datasets and real applications, including video background subtraction, ultrasound imaging, face modeling, and cloud removal from satellite imagery.
Published: 2026-01-29T21:23:36+00:00
Venue: IEEE Transactions on Pattern Analysis and Machine Intelligence
Score: 0.780 (must_read)</description><content:encoded>&lt;p&gt;&lt;strong&gt;Authors:&lt;/strong&gt; HanQin Cai; Chandra Kundu; Jialin Liu; Wotao Yin&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Journal:&lt;/strong&gt; IEEE Transactions on Pattern Analysis and Machine Intelligence&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;a href="https://doi.org/10.1109/tpami.2026.3659041"&gt;10.1109/tpami.2026.3659041&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Relevance:&lt;/strong&gt; 0.780 (must_read)&lt;/p&gt;
&lt;p&gt;Robust matrix completion (RMC) is a widely used machine learning tool that simultaneously tackles two critical issues in low-rank data analysis: missing data entries and extreme outliers. This paper proposes a novel scalable and learnable non-convex approach, coined Learned Robust Matrix Completion (LRMC), for large-scale RMC problems. LRMC enjoys low computational complexity with linear convergence. Motivated by the proposed theorem, the free parameters of LRMC can be effectively learned via deep unfolding to achieve optimum performance. Furthermore, this paper proposes a flexible feedforward-recurrent-mixed neural network framework that extends deep unfolding from fixed-number iterations to infinite iterations. The superior empirical performance of LRMC is verified with extensive experiments against state-of-the-art on synthetic datasets and real applications, including video background subtraction, ultrasound imaging, face modeling, and cloud removal from satellite imagery.&lt;/p&gt;</content:encoded></item><item><title>Semi-Supervised Masked Autoencoders: Unlocking Vision Transformer Potential with Limited Data</title><link>https://arxiv.org/abs/2601.20072v1</link><guid>http://arxiv.org/abs/2601.20072v1</guid><pubDate>Tue, 27 Jan 2026 21:32:22 +0000</pubDate><dc:creator>Atik Faysal</dc:creator><dc:creator>Mohammad Rostami</dc:creator><dc:creator>Reihaneh Gh. Roshan</dc:creator><dc:creator>Nikhil Muralidhar</dc:creator><dc:creator>Huaxia Wang</dc:creator><dc:type>preprint</dc:type><prism:publicationName>arXiv</prism:publicationName><description>We address the challenge of training Vision Transformers (ViTs) when labeled data is scarce but unlabeled data is abundant. We propose Semi-Supervised Masked Autoencoder (SSMAE), a framework that jointly optimizes masked image reconstruction and classification using both unlabeled and labeled samples with dynamically selected pseudo-labels. SSMAE introduces a validation-driven gating mechanism that activates pseudo-labeling only after the model achieves reliable, high-confidence predictions that are consistent across both weakly and strongly augmented views of the same image, reducing confirmation bias. On CIFAR-10 and CIFAR-100, SSMAE consistently outperforms supervised ViT and fine-tuned MAE, with the largest gains in low-label regimes (+9.24% over ViT on CIFAR-10 with 10% labels). Our results demonstrate that when pseudo-labels are introduced is as important as how they are generated for data-efficient transformer training. Codes are available at https://github.com/atik666/ssmae.
Published: 2026-01-27T21:32:22+00:00
Venue: arXiv
Score: 0.779 (must_read)</description><content:encoded>&lt;p&gt;&lt;strong&gt;Authors:&lt;/strong&gt; Atik Faysal; Mohammad Rostami; Reihaneh Gh. Roshan; Nikhil Muralidhar; Huaxia Wang&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Journal:&lt;/strong&gt; arXiv&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Relevance:&lt;/strong&gt; 0.779 (must_read)&lt;/p&gt;
&lt;p&gt;We address the challenge of training Vision Transformers (ViTs) when labeled data is scarce but unlabeled data is abundant. We propose Semi-Supervised Masked Autoencoder (SSMAE), a framework that jointly optimizes masked image reconstruction and classification using both unlabeled and labeled samples with dynamically selected pseudo-labels. SSMAE introduces a validation-driven gating mechanism that activates pseudo-labeling only after the model achieves reliable, high-confidence predictions that are consistent across both weakly and strongly augmented views of the same image, reducing confirmation bias. On CIFAR-10 and CIFAR-100, SSMAE consistently outperforms supervised ViT and fine-tuned MAE, with the largest gains in low-label regimes (+9.24% over ViT on CIFAR-10 with 10% labels). Our results demonstrate that when pseudo-labels are introduced is as important as how they are generated for data-efficient transformer training. Codes are available at https://github.com/atik666/ssmae.&lt;/p&gt;</content:encoded></item><item><title>Tackling Ill-Posedness of Reversible Image Conversion With Well-Posed Invertible Network</title><link>https://doi.org/10.1109/tpami.2026.3659125</link><guid>10.1109/tpami.2026.3659125</guid><pubDate>Thu, 29 Jan 2026 21:23:36 +0000</pubDate><dc:creator>Yuanfei Huang</dc:creator><dc:creator>Hua Huang</dc:creator><prism:publicationName>IEEE Transactions on Pattern Analysis and Machine Intelligence</prism:publicationName><prism:doi>10.1109/tpami.2026.3659125</prism:doi><description>Reversible image conversion (RIC) suffers from ill-posedness issues due to its forward conversion process being considered an underdetermined system. Despite employing invertible neural networks (INN), existing RIC methods intrinsically remain ill-posed as inevitably introducing uncertainty by incorporating randomly sampled variables. To tackle the ill-posedness dilemma, we focus on developing a reliable approximate left inverse for the underdetermined system by constructing an overdetermined system with a non-zero Gram determinant, thus ensuring a well-posed solution. Based on this principle, we propose a well-posed invertible 1 imes 1 1 imes 1 convolution (WIC), which eliminates the reliance on random variable sampling and enables the development of well-posed invertible networks. Furthermore, we design two innovative networks, WIN-Naïve and WIN, with the latter incorporating advanced skip-connections to enhance long-term memory. Our methods are evaluated across diverse RIC tasks, including reversible image hiding, image rescaling, and image decolorization, consistently achieving state-of-the-art performance. Extensive experiments validate the effectiveness of our approach, demonstrating its ability to overcome the bottlenecks of existing RIC solutions and setting a new benchmark in the field. Codes are available in https://github.com/BNU-ERC-ITEA/WIN.
Published: 2026-01-29T21:23:36+00:00
Venue: IEEE Transactions on Pattern Analysis and Machine Intelligence
Score: 0.779 (must_read)</description><content:encoded>&lt;p&gt;&lt;strong&gt;Authors:&lt;/strong&gt; Yuanfei Huang; Hua Huang&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Journal:&lt;/strong&gt; IEEE Transactions on Pattern Analysis and Machine Intelligence&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;a href="https://doi.org/10.1109/tpami.2026.3659125"&gt;10.1109/tpami.2026.3659125&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Relevance:&lt;/strong&gt; 0.779 (must_read)&lt;/p&gt;
&lt;p&gt;Reversible image conversion (RIC) suffers from ill-posedness issues due to its forward conversion process being considered an underdetermined system. Despite employing invertible neural networks (INN), existing RIC methods intrinsically remain ill-posed as inevitably introducing uncertainty by incorporating randomly sampled variables. To tackle the ill-posedness dilemma, we focus on developing a reliable approximate left inverse for the underdetermined system by constructing an overdetermined system with a non-zero Gram determinant, thus ensuring a well-posed solution. Based on this principle, we propose a well-posed invertible 1 imes 1 1 imes 1 convolution (WIC), which eliminates the reliance on random variable sampling and enables the development of well-posed invertible networks. Furthermore, we design two innovative networks, WIN-Naïve and WIN, with the latter incorporating advanced skip-connections to enhance long-term memory. Our methods are evaluated across diverse RIC tasks, including reversible image hiding, image rescaling, and image decolorization, consistently achieving state-of-the-art performance. Extensive experiments validate the effectiveness of our approach, demonstrating its ability to overcome the bottlenecks of existing RIC solutions and setting a new benchmark in the field. Codes are available in https://github.com/BNU-ERC-ITEA/WIN.&lt;/p&gt;</content:encoded></item><item><title>Quad-pol reconstruction of dual-pol SAR data via a physically constrained diffusion model for building damage assessment</title><link>https://doi.org/10.1016/j.jag.2026.105132</link><guid>10.1016/j.jag.2026.105132</guid><pubDate>Fri, 30 Jan 2026 10:02:57 +0000</pubDate><dc:creator>Zihuan Guo</dc:creator><dc:creator>Hong Zhang</dc:creator><dc:creator>Xiao-Ming Li</dc:creator><dc:creator>Yukun Fan</dc:creator><dc:creator>Haoxuan Duan</dc:creator><dc:creator>Qiming Zeng</dc:creator><dc:creator>Ji Ge</dc:creator><dc:creator>Chao Wang</dc:creator><prism:publicationName>International Journal of Applied Earth Observation and Geoinformation</prism:publicationName><prism:doi>10.1016/j.jag.2026.105132</prism:doi><description>Quad-polarimetric (quad-pol) synthetic aperture radar (SAR) data provides crucial polarimetric information for post-disaster building damage assessment. However, most current spaceborne SAR platforms prioritize dual-polarization (dual-pol) mode, which ensures high temporal and spatial data availability but limits damage analysis accuracy due to the absence of some polarimetric information. Existing methods for reconstructing dual-pol to quad-pol SAR data often fail to ensure that the reconstructed data meets fundamental physical properties, while traditional building damage detection methods still struggle to accurately capture complex depolarization effects. To address these challenges, this paper proposes a diffusion model-based method for reconstructing dual-pol data to quad-pol data, applied to post-earthquake building damage analysis. The method introduces a Positive Semi-definite Constraint Module and a Plug-and-Play SVD Parameter Fine-tuning Module to ensure the physical validity and accuracy of the reconstructed data. Additionally, a Stokes vector-based Degree of Polarization frequency analysis method is proposed to enhance the description of depolarization information. A multi-dimensional polarimetric feature combination is constructed for grid-level building damage assessment. Experiments on Gaofen-3, ALOS-2/PALSAR-2, and Sentinel-1 data show that the proposed method performs optimally in complex scenarios, with all pixels meeting the positive semi-definite constraint. Compared to the original dual-pol SAR data, building damage assessment using the reconstructed quad-pol SAR data resulted in an F1 score improvement of 16.3% and 8.4% for detecting moderately and severely damaged buildings, respectively. This research provides crucial technical support for fully harnessing the potential of dual-pol SAR data in building damage assessment.
Published: 2026-01-30T10:02:57+00:00
Venue: International Journal of Applied Earth Observation and Geoinformation
Score: 0.778 (must_read)</description><content:encoded>&lt;p&gt;&lt;strong&gt;Authors:&lt;/strong&gt; Zihuan Guo; Hong Zhang; Xiao-Ming Li; Yukun Fan; Haoxuan Duan; Qiming Zeng; Ji Ge; Chao Wang&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Journal:&lt;/strong&gt; International Journal of Applied Earth Observation and Geoinformation&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;a href="https://doi.org/10.1016/j.jag.2026.105132"&gt;10.1016/j.jag.2026.105132&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Relevance:&lt;/strong&gt; 0.778 (must_read)&lt;/p&gt;
&lt;p&gt;Quad-polarimetric (quad-pol) synthetic aperture radar (SAR) data provides crucial polarimetric information for post-disaster building damage assessment. However, most current spaceborne SAR platforms prioritize dual-polarization (dual-pol) mode, which ensures high temporal and spatial data availability but limits damage analysis accuracy due to the absence of some polarimetric information. Existing methods for reconstructing dual-pol to quad-pol SAR data often fail to ensure that the reconstructed data meets fundamental physical properties, while traditional building damage detection methods still struggle to accurately capture complex depolarization effects. To address these challenges, this paper proposes a diffusion model-based method for reconstructing dual-pol data to quad-pol data, applied to post-earthquake building damage analysis. The method introduces a Positive Semi-definite Constraint Module and a Plug-and-Play SVD Parameter Fine-tuning Module to ensure the physical validity and accuracy of the reconstructed data. Additionally, a Stokes vector-based Degree of Polarization frequency analysis method is proposed to enhance the description of depolarization information. A multi-dimensional polarimetric feature combination is constructed for grid-level building damage assessment. Experiments on Gaofen-3, ALOS-2/PALSAR-2, and Sentinel-1 data show that the proposed method performs optimally in complex scenarios, with all pixels meeting the positive semi-definite constraint. Compared to the original dual-pol SAR data, building damage assessment using the reconstructed quad-pol SAR data resulted in an F1 score improvement of 16.3% and 8.4% for detecting moderately and severely damaged buildings, respectively. This research provides crucial technical support for fully harnessing the potential of dual-pol SAR data in building damage assessment.&lt;/p&gt;</content:encoded></item><item><title>Unsupervised Multimodal Graph Completion Networks with Multi-level Contrastiveness for Modality-missing Conversation Understanding</title><link>https://doi.org/10.1016/j.inffus.2026.104197</link><guid>10.1016/j.inffus.2026.104197</guid><pubDate>Fri, 30 Jan 2026 00:38:35 +0000</pubDate><dc:creator>Sichao Fu</dc:creator><dc:creator>Songren Peng</dc:creator><dc:creator>Bin Zou</dc:creator><dc:creator>Xiao-Yuan Jing</dc:creator><dc:creator>Wei Yu</dc:creator><dc:creator>Qinmu Peng</dc:creator><dc:creator>Xinge You</dc:creator><prism:publicationName>Information Fusion</prism:publicationName><prism:doi>10.1016/j.inffus.2026.104197</prism:doi><description>Multimodal conversation understanding has received increasing research interest in recent years, which aims to integrate multimodal conversation information to improve the accuracy of computer understanding of user intentions. However, the existing multimodal conversation understanding methods often suffer from a conversation modality missing challenge, which seriously damages their superior performance. Recently emerged imputation-based incomplete multimodal learning (I 2 ML) provides an effective solution, which aims to reconstruct the missing modality features under the supervision of a downstream task. Such reliance on labels causes both the bias of the reconstructed modality features and the limitation of their scope of application. Besides, these proposed I 2 ML methods independently consider the missing modality features reconstruction process between different utterances, which further leads to a specific utterance over-reliance (model sub-optimal) issue. To address the above-mentioned issues, a more general unsupervised I 2 ML is proposed to effectively improve the performance of the modality-missing conversation understanding (M 2 CU) task, termed unsupervised multimodal graph completion networks (UMGCN). Specifically, to improve the accuracy of each reconstructed modality feature, an effective missing modality recovery module is designed to enhance the information interaction process between different utterances for generating robust missing modality recovery features. Then, a multi-level graph contrastive loss on the cross-structure and cross-view level is proposed to learn utterance-general conversation representations by maximizing the mutual information between the same conversation representations across different structures and views. Finally, the learned utterance-general conversation representations can be applied to arbitrary M 2 CU tasks. Extensive experiments on four datasets, seven missing rates and two M 2 CU tasks show that our proposed UMGCN outperforms the existing incomplete multimodal learning methods.
Published: 2026-01-30T00:38:35+00:00
Venue: Information Fusion
Score: 0.777 (must_read)</description><content:encoded>&lt;p&gt;&lt;strong&gt;Authors:&lt;/strong&gt; Sichao Fu; Songren Peng; Bin Zou; Xiao-Yuan Jing; Wei Yu; Qinmu Peng; Xinge You&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Journal:&lt;/strong&gt; Information Fusion&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;a href="https://doi.org/10.1016/j.inffus.2026.104197"&gt;10.1016/j.inffus.2026.104197&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Relevance:&lt;/strong&gt; 0.777 (must_read)&lt;/p&gt;
&lt;p&gt;Multimodal conversation understanding has received increasing research interest in recent years, which aims to integrate multimodal conversation information to improve the accuracy of computer understanding of user intentions. However, the existing multimodal conversation understanding methods often suffer from a conversation modality missing challenge, which seriously damages their superior performance. Recently emerged imputation-based incomplete multimodal learning (I 2 ML) provides an effective solution, which aims to reconstruct the missing modality features under the supervision of a downstream task. Such reliance on labels causes both the bias of the reconstructed modality features and the limitation of their scope of application. Besides, these proposed I 2 ML methods independently consider the missing modality features reconstruction process between different utterances, which further leads to a specific utterance over-reliance (model sub-optimal) issue. To address the above-mentioned issues, a more general unsupervised I 2 ML is proposed to effectively improve the performance of the modality-missing conversation understanding (M 2 CU) task, termed unsupervised multimodal graph completion networks (UMGCN). Specifically, to improve the accuracy of each reconstructed modality feature, an effective missing modality recovery module is designed to enhance the information interaction process between different utterances for generating robust missing modality recovery features. Then, a multi-level graph contrastive loss on the cross-structure and cross-view level is proposed to learn utterance-general conversation representations by maximizing the mutual information between the same conversation representations across different structures and views. Finally, the learned utterance-general conversation representations can be applied to arbitrary M 2 CU tasks. Extensive experiments on four datasets, seven missing rates and two M 2 CU tasks show that our proposed UMGCN outperforms the existing incomplete multimodal learning methods.&lt;/p&gt;</content:encoded></item><item><title>Fine-Grained Alignment Supervision Matters in Vision-and-Language Navigation</title><link>https://doi.org/10.1109/tpami.2026.3658949</link><guid>10.1109/tpami.2026.3658949</guid><pubDate>Thu, 29 Jan 2026 21:23:36 +0000</pubDate><dc:creator>Keji He</dc:creator><dc:creator>Yan Huang</dc:creator><dc:creator>Ya Jing</dc:creator><dc:creator>Qi Wu</dc:creator><dc:creator>Liang Wang</dc:creator><prism:publicationName>IEEE Transactions on Pattern Analysis and Machine Intelligence</prism:publicationName><prism:doi>10.1109/tpami.2026.3658949</prism:doi><description>The Vision-and-Language Navigation (VLN) task involves an agent navigating within 3D indoor environments based on provided instructions. Achieving cross-modal alignment presents one of the most critical challenges in VLN, as the predicted trajectory needs to precisely align with the given instruction. This paper focuses on addressing cross-modal alignment in VLN from a fine-grained perspective. Firstly, to address the issue of weak cross-modal alignment supervision arising from coarse-grained data, we introduce a human-annotated fine-grained VLN dataset called Landmark-RxR. This dataset aims to offer precise, fine-grained supervision for VLN. Secondly, in order to comprehensively demonstrate the potential and advantage of the fine-grained data from Landmark-RxR, we explore the core components of the training process that depend on the characteristics of the training data. These components include data augmentation, training paradigm, reward shaping, and navigation loss design. Leveraging our fine-grained data, we carefully design methods for handling them and introduce a novel evaluation mechanism. The experimental results demonstrate that the fine-grained data can effectively improve the agent's cross-modal alignment ability. Access to the Landmark-RxR dataset can be obtained from https://github.com/hekj/Landmark-RxR.
Published: 2026-01-29T21:23:36+00:00
Venue: IEEE Transactions on Pattern Analysis and Machine Intelligence
Score: 0.777 (must_read)</description><content:encoded>&lt;p&gt;&lt;strong&gt;Authors:&lt;/strong&gt; Keji He; Yan Huang; Ya Jing; Qi Wu; Liang Wang&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Journal:&lt;/strong&gt; IEEE Transactions on Pattern Analysis and Machine Intelligence&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;a href="https://doi.org/10.1109/tpami.2026.3658949"&gt;10.1109/tpami.2026.3658949&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Relevance:&lt;/strong&gt; 0.777 (must_read)&lt;/p&gt;
&lt;p&gt;The Vision-and-Language Navigation (VLN) task involves an agent navigating within 3D indoor environments based on provided instructions. Achieving cross-modal alignment presents one of the most critical challenges in VLN, as the predicted trajectory needs to precisely align with the given instruction. This paper focuses on addressing cross-modal alignment in VLN from a fine-grained perspective. Firstly, to address the issue of weak cross-modal alignment supervision arising from coarse-grained data, we introduce a human-annotated fine-grained VLN dataset called Landmark-RxR. This dataset aims to offer precise, fine-grained supervision for VLN. Secondly, in order to comprehensively demonstrate the potential and advantage of the fine-grained data from Landmark-RxR, we explore the core components of the training process that depend on the characteristics of the training data. These components include data augmentation, training paradigm, reward shaping, and navigation loss design. Leveraging our fine-grained data, we carefully design methods for handling them and introduce a novel evaluation mechanism. The experimental results demonstrate that the fine-grained data can effectively improve the agent&amp;#x27;s cross-modal alignment ability. Access to the Landmark-RxR dataset can be obtained from https://github.com/hekj/Landmark-RxR.&lt;/p&gt;</content:encoded></item></channel></rss>