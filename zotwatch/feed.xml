<?xml version='1.0' encoding='utf-8'?>
<rss xmlns:content="http://purl.org/rss/1.0/modules/content/" xmlns:dc="http://purl.org/dc/elements/1.1/" xmlns:prism="http://prismstandard.org/namespaces/basic/3.0/" version="2.0"><channel><title>ZotWatch Feed</title><link>https://ehehe.cn/zotwatch/</link><description>AI-assisted literature watch</description><lastBuildDate>Mon, 01 Dec 2025 03:04:09 +0000</lastBuildDate><item><title>GLUE3D: General Language Understanding Evaluation for 3D Point Clouds</title><link>https://doi.org/10.1016/j.inffus.2025.104007</link><guid>10.1016/j.inffus.2025.104007</guid><pubDate>Sat, 29 Nov 2025 04:50:54 +0000</pubDate><dc:creator>Giorgio Mariani</dc:creator><dc:creator>Alessandro Raganato</dc:creator><dc:creator>Simone Melzi</dc:creator><dc:creator>Gabriella Pasi</dc:creator><prism:publicationName>Information Fusion</prism:publicationName><prism:doi>10.1016/j.inffus.2025.104007</prism:doi><description>Multimodal Large Language Models have achieved impressive results on text and image benchmarks, yet their capacity to ground language in 3D geometry is still largely unexplored. Existing 3D evaluations are either confined to specialised domains, such as indoor scans, or hampered by poor texture fidelity, and none allow a fair, modality-aligned comparison with the 2D counterparts. Without a rigorous benchmark, it remains unclear whether current 3D-aware models genuinely grasp shape, colour, pose, and quantity, or merely echo memorised textual priors.
Published: 2025-11-29T04:50:54+00:00
Venue: Information Fusion
Score: 0.816 (must_read)</description><content:encoded>&lt;p&gt;&lt;strong&gt;Authors:&lt;/strong&gt; Giorgio Mariani; Alessandro Raganato; Simone Melzi; Gabriella Pasi&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Journal:&lt;/strong&gt; Information Fusion&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;a href="https://doi.org/10.1016/j.inffus.2025.104007"&gt;10.1016/j.inffus.2025.104007&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Relevance:&lt;/strong&gt; 0.816 (must_read)&lt;/p&gt;
&lt;p&gt;Multimodal Large Language Models have achieved impressive results on text and image benchmarks, yet their capacity to ground language in 3D geometry is still largely unexplored. Existing 3D evaluations are either confined to specialised domains, such as indoor scans, or hampered by poor texture fidelity, and none allow a fair, modality-aligned comparison with the 2D counterparts. Without a rigorous benchmark, it remains unclear whether current 3D-aware models genuinely grasp shape, colour, pose, and quantity, or merely echo memorised textual priors.&lt;/p&gt;</content:encoded></item><item><title>The Duality of Generative AI and Reinforcement Learning in Robotics: A Review</title><link>https://doi.org/10.1016/j.inffus.2025.104003</link><guid>10.1016/j.inffus.2025.104003</guid><pubDate>Sat, 29 Nov 2025 16:05:06 +0000</pubDate><dc:creator>Angelo Moroncelli</dc:creator><dc:creator>Vishal Soni</dc:creator><dc:creator>Marco Forgione</dc:creator><dc:creator>Dario Piga</dc:creator><dc:creator>Blerina Spahiu</dc:creator><dc:creator>Loris Roveda</dc:creator><prism:publicationName>Information Fusion</prism:publicationName><prism:doi>10.1016/j.inffus.2025.104003</prism:doi><description>Recently, generative AI and reinforcement learning (RL) have been redefining what is possible for AI agents that take information flows as input and produce intelligent behavior. As a result, we are seeing similar advancements in embodied AI and robotics for control policy generation. Our review paper examines the integration of generative AI models with RL to advance robotics. Our primary focus is on the duality between generative AI and RL for robotics downstream tasks. Specifically, we investigate: (1) The role of prominent generative AI tools as modular priors for multi-modal input fusion in RL tasks. (2) How RL can train, fine-tune and distill generative models for policy generation, such as VLA models, similarly to RL applications in large language models. We then propose a new taxonomy based on a considerable amount of selected papers.
Published: 2025-11-29T16:05:06+00:00
Venue: Information Fusion
Score: 0.813 (must_read)</description><content:encoded>&lt;p&gt;&lt;strong&gt;Authors:&lt;/strong&gt; Angelo Moroncelli; Vishal Soni; Marco Forgione; Dario Piga; Blerina Spahiu; Loris Roveda&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Journal:&lt;/strong&gt; Information Fusion&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;a href="https://doi.org/10.1016/j.inffus.2025.104003"&gt;10.1016/j.inffus.2025.104003&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Relevance:&lt;/strong&gt; 0.813 (must_read)&lt;/p&gt;
&lt;p&gt;Recently, generative AI and reinforcement learning (RL) have been redefining what is possible for AI agents that take information flows as input and produce intelligent behavior. As a result, we are seeing similar advancements in embodied AI and robotics for control policy generation. Our review paper examines the integration of generative AI models with RL to advance robotics. Our primary focus is on the duality between generative AI and RL for robotics downstream tasks. Specifically, we investigate: (1) The role of prominent generative AI tools as modular priors for multi-modal input fusion in RL tasks. (2) How RL can train, fine-tune and distill generative models for policy generation, such as VLA models, similarly to RL applications in large language models. We then propose a new taxonomy based on a considerable amount of selected papers.&lt;/p&gt;</content:encoded></item><item><title>PAGen: Phase-guided Amplitude Generation for Domain-adaptive Object Detection</title><link>https://arxiv.org/abs/2511.22029v1</link><guid>http://arxiv.org/abs/2511.22029v1</guid><pubDate>Thu, 27 Nov 2025 02:22:37 +0000</pubDate><dc:creator>Shuchen Du</dc:creator><dc:creator>Shuo Lei</dc:creator><dc:creator>Feiran Li</dc:creator><dc:creator>Jiacheng Li</dc:creator><dc:creator>Daisuke Iso</dc:creator><dc:type>preprint</dc:type><prism:publicationName>arXiv</prism:publicationName><description>Unsupervised domain adaptation (UDA) greatly facilitates the deployment of neural networks across diverse environments. However, most state-of-the-art approaches are overly complex, relying on challenging adversarial training strategies, or on elaborate architectural designs with auxiliary models for feature distillation and pseudo-label generation. In this work, we present a simple yet effective UDA method that learns to adapt image styles in the frequency domain to reduce the discrepancy between source and target domains. The proposed approach introduces only a lightweight pre-processing module during training and entirely discards it at inference time, thus incurring no additional computational overhead. We validate our method on domain-adaptive object detection (DAOD) tasks, where ground-truth annotations are easily accessible in source domains (e.g., normal-weather or synthetic conditions) but challenging to obtain in target domains (e.g., adverse weather or low-light scenes). Extensive experiments demonstrate that our method achieves substantial performance gains on multiple benchmarks, highlighting its practicality and effectiveness.
Published: 2025-11-27T02:22:37+00:00
Venue: arXiv
Score: 0.808 (must_read)</description><content:encoded>&lt;p&gt;&lt;strong&gt;Authors:&lt;/strong&gt; Shuchen Du; Shuo Lei; Feiran Li; Jiacheng Li; Daisuke Iso&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Journal:&lt;/strong&gt; arXiv&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Relevance:&lt;/strong&gt; 0.808 (must_read)&lt;/p&gt;
&lt;p&gt;Unsupervised domain adaptation (UDA) greatly facilitates the deployment of neural networks across diverse environments. However, most state-of-the-art approaches are overly complex, relying on challenging adversarial training strategies, or on elaborate architectural designs with auxiliary models for feature distillation and pseudo-label generation. In this work, we present a simple yet effective UDA method that learns to adapt image styles in the frequency domain to reduce the discrepancy between source and target domains. The proposed approach introduces only a lightweight pre-processing module during training and entirely discards it at inference time, thus incurring no additional computational overhead. We validate our method on domain-adaptive object detection (DAOD) tasks, where ground-truth annotations are easily accessible in source domains (e.g., normal-weather or synthetic conditions) but challenging to obtain in target domains (e.g., adverse weather or low-light scenes). Extensive experiments demonstrate that our method achieves substantial performance gains on multiple benchmarks, highlighting its practicality and effectiveness.&lt;/p&gt;</content:encoded></item><item><title>DriveVGGT: Visual Geometry Transformer for Autonomous Driving</title><link>https://arxiv.org/abs/2511.22264v1</link><guid>http://arxiv.org/abs/2511.22264v1</guid><pubDate>Thu, 27 Nov 2025 09:40:43 +0000</pubDate><dc:creator>Xiaosong Jia</dc:creator><dc:creator>Yanhao Liu</dc:creator><dc:creator>Junqi You</dc:creator><dc:creator>Renqiu Xia</dc:creator><dc:creator>Yu Hong</dc:creator><dc:creator>Junchi Yan</dc:creator><dc:type>preprint</dc:type><prism:publicationName>arXiv</prism:publicationName><description>Feed-forward reconstruction has recently gained significant attention, with VGGT being a notable example. However, directly applying VGGT to autonomous driving (AD) systems leads to sub-optimal results due to the different priors between the two tasks. In AD systems, several important new priors need to be considered: (i) The overlap between camera views is minimal, as autonomous driving sensor setups are designed to achieve coverage at a low cost. (ii) The camera intrinsics and extrinsics are known, which introduces more constraints on the output and also enables the estimation of absolute scale. (iii) Relative positions of all cameras remain fixed though the ego vehicle is in motion. To fully integrate these priors into a feed-forward framework, we propose DriveVGGT, a scale-aware 4D reconstruction framework specifically designed for autonomous driving data. Specifically, we propose a Temporal Video Attention (TVA) module to process multi-camera videos independently, which better leverages the spatiotemporal continuity within each single-camera sequence. Then, we propose a Multi-camera Consistency Attention (MCA) module to conduct window attention with normalized relative pose embeddings, aiming to establish consistency relationships across different cameras while restricting each token to attend only to nearby frames. Finally, we extend the standard VGGT heads by adding an absolute scale head and an ego vehicle pose head. Experiments show that DriveVGGT outperforms VGGT, StreamVGGT, fastVGGT on autonomous driving dataset while extensive ablation studies verify effectiveness of the proposed designs.
Published: 2025-11-27T09:40:43+00:00
Venue: arXiv
Score: 0.807 (must_read)</description><content:encoded>&lt;p&gt;&lt;strong&gt;Authors:&lt;/strong&gt; Xiaosong Jia; Yanhao Liu; Junqi You; Renqiu Xia; Yu Hong; Junchi Yan&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Journal:&lt;/strong&gt; arXiv&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Relevance:&lt;/strong&gt; 0.807 (must_read)&lt;/p&gt;
&lt;p&gt;Feed-forward reconstruction has recently gained significant attention, with VGGT being a notable example. However, directly applying VGGT to autonomous driving (AD) systems leads to sub-optimal results due to the different priors between the two tasks. In AD systems, several important new priors need to be considered: (i) The overlap between camera views is minimal, as autonomous driving sensor setups are designed to achieve coverage at a low cost. (ii) The camera intrinsics and extrinsics are known, which introduces more constraints on the output and also enables the estimation of absolute scale. (iii) Relative positions of all cameras remain fixed though the ego vehicle is in motion. To fully integrate these priors into a feed-forward framework, we propose DriveVGGT, a scale-aware 4D reconstruction framework specifically designed for autonomous driving data. Specifically, we propose a Temporal Video Attention (TVA) module to process multi-camera videos independently, which better leverages the spatiotemporal continuity within each single-camera sequence. Then, we propose a Multi-camera Consistency Attention (MCA) module to conduct window attention with normalized relative pose embeddings, aiming to establish consistency relationships across different cameras while restricting each token to attend only to nearby frames. Finally, we extend the standard VGGT heads by adding an absolute scale head and an ego vehicle pose head. Experiments show that DriveVGGT outperforms VGGT, StreamVGGT, fastVGGT on autonomous driving dataset while extensive ablation studies verify effectiveness of the proposed designs.&lt;/p&gt;</content:encoded></item><item><title>M3FNet: Multi-modal multi-temporal multi-scale data fusion network for tree species composition mapping</title><link>https://doi.org/10.1016/j.isprsjprs.2025.11.026</link><guid>10.1016/j.isprsjprs.2025.11.026</guid><pubDate>Sat, 29 Nov 2025 05:22:57 +0000</pubDate><dc:creator>Yuwei Cao</dc:creator><dc:creator>Nicholas C. Coops</dc:creator><dc:creator>Brent A. Murray</dc:creator><dc:creator>Ian Sinclair</dc:creator><dc:creator>Robere-McGugan Geordie</dc:creator><prism:publicationName>ISPRS Journal of Photogrammetry and Remote Sensing</prism:publicationName><prism:doi>10.1016/j.isprsjprs.2025.11.026</prism:doi><description>Accurate estimation and mapping of t ree s pecies c omposition (TSC) is crucial for sustainable forest management. Recent advances in Light Detection and Ranging (lidar) technology and the availability of moderate spatial resolution, surface reflectance time series passive optical imagery offer scalable and efficient approaches for automated TSC estimation. In this research we develop a novel deep learning framework, M3F-Net (Multi-modal, Multi-temporal, and Multi-scale Fusion Network), that integrates multi-temporal Sentinel-2 (S2) imagery and single photon lidar (SPL) data to estimate TSC for nine common species across the 630,000-hectare Romeo Malette Forest in Ontario, Canada. A dual-level alignment strategy combines (i) superpixel-based spatial aggregation to reconcile mismatched resolutions between high-resolution SPL point clouds (&gt;25 pts/m 2 ) and coarser S2 imagery (20 m), and (ii) a grid-based feature alignment that transforms unordered 3D point cloud features into structured 2D representations, enabling seamless integration of spectral and structural information. Within this aligned space, a multi-level Mamba-Fusion module jointly models multi-scale spatial patterns and seasonal dynamics through selective state-space modelling, efficiently capturing long-range dependencies while filtering redundant information. The framework achieves an R 2 score of 0.676, outperforming existing point cloud-based methods by 6% in TSC estimation. For leading species classification, our results are 6% better in terms of weighted F1, using either the TSC-based method or the standalone leading species classification method. Addition of seasonal S2 imagery added a 10% R 2 gain compared to the SPL-only mode. These results underscore the potential of fusing multi-modal and multi-temporal data with deep learning for scalable, high-accurate TSC estimation, offering a robust tool for large-scale management applications.
Published: 2025-11-29T05:22:57+00:00
Venue: ISPRS Journal of Photogrammetry and Remote Sensing
Score: 0.803 (must_read)</description><content:encoded>&lt;p&gt;&lt;strong&gt;Authors:&lt;/strong&gt; Yuwei Cao; Nicholas C. Coops; Brent A. Murray; Ian Sinclair; Robere-McGugan Geordie&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Journal:&lt;/strong&gt; ISPRS Journal of Photogrammetry and Remote Sensing&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;a href="https://doi.org/10.1016/j.isprsjprs.2025.11.026"&gt;10.1016/j.isprsjprs.2025.11.026&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Relevance:&lt;/strong&gt; 0.803 (must_read)&lt;/p&gt;
&lt;p&gt;Accurate estimation and mapping of t ree s pecies c omposition (TSC) is crucial for sustainable forest management. Recent advances in Light Detection and Ranging (lidar) technology and the availability of moderate spatial resolution, surface reflectance time series passive optical imagery offer scalable and efficient approaches for automated TSC estimation. In this research we develop a novel deep learning framework, M3F-Net (Multi-modal, Multi-temporal, and Multi-scale Fusion Network), that integrates multi-temporal Sentinel-2 (S2) imagery and single photon lidar (SPL) data to estimate TSC for nine common species across the 630,000-hectare Romeo Malette Forest in Ontario, Canada. A dual-level alignment strategy combines (i) superpixel-based spatial aggregation to reconcile mismatched resolutions between high-resolution SPL point clouds (&amp;gt;25 pts/m 2 ) and coarser S2 imagery (20 m), and (ii) a grid-based feature alignment that transforms unordered 3D point cloud features into structured 2D representations, enabling seamless integration of spectral and structural information. Within this aligned space, a multi-level Mamba-Fusion module jointly models multi-scale spatial patterns and seasonal dynamics through selective state-space modelling, efficiently capturing long-range dependencies while filtering redundant information. The framework achieves an R 2 score of 0.676, outperforming existing point cloud-based methods by 6% in TSC estimation. For leading species classification, our results are 6% better in terms of weighted F1, using either the TSC-based method or the standalone leading species classification method. Addition of seasonal S2 imagery added a 10% R 2 gain compared to the SPL-only mode. These results underscore the potential of fusing multi-modal and multi-temporal data with deep learning for scalable, high-accurate TSC estimation, offering a robust tool for large-scale management applications.&lt;/p&gt;</content:encoded></item><item><title>Label Noise Learning Based SAR Target Classification Method</title><link>https://doi.org/10.1016/j.neunet.2025.108373</link><guid>10.1016/j.neunet.2025.108373</guid><pubDate>Sat, 29 Nov 2025 23:41:11 +0000</pubDate><dc:creator>Hongqiang Wang</dc:creator><dc:creator>Yuqing Lan</dc:creator><dc:creator>Fuzhan Yue</dc:creator><dc:creator>Zhenghuan Xia</dc:creator><dc:creator>Tao Zhang</dc:creator><dc:creator>Yue Pang</dc:creator><prism:publicationName>Neural Networks</prism:publicationName><prism:doi>10.1016/j.neunet.2025.108373</prism:doi><description>The recognition of Synthetic Aperture Radar (SAR) Target is a critical task in SAR image interpretation. With their exceptional capacity to model complex data structures, Convolutional Neural Networks(CNNs) are now the standard architecture for addressing SAR image classification problems. However, these methods typically require large-scale labeled datasets for training. SAR images are inherently susceptible to both feature and label noise due to the technical sophistication of the imaging process and the high likelihood of human error during annotation. This often leads to a significant degradation in the performance of CNN-based classifiers. To mitigate feature noise, we propose a dynamic L p -norm regularization-based scattering feature extraction method that leverages neural networks to automatically estimate and adapt the regularization parameters at each layer. To address label noise, we further develop a robust representation learning framework for SAR target classification, which enhances model robustness by minimizing the distances between samples and their corresponding class prototypes. Extensive experiments conducted on three widely-used SAR datasets — MSTAR, SAR-ACD, and FUSAR — show that the proposed method consistently achieves robust classification accuracy across label noise levels from 0% to 60%, significantly mitigating the adverse effects of annotation inaccuracies.
Published: 2025-11-29T23:41:11+00:00
Venue: Neural Networks
Score: 0.800 (must_read)</description><content:encoded>&lt;p&gt;&lt;strong&gt;Authors:&lt;/strong&gt; Hongqiang Wang; Yuqing Lan; Fuzhan Yue; Zhenghuan Xia; Tao Zhang; Yue Pang&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Journal:&lt;/strong&gt; Neural Networks&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;a href="https://doi.org/10.1016/j.neunet.2025.108373"&gt;10.1016/j.neunet.2025.108373&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Relevance:&lt;/strong&gt; 0.800 (must_read)&lt;/p&gt;
&lt;p&gt;The recognition of Synthetic Aperture Radar (SAR) Target is a critical task in SAR image interpretation. With their exceptional capacity to model complex data structures, Convolutional Neural Networks(CNNs) are now the standard architecture for addressing SAR image classification problems. However, these methods typically require large-scale labeled datasets for training. SAR images are inherently susceptible to both feature and label noise due to the technical sophistication of the imaging process and the high likelihood of human error during annotation. This often leads to a significant degradation in the performance of CNN-based classifiers. To mitigate feature noise, we propose a dynamic L p -norm regularization-based scattering feature extraction method that leverages neural networks to automatically estimate and adapt the regularization parameters at each layer. To address label noise, we further develop a robust representation learning framework for SAR target classification, which enhances model robustness by minimizing the distances between samples and their corresponding class prototypes. Extensive experiments conducted on three widely-used SAR datasets — MSTAR, SAR-ACD, and FUSAR — show that the proposed method consistently achieves robust classification accuracy across label noise levels from 0% to 60%, significantly mitigating the adverse effects of annotation inaccuracies.&lt;/p&gt;</content:encoded></item><item><title>DivineTree: All-in-One 3D Tree Modeling with Diverse and Fused Visual Guidance</title><link>https://doi.org/10.1016/j.inffus.2025.104013</link><guid>10.1016/j.inffus.2025.104013</guid><pubDate>Sat, 29 Nov 2025 08:10:00 +0000</pubDate><dc:creator>Jiabo Xu</dc:creator><dc:creator>Bo Su</dc:creator><dc:creator>Jingbo Wei</dc:creator><dc:creator>Xiangyun Hu</dc:creator><dc:creator>Hengming Dai</dc:creator><dc:creator>Tao Ke</dc:creator><prism:publicationName>Information Fusion</prism:publicationName><prism:doi>10.1016/j.inffus.2025.104013</prism:doi><description>3D tree modeling is crucial in fields ranging from gaming and film production to environmental science. However, current learning-based methods are typically restricted to a single input modality (e.g., only images or only point clouds), and requiring difficult-to-acquire paired training data for each new input type. To overcome these limitations, we propose DivineTree, a novel method that generates 3D trees from diverse visual guidance-including point clouds (LiDAR or image-matched), images (photos, sketches, paintings), and crown polygons-using a single, unified model, in a zero-shot manner without requiring paired data or retraining. DivineTree consists of two core components: 1) An unconditional diffusion model trained on synthetic data to learn the distribution of 3D tree structures, represented as sequences of 4D line segments. 2) A Point Guidance sampling technique that incorporates diverse visual inputs as spatial constraints during the generative process, guiding the diffusion to produce a 3D tree that matches the input. Extensive experiments demonstrate that our method rapidly generates realistic and geometrically accurate 3D trees. On the challenging 10-forest benchmark for crown-to-tree generation, DivineTree achieves state-of-the-art performance in both geometric accuracy and visual realism. Furthermore, our method enables the fusion of multiple inputs, such as combining both side-view and top-view conditions, to generate 3D tree models that simultaneously satisfy multiple constraints.
Published: 2025-11-29T08:10:00+00:00
Venue: Information Fusion
Score: 0.797 (must_read)</description><content:encoded>&lt;p&gt;&lt;strong&gt;Authors:&lt;/strong&gt; Jiabo Xu; Bo Su; Jingbo Wei; Xiangyun Hu; Hengming Dai; Tao Ke&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Journal:&lt;/strong&gt; Information Fusion&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;a href="https://doi.org/10.1016/j.inffus.2025.104013"&gt;10.1016/j.inffus.2025.104013&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Relevance:&lt;/strong&gt; 0.797 (must_read)&lt;/p&gt;
&lt;p&gt;3D tree modeling is crucial in fields ranging from gaming and film production to environmental science. However, current learning-based methods are typically restricted to a single input modality (e.g., only images or only point clouds), and requiring difficult-to-acquire paired training data for each new input type. To overcome these limitations, we propose DivineTree, a novel method that generates 3D trees from diverse visual guidance-including point clouds (LiDAR or image-matched), images (photos, sketches, paintings), and crown polygons-using a single, unified model, in a zero-shot manner without requiring paired data or retraining. DivineTree consists of two core components: 1) An unconditional diffusion model trained on synthetic data to learn the distribution of 3D tree structures, represented as sequences of 4D line segments. 2) A Point Guidance sampling technique that incorporates diverse visual inputs as spatial constraints during the generative process, guiding the diffusion to produce a 3D tree that matches the input. Extensive experiments demonstrate that our method rapidly generates realistic and geometrically accurate 3D trees. On the challenging 10-forest benchmark for crown-to-tree generation, DivineTree achieves state-of-the-art performance in both geometric accuracy and visual realism. Furthermore, our method enables the fusion of multiple inputs, such as combining both side-view and top-view conditions, to generate 3D tree models that simultaneously satisfy multiple constraints.&lt;/p&gt;</content:encoded></item><item><title>DeepSeekMath-V2: Towards Self-Verifiable Mathematical Reasoning</title><link>https://arxiv.org/abs/2511.22570v1</link><guid>http://arxiv.org/abs/2511.22570v1</guid><pubDate>Thu, 27 Nov 2025 16:01:22 +0000</pubDate><dc:creator>Zhihong Shao</dc:creator><dc:creator>Yuxiang Luo</dc:creator><dc:creator>Chengda Lu</dc:creator><dc:creator>Z. Z. Ren</dc:creator><dc:creator>Jiewen Hu</dc:creator><dc:creator>Tian Ye</dc:creator><dc:creator>Zhibin Gou</dc:creator><dc:creator>Shirong Ma</dc:creator><dc:creator>Xiaokang Zhang</dc:creator><dc:type>preprint</dc:type><prism:publicationName>arXiv</prism:publicationName><description>Large language models have made significant progress in mathematical reasoning, which serves as an important testbed for AI and could impact scientific research if further advanced. By scaling reasoning with reinforcement learning that rewards correct final answers, LLMs have improved from poor performance to saturating quantitative reasoning competitions like AIME and HMMT in one year. However, this approach faces fundamental limitations. Pursuing higher final answer accuracy doesn't address a key issue: correct answers don't guarantee correct reasoning. Moreover, many mathematical tasks like theorem proving require rigorous step-by-step derivation rather than numerical answers, making final answer rewards inapplicable. To push the limits of deep reasoning, we believe it is necessary to verify the comprehensiveness and rigor of mathematical reasoning. Self-verification is particularly important for scaling test-time compute, especially for open problems without known solutions. Towards self-verifiable mathematical reasoning, we investigate how to train an accurate and faithful LLM-based verifier for theorem proving. We then train a proof generator using the verifier as the reward model, and incentivize the generator to identify and resolve as many issues as possible in their own proofs before finalizing them. To maintain the generation-verification gap as the generator becomes stronger, we propose to scale verification compute to automatically label new hard-to-verify proofs, creating training data to further improve the verifier. Our resulting model, DeepSeekMath-V2, demonstrates strong theorem-proving capabilities, achieving gold-level scores on IMO 2025 and CMO 2024 and a near-perfect 118/120 on Putnam 2024 with scaled test-time compute.
Published: 2025-11-27T16:01:22+00:00
Venue: arXiv
Score: 0.796 (must_read)</description><content:encoded>&lt;p&gt;&lt;strong&gt;Authors:&lt;/strong&gt; Zhihong Shao; Yuxiang Luo; Chengda Lu; Z. Z. Ren; Jiewen Hu; Tian Ye; Zhibin Gou; Shirong Ma; Xiaokang Zhang&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Journal:&lt;/strong&gt; arXiv&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Relevance:&lt;/strong&gt; 0.796 (must_read)&lt;/p&gt;
&lt;p&gt;Large language models have made significant progress in mathematical reasoning, which serves as an important testbed for AI and could impact scientific research if further advanced. By scaling reasoning with reinforcement learning that rewards correct final answers, LLMs have improved from poor performance to saturating quantitative reasoning competitions like AIME and HMMT in one year. However, this approach faces fundamental limitations. Pursuing higher final answer accuracy doesn&amp;#x27;t address a key issue: correct answers don&amp;#x27;t guarantee correct reasoning. Moreover, many mathematical tasks like theorem proving require rigorous step-by-step derivation rather than numerical answers, making final answer rewards inapplicable. To push the limits of deep reasoning, we believe it is necessary to verify the comprehensiveness and rigor of mathematical reasoning. Self-verification is particularly important for scaling test-time compute, especially for open problems without known solutions. Towards self-verifiable mathematical reasoning, we investigate how to train an accurate and faithful LLM-based verifier for theorem proving. We then train a proof generator using the verifier as the reward model, and incentivize the generator to identify and resolve as many issues as possible in their own proofs before finalizing them. To maintain the generation-verification gap as the generator becomes stronger, we propose to scale verification compute to automatically label new hard-to-verify proofs, creating training data to further improve the verifier. Our resulting model, DeepSeekMath-V2, demonstrates strong theorem-proving capabilities, achieving gold-level scores on IMO 2025 and CMO 2024 and a near-perfect 118/120 on Putnam 2024 with scaled test-time compute.&lt;/p&gt;</content:encoded></item><item><title>Hybrid-stage Association with Dynamicity Adaptation and Enhanced Cues for Multi-object Tracking and Segmentation</title><link>https://doi.org/10.1016/j.patcog.2025.112803</link><guid>10.1016/j.patcog.2025.112803</guid><pubDate>Sun, 30 Nov 2025 15:08:25 +0000</pubDate><dc:creator>Longtao Chen</dc:creator><dc:creator>Guoxing Liao</dc:creator><dc:creator>Yifan Shi</dc:creator><dc:creator>Jing Lou</dc:creator><dc:creator>Fenglei Xu</dc:creator><dc:creator>Huanqiang Zeng</dc:creator><prism:publicationName>Pattern Recognition</prism:publicationName><prism:doi>10.1016/j.patcog.2025.112803</prism:doi><description>The varying degrees of dynamicity in objects present significant challenges for multi-object tracking and segmentation (MOTS), often manifesting as transitions between severe and minor deformations and occlusions. Currently, mainstream data association methodologies in MOTS rely on one-time single-stage or patchwork-style multi-stage strategy, and those dependent on predefined cues struggle to adapt to variable dynamicity. To address this issue, we propose HD-Track, a Hybrid-stage data association approach with Dynamicity Adaptation and Enhanced Cues. The Hybrid-stage strategy performs data association through pre-association and re-correction association stages. First, we exploit appearance cue sensitivity to dynamicity variations to project object dynamicity via pre-association. Second, we introduce Dynamicity Adaptation, featuring Dynamicity Selection to choose reliable appearance cues based on pre-association results, and Occlusion Dynamicity Fusing to dynamically integrate appearance and motion cues based on historical mask area variations, enhancing re-correction association robustness. Additionally, we propose a Mask-based Attention Mechanism and a Quad-triangle Transformation, collectively known as Enhanced Cues, to strengthen the robustness of both cues. Our extensive experiments on the MOTS20 and KITTI MOTS datasets demonstrate that HD-Track delivers reliable performance across diverse scenarios.
Published: 2025-11-30T15:08:25+00:00
Venue: Pattern Recognition
Score: 0.794 (must_read)</description><content:encoded>&lt;p&gt;&lt;strong&gt;Authors:&lt;/strong&gt; Longtao Chen; Guoxing Liao; Yifan Shi; Jing Lou; Fenglei Xu; Huanqiang Zeng&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Journal:&lt;/strong&gt; Pattern Recognition&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;a href="https://doi.org/10.1016/j.patcog.2025.112803"&gt;10.1016/j.patcog.2025.112803&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Relevance:&lt;/strong&gt; 0.794 (must_read)&lt;/p&gt;
&lt;p&gt;The varying degrees of dynamicity in objects present significant challenges for multi-object tracking and segmentation (MOTS), often manifesting as transitions between severe and minor deformations and occlusions. Currently, mainstream data association methodologies in MOTS rely on one-time single-stage or patchwork-style multi-stage strategy, and those dependent on predefined cues struggle to adapt to variable dynamicity. To address this issue, we propose HD-Track, a Hybrid-stage data association approach with Dynamicity Adaptation and Enhanced Cues. The Hybrid-stage strategy performs data association through pre-association and re-correction association stages. First, we exploit appearance cue sensitivity to dynamicity variations to project object dynamicity via pre-association. Second, we introduce Dynamicity Adaptation, featuring Dynamicity Selection to choose reliable appearance cues based on pre-association results, and Occlusion Dynamicity Fusing to dynamically integrate appearance and motion cues based on historical mask area variations, enhancing re-correction association robustness. Additionally, we propose a Mask-based Attention Mechanism and a Quad-triangle Transformation, collectively known as Enhanced Cues, to strengthen the robustness of both cues. Our extensive experiments on the MOTS20 and KITTI MOTS datasets demonstrate that HD-Track delivers reliable performance across diverse scenarios.&lt;/p&gt;</content:encoded></item><item><title>Small Object Detection for Birds with Swin Transformer</title><link>https://arxiv.org/abs/2511.22310v1</link><guid>http://arxiv.org/abs/2511.22310v1</guid><pubDate>Thu, 27 Nov 2025 10:42:37 +0000</pubDate><dc:creator>Da Huo</dc:creator><dc:creator>Marc A. Kastner</dc:creator><dc:creator>Tingwei Liu</dc:creator><dc:creator>Yasutomo Kawanishi</dc:creator><dc:creator>Takatsugu Hirayama</dc:creator><dc:creator>Takahiro Komamizu</dc:creator><dc:creator>Ichiro Ide</dc:creator><dc:type>preprint</dc:type><prism:publicationName>arXiv</prism:publicationName><prism:doi>10.23919/MVA57639.2023.10216093</prism:doi><description>Object detection is the task of detecting objects in an image. In this task, the detection of small objects is particularly difficult. Other than the small size, it is also accompanied by difficulties due to blur, occlusion, and so on. Current small object detection methods are tailored to small and dense situations, such as pedestrians in a crowd or far objects in remote sensing scenarios. However, when the target object is small and sparse, there is a lack of objects available for training, making it more difficult to learn effective features. In this paper, we propose a specialized method for detecting a specific category of small objects; birds. Particularly, we improve the features learned by the neck; the sub-network between the backbone and the prediction head, to learn more effective features with a hierarchical design. We employ Swin Transformer to upsample the image features. Moreover, we change the shifted window size for adapting to small objects. Experiments show that the proposed Swin Transformer-based neck combined with CenterNet can lead to good performance by changing the window sizes. We further find that smaller window sizes (default 2) benefit mAPs for small object detection.
Published: 2025-11-27T10:42:37+00:00
Venue: arXiv
Score: 0.791 (must_read)</description><content:encoded>&lt;p&gt;&lt;strong&gt;Authors:&lt;/strong&gt; Da Huo; Marc A. Kastner; Tingwei Liu; Yasutomo Kawanishi; Takatsugu Hirayama; Takahiro Komamizu; Ichiro Ide&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Journal:&lt;/strong&gt; arXiv&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;a href="https://doi.org/10.23919/MVA57639.2023.10216093"&gt;10.23919/MVA57639.2023.10216093&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Relevance:&lt;/strong&gt; 0.791 (must_read)&lt;/p&gt;
&lt;p&gt;Object detection is the task of detecting objects in an image. In this task, the detection of small objects is particularly difficult. Other than the small size, it is also accompanied by difficulties due to blur, occlusion, and so on. Current small object detection methods are tailored to small and dense situations, such as pedestrians in a crowd or far objects in remote sensing scenarios. However, when the target object is small and sparse, there is a lack of objects available for training, making it more difficult to learn effective features. In this paper, we propose a specialized method for detecting a specific category of small objects; birds. Particularly, we improve the features learned by the neck; the sub-network between the backbone and the prediction head, to learn more effective features with a hierarchical design. We employ Swin Transformer to upsample the image features. Moreover, we change the shifted window size for adapting to small objects. Experiments show that the proposed Swin Transformer-based neck combined with CenterNet can lead to good performance by changing the window sizes. We further find that smaller window sizes (default 2) benefit mAPs for small object detection.&lt;/p&gt;</content:encoded></item><item><title>Emergent Extreme-View Geometry in 3D Foundation Models</title><link>https://arxiv.org/abs/2511.22686v1</link><guid>http://arxiv.org/abs/2511.22686v1</guid><pubDate>Thu, 27 Nov 2025 18:40:03 +0000</pubDate><dc:creator>Yiwen Zhang</dc:creator><dc:creator>Joseph Tung</dc:creator><dc:creator>Ruojin Cai</dc:creator><dc:creator>David Fouhey</dc:creator><dc:creator>Hadar Averbuch-Elor</dc:creator><dc:type>preprint</dc:type><prism:publicationName>arXiv</prism:publicationName><description>3D foundation models (3DFMs) have recently transformed 3D vision, enabling joint prediction of depths, poses, and point maps directly from images. Yet their ability to reason under extreme, non-overlapping views remains largely unexplored. In this work, we study their internal representations and find that 3DFMs exhibit an emergent understanding of extreme-view geometry, despite never being trained for such conditions. To further enhance these capabilities, we introduce a lightweight alignment scheme that refines their internal 3D representation by tuning only a small subset of backbone bias terms, leaving all decoder heads frozen. This targeted adaptation substantially improves relative pose estimation under extreme viewpoints without degrading per-image depth or point quality. Additionally, we contribute MegaUnScene, a new benchmark of Internet scenes unseen by existing 3DFMs, with dedicated test splits for both relative pose estimation and dense 3D reconstruction. All code and data will be released.
Published: 2025-11-27T18:40:03+00:00
Venue: arXiv
Score: 0.790 (must_read)</description><content:encoded>&lt;p&gt;&lt;strong&gt;Authors:&lt;/strong&gt; Yiwen Zhang; Joseph Tung; Ruojin Cai; David Fouhey; Hadar Averbuch-Elor&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Journal:&lt;/strong&gt; arXiv&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Relevance:&lt;/strong&gt; 0.790 (must_read)&lt;/p&gt;
&lt;p&gt;3D foundation models (3DFMs) have recently transformed 3D vision, enabling joint prediction of depths, poses, and point maps directly from images. Yet their ability to reason under extreme, non-overlapping views remains largely unexplored. In this work, we study their internal representations and find that 3DFMs exhibit an emergent understanding of extreme-view geometry, despite never being trained for such conditions. To further enhance these capabilities, we introduce a lightweight alignment scheme that refines their internal 3D representation by tuning only a small subset of backbone bias terms, leaving all decoder heads frozen. This targeted adaptation substantially improves relative pose estimation under extreme viewpoints without degrading per-image depth or point quality. Additionally, we contribute MegaUnScene, a new benchmark of Internet scenes unseen by existing 3DFMs, with dedicated test splits for both relative pose estimation and dense 3D reconstruction. All code and data will be released.&lt;/p&gt;</content:encoded></item><item><title>Toward Diffusible High-Dimensional Latent Spaces: A Frequency Perspective</title><link>https://arxiv.org/abs/2511.22249v1</link><guid>http://arxiv.org/abs/2511.22249v1</guid><pubDate>Thu, 27 Nov 2025 09:20:36 +0000</pubDate><dc:creator>Bolin Lai</dc:creator><dc:creator>Xudong Wang</dc:creator><dc:creator>Saketh Rambhatla</dc:creator><dc:creator>James M. Rehg</dc:creator><dc:creator>Zsolt Kira</dc:creator><dc:creator>Rohit Girdhar</dc:creator><dc:creator>Ishan Misra</dc:creator><dc:type>preprint</dc:type><prism:publicationName>arXiv</prism:publicationName><description>Latent diffusion has become the default paradigm for visual generation, yet we observe a persistent reconstruction-generation trade-off as latent dimensionality increases: higher-capacity autoencoders improve reconstruction fidelity but generation quality eventually declines. We trace this gap to the different behaviors in high-frequency encoding and decoding. Through controlled perturbations in both RGB and latent domains, we analyze encoder/decoder behaviors and find that decoders depend strongly on high-frequency latent components to recover details, whereas encoders under-represent high-frequency contents, yielding insufficient exposure and underfitting in high-frequency bands for diffusion model training. To address this issue, we introduce FreqWarm, a plug-and-play frequency warm-up curriculum that increases early-stage exposure to high-frequency latent signals during diffusion or flow-matching training -- without modifying or retraining the autoencoder. Applied across several high-dimensional autoencoders, FreqWarm consistently improves generation quality: decreasing gFID by 14.11 on Wan2.2-VAE, 6.13 on LTX-VAE, and 4.42 on DC-AE-f32, while remaining architecture-agnostic and compatible with diverse backbones. Our study shows that explicitly managing frequency exposure can successfully turn high-dimensional latent spaces into more diffusible targets.
Published: 2025-11-27T09:20:36+00:00
Venue: arXiv
Score: 0.788 (must_read)</description><content:encoded>&lt;p&gt;&lt;strong&gt;Authors:&lt;/strong&gt; Bolin Lai; Xudong Wang; Saketh Rambhatla; James M. Rehg; Zsolt Kira; Rohit Girdhar; Ishan Misra&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Journal:&lt;/strong&gt; arXiv&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Relevance:&lt;/strong&gt; 0.788 (must_read)&lt;/p&gt;
&lt;p&gt;Latent diffusion has become the default paradigm for visual generation, yet we observe a persistent reconstruction-generation trade-off as latent dimensionality increases: higher-capacity autoencoders improve reconstruction fidelity but generation quality eventually declines. We trace this gap to the different behaviors in high-frequency encoding and decoding. Through controlled perturbations in both RGB and latent domains, we analyze encoder/decoder behaviors and find that decoders depend strongly on high-frequency latent components to recover details, whereas encoders under-represent high-frequency contents, yielding insufficient exposure and underfitting in high-frequency bands for diffusion model training. To address this issue, we introduce FreqWarm, a plug-and-play frequency warm-up curriculum that increases early-stage exposure to high-frequency latent signals during diffusion or flow-matching training -- without modifying or retraining the autoencoder. Applied across several high-dimensional autoencoders, FreqWarm consistently improves generation quality: decreasing gFID by 14.11 on Wan2.2-VAE, 6.13 on LTX-VAE, and 4.42 on DC-AE-f32, while remaining architecture-agnostic and compatible with diverse backbones. Our study shows that explicitly managing frequency exposure can successfully turn high-dimensional latent spaces into more diffusible targets.&lt;/p&gt;</content:encoded></item><item><title>A duet of perception and reasoning: CLIP and LLM brainstorming for scene text recognition</title><link>https://doi.org/10.1016/j.neucom.2025.132236</link><guid>10.1016/j.neucom.2025.132236</guid><pubDate>Sat, 29 Nov 2025 16:00:38 +0000</pubDate><dc:creator>Zeguang Jia</dc:creator><dc:creator>Jianming Wang</dc:creator><dc:creator>Kehui Song</dc:creator><dc:creator>Zhilan Wang</dc:creator><dc:creator>Xiaohan Ma</dc:creator><dc:creator>Rize Jin</dc:creator><prism:publicationName>Neurocomputing</prism:publicationName><prism:doi>10.1016/j.neucom.2025.132236</prism:doi><description>Deciphering ambiguous or contextually complex text remains a major challenge in the field of Scene Text Recognition (STR). Most existing STR recognizers rely on specialized, small-scale decoders that lack access to higher-level world knowledge and are prone to propagating local prediction errors, making it difficult to perform the higher-order reasoning required in complex contexts. These limitations are especially pronounced when using unimodal visual backbones that are incapable of capturing semantic information. In this study, we propose a novel STR paradigm called the Visual-Linguistic Enhancement Network (VLENet), which aims to jointly enhance visual perception and linguistic reasoning. Specifically, VLENet employs a cross-modal pre-trained model (CLIP) to extract visual representations that are semantically aligned with textual content. Based on the recognizer’s initial visual and linguistic predictions, a large language model (LLM) is prompted to “brainstorm” a diverse set of plausible text candidates. Finally, a carefully designed visual-linguistic matching module computes similarity scores between the original image and each candidate to select the most accurate transcription.We demonstrate the effectiveness of VLENet across a wide range of Chinese and English benchmarks, achieving new state-of-the-art (SOTA) results. Furthermore, our analysis shows that VLENet performs particularly well on challenging datasets such as COCO and Uber, highlighting its strong ability to reason about and correct text in complex real-world scenarios.
Published: 2025-11-29T16:00:38+00:00
Venue: Neurocomputing
Score: 0.786 (must_read)</description><content:encoded>&lt;p&gt;&lt;strong&gt;Authors:&lt;/strong&gt; Zeguang Jia; Jianming Wang; Kehui Song; Zhilan Wang; Xiaohan Ma; Rize Jin&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Journal:&lt;/strong&gt; Neurocomputing&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;a href="https://doi.org/10.1016/j.neucom.2025.132236"&gt;10.1016/j.neucom.2025.132236&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Relevance:&lt;/strong&gt; 0.786 (must_read)&lt;/p&gt;
&lt;p&gt;Deciphering ambiguous or contextually complex text remains a major challenge in the field of Scene Text Recognition (STR). Most existing STR recognizers rely on specialized, small-scale decoders that lack access to higher-level world knowledge and are prone to propagating local prediction errors, making it difficult to perform the higher-order reasoning required in complex contexts. These limitations are especially pronounced when using unimodal visual backbones that are incapable of capturing semantic information. In this study, we propose a novel STR paradigm called the Visual-Linguistic Enhancement Network (VLENet), which aims to jointly enhance visual perception and linguistic reasoning. Specifically, VLENet employs a cross-modal pre-trained model (CLIP) to extract visual representations that are semantically aligned with textual content. Based on the recognizer’s initial visual and linguistic predictions, a large language model (LLM) is prompted to “brainstorm” a diverse set of plausible text candidates. Finally, a carefully designed visual-linguistic matching module computes similarity scores between the original image and each candidate to select the most accurate transcription.We demonstrate the effectiveness of VLENet across a wide range of Chinese and English benchmarks, achieving new state-of-the-art (SOTA) results. Furthermore, our analysis shows that VLENet performs particularly well on challenging datasets such as COCO and Uber, highlighting its strong ability to reason about and correct text in complex real-world scenarios.&lt;/p&gt;</content:encoded></item><item><title>Match-and-Fuse: Consistent Generation from Unstructured Image Sets</title><link>https://arxiv.org/abs/2511.22287v1</link><guid>http://arxiv.org/abs/2511.22287v1</guid><pubDate>Thu, 27 Nov 2025 10:11:27 +0000</pubDate><dc:creator>Kate Feingold</dc:creator><dc:creator>Omri Kaduri</dc:creator><dc:creator>Tali Dekel</dc:creator><dc:type>preprint</dc:type><prism:publicationName>arXiv</prism:publicationName><description>We present Match-and-Fuse - a zero-shot, training-free method for consistent controlled generation of unstructured image sets - collections that share a common visual element, yet differ in viewpoint, time of capture, and surrounding content. Unlike existing methods that operate on individual images or densely sampled videos, our framework performs set-to-set generation: given a source set and user prompts, it produces a new set that preserves cross-image consistency of shared content. Our key idea is to model the task as a graph, where each node corresponds to an image and each edge triggers a joint generation of image pairs. This formulation consolidates all pairwise generations into a unified framework, enforcing their local consistency while ensuring global coherence across the entire set. This is achieved by fusing internal features across image pairs, guided by dense input correspondences, without requiring masks or manual supervision. It also allows us to leverage an emergent prior in text-to-image models that encourages coherent generation when multiple views share a single canvas. Match-and-Fuse achieves state-of-the-art consistency and visual quality, and unlocks new capabilities for content creation from image collections.
Published: 2025-11-27T10:11:27+00:00
Venue: arXiv
Score: 0.786 (consider)</description><content:encoded>&lt;p&gt;&lt;strong&gt;Authors:&lt;/strong&gt; Kate Feingold; Omri Kaduri; Tali Dekel&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Journal:&lt;/strong&gt; arXiv&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Relevance:&lt;/strong&gt; 0.786 (consider)&lt;/p&gt;
&lt;p&gt;We present Match-and-Fuse - a zero-shot, training-free method for consistent controlled generation of unstructured image sets - collections that share a common visual element, yet differ in viewpoint, time of capture, and surrounding content. Unlike existing methods that operate on individual images or densely sampled videos, our framework performs set-to-set generation: given a source set and user prompts, it produces a new set that preserves cross-image consistency of shared content. Our key idea is to model the task as a graph, where each node corresponds to an image and each edge triggers a joint generation of image pairs. This formulation consolidates all pairwise generations into a unified framework, enforcing their local consistency while ensuring global coherence across the entire set. This is achieved by fusing internal features across image pairs, guided by dense input correspondences, without requiring masks or manual supervision. It also allows us to leverage an emergent prior in text-to-image models that encourages coherent generation when multiple views share a single canvas. Match-and-Fuse achieves state-of-the-art consistency and visual quality, and unlocks new capabilities for content creation from image collections.&lt;/p&gt;</content:encoded></item><item><title>Structure is Supervision: Multiview Masked Autoencoders for Radiology</title><link>https://arxiv.org/abs/2511.22294v1</link><guid>http://arxiv.org/abs/2511.22294v1</guid><pubDate>Thu, 27 Nov 2025 10:20:51 +0000</pubDate><dc:creator>Sonia Laguna</dc:creator><dc:creator>Andrea Agostini</dc:creator><dc:creator>Alain Ryser</dc:creator><dc:creator>Samuel Ruiperez-Campillo</dc:creator><dc:creator>Irene Cannistraci</dc:creator><dc:creator>Moritz Vandenhirtz</dc:creator><dc:creator>Stephan Mandt</dc:creator><dc:creator>Nicolas Deperrois</dc:creator><dc:creator>Farhad Nooralahzadeh</dc:creator><dc:creator>Michael Krauthammer</dc:creator><dc:creator>Thomas M. Sutter</dc:creator><dc:creator>Julia E. Vogt</dc:creator><dc:type>preprint</dc:type><prism:publicationName>arXiv</prism:publicationName><description>Building robust medical machine learning systems requires pretraining strategies that exploit the intrinsic structure present in clinical data. We introduce Multiview Masked Autoencoder (MVMAE), a self-supervised framework that leverages the natural multi-view organization of radiology studies to learn view-invariant and disease-relevant representations. MVMAE combines masked image reconstruction with cross-view alignment, transforming clinical redundancy across projections into a powerful self-supervisory signal. We further extend this approach with MVMAE-V2T, which incorporates radiology reports as an auxiliary text-based learning signal to enhance semantic grounding while preserving fully vision-based inference. Evaluated on a downstream disease classification task on three large-scale public datasets, MIMIC-CXR, CheXpert, and PadChest, MVMAE consistently outperforms supervised and vision-language baselines. Furthermore, MVMAE-V2T provides additional gains, particularly in low-label regimes where structured textual supervision is most beneficial. Together, these results establish the importance of structural and textual supervision as complementary paths toward scalable, clinically grounded medical foundation models.
Published: 2025-11-27T10:20:51+00:00
Venue: arXiv
Score: 0.784 (consider)</description><content:encoded>&lt;p&gt;&lt;strong&gt;Authors:&lt;/strong&gt; Sonia Laguna; Andrea Agostini; Alain Ryser; Samuel Ruiperez-Campillo; Irene Cannistraci; Moritz Vandenhirtz; Stephan Mandt; Nicolas Deperrois; Farhad Nooralahzadeh; Michael Krauthammer; Thomas M. Sutter; Julia E. Vogt&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Journal:&lt;/strong&gt; arXiv&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Relevance:&lt;/strong&gt; 0.784 (consider)&lt;/p&gt;
&lt;p&gt;Building robust medical machine learning systems requires pretraining strategies that exploit the intrinsic structure present in clinical data. We introduce Multiview Masked Autoencoder (MVMAE), a self-supervised framework that leverages the natural multi-view organization of radiology studies to learn view-invariant and disease-relevant representations. MVMAE combines masked image reconstruction with cross-view alignment, transforming clinical redundancy across projections into a powerful self-supervisory signal. We further extend this approach with MVMAE-V2T, which incorporates radiology reports as an auxiliary text-based learning signal to enhance semantic grounding while preserving fully vision-based inference. Evaluated on a downstream disease classification task on three large-scale public datasets, MIMIC-CXR, CheXpert, and PadChest, MVMAE consistently outperforms supervised and vision-language baselines. Furthermore, MVMAE-V2T provides additional gains, particularly in low-label regimes where structured textual supervision is most beneficial. Together, these results establish the importance of structural and textual supervision as complementary paths toward scalable, clinically grounded medical foundation models.&lt;/p&gt;</content:encoded></item><item><title>Batch Self-organizing Memory Neural Network for Continual Supervised Learning</title><link>https://doi.org/10.1016/j.neunet.2025.108390</link><guid>10.1016/j.neunet.2025.108390</guid><pubDate>Sun, 30 Nov 2025 23:03:43 +0000</pubDate><dc:creator>Jiahui Niu</dc:creator><dc:creator>Xin Ma</dc:creator><prism:publicationName>Neural Networks</prism:publicationName><prism:doi>10.1016/j.neunet.2025.108390</prism:doi><description>Continual learning enables artificial neural networks to learn new tasks without forgetting previously learned tasks, which is a key challenge in mimicking human intelligence. Dynamic architecture strategies are therefore employed to expand the capacity of deep networks, enabling them to incorporate new tasks while maintaining performance on previously learned ones. However, deep networks frequently struggle to select an appropriate network expansion strategy. Task identifiers, manually assigned to distinguish tasks, are often required to guide parameter or component selection for each task. To address these challenges, this paper proposes a Batch Self-organizing Memory Neural Network (Batch SOMNN) for continual supervised learning. First, a Batch Supervised Competitive Learning (BSCL) algorithm is proposed for competitive learning of new tasks without task identifiers. By identifying distributional shifts between new and old tasks, the algorithm dynamically generates new regions to accommodate new data while preserving existing memory. Second, a Memory Correction (MC) module is employed to selectively retain valuable information by utilizing the forgetting curve and an adaptive threshold matrix, which enhances the efficiency and cost-effectiveness of network expansion. Extensive experiments show that Batch SOMNN outperforms strong baselines in continual learning scenarios. To further enhance its scalability and performance on complex datasets, we extend Batch SOMNN with a deep feature extraction backbone and prototype-based classification, forming Deep Batch SOMNN (DB-SOMNN), which achieves state-of-the-art results on standard benchmarks.
Published: 2025-11-30T23:03:43+00:00
Venue: Neural Networks
Score: 0.782 (consider)</description><content:encoded>&lt;p&gt;&lt;strong&gt;Authors:&lt;/strong&gt; Jiahui Niu; Xin Ma&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Journal:&lt;/strong&gt; Neural Networks&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;a href="https://doi.org/10.1016/j.neunet.2025.108390"&gt;10.1016/j.neunet.2025.108390&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Relevance:&lt;/strong&gt; 0.782 (consider)&lt;/p&gt;
&lt;p&gt;Continual learning enables artificial neural networks to learn new tasks without forgetting previously learned tasks, which is a key challenge in mimicking human intelligence. Dynamic architecture strategies are therefore employed to expand the capacity of deep networks, enabling them to incorporate new tasks while maintaining performance on previously learned ones. However, deep networks frequently struggle to select an appropriate network expansion strategy. Task identifiers, manually assigned to distinguish tasks, are often required to guide parameter or component selection for each task. To address these challenges, this paper proposes a Batch Self-organizing Memory Neural Network (Batch SOMNN) for continual supervised learning. First, a Batch Supervised Competitive Learning (BSCL) algorithm is proposed for competitive learning of new tasks without task identifiers. By identifying distributional shifts between new and old tasks, the algorithm dynamically generates new regions to accommodate new data while preserving existing memory. Second, a Memory Correction (MC) module is employed to selectively retain valuable information by utilizing the forgetting curve and an adaptive threshold matrix, which enhances the efficiency and cost-effectiveness of network expansion. Extensive experiments show that Batch SOMNN outperforms strong baselines in continual learning scenarios. To further enhance its scalability and performance on complex datasets, we extend Batch SOMNN with a deep feature extraction backbone and prototype-based classification, forming Deep Batch SOMNN (DB-SOMNN), which achieves state-of-the-art results on standard benchmarks.&lt;/p&gt;</content:encoded></item><item><title>ReAG: Reasoning-Augmented Generation for Knowledge-based Visual Question Answering</title><link>https://arxiv.org/abs/2511.22715v1</link><guid>http://arxiv.org/abs/2511.22715v1</guid><pubDate>Thu, 27 Nov 2025 19:01:02 +0000</pubDate><dc:creator>Alberto Compagnoni</dc:creator><dc:creator>Marco Morini</dc:creator><dc:creator>Sara Sarto</dc:creator><dc:creator>Federico Cocchi</dc:creator><dc:creator>Davide Caffagni</dc:creator><dc:creator>Marcella Cornia</dc:creator><dc:creator>Lorenzo Baraldi</dc:creator><dc:creator>Rita Cucchiara</dc:creator><dc:type>preprint</dc:type><prism:publicationName>arXiv</prism:publicationName><description>Multimodal Large Language Models (MLLMs) have shown impressive capabilities in jointly understanding text, images, and videos, often evaluated via Visual Question Answering (VQA). However, even state-of-the-art MLLMs struggle with domain-specific or knowledge-intensive queries, where relevant information is underrepresented in pre-training data. Knowledge-based VQA (KB-VQA) addresses this by retrieving external documents to condition answer generation, but current retrieval-augmented approaches suffer from low precision, noisy passages, and limited reasoning. To address this, we propose ReAG, a novel Reasoning-Augmented Multimodal RAG approach that combines coarse- and fine-grained retrieval with a critic model that filters irrelevant passages, ensuring high-quality additional context. The model follows a multi-stage training strategy leveraging reinforcement learning to enhance reasoning over retrieved content, while supervised fine-tuning serves only as a cold start. Extensive experiments on Encyclopedic-VQA and InfoSeek demonstrate that ReAG significantly outperforms prior methods, improving answer accuracy and providing interpretable reasoning grounded in retrieved evidence. Our source code is publicly available at: https://github.com/aimagelab/ReAG.
Published: 2025-11-27T19:01:02+00:00
Venue: arXiv
Score: 0.782 (consider)</description><content:encoded>&lt;p&gt;&lt;strong&gt;Authors:&lt;/strong&gt; Alberto Compagnoni; Marco Morini; Sara Sarto; Federico Cocchi; Davide Caffagni; Marcella Cornia; Lorenzo Baraldi; Rita Cucchiara&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Journal:&lt;/strong&gt; arXiv&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Relevance:&lt;/strong&gt; 0.782 (consider)&lt;/p&gt;
&lt;p&gt;Multimodal Large Language Models (MLLMs) have shown impressive capabilities in jointly understanding text, images, and videos, often evaluated via Visual Question Answering (VQA). However, even state-of-the-art MLLMs struggle with domain-specific or knowledge-intensive queries, where relevant information is underrepresented in pre-training data. Knowledge-based VQA (KB-VQA) addresses this by retrieving external documents to condition answer generation, but current retrieval-augmented approaches suffer from low precision, noisy passages, and limited reasoning. To address this, we propose ReAG, a novel Reasoning-Augmented Multimodal RAG approach that combines coarse- and fine-grained retrieval with a critic model that filters irrelevant passages, ensuring high-quality additional context. The model follows a multi-stage training strategy leveraging reinforcement learning to enhance reasoning over retrieved content, while supervised fine-tuning serves only as a cold start. Extensive experiments on Encyclopedic-VQA and InfoSeek demonstrate that ReAG significantly outperforms prior methods, improving answer accuracy and providing interpretable reasoning grounded in retrieved evidence. Our source code is publicly available at: https://github.com/aimagelab/ReAG.&lt;/p&gt;</content:encoded></item><item><title>Improving Stochastic Action-Constrained Reinforcement Learning via Truncated Distributions</title><link>https://arxiv.org/abs/2511.22406v1</link><guid>http://arxiv.org/abs/2511.22406v1</guid><pubDate>Thu, 27 Nov 2025 12:33:36 +0000</pubDate><dc:creator>Roland Stolz</dc:creator><dc:creator>Michael Eichelbeck</dc:creator><dc:creator>Matthias Althoff</dc:creator><dc:type>preprint</dc:type><prism:publicationName>arXiv</prism:publicationName><description>In reinforcement learning (RL), it is often advantageous to consider additional constraints on the action space to ensure safety or action relevance. Existing work on such action-constrained RL faces challenges regarding effective policy updates, computational efficiency, and predictable runtime. Recent work proposes to use truncated normal distributions for stochastic policy gradient methods. However, the computation of key characteristics, such as the entropy, log-probability, and their gradients, becomes intractable under complex constraints. Hence, prior work approximates these using the non-truncated distributions, which severely degrades performance. We argue that accurate estimation of these characteristics is crucial in the action-constrained RL setting, and propose efficient numerical approximations for them. We also provide an efficient sampling strategy for truncated policy distributions and validate our approach on three benchmark environments, which demonstrate significant performance improvements when using accurate estimations.
Published: 2025-11-27T12:33:36+00:00
Venue: arXiv
Score: 0.782 (consider)</description><content:encoded>&lt;p&gt;&lt;strong&gt;Authors:&lt;/strong&gt; Roland Stolz; Michael Eichelbeck; Matthias Althoff&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Journal:&lt;/strong&gt; arXiv&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Relevance:&lt;/strong&gt; 0.782 (consider)&lt;/p&gt;
&lt;p&gt;In reinforcement learning (RL), it is often advantageous to consider additional constraints on the action space to ensure safety or action relevance. Existing work on such action-constrained RL faces challenges regarding effective policy updates, computational efficiency, and predictable runtime. Recent work proposes to use truncated normal distributions for stochastic policy gradient methods. However, the computation of key characteristics, such as the entropy, log-probability, and their gradients, becomes intractable under complex constraints. Hence, prior work approximates these using the non-truncated distributions, which severely degrades performance. We argue that accurate estimation of these characteristics is crucial in the action-constrained RL setting, and propose efficient numerical approximations for them. We also provide an efficient sampling strategy for truncated policy distributions and validate our approach on three benchmark environments, which demonstrate significant performance improvements when using accurate estimations.&lt;/p&gt;</content:encoded></item><item><title>CAT: A High-performance Cross-Attributes and Cross-Tasks for one-stage 3D object detection</title><link>https://doi.org/10.1016/j.knosys.2025.114995</link><guid>10.1016/j.knosys.2025.114995</guid><pubDate>Sat, 29 Nov 2025 23:41:51 +0000</pubDate><dc:creator>Yu Qin</dc:creator><dc:creator>Yiqiang Wu</dc:creator><dc:creator>Chang Liu</dc:creator><dc:creator>Chenghai Mao</dc:creator><dc:creator>Jia Liu</dc:creator><dc:creator>Jiacheng Sun</dc:creator><dc:creator>Yan Peng</dc:creator><prism:publicationName>Knowledge-Based Systems</prism:publicationName><prism:doi>10.1016/j.knosys.2025.114995</prism:doi><description>Real-time 3D object detection is a critical component of autonomous driving systems, yet existing one-stage detectors still face performance bottlenecks. We experimentally reveal that two kinds of incongruities suppress detection performance: (1) Attribute inconsistency refers to poor cooperation among regression attributes, which causes poor localization quality. (2) Task incongruity refers to the lack of correlation between regression and classification tasks, resulting in inefficient category prediction. To address these issues, this paper proposes a Cross-Attribute and Cross-Task (CAT) detector based on collaboration. This is the first framework to explicitly promote collaboration between regression attributes and regression and classification tasks. Specifically, to mitigate the incongruity among attributes, Regression Attribute Collaboration (RAC) is proposed to conduct joint prediction. RAC merges prediction branches to enhance the correlation between coordinates and geometric attributes in regression tasks. As for task incongruity, Cross-Task Collaboration (CTC) is designed based on a weighted incentive strategy. In particular, CTC uses geometric distribution features to incentivize classification scores, to establish an association between the regression and classification tasks. Comprehensive experiments demonstrate that CAT effectively mitigates cross-attribute and cross-task incongruity. The CAT method achieves state-of-the-art performance on the ONCE dataset and the Waymo dataset.
Published: 2025-11-29T23:41:51+00:00
Venue: Knowledge-Based Systems
Score: 0.781 (consider)</description><content:encoded>&lt;p&gt;&lt;strong&gt;Authors:&lt;/strong&gt; Yu Qin; Yiqiang Wu; Chang Liu; Chenghai Mao; Jia Liu; Jiacheng Sun; Yan Peng&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Journal:&lt;/strong&gt; Knowledge-Based Systems&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;a href="https://doi.org/10.1016/j.knosys.2025.114995"&gt;10.1016/j.knosys.2025.114995&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Relevance:&lt;/strong&gt; 0.781 (consider)&lt;/p&gt;
&lt;p&gt;Real-time 3D object detection is a critical component of autonomous driving systems, yet existing one-stage detectors still face performance bottlenecks. We experimentally reveal that two kinds of incongruities suppress detection performance: (1) Attribute inconsistency refers to poor cooperation among regression attributes, which causes poor localization quality. (2) Task incongruity refers to the lack of correlation between regression and classification tasks, resulting in inefficient category prediction. To address these issues, this paper proposes a Cross-Attribute and Cross-Task (CAT) detector based on collaboration. This is the first framework to explicitly promote collaboration between regression attributes and regression and classification tasks. Specifically, to mitigate the incongruity among attributes, Regression Attribute Collaboration (RAC) is proposed to conduct joint prediction. RAC merges prediction branches to enhance the correlation between coordinates and geometric attributes in regression tasks. As for task incongruity, Cross-Task Collaboration (CTC) is designed based on a weighted incentive strategy. In particular, CTC uses geometric distribution features to incentivize classification scores, to establish an association between the regression and classification tasks. Comprehensive experiments demonstrate that CAT effectively mitigates cross-attribute and cross-task incongruity. The CAT method achieves state-of-the-art performance on the ONCE dataset and the Waymo dataset.&lt;/p&gt;</content:encoded></item><item><title>PCNet3D++: A pillar-based cascaded 3D object detection model with an enhanced 2D backbone</title><link>https://doi.org/10.1016/j.imavis.2025.105854</link><guid>10.1016/j.imavis.2025.105854</guid><pubDate>Sat, 29 Nov 2025 23:39:29 +0000</pubDate><dc:creator>Thurimerla Prasanth</dc:creator><dc:creator>Ram Prasad Padhy</dc:creator><dc:creator>B. Sivaselvan</dc:creator><prism:publicationName>Image and Vision Computing</prism:publicationName><prism:doi>10.1016/j.imavis.2025.105854</prism:doi><description>Autonomous Vehicles (AVs) depend on sophisticated perception systems to serve as the vital component of intelligent transportation to ensure secure and smooth navigation. Perception is an essential component of AVs and enables real-time analysis and understanding of the environment for effective decision-making. 3D object detection (3D-OD) is crucial among perception tasks as it accurately determines the 3D geometry and spatial positioning of surrounding objects. The commonly used modalities for 3D-OD are camera, LiDAR, and sensor fusion. In this work, we propose a LiDAR-based 3D-OD approach using point cloud data. The proposed model achieves superior performance while maintaining computational efficiency. This approach utilizes Pillar-based LiDAR processing and uses only 2D convolutions. The model pipeline becomes simple and more efficient by employing only 2D convolutions. We propose a Cascaded Convolutional Backbone (CCB) integrated with 1 × 1 convolutions to improve detection accuracy. We combined the fast Pillar-based encoding with our lightweight backbone. The proposed model reduces complexity to make it well-suited for real-time navigation of an AV. We evaluated our model on the official KITTI test server. The model results are decent in 3D and Bird’s Eye View (BEV) detection benchmarks for the car and cyclist classes. The results of our proposed model are featured on the official KITTI leaderboard.
Published: 2025-11-29T23:39:29+00:00
Venue: Image and Vision Computing
Score: 0.781 (consider)</description><content:encoded>&lt;p&gt;&lt;strong&gt;Authors:&lt;/strong&gt; Thurimerla Prasanth; Ram Prasad Padhy; B. Sivaselvan&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Journal:&lt;/strong&gt; Image and Vision Computing&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;a href="https://doi.org/10.1016/j.imavis.2025.105854"&gt;10.1016/j.imavis.2025.105854&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Relevance:&lt;/strong&gt; 0.781 (consider)&lt;/p&gt;
&lt;p&gt;Autonomous Vehicles (AVs) depend on sophisticated perception systems to serve as the vital component of intelligent transportation to ensure secure and smooth navigation. Perception is an essential component of AVs and enables real-time analysis and understanding of the environment for effective decision-making. 3D object detection (3D-OD) is crucial among perception tasks as it accurately determines the 3D geometry and spatial positioning of surrounding objects. The commonly used modalities for 3D-OD are camera, LiDAR, and sensor fusion. In this work, we propose a LiDAR-based 3D-OD approach using point cloud data. The proposed model achieves superior performance while maintaining computational efficiency. This approach utilizes Pillar-based LiDAR processing and uses only 2D convolutions. The model pipeline becomes simple and more efficient by employing only 2D convolutions. We propose a Cascaded Convolutional Backbone (CCB) integrated with 1 × 1 convolutions to improve detection accuracy. We combined the fast Pillar-based encoding with our lightweight backbone. The proposed model reduces complexity to make it well-suited for real-time navigation of an AV. We evaluated our model on the official KITTI test server. The model results are decent in 3D and Bird’s Eye View (BEV) detection benchmarks for the car and cyclist classes. The results of our proposed model are featured on the official KITTI leaderboard.&lt;/p&gt;</content:encoded></item><item><title>GoPrune: Accelerated Structured Pruning with $\ell_{2,p}$-Norm Optimization</title><link>https://arxiv.org/abs/2511.22120v1</link><guid>http://arxiv.org/abs/2511.22120v1</guid><pubDate>Thu, 27 Nov 2025 05:24:31 +0000</pubDate><dc:creator>Li Xu</dc:creator><dc:creator>Xianchao Xiu</dc:creator><dc:type>preprint</dc:type><prism:publicationName>arXiv</prism:publicationName><description>Convolutional neural networks (CNNs) suffer from rapidly increasing storage and computational costs as their depth grows, which severely hinders their deployment on resource-constrained edge devices. Pruning is a practical approach for network compression, among which structured pruning is the most effective for inference acceleration. Although existing work has applied the $\ell_p$-norm to pruning, it only considers unstructured pruning with $p\in (0, 1)$ and has low computational efficiency. To overcome these limitations, we propose an accelerated structured pruning method called GoPrune. Our method employs the $\ell_{2,p}$-norm for sparse network learning, where the value of $p$ is extended to $[0, 1)$. Moreover, we develop an efficient optimization algorithm based on the proximal alternating minimization (PAM), and the resulting subproblems enjoy closed-form solutions, thus improving compression efficiency. Experiments on the CIFAR datasets using ResNet and VGG models demonstrate the superior performance of the proposed method in network pruning. Our code is available at https://github.com/xianchaoxiu/GoPrune.
Published: 2025-11-27T05:24:31+00:00
Venue: arXiv
Score: 0.781 (consider)</description><content:encoded>&lt;p&gt;&lt;strong&gt;Authors:&lt;/strong&gt; Li Xu; Xianchao Xiu&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Journal:&lt;/strong&gt; arXiv&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Relevance:&lt;/strong&gt; 0.781 (consider)&lt;/p&gt;
&lt;p&gt;Convolutional neural networks (CNNs) suffer from rapidly increasing storage and computational costs as their depth grows, which severely hinders their deployment on resource-constrained edge devices. Pruning is a practical approach for network compression, among which structured pruning is the most effective for inference acceleration. Although existing work has applied the $\ell_p$-norm to pruning, it only considers unstructured pruning with $p\in (0, 1)$ and has low computational efficiency. To overcome these limitations, we propose an accelerated structured pruning method called GoPrune. Our method employs the $\ell_{2,p}$-norm for sparse network learning, where the value of $p$ is extended to $[0, 1)$. Moreover, we develop an efficient optimization algorithm based on the proximal alternating minimization (PAM), and the resulting subproblems enjoy closed-form solutions, thus improving compression efficiency. Experiments on the CIFAR datasets using ResNet and VGG models demonstrate the superior performance of the proposed method in network pruning. Our code is available at https://github.com/xianchaoxiu/GoPrune.&lt;/p&gt;</content:encoded></item><item><title>Parameter-Efficient Image-to-Video Transfer Learning for Long-Term Visual Place Recognition</title><link>https://doi.org/10.1016/j.knosys.2025.115021</link><guid>10.1016/j.knosys.2025.115021</guid><pubDate>Sun, 30 Nov 2025 15:11:40 +0000</pubDate><dc:creator>Qilong Wu</dc:creator><dc:creator>Lin Li</dc:creator><dc:creator>Haihong Zhu</dc:creator><dc:creator>Peiwen Yao</dc:creator><dc:creator>Xinmei Wu</dc:creator><dc:creator>Yining Cui</dc:creator><dc:creator>Wei Zhu</dc:creator><dc:creator>Yukun Wu</dc:creator><prism:publicationName>Knowledge-Based Systems</prism:publicationName><prism:doi>10.1016/j.knosys.2025.115021</prism:doi><description>Visual Place Recognition (VPR) is pivotal for robust localization in autonomous navigation, yet long-term environmental dynamics and resource-intensive video models challenge its efficacy. This work introduces a parameter-efficient image-to-video transfer learning framework (I2VPR) to address these limitations, leveraging abundant image data to enhance video-based VPR. We propose a novel spatio-temporal convolution adapter (ST-ConvAdapter), a lightweight plug-and-play module that, unlike prior adapters which rely on MLPs or 2D convolutions, is specifically designed to model inter-frame dynamics. Integrated into pre-trained vision transformers, it enables hierarchical spatio-temporal feature learning with minimal parameter overhead. Leveraging depthwise 3D convolutions, I2VPR transfers robust spatial features from image models to video, effectively bridging domain gaps, suppressing noise, and capturing motion dynamics. Evaluations on benchmark datasets demonstrate that I2VPR surpasses state-of-the-art methods, achieving a 94.1% Recall@1 in cross-domain tests — a 15% improvement over prior art. Ablation studies confirm the design efficacy, highlighting its ability to balance spatial invariance and temporal sensitivity. This framework offers an efficient, scalable solution for resource-constrained VPR systems in dynamic real-world settings.
Published: 2025-11-30T15:11:40+00:00
Venue: Knowledge-Based Systems
Score: 0.780 (consider)</description><content:encoded>&lt;p&gt;&lt;strong&gt;Authors:&lt;/strong&gt; Qilong Wu; Lin Li; Haihong Zhu; Peiwen Yao; Xinmei Wu; Yining Cui; Wei Zhu; Yukun Wu&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Journal:&lt;/strong&gt; Knowledge-Based Systems&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;a href="https://doi.org/10.1016/j.knosys.2025.115021"&gt;10.1016/j.knosys.2025.115021&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Relevance:&lt;/strong&gt; 0.780 (consider)&lt;/p&gt;
&lt;p&gt;Visual Place Recognition (VPR) is pivotal for robust localization in autonomous navigation, yet long-term environmental dynamics and resource-intensive video models challenge its efficacy. This work introduces a parameter-efficient image-to-video transfer learning framework (I2VPR) to address these limitations, leveraging abundant image data to enhance video-based VPR. We propose a novel spatio-temporal convolution adapter (ST-ConvAdapter), a lightweight plug-and-play module that, unlike prior adapters which rely on MLPs or 2D convolutions, is specifically designed to model inter-frame dynamics. Integrated into pre-trained vision transformers, it enables hierarchical spatio-temporal feature learning with minimal parameter overhead. Leveraging depthwise 3D convolutions, I2VPR transfers robust spatial features from image models to video, effectively bridging domain gaps, suppressing noise, and capturing motion dynamics. Evaluations on benchmark datasets demonstrate that I2VPR surpasses state-of-the-art methods, achieving a 94.1% Recall@1 in cross-domain tests — a 15% improvement over prior art. Ablation studies confirm the design efficacy, highlighting its ability to balance spatial invariance and temporal sensitivity. This framework offers an efficient, scalable solution for resource-constrained VPR systems in dynamic real-world settings.&lt;/p&gt;</content:encoded></item><item><title>MoE3D: Mixture of Experts meets Multi-Modal 3D Understanding</title><link>https://arxiv.org/abs/2511.22103v1</link><guid>http://arxiv.org/abs/2511.22103v1</guid><pubDate>Thu, 27 Nov 2025 04:48:11 +0000</pubDate><dc:creator>Yu Li</dc:creator><dc:creator>Yuenan Hou</dc:creator><dc:creator>Yingmei Wei</dc:creator><dc:creator>Xinge Zhu</dc:creator><dc:creator>Yuexin Ma</dc:creator><dc:creator>Wenqi Shao</dc:creator><dc:creator>Yanming Guo</dc:creator><dc:type>preprint</dc:type><prism:publicationName>arXiv</prism:publicationName><description>Multi-modal 3D understanding is a fundamental task in computer vision. Previous multi-modal fusion methods typically employ a single, dense fusion network, struggling to handle the significant heterogeneity and complexity across modalities, leading to suboptimal performance. In this paper, we propose MoE3D, which integrates Mixture of Experts (MoE) into the multi-modal learning framework. The core is that we deploy a set of specialized "expert" networks, each adept at processing a specific modality or a mode of cross-modal interaction. Specifically, the MoE-based transformer is designed to better utilize the complementary information hidden in the visual features. Information aggregation module is put forward to further enhance the fusion performance. Top-1 gating is employed to make one expert process features with expert groups, ensuring high efficiency. We further propose a progressive pre-training strategy to better leverage the semantic and 2D prior, thus equipping the network with good initialization. Our MoE3D achieves competitive performance across four prevalent 3D understanding tasks. Notably, our MoE3D surpasses the top-performing counterpart by 6.1 mIoU on Multi3DRefer.
Published: 2025-11-27T04:48:11+00:00
Venue: arXiv
Score: 0.775 (consider)</description><content:encoded>&lt;p&gt;&lt;strong&gt;Authors:&lt;/strong&gt; Yu Li; Yuenan Hou; Yingmei Wei; Xinge Zhu; Yuexin Ma; Wenqi Shao; Yanming Guo&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Journal:&lt;/strong&gt; arXiv&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Relevance:&lt;/strong&gt; 0.775 (consider)&lt;/p&gt;
&lt;p&gt;Multi-modal 3D understanding is a fundamental task in computer vision. Previous multi-modal fusion methods typically employ a single, dense fusion network, struggling to handle the significant heterogeneity and complexity across modalities, leading to suboptimal performance. In this paper, we propose MoE3D, which integrates Mixture of Experts (MoE) into the multi-modal learning framework. The core is that we deploy a set of specialized &amp;quot;expert&amp;quot; networks, each adept at processing a specific modality or a mode of cross-modal interaction. Specifically, the MoE-based transformer is designed to better utilize the complementary information hidden in the visual features. Information aggregation module is put forward to further enhance the fusion performance. Top-1 gating is employed to make one expert process features with expert groups, ensuring high efficiency. We further propose a progressive pre-training strategy to better leverage the semantic and 2D prior, thus equipping the network with good initialization. Our MoE3D achieves competitive performance across four prevalent 3D understanding tasks. Notably, our MoE3D surpasses the top-performing counterpart by 6.1 mIoU on Multi3DRefer.&lt;/p&gt;</content:encoded></item><item><title>Matrix mixer analysis for time series classification: attention on tokenization</title><link>https://doi.org/10.1016/j.inffus.2025.104009</link><guid>10.1016/j.inffus.2025.104009</guid><pubDate>Sun, 30 Nov 2025 06:29:44 +0000</pubDate><dc:creator>Mohammad Mahdi Azizi</dc:creator><dc:creator>Bagher Babaali</dc:creator><prism:publicationName>Information Fusion</prism:publicationName><prism:doi>10.1016/j.inffus.2025.104009</prism:doi><description>Transformer-based foundational models have achieved state-of-the-art in natural language processing and computer vision, prompting interest in applying them to time series and biosignals. However, developing effective foundational models for time series classification faces significant challenges due to the inherent diversity of time series datasets. Key unresolved questions in this area include the impact of different tokenization and temporal fusion strategies and architectural designs on model performance. This study redefines the matrix mixer framework as a general architectural design toolbox. We analyze how matrix mixer structures and tokenization methods impact the effectiveness of time series classification models. The findings of this research are captured in the title ”Attention on Tokenization,” which emphasizes two key points: first, utilizing a token mixer like self-attention is more advantageous than relying on strategies without token-level temporal information fusion; and second, paying close attention to the tokenization process, particularly in choosing the optimal patch embedding configuration, can enhance model performance. The models developed in this study achieved state-of-the-art results on two widely used time series classification benchmarks, achieving the average accuracies of 73.1% in supervised settings and 86.0% in self-supervised settings, all using a unified architecture.
Published: 2025-11-30T06:29:44+00:00
Venue: Information Fusion
Score: 0.774 (consider)</description><content:encoded>&lt;p&gt;&lt;strong&gt;Authors:&lt;/strong&gt; Mohammad Mahdi Azizi; Bagher Babaali&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Journal:&lt;/strong&gt; Information Fusion&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;a href="https://doi.org/10.1016/j.inffus.2025.104009"&gt;10.1016/j.inffus.2025.104009&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Relevance:&lt;/strong&gt; 0.774 (consider)&lt;/p&gt;
&lt;p&gt;Transformer-based foundational models have achieved state-of-the-art in natural language processing and computer vision, prompting interest in applying them to time series and biosignals. However, developing effective foundational models for time series classification faces significant challenges due to the inherent diversity of time series datasets. Key unresolved questions in this area include the impact of different tokenization and temporal fusion strategies and architectural designs on model performance. This study redefines the matrix mixer framework as a general architectural design toolbox. We analyze how matrix mixer structures and tokenization methods impact the effectiveness of time series classification models. The findings of this research are captured in the title ”Attention on Tokenization,” which emphasizes two key points: first, utilizing a token mixer like self-attention is more advantageous than relying on strategies without token-level temporal information fusion; and second, paying close attention to the tokenization process, particularly in choosing the optimal patch embedding configuration, can enhance model performance. The models developed in this study achieved state-of-the-art results on two widely used time series classification benchmarks, achieving the average accuracies of 73.1% in supervised settings and 86.0% in self-supervised settings, all using a unified architecture.&lt;/p&gt;</content:encoded></item><item><title>SuRe: Surprise-Driven Prioritised Replay for Continual LLM Learning</title><link>https://arxiv.org/abs/2511.22367v1</link><guid>http://arxiv.org/abs/2511.22367v1</guid><pubDate>Thu, 27 Nov 2025 12:06:33 +0000</pubDate><dc:creator>Hugo Hazard</dc:creator><dc:creator>Zafeirios Fountas</dc:creator><dc:creator>Martin A. Benfeghoul</dc:creator><dc:creator>Adnan Oomerjee</dc:creator><dc:creator>Jun Wang</dc:creator><dc:creator>Haitham Bou-Ammar</dc:creator><dc:type>preprint</dc:type><prism:publicationName>arXiv</prism:publicationName><description>Continual learning, one's ability to adapt to a sequence of tasks without forgetting previously acquired knowledge, remains a major challenge in machine learning and a key gap between artificial and human intelligence. While regularisation and replay perform well in vision, they lag behind multi-task learning for large language models (LLMs), especially at scale with many tasks. We revisit replay and argue that two failure modes drive this gap: selection (what to rehearse) and integration (how to consolidate new knowledge). To address selection, we propose Surprise-prioritised Replay (SuRe), a simple, architecture-agnostic rule that ranks and stores the most surprising (high Negative Log-Likelihood) sequences. SuRe achieves state-of-the-art performance in the Large Number of Tasks (LNT) setting and delivers the best overall average across both Standard CL and LNT benchmarks. To address integration, we add a dual-learner design with fast and slow LoRA adapters merged via an exponential moving average (EMA), enabling rapid adaptation while stabilising long-term knowledge. Combining SuRe with the dual learner yields further gains, including improvements of up to +5 accuracy points on LNT over prior SOTA. Ablation studies confirm that our proposed method remains robust under reduced replay frequency and small buffer size, demonstrating both effectiveness and sample efficiency. Taken together, our results establish replay as a strong baseline for continual LLM fine-tuning and demonstrate that surprise-based selection and slow-weight consolidation are complementary components for mitigating catastrophic forgetting.
Published: 2025-11-27T12:06:33+00:00
Venue: arXiv
Score: 0.773 (consider)</description><content:encoded>&lt;p&gt;&lt;strong&gt;Authors:&lt;/strong&gt; Hugo Hazard; Zafeirios Fountas; Martin A. Benfeghoul; Adnan Oomerjee; Jun Wang; Haitham Bou-Ammar&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Journal:&lt;/strong&gt; arXiv&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Relevance:&lt;/strong&gt; 0.773 (consider)&lt;/p&gt;
&lt;p&gt;Continual learning, one&amp;#x27;s ability to adapt to a sequence of tasks without forgetting previously acquired knowledge, remains a major challenge in machine learning and a key gap between artificial and human intelligence. While regularisation and replay perform well in vision, they lag behind multi-task learning for large language models (LLMs), especially at scale with many tasks. We revisit replay and argue that two failure modes drive this gap: selection (what to rehearse) and integration (how to consolidate new knowledge). To address selection, we propose Surprise-prioritised Replay (SuRe), a simple, architecture-agnostic rule that ranks and stores the most surprising (high Negative Log-Likelihood) sequences. SuRe achieves state-of-the-art performance in the Large Number of Tasks (LNT) setting and delivers the best overall average across both Standard CL and LNT benchmarks. To address integration, we add a dual-learner design with fast and slow LoRA adapters merged via an exponential moving average (EMA), enabling rapid adaptation while stabilising long-term knowledge. Combining SuRe with the dual learner yields further gains, including improvements of up to +5 accuracy points on LNT over prior SOTA. Ablation studies confirm that our proposed method remains robust under reduced replay frequency and small buffer size, demonstrating both effectiveness and sample efficiency. Taken together, our results establish replay as a strong baseline for continual LLM fine-tuning and demonstrate that surprise-based selection and slow-weight consolidation are complementary components for mitigating catastrophic forgetting.&lt;/p&gt;</content:encoded></item><item><title>GazeTrack: High-Precision Eye Tracking Based on Regularization and Spatial Computing</title><link>https://arxiv.org/abs/2511.22607v1</link><guid>http://arxiv.org/abs/2511.22607v1</guid><pubDate>Thu, 27 Nov 2025 16:41:32 +0000</pubDate><dc:creator>Xiaoyin Yang</dc:creator><dc:type>preprint</dc:type><prism:publicationName>arXiv</prism:publicationName><description>Eye tracking has become increasingly important in virtual and augmented reality applications; however, the current gaze accuracy falls short of meeting the requirements for spatial computing. We designed a gaze collection framework and utilized high-precision equipment to gather the first precise benchmark dataset, GazeTrack, encompassing diverse ethnicities, ages, and visual acuity conditions for pupil localization and gaze tracking. We propose a novel shape error regularization method to constrain pupil ellipse fitting and train on open-source datasets, enhancing semantic segmentation and pupil position prediction accuracy. Additionally, we invent a novel coordinate transformation method similar to paper unfolding to accurately predict gaze vectors on the GazeTrack dataset. Finally, we built a gaze vector generation model that achieves reduced gaze angle error with lower computational complexity compared to other methods.
Published: 2025-11-27T16:41:32+00:00
Venue: arXiv
Score: 0.772 (consider)</description><content:encoded>&lt;p&gt;&lt;strong&gt;Authors:&lt;/strong&gt; Xiaoyin Yang&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Journal:&lt;/strong&gt; arXiv&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Relevance:&lt;/strong&gt; 0.772 (consider)&lt;/p&gt;
&lt;p&gt;Eye tracking has become increasingly important in virtual and augmented reality applications; however, the current gaze accuracy falls short of meeting the requirements for spatial computing. We designed a gaze collection framework and utilized high-precision equipment to gather the first precise benchmark dataset, GazeTrack, encompassing diverse ethnicities, ages, and visual acuity conditions for pupil localization and gaze tracking. We propose a novel shape error regularization method to constrain pupil ellipse fitting and train on open-source datasets, enhancing semantic segmentation and pupil position prediction accuracy. Additionally, we invent a novel coordinate transformation method similar to paper unfolding to accurately predict gaze vectors on the GazeTrack dataset. Finally, we built a gaze vector generation model that achieves reduced gaze angle error with lower computational complexity compared to other methods.&lt;/p&gt;</content:encoded></item><item><title>CAMN-FSOD: Class-aware memory network for few-shot infrared object detection</title><link>https://doi.org/10.1016/j.patrec.2025.11.033</link><guid>10.1016/j.patrec.2025.11.033</guid><pubDate>Sat, 29 Nov 2025 15:56:30 +0000</pubDate><dc:creator>Jing Hu</dc:creator><dc:creator>Hengkang Ye</dc:creator><dc:creator>Weiwei Zhong</dc:creator><dc:creator>Zican Shi</dc:creator><dc:creator>Yifan Chen</dc:creator><dc:creator>Jie Ren</dc:creator><dc:creator>Xiaohui Zhu</dc:creator><dc:creator>Li Fan</dc:creator><prism:publicationName>Pattern Recognition Letters</prism:publicationName><prism:doi>10.1016/j.patrec.2025.11.033</prism:doi><description>Cross-Domain Few-Shot Object Detection (CD-FSOD) from visible to infrared domains faces a critical challenge: object classification proves significantly more error-prone than localization under fine-tuning adaptation. This stems from substantial representational discrepancies in internal object features between domains, which hinder effective transfer. To enhance the saliency of infrared internal object features and mitigate classification errors in few-shot visible-to-infrared transfer, we propose the Class-Aware Memory Network for Few-Shot Object Detection (CAMN-FSOD). CAMN explicitly memories high-quality internal object features during fine-tuning and leverages memory to augment features,boosting recognition accuracy during inference. Furthermore, we introduce our two-stage Decoupled-Coupled Fine-tuning approach (DCFA) to combat CAMN overfitting in few-shot training and maximize its effectiveness. We establish a visible-infrared FSOD benchmark dataset for evaluation. Extensive experiments demonstrate that CAMN-FSOD significantly enhances the few-shot learning capability of the base model without increasing trainable parameters. In the 1-shot setting, our method achieves 42.0 mAP 50 , which is 14.4 points higher than the baseline, and an overall mAP of 25.2, showing an improvement of 2.3 points, outperforming existing methods.
Published: 2025-11-29T15:56:30+00:00
Venue: Pattern Recognition Letters
Score: 0.772 (consider)</description><content:encoded>&lt;p&gt;&lt;strong&gt;Authors:&lt;/strong&gt; Jing Hu; Hengkang Ye; Weiwei Zhong; Zican Shi; Yifan Chen; Jie Ren; Xiaohui Zhu; Li Fan&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Journal:&lt;/strong&gt; Pattern Recognition Letters&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;a href="https://doi.org/10.1016/j.patrec.2025.11.033"&gt;10.1016/j.patrec.2025.11.033&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Relevance:&lt;/strong&gt; 0.772 (consider)&lt;/p&gt;
&lt;p&gt;Cross-Domain Few-Shot Object Detection (CD-FSOD) from visible to infrared domains faces a critical challenge: object classification proves significantly more error-prone than localization under fine-tuning adaptation. This stems from substantial representational discrepancies in internal object features between domains, which hinder effective transfer. To enhance the saliency of infrared internal object features and mitigate classification errors in few-shot visible-to-infrared transfer, we propose the Class-Aware Memory Network for Few-Shot Object Detection (CAMN-FSOD). CAMN explicitly memories high-quality internal object features during fine-tuning and leverages memory to augment features,boosting recognition accuracy during inference. Furthermore, we introduce our two-stage Decoupled-Coupled Fine-tuning approach (DCFA) to combat CAMN overfitting in few-shot training and maximize its effectiveness. We establish a visible-infrared FSOD benchmark dataset for evaluation. Extensive experiments demonstrate that CAMN-FSOD significantly enhances the few-shot learning capability of the base model without increasing trainable parameters. In the 1-shot setting, our method achieves 42.0 mAP 50 , which is 14.4 points higher than the baseline, and an overall mAP of 25.2, showing an improvement of 2.3 points, outperforming existing methods.&lt;/p&gt;</content:encoded></item><item><title>Architecture Decoupling Is Not All You Need For Unified Multimodal Model</title><link>https://arxiv.org/abs/2511.22663v1</link><guid>http://arxiv.org/abs/2511.22663v1</guid><pubDate>Thu, 27 Nov 2025 17:55:25 +0000</pubDate><dc:creator>Dian Zheng</dc:creator><dc:creator>Manyuan Zhang</dc:creator><dc:creator>Hongyu Li</dc:creator><dc:creator>Kai Zou</dc:creator><dc:creator>Hongbo Liu</dc:creator><dc:creator>Ziyu Guo</dc:creator><dc:creator>Kaituo Feng</dc:creator><dc:creator>Yexin Liu</dc:creator><dc:creator>Ying Luo</dc:creator><dc:creator>Yan Feng</dc:creator><dc:creator>Peng Pei</dc:creator><dc:creator>Xunliang Cai</dc:creator><dc:creator>Hongsheng Li</dc:creator><dc:type>preprint</dc:type><prism:publicationName>arXiv</prism:publicationName><description>Unified multimodal models for image generation and understanding represent a significant step toward AGI and have attracted widespread attention from researchers. The main challenge of this task lies in the difficulty in establishing an optimal training paradigm due to inherent conflicting targets in understanding and generation tasks. To alleviate these conflicts and pursue higher performance, many researchers adopt varying degrees of model decoupling (e.g., Double image encoders, MOE/MOT architecture, or frozen MLLM). However, excessive model decoupling can lead to the loss of interleave generation ability, undermining the original intent of unified models. In this work, we aim to explore how to mitigate task conflicts without resorting to model decoupling. Firstly, we analyze why decoupling alleviates conflicts by studying the cross-modal attention behavior of models. We observe that model decoupling essentially drives models toward task-specific multimodal interaction patterns, as seen in Qwen-VL and HunyuanImage, and that the more thorough the decoupling, the more consistent the behavior becomes. Motivated by this observation, we propose Attention Interaction Alignment (AIA) loss, which explicitly learns Task-Specific multimodal interaction patterns during training. To demonstrate the generalizability of our AIA loss, we apply it to Emu3 and Janus-Pro during SFT and post-training stage respectively. Without bells and whistles, AIA not only refines cross-modal attention patterns, but also boosts both generation and understanding performance.
Published: 2025-11-27T17:55:25+00:00
Venue: arXiv
Score: 0.772 (consider)</description><content:encoded>&lt;p&gt;&lt;strong&gt;Authors:&lt;/strong&gt; Dian Zheng; Manyuan Zhang; Hongyu Li; Kai Zou; Hongbo Liu; Ziyu Guo; Kaituo Feng; Yexin Liu; Ying Luo; Yan Feng; Peng Pei; Xunliang Cai; Hongsheng Li&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Journal:&lt;/strong&gt; arXiv&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Relevance:&lt;/strong&gt; 0.772 (consider)&lt;/p&gt;
&lt;p&gt;Unified multimodal models for image generation and understanding represent a significant step toward AGI and have attracted widespread attention from researchers. The main challenge of this task lies in the difficulty in establishing an optimal training paradigm due to inherent conflicting targets in understanding and generation tasks. To alleviate these conflicts and pursue higher performance, many researchers adopt varying degrees of model decoupling (e.g., Double image encoders, MOE/MOT architecture, or frozen MLLM). However, excessive model decoupling can lead to the loss of interleave generation ability, undermining the original intent of unified models. In this work, we aim to explore how to mitigate task conflicts without resorting to model decoupling. Firstly, we analyze why decoupling alleviates conflicts by studying the cross-modal attention behavior of models. We observe that model decoupling essentially drives models toward task-specific multimodal interaction patterns, as seen in Qwen-VL and HunyuanImage, and that the more thorough the decoupling, the more consistent the behavior becomes. Motivated by this observation, we propose Attention Interaction Alignment (AIA) loss, which explicitly learns Task-Specific multimodal interaction patterns during training. To demonstrate the generalizability of our AIA loss, we apply it to Emu3 and Janus-Pro during SFT and post-training stage respectively. Without bells and whistles, AIA not only refines cross-modal attention patterns, but also boosts both generation and understanding performance.&lt;/p&gt;</content:encoded></item><item><title>SMFNet: Stacking multi-frame network for 4D spatial-temporal LiDAR semantic segmentation</title><link>https://doi.org/10.1016/j.neucom.2025.132233</link><guid>10.1016/j.neucom.2025.132233</guid><pubDate>Sat, 29 Nov 2025 16:00:45 +0000</pubDate><dc:creator>Xindong Guo</dc:creator><dc:creator>Zhongyu Chen</dc:creator><dc:creator>Rong Zhao</dc:creator><dc:creator>Xie Han</dc:creator><prism:publicationName>Neurocomputing</prism:publicationName><prism:doi>10.1016/j.neucom.2025.132233</prism:doi><description>In many real-world applications such as autonomous vehicles and robots, LiDAR sensor is a essential equipment due to its capacity of scanning surrounding environment. It produces sequential point clouds by rotating the laser emitter in a consecutive pattern. Therefore, semantic segmentation of the sequential point clouds is critical to understanding the surrounding environment for autonomous vehicles and robots. Unlike the semantic segmentation of single scan, this task requires distinguishing the moving objects from static ones. However, existing semantic segmentation methods for single scan perform poorly on the multi-scan task due to the lack of temporal information. In this paper, we propose a novel framework, which consists of a Spatial-Aware Feature Learning module (SAFL) and a Temporal-Aware Feature Learning module (TAFL), to extract spatial and temporal information in a unified pattern. Specifically, we project each point cloud into a pseudo-image by spherical projection and stack some sequential images along the temporal dimension, forming a 3D grid. First, the SAFL module extracts spatial features for each voxel using submanifold sparse convolution and reduces the resolution through a sparse convolution. Then, the TAFL module adopts a window-based Transformer along with a specially designed mask mechanism to learn the temporal information. Moreover, we design a Motion-Aware Feature Learning module (MAFL), using an optimized 2D network and residual images built from the stacked images to strengthen the moving feature learning and facilitate the prediction of moving objects. We evaluate our proposed method on the synthia4D and SemanticKITTI multi-scan datasets and the results demonstrate that our method achieves competitive results than most previous methods with less latency, which provides a novel idea to process 4D LiDAR tasks. The code is available at https://github.com/daojianqingchou/SMFNet.git .
Published: 2025-11-29T16:00:45+00:00
Venue: Neurocomputing
Score: 0.772 (consider)</description><content:encoded>&lt;p&gt;&lt;strong&gt;Authors:&lt;/strong&gt; Xindong Guo; Zhongyu Chen; Rong Zhao; Xie Han&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Journal:&lt;/strong&gt; Neurocomputing&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;a href="https://doi.org/10.1016/j.neucom.2025.132233"&gt;10.1016/j.neucom.2025.132233&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Relevance:&lt;/strong&gt; 0.772 (consider)&lt;/p&gt;
&lt;p&gt;In many real-world applications such as autonomous vehicles and robots, LiDAR sensor is a essential equipment due to its capacity of scanning surrounding environment. It produces sequential point clouds by rotating the laser emitter in a consecutive pattern. Therefore, semantic segmentation of the sequential point clouds is critical to understanding the surrounding environment for autonomous vehicles and robots. Unlike the semantic segmentation of single scan, this task requires distinguishing the moving objects from static ones. However, existing semantic segmentation methods for single scan perform poorly on the multi-scan task due to the lack of temporal information. In this paper, we propose a novel framework, which consists of a Spatial-Aware Feature Learning module (SAFL) and a Temporal-Aware Feature Learning module (TAFL), to extract spatial and temporal information in a unified pattern. Specifically, we project each point cloud into a pseudo-image by spherical projection and stack some sequential images along the temporal dimension, forming a 3D grid. First, the SAFL module extracts spatial features for each voxel using submanifold sparse convolution and reduces the resolution through a sparse convolution. Then, the TAFL module adopts a window-based Transformer along with a specially designed mask mechanism to learn the temporal information. Moreover, we design a Motion-Aware Feature Learning module (MAFL), using an optimized 2D network and residual images built from the stacked images to strengthen the moving feature learning and facilitate the prediction of moving objects. We evaluate our proposed method on the synthia4D and SemanticKITTI multi-scan datasets and the results demonstrate that our method achieves competitive results than most previous methods with less latency, which provides a novel idea to process 4D LiDAR tasks. The code is available at https://github.com/daojianqingchou/SMFNet.git .&lt;/p&gt;</content:encoded></item><item><title>SparseWorld-TC: Trajectory-Conditioned Sparse Occupancy World Model</title><link>https://arxiv.org/abs/2511.22039v1</link><guid>http://arxiv.org/abs/2511.22039v1</guid><pubDate>Thu, 27 Nov 2025 02:48:45 +0000</pubDate><dc:creator>Jiayuan Du</dc:creator><dc:creator>Yiming Zhao</dc:creator><dc:creator>Zhenglong Guo</dc:creator><dc:creator>Yong Pan</dc:creator><dc:creator>Wenbo Hou</dc:creator><dc:creator>Zhihui Hao</dc:creator><dc:creator>Kun Zhan</dc:creator><dc:creator>Qijun Chen</dc:creator><dc:type>preprint</dc:type><prism:publicationName>arXiv</prism:publicationName><description>This paper introduces a novel architecture for trajectory-conditioned forecasting of future 3D scene occupancy. In contrast to methods that rely on variational autoencoders (VAEs) to generate discrete occupancy tokens, which inherently limit representational capacity, our approach predicts multi-frame future occupancy in an end-to-end manner directly from raw image features. Inspired by the success of attention-based transformer architectures in foundational vision and language models such as GPT and VGGT, we employ a sparse occupancy representation that bypasses the intermediate bird's eye view (BEV) projection and its explicit geometric priors. This design allows the transformer to capture spatiotemporal dependencies more effectively. By avoiding both the finite-capacity constraint of discrete tokenization and the structural limitations of BEV representations, our method achieves state-of-the-art performance on the nuScenes benchmark for 1-3 second occupancy forecasting, outperforming existing approaches by a significant margin. Furthermore, it demonstrates robust scene dynamics understanding, consistently delivering high accuracy under arbitrary future trajectory conditioning.
Published: 2025-11-27T02:48:45+00:00
Venue: arXiv
Score: 0.772 (consider)</description><content:encoded>&lt;p&gt;&lt;strong&gt;Authors:&lt;/strong&gt; Jiayuan Du; Yiming Zhao; Zhenglong Guo; Yong Pan; Wenbo Hou; Zhihui Hao; Kun Zhan; Qijun Chen&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Journal:&lt;/strong&gt; arXiv&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Relevance:&lt;/strong&gt; 0.772 (consider)&lt;/p&gt;
&lt;p&gt;This paper introduces a novel architecture for trajectory-conditioned forecasting of future 3D scene occupancy. In contrast to methods that rely on variational autoencoders (VAEs) to generate discrete occupancy tokens, which inherently limit representational capacity, our approach predicts multi-frame future occupancy in an end-to-end manner directly from raw image features. Inspired by the success of attention-based transformer architectures in foundational vision and language models such as GPT and VGGT, we employ a sparse occupancy representation that bypasses the intermediate bird&amp;#x27;s eye view (BEV) projection and its explicit geometric priors. This design allows the transformer to capture spatiotemporal dependencies more effectively. By avoiding both the finite-capacity constraint of discrete tokenization and the structural limitations of BEV representations, our method achieves state-of-the-art performance on the nuScenes benchmark for 1-3 second occupancy forecasting, outperforming existing approaches by a significant margin. Furthermore, it demonstrates robust scene dynamics understanding, consistently delivering high accuracy under arbitrary future trajectory conditioning.&lt;/p&gt;</content:encoded></item></channel></rss>