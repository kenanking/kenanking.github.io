<?xml version='1.0' encoding='utf-8'?>
<rss xmlns:content="http://purl.org/rss/1.0/modules/content/" xmlns:dc="http://purl.org/dc/elements/1.1/" xmlns:prism="http://prismstandard.org/namespaces/basic/3.0/" version="2.0"><channel><title>ZotWatch Feed</title><link>https://ehehe.cn/zotwatch/</link><description>AI-assisted literature watch</description><lastBuildDate>Wed, 10 Dec 2025 02:47:53 +0000</lastBuildDate><item><title>Optical-to-SAR Domain Adaptation with Inversion Regularization for Unsupervised Ship Detection</title><link>https://doi.org/10.1016/j.knosys.2025.115044</link><guid>10.1016/j.knosys.2025.115044</guid><pubDate>Tue, 09 Dec 2025 07:42:52 +0000</pubDate><dc:creator>Shijie Wang</dc:creator><dc:creator>Yuanfei Huang</dc:creator><dc:creator>Ping Wang</dc:creator><dc:creator>Lei Lu</dc:creator><dc:creator>Hua Huang</dc:creator><prism:publicationName>Knowledge-Based Systems</prism:publicationName><prism:doi>10.1016/j.knosys.2025.115044</prism:doi><description>Due to the challenges of synthetic aperture radar (SAR) data acquisition and the high cost of manual annotation, utilizing labeled optical images to learn from unlabeled SAR data has received great attention. Cross-modal domain adaptation (DA) from optical to SAR imagery presents a particularly difficult problem because of the inherent modality gap between these two imaging paradigms. To address the problem of unsupervised ship detection in SAR images, we conduct domain adaptation experiments from ship images in the DIOR dataset to the SSDD dataset. However, traditional domain adaptation methods are insufficient to address the significant modality differences between optical and SAR images. Although the unconstrained feature alignment strategy is effective between domains with small differences, it inadvertently expels SAR features from the supervised recognition space, ultimately reducing detection performance. To mitigate this issue, we propose a new framework, DAIR, which integrates an innovative inversion regularization module (IRM) and a task-correlation enhancement (TCE) strategy to improve domain adaptation. Specifically, IRM acts as a feature-space regularizer to counteract deviation caused by aggressive alignment, while TCE explicitly models task interdependency to alleviate the effects of task independence. Evaluated on the DIOR and SSDD datasets, our method achieved improvements of 5% and 8% in AP50 and APm, respectively, over the baseline method DA Faster R-CNN. In few-shot scenarios, our method attained gains of 14.1%, 10.6%, and 15.4% in AP50 compared to the state-of-the-art method under 3-shot, 5-shot, and 10-shot settings, which demonstrates stronger generalization ability with limited data. Source code and models are available at https://github.com/whatbb/DAIR/tree/main
Published: 2025-12-09T07:42:52+00:00
Venue: Knowledge-Based Systems
Score: 0.851 (must_read)</description><content:encoded>&lt;p&gt;&lt;strong&gt;Authors:&lt;/strong&gt; Shijie Wang; Yuanfei Huang; Ping Wang; Lei Lu; Hua Huang&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Journal:&lt;/strong&gt; Knowledge-Based Systems&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;a href="https://doi.org/10.1016/j.knosys.2025.115044"&gt;10.1016/j.knosys.2025.115044&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Relevance:&lt;/strong&gt; 0.851 (must_read)&lt;/p&gt;
&lt;p&gt;Due to the challenges of synthetic aperture radar (SAR) data acquisition and the high cost of manual annotation, utilizing labeled optical images to learn from unlabeled SAR data has received great attention. Cross-modal domain adaptation (DA) from optical to SAR imagery presents a particularly difficult problem because of the inherent modality gap between these two imaging paradigms. To address the problem of unsupervised ship detection in SAR images, we conduct domain adaptation experiments from ship images in the DIOR dataset to the SSDD dataset. However, traditional domain adaptation methods are insufficient to address the significant modality differences between optical and SAR images. Although the unconstrained feature alignment strategy is effective between domains with small differences, it inadvertently expels SAR features from the supervised recognition space, ultimately reducing detection performance. To mitigate this issue, we propose a new framework, DAIR, which integrates an innovative inversion regularization module (IRM) and a task-correlation enhancement (TCE) strategy to improve domain adaptation. Specifically, IRM acts as a feature-space regularizer to counteract deviation caused by aggressive alignment, while TCE explicitly models task interdependency to alleviate the effects of task independence. Evaluated on the DIOR and SSDD datasets, our method achieved improvements of 5% and 8% in AP50 and APm, respectively, over the baseline method DA Faster R-CNN. In few-shot scenarios, our method attained gains of 14.1%, 10.6%, and 15.4% in AP50 compared to the state-of-the-art method under 3-shot, 5-shot, and 10-shot settings, which demonstrates stronger generalization ability with limited data. Source code and models are available at https://github.com/whatbb/DAIR/tree/main&lt;/p&gt;</content:encoded></item><item><title>SaSAM: Scale-aware segmentation anything model for multimodal remote sensing images</title><link>https://doi.org/10.1016/j.inffus.2025.104054</link><guid>10.1016/j.inffus.2025.104054</guid><pubDate>Tue, 09 Dec 2025 07:46:35 +0000</pubDate><dc:creator>You Ma</dc:creator><dc:creator>Hongwei Tong</dc:creator><dc:creator>Lin Chai</dc:creator><dc:creator>Shihan Mao</dc:creator><dc:creator>Yucheng Zhang</dc:creator><prism:publicationName>Information Fusion</prism:publicationName><prism:doi>10.1016/j.inffus.2025.104054</prism:doi><description>The segment anything model (SAM) has achieved remarkable progress in remote sensing image segmentation tasks due to its exceptional segmentation performance and generalization ability. However, existing SAM-based methods primarily focus on modeling single-modal data, which limits the feature representation of the model in complex scenarios. Additionally, these methods fail to fully exploit multi-scale information during fine-tuning and lack collaborative utilization of multi-level features. To address these issues, we propose the scale-aware SAM (SaSAM) framework, aiming to explore the potential of SAM for semantic segmentation of multimodal remote sensing images. Specifically, we first employ a dual attention feature fusion module to integrate multimodal features into a unified feature, enabling the adaptation of SAM to multimodal tasks without altering its original structure. Next, we design a mixture of multi-scale LoRA experts module that captures object features at different scales through multiple lightweight LoRA experts to enhance the scale-aware capability of the model. Subsequently, we introduce a multi-level feature adaptive aggregation module to fully utilize the multi-granularity features in the SAM encoder. Finally, the multi-scale features are input into the masked decoder to generate accurate segmentation results. Extensive experiments on three multimodal remote sensing datasets demonstrate the superiority of our method. The source code is available at https://github.com/MaYou1997/SaSAM .
Published: 2025-12-09T07:46:35+00:00
Venue: Information Fusion
Score: 0.839 (must_read)</description><content:encoded>&lt;p&gt;&lt;strong&gt;Authors:&lt;/strong&gt; You Ma; Hongwei Tong; Lin Chai; Shihan Mao; Yucheng Zhang&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Journal:&lt;/strong&gt; Information Fusion&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;a href="https://doi.org/10.1016/j.inffus.2025.104054"&gt;10.1016/j.inffus.2025.104054&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Relevance:&lt;/strong&gt; 0.839 (must_read)&lt;/p&gt;
&lt;p&gt;The segment anything model (SAM) has achieved remarkable progress in remote sensing image segmentation tasks due to its exceptional segmentation performance and generalization ability. However, existing SAM-based methods primarily focus on modeling single-modal data, which limits the feature representation of the model in complex scenarios. Additionally, these methods fail to fully exploit multi-scale information during fine-tuning and lack collaborative utilization of multi-level features. To address these issues, we propose the scale-aware SAM (SaSAM) framework, aiming to explore the potential of SAM for semantic segmentation of multimodal remote sensing images. Specifically, we first employ a dual attention feature fusion module to integrate multimodal features into a unified feature, enabling the adaptation of SAM to multimodal tasks without altering its original structure. Next, we design a mixture of multi-scale LoRA experts module that captures object features at different scales through multiple lightweight LoRA experts to enhance the scale-aware capability of the model. Subsequently, we introduce a multi-level feature adaptive aggregation module to fully utilize the multi-granularity features in the SAM encoder. Finally, the multi-scale features are input into the masked decoder to generate accurate segmentation results. Extensive experiments on three multimodal remote sensing datasets demonstrate the superiority of our method. The source code is available at https://github.com/MaYou1997/SaSAM .&lt;/p&gt;</content:encoded></item><item><title>Test-time Correction: An Online 3D Detection System via Visual Prompting</title><link>https://doi.org/10.1109/tpami.2025.3642076</link><guid>10.1109/tpami.2025.3642076</guid><pubDate>Tue, 09 Dec 2025 18:32:53 +0000</pubDate><dc:creator>Hanxue Zhang</dc:creator><dc:creator>Zetong Yang</dc:creator><dc:creator>Yanan Sun</dc:creator><dc:creator>Li Chen</dc:creator><dc:creator>Fei Xia</dc:creator><dc:creator>Fatma Güney</dc:creator><dc:creator>Hongyang Li</dc:creator><prism:publicationName>IEEE Transactions on Pattern Analysis and Machine Intelligence</prism:publicationName><prism:doi>10.1109/tpami.2025.3642076</prism:doi><description>This paper introduces Test-time Correction (TTC), an online 3D detection system designed to rectify test-time errors using various auxiliary feedback, aiming to enhance the safety of deployed autonomous driving systems. Unlike conventional offline 3D detectors that remain fixed during inference, TTC enables immediate online error correction without retraining, allowing autonomous vehicles to adapt to new scenarios and reduce deployment risks. To achieve this, we equip existing 3D detectors with an Online Adapter (OA) module-a prompt-driven query generator for real-time correction. At the core of OA module are visual prompts: image-based descriptions of objects of interest derived from auxiliary feedback such as mismatches with 2D detections, road descriptions, or user clicks. These visual prompts, collected from risky objects during inference, are maintained in a visual prompt buffer to enable continuous correction in future frames. By leveraging this mechanism, TTC consistently detects risky objects, achieving reliable, adaptive, and versatile driving autonomy. Extensive experiments show that TTC significantly improves instant error rectification over frozen 3D detectors, even under limited labels, zero-shot settings, and adverse conditions. We hope this work inspires future research on post-deployment online rectification systems for autonomous driving.
Published: 2025-12-09T18:32:53+00:00
Venue: IEEE Transactions on Pattern Analysis and Machine Intelligence
Score: 0.806 (must_read)</description><content:encoded>&lt;p&gt;&lt;strong&gt;Authors:&lt;/strong&gt; Hanxue Zhang; Zetong Yang; Yanan Sun; Li Chen; Fei Xia; Fatma Güney; Hongyang Li&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Journal:&lt;/strong&gt; IEEE Transactions on Pattern Analysis and Machine Intelligence&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;a href="https://doi.org/10.1109/tpami.2025.3642076"&gt;10.1109/tpami.2025.3642076&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Relevance:&lt;/strong&gt; 0.806 (must_read)&lt;/p&gt;
&lt;p&gt;This paper introduces Test-time Correction (TTC), an online 3D detection system designed to rectify test-time errors using various auxiliary feedback, aiming to enhance the safety of deployed autonomous driving systems. Unlike conventional offline 3D detectors that remain fixed during inference, TTC enables immediate online error correction without retraining, allowing autonomous vehicles to adapt to new scenarios and reduce deployment risks. To achieve this, we equip existing 3D detectors with an Online Adapter (OA) module-a prompt-driven query generator for real-time correction. At the core of OA module are visual prompts: image-based descriptions of objects of interest derived from auxiliary feedback such as mismatches with 2D detections, road descriptions, or user clicks. These visual prompts, collected from risky objects during inference, are maintained in a visual prompt buffer to enable continuous correction in future frames. By leveraging this mechanism, TTC consistently detects risky objects, achieving reliable, adaptive, and versatile driving autonomy. Extensive experiments show that TTC significantly improves instant error rectification over frozen 3D detectors, even under limited labels, zero-shot settings, and adverse conditions. We hope this work inspires future research on post-deployment online rectification systems for autonomous driving.&lt;/p&gt;</content:encoded></item><item><title>NSDSAM: Noise-Suppression-Driven SAM for Infrared Small Target Detection</title><link>https://doi.org/10.1109/tgrs.2025.3642125</link><guid>10.1109/tgrs.2025.3642125</guid><pubDate>Tue, 09 Dec 2025 18:32:55 +0000</pubDate><dc:creator>Wenxiao Xu</dc:creator><dc:creator>Qiyuan Yin</dc:creator><dc:creator>Chen Wu</dc:creator><dc:creator>Dianjie Lu</dc:creator><dc:creator>Guijuan Zhang</dc:creator><dc:creator>Zhuoran Zheng</dc:creator><prism:publicationName>IEEE Transactions on Geoscience and Remote Sensing</prism:publicationName><prism:doi>10.1109/tgrs.2025.3642125</prism:doi><description>Although Segment Anything Model (SAM) have recently achieved remarkable progress, their generalization capability in infrared small target detection remains limited due to the inherently high noise levels in infrared imagery. To preserve the generalization and noise suppression ability of the model, we propose an method called NSDSAM, a noise-suppression-driven approach that enhances SAM at both internal and external levels. Internally, we develop a Hybrid Adapter for suppressing the noise of feature maps, consisting of an MLP adapter and a self-attention adapter. The self-attention adapter first performs entropy-aware reconstruction of features from noisy inputs and employs a gating mechanism for soft-attention fusion, mitigating SAM’s sensitivity to noise. Externally, we design a Spatial-Frequency hybrid Module (SFHM) that jointly processes spatial and frequency domains to overcome the self-attention model’s bias toward low-frequency components, further strengthening the suppression of background clutter and noise. Extensive experiments on multiple infrared datasets demonstrate that the proposed method achieves state-of-the-art (SOTA) performance in infrared small target detection. The project code is available upon acceptance.
Published: 2025-12-09T18:32:55+00:00
Venue: IEEE Transactions on Geoscience and Remote Sensing
Score: 0.805 (must_read)</description><content:encoded>&lt;p&gt;&lt;strong&gt;Authors:&lt;/strong&gt; Wenxiao Xu; Qiyuan Yin; Chen Wu; Dianjie Lu; Guijuan Zhang; Zhuoran Zheng&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Journal:&lt;/strong&gt; IEEE Transactions on Geoscience and Remote Sensing&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;a href="https://doi.org/10.1109/tgrs.2025.3642125"&gt;10.1109/tgrs.2025.3642125&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Relevance:&lt;/strong&gt; 0.805 (must_read)&lt;/p&gt;
&lt;p&gt;Although Segment Anything Model (SAM) have recently achieved remarkable progress, their generalization capability in infrared small target detection remains limited due to the inherently high noise levels in infrared imagery. To preserve the generalization and noise suppression ability of the model, we propose an method called NSDSAM, a noise-suppression-driven approach that enhances SAM at both internal and external levels. Internally, we develop a Hybrid Adapter for suppressing the noise of feature maps, consisting of an MLP adapter and a self-attention adapter. The self-attention adapter first performs entropy-aware reconstruction of features from noisy inputs and employs a gating mechanism for soft-attention fusion, mitigating SAM’s sensitivity to noise. Externally, we design a Spatial-Frequency hybrid Module (SFHM) that jointly processes spatial and frequency domains to overcome the self-attention model’s bias toward low-frequency components, further strengthening the suppression of background clutter and noise. Extensive experiments on multiple infrared datasets demonstrate that the proposed method achieves state-of-the-art (SOTA) performance in infrared small target detection. The project code is available upon acceptance.&lt;/p&gt;</content:encoded></item><item><title>Self-Adaptive Vision-Language Tracking with Context Prompting</title><link>https://doi.org/10.1109/tip.2025.3635016</link><guid>10.1109/tip.2025.3635016</guid><pubDate>Mon, 08 Dec 2025 18:42:57 +0000</pubDate><dc:creator>Jie Zhao</dc:creator><dc:creator>Xin Chen</dc:creator><dc:creator>Shengming Li</dc:creator><dc:creator>Chunjuan Bo</dc:creator><dc:creator>Dong Wang</dc:creator><dc:creator>Huchuan Lu</dc:creator><prism:publicationName>IEEE Transactions on Image Processing</prism:publicationName><prism:doi>10.1109/tip.2025.3635016</prism:doi><description>Due to the substantial gap between vision and language modalities, along with the mismatch problem between fixed language descriptions and dynamic visual information, existing vision-language tracking methods exhibit performance on par with or slightly worse than vision-only tracking. Effectively exploiting the rich semantics of language to enhance tracking robustness remains an open challenge. To address these issues, we propose a self-adaptive vision-language tracking framework that leverages the pre-trained multi-modal CLIP model to obtain well-aligned visual-language representations. A novel context-aware prompting mechanism is introduced to dynamically adapt linguistic cues based on the evolving visual context during tracking. Specifically, our context prompter extracts dynamic visual features from the current search image and integrates them into the text encoding process, enabling self-updating language embeddings. Furthermore, our framework employs a unified one-stream Transformer architecture, supporting joint training for both vision-only and vision-language tracking scenarios. Our method not only bridges the modality gap but also enhances robustness by allowing language features to evolve with visual context. Extensive experiments on four vision-language tracking benchmarks demonstrate that our method effectively leverages the advantages of language to enhance visual tracking. Our large model can obtain 55.0% AUC on LaSOTEXT and 69.0% AUC on TNL2K. Additionally, our language-only tracking model achieves performance comparable to that of state-of-the-art vision-only tracking methods on TNL2K. Code is available at https://github.com/zj5559/SAVLT.
Published: 2025-12-08T18:42:57+00:00
Venue: IEEE Transactions on Image Processing
Score: 0.805 (must_read)</description><content:encoded>&lt;p&gt;&lt;strong&gt;Authors:&lt;/strong&gt; Jie Zhao; Xin Chen; Shengming Li; Chunjuan Bo; Dong Wang; Huchuan Lu&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Journal:&lt;/strong&gt; IEEE Transactions on Image Processing&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;a href="https://doi.org/10.1109/tip.2025.3635016"&gt;10.1109/tip.2025.3635016&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Relevance:&lt;/strong&gt; 0.805 (must_read)&lt;/p&gt;
&lt;p&gt;Due to the substantial gap between vision and language modalities, along with the mismatch problem between fixed language descriptions and dynamic visual information, existing vision-language tracking methods exhibit performance on par with or slightly worse than vision-only tracking. Effectively exploiting the rich semantics of language to enhance tracking robustness remains an open challenge. To address these issues, we propose a self-adaptive vision-language tracking framework that leverages the pre-trained multi-modal CLIP model to obtain well-aligned visual-language representations. A novel context-aware prompting mechanism is introduced to dynamically adapt linguistic cues based on the evolving visual context during tracking. Specifically, our context prompter extracts dynamic visual features from the current search image and integrates them into the text encoding process, enabling self-updating language embeddings. Furthermore, our framework employs a unified one-stream Transformer architecture, supporting joint training for both vision-only and vision-language tracking scenarios. Our method not only bridges the modality gap but also enhances robustness by allowing language features to evolve with visual context. Extensive experiments on four vision-language tracking benchmarks demonstrate that our method effectively leverages the advantages of language to enhance visual tracking. Our large model can obtain 55.0% AUC on LaSOTEXT and 69.0% AUC on TNL2K. Additionally, our language-only tracking model achieves performance comparable to that of state-of-the-art vision-only tracking methods on TNL2K. Code is available at https://github.com/zj5559/SAVLT.&lt;/p&gt;</content:encoded></item><item><title>Comparative Analysis and Parametric Tuning of PPO, GRPO, and DAPO for LLM Reasoning Enhancement</title><link>https://arxiv.org/abs/2512.07611v1</link><guid>http://arxiv.org/abs/2512.07611v1</guid><pubDate>Mon, 08 Dec 2025 14:58:19 +0000</pubDate><dc:creator>Yongsheng Lian</dc:creator><dc:type>preprint</dc:type><prism:publicationName>arXiv</prism:publicationName><description>This study presents a systematic comparison of three Reinforcement Learning (RL) algorithms (PPO, GRPO, and DAPO) for improving complex reasoning in large language models (LLMs). Our main contribution is a controlled transfer-learning evaluation: models are first fine-tuned on the specialized Countdown Game and then assessed on a suite of general-purpose reasoning benchmarks. Across all tasks, RL-trained models outperform their corresponding base models, although the degree of improvement differs by benchmark.
  Our parametric analysis offers practical guidance for RL-based LLM training. Increasing the group size in GRPO and DAPO leads to more stable training dynamics and higher accuracy, while the impact of the KL-penalty coefficient is non-monotonic. Additionally, we find that the Dynamic Sampling (DS) component in DAPO does not improve performance; in fact, the best overall results are achieved with DAPO when DS is disabled.
Published: 2025-12-08T14:58:19+00:00
Venue: arXiv
Score: 0.805 (must_read)</description><content:encoded>&lt;p&gt;&lt;strong&gt;Authors:&lt;/strong&gt; Yongsheng Lian&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Journal:&lt;/strong&gt; arXiv&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Relevance:&lt;/strong&gt; 0.805 (must_read)&lt;/p&gt;
&lt;p&gt;This study presents a systematic comparison of three Reinforcement Learning (RL) algorithms (PPO, GRPO, and DAPO) for improving complex reasoning in large language models (LLMs). Our main contribution is a controlled transfer-learning evaluation: models are first fine-tuned on the specialized Countdown Game and then assessed on a suite of general-purpose reasoning benchmarks. Across all tasks, RL-trained models outperform their corresponding base models, although the degree of improvement differs by benchmark.
  Our parametric analysis offers practical guidance for RL-based LLM training. Increasing the group size in GRPO and DAPO leads to more stable training dynamics and higher accuracy, while the impact of the KL-penalty coefficient is non-monotonic. Additionally, we find that the Dynamic Sampling (DS) component in DAPO does not improve performance; in fact, the best overall results are achieved with DAPO when DS is disabled.&lt;/p&gt;</content:encoded></item><item><title>EnBoT-SORT: Hierarchical fusion-association tracking with pseudo-sample generation for dense thermal infrared UAVs</title><link>https://doi.org/10.1016/j.isprsjprs.2025.12.001</link><guid>10.1016/j.isprsjprs.2025.12.001</guid><pubDate>Mon, 08 Dec 2025 11:36:39 +0000</pubDate><dc:creator>Jinxin Guo</dc:creator><dc:creator>Weida Zhan</dc:creator><dc:creator>Yu Chen</dc:creator><dc:creator>Depeng Zhu</dc:creator><dc:creator>Yichun Jiang</dc:creator><dc:creator>Xiaoyu Xu</dc:creator><dc:creator>Deng Han</dc:creator><prism:publicationName>ISPRS Journal of Photogrammetry and Remote Sensing</prism:publicationName><prism:doi>10.1016/j.isprsjprs.2025.12.001</prism:doi><description>Thermal infrared dense UAV target detection and tracking present significant challenges at both data and algorithmic levels. At the data level, there exists a scarcity of accurately annotated real-world samples coupled with high acquisition costs. At the algorithmic level, the key difficulty lies in addressing frequent identity switches caused by highly dense target clustering, frequent occlusions, and reappearances. To overcome these challenges, this paper proposes an innovative infrared pseudo-sample generation paradigm by designing a physically-driven Heterogeneous Interactive Degradation Model (HIDM). This model simulates real infrared imaging through background-target cooperative degradation mechanisms that account for multiple coupled degradation factors, combined with a random trajectory generation strategy to produce large-scale physically realistic pseudo-sample data, significantly enhancing the domain adaptability of the generated data. Building upon this foundation, we propose a hierarchical fusion-association tracking framework—EnBoT-SORT. This framework employs YOLOv12 as a powerful detector and innovatively incorporates a dynamic target density regulator, a hybrid feature association engine, and a trajectory continuity enhancement module into BoT-SORT, effectively maintaining the continuity and stability of target IDs. Experimental results demonstrate that EnBoT-SORT significantly outperforms existing trackers in intensive UAV motion scenarios, achieving state-of-the-art performance on the IRT-B and IRC-B datasets with HOTA scores of 68.7% and 67.3%, and MOTA scores of 76.2% and 74.6%, respectively. Furthermore, cross-modal experiments on real infrared and visible-light datasets indicate that EnBoT-SORT possesses strong generalization capabilities. This work provides a comprehensive solution for infrared-intensive UAV tracking, spanning from data generation to algorithmic optimization. Our code and datasets are available at GitHub .
Published: 2025-12-08T11:36:39+00:00
Venue: ISPRS Journal of Photogrammetry and Remote Sensing
Score: 0.803 (must_read)</description><content:encoded>&lt;p&gt;&lt;strong&gt;Authors:&lt;/strong&gt; Jinxin Guo; Weida Zhan; Yu Chen; Depeng Zhu; Yichun Jiang; Xiaoyu Xu; Deng Han&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Journal:&lt;/strong&gt; ISPRS Journal of Photogrammetry and Remote Sensing&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;a href="https://doi.org/10.1016/j.isprsjprs.2025.12.001"&gt;10.1016/j.isprsjprs.2025.12.001&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Relevance:&lt;/strong&gt; 0.803 (must_read)&lt;/p&gt;
&lt;p&gt;Thermal infrared dense UAV target detection and tracking present significant challenges at both data and algorithmic levels. At the data level, there exists a scarcity of accurately annotated real-world samples coupled with high acquisition costs. At the algorithmic level, the key difficulty lies in addressing frequent identity switches caused by highly dense target clustering, frequent occlusions, and reappearances. To overcome these challenges, this paper proposes an innovative infrared pseudo-sample generation paradigm by designing a physically-driven Heterogeneous Interactive Degradation Model (HIDM). This model simulates real infrared imaging through background-target cooperative degradation mechanisms that account for multiple coupled degradation factors, combined with a random trajectory generation strategy to produce large-scale physically realistic pseudo-sample data, significantly enhancing the domain adaptability of the generated data. Building upon this foundation, we propose a hierarchical fusion-association tracking framework—EnBoT-SORT. This framework employs YOLOv12 as a powerful detector and innovatively incorporates a dynamic target density regulator, a hybrid feature association engine, and a trajectory continuity enhancement module into BoT-SORT, effectively maintaining the continuity and stability of target IDs. Experimental results demonstrate that EnBoT-SORT significantly outperforms existing trackers in intensive UAV motion scenarios, achieving state-of-the-art performance on the IRT-B and IRC-B datasets with HOTA scores of 68.7% and 67.3%, and MOTA scores of 76.2% and 74.6%, respectively. Furthermore, cross-modal experiments on real infrared and visible-light datasets indicate that EnBoT-SORT possesses strong generalization capabilities. This work provides a comprehensive solution for infrared-intensive UAV tracking, spanning from data generation to algorithmic optimization. Our code and datasets are available at GitHub .&lt;/p&gt;</content:encoded></item><item><title>DINet: Depth-guided and Iterative Refinement Network for Salient Object Detection in Optical Remote Sensing Images</title><link>https://doi.org/10.1109/tgrs.2025.3641927</link><guid>10.1109/tgrs.2025.3641927</guid><pubDate>Tue, 09 Dec 2025 18:32:55 +0000</pubDate><dc:creator>Xihang Hu</dc:creator><dc:creator>Fuming Sun</dc:creator><dc:creator>Xiaoli Zhang</dc:creator><dc:creator>Chuanmin Jia</dc:creator><dc:creator>Siwei Ma</dc:creator><prism:publicationName>IEEE Transactions on Geoscience and Remote Sensing</prism:publicationName><prism:doi>10.1109/tgrs.2025.3641927</prism:doi><description>Optical remote sensing images (ORSI) feature unique scenes and complex imaging conditions. Specifically, they exhibit substantial variations in object scale, quantity, structure, and distribution. Consequently, salient object detection in ORSI (ORSI-SOD) is pivotal in ORSI content perception and understanding. Additionally, the limitations of the single modality impede the advancement of ORSI-SOD. To tackle these issues, we propose a Depth-guided and Iterative Refinement Network (DINet) for ORSI-SOD. By incorporating depth information as auxiliary cues, we introduce a multi-modal strategy for ORSI-SOD, resulting in improved accuracy in the localization and segmentation of salient objects. To address the variability of salient objects, we design an Aggregation Perception Enhancement (APE) Module. This module integrates complementary cues from cross-modal features using multi-dimensional attention mechanisms. By fostering cross-modal interactions, the APE module effectively preserves both detail and spatial location information. Furthermore, we propose an Iterative Guidance Refinement Decoder to handle boundary uncertainty. The decoder uses initial predictions to guide the decoding phase and iteratively refine results. Simultaneously, it minimizes noise from depth cues, yielding predictions with more accurate boundaries. Experimental comparisons with 22 state-of-the-art methods show that DINet exhibits superior performance while maintaining lightweight (11.98M) and real-time (55FPS) capabilities.
Published: 2025-12-09T18:32:55+00:00
Venue: IEEE Transactions on Geoscience and Remote Sensing
Score: 0.799 (must_read)</description><content:encoded>&lt;p&gt;&lt;strong&gt;Authors:&lt;/strong&gt; Xihang Hu; Fuming Sun; Xiaoli Zhang; Chuanmin Jia; Siwei Ma&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Journal:&lt;/strong&gt; IEEE Transactions on Geoscience and Remote Sensing&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;a href="https://doi.org/10.1109/tgrs.2025.3641927"&gt;10.1109/tgrs.2025.3641927&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Relevance:&lt;/strong&gt; 0.799 (must_read)&lt;/p&gt;
&lt;p&gt;Optical remote sensing images (ORSI) feature unique scenes and complex imaging conditions. Specifically, they exhibit substantial variations in object scale, quantity, structure, and distribution. Consequently, salient object detection in ORSI (ORSI-SOD) is pivotal in ORSI content perception and understanding. Additionally, the limitations of the single modality impede the advancement of ORSI-SOD. To tackle these issues, we propose a Depth-guided and Iterative Refinement Network (DINet) for ORSI-SOD. By incorporating depth information as auxiliary cues, we introduce a multi-modal strategy for ORSI-SOD, resulting in improved accuracy in the localization and segmentation of salient objects. To address the variability of salient objects, we design an Aggregation Perception Enhancement (APE) Module. This module integrates complementary cues from cross-modal features using multi-dimensional attention mechanisms. By fostering cross-modal interactions, the APE module effectively preserves both detail and spatial location information. Furthermore, we propose an Iterative Guidance Refinement Decoder to handle boundary uncertainty. The decoder uses initial predictions to guide the decoding phase and iteratively refine results. Simultaneously, it minimizes noise from depth cues, yielding predictions with more accurate boundaries. Experimental comparisons with 22 state-of-the-art methods show that DINet exhibits superior performance while maintaining lightweight (11.98M) and real-time (55FPS) capabilities.&lt;/p&gt;</content:encoded></item><item><title>rSIM: Incentivizing Reasoning Capabilities of LLMs via Reinforced Strategy Injection</title><link>https://arxiv.org/abs/2512.08300v1</link><guid>http://arxiv.org/abs/2512.08300v1</guid><pubDate>Tue, 09 Dec 2025 06:55:39 +0000</pubDate><dc:creator>Sijia Chen</dc:creator><dc:creator>Baochun Li</dc:creator><dc:creator>Di Niu</dc:creator><dc:type>preprint</dc:type><prism:publicationName>arXiv</prism:publicationName><description>Large language models (LLMs) are post-trained through reinforcement learning (RL) to evolve into Reasoning Language Models (RLMs), where the hallmark of this advanced reasoning is ``aha'' moments when they start to perform strategies, such as self-reflection and deep thinking, within chain of thoughts (CoTs). Motivated by this, this paper proposes a novel reinforced strategy injection mechanism (rSIM), that enables any LLM to become an RLM by employing a small planner to guide the LLM's CoT through the adaptive injection of reasoning strategies. To achieve this, the planner (leader agent) is jointly trained with an LLM (follower agent) using multi-agent RL (MARL), based on a leader-follower framework and straightforward rule-based rewards. Experimental results show that rSIM enables Qwen2.5-0.5B to become an RLM and significantly outperform Qwen2.5-14B. Moreover, the planner is generalizable: it only needs to be trained once and can be applied as a plug-in to substantially improve the reasoning capabilities of existing LLMs. In addition, the planner supports continual learning across various tasks, allowing its planning abilities to gradually improve and generalize to a wider range of problems.
Published: 2025-12-09T06:55:39+00:00
Venue: arXiv
Score: 0.798 (must_read)</description><content:encoded>&lt;p&gt;&lt;strong&gt;Authors:&lt;/strong&gt; Sijia Chen; Baochun Li; Di Niu&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Journal:&lt;/strong&gt; arXiv&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Relevance:&lt;/strong&gt; 0.798 (must_read)&lt;/p&gt;
&lt;p&gt;Large language models (LLMs) are post-trained through reinforcement learning (RL) to evolve into Reasoning Language Models (RLMs), where the hallmark of this advanced reasoning is ``aha&amp;#x27;&amp;#x27; moments when they start to perform strategies, such as self-reflection and deep thinking, within chain of thoughts (CoTs). Motivated by this, this paper proposes a novel reinforced strategy injection mechanism (rSIM), that enables any LLM to become an RLM by employing a small planner to guide the LLM&amp;#x27;s CoT through the adaptive injection of reasoning strategies. To achieve this, the planner (leader agent) is jointly trained with an LLM (follower agent) using multi-agent RL (MARL), based on a leader-follower framework and straightforward rule-based rewards. Experimental results show that rSIM enables Qwen2.5-0.5B to become an RLM and significantly outperform Qwen2.5-14B. Moreover, the planner is generalizable: it only needs to be trained once and can be applied as a plug-in to substantially improve the reasoning capabilities of existing LLMs. In addition, the planner supports continual learning across various tasks, allowing its planning abilities to gradually improve and generalize to a wider range of problems.&lt;/p&gt;</content:encoded></item><item><title>Beyond Pillars: Advancing 3D Object Detection with Salient Voxel Enhancement of LiDAR-4D Radar Fusion</title><link>https://doi.org/10.1016/j.patcog.2025.112841</link><guid>10.1016/j.patcog.2025.112841</guid><pubDate>Tue, 09 Dec 2025 00:21:13 +0000</pubDate><dc:creator>Pengfei Yang</dc:creator><dc:creator>Feng Wu</dc:creator><dc:creator>Minyang Liu</dc:creator><dc:creator>Ting Zhong</dc:creator><dc:creator>Fan Zhou</dc:creator><prism:publicationName>Pattern Recognition</prism:publicationName><prism:doi>10.1016/j.patcog.2025.112841</prism:doi><description>The fusion of LiDAR and 4D radar has emerged as a promising solution for robust and accurate 3D object detection in complex and adverse conditions. Existing methods typically rely on pillar-based representations, which, although computationally efficient, fail to provide fine-grained structural details necessary for precise object localization and recognition. In contrast, voxel-based representations offer richer spatial information but face challenges such as background noise and data quality disparity. To address these limitations, we propose SVEFusion, a voxel-based 3D object detection framework that integrates LiDAR and 4D radar data using a salient voxel enhancement mechanism. Our method introduces an adaptive feature alignment module and a novel spatial neighborhood attention module for efficient early-stage multi-modal voxel feature integration. Furthermore, we design a salient voxel enhancement mechanism that assigns higher weights to foreground voxels using a multi-scale weight prediction strategy, progressively refining weight accuracy with supervision loss. Experimental results demonstrate that SVEFusion significantly outperforms state-of-the-art methods, establishing a new benchmark in multi-modal 3D object detection. The source code and network weighting for reproducibility are available at https://github.com/icdm-adteam/SVEFusion .
Published: 2025-12-09T00:21:13+00:00
Venue: Pattern Recognition
Score: 0.796 (must_read)</description><content:encoded>&lt;p&gt;&lt;strong&gt;Authors:&lt;/strong&gt; Pengfei Yang; Feng Wu; Minyang Liu; Ting Zhong; Fan Zhou&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Journal:&lt;/strong&gt; Pattern Recognition&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;a href="https://doi.org/10.1016/j.patcog.2025.112841"&gt;10.1016/j.patcog.2025.112841&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Relevance:&lt;/strong&gt; 0.796 (must_read)&lt;/p&gt;
&lt;p&gt;The fusion of LiDAR and 4D radar has emerged as a promising solution for robust and accurate 3D object detection in complex and adverse conditions. Existing methods typically rely on pillar-based representations, which, although computationally efficient, fail to provide fine-grained structural details necessary for precise object localization and recognition. In contrast, voxel-based representations offer richer spatial information but face challenges such as background noise and data quality disparity. To address these limitations, we propose SVEFusion, a voxel-based 3D object detection framework that integrates LiDAR and 4D radar data using a salient voxel enhancement mechanism. Our method introduces an adaptive feature alignment module and a novel spatial neighborhood attention module for efficient early-stage multi-modal voxel feature integration. Furthermore, we design a salient voxel enhancement mechanism that assigns higher weights to foreground voxels using a multi-scale weight prediction strategy, progressively refining weight accuracy with supervision loss. Experimental results demonstrate that SVEFusion significantly outperforms state-of-the-art methods, establishing a new benchmark in multi-modal 3D object detection. The source code and network weighting for reproducibility are available at https://github.com/icdm-adteam/SVEFusion .&lt;/p&gt;</content:encoded></item><item><title>Rethinking Decoupled Knowledge Distillation: A Predictive Distribution Perspective</title><link>https://doi.org/10.1109/tnnls.2025.3639562</link><guid>10.1109/tnnls.2025.3639562</guid><pubDate>Tue, 09 Dec 2025 18:33:49 +0000</pubDate><dc:creator>Bowen Zheng</dc:creator><dc:creator>Ran Cheng</dc:creator><prism:publicationName>IEEE Transactions on Neural Networks and Learning Systems</prism:publicationName><prism:doi>10.1109/tnnls.2025.3639562</prism:doi><description>In the history of knowledge distillation (KD), the focus has once shifted over time from logit-based to feature-based approaches. However, this transition has been revisited with the advent of decoupled KD (DKD), which reemphasizes the importance of logit knowledge through advanced decoupling and weighting strategies. While DKD marks a significant advancement, its underlying mechanisms merit deeper exploration. As a response, we rethink DKD from a predictive distribution perspective. First, we introduce an enhanced version, the generalized DKD (GDKD) loss, which offers a more versatile method for decoupling logits. Then, we pay particular attention to the teacher model’s predictive distribution and its impact on the gradients of GDKD loss, uncovering two critical insights often overlooked: 1) the partitioning by the top logit considerably improves the interrelationship of nontop logits and 2) amplifying the focus on the distillation loss of nontop logits enhances the knowledge extraction among them. Utilizing these insights, we further propose a streamlined GDKD algorithm with an efficient partition strategy to handle the multimodality of teacher models’ predictive distribution. Our comprehensive experiments conducted on a variety of benchmarks, including CIFAR-100, ImageNet, Tiny-ImageNet, CUB-200-2011, and Cityscapes, demonstrate GDKD’s superior performance over both the original DKD and other leading KD methods. The code is available at https://github.com/ZaberKo/GDKD
Published: 2025-12-09T18:33:49+00:00
Venue: IEEE Transactions on Neural Networks and Learning Systems
Score: 0.793 (must_read)</description><content:encoded>&lt;p&gt;&lt;strong&gt;Authors:&lt;/strong&gt; Bowen Zheng; Ran Cheng&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Journal:&lt;/strong&gt; IEEE Transactions on Neural Networks and Learning Systems&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;a href="https://doi.org/10.1109/tnnls.2025.3639562"&gt;10.1109/tnnls.2025.3639562&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Relevance:&lt;/strong&gt; 0.793 (must_read)&lt;/p&gt;
&lt;p&gt;In the history of knowledge distillation (KD), the focus has once shifted over time from logit-based to feature-based approaches. However, this transition has been revisited with the advent of decoupled KD (DKD), which reemphasizes the importance of logit knowledge through advanced decoupling and weighting strategies. While DKD marks a significant advancement, its underlying mechanisms merit deeper exploration. As a response, we rethink DKD from a predictive distribution perspective. First, we introduce an enhanced version, the generalized DKD (GDKD) loss, which offers a more versatile method for decoupling logits. Then, we pay particular attention to the teacher model’s predictive distribution and its impact on the gradients of GDKD loss, uncovering two critical insights often overlooked: 1) the partitioning by the top logit considerably improves the interrelationship of nontop logits and 2) amplifying the focus on the distillation loss of nontop logits enhances the knowledge extraction among them. Utilizing these insights, we further propose a streamlined GDKD algorithm with an efficient partition strategy to handle the multimodality of teacher models’ predictive distribution. Our comprehensive experiments conducted on a variety of benchmarks, including CIFAR-100, ImageNet, Tiny-ImageNet, CUB-200-2011, and Cityscapes, demonstrate GDKD’s superior performance over both the original DKD and other leading KD methods. The code is available at https://github.com/ZaberKo/GDKD&lt;/p&gt;</content:encoded></item><item><title>Enhancing Descriptive Image Quality Assessment with A Large-scale Multi-modal Dataset</title><link>https://doi.org/10.1109/tip.2025.3639998</link><guid>10.1109/tip.2025.3639998</guid><pubDate>Tue, 09 Dec 2025 18:35:41 +0000</pubDate><dc:creator>Zhiyuan You</dc:creator><dc:creator>Jinjin Gu</dc:creator><dc:creator>Xin Cai</dc:creator><dc:creator>Zheyuan Li</dc:creator><dc:creator>Kaiwen Zhu</dc:creator><dc:creator>Chao Dong</dc:creator><dc:creator>Tianfan Xue</dc:creator><prism:publicationName>IEEE Transactions on Image Processing</prism:publicationName><prism:doi>10.1109/tip.2025.3639998</prism:doi><description>With the rapid advancement of Vision Language Models (VLMs), VLM-based Image Quality Assessment (IQA) seeks to describe image quality linguistically to align with human expression and capture the multifaceted nature of IQA tasks. However, current methods are still far from practical usage. First, prior works focus narrowly on specific sub-tasks or settings, which do not align with diverse real-world applications. Second, their performance is sub-optimal due to limitations in dataset coverage, scale, and quality. To overcome these challenges, we introduce Enhanced Descriptive image Quality Assessment (EDQA). Our method includes a multi-functional IQA task paradigm that encompasses both assessment and comparison tasks, brief and detailed responses, full-reference and non-reference scenarios. We introduce a ground-truth-informed dataset construction approach to enhance data quality, and scale up the dataset to 495K under the brief-detail joint framework. Consequently, we construct a comprehensive, large-scale, and high-quality dataset, named EDQA-495K. We also retain image resolution during training to better handle resolution-related quality issues, and estimate a confidence score that is helpful to filter out low-quality responses. Experimental results demonstrate that EDQA significantly outperforms traditional score-based methods, prior VLM-based IQA models, and proprietary GPT-4V in distortion identification, instant rating, and reasoning tasks. Our advantages are further confirmed by real-world applications including assessing the web-downloaded images and ranking model-processed images. Datasets and codes will be released.
Published: 2025-12-09T18:35:41+00:00
Venue: IEEE Transactions on Image Processing
Score: 0.792 (must_read)</description><content:encoded>&lt;p&gt;&lt;strong&gt;Authors:&lt;/strong&gt; Zhiyuan You; Jinjin Gu; Xin Cai; Zheyuan Li; Kaiwen Zhu; Chao Dong; Tianfan Xue&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Journal:&lt;/strong&gt; IEEE Transactions on Image Processing&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;a href="https://doi.org/10.1109/tip.2025.3639998"&gt;10.1109/tip.2025.3639998&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Relevance:&lt;/strong&gt; 0.792 (must_read)&lt;/p&gt;
&lt;p&gt;With the rapid advancement of Vision Language Models (VLMs), VLM-based Image Quality Assessment (IQA) seeks to describe image quality linguistically to align with human expression and capture the multifaceted nature of IQA tasks. However, current methods are still far from practical usage. First, prior works focus narrowly on specific sub-tasks or settings, which do not align with diverse real-world applications. Second, their performance is sub-optimal due to limitations in dataset coverage, scale, and quality. To overcome these challenges, we introduce Enhanced Descriptive image Quality Assessment (EDQA). Our method includes a multi-functional IQA task paradigm that encompasses both assessment and comparison tasks, brief and detailed responses, full-reference and non-reference scenarios. We introduce a ground-truth-informed dataset construction approach to enhance data quality, and scale up the dataset to 495K under the brief-detail joint framework. Consequently, we construct a comprehensive, large-scale, and high-quality dataset, named EDQA-495K. We also retain image resolution during training to better handle resolution-related quality issues, and estimate a confidence score that is helpful to filter out low-quality responses. Experimental results demonstrate that EDQA significantly outperforms traditional score-based methods, prior VLM-based IQA models, and proprietary GPT-4V in distortion identification, instant rating, and reasoning tasks. Our advantages are further confirmed by real-world applications including assessing the web-downloaded images and ranking model-processed images. Datasets and codes will be released.&lt;/p&gt;</content:encoded></item><item><title>Multi-level Alignment Network for Unsupervised Domain Adaptive Multi-modality Object Re-identification</title><link>https://doi.org/10.1016/j.knosys.2025.115015</link><guid>10.1016/j.knosys.2025.115015</guid><pubDate>Tue, 09 Dec 2025 07:42:51 +0000</pubDate><dc:creator>Yusong Sheng</dc:creator><dc:creator>Yuhe Ding</dc:creator><dc:creator>Aihua Zheng</dc:creator><dc:creator>Ziqi Liu</dc:creator><dc:creator>Zi Wang</dc:creator><dc:creator>Jin Tang</dc:creator><prism:publicationName>Knowledge-Based Systems</prism:publicationName><prism:doi>10.1016/j.knosys.2025.115015</prism:doi><description>Existing multi-modality object re-identification methods demonstrate robust performance in complex environments, but this is predominantly contingent upon the test and training data sharing an identical distribution. However, performance degrades significantly when applied to the real world (target domains) with distribution differences from the training data (source domain). Single-modality domain adaptation methods that do not account for multi-modality domain gaps and complementary modality information do not achieve satisfactory performance. To alleviate this, we first introduce the task of unsupervised domain adaptation multi-modality object ReID, aiming to address the challenge of distribution shift in multi-modality scenarios and its impact on model performance. We further propose a novel Multi-level Alignment Network (MAN), which performs alignment strategies at the pseudo-label level, domain level, and modality level by leveraging multi-modality information consistency, multi-modality distribution discrepancy, and multi-modality information diversity. Specifically, Consistency-driven Pseudo-label Alignment (CPA) aims to mitigate the effects of pseudo-label noise from clustering by aligning pseudo-labels and filtering reliable samples based on consistency scores. Prototype-guided Domain Distribution Alignment (PDA) narrows the domain gap between the source and target domains by minimizing the distribution distance between the prototype of one domain and the instances of another domain. Margin-preserved modality distribution alignment (MMA) aligns modality distributions within the same domain by keeping the distribution of instances to a marginal distance from the prototype distribution and preserves modality diversity and complementary information. Experiments conducted on vehicle and person datasets WMVeID863, RGBNT100, RGBNT201, and Market1501-MM validate the effectiveness of the proposed method.
Published: 2025-12-09T07:42:51+00:00
Venue: Knowledge-Based Systems
Score: 0.792 (must_read)</description><content:encoded>&lt;p&gt;&lt;strong&gt;Authors:&lt;/strong&gt; Yusong Sheng; Yuhe Ding; Aihua Zheng; Ziqi Liu; Zi Wang; Jin Tang&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Journal:&lt;/strong&gt; Knowledge-Based Systems&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;a href="https://doi.org/10.1016/j.knosys.2025.115015"&gt;10.1016/j.knosys.2025.115015&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Relevance:&lt;/strong&gt; 0.792 (must_read)&lt;/p&gt;
&lt;p&gt;Existing multi-modality object re-identification methods demonstrate robust performance in complex environments, but this is predominantly contingent upon the test and training data sharing an identical distribution. However, performance degrades significantly when applied to the real world (target domains) with distribution differences from the training data (source domain). Single-modality domain adaptation methods that do not account for multi-modality domain gaps and complementary modality information do not achieve satisfactory performance. To alleviate this, we first introduce the task of unsupervised domain adaptation multi-modality object ReID, aiming to address the challenge of distribution shift in multi-modality scenarios and its impact on model performance. We further propose a novel Multi-level Alignment Network (MAN), which performs alignment strategies at the pseudo-label level, domain level, and modality level by leveraging multi-modality information consistency, multi-modality distribution discrepancy, and multi-modality information diversity. Specifically, Consistency-driven Pseudo-label Alignment (CPA) aims to mitigate the effects of pseudo-label noise from clustering by aligning pseudo-labels and filtering reliable samples based on consistency scores. Prototype-guided Domain Distribution Alignment (PDA) narrows the domain gap between the source and target domains by minimizing the distribution distance between the prototype of one domain and the instances of another domain. Margin-preserved modality distribution alignment (MMA) aligns modality distributions within the same domain by keeping the distribution of instances to a marginal distance from the prototype distribution and preserves modality diversity and complementary information. Experiments conducted on vehicle and person datasets WMVeID863, RGBNT100, RGBNT201, and Market1501-MM validate the effectiveness of the proposed method.&lt;/p&gt;</content:encoded></item><item><title>Towards Accurate Procedure Planning in Instructional Videos: Visual State Generation Helps Task-Selective Diffusion</title><link>https://doi.org/10.1109/tpami.2025.3641798</link><guid>10.1109/tpami.2025.3641798</guid><pubDate>Tue, 09 Dec 2025 18:32:53 +0000</pubDate><dc:creator>Fen Fang</dc:creator><dc:creator>Muli Yang</dc:creator><dc:creator>Min Wu</dc:creator><dc:creator>Yanhua Yang</dc:creator><dc:creator>Qianli Xu</dc:creator><dc:creator>Joo-Hwee Lim</dc:creator><dc:creator>Xulei Yang</dc:creator><dc:creator>Hongyuan Zhu</dc:creator><prism:publicationName>IEEE Transactions on Pattern Analysis and Machine Intelligence</prism:publicationName><prism:doi>10.1109/tpami.2025.3641798</prism:doi><description>Procedure planning in instructional videos entails predicting an action sequence that transitions a given start state to a desired goal state. This task is particularly challenging due to two key sources of uncertainty: limited visual observations and an enormous decision space. The former results in multiple plausible plan variations due to missing intermediate visual states, while the latter complicates prediction by requiring selection from a large set of potential actions. Unlike prior work that addresses these issues implicitly, we propose an explicit solution. To mitigate the first challenge, we employ image generation models to synthesize diverse intermediate visual states using various text prompts, followed by a prompt selection module integrated within a diffusion model. To tackle the second challenge, we introduce a task-selective diffusion model that applies a task-specific mask to constrain the action space. As the effectiveness of this mask depends on accurate task classification, we further enhance visual representation by leveraging pre-trained vision-language models to generate action-aware, text-enriched multimodal embeddings. Extensive experiments on benchmark datasets validate the superior performance of our approach. The code will be available at github.com/ffzzy840304/Masked-PDPP.
Published: 2025-12-09T18:32:53+00:00
Venue: IEEE Transactions on Pattern Analysis and Machine Intelligence
Score: 0.791 (must_read)</description><content:encoded>&lt;p&gt;&lt;strong&gt;Authors:&lt;/strong&gt; Fen Fang; Muli Yang; Min Wu; Yanhua Yang; Qianli Xu; Joo-Hwee Lim; Xulei Yang; Hongyuan Zhu&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Journal:&lt;/strong&gt; IEEE Transactions on Pattern Analysis and Machine Intelligence&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;a href="https://doi.org/10.1109/tpami.2025.3641798"&gt;10.1109/tpami.2025.3641798&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Relevance:&lt;/strong&gt; 0.791 (must_read)&lt;/p&gt;
&lt;p&gt;Procedure planning in instructional videos entails predicting an action sequence that transitions a given start state to a desired goal state. This task is particularly challenging due to two key sources of uncertainty: limited visual observations and an enormous decision space. The former results in multiple plausible plan variations due to missing intermediate visual states, while the latter complicates prediction by requiring selection from a large set of potential actions. Unlike prior work that addresses these issues implicitly, we propose an explicit solution. To mitigate the first challenge, we employ image generation models to synthesize diverse intermediate visual states using various text prompts, followed by a prompt selection module integrated within a diffusion model. To tackle the second challenge, we introduce a task-selective diffusion model that applies a task-specific mask to constrain the action space. As the effectiveness of this mask depends on accurate task classification, we further enhance visual representation by leveraging pre-trained vision-language models to generate action-aware, text-enriched multimodal embeddings. Extensive experiments on benchmark datasets validate the superior performance of our approach. The code will be available at github.com/ffzzy840304/Masked-PDPP.&lt;/p&gt;</content:encoded></item><item><title>DA-Net: A Double Alignment Multimodal Learning Network for Point Cloud Quality Assessment</title><link>https://doi.org/10.1109/tip.2025.3640023</link><guid>10.1109/tip.2025.3640023</guid><pubDate>Tue, 09 Dec 2025 18:35:41 +0000</pubDate><dc:creator>Xinqiang Wu</dc:creator><dc:creator>Zhouyan He</dc:creator><dc:creator>Ting Luo</dc:creator><dc:creator>Gangyi Jiang</dc:creator><dc:creator>Wujie Zhou</dc:creator><dc:creator>Linwei Zhu</dc:creator><dc:creator>Weisi Lin</dc:creator><prism:publicationName>IEEE Transactions on Image Processing</prism:publicationName><prism:doi>10.1109/tip.2025.3640023</prism:doi><description>Existing multimodal point cloud quality assessment (PCQA) methods usually integrate 3D and 2D information to simulate human visual perception of distortions. However, due to the lack of consideration of spatial correspondence, they have difficulty to learn consistent distortion representations from different modalities in the same region of the PC. In addition, they also ignore the heterogeneity of modalities and rely on complex fusion mechanisms (e.g., attention) to integrate multimodal features. Both lead to limited performance and increased computational complexity. To address these limitations, we propose a novel double alignment multimodal learning network (DA-Net), which introduces two key alignment strategies. Specifically, the first is spatial pre-alignment strategy, which generates informative 2D patch for each 3D patch via an adaptive patch projection module (APPM), ensuring accurate spatial correspondence of different modalities prior to feature extraction. The second is a uniform feature alignment strategy, which includes feature disentanglement module (FDM) and feature mapping module (FMM) to relieve heterogeneity of modalities and guide the optimization of 2D and 3D encoder. Finally, multimodal features are simply integrated and regressed to obtain the quality score. Experimental results demonstrate that the DA-Net exhibits outstanding performance and generalization ability. It also achieves lower computational complexity compared with other multimodal PCQA methods. The source codes of DA-Net will be available at https://github.com/Rphone/DA-Net.
Published: 2025-12-09T18:35:41+00:00
Venue: IEEE Transactions on Image Processing
Score: 0.790 (must_read)</description><content:encoded>&lt;p&gt;&lt;strong&gt;Authors:&lt;/strong&gt; Xinqiang Wu; Zhouyan He; Ting Luo; Gangyi Jiang; Wujie Zhou; Linwei Zhu; Weisi Lin&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Journal:&lt;/strong&gt; IEEE Transactions on Image Processing&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;a href="https://doi.org/10.1109/tip.2025.3640023"&gt;10.1109/tip.2025.3640023&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Relevance:&lt;/strong&gt; 0.790 (must_read)&lt;/p&gt;
&lt;p&gt;Existing multimodal point cloud quality assessment (PCQA) methods usually integrate 3D and 2D information to simulate human visual perception of distortions. However, due to the lack of consideration of spatial correspondence, they have difficulty to learn consistent distortion representations from different modalities in the same region of the PC. In addition, they also ignore the heterogeneity of modalities and rely on complex fusion mechanisms (e.g., attention) to integrate multimodal features. Both lead to limited performance and increased computational complexity. To address these limitations, we propose a novel double alignment multimodal learning network (DA-Net), which introduces two key alignment strategies. Specifically, the first is spatial pre-alignment strategy, which generates informative 2D patch for each 3D patch via an adaptive patch projection module (APPM), ensuring accurate spatial correspondence of different modalities prior to feature extraction. The second is a uniform feature alignment strategy, which includes feature disentanglement module (FDM) and feature mapping module (FMM) to relieve heterogeneity of modalities and guide the optimization of 2D and 3D encoder. Finally, multimodal features are simply integrated and regressed to obtain the quality score. Experimental results demonstrate that the DA-Net exhibits outstanding performance and generalization ability. It also achieves lower computational complexity compared with other multimodal PCQA methods. The source codes of DA-Net will be available at https://github.com/Rphone/DA-Net.&lt;/p&gt;</content:encoded></item><item><title>PMFM-kdTransformer: An enhanced multi-modal fusion architecture leveraging knowledge distillation for intra-hour solar irradiance prediction</title><link>https://doi.org/10.1016/j.inffus.2025.104043</link><guid>10.1016/j.inffus.2025.104043</guid><pubDate>Mon, 08 Dec 2025 23:58:22 +0000</pubDate><dc:creator>Menggang Kou</dc:creator><dc:creator>Runze Li</dc:creator><dc:creator>Tong Niu</dc:creator><dc:creator>Quansheng Qian</dc:creator><dc:creator>Zhiwu Li</dc:creator><dc:creator>Jianzhou Wang</dc:creator><prism:publicationName>Information Fusion</prism:publicationName><prism:doi>10.1016/j.inffus.2025.104043</prism:doi><description>With the increasing proportion of photovoltaic power in the energy structure, the volatility of solar irradiance poses a significant challenge to grid security, making solar irradiance forecasting a critical approach for enhancing renewable energy integration. Existing methods face several limitations: purely temporal modeling struggles to capture dynamic correlations under abrupt weather conditions, while sky-image-based approaches, though promising, suffer from insufficient inter-modal interaction exploration in current multi-modal fusion models (MFMs). Additionally, the MFMs are hindered by the high cost of all-sky imagers, limiting their widespread adoption. To address these issues, this study proposes a parallel multimodal fusion model (PMFM) and a knowledge-distilled Transformer (kdTransformer). The PMFM adopts a parallel processing architecture with modality-specific feature extraction strategies for three data: cloud images processed through deformable convolution and pyramid structure, meteorological variables modeled via convolutional operations, and irradiation sequences processed through multilayer perceptrons. The extracted features are then fused through enhanced dynamic gating mechanisms with cross-modal attention. For regions lacking cloud image, the kdTransformer transfers the multimodal knowledge learned by the PMFM to a temporal model via knowledge distillation, where the distillation loss extends the traditional distillation framework by introducing a feature alignment loss and a relational distillation loss, combined dynamically with weights. Experiments demonstrate that the PMFM achieves a 7.92% average improvement across seven metrics and two year’s datasets over the renowned Sunset baseline, while the kdTransformer achieves a 24.58% average enhancement compared to the non-distilled Transformer. Furthermore, the PMFM-kdTransformer achieves a trade-off between computational efficiency (65.1 FLOPs/G) and prediction accuracy.
Published: 2025-12-08T23:58:22+00:00
Venue: Information Fusion
Score: 0.789 (must_read)</description><content:encoded>&lt;p&gt;&lt;strong&gt;Authors:&lt;/strong&gt; Menggang Kou; Runze Li; Tong Niu; Quansheng Qian; Zhiwu Li; Jianzhou Wang&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Journal:&lt;/strong&gt; Information Fusion&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;a href="https://doi.org/10.1016/j.inffus.2025.104043"&gt;10.1016/j.inffus.2025.104043&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Relevance:&lt;/strong&gt; 0.789 (must_read)&lt;/p&gt;
&lt;p&gt;With the increasing proportion of photovoltaic power in the energy structure, the volatility of solar irradiance poses a significant challenge to grid security, making solar irradiance forecasting a critical approach for enhancing renewable energy integration. Existing methods face several limitations: purely temporal modeling struggles to capture dynamic correlations under abrupt weather conditions, while sky-image-based approaches, though promising, suffer from insufficient inter-modal interaction exploration in current multi-modal fusion models (MFMs). Additionally, the MFMs are hindered by the high cost of all-sky imagers, limiting their widespread adoption. To address these issues, this study proposes a parallel multimodal fusion model (PMFM) and a knowledge-distilled Transformer (kdTransformer). The PMFM adopts a parallel processing architecture with modality-specific feature extraction strategies for three data: cloud images processed through deformable convolution and pyramid structure, meteorological variables modeled via convolutional operations, and irradiation sequences processed through multilayer perceptrons. The extracted features are then fused through enhanced dynamic gating mechanisms with cross-modal attention. For regions lacking cloud image, the kdTransformer transfers the multimodal knowledge learned by the PMFM to a temporal model via knowledge distillation, where the distillation loss extends the traditional distillation framework by introducing a feature alignment loss and a relational distillation loss, combined dynamically with weights. Experiments demonstrate that the PMFM achieves a 7.92% average improvement across seven metrics and two year’s datasets over the renowned Sunset baseline, while the kdTransformer achieves a 24.58% average enhancement compared to the non-distilled Transformer. Furthermore, the PMFM-kdTransformer achieves a trade-off between computational efficiency (65.1 FLOPs/G) and prediction accuracy.&lt;/p&gt;</content:encoded></item><item><title>Unleashing the Intrinsic Visual Representation Capability of Multimodal Large Language Models</title><link>https://arxiv.org/abs/2512.06281v1</link><guid>http://arxiv.org/abs/2512.06281v1</guid><pubDate>Sat, 06 Dec 2025 04:20:13 +0000</pubDate><dc:creator>Hengzhuang Li</dc:creator><dc:creator>Xinsong Zhang</dc:creator><dc:creator>Qiming Peng</dc:creator><dc:creator>Bin Luo</dc:creator><dc:creator>Han Hu</dc:creator><dc:creator>Dengyang Jiang</dc:creator><dc:creator>Han-Jia Ye</dc:creator><dc:creator>Teng Zhang</dc:creator><dc:creator>Hai Jin</dc:creator><dc:type>preprint</dc:type><prism:publicationName>arXiv</prism:publicationName><description>Multimodal Large Language Models (MLLMs) have demonstrated remarkable proficiency in multimodal tasks. Despite their impressive performance, MLLMs suffer from the modality imbalance issue, where visual information is often underutilized compared to textual representations in deeper layers, leading to degraded visual performance or hallucinations. This issue stems from the predominant reliance on next-text-token-prediction during training, which fails to provide direct visual supervisory signals, resulting in progressive homogenization of visual representations throughout the layers. To this end, we propose Latent Visual Reconstruction (LaVer), a novel training framework that facilitates MLLMs in learning more discriminative visual representations via masked image modeling in the joint latent semantic space of LLM. Our method offers direct visual activation to MLLMs, which exhibit increased visual attention allocation, indicating enhanced utilization of visual information. Extensive experiments across diverse benchmarks prove the superiority of our approach in various scenarios, especially those requiring dense visual capabilities. Code of LaVer is available at https://github.com/Fir-lat/LaVer.
Published: 2025-12-06T04:20:13+00:00
Venue: arXiv
Score: 0.788 (must_read)</description><content:encoded>&lt;p&gt;&lt;strong&gt;Authors:&lt;/strong&gt; Hengzhuang Li; Xinsong Zhang; Qiming Peng; Bin Luo; Han Hu; Dengyang Jiang; Han-Jia Ye; Teng Zhang; Hai Jin&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Journal:&lt;/strong&gt; arXiv&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Relevance:&lt;/strong&gt; 0.788 (must_read)&lt;/p&gt;
&lt;p&gt;Multimodal Large Language Models (MLLMs) have demonstrated remarkable proficiency in multimodal tasks. Despite their impressive performance, MLLMs suffer from the modality imbalance issue, where visual information is often underutilized compared to textual representations in deeper layers, leading to degraded visual performance or hallucinations. This issue stems from the predominant reliance on next-text-token-prediction during training, which fails to provide direct visual supervisory signals, resulting in progressive homogenization of visual representations throughout the layers. To this end, we propose Latent Visual Reconstruction (LaVer), a novel training framework that facilitates MLLMs in learning more discriminative visual representations via masked image modeling in the joint latent semantic space of LLM. Our method offers direct visual activation to MLLMs, which exhibit increased visual attention allocation, indicating enhanced utilization of visual information. Extensive experiments across diverse benchmarks prove the superiority of our approach in various scenarios, especially those requiring dense visual capabilities. Code of LaVer is available at https://github.com/Fir-lat/LaVer.&lt;/p&gt;</content:encoded></item><item><title>Multi-Scale Samples Transformer for Hyperspectral Image Classification</title><link>https://doi.org/10.1109/tgrs.2025.3642126</link><guid>10.1109/tgrs.2025.3642126</guid><pubDate>Tue, 09 Dec 2025 18:32:55 +0000</pubDate><dc:creator>Wei-Tao Zhang</dc:creator><dc:creator>Nuo Xu</dc:creator><dc:creator>Yv Bai</dc:creator><dc:creator>Ya-Ru Zhang</dc:creator><prism:publicationName>IEEE Transactions on Geoscience and Remote Sensing</prism:publicationName><prism:doi>10.1109/tgrs.2025.3642126</prism:doi><description>Hyperspectral image (HSI) captures hundreds of narrow bands, offering abundant spatial and spectral information across various application domains. In recent years, deep learning techniques have shown strong potential in HSI processing, with a range of models being explored. However, the fixed geometric structure of convolutional kernels in CNN model impedes interactions between long-term dependencies, and existing transformer methods have not fully harnessed the spectral structural characteristics of HSI data. To address these issues, we proposed a multiscale samples transformer (MSST) that effectively extracts spatial and spectral features via two separate branches respectively. In spectral branch, the spectral sequence of the individual target pixel is used as input sample, a spectral transformer (SpecFormer) is proposed to comprehensively extract the spectral structural features. In spatial branch, three-dimensional tensor consisting of target pixel and their neighboring pixels is used as input sample, dimension reduction is performed, and a spatial transformer (SpatFormer) is designed to efficiently extract the deep spatial features of HSI data. Consequently, MSST enlarges the receptive field and captures the comprehensive joint spatial-spectral features. Extensive experiments conducted on five widely used HSI datasets demonstrate the superior classification performance of the proposed framework. Our source codes will be available at https://github. com/zhwt-xidian/Multi-Scale-Samples-Transformer_MSST.
Published: 2025-12-09T18:32:55+00:00
Venue: IEEE Transactions on Geoscience and Remote Sensing
Score: 0.788 (must_read)</description><content:encoded>&lt;p&gt;&lt;strong&gt;Authors:&lt;/strong&gt; Wei-Tao Zhang; Nuo Xu; Yv Bai; Ya-Ru Zhang&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Journal:&lt;/strong&gt; IEEE Transactions on Geoscience and Remote Sensing&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;a href="https://doi.org/10.1109/tgrs.2025.3642126"&gt;10.1109/tgrs.2025.3642126&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Relevance:&lt;/strong&gt; 0.788 (must_read)&lt;/p&gt;
&lt;p&gt;Hyperspectral image (HSI) captures hundreds of narrow bands, offering abundant spatial and spectral information across various application domains. In recent years, deep learning techniques have shown strong potential in HSI processing, with a range of models being explored. However, the fixed geometric structure of convolutional kernels in CNN model impedes interactions between long-term dependencies, and existing transformer methods have not fully harnessed the spectral structural characteristics of HSI data. To address these issues, we proposed a multiscale samples transformer (MSST) that effectively extracts spatial and spectral features via two separate branches respectively. In spectral branch, the spectral sequence of the individual target pixel is used as input sample, a spectral transformer (SpecFormer) is proposed to comprehensively extract the spectral structural features. In spatial branch, three-dimensional tensor consisting of target pixel and their neighboring pixels is used as input sample, dimension reduction is performed, and a spatial transformer (SpatFormer) is designed to efficiently extract the deep spatial features of HSI data. Consequently, MSST enlarges the receptive field and captures the comprehensive joint spatial-spectral features. Extensive experiments conducted on five widely used HSI datasets demonstrate the superior classification performance of the proposed framework. Our source codes will be available at https://github. com/zhwt-xidian/Multi-Scale-Samples-Transformer_MSST.&lt;/p&gt;</content:encoded></item><item><title>M2Restore: Mixture-of-Experts-based Mamba-CNN Fusion Framework for All-in-One Image Restoration</title><link>https://doi.org/10.1109/tip.2025.3638662</link><guid>10.1109/tip.2025.3638662</guid><pubDate>Mon, 08 Dec 2025 18:42:57 +0000</pubDate><dc:creator>Yongzhen Wang</dc:creator><dc:creator>Yongjun Li</dc:creator><dc:creator>Zhuoran Zheng</dc:creator><dc:creator>Xiao-Ping Zhang</dc:creator><dc:creator>Mingqiang Wei</dc:creator><prism:publicationName>IEEE Transactions on Image Processing</prism:publicationName><prism:doi>10.1109/tip.2025.3638662</prism:doi><description>Natural images are often degraded by complex, composite degradations such as rain, snow, and haze, which adversely impact downstream vision applications. While existing image restoration efforts have achieved notable success, they are still hindered by two critical challenges: limited generalization across dynamically varying degradation scenarios and a suboptimal balance between preserving local details and modeling global dependencies. To overcome these challenges, we propose M2Restore, a novel Mixture-of-Experts (MoE)-based Mamba-CNN fusion framework for efficient and robust all-in-one image restoration. M2Restore introduces three key contributions: First, to boost the model’s generalization across diverse degradation conditions, we exploit a CLIP-guided MoE gating mechanism that fuses task-conditioned prompts with CLIP-derived semantic priors. This mechanism is further refined via cross-modal feature calibration, which enables precise expert selection for various degradation types. Second, to jointly capture global contextual dependencies and fine-grained local details, we design a dual-stream architecture that integrates the localized representational strength of CNNs with the long-range modeling efficiency of Mamba. This integration enables collaborative optimization of global semantic relationships and local structural fidelity, preserving global coherence while enhancing detail restoration. Third, we introduce an edge-aware dynamic gating mechanism that adaptively balances global modeling and local enhancement by reallocating computational attention to degradation-sensitive regions. This targeted focus leads to more efficient and precise restoration. Extensive experiments across multiple image restoration benchmarks validate the superiority of M2Restore in both visual quality and quantitative performance.
Published: 2025-12-08T18:42:57+00:00
Venue: IEEE Transactions on Image Processing
Score: 0.788 (must_read)</description><content:encoded>&lt;p&gt;&lt;strong&gt;Authors:&lt;/strong&gt; Yongzhen Wang; Yongjun Li; Zhuoran Zheng; Xiao-Ping Zhang; Mingqiang Wei&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Journal:&lt;/strong&gt; IEEE Transactions on Image Processing&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;a href="https://doi.org/10.1109/tip.2025.3638662"&gt;10.1109/tip.2025.3638662&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Relevance:&lt;/strong&gt; 0.788 (must_read)&lt;/p&gt;
&lt;p&gt;Natural images are often degraded by complex, composite degradations such as rain, snow, and haze, which adversely impact downstream vision applications. While existing image restoration efforts have achieved notable success, they are still hindered by two critical challenges: limited generalization across dynamically varying degradation scenarios and a suboptimal balance between preserving local details and modeling global dependencies. To overcome these challenges, we propose M2Restore, a novel Mixture-of-Experts (MoE)-based Mamba-CNN fusion framework for efficient and robust all-in-one image restoration. M2Restore introduces three key contributions: First, to boost the model’s generalization across diverse degradation conditions, we exploit a CLIP-guided MoE gating mechanism that fuses task-conditioned prompts with CLIP-derived semantic priors. This mechanism is further refined via cross-modal feature calibration, which enables precise expert selection for various degradation types. Second, to jointly capture global contextual dependencies and fine-grained local details, we design a dual-stream architecture that integrates the localized representational strength of CNNs with the long-range modeling efficiency of Mamba. This integration enables collaborative optimization of global semantic relationships and local structural fidelity, preserving global coherence while enhancing detail restoration. Third, we introduce an edge-aware dynamic gating mechanism that adaptively balances global modeling and local enhancement by reallocating computational attention to degradation-sensitive regions. This targeted focus leads to more efficient and precise restoration. Extensive experiments across multiple image restoration benchmarks validate the superiority of M2Restore in both visual quality and quantitative performance.&lt;/p&gt;</content:encoded></item><item><title>Bridging the Gap Between Computer Vision and Bioelectrical Signal Analysis</title><link>https://doi.org/10.1016/j.inffus.2025.104047</link><guid>10.1016/j.inffus.2025.104047</guid><pubDate>Mon, 08 Dec 2025 16:44:34 +0000</pubDate><dc:creator>Yanan Wang</dc:creator><dc:creator>Shuaicong Hu</dc:creator><dc:creator>Jian Liu</dc:creator><dc:creator>Aiguo Wang</dc:creator><dc:creator>Guohui Zhou</dc:creator><dc:creator>Cuiwei Yang</dc:creator><prism:publicationName>Information Fusion</prism:publicationName><prism:doi>10.1016/j.inffus.2025.104047</prism:doi><description>Bioelectrical signals are vital for non-invasive physiological monitoring and disease diagnosis. The application of artificial intelligence (AI) in automatic bioelectrical signal analysis has garnered attention. However, training supervised AI models requires abundant labeled data, which is particularly challenging in medical scenarios. Existing transfer learning (TL) approaches attempt to transfer knowledge from images in the computer vision (CV) domain, but face significant challenges due to dimensional difference and distribution gaps between CV and bioelectrical signal domains. Traditional domain adaptation (DA) assumes domain task invariance and focuses mainly on feature distribution difference, without considering label inconsistency and the complex cross-dimensional scenarios. Our motivation is to address the fundamental challenges in cross-domain knowledge transfer by jointly solving the dimensional gap and task discrepancy between CV and bioelectrical signal domains, thereby facilitating robust learning with limited labeled medical data. In this paper, we propose a cross-dimensional information transfer (CDIT) framework that enables effective information fusion through parallel encoding-decoding modules, which preserve the discriminative characteristics of both two-dimensional (2D) CV images and one-dimensional (1D) bioelectrical signals while mapping them into a shared feature space. Furthermore, we develop a cross-task DA method that synergistically integrates feature and label information to address the label inconsistency challenge in the DA phase of CDITF. We conduct extensive experiments on two representative scenarios using six databases. The experimental results demonstrate that CDITF with cross-task DA (CDITF-CTDA) achieves successful knowledge transfer from CV into bioelectrical signal domains despite their inherent difference, consistently outperforming baseline methods with improvements of 0.02–0.07 in AUC for bioelectrical signal analysis. These results demonstrate the effectiveness of CDITF-CTDA in leveraging CV knowledge for bioelectrical signal analysis under limited-labeled data medical scenarios.
Published: 2025-12-08T16:44:34+00:00
Venue: Information Fusion
Score: 0.786 (must_read)</description><content:encoded>&lt;p&gt;&lt;strong&gt;Authors:&lt;/strong&gt; Yanan Wang; Shuaicong Hu; Jian Liu; Aiguo Wang; Guohui Zhou; Cuiwei Yang&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Journal:&lt;/strong&gt; Information Fusion&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;a href="https://doi.org/10.1016/j.inffus.2025.104047"&gt;10.1016/j.inffus.2025.104047&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Relevance:&lt;/strong&gt; 0.786 (must_read)&lt;/p&gt;
&lt;p&gt;Bioelectrical signals are vital for non-invasive physiological monitoring and disease diagnosis. The application of artificial intelligence (AI) in automatic bioelectrical signal analysis has garnered attention. However, training supervised AI models requires abundant labeled data, which is particularly challenging in medical scenarios. Existing transfer learning (TL) approaches attempt to transfer knowledge from images in the computer vision (CV) domain, but face significant challenges due to dimensional difference and distribution gaps between CV and bioelectrical signal domains. Traditional domain adaptation (DA) assumes domain task invariance and focuses mainly on feature distribution difference, without considering label inconsistency and the complex cross-dimensional scenarios. Our motivation is to address the fundamental challenges in cross-domain knowledge transfer by jointly solving the dimensional gap and task discrepancy between CV and bioelectrical signal domains, thereby facilitating robust learning with limited labeled medical data. In this paper, we propose a cross-dimensional information transfer (CDIT) framework that enables effective information fusion through parallel encoding-decoding modules, which preserve the discriminative characteristics of both two-dimensional (2D) CV images and one-dimensional (1D) bioelectrical signals while mapping them into a shared feature space. Furthermore, we develop a cross-task DA method that synergistically integrates feature and label information to address the label inconsistency challenge in the DA phase of CDITF. We conduct extensive experiments on two representative scenarios using six databases. The experimental results demonstrate that CDITF with cross-task DA (CDITF-CTDA) achieves successful knowledge transfer from CV into bioelectrical signal domains despite their inherent difference, consistently outperforming baseline methods with improvements of 0.02–0.07 in AUC for bioelectrical signal analysis. These results demonstrate the effectiveness of CDITF-CTDA in leveraging CV knowledge for bioelectrical signal analysis under limited-labeled data medical scenarios.&lt;/p&gt;</content:encoded></item><item><title>Each Prompt Matters: Scaling Reinforcement Learning Without Wasting Rollouts on Hundred-Billion-Scale MoE</title><link>https://arxiv.org/abs/2512.07710v1</link><guid>http://arxiv.org/abs/2512.07710v1</guid><pubDate>Mon, 08 Dec 2025 16:57:43 +0000</pubDate><dc:creator>Anxiang Zeng</dc:creator><dc:creator>Haibo Zhang</dc:creator><dc:creator>Hailing Zhang</dc:creator><dc:creator>Kaixiang Mo</dc:creator><dc:creator>Liang Yao</dc:creator><dc:creator>Ling Hu</dc:creator><dc:creator>Long Zhang</dc:creator><dc:creator>Shuman Liu</dc:creator><dc:creator>Shuyi Xie</dc:creator><dc:creator>Yanshi Li</dc:creator><dc:creator>Yizhang Chen</dc:creator><dc:creator>Yuepeng Sheng</dc:creator><dc:creator>Yuwei Huang</dc:creator><dc:creator>Zhaochen Xu</dc:creator><dc:creator>Zhiqiang Zhou</dc:creator><dc:creator>Ziqin Liew</dc:creator><dc:type>preprint</dc:type><prism:publicationName>arXiv</prism:publicationName><description>We present CompassMax-V3-Thinking, a hundred-billion-scale MoE reasoning model trained with a new RL framework built on one principle: each prompt must matter. Scaling RL to this size exposes critical inefficiencies-zero-variance prompts that waste rollouts, unstable importance sampling over long horizons, advantage inversion from standard reward models, and systemic bottlenecks in rollout processing. To overcome these challenges, we introduce several unified innovations: (1) Multi-Stage Zero-Variance Elimination, which filters out non-informative prompts and stabilizes group-based policy optimization (e.g. GRPO) by removing wasted rollouts; (2) ESPO, an entropy-adaptive optimization method that balances token-level and sequence-level importance sampling to maintain stable learning dynamics; (3) a Router Replay strategy that aligns training-time MoE router decisions with inference-time behavior to mitigate train-infer discrepancies, coupled with a reward model adjustment to prevent advantage inversion; (4) a high-throughput RL system with FP8-precision rollouts, overlapped reward computation, and length-aware scheduling to eliminate performance bottlenecks. Together, these contributions form a cohesive pipeline that makes RL on hundred-billion-scale MoE models stable and efficient. The resulting model delivers strong performance across both internal and public evaluations.
Published: 2025-12-08T16:57:43+00:00
Venue: arXiv
Score: 0.786 (must_read)</description><content:encoded>&lt;p&gt;&lt;strong&gt;Authors:&lt;/strong&gt; Anxiang Zeng; Haibo Zhang; Hailing Zhang; Kaixiang Mo; Liang Yao; Ling Hu; Long Zhang; Shuman Liu; Shuyi Xie; Yanshi Li; Yizhang Chen; Yuepeng Sheng; Yuwei Huang; Zhaochen Xu; Zhiqiang Zhou; Ziqin Liew&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Journal:&lt;/strong&gt; arXiv&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Relevance:&lt;/strong&gt; 0.786 (must_read)&lt;/p&gt;
&lt;p&gt;We present CompassMax-V3-Thinking, a hundred-billion-scale MoE reasoning model trained with a new RL framework built on one principle: each prompt must matter. Scaling RL to this size exposes critical inefficiencies-zero-variance prompts that waste rollouts, unstable importance sampling over long horizons, advantage inversion from standard reward models, and systemic bottlenecks in rollout processing. To overcome these challenges, we introduce several unified innovations: (1) Multi-Stage Zero-Variance Elimination, which filters out non-informative prompts and stabilizes group-based policy optimization (e.g. GRPO) by removing wasted rollouts; (2) ESPO, an entropy-adaptive optimization method that balances token-level and sequence-level importance sampling to maintain stable learning dynamics; (3) a Router Replay strategy that aligns training-time MoE router decisions with inference-time behavior to mitigate train-infer discrepancies, coupled with a reward model adjustment to prevent advantage inversion; (4) a high-throughput RL system with FP8-precision rollouts, overlapped reward computation, and length-aware scheduling to eliminate performance bottlenecks. Together, these contributions form a cohesive pipeline that makes RL on hundred-billion-scale MoE models stable and efficient. The resulting model delivers strong performance across both internal and public evaluations.&lt;/p&gt;</content:encoded></item><item><title>DFIR-DETR: Frequency Domain Enhancement and Dynamic Feature Aggregation for Cross-Scene Small Object Detection</title><link>https://arxiv.org/abs/2512.07078v1</link><guid>http://arxiv.org/abs/2512.07078v1</guid><pubDate>Mon, 08 Dec 2025 01:25:10 +0000</pubDate><dc:creator>Bo Gao</dc:creator><dc:creator>Jingcheng Tong</dc:creator><dc:creator>Xingsheng Chen</dc:creator><dc:creator>Han Yu</dc:creator><dc:creator>Zichen Li</dc:creator><dc:type>preprint</dc:type><prism:publicationName>arXiv</prism:publicationName><description>Detecting small objects in UAV remote sensing images and identifying surface defects in industrial inspection remain difficult tasks. These applications face common obstacles: features are sparse and weak, backgrounds are cluttered, and object scales vary dramatically. Current transformer-based detectors, while powerful, struggle with three critical issues. First, features degrade severely as networks downsample progressively. Second, spatial convolutions cannot capture long-range dependencies effectively. Third, standard upsampling methods inflate feature maps unnecessarily.
  We introduce DFIR-DETR to tackle these problems through dynamic feature aggregation combined with frequency-domain processing. Our architecture builds on three novel components. The DCFA module uses dynamic K-sparse attention, cutting complexity from O(N2) down to O(NK), and employs spatial gated linear units for better nonlinear modeling. The DFPN module applies amplitude-normalized upsampling to prevent feature inflation and uses dual-path shuffle convolution to retain spatial details across scales. The FIRC3 module operates in the frequency domain, achieving global receptive fields without sacrificing efficiency.
  We tested our method extensively on NEU-DET and VisDrone datasets. Results show mAP50 scores of 92.9% and 51.6% respectively-both state-of-the-art. The model stays lightweight with just 11.7M parameters and 41.2 GFLOPs. Strong performance across two very different domains confirms that DFIR-DETR generalizes well and works effectively in resource-limited settings for cross-scene small object detection.
Published: 2025-12-08T01:25:10+00:00
Venue: arXiv
Score: 0.785 (must_read)</description><content:encoded>&lt;p&gt;&lt;strong&gt;Authors:&lt;/strong&gt; Bo Gao; Jingcheng Tong; Xingsheng Chen; Han Yu; Zichen Li&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Journal:&lt;/strong&gt; arXiv&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Relevance:&lt;/strong&gt; 0.785 (must_read)&lt;/p&gt;
&lt;p&gt;Detecting small objects in UAV remote sensing images and identifying surface defects in industrial inspection remain difficult tasks. These applications face common obstacles: features are sparse and weak, backgrounds are cluttered, and object scales vary dramatically. Current transformer-based detectors, while powerful, struggle with three critical issues. First, features degrade severely as networks downsample progressively. Second, spatial convolutions cannot capture long-range dependencies effectively. Third, standard upsampling methods inflate feature maps unnecessarily.
  We introduce DFIR-DETR to tackle these problems through dynamic feature aggregation combined with frequency-domain processing. Our architecture builds on three novel components. The DCFA module uses dynamic K-sparse attention, cutting complexity from O(N2) down to O(NK), and employs spatial gated linear units for better nonlinear modeling. The DFPN module applies amplitude-normalized upsampling to prevent feature inflation and uses dual-path shuffle convolution to retain spatial details across scales. The FIRC3 module operates in the frequency domain, achieving global receptive fields without sacrificing efficiency.
  We tested our method extensively on NEU-DET and VisDrone datasets. Results show mAP50 scores of 92.9% and 51.6% respectively-both state-of-the-art. The model stays lightweight with just 11.7M parameters and 41.2 GFLOPs. Strong performance across two very different domains confirms that DFIR-DETR generalizes well and works effectively in resource-limited settings for cross-scene small object detection.&lt;/p&gt;</content:encoded></item><item><title>RLAX: Large-Scale, Distributed Reinforcement Learning for Large Language Models on TPUs</title><link>https://arxiv.org/abs/2512.06392v1</link><guid>http://arxiv.org/abs/2512.06392v1</guid><pubDate>Sat, 06 Dec 2025 10:48:51 +0000</pubDate><dc:creator>Runlong Zhou</dc:creator><dc:creator>Lefan Zhang</dc:creator><dc:creator>Shang-Chen Wu</dc:creator><dc:creator>Kelvin Zou</dc:creator><dc:creator>Hanzhi Zhou</dc:creator><dc:creator>Ke Ye</dc:creator><dc:creator>Yihao Feng</dc:creator><dc:creator>Dong Yin</dc:creator><dc:creator>Alex Guillen Garcia</dc:creator><dc:creator>Dmytro Babych</dc:creator><dc:creator>Rohit Chatterjee</dc:creator><dc:creator>Matthew Hopkins</dc:creator><dc:creator>Xiang Kong</dc:creator><dc:creator>Chang Lan</dc:creator><dc:creator>Lezhi Li</dc:creator><dc:creator>Yiping Ma</dc:creator><dc:creator>Daniele Molinari</dc:creator><dc:creator>Senyu Tong</dc:creator><dc:creator>Yanchao Sun</dc:creator><dc:creator>Thomas Voice</dc:creator><dc:creator>Jianyu Wang</dc:creator><dc:creator>Chong Wang</dc:creator><dc:creator>Simon Wang</dc:creator><dc:creator>Floris Weers</dc:creator><dc:creator>Yechen Xu</dc:creator><dc:creator>Guolin Yin</dc:creator><dc:creator>Muyang Yu</dc:creator><dc:creator>Yi Zhang</dc:creator><dc:creator>Zheng Zhou</dc:creator><dc:creator>Danyang Zhuo</dc:creator><dc:creator>Ruoming Pang</dc:creator><dc:creator>Cheng Leong</dc:creator><dc:type>preprint</dc:type><prism:publicationName>arXiv</prism:publicationName><description>Reinforcement learning (RL) has emerged as the de-facto paradigm for improving the reasoning capabilities of large language models (LLMs). We have developed RLAX, a scalable RL framework on TPUs. RLAX employs a parameter-server architecture. A master trainer periodically pushes updated model weights to the parameter server while a fleet of inference workers pull the latest weights and generates new rollouts. We introduce a suite of system techniques to enable scalable and preemptible RL for a diverse set of state-of-art RL algorithms. To accelerate convergence and improve model quality, we have devised new dataset curation and alignment techniques. Large-scale evaluations show that RLAX improves QwQ-32B's pass@8 accuracy by 12.8% in just 12 hours 48 minutes on 1024 v5p TPUs, while remaining robust to preemptions during training.
Published: 2025-12-06T10:48:51+00:00
Venue: arXiv
Score: 0.784 (must_read)</description><content:encoded>&lt;p&gt;&lt;strong&gt;Authors:&lt;/strong&gt; Runlong Zhou; Lefan Zhang; Shang-Chen Wu; Kelvin Zou; Hanzhi Zhou; Ke Ye; Yihao Feng; Dong Yin; Alex Guillen Garcia; Dmytro Babych; Rohit Chatterjee; Matthew Hopkins; Xiang Kong; Chang Lan; Lezhi Li; Yiping Ma; Daniele Molinari; Senyu Tong; Yanchao Sun; Thomas Voice; Jianyu Wang; Chong Wang; Simon Wang; Floris Weers; Yechen Xu; Guolin Yin; Muyang Yu; Yi Zhang; Zheng Zhou; Danyang Zhuo; Ruoming Pang; Cheng Leong&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Journal:&lt;/strong&gt; arXiv&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Relevance:&lt;/strong&gt; 0.784 (must_read)&lt;/p&gt;
&lt;p&gt;Reinforcement learning (RL) has emerged as the de-facto paradigm for improving the reasoning capabilities of large language models (LLMs). We have developed RLAX, a scalable RL framework on TPUs. RLAX employs a parameter-server architecture. A master trainer periodically pushes updated model weights to the parameter server while a fleet of inference workers pull the latest weights and generates new rollouts. We introduce a suite of system techniques to enable scalable and preemptible RL for a diverse set of state-of-art RL algorithms. To accelerate convergence and improve model quality, we have devised new dataset curation and alignment techniques. Large-scale evaluations show that RLAX improves QwQ-32B&amp;#x27;s pass@8 accuracy by 12.8% in just 12 hours 48 minutes on 1024 v5p TPUs, while remaining robust to preemptions during training.&lt;/p&gt;</content:encoded></item><item><title>MulCLIP: A Multi-level Alignment Framework for Enhancing Fine-grained Long-context CLIP</title><link>https://arxiv.org/abs/2512.07128v1</link><guid>http://arxiv.org/abs/2512.07128v1</guid><pubDate>Mon, 08 Dec 2025 03:23:41 +0000</pubDate><dc:creator>Chau Truong</dc:creator><dc:creator>Hieu Ta Quang</dc:creator><dc:creator>Dung D. Le</dc:creator><dc:type>preprint</dc:type><prism:publicationName>arXiv</prism:publicationName><description>Vision-language models like CLIP show impressive ability to align images and text, but their training on short, concise captions makes them struggle with lengthy, detailed descriptions. Recent advances mitigate this challenge by leveraging region-proposal information to map visual regions with corresponding sentences from lengthy captions, yet incurring notable deployment costs. We introduce MulCLIP, a novel end-to-end multi-level alignment framework that bridges natural long-text structures with image components. MulCLIP first preserves global contrastive alignment between images and both summary and long captions, while extending positional embeddings for longer text sequences. To further enhance fine-grained understanding, we propose two novel strategies: (1) a token reconstruction alignment over locally calibrated features to strengthen semantic connections between words and image patches, and (2) a subcaption-aggregated patch alignment that automatically extracts and aggregates context-rich patches for each subcaption. Experimental results across diverse benchmarks demonstrate our method consistently improves downstream performance, while ablation studies confirm its multi-scale alignment is the key factor driving better fine-grained capability than region-proposal-assisted approaches, making it particularly suitable for diverse real-world applications.
Published: 2025-12-08T03:23:41+00:00
Venue: arXiv
Score: 0.784 (must_read)</description><content:encoded>&lt;p&gt;&lt;strong&gt;Authors:&lt;/strong&gt; Chau Truong; Hieu Ta Quang; Dung D. Le&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Journal:&lt;/strong&gt; arXiv&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Relevance:&lt;/strong&gt; 0.784 (must_read)&lt;/p&gt;
&lt;p&gt;Vision-language models like CLIP show impressive ability to align images and text, but their training on short, concise captions makes them struggle with lengthy, detailed descriptions. Recent advances mitigate this challenge by leveraging region-proposal information to map visual regions with corresponding sentences from lengthy captions, yet incurring notable deployment costs. We introduce MulCLIP, a novel end-to-end multi-level alignment framework that bridges natural long-text structures with image components. MulCLIP first preserves global contrastive alignment between images and both summary and long captions, while extending positional embeddings for longer text sequences. To further enhance fine-grained understanding, we propose two novel strategies: (1) a token reconstruction alignment over locally calibrated features to strengthen semantic connections between words and image patches, and (2) a subcaption-aggregated patch alignment that automatically extracts and aggregates context-rich patches for each subcaption. Experimental results across diverse benchmarks demonstrate our method consistently improves downstream performance, while ablation studies confirm its multi-scale alignment is the key factor driving better fine-grained capability than region-proposal-assisted approaches, making it particularly suitable for diverse real-world applications.&lt;/p&gt;</content:encoded></item><item><title>LightSearcher: Efficient DeepSearch via Experiential Memory</title><link>https://arxiv.org/abs/2512.06653v2</link><guid>http://arxiv.org/abs/2512.06653v2</guid><pubDate>Sun, 07 Dec 2025 04:29:52 +0000</pubDate><dc:creator>Hengzhi Lan</dc:creator><dc:creator>Yue Yu</dc:creator><dc:creator>Li Qian</dc:creator><dc:creator>Li Peng</dc:creator><dc:creator>Jie Wu</dc:creator><dc:creator>Wei Liu</dc:creator><dc:creator>Jian Luan</dc:creator><dc:creator>Ting Bai</dc:creator><dc:type>preprint</dc:type><prism:publicationName>arXiv</prism:publicationName><description>DeepSearch paradigms have become a core enabler for deep reasoning models, allowing them to invoke external search tools to access up-to-date, domain-specific knowledge beyond parametric boundaries, thereby enhancing the depth and factual reliability of reasoning. Building upon this foundation, recent advances in reinforcement learning (RL) have further empowered models to autonomously and strategically control search tool usage, optimizing when and how to query external knowledge sources. Yet, these RL-driven DeepSearch systems often reveal a see-saw trade-off between accuracy and efficiency-frequent tool invocations can improve factual correctness but lead to unnecessary computational overhead and diminished efficiency. To address this challenge, we propose LightSearcher, an efficient RL framework that incorporates textual experiential memory by learning contrastive reasoning trajectories to generate interpretable summaries of successful reasoning patterns. In addition, it employs an adaptive reward shaping mechanism that penalizes redundant tool calls only in correct-answer scenarios. This design effectively balances the inherent accuracy-efficiency trade-off in DeepSearch paradigms. Experiments on four multi-hop QA benchmarks show that LightSearcher maintains accuracy comparable to SOTA baseline ReSearch, while reducing search tool invocations by 39.6%, inference time by 48.6%, and token consumption by 21.2%, demonstrating its superior efficiency.
Published: 2025-12-07T04:29:52+00:00
Venue: arXiv
Score: 0.784 (must_read)</description><content:encoded>&lt;p&gt;&lt;strong&gt;Authors:&lt;/strong&gt; Hengzhi Lan; Yue Yu; Li Qian; Li Peng; Jie Wu; Wei Liu; Jian Luan; Ting Bai&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Journal:&lt;/strong&gt; arXiv&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Relevance:&lt;/strong&gt; 0.784 (must_read)&lt;/p&gt;
&lt;p&gt;DeepSearch paradigms have become a core enabler for deep reasoning models, allowing them to invoke external search tools to access up-to-date, domain-specific knowledge beyond parametric boundaries, thereby enhancing the depth and factual reliability of reasoning. Building upon this foundation, recent advances in reinforcement learning (RL) have further empowered models to autonomously and strategically control search tool usage, optimizing when and how to query external knowledge sources. Yet, these RL-driven DeepSearch systems often reveal a see-saw trade-off between accuracy and efficiency-frequent tool invocations can improve factual correctness but lead to unnecessary computational overhead and diminished efficiency. To address this challenge, we propose LightSearcher, an efficient RL framework that incorporates textual experiential memory by learning contrastive reasoning trajectories to generate interpretable summaries of successful reasoning patterns. In addition, it employs an adaptive reward shaping mechanism that penalizes redundant tool calls only in correct-answer scenarios. This design effectively balances the inherent accuracy-efficiency trade-off in DeepSearch paradigms. Experiments on four multi-hop QA benchmarks show that LightSearcher maintains accuracy comparable to SOTA baseline ReSearch, while reducing search tool invocations by 39.6%, inference time by 48.6%, and token consumption by 21.2%, demonstrating its superior efficiency.&lt;/p&gt;</content:encoded></item><item><title>Hyperspectral Anomaly Detection via Hybrid Convolutional and Transformer-Based U-Net With Error Attention Mechanism</title><link>https://doi.org/10.1109/tnnls.2025.3634765</link><guid>10.1109/tnnls.2025.3634765</guid><pubDate>Mon, 08 Dec 2025 18:41:27 +0000</pubDate><dc:creator>Xiaoyi Wang</dc:creator><dc:creator>Peng Wang</dc:creator><dc:creator>Juan Cheng</dc:creator><dc:creator>Daiyin Zhu</dc:creator><dc:creator>Henry Leung</dc:creator><dc:creator>Paolo Gamba</dc:creator><prism:publicationName>IEEE Transactions on Neural Networks and Learning Systems</prism:publicationName><prism:doi>10.1109/tnnls.2025.3634765</prism:doi><description>Hyperspectral anomaly detection is a crucial technique for recognizing abnormal pixels in hyperspectral images (HSIs), that is, those with distinct spectral characteristics from those of the surrounding background. Traditional methods always fall short in effectively leveraging the information regarding the spectral and spatial aspects of the dataset simultaneously, limiting their detection performances. This article proposes a novel framework using U-Net, termed hybrid convolution and transformer-based U-Net (HCT-Unet), which integrates convolution with a multihead attention mechanism in Transformer for enhanced hyperspectral anomaly detection. To ensure a more comprehensive understanding of spatial and spectral interactions, the HCT-Unet architecture capitalizes on the strengths of local feature extraction of convolutional layers and the capabilities of the long-range dependency modeling of Transformers. A key innovation of this framework is an error attention mechanism, which facilitates adaptive multiscale feature fusion and enhances the feature representation capacity. Furthermore, a new anomaly score calculation method is proposed, which combines reconstruction error with the pixelwise structural similarity index (SSIM) to determine pixel anomaly from both local structural preservation and global spectral consistency perspectives. Experiments carried out on seven different hyperspectral datasets reveal that the proposed method consistently outperforms the widely accepted state-of-the-art methods in hyperspectral anomaly detection.
Published: 2025-12-08T18:41:27+00:00
Venue: IEEE Transactions on Neural Networks and Learning Systems
Score: 0.783 (must_read)</description><content:encoded>&lt;p&gt;&lt;strong&gt;Authors:&lt;/strong&gt; Xiaoyi Wang; Peng Wang; Juan Cheng; Daiyin Zhu; Henry Leung; Paolo Gamba&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Journal:&lt;/strong&gt; IEEE Transactions on Neural Networks and Learning Systems&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;a href="https://doi.org/10.1109/tnnls.2025.3634765"&gt;10.1109/tnnls.2025.3634765&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Relevance:&lt;/strong&gt; 0.783 (must_read)&lt;/p&gt;
&lt;p&gt;Hyperspectral anomaly detection is a crucial technique for recognizing abnormal pixels in hyperspectral images (HSIs), that is, those with distinct spectral characteristics from those of the surrounding background. Traditional methods always fall short in effectively leveraging the information regarding the spectral and spatial aspects of the dataset simultaneously, limiting their detection performances. This article proposes a novel framework using U-Net, termed hybrid convolution and transformer-based U-Net (HCT-Unet), which integrates convolution with a multihead attention mechanism in Transformer for enhanced hyperspectral anomaly detection. To ensure a more comprehensive understanding of spatial and spectral interactions, the HCT-Unet architecture capitalizes on the strengths of local feature extraction of convolutional layers and the capabilities of the long-range dependency modeling of Transformers. A key innovation of this framework is an error attention mechanism, which facilitates adaptive multiscale feature fusion and enhances the feature representation capacity. Furthermore, a new anomaly score calculation method is proposed, which combines reconstruction error with the pixelwise structural similarity index (SSIM) to determine pixel anomaly from both local structural preservation and global spectral consistency perspectives. Experiments carried out on seven different hyperspectral datasets reveal that the proposed method consistently outperforms the widely accepted state-of-the-art methods in hyperspectral anomaly detection.&lt;/p&gt;</content:encoded></item><item><title>Enhancing monocular height estimation via sparse LiDAR-guided correction</title><link>https://doi.org/10.1016/j.isprsjprs.2025.12.004</link><guid>10.1016/j.isprsjprs.2025.12.004</guid><pubDate>Mon, 08 Dec 2025 15:28:16 +0000</pubDate><dc:creator>Jian Song</dc:creator><dc:creator>Hongruixuan Chen</dc:creator><dc:creator>Naoto Yokoya</dc:creator><prism:publicationName>ISPRS Journal of Photogrammetry and Remote Sensing</prism:publicationName><prism:doi>10.1016/j.isprsjprs.2025.12.004</prism:doi><description>Monocular height estimation (MHE) from very-high-resolution (VHR) remote sensing imagery via deep learning is notoriously challenging due to the lack of sufficient structural information. Conventional digital elevation models (DEMs), typically derived from airborne LiDAR or multi-view stereo, remain costly and geographically limited. While state-of-the-art monocular height estimation (MHE) and depth estimation (MDE) models show great promise, their robustness under varied illumination conditions remains a significant challenge. To address this, we introduce a novel and fully automated correction pipeline that integrates sparse, imperfect global LiDAR measurements (ICESat-2) with deep learning outputs to enhance local accuracy and robustness. Importantly, the entire workflow is fully automated and built solely on publicly available models and datasets, requiring only a single georeferenced optical image to generate corrected height maps, thereby ensuring unprecedented accessibility and global scalability. Furthermore, we establish the first comprehensive benchmark for this task, evaluating a suite of correction methods that includes two random forest-based approaches, four parameter-efficient fine-tuning techniques, and full fine-tuning. We conduct extensive experiments across six large-scale, diverse regions at 0.5 m resolution, totaling approximately 297 km 2 " role="presentation"&gt; 2 2 , encompassing the urban cores of Tokyo, Paris, and São Paulo, as well as mixed suburban and forest landscapes. Experimental results demonstrate that the best-performing correction method reduces the MHE model’s mean absolute error (MAE) by an average of 30.9% and improves its F 1 HE " role="presentation"&gt; F 1 HE F 1 HE score by 44.2%. For the MDE model, the MAE is improved by 24.1% and the F 1 HE " role="presentation"&gt; F 1 HE F 1 HE score by 25.1%. These findings validate the effectiveness of our correction pipeline, demonstrating how sparse real-world LiDAR data can systematically bolster the robustness of both MHE and MDE models and paving the way for scalable, low-cost, and globally applicable 3D mapping solutions.
Published: 2025-12-08T15:28:16+00:00
Venue: ISPRS Journal of Photogrammetry and Remote Sensing
Score: 0.782 (must_read)</description><content:encoded>&lt;p&gt;&lt;strong&gt;Authors:&lt;/strong&gt; Jian Song; Hongruixuan Chen; Naoto Yokoya&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Journal:&lt;/strong&gt; ISPRS Journal of Photogrammetry and Remote Sensing&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;a href="https://doi.org/10.1016/j.isprsjprs.2025.12.004"&gt;10.1016/j.isprsjprs.2025.12.004&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Relevance:&lt;/strong&gt; 0.782 (must_read)&lt;/p&gt;
&lt;p&gt;Monocular height estimation (MHE) from very-high-resolution (VHR) remote sensing imagery via deep learning is notoriously challenging due to the lack of sufficient structural information. Conventional digital elevation models (DEMs), typically derived from airborne LiDAR or multi-view stereo, remain costly and geographically limited. While state-of-the-art monocular height estimation (MHE) and depth estimation (MDE) models show great promise, their robustness under varied illumination conditions remains a significant challenge. To address this, we introduce a novel and fully automated correction pipeline that integrates sparse, imperfect global LiDAR measurements (ICESat-2) with deep learning outputs to enhance local accuracy and robustness. Importantly, the entire workflow is fully automated and built solely on publicly available models and datasets, requiring only a single georeferenced optical image to generate corrected height maps, thereby ensuring unprecedented accessibility and global scalability. Furthermore, we establish the first comprehensive benchmark for this task, evaluating a suite of correction methods that includes two random forest-based approaches, four parameter-efficient fine-tuning techniques, and full fine-tuning. We conduct extensive experiments across six large-scale, diverse regions at 0.5 m resolution, totaling approximately 297 km 2 &amp;quot; role=&amp;quot;presentation&amp;quot;&amp;gt; 2 2 , encompassing the urban cores of Tokyo, Paris, and São Paulo, as well as mixed suburban and forest landscapes. Experimental results demonstrate that the best-performing correction method reduces the MHE model’s mean absolute error (MAE) by an average of 30.9% and improves its F 1 HE &amp;quot; role=&amp;quot;presentation&amp;quot;&amp;gt; F 1 HE F 1 HE score by 44.2%. For the MDE model, the MAE is improved by 24.1% and the F 1 HE &amp;quot; role=&amp;quot;presentation&amp;quot;&amp;gt; F 1 HE F 1 HE score by 25.1%. These findings validate the effectiveness of our correction pipeline, demonstrating how sparse real-world LiDAR data can systematically bolster the robustness of both MHE and MDE models and paving the way for scalable, low-cost, and globally applicable 3D mapping solutions.&lt;/p&gt;</content:encoded></item><item><title>A Unified Decision Rule for Generalized Out-of-Distribution Detection</title><link>https://doi.org/10.1109/tpami.2025.3642151</link><guid>10.1109/tpami.2025.3642151</guid><pubDate>Tue, 09 Dec 2025 18:32:53 +0000</pubDate><dc:creator>Xinsong Ma</dc:creator><dc:creator>Jie Wu</dc:creator><dc:creator>Xin Zou</dc:creator><dc:creator>Weiwei Liu</dc:creator><prism:publicationName>IEEE Transactions on Pattern Analysis and Machine Intelligence</prism:publicationName><prism:doi>10.1109/tpami.2025.3642151</prism:doi><description>Generalized Out-of-distribution (OOD) detection task plays the key role in reliable and safety-critical applications. Existing researches mainly devote to designing or training the powerful score function but overlook investigating the decision rule based on the proposed score function. Different from previous work, this paper aims to design a decision rule with rigorous theoretical guarantee and well empirical performance. Specifically, we provide a new insight for the OOD detection task from a hypothesis testing perspective and propose a novel generalized Benjamini Hochberg (g-BH) procedure to solve the testing problem. Teoretically, the g-BH procedure controls false discovery rate (FDR) under pre-specified level without the consideration of dependence for the p-values. Furthermore, we derive an upper bound and a lower bound of the expectation of false positive rate (FPR) for the g-BH procedure based on the tailed generalized Gaussian distribution family, indicating that the FPR of g-BH procedure converges to zero in probability. Finally, the extensive experimental results verify the superiority of g-BH procedure over the traditional threshold-based decision rule on several generalized OOD detection benchmarks.
Published: 2025-12-09T18:32:53+00:00
Venue: IEEE Transactions on Pattern Analysis and Machine Intelligence
Score: 0.781 (must_read)</description><content:encoded>&lt;p&gt;&lt;strong&gt;Authors:&lt;/strong&gt; Xinsong Ma; Jie Wu; Xin Zou; Weiwei Liu&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Journal:&lt;/strong&gt; IEEE Transactions on Pattern Analysis and Machine Intelligence&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;a href="https://doi.org/10.1109/tpami.2025.3642151"&gt;10.1109/tpami.2025.3642151&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Relevance:&lt;/strong&gt; 0.781 (must_read)&lt;/p&gt;
&lt;p&gt;Generalized Out-of-distribution (OOD) detection task plays the key role in reliable and safety-critical applications. Existing researches mainly devote to designing or training the powerful score function but overlook investigating the decision rule based on the proposed score function. Different from previous work, this paper aims to design a decision rule with rigorous theoretical guarantee and well empirical performance. Specifically, we provide a new insight for the OOD detection task from a hypothesis testing perspective and propose a novel generalized Benjamini Hochberg (g-BH) procedure to solve the testing problem. Teoretically, the g-BH procedure controls false discovery rate (FDR) under pre-specified level without the consideration of dependence for the p-values. Furthermore, we derive an upper bound and a lower bound of the expectation of false positive rate (FPR) for the g-BH procedure based on the tailed generalized Gaussian distribution family, indicating that the FPR of g-BH procedure converges to zero in probability. Finally, the extensive experimental results verify the superiority of g-BH procedure over the traditional threshold-based decision rule on several generalized OOD detection benchmarks.&lt;/p&gt;</content:encoded></item><item><title>AI-Generated Image Quality Assessment Based on Task-Specific Prompt and Multi-Granularity Similarity</title><link>https://doi.org/10.1109/tip.2025.3639984</link><guid>10.1109/tip.2025.3639984</guid><pubDate>Tue, 09 Dec 2025 18:35:41 +0000</pubDate><dc:creator>Jili Xia</dc:creator><dc:creator>Lihuo He</dc:creator><dc:creator>Cheng Deng</dc:creator><dc:creator>Leida Li</dc:creator><dc:creator>Xinbo Gao</dc:creator><prism:publicationName>IEEE Transactions on Image Processing</prism:publicationName><prism:doi>10.1109/tip.2025.3639984</prism:doi><description>Recently, AI-generated images (AIGIs), synthesized based on initial textual prompts, have attracted widespread attention. However, due to limitations in current generation techniques, these images often exhibit degraded perceptual quality and semantic misalignment with the guiding prompts. Therefore, evaluating both perceptual quality and text-to-image alignment is essential for optimizing the performance of generative models. Existing methods design textual prompts solely based on the initial prompt for both perceptual and alignment quality tasks, and compute only coarse-grained similarity between the designed prompt and the generated image. However, such task-agnostic prompts overlook the distinctions between the perceptual and alignment quality tasks, and coarse-level similarity fails to capture semantic details, leading to suboptimal evaluation performance. To address these challenges, we propose a novel AIGI quality assessment framework, termed TPMS, which incorporates task-specific prompt and multi-granularity similarity computation. The task-specific prompt constructs dedicated prompts for perceptual and alignment quality respectively, allowing the model to capture distinct quality cues tailored to each evaluation task. Multi-granularity similarity measures the coarse-level similarity between the generated image and task-specific prompts to capture global quality characteristics, and the fine-level similarity between the generated image and the initial prompt to enhance semantic detail awareness. By integrating these two complementary similarities, TPMS enables precise and robust quality prediction. Extensive experiments on four widely-used AIGI quality benchmarks validate the effectiveness and superiority of the proposed framework.
Published: 2025-12-09T18:35:41+00:00
Venue: IEEE Transactions on Image Processing
Score: 0.781 (must_read)</description><content:encoded>&lt;p&gt;&lt;strong&gt;Authors:&lt;/strong&gt; Jili Xia; Lihuo He; Cheng Deng; Leida Li; Xinbo Gao&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Journal:&lt;/strong&gt; IEEE Transactions on Image Processing&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;a href="https://doi.org/10.1109/tip.2025.3639984"&gt;10.1109/tip.2025.3639984&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Relevance:&lt;/strong&gt; 0.781 (must_read)&lt;/p&gt;
&lt;p&gt;Recently, AI-generated images (AIGIs), synthesized based on initial textual prompts, have attracted widespread attention. However, due to limitations in current generation techniques, these images often exhibit degraded perceptual quality and semantic misalignment with the guiding prompts. Therefore, evaluating both perceptual quality and text-to-image alignment is essential for optimizing the performance of generative models. Existing methods design textual prompts solely based on the initial prompt for both perceptual and alignment quality tasks, and compute only coarse-grained similarity between the designed prompt and the generated image. However, such task-agnostic prompts overlook the distinctions between the perceptual and alignment quality tasks, and coarse-level similarity fails to capture semantic details, leading to suboptimal evaluation performance. To address these challenges, we propose a novel AIGI quality assessment framework, termed TPMS, which incorporates task-specific prompt and multi-granularity similarity computation. The task-specific prompt constructs dedicated prompts for perceptual and alignment quality respectively, allowing the model to capture distinct quality cues tailored to each evaluation task. Multi-granularity similarity measures the coarse-level similarity between the generated image and task-specific prompts to capture global quality characteristics, and the fine-level similarity between the generated image and the initial prompt to enhance semantic detail awareness. By integrating these two complementary similarities, TPMS enables precise and robust quality prediction. Extensive experiments on four widely-used AIGI quality benchmarks validate the effectiveness and superiority of the proposed framework.&lt;/p&gt;</content:encoded></item><item><title>SegAssess: Panoramic quality mapping for robust and transferable unsupervised segmentation assessment</title><link>https://doi.org/10.1109/tgrs.2025.3641753</link><guid>10.1109/tgrs.2025.3641753</guid><pubDate>Mon, 08 Dec 2025 18:40:17 +0000</pubDate><dc:creator>Bingnan Yang</dc:creator><dc:creator>Mi Zhang</dc:creator><dc:creator>Zhili Zhang</dc:creator><dc:creator>Zhan Zhang</dc:creator><dc:creator>Yuanxin Zhao</dc:creator><dc:creator>Xiangyun Hu</dc:creator><dc:creator>Jianya Gong</dc:creator><prism:publicationName>IEEE Transactions on Geoscience and Remote Sensing</prism:publicationName><prism:doi>10.1109/tgrs.2025.3641753</prism:doi><description>High-quality image segmentation is fundamental to pixel-level geospatial analysis in remote sensing, necessitating robust segmentation quality assessment (SQA), particularly in unsupervised settings lacking ground truth. Although recent deep learning (DL) based unsupervised SQA methods show potential, they often suffer from coarse evaluation granularity, incomplete assessments, and poor transferability. To overcome these limitations, this paper introduces Panoramic Quality Mapping (PQM) as a new paradigm for comprehensive, pixel-wise SQA, and presents SegAssess, a novel deep learning framework realizing this approach. SegAssess distinctively formulates SQA as a fine-grained, four-class panoramic segmentation task, classifying pixels within a segmentation mask under evaluation into true positive (TP), false positive (FP), true negative (TN), and false negative (FN) categories, thereby generating a complete quality map. Leveraging an enhanced Segment Anything Model (SAM) architecture, SegAssess uniquely employs the input mask as a prompt for effective feature integration via cross-attention. Key innovations include an Edge Guided Compaction (EGC) branch with an Aggregated Semantic Filter (ASF) module to refine predictions near challenging object edges, and an Augmented Mixup Sampling (AMS) training strategy integrating multi-source masks to significantly boost cross-domain robustness and zero-shot transferability. Comprehensive experiments demonstrate that SegAssess achieves state-of-the-art (SOTA) performance and exhibits remarkable zero-shot transferability to unseen masks. The code is available at https://github.com/Yangbn97/SegAssess.
Published: 2025-12-08T18:40:17+00:00
Venue: IEEE Transactions on Geoscience and Remote Sensing
Score: 0.781 (must_read)</description><content:encoded>&lt;p&gt;&lt;strong&gt;Authors:&lt;/strong&gt; Bingnan Yang; Mi Zhang; Zhili Zhang; Zhan Zhang; Yuanxin Zhao; Xiangyun Hu; Jianya Gong&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Journal:&lt;/strong&gt; IEEE Transactions on Geoscience and Remote Sensing&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;a href="https://doi.org/10.1109/tgrs.2025.3641753"&gt;10.1109/tgrs.2025.3641753&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Relevance:&lt;/strong&gt; 0.781 (must_read)&lt;/p&gt;
&lt;p&gt;High-quality image segmentation is fundamental to pixel-level geospatial analysis in remote sensing, necessitating robust segmentation quality assessment (SQA), particularly in unsupervised settings lacking ground truth. Although recent deep learning (DL) based unsupervised SQA methods show potential, they often suffer from coarse evaluation granularity, incomplete assessments, and poor transferability. To overcome these limitations, this paper introduces Panoramic Quality Mapping (PQM) as a new paradigm for comprehensive, pixel-wise SQA, and presents SegAssess, a novel deep learning framework realizing this approach. SegAssess distinctively formulates SQA as a fine-grained, four-class panoramic segmentation task, classifying pixels within a segmentation mask under evaluation into true positive (TP), false positive (FP), true negative (TN), and false negative (FN) categories, thereby generating a complete quality map. Leveraging an enhanced Segment Anything Model (SAM) architecture, SegAssess uniquely employs the input mask as a prompt for effective feature integration via cross-attention. Key innovations include an Edge Guided Compaction (EGC) branch with an Aggregated Semantic Filter (ASF) module to refine predictions near challenging object edges, and an Augmented Mixup Sampling (AMS) training strategy integrating multi-source masks to significantly boost cross-domain robustness and zero-shot transferability. Comprehensive experiments demonstrate that SegAssess achieves state-of-the-art (SOTA) performance and exhibits remarkable zero-shot transferability to unseen masks. The code is available at https://github.com/Yangbn97/SegAssess.&lt;/p&gt;</content:encoded></item></channel></rss>