<?xml version='1.0' encoding='utf-8'?>
<rss xmlns:content="http://purl.org/rss/1.0/modules/content/" xmlns:dc="http://purl.org/dc/elements/1.1/" xmlns:prism="http://prismstandard.org/namespaces/basic/3.0/" version="2.0"><channel><title>ZotWatch Feed</title><link>https://ehehe.cn/zotwatch/</link><description>AI-assisted literature watch</description><lastBuildDate>Tue, 20 Jan 2026 02:47:58 +0000</lastBuildDate><item><title>Dual-Layer Prompt Ensembles: Leveraging System- and User-Level Instructions for Robust LLM-Based Query Expansion and Rank Fusion</title><link>https://doi.org/10.1016/j.inffus.2026.104160</link><guid>10.1016/j.inffus.2026.104160</guid><pubDate>Sun, 18 Jan 2026 04:47:19 +0000</pubDate><dc:creator>Minghan Li</dc:creator><dc:creator>Ercong Nie</dc:creator><dc:creator>Huiping Huang</dc:creator><dc:creator>Xinxuan Lv</dc:creator><dc:creator>Guodong Zhou</dc:creator><prism:publicationName>Information Fusion</prism:publicationName><prism:doi>10.1016/j.inffus.2026.104160</prism:doi><description>Large Language Models (LLMs) show strong potential for query expansion (QE), but their effectiveness is highly sensitive to prompt design. This paper investigates whether exploiting the system–user prompt distinction in chat-based LLMs improves QE, and how multiple expansions should be combined. We propose Dual-Layer Prompt Ensembles, which pair a behavioural system prompt with varied user prompts to generate diverse expansions, and aggregate their BM25-ranked lists using lightweight SU-RankFusion schemes. Experiments on six heterogeneous datasets show that dual-layer prompting consistently outperforms strong single-prompt baselines. For example, on Touche-2020 a dual-layer configuration improves nDCG@10 from 0.4177 (QE-CoT) to 0.4696, and SU-RankFusion further raises it to 0.4797. On Robust04 and DBPedia, SU-RankFusion improves nDCG@10 over BM25 by 24.7% and 25.5%, respectively, with similar gains on NFCorpus, FiQA, and TREC-COVID. These results demonstrate that system–user prompt ensembles are effective for QE, and that simple fusion transforms prompt-level diversity into stable retrieval improvements.
Published: 2026-01-18T04:47:19+00:00
Venue: Information Fusion
Score: 0.817 (must_read)</description><content:encoded>&lt;p&gt;&lt;strong&gt;Authors:&lt;/strong&gt; Minghan Li; Ercong Nie; Huiping Huang; Xinxuan Lv; Guodong Zhou&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Journal:&lt;/strong&gt; Information Fusion&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;a href="https://doi.org/10.1016/j.inffus.2026.104160"&gt;10.1016/j.inffus.2026.104160&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Relevance:&lt;/strong&gt; 0.817 (must_read)&lt;/p&gt;
&lt;p&gt;Large Language Models (LLMs) show strong potential for query expansion (QE), but their effectiveness is highly sensitive to prompt design. This paper investigates whether exploiting the system–user prompt distinction in chat-based LLMs improves QE, and how multiple expansions should be combined. We propose Dual-Layer Prompt Ensembles, which pair a behavioural system prompt with varied user prompts to generate diverse expansions, and aggregate their BM25-ranked lists using lightweight SU-RankFusion schemes. Experiments on six heterogeneous datasets show that dual-layer prompting consistently outperforms strong single-prompt baselines. For example, on Touche-2020 a dual-layer configuration improves nDCG@10 from 0.4177 (QE-CoT) to 0.4696, and SU-RankFusion further raises it to 0.4797. On Robust04 and DBPedia, SU-RankFusion improves nDCG@10 over BM25 by 24.7% and 25.5%, respectively, with similar gains on NFCorpus, FiQA, and TREC-COVID. These results demonstrate that system–user prompt ensembles are effective for QE, and that simple fusion transforms prompt-level diversity into stable retrieval improvements.&lt;/p&gt;</content:encoded></item><item><title>Spatial-X fusion for multi-source satellite imageries</title><link>https://doi.org/10.1016/j.rse.2025.115214</link><guid>10.1016/j.rse.2025.115214</guid><pubDate>Mon, 19 Jan 2026 11:34:50 +0000</pubDate><dc:creator>Jiang He</dc:creator><dc:creator>Liupeng Lin</dc:creator><dc:creator>Zhuo Zheng</dc:creator><dc:creator>Qiangqiang Yuan</dc:creator><dc:creator>Jie Li</dc:creator><dc:creator>Liangpei Zhang</dc:creator><dc:creator>Xiao xiang Zhu</dc:creator><prism:publicationName>Remote Sensing of Environment</prism:publicationName><prism:doi>10.1016/j.rse.2025.115214</prism:doi><description>Multi-source remote sensing data can highlight different types of information based on user needs, resulting in large volumes of data and significant challenges. Hardware and environmental constraints create mutual dependencies between information types, particularly between spatial data and other types, limiting the development of high-precision applications. Traditional methods are task-specific, leading to many algorithms without a unified solution, which greatly increases the computational and deployment costs of image fusion. In this paper, we summarize four remote sensing fusion tasks, including pan-sharpening, hyperspectral-multispectral fusion, spatio-temporal fusion, and polarimetric SAR fusion. By defining the spectral, temporal, and polarimetric information, as X, we propose the concept of generalized spatial-channel fusion, referred to as Spatial-X fusion. Then, we design an end-to-end network SpaXFus, a generalized spatial-channel fusion framework through a model-driven unfolding approach that exploits spatial-X intrinsic interactions to capture internal dependencies and self-interactions. Comprehensive experimental results demonstrate the superiority of SpaXFus, e.g., SpaXFus can achieve four remote sensing image fusion tasks with superior performance (across all fusion tasks, spectral distortion decreases by 25.48 %, while spatial details improve by 7.5 %) and shows huge improvements across multiple types of downstream applications, including vegetation index generation, fine-grained image classification, change detection, and SAR vegetation extraction.
Published: 2026-01-19T11:34:50+00:00
Venue: Remote Sensing of Environment
Score: 0.802 (must_read)</description><content:encoded>&lt;p&gt;&lt;strong&gt;Authors:&lt;/strong&gt; Jiang He; Liupeng Lin; Zhuo Zheng; Qiangqiang Yuan; Jie Li; Liangpei Zhang; Xiao xiang Zhu&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Journal:&lt;/strong&gt; Remote Sensing of Environment&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;a href="https://doi.org/10.1016/j.rse.2025.115214"&gt;10.1016/j.rse.2025.115214&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Relevance:&lt;/strong&gt; 0.802 (must_read)&lt;/p&gt;
&lt;p&gt;Multi-source remote sensing data can highlight different types of information based on user needs, resulting in large volumes of data and significant challenges. Hardware and environmental constraints create mutual dependencies between information types, particularly between spatial data and other types, limiting the development of high-precision applications. Traditional methods are task-specific, leading to many algorithms without a unified solution, which greatly increases the computational and deployment costs of image fusion. In this paper, we summarize four remote sensing fusion tasks, including pan-sharpening, hyperspectral-multispectral fusion, spatio-temporal fusion, and polarimetric SAR fusion. By defining the spectral, temporal, and polarimetric information, as X, we propose the concept of generalized spatial-channel fusion, referred to as Spatial-X fusion. Then, we design an end-to-end network SpaXFus, a generalized spatial-channel fusion framework through a model-driven unfolding approach that exploits spatial-X intrinsic interactions to capture internal dependencies and self-interactions. Comprehensive experimental results demonstrate the superiority of SpaXFus, e.g., SpaXFus can achieve four remote sensing image fusion tasks with superior performance (across all fusion tasks, spectral distortion decreases by 25.48 %, while spatial details improve by 7.5 %) and shows huge improvements across multiple types of downstream applications, including vegetation index generation, fine-grained image classification, change detection, and SAR vegetation extraction.&lt;/p&gt;</content:encoded></item><item><title>ImCapDA: Fine-tuning CLIP via Image Captions for Unsupervised Domain Adaptation</title><link>https://doi.org/10.1016/j.eswa.2026.131248</link><guid>10.1016/j.eswa.2026.131248</guid><pubDate>Sun, 18 Jan 2026 03:11:38 +0000</pubDate><dc:creator>Weiwei Xiang</dc:creator><dc:creator>Guangyi Xiao</dc:creator><dc:creator>Shun Peng</dc:creator><dc:creator>Hao Chen</dc:creator><dc:creator>Liming Ding</dc:creator><dc:creator>Lei Yang</dc:creator><prism:publicationName>Expert Systems with Applications</prism:publicationName><prism:doi>10.1016/j.eswa.2026.131248</prism:doi><description>Large vision–language models (VLMs) such as CLIP have demonstrated remarkable zero-shot generalization, yet their potential for unsupervised domain adaptation (UDA) remains underexplored. Existing approaches typically enhance transfer by optimizing visual representations via encoder fine-tuning or improving text prompts, but they either overlook fine-tuning of the text encoder or fail to fully exploit multimodal alignment, often suffering from catastrophic forgetting or limited domain generalization.
Published: 2026-01-18T03:11:38+00:00
Venue: Expert Systems with Applications
Score: 0.802 (must_read)</description><content:encoded>&lt;p&gt;&lt;strong&gt;Authors:&lt;/strong&gt; Weiwei Xiang; Guangyi Xiao; Shun Peng; Hao Chen; Liming Ding; Lei Yang&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Journal:&lt;/strong&gt; Expert Systems with Applications&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;a href="https://doi.org/10.1016/j.eswa.2026.131248"&gt;10.1016/j.eswa.2026.131248&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Relevance:&lt;/strong&gt; 0.802 (must_read)&lt;/p&gt;
&lt;p&gt;Large vision–language models (VLMs) such as CLIP have demonstrated remarkable zero-shot generalization, yet their potential for unsupervised domain adaptation (UDA) remains underexplored. Existing approaches typically enhance transfer by optimizing visual representations via encoder fine-tuning or improving text prompts, but they either overlook fine-tuning of the text encoder or fail to fully exploit multimodal alignment, often suffering from catastrophic forgetting or limited domain generalization.&lt;/p&gt;</content:encoded></item><item><title>AMS-Former: Adaptive multi-scale transformer for multi-modal image matching</title><link>https://doi.org/10.1016/j.isprsjprs.2026.01.021</link><guid>10.1016/j.isprsjprs.2026.01.021</guid><pubDate>Mon, 19 Jan 2026 08:34:48 +0000</pubDate><dc:creator>Jiahao Rao</dc:creator><dc:creator>Rui Liu</dc:creator><dc:creator>Jianjun Guan</dc:creator><dc:creator>Xin Tian</dc:creator><prism:publicationName>ISPRS Journal of Photogrammetry and Remote Sensing</prism:publicationName><prism:doi>10.1016/j.isprsjprs.2026.01.021</prism:doi><description>Multi-modal image (MMI) matching plays a crucial role in the fusion of multi-source image information. However, due to the significant geometric and modality differences in MMI, existing methods often fail to achieve satisfactory matching performance. To address these challenges, we propose an end-to-end MMI matching approach, named adaptive multi-scale transformer (AMS-Former). First, AMS-Former constructs a multi-scale image matching framework that integrates contextual information across different scales, effectively identifying potential corresponding points and thereby improving matching accuracy. To handle the challenges caused by modality differences, we design a cross-modal feature extraction module with an adaptive modulation strategy. This module effectively couples features from different modalities, enhancing feature representation and improving model robustness under complex modality differences. To further enhance matching performance, we design a suitable loss function for the proposed AMS-Former to guide the optimization of network parameters. Finally, we use a cross-scale mutual supervision strategy to remove incorrect corresponding points and enhance the reliability of the matching results. Extensive experiments on five MMI datasets demonstrate that AMS-Former outperforms state-of-the-art methods, including RIFT, ASS, COFSM, POS-GIFT, Matchformer, SEMLA, TopicFM, and Lightglue. Our code is available at: https://github.com/Henryrjh/AMS_Former .
Published: 2026-01-19T08:34:48+00:00
Venue: ISPRS Journal of Photogrammetry and Remote Sensing
Score: 0.797 (must_read)</description><content:encoded>&lt;p&gt;&lt;strong&gt;Authors:&lt;/strong&gt; Jiahao Rao; Rui Liu; Jianjun Guan; Xin Tian&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Journal:&lt;/strong&gt; ISPRS Journal of Photogrammetry and Remote Sensing&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;a href="https://doi.org/10.1016/j.isprsjprs.2026.01.021"&gt;10.1016/j.isprsjprs.2026.01.021&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Relevance:&lt;/strong&gt; 0.797 (must_read)&lt;/p&gt;
&lt;p&gt;Multi-modal image (MMI) matching plays a crucial role in the fusion of multi-source image information. However, due to the significant geometric and modality differences in MMI, existing methods often fail to achieve satisfactory matching performance. To address these challenges, we propose an end-to-end MMI matching approach, named adaptive multi-scale transformer (AMS-Former). First, AMS-Former constructs a multi-scale image matching framework that integrates contextual information across different scales, effectively identifying potential corresponding points and thereby improving matching accuracy. To handle the challenges caused by modality differences, we design a cross-modal feature extraction module with an adaptive modulation strategy. This module effectively couples features from different modalities, enhancing feature representation and improving model robustness under complex modality differences. To further enhance matching performance, we design a suitable loss function for the proposed AMS-Former to guide the optimization of network parameters. Finally, we use a cross-scale mutual supervision strategy to remove incorrect corresponding points and enhance the reliability of the matching results. Extensive experiments on five MMI datasets demonstrate that AMS-Former outperforms state-of-the-art methods, including RIFT, ASS, COFSM, POS-GIFT, Matchformer, SEMLA, TopicFM, and Lightglue. Our code is available at: https://github.com/Henryrjh/AMS_Former .&lt;/p&gt;</content:encoded></item><item><title>Low-Rank Key Value Attention</title><link>https://arxiv.org/abs/2601.11471v1</link><guid>http://arxiv.org/abs/2601.11471v1</guid><pubDate>Fri, 16 Jan 2026 17:56:40 +0000</pubDate><dc:creator>James O'Neill</dc:creator><dc:creator>Robert Clancy</dc:creator><dc:creator>Mariia Matskevichus</dc:creator><dc:creator>Fergal Reid</dc:creator><dc:type>preprint</dc:type><prism:publicationName>arXiv</prism:publicationName><description>Transformer pretraining is increasingly constrained by memory and compute requirements, with the key-value (KV) cache emerging as a dominant bottleneck during training and autoregressive decoding. We propose \textit{low-rank KV adaptation} (LRKV), a simple modification of multi-head attention that reduces KV cache memory by exploiting redundancy across attention heads while preserving full token-level resolution. Each layer uses a shared full-rank KV projection augmented with low-rank, head-specific residuals, yielding a continuous trade-off between complete sharing and fully independent attention.
  LRKV is a drop-in replacement for standard multi-head attention and directly subsumes query-sharing approaches such as multi-query and grouped-query attention, while remaining distinct from latent-compression methods such as multi-latent attention (MLA). Across large-scale pretraining experiments, LRKV consistently achieves faster loss reduction, lower validation perplexity, and stronger downstream task performance than standard attention, MQA/GQA, and MLA. At the 2.5B scale, LRKV outperforms standard attention while using roughly half the KV cache, and reaches equivalent model quality with up to \textbf{20-25\% less training compute} when measured in cumulative FLOPs. To explain these gains, we analyze attention head structure in operator space and show that LRKV preserves nearly all functional head diversity relative to standard attention, whereas more aggressive KV-sharing mechanisms rely on compensatory query specialization. Together, these results establish LRKV as a practical and effective attention mechanism for scaling Transformer pretraining under memory- and compute-constrained regimes.
Published: 2026-01-16T17:56:40+00:00
Venue: arXiv
Score: 0.796 (must_read)</description><content:encoded>&lt;p&gt;&lt;strong&gt;Authors:&lt;/strong&gt; James O&amp;#x27;Neill; Robert Clancy; Mariia Matskevichus; Fergal Reid&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Journal:&lt;/strong&gt; arXiv&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Relevance:&lt;/strong&gt; 0.796 (must_read)&lt;/p&gt;
&lt;p&gt;Transformer pretraining is increasingly constrained by memory and compute requirements, with the key-value (KV) cache emerging as a dominant bottleneck during training and autoregressive decoding. We propose \textit{low-rank KV adaptation} (LRKV), a simple modification of multi-head attention that reduces KV cache memory by exploiting redundancy across attention heads while preserving full token-level resolution. Each layer uses a shared full-rank KV projection augmented with low-rank, head-specific residuals, yielding a continuous trade-off between complete sharing and fully independent attention.
  LRKV is a drop-in replacement for standard multi-head attention and directly subsumes query-sharing approaches such as multi-query and grouped-query attention, while remaining distinct from latent-compression methods such as multi-latent attention (MLA). Across large-scale pretraining experiments, LRKV consistently achieves faster loss reduction, lower validation perplexity, and stronger downstream task performance than standard attention, MQA/GQA, and MLA. At the 2.5B scale, LRKV outperforms standard attention while using roughly half the KV cache, and reaches equivalent model quality with up to \textbf{20-25\% less training compute} when measured in cumulative FLOPs. To explain these gains, we analyze attention head structure in operator space and show that LRKV preserves nearly all functional head diversity relative to standard attention, whereas more aggressive KV-sharing mechanisms rely on compensatory query specialization. Together, these results establish LRKV as a practical and effective attention mechanism for scaling Transformer pretraining under memory- and compute-constrained regimes.&lt;/p&gt;</content:encoded></item><item><title>Beyond Model Scaling: Test-Time Intervention for Efficient Deep Reasoning</title><link>https://arxiv.org/abs/2601.11252v1</link><guid>http://arxiv.org/abs/2601.11252v1</guid><pubDate>Fri, 16 Jan 2026 13:00:42 +0000</pubDate><dc:creator>Qianyue Wang</dc:creator><dc:creator>Jinwu Hu</dc:creator><dc:creator>Yufeng Wang</dc:creator><dc:creator>Huanxiang Lin</dc:creator><dc:creator>Bolin Chen</dc:creator><dc:creator>Zhiquan Wen</dc:creator><dc:creator>Yaofo Chen</dc:creator><dc:creator>Mingkui Tan</dc:creator><dc:type>preprint</dc:type><prism:publicationName>arXiv</prism:publicationName><description>Large Reasoning Models (LRMs) excel at multi-step reasoning but often suffer from inefficient reasoning processes like overthinking and overshoot, where excessive or misdirected reasoning increases computational cost and degrades performance. Existing efficient reasoning methods operate in a closed-loop manner, lacking mechanisms for external intervention to guide the reasoning process. To address this, we propose Think-with-Me, a novel test-time interactive reasoning paradigm that introduces external feedback intervention into the reasoning process. Our key insights are that transitional conjunctions serve as natural points for intervention, signaling phases of self-validation or exploration and using transitional words appropriately to prolong the reasoning enhances performance, while excessive use affects performance. Building on these insights, Think-with-Me pauses reasoning at these points for external feedback, adaptively extending or terminating reasoning to reduce redundancy while preserving accuracy. The feedback is generated via a multi-criteria evaluation (rationality and completeness) and comes from either human or LLM proxies. We train the target model using Group Relative Policy Optimization (GRPO) to adapt to this interactive mode. Experiments show that Think-with-Me achieves a superior balance between accuracy and reasoning length under limited context windows. On AIME24, Think-with-Me outperforms QwQ-32B by 7.19% in accuracy while reducing average reasoning length by 81% under an 8K window. The paradigm also benefits security and creative tasks.
Published: 2026-01-16T13:00:42+00:00
Venue: arXiv
Score: 0.795 (must_read)</description><content:encoded>&lt;p&gt;&lt;strong&gt;Authors:&lt;/strong&gt; Qianyue Wang; Jinwu Hu; Yufeng Wang; Huanxiang Lin; Bolin Chen; Zhiquan Wen; Yaofo Chen; Mingkui Tan&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Journal:&lt;/strong&gt; arXiv&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Relevance:&lt;/strong&gt; 0.795 (must_read)&lt;/p&gt;
&lt;p&gt;Large Reasoning Models (LRMs) excel at multi-step reasoning but often suffer from inefficient reasoning processes like overthinking and overshoot, where excessive or misdirected reasoning increases computational cost and degrades performance. Existing efficient reasoning methods operate in a closed-loop manner, lacking mechanisms for external intervention to guide the reasoning process. To address this, we propose Think-with-Me, a novel test-time interactive reasoning paradigm that introduces external feedback intervention into the reasoning process. Our key insights are that transitional conjunctions serve as natural points for intervention, signaling phases of self-validation or exploration and using transitional words appropriately to prolong the reasoning enhances performance, while excessive use affects performance. Building on these insights, Think-with-Me pauses reasoning at these points for external feedback, adaptively extending or terminating reasoning to reduce redundancy while preserving accuracy. The feedback is generated via a multi-criteria evaluation (rationality and completeness) and comes from either human or LLM proxies. We train the target model using Group Relative Policy Optimization (GRPO) to adapt to this interactive mode. Experiments show that Think-with-Me achieves a superior balance between accuracy and reasoning length under limited context windows. On AIME24, Think-with-Me outperforms QwQ-32B by 7.19% in accuracy while reducing average reasoning length by 81% under an 8K window. The paradigm also benefits security and creative tasks.&lt;/p&gt;</content:encoded></item><item><title>An Adaptive Regularized Topological Segmentation Network Integrating Inter-Class Relations and Occlusion Information for Vehicle Component Recognition</title><link>https://doi.org/10.1016/j.inffus.2026.104157</link><guid>10.1016/j.inffus.2026.104157</guid><pubDate>Sun, 18 Jan 2026 04:47:28 +0000</pubDate><dc:creator>Xunqi Zhou</dc:creator><dc:creator>Zhenqi Zhang</dc:creator><dc:creator>Zifeng Wu</dc:creator><dc:creator>Qianming Wang</dc:creator><dc:creator>Jing Teng</dc:creator><dc:creator>Jinlong Liu</dc:creator><dc:creator>Yongjie Zhai</dc:creator><prism:publicationName>Information Fusion</prism:publicationName><prism:doi>10.1016/j.inffus.2026.104157</prism:doi><description>In intelligent vehicle damage assessment, component recognition faces challenges such as significant intra-class variability and minimal inter-class differences, which hinder detection, as well as occlusions and ambiguous boundaries, which complicate segmentation. We generalize these problems into three core aspects: inter-object relational modeling, semantic-detail information balancing, and occlusion-aware decoupling. To this end, we propose the Adaptive Regularized Topological Segmentation (ARTSeg) network, comprising three complementary modules: Inter-Class Graph Constraint (ICGC), Constrained Detail Feature Backtracking (CDFB), and Topological Decoupling Segmentation (TDS). Each module is purposefully designed, integrated in a progressive structure, and synergistically reinforces the others to enhance overall performance. Specifically, ICGC clusters intra-class features and establishes implicit topological constraints among categories during feature extraction, enabling the model to better capture inter-class relationships and improve detection representation. Subsequently, CDFB evaluates the impact of channel-wise feature information within each candidate region on segmentation accuracy and computational cost, dynamically selecting appropriate feature resolutions for individual instances while balancing the demands of detection and segmentation tasks. Finally, TDS introduces topological associations between occluded and occluding regions at the feature level and decouples them at the task level, explicitly modeling generalized occlusion regions and enhancing segmentation performance. We quantitatively and qualitatively evaluate ARTSeg on a 59-category vehicle component dataset constructed for insurance damage assessment, achieving notable improvements in addressing the aforementioned problems. Experiments on two public datasets, DSMLR and Carparts, further validate the generalization capability of the proposed method. Results indicate that ARTSeg provides practical guidance for component recognition in intelligent vehicle damage assessment.
Published: 2026-01-18T04:47:28+00:00
Venue: Information Fusion
Score: 0.792 (must_read)</description><content:encoded>&lt;p&gt;&lt;strong&gt;Authors:&lt;/strong&gt; Xunqi Zhou; Zhenqi Zhang; Zifeng Wu; Qianming Wang; Jing Teng; Jinlong Liu; Yongjie Zhai&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Journal:&lt;/strong&gt; Information Fusion&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;a href="https://doi.org/10.1016/j.inffus.2026.104157"&gt;10.1016/j.inffus.2026.104157&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Relevance:&lt;/strong&gt; 0.792 (must_read)&lt;/p&gt;
&lt;p&gt;In intelligent vehicle damage assessment, component recognition faces challenges such as significant intra-class variability and minimal inter-class differences, which hinder detection, as well as occlusions and ambiguous boundaries, which complicate segmentation. We generalize these problems into three core aspects: inter-object relational modeling, semantic-detail information balancing, and occlusion-aware decoupling. To this end, we propose the Adaptive Regularized Topological Segmentation (ARTSeg) network, comprising three complementary modules: Inter-Class Graph Constraint (ICGC), Constrained Detail Feature Backtracking (CDFB), and Topological Decoupling Segmentation (TDS). Each module is purposefully designed, integrated in a progressive structure, and synergistically reinforces the others to enhance overall performance. Specifically, ICGC clusters intra-class features and establishes implicit topological constraints among categories during feature extraction, enabling the model to better capture inter-class relationships and improve detection representation. Subsequently, CDFB evaluates the impact of channel-wise feature information within each candidate region on segmentation accuracy and computational cost, dynamically selecting appropriate feature resolutions for individual instances while balancing the demands of detection and segmentation tasks. Finally, TDS introduces topological associations between occluded and occluding regions at the feature level and decouples them at the task level, explicitly modeling generalized occlusion regions and enhancing segmentation performance. We quantitatively and qualitatively evaluate ARTSeg on a 59-category vehicle component dataset constructed for insurance damage assessment, achieving notable improvements in addressing the aforementioned problems. Experiments on two public datasets, DSMLR and Carparts, further validate the generalization capability of the proposed method. Results indicate that ARTSeg provides practical guidance for component recognition in intelligent vehicle damage assessment.&lt;/p&gt;</content:encoded></item><item><title>FAQ: Mitigating Quantization Error via Regenerating Calibration Data with Family-Aware Quantization</title><link>https://arxiv.org/abs/2601.11200v1</link><guid>http://arxiv.org/abs/2601.11200v1</guid><pubDate>Fri, 16 Jan 2026 11:22:23 +0000</pubDate><dc:creator>Haiyang Xiao</dc:creator><dc:creator>Weiqing Li</dc:creator><dc:creator>Jinyue Guo</dc:creator><dc:creator>Guochao Jiang</dc:creator><dc:creator>Guohua Liu</dc:creator><dc:creator>Yuewei Zhang</dc:creator><dc:type>preprint</dc:type><prism:publicationName>arXiv</prism:publicationName><description>Although post-training quantization (PTQ) provides an efficient numerical compression scheme for deploying large language models (LLMs) on resource-constrained devices, the representativeness and universality of calibration data remain a core bottleneck in determining the accuracy of quantization parameters. Traditional PTQ methods typically rely on limited samples, making it difficult to capture the activation distribution during the inference phase, leading to biases in quantization parameters. To address this, we propose \textbf{FAQ} (Family-Aware Quantization), a calibration data regeneration framework that leverages prior knowledge from LLMs of the same family to generate high-fidelity calibration samples. Specifically, FAQ first inputs the original calibration samples into a larger LLM from the same family as the target model, regenerating a series of high-fidelity calibration data using a highly consistent knowledge system. Subsequently, this data, carrying Chain-of-Thought reasoning and conforming to the expected activation distribution, undergoes group competition under expert guidance to select the best samples, which are then re-normalized to enhance the effectiveness of standard PTQ. Experiments on multiple model series, including Qwen3-8B, show that FAQ reduces accuracy loss by up to 28.5\% compared to the baseline with original calibration data, demonstrating its powerful potential and contribution.
Published: 2026-01-16T11:22:23+00:00
Venue: arXiv
Score: 0.783 (must_read)</description><content:encoded>&lt;p&gt;&lt;strong&gt;Authors:&lt;/strong&gt; Haiyang Xiao; Weiqing Li; Jinyue Guo; Guochao Jiang; Guohua Liu; Yuewei Zhang&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Journal:&lt;/strong&gt; arXiv&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Relevance:&lt;/strong&gt; 0.783 (must_read)&lt;/p&gt;
&lt;p&gt;Although post-training quantization (PTQ) provides an efficient numerical compression scheme for deploying large language models (LLMs) on resource-constrained devices, the representativeness and universality of calibration data remain a core bottleneck in determining the accuracy of quantization parameters. Traditional PTQ methods typically rely on limited samples, making it difficult to capture the activation distribution during the inference phase, leading to biases in quantization parameters. To address this, we propose \textbf{FAQ} (Family-Aware Quantization), a calibration data regeneration framework that leverages prior knowledge from LLMs of the same family to generate high-fidelity calibration samples. Specifically, FAQ first inputs the original calibration samples into a larger LLM from the same family as the target model, regenerating a series of high-fidelity calibration data using a highly consistent knowledge system. Subsequently, this data, carrying Chain-of-Thought reasoning and conforming to the expected activation distribution, undergoes group competition under expert guidance to select the best samples, which are then re-normalized to enhance the effectiveness of standard PTQ. Experiments on multiple model series, including Qwen3-8B, show that FAQ reduces accuracy loss by up to 28.5\% compared to the baseline with original calibration data, demonstrating its powerful potential and contribution.&lt;/p&gt;</content:encoded></item><item><title>PIDE-Net: A Heterogeneous Processing Paradigm for UAV Object Detection</title><link>https://doi.org/10.1016/j.eswa.2026.131194</link><guid>10.1016/j.eswa.2026.131194</guid><pubDate>Sun, 18 Jan 2026 06:31:10 +0000</pubDate><dc:creator>Shuming Lin</dc:creator><dc:creator>Sang Fyeng</dc:creator><dc:creator>Jinyi Liang</dc:creator><dc:creator>Junnan Tan</dc:creator><prism:publicationName>Expert Systems with Applications</prism:publicationName><prism:doi>10.1016/j.eswa.2026.131194</prism:doi><description>Small object detection in unmanned aerial vehicle (UAV) imagery confronts multifaceted technical challenges encompassing severe geometric deformations, dense target clustering, and stringent computational resource constraints. Contemporary detection frameworks predominantly adopt homogeneous processing paradigms, which suffer from systematic information deterioration across feature representation, contextual modeling, and multi-scale fusion stages, constituting a fundamental performance bottlenecks in UAV scenarios. This paper introduces PIDE-Net (Progressive Information Disentanglement and Enhancement Network), establishing a heterogeneous processing paradigm that achieves synergistic optimization of detection accuracy and computational efficiency. The framework implements progressive information refinement through three core modules.The Position-aware Refined Interactive Semantic Module (PRISM) employs a position-semantic feature disentanglement mechanism to address information confusion in complex scenarios at the source of feature representation.The Semantic-Guided State Space Module (SG-SSM) introduces content-driven attention state space equations, enabling efficient global context modeling with O(n) linear complexity. Finally, the Progressive Enhancement Pyramid Network (PEP-Net) adopts spatial weaving upsampling mechanisms to preserve sparse information integrity during multi-scale feature fusion.Experimental results demonstrate that PIDE-Net achieves AP 50 of 49.4%, 65.2%, and 52.6% on VisDrone2019, DOTA1.0, and AI-TODv2 datasets respectively, with AP S reaching 22.3%, 35.2%, and 35.6%, while maintaining only 15.4M parameters. Additionally, the framework achieves 59.4 FPS on edge devices. This methodology provides a novel technical paradigm for the collaborative design of high-precision, high-efficiency UAV detection systems. It offers a theoretical and practical foundation for the evolution from homogeneous to heterogeneous processing in computer vision.
Published: 2026-01-18T06:31:10+00:00
Venue: Expert Systems with Applications
Score: 0.782 (must_read)</description><content:encoded>&lt;p&gt;&lt;strong&gt;Authors:&lt;/strong&gt; Shuming Lin; Sang Fyeng; Jinyi Liang; Junnan Tan&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Journal:&lt;/strong&gt; Expert Systems with Applications&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;a href="https://doi.org/10.1016/j.eswa.2026.131194"&gt;10.1016/j.eswa.2026.131194&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Relevance:&lt;/strong&gt; 0.782 (must_read)&lt;/p&gt;
&lt;p&gt;Small object detection in unmanned aerial vehicle (UAV) imagery confronts multifaceted technical challenges encompassing severe geometric deformations, dense target clustering, and stringent computational resource constraints. Contemporary detection frameworks predominantly adopt homogeneous processing paradigms, which suffer from systematic information deterioration across feature representation, contextual modeling, and multi-scale fusion stages, constituting a fundamental performance bottlenecks in UAV scenarios. This paper introduces PIDE-Net (Progressive Information Disentanglement and Enhancement Network), establishing a heterogeneous processing paradigm that achieves synergistic optimization of detection accuracy and computational efficiency. The framework implements progressive information refinement through three core modules.The Position-aware Refined Interactive Semantic Module (PRISM) employs a position-semantic feature disentanglement mechanism to address information confusion in complex scenarios at the source of feature representation.The Semantic-Guided State Space Module (SG-SSM) introduces content-driven attention state space equations, enabling efficient global context modeling with O(n) linear complexity. Finally, the Progressive Enhancement Pyramid Network (PEP-Net) adopts spatial weaving upsampling mechanisms to preserve sparse information integrity during multi-scale feature fusion.Experimental results demonstrate that PIDE-Net achieves AP 50 of 49.4%, 65.2%, and 52.6% on VisDrone2019, DOTA1.0, and AI-TODv2 datasets respectively, with AP S reaching 22.3%, 35.2%, and 35.6%, while maintaining only 15.4M parameters. Additionally, the framework achieves 59.4 FPS on edge devices. This methodology provides a novel technical paradigm for the collaborative design of high-precision, high-efficiency UAV detection systems. It offers a theoretical and practical foundation for the evolution from homogeneous to heterogeneous processing in computer vision.&lt;/p&gt;</content:encoded></item><item><title>MHA2MLA-VLM: Enabling DeepSeek's Economical Multi-Head Latent Attention across Vision-Language Models</title><link>https://arxiv.org/abs/2601.11464v1</link><guid>http://arxiv.org/abs/2601.11464v1</guid><pubDate>Fri, 16 Jan 2026 17:45:34 +0000</pubDate><dc:creator>Xiaoran Fan</dc:creator><dc:creator>Zhichao Sun</dc:creator><dc:creator>Tao Ji</dc:creator><dc:creator>Lixing Shen</dc:creator><dc:creator>Tao Gui</dc:creator><dc:type>preprint</dc:type><prism:publicationName>arXiv</prism:publicationName><description>As vision-language models (VLMs) tackle increasingly complex and multimodal tasks, the rapid growth of Key-Value (KV) cache imposes significant memory and computational bottlenecks during inference. While Multi-Head Latent Attention (MLA) offers an effective means to compress the KV cache and accelerate inference, adapting existing VLMs to the MLA architecture without costly pretraining remains largely unexplored. In this work, we present MHA2MLA-VLM, a parameter-efficient and multimodal-aware framework for converting off-the-shelf VLMs to MLA. Our approach features two core techniques: (1) a modality-adaptive partial-RoPE strategy that supports both traditional and multimodal settings by selectively masking nonessential dimensions, and (2) a modality-decoupled low-rank approximation method that independently compresses the visual and textual KV spaces. Furthermore, we introduce parameter-efficient fine-tuning to minimize adaptation cost and demonstrate that minimizing output activation error, rather than parameter distance, substantially reduces performance loss. Extensive experiments on three representative VLMs show that MHA2MLA-VLM restores original model performance with minimal supervised data, significantly reduces KV cache footprint, and integrates seamlessly with KV quantization.
Published: 2026-01-16T17:45:34+00:00
Venue: arXiv
Score: 0.782 (must_read)</description><content:encoded>&lt;p&gt;&lt;strong&gt;Authors:&lt;/strong&gt; Xiaoran Fan; Zhichao Sun; Tao Ji; Lixing Shen; Tao Gui&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Journal:&lt;/strong&gt; arXiv&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Relevance:&lt;/strong&gt; 0.782 (must_read)&lt;/p&gt;
&lt;p&gt;As vision-language models (VLMs) tackle increasingly complex and multimodal tasks, the rapid growth of Key-Value (KV) cache imposes significant memory and computational bottlenecks during inference. While Multi-Head Latent Attention (MLA) offers an effective means to compress the KV cache and accelerate inference, adapting existing VLMs to the MLA architecture without costly pretraining remains largely unexplored. In this work, we present MHA2MLA-VLM, a parameter-efficient and multimodal-aware framework for converting off-the-shelf VLMs to MLA. Our approach features two core techniques: (1) a modality-adaptive partial-RoPE strategy that supports both traditional and multimodal settings by selectively masking nonessential dimensions, and (2) a modality-decoupled low-rank approximation method that independently compresses the visual and textual KV spaces. Furthermore, we introduce parameter-efficient fine-tuning to minimize adaptation cost and demonstrate that minimizing output activation error, rather than parameter distance, substantially reduces performance loss. Extensive experiments on three representative VLMs show that MHA2MLA-VLM restores original model performance with minimal supervised data, significantly reduces KV cache footprint, and integrates seamlessly with KV quantization.&lt;/p&gt;</content:encoded></item><item><title>SLNMapping: Super Lightweight Neural Mapping in Large-Scale Scenes</title><link>https://doi.org/10.1007/s11263-025-02581-6</link><guid>10.1007/s11263-025-02581-6</guid><pubDate>Mon, 19 Jan 2026 08:28:15 +0000</pubDate><dc:creator>Chenhui Shi</dc:creator><dc:creator>Fulin Tang</dc:creator><dc:creator>Hao Wei</dc:creator><dc:creator>Yihong Wu</dc:creator><prism:publicationName>International Journal of Computer Vision</prism:publicationName><prism:doi>10.1007/s11263-025-02581-6</prism:doi><description>We propose SLNMapping, a novel neural mapping framework for super lightweight reconstruction in large-scale scenes. The core is a new ultra-compact neural map representation composed of a set of feature-independent local signed distance functions (SDFs) with outstanding expressiveness. To support efficient optimization, we introduce a novel parallel local SDF detection algorithm that enables real-time updates of local SDF states. Based on the excellent representation, we develop a three-stage mapping strategy for efficient, accurate, and lightweight large-scale reconstruction from streaming LiDAR frames. First, an incremental mapping module is introduced for accurate online pose estimation and simultaneous construction of a globally consistent neural map. Then, we perform offline global optimization to refine the reconstruction quality for the initial map. Finally, we propose an innovative neural map simplification method tailored for our representation, which aggregates the redundant local SDFs to further reduce the memory usage while preserving geometric fidelity. Extensive experiments demonstrate that our approach delivers superior localization accuracy and achieves state-of-the-art mapping performance with high efficiency and extremely low map memory consumption, especially requiring only about 1/10 the memory on the Oxford Spires dataset compared with existing advanced methods.
Published: 2026-01-19T08:28:15+00:00
Venue: International Journal of Computer Vision
Score: 0.782 (must_read)</description><content:encoded>&lt;p&gt;&lt;strong&gt;Authors:&lt;/strong&gt; Chenhui Shi; Fulin Tang; Hao Wei; Yihong Wu&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Journal:&lt;/strong&gt; International Journal of Computer Vision&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;a href="https://doi.org/10.1007/s11263-025-02581-6"&gt;10.1007/s11263-025-02581-6&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Relevance:&lt;/strong&gt; 0.782 (must_read)&lt;/p&gt;
&lt;p&gt;We propose SLNMapping, a novel neural mapping framework for super lightweight reconstruction in large-scale scenes. The core is a new ultra-compact neural map representation composed of a set of feature-independent local signed distance functions (SDFs) with outstanding expressiveness. To support efficient optimization, we introduce a novel parallel local SDF detection algorithm that enables real-time updates of local SDF states. Based on the excellent representation, we develop a three-stage mapping strategy for efficient, accurate, and lightweight large-scale reconstruction from streaming LiDAR frames. First, an incremental mapping module is introduced for accurate online pose estimation and simultaneous construction of a globally consistent neural map. Then, we perform offline global optimization to refine the reconstruction quality for the initial map. Finally, we propose an innovative neural map simplification method tailored for our representation, which aggregates the redundant local SDFs to further reduce the memory usage while preserving geometric fidelity. Extensive experiments demonstrate that our approach delivers superior localization accuracy and achieves state-of-the-art mapping performance with high efficiency and extremely low map memory consumption, especially requiring only about 1/10 the memory on the Oxford Spires dataset compared with existing advanced methods.&lt;/p&gt;</content:encoded></item><item><title>BAPO: Boundary-Aware Policy Optimization for Reliable Agentic Search</title><link>https://arxiv.org/abs/2601.11037v1</link><guid>http://arxiv.org/abs/2601.11037v1</guid><pubDate>Fri, 16 Jan 2026 07:06:58 +0000</pubDate><dc:creator>Shiyu Liu</dc:creator><dc:creator>Yongjing Yin</dc:creator><dc:creator>Jianhao Yan</dc:creator><dc:creator>Yunbo Tang</dc:creator><dc:creator>Qinggang Zhang</dc:creator><dc:creator>Bei Li</dc:creator><dc:creator>Xin Chen</dc:creator><dc:creator>Jingang Wang</dc:creator><dc:creator>Xunliang Cai</dc:creator><dc:creator>Jinsong Su</dc:creator><dc:type>preprint</dc:type><prism:publicationName>arXiv</prism:publicationName><description>RL-based agentic search enables LLMs to solve complex questions via dynamic planning and external search. While this approach significantly enhances accuracy with agent policies optimized via large-scale reinforcement learning, we identify a critical gap in reliability: these agents fail to recognize their reasoning boundaries and rarely admit ``I DON'T KNOW'' (IDK) even when evidence is insufficient or reasoning reaches its limit. The lack of reliability often leads to plausible but unreliable answers, introducing significant risks in many real-world scenarios. To this end, we propose Boundary-Aware Policy Optimization (BAPO), a novel RL framework designed to cultivate reliable boundary awareness without compromising accuracy. BAPO introduces two key components: (i) a group-based boundary-aware reward that encourages an IDK response only when the reasoning reaches its limit, and (ii) an adaptive reward modulator that strategically suspends this reward during early exploration, preventing the model from exploiting IDK as a shortcut. Extensive experiments on four benchmarks demonstrate that BAPO substantially enhances the overall reliability of agentic search.
Published: 2026-01-16T07:06:58+00:00
Venue: arXiv
Score: 0.780 (must_read)</description><content:encoded>&lt;p&gt;&lt;strong&gt;Authors:&lt;/strong&gt; Shiyu Liu; Yongjing Yin; Jianhao Yan; Yunbo Tang; Qinggang Zhang; Bei Li; Xin Chen; Jingang Wang; Xunliang Cai; Jinsong Su&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Journal:&lt;/strong&gt; arXiv&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Relevance:&lt;/strong&gt; 0.780 (must_read)&lt;/p&gt;
&lt;p&gt;RL-based agentic search enables LLMs to solve complex questions via dynamic planning and external search. While this approach significantly enhances accuracy with agent policies optimized via large-scale reinforcement learning, we identify a critical gap in reliability: these agents fail to recognize their reasoning boundaries and rarely admit ``I DON&amp;#x27;T KNOW&amp;#x27;&amp;#x27; (IDK) even when evidence is insufficient or reasoning reaches its limit. The lack of reliability often leads to plausible but unreliable answers, introducing significant risks in many real-world scenarios. To this end, we propose Boundary-Aware Policy Optimization (BAPO), a novel RL framework designed to cultivate reliable boundary awareness without compromising accuracy. BAPO introduces two key components: (i) a group-based boundary-aware reward that encourages an IDK response only when the reasoning reaches its limit, and (ii) an adaptive reward modulator that strategically suspends this reward during early exploration, preventing the model from exploiting IDK as a shortcut. Extensive experiments on four benchmarks demonstrate that BAPO substantially enhances the overall reliability of agentic search.&lt;/p&gt;</content:encoded></item><item><title>Seeing through the noise: A cross-modal guided framework for hyperspectral image classification under multi-type degradations</title><link>https://doi.org/10.1016/j.jag.2026.105117</link><guid>10.1016/j.jag.2026.105117</guid><pubDate>Sun, 18 Jan 2026 04:51:27 +0000</pubDate><dc:creator>Hui Liu</dc:creator><dc:creator>Wei Tong</dc:creator><dc:creator>Ning Chen</dc:creator><dc:creator>Tao Xie</dc:creator><dc:creator>Chenjia Huang</dc:creator><dc:creator>Xia Yue</dc:creator><dc:creator>Zhou Huang</dc:creator><prism:publicationName>International Journal of Applied Earth Observation and Geoinformation</prism:publicationName><prism:doi>10.1016/j.jag.2026.105117</prism:doi><description>Recent advances in deep learning and multimodal data fusion technologies have significantly enhanced hyperspectral image (HSI) classification performance. Nevertheless, classification accuracy of hyperspectral data continues to degrade substantially under diverse degradation scenarios, such as noise interference, spectral distortion, or reduced resolution. To robustly address this challenge, this paper proposes a novel cross-modal guided classification framework that integrates active remote sensing data (e.g., LiDAR) to improve classification resilience under degraded conditions. Specifically, we introduce a Cross-Modal Feature Pyramid Guidance (CMFPG) module, which effectively utilizes cross-modal information across multiple levels and scales to guide hyperspectral feature extraction and fusion, thereby enhancing modeling stability in degraded environments. Additionally, we develop the HyperGroupMix module, which enhances cross-domain adaptability through grouping spectral bands, extracting statistical features, and transferring features across samples. Experimental results conducted under complex degradation conditions demonstrate that our proposed method exhibits stable high-level classification accuracy and robustness in overall performance. The code is accessible at: https://github.com/miliwww/CMGF
Published: 2026-01-18T04:51:27+00:00
Venue: International Journal of Applied Earth Observation and Geoinformation
Score: 0.779 (consider)</description><content:encoded>&lt;p&gt;&lt;strong&gt;Authors:&lt;/strong&gt; Hui Liu; Wei Tong; Ning Chen; Tao Xie; Chenjia Huang; Xia Yue; Zhou Huang&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Journal:&lt;/strong&gt; International Journal of Applied Earth Observation and Geoinformation&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;a href="https://doi.org/10.1016/j.jag.2026.105117"&gt;10.1016/j.jag.2026.105117&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Relevance:&lt;/strong&gt; 0.779 (consider)&lt;/p&gt;
&lt;p&gt;Recent advances in deep learning and multimodal data fusion technologies have significantly enhanced hyperspectral image (HSI) classification performance. Nevertheless, classification accuracy of hyperspectral data continues to degrade substantially under diverse degradation scenarios, such as noise interference, spectral distortion, or reduced resolution. To robustly address this challenge, this paper proposes a novel cross-modal guided classification framework that integrates active remote sensing data (e.g., LiDAR) to improve classification resilience under degraded conditions. Specifically, we introduce a Cross-Modal Feature Pyramid Guidance (CMFPG) module, which effectively utilizes cross-modal information across multiple levels and scales to guide hyperspectral feature extraction and fusion, thereby enhancing modeling stability in degraded environments. Additionally, we develop the HyperGroupMix module, which enhances cross-domain adaptability through grouping spectral bands, extracting statistical features, and transferring features across samples. Experimental results conducted under complex degradation conditions demonstrate that our proposed method exhibits stable high-level classification accuracy and robustness in overall performance. The code is accessible at: https://github.com/miliwww/CMGF&lt;/p&gt;</content:encoded></item><item><title>面向遥感图像解译的参数高效微调研究综述</title><link>https://doi.org/10.11834/jig.250105</link><guid>10.11834/jig.250105</guid><pubDate>Mon, 19 Jan 2026 01:26:13 +0000</pubDate><dc:creator>Chen Shiqi</dc:creator><dc:creator>Yang Xue</dc:creator><dc:creator>Zhu Rongqiang</dc:creator><dc:creator>Liao Ning</dc:creator><dc:creator>Zhao Weiwei</dc:creator><prism:publicationName>Journal of Image and Graphics</prism:publicationName><prism:doi>10.11834/jig.250105</prism:doi><description>海量遥感数据的获取和AI大模型的发展极大程度地推动了智能化遥感图像解译的下游应用落地。“预训练 + 微调”是视觉语言基础大模型适配下游领域的经典范式，能有效将基础模型的知识迁移至新任务中。尽管遥感大模型发展如火如荼且在下游任务中表现突出，扩展的模型规模和高昂的训练成本使其难以适用于资源受限、标签不足、需求动态的实际应用场景。为使模型快速适应特定下游任务且有效避免额外训练资源消耗，参数高效微调方法得以广泛研究，并逐渐应用于遥感图像解译当中，成为当下的研究热点。本文面向不同类型的参数高效微调方法和解译任务，对提示词微调、适配器微调和低秩自适应微调三大类方法展开调研并梳理了现有研究工作。此外，本文收集归纳并总结了多个代表性数据集上30余种用于遥感图像解译任务的参数高效微调方法的性能，并从模型精度、训练参数量和推理耗时角度综合评估了方法性能，有助于启发研究者提出新方法并进行公平比较。最后，本文结合当前现状从多模态生成式任务、模型可解释性、边缘端部署应用的角度，展望并讨论了该交叉领域的未来研究方向，旨在为打造“AI + 遥感”的下游应用生态提供理论参考与研究思路。
Published: 2026-01-19T01:26:13+00:00
Venue: Journal of Image and Graphics
Score: 0.778 (consider)</description><content:encoded>&lt;p&gt;&lt;strong&gt;Authors:&lt;/strong&gt; Chen Shiqi; Yang Xue; Zhu Rongqiang; Liao Ning; Zhao Weiwei&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Journal:&lt;/strong&gt; Journal of Image and Graphics&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;a href="https://doi.org/10.11834/jig.250105"&gt;10.11834/jig.250105&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Relevance:&lt;/strong&gt; 0.778 (consider)&lt;/p&gt;
&lt;p&gt;海量遥感数据的获取和AI大模型的发展极大程度地推动了智能化遥感图像解译的下游应用落地。“预训练 + 微调”是视觉语言基础大模型适配下游领域的经典范式，能有效将基础模型的知识迁移至新任务中。尽管遥感大模型发展如火如荼且在下游任务中表现突出，扩展的模型规模和高昂的训练成本使其难以适用于资源受限、标签不足、需求动态的实际应用场景。为使模型快速适应特定下游任务且有效避免额外训练资源消耗，参数高效微调方法得以广泛研究，并逐渐应用于遥感图像解译当中，成为当下的研究热点。本文面向不同类型的参数高效微调方法和解译任务，对提示词微调、适配器微调和低秩自适应微调三大类方法展开调研并梳理了现有研究工作。此外，本文收集归纳并总结了多个代表性数据集上30余种用于遥感图像解译任务的参数高效微调方法的性能，并从模型精度、训练参数量和推理耗时角度综合评估了方法性能，有助于启发研究者提出新方法并进行公平比较。最后，本文结合当前现状从多模态生成式任务、模型可解释性、边缘端部署应用的角度，展望并讨论了该交叉领域的未来研究方向，旨在为打造“AI + 遥感”的下游应用生态提供理论参考与研究思路。&lt;/p&gt;</content:encoded></item><item><title>Knowledge is Not Enough: Injecting RL Skills for Continual Adaptation</title><link>https://arxiv.org/abs/2601.11258v1</link><guid>http://arxiv.org/abs/2601.11258v1</guid><pubDate>Fri, 16 Jan 2026 13:08:16 +0000</pubDate><dc:creator>Pingzhi Tang</dc:creator><dc:creator>Yiding Wang</dc:creator><dc:creator>Muhan Zhang</dc:creator><dc:type>preprint</dc:type><prism:publicationName>arXiv</prism:publicationName><description>Large Language Models (LLMs) face the "knowledge cutoff" challenge, where their frozen parametric memory prevents direct internalization of new information. While Supervised Fine-Tuning (SFT) is commonly used to update model knowledge, it often updates factual content without reliably improving the model's ability to use the newly incorporated information for question answering or decision-making. Reinforcement Learning (RL) is essential for acquiring reasoning skills; however, its high computational cost makes it impractical for efficient online adaptation. We empirically observe that the parameter updates induced by SFT and RL are nearly orthogonal. Based on this observation, we propose Parametric Skill Transfer (PaST), a framework that supports modular skill transfer for efficient and effective knowledge adaptation. By extracting a domain-agnostic Skill Vector from a source domain, we can linearly inject knowledge manipulation skills into a target model after it has undergone lightweight SFT on new data. Experiments on knowledge-incorporation QA (SQuAD, LooGLE) and agentic tool-use benchmarks (ToolBench) demonstrate the effectiveness of our method. On SQuAD, PaST outperforms the state-of-the-art self-editing SFT baseline by up to 9.9 points. PaST further scales to long-context QA on LooGLE with an 8.0-point absolute accuracy gain, and improves zero-shot ToolBench success rates by +10.3 points on average with consistent gains across tool categories, indicating strong scalability and cross-domain transferability of the Skill Vector.
Published: 2026-01-16T13:08:16+00:00
Venue: arXiv
Score: 0.777 (consider)</description><content:encoded>&lt;p&gt;&lt;strong&gt;Authors:&lt;/strong&gt; Pingzhi Tang; Yiding Wang; Muhan Zhang&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Journal:&lt;/strong&gt; arXiv&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Relevance:&lt;/strong&gt; 0.777 (consider)&lt;/p&gt;
&lt;p&gt;Large Language Models (LLMs) face the &amp;quot;knowledge cutoff&amp;quot; challenge, where their frozen parametric memory prevents direct internalization of new information. While Supervised Fine-Tuning (SFT) is commonly used to update model knowledge, it often updates factual content without reliably improving the model&amp;#x27;s ability to use the newly incorporated information for question answering or decision-making. Reinforcement Learning (RL) is essential for acquiring reasoning skills; however, its high computational cost makes it impractical for efficient online adaptation. We empirically observe that the parameter updates induced by SFT and RL are nearly orthogonal. Based on this observation, we propose Parametric Skill Transfer (PaST), a framework that supports modular skill transfer for efficient and effective knowledge adaptation. By extracting a domain-agnostic Skill Vector from a source domain, we can linearly inject knowledge manipulation skills into a target model after it has undergone lightweight SFT on new data. Experiments on knowledge-incorporation QA (SQuAD, LooGLE) and agentic tool-use benchmarks (ToolBench) demonstrate the effectiveness of our method. On SQuAD, PaST outperforms the state-of-the-art self-editing SFT baseline by up to 9.9 points. PaST further scales to long-context QA on LooGLE with an 8.0-point absolute accuracy gain, and improves zero-shot ToolBench success rates by +10.3 points on average with consistent gains across tool categories, indicating strong scalability and cross-domain transferability of the Skill Vector.&lt;/p&gt;</content:encoded></item><item><title>SoLA-Vision: Fine-grained Layer-wise Linear Softmax Hybrid Attention</title><link>https://arxiv.org/abs/2601.11164v1</link><guid>http://arxiv.org/abs/2601.11164v1</guid><pubDate>Fri, 16 Jan 2026 10:26:53 +0000</pubDate><dc:creator>Ruibang Li</dc:creator><dc:creator>Guan Luo</dc:creator><dc:creator>Yiwei Zhang</dc:creator><dc:creator>Jin Gao</dc:creator><dc:creator>Bing Li</dc:creator><dc:creator>Weiming Hu</dc:creator><dc:type>preprint</dc:type><prism:publicationName>arXiv</prism:publicationName><description>Standard softmax self-attention excels in vision tasks but incurs quadratic complexity O(N^2), limiting high-resolution deployment. Linear attention reduces the cost to O(N), yet its compressed state representations can impair modeling capacity and accuracy. We present an analytical study that contrasts linear and softmax attention for visual representation learning from a layer-stacking perspective. We further conduct systematic experiments on layer-wise hybridization patterns of linear and softmax attention. Our results show that, compared with rigid intra-block hybrid designs, fine-grained layer-wise hybridization can match or surpass performance while requiring fewer softmax layers. Building on these findings, we propose SoLA-Vision (Softmax-Linear Attention Vision), a flexible layer-wise hybrid attention backbone that enables fine-grained control over how linear and softmax attention are integrated. By strategically inserting a small number of global softmax layers, SoLA-Vision achieves a strong trade-off between accuracy and computational cost. On ImageNet-1K, SoLA-Vision outperforms purely linear and other hybrid attention models. On dense prediction tasks, it consistently surpasses strong baselines by a considerable margin. Code will be released.
Published: 2026-01-16T10:26:53+00:00
Venue: arXiv
Score: 0.777 (consider)</description><content:encoded>&lt;p&gt;&lt;strong&gt;Authors:&lt;/strong&gt; Ruibang Li; Guan Luo; Yiwei Zhang; Jin Gao; Bing Li; Weiming Hu&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Journal:&lt;/strong&gt; arXiv&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Relevance:&lt;/strong&gt; 0.777 (consider)&lt;/p&gt;
&lt;p&gt;Standard softmax self-attention excels in vision tasks but incurs quadratic complexity O(N^2), limiting high-resolution deployment. Linear attention reduces the cost to O(N), yet its compressed state representations can impair modeling capacity and accuracy. We present an analytical study that contrasts linear and softmax attention for visual representation learning from a layer-stacking perspective. We further conduct systematic experiments on layer-wise hybridization patterns of linear and softmax attention. Our results show that, compared with rigid intra-block hybrid designs, fine-grained layer-wise hybridization can match or surpass performance while requiring fewer softmax layers. Building on these findings, we propose SoLA-Vision (Softmax-Linear Attention Vision), a flexible layer-wise hybrid attention backbone that enables fine-grained control over how linear and softmax attention are integrated. By strategically inserting a small number of global softmax layers, SoLA-Vision achieves a strong trade-off between accuracy and computational cost. On ImageNet-1K, SoLA-Vision outperforms purely linear and other hybrid attention models. On dense prediction tasks, it consistently surpasses strong baselines by a considerable margin. Code will be released.&lt;/p&gt;</content:encoded></item><item><title>WEGLA-NormGAN: wavelet-enhanced Cycle-GAN with global-local attention for radiometric normalization of remote sensing images</title><link>https://doi.org/10.1016/j.isprsjprs.2026.01.020</link><guid>10.1016/j.isprsjprs.2026.01.020</guid><pubDate>Mon, 19 Jan 2026 11:01:56 +0000</pubDate><dc:creator>Wenxia Gan</dc:creator><dc:creator>Yu Feng</dc:creator><dc:creator>Jianhao Miao</dc:creator><dc:creator>Xinghua Li</dc:creator><dc:creator>Huanfeng Shen</dc:creator><prism:publicationName>ISPRS Journal of Photogrammetry and Remote Sensing</prism:publicationName><prism:doi>10.1016/j.isprsjprs.2026.01.020</prism:doi><description>The diversity of satellite remote sensing images has significantly enhanced the capability to observe surface information on Earth. However, multi-temporal optical remote sensing images acquired from different sensor platforms often exhibit substantial radiometric discrepancies, and it is difficult to obtain overlapping reference images, which poses critical challenges for seamless large-scale mosaicking, including global radiometric inconsistency, unsmooth local transitions, and visible seamlines. Existing traditional and deep learning methods can achieve reasonable performance on paired datasets, but often face challenges in balancing spatial structural integrity with enhanced radiometric consistency and generalizing to unseen images. To address these issues, a wavelet-enhanced radiometric normalization network called WEGLA-NormGAN is proposed to generate radiometrically normalized imagery with sound radiometric consistency and spatial fidelity. This framework integrates frequency-domain and spatial-domain information to achieve consistent multi-scale radiometric feature modeling while ensuring spatial structural fidelity. Firstly, wavelet transform is introduced to effectively decouple radiometric information and structural features from images, explicitly enhancing radiometric feature representation and edge-texture preservation. Secondly, a U-Net architecture with multi-scale modeling advantages is fused with an adaptive attention mechanism incorporating residual structures. This hybrid design employs a statistical alignment strategy to efficiently extract global shallow features and local statistical information, adaptively adjust the dynamic attention of unseen data, and alleviate local distortions, improving radiometric consistency and achieving high-fidelity spatial structure preservation. The proposed framework generates radiometrically normalized imagery that harmonizes radiometric consistency with spatial fidelity, while achieving outstanding radiometric normalization even in unseen scenarios. Extensive experiments were conducted on two public datasets and a self-constructed dataset. The results demonstrate that WEGLA-NormGAN outperforms seven state-of-the-art methods in cross-temporal scenarios and five in cross-spatiotemporal scenarios in terms of radiometric consistency, structural fidelity, and robustness. The code is available at https://github.com/WITRS/WeGLA-Norm.git .
Published: 2026-01-19T11:01:56+00:00
Venue: ISPRS Journal of Photogrammetry and Remote Sensing
Score: 0.776 (consider)</description><content:encoded>&lt;p&gt;&lt;strong&gt;Authors:&lt;/strong&gt; Wenxia Gan; Yu Feng; Jianhao Miao; Xinghua Li; Huanfeng Shen&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Journal:&lt;/strong&gt; ISPRS Journal of Photogrammetry and Remote Sensing&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;a href="https://doi.org/10.1016/j.isprsjprs.2026.01.020"&gt;10.1016/j.isprsjprs.2026.01.020&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Relevance:&lt;/strong&gt; 0.776 (consider)&lt;/p&gt;
&lt;p&gt;The diversity of satellite remote sensing images has significantly enhanced the capability to observe surface information on Earth. However, multi-temporal optical remote sensing images acquired from different sensor platforms often exhibit substantial radiometric discrepancies, and it is difficult to obtain overlapping reference images, which poses critical challenges for seamless large-scale mosaicking, including global radiometric inconsistency, unsmooth local transitions, and visible seamlines. Existing traditional and deep learning methods can achieve reasonable performance on paired datasets, but often face challenges in balancing spatial structural integrity with enhanced radiometric consistency and generalizing to unseen images. To address these issues, a wavelet-enhanced radiometric normalization network called WEGLA-NormGAN is proposed to generate radiometrically normalized imagery with sound radiometric consistency and spatial fidelity. This framework integrates frequency-domain and spatial-domain information to achieve consistent multi-scale radiometric feature modeling while ensuring spatial structural fidelity. Firstly, wavelet transform is introduced to effectively decouple radiometric information and structural features from images, explicitly enhancing radiometric feature representation and edge-texture preservation. Secondly, a U-Net architecture with multi-scale modeling advantages is fused with an adaptive attention mechanism incorporating residual structures. This hybrid design employs a statistical alignment strategy to efficiently extract global shallow features and local statistical information, adaptively adjust the dynamic attention of unseen data, and alleviate local distortions, improving radiometric consistency and achieving high-fidelity spatial structure preservation. The proposed framework generates radiometrically normalized imagery that harmonizes radiometric consistency with spatial fidelity, while achieving outstanding radiometric normalization even in unseen scenarios. Extensive experiments were conducted on two public datasets and a self-constructed dataset. The results demonstrate that WEGLA-NormGAN outperforms seven state-of-the-art methods in cross-temporal scenarios and five in cross-spatiotemporal scenarios in terms of radiometric consistency, structural fidelity, and robustness. The code is available at https://github.com/WITRS/WeGLA-Norm.git .&lt;/p&gt;</content:encoded></item><item><title>Spurious Rewards Paradox: Mechanistically Understanding How RLVR Activates Memorization Shortcuts in LLMs</title><link>https://arxiv.org/abs/2601.11061v1</link><guid>http://arxiv.org/abs/2601.11061v1</guid><pubDate>Fri, 16 Jan 2026 07:55:38 +0000</pubDate><dc:creator>Lecheng Yan</dc:creator><dc:creator>Ruizhe Li</dc:creator><dc:creator>Guanhua Chen</dc:creator><dc:creator>Qing Li</dc:creator><dc:creator>Jiahui Geng</dc:creator><dc:creator>Wenxi Li</dc:creator><dc:creator>Vincent Wang</dc:creator><dc:creator>Chris Lee</dc:creator><dc:type>preprint</dc:type><prism:publicationName>arXiv</prism:publicationName><description>Reinforcement Learning with Verifiable Rewards (RLVR) is highly effective for enhancing LLM reasoning, yet recent evidence shows models like Qwen 2.5 achieve significant gains even with spurious or incorrect rewards. We investigate this phenomenon and identify a "Perplexity Paradox": spurious RLVR triggers a divergence where answer-token perplexity drops while prompt-side coherence degrades, suggesting the model is bypassing reasoning in favor of memorization. Using Path Patching, Logit Lens, JSD analysis, and Neural Differential Equations, we uncover a hidden Anchor-Adapter circuit that facilitates this shortcut. We localize a Functional Anchor in the middle layers (L18-20) that triggers the retrieval of memorized solutions, followed by Structural Adapters in later layers (L21+) that transform representations to accommodate the shortcut signal. Finally, we demonstrate that scaling specific MLP keys within this circuit allows for bidirectional causal steering-artificially amplifying or suppressing contamination-driven performance. Our results provide a mechanistic roadmap for identifying and mitigating data contamination in RLVR-tuned models. Code is available at https://github.com/idwts/How-RLVR-Activates-Memorization-Shortcuts.
Published: 2026-01-16T07:55:38+00:00
Venue: arXiv
Score: 0.774 (consider)</description><content:encoded>&lt;p&gt;&lt;strong&gt;Authors:&lt;/strong&gt; Lecheng Yan; Ruizhe Li; Guanhua Chen; Qing Li; Jiahui Geng; Wenxi Li; Vincent Wang; Chris Lee&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Journal:&lt;/strong&gt; arXiv&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Relevance:&lt;/strong&gt; 0.774 (consider)&lt;/p&gt;
&lt;p&gt;Reinforcement Learning with Verifiable Rewards (RLVR) is highly effective for enhancing LLM reasoning, yet recent evidence shows models like Qwen 2.5 achieve significant gains even with spurious or incorrect rewards. We investigate this phenomenon and identify a &amp;quot;Perplexity Paradox&amp;quot;: spurious RLVR triggers a divergence where answer-token perplexity drops while prompt-side coherence degrades, suggesting the model is bypassing reasoning in favor of memorization. Using Path Patching, Logit Lens, JSD analysis, and Neural Differential Equations, we uncover a hidden Anchor-Adapter circuit that facilitates this shortcut. We localize a Functional Anchor in the middle layers (L18-20) that triggers the retrieval of memorized solutions, followed by Structural Adapters in later layers (L21+) that transform representations to accommodate the shortcut signal. Finally, we demonstrate that scaling specific MLP keys within this circuit allows for bidirectional causal steering-artificially amplifying or suppressing contamination-driven performance. Our results provide a mechanistic roadmap for identifying and mitigating data contamination in RLVR-tuned models. Code is available at https://github.com/idwts/How-RLVR-Activates-Memorization-Shortcuts.&lt;/p&gt;</content:encoded></item><item><title>Multi-scale steering of large vision language models via visual information intervention</title><link>https://doi.org/10.1016/j.neucom.2026.132780</link><guid>10.1016/j.neucom.2026.132780</guid><pubDate>Mon, 19 Jan 2026 07:24:14 +0000</pubDate><dc:creator>Dongliang Zhao</dc:creator><dc:creator>Bo Sun</dc:creator><dc:creator>Jun He</dc:creator><dc:creator>Yinghui Zhang</dc:creator><prism:publicationName>Neurocomputing</prism:publicationName><prism:doi>10.1016/j.neucom.2026.132780</prism:doi><description>Hallucination poses a challenge to the deployment of large vision-language models in applications. Visual information intervention, as an effective approach for mitigating hallucinations, steers model behavior in the intended direction by enhancing the stability of visual feature representations during inference. However, existing visual information intervention methods typically rely on globally steered single-scale representations and lack local multi-scale visual information. This limitation undermines their ability to mitigate hallucinations caused by representational biases across multi-scales. Therefore, we propose a training-free visual information intervention method based on adaptive fusion of multi-scale visual information. First, we construct a multi-scale pyramid structure to capture visual information at different local scales. Then, an adaptive cosine distance weighted aggregation module is designed to dynamically adjust the steering weights of each scale based on the semantic correlation of visual information across different scales, thereby enabling more accurate retention and fusion of multi-scale visual semantic information. Finally, we leverage the activations from intermediate layers to facilitate semantic decoding, thus alleviating the issue where semantically relevant tokens exhibit peak activations in intermediate layers but fail to manifest in the final output layer. Extensive experiments show that the proposed method can effectively reduce hallucinations and outperform state-of-the-art methods on multiple metrics.
Published: 2026-01-19T07:24:14+00:00
Venue: Neurocomputing
Score: 0.773 (consider)</description><content:encoded>&lt;p&gt;&lt;strong&gt;Authors:&lt;/strong&gt; Dongliang Zhao; Bo Sun; Jun He; Yinghui Zhang&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Journal:&lt;/strong&gt; Neurocomputing&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;a href="https://doi.org/10.1016/j.neucom.2026.132780"&gt;10.1016/j.neucom.2026.132780&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Relevance:&lt;/strong&gt; 0.773 (consider)&lt;/p&gt;
&lt;p&gt;Hallucination poses a challenge to the deployment of large vision-language models in applications. Visual information intervention, as an effective approach for mitigating hallucinations, steers model behavior in the intended direction by enhancing the stability of visual feature representations during inference. However, existing visual information intervention methods typically rely on globally steered single-scale representations and lack local multi-scale visual information. This limitation undermines their ability to mitigate hallucinations caused by representational biases across multi-scales. Therefore, we propose a training-free visual information intervention method based on adaptive fusion of multi-scale visual information. First, we construct a multi-scale pyramid structure to capture visual information at different local scales. Then, an adaptive cosine distance weighted aggregation module is designed to dynamically adjust the steering weights of each scale based on the semantic correlation of visual information across different scales, thereby enabling more accurate retention and fusion of multi-scale visual semantic information. Finally, we leverage the activations from intermediate layers to facilitate semantic decoding, thus alleviating the issue where semantically relevant tokens exhibit peak activations in intermediate layers but fail to manifest in the final output layer. Extensive experiments show that the proposed method can effectively reduce hallucinations and outperform state-of-the-art methods on multiple metrics.&lt;/p&gt;</content:encoded></item><item><title>SAMannot: A Memory-Efficient, Local, Open-source Framework for Interactive Video Instance Segmentation based on SAM2</title><link>https://arxiv.org/abs/2601.11301v1</link><guid>http://arxiv.org/abs/2601.11301v1</guid><pubDate>Fri, 16 Jan 2026 13:55:10 +0000</pubDate><dc:creator>Gergely Dinya</dc:creator><dc:creator>András Gelencsér</dc:creator><dc:creator>Krisztina Kupán</dc:creator><dc:creator>Clemens Küpper</dc:creator><dc:creator>Kristóf Karacs</dc:creator><dc:creator>Anna Gelencsér-Horváth</dc:creator><dc:type>preprint</dc:type><prism:publicationName>arXiv</prism:publicationName><description>Current research workflows for precise video segmentation are often forced into a compromise between labor-intensive manual curation, costly commercial platforms, and/or privacy-compromising cloud-based services. The demand for high-fidelity video instance segmentation in research is often hindered by the bottleneck of manual annotation and the privacy concerns of cloud-based tools. We present SAMannot, an open-source, local framework that integrates the Segment Anything Model 2 (SAM2) into a human-in-the-loop workflow. To address the high resource requirements of foundation models, we modified the SAM2 dependency and implemented a processing layer that minimizes computational overhead and maximizes throughput, ensuring a highly responsive user interface. Key features include persistent instance identity management, an automated ``lock-and-refine'' workflow with barrier frames, and a mask-skeletonization-based auto-prompting mechanism. SAMannot facilitates the generation of research-ready datasets in YOLO and PNG formats alongside structured interaction logs. Verified through animal behavior tracking use-cases and subsets of the LVOS and DAVIS benchmark datasets, the tool provides a scalable, private, and cost-effective alternative to commercial platforms for complex video annotation tasks.
Published: 2026-01-16T13:55:10+00:00
Venue: arXiv
Score: 0.771 (consider)</description><content:encoded>&lt;p&gt;&lt;strong&gt;Authors:&lt;/strong&gt; Gergely Dinya; András Gelencsér; Krisztina Kupán; Clemens Küpper; Kristóf Karacs; Anna Gelencsér-Horváth&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Journal:&lt;/strong&gt; arXiv&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Relevance:&lt;/strong&gt; 0.771 (consider)&lt;/p&gt;
&lt;p&gt;Current research workflows for precise video segmentation are often forced into a compromise between labor-intensive manual curation, costly commercial platforms, and/or privacy-compromising cloud-based services. The demand for high-fidelity video instance segmentation in research is often hindered by the bottleneck of manual annotation and the privacy concerns of cloud-based tools. We present SAMannot, an open-source, local framework that integrates the Segment Anything Model 2 (SAM2) into a human-in-the-loop workflow. To address the high resource requirements of foundation models, we modified the SAM2 dependency and implemented a processing layer that minimizes computational overhead and maximizes throughput, ensuring a highly responsive user interface. Key features include persistent instance identity management, an automated ``lock-and-refine&amp;#x27;&amp;#x27; workflow with barrier frames, and a mask-skeletonization-based auto-prompting mechanism. SAMannot facilitates the generation of research-ready datasets in YOLO and PNG formats alongside structured interaction logs. Verified through animal behavior tracking use-cases and subsets of the LVOS and DAVIS benchmark datasets, the tool provides a scalable, private, and cost-effective alternative to commercial platforms for complex video annotation tasks.&lt;/p&gt;</content:encoded></item><item><title>Think-Clip-Sample: Slow-Fast Frame Selection for Video Understanding</title><link>https://arxiv.org/abs/2601.11359v1</link><guid>http://arxiv.org/abs/2601.11359v1</guid><pubDate>Fri, 16 Jan 2026 15:14:04 +0000</pubDate><dc:creator>Wenhui Tan</dc:creator><dc:creator>Ruihua Song</dc:creator><dc:creator>Jiaze Li</dc:creator><dc:creator>Jianzhong Ju</dc:creator><dc:creator>Zhenbo Luo</dc:creator><dc:type>preprint</dc:type><prism:publicationName>arXiv</prism:publicationName><description>Recent progress in multi-modal large language models (MLLMs) has significantly advanced video understanding. However, their performance on long-form videos remains limited by computational constraints and suboptimal frame selection. We present Think-Clip-Sample (TCS), a training-free framework that enhances long video understanding through two key components: (i) Multi-Query Reasoning, which generates multiple queries to capture complementary aspects of the question and video; and (ii) Clip-level Slow-Fast Sampling, which adaptively balances dense local details and sparse global context. Extensive experiments on MLVU, LongVideoBench, and VideoMME demonstrate that TCS consistently improves performance across different MLLMs, boosting up to 6.9% accuracy, and is capable of achieving comparable accuracy with 50% fewer inference time cost, highlighting both efficiency and efficacy of TCS on long video understanding.
Published: 2026-01-16T15:14:04+00:00
Venue: arXiv
Score: 0.771 (consider)</description><content:encoded>&lt;p&gt;&lt;strong&gt;Authors:&lt;/strong&gt; Wenhui Tan; Ruihua Song; Jiaze Li; Jianzhong Ju; Zhenbo Luo&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Journal:&lt;/strong&gt; arXiv&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Relevance:&lt;/strong&gt; 0.771 (consider)&lt;/p&gt;
&lt;p&gt;Recent progress in multi-modal large language models (MLLMs) has significantly advanced video understanding. However, their performance on long-form videos remains limited by computational constraints and suboptimal frame selection. We present Think-Clip-Sample (TCS), a training-free framework that enhances long video understanding through two key components: (i) Multi-Query Reasoning, which generates multiple queries to capture complementary aspects of the question and video; and (ii) Clip-level Slow-Fast Sampling, which adaptively balances dense local details and sparse global context. Extensive experiments on MLVU, LongVideoBench, and VideoMME demonstrate that TCS consistently improves performance across different MLLMs, boosting up to 6.9% accuracy, and is capable of achieving comparable accuracy with 50% fewer inference time cost, highlighting both efficiency and efficacy of TCS on long video understanding.&lt;/p&gt;</content:encoded></item><item><title>X-Distill: Cross-Architecture Vision Distillation for Visuomotor Learning</title><link>https://arxiv.org/abs/2601.11269v1</link><guid>http://arxiv.org/abs/2601.11269v1</guid><pubDate>Fri, 16 Jan 2026 13:15:55 +0000</pubDate><dc:creator>Maanping Shao</dc:creator><dc:creator>Feihong Zhang</dc:creator><dc:creator>Gu Zhang</dc:creator><dc:creator>Baiye Cheng</dc:creator><dc:creator>Zhengrong Xue</dc:creator><dc:creator>Huazhe Xu</dc:creator><dc:type>preprint</dc:type><prism:publicationName>arXiv</prism:publicationName><description>Visuomotor policies often leverage large pre-trained Vision Transformers (ViTs) for their powerful generalization capabilities. However, their significant data requirements present a major challenge in the data-scarce context of most robotic learning settings, where compact CNNs with strong inductive biases can be more easily optimized. To address this trade-off, we introduce X-Distill, a simple yet highly effective method that synergizes the strengths of both architectures. Our approach involves an offline, cross-architecture knowledge distillation, transferring the rich visual representations of a large, frozen DINOv2 teacher to a compact ResNet-18 student on the general-purpose ImageNet dataset. This distilled encoder, now endowed with powerful visual priors, is then jointly fine-tuned with a diffusion policy head on the target manipulation tasks. Extensive experiments on $34$ simulated benchmarks and $5$ challenging real-world tasks demonstrate that our method consistently outperforms policies equipped with from-scratch ResNet or fine-tuned DINOv2 encoders. Notably, X-Distill also surpasses 3D encoders that utilize privileged point cloud observations or much larger Vision-Language Models. Our work highlights the efficacy of a simple, well-founded distillation strategy for achieving state-of-the-art performance in data-efficient robotic manipulation.
Published: 2026-01-16T13:15:55+00:00
Venue: arXiv
Score: 0.771 (consider)</description><content:encoded>&lt;p&gt;&lt;strong&gt;Authors:&lt;/strong&gt; Maanping Shao; Feihong Zhang; Gu Zhang; Baiye Cheng; Zhengrong Xue; Huazhe Xu&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Journal:&lt;/strong&gt; arXiv&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Relevance:&lt;/strong&gt; 0.771 (consider)&lt;/p&gt;
&lt;p&gt;Visuomotor policies often leverage large pre-trained Vision Transformers (ViTs) for their powerful generalization capabilities. However, their significant data requirements present a major challenge in the data-scarce context of most robotic learning settings, where compact CNNs with strong inductive biases can be more easily optimized. To address this trade-off, we introduce X-Distill, a simple yet highly effective method that synergizes the strengths of both architectures. Our approach involves an offline, cross-architecture knowledge distillation, transferring the rich visual representations of a large, frozen DINOv2 teacher to a compact ResNet-18 student on the general-purpose ImageNet dataset. This distilled encoder, now endowed with powerful visual priors, is then jointly fine-tuned with a diffusion policy head on the target manipulation tasks. Extensive experiments on $34$ simulated benchmarks and $5$ challenging real-world tasks demonstrate that our method consistently outperforms policies equipped with from-scratch ResNet or fine-tuned DINOv2 encoders. Notably, X-Distill also surpasses 3D encoders that utilize privileged point cloud observations or much larger Vision-Language Models. Our work highlights the efficacy of a simple, well-founded distillation strategy for achieving state-of-the-art performance in data-efficient robotic manipulation.&lt;/p&gt;</content:encoded></item><item><title>ReCreate: Reasoning and Creating Domain Agents Driven by Experience</title><link>https://arxiv.org/abs/2601.11100v1</link><guid>http://arxiv.org/abs/2601.11100v1</guid><pubDate>Fri, 16 Jan 2026 09:00:03 +0000</pubDate><dc:creator>Zhezheng Hao</dc:creator><dc:creator>Hong Wang</dc:creator><dc:creator>Jian Luo</dc:creator><dc:creator>Jianqing Zhang</dc:creator><dc:creator>Yuyan Zhou</dc:creator><dc:creator>Qiang Lin</dc:creator><dc:creator>Can Wang</dc:creator><dc:creator>Hande Dong</dc:creator><dc:creator>Jiawei Chen</dc:creator><dc:type>preprint</dc:type><prism:publicationName>arXiv</prism:publicationName><description>Large Language Model agents are reshaping the industrial landscape. However, most practical agents remain human-designed because tasks differ widely, making them labor-intensive to build. This situation poses a central question: can we automatically create and adapt domain agents in the wild? While several recent approaches have sought to automate agent creation, they typically treat agent generation as a black-box procedure and rely solely on final performance metrics to guide the process. Such strategies overlook critical evidence explaining why an agent succeeds or fails, and often require high computational costs. To address these limitations, we propose ReCreate, an experience-driven framework for the automatic creation of domain agents. ReCreate systematically leverages agent interaction histories, which provide rich concrete signals on both the causes of success or failure and the avenues for improvement. Specifically, we introduce an agent-as-optimizer paradigm that effectively learns from experience via three key components: (i) an experience storage and retrieval mechanism for on-demand inspection; (ii) a reasoning-creating synergy pipeline that maps execution experience into scaffold edits; and (iii) hierarchical updates that abstract instance-level details into reusable domain patterns. In experiments across diverse domains, ReCreate consistently outperforms human-designed agents and existing automated agent generation methods, even when starting from minimal seed scaffolds.
Published: 2026-01-16T09:00:03+00:00
Venue: arXiv
Score: 0.770 (consider)</description><content:encoded>&lt;p&gt;&lt;strong&gt;Authors:&lt;/strong&gt; Zhezheng Hao; Hong Wang; Jian Luo; Jianqing Zhang; Yuyan Zhou; Qiang Lin; Can Wang; Hande Dong; Jiawei Chen&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Journal:&lt;/strong&gt; arXiv&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Relevance:&lt;/strong&gt; 0.770 (consider)&lt;/p&gt;
&lt;p&gt;Large Language Model agents are reshaping the industrial landscape. However, most practical agents remain human-designed because tasks differ widely, making them labor-intensive to build. This situation poses a central question: can we automatically create and adapt domain agents in the wild? While several recent approaches have sought to automate agent creation, they typically treat agent generation as a black-box procedure and rely solely on final performance metrics to guide the process. Such strategies overlook critical evidence explaining why an agent succeeds or fails, and often require high computational costs. To address these limitations, we propose ReCreate, an experience-driven framework for the automatic creation of domain agents. ReCreate systematically leverages agent interaction histories, which provide rich concrete signals on both the causes of success or failure and the avenues for improvement. Specifically, we introduce an agent-as-optimizer paradigm that effectively learns from experience via three key components: (i) an experience storage and retrieval mechanism for on-demand inspection; (ii) a reasoning-creating synergy pipeline that maps execution experience into scaffold edits; and (iii) hierarchical updates that abstract instance-level details into reusable domain patterns. In experiments across diverse domains, ReCreate consistently outperforms human-designed agents and existing automated agent generation methods, even when starting from minimal seed scaffolds.&lt;/p&gt;</content:encoded></item><item><title>What Matters in Data Curation for Multimodal Reasoning? Insights from the DCVLR Challenge</title><link>https://arxiv.org/abs/2601.10922v1</link><guid>http://arxiv.org/abs/2601.10922v1</guid><pubDate>Fri, 16 Jan 2026 00:50:01 +0000</pubDate><dc:creator>Yosub Shin</dc:creator><dc:creator>Michael Buriek</dc:creator><dc:creator>Boris Sobolev</dc:creator><dc:creator>Pavel Bushuyeu</dc:creator><dc:creator>Vikas Kumar</dc:creator><dc:creator>Haoyang Xu</dc:creator><dc:creator>Samuel Watson</dc:creator><dc:creator>Igor Molybog</dc:creator><dc:type>preprint</dc:type><prism:publicationName>arXiv</prism:publicationName><description>We study data curation for multimodal reasoning through the NeurIPS 2025 Data Curation for Vision-Language Reasoning (DCVLR) challenge, which isolates dataset selection by fixing the model and training protocol. Using a compact curated dataset derived primarily from Walton Multimodal Cold Start, our submission placed first in the challenge. Through post-competition ablations, we show that difficulty-based example selection on an aligned base dataset is the dominant driver of performance gains. Increasing dataset size does not reliably improve mean accuracy under the fixed training recipe, but mainly reduces run-to-run variance, while commonly used diversity and synthetic augmentation heuristics provide no additional benefit and often degrade performance. These results characterize DCVLR as a saturation-regime evaluation and highlight the central role of alignment and difficulty in data-efficient multimodal reasoning.
Published: 2026-01-16T00:50:01+00:00
Venue: arXiv
Score: 0.765 (consider)</description><content:encoded>&lt;p&gt;&lt;strong&gt;Authors:&lt;/strong&gt; Yosub Shin; Michael Buriek; Boris Sobolev; Pavel Bushuyeu; Vikas Kumar; Haoyang Xu; Samuel Watson; Igor Molybog&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Journal:&lt;/strong&gt; arXiv&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Relevance:&lt;/strong&gt; 0.765 (consider)&lt;/p&gt;
&lt;p&gt;We study data curation for multimodal reasoning through the NeurIPS 2025 Data Curation for Vision-Language Reasoning (DCVLR) challenge, which isolates dataset selection by fixing the model and training protocol. Using a compact curated dataset derived primarily from Walton Multimodal Cold Start, our submission placed first in the challenge. Through post-competition ablations, we show that difficulty-based example selection on an aligned base dataset is the dominant driver of performance gains. Increasing dataset size does not reliably improve mean accuracy under the fixed training recipe, but mainly reduces run-to-run variance, while commonly used diversity and synthetic augmentation heuristics provide no additional benefit and often degrade performance. These results characterize DCVLR as a saturation-regime evaluation and highlight the central role of alignment and difficulty in data-efficient multimodal reasoning.&lt;/p&gt;</content:encoded></item><item><title>FORESTLLM: Large Language Models Make Random Forest Great on Few-shot Tabular Learning</title><link>https://arxiv.org/abs/2601.11311v1</link><guid>http://arxiv.org/abs/2601.11311v1</guid><pubDate>Fri, 16 Jan 2026 14:08:51 +0000</pubDate><dc:creator>Zhihan Yang</dc:creator><dc:creator>Jiaqi Wei</dc:creator><dc:creator>Xiang Zhang</dc:creator><dc:creator>Haoyu Dong</dc:creator><dc:creator>Yiwen Wang</dc:creator><dc:creator>Xiaoke Guo</dc:creator><dc:creator>Pengkun Zhang</dc:creator><dc:creator>Yiwei Xu</dc:creator><dc:creator>Chenyu You</dc:creator><dc:type>preprint</dc:type><prism:publicationName>arXiv</prism:publicationName><description>Tabular data high-stakes critical decision-making in domains such as finance, healthcare, and scientific discovery. Yet, learning effectively from tabular data in few-shot settings, where labeled examples are scarce, remains a fundamental challenge. Traditional tree-based methods often falter in these regimes due to their reliance on statistical purity metrics, which become unstable and prone to overfitting with limited supervision. At the same time, direct applications of large language models (LLMs) often overlook its inherent structure, leading to suboptimal performance. To overcome these limitations, we propose FORESTLLM, a novel framework that unifies the structural inductive biases of decision forests with the semantic reasoning capabilities of LLMs. Crucially, FORESTLLM leverages the LLM only during training, treating it as an offline model designer that encodes rich, contextual knowledge into a lightweight, interpretable forest model, eliminating the need for LLM inference at test time. Our method is two-fold. First, we introduce a semantic splitting criterion in which the LLM evaluates candidate partitions based on their coherence over both labeled and unlabeled data, enabling the induction of more robust and generalizable tree structures under few-shot supervision. Second, we propose a one-time in-context inference mechanism for leaf node stabilization, where the LLM distills the decision path and its supporting examples into a concise, deterministic prediction, replacing noisy empirical estimates with semantically informed outputs. Across a diverse suite of few-shot classification and regression benchmarks, FORESTLLM achieves state-of-the-art performance.
Published: 2026-01-16T14:08:51+00:00
Venue: arXiv
Score: 0.765 (consider)</description><content:encoded>&lt;p&gt;&lt;strong&gt;Authors:&lt;/strong&gt; Zhihan Yang; Jiaqi Wei; Xiang Zhang; Haoyu Dong; Yiwen Wang; Xiaoke Guo; Pengkun Zhang; Yiwei Xu; Chenyu You&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Journal:&lt;/strong&gt; arXiv&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Relevance:&lt;/strong&gt; 0.765 (consider)&lt;/p&gt;
&lt;p&gt;Tabular data high-stakes critical decision-making in domains such as finance, healthcare, and scientific discovery. Yet, learning effectively from tabular data in few-shot settings, where labeled examples are scarce, remains a fundamental challenge. Traditional tree-based methods often falter in these regimes due to their reliance on statistical purity metrics, which become unstable and prone to overfitting with limited supervision. At the same time, direct applications of large language models (LLMs) often overlook its inherent structure, leading to suboptimal performance. To overcome these limitations, we propose FORESTLLM, a novel framework that unifies the structural inductive biases of decision forests with the semantic reasoning capabilities of LLMs. Crucially, FORESTLLM leverages the LLM only during training, treating it as an offline model designer that encodes rich, contextual knowledge into a lightweight, interpretable forest model, eliminating the need for LLM inference at test time. Our method is two-fold. First, we introduce a semantic splitting criterion in which the LLM evaluates candidate partitions based on their coherence over both labeled and unlabeled data, enabling the induction of more robust and generalizable tree structures under few-shot supervision. Second, we propose a one-time in-context inference mechanism for leaf node stabilization, where the LLM distills the decision path and its supporting examples into a concise, deterministic prediction, replacing noisy empirical estimates with semantically informed outputs. Across a diverse suite of few-shot classification and regression benchmarks, FORESTLLM achieves state-of-the-art performance.&lt;/p&gt;</content:encoded></item><item><title>Sparse Data Tree Canopy Segmentation: Fine-Tuning Leading Pretrained Models on Only 150 Images</title><link>https://arxiv.org/abs/2601.10931v1</link><guid>http://arxiv.org/abs/2601.10931v1</guid><pubDate>Fri, 16 Jan 2026 01:20:32 +0000</pubDate><dc:creator>David Szczecina</dc:creator><dc:creator>Hudson Sun</dc:creator><dc:creator>Anthony Bertnyk</dc:creator><dc:creator>Niloofar Azad</dc:creator><dc:creator>Kyle Gao</dc:creator><dc:creator>Lincoln Linlin Xu</dc:creator><dc:type>preprint</dc:type><prism:publicationName>arXiv</prism:publicationName><description>Tree canopy detection from aerial imagery is an important task for environmental monitoring, urban planning, and ecosystem analysis. Simulating real-life data annotation scarcity, the Solafune Tree Canopy Detection competition provides a small and imbalanced dataset of only 150 annotated images, posing significant challenges for training deep models without severe overfitting. In this work, we evaluate five representative architectures, YOLOv11, Mask R-CNN, DeepLabv3, Swin-UNet, and DINOv2, to assess their suitability for canopy segmentation under extreme data scarcity. Our experiments show that pretrained convolution-based models, particularly YOLOv11 and Mask R-CNN, generalize significantly better than pretrained transformer-based models. DeeplabV3, Swin-UNet and DINOv2 underperform likely due to differences between semantic and instance segmentation tasks, the high data requirements of Vision Transformers, and the lack of strong inductive biases. These findings confirm that transformer-based architectures struggle in low-data regimes without substantial pretraining or augmentation and that differences between semantic and instance segmentation further affect model performance. We provide a detailed analysis of training strategies, augmentation policies, and model behavior under the small-data constraint and demonstrate that lightweight CNN-based methods remain the most reliable for canopy detection on limited imagery.
Published: 2026-01-16T01:20:32+00:00
Venue: arXiv
Score: 0.763 (consider)</description><content:encoded>&lt;p&gt;&lt;strong&gt;Authors:&lt;/strong&gt; David Szczecina; Hudson Sun; Anthony Bertnyk; Niloofar Azad; Kyle Gao; Lincoln Linlin Xu&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Journal:&lt;/strong&gt; arXiv&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Relevance:&lt;/strong&gt; 0.763 (consider)&lt;/p&gt;
&lt;p&gt;Tree canopy detection from aerial imagery is an important task for environmental monitoring, urban planning, and ecosystem analysis. Simulating real-life data annotation scarcity, the Solafune Tree Canopy Detection competition provides a small and imbalanced dataset of only 150 annotated images, posing significant challenges for training deep models without severe overfitting. In this work, we evaluate five representative architectures, YOLOv11, Mask R-CNN, DeepLabv3, Swin-UNet, and DINOv2, to assess their suitability for canopy segmentation under extreme data scarcity. Our experiments show that pretrained convolution-based models, particularly YOLOv11 and Mask R-CNN, generalize significantly better than pretrained transformer-based models. DeeplabV3, Swin-UNet and DINOv2 underperform likely due to differences between semantic and instance segmentation tasks, the high data requirements of Vision Transformers, and the lack of strong inductive biases. These findings confirm that transformer-based architectures struggle in low-data regimes without substantial pretraining or augmentation and that differences between semantic and instance segmentation further affect model performance. We provide a detailed analysis of training strategies, augmentation policies, and model behavior under the small-data constraint and demonstrate that lightweight CNN-based methods remain the most reliable for canopy detection on limited imagery.&lt;/p&gt;</content:encoded></item><item><title>TayMAML: A meta reinforcement learning-based task scheduling method for edge computing</title><link>https://doi.org/10.1016/j.eswa.2026.131253</link><guid>10.1016/j.eswa.2026.131253</guid><pubDate>Sun, 18 Jan 2026 22:49:38 +0000</pubDate><dc:creator>Tao Ju</dc:creator><dc:creator>Zhiqing Wang</dc:creator><dc:creator>Heting Kang</dc:creator><dc:creator>Jiuyuan Huo</dc:creator><dc:creator>Tao Gu</dc:creator><prism:publicationName>Expert Systems with Applications</prism:publicationName><prism:doi>10.1016/j.eswa.2026.131253</prism:doi><description>This paper presents TayMAML, an edge computing task scheduling algorithm designed to address the challenges of suboptimal generalization and the trade-off between computational efficiency and accuracy in traditional meta-reinforcement learning algorithms within dynamically heterogeneous edge environments. To enhance task scheduling performance, we first propose a biased sampling strategy that evaluates task learning progress based on training loss. This strategy determines the number of test samples for various tasks, ensuring consistency between training and testing task distributions. Additionally, a lightweight distribution consistency strategy is introduced to further reduce disparities between training and testing distributions. This approach quantifies distribution differences and incorporates these differentials into the original meta-loss for meta-updates. Through theoretical derivation, we isolate the second-order derivative term in the meta-update process. Leveraging Taylor expansion, we derive a first-order approximation of the second-order derivative, enabling precise parameter updates while avoiding the computational overhead typically associated with second-order derivatives in meta-reinforcement learning. Experimental evaluations demonstrate that TayMAML significantly improves model generalization and stability, reduces system latency and energy consumption, and effectively supports real-time task requirements in dynamically heterogeneous edge environments, outperforming existing state-of-the-art algorithms.
Published: 2026-01-18T22:49:38+00:00
Venue: Expert Systems with Applications
Score: 0.763 (consider)</description><content:encoded>&lt;p&gt;&lt;strong&gt;Authors:&lt;/strong&gt; Tao Ju; Zhiqing Wang; Heting Kang; Jiuyuan Huo; Tao Gu&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Journal:&lt;/strong&gt; Expert Systems with Applications&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;a href="https://doi.org/10.1016/j.eswa.2026.131253"&gt;10.1016/j.eswa.2026.131253&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Relevance:&lt;/strong&gt; 0.763 (consider)&lt;/p&gt;
&lt;p&gt;This paper presents TayMAML, an edge computing task scheduling algorithm designed to address the challenges of suboptimal generalization and the trade-off between computational efficiency and accuracy in traditional meta-reinforcement learning algorithms within dynamically heterogeneous edge environments. To enhance task scheduling performance, we first propose a biased sampling strategy that evaluates task learning progress based on training loss. This strategy determines the number of test samples for various tasks, ensuring consistency between training and testing task distributions. Additionally, a lightweight distribution consistency strategy is introduced to further reduce disparities between training and testing distributions. This approach quantifies distribution differences and incorporates these differentials into the original meta-loss for meta-updates. Through theoretical derivation, we isolate the second-order derivative term in the meta-update process. Leveraging Taylor expansion, we derive a first-order approximation of the second-order derivative, enabling precise parameter updates while avoiding the computational overhead typically associated with second-order derivatives in meta-reinforcement learning. Experimental evaluations demonstrate that TayMAML significantly improves model generalization and stability, reduces system latency and energy consumption, and effectively supports real-time task requirements in dynamically heterogeneous edge environments, outperforming existing state-of-the-art algorithms.&lt;/p&gt;</content:encoded></item><item><title>SME-YOLO: A Real-Time Detector for Tiny Defect Detection on PCB Surfaces</title><link>https://arxiv.org/abs/2601.11402v1</link><guid>http://arxiv.org/abs/2601.11402v1</guid><pubDate>Fri, 16 Jan 2026 16:14:56 +0000</pubDate><dc:creator>Meng Han</dc:creator><dc:type>preprint</dc:type><prism:publicationName>arXiv</prism:publicationName><description>Surface defects on Printed Circuit Boards (PCBs) directly compromise product reliability and safety. However, achieving high-precision detection is challenging because PCB defects are typically characterized by tiny sizes, high texture similarity, and uneven scale distributions. To address these challenges, this paper proposes a novel framework based on YOLOv11n, named SME-YOLO (Small-target Multi-scale Enhanced YOLO). First, we employ the Normalized Wasserstein Distance Loss (NWDLoss). This metric effectively mitigates the sensitivity of Intersection over Union (IoU) to positional deviations in tiny objects. Second, the original upsampling module is replaced by the Efficient Upsampling Convolution Block (EUCB). By utilizing multi-scale convolutions, the EUCB gradually recovers spatial resolution and enhances the preservation of edge and texture details for tiny defects. Finally, this paper proposes the Multi-Scale Focused Attention (MSFA) module. Tailored to the specific spatial distribution of PCB defects, this module adaptively strengthens perception within key scale intervals, achieving efficient fusion of local fine-grained features and global context information. Experimental results on the PKU-PCB dataset demonstrate that SME-YOLO achieves state-of-the-art performance. Specifically, compared to the baseline YOLOv11n, SME-YOLO improves mAP by 2.2% and Precision by 4%, validating the effectiveness of the proposed method.
Published: 2026-01-16T16:14:56+00:00
Venue: arXiv
Score: 0.763 (consider)</description><content:encoded>&lt;p&gt;&lt;strong&gt;Authors:&lt;/strong&gt; Meng Han&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Journal:&lt;/strong&gt; arXiv&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Relevance:&lt;/strong&gt; 0.763 (consider)&lt;/p&gt;
&lt;p&gt;Surface defects on Printed Circuit Boards (PCBs) directly compromise product reliability and safety. However, achieving high-precision detection is challenging because PCB defects are typically characterized by tiny sizes, high texture similarity, and uneven scale distributions. To address these challenges, this paper proposes a novel framework based on YOLOv11n, named SME-YOLO (Small-target Multi-scale Enhanced YOLO). First, we employ the Normalized Wasserstein Distance Loss (NWDLoss). This metric effectively mitigates the sensitivity of Intersection over Union (IoU) to positional deviations in tiny objects. Second, the original upsampling module is replaced by the Efficient Upsampling Convolution Block (EUCB). By utilizing multi-scale convolutions, the EUCB gradually recovers spatial resolution and enhances the preservation of edge and texture details for tiny defects. Finally, this paper proposes the Multi-Scale Focused Attention (MSFA) module. Tailored to the specific spatial distribution of PCB defects, this module adaptively strengthens perception within key scale intervals, achieving efficient fusion of local fine-grained features and global context information. Experimental results on the PKU-PCB dataset demonstrate that SME-YOLO achieves state-of-the-art performance. Specifically, compared to the baseline YOLOv11n, SME-YOLO improves mAP by 2.2% and Precision by 4%, validating the effectiveness of the proposed method.&lt;/p&gt;</content:encoded></item><item><title>Multi-Source Temporal-Depth Fusion for Robust End-to-End Visual Odometry</title><link>https://doi.org/10.1016/j.neunet.2026.108598</link><guid>10.1016/j.neunet.2026.108598</guid><pubDate>Sun, 18 Jan 2026 02:47:54 +0000</pubDate><dc:creator>Sihang Zhang</dc:creator><dc:creator>Congqi Cao</dc:creator><dc:creator>Qiang Gao</dc:creator><dc:creator>Ganchao Liu</dc:creator><prism:publicationName>Neural Networks</prism:publicationName><prism:doi>10.1016/j.neunet.2026.108598</prism:doi><description>End-to-end visual odometry models have recently achieved localization accuracy on par with conventional techniques, while effectively reducing the occurrence of catastrophic failures. However, the relevant models cannot leverage the complete time-series data for pose adjustment and optimization. Moreover, these models are limited to using joint depth prediction tasks merely as a means of scale constraint, lacking effective utilization of depth information. In this paper, we propose an end-to-end multi-source visual odometry (MVO) model that dynamically integrates the key components of hybrid visual odometry pipelines into a unified, learnable deep framework. Specifically, we propose TimePoseNet to model the mapping relationship from time to pose, capturing temporal dependencies across the entire sequence. Additionally, a wavelet convolutional attention mechanism is employed to extract global depth information from the depth map, which is then directly embedded into the pose features to dynamically constrain scale ambiguity. Furthermore, temporal and depth cues are jointly incorporated into the post-processing stage of pose estimation. The proposed method attains state-of-the-art performance on both the KITTI benchmark and the newly introduced UAV-2025 dataset, while preserving computational efficiency during inference.
Published: 2026-01-18T02:47:54+00:00
Venue: Neural Networks
Score: 0.762 (consider)</description><content:encoded>&lt;p&gt;&lt;strong&gt;Authors:&lt;/strong&gt; Sihang Zhang; Congqi Cao; Qiang Gao; Ganchao Liu&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Journal:&lt;/strong&gt; Neural Networks&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;a href="https://doi.org/10.1016/j.neunet.2026.108598"&gt;10.1016/j.neunet.2026.108598&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Relevance:&lt;/strong&gt; 0.762 (consider)&lt;/p&gt;
&lt;p&gt;End-to-end visual odometry models have recently achieved localization accuracy on par with conventional techniques, while effectively reducing the occurrence of catastrophic failures. However, the relevant models cannot leverage the complete time-series data for pose adjustment and optimization. Moreover, these models are limited to using joint depth prediction tasks merely as a means of scale constraint, lacking effective utilization of depth information. In this paper, we propose an end-to-end multi-source visual odometry (MVO) model that dynamically integrates the key components of hybrid visual odometry pipelines into a unified, learnable deep framework. Specifically, we propose TimePoseNet to model the mapping relationship from time to pose, capturing temporal dependencies across the entire sequence. Additionally, a wavelet convolutional attention mechanism is employed to extract global depth information from the depth map, which is then directly embedded into the pose features to dynamically constrain scale ambiguity. Furthermore, temporal and depth cues are jointly incorporated into the post-processing stage of pose estimation. The proposed method attains state-of-the-art performance on both the KITTI benchmark and the newly introduced UAV-2025 dataset, while preserving computational efficiency during inference.&lt;/p&gt;</content:encoded></item><item><title>人工智能生成图像检测技术综述</title><link>https://doi.org/10.11834/jig.250053</link><guid>10.11834/jig.250053</guid><pubDate>Mon, 19 Jan 2026 01:26:10 +0000</pubDate><dc:creator>Li Meiling</dc:creator><dc:creator>Qian Zhenxing</dc:creator><dc:creator>Zhang Xinpeng</dc:creator><prism:publicationName>Journal of Image and Graphics</prism:publicationName><prism:doi>10.11834/jig.250053</prism:doi><description>人工智能生成图像检测旨在判断图像是否由生成模型生成，是人工智能安全与治理领域一个重要研究方向。然而当前生成图像检测方法因生成模型结构的多样性、生成图像的复杂性以及对生成图像后处理操作的不确定性难以实现高效和鲁棒检测。早期的检测方法重点关注对生成对抗网络生成内容的检测。近年来，扩散模型生成图像的检测受到广泛关注，与之相关的检测方法涌现，并表现出优越性能。因此，首先对近年来的主流图像生成模型进行梳理，然后从监督范式、学习方式、检测依据、骨干网络、技术手段和可解释性多种维度对现有生成图像检测方法进行分类，并以检测依据（像素域特征、频域特征、预训练模型特征、融合特征和特定规则）作为主要划分标准，对各类研究工作的基本思想与特点进行详细阐述与综合分析。此外，列举了当前用于通用生成图像检测的基准数据集，从数据集结构和规模等方面进行综合比较，并对面向检测方法的综合评测基准进行汇总。关于评估维度，从域内准确性、域外泛化性和鲁棒性3个层面进行介绍。之后，对代表性检测方法进行横向比较，就检测结果进行分析。最后，对当前生成图像检测领域待解决的问题进行总结，并对未来的研究方向进行展望。
Published: 2026-01-19T01:26:10+00:00
Venue: Journal of Image and Graphics
Score: 0.762 (consider)</description><content:encoded>&lt;p&gt;&lt;strong&gt;Authors:&lt;/strong&gt; Li Meiling; Qian Zhenxing; Zhang Xinpeng&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Journal:&lt;/strong&gt; Journal of Image and Graphics&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;a href="https://doi.org/10.11834/jig.250053"&gt;10.11834/jig.250053&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Relevance:&lt;/strong&gt; 0.762 (consider)&lt;/p&gt;
&lt;p&gt;人工智能生成图像检测旨在判断图像是否由生成模型生成，是人工智能安全与治理领域一个重要研究方向。然而当前生成图像检测方法因生成模型结构的多样性、生成图像的复杂性以及对生成图像后处理操作的不确定性难以实现高效和鲁棒检测。早期的检测方法重点关注对生成对抗网络生成内容的检测。近年来，扩散模型生成图像的检测受到广泛关注，与之相关的检测方法涌现，并表现出优越性能。因此，首先对近年来的主流图像生成模型进行梳理，然后从监督范式、学习方式、检测依据、骨干网络、技术手段和可解释性多种维度对现有生成图像检测方法进行分类，并以检测依据（像素域特征、频域特征、预训练模型特征、融合特征和特定规则）作为主要划分标准，对各类研究工作的基本思想与特点进行详细阐述与综合分析。此外，列举了当前用于通用生成图像检测的基准数据集，从数据集结构和规模等方面进行综合比较，并对面向检测方法的综合评测基准进行汇总。关于评估维度，从域内准确性、域外泛化性和鲁棒性3个层面进行介绍。之后，对代表性检测方法进行横向比较，就检测结果进行分析。最后，对当前生成图像检测领域待解决的问题进行总结，并对未来的研究方向进行展望。&lt;/p&gt;</content:encoded></item></channel></rss>