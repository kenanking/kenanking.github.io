<?xml version='1.0' encoding='utf-8'?>
<rss xmlns:content="http://purl.org/rss/1.0/modules/content/" xmlns:dc="http://purl.org/dc/elements/1.1/" xmlns:prism="http://prismstandard.org/namespaces/basic/3.0/" version="2.0"><channel><title>ZotWatch Feed</title><link>https://ehehe.cn/zotwatch/</link><description>AI-assisted literature watch</description><lastBuildDate>Fri, 13 Feb 2026 03:43:16 +0000</lastBuildDate><item><title>跨尺度自适应频域增强的海上船舶检测</title><link>https://doi.org/10.11834/jig.250548</link><guid>10.11834/jig.250548</guid><pubDate>Wed, 11 Feb 2026 07:27:49 +0000</pubDate><dc:creator>Wang Yingjun</dc:creator><dc:creator>Yang Xiaopeng</dc:creator><dc:creator>Zhou Ling</dc:creator><dc:creator>Lu Haoxiang</dc:creator><dc:creator>Zhao Wenyi</dc:creator><dc:creator>Zhang Weidong</dc:creator><prism:publicationName>Journal of Image and Graphics</prism:publicationName><prism:doi>10.11834/jig.250548</prism:doi><description>目的海上船舶目标检测对海域管理和交通安全至关重要，但受复杂环境影响，常出现遮挡、模糊和细节丢失等问题，现有方法检测精度不足、易误检漏检，难以满足船舶精确识别需求。基于此，本文提出一种跨尺度自适应频域增强的海上船舶检测方法。方法以YOLO11为基线模型进行针对性改进，首先，设计了一个自适应频域特征增强模块（Adaptive Frequency-domain Feature Enhancement Module， AFEM）用于海上船舶细节特征的增强。该模块针对不同尺度的特征信息，采用傅里叶变换将特征信息转换到频域，通过门控单元对全局和局部信息进行自适应增强，全面增强网络对海上退化特征的提取能力。其次，在颈部引入一个多尺度特征感知模块（Multi-scale Feature Perception Module， MFP）。使用不同的卷积核捕获多尺度特征，高效挖掘并利用海上船舶图像的上下文特征信息，引导网络精准聚焦船舶目标特征，有效抑制复杂背景与遮挡带来的干扰，缓解小目标船舶的特征丢失现象，显著降低海上船舶检测的错检与漏检率。结果在MVDD（Marine Vessel Detection Dataset）和RTTS（Real-world Task-Driven Testing Set）数据集上的平均精确度（mean Average Precision at 50% IOU， mAP50）分别达到95.18％和74.79％，对13类船舶的检测表现优异，尤其在小目标、遮挡船舶检测中优势显著。同时，参数量仅有6.29M，推理速度达到227 FPS（Frames Per Second）。通过与目前最先进的16种不同类型方法的比较，本文提出的方法检测性能更优，在检测精度和模型复杂度之间实现了更好的平衡。结论本文所提方法不仅在海上表现出色，对于陆地的恶劣天气条件也有较强的适应能力，展现出较好的鲁棒性和泛化性，同时具备较高的可部署性和实际应用价值。
Published: 2026-02-11T07:27:49+00:00
Venue: Journal of Image and Graphics
Score: 0.849 (must_read)</description><content:encoded>&lt;p&gt;&lt;strong&gt;Authors:&lt;/strong&gt; Wang Yingjun; Yang Xiaopeng; Zhou Ling; Lu Haoxiang; Zhao Wenyi; Zhang Weidong&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Journal:&lt;/strong&gt; Journal of Image and Graphics&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;a href="https://doi.org/10.11834/jig.250548"&gt;10.11834/jig.250548&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Relevance:&lt;/strong&gt; 0.849 (must_read)&lt;/p&gt;
&lt;p&gt;目的海上船舶目标检测对海域管理和交通安全至关重要，但受复杂环境影响，常出现遮挡、模糊和细节丢失等问题，现有方法检测精度不足、易误检漏检，难以满足船舶精确识别需求。基于此，本文提出一种跨尺度自适应频域增强的海上船舶检测方法。方法以YOLO11为基线模型进行针对性改进，首先，设计了一个自适应频域特征增强模块（Adaptive Frequency-domain Feature Enhancement Module， AFEM）用于海上船舶细节特征的增强。该模块针对不同尺度的特征信息，采用傅里叶变换将特征信息转换到频域，通过门控单元对全局和局部信息进行自适应增强，全面增强网络对海上退化特征的提取能力。其次，在颈部引入一个多尺度特征感知模块（Multi-scale Feature Perception Module， MFP）。使用不同的卷积核捕获多尺度特征，高效挖掘并利用海上船舶图像的上下文特征信息，引导网络精准聚焦船舶目标特征，有效抑制复杂背景与遮挡带来的干扰，缓解小目标船舶的特征丢失现象，显著降低海上船舶检测的错检与漏检率。结果在MVDD（Marine Vessel Detection Dataset）和RTTS（Real-world Task-Driven Testing Set）数据集上的平均精确度（mean Average Precision at 50% IOU， mAP50）分别达到95.18％和74.79％，对13类船舶的检测表现优异，尤其在小目标、遮挡船舶检测中优势显著。同时，参数量仅有6.29M，推理速度达到227 FPS（Frames Per Second）。通过与目前最先进的16种不同类型方法的比较，本文提出的方法检测性能更优，在检测精度和模型复杂度之间实现了更好的平衡。结论本文所提方法不仅在海上表现出色，对于陆地的恶劣天气条件也有较强的适应能力，展现出较好的鲁棒性和泛化性，同时具备较高的可部署性和实际应用价值。&lt;/p&gt;</content:encoded></item><item><title>Boosting Learning Efficiency in Few-Shot Tasks With Layer-Adaptive PID Control</title><link>https://doi.org/10.1109/tpami.2026.3663608</link><guid>10.1109/tpami.2026.3663608</guid><pubDate>Wed, 11 Feb 2026 20:55:51 +0000</pubDate><dc:creator>Pengfei Zhang</dc:creator><dc:creator>Xinde Li</dc:creator><dc:creator>Le Yu</dc:creator><dc:creator>Zhentong Zhang</dc:creator><dc:creator>Fir Dunkin</dc:creator><dc:creator>Huaping Liu</dc:creator><dc:creator>Zhijun Li</dc:creator><prism:publicationName>IEEE Transactions on Pattern Analysis and Machine Intelligence</prism:publicationName><prism:doi>10.1109/tpami.2026.3663608</prism:doi><description>Few-shot learning seeks to recognize novel classes from limited examples. Model-agnostic meta-learning (MAML), known for its simplicity and flexibility, learns an effective initialization for fast adaptation in data-scarce settings. However, MAML-based methods face challenges when there is a significant distributional shift between training and testing tasks, leading to inefficient learning and poor generalization across domains. In this work, we identify the core issues: inflexible weight update rules and limited adaptive learning capabilities. Instead of focusing solely on better initialization, we aim to enhance the adaptation process. Consequently, we propose a novel Layer-Adaptive Proportional-Integral-Derivative (LA-PID) optimizer integrated into a meta-learning framework. This design incorporates classical control theory, utilizing PID control to dynamically adjust task-specific gains at each network layer. Additionally, the theoretical conditions for optimal hyperparameter initialization and global model convergence are addressed from both control and optimization perspectives. Experiments on benchmark datasets show that LA-PID achieves state-of-the-art performance in few-shot classification, cross-domain, and regression tasks, while requiring fewer training steps.
Published: 2026-02-11T20:55:51+00:00
Venue: IEEE Transactions on Pattern Analysis and Machine Intelligence
Score: 0.847 (must_read)</description><content:encoded>&lt;p&gt;&lt;strong&gt;Authors:&lt;/strong&gt; Pengfei Zhang; Xinde Li; Le Yu; Zhentong Zhang; Fir Dunkin; Huaping Liu; Zhijun Li&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Journal:&lt;/strong&gt; IEEE Transactions on Pattern Analysis and Machine Intelligence&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;a href="https://doi.org/10.1109/tpami.2026.3663608"&gt;10.1109/tpami.2026.3663608&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Relevance:&lt;/strong&gt; 0.847 (must_read)&lt;/p&gt;
&lt;p&gt;Few-shot learning seeks to recognize novel classes from limited examples. Model-agnostic meta-learning (MAML), known for its simplicity and flexibility, learns an effective initialization for fast adaptation in data-scarce settings. However, MAML-based methods face challenges when there is a significant distributional shift between training and testing tasks, leading to inefficient learning and poor generalization across domains. In this work, we identify the core issues: inflexible weight update rules and limited adaptive learning capabilities. Instead of focusing solely on better initialization, we aim to enhance the adaptation process. Consequently, we propose a novel Layer-Adaptive Proportional-Integral-Derivative (LA-PID) optimizer integrated into a meta-learning framework. This design incorporates classical control theory, utilizing PID control to dynamically adjust task-specific gains at each network layer. Additionally, the theoretical conditions for optimal hyperparameter initialization and global model convergence are addressed from both control and optimization perspectives. Experiments on benchmark datasets show that LA-PID achieves state-of-the-art performance in few-shot classification, cross-domain, and regression tasks, while requiring fewer training steps.&lt;/p&gt;</content:encoded></item><item><title>FGOM-RTDETR: Far-Shore Guided Object-Focusing Multiscale Network with Real-Time Detection Transformer for Infrared Ship Target Detection</title><link>https://doi.org/10.1109/tgrs.2026.3663601</link><guid>10.1109/tgrs.2026.3663601</guid><pubDate>Wed, 11 Feb 2026 20:55:56 +0000</pubDate><dc:creator>Haobin Wang</dc:creator><dc:creator>Bo-Hui Tang</dc:creator><dc:creator>Fangliang Cai</dc:creator><dc:creator>Menghua Li</dc:creator><dc:creator>Zheng Zhang</dc:creator><dc:creator>Dong Fan</dc:creator><prism:publicationName>IEEE Transactions on Geoscience and Remote Sensing</prism:publicationName><prism:doi>10.1109/tgrs.2026.3663601</prism:doi><description>Infrared ship detection plays a critical role in both civilian and military applications, including tracking, collision avoidance, and maritime security. However, challenges such as low image resolution and complex backgrounds hinder detection accuracy. This study proposes a novel detection algorithm, Far-shore Guided Object-focusing Multiscale Network with Real-Time Detection Transformer (FGOM-RTDETR), which integrates multi-scale local and global features. Built upon the RT-DETR framework, our method introduces a Feature Grouping Module (FGOM) to enhance multi-scale representation. FGOM consists of three key components: the Feature Reparameterization Module (FRep), the Coordinate Attention Golden Feature Pyramid Network (CAGoldFPN), and the Multi-Scale Stacked Network (MuSSNet). The FRep module addresses the loss of channel information caused by the small size of thermal infrared ship targets and the complexity of background features. The CAGoldFPN module improves multi-scale feature fusion, while MuSSNet mitigates issues of high target similarity and the susceptibility of small targets to being overlooked. Experimental results show that, compared with the baseline RT-DETR model, FGOM-RTDETR achieves notable performance gains: precision improves from 0.921 to 0.942, mAP50 rises from 0.938 to 0.958, and recall increases from 0.923 to 0.934. These results demonstrate that FGOM-RTDETR delivers superior detection performance for infrared ship targets.
Published: 2026-02-11T20:55:56+00:00
Venue: IEEE Transactions on Geoscience and Remote Sensing
Score: 0.839 (must_read)</description><content:encoded>&lt;p&gt;&lt;strong&gt;Authors:&lt;/strong&gt; Haobin Wang; Bo-Hui Tang; Fangliang Cai; Menghua Li; Zheng Zhang; Dong Fan&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Journal:&lt;/strong&gt; IEEE Transactions on Geoscience and Remote Sensing&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;a href="https://doi.org/10.1109/tgrs.2026.3663601"&gt;10.1109/tgrs.2026.3663601&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Relevance:&lt;/strong&gt; 0.839 (must_read)&lt;/p&gt;
&lt;p&gt;Infrared ship detection plays a critical role in both civilian and military applications, including tracking, collision avoidance, and maritime security. However, challenges such as low image resolution and complex backgrounds hinder detection accuracy. This study proposes a novel detection algorithm, Far-shore Guided Object-focusing Multiscale Network with Real-Time Detection Transformer (FGOM-RTDETR), which integrates multi-scale local and global features. Built upon the RT-DETR framework, our method introduces a Feature Grouping Module (FGOM) to enhance multi-scale representation. FGOM consists of three key components: the Feature Reparameterization Module (FRep), the Coordinate Attention Golden Feature Pyramid Network (CAGoldFPN), and the Multi-Scale Stacked Network (MuSSNet). The FRep module addresses the loss of channel information caused by the small size of thermal infrared ship targets and the complexity of background features. The CAGoldFPN module improves multi-scale feature fusion, while MuSSNet mitigates issues of high target similarity and the susceptibility of small targets to being overlooked. Experimental results show that, compared with the baseline RT-DETR model, FGOM-RTDETR achieves notable performance gains: precision improves from 0.921 to 0.942, mAP50 rises from 0.938 to 0.958, and recall increases from 0.923 to 0.934. These results demonstrate that FGOM-RTDETR delivers superior detection performance for infrared ship targets.&lt;/p&gt;</content:encoded></item><item><title>Adaptive Fine-Grained Fusion Network for Multimodal UAV Object Detection</title><link>https://doi.org/10.1109/tip.2026.3661868</link><guid>10.1109/tip.2026.3661868</guid><pubDate>Wed, 11 Feb 2026 20:57:46 +0000</pubDate><dc:creator>Zhanyan Tang</dc:creator><dc:creator>Zhihao Wu</dc:creator><dc:creator>Mu Li</dc:creator><dc:creator>Jie Wen</dc:creator><dc:creator>Bob Zhang</dc:creator><dc:creator>Yong Xu</dc:creator><dc:creator>Jianqiang Li</dc:creator><prism:publicationName>IEEE Transactions on Image Processing</prism:publicationName><prism:doi>10.1109/tip.2026.3661868</prism:doi><description>Multimodal perception and fusion play a vital role in unmanned aerial vehicle (UAV) object detection. Existing methods typically adopt global fusion strategies across modalities. However, due to illumination variation, the effectiveness of RGB and infrared modalities may differ across local regions within the same image, particularly in UAV perspectives where occlusions and dense small objects are prevalent, leading to suboptimal performance of global fusion methods. To address this issue, we propose an adaptive fine-grained fusion network for multimodal UAV object detection. First, we design a local feature consistency-based modality fusion module, which adaptively assigns local fusion weights according to the structural consistency of high-response regions across modalities, thereby enabling more effective aggregation of object-relevant features. Second, we introduce a mutual information-guided feature contrastive loss to encourage the preservation of modality-specific information during the early training phase. Experimental results demonstrate that the proposed method effectively addresses the issue of object occlusion in UAV perspectives, achieving state-of-the-art performance on multimodal UAV object detection benchmarks. Code will be available at https://github.com/lingf5877/AFFNet.
Published: 2026-02-11T20:57:46+00:00
Venue: IEEE Transactions on Image Processing
Score: 0.836 (must_read)</description><content:encoded>&lt;p&gt;&lt;strong&gt;Authors:&lt;/strong&gt; Zhanyan Tang; Zhihao Wu; Mu Li; Jie Wen; Bob Zhang; Yong Xu; Jianqiang Li&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Journal:&lt;/strong&gt; IEEE Transactions on Image Processing&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;a href="https://doi.org/10.1109/tip.2026.3661868"&gt;10.1109/tip.2026.3661868&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Relevance:&lt;/strong&gt; 0.836 (must_read)&lt;/p&gt;
&lt;p&gt;Multimodal perception and fusion play a vital role in unmanned aerial vehicle (UAV) object detection. Existing methods typically adopt global fusion strategies across modalities. However, due to illumination variation, the effectiveness of RGB and infrared modalities may differ across local regions within the same image, particularly in UAV perspectives where occlusions and dense small objects are prevalent, leading to suboptimal performance of global fusion methods. To address this issue, we propose an adaptive fine-grained fusion network for multimodal UAV object detection. First, we design a local feature consistency-based modality fusion module, which adaptively assigns local fusion weights according to the structural consistency of high-response regions across modalities, thereby enabling more effective aggregation of object-relevant features. Second, we introduce a mutual information-guided feature contrastive loss to encourage the preservation of modality-specific information during the early training phase. Experimental results demonstrate that the proposed method effectively addresses the issue of object occlusion in UAV perspectives, achieving state-of-the-art performance on multimodal UAV object detection benchmarks. Code will be available at https://github.com/lingf5877/AFFNet.&lt;/p&gt;</content:encoded></item><item><title>HLNet: A Lightweight Network for Ship Detection in Complex SAR Environments</title><link>https://doi.org/10.3390/rs18040577</link><guid>10.3390/rs18040577</guid><pubDate>Thu, 12 Feb 2026 15:28:11 +0000</pubDate><dc:creator>Xiaopeng Guo</dc:creator><dc:creator>Fan Deng</dc:creator><dc:creator>Jie Gong</dc:creator><dc:creator>Jing Zhang</dc:creator><dc:creator>Jiajia Guo</dc:creator><dc:creator>Yong Wang</dc:creator><dc:creator>Yinmei Zeng</dc:creator><dc:creator>Gongquan Li</dc:creator><prism:publicationName>Remote Sensing</prism:publicationName><prism:doi>10.3390/rs18040577</prism:doi><description>The coherent speckle noise in synthetic aperture radar (SAR) imagery, together with complex sea clutter and large variations in ship target scales, poses significant challenges to accurate and robust ship detection, particularly under strict lightweight constraints required by satellite-borne and airborne platforms. To address this issue, this paper proposes a high-precision lightweight detection network, termed High-Lightweight Net (HLNet), specifically designed for SAR ship detection. The network incorporates a novel multi-scale backbone, Multi-Scale Net (MSNet), which integrates dynamic feature completion and multi-core parallel convolutions to alleviate small-target feature loss and suppress background interference. To further enhance multi-scale feature fusion while reducing model complexity, a lightweight path aggregation feature pyramid network, High-Lightweight Feature Pyramid (HLPAFPN), is introduced by reconstructing fusion pathways and removing redundant channels. In addition, a lightweight detection head, High-Lightweight Head (HLHead), is designed by combining grouped convolutions with distribution focal loss to improve localization robustness under low signal-to-noise ratio conditions. Extensive experiments conducted on the public SSDD and HRSID datasets demonstrate that HLNet achieves mAP50 scores of 98.3% and 91.7%, respectively, with only 0.66 M parameters. Extensive evaluations on the more challenging CSID subset, composed of complex scenes selected from SSDD and HRSID, demonstrate that HLNet attains an mAP50 of 75.9%, outperforming the baseline by 4.3%. These results indicate that HLNet achieves an effective balance between detection accuracy and computational efficiency, making it well-suited for deployment on resource-constrained SAR platforms.
Published: 2026-02-12T15:28:11+00:00
Venue: Remote Sensing
Score: 0.828 (must_read)</description><content:encoded>&lt;p&gt;&lt;strong&gt;Authors:&lt;/strong&gt; Xiaopeng Guo; Fan Deng; Jie Gong; Jing Zhang; Jiajia Guo; Yong Wang; Yinmei Zeng; Gongquan Li&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Journal:&lt;/strong&gt; Remote Sensing&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;a href="https://doi.org/10.3390/rs18040577"&gt;10.3390/rs18040577&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Relevance:&lt;/strong&gt; 0.828 (must_read)&lt;/p&gt;
&lt;p&gt;The coherent speckle noise in synthetic aperture radar (SAR) imagery, together with complex sea clutter and large variations in ship target scales, poses significant challenges to accurate and robust ship detection, particularly under strict lightweight constraints required by satellite-borne and airborne platforms. To address this issue, this paper proposes a high-precision lightweight detection network, termed High-Lightweight Net (HLNet), specifically designed for SAR ship detection. The network incorporates a novel multi-scale backbone, Multi-Scale Net (MSNet), which integrates dynamic feature completion and multi-core parallel convolutions to alleviate small-target feature loss and suppress background interference. To further enhance multi-scale feature fusion while reducing model complexity, a lightweight path aggregation feature pyramid network, High-Lightweight Feature Pyramid (HLPAFPN), is introduced by reconstructing fusion pathways and removing redundant channels. In addition, a lightweight detection head, High-Lightweight Head (HLHead), is designed by combining grouped convolutions with distribution focal loss to improve localization robustness under low signal-to-noise ratio conditions. Extensive experiments conducted on the public SSDD and HRSID datasets demonstrate that HLNet achieves mAP50 scores of 98.3% and 91.7%, respectively, with only 0.66 M parameters. Extensive evaluations on the more challenging CSID subset, composed of complex scenes selected from SSDD and HRSID, demonstrate that HLNet attains an mAP50 of 75.9%, outperforming the baseline by 4.3%. These results indicate that HLNet achieves an effective balance between detection accuracy and computational efficiency, making it well-suited for deployment on resource-constrained SAR platforms.&lt;/p&gt;</content:encoded></item><item><title>What matters in building vision–language–action models for generalist robots</title><link>https://doi.org/10.1038/s42256-025-01168-7</link><guid>10.1038/s42256-025-01168-7</guid><pubDate>Wed, 11 Feb 2026 10:02:25 +0000</pubDate><dc:creator>Xinghang Li</dc:creator><dc:creator>Peiyan Li</dc:creator><dc:creator>Long Qian</dc:creator><dc:creator>Minghuan Liu</dc:creator><dc:creator>Dong Wang</dc:creator><dc:creator>Jirong Liu</dc:creator><dc:creator>Bingyi Kang</dc:creator><dc:creator>Xiao Ma</dc:creator><dc:creator>Xinlong Wang</dc:creator><dc:creator>Di Guo</dc:creator><dc:creator>Tao Kong</dc:creator><dc:creator>Hanbo Zhang</dc:creator><dc:creator>Huaping Liu</dc:creator><prism:publicationName>Nature Machine Intelligence</prism:publicationName><prism:doi>10.1038/s42256-025-01168-7</prism:doi><description>To utilize foundation vision–language models (VLMs) for robotic tasks and motion planning, the community has proposed different methods for injecting action components into VLMs and building the vision–language–action models (VLAs). Here we disclose the key factors that significantly influence the performance of VLA on robot manipulation problems and focus on answering three essential design choices: which backbone to select, how to formulate the VLA architectures and when to add cross-embodiment data. The obtained results convince us firmly to explain why we prefer VLA and develop a new family of VLAs, RoboVLMs, which require very few manual designs and achieve a new state-of-the-art performance in three simulation tasks and real-world experiments. Through our extensive experiments, which include over 8 VLM backbones, 4 policy architectures and over 600 distinct designed experiments, we provide a detailed guidebook for the future design of VLAs. In addition to the study, the highly flexible RoboVLMs framework, which supports easy integrations of new VLMs and free combinations of various design choices, is made public to facilitate future research. We open-source all details, including codes, models, datasets and toolkits, along with detailed training and evaluation recipes at robovlms.github.io . Vision–language–action models recently emerged as a tool for robotics. Here Li and colleagues compare vision–language–action models and highlight what makes a model useful.
Published: 2026-02-11T10:02:25+00:00
Venue: Nature Machine Intelligence
Score: 0.826 (must_read)</description><content:encoded>&lt;p&gt;&lt;strong&gt;Authors:&lt;/strong&gt; Xinghang Li; Peiyan Li; Long Qian; Minghuan Liu; Dong Wang; Jirong Liu; Bingyi Kang; Xiao Ma; Xinlong Wang; Di Guo; Tao Kong; Hanbo Zhang; Huaping Liu&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Journal:&lt;/strong&gt; Nature Machine Intelligence&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;a href="https://doi.org/10.1038/s42256-025-01168-7"&gt;10.1038/s42256-025-01168-7&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Relevance:&lt;/strong&gt; 0.826 (must_read)&lt;/p&gt;
&lt;p&gt;To utilize foundation vision–language models (VLMs) for robotic tasks and motion planning, the community has proposed different methods for injecting action components into VLMs and building the vision–language–action models (VLAs). Here we disclose the key factors that significantly influence the performance of VLA on robot manipulation problems and focus on answering three essential design choices: which backbone to select, how to formulate the VLA architectures and when to add cross-embodiment data. The obtained results convince us firmly to explain why we prefer VLA and develop a new family of VLAs, RoboVLMs, which require very few manual designs and achieve a new state-of-the-art performance in three simulation tasks and real-world experiments. Through our extensive experiments, which include over 8 VLM backbones, 4 policy architectures and over 600 distinct designed experiments, we provide a detailed guidebook for the future design of VLAs. In addition to the study, the highly flexible RoboVLMs framework, which supports easy integrations of new VLMs and free combinations of various design choices, is made public to facilitate future research. We open-source all details, including codes, models, datasets and toolkits, along with detailed training and evaluation recipes at robovlms.github.io . Vision–language–action models recently emerged as a tool for robotics. Here Li and colleagues compare vision–language–action models and highlight what makes a model useful.&lt;/p&gt;</content:encoded></item><item><title>PIA: Fusing Edge Prior Information into Attention for Semantic Segmentation in Vision Transformer</title><link>https://doi.org/10.1016/j.inffus.2026.104222</link><guid>10.1016/j.inffus.2026.104222</guid><pubDate>Wed, 11 Feb 2026 16:53:21 +0000</pubDate><dc:creator>Ruijie Xiao</dc:creator><dc:creator>Bo Yang</dc:creator><dc:creator>Qianyang Zhu</dc:creator><prism:publicationName>Information Fusion</prism:publicationName><prism:doi>10.1016/j.inffus.2026.104222</prism:doi><description>Swin Transformer introduced window self-attention (WSA) to improve the performance of Vision Transformer (ViT) in semantic segmentation. However, the attention patch group partition in WSA is solely based on spatial positions, which ignores the spatial frequency relationships. It may limit the ability of attention mechanisms to fully exploit the inductive biases. To address this issue, we propose a novel attention mechanism that integrates traditional computer vision techniques with deep learning approaches, named P rior I nformation A ttention (PIA). PIA redefines the grouping strategy by fusing edge prior information (edge detection results) to re-organize image patches into flexible group windows. It enables the attention computation to query image patches that share similar edge intensities but are spatially distant. Besides, Feature Exchanging Strategy (FES) is further introduced to refine feature boundaries via cross-group fusion. Building upon PIA and FES, we propose a transformer backbone named PIA Transformer (PIAT). To validate the effectiveness of PIAT, we compare it with the state-of-the-art semantic segmentation models on 4 datasets (Cityscapes, ADE20K, DLRSD and CamVid). Experimental results demonstrate that PIAT outperforms the baseline methods in all four datasets.
Published: 2026-02-11T16:53:21+00:00
Venue: Information Fusion
Score: 0.823 (must_read)</description><content:encoded>&lt;p&gt;&lt;strong&gt;Authors:&lt;/strong&gt; Ruijie Xiao; Bo Yang; Qianyang Zhu&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Journal:&lt;/strong&gt; Information Fusion&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;a href="https://doi.org/10.1016/j.inffus.2026.104222"&gt;10.1016/j.inffus.2026.104222&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Relevance:&lt;/strong&gt; 0.823 (must_read)&lt;/p&gt;
&lt;p&gt;Swin Transformer introduced window self-attention (WSA) to improve the performance of Vision Transformer (ViT) in semantic segmentation. However, the attention patch group partition in WSA is solely based on spatial positions, which ignores the spatial frequency relationships. It may limit the ability of attention mechanisms to fully exploit the inductive biases. To address this issue, we propose a novel attention mechanism that integrates traditional computer vision techniques with deep learning approaches, named P rior I nformation A ttention (PIA). PIA redefines the grouping strategy by fusing edge prior information (edge detection results) to re-organize image patches into flexible group windows. It enables the attention computation to query image patches that share similar edge intensities but are spatially distant. Besides, Feature Exchanging Strategy (FES) is further introduced to refine feature boundaries via cross-group fusion. Building upon PIA and FES, we propose a transformer backbone named PIA Transformer (PIAT). To validate the effectiveness of PIAT, we compare it with the state-of-the-art semantic segmentation models on 4 datasets (Cityscapes, ADE20K, DLRSD and CamVid). Experimental results demonstrate that PIAT outperforms the baseline methods in all four datasets.&lt;/p&gt;</content:encoded></item><item><title>GIC-FAFNet: Global-Local Information Coordination and Feature Alignment Fusion Network for Remote Sensing Object Detection</title><link>https://doi.org/10.1016/j.patcog.2026.113292</link><guid>10.1016/j.patcog.2026.113292</guid><pubDate>Thu, 12 Feb 2026 07:53:33 +0000</pubDate><dc:creator>Yinggan Tang</dc:creator><dc:creator>Ziteng Zhao</dc:creator><dc:creator>Quansheng Xu</dc:creator><prism:publicationName>Pattern Recognition</prism:publicationName><prism:doi>10.1016/j.patcog.2026.113292</prism:doi><description>The detection of small and multi-scale objects in remote sensing images (RSIs) remains a challenging task due to limited feature representation of small targets and insufficient use of spatial information across scales. To address these issues, we propose a novel Global-Local Information Coordination and Feature Alignment Fusion Network (GIC-FAFNet). First, we propose a Multi-level Feature Information Aggregation Module (MFIAM) that integrates local and global contextual cues, enriching small-object feature representation and partially mitigating the weakening or loss of small-object features caused by repeated down-sampling in deep networks. Second, we introduce a Feature Alignment Pyramid Network (FAPN) that effectively combines precise spatial details with high-level semantic information, improving localization accuracy for multi-scale objects. Additionally, a Detail Extraction Module (DEM) is developed to adaptively enhance features for objects of diverse scales and shapes. Extensive experiments on four public remote sensing datasets demonstrate that the proposed method achieves superior performance compared to state-of-the-art approaches. The code is available at: https://github.com/woshio/GIC-FAFNet .
Published: 2026-02-12T07:53:33+00:00
Venue: Pattern Recognition
Score: 0.821 (must_read)</description><content:encoded>&lt;p&gt;&lt;strong&gt;Authors:&lt;/strong&gt; Yinggan Tang; Ziteng Zhao; Quansheng Xu&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Journal:&lt;/strong&gt; Pattern Recognition&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;a href="https://doi.org/10.1016/j.patcog.2026.113292"&gt;10.1016/j.patcog.2026.113292&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Relevance:&lt;/strong&gt; 0.821 (must_read)&lt;/p&gt;
&lt;p&gt;The detection of small and multi-scale objects in remote sensing images (RSIs) remains a challenging task due to limited feature representation of small targets and insufficient use of spatial information across scales. To address these issues, we propose a novel Global-Local Information Coordination and Feature Alignment Fusion Network (GIC-FAFNet). First, we propose a Multi-level Feature Information Aggregation Module (MFIAM) that integrates local and global contextual cues, enriching small-object feature representation and partially mitigating the weakening or loss of small-object features caused by repeated down-sampling in deep networks. Second, we introduce a Feature Alignment Pyramid Network (FAPN) that effectively combines precise spatial details with high-level semantic information, improving localization accuracy for multi-scale objects. Additionally, a Detail Extraction Module (DEM) is developed to adaptively enhance features for objects of diverse scales and shapes. Extensive experiments on four public remote sensing datasets demonstrate that the proposed method achieves superior performance compared to state-of-the-art approaches. The code is available at: https://github.com/woshio/GIC-FAFNet .&lt;/p&gt;</content:encoded></item><item><title>Hybrid Granularity Distribution Estimation for Few-Shot Learning: Statistics Transfer from Categories and Instances</title><link>https://doi.org/10.1109/tip.2026.3661814</link><guid>10.1109/tip.2026.3661814</guid><pubDate>Wed, 11 Feb 2026 20:57:46 +0000</pubDate><dc:creator>Shuo Wang</dc:creator><dc:creator>Tianyu Qi</dc:creator><dc:creator>Xingyu Zhu</dc:creator><dc:creator>Yanbin Hao</dc:creator><dc:creator>Beier Zhu</dc:creator><dc:creator>Hanwang Zhang</dc:creator><dc:creator>Meng Wang</dc:creator><prism:publicationName>IEEE Transactions on Image Processing</prism:publicationName><prism:doi>10.1109/tip.2026.3661814</prism:doi><description>Distribution estimation is a pivotal strategy in few-shot learning (FSL) to mitigate data scarcity by sampling from estimated distributions, utilizing statistical properties (mean and variance) transferred from related base categories. However, category-level estimation alone often fails to generate representative samples due to significant dissimilarities between base and novel categories, leading to suboptimal performance. To address this limitation, we propose Hybrid Granularity Distribution Estimation (HGDE), which integrates both coarse-grained category-level statistics and fine-grained instance-level statistics. By leveraging instance statistics from the nearest base samples, HGDE enhances the characterization of novel categories, capturing subtle features that category-level estimation overlooks. These statistics are fused through linear interpolation to form a robust distribution for novel categories, ensuring both diversity and representativeness in generated samples. Additionally, HGDE employs refined estimation techniques, such as weighted summation for mean calculation and principal component retention for covariance, to further improve accuracy. Empirical evaluations on four FSL benchmarks, including Mini-ImageNet, Tiered-ImageNet, CUB and CIFAR-FS, demonstrate that HGDE offers effective distribution estimation capabilities and leads to notable accuracy gains, with improvements of more than 1.8% in 1-shot tasks on CUB. These results highlight HGDE’s ability to balance mean precision and variance diversity, making it a versatile and effective solution for FSL.
Published: 2026-02-11T20:57:46+00:00
Venue: IEEE Transactions on Image Processing
Score: 0.820 (must_read)</description><content:encoded>&lt;p&gt;&lt;strong&gt;Authors:&lt;/strong&gt; Shuo Wang; Tianyu Qi; Xingyu Zhu; Yanbin Hao; Beier Zhu; Hanwang Zhang; Meng Wang&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Journal:&lt;/strong&gt; IEEE Transactions on Image Processing&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;a href="https://doi.org/10.1109/tip.2026.3661814"&gt;10.1109/tip.2026.3661814&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Relevance:&lt;/strong&gt; 0.820 (must_read)&lt;/p&gt;
&lt;p&gt;Distribution estimation is a pivotal strategy in few-shot learning (FSL) to mitigate data scarcity by sampling from estimated distributions, utilizing statistical properties (mean and variance) transferred from related base categories. However, category-level estimation alone often fails to generate representative samples due to significant dissimilarities between base and novel categories, leading to suboptimal performance. To address this limitation, we propose Hybrid Granularity Distribution Estimation (HGDE), which integrates both coarse-grained category-level statistics and fine-grained instance-level statistics. By leveraging instance statistics from the nearest base samples, HGDE enhances the characterization of novel categories, capturing subtle features that category-level estimation overlooks. These statistics are fused through linear interpolation to form a robust distribution for novel categories, ensuring both diversity and representativeness in generated samples. Additionally, HGDE employs refined estimation techniques, such as weighted summation for mean calculation and principal component retention for covariance, to further improve accuracy. Empirical evaluations on four FSL benchmarks, including Mini-ImageNet, Tiered-ImageNet, CUB and CIFAR-FS, demonstrate that HGDE offers effective distribution estimation capabilities and leads to notable accuracy gains, with improvements of more than 1.8% in 1-shot tasks on CUB. These results highlight HGDE’s ability to balance mean precision and variance diversity, making it a versatile and effective solution for FSL.&lt;/p&gt;</content:encoded></item><item><title>OmniHD-Scenes: A Next-Generation Multimodal Dataset for Autonomous Driving</title><link>https://doi.org/10.1109/tpami.2026.3663672</link><guid>10.1109/tpami.2026.3663672</guid><pubDate>Wed, 11 Feb 2026 20:55:51 +0000</pubDate><dc:creator>Lianqing Zheng</dc:creator><dc:creator>Long Yang</dc:creator><dc:creator>Qunshu Lin</dc:creator><dc:creator>Wenjin Ai</dc:creator><dc:creator>Minghao Liu</dc:creator><dc:creator>Shouyi Lu</dc:creator><dc:creator>Jianan Liu</dc:creator><dc:creator>Hongze Ren</dc:creator><dc:creator>Jingyue Mo</dc:creator><dc:creator>Xiaokai Bai</dc:creator><dc:creator>Jie Bai</dc:creator><dc:creator>Zhixiong Ma</dc:creator><dc:creator>Xichan Zhu</dc:creator><prism:publicationName>IEEE Transactions on Pattern Analysis and Machine Intelligence</prism:publicationName><prism:doi>10.1109/tpami.2026.3663672</prism:doi><description>The rapid advancement of deep learning has intensified the need for comprehensive data for use by autonomous driving algorithms. High-quality datasets are crucial for the development of effective data-driven autonomous driving solutions. Next-generation autonomous driving datasets must be multimodal, incorporating data from advanced sensors that feature extensive data coverage, detailed annotations, and diverse scene representation. To address this need, we present OmniHD-Scenes, a large-scale multimodal dataset that provides comprehensive omnidirectional high-definition data. The OmniHD-Scenes dataset combines data from 128-beam LiDAR, six cameras, and six 4D imaging radar systems to achieve full environmental perception. The dataset comprises 1501 clips, each approximately 30-s long, totaling more than 450 K synchronized frames and more than 5.85 million synchronized sensor data points. We also propose a novel 4D annotation pipeline. To date, we have annotated 200 clips with more than 514 K precise 3D bounding boxes. These clips also include semantic segmentation annotations for static scene elements. Additionally, we introduce a novel automated pipeline for generation of the dense occupancy ground truth, which effectively leverages information from non-key frames. Alongside the proposed dataset, we establish comprehensive evaluation metrics, baseline models, and benchmarks for 3D detection and semantic occupancy prediction. These benchmarks utilize surround-view cameras and 4D imaging radar to explore cost-effective sensor solutions for autonomous driving applications. Extensive experiments demonstrate the effectiveness of our low-cost sensor configuration and its robustness under adverse conditions. The dataset is available at https://www.2077ai.com/OmniHD-Scenes.
Published: 2026-02-11T20:55:51+00:00
Venue: IEEE Transactions on Pattern Analysis and Machine Intelligence
Score: 0.816 (must_read)</description><content:encoded>&lt;p&gt;&lt;strong&gt;Authors:&lt;/strong&gt; Lianqing Zheng; Long Yang; Qunshu Lin; Wenjin Ai; Minghao Liu; Shouyi Lu; Jianan Liu; Hongze Ren; Jingyue Mo; Xiaokai Bai; Jie Bai; Zhixiong Ma; Xichan Zhu&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Journal:&lt;/strong&gt; IEEE Transactions on Pattern Analysis and Machine Intelligence&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;a href="https://doi.org/10.1109/tpami.2026.3663672"&gt;10.1109/tpami.2026.3663672&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Relevance:&lt;/strong&gt; 0.816 (must_read)&lt;/p&gt;
&lt;p&gt;The rapid advancement of deep learning has intensified the need for comprehensive data for use by autonomous driving algorithms. High-quality datasets are crucial for the development of effective data-driven autonomous driving solutions. Next-generation autonomous driving datasets must be multimodal, incorporating data from advanced sensors that feature extensive data coverage, detailed annotations, and diverse scene representation. To address this need, we present OmniHD-Scenes, a large-scale multimodal dataset that provides comprehensive omnidirectional high-definition data. The OmniHD-Scenes dataset combines data from 128-beam LiDAR, six cameras, and six 4D imaging radar systems to achieve full environmental perception. The dataset comprises 1501 clips, each approximately 30-s long, totaling more than 450 K synchronized frames and more than 5.85 million synchronized sensor data points. We also propose a novel 4D annotation pipeline. To date, we have annotated 200 clips with more than 514 K precise 3D bounding boxes. These clips also include semantic segmentation annotations for static scene elements. Additionally, we introduce a novel automated pipeline for generation of the dense occupancy ground truth, which effectively leverages information from non-key frames. Alongside the proposed dataset, we establish comprehensive evaluation metrics, baseline models, and benchmarks for 3D detection and semantic occupancy prediction. These benchmarks utilize surround-view cameras and 4D imaging radar to explore cost-effective sensor solutions for autonomous driving applications. Extensive experiments demonstrate the effectiveness of our low-cost sensor configuration and its robustness under adverse conditions. The dataset is available at https://www.2077ai.com/OmniHD-Scenes.&lt;/p&gt;</content:encoded></item><item><title>Thermal3D-GS: Physics-induced 3D Gaussians for Thermal Infrared Novel-view Synthesis with a Large-Scale Dataset</title><link>https://doi.org/10.1109/tpami.2026.3663966</link><guid>10.1109/tpami.2026.3663966</guid><pubDate>Wed, 11 Feb 2026 20:55:51 +0000</pubDate><dc:creator>Qian Chen</dc:creator><dc:creator>Shihao Shu</dc:creator><dc:creator>Heng Sun</dc:creator><dc:creator>Junzhang Chen</dc:creator><dc:creator>Xiangzhi Bai</dc:creator><prism:publicationName>IEEE Transactions on Pattern Analysis and Machine Intelligence</prism:publicationName><prism:doi>10.1109/tpami.2026.3663966</prism:doi><description>Thermal infrared imaging has attracted widespread attention in many fields due to the advantages of all-weather imaging and strong penetration. However, existing methods for thermal infrared novel-view synthesis often produce results with coarse details and floating artifacts, primarily caused by physical factors such as atmospheric transmission effects and thermal conduction. These challenges hinder accurate reconstruction of intricate structures and temperature distributions in thermal scenes, limiting the practical utility of previous approaches. To address these limitations, this paper introduces a physics-induced 3D Gaussian splatting method named Thermal3D-GS, the first novel-view synthesis method that relies exclusively on thermal infrared image. Thermal3D-GS begins by modeling atmospheric transmission effects and thermal conduction in three-dimensional media using neural networks. Additionally, considering the sparse features of infrared images, sparse feature priors are designed to improve the reconstruction accuracy of thermal infrared images. Furthermore, to validate the effectiveness of our method, the first large-scale benchmark dataset named Thermal Infrared Novel-view Synthesis Dataset (TI-NSD) is created. This dataset comprises 50 authentic thermal infrared video scenes, covering indoor, outdoor, traffic and UAV(Unmanned Aerial Vehicle) scenarios, with a total of 15,213 frames of thermal infrared image data. In addition, an expanded validation thermal infrared dataset, which includes three high-resolution scenes and five special scenes under varying atmospheric conditions and complex propagation media is constructed to assess generalization performance of the proposed method. Based on this dataset, this paper experimentally verifies the effectiveness of Thermal3D-GS. The results indicate that our method outperforms the baseline method with a 3.19 dB improvement in PSNR and significantly addresses the issues of floaters and indistinct edge features pr...
Published: 2026-02-11T20:55:51+00:00
Venue: IEEE Transactions on Pattern Analysis and Machine Intelligence
Score: 0.812 (must_read)</description><content:encoded>&lt;p&gt;&lt;strong&gt;Authors:&lt;/strong&gt; Qian Chen; Shihao Shu; Heng Sun; Junzhang Chen; Xiangzhi Bai&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Journal:&lt;/strong&gt; IEEE Transactions on Pattern Analysis and Machine Intelligence&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;a href="https://doi.org/10.1109/tpami.2026.3663966"&gt;10.1109/tpami.2026.3663966&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Relevance:&lt;/strong&gt; 0.812 (must_read)&lt;/p&gt;
&lt;p&gt;Thermal infrared imaging has attracted widespread attention in many fields due to the advantages of all-weather imaging and strong penetration. However, existing methods for thermal infrared novel-view synthesis often produce results with coarse details and floating artifacts, primarily caused by physical factors such as atmospheric transmission effects and thermal conduction. These challenges hinder accurate reconstruction of intricate structures and temperature distributions in thermal scenes, limiting the practical utility of previous approaches. To address these limitations, this paper introduces a physics-induced 3D Gaussian splatting method named Thermal3D-GS, the first novel-view synthesis method that relies exclusively on thermal infrared image. Thermal3D-GS begins by modeling atmospheric transmission effects and thermal conduction in three-dimensional media using neural networks. Additionally, considering the sparse features of infrared images, sparse feature priors are designed to improve the reconstruction accuracy of thermal infrared images. Furthermore, to validate the effectiveness of our method, the first large-scale benchmark dataset named Thermal Infrared Novel-view Synthesis Dataset (TI-NSD) is created. This dataset comprises 50 authentic thermal infrared video scenes, covering indoor, outdoor, traffic and UAV(Unmanned Aerial Vehicle) scenarios, with a total of 15,213 frames of thermal infrared image data. In addition, an expanded validation thermal infrared dataset, which includes three high-resolution scenes and five special scenes under varying atmospheric conditions and complex propagation media is constructed to assess generalization performance of the proposed method. Based on this dataset, this paper experimentally verifies the effectiveness of Thermal3D-GS. The results indicate that our method outperforms the baseline method with a 3.19 dB improvement in PSNR and significantly addresses the issues of floaters and indistinct edge features pr...&lt;/p&gt;</content:encoded></item><item><title>Pyramid Token Pruning for High-Resolution Large Vision-Language Models via Region, Token, and Instruction-Guided Importance</title><link>https://doi.org/10.1109/tcsvt.2026.3663658</link><guid>10.1109/tcsvt.2026.3663658</guid><pubDate>Wed, 11 Feb 2026 20:57:15 +0000</pubDate><dc:creator>Yuxuan Liang</dc:creator><dc:creator>Xu Li</dc:creator><dc:creator>Xiaolei Chen</dc:creator><dc:creator>Haotian Chen</dc:creator><dc:creator>Yi Zhen</dc:creator><dc:creator>Zhe Liu</dc:creator><dc:creator>Rui Zhu</dc:creator><dc:creator>Bin Li</dc:creator><dc:creator>Xiangyang Xue</dc:creator><prism:publicationName>IEEE Transactions on Circuits and Systems for Video Technology</prism:publicationName><prism:doi>10.1109/tcsvt.2026.3663658</prism:doi><description>Large Vision-Language Models (LVLMs) have recently demonstrated strong multimodal understanding, yet their fine-grained visual perception is often constrained by low input resolutions. A common remedy is to partition high-resolution images into multiple sub-images for separate encoding, but this approach drastically inflates the number of visual tokens and introduces prohibitive inference overhead. To overcome this challenge, we propose Pyramid Token Pruning (PTP), a training-free strategy that hierarchically integrates bottom-up visual saliency at both region and token levels with top-down instruction-guided relevance. Inspired by human visual cognition, PTP selectively preserves more tokens from salient regions while further emphasizing those most relevant to task instructions. Extensive experiments on 13 diverse benchmarks show that PTP substantially reduces computational cost, memory usage, and inference latency, with negligible performance degradation.
Published: 2026-02-11T20:57:15+00:00
Venue: IEEE Transactions on Circuits and Systems for Video Technology
Score: 0.811 (must_read)</description><content:encoded>&lt;p&gt;&lt;strong&gt;Authors:&lt;/strong&gt; Yuxuan Liang; Xu Li; Xiaolei Chen; Haotian Chen; Yi Zhen; Zhe Liu; Rui Zhu; Bin Li; Xiangyang Xue&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Journal:&lt;/strong&gt; IEEE Transactions on Circuits and Systems for Video Technology&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;a href="https://doi.org/10.1109/tcsvt.2026.3663658"&gt;10.1109/tcsvt.2026.3663658&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Relevance:&lt;/strong&gt; 0.811 (must_read)&lt;/p&gt;
&lt;p&gt;Large Vision-Language Models (LVLMs) have recently demonstrated strong multimodal understanding, yet their fine-grained visual perception is often constrained by low input resolutions. A common remedy is to partition high-resolution images into multiple sub-images for separate encoding, but this approach drastically inflates the number of visual tokens and introduces prohibitive inference overhead. To overcome this challenge, we propose Pyramid Token Pruning (PTP), a training-free strategy that hierarchically integrates bottom-up visual saliency at both region and token levels with top-down instruction-guided relevance. Inspired by human visual cognition, PTP selectively preserves more tokens from salient regions while further emphasizing those most relevant to task instructions. Extensive experiments on 13 diverse benchmarks show that PTP substantially reduces computational cost, memory usage, and inference latency, with negligible performance degradation.&lt;/p&gt;</content:encoded></item><item><title>LMCNet: Lightweight Modality Compensation Network via Knowledge Distillation for Salient Ship Detection under Missing Modality Conditions</title><link>https://doi.org/10.1109/taes.2026.3664356</link><guid>10.1109/taes.2026.3664356</guid><pubDate>Thu, 12 Feb 2026 21:02:31 +0000</pubDate><dc:creator>Weibao Xue</dc:creator><dc:creator>Jiaqiu Ai</dc:creator><dc:creator>Yanan Zhu</dc:creator><dc:creator>Xinyu Sun</dc:creator><dc:creator>Yong Zhang</dc:creator><dc:creator>Gui Gao</dc:creator><prism:publicationName>IEEE Transactions on Aerospace and Electronic Systems</prism:publicationName><prism:doi>10.1109/taes.2026.3664356</prism:doi><description>Salient ship detection is critical for maritime applications that require accurate localization. Although the fusion of Synthetic Aperture Radar (SAR) and Automatic Identification System (AIS) data has proven effective in enhancing saliency and suppressing background clutter, practical deployment faces two major limitations: the limited unavailability of AIS data and the high computational overhead of existing multimodal models. These limitations pose a fundamental challenge in balancing detection accuracy and efficiency under missing modality and resource-constrained environments. To address this issue, a Lightweight Modality Compensation Network (LMCNet) is proposed. A multimodal teacher network is trained with SAR and AIS inputs to learn rich and complementary representations. Meanwhile, a compact, single-modality student network that relies only on SAR is designed to support low-cost, real-time deployment. To enable robust knowledge compensation and transfer, this paper designs a knowledge distillation strategy consisting of three modules: structure-aware attention distillation for spatial alignment, cross-head teacher distillation for semantic enhancement, and adaptive loss scheduling for dynamic optimization. This unified design allows the student model to inherit spatial precision and semantic awareness from the teacher, achieving strong performance even with limited input modalities. Extensive experiments on two datasets show that our distilled student model improves Eξ E by 1.39% and Fwβ by 4.37% compared to state-of-the-art methods, demonstrating its superior balance of accuracy and efficiency in real-world deployment scenarios.
Published: 2026-02-12T21:02:31+00:00
Venue: IEEE Transactions on Aerospace and Electronic Systems
Score: 0.809 (must_read)</description><content:encoded>&lt;p&gt;&lt;strong&gt;Authors:&lt;/strong&gt; Weibao Xue; Jiaqiu Ai; Yanan Zhu; Xinyu Sun; Yong Zhang; Gui Gao&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Journal:&lt;/strong&gt; IEEE Transactions on Aerospace and Electronic Systems&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;a href="https://doi.org/10.1109/taes.2026.3664356"&gt;10.1109/taes.2026.3664356&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Relevance:&lt;/strong&gt; 0.809 (must_read)&lt;/p&gt;
&lt;p&gt;Salient ship detection is critical for maritime applications that require accurate localization. Although the fusion of Synthetic Aperture Radar (SAR) and Automatic Identification System (AIS) data has proven effective in enhancing saliency and suppressing background clutter, practical deployment faces two major limitations: the limited unavailability of AIS data and the high computational overhead of existing multimodal models. These limitations pose a fundamental challenge in balancing detection accuracy and efficiency under missing modality and resource-constrained environments. To address this issue, a Lightweight Modality Compensation Network (LMCNet) is proposed. A multimodal teacher network is trained with SAR and AIS inputs to learn rich and complementary representations. Meanwhile, a compact, single-modality student network that relies only on SAR is designed to support low-cost, real-time deployment. To enable robust knowledge compensation and transfer, this paper designs a knowledge distillation strategy consisting of three modules: structure-aware attention distillation for spatial alignment, cross-head teacher distillation for semantic enhancement, and adaptive loss scheduling for dynamic optimization. This unified design allows the student model to inherit spatial precision and semantic awareness from the teacher, achieving strong performance even with limited input modalities. Extensive experiments on two datasets show that our distilled student model improves Eξ E by 1.39% and Fwβ by 4.37% compared to state-of-the-art methods, demonstrating its superior balance of accuracy and efficiency in real-world deployment scenarios.&lt;/p&gt;</content:encoded></item><item><title>Cross-Modal Attention-Modulated Feature Enhancement Network for Visible-Infrared Ship Detection</title><link>https://doi.org/10.1109/jstars.2026.3664123</link><guid>10.1109/jstars.2026.3664123</guid><pubDate>Thu, 12 Feb 2026 20:59:57 +0000</pubDate><dc:creator>Yaxin Lei</dc:creator><dc:creator>Wuxia Zhang</dc:creator><dc:creator>Xiaochen Niu</dc:creator><dc:creator>Hailong Ning</dc:creator><dc:creator>Xiaoqiang Lu</dc:creator><prism:publicationName>IEEE Journal of Selected Topics in Applied Earth Observations and Remote Sensing</prism:publicationName><prism:doi>10.1109/jstars.2026.3664123</prism:doi><description>Ship detection is crucial for tasks such as harbor dynamic surveillance and maritime traffic management, aiming to identify and locate ships. Multimodal object detection based on visible-infrared data is well suited for ship detection because it enables all-weather object detection. However, it lacks paired visible-infrared ship datasets, and there still exist problems such as insufficient consistency and complementarity of cross-modal semantic information, as well as a large amount of redundant information when fusing different modalities. To address these problems, two pseudo-infrared datasets are established, and a Cross-modal Attention-modulated Feature Enhancement Network (CAMFEN) approach for ship detection is proposed. CAMFEN mainly utilizes the Cross-modal Collaborative-Differential Enhancement Feature Fusion module ( m{C}^{2}m{DEFF} m{C}^{2}m{DEFF} ) to effectively fuse the information of different modalities, which consists of Collaborative Attention Modulation Block (CAMB), Differential Attention Modulation Block (DAMB), and Global Feature Guided Fusion Block (GFGFB). The CAMB achieves cross-modal semantic alignment through channel attention modulation, eliminates geometric offset with spatial adaptive calibration, strengthens common semantics representation from the perspective of collaborative integrity, and equalizes modal contribution. The DAMB selects discriminant difference features through channel attention screening, uses spatial attention to focus on target regions, and mines complementary or contradictory information from the perspective of difference specificity, enhancing modal specificity and suppressing redundant noise. Finally, the enhanced single modality features obtained by CAMB and DAMB are fed into GFGFB, which guides the optimized fusion of enhanced single modality features from a global perspective. The proposed method has been validated on HRSC2016 and DOTAv1.0 datasets, and the experimental results show that CAMFEN outperforms existing ship d...
Published: 2026-02-12T20:59:57+00:00
Venue: IEEE Journal of Selected Topics in Applied Earth Observations and Remote Sensing
Score: 0.806 (must_read)</description><content:encoded>&lt;p&gt;&lt;strong&gt;Authors:&lt;/strong&gt; Yaxin Lei; Wuxia Zhang; Xiaochen Niu; Hailong Ning; Xiaoqiang Lu&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Journal:&lt;/strong&gt; IEEE Journal of Selected Topics in Applied Earth Observations and Remote Sensing&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;a href="https://doi.org/10.1109/jstars.2026.3664123"&gt;10.1109/jstars.2026.3664123&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Relevance:&lt;/strong&gt; 0.806 (must_read)&lt;/p&gt;
&lt;p&gt;Ship detection is crucial for tasks such as harbor dynamic surveillance and maritime traffic management, aiming to identify and locate ships. Multimodal object detection based on visible-infrared data is well suited for ship detection because it enables all-weather object detection. However, it lacks paired visible-infrared ship datasets, and there still exist problems such as insufficient consistency and complementarity of cross-modal semantic information, as well as a large amount of redundant information when fusing different modalities. To address these problems, two pseudo-infrared datasets are established, and a Cross-modal Attention-modulated Feature Enhancement Network (CAMFEN) approach for ship detection is proposed. CAMFEN mainly utilizes the Cross-modal Collaborative-Differential Enhancement Feature Fusion module ( m{C}^{2}m{DEFF} m{C}^{2}m{DEFF} ) to effectively fuse the information of different modalities, which consists of Collaborative Attention Modulation Block (CAMB), Differential Attention Modulation Block (DAMB), and Global Feature Guided Fusion Block (GFGFB). The CAMB achieves cross-modal semantic alignment through channel attention modulation, eliminates geometric offset with spatial adaptive calibration, strengthens common semantics representation from the perspective of collaborative integrity, and equalizes modal contribution. The DAMB selects discriminant difference features through channel attention screening, uses spatial attention to focus on target regions, and mines complementary or contradictory information from the perspective of difference specificity, enhancing modal specificity and suppressing redundant noise. Finally, the enhanced single modality features obtained by CAMB and DAMB are fed into GFGFB, which guides the optimized fusion of enhanced single modality features from a global perspective. The proposed method has been validated on HRSC2016 and DOTAv1.0 datasets, and the experimental results show that CAMFEN outperforms existing ship d...&lt;/p&gt;</content:encoded></item><item><title>红外视频卫星空中动目标检测数据集及其评估</title><link>https://doi.org/10.11834/jig.250536</link><guid>10.11834/jig.250536</guid><pubDate>Wed, 11 Feb 2026 07:27:45 +0000</pubDate><dc:creator>Li Ruojing</dc:creator><dc:creator>Li Zhaoxu</dc:creator><dc:creator>Chen Nuo</dc:creator><dc:creator>Guo Gaowei</dc:creator><dc:creator>Dou Zechao</dc:creator><dc:creator>Long Zhengxing</dc:creator><dc:creator>Luo Yihang</dc:creator><dc:creator>Zeng Yaoyuan</dc:creator><dc:creator>Sheng Weidong</dc:creator><dc:creator>Li Boyang</dc:creator><dc:creator>Li Zhijun</dc:creator><dc:creator>Li Miao</dc:creator><dc:creator>An Wei</dc:creator><dc:creator>Li Haixin</dc:creator><dc:creator>Yu Zhiqiang</dc:creator><dc:creator>Yin Xiaoyu</dc:creator><dc:creator>Zha Xuyang</dc:creator><dc:creator>Zeng Baiwen</dc:creator><dc:creator>Wang Wufan</dc:creator><dc:creator>Zhang Bo</dc:creator><dc:creator>Lu Yue</dc:creator><dc:creator>Chu Donghao</dc:creator><dc:creator>Li Ziyi</dc:creator><dc:creator>Huang Kangwei</dc:creator><dc:creator>Yang Borui</dc:creator><dc:creator>Xing Yinghui</dc:creator><dc:creator>Zhang Shizhou</dc:creator><dc:creator>张世周</dc:creator><prism:publicationName>Journal of Image and Graphics</prism:publicationName><prism:doi>10.11834/jig.250536</prism:doi><description>目的红外视频卫星是探测空中动目标的重要手段，红外小目标检测技术是其关键基础。深度学习显著推动了单帧红外小目标检测，然而卫星红外视频中的空中动目标普遍空域显著性低、场景复杂，单帧方法难以有效检测，因此亟需发展融合时域信息的红外极弱小目标检测技术。但该领域长期缺乏视频数据集，严重制约了相关技术的发展与应用。为突破此瓶颈，该文构建了首个包含大量真实场景的红外视频卫星空中动目标检测数据集。方法基于武汉一号卫星采集20126帧真实红外视频卫星动目标数据，设计两阶段“由粗到精”的标注方法，完成29757个空中动目标的精标注。为了丰富场景多样性，进一步融合两大真实天基背景下的仿真动目标数据，构建包含1401个真实场景、122265帧视频图像、454116个目标的红外视频卫星空中动目标检测数据集。数据集提供实例级掩码标签，支持空中动目标检测与跟踪技术研究，并提出了相关评价指标。结果数据分析表明，该数据集中真实目标的平均信噪比仅为3.06，超过80%的目标信噪比低于2，且实测场景中的目标与背景存在丰富的动态变化与相互干扰，呈现难以模拟的复杂性。结论基于该数据集开展了首届红外视频卫星空中动目标检测比赛，充分验证了该数据集的高挑战性与实际价值，对于红外极弱小目标检测技术研究具有重要支持作用。数据集获取链接：https：//github.com/TinaLRJ/DeepPro（科学数据银行：Infrared video satellite aerial moving target detection dataset）。
Published: 2026-02-11T07:27:45+00:00
Venue: Journal of Image and Graphics
Score: 0.805 (must_read)</description><content:encoded>&lt;p&gt;&lt;strong&gt;Authors:&lt;/strong&gt; Li Ruojing; Li Zhaoxu; Chen Nuo; Guo Gaowei; Dou Zechao; Long Zhengxing; Luo Yihang; Zeng Yaoyuan; Sheng Weidong; Li Boyang; Li Zhijun; Li Miao; An Wei; Li Haixin; Yu Zhiqiang; Yin Xiaoyu; Zha Xuyang; Zeng Baiwen; Wang Wufan; Zhang Bo; Lu Yue; Chu Donghao; Li Ziyi; Huang Kangwei; Yang Borui; Xing Yinghui; Zhang Shizhou; 张世周&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Journal:&lt;/strong&gt; Journal of Image and Graphics&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;a href="https://doi.org/10.11834/jig.250536"&gt;10.11834/jig.250536&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Relevance:&lt;/strong&gt; 0.805 (must_read)&lt;/p&gt;
&lt;p&gt;目的红外视频卫星是探测空中动目标的重要手段，红外小目标检测技术是其关键基础。深度学习显著推动了单帧红外小目标检测，然而卫星红外视频中的空中动目标普遍空域显著性低、场景复杂，单帧方法难以有效检测，因此亟需发展融合时域信息的红外极弱小目标检测技术。但该领域长期缺乏视频数据集，严重制约了相关技术的发展与应用。为突破此瓶颈，该文构建了首个包含大量真实场景的红外视频卫星空中动目标检测数据集。方法基于武汉一号卫星采集20126帧真实红外视频卫星动目标数据，设计两阶段“由粗到精”的标注方法，完成29757个空中动目标的精标注。为了丰富场景多样性，进一步融合两大真实天基背景下的仿真动目标数据，构建包含1401个真实场景、122265帧视频图像、454116个目标的红外视频卫星空中动目标检测数据集。数据集提供实例级掩码标签，支持空中动目标检测与跟踪技术研究，并提出了相关评价指标。结果数据分析表明，该数据集中真实目标的平均信噪比仅为3.06，超过80%的目标信噪比低于2，且实测场景中的目标与背景存在丰富的动态变化与相互干扰，呈现难以模拟的复杂性。结论基于该数据集开展了首届红外视频卫星空中动目标检测比赛，充分验证了该数据集的高挑战性与实际价值，对于红外极弱小目标检测技术研究具有重要支持作用。数据集获取链接：https：//github.com/TinaLRJ/DeepPro（科学数据银行：Infrared video satellite aerial moving target detection dataset）。&lt;/p&gt;</content:encoded></item><item><title>Consistent and Controllable Image Animation with Motion Linear Diffusion Transformers</title><link>https://doi.org/10.1109/tpami.2026.3664227</link><guid>10.1109/tpami.2026.3664227</guid><pubDate>Thu, 12 Feb 2026 20:59:14 +0000</pubDate><dc:creator>Xin Ma</dc:creator><dc:creator>Yaohui Wang</dc:creator><dc:creator>Genyun Jia</dc:creator><dc:creator>Xinyuan Chen</dc:creator><dc:creator>Tien-Tsin Wong</dc:creator><dc:creator>Cunjian Chen</dc:creator><prism:publicationName>IEEE Transactions on Pattern Analysis and Machine Intelligence</prism:publicationName><prism:doi>10.1109/tpami.2026.3664227</prism:doi><description>Image animation has seen significant progress, driven by the powerful generative capabilities of diffusion models. However, maintaining appearance consistency with static input images and mitigating abrupt motion transitions in generated animations remain persistent challenges. While text-to-video (T2V) generation has demonstrated impressive performance with diffusion transformer models, the image animation field still largely relies on U-Net-based diffusion models, which lag behind the latest T2V approaches. Moreover, the quadratic complexity of vanilla self-attention mechanisms in Transformers imposes heavy computational demands, making image animation particularly resource-intensive. To address these issues, we propose MiraMo, a framework designed to enhance efficiency, appearance consistency, and motion smoothness in image animation. Specifically, MiraMo introduces three key elements: (1) A foundational text-to-video architecture replacing vanilla self-attention with efficient linear attention to reduce computational overhead while preserving generation quality; (2) A novel motion residual learning paradigm that focuses on modeling motion dynamics rather than directly predicting frames, improving temporal consistency; and (3) A DCT-based noise refinement strategy during inference to suppress sudden motion artifacts, complemented by a dynamics control module to balance motion smoothness and expressiveness. Extensive experiments against state-of-the-art methods validate the superiority of MiraMo in generating consistent, smooth, and controllable animations with accelerated inference speed. Additionally, we demonstrate the versatility of MiraMo through applications in motion transfer and video editing tasks. The project page is available at https://maxin-cn.github.io/miramo_project.
Published: 2026-02-12T20:59:14+00:00
Venue: IEEE Transactions on Pattern Analysis and Machine Intelligence
Score: 0.804 (must_read)</description><content:encoded>&lt;p&gt;&lt;strong&gt;Authors:&lt;/strong&gt; Xin Ma; Yaohui Wang; Genyun Jia; Xinyuan Chen; Tien-Tsin Wong; Cunjian Chen&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Journal:&lt;/strong&gt; IEEE Transactions on Pattern Analysis and Machine Intelligence&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;a href="https://doi.org/10.1109/tpami.2026.3664227"&gt;10.1109/tpami.2026.3664227&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Relevance:&lt;/strong&gt; 0.804 (must_read)&lt;/p&gt;
&lt;p&gt;Image animation has seen significant progress, driven by the powerful generative capabilities of diffusion models. However, maintaining appearance consistency with static input images and mitigating abrupt motion transitions in generated animations remain persistent challenges. While text-to-video (T2V) generation has demonstrated impressive performance with diffusion transformer models, the image animation field still largely relies on U-Net-based diffusion models, which lag behind the latest T2V approaches. Moreover, the quadratic complexity of vanilla self-attention mechanisms in Transformers imposes heavy computational demands, making image animation particularly resource-intensive. To address these issues, we propose MiraMo, a framework designed to enhance efficiency, appearance consistency, and motion smoothness in image animation. Specifically, MiraMo introduces three key elements: (1) A foundational text-to-video architecture replacing vanilla self-attention with efficient linear attention to reduce computational overhead while preserving generation quality; (2) A novel motion residual learning paradigm that focuses on modeling motion dynamics rather than directly predicting frames, improving temporal consistency; and (3) A DCT-based noise refinement strategy during inference to suppress sudden motion artifacts, complemented by a dynamics control module to balance motion smoothness and expressiveness. Extensive experiments against state-of-the-art methods validate the superiority of MiraMo in generating consistent, smooth, and controllable animations with accelerated inference speed. Additionally, we demonstrate the versatility of MiraMo through applications in motion transfer and video editing tasks. The project page is available at https://maxin-cn.github.io/miramo_project.&lt;/p&gt;</content:encoded></item><item><title>Reusability Report: Evaluating the performance of a meta-learning foundation model on predicting the antibacterial activity of natural products</title><link>https://doi.org/10.1038/s42256-026-01187-y</link><guid>10.1038/s42256-026-01187-y</guid><pubDate>Thu, 12 Feb 2026 10:03:34 +0000</pubDate><dc:creator>Caitlin M. Butt</dc:creator><dc:creator>Allison S. Walker</dc:creator><prism:publicationName>Nature Machine Intelligence</prism:publicationName><prism:doi>10.1038/s42256-026-01187-y</prism:doi><description>Abstract Deep learning foundation models are becoming increasingly popular for use in bioactivity prediction. Recently, Feng et al. developed ActFound, a bioactive foundation model that jointly uses pairwise learning and meta-learning. By utilizing these techniques, the model is capable of being fine-tuned to a more specific bioactivity task with only a small amount of new data. Here, to investigate the generalizability of the model, we looked to fine-tune the foundation model on an antibacterial natural products (NPs) dataset. Large, labelled NPs datasets, which are needed to train traditional deep learning methods, are scarce. Therefore, the bioactivity prediction of NPs is an ideal task for foundation models. We studied the performance of ActFound on the NPs dataset using a range of few-shot settings. Additionally, we compared ActFound’s performance with those of other state-of-the-art models in the field. We found ActFound was unable to reach the same level of accuracy on the antibacterial NPs dataset as it did on other cross-domain tasks reported in the original publication. However, ActFound displayed comparable or better performance compared to the other models studied, especially at the low-shot settings. Our results establish ActFound as a useful foundation model for the bioactivity prediction of tasks with limited data, particularly for datasets that contain the bioactivities of similar compounds.
Published: 2026-02-12T10:03:34+00:00
Venue: Nature Machine Intelligence
Score: 0.801 (must_read)</description><content:encoded>&lt;p&gt;&lt;strong&gt;Authors:&lt;/strong&gt; Caitlin M. Butt; Allison S. Walker&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Journal:&lt;/strong&gt; Nature Machine Intelligence&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;a href="https://doi.org/10.1038/s42256-026-01187-y"&gt;10.1038/s42256-026-01187-y&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Relevance:&lt;/strong&gt; 0.801 (must_read)&lt;/p&gt;
&lt;p&gt;Abstract Deep learning foundation models are becoming increasingly popular for use in bioactivity prediction. Recently, Feng et al. developed ActFound, a bioactive foundation model that jointly uses pairwise learning and meta-learning. By utilizing these techniques, the model is capable of being fine-tuned to a more specific bioactivity task with only a small amount of new data. Here, to investigate the generalizability of the model, we looked to fine-tune the foundation model on an antibacterial natural products (NPs) dataset. Large, labelled NPs datasets, which are needed to train traditional deep learning methods, are scarce. Therefore, the bioactivity prediction of NPs is an ideal task for foundation models. We studied the performance of ActFound on the NPs dataset using a range of few-shot settings. Additionally, we compared ActFound’s performance with those of other state-of-the-art models in the field. We found ActFound was unable to reach the same level of accuracy on the antibacterial NPs dataset as it did on other cross-domain tasks reported in the original publication. However, ActFound displayed comparable or better performance compared to the other models studied, especially at the low-shot settings. Our results establish ActFound as a useful foundation model for the bioactivity prediction of tasks with limited data, particularly for datasets that contain the bioactivities of similar compounds.&lt;/p&gt;</content:encoded></item><item><title>Knowledge-data-model-driven multimodal few-shot learning for hyperspectral fine classification: Generalization across sensor, category and scene</title><link>https://doi.org/10.1016/j.isprsjprs.2026.02.001</link><guid>10.1016/j.isprsjprs.2026.02.001</guid><pubDate>Wed, 11 Feb 2026 20:40:55 +0000</pubDate><dc:creator>Qiqi Zhu</dc:creator><dc:creator>Mingzhen Xu</dc:creator><dc:creator>Rui Ma</dc:creator><dc:creator>Longli Ran</dc:creator><dc:creator>Jiayao Xue</dc:creator><dc:creator>Qingfeng Guan</dc:creator><prism:publicationName>ISPRS Journal of Photogrammetry and Remote Sensing</prism:publicationName><prism:doi>10.1016/j.isprsjprs.2026.02.001</prism:doi><description>Fine-grained land-cover mapping is crucial for accurately assessing environmental degradation and monitoring socioeconomic dynamics. Few-shot learning of hyperspectral images offers a promising solution in cases where sample collection is limited. However, previous studies, such as tree species mapping, typically use 1% or 0.5% of samples per class, yielding thousands of samples for common species but struggling to identify unseen or rare species (only one sample/shot) in real-world scenarios. Furthermore, inevitable cross-sensor, cross-category, and cross-scene variations significantly increase the occurrence of unseen or rare classes and spectral heterogeneity within common land-cover types. To this being, we propose Knowing-Net, a knowledge-data-model-driven multimodal few-shot learning network, to bridge the application gap for fine-grained mapping of unseen or rare classes. In Knowing-Net, prior knowledge of sensor, i.e., spectral parameters, is leveraged to reconstruct cross-sensor hyperspectral images, mitigating heterogeneity in spectral responses across datasets and enabling cross-domain transfer across different sensors, scenes, and land cover types. To breakthrough the gap in recognizing unseen classes, multimodal data, including textual descriptions and natural images of unseen classes, is embedded into network to construct shared side information through modality-specific feature learning. By designing a cross-alignment mechanism for hyperspectral and multimodal information in a shared semantic space, distinct encoders are guided to produce consistent distribution for the same class across different modalities, reducing sample dependency and facilitating the identification of unseen or rare classes. Finally, inspired by the first law of geography, a sliding discriminant window is designed to incorporate spatial context, enhancing geography interpretability and robustness to noise. We evaluate Knowing-Net on five challenging airborne hyperspectral datasets with a fine-grained classification system, covering crop type, tree species, and similar urban land covers with varying materials. Extensive experiments on five datasets consistently demonstrate Knowing-net’s superiority over state-of-the-art methods in both mapping performance and cross-domain generalization. Notably, the unified framework achieves state-of-the-art results in one-shot learning and establishes a new paradigm in zero-shot classification for fine-grained land cover tasks. To the best of our knowledge, this is the first comprehensive generalization of FSL across sensor, category, and scene for hyperspectral image-based fine mapping.
Published: 2026-02-11T20:40:55+00:00
Venue: ISPRS Journal of Photogrammetry and Remote Sensing
Score: 0.800 (must_read)</description><content:encoded>&lt;p&gt;&lt;strong&gt;Authors:&lt;/strong&gt; Qiqi Zhu; Mingzhen Xu; Rui Ma; Longli Ran; Jiayao Xue; Qingfeng Guan&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Journal:&lt;/strong&gt; ISPRS Journal of Photogrammetry and Remote Sensing&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;a href="https://doi.org/10.1016/j.isprsjprs.2026.02.001"&gt;10.1016/j.isprsjprs.2026.02.001&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Relevance:&lt;/strong&gt; 0.800 (must_read)&lt;/p&gt;
&lt;p&gt;Fine-grained land-cover mapping is crucial for accurately assessing environmental degradation and monitoring socioeconomic dynamics. Few-shot learning of hyperspectral images offers a promising solution in cases where sample collection is limited. However, previous studies, such as tree species mapping, typically use 1% or 0.5% of samples per class, yielding thousands of samples for common species but struggling to identify unseen or rare species (only one sample/shot) in real-world scenarios. Furthermore, inevitable cross-sensor, cross-category, and cross-scene variations significantly increase the occurrence of unseen or rare classes and spectral heterogeneity within common land-cover types. To this being, we propose Knowing-Net, a knowledge-data-model-driven multimodal few-shot learning network, to bridge the application gap for fine-grained mapping of unseen or rare classes. In Knowing-Net, prior knowledge of sensor, i.e., spectral parameters, is leveraged to reconstruct cross-sensor hyperspectral images, mitigating heterogeneity in spectral responses across datasets and enabling cross-domain transfer across different sensors, scenes, and land cover types. To breakthrough the gap in recognizing unseen classes, multimodal data, including textual descriptions and natural images of unseen classes, is embedded into network to construct shared side information through modality-specific feature learning. By designing a cross-alignment mechanism for hyperspectral and multimodal information in a shared semantic space, distinct encoders are guided to produce consistent distribution for the same class across different modalities, reducing sample dependency and facilitating the identification of unseen or rare classes. Finally, inspired by the first law of geography, a sliding discriminant window is designed to incorporate spatial context, enhancing geography interpretability and robustness to noise. We evaluate Knowing-Net on five challenging airborne hyperspectral datasets with a fine-grained classification system, covering crop type, tree species, and similar urban land covers with varying materials. Extensive experiments on five datasets consistently demonstrate Knowing-net’s superiority over state-of-the-art methods in both mapping performance and cross-domain generalization. Notably, the unified framework achieves state-of-the-art results in one-shot learning and establishes a new paradigm in zero-shot classification for fine-grained land cover tasks. To the best of our knowledge, this is the first comprehensive generalization of FSL across sensor, category, and scene for hyperspectral image-based fine mapping.&lt;/p&gt;</content:encoded></item><item><title>DNGaussian++: Improving Sparse-View Gaussian Radiance Fields with Depth Normalization</title><link>https://doi.org/10.1109/tpami.2026.3664307</link><guid>10.1109/tpami.2026.3664307</guid><pubDate>Thu, 12 Feb 2026 20:59:14 +0000</pubDate><dc:creator>Jiahe Li</dc:creator><dc:creator>Jiawei Zhang</dc:creator><dc:creator>Xiaohan Yu</dc:creator><dc:creator>Xiao Bai</dc:creator><dc:creator>Jin Zheng</dc:creator><dc:creator>Xin Ning</dc:creator><dc:creator>Lin Gu</dc:creator><prism:publicationName>IEEE Transactions on Pattern Analysis and Machine Intelligence</prism:publicationName><prism:doi>10.1109/tpami.2026.3664307</prism:doi><description>Synthesizing novel views from sparse views has achieved impressive advances with radiance fields, yet prevailing methods suffer from high consumption or insufficient refinement capability. This paper introduces DNGaussian, a depth-regularized framework based on 3D Gaussian Splatting, offering real-time and high-quality few-shot novel view synthesis at low costs. Our motivation stems from the remarkable advancement of recent 3D Gaussian Splatting, despite it will encounter a geometry degradation when input views decrease. In the Gaussian radiance fields, we find this degradation in scene geometry primarily lined to the positioning of Gaussian primitives and can be mitigated by depth constraint. Consequently, we propose a Hard and Soft Depth Regularization to restore accurate scene geometry under coarse monocular depth supervision while maintaining a fine-grained color appearance. To further refine detailed geometry, we introduce Global-Local Depth Normalization, enhancing the focus on small local depth changes. Although DNGaussian shows impressive performance, its patch-wise regularization obscures the inconsistency in cross-patch errors. Additionally, primitives can still be irreversibly trapped in local minima under sparse views, even if depth regularization is applied. In this paper, we propose an extended version, DNGaussian++. First, a Geometry Instance Regularizer is developed to enable depth regularization for continuous consistency by exploiting reliable instance-level depth cues. Leveraging the depth gradient guidance, we then propose a Depth-Guided Geometry Reorganization to address the aforementioned local minima problem with high representation efficiency. Extensive experiments show that DNGaussian++ exhibits state-of-the-art performance in multiple datasets and scenarios with high efficiency, and the broad applicability and effectiveness are verified on various backbones and tasks.
Published: 2026-02-12T20:59:14+00:00
Venue: IEEE Transactions on Pattern Analysis and Machine Intelligence
Score: 0.799 (must_read)</description><content:encoded>&lt;p&gt;&lt;strong&gt;Authors:&lt;/strong&gt; Jiahe Li; Jiawei Zhang; Xiaohan Yu; Xiao Bai; Jin Zheng; Xin Ning; Lin Gu&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Journal:&lt;/strong&gt; IEEE Transactions on Pattern Analysis and Machine Intelligence&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;a href="https://doi.org/10.1109/tpami.2026.3664307"&gt;10.1109/tpami.2026.3664307&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Relevance:&lt;/strong&gt; 0.799 (must_read)&lt;/p&gt;
&lt;p&gt;Synthesizing novel views from sparse views has achieved impressive advances with radiance fields, yet prevailing methods suffer from high consumption or insufficient refinement capability. This paper introduces DNGaussian, a depth-regularized framework based on 3D Gaussian Splatting, offering real-time and high-quality few-shot novel view synthesis at low costs. Our motivation stems from the remarkable advancement of recent 3D Gaussian Splatting, despite it will encounter a geometry degradation when input views decrease. In the Gaussian radiance fields, we find this degradation in scene geometry primarily lined to the positioning of Gaussian primitives and can be mitigated by depth constraint. Consequently, we propose a Hard and Soft Depth Regularization to restore accurate scene geometry under coarse monocular depth supervision while maintaining a fine-grained color appearance. To further refine detailed geometry, we introduce Global-Local Depth Normalization, enhancing the focus on small local depth changes. Although DNGaussian shows impressive performance, its patch-wise regularization obscures the inconsistency in cross-patch errors. Additionally, primitives can still be irreversibly trapped in local minima under sparse views, even if depth regularization is applied. In this paper, we propose an extended version, DNGaussian++. First, a Geometry Instance Regularizer is developed to enable depth regularization for continuous consistency by exploiting reliable instance-level depth cues. Leveraging the depth gradient guidance, we then propose a Depth-Guided Geometry Reorganization to address the aforementioned local minima problem with high representation efficiency. Extensive experiments show that DNGaussian++ exhibits state-of-the-art performance in multiple datasets and scenarios with high efficiency, and the broad applicability and effectiveness are verified on various backbones and tasks.&lt;/p&gt;</content:encoded></item><item><title>FGAA-FPN: Foreground-Guided Angle-Aware Feature Pyramid Network for Oriented Object Detection</title><link>https://arxiv.org/abs/2602.10710v1</link><guid>http://arxiv.org/abs/2602.10710v1</guid><pubDate>Wed, 11 Feb 2026 10:15:06 +0000</pubDate><dc:creator>Jialin Ma</dc:creator><dc:type>preprint</dc:type><prism:publicationName>arXiv</prism:publicationName><description>With the increasing availability of high-resolution remote sensing and aerial imagery, oriented object detection has become a key capability for geographic information updating, maritime surveillance, and disaster response. However, it remains challenging due to cluttered backgrounds, severe scale variation, and large orientation changes. Existing approaches largely improve performance through multi-scale feature fusion with feature pyramid networks or contextual modeling with attention, but they often lack explicit foreground modeling and do not leverage geometric orientation priors, which limits feature discriminability. To overcome these limitations, we propose FGAA-FPN, a Foreground-Guided Angle-Aware Feature Pyramid Network for oriented object detection. FGAA-FPN is built on a hierarchical functional decomposition that accounts for the distinct spatial resolution and semantic abstraction across pyramid levels, thereby strengthening multi-scale representations. Concretely, a Foreground-Guided Feature Modulation module learns foreground saliency under weak supervision to enhance object regions and suppress background interference in low-level features. In parallel, an Angle-Aware Multi-Head Attention module encodes relative orientation relationships to guide global interactions among high-level semantic features. Extensive experiments on DOTA v1.0 and DOTA v1.5 demonstrate that FGAA-FPN achieves state-of-the-art results, reaching 75.5% and 68.3% mAP, respectively.
Published: 2026-02-11T10:15:06+00:00
Venue: arXiv
Score: 0.798 (must_read)</description><content:encoded>&lt;p&gt;&lt;strong&gt;Authors:&lt;/strong&gt; Jialin Ma&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Journal:&lt;/strong&gt; arXiv&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Relevance:&lt;/strong&gt; 0.798 (must_read)&lt;/p&gt;
&lt;p&gt;With the increasing availability of high-resolution remote sensing and aerial imagery, oriented object detection has become a key capability for geographic information updating, maritime surveillance, and disaster response. However, it remains challenging due to cluttered backgrounds, severe scale variation, and large orientation changes. Existing approaches largely improve performance through multi-scale feature fusion with feature pyramid networks or contextual modeling with attention, but they often lack explicit foreground modeling and do not leverage geometric orientation priors, which limits feature discriminability. To overcome these limitations, we propose FGAA-FPN, a Foreground-Guided Angle-Aware Feature Pyramid Network for oriented object detection. FGAA-FPN is built on a hierarchical functional decomposition that accounts for the distinct spatial resolution and semantic abstraction across pyramid levels, thereby strengthening multi-scale representations. Concretely, a Foreground-Guided Feature Modulation module learns foreground saliency under weak supervision to enhance object regions and suppress background interference in low-level features. In parallel, an Angle-Aware Multi-Head Attention module encodes relative orientation relationships to guide global interactions among high-level semantic features. Extensive experiments on DOTA v1.0 and DOTA v1.5 demonstrate that FGAA-FPN achieves state-of-the-art results, reaching 75.5% and 68.3% mAP, respectively.&lt;/p&gt;</content:encoded></item><item><title>Multi-Expert Learning Framework with the State Space Model for Optical and SAR Image Registration</title><link>https://doi.org/10.1109/tgrs.2026.3664116</link><guid>10.1109/tgrs.2026.3664116</guid><pubDate>Thu, 12 Feb 2026 20:59:23 +0000</pubDate><dc:creator>Wei Wang</dc:creator><dc:creator>Dou Quan</dc:creator><dc:creator>Ning Huyan</dc:creator><dc:creator>Chonghua Lv</dc:creator><dc:creator>Shuang Wang</dc:creator><dc:creator>Yunan Li</dc:creator><dc:creator>Licheng Jiao</dc:creator><prism:publicationName>IEEE Transactions on Geoscience and Remote Sensing</prism:publicationName><prism:doi>10.1109/tgrs.2026.3664116</prism:doi><description>Optical and Synthetic Aperture Radar (SAR) image registration is crucial for multi-modal image fusion and applications. However, several challenges limit the performance of existing deep learning-based methods in cross-modal image registration: (i) significant nonlinear radiometric variations between optical and SAR images affect the shared feature learning and matching; (ii) limited textures in images hinder discriminative feature extraction; (iii) the local receptive field of Convolutional Neural Networks (CNNs) restricts the learning of contextual information, while the Transformer can capture long-range global features but with high computational complexity. To address these issues, this paper proposes a multi-expert learning framework with the State Space Model (ME-SSM) for optical and SAR image registration. Firstly, to improve the registration performance with limited textures, ME-SSM constructs a multi-expert learning framework to capture shared features from multi-modal images. Specifically, it extracts features from various transformations of the input image and employs a learnable soft router to dynamically fuse these features, thereby enriching feature representations and improving registration performance. Secondly, ME-SSM introduces a state space model, Mamba, for feature extraction, which employs a multi-directional cross-scanning strategy to efficiently capture global contextual relationships with linear complexity. ME-SSM can expand the receptive field, enhance image registration accuracy, and avoid incurring high computational costs. Additionally, ME-SSM uses a multi-level feature aggregation (MFA) module to enhance the multi-scale feature fusion and interaction. Extensive experiments have demonstrated the effectiveness and advantages of our proposed ME-SSM on optical and SAR image registration. Specifically, ME-SSM improves the correct matching rate (CMR) by 7.14% and 1.95% based on thresholds 1 and 3, respectively, on the SEN1-2 dataset, and incr...
Published: 2026-02-12T20:59:23+00:00
Venue: IEEE Transactions on Geoscience and Remote Sensing
Score: 0.798 (must_read)</description><content:encoded>&lt;p&gt;&lt;strong&gt;Authors:&lt;/strong&gt; Wei Wang; Dou Quan; Ning Huyan; Chonghua Lv; Shuang Wang; Yunan Li; Licheng Jiao&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Journal:&lt;/strong&gt; IEEE Transactions on Geoscience and Remote Sensing&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;a href="https://doi.org/10.1109/tgrs.2026.3664116"&gt;10.1109/tgrs.2026.3664116&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Relevance:&lt;/strong&gt; 0.798 (must_read)&lt;/p&gt;
&lt;p&gt;Optical and Synthetic Aperture Radar (SAR) image registration is crucial for multi-modal image fusion and applications. However, several challenges limit the performance of existing deep learning-based methods in cross-modal image registration: (i) significant nonlinear radiometric variations between optical and SAR images affect the shared feature learning and matching; (ii) limited textures in images hinder discriminative feature extraction; (iii) the local receptive field of Convolutional Neural Networks (CNNs) restricts the learning of contextual information, while the Transformer can capture long-range global features but with high computational complexity. To address these issues, this paper proposes a multi-expert learning framework with the State Space Model (ME-SSM) for optical and SAR image registration. Firstly, to improve the registration performance with limited textures, ME-SSM constructs a multi-expert learning framework to capture shared features from multi-modal images. Specifically, it extracts features from various transformations of the input image and employs a learnable soft router to dynamically fuse these features, thereby enriching feature representations and improving registration performance. Secondly, ME-SSM introduces a state space model, Mamba, for feature extraction, which employs a multi-directional cross-scanning strategy to efficiently capture global contextual relationships with linear complexity. ME-SSM can expand the receptive field, enhance image registration accuracy, and avoid incurring high computational costs. Additionally, ME-SSM uses a multi-level feature aggregation (MFA) module to enhance the multi-scale feature fusion and interaction. Extensive experiments have demonstrated the effectiveness and advantages of our proposed ME-SSM on optical and SAR image registration. Specifically, ME-SSM improves the correct matching rate (CMR) by 7.14% and 1.95% based on thresholds 1 and 3, respectively, on the SEN1-2 dataset, and incr...&lt;/p&gt;</content:encoded></item><item><title>Dual Adaptive Disentangled Representation Learning with Multimodal Data for Disease Diagnosis</title><link>https://doi.org/10.1109/tpami.2026.3664047</link><guid>10.1109/tpami.2026.3664047</guid><pubDate>Thu, 12 Feb 2026 20:59:14 +0000</pubDate><dc:creator>Xiumei Chen</dc:creator><dc:creator>Wenliang Pan</dc:creator><dc:creator>Tao Wang</dc:creator><dc:creator>Xinyue Zhang</dc:creator><dc:creator>Wei Xiong</dc:creator><dc:creator>Ting Tian</dc:creator><dc:creator>Qianjin Feng</dc:creator><dc:creator>Meiyan Huang</dc:creator><prism:publicationName>IEEE Transactions on Pattern Analysis and Machine Intelligence</prism:publicationName><prism:doi>10.1109/tpami.2026.3664047</prism:doi><description>The use of imaging and genetic data for biomarker detection and disease diagnosis can deepen the understanding of disease pathogenesis and assist in clinical diagnosis. However, current methods face two major challenges: 1) the significant heterogeneity between multimodal data hampers modality fusion. 2) Effectively exploring consistency and variability information from similar diseases for enhancing model performance is difficult. In this paper, we propose a novel unified frame work, termed dual adaptive disentangled representation learning (DADRL), to simultaneously achieve disease-shared and disease specific biomarker detection as well as disease diagnosis. Our DADRL comprises three components: 1) a biology information constraints-based modality fusion strategy is applied to adaptively explore inter- and intra-modal correlations, thereby effectively fusing multimodal data. 2) A unified framework that integrates modality fusion and disease diagnosis is proposed to mine disease-related information for simultaneously accomplishing disease-related biomarker detection and disease diagnosis. 3) Disentangled representation learning and several adaptive metric constraints are incorporated into the unified framework to adaptively separate disease-specific information from disease shared feature representations for effectively identifying disease shared and disease-specific biomarkers, thereby deepening the understanding of disease pathogenesis. Extensive experiments on multiple real datasets and simulated data demonstrate that our method significantly improves performance of biomarker detection and disease diagnosis.
Published: 2026-02-12T20:59:14+00:00
Venue: IEEE Transactions on Pattern Analysis and Machine Intelligence
Score: 0.797 (must_read)</description><content:encoded>&lt;p&gt;&lt;strong&gt;Authors:&lt;/strong&gt; Xiumei Chen; Wenliang Pan; Tao Wang; Xinyue Zhang; Wei Xiong; Ting Tian; Qianjin Feng; Meiyan Huang&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Journal:&lt;/strong&gt; IEEE Transactions on Pattern Analysis and Machine Intelligence&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;a href="https://doi.org/10.1109/tpami.2026.3664047"&gt;10.1109/tpami.2026.3664047&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Relevance:&lt;/strong&gt; 0.797 (must_read)&lt;/p&gt;
&lt;p&gt;The use of imaging and genetic data for biomarker detection and disease diagnosis can deepen the understanding of disease pathogenesis and assist in clinical diagnosis. However, current methods face two major challenges: 1) the significant heterogeneity between multimodal data hampers modality fusion. 2) Effectively exploring consistency and variability information from similar diseases for enhancing model performance is difficult. In this paper, we propose a novel unified frame work, termed dual adaptive disentangled representation learning (DADRL), to simultaneously achieve disease-shared and disease specific biomarker detection as well as disease diagnosis. Our DADRL comprises three components: 1) a biology information constraints-based modality fusion strategy is applied to adaptively explore inter- and intra-modal correlations, thereby effectively fusing multimodal data. 2) A unified framework that integrates modality fusion and disease diagnosis is proposed to mine disease-related information for simultaneously accomplishing disease-related biomarker detection and disease diagnosis. 3) Disentangled representation learning and several adaptive metric constraints are incorporated into the unified framework to adaptively separate disease-specific information from disease shared feature representations for effectively identifying disease shared and disease-specific biomarkers, thereby deepening the understanding of disease pathogenesis. Extensive experiments on multiple real datasets and simulated data demonstrate that our method significantly improves performance of biomarker detection and disease diagnosis.&lt;/p&gt;</content:encoded></item><item><title>Enhancing MMDiT-Based Text-to-Image Models for Similar Subject Generation</title><link>https://doi.org/10.1109/tpami.2026.3663759</link><guid>10.1109/tpami.2026.3663759</guid><pubDate>Wed, 11 Feb 2026 20:55:51 +0000</pubDate><dc:creator>Tianyi Wei</dc:creator><dc:creator>Dongdong Chen</dc:creator><dc:creator>Yifan Zhou</dc:creator><dc:creator>Xingang Pan</dc:creator><prism:publicationName>IEEE Transactions on Pattern Analysis and Machine Intelligence</prism:publicationName><prism:doi>10.1109/tpami.2026.3663759</prism:doi><description>Representing the cutting-edge technique of text-to-image models, the latest Multimodal Diffusion Transformer (MMDiT) largely mitigates many generation issues existing in previous models. However, we discover that it still suffers from subject neglect or mixing when the input text prompt contains multiple subjects of similar semantics or appearance. We identify three possible ambiguities within the MMDiT architecture that cause this problem: Inter-block Ambiguity, Text Encoder Ambiguity, and Semantic Ambiguity. To address these issues, we propose to repair the ambiguous latent on-the-fly by test-time optimization at early denoising steps. In detail, we design three loss functions: Block Alignment Loss, Text Encoder Alignment Loss, and Overlap Loss, each tailored to mitigate these ambiguities. Despite significant improvements, we observe that semantic ambiguity persists when generating multiple similar subjects, as the guidance provided by overlap loss is not explicit enough. Therefore, we further propose Overlap Online Detection and Back-to-Start Sampling Strategy to alleviate the problem. Experimental results on a newly constructed challenging dataset of similar subjects validate the effectiveness of our approach, showing superior generation quality and much higher success rates over existing methods. The consistent and substantial improvements observed across multiple MMDiT based text-to-image models such as SD3, SD3.5 and FLUX provide strong evidence of the general applicability of our method. Project page: https://wtybest.github.io/projects/EnMMDiT/
Published: 2026-02-11T20:55:51+00:00
Venue: IEEE Transactions on Pattern Analysis and Machine Intelligence
Score: 0.796 (must_read)</description><content:encoded>&lt;p&gt;&lt;strong&gt;Authors:&lt;/strong&gt; Tianyi Wei; Dongdong Chen; Yifan Zhou; Xingang Pan&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Journal:&lt;/strong&gt; IEEE Transactions on Pattern Analysis and Machine Intelligence&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;a href="https://doi.org/10.1109/tpami.2026.3663759"&gt;10.1109/tpami.2026.3663759&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Relevance:&lt;/strong&gt; 0.796 (must_read)&lt;/p&gt;
&lt;p&gt;Representing the cutting-edge technique of text-to-image models, the latest Multimodal Diffusion Transformer (MMDiT) largely mitigates many generation issues existing in previous models. However, we discover that it still suffers from subject neglect or mixing when the input text prompt contains multiple subjects of similar semantics or appearance. We identify three possible ambiguities within the MMDiT architecture that cause this problem: Inter-block Ambiguity, Text Encoder Ambiguity, and Semantic Ambiguity. To address these issues, we propose to repair the ambiguous latent on-the-fly by test-time optimization at early denoising steps. In detail, we design three loss functions: Block Alignment Loss, Text Encoder Alignment Loss, and Overlap Loss, each tailored to mitigate these ambiguities. Despite significant improvements, we observe that semantic ambiguity persists when generating multiple similar subjects, as the guidance provided by overlap loss is not explicit enough. Therefore, we further propose Overlap Online Detection and Back-to-Start Sampling Strategy to alleviate the problem. Experimental results on a newly constructed challenging dataset of similar subjects validate the effectiveness of our approach, showing superior generation quality and much higher success rates over existing methods. The consistent and substantial improvements observed across multiple MMDiT based text-to-image models such as SD3, SD3.5 and FLUX provide strong evidence of the general applicability of our method. Project page: https://wtybest.github.io/projects/EnMMDiT/&lt;/p&gt;</content:encoded></item><item><title>ADVersa: Abductive Driving Accident Video Understanding</title><link>https://doi.org/10.1109/tpami.2026.3663545</link><guid>10.1109/tpami.2026.3663545</guid><pubDate>Wed, 11 Feb 2026 20:55:51 +0000</pubDate><dc:creator>Lei-Lei Li</dc:creator><dc:creator>Jianwu Fang</dc:creator><dc:creator>Junbin Xiao</dc:creator><dc:creator>Hongkai Yu</dc:creator><dc:creator>Chen Lv</dc:creator><dc:creator>Jianru Xue</dc:creator><dc:creator>Zhengguo Li</dc:creator><dc:creator>Tat-Seng Chua</dc:creator><prism:publicationName>IEEE Transactions on Pattern Analysis and Machine Intelligence</prism:publicationName><prism:doi>10.1109/tpami.2026.3663545</prism:doi><description>Understanding traffic accident scenes is a long-standing research for vision-based safe driving. It seeks to answer why accidents occur, how near-crash scenes develop, and what the key elements of an accident are. This research is challenging due to the scarcity and fragmentation of accident data, as well as the complex accident environments. To study this, we present a framework of Abductive Driving accident Video understanding (ADVersa), which infers a plausible visual and textual explanation for the absent near-crash scenes. ADVersa underscores three groups of tasks: 1) visual past recovery of near-crash scenes, 2) visual prediction of near-crash scenes, and 3) accident cause involved video synthesis. To support the study, we first contribute MM-AU, a novel dataset for Multi-Modal Accident video Understanding. MM-AU contains 11,727 in-the-wild driving accident videos with temporally aligned text descriptions, 2.23 million well-annotated object boxes, and 58,650 pairs of video-based accident cause texts. We then propose an Abductive CLIP model and a Contrastive Graph Video Pre-training (CGVP) model, which exploit relation-aware cross-modal semantic learning to drive spatially abductive and temporally abductive accident video diffusion. Extensive experiments verify the superiority of ADVersa to the state-of-the-art approaches on different tasks, i.e., historical near-crash video frame recovering, crashing video frame prediction, textual accident cause and category reasoning, normal-to-accident video synthesis, and accident video editing. With these efforts, we hope this research can advance the progress on multimodal accident video understanding.
Published: 2026-02-11T20:55:51+00:00
Venue: IEEE Transactions on Pattern Analysis and Machine Intelligence
Score: 0.795 (must_read)</description><content:encoded>&lt;p&gt;&lt;strong&gt;Authors:&lt;/strong&gt; Lei-Lei Li; Jianwu Fang; Junbin Xiao; Hongkai Yu; Chen Lv; Jianru Xue; Zhengguo Li; Tat-Seng Chua&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Journal:&lt;/strong&gt; IEEE Transactions on Pattern Analysis and Machine Intelligence&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;a href="https://doi.org/10.1109/tpami.2026.3663545"&gt;10.1109/tpami.2026.3663545&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Relevance:&lt;/strong&gt; 0.795 (must_read)&lt;/p&gt;
&lt;p&gt;Understanding traffic accident scenes is a long-standing research for vision-based safe driving. It seeks to answer why accidents occur, how near-crash scenes develop, and what the key elements of an accident are. This research is challenging due to the scarcity and fragmentation of accident data, as well as the complex accident environments. To study this, we present a framework of Abductive Driving accident Video understanding (ADVersa), which infers a plausible visual and textual explanation for the absent near-crash scenes. ADVersa underscores three groups of tasks: 1) visual past recovery of near-crash scenes, 2) visual prediction of near-crash scenes, and 3) accident cause involved video synthesis. To support the study, we first contribute MM-AU, a novel dataset for Multi-Modal Accident video Understanding. MM-AU contains 11,727 in-the-wild driving accident videos with temporally aligned text descriptions, 2.23 million well-annotated object boxes, and 58,650 pairs of video-based accident cause texts. We then propose an Abductive CLIP model and a Contrastive Graph Video Pre-training (CGVP) model, which exploit relation-aware cross-modal semantic learning to drive spatially abductive and temporally abductive accident video diffusion. Extensive experiments verify the superiority of ADVersa to the state-of-the-art approaches on different tasks, i.e., historical near-crash video frame recovering, crashing video frame prediction, textual accident cause and category reasoning, normal-to-accident video synthesis, and accident video editing. With these efforts, we hope this research can advance the progress on multimodal accident video understanding.&lt;/p&gt;</content:encoded></item><item><title>非空间配准的多模态目标检测决策融合策略</title><link>https://doi.org/10.11834/jig.250326</link><guid>10.11834/jig.250326</guid><pubDate>Wed, 11 Feb 2026 07:28:27 +0000</pubDate><dc:creator>Zhang Rong</dc:creator><dc:creator>Yao Liang</dc:creator><dc:creator>Zhang Yixin</dc:creator><dc:creator>Wang Yijun</dc:creator><dc:creator>Zhang Chuanyi</dc:creator><dc:creator>Liu Fan</dc:creator><prism:publicationName>Journal of Image and Graphics</prism:publicationName><prism:doi>10.11834/jig.250326</prism:doi><description>目的多模态目标检测通过融合红外与可见光等多源传感器数据，有效提升了模型在复杂环境下的检测精度与鲁棒性。然而，现有方法普遍基于严格配准的多模态数据开展研究，不能直接适配实际应用中不同模态相机得到的非空间配准图像，在图像输入算法前仍需完成配准，这损失了实时性和灵活性。为此，提出一种非空间配准条件下的多模态目标检测任务，并设计了一种非空间配准的多模态目标检测决策融合方法。在数据层面，利用3种公共数据集模拟双光载荷拍摄得到的非空间配准图像对，在不引入额外标注成本的前提下，为新的任务提供了基准数据。在算法层面，设计了一种基于图结构的非空间配准决策融合方法。方法首先，根据不同模态检测器的检测结果构建带权有向图，实现不同模态目标的图结构化表示；接着，利用图结构中目标间的相对位置关系，实现跨模态目标的自适应匹配；最终，对匹配成功的目标进行决策融合，并设计了模态迁移策略以实现多模态信息的高效互补。结果在3个数据集上的实验结果表明，本文方法在非空间配准场景下较单模态检测器实现了最大10.03%的漏检率降幅。同时，该方法在配准数据集上同样适用，相较于多光谱行人检测Transformer（multi spectral pedestrian detection Transformer，MS-DETR）、动态自适应多光谱检测Transformer（dynamic adaptive multispectral detection Transformer，DAMSDet）等先进多模态目标检测方法，检测准确率提升了6.8%。结论本文所提出的非空间配准多模态目标检测决策融合方法，能够很好地适应存在空间差异的实际场景，并且相比其他先进的多模态目标检测模型，有更高的准确率和鲁棒性。相关的代码与数据集将在此仓库公开：https://github.com/1e12Leon/ProbDet。
Published: 2026-02-11T07:28:27+00:00
Venue: Journal of Image and Graphics
Score: 0.795 (must_read)</description><content:encoded>&lt;p&gt;&lt;strong&gt;Authors:&lt;/strong&gt; Zhang Rong; Yao Liang; Zhang Yixin; Wang Yijun; Zhang Chuanyi; Liu Fan&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Journal:&lt;/strong&gt; Journal of Image and Graphics&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;a href="https://doi.org/10.11834/jig.250326"&gt;10.11834/jig.250326&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Relevance:&lt;/strong&gt; 0.795 (must_read)&lt;/p&gt;
&lt;p&gt;目的多模态目标检测通过融合红外与可见光等多源传感器数据，有效提升了模型在复杂环境下的检测精度与鲁棒性。然而，现有方法普遍基于严格配准的多模态数据开展研究，不能直接适配实际应用中不同模态相机得到的非空间配准图像，在图像输入算法前仍需完成配准，这损失了实时性和灵活性。为此，提出一种非空间配准条件下的多模态目标检测任务，并设计了一种非空间配准的多模态目标检测决策融合方法。在数据层面，利用3种公共数据集模拟双光载荷拍摄得到的非空间配准图像对，在不引入额外标注成本的前提下，为新的任务提供了基准数据。在算法层面，设计了一种基于图结构的非空间配准决策融合方法。方法首先，根据不同模态检测器的检测结果构建带权有向图，实现不同模态目标的图结构化表示；接着，利用图结构中目标间的相对位置关系，实现跨模态目标的自适应匹配；最终，对匹配成功的目标进行决策融合，并设计了模态迁移策略以实现多模态信息的高效互补。结果在3个数据集上的实验结果表明，本文方法在非空间配准场景下较单模态检测器实现了最大10.03%的漏检率降幅。同时，该方法在配准数据集上同样适用，相较于多光谱行人检测Transformer（multi spectral pedestrian detection Transformer，MS-DETR）、动态自适应多光谱检测Transformer（dynamic adaptive multispectral detection Transformer，DAMSDet）等先进多模态目标检测方法，检测准确率提升了6.8%。结论本文所提出的非空间配准多模态目标检测决策融合方法，能够很好地适应存在空间差异的实际场景，并且相比其他先进的多模态目标检测模型，有更高的准确率和鲁棒性。相关的代码与数据集将在此仓库公开：https://github.com/1e12Leon/ProbDet。&lt;/p&gt;</content:encoded></item><item><title>AurigaNet: A Real-Time Multi-Task Network for Enhanced Urban Driving Perception</title><link>https://arxiv.org/abs/2602.10660v1</link><guid>http://arxiv.org/abs/2602.10660v1</guid><pubDate>Wed, 11 Feb 2026 09:04:29 +0000</pubDate><dc:creator>Kiarash Ghasemzadeh</dc:creator><dc:creator>Sedigheh Dehghani</dc:creator><dc:type>preprint</dc:type><prism:publicationName>arXiv</prism:publicationName><description>Self-driving cars hold significant potential to reduce traffic accidents, alleviate congestion, and enhance urban mobility. However, developing reliable AI systems for autonomous vehicles remains a substantial challenge. Over the past decade, multi-task learning has emerged as a powerful approach to address complex problems in driving perception. Multi-task networks offer several advantages, including increased computational efficiency, real-time processing capabilities, optimized resource utilization, and improved generalization. In this study, we present AurigaNet, an advanced multi-task network architecture designed to push the boundaries of autonomous driving perception. AurigaNet integrates three critical tasks: object detection, lane detection, and drivable area instance segmentation. The system is trained and evaluated using the BDD100K dataset, renowned for its diversity in driving conditions. Key innovations of AurigaNet include its end-to-end instance segmentation capability, which significantly enhances both accuracy and efficiency in path estimation for autonomous vehicles. Experimental results demonstrate that AurigaNet achieves an 85.2% IoU in drivable area segmentation, outperforming its closest competitor by 0.7%. In lane detection, AurigaNet achieves a remarkable 60.8% IoU, surpassing other models by more than 30%. Furthermore, the network achieves an mAP@0.5:0.95 of 47.6% in traffic object detection, exceeding the next leading model by 2.9%. Additionally, we validate the practical feasibility of AurigaNet by deploying it on embedded devices such as the Jetson Orin NX, where it demonstrates competitive real-time performance. These results underscore AurigaNet's potential as a robust and efficient solution for autonomous driving perception systems. The code can be found here https://github.com/KiaRational/AurigaNet.
Published: 2026-02-11T09:04:29+00:00
Venue: arXiv
Score: 0.795 (must_read)</description><content:encoded>&lt;p&gt;&lt;strong&gt;Authors:&lt;/strong&gt; Kiarash Ghasemzadeh; Sedigheh Dehghani&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Journal:&lt;/strong&gt; arXiv&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Relevance:&lt;/strong&gt; 0.795 (must_read)&lt;/p&gt;
&lt;p&gt;Self-driving cars hold significant potential to reduce traffic accidents, alleviate congestion, and enhance urban mobility. However, developing reliable AI systems for autonomous vehicles remains a substantial challenge. Over the past decade, multi-task learning has emerged as a powerful approach to address complex problems in driving perception. Multi-task networks offer several advantages, including increased computational efficiency, real-time processing capabilities, optimized resource utilization, and improved generalization. In this study, we present AurigaNet, an advanced multi-task network architecture designed to push the boundaries of autonomous driving perception. AurigaNet integrates three critical tasks: object detection, lane detection, and drivable area instance segmentation. The system is trained and evaluated using the BDD100K dataset, renowned for its diversity in driving conditions. Key innovations of AurigaNet include its end-to-end instance segmentation capability, which significantly enhances both accuracy and efficiency in path estimation for autonomous vehicles. Experimental results demonstrate that AurigaNet achieves an 85.2% IoU in drivable area segmentation, outperforming its closest competitor by 0.7%. In lane detection, AurigaNet achieves a remarkable 60.8% IoU, surpassing other models by more than 30%. Furthermore, the network achieves an mAP@0.5:0.95 of 47.6% in traffic object detection, exceeding the next leading model by 2.9%. Additionally, we validate the practical feasibility of AurigaNet by deploying it on embedded devices such as the Jetson Orin NX, where it demonstrates competitive real-time performance. These results underscore AurigaNet&amp;#x27;s potential as a robust and efficient solution for autonomous driving perception systems. The code can be found here https://github.com/KiaRational/AurigaNet.&lt;/p&gt;</content:encoded></item><item><title>When large language models are reliable for judging empathic communication</title><link>https://doi.org/10.1038/s42256-025-01169-6</link><guid>10.1038/s42256-025-01169-6</guid><pubDate>Wed, 11 Feb 2026 10:02:22 +0000</pubDate><dc:creator>Aakriti Kumar</dc:creator><dc:creator>Nalin Poungpeth</dc:creator><dc:creator>Diyi Yang</dc:creator><dc:creator>Erina Farrell</dc:creator><dc:creator>Bruce L. Lambert</dc:creator><dc:creator>Matthew Groh</dc:creator><prism:publicationName>Nature Machine Intelligence</prism:publicationName><prism:doi>10.1038/s42256-025-01169-6</prism:doi><description>Abstract Large language models (LLMs) excel at generating empathic responses in text-based conversations. But, how reliably do they judge the nuances of empathic communication? Here we investigate this question by comparing how experts, crowdworkers and LLMs annotate empathic communication across four evaluative frameworks drawn from psychology, natural language processing and communications applied to 200 real-world conversations where one speaker shares a personal problem and the other offers support. Drawing on 3,150 expert annotations, 2,844 crowd annotations and 3,150 LLM annotations, we assess interrater reliability between these three annotator groups. We find that expert agreement is high but varies across the frameworks’ subcomponents depending on their clarity, complexity and subjectivity. We show that expert agreement offers a more informative benchmark for contextualizing LLM performance than standard classification metrics. Across all four frameworks, LLMs consistently approach this expert level benchmark and exceed the reliability of crowdworkers. These results demonstrate how LLMs, when validated on specific tasks with appropriate benchmarks, can support transparency and oversight in emotionally sensitive applications including their use as conversational companions.
Published: 2026-02-11T10:02:22+00:00
Venue: Nature Machine Intelligence
Score: 0.794 (must_read)</description><content:encoded>&lt;p&gt;&lt;strong&gt;Authors:&lt;/strong&gt; Aakriti Kumar; Nalin Poungpeth; Diyi Yang; Erina Farrell; Bruce L. Lambert; Matthew Groh&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Journal:&lt;/strong&gt; Nature Machine Intelligence&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;a href="https://doi.org/10.1038/s42256-025-01169-6"&gt;10.1038/s42256-025-01169-6&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Relevance:&lt;/strong&gt; 0.794 (must_read)&lt;/p&gt;
&lt;p&gt;Abstract Large language models (LLMs) excel at generating empathic responses in text-based conversations. But, how reliably do they judge the nuances of empathic communication? Here we investigate this question by comparing how experts, crowdworkers and LLMs annotate empathic communication across four evaluative frameworks drawn from psychology, natural language processing and communications applied to 200 real-world conversations where one speaker shares a personal problem and the other offers support. Drawing on 3,150 expert annotations, 2,844 crowd annotations and 3,150 LLM annotations, we assess interrater reliability between these three annotator groups. We find that expert agreement is high but varies across the frameworks’ subcomponents depending on their clarity, complexity and subjectivity. We show that expert agreement offers a more informative benchmark for contextualizing LLM performance than standard classification metrics. Across all four frameworks, LLMs consistently approach this expert level benchmark and exceed the reliability of crowdworkers. These results demonstrate how LLMs, when validated on specific tasks with appropriate benchmarks, can support transparency and oversight in emotionally sensitive applications including their use as conversational companions.&lt;/p&gt;</content:encoded></item><item><title>CDFIT: A Transformer Using Cross-Modal Dual-Stream Feature Interaction for Multispectral Pedestrian Detection</title><link>https://doi.org/10.1109/tits.2025.3649738</link><guid>10.1109/tits.2025.3649738</guid><pubDate>Thu, 12 Feb 2026 21:01:24 +0000</pubDate><dc:creator>Zihao Huang</dc:creator><dc:creator>Wenshi Li</dc:creator><dc:creator>Yuzhen Zhang</dc:creator><dc:creator>Jiaren Guo</dc:creator><dc:creator>Jianyin Zheng</dc:creator><dc:creator>Guang Ji</dc:creator><dc:creator>Yanyun Tao</dc:creator><prism:publicationName>IEEE Transactions on Intelligent Transportation Systems</prism:publicationName><prism:doi>10.1109/tits.2025.3649738</prism:doi><description>Modality imbalance is a significant challenge for multi-modal interaction at various depths in multispectral pedestrian detection under varying illumination environments. To overcome the limitations of current cross attention in addressing the modality imbalance, we propose the Cross-Modal Dual-Stream Feature Interaction Transformer (CDFIT). CDFIT capitalizes on the Transformer’s ability to learn long-range dependencies, extracting global intra-modal and inter-modal correlations during the feature interaction phase. Crucially, in order to effectively eliminate the interference of the self-attention within one modality to the alternative one, we propose horizontal and vertical correlation decoupling modes to divide and reassemble the attention maps in CDFIT. This facilitates more purified inter-modal attention while preserving relevant intra-modal self-attention, reducing the information interference. Meanwhile, in CDFIT, we expand Transformer into dual-stream pathways to align and assemble the information from RGB and thermal modalities across depths separately, thereby greatly enhancing the performance of multispectral object detection. Comprehensive experiments and ablation studies on benchmark datasets demonstrate that CDFIT achieves superior performance compared with state-of-the-art methods.
Published: 2026-02-12T21:01:24+00:00
Venue: IEEE Transactions on Intelligent Transportation Systems
Score: 0.794 (must_read)</description><content:encoded>&lt;p&gt;&lt;strong&gt;Authors:&lt;/strong&gt; Zihao Huang; Wenshi Li; Yuzhen Zhang; Jiaren Guo; Jianyin Zheng; Guang Ji; Yanyun Tao&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Journal:&lt;/strong&gt; IEEE Transactions on Intelligent Transportation Systems&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;a href="https://doi.org/10.1109/tits.2025.3649738"&gt;10.1109/tits.2025.3649738&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Relevance:&lt;/strong&gt; 0.794 (must_read)&lt;/p&gt;
&lt;p&gt;Modality imbalance is a significant challenge for multi-modal interaction at various depths in multispectral pedestrian detection under varying illumination environments. To overcome the limitations of current cross attention in addressing the modality imbalance, we propose the Cross-Modal Dual-Stream Feature Interaction Transformer (CDFIT). CDFIT capitalizes on the Transformer’s ability to learn long-range dependencies, extracting global intra-modal and inter-modal correlations during the feature interaction phase. Crucially, in order to effectively eliminate the interference of the self-attention within one modality to the alternative one, we propose horizontal and vertical correlation decoupling modes to divide and reassemble the attention maps in CDFIT. This facilitates more purified inter-modal attention while preserving relevant intra-modal self-attention, reducing the information interference. Meanwhile, in CDFIT, we expand Transformer into dual-stream pathways to align and assemble the information from RGB and thermal modalities across depths separately, thereby greatly enhancing the performance of multispectral object detection. Comprehensive experiments and ablation studies on benchmark datasets demonstrate that CDFIT achieves superior performance compared with state-of-the-art methods.&lt;/p&gt;</content:encoded></item><item><title>激光雷达智能处理关键技术研究进展</title><link>https://doi.org/10.11834/jig.250664</link><guid>10.11834/jig.250664</guid><pubDate>Wed, 11 Feb 2026 07:27:51 +0000</pubDate><dc:creator>Ao Sheng</dc:creator><dc:creator>Wen Chenglu</dc:creator><dc:creator>Li Wen</dc:creator><dc:creator>Liu Dunqiang</dc:creator><dc:creator>Xing Leyuan</dc:creator><dc:creator>Li Mingzhe</dc:creator><dc:creator>Guo Yulan</dc:creator><dc:creator>Wang Cheng</dc:creator><prism:publicationName>Journal of Image and Graphics</prism:publicationName><prism:doi>10.11834/jig.250664</prism:doi><description>激光雷达作为三维环境感知的核心传感器，在自动驾驶、机器人、增强现实等领域发挥着不可替代的作用。随着人工智能技术的快速发展，激光雷达智能处理技术已成为研究热点。本文围绕三维目标检测、激光雷达定位、人体动作捕捉与语言推理四大关键任务，对国内外研究进展进行了系统梳理与深入分析。首先，本文总结了该领域的核心任务定义与关键挑战。其次，本文结合任务特性，对相关技术进行了系统分类与方法解析，深入比较各类方法在不同场景下的适用性与性能优势。本文提及的算法、数据集和评估指标已汇总至https：//github.com/aosheng1996/DL4LiDAR。接下来，本文对国内外研究进展进行了对比分析，指出国外研究在模型体系与数据构建方面基础坚实，国内研究在算法效率与工程化落地方面发展迅速。最后，本文从算法融合、任务扩展与系统优化三个层面展望了激光雷达智能处理的未来发展趋势，以期为学术界与工业界提供理论参考，推动激光雷达智能处理技术的进一步发展。
Published: 2026-02-11T07:27:51+00:00
Venue: Journal of Image and Graphics
Score: 0.793 (must_read)</description><content:encoded>&lt;p&gt;&lt;strong&gt;Authors:&lt;/strong&gt; Ao Sheng; Wen Chenglu; Li Wen; Liu Dunqiang; Xing Leyuan; Li Mingzhe; Guo Yulan; Wang Cheng&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Journal:&lt;/strong&gt; Journal of Image and Graphics&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;a href="https://doi.org/10.11834/jig.250664"&gt;10.11834/jig.250664&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Relevance:&lt;/strong&gt; 0.793 (must_read)&lt;/p&gt;
&lt;p&gt;激光雷达作为三维环境感知的核心传感器，在自动驾驶、机器人、增强现实等领域发挥着不可替代的作用。随着人工智能技术的快速发展，激光雷达智能处理技术已成为研究热点。本文围绕三维目标检测、激光雷达定位、人体动作捕捉与语言推理四大关键任务，对国内外研究进展进行了系统梳理与深入分析。首先，本文总结了该领域的核心任务定义与关键挑战。其次，本文结合任务特性，对相关技术进行了系统分类与方法解析，深入比较各类方法在不同场景下的适用性与性能优势。本文提及的算法、数据集和评估指标已汇总至https：//github.com/aosheng1996/DL4LiDAR。接下来，本文对国内外研究进展进行了对比分析，指出国外研究在模型体系与数据构建方面基础坚实，国内研究在算法效率与工程化落地方面发展迅速。最后，本文从算法融合、任务扩展与系统优化三个层面展望了激光雷达智能处理的未来发展趋势，以期为学术界与工业界提供理论参考，推动激光雷达智能处理技术的进一步发展。&lt;/p&gt;</content:encoded></item><item><title>Distortion-Aware Depth Self-Updating for Self-Supervised Fisheye Monocular Depth Estimation</title><link>https://doi.org/10.1109/tip.2026.3661813</link><guid>10.1109/tip.2026.3661813</guid><pubDate>Wed, 11 Feb 2026 20:57:46 +0000</pubDate><dc:creator>Yihang Xu</dc:creator><dc:creator>Qiulei Dong</dc:creator><prism:publicationName>IEEE Transactions on Image Processing</prism:publicationName><prism:doi>10.1109/tip.2026.3661813</prism:doi><description>Self-supervised monocular depth estimation for fisheye cameras has attracted much attention in recent years due to their large view range. However, the performances of existing methods in this field are generally limited due to the inevitable severe distortions in fisheye images. To address this problem, we propose a distortion-aware depth self-updating network for self-supervised fisheye monocular depth estimation called DDS-Net. The proposed DDS-Net method employs a coarse-to-fine learning strategy, in which an explored fine depth predictor for predicting final depth is optimized with the predicted scene depths by a pretrained coarse depth predictor. The fine depth predictor contains a distortion-aware fisheye cost volume construction module and a depth self-updating module. The distortion-aware fisheye cost volume construction module is designed to construct a fisheye cost volume by learning the corresponding feature matching cost between continuous fisheye frames, which enables more accurate pixel-level depth cues to be captured under severe distortions. Based on the constructed cost volume and the initial depth estimated by the pretrained coarse depth predictor, the depth self-updating module is designed to self-update the depth map in an iterative manner. Extensive experimental results on 3 fisheye datasets demonstrate that the proposed method significantly outperforms 14 state-of-the-art methods for fisheye monocular depth estimation.
Published: 2026-02-11T20:57:46+00:00
Venue: IEEE Transactions on Image Processing
Score: 0.792 (must_read)</description><content:encoded>&lt;p&gt;&lt;strong&gt;Authors:&lt;/strong&gt; Yihang Xu; Qiulei Dong&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Journal:&lt;/strong&gt; IEEE Transactions on Image Processing&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;a href="https://doi.org/10.1109/tip.2026.3661813"&gt;10.1109/tip.2026.3661813&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Relevance:&lt;/strong&gt; 0.792 (must_read)&lt;/p&gt;
&lt;p&gt;Self-supervised monocular depth estimation for fisheye cameras has attracted much attention in recent years due to their large view range. However, the performances of existing methods in this field are generally limited due to the inevitable severe distortions in fisheye images. To address this problem, we propose a distortion-aware depth self-updating network for self-supervised fisheye monocular depth estimation called DDS-Net. The proposed DDS-Net method employs a coarse-to-fine learning strategy, in which an explored fine depth predictor for predicting final depth is optimized with the predicted scene depths by a pretrained coarse depth predictor. The fine depth predictor contains a distortion-aware fisheye cost volume construction module and a depth self-updating module. The distortion-aware fisheye cost volume construction module is designed to construct a fisheye cost volume by learning the corresponding feature matching cost between continuous fisheye frames, which enables more accurate pixel-level depth cues to be captured under severe distortions. Based on the constructed cost volume and the initial depth estimated by the pretrained coarse depth predictor, the depth self-updating module is designed to self-update the depth map in an iterative manner. Extensive experimental results on 3 fisheye datasets demonstrate that the proposed method significantly outperforms 14 state-of-the-art methods for fisheye monocular depth estimation.&lt;/p&gt;</content:encoded></item></channel></rss>