<?xml version='1.0' encoding='utf-8'?>
<rss xmlns:content="http://purl.org/rss/1.0/modules/content/" xmlns:dc="http://purl.org/dc/elements/1.1/" xmlns:prism="http://prismstandard.org/namespaces/basic/3.0/" version="2.0"><channel><title>ZotWatch Feed</title><link>https://ehehe.cn/zotwatch/</link><description>AI-assisted literature watch</description><lastBuildDate>Mon, 26 Jan 2026 02:58:31 +0000</lastBuildDate><item><title>Controlled Subspace Fusion for Language Model Continual Learning</title><link>https://doi.org/10.1016/j.inffus.2026.104184</link><guid>10.1016/j.inffus.2026.104184</guid><pubDate>Sat, 24 Jan 2026 23:23:54 +0000</pubDate><dc:creator>Xingcan Bao</dc:creator><dc:creator>Jianzhou Feng</dc:creator><dc:creator>Yiru Huo</dc:creator><dc:creator>Huaxiao Qiu</dc:creator><dc:creator>Haoran Yu</dc:creator><dc:creator>Shenyuan Ren</dc:creator><dc:creator>Jiadong Ren</dc:creator><prism:publicationName>Information Fusion</prism:publicationName><prism:doi>10.1016/j.inffus.2026.104184</prism:doi><description>Large language models (LLMs) have demonstrated remarkable performance across diverse natural language processing tasks. However, they still face significant challenges in multi-task continual learning, particularly in dynamic environments where tasks evolve sequentially and resources are constrained. Existing approaches typically learn separate adapter modules for each task, leading to a linear increase in parameters as tasks accumulate and thus hindering scalability and deployment efficiency. In this paper, we propose Controlled Subspace Fusion (CSF), a rehearsal-free and task-agnostic continual learning framework for language models that integrates knowledge across tasks while preventing parameter explosion. CSF introduces a shared low-rank projection subspace to provide a unified representational foundation, thereby enhancing consistency and facilitating cross-task knowledge transfer. In addition, we design an incremental subspace fusion mechanism that adaptively merges new task adapters with previously fused representations, while suppressing redundant parameter growth. As a result, the framework achieves scalable and robust knowledge fusion across sequential tasks. We evaluate CSF on mainstream architectures, including LLaMA and T5, across model scales ranging from 220M to 13B parameters. Experimental results on continual learning benchmarks demonstrate that CSF not only achieves superior average accuracy and parameter efficiency compared to existing approaches, but also provides a scalable and deployment-friendly solution that supports efficient knowledge fusion.
Published: 2026-01-24T23:23:54+00:00
Venue: Information Fusion
Score: 0.842 (must_read)</description><content:encoded>&lt;p&gt;&lt;strong&gt;Authors:&lt;/strong&gt; Xingcan Bao; Jianzhou Feng; Yiru Huo; Huaxiao Qiu; Haoran Yu; Shenyuan Ren; Jiadong Ren&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Journal:&lt;/strong&gt; Information Fusion&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;a href="https://doi.org/10.1016/j.inffus.2026.104184"&gt;10.1016/j.inffus.2026.104184&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Relevance:&lt;/strong&gt; 0.842 (must_read)&lt;/p&gt;
&lt;p&gt;Large language models (LLMs) have demonstrated remarkable performance across diverse natural language processing tasks. However, they still face significant challenges in multi-task continual learning, particularly in dynamic environments where tasks evolve sequentially and resources are constrained. Existing approaches typically learn separate adapter modules for each task, leading to a linear increase in parameters as tasks accumulate and thus hindering scalability and deployment efficiency. In this paper, we propose Controlled Subspace Fusion (CSF), a rehearsal-free and task-agnostic continual learning framework for language models that integrates knowledge across tasks while preventing parameter explosion. CSF introduces a shared low-rank projection subspace to provide a unified representational foundation, thereby enhancing consistency and facilitating cross-task knowledge transfer. In addition, we design an incremental subspace fusion mechanism that adaptively merges new task adapters with previously fused representations, while suppressing redundant parameter growth. As a result, the framework achieves scalable and robust knowledge fusion across sequential tasks. We evaluate CSF on mainstream architectures, including LLaMA and T5, across model scales ranging from 220M to 13B parameters. Experimental results on continual learning benchmarks demonstrate that CSF not only achieves superior average accuracy and parameter efficiency compared to existing approaches, but also provides a scalable and deployment-friendly solution that supports efficient knowledge fusion.&lt;/p&gt;</content:encoded></item><item><title>Entropy-Guided Condensing for Vision Transformer</title><link>https://doi.org/10.1007/s11263-026-02753-y</link><guid>10.1007/s11263-026-02753-y</guid><pubDate>Sat, 24 Jan 2026 18:57:21 +0000</pubDate><dc:creator>Sihao Lin</dc:creator><dc:creator>Pumeng Lyu</dc:creator><dc:creator>Dongrui Liu</dc:creator><dc:creator>Zhihui Li</dc:creator><dc:creator>Wenguan Wang</dc:creator><dc:creator>Xiaojun Chang</dc:creator><dc:creator>Yuhui Zheng</dc:creator><prism:publicationName>International Journal of Computer Vision</prism:publicationName><prism:doi>10.1007/s11263-026-02753-y</prism:doi><description>Recent success in the self-attention mechanism for the vision domain underscores the need for efficient vision transformers (ViTs). This work investigates the layer-wise learning capacity of ViT and aims to condense it along depth dimension by removing the uninformative layers, guided by transfer entropy. As an initial exploration, we inspect the condensation within a transformer block. Specifically, we identify that the MLP layer can elicit entropy on par with the attention layer within a block and some MLPs may be underutilized given low entropy. Therefore, we are motivated to integrate non-essential attention layers into their MLP counterparts by degenerating them into identical mapping, referred to as Dilution Learning, where a sparse mask is applied to the attention layer and decays during training. Although dilution learning is verified on a series of ViT architectures, it has shown instability in scale-enhanced ViT, such as DeiT-L, as the learnable scale is difficult to converge. The issue stems from the coupling of the decaying sparse mask with the unbounded learnable scale in the attention layers, making it difficult to be jointly optimized. To mitigate this problem, we use a simplified optimization strategy that alternatively optimizes the learnable scale and the sparse mask. In this way, we decouple their learning process and stabilize the training of scale-enhanced ViT. Additionally, our new approach can augment the previous layer-wise condensation to block-wise level, further enhancing efficiency. Our model series demonstrates superior results on a variety of vision tasks and benchmarks. For example, our method removes 50% attention layers or 30% transformer blocks of DeiT-B without performance compromise on ImageNet-1k.
Published: 2026-01-24T18:57:21+00:00
Venue: International Journal of Computer Vision
Score: 0.797 (must_read)</description><content:encoded>&lt;p&gt;&lt;strong&gt;Authors:&lt;/strong&gt; Sihao Lin; Pumeng Lyu; Dongrui Liu; Zhihui Li; Wenguan Wang; Xiaojun Chang; Yuhui Zheng&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Journal:&lt;/strong&gt; International Journal of Computer Vision&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;a href="https://doi.org/10.1007/s11263-026-02753-y"&gt;10.1007/s11263-026-02753-y&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Relevance:&lt;/strong&gt; 0.797 (must_read)&lt;/p&gt;
&lt;p&gt;Recent success in the self-attention mechanism for the vision domain underscores the need for efficient vision transformers (ViTs). This work investigates the layer-wise learning capacity of ViT and aims to condense it along depth dimension by removing the uninformative layers, guided by transfer entropy. As an initial exploration, we inspect the condensation within a transformer block. Specifically, we identify that the MLP layer can elicit entropy on par with the attention layer within a block and some MLPs may be underutilized given low entropy. Therefore, we are motivated to integrate non-essential attention layers into their MLP counterparts by degenerating them into identical mapping, referred to as Dilution Learning, where a sparse mask is applied to the attention layer and decays during training. Although dilution learning is verified on a series of ViT architectures, it has shown instability in scale-enhanced ViT, such as DeiT-L, as the learnable scale is difficult to converge. The issue stems from the coupling of the decaying sparse mask with the unbounded learnable scale in the attention layers, making it difficult to be jointly optimized. To mitigate this problem, we use a simplified optimization strategy that alternatively optimizes the learnable scale and the sparse mask. In this way, we decouple their learning process and stabilize the training of scale-enhanced ViT. Additionally, our new approach can augment the previous layer-wise condensation to block-wise level, further enhancing efficiency. Our model series demonstrates superior results on a variety of vision tasks and benchmarks. For example, our method removes 50% attention layers or 30% transformer blocks of DeiT-B without performance compromise on ImageNet-1k.&lt;/p&gt;</content:encoded></item><item><title>DCCS-Det: Directional Context and Cross-Scale-Aware Detector for Infrared Small Target</title><link>https://arxiv.org/abs/2601.16428v1</link><guid>http://arxiv.org/abs/2601.16428v1</guid><pubDate>Fri, 23 Jan 2026 03:53:59 +0000</pubDate><dc:creator>Shuying Li</dc:creator><dc:creator>Qiang Ma</dc:creator><dc:creator>San Zhang</dc:creator><dc:creator>Chuang Yang</dc:creator><dc:type>preprint</dc:type><prism:publicationName>arXiv</prism:publicationName><prism:doi>10.1109/TGRS.2025.3646345</prism:doi><description>Infrared small target detection (IRSTD) is critical for applications like remote sensing and surveillance, which aims to identify small, low-contrast targets against complex backgrounds. However, existing methods often struggle with inadequate joint modeling of local-global features (harming target-background discrimination) or feature redundancy and semantic dilution (degrading target representation quality). To tackle these issues, we propose DCCS-Det (Directional Context and Cross-Scale Aware Detector for Infrared Small Target), a novel detector that incorporates a Dual-stream Saliency Enhancement (DSE) block and a Latent-aware Semantic Extraction and Aggregation (LaSEA) module. The DSE block integrates localized perception with direction-aware context aggregation to help capture long-range spatial dependencies and local details. On this basis, the LaSEA module mitigates feature degradation via cross-scale feature extraction and random pooling sampling strategies, enhancing discriminative features and suppressing noise. Extensive experiments show that DCCS-Det achieves state-of-the-art detection accuracy with competitive efficiency across multiple datasets. Ablation studies further validate the contributions of DSE and LaSEA in improving target perception and feature representation under complex scenarios. \href{https://huggingface.co/InPeerReview/InfraredSmallTargetDetection-IRSTD.DCCS}{DCCS-Det Official Code is Available Here!}
Published: 2026-01-23T03:53:59+00:00
Venue: arXiv
Score: 0.795 (must_read)</description><content:encoded>&lt;p&gt;&lt;strong&gt;Authors:&lt;/strong&gt; Shuying Li; Qiang Ma; San Zhang; Chuang Yang&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Journal:&lt;/strong&gt; arXiv&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;a href="https://doi.org/10.1109/TGRS.2025.3646345"&gt;10.1109/TGRS.2025.3646345&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Relevance:&lt;/strong&gt; 0.795 (must_read)&lt;/p&gt;
&lt;p&gt;Infrared small target detection (IRSTD) is critical for applications like remote sensing and surveillance, which aims to identify small, low-contrast targets against complex backgrounds. However, existing methods often struggle with inadequate joint modeling of local-global features (harming target-background discrimination) or feature redundancy and semantic dilution (degrading target representation quality). To tackle these issues, we propose DCCS-Det (Directional Context and Cross-Scale Aware Detector for Infrared Small Target), a novel detector that incorporates a Dual-stream Saliency Enhancement (DSE) block and a Latent-aware Semantic Extraction and Aggregation (LaSEA) module. The DSE block integrates localized perception with direction-aware context aggregation to help capture long-range spatial dependencies and local details. On this basis, the LaSEA module mitigates feature degradation via cross-scale feature extraction and random pooling sampling strategies, enhancing discriminative features and suppressing noise. Extensive experiments show that DCCS-Det achieves state-of-the-art detection accuracy with competitive efficiency across multiple datasets. Ablation studies further validate the contributions of DSE and LaSEA in improving target perception and feature representation under complex scenarios. \href{https://huggingface.co/InPeerReview/InfraredSmallTargetDetection-IRSTD.DCCS}{DCCS-Det Official Code is Available Here!}&lt;/p&gt;</content:encoded></item><item><title>MDAFNet: Multiscale Differential Edge and Adaptive Frequency Guided Network for Infrared Small Target Detection</title><link>https://arxiv.org/abs/2601.16434v1</link><guid>http://arxiv.org/abs/2601.16434v1</guid><pubDate>Fri, 23 Jan 2026 04:16:16 +0000</pubDate><dc:creator>Shuying Li</dc:creator><dc:creator>Qiang Ma</dc:creator><dc:creator>San Zhang</dc:creator><dc:creator>Wuwei Wang</dc:creator><dc:creator>Chuang Yang</dc:creator><dc:type>preprint</dc:type><prism:publicationName>arXiv</prism:publicationName><prism:doi>10.1109/LGRS.2025.3645669</prism:doi><description>Infrared small target detection (IRSTD) plays a crucial role in numerous military and civilian applications. However, existing methods often face the gradual degradation of target edge pixels as the number of network layers increases, and traditional convolution struggles to differentiate between frequency components during feature extraction, leading to low-frequency backgrounds interfering with high-frequency targets and high-frequency noise triggering false detections. To address these limitations, we propose MDAFNet (Multi-scale Differential Edge and Adaptive Frequency Guided Network for Infrared Small Target Detection), which integrates the Multi-Scale Differential Edge (MSDE) module and Dual-Domain Adaptive Feature Enhancement (DAFE) module. The MSDE module, through a multi-scale edge extraction and enhancement mechanism, effectively compensates for the cumulative loss of target edge information during downsampling. The DAFE module combines frequency domain processing mechanisms with simulated frequency decomposition and fusion mechanisms in the spatial domain to effectively improve the network's capability to adaptively enhance high-frequency targets and selectively suppress high-frequency noise. Experimental results on multiple datasets demonstrate the superior detection performance of MDAFNet.
Published: 2026-01-23T04:16:16+00:00
Venue: arXiv
Score: 0.794 (must_read)</description><content:encoded>&lt;p&gt;&lt;strong&gt;Authors:&lt;/strong&gt; Shuying Li; Qiang Ma; San Zhang; Wuwei Wang; Chuang Yang&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Journal:&lt;/strong&gt; arXiv&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;a href="https://doi.org/10.1109/LGRS.2025.3645669"&gt;10.1109/LGRS.2025.3645669&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Relevance:&lt;/strong&gt; 0.794 (must_read)&lt;/p&gt;
&lt;p&gt;Infrared small target detection (IRSTD) plays a crucial role in numerous military and civilian applications. However, existing methods often face the gradual degradation of target edge pixels as the number of network layers increases, and traditional convolution struggles to differentiate between frequency components during feature extraction, leading to low-frequency backgrounds interfering with high-frequency targets and high-frequency noise triggering false detections. To address these limitations, we propose MDAFNet (Multi-scale Differential Edge and Adaptive Frequency Guided Network for Infrared Small Target Detection), which integrates the Multi-Scale Differential Edge (MSDE) module and Dual-Domain Adaptive Feature Enhancement (DAFE) module. The MSDE module, through a multi-scale edge extraction and enhancement mechanism, effectively compensates for the cumulative loss of target edge information during downsampling. The DAFE module combines frequency domain processing mechanisms with simulated frequency decomposition and fusion mechanisms in the spatial domain to effectively improve the network&amp;#x27;s capability to adaptively enhance high-frequency targets and selectively suppress high-frequency noise. Experimental results on multiple datasets demonstrate the superior detection performance of MDAFNet.&lt;/p&gt;</content:encoded></item><item><title>Reasoning Promotes Robustness in Theory of Mind Tasks</title><link>https://arxiv.org/abs/2601.16853v1</link><guid>http://arxiv.org/abs/2601.16853v1</guid><pubDate>Fri, 23 Jan 2026 16:01:24 +0000</pubDate><dc:creator>Ian B. de Haan</dc:creator><dc:creator>Peter van der Putten</dc:creator><dc:creator>Max van Duijn</dc:creator><dc:type>preprint</dc:type><prism:publicationName>arXiv</prism:publicationName><description>Large language models (LLMs) have recently shown strong performance on Theory of Mind (ToM) tests, prompting debate about the nature and true performance of the underlying capabilities. At the same time, reasoning-oriented LLMs trained via reinforcement learning with verifiable rewards (RLVR) have achieved notable improvements across a range of benchmarks. This paper examines the behavior of such reasoning models in ToM tasks, using novel adaptations of machine psychological experiments and results from established benchmarks. We observe that reasoning models consistently exhibit increased robustness to prompt variations and task perturbations. Our analysis indicates that the observed gains are more plausibly attributed to increased robustness in finding the correct solution, rather than to fundamentally new forms of ToM reasoning. We discuss the implications of this interpretation for evaluating social-cognitive behavior in LLMs.
Published: 2026-01-23T16:01:24+00:00
Venue: arXiv
Score: 0.792 (must_read)</description><content:encoded>&lt;p&gt;&lt;strong&gt;Authors:&lt;/strong&gt; Ian B. de Haan; Peter van der Putten; Max van Duijn&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Journal:&lt;/strong&gt; arXiv&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Relevance:&lt;/strong&gt; 0.792 (must_read)&lt;/p&gt;
&lt;p&gt;Large language models (LLMs) have recently shown strong performance on Theory of Mind (ToM) tests, prompting debate about the nature and true performance of the underlying capabilities. At the same time, reasoning-oriented LLMs trained via reinforcement learning with verifiable rewards (RLVR) have achieved notable improvements across a range of benchmarks. This paper examines the behavior of such reasoning models in ToM tasks, using novel adaptations of machine psychological experiments and results from established benchmarks. We observe that reasoning models consistently exhibit increased robustness to prompt variations and task perturbations. Our analysis indicates that the observed gains are more plausibly attributed to increased robustness in finding the correct solution, rather than to fundamentally new forms of ToM reasoning. We discuss the implications of this interpretation for evaluating social-cognitive behavior in LLMs.&lt;/p&gt;</content:encoded></item><item><title>LongCat-Flash-Thinking-2601 Technical Report</title><link>https://arxiv.org/abs/2601.16725v1</link><guid>http://arxiv.org/abs/2601.16725v1</guid><pubDate>Fri, 23 Jan 2026 13:20:09 +0000</pubDate><dc:creator>Meituan LongCat Team</dc:creator><dc:creator>Anchun Gui</dc:creator><dc:creator>Bei Li</dc:creator><dc:creator>Bingyang Tao</dc:creator><dc:creator>Bole Zhou</dc:creator><dc:creator>Borun Chen</dc:creator><dc:creator>Chao Zhang</dc:creator><dc:creator>Chao Zhang</dc:creator><dc:creator>Chen Gao</dc:creator><dc:creator>Chen Zhang</dc:creator><dc:creator>Chengcheng Han</dc:creator><dc:creator>Chenhui Yang</dc:creator><dc:creator>Chuyu Zhang</dc:creator><dc:creator>Cong Chen</dc:creator><dc:creator>Cunguang Wang</dc:creator><dc:creator>Daoru Pan</dc:creator><dc:creator>Defei Bu</dc:creator><dc:creator>Dengchang Zhao</dc:creator><dc:creator>Di Xiu</dc:creator><dc:creator>Dishan Liu</dc:creator><dc:creator>Dongyu Ru</dc:creator><dc:creator>Dunwei Tu</dc:creator><dc:creator>Fan Wu</dc:creator><dc:creator>Fengcheng Yuan</dc:creator><dc:creator>Fengcun Li</dc:creator><dc:creator>Gang Xu</dc:creator><dc:creator>Guanyu Wu</dc:creator><dc:creator>Guoyuan Lin</dc:creator><dc:creator>Haibin Wang</dc:creator><dc:creator>Hansi Yang</dc:creator><dc:creator>Hao Yang</dc:creator><dc:creator>Haonan Yan</dc:creator><dc:creator>Haoxiang Ma</dc:creator><dc:creator>Haoxing Wen</dc:creator><dc:creator>Hongyan Hao</dc:creator><dc:creator>Hongyin Tang</dc:creator><dc:creator>Hongyu Zang</dc:creator><dc:creator>Hongzhi Ni</dc:creator><dc:creator>Hui Su</dc:creator><dc:creator>Jiacheng Zhang</dc:creator><dc:creator>Jiahong Zhou</dc:creator><dc:creator>Jiahuan Li</dc:creator><dc:creator>Jiaming Wang</dc:creator><dc:creator>Jian Yang</dc:creator><dc:creator>Jianfei Zhang</dc:creator><dc:creator>Jianhao Xu</dc:creator><dc:creator>Jianing Wang</dc:creator><dc:creator>Jiapeng Zhu</dc:creator><dc:creator>Jiaqi Sun</dc:creator><dc:creator>Jiarong Shi</dc:creator><dc:creator>Jiarui Zhao</dc:creator><dc:creator>Jingang Wang</dc:creator><dc:creator>Jinluan Yang</dc:creator><dc:creator>Jinrui Ding</dc:creator><dc:creator>Jinwei Xiao</dc:creator><dc:creator>Jiyuan He</dc:creator><dc:creator>Juncan Xu</dc:creator><dc:creator>Kefeng Zhang</dc:creator><dc:creator>Keheng Wang</dc:creator><dc:creator>Li Wei</dc:creator><dc:creator>Lianhui Ma</dc:creator><dc:creator>Lin Qiu</dc:creator><dc:creator>Lingbing Kong</dc:creator><dc:creator>Lingchuan Liu</dc:creator><dc:creator>Linsen Guo</dc:creator><dc:creator>Mengshen Zhu</dc:creator><dc:creator>Mengxia Shen</dc:creator><dc:creator>Mingyang Zhu</dc:creator><dc:creator>Peiguang Li</dc:creator><dc:creator>Peng Pei</dc:creator><dc:creator>Pengcheng Jia</dc:creator><dc:creator>Pengtao Zhang</dc:creator><dc:creator>Peng Zhao</dc:creator><dc:creator>Qi Gu</dc:creator><dc:creator>Qiong Huang</dc:creator><dc:creator>Qiyuan Duan</dc:creator><dc:creator>Quanchi Weng</dc:creator><dc:creator>Rongxiang Weng</dc:creator><dc:creator>Rongzhi Zhang</dc:creator><dc:creator>Rumei Li</dc:creator><dc:creator>Shanglin Lei</dc:creator><dc:creator>Shengnan An</dc:creator><dc:creator>Shijun Dai</dc:creator><dc:creator>Shuaikang Liu</dc:creator><dc:creator>Shuang Zhou</dc:creator><dc:creator>Shuo Wang</dc:creator><dc:creator>Songyuan Zhao</dc:creator><dc:creator>Tao Liang</dc:creator><dc:creator>Tianhao Hu</dc:creator><dc:creator>Tianze Chen</dc:creator><dc:creator>Wei Liu</dc:creator><dc:creator>Wei Shi</dc:creator><dc:creator>Wei Wang</dc:creator><dc:creator>Weifeng Tang</dc:creator><dc:creator>Wenjie Shi</dc:creator><dc:creator>Wenlong Zhu</dc:creator><dc:creator>Wentao Chen</dc:creator><dc:creator>Wentao Shi</dc:creator><dc:creator>Xi Su</dc:creator><dc:creator>Xiangcheng Liu</dc:creator><dc:creator>Xiandi Ma</dc:creator><dc:creator>Xiangyu Xi</dc:creator><dc:creator>Xiangyuan Liu</dc:creator><dc:creator>Xiangzhou Huang</dc:creator><dc:creator>Xiao Liu</dc:creator><dc:creator>Xiaodong Cai</dc:creator><dc:creator>Xiaolong Chen</dc:creator><dc:creator>Xiaowei Shi</dc:creator><dc:creator>Xiaoyu Li</dc:creator><dc:creator>Xin Chen</dc:creator><dc:creator>Xingchen Liu</dc:creator><dc:creator>Xuan Huang</dc:creator><dc:creator>Xuezhi Cao</dc:creator><dc:creator>Xunliang Cai</dc:creator><dc:creator>Yan Chen</dc:creator><dc:creator>Yang Bai</dc:creator><dc:creator>Yang Liu</dc:creator><dc:creator>Yang Yang</dc:creator><dc:creator>Yang Zheng</dc:creator><dc:creator>Yaoming Wang</dc:creator><dc:creator>Yaoming Zhu</dc:creator><dc:creator>Yaqi Huo</dc:creator><dc:creator>Yanyu Chen</dc:creator><dc:creator>Yaorui Shi</dc:creator><dc:creator>Yerui Sun</dc:creator><dc:creator>Yi Zhang</dc:creator><dc:creator>Yihao Chen</dc:creator><dc:creator>Yi-Kai Zhang</dc:creator><dc:creator>Yifan Lu</dc:creator><dc:creator>Yifan Zhao</dc:creator><dc:creator>Yitao Zhai</dc:creator><dc:creator>Yongjing Yin</dc:creator><dc:creator>Yongwei Zhou</dc:creator><dc:creator>Youshao Xiao</dc:creator><dc:creator>Yuchuan Dai</dc:creator><dc:creator>Yuchen Xie</dc:creator><dc:creator>Yuchen Yu</dc:creator><dc:creator>Yufei Zhang</dc:creator><dc:creator>Yuhuai Wei</dc:creator><dc:creator>Yulei Qian</dc:creator><dc:creator>Yunfan Liang</dc:creator><dc:creator>Yunke Zhao</dc:creator><dc:creator>Yuwei Jiang</dc:creator><dc:creator>Yuxin Bian</dc:creator><dc:creator>Yuxin Chen</dc:creator><dc:creator>Yuxin Liu</dc:creator><dc:creator>Yue Xu</dc:creator><dc:creator>Yueqing Sun</dc:creator><dc:creator>Zeyang Yu</dc:creator><dc:creator>Zhao Yang</dc:creator><dc:creator>Zhengsheng Huang</dc:creator><dc:creator>Zhengyu Chen</dc:creator><dc:creator>Zhijian Liu</dc:creator><dc:creator>Zhikang Xia</dc:creator><dc:creator>Zhimin Lin</dc:creator><dc:creator>Zhiyuan Yao</dc:creator><dc:creator>Zhuofan Chen</dc:creator><dc:creator>Zhuowen Han</dc:creator><dc:creator>Zijian Zhang</dc:creator><dc:creator>Ziran Li</dc:creator><dc:creator>Ziwen Wang</dc:creator><dc:creator>Ziyuan Zhuang</dc:creator><dc:type>preprint</dc:type><prism:publicationName>arXiv</prism:publicationName><description>We introduce LongCat-Flash-Thinking-2601, a 560-billion-parameter open-source Mixture-of-Experts (MoE) reasoning model with superior agentic reasoning capability. LongCat-Flash-Thinking-2601 achieves state-of-the-art performance among open-source models on a wide range of agentic benchmarks, including agentic search, agentic tool use, and tool-integrated reasoning. Beyond benchmark performance, the model demonstrates strong generalization to complex tool interactions and robust behavior under noisy real-world environments. Its advanced capability stems from a unified training framework that combines domain-parallel expert training with subsequent fusion, together with an end-to-end co-design of data construction, environments, algorithms, and infrastructure spanning from pre-training to post-training. In particular, the model's strong generalization capability in complex tool-use are driven by our in-depth exploration of environment scaling and principled task construction. To optimize long-tailed, skewed generation and multi-turn agentic interactions, and to enable stable training across over 10,000 environments spanning more than 20 domains, we systematically extend our asynchronous reinforcement learning framework, DORA, for stable and efficient large-scale multi-environment training. Furthermore, recognizing that real-world tasks are inherently noisy, we conduct a systematic analysis and decomposition of real-world noise patterns, and design targeted training procedures to explicitly incorporate such imperfections into the training process, resulting in improved robustness for real-world applications. To further enhance performance on complex reasoning tasks, we introduce a Heavy Thinking mode that enables effective test-time scaling by jointly expanding reasoning depth and width through intensive parallel thinking.
Published: 2026-01-23T13:20:09+00:00
Venue: arXiv
Score: 0.792 (must_read)</description><content:encoded>&lt;p&gt;&lt;strong&gt;Authors:&lt;/strong&gt; Meituan LongCat Team; Anchun Gui; Bei Li; Bingyang Tao; Bole Zhou; Borun Chen; Chao Zhang; Chao Zhang; Chen Gao; Chen Zhang; Chengcheng Han; Chenhui Yang; Chuyu Zhang; Cong Chen; Cunguang Wang; Daoru Pan; Defei Bu; Dengchang Zhao; Di Xiu; Dishan Liu; Dongyu Ru; Dunwei Tu; Fan Wu; Fengcheng Yuan; Fengcun Li; Gang Xu; Guanyu Wu; Guoyuan Lin; Haibin Wang; Hansi Yang; Hao Yang; Haonan Yan; Haoxiang Ma; Haoxing Wen; Hongyan Hao; Hongyin Tang; Hongyu Zang; Hongzhi Ni; Hui Su; Jiacheng Zhang; Jiahong Zhou; Jiahuan Li; Jiaming Wang; Jian Yang; Jianfei Zhang; Jianhao Xu; Jianing Wang; Jiapeng Zhu; Jiaqi Sun; Jiarong Shi; Jiarui Zhao; Jingang Wang; Jinluan Yang; Jinrui Ding; Jinwei Xiao; Jiyuan He; Juncan Xu; Kefeng Zhang; Keheng Wang; Li Wei; Lianhui Ma; Lin Qiu; Lingbing Kong; Lingchuan Liu; Linsen Guo; Mengshen Zhu; Mengxia Shen; Mingyang Zhu; Peiguang Li; Peng Pei; Pengcheng Jia; Pengtao Zhang; Peng Zhao; Qi Gu; Qiong Huang; Qiyuan Duan; Quanchi Weng; Rongxiang Weng; Rongzhi Zhang; Rumei Li; Shanglin Lei; Shengnan An; Shijun Dai; Shuaikang Liu; Shuang Zhou; Shuo Wang; Songyuan Zhao; Tao Liang; Tianhao Hu; Tianze Chen; Wei Liu; Wei Shi; Wei Wang; Weifeng Tang; Wenjie Shi; Wenlong Zhu; Wentao Chen; Wentao Shi; Xi Su; Xiangcheng Liu; Xiandi Ma; Xiangyu Xi; Xiangyuan Liu; Xiangzhou Huang; Xiao Liu; Xiaodong Cai; Xiaolong Chen; Xiaowei Shi; Xiaoyu Li; Xin Chen; Xingchen Liu; Xuan Huang; Xuezhi Cao; Xunliang Cai; Yan Chen; Yang Bai; Yang Liu; Yang Yang; Yang Zheng; Yaoming Wang; Yaoming Zhu; Yaqi Huo; Yanyu Chen; Yaorui Shi; Yerui Sun; Yi Zhang; Yihao Chen; Yi-Kai Zhang; Yifan Lu; Yifan Zhao; Yitao Zhai; Yongjing Yin; Yongwei Zhou; Youshao Xiao; Yuchuan Dai; Yuchen Xie; Yuchen Yu; Yufei Zhang; Yuhuai Wei; Yulei Qian; Yunfan Liang; Yunke Zhao; Yuwei Jiang; Yuxin Bian; Yuxin Chen; Yuxin Liu; Yue Xu; Yueqing Sun; Zeyang Yu; Zhao Yang; Zhengsheng Huang; Zhengyu Chen; Zhijian Liu; Zhikang Xia; Zhimin Lin; Zhiyuan Yao; Zhuofan Chen; Zhuowen Han; Zijian Zhang; Ziran Li; Ziwen Wang; Ziyuan Zhuang&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Journal:&lt;/strong&gt; arXiv&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Relevance:&lt;/strong&gt; 0.792 (must_read)&lt;/p&gt;
&lt;p&gt;We introduce LongCat-Flash-Thinking-2601, a 560-billion-parameter open-source Mixture-of-Experts (MoE) reasoning model with superior agentic reasoning capability. LongCat-Flash-Thinking-2601 achieves state-of-the-art performance among open-source models on a wide range of agentic benchmarks, including agentic search, agentic tool use, and tool-integrated reasoning. Beyond benchmark performance, the model demonstrates strong generalization to complex tool interactions and robust behavior under noisy real-world environments. Its advanced capability stems from a unified training framework that combines domain-parallel expert training with subsequent fusion, together with an end-to-end co-design of data construction, environments, algorithms, and infrastructure spanning from pre-training to post-training. In particular, the model&amp;#x27;s strong generalization capability in complex tool-use are driven by our in-depth exploration of environment scaling and principled task construction. To optimize long-tailed, skewed generation and multi-turn agentic interactions, and to enable stable training across over 10,000 environments spanning more than 20 domains, we systematically extend our asynchronous reinforcement learning framework, DORA, for stable and efficient large-scale multi-environment training. Furthermore, recognizing that real-world tasks are inherently noisy, we conduct a systematic analysis and decomposition of real-world noise patterns, and design targeted training procedures to explicitly incorporate such imperfections into the training process, resulting in improved robustness for real-world applications. To further enhance performance on complex reasoning tasks, we introduce a Heavy Thinking mode that enables effective test-time scaling by jointly expanding reasoning depth and width through intensive parallel thinking.&lt;/p&gt;</content:encoded></item><item><title>PoseAdapter: Efficiently Transferring 2D Human Pose Estimator to 3D Whole-Body Task via Adapter</title><link>https://doi.org/10.1016/j.patcog.2026.113154</link><guid>10.1016/j.patcog.2026.113154</guid><pubDate>Sat, 24 Jan 2026 16:09:04 +0000</pubDate><dc:creator>Ze Feng</dc:creator><dc:creator>Sen Yang</dc:creator><dc:creator>Jiang-Jiang Liu</dc:creator><dc:creator>Wankou Yang</dc:creator><prism:publicationName>Pattern Recognition</prism:publicationName><prism:doi>10.1016/j.patcog.2026.113154</prism:doi><description>In this paper, we explore the task of 3D whole-body pose estimation based on a single-frame image and propose a new paradigm called PoseAdapter, which exploits a well-pretrained 2D human pose estimation model equipped with Adapter. The mainstream paradigms for 3D human pose estimation typically require multiple stages, such as human box detection, 2D pose estimation, and lifting to 3D coordinates. Such a multi-stage approach probably loses context information in the compression process, resulting in inferior pose results, particularly for the dense prediction tasks such as 3D Whole-Body pose estimation. To improve the accuracy of pose estimation, some methods even use multi-frame fusion to enhance the current pose, including input from future frames, which is inherently non-causal. Considering that end-to-end 2D human pose methods could extract human-related and keypoint-specific visual features, we want to employ them as a general vision-based human analysis model and enable it to predict 3D whole-body poses. By freezing most of the parameters of the 2D model and tuning the newly added adapter, PoseAdapter could transfer the 2D estimator to the 3D pose task in a parameter-efficient manner, while retaining the original ability of distinguishing multiple human instances. Quantitative experimental results on H3WB demonstrate that PoseAdapter with fewer trainable parameters achieves an accuracy of 62.74mm MPJPE. Qualitative research also shows that PoseAdapter could predict multi-person 3D Whole-Body pose results and can generalize to out-of-domain datasets, such as COCO.
Published: 2026-01-24T16:09:04+00:00
Venue: Pattern Recognition
Score: 0.791 (must_read)</description><content:encoded>&lt;p&gt;&lt;strong&gt;Authors:&lt;/strong&gt; Ze Feng; Sen Yang; Jiang-Jiang Liu; Wankou Yang&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Journal:&lt;/strong&gt; Pattern Recognition&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;a href="https://doi.org/10.1016/j.patcog.2026.113154"&gt;10.1016/j.patcog.2026.113154&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Relevance:&lt;/strong&gt; 0.791 (must_read)&lt;/p&gt;
&lt;p&gt;In this paper, we explore the task of 3D whole-body pose estimation based on a single-frame image and propose a new paradigm called PoseAdapter, which exploits a well-pretrained 2D human pose estimation model equipped with Adapter. The mainstream paradigms for 3D human pose estimation typically require multiple stages, such as human box detection, 2D pose estimation, and lifting to 3D coordinates. Such a multi-stage approach probably loses context information in the compression process, resulting in inferior pose results, particularly for the dense prediction tasks such as 3D Whole-Body pose estimation. To improve the accuracy of pose estimation, some methods even use multi-frame fusion to enhance the current pose, including input from future frames, which is inherently non-causal. Considering that end-to-end 2D human pose methods could extract human-related and keypoint-specific visual features, we want to employ them as a general vision-based human analysis model and enable it to predict 3D whole-body poses. By freezing most of the parameters of the 2D model and tuning the newly added adapter, PoseAdapter could transfer the 2D estimator to the 3D pose task in a parameter-efficient manner, while retaining the original ability of distinguishing multiple human instances. Quantitative experimental results on H3WB demonstrate that PoseAdapter with fewer trainable parameters achieves an accuracy of 62.74mm MPJPE. Qualitative research also shows that PoseAdapter could predict multi-person 3D Whole-Body pose results and can generalize to out-of-domain datasets, such as COCO.&lt;/p&gt;</content:encoded></item><item><title>S2I-DiT: Unlocking the Semantic-to-Image Transferability by Fine-tuning Large Diffusion Transformer Models</title><link>https://doi.org/10.1016/j.patcog.2026.113158</link><guid>10.1016/j.patcog.2026.113158</guid><pubDate>Sun, 25 Jan 2026 15:29:56 +0000</pubDate><dc:creator>Gang Li</dc:creator><dc:creator>Enze Xie</dc:creator><dc:creator>Chongjian Ge</dc:creator><dc:creator>Xiang Li</dc:creator><dc:creator>Lingyu Si</dc:creator><dc:creator>Changwen Zheng</dc:creator><dc:creator>Zhenguo Li</dc:creator><prism:publicationName>Pattern Recognition</prism:publicationName><prism:doi>10.1016/j.patcog.2026.113158</prism:doi><description>Denoising Diffusion Probabilistic Models (DDPMs) have made significant progress in image generation. Recent works in semantic-to-image (S2I) synthesis have also shifted from the previously de facto GAN-based methods to DDPMs, yielding better results. However, these works mostly employ a U-Net structure and vanilla training-from-scratch scheme for S2I, unconsciously neglecting the potential benefits offered by task-related pre-training. In this work, we introduce a Transformer-based architecture, namely S2I-DiT, and reconsider the merits of a pre-trained large diffusion model for cross-task adaptation (i.e., from the class-conditional generation to S2I). In S2I-DiT, we propose the integration of semantic embedders within Diffusion Transformers (DiTs) to maximize the utilization of semantic information. The semantic embedder densely encodes semantic layouts to guide the adaptive normalization process. We configure semantic embedders in a layer-wise manner to learn pixel-level correspondence, enabling finer-grained semantic-to-image control. Besides, to fully unleash the cross-task transferability of DDPMs, we introduce a two-stage fine-tuning strategy, which involves initially adapting the semantic embedders in the pixel-level space, followed by fine-tuning the partial/entire model for cross-task adaptation. Notably, S2I-DiT pioneers the application of Large Diffusion Transformers to cross-task fine-tuning. Extensive experiments on four benchmark datasets demonstrate S2I-DiT’s effectiveness, as it achieves state-of-the-art performance in terms of quality (FID) and diversity (LPIPS), while consuming fewer training iterations. This work establishes a new state-of-the-art for semantic-to-image generation and provides valuable insights into cross-task transferability of large generative models.
Published: 2026-01-25T15:29:56+00:00
Venue: Pattern Recognition
Score: 0.788 (must_read)</description><content:encoded>&lt;p&gt;&lt;strong&gt;Authors:&lt;/strong&gt; Gang Li; Enze Xie; Chongjian Ge; Xiang Li; Lingyu Si; Changwen Zheng; Zhenguo Li&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Journal:&lt;/strong&gt; Pattern Recognition&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;a href="https://doi.org/10.1016/j.patcog.2026.113158"&gt;10.1016/j.patcog.2026.113158&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Relevance:&lt;/strong&gt; 0.788 (must_read)&lt;/p&gt;
&lt;p&gt;Denoising Diffusion Probabilistic Models (DDPMs) have made significant progress in image generation. Recent works in semantic-to-image (S2I) synthesis have also shifted from the previously de facto GAN-based methods to DDPMs, yielding better results. However, these works mostly employ a U-Net structure and vanilla training-from-scratch scheme for S2I, unconsciously neglecting the potential benefits offered by task-related pre-training. In this work, we introduce a Transformer-based architecture, namely S2I-DiT, and reconsider the merits of a pre-trained large diffusion model for cross-task adaptation (i.e., from the class-conditional generation to S2I). In S2I-DiT, we propose the integration of semantic embedders within Diffusion Transformers (DiTs) to maximize the utilization of semantic information. The semantic embedder densely encodes semantic layouts to guide the adaptive normalization process. We configure semantic embedders in a layer-wise manner to learn pixel-level correspondence, enabling finer-grained semantic-to-image control. Besides, to fully unleash the cross-task transferability of DDPMs, we introduce a two-stage fine-tuning strategy, which involves initially adapting the semantic embedders in the pixel-level space, followed by fine-tuning the partial/entire model for cross-task adaptation. Notably, S2I-DiT pioneers the application of Large Diffusion Transformers to cross-task fine-tuning. Extensive experiments on four benchmark datasets demonstrate S2I-DiT’s effectiveness, as it achieves state-of-the-art performance in terms of quality (FID) and diversity (LPIPS), while consuming fewer training iterations. This work establishes a new state-of-the-art for semantic-to-image generation and provides valuable insights into cross-task transferability of large generative models.&lt;/p&gt;</content:encoded></item><item><title>SAMTok: Representing Any Mask with Two Words</title><link>https://arxiv.org/abs/2601.16093v1</link><guid>http://arxiv.org/abs/2601.16093v1</guid><pubDate>Thu, 22 Jan 2026 16:44:09 +0000</pubDate><dc:creator>Yikang Zhou</dc:creator><dc:creator>Tao Zhang</dc:creator><dc:creator>Dengxian Gong</dc:creator><dc:creator>Yuanzheng Wu</dc:creator><dc:creator>Ye Tian</dc:creator><dc:creator>Haochen Wang</dc:creator><dc:creator>Haobo Yuan</dc:creator><dc:creator>Jiacong Wang</dc:creator><dc:creator>Lu Qi</dc:creator><dc:creator>Hao Fei</dc:creator><dc:creator>Anran Wang</dc:creator><dc:creator>Zhuochen Wang</dc:creator><dc:creator>Yujing Wang</dc:creator><dc:creator>Cheng Chen</dc:creator><dc:creator>Shunping Ji</dc:creator><dc:creator>Xiangtai Li</dc:creator><dc:type>preprint</dc:type><prism:publicationName>arXiv</prism:publicationName><description>Pixel-wise capabilities are essential for building interactive intelligent systems. However, pixel-wise multi-modal LLMs (MLLMs) remain difficult to scale due to complex region-level encoders, specialized segmentation decoders, and incompatible training objectives. To address these challenges, we present SAMTok, a discrete mask tokenizer that converts any region mask into two special tokens and reconstructs the mask using these tokens with high fidelity. By treating masks as new language tokens, SAMTok enables base MLLMs (such as the QwenVL series) to learn pixel-wise capabilities through standard next-token prediction and simple reinforcement learning, without architectural modifications and specialized loss design. SAMTok builds on SAM2 and is trained on 209M diverse masks using a mask encoder and residual vector quantizer to produce discrete, compact, and information-rich tokens. With 5M SAMTok-formatted mask understanding and generation data samples, QwenVL-SAMTok attains state-of-the-art or comparable results on region captioning, region VQA, grounded conversation, referring segmentation, scene graph parsing, and multi-round interactive segmentation. We further introduce a textual answer-matching reward that enables efficient reinforcement learning for mask generation, delivering substantial improvements on GRES and GCG benchmarks. Our results demonstrate a scalable and straightforward paradigm for equipping MLLMs with strong pixel-wise capabilities. Our code and models are available.
Published: 2026-01-22T16:44:09+00:00
Venue: arXiv
Score: 0.785 (must_read)</description><content:encoded>&lt;p&gt;&lt;strong&gt;Authors:&lt;/strong&gt; Yikang Zhou; Tao Zhang; Dengxian Gong; Yuanzheng Wu; Ye Tian; Haochen Wang; Haobo Yuan; Jiacong Wang; Lu Qi; Hao Fei; Anran Wang; Zhuochen Wang; Yujing Wang; Cheng Chen; Shunping Ji; Xiangtai Li&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Journal:&lt;/strong&gt; arXiv&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Relevance:&lt;/strong&gt; 0.785 (must_read)&lt;/p&gt;
&lt;p&gt;Pixel-wise capabilities are essential for building interactive intelligent systems. However, pixel-wise multi-modal LLMs (MLLMs) remain difficult to scale due to complex region-level encoders, specialized segmentation decoders, and incompatible training objectives. To address these challenges, we present SAMTok, a discrete mask tokenizer that converts any region mask into two special tokens and reconstructs the mask using these tokens with high fidelity. By treating masks as new language tokens, SAMTok enables base MLLMs (such as the QwenVL series) to learn pixel-wise capabilities through standard next-token prediction and simple reinforcement learning, without architectural modifications and specialized loss design. SAMTok builds on SAM2 and is trained on 209M diverse masks using a mask encoder and residual vector quantizer to produce discrete, compact, and information-rich tokens. With 5M SAMTok-formatted mask understanding and generation data samples, QwenVL-SAMTok attains state-of-the-art or comparable results on region captioning, region VQA, grounded conversation, referring segmentation, scene graph parsing, and multi-round interactive segmentation. We further introduce a textual answer-matching reward that enables efficient reinforcement learning for mask generation, delivering substantial improvements on GRES and GCG benchmarks. Our results demonstrate a scalable and straightforward paradigm for equipping MLLMs with strong pixel-wise capabilities. Our code and models are available.&lt;/p&gt;</content:encoded></item><item><title>Generative Model-Based Mixed-Semantic Enhancement for Transductive Zero-Shot Learning</title><link>https://doi.org/10.1016/j.patcog.2026.113124</link><guid>10.1016/j.patcog.2026.113124</guid><pubDate>Sat, 24 Jan 2026 23:17:12 +0000</pubDate><dc:creator>Huaizhou Qi</dc:creator><dc:creator>Yang Liu</dc:creator><dc:creator>Jungong Han</dc:creator><dc:creator>Lei Zhang</dc:creator><prism:publicationName>Pattern Recognition</prism:publicationName><prism:doi>10.1016/j.patcog.2026.113124</prism:doi><description>Zero-shot learning (ZSL) addresses the critical challenge of recognizing and classifying instances from categories not seen during training. Although generative model-based approaches have achieved notable success in ZSL, their predominant reliance on forward generation strategies coupled with excessive dependence on auxiliary information hampers model generalization and robustness. To overcome these limitations, we propose a Mixed-Semantic Enhancement framework inspired by interpolation-based feature extraction. This novel approach is designed to synthesize enriched auxiliary information through integrating authentic semantic cues, thereby refining the mapping from semantic descriptions to visual features. The enhanced feature synthesis capability enables better discrimination of ambiguous classes while preserving inter-class relationships. In addition, we establish bidirectional alignment between visual features and auxiliary information. This cross-modal interaction mechanism not only strengthens the generator’s training process through feature consistency constraints but also facilitates dynamic information exchange between modalities. Extensive experiments in a transductive setting across four benchmark datasets demonstrate significant performance gains, highlighting the robustness and effectiveness of our approach in advancing generative ZSL models.
Published: 2026-01-24T23:17:12+00:00
Venue: Pattern Recognition
Score: 0.785 (must_read)</description><content:encoded>&lt;p&gt;&lt;strong&gt;Authors:&lt;/strong&gt; Huaizhou Qi; Yang Liu; Jungong Han; Lei Zhang&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Journal:&lt;/strong&gt; Pattern Recognition&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;a href="https://doi.org/10.1016/j.patcog.2026.113124"&gt;10.1016/j.patcog.2026.113124&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Relevance:&lt;/strong&gt; 0.785 (must_read)&lt;/p&gt;
&lt;p&gt;Zero-shot learning (ZSL) addresses the critical challenge of recognizing and classifying instances from categories not seen during training. Although generative model-based approaches have achieved notable success in ZSL, their predominant reliance on forward generation strategies coupled with excessive dependence on auxiliary information hampers model generalization and robustness. To overcome these limitations, we propose a Mixed-Semantic Enhancement framework inspired by interpolation-based feature extraction. This novel approach is designed to synthesize enriched auxiliary information through integrating authentic semantic cues, thereby refining the mapping from semantic descriptions to visual features. The enhanced feature synthesis capability enables better discrimination of ambiguous classes while preserving inter-class relationships. In addition, we establish bidirectional alignment between visual features and auxiliary information. This cross-modal interaction mechanism not only strengthens the generator’s training process through feature consistency constraints but also facilitates dynamic information exchange between modalities. Extensive experiments in a transductive setting across four benchmark datasets demonstrate significant performance gains, highlighting the robustness and effectiveness of our approach in advancing generative ZSL models.&lt;/p&gt;</content:encoded></item><item><title>Integrating Knowledge Distillation Methods: A Sequential Multi-Stage Framework</title><link>https://arxiv.org/abs/2601.15657v1</link><guid>http://arxiv.org/abs/2601.15657v1</guid><pubDate>Thu, 22 Jan 2026 05:13:12 +0000</pubDate><dc:creator>Yinxi Tian</dc:creator><dc:creator>Changwu Huang</dc:creator><dc:creator>Ke Tang</dc:creator><dc:creator>Xin Yao</dc:creator><dc:type>preprint</dc:type><prism:publicationName>arXiv</prism:publicationName><description>Knowledge distillation (KD) transfers knowledge from large teacher models to compact student models, enabling efficient deployment on resource constrained devices. While diverse KD methods, including response based, feature based, and relation based approaches, capture different aspects of teacher knowledge, integrating multiple methods or knowledge sources is promising but often hampered by complex implementation, inflexible combinations, and catastrophic forgetting, which limits practical effectiveness.
  This work proposes SMSKD (Sequential Multi Stage Knowledge Distillation), a flexible framework that sequentially integrates heterogeneous KD methods. At each stage, the student is trained with a specific distillation method, while a frozen reference model from the previous stage anchors learned knowledge to mitigate forgetting. In addition, we introduce an adaptive weighting mechanism based on the teacher true class probability (TCP) that dynamically adjusts the reference loss per sample to balance knowledge retention and integration.
  By design, SMSKD supports arbitrary method combinations and stage counts with negligible computational overhead. Extensive experiments show that SMSKD consistently improves student accuracy across diverse teacher student architectures and method combinations, outperforming existing baselines. Ablation studies confirm that stage wise distillation and reference model supervision are primary contributors to performance gains, with TCP based adaptive weighting providing complementary benefits. Overall, SMSKD is a practical and resource efficient solution for integrating heterogeneous KD methods.
Published: 2026-01-22T05:13:12+00:00
Venue: arXiv
Score: 0.780 (must_read)</description><content:encoded>&lt;p&gt;&lt;strong&gt;Authors:&lt;/strong&gt; Yinxi Tian; Changwu Huang; Ke Tang; Xin Yao&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Journal:&lt;/strong&gt; arXiv&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Relevance:&lt;/strong&gt; 0.780 (must_read)&lt;/p&gt;
&lt;p&gt;Knowledge distillation (KD) transfers knowledge from large teacher models to compact student models, enabling efficient deployment on resource constrained devices. While diverse KD methods, including response based, feature based, and relation based approaches, capture different aspects of teacher knowledge, integrating multiple methods or knowledge sources is promising but often hampered by complex implementation, inflexible combinations, and catastrophic forgetting, which limits practical effectiveness.
  This work proposes SMSKD (Sequential Multi Stage Knowledge Distillation), a flexible framework that sequentially integrates heterogeneous KD methods. At each stage, the student is trained with a specific distillation method, while a frozen reference model from the previous stage anchors learned knowledge to mitigate forgetting. In addition, we introduce an adaptive weighting mechanism based on the teacher true class probability (TCP) that dynamically adjusts the reference loss per sample to balance knowledge retention and integration.
  By design, SMSKD supports arbitrary method combinations and stage counts with negligible computational overhead. Extensive experiments show that SMSKD consistently improves student accuracy across diverse teacher student architectures and method combinations, outperforming existing baselines. Ablation studies confirm that stage wise distillation and reference model supervision are primary contributors to performance gains, with TCP based adaptive weighting providing complementary benefits. Overall, SMSKD is a practical and resource efficient solution for integrating heterogeneous KD methods.&lt;/p&gt;</content:encoded></item><item><title>When Sharpening Becomes Collapse: Sampling Bias and Semantic Coupling in RL with Verifiable Rewards</title><link>https://arxiv.org/abs/2601.15609v1</link><guid>http://arxiv.org/abs/2601.15609v1</guid><pubDate>Thu, 22 Jan 2026 03:15:57 +0000</pubDate><dc:creator>Mingyuan Fan</dc:creator><dc:creator>Weiguang Han</dc:creator><dc:creator>Daixin Wang</dc:creator><dc:creator>Cen Chen</dc:creator><dc:creator>Zhiqiang Zhang</dc:creator><dc:creator>Jun Zhou</dc:creator><dc:type>preprint</dc:type><prism:publicationName>arXiv</prism:publicationName><description>Reinforcement Learning with Verifiable Rewards (RLVR) is a central paradigm for turning large language models (LLMs) into reliable problem solvers, especially in logic-heavy domains. Despite its empirical success, it remains unclear whether RLVR elicits novel capabilities or merely sharpens the distribution over existing knowledge. We study this by formalizing over-sharpening, a phenomenon where the policy collapses onto limited modes, suppressing valid alternatives. At a high level, we discover finite-batch updates intrinsically bias learning toward sampled modes, triggering a collapse that propagates globally via semantic coupling. To mitigate this, we propose inverse-success advantage calibration to prioritize difficult queries and distribution-level calibration to diversify sampling via a memory network. Empirical evaluations validate that our strategies can effectively improve generalization.
Published: 2026-01-22T03:15:57+00:00
Venue: arXiv
Score: 0.778 (must_read)</description><content:encoded>&lt;p&gt;&lt;strong&gt;Authors:&lt;/strong&gt; Mingyuan Fan; Weiguang Han; Daixin Wang; Cen Chen; Zhiqiang Zhang; Jun Zhou&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Journal:&lt;/strong&gt; arXiv&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Relevance:&lt;/strong&gt; 0.778 (must_read)&lt;/p&gt;
&lt;p&gt;Reinforcement Learning with Verifiable Rewards (RLVR) is a central paradigm for turning large language models (LLMs) into reliable problem solvers, especially in logic-heavy domains. Despite its empirical success, it remains unclear whether RLVR elicits novel capabilities or merely sharpens the distribution over existing knowledge. We study this by formalizing over-sharpening, a phenomenon where the policy collapses onto limited modes, suppressing valid alternatives. At a high level, we discover finite-batch updates intrinsically bias learning toward sampled modes, triggering a collapse that propagates globally via semantic coupling. To mitigate this, we propose inverse-success advantage calibration to prioritize difficult queries and distribution-level calibration to diversify sampling via a memory network. Empirical evaluations validate that our strategies can effectively improve generalization.&lt;/p&gt;</content:encoded></item><item><title>PyraTok: Language-Aligned Pyramidal Tokenizer for Video Understanding and Generation</title><link>https://arxiv.org/abs/2601.16210v1</link><guid>http://arxiv.org/abs/2601.16210v1</guid><pubDate>Thu, 22 Jan 2026 18:58:55 +0000</pubDate><dc:creator>Onkar Susladkar</dc:creator><dc:creator>Tushar Prakash</dc:creator><dc:creator>Adheesh Juvekar</dc:creator><dc:creator>Kiet A. Nguyen</dc:creator><dc:creator>Dong-Hwan Jang</dc:creator><dc:creator>Inderjit S Dhillon</dc:creator><dc:creator>Ismini Lourentzou</dc:creator><dc:type>preprint</dc:type><prism:publicationName>arXiv</prism:publicationName><description>Discrete video VAEs underpin modern text-to-video generation and video understanding systems, yet existing tokenizers typically learn visual codebooks at a single scale with limited vocabularies and shallow language supervision, leading to poor cross-modal alignment and zero-shot transfer. We introduce PyraTok, a language-aligned pyramidal tokenizer that learns semantically structured discrete latents across multiple spatiotemporal resolutions. PyraTok builds on a pretrained video VAE and a novel Language aligned Pyramidal Quantization (LaPQ) module that discretizes encoder features at several depths using a shared large binary codebook, yielding compact yet expressive video token sequences. To tightly couple visual tokens with language, PyraTok jointly optimizes multi-scale text-guided quantization and a global autoregressive objective over the token hierarchy. Across ten benchmarks, PyraTok delivers state-of-the-art (SOTA) video reconstruction, consistently improves text-to-video quality, and sets new SOTA zero-shot performance on video segmentation, temporal action localization, and video understanding, scaling robustly to up to 4K/8K resolutions.
Published: 2026-01-22T18:58:55+00:00
Venue: arXiv
Score: 0.777 (must_read)</description><content:encoded>&lt;p&gt;&lt;strong&gt;Authors:&lt;/strong&gt; Onkar Susladkar; Tushar Prakash; Adheesh Juvekar; Kiet A. Nguyen; Dong-Hwan Jang; Inderjit S Dhillon; Ismini Lourentzou&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Journal:&lt;/strong&gt; arXiv&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Relevance:&lt;/strong&gt; 0.777 (must_read)&lt;/p&gt;
&lt;p&gt;Discrete video VAEs underpin modern text-to-video generation and video understanding systems, yet existing tokenizers typically learn visual codebooks at a single scale with limited vocabularies and shallow language supervision, leading to poor cross-modal alignment and zero-shot transfer. We introduce PyraTok, a language-aligned pyramidal tokenizer that learns semantically structured discrete latents across multiple spatiotemporal resolutions. PyraTok builds on a pretrained video VAE and a novel Language aligned Pyramidal Quantization (LaPQ) module that discretizes encoder features at several depths using a shared large binary codebook, yielding compact yet expressive video token sequences. To tightly couple visual tokens with language, PyraTok jointly optimizes multi-scale text-guided quantization and a global autoregressive objective over the token hierarchy. Across ten benchmarks, PyraTok delivers state-of-the-art (SOTA) video reconstruction, consistently improves text-to-video quality, and sets new SOTA zero-shot performance on video segmentation, temporal action localization, and video understanding, scaling robustly to up to 4K/8K resolutions.&lt;/p&gt;</content:encoded></item><item><title>Adversarial perturbation for RGB-T tracking via intra-modal excavation and cross-modal collusion</title><link>https://doi.org/10.1016/j.inffus.2026.104183</link><guid>10.1016/j.inffus.2026.104183</guid><pubDate>Sat, 24 Jan 2026 00:25:58 +0000</pubDate><dc:creator>Xinyu Xiang</dc:creator><dc:creator>Xuying Wu</dc:creator><dc:creator>Shengxiang Li</dc:creator><dc:creator>Qinglong Yan</dc:creator><dc:creator>Tong Zou</dc:creator><dc:creator>Hao Zhang</dc:creator><dc:creator>Jiayi Ma</dc:creator><prism:publicationName>Information Fusion</prism:publicationName><prism:doi>10.1016/j.inffus.2026.104183</prism:doi><description>Existing adversarial perturbation attack for visual object trackers mainly focuses on RGB modality, yet research on RGB-T trackers’ adversarial perturbation remains unexplored. To address this gap, we propose an I ntra-modal excavation and C ross-modal collusion adversarial perturbation attack algorithm (ICAttack) for RGB-T Tracking. Firstly, we establish a novel intra-modal adversarial clues excavation (ImAE) paradigm. By leveraging the unique distribution properties of each modality as a prior, we independently extract the attack cues of different modalities from the public noise space. Building upon this, we develop a cross-modal adversarial collusion (CmAC) strategy, which enables implicit and dynamic interaction between the adversarial tokens of two modalities. This interaction facilitates negotiation and collaboration, achieving a synergistic attack gain for RGB-T trackers that surpasses the effect of a single-modality attack. The above process, from intra-modal excavation to cross-modal collusion, creates a progressive and systematic attack framework for RGB-T trackers. Besides, by introducing the spatial adversarial intensity control module and precise response disruption loss, we further enhance both the attack stealthiness and precision of our adversarial perturbations. The control module reduces attack strength in less critical areas to improve stealth. The disruption loss uses a small mask on the tracker’s brightest semantic response region, concentrating the perturbation to interfere with the tracker’s target awareness precisely. Extensive evaluations of attack performances in different SOTA victimized RGB-T trackers demonstrate the advantages of ICAttack in terms of specificity and effectiveness of cross-modal attacks. Moreover, we offer a user-friendly interface to promote the practical deployment of adversarial perturbations.
Published: 2026-01-24T00:25:58+00:00
Venue: Information Fusion
Score: 0.777 (must_read)</description><content:encoded>&lt;p&gt;&lt;strong&gt;Authors:&lt;/strong&gt; Xinyu Xiang; Xuying Wu; Shengxiang Li; Qinglong Yan; Tong Zou; Hao Zhang; Jiayi Ma&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Journal:&lt;/strong&gt; Information Fusion&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;a href="https://doi.org/10.1016/j.inffus.2026.104183"&gt;10.1016/j.inffus.2026.104183&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Relevance:&lt;/strong&gt; 0.777 (must_read)&lt;/p&gt;
&lt;p&gt;Existing adversarial perturbation attack for visual object trackers mainly focuses on RGB modality, yet research on RGB-T trackers’ adversarial perturbation remains unexplored. To address this gap, we propose an I ntra-modal excavation and C ross-modal collusion adversarial perturbation attack algorithm (ICAttack) for RGB-T Tracking. Firstly, we establish a novel intra-modal adversarial clues excavation (ImAE) paradigm. By leveraging the unique distribution properties of each modality as a prior, we independently extract the attack cues of different modalities from the public noise space. Building upon this, we develop a cross-modal adversarial collusion (CmAC) strategy, which enables implicit and dynamic interaction between the adversarial tokens of two modalities. This interaction facilitates negotiation and collaboration, achieving a synergistic attack gain for RGB-T trackers that surpasses the effect of a single-modality attack. The above process, from intra-modal excavation to cross-modal collusion, creates a progressive and systematic attack framework for RGB-T trackers. Besides, by introducing the spatial adversarial intensity control module and precise response disruption loss, we further enhance both the attack stealthiness and precision of our adversarial perturbations. The control module reduces attack strength in less critical areas to improve stealth. The disruption loss uses a small mask on the tracker’s brightest semantic response region, concentrating the perturbation to interfere with the tracker’s target awareness precisely. Extensive evaluations of attack performances in different SOTA victimized RGB-T trackers demonstrate the advantages of ICAttack in terms of specificity and effectiveness of cross-modal attacks. Moreover, we offer a user-friendly interface to promote the practical deployment of adversarial perturbations.&lt;/p&gt;</content:encoded></item><item><title>HVD: Human Vision-Driven Video Representation Learning for Text-Video Retrieval</title><link>https://arxiv.org/abs/2601.16155v1</link><guid>http://arxiv.org/abs/2601.16155v1</guid><pubDate>Thu, 22 Jan 2026 17:57:42 +0000</pubDate><dc:creator>Zequn Xie</dc:creator><dc:creator>Xin Liu</dc:creator><dc:creator>Boyun Zhang</dc:creator><dc:creator>Yuxiao Lin</dc:creator><dc:creator>Sihang Cai</dc:creator><dc:creator>Tao Jin</dc:creator><dc:type>preprint</dc:type><prism:publicationName>arXiv</prism:publicationName><description>The success of CLIP has driven substantial progress in text-video retrieval. However, current methods often suffer from "blind" feature interaction, where the model struggles to discern key visual information from background noise due to the sparsity of textual queries. To bridge this gap, we draw inspiration from human cognitive behavior and propose the Human Vision-Driven (HVD) model. Our framework establishes a coarse-to-fine alignment mechanism comprising two key components: the Frame Features Selection Module (FFSM) and the Patch Features Compression Module (PFCM). FFSM mimics the human macro-perception ability by selecting key frames to eliminate temporal redundancy. Subsequently, PFCM simulates micro-perception by aggregating patch features into salient visual entities through an advanced attention mechanism, enabling precise entity-level matching. Extensive experiments on five benchmarks demonstrate that HVD not only captures human-like visual focus but also achieves state-of-the-art performance.
Published: 2026-01-22T17:57:42+00:00
Venue: arXiv
Score: 0.777 (must_read)</description><content:encoded>&lt;p&gt;&lt;strong&gt;Authors:&lt;/strong&gt; Zequn Xie; Xin Liu; Boyun Zhang; Yuxiao Lin; Sihang Cai; Tao Jin&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Journal:&lt;/strong&gt; arXiv&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Relevance:&lt;/strong&gt; 0.777 (must_read)&lt;/p&gt;
&lt;p&gt;The success of CLIP has driven substantial progress in text-video retrieval. However, current methods often suffer from &amp;quot;blind&amp;quot; feature interaction, where the model struggles to discern key visual information from background noise due to the sparsity of textual queries. To bridge this gap, we draw inspiration from human cognitive behavior and propose the Human Vision-Driven (HVD) model. Our framework establishes a coarse-to-fine alignment mechanism comprising two key components: the Frame Features Selection Module (FFSM) and the Patch Features Compression Module (PFCM). FFSM mimics the human macro-perception ability by selecting key frames to eliminate temporal redundancy. Subsequently, PFCM simulates micro-perception by aggregating patch features into salient visual entities through an advanced attention mechanism, enabling precise entity-level matching. Extensive experiments on five benchmarks demonstrate that HVD not only captures human-like visual focus but also achieves state-of-the-art performance.&lt;/p&gt;</content:encoded></item><item><title>Towards a Theoretical Understanding to the Generalization of RLHF</title><link>https://arxiv.org/abs/2601.16403v1</link><guid>http://arxiv.org/abs/2601.16403v1</guid><pubDate>Fri, 23 Jan 2026 02:30:16 +0000</pubDate><dc:creator>Zhaochun Li</dc:creator><dc:creator>Mingyang Yi</dc:creator><dc:creator>Yue Wang</dc:creator><dc:creator>Shisheng Cui</dc:creator><dc:creator>Yong Liu</dc:creator><dc:type>preprint</dc:type><prism:publicationName>arXiv</prism:publicationName><description>Reinforcement Learning from Human Feedback (RLHF) and its variants have emerged as the dominant approaches for aligning Large Language Models with human intent. While empirically effective, the theoretical generalization properties of these methods in high-dimensional settings remain to be explored. To this end, we build the generalization theory on RLHF of LLMs under the linear reward model, through the framework of algorithmic stability. In contrast to the existing works built upon the consistency of maximum likelihood estimations on reward model, our analysis is presented under an end-to-end learning framework, which is consistent with practice. Concretely, we prove that under a key \textbf{feature coverage} condition, the empirical optima of policy model have a generalization bound of order $\mathcal{O}(n^{-\frac{1}{2}})$. Moreover, the results can be extrapolated to parameters obtained by gradient-based learning algorithms, i.e., Gradient Ascent (GA) and Stochastic Gradient Ascent (SGA). Thus, we argue that our results provide new theoretical evidence for the empirically observed generalization of LLMs after RLHF.
Published: 2026-01-23T02:30:16+00:00
Venue: arXiv
Score: 0.775 (must_read)</description><content:encoded>&lt;p&gt;&lt;strong&gt;Authors:&lt;/strong&gt; Zhaochun Li; Mingyang Yi; Yue Wang; Shisheng Cui; Yong Liu&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Journal:&lt;/strong&gt; arXiv&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Relevance:&lt;/strong&gt; 0.775 (must_read)&lt;/p&gt;
&lt;p&gt;Reinforcement Learning from Human Feedback (RLHF) and its variants have emerged as the dominant approaches for aligning Large Language Models with human intent. While empirically effective, the theoretical generalization properties of these methods in high-dimensional settings remain to be explored. To this end, we build the generalization theory on RLHF of LLMs under the linear reward model, through the framework of algorithmic stability. In contrast to the existing works built upon the consistency of maximum likelihood estimations on reward model, our analysis is presented under an end-to-end learning framework, which is consistent with practice. Concretely, we prove that under a key \textbf{feature coverage} condition, the empirical optima of policy model have a generalization bound of order $\mathcal{O}(n^{-\frac{1}{2}})$. Moreover, the results can be extrapolated to parameters obtained by gradient-based learning algorithms, i.e., Gradient Ascent (GA) and Stochastic Gradient Ascent (SGA). Thus, we argue that our results provide new theoretical evidence for the empirically observed generalization of LLMs after RLHF.&lt;/p&gt;</content:encoded></item><item><title>Holistic prediction comparison for knowledge distillation</title><link>https://doi.org/10.1016/j.neucom.2026.132754</link><guid>10.1016/j.neucom.2026.132754</guid><pubDate>Sat, 24 Jan 2026 16:14:35 +0000</pubDate><dc:creator>Tongtong Su</dc:creator><dc:creator>Chengmin Yan</dc:creator><dc:creator>Huilin Liu</dc:creator><dc:creator>Jiale Si</dc:creator><dc:creator>Xukai Wang</dc:creator><dc:creator>Jinqi Zhu</dc:creator><dc:creator>Chenyang Wang</dc:creator><dc:creator>Xiguo Zhou</dc:creator><dc:creator>Jia Guo</dc:creator><prism:publicationName>Neurocomputing</prism:publicationName><prism:doi>10.1016/j.neucom.2026.132754</prism:doi><description>Knowledge distillation (KD) transfers knowledge from a heavy teacher model to a lightweight student. Despite their performance, feature-based KD methods incur high computational overhead, whereas logit-based methods offer an efficient alternative by leveraging the information-rich final outputs. In this paper, we propose a conceptual shift to decouple knowledge transfer at the output layer , instantiated in our framework H olistic P rediction C omparison for K nowledge D istillation (HPCKD), to bridge the performance-efficiency gap. Specifically, we decouple the output into three complementary facets: point-wise, pair-wise, and manifold-wise. This decoupled perspective allows us to construct a holistic and efficient distillation framework that explicitly targets each knowledge type with a purpose-built alignment mechanism. To coordinate these heterogeneous objectives, we further introduce an Adaptive Loss Adjustment Module (ALAM), which automates the balance across alignment losses during training based on their real-time contributions, ensuring stable and synergistic optimization. Extensive experiments on ImageNet, few-shot learning, and COCO detection establish the new state-of-the-art, boosting ResNet-18 performance by +2.47 % Top-1 accuracy, +8.4 % (5-way 1-shot), and +3.75 % mAP, respectively. Our work opens new pathways for efficient neural network compression by demonstrating that sophisticated knowledge transfer can be effectively accomplished through strategic output-layer decomposition, potentially reducing reliance on complex intermediate feature distillation methods. Code is available here: https://github.com/nanxiaotong/HPCKD .
Published: 2026-01-24T16:14:35+00:00
Venue: Neurocomputing
Score: 0.775 (must_read)</description><content:encoded>&lt;p&gt;&lt;strong&gt;Authors:&lt;/strong&gt; Tongtong Su; Chengmin Yan; Huilin Liu; Jiale Si; Xukai Wang; Jinqi Zhu; Chenyang Wang; Xiguo Zhou; Jia Guo&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Journal:&lt;/strong&gt; Neurocomputing&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;a href="https://doi.org/10.1016/j.neucom.2026.132754"&gt;10.1016/j.neucom.2026.132754&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Relevance:&lt;/strong&gt; 0.775 (must_read)&lt;/p&gt;
&lt;p&gt;Knowledge distillation (KD) transfers knowledge from a heavy teacher model to a lightweight student. Despite their performance, feature-based KD methods incur high computational overhead, whereas logit-based methods offer an efficient alternative by leveraging the information-rich final outputs. In this paper, we propose a conceptual shift to decouple knowledge transfer at the output layer , instantiated in our framework H olistic P rediction C omparison for K nowledge D istillation (HPCKD), to bridge the performance-efficiency gap. Specifically, we decouple the output into three complementary facets: point-wise, pair-wise, and manifold-wise. This decoupled perspective allows us to construct a holistic and efficient distillation framework that explicitly targets each knowledge type with a purpose-built alignment mechanism. To coordinate these heterogeneous objectives, we further introduce an Adaptive Loss Adjustment Module (ALAM), which automates the balance across alignment losses during training based on their real-time contributions, ensuring stable and synergistic optimization. Extensive experiments on ImageNet, few-shot learning, and COCO detection establish the new state-of-the-art, boosting ResNet-18 performance by +2.47 % Top-1 accuracy, +8.4 % (5-way 1-shot), and +3.75 % mAP, respectively. Our work opens new pathways for efficient neural network compression by demonstrating that sophisticated knowledge transfer can be effectively accomplished through strategic output-layer decomposition, potentially reducing reliance on complex intermediate feature distillation methods. Code is available here: https://github.com/nanxiaotong/HPCKD .&lt;/p&gt;</content:encoded></item><item><title>X-Aligner: Composed Visual Retrieval without the Bells and Whistles</title><link>https://arxiv.org/abs/2601.16582v1</link><guid>http://arxiv.org/abs/2601.16582v1</guid><pubDate>Fri, 23 Jan 2026 09:33:38 +0000</pubDate><dc:creator>Yuqian Zheng</dc:creator><dc:creator>Mariana-Iuliana Georgescu</dc:creator><dc:type>preprint</dc:type><prism:publicationName>arXiv</prism:publicationName><description>Composed Video Retrieval (CoVR) facilitates video retrieval by combining visual and textual queries. However, existing CoVR frameworks typically fuse multimodal inputs in a single stage, achieving only marginal gains over initial baseline. To address this, we propose a novel CoVR framework that leverages the representational power of Vision Language Models (VLMs). Our framework incorporates a novel cross-attention module X-Aligner, composed of cross-attention layers that progressively fuse visual and textual inputs and align their multimodal representation with that of the target video. To further enhance the representation of the multimodal query, we incorporate the caption of the visual query as an additional input. The framework is trained in two stages to preserve the pretrained VLM representation. In the first stage, only the newly introduced module is trained, while in the second stage, the textual query encoder is also fine-tuned. We implement our framework on top of BLIP-family architecture, namely BLIP and BLIP-2, and train it on the Webvid-CoVR data set. In addition to in-domain evaluation on Webvid-CoVR-Test, we perform zero-shot evaluations on the Composed Image Retrieval (CIR) data sets CIRCO and Fashion-IQ. Our framework achieves state-of-the-art performance on CoVR obtaining a Recall@1 of 63.93% on Webvid-CoVR-Test, and demonstrates strong zero-shot generalization on CIR tasks.
Published: 2026-01-23T09:33:38+00:00
Venue: arXiv
Score: 0.774 (must_read)</description><content:encoded>&lt;p&gt;&lt;strong&gt;Authors:&lt;/strong&gt; Yuqian Zheng; Mariana-Iuliana Georgescu&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Journal:&lt;/strong&gt; arXiv&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Relevance:&lt;/strong&gt; 0.774 (must_read)&lt;/p&gt;
&lt;p&gt;Composed Video Retrieval (CoVR) facilitates video retrieval by combining visual and textual queries. However, existing CoVR frameworks typically fuse multimodal inputs in a single stage, achieving only marginal gains over initial baseline. To address this, we propose a novel CoVR framework that leverages the representational power of Vision Language Models (VLMs). Our framework incorporates a novel cross-attention module X-Aligner, composed of cross-attention layers that progressively fuse visual and textual inputs and align their multimodal representation with that of the target video. To further enhance the representation of the multimodal query, we incorporate the caption of the visual query as an additional input. The framework is trained in two stages to preserve the pretrained VLM representation. In the first stage, only the newly introduced module is trained, while in the second stage, the textual query encoder is also fine-tuned. We implement our framework on top of BLIP-family architecture, namely BLIP and BLIP-2, and train it on the Webvid-CoVR data set. In addition to in-domain evaluation on Webvid-CoVR-Test, we perform zero-shot evaluations on the Composed Image Retrieval (CIR) data sets CIRCO and Fashion-IQ. Our framework achieves state-of-the-art performance on CoVR obtaining a Recall@1 of 63.93% on Webvid-CoVR-Test, and demonstrates strong zero-shot generalization on CIR tasks.&lt;/p&gt;</content:encoded></item><item><title>LUMINA: Long-horizon Understanding for Multi-turn Interactive Agents</title><link>https://arxiv.org/abs/2601.16649v1</link><guid>http://arxiv.org/abs/2601.16649v1</guid><pubDate>Fri, 23 Jan 2026 11:13:12 +0000</pubDate><dc:creator>Amin Rakhsha</dc:creator><dc:creator>Thomas Hehn</dc:creator><dc:creator>Pietro Mazzaglia</dc:creator><dc:creator>Fabio Valerio Massoli</dc:creator><dc:creator>Arash Behboodi</dc:creator><dc:creator>Tribhuvanesh Orekondy</dc:creator><dc:type>preprint</dc:type><prism:publicationName>arXiv</prism:publicationName><description>Large language models can perform well on many isolated tasks, yet they continue to struggle on multi-turn, long-horizon agentic problems that require skills such as planning, state tracking, and long context processing. In this work, we aim to better understand the relative importance of advancing these underlying capabilities for success on such tasks. We develop an oracle counterfactual framework for multi-turn problems that asks: how would an agent perform if it could leverage an oracle to perfectly perform a specific task? The change in the agent's performance due to this oracle assistance allows us to measure the criticality of such oracle skill in the future advancement of AI agents. We introduce a suite of procedurally generated, game-like tasks with tunable complexity. These controlled environments allow us to provide precise oracle interventions, such as perfect planning or flawless state tracking, and make it possible to isolate the contribution of each oracle without confounding effects present in real-world benchmarks. Our results show that while some interventions (e.g., planning) consistently improve performance across settings, the usefulness of other skills is dependent on the properties of the environment and language model. Our work sheds light on the challenges of multi-turn agentic environments to guide the future efforts in the development of AI agents and language models.
Published: 2026-01-23T11:13:12+00:00
Venue: arXiv
Score: 0.771 (consider)</description><content:encoded>&lt;p&gt;&lt;strong&gt;Authors:&lt;/strong&gt; Amin Rakhsha; Thomas Hehn; Pietro Mazzaglia; Fabio Valerio Massoli; Arash Behboodi; Tribhuvanesh Orekondy&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Journal:&lt;/strong&gt; arXiv&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Relevance:&lt;/strong&gt; 0.771 (consider)&lt;/p&gt;
&lt;p&gt;Large language models can perform well on many isolated tasks, yet they continue to struggle on multi-turn, long-horizon agentic problems that require skills such as planning, state tracking, and long context processing. In this work, we aim to better understand the relative importance of advancing these underlying capabilities for success on such tasks. We develop an oracle counterfactual framework for multi-turn problems that asks: how would an agent perform if it could leverage an oracle to perfectly perform a specific task? The change in the agent&amp;#x27;s performance due to this oracle assistance allows us to measure the criticality of such oracle skill in the future advancement of AI agents. We introduce a suite of procedurally generated, game-like tasks with tunable complexity. These controlled environments allow us to provide precise oracle interventions, such as perfect planning or flawless state tracking, and make it possible to isolate the contribution of each oracle without confounding effects present in real-world benchmarks. Our results show that while some interventions (e.g., planning) consistently improve performance across settings, the usefulness of other skills is dependent on the properties of the environment and language model. Our work sheds light on the challenges of multi-turn agentic environments to guide the future efforts in the development of AI agents and language models.&lt;/p&gt;</content:encoded></item><item><title>Finite-Time Analysis of Gradient Descent for Shallow Transformers</title><link>https://arxiv.org/abs/2601.16514v1</link><guid>http://arxiv.org/abs/2601.16514v1</guid><pubDate>Fri, 23 Jan 2026 07:28:17 +0000</pubDate><dc:creator>Enes Arda</dc:creator><dc:creator>Semih Cayci</dc:creator><dc:creator>Atilla Eryilmaz</dc:creator><dc:type>preprint</dc:type><prism:publicationName>arXiv</prism:publicationName><description>Understanding why Transformers perform so well remains challenging due to their non-convex optimization landscape. In this work, we analyze a shallow Transformer with $m$ independent heads trained by projected gradient descent in the kernel regime. Our analysis reveals two main findings: (i) the width required for nonasymptotic guarantees scales only logarithmically with the sample size $n$, and (ii) the optimization error is independent of the sequence length $T$. This contrasts sharply with recurrent architectures, where the optimization error can grow exponentially with $T$. The trade-off is memory: to keep the full context, the Transformer's memory requirement grows with the sequence length. We validate our theoretical results numerically in a teacher-student setting and confirm the predicted scaling laws for Transformers.
Published: 2026-01-23T07:28:17+00:00
Venue: arXiv
Score: 0.771 (consider)</description><content:encoded>&lt;p&gt;&lt;strong&gt;Authors:&lt;/strong&gt; Enes Arda; Semih Cayci; Atilla Eryilmaz&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Journal:&lt;/strong&gt; arXiv&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Relevance:&lt;/strong&gt; 0.771 (consider)&lt;/p&gt;
&lt;p&gt;Understanding why Transformers perform so well remains challenging due to their non-convex optimization landscape. In this work, we analyze a shallow Transformer with $m$ independent heads trained by projected gradient descent in the kernel regime. Our analysis reveals two main findings: (i) the width required for nonasymptotic guarantees scales only logarithmically with the sample size $n$, and (ii) the optimization error is independent of the sequence length $T$. This contrasts sharply with recurrent architectures, where the optimization error can grow exponentially with $T$. The trade-off is memory: to keep the full context, the Transformer&amp;#x27;s memory requirement grows with the sequence length. We validate our theoretical results numerically in a teacher-student setting and confirm the predicted scaling laws for Transformers.&lt;/p&gt;</content:encoded></item><item><title>Matrix-Transformation Based Low-Rank Adaptation (MTLoRA): A Brain-Inspired Method for Parameter-Efficient Fine-Tuning</title><link>https://doi.org/10.1016/j.neunet.2026.108642</link><guid>10.1016/j.neunet.2026.108642</guid><pubDate>Sun, 25 Jan 2026 15:32:09 +0000</pubDate><dc:creator>Yao Liang</dc:creator><dc:creator>Yuwei Wang</dc:creator><dc:creator>Yang Li</dc:creator><dc:creator>Yi Zeng</dc:creator><prism:publicationName>Neural Networks</prism:publicationName><prism:doi>10.1016/j.neunet.2026.108642</prism:doi><description>Parameter-efficient fine-tuning (PEFT) reduces the compute and memory demands of adapting large language models, yet standard low-rank adapters (e.g., LoRA) can lag full fine-tuning in performance and stability because they restrict updates to a fixed rank- r subspace. We propose Matrix-Transformation based Low-Rank Adaptation (MTLoRA), a brain-inspired extension that inserts a learnable r × r transformation T into the low-rank update ( &amp;#x394; W = B T A " role="presentation"&gt; Δ W = B T A Δ W = B T A ). By endowing the subspace with data-adapted geometry (e.g., rotations, scalings, and shears), MTLoRA reparameterizes the rank- r hypothesis class, improving its conditioning and inductive bias at negligible O ( r 2 ) overhead, and recovers LoRA when T = I r " role="presentation"&gt; T = I r T = I r . We instantiate four structures for T —SHIM ( T = C ) " role="presentation"&gt; ( T = C ) ( T = C ) , ICFM ( T = C C &amp;#x22A4; ) " role="presentation"&gt; ( T = C C ⊤ ) ( T = C C ⊤ ) , CTCM ( T = C D ) " role="presentation"&gt; ( T = C D ) ( T = C D ) , and DTSM ( T = C + D ) " role="presentation"&gt; ( T = C + D ) ( T = C + D ) —providing complementary inductive biases (change of basis, PSD metric, staged mixing, dual superposition). An optimization analysis shows that T acts as a learned preconditioner within the subspace, yielding spectral-norm step-size bounds and operator-norm variance contraction that stabilize training. Empirically, MTLoRA delivers consistent gains while preserving PEFT efficiency: on GLUE (General Language Understanding Evaluation) with DeBERTaV3-base, MTLoRA improves the average over LoRA by (+2.0) points (86.9 → 88.9) and matches AdaLoRA (88.9) without any pruning schedule; on natural language generation with GPT-2 Medium, it raises BLEU on DART by (+0.95) and on WebNLG by (+0.56); and in multimodal instruction tuning with LLaVA-1.5-7B, DTSM attains the best average (69.91) with ∼ 4.7% trainable parameters, outperforming full fine-tuning and strong PEFT baselines. These results indicate that learning geometry inside the low-rank subspace improves both effectiveness and stability, making MTLoRA a practical, plug-compatible alternative to LoRA for large-model fine-tuning.
Published: 2026-01-25T15:32:09+00:00
Venue: Neural Networks
Score: 0.770 (consider)</description><content:encoded>&lt;p&gt;&lt;strong&gt;Authors:&lt;/strong&gt; Yao Liang; Yuwei Wang; Yang Li; Yi Zeng&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Journal:&lt;/strong&gt; Neural Networks&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;a href="https://doi.org/10.1016/j.neunet.2026.108642"&gt;10.1016/j.neunet.2026.108642&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Relevance:&lt;/strong&gt; 0.770 (consider)&lt;/p&gt;
&lt;p&gt;Parameter-efficient fine-tuning (PEFT) reduces the compute and memory demands of adapting large language models, yet standard low-rank adapters (e.g., LoRA) can lag full fine-tuning in performance and stability because they restrict updates to a fixed rank- r subspace. We propose Matrix-Transformation based Low-Rank Adaptation (MTLoRA), a brain-inspired extension that inserts a learnable r × r transformation T into the low-rank update ( &amp;amp;#x394; W = B T A &amp;quot; role=&amp;quot;presentation&amp;quot;&amp;gt; Δ W = B T A Δ W = B T A ). By endowing the subspace with data-adapted geometry (e.g., rotations, scalings, and shears), MTLoRA reparameterizes the rank- r hypothesis class, improving its conditioning and inductive bias at negligible O ( r 2 ) overhead, and recovers LoRA when T = I r &amp;quot; role=&amp;quot;presentation&amp;quot;&amp;gt; T = I r T = I r . We instantiate four structures for T —SHIM ( T = C ) &amp;quot; role=&amp;quot;presentation&amp;quot;&amp;gt; ( T = C ) ( T = C ) , ICFM ( T = C C &amp;amp;#x22A4; ) &amp;quot; role=&amp;quot;presentation&amp;quot;&amp;gt; ( T = C C ⊤ ) ( T = C C ⊤ ) , CTCM ( T = C D ) &amp;quot; role=&amp;quot;presentation&amp;quot;&amp;gt; ( T = C D ) ( T = C D ) , and DTSM ( T = C + D ) &amp;quot; role=&amp;quot;presentation&amp;quot;&amp;gt; ( T = C + D ) ( T = C + D ) —providing complementary inductive biases (change of basis, PSD metric, staged mixing, dual superposition). An optimization analysis shows that T acts as a learned preconditioner within the subspace, yielding spectral-norm step-size bounds and operator-norm variance contraction that stabilize training. Empirically, MTLoRA delivers consistent gains while preserving PEFT efficiency: on GLUE (General Language Understanding Evaluation) with DeBERTaV3-base, MTLoRA improves the average over LoRA by (+2.0) points (86.9 → 88.9) and matches AdaLoRA (88.9) without any pruning schedule; on natural language generation with GPT-2 Medium, it raises BLEU on DART by (+0.95) and on WebNLG by (+0.56); and in multimodal instruction tuning with LLaVA-1.5-7B, DTSM attains the best average (69.91) with ∼ 4.7% trainable parameters, outperforming full fine-tuning and strong PEFT baselines. These results indicate that learning geometry inside the low-rank subspace improves both effectiveness and stability, making MTLoRA a practical, plug-compatible alternative to LoRA for large-model fine-tuning.&lt;/p&gt;</content:encoded></item><item><title>Domain Generalization via Domain Uncertainty Shrinkage</title><link>https://doi.org/10.1016/j.patcog.2026.113118</link><guid>10.1016/j.patcog.2026.113118</guid><pubDate>Sat, 24 Jan 2026 00:15:20 +0000</pubDate><dc:creator>Jun-Zheng Chu</dc:creator><dc:creator>Bin Pan</dc:creator><dc:creator>Tian-Yang Shi</dc:creator><dc:creator>Zhen-Wei Shi</dc:creator><prism:publicationName>Pattern Recognition</prism:publicationName><prism:doi>10.1016/j.patcog.2026.113118</prism:doi><description>Ensuring model robustness against distributional shifts still presents a significant challenge in many machine learning applications. To address this issue, a wide range of domain generalization (DG) methods have been developed. However, these approaches mainly focus on invariant representations by leveraging multiple source domain data, which ignore the uncertainty presented from different domains. In this paper, we establish a novel DG framework in form of evidential deep learning (EDL-DG). To reach DG objective under finite given domains, we propose a new Domain Uncertainty Shrinkage ( DUS ) regularization scheme on the output Dirichlet distribution parameters, which achieves better generalization across unseen domains without introducing additional structures. Theoretically, we analyze the convergence of EDL-DG, and provide a generalization bound in the framework of PAC-Bayesian learning. We show that our proposed method reduce the PAC-Bayesian bound under certain conditions, and thus achieve better generalization across unseen domains. In our experiments, we validate the effectiveness our proposed method on DomainBed benchmark in multiple real-world datasets.
Published: 2026-01-24T00:15:20+00:00
Venue: Pattern Recognition
Score: 0.770 (consider)</description><content:encoded>&lt;p&gt;&lt;strong&gt;Authors:&lt;/strong&gt; Jun-Zheng Chu; Bin Pan; Tian-Yang Shi; Zhen-Wei Shi&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Journal:&lt;/strong&gt; Pattern Recognition&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;a href="https://doi.org/10.1016/j.patcog.2026.113118"&gt;10.1016/j.patcog.2026.113118&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Relevance:&lt;/strong&gt; 0.770 (consider)&lt;/p&gt;
&lt;p&gt;Ensuring model robustness against distributional shifts still presents a significant challenge in many machine learning applications. To address this issue, a wide range of domain generalization (DG) methods have been developed. However, these approaches mainly focus on invariant representations by leveraging multiple source domain data, which ignore the uncertainty presented from different domains. In this paper, we establish a novel DG framework in form of evidential deep learning (EDL-DG). To reach DG objective under finite given domains, we propose a new Domain Uncertainty Shrinkage ( DUS ) regularization scheme on the output Dirichlet distribution parameters, which achieves better generalization across unseen domains without introducing additional structures. Theoretically, we analyze the convergence of EDL-DG, and provide a generalization bound in the framework of PAC-Bayesian learning. We show that our proposed method reduce the PAC-Bayesian bound under certain conditions, and thus achieve better generalization across unseen domains. In our experiments, we validate the effectiveness our proposed method on DomainBed benchmark in multiple real-world datasets.&lt;/p&gt;</content:encoded></item><item><title>Understanding the Transfer Limits of Vision Foundation Models</title><link>https://arxiv.org/abs/2601.15888v1</link><guid>http://arxiv.org/abs/2601.15888v1</guid><pubDate>Thu, 22 Jan 2026 12:07:56 +0000</pubDate><dc:creator>Shiqi Huang</dc:creator><dc:creator>Yipei Wang</dc:creator><dc:creator>Natasha Thorley</dc:creator><dc:creator>Alexander Ng</dc:creator><dc:creator>Shaheer Saeed</dc:creator><dc:creator>Mark Emberton</dc:creator><dc:creator>Shonit Punwani</dc:creator><dc:creator>Veeru Kasivisvanathan</dc:creator><dc:creator>Dean Barratt</dc:creator><dc:creator>Daniel Alexander</dc:creator><dc:creator>Yipeng Hu</dc:creator><dc:type>preprint</dc:type><prism:publicationName>arXiv</prism:publicationName><description>Foundation models leverage large-scale pretraining to capture extensive knowledge, demonstrating generalization in a wide range of language tasks. By comparison, vision foundation models (VFMs) often exhibit uneven improvements across downstream tasks, despite substantial computational investment. We postulate that this limitation arises from a mismatch between pretraining objectives and the demands of downstream vision-and-imaging tasks. Pretraining strategies like masked image reconstruction or contrastive learning shape representations for tasks such as recovery of generic visual patterns or global semantic structures, which may not align with the task-specific requirements of downstream applications including segmentation, classification, or image synthesis. To investigate this in a concrete real-world clinical area, we assess two VFMs, a reconstruction-focused MAE-based model (ProFound) and a contrastive-learning-based model (ProViCNet), on five prostate multiparametric MR imaging tasks, examining how such task alignment influences transfer performance, i.e., from pretraining to fine-tuning. Our findings indicate that better alignment between pretraining and downstream tasks, measured by simple divergence metrics such as maximum-mean-discrepancy (MMD) between the same features before and after fine-tuning, correlates with greater performance improvements and faster convergence, emphasizing the importance of designing and analyzing pretraining objectives with downstream applicability in mind.
Published: 2026-01-22T12:07:56+00:00
Venue: arXiv
Score: 0.769 (consider)</description><content:encoded>&lt;p&gt;&lt;strong&gt;Authors:&lt;/strong&gt; Shiqi Huang; Yipei Wang; Natasha Thorley; Alexander Ng; Shaheer Saeed; Mark Emberton; Shonit Punwani; Veeru Kasivisvanathan; Dean Barratt; Daniel Alexander; Yipeng Hu&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Journal:&lt;/strong&gt; arXiv&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Relevance:&lt;/strong&gt; 0.769 (consider)&lt;/p&gt;
&lt;p&gt;Foundation models leverage large-scale pretraining to capture extensive knowledge, demonstrating generalization in a wide range of language tasks. By comparison, vision foundation models (VFMs) often exhibit uneven improvements across downstream tasks, despite substantial computational investment. We postulate that this limitation arises from a mismatch between pretraining objectives and the demands of downstream vision-and-imaging tasks. Pretraining strategies like masked image reconstruction or contrastive learning shape representations for tasks such as recovery of generic visual patterns or global semantic structures, which may not align with the task-specific requirements of downstream applications including segmentation, classification, or image synthesis. To investigate this in a concrete real-world clinical area, we assess two VFMs, a reconstruction-focused MAE-based model (ProFound) and a contrastive-learning-based model (ProViCNet), on five prostate multiparametric MR imaging tasks, examining how such task alignment influences transfer performance, i.e., from pretraining to fine-tuning. Our findings indicate that better alignment between pretraining and downstream tasks, measured by simple divergence metrics such as maximum-mean-discrepancy (MMD) between the same features before and after fine-tuning, correlates with greater performance improvements and faster convergence, emphasizing the importance of designing and analyzing pretraining objectives with downstream applicability in mind.&lt;/p&gt;</content:encoded></item><item><title>Affective Computing in the Era of Large Language Models: A Survey from the NLP Perspective</title><link>https://doi.org/10.1016/j.knosys.2026.115411</link><guid>10.1016/j.knosys.2026.115411</guid><pubDate>Sun, 25 Jan 2026 15:32:56 +0000</pubDate><dc:creator>Yiqun Zhang</dc:creator><dc:creator>Xiaocui Yang</dc:creator><dc:creator>Xingle Xu</dc:creator><dc:creator>Zeran Gao</dc:creator><dc:creator>Yijie Huang</dc:creator><dc:creator>Shiyi Mu</dc:creator><dc:creator>Shi Feng</dc:creator><dc:creator>Daling Wang</dc:creator><dc:creator>Yifei Zhang</dc:creator><dc:creator>Kaisong Song</dc:creator><dc:creator>Ge Yu</dc:creator><prism:publicationName>Knowledge-Based Systems</prism:publicationName><prism:doi>10.1016/j.knosys.2026.115411</prism:doi><description>Affective Computing (AC) integrates computer science, psychology, and cognitive science to enable machines to recognize, interpret, and simulate human emotions across domains such as social media, finance, healthcare, and education. AC commonly centers on two task families: Affective Understanding (AU) and Affective Generation (AG). While fine-tuned pre-trained language models (PLMs) have achieved solid AU performance, they often generalize poorly across tasks and remain limited for AG, especially in producing diverse, emotionally appropriate responses. The advent of Large Language Models (LLMs) (e.g., ChatGPT and LLaMA) has catalyzed a paradigm shift by offering in-context learning, broader world knowledge, and stronger sequence generation. This survey presents an Natural Language Processing (NLP)-oriented overview of AC in the LLM era. We (i) consolidate traditional AC tasks and preliminary LLM-based studies; (ii) review adaptation techniques that improve AU/AG, including Instruction Tuning (full and parameter-efficient methods), Prompt Engineering (zero/few-shot, chain-of-thought, agent-based prompting), and Reinforcement Learning (RL). For the latter, we summarize RL from human preferences, verifiable/programmatic rewards, and model(s) feedback, which provide preference- or rule-grounded optimization signals that can help steer AU/AG toward empathy, safety, and planning, achieving finer-grained or multi-objective control. To assess progress, we compile benchmarks and evaluation practices for both AU and AG. We also discuss open challenges—from ethics, data quality, and safety to robust evaluation and resource efficiency—and outline research directions. We hope this survey clarifies the landscape and offers practical guidance for building affect-aware, reliable, and responsible LLM systems.
Published: 2026-01-25T15:32:56+00:00
Venue: Knowledge-Based Systems
Score: 0.768 (consider)</description><content:encoded>&lt;p&gt;&lt;strong&gt;Authors:&lt;/strong&gt; Yiqun Zhang; Xiaocui Yang; Xingle Xu; Zeran Gao; Yijie Huang; Shiyi Mu; Shi Feng; Daling Wang; Yifei Zhang; Kaisong Song; Ge Yu&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Journal:&lt;/strong&gt; Knowledge-Based Systems&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;a href="https://doi.org/10.1016/j.knosys.2026.115411"&gt;10.1016/j.knosys.2026.115411&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Relevance:&lt;/strong&gt; 0.768 (consider)&lt;/p&gt;
&lt;p&gt;Affective Computing (AC) integrates computer science, psychology, and cognitive science to enable machines to recognize, interpret, and simulate human emotions across domains such as social media, finance, healthcare, and education. AC commonly centers on two task families: Affective Understanding (AU) and Affective Generation (AG). While fine-tuned pre-trained language models (PLMs) have achieved solid AU performance, they often generalize poorly across tasks and remain limited for AG, especially in producing diverse, emotionally appropriate responses. The advent of Large Language Models (LLMs) (e.g., ChatGPT and LLaMA) has catalyzed a paradigm shift by offering in-context learning, broader world knowledge, and stronger sequence generation. This survey presents an Natural Language Processing (NLP)-oriented overview of AC in the LLM era. We (i) consolidate traditional AC tasks and preliminary LLM-based studies; (ii) review adaptation techniques that improve AU/AG, including Instruction Tuning (full and parameter-efficient methods), Prompt Engineering (zero/few-shot, chain-of-thought, agent-based prompting), and Reinforcement Learning (RL). For the latter, we summarize RL from human preferences, verifiable/programmatic rewards, and model(s) feedback, which provide preference- or rule-grounded optimization signals that can help steer AU/AG toward empathy, safety, and planning, achieving finer-grained or multi-objective control. To assess progress, we compile benchmarks and evaluation practices for both AU and AG. We also discuss open challenges—from ethics, data quality, and safety to robust evaluation and resource efficiency—and outline research directions. We hope this survey clarifies the landscape and offers practical guidance for building affect-aware, reliable, and responsible LLM systems.&lt;/p&gt;</content:encoded></item><item><title>From Passive Metric to Active Signal: The Evolving Role of Uncertainty Quantification in Large Language Models</title><link>https://arxiv.org/abs/2601.15690v1</link><guid>http://arxiv.org/abs/2601.15690v1</guid><pubDate>Thu, 22 Jan 2026 06:21:31 +0000</pubDate><dc:creator>Jiaxin Zhang</dc:creator><dc:creator>Wendi Cui</dc:creator><dc:creator>Zhuohang Li</dc:creator><dc:creator>Lifu Huang</dc:creator><dc:creator>Bradley Malin</dc:creator><dc:creator>Caiming Xiong</dc:creator><dc:creator>Chien-Sheng Wu</dc:creator><dc:type>preprint</dc:type><prism:publicationName>arXiv</prism:publicationName><description>While Large Language Models (LLMs) show remarkable capabilities, their unreliability remains a critical barrier to deployment in high-stakes domains. This survey charts a functional evolution in addressing this challenge: the evolution of uncertainty from a passive diagnostic metric to an active control signal guiding real-time model behavior. We demonstrate how uncertainty is leveraged as an active control signal across three frontiers: in \textbf{advanced reasoning} to optimize computation and trigger self-correction; in \textbf{autonomous agents} to govern metacognitive decisions about tool use and information seeking; and in \textbf{reinforcement learning} to mitigate reward hacking and enable self-improvement via intrinsic rewards. By grounding these advancements in emerging theoretical frameworks like Bayesian methods and Conformal Prediction, we provide a unified perspective on this transformative trend. This survey provides a comprehensive overview, critical analysis, and practical design patterns, arguing that mastering the new trend of uncertainty is essential for building the next generation of scalable, reliable, and trustworthy AI.
Published: 2026-01-22T06:21:31+00:00
Venue: arXiv
Score: 0.768 (consider)</description><content:encoded>&lt;p&gt;&lt;strong&gt;Authors:&lt;/strong&gt; Jiaxin Zhang; Wendi Cui; Zhuohang Li; Lifu Huang; Bradley Malin; Caiming Xiong; Chien-Sheng Wu&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Journal:&lt;/strong&gt; arXiv&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Relevance:&lt;/strong&gt; 0.768 (consider)&lt;/p&gt;
&lt;p&gt;While Large Language Models (LLMs) show remarkable capabilities, their unreliability remains a critical barrier to deployment in high-stakes domains. This survey charts a functional evolution in addressing this challenge: the evolution of uncertainty from a passive diagnostic metric to an active control signal guiding real-time model behavior. We demonstrate how uncertainty is leveraged as an active control signal across three frontiers: in \textbf{advanced reasoning} to optimize computation and trigger self-correction; in \textbf{autonomous agents} to govern metacognitive decisions about tool use and information seeking; and in \textbf{reinforcement learning} to mitigate reward hacking and enable self-improvement via intrinsic rewards. By grounding these advancements in emerging theoretical frameworks like Bayesian methods and Conformal Prediction, we provide a unified perspective on this transformative trend. This survey provides a comprehensive overview, critical analysis, and practical design patterns, arguing that mastering the new trend of uncertainty is essential for building the next generation of scalable, reliable, and trustworthy AI.&lt;/p&gt;</content:encoded></item><item><title>Consistency-Regularized GAN for Few-Shot SAR Target Recognition</title><link>https://arxiv.org/abs/2601.15681v1</link><guid>http://arxiv.org/abs/2601.15681v1</guid><pubDate>Thu, 22 Jan 2026 06:02:39 +0000</pubDate><dc:creator>Yikui Zhai</dc:creator><dc:creator>Shikuang Liu</dc:creator><dc:creator>Wenlve Zhou</dc:creator><dc:creator>Hongsheng Zhang</dc:creator><dc:creator>Zhiheng Zhou</dc:creator><dc:creator>Xiaolin Tian</dc:creator><dc:creator>C. L. Philip Chen</dc:creator><dc:type>preprint</dc:type><prism:publicationName>arXiv</prism:publicationName><description>Few-shot recognition in synthetic aperture radar (SAR) imagery remains a critical bottleneck for real-world applications due to extreme data scarcity. A promising strategy involves synthesizing a large dataset with a generative adversarial network (GAN), pre-training a model via self-supervised learning (SSL), and then fine-tuning on the few labeled samples. However, this approach faces a fundamental paradox: conventional GANs themselves require abundant data for stable training, contradicting the premise of few-shot learning. To resolve this, we propose the consistency-regularized generative adversarial network (Cr-GAN), a novel framework designed to synthesize diverse, high-fidelity samples even when trained under these severe data limitations. Cr-GAN introduces a dual-branch discriminator that decouples adversarial training from representation learning. This architecture enables a channel-wise feature interpolation strategy to create novel latent features, complemented by a dual-domain cycle consistency mechanism that ensures semantic integrity. Our Cr-GAN framework is adaptable to various GAN architectures, and its synthesized data effectively boosts multiple SSL algorithms. Extensive experiments on the MSTAR and SRSDD datasets validate our approach, with Cr-GAN achieving a highly competitive accuracy of 71.21% and 51.64%, respectively, in the 8-shot setting, significantly outperforming leading baselines, while requiring only ~5 of the parameters of state-of-the-art diffusion models. Code is available at: https://github.com/yikuizhai/Cr-GAN.
Published: 2026-01-22T06:02:39+00:00
Venue: arXiv
Score: 0.768 (consider)</description><content:encoded>&lt;p&gt;&lt;strong&gt;Authors:&lt;/strong&gt; Yikui Zhai; Shikuang Liu; Wenlve Zhou; Hongsheng Zhang; Zhiheng Zhou; Xiaolin Tian; C. L. Philip Chen&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Journal:&lt;/strong&gt; arXiv&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Relevance:&lt;/strong&gt; 0.768 (consider)&lt;/p&gt;
&lt;p&gt;Few-shot recognition in synthetic aperture radar (SAR) imagery remains a critical bottleneck for real-world applications due to extreme data scarcity. A promising strategy involves synthesizing a large dataset with a generative adversarial network (GAN), pre-training a model via self-supervised learning (SSL), and then fine-tuning on the few labeled samples. However, this approach faces a fundamental paradox: conventional GANs themselves require abundant data for stable training, contradicting the premise of few-shot learning. To resolve this, we propose the consistency-regularized generative adversarial network (Cr-GAN), a novel framework designed to synthesize diverse, high-fidelity samples even when trained under these severe data limitations. Cr-GAN introduces a dual-branch discriminator that decouples adversarial training from representation learning. This architecture enables a channel-wise feature interpolation strategy to create novel latent features, complemented by a dual-domain cycle consistency mechanism that ensures semantic integrity. Our Cr-GAN framework is adaptable to various GAN architectures, and its synthesized data effectively boosts multiple SSL algorithms. Extensive experiments on the MSTAR and SRSDD datasets validate our approach, with Cr-GAN achieving a highly competitive accuracy of 71.21% and 51.64%, respectively, in the 8-shot setting, significantly outperforming leading baselines, while requiring only ~5 of the parameters of state-of-the-art diffusion models. Code is available at: https://github.com/yikuizhai/Cr-GAN.&lt;/p&gt;</content:encoded></item><item><title>Recurrence mimicking learning: Eliminating sequential rollouts in offline recurrent reinforcement learning</title><link>https://doi.org/10.1016/j.neucom.2026.132807</link><guid>10.1016/j.neucom.2026.132807</guid><pubDate>Sat, 24 Jan 2026 00:22:04 +0000</pubDate><dc:creator>Tomasz Witkowski</dc:creator><dc:creator>Krzysztof Kania</dc:creator><dc:creator>Tomasz Wachowicz</dc:creator><prism:publicationName>Neurocomputing</prism:publicationName><prism:doi>10.1016/j.neucom.2026.132807</prism:doi><description>Recurrent Reinforcement Learning (RRL) is widely used in settings where actions depend on previous decisions, such as dynamic decision-making. However, offline RRL suffers from a major computational drawback: it evaluates trajectories step by step, making training inefficient for long horizons, complex models, and high-dimensional features. To address this, we propose Recurrence Mimicking Learning (RML), an approach that reorders offline RRL rollouts to require only two batched forward passes per epoch, independent of horizon length. RML enumerates all previous actions in a single pass and reconstructs the exact recurrent path through a lightweight selection step. Experiments show that RML preserves the exact final action trajectory of standard offline RRL, allows direct optimization of global rewards, and reduces training computation time to approximately 5% of the conventional approach, while scaling efficiently with both sequence length and action space size.
Published: 2026-01-24T00:22:04+00:00
Venue: Neurocomputing
Score: 0.767 (consider)</description><content:encoded>&lt;p&gt;&lt;strong&gt;Authors:&lt;/strong&gt; Tomasz Witkowski; Krzysztof Kania; Tomasz Wachowicz&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Journal:&lt;/strong&gt; Neurocomputing&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;a href="https://doi.org/10.1016/j.neucom.2026.132807"&gt;10.1016/j.neucom.2026.132807&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Relevance:&lt;/strong&gt; 0.767 (consider)&lt;/p&gt;
&lt;p&gt;Recurrent Reinforcement Learning (RRL) is widely used in settings where actions depend on previous decisions, such as dynamic decision-making. However, offline RRL suffers from a major computational drawback: it evaluates trajectories step by step, making training inefficient for long horizons, complex models, and high-dimensional features. To address this, we propose Recurrence Mimicking Learning (RML), an approach that reorders offline RRL rollouts to require only two batched forward passes per epoch, independent of horizon length. RML enumerates all previous actions in a single pass and reconstructs the exact recurrent path through a lightweight selection step. Experiments show that RML preserves the exact final action trajectory of standard offline RRL, allows direct optimization of global rewards, and reduces training computation time to approximately 5% of the conventional approach, while scaling efficiently with both sequence length and action space size.&lt;/p&gt;</content:encoded></item><item><title>Auto-Regressive Masked Diffusion Models</title><link>https://arxiv.org/abs/2601.16971v1</link><guid>http://arxiv.org/abs/2601.16971v1</guid><pubDate>Fri, 23 Jan 2026 18:42:30 +0000</pubDate><dc:creator>Mahdi Karami</dc:creator><dc:creator>Ali Ghodsi</dc:creator><dc:type>preprint</dc:type><prism:publicationName>arXiv</prism:publicationName><description>Masked diffusion models (MDMs) have emerged as a promising approach for language modeling, yet they face a performance gap compared to autoregressive models (ARMs) and require more training iterations. In this work, we present the Auto-Regressive Masked Diffusion (ARMD) model, an architecture designed to close this gap by unifying the training efficiency of autoregressive models with the parallel generation capabilities of diffusion-based models. Our key insight is to reframe the masked diffusion process as a block-wise causal model. This perspective allows us to design a strictly causal, permutation-equivariant architecture that computes all conditional probabilities across multiple denoising steps in a single, parallel forward pass. The resulting architecture supports efficient, autoregressive-style decoding and a progressive permutation training scheme, allowing the model to learn both canonical left-to-right and random token orderings. Leveraging this flexibility, we introduce a novel strided parallel generation strategy that accelerates inference by generating tokens in parallel streams while maintaining global coherence. Empirical results demonstrate that ARMD achieves state-of-the-art performance on standard language modeling benchmarks, outperforming established diffusion baselines while requiring significantly fewer training steps. Furthermore, it establishes a new benchmark for parallel text generation, effectively bridging the performance gap between parallel and sequential decoding.
Published: 2026-01-23T18:42:30+00:00
Venue: arXiv
Score: 0.767 (consider)</description><content:encoded>&lt;p&gt;&lt;strong&gt;Authors:&lt;/strong&gt; Mahdi Karami; Ali Ghodsi&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Journal:&lt;/strong&gt; arXiv&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Relevance:&lt;/strong&gt; 0.767 (consider)&lt;/p&gt;
&lt;p&gt;Masked diffusion models (MDMs) have emerged as a promising approach for language modeling, yet they face a performance gap compared to autoregressive models (ARMs) and require more training iterations. In this work, we present the Auto-Regressive Masked Diffusion (ARMD) model, an architecture designed to close this gap by unifying the training efficiency of autoregressive models with the parallel generation capabilities of diffusion-based models. Our key insight is to reframe the masked diffusion process as a block-wise causal model. This perspective allows us to design a strictly causal, permutation-equivariant architecture that computes all conditional probabilities across multiple denoising steps in a single, parallel forward pass. The resulting architecture supports efficient, autoregressive-style decoding and a progressive permutation training scheme, allowing the model to learn both canonical left-to-right and random token orderings. Leveraging this flexibility, we introduce a novel strided parallel generation strategy that accelerates inference by generating tokens in parallel streams while maintaining global coherence. Empirical results demonstrate that ARMD achieves state-of-the-art performance on standard language modeling benchmarks, outperforming established diffusion baselines while requiring significantly fewer training steps. Furthermore, it establishes a new benchmark for parallel text generation, effectively bridging the performance gap between parallel and sequential decoding.&lt;/p&gt;</content:encoded></item><item><title>GPA-VGGT:Adapting VGGT to Large scale Localization by self-Supervised learning with Geometry and Physics Aware loss</title><link>https://arxiv.org/abs/2601.16885v1</link><guid>http://arxiv.org/abs/2601.16885v1</guid><pubDate>Fri, 23 Jan 2026 16:46:59 +0000</pubDate><dc:creator>Yangfan Xu</dc:creator><dc:creator>Lilian Zhang</dc:creator><dc:creator>Xiaofeng He</dc:creator><dc:creator>Pengdong Wu</dc:creator><dc:creator>Wenqi Wu</dc:creator><dc:creator>Jun Mao</dc:creator><dc:type>preprint</dc:type><prism:publicationName>arXiv</prism:publicationName><description>Transformer-based general visual geometry frameworks have shown promising performance in camera pose estimation and 3D scene understanding. Recent advancements in Visual Geometry Grounded Transformer (VGGT) models have shown great promise in camera pose estimation and 3D reconstruction. However, these models typically rely on ground truth labels for training, posing challenges when adapting to unlabeled and unseen scenes. In this paper, we propose a self-supervised framework to train VGGT with unlabeled data, thereby enhancing its localization capability in large-scale environments. To achieve this, we extend conventional pair-wise relations to sequence-wise geometric constraints for self-supervised learning. Specifically, in each sequence, we sample multiple source frames and geometrically project them onto different target frames, which improves temporal feature consistency. We formulate physical photometric consistency and geometric constraints as a joint optimization loss to circumvent the requirement for hard labels. By training the model with this proposed method, not only the local and global cross-view attention layers but also the camera and depth heads can effectively capture the underlying multi-view geometry. Experiments demonstrate that the model converges within hundreds of iterations and achieves significant improvements in large-scale localization. Our code will be released at https://github.com/X-yangfan/GPA-VGGT.
Published: 2026-01-23T16:46:59+00:00
Venue: arXiv
Score: 0.766 (consider)</description><content:encoded>&lt;p&gt;&lt;strong&gt;Authors:&lt;/strong&gt; Yangfan Xu; Lilian Zhang; Xiaofeng He; Pengdong Wu; Wenqi Wu; Jun Mao&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Journal:&lt;/strong&gt; arXiv&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Relevance:&lt;/strong&gt; 0.766 (consider)&lt;/p&gt;
&lt;p&gt;Transformer-based general visual geometry frameworks have shown promising performance in camera pose estimation and 3D scene understanding. Recent advancements in Visual Geometry Grounded Transformer (VGGT) models have shown great promise in camera pose estimation and 3D reconstruction. However, these models typically rely on ground truth labels for training, posing challenges when adapting to unlabeled and unseen scenes. In this paper, we propose a self-supervised framework to train VGGT with unlabeled data, thereby enhancing its localization capability in large-scale environments. To achieve this, we extend conventional pair-wise relations to sequence-wise geometric constraints for self-supervised learning. Specifically, in each sequence, we sample multiple source frames and geometrically project them onto different target frames, which improves temporal feature consistency. We formulate physical photometric consistency and geometric constraints as a joint optimization loss to circumvent the requirement for hard labels. By training the model with this proposed method, not only the local and global cross-view attention layers but also the camera and depth heads can effectively capture the underlying multi-view geometry. Experiments demonstrate that the model converges within hundreds of iterations and achieves significant improvements in large-scale localization. Our code will be released at https://github.com/X-yangfan/GPA-VGGT.&lt;/p&gt;</content:encoded></item><item><title>MambaOcc: Visual State Space Models for BEV-based Occupancy Prediction with Local Adaptive Reordering</title><link>https://doi.org/10.1016/j.eswa.2026.131305</link><guid>10.1016/j.eswa.2026.131305</guid><pubDate>Sat, 24 Jan 2026 16:15:31 +0000</pubDate><dc:creator>Yonglin Tian</dc:creator><dc:creator>Songlin Bai</dc:creator><dc:creator>Zhiyao Luo</dc:creator><dc:creator>Yutong Wang</dc:creator><dc:creator>Hui Zhang</dc:creator><dc:creator>Baoqing Guo</dc:creator><dc:creator>Yisheng Lv</dc:creator><dc:creator>Fei-Yue Wang</dc:creator><prism:publicationName>Expert Systems with Applications</prism:publicationName><prism:doi>10.1016/j.eswa.2026.131305</prism:doi><description>Occupancy prediction has attracted intensive attention and shown great superiority in the development of autonomous driving systems. The fine-grained environmental representation brought by occupancy prediction in terms of both geometry and semantic information has facilitated the general perception and safe planning under open scenarios. However, it also brings high computation costs and heavy parameters in existing works that utilize voxel-based 3d dense representation and Transformer-based quadratic attention. To address these challenges, in this paper, we propose a Mamba-based occupancy prediction method (MambaOcc) adopting BEV features to ease the burden of 3D scenario representation, and linear Mamba-style attention to achieve efficient long-range perception. Besides, to address the sensitivity of Mamba to sequence order, we propose a local adaptive reordering (LAR) mechanism with deformable convolution and design a hybrid BEV encoder comprised of convolution layers and Mamba. Extensive experiments on the Occ3D-nuScenes dataset demonstrate that MambaOcc achieves state-of-the-art performance in terms of both accuracy and computational efficiency. For example, compared to FlashOcc, MambaOcc delivers superior results while reducing the number of parameters by 42% and computational costs by 39%. The code is available at https://github.com/Hub-Tian/MambaOcc .
Published: 2026-01-24T16:15:31+00:00
Venue: Expert Systems with Applications
Score: 0.764 (consider)</description><content:encoded>&lt;p&gt;&lt;strong&gt;Authors:&lt;/strong&gt; Yonglin Tian; Songlin Bai; Zhiyao Luo; Yutong Wang; Hui Zhang; Baoqing Guo; Yisheng Lv; Fei-Yue Wang&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Journal:&lt;/strong&gt; Expert Systems with Applications&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;a href="https://doi.org/10.1016/j.eswa.2026.131305"&gt;10.1016/j.eswa.2026.131305&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Relevance:&lt;/strong&gt; 0.764 (consider)&lt;/p&gt;
&lt;p&gt;Occupancy prediction has attracted intensive attention and shown great superiority in the development of autonomous driving systems. The fine-grained environmental representation brought by occupancy prediction in terms of both geometry and semantic information has facilitated the general perception and safe planning under open scenarios. However, it also brings high computation costs and heavy parameters in existing works that utilize voxel-based 3d dense representation and Transformer-based quadratic attention. To address these challenges, in this paper, we propose a Mamba-based occupancy prediction method (MambaOcc) adopting BEV features to ease the burden of 3D scenario representation, and linear Mamba-style attention to achieve efficient long-range perception. Besides, to address the sensitivity of Mamba to sequence order, we propose a local adaptive reordering (LAR) mechanism with deformable convolution and design a hybrid BEV encoder comprised of convolution layers and Mamba. Extensive experiments on the Occ3D-nuScenes dataset demonstrate that MambaOcc achieves state-of-the-art performance in terms of both accuracy and computational efficiency. For example, compared to FlashOcc, MambaOcc delivers superior results while reducing the number of parameters by 42% and computational costs by 39%. The code is available at https://github.com/Hub-Tian/MambaOcc .&lt;/p&gt;</content:encoded></item></channel></rss>