<?xml version='1.0' encoding='utf-8'?>
<rss xmlns:content="http://purl.org/rss/1.0/modules/content/" xmlns:dc="http://purl.org/dc/elements/1.1/" xmlns:prism="http://prismstandard.org/namespaces/basic/3.0/" version="2.0"><channel><title>ZotWatch Feed</title><link>https://ehehe.cn/zotwatch/</link><description>AI-assisted literature watch</description><lastBuildDate>Mon, 19 Jan 2026 02:51:27 +0000</lastBuildDate><item><title>ImCapDA: Fine-tuning CLIP via Image Captions for Unsupervised Domain Adaptation</title><link>https://doi.org/10.1016/j.eswa.2026.131248</link><guid>10.1016/j.eswa.2026.131248</guid><pubDate>Sun, 18 Jan 2026 03:11:38 +0000</pubDate><dc:creator>Weiwei Xiang</dc:creator><dc:creator>Guangyi Xiao</dc:creator><dc:creator>Shun Peng</dc:creator><dc:creator>Hao Chen</dc:creator><dc:creator>Liming Ding</dc:creator><dc:creator>Lei Yang</dc:creator><prism:publicationName>Expert Systems with Applications</prism:publicationName><prism:doi>10.1016/j.eswa.2026.131248</prism:doi><description>Large vision–language models (VLMs) such as CLIP have demonstrated remarkable zero-shot generalization, yet their potential for unsupervised domain adaptation (UDA) remains underexplored. Existing approaches typically enhance transfer by optimizing visual representations via encoder fine-tuning or improving text prompts, but they either overlook fine-tuning of the text encoder or fail to fully exploit multimodal alignment, often suffering from catastrophic forgetting or limited domain generalization.
Published: 2026-01-18T03:11:38+00:00
Venue: Expert Systems with Applications
Score: 0.804 (must_read)</description><content:encoded>&lt;p&gt;&lt;strong&gt;Authors:&lt;/strong&gt; Weiwei Xiang; Guangyi Xiao; Shun Peng; Hao Chen; Liming Ding; Lei Yang&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Journal:&lt;/strong&gt; Expert Systems with Applications&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;a href="https://doi.org/10.1016/j.eswa.2026.131248"&gt;10.1016/j.eswa.2026.131248&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Relevance:&lt;/strong&gt; 0.804 (must_read)&lt;/p&gt;
&lt;p&gt;Large vision–language models (VLMs) such as CLIP have demonstrated remarkable zero-shot generalization, yet their potential for unsupervised domain adaptation (UDA) remains underexplored. Existing approaches typically enhance transfer by optimizing visual representations via encoder fine-tuning or improving text prompts, but they either overlook fine-tuning of the text encoder or fail to fully exploit multimodal alignment, often suffering from catastrophic forgetting or limited domain generalization.&lt;/p&gt;</content:encoded></item><item><title>YOLO-PICO: Lightweight Object Recognition in Remote Sensing Images using Expansion Attention Modules</title><link>https://doi.org/10.1016/j.patcog.2026.113114</link><guid>10.1016/j.patcog.2026.113114</guid><pubDate>Sat, 17 Jan 2026 15:41:57 +0000</pubDate><dc:creator>Mohamad Ebrahim Aghili</dc:creator><dc:creator>Hassan Ghassemian</dc:creator><dc:creator>Maryam Imani</dc:creator><prism:publicationName>Pattern Recognition</prism:publicationName><prism:doi>10.1016/j.patcog.2026.113114</prism:doi><description>Recognizing small objects in remote sensing imagery remains a significant challenge. This paper introduces YOLO-PICO, a novel and highly efficient object detector designed for small object recognition. At its core is the Expansion Attention (EA) Module, a new operator for spatial-channel feature fusion that enhances fine-grained details with minimal computational cost. This allows YOLO-PICO to achieve competitive performance with significantly fewer parameters than existing models, as demonstrated by our new parameter efficiency metric, Size-Normalized Average Precision (SNAP). Furthermore, we show that YOLO-PICO's efficiency makes it an ideal foundation for an Ensemble of Specialists (EoS) framework, a decision-level fusion strategy that substantially boosts detection accuracy with a modest increase in inference time. Our results demonstrate that this combination of an efficient core model and an advanced fusion strategy offers a compelling solution for high-performance recognition on resource-constrained platforms. The code will be made available at: https://github.com/MohamadEbrahimAghili/YOLO-PICO .
Published: 2026-01-17T15:41:57+00:00
Venue: Pattern Recognition
Score: 0.799 (must_read)</description><content:encoded>&lt;p&gt;&lt;strong&gt;Authors:&lt;/strong&gt; Mohamad Ebrahim Aghili; Hassan Ghassemian; Maryam Imani&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Journal:&lt;/strong&gt; Pattern Recognition&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;a href="https://doi.org/10.1016/j.patcog.2026.113114"&gt;10.1016/j.patcog.2026.113114&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Relevance:&lt;/strong&gt; 0.799 (must_read)&lt;/p&gt;
&lt;p&gt;Recognizing small objects in remote sensing imagery remains a significant challenge. This paper introduces YOLO-PICO, a novel and highly efficient object detector designed for small object recognition. At its core is the Expansion Attention (EA) Module, a new operator for spatial-channel feature fusion that enhances fine-grained details with minimal computational cost. This allows YOLO-PICO to achieve competitive performance with significantly fewer parameters than existing models, as demonstrated by our new parameter efficiency metric, Size-Normalized Average Precision (SNAP). Furthermore, we show that YOLO-PICO&amp;#x27;s efficiency makes it an ideal foundation for an Ensemble of Specialists (EoS) framework, a decision-level fusion strategy that substantially boosts detection accuracy with a modest increase in inference time. Our results demonstrate that this combination of an efficient core model and an advanced fusion strategy offers a compelling solution for high-performance recognition on resource-constrained platforms. The code will be made available at: https://github.com/MohamadEbrahimAghili/YOLO-PICO .&lt;/p&gt;</content:encoded></item><item><title>An Adaptive Regularized Topological Segmentation Network Integrating Inter-Class Relations and Occlusion Information for Vehicle Component Recognition</title><link>https://doi.org/10.1016/j.inffus.2026.104157</link><guid>10.1016/j.inffus.2026.104157</guid><pubDate>Sun, 18 Jan 2026 04:47:28 +0000</pubDate><dc:creator>Xunqi Zhou</dc:creator><dc:creator>Zhenqi Zhang</dc:creator><dc:creator>Zifeng Wu</dc:creator><dc:creator>Qianming Wang</dc:creator><dc:creator>Jing Teng</dc:creator><dc:creator>Jinlong Liu</dc:creator><dc:creator>Yongjie Zhai</dc:creator><prism:publicationName>Information Fusion</prism:publicationName><prism:doi>10.1016/j.inffus.2026.104157</prism:doi><description>In intelligent vehicle damage assessment, component recognition faces challenges such as significant intra-class variability and minimal inter-class differences, which hinder detection, as well as occlusions and ambiguous boundaries, which complicate segmentation. We generalize these problems into three core aspects: inter-object relational modeling, semantic-detail information balancing, and occlusion-aware decoupling. To this end, we propose the Adaptive Regularized Topological Segmentation (ARTSeg) network, comprising three complementary modules: Inter-Class Graph Constraint (ICGC), Constrained Detail Feature Backtracking (CDFB), and Topological Decoupling Segmentation (TDS). Each module is purposefully designed, integrated in a progressive structure, and synergistically reinforces the others to enhance overall performance. Specifically, ICGC clusters intra-class features and establishes implicit topological constraints among categories during feature extraction, enabling the model to better capture inter-class relationships and improve detection representation. Subsequently, CDFB evaluates the impact of channel-wise feature information within each candidate region on segmentation accuracy and computational cost, dynamically selecting appropriate feature resolutions for individual instances while balancing the demands of detection and segmentation tasks. Finally, TDS introduces topological associations between occluded and occluding regions at the feature level and decouples them at the task level, explicitly modeling generalized occlusion regions and enhancing segmentation performance. We quantitatively and qualitatively evaluate ARTSeg on a 59-category vehicle component dataset constructed for insurance damage assessment, achieving notable improvements in addressing the aforementioned problems. Experiments on two public datasets, DSMLR and Carparts, further validate the generalization capability of the proposed method. Results indicate that ARTSeg provides practical guidance for component recognition in intelligent vehicle damage assessment.
Published: 2026-01-18T04:47:28+00:00
Venue: Information Fusion
Score: 0.799 (must_read)</description><content:encoded>&lt;p&gt;&lt;strong&gt;Authors:&lt;/strong&gt; Xunqi Zhou; Zhenqi Zhang; Zifeng Wu; Qianming Wang; Jing Teng; Jinlong Liu; Yongjie Zhai&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Journal:&lt;/strong&gt; Information Fusion&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;a href="https://doi.org/10.1016/j.inffus.2026.104157"&gt;10.1016/j.inffus.2026.104157&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Relevance:&lt;/strong&gt; 0.799 (must_read)&lt;/p&gt;
&lt;p&gt;In intelligent vehicle damage assessment, component recognition faces challenges such as significant intra-class variability and minimal inter-class differences, which hinder detection, as well as occlusions and ambiguous boundaries, which complicate segmentation. We generalize these problems into three core aspects: inter-object relational modeling, semantic-detail information balancing, and occlusion-aware decoupling. To this end, we propose the Adaptive Regularized Topological Segmentation (ARTSeg) network, comprising three complementary modules: Inter-Class Graph Constraint (ICGC), Constrained Detail Feature Backtracking (CDFB), and Topological Decoupling Segmentation (TDS). Each module is purposefully designed, integrated in a progressive structure, and synergistically reinforces the others to enhance overall performance. Specifically, ICGC clusters intra-class features and establishes implicit topological constraints among categories during feature extraction, enabling the model to better capture inter-class relationships and improve detection representation. Subsequently, CDFB evaluates the impact of channel-wise feature information within each candidate region on segmentation accuracy and computational cost, dynamically selecting appropriate feature resolutions for individual instances while balancing the demands of detection and segmentation tasks. Finally, TDS introduces topological associations between occluded and occluding regions at the feature level and decouples them at the task level, explicitly modeling generalized occlusion regions and enhancing segmentation performance. We quantitatively and qualitatively evaluate ARTSeg on a 59-category vehicle component dataset constructed for insurance damage assessment, achieving notable improvements in addressing the aforementioned problems. Experiments on two public datasets, DSMLR and Carparts, further validate the generalization capability of the proposed method. Results indicate that ARTSeg provides practical guidance for component recognition in intelligent vehicle damage assessment.&lt;/p&gt;</content:encoded></item><item><title>PIDE-Net: A Heterogeneous Processing Paradigm for UAV Object Detection</title><link>https://doi.org/10.1016/j.eswa.2026.131194</link><guid>10.1016/j.eswa.2026.131194</guid><pubDate>Sun, 18 Jan 2026 06:31:10 +0000</pubDate><dc:creator>Shuming Lin</dc:creator><dc:creator>Sang Fyeng</dc:creator><dc:creator>Jinyi Liang</dc:creator><dc:creator>Junnan Tan</dc:creator><prism:publicationName>Expert Systems with Applications</prism:publicationName><prism:doi>10.1016/j.eswa.2026.131194</prism:doi><description>Small object detection in unmanned aerial vehicle (UAV) imagery confronts multifaceted technical challenges encompassing severe geometric deformations, dense target clustering, and stringent computational resource constraints. Contemporary detection frameworks predominantly adopt homogeneous processing paradigms, which suffer from systematic information deterioration across feature representation, contextual modeling, and multi-scale fusion stages, constituting a fundamental performance bottlenecks in UAV scenarios. This paper introduces PIDE-Net (Progressive Information Disentanglement and Enhancement Network), establishing a heterogeneous processing paradigm that achieves synergistic optimization of detection accuracy and computational efficiency. The framework implements progressive information refinement through three core modules.The Position-aware Refined Interactive Semantic Module (PRISM) employs a position-semantic feature disentanglement mechanism to address information confusion in complex scenarios at the source of feature representation.The Semantic-Guided State Space Module (SG-SSM) introduces content-driven attention state space equations, enabling efficient global context modeling with O(n) linear complexity. Finally, the Progressive Enhancement Pyramid Network (PEP-Net) adopts spatial weaving upsampling mechanisms to preserve sparse information integrity during multi-scale feature fusion.Experimental results demonstrate that PIDE-Net achieves AP 50 of 49.4%, 65.2%, and 52.6% on VisDrone2019, DOTA1.0, and AI-TODv2 datasets respectively, with AP S reaching 22.3%, 35.2%, and 35.6%, while maintaining only 15.4M parameters. Additionally, the framework achieves 59.4 FPS on edge devices. This methodology provides a novel technical paradigm for the collaborative design of high-precision, high-efficiency UAV detection systems. It offers a theoretical and practical foundation for the evolution from homogeneous to heterogeneous processing in computer vision.
Published: 2026-01-18T06:31:10+00:00
Venue: Expert Systems with Applications
Score: 0.793 (must_read)</description><content:encoded>&lt;p&gt;&lt;strong&gt;Authors:&lt;/strong&gt; Shuming Lin; Sang Fyeng; Jinyi Liang; Junnan Tan&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Journal:&lt;/strong&gt; Expert Systems with Applications&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;a href="https://doi.org/10.1016/j.eswa.2026.131194"&gt;10.1016/j.eswa.2026.131194&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Relevance:&lt;/strong&gt; 0.793 (must_read)&lt;/p&gt;
&lt;p&gt;Small object detection in unmanned aerial vehicle (UAV) imagery confronts multifaceted technical challenges encompassing severe geometric deformations, dense target clustering, and stringent computational resource constraints. Contemporary detection frameworks predominantly adopt homogeneous processing paradigms, which suffer from systematic information deterioration across feature representation, contextual modeling, and multi-scale fusion stages, constituting a fundamental performance bottlenecks in UAV scenarios. This paper introduces PIDE-Net (Progressive Information Disentanglement and Enhancement Network), establishing a heterogeneous processing paradigm that achieves synergistic optimization of detection accuracy and computational efficiency. The framework implements progressive information refinement through three core modules.The Position-aware Refined Interactive Semantic Module (PRISM) employs a position-semantic feature disentanglement mechanism to address information confusion in complex scenarios at the source of feature representation.The Semantic-Guided State Space Module (SG-SSM) introduces content-driven attention state space equations, enabling efficient global context modeling with O(n) linear complexity. Finally, the Progressive Enhancement Pyramid Network (PEP-Net) adopts spatial weaving upsampling mechanisms to preserve sparse information integrity during multi-scale feature fusion.Experimental results demonstrate that PIDE-Net achieves AP 50 of 49.4%, 65.2%, and 52.6% on VisDrone2019, DOTA1.0, and AI-TODv2 datasets respectively, with AP S reaching 22.3%, 35.2%, and 35.6%, while maintaining only 15.4M parameters. Additionally, the framework achieves 59.4 FPS on edge devices. This methodology provides a novel technical paradigm for the collaborative design of high-precision, high-efficiency UAV detection systems. It offers a theoretical and practical foundation for the evolution from homogeneous to heterogeneous processing in computer vision.&lt;/p&gt;</content:encoded></item><item><title>Seeing through the noise: A cross-modal guided framework for hyperspectral image classification under multi-type degradations</title><link>https://doi.org/10.1016/j.jag.2026.105117</link><guid>10.1016/j.jag.2026.105117</guid><pubDate>Sun, 18 Jan 2026 04:51:27 +0000</pubDate><dc:creator>Hui Liu</dc:creator><dc:creator>Wei Tong</dc:creator><dc:creator>Ning Chen</dc:creator><dc:creator>Tao Xie</dc:creator><dc:creator>Chenjia Huang</dc:creator><dc:creator>Xia Yue</dc:creator><dc:creator>Zhou Huang</dc:creator><prism:publicationName>International Journal of Applied Earth Observation and Geoinformation</prism:publicationName><prism:doi>10.1016/j.jag.2026.105117</prism:doi><description>Recent advances in deep learning and multimodal data fusion technologies have significantly enhanced hyperspectral image (HSI) classification performance. Nevertheless, classification accuracy of hyperspectral data continues to degrade substantially under diverse degradation scenarios, such as noise interference, spectral distortion, or reduced resolution. To robustly address this challenge, this paper proposes a novel cross-modal guided classification framework that integrates active remote sensing data (e.g., LiDAR) to improve classification resilience under degraded conditions. Specifically, we introduce a Cross-Modal Feature Pyramid Guidance (CMFPG) module, which effectively utilizes cross-modal information across multiple levels and scales to guide hyperspectral feature extraction and fusion, thereby enhancing modeling stability in degraded environments. Additionally, we develop the HyperGroupMix module, which enhances cross-domain adaptability through grouping spectral bands, extracting statistical features, and transferring features across samples. Experimental results conducted under complex degradation conditions demonstrate that our proposed method exhibits stable high-level classification accuracy and robustness in overall performance. The code is accessible at: https://github.com/miliwww/CMGF
Published: 2026-01-18T04:51:27+00:00
Venue: International Journal of Applied Earth Observation and Geoinformation
Score: 0.785 (must_read)</description><content:encoded>&lt;p&gt;&lt;strong&gt;Authors:&lt;/strong&gt; Hui Liu; Wei Tong; Ning Chen; Tao Xie; Chenjia Huang; Xia Yue; Zhou Huang&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Journal:&lt;/strong&gt; International Journal of Applied Earth Observation and Geoinformation&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;a href="https://doi.org/10.1016/j.jag.2026.105117"&gt;10.1016/j.jag.2026.105117&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Relevance:&lt;/strong&gt; 0.785 (must_read)&lt;/p&gt;
&lt;p&gt;Recent advances in deep learning and multimodal data fusion technologies have significantly enhanced hyperspectral image (HSI) classification performance. Nevertheless, classification accuracy of hyperspectral data continues to degrade substantially under diverse degradation scenarios, such as noise interference, spectral distortion, or reduced resolution. To robustly address this challenge, this paper proposes a novel cross-modal guided classification framework that integrates active remote sensing data (e.g., LiDAR) to improve classification resilience under degraded conditions. Specifically, we introduce a Cross-Modal Feature Pyramid Guidance (CMFPG) module, which effectively utilizes cross-modal information across multiple levels and scales to guide hyperspectral feature extraction and fusion, thereby enhancing modeling stability in degraded environments. Additionally, we develop the HyperGroupMix module, which enhances cross-domain adaptability through grouping spectral bands, extracting statistical features, and transferring features across samples. Experimental results conducted under complex degradation conditions demonstrate that our proposed method exhibits stable high-level classification accuracy and robustness in overall performance. The code is accessible at: https://github.com/miliwww/CMGF&lt;/p&gt;</content:encoded></item><item><title>Medical SAM3: A Foundation Model for Universal Prompt-Driven Medical Image Segmentation</title><link>https://arxiv.org/abs/2601.10880v1</link><guid>http://arxiv.org/abs/2601.10880v1</guid><pubDate>Thu, 15 Jan 2026 22:18:14 +0000</pubDate><dc:creator>Chongcong Jiang</dc:creator><dc:creator>Tianxingjian Ding</dc:creator><dc:creator>Chuhan Song</dc:creator><dc:creator>Jiachen Tu</dc:creator><dc:creator>Ziyang Yan</dc:creator><dc:creator>Yihua Shao</dc:creator><dc:creator>Zhenyi Wang</dc:creator><dc:creator>Yuzhang Shang</dc:creator><dc:creator>Tianyu Han</dc:creator><dc:creator>Yu Tian</dc:creator><dc:type>preprint</dc:type><prism:publicationName>arXiv</prism:publicationName><description>Promptable segmentation foundation models such as SAM3 have demonstrated strong generalization capabilities through interactive and concept-based prompting. However, their direct applicability to medical image segmentation remains limited by severe domain shifts, the absence of privileged spatial prompts, and the need to reason over complex anatomical and volumetric structures. Here we present Medical SAM3, a foundation model for universal prompt-driven medical image segmentation, obtained by fully fine-tuning SAM3 on large-scale, heterogeneous 2D and 3D medical imaging datasets with paired segmentation masks and text prompts. Through a systematic analysis of vanilla SAM3, we observe that its performance degrades substantially on medical data, with its apparent competitiveness largely relying on strong geometric priors such as ground-truth-derived bounding boxes. These findings motivate full model adaptation beyond prompt engineering alone. By fine-tuning SAM3's model parameters on 33 datasets spanning 10 medical imaging modalities, Medical SAM3 acquires robust domain-specific representations while preserving prompt-driven flexibility. Extensive experiments across organs, imaging modalities, and dimensionalities demonstrate consistent and significant performance gains, particularly in challenging scenarios characterized by semantic ambiguity, complex morphology, and long-range 3D context. Our results establish Medical SAM3 as a universal, text-guided segmentation foundation model for medical imaging and highlight the importance of holistic model adaptation for achieving robust prompt-driven segmentation under severe domain shift. Code and model will be made available at https://github.com/AIM-Research-Lab/Medical-SAM3.
Published: 2026-01-15T22:18:14+00:00
Venue: arXiv
Score: 0.784 (must_read)</description><content:encoded>&lt;p&gt;&lt;strong&gt;Authors:&lt;/strong&gt; Chongcong Jiang; Tianxingjian Ding; Chuhan Song; Jiachen Tu; Ziyang Yan; Yihua Shao; Zhenyi Wang; Yuzhang Shang; Tianyu Han; Yu Tian&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Journal:&lt;/strong&gt; arXiv&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Relevance:&lt;/strong&gt; 0.784 (must_read)&lt;/p&gt;
&lt;p&gt;Promptable segmentation foundation models such as SAM3 have demonstrated strong generalization capabilities through interactive and concept-based prompting. However, their direct applicability to medical image segmentation remains limited by severe domain shifts, the absence of privileged spatial prompts, and the need to reason over complex anatomical and volumetric structures. Here we present Medical SAM3, a foundation model for universal prompt-driven medical image segmentation, obtained by fully fine-tuning SAM3 on large-scale, heterogeneous 2D and 3D medical imaging datasets with paired segmentation masks and text prompts. Through a systematic analysis of vanilla SAM3, we observe that its performance degrades substantially on medical data, with its apparent competitiveness largely relying on strong geometric priors such as ground-truth-derived bounding boxes. These findings motivate full model adaptation beyond prompt engineering alone. By fine-tuning SAM3&amp;#x27;s model parameters on 33 datasets spanning 10 medical imaging modalities, Medical SAM3 acquires robust domain-specific representations while preserving prompt-driven flexibility. Extensive experiments across organs, imaging modalities, and dimensionalities demonstrate consistent and significant performance gains, particularly in challenging scenarios characterized by semantic ambiguity, complex morphology, and long-range 3D context. Our results establish Medical SAM3 as a universal, text-guided segmentation foundation model for medical imaging and highlight the importance of holistic model adaptation for achieving robust prompt-driven segmentation under severe domain shift. Code and model will be made available at https://github.com/AIM-Research-Lab/Medical-SAM3.&lt;/p&gt;</content:encoded></item><item><title>The Spatial Blindspot of Vision-Language Models</title><link>https://arxiv.org/abs/2601.09954v1</link><guid>http://arxiv.org/abs/2601.09954v1</guid><pubDate>Thu, 15 Jan 2026 00:30:34 +0000</pubDate><dc:creator>Nahid Alam</dc:creator><dc:creator>Leema Krishna Murali</dc:creator><dc:creator>Siddhant Bharadwaj</dc:creator><dc:creator>Patrick Liu</dc:creator><dc:creator>Timothy Chung</dc:creator><dc:creator>Drishti Sharma</dc:creator><dc:creator>Akshata A</dc:creator><dc:creator>Kranthi Kiran</dc:creator><dc:creator>Wesley Tam</dc:creator><dc:creator>Bala Krishna S Vegesna</dc:creator><dc:type>preprint</dc:type><prism:publicationName>arXiv</prism:publicationName><description>Vision-language models (VLMs) have advanced rapidly, but their ability to capture spatial relationships remains a blindspot. Current VLMs are typically built with contrastive language-image pretraining (CLIP) style image encoders. The training recipe often flattens images into 1D patch sequences, discarding the 2D structure necessary for spatial reasoning. We argue that this lack of spatial awareness is a missing dimension in VLM design and a bottleneck for applications requiring spatial grounding, such as robotics and embodied AI. To address this, we investigate (i) image encoders trained with alternative objectives and (ii) 2D positional encodings. Our experiments show that these architectural choices can lead to improved spatial reasoning on several benchmarks.
Published: 2026-01-15T00:30:34+00:00
Venue: arXiv
Score: 0.781 (must_read)</description><content:encoded>&lt;p&gt;&lt;strong&gt;Authors:&lt;/strong&gt; Nahid Alam; Leema Krishna Murali; Siddhant Bharadwaj; Patrick Liu; Timothy Chung; Drishti Sharma; Akshata A; Kranthi Kiran; Wesley Tam; Bala Krishna S Vegesna&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Journal:&lt;/strong&gt; arXiv&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Relevance:&lt;/strong&gt; 0.781 (must_read)&lt;/p&gt;
&lt;p&gt;Vision-language models (VLMs) have advanced rapidly, but their ability to capture spatial relationships remains a blindspot. Current VLMs are typically built with contrastive language-image pretraining (CLIP) style image encoders. The training recipe often flattens images into 1D patch sequences, discarding the 2D structure necessary for spatial reasoning. We argue that this lack of spatial awareness is a missing dimension in VLM design and a bottleneck for applications requiring spatial grounding, such as robotics and embodied AI. To address this, we investigate (i) image encoders trained with alternative objectives and (ii) 2D positional encodings. Our experiments show that these architectural choices can lead to improved spatial reasoning on several benchmarks.&lt;/p&gt;</content:encoded></item><item><title>LaViT: Aligning Latent Visual Thoughts for Multi-modal Reasoning</title><link>https://arxiv.org/abs/2601.10129v1</link><guid>http://arxiv.org/abs/2601.10129v1</guid><pubDate>Thu, 15 Jan 2026 07:14:24 +0000</pubDate><dc:creator>Linquan Wu</dc:creator><dc:creator>Tianxiang Jiang</dc:creator><dc:creator>Yifei Dong</dc:creator><dc:creator>Haoyu Yang</dc:creator><dc:creator>Fengji Zhang</dc:creator><dc:creator>Shichaang Meng</dc:creator><dc:creator>Ai Xuan</dc:creator><dc:creator>Linqi Song</dc:creator><dc:creator>Jacky Keung</dc:creator><dc:type>preprint</dc:type><prism:publicationName>arXiv</prism:publicationName><description>Current multimodal latent reasoning often relies on external supervision (e.g., auxiliary images), ignoring intrinsic visual attention dynamics. In this work, we identify a critical Perception Gap in distillation: student models frequently mimic a teacher's textual output while attending to fundamentally divergent visual regions, effectively relying on language priors rather than grounded perception. To bridge this, we propose LaViT, a framework that aligns latent visual thoughts rather than static embeddings. LaViT compels the student to autoregressively reconstruct the teacher's visual semantics and attention trajectories prior to text generation, employing a curriculum sensory gating mechanism to prevent shortcut learning. Extensive experiments show that LaViT significantly enhances visual grounding, achieving up to +16.9% gains on complex reasoning tasks and enabling a compact 3B model to outperform larger open-source variants and proprietary models like GPT-4o.
Published: 2026-01-15T07:14:24+00:00
Venue: arXiv
Score: 0.780 (must_read)</description><content:encoded>&lt;p&gt;&lt;strong&gt;Authors:&lt;/strong&gt; Linquan Wu; Tianxiang Jiang; Yifei Dong; Haoyu Yang; Fengji Zhang; Shichaang Meng; Ai Xuan; Linqi Song; Jacky Keung&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Journal:&lt;/strong&gt; arXiv&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Relevance:&lt;/strong&gt; 0.780 (must_read)&lt;/p&gt;
&lt;p&gt;Current multimodal latent reasoning often relies on external supervision (e.g., auxiliary images), ignoring intrinsic visual attention dynamics. In this work, we identify a critical Perception Gap in distillation: student models frequently mimic a teacher&amp;#x27;s textual output while attending to fundamentally divergent visual regions, effectively relying on language priors rather than grounded perception. To bridge this, we propose LaViT, a framework that aligns latent visual thoughts rather than static embeddings. LaViT compels the student to autoregressively reconstruct the teacher&amp;#x27;s visual semantics and attention trajectories prior to text generation, employing a curriculum sensory gating mechanism to prevent shortcut learning. Extensive experiments show that LaViT significantly enhances visual grounding, achieving up to +16.9% gains on complex reasoning tasks and enabling a compact 3B model to outperform larger open-source variants and proprietary models like GPT-4o.&lt;/p&gt;</content:encoded></item><item><title>From One-to-One to Many-to-Many: Dynamic Cross-Layer Injection for Deep Vision-Language Fusion</title><link>https://arxiv.org/abs/2601.10710v1</link><guid>http://arxiv.org/abs/2601.10710v1</guid><pubDate>Thu, 15 Jan 2026 18:59:10 +0000</pubDate><dc:creator>Cheng Chen</dc:creator><dc:creator>Yuyu Guo</dc:creator><dc:creator>Pengpeng Zeng</dc:creator><dc:creator>Jingkuan Song</dc:creator><dc:creator>Peng Di</dc:creator><dc:creator>Hang Yu</dc:creator><dc:creator>Lianli Gao</dc:creator><dc:type>preprint</dc:type><prism:publicationName>arXiv</prism:publicationName><description>Vision-Language Models (VLMs) create a severe visual feature bottleneck by using a crude, asymmetric connection that links only the output of the vision encoder to the input of the large language model (LLM). This static architecture fundamentally limits the ability of LLMs to achieve comprehensive alignment with hierarchical visual knowledge, compromising their capacity to accurately integrate local details with global semantics into coherent reasoning. To resolve this, we introduce Cross-Layer Injection (CLI), a novel and lightweight framework that forges a dynamic many-to-many bridge between the two modalities. CLI consists of two synergistic, parameter-efficient components: an Adaptive Multi-Projection (AMP) module that harmonizes features from diverse vision layers, and an Adaptive Gating Fusion (AGF) mechanism that empowers the LLM to selectively inject the most relevant visual information based on its real-time decoding context. We validate the effectiveness and versatility of CLI by integrating it into LLaVA-OneVision and LLaVA-1.5. Extensive experiments on 18 diverse benchmarks demonstrate significant performance improvements, establishing CLI as a scalable paradigm that unlocks deeper multimodal understanding by granting LLMs on-demand access to the full visual hierarchy.
Published: 2026-01-15T18:59:10+00:00
Venue: arXiv
Score: 0.777 (must_read)</description><content:encoded>&lt;p&gt;&lt;strong&gt;Authors:&lt;/strong&gt; Cheng Chen; Yuyu Guo; Pengpeng Zeng; Jingkuan Song; Peng Di; Hang Yu; Lianli Gao&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Journal:&lt;/strong&gt; arXiv&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Relevance:&lt;/strong&gt; 0.777 (must_read)&lt;/p&gt;
&lt;p&gt;Vision-Language Models (VLMs) create a severe visual feature bottleneck by using a crude, asymmetric connection that links only the output of the vision encoder to the input of the large language model (LLM). This static architecture fundamentally limits the ability of LLMs to achieve comprehensive alignment with hierarchical visual knowledge, compromising their capacity to accurately integrate local details with global semantics into coherent reasoning. To resolve this, we introduce Cross-Layer Injection (CLI), a novel and lightweight framework that forges a dynamic many-to-many bridge between the two modalities. CLI consists of two synergistic, parameter-efficient components: an Adaptive Multi-Projection (AMP) module that harmonizes features from diverse vision layers, and an Adaptive Gating Fusion (AGF) mechanism that empowers the LLM to selectively inject the most relevant visual information based on its real-time decoding context. We validate the effectiveness and versatility of CLI by integrating it into LLaVA-OneVision and LLaVA-1.5. Extensive experiments on 18 diverse benchmarks demonstrate significant performance improvements, establishing CLI as a scalable paradigm that unlocks deeper multimodal understanding by granting LLMs on-demand access to the full visual hierarchy.&lt;/p&gt;</content:encoded></item><item><title>Large Foundation Model Empowered Region-aware Underwater Image Captioning</title><link>https://doi.org/10.1007/s11263-025-02650-w</link><guid>10.1007/s11263-025-02650-w</guid><pubDate>Sat, 17 Jan 2026 05:47:18 +0000</pubDate><dc:creator>Huanyu Li</dc:creator><dc:creator>Li Li</dc:creator><dc:creator>Hao Wang</dc:creator><dc:creator>Weibo Zhang</dc:creator><dc:creator>Peng Ren</dc:creator><prism:publicationName>International Journal of Computer Vision</prism:publicationName><prism:doi>10.1007/s11263-025-02650-w</prism:doi><description>Underwater image captioning facilitates the transformation from visual perception to semantic understanding in underwater computer vision. Despite advancements in this field, challenges remain in generating high-quality captions for underwater images. These challenges typically stem from (a) ambiguity between object and background regions for feature extraction, and (b) insufficient feature fusion across all regions. To address these challenges, we develop a large foundation model empowered region-aware underwater image captioning framework. Our novel contributions are two-fold: (a) A region-discriminative feature extraction strategy powered by the large foundation segment anything model (SAM) is developed. This strategy accurately delineates object and background regions through segmentation maps, enabling precise extraction of region-discriminative features. (b) A region-guided feature fusion strategy comprehensively fusing regional information throughout an encoding-decoding process is presented. This strategy utilizes a region-guided encoder for the progressive layer-wise fusion of region-discriminative features and grid features, followed by a meshed memory decoder that fuses multi-level encoded features, thereby enhancing the decoded features. Together, these contributions result in the generation of accurate and comprehensive underwater image captions. Experimental evaluations on three datasets demonstrate that our proposed framework achieves state-of-the-art performance for underwater image captioning.
Published: 2026-01-17T05:47:18+00:00
Venue: International Journal of Computer Vision
Score: 0.776 (must_read)</description><content:encoded>&lt;p&gt;&lt;strong&gt;Authors:&lt;/strong&gt; Huanyu Li; Li Li; Hao Wang; Weibo Zhang; Peng Ren&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Journal:&lt;/strong&gt; International Journal of Computer Vision&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;a href="https://doi.org/10.1007/s11263-025-02650-w"&gt;10.1007/s11263-025-02650-w&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Relevance:&lt;/strong&gt; 0.776 (must_read)&lt;/p&gt;
&lt;p&gt;Underwater image captioning facilitates the transformation from visual perception to semantic understanding in underwater computer vision. Despite advancements in this field, challenges remain in generating high-quality captions for underwater images. These challenges typically stem from (a) ambiguity between object and background regions for feature extraction, and (b) insufficient feature fusion across all regions. To address these challenges, we develop a large foundation model empowered region-aware underwater image captioning framework. Our novel contributions are two-fold: (a) A region-discriminative feature extraction strategy powered by the large foundation segment anything model (SAM) is developed. This strategy accurately delineates object and background regions through segmentation maps, enabling precise extraction of region-discriminative features. (b) A region-guided feature fusion strategy comprehensively fusing regional information throughout an encoding-decoding process is presented. This strategy utilizes a region-guided encoder for the progressive layer-wise fusion of region-discriminative features and grid features, followed by a meshed memory decoder that fuses multi-level encoded features, thereby enhancing the decoded features. Together, these contributions result in the generation of accurate and comprehensive underwater image captions. Experimental evaluations on three datasets demonstrate that our proposed framework achieves state-of-the-art performance for underwater image captioning.&lt;/p&gt;</content:encoded></item><item><title>DSA-Diff: Dynamic Schedule Alignment for Training-Inference Consistent Modality Translation in x-prediction Diffusion Model</title><link>https://doi.org/10.1016/j.neunet.2026.108611</link><guid>10.1016/j.neunet.2026.108611</guid><pubDate>Sat, 17 Jan 2026 00:33:09 +0000</pubDate><dc:creator>Xianhua Zeng</dc:creator><dc:creator>Yixin Xiang</dc:creator><dc:creator>Jian Zhang</dc:creator><dc:creator>Bowen Lu</dc:creator><prism:publicationName>Neural Networks</prism:publicationName><prism:doi>10.1016/j.neunet.2026.108611</prism:doi><description>For modality translation tasks, diffusion models based on x-prediction offer faster and more accurate image generation compared to traditional ϵ-prediction. However, they often suffer from training-inference inconsistency (TII), which arises from a mismatch between the Gaussian distribution assumed by the preset noise schedule and the true data distribution. To address this, we propose DSA-Diff, a novel framework that employs dual noise schedules to decouple the training and inference processes. Our approach decomposes the noise schedule along three dimensions: noise sequence, timestep, and correction matrix, and introduces a Bayesian-Greedy Alignment Scheduler (BGAS) to dynamically reconstruct the inference schedule. BGAS combines greedy initialization and Bayesian optimization to align the generated data manifold with the true one. Additionally, we introduce progressive target prediction and multi-scale perceptual alignment to enhance the robustness and detail fidelity of the x-prediction model. Experiments on four datasets show that DSA-Diff achieves high-fidelity image synthesis in only 4-10 adaptive inference steps, with minimal computational cost (68 GFLOPS). It improves the SSIM metric by up to 2.56% in TFW dataset using only one additional algorithmic module, effectively mitigating TII. Code and models are available at: https://github.com/ElephantOH/DSA-Diff .
Published: 2026-01-17T00:33:09+00:00
Venue: Neural Networks
Score: 0.775 (must_read)</description><content:encoded>&lt;p&gt;&lt;strong&gt;Authors:&lt;/strong&gt; Xianhua Zeng; Yixin Xiang; Jian Zhang; Bowen Lu&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Journal:&lt;/strong&gt; Neural Networks&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;a href="https://doi.org/10.1016/j.neunet.2026.108611"&gt;10.1016/j.neunet.2026.108611&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Relevance:&lt;/strong&gt; 0.775 (must_read)&lt;/p&gt;
&lt;p&gt;For modality translation tasks, diffusion models based on x-prediction offer faster and more accurate image generation compared to traditional ϵ-prediction. However, they often suffer from training-inference inconsistency (TII), which arises from a mismatch between the Gaussian distribution assumed by the preset noise schedule and the true data distribution. To address this, we propose DSA-Diff, a novel framework that employs dual noise schedules to decouple the training and inference processes. Our approach decomposes the noise schedule along three dimensions: noise sequence, timestep, and correction matrix, and introduces a Bayesian-Greedy Alignment Scheduler (BGAS) to dynamically reconstruct the inference schedule. BGAS combines greedy initialization and Bayesian optimization to align the generated data manifold with the true one. Additionally, we introduce progressive target prediction and multi-scale perceptual alignment to enhance the robustness and detail fidelity of the x-prediction model. Experiments on four datasets show that DSA-Diff achieves high-fidelity image synthesis in only 4-10 adaptive inference steps, with minimal computational cost (68 GFLOPS). It improves the SSIM metric by up to 2.56% in TFW dataset using only one additional algorithmic module, effectively mitigating TII. Code and models are available at: https://github.com/ElephantOH/DSA-Diff .&lt;/p&gt;</content:encoded></item><item><title>Dual-Layer Prompt Ensembles: Leveraging System- and User-Level Instructions for Robust LLM-Based Query Expansion and Rank Fusion</title><link>https://doi.org/10.1016/j.inffus.2026.104160</link><guid>10.1016/j.inffus.2026.104160</guid><pubDate>Sun, 18 Jan 2026 04:47:19 +0000</pubDate><dc:creator>Minghan Li</dc:creator><dc:creator>Ercong Nie</dc:creator><dc:creator>Huiping Huang</dc:creator><dc:creator>Xinxuan Lv</dc:creator><dc:creator>Guodong Zhou</dc:creator><prism:publicationName>Information Fusion</prism:publicationName><prism:doi>10.1016/j.inffus.2026.104160</prism:doi><description>Large Language Models (LLMs) show strong potential for query expansion (QE), but their effectiveness is highly sensitive to prompt design. This paper investigates whether exploiting the system–user prompt distinction in chat-based LLMs improves QE, and how multiple expansions should be combined. We propose Dual-Layer Prompt Ensembles, which pair a behavioural system prompt with varied user prompts to generate diverse expansions, and aggregate their BM25-ranked lists using lightweight SU-RankFusion schemes. Experiments on six heterogeneous datasets show that dual-layer prompting consistently outperforms strong single-prompt baselines. For example, on Touche-2020 a dual-layer configuration improves nDCG@10 from 0.4177 (QE-CoT) to 0.4696, and SU-RankFusion further raises it to 0.4797. On Robust04 and DBPedia, SU-RankFusion improves nDCG@10 over BM25 by 24.7% and 25.5%, respectively, with similar gains on NFCorpus, FiQA, and TREC-COVID. These results demonstrate that system–user prompt ensembles are effective for QE, and that simple fusion transforms prompt-level diversity into stable retrieval improvements.
Published: 2026-01-18T04:47:19+00:00
Venue: Information Fusion
Score: 0.775 (must_read)</description><content:encoded>&lt;p&gt;&lt;strong&gt;Authors:&lt;/strong&gt; Minghan Li; Ercong Nie; Huiping Huang; Xinxuan Lv; Guodong Zhou&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Journal:&lt;/strong&gt; Information Fusion&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;a href="https://doi.org/10.1016/j.inffus.2026.104160"&gt;10.1016/j.inffus.2026.104160&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Relevance:&lt;/strong&gt; 0.775 (must_read)&lt;/p&gt;
&lt;p&gt;Large Language Models (LLMs) show strong potential for query expansion (QE), but their effectiveness is highly sensitive to prompt design. This paper investigates whether exploiting the system–user prompt distinction in chat-based LLMs improves QE, and how multiple expansions should be combined. We propose Dual-Layer Prompt Ensembles, which pair a behavioural system prompt with varied user prompts to generate diverse expansions, and aggregate their BM25-ranked lists using lightweight SU-RankFusion schemes. Experiments on six heterogeneous datasets show that dual-layer prompting consistently outperforms strong single-prompt baselines. For example, on Touche-2020 a dual-layer configuration improves nDCG@10 from 0.4177 (QE-CoT) to 0.4696, and SU-RankFusion further raises it to 0.4797. On Robust04 and DBPedia, SU-RankFusion improves nDCG@10 over BM25 by 24.7% and 25.5%, respectively, with similar gains on NFCorpus, FiQA, and TREC-COVID. These results demonstrate that system–user prompt ensembles are effective for QE, and that simple fusion transforms prompt-level diversity into stable retrieval improvements.&lt;/p&gt;</content:encoded></item><item><title>SME-YOLO: A Real-Time Detector for Tiny Defect Detection on PCB Surfaces</title><link>https://arxiv.org/abs/2601.11402v1</link><guid>http://arxiv.org/abs/2601.11402v1</guid><pubDate>Fri, 16 Jan 2026 16:14:56 +0000</pubDate><dc:creator>Meng Han</dc:creator><dc:type>preprint</dc:type><prism:publicationName>arXiv</prism:publicationName><description>Surface defects on Printed Circuit Boards (PCBs) directly compromise product reliability and safety. However, achieving high-precision detection is challenging because PCB defects are typically characterized by tiny sizes, high texture similarity, and uneven scale distributions. To address these challenges, this paper proposes a novel framework based on YOLOv11n, named SME-YOLO (Small-target Multi-scale Enhanced YOLO). First, we employ the Normalized Wasserstein Distance Loss (NWDLoss). This metric effectively mitigates the sensitivity of Intersection over Union (IoU) to positional deviations in tiny objects. Second, the original upsampling module is replaced by the Efficient Upsampling Convolution Block (EUCB). By utilizing multi-scale convolutions, the EUCB gradually recovers spatial resolution and enhances the preservation of edge and texture details for tiny defects. Finally, this paper proposes the Multi-Scale Focused Attention (MSFA) module. Tailored to the specific spatial distribution of PCB defects, this module adaptively strengthens perception within key scale intervals, achieving efficient fusion of local fine-grained features and global context information. Experimental results on the PKU-PCB dataset demonstrate that SME-YOLO achieves state-of-the-art performance. Specifically, compared to the baseline YOLOv11n, SME-YOLO improves mAP by 2.2% and Precision by 4%, validating the effectiveness of the proposed method.
Published: 2026-01-16T16:14:56+00:00
Venue: arXiv
Score: 0.772 (must_read)</description><content:encoded>&lt;p&gt;&lt;strong&gt;Authors:&lt;/strong&gt; Meng Han&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Journal:&lt;/strong&gt; arXiv&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Relevance:&lt;/strong&gt; 0.772 (must_read)&lt;/p&gt;
&lt;p&gt;Surface defects on Printed Circuit Boards (PCBs) directly compromise product reliability and safety. However, achieving high-precision detection is challenging because PCB defects are typically characterized by tiny sizes, high texture similarity, and uneven scale distributions. To address these challenges, this paper proposes a novel framework based on YOLOv11n, named SME-YOLO (Small-target Multi-scale Enhanced YOLO). First, we employ the Normalized Wasserstein Distance Loss (NWDLoss). This metric effectively mitigates the sensitivity of Intersection over Union (IoU) to positional deviations in tiny objects. Second, the original upsampling module is replaced by the Efficient Upsampling Convolution Block (EUCB). By utilizing multi-scale convolutions, the EUCB gradually recovers spatial resolution and enhances the preservation of edge and texture details for tiny defects. Finally, this paper proposes the Multi-Scale Focused Attention (MSFA) module. Tailored to the specific spatial distribution of PCB defects, this module adaptively strengthens perception within key scale intervals, achieving efficient fusion of local fine-grained features and global context information. Experimental results on the PKU-PCB dataset demonstrate that SME-YOLO achieves state-of-the-art performance. Specifically, compared to the baseline YOLOv11n, SME-YOLO improves mAP by 2.2% and Precision by 4%, validating the effectiveness of the proposed method.&lt;/p&gt;</content:encoded></item><item><title>AmPLe: Supporting Vision-Language Models via Adaptive-Debiased Ensemble Multi-Prompt Learning</title><link>https://doi.org/10.1007/s11263-025-02591-4</link><guid>10.1007/s11263-025-02591-4</guid><pubDate>Sat, 17 Jan 2026 05:58:16 +0000</pubDate><dc:creator>Fei Song</dc:creator><dc:creator>Yi Li</dc:creator><dc:creator>Jiangmeng Li</dc:creator><dc:creator>Rui Wang</dc:creator><dc:creator>Changwen Zheng</dc:creator><dc:creator>Fanjiang Xu</dc:creator><dc:creator>Hui Xiong</dc:creator><prism:publicationName>International Journal of Computer Vision</prism:publicationName><prism:doi>10.1007/s11263-025-02591-4</prism:doi><description>Multi-prompt learning methods have emerged as an effective approach for facilitating the rapid adaptation of vision-language models to downstream tasks with limited resources. Existing multi-prompt learning methods primarily focus on utilizing various meticulously designed prompts within a single foundation vision-language model to achieve superior performance. However, the overlooked model-prompt matching bias hinders the development of multi-prompt learning, i.e., the same prompt can convey different semantics across distinct vision-language models, such as CLIP-ViT-B/16 and CLIP-ViT-B/32, resulting in inconsistent predictions of identical prompt. To mitigate the impact of this bias on downstream tasks, we explore an ensemble learning approach to sufficiently aggregate the benefits of diverse predictions. Additionally, we further disclose the presence of sample-prompt matching bias, which originates from the prompt-irrelevant semantics encapsulated in the input samples. Thus, directly utilizing all information from the input samples for generating weights of ensemble learning can lead to suboptimal performance. In response, we extract prompt-relevant semantics from input samples by leveraging the guidance of the information theory-based analysis, adaptively calculating debiased ensemble weights. Overall, we propose Adaptive-Debiased Ensemble Multi-Prompt Learning, abbreviated as AmPLe, to mitigate the two types of bias simultaneously. Extensive experiments on three representative tasks, i.e., generalization to novel classes, new target datasets, and unseen domain shifts, show that AmPLe can widely outperform existing methods. Theoretical validation from a causal perspective further supports the effectiveness of AmPLe. The source code can be accessed at https://github.com/FF2127/AmPLe .
Published: 2026-01-17T05:58:16+00:00
Venue: International Journal of Computer Vision
Score: 0.770 (must_read)</description><content:encoded>&lt;p&gt;&lt;strong&gt;Authors:&lt;/strong&gt; Fei Song; Yi Li; Jiangmeng Li; Rui Wang; Changwen Zheng; Fanjiang Xu; Hui Xiong&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Journal:&lt;/strong&gt; International Journal of Computer Vision&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;a href="https://doi.org/10.1007/s11263-025-02591-4"&gt;10.1007/s11263-025-02591-4&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Relevance:&lt;/strong&gt; 0.770 (must_read)&lt;/p&gt;
&lt;p&gt;Multi-prompt learning methods have emerged as an effective approach for facilitating the rapid adaptation of vision-language models to downstream tasks with limited resources. Existing multi-prompt learning methods primarily focus on utilizing various meticulously designed prompts within a single foundation vision-language model to achieve superior performance. However, the overlooked model-prompt matching bias hinders the development of multi-prompt learning, i.e., the same prompt can convey different semantics across distinct vision-language models, such as CLIP-ViT-B/16 and CLIP-ViT-B/32, resulting in inconsistent predictions of identical prompt. To mitigate the impact of this bias on downstream tasks, we explore an ensemble learning approach to sufficiently aggregate the benefits of diverse predictions. Additionally, we further disclose the presence of sample-prompt matching bias, which originates from the prompt-irrelevant semantics encapsulated in the input samples. Thus, directly utilizing all information from the input samples for generating weights of ensemble learning can lead to suboptimal performance. In response, we extract prompt-relevant semantics from input samples by leveraging the guidance of the information theory-based analysis, adaptively calculating debiased ensemble weights. Overall, we propose Adaptive-Debiased Ensemble Multi-Prompt Learning, abbreviated as AmPLe, to mitigate the two types of bias simultaneously. Extensive experiments on three representative tasks, i.e., generalization to novel classes, new target datasets, and unseen domain shifts, show that AmPLe can widely outperform existing methods. Theoretical validation from a causal perspective further supports the effectiveness of AmPLe. The source code can be accessed at https://github.com/FF2127/AmPLe .&lt;/p&gt;</content:encoded></item><item><title>A Novel Knowledge Distillation and Hybrid Explainability Approach for Phenology Stage Classification from Multi-Source Time Series</title><link>https://doi.org/10.1016/j.inffus.2026.104158</link><guid>10.1016/j.inffus.2026.104158</guid><pubDate>Sat, 17 Jan 2026 00:42:36 +0000</pubDate><dc:creator>Naeem Ullah</dc:creator><dc:creator>Andrés Manuel Chacón-Maldonado</dc:creator><dc:creator>Francisco Martínez-Álvarez</dc:creator><dc:creator>Ivanoe De Falco</dc:creator><dc:creator>Giovanna Sannino</dc:creator><prism:publicationName>Information Fusion</prism:publicationName><prism:doi>10.1016/j.inffus.2026.104158</prism:doi><description>Accurate phenological stage classification is crucial for addressing global challenges to food security posed by climate change, water scarcity, and land degradation. It enables precision agriculture by optimizing key interventions such as irrigation, fertilization, and pest control. While deep learning offers powerful tools, existing methods face four key limitations: reliance on narrow features and models, limited long-term forecasting capability, computational inefficiency, and opaque, unvalidated explanations. To overcome these limitations, this paper presents a deep learning framework for phenology classification, utilizing multi-source time series data from satellite imagery, meteorological stations, and field observations. The approach emphasizes temporal consistency, spatial adaptability, computational efficiency, and explainability. A feature engineering pipeline extracts temporal dynamics via lag features, rolling statistics, Fourier transforms and seasonal encodings. Feature selection combines incremental strategies with classical filter, wrapper, and embedded methods. Deep learning models across multiple paradigms—feedforward, recurrent, convolutional, and attention-based—are benchmarked under multi-horizon forecasting tasks. To reduce model complexity while preserving performance where possible, the framework employs knowledge distillation, transferring predictive knowledge from complex teacher models to compact and deployable student models. For model interpretability, a new Hybrid SHAP-Association Rule Explainability approach is proposed, integrating model-driven and data-driven explanations. Agreement between views is quantified using trust metrics: precision@k, coverage, and Jaccard similarity, with a retraining-based validation mechanism. Experiments on phenology data from Andalusia demonstrate high accuracy, strong generalizability, trustworthy explanations and resource-efficient phenology monitoring in agricultural systems.
Published: 2026-01-17T00:42:36+00:00
Venue: Information Fusion
Score: 0.770 (must_read)</description><content:encoded>&lt;p&gt;&lt;strong&gt;Authors:&lt;/strong&gt; Naeem Ullah; Andrés Manuel Chacón-Maldonado; Francisco Martínez-Álvarez; Ivanoe De Falco; Giovanna Sannino&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Journal:&lt;/strong&gt; Information Fusion&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;a href="https://doi.org/10.1016/j.inffus.2026.104158"&gt;10.1016/j.inffus.2026.104158&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Relevance:&lt;/strong&gt; 0.770 (must_read)&lt;/p&gt;
&lt;p&gt;Accurate phenological stage classification is crucial for addressing global challenges to food security posed by climate change, water scarcity, and land degradation. It enables precision agriculture by optimizing key interventions such as irrigation, fertilization, and pest control. While deep learning offers powerful tools, existing methods face four key limitations: reliance on narrow features and models, limited long-term forecasting capability, computational inefficiency, and opaque, unvalidated explanations. To overcome these limitations, this paper presents a deep learning framework for phenology classification, utilizing multi-source time series data from satellite imagery, meteorological stations, and field observations. The approach emphasizes temporal consistency, spatial adaptability, computational efficiency, and explainability. A feature engineering pipeline extracts temporal dynamics via lag features, rolling statistics, Fourier transforms and seasonal encodings. Feature selection combines incremental strategies with classical filter, wrapper, and embedded methods. Deep learning models across multiple paradigms—feedforward, recurrent, convolutional, and attention-based—are benchmarked under multi-horizon forecasting tasks. To reduce model complexity while preserving performance where possible, the framework employs knowledge distillation, transferring predictive knowledge from complex teacher models to compact and deployable student models. For model interpretability, a new Hybrid SHAP-Association Rule Explainability approach is proposed, integrating model-driven and data-driven explanations. Agreement between views is quantified using trust metrics: precision@k, coverage, and Jaccard similarity, with a retraining-based validation mechanism. Experiments on phenology data from Andalusia demonstrate high accuracy, strong generalizability, trustworthy explanations and resource-efficient phenology monitoring in agricultural systems.&lt;/p&gt;</content:encoded></item><item><title>Class semantics guided knowledge distillation for few-shot class incremental learning</title><link>https://doi.org/10.1016/j.ins.2026.123126</link><guid>10.1016/j.ins.2026.123126</guid><pubDate>Sat, 17 Jan 2026 00:24:52 +0000</pubDate><dc:creator>Ping Li</dc:creator><dc:creator>Jiajun Chen</dc:creator><dc:creator>Shaoqi Tian</dc:creator><dc:creator>Ran Wang</dc:creator><prism:publicationName>Information Sciences</prism:publicationName><prism:doi>10.1016/j.ins.2026.123126</prism:doi><description>Few-shot class-incremental learning requires a model to incrementally learn to recognize novel classes from limited samples while preserving its ability to classify previously learned base and old classes. It presents two main challenges, i.e., catastrophic forgetting on old classes due to the absence of their samples during incremental phases, and overfitting on the few available samples of novel classes. To address these issues, we propose a Class Semantics guided Knowledge Distillation ( CSKD ) method. In the base session, CSKD leverages the pre-trained vision-language model CLIP (Contrastive Language-Image Pre-Training) to perform knowledge distillation for enhancing the base model. During each incremental session, the method utilizes the CLIP-derived class textual semantics to guide the optimization of classifier, thereby alleviating over-fitting on novel classes and forgetting of prior knowledge. Extensive experiments on three image datasets, i.e., mini-ImageNet, CUB200, and CIFAR100, as well as two video datasets, i.e., UCF101 and HMDB51, demonstrate CSKD outperforms SOTA competitive alternatives, showing particularly strong generalization ability on novel classes. Code is available at https://github.com/mlvccn/CSKD_Fewshot .
Published: 2026-01-17T00:24:52+00:00
Venue: Information Sciences
Score: 0.770 (must_read)</description><content:encoded>&lt;p&gt;&lt;strong&gt;Authors:&lt;/strong&gt; Ping Li; Jiajun Chen; Shaoqi Tian; Ran Wang&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Journal:&lt;/strong&gt; Information Sciences&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;a href="https://doi.org/10.1016/j.ins.2026.123126"&gt;10.1016/j.ins.2026.123126&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Relevance:&lt;/strong&gt; 0.770 (must_read)&lt;/p&gt;
&lt;p&gt;Few-shot class-incremental learning requires a model to incrementally learn to recognize novel classes from limited samples while preserving its ability to classify previously learned base and old classes. It presents two main challenges, i.e., catastrophic forgetting on old classes due to the absence of their samples during incremental phases, and overfitting on the few available samples of novel classes. To address these issues, we propose a Class Semantics guided Knowledge Distillation ( CSKD ) method. In the base session, CSKD leverages the pre-trained vision-language model CLIP (Contrastive Language-Image Pre-Training) to perform knowledge distillation for enhancing the base model. During each incremental session, the method utilizes the CLIP-derived class textual semantics to guide the optimization of classifier, thereby alleviating over-fitting on novel classes and forgetting of prior knowledge. Extensive experiments on three image datasets, i.e., mini-ImageNet, CUB200, and CIFAR100, as well as two video datasets, i.e., UCF101 and HMDB51, demonstrate CSKD outperforms SOTA competitive alternatives, showing particularly strong generalization ability on novel classes. Code is available at https://github.com/mlvccn/CSKD_Fewshot .&lt;/p&gt;</content:encoded></item><item><title>One Model, Many Behaviors: Training-Induced Effects on Out-of-Distribution Detection</title><link>https://arxiv.org/abs/2601.10836v1</link><guid>http://arxiv.org/abs/2601.10836v1</guid><pubDate>Thu, 15 Jan 2026 20:11:01 +0000</pubDate><dc:creator>Gerhard Krumpl</dc:creator><dc:creator>Henning Avenhaus</dc:creator><dc:creator>Horst Possegger</dc:creator><dc:type>preprint</dc:type><prism:publicationName>arXiv</prism:publicationName><description>Out-of-distribution (OOD) detection is crucial for deploying robust and reliable machine-learning systems in open-world settings. Despite steady advances in OOD detectors, their interplay with modern training pipelines that maximize in-distribution (ID) accuracy and generalization remains under-explored. We investigate this link through a comprehensive empirical study. Fixing the architecture to the widely adopted ResNet-50, we benchmark 21 post-hoc, state-of-the-art OOD detection methods across 56 ImageNet-trained models obtained via diverse training strategies and evaluate them on eight OOD test sets. Contrary to the common assumption that higher ID accuracy implies better OOD detection performance, we uncover a non-monotonic relationship: OOD performance initially improves with accuracy but declines once advanced training recipes push accuracy beyond the baseline. Moreover, we observe a strong interdependence between training strategy, detector choice, and resulting OOD performance, indicating that no single method is universally optimal.
Published: 2026-01-15T20:11:01+00:00
Venue: arXiv
Score: 0.769 (must_read)</description><content:encoded>&lt;p&gt;&lt;strong&gt;Authors:&lt;/strong&gt; Gerhard Krumpl; Henning Avenhaus; Horst Possegger&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Journal:&lt;/strong&gt; arXiv&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Relevance:&lt;/strong&gt; 0.769 (must_read)&lt;/p&gt;
&lt;p&gt;Out-of-distribution (OOD) detection is crucial for deploying robust and reliable machine-learning systems in open-world settings. Despite steady advances in OOD detectors, their interplay with modern training pipelines that maximize in-distribution (ID) accuracy and generalization remains under-explored. We investigate this link through a comprehensive empirical study. Fixing the architecture to the widely adopted ResNet-50, we benchmark 21 post-hoc, state-of-the-art OOD detection methods across 56 ImageNet-trained models obtained via diverse training strategies and evaluate them on eight OOD test sets. Contrary to the common assumption that higher ID accuracy implies better OOD detection performance, we uncover a non-monotonic relationship: OOD performance initially improves with accuracy but declines once advanced training recipes push accuracy beyond the baseline. Moreover, we observe a strong interdependence between training strategy, detector choice, and resulting OOD performance, indicating that no single method is universally optimal.&lt;/p&gt;</content:encoded></item><item><title>DanQing: An Up-to-Date Large-Scale Chinese Vision-Language Pre-training Dataset</title><link>https://arxiv.org/abs/2601.10305v1</link><guid>http://arxiv.org/abs/2601.10305v1</guid><pubDate>Thu, 15 Jan 2026 11:28:58 +0000</pubDate><dc:creator>Hengyu Shen</dc:creator><dc:creator>Tiancheng Gu</dc:creator><dc:creator>Bin Qin</dc:creator><dc:creator>Lan Wu</dc:creator><dc:creator>Yuling Wu</dc:creator><dc:creator>Shuo Tan</dc:creator><dc:creator>Zelong Sun</dc:creator><dc:creator>Jun Wang</dc:creator><dc:creator>Nan Wu</dc:creator><dc:creator>Xiang An</dc:creator><dc:creator>Weidong Cai</dc:creator><dc:creator>Ziyong Feng</dc:creator><dc:creator>Kaicheng Yang</dc:creator><dc:type>preprint</dc:type><prism:publicationName>arXiv</prism:publicationName><description>Vision-Language Pre-training (VLP) models demonstrate strong performance across various downstream tasks by learning from large-scale image-text pairs through contrastive pretraining. The release of extensive English image-text datasets (e.g., COYO-700M and LAION-400M) has enabled widespread adoption of models such as CLIP and SigLIP in tasks including cross-modal retrieval and image captioning. However, the advancement of Chinese vision-language pretraining has substantially lagged behind, due to the scarcity of high-quality Chinese image-text data. To address this gap, we develop a comprehensive pipeline for constructing a high-quality Chinese cross-modal dataset. As a result, we propose DanQing, which contains 100 million image-text pairs collected from Common Crawl. Different from existing datasets, DanQing is curated through a more rigorous selection process, yielding superior data quality. Moreover, DanQing is primarily built from 2024-2025 web data, enabling models to better capture evolving semantic trends and thus offering greater practical utility. We compare DanQing with existing datasets by continual pre-training of the SigLIP2 model. Experimental results show that DanQing consistently achieves superior performance across a range of Chinese downstream tasks, including zero-shot classification, cross-modal retrieval, and LMM-based evaluations. To facilitate further research in Chinese vision-language pre-training, we will open-source the DanQing dataset under the Creative Common CC-BY 4.0 license.
Published: 2026-01-15T11:28:58+00:00
Venue: arXiv
Score: 0.768 (must_read)</description><content:encoded>&lt;p&gt;&lt;strong&gt;Authors:&lt;/strong&gt; Hengyu Shen; Tiancheng Gu; Bin Qin; Lan Wu; Yuling Wu; Shuo Tan; Zelong Sun; Jun Wang; Nan Wu; Xiang An; Weidong Cai; Ziyong Feng; Kaicheng Yang&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Journal:&lt;/strong&gt; arXiv&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Relevance:&lt;/strong&gt; 0.768 (must_read)&lt;/p&gt;
&lt;p&gt;Vision-Language Pre-training (VLP) models demonstrate strong performance across various downstream tasks by learning from large-scale image-text pairs through contrastive pretraining. The release of extensive English image-text datasets (e.g., COYO-700M and LAION-400M) has enabled widespread adoption of models such as CLIP and SigLIP in tasks including cross-modal retrieval and image captioning. However, the advancement of Chinese vision-language pretraining has substantially lagged behind, due to the scarcity of high-quality Chinese image-text data. To address this gap, we develop a comprehensive pipeline for constructing a high-quality Chinese cross-modal dataset. As a result, we propose DanQing, which contains 100 million image-text pairs collected from Common Crawl. Different from existing datasets, DanQing is curated through a more rigorous selection process, yielding superior data quality. Moreover, DanQing is primarily built from 2024-2025 web data, enabling models to better capture evolving semantic trends and thus offering greater practical utility. We compare DanQing with existing datasets by continual pre-training of the SigLIP2 model. Experimental results show that DanQing consistently achieves superior performance across a range of Chinese downstream tasks, including zero-shot classification, cross-modal retrieval, and LMM-based evaluations. To facilitate further research in Chinese vision-language pre-training, we will open-source the DanQing dataset under the Creative Common CC-BY 4.0 license.&lt;/p&gt;</content:encoded></item><item><title>MHA2MLA-VLM: Enabling DeepSeek's Economical Multi-Head Latent Attention across Vision-Language Models</title><link>https://arxiv.org/abs/2601.11464v1</link><guid>http://arxiv.org/abs/2601.11464v1</guid><pubDate>Fri, 16 Jan 2026 17:45:34 +0000</pubDate><dc:creator>Xiaoran Fan</dc:creator><dc:creator>Zhichao Sun</dc:creator><dc:creator>Tao Ji</dc:creator><dc:creator>Lixing Shen</dc:creator><dc:creator>Tao Gui</dc:creator><dc:type>preprint</dc:type><prism:publicationName>arXiv</prism:publicationName><description>As vision-language models (VLMs) tackle increasingly complex and multimodal tasks, the rapid growth of Key-Value (KV) cache imposes significant memory and computational bottlenecks during inference. While Multi-Head Latent Attention (MLA) offers an effective means to compress the KV cache and accelerate inference, adapting existing VLMs to the MLA architecture without costly pretraining remains largely unexplored. In this work, we present MHA2MLA-VLM, a parameter-efficient and multimodal-aware framework for converting off-the-shelf VLMs to MLA. Our approach features two core techniques: (1) a modality-adaptive partial-RoPE strategy that supports both traditional and multimodal settings by selectively masking nonessential dimensions, and (2) a modality-decoupled low-rank approximation method that independently compresses the visual and textual KV spaces. Furthermore, we introduce parameter-efficient fine-tuning to minimize adaptation cost and demonstrate that minimizing output activation error, rather than parameter distance, substantially reduces performance loss. Extensive experiments on three representative VLMs show that MHA2MLA-VLM restores original model performance with minimal supervised data, significantly reduces KV cache footprint, and integrates seamlessly with KV quantization.
Published: 2026-01-16T17:45:34+00:00
Venue: arXiv
Score: 0.766 (consider)</description><content:encoded>&lt;p&gt;&lt;strong&gt;Authors:&lt;/strong&gt; Xiaoran Fan; Zhichao Sun; Tao Ji; Lixing Shen; Tao Gui&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Journal:&lt;/strong&gt; arXiv&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Relevance:&lt;/strong&gt; 0.766 (consider)&lt;/p&gt;
&lt;p&gt;As vision-language models (VLMs) tackle increasingly complex and multimodal tasks, the rapid growth of Key-Value (KV) cache imposes significant memory and computational bottlenecks during inference. While Multi-Head Latent Attention (MLA) offers an effective means to compress the KV cache and accelerate inference, adapting existing VLMs to the MLA architecture without costly pretraining remains largely unexplored. In this work, we present MHA2MLA-VLM, a parameter-efficient and multimodal-aware framework for converting off-the-shelf VLMs to MLA. Our approach features two core techniques: (1) a modality-adaptive partial-RoPE strategy that supports both traditional and multimodal settings by selectively masking nonessential dimensions, and (2) a modality-decoupled low-rank approximation method that independently compresses the visual and textual KV spaces. Furthermore, we introduce parameter-efficient fine-tuning to minimize adaptation cost and demonstrate that minimizing output activation error, rather than parameter distance, substantially reduces performance loss. Extensive experiments on three representative VLMs show that MHA2MLA-VLM restores original model performance with minimal supervised data, significantly reduces KV cache footprint, and integrates seamlessly with KV quantization.&lt;/p&gt;</content:encoded></item><item><title>SoLA-Vision: Fine-grained Layer-wise Linear Softmax Hybrid Attention</title><link>https://arxiv.org/abs/2601.11164v1</link><guid>http://arxiv.org/abs/2601.11164v1</guid><pubDate>Fri, 16 Jan 2026 10:26:53 +0000</pubDate><dc:creator>Ruibang Li</dc:creator><dc:creator>Guan Luo</dc:creator><dc:creator>Yiwei Zhang</dc:creator><dc:creator>Jin Gao</dc:creator><dc:creator>Bing Li</dc:creator><dc:creator>Weiming Hu</dc:creator><dc:type>preprint</dc:type><prism:publicationName>arXiv</prism:publicationName><description>Standard softmax self-attention excels in vision tasks but incurs quadratic complexity O(N^2), limiting high-resolution deployment. Linear attention reduces the cost to O(N), yet its compressed state representations can impair modeling capacity and accuracy. We present an analytical study that contrasts linear and softmax attention for visual representation learning from a layer-stacking perspective. We further conduct systematic experiments on layer-wise hybridization patterns of linear and softmax attention. Our results show that, compared with rigid intra-block hybrid designs, fine-grained layer-wise hybridization can match or surpass performance while requiring fewer softmax layers. Building on these findings, we propose SoLA-Vision (Softmax-Linear Attention Vision), a flexible layer-wise hybrid attention backbone that enables fine-grained control over how linear and softmax attention are integrated. By strategically inserting a small number of global softmax layers, SoLA-Vision achieves a strong trade-off between accuracy and computational cost. On ImageNet-1K, SoLA-Vision outperforms purely linear and other hybrid attention models. On dense prediction tasks, it consistently surpasses strong baselines by a considerable margin. Code will be released.
Published: 2026-01-16T10:26:53+00:00
Venue: arXiv
Score: 0.766 (consider)</description><content:encoded>&lt;p&gt;&lt;strong&gt;Authors:&lt;/strong&gt; Ruibang Li; Guan Luo; Yiwei Zhang; Jin Gao; Bing Li; Weiming Hu&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Journal:&lt;/strong&gt; arXiv&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Relevance:&lt;/strong&gt; 0.766 (consider)&lt;/p&gt;
&lt;p&gt;Standard softmax self-attention excels in vision tasks but incurs quadratic complexity O(N^2), limiting high-resolution deployment. Linear attention reduces the cost to O(N), yet its compressed state representations can impair modeling capacity and accuracy. We present an analytical study that contrasts linear and softmax attention for visual representation learning from a layer-stacking perspective. We further conduct systematic experiments on layer-wise hybridization patterns of linear and softmax attention. Our results show that, compared with rigid intra-block hybrid designs, fine-grained layer-wise hybridization can match or surpass performance while requiring fewer softmax layers. Building on these findings, we propose SoLA-Vision (Softmax-Linear Attention Vision), a flexible layer-wise hybrid attention backbone that enables fine-grained control over how linear and softmax attention are integrated. By strategically inserting a small number of global softmax layers, SoLA-Vision achieves a strong trade-off between accuracy and computational cost. On ImageNet-1K, SoLA-Vision outperforms purely linear and other hybrid attention models. On dense prediction tasks, it consistently surpasses strong baselines by a considerable margin. Code will be released.&lt;/p&gt;</content:encoded></item><item><title>Sparse Data Tree Canopy Segmentation: Fine-Tuning Leading Pretrained Models on Only 150 Images</title><link>https://arxiv.org/abs/2601.10931v1</link><guid>http://arxiv.org/abs/2601.10931v1</guid><pubDate>Fri, 16 Jan 2026 01:20:32 +0000</pubDate><dc:creator>David Szczecina</dc:creator><dc:creator>Hudson Sun</dc:creator><dc:creator>Anthony Bertnyk</dc:creator><dc:creator>Niloofar Azad</dc:creator><dc:creator>Kyle Gao</dc:creator><dc:creator>Lincoln Linlin Xu</dc:creator><dc:type>preprint</dc:type><prism:publicationName>arXiv</prism:publicationName><description>Tree canopy detection from aerial imagery is an important task for environmental monitoring, urban planning, and ecosystem analysis. Simulating real-life data annotation scarcity, the Solafune Tree Canopy Detection competition provides a small and imbalanced dataset of only 150 annotated images, posing significant challenges for training deep models without severe overfitting. In this work, we evaluate five representative architectures, YOLOv11, Mask R-CNN, DeepLabv3, Swin-UNet, and DINOv2, to assess their suitability for canopy segmentation under extreme data scarcity. Our experiments show that pretrained convolution-based models, particularly YOLOv11 and Mask R-CNN, generalize significantly better than pretrained transformer-based models. DeeplabV3, Swin-UNet and DINOv2 underperform likely due to differences between semantic and instance segmentation tasks, the high data requirements of Vision Transformers, and the lack of strong inductive biases. These findings confirm that transformer-based architectures struggle in low-data regimes without substantial pretraining or augmentation and that differences between semantic and instance segmentation further affect model performance. We provide a detailed analysis of training strategies, augmentation policies, and model behavior under the small-data constraint and demonstrate that lightweight CNN-based methods remain the most reliable for canopy detection on limited imagery.
Published: 2026-01-16T01:20:32+00:00
Venue: arXiv
Score: 0.765 (consider)</description><content:encoded>&lt;p&gt;&lt;strong&gt;Authors:&lt;/strong&gt; David Szczecina; Hudson Sun; Anthony Bertnyk; Niloofar Azad; Kyle Gao; Lincoln Linlin Xu&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Journal:&lt;/strong&gt; arXiv&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Relevance:&lt;/strong&gt; 0.765 (consider)&lt;/p&gt;
&lt;p&gt;Tree canopy detection from aerial imagery is an important task for environmental monitoring, urban planning, and ecosystem analysis. Simulating real-life data annotation scarcity, the Solafune Tree Canopy Detection competition provides a small and imbalanced dataset of only 150 annotated images, posing significant challenges for training deep models without severe overfitting. In this work, we evaluate five representative architectures, YOLOv11, Mask R-CNN, DeepLabv3, Swin-UNet, and DINOv2, to assess their suitability for canopy segmentation under extreme data scarcity. Our experiments show that pretrained convolution-based models, particularly YOLOv11 and Mask R-CNN, generalize significantly better than pretrained transformer-based models. DeeplabV3, Swin-UNet and DINOv2 underperform likely due to differences between semantic and instance segmentation tasks, the high data requirements of Vision Transformers, and the lack of strong inductive biases. These findings confirm that transformer-based architectures struggle in low-data regimes without substantial pretraining or augmentation and that differences between semantic and instance segmentation further affect model performance. We provide a detailed analysis of training strategies, augmentation policies, and model behavior under the small-data constraint and demonstrate that lightweight CNN-based methods remain the most reliable for canopy detection on limited imagery.&lt;/p&gt;</content:encoded></item><item><title>Low-Rank Key Value Attention</title><link>https://arxiv.org/abs/2601.11471v1</link><guid>http://arxiv.org/abs/2601.11471v1</guid><pubDate>Fri, 16 Jan 2026 17:56:40 +0000</pubDate><dc:creator>James O'Neill</dc:creator><dc:creator>Robert Clancy</dc:creator><dc:creator>Mariia Matskevichus</dc:creator><dc:creator>Fergal Reid</dc:creator><dc:type>preprint</dc:type><prism:publicationName>arXiv</prism:publicationName><description>Transformer pretraining is increasingly constrained by memory and compute requirements, with the key-value (KV) cache emerging as a dominant bottleneck during training and autoregressive decoding. We propose \textit{low-rank KV adaptation} (LRKV), a simple modification of multi-head attention that reduces KV cache memory by exploiting redundancy across attention heads while preserving full token-level resolution. Each layer uses a shared full-rank KV projection augmented with low-rank, head-specific residuals, yielding a continuous trade-off between complete sharing and fully independent attention.
  LRKV is a drop-in replacement for standard multi-head attention and directly subsumes query-sharing approaches such as multi-query and grouped-query attention, while remaining distinct from latent-compression methods such as multi-latent attention (MLA). Across large-scale pretraining experiments, LRKV consistently achieves faster loss reduction, lower validation perplexity, and stronger downstream task performance than standard attention, MQA/GQA, and MLA. At the 2.5B scale, LRKV outperforms standard attention while using roughly half the KV cache, and reaches equivalent model quality with up to \textbf{20-25\% less training compute} when measured in cumulative FLOPs. To explain these gains, we analyze attention head structure in operator space and show that LRKV preserves nearly all functional head diversity relative to standard attention, whereas more aggressive KV-sharing mechanisms rely on compensatory query specialization. Together, these results establish LRKV as a practical and effective attention mechanism for scaling Transformer pretraining under memory- and compute-constrained regimes.
Published: 2026-01-16T17:56:40+00:00
Venue: arXiv
Score: 0.765 (consider)</description><content:encoded>&lt;p&gt;&lt;strong&gt;Authors:&lt;/strong&gt; James O&amp;#x27;Neill; Robert Clancy; Mariia Matskevichus; Fergal Reid&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Journal:&lt;/strong&gt; arXiv&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Relevance:&lt;/strong&gt; 0.765 (consider)&lt;/p&gt;
&lt;p&gt;Transformer pretraining is increasingly constrained by memory and compute requirements, with the key-value (KV) cache emerging as a dominant bottleneck during training and autoregressive decoding. We propose \textit{low-rank KV adaptation} (LRKV), a simple modification of multi-head attention that reduces KV cache memory by exploiting redundancy across attention heads while preserving full token-level resolution. Each layer uses a shared full-rank KV projection augmented with low-rank, head-specific residuals, yielding a continuous trade-off between complete sharing and fully independent attention.
  LRKV is a drop-in replacement for standard multi-head attention and directly subsumes query-sharing approaches such as multi-query and grouped-query attention, while remaining distinct from latent-compression methods such as multi-latent attention (MLA). Across large-scale pretraining experiments, LRKV consistently achieves faster loss reduction, lower validation perplexity, and stronger downstream task performance than standard attention, MQA/GQA, and MLA. At the 2.5B scale, LRKV outperforms standard attention while using roughly half the KV cache, and reaches equivalent model quality with up to \textbf{20-25\% less training compute} when measured in cumulative FLOPs. To explain these gains, we analyze attention head structure in operator space and show that LRKV preserves nearly all functional head diversity relative to standard attention, whereas more aggressive KV-sharing mechanisms rely on compensatory query specialization. Together, these results establish LRKV as a practical and effective attention mechanism for scaling Transformer pretraining under memory- and compute-constrained regimes.&lt;/p&gt;</content:encoded></item><item><title>A snow properties-aware deep learning framework for penetration bias estimation of TanDEM-X DEMs over ice sheets</title><link>https://doi.org/10.1016/j.rse.2026.115243</link><guid>10.1016/j.rse.2026.115243</guid><pubDate>Sat, 17 Jan 2026 17:52:37 +0000</pubDate><dc:creator>Alexandre Becker Campos</dc:creator><dc:creator>Antoine Diez-Latteur</dc:creator><dc:creator>José-Luis Bueso-Bello</dc:creator><dc:creator>Matthias H. Braun</dc:creator><dc:creator>Paola Rizzoli</dc:creator><prism:publicationName>Remote Sensing of Environment</prism:publicationName><prism:doi>10.1016/j.rse.2026.115243</prism:doi><description>The accurate assessment of glacier volume and mass changes as well as snow depth is crucial for understanding glaciological processes and the impact of climate change. TanDEM-X, an X-band spaceborne interferometric synthetic aperture radar (InSAR) mission, offers global, high-resolution digital elevation models (DEMs) that are invaluable for these studies. However, the inherent variability in radar wave penetration into snow and ice creates challenges in accurately estimating surface elevation changes and snow depth. Variations in the acquisition geometry and snow properties can affect the estimation of the radar mean phase center, leading to penetration bias and an underestimation of the surface topographic height. In this work, we propose a novel deep learning framework for estimating the penetration bias of TanDEM-X DEMs over ice sheets, by combining the knowledge of the physical properties of snow and the InSAR system for the development of a robust regression framework. Due to the lack of extended reference data, which jeopardizes the use of fully-supervised data-driven approaches, we propose a deep learning approach based on two intrinsically connected tasks: a first unsupervised snow facies segmentation model designed to capture the overall properties of the snowpack independently of the single-pass InSAR acquisition geometries; and a subsequent downstream penetration bias regression model. To ensure that the robustness against the InSAR geometries of the first model is preserved, we propose two approaches: first, we employ a fine-tuning approach to transfer the weights of the segmentation model for a downstream regression task, leveraging the knowledge acquired by the pretext segmentation task for the regression of the penetration bias of TanDEM-X DEMs; and second, a multitask learning approach for the downstream task by jointly training both the segmentation and regression models, ensuring that the snow-related feature representations identified during the segmentation task are consistently leveraged to improve the final regression performance. We demonstrate that utilizing the first model as a pretext task improves convergence and overall performance, whereas the multitask approach enables better generalization. Experimental results over the Greenland Ice Sheet during boreal winter, using IceBridge laser altimeter measurements as reference data, demonstrate that our method estimates the penetration bias with a coefficient of determination R 2 " role="presentation"&gt; R 2 R 2 = 90% and RMSE of 0.65 m, independently of the InSAR acquisition geometry and snow properties. The work performed here is crucial for enhancing the accuracy of TanDEM-X DEMs over snow and ice-covered regions, thereby improving our understanding of glaciological processes and their climatic responses.
Published: 2026-01-17T17:52:37+00:00
Venue: Remote Sensing of Environment
Score: 0.765 (consider)</description><content:encoded>&lt;p&gt;&lt;strong&gt;Authors:&lt;/strong&gt; Alexandre Becker Campos; Antoine Diez-Latteur; José-Luis Bueso-Bello; Matthias H. Braun; Paola Rizzoli&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Journal:&lt;/strong&gt; Remote Sensing of Environment&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;a href="https://doi.org/10.1016/j.rse.2026.115243"&gt;10.1016/j.rse.2026.115243&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Relevance:&lt;/strong&gt; 0.765 (consider)&lt;/p&gt;
&lt;p&gt;The accurate assessment of glacier volume and mass changes as well as snow depth is crucial for understanding glaciological processes and the impact of climate change. TanDEM-X, an X-band spaceborne interferometric synthetic aperture radar (InSAR) mission, offers global, high-resolution digital elevation models (DEMs) that are invaluable for these studies. However, the inherent variability in radar wave penetration into snow and ice creates challenges in accurately estimating surface elevation changes and snow depth. Variations in the acquisition geometry and snow properties can affect the estimation of the radar mean phase center, leading to penetration bias and an underestimation of the surface topographic height. In this work, we propose a novel deep learning framework for estimating the penetration bias of TanDEM-X DEMs over ice sheets, by combining the knowledge of the physical properties of snow and the InSAR system for the development of a robust regression framework. Due to the lack of extended reference data, which jeopardizes the use of fully-supervised data-driven approaches, we propose a deep learning approach based on two intrinsically connected tasks: a first unsupervised snow facies segmentation model designed to capture the overall properties of the snowpack independently of the single-pass InSAR acquisition geometries; and a subsequent downstream penetration bias regression model. To ensure that the robustness against the InSAR geometries of the first model is preserved, we propose two approaches: first, we employ a fine-tuning approach to transfer the weights of the segmentation model for a downstream regression task, leveraging the knowledge acquired by the pretext segmentation task for the regression of the penetration bias of TanDEM-X DEMs; and second, a multitask learning approach for the downstream task by jointly training both the segmentation and regression models, ensuring that the snow-related feature representations identified during the segmentation task are consistently leveraged to improve the final regression performance. We demonstrate that utilizing the first model as a pretext task improves convergence and overall performance, whereas the multitask approach enables better generalization. Experimental results over the Greenland Ice Sheet during boreal winter, using IceBridge laser altimeter measurements as reference data, demonstrate that our method estimates the penetration bias with a coefficient of determination R 2 &amp;quot; role=&amp;quot;presentation&amp;quot;&amp;gt; R 2 R 2 = 90% and RMSE of 0.65 m, independently of the InSAR acquisition geometry and snow properties. The work performed here is crucial for enhancing the accuracy of TanDEM-X DEMs over snow and ice-covered regions, thereby improving our understanding of glaciological processes and their climatic responses.&lt;/p&gt;</content:encoded></item><item><title>Generative AI in different imaging modalities for disease diagnosis: A review</title><link>https://doi.org/10.1016/j.eswa.2026.131162</link><guid>10.1016/j.eswa.2026.131162</guid><pubDate>Sat, 17 Jan 2026 00:36:01 +0000</pubDate><dc:creator>Tariq Ali</dc:creator><dc:creator>Zia-ur Rehmam</dc:creator><dc:creator>Mohammad Hijji</dc:creator><dc:creator>Muhammad Ayaz</dc:creator><dc:creator>Saleh Albelwi</dc:creator><dc:creator>Maria Ijaz</dc:creator><prism:publicationName>Expert Systems with Applications</prism:publicationName><prism:doi>10.1016/j.eswa.2026.131162</prism:doi><description>The rapid development of Artificial Intelligence (AI) has led to significant advances in several industries, particularly in medical imaging. Notable advancements in this domain include generative AI models, particularly various variations of Generative adversarial networks (GANs). Generative AI is growing as a revolutionary technology in clinical imaging, providing novel abilities for enhancing data, image improvement, and particular disease analysis. This review systematically examines more than 150 research papers published from 2018 to 2025, concentrating on the use of generative methods in key imaging techniques such as OCT, X-ray, CT, MRI, PET, retinal fundus imaging, mammography, and ultrasound. This study categorizes and compares the performance of GANs, Variational Autoencoders (VAEs), and diffusion approaches across different applications, including reconstruction of images, high resolution, noise elimination, anomaly identification, and synthetic data production for detecting diseases. The review analysis indicates that generative AI methods have shown significant enhancements in medical diagnostics, with accuracy improvements of between 5 and 20 per cent when synthetically enhanced data is utilized, and the quality of image measures increases between 15 and 40 per cent among modalities, including MRI and CT. The research presents findings indicating that diffusion approaches surpass GANs in generating excellent clinical images, although GANs maintain computational efficiency for real-time augmentation applications. Moreover, we suggest prospective avenues for future studies to address the current constraints and fulfill the changing needs of the medical field. The article summarizes the basic ideas behind generative AI, particularly its ability to produce lifelike and highly accurate visuals. Afterward, it explores each imaging technique’s particular difficulties and possibilities in diagnosing diseases
Published: 2026-01-17T00:36:01+00:00
Venue: Expert Systems with Applications
Score: 0.764 (consider)</description><content:encoded>&lt;p&gt;&lt;strong&gt;Authors:&lt;/strong&gt; Tariq Ali; Zia-ur Rehmam; Mohammad Hijji; Muhammad Ayaz; Saleh Albelwi; Maria Ijaz&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Journal:&lt;/strong&gt; Expert Systems with Applications&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;a href="https://doi.org/10.1016/j.eswa.2026.131162"&gt;10.1016/j.eswa.2026.131162&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Relevance:&lt;/strong&gt; 0.764 (consider)&lt;/p&gt;
&lt;p&gt;The rapid development of Artificial Intelligence (AI) has led to significant advances in several industries, particularly in medical imaging. Notable advancements in this domain include generative AI models, particularly various variations of Generative adversarial networks (GANs). Generative AI is growing as a revolutionary technology in clinical imaging, providing novel abilities for enhancing data, image improvement, and particular disease analysis. This review systematically examines more than 150 research papers published from 2018 to 2025, concentrating on the use of generative methods in key imaging techniques such as OCT, X-ray, CT, MRI, PET, retinal fundus imaging, mammography, and ultrasound. This study categorizes and compares the performance of GANs, Variational Autoencoders (VAEs), and diffusion approaches across different applications, including reconstruction of images, high resolution, noise elimination, anomaly identification, and synthetic data production for detecting diseases. The review analysis indicates that generative AI methods have shown significant enhancements in medical diagnostics, with accuracy improvements of between 5 and 20 per cent when synthetically enhanced data is utilized, and the quality of image measures increases between 15 and 40 per cent among modalities, including MRI and CT. The research presents findings indicating that diffusion approaches surpass GANs in generating excellent clinical images, although GANs maintain computational efficiency for real-time augmentation applications. Moreover, we suggest prospective avenues for future studies to address the current constraints and fulfill the changing needs of the medical field. The article summarizes the basic ideas behind generative AI, particularly its ability to produce lifelike and highly accurate visuals. Afterward, it explores each imaging technique’s particular difficulties and possibilities in diagnosing diseases&lt;/p&gt;</content:encoded></item><item><title>Think-Then-Generate: Reasoning-Aware Text-to-Image Diffusion with LLM Encoders</title><link>https://arxiv.org/abs/2601.10332v1</link><guid>http://arxiv.org/abs/2601.10332v1</guid><pubDate>Thu, 15 Jan 2026 12:19:05 +0000</pubDate><dc:creator>Siqi Kou</dc:creator><dc:creator>Jiachun Jin</dc:creator><dc:creator>Zetong Zhou</dc:creator><dc:creator>Ye Ma</dc:creator><dc:creator>Yugang Wang</dc:creator><dc:creator>Quan Chen</dc:creator><dc:creator>Peng Jiang</dc:creator><dc:creator>Xiao Yang</dc:creator><dc:creator>Jun Zhu</dc:creator><dc:creator>Kai Yu</dc:creator><dc:creator>Zhijie Deng</dc:creator><dc:type>preprint</dc:type><prism:publicationName>arXiv</prism:publicationName><description>Recent progress in text-to-image (T2I) diffusion models (DMs) has enabled high-quality visual synthesis from diverse textual prompts. Yet, most existing T2I DMs, even those equipped with large language model (LLM)-based text encoders, remain text-pixel mappers -- they employ LLMs merely as text encoders, without leveraging their inherent reasoning capabilities to infer what should be visually depicted given the textual prompt. To move beyond such literal generation, we propose the think-then-generate (T2G) paradigm, where the LLM-based text encoder is encouraged to reason about and rewrite raw user prompts; the states of the rewritten prompts then serve as diffusion conditioning. To achieve this, we first activate the think-then-rewrite pattern of the LLM encoder with a lightweight supervised fine-tuning process. Subsequently, the LLM encoder and diffusion backbone are co-optimized to ensure faithful reasoning about the context and accurate rendering of the semantics via Dual-GRPO. In particular, the text encoder is reinforced using image-grounded rewards to infer and recall world knowledge, while the diffusion backbone is pushed to produce semantically consistent and visually coherent images. Experiments show substantial improvements in factual consistency, semantic alignment, and visual realism across reasoning-based image generation and editing benchmarks, achieving 0.79 on WISE score, nearly on par with GPT-4. Our results constitute a promising step toward next-generation unified models with reasoning, expression, and demonstration capacities.
Published: 2026-01-15T12:19:05+00:00
Venue: arXiv
Score: 0.764 (consider)</description><content:encoded>&lt;p&gt;&lt;strong&gt;Authors:&lt;/strong&gt; Siqi Kou; Jiachun Jin; Zetong Zhou; Ye Ma; Yugang Wang; Quan Chen; Peng Jiang; Xiao Yang; Jun Zhu; Kai Yu; Zhijie Deng&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Journal:&lt;/strong&gt; arXiv&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Relevance:&lt;/strong&gt; 0.764 (consider)&lt;/p&gt;
&lt;p&gt;Recent progress in text-to-image (T2I) diffusion models (DMs) has enabled high-quality visual synthesis from diverse textual prompts. Yet, most existing T2I DMs, even those equipped with large language model (LLM)-based text encoders, remain text-pixel mappers -- they employ LLMs merely as text encoders, without leveraging their inherent reasoning capabilities to infer what should be visually depicted given the textual prompt. To move beyond such literal generation, we propose the think-then-generate (T2G) paradigm, where the LLM-based text encoder is encouraged to reason about and rewrite raw user prompts; the states of the rewritten prompts then serve as diffusion conditioning. To achieve this, we first activate the think-then-rewrite pattern of the LLM encoder with a lightweight supervised fine-tuning process. Subsequently, the LLM encoder and diffusion backbone are co-optimized to ensure faithful reasoning about the context and accurate rendering of the semantics via Dual-GRPO. In particular, the text encoder is reinforced using image-grounded rewards to infer and recall world knowledge, while the diffusion backbone is pushed to produce semantically consistent and visually coherent images. Experiments show substantial improvements in factual consistency, semantic alignment, and visual realism across reasoning-based image generation and editing benchmarks, achieving 0.79 on WISE score, nearly on par with GPT-4. Our results constitute a promising step toward next-generation unified models with reasoning, expression, and demonstration capacities.&lt;/p&gt;</content:encoded></item><item><title>Molmo2: Open Weights and Data for Vision-Language Models with Video Understanding and Grounding</title><link>https://arxiv.org/abs/2601.10611v1</link><guid>http://arxiv.org/abs/2601.10611v1</guid><pubDate>Thu, 15 Jan 2026 17:27:44 +0000</pubDate><dc:creator>Christopher Clark</dc:creator><dc:creator>Jieyu Zhang</dc:creator><dc:creator>Zixian Ma</dc:creator><dc:creator>Jae Sung Park</dc:creator><dc:creator>Mohammadreza Salehi</dc:creator><dc:creator>Rohun Tripathi</dc:creator><dc:creator>Sangho Lee</dc:creator><dc:creator>Zhongzheng Ren</dc:creator><dc:creator>Chris Dongjoo Kim</dc:creator><dc:creator>Yinuo Yang</dc:creator><dc:creator>Vincent Shao</dc:creator><dc:creator>Yue Yang</dc:creator><dc:creator>Weikai Huang</dc:creator><dc:creator>Ziqi Gao</dc:creator><dc:creator>Taira Anderson</dc:creator><dc:creator>Jianrui Zhang</dc:creator><dc:creator>Jitesh Jain</dc:creator><dc:creator>George Stoica</dc:creator><dc:creator>Winson Han</dc:creator><dc:creator>Ali Farhadi</dc:creator><dc:creator>Ranjay Krishna</dc:creator><dc:type>preprint</dc:type><prism:publicationName>arXiv</prism:publicationName><description>Today's strongest video-language models (VLMs) remain proprietary. The strongest open-weight models either rely on synthetic data from proprietary VLMs, effectively distilling from them, or do not disclose their training data or recipe. As a result, the open-source community lacks the foundations needed to improve on the state-of-the-art video (and image) language models. Crucially, many downstream applications require more than just high-level video understanding; they require grounding -- either by pointing or by tracking in pixels. Even proprietary models lack this capability. We present Molmo2, a new family of VLMs that are state-of-the-art among open-source models and demonstrate exceptional new capabilities in point-driven grounding in single image, multi-image, and video tasks. Our key contribution is a collection of 7 new video datasets and 2 multi-image datasets, including a dataset of highly detailed video captions for pre-training, a free-form video Q&amp;A dataset for fine-tuning, a new object tracking dataset with complex queries, and an innovative new video pointing dataset, all collected without the use of closed VLMs. We also present a training recipe for this data utilizing an efficient packing and message-tree encoding scheme, and show bi-directional attention on vision tokens and a novel token-weight strategy improves performance. Our best-in-class 8B model outperforms others in the class of open weight and data models on short videos, counting, and captioning, and is competitive on long-videos. On video-grounding Molmo2 significantly outperforms existing open-weight models like Qwen3-VL (35.5 vs 29.6 accuracy on video counting) and surpasses proprietary models like Gemini 3 Pro on some tasks (38.4 vs 20.0 F1 on video pointing and 56.2 vs 41.1 J&amp;F on video tracking).
Published: 2026-01-15T17:27:44+00:00
Venue: arXiv
Score: 0.764 (consider)</description><content:encoded>&lt;p&gt;&lt;strong&gt;Authors:&lt;/strong&gt; Christopher Clark; Jieyu Zhang; Zixian Ma; Jae Sung Park; Mohammadreza Salehi; Rohun Tripathi; Sangho Lee; Zhongzheng Ren; Chris Dongjoo Kim; Yinuo Yang; Vincent Shao; Yue Yang; Weikai Huang; Ziqi Gao; Taira Anderson; Jianrui Zhang; Jitesh Jain; George Stoica; Winson Han; Ali Farhadi; Ranjay Krishna&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Journal:&lt;/strong&gt; arXiv&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Relevance:&lt;/strong&gt; 0.764 (consider)&lt;/p&gt;
&lt;p&gt;Today&amp;#x27;s strongest video-language models (VLMs) remain proprietary. The strongest open-weight models either rely on synthetic data from proprietary VLMs, effectively distilling from them, or do not disclose their training data or recipe. As a result, the open-source community lacks the foundations needed to improve on the state-of-the-art video (and image) language models. Crucially, many downstream applications require more than just high-level video understanding; they require grounding -- either by pointing or by tracking in pixels. Even proprietary models lack this capability. We present Molmo2, a new family of VLMs that are state-of-the-art among open-source models and demonstrate exceptional new capabilities in point-driven grounding in single image, multi-image, and video tasks. Our key contribution is a collection of 7 new video datasets and 2 multi-image datasets, including a dataset of highly detailed video captions for pre-training, a free-form video Q&amp;amp;A dataset for fine-tuning, a new object tracking dataset with complex queries, and an innovative new video pointing dataset, all collected without the use of closed VLMs. We also present a training recipe for this data utilizing an efficient packing and message-tree encoding scheme, and show bi-directional attention on vision tokens and a novel token-weight strategy improves performance. Our best-in-class 8B model outperforms others in the class of open weight and data models on short videos, counting, and captioning, and is competitive on long-videos. On video-grounding Molmo2 significantly outperforms existing open-weight models like Qwen3-VL (35.5 vs 29.6 accuracy on video counting) and surpasses proprietary models like Gemini 3 Pro on some tasks (38.4 vs 20.0 F1 on video pointing and 56.2 vs 41.1 J&amp;amp;F on video tracking).&lt;/p&gt;</content:encoded></item><item><title>PRL: Process Reward Learning Improves LLMs' Reasoning Ability and Broadens the Reasoning Boundary</title><link>https://arxiv.org/abs/2601.10201v1</link><guid>http://arxiv.org/abs/2601.10201v1</guid><pubDate>Thu, 15 Jan 2026 09:01:53 +0000</pubDate><dc:creator>Jiarui Yao</dc:creator><dc:creator>Ruida Wang</dc:creator><dc:creator>Tong Zhang</dc:creator><dc:type>preprint</dc:type><prism:publicationName>arXiv</prism:publicationName><description>Improving the reasoning abilities of Large Language Models (LLMs) has been a continuous topic recently. But most relevant works are based on outcome rewards at the trajectory level, missing fine-grained supervision during the reasoning process. Other existing training frameworks that try to combine process signals together to optimize LLMs also rely heavily on tedious additional steps like MCTS, training a separate reward model, etc., doing harm to the training efficiency. Moreover, the intuition behind the process signals design lacks rigorous theoretical support, leaving the understanding of the optimization mechanism opaque. In this paper, we propose Process Reward Learning (PRL), which decomposes the entropy regularized reinforcement learning objective into intermediate steps, with rigorous process rewards that could be assigned to models accordingly. Starting from theoretical motivation, we derive the formulation of PRL that is essentially equivalent to the objective of reward maximization plus a KL-divergence penalty term between the policy model and a reference model. However, PRL could turn the outcome reward into process supervision signals, which helps better guide the exploration during RL optimization. From our experiment results, we demonstrate that PRL not only improves the average performance for LLMs' reasoning ability measured by average @ n, but also broadens the reasoning boundary by improving the pass @ n metric. Extensive experiments show the effectiveness of PRL could be verified and generalized.
Published: 2026-01-15T09:01:53+00:00
Venue: arXiv
Score: 0.764 (consider)</description><content:encoded>&lt;p&gt;&lt;strong&gt;Authors:&lt;/strong&gt; Jiarui Yao; Ruida Wang; Tong Zhang&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Journal:&lt;/strong&gt; arXiv&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Relevance:&lt;/strong&gt; 0.764 (consider)&lt;/p&gt;
&lt;p&gt;Improving the reasoning abilities of Large Language Models (LLMs) has been a continuous topic recently. But most relevant works are based on outcome rewards at the trajectory level, missing fine-grained supervision during the reasoning process. Other existing training frameworks that try to combine process signals together to optimize LLMs also rely heavily on tedious additional steps like MCTS, training a separate reward model, etc., doing harm to the training efficiency. Moreover, the intuition behind the process signals design lacks rigorous theoretical support, leaving the understanding of the optimization mechanism opaque. In this paper, we propose Process Reward Learning (PRL), which decomposes the entropy regularized reinforcement learning objective into intermediate steps, with rigorous process rewards that could be assigned to models accordingly. Starting from theoretical motivation, we derive the formulation of PRL that is essentially equivalent to the objective of reward maximization plus a KL-divergence penalty term between the policy model and a reference model. However, PRL could turn the outcome reward into process supervision signals, which helps better guide the exploration during RL optimization. From our experiment results, we demonstrate that PRL not only improves the average performance for LLMs&amp;#x27; reasoning ability measured by average @ n, but also broadens the reasoning boundary by improving the pass @ n metric. Extensive experiments show the effectiveness of PRL could be verified and generalized.&lt;/p&gt;</content:encoded></item><item><title>SAMannot: A Memory-Efficient, Local, Open-source Framework for Interactive Video Instance Segmentation based on SAM2</title><link>https://arxiv.org/abs/2601.11301v1</link><guid>http://arxiv.org/abs/2601.11301v1</guid><pubDate>Fri, 16 Jan 2026 13:55:10 +0000</pubDate><dc:creator>Gergely Dinya</dc:creator><dc:creator>András Gelencsér</dc:creator><dc:creator>Krisztina Kupán</dc:creator><dc:creator>Clemens Küpper</dc:creator><dc:creator>Kristóf Karacs</dc:creator><dc:creator>Anna Gelencsér-Horváth</dc:creator><dc:type>preprint</dc:type><prism:publicationName>arXiv</prism:publicationName><description>Current research workflows for precise video segmentation are often forced into a compromise between labor-intensive manual curation, costly commercial platforms, and/or privacy-compromising cloud-based services. The demand for high-fidelity video instance segmentation in research is often hindered by the bottleneck of manual annotation and the privacy concerns of cloud-based tools. We present SAMannot, an open-source, local framework that integrates the Segment Anything Model 2 (SAM2) into a human-in-the-loop workflow. To address the high resource requirements of foundation models, we modified the SAM2 dependency and implemented a processing layer that minimizes computational overhead and maximizes throughput, ensuring a highly responsive user interface. Key features include persistent instance identity management, an automated ``lock-and-refine'' workflow with barrier frames, and a mask-skeletonization-based auto-prompting mechanism. SAMannot facilitates the generation of research-ready datasets in YOLO and PNG formats alongside structured interaction logs. Verified through animal behavior tracking use-cases and subsets of the LVOS and DAVIS benchmark datasets, the tool provides a scalable, private, and cost-effective alternative to commercial platforms for complex video annotation tasks.
Published: 2026-01-16T13:55:10+00:00
Venue: arXiv
Score: 0.763 (consider)</description><content:encoded>&lt;p&gt;&lt;strong&gt;Authors:&lt;/strong&gt; Gergely Dinya; András Gelencsér; Krisztina Kupán; Clemens Küpper; Kristóf Karacs; Anna Gelencsér-Horváth&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Journal:&lt;/strong&gt; arXiv&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Relevance:&lt;/strong&gt; 0.763 (consider)&lt;/p&gt;
&lt;p&gt;Current research workflows for precise video segmentation are often forced into a compromise between labor-intensive manual curation, costly commercial platforms, and/or privacy-compromising cloud-based services. The demand for high-fidelity video instance segmentation in research is often hindered by the bottleneck of manual annotation and the privacy concerns of cloud-based tools. We present SAMannot, an open-source, local framework that integrates the Segment Anything Model 2 (SAM2) into a human-in-the-loop workflow. To address the high resource requirements of foundation models, we modified the SAM2 dependency and implemented a processing layer that minimizes computational overhead and maximizes throughput, ensuring a highly responsive user interface. Key features include persistent instance identity management, an automated ``lock-and-refine&amp;#x27;&amp;#x27; workflow with barrier frames, and a mask-skeletonization-based auto-prompting mechanism. SAMannot facilitates the generation of research-ready datasets in YOLO and PNG formats alongside structured interaction logs. Verified through animal behavior tracking use-cases and subsets of the LVOS and DAVIS benchmark datasets, the tool provides a scalable, private, and cost-effective alternative to commercial platforms for complex video annotation tasks.&lt;/p&gt;</content:encoded></item><item><title>Context-Aware Semantic Segmentation via Stage-Wise Attention</title><link>https://arxiv.org/abs/2601.11310v1</link><guid>http://arxiv.org/abs/2601.11310v1</guid><pubDate>Fri, 16 Jan 2026 14:06:46 +0000</pubDate><dc:creator>Antoine Carreaud</dc:creator><dc:creator>Elias Naha</dc:creator><dc:creator>Arthur Chansel</dc:creator><dc:creator>Nina Lahellec</dc:creator><dc:creator>Jan Skaloud</dc:creator><dc:creator>Adrien Gressin</dc:creator><dc:type>preprint</dc:type><prism:publicationName>arXiv</prism:publicationName><description>Semantic ultra high resolution image (UHR) segmentation is essential in remote sensing applications such as aerial mapping and environmental monitoring. Transformer-based models struggle in this setting because memory grows quadratically with token count, constraining either the contextual scope or the spatial resolution. We introduce CASWiT (Context-Aware Stage-Wise Transformer), a dual-branch, Swin-based architecture that injects global cues into fine-grained UHR features. A context encoder processes a downsampled neighborhood to capture long-range dependencies, while a high resolution encoder extracts detailed features from UHR patches. A cross-scale fusion module, combining cross-attention and gated feature injection, enriches high-resolution tokens with context. Beyond architecture, we propose a SimMIM-style pretraining. We mask 75% of the high-resolution image tokens and the low-resolution center region that spatially corresponds to the UHR patch, then train the shared dual-encoder with small decoder to reconstruct the UHR initial image. Extensive experiments on the large-scale IGN FLAIR-HUB aerial dataset demonstrate the effectiveness of CASWiT. Our method achieves 65.83% mIoU, outperforming RGB baselines by 1.78 points. On URUR, CASWiT achieves 49.1% mIoU, surpassing the current SoTA by +0.9% under the official evaluation protocol. All codes are provided on: https://huggingface.co/collections/heig-vd-geo/caswit.
Published: 2026-01-16T14:06:46+00:00
Venue: arXiv
Score: 0.763 (consider)</description><content:encoded>&lt;p&gt;&lt;strong&gt;Authors:&lt;/strong&gt; Antoine Carreaud; Elias Naha; Arthur Chansel; Nina Lahellec; Jan Skaloud; Adrien Gressin&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Journal:&lt;/strong&gt; arXiv&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Relevance:&lt;/strong&gt; 0.763 (consider)&lt;/p&gt;
&lt;p&gt;Semantic ultra high resolution image (UHR) segmentation is essential in remote sensing applications such as aerial mapping and environmental monitoring. Transformer-based models struggle in this setting because memory grows quadratically with token count, constraining either the contextual scope or the spatial resolution. We introduce CASWiT (Context-Aware Stage-Wise Transformer), a dual-branch, Swin-based architecture that injects global cues into fine-grained UHR features. A context encoder processes a downsampled neighborhood to capture long-range dependencies, while a high resolution encoder extracts detailed features from UHR patches. A cross-scale fusion module, combining cross-attention and gated feature injection, enriches high-resolution tokens with context. Beyond architecture, we propose a SimMIM-style pretraining. We mask 75% of the high-resolution image tokens and the low-resolution center region that spatially corresponds to the UHR patch, then train the shared dual-encoder with small decoder to reconstruct the UHR initial image. Extensive experiments on the large-scale IGN FLAIR-HUB aerial dataset demonstrate the effectiveness of CASWiT. Our method achieves 65.83% mIoU, outperforming RGB baselines by 1.78 points. On URUR, CASWiT achieves 49.1% mIoU, surpassing the current SoTA by +0.9% under the official evaluation protocol. All codes are provided on: https://huggingface.co/collections/heig-vd-geo/caswit.&lt;/p&gt;</content:encoded></item><item><title>Probabilistic Modeling of Disparity Uncertainty for Robust and Efficient Stereo Matching</title><link>https://doi.org/10.1016/j.patcog.2026.113102</link><guid>10.1016/j.patcog.2026.113102</guid><pubDate>Sun, 18 Jan 2026 06:28:58 +0000</pubDate><dc:creator>Wenxiao Cai</dc:creator><dc:creator>Dongting Hu</dc:creator><dc:creator>Ruoyan Yin</dc:creator><dc:creator>Jiankang Deng</dc:creator><dc:creator>Huan Fu</dc:creator><dc:creator>Wankou Yang</dc:creator><dc:creator>Mingming Gong</dc:creator><prism:publicationName>Pattern Recognition</prism:publicationName><prism:doi>10.1016/j.patcog.2026.113102</prism:doi><description>Stereo matching plays a crucial role in various applications, where understanding uncertainty can enhance both safety and reliability. Despite this, the estimation and analysis of uncertainty in stereo matching have been largely overlooked. Previous works struggle to separate it into data (aleatoric) and model (epistemic) components and often provide limited interpretations of uncertainty. This interpretability is essential, as it allows for a clearer understanding of the underlying sources of error, enhancing both prediction confidence and decision-making processes. In this paper, we propose a new uncertainty-aware stereo matching framework. We adopt Bayes risk as the measurement of uncertainty and use it to separately estimate data and model uncertainty. We systematically analyze data uncertainty based on the probabilistic distribution of disparity and efficiently estimate model uncertainty without repeated model training. Experiments are conducted on four stereo benchmarks, and the results demonstrate that our method can estimate uncertainty accurately and efficiently, without sacrificing the disparity prediction accuracy.
Published: 2026-01-18T06:28:58+00:00
Venue: Pattern Recognition
Score: 0.763 (consider)</description><content:encoded>&lt;p&gt;&lt;strong&gt;Authors:&lt;/strong&gt; Wenxiao Cai; Dongting Hu; Ruoyan Yin; Jiankang Deng; Huan Fu; Wankou Yang; Mingming Gong&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Journal:&lt;/strong&gt; Pattern Recognition&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;a href="https://doi.org/10.1016/j.patcog.2026.113102"&gt;10.1016/j.patcog.2026.113102&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Relevance:&lt;/strong&gt; 0.763 (consider)&lt;/p&gt;
&lt;p&gt;Stereo matching plays a crucial role in various applications, where understanding uncertainty can enhance both safety and reliability. Despite this, the estimation and analysis of uncertainty in stereo matching have been largely overlooked. Previous works struggle to separate it into data (aleatoric) and model (epistemic) components and often provide limited interpretations of uncertainty. This interpretability is essential, as it allows for a clearer understanding of the underlying sources of error, enhancing both prediction confidence and decision-making processes. In this paper, we propose a new uncertainty-aware stereo matching framework. We adopt Bayes risk as the measurement of uncertainty and use it to separately estimate data and model uncertainty. We systematically analyze data uncertainty based on the probabilistic distribution of disparity and efficiently estimate model uncertainty without repeated model training. Experiments are conducted on four stereo benchmarks, and the results demonstrate that our method can estimate uncertainty accurately and efficiently, without sacrificing the disparity prediction accuracy.&lt;/p&gt;</content:encoded></item></channel></rss>