<?xml version='1.0' encoding='utf-8'?>
<rss xmlns:content="http://purl.org/rss/1.0/modules/content/" xmlns:dc="http://purl.org/dc/elements/1.1/" xmlns:prism="http://prismstandard.org/namespaces/basic/3.0/" version="2.0"><channel><title>ZotWatch Feed</title><link>https://ehehe.cn/zotwatch/</link><description>AI-assisted literature watch</description><lastBuildDate>Thu, 18 Dec 2025 03:01:14 +0000</lastBuildDate><item><title>Boosting Multi-Modal Large Language Model With Enhanced Visual Features</title><link>https://doi.org/10.1109/tpami.2025.3644851</link><guid>10.1109/tpami.2025.3644851</guid><pubDate>Wed, 17 Dec 2025 18:46:02 +0000</pubDate><dc:creator>Yiwei Ma</dc:creator><dc:creator>Weihuang Lin</dc:creator><dc:creator>Zhibin Wang</dc:creator><dc:creator>Jiayi Ji</dc:creator><dc:creator>Xiaoshuai Sun</dc:creator><dc:creator>Chia-Wen Lin</dc:creator><dc:creator>Rongrong Ji</dc:creator><prism:publicationName>IEEE Transactions on Pattern Analysis and Machine Intelligence</prism:publicationName><prism:doi>10.1109/tpami.2025.3644851</prism:doi><description>Recent advancements in computer vision (CV) and large language models (LLMs) have spurred significant interest in multi-modal large language models (MLLMs), which aim to integrate visual and textual modalities for enhanced understanding and generation tasks. While much of the existing research focuses on optimizing projectors and LLMs to improve MLLM performance, a critical question remains underexplored: Has the full potential of visual features in MLLMs been realized? To address this question, we identify two key limitations in current MLLM architectures and propose vMLLM, a vision-enhanced MLLM designed to fully leverage the capabilities of visual features. vMLLM introduces two novel components: the Multi-level Aggregation Module (MAM) and the Intra- and inter-modal Enhancement Module (IEM). The MAM aggregates multi-layer features from the vision encoder, capturing both high-level semantic information and low-level spatial details, thereby enriching the visual representation. The IEM enhances visual features through intra- and inter-modal interactions, effectively suppressing irrelevant information while amplifying task-relevant features, leading to more robust multimodal understanding. We conduct extensive experiments on multiple benchmarks, evaluating vMLLM across diverse settings, including different vision encoders, training dataset scales, and varying sizes of LLMs. Our results demonstrate that vMLLM consistently achieves significant performance improvements, validating its effectiveness in harnessing the potential of visual features. These findings highlight the importance of optimizing visual feature extraction and interaction mechanisms in MLLMs, paving the way for more advanced multimodal AI systems. To promote reproducibility and further research, we have made the code and pre-trained models publicly available on GitHub: https://github.com/xmu-xiaoma666/vMLLM.
Published: 2025-12-17T18:46:02+00:00
Venue: IEEE Transactions on Pattern Analysis and Machine Intelligence
Score: 0.837 (must_read)</description><content:encoded>&lt;p&gt;&lt;strong&gt;Authors:&lt;/strong&gt; Yiwei Ma; Weihuang Lin; Zhibin Wang; Jiayi Ji; Xiaoshuai Sun; Chia-Wen Lin; Rongrong Ji&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Journal:&lt;/strong&gt; IEEE Transactions on Pattern Analysis and Machine Intelligence&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;a href="https://doi.org/10.1109/tpami.2025.3644851"&gt;10.1109/tpami.2025.3644851&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Relevance:&lt;/strong&gt; 0.837 (must_read)&lt;/p&gt;
&lt;p&gt;Recent advancements in computer vision (CV) and large language models (LLMs) have spurred significant interest in multi-modal large language models (MLLMs), which aim to integrate visual and textual modalities for enhanced understanding and generation tasks. While much of the existing research focuses on optimizing projectors and LLMs to improve MLLM performance, a critical question remains underexplored: Has the full potential of visual features in MLLMs been realized? To address this question, we identify two key limitations in current MLLM architectures and propose vMLLM, a vision-enhanced MLLM designed to fully leverage the capabilities of visual features. vMLLM introduces two novel components: the Multi-level Aggregation Module (MAM) and the Intra- and inter-modal Enhancement Module (IEM). The MAM aggregates multi-layer features from the vision encoder, capturing both high-level semantic information and low-level spatial details, thereby enriching the visual representation. The IEM enhances visual features through intra- and inter-modal interactions, effectively suppressing irrelevant information while amplifying task-relevant features, leading to more robust multimodal understanding. We conduct extensive experiments on multiple benchmarks, evaluating vMLLM across diverse settings, including different vision encoders, training dataset scales, and varying sizes of LLMs. Our results demonstrate that vMLLM consistently achieves significant performance improvements, validating its effectiveness in harnessing the potential of visual features. These findings highlight the importance of optimizing visual feature extraction and interaction mechanisms in MLLMs, paving the way for more advanced multimodal AI systems. To promote reproducibility and further research, we have made the code and pre-trained models publicly available on GitHub: https://github.com/xmu-xiaoma666/vMLLM.&lt;/p&gt;</content:encoded></item><item><title>$\ell _{0}$-Regularized Sparse Coding-based Interpretable Network for Multi-Modal Image Fusion</title><link>https://doi.org/10.1109/tpami.2025.3643898</link><guid>10.1109/tpami.2025.3643898</guid><pubDate>Wed, 17 Dec 2025 18:46:02 +0000</pubDate><dc:creator>Gargi Panda</dc:creator><dc:creator>Soumitra Kundu</dc:creator><dc:creator>Saumik Bhattacharya</dc:creator><dc:creator>Aurobinda Routray</dc:creator><prism:publicationName>IEEE Transactions on Pattern Analysis and Machine Intelligence</prism:publicationName><prism:doi>10.1109/tpami.2025.3643898</prism:doi><description>Multi-modal image fusion (MMIF) enhances the information content of the fused image by combining the unique as well as common features obtained from different modality sensor images, improving visualization, object detection, and many more tasks. In this work, we introduce an interpretable network for the MMIF task, named FNet, based on an \ell _{0} \ell _{0} -regularized multi-modal convolutional sparse coding (MCSC) model. Specifically, for solving the \ell _{0} \ell _{0} -regularized CSC problem, we design a learnable \ell _{0} \ell _{0} -regularized sparse coding (LZSC) block in a principled manner through deep unfolding. Given different modality source images, FNet first separates the unique and common features from them using the LZSC block and then these features are combined to generate the final fused image. Additionally, we propose an \ell _{0} \ell _{0} -regularized MCSC model for the inverse fusion process. Based on this model, we introduce an interpretable inverse fusion network named IFNet, which is utilized during FNet's training. Extensive experiments show that FNet achieves high-quality fusion results across eight different MMIF datasets. Furthermore, we show that FNet enhances downstream object detection 0, 0, 0 and semantic segmentation in visible-thermal image pairs. We have also visualized the intermediate results of FNet, which demonstrates the good interpretability of our network. Link for code and models: https://github.com/ggpp132/code.
Published: 2025-12-17T18:46:02+00:00
Venue: IEEE Transactions on Pattern Analysis and Machine Intelligence
Score: 0.831 (must_read)</description><content:encoded>&lt;p&gt;&lt;strong&gt;Authors:&lt;/strong&gt; Gargi Panda; Soumitra Kundu; Saumik Bhattacharya; Aurobinda Routray&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Journal:&lt;/strong&gt; IEEE Transactions on Pattern Analysis and Machine Intelligence&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;a href="https://doi.org/10.1109/tpami.2025.3643898"&gt;10.1109/tpami.2025.3643898&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Relevance:&lt;/strong&gt; 0.831 (must_read)&lt;/p&gt;
&lt;p&gt;Multi-modal image fusion (MMIF) enhances the information content of the fused image by combining the unique as well as common features obtained from different modality sensor images, improving visualization, object detection, and many more tasks. In this work, we introduce an interpretable network for the MMIF task, named FNet, based on an \ell _{0} \ell _{0} -regularized multi-modal convolutional sparse coding (MCSC) model. Specifically, for solving the \ell _{0} \ell _{0} -regularized CSC problem, we design a learnable \ell _{0} \ell _{0} -regularized sparse coding (LZSC) block in a principled manner through deep unfolding. Given different modality source images, FNet first separates the unique and common features from them using the LZSC block and then these features are combined to generate the final fused image. Additionally, we propose an \ell _{0} \ell _{0} -regularized MCSC model for the inverse fusion process. Based on this model, we introduce an interpretable inverse fusion network named IFNet, which is utilized during FNet&amp;#x27;s training. Extensive experiments show that FNet achieves high-quality fusion results across eight different MMIF datasets. Furthermore, we show that FNet enhances downstream object detection 0, 0, 0 and semantic segmentation in visible-thermal image pairs. We have also visualized the intermediate results of FNet, which demonstrates the good interpretability of our network. Link for code and models: https://github.com/ggpp132/code.&lt;/p&gt;</content:encoded></item><item><title>SCG-FSOD: Semantic Correlation-Guided Few-shot Object Detection in Remote Sensing Images</title><link>https://doi.org/10.1109/tgrs.2025.3645045</link><guid>10.1109/tgrs.2025.3645045</guid><pubDate>Wed, 17 Dec 2025 18:46:07 +0000</pubDate><dc:creator>Hengchao Hu</dc:creator><dc:creator>Aobo Li</dc:creator><dc:creator>Jinjian Wu</dc:creator><dc:creator>Jie Feng</dc:creator><dc:creator>Yaoqiang Jia</dc:creator><prism:publicationName>IEEE Transactions on Geoscience and Remote Sensing</prism:publicationName><prism:doi>10.1109/tgrs.2025.3645045</prism:doi><description>Few-shot object detection (FSOD) in remote sensing imagery aims to achieve accurate detection of novel object categories with limited training samples. However, current mainstream transfer learning based methods are frequently constrained by two major challenges. Firstly, due to the scarcity of novel class samples and the bias of pre-trained models towards base classes, novel classes may rely overly on base class knowledge to construct feature representations, causing novel class features to be easily confused with similar base classes. Secondly, the inter-class similarity and intra-class diversity in remote sensing images can further exacerbate classification confusion. To tackle the above problems, we propose a semantic correlation-guided method for FSOD (SCG-FSOD) in remote sensing images. Specifically, we design an inter-class semantic correlation transfer (ISCT) module to fully explore the semantic correlations between base and novel classes, employing knowledge distillation for cross-class correlation transfer. This module effectively mitigates base-class bias while enhancing the discriminative power of novel class features. Furthermore, we propose a semantic correlation-driven supervised contrastive learning (SSCL) module, which employs semantic correlation priors to weight negative sample pairs in supervised contrastive learning. By imposing stronger separation constraints on negative pairs with high inter-class similarity, this module significantly alleviates feature confusion in remote sensing images. Extensive experiments conducted on two public benchmark datasets (DIOR and NWPU VHR-10.v2) demonstrate the effectiveness of our proposed method, which achieves competitive performance compared with several state-of-the-art approaches.
Published: 2025-12-17T18:46:07+00:00
Venue: IEEE Transactions on Geoscience and Remote Sensing
Score: 0.822 (must_read)</description><content:encoded>&lt;p&gt;&lt;strong&gt;Authors:&lt;/strong&gt; Hengchao Hu; Aobo Li; Jinjian Wu; Jie Feng; Yaoqiang Jia&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Journal:&lt;/strong&gt; IEEE Transactions on Geoscience and Remote Sensing&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;a href="https://doi.org/10.1109/tgrs.2025.3645045"&gt;10.1109/tgrs.2025.3645045&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Relevance:&lt;/strong&gt; 0.822 (must_read)&lt;/p&gt;
&lt;p&gt;Few-shot object detection (FSOD) in remote sensing imagery aims to achieve accurate detection of novel object categories with limited training samples. However, current mainstream transfer learning based methods are frequently constrained by two major challenges. Firstly, due to the scarcity of novel class samples and the bias of pre-trained models towards base classes, novel classes may rely overly on base class knowledge to construct feature representations, causing novel class features to be easily confused with similar base classes. Secondly, the inter-class similarity and intra-class diversity in remote sensing images can further exacerbate classification confusion. To tackle the above problems, we propose a semantic correlation-guided method for FSOD (SCG-FSOD) in remote sensing images. Specifically, we design an inter-class semantic correlation transfer (ISCT) module to fully explore the semantic correlations between base and novel classes, employing knowledge distillation for cross-class correlation transfer. This module effectively mitigates base-class bias while enhancing the discriminative power of novel class features. Furthermore, we propose a semantic correlation-driven supervised contrastive learning (SSCL) module, which employs semantic correlation priors to weight negative sample pairs in supervised contrastive learning. By imposing stronger separation constraints on negative pairs with high inter-class similarity, this module significantly alleviates feature confusion in remote sensing images. Extensive experiments conducted on two public benchmark datasets (DIOR and NWPU VHR-10.v2) demonstrate the effectiveness of our proposed method, which achieves competitive performance compared with several state-of-the-art approaches.&lt;/p&gt;</content:encoded></item><item><title>Incomplete Modalities Restoration via Hierarchical Adaptation for Robust Multimodal Segmentation</title><link>https://doi.org/10.1109/tip.2025.3642612</link><guid>10.1109/tip.2025.3642612</guid><pubDate>Wed, 17 Dec 2025 18:49:42 +0000</pubDate><dc:creator>Yujia Sun</dc:creator><dc:creator>Weisheng Dong</dc:creator><dc:creator>Peng Wu</dc:creator><dc:creator>Mingtao Feng</dc:creator><dc:creator>Tao Huang</dc:creator><dc:creator>Xin Li</dc:creator><dc:creator>Guangming Shi</dc:creator><prism:publicationName>IEEE Transactions on Image Processing</prism:publicationName><prism:doi>10.1109/tip.2025.3642612</prism:doi><description>Multimodal semantic segmentation has significantly advanced the field of semantic segmentation by integrating data from multiple sources. However, this task often encounters missing modality scenarios due to challenges such as sensor failures or data transmission errors, which can result in substantial performance degradation. Existing approaches to addressing missing modalities predominantly involve training separate models tailored to specific missing scenarios, typically requiring considerable computational resources. In this paper, we propose a Hierarchical Adaptation framework to Restore Missing Modalities for Multimodal segmentation (HARM3), which enables frozen pretrained multimodal models to be directly applied to missing-modality semantic segmentation tasks with minimal parameter updates. Central to HARM3 is a text-instructed missing modality prompt module, which learns multimodal semantic knowledge by utilizing available modalities and textual instructions to generate prompts for the missing modalities. By incorporating a small set of trainable parameters, this module effectively facilitates knowledge transfer between high-resource domains and low-resource domains where missing modalities are more prevalent. Besides, to further enhance the model’s robustness and adaptability, we introduce adaptive perturbation training and an affine modality adapter. Extensive experimental results demonstrate the effectiveness and robustness of HARM3 across a variety of missing modality scenarios.
Published: 2025-12-17T18:49:42+00:00
Venue: IEEE Transactions on Image Processing
Score: 0.816 (must_read)</description><content:encoded>&lt;p&gt;&lt;strong&gt;Authors:&lt;/strong&gt; Yujia Sun; Weisheng Dong; Peng Wu; Mingtao Feng; Tao Huang; Xin Li; Guangming Shi&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Journal:&lt;/strong&gt; IEEE Transactions on Image Processing&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;a href="https://doi.org/10.1109/tip.2025.3642612"&gt;10.1109/tip.2025.3642612&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Relevance:&lt;/strong&gt; 0.816 (must_read)&lt;/p&gt;
&lt;p&gt;Multimodal semantic segmentation has significantly advanced the field of semantic segmentation by integrating data from multiple sources. However, this task often encounters missing modality scenarios due to challenges such as sensor failures or data transmission errors, which can result in substantial performance degradation. Existing approaches to addressing missing modalities predominantly involve training separate models tailored to specific missing scenarios, typically requiring considerable computational resources. In this paper, we propose a Hierarchical Adaptation framework to Restore Missing Modalities for Multimodal segmentation (HARM3), which enables frozen pretrained multimodal models to be directly applied to missing-modality semantic segmentation tasks with minimal parameter updates. Central to HARM3 is a text-instructed missing modality prompt module, which learns multimodal semantic knowledge by utilizing available modalities and textual instructions to generate prompts for the missing modalities. By incorporating a small set of trainable parameters, this module effectively facilitates knowledge transfer between high-resource domains and low-resource domains where missing modalities are more prevalent. Besides, to further enhance the model’s robustness and adaptability, we introduce adaptive perturbation training and an affine modality adapter. Extensive experimental results demonstrate the effectiveness and robustness of HARM3 across a variety of missing modality scenarios.&lt;/p&gt;</content:encoded></item><item><title>Improving the Stability and Efficiency of Diffusion Models for Content Consistent Super-Resolution</title><link>https://doi.org/10.1109/tip.2025.3640863</link><guid>10.1109/tip.2025.3640863</guid><pubDate>Wed, 17 Dec 2025 18:49:42 +0000</pubDate><dc:creator>Lingchen Sun</dc:creator><dc:creator>Rongyuan Wu</dc:creator><dc:creator>Jie Liang</dc:creator><dc:creator>Zhengqiang Zhang</dc:creator><dc:creator>Hongwei Yong</dc:creator><dc:creator>Lei Zhang</dc:creator><prism:publicationName>IEEE Transactions on Image Processing</prism:publicationName><prism:doi>10.1109/tip.2025.3640863</prism:doi><description>The generative priors of pre-trained latent diffusion models (DMs) have demonstrated great potential to enhance the visual quality of image super-resolution (SR) results. However, the noise sampling process in DMs introduces randomness in the SR outputs, and the generated contents can differ a lot with different noise samples. The multi-step diffusion process can be accelerated by distilling methods, but the generative capacity is difficult to control. To address these issues, we analyze the respective advantages of DMs and generative adversarial networks (GANs) and propose to partition the generative SR process into two stages, where the DM is employed for reconstructing image structures and the GAN is employed for improving fine-grained details. Specifically, we propose a non-uniform timestep sampling strategy in the first stage. A single timestep sampling is first applied to extract the coarse information from the input image, then a few reverse steps are used to reconstruct the main structures. In the second stage, we finetune the decoder of the pre-trained variational auto-encoder by adversarial GAN training for deterministic detail enhancement. Once trained, our proposed method, namely content consistent super-resolution (CCSR), allows flexible use of different diffusion steps in the inference stage without re-training. Extensive experiments show that with 2 or even 1 diffusion step, CCSR can significantly improve the content consistency of SR outputs while keeping high perceptual quality. Codes and models can be found at https://github.com/csslc/CCSR.
Published: 2025-12-17T18:49:42+00:00
Venue: IEEE Transactions on Image Processing
Score: 0.815 (must_read)</description><content:encoded>&lt;p&gt;&lt;strong&gt;Authors:&lt;/strong&gt; Lingchen Sun; Rongyuan Wu; Jie Liang; Zhengqiang Zhang; Hongwei Yong; Lei Zhang&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Journal:&lt;/strong&gt; IEEE Transactions on Image Processing&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;a href="https://doi.org/10.1109/tip.2025.3640863"&gt;10.1109/tip.2025.3640863&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Relevance:&lt;/strong&gt; 0.815 (must_read)&lt;/p&gt;
&lt;p&gt;The generative priors of pre-trained latent diffusion models (DMs) have demonstrated great potential to enhance the visual quality of image super-resolution (SR) results. However, the noise sampling process in DMs introduces randomness in the SR outputs, and the generated contents can differ a lot with different noise samples. The multi-step diffusion process can be accelerated by distilling methods, but the generative capacity is difficult to control. To address these issues, we analyze the respective advantages of DMs and generative adversarial networks (GANs) and propose to partition the generative SR process into two stages, where the DM is employed for reconstructing image structures and the GAN is employed for improving fine-grained details. Specifically, we propose a non-uniform timestep sampling strategy in the first stage. A single timestep sampling is first applied to extract the coarse information from the input image, then a few reverse steps are used to reconstruct the main structures. In the second stage, we finetune the decoder of the pre-trained variational auto-encoder by adversarial GAN training for deterministic detail enhancement. Once trained, our proposed method, namely content consistent super-resolution (CCSR), allows flexible use of different diffusion steps in the inference stage without re-training. Extensive experiments show that with 2 or even 1 diffusion step, CCSR can significantly improve the content consistency of SR outputs while keeping high perceptual quality. Codes and models can be found at https://github.com/csslc/CCSR.&lt;/p&gt;</content:encoded></item><item><title>NiCI-Pruning: Enhancing Diffusion Model Pruning via Noise in Clean Image Guidance</title><link>https://doi.org/10.1109/tip.2025.3643138</link><guid>10.1109/tip.2025.3643138</guid><pubDate>Wed, 17 Dec 2025 18:49:42 +0000</pubDate><dc:creator>Junzhu Mao</dc:creator><dc:creator>Zeren Sun</dc:creator><dc:creator>Yazhou Yao</dc:creator><dc:creator>Tianfei Zhou</dc:creator><dc:creator>Liqiang Nie</dc:creator><dc:creator>Xiansheng Hua</dc:creator><prism:publicationName>IEEE Transactions on Image Processing</prism:publicationName><prism:doi>10.1109/tip.2025.3643138</prism:doi><description>The substantial successes achieved by diffusion probabilistic models have prompted the study of their employment in resource-limited scenarios. Pruning methods have been proven effective in compressing discriminative models relying on the correlation between training losses and model performances. However, diffusion models employ an iterative process for generating high-quality images, leading to a breakdown of such connections. To address this challenge, we propose a simple yet effective method, named NiCI-Pruning (Noise in Clean Image Pruning), for the compression of diffusion models. NiCI-Pruning capitalizes the noise predicted by the model based on clean image inputs, favoring it as a feature for establishing reconstruction losses. Accordingly, Taylor expansion is employed for the proposed reconstruction loss to evaluate the parameter importance effectively. Moreover, we propose an interval sampling strategy that incorporates a timestep-weighted schema, alleviating the risk of misleading information obtained at later timesteps. We provide comprehensive experimental results to affirm the superiority of our proposed approach. Notably, our method achieves a remarkable average reduction of 30.4% in FID score increase across five different datasets compared to the state-of-the-art diffusion pruning method at equivalent pruning rates. Our code and models have been made available at https://github.com/ NUST-Machine-Intelligence-Laboratory/NiCI-Pruning.
Published: 2025-12-17T18:49:42+00:00
Venue: IEEE Transactions on Image Processing
Score: 0.814 (must_read)</description><content:encoded>&lt;p&gt;&lt;strong&gt;Authors:&lt;/strong&gt; Junzhu Mao; Zeren Sun; Yazhou Yao; Tianfei Zhou; Liqiang Nie; Xiansheng Hua&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Journal:&lt;/strong&gt; IEEE Transactions on Image Processing&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;a href="https://doi.org/10.1109/tip.2025.3643138"&gt;10.1109/tip.2025.3643138&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Relevance:&lt;/strong&gt; 0.814 (must_read)&lt;/p&gt;
&lt;p&gt;The substantial successes achieved by diffusion probabilistic models have prompted the study of their employment in resource-limited scenarios. Pruning methods have been proven effective in compressing discriminative models relying on the correlation between training losses and model performances. However, diffusion models employ an iterative process for generating high-quality images, leading to a breakdown of such connections. To address this challenge, we propose a simple yet effective method, named NiCI-Pruning (Noise in Clean Image Pruning), for the compression of diffusion models. NiCI-Pruning capitalizes the noise predicted by the model based on clean image inputs, favoring it as a feature for establishing reconstruction losses. Accordingly, Taylor expansion is employed for the proposed reconstruction loss to evaluate the parameter importance effectively. Moreover, we propose an interval sampling strategy that incorporates a timestep-weighted schema, alleviating the risk of misleading information obtained at later timesteps. We provide comprehensive experimental results to affirm the superiority of our proposed approach. Notably, our method achieves a remarkable average reduction of 30.4% in FID score increase across five different datasets compared to the state-of-the-art diffusion pruning method at equivalent pruning rates. Our code and models have been made available at https://github.com/ NUST-Machine-Intelligence-Laboratory/NiCI-Pruning.&lt;/p&gt;</content:encoded></item><item><title>Fine-Grained Visual Classification via Adaptive Attention Quantization Transformer</title><link>https://doi.org/10.1109/tnnls.2025.3643809</link><guid>10.1109/tnnls.2025.3643809</guid><pubDate>Wed, 17 Dec 2025 18:47:14 +0000</pubDate><dc:creator>Shishi Qiao</dc:creator><dc:creator>Shixian Li</dc:creator><dc:creator>Haiyong Zheng</dc:creator><prism:publicationName>IEEE Transactions on Neural Networks and Learning Systems</prism:publicationName><prism:doi>10.1109/tnnls.2025.3643809</prism:doi><description>Vision transformer (ViT) has recently demonstrated remarkable performance in fine-grained visual classification (FGVC). However, most existing ViT-based methods often overlook the varied focus of different attention heads, in which heads that attend to nondiscriminative regions would dilute the discriminative signal crucial for FGVC. To address such issues, we propose a novel adaptive attention quantization transformer (A2QTrans) for FGVC to select the key discriminative features by analyzing the heads’ attention, which comprises three key modules: the adaptive quantization selection (AQS) module, the background elimination (BE) module, and the dynamic hybrid optimization (DHO) module. Specifically, the AQS module dynamically selects the most discriminative features in a data-driven manner by quantizing the attention scores across multiple attention heads with a global, learnable threshold. This process effectively filters out generally irrelevant information from nondiscriminative tokens, thus concentrating attention on important regions. To address the nondifferentiability inherent in updating this threshold during binarization, our AQS module employs a straight-through estimator (STE) for discrete optimization, enabling end-to-end gradient backpropagation. In addition, we utilize the prior that background regions usually do not contain meaningful information, and design the BE module to further calibrate the focus of the attention heads to the main objects in images. Finally, the DHO module adaptively optimizes and integrates the attentive results of the AQS and BE modules to achieve optimal classification performance. Extensive experiments conducted on four challenging FGVC benchmark datasets and three ViT variants demonstrate A2QTrans’s superior performance, achieving state-of-the-art (SOTA) results. The source code is available at https://github.com/Lishixian0817/A2QTrans
Published: 2025-12-17T18:47:14+00:00
Venue: IEEE Transactions on Neural Networks and Learning Systems
Score: 0.811 (must_read)</description><content:encoded>&lt;p&gt;&lt;strong&gt;Authors:&lt;/strong&gt; Shishi Qiao; Shixian Li; Haiyong Zheng&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Journal:&lt;/strong&gt; IEEE Transactions on Neural Networks and Learning Systems&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;a href="https://doi.org/10.1109/tnnls.2025.3643809"&gt;10.1109/tnnls.2025.3643809&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Relevance:&lt;/strong&gt; 0.811 (must_read)&lt;/p&gt;
&lt;p&gt;Vision transformer (ViT) has recently demonstrated remarkable performance in fine-grained visual classification (FGVC). However, most existing ViT-based methods often overlook the varied focus of different attention heads, in which heads that attend to nondiscriminative regions would dilute the discriminative signal crucial for FGVC. To address such issues, we propose a novel adaptive attention quantization transformer (A2QTrans) for FGVC to select the key discriminative features by analyzing the heads’ attention, which comprises three key modules: the adaptive quantization selection (AQS) module, the background elimination (BE) module, and the dynamic hybrid optimization (DHO) module. Specifically, the AQS module dynamically selects the most discriminative features in a data-driven manner by quantizing the attention scores across multiple attention heads with a global, learnable threshold. This process effectively filters out generally irrelevant information from nondiscriminative tokens, thus concentrating attention on important regions. To address the nondifferentiability inherent in updating this threshold during binarization, our AQS module employs a straight-through estimator (STE) for discrete optimization, enabling end-to-end gradient backpropagation. In addition, we utilize the prior that background regions usually do not contain meaningful information, and design the BE module to further calibrate the focus of the attention heads to the main objects in images. Finally, the DHO module adaptively optimizes and integrates the attentive results of the AQS and BE modules to achieve optimal classification performance. Extensive experiments conducted on four challenging FGVC benchmark datasets and three ViT variants demonstrate A2QTrans’s superior performance, achieving state-of-the-art (SOTA) results. The source code is available at https://github.com/Lishixian0817/A2QTrans&lt;/p&gt;</content:encoded></item><item><title>S2FEINet: A Spatial-Spectral Feature Extraction and Interactive Network for Fusing Hyperspectral and Multispectral Images</title><link>https://doi.org/10.1016/j.inffus.2025.104066</link><guid>10.1016/j.inffus.2025.104066</guid><pubDate>Tue, 16 Dec 2025 07:50:31 +0000</pubDate><dc:creator>Yong Zhang</dc:creator><dc:creator>Dayun Wu</dc:creator><dc:creator>Wenlong Ke</dc:creator><dc:creator>Wenzhe Liu</dc:creator><prism:publicationName>Information Fusion</prism:publicationName><prism:doi>10.1016/j.inffus.2025.104066</prism:doi><description>Hyperspectral images (HSIs) provide rich spectral information valuable for numerous applications, but their limited spatial resolution often restricts practical utility. To address this challenge, fusion with high-resolution multispectral images (MSIs) has become an effective strategy for spatial enhancement. While recent deep learning-based fusion methods have shown promising results, several critical limitations persist. Convolutional neural networks (CNNs) are constrained by local receptive fields in capturing global dependencies, while conventional attention mechanisms tend to underutilize local features. Moreover, most existing approaches prioritize spatial features over spectral features, resulting in inadequate spectral reconstruction and loss of spectral details in the fused image. Additionally, many existing methods exhibit insufficient capability in enhancing spatial details and refining spectral information. To overcome these limitations, we propose a novel structured integration framework with an explicit interaction stage, termed the Spatial-Spectral Feature Extraction and Interaction Network (S 2 FEINet). Our architecture incorporates two dedicated modules: the Spectral Feature Extraction (SpeFE) module that captures long-range dependencies in low-resolution HSIs while learning inter-band correlations, and the Spatial Feature Extraction (SpaFE) module that extracts enhanced spatial features from high-resolution MSIs. These modules interact through a cross-domain fusion mechanism to achieve balanced spatial-spectral enhancement. Comprehensive experiments on four benchmark datasets and one real-world dataset demonstrate that S 2 FEINet outperforms ten state-of-the-art methods across multiple evaluation metrics. Specifically, compared to the second-ranked method, S 2 FEINet achieves improvements in mean peak signal-to-noise ratio (MPSNR) by 0.8082 dB, 0.1107 dB, 0.4310 dB, 0.1884 dB, and 0.2238 dB, respectively, across the datasets. The code is available at https://github.com/lab-807/SSFEINet .
Published: 2025-12-16T07:50:31+00:00
Venue: Information Fusion
Score: 0.810 (must_read)</description><content:encoded>&lt;p&gt;&lt;strong&gt;Authors:&lt;/strong&gt; Yong Zhang; Dayun Wu; Wenlong Ke; Wenzhe Liu&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Journal:&lt;/strong&gt; Information Fusion&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;a href="https://doi.org/10.1016/j.inffus.2025.104066"&gt;10.1016/j.inffus.2025.104066&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Relevance:&lt;/strong&gt; 0.810 (must_read)&lt;/p&gt;
&lt;p&gt;Hyperspectral images (HSIs) provide rich spectral information valuable for numerous applications, but their limited spatial resolution often restricts practical utility. To address this challenge, fusion with high-resolution multispectral images (MSIs) has become an effective strategy for spatial enhancement. While recent deep learning-based fusion methods have shown promising results, several critical limitations persist. Convolutional neural networks (CNNs) are constrained by local receptive fields in capturing global dependencies, while conventional attention mechanisms tend to underutilize local features. Moreover, most existing approaches prioritize spatial features over spectral features, resulting in inadequate spectral reconstruction and loss of spectral details in the fused image. Additionally, many existing methods exhibit insufficient capability in enhancing spatial details and refining spectral information. To overcome these limitations, we propose a novel structured integration framework with an explicit interaction stage, termed the Spatial-Spectral Feature Extraction and Interaction Network (S 2 FEINet). Our architecture incorporates two dedicated modules: the Spectral Feature Extraction (SpeFE) module that captures long-range dependencies in low-resolution HSIs while learning inter-band correlations, and the Spatial Feature Extraction (SpaFE) module that extracts enhanced spatial features from high-resolution MSIs. These modules interact through a cross-domain fusion mechanism to achieve balanced spatial-spectral enhancement. Comprehensive experiments on four benchmark datasets and one real-world dataset demonstrate that S 2 FEINet outperforms ten state-of-the-art methods across multiple evaluation metrics. Specifically, compared to the second-ranked method, S 2 FEINet achieves improvements in mean peak signal-to-noise ratio (MPSNR) by 0.8082 dB, 0.1107 dB, 0.4310 dB, 0.1884 dB, and 0.2238 dB, respectively, across the datasets. The code is available at https://github.com/lab-807/SSFEINet .&lt;/p&gt;</content:encoded></item><item><title>Route-DETR: Pairwise Query Routing in Transformers for Object Detection</title><link>https://arxiv.org/abs/2512.13876v1</link><guid>http://arxiv.org/abs/2512.13876v1</guid><pubDate>Mon, 15 Dec 2025 20:26:58 +0000</pubDate><dc:creator>Ye Zhang</dc:creator><dc:creator>Qi Chen</dc:creator><dc:creator>Wenyou Huang</dc:creator><dc:creator>Rui Liu</dc:creator><dc:creator>Zhengjian Kang</dc:creator><dc:type>preprint</dc:type><prism:publicationName>arXiv</prism:publicationName><description>Detection Transformer (DETR) offers an end-to-end solution for object detection by eliminating hand-crafted components like non-maximum suppression. However, DETR suffers from inefficient query competition where multiple queries converge to similar positions, leading to redundant computations. We present Route-DETR, which addresses these issues through adaptive pairwise routing in decoder self-attention layers. Our key insight is distinguishing between competing queries (targeting the same object) versus complementary queries (targeting different objects) using inter-query similarity, confidence scores, and geometry. We introduce dual routing mechanisms: suppressor routes that modulate attention between competing queries to reduce duplication, and delegator routes that encourage exploration of different regions. These are implemented via learnable low-rank attention biases enabling asymmetric query interactions. A dual-branch training strategy incorporates routing biases only during training while preserving standard attention for inference, ensuring no additional computational cost. Experiments on COCO and Cityscapes demonstrate consistent improvements across multiple DETR baselines, achieving +1.7% mAP gain over DINO on ResNet-50 and reaching 57.6% mAP on Swin-L, surpassing prior state-of-the-art models.
Published: 2025-12-15T20:26:58+00:00
Venue: arXiv
Score: 0.809 (must_read)</description><content:encoded>&lt;p&gt;&lt;strong&gt;Authors:&lt;/strong&gt; Ye Zhang; Qi Chen; Wenyou Huang; Rui Liu; Zhengjian Kang&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Journal:&lt;/strong&gt; arXiv&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Relevance:&lt;/strong&gt; 0.809 (must_read)&lt;/p&gt;
&lt;p&gt;Detection Transformer (DETR) offers an end-to-end solution for object detection by eliminating hand-crafted components like non-maximum suppression. However, DETR suffers from inefficient query competition where multiple queries converge to similar positions, leading to redundant computations. We present Route-DETR, which addresses these issues through adaptive pairwise routing in decoder self-attention layers. Our key insight is distinguishing between competing queries (targeting the same object) versus complementary queries (targeting different objects) using inter-query similarity, confidence scores, and geometry. We introduce dual routing mechanisms: suppressor routes that modulate attention between competing queries to reduce duplication, and delegator routes that encourage exploration of different regions. These are implemented via learnable low-rank attention biases enabling asymmetric query interactions. A dual-branch training strategy incorporates routing biases only during training while preserving standard attention for inference, ensuring no additional computational cost. Experiments on COCO and Cityscapes demonstrate consistent improvements across multiple DETR baselines, achieving +1.7% mAP gain over DINO on ResNet-50 and reaching 57.6% mAP on Swin-L, surpassing prior state-of-the-art models.&lt;/p&gt;</content:encoded></item><item><title>UniqueSplat: View-conditioned 3D Gaussian Splatting for Generalizable 3D Reconstruction</title><link>https://doi.org/10.1109/tip.2025.3642574</link><guid>10.1109/tip.2025.3642574</guid><pubDate>Wed, 17 Dec 2025 18:49:42 +0000</pubDate><dc:creator>Haixu Song</dc:creator><dc:creator>Xiaoke Yang</dc:creator><dc:creator>Shengjun Zhang</dc:creator><dc:creator>Jiwen Lu</dc:creator><dc:creator>Yueqi Duan</dc:creator><prism:publicationName>IEEE Transactions on Image Processing</prism:publicationName><prism:doi>10.1109/tip.2025.3642574</prism:doi><description>In this paper, we propose UniqueSplat, a view-conditioned feed-forward 3D Gaussian Splatting model to reconstruct customized 3D radiance fields for each view query. Existing feed-forward methods such as pixelSplat and MVS-plat aim to generate fixed Gaussians across all views of each scene by minimizing the error between rendered views and ground-truth images. However, such fixed Gaussians generally render images from all views and lack the ability to adapt to specific viewpoints, as they do not incorporate target view information when predicting Gaussians. To address this, our UniqueSplat learns the view-conditioned information as a prior and incorporates this knowledge into network parameters, so that Gaussians are dynamically adjusted in accordance with different views. Specifically, we propose a two-branch view-conditioned hyperNetwork to simultaneously learn view-agnostic embeddings and view-specific knowledge, which not only explores the shareable knowledge from various views, but also adapts the model to specific views at test time. Extensive experiments on widely-used datasets including RealEstate10K, ACID and DTU demonstrate the superiority of UniqueSplat over the state-of-the-art methods. Moreover, UniqueSplat encouragingly outperforms existing methods in cross-dataset evaluation, showing its notable generalization ability.
Published: 2025-12-17T18:49:42+00:00
Venue: IEEE Transactions on Image Processing
Score: 0.807 (must_read)</description><content:encoded>&lt;p&gt;&lt;strong&gt;Authors:&lt;/strong&gt; Haixu Song; Xiaoke Yang; Shengjun Zhang; Jiwen Lu; Yueqi Duan&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Journal:&lt;/strong&gt; IEEE Transactions on Image Processing&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;a href="https://doi.org/10.1109/tip.2025.3642574"&gt;10.1109/tip.2025.3642574&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Relevance:&lt;/strong&gt; 0.807 (must_read)&lt;/p&gt;
&lt;p&gt;In this paper, we propose UniqueSplat, a view-conditioned feed-forward 3D Gaussian Splatting model to reconstruct customized 3D radiance fields for each view query. Existing feed-forward methods such as pixelSplat and MVS-plat aim to generate fixed Gaussians across all views of each scene by minimizing the error between rendered views and ground-truth images. However, such fixed Gaussians generally render images from all views and lack the ability to adapt to specific viewpoints, as they do not incorporate target view information when predicting Gaussians. To address this, our UniqueSplat learns the view-conditioned information as a prior and incorporates this knowledge into network parameters, so that Gaussians are dynamically adjusted in accordance with different views. Specifically, we propose a two-branch view-conditioned hyperNetwork to simultaneously learn view-agnostic embeddings and view-specific knowledge, which not only explores the shareable knowledge from various views, but also adapts the model to specific views at test time. Extensive experiments on widely-used datasets including RealEstate10K, ACID and DTU demonstrate the superiority of UniqueSplat over the state-of-the-art methods. Moreover, UniqueSplat encouragingly outperforms existing methods in cross-dataset evaluation, showing its notable generalization ability.&lt;/p&gt;</content:encoded></item><item><title>CMKD: CNN/Transformer-Based Cross-Model Knowledge Distillation for Audio Classification</title><link>https://doi.org/10.1109/tpami.2025.3644853</link><guid>10.1109/tpami.2025.3644853</guid><pubDate>Wed, 17 Dec 2025 18:46:02 +0000</pubDate><dc:creator>Yuan Gong</dc:creator><dc:creator>Sameer Khurana</dc:creator><dc:creator>Andrew Rouditchenko</dc:creator><dc:creator>James Glass</dc:creator><prism:publicationName>IEEE Transactions on Pattern Analysis and Machine Intelligence</prism:publicationName><prism:doi>10.1109/tpami.2025.3644853</prism:doi><description>Audio classification is an active research area with a wide range of applications. Over the past decade, convolutional neural networks (CNNs) have been the de-facto standard building block for end-to-end audio classification models. Recently, neural networks based solely on self-attention mechanisms such as the Audio Spectrogram Transformer (AST) have been shown to outperform CNNs. In this paper, we find an intriguing interaction between the two very different models - CNN and AST models are good teachers for each other. When we use either of them as the teacher and train the other model as the student via knowledge distillation (KD), the performance of the student model noticeably improves, and in many cases, is better than the teacher model. In our experiments with this CNN/Transformer Cross-Model Knowledge Distillation (CMKD) method we achieve new state-of-the-art performance on FSD50K, AudioSet, and ESC-50.
Published: 2025-12-17T18:46:02+00:00
Venue: IEEE Transactions on Pattern Analysis and Machine Intelligence
Score: 0.804 (must_read)</description><content:encoded>&lt;p&gt;&lt;strong&gt;Authors:&lt;/strong&gt; Yuan Gong; Sameer Khurana; Andrew Rouditchenko; James Glass&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Journal:&lt;/strong&gt; IEEE Transactions on Pattern Analysis and Machine Intelligence&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;a href="https://doi.org/10.1109/tpami.2025.3644853"&gt;10.1109/tpami.2025.3644853&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Relevance:&lt;/strong&gt; 0.804 (must_read)&lt;/p&gt;
&lt;p&gt;Audio classification is an active research area with a wide range of applications. Over the past decade, convolutional neural networks (CNNs) have been the de-facto standard building block for end-to-end audio classification models. Recently, neural networks based solely on self-attention mechanisms such as the Audio Spectrogram Transformer (AST) have been shown to outperform CNNs. In this paper, we find an intriguing interaction between the two very different models - CNN and AST models are good teachers for each other. When we use either of them as the teacher and train the other model as the student via knowledge distillation (KD), the performance of the student model noticeably improves, and in many cases, is better than the teacher model. In our experiments with this CNN/Transformer Cross-Model Knowledge Distillation (CMKD) method we achieve new state-of-the-art performance on FSD50K, AudioSet, and ESC-50.&lt;/p&gt;</content:encoded></item><item><title>Cross-Scale Context-Aware Ship Detection in SAR Images using CSCF-Net</title><link>https://doi.org/10.1109/lgrs.2025.3645569</link><guid>10.1109/lgrs.2025.3645569</guid><pubDate>Wed, 17 Dec 2025 18:50:17 +0000</pubDate><dc:creator>Liangang Qi</dc:creator><dc:creator>Chen Huang</dc:creator><dc:creator>Qiang Guo</dc:creator><prism:publicationName>IEEE Geoscience and Remote Sensing Letters</prism:publicationName><prism:doi>10.1109/lgrs.2025.3645569</prism:doi><description>SAR ship detection faces challenges such as scale diversity, weak target features, and strong background interference. To address these issues, we propose CSCF-Net, integrating a Multi-Scale Feature Fusion (MSFF) module for cross-scale contextual aggregation and a Multi-Task Interactive Detection Head (MTIDH) for task-specific optimization through dynamic deformable convolution. Extensive experiments on three SAR datasets demonstrate superior performance: CSCF-Net achieves 98.5% mAP on SSDD, 91.5% on HRSID, and 98.1% on SAR-Ship-Dataset, with 1.0%, 2.2%, and 0.9% improvements over baseline respectively, outperforming state-of-the-art methods and validating the effectiveness of our proposed method.
Published: 2025-12-17T18:50:17+00:00
Venue: IEEE Geoscience and Remote Sensing Letters
Score: 0.802 (must_read)</description><content:encoded>&lt;p&gt;&lt;strong&gt;Authors:&lt;/strong&gt; Liangang Qi; Chen Huang; Qiang Guo&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Journal:&lt;/strong&gt; IEEE Geoscience and Remote Sensing Letters&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;a href="https://doi.org/10.1109/lgrs.2025.3645569"&gt;10.1109/lgrs.2025.3645569&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Relevance:&lt;/strong&gt; 0.802 (must_read)&lt;/p&gt;
&lt;p&gt;SAR ship detection faces challenges such as scale diversity, weak target features, and strong background interference. To address these issues, we propose CSCF-Net, integrating a Multi-Scale Feature Fusion (MSFF) module for cross-scale contextual aggregation and a Multi-Task Interactive Detection Head (MTIDH) for task-specific optimization through dynamic deformable convolution. Extensive experiments on three SAR datasets demonstrate superior performance: CSCF-Net achieves 98.5% mAP on SSDD, 91.5% on HRSID, and 98.1% on SAR-Ship-Dataset, with 1.0%, 2.2%, and 0.9% improvements over baseline respectively, outperforming state-of-the-art methods and validating the effectiveness of our proposed method.&lt;/p&gt;</content:encoded></item><item><title>Enhancing Maritime Search and Rescue: Incremental Unsupervised Domain Adaptation with Synthetic Data and Pseudo-labeling</title><link>https://doi.org/10.1016/j.eswa.2025.130864</link><guid>10.1016/j.eswa.2025.130864</guid><pubDate>Tue, 16 Dec 2025 16:52:36 +0000</pubDate><dc:creator>Juan P. Martinez-Esteso</dc:creator><dc:creator>Francisco J. Castellanos</dc:creator><dc:creator>Antonio Javier Gallego</dc:creator><prism:publicationName>Expert Systems with Applications</prism:publicationName><prism:doi>10.1016/j.eswa.2025.130864</prism:doi><description>Maritime search and rescue operations are critical for saving lives in emergencies, where time is a decisive factor since delays can drastically reduce the chances of survival for those in distress. These missions are particularly challenging due to the inherent complexity of the maritime environment, marked by changing weather, dynamic sea states, and limited visibility. Developing reliable machine learning systems for this task typically requires large amounts of labeled data that capture all possible operating conditions. However, collecting and annotating such data is costly and often unfeasible in real-world maritime scenarios. To address this limitation, we propose a domain adaptation strategy for a segmentation-based detection model that estimates a probability map indicating the presence of human bodies at sea. The method enables unsupervised learning to adapt from a labeled synthetic domain to a real, unlabeled domain by employing a Domain-Adversarial Neural Network that aligns feature representations across domains, and an iterative pseudo-labeling process that selects high-confidence predictions on the target data to progressively refine the model. By leveraging synthetic data—automatically generated and labeled—our approach adapts effectively to real-world conditions without requiring manual annotation. Experimental results show that our method outperforms several state-of-the-art detectors while maintaining a lightweight architecture. Moreover, it generalizes well under diverse and adverse environmental conditions, including fog, rain, and low-light scenes, demonstrating its robustness and suitability for real-world deployment in critical rescue operations.
Published: 2025-12-16T16:52:36+00:00
Venue: Expert Systems with Applications
Score: 0.802 (must_read)</description><content:encoded>&lt;p&gt;&lt;strong&gt;Authors:&lt;/strong&gt; Juan P. Martinez-Esteso; Francisco J. Castellanos; Antonio Javier Gallego&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Journal:&lt;/strong&gt; Expert Systems with Applications&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;a href="https://doi.org/10.1016/j.eswa.2025.130864"&gt;10.1016/j.eswa.2025.130864&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Relevance:&lt;/strong&gt; 0.802 (must_read)&lt;/p&gt;
&lt;p&gt;Maritime search and rescue operations are critical for saving lives in emergencies, where time is a decisive factor since delays can drastically reduce the chances of survival for those in distress. These missions are particularly challenging due to the inherent complexity of the maritime environment, marked by changing weather, dynamic sea states, and limited visibility. Developing reliable machine learning systems for this task typically requires large amounts of labeled data that capture all possible operating conditions. However, collecting and annotating such data is costly and often unfeasible in real-world maritime scenarios. To address this limitation, we propose a domain adaptation strategy for a segmentation-based detection model that estimates a probability map indicating the presence of human bodies at sea. The method enables unsupervised learning to adapt from a labeled synthetic domain to a real, unlabeled domain by employing a Domain-Adversarial Neural Network that aligns feature representations across domains, and an iterative pseudo-labeling process that selects high-confidence predictions on the target data to progressively refine the model. By leveraging synthetic data—automatically generated and labeled—our approach adapts effectively to real-world conditions without requiring manual annotation. Experimental results show that our method outperforms several state-of-the-art detectors while maintaining a lightweight architecture. Moreover, it generalizes well under diverse and adverse environmental conditions, including fog, rain, and low-light scenes, demonstrating its robustness and suitability for real-world deployment in critical rescue operations.&lt;/p&gt;</content:encoded></item><item><title>Solving finite element methods with spiking networks</title><link>https://doi.org/10.1038/s42256-025-01158-9</link><guid>10.1038/s42256-025-01158-9</guid><pubDate>Wed, 17 Dec 2025 10:02:54 +0000</pubDate><dc:creator>Wenhao Song</dc:creator><dc:creator>Zixu Wang</dc:creator><dc:creator>J. Joshua Yang</dc:creator><prism:publicationName>Nature Machine Intelligence</prism:publicationName><prism:doi>10.1038/s42256-025-01158-9</prism:doi><description>Brain-inspired computing can enhance the finite element method, a cornerstone of scientific modelling, by reducing energy costs and reframing numerical simulation through neural dynamics.
Published: 2025-12-17T10:02:54+00:00
Venue: Nature Machine Intelligence
Score: 0.801 (must_read)</description><content:encoded>&lt;p&gt;&lt;strong&gt;Authors:&lt;/strong&gt; Wenhao Song; Zixu Wang; J. Joshua Yang&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Journal:&lt;/strong&gt; Nature Machine Intelligence&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;a href="https://doi.org/10.1038/s42256-025-01158-9"&gt;10.1038/s42256-025-01158-9&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Relevance:&lt;/strong&gt; 0.801 (must_read)&lt;/p&gt;
&lt;p&gt;Brain-inspired computing can enhance the finite element method, a cornerstone of scientific modelling, by reducing energy costs and reframing numerical simulation through neural dynamics.&lt;/p&gt;</content:encoded></item><item><title>MDAFNet: Multi-scale Differential Edge and Adaptive Frequency Guided Network for Infrared Small Target Detection</title><link>https://doi.org/10.1109/lgrs.2025.3645669</link><guid>10.1109/lgrs.2025.3645669</guid><pubDate>Wed, 17 Dec 2025 18:50:17 +0000</pubDate><dc:creator>Shuying Li</dc:creator><dc:creator>Qiang Ma</dc:creator><dc:creator>San Zhang</dc:creator><dc:creator>Wuwei Wang</dc:creator><dc:creator>Chuang Yang</dc:creator><prism:publicationName>IEEE Geoscience and Remote Sensing Letters</prism:publicationName><prism:doi>10.1109/lgrs.2025.3645669</prism:doi><description>Infrared small target detection (IRSTD) plays a crucial role in numerous military and civilian applications. However, existing methods often face the gradual degradation of target edge pixels as the number of network layers increases, and traditional convolution struggles to differentiate between frequency components during feature extraction, leading to low-frequency backgrounds interfering with high-frequency targets and high-frequency noise triggering false detections. To address these limitations, we propose MDAFNet (Multi-scale Differential Edge and Adaptive Frequency Guided Network for Infrared Small Target Detection), which integrates the Multi-Scale Differential Edge (MSDE) module and Dual-Domain Adaptive Feature Enhancement (DAFE) module. The MSDE module, through a multi-scale edge extraction and enhancement mechanism, effectively compensates for the cumulative loss of target edge information during downsampling. The DAFE module combines frequency domain processing mechanisms with simulated frequency decomposition and fusion mechanisms in the spatial domain to effectively improve the network’s capability to adaptively enhance high-frequency targets and selectively suppress high-frequency noise. Experimental results on multiple datasets demonstrate the superior detection performance of MDAFNet.
Published: 2025-12-17T18:50:17+00:00
Venue: IEEE Geoscience and Remote Sensing Letters
Score: 0.798 (must_read)</description><content:encoded>&lt;p&gt;&lt;strong&gt;Authors:&lt;/strong&gt; Shuying Li; Qiang Ma; San Zhang; Wuwei Wang; Chuang Yang&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Journal:&lt;/strong&gt; IEEE Geoscience and Remote Sensing Letters&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;a href="https://doi.org/10.1109/lgrs.2025.3645669"&gt;10.1109/lgrs.2025.3645669&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Relevance:&lt;/strong&gt; 0.798 (must_read)&lt;/p&gt;
&lt;p&gt;Infrared small target detection (IRSTD) plays a crucial role in numerous military and civilian applications. However, existing methods often face the gradual degradation of target edge pixels as the number of network layers increases, and traditional convolution struggles to differentiate between frequency components during feature extraction, leading to low-frequency backgrounds interfering with high-frequency targets and high-frequency noise triggering false detections. To address these limitations, we propose MDAFNet (Multi-scale Differential Edge and Adaptive Frequency Guided Network for Infrared Small Target Detection), which integrates the Multi-Scale Differential Edge (MSDE) module and Dual-Domain Adaptive Feature Enhancement (DAFE) module. The MSDE module, through a multi-scale edge extraction and enhancement mechanism, effectively compensates for the cumulative loss of target edge information during downsampling. The DAFE module combines frequency domain processing mechanisms with simulated frequency decomposition and fusion mechanisms in the spatial domain to effectively improve the network’s capability to adaptively enhance high-frequency targets and selectively suppress high-frequency noise. Experimental results on multiple datasets demonstrate the superior detection performance of MDAFNet.&lt;/p&gt;</content:encoded></item><item><title>Local Constraints Convolutional Neural Network for SAR Image Denoising and Target Configuration Recognition</title><link>https://doi.org/10.1109/taes.2025.3645154</link><guid>10.1109/taes.2025.3645154</guid><pubDate>Wed, 17 Dec 2025 18:49:24 +0000</pubDate><dc:creator>Ming Liu</dc:creator><dc:creator>Zhenning Dong</dc:creator><dc:creator>Shichao Chen</dc:creator><dc:creator>Mingliang Tao</dc:creator><dc:creator>Mengdao Xing</dc:creator><prism:publicationName>IEEE Transactions on Aerospace and Electronic Systems</prism:publicationName><prism:doi>10.1109/taes.2025.3645154</prism:doi><description>Synthetic aperture radar (SAR) target recognition is an important branch of SAR image processing. To overcome the influences of inevitable speckle noise, especially for similar configurations recognition, we propose a local constraints convolutional neural network (LC-CNN) for joint SAR image denoising and target configurations recognition. The proposed LC-CNN enhances recognition performance through a collaboratively designed of multi-task loss function. In the denoising stage, a speckle suppression loss is designed to smooth background noise whereas retaining target details. In the recognition stage, a local structure maintenance loss is designed to enhance discrimination of similar configurations by maintaining local geometric relationships. And a feature invariance loss is established to ensure core target features remain stable after denoising. Experimental results demonstrate LC-CNN's robustness under varying speckle noise levels and excellent performance in similar SAR target configurations recognition.
Published: 2025-12-17T18:49:24+00:00
Venue: IEEE Transactions on Aerospace and Electronic Systems
Score: 0.798 (must_read)</description><content:encoded>&lt;p&gt;&lt;strong&gt;Authors:&lt;/strong&gt; Ming Liu; Zhenning Dong; Shichao Chen; Mingliang Tao; Mengdao Xing&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Journal:&lt;/strong&gt; IEEE Transactions on Aerospace and Electronic Systems&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;a href="https://doi.org/10.1109/taes.2025.3645154"&gt;10.1109/taes.2025.3645154&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Relevance:&lt;/strong&gt; 0.798 (must_read)&lt;/p&gt;
&lt;p&gt;Synthetic aperture radar (SAR) target recognition is an important branch of SAR image processing. To overcome the influences of inevitable speckle noise, especially for similar configurations recognition, we propose a local constraints convolutional neural network (LC-CNN) for joint SAR image denoising and target configurations recognition. The proposed LC-CNN enhances recognition performance through a collaboratively designed of multi-task loss function. In the denoising stage, a speckle suppression loss is designed to smooth background noise whereas retaining target details. In the recognition stage, a local structure maintenance loss is designed to enhance discrimination of similar configurations by maintaining local geometric relationships. And a feature invariance loss is established to ensure core target features remain stable after denoising. Experimental results demonstrate LC-CNN&amp;#x27;s robustness under varying speckle noise levels and excellent performance in similar SAR target configurations recognition.&lt;/p&gt;</content:encoded></item><item><title>SoFlow: Solution Flow Models for One-Step Generative Modeling</title><link>https://arxiv.org/abs/2512.15657v1</link><guid>http://arxiv.org/abs/2512.15657v1</guid><pubDate>Wed, 17 Dec 2025 18:10:17 +0000</pubDate><dc:creator>Tianze Luo</dc:creator><dc:creator>Haotian Yuan</dc:creator><dc:creator>Zhuang Liu</dc:creator><dc:type>preprint</dc:type><prism:publicationName>arXiv</prism:publicationName><description>The multi-step denoising process in diffusion and Flow Matching models causes major efficiency issues, which motivates research on few-step generation. We present Solution Flow Models (SoFlow), a framework for one-step generation from scratch. By analyzing the relationship between the velocity function and the solution function of the velocity ordinary differential equation (ODE), we propose a Flow Matching loss and a solution consistency loss to train our models. The Flow Matching loss allows our models to provide estimated velocity fields for Classifier-Free Guidance (CFG) during training, which improves generation performance. Notably, our consistency loss does not require the calculation of the Jacobian-vector product (JVP), a common requirement in recent works that is not well-optimized in deep learning frameworks like PyTorch. Experimental results indicate that, when trained from scratch using the same Diffusion Transformer (DiT) architecture and an equal number of training epochs, our models achieve better FID-50K scores than MeanFlow models on the ImageNet 256x256 dataset.
Published: 2025-12-17T18:10:17+00:00
Venue: arXiv
Score: 0.797 (must_read)</description><content:encoded>&lt;p&gt;&lt;strong&gt;Authors:&lt;/strong&gt; Tianze Luo; Haotian Yuan; Zhuang Liu&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Journal:&lt;/strong&gt; arXiv&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Relevance:&lt;/strong&gt; 0.797 (must_read)&lt;/p&gt;
&lt;p&gt;The multi-step denoising process in diffusion and Flow Matching models causes major efficiency issues, which motivates research on few-step generation. We present Solution Flow Models (SoFlow), a framework for one-step generation from scratch. By analyzing the relationship between the velocity function and the solution function of the velocity ordinary differential equation (ODE), we propose a Flow Matching loss and a solution consistency loss to train our models. The Flow Matching loss allows our models to provide estimated velocity fields for Classifier-Free Guidance (CFG) during training, which improves generation performance. Notably, our consistency loss does not require the calculation of the Jacobian-vector product (JVP), a common requirement in recent works that is not well-optimized in deep learning frameworks like PyTorch. Experimental results indicate that, when trained from scratch using the same Diffusion Transformer (DiT) architecture and an equal number of training epochs, our models achieve better FID-50K scores than MeanFlow models on the ImageNet 256x256 dataset.&lt;/p&gt;</content:encoded></item><item><title>VajraV1 -- The most accurate Real Time Object Detector of the YOLO family</title><link>https://arxiv.org/abs/2512.13834v1</link><guid>http://arxiv.org/abs/2512.13834v1</guid><pubDate>Mon, 15 Dec 2025 19:16:15 +0000</pubDate><dc:creator>Naman Balbir Singh Makkar</dc:creator><dc:type>preprint</dc:type><prism:publicationName>arXiv</prism:publicationName><description>Recent years have seen significant advances in real-time object detection, with the release of YOLOv10, YOLO11, YOLOv12, and YOLOv13 between 2024 and 2025. This technical report presents the VajraV1 model architecture, which introduces architectural enhancements over existing YOLO-based detectors. VajraV1 combines effective design choices from prior YOLO models to achieve state-of-the-art accuracy among real-time object detectors while maintaining competitive inference speed.
  On the COCO validation set, VajraV1-Nano achieves 44.3% mAP, outperforming YOLOv12-N by 3.7% and YOLOv13-N by 2.7% at latency competitive with YOLOv12-N and YOLOv11-N. VajraV1-Small achieves 50.4% mAP, exceeding YOLOv12-S and YOLOv13-S by 2.4%. VajraV1-Medium achieves 52.7% mAP, outperforming YOLOv12-M by 0.2%. VajraV1-Large achieves 53.7% mAP, surpassing YOLOv13-L by 0.3%. VajraV1-Xlarge achieves 56.2% mAP, outperforming all existing real-time object detectors.
Published: 2025-12-15T19:16:15+00:00
Venue: arXiv
Score: 0.797 (must_read)</description><content:encoded>&lt;p&gt;&lt;strong&gt;Authors:&lt;/strong&gt; Naman Balbir Singh Makkar&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Journal:&lt;/strong&gt; arXiv&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Relevance:&lt;/strong&gt; 0.797 (must_read)&lt;/p&gt;
&lt;p&gt;Recent years have seen significant advances in real-time object detection, with the release of YOLOv10, YOLO11, YOLOv12, and YOLOv13 between 2024 and 2025. This technical report presents the VajraV1 model architecture, which introduces architectural enhancements over existing YOLO-based detectors. VajraV1 combines effective design choices from prior YOLO models to achieve state-of-the-art accuracy among real-time object detectors while maintaining competitive inference speed.
  On the COCO validation set, VajraV1-Nano achieves 44.3% mAP, outperforming YOLOv12-N by 3.7% and YOLOv13-N by 2.7% at latency competitive with YOLOv12-N and YOLOv11-N. VajraV1-Small achieves 50.4% mAP, exceeding YOLOv12-S and YOLOv13-S by 2.4%. VajraV1-Medium achieves 52.7% mAP, outperforming YOLOv12-M by 0.2%. VajraV1-Large achieves 53.7% mAP, surpassing YOLOv13-L by 0.3%. VajraV1-Xlarge achieves 56.2% mAP, outperforming all existing real-time object detectors.&lt;/p&gt;</content:encoded></item><item><title>Hiding Local Manipulations on SAR Images: a Counter-Forensic Attack</title><link>https://doi.org/10.1109/tip.2025.3643154</link><guid>10.1109/tip.2025.3643154</guid><pubDate>Wed, 17 Dec 2025 18:49:42 +0000</pubDate><dc:creator>Sara Mandelli</dc:creator><dc:creator>Edoardo Daniele Cannas</dc:creator><dc:creator>Paolo Bestagini</dc:creator><dc:creator>Stefano Tebaldini</dc:creator><dc:creator>Stefano Tubaro</dc:creator><prism:publicationName>IEEE Transactions on Image Processing</prism:publicationName><prism:doi>10.1109/tip.2025.3643154</prism:doi><description>The vast accessibility of Synthetic Aperture Radar (SAR) images through online portals has propelled the research across various fields. This widespread use and easy availability have unfortunately made SAR data susceptible to malicious alterations, such as local editing applied to the images for inserting or covering the presence of sensitive targets. To contrast malicious manipulations, in the last years the forensic community has begun to dig into the SAR manipulation issue, proposing detectors that effectively localize the tampering traces in amplitude images. Nonetheless, in this paper we demonstrate that an expert practitioner can exploit the complex nature of SAR data to obscure any signs of manipulation within a locally altered amplitude image. We refer to this approach as a counter-forensic attack. To achieve the concealment of manipulation traces, the attacker can simulate a re-acquisition of the manipulated scene by the SAR system that initially generated the pristine image. In doing so, the attacker can obscure any evidence of manipulation, making it appear as if the image was legitimately produced by the system. This attack has unique features that make it both highly generalizable and relatively easy to apply. First, it is a black-box attack, meaning it is not designed to deceive a specific forensic detector. Furthermore, it does not require a training phase and is not based on adversarial operations. We assess the effectiveness of the proposed counter-forensic approach across diverse scenarios, examining various manipulation operations. The obtained results indicate that our devised attack successfully eliminates traces of manipulation, deceiving even the most advanced forensic detectors.
Published: 2025-12-17T18:49:42+00:00
Venue: IEEE Transactions on Image Processing
Score: 0.793 (must_read)</description><content:encoded>&lt;p&gt;&lt;strong&gt;Authors:&lt;/strong&gt; Sara Mandelli; Edoardo Daniele Cannas; Paolo Bestagini; Stefano Tebaldini; Stefano Tubaro&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Journal:&lt;/strong&gt; IEEE Transactions on Image Processing&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;a href="https://doi.org/10.1109/tip.2025.3643154"&gt;10.1109/tip.2025.3643154&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Relevance:&lt;/strong&gt; 0.793 (must_read)&lt;/p&gt;
&lt;p&gt;The vast accessibility of Synthetic Aperture Radar (SAR) images through online portals has propelled the research across various fields. This widespread use and easy availability have unfortunately made SAR data susceptible to malicious alterations, such as local editing applied to the images for inserting or covering the presence of sensitive targets. To contrast malicious manipulations, in the last years the forensic community has begun to dig into the SAR manipulation issue, proposing detectors that effectively localize the tampering traces in amplitude images. Nonetheless, in this paper we demonstrate that an expert practitioner can exploit the complex nature of SAR data to obscure any signs of manipulation within a locally altered amplitude image. We refer to this approach as a counter-forensic attack. To achieve the concealment of manipulation traces, the attacker can simulate a re-acquisition of the manipulated scene by the SAR system that initially generated the pristine image. In doing so, the attacker can obscure any evidence of manipulation, making it appear as if the image was legitimately produced by the system. This attack has unique features that make it both highly generalizable and relatively easy to apply. First, it is a black-box attack, meaning it is not designed to deceive a specific forensic detector. Furthermore, it does not require a training phase and is not based on adversarial operations. We assess the effectiveness of the proposed counter-forensic approach across diverse scenarios, examining various manipulation operations. The obtained results indicate that our devised attack successfully eliminates traces of manipulation, deceiving even the most advanced forensic detectors.&lt;/p&gt;</content:encoded></item><item><title>PAENet: A Part-Aware Attention Enhancement Network for SAR Ship Recognition</title><link>https://doi.org/10.1109/lgrs.2025.3645661</link><guid>10.1109/lgrs.2025.3645661</guid><pubDate>Wed, 17 Dec 2025 18:50:17 +0000</pubDate><dc:creator>Xueli Pan</dc:creator><dc:creator>Shuochen Zhang</dc:creator><dc:creator>Mingbo Han</dc:creator><dc:creator>Guisheng Liao</dc:creator><dc:creator>Lixia Yang</dc:creator><dc:creator>Yingsong Li</dc:creator><dc:creator>Donghai Ren</dc:creator><prism:publicationName>IEEE Geoscience and Remote Sensing Letters</prism:publicationName><prism:doi>10.1109/lgrs.2025.3645661</prism:doi><description>Synthetic aperture radar (SAR) ship recognition plays a critical role in maritime security. Unlike optical imagery, SAR ship recognition faces inherent challenges mainly due to its imaging mechanism and the structural characteristics of ships, especially concerning inter-class similarities and intra-class variations. Fine-grained information allows the network to focus on discriminative scattering substructures, thereby facilitating more effective ship recognition. For this reason, this letter introduces PAENet—a novel SAR ship recognition network that incorporates local part-aware attention enhancement. PAENet constructs part-level scattering representations using scattering keypoints, allowing for adaptive concentration on fine-grained features. Importantly, a dedicated part information attention module (PIAM) is introduced to refine distinctive features based on the cosine similarity measure. Extensive evaluations on the standardized public OpenSARShip datastet with three-category and six-category ships demonstrate that the proposed network achieves state-of-the-art performance in SAR ship recognition.
Published: 2025-12-17T18:50:17+00:00
Venue: IEEE Geoscience and Remote Sensing Letters
Score: 0.793 (must_read)</description><content:encoded>&lt;p&gt;&lt;strong&gt;Authors:&lt;/strong&gt; Xueli Pan; Shuochen Zhang; Mingbo Han; Guisheng Liao; Lixia Yang; Yingsong Li; Donghai Ren&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Journal:&lt;/strong&gt; IEEE Geoscience and Remote Sensing Letters&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;a href="https://doi.org/10.1109/lgrs.2025.3645661"&gt;10.1109/lgrs.2025.3645661&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Relevance:&lt;/strong&gt; 0.793 (must_read)&lt;/p&gt;
&lt;p&gt;Synthetic aperture radar (SAR) ship recognition plays a critical role in maritime security. Unlike optical imagery, SAR ship recognition faces inherent challenges mainly due to its imaging mechanism and the structural characteristics of ships, especially concerning inter-class similarities and intra-class variations. Fine-grained information allows the network to focus on discriminative scattering substructures, thereby facilitating more effective ship recognition. For this reason, this letter introduces PAENet—a novel SAR ship recognition network that incorporates local part-aware attention enhancement. PAENet constructs part-level scattering representations using scattering keypoints, allowing for adaptive concentration on fine-grained features. Importantly, a dedicated part information attention module (PIAM) is introduced to refine distinctive features based on the cosine similarity measure. Extensive evaluations on the standardized public OpenSARShip datastet with three-category and six-category ships demonstrate that the proposed network achieves state-of-the-art performance in SAR ship recognition.&lt;/p&gt;</content:encoded></item><item><title>Cross-Level Sensor Fusion with Object Lists via Transformer for 3D Object Detection</title><link>https://arxiv.org/abs/2512.12884v1</link><guid>http://arxiv.org/abs/2512.12884v1</guid><pubDate>Sun, 14 Dec 2025 23:56:16 +0000</pubDate><dc:creator>Xiangzhong Liu</dc:creator><dc:creator>Jiajie Zhang</dc:creator><dc:creator>Hao Shen</dc:creator><dc:type>preprint</dc:type><prism:publicationName>arXiv</prism:publicationName><prism:doi>10.1109/IV64158.2025.11097627</prism:doi><description>In automotive sensor fusion systems, smart sensors and Vehicle-to-Everything (V2X) modules are commonly utilized. Sensor data from these systems are typically available only as processed object lists rather than raw sensor data from traditional sensors. Instead of processing other raw data separately and then fusing them at the object level, we propose an end-to-end cross-level fusion concept with Transformer, which integrates highly abstract object list information with raw camera images for 3D object detection. Object lists are fed into a Transformer as denoising queries and propagated together with learnable queries through the latter feature aggregation process. Additionally, a deformable Gaussian mask, derived from the positional and size dimensional priors from the object lists, is explicitly integrated into the Transformer decoder. This directs attention toward the target area of interest and accelerates model training convergence. Furthermore, as there is no public dataset containing object lists as a standalone modality, we propose an approach to generate pseudo object lists from ground-truth bounding boxes by simulating state noise and false positives and negatives. As the first work to conduct cross-level fusion, our approach shows substantial performance improvements over the vision-based baseline on the nuScenes dataset. It demonstrates its generalization capability over diverse noise levels of simulated object lists and real detectors.
Published: 2025-12-14T23:56:16+00:00
Venue: arXiv
Score: 0.792 (must_read)</description><content:encoded>&lt;p&gt;&lt;strong&gt;Authors:&lt;/strong&gt; Xiangzhong Liu; Jiajie Zhang; Hao Shen&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Journal:&lt;/strong&gt; arXiv&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;a href="https://doi.org/10.1109/IV64158.2025.11097627"&gt;10.1109/IV64158.2025.11097627&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Relevance:&lt;/strong&gt; 0.792 (must_read)&lt;/p&gt;
&lt;p&gt;In automotive sensor fusion systems, smart sensors and Vehicle-to-Everything (V2X) modules are commonly utilized. Sensor data from these systems are typically available only as processed object lists rather than raw sensor data from traditional sensors. Instead of processing other raw data separately and then fusing them at the object level, we propose an end-to-end cross-level fusion concept with Transformer, which integrates highly abstract object list information with raw camera images for 3D object detection. Object lists are fed into a Transformer as denoising queries and propagated together with learnable queries through the latter feature aggregation process. Additionally, a deformable Gaussian mask, derived from the positional and size dimensional priors from the object lists, is explicitly integrated into the Transformer decoder. This directs attention toward the target area of interest and accelerates model training convergence. Furthermore, as there is no public dataset containing object lists as a standalone modality, we propose an approach to generate pseudo object lists from ground-truth bounding boxes by simulating state noise and false positives and negatives. As the first work to conduct cross-level fusion, our approach shows substantial performance improvements over the vision-based baseline on the nuScenes dataset. It demonstrates its generalization capability over diverse noise levels of simulated object lists and real detectors.&lt;/p&gt;</content:encoded></item><item><title>Weakly and Single-Frame Supervised Temporal Sentence Grounding With Gaussian-Based Contrastive Proposal Learning</title><link>https://doi.org/10.1109/tpami.2025.3644900</link><guid>10.1109/tpami.2025.3644900</guid><pubDate>Wed, 17 Dec 2025 18:46:02 +0000</pubDate><dc:creator>Minghang Zheng</dc:creator><dc:creator>Yanjie Huang</dc:creator><dc:creator>Qingchao Chen</dc:creator><dc:creator>Yuxin Peng</dc:creator><dc:creator>Yang Liu</dc:creator><prism:publicationName>IEEE Transactions on Pattern Analysis and Machine Intelligence</prism:publicationName><prism:doi>10.1109/tpami.2025.3644900</prism:doi><description>Temporal sentence grounding aims to localize moments corresponding to language queries from videos. As labeling the temporal boundaries is cumbersome and subjective, the weakly-supervised methods have received increasing attention. Most of the existing weakly-supervised methods generate proposals by sliding windows, which are content-independent and of low quality. Besides, they train their model to distinguish positive visual-language pairs from negative ones randomly collected from other videos, ignoring the highly confusing video segments within the same video. In this paper, we propose Contrastive Proposal Learning (CPL) to overcome the above limitations. Specifically, we use multiple learnable asymmetric Gaussian functions to generate both positive and negative proposals within the same video. Then, we propose a controllable, easy-to-hard negative proposal mining strategy to collect negative samples within the same video, which enables CPL to distinguish highly confusing scenes. Finally, we propose an extension of the proposal generation algorithm to explore the use of low-cost single-frame annotation and achieve a balance between annotation burden and grounding performance. Our CPL can be applied to both MIL-based and reconstruction-based mainstream frameworks and achieves state-of-the-art performance on Charades-STA, ActivityNet Captions, and DiDeMo datasets.
Published: 2025-12-17T18:46:02+00:00
Venue: IEEE Transactions on Pattern Analysis and Machine Intelligence
Score: 0.792 (must_read)</description><content:encoded>&lt;p&gt;&lt;strong&gt;Authors:&lt;/strong&gt; Minghang Zheng; Yanjie Huang; Qingchao Chen; Yuxin Peng; Yang Liu&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Journal:&lt;/strong&gt; IEEE Transactions on Pattern Analysis and Machine Intelligence&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;a href="https://doi.org/10.1109/tpami.2025.3644900"&gt;10.1109/tpami.2025.3644900&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Relevance:&lt;/strong&gt; 0.792 (must_read)&lt;/p&gt;
&lt;p&gt;Temporal sentence grounding aims to localize moments corresponding to language queries from videos. As labeling the temporal boundaries is cumbersome and subjective, the weakly-supervised methods have received increasing attention. Most of the existing weakly-supervised methods generate proposals by sliding windows, which are content-independent and of low quality. Besides, they train their model to distinguish positive visual-language pairs from negative ones randomly collected from other videos, ignoring the highly confusing video segments within the same video. In this paper, we propose Contrastive Proposal Learning (CPL) to overcome the above limitations. Specifically, we use multiple learnable asymmetric Gaussian functions to generate both positive and negative proposals within the same video. Then, we propose a controllable, easy-to-hard negative proposal mining strategy to collect negative samples within the same video, which enables CPL to distinguish highly confusing scenes. Finally, we propose an extension of the proposal generation algorithm to explore the use of low-cost single-frame annotation and achieve a balance between annotation burden and grounding performance. Our CPL can be applied to both MIL-based and reconstruction-based mainstream frameworks and achieves state-of-the-art performance on Charades-STA, ActivityNet Captions, and DiDeMo datasets.&lt;/p&gt;</content:encoded></item><item><title>Lightweight framework for underground pipeline recognition and spatial localization based on multi-view 2D GPR images</title><link>https://doi.org/10.1109/tgrs.2025.3645032</link><guid>10.1109/tgrs.2025.3645032</guid><pubDate>Wed, 17 Dec 2025 18:46:07 +0000</pubDate><dc:creator>Haotian Lv</dc:creator><dc:creator>Chao Li</dc:creator><dc:creator>Jiangbo Dai</dc:creator><dc:creator>Yuhui Zhang</dc:creator><dc:creator>Zepeng Fan</dc:creator><dc:creator>Yiqiu Tan</dc:creator><dc:creator>Dawei Wang</dc:creator><dc:creator>Binglei Xie</dc:creator><prism:publicationName>IEEE Transactions on Geoscience and Remote Sensing</prism:publicationName><prism:doi>10.1109/tgrs.2025.3645032</prism:doi><description>To address the issues of weak correlation between multi-view features, low recognition accuracy of small-scale targets, and insufficient robustness in complex scenarios in underground pipeline detection using three-dimensional ground penetrating radar (3D GPR), this paper proposes a three-dimensional pipeline intelligent detection framework that integrates multi-strategy improved deep learning technology. This paper explores a novel pathway to achieve accurate 3D localization through lightweight joint analysis of multi-view 2D GPR images. First, based on a B/C/D-Scan three-view joint analysis strategy, a three-dimensional pipeline three-view feature evaluation method is established by cross-validating forward simulation results obtained using time-domain finite difference (FDTD) methods with actual measurement data. Second, the DCO-YOLO framework is proposed, which integrates DySample dynamic upsampling, Convolutional gate linear unit (CGLU), and OutlookAttention cross-dimensional correlation mechanisms into the original YOLOv11 algorithm, significantly improving the small-scale pipeline edge feature extraction capability. Furthermore, a 3D-DIoU spatial feature matching algorithm is proposed, which integrates three-dimensional geometric constraints and center distance penalty terms to achieve automated association of multi-view annotations. The three-view fusion strategy resolves inherent ambiguities in single-view detection. Experiments based on 100 kilometers of real urban underground pipeline data show that the proposed method achieves accuracy, recall, and mean average precision of 96.2%, 93.3%, and 96.7%, respectively, in complex multi-pipeline scenarios, which are 2.0%, 2.1%, and 0.9% higher than the baseline model. Ablation experiments validated the synergistic optimization effect of the dynamic feature enhancement module, and Grad-CAM++ heatmap visualization demonstrated that the improved model significantly enhanced its ability to focus on pipeline geometri...
Published: 2025-12-17T18:46:07+00:00
Venue: IEEE Transactions on Geoscience and Remote Sensing
Score: 0.790 (must_read)</description><content:encoded>&lt;p&gt;&lt;strong&gt;Authors:&lt;/strong&gt; Haotian Lv; Chao Li; Jiangbo Dai; Yuhui Zhang; Zepeng Fan; Yiqiu Tan; Dawei Wang; Binglei Xie&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Journal:&lt;/strong&gt; IEEE Transactions on Geoscience and Remote Sensing&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;a href="https://doi.org/10.1109/tgrs.2025.3645032"&gt;10.1109/tgrs.2025.3645032&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Relevance:&lt;/strong&gt; 0.790 (must_read)&lt;/p&gt;
&lt;p&gt;To address the issues of weak correlation between multi-view features, low recognition accuracy of small-scale targets, and insufficient robustness in complex scenarios in underground pipeline detection using three-dimensional ground penetrating radar (3D GPR), this paper proposes a three-dimensional pipeline intelligent detection framework that integrates multi-strategy improved deep learning technology. This paper explores a novel pathway to achieve accurate 3D localization through lightweight joint analysis of multi-view 2D GPR images. First, based on a B/C/D-Scan three-view joint analysis strategy, a three-dimensional pipeline three-view feature evaluation method is established by cross-validating forward simulation results obtained using time-domain finite difference (FDTD) methods with actual measurement data. Second, the DCO-YOLO framework is proposed, which integrates DySample dynamic upsampling, Convolutional gate linear unit (CGLU), and OutlookAttention cross-dimensional correlation mechanisms into the original YOLOv11 algorithm, significantly improving the small-scale pipeline edge feature extraction capability. Furthermore, a 3D-DIoU spatial feature matching algorithm is proposed, which integrates three-dimensional geometric constraints and center distance penalty terms to achieve automated association of multi-view annotations. The three-view fusion strategy resolves inherent ambiguities in single-view detection. Experiments based on 100 kilometers of real urban underground pipeline data show that the proposed method achieves accuracy, recall, and mean average precision of 96.2%, 93.3%, and 96.7%, respectively, in complex multi-pipeline scenarios, which are 2.0%, 2.1%, and 0.9% higher than the baseline model. Ablation experiments validated the synergistic optimization effect of the dynamic feature enhancement module, and Grad-CAM++ heatmap visualization demonstrated that the improved model significantly enhanced its ability to focus on pipeline geometri...&lt;/p&gt;</content:encoded></item><item><title>SuperCLIP: CLIP with Simple Classification Supervision</title><link>https://arxiv.org/abs/2512.14480v1</link><guid>http://arxiv.org/abs/2512.14480v1</guid><pubDate>Tue, 16 Dec 2025 15:11:53 +0000</pubDate><dc:creator>Weiheng Zhao</dc:creator><dc:creator>Zilong Huang</dc:creator><dc:creator>Jiashi Feng</dc:creator><dc:creator>Xinggang Wang</dc:creator><dc:type>preprint</dc:type><prism:publicationName>arXiv</prism:publicationName><description>Contrastive Language-Image Pretraining (CLIP) achieves strong generalization in vision-language tasks by aligning images and texts in a shared embedding space. However, recent findings show that CLIP-like models still underutilize fine-grained semantic signals in text, and this issue becomes even more pronounced when dealing with long and detailed captions. This stems from CLIP's training objective, which optimizes only global image-text similarity and overlooks token-level supervision - limiting its ability to achieve fine-grained visual-text alignment. To address this, we propose SuperCLIP, a simple yet effective framework that augments contrastive learning with classification-based supervision. By adding only a lightweight linear layer to the vision encoder, SuperCLIP leverages token-level cues to enhance visual-textual alignment - with just a 0.077% increase in total FLOPs, and no need for additional annotated data. Experiments show that SuperCLIP consistently improves zero-shot classification, image-text retrieval, and purely visual tasks. These gains hold regardless of whether the model is trained on original web data or rich re-captioned data, demonstrating SuperCLIP's ability to recover textual supervision in both cases. Furthermore, SuperCLIP alleviates CLIP's small-batch performance drop through classification-based supervision that avoids reliance on large batch sizes. Code and models will be made open source.
Published: 2025-12-16T15:11:53+00:00
Venue: arXiv
Score: 0.789 (must_read)</description><content:encoded>&lt;p&gt;&lt;strong&gt;Authors:&lt;/strong&gt; Weiheng Zhao; Zilong Huang; Jiashi Feng; Xinggang Wang&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Journal:&lt;/strong&gt; arXiv&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Relevance:&lt;/strong&gt; 0.789 (must_read)&lt;/p&gt;
&lt;p&gt;Contrastive Language-Image Pretraining (CLIP) achieves strong generalization in vision-language tasks by aligning images and texts in a shared embedding space. However, recent findings show that CLIP-like models still underutilize fine-grained semantic signals in text, and this issue becomes even more pronounced when dealing with long and detailed captions. This stems from CLIP&amp;#x27;s training objective, which optimizes only global image-text similarity and overlooks token-level supervision - limiting its ability to achieve fine-grained visual-text alignment. To address this, we propose SuperCLIP, a simple yet effective framework that augments contrastive learning with classification-based supervision. By adding only a lightweight linear layer to the vision encoder, SuperCLIP leverages token-level cues to enhance visual-textual alignment - with just a 0.077% increase in total FLOPs, and no need for additional annotated data. Experiments show that SuperCLIP consistently improves zero-shot classification, image-text retrieval, and purely visual tasks. These gains hold regardless of whether the model is trained on original web data or rich re-captioned data, demonstrating SuperCLIP&amp;#x27;s ability to recover textual supervision in both cases. Furthermore, SuperCLIP alleviates CLIP&amp;#x27;s small-batch performance drop through classification-based supervision that avoids reliance on large batch sizes. Code and models will be made open source.&lt;/p&gt;</content:encoded></item><item><title>LLM-Informed Global-Local Contextualization for Zero-Shot Food Detection</title><link>https://doi.org/10.1016/j.patcog.2025.112928</link><guid>10.1016/j.patcog.2025.112928</guid><pubDate>Tue, 16 Dec 2025 00:05:37 +0000</pubDate><dc:creator>Xinlong Wang</dc:creator><dc:creator>Weiqing Min</dc:creator><dc:creator>Guorui Sheng</dc:creator><dc:creator>Jingru Song</dc:creator><dc:creator>Yancun Yang</dc:creator><dc:creator>Tao Yao</dc:creator><dc:creator>Shuqiang Jiang</dc:creator><prism:publicationName>Pattern Recognition</prism:publicationName><prism:doi>10.1016/j.patcog.2025.112928</prism:doi><description>Zero-Shot Detection, the ability to detect novel objects without training samples, exhibits immense potential in an ever-changing world, particularly in scenarios requiring the identification of emerging categories. However, effectively applying ZSD to fine-grained domains, characterized by high inter-class similarity and notable intra-class diversity, remains a significant challenge. This is particularly pronounced in the food domain, where the intricate nature of food attributes—notably the pervasive visual ambiguity among related culinary categories and the extensive spectrum of appearances within each food category—severely constrains the performance of existing methods. To address these specific challenges in the food domain, we introduce Zero-Shot Food Detection with Semantic Space and Feature Fusion (ZeSF), a novel framework tailored for Zero-Shot Food Detection. ZeSF integrates two key modules: (1) Multi-Scale Context Integration Module (MSCIM) that employs dilated convolutions for hierarchical feature extraction and adaptive multi-scale fusion to capture subtle, fine-grained visual distinctions; and (2) Contextual Text Feature Enhancement Module (CTFEM) that leverages Large Language Models to generate semantically rich textual embeddings, encompassing both global attributes and discriminative local descriptors. Critically, a cross-modal alignment further harmonizes visual and textual features. Comprehensive evaluations on the UEC FOOD 256 and Food Objects With Attributes (FOWA) datasets affirm ZeSF’s superiority, achieving significant improvements in the Harmonic Mean for the Generalized ZSD setting. Crucially, we further validate the framework’s generalization capability on the MS COCO and PASCAL VOC benchmarks, where it again outperforms strong baselines. The source code will be publicly available upon publication.
Published: 2025-12-16T00:05:37+00:00
Venue: Pattern Recognition
Score: 0.789 (must_read)</description><content:encoded>&lt;p&gt;&lt;strong&gt;Authors:&lt;/strong&gt; Xinlong Wang; Weiqing Min; Guorui Sheng; Jingru Song; Yancun Yang; Tao Yao; Shuqiang Jiang&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Journal:&lt;/strong&gt; Pattern Recognition&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;a href="https://doi.org/10.1016/j.patcog.2025.112928"&gt;10.1016/j.patcog.2025.112928&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Relevance:&lt;/strong&gt; 0.789 (must_read)&lt;/p&gt;
&lt;p&gt;Zero-Shot Detection, the ability to detect novel objects without training samples, exhibits immense potential in an ever-changing world, particularly in scenarios requiring the identification of emerging categories. However, effectively applying ZSD to fine-grained domains, characterized by high inter-class similarity and notable intra-class diversity, remains a significant challenge. This is particularly pronounced in the food domain, where the intricate nature of food attributes—notably the pervasive visual ambiguity among related culinary categories and the extensive spectrum of appearances within each food category—severely constrains the performance of existing methods. To address these specific challenges in the food domain, we introduce Zero-Shot Food Detection with Semantic Space and Feature Fusion (ZeSF), a novel framework tailored for Zero-Shot Food Detection. ZeSF integrates two key modules: (1) Multi-Scale Context Integration Module (MSCIM) that employs dilated convolutions for hierarchical feature extraction and adaptive multi-scale fusion to capture subtle, fine-grained visual distinctions; and (2) Contextual Text Feature Enhancement Module (CTFEM) that leverages Large Language Models to generate semantically rich textual embeddings, encompassing both global attributes and discriminative local descriptors. Critically, a cross-modal alignment further harmonizes visual and textual features. Comprehensive evaluations on the UEC FOOD 256 and Food Objects With Attributes (FOWA) datasets affirm ZeSF’s superiority, achieving significant improvements in the Harmonic Mean for the Generalized ZSD setting. Crucially, we further validate the framework’s generalization capability on the MS COCO and PASCAL VOC benchmarks, where it again outperforms strong baselines. The source code will be publicly available upon publication.&lt;/p&gt;</content:encoded></item><item><title>Multimodal Fusion with Vision-Language-Action Models for Robotic Manipulation: A Systematic Review</title><link>https://doi.org/10.1016/j.inffus.2025.104062</link><guid>10.1016/j.inffus.2025.104062</guid><pubDate>Tue, 16 Dec 2025 16:16:13 +0000</pubDate><dc:creator>Muhayy Ud Din</dc:creator><dc:creator>Waseem Akram</dc:creator><dc:creator>Lyes Saad Saoud</dc:creator><dc:creator>Jan Rosell</dc:creator><dc:creator>Irfan Hussain</dc:creator><prism:publicationName>Information Fusion</prism:publicationName><prism:doi>10.1016/j.inffus.2025.104062</prism:doi><description>Vision Language Action (VLA) models represent a new frontier in robotics by unifying perception, reasoning, and control within a single multimodal learning framework. By integrating visual, linguistic, and action modalities, they enable multimodal fusion systems designed for instruction-driven manipulation and generalist autonomy. This systematic review synthesizes the state of the art in VLA research with an emphasis on architectures, algorithms, and applications relevant to robotic manipulation. We examine 102 models, 26 foundational datasets, and 12 simulation platforms, categorizing them according to their fusion strategies and integration mechanisms. Foundational datasets are evaluated using a novel criterion based on task complexity, modality richness, and dataset scale, allowing a comparative analysis of their suitability for generalist policy learning. We further introduce a structured taxonomy of fusion hierarchies and encoder-decoder families, together with a two-dimensional dataset characterization framework and a meta-analytic benchmarking protocol that quantitatively links design variables to empirical performance across benchmarks. Our analysis shows that hierarchical and late fusion architectures achieve the highest manipulation success and generalization, confirming the benefit of multi-level cross-modal integration. Diffusion-based decoders demonstrate superior cross-domain transfer and robustness compared to autoregressive heads. Dataset analysis highlights a persistent lack of benchmarks that combine high-complexity, multimodal, and long-horizon tasks, while existing simulators offer limited multimodal synchronization and real-to-sim consistency. To address these gaps, we propose the VLA Fusion Evaluation Benchmark to quantify fusion efficiency and alignment. Drawing on both academic and industrial advances, the review outlines future research directions in adaptive and modular fusion architectures, computational resource optimization, and the deployment of interpretable, resource-efficient robotic systems. We further propose a forward-looking agentic VLA paradigm where LLM planners integrate VLA skills as verifiable tools within a closed feedback loop for adaptive and self-improving robotic control. This work provides both a conceptual foundation and a quantitative roadmap for advancing embodied intelligence through multimodal information fusion across robotic domains.
Published: 2025-12-16T16:16:13+00:00
Venue: Information Fusion
Score: 0.788 (must_read)</description><content:encoded>&lt;p&gt;&lt;strong&gt;Authors:&lt;/strong&gt; Muhayy Ud Din; Waseem Akram; Lyes Saad Saoud; Jan Rosell; Irfan Hussain&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Journal:&lt;/strong&gt; Information Fusion&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;a href="https://doi.org/10.1016/j.inffus.2025.104062"&gt;10.1016/j.inffus.2025.104062&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Relevance:&lt;/strong&gt; 0.788 (must_read)&lt;/p&gt;
&lt;p&gt;Vision Language Action (VLA) models represent a new frontier in robotics by unifying perception, reasoning, and control within a single multimodal learning framework. By integrating visual, linguistic, and action modalities, they enable multimodal fusion systems designed for instruction-driven manipulation and generalist autonomy. This systematic review synthesizes the state of the art in VLA research with an emphasis on architectures, algorithms, and applications relevant to robotic manipulation. We examine 102 models, 26 foundational datasets, and 12 simulation platforms, categorizing them according to their fusion strategies and integration mechanisms. Foundational datasets are evaluated using a novel criterion based on task complexity, modality richness, and dataset scale, allowing a comparative analysis of their suitability for generalist policy learning. We further introduce a structured taxonomy of fusion hierarchies and encoder-decoder families, together with a two-dimensional dataset characterization framework and a meta-analytic benchmarking protocol that quantitatively links design variables to empirical performance across benchmarks. Our analysis shows that hierarchical and late fusion architectures achieve the highest manipulation success and generalization, confirming the benefit of multi-level cross-modal integration. Diffusion-based decoders demonstrate superior cross-domain transfer and robustness compared to autoregressive heads. Dataset analysis highlights a persistent lack of benchmarks that combine high-complexity, multimodal, and long-horizon tasks, while existing simulators offer limited multimodal synchronization and real-to-sim consistency. To address these gaps, we propose the VLA Fusion Evaluation Benchmark to quantify fusion efficiency and alignment. Drawing on both academic and industrial advances, the review outlines future research directions in adaptive and modular fusion architectures, computational resource optimization, and the deployment of interpretable, resource-efficient robotic systems. We further propose a forward-looking agentic VLA paradigm where LLM planners integrate VLA skills as verifiable tools within a closed feedback loop for adaptive and self-improving robotic control. This work provides both a conceptual foundation and a quantitative roadmap for advancing embodied intelligence through multimodal information fusion across robotic domains.&lt;/p&gt;</content:encoded></item><item><title>FA-Net: A Feature Alignment Network for Video-based Visible-Infrared Person Re-Identification</title><link>https://doi.org/10.1109/tip.2025.3642633</link><guid>10.1109/tip.2025.3642633</guid><pubDate>Wed, 17 Dec 2025 18:49:42 +0000</pubDate><dc:creator>Xi Yang</dc:creator><dc:creator>Wenjiao Dong</dc:creator><dc:creator>Xian Wang</dc:creator><dc:creator>De Cheng</dc:creator><dc:creator>Nannan Wang</dc:creator><prism:publicationName>IEEE Transactions on Image Processing</prism:publicationName><prism:doi>10.1109/tip.2025.3642633</prism:doi><description>Video-based visible-infrared person re-identification (VVI-ReID) aims to match target pedestrians between visible and infrared videos, which is significantly applied in 24-hour surveillance systems. The key of VVI-ReID is to learn modality invariant and spatio-temporal invariant sequence-level representation to solve the challenges such as modality differences, spatio-temporal misalignment, and domain shift noise. However, existing methods predominantly emphasize on reducing modality discrepancy while relatively neglect temporal misalignment and domain shift noise reduction. To this end, this paper proposes a VVI-ReID framework called Feature Alignment Network (FA-Net) from the perspective of feature alignment, aiming to mitigate temporal misalignment. FA-Net comprises two main alignment modules: Spatial-Temporal Alignment Module (STAM) and Modality Distribution Constraint (MDC). STAM integrates global and local features to ensure individuals’ spatial representation alignment. Additionally, STAM also establishes temporal relationships by exploring inter-frame features to address cross-frame person feature matching. Furthermore, we introduce the Modality Distribution Constraint (MDC), which utilizes a symmetric distribution loss to align the distributions of features from different modalities. Besides, the SAM Guidance Augmentation (SAM-GA) strategy is designed to transform the image space of RGB and IR frames to provide more informative and less noisy frame information. Extensive experimental results demonstrate the effectiveness of the proposed method, surpassing existing state-of-the-art methods. Our code will be available at: https://github.com/code/FANet.
Published: 2025-12-17T18:49:42+00:00
Venue: IEEE Transactions on Image Processing
Score: 0.788 (must_read)</description><content:encoded>&lt;p&gt;&lt;strong&gt;Authors:&lt;/strong&gt; Xi Yang; Wenjiao Dong; Xian Wang; De Cheng; Nannan Wang&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Journal:&lt;/strong&gt; IEEE Transactions on Image Processing&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;a href="https://doi.org/10.1109/tip.2025.3642633"&gt;10.1109/tip.2025.3642633&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Relevance:&lt;/strong&gt; 0.788 (must_read)&lt;/p&gt;
&lt;p&gt;Video-based visible-infrared person re-identification (VVI-ReID) aims to match target pedestrians between visible and infrared videos, which is significantly applied in 24-hour surveillance systems. The key of VVI-ReID is to learn modality invariant and spatio-temporal invariant sequence-level representation to solve the challenges such as modality differences, spatio-temporal misalignment, and domain shift noise. However, existing methods predominantly emphasize on reducing modality discrepancy while relatively neglect temporal misalignment and domain shift noise reduction. To this end, this paper proposes a VVI-ReID framework called Feature Alignment Network (FA-Net) from the perspective of feature alignment, aiming to mitigate temporal misalignment. FA-Net comprises two main alignment modules: Spatial-Temporal Alignment Module (STAM) and Modality Distribution Constraint (MDC). STAM integrates global and local features to ensure individuals’ spatial representation alignment. Additionally, STAM also establishes temporal relationships by exploring inter-frame features to address cross-frame person feature matching. Furthermore, we introduce the Modality Distribution Constraint (MDC), which utilizes a symmetric distribution loss to align the distributions of features from different modalities. Besides, the SAM Guidance Augmentation (SAM-GA) strategy is designed to transform the image space of RGB and IR frames to provide more informative and less noisy frame information. Extensive experimental results demonstrate the effectiveness of the proposed method, surpassing existing state-of-the-art methods. Our code will be available at: https://github.com/code/FANet.&lt;/p&gt;</content:encoded></item><item><title>Learning Quaternion Convolutional Neural Networks for PolSAR Target Recognition</title><link>https://doi.org/10.1109/taes.2025.3645163</link><guid>10.1109/taes.2025.3645163</guid><pubDate>Wed, 17 Dec 2025 18:49:24 +0000</pubDate><dc:creator>Huiping Lin</dc:creator><dc:creator>Junjun Yin</dc:creator><dc:creator>Jian Yang</dc:creator><prism:publicationName>IEEE Transactions on Aerospace and Electronic Systems</prism:publicationName><prism:doi>10.1109/taes.2025.3645163</prism:doi><description>Polarization offers rich information for enhancing target recognition in synthetic aperture radar (SAR) imagery. However, most existing SAR target recognition methods rely on single-channel data, and the potential of multi-channel polarimetric images remains underexplored. In this paper, we propose an end to-end target recognition framework for polarimetric SAR (Pol SAR) images based on a quaternion convolutional neural network (QCNN) operating in the Poincare sphere parameter domain. The QCNN is constructed with a sequence of quaternion operation layers and incorporates a specialized loss function designed for quaternion-valued representations. To address the mismatch problem in the quaternion field, we introduce quaternion maximum pooling (QuatMaxPool) and quaternion average pooling (QuatAvgPool) operations. To the best of our knowledge, this is the first QCNN developed for PolSAR target recognition. Experiments on both simulated and real datasets demonstrate that the proposed QCNN achieves recognition performance comparable to state-of-the-art real- and complex-valued models while requiring significantly fewer parameters and offering enhanced physical interpretability, thereby validating the effectiveness and superiority of the proposed approach
Published: 2025-12-17T18:49:24+00:00
Venue: IEEE Transactions on Aerospace and Electronic Systems
Score: 0.788 (must_read)</description><content:encoded>&lt;p&gt;&lt;strong&gt;Authors:&lt;/strong&gt; Huiping Lin; Junjun Yin; Jian Yang&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Journal:&lt;/strong&gt; IEEE Transactions on Aerospace and Electronic Systems&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;a href="https://doi.org/10.1109/taes.2025.3645163"&gt;10.1109/taes.2025.3645163&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Relevance:&lt;/strong&gt; 0.788 (must_read)&lt;/p&gt;
&lt;p&gt;Polarization offers rich information for enhancing target recognition in synthetic aperture radar (SAR) imagery. However, most existing SAR target recognition methods rely on single-channel data, and the potential of multi-channel polarimetric images remains underexplored. In this paper, we propose an end to-end target recognition framework for polarimetric SAR (Pol SAR) images based on a quaternion convolutional neural network (QCNN) operating in the Poincare sphere parameter domain. The QCNN is constructed with a sequence of quaternion operation layers and incorporates a specialized loss function designed for quaternion-valued representations. To address the mismatch problem in the quaternion field, we introduce quaternion maximum pooling (QuatMaxPool) and quaternion average pooling (QuatAvgPool) operations. To the best of our knowledge, this is the first QCNN developed for PolSAR target recognition. Experiments on both simulated and real datasets demonstrate that the proposed QCNN achieves recognition performance comparable to state-of-the-art real- and complex-valued models while requiring significantly fewer parameters and offering enhanced physical interpretability, thereby validating the effectiveness and superiority of the proposed approach&lt;/p&gt;</content:encoded></item><item><title>4D-RaDiff: Latent Diffusion for 4D Radar Point Cloud Generation</title><link>https://arxiv.org/abs/2512.14235v1</link><guid>http://arxiv.org/abs/2512.14235v1</guid><pubDate>Tue, 16 Dec 2025 09:43:05 +0000</pubDate><dc:creator>Jimmie Kwok</dc:creator><dc:creator>Holger Caesar</dc:creator><dc:creator>Andras Palffy</dc:creator><dc:type>preprint</dc:type><prism:publicationName>arXiv</prism:publicationName><description>Automotive radar has shown promising developments in environment perception due to its cost-effectiveness and robustness in adverse weather conditions. However, the limited availability of annotated radar data poses a significant challenge for advancing radar-based perception systems. To address this limitation, we propose a novel framework to generate 4D radar point clouds for training and evaluating object detectors. Unlike image-based diffusion, our method is designed to consider the sparsity and unique characteristics of radar point clouds by applying diffusion to a latent point cloud representation. Within this latent space, generation is controlled via conditioning at either the object or scene level. The proposed 4D-RaDiff converts unlabeled bounding boxes into high-quality radar annotations and transforms existing LiDAR point cloud data into realistic radar scenes. Experiments demonstrate that incorporating synthetic radar data of 4D-RaDiff as data augmentation method during training consistently improves object detection performance compared to training on real data only. In addition, pre-training on our synthetic data reduces the amount of required annotated radar data by up to 90% while achieving comparable object detection performance.
Published: 2025-12-16T09:43:05+00:00
Venue: arXiv
Score: 0.788 (must_read)</description><content:encoded>&lt;p&gt;&lt;strong&gt;Authors:&lt;/strong&gt; Jimmie Kwok; Holger Caesar; Andras Palffy&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Journal:&lt;/strong&gt; arXiv&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Relevance:&lt;/strong&gt; 0.788 (must_read)&lt;/p&gt;
&lt;p&gt;Automotive radar has shown promising developments in environment perception due to its cost-effectiveness and robustness in adverse weather conditions. However, the limited availability of annotated radar data poses a significant challenge for advancing radar-based perception systems. To address this limitation, we propose a novel framework to generate 4D radar point clouds for training and evaluating object detectors. Unlike image-based diffusion, our method is designed to consider the sparsity and unique characteristics of radar point clouds by applying diffusion to a latent point cloud representation. Within this latent space, generation is controlled via conditioning at either the object or scene level. The proposed 4D-RaDiff converts unlabeled bounding boxes into high-quality radar annotations and transforms existing LiDAR point cloud data into realistic radar scenes. Experiments demonstrate that incorporating synthetic radar data of 4D-RaDiff as data augmentation method during training consistently improves object detection performance compared to training on real data only. In addition, pre-training on our synthetic data reduces the amount of required annotated radar data by up to 90% while achieving comparable object detection performance.&lt;/p&gt;</content:encoded></item><item><title>TSDFuse: Teacher-Student Supervised Explicit Decoupling of Shared and Distinct Features for Infrared-Visible Image Fusion</title><link>https://doi.org/10.1016/j.eswa.2025.130798</link><guid>10.1016/j.eswa.2025.130798</guid><pubDate>Tue, 16 Dec 2025 00:12:31 +0000</pubDate><dc:creator>Jie Li</dc:creator><dc:creator>Gangzhu Qiao</dc:creator><dc:creator>Jianghui Cai</dc:creator><dc:creator>Yubing Luo</dc:creator><prism:publicationName>Expert Systems with Applications</prism:publicationName><prism:doi>10.1016/j.eswa.2025.130798</prism:doi><description>Infrared-visible image fusion integrates the complementary strengths of two modalities to produce images with rich textures and clear targets. However, because features are highly entangled and difficult to constrain, the fusion process often suffers from information loss. Existing approaches commonly attempt feature disentanglement to mitigate such loss, yet they still suffer from under-disentanglement, cross-talk between decomposed components, and under-utilization of modality-specific cues. To address these issues, we introduce, to our knowledge, the first teacher-student fusion framework that explicitly supervises the shared features. The framework employs a DRM to degrade visible images into pseudo-infrared representations, which serve as explicit pseudo-labels for learning shared features. A TS-EDCRM is then designed to achieve effective separation of shared and modality-specific representations through collaborative learning and cross-reconstruction, thereby suppressing feature leakage. Finally, a FDFM refines the decoder to produce fused images with sharper details and richer information. Across four public datasets (MSRS, LLVIP, TNO, and M3FD) and twelve state-of-the-art baselines, our method delivers consistent gains on EN, SF, AG, and SD, while maintaining information fidelity and cross-modal balance on VIF and Qabf. Ablation studies show that explicit shared supervision enforces shared-feature consistency, cross-reconstruction improves the separability of modality-specific features, and decoder fine-tuning further boosts the final fusion quality. Code will be released at https://github.com/no9951lj/TSDFuse .
Published: 2025-12-16T00:12:31+00:00
Venue: Expert Systems with Applications
Score: 0.787 (must_read)</description><content:encoded>&lt;p&gt;&lt;strong&gt;Authors:&lt;/strong&gt; Jie Li; Gangzhu Qiao; Jianghui Cai; Yubing Luo&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Journal:&lt;/strong&gt; Expert Systems with Applications&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;a href="https://doi.org/10.1016/j.eswa.2025.130798"&gt;10.1016/j.eswa.2025.130798&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Relevance:&lt;/strong&gt; 0.787 (must_read)&lt;/p&gt;
&lt;p&gt;Infrared-visible image fusion integrates the complementary strengths of two modalities to produce images with rich textures and clear targets. However, because features are highly entangled and difficult to constrain, the fusion process often suffers from information loss. Existing approaches commonly attempt feature disentanglement to mitigate such loss, yet they still suffer from under-disentanglement, cross-talk between decomposed components, and under-utilization of modality-specific cues. To address these issues, we introduce, to our knowledge, the first teacher-student fusion framework that explicitly supervises the shared features. The framework employs a DRM to degrade visible images into pseudo-infrared representations, which serve as explicit pseudo-labels for learning shared features. A TS-EDCRM is then designed to achieve effective separation of shared and modality-specific representations through collaborative learning and cross-reconstruction, thereby suppressing feature leakage. Finally, a FDFM refines the decoder to produce fused images with sharper details and richer information. Across four public datasets (MSRS, LLVIP, TNO, and M3FD) and twelve state-of-the-art baselines, our method delivers consistent gains on EN, SF, AG, and SD, while maintaining information fidelity and cross-modal balance on VIF and Qabf. Ablation studies show that explicit shared supervision enforces shared-feature consistency, cross-reconstruction improves the separability of modality-specific features, and decoder fine-tuning further boosts the final fusion quality. Code will be released at https://github.com/no9951lj/TSDFuse .&lt;/p&gt;</content:encoded></item></channel></rss>