<?xml version='1.0' encoding='utf-8'?>
<rss xmlns:content="http://purl.org/rss/1.0/modules/content/" xmlns:dc="http://purl.org/dc/elements/1.1/" xmlns:prism="http://prismstandard.org/namespaces/basic/3.0/" version="2.0"><channel><title>ZotWatch Feed</title><link>https://ehehe.cn/zotwatch/</link><description>AI-assisted literature watch</description><lastBuildDate>Sat, 17 Jan 2026 02:42:24 +0000</lastBuildDate><item><title>Amplitude, Phase, and Gradient Recovery from Compressed SAR Images</title><link>https://doi.org/10.1109/tcsvt.2026.3654125</link><guid>10.1109/tcsvt.2026.3654125</guid><pubDate>Thu, 15 Jan 2026 20:51:57 +0000</pubDate><dc:creator>Paras Maharjan</dc:creator><dc:creator>Zhu Li</dc:creator><dc:creator>Chris McGuiness</dc:creator><prism:publicationName>IEEE Transactions on Circuits and Systems for Video Technology</prism:publicationName><prism:doi>10.1109/tcsvt.2026.3654125</prism:doi><description>Compressing Synthetic Aperture Radar (SAR) images presents unique challenges due to the high dynamic range and inherent acquisition noise in the amplitude signal, as well as the noise-sensitive and limited information content in the phase signal. Traditional compression methods, such as JPEG and JPEG2000, although widely used, often fail to preserve SAR image quality due to their susceptibility to compression artifacts. The continuous capture of high-resolution raw SAR images over extended periods on drones and Unmanned Aerial Vehicles (UAVs), combined with constraints on computational resources, bandwidth, and onboard storage, further complicates the problem. An effective and efficient compression pipeline is essential for either onboard storage or real-time transmission to ground stations. In this work, we propose a hybrid solution for complex-valued SAR image compression by utilizing the Versatile Video Coding (VVC) framework as a backbone compression engine and employing a deep learning-based method that operates jointly in the pixel and transform domains for deblocking and reconstructing SAR amplitude and phase images. Specifically, we design task-specific compression artifact removal networks called AmpRes and AngRes for amplitude and phase reconstruction, respectively. Additionally, we introduce the GradRes network to learn gradients for SAR Scale-Invariant Feature Transform (SAR-SIFT), resulting in robust orientation and magnitude estimations that improve downstream tasks such as keypoints detection and matching in noisy and compressed scenarios. Experimental results demonstrate that our approach achieves a 10% Bjøntegaard Delta (BD)-Rate savings over VVC for amplitude recovery, along with notable improvement in phase reconstruction, and delivers an average of 34% improvement in SAR-SIFT repeatability.
Published: 2026-01-15T20:51:57+00:00
Venue: IEEE Transactions on Circuits and Systems for Video Technology
Score: 0.826 (must_read)</description><content:encoded>&lt;p&gt;&lt;strong&gt;Authors:&lt;/strong&gt; Paras Maharjan; Zhu Li; Chris McGuiness&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Journal:&lt;/strong&gt; IEEE Transactions on Circuits and Systems for Video Technology&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;a href="https://doi.org/10.1109/tcsvt.2026.3654125"&gt;10.1109/tcsvt.2026.3654125&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Relevance:&lt;/strong&gt; 0.826 (must_read)&lt;/p&gt;
&lt;p&gt;Compressing Synthetic Aperture Radar (SAR) images presents unique challenges due to the high dynamic range and inherent acquisition noise in the amplitude signal, as well as the noise-sensitive and limited information content in the phase signal. Traditional compression methods, such as JPEG and JPEG2000, although widely used, often fail to preserve SAR image quality due to their susceptibility to compression artifacts. The continuous capture of high-resolution raw SAR images over extended periods on drones and Unmanned Aerial Vehicles (UAVs), combined with constraints on computational resources, bandwidth, and onboard storage, further complicates the problem. An effective and efficient compression pipeline is essential for either onboard storage or real-time transmission to ground stations. In this work, we propose a hybrid solution for complex-valued SAR image compression by utilizing the Versatile Video Coding (VVC) framework as a backbone compression engine and employing a deep learning-based method that operates jointly in the pixel and transform domains for deblocking and reconstructing SAR amplitude and phase images. Specifically, we design task-specific compression artifact removal networks called AmpRes and AngRes for amplitude and phase reconstruction, respectively. Additionally, we introduce the GradRes network to learn gradients for SAR Scale-Invariant Feature Transform (SAR-SIFT), resulting in robust orientation and magnitude estimations that improve downstream tasks such as keypoints detection and matching in noisy and compressed scenarios. Experimental results demonstrate that our approach achieves a 10% Bjøntegaard Delta (BD)-Rate savings over VVC for amplitude recovery, along with notable improvement in phase reconstruction, and delivers an average of 34% improvement in SAR-SIFT repeatability.&lt;/p&gt;</content:encoded></item><item><title>SRAW-Attack: Space-Reweighted Adversarial Warping Attack for SAR Target Recognition</title><link>https://arxiv.org/abs/2601.10324v1</link><guid>http://arxiv.org/abs/2601.10324v1</guid><pubDate>Thu, 15 Jan 2026 12:09:49 +0000</pubDate><dc:creator>Yiming Zhang</dc:creator><dc:creator>Weibo Qin</dc:creator><dc:creator>Yuntian Liu</dc:creator><dc:creator>Feng Wang</dc:creator><dc:type>preprint</dc:type><prism:publicationName>arXiv</prism:publicationName><description>Synthetic aperture radar (SAR) imagery exhibits intrinsic information sparsity due to its unique electromagnetic scattering mechanism. Despite the widespread adoption of deep neural network (DNN)-based SAR automatic target recognition (SAR-ATR) systems, they remain vulnerable to adversarial examples and tend to over-rely on background regions, leading to degraded adversarial robustness. Existing adversarial attacks for SAR-ATR often require visually perceptible distortions to achieve effective performance, thereby necessitating an attack method that balances effectiveness and stealthiness. In this paper, a novel attack method termed Space-Reweighted Adversarial Warping (SRAW) is proposed, which generates adversarial examples through optimized spatial deformation with reweighted budgets across foreground and background regions. Extensive experiments demonstrate that SRAW significantly degrades the performance of state-of-the-art SAR-ATR models and consistently outperforms existing methods in terms of imperceptibility and adversarial transferability. Code is made available at https://github.com/boremycin/SAR-ATR-TransAttack.
Published: 2026-01-15T12:09:49+00:00
Venue: arXiv
Score: 0.807 (must_read)</description><content:encoded>&lt;p&gt;&lt;strong&gt;Authors:&lt;/strong&gt; Yiming Zhang; Weibo Qin; Yuntian Liu; Feng Wang&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Journal:&lt;/strong&gt; arXiv&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Relevance:&lt;/strong&gt; 0.807 (must_read)&lt;/p&gt;
&lt;p&gt;Synthetic aperture radar (SAR) imagery exhibits intrinsic information sparsity due to its unique electromagnetic scattering mechanism. Despite the widespread adoption of deep neural network (DNN)-based SAR automatic target recognition (SAR-ATR) systems, they remain vulnerable to adversarial examples and tend to over-rely on background regions, leading to degraded adversarial robustness. Existing adversarial attacks for SAR-ATR often require visually perceptible distortions to achieve effective performance, thereby necessitating an attack method that balances effectiveness and stealthiness. In this paper, a novel attack method termed Space-Reweighted Adversarial Warping (SRAW) is proposed, which generates adversarial examples through optimized spatial deformation with reweighted budgets across foreground and background regions. Extensive experiments demonstrate that SRAW significantly degrades the performance of state-of-the-art SAR-ATR models and consistently outperforms existing methods in terms of imperceptibility and adversarial transferability. Code is made available at https://github.com/boremycin/SAR-ATR-TransAttack.&lt;/p&gt;</content:encoded></item><item><title>RECREATE: Supervised contrastive learning and inpainting based hyperspectral image denoising</title><link>https://doi.org/10.1016/j.isprsjprs.2026.01.022</link><guid>10.1016/j.isprsjprs.2026.01.022</guid><pubDate>Fri, 16 Jan 2026 12:27:16 +0000</pubDate><dc:creator>Aditya Dixit</dc:creator><dc:creator>Anup Kumar Gupta</dc:creator><dc:creator>Puneet Gupta</dc:creator><dc:creator>Ankur Garg</dc:creator><prism:publicationName>ISPRS Journal of Photogrammetry and Remote Sensing</prism:publicationName><prism:doi>10.1016/j.isprsjprs.2026.01.022</prism:doi><description>Hyperspectral image (HSI) contains information at various spectra, making it valuable in several real-world applications such as environmental monitoring, agriculture, and remote sensing. However, the acquisition process often introduces noise, necessitating effective HSI denoising methods to maintain its applicability. Deep Learning (DL) is considered as the de-facto for HSI denoising, but it requires a significant number of training samples to optimize network parameters for effective denoising outcomes. However, obtaining extensive datasets is challenging in HSI, leading to epistemic uncertainties and thereby deteriorating the denoising performance. This paper introduces a novel supervised contrastive learning (SCL) method, RECREATE , to enhance feature learning and mitigate the issue of epistemic uncertainty for HSI denoising. Furthermore, we introduce the exploration of image inpainting as an auxiliary task to enhance the HSI denoising performance. By adding HSI inpainting to CL, our method essentially enhances HSI denoising by increasing training datasets and enforcing improved feature learning. Experimental outcomes on various HSI datasets validate the efficacy of RECREATE , showcasing its potential for integration with existing HSI denoising techniques to enhance their performance, both qualitatively and quantitatively. This innovative method holds promise for addressing the limitations posed by limited training data and thereby advancing the field toward proposing better HSI denoising methods.
Published: 2026-01-16T12:27:16+00:00
Venue: ISPRS Journal of Photogrammetry and Remote Sensing
Score: 0.806 (must_read)</description><content:encoded>&lt;p&gt;&lt;strong&gt;Authors:&lt;/strong&gt; Aditya Dixit; Anup Kumar Gupta; Puneet Gupta; Ankur Garg&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Journal:&lt;/strong&gt; ISPRS Journal of Photogrammetry and Remote Sensing&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;a href="https://doi.org/10.1016/j.isprsjprs.2026.01.022"&gt;10.1016/j.isprsjprs.2026.01.022&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Relevance:&lt;/strong&gt; 0.806 (must_read)&lt;/p&gt;
&lt;p&gt;Hyperspectral image (HSI) contains information at various spectra, making it valuable in several real-world applications such as environmental monitoring, agriculture, and remote sensing. However, the acquisition process often introduces noise, necessitating effective HSI denoising methods to maintain its applicability. Deep Learning (DL) is considered as the de-facto for HSI denoising, but it requires a significant number of training samples to optimize network parameters for effective denoising outcomes. However, obtaining extensive datasets is challenging in HSI, leading to epistemic uncertainties and thereby deteriorating the denoising performance. This paper introduces a novel supervised contrastive learning (SCL) method, RECREATE , to enhance feature learning and mitigate the issue of epistemic uncertainty for HSI denoising. Furthermore, we introduce the exploration of image inpainting as an auxiliary task to enhance the HSI denoising performance. By adding HSI inpainting to CL, our method essentially enhances HSI denoising by increasing training datasets and enforcing improved feature learning. Experimental outcomes on various HSI datasets validate the efficacy of RECREATE , showcasing its potential for integration with existing HSI denoising techniques to enhance their performance, both qualitatively and quantitatively. This innovative method holds promise for addressing the limitations posed by limited training data and thereby advancing the field toward proposing better HSI denoising methods.&lt;/p&gt;</content:encoded></item><item><title>AnchorReF: A novel anchor-based visual re-localization framework aided by multi-sensor data fusion</title><link>https://doi.org/10.1016/j.isprsjprs.2026.01.019</link><guid>10.1016/j.isprsjprs.2026.01.019</guid><pubDate>Fri, 16 Jan 2026 12:27:16 +0000</pubDate><dc:creator>Hao Wu</dc:creator><dc:creator>Yu Ran</dc:creator><dc:creator>Xiaoxiang Zhang</dc:creator><dc:creator>Xinying Luo</dc:creator><dc:creator>Li Wang</dc:creator><dc:creator>Teng Zhao</dc:creator><dc:creator>Yongcheng Song</dc:creator><dc:creator>Zhijun Zhang</dc:creator><dc:creator>Huisong Zhang</dc:creator><dc:creator>Jin Liu</dc:creator><dc:creator>Jian Li</dc:creator><prism:publicationName>ISPRS Journal of Photogrammetry and Remote Sensing</prism:publicationName><prism:doi>10.1016/j.isprsjprs.2026.01.019</prism:doi><description>Visual relocalization estimates the precise pose of a query image within a pre-built visual map, serving as a fundamental component for robot navigation, autonomous driving, surveying and mapping, etc. In the past few decades, significant research efforts have been devoted to achieving high relocalization accuracy. However, challenges remain when the query images exhibit significant changes compared to the reference scene. This paper primarily addresses the problem of pose verification and correction of inaccurate pose estimations from the relocalization. We propose a novel anchor-based visual relocalization framework that achieves robust pose estimations through multi-view co-visibility verification. Our approach further utilizes a tightly-coupled multi-sensor data fusion for pose refinement. Comprehensive evaluations on large-scale, real-world urban driving datasets (containing frequent dynamic objects, severe occlusions, and long-term environmental changes) demonstrate that our framework achieves state-of-the-art performance. Specifically, compared to traditional SFM-based and Transformer-based methods under these challenging conditions, our approach reduces the translation error by 46.2% and the rotation error by 8.55%.
Published: 2026-01-16T12:27:16+00:00
Venue: ISPRS Journal of Photogrammetry and Remote Sensing
Score: 0.806 (must_read)</description><content:encoded>&lt;p&gt;&lt;strong&gt;Authors:&lt;/strong&gt; Hao Wu; Yu Ran; Xiaoxiang Zhang; Xinying Luo; Li Wang; Teng Zhao; Yongcheng Song; Zhijun Zhang; Huisong Zhang; Jin Liu; Jian Li&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Journal:&lt;/strong&gt; ISPRS Journal of Photogrammetry and Remote Sensing&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;a href="https://doi.org/10.1016/j.isprsjprs.2026.01.019"&gt;10.1016/j.isprsjprs.2026.01.019&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Relevance:&lt;/strong&gt; 0.806 (must_read)&lt;/p&gt;
&lt;p&gt;Visual relocalization estimates the precise pose of a query image within a pre-built visual map, serving as a fundamental component for robot navigation, autonomous driving, surveying and mapping, etc. In the past few decades, significant research efforts have been devoted to achieving high relocalization accuracy. However, challenges remain when the query images exhibit significant changes compared to the reference scene. This paper primarily addresses the problem of pose verification and correction of inaccurate pose estimations from the relocalization. We propose a novel anchor-based visual relocalization framework that achieves robust pose estimations through multi-view co-visibility verification. Our approach further utilizes a tightly-coupled multi-sensor data fusion for pose refinement. Comprehensive evaluations on large-scale, real-world urban driving datasets (containing frequent dynamic objects, severe occlusions, and long-term environmental changes) demonstrate that our framework achieves state-of-the-art performance. Specifically, compared to traditional SFM-based and Transformer-based methods under these challenging conditions, our approach reduces the translation error by 46.2% and the rotation error by 8.55%.&lt;/p&gt;</content:encoded></item><item><title>COP: CrOss-View Attention Prompt for Zero-Shot Sketch-Based Image Retrieval</title><link>https://doi.org/10.1109/tmm.2026.3654420</link><guid>10.1109/tmm.2026.3654420</guid><pubDate>Thu, 15 Jan 2026 20:50:42 +0000</pubDate><dc:creator>Jiahao Zheng</dc:creator><dc:creator>Yu Tang</dc:creator><dc:creator>Yongcan Luo</dc:creator><dc:creator>Ning Chen</dc:creator><dc:creator>Dan Zeng</dc:creator><dc:creator>Dapeng Wu</dc:creator><prism:publicationName>IEEE Transactions on Multimedia</prism:publicationName><prism:doi>10.1109/tmm.2026.3654420</prism:doi><description>Zero-shot Sketch-based Image Retrieval (ZS-SBIR) is a challenging yet rewarding task, as it demands models to possess both brain- like zero-shot learning and cross-view alignment capabilities. Recent advances suggest that powerful pre-trained vision encoders, such as CLIP, offer a promising alternative for addressing the ZS-SBIR task. However, the problem of simultaneously evoking the zero-shot learning capability and cross-view alignment capability of pre-trained vision encoders has barely been discussed. To this end, we propose the CrOss-view Attention Prompt (COP) framework, which is composed of an Attention Prompt module and a Cross-view Query module. Specifically, we formulate prompt construction as a retrieval problem by introducing a prompt pool and attention mechanism, thereby constructing attention prompts with fine granularity to enhance the zero-shot learning capability. Furthermore, to endow COP with cross-view alignment capabilities, we replace single-view queries with carefully designed cross-view queries, which can be smoothly inserted into the Attention Prompt module. The proposed COP is scenario-agnostic and supports vision encoders with diverse pre-training schemes. Comprehensive experiments show that COP achieves competitive performance in ZS-SBIR, Generalized ZS-SBIR, and Cross-data ZS-SBIR scenarios, regardless of whether it is based on the ImageNet pre-trained vision encoder or the CLIP pre-trained vision encoder.
Published: 2026-01-15T20:50:42+00:00
Venue: IEEE Transactions on Multimedia
Score: 0.804 (must_read)</description><content:encoded>&lt;p&gt;&lt;strong&gt;Authors:&lt;/strong&gt; Jiahao Zheng; Yu Tang; Yongcan Luo; Ning Chen; Dan Zeng; Dapeng Wu&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Journal:&lt;/strong&gt; IEEE Transactions on Multimedia&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;a href="https://doi.org/10.1109/tmm.2026.3654420"&gt;10.1109/tmm.2026.3654420&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Relevance:&lt;/strong&gt; 0.804 (must_read)&lt;/p&gt;
&lt;p&gt;Zero-shot Sketch-based Image Retrieval (ZS-SBIR) is a challenging yet rewarding task, as it demands models to possess both brain- like zero-shot learning and cross-view alignment capabilities. Recent advances suggest that powerful pre-trained vision encoders, such as CLIP, offer a promising alternative for addressing the ZS-SBIR task. However, the problem of simultaneously evoking the zero-shot learning capability and cross-view alignment capability of pre-trained vision encoders has barely been discussed. To this end, we propose the CrOss-view Attention Prompt (COP) framework, which is composed of an Attention Prompt module and a Cross-view Query module. Specifically, we formulate prompt construction as a retrieval problem by introducing a prompt pool and attention mechanism, thereby constructing attention prompts with fine granularity to enhance the zero-shot learning capability. Furthermore, to endow COP with cross-view alignment capabilities, we replace single-view queries with carefully designed cross-view queries, which can be smoothly inserted into the Attention Prompt module. The proposed COP is scenario-agnostic and supports vision encoders with diverse pre-training schemes. Comprehensive experiments show that COP achieves competitive performance in ZS-SBIR, Generalized ZS-SBIR, and Cross-data ZS-SBIR scenarios, regardless of whether it is based on the ImageNet pre-trained vision encoder or the CLIP pre-trained vision encoder.&lt;/p&gt;</content:encoded></item><item><title>Data-Interactive Mamba Driven SAR-Optical Fusion Cloud Removal</title><link>https://doi.org/10.1016/j.knosys.2026.115301</link><guid>10.1016/j.knosys.2026.115301</guid><pubDate>Thu, 15 Jan 2026 16:17:19 +0000</pubDate><dc:creator>Fajing Liu</dc:creator><dc:creator>En Li</dc:creator><dc:creator>Yixiao Liu</dc:creator><dc:creator>Sijie Zhou</dc:creator><dc:creator>Yuanyuan Wu</dc:creator><dc:creator>Chao Ren</dc:creator><prism:publicationName>Knowledge-Based Systems</prism:publicationName><prism:doi>10.1016/j.knosys.2026.115301</prism:doi><description>Synthetic aperture radar (SAR) imagery, synchronized with optical (OPT) data, enables reconstructing remote sensing images obscured by clouds and shadows owing to their ability to penetrate clouds. However, existing SAR-optical fusion techniques for cloud removal are few and predominantly target global cloud removal, resulting in insufficient feature reconstruction in dense cloud areas and introducing artifacts in cloud-free regions. Moreover, current methods lack independent learning from individual modalities and fail to establish comprehensive associations for data recovery when either SAR or OPT signals are weak. To effectively solve the above problems, a novel model (MDFuse-CR) is proposed in this paper. Firstly, the data-interactive vision mamba (DI_ViM) based on the state space model is presented, then utilized with an invertible neural network (INN) to create a dual-branch complementary-individual feature extraction module. Secondly, a cloud-shadow adaptive loss function derived from the cloud-shadow detection algorithm is introduced, aimed at minimizing the impact of processing on cloud-free areas. Thirdly, a pre-training method is adopted to remove noise in SAR and OPT images. Finally, the superiority of the approach in terms of cloud removal efficacy and inference speed in the absence of the transformer is substantiated. The experimental results on public dataset SEN12MS-CR show that the proposed method exhibits superior performance. Compared with the second-best method, the average PSNR and SSIM values of MDFuse-CR increase by 1.4% and 2.8% respectively, while the average MAE and SAM values decrease by 5.4% and 1.8% respectively. The source code is available at https://github.com/Jing220/MDFuse-CR .
Published: 2026-01-15T16:17:19+00:00
Venue: Knowledge-Based Systems
Score: 0.800 (must_read)</description><content:encoded>&lt;p&gt;&lt;strong&gt;Authors:&lt;/strong&gt; Fajing Liu; En Li; Yixiao Liu; Sijie Zhou; Yuanyuan Wu; Chao Ren&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Journal:&lt;/strong&gt; Knowledge-Based Systems&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;a href="https://doi.org/10.1016/j.knosys.2026.115301"&gt;10.1016/j.knosys.2026.115301&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Relevance:&lt;/strong&gt; 0.800 (must_read)&lt;/p&gt;
&lt;p&gt;Synthetic aperture radar (SAR) imagery, synchronized with optical (OPT) data, enables reconstructing remote sensing images obscured by clouds and shadows owing to their ability to penetrate clouds. However, existing SAR-optical fusion techniques for cloud removal are few and predominantly target global cloud removal, resulting in insufficient feature reconstruction in dense cloud areas and introducing artifacts in cloud-free regions. Moreover, current methods lack independent learning from individual modalities and fail to establish comprehensive associations for data recovery when either SAR or OPT signals are weak. To effectively solve the above problems, a novel model (MDFuse-CR) is proposed in this paper. Firstly, the data-interactive vision mamba (DI_ViM) based on the state space model is presented, then utilized with an invertible neural network (INN) to create a dual-branch complementary-individual feature extraction module. Secondly, a cloud-shadow adaptive loss function derived from the cloud-shadow detection algorithm is introduced, aimed at minimizing the impact of processing on cloud-free areas. Thirdly, a pre-training method is adopted to remove noise in SAR and OPT images. Finally, the superiority of the approach in terms of cloud removal efficacy and inference speed in the absence of the transformer is substantiated. The experimental results on public dataset SEN12MS-CR show that the proposed method exhibits superior performance. Compared with the second-best method, the average PSNR and SSIM values of MDFuse-CR increase by 1.4% and 2.8% respectively, while the average MAE and SAM values decrease by 5.4% and 1.8% respectively. The source code is available at https://github.com/Jing220/MDFuse-CR .&lt;/p&gt;</content:encoded></item><item><title>Dual-Domain Adaptation Networks for Realistic Image Super-Resolution</title><link>https://doi.org/10.1109/tmm.2026.3654395</link><guid>10.1109/tmm.2026.3654395</guid><pubDate>Thu, 15 Jan 2026 20:50:42 +0000</pubDate><dc:creator>Chaowei Fang</dc:creator><dc:creator>Bolin Fu</dc:creator><dc:creator>De Cheng</dc:creator><dc:creator>Lechao Cheng</dc:creator><dc:creator>Guanbin Li</dc:creator><prism:publicationName>IEEE Transactions on Multimedia</prism:publicationName><prism:doi>10.1109/tmm.2026.3654395</prism:doi><description>Realistic image super-resolution (SR) focuses on transforming real-world low-resolution (LR) images into high-resolution (HR) ones, handling more complex degradation patterns than synthetic SR tasks. This is critical for applications like surveillance, medical imaging, and consumer electronics. However, current methods struggle with limited real-world LR-HR data, impacting the learning of basic image features. Pre-trained SR models from large-scale synthetic datasets offer valuable prior knowledge, which can improve generalization, speed up training, and reduce the need for extensive real-world data in realistic SR tasks. In this paper, we introduce a novel approach, Dual-domain Adaptation Networks, which is able to efficiently adapt pre-trained image SR models from simulated to real-world datasets. To achieve this target, we first set up a spatial-domain adaptation strategy through selectively updating parameters of pre-trained models and employing the low-rank adaptation technique to adjust frozen parameters. Recognizing that image super-resolution involves recovering high-frequency components, we further integrate a frequency domain adaptation branch into the adapted model, which combines the spectral data of the input and the spatial-domain backbone's intermediate features to infer HR frequency maps, enhancing the SR result. Experimental evaluations on public realistic image SR benchmarks, including RealSR, D2CRealSR, and DRealSR, demonstrate the superiority of our proposed method over existing state-of-the-art models.
Published: 2026-01-15T20:50:42+00:00
Venue: IEEE Transactions on Multimedia
Score: 0.800 (must_read)</description><content:encoded>&lt;p&gt;&lt;strong&gt;Authors:&lt;/strong&gt; Chaowei Fang; Bolin Fu; De Cheng; Lechao Cheng; Guanbin Li&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Journal:&lt;/strong&gt; IEEE Transactions on Multimedia&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;a href="https://doi.org/10.1109/tmm.2026.3654395"&gt;10.1109/tmm.2026.3654395&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Relevance:&lt;/strong&gt; 0.800 (must_read)&lt;/p&gt;
&lt;p&gt;Realistic image super-resolution (SR) focuses on transforming real-world low-resolution (LR) images into high-resolution (HR) ones, handling more complex degradation patterns than synthetic SR tasks. This is critical for applications like surveillance, medical imaging, and consumer electronics. However, current methods struggle with limited real-world LR-HR data, impacting the learning of basic image features. Pre-trained SR models from large-scale synthetic datasets offer valuable prior knowledge, which can improve generalization, speed up training, and reduce the need for extensive real-world data in realistic SR tasks. In this paper, we introduce a novel approach, Dual-domain Adaptation Networks, which is able to efficiently adapt pre-trained image SR models from simulated to real-world datasets. To achieve this target, we first set up a spatial-domain adaptation strategy through selectively updating parameters of pre-trained models and employing the low-rank adaptation technique to adjust frozen parameters. Recognizing that image super-resolution involves recovering high-frequency components, we further integrate a frequency domain adaptation branch into the adapted model, which combines the spectral data of the input and the spatial-domain backbone&amp;#x27;s intermediate features to infer HR frequency maps, enhancing the SR result. Experimental evaluations on public realistic image SR benchmarks, including RealSR, D2CRealSR, and DRealSR, demonstrate the superiority of our proposed method over existing state-of-the-art models.&lt;/p&gt;</content:encoded></item><item><title>SAMSOD: Rethinking SAM Optimization for RGB-T Salient Object Detection</title><link>https://doi.org/10.1109/tmm.2026.3654410</link><guid>10.1109/tmm.2026.3654410</guid><pubDate>Thu, 15 Jan 2026 20:50:42 +0000</pubDate><dc:creator>Zhengyi Liu</dc:creator><dc:creator>Xinrui Wang</dc:creator><dc:creator>Xianyong Fang</dc:creator><dc:creator>Zhengzheng Tu</dc:creator><dc:creator>Linbo Wang</dc:creator><prism:publicationName>IEEE Transactions on Multimedia</prism:publicationName><prism:doi>10.1109/tmm.2026.3654410</prism:doi><description>RGB-T salient object detection (SOD) aims to segment attractive objects by combining RGB and thermal infrared images. To enhance performance, the Segment Anything Model has been fine-tuned for this task. However, the imbalance convergence of two modalities and significant gradient difference between high- and low- activations are ignored, thereby leaving room for further performance enhancement. In this paper, we propose a model called SAMSOD, which utilizes unimodal supervision to enhance the learning of non-dominant modality and employs gradient deconfliction to reduce the impact of conflicting gradients on model convergence. The method also leverages two decoupled adapters to separately mask high- and low-activation neurons, emphasizing foreground objects by enhancing background learning. Fundamental experiments on RGB-T SOD benchmark datasets and generalizability experiments on scribble supervised RGB-T SOD, fully supervised RGB-D SOD datasets and full-supervised RGB-D rail surface defect detection all demonstrate the effectiveness of our proposed method.
Published: 2026-01-15T20:50:42+00:00
Venue: IEEE Transactions on Multimedia
Score: 0.797 (must_read)</description><content:encoded>&lt;p&gt;&lt;strong&gt;Authors:&lt;/strong&gt; Zhengyi Liu; Xinrui Wang; Xianyong Fang; Zhengzheng Tu; Linbo Wang&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Journal:&lt;/strong&gt; IEEE Transactions on Multimedia&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;a href="https://doi.org/10.1109/tmm.2026.3654410"&gt;10.1109/tmm.2026.3654410&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Relevance:&lt;/strong&gt; 0.797 (must_read)&lt;/p&gt;
&lt;p&gt;RGB-T salient object detection (SOD) aims to segment attractive objects by combining RGB and thermal infrared images. To enhance performance, the Segment Anything Model has been fine-tuned for this task. However, the imbalance convergence of two modalities and significant gradient difference between high- and low- activations are ignored, thereby leaving room for further performance enhancement. In this paper, we propose a model called SAMSOD, which utilizes unimodal supervision to enhance the learning of non-dominant modality and employs gradient deconfliction to reduce the impact of conflicting gradients on model convergence. The method also leverages two decoupled adapters to separately mask high- and low-activation neurons, emphasizing foreground objects by enhancing background learning. Fundamental experiments on RGB-T SOD benchmark datasets and generalizability experiments on scribble supervised RGB-T SOD, fully supervised RGB-D SOD datasets and full-supervised RGB-D rail surface defect detection all demonstrate the effectiveness of our proposed method.&lt;/p&gt;</content:encoded></item><item><title>&lt;b&gt;MoE-LLaVA&lt;/b&gt;
                    : Mixture of Experts for Large Vision-Language Models</title><link>https://doi.org/10.1109/tmm.2026.3654458</link><guid>10.1109/tmm.2026.3654458</guid><pubDate>Thu, 15 Jan 2026 20:50:42 +0000</pubDate><dc:creator>Bin Lin</dc:creator><dc:creator>Zhenyu Tang</dc:creator><dc:creator>Yang Ye</dc:creator><dc:creator>Jinfa Huang</dc:creator><dc:creator>Junwu Zhang</dc:creator><dc:creator>Yatian Pang</dc:creator><dc:creator>Peng Jin</dc:creator><dc:creator>Munan Ning</dc:creator><dc:creator>Jiebo Luo</dc:creator><dc:creator>Li Yuan</dc:creator><prism:publicationName>IEEE Transactions on Multimedia</prism:publicationName><prism:doi>10.1109/tmm.2026.3654458</prism:doi><description>Recently, remarkable progress has been made in scaling up Large Language Models (LLMs) through the use of the sparse Mixture-of-Expert (MoE) layers without significantly increasing computational cost. However, the transition from a pre-trained LLM to a sparse Large Vision-Language Model (LVLM) with MoE remains an open challenge. Directly fine-tuning an LLM to a sparse LVLM often leads to training collapse, characterized by (1) a large modality feature distribution gap and (2) expert load imbalance. This paper proposes a three-stage decoupled weight training process. In the first two stages, the model learns to adapt the LLM to an LVLM. In the third stage, the FFN weights from the second stage are used as lossless initialization for expert weights, effectively constructing a sparse model with a vast number of parameters while maintaining constant computational cost. Through extensive ablation experiments, we derive three empirical guidelines and propose a sparse LVLM termed MoE-LLaVA. MoE-LLaVA is a MoE-based sparse LVLM architecture, which uniquely activates only the top- k k experts through routers during deployment, keeping the remaining experts inactive. Extensive experiments demonstrate that MoE-LLaVA outperforms LLaVA-1.5-7B with an average improvement of 4.6 across nine visual understanding benchmarks. Notably, with only 2.2B active parameters, our MoE-LLaVA shows comparable result with LLaVA-1.5-13B (87.0 vs. 85.9) on POPE benchmark. Our work establishes a baseline for sparse LVLMs and provides empirical guidelines for exploring the sparse LVLMs. Our code is available at: https://github.com/PKU-YuanGroup/MoE-LLaVA.
Published: 2026-01-15T20:50:42+00:00
Venue: IEEE Transactions on Multimedia
Score: 0.796 (must_read)</description><content:encoded>&lt;p&gt;&lt;strong&gt;Authors:&lt;/strong&gt; Bin Lin; Zhenyu Tang; Yang Ye; Jinfa Huang; Junwu Zhang; Yatian Pang; Peng Jin; Munan Ning; Jiebo Luo; Li Yuan&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Journal:&lt;/strong&gt; IEEE Transactions on Multimedia&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;a href="https://doi.org/10.1109/tmm.2026.3654458"&gt;10.1109/tmm.2026.3654458&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Relevance:&lt;/strong&gt; 0.796 (must_read)&lt;/p&gt;
&lt;p&gt;Recently, remarkable progress has been made in scaling up Large Language Models (LLMs) through the use of the sparse Mixture-of-Expert (MoE) layers without significantly increasing computational cost. However, the transition from a pre-trained LLM to a sparse Large Vision-Language Model (LVLM) with MoE remains an open challenge. Directly fine-tuning an LLM to a sparse LVLM often leads to training collapse, characterized by (1) a large modality feature distribution gap and (2) expert load imbalance. This paper proposes a three-stage decoupled weight training process. In the first two stages, the model learns to adapt the LLM to an LVLM. In the third stage, the FFN weights from the second stage are used as lossless initialization for expert weights, effectively constructing a sparse model with a vast number of parameters while maintaining constant computational cost. Through extensive ablation experiments, we derive three empirical guidelines and propose a sparse LVLM termed MoE-LLaVA. MoE-LLaVA is a MoE-based sparse LVLM architecture, which uniquely activates only the top- k k experts through routers during deployment, keeping the remaining experts inactive. Extensive experiments demonstrate that MoE-LLaVA outperforms LLaVA-1.5-7B with an average improvement of 4.6 across nine visual understanding benchmarks. Notably, with only 2.2B active parameters, our MoE-LLaVA shows comparable result with LLaVA-1.5-13B (87.0 vs. 85.9) on POPE benchmark. Our work establishes a baseline for sparse LVLMs and provides empirical guidelines for exploring the sparse LVLMs. Our code is available at: https://github.com/PKU-YuanGroup/MoE-LLaVA.&lt;/p&gt;</content:encoded></item><item><title>MulMoSenT: Multimodal Sentiment Analysis for a Low-Resource Language Using Textual-Visual Cross-Attention and Fusion</title><link>https://doi.org/10.1016/j.inffus.2026.104129</link><guid>10.1016/j.inffus.2026.104129</guid><pubDate>Thu, 15 Jan 2026 23:44:22 +0000</pubDate><dc:creator>Sadia Afroze</dc:creator><dc:creator>Md Rajib Hossain</dc:creator><dc:creator>Mohammed Moshiul Hoque</dc:creator><dc:creator>Nazmul Siddique</dc:creator><prism:publicationName>Information Fusion</prism:publicationName><prism:doi>10.1016/j.inffus.2026.104129</prism:doi><description>The widespread availability of the Internet and the growing use of smart devices have fueled the rapid expansion of multimodal (image-text) sentiment analysis (MSA), a burgeoning research field. This growth is driven by the massive volume of image-text data generated by these technologies. However, MSA faces significant challenges, notably the misalignment between images and text, where an image may carry multiple interpretations or contradict its paired text. In addition, short textual content often lacks sufficient context, complicating sentiment prediction. These issues are particularly acute in low-resource languages, where annotated image-text corpora are scarce, and Vision-Language Models (VLMs) and Large Language Models (LLMs) exhibit limited performance. This research introduces MulMoSenT , a multimodal image-text sentiment analysis system tailored to tackle these challenges for low-resource languages. The development of MulMoSenT unfolds across four key phases: corpus development, baseline model evaluation and selection, hyperparameter adaptation, and model fine-tuning and inference. The proposed MulMoSenT model achieves a peak accuracy of 84.90%, surpassing all baseline models. Delivers a 37. 83% improvement over VLMs, a 35.28% gain over image-only models, and a 0.71% enhancement over text-only models. Both the dataset and the solution are publicly accessible at: https://github.com/sadia-afroze/MulMoSenT .
Published: 2026-01-15T23:44:22+00:00
Venue: Information Fusion
Score: 0.795 (must_read)</description><content:encoded>&lt;p&gt;&lt;strong&gt;Authors:&lt;/strong&gt; Sadia Afroze; Md Rajib Hossain; Mohammed Moshiul Hoque; Nazmul Siddique&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Journal:&lt;/strong&gt; Information Fusion&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;a href="https://doi.org/10.1016/j.inffus.2026.104129"&gt;10.1016/j.inffus.2026.104129&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Relevance:&lt;/strong&gt; 0.795 (must_read)&lt;/p&gt;
&lt;p&gt;The widespread availability of the Internet and the growing use of smart devices have fueled the rapid expansion of multimodal (image-text) sentiment analysis (MSA), a burgeoning research field. This growth is driven by the massive volume of image-text data generated by these technologies. However, MSA faces significant challenges, notably the misalignment between images and text, where an image may carry multiple interpretations or contradict its paired text. In addition, short textual content often lacks sufficient context, complicating sentiment prediction. These issues are particularly acute in low-resource languages, where annotated image-text corpora are scarce, and Vision-Language Models (VLMs) and Large Language Models (LLMs) exhibit limited performance. This research introduces MulMoSenT , a multimodal image-text sentiment analysis system tailored to tackle these challenges for low-resource languages. The development of MulMoSenT unfolds across four key phases: corpus development, baseline model evaluation and selection, hyperparameter adaptation, and model fine-tuning and inference. The proposed MulMoSenT model achieves a peak accuracy of 84.90%, surpassing all baseline models. Delivers a 37. 83% improvement over VLMs, a 35.28% gain over image-only models, and a 0.71% enhancement over text-only models. Both the dataset and the solution are publicly accessible at: https://github.com/sadia-afroze/MulMoSenT .&lt;/p&gt;</content:encoded></item><item><title>Fast Track Anything with Sparse Spatio-Temporal Propagation for Unified Video Segmentation</title><link>https://doi.org/10.1109/tip.2025.3649365</link><guid>10.1109/tip.2025.3649365</guid><pubDate>Thu, 15 Jan 2026 20:52:29 +0000</pubDate><dc:creator>Jisheng Dang</dc:creator><dc:creator>Huicheng Zheng</dc:creator><dc:creator>Zhixuan Chen</dc:creator><dc:creator>Zhang Li</dc:creator><dc:creator>Yulan Guo</dc:creator><dc:creator>Tat-Seng Chua</dc:creator><prism:publicationName>IEEE Transactions on Image Processing</prism:publicationName><prism:doi>10.1109/tip.2025.3649365</prism:doi><description>advances in "track-anything" models have significantly improved fine-grained video understanding by simultaneously handling multiple video segmentation and tracking tasks. However, existing models often struggle with robust and efficient temporal propagation. To address these challenges, we propose the Sparse Spatio-Temporal Propagation (SSTP) method, which achieves robust and efficient unified video segmentation by selectively leveraging key spatio-temporal features in videos. Specifically, we design a dynamic 3D spatio-temporal convolution to aggregate global multi-frame spatio-temporal information into memory frames during memory construction. Additionally, we introduce a spatio-temporal aggregation reading strategy to efficiently aggregate the relevant spatio-temporal features from multiple memory frames during memory retrieval. By combining SSTP with an image segmentation foundation model, such as the segment anything model, our method effectively addresses multiple data-scarce video segmentation tasks. Our experimental results demonstrate state-of-the-art performance on five video segmentation tasks across eleven datasets, outperforming both task-specific and unified methods. Notably, SSTP exhibits strong robustness in handling sparse, low-frame-rate videos, making it well-suited for real-world applications.
Published: 2026-01-15T20:52:29+00:00
Venue: IEEE Transactions on Image Processing
Score: 0.794 (must_read)</description><content:encoded>&lt;p&gt;&lt;strong&gt;Authors:&lt;/strong&gt; Jisheng Dang; Huicheng Zheng; Zhixuan Chen; Zhang Li; Yulan Guo; Tat-Seng Chua&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Journal:&lt;/strong&gt; IEEE Transactions on Image Processing&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;a href="https://doi.org/10.1109/tip.2025.3649365"&gt;10.1109/tip.2025.3649365&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Relevance:&lt;/strong&gt; 0.794 (must_read)&lt;/p&gt;
&lt;p&gt;advances in &amp;quot;track-anything&amp;quot; models have significantly improved fine-grained video understanding by simultaneously handling multiple video segmentation and tracking tasks. However, existing models often struggle with robust and efficient temporal propagation. To address these challenges, we propose the Sparse Spatio-Temporal Propagation (SSTP) method, which achieves robust and efficient unified video segmentation by selectively leveraging key spatio-temporal features in videos. Specifically, we design a dynamic 3D spatio-temporal convolution to aggregate global multi-frame spatio-temporal information into memory frames during memory construction. Additionally, we introduce a spatio-temporal aggregation reading strategy to efficiently aggregate the relevant spatio-temporal features from multiple memory frames during memory retrieval. By combining SSTP with an image segmentation foundation model, such as the segment anything model, our method effectively addresses multiple data-scarce video segmentation tasks. Our experimental results demonstrate state-of-the-art performance on five video segmentation tasks across eleven datasets, outperforming both task-specific and unified methods. Notably, SSTP exhibits strong robustness in handling sparse, low-frame-rate videos, making it well-suited for real-world applications.&lt;/p&gt;</content:encoded></item><item><title>Disentangle Object and Non-object Infrared Features via Language Guidance</title><link>https://arxiv.org/abs/2601.09228v1</link><guid>http://arxiv.org/abs/2601.09228v1</guid><pubDate>Wed, 14 Jan 2026 06:59:54 +0000</pubDate><dc:creator>Fan Liu</dc:creator><dc:creator>Ting Wu</dc:creator><dc:creator>Chuanyi Zhang</dc:creator><dc:creator>Liang Yao</dc:creator><dc:creator>Xing Ma</dc:creator><dc:creator>Yuhui Zheng</dc:creator><dc:type>preprint</dc:type><prism:publicationName>arXiv</prism:publicationName><description>Infrared object detection focuses on identifying and locating objects in complex environments (\eg, dark, snow, and rain) where visible imaging cameras are disabled by poor illumination. However, due to low contrast and weak edge information in infrared images, it is challenging to extract discriminative object features for robust detection. To deal with this issue, we propose a novel vision-language representation learning paradigm for infrared object detection. An additional textual supervision with rich semantic information is explored to guide the disentanglement of object and non-object features. Specifically, we propose a Semantic Feature Alignment (SFA) module to align the object features with the corresponding text features. Furthermore, we develop an Object Feature Disentanglement (OFD) module that disentangles text-aligned object features and non-object features by minimizing their correlation. Finally, the disentangled object features are entered into the detection head. In this manner, the detection performance can be remarkably enhanced via more discriminative and less noisy features. Extensive experimental results demonstrate that our approach achieves superior performance on two benchmarks: M\textsuperscript{3}FD (83.7\% mAP), FLIR (86.1\% mAP). Our code will be publicly available once the paper is accepted.
Published: 2026-01-14T06:59:54+00:00
Venue: arXiv
Score: 0.794 (must_read)</description><content:encoded>&lt;p&gt;&lt;strong&gt;Authors:&lt;/strong&gt; Fan Liu; Ting Wu; Chuanyi Zhang; Liang Yao; Xing Ma; Yuhui Zheng&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Journal:&lt;/strong&gt; arXiv&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Relevance:&lt;/strong&gt; 0.794 (must_read)&lt;/p&gt;
&lt;p&gt;Infrared object detection focuses on identifying and locating objects in complex environments (\eg, dark, snow, and rain) where visible imaging cameras are disabled by poor illumination. However, due to low contrast and weak edge information in infrared images, it is challenging to extract discriminative object features for robust detection. To deal with this issue, we propose a novel vision-language representation learning paradigm for infrared object detection. An additional textual supervision with rich semantic information is explored to guide the disentanglement of object and non-object features. Specifically, we propose a Semantic Feature Alignment (SFA) module to align the object features with the corresponding text features. Furthermore, we develop an Object Feature Disentanglement (OFD) module that disentangles text-aligned object features and non-object features by minimizing their correlation. Finally, the disentangled object features are entered into the detection head. In this manner, the detection performance can be remarkably enhanced via more discriminative and less noisy features. Extensive experimental results demonstrate that our approach achieves superior performance on two benchmarks: M\textsuperscript{3}FD (83.7\% mAP), FLIR (86.1\% mAP). Our code will be publicly available once the paper is accepted.&lt;/p&gt;</content:encoded></item><item><title>TCD: Towards Consistent and unified single stage object Detection</title><link>https://doi.org/10.1016/j.neucom.2026.132720</link><guid>10.1016/j.neucom.2026.132720</guid><pubDate>Thu, 15 Jan 2026 23:39:46 +0000</pubDate><dc:creator>Pengfei Liu</dc:creator><dc:creator>Shuxian Shang</dc:creator><dc:creator>Changguang Song</dc:creator><dc:creator>Fang Li</dc:creator><dc:creator>Yuhan Guo</dc:creator><dc:creator>Yuetong Chang</dc:creator><dc:creator>Han Feng</dc:creator><dc:creator>Guangming Xia</dc:creator><dc:creator>Biwei Wu</dc:creator><dc:creator>Jiubin Tan</dc:creator><dc:creator>Weibo Wang</dc:creator><prism:publicationName>Neurocomputing</prism:publicationName><prism:doi>10.1016/j.neucom.2026.132720</prism:doi><description>Single-stage convolutional detectors typically rely on preset anchors or points. However, such designs suffer from two inherent limitations: (1) the mismatch between preset intermediates and model parameters often leads to ambiguous label assignments; (2) the static nature of the sample set restricts its ability to exploit optimization dynamics for self-refinement, thereby impeding progressive improvement in assignment accuracy. To overcome these issues, we construct a dynamic sample set derived from the model’s predicted boxes, ensuring precise alignment between the sample set and model parameters. In addition, a s core- d istribution- g uided feature selection strategy (SDG) is introduced to further optimize label assignment, by analyzing the quality-score distribution of candidate samples and adaptively determining the appropriate number of positive samples for each ground-truth. Building upon these designs, we develop a unified TCD detector that bridges the technical gap in label assignment for single-stage convolutional detectors. This unification enables enhanced classification–regression consistency through incorporating a learnable localization-quality estimation branch and refining the regression loss with joint optimization of classification and localization scores, along with their discrepancy. Experimental results demonstrate that TCD achieves a competitive performance of 46.9 AP on the COCO test-dev with a ResNeXt-101 backbone.
Published: 2026-01-15T23:39:46+00:00
Venue: Neurocomputing
Score: 0.793 (must_read)</description><content:encoded>&lt;p&gt;&lt;strong&gt;Authors:&lt;/strong&gt; Pengfei Liu; Shuxian Shang; Changguang Song; Fang Li; Yuhan Guo; Yuetong Chang; Han Feng; Guangming Xia; Biwei Wu; Jiubin Tan; Weibo Wang&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Journal:&lt;/strong&gt; Neurocomputing&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;a href="https://doi.org/10.1016/j.neucom.2026.132720"&gt;10.1016/j.neucom.2026.132720&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Relevance:&lt;/strong&gt; 0.793 (must_read)&lt;/p&gt;
&lt;p&gt;Single-stage convolutional detectors typically rely on preset anchors or points. However, such designs suffer from two inherent limitations: (1) the mismatch between preset intermediates and model parameters often leads to ambiguous label assignments; (2) the static nature of the sample set restricts its ability to exploit optimization dynamics for self-refinement, thereby impeding progressive improvement in assignment accuracy. To overcome these issues, we construct a dynamic sample set derived from the model’s predicted boxes, ensuring precise alignment between the sample set and model parameters. In addition, a s core- d istribution- g uided feature selection strategy (SDG) is introduced to further optimize label assignment, by analyzing the quality-score distribution of candidate samples and adaptively determining the appropriate number of positive samples for each ground-truth. Building upon these designs, we develop a unified TCD detector that bridges the technical gap in label assignment for single-stage convolutional detectors. This unification enables enhanced classification–regression consistency through incorporating a learnable localization-quality estimation branch and refining the regression loss with joint optimization of classification and localization scores, along with their discrepancy. Experimental results demonstrate that TCD achieves a competitive performance of 46.9 AP on the COCO test-dev with a ResNeXt-101 backbone.&lt;/p&gt;</content:encoded></item><item><title>LiteEmbed: Adapting CLIP to Rare Classes</title><link>https://arxiv.org/abs/2601.09661v1</link><guid>http://arxiv.org/abs/2601.09661v1</guid><pubDate>Wed, 14 Jan 2026 17:53:11 +0000</pubDate><dc:creator>Aishwarya Agarwal</dc:creator><dc:creator>Srikrishna Karanam</dc:creator><dc:creator>Vineet Gandhi</dc:creator><dc:type>preprint</dc:type><prism:publicationName>arXiv</prism:publicationName><description>Large-scale vision-language models such as CLIP achieve strong zero-shot recognition but struggle with classes that are rarely seen during pretraining, including newly emerging entities and culturally specific categories. We introduce LiteEmbed, a lightweight framework for few-shot personalization of CLIP that enables new classes to be added without retraining its encoders. LiteEmbed performs subspace-guided optimization of text embeddings within CLIP's vocabulary, leveraging a PCA-based decomposition that disentangles coarse semantic directions from fine-grained variations. Two complementary objectives, coarse alignment and fine separation, jointly preserve global semantic consistency while enhancing discriminability among visually similar classes. Once optimized, the embeddings are plug-and-play, seamlessly substituting CLIP's original text features across classification, retrieval, segmentation, and detection tasks. Extensive experiments demonstrate substantial gains over prior methods, establishing LiteEmbed as an effective approach for adapting CLIP to underrepresented, rare, or unseen classes.
Published: 2026-01-14T17:53:11+00:00
Venue: arXiv
Score: 0.790 (must_read)</description><content:encoded>&lt;p&gt;&lt;strong&gt;Authors:&lt;/strong&gt; Aishwarya Agarwal; Srikrishna Karanam; Vineet Gandhi&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Journal:&lt;/strong&gt; arXiv&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Relevance:&lt;/strong&gt; 0.790 (must_read)&lt;/p&gt;
&lt;p&gt;Large-scale vision-language models such as CLIP achieve strong zero-shot recognition but struggle with classes that are rarely seen during pretraining, including newly emerging entities and culturally specific categories. We introduce LiteEmbed, a lightweight framework for few-shot personalization of CLIP that enables new classes to be added without retraining its encoders. LiteEmbed performs subspace-guided optimization of text embeddings within CLIP&amp;#x27;s vocabulary, leveraging a PCA-based decomposition that disentangles coarse semantic directions from fine-grained variations. Two complementary objectives, coarse alignment and fine separation, jointly preserve global semantic consistency while enhancing discriminability among visually similar classes. Once optimized, the embeddings are plug-and-play, seamlessly substituting CLIP&amp;#x27;s original text features across classification, retrieval, segmentation, and detection tasks. Extensive experiments demonstrate substantial gains over prior methods, establishing LiteEmbed as an effective approach for adapting CLIP to underrepresented, rare, or unseen classes.&lt;/p&gt;</content:encoded></item><item><title>FourierSR: A Fourier Token-based Plugin for Efficient Image Super-Resolution</title><link>https://doi.org/10.1109/tip.2025.3648872</link><guid>10.1109/tip.2025.3648872</guid><pubDate>Thu, 15 Jan 2026 20:52:29 +0000</pubDate><dc:creator>Wenjie Li</dc:creator><dc:creator>Heng Guo</dc:creator><dc:creator>Yuefeng Hou</dc:creator><dc:creator>Zhanyu Ma</dc:creator><prism:publicationName>IEEE Transactions on Image Processing</prism:publicationName><prism:doi>10.1109/tip.2025.3648872</prism:doi><description>Image super-resolution (SR) aims to recover low-resolution images to high-resolution images, where improving SR efficiency is a high-profile challenge. However, commonly used units in SR, like convolutions and window-based Transformers, have limited receptive fields, making it challenging to apply them to improve SR under extremely limited computational cost. To address this issue, inspired by modeling convolution theorem through token mix, we propose a Fourier token-based plugin called FourierSR to improve SR uniformly, which avoids the instability or inefficiency of existing token mix technologies when applied as plug-ins. Furthermore, compared to convolutions and windows-based Transformers, our FourierSR only utilizes Fourier transform and multiplication operations, greatly reducing complexity while having global receptive fields. Experiments show that our FourierSR as a plugin brings an average PSNR gain of 0.34dB for existing efficient SR methods on Manga109 test set at the scale of ×4, while the average increase in the number of Params and FLOPs is only 0.6% and 1.5% of original sizes. Code link: https://github.com/PRIS-CV/FourierSR.
Published: 2026-01-15T20:52:29+00:00
Venue: IEEE Transactions on Image Processing
Score: 0.790 (must_read)</description><content:encoded>&lt;p&gt;&lt;strong&gt;Authors:&lt;/strong&gt; Wenjie Li; Heng Guo; Yuefeng Hou; Zhanyu Ma&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Journal:&lt;/strong&gt; IEEE Transactions on Image Processing&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;a href="https://doi.org/10.1109/tip.2025.3648872"&gt;10.1109/tip.2025.3648872&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Relevance:&lt;/strong&gt; 0.790 (must_read)&lt;/p&gt;
&lt;p&gt;Image super-resolution (SR) aims to recover low-resolution images to high-resolution images, where improving SR efficiency is a high-profile challenge. However, commonly used units in SR, like convolutions and window-based Transformers, have limited receptive fields, making it challenging to apply them to improve SR under extremely limited computational cost. To address this issue, inspired by modeling convolution theorem through token mix, we propose a Fourier token-based plugin called FourierSR to improve SR uniformly, which avoids the instability or inefficiency of existing token mix technologies when applied as plug-ins. Furthermore, compared to convolutions and windows-based Transformers, our FourierSR only utilizes Fourier transform and multiplication operations, greatly reducing complexity while having global receptive fields. Experiments show that our FourierSR as a plugin brings an average PSNR gain of 0.34dB for existing efficient SR methods on Manga109 test set at the scale of ×4, while the average increase in the number of Params and FLOPs is only 0.6% and 1.5% of original sizes. Code link: https://github.com/PRIS-CV/FourierSR.&lt;/p&gt;</content:encoded></item><item><title>Exploiting Class-agnostic Visual Prior for Few-shot Keypoint Detection</title><link>https://doi.org/10.1007/s11263-025-02671-5</link><guid>10.1007/s11263-025-02671-5</guid><pubDate>Fri, 16 Jan 2026 05:21:57 +0000</pubDate><dc:creator>Changsheng Lu</dc:creator><dc:creator>Hao Zhu</dc:creator><dc:creator>Piotr Koniusz</dc:creator><prism:publicationName>International Journal of Computer Vision</prism:publicationName><prism:doi>10.1007/s11263-025-02671-5</prism:doi><description>Abstract Deep learning based keypoint detectors can localize specific object (or body) parts well, but still fall short of general keypoint detection. Instead, few-shot keypoint detection (FSKD) is an underexplored yet more general task of localizing either base or novel keypoints, depending on the prompted support samples. In FSKD, how to build robust keypoint representations is the key to success. To this end, we propose an FSKD approach that models relations between keypoints. As keypoints are located on objects, we exploit a class-agnostic visual prior, i.e ., the unsupervised saliency map or DINO attentiveness map to obtain the region of focus within which we perform relation learning between object patches. The class-agnostic visual prior also helps suppress the background noise largely irrelevant to keypoint locations. Then, we propose a novel Visual Prior guided Vision Transformer (VPViT). The visual prior maps are refined by a bespoke morphology learner to include relevant context of objects. The masked self-attention of VPViT takes the adapted prior map as a soft mask to constrain the self-attention to foregrounds. As robust FSKD must also deal with the low number of support samples and occlusions, based on VPViT, we further investigate i) transductive FSKD to enhance keypoint representations with unlabeled data and ii) FSKD with masking and alignment (MAA) to improve robustness. We show that our model performs well in seven public datasets, and also significantly improves the accuracy in transductive inference and under occlusions. Source codes are available at https://github.com/AlanLuSun/VPViT .
Published: 2026-01-16T05:21:57+00:00
Venue: International Journal of Computer Vision
Score: 0.787 (must_read)</description><content:encoded>&lt;p&gt;&lt;strong&gt;Authors:&lt;/strong&gt; Changsheng Lu; Hao Zhu; Piotr Koniusz&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Journal:&lt;/strong&gt; International Journal of Computer Vision&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;a href="https://doi.org/10.1007/s11263-025-02671-5"&gt;10.1007/s11263-025-02671-5&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Relevance:&lt;/strong&gt; 0.787 (must_read)&lt;/p&gt;
&lt;p&gt;Abstract Deep learning based keypoint detectors can localize specific object (or body) parts well, but still fall short of general keypoint detection. Instead, few-shot keypoint detection (FSKD) is an underexplored yet more general task of localizing either base or novel keypoints, depending on the prompted support samples. In FSKD, how to build robust keypoint representations is the key to success. To this end, we propose an FSKD approach that models relations between keypoints. As keypoints are located on objects, we exploit a class-agnostic visual prior, i.e ., the unsupervised saliency map or DINO attentiveness map to obtain the region of focus within which we perform relation learning between object patches. The class-agnostic visual prior also helps suppress the background noise largely irrelevant to keypoint locations. Then, we propose a novel Visual Prior guided Vision Transformer (VPViT). The visual prior maps are refined by a bespoke morphology learner to include relevant context of objects. The masked self-attention of VPViT takes the adapted prior map as a soft mask to constrain the self-attention to foregrounds. As robust FSKD must also deal with the low number of support samples and occlusions, based on VPViT, we further investigate i) transductive FSKD to enhance keypoint representations with unlabeled data and ii) FSKD with masking and alignment (MAA) to improve robustness. We show that our model performs well in seven public datasets, and also significantly improves the accuracy in transductive inference and under occlusions. Source codes are available at https://github.com/AlanLuSun/VPViT .&lt;/p&gt;</content:encoded></item><item><title>Generative Compositional Zero-Shot Learning Using Learnable Primitive Disparity</title><link>https://doi.org/10.1016/j.knosys.2026.115278</link><guid>10.1016/j.knosys.2026.115278</guid><pubDate>Thu, 15 Jan 2026 07:17:30 +0000</pubDate><dc:creator>Minho Kim</dc:creator><dc:creator>Byeongkeun Kang</dc:creator><dc:creator>Yeejin Lee</dc:creator><prism:publicationName>Knowledge-Based Systems</prism:publicationName><prism:doi>10.1016/j.knosys.2026.115278</prism:doi><description>Compositional zero-shot learning aims to recognize both object and attribute categories from images, including novel attribute–object combinations that are not observed during training. A key challenge is correctly identifying unseen compositions without supervision while avoiding reliance on inconsistent associations between class names and visual content. This issue mainly arises from the use of fixed text embeddings that are directly tied to class labels. To overcome these challenges, we propose a novel framework that learns primitive disparities without depending on textual labels. Our method integrates an embedding-based strategy with a generative framework, an approach that has received limited attention in compositional learning. Specifically, primitive classes are identified by comparing visual and textual representations in a shared embedding space. To improve visual feature quality, we introduce a region-specific feature aggregation strategy that effectively captures attribute-related information. In addition, to mitigate data scarcity in zero-shot learning scenarios, we design a generative module that synthesizes unseen features using metric-learning-based triplets and feature disparity modeling with learnable class features. This module enables feature synthesis in a unified visual space, reducing dependence on text-driven knowledge commonly used in existing methods. The synthesized features are then used to jointly refine both visual and textual representations, leading to improved generalization performance. Extensive experiments on four widely used benchmark datasets demonstrate that our method outperforms state-of-the-art approaches. The code will be released upon publication.
Published: 2026-01-15T07:17:30+00:00
Venue: Knowledge-Based Systems
Score: 0.786 (must_read)</description><content:encoded>&lt;p&gt;&lt;strong&gt;Authors:&lt;/strong&gt; Minho Kim; Byeongkeun Kang; Yeejin Lee&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Journal:&lt;/strong&gt; Knowledge-Based Systems&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;a href="https://doi.org/10.1016/j.knosys.2026.115278"&gt;10.1016/j.knosys.2026.115278&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Relevance:&lt;/strong&gt; 0.786 (must_read)&lt;/p&gt;
&lt;p&gt;Compositional zero-shot learning aims to recognize both object and attribute categories from images, including novel attribute–object combinations that are not observed during training. A key challenge is correctly identifying unseen compositions without supervision while avoiding reliance on inconsistent associations between class names and visual content. This issue mainly arises from the use of fixed text embeddings that are directly tied to class labels. To overcome these challenges, we propose a novel framework that learns primitive disparities without depending on textual labels. Our method integrates an embedding-based strategy with a generative framework, an approach that has received limited attention in compositional learning. Specifically, primitive classes are identified by comparing visual and textual representations in a shared embedding space. To improve visual feature quality, we introduce a region-specific feature aggregation strategy that effectively captures attribute-related information. In addition, to mitigate data scarcity in zero-shot learning scenarios, we design a generative module that synthesizes unseen features using metric-learning-based triplets and feature disparity modeling with learnable class features. This module enables feature synthesis in a unified visual space, reducing dependence on text-driven knowledge commonly used in existing methods. The synthesized features are then used to jointly refine both visual and textual representations, leading to improved generalization performance. Extensive experiments on four widely used benchmark datasets demonstrate that our method outperforms state-of-the-art approaches. The code will be released upon publication.&lt;/p&gt;</content:encoded></item><item><title>FLGF-Unet：融合局部-全局特征的光学遥感图像遥感建筑物提取网络</title><link>https://doi.org/10.11834/jrs.20264516</link><guid>10.11834/jrs.20264516</guid><pubDate>Thu, 15 Jan 2026 08:27:58 +0000</pubDate><dc:creator>LI Guoyan</dc:creator><dc:creator>LIU Tao</dc:creator><dc:creator>WANG Li</dc:creator><dc:creator>LIU Yi</dc:creator><prism:publicationName>National Remote Sensing Bulletin</prism:publicationName><prism:doi>10.11834/jrs.20264516</prism:doi><description>遥感图像的语义分割在城市变化检测、环境保护、地质灾害识别等领域具有重要作用。针对当前遥感建筑物提取中存在的漏检、误检、因树木遮挡或类似物体干扰导致提取不完整等问题，本文基于UNet网络提出一种改进的建筑物提取网络--融合局部-全局特征网络（Fusion of local global features network，FLGF-UNet）。FLGF-UNet的并行特征融合方式确保每个阶段的特征都包含细粒度的局部信息和全局依赖，使得网络在每一阶段的特征表示中同时具备局部和全局信息，有效克服Transformer在局部信息交换上的不足，同时在全局信息建模方面优于传统CNN。此外，为弥补编码器和解码器之间的语义鸿沟，编解码器之间加入交互融合（Interactive Fusion，IF）模块，增强空间细节、全局上下文和语义特征的融合效果。为验证FLGF-UNet的优越性和通用性，在WHU、Massachusetts数据集和中国典型城市建筑物实例数据集上，将所提网络与U2Net、Swin Transformer、MA-Net、HD-Net和RS-Mamba等网络进行对比。结果表明，FLGF-UNet在性能上优于其他SOTA网络，具有较高的实际应用价值。
Published: 2026-01-15T08:27:58+00:00
Venue: National Remote Sensing Bulletin
Score: 0.786 (must_read)</description><content:encoded>&lt;p&gt;&lt;strong&gt;Authors:&lt;/strong&gt; LI Guoyan; LIU Tao; WANG Li; LIU Yi&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Journal:&lt;/strong&gt; National Remote Sensing Bulletin&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;a href="https://doi.org/10.11834/jrs.20264516"&gt;10.11834/jrs.20264516&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Relevance:&lt;/strong&gt; 0.786 (must_read)&lt;/p&gt;
&lt;p&gt;遥感图像的语义分割在城市变化检测、环境保护、地质灾害识别等领域具有重要作用。针对当前遥感建筑物提取中存在的漏检、误检、因树木遮挡或类似物体干扰导致提取不完整等问题，本文基于UNet网络提出一种改进的建筑物提取网络--融合局部-全局特征网络（Fusion of local global features network，FLGF-UNet）。FLGF-UNet的并行特征融合方式确保每个阶段的特征都包含细粒度的局部信息和全局依赖，使得网络在每一阶段的特征表示中同时具备局部和全局信息，有效克服Transformer在局部信息交换上的不足，同时在全局信息建模方面优于传统CNN。此外，为弥补编码器和解码器之间的语义鸿沟，编解码器之间加入交互融合（Interactive Fusion，IF）模块，增强空间细节、全局上下文和语义特征的融合效果。为验证FLGF-UNet的优越性和通用性，在WHU、Massachusetts数据集和中国典型城市建筑物实例数据集上，将所提网络与U2Net、Swin Transformer、MA-Net、HD-Net和RS-Mamba等网络进行对比。结果表明，FLGF-UNet在性能上优于其他SOTA网络，具有较高的实际应用价值。&lt;/p&gt;</content:encoded></item><item><title>MMLGNet: Cross-Modal Alignment of Remote Sensing Data using CLIP</title><link>https://arxiv.org/abs/2601.08420v1</link><guid>http://arxiv.org/abs/2601.08420v1</guid><pubDate>Tue, 13 Jan 2026 10:44:37 +0000</pubDate><dc:creator>Aditya Chaudhary</dc:creator><dc:creator>Sneha Barman</dc:creator><dc:creator>Mainak Singha</dc:creator><dc:creator>Ankit Jha</dc:creator><dc:creator>Girish Mishra</dc:creator><dc:creator>Biplab Banerjee</dc:creator><dc:type>preprint</dc:type><prism:publicationName>arXiv</prism:publicationName><description>In this paper, we propose a novel multimodal framework, Multimodal Language-Guided Network (MMLGNet), to align heterogeneous remote sensing modalities like Hyperspectral Imaging (HSI) and LiDAR with natural language semantics using vision-language models such as CLIP. With the increasing availability of multimodal Earth observation data, there is a growing need for methods that effectively fuse spectral, spatial, and geometric information while enabling semantic-level understanding. MMLGNet employs modality-specific encoders and aligns visual features with handcrafted textual embeddings in a shared latent space via bi-directional contrastive learning. Inspired by CLIP's training paradigm, our approach bridges the gap between high-dimensional remote sensing data and language-guided interpretation. Notably, MMLGNet achieves strong performance with simple CNN-based encoders, outperforming several established multimodal visual-only methods on two benchmark datasets, demonstrating the significant benefit of language supervision. Codes are available at https://github.com/AdityaChaudhary2913/CLIP_HSI.
Published: 2026-01-13T10:44:37+00:00
Venue: arXiv
Score: 0.785 (must_read)</description><content:encoded>&lt;p&gt;&lt;strong&gt;Authors:&lt;/strong&gt; Aditya Chaudhary; Sneha Barman; Mainak Singha; Ankit Jha; Girish Mishra; Biplab Banerjee&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Journal:&lt;/strong&gt; arXiv&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Relevance:&lt;/strong&gt; 0.785 (must_read)&lt;/p&gt;
&lt;p&gt;In this paper, we propose a novel multimodal framework, Multimodal Language-Guided Network (MMLGNet), to align heterogeneous remote sensing modalities like Hyperspectral Imaging (HSI) and LiDAR with natural language semantics using vision-language models such as CLIP. With the increasing availability of multimodal Earth observation data, there is a growing need for methods that effectively fuse spectral, spatial, and geometric information while enabling semantic-level understanding. MMLGNet employs modality-specific encoders and aligns visual features with handcrafted textual embeddings in a shared latent space via bi-directional contrastive learning. Inspired by CLIP&amp;#x27;s training paradigm, our approach bridges the gap between high-dimensional remote sensing data and language-guided interpretation. Notably, MMLGNet achieves strong performance with simple CNN-based encoders, outperforming several established multimodal visual-only methods on two benchmark datasets, demonstrating the significant benefit of language supervision. Codes are available at https://github.com/AdityaChaudhary2913/CLIP_HSI.&lt;/p&gt;</content:encoded></item><item><title>Breaking the Limits of Open-Weight CLIP: An Optimization Framework for Self-supervised Fine-tuning of CLIP</title><link>https://arxiv.org/abs/2601.09859v1</link><guid>http://arxiv.org/abs/2601.09859v1</guid><pubDate>Wed, 14 Jan 2026 20:38:36 +0000</pubDate><dc:creator>Anant Mehta</dc:creator><dc:creator>Xiyuan Wei</dc:creator><dc:creator>Xingyu Chen</dc:creator><dc:creator>Tianbao Yang</dc:creator><dc:type>preprint</dc:type><prism:publicationName>arXiv</prism:publicationName><description>CLIP has become a cornerstone of multimodal representation learning, yet improving its performance typically requires a prohibitively costly process of training from scratch on billions of samples. We ask a different question: Can we improve the performance of open-weight CLIP models across various downstream tasks using only existing self-supervised datasets? Unlike supervised fine-tuning, which adapts a pretrained model to a single downstream task, our setting seeks to improve general performance across various tasks. However, as both our experiments and prior studies reveal, simply applying standard training protocols starting from an open-weight CLIP model often fails, leading to performance degradation. In this paper, we introduce TuneCLIP, a self-supervised fine-tuning framework that overcomes the performance degradation. TuneCLIP has two key components: (1) a warm-up stage of recovering optimization statistics to reduce cold-start bias, inspired by theoretical analysis, and (2) a fine-tuning stage of optimizing a new contrastive loss to mitigate the penalization on false negative pairs. Our extensive experiments show that TuneCLIP consistently improves performance across model architectures and scales. Notably, it elevates leading open-weight models like SigLIP (ViT-B/16), achieving gains of up to +2.5% on ImageNet and related out-of-distribution benchmarks, and +1.2% on the highly competitive DataComp benchmark, setting a new strong baseline for efficient post-pretraining adaptation.
Published: 2026-01-14T20:38:36+00:00
Venue: arXiv
Score: 0.784 (must_read)</description><content:encoded>&lt;p&gt;&lt;strong&gt;Authors:&lt;/strong&gt; Anant Mehta; Xiyuan Wei; Xingyu Chen; Tianbao Yang&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Journal:&lt;/strong&gt; arXiv&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Relevance:&lt;/strong&gt; 0.784 (must_read)&lt;/p&gt;
&lt;p&gt;CLIP has become a cornerstone of multimodal representation learning, yet improving its performance typically requires a prohibitively costly process of training from scratch on billions of samples. We ask a different question: Can we improve the performance of open-weight CLIP models across various downstream tasks using only existing self-supervised datasets? Unlike supervised fine-tuning, which adapts a pretrained model to a single downstream task, our setting seeks to improve general performance across various tasks. However, as both our experiments and prior studies reveal, simply applying standard training protocols starting from an open-weight CLIP model often fails, leading to performance degradation. In this paper, we introduce TuneCLIP, a self-supervised fine-tuning framework that overcomes the performance degradation. TuneCLIP has two key components: (1) a warm-up stage of recovering optimization statistics to reduce cold-start bias, inspired by theoretical analysis, and (2) a fine-tuning stage of optimizing a new contrastive loss to mitigate the penalization on false negative pairs. Our extensive experiments show that TuneCLIP consistently improves performance across model architectures and scales. Notably, it elevates leading open-weight models like SigLIP (ViT-B/16), achieving gains of up to +2.5% on ImageNet and related out-of-distribution benchmarks, and +1.2% on the highly competitive DataComp benchmark, setting a new strong baseline for efficient post-pretraining adaptation.&lt;/p&gt;</content:encoded></item><item><title>Image Super-Resolution using Hierarchical Cross-Scale Self-Similarity</title><link>https://doi.org/10.1109/tmm.2026.3654442</link><guid>10.1109/tmm.2026.3654442</guid><pubDate>Thu, 15 Jan 2026 20:50:42 +0000</pubDate><dc:creator>Xiancheng Zhu</dc:creator><dc:creator>Detian Huang</dc:creator><dc:creator>Taiheng Zeng</dc:creator><dc:creator>Xiaoqian Huang</dc:creator><dc:creator>Zhenzhen Hu</dc:creator><dc:creator>Huanqiang Zeng</dc:creator><prism:publicationName>IEEE Transactions on Multimedia</prism:publicationName><prism:doi>10.1109/tmm.2026.3654442</prism:doi><description>Previous studies have revealed that extending the spatial range of informative pixels offers positive performance gains for image Super-Resolution (SR). To activate more informative pixels, considerable efforts have been devoted to exploring various variants of non-local attention mechanisms for capturing image self-similarity. However, even the state-of-the-art non-local attention mechanisms ignore an inherent property of images, namely hierarchical cross-scale self-similarity. In this paper, we propose the first Hierarchical Cross-Scale Attention (HCSA). Specifically, we first extend the search space to multiple feature maps from a single feature map, and then model cross-scale feature correspondences among different layers. This allows HCSA to activate more informative pixels for image SR by adaptively rescaling and aggregating input pixels and large-scale patches within different feature maps. To ensure accurate cross scale feature matching, we propose to replace plain down sampling operations (e.g., interpolation, pooling) with Haar Wavelet Transform (HWT) encoding, which transfers spatial information of feature maps into the channel dimension, effectively avoiding important information loss. Considering that softmax normal ization in the standard non-local attention often leads to homogeneous feature aggregation due to the amplification of small similarity weights, we propose a simple yet effective Adaptive Selection (AS) operator. This operator generates a learnable sparse mask to remove redundant features, enabling HCSA to perform discriminative feature aggregation. As a generic building block, the proposed HCSA can be flexibly integrated into existing CNN- or Transformer-based SR models, significantly strengthening cross-layer information interaction and cross-scale feature representation. Quantitative and qualitative results demonstrate that our HCSA facilitates existing SR models to achieve superior accuracy and visual quality.
Published: 2026-01-15T20:50:42+00:00
Venue: IEEE Transactions on Multimedia
Score: 0.784 (must_read)</description><content:encoded>&lt;p&gt;&lt;strong&gt;Authors:&lt;/strong&gt; Xiancheng Zhu; Detian Huang; Taiheng Zeng; Xiaoqian Huang; Zhenzhen Hu; Huanqiang Zeng&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Journal:&lt;/strong&gt; IEEE Transactions on Multimedia&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;a href="https://doi.org/10.1109/tmm.2026.3654442"&gt;10.1109/tmm.2026.3654442&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Relevance:&lt;/strong&gt; 0.784 (must_read)&lt;/p&gt;
&lt;p&gt;Previous studies have revealed that extending the spatial range of informative pixels offers positive performance gains for image Super-Resolution (SR). To activate more informative pixels, considerable efforts have been devoted to exploring various variants of non-local attention mechanisms for capturing image self-similarity. However, even the state-of-the-art non-local attention mechanisms ignore an inherent property of images, namely hierarchical cross-scale self-similarity. In this paper, we propose the first Hierarchical Cross-Scale Attention (HCSA). Specifically, we first extend the search space to multiple feature maps from a single feature map, and then model cross-scale feature correspondences among different layers. This allows HCSA to activate more informative pixels for image SR by adaptively rescaling and aggregating input pixels and large-scale patches within different feature maps. To ensure accurate cross scale feature matching, we propose to replace plain down sampling operations (e.g., interpolation, pooling) with Haar Wavelet Transform (HWT) encoding, which transfers spatial information of feature maps into the channel dimension, effectively avoiding important information loss. Considering that softmax normal ization in the standard non-local attention often leads to homogeneous feature aggregation due to the amplification of small similarity weights, we propose a simple yet effective Adaptive Selection (AS) operator. This operator generates a learnable sparse mask to remove redundant features, enabling HCSA to perform discriminative feature aggregation. As a generic building block, the proposed HCSA can be flexibly integrated into existing CNN- or Transformer-based SR models, significantly strengthening cross-layer information interaction and cross-scale feature representation. Quantitative and qualitative results demonstrate that our HCSA facilitates existing SR models to achieve superior accuracy and visual quality.&lt;/p&gt;</content:encoded></item><item><title>EfficientFSL: Enhancing Few-Shot Classification via Query-Only Tuning in Vision Transformers</title><link>https://arxiv.org/abs/2601.08499v2</link><guid>http://arxiv.org/abs/2601.08499v2</guid><pubDate>Tue, 13 Jan 2026 12:33:02 +0000</pubDate><dc:creator>Wenwen Liao</dc:creator><dc:creator>Hang Ruan</dc:creator><dc:creator>Jianbo Yu</dc:creator><dc:creator>Bing Song</dc:creator><dc:creator>YuansongWang</dc:creator><dc:creator>Xiaofeng Yang</dc:creator><dc:type>preprint</dc:type><prism:publicationName>arXiv</prism:publicationName><description>Large models such as Vision Transformers (ViTs) have demonstrated remarkable superiority over smaller architectures like ResNet in few-shot classification, owing to their powerful representational capacity. However, fine-tuning such large models demands extensive GPU memory and prolonged training time, making them impractical for many real-world low-resource scenarios. To bridge this gap, we propose EfficientFSL, a query-only fine-tuning framework tailored specifically for few-shot classification with ViT, which achieves competitive performance while significantly reducing computational overhead. EfficientFSL fully leverages the knowledge embedded in the pre-trained model and its strong comprehension ability, achieving high classification accuracy with an extremely small number of tunable parameters. Specifically, we introduce a lightweight trainable Forward Block to synthesize task-specific queries that extract informative features from the intermediate representations of the pre-trained model in a query-only manner. We further propose a Combine Block to fuse multi-layer outputs, enhancing the depth and robustness of feature representations. Finally, a Support-Query Attention Block mitigates distribution shift by adjusting prototypes to align with the query set distribution. With minimal trainable parameters, EfficientFSL achieves state-of-the-art performance on four in-domain few-shot datasets and six cross-domain datasets, demonstrating its effectiveness in real-world applications.
Published: 2026-01-13T12:33:02+00:00
Venue: arXiv
Score: 0.784 (must_read)</description><content:encoded>&lt;p&gt;&lt;strong&gt;Authors:&lt;/strong&gt; Wenwen Liao; Hang Ruan; Jianbo Yu; Bing Song; YuansongWang; Xiaofeng Yang&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Journal:&lt;/strong&gt; arXiv&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Relevance:&lt;/strong&gt; 0.784 (must_read)&lt;/p&gt;
&lt;p&gt;Large models such as Vision Transformers (ViTs) have demonstrated remarkable superiority over smaller architectures like ResNet in few-shot classification, owing to their powerful representational capacity. However, fine-tuning such large models demands extensive GPU memory and prolonged training time, making them impractical for many real-world low-resource scenarios. To bridge this gap, we propose EfficientFSL, a query-only fine-tuning framework tailored specifically for few-shot classification with ViT, which achieves competitive performance while significantly reducing computational overhead. EfficientFSL fully leverages the knowledge embedded in the pre-trained model and its strong comprehension ability, achieving high classification accuracy with an extremely small number of tunable parameters. Specifically, we introduce a lightweight trainable Forward Block to synthesize task-specific queries that extract informative features from the intermediate representations of the pre-trained model in a query-only manner. We further propose a Combine Block to fuse multi-layer outputs, enhancing the depth and robustness of feature representations. Finally, a Support-Query Attention Block mitigates distribution shift by adjusting prototypes to align with the query set distribution. With minimal trainable parameters, EfficientFSL achieves state-of-the-art performance on four in-domain few-shot datasets and six cross-domain datasets, demonstrating its effectiveness in real-world applications.&lt;/p&gt;</content:encoded></item><item><title>SAM3-DMS: Decoupled Memory Selection for Multi-target Video Segmentation of SAM3</title><link>https://arxiv.org/abs/2601.09699v1</link><guid>http://arxiv.org/abs/2601.09699v1</guid><pubDate>Wed, 14 Jan 2026 18:52:14 +0000</pubDate><dc:creator>Ruiqi Shen</dc:creator><dc:creator>Chang Liu</dc:creator><dc:creator>Henghui Ding</dc:creator><dc:type>preprint</dc:type><prism:publicationName>arXiv</prism:publicationName><description>Segment Anything 3 (SAM3) has established a powerful foundation that robustly detects, segments, and tracks specified targets in videos. However, in its original implementation, its group-level collective memory selection is suboptimal for complex multi-object scenarios, as it employs a synchronized decision across all concurrent targets conditioned on their average performance, often overlooking individual reliability. To this end, we propose SAM3-DMS, a training-free decoupled strategy that utilizes fine-grained memory selection on individual objects. Experiments demonstrate that our approach achieves robust identity preservation and tracking stability. Notably, our advantage becomes more pronounced with increased target density, establishing a solid foundation for simultaneous multi-target video segmentation in the wild.
Published: 2026-01-14T18:52:14+00:00
Venue: arXiv
Score: 0.782 (must_read)</description><content:encoded>&lt;p&gt;&lt;strong&gt;Authors:&lt;/strong&gt; Ruiqi Shen; Chang Liu; Henghui Ding&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Journal:&lt;/strong&gt; arXiv&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Relevance:&lt;/strong&gt; 0.782 (must_read)&lt;/p&gt;
&lt;p&gt;Segment Anything 3 (SAM3) has established a powerful foundation that robustly detects, segments, and tracks specified targets in videos. However, in its original implementation, its group-level collective memory selection is suboptimal for complex multi-object scenarios, as it employs a synchronized decision across all concurrent targets conditioned on their average performance, often overlooking individual reliability. To this end, we propose SAM3-DMS, a training-free decoupled strategy that utilizes fine-grained memory selection on individual objects. Experiments demonstrate that our approach achieves robust identity preservation and tracking stability. Notably, our advantage becomes more pronounced with increased target density, establishing a solid foundation for simultaneous multi-target video segmentation in the wild.&lt;/p&gt;</content:encoded></item><item><title>MMCPose: Multimodal Condition-Driven 3D Human Pose Estimation Via Diffusion Models</title><link>https://doi.org/10.1109/tmm.2026.3654424</link><guid>10.1109/tmm.2026.3654424</guid><pubDate>Thu, 15 Jan 2026 20:50:42 +0000</pubDate><dc:creator>Xixia Xu</dc:creator><dc:creator>Jiamao Li</dc:creator><prism:publicationName>IEEE Transactions on Multimedia</prism:publicationName><prism:doi>10.1109/tmm.2026.3654424</prism:doi><description>Nowadays, diffusion-based methods for monocular 3D human pose estimation (3D HPE) have achieved state-of-the-art performance by directly regressing the 3D joint coordinates from the 2D observations. Although some methods incorporated the human body prior to improve the denoising quality, the absense of the structural relation and pose-aware guidance make these models prone to generating unreasonable poses. The challenge is noticeable in complex conditions such as occlusions and crowded scenarios. To alleviate this, we present MMCPose, a novel Multi-modal Condition-driven 3D HPE framework via diffusion models that capitalizes on the benefits of the multi-modal conditioning input. Specifically, we propose Multi-modal Condition Learning (MCL) strategy to incorporate multi-modal conditions such as joint- wise relation, part-aware prompt and pose-aware mask to improve the generation quality. The MCL block consists of (i) Joint- wise Relation Condition Learning (JRCL) models the flexible joint- wise relation via GCN to mitigate disturbances arising from confused joints. (ii) Part-aware Prompt Condition Learning (PPCL) constructs multi-granular prompts via accessible texts and feasible knowledge of body parts with learnable prompts to model implicit textual guidance. (iii) Pose-aware Mask Condition Learning (PMCL) designs a pose-specific mask to increase the model's emphasis to the pose region, augmenting the precision in capturing intricate pose details. Furthermore, we explore a multi-modal condition-pose interaction learning (MCPI) mechanism to establish interaction between the learned multi-modal conditions and poses to maximize the power of condition effect. This method fully unleashes the perceptual capability of the multi-modal conditions in diffusion-based 3D HPE. Extensive evaluations conducted on two popular benchmarks (e.g., Human3.6 M, MPI-INF-3DHP) and achieve new state-of-the-art performance.
Published: 2026-01-15T20:50:42+00:00
Venue: IEEE Transactions on Multimedia
Score: 0.782 (must_read)</description><content:encoded>&lt;p&gt;&lt;strong&gt;Authors:&lt;/strong&gt; Xixia Xu; Jiamao Li&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Journal:&lt;/strong&gt; IEEE Transactions on Multimedia&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;a href="https://doi.org/10.1109/tmm.2026.3654424"&gt;10.1109/tmm.2026.3654424&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Relevance:&lt;/strong&gt; 0.782 (must_read)&lt;/p&gt;
&lt;p&gt;Nowadays, diffusion-based methods for monocular 3D human pose estimation (3D HPE) have achieved state-of-the-art performance by directly regressing the 3D joint coordinates from the 2D observations. Although some methods incorporated the human body prior to improve the denoising quality, the absense of the structural relation and pose-aware guidance make these models prone to generating unreasonable poses. The challenge is noticeable in complex conditions such as occlusions and crowded scenarios. To alleviate this, we present MMCPose, a novel Multi-modal Condition-driven 3D HPE framework via diffusion models that capitalizes on the benefits of the multi-modal conditioning input. Specifically, we propose Multi-modal Condition Learning (MCL) strategy to incorporate multi-modal conditions such as joint- wise relation, part-aware prompt and pose-aware mask to improve the generation quality. The MCL block consists of (i) Joint- wise Relation Condition Learning (JRCL) models the flexible joint- wise relation via GCN to mitigate disturbances arising from confused joints. (ii) Part-aware Prompt Condition Learning (PPCL) constructs multi-granular prompts via accessible texts and feasible knowledge of body parts with learnable prompts to model implicit textual guidance. (iii) Pose-aware Mask Condition Learning (PMCL) designs a pose-specific mask to increase the model&amp;#x27;s emphasis to the pose region, augmenting the precision in capturing intricate pose details. Furthermore, we explore a multi-modal condition-pose interaction learning (MCPI) mechanism to establish interaction between the learned multi-modal conditions and poses to maximize the power of condition effect. This method fully unleashes the perceptual capability of the multi-modal conditions in diffusion-based 3D HPE. Extensive evaluations conducted on two popular benchmarks (e.g., Human3.6 M, MPI-INF-3DHP) and achieve new state-of-the-art performance.&lt;/p&gt;</content:encoded></item><item><title>Beyond the final layer: Attentive multilayer fusion for vision transformers</title><link>https://arxiv.org/abs/2601.09322v1</link><guid>http://arxiv.org/abs/2601.09322v1</guid><pubDate>Wed, 14 Jan 2026 09:50:09 +0000</pubDate><dc:creator>Laure Ciernik</dc:creator><dc:creator>Marco Morik</dc:creator><dc:creator>Lukas Thede</dc:creator><dc:creator>Luca Eyring</dc:creator><dc:creator>Shinichi Nakajima</dc:creator><dc:creator>Zeynep Akata</dc:creator><dc:creator>Lukas Muttenthaler</dc:creator><dc:type>preprint</dc:type><prism:publicationName>arXiv</prism:publicationName><description>With the rise of large-scale foundation models, efficiently adapting them to downstream tasks remains a central challenge. Linear probing, which freezes the backbone and trains a lightweight head, is computationally efficient but often restricted to last-layer representations. We show that task-relevant information is distributed across the network hierarchy rather than solely encoded in any of the last layers. To leverage this distribution of information, we apply an attentive probing mechanism that dynamically fuses representations from all layers of a Vision Transformer. This mechanism learns to identify the most relevant layers for a target task and combines low-level structural cues with high-level semantic abstractions. Across 20 diverse datasets and multiple pretrained foundation models, our method achieves consistent, substantial gains over standard linear probes. Attention heatmaps further reveal that tasks different from the pre-training domain benefit most from intermediate representations. Overall, our findings underscore the value of intermediate layer information and demonstrate a principled, task aware approach for unlocking their potential in probing-based adaptation.
Published: 2026-01-14T09:50:09+00:00
Venue: arXiv
Score: 0.780 (must_read)</description><content:encoded>&lt;p&gt;&lt;strong&gt;Authors:&lt;/strong&gt; Laure Ciernik; Marco Morik; Lukas Thede; Luca Eyring; Shinichi Nakajima; Zeynep Akata; Lukas Muttenthaler&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Journal:&lt;/strong&gt; arXiv&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Relevance:&lt;/strong&gt; 0.780 (must_read)&lt;/p&gt;
&lt;p&gt;With the rise of large-scale foundation models, efficiently adapting them to downstream tasks remains a central challenge. Linear probing, which freezes the backbone and trains a lightweight head, is computationally efficient but often restricted to last-layer representations. We show that task-relevant information is distributed across the network hierarchy rather than solely encoded in any of the last layers. To leverage this distribution of information, we apply an attentive probing mechanism that dynamically fuses representations from all layers of a Vision Transformer. This mechanism learns to identify the most relevant layers for a target task and combines low-level structural cues with high-level semantic abstractions. Across 20 diverse datasets and multiple pretrained foundation models, our method achieves consistent, substantial gains over standard linear probes. Attention heatmaps further reveal that tasks different from the pre-training domain benefit most from intermediate representations. Overall, our findings underscore the value of intermediate layer information and demonstrate a principled, task aware approach for unlocking their potential in probing-based adaptation.&lt;/p&gt;</content:encoded></item><item><title>YOLOBirDrone: Dataset for Bird vs Drone Detection and Classification and a YOLO based enhanced learning architecture</title><link>https://arxiv.org/abs/2601.08319v1</link><guid>http://arxiv.org/abs/2601.08319v1</guid><pubDate>Tue, 13 Jan 2026 08:17:28 +0000</pubDate><dc:creator>Dapinder Kaur</dc:creator><dc:creator>Neeraj Battish</dc:creator><dc:creator>Arnav Bhavsar</dc:creator><dc:creator>Shashi Poddar</dc:creator><dc:type>preprint</dc:type><prism:publicationName>arXiv</prism:publicationName><description>The use of aerial drones for commercial and defense applications has benefited in many ways and is therefore utilized in several different application domains. However, they are also increasingly used for targeted attacks, posing a significant safety challenge and necessitating the development of drone detection systems. Vision-based drone detection systems currently have an accuracy limitation and struggle to distinguish between drones and birds, particularly when the birds are small in size. This research work proposes a novel YOLOBirDrone architecture that improves the detection and classification accuracy of birds and drones. YOLOBirDrone has different components, including an adaptive and extended layer aggregation (AELAN), a multi-scale progressive dual attention module (MPDA), and a reverse MPDA (RMPDA) to preserve shape information and enrich features with local and global spatial and channel information. A large-scale dataset, BirDrone, is also introduced in this article, which includes small and challenging objects for robust aerial object identification. Experimental results demonstrate an improvement in performance metrics through the proposed YOLOBirDrone architecture compared to other state-of-the-art algorithms, with detection accuracy reaching approximately 85% across various scenarios.
Published: 2026-01-13T08:17:28+00:00
Venue: arXiv
Score: 0.780 (must_read)</description><content:encoded>&lt;p&gt;&lt;strong&gt;Authors:&lt;/strong&gt; Dapinder Kaur; Neeraj Battish; Arnav Bhavsar; Shashi Poddar&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Journal:&lt;/strong&gt; arXiv&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Relevance:&lt;/strong&gt; 0.780 (must_read)&lt;/p&gt;
&lt;p&gt;The use of aerial drones for commercial and defense applications has benefited in many ways and is therefore utilized in several different application domains. However, they are also increasingly used for targeted attacks, posing a significant safety challenge and necessitating the development of drone detection systems. Vision-based drone detection systems currently have an accuracy limitation and struggle to distinguish between drones and birds, particularly when the birds are small in size. This research work proposes a novel YOLOBirDrone architecture that improves the detection and classification accuracy of birds and drones. YOLOBirDrone has different components, including an adaptive and extended layer aggregation (AELAN), a multi-scale progressive dual attention module (MPDA), and a reverse MPDA (RMPDA) to preserve shape information and enrich features with local and global spatial and channel information. A large-scale dataset, BirDrone, is also introduced in this article, which includes small and challenging objects for robust aerial object identification. Experimental results demonstrate an improvement in performance metrics through the proposed YOLOBirDrone architecture compared to other state-of-the-art algorithms, with detection accuracy reaching approximately 85% across various scenarios.&lt;/p&gt;</content:encoded></item><item><title>Gradient-Prior-Guided Dual-Branch Network for Preserving Fine Structures in Remote Sensing Image Super-Resolution</title><link>https://doi.org/10.1109/tgrs.2026.3654769</link><guid>10.1109/tgrs.2026.3654769</guid><pubDate>Thu, 15 Jan 2026 20:49:54 +0000</pubDate><dc:creator>Ruyi Feng</dc:creator><dc:creator>Zhijie Zhang</dc:creator><dc:creator>Lizhe Wang</dc:creator><prism:publicationName>IEEE Transactions on Geoscience and Remote Sensing</prism:publicationName><prism:doi>10.1109/tgrs.2026.3654769</prism:doi><description>In the task of remote sensing image super-resolution reconstruction, both CNN and Transformer have demonstrated impressive capabilities. However, many existing dual-branch models only perform feature fusion through simple operations such as concatenation or weighting, which fail to fully exploit the complementary advantages of CNN and Transformer in feature representation. To address this limitation, this paper proposes a novel Gradient-Prior-Guided Dual-Branch Network (GPG-DBN) for remote sensing image super-resolution. Specifically, the model incorporates an enhanced Transformer structure to capture long-range dependencies and a CNN branch to extract local structural details. To further enhance detail recovery, a Gradient-aware Enhancement Module (GEM) is introduced to guide the network using explicit gradient priors, effectively highlighting high-frequency information such as edges and contours. In addition, we design a Feature Fusion Module (FFM) to deeply integrate the global and local features from both branches, achieving more effective information interaction and complementary learning. Extensive experiments conducted on the UC-Merced and WHU-RS19 datasets demonstrate that the proposed GPG-DBN achieves superior performance in both quantitative metrics and visual quality compared to existing state-of-the-art methods, validating its effectiveness and robustness.
Published: 2026-01-15T20:49:54+00:00
Venue: IEEE Transactions on Geoscience and Remote Sensing
Score: 0.780 (must_read)</description><content:encoded>&lt;p&gt;&lt;strong&gt;Authors:&lt;/strong&gt; Ruyi Feng; Zhijie Zhang; Lizhe Wang&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Journal:&lt;/strong&gt; IEEE Transactions on Geoscience and Remote Sensing&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;a href="https://doi.org/10.1109/tgrs.2026.3654769"&gt;10.1109/tgrs.2026.3654769&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Relevance:&lt;/strong&gt; 0.780 (must_read)&lt;/p&gt;
&lt;p&gt;In the task of remote sensing image super-resolution reconstruction, both CNN and Transformer have demonstrated impressive capabilities. However, many existing dual-branch models only perform feature fusion through simple operations such as concatenation or weighting, which fail to fully exploit the complementary advantages of CNN and Transformer in feature representation. To address this limitation, this paper proposes a novel Gradient-Prior-Guided Dual-Branch Network (GPG-DBN) for remote sensing image super-resolution. Specifically, the model incorporates an enhanced Transformer structure to capture long-range dependencies and a CNN branch to extract local structural details. To further enhance detail recovery, a Gradient-aware Enhancement Module (GEM) is introduced to guide the network using explicit gradient priors, effectively highlighting high-frequency information such as edges and contours. In addition, we design a Feature Fusion Module (FFM) to deeply integrate the global and local features from both branches, achieving more effective information interaction and complementary learning. Extensive experiments conducted on the UC-Merced and WHU-RS19 datasets demonstrate that the proposed GPG-DBN achieves superior performance in both quantitative metrics and visual quality compared to existing state-of-the-art methods, validating its effectiveness and robustness.&lt;/p&gt;</content:encoded></item><item><title>The Spatial Blindspot of Vision-Language Models</title><link>https://arxiv.org/abs/2601.09954v1</link><guid>http://arxiv.org/abs/2601.09954v1</guid><pubDate>Thu, 15 Jan 2026 00:30:34 +0000</pubDate><dc:creator>Nahid Alam</dc:creator><dc:creator>Leema Krishna Murali</dc:creator><dc:creator>Siddhant Bharadwaj</dc:creator><dc:creator>Patrick Liu</dc:creator><dc:creator>Timothy Chung</dc:creator><dc:creator>Drishti Sharma</dc:creator><dc:creator>Akshata A</dc:creator><dc:creator>Kranthi Kiran</dc:creator><dc:creator>Wesley Tam</dc:creator><dc:creator>Bala Krishna S Vegesna</dc:creator><dc:type>preprint</dc:type><prism:publicationName>arXiv</prism:publicationName><description>Vision-language models (VLMs) have advanced rapidly, but their ability to capture spatial relationships remains a blindspot. Current VLMs are typically built with contrastive language-image pretraining (CLIP) style image encoders. The training recipe often flattens images into 1D patch sequences, discarding the 2D structure necessary for spatial reasoning. We argue that this lack of spatial awareness is a missing dimension in VLM design and a bottleneck for applications requiring spatial grounding, such as robotics and embodied AI. To address this, we investigate (i) image encoders trained with alternative objectives and (ii) 2D positional encodings. Our experiments show that these architectural choices can lead to improved spatial reasoning on several benchmarks.
Published: 2026-01-15T00:30:34+00:00
Venue: arXiv
Score: 0.779 (must_read)</description><content:encoded>&lt;p&gt;&lt;strong&gt;Authors:&lt;/strong&gt; Nahid Alam; Leema Krishna Murali; Siddhant Bharadwaj; Patrick Liu; Timothy Chung; Drishti Sharma; Akshata A; Kranthi Kiran; Wesley Tam; Bala Krishna S Vegesna&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Journal:&lt;/strong&gt; arXiv&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Relevance:&lt;/strong&gt; 0.779 (must_read)&lt;/p&gt;
&lt;p&gt;Vision-language models (VLMs) have advanced rapidly, but their ability to capture spatial relationships remains a blindspot. Current VLMs are typically built with contrastive language-image pretraining (CLIP) style image encoders. The training recipe often flattens images into 1D patch sequences, discarding the 2D structure necessary for spatial reasoning. We argue that this lack of spatial awareness is a missing dimension in VLM design and a bottleneck for applications requiring spatial grounding, such as robotics and embodied AI. To address this, we investigate (i) image encoders trained with alternative objectives and (ii) 2D positional encodings. Our experiments show that these architectural choices can lead to improved spatial reasoning on several benchmarks.&lt;/p&gt;</content:encoded></item><item><title>DS3former: Dual-Stream Semantic Separation Transformer for Single Image Reflection Separation</title><link>https://doi.org/10.1109/tmm.2026.3654343</link><guid>10.1109/tmm.2026.3654343</guid><pubDate>Thu, 15 Jan 2026 20:50:42 +0000</pubDate><dc:creator>Wenbin Yin</dc:creator><dc:creator>Junkang Zhang</dc:creator><dc:creator>Faming Fang</dc:creator><dc:creator>Guixu Zhang</dc:creator><prism:publicationName>IEEE Transactions on Multimedia</prism:publicationName><prism:doi>10.1109/tmm.2026.3654343</prism:doi><description>Single image reflection separation is a challenging and ill-posed problem owing to diverse reflective surfaces and lighting conditions. This paper introduces DS3former, a dual-stream transformer network employing a semantic separation strategy to effectively distinguish between the transmission (T) and reflection (R) layers. We observe that within pre-trained deep semantic features of mixed images, individual channels exhibit varying affinities towards either the T or R layer, facilitating their differentiation. Based on this observation, we propose a novel semantic separation attention mechanism that adaptively extracts layer-specific features from different channels and performs inter-stream feature transfer and aggregation to enhance separation. To further improve performance at the semantic level, features from deeper decoder stages and external pre-trained models are integrated to guide the separation process in shallower encoder layers. Experimental results show that the proposed method outperforms state-of-the-art reflection separation methods in terms of quantitative metrics and visual quality.
Published: 2026-01-15T20:50:42+00:00
Venue: IEEE Transactions on Multimedia
Score: 0.779 (must_read)</description><content:encoded>&lt;p&gt;&lt;strong&gt;Authors:&lt;/strong&gt; Wenbin Yin; Junkang Zhang; Faming Fang; Guixu Zhang&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Journal:&lt;/strong&gt; IEEE Transactions on Multimedia&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;a href="https://doi.org/10.1109/tmm.2026.3654343"&gt;10.1109/tmm.2026.3654343&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Relevance:&lt;/strong&gt; 0.779 (must_read)&lt;/p&gt;
&lt;p&gt;Single image reflection separation is a challenging and ill-posed problem owing to diverse reflective surfaces and lighting conditions. This paper introduces DS3former, a dual-stream transformer network employing a semantic separation strategy to effectively distinguish between the transmission (T) and reflection (R) layers. We observe that within pre-trained deep semantic features of mixed images, individual channels exhibit varying affinities towards either the T or R layer, facilitating their differentiation. Based on this observation, we propose a novel semantic separation attention mechanism that adaptively extracts layer-specific features from different channels and performs inter-stream feature transfer and aggregation to enhance separation. To further improve performance at the semantic level, features from deeper decoder stages and external pre-trained models are integrated to guide the separation process in shallower encoder layers. Experimental results show that the proposed method outperforms state-of-the-art reflection separation methods in terms of quantitative metrics and visual quality.&lt;/p&gt;</content:encoded></item><item><title>Source-Free Domain Adaptation for Geospatial Point Cloud Semantic Segmentation</title><link>https://arxiv.org/abs/2601.08375v1</link><guid>http://arxiv.org/abs/2601.08375v1</guid><pubDate>Tue, 13 Jan 2026 09:37:57 +0000</pubDate><dc:creator>Yuan Gao</dc:creator><dc:creator>Di Cao</dc:creator><dc:creator>Xiaohuan Xi</dc:creator><dc:creator>Sheng Nie</dc:creator><dc:creator>Shaobo Xia</dc:creator><dc:creator>Cheng Wang</dc:creator><dc:type>preprint</dc:type><prism:publicationName>arXiv</prism:publicationName><description>Semantic segmentation of 3D geospatial point clouds is pivotal for remote sensing applications. However, variations in geographic patterns across regions and data acquisition strategies induce significant domain shifts, severely degrading the performance of deployed models. Existing domain adaptation methods typically rely on access to source-domain data. However, this requirement is rarely met due to data privacy concerns, regulatory policies, and data transmission limitations. This motivates the largely underexplored setting of source-free unsupervised domain adaptation (SFUDA), where only a pretrained model and unlabeled target-domain data are available. In this paper, we propose LoGo (Local-Global Dual-Consensus), a novel SFUDA framework specifically designed for geospatial point clouds. At the local level, we introduce a class-balanced prototype estimation module that abandons conventional global threshold filtering in favor of an intra-class independent anchor mining strategy. This ensures that robust feature prototypes can be generated even for sample-scarce tail classes, effectively mitigating the feature collapse caused by long-tailed distributions. At the global level, we introduce an optimal transport-based global distribution alignment module that formulates pseudo-label assignment as a global optimization problem. By enforcing global distribution constraints, this module effectively corrects the over-dominance of head classes inherent in local greedy assignments, preventing model predictions from being severely biased towards majority classes. Finally, we propose a dual-consistency pseudo-label filtering mechanism. This strategy retains only high-confidence pseudo-labels where local multi-augmented ensemble predictions align with global optimal transport assignments for self-training.
Published: 2026-01-13T09:37:57+00:00
Venue: arXiv
Score: 0.778 (must_read)</description><content:encoded>&lt;p&gt;&lt;strong&gt;Authors:&lt;/strong&gt; Yuan Gao; Di Cao; Xiaohuan Xi; Sheng Nie; Shaobo Xia; Cheng Wang&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Journal:&lt;/strong&gt; arXiv&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Relevance:&lt;/strong&gt; 0.778 (must_read)&lt;/p&gt;
&lt;p&gt;Semantic segmentation of 3D geospatial point clouds is pivotal for remote sensing applications. However, variations in geographic patterns across regions and data acquisition strategies induce significant domain shifts, severely degrading the performance of deployed models. Existing domain adaptation methods typically rely on access to source-domain data. However, this requirement is rarely met due to data privacy concerns, regulatory policies, and data transmission limitations. This motivates the largely underexplored setting of source-free unsupervised domain adaptation (SFUDA), where only a pretrained model and unlabeled target-domain data are available. In this paper, we propose LoGo (Local-Global Dual-Consensus), a novel SFUDA framework specifically designed for geospatial point clouds. At the local level, we introduce a class-balanced prototype estimation module that abandons conventional global threshold filtering in favor of an intra-class independent anchor mining strategy. This ensures that robust feature prototypes can be generated even for sample-scarce tail classes, effectively mitigating the feature collapse caused by long-tailed distributions. At the global level, we introduce an optimal transport-based global distribution alignment module that formulates pseudo-label assignment as a global optimization problem. By enforcing global distribution constraints, this module effectively corrects the over-dominance of head classes inherent in local greedy assignments, preventing model predictions from being severely biased towards majority classes. Finally, we propose a dual-consistency pseudo-label filtering mechanism. This strategy retains only high-confidence pseudo-labels where local multi-augmented ensemble predictions align with global optimal transport assignments for self-training.&lt;/p&gt;</content:encoded></item></channel></rss>