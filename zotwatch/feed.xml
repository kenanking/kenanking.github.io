<?xml version='1.0' encoding='utf-8'?>
<rss version="2.0"><channel><title>ZotWatch Feed</title><link>https://ehehe.cn/zotwatch/</link><description>AI-assisted literature watch</description><lastBuildDate>Fri, 28 Nov 2025 17:32:38 +0000</lastBuildDate><item><title>MMT: Multimodal meta-training for few-shot object detection</title><link>https://doi.org/10.1016/j.neucom.2025.132197</link><guid>10.1016/j.neucom.2025.132197</guid><pubDate>Thu, 27 Nov 2025 07:57:02 +0000</pubDate><category>Neurocomputing</category><description>Few-Shot Object Detection (FSOD) aims to detect objects from novel classes using only a few labeled instances per class. Recently, several FSOD approaches have incorporated vision-language models (VLMs) to leverage textual semantics for improving visual representations. However, VLM-based FSOD methods still face two major challenges: (1) the alignment bias between textual and regional features, which leads to unstable or suboptimal performance on novel categories; and (2) the lack of efficient training strategies, as most methods rely on repeatedly fine-tuning models on limited novel samples, which contradicts the few-shot learning paradigm and incurs substantial computational cost. To address these issues, we propose a Multimodal Meta-Training (MMT) framework that enhances both semantic alignment and training efficiency in FSOD. MMT consists of two core components: (1) a Region Feature Enhancement Module (RFEM), which refines visual region representations through cross-modal fusion with textual features to alleviate semantic misalignment; and (2) a Meta-Training Strategy, which adopts an inner–outer loop optimization scheme to improve model generalization and reduce training overhead. Extensive experiments on PASCAL VOC and MS COCO demonstrate that MMT achieves superior detection accuracy on novel classes while significantly reducing training time.
Published: 2025-11-27T07:57:02+00:00
Venue: Neurocomputing
Score: 0.834 (must_read)</description></item><item><title>Dual-Stream Multi-Modal Fusion with Local-Global Attention for Remote Sensing Object Detection</title><link>https://doi.org/10.1109/jstars.2025.3637891</link><guid>10.1109/jstars.2025.3637891</guid><pubDate>Thu, 27 Nov 2025 18:58:13 +0000</pubDate><category>IEEE Journal of Selected Topics in Applied Earth Observations and Remote Sensing</category><description>Object detection in remote sensing imagery plays a crucial role in providing precise geospatial information for urban planning and environmental monitoring. However, real-world remote sensing scenarios often involve complex conditions such as varying illumination, weather interference, and low signal-to-noise ratios, which significantly degrade the performance of traditional single-modal detection methods. To overcome these limitations, multimodal object detection has developed, demonstrating great potential by integrating complementary information from multiple modalities. Nevertheless, existing multimodal frameworks still face challenges such as insufficient cross-modal interaction, limited learning of complementary features, and high computational costs due to redundant fusion in complex environments. To overcome these challenges, we propose an enhanced multi-modal fusion strategy aimed at maximizing cross-modal feature learning capabilities. Our method employs a dual-backbone architecture to extract mode-specific representations independently, integrating a direction attention (DA) module at an early stage of each backbone to enhance discriminative feature extraction. We then introduce a dual-stream feature fusion network (DSFN) to effectively fuse cross-modal features, generating rich representations for the detection head. Additionally, we embed a local-global channel attention (LGCA) mechanism in the head stage to strengthen feature learning in the channel dimension before generating the final prediction. Extensive experiments on the widely used VEDAI multimodal remote sensing dataset demonstrate that our method achieves state-of-the-art performance, while evaluations on single-modal datasets confirm its exceptional generalization capability.
Published: 2025-11-27T18:58:13+00:00
Venue: IEEE Journal of Selected Topics in Applied Earth Observations and Remote Sensing
Score: 0.829 (must_read)</description></item><item><title>From Diffusion to One-Step Generation: A Comparative Study of Flow-Based Models with Application to Image Inpainting</title><link>https://arxiv.org/abs/2511.21215v1</link><guid>http://arxiv.org/abs/2511.21215v1</guid><pubDate>Wed, 26 Nov 2025 09:44:51 +0000</pubDate><category>arXiv</category><description>We present a comprehensive comparative study of three generative modeling paradigms: Denoising Diffusion Probabilistic Models (DDPM), Conditional Flow Matching (CFM), and MeanFlow. While DDPM and CFM require iterative sampling, MeanFlow enables direct one-step generation by modeling the average velocity over time intervals. We implement all three methods using a unified TinyUNet architecture (&lt;1.5M parameters) on CIFAR-10, demonstrating that CFM achieves an FID of 24.15 with 50 steps, significantly outperforming DDPM (FID 402.98). MeanFlow achieves FID 29.15 with single-step sampling -- a 50X reduction in inference time. We further extend CFM to image inpainting, implementing mask-guided sampling with four mask types (center, random bbox, irregular, half). Our fine-tuned inpainting model achieves substantial improvements: PSNR increases from 4.95 to 8.57 dB on center masks (+73%), and SSIM improves from 0.289 to 0.418 (+45%), demonstrating the effectiveness of inpainting-aware training.
Published: 2025-11-26T09:44:51+00:00
Venue: arXiv
Score: 0.817 (must_read)</description></item><item><title>Leader-Based Multiexpert Neural Network for High-Level Visual Tasks</title><link>https://doi.org/10.1109/tnnls.2025.3631509</link><guid>10.1109/tnnls.2025.3631509</guid><pubDate>Thu, 27 Nov 2025 18:58:39 +0000</pubDate><category>IEEE Transactions on Neural Networks and Learning Systems</category><description>Remarkable progress has been achieved in the detection and segmentation of the baseline; however, for high-level visual tasks in complex scenes (e.g., dense, occlusion, scale diversity, high background noise, etc.), existing frameworks often fail to provide satisfactory performance. To further improve the object recognition ability, this article introduces a leader-based multiexpert mechanism into the detection and segmentation tasks. In this work, we first design a leader-based attention learning layer to fully integrate multilevel features from the backbone network, which can effectively obtain global semantics and assign instructions to detection experts. Then, we propose multiple feature pyramids with dual fusion paths to replace the traditional single pipeline using semantic and spatial allocators. With this strategy, we can further establish deep supervision for multiple experts during training and sufficiently utilize the multiexpert detection results from leaders’ assignments during reasoning, thereby comprehensively improving the performance of the model in complex scenarios. In the experiment, we established ablation studies and performance comparisons on COCO 2017 detection and segmentation tasks. Finally, we demonstrated the model’s performance in three complex application scenarios (remote sensing, autonomous driving, and industrial fields), and the results showed our advantages.
Published: 2025-11-27T18:58:39+00:00
Venue: IEEE Transactions on Neural Networks and Learning Systems
Score: 0.808 (must_read)</description></item><item><title>Learn by Reasoning: Analogical Weight Generation for Few-Shot Class-Incremental Learning</title><link>https://doi.org/10.1109/tcsvt.2025.3637903</link><guid>10.1109/tcsvt.2025.3637903</guid><pubDate>Thu, 27 Nov 2025 18:59:56 +0000</pubDate><category>IEEE Transactions on Circuits and Systems for Video Technology</category><description>Few-shot class-incremental Learning (FSCIL) enables models to learn new classes from limited data while retaining performance on previously learned classes. Traditional FSCIL methods often require fine-tuning parameters with limited new class data and suffer from a separation between learning new classes and utilizing old knowledge. Inspired by the analogical learning mechanisms of the human brain, we propose a novel analogical generative method. Our approach includes the Brain-Inspired Analogical Generator (BiAG), which derives new class weights from existing classes without parameter fine-tuning during incremental stages. BiAG consists of three components: Weight Self-Attention Module (WSA), Weight &amp; Prototype Analogical Attention Module (WPAA), and Semantic Conversion Module (SCM). SCM uses Neural Collapse theory for semantic conversion, WSA supplements new class weights, and WPAA computes analogies to generate new class weights. Experiments on miniImageNet, CUB-200, and CIFAR-100 datasets demonstrate that our method achieves higher final and average accuracy compared to SOTA methods.
Published: 2025-11-27T18:59:56+00:00
Venue: IEEE Transactions on Circuits and Systems for Video Technology
Score: 0.806 (must_read)</description></item><item><title>ReSAM: Refine, Requery, and Reinforce: Self-Prompting Point-Supervised Segmentation for Remote Sensing Images</title><link>https://arxiv.org/abs/2511.21606v1</link><guid>http://arxiv.org/abs/2511.21606v1</guid><pubDate>Wed, 26 Nov 2025 17:26:00 +0000</pubDate><category>arXiv</category><description>Interactive segmentation models such as the Segment Anything Model (SAM) have demonstrated remarkable generalization on natural images, but perform suboptimally on remote sensing imagery (RSI) due to severe domain shift and the scarcity of dense annotations. To address this, we propose a self-prompting, point-supervised framework that adapts SAM to RSIs using only sparse point annotations. Our method employs a Refine-Requery-Reinforce loop, where coarse pseudo-masks are generated from initial points (Refine), improved with self-constructed box prompts (Requery), and embeddings are aligned across iterations to reduce confirmation bias (Reinforce). Without relying on full-mask supervision, our approach progressively enhances SAM's segmentation quality and domain robustness through self-guided prompt adaptation . We evaluate our proposed method on three RSI benchmark datasets, including WHU, HRSID, and NWPU VHR-10, showing that our method consistently surpasses pretrained SAM and recent point-supervised segmentation methods. Our results demonstrate that self-prompting and semantic alignment provide an efficient path towards scalable, point-level adaptation of foundation segmentation models for remote sensing applications.
Published: 2025-11-26T17:26:00+00:00
Venue: arXiv
Score: 0.802 (must_read)</description></item><item><title>DCTC-Net: Dual-Branch Cross-Fusion Transformer–CNN Architecture for Medical Image Segmentation</title><link>https://doi.org/10.1109/tnnls.2025.3628995</link><guid>10.1109/tnnls.2025.3628995</guid><pubDate>Thu, 27 Nov 2025 18:58:39 +0000</pubDate><category>IEEE Transactions on Neural Networks and Learning Systems</category><description>Hybrid architectures that combine convolutional neural networks (CNNs) with Transformers have emerged as a promising approach for medical image segmentation. However, existing networks based on this hybrid architecture often encounter two challenges. First, while the CNN branch effectively captures local image features through convolution operations, vanilla convolution lacks the ability to achieve adaptive feature extraction. Second, although the Transformer branch can model global image information, conventional self-attention (SA) primarily focuses on spatial relationships, neglecting channel and cross-dimensional attention, leading to suboptimal segmentation results, particularly for medical images with complex backgrounds. To address these limitations, we propose a dual-branch cross-fusion Transformer–CNN architecture for medical image segmentation (DCTC-Net). Our network provides two key advantages. First, a dynamic deformable convolution (DDConv) is integrated into the CNN branch to overcome the limitations of adaptive feature extraction with fixed-size convolution kernels and also eliminate the issue of shared convolution kernel parameters across different inputs, significantly enhancing the feature expression capabilities of the CNN branch. Second, a (shifted)-window adaptive complementary attention module ((S)W-ACAM) and compact convolutional projection are incorporated into the Transformer branch, enabling the network to comprehensively learn cross-dimensional long-range dependencies in medical images. Experimental results demonstrate that the proposed DCTC-Net achieves superior medical image segmentation performance compared to state-of-the-art (SOTA) methods, including CNN and Transformer networks. In addition, our DCTC-Net requires fewer parameters and lower computational costs and does not rely on pretraining.
Published: 2025-11-27T18:58:39+00:00
Venue: IEEE Transactions on Neural Networks and Learning Systems
Score: 0.802 (must_read)</description></item><item><title>Deep Tabular Representation Corrector</title><link>https://doi.org/10.1109/tpami.2025.3637810</link><guid>10.1109/tpami.2025.3637810</guid><pubDate>Thu, 27 Nov 2025 18:57:48 +0000</pubDate><category>IEEE Transactions on Pattern Analysis and Machine Intelligence</category><description>Tabular data have been playing a mostly important role in diverse real-world fields, such as healthcare, engineering, finance, etc. The recent success of deep learning has fostered many deep networks (e.g., Transformer, ResNet) based tabular learning methods. Generally, existing deep tabular machine learning methods are along with the two paradigms, i.e., inlearning and pre-learning. In-learning methods need to train networks from scratch or impose extra constraints to regulate the representations which nonetheless train multiple tasks simultaneously and make learning more difficult, while prelearning methods design several pretext tasks for pre-training and then conduct task-specific fine-tuning, which however need much extra training effort with prior knowledge. In this paper, we introduce a novel deep Tabular Representation Corrector, TRC, to enhance any trained deep tabular model's representations without altering its parameters in a model-agnostic manner. Specifically, targeting the representation shift and representation redundancy that hinder prediction, we propose two tasks, i.e., (i) Tabular Representation Re-estimation, that involves training a shift estimator to calculate the inherent shift of tabular representations to subsequently mitigate it, thereby re-estimating the representations and (ii) Tabular Space Mapping, that transforms the above re-estimated representations into a light-embedding vector space via a coordinate estimator while preserves crucial predictive information to minimize redundancy. The two tasks jointly enhance the representations of deep tabular models without touching on the original models thus enjoying high efficiency. Finally, we conduct extensive experiments on state-of-the-art deep tabular machine learning models coupled with TRC on various tabular benchmarks which have shown consistent superiority.
Published: 2025-11-27T18:57:48+00:00
Venue: IEEE Transactions on Pattern Analysis and Machine Intelligence
Score: 0.801 (must_read)</description></item><item><title>HTTM: Head-wise Temporal Token Merging for Faster VGGT</title><link>https://arxiv.org/abs/2511.21317v1</link><guid>http://arxiv.org/abs/2511.21317v1</guid><pubDate>Wed, 26 Nov 2025 12:04:03 +0000</pubDate><category>arXiv</category><description>The Visual Geometry Grounded Transformer (VGGT) marks a significant leap forward in 3D scene reconstruction, as it is the first model that directly infers all key 3D attributes (camera poses, depths, and dense geometry) jointly in one pass. However, this joint inference mechanism requires global attention layers that perform all-to-all attention computation on tokens from all views. For reconstruction of large scenes with long-sequence inputs, this causes a significant latency bottleneck. In this paper, we propose head-wise temporal merging (HTTM), a training-free 3D token merging method for accelerating VGGT. Existing merging techniques merge tokens uniformly across different attention heads, resulting in identical tokens in the layers' output, which hinders the model's representational ability. HTTM tackles this problem by merging tokens in multi-head granularity, which preserves the uniqueness of feature tokens after head concatenation. Additionally, this enables HTTM to leverage the spatial locality and temporal correspondence observed at the head level to achieve higher merging ratios with lower merging costs compared to existing methods. Thus, HTTM achieves up to 7x acceleration with negligible performance drops in a GPU-based inference.
Published: 2025-11-26T12:04:03+00:00
Venue: arXiv
Score: 0.797 (must_read)</description></item><item><title>Beyond static imaging: A dynamic decision paradigm for robust array-SAR in diverse sensing scenarios</title><link>https://doi.org/10.1016/j.isprsjprs.2025.11.023</link><guid>10.1016/j.isprsjprs.2025.11.023</guid><pubDate>Thu, 27 Nov 2025 23:11:42 +0000</pubDate><category>ISPRS Journal of Photogrammetry and Remote Sensing</category><description>Array synthetic aperture radar (array-SAR) is a popular radar imaging technique for 3D scene sensing, especially for urban area. Recently, deep learning imaging methods have achieved significant advancements, showing promise for large-scale spatial sensing. However current methods struggle with generalization because their imaging pipelines are static—key parameters are fixed after training—so performance degrades across varying noise levels, measurement models, and scene distributions—a critical gap that remains insufficiently addressed. We address this by recasting array-SAR imaging as a dynamic Markov decision process. And we introduce a state–sequence–decision framework: a sequence of state transitions, where each state triggers learnable actions determined by decision that adapt step size, regularization threshold, and stopping based on the evolving state. We have conducted extensive experiments across a wide range of noise conditions (0–10 dB), measurement models (from ground-based to airborne systems, with 10%–50% sampling ratios), and scene distributions in both near-field and far-field sensing scenarios. Across all these settings, the proposed method consistently outperforms representative baselines, achieving average gains of 5.1 dB in PSNR and 0.35 in SSIM, demonstrating strong robustness across diverse sensing environments.
Published: 2025-11-27T23:11:42+00:00
Venue: ISPRS Journal of Photogrammetry and Remote Sensing
Score: 0.796 (must_read)</description></item><item><title>Sawtooth Sampling for Time Series Denoising Diffusion Implicit Models</title><link>https://arxiv.org/abs/2511.21320v1</link><guid>http://arxiv.org/abs/2511.21320v1</guid><pubDate>Wed, 26 Nov 2025 12:05:44 +0000</pubDate><category>arXiv</category><description>Denoising Diffusion Probabilistic Models (DDPMs) can generate synthetic timeseries data to help improve the performance of a classifier, but their sampling process is computationally expensive. We address this by combining implicit diffusion models with a novel Sawtooth Sampler that accelerates the reverse process and can be applied to any pretrained diffusion model. Our approach achieves a 30 times speed-up over the standard baseline while also enhancing the quality of the generated sequences for classification tasks.
Published: 2025-11-26T12:05:44+00:00
Venue: arXiv
Score: 0.795 (must_read)</description></item><item><title>Escaping the Verifier: Learning to Reason via Demonstrations</title><link>https://arxiv.org/abs/2511.21667v1</link><guid>http://arxiv.org/abs/2511.21667v1</guid><pubDate>Wed, 26 Nov 2025 18:42:52 +0000</pubDate><category>arXiv</category><description>Training Large Language Models (LLMs) to reason often relies on Reinforcement Learning (RL) with task-specific verifiers. However, many real-world reasoning-intensive tasks lack verifiers, despite offering abundant expert demonstrations that remain under-utilized for reasoning-focused training. We introduce RARO (Relativistic Adversarial Reasoning Optimization) that learns strong reasoning capabilities from only expert demonstrations via Inverse Reinforcement Learning. Our method sets up an adversarial interaction between a policy (generator) and a relativistic critic (discriminator): the policy learns to mimic expert answers, while the critic learns to compare and distinguish between policy and expert answers. Our method trains both the policy and the critic jointly and continuously via RL, and we identify the key stabilization techniques required for robust learning. Empirically, RARO significantly outperforms strong verifier-free baselines on all of our evaluation tasks -- Countdown, DeepMath, and Poetry Writing -- and enjoys the same robust scaling trends as RL on verifiable tasks. These results demonstrate that our method effectively elicits strong reasoning performance from expert demonstrations alone, enabling robust reasoning learning even when task-specific verifiers are unavailable.
Published: 2025-11-26T18:42:52+00:00
Venue: arXiv
Score: 0.793 (must_read)</description></item><item><title>On the Efficient Adaptive Streaming of 3D Gaussian Splatting over Dynamic Networks</title><link>https://doi.org/10.1109/tcsvt.2025.3637837</link><guid>10.1109/tcsvt.2025.3637837</guid><pubDate>Thu, 27 Nov 2025 18:59:56 +0000</pubDate><category>IEEE Transactions on Circuits and Systems for Video Technology</category><description>3D Gaussian Splatting (3DGS) has recently emerged as a promising representation for immersive media. Its explicit splat-based structure offers high visual quality and real-time rendering, making it particularly suitable for six degrees of freedom streaming applications. However, its deployment in practical streaming scenarios is still limited due to several key challenges such as the large data volume, and insufficient support for dynamic bitrate adaptation under fluctuating network conditions. This paper presents an efficient 3DGS streaming framework that operates directly on pre-generated 3DGS models without retraining or fine-tuning. First, a training-free perceptual pruning method, which removes visually redundant Gaussians according to the human visual system metrics, is introduced. The resulting 3DGS is then encoded into a compact representation using the extended 3D codecs, exploiting its point-based structure. Next, we build a scene-specific bitrate ladder through analyzing the trade-off between resolution, bitrate, and perceptual quality. This enables efficient and fine-grained representation selection. Finally, a progressive streaming mechanism is developed. It is driven by a reinforcement learning scheduler that adaptively decides whether to download new content or enhance previously buffered content based on real-time network feedback. Experiments on real-world 3DGS datasets and bandwidth traces show that the proposed method evidently improves the quality of experience and streaming efficiency in various network scenarios.
Published: 2025-11-27T18:59:56+00:00
Venue: IEEE Transactions on Circuits and Systems for Video Technology
Score: 0.793 (must_read)</description></item><item><title>Seeing without Pixels: Perception from Camera Trajectories</title><link>https://arxiv.org/abs/2511.21681v1</link><guid>http://arxiv.org/abs/2511.21681v1</guid><pubDate>Wed, 26 Nov 2025 18:57:01 +0000</pubDate><category>arXiv</category><description>Can one perceive a video's content without seeing its pixels, just from the camera trajectory-the path it carves through space? This paper is the first to systematically investigate this seemingly implausible question. Towards this end, we propose a contrastive learning framework to train CamFormer, a dedicated encoder that projects camera pose trajectories into a joint embedding space, aligning them with natural language. We find that, contrary to its apparent simplicity, the camera trajectory is a remarkably informative signal to uncover video content. In other words, "how you move" can indeed reveal "what you are doing" (egocentric) or "observing" (exocentric). We demonstrate the versatility of our learned CamFormer embeddings on a diverse suite of downstream tasks, ranging from cross-modal alignment to classification and temporal analysis. Importantly, our representations are robust across diverse camera pose estimation methods, including both high-fidelity multi-sensored and standard RGB-only estimators. Our findings establish camera trajectory as a lightweight, robust, and versatile modality for perceiving video content.
Published: 2025-11-26T18:57:01+00:00
Venue: arXiv
Score: 0.791 (must_read)</description></item><item><title>OVOD-Agent: A Markov-Bandit Framework for Proactive Visual Reasoning and Self-Evolving Detection</title><link>https://arxiv.org/abs/2511.21064v1</link><guid>http://arxiv.org/abs/2511.21064v1</guid><pubDate>Wed, 26 Nov 2025 05:08:26 +0000</pubDate><category>arXiv</category><description>Open-Vocabulary Object Detection (OVOD) aims to enable detectors to generalize across categories by leveraging semantic information. Although existing methods are pretrained on large vision-language datasets, their inference is still limited to fixed category names, creating a gap between multimodal training and unimodal inference. Previous work has shown that improving textual representation can significantly enhance OVOD performance, indicating that the textual space is still underexplored. To this end, we propose OVOD-Agent, which transforms passive category matching into proactive visual reasoning and self-evolving detection. Inspired by the Chain-of-Thought (CoT) paradigm, OVOD-Agent extends the textual optimization process into an interpretable Visual-CoT with explicit actions. OVOD's lightweight nature makes LLM-based management unsuitable; instead, we model visual context transitions as a Weakly Markovian Decision Process (w-MDP) over eight state spaces, which naturally represents the agent's state, memory, and interaction dynamics. A Bandit module generates exploration signals under limited supervision, helping the agent focus on uncertain regions and adapt its detection policy. We further integrate Markov transition matrices with Bandit trajectories for self-supervised Reward Model (RM) optimization, forming a closed loop from Bandit exploration to RM learning. Experiments on COCO and LVIS show that OVOD-Agent provides consistent improvements across OVOD backbones, particularly on rare categories, confirming the effectiveness of the proposed framework.
Published: 2025-11-26T05:08:26+00:00
Venue: arXiv
Score: 0.791 (must_read)</description></item><item><title>MLPMoE: Zero-Shot Architectural Metamorphosis of Dense LLM MLPs into Static Mixture-of-Experts</title><link>https://arxiv.org/abs/2511.21089v1</link><guid>http://arxiv.org/abs/2511.21089v1</guid><pubDate>Wed, 26 Nov 2025 06:14:26 +0000</pubDate><category>arXiv</category><description>Large Language Models (LLMs) are predominantly deployed as dense transformers, where every parameter in every feed-forward block is activated for every token. While architecturally simple, this is computationally inefficient, since inference costs scale linearly with parameter count. Recent upcycling methods such as MoEfication, CMoE, ToMoE, and MoORE reveal that much of the useful computation lives in sparse, semi-modular substructures inside dense feed-forward networks, but these approaches typically rely on clustering, activation profiling, singular value decomposition, or custom routing that requires calibration data. This paper introduces MLPMoE (MLP Mixture-of-Experts), a training-free, deterministic transformation that restructures the dense MLP in transformer blocks into a static, high-cardinality mixture of experts. The transformation uses simple tensor slicing and summation, reinterpreting the algebra of tensor parallelism as a topological conversion rather than a distributed training pattern. We further introduce Fractal Fade (differential branch sparsity) and Compensated Pruning (variance-preserving branch reduction) as lightweight mechanisms for structured sparsity. On Qwen2.5-0.5B-Instruct and DeepSeek-R1-Distill-Llama-8B, the zero-shot MLPMoE transform changes a proxy perplexity metric by less than 0.05 percent while keeping the parameter count effectively constant. On the 8B model, differential sparsity removes about 20 percent of MLP parameters while keeping perplexity within about 2 percent of the dense baseline. The method operates entirely post hoc on existing checkpoints and does not require gradients, calibration sets, or router training. Code is available at https://gist.github.com/iwallarm/fc2ef1eddf226ca7814f9e5e2ae9bad1
Published: 2025-11-26T06:14:26+00:00
Venue: arXiv
Score: 0.791 (must_read)</description></item><item><title>GatedFusion-Net: Per-Pixel Modality Weighting in a Five-Cue Transformer For RGB-D-I-T-UV Fusion</title><link>https://doi.org/10.1016/j.inffus.2025.103986</link><guid>10.1016/j.inffus.2025.103986</guid><pubDate>Thu, 27 Nov 2025 00:18:33 +0000</pubDate><category>Information Fusion</category><description>We introduce GatedFusion-Net (GF-Net), built on the SegFormer Transformer backbone, as the first architecture to unify RGB, depth ( D ), infrared intensity ( I ), thermal ( T ), and ultraviolet ( UV ) imagery for dense semantic segmentation on the MM5 dataset. GF-Net departs from the CMX baseline via: (1) stage-wise RGB-intensity-depth enhancement that injects geometrically aligned D, I cues at each encoder stage, together with surface normals ( N ), improving illumination invariance without adding parameters; (2) per-pixel sigmoid gating, where independent Sigmoid Gate blocks learn spatial confidence masks for T and UV and add their contributions to the RGB+DIN base, trimming computational cost while preserving accuracy; and (3) modality-wise normalisation using per-stream statistics computed on MM5 to stabilise training and balance cross-cue influence. An ablation study reveals that the five-modality configuration (RGB+DIN+T+UV) achieves a peak mean IoU of 88.3%, with the UV channel contributing a 1.7-percentage-point gain under optimal lighting (RGB3). Under challenging illumination, it maintains comparable performance, indicating complementary but situational value. Modality-ablation experiments reveal strong sensitivity: removing RGB, T, DIN , or UV yields relative mean IoU reductions of 83.4%, 63.3%, 56.5%, and 30.1%, respectively. Sigmoid-Gate fusion behaves primarily as static, lighting-dependent weighting rather than adapting to sensor loss. Throughput on an RTX 3090 with a MiT-B0 backbone is real-time: 640 × 480 at 74 fps for RGB+DIN+T, 55 fps for RGB+DIN+T+UV, and 41 fps with five gated streams. These results establish the first RGB-D-I-T-UV segmentation baselines on MM5 and show that per-pixel sigmoid gating is a lightweight, effective alternative to heavier attention-based fusion.
Published: 2025-11-27T00:18:33+00:00
Venue: Information Fusion
Score: 0.790 (must_read)</description></item><item><title>Breaking the Safety-Capability Tradeoff: Reinforcement Learning with Verifiable Rewards Maintains Safety Guardrails in LLMs</title><link>https://arxiv.org/abs/2511.21050v1</link><guid>http://arxiv.org/abs/2511.21050v1</guid><pubDate>Wed, 26 Nov 2025 04:36:34 +0000</pubDate><category>arXiv</category><description>Fine-tuning large language models (LLMs) for downstream tasks typically exhibit a fundamental safety-capability tradeoff, where improving task performance degrades safety alignment even on benign datasets. This degradation persists across standard approaches including supervised finetuning (SFT) and reinforcement learning from human feedback (RLHF). While reinforcement learning with verifiable rewards (RLVR) has emerged as a promising alternative that optimizes models on objectively measurable tasks, its safety implications remain unexplored. We present the first comprehensive theoretical and empirical analysis of safety properties in RLVR. Theoretically, we derive upper bounds on safety drift under KL-constrained optimization and prove conditions under which safety degradation is eliminated. Empirically, we conduct extensive experiments across five adversarial safety benchmarks, demonstrating that RLVR can simultaneously enhance reasoning capabilities while maintaining or improving safety guardrails. Our comprehensive ablation studies examine the effects of optimization algorithms, model scale, and task domains. Our findings challenge the prevailing assumption of an inevitable safety capability trade-off, and establish that a specific training methodology can achieve both objectives simultaneously, providing insights for the safe deployment of reasoning-capable LLMs.
Published: 2025-11-26T04:36:34+00:00
Venue: arXiv
Score: 0.790 (must_read)</description></item><item><title>Scaling Foundation Models for Radar Scene Understanding</title><link>https://arxiv.org/abs/2511.21105v1</link><guid>http://arxiv.org/abs/2511.21105v1</guid><pubDate>Wed, 26 Nov 2025 06:41:00 +0000</pubDate><category>arXiv</category><description>Radar sensors provide reliable perception across adverse weather, lighting, and long-range conditions. Recent advances in foundation models have transformed visual and language understanding, yet their integration with radar sensing remains largely underexplored. Existing radar approaches are fragmented and task-specific; each downstream task employs distinct architectures and training objectives, preventing transfer across tasks. In this work, we introduce RadarFM: a radar foundation model that learns unified scene-level representations through structured spatial language supervision. We make two key contributions: (1) a structured caption framework that encodes vehicle distributions in native radar coordinates, and (2) a hash-aware contrastive learning objective that quantifies continuous scene similarity rather than binary matching, enabling fine-grained spatial reasoning. Leveraging the CARLA simulator, we generate large-scale, well-annotated radar datasets across diverse driving scenarios. We also propose localization-aware metrics that assess spatial accuracy beyond traditional detection measures.
Published: 2025-11-26T06:41:00+00:00
Venue: arXiv
Score: 0.789 (must_read)</description></item><item><title>Mean Teacher Based on Class Prototype Contrast for Domain Adaptive Object Detection</title><link>https://doi.org/10.1016/j.neunet.2025.108377</link><guid>10.1016/j.neunet.2025.108377</guid><pubDate>Thu, 27 Nov 2025 07:55:50 +0000</pubDate><category>Neural Networks</category><description>Unsupervised domain adaptive object detection (UDAOD) aims to effectively apply the detector trained on a labeled (source domain) and an unlabeled (target domain) dataset to the target domain. The mean teacher framework has demonstrated good applicability and wide application in this task. However, influenced by the difference between the two domains, the teacher model often generates many false positive objects. The pseudo-labels cannot sufficiently include all classes of objects in an image because of single-threshold filtering, causing the model to perform poorly in detection tasks. Therefore, we propose a new student-teacher framework, the mean teacher, which is based on class prototype contrast (PCMT). Utilizing class prototypes to preserve the features that are common in objects of the same class to address the problem of significant feature differences that may exist between these objects. Then, the class prototypes are applied to contrastive learning, so that the model can distinguish various classes more accurately while align the features of the same class across domains. In addition, we design a pseudo-label filtering method based on bounding box localization to retain potentially valid pseudo-labels. Experiments show that PCMT achieves superior performance under different domain adaptive conditions. For the Cityscapes→BDD100K dataset, we obtain the best mean average precision (mAP) of 43.5%, which is 5.0% greater than the state-of-the-art (SOTA).
Published: 2025-11-27T07:55:50+00:00
Venue: Neural Networks
Score: 0.789 (must_read)</description></item><item><title>MBLKNet: a large kernel convolution-driven network with multi-task self-supervised learning for SAR maritime target classification</title><link>https://doi.org/10.1080/10095020.2025.2584937</link><guid>10.1080/10095020.2025.2584937</guid><pubDate>Thu, 27 Nov 2025 15:46:43 +0000</pubDate><category>Geo-spatial Information Science</category><description>Synthetic aperture radar (SAR) maritime target classification serves as a critical component in modern maritime surveillance. While deep learning networks, particularly convolutional neural networks (CNNs), have driven substantial progress in this domain, three key challenges constrain their performance and practical deployment: 1) In SAR maritime images, complex inshore backgrounds and speckle noise are prevalent. Targets such as ships span a wide range of scales due to different imaging resolutions and intrinsic size variability, exacerbating inter-class similarity and intra-class variability, 2) Labeled data for SAR maritime target classification are scarce, and sensor imaging modes differ markedly across platforms, and 3) Existing CNNs that fuse traditional hand-crafted features often explicitly treat hand-crafted feature extraction as a necessary component of the network and primarily focus on classification performance, overlooking the requirement to efficiently leverage their feature extraction capabilities in downstream tasks. To overcome these challenges, this article proposes a novel SAR maritime target classification network (MBLKNet) based on large kernel convolution and multi-task self-supervised learning. In MBLKNet, four improved designs for network structure are proposed to enhance classification accuracy: 1) macro design, 2) multi-branch large kernel convolution module (MBLKCM), 3) lightweight channel-interactive multi-layer perceptron (LCIMLP), and 4) micro design. In addition, a multi-resolution unlabeled SAR maritime target dataset (SL-SARShip) and a masked image modeling framework, HOGSparK, are proposed to enable the pre-training of MBLKNet under joint supervision of pixel and HOG features. Comparison results on OpenSARShip 2.0 and FUSAR-Ship with state-of-the-art networks, as well as experiments on SSDD for SAR downstream target detection and instance segmentation, demonstrate that the proposed MBLKNet achieves superior performance and strong feature extraction ability.
Published: 2025-11-27T15:46:43+00:00
Venue: Geo-spatial Information Science
Score: 0.788 (must_read)</description></item><item><title>The More, the Merrier: Contrastive Fusion for Higher-Order Multimodal Alignment</title><link>https://arxiv.org/abs/2511.21331v1</link><guid>http://arxiv.org/abs/2511.21331v1</guid><pubDate>Wed, 26 Nov 2025 12:25:55 +0000</pubDate><category>arXiv</category><description>Learning joint representations across multiple modalities remains a central challenge in multimodal machine learning. Prevailing approaches predominantly operate in pairwise settings, aligning two modalities at a time. While some recent methods aim to capture higher-order interactions among multiple modalities, they often overlook or insufficiently preserve pairwise relationships, limiting their effectiveness on single-modality tasks. In this work, we introduce Contrastive Fusion (ConFu), a framework that jointly embeds both individual modalities and their fused combinations into a unified representation space, where modalities and their fused counterparts are aligned. ConFu extends traditional pairwise contrastive objectives with an additional fused-modality contrastive term, encouraging the joint embedding of modality pairs with a third modality. This formulation enables ConFu to capture higher-order dependencies, such as XOR-like relationships, that cannot be recovered through pairwise alignment alone, while still maintaining strong pairwise correspondence. We evaluate ConFu on synthetic and real-world multimodal benchmarks, assessing its ability to exploit cross-modal complementarity, capture higher-order dependencies, and scale with increasing multimodal complexity. Across these settings, ConFu demonstrates competitive performance on retrieval and classification tasks, while supporting unified one-to-one and two-to-one retrieval within a single contrastive framework.
Published: 2025-11-26T12:25:55+00:00
Venue: arXiv
Score: 0.788 (must_read)</description></item><item><title>Aligning LLMs Toward Multi-Turn Conversational Outcomes Using Iterative PPO</title><link>https://arxiv.org/abs/2511.21638v1</link><guid>http://arxiv.org/abs/2511.21638v1</guid><pubDate>Wed, 26 Nov 2025 18:12:16 +0000</pubDate><category>arXiv</category><description>Optimizing large language models (LLMs) for multi-turn conversational outcomes remains a significant challenge, especially in goal-oriented settings like AI marketing or sales agents who facilitate transactions via messaging platforms. The difficulty stems from sparse, long-horizon rewards and the discrepancy between response-level planning and token-level generation. In this technical note, we propose a formal reduction of the multi-turn RL problem into a sequence of single-turn RLHF-style problems. This is achieved by setting a learned multi-turn Q-function as the reward model for the single-turn problem. We demonstrate and prove a key insight: solving this single-turn RL problem with standard token-level PPO is equivalent to a policy improvement step within the multi-turn problem. This insight naturally leads to Iterative PPO, a batch online policy iteration algorithm that alternates between fitting Q-functions from logged conversation trajectories and improving the policy. A major practical advantage is that Iterative PPO directly leverages stable, off-the-shelf single-turn RLHF tools, making it straightforward to implement. Our method occupies a middle ground between fully online and fully offline approaches, retaining the adaptability of online updates while gaining the stability benefits of offline training.
Published: 2025-11-26T18:12:16+00:00
Venue: arXiv
Score: 0.788 (must_read)</description></item><item><title>ICPO: Intrinsic Confidence-Driven Group Relative Preference Optimization for Efficient Reinforcement Learning</title><link>https://arxiv.org/abs/2511.21005v1</link><guid>http://arxiv.org/abs/2511.21005v1</guid><pubDate>Wed, 26 Nov 2025 03:10:15 +0000</pubDate><category>arXiv</category><description>Reinforcement Learning with Verifiable Rewards (RLVR) demonstrates significant potential in enhancing the reasoning capabilities of Large Language Models (LLMs). However, existing RLVR methods are often constrained by issues such as coarse-grained rewards, reward noise, and inefficient exploration, which lead to unstable training and entropy collapse. To address this challenge, we propose the Intrinsic Confidence-Driven Group Relative Preference Optimization method (ICPO). The intuition behind it lies in the fact that the probabilities of an LLM generating different responses can inherently and directly reflect its self-assessment of the reasoning process. Inspired by the idea of preference modeling, ICPO calculates a preference advantage score for each response by comparing the relative generation probabilities of multiple responses under the same input prompt, and integrates this score with verifiable rewards to guide the exploration process. We have discovered that the preference advantage score not only alleviates the issues of coarse-grained rewards and reward noise but also effectively curbs overconfident errors, enhances the relative superiority of undervalued high-quality responses, and prevents the model from overfitting to specific strategies, thereby facilitating more thorough exploration. Comprehensive experiments across four general-domain benchmarks and three mathematical benchmarks demonstrate that ICPO steadily boosts reasoning compared to GRPO.
Published: 2025-11-26T03:10:15+00:00
Venue: arXiv
Score: 0.787 (must_read)</description></item><item><title>IP-Controller: Decomposition and Optimization of Cross-Attention Maps for Accurate Subject-Driven Text-to-Image Diffusion Generation</title><link>https://doi.org/10.1109/tcsvt.2025.3637822</link><guid>10.1109/tcsvt.2025.3637822</guid><pubDate>Thu, 27 Nov 2025 18:59:56 +0000</pubDate><category>IEEE Transactions on Circuits and Systems for Video Technology</category><description>Although large pretrained stable diffusion (SD) models can generate high-quality images from prompts, they cannot generate images that are consistent with the fine-grained characteristics of a specific identity V ∗ (e.g., an anime character). Subject-driven generation focuses on exploring and leveraging the prior knowledge within a model to achieve the goals of ID and context preservation. There have been efforts, such as DreamBooth, to conduct subject-driven generation; however, they suffer from ID and context mistakes. An ID mistake means a feature loss of V ∗, and a context mistake means that the generated image does not align with the given prompt. To rectify these problems, in this paper, we propose masked fine-tuning for efficient feature learning of V ∗, then propose IP-Controller for decomposing and optimizing cross-attention maps of V ∗ and prompt words other than V ∗. Specifically, we generate the cross-attention map using a vanilla input prompt and decompose it into an ID cross-attention map (matching V ∗) and a context cross-attention map (matching prompt words other than V ∗). Next, we generate fitter ID and context cross-attention maps on the basis of the input ID and context prompts, respectively. We optimize the ID and context cross-attention maps with the fitter ID and context cross-attention maps, respectively, so that the diffusion process pays fitter attention for specific contents. Experiments show that IP-Controller correctly integrates the core features of V ∗ and the semantic context of the prompt words other than V ∗ and generates high-quality images for the given prompt.
Published: 2025-11-27T18:59:56+00:00
Venue: IEEE Transactions on Circuits and Systems for Video Technology
Score: 0.786 (must_read)</description></item><item><title>EM-KD: Distilling Efficient Multimodal Large Language Model with Unbalanced Vision Tokens</title><link>https://arxiv.org/abs/2511.21106v1</link><guid>http://arxiv.org/abs/2511.21106v1</guid><pubDate>Wed, 26 Nov 2025 06:45:59 +0000</pubDate><category>arXiv</category><description>Efficient Multimodal Large Language Models (MLLMs) compress vision tokens to reduce resource consumption, but the loss of visual information can degrade comprehension capabilities. Although some priors introduce Knowledge Distillation to enhance student models, they overlook the fundamental differences in fine-grained vision comprehension caused by unbalanced vision tokens between the efficient student and vanilla teacher. In this paper, we propose EM-KD, a novel paradigm that enhances the Efficient MLLMs with Knowledge Distillation. To overcome the challenge of unbalanced vision tokens, we first calculate the Manhattan distance between the vision logits of teacher and student, and then align them in the spatial dimension with the Hungarian matching algorithm. After alignment, EM-KD introduces two distillation strategies: 1) Vision-Language Affinity Distillation (VLAD) and 2) Vision Semantic Distillation (VSD). Specifically, VLAD calculates the affinity matrix between text tokens and aligned vision tokens, and minimizes the smooth L1 distance of the student and the teacher affinity matrices. Considering the semantic richness of vision logits in the final layer, VSD employs the reverse KL divergence to measure the discrete probability distributions of the aligned vision logits over the vocabulary space. Comprehensive evaluation on diverse benchmarks demonstrates that EM-KD trained model outperforms prior Efficient MLLMs on both accuracy and efficiency with a large margin, validating its effectiveness. Compared with previous distillation methods, which are equipped with our proposed vision token matching strategy for fair comparison, EM-KD also achieves better performance.
Published: 2025-11-26T06:45:59+00:00
Venue: arXiv
Score: 0.785 (must_read)</description></item><item><title>Frequency-Aware Token Reduction for Efficient Vision Transformer</title><link>https://arxiv.org/abs/2511.21477v1</link><guid>http://arxiv.org/abs/2511.21477v1</guid><pubDate>Wed, 26 Nov 2025 15:10:04 +0000</pubDate><category>arXiv</category><description>Vision Transformers have demonstrated exceptional performance across various computer vision tasks, yet their quadratic computational complexity concerning token length remains a significant challenge. To address this, token reduction methods have been widely explored. However, existing approaches often overlook the frequency characteristics of self-attention, such as rank collapsing and over-smoothing phenomenon. In this paper, we propose a frequency-aware token reduction strategy that improves computational efficiency while preserving performance by mitigating rank collapsing. Our method partitions tokens into high-frequency tokens and low-frequency tokens. high-frequency tokens are selectively preserved, while low-frequency tokens are aggregated into a compact direct current token to retain essential low-frequency components. Through extensive experiments and analysis, we demonstrate that our approach significantly improves accuracy while reducing computational overhead and mitigating rank collapsing and over smoothing. Furthermore, we analyze the previous methods, shedding light on their implicit frequency characteristics and limitations.
Published: 2025-11-26T15:10:04+00:00
Venue: arXiv
Score: 0.785 (must_read)</description></item><item><title>MiDUNet: Model Inspired Deep Unfolding Network for Non-homogeneous Image Dehazing</title><link>https://doi.org/10.1109/tgrs.2025.3638128</link><guid>10.1109/tgrs.2025.3638128</guid><pubDate>Thu, 27 Nov 2025 18:57:52 +0000</pubDate><category>IEEE Transactions on Geoscience and Remote Sensing</category><description>Optical remote sensing techniques, particularly those relying on visible optical sensors, are critical for Earth observations. However, atmospheric haze severely degrades image quality owing to light scattering effects. These effects often lead to clouds in the observed images, resulting in partial image degradation. To address the challenge of non-homogeneous image dehazing, we propose a model-inspired deep unfolding network (MiDUNet). The architecture of MiDUNet is strictly derived and inspired by a novel alternating minimization algorithm. During algorithm design, traditional image regularization terms are replaced by several nonlinear functions to align with neural network processing, increasing flexibility and applicability. Afterward, a Multi-scale Spatial frequency Residual Block (MSRB) is used to capture complex scene structures by fusing multi-scale spatial information and frequency information. With these spatial-frequency features and a Multi-path Feature Enhancement Block (MFEB), local and global information are obtained, which increases the robustness to non-homogeneous dehazing and image texture preservation. Numerical experiments with remote sensing images and natural images demonstrate the competitive advantages of MiDUNet in non-homogeneous dense haze scenarios.
Published: 2025-11-27T18:57:52+00:00
Venue: IEEE Transactions on Geoscience and Remote Sensing
Score: 0.782 (must_read)</description></item><item><title>Quality-aware Spatio-temporal Transformer Network for RGBT Tracking</title><link>https://doi.org/10.1109/tip.2025.3635483</link><guid>10.1109/tip.2025.3635483</guid><pubDate>Thu, 27 Nov 2025 19:00:17 +0000</pubDate><category>IEEE Transactions on Image Processing</category><description>Transformer-based RGBT tracking has attracted much attention due to the strong modeling capacity of self attention and cross attention mechanisms. These attention mechanisms utilize the correlations among tokens to construct powerful feature representations, but are easily affected by low-quality tokens. To address this issue, we propose a novel Quality-aware Spatio-temporal Transformer Network (QSTNet), which calculates the quality weights of tokens in search regions based on the correlation with multimodal template tokens to suppress the negative effects of low-quality tokens in spatio-temporal feature representations, for robust RGBT tracking. In particular, we argue that the correlation between search tokens of one modality and multimodal template tokens could reflect the quality of these search tokens, and thus design the Quality-aware Token Weighting Module (QTWM) based on the correlation matrix of search and template tokens to suppress the negative effects of low-quality tokens. Specifically, we calculate the difference matrix derived from the attention matrices of the search tokens from both modalities and the multimodal template tokens, and then assign the quality weight for each search token based on the difference matrix, which reflects the relative correlation of search tokens from different modalities to multimodal template tokens. In addition, we propose the Prompt-based Spatio-temporal Encoder Module (PSEM) to utilize spatio-temporal multimodal information while alleviating the impact of low-quality spatio-temporal features. Extensive experiments on four RGBT benchmark datasets demonstrate that the proposed QSTNet exhibits superior performance compared to other state-of-the-art tracking methods. Our code and supplementary video are now available: https://zhaodongah.github.io/QSTNet.
Published: 2025-11-27T19:00:17+00:00
Venue: IEEE Transactions on Image Processing
Score: 0.782 (must_read)</description></item><item><title>Knowledge Completes the Vision: A Multimodal Entity-aware Retrieval-Augmented Generation Framework for News Image Captioning</title><link>https://arxiv.org/abs/2511.21002v1</link><guid>http://arxiv.org/abs/2511.21002v1</guid><pubDate>Wed, 26 Nov 2025 03:03:52 +0000</pubDate><category>arXiv</category><description>News image captioning aims to produce journalistically informative descriptions by combining visual content with contextual cues from associated articles. Despite recent advances, existing methods struggle with three key challenges: (1) incomplete information coverage, (2) weak cross-modal alignment, and (3) suboptimal visual-entity grounding. To address these issues, we introduce MERGE, the first Multimodal Entity-aware Retrieval-augmented GEneration framework for news image captioning. MERGE constructs an entity-centric multimodal knowledge base (EMKB) that integrates textual, visual, and structured knowledge, enabling enriched background retrieval. It improves cross-modal alignment through a multistage hypothesis-caption strategy and enhances visual-entity matching via dynamic retrieval guided by image content. Extensive experiments on GoodNews and NYTimes800k show that MERGE significantly outperforms state-of-the-art baselines, with CIDEr gains of +6.84 and +1.16 in caption quality, and F1-score improvements of +4.14 and +2.64 in named entity recognition. Notably, MERGE also generalizes well to the unseen Visual News dataset, achieving +20.17 in CIDEr and +6.22 in F1-score, demonstrating strong robustness and domain adaptability.
Published: 2025-11-26T03:03:52+00:00
Venue: arXiv
Score: 0.781 (must_read)</description></item></channel></rss>