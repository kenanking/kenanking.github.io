<?xml version='1.0' encoding='utf-8'?>
<rss xmlns:content="http://purl.org/rss/1.0/modules/content/" xmlns:dc="http://purl.org/dc/elements/1.1/" xmlns:prism="http://prismstandard.org/namespaces/basic/3.0/" version="2.0"><channel><title>ZotWatch Feed</title><link>https://ehehe.cn/zotwatch/</link><description>AI-assisted literature watch</description><lastBuildDate>Thu, 12 Feb 2026 03:53:10 +0000</lastBuildDate><item><title>跨尺度自适应频域增强的海上船舶检测</title><link>https://doi.org/10.11834/jig.250548</link><guid>10.11834/jig.250548</guid><pubDate>Wed, 11 Feb 2026 07:27:49 +0000</pubDate><dc:creator>Wang Yingjun</dc:creator><dc:creator>Yang Xiaopeng</dc:creator><dc:creator>Zhou Ling</dc:creator><dc:creator>Lu Haoxiang</dc:creator><dc:creator>Zhao Wenyi</dc:creator><dc:creator>Zhang Weidong</dc:creator><prism:publicationName>Journal of Image and Graphics</prism:publicationName><prism:doi>10.11834/jig.250548</prism:doi><description>目的海上船舶目标检测对海域管理和交通安全至关重要，但受复杂环境影响，常出现遮挡、模糊和细节丢失等问题，现有方法检测精度不足、易误检漏检，难以满足船舶精确识别需求。基于此，本文提出一种跨尺度自适应频域增强的海上船舶检测方法。方法以YOLO11为基线模型进行针对性改进，首先，设计了一个自适应频域特征增强模块（Adaptive Frequency-domain Feature Enhancement Module， AFEM）用于海上船舶细节特征的增强。该模块针对不同尺度的特征信息，采用傅里叶变换将特征信息转换到频域，通过门控单元对全局和局部信息进行自适应增强，全面增强网络对海上退化特征的提取能力。其次，在颈部引入一个多尺度特征感知模块（Multi-scale Feature Perception Module， MFP）。使用不同的卷积核捕获多尺度特征，高效挖掘并利用海上船舶图像的上下文特征信息，引导网络精准聚焦船舶目标特征，有效抑制复杂背景与遮挡带来的干扰，缓解小目标船舶的特征丢失现象，显著降低海上船舶检测的错检与漏检率。结果在MVDD（Marine Vessel Detection Dataset）和RTTS（Real-world Task-Driven Testing Set）数据集上的平均精确度（mean Average Precision at 50% IOU， mAP50）分别达到95.18％和74.79％，对13类船舶的检测表现优异，尤其在小目标、遮挡船舶检测中优势显著。同时，参数量仅有6.29M，推理速度达到227 FPS（Frames Per Second）。通过与目前最先进的16种不同类型方法的比较，本文提出的方法检测性能更优，在检测精度和模型复杂度之间实现了更好的平衡。结论本文所提方法不仅在海上表现出色，对于陆地的恶劣天气条件也有较强的适应能力，展现出较好的鲁棒性和泛化性，同时具备较高的可部署性和实际应用价值。
Published: 2026-02-11T07:27:49+00:00
Venue: Journal of Image and Graphics
Score: 0.849 (must_read)</description><content:encoded>&lt;p&gt;&lt;strong&gt;Authors:&lt;/strong&gt; Wang Yingjun; Yang Xiaopeng; Zhou Ling; Lu Haoxiang; Zhao Wenyi; Zhang Weidong&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Journal:&lt;/strong&gt; Journal of Image and Graphics&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;a href="https://doi.org/10.11834/jig.250548"&gt;10.11834/jig.250548&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Relevance:&lt;/strong&gt; 0.849 (must_read)&lt;/p&gt;
&lt;p&gt;目的海上船舶目标检测对海域管理和交通安全至关重要，但受复杂环境影响，常出现遮挡、模糊和细节丢失等问题，现有方法检测精度不足、易误检漏检，难以满足船舶精确识别需求。基于此，本文提出一种跨尺度自适应频域增强的海上船舶检测方法。方法以YOLO11为基线模型进行针对性改进，首先，设计了一个自适应频域特征增强模块（Adaptive Frequency-domain Feature Enhancement Module， AFEM）用于海上船舶细节特征的增强。该模块针对不同尺度的特征信息，采用傅里叶变换将特征信息转换到频域，通过门控单元对全局和局部信息进行自适应增强，全面增强网络对海上退化特征的提取能力。其次，在颈部引入一个多尺度特征感知模块（Multi-scale Feature Perception Module， MFP）。使用不同的卷积核捕获多尺度特征，高效挖掘并利用海上船舶图像的上下文特征信息，引导网络精准聚焦船舶目标特征，有效抑制复杂背景与遮挡带来的干扰，缓解小目标船舶的特征丢失现象，显著降低海上船舶检测的错检与漏检率。结果在MVDD（Marine Vessel Detection Dataset）和RTTS（Real-world Task-Driven Testing Set）数据集上的平均精确度（mean Average Precision at 50% IOU， mAP50）分别达到95.18％和74.79％，对13类船舶的检测表现优异，尤其在小目标、遮挡船舶检测中优势显著。同时，参数量仅有6.29M，推理速度达到227 FPS（Frames Per Second）。通过与目前最先进的16种不同类型方法的比较，本文提出的方法检测性能更优，在检测精度和模型复杂度之间实现了更好的平衡。结论本文所提方法不仅在海上表现出色，对于陆地的恶劣天气条件也有较强的适应能力，展现出较好的鲁棒性和泛化性，同时具备较高的可部署性和实际应用价值。&lt;/p&gt;</content:encoded></item><item><title>EAV-DETR: Efficient Arbitrary-View oriented object detection with probabilistic guarantees for UAV imagery</title><link>https://doi.org/10.1016/j.isprsjprs.2026.02.009</link><guid>10.1016/j.isprsjprs.2026.02.009</guid><pubDate>Tue, 10 Feb 2026 05:19:37 +0000</pubDate><dc:creator>Haoyu Zuo</dc:creator><dc:creator>Minghao Ning</dc:creator><dc:creator>Yiming Shu</dc:creator><dc:creator>Shucheng Huang</dc:creator><dc:creator>Chen Sun</dc:creator><prism:publicationName>ISPRS Journal of Photogrammetry and Remote Sensing</prism:publicationName><prism:doi>10.1016/j.isprsjprs.2026.02.009</prism:doi><description>Oriented object detection is critical for enhancing the visual perception of unmanned aerial vehicles (UAVs). However, existing detectors primarily designed for general aerial imagery often struggle to address the unique challenges of UAV imagery, including substantial scale variations, dense clustering, and arbitrary orientations. Furthermore, these models lack probabilistic guarantees required for safety-critical applications. To address these challenges, we propose EAV-DETR, an efficient oriented object detection transformer designed for UAV imagery. Specifically, we first propose a novel scale-adaptive center supervision (SACS) strategy that explicitly enhances the encoder’s feature representations by imposing pixel-level localization constraints with zero inference overhead. Second, we design an anisotropic decoupled rotational attention (ADRA) module, which achieves superior feature alignment for objects of arbitrary morphology by generating a non-rigid adaptive sampling field. Finally, we propose a pose-aware Mondrian conformal prediction (PA-MCP) method, which utilizes the UAV’s flight pose as a physical prior to generate prediction sets with conditional coverage guarantees, thereby providing reliable uncertainty quantification. Extensive experiments on multiple aerial imagery datasets validate the effectiveness of our model. Compared to previous state-of-the-art methods, EAV-DETR improves AP 75 " role="presentation"&gt; AP 75 AP 75 on CODrone by 1.76% while achieving a 52% faster inference speed (46.38 vs 30.55 FPS), and improves AP 50 : 95 " role="presentation"&gt; AP 50 : 95 AP 50 : 95 on UAV-ROD by 3.17%. Our code is available at https://github.com/zzzhak/EAV-DETR .
Published: 2026-02-10T05:19:37+00:00
Venue: ISPRS Journal of Photogrammetry and Remote Sensing
Score: 0.843 (must_read)</description><content:encoded>&lt;p&gt;&lt;strong&gt;Authors:&lt;/strong&gt; Haoyu Zuo; Minghao Ning; Yiming Shu; Shucheng Huang; Chen Sun&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Journal:&lt;/strong&gt; ISPRS Journal of Photogrammetry and Remote Sensing&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;a href="https://doi.org/10.1016/j.isprsjprs.2026.02.009"&gt;10.1016/j.isprsjprs.2026.02.009&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Relevance:&lt;/strong&gt; 0.843 (must_read)&lt;/p&gt;
&lt;p&gt;Oriented object detection is critical for enhancing the visual perception of unmanned aerial vehicles (UAVs). However, existing detectors primarily designed for general aerial imagery often struggle to address the unique challenges of UAV imagery, including substantial scale variations, dense clustering, and arbitrary orientations. Furthermore, these models lack probabilistic guarantees required for safety-critical applications. To address these challenges, we propose EAV-DETR, an efficient oriented object detection transformer designed for UAV imagery. Specifically, we first propose a novel scale-adaptive center supervision (SACS) strategy that explicitly enhances the encoder’s feature representations by imposing pixel-level localization constraints with zero inference overhead. Second, we design an anisotropic decoupled rotational attention (ADRA) module, which achieves superior feature alignment for objects of arbitrary morphology by generating a non-rigid adaptive sampling field. Finally, we propose a pose-aware Mondrian conformal prediction (PA-MCP) method, which utilizes the UAV’s flight pose as a physical prior to generate prediction sets with conditional coverage guarantees, thereby providing reliable uncertainty quantification. Extensive experiments on multiple aerial imagery datasets validate the effectiveness of our model. Compared to previous state-of-the-art methods, EAV-DETR improves AP 75 &amp;quot; role=&amp;quot;presentation&amp;quot;&amp;gt; AP 75 AP 75 on CODrone by 1.76% while achieving a 52% faster inference speed (46.38 vs 30.55 FPS), and improves AP 50 : 95 &amp;quot; role=&amp;quot;presentation&amp;quot;&amp;gt; AP 50 : 95 AP 50 : 95 on UAV-ROD by 3.17%. Our code is available at https://github.com/zzzhak/EAV-DETR .&lt;/p&gt;</content:encoded></item><item><title>FGOM-RTDETR: Far-Shore Guided Object-Focusing Multiscale Network with Real-Time Detection Transformer for Infrared Ship Target Detection</title><link>https://doi.org/10.1109/tgrs.2026.3663601</link><guid>10.1109/tgrs.2026.3663601</guid><pubDate>Wed, 11 Feb 2026 20:55:56 +0000</pubDate><dc:creator>Haobin Wang</dc:creator><dc:creator>Bo-Hui Tang</dc:creator><dc:creator>Fangliang Cai</dc:creator><dc:creator>Menghua Li</dc:creator><dc:creator>Zheng Zhang</dc:creator><dc:creator>Dong Fan</dc:creator><prism:publicationName>IEEE Transactions on Geoscience and Remote Sensing</prism:publicationName><prism:doi>10.1109/tgrs.2026.3663601</prism:doi><description>Infrared ship detection plays a critical role in both civilian and military applications, including tracking, collision avoidance, and maritime security. However, challenges such as low image resolution and complex backgrounds hinder detection accuracy. This study proposes a novel detection algorithm, Far-shore Guided Object-focusing Multiscale Network with Real-Time Detection Transformer (FGOM-RTDETR), which integrates multi-scale local and global features. Built upon the RT-DETR framework, our method introduces a Feature Grouping Module (FGOM) to enhance multi-scale representation. FGOM consists of three key components: the Feature Reparameterization Module (FRep), the Coordinate Attention Golden Feature Pyramid Network (CAGoldFPN), and the Multi-Scale Stacked Network (MuSSNet). The FRep module addresses the loss of channel information caused by the small size of thermal infrared ship targets and the complexity of background features. The CAGoldFPN module improves multi-scale feature fusion, while MuSSNet mitigates issues of high target similarity and the susceptibility of small targets to being overlooked. Experimental results show that, compared with the baseline RT-DETR model, FGOM-RTDETR achieves notable performance gains: precision improves from 0.921 to 0.942, mAP50 rises from 0.938 to 0.958, and recall increases from 0.923 to 0.934. These results demonstrate that FGOM-RTDETR delivers superior detection performance for infrared ship targets.
Published: 2026-02-11T20:55:56+00:00
Venue: IEEE Transactions on Geoscience and Remote Sensing
Score: 0.839 (must_read)</description><content:encoded>&lt;p&gt;&lt;strong&gt;Authors:&lt;/strong&gt; Haobin Wang; Bo-Hui Tang; Fangliang Cai; Menghua Li; Zheng Zhang; Dong Fan&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Journal:&lt;/strong&gt; IEEE Transactions on Geoscience and Remote Sensing&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;a href="https://doi.org/10.1109/tgrs.2026.3663601"&gt;10.1109/tgrs.2026.3663601&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Relevance:&lt;/strong&gt; 0.839 (must_read)&lt;/p&gt;
&lt;p&gt;Infrared ship detection plays a critical role in both civilian and military applications, including tracking, collision avoidance, and maritime security. However, challenges such as low image resolution and complex backgrounds hinder detection accuracy. This study proposes a novel detection algorithm, Far-shore Guided Object-focusing Multiscale Network with Real-Time Detection Transformer (FGOM-RTDETR), which integrates multi-scale local and global features. Built upon the RT-DETR framework, our method introduces a Feature Grouping Module (FGOM) to enhance multi-scale representation. FGOM consists of three key components: the Feature Reparameterization Module (FRep), the Coordinate Attention Golden Feature Pyramid Network (CAGoldFPN), and the Multi-Scale Stacked Network (MuSSNet). The FRep module addresses the loss of channel information caused by the small size of thermal infrared ship targets and the complexity of background features. The CAGoldFPN module improves multi-scale feature fusion, while MuSSNet mitigates issues of high target similarity and the susceptibility of small targets to being overlooked. Experimental results show that, compared with the baseline RT-DETR model, FGOM-RTDETR achieves notable performance gains: precision improves from 0.921 to 0.942, mAP50 rises from 0.938 to 0.958, and recall increases from 0.923 to 0.934. These results demonstrate that FGOM-RTDETR delivers superior detection performance for infrared ship targets.&lt;/p&gt;</content:encoded></item><item><title>Adaptive Fine-Grained Fusion Network for Multimodal UAV Object Detection</title><link>https://doi.org/10.1109/tip.2026.3661868</link><guid>10.1109/tip.2026.3661868</guid><pubDate>Wed, 11 Feb 2026 20:57:46 +0000</pubDate><dc:creator>Zhanyan Tang</dc:creator><dc:creator>Zhihao Wu</dc:creator><dc:creator>Mu Li</dc:creator><dc:creator>Jie Wen</dc:creator><dc:creator>Bob Zhang</dc:creator><dc:creator>Yong Xu</dc:creator><dc:creator>Jianqiang Li</dc:creator><prism:publicationName>IEEE Transactions on Image Processing</prism:publicationName><prism:doi>10.1109/tip.2026.3661868</prism:doi><description>Multimodal perception and fusion play a vital role in unmanned aerial vehicle (UAV) object detection. Existing methods typically adopt global fusion strategies across modalities. However, due to illumination variation, the effectiveness of RGB and infrared modalities may differ across local regions within the same image, particularly in UAV perspectives where occlusions and dense small objects are prevalent, leading to suboptimal performance of global fusion methods. To address this issue, we propose an adaptive fine-grained fusion network for multimodal UAV object detection. First, we design a local feature consistency-based modality fusion module, which adaptively assigns local fusion weights according to the structural consistency of high-response regions across modalities, thereby enabling more effective aggregation of object-relevant features. Second, we introduce a mutual information-guided feature contrastive loss to encourage the preservation of modality-specific information during the early training phase. Experimental results demonstrate that the proposed method effectively addresses the issue of object occlusion in UAV perspectives, achieving state-of-the-art performance on multimodal UAV object detection benchmarks. Code will be available at https://github.com/lingf5877/AFFNet.
Published: 2026-02-11T20:57:46+00:00
Venue: IEEE Transactions on Image Processing
Score: 0.836 (must_read)</description><content:encoded>&lt;p&gt;&lt;strong&gt;Authors:&lt;/strong&gt; Zhanyan Tang; Zhihao Wu; Mu Li; Jie Wen; Bob Zhang; Yong Xu; Jianqiang Li&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Journal:&lt;/strong&gt; IEEE Transactions on Image Processing&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;a href="https://doi.org/10.1109/tip.2026.3661868"&gt;10.1109/tip.2026.3661868&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Relevance:&lt;/strong&gt; 0.836 (must_read)&lt;/p&gt;
&lt;p&gt;Multimodal perception and fusion play a vital role in unmanned aerial vehicle (UAV) object detection. Existing methods typically adopt global fusion strategies across modalities. However, due to illumination variation, the effectiveness of RGB and infrared modalities may differ across local regions within the same image, particularly in UAV perspectives where occlusions and dense small objects are prevalent, leading to suboptimal performance of global fusion methods. To address this issue, we propose an adaptive fine-grained fusion network for multimodal UAV object detection. First, we design a local feature consistency-based modality fusion module, which adaptively assigns local fusion weights according to the structural consistency of high-response regions across modalities, thereby enabling more effective aggregation of object-relevant features. Second, we introduce a mutual information-guided feature contrastive loss to encourage the preservation of modality-specific information during the early training phase. Experimental results demonstrate that the proposed method effectively addresses the issue of object occlusion in UAV perspectives, achieving state-of-the-art performance on multimodal UAV object detection benchmarks. Code will be available at https://github.com/lingf5877/AFFNet.&lt;/p&gt;</content:encoded></item><item><title>What matters in building vision–language–action models for generalist robots</title><link>https://doi.org/10.1038/s42256-025-01168-7</link><guid>10.1038/s42256-025-01168-7</guid><pubDate>Wed, 11 Feb 2026 10:02:25 +0000</pubDate><dc:creator>Xinghang Li</dc:creator><dc:creator>Peiyan Li</dc:creator><dc:creator>Long Qian</dc:creator><dc:creator>Minghuan Liu</dc:creator><dc:creator>Dong Wang</dc:creator><dc:creator>Jirong Liu</dc:creator><dc:creator>Bingyi Kang</dc:creator><dc:creator>Xiao Ma</dc:creator><dc:creator>Xinlong Wang</dc:creator><dc:creator>Di Guo</dc:creator><dc:creator>Tao Kong</dc:creator><dc:creator>Hanbo Zhang</dc:creator><dc:creator>Huaping Liu</dc:creator><prism:publicationName>Nature Machine Intelligence</prism:publicationName><prism:doi>10.1038/s42256-025-01168-7</prism:doi><description>To utilize foundation vision–language models (VLMs) for robotic tasks and motion planning, the community has proposed different methods for injecting action components into VLMs and building the vision–language–action models (VLAs). Here we disclose the key factors that significantly influence the performance of VLA on robot manipulation problems and focus on answering three essential design choices: which backbone to select, how to formulate the VLA architectures and when to add cross-embodiment data. The obtained results convince us firmly to explain why we prefer VLA and develop a new family of VLAs, RoboVLMs, which require very few manual designs and achieve a new state-of-the-art performance in three simulation tasks and real-world experiments. Through our extensive experiments, which include over 8 VLM backbones, 4 policy architectures and over 600 distinct designed experiments, we provide a detailed guidebook for the future design of VLAs. In addition to the study, the highly flexible RoboVLMs framework, which supports easy integrations of new VLMs and free combinations of various design choices, is made public to facilitate future research. We open-source all details, including codes, models, datasets and toolkits, along with detailed training and evaluation recipes at robovlms.github.io . Vision–language–action models recently emerged as a tool for robotics. Here Li and colleagues compare vision–language–action models and highlight what makes a model useful.
Published: 2026-02-11T10:02:25+00:00
Venue: Nature Machine Intelligence
Score: 0.826 (must_read)</description><content:encoded>&lt;p&gt;&lt;strong&gt;Authors:&lt;/strong&gt; Xinghang Li; Peiyan Li; Long Qian; Minghuan Liu; Dong Wang; Jirong Liu; Bingyi Kang; Xiao Ma; Xinlong Wang; Di Guo; Tao Kong; Hanbo Zhang; Huaping Liu&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Journal:&lt;/strong&gt; Nature Machine Intelligence&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;a href="https://doi.org/10.1038/s42256-025-01168-7"&gt;10.1038/s42256-025-01168-7&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Relevance:&lt;/strong&gt; 0.826 (must_read)&lt;/p&gt;
&lt;p&gt;To utilize foundation vision–language models (VLMs) for robotic tasks and motion planning, the community has proposed different methods for injecting action components into VLMs and building the vision–language–action models (VLAs). Here we disclose the key factors that significantly influence the performance of VLA on robot manipulation problems and focus on answering three essential design choices: which backbone to select, how to formulate the VLA architectures and when to add cross-embodiment data. The obtained results convince us firmly to explain why we prefer VLA and develop a new family of VLAs, RoboVLMs, which require very few manual designs and achieve a new state-of-the-art performance in three simulation tasks and real-world experiments. Through our extensive experiments, which include over 8 VLM backbones, 4 policy architectures and over 600 distinct designed experiments, we provide a detailed guidebook for the future design of VLAs. In addition to the study, the highly flexible RoboVLMs framework, which supports easy integrations of new VLMs and free combinations of various design choices, is made public to facilitate future research. We open-source all details, including codes, models, datasets and toolkits, along with detailed training and evaluation recipes at robovlms.github.io . Vision–language–action models recently emerged as a tool for robotics. Here Li and colleagues compare vision–language–action models and highlight what makes a model useful.&lt;/p&gt;</content:encoded></item><item><title>PIA: Fusing Edge Prior Information into Attention for Semantic Segmentation in Vision Transformer</title><link>https://doi.org/10.1016/j.inffus.2026.104222</link><guid>10.1016/j.inffus.2026.104222</guid><pubDate>Wed, 11 Feb 2026 16:53:21 +0000</pubDate><dc:creator>Ruijie Xiao</dc:creator><dc:creator>Bo Yang</dc:creator><dc:creator>Qianyang Zhu</dc:creator><prism:publicationName>Information Fusion</prism:publicationName><prism:doi>10.1016/j.inffus.2026.104222</prism:doi><description>Swin Transformer introduced window self-attention (WSA) to improve the performance of Vision Transformer (ViT) in semantic segmentation. However, the attention patch group partition in WSA is solely based on spatial positions, which ignores the spatial frequency relationships. It may limit the ability of attention mechanisms to fully exploit the inductive biases. To address this issue, we propose a novel attention mechanism that integrates traditional computer vision techniques with deep learning approaches, named P rior I nformation A ttention (PIA). PIA redefines the grouping strategy by fusing edge prior information (edge detection results) to re-organize image patches into flexible group windows. It enables the attention computation to query image patches that share similar edge intensities but are spatially distant. Besides, Feature Exchanging Strategy (FES) is further introduced to refine feature boundaries via cross-group fusion. Building upon PIA and FES, we propose a transformer backbone named PIA Transformer (PIAT). To validate the effectiveness of PIAT, we compare it with the state-of-the-art semantic segmentation models on 4 datasets (Cityscapes, ADE20K, DLRSD and CamVid). Experimental results demonstrate that PIAT outperforms the baseline methods in all four datasets.
Published: 2026-02-11T16:53:21+00:00
Venue: Information Fusion
Score: 0.823 (must_read)</description><content:encoded>&lt;p&gt;&lt;strong&gt;Authors:&lt;/strong&gt; Ruijie Xiao; Bo Yang; Qianyang Zhu&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Journal:&lt;/strong&gt; Information Fusion&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;a href="https://doi.org/10.1016/j.inffus.2026.104222"&gt;10.1016/j.inffus.2026.104222&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Relevance:&lt;/strong&gt; 0.823 (must_read)&lt;/p&gt;
&lt;p&gt;Swin Transformer introduced window self-attention (WSA) to improve the performance of Vision Transformer (ViT) in semantic segmentation. However, the attention patch group partition in WSA is solely based on spatial positions, which ignores the spatial frequency relationships. It may limit the ability of attention mechanisms to fully exploit the inductive biases. To address this issue, we propose a novel attention mechanism that integrates traditional computer vision techniques with deep learning approaches, named P rior I nformation A ttention (PIA). PIA redefines the grouping strategy by fusing edge prior information (edge detection results) to re-organize image patches into flexible group windows. It enables the attention computation to query image patches that share similar edge intensities but are spatially distant. Besides, Feature Exchanging Strategy (FES) is further introduced to refine feature boundaries via cross-group fusion. Building upon PIA and FES, we propose a transformer backbone named PIA Transformer (PIAT). To validate the effectiveness of PIAT, we compare it with the state-of-the-art semantic segmentation models on 4 datasets (Cityscapes, ADE20K, DLRSD and CamVid). Experimental results demonstrate that PIAT outperforms the baseline methods in all four datasets.&lt;/p&gt;</content:encoded></item><item><title>Hybrid Granularity Distribution Estimation for Few-Shot Learning: Statistics Transfer from Categories and Instances</title><link>https://doi.org/10.1109/tip.2026.3661814</link><guid>10.1109/tip.2026.3661814</guid><pubDate>Wed, 11 Feb 2026 20:57:46 +0000</pubDate><dc:creator>Shuo Wang</dc:creator><dc:creator>Tianyu Qi</dc:creator><dc:creator>Xingyu Zhu</dc:creator><dc:creator>Yanbin Hao</dc:creator><dc:creator>Beier Zhu</dc:creator><dc:creator>Hanwang Zhang</dc:creator><dc:creator>Meng Wang</dc:creator><prism:publicationName>IEEE Transactions on Image Processing</prism:publicationName><prism:doi>10.1109/tip.2026.3661814</prism:doi><description>Distribution estimation is a pivotal strategy in few-shot learning (FSL) to mitigate data scarcity by sampling from estimated distributions, utilizing statistical properties (mean and variance) transferred from related base categories. However, category-level estimation alone often fails to generate representative samples due to significant dissimilarities between base and novel categories, leading to suboptimal performance. To address this limitation, we propose Hybrid Granularity Distribution Estimation (HGDE), which integrates both coarse-grained category-level statistics and fine-grained instance-level statistics. By leveraging instance statistics from the nearest base samples, HGDE enhances the characterization of novel categories, capturing subtle features that category-level estimation overlooks. These statistics are fused through linear interpolation to form a robust distribution for novel categories, ensuring both diversity and representativeness in generated samples. Additionally, HGDE employs refined estimation techniques, such as weighted summation for mean calculation and principal component retention for covariance, to further improve accuracy. Empirical evaluations on four FSL benchmarks, including Mini-ImageNet, Tiered-ImageNet, CUB and CIFAR-FS, demonstrate that HGDE offers effective distribution estimation capabilities and leads to notable accuracy gains, with improvements of more than 1.8% in 1-shot tasks on CUB. These results highlight HGDE’s ability to balance mean precision and variance diversity, making it a versatile and effective solution for FSL.
Published: 2026-02-11T20:57:46+00:00
Venue: IEEE Transactions on Image Processing
Score: 0.820 (must_read)</description><content:encoded>&lt;p&gt;&lt;strong&gt;Authors:&lt;/strong&gt; Shuo Wang; Tianyu Qi; Xingyu Zhu; Yanbin Hao; Beier Zhu; Hanwang Zhang; Meng Wang&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Journal:&lt;/strong&gt; IEEE Transactions on Image Processing&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;a href="https://doi.org/10.1109/tip.2026.3661814"&gt;10.1109/tip.2026.3661814&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Relevance:&lt;/strong&gt; 0.820 (must_read)&lt;/p&gt;
&lt;p&gt;Distribution estimation is a pivotal strategy in few-shot learning (FSL) to mitigate data scarcity by sampling from estimated distributions, utilizing statistical properties (mean and variance) transferred from related base categories. However, category-level estimation alone often fails to generate representative samples due to significant dissimilarities between base and novel categories, leading to suboptimal performance. To address this limitation, we propose Hybrid Granularity Distribution Estimation (HGDE), which integrates both coarse-grained category-level statistics and fine-grained instance-level statistics. By leveraging instance statistics from the nearest base samples, HGDE enhances the characterization of novel categories, capturing subtle features that category-level estimation overlooks. These statistics are fused through linear interpolation to form a robust distribution for novel categories, ensuring both diversity and representativeness in generated samples. Additionally, HGDE employs refined estimation techniques, such as weighted summation for mean calculation and principal component retention for covariance, to further improve accuracy. Empirical evaluations on four FSL benchmarks, including Mini-ImageNet, Tiered-ImageNet, CUB and CIFAR-FS, demonstrate that HGDE offers effective distribution estimation capabilities and leads to notable accuracy gains, with improvements of more than 1.8% in 1-shot tasks on CUB. These results highlight HGDE’s ability to balance mean precision and variance diversity, making it a versatile and effective solution for FSL.&lt;/p&gt;</content:encoded></item><item><title>Cross-Modal Mapping: Mitigating the Modality Gap for Few-Shot Classification</title><link>https://doi.org/10.1016/j.patcog.2026.113285</link><guid>10.1016/j.patcog.2026.113285</guid><pubDate>Tue, 10 Feb 2026 15:59:50 +0000</pubDate><dc:creator>Xi Yang</dc:creator><dc:creator>Wulin Xie</dc:creator><dc:creator>Pai Peng</dc:creator><dc:creator>Jie Wen</dc:creator><dc:creator>Xiaohuan Lu</dc:creator><prism:publicationName>Pattern Recognition</prism:publicationName><prism:doi>10.1016/j.patcog.2026.113285</prism:doi><description>Few-shot classification remains a critical challenge in the field of computer vision, particularly in data-scarce environments. Existing methods typically rely on pre-trained visual-language models, such as CLIP. However, due to the modality gap, which is the inconsistent distribution of image and text features in the joint embedding space, directly using these features as class prototypes often leads to suboptimal performance. To address this issue, we propose a novel Cross-Modal Mapping (CMM) method. This method globally aligns image features with the text feature space through linear transformation and optimizes their local spatial relationships using triplet loss, thereby significantly enhancing cross-modal consistency. Experimental results show that compared to other methods, CMM simplifies the training process and demonstrates higher efficiency. Furthermore, CMM improves the average Top-1 accuracy by 1.06% on 11 benchmark datasets compared to methods that partially fine-tune the backbone, and it exhibits excellent performance on 4 distribution-shifted datasets. Notably, CMM effectively mitigates the modality gap in pre-trained models, enabling text features to serve as effective class prototypes for image features, thus providing an efficient and highly generalizable solution for few-shot learning.
Published: 2026-02-10T15:59:50+00:00
Venue: Pattern Recognition
Score: 0.813 (must_read)</description><content:encoded>&lt;p&gt;&lt;strong&gt;Authors:&lt;/strong&gt; Xi Yang; Wulin Xie; Pai Peng; Jie Wen; Xiaohuan Lu&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Journal:&lt;/strong&gt; Pattern Recognition&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;a href="https://doi.org/10.1016/j.patcog.2026.113285"&gt;10.1016/j.patcog.2026.113285&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Relevance:&lt;/strong&gt; 0.813 (must_read)&lt;/p&gt;
&lt;p&gt;Few-shot classification remains a critical challenge in the field of computer vision, particularly in data-scarce environments. Existing methods typically rely on pre-trained visual-language models, such as CLIP. However, due to the modality gap, which is the inconsistent distribution of image and text features in the joint embedding space, directly using these features as class prototypes often leads to suboptimal performance. To address this issue, we propose a novel Cross-Modal Mapping (CMM) method. This method globally aligns image features with the text feature space through linear transformation and optimizes their local spatial relationships using triplet loss, thereby significantly enhancing cross-modal consistency. Experimental results show that compared to other methods, CMM simplifies the training process and demonstrates higher efficiency. Furthermore, CMM improves the average Top-1 accuracy by 1.06% on 11 benchmark datasets compared to methods that partially fine-tune the backbone, and it exhibits excellent performance on 4 distribution-shifted datasets. Notably, CMM effectively mitigates the modality gap in pre-trained models, enabling text features to serve as effective class prototypes for image features, thus providing an efficient and highly generalizable solution for few-shot learning.&lt;/p&gt;</content:encoded></item><item><title>Thermal3D-GS: Physics-induced 3D Gaussians for Thermal Infrared Novel-view Synthesis with a Large-Scale Dataset</title><link>https://doi.org/10.1109/tpami.2026.3663966</link><guid>10.1109/tpami.2026.3663966</guid><pubDate>Wed, 11 Feb 2026 20:55:51 +0000</pubDate><dc:creator>Qian Chen</dc:creator><dc:creator>Shihao Shu</dc:creator><dc:creator>Heng Sun</dc:creator><dc:creator>Junzhang Chen</dc:creator><dc:creator>Xiangzhi Bai</dc:creator><prism:publicationName>IEEE Transactions on Pattern Analysis and Machine Intelligence</prism:publicationName><prism:doi>10.1109/tpami.2026.3663966</prism:doi><description>Thermal infrared imaging has attracted widespread attention in many fields due to the advantages of all-weather imaging and strong penetration. However, existing methods for thermal infrared novel-view synthesis often produce results with coarse details and floating artifacts, primarily caused by physical factors such as atmospheric transmission effects and thermal conduction. These challenges hinder accurate reconstruction of intricate structures and temperature distributions in thermal scenes, limiting the practical utility of previous approaches. To address these limitations, this paper introduces a physics-induced 3D Gaussian splatting method named Thermal3D-GS, the first novel-view synthesis method that relies exclusively on thermal infrared image. Thermal3D-GS begins by modeling atmospheric transmission effects and thermal conduction in three-dimensional media using neural networks. Additionally, considering the sparse features of infrared images, sparse feature priors are designed to improve the reconstruction accuracy of thermal infrared images. Furthermore, to validate the effectiveness of our method, the first large-scale benchmark dataset named Thermal Infrared Novel-view Synthesis Dataset (TI-NSD) is created. This dataset comprises 50 authentic thermal infrared video scenes, covering indoor, outdoor, traffic and UAV(Unmanned Aerial Vehicle) scenarios, with a total of 15,213 frames of thermal infrared image data. In addition, an expanded validation thermal infrared dataset, which includes three high-resolution scenes and five special scenes under varying atmospheric conditions and complex propagation media is constructed to assess generalization performance of the proposed method. Based on this dataset, this paper experimentally verifies the effectiveness of Thermal3D-GS. The results indicate that our method outperforms the baseline method with a 3.19 dB improvement in PSNR and significantly addresses the issues of floaters and indistinct edge features pr...
Published: 2026-02-11T20:55:51+00:00
Venue: IEEE Transactions on Pattern Analysis and Machine Intelligence
Score: 0.812 (must_read)</description><content:encoded>&lt;p&gt;&lt;strong&gt;Authors:&lt;/strong&gt; Qian Chen; Shihao Shu; Heng Sun; Junzhang Chen; Xiangzhi Bai&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Journal:&lt;/strong&gt; IEEE Transactions on Pattern Analysis and Machine Intelligence&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;a href="https://doi.org/10.1109/tpami.2026.3663966"&gt;10.1109/tpami.2026.3663966&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Relevance:&lt;/strong&gt; 0.812 (must_read)&lt;/p&gt;
&lt;p&gt;Thermal infrared imaging has attracted widespread attention in many fields due to the advantages of all-weather imaging and strong penetration. However, existing methods for thermal infrared novel-view synthesis often produce results with coarse details and floating artifacts, primarily caused by physical factors such as atmospheric transmission effects and thermal conduction. These challenges hinder accurate reconstruction of intricate structures and temperature distributions in thermal scenes, limiting the practical utility of previous approaches. To address these limitations, this paper introduces a physics-induced 3D Gaussian splatting method named Thermal3D-GS, the first novel-view synthesis method that relies exclusively on thermal infrared image. Thermal3D-GS begins by modeling atmospheric transmission effects and thermal conduction in three-dimensional media using neural networks. Additionally, considering the sparse features of infrared images, sparse feature priors are designed to improve the reconstruction accuracy of thermal infrared images. Furthermore, to validate the effectiveness of our method, the first large-scale benchmark dataset named Thermal Infrared Novel-view Synthesis Dataset (TI-NSD) is created. This dataset comprises 50 authentic thermal infrared video scenes, covering indoor, outdoor, traffic and UAV(Unmanned Aerial Vehicle) scenarios, with a total of 15,213 frames of thermal infrared image data. In addition, an expanded validation thermal infrared dataset, which includes three high-resolution scenes and five special scenes under varying atmospheric conditions and complex propagation media is constructed to assess generalization performance of the proposed method. Based on this dataset, this paper experimentally verifies the effectiveness of Thermal3D-GS. The results indicate that our method outperforms the baseline method with a 3.19 dB improvement in PSNR and significantly addresses the issues of floaters and indistinct edge features pr...&lt;/p&gt;</content:encoded></item><item><title>Pyramid Token Pruning for High-Resolution Large Vision-Language Models via Region, Token, and Instruction-Guided Importance</title><link>https://doi.org/10.1109/tcsvt.2026.3663658</link><guid>10.1109/tcsvt.2026.3663658</guid><pubDate>Wed, 11 Feb 2026 20:57:15 +0000</pubDate><dc:creator>Yuxuan Liang</dc:creator><dc:creator>Xu Li</dc:creator><dc:creator>Xiaolei Chen</dc:creator><dc:creator>Haotian Chen</dc:creator><dc:creator>Yi Zhen</dc:creator><dc:creator>Zhe Liu</dc:creator><dc:creator>Rui Zhu</dc:creator><dc:creator>Bin Li</dc:creator><dc:creator>Xiangyang Xue</dc:creator><prism:publicationName>IEEE Transactions on Circuits and Systems for Video Technology</prism:publicationName><prism:doi>10.1109/tcsvt.2026.3663658</prism:doi><description>Large Vision-Language Models (LVLMs) have recently demonstrated strong multimodal understanding, yet their fine-grained visual perception is often constrained by low input resolutions. A common remedy is to partition high-resolution images into multiple sub-images for separate encoding, but this approach drastically inflates the number of visual tokens and introduces prohibitive inference overhead. To overcome this challenge, we propose Pyramid Token Pruning (PTP), a training-free strategy that hierarchically integrates bottom-up visual saliency at both region and token levels with top-down instruction-guided relevance. Inspired by human visual cognition, PTP selectively preserves more tokens from salient regions while further emphasizing those most relevant to task instructions. Extensive experiments on 13 diverse benchmarks show that PTP substantially reduces computational cost, memory usage, and inference latency, with negligible performance degradation.
Published: 2026-02-11T20:57:15+00:00
Venue: IEEE Transactions on Circuits and Systems for Video Technology
Score: 0.811 (must_read)</description><content:encoded>&lt;p&gt;&lt;strong&gt;Authors:&lt;/strong&gt; Yuxuan Liang; Xu Li; Xiaolei Chen; Haotian Chen; Yi Zhen; Zhe Liu; Rui Zhu; Bin Li; Xiangyang Xue&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Journal:&lt;/strong&gt; IEEE Transactions on Circuits and Systems for Video Technology&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;a href="https://doi.org/10.1109/tcsvt.2026.3663658"&gt;10.1109/tcsvt.2026.3663658&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Relevance:&lt;/strong&gt; 0.811 (must_read)&lt;/p&gt;
&lt;p&gt;Large Vision-Language Models (LVLMs) have recently demonstrated strong multimodal understanding, yet their fine-grained visual perception is often constrained by low input resolutions. A common remedy is to partition high-resolution images into multiple sub-images for separate encoding, but this approach drastically inflates the number of visual tokens and introduces prohibitive inference overhead. To overcome this challenge, we propose Pyramid Token Pruning (PTP), a training-free strategy that hierarchically integrates bottom-up visual saliency at both region and token levels with top-down instruction-guided relevance. Inspired by human visual cognition, PTP selectively preserves more tokens from salient regions while further emphasizing those most relevant to task instructions. Extensive experiments on 13 diverse benchmarks show that PTP substantially reduces computational cost, memory usage, and inference latency, with negligible performance degradation.&lt;/p&gt;</content:encoded></item><item><title>Complex-Valued Source-Free Domain Adaptation for PolSAR Image Classification</title><link>https://doi.org/10.1109/tgrs.2026.3663159</link><guid>10.1109/tgrs.2026.3663159</guid><pubDate>Tue, 10 Feb 2026 21:05:00 +0000</pubDate><dc:creator>Ningwei Wang</dc:creator><dc:creator>Weiqiang Jin</dc:creator><dc:creator>Haixia Bi</dc:creator><dc:creator>Chen Xu</dc:creator><dc:creator>Fan Li</dc:creator><prism:publicationName>IEEE Transactions on Geoscience and Remote Sensing</prism:publicationName><prism:doi>10.1109/tgrs.2026.3663159</prism:doi><description>The discrepancies of sensors, acquisition conditions and terrain class distributions in polarimetric synthetic aperture radar (PolSAR) data has aroused the cross domain PolSAR image classification problem. Domain adaptation, which aims to improve the classification performance of target domain with knowledge from the source domain, is a promising approach to address this issue. However, conventional domain adaptation methods typically assume access to both source and target domain data, which may be infeasible in real-world settings due to data privacy and confidentiality concerns. Therefore, it is crucial to develop source-free domain adaptation PolSAR image classification methods which rely only on target domain data. In this scenario, how to make full use of the label sparse target domain data is a consequent challenge to overcome. To this end, we propose CVSFDA, a complex-valued source-free domain adaptation framework tailored for PolSAR image classification. CVSFDA incorporates a complex-valued multiscale prototypical matching module (CVMP) and a complex-valued relation network (CVRN). Given the pretrained source domain model and the limited labeled samples in target domain, CVMP captures the similarities between samples leveraging the hierarchical spatial information across multiple encoder layers. A similarity convolutional network is further devised in CVRN, to comprehensively model class-specific information in the target domain. Extensive experiments on four benchmark PolSAR datasets demonstrate that CVSFDA achieves superior classification accuracy and generalization ability compared to existing domain adaptation methods.
Published: 2026-02-10T21:05:00+00:00
Venue: IEEE Transactions on Geoscience and Remote Sensing
Score: 0.808 (must_read)</description><content:encoded>&lt;p&gt;&lt;strong&gt;Authors:&lt;/strong&gt; Ningwei Wang; Weiqiang Jin; Haixia Bi; Chen Xu; Fan Li&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Journal:&lt;/strong&gt; IEEE Transactions on Geoscience and Remote Sensing&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;a href="https://doi.org/10.1109/tgrs.2026.3663159"&gt;10.1109/tgrs.2026.3663159&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Relevance:&lt;/strong&gt; 0.808 (must_read)&lt;/p&gt;
&lt;p&gt;The discrepancies of sensors, acquisition conditions and terrain class distributions in polarimetric synthetic aperture radar (PolSAR) data has aroused the cross domain PolSAR image classification problem. Domain adaptation, which aims to improve the classification performance of target domain with knowledge from the source domain, is a promising approach to address this issue. However, conventional domain adaptation methods typically assume access to both source and target domain data, which may be infeasible in real-world settings due to data privacy and confidentiality concerns. Therefore, it is crucial to develop source-free domain adaptation PolSAR image classification methods which rely only on target domain data. In this scenario, how to make full use of the label sparse target domain data is a consequent challenge to overcome. To this end, we propose CVSFDA, a complex-valued source-free domain adaptation framework tailored for PolSAR image classification. CVSFDA incorporates a complex-valued multiscale prototypical matching module (CVMP) and a complex-valued relation network (CVRN). Given the pretrained source domain model and the limited labeled samples in target domain, CVMP captures the similarities between samples leveraging the hierarchical spatial information across multiple encoder layers. A similarity convolutional network is further devised in CVRN, to comprehensively model class-specific information in the target domain. Extensive experiments on four benchmark PolSAR datasets demonstrate that CVSFDA achieves superior classification accuracy and generalization ability compared to existing domain adaptation methods.&lt;/p&gt;</content:encoded></item><item><title>Vehicle-centric Perception via Multimodal Structured Pre-training</title><link>https://doi.org/10.1109/tcsvt.2026.3663409</link><guid>10.1109/tcsvt.2026.3663409</guid><pubDate>Tue, 10 Feb 2026 21:06:49 +0000</pubDate><dc:creator>Wentao Wu</dc:creator><dc:creator>Xiao Wang</dc:creator><dc:creator>Chenglong Li</dc:creator><dc:creator>Jin Tang</dc:creator><dc:creator>Bin Luo</dc:creator><prism:publicationName>IEEE Transactions on Circuits and Systems for Video Technology</prism:publicationName><prism:doi>10.1109/tcsvt.2026.3663409</prism:doi><description>Vehicle-centric perception plays a crucial role in many intelligent systems, including large-scale surveillance systems, intelligent transportation, and autonomous driving. Existing approaches typically employ general pre-trained weights to initialize backbone networks, followed by task-specific fine-tuning. However, these models lack effective learning of vehiclerelated knowledge during pre-training, resulting in poor capability for modeling general vehicle perception representations. To handle this problem, we propose VehicleMAE-V2, a novel vehicle-centric pre-trained large model. By exploring and exploiting vehicle-related multimodal structured priors to guide the masked token reconstruction process, our approach can significantly enhance the model’s capability to learn generalizable representations for vehicle-centric perception. Specifically, we design the Symmetry-guided Mask Module (SMM), Contour-guided Representation Module (CRM) and Semantics-guided Representation Module (SRM) to incorporate three kinds of structured priors into token reconstruction including symmetry, contour and semantics of vehicles respectively. SMM utilizes the vehicle symmetry constraints to avoid retaining symmetric patches and can thus select high-quality masked image patches and reduce information redundancy. CRM minimizes the prob23 ability distribution divergence between contour features and reconstructed features and can thus preserve holistic vehicle structure information during pixel-level reconstruction. SRM aligns image-text features through contrastive learning and cross-modal distillation to address the feature confusion caused by insufficient semantic understanding during masked reconstruction. To support the pre-training of VehicleMAE-V2, we construct Autobot4M, a large-scale dataset comprising approximately 4 million vehicle images and 12,693 text descriptions. Extensive experiments on five downstream tasks demonstrate the superior performance of VehicleMAE-V2. The sour...
Published: 2026-02-10T21:06:49+00:00
Venue: IEEE Transactions on Circuits and Systems for Video Technology
Score: 0.806 (must_read)</description><content:encoded>&lt;p&gt;&lt;strong&gt;Authors:&lt;/strong&gt; Wentao Wu; Xiao Wang; Chenglong Li; Jin Tang; Bin Luo&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Journal:&lt;/strong&gt; IEEE Transactions on Circuits and Systems for Video Technology&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;a href="https://doi.org/10.1109/tcsvt.2026.3663409"&gt;10.1109/tcsvt.2026.3663409&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Relevance:&lt;/strong&gt; 0.806 (must_read)&lt;/p&gt;
&lt;p&gt;Vehicle-centric perception plays a crucial role in many intelligent systems, including large-scale surveillance systems, intelligent transportation, and autonomous driving. Existing approaches typically employ general pre-trained weights to initialize backbone networks, followed by task-specific fine-tuning. However, these models lack effective learning of vehiclerelated knowledge during pre-training, resulting in poor capability for modeling general vehicle perception representations. To handle this problem, we propose VehicleMAE-V2, a novel vehicle-centric pre-trained large model. By exploring and exploiting vehicle-related multimodal structured priors to guide the masked token reconstruction process, our approach can significantly enhance the model’s capability to learn generalizable representations for vehicle-centric perception. Specifically, we design the Symmetry-guided Mask Module (SMM), Contour-guided Representation Module (CRM) and Semantics-guided Representation Module (SRM) to incorporate three kinds of structured priors into token reconstruction including symmetry, contour and semantics of vehicles respectively. SMM utilizes the vehicle symmetry constraints to avoid retaining symmetric patches and can thus select high-quality masked image patches and reduce information redundancy. CRM minimizes the prob23 ability distribution divergence between contour features and reconstructed features and can thus preserve holistic vehicle structure information during pixel-level reconstruction. SRM aligns image-text features through contrastive learning and cross-modal distillation to address the feature confusion caused by insufficient semantic understanding during masked reconstruction. To support the pre-training of VehicleMAE-V2, we construct Autobot4M, a large-scale dataset comprising approximately 4 million vehicle images and 12,693 text descriptions. Extensive experiments on five downstream tasks demonstrate the superior performance of VehicleMAE-V2. The sour...&lt;/p&gt;</content:encoded></item><item><title>红外视频卫星空中动目标检测数据集及其评估</title><link>https://doi.org/10.11834/jig.250536</link><guid>10.11834/jig.250536</guid><pubDate>Wed, 11 Feb 2026 07:27:45 +0000</pubDate><dc:creator>Li Ruojing</dc:creator><dc:creator>Li Zhaoxu</dc:creator><dc:creator>Chen Nuo</dc:creator><dc:creator>Guo Gaowei</dc:creator><dc:creator>Dou Zechao</dc:creator><dc:creator>Long Zhengxing</dc:creator><dc:creator>Luo Yihang</dc:creator><dc:creator>Zeng Yaoyuan</dc:creator><dc:creator>Sheng Weidong</dc:creator><dc:creator>Li Boyang</dc:creator><dc:creator>Li Zhijun</dc:creator><dc:creator>Li Miao</dc:creator><dc:creator>An Wei</dc:creator><dc:creator>Li Haixin</dc:creator><dc:creator>Yu Zhiqiang</dc:creator><dc:creator>Yin Xiaoyu</dc:creator><dc:creator>Zha Xuyang</dc:creator><dc:creator>Zeng Baiwen</dc:creator><dc:creator>Wang Wufan</dc:creator><dc:creator>Zhang Bo</dc:creator><dc:creator>Lu Yue</dc:creator><dc:creator>Chu Donghao</dc:creator><dc:creator>Li Ziyi</dc:creator><dc:creator>Huang Kangwei</dc:creator><dc:creator>Yang Borui</dc:creator><dc:creator>Xing Yinghui</dc:creator><dc:creator>Zhang Shizhou</dc:creator><dc:creator>张世周</dc:creator><prism:publicationName>Journal of Image and Graphics</prism:publicationName><prism:doi>10.11834/jig.250536</prism:doi><description>目的红外视频卫星是探测空中动目标的重要手段，红外小目标检测技术是其关键基础。深度学习显著推动了单帧红外小目标检测，然而卫星红外视频中的空中动目标普遍空域显著性低、场景复杂，单帧方法难以有效检测，因此亟需发展融合时域信息的红外极弱小目标检测技术。但该领域长期缺乏视频数据集，严重制约了相关技术的发展与应用。为突破此瓶颈，该文构建了首个包含大量真实场景的红外视频卫星空中动目标检测数据集。方法基于武汉一号卫星采集20126帧真实红外视频卫星动目标数据，设计两阶段“由粗到精”的标注方法，完成29757个空中动目标的精标注。为了丰富场景多样性，进一步融合两大真实天基背景下的仿真动目标数据，构建包含1401个真实场景、122265帧视频图像、454116个目标的红外视频卫星空中动目标检测数据集。数据集提供实例级掩码标签，支持空中动目标检测与跟踪技术研究，并提出了相关评价指标。结果数据分析表明，该数据集中真实目标的平均信噪比仅为3.06，超过80%的目标信噪比低于2，且实测场景中的目标与背景存在丰富的动态变化与相互干扰，呈现难以模拟的复杂性。结论基于该数据集开展了首届红外视频卫星空中动目标检测比赛，充分验证了该数据集的高挑战性与实际价值，对于红外极弱小目标检测技术研究具有重要支持作用。数据集获取链接：https：//github.com/TinaLRJ/DeepPro（科学数据银行：Infrared video satellite aerial moving target detection dataset）。
Published: 2026-02-11T07:27:45+00:00
Venue: Journal of Image and Graphics
Score: 0.805 (must_read)</description><content:encoded>&lt;p&gt;&lt;strong&gt;Authors:&lt;/strong&gt; Li Ruojing; Li Zhaoxu; Chen Nuo; Guo Gaowei; Dou Zechao; Long Zhengxing; Luo Yihang; Zeng Yaoyuan; Sheng Weidong; Li Boyang; Li Zhijun; Li Miao; An Wei; Li Haixin; Yu Zhiqiang; Yin Xiaoyu; Zha Xuyang; Zeng Baiwen; Wang Wufan; Zhang Bo; Lu Yue; Chu Donghao; Li Ziyi; Huang Kangwei; Yang Borui; Xing Yinghui; Zhang Shizhou; 张世周&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Journal:&lt;/strong&gt; Journal of Image and Graphics&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;a href="https://doi.org/10.11834/jig.250536"&gt;10.11834/jig.250536&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Relevance:&lt;/strong&gt; 0.805 (must_read)&lt;/p&gt;
&lt;p&gt;目的红外视频卫星是探测空中动目标的重要手段，红外小目标检测技术是其关键基础。深度学习显著推动了单帧红外小目标检测，然而卫星红外视频中的空中动目标普遍空域显著性低、场景复杂，单帧方法难以有效检测，因此亟需发展融合时域信息的红外极弱小目标检测技术。但该领域长期缺乏视频数据集，严重制约了相关技术的发展与应用。为突破此瓶颈，该文构建了首个包含大量真实场景的红外视频卫星空中动目标检测数据集。方法基于武汉一号卫星采集20126帧真实红外视频卫星动目标数据，设计两阶段“由粗到精”的标注方法，完成29757个空中动目标的精标注。为了丰富场景多样性，进一步融合两大真实天基背景下的仿真动目标数据，构建包含1401个真实场景、122265帧视频图像、454116个目标的红外视频卫星空中动目标检测数据集。数据集提供实例级掩码标签，支持空中动目标检测与跟踪技术研究，并提出了相关评价指标。结果数据分析表明，该数据集中真实目标的平均信噪比仅为3.06，超过80%的目标信噪比低于2，且实测场景中的目标与背景存在丰富的动态变化与相互干扰，呈现难以模拟的复杂性。结论基于该数据集开展了首届红外视频卫星空中动目标检测比赛，充分验证了该数据集的高挑战性与实际价值，对于红外极弱小目标检测技术研究具有重要支持作用。数据集获取链接：https：//github.com/TinaLRJ/DeepPro（科学数据银行：Infrared video satellite aerial moving target detection dataset）。&lt;/p&gt;</content:encoded></item><item><title>Knowledge-data-model-driven multimodal few-shot learning for hyperspectral fine classification: Generalization across sensor, category and scene</title><link>https://doi.org/10.1016/j.isprsjprs.2026.02.001</link><guid>10.1016/j.isprsjprs.2026.02.001</guid><pubDate>Wed, 11 Feb 2026 20:40:55 +0000</pubDate><dc:creator>Qiqi Zhu</dc:creator><dc:creator>Mingzhen Xu</dc:creator><dc:creator>Rui Ma</dc:creator><dc:creator>Longli Ran</dc:creator><dc:creator>Jiayao Xue</dc:creator><dc:creator>Qingfeng Guan</dc:creator><prism:publicationName>ISPRS Journal of Photogrammetry and Remote Sensing</prism:publicationName><prism:doi>10.1016/j.isprsjprs.2026.02.001</prism:doi><description>Fine-grained land-cover mapping is crucial for accurately assessing environmental degradation and monitoring socioeconomic dynamics. Few-shot learning of hyperspectral images offers a promising solution in cases where sample collection is limited. However, previous studies, such as tree species mapping, typically use 1% or 0.5% of samples per class, yielding thousands of samples for common species but struggling to identify unseen or rare species (only one sample/shot) in real-world scenarios. Furthermore, inevitable cross-sensor, cross-category, and cross-scene variations significantly increase the occurrence of unseen or rare classes and spectral heterogeneity within common land-cover types. To this being, we propose Knowing-Net, a knowledge-data-model-driven multimodal few-shot learning network, to bridge the application gap for fine-grained mapping of unseen or rare classes. In Knowing-Net, prior knowledge of sensor, i.e., spectral parameters, is leveraged to reconstruct cross-sensor hyperspectral images, mitigating heterogeneity in spectral responses across datasets and enabling cross-domain transfer across different sensors, scenes, and land cover types. To breakthrough the gap in recognizing unseen classes, multimodal data, including textual descriptions and natural images of unseen classes, is embedded into network to construct shared side information through modality-specific feature learning. By designing a cross-alignment mechanism for hyperspectral and multimodal information in a shared semantic space, distinct encoders are guided to produce consistent distribution for the same class across different modalities, reducing sample dependency and facilitating the identification of unseen or rare classes. Finally, inspired by the first law of geography, a sliding discriminant window is designed to incorporate spatial context, enhancing geography interpretability and robustness to noise. We evaluate Knowing-Net on five challenging airborne hyperspectral datasets with a fine-grained classification system, covering crop type, tree species, and similar urban land covers with varying materials. Extensive experiments on five datasets consistently demonstrate Knowing-net’s superiority over state-of-the-art methods in both mapping performance and cross-domain generalization. Notably, the unified framework achieves state-of-the-art results in one-shot learning and establishes a new paradigm in zero-shot classification for fine-grained land cover tasks. To the best of our knowledge, this is the first comprehensive generalization of FSL across sensor, category, and scene for hyperspectral image-based fine mapping.
Published: 2026-02-11T20:40:55+00:00
Venue: ISPRS Journal of Photogrammetry and Remote Sensing
Score: 0.800 (must_read)</description><content:encoded>&lt;p&gt;&lt;strong&gt;Authors:&lt;/strong&gt; Qiqi Zhu; Mingzhen Xu; Rui Ma; Longli Ran; Jiayao Xue; Qingfeng Guan&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Journal:&lt;/strong&gt; ISPRS Journal of Photogrammetry and Remote Sensing&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;a href="https://doi.org/10.1016/j.isprsjprs.2026.02.001"&gt;10.1016/j.isprsjprs.2026.02.001&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Relevance:&lt;/strong&gt; 0.800 (must_read)&lt;/p&gt;
&lt;p&gt;Fine-grained land-cover mapping is crucial for accurately assessing environmental degradation and monitoring socioeconomic dynamics. Few-shot learning of hyperspectral images offers a promising solution in cases where sample collection is limited. However, previous studies, such as tree species mapping, typically use 1% or 0.5% of samples per class, yielding thousands of samples for common species but struggling to identify unseen or rare species (only one sample/shot) in real-world scenarios. Furthermore, inevitable cross-sensor, cross-category, and cross-scene variations significantly increase the occurrence of unseen or rare classes and spectral heterogeneity within common land-cover types. To this being, we propose Knowing-Net, a knowledge-data-model-driven multimodal few-shot learning network, to bridge the application gap for fine-grained mapping of unseen or rare classes. In Knowing-Net, prior knowledge of sensor, i.e., spectral parameters, is leveraged to reconstruct cross-sensor hyperspectral images, mitigating heterogeneity in spectral responses across datasets and enabling cross-domain transfer across different sensors, scenes, and land cover types. To breakthrough the gap in recognizing unseen classes, multimodal data, including textual descriptions and natural images of unseen classes, is embedded into network to construct shared side information through modality-specific feature learning. By designing a cross-alignment mechanism for hyperspectral and multimodal information in a shared semantic space, distinct encoders are guided to produce consistent distribution for the same class across different modalities, reducing sample dependency and facilitating the identification of unseen or rare classes. Finally, inspired by the first law of geography, a sliding discriminant window is designed to incorporate spatial context, enhancing geography interpretability and robustness to noise. We evaluate Knowing-Net on five challenging airborne hyperspectral datasets with a fine-grained classification system, covering crop type, tree species, and similar urban land covers with varying materials. Extensive experiments on five datasets consistently demonstrate Knowing-net’s superiority over state-of-the-art methods in both mapping performance and cross-domain generalization. Notably, the unified framework achieves state-of-the-art results in one-shot learning and establishes a new paradigm in zero-shot classification for fine-grained land cover tasks. To the best of our knowledge, this is the first comprehensive generalization of FSL across sensor, category, and scene for hyperspectral image-based fine mapping.&lt;/p&gt;</content:encoded></item><item><title>HSI-LiDAR Joint Classification via Progressive Spatial-Spectral-Frequency Fusion Learning</title><link>https://doi.org/10.1109/tgrs.2026.3663376</link><guid>10.1109/tgrs.2026.3663376</guid><pubDate>Tue, 10 Feb 2026 21:05:00 +0000</pubDate><dc:creator>Xinxin Liu</dc:creator><dc:creator>Xiaoqing Tang</dc:creator><dc:creator>Ting Lu</dc:creator><dc:creator>Kexin Ding</dc:creator><prism:publicationName>IEEE Transactions on Geoscience and Remote Sensing</prism:publicationName><prism:doi>10.1109/tgrs.2026.3663376</prism:doi><description>Hyperspectral imagery (HSI) and light detection and ranging (LiDAR) data joint classification leverages multi-modal information to improve classification performance. However, existing methods often suffer from inadequate feature representation due to their reliance on spatial-spectral domains while ignoring complementary frequency-domain cues, limiting their ability to capture cross-modal variations. Moreover, even when frequency information is incorporated, most approaches fail to achieve deep cross-domain interaction, either treating spatial and frequency features in isolation or relying on single-stage fusion, which hinders effective modality gap bridging. To address these challenges, we propose a novel progressive spatial-spectral-frequency fusion learning (PS2F2L) network, a lightweight yet powerful framework for HSI and LiDAR classification. Our method employs a multi-stage progressive fusion strategy to hierarchically integrate multimodal features, mitigating modal conflicts while capturing discriminative features. In the first stage, dual branches—spatial-spectral feature learning (S2FL) and spatial-frequency feature learning (SF2L)—jointly extract features from complementary domains, overcoming single-domain limitations. The S2FL branch combines 1D/2D convolutions to model spectral-spatial relationships, while SF2L utilizes discrete wavelet transforms to capture spatial-frequency patterns. In the second stage, an interactive spatial-spectral-frequency fusion module enhances feature discriminability by promoting deep information exchange between spatial-spectral and spatial-requency representations. Finally, adaptive decision-level fusion refines classification by consolidating multi-domain predictions. Extensive experiments on three public datasets demonstrate the superiority of PS2F2L, validating its effectiveness in achieving robust and accurate multimodal classification.
Published: 2026-02-10T21:05:00+00:00
Venue: IEEE Transactions on Geoscience and Remote Sensing
Score: 0.800 (must_read)</description><content:encoded>&lt;p&gt;&lt;strong&gt;Authors:&lt;/strong&gt; Xinxin Liu; Xiaoqing Tang; Ting Lu; Kexin Ding&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Journal:&lt;/strong&gt; IEEE Transactions on Geoscience and Remote Sensing&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;a href="https://doi.org/10.1109/tgrs.2026.3663376"&gt;10.1109/tgrs.2026.3663376&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Relevance:&lt;/strong&gt; 0.800 (must_read)&lt;/p&gt;
&lt;p&gt;Hyperspectral imagery (HSI) and light detection and ranging (LiDAR) data joint classification leverages multi-modal information to improve classification performance. However, existing methods often suffer from inadequate feature representation due to their reliance on spatial-spectral domains while ignoring complementary frequency-domain cues, limiting their ability to capture cross-modal variations. Moreover, even when frequency information is incorporated, most approaches fail to achieve deep cross-domain interaction, either treating spatial and frequency features in isolation or relying on single-stage fusion, which hinders effective modality gap bridging. To address these challenges, we propose a novel progressive spatial-spectral-frequency fusion learning (PS2F2L) network, a lightweight yet powerful framework for HSI and LiDAR classification. Our method employs a multi-stage progressive fusion strategy to hierarchically integrate multimodal features, mitigating modal conflicts while capturing discriminative features. In the first stage, dual branches—spatial-spectral feature learning (S2FL) and spatial-frequency feature learning (SF2L)—jointly extract features from complementary domains, overcoming single-domain limitations. The S2FL branch combines 1D/2D convolutions to model spectral-spatial relationships, while SF2L utilizes discrete wavelet transforms to capture spatial-frequency patterns. In the second stage, an interactive spatial-spectral-frequency fusion module enhances feature discriminability by promoting deep information exchange between spatial-spectral and spatial-requency representations. Finally, adaptive decision-level fusion refines classification by consolidating multi-domain predictions. Extensive experiments on three public datasets demonstrate the superiority of PS2F2L, validating its effectiveness in achieving robust and accurate multimodal classification.&lt;/p&gt;</content:encoded></item><item><title>Monocular Multi-object 3D Visual Language Tracking</title><link>https://doi.org/10.1109/tip.2026.3661407</link><guid>10.1109/tip.2026.3661407</guid><pubDate>Tue, 10 Feb 2026 21:07:05 +0000</pubDate><dc:creator>Hongkai Wei</dc:creator><dc:creator>Rong Wang</dc:creator><dc:creator>Haixiang Hu</dc:creator><dc:creator>Shijie Sun</dc:creator><dc:creator>Xiangyu Song</dc:creator><dc:creator>Mingtao Feng</dc:creator><dc:creator>Keyu Guo</dc:creator><dc:creator>Yongle Huang</dc:creator><dc:creator>Hua Cui</dc:creator><dc:creator>Naveed Akhtar</dc:creator><prism:publicationName>IEEE Transactions on Image Processing</prism:publicationName><prism:doi>10.1109/tip.2026.3661407</prism:doi><description>Visual Language Tracking (VLT) enables machines to perform tracking in real world through human-like language descriptions. However, existing VLT methods are limited to 2D spatial tracking or single-object 3D tracking and do not support multi-object 3D tracking within monocular video. This limitation arises because advancements in 3D multi-object tracking have predominantly relied on sensor-based data (e.g., point clouds, depth sensors) that lacks corresponding language descriptions. Moreover, natural language descriptions in existing VLT literature often suffer from redundancy, impeding the efficient and precise localization of multiple objects. We present the first technique to extend VLT to multi-object 3D tracking using monocular video. We introduce a comprehensive framework that includes (i) a Monocular Multi-object 3D Visual Language Tracking (MoMo-3DVLT) task, (ii) a large-scale dataset, MoMo-3DRoVLT, tailored for this task, and (iii) a custom neural model. Our dataset, generated with the aid of Large Language Models (LLMs) and manual verification, contains 8,216 video sequences annotated with both 2D and 3D bounding boxes, with each sequence accompanied by three freely generated, human-level textual descriptions. We propose MoMo-3DVLTracker, the first neural model specifically designed for MoMo-3DVLT. This model integrates a multimodal feature extractor, a visual language encoder-decoder, and modules for detection and tracking, setting a strong baseline for MoMo-3DVLT. Beyond existing paradigms, it introduces a task-specific structural coupling that integrates a differentiable linked-memory mechanism with depth-guided and language-conditioned reasoning for robust monocular 3D multi-object tracking. Experimental results demonstrate that our approach outperforms existing methods on the MoMo-3DRoVLT dataset. Our dataset and code are available at Github.
Published: 2026-02-10T21:07:05+00:00
Venue: IEEE Transactions on Image Processing
Score: 0.799 (must_read)</description><content:encoded>&lt;p&gt;&lt;strong&gt;Authors:&lt;/strong&gt; Hongkai Wei; Rong Wang; Haixiang Hu; Shijie Sun; Xiangyu Song; Mingtao Feng; Keyu Guo; Yongle Huang; Hua Cui; Naveed Akhtar&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Journal:&lt;/strong&gt; IEEE Transactions on Image Processing&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;a href="https://doi.org/10.1109/tip.2026.3661407"&gt;10.1109/tip.2026.3661407&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Relevance:&lt;/strong&gt; 0.799 (must_read)&lt;/p&gt;
&lt;p&gt;Visual Language Tracking (VLT) enables machines to perform tracking in real world through human-like language descriptions. However, existing VLT methods are limited to 2D spatial tracking or single-object 3D tracking and do not support multi-object 3D tracking within monocular video. This limitation arises because advancements in 3D multi-object tracking have predominantly relied on sensor-based data (e.g., point clouds, depth sensors) that lacks corresponding language descriptions. Moreover, natural language descriptions in existing VLT literature often suffer from redundancy, impeding the efficient and precise localization of multiple objects. We present the first technique to extend VLT to multi-object 3D tracking using monocular video. We introduce a comprehensive framework that includes (i) a Monocular Multi-object 3D Visual Language Tracking (MoMo-3DVLT) task, (ii) a large-scale dataset, MoMo-3DRoVLT, tailored for this task, and (iii) a custom neural model. Our dataset, generated with the aid of Large Language Models (LLMs) and manual verification, contains 8,216 video sequences annotated with both 2D and 3D bounding boxes, with each sequence accompanied by three freely generated, human-level textual descriptions. We propose MoMo-3DVLTracker, the first neural model specifically designed for MoMo-3DVLT. This model integrates a multimodal feature extractor, a visual language encoder-decoder, and modules for detection and tracking, setting a strong baseline for MoMo-3DVLT. Beyond existing paradigms, it introduces a task-specific structural coupling that integrates a differentiable linked-memory mechanism with depth-guided and language-conditioned reasoning for robust monocular 3D multi-object tracking. Experimental results demonstrate that our approach outperforms existing methods on the MoMo-3DRoVLT dataset. Our dataset and code are available at Github.&lt;/p&gt;</content:encoded></item><item><title>FGAA-FPN: Foreground-Guided Angle-Aware Feature Pyramid Network for Oriented Object Detection</title><link>https://arxiv.org/abs/2602.10710v1</link><guid>http://arxiv.org/abs/2602.10710v1</guid><pubDate>Wed, 11 Feb 2026 10:15:06 +0000</pubDate><dc:creator>Jialin Ma</dc:creator><dc:type>preprint</dc:type><prism:publicationName>arXiv</prism:publicationName><description>With the increasing availability of high-resolution remote sensing and aerial imagery, oriented object detection has become a key capability for geographic information updating, maritime surveillance, and disaster response. However, it remains challenging due to cluttered backgrounds, severe scale variation, and large orientation changes. Existing approaches largely improve performance through multi-scale feature fusion with feature pyramid networks or contextual modeling with attention, but they often lack explicit foreground modeling and do not leverage geometric orientation priors, which limits feature discriminability. To overcome these limitations, we propose FGAA-FPN, a Foreground-Guided Angle-Aware Feature Pyramid Network for oriented object detection. FGAA-FPN is built on a hierarchical functional decomposition that accounts for the distinct spatial resolution and semantic abstraction across pyramid levels, thereby strengthening multi-scale representations. Concretely, a Foreground-Guided Feature Modulation module learns foreground saliency under weak supervision to enhance object regions and suppress background interference in low-level features. In parallel, an Angle-Aware Multi-Head Attention module encodes relative orientation relationships to guide global interactions among high-level semantic features. Extensive experiments on DOTA v1.0 and DOTA v1.5 demonstrate that FGAA-FPN achieves state-of-the-art results, reaching 75.5% and 68.3% mAP, respectively.
Published: 2026-02-11T10:15:06+00:00
Venue: arXiv
Score: 0.798 (must_read)</description><content:encoded>&lt;p&gt;&lt;strong&gt;Authors:&lt;/strong&gt; Jialin Ma&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Journal:&lt;/strong&gt; arXiv&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Relevance:&lt;/strong&gt; 0.798 (must_read)&lt;/p&gt;
&lt;p&gt;With the increasing availability of high-resolution remote sensing and aerial imagery, oriented object detection has become a key capability for geographic information updating, maritime surveillance, and disaster response. However, it remains challenging due to cluttered backgrounds, severe scale variation, and large orientation changes. Existing approaches largely improve performance through multi-scale feature fusion with feature pyramid networks or contextual modeling with attention, but they often lack explicit foreground modeling and do not leverage geometric orientation priors, which limits feature discriminability. To overcome these limitations, we propose FGAA-FPN, a Foreground-Guided Angle-Aware Feature Pyramid Network for oriented object detection. FGAA-FPN is built on a hierarchical functional decomposition that accounts for the distinct spatial resolution and semantic abstraction across pyramid levels, thereby strengthening multi-scale representations. Concretely, a Foreground-Guided Feature Modulation module learns foreground saliency under weak supervision to enhance object regions and suppress background interference in low-level features. In parallel, an Angle-Aware Multi-Head Attention module encodes relative orientation relationships to guide global interactions among high-level semantic features. Extensive experiments on DOTA v1.0 and DOTA v1.5 demonstrate that FGAA-FPN achieves state-of-the-art results, reaching 75.5% and 68.3% mAP, respectively.&lt;/p&gt;</content:encoded></item><item><title>CCMANet: A Cross-Layer Cascade Network with Multi-Attention Mechanisms for Remote Sensing Object Detection</title><link>https://doi.org/10.1109/jstars.2026.3663387</link><guid>10.1109/jstars.2026.3663387</guid><pubDate>Tue, 10 Feb 2026 21:05:21 +0000</pubDate><dc:creator>Jinlong Mei</dc:creator><dc:creator>Wentao Lyu</dc:creator><dc:creator>Qing Guo</dc:creator><dc:creator>Yuzhen Xu</dc:creator><dc:creator>Zhijiang Deng</dc:creator><prism:publicationName>IEEE Journal of Selected Topics in Applied Earth Observations and Remote Sensing</prism:publicationName><prism:doi>10.1109/jstars.2026.3663387</prism:doi><description>Remote sensing object detection plays a vital role in both civilian applications and national defense security. However, remote sensing images typically exhibit characteristics such as wide coverage, significant variations in object scales, dense object distribution, and severe background interference. These factors greatly limit the applicability of existing detection methods in remote sensing scenarios. To address these challenges, this paper proposes a cross-layer cascade network with multi-attention mechanisms for remote sensing object detection (CCMANet). The proposed network incorporates different types of attention mechanisms at different stages to tackle background interference and dense multi-target detection, and leverages cross-layer cascading to progressively optimize feature representations, thereby achieving higher detection accuracy. Specifically, a multi-attention collaborative module is first introduced for feature filtering and suppression of complex backgrounds, highlighting useful remote sensing object features. Then, a maximum feature fusion module is employed in the feature fusion stage to enhance the diversity and representational capacity of the fused features. Finally, an improved dual-spatial pyramid pooling module combines two distinct spatial feature representations to further enrich target features in remote sensing images, ensuring that the dense and diverse remote sensing object information is preserved throughout the detection pipeline. Experiments on the DIOR, NWPU VHR-10, and RSOD datasets validate the effectiveness of the proposed method, achieving the highest mAP of 0.773, 0.952, and 0.973, respectively. Our code is available at https://github.com/meijinlong/CCMANet.
Published: 2026-02-10T21:05:21+00:00
Venue: IEEE Journal of Selected Topics in Applied Earth Observations and Remote Sensing
Score: 0.797 (must_read)</description><content:encoded>&lt;p&gt;&lt;strong&gt;Authors:&lt;/strong&gt; Jinlong Mei; Wentao Lyu; Qing Guo; Yuzhen Xu; Zhijiang Deng&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Journal:&lt;/strong&gt; IEEE Journal of Selected Topics in Applied Earth Observations and Remote Sensing&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;a href="https://doi.org/10.1109/jstars.2026.3663387"&gt;10.1109/jstars.2026.3663387&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Relevance:&lt;/strong&gt; 0.797 (must_read)&lt;/p&gt;
&lt;p&gt;Remote sensing object detection plays a vital role in both civilian applications and national defense security. However, remote sensing images typically exhibit characteristics such as wide coverage, significant variations in object scales, dense object distribution, and severe background interference. These factors greatly limit the applicability of existing detection methods in remote sensing scenarios. To address these challenges, this paper proposes a cross-layer cascade network with multi-attention mechanisms for remote sensing object detection (CCMANet). The proposed network incorporates different types of attention mechanisms at different stages to tackle background interference and dense multi-target detection, and leverages cross-layer cascading to progressively optimize feature representations, thereby achieving higher detection accuracy. Specifically, a multi-attention collaborative module is first introduced for feature filtering and suppression of complex backgrounds, highlighting useful remote sensing object features. Then, a maximum feature fusion module is employed in the feature fusion stage to enhance the diversity and representational capacity of the fused features. Finally, an improved dual-spatial pyramid pooling module combines two distinct spatial feature representations to further enrich target features in remote sensing images, ensuring that the dense and diverse remote sensing object information is preserved throughout the detection pipeline. Experiments on the DIOR, NWPU VHR-10, and RSOD datasets validate the effectiveness of the proposed method, achieving the highest mAP of 0.773, 0.952, and 0.973, respectively. Our code is available at https://github.com/meijinlong/CCMANet.&lt;/p&gt;</content:encoded></item><item><title>You Only Train Once: A Unified Framework for Both Full-Reference and No-Reference Image Quality Assessment</title><link>https://doi.org/10.1109/tip.2026.3661408</link><guid>10.1109/tip.2026.3661408</guid><pubDate>Tue, 10 Feb 2026 21:07:05 +0000</pubDate><dc:creator>Yi Ke Yun</dc:creator><dc:creator>Weisi Lin</dc:creator><prism:publicationName>IEEE Transactions on Image Processing</prism:publicationName><prism:doi>10.1109/tip.2026.3661408</prism:doi><description>Existing Image Quality Assessment (IQA) models are limited to either full reference or no reference evaluation tasks, while humans can seamlessly switch between these assessment types. This motivates us to explore resolving these two tasks using a versatile model. In this work, we propose a novel framework that unifies full reference and no reference IQA. Our approach utilizes an encoder to extract multi-level features from images and introduces a Hierarchical Attention module to adaptively handle spatial distortions for both full reference and no reference inputs. Additionally, we develop a Semantic Distortion Aware module to analyze feature correlations between shallow and deep layers of the encoder, thereby accounting for the varying effects of different distortions on these layers. Our proposed framework achieves state-of-the-art performance for both full-reference and no-reference IQA tasks when trained separately. Furthermore, when the model is trained jointly on both types of tasks, it not only enhances performance in no-reference IQA but also maintains competitive results in full-reference IQA. This integrated approach facilitates a single training process that efficiently addresses both IQA tasks, representing a significant advancement in model versatility and performance.
Published: 2026-02-10T21:07:05+00:00
Venue: IEEE Transactions on Image Processing
Score: 0.796 (must_read)</description><content:encoded>&lt;p&gt;&lt;strong&gt;Authors:&lt;/strong&gt; Yi Ke Yun; Weisi Lin&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Journal:&lt;/strong&gt; IEEE Transactions on Image Processing&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;a href="https://doi.org/10.1109/tip.2026.3661408"&gt;10.1109/tip.2026.3661408&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Relevance:&lt;/strong&gt; 0.796 (must_read)&lt;/p&gt;
&lt;p&gt;Existing Image Quality Assessment (IQA) models are limited to either full reference or no reference evaluation tasks, while humans can seamlessly switch between these assessment types. This motivates us to explore resolving these two tasks using a versatile model. In this work, we propose a novel framework that unifies full reference and no reference IQA. Our approach utilizes an encoder to extract multi-level features from images and introduces a Hierarchical Attention module to adaptively handle spatial distortions for both full reference and no reference inputs. Additionally, we develop a Semantic Distortion Aware module to analyze feature correlations between shallow and deep layers of the encoder, thereby accounting for the varying effects of different distortions on these layers. Our proposed framework achieves state-of-the-art performance for both full-reference and no-reference IQA tasks when trained separately. Furthermore, when the model is trained jointly on both types of tasks, it not only enhances performance in no-reference IQA but also maintains competitive results in full-reference IQA. This integrated approach facilitates a single training process that efficiently addresses both IQA tasks, representing a significant advancement in model versatility and performance.&lt;/p&gt;</content:encoded></item><item><title>非空间配准的多模态目标检测决策融合策略</title><link>https://doi.org/10.11834/jig.250326</link><guid>10.11834/jig.250326</guid><pubDate>Wed, 11 Feb 2026 07:28:27 +0000</pubDate><dc:creator>Zhang Rong</dc:creator><dc:creator>Yao Liang</dc:creator><dc:creator>Zhang Yixin</dc:creator><dc:creator>Wang Yijun</dc:creator><dc:creator>Zhang Chuanyi</dc:creator><dc:creator>Liu Fan</dc:creator><prism:publicationName>Journal of Image and Graphics</prism:publicationName><prism:doi>10.11834/jig.250326</prism:doi><description>目的多模态目标检测通过融合红外与可见光等多源传感器数据，有效提升了模型在复杂环境下的检测精度与鲁棒性。然而，现有方法普遍基于严格配准的多模态数据开展研究，不能直接适配实际应用中不同模态相机得到的非空间配准图像，在图像输入算法前仍需完成配准，这损失了实时性和灵活性。为此，提出一种非空间配准条件下的多模态目标检测任务，并设计了一种非空间配准的多模态目标检测决策融合方法。在数据层面，利用3种公共数据集模拟双光载荷拍摄得到的非空间配准图像对，在不引入额外标注成本的前提下，为新的任务提供了基准数据。在算法层面，设计了一种基于图结构的非空间配准决策融合方法。方法首先，根据不同模态检测器的检测结果构建带权有向图，实现不同模态目标的图结构化表示；接着，利用图结构中目标间的相对位置关系，实现跨模态目标的自适应匹配；最终，对匹配成功的目标进行决策融合，并设计了模态迁移策略以实现多模态信息的高效互补。结果在3个数据集上的实验结果表明，本文方法在非空间配准场景下较单模态检测器实现了最大10.03%的漏检率降幅。同时，该方法在配准数据集上同样适用，相较于多光谱行人检测Transformer（multi spectral pedestrian detection Transformer，MS-DETR）、动态自适应多光谱检测Transformer（dynamic adaptive multispectral detection Transformer，DAMSDet）等先进多模态目标检测方法，检测准确率提升了6.8%。结论本文所提出的非空间配准多模态目标检测决策融合方法，能够很好地适应存在空间差异的实际场景，并且相比其他先进的多模态目标检测模型，有更高的准确率和鲁棒性。相关的代码与数据集将在此仓库公开：https://github.com/1e12Leon/ProbDet。
Published: 2026-02-11T07:28:27+00:00
Venue: Journal of Image and Graphics
Score: 0.795 (must_read)</description><content:encoded>&lt;p&gt;&lt;strong&gt;Authors:&lt;/strong&gt; Zhang Rong; Yao Liang; Zhang Yixin; Wang Yijun; Zhang Chuanyi; Liu Fan&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Journal:&lt;/strong&gt; Journal of Image and Graphics&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;a href="https://doi.org/10.11834/jig.250326"&gt;10.11834/jig.250326&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Relevance:&lt;/strong&gt; 0.795 (must_read)&lt;/p&gt;
&lt;p&gt;目的多模态目标检测通过融合红外与可见光等多源传感器数据，有效提升了模型在复杂环境下的检测精度与鲁棒性。然而，现有方法普遍基于严格配准的多模态数据开展研究，不能直接适配实际应用中不同模态相机得到的非空间配准图像，在图像输入算法前仍需完成配准，这损失了实时性和灵活性。为此，提出一种非空间配准条件下的多模态目标检测任务，并设计了一种非空间配准的多模态目标检测决策融合方法。在数据层面，利用3种公共数据集模拟双光载荷拍摄得到的非空间配准图像对，在不引入额外标注成本的前提下，为新的任务提供了基准数据。在算法层面，设计了一种基于图结构的非空间配准决策融合方法。方法首先，根据不同模态检测器的检测结果构建带权有向图，实现不同模态目标的图结构化表示；接着，利用图结构中目标间的相对位置关系，实现跨模态目标的自适应匹配；最终，对匹配成功的目标进行决策融合，并设计了模态迁移策略以实现多模态信息的高效互补。结果在3个数据集上的实验结果表明，本文方法在非空间配准场景下较单模态检测器实现了最大10.03%的漏检率降幅。同时，该方法在配准数据集上同样适用，相较于多光谱行人检测Transformer（multi spectral pedestrian detection Transformer，MS-DETR）、动态自适应多光谱检测Transformer（dynamic adaptive multispectral detection Transformer，DAMSDet）等先进多模态目标检测方法，检测准确率提升了6.8%。结论本文所提出的非空间配准多模态目标检测决策融合方法，能够很好地适应存在空间差异的实际场景，并且相比其他先进的多模态目标检测模型，有更高的准确率和鲁棒性。相关的代码与数据集将在此仓库公开：https://github.com/1e12Leon/ProbDet。&lt;/p&gt;</content:encoded></item><item><title>AurigaNet: A Real-Time Multi-Task Network for Enhanced Urban Driving Perception</title><link>https://arxiv.org/abs/2602.10660v1</link><guid>http://arxiv.org/abs/2602.10660v1</guid><pubDate>Wed, 11 Feb 2026 09:04:29 +0000</pubDate><dc:creator>Kiarash Ghasemzadeh</dc:creator><dc:creator>Sedigheh Dehghani</dc:creator><dc:type>preprint</dc:type><prism:publicationName>arXiv</prism:publicationName><description>Self-driving cars hold significant potential to reduce traffic accidents, alleviate congestion, and enhance urban mobility. However, developing reliable AI systems for autonomous vehicles remains a substantial challenge. Over the past decade, multi-task learning has emerged as a powerful approach to address complex problems in driving perception. Multi-task networks offer several advantages, including increased computational efficiency, real-time processing capabilities, optimized resource utilization, and improved generalization. In this study, we present AurigaNet, an advanced multi-task network architecture designed to push the boundaries of autonomous driving perception. AurigaNet integrates three critical tasks: object detection, lane detection, and drivable area instance segmentation. The system is trained and evaluated using the BDD100K dataset, renowned for its diversity in driving conditions. Key innovations of AurigaNet include its end-to-end instance segmentation capability, which significantly enhances both accuracy and efficiency in path estimation for autonomous vehicles. Experimental results demonstrate that AurigaNet achieves an 85.2% IoU in drivable area segmentation, outperforming its closest competitor by 0.7%. In lane detection, AurigaNet achieves a remarkable 60.8% IoU, surpassing other models by more than 30%. Furthermore, the network achieves an mAP@0.5:0.95 of 47.6% in traffic object detection, exceeding the next leading model by 2.9%. Additionally, we validate the practical feasibility of AurigaNet by deploying it on embedded devices such as the Jetson Orin NX, where it demonstrates competitive real-time performance. These results underscore AurigaNet's potential as a robust and efficient solution for autonomous driving perception systems. The code can be found here https://github.com/KiaRational/AurigaNet.
Published: 2026-02-11T09:04:29+00:00
Venue: arXiv
Score: 0.795 (must_read)</description><content:encoded>&lt;p&gt;&lt;strong&gt;Authors:&lt;/strong&gt; Kiarash Ghasemzadeh; Sedigheh Dehghani&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Journal:&lt;/strong&gt; arXiv&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Relevance:&lt;/strong&gt; 0.795 (must_read)&lt;/p&gt;
&lt;p&gt;Self-driving cars hold significant potential to reduce traffic accidents, alleviate congestion, and enhance urban mobility. However, developing reliable AI systems for autonomous vehicles remains a substantial challenge. Over the past decade, multi-task learning has emerged as a powerful approach to address complex problems in driving perception. Multi-task networks offer several advantages, including increased computational efficiency, real-time processing capabilities, optimized resource utilization, and improved generalization. In this study, we present AurigaNet, an advanced multi-task network architecture designed to push the boundaries of autonomous driving perception. AurigaNet integrates three critical tasks: object detection, lane detection, and drivable area instance segmentation. The system is trained and evaluated using the BDD100K dataset, renowned for its diversity in driving conditions. Key innovations of AurigaNet include its end-to-end instance segmentation capability, which significantly enhances both accuracy and efficiency in path estimation for autonomous vehicles. Experimental results demonstrate that AurigaNet achieves an 85.2% IoU in drivable area segmentation, outperforming its closest competitor by 0.7%. In lane detection, AurigaNet achieves a remarkable 60.8% IoU, surpassing other models by more than 30%. Furthermore, the network achieves an mAP@0.5:0.95 of 47.6% in traffic object detection, exceeding the next leading model by 2.9%. Additionally, we validate the practical feasibility of AurigaNet by deploying it on embedded devices such as the Jetson Orin NX, where it demonstrates competitive real-time performance. These results underscore AurigaNet&amp;#x27;s potential as a robust and efficient solution for autonomous driving perception systems. The code can be found here https://github.com/KiaRational/AurigaNet.&lt;/p&gt;</content:encoded></item><item><title>LACT-Fusion: Linear Attention-Guided Cross-Modal Learning for Infrared and Visible Image Fusion</title><link>https://doi.org/10.1016/j.knosys.2026.115531</link><guid>10.1016/j.knosys.2026.115531</guid><pubDate>Tue, 10 Feb 2026 00:57:40 +0000</pubDate><dc:creator>Zhao Cai</dc:creator><dc:creator>Yong Ma</dc:creator><dc:creator>Qi Peng</dc:creator><dc:creator>Weizhong Li</dc:creator><dc:creator>Ge Wang</dc:creator><dc:creator>Jun Huang</dc:creator><dc:creator>Fan Fan</dc:creator><dc:creator>Qian Wang</dc:creator><prism:publicationName>Knowledge-Based Systems</prism:publicationName><prism:doi>10.1016/j.knosys.2026.115531</prism:doi><description>Infrared and visible image fusion aims to extract intrinsic features from both modalities and generate high-quality images that preserve complementary information. Despite the success of Transformer-based image fusion methods in modeling global dependencies, they inherently lack local inductive biases, often resulting in the loss of fine-grained details in the fused images. Moreover, adaptive interaction across modalities remains suboptimal, limiting the preservation of modality-specific information during fusion. To address these challenges, we propose LACT-Fusion, a novel fusion framework based on Transformer. Specifically, a linear attention module with an auxiliary matrix is developed to replace the conventional self-attention mechanism, effectively reducing computational complexity while improving the adaptive modeling of complementary features from different modalities. In addition, a Local Attention-based Multi-scale Feature Enhancement Block (LFEB) is designed to strengthen texture and structural representation, enhancing the clarity and fidelity of the fused images. Extensive experiments on multiple public datasets demonstrate that LACT-Fusion consistently outperforms state-of-the-art methods in both quantitative metrics and qualitative evaluations, confirming its superior fusion performance and strong potential for practical applications. The sources code will be published in https://github.com/zc617/LACTFusion .
Published: 2026-02-10T00:57:40+00:00
Venue: Knowledge-Based Systems
Score: 0.795 (must_read)</description><content:encoded>&lt;p&gt;&lt;strong&gt;Authors:&lt;/strong&gt; Zhao Cai; Yong Ma; Qi Peng; Weizhong Li; Ge Wang; Jun Huang; Fan Fan; Qian Wang&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Journal:&lt;/strong&gt; Knowledge-Based Systems&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;a href="https://doi.org/10.1016/j.knosys.2026.115531"&gt;10.1016/j.knosys.2026.115531&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Relevance:&lt;/strong&gt; 0.795 (must_read)&lt;/p&gt;
&lt;p&gt;Infrared and visible image fusion aims to extract intrinsic features from both modalities and generate high-quality images that preserve complementary information. Despite the success of Transformer-based image fusion methods in modeling global dependencies, they inherently lack local inductive biases, often resulting in the loss of fine-grained details in the fused images. Moreover, adaptive interaction across modalities remains suboptimal, limiting the preservation of modality-specific information during fusion. To address these challenges, we propose LACT-Fusion, a novel fusion framework based on Transformer. Specifically, a linear attention module with an auxiliary matrix is developed to replace the conventional self-attention mechanism, effectively reducing computational complexity while improving the adaptive modeling of complementary features from different modalities. In addition, a Local Attention-based Multi-scale Feature Enhancement Block (LFEB) is designed to strengthen texture and structural representation, enhancing the clarity and fidelity of the fused images. Extensive experiments on multiple public datasets demonstrate that LACT-Fusion consistently outperforms state-of-the-art methods in both quantitative metrics and qualitative evaluations, confirming its superior fusion performance and strong potential for practical applications. The sources code will be published in https://github.com/zc617/LACTFusion .&lt;/p&gt;</content:encoded></item><item><title>MambaFlow: A Novel and Flow-Guided State Space Model for Scene Flow Estimation</title><link>https://doi.org/10.1109/tiv.2026.3663171</link><guid>10.1109/tiv.2026.3663171</guid><pubDate>Tue, 10 Feb 2026 21:06:37 +0000</pubDate><dc:creator>Jiehao Luo</dc:creator><dc:creator>Jintao Cheng</dc:creator><dc:creator>Qingwen Zhang</dc:creator><dc:creator>Bohuan Xue</dc:creator><dc:creator>Rui Fan</dc:creator><dc:creator>Xiaoyu Tang</dc:creator><prism:publicationName>IEEE Transactions on Intelligent Vehicles</prism:publicationName><prism:doi>10.1109/tiv.2026.3663171</prism:doi><description>Scene flow estimation aims to predict 3D motion from consecutive point cloud frames, which is of great interest in autonomous driving field. Existing methods face challenges such as insufficient spatio-temporal modeling and inherent loss of fine-grained feature during voxelization. However, the success of Mamba, a representative state space model (SSM) that enables global modeling with linear complexity, provides a promising solution. In this paper, we propose MambaFlow, a novel scene flow estimation network with a mamba-based decoder. It enables deep interaction and coupling of spatio-temporal features using a well-designed backbone. Innovatively, we steer the global attention modeling of voxel-based features with point offset information using an efficient Mamba-based decoder, learning voxel-to-point patterns that are used to devoxelize shared voxel representations into point-wise features. To further enhance the model's generalization capabilities across diverse scenarios, we propose a novel scene-adaptive loss function that automatically adapts to different motion patterns. Extensive experiments on the Argoverse 2 benchmark demonstrate that MambaFlow achieves state-of-the-art performance with real-time inference speed among existing works, enabling accurate flow estimation in real-world urban scenarios. The code is available at https://github.com/SCNU-RISLAB/MambaFlow.
Published: 2026-02-10T21:06:37+00:00
Venue: IEEE Transactions on Intelligent Vehicles
Score: 0.795 (must_read)</description><content:encoded>&lt;p&gt;&lt;strong&gt;Authors:&lt;/strong&gt; Jiehao Luo; Jintao Cheng; Qingwen Zhang; Bohuan Xue; Rui Fan; Xiaoyu Tang&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Journal:&lt;/strong&gt; IEEE Transactions on Intelligent Vehicles&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;a href="https://doi.org/10.1109/tiv.2026.3663171"&gt;10.1109/tiv.2026.3663171&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Relevance:&lt;/strong&gt; 0.795 (must_read)&lt;/p&gt;
&lt;p&gt;Scene flow estimation aims to predict 3D motion from consecutive point cloud frames, which is of great interest in autonomous driving field. Existing methods face challenges such as insufficient spatio-temporal modeling and inherent loss of fine-grained feature during voxelization. However, the success of Mamba, a representative state space model (SSM) that enables global modeling with linear complexity, provides a promising solution. In this paper, we propose MambaFlow, a novel scene flow estimation network with a mamba-based decoder. It enables deep interaction and coupling of spatio-temporal features using a well-designed backbone. Innovatively, we steer the global attention modeling of voxel-based features with point offset information using an efficient Mamba-based decoder, learning voxel-to-point patterns that are used to devoxelize shared voxel representations into point-wise features. To further enhance the model&amp;#x27;s generalization capabilities across diverse scenarios, we propose a novel scene-adaptive loss function that automatically adapts to different motion patterns. Extensive experiments on the Argoverse 2 benchmark demonstrate that MambaFlow achieves state-of-the-art performance with real-time inference speed among existing works, enabling accurate flow estimation in real-world urban scenarios. The code is available at https://github.com/SCNU-RISLAB/MambaFlow.&lt;/p&gt;</content:encoded></item><item><title>When large language models are reliable for judging empathic communication</title><link>https://doi.org/10.1038/s42256-025-01169-6</link><guid>10.1038/s42256-025-01169-6</guid><pubDate>Wed, 11 Feb 2026 10:02:22 +0000</pubDate><dc:creator>Aakriti Kumar</dc:creator><dc:creator>Nalin Poungpeth</dc:creator><dc:creator>Diyi Yang</dc:creator><dc:creator>Erina Farrell</dc:creator><dc:creator>Bruce L. Lambert</dc:creator><dc:creator>Matthew Groh</dc:creator><prism:publicationName>Nature Machine Intelligence</prism:publicationName><prism:doi>10.1038/s42256-025-01169-6</prism:doi><description>Abstract Large language models (LLMs) excel at generating empathic responses in text-based conversations. But, how reliably do they judge the nuances of empathic communication? Here we investigate this question by comparing how experts, crowdworkers and LLMs annotate empathic communication across four evaluative frameworks drawn from psychology, natural language processing and communications applied to 200 real-world conversations where one speaker shares a personal problem and the other offers support. Drawing on 3,150 expert annotations, 2,844 crowd annotations and 3,150 LLM annotations, we assess interrater reliability between these three annotator groups. We find that expert agreement is high but varies across the frameworks’ subcomponents depending on their clarity, complexity and subjectivity. We show that expert agreement offers a more informative benchmark for contextualizing LLM performance than standard classification metrics. Across all four frameworks, LLMs consistently approach this expert level benchmark and exceed the reliability of crowdworkers. These results demonstrate how LLMs, when validated on specific tasks with appropriate benchmarks, can support transparency and oversight in emotionally sensitive applications including their use as conversational companions.
Published: 2026-02-11T10:02:22+00:00
Venue: Nature Machine Intelligence
Score: 0.794 (must_read)</description><content:encoded>&lt;p&gt;&lt;strong&gt;Authors:&lt;/strong&gt; Aakriti Kumar; Nalin Poungpeth; Diyi Yang; Erina Farrell; Bruce L. Lambert; Matthew Groh&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Journal:&lt;/strong&gt; Nature Machine Intelligence&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;a href="https://doi.org/10.1038/s42256-025-01169-6"&gt;10.1038/s42256-025-01169-6&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Relevance:&lt;/strong&gt; 0.794 (must_read)&lt;/p&gt;
&lt;p&gt;Abstract Large language models (LLMs) excel at generating empathic responses in text-based conversations. But, how reliably do they judge the nuances of empathic communication? Here we investigate this question by comparing how experts, crowdworkers and LLMs annotate empathic communication across four evaluative frameworks drawn from psychology, natural language processing and communications applied to 200 real-world conversations where one speaker shares a personal problem and the other offers support. Drawing on 3,150 expert annotations, 2,844 crowd annotations and 3,150 LLM annotations, we assess interrater reliability between these three annotator groups. We find that expert agreement is high but varies across the frameworks’ subcomponents depending on their clarity, complexity and subjectivity. We show that expert agreement offers a more informative benchmark for contextualizing LLM performance than standard classification metrics. Across all four frameworks, LLMs consistently approach this expert level benchmark and exceed the reliability of crowdworkers. These results demonstrate how LLMs, when validated on specific tasks with appropriate benchmarks, can support transparency and oversight in emotionally sensitive applications including their use as conversational companions.&lt;/p&gt;</content:encoded></item><item><title>GL-DT: Multi-UAV Detection and Tracking with Global-Local Integration</title><link>https://doi.org/10.1109/tgrs.2026.3663235</link><guid>10.1109/tgrs.2026.3663235</guid><pubDate>Tue, 10 Feb 2026 21:05:00 +0000</pubDate><dc:creator>Juanqin Liu</dc:creator><dc:creator>Leonardo Plotegher</dc:creator><dc:creator>Eloy Roura</dc:creator><dc:creator>Shaoming He</dc:creator><prism:publicationName>IEEE Transactions on Geoscience and Remote Sensing</prism:publicationName><prism:doi>10.1109/tgrs.2026.3663235</prism:doi><description>The extensive application of unmanned aerial vehicles (UAVs) in military reconnaissance, environmental monitoring, and related domains has created an urgent need for accurate and efficient multi-object tracking (MOT) technologies, which are also essential for UAV situational awareness. However, complex backgrounds, small-scale targets, and frequent occlusions and interactions continue to challenge existing methods in terms of detection accuracy and trajectory continuity. To address these issues, this paper proposes the Global-Local Detection and Tracking (GL-DT) framework. It employs a Spatio-Temporal Feature Fusion (STFF) module to jointly model motion and appearance features, combined with a global-local collaborative detection strategy, effectively enhancing small-target detection. Building upon this, the JPTrack tracking algorithm is introduced to mitigate common issues such as ID switches and trajectory fragmentation. Experimental results demonstrate that the proposed approach significantly improves the continuity and stability of MOT while maintaining real-time performance, providing strong support for the advancement of UAV detection and tracking technologies.
Published: 2026-02-10T21:05:00+00:00
Venue: IEEE Transactions on Geoscience and Remote Sensing
Score: 0.793 (must_read)</description><content:encoded>&lt;p&gt;&lt;strong&gt;Authors:&lt;/strong&gt; Juanqin Liu; Leonardo Plotegher; Eloy Roura; Shaoming He&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Journal:&lt;/strong&gt; IEEE Transactions on Geoscience and Remote Sensing&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;a href="https://doi.org/10.1109/tgrs.2026.3663235"&gt;10.1109/tgrs.2026.3663235&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Relevance:&lt;/strong&gt; 0.793 (must_read)&lt;/p&gt;
&lt;p&gt;The extensive application of unmanned aerial vehicles (UAVs) in military reconnaissance, environmental monitoring, and related domains has created an urgent need for accurate and efficient multi-object tracking (MOT) technologies, which are also essential for UAV situational awareness. However, complex backgrounds, small-scale targets, and frequent occlusions and interactions continue to challenge existing methods in terms of detection accuracy and trajectory continuity. To address these issues, this paper proposes the Global-Local Detection and Tracking (GL-DT) framework. It employs a Spatio-Temporal Feature Fusion (STFF) module to jointly model motion and appearance features, combined with a global-local collaborative detection strategy, effectively enhancing small-target detection. Building upon this, the JPTrack tracking algorithm is introduced to mitigate common issues such as ID switches and trajectory fragmentation. Experimental results demonstrate that the proposed approach significantly improves the continuity and stability of MOT while maintaining real-time performance, providing strong support for the advancement of UAV detection and tracking technologies.&lt;/p&gt;</content:encoded></item><item><title>激光雷达智能处理关键技术研究进展</title><link>https://doi.org/10.11834/jig.250664</link><guid>10.11834/jig.250664</guid><pubDate>Wed, 11 Feb 2026 07:27:51 +0000</pubDate><dc:creator>Ao Sheng</dc:creator><dc:creator>Wen Chenglu</dc:creator><dc:creator>Li Wen</dc:creator><dc:creator>Liu Dunqiang</dc:creator><dc:creator>Xing Leyuan</dc:creator><dc:creator>Li Mingzhe</dc:creator><dc:creator>Guo Yulan</dc:creator><dc:creator>Wang Cheng</dc:creator><prism:publicationName>Journal of Image and Graphics</prism:publicationName><prism:doi>10.11834/jig.250664</prism:doi><description>激光雷达作为三维环境感知的核心传感器，在自动驾驶、机器人、增强现实等领域发挥着不可替代的作用。随着人工智能技术的快速发展，激光雷达智能处理技术已成为研究热点。本文围绕三维目标检测、激光雷达定位、人体动作捕捉与语言推理四大关键任务，对国内外研究进展进行了系统梳理与深入分析。首先，本文总结了该领域的核心任务定义与关键挑战。其次，本文结合任务特性，对相关技术进行了系统分类与方法解析，深入比较各类方法在不同场景下的适用性与性能优势。本文提及的算法、数据集和评估指标已汇总至https：//github.com/aosheng1996/DL4LiDAR。接下来，本文对国内外研究进展进行了对比分析，指出国外研究在模型体系与数据构建方面基础坚实，国内研究在算法效率与工程化落地方面发展迅速。最后，本文从算法融合、任务扩展与系统优化三个层面展望了激光雷达智能处理的未来发展趋势，以期为学术界与工业界提供理论参考，推动激光雷达智能处理技术的进一步发展。
Published: 2026-02-11T07:27:51+00:00
Venue: Journal of Image and Graphics
Score: 0.793 (must_read)</description><content:encoded>&lt;p&gt;&lt;strong&gt;Authors:&lt;/strong&gt; Ao Sheng; Wen Chenglu; Li Wen; Liu Dunqiang; Xing Leyuan; Li Mingzhe; Guo Yulan; Wang Cheng&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Journal:&lt;/strong&gt; Journal of Image and Graphics&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;a href="https://doi.org/10.11834/jig.250664"&gt;10.11834/jig.250664&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Relevance:&lt;/strong&gt; 0.793 (must_read)&lt;/p&gt;
&lt;p&gt;激光雷达作为三维环境感知的核心传感器，在自动驾驶、机器人、增强现实等领域发挥着不可替代的作用。随着人工智能技术的快速发展，激光雷达智能处理技术已成为研究热点。本文围绕三维目标检测、激光雷达定位、人体动作捕捉与语言推理四大关键任务，对国内外研究进展进行了系统梳理与深入分析。首先，本文总结了该领域的核心任务定义与关键挑战。其次，本文结合任务特性，对相关技术进行了系统分类与方法解析，深入比较各类方法在不同场景下的适用性与性能优势。本文提及的算法、数据集和评估指标已汇总至https：//github.com/aosheng1996/DL4LiDAR。接下来，本文对国内外研究进展进行了对比分析，指出国外研究在模型体系与数据构建方面基础坚实，国内研究在算法效率与工程化落地方面发展迅速。最后，本文从算法融合、任务扩展与系统优化三个层面展望了激光雷达智能处理的未来发展趋势，以期为学术界与工业界提供理论参考，推动激光雷达智能处理技术的进一步发展。&lt;/p&gt;</content:encoded></item><item><title>Distortion-Aware Depth Self-Updating for Self-Supervised Fisheye Monocular Depth Estimation</title><link>https://doi.org/10.1109/tip.2026.3661813</link><guid>10.1109/tip.2026.3661813</guid><pubDate>Wed, 11 Feb 2026 20:57:46 +0000</pubDate><dc:creator>Yihang Xu</dc:creator><dc:creator>Qiulei Dong</dc:creator><prism:publicationName>IEEE Transactions on Image Processing</prism:publicationName><prism:doi>10.1109/tip.2026.3661813</prism:doi><description>Self-supervised monocular depth estimation for fisheye cameras has attracted much attention in recent years due to their large view range. However, the performances of existing methods in this field are generally limited due to the inevitable severe distortions in fisheye images. To address this problem, we propose a distortion-aware depth self-updating network for self-supervised fisheye monocular depth estimation called DDS-Net. The proposed DDS-Net method employs a coarse-to-fine learning strategy, in which an explored fine depth predictor for predicting final depth is optimized with the predicted scene depths by a pretrained coarse depth predictor. The fine depth predictor contains a distortion-aware fisheye cost volume construction module and a depth self-updating module. The distortion-aware fisheye cost volume construction module is designed to construct a fisheye cost volume by learning the corresponding feature matching cost between continuous fisheye frames, which enables more accurate pixel-level depth cues to be captured under severe distortions. Based on the constructed cost volume and the initial depth estimated by the pretrained coarse depth predictor, the depth self-updating module is designed to self-update the depth map in an iterative manner. Extensive experimental results on 3 fisheye datasets demonstrate that the proposed method significantly outperforms 14 state-of-the-art methods for fisheye monocular depth estimation.
Published: 2026-02-11T20:57:46+00:00
Venue: IEEE Transactions on Image Processing
Score: 0.792 (must_read)</description><content:encoded>&lt;p&gt;&lt;strong&gt;Authors:&lt;/strong&gt; Yihang Xu; Qiulei Dong&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Journal:&lt;/strong&gt; IEEE Transactions on Image Processing&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;a href="https://doi.org/10.1109/tip.2026.3661813"&gt;10.1109/tip.2026.3661813&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Relevance:&lt;/strong&gt; 0.792 (must_read)&lt;/p&gt;
&lt;p&gt;Self-supervised monocular depth estimation for fisheye cameras has attracted much attention in recent years due to their large view range. However, the performances of existing methods in this field are generally limited due to the inevitable severe distortions in fisheye images. To address this problem, we propose a distortion-aware depth self-updating network for self-supervised fisheye monocular depth estimation called DDS-Net. The proposed DDS-Net method employs a coarse-to-fine learning strategy, in which an explored fine depth predictor for predicting final depth is optimized with the predicted scene depths by a pretrained coarse depth predictor. The fine depth predictor contains a distortion-aware fisheye cost volume construction module and a depth self-updating module. The distortion-aware fisheye cost volume construction module is designed to construct a fisheye cost volume by learning the corresponding feature matching cost between continuous fisheye frames, which enables more accurate pixel-level depth cues to be captured under severe distortions. Based on the constructed cost volume and the initial depth estimated by the pretrained coarse depth predictor, the depth self-updating module is designed to self-update the depth map in an iterative manner. Extensive experimental results on 3 fisheye datasets demonstrate that the proposed method significantly outperforms 14 state-of-the-art methods for fisheye monocular depth estimation.&lt;/p&gt;</content:encoded></item><item><title>Efficient-SAM2: Accelerating SAM2 with Object-Aware Visual Encoding and Memory Retrieval</title><link>https://arxiv.org/abs/2602.08224v2</link><guid>http://arxiv.org/abs/2602.08224v2</guid><pubDate>Mon, 09 Feb 2026 02:58:33 +0000</pubDate><dc:creator>Jing Zhang</dc:creator><dc:creator>Zhikai Li</dc:creator><dc:creator>Xuewen Liu</dc:creator><dc:creator>Qingyi Gu</dc:creator><dc:type>preprint</dc:type><prism:publicationName>arXiv</prism:publicationName><description>Segment Anything Model 2 (SAM2) shows excellent performance in video object segmentation tasks; however, the heavy computational burden hinders its application in real-time video processing. Although there have been efforts to improve the efficiency of SAM2, most of them focus on retraining a lightweight backbone, with little exploration into post-training acceleration. In this paper, we observe that SAM2 exhibits sparse perception pattern as biological vision, which provides opportunities for eliminating redundant computation and acceleration: i) In mask decoder, the attention primarily focuses on the foreground objects, whereas the image encoder in the earlier stage exhibits a broad attention span, which results in unnecessary computation to background regions. ii) In memory bank, only a small subset of tokens in each frame contribute significantly to memory attention, and the salient regions exhibit temporal consistency, making full-token computation redundant. With these insights, we propose Efficient-SAM2, which promotes SAM2 to adaptively focus on object regions while eliminating task-irrelevant computations, thereby significantly improving inference efficiency. Specifically, for image encoder, we propose object-aware Sparse Window Routing (SWR), a window-level computation allocation mechanism that leverages the consistency and saliency cues from the previous-frame decoder to route background regions into a lightweight shortcut branch. Moreover, for memory attention, we propose object-aware Sparse Memory Retrieval (SMR), which allows only the salient memory tokens in each frame to participate in computation, with the saliency pattern reused from their first recollection. With negligible additional parameters and minimal training overhead, Efficient-SAM2 delivers 1.68x speedup on SAM2.1-L model with only 1.0% accuracy drop on SA-V test set.
Published: 2026-02-09T02:58:33+00:00
Venue: arXiv
Score: 0.792 (must_read)</description><content:encoded>&lt;p&gt;&lt;strong&gt;Authors:&lt;/strong&gt; Jing Zhang; Zhikai Li; Xuewen Liu; Qingyi Gu&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Journal:&lt;/strong&gt; arXiv&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Relevance:&lt;/strong&gt; 0.792 (must_read)&lt;/p&gt;
&lt;p&gt;Segment Anything Model 2 (SAM2) shows excellent performance in video object segmentation tasks; however, the heavy computational burden hinders its application in real-time video processing. Although there have been efforts to improve the efficiency of SAM2, most of them focus on retraining a lightweight backbone, with little exploration into post-training acceleration. In this paper, we observe that SAM2 exhibits sparse perception pattern as biological vision, which provides opportunities for eliminating redundant computation and acceleration: i) In mask decoder, the attention primarily focuses on the foreground objects, whereas the image encoder in the earlier stage exhibits a broad attention span, which results in unnecessary computation to background regions. ii) In memory bank, only a small subset of tokens in each frame contribute significantly to memory attention, and the salient regions exhibit temporal consistency, making full-token computation redundant. With these insights, we propose Efficient-SAM2, which promotes SAM2 to adaptively focus on object regions while eliminating task-irrelevant computations, thereby significantly improving inference efficiency. Specifically, for image encoder, we propose object-aware Sparse Window Routing (SWR), a window-level computation allocation mechanism that leverages the consistency and saliency cues from the previous-frame decoder to route background regions into a lightweight shortcut branch. Moreover, for memory attention, we propose object-aware Sparse Memory Retrieval (SMR), which allows only the salient memory tokens in each frame to participate in computation, with the saliency pattern reused from their first recollection. With negligible additional parameters and minimal training overhead, Efficient-SAM2 delivers 1.68x speedup on SAM2.1-L model with only 1.0% accuracy drop on SA-V test set.&lt;/p&gt;</content:encoded></item><item><title>深度学习驱动的海上无人船智能感知与决策技术进展</title><link>https://doi.org/10.11834/jig.250439</link><guid>10.11834/jig.250439</guid><pubDate>Wed, 11 Feb 2026 07:27:45 +0000</pubDate><dc:creator>Wang Yuying</dc:creator><dc:creator>Wu Hao</dc:creator><dc:creator>Qing Yuhao</dc:creator><dc:creator>Zhang Weidong</dc:creator><dc:creator>Shen Liquan</dc:creator><dc:creator>Xu Xin</dc:creator><prism:publicationName>Journal of Image and Graphics</prism:publicationName><prism:doi>10.11834/jig.250439</prism:doi><description>近年来智能无人系统技术持续推动海上无人水面艇 （Unmanned Surface Vehicles， USVs）感知与决策能力的提升，涌现出诸多面向复杂海洋环境的创新研究与实践部署。本文首先系统梳理了USV的发展历程与体系架构，分析其在船体设计、动力系统、通信控制与多传感器集成等方面的演进特征；进而围绕智能感知这一核心环节，重点综述了深度学习模型及多模态传感器融合在海上目标检测、障碍物识别、海况感知与多目标跟踪等任务中的应用进展，结合典型海事视觉数据集探讨了算法在跨域泛化、实时性与环境鲁棒性方面面临的挑战；进一步，本文总结了基于感知的导航、制导与控制方法，以及多船协同与群体智能在复杂动态海域中的研究现状与应用前景；最后，从恶劣海况下的感知稳健性、多模态融合机制、实时安全决策与分布式协同等角度，展望了海上无人船智能技术未来发展的关键问题与研究方向。
Published: 2026-02-11T07:27:45+00:00
Venue: Journal of Image and Graphics
Score: 0.792 (must_read)</description><content:encoded>&lt;p&gt;&lt;strong&gt;Authors:&lt;/strong&gt; Wang Yuying; Wu Hao; Qing Yuhao; Zhang Weidong; Shen Liquan; Xu Xin&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Journal:&lt;/strong&gt; Journal of Image and Graphics&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;a href="https://doi.org/10.11834/jig.250439"&gt;10.11834/jig.250439&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Relevance:&lt;/strong&gt; 0.792 (must_read)&lt;/p&gt;
&lt;p&gt;近年来智能无人系统技术持续推动海上无人水面艇 （Unmanned Surface Vehicles， USVs）感知与决策能力的提升，涌现出诸多面向复杂海洋环境的创新研究与实践部署。本文首先系统梳理了USV的发展历程与体系架构，分析其在船体设计、动力系统、通信控制与多传感器集成等方面的演进特征；进而围绕智能感知这一核心环节，重点综述了深度学习模型及多模态传感器融合在海上目标检测、障碍物识别、海况感知与多目标跟踪等任务中的应用进展，结合典型海事视觉数据集探讨了算法在跨域泛化、实时性与环境鲁棒性方面面临的挑战；进一步，本文总结了基于感知的导航、制导与控制方法，以及多船协同与群体智能在复杂动态海域中的研究现状与应用前景；最后，从恶劣海况下的感知稳健性、多模态融合机制、实时安全决策与分布式协同等角度，展望了海上无人船智能技术未来发展的关键问题与研究方向。&lt;/p&gt;</content:encoded></item><item><title>Advancing autonomous driving systems: A 3-dimensional u-net framework for object detection via fusion of camera and LiDAR sensors</title><link>https://doi.org/10.1016/j.ins.2026.123215</link><guid>10.1016/j.ins.2026.123215</guid><pubDate>Wed, 11 Feb 2026 00:21:02 +0000</pubDate><dc:creator>Ali Foroutannia</dc:creator><dc:creator>Afshin Shoeibi</dc:creator><dc:creator>Amin Beheshti</dc:creator><dc:creator>Hamid Alinejad-Rokny</dc:creator><dc:creator>Sai Ho Ling</dc:creator><dc:creator>Hak-Keung Lam</dc:creator><prism:publicationName>Information Sciences</prism:publicationName><prism:doi>10.1016/j.ins.2026.123215</prism:doi><description>Object recognition is essential for autonomous cars, and the amalgamation of camera and light detection and ranging (LiDAR) sensor data has emerged as a pivotal method for accurate three-dimensional (3D) object recognition. Contemporary algorithms face challenges with fragmented data, high processing costs, insufficient resolution, and limited dynamic information. This study presents a novel approach utilising 3D U-Net deep learning for precise 3D object detection and localisation by integrating camera and LiDAR data. The process involves obtaining and preprocessing camera and LiDAR data, utilising a geometric 3D frustum method to extract 3D information from LiDAR based on 2D camera bounding boxes, and training a You Only Look Once version 4 (YOLO v4) network to recognise these boundaries in camera images. The detected images are combined with LiDAR data, and a deep U-Net network is utilised to define 3D bounding boxes. Performance is assessed at various noise levels (0 %, 1 %, 2 %, 5 %, and 10 %) in the composite images. This method leverages the benefits of both sensors to better object recognition across diverse shapes and sizes, even in challenging situations, signifying a significant progression towards safer and more reliable autonomous vehicles with improved situational awareness in intricate urban environments.
Published: 2026-02-11T00:21:02+00:00
Venue: Information Sciences
Score: 0.791 (must_read)</description><content:encoded>&lt;p&gt;&lt;strong&gt;Authors:&lt;/strong&gt; Ali Foroutannia; Afshin Shoeibi; Amin Beheshti; Hamid Alinejad-Rokny; Sai Ho Ling; Hak-Keung Lam&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Journal:&lt;/strong&gt; Information Sciences&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;a href="https://doi.org/10.1016/j.ins.2026.123215"&gt;10.1016/j.ins.2026.123215&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Relevance:&lt;/strong&gt; 0.791 (must_read)&lt;/p&gt;
&lt;p&gt;Object recognition is essential for autonomous cars, and the amalgamation of camera and light detection and ranging (LiDAR) sensor data has emerged as a pivotal method for accurate three-dimensional (3D) object recognition. Contemporary algorithms face challenges with fragmented data, high processing costs, insufficient resolution, and limited dynamic information. This study presents a novel approach utilising 3D U-Net deep learning for precise 3D object detection and localisation by integrating camera and LiDAR data. The process involves obtaining and preprocessing camera and LiDAR data, utilising a geometric 3D frustum method to extract 3D information from LiDAR based on 2D camera bounding boxes, and training a You Only Look Once version 4 (YOLO v4) network to recognise these boundaries in camera images. The detected images are combined with LiDAR data, and a deep U-Net network is utilised to define 3D bounding boxes. Performance is assessed at various noise levels (0 %, 1 %, 2 %, 5 %, and 10 %) in the composite images. This method leverages the benefits of both sensors to better object recognition across diverse shapes and sizes, even in challenging situations, signifying a significant progression towards safer and more reliable autonomous vehicles with improved situational awareness in intricate urban environments.&lt;/p&gt;</content:encoded></item></channel></rss>