<?xml version='1.0' encoding='utf-8'?>
<rss xmlns:content="http://purl.org/rss/1.0/modules/content/" xmlns:dc="http://purl.org/dc/elements/1.1/" xmlns:prism="http://prismstandard.org/namespaces/basic/3.0/" version="2.0"><channel><title>ZotWatch Feed</title><link>https://ehehe.cn/zotwatch/</link><description>AI-assisted literature watch</description><lastBuildDate>Wed, 28 Jan 2026 02:51:25 +0000</lastBuildDate><item><title>Spatial-Frequency Domain Joint Learning With Shape Constraints for Fine-Grained Aircraft Detection in SAR Imagery</title><link>https://doi.org/10.1109/jstars.2026.3657853</link><guid>10.1109/jstars.2026.3657853</guid><pubDate>Tue, 27 Jan 2026 05:52:07 +0000</pubDate><dc:creator>Ru Luo</dc:creator><dc:creator>Qishan He</dc:creator><dc:creator>Jiajin Li</dc:creator><dc:creator>Siqian Zhang</dc:creator><dc:creator>Lingjun Zhao</dc:creator><dc:creator>Kefeng Ji</dc:creator><prism:publicationName>IEEE Journal of Selected Topics in Applied Earth Observations and Remote Sensing</prism:publicationName><prism:doi>10.1109/jstars.2026.3657853</prism:doi><description>Fine-grained aircraft detection aims to detect aircraft and identify its subcategory, which is important for military reconnaissance and airport management. Compared with optical imagery, aircraft in Synthetic Aperture Radar (SAR) images exhibit high azimuth sensitivity and discrete scattering characteristics, leading to significant intra-class variance and topological fragmentation, which make fine-grained aircraft detection very challenging. Existing methods mainly rely on spatial domain feature processing and scattering keypoint supervision, which do not fully utilize frequency domain features that are particularly important for fine-grained detection. This paper proposes a novel dual domain feature learning architecture with shape constraints, SAR-SFNet, to enhance the fine-grained aircraft detection performance in SAR imagery. First, a Spatial-Frequency Domain Joint Learning (SFDJL) is proposed via integrating Fractional Gabor Transform (FrGT)'s localized, orientation-tuned responses with the Fourier's global contextual cues to enhance the saliency of aircraft under varied aspect angles. Second, a Class-Aware Shape Constraint (CASC) is designed by leveraging class-specific shape priors to mitigate intra-class variance and topological fragmentation. Extensive experiments on SAR-RADD and FAIR-CSAR datasets demonstrate that SAR-SFNet achieves a mean Average Precision (mAP) of 79.3% and 50.6%, outperforming state-of-the-art methods by 3.7% and 5.2%, respectively, while maintaining a competitive inference speed of 39.5 Frames Per Second (FPS). Furthermore, with a lightweight architecture of 7.8 M parameters and 15.3 G Floating Point Operations (FLOPs), the proposed method exhibits its potential for resource-constrained, real-time applications.
Published: 2026-01-27T05:52:07+00:00
Venue: IEEE Journal of Selected Topics in Applied Earth Observations and Remote Sensing
Score: 0.824 (must_read)</description><content:encoded>&lt;p&gt;&lt;strong&gt;Authors:&lt;/strong&gt; Ru Luo; Qishan He; Jiajin Li; Siqian Zhang; Lingjun Zhao; Kefeng Ji&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Journal:&lt;/strong&gt; IEEE Journal of Selected Topics in Applied Earth Observations and Remote Sensing&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;a href="https://doi.org/10.1109/jstars.2026.3657853"&gt;10.1109/jstars.2026.3657853&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Relevance:&lt;/strong&gt; 0.824 (must_read)&lt;/p&gt;
&lt;p&gt;Fine-grained aircraft detection aims to detect aircraft and identify its subcategory, which is important for military reconnaissance and airport management. Compared with optical imagery, aircraft in Synthetic Aperture Radar (SAR) images exhibit high azimuth sensitivity and discrete scattering characteristics, leading to significant intra-class variance and topological fragmentation, which make fine-grained aircraft detection very challenging. Existing methods mainly rely on spatial domain feature processing and scattering keypoint supervision, which do not fully utilize frequency domain features that are particularly important for fine-grained detection. This paper proposes a novel dual domain feature learning architecture with shape constraints, SAR-SFNet, to enhance the fine-grained aircraft detection performance in SAR imagery. First, a Spatial-Frequency Domain Joint Learning (SFDJL) is proposed via integrating Fractional Gabor Transform (FrGT)&amp;#x27;s localized, orientation-tuned responses with the Fourier&amp;#x27;s global contextual cues to enhance the saliency of aircraft under varied aspect angles. Second, a Class-Aware Shape Constraint (CASC) is designed by leveraging class-specific shape priors to mitigate intra-class variance and topological fragmentation. Extensive experiments on SAR-RADD and FAIR-CSAR datasets demonstrate that SAR-SFNet achieves a mean Average Precision (mAP) of 79.3% and 50.6%, outperforming state-of-the-art methods by 3.7% and 5.2%, respectively, while maintaining a competitive inference speed of 39.5 Frames Per Second (FPS). Furthermore, with a lightweight architecture of 7.8 M parameters and 15.3 G Floating Point Operations (FLOPs), the proposed method exhibits its potential for resource-constrained, real-time applications.&lt;/p&gt;</content:encoded></item><item><title>Parameter-Efficient Fine-Tuning Methods for Pretrained Language Models: A Critical Review and Assessment</title><link>https://doi.org/10.1109/tpami.2026.3657354</link><guid>10.1109/tpami.2026.3657354</guid><pubDate>Tue, 27 Jan 2026 05:48:07 +0000</pubDate><dc:creator>Lingling Xu</dc:creator><dc:creator>Haoran Xie</dc:creator><dc:creator>S. Joe Qin</dc:creator><dc:creator>Xiaohui Tao</dc:creator><dc:creator>Fu Lee Wang</dc:creator><prism:publicationName>IEEE Transactions on Pattern Analysis and Machine Intelligence</prism:publicationName><prism:doi>10.1109/tpami.2026.3657354</prism:doi><description>With the continuous growth in the number of parameters of the Transformer-based pretrained language models (PLMs), particularly the emergence of large language models (LLMs) with billions of parameters, many natural language processing (NLP) tasks have demonstrated remarkable success. However, the enormous size and computational demands of these models pose significant challenges for adapting them to specific downstream tasks, especially in environments with limited computational resources. Parameter-Efficient Fine-Tuning (PEFT) offers an effective solution by reducing the number of fine-tuning parameters and memory usage while achieving comparable performance to full fine-tuning. The demands for fine-tuning PLMs, especially LLMs, have led to a surge in the development of PEFT methods, as depicted in Fig. 1. In this paper, we present a comprehensive and systematic review of PEFT methods for PLMs. We summarize these PEFT methods, discuss their applications, and outline future directions. Furthermore, extensive experiments are conducted using several representative PEFT methods to better understand their effectiveness in parameter efficiency and memory efficiency. By offering insights into the latest advancements and practical applications, this survey serves as an invaluable resource for researchers and practitioners seeking to navigate the challenges and opportunities presented by PEFT in the context of PLMs.
Published: 2026-01-27T05:48:07+00:00
Venue: IEEE Transactions on Pattern Analysis and Machine Intelligence
Score: 0.820 (must_read)</description><content:encoded>&lt;p&gt;&lt;strong&gt;Authors:&lt;/strong&gt; Lingling Xu; Haoran Xie; S. Joe Qin; Xiaohui Tao; Fu Lee Wang&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Journal:&lt;/strong&gt; IEEE Transactions on Pattern Analysis and Machine Intelligence&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;a href="https://doi.org/10.1109/tpami.2026.3657354"&gt;10.1109/tpami.2026.3657354&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Relevance:&lt;/strong&gt; 0.820 (must_read)&lt;/p&gt;
&lt;p&gt;With the continuous growth in the number of parameters of the Transformer-based pretrained language models (PLMs), particularly the emergence of large language models (LLMs) with billions of parameters, many natural language processing (NLP) tasks have demonstrated remarkable success. However, the enormous size and computational demands of these models pose significant challenges for adapting them to specific downstream tasks, especially in environments with limited computational resources. Parameter-Efficient Fine-Tuning (PEFT) offers an effective solution by reducing the number of fine-tuning parameters and memory usage while achieving comparable performance to full fine-tuning. The demands for fine-tuning PLMs, especially LLMs, have led to a surge in the development of PEFT methods, as depicted in Fig. 1. In this paper, we present a comprehensive and systematic review of PEFT methods for PLMs. We summarize these PEFT methods, discuss their applications, and outline future directions. Furthermore, extensive experiments are conducted using several representative PEFT methods to better understand their effectiveness in parameter efficiency and memory efficiency. By offering insights into the latest advancements and practical applications, this survey serves as an invaluable resource for researchers and practitioners seeking to navigate the challenges and opportunities presented by PEFT in the context of PLMs.&lt;/p&gt;</content:encoded></item><item><title>PISTTN: Profile-aware Infrared Small Target Tracking Network using Spatiotemporal Context Information</title><link>https://doi.org/10.1109/tgrs.2026.3657763</link><guid>10.1109/tgrs.2026.3657763</guid><pubDate>Tue, 27 Jan 2026 05:48:56 +0000</pubDate><dc:creator>Xingyu Zhou</dc:creator><dc:creator>Yue Hu</dc:creator><prism:publicationName>IEEE Transactions on Geoscience and Remote Sensing</prism:publicationName><prism:doi>10.1109/tgrs.2026.3657763</prism:doi><description>Infrared small target detection and tracking play an increasingly important role in both military and civilian applications. However, challenges persist due to the small target size and low signal-to-noise ratio. For single-target detection and tracking, most existing methods require annotation in the initial frame. For multi-target detection and tracking, detectors often need to perform detection on each frame before tracking, which loses temporal features and struggles to handle occlusion effectively. Moreover, in some scenarios, the target often degenerates into a single point, posing significant challenges for detection and tracking. To address the challenges, we reformulate the infrared small target tracking task as a spatiotemporal profile detection problem, and proposes a novel infrared small target tracking network that unifies tracking and detection into a single end-to-end trainable architecture, termed the Profile-aware Infrared Small Target Tracking Network (PISTTN). Specifically, to address the loss of spatiotemporal information caused by single-frame detection in traditional tracking algorithms, we introduce a spatiotemporal tensor encoding module. This module automatically constructs sparse tensors based on target characteristics and employs 3D sparse convolution to extract profile-aware To address the challenges in detecting point-like targets, we propose a small target query module that integrates multi-scale features to enhance adaptability and generalization across varying target appearances, while generating distinct queries for different targets. In addition, we incorporate a profile detector to predict the spatiotemporal profile of targets, enabling accurate trajectory estimation through an efficient tracking strategy. Experimental results on multiple datasets demonstrate that the proposed network outperforms existing state-of-the-art methods in terms of visual and quantitative assessment.
Published: 2026-01-27T05:48:56+00:00
Venue: IEEE Transactions on Geoscience and Remote Sensing
Score: 0.818 (must_read)</description><content:encoded>&lt;p&gt;&lt;strong&gt;Authors:&lt;/strong&gt; Xingyu Zhou; Yue Hu&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Journal:&lt;/strong&gt; IEEE Transactions on Geoscience and Remote Sensing&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;a href="https://doi.org/10.1109/tgrs.2026.3657763"&gt;10.1109/tgrs.2026.3657763&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Relevance:&lt;/strong&gt; 0.818 (must_read)&lt;/p&gt;
&lt;p&gt;Infrared small target detection and tracking play an increasingly important role in both military and civilian applications. However, challenges persist due to the small target size and low signal-to-noise ratio. For single-target detection and tracking, most existing methods require annotation in the initial frame. For multi-target detection and tracking, detectors often need to perform detection on each frame before tracking, which loses temporal features and struggles to handle occlusion effectively. Moreover, in some scenarios, the target often degenerates into a single point, posing significant challenges for detection and tracking. To address the challenges, we reformulate the infrared small target tracking task as a spatiotemporal profile detection problem, and proposes a novel infrared small target tracking network that unifies tracking and detection into a single end-to-end trainable architecture, termed the Profile-aware Infrared Small Target Tracking Network (PISTTN). Specifically, to address the loss of spatiotemporal information caused by single-frame detection in traditional tracking algorithms, we introduce a spatiotemporal tensor encoding module. This module automatically constructs sparse tensors based on target characteristics and employs 3D sparse convolution to extract profile-aware To address the challenges in detecting point-like targets, we propose a small target query module that integrates multi-scale features to enhance adaptability and generalization across varying target appearances, while generating distinct queries for different targets. In addition, we incorporate a profile detector to predict the spatiotemporal profile of targets, enabling accurate trajectory estimation through an efficient tracking strategy. Experimental results on multiple datasets demonstrate that the proposed network outperforms existing state-of-the-art methods in terms of visual and quantitative assessment.&lt;/p&gt;</content:encoded></item><item><title>TextMonkey: an OCR-Free Large Multimodal Model for Understanding Document</title><link>https://doi.org/10.1109/tpami.2026.3653415</link><guid>10.1109/tpami.2026.3653415</guid><pubDate>Tue, 27 Jan 2026 05:48:07 +0000</pubDate><dc:creator>Yuliang Liu</dc:creator><dc:creator>Biao Yang</dc:creator><dc:creator>Qiang Liu</dc:creator><dc:creator>Zhang Li</dc:creator><dc:creator>Zhiyin Ma</dc:creator><dc:creator>Shuo Zhang</dc:creator><dc:creator>Xiang Bai</dc:creator><prism:publicationName>IEEE Transactions on Pattern Analysis and Machine Intelligence</prism:publicationName><prism:doi>10.1109/tpami.2026.3653415</prism:doi><description>We present TextMonkey, a large multimodal model (LMM) tailored for text-centric tasks. Our approach introduces enhancement across several dimensions: By adopting Shifted Window Attention layer, we achieve cross-window connectivity at higher input resolutions and stabilize early training; We hypothesize that images may contain redundant tokens, and by using similarity to filter out significant tokens, we can not only streamline the token length but also enhance the model's performance. Moreover, by expanding our model's capabilities to encompass text spotting and grounding, and incorporating positional information into responses, we enhance interpretability. Evaluation on 12 benchmarks shows notable improvements: 5.2% in Scene Text-Centric tasks (including STVQA, TextVQA, and OCRVQA), 6.9% in Document-Oriented tasks (such as DocVQA, InfoVQA, ChartVQA, DeepForm, Kleister Charity, and WikiTableQuestions), and 2.8% in Key Information Extraction tasks (comprising FUNSD, SROIE, and POIE). It outperforms in scene text spotting with a 10.9% increase and sets a new standard on OCRBench, a comprehensive benchmark consisting of 29 OCR-related assessments, with a score of 561, surpassing previous open-sourced large multimodal models for document understanding. Code is released at https://github.com/Yuliang-Liu/Monkey.
Published: 2026-01-27T05:48:07+00:00
Venue: IEEE Transactions on Pattern Analysis and Machine Intelligence
Score: 0.814 (must_read)</description><content:encoded>&lt;p&gt;&lt;strong&gt;Authors:&lt;/strong&gt; Yuliang Liu; Biao Yang; Qiang Liu; Zhang Li; Zhiyin Ma; Shuo Zhang; Xiang Bai&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Journal:&lt;/strong&gt; IEEE Transactions on Pattern Analysis and Machine Intelligence&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;a href="https://doi.org/10.1109/tpami.2026.3653415"&gt;10.1109/tpami.2026.3653415&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Relevance:&lt;/strong&gt; 0.814 (must_read)&lt;/p&gt;
&lt;p&gt;We present TextMonkey, a large multimodal model (LMM) tailored for text-centric tasks. Our approach introduces enhancement across several dimensions: By adopting Shifted Window Attention layer, we achieve cross-window connectivity at higher input resolutions and stabilize early training; We hypothesize that images may contain redundant tokens, and by using similarity to filter out significant tokens, we can not only streamline the token length but also enhance the model&amp;#x27;s performance. Moreover, by expanding our model&amp;#x27;s capabilities to encompass text spotting and grounding, and incorporating positional information into responses, we enhance interpretability. Evaluation on 12 benchmarks shows notable improvements: 5.2% in Scene Text-Centric tasks (including STVQA, TextVQA, and OCRVQA), 6.9% in Document-Oriented tasks (such as DocVQA, InfoVQA, ChartVQA, DeepForm, Kleister Charity, and WikiTableQuestions), and 2.8% in Key Information Extraction tasks (comprising FUNSD, SROIE, and POIE). It outperforms in scene text spotting with a 10.9% increase and sets a new standard on OCRBench, a comprehensive benchmark consisting of 29 OCR-related assessments, with a score of 561, surpassing previous open-sourced large multimodal models for document understanding. Code is released at https://github.com/Yuliang-Liu/Monkey.&lt;/p&gt;</content:encoded></item><item><title>Consistency-Regularized GAN for Few-Shot SAR Target Recognition</title><link>https://doi.org/10.1109/tgrs.2026.3657831</link><guid>10.1109/tgrs.2026.3657831</guid><pubDate>Tue, 27 Jan 2026 05:48:56 +0000</pubDate><dc:creator>Yikui Zhai</dc:creator><dc:creator>Shikuang Liu</dc:creator><dc:creator>Wenlve Zhou</dc:creator><dc:creator>Hongsheng Zhang</dc:creator><dc:creator>Zhiheng Zhou</dc:creator><dc:creator>Xiaolin Tian</dc:creator><dc:creator>C. L. Philip Chen</dc:creator><prism:publicationName>IEEE Transactions on Geoscience and Remote Sensing</prism:publicationName><prism:doi>10.1109/tgrs.2026.3657831</prism:doi><description>Few-shot recognition in synthetic aperture radar (SAR) imagery remains a critical bottleneck for real-world applications due to extreme data scarcity. A promising strategy involves synthesizing a large dataset with a generative adversarial network (GAN), pre-training a model via self-supervised learning (SSL), and then fine-tuning on the few labeled samples. However, this approach faces a fundamental paradox: conventional GANs themselves require abundant data for stable training, contradicting the premise of few-shot learning. To resolve this, we propose the consistency-regularized generative adversarial network (Cr-GAN), a novel framework designed to synthesize diverse, high-fidelity samples even when trained under these severe data limitations. Cr-GAN introduces a dual-branch discriminator that decouples adversarial training from representation learning. This architecture enables a channel-wise feature interpolation strategy to create novel latent features, complemented by a dual-domain cycle consistency mechanism that ensures semantic integrity. Our Cr-GAN framework is adaptable to various GAN architectures, and its synthesized data effectively boosts multiple SSL algorithms. Extensive experiments on the MSTAR and SRSDD datasets validate our approach, with Cr-GAN achieving a highly competitive accuracy of 71.21% and 51.64%, respectively, in the 8-shot setting, significantly outperforming leading baselines, while requiring only ~5% of the parameters of state-of-the-art diffusion models. Code is available at: https://github.com/yikuizhai/Cr-GAN.
Published: 2026-01-27T05:48:56+00:00
Venue: IEEE Transactions on Geoscience and Remote Sensing
Score: 0.811 (must_read)</description><content:encoded>&lt;p&gt;&lt;strong&gt;Authors:&lt;/strong&gt; Yikui Zhai; Shikuang Liu; Wenlve Zhou; Hongsheng Zhang; Zhiheng Zhou; Xiaolin Tian; C. L. Philip Chen&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Journal:&lt;/strong&gt; IEEE Transactions on Geoscience and Remote Sensing&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;a href="https://doi.org/10.1109/tgrs.2026.3657831"&gt;10.1109/tgrs.2026.3657831&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Relevance:&lt;/strong&gt; 0.811 (must_read)&lt;/p&gt;
&lt;p&gt;Few-shot recognition in synthetic aperture radar (SAR) imagery remains a critical bottleneck for real-world applications due to extreme data scarcity. A promising strategy involves synthesizing a large dataset with a generative adversarial network (GAN), pre-training a model via self-supervised learning (SSL), and then fine-tuning on the few labeled samples. However, this approach faces a fundamental paradox: conventional GANs themselves require abundant data for stable training, contradicting the premise of few-shot learning. To resolve this, we propose the consistency-regularized generative adversarial network (Cr-GAN), a novel framework designed to synthesize diverse, high-fidelity samples even when trained under these severe data limitations. Cr-GAN introduces a dual-branch discriminator that decouples adversarial training from representation learning. This architecture enables a channel-wise feature interpolation strategy to create novel latent features, complemented by a dual-domain cycle consistency mechanism that ensures semantic integrity. Our Cr-GAN framework is adaptable to various GAN architectures, and its synthesized data effectively boosts multiple SSL algorithms. Extensive experiments on the MSTAR and SRSDD datasets validate our approach, with Cr-GAN achieving a highly competitive accuracy of 71.21% and 51.64%, respectively, in the 8-shot setting, significantly outperforming leading baselines, while requiring only ~5% of the parameters of state-of-the-art diffusion models. Code is available at: https://github.com/yikuizhai/Cr-GAN.&lt;/p&gt;</content:encoded></item><item><title>Unified Local and Global Transformer for Infrared Small UAV Tracking</title><link>https://doi.org/10.1109/tgrs.2026.3657906</link><guid>10.1109/tgrs.2026.3657906</guid><pubDate>Tue, 27 Jan 2026 05:48:56 +0000</pubDate><dc:creator>Yaohong Chen</dc:creator><dc:creator>Tianlei Ma</dc:creator><dc:creator>Donglin Xue</dc:creator><dc:creator>Xinhao Liu</dc:creator><dc:creator>Weining Chen</dc:creator><prism:publicationName>IEEE Transactions on Geoscience and Remote Sensing</prism:publicationName><prism:doi>10.1109/tgrs.2026.3657906</prism:doi><description>Long-term tracking of infrared small unmanned aerial vehicle (UAV) poses substantial challenges, including dynamic backgrounds, clutter interference, and target occlusion. This study introduces an innovative method that integrates a local tracking network with a global search strategy to effectively address these issues. Firstly, a background motion estimation module is proposed to mitigate dynamic background interference by aligning consecutive frames through motion state assessment. Secondly, a full-transformer local tracking network is developed to suppress background clutter. It enhances feature representation using Spectformer as the backbone and leverages cross-attention mechanisms to robustly handle clutter. Finally, a global search strategy featuring a large-scale search module is designed to address target occlusion. This module provides reliable local search regions for the tracking network when occlusion occurs. Extensive experiments on infrared drone datasets validate that the proposed method outperforms state-of-the-art approaches, achieving high success rates, high precision, and high real-time processing at 45 FPS under specific configurations.
Published: 2026-01-27T05:48:56+00:00
Venue: IEEE Transactions on Geoscience and Remote Sensing
Score: 0.809 (must_read)</description><content:encoded>&lt;p&gt;&lt;strong&gt;Authors:&lt;/strong&gt; Yaohong Chen; Tianlei Ma; Donglin Xue; Xinhao Liu; Weining Chen&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Journal:&lt;/strong&gt; IEEE Transactions on Geoscience and Remote Sensing&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;a href="https://doi.org/10.1109/tgrs.2026.3657906"&gt;10.1109/tgrs.2026.3657906&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Relevance:&lt;/strong&gt; 0.809 (must_read)&lt;/p&gt;
&lt;p&gt;Long-term tracking of infrared small unmanned aerial vehicle (UAV) poses substantial challenges, including dynamic backgrounds, clutter interference, and target occlusion. This study introduces an innovative method that integrates a local tracking network with a global search strategy to effectively address these issues. Firstly, a background motion estimation module is proposed to mitigate dynamic background interference by aligning consecutive frames through motion state assessment. Secondly, a full-transformer local tracking network is developed to suppress background clutter. It enhances feature representation using Spectformer as the backbone and leverages cross-attention mechanisms to robustly handle clutter. Finally, a global search strategy featuring a large-scale search module is designed to address target occlusion. This module provides reliable local search regions for the tracking network when occlusion occurs. Extensive experiments on infrared drone datasets validate that the proposed method outperforms state-of-the-art approaches, achieving high success rates, high precision, and high real-time processing at 45 FPS under specific configurations.&lt;/p&gt;</content:encoded></item><item><title>SU-RMT: Toward Bridging Semantic Representation and Structural Detail Modeling for Medical Image Segmentation</title><link>https://doi.org/10.1016/j.inffus.2026.104182</link><guid>10.1016/j.inffus.2026.104182</guid><pubDate>Mon, 26 Jan 2026 06:56:57 +0000</pubDate><dc:creator>Peibo Song</dc:creator><dc:creator>Zihao Wang</dc:creator><dc:creator>Jinshuo Zhang</dc:creator><dc:creator>Shujun Fu</dc:creator><dc:creator>Yunfeng Zhang</dc:creator><dc:creator>Wei Wu</dc:creator><dc:creator>Fangxun Bao</dc:creator><prism:publicationName>Information Fusion</prism:publicationName><prism:doi>10.1016/j.inffus.2026.104182</prism:doi><description>Accurate medical image segmentation requires models that capture high-level semantics while preserving fine-grained structural details, due to anatomical heterogeneity and subtle textures in clinical scenarios. However, existing U-shaped networks usually lack a unified perspective to reconcile semantic representation with structural detail. To this end, we present SU-RMT , a U-shaped network that embodies this unified perspective by redesigning the encoder, bottleneck, and skip connection. The encoder employs the Dy namic S patial A ttention (DySA) mechanism to capture global context with spatial priors. The bottleneck introduces a H ybrid S pectral A daptive (HSA) module to transform abstract semantics into structure-aware features. The first skip connection incorporates a F requency- F used (F 2 ) block to enhance boundary details without amplifying noise. Across several medical image segmentation tasks, SU-RMT demonstrates strong performance. The code is at the link .
Published: 2026-01-26T06:56:57+00:00
Venue: Information Fusion
Score: 0.806 (must_read)</description><content:encoded>&lt;p&gt;&lt;strong&gt;Authors:&lt;/strong&gt; Peibo Song; Zihao Wang; Jinshuo Zhang; Shujun Fu; Yunfeng Zhang; Wei Wu; Fangxun Bao&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Journal:&lt;/strong&gt; Information Fusion&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;a href="https://doi.org/10.1016/j.inffus.2026.104182"&gt;10.1016/j.inffus.2026.104182&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Relevance:&lt;/strong&gt; 0.806 (must_read)&lt;/p&gt;
&lt;p&gt;Accurate medical image segmentation requires models that capture high-level semantics while preserving fine-grained structural details, due to anatomical heterogeneity and subtle textures in clinical scenarios. However, existing U-shaped networks usually lack a unified perspective to reconcile semantic representation with structural detail. To this end, we present SU-RMT , a U-shaped network that embodies this unified perspective by redesigning the encoder, bottleneck, and skip connection. The encoder employs the Dy namic S patial A ttention (DySA) mechanism to capture global context with spatial priors. The bottleneck introduces a H ybrid S pectral A daptive (HSA) module to transform abstract semantics into structure-aware features. The first skip connection incorporates a F requency- F used (F 2 ) block to enhance boundary details without amplifying noise. Across several medical image segmentation tasks, SU-RMT demonstrates strong performance. The code is at the link .&lt;/p&gt;</content:encoded></item><item><title>Learning Global Dynamic Query for Large–Motion Infrared Small Target Detection</title><link>https://doi.org/10.1109/tgrs.2026.3657842</link><guid>10.1109/tgrs.2026.3657842</guid><pubDate>Tue, 27 Jan 2026 05:48:56 +0000</pubDate><dc:creator>Chuiyi Deng</dc:creator><dc:creator>Yanyin Guo</dc:creator><dc:creator>Xiang Xu</dc:creator><dc:creator>Zhuoyi Zhao</dc:creator><dc:creator>Yixin Xia</dc:creator><dc:creator>Runxuan An</dc:creator><dc:creator>Junwei Li</dc:creator><dc:creator>Antonio Plaza</dc:creator><prism:publicationName>IEEE Transactions on Geoscience and Remote Sensing</prism:publicationName><prism:doi>10.1109/tgrs.2026.3657842</prism:doi><description>Motion Infrared Small Target Detection (MIRSTD) leverages multi-frame temporal dependencies to improve detection robustness. However, existing methods have difficulty modeling global consistency and achieving precise alignment in complex motion and large displacement scenarios, leading to dispersed target representations and higher error rates. To address these challenges, we propose Dynamic Query Aligner (DQAligner), which introduces global random large-displacement augmentation and a cross-scale bidirectional shared attention mechanism to enhance inter-frame consistency. A dynamic receptive field pyramid deformable convolution decomposes complex multi-scale motions, enabling precise target alignment. Furthermore, class query memory serves as the generalized residual form of deformable convolution, which iteratively learns dynamic query representations to facilitate global target localization within each frame and maintain semantic consistency across frames. DQAligner achieves a paradigm shift from rigid alignment to flexible matching, and significantly boosts detection performance in large displacement and dynamic scenarios. Experiments on extensive stationary and moving platform datasets show that DQAligner outperforms existing methods, especially under complex motion and low signal-noise-rate conditions. Code will be available at https://github.com/dengfa02/DQAligner_MIRSTD.
Published: 2026-01-27T05:48:56+00:00
Venue: IEEE Transactions on Geoscience and Remote Sensing
Score: 0.799 (must_read)</description><content:encoded>&lt;p&gt;&lt;strong&gt;Authors:&lt;/strong&gt; Chuiyi Deng; Yanyin Guo; Xiang Xu; Zhuoyi Zhao; Yixin Xia; Runxuan An; Junwei Li; Antonio Plaza&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Journal:&lt;/strong&gt; IEEE Transactions on Geoscience and Remote Sensing&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;a href="https://doi.org/10.1109/tgrs.2026.3657842"&gt;10.1109/tgrs.2026.3657842&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Relevance:&lt;/strong&gt; 0.799 (must_read)&lt;/p&gt;
&lt;p&gt;Motion Infrared Small Target Detection (MIRSTD) leverages multi-frame temporal dependencies to improve detection robustness. However, existing methods have difficulty modeling global consistency and achieving precise alignment in complex motion and large displacement scenarios, leading to dispersed target representations and higher error rates. To address these challenges, we propose Dynamic Query Aligner (DQAligner), which introduces global random large-displacement augmentation and a cross-scale bidirectional shared attention mechanism to enhance inter-frame consistency. A dynamic receptive field pyramid deformable convolution decomposes complex multi-scale motions, enabling precise target alignment. Furthermore, class query memory serves as the generalized residual form of deformable convolution, which iteratively learns dynamic query representations to facilitate global target localization within each frame and maintain semantic consistency across frames. DQAligner achieves a paradigm shift from rigid alignment to flexible matching, and significantly boosts detection performance in large displacement and dynamic scenarios. Experiments on extensive stationary and moving platform datasets show that DQAligner outperforms existing methods, especially under complex motion and low signal-noise-rate conditions. Code will be available at https://github.com/dengfa02/DQAligner_MIRSTD.&lt;/p&gt;</content:encoded></item><item><title>MSMC: Multi-Scale Embedding and Meta-Contrastive Learning for Few-Shot Fine-Grained SAR Target Classification</title><link>https://doi.org/10.3390/rs18030415</link><guid>10.3390/rs18030415</guid><pubDate>Tue, 27 Jan 2026 09:05:27 +0000</pubDate><dc:creator>Bowen Chen</dc:creator><dc:creator>Minjia Yang</dc:creator><dc:creator>Yue Wang</dc:creator><dc:creator>Xueru Bai</dc:creator><prism:publicationName>Remote Sensing</prism:publicationName><prism:doi>10.3390/rs18030415</prism:doi><description>Constrained by observation conditions and high inter-class similarity, effective feature extraction and classification of synthetic aperture radar (SAR) targets in few-shot scenarios remains a persistent challenge. To address this issue, this article proposes a few-shot fine-grained SAR target classification method based on multi-scale embedding network and meta-contrastive learning (MSMC). Specifically, the MSMC integrates two complementary training pipelines; the first employs metric-based meta-learning to facilitate few-shot classification, while the second adopts an auxiliary training strategy to enhance feature diversity through contrastive learning. Furthermore, a shared multi-scale embedding network (MSEN) is designed to extract discriminative multi-scale features via adaptive candidate region generation and joint multi-scale embedding. The experimental results on the MSTAR dataset demonstrate that the proposed method achieves superior few-shot fine-grained classification performance compared to existing methods.
Published: 2026-01-27T09:05:27+00:00
Venue: Remote Sensing
Score: 0.796 (must_read)</description><content:encoded>&lt;p&gt;&lt;strong&gt;Authors:&lt;/strong&gt; Bowen Chen; Minjia Yang; Yue Wang; Xueru Bai&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Journal:&lt;/strong&gt; Remote Sensing&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;a href="https://doi.org/10.3390/rs18030415"&gt;10.3390/rs18030415&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Relevance:&lt;/strong&gt; 0.796 (must_read)&lt;/p&gt;
&lt;p&gt;Constrained by observation conditions and high inter-class similarity, effective feature extraction and classification of synthetic aperture radar (SAR) targets in few-shot scenarios remains a persistent challenge. To address this issue, this article proposes a few-shot fine-grained SAR target classification method based on multi-scale embedding network and meta-contrastive learning (MSMC). Specifically, the MSMC integrates two complementary training pipelines; the first employs metric-based meta-learning to facilitate few-shot classification, while the second adopts an auxiliary training strategy to enhance feature diversity through contrastive learning. Furthermore, a shared multi-scale embedding network (MSEN) is designed to extract discriminative multi-scale features via adaptive candidate region generation and joint multi-scale embedding. The experimental results on the MSTAR dataset demonstrate that the proposed method achieves superior few-shot fine-grained classification performance compared to existing methods.&lt;/p&gt;</content:encoded></item><item><title>EFSI-DETR: Efficient Frequency-Semantic Integration for Real-Time Small Object Detection in UAV Imagery</title><link>https://arxiv.org/abs/2601.18597v1</link><guid>http://arxiv.org/abs/2601.18597v1</guid><pubDate>Mon, 26 Jan 2026 15:41:37 +0000</pubDate><dc:creator>Yu Xia</dc:creator><dc:creator>Chang Liu</dc:creator><dc:creator>Tianqi Xiang</dc:creator><dc:creator>Zhigang Tu</dc:creator><dc:type>preprint</dc:type><prism:publicationName>arXiv</prism:publicationName><description>Real-time small object detection in Unmanned Aerial Vehicle (UAV) imagery remains challenging due to limited feature representation and ineffective multi-scale fusion. Existing methods underutilize frequency information and rely on static convolutional operations, which constrain the capacity to obtain rich feature representations and hinder the effective exploitation of deep semantic features. To address these issues, we propose EFSI-DETR, a novel detection framework that integrates efficient semantic feature enhancement with dynamic frequency-spatial guidance. EFSI-DETR comprises two main components: (1) a Dynamic Frequency-Spatial Unified Synergy Network (DyFusNet) that jointly exploits frequency and spatial cues for robust multi-scale feature fusion, (2) an Efficient Semantic Feature Concentrator (ESFC) that enables deep semantic extraction with minimal computational cost. Furthermore, a Fine-grained Feature Retention (FFR) strategy is adopted to incorporate spatially rich shallow features during fusion to preserve fine-grained details, crucial for small object detection in UAV imagery. Extensive experiments on VisDrone and CODrone benchmarks demonstrate that our EFSI-DETR achieves the state-of-the-art performance with real-time efficiency, yielding improvement of \textbf{1.6}\% and \textbf{5.8}\% in AP and AP$_{s}$ on VisDrone, while obtaining \textbf{188} FPS inference speed on a single RTX 4090 GPU.
Published: 2026-01-26T15:41:37+00:00
Venue: arXiv
Score: 0.794 (must_read)</description><content:encoded>&lt;p&gt;&lt;strong&gt;Authors:&lt;/strong&gt; Yu Xia; Chang Liu; Tianqi Xiang; Zhigang Tu&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Journal:&lt;/strong&gt; arXiv&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Relevance:&lt;/strong&gt; 0.794 (must_read)&lt;/p&gt;
&lt;p&gt;Real-time small object detection in Unmanned Aerial Vehicle (UAV) imagery remains challenging due to limited feature representation and ineffective multi-scale fusion. Existing methods underutilize frequency information and rely on static convolutional operations, which constrain the capacity to obtain rich feature representations and hinder the effective exploitation of deep semantic features. To address these issues, we propose EFSI-DETR, a novel detection framework that integrates efficient semantic feature enhancement with dynamic frequency-spatial guidance. EFSI-DETR comprises two main components: (1) a Dynamic Frequency-Spatial Unified Synergy Network (DyFusNet) that jointly exploits frequency and spatial cues for robust multi-scale feature fusion, (2) an Efficient Semantic Feature Concentrator (ESFC) that enables deep semantic extraction with minimal computational cost. Furthermore, a Fine-grained Feature Retention (FFR) strategy is adopted to incorporate spatially rich shallow features during fusion to preserve fine-grained details, crucial for small object detection in UAV imagery. Extensive experiments on VisDrone and CODrone benchmarks demonstrate that our EFSI-DETR achieves the state-of-the-art performance with real-time efficiency, yielding improvement of \textbf{1.6}\% and \textbf{5.8}\% in AP and AP$_{s}$ on VisDrone, while obtaining \textbf{188} FPS inference speed on a single RTX 4090 GPU.&lt;/p&gt;</content:encoded></item><item><title>VAE-REPA: Variational Autoencoder Representation Alignment for Efficient Diffusion Training</title><link>https://arxiv.org/abs/2601.17830v1</link><guid>http://arxiv.org/abs/2601.17830v1</guid><pubDate>Sun, 25 Jan 2026 13:22:38 +0000</pubDate><dc:creator>Mengmeng Wang</dc:creator><dc:creator>Dengyang Jiang</dc:creator><dc:creator>Liuzhuozheng Li</dc:creator><dc:creator>Yucheng Lin</dc:creator><dc:creator>Guojiang Shen</dc:creator><dc:creator>Xiangjie Kong</dc:creator><dc:creator>Yong Liu</dc:creator><dc:creator>Guang Dai</dc:creator><dc:creator>Jingdong Wang</dc:creator><dc:type>preprint</dc:type><prism:publicationName>arXiv</prism:publicationName><description>Denoising-based diffusion transformers, despite their strong generation performance, suffer from inefficient training convergence. Existing methods addressing this issue, such as REPA (relying on external representation encoders) or SRA (requiring dual-model setups), inevitably incur heavy computational overhead during training due to external dependencies. To tackle these challenges, this paper proposes \textbf{\namex}, a lightweight intrinsic guidance framework for efficient diffusion training. \name leverages off-the-shelf pre-trained Variational Autoencoder (VAE) features: their reconstruction property ensures inherent encoding of visual priors like rich texture details, structural patterns, and basic semantic information. Specifically, \name aligns the intermediate latent features of diffusion transformers with VAE features via a lightweight projection layer, supervised by a feature alignment loss. This design accelerates training without extra representation encoders or dual-model maintenance, resulting in a simple yet effective pipeline. Extensive experiments demonstrate that \name improves both generation quality and training convergence speed compared to vanilla diffusion transformers, matches or outperforms state-of-the-art acceleration methods, and incurs merely 4\% extra GFLOPs with zero additional cost for external guidance models.
Published: 2026-01-25T13:22:38+00:00
Venue: arXiv
Score: 0.788 (must_read)</description><content:encoded>&lt;p&gt;&lt;strong&gt;Authors:&lt;/strong&gt; Mengmeng Wang; Dengyang Jiang; Liuzhuozheng Li; Yucheng Lin; Guojiang Shen; Xiangjie Kong; Yong Liu; Guang Dai; Jingdong Wang&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Journal:&lt;/strong&gt; arXiv&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Relevance:&lt;/strong&gt; 0.788 (must_read)&lt;/p&gt;
&lt;p&gt;Denoising-based diffusion transformers, despite their strong generation performance, suffer from inefficient training convergence. Existing methods addressing this issue, such as REPA (relying on external representation encoders) or SRA (requiring dual-model setups), inevitably incur heavy computational overhead during training due to external dependencies. To tackle these challenges, this paper proposes \textbf{\namex}, a lightweight intrinsic guidance framework for efficient diffusion training. \name leverages off-the-shelf pre-trained Variational Autoencoder (VAE) features: their reconstruction property ensures inherent encoding of visual priors like rich texture details, structural patterns, and basic semantic information. Specifically, \name aligns the intermediate latent features of diffusion transformers with VAE features via a lightweight projection layer, supervised by a feature alignment loss. This design accelerates training without extra representation encoders or dual-model maintenance, resulting in a simple yet effective pipeline. Extensive experiments demonstrate that \name improves both generation quality and training convergence speed compared to vanilla diffusion transformers, matches or outperforms state-of-the-art acceleration methods, and incurs merely 4\% extra GFLOPs with zero additional cost for external guidance models.&lt;/p&gt;</content:encoded></item><item><title>DHPT: Dual-Modality Heterogeneous Prompt Tuning for Online Test-time Adaption in Vision-language Models</title><link>https://doi.org/10.1109/tcsvt.2026.3657756</link><guid>10.1109/tcsvt.2026.3657756</guid><pubDate>Tue, 27 Jan 2026 06:05:44 +0000</pubDate><dc:creator>Guiqin Wang</dc:creator><dc:creator>Peng Zhao</dc:creator><dc:creator>Xiang Wang</dc:creator><dc:creator>Haoran Guo</dc:creator><dc:creator>Nan Qi</dc:creator><dc:creator>Shusen Yang</dc:creator><dc:creator>Qinghai Guo</dc:creator><prism:publicationName>IEEE Transactions on Circuits and Systems for Video Technology</prism:publicationName><prism:doi>10.1109/tcsvt.2026.3657756</prism:doi><description>Test-Time Adaptation (TTA) has recently emerged as a promising research direction, enabling vision-language models (VLMs) to adapt to unlabeled test data in zero-shot settings. Among TTA approaches, test-time prompt tuning has shown great potential for enhancing the practical applicability of VLMs. However, existing methods typically either focus on adapting a single modality or apply uniform optimization to both modalities, without explicitly defining modality-specific optimization objectives. Such a one-size-fits-all strategy often results in suboptimal performance under test-time conditions. To address this limitation, we propose Dual-modality Heterogeneous Prompt Tuning (DHPT), a novel framework designed to simultaneously capture fine-grained textual semantics and alleviate domain shift noise in the visual modality. Specifically, we leverage a large language model to provide textual cognition guidance for the text encoder, while on the vision side, we develop a lightweight calibration module that adaptively mitigates domain shift noise across different scales. Furthermore, we introduce a cluster-tight optimization objective that enhances the stability and generalizability of prompt tuning under distribution shifts. Extensive experiments conducted on 11 benchmark datasets demonstrate that DHPT consistently and significantly outperforms existing TTA methods for VLMs.
Published: 2026-01-27T06:05:44+00:00
Venue: IEEE Transactions on Circuits and Systems for Video Technology
Score: 0.787 (must_read)</description><content:encoded>&lt;p&gt;&lt;strong&gt;Authors:&lt;/strong&gt; Guiqin Wang; Peng Zhao; Xiang Wang; Haoran Guo; Nan Qi; Shusen Yang; Qinghai Guo&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Journal:&lt;/strong&gt; IEEE Transactions on Circuits and Systems for Video Technology&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;a href="https://doi.org/10.1109/tcsvt.2026.3657756"&gt;10.1109/tcsvt.2026.3657756&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Relevance:&lt;/strong&gt; 0.787 (must_read)&lt;/p&gt;
&lt;p&gt;Test-Time Adaptation (TTA) has recently emerged as a promising research direction, enabling vision-language models (VLMs) to adapt to unlabeled test data in zero-shot settings. Among TTA approaches, test-time prompt tuning has shown great potential for enhancing the practical applicability of VLMs. However, existing methods typically either focus on adapting a single modality or apply uniform optimization to both modalities, without explicitly defining modality-specific optimization objectives. Such a one-size-fits-all strategy often results in suboptimal performance under test-time conditions. To address this limitation, we propose Dual-modality Heterogeneous Prompt Tuning (DHPT), a novel framework designed to simultaneously capture fine-grained textual semantics and alleviate domain shift noise in the visual modality. Specifically, we leverage a large language model to provide textual cognition guidance for the text encoder, while on the vision side, we develop a lightweight calibration module that adaptively mitigates domain shift noise across different scales. Furthermore, we introduce a cluster-tight optimization objective that enhances the stability and generalizability of prompt tuning under distribution shifts. Extensive experiments conducted on 11 benchmark datasets demonstrate that DHPT consistently and significantly outperforms existing TTA methods for VLMs.&lt;/p&gt;</content:encoded></item><item><title>S²TA-Fuse: Semantic-Superpixel Tokenized Attention for Spatial Spectral Fusion</title><link>https://doi.org/10.1109/tgrs.2026.3657766</link><guid>10.1109/tgrs.2026.3657766</guid><pubDate>Tue, 27 Jan 2026 05:48:56 +0000</pubDate><dc:creator>Jiawei Jiang</dc:creator><dc:creator>Wei Li</dc:creator><dc:creator>Jieyuan Pei</dc:creator><dc:creator>Junwei Zhu</dc:creator><dc:creator>Honghui Xu</dc:creator><dc:creator>Yuchao Feng</dc:creator><dc:creator>Jianwei Zheng</dc:creator><prism:publicationName>IEEE Transactions on Geoscience and Remote Sensing</prism:publicationName><prism:doi>10.1109/tgrs.2026.3657766</prism:doi><description>Superpixel paradigms have long been regarded as a well-established approach of mitigating spatial redundancy in hyperspectral images, yet their reliance on non-differentiable and irreversible segmentation renders them unsuitable for end-to-end spatial–spectral fusion (SSF). To tackle this limitation, this study introduces S²TA-Fuse, a transformer-based solver named Semantic-Superpixel Tokenized Attention for Fusion, which preserves the efficiency of superpixels while removing the need for explicit segmentation. The central design lies in a semantic attention mechanism that adaptively organizes pixels into deformable and content-aware semantic groups. Pixels sharing similar latent states are softly aggregated and encoded as compact tokens, upon which attention is computed to capture intricate long-range dependencies. This formulation endows the model with an inherent ability to accommodate scale variations while maintaining linear computational complexity with respect to the number of pixels. On top of the semantic backbone, two complementary components are devised. The Local Spectral Pyramid enhances the representation of multi-scale spectral cues in the spatial domain, whereas FreqNet supplements global information by modeling frequency-dependent variations through amplitude and phase decomposition. Comprehensive experiments on widely used benchmarks for spatial–spectral fusion demonstrate that S²TA-Fuse consistently surpasses the state of the art both in quantitative accuracy and visual fidelity.
Published: 2026-01-27T05:48:56+00:00
Venue: IEEE Transactions on Geoscience and Remote Sensing
Score: 0.785 (must_read)</description><content:encoded>&lt;p&gt;&lt;strong&gt;Authors:&lt;/strong&gt; Jiawei Jiang; Wei Li; Jieyuan Pei; Junwei Zhu; Honghui Xu; Yuchao Feng; Jianwei Zheng&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Journal:&lt;/strong&gt; IEEE Transactions on Geoscience and Remote Sensing&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;a href="https://doi.org/10.1109/tgrs.2026.3657766"&gt;10.1109/tgrs.2026.3657766&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Relevance:&lt;/strong&gt; 0.785 (must_read)&lt;/p&gt;
&lt;p&gt;Superpixel paradigms have long been regarded as a well-established approach of mitigating spatial redundancy in hyperspectral images, yet their reliance on non-differentiable and irreversible segmentation renders them unsuitable for end-to-end spatial–spectral fusion (SSF). To tackle this limitation, this study introduces S²TA-Fuse, a transformer-based solver named Semantic-Superpixel Tokenized Attention for Fusion, which preserves the efficiency of superpixels while removing the need for explicit segmentation. The central design lies in a semantic attention mechanism that adaptively organizes pixels into deformable and content-aware semantic groups. Pixels sharing similar latent states are softly aggregated and encoded as compact tokens, upon which attention is computed to capture intricate long-range dependencies. This formulation endows the model with an inherent ability to accommodate scale variations while maintaining linear computational complexity with respect to the number of pixels. On top of the semantic backbone, two complementary components are devised. The Local Spectral Pyramid enhances the representation of multi-scale spectral cues in the spatial domain, whereas FreqNet supplements global information by modeling frequency-dependent variations through amplitude and phase decomposition. Comprehensive experiments on widely used benchmarks for spatial–spectral fusion demonstrate that S²TA-Fuse consistently surpasses the state of the art both in quantitative accuracy and visual fidelity.&lt;/p&gt;</content:encoded></item><item><title>Instance-Guided Radar Depth Estimation for 3D Object Detection</title><link>https://arxiv.org/abs/2601.19314v1</link><guid>http://arxiv.org/abs/2601.19314v1</guid><pubDate>Tue, 27 Jan 2026 07:53:24 +0000</pubDate><dc:creator>Chen-Chou Lo</dc:creator><dc:creator>Patrick Vandewalle</dc:creator><dc:type>preprint</dc:type><prism:publicationName>arXiv</prism:publicationName><description>Accurate depth estimation is fundamental to 3D perception in autonomous driving, supporting tasks such as detection, tracking, and motion planning. However, monocular camera-based 3D detection suffers from depth ambiguity and reduced robustness under challenging conditions. Radar provides complementary advantages such as resilience to poor lighting and adverse weather, but its sparsity and low resolution limit its direct use in detection frameworks. This motivates the need for effective Radar-camera fusion with improved preprocessing and depth estimation strategies. We propose an end-to-end framework that enhances monocular 3D object detection through two key components. First, we introduce InstaRadar, an instance segmentation-guided expansion method that leverages pre-trained segmentation masks to enhance Radar density and semantic alignment, producing a more structured representation. InstaRadar achieves state-of-the-art results in Radar-guided depth estimation, showing its effectiveness in generating high-quality depth features. Second, we integrate the pre-trained RCDPT into the BEVDepth framework as a replacement for its depth module. With InstaRadar-enhanced inputs, the RCDPT integration consistently improves 3D detection performance. Overall, these components yield steady gains over the baseline BEVDepth model, demonstrating the effectiveness of InstaRadar and the advantage of explicit depth supervision in 3D object detection. Although the framework lags behind Radar-camera fusion models that directly extract BEV features, since Radar serves only as guidance rather than an independent feature stream, this limitation highlights potential for improvement. Future work will extend InstaRadar to point cloud-like representations and integrate a dedicated Radar branch with temporal cues for enhanced BEV fusion.
Published: 2026-01-27T07:53:24+00:00
Venue: arXiv
Score: 0.785 (must_read)</description><content:encoded>&lt;p&gt;&lt;strong&gt;Authors:&lt;/strong&gt; Chen-Chou Lo; Patrick Vandewalle&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Journal:&lt;/strong&gt; arXiv&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Relevance:&lt;/strong&gt; 0.785 (must_read)&lt;/p&gt;
&lt;p&gt;Accurate depth estimation is fundamental to 3D perception in autonomous driving, supporting tasks such as detection, tracking, and motion planning. However, monocular camera-based 3D detection suffers from depth ambiguity and reduced robustness under challenging conditions. Radar provides complementary advantages such as resilience to poor lighting and adverse weather, but its sparsity and low resolution limit its direct use in detection frameworks. This motivates the need for effective Radar-camera fusion with improved preprocessing and depth estimation strategies. We propose an end-to-end framework that enhances monocular 3D object detection through two key components. First, we introduce InstaRadar, an instance segmentation-guided expansion method that leverages pre-trained segmentation masks to enhance Radar density and semantic alignment, producing a more structured representation. InstaRadar achieves state-of-the-art results in Radar-guided depth estimation, showing its effectiveness in generating high-quality depth features. Second, we integrate the pre-trained RCDPT into the BEVDepth framework as a replacement for its depth module. With InstaRadar-enhanced inputs, the RCDPT integration consistently improves 3D detection performance. Overall, these components yield steady gains over the baseline BEVDepth model, demonstrating the effectiveness of InstaRadar and the advantage of explicit depth supervision in 3D object detection. Although the framework lags behind Radar-camera fusion models that directly extract BEV features, since Radar serves only as guidance rather than an independent feature stream, this limitation highlights potential for improvement. Future work will extend InstaRadar to point cloud-like representations and integrate a dedicated Radar branch with temporal cues for enhanced BEV fusion.&lt;/p&gt;</content:encoded></item><item><title>Goal-oriented Dynamic Weight Optimization for Multi-Object Navigation</title><link>https://doi.org/10.1109/tpami.2026.3657778</link><guid>10.1109/tpami.2026.3657778</guid><pubDate>Tue, 27 Jan 2026 05:48:07 +0000</pubDate><dc:creator>Haitao Zeng</dc:creator><dc:creator>Xinhang Song</dc:creator><dc:creator>Shuqiang Jiang</dc:creator><prism:publicationName>IEEE Transactions on Pattern Analysis and Machine Intelligence</prism:publicationName><prism:doi>10.1109/tpami.2026.3657778</prism:doi><description>Multi-object navigation (MON) tasks involve sequentially locating multiple targets in an unknown environment, requiring global long-term planning under incomplete information. This necessitates that the agent dynamically balance immediate actions and long-term rewards while considering both local adaptability and global foresight. However, current methods overly focus on local path optimization, which leads to slower convergence in sparse reward settings and increases the risk of deadlocks or trap states. The core challenge of MON lies in the deformation of the shared decision space, where independent optimization leads to redundant and overlapping paths. Thus, path planning requires dynamic, cross-task optimization rather than simple subtask aggregation. To minimize overall effort, the optimization process should adaptively balance task contributions through weight adjustment. Thus, we propose the Goal-oriented Dynamic Weight Optimization (GDWO) algorithm. GDWO integrates target-specific value loss functions into a unified optimization framework and dynamically adjusts weights through gradient-based updates. To prevent over-optimization, weights are normalized during training according to navigation success rates, prioritizing more challenging targets. This adaptive mechanism effectively addresses the challenge of sparse rewards and improves convergence efficiency. By leveraging this mechanism, GDWO unifies multiple objectives within a unified decision space, achieving efficient optimization and balancing short-term gains with long-term goals. Additionally, we introduce two auxiliary modules: prior knowledge-based navigation and frontier-aware exploration to further enhance GDWO's performance. Experimental results on the Gibson and Matterport3D datasets demonstrate that GDWO achieves improvements in key metrics for MON tasks. It optimizes path planning, reduces exploration costs, and enhances navigation efficiency, enabling the agent to perform tasks more effective...
Published: 2026-01-27T05:48:07+00:00
Venue: IEEE Transactions on Pattern Analysis and Machine Intelligence
Score: 0.783 (must_read)</description><content:encoded>&lt;p&gt;&lt;strong&gt;Authors:&lt;/strong&gt; Haitao Zeng; Xinhang Song; Shuqiang Jiang&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Journal:&lt;/strong&gt; IEEE Transactions on Pattern Analysis and Machine Intelligence&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;a href="https://doi.org/10.1109/tpami.2026.3657778"&gt;10.1109/tpami.2026.3657778&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Relevance:&lt;/strong&gt; 0.783 (must_read)&lt;/p&gt;
&lt;p&gt;Multi-object navigation (MON) tasks involve sequentially locating multiple targets in an unknown environment, requiring global long-term planning under incomplete information. This necessitates that the agent dynamically balance immediate actions and long-term rewards while considering both local adaptability and global foresight. However, current methods overly focus on local path optimization, which leads to slower convergence in sparse reward settings and increases the risk of deadlocks or trap states. The core challenge of MON lies in the deformation of the shared decision space, where independent optimization leads to redundant and overlapping paths. Thus, path planning requires dynamic, cross-task optimization rather than simple subtask aggregation. To minimize overall effort, the optimization process should adaptively balance task contributions through weight adjustment. Thus, we propose the Goal-oriented Dynamic Weight Optimization (GDWO) algorithm. GDWO integrates target-specific value loss functions into a unified optimization framework and dynamically adjusts weights through gradient-based updates. To prevent over-optimization, weights are normalized during training according to navigation success rates, prioritizing more challenging targets. This adaptive mechanism effectively addresses the challenge of sparse rewards and improves convergence efficiency. By leveraging this mechanism, GDWO unifies multiple objectives within a unified decision space, achieving efficient optimization and balancing short-term gains with long-term goals. Additionally, we introduce two auxiliary modules: prior knowledge-based navigation and frontier-aware exploration to further enhance GDWO&amp;#x27;s performance. Experimental results on the Gibson and Matterport3D datasets demonstrate that GDWO achieves improvements in key metrics for MON tasks. It optimizes path planning, reduces exploration costs, and enhances navigation efficiency, enabling the agent to perform tasks more effective...&lt;/p&gt;</content:encoded></item><item><title>Cross-Domain Transfer with Self-Supervised Spectral-Spatial Modeling for Hyperspectral Image Classification</title><link>https://arxiv.org/abs/2601.18088v1</link><guid>http://arxiv.org/abs/2601.18088v1</guid><pubDate>Mon, 26 Jan 2026 02:52:35 +0000</pubDate><dc:creator>Jianshu Chao</dc:creator><dc:creator>Tianhua Lv</dc:creator><dc:creator>Qiqiong Ma</dc:creator><dc:creator>Yunfei Qiu</dc:creator><dc:creator>Li Fang</dc:creator><dc:creator>Huifang Shen</dc:creator><dc:creator>Wei Yao</dc:creator><dc:type>preprint</dc:type><prism:publicationName>arXiv</prism:publicationName><description>Self-supervised learning has demonstrated considerable potential in hyperspectral representation, yet its application in cross-domain transfer scenarios remains under-explored. Existing methods, however, still rely on source domain annotations and are susceptible to distribution shifts, leading to degraded generalization performance in the target domain. To address this, this paper proposes a self-supervised cross-domain transfer framework that learns transferable spectral-spatial joint representations without source labels and achieves efficient adaptation under few samples in the target domain. During the self-supervised pre-training phase, a Spatial-Spectral Transformer (S2Former) module is designed. It adopts a dual-branch spatial-spectral transformer and introduces a bidirectional cross-attention mechanism to achieve spectral-spatial collaborative modeling: the spatial branch enhances structural awareness through random masking, while the spectral branch captures fine-grained differences. Both branches mutually guide each other to improve semantic consistency. We further propose a Frequency Domain Constraint (FDC) to maintain frequency-domain consistency through real Fast Fourier Transform (rFFT) and high-frequency magnitude loss, thereby enhancing the model's capability to discern fine details and boundaries. During the fine-tuning phase, we introduce a Diffusion-Aligned Fine-tuning (DAFT) distillation mechanism. This aligns semantic evolution trajectories through a teacher-student structure, enabling robust transfer learning under low-label conditions. Experimental results demonstrate stable classification performance and strong cross-domain adaptability across four hyperspectral datasets, validating the method's effectiveness under resource-constrained conditions.
Published: 2026-01-26T02:52:35+00:00
Venue: arXiv
Score: 0.782 (must_read)</description><content:encoded>&lt;p&gt;&lt;strong&gt;Authors:&lt;/strong&gt; Jianshu Chao; Tianhua Lv; Qiqiong Ma; Yunfei Qiu; Li Fang; Huifang Shen; Wei Yao&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Journal:&lt;/strong&gt; arXiv&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Relevance:&lt;/strong&gt; 0.782 (must_read)&lt;/p&gt;
&lt;p&gt;Self-supervised learning has demonstrated considerable potential in hyperspectral representation, yet its application in cross-domain transfer scenarios remains under-explored. Existing methods, however, still rely on source domain annotations and are susceptible to distribution shifts, leading to degraded generalization performance in the target domain. To address this, this paper proposes a self-supervised cross-domain transfer framework that learns transferable spectral-spatial joint representations without source labels and achieves efficient adaptation under few samples in the target domain. During the self-supervised pre-training phase, a Spatial-Spectral Transformer (S2Former) module is designed. It adopts a dual-branch spatial-spectral transformer and introduces a bidirectional cross-attention mechanism to achieve spectral-spatial collaborative modeling: the spatial branch enhances structural awareness through random masking, while the spectral branch captures fine-grained differences. Both branches mutually guide each other to improve semantic consistency. We further propose a Frequency Domain Constraint (FDC) to maintain frequency-domain consistency through real Fast Fourier Transform (rFFT) and high-frequency magnitude loss, thereby enhancing the model&amp;#x27;s capability to discern fine details and boundaries. During the fine-tuning phase, we introduce a Diffusion-Aligned Fine-tuning (DAFT) distillation mechanism. This aligns semantic evolution trajectories through a teacher-student structure, enabling robust transfer learning under low-label conditions. Experimental results demonstrate stable classification performance and strong cross-domain adaptability across four hyperspectral datasets, validating the method&amp;#x27;s effectiveness under resource-constrained conditions.&lt;/p&gt;</content:encoded></item><item><title>LatentMoE: Toward Optimal Accuracy per FLOP and Parameter in Mixture of Experts</title><link>https://arxiv.org/abs/2601.18089v1</link><guid>http://arxiv.org/abs/2601.18089v1</guid><pubDate>Mon, 26 Jan 2026 02:59:23 +0000</pubDate><dc:creator>Venmugil Elango</dc:creator><dc:creator>Nidhi Bhatia</dc:creator><dc:creator>Roger Waleffe</dc:creator><dc:creator>Rasoul Shafipour</dc:creator><dc:creator>Tomer Asida</dc:creator><dc:creator>Abhinav Khattar</dc:creator><dc:creator>Nave Assaf</dc:creator><dc:creator>Maximilian Golub</dc:creator><dc:creator>Joey Guman</dc:creator><dc:creator>Tiyasa Mitra</dc:creator><dc:creator>Ritchie Zhao</dc:creator><dc:creator>Ritika Borkar</dc:creator><dc:creator>Ran Zilberstein</dc:creator><dc:creator>Mostofa Patwary</dc:creator><dc:creator>Mohammad Shoeybi</dc:creator><dc:creator>Bita Rouhani</dc:creator><dc:type>preprint</dc:type><prism:publicationName>arXiv</prism:publicationName><description>Mixture of Experts (MoEs) have become a central component of many state-of-the-art open-source and proprietary large language models. Despite their widespread adoption, it remains unclear how close existing MoE architectures are to optimal with respect to inference cost, as measured by accuracy per floating-point operation and per parameter. In this work, we revisit MoE design from a hardware-software co-design perspective, grounded in empirical and theoretical considerations. We characterize key performance bottlenecks across diverse deployment regimes, spanning offline high-throughput execution and online, latency-critical inference. Guided by these insights, we introduce LatentMoE, a new model architecture resulting from systematic design exploration and optimized for maximal accuracy per unit of compute. Empirical design space exploration at scales of up to 95B parameters and over a 1T-token training horizon, together with supporting theoretical analysis, shows that LatentMoE consistently outperforms standard MoE architectures in terms of accuracy per FLOP and per parameter. Given its strong performance, the LatentMoE architecture has been adopted by the flagship Nemotron-3 Super and Ultra models and scaled to substantially larger regimes, including longer token horizons and larger model sizes, as reported in Nvidia et al. (arXiv:2512.20856).
Published: 2026-01-26T02:59:23+00:00
Venue: arXiv
Score: 0.782 (must_read)</description><content:encoded>&lt;p&gt;&lt;strong&gt;Authors:&lt;/strong&gt; Venmugil Elango; Nidhi Bhatia; Roger Waleffe; Rasoul Shafipour; Tomer Asida; Abhinav Khattar; Nave Assaf; Maximilian Golub; Joey Guman; Tiyasa Mitra; Ritchie Zhao; Ritika Borkar; Ran Zilberstein; Mostofa Patwary; Mohammad Shoeybi; Bita Rouhani&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Journal:&lt;/strong&gt; arXiv&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Relevance:&lt;/strong&gt; 0.782 (must_read)&lt;/p&gt;
&lt;p&gt;Mixture of Experts (MoEs) have become a central component of many state-of-the-art open-source and proprietary large language models. Despite their widespread adoption, it remains unclear how close existing MoE architectures are to optimal with respect to inference cost, as measured by accuracy per floating-point operation and per parameter. In this work, we revisit MoE design from a hardware-software co-design perspective, grounded in empirical and theoretical considerations. We characterize key performance bottlenecks across diverse deployment regimes, spanning offline high-throughput execution and online, latency-critical inference. Guided by these insights, we introduce LatentMoE, a new model architecture resulting from systematic design exploration and optimized for maximal accuracy per unit of compute. Empirical design space exploration at scales of up to 95B parameters and over a 1T-token training horizon, together with supporting theoretical analysis, shows that LatentMoE consistently outperforms standard MoE architectures in terms of accuracy per FLOP and per parameter. Given its strong performance, the LatentMoE architecture has been adopted by the flagship Nemotron-3 Super and Ultra models and scaled to substantially larger regimes, including longer token horizons and larger model sizes, as reported in Nvidia et al. (arXiv:2512.20856).&lt;/p&gt;</content:encoded></item><item><title>$\infty$-MoE: Generalizing Mixture of Experts to Infinite Experts</title><link>https://arxiv.org/abs/2601.17680v1</link><guid>http://arxiv.org/abs/2601.17680v1</guid><pubDate>Sun, 25 Jan 2026 03:55:51 +0000</pubDate><dc:creator>Shota Takashiro</dc:creator><dc:creator>Takeshi Kojima</dc:creator><dc:creator>Shohei Taniguchi</dc:creator><dc:creator>Yusuke Iwasawa</dc:creator><dc:creator>Yutaka Matsuo</dc:creator><dc:type>preprint</dc:type><prism:publicationName>arXiv</prism:publicationName><description>The Mixture of Experts (MoE) selects a few feed-forward networks (FFNs) per token, achieving an effective trade-off between computational cost and performance. In conventional MoE, each expert is treated as entirely independent, and experts are combined in a discrete space. As a result, when the number of experts increases, it becomes difficult to train each expert effectively. To stabilize training while increasing the number of experts, we propose $\infty$-MoE that selects a portion of the parameters of large FFNs based on continuous values sampled for each token. By considering experts in a continuous space, this approach allows for an infinite number of experts while maintaining computational efficiency. Experiments show that a GPT-2 Small-based $\infty$-MoE model, with 129M active and 186M total parameters, achieves comparable performance to a dense GPT-2 Medium with 350M parameters. Adjusting the number of sampled experts at inference time allows for a flexible trade-off between accuracy and speed, with an improvement of up to 2.5\% in accuracy over conventional MoE.
Published: 2026-01-25T03:55:51+00:00
Venue: arXiv
Score: 0.782 (must_read)</description><content:encoded>&lt;p&gt;&lt;strong&gt;Authors:&lt;/strong&gt; Shota Takashiro; Takeshi Kojima; Shohei Taniguchi; Yusuke Iwasawa; Yutaka Matsuo&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Journal:&lt;/strong&gt; arXiv&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Relevance:&lt;/strong&gt; 0.782 (must_read)&lt;/p&gt;
&lt;p&gt;The Mixture of Experts (MoE) selects a few feed-forward networks (FFNs) per token, achieving an effective trade-off between computational cost and performance. In conventional MoE, each expert is treated as entirely independent, and experts are combined in a discrete space. As a result, when the number of experts increases, it becomes difficult to train each expert effectively. To stabilize training while increasing the number of experts, we propose $\infty$-MoE that selects a portion of the parameters of large FFNs based on continuous values sampled for each token. By considering experts in a continuous space, this approach allows for an infinite number of experts while maintaining computational efficiency. Experiments show that a GPT-2 Small-based $\infty$-MoE model, with 129M active and 186M total parameters, achieves comparable performance to a dense GPT-2 Medium with 350M parameters. Adjusting the number of sampled experts at inference time allows for a flexible trade-off between accuracy and speed, with an improvement of up to 2.5\% in accuracy over conventional MoE.&lt;/p&gt;</content:encoded></item><item><title>Adaptive Domain Shift in Diffusion Models for Cross-Modality Image Translation</title><link>https://arxiv.org/abs/2601.18623v1</link><guid>http://arxiv.org/abs/2601.18623v1</guid><pubDate>Mon, 26 Jan 2026 16:00:36 +0000</pubDate><dc:creator>Zihao Wang</dc:creator><dc:creator>Yuzhou Chen</dc:creator><dc:creator>Shaogang Ren</dc:creator><dc:type>preprint</dc:type><prism:publicationName>arXiv</prism:publicationName><description>Cross-modal image translation remains brittle and inefficient. Standard diffusion approaches often rely on a single, global linear transfer between domains. We find that this shortcut forces the sampler to traverse off-manifold, high-cost regions, inflating the correction burden and inviting semantic drift. We refer to this shared failure mode as fixed-schedule domain transfer. In this paper, we embed domain-shift dynamics directly into the generative process. Our model predicts a spatially varying mixing field at every reverse step and injects an explicit, target-consistent restoration term into the drift. This in-step guidance keeps large updates on-manifold and shifts the model's role from global alignment to local residual correction. We provide a continuous-time formulation with an exact solution form and derive a practical first-order sampler that preserves marginal consistency. Empirically, across translation tasks in medical imaging, remote sensing, and electroluminescence semantic mapping, our framework improves structural fidelity and semantic consistency while converging in fewer denoising steps.
Published: 2026-01-26T16:00:36+00:00
Venue: arXiv
Score: 0.779 (must_read)</description><content:encoded>&lt;p&gt;&lt;strong&gt;Authors:&lt;/strong&gt; Zihao Wang; Yuzhou Chen; Shaogang Ren&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Journal:&lt;/strong&gt; arXiv&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Relevance:&lt;/strong&gt; 0.779 (must_read)&lt;/p&gt;
&lt;p&gt;Cross-modal image translation remains brittle and inefficient. Standard diffusion approaches often rely on a single, global linear transfer between domains. We find that this shortcut forces the sampler to traverse off-manifold, high-cost regions, inflating the correction burden and inviting semantic drift. We refer to this shared failure mode as fixed-schedule domain transfer. In this paper, we embed domain-shift dynamics directly into the generative process. Our model predicts a spatially varying mixing field at every reverse step and injects an explicit, target-consistent restoration term into the drift. This in-step guidance keeps large updates on-manifold and shifts the model&amp;#x27;s role from global alignment to local residual correction. We provide a continuous-time formulation with an exact solution form and derive a practical first-order sampler that preserves marginal consistency. Empirically, across translation tasks in medical imaging, remote sensing, and electroluminescence semantic mapping, our framework improves structural fidelity and semantic consistency while converging in fewer denoising steps.&lt;/p&gt;</content:encoded></item><item><title>PGSC: A Gradient Sparsification Communication Optimization Criterion for Nonequilibrium Thermodynamics</title><link>https://doi.org/10.1016/j.inffus.2026.104188</link><guid>10.1016/j.inffus.2026.104188</guid><pubDate>Tue, 27 Jan 2026 07:48:48 +0000</pubDate><dc:creator>Wenlong Zhang</dc:creator><dc:creator>Ying Li</dc:creator><dc:creator>Hanhan Du</dc:creator><dc:creator>Yan Wei</dc:creator><dc:creator>Aiqing Fang</dc:creator><prism:publicationName>Information Fusion</prism:publicationName><prism:doi>10.1016/j.inffus.2026.104188</prism:doi><description>Gradient compression can reduce communication overhead. However, current static sparsity techniques may disturb gradient dynamics, resulting in unstable model convergence and reduced feature discriminative ability, whereas transmitting the complete gradient leads to high costs. To address this issue, inspired by nonequilibrium thermodynamics, this paper proposes a Physics-guided Gradient Sparsification Criterion (PGSC). Specifically, we formulate a continuous field equation based on the gradient magnitude distribution, deriving an adaptive decay rule for the sparsification threshold during the training phase. We then dynamically adjust the sparsification threshold according to this rule, effectively addressing the complexity of multimodal features and ensuring consistent information transmission. Our method achieves adaptive co-optimization of gradient compression and model accuracy by establishing a dynamic equilibrium mechanism between gradient dissipation and information entropy. This approach ensures stable convergence rates while preserving the gradient structure of multi-scale features. Extensive experiments on public datasets, including CIFAR-10, MNIST, and FLIR_ADAS_v2, demonstrate significant advantages over competitors such as TopK and quantization compression, while also reducing communication costs.
Published: 2026-01-27T07:48:48+00:00
Venue: Information Fusion
Score: 0.779 (must_read)</description><content:encoded>&lt;p&gt;&lt;strong&gt;Authors:&lt;/strong&gt; Wenlong Zhang; Ying Li; Hanhan Du; Yan Wei; Aiqing Fang&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Journal:&lt;/strong&gt; Information Fusion&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;a href="https://doi.org/10.1016/j.inffus.2026.104188"&gt;10.1016/j.inffus.2026.104188&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Relevance:&lt;/strong&gt; 0.779 (must_read)&lt;/p&gt;
&lt;p&gt;Gradient compression can reduce communication overhead. However, current static sparsity techniques may disturb gradient dynamics, resulting in unstable model convergence and reduced feature discriminative ability, whereas transmitting the complete gradient leads to high costs. To address this issue, inspired by nonequilibrium thermodynamics, this paper proposes a Physics-guided Gradient Sparsification Criterion (PGSC). Specifically, we formulate a continuous field equation based on the gradient magnitude distribution, deriving an adaptive decay rule for the sparsification threshold during the training phase. We then dynamically adjust the sparsification threshold according to this rule, effectively addressing the complexity of multimodal features and ensuring consistent information transmission. Our method achieves adaptive co-optimization of gradient compression and model accuracy by establishing a dynamic equilibrium mechanism between gradient dissipation and information entropy. This approach ensures stable convergence rates while preserving the gradient structure of multi-scale features. Extensive experiments on public datasets, including CIFAR-10, MNIST, and FLIR_ADAS_v2, demonstrate significant advantages over competitors such as TopK and quantization compression, while also reducing communication costs.&lt;/p&gt;</content:encoded></item><item><title>A novel framework for marine oil spill detection in SAR imagery fusing edge supervision enhancement and group attention mechanism</title><link>https://doi.org/10.1016/j.rsase.2026.101901</link><guid>10.1016/j.rsase.2026.101901</guid><pubDate>Tue, 27 Jan 2026 07:53:53 +0000</pubDate><dc:creator>Xinrong Lyu</dc:creator><dc:creator>Haosha Su</dc:creator><dc:creator>Christos Grecos</dc:creator><dc:creator>Peng Ren</dc:creator><prism:publicationName>Remote Sensing Applications: Society and Environment</prism:publicationName><prism:doi>10.1016/j.rsase.2026.101901</prism:doi><description>Rapid and accurate detection of marine oil spills is crucial for environmental protection and emergency response. Synthetic Aperture Radar (SAR), a primary tool for sea surface oil spill monitoring, faces persistent challenges such as varying spill scales, blurred boundaries, and confusion with look-alike phenomena. To address these issues, this study proposes OilSeg-SARNet, a novel architecture tailored for SAR oil spill detection. The model incorporates a Group Convolutional Block Attention Module Enhancer to emphasize salient features and suppress background noise, an Atrous Spatial Pyramid Pooling module to capture multi-scale contextual information, and an improved Edge Supervision Enhancement Module to refine boundary representation and facilitate gradient propagation. These components work synergistically to enhance detection precision under complex marine conditions. Experimental results on the public SAR Oil Spill Detection Dataset demonstrate that OilSeg-SARNet achieves class-specific Intersection-over-Unions (IoUs) of 61.33%, 64.86%, and 45.10% for oil spill, look-alike, and ship categories, respectively, outperforming the best prior method by +0.85%, +3.73%, and +9.89%, respectively. The model attains an overall mean IoU (mIoU) of 72.22% and an F 1 " role="presentation"&gt; 1 1 -score of 79.33%. The proposed model surpasses existing methods with reduced complexity, offering a reliable and efficient framework for marine oil spill monitoring, thereby enhancing early detection and supporting timely environmental response.
Published: 2026-01-27T07:53:53+00:00
Venue: Remote Sensing Applications: Society and Environment
Score: 0.778 (must_read)</description><content:encoded>&lt;p&gt;&lt;strong&gt;Authors:&lt;/strong&gt; Xinrong Lyu; Haosha Su; Christos Grecos; Peng Ren&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Journal:&lt;/strong&gt; Remote Sensing Applications: Society and Environment&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;a href="https://doi.org/10.1016/j.rsase.2026.101901"&gt;10.1016/j.rsase.2026.101901&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Relevance:&lt;/strong&gt; 0.778 (must_read)&lt;/p&gt;
&lt;p&gt;Rapid and accurate detection of marine oil spills is crucial for environmental protection and emergency response. Synthetic Aperture Radar (SAR), a primary tool for sea surface oil spill monitoring, faces persistent challenges such as varying spill scales, blurred boundaries, and confusion with look-alike phenomena. To address these issues, this study proposes OilSeg-SARNet, a novel architecture tailored for SAR oil spill detection. The model incorporates a Group Convolutional Block Attention Module Enhancer to emphasize salient features and suppress background noise, an Atrous Spatial Pyramid Pooling module to capture multi-scale contextual information, and an improved Edge Supervision Enhancement Module to refine boundary representation and facilitate gradient propagation. These components work synergistically to enhance detection precision under complex marine conditions. Experimental results on the public SAR Oil Spill Detection Dataset demonstrate that OilSeg-SARNet achieves class-specific Intersection-over-Unions (IoUs) of 61.33%, 64.86%, and 45.10% for oil spill, look-alike, and ship categories, respectively, outperforming the best prior method by +0.85%, +3.73%, and +9.89%, respectively. The model attains an overall mean IoU (mIoU) of 72.22% and an F 1 &amp;quot; role=&amp;quot;presentation&amp;quot;&amp;gt; 1 1 -score of 79.33%. The proposed model surpasses existing methods with reduced complexity, offering a reliable and efficient framework for marine oil spill monitoring, thereby enhancing early detection and supporting timely environmental response.&lt;/p&gt;</content:encoded></item><item><title>Source-Free Domain Adaptation by Optimizing Batch-Wise Cosine Similarity</title><link>https://arxiv.org/abs/2601.17408v1</link><guid>http://arxiv.org/abs/2601.17408v1</guid><pubDate>Sat, 24 Jan 2026 10:51:25 +0000</pubDate><dc:creator>Harsharaj Pathak</dc:creator><dc:creator>Vineeth N Balasubramanian</dc:creator><dc:type>preprint</dc:type><prism:publicationName>arXiv</prism:publicationName><prism:doi>10.5281/zenodo.17767092</prism:doi><description>Source-Free Domain Adaptation (SFDA) is an emerging area of research that aims to adapt a model trained on a labeled source domain to an unlabeled target domain without accessing the source data. Most of the successful methods in this area rely on the concept of neighborhood consistency but are prone to errors due to misleading neighborhood information. In this paper, we explore this approach from the point of view of learning more informative clusters and mitigating the effect of noisy neighbors using a concept called neighborhood signature, and demonstrate that adaptation can be achieved using just a single loss term tailored to optimize the similarity and dissimilarity of predictions of samples in the target domain. In particular, our proposed method outperforms existing methods in the challenging VisDA dataset while also yielding competitive results on other benchmark datasets.
Published: 2026-01-24T10:51:25+00:00
Venue: arXiv
Score: 0.778 (must_read)</description><content:encoded>&lt;p&gt;&lt;strong&gt;Authors:&lt;/strong&gt; Harsharaj Pathak; Vineeth N Balasubramanian&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Journal:&lt;/strong&gt; arXiv&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;a href="https://doi.org/10.5281/zenodo.17767092"&gt;10.5281/zenodo.17767092&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Relevance:&lt;/strong&gt; 0.778 (must_read)&lt;/p&gt;
&lt;p&gt;Source-Free Domain Adaptation (SFDA) is an emerging area of research that aims to adapt a model trained on a labeled source domain to an unlabeled target domain without accessing the source data. Most of the successful methods in this area rely on the concept of neighborhood consistency but are prone to errors due to misleading neighborhood information. In this paper, we explore this approach from the point of view of learning more informative clusters and mitigating the effect of noisy neighbors using a concept called neighborhood signature, and demonstrate that adaptation can be achieved using just a single loss term tailored to optimize the similarity and dissimilarity of predictions of samples in the target domain. In particular, our proposed method outperforms existing methods in the challenging VisDA dataset while also yielding competitive results on other benchmark datasets.&lt;/p&gt;</content:encoded></item><item><title>Persistent Scatterers Detection supported by Deep Learning: a Solution Based on U-Net</title><link>https://doi.org/10.1109/tgrs.2026.3657993</link><guid>10.1109/tgrs.2026.3657993</guid><pubDate>Tue, 27 Jan 2026 05:48:56 +0000</pubDate><dc:creator>Weili Tang</dc:creator><dc:creator>Sergio Vitale</dc:creator><dc:creator>Simona Verde</dc:creator><dc:creator>Gianfranco Fornaro</dc:creator><prism:publicationName>IEEE Transactions on Geoscience and Remote Sensing</prism:publicationName><prism:doi>10.1109/tgrs.2026.3657993</prism:doi><description>Multitemporal Differential Interferometric Synthetic Aperture Radar (MT-DInSAR) allows accurate and longterm monitoring of displacements of ground Persistent Scatterers (PS). PS are typically detected using suitable statistical tests, built to strictly control the probability of false alarm. At full resolution, this detection strategy can lead to the rejection of PS characterized by spatial consistency of the estimated parameters. Reducing the density of PS measurements can impact the interpretation of the results. In this work we investigate the integration of a Deep-Learning (DL) solution, specifically U-Net, at the stage of PS detection. A three stream U-Net is proposed to replace the typical thresholding of the classical statistical indicators. Results on simulated data and on data acquired by the sensors of the COSMO-SkyMed (CSK) and COSMO-SkyMed Second Generation (CSG) constellation, demonstrate the superior performances of the proposed DL- PS detection scheme over the classical one.
Published: 2026-01-27T05:48:56+00:00
Venue: IEEE Transactions on Geoscience and Remote Sensing
Score: 0.778 (must_read)</description><content:encoded>&lt;p&gt;&lt;strong&gt;Authors:&lt;/strong&gt; Weili Tang; Sergio Vitale; Simona Verde; Gianfranco Fornaro&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Journal:&lt;/strong&gt; IEEE Transactions on Geoscience and Remote Sensing&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;a href="https://doi.org/10.1109/tgrs.2026.3657993"&gt;10.1109/tgrs.2026.3657993&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Relevance:&lt;/strong&gt; 0.778 (must_read)&lt;/p&gt;
&lt;p&gt;Multitemporal Differential Interferometric Synthetic Aperture Radar (MT-DInSAR) allows accurate and longterm monitoring of displacements of ground Persistent Scatterers (PS). PS are typically detected using suitable statistical tests, built to strictly control the probability of false alarm. At full resolution, this detection strategy can lead to the rejection of PS characterized by spatial consistency of the estimated parameters. Reducing the density of PS measurements can impact the interpretation of the results. In this work we investigate the integration of a Deep-Learning (DL) solution, specifically U-Net, at the stage of PS detection. A three stream U-Net is proposed to replace the typical thresholding of the classical statistical indicators. Results on simulated data and on data acquired by the sensors of the COSMO-SkyMed (CSK) and COSMO-SkyMed Second Generation (CSG) constellation, demonstrate the superior performances of the proposed DL- PS detection scheme over the classical one.&lt;/p&gt;</content:encoded></item><item><title>YOLO-DS: Fine-Grained Feature Decoupling via Dual-Statistic Synergy Operator for Object Detection</title><link>https://arxiv.org/abs/2601.18172v1</link><guid>http://arxiv.org/abs/2601.18172v1</guid><pubDate>Mon, 26 Jan 2026 05:50:32 +0000</pubDate><dc:creator>Lin Huang</dc:creator><dc:creator>Yujuan Tan</dc:creator><dc:creator>Weisheng Li</dc:creator><dc:creator>Shitai Shan</dc:creator><dc:creator>Liu Liu</dc:creator><dc:creator>Bo Liu</dc:creator><dc:creator>Linlin Shen</dc:creator><dc:creator>Jing Yu</dc:creator><dc:creator>Yue Niu</dc:creator><dc:type>preprint</dc:type><prism:publicationName>arXiv</prism:publicationName><description>One-stage object detection, particularly the YOLO series, strikes a favorable balance between accuracy and efficiency. However, existing YOLO detectors lack explicit modeling of heterogeneous object responses within shared feature channels, which limits further performance gains. To address this, we propose YOLO-DS, a framework built around a novel Dual-Statistic Synergy Operator (DSO). The DSO decouples object features by jointly modeling the channel-wise mean and the peak-to-mean difference. Building upon the DSO, we design two lightweight gating modules: the Dual-Statistic Synergy Gating (DSG) module for adaptive channel-wise feature selection, and the Multi-Path Segmented Gating (MSG) module for depth-wise feature weighting. On the MS-COCO benchmark, YOLO-DS consistently outperforms YOLOv8 across five model scales (N, S, M, L, X), achieving AP gains of 1.1% to 1.7% with only a minimal increase in inference latency. Extensive visualization, ablation, and comparative studies validate the effectiveness of our approach, demonstrating its superior capability in discriminating heterogeneous objects with high efficiency.
Published: 2026-01-26T05:50:32+00:00
Venue: arXiv
Score: 0.778 (must_read)</description><content:encoded>&lt;p&gt;&lt;strong&gt;Authors:&lt;/strong&gt; Lin Huang; Yujuan Tan; Weisheng Li; Shitai Shan; Liu Liu; Bo Liu; Linlin Shen; Jing Yu; Yue Niu&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Journal:&lt;/strong&gt; arXiv&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Relevance:&lt;/strong&gt; 0.778 (must_read)&lt;/p&gt;
&lt;p&gt;One-stage object detection, particularly the YOLO series, strikes a favorable balance between accuracy and efficiency. However, existing YOLO detectors lack explicit modeling of heterogeneous object responses within shared feature channels, which limits further performance gains. To address this, we propose YOLO-DS, a framework built around a novel Dual-Statistic Synergy Operator (DSO). The DSO decouples object features by jointly modeling the channel-wise mean and the peak-to-mean difference. Building upon the DSO, we design two lightweight gating modules: the Dual-Statistic Synergy Gating (DSG) module for adaptive channel-wise feature selection, and the Multi-Path Segmented Gating (MSG) module for depth-wise feature weighting. On the MS-COCO benchmark, YOLO-DS consistently outperforms YOLOv8 across five model scales (N, S, M, L, X), achieving AP gains of 1.1% to 1.7% with only a minimal increase in inference latency. Extensive visualization, ablation, and comparative studies validate the effectiveness of our approach, demonstrating its superior capability in discriminating heterogeneous objects with high efficiency.&lt;/p&gt;</content:encoded></item><item><title>ProMoT: Progressive Prompting of Modality and Temporal Dynamics for RGB-T Tracking</title><link>https://doi.org/10.1109/tcsvt.2026.3657773</link><guid>10.1109/tcsvt.2026.3657773</guid><pubDate>Tue, 27 Jan 2026 06:05:44 +0000</pubDate><dc:creator>Jia Chen</dc:creator><dc:creator>Rui Xu</dc:creator><dc:creator>Si Chen</dc:creator><dc:creator>Yuzhen Niu</dc:creator><dc:creator>Yan Yan</dc:creator><dc:creator>Da-Han Wang</dc:creator><prism:publicationName>IEEE Transactions on Circuits and Systems for Video Technology</prism:publicationName><prism:doi>10.1109/tcsvt.2026.3657773</prism:doi><description>RGB-T tracking benefits from the complementary nature of RGB and TIR modalities, yet their relative reliability for target localization often shifts over time. Most existing trackers fail to adapt to such modality and temporal dynamics in a unified and effective manner, resulting in target representations that are neither discriminative nor temporally consistent. In this paper, we propose ProMoT, a novel tracking framework that jointly integrates cross-modal and temporal cues into a progressive prompting process, enabling continuous retrieval of target-aware representations. Specifically, we design an adaptive target query generator (QueryGen), which selectively aggregates informative spatio-temporal cues from diverse ghost representations through the dynamic sparse ghost fusion mechanism, thereby enabling the generation of target-aware queries. To further preserve fine-grained, temporally consistent target cues, we introduce a high-order contextual prompt updater (PromptUpdater), which encodes high-order cross-modal representations from current and previous frames. These prompts establish the compact and discriminative inter-frame context to not only refine the current frame’s features but also guide target localization in future frames. All components are built upon a parameter-shared backbone for RGB and TIR inputs, forming our complete ProMoT framework. Extensive experiments on both complete and missing modality RGB-T tracking benchmarks show that ProMoT consistently achieves state-of-the-art performance while balancing efficiency.
Published: 2026-01-27T06:05:44+00:00
Venue: IEEE Transactions on Circuits and Systems for Video Technology
Score: 0.777 (must_read)</description><content:encoded>&lt;p&gt;&lt;strong&gt;Authors:&lt;/strong&gt; Jia Chen; Rui Xu; Si Chen; Yuzhen Niu; Yan Yan; Da-Han Wang&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Journal:&lt;/strong&gt; IEEE Transactions on Circuits and Systems for Video Technology&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;a href="https://doi.org/10.1109/tcsvt.2026.3657773"&gt;10.1109/tcsvt.2026.3657773&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Relevance:&lt;/strong&gt; 0.777 (must_read)&lt;/p&gt;
&lt;p&gt;RGB-T tracking benefits from the complementary nature of RGB and TIR modalities, yet their relative reliability for target localization often shifts over time. Most existing trackers fail to adapt to such modality and temporal dynamics in a unified and effective manner, resulting in target representations that are neither discriminative nor temporally consistent. In this paper, we propose ProMoT, a novel tracking framework that jointly integrates cross-modal and temporal cues into a progressive prompting process, enabling continuous retrieval of target-aware representations. Specifically, we design an adaptive target query generator (QueryGen), which selectively aggregates informative spatio-temporal cues from diverse ghost representations through the dynamic sparse ghost fusion mechanism, thereby enabling the generation of target-aware queries. To further preserve fine-grained, temporally consistent target cues, we introduce a high-order contextual prompt updater (PromptUpdater), which encodes high-order cross-modal representations from current and previous frames. These prompts establish the compact and discriminative inter-frame context to not only refine the current frame’s features but also guide target localization in future frames. All components are built upon a parameter-shared backbone for RGB and TIR inputs, forming our complete ProMoT framework. Extensive experiments on both complete and missing modality RGB-T tracking benchmarks show that ProMoT consistently achieves state-of-the-art performance while balancing efficiency.&lt;/p&gt;</content:encoded></item><item><title>Implicit Non-Causal Factors are Out via Dataset Splitting for Domain Generalization Object Detection</title><link>https://arxiv.org/abs/2601.19127v1</link><guid>http://arxiv.org/abs/2601.19127v1</guid><pubDate>Tue, 27 Jan 2026 02:52:13 +0000</pubDate><dc:creator>Zhilong Zhang</dc:creator><dc:creator>Lei Zhang</dc:creator><dc:creator>Qing He</dc:creator><dc:creator>Shuyin Xia</dc:creator><dc:creator>Guoyin Wang</dc:creator><dc:creator>Fuxiang Huang</dc:creator><dc:type>preprint</dc:type><prism:publicationName>arXiv</prism:publicationName><description>Open world object detection faces a significant challenge in domain-invariant representation, i.e., implicit non-causal factors. Most domain generalization (DG) methods based on domain adversarial learning (DAL) pay much attention to learn domain-invariant information, but often overlook the potential non-causal factors. We unveil two critical causes: 1) The domain discriminator-based DAL method is subject to the extremely sparse domain label, i.e., assigning only one domain label to each dataset, thus can only associate explicit non-causal factor, which is incredibly limited. 2) The non-causal factors, induced by unidentified data bias, are excessively implicit and cannot be solely discerned by conventional DAL paradigm. Based on these key findings, inspired by the Granular-Ball perspective, we propose an improved DAL method, i.e., GB-DAL. The proposed GB-DAL utilizes Prototype-based Granular Ball Splitting (PGBS) module to generate more dense domains from limited datasets, akin to more fine-grained granular balls, indicating more potential non-causal factors. Inspired by adversarial perturbations akin to non-causal factors, we propose a Simulated Non-causal Factors (SNF) module as a means of data augmentation to reduce the implicitness of non-causal factors, and facilitate the training of GB-DAL. Comparative experiments on numerous benchmarks demonstrate that our method achieves better generalization performance in novel circumstances.
Published: 2026-01-27T02:52:13+00:00
Venue: arXiv
Score: 0.777 (must_read)</description><content:encoded>&lt;p&gt;&lt;strong&gt;Authors:&lt;/strong&gt; Zhilong Zhang; Lei Zhang; Qing He; Shuyin Xia; Guoyin Wang; Fuxiang Huang&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Journal:&lt;/strong&gt; arXiv&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Relevance:&lt;/strong&gt; 0.777 (must_read)&lt;/p&gt;
&lt;p&gt;Open world object detection faces a significant challenge in domain-invariant representation, i.e., implicit non-causal factors. Most domain generalization (DG) methods based on domain adversarial learning (DAL) pay much attention to learn domain-invariant information, but often overlook the potential non-causal factors. We unveil two critical causes: 1) The domain discriminator-based DAL method is subject to the extremely sparse domain label, i.e., assigning only one domain label to each dataset, thus can only associate explicit non-causal factor, which is incredibly limited. 2) The non-causal factors, induced by unidentified data bias, are excessively implicit and cannot be solely discerned by conventional DAL paradigm. Based on these key findings, inspired by the Granular-Ball perspective, we propose an improved DAL method, i.e., GB-DAL. The proposed GB-DAL utilizes Prototype-based Granular Ball Splitting (PGBS) module to generate more dense domains from limited datasets, akin to more fine-grained granular balls, indicating more potential non-causal factors. Inspired by adversarial perturbations akin to non-causal factors, we propose a Simulated Non-causal Factors (SNF) module as a means of data augmentation to reduce the implicitness of non-causal factors, and facilitate the training of GB-DAL. Comparative experiments on numerous benchmarks demonstrate that our method achieves better generalization performance in novel circumstances.&lt;/p&gt;</content:encoded></item><item><title>R^3: Replay, Reflection, and Ranking Rewards for LLM Reinforcement Learning</title><link>https://arxiv.org/abs/2601.19620v1</link><guid>http://arxiv.org/abs/2601.19620v1</guid><pubDate>Tue, 27 Jan 2026 13:55:34 +0000</pubDate><dc:creator>Zhizheng Jiang</dc:creator><dc:creator>Kang Zhao</dc:creator><dc:creator>Weikai Xu</dc:creator><dc:creator>Xinkui Lin</dc:creator><dc:creator>Wei Liu</dc:creator><dc:creator>Jian Luan</dc:creator><dc:creator>Shuo Shang</dc:creator><dc:creator>Peng Han</dc:creator><dc:type>preprint</dc:type><prism:publicationName>arXiv</prism:publicationName><description>Large reasoning models (LRMs) aim to solve diverse and complex problems through structured reasoning. Recent advances in group-based policy optimization methods have shown promise in enabling stable advantage estimation without reliance on process-level annotations. However, these methods rely on advantage gaps induced by high-quality samples within the same batch, which makes the training process fragile and inefficient when intra-group advantages collapse under challenging tasks. To address these problems, we propose a reinforcement learning mechanism named \emph{\textbf{R^3}} that along three directions: (1) a \emph{cross-context \underline{\textbf{R}}eplay} strategy that maintains the intra-group advantage by recalling valuable examples from historical trajectories of the same query, (2) an \emph{in-context self-\underline{\textbf{R}}eflection} mechanism enabling models to refine outputs by leveraging past failures, and (3) a \emph{structural entropy \underline{\textbf{R}}anking reward}, which assigns relative rewards to truncated or failed samples by ranking responses based on token-level entropy patterns, capturing both local exploration and global stability. We implement our method on Deepseek-R1-Distill-Qwen-1.5B and train it on the DeepscaleR-40k in the math domain. Experiments demonstrate our method achieves SoTA performance on several math benchmarks, representing significant improvements and fewer reasoning tokens over the base models. Code and model will be released.
Published: 2026-01-27T13:55:34+00:00
Venue: arXiv
Score: 0.776 (must_read)</description><content:encoded>&lt;p&gt;&lt;strong&gt;Authors:&lt;/strong&gt; Zhizheng Jiang; Kang Zhao; Weikai Xu; Xinkui Lin; Wei Liu; Jian Luan; Shuo Shang; Peng Han&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Journal:&lt;/strong&gt; arXiv&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Relevance:&lt;/strong&gt; 0.776 (must_read)&lt;/p&gt;
&lt;p&gt;Large reasoning models (LRMs) aim to solve diverse and complex problems through structured reasoning. Recent advances in group-based policy optimization methods have shown promise in enabling stable advantage estimation without reliance on process-level annotations. However, these methods rely on advantage gaps induced by high-quality samples within the same batch, which makes the training process fragile and inefficient when intra-group advantages collapse under challenging tasks. To address these problems, we propose a reinforcement learning mechanism named \emph{\textbf{R^3}} that along three directions: (1) a \emph{cross-context \underline{\textbf{R}}eplay} strategy that maintains the intra-group advantage by recalling valuable examples from historical trajectories of the same query, (2) an \emph{in-context self-\underline{\textbf{R}}eflection} mechanism enabling models to refine outputs by leveraging past failures, and (3) a \emph{structural entropy \underline{\textbf{R}}anking reward}, which assigns relative rewards to truncated or failed samples by ranking responses based on token-level entropy patterns, capturing both local exploration and global stability. We implement our method on Deepseek-R1-Distill-Qwen-1.5B and train it on the DeepscaleR-40k in the math domain. Experiments demonstrate our method achieves SoTA performance on several math benchmarks, representing significant improvements and fewer reasoning tokens over the base models. Code and model will be released.&lt;/p&gt;</content:encoded></item><item><title>Proposing and solving olympiad geometry with guided tree search</title><link>https://doi.org/10.1038/s42256-025-01164-x</link><guid>10.1038/s42256-025-01164-x</guid><pubDate>Mon, 26 Jan 2026 10:02:30 +0000</pubDate><dc:creator>Chi Zhang</dc:creator><dc:creator>Jiajun Song</dc:creator><dc:creator>Siyu Li</dc:creator><dc:creator>Yitao Liang</dc:creator><dc:creator>Yuxi Ma</dc:creator><dc:creator>Wei Wang</dc:creator><dc:creator>Yixin Zhu</dc:creator><dc:creator>Song-Chun Zhu</dc:creator><prism:publicationName>Nature Machine Intelligence</prism:publicationName><prism:doi>10.1038/s42256-025-01164-x</prism:doi><description>Mathematics olympiads are prestigious competitions in which both proposing and solving problems are highly honoured. Building artificial intelligence systems capable of addressing these olympiad-level challenges remains an open frontier in automated reasoning, particularly in geometry due to its unique blend of numerical precision and spatial intuition. Here we show that TongGeometry, a neuro-symbolic system using guided tree search, both discovers and proves olympiad-level geometry theorems. Within the same computational budget as existing state-of-the-art systems, TongGeometry establishes a larger repository of geometry theorems: 6.7 billion requiring auxiliary constructions, including 4.1 billion exhibiting geometric symmetry. Among these, three of TongGeometry’s discoveries were selected for regional mathematical olympiads, appearing in a national team qualifying exam in China and a top civil olympiad in the USA. Guided by fine-tuned large language models, TongGeometry solved all International Mathematical Olympiad geometry problems in the IMO-AG-30 benchmark, outperforming average top human competitors on this specific dataset. It also surpasses the existing state of the art across a broader spectrum of olympiad-level problems and requires only consumer-grade computing resources. These results demonstrate that TongGeometry operates as both a mathematical discoverer and a solver, becoming an artificial intelligence system to achieve this dual capability. The deployment of a preliminary system based on TongGeometry demonstrates practical applications and opens fresh possibilities for artificial-intelligence-assisted mathematical research and education. TongGeometry both solves and proposes olympiad-level geometry problems. It uses guided tree search to find hard but concise problems, making advanced mathematical reasoning more accessible.
Published: 2026-01-26T10:02:30+00:00
Venue: Nature Machine Intelligence
Score: 0.775 (must_read)</description><content:encoded>&lt;p&gt;&lt;strong&gt;Authors:&lt;/strong&gt; Chi Zhang; Jiajun Song; Siyu Li; Yitao Liang; Yuxi Ma; Wei Wang; Yixin Zhu; Song-Chun Zhu&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Journal:&lt;/strong&gt; Nature Machine Intelligence&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;a href="https://doi.org/10.1038/s42256-025-01164-x"&gt;10.1038/s42256-025-01164-x&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Relevance:&lt;/strong&gt; 0.775 (must_read)&lt;/p&gt;
&lt;p&gt;Mathematics olympiads are prestigious competitions in which both proposing and solving problems are highly honoured. Building artificial intelligence systems capable of addressing these olympiad-level challenges remains an open frontier in automated reasoning, particularly in geometry due to its unique blend of numerical precision and spatial intuition. Here we show that TongGeometry, a neuro-symbolic system using guided tree search, both discovers and proves olympiad-level geometry theorems. Within the same computational budget as existing state-of-the-art systems, TongGeometry establishes a larger repository of geometry theorems: 6.7 billion requiring auxiliary constructions, including 4.1 billion exhibiting geometric symmetry. Among these, three of TongGeometry’s discoveries were selected for regional mathematical olympiads, appearing in a national team qualifying exam in China and a top civil olympiad in the USA. Guided by fine-tuned large language models, TongGeometry solved all International Mathematical Olympiad geometry problems in the IMO-AG-30 benchmark, outperforming average top human competitors on this specific dataset. It also surpasses the existing state of the art across a broader spectrum of olympiad-level problems and requires only consumer-grade computing resources. These results demonstrate that TongGeometry operates as both a mathematical discoverer and a solver, becoming an artificial intelligence system to achieve this dual capability. The deployment of a preliminary system based on TongGeometry demonstrates practical applications and opens fresh possibilities for artificial-intelligence-assisted mathematical research and education. TongGeometry both solves and proposes olympiad-level geometry problems. It uses guided tree search to find hard but concise problems, making advanced mathematical reasoning more accessible.&lt;/p&gt;</content:encoded></item><item><title>Smoothing the Score Function for Generalization in Diffusion Models: An Optimization-based Explanation Framework</title><link>https://arxiv.org/abs/2601.19285v1</link><guid>http://arxiv.org/abs/2601.19285v1</guid><pubDate>Tue, 27 Jan 2026 07:16:44 +0000</pubDate><dc:creator>Xinyu Zhou</dc:creator><dc:creator>Jiawei Zhang</dc:creator><dc:creator>Stephen J. Wright</dc:creator><dc:type>preprint</dc:type><prism:publicationName>arXiv</prism:publicationName><description>Diffusion models achieve remarkable generation quality, yet face a fundamental challenge known as memorization, where generated samples can replicate training samples exactly. We develop a theoretical framework to explain this phenomenon by showing that the empirical score function (the score function corresponding to the empirical distribution) is a weighted sum of the score functions of Gaussian distributions, in which the weights are sharp softmax functions. This structure causes individual training samples to dominate the score function, resulting in sampling collapse. In practice, approximating the empirical score function with a neural network can partially alleviate this issue and improve generalization. Our theoretical framework explains why: In training, the neural network learns a smoother approximation of the weighted sum, allowing the sampling process to be influenced by local manifolds rather than single points. Leveraging this insight, we propose two novel methods to further enhance generalization: (1) Noise Unconditioning enables each training sample to adaptively determine its score function weight to increase the effect of more training samples, thereby preventing single-point dominance and mitigating collapse. (2) Temperature Smoothing introduces an explicit parameter to control the smoothness. By increasing the temperature in the softmax weights, we naturally reduce the dominance of any single training sample and mitigate memorization. Experiments across multiple datasets validate our theoretical analysis and demonstrate the effectiveness of the proposed methods in improving generalization while maintaining high generation quality.
Published: 2026-01-27T07:16:44+00:00
Venue: arXiv
Score: 0.775 (must_read)</description><content:encoded>&lt;p&gt;&lt;strong&gt;Authors:&lt;/strong&gt; Xinyu Zhou; Jiawei Zhang; Stephen J. Wright&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Journal:&lt;/strong&gt; arXiv&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Relevance:&lt;/strong&gt; 0.775 (must_read)&lt;/p&gt;
&lt;p&gt;Diffusion models achieve remarkable generation quality, yet face a fundamental challenge known as memorization, where generated samples can replicate training samples exactly. We develop a theoretical framework to explain this phenomenon by showing that the empirical score function (the score function corresponding to the empirical distribution) is a weighted sum of the score functions of Gaussian distributions, in which the weights are sharp softmax functions. This structure causes individual training samples to dominate the score function, resulting in sampling collapse. In practice, approximating the empirical score function with a neural network can partially alleviate this issue and improve generalization. Our theoretical framework explains why: In training, the neural network learns a smoother approximation of the weighted sum, allowing the sampling process to be influenced by local manifolds rather than single points. Leveraging this insight, we propose two novel methods to further enhance generalization: (1) Noise Unconditioning enables each training sample to adaptively determine its score function weight to increase the effect of more training samples, thereby preventing single-point dominance and mitigating collapse. (2) Temperature Smoothing introduces an explicit parameter to control the smoothness. By increasing the temperature in the softmax weights, we naturally reduce the dominance of any single training sample and mitigate memorization. Experiments across multiple datasets validate our theoretical analysis and demonstrate the effectiveness of the proposed methods in improving generalization while maintaining high generation quality.&lt;/p&gt;</content:encoded></item><item><title>MV-SAM: Multi-view Promptable Segmentation using Pointmap Guidance</title><link>https://arxiv.org/abs/2601.17866v1</link><guid>http://arxiv.org/abs/2601.17866v1</guid><pubDate>Sun, 25 Jan 2026 15:00:37 +0000</pubDate><dc:creator>Yoonwoo Jeong</dc:creator><dc:creator>Cheng Sun</dc:creator><dc:creator>Yu-Chiang Frank Wang</dc:creator><dc:creator>Minsu Cho</dc:creator><dc:creator>Jaesung Choe</dc:creator><dc:type>preprint</dc:type><prism:publicationName>arXiv</prism:publicationName><description>Promptable segmentation has emerged as a powerful paradigm in computer vision, enabling users to guide models in parsing complex scenes with prompts such as clicks, boxes, or textual cues. Recent advances, exemplified by the Segment Anything Model (SAM), have extended this paradigm to videos and multi-view images. However, the lack of 3D awareness often leads to inconsistent results, necessitating costly per-scene optimization to enforce 3D consistency. In this work, we introduce MV-SAM, a framework for multi-view segmentation that achieves 3D consistency using pointmaps -- 3D points reconstructed from unposed images by recent visual geometry models. Leveraging the pixel-point one-to-one correspondence of pointmaps, MV-SAM lifts images and prompts into 3D space, eliminating the need for explicit 3D networks or annotated 3D data. Specifically, MV-SAM extends SAM by lifting image embeddings from its pretrained encoder into 3D point embeddings, which are decoded by a transformer using cross-attention with 3D prompt embeddings. This design aligns 2D interactions with 3D geometry, enabling the model to implicitly learn consistent masks across views through 3D positional embeddings. Trained on the SA-1B dataset, our method generalizes well across domains, outperforming SAM2-Video and achieving comparable performance with per-scene optimization baselines on NVOS, SPIn-NeRF, ScanNet++, uCo3D, and DL3DV benchmarks. Code will be released.
Published: 2026-01-25T15:00:37+00:00
Venue: arXiv
Score: 0.774 (must_read)</description><content:encoded>&lt;p&gt;&lt;strong&gt;Authors:&lt;/strong&gt; Yoonwoo Jeong; Cheng Sun; Yu-Chiang Frank Wang; Minsu Cho; Jaesung Choe&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Journal:&lt;/strong&gt; arXiv&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Relevance:&lt;/strong&gt; 0.774 (must_read)&lt;/p&gt;
&lt;p&gt;Promptable segmentation has emerged as a powerful paradigm in computer vision, enabling users to guide models in parsing complex scenes with prompts such as clicks, boxes, or textual cues. Recent advances, exemplified by the Segment Anything Model (SAM), have extended this paradigm to videos and multi-view images. However, the lack of 3D awareness often leads to inconsistent results, necessitating costly per-scene optimization to enforce 3D consistency. In this work, we introduce MV-SAM, a framework for multi-view segmentation that achieves 3D consistency using pointmaps -- 3D points reconstructed from unposed images by recent visual geometry models. Leveraging the pixel-point one-to-one correspondence of pointmaps, MV-SAM lifts images and prompts into 3D space, eliminating the need for explicit 3D networks or annotated 3D data. Specifically, MV-SAM extends SAM by lifting image embeddings from its pretrained encoder into 3D point embeddings, which are decoded by a transformer using cross-attention with 3D prompt embeddings. This design aligns 2D interactions with 3D geometry, enabling the model to implicitly learn consistent masks across views through 3D positional embeddings. Trained on the SA-1B dataset, our method generalizes well across domains, outperforming SAM2-Video and achieving comparable performance with per-scene optimization baselines on NVOS, SPIn-NeRF, ScanNet++, uCo3D, and DL3DV benchmarks. Code will be released.&lt;/p&gt;</content:encoded></item></channel></rss>