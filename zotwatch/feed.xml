<?xml version='1.0' encoding='utf-8'?>
<rss xmlns:content="http://purl.org/rss/1.0/modules/content/" xmlns:dc="http://purl.org/dc/elements/1.1/" xmlns:prism="http://prismstandard.org/namespaces/basic/3.0/" version="2.0"><channel><title>ZotWatch Feed</title><link>https://ehehe.cn/zotwatch/</link><description>AI-assisted literature watch</description><lastBuildDate>Sun, 14 Dec 2025 02:46:06 +0000</lastBuildDate><item><title>RingMoE: Mixture-of-Modality-Experts Multi-Modal Foundation Models for Universal Remote Sensing Image Interpretation</title><link>https://doi.org/10.1109/tpami.2025.3643453</link><guid>10.1109/tpami.2025.3643453</guid><pubDate>Fri, 12 Dec 2025 18:35:38 +0000</pubDate><dc:creator>Hanbo Bi</dc:creator><dc:creator>Yingchao Feng</dc:creator><dc:creator>Boyuan Tong</dc:creator><dc:creator>Mengyu Wang</dc:creator><dc:creator>Haichen Yu</dc:creator><dc:creator>Yongqiang Mao</dc:creator><dc:creator>Hao Chang</dc:creator><dc:creator>Wenhui Diao</dc:creator><dc:creator>Peijin Wang</dc:creator><dc:creator>Yue Yu</dc:creator><dc:creator>Hanyang Peng</dc:creator><dc:creator>Yehong Zhang</dc:creator><dc:creator>Kun Fu</dc:creator><dc:creator>Xian Sun</dc:creator><prism:publicationName>IEEE Transactions on Pattern Analysis and Machine Intelligence</prism:publicationName><prism:doi>10.1109/tpami.2025.3643453</prism:doi><description>The rapid advancement of foundation models has revolutionized visual representation learning in a self-supervised manner. However, their application in remote sensing (RS) remains constrained by a fundamental gap: existing models predominantly handle single or limited modalities, overlooking the inherently multi-modal nature of RS observations. Optical, synthetic aperture radar (SAR), and multi-spectral data offer complementary insights that significantly reduce the inherent ambiguity and uncertainty in single-source analysis. To bridge this gap, we introduce RingMoE, a unified multi-modal RS foundation model with 14.7 billion parameters, pre-trained on 400 million multi-modal RS images from nine satellites. RingMoE incorporates three key innovations: (1) A hierarchical Mixture-of-Experts (MoE) architecture comprising modal-specialized, collaborative, and shared experts, effectively modeling intra-modal knowledge while capturing cross-modal dependencies to mitigate conflicts between modal representations; (2) Physics-informed self-supervised learning, explicitly embedding sensor-specific radiometric characteristics into the pre-training objectives; (3) Dynamic expert pruning, enabling adaptive model compression from 14.7B to 1B parameters while maintaining performance, facilitating efficient deployment in Earth observation applications. Evaluated across 23 benchmarks spanning six key RS tasks (i.e., classification, detection, segmentation, tracking, change detection, and depth estimation), RingMoE outperforms existing foundation models and sets new SOTAs, demonstrating remarkable adaptability from single-modal to multi-modal scenarios. Beyond theoretical progress, it has been deployed and trialed in multiple sectors, including emergency response, land management, marine sciences, and urban planning.
Published: 2025-12-12T18:35:38+00:00
Venue: IEEE Transactions on Pattern Analysis and Machine Intelligence
Score: 0.851 (must_read)</description><content:encoded>&lt;p&gt;&lt;strong&gt;Authors:&lt;/strong&gt; Hanbo Bi; Yingchao Feng; Boyuan Tong; Mengyu Wang; Haichen Yu; Yongqiang Mao; Hao Chang; Wenhui Diao; Peijin Wang; Yue Yu; Hanyang Peng; Yehong Zhang; Kun Fu; Xian Sun&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Journal:&lt;/strong&gt; IEEE Transactions on Pattern Analysis and Machine Intelligence&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;a href="https://doi.org/10.1109/tpami.2025.3643453"&gt;10.1109/tpami.2025.3643453&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Relevance:&lt;/strong&gt; 0.851 (must_read)&lt;/p&gt;
&lt;p&gt;The rapid advancement of foundation models has revolutionized visual representation learning in a self-supervised manner. However, their application in remote sensing (RS) remains constrained by a fundamental gap: existing models predominantly handle single or limited modalities, overlooking the inherently multi-modal nature of RS observations. Optical, synthetic aperture radar (SAR), and multi-spectral data offer complementary insights that significantly reduce the inherent ambiguity and uncertainty in single-source analysis. To bridge this gap, we introduce RingMoE, a unified multi-modal RS foundation model with 14.7 billion parameters, pre-trained on 400 million multi-modal RS images from nine satellites. RingMoE incorporates three key innovations: (1) A hierarchical Mixture-of-Experts (MoE) architecture comprising modal-specialized, collaborative, and shared experts, effectively modeling intra-modal knowledge while capturing cross-modal dependencies to mitigate conflicts between modal representations; (2) Physics-informed self-supervised learning, explicitly embedding sensor-specific radiometric characteristics into the pre-training objectives; (3) Dynamic expert pruning, enabling adaptive model compression from 14.7B to 1B parameters while maintaining performance, facilitating efficient deployment in Earth observation applications. Evaluated across 23 benchmarks spanning six key RS tasks (i.e., classification, detection, segmentation, tracking, change detection, and depth estimation), RingMoE outperforms existing foundation models and sets new SOTAs, demonstrating remarkable adaptability from single-modal to multi-modal scenarios. Beyond theoretical progress, it has been deployed and trialed in multiple sectors, including emergency response, land management, marine sciences, and urban planning.&lt;/p&gt;</content:encoded></item><item><title>A real-time surface defect detection model based on adaptive feature information selection and fusion</title><link>https://doi.org/10.1016/j.inffus.2025.104041</link><guid>10.1016/j.inffus.2025.104041</guid><pubDate>Fri, 12 Dec 2025 00:32:09 +0000</pubDate><dc:creator>Li-Juan Liu</dc:creator><dc:creator>Shao-Qi Sun</dc:creator><dc:creator>Hamid Reza Karimi</dc:creator><prism:publicationName>Information Fusion</prism:publicationName><prism:doi>10.1016/j.inffus.2025.104041</prism:doi><description>In contemporary computer vision, You Only Look Once (YOLO) has become a benchmark for object detection, widely used in domains from intelligent manufacturing—such as industrial quality control and automated inspection—to real-time video surveillance. For example, detecting surface defects on steel products or electronic components in production lines relies on such algorithms to maintain high quality and safety. Despite YOLO’s excellent speed and accuracy in many tasks, it still faces difficulties in certain challenging conditions, notably high dynamic range scenes, complex backgrounds, and the detection of small or subtle objects. These conditions are common in practice—for instance, on shiny metal surfaces with uneven lighting or in busy surveillance scenes—where conventional YOLO models struggle to capture fine details reliably. To overcome these limitations, we propose an improved YOLO-based framework featuring a novel Dynamic Cross-Scale Feature Fusion Module (Dy-CCFM) and a Dual-path Downsampling Convolution Module (DDConv). These modules enhance multi-scale feature representation and preserve detail under extreme lighting and background clutter, which is crucial for monitoring in complex environments. Additionally, we employ the Minimum Point Distance Intersection over Union (MPDIoU) as an optimized loss function for bounding box regression, significantly improving the localization of small objects. Thanks to these innovations, the model achieves a mean Average Precision (mAP) of 75.1% on the challenging Northeastern University surface defect (NEU-DET) dataset, while the smallest variant is only 1.6M in size. Compared to YOLOv8, our approach improves mAP by 2.1% while also delivering higher inference speed (FPS), and it surpasses the Detection Transformer (DETR) by 5.0% mAP. The model further demonstrates excellent generalization on the Google Cloud 10 Defect Detection (GC10-DET) dataset. This enhanced detection algorithm not only improves performance but also offers significant practical value in intelligent manufacturing and automated inspection systems, intelligent video surveillance, and autonomous vehicles, where reliable real-time detection of small defects or targets is critical.
Published: 2025-12-12T00:32:09+00:00
Venue: Information Fusion
Score: 0.845 (must_read)</description><content:encoded>&lt;p&gt;&lt;strong&gt;Authors:&lt;/strong&gt; Li-Juan Liu; Shao-Qi Sun; Hamid Reza Karimi&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Journal:&lt;/strong&gt; Information Fusion&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;a href="https://doi.org/10.1016/j.inffus.2025.104041"&gt;10.1016/j.inffus.2025.104041&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Relevance:&lt;/strong&gt; 0.845 (must_read)&lt;/p&gt;
&lt;p&gt;In contemporary computer vision, You Only Look Once (YOLO) has become a benchmark for object detection, widely used in domains from intelligent manufacturing—such as industrial quality control and automated inspection—to real-time video surveillance. For example, detecting surface defects on steel products or electronic components in production lines relies on such algorithms to maintain high quality and safety. Despite YOLO’s excellent speed and accuracy in many tasks, it still faces difficulties in certain challenging conditions, notably high dynamic range scenes, complex backgrounds, and the detection of small or subtle objects. These conditions are common in practice—for instance, on shiny metal surfaces with uneven lighting or in busy surveillance scenes—where conventional YOLO models struggle to capture fine details reliably. To overcome these limitations, we propose an improved YOLO-based framework featuring a novel Dynamic Cross-Scale Feature Fusion Module (Dy-CCFM) and a Dual-path Downsampling Convolution Module (DDConv). These modules enhance multi-scale feature representation and preserve detail under extreme lighting and background clutter, which is crucial for monitoring in complex environments. Additionally, we employ the Minimum Point Distance Intersection over Union (MPDIoU) as an optimized loss function for bounding box regression, significantly improving the localization of small objects. Thanks to these innovations, the model achieves a mean Average Precision (mAP) of 75.1% on the challenging Northeastern University surface defect (NEU-DET) dataset, while the smallest variant is only 1.6M in size. Compared to YOLOv8, our approach improves mAP by 2.1% while also delivering higher inference speed (FPS), and it surpasses the Detection Transformer (DETR) by 5.0% mAP. The model further demonstrates excellent generalization on the Google Cloud 10 Defect Detection (GC10-DET) dataset. This enhanced detection algorithm not only improves performance but also offers significant practical value in intelligent manufacturing and automated inspection systems, intelligent video surveillance, and autonomous vehicles, where reliable real-time detection of small defects or targets is critical.&lt;/p&gt;</content:encoded></item><item><title>Gradient-Guided Learning Network for Infrared Small Target Detection</title><link>https://arxiv.org/abs/2512.09497v1</link><guid>http://arxiv.org/abs/2512.09497v1</guid><pubDate>Wed, 10 Dec 2025 10:21:08 +0000</pubDate><dc:creator>Jinmiao Zhao</dc:creator><dc:creator>Chuang Yu</dc:creator><dc:creator>Zelin Shi</dc:creator><dc:creator>Yunpeng Liu</dc:creator><dc:creator>Yingdi Zhang</dc:creator><dc:type>preprint</dc:type><prism:publicationName>arXiv</prism:publicationName><prism:doi>10.1109/LGRS.2023.3308783</prism:doi><description>Recently, infrared small target detection has attracted extensive attention. However, due to the small size and the lack of intrinsic features of infrared small targets, the existing methods generally have the problem of inaccurate edge positioning and the target is easily submerged by the background. Therefore, we propose an innovative gradient-guided learning network (GGL-Net). Specifically, we are the first to explore the introduction of gradient magnitude images into the deep learning-based infrared small target detection method, which is conducive to emphasizing the edge details and alleviating the problem of inaccurate edge positioning of small targets. On this basis, we propose a novel dual-branch feature extraction network that utilizes the proposed gradient supplementary module (GSM) to encode raw gradient information into deeper network layers and embeds attention mechanisms reasonably to enhance feature extraction ability. In addition, we construct a two-way guidance fusion module (TGFM), which fully considers the characteristics of feature maps at different levels. It can facilitate the effective fusion of multi-scale feature maps and extract richer semantic information and detailed information through reasonable two-way guidance. Extensive experiments prove that GGL-Net has achieves state-of-the-art results on the public real NUAA-SIRST dataset and the public synthetic NUDT-SIRST dataset. Our code has been integrated into https://github.com/YuChuang1205/MSDA-Net
Published: 2025-12-10T10:21:08+00:00
Venue: arXiv
Score: 0.817 (must_read)</description><content:encoded>&lt;p&gt;&lt;strong&gt;Authors:&lt;/strong&gt; Jinmiao Zhao; Chuang Yu; Zelin Shi; Yunpeng Liu; Yingdi Zhang&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Journal:&lt;/strong&gt; arXiv&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;a href="https://doi.org/10.1109/LGRS.2023.3308783"&gt;10.1109/LGRS.2023.3308783&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Relevance:&lt;/strong&gt; 0.817 (must_read)&lt;/p&gt;
&lt;p&gt;Recently, infrared small target detection has attracted extensive attention. However, due to the small size and the lack of intrinsic features of infrared small targets, the existing methods generally have the problem of inaccurate edge positioning and the target is easily submerged by the background. Therefore, we propose an innovative gradient-guided learning network (GGL-Net). Specifically, we are the first to explore the introduction of gradient magnitude images into the deep learning-based infrared small target detection method, which is conducive to emphasizing the edge details and alleviating the problem of inaccurate edge positioning of small targets. On this basis, we propose a novel dual-branch feature extraction network that utilizes the proposed gradient supplementary module (GSM) to encode raw gradient information into deeper network layers and embeds attention mechanisms reasonably to enhance feature extraction ability. In addition, we construct a two-way guidance fusion module (TGFM), which fully considers the characteristics of feature maps at different levels. It can facilitate the effective fusion of multi-scale feature maps and extract richer semantic information and detailed information through reasonable two-way guidance. Extensive experiments prove that GGL-Net has achieves state-of-the-art results on the public real NUAA-SIRST dataset and the public synthetic NUDT-SIRST dataset. Our code has been integrated into https://github.com/YuChuang1205/MSDA-Net&lt;/p&gt;</content:encoded></item><item><title>Homogeneous Multimodal Adaptive Cross-Attention Fusion with Confidence-Aware Keypoints Evaluation for 6DoF Pose Estimation</title><link>https://doi.org/10.1016/j.inffus.2025.104059</link><guid>10.1016/j.inffus.2025.104059</guid><pubDate>Sat, 13 Dec 2025 23:37:01 +0000</pubDate><dc:creator>Yi Guo</dc:creator><dc:creator>Fei Wang</dc:creator><dc:creator>Hao Chu</dc:creator><dc:creator>Jindong Yu</dc:creator><dc:creator>Shuai Han</dc:creator><prism:publicationName>Information Fusion</prism:publicationName><prism:doi>10.1016/j.inffus.2025.104059</prism:doi><description>6D pose estimation from RGB-D data constitutes a pivotal research area in computer vision, where the primary challenge resides in effectively integrating RGB and depth modalities. We propose an innovative homogeneous multimodal cross-attention fusion framework for object 6D pose through directly processing raw RGB-D data for feature extraction rather than traditional point cloud-based two-branch architectures. We employ the global-local embeddings and adaptive cross-attention fusion to exploit the inherent similarity of homogeneous multimodal information. Furthermore, we design a confidence-aware keypoint evaluation module to enhance localization accuracy and robustness. Comparative analysis experiments on three popular benchmark datasets, complemented by systematic ablation analyses, demonstrate the efficacy of our method in achieving superior performance on Occlusion-LineMOD (79.6%), YCB-Video (97.2%), and MP6D (93.60%). Finally, we verify the applicability of our method in difficult conditions.
Published: 2025-12-13T23:37:01+00:00
Venue: Information Fusion
Score: 0.814 (must_read)</description><content:encoded>&lt;p&gt;&lt;strong&gt;Authors:&lt;/strong&gt; Yi Guo; Fei Wang; Hao Chu; Jindong Yu; Shuai Han&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Journal:&lt;/strong&gt; Information Fusion&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;a href="https://doi.org/10.1016/j.inffus.2025.104059"&gt;10.1016/j.inffus.2025.104059&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Relevance:&lt;/strong&gt; 0.814 (must_read)&lt;/p&gt;
&lt;p&gt;6D pose estimation from RGB-D data constitutes a pivotal research area in computer vision, where the primary challenge resides in effectively integrating RGB and depth modalities. We propose an innovative homogeneous multimodal cross-attention fusion framework for object 6D pose through directly processing raw RGB-D data for feature extraction rather than traditional point cloud-based two-branch architectures. We employ the global-local embeddings and adaptive cross-attention fusion to exploit the inherent similarity of homogeneous multimodal information. Furthermore, we design a confidence-aware keypoint evaluation module to enhance localization accuracy and robustness. Comparative analysis experiments on three popular benchmark datasets, complemented by systematic ablation analyses, demonstrate the efficacy of our method in achieving superior performance on Occlusion-LineMOD (79.6%), YCB-Video (97.2%), and MP6D (93.60%). Finally, we verify the applicability of our method in difficult conditions.&lt;/p&gt;</content:encoded></item><item><title>Generating Any Changes in the Noise Domain</title><link>https://doi.org/10.1109/tpami.2025.3643733</link><guid>10.1109/tpami.2025.3643733</guid><pubDate>Fri, 12 Dec 2025 18:35:38 +0000</pubDate><dc:creator>Qiang Liu</dc:creator><dc:creator>Yang Kuang</dc:creator><dc:creator>Jun Yue</dc:creator><dc:creator>Pedram Ghamisi</dc:creator><dc:creator>Weiying Xie</dc:creator><dc:creator>Leyuan Fang</dc:creator><prism:publicationName>IEEE Transactions on Pattern Analysis and Machine Intelligence</prism:publicationName><prism:doi>10.1109/tpami.2025.3643733</prism:doi><description>Change detection is essential in Earth observation, yet current models heavily rely on large-scale annotated datasets. Generative models offer a promising alternative by synthesizing training data, but generating temporally coherent image pairs with realistic, semantically meaningful changes remains a significant challenge. Existing approaches typically simulate changes by generating pre- and post-change label maps using either heuristic rules (e.g., copy-pasting) or text prompts. However, the former offers limited change diversity, while the latter often fails to maintain spatial consistency between image pairs. We observe that the noise space of diffusion models encodes strong generative capacity and spatial controllability: localized perturbations in the noise can yield meaningful, interpretable changes in corresponding image regions. Motivated by this, we propose Noise2Change, a framework for simulating change directly in the noise domain. The key idea is to manipulate the semantic composition of the initial noise sampled from the noise domain, such that the diffusion process generates structurally consistent pre- and post-change images reflecting realistic transformations. Since the unperturbed noise is shared between both images, the resulting pairs exhibit strong temporal alignment and semantic coherence, effectively addressing the trade-off between realism and consistency. Concretely, we employ a discrete diffusion model to extract high-level semantics from the initial noise. Guided by these semantics, we introduce a change simulation strategy that optimizes the noise to encode intended changes. The modified noise is then used to drive the diffusion process, yielding pre- and post-change label maps with natural structural transitions. These maps are passed through a unified framework for image generation and label refinement, producing highly aligned image-label pairs. Our framework supports diverse change types across a wide range of scenarios. Extensive ex...
Published: 2025-12-12T18:35:38+00:00
Venue: IEEE Transactions on Pattern Analysis and Machine Intelligence
Score: 0.811 (must_read)</description><content:encoded>&lt;p&gt;&lt;strong&gt;Authors:&lt;/strong&gt; Qiang Liu; Yang Kuang; Jun Yue; Pedram Ghamisi; Weiying Xie; Leyuan Fang&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Journal:&lt;/strong&gt; IEEE Transactions on Pattern Analysis and Machine Intelligence&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;a href="https://doi.org/10.1109/tpami.2025.3643733"&gt;10.1109/tpami.2025.3643733&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Relevance:&lt;/strong&gt; 0.811 (must_read)&lt;/p&gt;
&lt;p&gt;Change detection is essential in Earth observation, yet current models heavily rely on large-scale annotated datasets. Generative models offer a promising alternative by synthesizing training data, but generating temporally coherent image pairs with realistic, semantically meaningful changes remains a significant challenge. Existing approaches typically simulate changes by generating pre- and post-change label maps using either heuristic rules (e.g., copy-pasting) or text prompts. However, the former offers limited change diversity, while the latter often fails to maintain spatial consistency between image pairs. We observe that the noise space of diffusion models encodes strong generative capacity and spatial controllability: localized perturbations in the noise can yield meaningful, interpretable changes in corresponding image regions. Motivated by this, we propose Noise2Change, a framework for simulating change directly in the noise domain. The key idea is to manipulate the semantic composition of the initial noise sampled from the noise domain, such that the diffusion process generates structurally consistent pre- and post-change images reflecting realistic transformations. Since the unperturbed noise is shared between both images, the resulting pairs exhibit strong temporal alignment and semantic coherence, effectively addressing the trade-off between realism and consistency. Concretely, we employ a discrete diffusion model to extract high-level semantics from the initial noise. Guided by these semantics, we introduce a change simulation strategy that optimizes the noise to encode intended changes. The modified noise is then used to drive the diffusion process, yielding pre- and post-change label maps with natural structural transitions. These maps are passed through a unified framework for image generation and label refinement, producing highly aligned image-label pairs. Our framework supports diverse change types across a wide range of scenarios. Extensive ex...&lt;/p&gt;</content:encoded></item><item><title>SSP-SAM: SAM with Semantic-Spatial Prompt for Referring Expression Segmentation</title><link>https://doi.org/10.1109/tcsvt.2025.3643649</link><guid>10.1109/tcsvt.2025.3643649</guid><pubDate>Fri, 12 Dec 2025 18:37:50 +0000</pubDate><dc:creator>Wei Tang</dc:creator><dc:creator>Xuejing Liu</dc:creator><dc:creator>Yanpeng Sun</dc:creator><dc:creator>Zechao Li</dc:creator><prism:publicationName>IEEE Transactions on Circuits and Systems for Video Technology</prism:publicationName><prism:doi>10.1109/tcsvt.2025.3643649</prism:doi><description>The Segment Anything Model (SAM) excels at general image segmentation but has limited ability to understand natural language, which restricts its direct application in Referring Expression Segmentation (RES). Toward this end, we propose SSP-SAM, a framework that fully utilizes SAM’s segmentation capabilities by integrating a Semantic-Spatial Prompt (SSP) encoder. Specifically, we incorporate both visual and linguistic attention adapters into the SSP encoder, which highlight salient objects within the visual features and discriminative phrases within the linguistic features. This design enhances the referent representation for the prompt generator, resulting in high-quality SSPs that enable SAM to generate precise masks guided by language. Although not specifically designed for Generalized RES (GRES), where the referent may correspond to zero, one, or multiple objects, SSP-SAM naturally supports this more flexible setting without additional modifications. Extensive experiments on widely used RES and GRES benchmarks confirm the superiority of our method. Notably, our approach generates segmentation masks of high quality, achieving strong precision even at strict thresholds such as Pr@0.9. Further evaluation on the PhraseCut dataset demonstrates improved performance in open-vocabulary scenarios compared to existing state-of-the-art RES methods. The code and checkpoints are available at: https://github.com/WayneTomas/SSP-SAM.
Published: 2025-12-12T18:37:50+00:00
Venue: IEEE Transactions on Circuits and Systems for Video Technology
Score: 0.804 (must_read)</description><content:encoded>&lt;p&gt;&lt;strong&gt;Authors:&lt;/strong&gt; Wei Tang; Xuejing Liu; Yanpeng Sun; Zechao Li&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Journal:&lt;/strong&gt; IEEE Transactions on Circuits and Systems for Video Technology&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;a href="https://doi.org/10.1109/tcsvt.2025.3643649"&gt;10.1109/tcsvt.2025.3643649&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Relevance:&lt;/strong&gt; 0.804 (must_read)&lt;/p&gt;
&lt;p&gt;The Segment Anything Model (SAM) excels at general image segmentation but has limited ability to understand natural language, which restricts its direct application in Referring Expression Segmentation (RES). Toward this end, we propose SSP-SAM, a framework that fully utilizes SAM’s segmentation capabilities by integrating a Semantic-Spatial Prompt (SSP) encoder. Specifically, we incorporate both visual and linguistic attention adapters into the SSP encoder, which highlight salient objects within the visual features and discriminative phrases within the linguistic features. This design enhances the referent representation for the prompt generator, resulting in high-quality SSPs that enable SAM to generate precise masks guided by language. Although not specifically designed for Generalized RES (GRES), where the referent may correspond to zero, one, or multiple objects, SSP-SAM naturally supports this more flexible setting without additional modifications. Extensive experiments on widely used RES and GRES benchmarks confirm the superiority of our method. Notably, our approach generates segmentation masks of high quality, achieving strong precision even at strict thresholds such as Pr@0.9. Further evaluation on the PhraseCut dataset demonstrates improved performance in open-vocabulary scenarios compared to existing state-of-the-art RES methods. The code and checkpoints are available at: https://github.com/WayneTomas/SSP-SAM.&lt;/p&gt;</content:encoded></item><item><title>Developing Evolving Adaptability in Biological Intelligence: A Novel Biologically-Inspired Continual Learning Model for Video Saliency Prediction</title><link>https://doi.org/10.1109/tpami.2025.3643517</link><guid>10.1109/tpami.2025.3643517</guid><pubDate>Fri, 12 Dec 2025 18:35:38 +0000</pubDate><dc:creator>Dandan Zhu</dc:creator><dc:creator>Kaiwei Zhang</dc:creator><dc:creator>Kun Zhu</dc:creator><dc:creator>Nana Zhang</dc:creator><dc:creator>Xiongkuo Min</dc:creator><dc:creator>Guangtao Zhai</dc:creator><dc:creator>Xiaokang Yang</dc:creator><prism:publicationName>IEEE Transactions on Pattern Analysis and Machine Intelligence</prism:publicationName><prism:doi>10.1109/tpami.2025.3643517</prism:doi><description>In the era of deep learning, video saliency prediction task still remains major challenge due to the issue of catastrophic forgetting during feature learning. Most prior works commonly employ generative replay strategies to generate pseudo-samples from previous tasks, enabling them to recall the data distribution. However, scaling up generative replay to accommodate class-incremental and task-incremental settings poses challenges, as generated data with low quality can severely deteriorate performance. Additionally, existing advances mainly focus on preserving memory stability to alleviate catastrophic forgetting, but they remain difficult to flexibly adapt to incremental changes in dynamic scenes. To achieve a better balance between memory stability and learning plasticity, we propose a novel biologically-inspired continual learning (BICL) model tailored to effectively predict human attention in dynamic scenes while mitigate catastrophic forgetting. In particular, inspired by the function of the hippocampus in the human neural system, we elaborately design a visual saliency memory bank module to explicitly store and retrieve representative features from previous tasks. Furthermore, drawing inspiration from the Drosophila γ \gamma MB system, we propose an active forgetting strategy equipped with multiple parallel adaptive learner modules, which can appropriately attenuate old memories in parameter distribution to enhance learning plasticity to adapt to new tasks, and accordingly to ensure compatibility among multiple learners. Notably, without compromising the performance of old tasks, our proposed model can achieve a better trade-off between memory stability and learning plasticity. Through extensive experiments on several benchmark datasets, our model not only enhances performance in task-incremental settings, but also potentially provides deep insights into neurological adaptive mechanisms.
Published: 2025-12-12T18:35:38+00:00
Venue: IEEE Transactions on Pattern Analysis and Machine Intelligence
Score: 0.804 (must_read)</description><content:encoded>&lt;p&gt;&lt;strong&gt;Authors:&lt;/strong&gt; Dandan Zhu; Kaiwei Zhang; Kun Zhu; Nana Zhang; Xiongkuo Min; Guangtao Zhai; Xiaokang Yang&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Journal:&lt;/strong&gt; IEEE Transactions on Pattern Analysis and Machine Intelligence&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;a href="https://doi.org/10.1109/tpami.2025.3643517"&gt;10.1109/tpami.2025.3643517&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Relevance:&lt;/strong&gt; 0.804 (must_read)&lt;/p&gt;
&lt;p&gt;In the era of deep learning, video saliency prediction task still remains major challenge due to the issue of catastrophic forgetting during feature learning. Most prior works commonly employ generative replay strategies to generate pseudo-samples from previous tasks, enabling them to recall the data distribution. However, scaling up generative replay to accommodate class-incremental and task-incremental settings poses challenges, as generated data with low quality can severely deteriorate performance. Additionally, existing advances mainly focus on preserving memory stability to alleviate catastrophic forgetting, but they remain difficult to flexibly adapt to incremental changes in dynamic scenes. To achieve a better balance between memory stability and learning plasticity, we propose a novel biologically-inspired continual learning (BICL) model tailored to effectively predict human attention in dynamic scenes while mitigate catastrophic forgetting. In particular, inspired by the function of the hippocampus in the human neural system, we elaborately design a visual saliency memory bank module to explicitly store and retrieve representative features from previous tasks. Furthermore, drawing inspiration from the Drosophila γ \gamma MB system, we propose an active forgetting strategy equipped with multiple parallel adaptive learner modules, which can appropriately attenuate old memories in parameter distribution to enhance learning plasticity to adapt to new tasks, and accordingly to ensure compatibility among multiple learners. Notably, without compromising the performance of old tasks, our proposed model can achieve a better trade-off between memory stability and learning plasticity. Through extensive experiments on several benchmark datasets, our model not only enhances performance in task-incremental settings, but also potentially provides deep insights into neurological adaptive mechanisms.&lt;/p&gt;</content:encoded></item><item><title>MG-LLaVA: Towards Multi-Granularity Visual Instruction Tuning</title><link>https://doi.org/10.1109/tcsvt.2025.3643469</link><guid>10.1109/tcsvt.2025.3643469</guid><pubDate>Fri, 12 Dec 2025 18:37:50 +0000</pubDate><dc:creator>Xiangyu Zhao</dc:creator><dc:creator>Xiangtai Li</dc:creator><dc:creator>Haodong Duan</dc:creator><dc:creator>Haian Huang</dc:creator><dc:creator>Yining Li</dc:creator><dc:creator>Kai Chen</dc:creator><dc:creator>Hua Yang</dc:creator><prism:publicationName>IEEE Transactions on Circuits and Systems for Video Technology</prism:publicationName><prism:doi>10.1109/tcsvt.2025.3643469</prism:doi><description>Multi-modal large language models (MLLMs) have made significant strides in various visual understanding tasks. However, the majority of these models are constrained to process low-resolution images, which limits their effectiveness in perception tasks that necessitate detailed visual information. In our study, we present MG-LLaVA, an innovative MLLM that enhances the model’s visual processing capabilities by incorporating a multi-granularity vision flow, which includes low-resolution, high-resolution, and object-centric features. We propose the integration of an additional high-resolution visual encoder to capture fine-grained details, which are then fused with base visual features through a Conv-Gate fusion network. To further refine the model’s object recognition abilities, we incorporate object-level features derived from bounding boxes identified by offline detectors. Being trained solely on publicly available multimodal data through instruction tuning, MG-LLaVA demonstrates exceptional perception skills. We instantiate MG-LLaVA with a wide variety of language encoders, ranging from 3.8B to 34B, to evaluate the model’s performance comprehensively. Extensive evaluations across multiple benchmarks demonstrate that MG-LLaVA outperforms existing MLLMs of comparable parameter sizes, showcasing its remarkable efficacy.
Published: 2025-12-12T18:37:50+00:00
Venue: IEEE Transactions on Circuits and Systems for Video Technology
Score: 0.803 (must_read)</description><content:encoded>&lt;p&gt;&lt;strong&gt;Authors:&lt;/strong&gt; Xiangyu Zhao; Xiangtai Li; Haodong Duan; Haian Huang; Yining Li; Kai Chen; Hua Yang&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Journal:&lt;/strong&gt; IEEE Transactions on Circuits and Systems for Video Technology&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;a href="https://doi.org/10.1109/tcsvt.2025.3643469"&gt;10.1109/tcsvt.2025.3643469&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Relevance:&lt;/strong&gt; 0.803 (must_read)&lt;/p&gt;
&lt;p&gt;Multi-modal large language models (MLLMs) have made significant strides in various visual understanding tasks. However, the majority of these models are constrained to process low-resolution images, which limits their effectiveness in perception tasks that necessitate detailed visual information. In our study, we present MG-LLaVA, an innovative MLLM that enhances the model’s visual processing capabilities by incorporating a multi-granularity vision flow, which includes low-resolution, high-resolution, and object-centric features. We propose the integration of an additional high-resolution visual encoder to capture fine-grained details, which are then fused with base visual features through a Conv-Gate fusion network. To further refine the model’s object recognition abilities, we incorporate object-level features derived from bounding boxes identified by offline detectors. Being trained solely on publicly available multimodal data through instruction tuning, MG-LLaVA demonstrates exceptional perception skills. We instantiate MG-LLaVA with a wide variety of language encoders, ranging from 3.8B to 34B, to evaluate the model’s performance comprehensively. Extensive evaluations across multiple benchmarks demonstrate that MG-LLaVA outperforms existing MLLMs of comparable parameter sizes, showcasing its remarkable efficacy.&lt;/p&gt;</content:encoded></item><item><title>Attention-driven feature enhancement network for object detection</title><link>https://doi.org/10.1016/j.neucom.2025.132028</link><guid>10.1016/j.neucom.2025.132028</guid><pubDate>Fri, 12 Dec 2025 07:57:51 +0000</pubDate><dc:creator>Yang Li</dc:creator><dc:creator>Yongsheng Dong</dc:creator><dc:creator>Siming Jia</dc:creator><dc:creator>Zhifan Li</dc:creator><dc:creator>Lintao Zheng</dc:creator><dc:creator>Yaxin Li</dc:creator><dc:creator>Ruijuan Zheng</dc:creator><prism:publicationName>Neurocomputing</prism:publicationName><prism:doi>10.1016/j.neucom.2025.132028</prism:doi><description>In recent years, object detection technology based on deep learning makes great progress. However, the existing deep learning-based object detection methods can not achieve satisfactory detection performance for small-size objects because they have defects in the processing of detailed information in the process of extracting features layer by layer. To alleviate this issue, in this paper we propose an Attention-driven Feature Enhancement Network (AFENet) for object detection. Particularly, we first propose a Multi-branch Feature preservation Enhancement Module (MFEM), which employs a multi-branch architecture and path design, allowing each layer within the module to learn feature extraction from the original features rich in detailed information. Furthermore, we propose a Joint Residual Attention Mechanism (JRAM). It focuses on the corresponding important weight information through an attention mechanism and utilizes residual connections are employed to retain the initial features and support the characteristics of deep learning, helping the model to perform better in deep learning and to capture the details of small targets more effectively. Experimental results on the PASCAL VOC2007+2012, Microsoft COCO2017, and VisDrone2019 datasets reveal that our proposed AFENet is effective and can achieve competitive detection performance when compared to several representative methods. The code is available at https://github.com/yang-Detection/AFENet .
Published: 2025-12-12T07:57:51+00:00
Venue: Neurocomputing
Score: 0.801 (must_read)</description><content:encoded>&lt;p&gt;&lt;strong&gt;Authors:&lt;/strong&gt; Yang Li; Yongsheng Dong; Siming Jia; Zhifan Li; Lintao Zheng; Yaxin Li; Ruijuan Zheng&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Journal:&lt;/strong&gt; Neurocomputing&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;a href="https://doi.org/10.1016/j.neucom.2025.132028"&gt;10.1016/j.neucom.2025.132028&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Relevance:&lt;/strong&gt; 0.801 (must_read)&lt;/p&gt;
&lt;p&gt;In recent years, object detection technology based on deep learning makes great progress. However, the existing deep learning-based object detection methods can not achieve satisfactory detection performance for small-size objects because they have defects in the processing of detailed information in the process of extracting features layer by layer. To alleviate this issue, in this paper we propose an Attention-driven Feature Enhancement Network (AFENet) for object detection. Particularly, we first propose a Multi-branch Feature preservation Enhancement Module (MFEM), which employs a multi-branch architecture and path design, allowing each layer within the module to learn feature extraction from the original features rich in detailed information. Furthermore, we propose a Joint Residual Attention Mechanism (JRAM). It focuses on the corresponding important weight information through an attention mechanism and utilizes residual connections are employed to retain the initial features and support the characteristics of deep learning, helping the model to perform better in deep learning and to capture the details of small targets more effectively. Experimental results on the PASCAL VOC2007+2012, Microsoft COCO2017, and VisDrone2019 datasets reveal that our proposed AFENet is effective and can achieve competitive detection performance when compared to several representative methods. The code is available at https://github.com/yang-Detection/AFENet .&lt;/p&gt;</content:encoded></item><item><title>A Dual-Driven Hybrid Tracking Architecture for Radar Targets Based on Innovation</title><link>https://doi.org/10.1016/j.inffus.2025.104056</link><guid>10.1016/j.inffus.2025.104056</guid><pubDate>Fri, 12 Dec 2025 17:34:27 +0000</pubDate><dc:creator>Yanwen Bai</dc:creator><dc:creator>Jibin Zheng</dc:creator><dc:creator>Hanxing Shao</dc:creator><dc:creator>Hongwei Liu</dc:creator><prism:publicationName>Information Fusion</prism:publicationName><prism:doi>10.1016/j.inffus.2025.104056</prism:doi><description>Targets such as hypersonic missiles and stealth aircraft are characterized by complex motion patterns, strong maneuverability, and anomalous radar measurement statistics. Although model-driven radar target tracking methods offer physical interpretability, they suffer from their dependence on explicit prior assumptions. Data-driven methods can theoretically approximate arbitrarily complex motions through nonlinear mappings, but suffer from poor interpretability, vulnerability to noise during feature extraction, and loss of low-frequency maneuvering features due to sample imbalance. Therefore, this paper proposes a Dual-Driven Hybrid Tracking Architecture Based on Innovation (DDHTA), which fuses the advantages of both model-driven and data-driven approaches. First, a model-driven approach is adopted for basic state estimation, and a Dual Condition Judgment Adjustment (DCJA) method is proposed to adaptively adjust the measurement error variance, thereby providing a high-quality baseline estimate for the data-driven layer and reducing the interference of anomalous noise on feature extraction. Further, in the data-driven layer, a Dual-Scale Temporal Network (DSTNet) is designed. By learning the mapping from the innovation to the estimation errors, it combines the strengths of causal dilated convolution and multi-head self-attention to provide dynamic compensation, which corrects the estimation errors of the model-driven method. Numerical simulation results demonstrate that the proposed method enhances the algorithm’s ability to handle target maneuvers in complex environments, achieving higher tracking accuracy and robustness.
Published: 2025-12-12T17:34:27+00:00
Venue: Information Fusion
Score: 0.797 (must_read)</description><content:encoded>&lt;p&gt;&lt;strong&gt;Authors:&lt;/strong&gt; Yanwen Bai; Jibin Zheng; Hanxing Shao; Hongwei Liu&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Journal:&lt;/strong&gt; Information Fusion&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;a href="https://doi.org/10.1016/j.inffus.2025.104056"&gt;10.1016/j.inffus.2025.104056&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Relevance:&lt;/strong&gt; 0.797 (must_read)&lt;/p&gt;
&lt;p&gt;Targets such as hypersonic missiles and stealth aircraft are characterized by complex motion patterns, strong maneuverability, and anomalous radar measurement statistics. Although model-driven radar target tracking methods offer physical interpretability, they suffer from their dependence on explicit prior assumptions. Data-driven methods can theoretically approximate arbitrarily complex motions through nonlinear mappings, but suffer from poor interpretability, vulnerability to noise during feature extraction, and loss of low-frequency maneuvering features due to sample imbalance. Therefore, this paper proposes a Dual-Driven Hybrid Tracking Architecture Based on Innovation (DDHTA), which fuses the advantages of both model-driven and data-driven approaches. First, a model-driven approach is adopted for basic state estimation, and a Dual Condition Judgment Adjustment (DCJA) method is proposed to adaptively adjust the measurement error variance, thereby providing a high-quality baseline estimate for the data-driven layer and reducing the interference of anomalous noise on feature extraction. Further, in the data-driven layer, a Dual-Scale Temporal Network (DSTNet) is designed. By learning the mapping from the innovation to the estimation errors, it combines the strengths of causal dilated convolution and multi-head self-attention to provide dynamic compensation, which corrects the estimation errors of the model-driven method. Numerical simulation results demonstrate that the proposed method enhances the algorithm’s ability to handle target maneuvers in complex environments, achieving higher tracking accuracy and robustness.&lt;/p&gt;</content:encoded></item><item><title>Traffic Scene Small Target Detection Method Based on YOLOv8n-SPTS Model for Autonomous Driving</title><link>https://arxiv.org/abs/2512.09296v1</link><guid>http://arxiv.org/abs/2512.09296v1</guid><pubDate>Wed, 10 Dec 2025 03:46:57 +0000</pubDate><dc:creator>Songhan Wu</dc:creator><dc:type>preprint</dc:type><prism:publicationName>arXiv</prism:publicationName><description>This paper focuses on the key issue in autonomous driving: small target recognition in dynamic perception. Existing algorithms suffer from poor detection performance due to missing small target information, scale imbalance, and occlusion. We propose an improved YOLOv8n-SPTS model, which enhances the detection accuracy of small traffic targets through three key innovations: First, optimizing the feature extraction module. In the Backbone Bottleneck structure of YOLOv8n, 4 traditional convolution modules are replaced with Space-to-Depth Convolution (SPD-Conv) modules. This module retains fine-grained information through space-to-depth conversion, reduces information loss, and enhances the ability to capture features of low-resolution small targets. Second, enhancing feature fusion capability. The Spatial Pyramid Pooling - Fast Cross Stage Partial Connection (SPPFCSPC) module is introduced to replace the original SPPF module, integrating the multi-scale feature extraction from Spatial Pyramid Pooling (SPP) and the feature fusion mechanism of Cross Stage Partial Connection (CSP), thereby improving the model's contextual understanding of complex scenes and multi-scale feature expression ability. Third, designing a dedicated detection structure for small targets. A Triple-Stage Feature Pyramid (TSFP) structure is proposed, which adds a 160*160 small target detection head to the original detection heads to fully utilize high-resolution features in shallow layers; meanwhile, redundant large target detection heads are removed to balance computational efficiency. Comparative experiments on the VisDrone2019-DET dataset show that YOLOv8n-SPTS model ranks first in precision (61.9%), recall (48.3%), mAP@0.5 (52.6%), and mAP@0.5:0.95 (32.6%). Visualization results verify that the miss rate of small targets such as pedestrians and bicycles in occluded and dense scenes is significantly reduced.
Published: 2025-12-10T03:46:57+00:00
Venue: arXiv
Score: 0.796 (must_read)</description><content:encoded>&lt;p&gt;&lt;strong&gt;Authors:&lt;/strong&gt; Songhan Wu&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Journal:&lt;/strong&gt; arXiv&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Relevance:&lt;/strong&gt; 0.796 (must_read)&lt;/p&gt;
&lt;p&gt;This paper focuses on the key issue in autonomous driving: small target recognition in dynamic perception. Existing algorithms suffer from poor detection performance due to missing small target information, scale imbalance, and occlusion. We propose an improved YOLOv8n-SPTS model, which enhances the detection accuracy of small traffic targets through three key innovations: First, optimizing the feature extraction module. In the Backbone Bottleneck structure of YOLOv8n, 4 traditional convolution modules are replaced with Space-to-Depth Convolution (SPD-Conv) modules. This module retains fine-grained information through space-to-depth conversion, reduces information loss, and enhances the ability to capture features of low-resolution small targets. Second, enhancing feature fusion capability. The Spatial Pyramid Pooling - Fast Cross Stage Partial Connection (SPPFCSPC) module is introduced to replace the original SPPF module, integrating the multi-scale feature extraction from Spatial Pyramid Pooling (SPP) and the feature fusion mechanism of Cross Stage Partial Connection (CSP), thereby improving the model&amp;#x27;s contextual understanding of complex scenes and multi-scale feature expression ability. Third, designing a dedicated detection structure for small targets. A Triple-Stage Feature Pyramid (TSFP) structure is proposed, which adds a 160*160 small target detection head to the original detection heads to fully utilize high-resolution features in shallow layers; meanwhile, redundant large target detection heads are removed to balance computational efficiency. Comparative experiments on the VisDrone2019-DET dataset show that YOLOv8n-SPTS model ranks first in precision (61.9%), recall (48.3%), mAP@0.5 (52.6%), and mAP@0.5:0.95 (32.6%). Visualization results verify that the miss rate of small targets such as pedestrians and bicycles in occluded and dense scenes is significantly reduced.&lt;/p&gt;</content:encoded></item><item><title>Controllable Image-Guided Generation via Dynamic Gaussian Spectral Modulation</title><link>https://doi.org/10.1016/j.eswa.2025.130809</link><guid>10.1016/j.eswa.2025.130809</guid><pubDate>Sat, 13 Dec 2025 23:34:55 +0000</pubDate><dc:creator>Shuocheng Wang</dc:creator><dc:creator>Qingfeng Wu</dc:creator><dc:creator>yuanbo Xing</dc:creator><dc:creator>mengyuan Ge</dc:creator><prism:publicationName>Expert Systems with Applications</prism:publicationName><prism:doi>10.1016/j.eswa.2025.130809</prism:doi><description>Diffusion models have achieved impressive results in image generation, but existing approaches often struggle with fine-grained control over the synthesis process, limiting their adaptability across different tasks. To address this issue, we introduce a novel diffusion framework that integrates adaptive Gaussian filtering into the denoising process, allowing dynamic modulation of structural and textural information. Furthermore, we design a Bidirectional Optimization Framework, which consists of two progressive phases: (1) Noise-to-Structure Optimization, ensuring global structural consistency through controlled spectral modulation, and (2) Structure-to-Texture Optimization, enhancing fine-grained details via gradient-based refinement. The proposed approach operates without additional training, supporting various image translation tasks, including cross-domain transformations and image to image translation. Extensive experiments on multiple datasets, including FFHQ and AFHQ, demonstrate that the proposed method achieves significant improvements over existing approaches, delivering superior generative quality and broader applicability in real-world scenarios.
Published: 2025-12-13T23:34:55+00:00
Venue: Expert Systems with Applications
Score: 0.795 (must_read)</description><content:encoded>&lt;p&gt;&lt;strong&gt;Authors:&lt;/strong&gt; Shuocheng Wang; Qingfeng Wu; yuanbo Xing; mengyuan Ge&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Journal:&lt;/strong&gt; Expert Systems with Applications&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;a href="https://doi.org/10.1016/j.eswa.2025.130809"&gt;10.1016/j.eswa.2025.130809&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Relevance:&lt;/strong&gt; 0.795 (must_read)&lt;/p&gt;
&lt;p&gt;Diffusion models have achieved impressive results in image generation, but existing approaches often struggle with fine-grained control over the synthesis process, limiting their adaptability across different tasks. To address this issue, we introduce a novel diffusion framework that integrates adaptive Gaussian filtering into the denoising process, allowing dynamic modulation of structural and textural information. Furthermore, we design a Bidirectional Optimization Framework, which consists of two progressive phases: (1) Noise-to-Structure Optimization, ensuring global structural consistency through controlled spectral modulation, and (2) Structure-to-Texture Optimization, enhancing fine-grained details via gradient-based refinement. The proposed approach operates without additional training, supporting various image translation tasks, including cross-domain transformations and image to image translation. Extensive experiments on multiple datasets, including FFHQ and AFHQ, demonstrate that the proposed method achieves significant improvements over existing approaches, delivering superior generative quality and broader applicability in real-world scenarios.&lt;/p&gt;</content:encoded></item><item><title>Hands-on Evaluation of Visual Transformers for Object Recognition and Detection</title><link>https://arxiv.org/abs/2512.09579v1</link><guid>http://arxiv.org/abs/2512.09579v1</guid><pubDate>Wed, 10 Dec 2025 12:15:48 +0000</pubDate><dc:creator>Dimitrios N. Vlachogiannis</dc:creator><dc:creator>Dimitrios A. Koutsomitropoulos</dc:creator><dc:type>preprint</dc:type><prism:publicationName>arXiv</prism:publicationName><description>Convolutional Neural Networks (CNNs) for computer vision sometimes struggle with understanding images in a global context, as they mainly focus on local patterns. On the other hand, Vision Transformers (ViTs), inspired by models originally created for language processing, use self-attention mechanisms, which allow them to understand relationships across the entire image. In this paper, we compare different types of ViTs (pure, hierarchical, and hybrid) against traditional CNN models across various tasks, including object recognition, detection, and medical image classification. We conduct thorough tests on standard datasets like ImageNet for image classification and COCO for object detection. Additionally, we apply these models to medical imaging using the ChestX-ray14 dataset. We find that hybrid and hierarchical transformers, especially Swin and CvT, offer a strong balance between accuracy and computational resources. Furthermore, by experimenting with data augmentation techniques on medical images, we discover significant performance improvements, particularly with the Swin Transformer model. Overall, our results indicate that Vision Transformers are competitive and, in many cases, outperform traditional CNNs, especially in scenarios requiring the understanding of global visual contexts like medical imaging.
Published: 2025-12-10T12:15:48+00:00
Venue: arXiv
Score: 0.794 (must_read)</description><content:encoded>&lt;p&gt;&lt;strong&gt;Authors:&lt;/strong&gt; Dimitrios N. Vlachogiannis; Dimitrios A. Koutsomitropoulos&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Journal:&lt;/strong&gt; arXiv&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Relevance:&lt;/strong&gt; 0.794 (must_read)&lt;/p&gt;
&lt;p&gt;Convolutional Neural Networks (CNNs) for computer vision sometimes struggle with understanding images in a global context, as they mainly focus on local patterns. On the other hand, Vision Transformers (ViTs), inspired by models originally created for language processing, use self-attention mechanisms, which allow them to understand relationships across the entire image. In this paper, we compare different types of ViTs (pure, hierarchical, and hybrid) against traditional CNN models across various tasks, including object recognition, detection, and medical image classification. We conduct thorough tests on standard datasets like ImageNet for image classification and COCO for object detection. Additionally, we apply these models to medical imaging using the ChestX-ray14 dataset. We find that hybrid and hierarchical transformers, especially Swin and CvT, offer a strong balance between accuracy and computational resources. Furthermore, by experimenting with data augmentation techniques on medical images, we discover significant performance improvements, particularly with the Swin Transformer model. Overall, our results indicate that Vision Transformers are competitive and, in many cases, outperform traditional CNNs, especially in scenarios requiring the understanding of global visual contexts like medical imaging.&lt;/p&gt;</content:encoded></item><item><title>Recent Advances in Discrete Speech Tokens: A Review</title><link>https://doi.org/10.1109/tpami.2025.3643619</link><guid>10.1109/tpami.2025.3643619</guid><pubDate>Fri, 12 Dec 2025 18:35:38 +0000</pubDate><dc:creator>Yiwei Guo</dc:creator><dc:creator>Zhihan Li</dc:creator><dc:creator>Hankun Wang</dc:creator><dc:creator>Bohan Li</dc:creator><dc:creator>Chongtian Shao</dc:creator><dc:creator>Hanglei Zhang</dc:creator><dc:creator>Chenpeng Du</dc:creator><dc:creator>Xie Chen</dc:creator><dc:creator>Shujie Liu</dc:creator><dc:creator>Kai Yu</dc:creator><prism:publicationName>IEEE Transactions on Pattern Analysis and Machine Intelligence</prism:publicationName><prism:doi>10.1109/tpami.2025.3643619</prism:doi><description>The rapid advancement of speech generation technologies in the era of large language models (LLMs) has established discrete speech tokens as a foundational paradigm for speech representation. These tokens, characterized by their discrete, compact, and concise nature, are not only advantageous for efficient transmission and storage, but also inherently compatible with the language modeling framework, enabling seamless integration of speech into text-dominated LLM architectures. Current research categorizes discrete speech tokens into two principal classes: acoustic tokens and semantic tokens, each of which has evolved into a rich research domain characterized by unique design philosophies and methodological approaches. This survey systematically synthesizes the existing taxonomy and recent innovations in discrete speech tokenization, conducts a critical examination of the strengths and limitations of each paradigm, and presents systematic experimental comparisons across token types. Furthermore, we identify persistent challenges in the field and propose potential research directions, aiming to offer actionable insights to inspire future advancements in the development and application of discrete speech tokens.
Published: 2025-12-12T18:35:38+00:00
Venue: IEEE Transactions on Pattern Analysis and Machine Intelligence
Score: 0.794 (must_read)</description><content:encoded>&lt;p&gt;&lt;strong&gt;Authors:&lt;/strong&gt; Yiwei Guo; Zhihan Li; Hankun Wang; Bohan Li; Chongtian Shao; Hanglei Zhang; Chenpeng Du; Xie Chen; Shujie Liu; Kai Yu&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Journal:&lt;/strong&gt; IEEE Transactions on Pattern Analysis and Machine Intelligence&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;a href="https://doi.org/10.1109/tpami.2025.3643619"&gt;10.1109/tpami.2025.3643619&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Relevance:&lt;/strong&gt; 0.794 (must_read)&lt;/p&gt;
&lt;p&gt;The rapid advancement of speech generation technologies in the era of large language models (LLMs) has established discrete speech tokens as a foundational paradigm for speech representation. These tokens, characterized by their discrete, compact, and concise nature, are not only advantageous for efficient transmission and storage, but also inherently compatible with the language modeling framework, enabling seamless integration of speech into text-dominated LLM architectures. Current research categorizes discrete speech tokens into two principal classes: acoustic tokens and semantic tokens, each of which has evolved into a rich research domain characterized by unique design philosophies and methodological approaches. This survey systematically synthesizes the existing taxonomy and recent innovations in discrete speech tokenization, conducts a critical examination of the strengths and limitations of each paradigm, and presents systematic experimental comparisons across token types. Furthermore, we identify persistent challenges in the field and propose potential research directions, aiming to offer actionable insights to inspire future advancements in the development and application of discrete speech tokens.&lt;/p&gt;</content:encoded></item><item><title>LiM-YOLO: Less is More with Pyramid Level Shift and Normalized Auxiliary Branch for Ship Detection in Optical Remote Sensing Imagery</title><link>https://arxiv.org/abs/2512.09700v1</link><guid>http://arxiv.org/abs/2512.09700v1</guid><pubDate>Wed, 10 Dec 2025 14:48:58 +0000</pubDate><dc:creator>Seon-Hoon Kim</dc:creator><dc:creator>Hyeji Sim</dc:creator><dc:creator>Youeyun Jung</dc:creator><dc:creator>Ok-Chul Jung</dc:creator><dc:creator>Yerin Kim</dc:creator><dc:type>preprint</dc:type><prism:publicationName>arXiv</prism:publicationName><description>Applying general-purpose object detectors to ship detection in satellite imagery presents significant challenges due to the extreme scale disparity and morphological anisotropy of maritime targets. Standard architectures utilizing stride-32 (P5) layers often fail to resolve narrow vessels, resulting in spatial feature dilution. In this work, we propose LiM-YOLO, a specialized detector designed to resolve these domain-specific conflicts. Based on a statistical analysis of ship scales, we introduce a Pyramid Level Shift Strategy that reconfigures the detection head to P2-P4. This shift ensures compliance with Nyquist sampling criteria for small objects while eliminating the computational redundancy of deep layers. To further enhance training stability on high-resolution inputs, we incorporate a Group Normalized Convolutional Block for Linear Projection (GN-CBLinear), which mitigates gradient volatility in micro-batch settings. Validated on SODA-A, DOTA-v1.5, FAIR1M-v2.0, and ShipRSImageNet-V1, LiM-YOLO demonstrates superior detection accuracy and efficiency compared to state-of-the-art models. The code is available at https://github.com/egshkim/LiM-YOLO.
Published: 2025-12-10T14:48:58+00:00
Venue: arXiv
Score: 0.793 (must_read)</description><content:encoded>&lt;p&gt;&lt;strong&gt;Authors:&lt;/strong&gt; Seon-Hoon Kim; Hyeji Sim; Youeyun Jung; Ok-Chul Jung; Yerin Kim&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Journal:&lt;/strong&gt; arXiv&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Relevance:&lt;/strong&gt; 0.793 (must_read)&lt;/p&gt;
&lt;p&gt;Applying general-purpose object detectors to ship detection in satellite imagery presents significant challenges due to the extreme scale disparity and morphological anisotropy of maritime targets. Standard architectures utilizing stride-32 (P5) layers often fail to resolve narrow vessels, resulting in spatial feature dilution. In this work, we propose LiM-YOLO, a specialized detector designed to resolve these domain-specific conflicts. Based on a statistical analysis of ship scales, we introduce a Pyramid Level Shift Strategy that reconfigures the detection head to P2-P4. This shift ensures compliance with Nyquist sampling criteria for small objects while eliminating the computational redundancy of deep layers. To further enhance training stability on high-resolution inputs, we incorporate a Group Normalized Convolutional Block for Linear Projection (GN-CBLinear), which mitigates gradient volatility in micro-batch settings. Validated on SODA-A, DOTA-v1.5, FAIR1M-v2.0, and ShipRSImageNet-V1, LiM-YOLO demonstrates superior detection accuracy and efficiency compared to state-of-the-art models. The code is available at https://github.com/egshkim/LiM-YOLO.&lt;/p&gt;</content:encoded></item><item><title>Moving vehicles tracking from satellite video data based on spatiotemporal high-order relation learning and reasoning</title><link>https://doi.org/10.1016/j.jag.2025.105015</link><guid>10.1016/j.jag.2025.105015</guid><pubDate>Sat, 13 Dec 2025 21:59:02 +0000</pubDate><dc:creator>Ziyuan Feng</dc:creator><dc:creator>Xianfeng Zhang</dc:creator><dc:creator>Bo Zhou</dc:creator><dc:creator>Miao Ren</dc:creator><dc:creator>Xiaobo Zhi</dc:creator><prism:publicationName>International Journal of Applied Earth Observation and Geoinformation</prism:publicationName><prism:doi>10.1016/j.jag.2025.105015</prism:doi><description>Tracking moving vehicles in satellite videos presents several challenges, including complex background interference and the difficulty of detecting small targets. Most existing multiple object tracking (MOT) methods utilize convolutional models to capture local semantics or self-attention mechanisms to address global semantics for moving target detection. However, these methods tend to struggle with small and visually similar targets, making them particularly vulnerable to complex background interference, which often results in a large number of false positives and missed detections. Furthermore, many current approaches rely on the Hungarian matching algorithm or other intricate, unlearnable association optimization methods to achieve effective tracking once relevant information is gathered. This reliance often yields suboptimal outputs from the network models. To tackle these issues, this article presents an end-to-end graph network based on spatiotemporal high-order relation learning and reasoning for vehicle tracking in satellite video. The representation module of spatial high-order relations is designed to capture the spatial high-order relations between moving vehicles and their local environments, as well as global key references. Meanwhile, the temporal semantic reasoning module focuses on analyzing the evolution of these spatial high-order relations over time, thereby constructing the spatiotemporal high-order connections among the targets of interest and ensuring the continuous and stable detection of moving vehicles. Ultimately, a graph network based on spatiotemporal high-order relation reasoning is developed to perform learnable associations of target information across video frames, achieving a globally optimal solution to the tracking problem. Comparative experiments on the SatVideoDT, CGSTL, and ShuangQing-1 satellite video datasets demonstrate that the proposed method effectively enables end-to-end tracking of moving vehicles, attaining state-of-the-art performance across most evaluation metrics. On the SatVideoDT dataset, the model achieves a Multiple Object Tracking Accuracy (MOTA) of 65.1% and an Identity F1 Score (IDF1) of 70.9%. The proposed network model holds significant promise for the automated interpretation of satellite video data. The code is available at https://github.com/zsspo/GHOST-R.
Published: 2025-12-13T21:59:02+00:00
Venue: International Journal of Applied Earth Observation and Geoinformation
Score: 0.792 (must_read)</description><content:encoded>&lt;p&gt;&lt;strong&gt;Authors:&lt;/strong&gt; Ziyuan Feng; Xianfeng Zhang; Bo Zhou; Miao Ren; Xiaobo Zhi&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Journal:&lt;/strong&gt; International Journal of Applied Earth Observation and Geoinformation&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;a href="https://doi.org/10.1016/j.jag.2025.105015"&gt;10.1016/j.jag.2025.105015&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Relevance:&lt;/strong&gt; 0.792 (must_read)&lt;/p&gt;
&lt;p&gt;Tracking moving vehicles in satellite videos presents several challenges, including complex background interference and the difficulty of detecting small targets. Most existing multiple object tracking (MOT) methods utilize convolutional models to capture local semantics or self-attention mechanisms to address global semantics for moving target detection. However, these methods tend to struggle with small and visually similar targets, making them particularly vulnerable to complex background interference, which often results in a large number of false positives and missed detections. Furthermore, many current approaches rely on the Hungarian matching algorithm or other intricate, unlearnable association optimization methods to achieve effective tracking once relevant information is gathered. This reliance often yields suboptimal outputs from the network models. To tackle these issues, this article presents an end-to-end graph network based on spatiotemporal high-order relation learning and reasoning for vehicle tracking in satellite video. The representation module of spatial high-order relations is designed to capture the spatial high-order relations between moving vehicles and their local environments, as well as global key references. Meanwhile, the temporal semantic reasoning module focuses on analyzing the evolution of these spatial high-order relations over time, thereby constructing the spatiotemporal high-order connections among the targets of interest and ensuring the continuous and stable detection of moving vehicles. Ultimately, a graph network based on spatiotemporal high-order relation reasoning is developed to perform learnable associations of target information across video frames, achieving a globally optimal solution to the tracking problem. Comparative experiments on the SatVideoDT, CGSTL, and ShuangQing-1 satellite video datasets demonstrate that the proposed method effectively enables end-to-end tracking of moving vehicles, attaining state-of-the-art performance across most evaluation metrics. On the SatVideoDT dataset, the model achieves a Multiple Object Tracking Accuracy (MOTA) of 65.1% and an Identity F1 Score (IDF1) of 70.9%. The proposed network model holds significant promise for the automated interpretation of satellite video data. The code is available at https://github.com/zsspo/GHOST-R.&lt;/p&gt;</content:encoded></item><item><title>A prototype-based semi-supervised learning method for few-shot SAR target recognition</title><link>https://doi.org/10.1109/jstars.2025.3643525</link><guid>10.1109/jstars.2025.3643525</guid><pubDate>Fri, 12 Dec 2025 18:36:08 +0000</pubDate><dc:creator>Ruikang Hu</dc:creator><dc:creator>Ye Li</dc:creator><dc:creator>Haiyan Zhu</dc:creator><dc:creator>Xu Lan</dc:creator><dc:creator>Li Liu</dc:creator><dc:creator>Jingyuan Xia</dc:creator><prism:publicationName>IEEE Journal of Selected Topics in Applied Earth Observations and Remote Sensing</prism:publicationName><prism:doi>10.1109/jstars.2025.3643525</prism:doi><description>Deep learning based methods have achieved extraordinary success in SAR automatic target recognition. However, deep learning conventionally necessitates a substantial number of labeled samples to achieve effective training, and labeled samples of new classes in real-world scenarios are scarce, which limits the performance of existing methods in the few-shot task. In response to this issue, this paper proposes a prototype based semi-supervised learning method for few-shot SAR target recognition, named WST-DRFSL. The method consists of two stages: the base learning stage and the dynamic refinement stage. In the first stage, a robust encoder is trained on both labeled and unlabeled samples of base classes via Consistency Regularization (CR). Then, in the second stage, pseudo-labels and CR are iteratively applied to new classes' few labeled samples and abundant unlabeled samples to achieve superior new-class recognition performance. Furthermore, the Wavelet Scattering Transform (WST) is employed in both stages to fully exploit the scattering characteristics of SAR images. Extensive simulations on MSTAR, FUSAR, OpenSARShip, and SAMPLE datasets have demonstrated that the proposed method surpasses the state-of the-art recognition accuracy on the few-shot learning tasks. The code is available at https://github.com/Cthanta/WST-DRFSL.
Published: 2025-12-12T18:36:08+00:00
Venue: IEEE Journal of Selected Topics in Applied Earth Observations and Remote Sensing
Score: 0.791 (must_read)</description><content:encoded>&lt;p&gt;&lt;strong&gt;Authors:&lt;/strong&gt; Ruikang Hu; Ye Li; Haiyan Zhu; Xu Lan; Li Liu; Jingyuan Xia&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Journal:&lt;/strong&gt; IEEE Journal of Selected Topics in Applied Earth Observations and Remote Sensing&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;a href="https://doi.org/10.1109/jstars.2025.3643525"&gt;10.1109/jstars.2025.3643525&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Relevance:&lt;/strong&gt; 0.791 (must_read)&lt;/p&gt;
&lt;p&gt;Deep learning based methods have achieved extraordinary success in SAR automatic target recognition. However, deep learning conventionally necessitates a substantial number of labeled samples to achieve effective training, and labeled samples of new classes in real-world scenarios are scarce, which limits the performance of existing methods in the few-shot task. In response to this issue, this paper proposes a prototype based semi-supervised learning method for few-shot SAR target recognition, named WST-DRFSL. The method consists of two stages: the base learning stage and the dynamic refinement stage. In the first stage, a robust encoder is trained on both labeled and unlabeled samples of base classes via Consistency Regularization (CR). Then, in the second stage, pseudo-labels and CR are iteratively applied to new classes&amp;#x27; few labeled samples and abundant unlabeled samples to achieve superior new-class recognition performance. Furthermore, the Wavelet Scattering Transform (WST) is employed in both stages to fully exploit the scattering characteristics of SAR images. Extensive simulations on MSTAR, FUSAR, OpenSARShip, and SAMPLE datasets have demonstrated that the proposed method surpasses the state-of the-art recognition accuracy on the few-shot learning tasks. The code is available at https://github.com/Cthanta/WST-DRFSL.&lt;/p&gt;</content:encoded></item><item><title>Redundancy Mitigation: Towards Accurate and Efficient Image-Text Retrieval</title><link>https://doi.org/10.1109/tcsvt.2025.3643601</link><guid>10.1109/tcsvt.2025.3643601</guid><pubDate>Fri, 12 Dec 2025 18:37:50 +0000</pubDate><dc:creator>Kun Wang</dc:creator><dc:creator>Yupeng Hu</dc:creator><dc:creator>Hao Liu</dc:creator><dc:creator>Lirong Jie</dc:creator><dc:creator>Liqiang Nie</dc:creator><prism:publicationName>IEEE Transactions on Circuits and Systems for Video Technology</prism:publicationName><prism:doi>10.1109/tcsvt.2025.3643601</prism:doi><description>Image-text retrieval (ITR) is a pivotal task in cross-modal research. However, existing methods often suffer from a fundamental yet overlooked challenge: redundancy. This issue manifests as both semantic redundancy within unimodal representations and relationship redundancy in cross-modal alignments. This not only inflates computational costs but also degrades retrieval accuracy by masking salient features and reinforcing spurious correlations. In this work, we are the first to explicitly analyze and address the ITR problem from a redundancy perspective by proposing the iMage-text rEtrieval rEdundancy miTigation (MEET) framework. MEET employs a cascaded, two-stage process to systematically mitigate both forms of redundancy. First, for Semantic Redundancy Mitigation, it repurposes deep hashing and quantization as synergistic tools, producing compact yet highly discriminative representations. Second, for Relationship Redundancy Mitigation, it progressively refines the cross-modal alignment space by filtering misleading negative samples and adaptively reweighting informative pairs. The structural integration of these modules under a unified optimization objective provides a clear and interpretable pathway to retrieval. Extensive experiments on multiple benchmarks demonstrate that MEET consistently surpasses state-of-the-art methods, validating its effectiveness and generalizability.
Published: 2025-12-12T18:37:50+00:00
Venue: IEEE Transactions on Circuits and Systems for Video Technology
Score: 0.790 (must_read)</description><content:encoded>&lt;p&gt;&lt;strong&gt;Authors:&lt;/strong&gt; Kun Wang; Yupeng Hu; Hao Liu; Lirong Jie; Liqiang Nie&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Journal:&lt;/strong&gt; IEEE Transactions on Circuits and Systems for Video Technology&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;a href="https://doi.org/10.1109/tcsvt.2025.3643601"&gt;10.1109/tcsvt.2025.3643601&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Relevance:&lt;/strong&gt; 0.790 (must_read)&lt;/p&gt;
&lt;p&gt;Image-text retrieval (ITR) is a pivotal task in cross-modal research. However, existing methods often suffer from a fundamental yet overlooked challenge: redundancy. This issue manifests as both semantic redundancy within unimodal representations and relationship redundancy in cross-modal alignments. This not only inflates computational costs but also degrades retrieval accuracy by masking salient features and reinforcing spurious correlations. In this work, we are the first to explicitly analyze and address the ITR problem from a redundancy perspective by proposing the iMage-text rEtrieval rEdundancy miTigation (MEET) framework. MEET employs a cascaded, two-stage process to systematically mitigate both forms of redundancy. First, for Semantic Redundancy Mitigation, it repurposes deep hashing and quantization as synergistic tools, producing compact yet highly discriminative representations. Second, for Relationship Redundancy Mitigation, it progressively refines the cross-modal alignment space by filtering misleading negative samples and adaptively reweighting informative pairs. The structural integration of these modules under a unified optimization objective provides a clear and interpretable pathway to retrieval. Extensive experiments on multiple benchmarks demonstrate that MEET consistently surpasses state-of-the-art methods, validating its effectiveness and generalizability.&lt;/p&gt;</content:encoded></item><item><title>SPAX: Fully Sparse Framework with Hierarchical Spatio-temporal Fusion for Moving Object Tracking in Satellite Videos</title><link>https://doi.org/10.1109/tgrs.2025.3643466</link><guid>10.1109/tgrs.2025.3643466</guid><pubDate>Fri, 12 Dec 2025 18:35:46 +0000</pubDate><dc:creator>Zhehao Xiao</dc:creator><dc:creator>Fang Xu</dc:creator><dc:creator>Chuandong Liu</dc:creator><dc:creator>Wen Yang</dc:creator><dc:creator>Gui-Song Xia</dc:creator><prism:publicationName>IEEE Transactions on Geoscience and Remote Sensing</prism:publicationName><prism:doi>10.1109/tgrs.2025.3643466</prism:doi><description>Moving object tracking is a fundamental task in video satellite technologies. Remote sensing scenes feature small objects and large background ratios in spatial dimension, leading existing methods characterized by dense computation to incur considerable unnecessary computational overhead of redundant regions. Moreover, the coupled motion between the satellite platform and ground objects introduces temporal complexity that current methods find difficult to achieve accurate dynamic perception. To address these issues, we propose a fully sparse framework with hierarchical spatio-temporal fusion (SPAX). Specifically, SPAX utilizes an object-centric fully-sparse paradigm to reduce computational redundancy by focusing only on foreground regions. Furthermore, we adopt the hierarchical spatio-temporal fusion (HSF) to address the complexity of dual-motion coupling through intra-frame multi-scale feature fusion, inter-frame symmetric feature interaction, and inter-frame asymmetric feature interaction, thereby enabling comprehensive temporal information utilization. Additionally, we propose a plug-and-play Gaussian-based trajectory association (GTA) strategy to mitigate the negative impact of observational drifts and accumulated errors. Experiments show that SPAX outperforms previous methods on two popular benchmarks, achieving notable improvements of 5.1 and 7.6 on MOTA. While achieving the state-of-the-art (SOTA) performance, SPAX reduces GFLOPs by 88.4% and delivers a 2.7× speedup on SatVideoDT dataset, along with 93.2% GFLOPs reduction and up to a 3.1× acceleration on SatMTB-MOT dataset compared to our baseline. Furthermore, SPAX-Light outperforms the previous SOTA method by 6.6 MOTA and runs at 5.9× its inference speed on SatMTB-MOT dataset. Our project page: https://lebron-2016.github.io/SPAX-page.
Published: 2025-12-12T18:35:46+00:00
Venue: IEEE Transactions on Geoscience and Remote Sensing
Score: 0.790 (must_read)</description><content:encoded>&lt;p&gt;&lt;strong&gt;Authors:&lt;/strong&gt; Zhehao Xiao; Fang Xu; Chuandong Liu; Wen Yang; Gui-Song Xia&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Journal:&lt;/strong&gt; IEEE Transactions on Geoscience and Remote Sensing&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;a href="https://doi.org/10.1109/tgrs.2025.3643466"&gt;10.1109/tgrs.2025.3643466&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Relevance:&lt;/strong&gt; 0.790 (must_read)&lt;/p&gt;
&lt;p&gt;Moving object tracking is a fundamental task in video satellite technologies. Remote sensing scenes feature small objects and large background ratios in spatial dimension, leading existing methods characterized by dense computation to incur considerable unnecessary computational overhead of redundant regions. Moreover, the coupled motion between the satellite platform and ground objects introduces temporal complexity that current methods find difficult to achieve accurate dynamic perception. To address these issues, we propose a fully sparse framework with hierarchical spatio-temporal fusion (SPAX). Specifically, SPAX utilizes an object-centric fully-sparse paradigm to reduce computational redundancy by focusing only on foreground regions. Furthermore, we adopt the hierarchical spatio-temporal fusion (HSF) to address the complexity of dual-motion coupling through intra-frame multi-scale feature fusion, inter-frame symmetric feature interaction, and inter-frame asymmetric feature interaction, thereby enabling comprehensive temporal information utilization. Additionally, we propose a plug-and-play Gaussian-based trajectory association (GTA) strategy to mitigate the negative impact of observational drifts and accumulated errors. Experiments show that SPAX outperforms previous methods on two popular benchmarks, achieving notable improvements of 5.1 and 7.6 on MOTA. While achieving the state-of-the-art (SOTA) performance, SPAX reduces GFLOPs by 88.4% and delivers a 2.7× speedup on SatVideoDT dataset, along with 93.2% GFLOPs reduction and up to a 3.1× acceleration on SatMTB-MOT dataset compared to our baseline. Furthermore, SPAX-Light outperforms the previous SOTA method by 6.6 MOTA and runs at 5.9× its inference speed on SatMTB-MOT dataset. Our project page: https://lebron-2016.github.io/SPAX-page.&lt;/p&gt;</content:encoded></item><item><title>MODA: The First Challenging Benchmark for Multispectral Object Detection in Aerial Images</title><link>https://arxiv.org/abs/2512.09489v1</link><guid>http://arxiv.org/abs/2512.09489v1</guid><pubDate>Wed, 10 Dec 2025 10:07:06 +0000</pubDate><dc:creator>Shuaihao Han</dc:creator><dc:creator>Tingfa Xu</dc:creator><dc:creator>Peifu Liu</dc:creator><dc:creator>Jianan Li</dc:creator><dc:type>preprint</dc:type><prism:publicationName>arXiv</prism:publicationName><description>Aerial object detection faces significant challenges in real-world scenarios, such as small objects and extensive background interference, which limit the performance of RGB-based detectors with insufficient discriminative information. Multispectral images (MSIs) capture additional spectral cues across multiple bands, offering a promising alternative. However, the lack of training data has been the primary bottleneck to exploiting the potential of MSIs. To address this gap, we introduce the first large-scale dataset for Multispectral Object Detection in Aerial images (MODA), which comprises 14,041 MSIs and 330,191 annotations across diverse, challenging scenarios, providing a comprehensive data foundation for this field. Furthermore, to overcome challenges inherent to aerial object detection using MSIs, we propose OSSDet, a framework that integrates spectral and spatial information with object-aware cues. OSSDet employs a cascaded spectral-spatial modulation structure to optimize target perception, aggregates spectrally related features by exploiting spectral similarities to reinforce intra-object correlations, and suppresses irrelevant background via object-aware masking. Moreover, cross-spectral attention further refines object-related representations under explicit object-aware guidance. Extensive experiments demonstrate that OSSDet outperforms existing methods with comparable parameters and efficiency.
Published: 2025-12-10T10:07:06+00:00
Venue: arXiv
Score: 0.787 (must_read)</description><content:encoded>&lt;p&gt;&lt;strong&gt;Authors:&lt;/strong&gt; Shuaihao Han; Tingfa Xu; Peifu Liu; Jianan Li&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Journal:&lt;/strong&gt; arXiv&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Relevance:&lt;/strong&gt; 0.787 (must_read)&lt;/p&gt;
&lt;p&gt;Aerial object detection faces significant challenges in real-world scenarios, such as small objects and extensive background interference, which limit the performance of RGB-based detectors with insufficient discriminative information. Multispectral images (MSIs) capture additional spectral cues across multiple bands, offering a promising alternative. However, the lack of training data has been the primary bottleneck to exploiting the potential of MSIs. To address this gap, we introduce the first large-scale dataset for Multispectral Object Detection in Aerial images (MODA), which comprises 14,041 MSIs and 330,191 annotations across diverse, challenging scenarios, providing a comprehensive data foundation for this field. Furthermore, to overcome challenges inherent to aerial object detection using MSIs, we propose OSSDet, a framework that integrates spectral and spatial information with object-aware cues. OSSDet employs a cascaded spectral-spatial modulation structure to optimize target perception, aggregates spectrally related features by exploiting spectral similarities to reinforce intra-object correlations, and suppresses irrelevant background via object-aware masking. Moreover, cross-spectral attention further refines object-related representations under explicit object-aware guidance. Extensive experiments demonstrate that OSSDet outperforms existing methods with comparable parameters and efficiency.&lt;/p&gt;</content:encoded></item><item><title>Self-Supervised Spatial-Temporal Consistency for Source-Free Domain Adaptive Segmentation in Remote Sensing Images</title><link>https://doi.org/10.1109/tgrs.2025.3643744</link><guid>10.1109/tgrs.2025.3643744</guid><pubDate>Fri, 12 Dec 2025 18:35:46 +0000</pubDate><dc:creator>Zhihao Xi</dc:creator><dc:creator>Yu Meng</dc:creator><dc:creator>Yupeng Deng</dc:creator><dc:creator>Yuman Feng</dc:creator><dc:creator>Diyou Liu</dc:creator><dc:creator>Jingbo Chen</dc:creator><prism:publicationName>IEEE Transactions on Geoscience and Remote Sensing</prism:publicationName><prism:doi>10.1109/tgrs.2025.3643744</prism:doi><description>In conventional Unsupervised domain adaptation (UDA), model knowledge is transferred to the target domain with access to annotated source data, which is an unsuitable strategy for cross-domain scenarios involving data privacy and confidentiality. In this paper, we focus on source-free domain adaptation (SFDA) for semantic segmentation tasks, which adapts source-trained models to unannotated target domains without relying on source data. Self-training paradigms dominate the existing approaches, but often suffer from significant performance degradation due to unreliable pseudolabels and knowledge forgetting. To address these challenges, we propose innovative self-supervised spatial–temporal consistency learning for source-free domain adaptive segmentation (S3T-SFDA). Specifically, to address pseudolabel ambiguity in various spatial contexts, a spatial multiview consistency (SMVC) mechanism is proposed to constrain the semantic consistency across different spatial views of the same image. To mitigate the knowledge forgetting problem caused by a lack of supervisory information, a temporal dynamic consistency (TDC) mechanism, which harnesses historical knowledge consistency to regularize the current model evolution direction, is proposed. Furthermore, to improve the category-discriminative representation capabilities under domain shifts, a spatial-temporal contrastive (STC) strategy is designed to promote the intrinsic semantic association of features belonging to different categories in the target domain. Extensive experiments conducted on two domain-adaptive remote sensing (RS) segmentation benchmarks. The results demonstrate the flexibility of the proposed method when integrated into various advanced segmentation architectures, as well as its excellent generalization performance across different cross-domain RS scenarios.
Published: 2025-12-12T18:35:46+00:00
Venue: IEEE Transactions on Geoscience and Remote Sensing
Score: 0.785 (must_read)</description><content:encoded>&lt;p&gt;&lt;strong&gt;Authors:&lt;/strong&gt; Zhihao Xi; Yu Meng; Yupeng Deng; Yuman Feng; Diyou Liu; Jingbo Chen&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Journal:&lt;/strong&gt; IEEE Transactions on Geoscience and Remote Sensing&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;a href="https://doi.org/10.1109/tgrs.2025.3643744"&gt;10.1109/tgrs.2025.3643744&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Relevance:&lt;/strong&gt; 0.785 (must_read)&lt;/p&gt;
&lt;p&gt;In conventional Unsupervised domain adaptation (UDA), model knowledge is transferred to the target domain with access to annotated source data, which is an unsuitable strategy for cross-domain scenarios involving data privacy and confidentiality. In this paper, we focus on source-free domain adaptation (SFDA) for semantic segmentation tasks, which adapts source-trained models to unannotated target domains without relying on source data. Self-training paradigms dominate the existing approaches, but often suffer from significant performance degradation due to unreliable pseudolabels and knowledge forgetting. To address these challenges, we propose innovative self-supervised spatial–temporal consistency learning for source-free domain adaptive segmentation (S3T-SFDA). Specifically, to address pseudolabel ambiguity in various spatial contexts, a spatial multiview consistency (SMVC) mechanism is proposed to constrain the semantic consistency across different spatial views of the same image. To mitigate the knowledge forgetting problem caused by a lack of supervisory information, a temporal dynamic consistency (TDC) mechanism, which harnesses historical knowledge consistency to regularize the current model evolution direction, is proposed. Furthermore, to improve the category-discriminative representation capabilities under domain shifts, a spatial-temporal contrastive (STC) strategy is designed to promote the intrinsic semantic association of features belonging to different categories in the target domain. Extensive experiments conducted on two domain-adaptive remote sensing (RS) segmentation benchmarks. The results demonstrate the flexibility of the proposed method when integrated into various advanced segmentation architectures, as well as its excellent generalization performance across different cross-domain RS scenarios.&lt;/p&gt;</content:encoded></item><item><title>Glob-Diffusion: A Global Consistent Diffusion Model for Large-Scale Image Generation</title><link>https://doi.org/10.1109/tcsvt.2025.3643743</link><guid>10.1109/tcsvt.2025.3643743</guid><pubDate>Fri, 12 Dec 2025 18:37:50 +0000</pubDate><dc:creator>Yuhan Kang</dc:creator><dc:creator>Hengcan Shi</dc:creator><dc:creator>Hao Liu</dc:creator><dc:creator>Weiying Xie</dc:creator><dc:creator>Leyuan Fang</dc:creator><dc:creator>Lorenzo Bruzzone</dc:creator><prism:publicationName>IEEE Transactions on Circuits and Systems for Video Technology</prism:publicationName><prism:doi>10.1109/tcsvt.2025.3643743</prism:doi><description>Large-scale images play a crucial role in geospatial surveying, as they cover an extensively broad view and diverse objects. Due to computational limitations, existing methods rely on generating large-scale images in patches. However, the lack of global guidance in these methods often leads to significant logical errors among different patches. To address this issue, we propose a Global Consistency Diffusion model (Glob-Diffusion) for large-scale image generation. The core idea is to utilize the global consistency of small-scale images to guide the generation of large-scale images. Specifically, we introduce a Hierarchical Distributed Guidance (HDG) module that extracts patch prompts with different semantic hierarchies from small-scale images, distributedly embedding them into the generation of large-scale images to maintain global consistency across various regions. In addition, we further design a Region Guided Adapter (RGA) that dynamically optimizes the guidance strength of patch prompts by comparing differences across generated regions, effectively improving the realism of large-scale images. Our method demonstrates remarkable visual synthesis results across various natural scenes, effectively preserving global consistency in large-scale images, and also significantly enhancing the generation quality of large-scale remote sensing images. Code will be available at https://github.com/kyh433/Glob-Diffusion.
Published: 2025-12-12T18:37:50+00:00
Venue: IEEE Transactions on Circuits and Systems for Video Technology
Score: 0.783 (must_read)</description><content:encoded>&lt;p&gt;&lt;strong&gt;Authors:&lt;/strong&gt; Yuhan Kang; Hengcan Shi; Hao Liu; Weiying Xie; Leyuan Fang; Lorenzo Bruzzone&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Journal:&lt;/strong&gt; IEEE Transactions on Circuits and Systems for Video Technology&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;a href="https://doi.org/10.1109/tcsvt.2025.3643743"&gt;10.1109/tcsvt.2025.3643743&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Relevance:&lt;/strong&gt; 0.783 (must_read)&lt;/p&gt;
&lt;p&gt;Large-scale images play a crucial role in geospatial surveying, as they cover an extensively broad view and diverse objects. Due to computational limitations, existing methods rely on generating large-scale images in patches. However, the lack of global guidance in these methods often leads to significant logical errors among different patches. To address this issue, we propose a Global Consistency Diffusion model (Glob-Diffusion) for large-scale image generation. The core idea is to utilize the global consistency of small-scale images to guide the generation of large-scale images. Specifically, we introduce a Hierarchical Distributed Guidance (HDG) module that extracts patch prompts with different semantic hierarchies from small-scale images, distributedly embedding them into the generation of large-scale images to maintain global consistency across various regions. In addition, we further design a Region Guided Adapter (RGA) that dynamically optimizes the guidance strength of patch prompts by comparing differences across generated regions, effectively improving the realism of large-scale images. Our method demonstrates remarkable visual synthesis results across various natural scenes, effectively preserving global consistency in large-scale images, and also significantly enhancing the generation quality of large-scale remote sensing images. Code will be available at https://github.com/kyh433/Glob-Diffusion.&lt;/p&gt;</content:encoded></item><item><title>Oriented Vehicle Joint Detection and Tracking in Satellite Video via Identifier-free Point Supervision</title><link>https://doi.org/10.1109/tgrs.2025.3643494</link><guid>10.1109/tgrs.2025.3643494</guid><pubDate>Fri, 12 Dec 2025 18:35:46 +0000</pubDate><dc:creator>Yuping Liang</dc:creator><dc:creator>Jinjian Wu</dc:creator><dc:creator>Junpeng Zhang</dc:creator><dc:creator>Yuxuan Chang</dc:creator><dc:creator>Jie Feng</dc:creator><dc:creator>Guangming Shi</dc:creator><prism:publicationName>IEEE Transactions on Geoscience and Remote Sensing</prism:publicationName><prism:doi>10.1109/tgrs.2025.3643494</prism:doi><description>Oriented vehicle detection and tracking play a crucial role in various real-world applications. Yet, existing advanced models heavily rely on abundant and accurate oriented bounding box and tracking identifier annotations, which are extremely labor-intensive for satellite videos. In this paper, we endeavor to employ identifier-free point annotation to achieve the competitive performance while minimizing annotation costs. Specifically, each instance across video frames are labeled by single points, without providing its instance identifier. Building upon this setting, we introduce an oriented vehicle joint detection and tracking framework for satellite video, focusing on enhancing model performance by carefully-designed sample acquisition and robust learning processes. Firstly, we leverage temporal and visual information to generate sequence-aligned pseudo-labels and visually-aligned synthetic objects, which complement each other during training by providing both exact appearance and annotation information. Secondly, a novel spatio-temporal consistency metric is developed to assess sample quality, which is then incorporated into a curriculum learning schedule. This strategy facilitates a gradual learning progression from high-quality data to low-quality or noisy examples, thereyby minimizing interference from potentially misleading samples. Finally, an end-to-end oriented object joint detection and tracking network is constructed to enable effective oriented vehicle dynamic analysis. Extensive ablation and experimental results on two satellite video datasets demonstrate the superiority of our proposed method.
Published: 2025-12-12T18:35:46+00:00
Venue: IEEE Transactions on Geoscience and Remote Sensing
Score: 0.780 (must_read)</description><content:encoded>&lt;p&gt;&lt;strong&gt;Authors:&lt;/strong&gt; Yuping Liang; Jinjian Wu; Junpeng Zhang; Yuxuan Chang; Jie Feng; Guangming Shi&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Journal:&lt;/strong&gt; IEEE Transactions on Geoscience and Remote Sensing&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;a href="https://doi.org/10.1109/tgrs.2025.3643494"&gt;10.1109/tgrs.2025.3643494&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Relevance:&lt;/strong&gt; 0.780 (must_read)&lt;/p&gt;
&lt;p&gt;Oriented vehicle detection and tracking play a crucial role in various real-world applications. Yet, existing advanced models heavily rely on abundant and accurate oriented bounding box and tracking identifier annotations, which are extremely labor-intensive for satellite videos. In this paper, we endeavor to employ identifier-free point annotation to achieve the competitive performance while minimizing annotation costs. Specifically, each instance across video frames are labeled by single points, without providing its instance identifier. Building upon this setting, we introduce an oriented vehicle joint detection and tracking framework for satellite video, focusing on enhancing model performance by carefully-designed sample acquisition and robust learning processes. Firstly, we leverage temporal and visual information to generate sequence-aligned pseudo-labels and visually-aligned synthetic objects, which complement each other during training by providing both exact appearance and annotation information. Secondly, a novel spatio-temporal consistency metric is developed to assess sample quality, which is then incorporated into a curriculum learning schedule. This strategy facilitates a gradual learning progression from high-quality data to low-quality or noisy examples, thereyby minimizing interference from potentially misleading samples. Finally, an end-to-end oriented object joint detection and tracking network is constructed to enable effective oriented vehicle dynamic analysis. Extensive ablation and experimental results on two satellite video datasets demonstrate the superiority of our proposed method.&lt;/p&gt;</content:encoded></item><item><title>PH-Mamba: Enhancing Mamba with Position Encoding and Harmonized Attention for Image Deraining and Beyond</title><link>https://doi.org/10.1109/tip.2025.3638153</link><guid>10.1109/tip.2025.3638153</guid><pubDate>Fri, 12 Dec 2025 18:38:08 +0000</pubDate><dc:creator>Kui Jiang</dc:creator><dc:creator>Junjun Jiang</dc:creator><dc:creator>Xianming Liu</dc:creator><dc:creator>Hongxun Yao</dc:creator><dc:creator>Chia-Wen Lin</dc:creator><prism:publicationName>IEEE Transactions on Image Processing</prism:publicationName><prism:doi>10.1109/tip.2025.3638153</prism:doi><description>Mamba and its variants excel at modeling long-range dependencies with linear computational complexity, making them effective for diverse vision tasks. However, Mamba’s reliance on unfolding 1D sequential representations necessitates multiple directional scans to recover lost spatial dependencies. This introduces significant computational overhead, redundant token traversal, and inefficiencies that compromise accuracy in real-world applications. To this end, we propose PH-Mamba, a novel framework integrating position encoding and harmonized attention for image deraining and beyond. PH-Mamba transforms Mamba’s scanning process into a position-guided, unidirectional scanning that selectively prioritizes degradation-relevant tokens. Specifically, we devise a position-guided hybrid Mamba module (PHMM) that jointly encodes perturbation features alongside their spatial coordinates and harmonized representation to model consistent degradation patterns. Within PHMM, a harmonized Transformer is developed to focus on uncertain regions while suppressing noise interference, thereby improving spatial modeling fidelity. Additionally, we employ a vector decomposition and synthesis strategy to enable the unified representation layout to global degradation by directional scanning while minimizing redundancy. By cascading multiple PHMM blocks, PH-Mamba combines global positional guidance with local differential features to strengthen contextual learning. Extensive experiments demonstrate the superiority of PH-Mamba across low-level image restoration benchmarks. For example, compared to NeRD, PH-Mamba achieves a 0.60 dB PSNR improvement while requiring 88.9% fewer parameters, 36.2% less computation, and 63.0% faster inference time.
Published: 2025-12-12T18:38:08+00:00
Venue: IEEE Transactions on Image Processing
Score: 0.777 (must_read)</description><content:encoded>&lt;p&gt;&lt;strong&gt;Authors:&lt;/strong&gt; Kui Jiang; Junjun Jiang; Xianming Liu; Hongxun Yao; Chia-Wen Lin&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Journal:&lt;/strong&gt; IEEE Transactions on Image Processing&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;a href="https://doi.org/10.1109/tip.2025.3638153"&gt;10.1109/tip.2025.3638153&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Relevance:&lt;/strong&gt; 0.777 (must_read)&lt;/p&gt;
&lt;p&gt;Mamba and its variants excel at modeling long-range dependencies with linear computational complexity, making them effective for diverse vision tasks. However, Mamba’s reliance on unfolding 1D sequential representations necessitates multiple directional scans to recover lost spatial dependencies. This introduces significant computational overhead, redundant token traversal, and inefficiencies that compromise accuracy in real-world applications. To this end, we propose PH-Mamba, a novel framework integrating position encoding and harmonized attention for image deraining and beyond. PH-Mamba transforms Mamba’s scanning process into a position-guided, unidirectional scanning that selectively prioritizes degradation-relevant tokens. Specifically, we devise a position-guided hybrid Mamba module (PHMM) that jointly encodes perturbation features alongside their spatial coordinates and harmonized representation to model consistent degradation patterns. Within PHMM, a harmonized Transformer is developed to focus on uncertain regions while suppressing noise interference, thereby improving spatial modeling fidelity. Additionally, we employ a vector decomposition and synthesis strategy to enable the unified representation layout to global degradation by directional scanning while minimizing redundancy. By cascading multiple PHMM blocks, PH-Mamba combines global positional guidance with local differential features to strengthen contextual learning. Extensive experiments demonstrate the superiority of PH-Mamba across low-level image restoration benchmarks. For example, compared to NeRD, PH-Mamba achieves a 0.60 dB PSNR improvement while requiring 88.9% fewer parameters, 36.2% less computation, and 63.0% faster inference time.&lt;/p&gt;</content:encoded></item><item><title>A Unified Data-Aware Fidelity and Regularization Learning Paradigm for Thick Cloud Removal of Multi-Temporal Remote Sensing Images</title><link>https://doi.org/10.1109/tgrs.2025.3643646</link><guid>10.1109/tgrs.2025.3643646</guid><pubDate>Fri, 12 Dec 2025 18:35:46 +0000</pubDate><dc:creator>Hao Peng</dc:creator><dc:creator>Ting-Zhu Huang</dc:creator><dc:creator>Xi-Le Zhao</dc:creator><dc:creator>Wei-Hao Wu</dc:creator><dc:creator>Jie Lin</dc:creator><dc:creator>Teng-Yu Ji</dc:creator><prism:publicationName>IEEE Transactions on Geoscience and Remote Sensing</prism:publicationName><prism:doi>10.1109/tgrs.2025.3643646</prism:doi><description>Thick cloud removal is a long-standing and critical challenge in remote sensing (RS) image processing, with the increasing availability of multi-temporal RS images offering new opportunities to address this problem. The main limitation of existing cloud removal methods is that the classical fidelity only considers original pixel domain or handcrafted/pretrained filtered domains, overlooking the individuality filters and the corresponding feature behind each RS image, which leads to evident detail discrepancies. To address this issue, we suggest a data-aware fidelity based on the untrained neural network, which encourages deep data-aware feature matching between the contaminated image and the guidance image. Complementary to the data-aware fidelity, we design the deep self-representation to implicitly impose regularization benefiting from the same untrained neural network. Equipped with the elaborately designed fidelity and regularization, we propose a unified data-aware fidelity and regularization learning (called DAFRL) paradigm for thick cloud removal that flexibly adapts to diverse multi-temporal RS images. Under this paradigm, the fidelity and regularization are empowered by the same untrained neural network, serving distinct functions while collaborating organically. Experimental results on both simulated and real datasets show that the proposed DAFRL effectively preserves fine details and outperforms the compared methods.
Published: 2025-12-12T18:35:46+00:00
Venue: IEEE Transactions on Geoscience and Remote Sensing
Score: 0.777 (must_read)</description><content:encoded>&lt;p&gt;&lt;strong&gt;Authors:&lt;/strong&gt; Hao Peng; Ting-Zhu Huang; Xi-Le Zhao; Wei-Hao Wu; Jie Lin; Teng-Yu Ji&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Journal:&lt;/strong&gt; IEEE Transactions on Geoscience and Remote Sensing&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;a href="https://doi.org/10.1109/tgrs.2025.3643646"&gt;10.1109/tgrs.2025.3643646&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Relevance:&lt;/strong&gt; 0.777 (must_read)&lt;/p&gt;
&lt;p&gt;Thick cloud removal is a long-standing and critical challenge in remote sensing (RS) image processing, with the increasing availability of multi-temporal RS images offering new opportunities to address this problem. The main limitation of existing cloud removal methods is that the classical fidelity only considers original pixel domain or handcrafted/pretrained filtered domains, overlooking the individuality filters and the corresponding feature behind each RS image, which leads to evident detail discrepancies. To address this issue, we suggest a data-aware fidelity based on the untrained neural network, which encourages deep data-aware feature matching between the contaminated image and the guidance image. Complementary to the data-aware fidelity, we design the deep self-representation to implicitly impose regularization benefiting from the same untrained neural network. Equipped with the elaborately designed fidelity and regularization, we propose a unified data-aware fidelity and regularization learning (called DAFRL) paradigm for thick cloud removal that flexibly adapts to diverse multi-temporal RS images. Under this paradigm, the fidelity and regularization are empowered by the same untrained neural network, serving distinct functions while collaborating organically. Experimental results on both simulated and real datasets show that the proposed DAFRL effectively preserves fine details and outperforms the compared methods.&lt;/p&gt;</content:encoded></item><item><title>Video Depth Propagation</title><link>https://arxiv.org/abs/2512.10725v1</link><guid>http://arxiv.org/abs/2512.10725v1</guid><pubDate>Thu, 11 Dec 2025 15:08:37 +0000</pubDate><dc:creator>Luigi Piccinelli</dc:creator><dc:creator>Thiemo Wandel</dc:creator><dc:creator>Christos Sakaridis</dc:creator><dc:creator>Wim Abbeloos</dc:creator><dc:creator>Luc Van Gool</dc:creator><dc:type>preprint</dc:type><prism:publicationName>arXiv</prism:publicationName><description>Depth estimation in videos is essential for visual perception in real-world applications. However, existing methods either rely on simple frame-by-frame monocular models, leading to temporal inconsistencies and inaccuracies, or use computationally demanding temporal modeling, unsuitable for real-time applications. These limitations significantly restrict general applicability and performance in practical settings. To address this, we propose VeloDepth, an efficient and robust online video depth estimation pipeline that effectively leverages spatiotemporal priors from previous depth predictions and performs deep feature propagation. Our method introduces a novel Propagation Module that refines and propagates depth features and predictions using flow-based warping coupled with learned residual corrections. In addition, our design structurally enforces temporal consistency, resulting in stable depth predictions across consecutive frames with improved efficiency. Comprehensive zero-shot evaluation on multiple benchmarks demonstrates the state-of-the-art temporal consistency and competitive accuracy of VeloDepth, alongside its significantly faster inference compared to existing video-based depth estimators. VeloDepth thus provides a practical, efficient, and accurate solution for real-time depth estimation suitable for diverse perception tasks. Code and models are available at https://github.com/lpiccinelli-eth/velodepth
Published: 2025-12-11T15:08:37+00:00
Venue: arXiv
Score: 0.777 (must_read)</description><content:encoded>&lt;p&gt;&lt;strong&gt;Authors:&lt;/strong&gt; Luigi Piccinelli; Thiemo Wandel; Christos Sakaridis; Wim Abbeloos; Luc Van Gool&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Journal:&lt;/strong&gt; arXiv&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Relevance:&lt;/strong&gt; 0.777 (must_read)&lt;/p&gt;
&lt;p&gt;Depth estimation in videos is essential for visual perception in real-world applications. However, existing methods either rely on simple frame-by-frame monocular models, leading to temporal inconsistencies and inaccuracies, or use computationally demanding temporal modeling, unsuitable for real-time applications. These limitations significantly restrict general applicability and performance in practical settings. To address this, we propose VeloDepth, an efficient and robust online video depth estimation pipeline that effectively leverages spatiotemporal priors from previous depth predictions and performs deep feature propagation. Our method introduces a novel Propagation Module that refines and propagates depth features and predictions using flow-based warping coupled with learned residual corrections. In addition, our design structurally enforces temporal consistency, resulting in stable depth predictions across consecutive frames with improved efficiency. Comprehensive zero-shot evaluation on multiple benchmarks demonstrates the state-of-the-art temporal consistency and competitive accuracy of VeloDepth, alongside its significantly faster inference compared to existing video-based depth estimators. VeloDepth thus provides a practical, efficient, and accurate solution for real-time depth estimation suitable for diverse perception tasks. Code and models are available at https://github.com/lpiccinelli-eth/velodepth&lt;/p&gt;</content:encoded></item><item><title>Community-Aware Multi-View Representation Learning With Incomplete Information</title><link>https://doi.org/10.1109/tpami.2025.3639582</link><guid>10.1109/tpami.2025.3639582</guid><pubDate>Fri, 12 Dec 2025 18:35:38 +0000</pubDate><dc:creator>Haobin Li</dc:creator><dc:creator>Yijie Lin</dc:creator><dc:creator>Peng Hu</dc:creator><dc:creator>Mouxing Yang</dc:creator><dc:creator>Xi Peng</dc:creator><prism:publicationName>IEEE Transactions on Pattern Analysis and Machine Intelligence</prism:publicationName><prism:doi>10.1109/tpami.2025.3639582</prism:doi><description>Due to the complexity of data collection in the real world, Multi-view Representation Learning (MvRL) always encounters the incomplete information challenge, typically manifested as the Sample-missing Problem (SP) and the View-unaligned Problem (VP). Although several methods have been proposed, they fail to find a good trade-off among sample restoration, view alignment, and data diversity preservation. To address this issue, we take and mathematically formulate two sociological concepts for MvRL, i.e., community commonality and community versatility, where the former refers to the identical custom shared within the same community, and the latter refers to the similar but non-identical custom within communities of the same minority. One could find that the community commonality can enhance the compactness of view-specific clusters, and the community versatility can preserve the view diversity. Moreover, combining both of them could facilitate achieving robust MvRL with incomplete information. With the formulations, we propose a novel method dubbed Community-Aware Multi-viEw RepresentAtion learning with incomplete information (CAMERA). In brief, CAMERA employs a novel dual-stream network and an elaborate objective function that theoretically and empirically embraces community commonality and versatility. Extensive experimental results on seven datasets demonstrate that CAMERA remarkably outperforms 24 competitive multi-view learning methods on clustering, classification, and human action recognition tasks.
Published: 2025-12-12T18:35:38+00:00
Venue: IEEE Transactions on Pattern Analysis and Machine Intelligence
Score: 0.777 (must_read)</description><content:encoded>&lt;p&gt;&lt;strong&gt;Authors:&lt;/strong&gt; Haobin Li; Yijie Lin; Peng Hu; Mouxing Yang; Xi Peng&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Journal:&lt;/strong&gt; IEEE Transactions on Pattern Analysis and Machine Intelligence&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;a href="https://doi.org/10.1109/tpami.2025.3639582"&gt;10.1109/tpami.2025.3639582&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Relevance:&lt;/strong&gt; 0.777 (must_read)&lt;/p&gt;
&lt;p&gt;Due to the complexity of data collection in the real world, Multi-view Representation Learning (MvRL) always encounters the incomplete information challenge, typically manifested as the Sample-missing Problem (SP) and the View-unaligned Problem (VP). Although several methods have been proposed, they fail to find a good trade-off among sample restoration, view alignment, and data diversity preservation. To address this issue, we take and mathematically formulate two sociological concepts for MvRL, i.e., community commonality and community versatility, where the former refers to the identical custom shared within the same community, and the latter refers to the similar but non-identical custom within communities of the same minority. One could find that the community commonality can enhance the compactness of view-specific clusters, and the community versatility can preserve the view diversity. Moreover, combining both of them could facilitate achieving robust MvRL with incomplete information. With the formulations, we propose a novel method dubbed Community-Aware Multi-viEw RepresentAtion learning with incomplete information (CAMERA). In brief, CAMERA employs a novel dual-stream network and an elaborate objective function that theoretically and empirically embraces community commonality and versatility. Extensive experimental results on seven datasets demonstrate that CAMERA remarkably outperforms 24 competitive multi-view learning methods on clustering, classification, and human action recognition tasks.&lt;/p&gt;</content:encoded></item><item><title>E-RayZer: Self-supervised 3D Reconstruction as Spatial Visual Pre-training</title><link>https://arxiv.org/abs/2512.10950v1</link><guid>http://arxiv.org/abs/2512.10950v1</guid><pubDate>Thu, 11 Dec 2025 18:59:53 +0000</pubDate><dc:creator>Qitao Zhao</dc:creator><dc:creator>Hao Tan</dc:creator><dc:creator>Qianqian Wang</dc:creator><dc:creator>Sai Bi</dc:creator><dc:creator>Kai Zhang</dc:creator><dc:creator>Kalyan Sunkavalli</dc:creator><dc:creator>Shubham Tulsiani</dc:creator><dc:creator>Hanwen Jiang</dc:creator><dc:type>preprint</dc:type><prism:publicationName>arXiv</prism:publicationName><description>Self-supervised pre-training has revolutionized foundation models for languages, individual 2D images and videos, but remains largely unexplored for learning 3D-aware representations from multi-view images. In this paper, we present E-RayZer, a self-supervised large 3D Vision model that learns truly 3D-aware representations directly from unlabeled images. Unlike prior self-supervised methods such as RayZer that infer 3D indirectly through latent-space view synthesis, E-RayZer operates directly in 3D space, performing self-supervised 3D reconstruction with Explicit geometry. This formulation eliminates shortcut solutions and yields representations that are geometrically grounded. To ensure convergence and scalability, we introduce a novel fine-grained learning curriculum that organizes training from easy to hard samples and harmonizes heterogeneous data sources in an entirely unsupervised manner. Experiments demonstrate that E-RayZer significantly outperforms RayZer on pose estimation, matches or sometimes surpasses fully supervised reconstruction models such as VGGT. Furthermore, its learned representations outperform leading visual pre-training models (e.g., DINOv3, CroCo v2, VideoMAE V2, and RayZer) when transferring to 3D downstream tasks, establishing E-RayZer as a new paradigm for 3D-aware visual pre-training.
Published: 2025-12-11T18:59:53+00:00
Venue: arXiv
Score: 0.776 (must_read)</description><content:encoded>&lt;p&gt;&lt;strong&gt;Authors:&lt;/strong&gt; Qitao Zhao; Hao Tan; Qianqian Wang; Sai Bi; Kai Zhang; Kalyan Sunkavalli; Shubham Tulsiani; Hanwen Jiang&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Journal:&lt;/strong&gt; arXiv&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Relevance:&lt;/strong&gt; 0.776 (must_read)&lt;/p&gt;
&lt;p&gt;Self-supervised pre-training has revolutionized foundation models for languages, individual 2D images and videos, but remains largely unexplored for learning 3D-aware representations from multi-view images. In this paper, we present E-RayZer, a self-supervised large 3D Vision model that learns truly 3D-aware representations directly from unlabeled images. Unlike prior self-supervised methods such as RayZer that infer 3D indirectly through latent-space view synthesis, E-RayZer operates directly in 3D space, performing self-supervised 3D reconstruction with Explicit geometry. This formulation eliminates shortcut solutions and yields representations that are geometrically grounded. To ensure convergence and scalability, we introduce a novel fine-grained learning curriculum that organizes training from easy to hard samples and harmonizes heterogeneous data sources in an entirely unsupervised manner. Experiments demonstrate that E-RayZer significantly outperforms RayZer on pose estimation, matches or sometimes surpasses fully supervised reconstruction models such as VGGT. Furthermore, its learned representations outperform leading visual pre-training models (e.g., DINOv3, CroCo v2, VideoMAE V2, and RayZer) when transferring to 3D downstream tasks, establishing E-RayZer as a new paradigm for 3D-aware visual pre-training.&lt;/p&gt;</content:encoded></item><item><title>Adaptive Attention-based Unsupervised Domain Adaptation for Egocentric Action Recognition</title><link>https://doi.org/10.1109/tcsvt.2025.3643449</link><guid>10.1109/tcsvt.2025.3643449</guid><pubDate>Fri, 12 Dec 2025 18:37:50 +0000</pubDate><dc:creator>Yishan Zou</dc:creator><dc:creator>Chris Nugent</dc:creator><dc:creator>Matthew Burns</dc:creator><dc:creator>Shengli Wu</dc:creator><dc:creator>Lei Zhu</dc:creator><dc:creator>Meng Liu</dc:creator><prism:publicationName>IEEE Transactions on Circuits and Systems for Video Technology</prism:publicationName><prism:doi>10.1109/tcsvt.2025.3643449</prism:doi><description>In egocentric videos, collecting and annotating supervised data is more complicated and time-consuming than in exocentric videos, limiting research in this area. As a remedy, Unsupervised Domain Adaptation (UDA) enhances model performance on unlabeled target domains by bridging the distribution gap between source and target domains. However, UDA for egocentric action recognition is under-explored, facing unique challenges such as simultaneous learning of verb and noun representations, focusing on human-object interactions, and managing excessive verb-noun combinations. To tackle these issues, we propose a novel Unsupervised Domain Adaptation for Egocentric Action Recognition (UDA-EAR) approach that adaptively models egocentric actions and facilitates cross-domain knowledge transfer, improving recognition performance in unlabeled target domains. Specifically, our UDA-EAR employs adaptive spatio-temporal and spatio-channel attention in a dual-branch pipeline to focus on motion intervals and interaction regions, respectively, allowing specialized learning of discriminative representations while avoiding negative combination dependencies from domain gaps. Additionally, an adversarial domain alignment mechanism aligns the data distributions between source and target domains, effectively transferring fine-grained verb-noun knowledge of egocentric videos. Extensive experiments demonstrate that our UDA-EAR outperforms state-of-the-art baselines on widely used egocentric datasets, significantly improving egocentric action recognition accuracy. Our source codes and datasets are available at https://github.com/zou-y23/UDA-EAR.
Published: 2025-12-12T18:37:50+00:00
Venue: IEEE Transactions on Circuits and Systems for Video Technology
Score: 0.775 (must_read)</description><content:encoded>&lt;p&gt;&lt;strong&gt;Authors:&lt;/strong&gt; Yishan Zou; Chris Nugent; Matthew Burns; Shengli Wu; Lei Zhu; Meng Liu&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Journal:&lt;/strong&gt; IEEE Transactions on Circuits and Systems for Video Technology&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;a href="https://doi.org/10.1109/tcsvt.2025.3643449"&gt;10.1109/tcsvt.2025.3643449&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Relevance:&lt;/strong&gt; 0.775 (must_read)&lt;/p&gt;
&lt;p&gt;In egocentric videos, collecting and annotating supervised data is more complicated and time-consuming than in exocentric videos, limiting research in this area. As a remedy, Unsupervised Domain Adaptation (UDA) enhances model performance on unlabeled target domains by bridging the distribution gap between source and target domains. However, UDA for egocentric action recognition is under-explored, facing unique challenges such as simultaneous learning of verb and noun representations, focusing on human-object interactions, and managing excessive verb-noun combinations. To tackle these issues, we propose a novel Unsupervised Domain Adaptation for Egocentric Action Recognition (UDA-EAR) approach that adaptively models egocentric actions and facilitates cross-domain knowledge transfer, improving recognition performance in unlabeled target domains. Specifically, our UDA-EAR employs adaptive spatio-temporal and spatio-channel attention in a dual-branch pipeline to focus on motion intervals and interaction regions, respectively, allowing specialized learning of discriminative representations while avoiding negative combination dependencies from domain gaps. Additionally, an adversarial domain alignment mechanism aligns the data distributions between source and target domains, effectively transferring fine-grained verb-noun knowledge of egocentric videos. Extensive experiments demonstrate that our UDA-EAR outperforms state-of-the-art baselines on widely used egocentric datasets, significantly improving egocentric action recognition accuracy. Our source codes and datasets are available at https://github.com/zou-y23/UDA-EAR.&lt;/p&gt;</content:encoded></item><item><title>Mul-VMamba: Multimodal semantic segmentation using selection-fusion-based Vision-Mamba</title><link>https://doi.org/10.1016/j.knosys.2025.115119</link><guid>10.1016/j.knosys.2025.115119</guid><pubDate>Sat, 13 Dec 2025 02:33:32 +0000</pubDate><dc:creator>Rongrong Ni</dc:creator><dc:creator>Yuanhui Guo</dc:creator><dc:creator>Biao Yang</dc:creator><dc:creator>Yi Liu</dc:creator><dc:creator>Hai Wang</dc:creator><dc:creator>Chuan Hu</dc:creator><prism:publicationName>Knowledge-Based Systems</prism:publicationName><prism:doi>10.1016/j.knosys.2025.115119</prism:doi><description>For tasks such as autonomous driving and remote sensing, integrating multimodal data (RGB, depth, infrared, and others) can significantly enhance the accuracy and robustness of semantic segmentation under complex environmental conditions, thereby providing precise and reliable information for downstream tasks. However, existing approaches emphasize segmentation accuracy at the expense of efficiency. To address this trade-off, we propose a multimodal semantic segmentation network based on the linear complexity Selective State Space Model (S6, a.k.a Mamba), dubbed Mul-VMamba. Mul-VMamba establishes selection-fusion relationships among multimodal features, enabling semantic segmentation with any input modalities. Specifically, the Mamba Spatial-consistency Selective Module (MSSM) adaptively extracts feature mapping relationships and filters out redundant features at identical spatial locations, preserving the spatial relationships between each modality. Additionally, the Mamba Cross-Fusion Module (MCFM) introduces a Cross Selective State Space Model (Cross-S6), establishing the relationship between S6 and multimodal features, achieving optimal fusion performance. Qualitative and quantitative evaluations on the MCubes and DeLiVER datasets demonstrate the efficacy and efficiency of Mul-VMamba. Notably, Mul-VMamba achieves 54.65% / 68.98% mIoU on Mcubes / DeLiVER datasets using only 55.33M params. The source code of Mul-VMamba is publicly available at https://github.com/Mask0913/Mul-VMamba .
Published: 2025-12-13T02:33:32+00:00
Venue: Knowledge-Based Systems
Score: 0.775 (must_read)</description><content:encoded>&lt;p&gt;&lt;strong&gt;Authors:&lt;/strong&gt; Rongrong Ni; Yuanhui Guo; Biao Yang; Yi Liu; Hai Wang; Chuan Hu&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Journal:&lt;/strong&gt; Knowledge-Based Systems&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;a href="https://doi.org/10.1016/j.knosys.2025.115119"&gt;10.1016/j.knosys.2025.115119&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Relevance:&lt;/strong&gt; 0.775 (must_read)&lt;/p&gt;
&lt;p&gt;For tasks such as autonomous driving and remote sensing, integrating multimodal data (RGB, depth, infrared, and others) can significantly enhance the accuracy and robustness of semantic segmentation under complex environmental conditions, thereby providing precise and reliable information for downstream tasks. However, existing approaches emphasize segmentation accuracy at the expense of efficiency. To address this trade-off, we propose a multimodal semantic segmentation network based on the linear complexity Selective State Space Model (S6, a.k.a Mamba), dubbed Mul-VMamba. Mul-VMamba establishes selection-fusion relationships among multimodal features, enabling semantic segmentation with any input modalities. Specifically, the Mamba Spatial-consistency Selective Module (MSSM) adaptively extracts feature mapping relationships and filters out redundant features at identical spatial locations, preserving the spatial relationships between each modality. Additionally, the Mamba Cross-Fusion Module (MCFM) introduces a Cross Selective State Space Model (Cross-S6), establishing the relationship between S6 and multimodal features, achieving optimal fusion performance. Qualitative and quantitative evaluations on the MCubes and DeLiVER datasets demonstrate the efficacy and efficiency of Mul-VMamba. Notably, Mul-VMamba achieves 54.65% / 68.98% mIoU on Mcubes / DeLiVER datasets using only 55.33M params. The source code of Mul-VMamba is publicly available at https://github.com/Mask0913/Mul-VMamba .&lt;/p&gt;</content:encoded></item></channel></rss>