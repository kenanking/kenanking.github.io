<?xml version='1.0' encoding='utf-8'?>
<rss xmlns:content="http://purl.org/rss/1.0/modules/content/" xmlns:dc="http://purl.org/dc/elements/1.1/" xmlns:prism="http://prismstandard.org/namespaces/basic/3.0/" version="2.0"><channel><title>ZotWatch Feed</title><link>https://ehehe.cn/zotwatch/</link><description>AI-assisted literature watch</description><lastBuildDate>Fri, 02 Jan 2026 02:42:03 +0000</lastBuildDate><item><title>FSFNet: Frequency-Spatial Domain Fusion Network for Infrared Small Target Detection</title><link>https://doi.org/10.1109/tgrs.2025.3650074</link><guid>10.1109/tgrs.2025.3650074</guid><pubDate>Thu, 01 Jan 2026 18:37:16 +0000</pubDate><dc:creator>Riyao Chen</dc:creator><dc:creator>Wenxiao Tang</dc:creator><dc:creator>Mingchao Yang</dc:creator><dc:creator>Wenxiong Kang</dc:creator><prism:publicationName>IEEE Transactions on Geoscience and Remote Sensing</prism:publicationName><prism:doi>10.1109/tgrs.2025.3650074</prism:doi><description>Infrared small target detection (IRSTD) is inherently challenging due to the weak feature representation of small targets and significant interference caused by complex backgrounds. Recent mainstream IRSTD methods predominantly emphasize feature extraction in the spatial domain but often neglect crucial complementary information contained in the frequency domain, resulting in limited detection performance. To address this issue, we propose a novel Frequency-Spatial Fusion Network (FSFNet), which explicitly integrates spatial-domain features and frequency-domain information through two innovative modules: the Frequency-Spatial Block (FSBlock) and the Frequency-Spatial Feature Fusion Module (FSFFM). Specifically, the FSBlock, equipped with a Spectral Band Gate (SBG) module, selectively emphasizes informative spectral bands to capture global contextual patterns and periodic characteristics in the frequency domain, thereby complementing traditional spatial-domain local representations and enhancing the network’s ability to distinguish small targets from cluttered backgrounds. Furthermore, the FSFFM adaptively combines these multi-domain features via a frequency-guided cross-channel attention mechanism, effectively bridging the semantic gap between the spatial and frequency domains. Extensive experimental evaluations on three publicly benchmark datasets demonstrate that the proposed FSFNet consistently achieves superior detection accuracy and robustness compared with state-of-the-art methods.
Published: 2026-01-01T18:37:16+00:00
Venue: IEEE Transactions on Geoscience and Remote Sensing
Score: 0.831 (must_read)</description><content:encoded>&lt;p&gt;&lt;strong&gt;Authors:&lt;/strong&gt; Riyao Chen; Wenxiao Tang; Mingchao Yang; Wenxiong Kang&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Journal:&lt;/strong&gt; IEEE Transactions on Geoscience and Remote Sensing&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;a href="https://doi.org/10.1109/tgrs.2025.3650074"&gt;10.1109/tgrs.2025.3650074&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Relevance:&lt;/strong&gt; 0.831 (must_read)&lt;/p&gt;
&lt;p&gt;Infrared small target detection (IRSTD) is inherently challenging due to the weak feature representation of small targets and significant interference caused by complex backgrounds. Recent mainstream IRSTD methods predominantly emphasize feature extraction in the spatial domain but often neglect crucial complementary information contained in the frequency domain, resulting in limited detection performance. To address this issue, we propose a novel Frequency-Spatial Fusion Network (FSFNet), which explicitly integrates spatial-domain features and frequency-domain information through two innovative modules: the Frequency-Spatial Block (FSBlock) and the Frequency-Spatial Feature Fusion Module (FSFFM). Specifically, the FSBlock, equipped with a Spectral Band Gate (SBG) module, selectively emphasizes informative spectral bands to capture global contextual patterns and periodic characteristics in the frequency domain, thereby complementing traditional spatial-domain local representations and enhancing the network’s ability to distinguish small targets from cluttered backgrounds. Furthermore, the FSFFM adaptively combines these multi-domain features via a frequency-guided cross-channel attention mechanism, effectively bridging the semantic gap between the spatial and frequency domains. Extensive experimental evaluations on three publicly benchmark datasets demonstrate that the proposed FSFNet consistently achieves superior detection accuracy and robustness compared with state-of-the-art methods.&lt;/p&gt;</content:encoded></item><item><title>Token Calibration for Transformer-based Domain Adaptation</title><link>https://doi.org/10.1109/tip.2025.3647367</link><guid>10.1109/tip.2025.3647367</guid><pubDate>Thu, 01 Jan 2026 18:39:24 +0000</pubDate><dc:creator>Xiaowei Fu</dc:creator><dc:creator>Shiyu Ye</dc:creator><dc:creator>Chenxu Zhang</dc:creator><dc:creator>Fuxiang Huang</dc:creator><dc:creator>Xin Xu</dc:creator><dc:creator>Lei Zhang</dc:creator><prism:publicationName>IEEE Transactions on Image Processing</prism:publicationName><prism:doi>10.1109/tip.2025.3647367</prism:doi><description>Unsupervised Domain Adaptation (UDA) aims to transfer knowledge from a labeled source domain to an unlabeled target domain by learning domain-invariant representations. Motivated by the recent success of Vision Transformers (ViTs), several UDA approaches have adopted ViT architectures to exploit fine-grained patch-level representations, which are unified as Transformer-based Domain Adaptation (TransDA) independent of CNN-based. However, we have a key observation in TransDA: due to inherent domain shifts, patches (tokens) from different semantic categories across domains may exhibit abnormally high similarities, which can mislead the self-attention mechanism and degrade adaptation performance. To solve that, we propose a novel Patch-Adaptation Transformer (PATrans), which first identifies similarity-anomalous patches and then adaptively suppresses their negative impact to domain alignment, i.e. token calibration. Specifically, we introduce a Patch-Adaptation Attention (PAA) mechanism to replace the standard self-attention mechanism, which consists of a weight-shared triple-branch mixed attention mechanism and a patch-level domain discriminator. The mixed attention integrates self-attention and cross-attention to enhance intra-domain feature modeling and inter-domain similarity estimation. Meanwhile, the patch-level domain discriminator quantifies the anomaly probability of each patch, enabling dynamic reweighting to mitigate the impact of unreliable patch correspondences. Furthermore, we introduce a contrastive attention regularization strategy, which leverages category-level information in a contrastive learning framework to promote class-consistent attention distributions. Extensive experiments on four benchmark datasets demonstrate that PATrans attains significant improvements over existing state-of-the-art UDA methods (e.g., 89.2% on the VisDA-2017). Code is available at: https://github.com/YSY145/PATrans.
Published: 2026-01-01T18:39:24+00:00
Venue: IEEE Transactions on Image Processing
Score: 0.822 (must_read)</description><content:encoded>&lt;p&gt;&lt;strong&gt;Authors:&lt;/strong&gt; Xiaowei Fu; Shiyu Ye; Chenxu Zhang; Fuxiang Huang; Xin Xu; Lei Zhang&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Journal:&lt;/strong&gt; IEEE Transactions on Image Processing&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;a href="https://doi.org/10.1109/tip.2025.3647367"&gt;10.1109/tip.2025.3647367&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Relevance:&lt;/strong&gt; 0.822 (must_read)&lt;/p&gt;
&lt;p&gt;Unsupervised Domain Adaptation (UDA) aims to transfer knowledge from a labeled source domain to an unlabeled target domain by learning domain-invariant representations. Motivated by the recent success of Vision Transformers (ViTs), several UDA approaches have adopted ViT architectures to exploit fine-grained patch-level representations, which are unified as Transformer-based Domain Adaptation (TransDA) independent of CNN-based. However, we have a key observation in TransDA: due to inherent domain shifts, patches (tokens) from different semantic categories across domains may exhibit abnormally high similarities, which can mislead the self-attention mechanism and degrade adaptation performance. To solve that, we propose a novel Patch-Adaptation Transformer (PATrans), which first identifies similarity-anomalous patches and then adaptively suppresses their negative impact to domain alignment, i.e. token calibration. Specifically, we introduce a Patch-Adaptation Attention (PAA) mechanism to replace the standard self-attention mechanism, which consists of a weight-shared triple-branch mixed attention mechanism and a patch-level domain discriminator. The mixed attention integrates self-attention and cross-attention to enhance intra-domain feature modeling and inter-domain similarity estimation. Meanwhile, the patch-level domain discriminator quantifies the anomaly probability of each patch, enabling dynamic reweighting to mitigate the impact of unreliable patch correspondences. Furthermore, we introduce a contrastive attention regularization strategy, which leverages category-level information in a contrastive learning framework to promote class-consistent attention distributions. Extensive experiments on four benchmark datasets demonstrate that PATrans attains significant improvements over existing state-of-the-art UDA methods (e.g., 89.2% on the VisDA-2017). Code is available at: https://github.com/YSY145/PATrans.&lt;/p&gt;</content:encoded></item><item><title>SAR-UMSDN: The Unsupervised Multimodal Ship Detection Network Based on SAR Images</title><link>https://doi.org/10.1109/jstars.2025.3649747</link><guid>10.1109/jstars.2025.3649747</guid><pubDate>Wed, 31 Dec 2025 18:44:28 +0000</pubDate><dc:creator>Junpeng Ai</dc:creator><dc:creator>Liang Luo</dc:creator><dc:creator>Shijie Wang</dc:creator><dc:creator>Liandong Hao</dc:creator><prism:publicationName>IEEE Journal of Selected Topics in Applied Earth Observations and Remote Sensing</prism:publicationName><prism:doi>10.1109/jstars.2025.3649747</prism:doi><description>In response to the problems faced by synthetic aperture radar (SAR) images in detecting ships in complex near-shore scenarios, such as low signal-to-noise ratio of small targets and limited feature representation capacity of a single SAR mode, and also due to the fact that real optical images in near-shore areas are often affected by conditions like clouds and day/night, and thus difficult to be real-time registered with SAR images. We propose an unsupervised multimodal SAR image ship detection network (SAR-UMSDN) with its performance enhanced by deriving multimodal inputs from SAR data. SAR-UMSDN consists of a self-supervised SAR image enhancement network (URSIEN), SAR colorization module based on generative adversarial networks (cGAN4ColSAR), and multimodal detection model. URSIEN performs unlabeled illumination–reflection decoupling and detail restoration to enhance the original SAR image, while cGAN4ColSAR maps the single-channel SAR image onto a pseudo-color image with rich optical textures. We use images of these two modalities as inputs to the multimodal SAR-UMSDN for ablation tests, model comparisons, and other evaluations. URSIEN achieves a peak signal-to-noise ratio of 28.63 dB and inference speed of 0.0017 s for a model size of 0.4 million parameters, improving most indicators compared with other image enhancement models. After applying URSIEN, the mean average precision at 0.5 (mAP@0.5) of SAR-UMSDN is 0.4% higher than that of the baseline model, and after using cGAN4ColSAR, mAP@0.5 is 1.9% higher than that of the baseline model. The combined effect of the two components improves the SAR-UMSDN mAP@0.5 to 93.5%. SAR-UMSDN outperforms 10 baseline models and 7 state-of-the-art models, with its mAP@0.5–0.95 (67.4%) being superior to that of traditional single-modal and mainstream baseline models, showing improvements of 1.4–2.7%. The generalization experiments conducted on six public ship datasets have shown that SAR-UMSDN has a significant advantage in ship...
Published: 2025-12-31T18:44:28+00:00
Venue: IEEE Journal of Selected Topics in Applied Earth Observations and Remote Sensing
Score: 0.816 (must_read)</description><content:encoded>&lt;p&gt;&lt;strong&gt;Authors:&lt;/strong&gt; Junpeng Ai; Liang Luo; Shijie Wang; Liandong Hao&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Journal:&lt;/strong&gt; IEEE Journal of Selected Topics in Applied Earth Observations and Remote Sensing&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;a href="https://doi.org/10.1109/jstars.2025.3649747"&gt;10.1109/jstars.2025.3649747&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Relevance:&lt;/strong&gt; 0.816 (must_read)&lt;/p&gt;
&lt;p&gt;In response to the problems faced by synthetic aperture radar (SAR) images in detecting ships in complex near-shore scenarios, such as low signal-to-noise ratio of small targets and limited feature representation capacity of a single SAR mode, and also due to the fact that real optical images in near-shore areas are often affected by conditions like clouds and day/night, and thus difficult to be real-time registered with SAR images. We propose an unsupervised multimodal SAR image ship detection network (SAR-UMSDN) with its performance enhanced by deriving multimodal inputs from SAR data. SAR-UMSDN consists of a self-supervised SAR image enhancement network (URSIEN), SAR colorization module based on generative adversarial networks (cGAN4ColSAR), and multimodal detection model. URSIEN performs unlabeled illumination–reflection decoupling and detail restoration to enhance the original SAR image, while cGAN4ColSAR maps the single-channel SAR image onto a pseudo-color image with rich optical textures. We use images of these two modalities as inputs to the multimodal SAR-UMSDN for ablation tests, model comparisons, and other evaluations. URSIEN achieves a peak signal-to-noise ratio of 28.63 dB and inference speed of 0.0017 s for a model size of 0.4 million parameters, improving most indicators compared with other image enhancement models. After applying URSIEN, the mean average precision at 0.5 (mAP@0.5) of SAR-UMSDN is 0.4% higher than that of the baseline model, and after using cGAN4ColSAR, mAP@0.5 is 1.9% higher than that of the baseline model. The combined effect of the two components improves the SAR-UMSDN mAP@0.5 to 93.5%. SAR-UMSDN outperforms 10 baseline models and 7 state-of-the-art models, with its mAP@0.5–0.95 (67.4%) being superior to that of traditional single-modal and mainstream baseline models, showing improvements of 1.4–2.7%. The generalization experiments conducted on six public ship datasets have shown that SAR-UMSDN has a significant advantage in ship...&lt;/p&gt;</content:encoded></item><item><title>Exploring Syn-to-Real Domain Adaptation for Military Target Detection</title><link>https://arxiv.org/abs/2512.23208v1</link><guid>http://arxiv.org/abs/2512.23208v1</guid><pubDate>Mon, 29 Dec 2025 05:05:41 +0000</pubDate><dc:creator>Jongoh Jeong</dc:creator><dc:creator>Youngjin Oh</dc:creator><dc:creator>Gyeongrae Nam</dc:creator><dc:creator>Jeongeun Lee</dc:creator><dc:creator>Kuk-Jin Yoon</dc:creator><dc:type>preprint</dc:type><prism:publicationName>arXiv</prism:publicationName><description>Object detection is one of the key target tasks of interest in the context of civil and military applications. In particular, the real-world deployment of target detection methods is pivotal in the decision-making process during military command and reconnaissance. However, current domain adaptive object detection algorithms consider adapting one domain to another similar one only within the scope of natural or autonomous driving scenes. Since military domains often deal with a mixed variety of environments, detecting objects from multiple varying target domains poses a greater challenge. Several studies for armored military target detection have made use of synthetic aperture radar (SAR) data due to its robustness to all weather, long range, and high-resolution characteristics. Nevertheless, the costs of SAR data acquisition and processing are still much higher than those of the conventional RGB camera, which is a more affordable alternative with significantly lower data processing time. Furthermore, the lack of military target detection datasets limits the use of such a low-cost approach. To mitigate these issues, we propose to generate RGB-based synthetic data using a photorealistic visual tool, Unreal Engine, for military target detection in a cross-domain setting. To this end, we conducted synthetic-to-real transfer experiments by training our synthetic dataset and validating on our web-collected real military target datasets. We benchmark the state-of-the-art domain adaptation methods distinguished by the degree of supervision on our proposed train-val dataset pair, and find that current methods using minimal hints on the image (e.g., object class) achieve a substantial improvement over unsupervised or semi-supervised DA methods. From these observations, we recognize the current challenges that remain to be overcome.
Published: 2025-12-29T05:05:41+00:00
Venue: arXiv
Score: 0.814 (must_read)</description><content:encoded>&lt;p&gt;&lt;strong&gt;Authors:&lt;/strong&gt; Jongoh Jeong; Youngjin Oh; Gyeongrae Nam; Jeongeun Lee; Kuk-Jin Yoon&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Journal:&lt;/strong&gt; arXiv&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Relevance:&lt;/strong&gt; 0.814 (must_read)&lt;/p&gt;
&lt;p&gt;Object detection is one of the key target tasks of interest in the context of civil and military applications. In particular, the real-world deployment of target detection methods is pivotal in the decision-making process during military command and reconnaissance. However, current domain adaptive object detection algorithms consider adapting one domain to another similar one only within the scope of natural or autonomous driving scenes. Since military domains often deal with a mixed variety of environments, detecting objects from multiple varying target domains poses a greater challenge. Several studies for armored military target detection have made use of synthetic aperture radar (SAR) data due to its robustness to all weather, long range, and high-resolution characteristics. Nevertheless, the costs of SAR data acquisition and processing are still much higher than those of the conventional RGB camera, which is a more affordable alternative with significantly lower data processing time. Furthermore, the lack of military target detection datasets limits the use of such a low-cost approach. To mitigate these issues, we propose to generate RGB-based synthetic data using a photorealistic visual tool, Unreal Engine, for military target detection in a cross-domain setting. To this end, we conducted synthetic-to-real transfer experiments by training our synthetic dataset and validating on our web-collected real military target datasets. We benchmark the state-of-the-art domain adaptation methods distinguished by the degree of supervision on our proposed train-val dataset pair, and find that current methods using minimal hints on the image (e.g., object class) achieve a substantial improvement over unsupervised or semi-supervised DA methods. From these observations, we recognize the current challenges that remain to be overcome.&lt;/p&gt;</content:encoded></item><item><title>Cross-modality Feature Aggregation for Cross-domain Point Cloud Representation Learning</title><link>https://doi.org/10.1109/tip.2025.3646890</link><guid>10.1109/tip.2025.3646890</guid><pubDate>Wed, 31 Dec 2025 18:46:40 +0000</pubDate><dc:creator>Guoqing Wang</dc:creator><dc:creator>Chao Ma</dc:creator><dc:creator>Xiaokang Yang</dc:creator><prism:publicationName>IEEE Transactions on Image Processing</prism:publicationName><prism:doi>10.1109/tip.2025.3646890</prism:doi><description>Existing methods for learning 3D point cloud representation often use a single dataset-specific training and testing approach, leading to performance drops due to significant domain shifts between training and testing data. While recent crossdomain methods have made promising progress, the lack of inherent semantic information in point clouds makes models prone to overfitting specific datasets. As such, we introduce 3D-CFA, a simple yet effective cross-modality feature aggregation method for cross-domain 3D point cloud representation learning. 3D-CFA aggregates the geometry tokens with semantic tokens derived from multi-view images, which are projected from the point cloud, thus generating more transferable features for cross-domain 3D point cloud representation learning. Specifically, 3D-CFA consists of two main components: a cross-modality feature aggregation module and an elastic domain alignment module. The cross-modality feature aggregation module converts unordered points into multi-view images using the modality transformation module. Then, the geometry tokens and semantic tokens extracted from the geometry encoder and semantic encoder are fed into the cross-modal projector to get the transferable 3D tokens. A key insight of this design is that the semantic tokens can serve as a bridge between the 3D point cloud model and the 2D foundation model, greatly promoting the generalization of cross-domain models facing the severe domain shift. Finally, the elastic domain alignment module learns the hierarchical domain-invariant features of different training domains for either domain adaptation or domain generalization protocols. 3D-CFA finds a better way to transfer the knowledge of the 2D foundation model pre-trained at scale, meanwhile only introducing a few extra trainable parameters. Comprehensive experiments on several cross-domain point cloud benchmarks demonstrate the effectiveness of the proposed method compared to the state-of-the-art methods.
Published: 2025-12-31T18:46:40+00:00
Venue: IEEE Transactions on Image Processing
Score: 0.810 (must_read)</description><content:encoded>&lt;p&gt;&lt;strong&gt;Authors:&lt;/strong&gt; Guoqing Wang; Chao Ma; Xiaokang Yang&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Journal:&lt;/strong&gt; IEEE Transactions on Image Processing&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;a href="https://doi.org/10.1109/tip.2025.3646890"&gt;10.1109/tip.2025.3646890&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Relevance:&lt;/strong&gt; 0.810 (must_read)&lt;/p&gt;
&lt;p&gt;Existing methods for learning 3D point cloud representation often use a single dataset-specific training and testing approach, leading to performance drops due to significant domain shifts between training and testing data. While recent crossdomain methods have made promising progress, the lack of inherent semantic information in point clouds makes models prone to overfitting specific datasets. As such, we introduce 3D-CFA, a simple yet effective cross-modality feature aggregation method for cross-domain 3D point cloud representation learning. 3D-CFA aggregates the geometry tokens with semantic tokens derived from multi-view images, which are projected from the point cloud, thus generating more transferable features for cross-domain 3D point cloud representation learning. Specifically, 3D-CFA consists of two main components: a cross-modality feature aggregation module and an elastic domain alignment module. The cross-modality feature aggregation module converts unordered points into multi-view images using the modality transformation module. Then, the geometry tokens and semantic tokens extracted from the geometry encoder and semantic encoder are fed into the cross-modal projector to get the transferable 3D tokens. A key insight of this design is that the semantic tokens can serve as a bridge between the 3D point cloud model and the 2D foundation model, greatly promoting the generalization of cross-domain models facing the severe domain shift. Finally, the elastic domain alignment module learns the hierarchical domain-invariant features of different training domains for either domain adaptation or domain generalization protocols. 3D-CFA finds a better way to transfer the knowledge of the 2D foundation model pre-trained at scale, meanwhile only introducing a few extra trainable parameters. Comprehensive experiments on several cross-domain point cloud benchmarks demonstrate the effectiveness of the proposed method compared to the state-of-the-art methods.&lt;/p&gt;</content:encoded></item><item><title>Harnessing the power of single-cell large language models with parameter-efficient fine-tuning using scPEFT</title><link>https://doi.org/10.1038/s42256-025-01170-z</link><guid>10.1038/s42256-025-01170-z</guid><pubDate>Wed, 31 Dec 2025 10:02:00 +0000</pubDate><dc:creator>Fei He</dc:creator><dc:creator>Ruixin Fei</dc:creator><dc:creator>Jordan E. Krull</dc:creator><dc:creator>Yang Yu</dc:creator><dc:creator>Xinyu Zhang</dc:creator><dc:creator>Xianyu Wang</dc:creator><dc:creator>Hao Cheng</dc:creator><dc:creator>Mingyue Gao</dc:creator><dc:creator>Li Su</dc:creator><dc:creator>Yibo Chen</dc:creator><dc:creator>Jinpu Li</dc:creator><dc:creator>Baichuan Jin</dc:creator><dc:creator>Yuzhou Chang</dc:creator><dc:creator>Anjun Ma</dc:creator><dc:creator>Qin Ma</dc:creator><dc:creator>Dong Xu</dc:creator><prism:publicationName>Nature Machine Intelligence</prism:publicationName><prism:doi>10.1038/s42256-025-01170-z</prism:doi><description>Single-cell large language models (scLLMs) capture essential biological insights from vast single-cell atlases but struggle in out-of-context applications, where zero-shot predictions can be unreliable. To address this, here we introduce a single-cell parameter-efficient fine-tuning (scPEFT) framework that integrates learnable, low-dimensional adapters into scLLMs. By freezing the backbone model and updating only the adapter parameters, scPEFT efficiently adapts to specific tasks using limited custom data. This approach mitigates catastrophic forgetting, reduces parameter tuning by over 96% and decreases GPU memory usage by more than half, thus substantially enhancing the accessibility of scLLMs for resource-constrained researchers. When validated across diverse datasets, scPEFT outperformed zero-shot models and traditional fine-tuning in disease-specific, cross-species and undercharacterized cell population tasks. Its attention-mechanism analysis identified COVID-related genes associated with specific cell states and uncovered unique blood cell subpopulations, demonstrating the capacity of scPEFT for condition-specific interpretations. These findings position scPEFT as an efficient solution for enhancing the utility of scLLMs in general single-cell analyses. He et al. present a parameter-efficient fine-tuning method for single-cell language models that improves performance on unseen diseases, treatments and cell types.
Published: 2025-12-31T10:02:00+00:00
Venue: Nature Machine Intelligence
Score: 0.809 (must_read)</description><content:encoded>&lt;p&gt;&lt;strong&gt;Authors:&lt;/strong&gt; Fei He; Ruixin Fei; Jordan E. Krull; Yang Yu; Xinyu Zhang; Xianyu Wang; Hao Cheng; Mingyue Gao; Li Su; Yibo Chen; Jinpu Li; Baichuan Jin; Yuzhou Chang; Anjun Ma; Qin Ma; Dong Xu&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Journal:&lt;/strong&gt; Nature Machine Intelligence&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;a href="https://doi.org/10.1038/s42256-025-01170-z"&gt;10.1038/s42256-025-01170-z&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Relevance:&lt;/strong&gt; 0.809 (must_read)&lt;/p&gt;
&lt;p&gt;Single-cell large language models (scLLMs) capture essential biological insights from vast single-cell atlases but struggle in out-of-context applications, where zero-shot predictions can be unreliable. To address this, here we introduce a single-cell parameter-efficient fine-tuning (scPEFT) framework that integrates learnable, low-dimensional adapters into scLLMs. By freezing the backbone model and updating only the adapter parameters, scPEFT efficiently adapts to specific tasks using limited custom data. This approach mitigates catastrophic forgetting, reduces parameter tuning by over 96% and decreases GPU memory usage by more than half, thus substantially enhancing the accessibility of scLLMs for resource-constrained researchers. When validated across diverse datasets, scPEFT outperformed zero-shot models and traditional fine-tuning in disease-specific, cross-species and undercharacterized cell population tasks. Its attention-mechanism analysis identified COVID-related genes associated with specific cell states and uncovered unique blood cell subpopulations, demonstrating the capacity of scPEFT for condition-specific interpretations. These findings position scPEFT as an efficient solution for enhancing the utility of scLLMs in general single-cell analyses. He et al. present a parameter-efficient fine-tuning method for single-cell language models that improves performance on unseen diseases, treatments and cell types.&lt;/p&gt;</content:encoded></item><item><title>Few-Shot Object Detection on Remote Sensing Images Based on Decoupled Training, Contrastive Learning and Self-Training</title><link>https://doi.org/10.1109/jstars.2025.3650394</link><guid>10.1109/jstars.2025.3650394</guid><pubDate>Thu, 01 Jan 2026 18:37:35 +0000</pubDate><dc:creator>Shun Zhang</dc:creator><dc:creator>Xuebin Zhang</dc:creator><dc:creator>Yaohui Xu</dc:creator><dc:creator>Ke Wang</dc:creator><prism:publicationName>IEEE Journal of Selected Topics in Applied Earth Observations and Remote Sensing</prism:publicationName><prism:doi>10.1109/jstars.2025.3650394</prism:doi><description>Few-shot object detection (FSOD) in remote sensing imagery faces two critical challenges compared to general methods trained on large datasets: (1) Only a few labeled instances leveraged as the training set significantly limit the feature representation learning of deep neural networks; (2) Remote sensing image data contains complicated background and multiple objects with greatly different sizes in the same image, which leads the detector to large numbers of false alarms and miss detections. This paper proposes a FSOD framework (called DeCL-Det) that applies self-training to generate high-quality pseudo-annotations from unlabeled target domain data. These refined pseudo-labels are iteratively integrated into the training set to expand supervision for novel classes. An auxiliary network is introduced to mitigate label noise by rectifying misclassifications in pseudo-labeled regions, ensuring robust learning. For multi-scale feature learning, we propose a gradient-decoupled framework, GCFPN, combining Feature Pyramid Networks (FPN) with a Gradient Decoupled Layer (GDL). FPN is to extract multi-scale feature representations, and GDL is to decouple the modules between the Region Proposal Network (RPN) and RCNN head into two stages or tasks through gradients. The two modules, FPN and GDL, train Faster R-CNN in a decoupled way to facilitate the multi-scale feature learning of novel objects. To further enhance the classification ability, we introduce a supervised contrastive learning head to enhance feature discrimination, reinforcing robustness in few-shot object detection. Experiments on the DIOR dataset indicate that our method performs better than several existing approaches and achieves competitive results.
Published: 2026-01-01T18:37:35+00:00
Venue: IEEE Journal of Selected Topics in Applied Earth Observations and Remote Sensing
Score: 0.809 (must_read)</description><content:encoded>&lt;p&gt;&lt;strong&gt;Authors:&lt;/strong&gt; Shun Zhang; Xuebin Zhang; Yaohui Xu; Ke Wang&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Journal:&lt;/strong&gt; IEEE Journal of Selected Topics in Applied Earth Observations and Remote Sensing&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;a href="https://doi.org/10.1109/jstars.2025.3650394"&gt;10.1109/jstars.2025.3650394&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Relevance:&lt;/strong&gt; 0.809 (must_read)&lt;/p&gt;
&lt;p&gt;Few-shot object detection (FSOD) in remote sensing imagery faces two critical challenges compared to general methods trained on large datasets: (1) Only a few labeled instances leveraged as the training set significantly limit the feature representation learning of deep neural networks; (2) Remote sensing image data contains complicated background and multiple objects with greatly different sizes in the same image, which leads the detector to large numbers of false alarms and miss detections. This paper proposes a FSOD framework (called DeCL-Det) that applies self-training to generate high-quality pseudo-annotations from unlabeled target domain data. These refined pseudo-labels are iteratively integrated into the training set to expand supervision for novel classes. An auxiliary network is introduced to mitigate label noise by rectifying misclassifications in pseudo-labeled regions, ensuring robust learning. For multi-scale feature learning, we propose a gradient-decoupled framework, GCFPN, combining Feature Pyramid Networks (FPN) with a Gradient Decoupled Layer (GDL). FPN is to extract multi-scale feature representations, and GDL is to decouple the modules between the Region Proposal Network (RPN) and RCNN head into two stages or tasks through gradients. The two modules, FPN and GDL, train Faster R-CNN in a decoupled way to facilitate the multi-scale feature learning of novel objects. To further enhance the classification ability, we introduce a supervised contrastive learning head to enhance feature discrimination, reinforcing robustness in few-shot object detection. Experiments on the DIOR dataset indicate that our method performs better than several existing approaches and achieves competitive results.&lt;/p&gt;</content:encoded></item><item><title>Learning intermediate physical states for inverse metasurface design</title><link>https://doi.org/10.1038/s42256-025-01167-8</link><guid>10.1038/s42256-025-01167-8</guid><pubDate>Wed, 31 Dec 2025 16:02:04 +0000</pubDate><dc:creator>Chun-Teh Chen</dc:creator><prism:publicationName>Nature Machine Intelligence</prism:publicationName><prism:doi>10.1038/s42256-025-01167-8</prism:doi><description>Deep generative models that learn intermediate surface-current maps, rather than layouts directly, offer a more stable route to inverse design of tunable and stacked metasurfaces.
Published: 2025-12-31T16:02:04+00:00
Venue: Nature Machine Intelligence
Score: 0.809 (must_read)</description><content:encoded>&lt;p&gt;&lt;strong&gt;Authors:&lt;/strong&gt; Chun-Teh Chen&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Journal:&lt;/strong&gt; Nature Machine Intelligence&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;a href="https://doi.org/10.1038/s42256-025-01167-8"&gt;10.1038/s42256-025-01167-8&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Relevance:&lt;/strong&gt; 0.809 (must_read)&lt;/p&gt;
&lt;p&gt;Deep generative models that learn intermediate surface-current maps, rather than layouts directly, offer a more stable route to inverse design of tunable and stacked metasurfaces.&lt;/p&gt;</content:encoded></item><item><title>MSCK-Net: Multiscale Chinese Knot Convolutional Network for Dim and Small Infrared Ship Detection</title><link>https://doi.org/10.1109/tgrs.2025.3649839</link><guid>10.1109/tgrs.2025.3649839</guid><pubDate>Wed, 31 Dec 2025 18:44:00 +0000</pubDate><dc:creator>Yuhao Lin</dc:creator><dc:creator>Dongliang Peng</dc:creator><dc:creator>Liang Wang</dc:creator><dc:creator>Lingjie Jiang</dc:creator><dc:creator>Haewoon Nam</dc:creator><prism:publicationName>IEEE Transactions on Geoscience and Remote Sensing</prism:publicationName><prism:doi>10.1109/tgrs.2025.3649839</prism:doi><description>Remote sensing infrared ship detection is crucial for maritime safety and traffic management in civil-military integrated applications. However, the detection accuracy and robustness of current methods are still limited by dataset constraints, including small scale, narrow target size distribution, sparse targets, and high scenario specificity. To address these problems, we integrate publicly available datasets to construct IRShip—a relatively large-scale infrared ship detection dataset comprising 27,138 images, significantly improving data scale and diversity. We further design a copy-poisson blend (CP-PB) offline data augmentation approach and introduce a dense one-to-one (Dense-O2O) online augmentation strategy, which improve adaptability to complex backgrounds and scale variations, mitigate problems from sparse targets, and improve dataset utility and model robustness. We propose MSCK-Net, a multiscale chinese knot convolutional network tailored for detecting dim and small infrared ship targets (DSIRST). Specifically, we propose a novel Chinese Knot Convolution (CKConv) that better aligns with both the local features of small targets and the morphological characteristics of ship targets, thereby significantly enhancing low-level feature representation. Built with CKConv, the multiscale knot block (MSK-Block) and Stem-ck modules enhance deep feature transmission efficiency and global modeling of DSIRST, leading to notable gains in detection performance. Extensive experiments on IRShip, NUDT-SIRST-Sea, ISDD, IRSDSS, and Maritime-sirst demonstrate that MSCK-Net-M achieves state-of-the-art performance, with AP50, AP75, and AP of 82.5%, 53.1%, and 50.9% on IRShip, significantly outperforming the existing 20 general object detectors and 6 infrared target detectors. Generalization experiments on four sub-datasets further verify the effectiveness and robustness of our proposed method. The code and dataset are available at: https://github.com/sjxxrh/MSCK-Net.
Published: 2025-12-31T18:44:00+00:00
Venue: IEEE Transactions on Geoscience and Remote Sensing
Score: 0.803 (must_read)</description><content:encoded>&lt;p&gt;&lt;strong&gt;Authors:&lt;/strong&gt; Yuhao Lin; Dongliang Peng; Liang Wang; Lingjie Jiang; Haewoon Nam&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Journal:&lt;/strong&gt; IEEE Transactions on Geoscience and Remote Sensing&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;a href="https://doi.org/10.1109/tgrs.2025.3649839"&gt;10.1109/tgrs.2025.3649839&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Relevance:&lt;/strong&gt; 0.803 (must_read)&lt;/p&gt;
&lt;p&gt;Remote sensing infrared ship detection is crucial for maritime safety and traffic management in civil-military integrated applications. However, the detection accuracy and robustness of current methods are still limited by dataset constraints, including small scale, narrow target size distribution, sparse targets, and high scenario specificity. To address these problems, we integrate publicly available datasets to construct IRShip—a relatively large-scale infrared ship detection dataset comprising 27,138 images, significantly improving data scale and diversity. We further design a copy-poisson blend (CP-PB) offline data augmentation approach and introduce a dense one-to-one (Dense-O2O) online augmentation strategy, which improve adaptability to complex backgrounds and scale variations, mitigate problems from sparse targets, and improve dataset utility and model robustness. We propose MSCK-Net, a multiscale chinese knot convolutional network tailored for detecting dim and small infrared ship targets (DSIRST). Specifically, we propose a novel Chinese Knot Convolution (CKConv) that better aligns with both the local features of small targets and the morphological characteristics of ship targets, thereby significantly enhancing low-level feature representation. Built with CKConv, the multiscale knot block (MSK-Block) and Stem-ck modules enhance deep feature transmission efficiency and global modeling of DSIRST, leading to notable gains in detection performance. Extensive experiments on IRShip, NUDT-SIRST-Sea, ISDD, IRSDSS, and Maritime-sirst demonstrate that MSCK-Net-M achieves state-of-the-art performance, with AP50, AP75, and AP of 82.5%, 53.1%, and 50.9% on IRShip, significantly outperforming the existing 20 general object detectors and 6 infrared target detectors. Generalization experiments on four sub-datasets further verify the effectiveness and robustness of our proposed method. The code and dataset are available at: https://github.com/sjxxrh/MSCK-Net.&lt;/p&gt;</content:encoded></item><item><title>Multispectral Remote Sensing Object Detection via Selective Cross-modal Interaction and Aggregation</title><link>https://doi.org/10.1016/j.neunet.2025.108533</link><guid>10.1016/j.neunet.2025.108533</guid><pubDate>Wed, 31 Dec 2025 00:15:06 +0000</pubDate><dc:creator>Minghao Cui</dc:creator><dc:creator>Jing Nie</dc:creator><dc:creator>Hanqing Sun</dc:creator><dc:creator>Jin Xie</dc:creator><dc:creator>Jiale Cao</dc:creator><dc:creator>Yanwei Pang</dc:creator><dc:creator>Xuelong Li</dc:creator><prism:publicationName>Neural Networks</prism:publicationName><prism:doi>10.1016/j.neunet.2025.108533</prism:doi><description>Multispectral remote sensing object detection plays a vital role in a wide range of geoscience and remote sensing applications, such as environmental monitoring and disaster monitoring, by leveraging complementary information from RGB and infrared modalities. The performance of such systems heavily relies on the effective fusion of information across these modalities. A key challenge lies in capturing meaningful cross-modal long-range dependencies that aid object localization and identification, while simultaneously suppressing noise and irrelevant information during feature fusion to enhance the discriminative quality of the fused representations. To address these challenges, we propose a novel framework, termed Selective cross-modal Interaction and Aggregation (SIA), which comprises two key components: the Selective Cross-modal Interaction (SCI) module and the Selective Feature Aggregation (SFA) module. The SCI module addresses the inefficiency of traditional cross-modal attention mechanisms by selectively prioritizing the most informative long-range dependencies. This significantly reduces computational costs while maintaining high detection accuracy. The SFA module utilizes a gating mechanism to effectively filter out noise and redundant information introduced by equal-weight fusion, thereby yielding more discriminative feature representations. Comprehensive experiments are conducted on the challenging multispectral remote sensing object detection benchmark DroneVehicle, as well as two additional multispectral urban object detection datasets, M 3 FD and LLVIP. The proposed approach consistently achieves superior detection accuracy across all datasets. Notably, on the DroneVehicle test set, our method outperforms the recently introduced C 2 Former by 2.8% mAP@0.5, while incurring lower computational cost.
Published: 2025-12-31T00:15:06+00:00
Venue: Neural Networks
Score: 0.802 (must_read)</description><content:encoded>&lt;p&gt;&lt;strong&gt;Authors:&lt;/strong&gt; Minghao Cui; Jing Nie; Hanqing Sun; Jin Xie; Jiale Cao; Yanwei Pang; Xuelong Li&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Journal:&lt;/strong&gt; Neural Networks&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;a href="https://doi.org/10.1016/j.neunet.2025.108533"&gt;10.1016/j.neunet.2025.108533&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Relevance:&lt;/strong&gt; 0.802 (must_read)&lt;/p&gt;
&lt;p&gt;Multispectral remote sensing object detection plays a vital role in a wide range of geoscience and remote sensing applications, such as environmental monitoring and disaster monitoring, by leveraging complementary information from RGB and infrared modalities. The performance of such systems heavily relies on the effective fusion of information across these modalities. A key challenge lies in capturing meaningful cross-modal long-range dependencies that aid object localization and identification, while simultaneously suppressing noise and irrelevant information during feature fusion to enhance the discriminative quality of the fused representations. To address these challenges, we propose a novel framework, termed Selective cross-modal Interaction and Aggregation (SIA), which comprises two key components: the Selective Cross-modal Interaction (SCI) module and the Selective Feature Aggregation (SFA) module. The SCI module addresses the inefficiency of traditional cross-modal attention mechanisms by selectively prioritizing the most informative long-range dependencies. This significantly reduces computational costs while maintaining high detection accuracy. The SFA module utilizes a gating mechanism to effectively filter out noise and redundant information introduced by equal-weight fusion, thereby yielding more discriminative feature representations. Comprehensive experiments are conducted on the challenging multispectral remote sensing object detection benchmark DroneVehicle, as well as two additional multispectral urban object detection datasets, M 3 FD and LLVIP. The proposed approach consistently achieves superior detection accuracy across all datasets. Notably, on the DroneVehicle test set, our method outperforms the recently introduced C 2 Former by 2.8% mAP@0.5, while incurring lower computational cost.&lt;/p&gt;</content:encoded></item><item><title>Coupled Diffusion Posterior Sampling for Unsupervised Hyperspectral and Multispectral Images Fusion</title><link>https://doi.org/10.1109/tip.2025.3647207</link><guid>10.1109/tip.2025.3647207</guid><pubDate>Thu, 01 Jan 2026 18:39:24 +0000</pubDate><dc:creator>Yang Xu</dc:creator><dc:creator>Jian Zhu</dc:creator><dc:creator>Danfeng Hong</dc:creator><dc:creator>Zhihui Wei</dc:creator><dc:creator>Zebin Wu</dc:creator><prism:publicationName>IEEE Transactions on Image Processing</prism:publicationName><prism:doi>10.1109/tip.2025.3647207</prism:doi><description>Hyperspectral images (HSIs) and multispectral images (MSIs) fusion is a hot topic in the remote sensing society. A high-resolution HSI (HR-HSI) can be obtained by fusing a low-resolution HSI (LR-HSI) and a high-resolution MSI (HR-MSI) or RGB image. However, most deep learning-based methods require a large amount of HR-HSIs for supervised training, which is very rare in practice. In this paper, we propose a coupled diffusion posterior sampling (CDPS) method for HSI and MSI fusion in which the HR-HSIs are no longer required in the training process. Because the LR-HSI contains the spectral information and HR-MSI contains the spatial information of the captured scene, we design an unsupervised strategy that learns the required diffusion priors directly and solely from the input test image pair (the LR-HSI and HR-MSI themselves). Then, a coupled diffusion posterior sampling method is proposed to introduce the two priors in the diffusion posterior sampling which leverages the observed LR-HSI and HR-MSI as fidelity terms. Experimental results demonstrate that the proposed method outperforms other state-of-the-art unsupervised HSI and MSI fusion methods. Additionally, this method utilizes smaller networks that are simpler and easier to train without other data.
Published: 2026-01-01T18:39:24+00:00
Venue: IEEE Transactions on Image Processing
Score: 0.802 (must_read)</description><content:encoded>&lt;p&gt;&lt;strong&gt;Authors:&lt;/strong&gt; Yang Xu; Jian Zhu; Danfeng Hong; Zhihui Wei; Zebin Wu&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Journal:&lt;/strong&gt; IEEE Transactions on Image Processing&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;a href="https://doi.org/10.1109/tip.2025.3647207"&gt;10.1109/tip.2025.3647207&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Relevance:&lt;/strong&gt; 0.802 (must_read)&lt;/p&gt;
&lt;p&gt;Hyperspectral images (HSIs) and multispectral images (MSIs) fusion is a hot topic in the remote sensing society. A high-resolution HSI (HR-HSI) can be obtained by fusing a low-resolution HSI (LR-HSI) and a high-resolution MSI (HR-MSI) or RGB image. However, most deep learning-based methods require a large amount of HR-HSIs for supervised training, which is very rare in practice. In this paper, we propose a coupled diffusion posterior sampling (CDPS) method for HSI and MSI fusion in which the HR-HSIs are no longer required in the training process. Because the LR-HSI contains the spectral information and HR-MSI contains the spatial information of the captured scene, we design an unsupervised strategy that learns the required diffusion priors directly and solely from the input test image pair (the LR-HSI and HR-MSI themselves). Then, a coupled diffusion posterior sampling method is proposed to introduce the two priors in the diffusion posterior sampling which leverages the observed LR-HSI and HR-MSI as fidelity terms. Experimental results demonstrate that the proposed method outperforms other state-of-the-art unsupervised HSI and MSI fusion methods. Additionally, this method utilizes smaller networks that are simpler and easier to train without other data.&lt;/p&gt;</content:encoded></item><item><title>Semi-Supervised Diversity-Aware Domain Adaptation for 3D Object detection</title><link>https://arxiv.org/abs/2512.24922v1</link><guid>http://arxiv.org/abs/2512.24922v1</guid><pubDate>Wed, 31 Dec 2025 15:26:09 +0000</pubDate><dc:creator>Bartłomiej Olber</dc:creator><dc:creator>Jakub Winter</dc:creator><dc:creator>Paweł Wawrzyński</dc:creator><dc:creator>Andrii Gamalii</dc:creator><dc:creator>Daniel Górniak</dc:creator><dc:creator>Marcin Łojek</dc:creator><dc:creator>Robert Nowak</dc:creator><dc:creator>Krystian Radlak</dc:creator><dc:type>preprint</dc:type><prism:publicationName>arXiv</prism:publicationName><description>3D object detectors are fundamental components of perception systems in autonomous vehicles. While these detectors achieve remarkable performance on standard autonomous driving benchmarks, they often struggle to generalize across different domains - for instance, a model trained in the U.S. may perform poorly in regions like Asia or Europe. This paper presents a novel lidar domain adaptation method based on neuron activation patterns, demonstrating that state-of-the-art performance can be achieved by annotating only a small, representative, and diverse subset of samples from the target domain if they are correctly selected. The proposed approach requires very small annotation budget and, when combined with post-training techniques inspired by continual learning prevent weight drift from the original model. Empirical evaluation shows that the proposed domain adaptation approach outperforms both linear probing and state-of-the-art domain adaptation techniques.
Published: 2025-12-31T15:26:09+00:00
Venue: arXiv
Score: 0.801 (must_read)</description><content:encoded>&lt;p&gt;&lt;strong&gt;Authors:&lt;/strong&gt; Bartłomiej Olber; Jakub Winter; Paweł Wawrzyński; Andrii Gamalii; Daniel Górniak; Marcin Łojek; Robert Nowak; Krystian Radlak&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Journal:&lt;/strong&gt; arXiv&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Relevance:&lt;/strong&gt; 0.801 (must_read)&lt;/p&gt;
&lt;p&gt;3D object detectors are fundamental components of perception systems in autonomous vehicles. While these detectors achieve remarkable performance on standard autonomous driving benchmarks, they often struggle to generalize across different domains - for instance, a model trained in the U.S. may perform poorly in regions like Asia or Europe. This paper presents a novel lidar domain adaptation method based on neuron activation patterns, demonstrating that state-of-the-art performance can be achieved by annotating only a small, representative, and diverse subset of samples from the target domain if they are correctly selected. The proposed approach requires very small annotation budget and, when combined with post-training techniques inspired by continual learning prevent weight drift from the original model. Empirical evaluation shows that the proposed domain adaptation approach outperforms both linear probing and state-of-the-art domain adaptation techniques.&lt;/p&gt;</content:encoded></item><item><title>DARFNet: A Divergence-Aware Reciprocal Fusion Network for Multispectral Feature Alignment and Fusion</title><link>https://doi.org/10.1109/jstars.2025.3647819</link><guid>10.1109/jstars.2025.3647819</guid><pubDate>Wed, 31 Dec 2025 18:44:28 +0000</pubDate><dc:creator>Junyu Huang</dc:creator><dc:creator>Jiawei Chen</dc:creator><dc:creator>Renbo Luo</dc:creator><dc:creator>Yongan Lu</dc:creator><dc:creator>Jinxin Yang</dc:creator><dc:creator>Zhifeng Wu</dc:creator><prism:publicationName>IEEE Journal of Selected Topics in Applied Earth Observations and Remote Sensing</prism:publicationName><prism:doi>10.1109/jstars.2025.3647819</prism:doi><description>Robust detection of small objects in remote sensing imagery remains a significant challenge due to complex backgrounds, scale variation, and modality inconsistency. In this article, we propose DARFNet, a novel multispectral detection framework that effectively integrates RGB and infrared information for accurate small object localization. DARFNet employs a dual-branch architecture with a dynamic attention-based fusion mechanism to adaptively enhance complementary features. In addition, we incorporate lightweight yet expressive modules–ODConv and ConvNeXtBlock–to boost detection performance while maintaining computational efficiency. Extensive experiments on three widely-used benchmarks, including VEDAI, NWPU, and DroneVehicle, demonstrate that DARFNet outperforms state-of-the-art methods in both accuracy and efficiency. Notably, our model shows superior performance in detecting small and densely distributed targets under complex remote sensing conditions.
Published: 2025-12-31T18:44:28+00:00
Venue: IEEE Journal of Selected Topics in Applied Earth Observations and Remote Sensing
Score: 0.800 (must_read)</description><content:encoded>&lt;p&gt;&lt;strong&gt;Authors:&lt;/strong&gt; Junyu Huang; Jiawei Chen; Renbo Luo; Yongan Lu; Jinxin Yang; Zhifeng Wu&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Journal:&lt;/strong&gt; IEEE Journal of Selected Topics in Applied Earth Observations and Remote Sensing&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;a href="https://doi.org/10.1109/jstars.2025.3647819"&gt;10.1109/jstars.2025.3647819&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Relevance:&lt;/strong&gt; 0.800 (must_read)&lt;/p&gt;
&lt;p&gt;Robust detection of small objects in remote sensing imagery remains a significant challenge due to complex backgrounds, scale variation, and modality inconsistency. In this article, we propose DARFNet, a novel multispectral detection framework that effectively integrates RGB and infrared information for accurate small object localization. DARFNet employs a dual-branch architecture with a dynamic attention-based fusion mechanism to adaptively enhance complementary features. In addition, we incorporate lightweight yet expressive modules–ODConv and ConvNeXtBlock–to boost detection performance while maintaining computational efficiency. Extensive experiments on three widely-used benchmarks, including VEDAI, NWPU, and DroneVehicle, demonstrate that DARFNet outperforms state-of-the-art methods in both accuracy and efficiency. Notably, our model shows superior performance in detecting small and densely distributed targets under complex remote sensing conditions.&lt;/p&gt;</content:encoded></item><item><title>Cross-Modality Fusion of Visible Light, Infrared, and SAR Images Under Few-Shot Conditions for Target Recognition</title><link>https://doi.org/10.1109/jstars.2025.3649648</link><guid>10.1109/jstars.2025.3649648</guid><pubDate>Wed, 31 Dec 2025 18:44:28 +0000</pubDate><dc:creator>Chao Li</dc:creator><dc:creator>Jiacheng Ni</dc:creator><dc:creator>Ying Luo</dc:creator><dc:creator>Dan Wang</dc:creator><dc:creator>Qun Zhang</dc:creator><prism:publicationName>IEEE Journal of Selected Topics in Applied Earth Observations and Remote Sensing</prism:publicationName><prism:doi>10.1109/jstars.2025.3649648</prism:doi><description>In recent years, the advancement of remote sensing technology has driven the widespread application of multi-modal image fusion recognition technology. By integrating data from different sensors, this technology achieves target information complementarity and improves recognition accuracy. However, it still faces challenges including data heterogeneity and feature redundancy, particularly under few-shot conditions. To address this issue, this paper proposes a multi-source heterogeneous image fusion recognition (MHIF) method for few-shot scenarios. First, we propose a cross-modal sampling (CMS) module with a maximum traversal (MT) method to efficiently generate diverse training combinations, thereby expanding the effective sample space. Next, we design an image quality assessment (IQA) module that adaptively weights features from different modalities, optimizing fusion by emphasizing high-quality information. Furthermore, we propose an intra-modal bidirectional guided cross-attention (IBGC) module to mutually enhance the base and modality-specific features, effectively preserving critical details within each modality. Finally, a stepwise fusion strategy progressively integrates these refined features, effectively reducing redundancy and modal interference. Experimental results on few-shot datasets demonstrate that the proposed method exhibits significant advantages in few-shot multi-modal image fusion recognition tasks.
Published: 2025-12-31T18:44:28+00:00
Venue: IEEE Journal of Selected Topics in Applied Earth Observations and Remote Sensing
Score: 0.799 (must_read)</description><content:encoded>&lt;p&gt;&lt;strong&gt;Authors:&lt;/strong&gt; Chao Li; Jiacheng Ni; Ying Luo; Dan Wang; Qun Zhang&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Journal:&lt;/strong&gt; IEEE Journal of Selected Topics in Applied Earth Observations and Remote Sensing&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;a href="https://doi.org/10.1109/jstars.2025.3649648"&gt;10.1109/jstars.2025.3649648&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Relevance:&lt;/strong&gt; 0.799 (must_read)&lt;/p&gt;
&lt;p&gt;In recent years, the advancement of remote sensing technology has driven the widespread application of multi-modal image fusion recognition technology. By integrating data from different sensors, this technology achieves target information complementarity and improves recognition accuracy. However, it still faces challenges including data heterogeneity and feature redundancy, particularly under few-shot conditions. To address this issue, this paper proposes a multi-source heterogeneous image fusion recognition (MHIF) method for few-shot scenarios. First, we propose a cross-modal sampling (CMS) module with a maximum traversal (MT) method to efficiently generate diverse training combinations, thereby expanding the effective sample space. Next, we design an image quality assessment (IQA) module that adaptively weights features from different modalities, optimizing fusion by emphasizing high-quality information. Furthermore, we propose an intra-modal bidirectional guided cross-attention (IBGC) module to mutually enhance the base and modality-specific features, effectively preserving critical details within each modality. Finally, a stepwise fusion strategy progressively integrates these refined features, effectively reducing redundancy and modal interference. Experimental results on few-shot datasets demonstrate that the proposed method exhibits significant advantages in few-shot multi-modal image fusion recognition tasks.&lt;/p&gt;</content:encoded></item><item><title>YOLO-Master: MOE-Accelerated with Specialized Transformers for Enhanced Real-time Detection</title><link>https://arxiv.org/abs/2512.23273v2</link><guid>http://arxiv.org/abs/2512.23273v2</guid><pubDate>Mon, 29 Dec 2025 07:54:49 +0000</pubDate><dc:creator>Xu Lin</dc:creator><dc:creator>Jinlong Peng</dc:creator><dc:creator>Zhenye Gan</dc:creator><dc:creator>Jiawen Zhu</dc:creator><dc:creator>Jun Liu</dc:creator><dc:type>preprint</dc:type><prism:publicationName>arXiv</prism:publicationName><description>Existing Real-Time Object Detection (RTOD) methods commonly adopt YOLO-like architectures for their favorable trade-off between accuracy and speed. However, these models rely on static dense computation that applies uniform processing to all inputs, misallocating representational capacity and computational resources such as over-allocating on trivial scenes while under-serving complex ones. This mismatch results in both computational redundancy and suboptimal detection performance. To overcome this limitation, we propose YOLO-Master, a novel YOLO-like framework that introduces instance-conditional adaptive computation for RTOD. This is achieved through a Efficient Sparse Mixture-of-Experts (ES-MoE) block that dynamically allocates computational resources to each input according to its scene complexity. At its core, a lightweight dynamic routing network guides expert specialization during training through a diversity enhancing objective, encouraging complementary expertise among experts. Additionally, the routing network adaptively learns to activate only the most relevant experts, thereby improving detection performance while minimizing computational overhead during inference. Comprehensive experiments on five large-scale benchmarks demonstrate the superiority of YOLO-Master. On MS COCO, our model achieves 42.4% AP with 1.62ms latency, outperforming YOLOv13-N by +0.8% mAP and 17.8% faster inference. Notably, the gains are most pronounced on challenging dense scenes, while the model preserves efficiency on typical inputs and maintains real-time inference speed. Code will be available.
Published: 2025-12-29T07:54:49+00:00
Venue: arXiv
Score: 0.799 (must_read)</description><content:encoded>&lt;p&gt;&lt;strong&gt;Authors:&lt;/strong&gt; Xu Lin; Jinlong Peng; Zhenye Gan; Jiawen Zhu; Jun Liu&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Journal:&lt;/strong&gt; arXiv&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Relevance:&lt;/strong&gt; 0.799 (must_read)&lt;/p&gt;
&lt;p&gt;Existing Real-Time Object Detection (RTOD) methods commonly adopt YOLO-like architectures for their favorable trade-off between accuracy and speed. However, these models rely on static dense computation that applies uniform processing to all inputs, misallocating representational capacity and computational resources such as over-allocating on trivial scenes while under-serving complex ones. This mismatch results in both computational redundancy and suboptimal detection performance. To overcome this limitation, we propose YOLO-Master, a novel YOLO-like framework that introduces instance-conditional adaptive computation for RTOD. This is achieved through a Efficient Sparse Mixture-of-Experts (ES-MoE) block that dynamically allocates computational resources to each input according to its scene complexity. At its core, a lightweight dynamic routing network guides expert specialization during training through a diversity enhancing objective, encouraging complementary expertise among experts. Additionally, the routing network adaptively learns to activate only the most relevant experts, thereby improving detection performance while minimizing computational overhead during inference. Comprehensive experiments on five large-scale benchmarks demonstrate the superiority of YOLO-Master. On MS COCO, our model achieves 42.4% AP with 1.62ms latency, outperforming YOLOv13-N by +0.8% mAP and 17.8% faster inference. Notably, the gains are most pronounced on challenging dense scenes, while the model preserves efficiency on typical inputs and maintains real-time inference speed. Code will be available.&lt;/p&gt;</content:encoded></item><item><title>FUSE-RSVLM: Feature Fusion Vision-Language Model for Remote Sensing</title><link>https://arxiv.org/abs/2512.24022v1</link><guid>http://arxiv.org/abs/2512.24022v1</guid><pubDate>Tue, 30 Dec 2025 06:48:07 +0000</pubDate><dc:creator>Yunkai Dang</dc:creator><dc:creator>Donghao Wang</dc:creator><dc:creator>Jiacheng Yang</dc:creator><dc:creator>Yifan Jiang</dc:creator><dc:creator>Meiyi Zhu</dc:creator><dc:creator>Yuekun Yang</dc:creator><dc:creator>Cong Wang</dc:creator><dc:creator>Qi Fan</dc:creator><dc:creator>Wenbin Li</dc:creator><dc:creator>Yang Gao</dc:creator><dc:type>preprint</dc:type><prism:publicationName>arXiv</prism:publicationName><description>Large vision-language models (VLMs) exhibit strong performance across various tasks. However, these VLMs encounter significant challenges when applied to the remote sensing domain due to the inherent differences between remote sensing images and natural images. Existing remote sensing VLMs often fail to extract fine-grained visual features and suffer from visual forgetting during deep language processing. To address this, we introduce MF-RSVLM, a Multi-Feature Fusion Remote Sensing Vision--Language Model that effectively extracts and fuses visual features for RS understanding. MF-RSVLM learns multi-scale visual representations and combines global context with local details, improving the capture of small and complex structures in RS scenes. A recurrent visual feature injection scheme ensures the language model remains grounded in visual evidence and reduces visual forgetting during generation. Extensive experiments on diverse RS benchmarks show that MF-RSVLM achieves state-of-the-art or highly competitive performance across remote sensing classification, image captioning, and VQA tasks. Our code is publicly available at https://github.com/Yunkaidang/RSVLM.
Published: 2025-12-30T06:48:07+00:00
Venue: arXiv
Score: 0.795 (must_read)</description><content:encoded>&lt;p&gt;&lt;strong&gt;Authors:&lt;/strong&gt; Yunkai Dang; Donghao Wang; Jiacheng Yang; Yifan Jiang; Meiyi Zhu; Yuekun Yang; Cong Wang; Qi Fan; Wenbin Li; Yang Gao&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Journal:&lt;/strong&gt; arXiv&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Relevance:&lt;/strong&gt; 0.795 (must_read)&lt;/p&gt;
&lt;p&gt;Large vision-language models (VLMs) exhibit strong performance across various tasks. However, these VLMs encounter significant challenges when applied to the remote sensing domain due to the inherent differences between remote sensing images and natural images. Existing remote sensing VLMs often fail to extract fine-grained visual features and suffer from visual forgetting during deep language processing. To address this, we introduce MF-RSVLM, a Multi-Feature Fusion Remote Sensing Vision--Language Model that effectively extracts and fuses visual features for RS understanding. MF-RSVLM learns multi-scale visual representations and combines global context with local details, improving the capture of small and complex structures in RS scenes. A recurrent visual feature injection scheme ensures the language model remains grounded in visual evidence and reduces visual forgetting during generation. Extensive experiments on diverse RS benchmarks show that MF-RSVLM achieves state-of-the-art or highly competitive performance across remote sensing classification, image captioning, and VQA tasks. Our code is publicly available at https://github.com/Yunkaidang/RSVLM.&lt;/p&gt;</content:encoded></item><item><title>Dimensional compensation for small-sample and small-size insulator burn mark via RGB-point cloud fusion in power grid inspection</title><link>https://doi.org/10.1016/j.inffus.2025.104105</link><guid>10.1016/j.inffus.2025.104105</guid><pubDate>Wed, 31 Dec 2025 00:19:21 +0000</pubDate><dc:creator>Junqiu Tang</dc:creator><dc:creator>Zhikang Yuan</dc:creator><dc:creator>Zixiang Wei</dc:creator><dc:creator>Shuojie Gao</dc:creator><dc:creator>Changyong Shen</dc:creator><prism:publicationName>Information Fusion</prism:publicationName><prism:doi>10.1016/j.inffus.2025.104105</prism:doi><description>To address the challenge of scarce burn mark samples in power infrastructure inspection, we introduce the Insulator Burn Mark RGB-Point Cloud (IBMR) dataset, the first publicly available benchmark featuring RGB-point clouds with pixel-level annotations for both insulators and burn marks. To tackle the critical issue of severe class imbalance caused by the vast number of background points and the small size of burn marks, we propose a novel two-stage RGB-point cloud segmentation framework. This framework integrates DCCU-Sampling, an innovative downsampling algorithm that effectively suppresses background points while preserving critical structures of the targets, and BB-Backtracking, a geometric recovery method that reconstructs fine-grained burn mark details lost during downsampling process. Experimental results validate the framework’s effectiveness, achieving 81.21% mIoU with 32 training samples and 68.37% mIoU with only 14 samples. The dataset is publicly available at https://huggingface.co/datasets/Junqiu-Tang/IBMR .
Published: 2025-12-31T00:19:21+00:00
Venue: Information Fusion
Score: 0.793 (must_read)</description><content:encoded>&lt;p&gt;&lt;strong&gt;Authors:&lt;/strong&gt; Junqiu Tang; Zhikang Yuan; Zixiang Wei; Shuojie Gao; Changyong Shen&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Journal:&lt;/strong&gt; Information Fusion&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;a href="https://doi.org/10.1016/j.inffus.2025.104105"&gt;10.1016/j.inffus.2025.104105&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Relevance:&lt;/strong&gt; 0.793 (must_read)&lt;/p&gt;
&lt;p&gt;To address the challenge of scarce burn mark samples in power infrastructure inspection, we introduce the Insulator Burn Mark RGB-Point Cloud (IBMR) dataset, the first publicly available benchmark featuring RGB-point clouds with pixel-level annotations for both insulators and burn marks. To tackle the critical issue of severe class imbalance caused by the vast number of background points and the small size of burn marks, we propose a novel two-stage RGB-point cloud segmentation framework. This framework integrates DCCU-Sampling, an innovative downsampling algorithm that effectively suppresses background points while preserving critical structures of the targets, and BB-Backtracking, a geometric recovery method that reconstructs fine-grained burn mark details lost during downsampling process. Experimental results validate the framework’s effectiveness, achieving 81.21% mIoU with 32 training samples and 68.37% mIoU with only 14 samples. The dataset is publicly available at https://huggingface.co/datasets/Junqiu-Tang/IBMR .&lt;/p&gt;</content:encoded></item><item><title>YOLO-CMFM: A Visible-SAR Multimodal Object Detection Method Based on Edge-Guided and Gated Cross-Attention Fusion</title><link>https://doi.org/10.3390/rs18010136</link><guid>10.3390/rs18010136</guid><pubDate>Wed, 31 Dec 2025 14:08:11 +0000</pubDate><dc:creator>Xuyang Zhao</dc:creator><dc:creator>Lijun Zhao</dc:creator><dc:creator>Keli Shi</dc:creator><dc:creator>Ruotian Ren</dc:creator><dc:creator>Zheng Zhang</dc:creator><prism:publicationName>Remote Sensing</prism:publicationName><prism:doi>10.3390/rs18010136</prism:doi><description>To address the challenges of cross-modal feature misalignment and ineffective information fusion caused by the inherent differences in imaging mechanisms, noise statistics, and semantic representations between visible and synthetic aperture radar (SAR) imagery, this paper proposes a multimodal remote sensing object detection method, namely YOLO-CMFM. Built upon the Ultralytics YOLOv11 framework, the proposed approach designs a Cross-Modal Fusion Module (CMFM) that systematically enhances detection accuracy and robustness from the perspectives of modality alignment, feature interaction, and adaptive fusion. Specifically, (1) a Learnable Edge-Guided Attention (LEGA) module is constructed, which leverages a learnable Gaussian saliency prior to achieve edge-oriented cross-modal alignment, effectively mitigating edge-structure mismatches across modalities; (2) a Bidirectional Cross-Attention (BCA) module is developed to enable deep semantic interaction and global contextual aggregation; (3) a Context-Guided Gating (CGG) module is designed to dynamically generate complementary weights based on multimodal source features and global contextual information, thereby achieving adaptive fusion across modalities. Extensive experiments conducted on the OGSOD 1.0 dataset demonstrate that the proposed YOLO-CMFM achieves an mAP@50 of 96.2% and an mAP@50:95 of 75.1%. While maintaining competitive performance comparable to mainstream approaches at lower IoU thresholds, the proposed method significantly outperforms existing counterparts at high IoU thresholds, highlighting its superior capability in precise object localization. Also, the experimental results on the OSPRC dataset demonstrate that the proposed method can consistently achieve stable gains under different kinds of imaging conditions, including diverse SAR polarizations, spatial resolutions, and cloud occlusion conditions. Moreover, the CMFM can be flexibly integrated into different detection frameworks, which further validates its strong generalization and transferability in multimodal remote sensing object detection tasks.
Published: 2025-12-31T14:08:11+00:00
Venue: Remote Sensing
Score: 0.792 (must_read)</description><content:encoded>&lt;p&gt;&lt;strong&gt;Authors:&lt;/strong&gt; Xuyang Zhao; Lijun Zhao; Keli Shi; Ruotian Ren; Zheng Zhang&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Journal:&lt;/strong&gt; Remote Sensing&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;a href="https://doi.org/10.3390/rs18010136"&gt;10.3390/rs18010136&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Relevance:&lt;/strong&gt; 0.792 (must_read)&lt;/p&gt;
&lt;p&gt;To address the challenges of cross-modal feature misalignment and ineffective information fusion caused by the inherent differences in imaging mechanisms, noise statistics, and semantic representations between visible and synthetic aperture radar (SAR) imagery, this paper proposes a multimodal remote sensing object detection method, namely YOLO-CMFM. Built upon the Ultralytics YOLOv11 framework, the proposed approach designs a Cross-Modal Fusion Module (CMFM) that systematically enhances detection accuracy and robustness from the perspectives of modality alignment, feature interaction, and adaptive fusion. Specifically, (1) a Learnable Edge-Guided Attention (LEGA) module is constructed, which leverages a learnable Gaussian saliency prior to achieve edge-oriented cross-modal alignment, effectively mitigating edge-structure mismatches across modalities; (2) a Bidirectional Cross-Attention (BCA) module is developed to enable deep semantic interaction and global contextual aggregation; (3) a Context-Guided Gating (CGG) module is designed to dynamically generate complementary weights based on multimodal source features and global contextual information, thereby achieving adaptive fusion across modalities. Extensive experiments conducted on the OGSOD 1.0 dataset demonstrate that the proposed YOLO-CMFM achieves an mAP@50 of 96.2% and an mAP@50:95 of 75.1%. While maintaining competitive performance comparable to mainstream approaches at lower IoU thresholds, the proposed method significantly outperforms existing counterparts at high IoU thresholds, highlighting its superior capability in precise object localization. Also, the experimental results on the OSPRC dataset demonstrate that the proposed method can consistently achieve stable gains under different kinds of imaging conditions, including diverse SAR polarizations, spatial resolutions, and cloud occlusion conditions. Moreover, the CMFM can be flexibly integrated into different detection frameworks, which further validates its strong generalization and transferability in multimodal remote sensing object detection tasks.&lt;/p&gt;</content:encoded></item><item><title>Text Augmentation for Vision: Modality-Preference Aware Few-Shot Learning</title><link>https://doi.org/10.1016/j.knosys.2025.115122</link><guid>10.1016/j.knosys.2025.115122</guid><pubDate>Wed, 31 Dec 2025 23:34:11 +0000</pubDate><dc:creator>Zehua Hao</dc:creator><dc:creator>Fang Liu</dc:creator><dc:creator>Shuo Li</dc:creator><dc:creator>Yaoyang Du</dc:creator><dc:creator>Jiahao Wang</dc:creator><dc:creator>Hao Wang</dc:creator><dc:creator>Xinyan Huang</dc:creator><dc:creator>Licheng Jiao</dc:creator><prism:publicationName>Knowledge-Based Systems</prism:publicationName><prism:doi>10.1016/j.knosys.2025.115122</prism:doi><description>Recent advances in vision-language models such as CLIP show great potential for few-shot learning, but their performance declines under extreme low-shot scenarios due to limited supervision and the suboptimality of enforcing a unified optimization objective across heterogeneous modalities. To address this, we propose a modality-preference aware framework with textual augmentation to enhance vision-centric few-shot image classification. By treating text descriptions as auxiliary training samples, our method enables effective and scalable augmentation without generating synthetic images. We introduce a training strategy called Alternating-Modality Supervision (AMS), where vision and text samples alternately supervise a shared classifier to mitigate gradient conflicts. Crucially, we identify a Modality-Preference Phenomenon grounded in distinct feature geometries, where high-dimensional visual features favor cross-entropy (CE) for discrimination, while semantic textual features prefer mean squared error (MSE) for manifold alignment. Based on this, we propose Modality-Preference Loss Assignment (MPLA), which aligns each modality with its preferred objective and improves optimization stability. Extensive experiments on diverse datasets and backbone architectures confirm that combining MPLA with AMS improves few-shot performance and demonstrates strong generalizability.
Published: 2025-12-31T23:34:11+00:00
Venue: Knowledge-Based Systems
Score: 0.792 (must_read)</description><content:encoded>&lt;p&gt;&lt;strong&gt;Authors:&lt;/strong&gt; Zehua Hao; Fang Liu; Shuo Li; Yaoyang Du; Jiahao Wang; Hao Wang; Xinyan Huang; Licheng Jiao&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Journal:&lt;/strong&gt; Knowledge-Based Systems&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;a href="https://doi.org/10.1016/j.knosys.2025.115122"&gt;10.1016/j.knosys.2025.115122&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Relevance:&lt;/strong&gt; 0.792 (must_read)&lt;/p&gt;
&lt;p&gt;Recent advances in vision-language models such as CLIP show great potential for few-shot learning, but their performance declines under extreme low-shot scenarios due to limited supervision and the suboptimality of enforcing a unified optimization objective across heterogeneous modalities. To address this, we propose a modality-preference aware framework with textual augmentation to enhance vision-centric few-shot image classification. By treating text descriptions as auxiliary training samples, our method enables effective and scalable augmentation without generating synthetic images. We introduce a training strategy called Alternating-Modality Supervision (AMS), where vision and text samples alternately supervise a shared classifier to mitigate gradient conflicts. Crucially, we identify a Modality-Preference Phenomenon grounded in distinct feature geometries, where high-dimensional visual features favor cross-entropy (CE) for discrimination, while semantic textual features prefer mean squared error (MSE) for manifold alignment. Based on this, we propose Modality-Preference Loss Assignment (MPLA), which aligns each modality with its preferred objective and improves optimization stability. Extensive experiments on diverse datasets and backbone architectures confirm that combining MPLA with AMS improves few-shot performance and demonstrates strong generalizability.&lt;/p&gt;</content:encoded></item><item><title>Current-diffusion model for metasurface structure discoveries with spatial-frequency dynamics</title><link>https://doi.org/10.1038/s42256-025-01162-z</link><guid>10.1038/s42256-025-01162-z</guid><pubDate>Wed, 31 Dec 2025 16:02:03 +0000</pubDate><dc:creator>Erji Li</dc:creator><dc:creator>Yusong Wang</dc:creator><dc:creator>Lei Jin</dc:creator><dc:creator>Zheng Zong</dc:creator><dc:creator>Enze Zhu</dc:creator><dc:creator>Bao Wang</dc:creator><dc:creator>Qian Wang</dc:creator><dc:creator>Zongyin Yang</dc:creator><dc:creator>Wen-Yan Yin</dc:creator><dc:creator>Zhun Wei</dc:creator><prism:publicationName>Nature Machine Intelligence</prism:publicationName><prism:doi>10.1038/s42256-025-01162-z</prism:doi><description>In AI-driven metamaterials discovery, designing metasurfaces requires extrapolation to unexplored performance regimes to discover new structures. Here we introduce MetaAI, a physics-aware current-diffusion framework that synergizes spatial topologies and frequency-domain responses to discover non-intuitive metasurface architectures. Unlike conventional inverse design constrained by predefined specifications, MetaAI operates as a performance synthesizer by generating electrical current distributions that bridge electromagnetic performance and metasurface structures. This enables both in-distribution and out-of-distribution targets with diverse topologies. The core innovation of the proposed framework lies in its dual-domain diffusion module, which directly correlates meta-atom current mechanisms with electromagnetic behaviours to enable the discovery of structures with 17.2% wider operational bandwidths. We validate MetaAI across single-layer, multilayer and dynamically tunable metasurfaces, demonstrating out-of-distribution generalization across full-wave simulations and experimental prototypes. Metasurface design driven by AI faces challenges, such as extrapolation to unexplored performance regimes. MetaAI, a physics-aware current-diffusion framework, is introduced to advance metamaterial discovery from interpolation to extrapolation.
Published: 2025-12-31T16:02:03+00:00
Venue: Nature Machine Intelligence
Score: 0.790 (must_read)</description><content:encoded>&lt;p&gt;&lt;strong&gt;Authors:&lt;/strong&gt; Erji Li; Yusong Wang; Lei Jin; Zheng Zong; Enze Zhu; Bao Wang; Qian Wang; Zongyin Yang; Wen-Yan Yin; Zhun Wei&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Journal:&lt;/strong&gt; Nature Machine Intelligence&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;a href="https://doi.org/10.1038/s42256-025-01162-z"&gt;10.1038/s42256-025-01162-z&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Relevance:&lt;/strong&gt; 0.790 (must_read)&lt;/p&gt;
&lt;p&gt;In AI-driven metamaterials discovery, designing metasurfaces requires extrapolation to unexplored performance regimes to discover new structures. Here we introduce MetaAI, a physics-aware current-diffusion framework that synergizes spatial topologies and frequency-domain responses to discover non-intuitive metasurface architectures. Unlike conventional inverse design constrained by predefined specifications, MetaAI operates as a performance synthesizer by generating electrical current distributions that bridge electromagnetic performance and metasurface structures. This enables both in-distribution and out-of-distribution targets with diverse topologies. The core innovation of the proposed framework lies in its dual-domain diffusion module, which directly correlates meta-atom current mechanisms with electromagnetic behaviours to enable the discovery of structures with 17.2% wider operational bandwidths. We validate MetaAI across single-layer, multilayer and dynamically tunable metasurfaces, demonstrating out-of-distribution generalization across full-wave simulations and experimental prototypes. Metasurface design driven by AI faces challenges, such as extrapolation to unexplored performance regimes. MetaAI, a physics-aware current-diffusion framework, is introduced to advance metamaterial discovery from interpolation to extrapolation.&lt;/p&gt;</content:encoded></item><item><title>OFL-SAM2: Prompt SAM2 with Online Few-shot Learner for Efficient Medical Image Segmentation</title><link>https://arxiv.org/abs/2512.24861v1</link><guid>http://arxiv.org/abs/2512.24861v1</guid><pubDate>Wed, 31 Dec 2025 13:41:16 +0000</pubDate><dc:creator>Meng Lan</dc:creator><dc:creator>Lefei Zhang</dc:creator><dc:creator>Xiaomeng Li</dc:creator><dc:type>preprint</dc:type><prism:publicationName>arXiv</prism:publicationName><description>The Segment Anything Model 2 (SAM2) has demonstrated remarkable promptable visual segmentation capabilities in video data, showing potential for extension to medical image segmentation (MIS) tasks involving 3D volumes and temporally correlated 2D image sequences. However, adapting SAM2 to MIS presents several challenges, including the need for extensive annotated medical data for fine-tuning and high-quality manual prompts, which are both labor-intensive and require intervention from medical experts. To address these challenges, we introduce OFL-SAM2, a prompt-free SAM2 framework for label-efficient MIS. Our core idea is to leverage limited annotated samples to train a lightweight mapping network that captures medical knowledge and transforms generic image features into target features, thereby providing additional discriminative target representations for each frame and eliminating the need for manual prompts. Crucially, the mapping network supports online parameter update during inference, enhancing the model's generalization across test sequences. Technically, we introduce two key components: (1) an online few-shot learner that trains the mapping network to generate target features using limited data, and (2) an adaptive fusion module that dynamically integrates the target features with the memory-attention features generated by frozen SAM2, leading to accurate and robust target representation. Extensive experiments on three diverse MIS datasets demonstrate that OFL-SAM2 achieves state-of-the-art performance with limited training data.
Published: 2025-12-31T13:41:16+00:00
Venue: arXiv
Score: 0.789 (must_read)</description><content:encoded>&lt;p&gt;&lt;strong&gt;Authors:&lt;/strong&gt; Meng Lan; Lefei Zhang; Xiaomeng Li&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Journal:&lt;/strong&gt; arXiv&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Relevance:&lt;/strong&gt; 0.789 (must_read)&lt;/p&gt;
&lt;p&gt;The Segment Anything Model 2 (SAM2) has demonstrated remarkable promptable visual segmentation capabilities in video data, showing potential for extension to medical image segmentation (MIS) tasks involving 3D volumes and temporally correlated 2D image sequences. However, adapting SAM2 to MIS presents several challenges, including the need for extensive annotated medical data for fine-tuning and high-quality manual prompts, which are both labor-intensive and require intervention from medical experts. To address these challenges, we introduce OFL-SAM2, a prompt-free SAM2 framework for label-efficient MIS. Our core idea is to leverage limited annotated samples to train a lightweight mapping network that captures medical knowledge and transforms generic image features into target features, thereby providing additional discriminative target representations for each frame and eliminating the need for manual prompts. Crucially, the mapping network supports online parameter update during inference, enhancing the model&amp;#x27;s generalization across test sequences. Technically, we introduce two key components: (1) an online few-shot learner that trains the mapping network to generate target features using limited data, and (2) an adaptive fusion module that dynamically integrates the target features with the memory-attention features generated by frozen SAM2, leading to accurate and robust target representation. Extensive experiments on three diverse MIS datasets demonstrate that OFL-SAM2 achieves state-of-the-art performance with limited training data.&lt;/p&gt;</content:encoded></item><item><title>Multi-source Heterogeneous Domain Adaptation with Dual-adversarial Feature Alignment</title><link>https://doi.org/10.1016/j.eswa.2025.131060</link><guid>10.1016/j.eswa.2025.131060</guid><pubDate>Wed, 31 Dec 2025 08:06:01 +0000</pubDate><dc:creator>Yun Zhang</dc:creator><dc:creator>Lei Song</dc:creator><dc:creator>Haitian Sun</dc:creator><prism:publicationName>Expert Systems with Applications</prism:publicationName><prism:doi>10.1016/j.eswa.2025.131060</prism:doi><description>Heterogeneous domain adaptation (HeDA) aims to leverage knowledge from source data to learn a robust classifier for a heterogeneous target domain, where only a few labeled target samples are available. Real-world applications frequently involve scenarios where the source data are drawn from multiple heterogeneous source domains, termed multi-source heterogeneous domain adaptation (MHeDA). Many existing studies on MHeDA focus on minimizing the distribution divergence between each pair of source and target domains to extract domain-invariant feature representations. However, the discrepancy between labeled and unlabeled target data caused by selection bias has been overlooked, leading to unsatisfactory transfer performance. Furthermore, the discriminability of target representations is not fully strengthened, which limits the generalization ability of the trained model. In this paper, we propose a dual-adversarial feature alignment (DAFA) framework for MHeDA that performs both domain-level and category-level adversarial learning to address these challenges. Specifically, DAFA aligns the domain-level distributions of target and multiple source domains through adversarial learning. The category-level distribution adaptation is achieved through alternately minimizing and maximizing the prediction uncertainty of target domain. Compared with previous works, DAFA not only minimizes the distribution divergence between the target and multiple source domains but also reduces intra-domain discrepancy within the target domain. Experiments on various text-to-text, image-to-image, and image-to-text heterogeneous transfer tasks demonstrate that the proposed DAFA significantly outperforms state-of-the-art MHeDA methods.
Published: 2025-12-31T08:06:01+00:00
Venue: Expert Systems with Applications
Score: 0.788 (must_read)</description><content:encoded>&lt;p&gt;&lt;strong&gt;Authors:&lt;/strong&gt; Yun Zhang; Lei Song; Haitian Sun&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Journal:&lt;/strong&gt; Expert Systems with Applications&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;a href="https://doi.org/10.1016/j.eswa.2025.131060"&gt;10.1016/j.eswa.2025.131060&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Relevance:&lt;/strong&gt; 0.788 (must_read)&lt;/p&gt;
&lt;p&gt;Heterogeneous domain adaptation (HeDA) aims to leverage knowledge from source data to learn a robust classifier for a heterogeneous target domain, where only a few labeled target samples are available. Real-world applications frequently involve scenarios where the source data are drawn from multiple heterogeneous source domains, termed multi-source heterogeneous domain adaptation (MHeDA). Many existing studies on MHeDA focus on minimizing the distribution divergence between each pair of source and target domains to extract domain-invariant feature representations. However, the discrepancy between labeled and unlabeled target data caused by selection bias has been overlooked, leading to unsatisfactory transfer performance. Furthermore, the discriminability of target representations is not fully strengthened, which limits the generalization ability of the trained model. In this paper, we propose a dual-adversarial feature alignment (DAFA) framework for MHeDA that performs both domain-level and category-level adversarial learning to address these challenges. Specifically, DAFA aligns the domain-level distributions of target and multiple source domains through adversarial learning. The category-level distribution adaptation is achieved through alternately minimizing and maximizing the prediction uncertainty of target domain. Compared with previous works, DAFA not only minimizes the distribution divergence between the target and multiple source domains but also reduces intra-domain discrepancy within the target domain. Experiments on various text-to-text, image-to-image, and image-to-text heterogeneous transfer tasks demonstrate that the proposed DAFA significantly outperforms state-of-the-art MHeDA methods.&lt;/p&gt;</content:encoded></item><item><title>Bridging the Sim-to-Real Gap in RF Localization with Large-Scale Synthetic Pretraining</title><link>https://doi.org/10.1016/j.inffus.2025.104104</link><guid>10.1016/j.inffus.2025.104104</guid><pubDate>Wed, 31 Dec 2025 00:19:29 +0000</pubDate><dc:creator>Armen Manukyan</dc:creator><dc:creator>Rafayel Mkrtchyan</dc:creator><dc:creator>Ararat Saribekyan</dc:creator><dc:creator>Theofanis P. Raptis</dc:creator><dc:creator>Hrant Khachatrian</dc:creator><prism:publicationName>Information Fusion</prism:publicationName><prism:doi>10.1016/j.inffus.2025.104104</prism:doi><description>Radio frequency (RF) fingerprinting is a promising localization technique for GPS-denied environments, yet it tends to suffer from a fundamental limitation: Poor generalization to previously unmapped areas. Traditional methods such as k -nearest neighbors ( k -NN) perform well where data is available but may fail on unseen streets, limiting real-world deployment. Deep learning (DL) offers potential remedies by learning spatial-RF patterns that generalize, but requires far more training data than what simple real-world measurement campaigns can provide. In this paper, we investigate whether synthetic data can bridge this generalization gap. Using (i) a real-world dataset from Rome and (ii) NVIDIA’s open-source ray-tracing simulator Sionna, we generate synthetic datasets under varying realism and scale conditions. Specifically, we use Dataset A containing real-world measurements with real base stations (BS) and real signals, and create Dataset B using real BS locations but simulated signals, Dataset C with both simulated BS locations and signals, and Dataset B’ which represents an optimized version of Dataset B where BS parameters are calibrated via Gaussian Process to maximize signal correlation with Dataset A. Our evaluation reveals a pronounced sim-to-real gap: Models achieving 25m error on synthetic data degrade to 184m on real data. Nonetheless, pretraining on synthetic data reduces real-world localization error from 323m to 162m; a 50% improvement over real-only training. Notably, simulation fidelity proves more important than scale: A smaller calibrated dataset (53K samples) outperforms a larger uncalibrated one (274K samples). To further evaluate the generalization capabilities of the models, we conduct experiments on an unseen geographical region using a real-world dataset from Oslo. In the zero-shot setting, the models achieve a root mean square error (RMSE) of 132.2m on the entire dataset, and 61.5m on unseen streets after fine-tuning on Oslo data. While challenges remain before meeting more practical localization accuracy, this work provides a systematic study in the field of wireless communication of synthetic-to-real transfer in RF localization and highlights the value of simulation-aware pretraining for generalizing DL models to real-world scenarios.
Published: 2025-12-31T00:19:29+00:00
Venue: Information Fusion
Score: 0.787 (must_read)</description><content:encoded>&lt;p&gt;&lt;strong&gt;Authors:&lt;/strong&gt; Armen Manukyan; Rafayel Mkrtchyan; Ararat Saribekyan; Theofanis P. Raptis; Hrant Khachatrian&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Journal:&lt;/strong&gt; Information Fusion&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;a href="https://doi.org/10.1016/j.inffus.2025.104104"&gt;10.1016/j.inffus.2025.104104&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Relevance:&lt;/strong&gt; 0.787 (must_read)&lt;/p&gt;
&lt;p&gt;Radio frequency (RF) fingerprinting is a promising localization technique for GPS-denied environments, yet it tends to suffer from a fundamental limitation: Poor generalization to previously unmapped areas. Traditional methods such as k -nearest neighbors ( k -NN) perform well where data is available but may fail on unseen streets, limiting real-world deployment. Deep learning (DL) offers potential remedies by learning spatial-RF patterns that generalize, but requires far more training data than what simple real-world measurement campaigns can provide. In this paper, we investigate whether synthetic data can bridge this generalization gap. Using (i) a real-world dataset from Rome and (ii) NVIDIA’s open-source ray-tracing simulator Sionna, we generate synthetic datasets under varying realism and scale conditions. Specifically, we use Dataset A containing real-world measurements with real base stations (BS) and real signals, and create Dataset B using real BS locations but simulated signals, Dataset C with both simulated BS locations and signals, and Dataset B’ which represents an optimized version of Dataset B where BS parameters are calibrated via Gaussian Process to maximize signal correlation with Dataset A. Our evaluation reveals a pronounced sim-to-real gap: Models achieving 25m error on synthetic data degrade to 184m on real data. Nonetheless, pretraining on synthetic data reduces real-world localization error from 323m to 162m; a 50% improvement over real-only training. Notably, simulation fidelity proves more important than scale: A smaller calibrated dataset (53K samples) outperforms a larger uncalibrated one (274K samples). To further evaluate the generalization capabilities of the models, we conduct experiments on an unseen geographical region using a real-world dataset from Oslo. In the zero-shot setting, the models achieve a root mean square error (RMSE) of 132.2m on the entire dataset, and 61.5m on unseen streets after fine-tuning on Oslo data. While challenges remain before meeting more practical localization accuracy, this work provides a systematic study in the field of wireless communication of synthetic-to-real transfer in RF localization and highlights the value of simulation-aware pretraining for generalizing DL models to real-world scenarios.&lt;/p&gt;</content:encoded></item><item><title>Infrared and Visible Image Fusion via Iterative Feature Decomposition and Deep Balanced Fusion</title><link>https://doi.org/10.1016/j.patcog.2025.113022</link><guid>10.1016/j.patcog.2025.113022</guid><pubDate>Wed, 31 Dec 2025 07:55:41 +0000</pubDate><dc:creator>Wei Li</dc:creator><dc:creator>Baojia Li</dc:creator><dc:creator>Haiyu Song</dc:creator><dc:creator>Pengjie Wang</dc:creator><dc:creator>Zeyu Wang</dc:creator><prism:publicationName>Pattern Recognition</prism:publicationName><prism:doi>10.1016/j.patcog.2025.113022</prism:doi><description>Infrared and visible image fusion (IVF) combines thermal cues from infrared images with structural details from visible images, improving perception in complex conditions. Current fusion methods typically follow a decomposition-fusion-reconstruction paradigm, but face two major limitations. First, one-shot decomposition strategy inherently assumes equal complexity and importance of information across modalities, ignoring structural differences. Second, redundant or conflicting information is typically unhandled in high-frequency fusion, resulting in structural degradation and detail loss. To address these issues, we propose a novel IVF framework based on the Adaptive Iterative Feature Decomposition module (AIFD) and the Deep-Balanced High-Frequency Fusion module (DBHF). AIFD dynamically adjusts the number of decomposition iterations for infrared and visible images independently, guided by global semantic similarity and channel-wise cosine similarity. DBHF recursively integrates high-frequency features and uses a discriminative network as a learnable convergence criterion, effectively suppressing redundant or conflicting information. Extensive experiments on public benchmarks demonstrate that our method achieves state-of-the-art (SOTA) performance, with improvements of up to 12.3% in SF, 8.9% in AG on RoadScene, and 1.7% in VIF on M 3 FD.
Published: 2025-12-31T07:55:41+00:00
Venue: Pattern Recognition
Score: 0.786 (must_read)</description><content:encoded>&lt;p&gt;&lt;strong&gt;Authors:&lt;/strong&gt; Wei Li; Baojia Li; Haiyu Song; Pengjie Wang; Zeyu Wang&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Journal:&lt;/strong&gt; Pattern Recognition&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;a href="https://doi.org/10.1016/j.patcog.2025.113022"&gt;10.1016/j.patcog.2025.113022&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Relevance:&lt;/strong&gt; 0.786 (must_read)&lt;/p&gt;
&lt;p&gt;Infrared and visible image fusion (IVF) combines thermal cues from infrared images with structural details from visible images, improving perception in complex conditions. Current fusion methods typically follow a decomposition-fusion-reconstruction paradigm, but face two major limitations. First, one-shot decomposition strategy inherently assumes equal complexity and importance of information across modalities, ignoring structural differences. Second, redundant or conflicting information is typically unhandled in high-frequency fusion, resulting in structural degradation and detail loss. To address these issues, we propose a novel IVF framework based on the Adaptive Iterative Feature Decomposition module (AIFD) and the Deep-Balanced High-Frequency Fusion module (DBHF). AIFD dynamically adjusts the number of decomposition iterations for infrared and visible images independently, guided by global semantic similarity and channel-wise cosine similarity. DBHF recursively integrates high-frequency features and uses a discriminative network as a learnable convergence criterion, effectively suppressing redundant or conflicting information. Extensive experiments on public benchmarks demonstrate that our method achieves state-of-the-art (SOTA) performance, with improvements of up to 12.3% in SF, 8.9% in AG on RoadScene, and 1.7% in VIF on M 3 FD.&lt;/p&gt;</content:encoded></item><item><title>Adaptive Adversarial Cross-Domain Segmentation Network for High-Resolution Remote Sensing Images</title><link>https://doi.org/10.1109/jstars.2025.3650193</link><guid>10.1109/jstars.2025.3650193</guid><pubDate>Thu, 01 Jan 2026 18:37:35 +0000</pubDate><dc:creator>Jianfen Wei</dc:creator><dc:creator>Ping Yang</dc:creator><dc:creator>Chang Wang</dc:creator><dc:creator>Chunxiang Shi</dc:creator><dc:creator>Renlong Hang</dc:creator><prism:publicationName>IEEE Journal of Selected Topics in Applied Earth Observations and Remote Sensing</prism:publicationName><prism:doi>10.1109/jstars.2025.3650193</prism:doi><description>Convolutional neural networks (CNNs) have dominated the field of semantic segmentation of high-resolution remote sensing images (HRSIs). These models often assume that training and test data follow the same distribution. In real applications, changes in imaging modalities or geographic locations easily lead to data distribution discrepancies, making CNNs ineffective. Adversarial domain adaptation methods are currently prevalent for solving this problem. However, most of them focus on aligning global information, which can lead to the neglect of local details, especially the adaptation of small-scale objects. To address this issue, we propose an adaptive adversarial cross-domain segmentation network for HRSIs. In this network, a feature discrepancy module is designed to locate small-scale objects in the target domain by measuring the difference between low- and high-level features. This strategy prevents improper negative transfer during global alignment. Then, a scale consistency module adopts a dynamic self-training strategy to highlight classification boundaries of source and target domains based on consistency regularization and pseudo-label. Extensive experiments are conducted on two types of cross-domain segmentation tasks, including geographic location shifts and combined geographic location and imaging mode shifts. Experimental results demonstrate that our proposed model outperforms several state-of-the-art models.
Published: 2026-01-01T18:37:35+00:00
Venue: IEEE Journal of Selected Topics in Applied Earth Observations and Remote Sensing
Score: 0.786 (must_read)</description><content:encoded>&lt;p&gt;&lt;strong&gt;Authors:&lt;/strong&gt; Jianfen Wei; Ping Yang; Chang Wang; Chunxiang Shi; Renlong Hang&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Journal:&lt;/strong&gt; IEEE Journal of Selected Topics in Applied Earth Observations and Remote Sensing&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;a href="https://doi.org/10.1109/jstars.2025.3650193"&gt;10.1109/jstars.2025.3650193&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Relevance:&lt;/strong&gt; 0.786 (must_read)&lt;/p&gt;
&lt;p&gt;Convolutional neural networks (CNNs) have dominated the field of semantic segmentation of high-resolution remote sensing images (HRSIs). These models often assume that training and test data follow the same distribution. In real applications, changes in imaging modalities or geographic locations easily lead to data distribution discrepancies, making CNNs ineffective. Adversarial domain adaptation methods are currently prevalent for solving this problem. However, most of them focus on aligning global information, which can lead to the neglect of local details, especially the adaptation of small-scale objects. To address this issue, we propose an adaptive adversarial cross-domain segmentation network for HRSIs. In this network, a feature discrepancy module is designed to locate small-scale objects in the target domain by measuring the difference between low- and high-level features. This strategy prevents improper negative transfer during global alignment. Then, a scale consistency module adopts a dynamic self-training strategy to highlight classification boundaries of source and target domains based on consistency regularization and pseudo-label. Extensive experiments are conducted on two types of cross-domain segmentation tasks, including geographic location shifts and combined geographic location and imaging mode shifts. Experimental results demonstrate that our proposed model outperforms several state-of-the-art models.&lt;/p&gt;</content:encoded></item><item><title>HyPyraMamba: A Pyramid Spectral Attention and Mamba-Based Architecture for Robust Hyperspectral Image Classification</title><link>https://doi.org/10.1109/tgrs.2025.3650350</link><guid>10.1109/tgrs.2025.3650350</guid><pubDate>Thu, 01 Jan 2026 18:37:16 +0000</pubDate><dc:creator>Dekai Li</dc:creator><dc:creator>Uzair Aslam Bhatti</dc:creator><dc:creator>Mengxing Huang</dc:creator><dc:creator>Lorenzo Bruzzone</dc:creator><dc:creator>Jiaxin Li</dc:creator><prism:publicationName>IEEE Transactions on Geoscience and Remote Sensing</prism:publicationName><prism:doi>10.1109/tgrs.2025.3650350</prism:doi><description>In hyperspectral image (HSI) classification, the high-dimensionality and the complex coupling of spatial-spectral features present severe challenges to existing deep learning methods in terms of accuracy, generalization, and computational efficiency. Researchers have recently explored CNN and Transformer-based methods to overcome these limitations, but CNN's limited receptive field prevents effective modeling of long-range dependencies, while Transformers suffer from high computational cost and inefficiency in high-dimensional data. Motivated by these limitations, the state-space model (SSM) Mamba shows great potential as an efficient alternative for sequence and dependency modeling. Building on this foundation, we propose HyPyraMamba, a novel architecture designed to effectively overcome the above challenges. It integrates the Pyramid Spectral Attention (PSA) module to capture multi-scale key spectral features, thereby reducing interference caused by spectral redundancy. We developed an Adaptive Expert Depthwise Convolution (AEDC) module that enhances the model's ability to express multi-scale spatial-spectral features, and a sequence modeling module, Mamba. In the Mamba module, we utilize the spatial Mamba and spectral Mamba branches to enhance spatial structure and spectral correlation modeling. Extensive experiments on four benchmark HSI datasets demonstrate that HyPyraMamba significantly outperforms several recent state-of-the-art methods and provides a favorable accuracy–efficiency trade-off. In particular, class-wise analyses on spectrally similar land-cover categories (e.g., different soybean and bareland types) show that HyPyraMamba markedly reduces mutual confusion compared with CNN-, Transformer-, and Mamba-based baselines. The code will be available at https://github.com/dekai-li/HyPyraMamba.
Published: 2026-01-01T18:37:16+00:00
Venue: IEEE Transactions on Geoscience and Remote Sensing
Score: 0.785 (must_read)</description><content:encoded>&lt;p&gt;&lt;strong&gt;Authors:&lt;/strong&gt; Dekai Li; Uzair Aslam Bhatti; Mengxing Huang; Lorenzo Bruzzone; Jiaxin Li&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Journal:&lt;/strong&gt; IEEE Transactions on Geoscience and Remote Sensing&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;a href="https://doi.org/10.1109/tgrs.2025.3650350"&gt;10.1109/tgrs.2025.3650350&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Relevance:&lt;/strong&gt; 0.785 (must_read)&lt;/p&gt;
&lt;p&gt;In hyperspectral image (HSI) classification, the high-dimensionality and the complex coupling of spatial-spectral features present severe challenges to existing deep learning methods in terms of accuracy, generalization, and computational efficiency. Researchers have recently explored CNN and Transformer-based methods to overcome these limitations, but CNN&amp;#x27;s limited receptive field prevents effective modeling of long-range dependencies, while Transformers suffer from high computational cost and inefficiency in high-dimensional data. Motivated by these limitations, the state-space model (SSM) Mamba shows great potential as an efficient alternative for sequence and dependency modeling. Building on this foundation, we propose HyPyraMamba, a novel architecture designed to effectively overcome the above challenges. It integrates the Pyramid Spectral Attention (PSA) module to capture multi-scale key spectral features, thereby reducing interference caused by spectral redundancy. We developed an Adaptive Expert Depthwise Convolution (AEDC) module that enhances the model&amp;#x27;s ability to express multi-scale spatial-spectral features, and a sequence modeling module, Mamba. In the Mamba module, we utilize the spatial Mamba and spectral Mamba branches to enhance spatial structure and spectral correlation modeling. Extensive experiments on four benchmark HSI datasets demonstrate that HyPyraMamba significantly outperforms several recent state-of-the-art methods and provides a favorable accuracy–efficiency trade-off. In particular, class-wise analyses on spectrally similar land-cover categories (e.g., different soybean and bareland types) show that HyPyraMamba markedly reduces mutual confusion compared with CNN-, Transformer-, and Mamba-based baselines. The code will be available at https://github.com/dekai-li/HyPyraMamba.&lt;/p&gt;</content:encoded></item><item><title>Forging Spatial Intelligence: A Roadmap of Multi-Modal Data Pre-Training for Autonomous Systems</title><link>https://arxiv.org/abs/2512.24385v1</link><guid>http://arxiv.org/abs/2512.24385v1</guid><pubDate>Tue, 30 Dec 2025 17:58:01 +0000</pubDate><dc:creator>Song Wang</dc:creator><dc:creator>Lingdong Kong</dc:creator><dc:creator>Xiaolu Liu</dc:creator><dc:creator>Hao Shi</dc:creator><dc:creator>Wentong Li</dc:creator><dc:creator>Jianke Zhu</dc:creator><dc:creator>Steven C. H. Hoi</dc:creator><dc:type>preprint</dc:type><prism:publicationName>arXiv</prism:publicationName><description>The rapid advancement of autonomous systems, including self-driving vehicles and drones, has intensified the need to forge true Spatial Intelligence from multi-modal onboard sensor data. While foundation models excel in single-modal contexts, integrating their capabilities across diverse sensors like cameras and LiDAR to create a unified understanding remains a formidable challenge. This paper presents a comprehensive framework for multi-modal pre-training, identifying the core set of techniques driving progress toward this goal. We dissect the interplay between foundational sensor characteristics and learning strategies, evaluating the role of platform-specific datasets in enabling these advancements. Our central contribution is the formulation of a unified taxonomy for pre-training paradigms: ranging from single-modality baselines to sophisticated unified frameworks that learn holistic representations for advanced tasks like 3D object detection and semantic occupancy prediction. Furthermore, we investigate the integration of textual inputs and occupancy representations to facilitate open-world perception and planning. Finally, we identify critical bottlenecks, such as computational efficiency and model scalability, and propose a roadmap toward general-purpose multi-modal foundation models capable of achieving robust Spatial Intelligence for real-world deployment.
Published: 2025-12-30T17:58:01+00:00
Venue: arXiv
Score: 0.785 (must_read)</description><content:encoded>&lt;p&gt;&lt;strong&gt;Authors:&lt;/strong&gt; Song Wang; Lingdong Kong; Xiaolu Liu; Hao Shi; Wentong Li; Jianke Zhu; Steven C. H. Hoi&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Journal:&lt;/strong&gt; arXiv&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Relevance:&lt;/strong&gt; 0.785 (must_read)&lt;/p&gt;
&lt;p&gt;The rapid advancement of autonomous systems, including self-driving vehicles and drones, has intensified the need to forge true Spatial Intelligence from multi-modal onboard sensor data. While foundation models excel in single-modal contexts, integrating their capabilities across diverse sensors like cameras and LiDAR to create a unified understanding remains a formidable challenge. This paper presents a comprehensive framework for multi-modal pre-training, identifying the core set of techniques driving progress toward this goal. We dissect the interplay between foundational sensor characteristics and learning strategies, evaluating the role of platform-specific datasets in enabling these advancements. Our central contribution is the formulation of a unified taxonomy for pre-training paradigms: ranging from single-modality baselines to sophisticated unified frameworks that learn holistic representations for advanced tasks like 3D object detection and semantic occupancy prediction. Furthermore, we investigate the integration of textual inputs and occupancy representations to facilitate open-world perception and planning. Finally, we identify critical bottlenecks, such as computational efficiency and model scalability, and propose a roadmap toward general-purpose multi-modal foundation models capable of achieving robust Spatial Intelligence for real-world deployment.&lt;/p&gt;</content:encoded></item><item><title>Scaling Remote Sensing Foundation Models: Data Domain Tradeoffs at the Peta-Scale</title><link>https://arxiv.org/abs/2512.23903v1</link><guid>http://arxiv.org/abs/2512.23903v1</guid><pubDate>Mon, 29 Dec 2025 23:53:11 +0000</pubDate><dc:creator>Charith Wickrema</dc:creator><dc:creator>Eliza Mace</dc:creator><dc:creator>Hunter Brown</dc:creator><dc:creator>Heidys Cabrera</dc:creator><dc:creator>Nick Krall</dc:creator><dc:creator>Matthew O'Neill</dc:creator><dc:creator>Shivangi Sarkar</dc:creator><dc:creator>Lowell Weissman</dc:creator><dc:creator>Eric Hughes</dc:creator><dc:creator>Guido Zarrella</dc:creator><dc:type>preprint</dc:type><prism:publicationName>arXiv</prism:publicationName><description>We explore the scaling behaviors of artificial intelligence to establish practical techniques for training foundation models on high-resolution electro-optical (EO) datasets that exceed the current state-of-the-art scale by orders of magnitude. Modern multimodal machine learning (ML) applications, such as generative artificial intelligence (GenAI) systems for image captioning, search, and reasoning, depend on robust, domain-specialized encoders for non-text modalities. In natural-image domains where internet-scale data is plentiful, well-established scaling laws help optimize the joint scaling of model capacity, training compute, and dataset size. Unfortunately, these relationships are much less well-understood in high-value domains like remote sensing (RS). Using over a quadrillion pixels of commercial satellite EO data and the MITRE Federal AI Sandbox, we train progressively larger vision transformer (ViT) backbones, report success and failure modes observed at petascale, and analyze implications for bridging domain gaps across additional RS modalities. We observe that even at this scale, performance is consistent with a data limited regime rather than a model parameter-limited one. These practical insights are intended to inform data-collection strategies, compute budgets, and optimization schedules that advance the future development of frontier-scale RS foundation models.
Published: 2025-12-29T23:53:11+00:00
Venue: arXiv
Score: 0.784 (must_read)</description><content:encoded>&lt;p&gt;&lt;strong&gt;Authors:&lt;/strong&gt; Charith Wickrema; Eliza Mace; Hunter Brown; Heidys Cabrera; Nick Krall; Matthew O&amp;#x27;Neill; Shivangi Sarkar; Lowell Weissman; Eric Hughes; Guido Zarrella&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Journal:&lt;/strong&gt; arXiv&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Relevance:&lt;/strong&gt; 0.784 (must_read)&lt;/p&gt;
&lt;p&gt;We explore the scaling behaviors of artificial intelligence to establish practical techniques for training foundation models on high-resolution electro-optical (EO) datasets that exceed the current state-of-the-art scale by orders of magnitude. Modern multimodal machine learning (ML) applications, such as generative artificial intelligence (GenAI) systems for image captioning, search, and reasoning, depend on robust, domain-specialized encoders for non-text modalities. In natural-image domains where internet-scale data is plentiful, well-established scaling laws help optimize the joint scaling of model capacity, training compute, and dataset size. Unfortunately, these relationships are much less well-understood in high-value domains like remote sensing (RS). Using over a quadrillion pixels of commercial satellite EO data and the MITRE Federal AI Sandbox, we train progressively larger vision transformer (ViT) backbones, report success and failure modes observed at petascale, and analyze implications for bridging domain gaps across additional RS modalities. We observe that even at this scale, performance is consistent with a data limited regime rather than a model parameter-limited one. These practical insights are intended to inform data-collection strategies, compute budgets, and optimization schedules that advance the future development of frontier-scale RS foundation models.&lt;/p&gt;</content:encoded></item><item><title>GVSynergy-Det: Synergistic Gaussian-Voxel Representations for Multi-View 3D Object Detection</title><link>https://arxiv.org/abs/2512.23176v1</link><guid>http://arxiv.org/abs/2512.23176v1</guid><pubDate>Mon, 29 Dec 2025 03:34:39 +0000</pubDate><dc:creator>Yi Zhang</dc:creator><dc:creator>Yi Wang</dc:creator><dc:creator>Lei Yao</dc:creator><dc:creator>Lap-Pui Chau</dc:creator><dc:type>preprint</dc:type><prism:publicationName>arXiv</prism:publicationName><description>Image-based 3D object detection aims to identify and localize objects in 3D space using only RGB images, eliminating the need for expensive depth sensors required by point cloud-based methods. Existing image-based approaches face two critical challenges: methods achieving high accuracy typically require dense 3D supervision, while those operating without such supervision struggle to extract accurate geometry from images alone. In this paper, we present GVSynergy-Det, a novel framework that enhances 3D detection through synergistic Gaussian-Voxel representation learning. Our key insight is that continuous Gaussian and discrete voxel representations capture complementary geometric information: Gaussians excel at modeling fine-grained surface details while voxels provide structured spatial context. We introduce a dual-representation architecture that: 1) adapts generalizable Gaussian Splatting to extract complementary geometric features for detection tasks, and 2) develops a cross-representation enhancement mechanism that enriches voxel features with geometric details from Gaussian fields. Unlike previous methods that either rely on time-consuming per-scene optimization or utilize Gaussian representations solely for depth regularization, our synergistic strategy directly leverages features from both representations through learnable integration, enabling more accurate object localization. Extensive experiments demonstrate that GVSynergy-Det achieves state-of-the-art results on challenging indoor benchmarks, significantly outperforming existing methods on both ScanNetV2 and ARKitScenes datasets, all without requiring any depth or dense 3D geometry supervision (e.g., point clouds or TSDF).
Published: 2025-12-29T03:34:39+00:00
Venue: arXiv
Score: 0.784 (must_read)</description><content:encoded>&lt;p&gt;&lt;strong&gt;Authors:&lt;/strong&gt; Yi Zhang; Yi Wang; Lei Yao; Lap-Pui Chau&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Journal:&lt;/strong&gt; arXiv&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Relevance:&lt;/strong&gt; 0.784 (must_read)&lt;/p&gt;
&lt;p&gt;Image-based 3D object detection aims to identify and localize objects in 3D space using only RGB images, eliminating the need for expensive depth sensors required by point cloud-based methods. Existing image-based approaches face two critical challenges: methods achieving high accuracy typically require dense 3D supervision, while those operating without such supervision struggle to extract accurate geometry from images alone. In this paper, we present GVSynergy-Det, a novel framework that enhances 3D detection through synergistic Gaussian-Voxel representation learning. Our key insight is that continuous Gaussian and discrete voxel representations capture complementary geometric information: Gaussians excel at modeling fine-grained surface details while voxels provide structured spatial context. We introduce a dual-representation architecture that: 1) adapts generalizable Gaussian Splatting to extract complementary geometric features for detection tasks, and 2) develops a cross-representation enhancement mechanism that enriches voxel features with geometric details from Gaussian fields. Unlike previous methods that either rely on time-consuming per-scene optimization or utilize Gaussian representations solely for depth regularization, our synergistic strategy directly leverages features from both representations through learnable integration, enabling more accurate object localization. Extensive experiments demonstrate that GVSynergy-Det achieves state-of-the-art results on challenging indoor benchmarks, significantly outperforming existing methods on both ScanNetV2 and ARKitScenes datasets, all without requiring any depth or dense 3D geometry supervision (e.g., point clouds or TSDF).&lt;/p&gt;</content:encoded></item><item><title>Planes, Not A380: How Prompting, Context, and Granularity Shape VLM Performance in Aerial Imagery</title><link>https://doi.org/10.1109/jstars.2025.3649701</link><guid>10.1109/jstars.2025.3649701</guid><pubDate>Wed, 31 Dec 2025 18:44:28 +0000</pubDate><dc:creator>Naël Ouerghemi</dc:creator><dc:creator>Ciprian Tomoiagă</dc:creator><dc:creator>Marcin Detyniecki</dc:creator><prism:publicationName>IEEE Journal of Selected Topics in Applied Earth Observations and Remote Sensing</prism:publicationName><prism:doi>10.1109/jstars.2025.3649701</prism:doi><description>Vision-Language Models (VLMs) have demonstrated impressive generalization abilities across various vision-and-language tasks, yet their effectiveness for object classification in remote sensing imagery remains uncertain. Accurate overhead object classification, combined with class-agnostic segmenters or region proposal networks (e.g., the Segment Anything Model), can power large-scale synthetic dataset pipelines (like LocateAnythingOnEarth), broadening category sets and improving automated object recognition. In this work, we benchmark several state-of-the-art VLMs, including GPT-4o and Gemini Flash 2.0, across multiple aerial imagery datasets (iSAID, SIOR, FAST). Our experiments reveal that while current VLMs excel in coarse-level object classification (e.g., distinguishing vehicles from buildings), their performance significantly deteriorates when tasked with fine-grained distinctions (e.g., differentiating specific aircraft models). We further show that appropriate visual prompting techniques and contextual information substantially influence model performance: coarse-grained prompts that preserve broader scene context generally outperform precise, fine-grained prompts. Moreover, we find that context can both enhance and hinder model accuracy, depending on its relevance and granularity. These findings highlight critical considerations for leveraging VLMs in remote sensing and provide actionable insights toward developing robust open-vocabulary classification frameworks suitable for aerial imagery.
Published: 2025-12-31T18:44:28+00:00
Venue: IEEE Journal of Selected Topics in Applied Earth Observations and Remote Sensing
Score: 0.782 (must_read)</description><content:encoded>&lt;p&gt;&lt;strong&gt;Authors:&lt;/strong&gt; Naël Ouerghemi; Ciprian Tomoiagă; Marcin Detyniecki&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Journal:&lt;/strong&gt; IEEE Journal of Selected Topics in Applied Earth Observations and Remote Sensing&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;a href="https://doi.org/10.1109/jstars.2025.3649701"&gt;10.1109/jstars.2025.3649701&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Relevance:&lt;/strong&gt; 0.782 (must_read)&lt;/p&gt;
&lt;p&gt;Vision-Language Models (VLMs) have demonstrated impressive generalization abilities across various vision-and-language tasks, yet their effectiveness for object classification in remote sensing imagery remains uncertain. Accurate overhead object classification, combined with class-agnostic segmenters or region proposal networks (e.g., the Segment Anything Model), can power large-scale synthetic dataset pipelines (like LocateAnythingOnEarth), broadening category sets and improving automated object recognition. In this work, we benchmark several state-of-the-art VLMs, including GPT-4o and Gemini Flash 2.0, across multiple aerial imagery datasets (iSAID, SIOR, FAST). Our experiments reveal that while current VLMs excel in coarse-level object classification (e.g., distinguishing vehicles from buildings), their performance significantly deteriorates when tasked with fine-grained distinctions (e.g., differentiating specific aircraft models). We further show that appropriate visual prompting techniques and contextual information substantially influence model performance: coarse-grained prompts that preserve broader scene context generally outperform precise, fine-grained prompts. Moreover, we find that context can both enhance and hinder model accuracy, depending on its relevance and granularity. These findings highlight critical considerations for leveraging VLMs in remote sensing and provide actionable insights toward developing robust open-vocabulary classification frameworks suitable for aerial imagery.&lt;/p&gt;</content:encoded></item></channel></rss>