<?xml version='1.0' encoding='utf-8'?>
<rss xmlns:content="http://purl.org/rss/1.0/modules/content/" xmlns:dc="http://purl.org/dc/elements/1.1/" xmlns:prism="http://prismstandard.org/namespaces/basic/3.0/" version="2.0"><channel><title>ZotWatch Feed</title><link>https://ehehe.cn/zotwatch/</link><description>AI-assisted literature watch</description><lastBuildDate>Mon, 15 Dec 2025 02:46:25 +0000</lastBuildDate><item><title>Homogeneous Multimodal Adaptive Cross-Attention Fusion with Confidence-Aware Keypoints Evaluation for 6DoF Pose Estimation</title><link>https://doi.org/10.1016/j.inffus.2025.104059</link><guid>10.1016/j.inffus.2025.104059</guid><pubDate>Sat, 13 Dec 2025 23:37:01 +0000</pubDate><dc:creator>Yi Guo</dc:creator><dc:creator>Fei Wang</dc:creator><dc:creator>Hao Chu</dc:creator><dc:creator>Jindong Yu</dc:creator><dc:creator>Shuai Han</dc:creator><prism:publicationName>Information Fusion</prism:publicationName><prism:doi>10.1016/j.inffus.2025.104059</prism:doi><description>6D pose estimation from RGB-D data constitutes a pivotal research area in computer vision, where the primary challenge resides in effectively integrating RGB and depth modalities. We propose an innovative homogeneous multimodal cross-attention fusion framework for object 6D pose through directly processing raw RGB-D data for feature extraction rather than traditional point cloud-based two-branch architectures. We employ the global-local embeddings and adaptive cross-attention fusion to exploit the inherent similarity of homogeneous multimodal information. Furthermore, we design a confidence-aware keypoint evaluation module to enhance localization accuracy and robustness. Comparative analysis experiments on three popular benchmark datasets, complemented by systematic ablation analyses, demonstrate the efficacy of our method in achieving superior performance on Occlusion-LineMOD (79.6%), YCB-Video (97.2%), and MP6D (93.60%). Finally, we verify the applicability of our method in difficult conditions.
Published: 2025-12-13T23:37:01+00:00
Venue: Information Fusion
Score: 0.808 (must_read)</description><content:encoded>&lt;p&gt;&lt;strong&gt;Authors:&lt;/strong&gt; Yi Guo; Fei Wang; Hao Chu; Jindong Yu; Shuai Han&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Journal:&lt;/strong&gt; Information Fusion&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;a href="https://doi.org/10.1016/j.inffus.2025.104059"&gt;10.1016/j.inffus.2025.104059&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Relevance:&lt;/strong&gt; 0.808 (must_read)&lt;/p&gt;
&lt;p&gt;6D pose estimation from RGB-D data constitutes a pivotal research area in computer vision, where the primary challenge resides in effectively integrating RGB and depth modalities. We propose an innovative homogeneous multimodal cross-attention fusion framework for object 6D pose through directly processing raw RGB-D data for feature extraction rather than traditional point cloud-based two-branch architectures. We employ the global-local embeddings and adaptive cross-attention fusion to exploit the inherent similarity of homogeneous multimodal information. Furthermore, we design a confidence-aware keypoint evaluation module to enhance localization accuracy and robustness. Comparative analysis experiments on three popular benchmark datasets, complemented by systematic ablation analyses, demonstrate the efficacy of our method in achieving superior performance on Occlusion-LineMOD (79.6%), YCB-Video (97.2%), and MP6D (93.60%). Finally, we verify the applicability of our method in difficult conditions.&lt;/p&gt;</content:encoded></item><item><title>Controllable Image-Guided Generation via Dynamic Gaussian Spectral Modulation</title><link>https://doi.org/10.1016/j.eswa.2025.130809</link><guid>10.1016/j.eswa.2025.130809</guid><pubDate>Sat, 13 Dec 2025 23:34:55 +0000</pubDate><dc:creator>Shuocheng Wang</dc:creator><dc:creator>Qingfeng Wu</dc:creator><dc:creator>yuanbo Xing</dc:creator><dc:creator>mengyuan Ge</dc:creator><prism:publicationName>Expert Systems with Applications</prism:publicationName><prism:doi>10.1016/j.eswa.2025.130809</prism:doi><description>Diffusion models have achieved impressive results in image generation, but existing approaches often struggle with fine-grained control over the synthesis process, limiting their adaptability across different tasks. To address this issue, we introduce a novel diffusion framework that integrates adaptive Gaussian filtering into the denoising process, allowing dynamic modulation of structural and textural information. Furthermore, we design a Bidirectional Optimization Framework, which consists of two progressive phases: (1) Noise-to-Structure Optimization, ensuring global structural consistency through controlled spectral modulation, and (2) Structure-to-Texture Optimization, enhancing fine-grained details via gradient-based refinement. The proposed approach operates without additional training, supporting various image translation tasks, including cross-domain transformations and image to image translation. Extensive experiments on multiple datasets, including FFHQ and AFHQ, demonstrate that the proposed method achieves significant improvements over existing approaches, delivering superior generative quality and broader applicability in real-world scenarios.
Published: 2025-12-13T23:34:55+00:00
Venue: Expert Systems with Applications
Score: 0.787 (must_read)</description><content:encoded>&lt;p&gt;&lt;strong&gt;Authors:&lt;/strong&gt; Shuocheng Wang; Qingfeng Wu; yuanbo Xing; mengyuan Ge&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Journal:&lt;/strong&gt; Expert Systems with Applications&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;a href="https://doi.org/10.1016/j.eswa.2025.130809"&gt;10.1016/j.eswa.2025.130809&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Relevance:&lt;/strong&gt; 0.787 (must_read)&lt;/p&gt;
&lt;p&gt;Diffusion models have achieved impressive results in image generation, but existing approaches often struggle with fine-grained control over the synthesis process, limiting their adaptability across different tasks. To address this issue, we introduce a novel diffusion framework that integrates adaptive Gaussian filtering into the denoising process, allowing dynamic modulation of structural and textural information. Furthermore, we design a Bidirectional Optimization Framework, which consists of two progressive phases: (1) Noise-to-Structure Optimization, ensuring global structural consistency through controlled spectral modulation, and (2) Structure-to-Texture Optimization, enhancing fine-grained details via gradient-based refinement. The proposed approach operates without additional training, supporting various image translation tasks, including cross-domain transformations and image to image translation. Extensive experiments on multiple datasets, including FFHQ and AFHQ, demonstrate that the proposed method achieves significant improvements over existing approaches, delivering superior generative quality and broader applicability in real-world scenarios.&lt;/p&gt;</content:encoded></item><item><title>Moving vehicles tracking from satellite video data based on spatiotemporal high-order relation learning and reasoning</title><link>https://doi.org/10.1016/j.jag.2025.105015</link><guid>10.1016/j.jag.2025.105015</guid><pubDate>Sat, 13 Dec 2025 21:59:02 +0000</pubDate><dc:creator>Ziyuan Feng</dc:creator><dc:creator>Xianfeng Zhang</dc:creator><dc:creator>Bo Zhou</dc:creator><dc:creator>Miao Ren</dc:creator><dc:creator>Xiaobo Zhi</dc:creator><prism:publicationName>International Journal of Applied Earth Observation and Geoinformation</prism:publicationName><prism:doi>10.1016/j.jag.2025.105015</prism:doi><description>Tracking moving vehicles in satellite videos presents several challenges, including complex background interference and the difficulty of detecting small targets. Most existing multiple object tracking (MOT) methods utilize convolutional models to capture local semantics or self-attention mechanisms to address global semantics for moving target detection. However, these methods tend to struggle with small and visually similar targets, making them particularly vulnerable to complex background interference, which often results in a large number of false positives and missed detections. Furthermore, many current approaches rely on the Hungarian matching algorithm or other intricate, unlearnable association optimization methods to achieve effective tracking once relevant information is gathered. This reliance often yields suboptimal outputs from the network models. To tackle these issues, this article presents an end-to-end graph network based on spatiotemporal high-order relation learning and reasoning for vehicle tracking in satellite video. The representation module of spatial high-order relations is designed to capture the spatial high-order relations between moving vehicles and their local environments, as well as global key references. Meanwhile, the temporal semantic reasoning module focuses on analyzing the evolution of these spatial high-order relations over time, thereby constructing the spatiotemporal high-order connections among the targets of interest and ensuring the continuous and stable detection of moving vehicles. Ultimately, a graph network based on spatiotemporal high-order relation reasoning is developed to perform learnable associations of target information across video frames, achieving a globally optimal solution to the tracking problem. Comparative experiments on the SatVideoDT, CGSTL, and ShuangQing-1 satellite video datasets demonstrate that the proposed method effectively enables end-to-end tracking of moving vehicles, attaining state-of-the-art performance across most evaluation metrics. On the SatVideoDT dataset, the model achieves a Multiple Object Tracking Accuracy (MOTA) of 65.1% and an Identity F1 Score (IDF1) of 70.9%. The proposed network model holds significant promise for the automated interpretation of satellite video data. The code is available at https://github.com/zsspo/GHOST-R.
Published: 2025-12-13T21:59:02+00:00
Venue: International Journal of Applied Earth Observation and Geoinformation
Score: 0.784 (must_read)</description><content:encoded>&lt;p&gt;&lt;strong&gt;Authors:&lt;/strong&gt; Ziyuan Feng; Xianfeng Zhang; Bo Zhou; Miao Ren; Xiaobo Zhi&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Journal:&lt;/strong&gt; International Journal of Applied Earth Observation and Geoinformation&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;a href="https://doi.org/10.1016/j.jag.2025.105015"&gt;10.1016/j.jag.2025.105015&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Relevance:&lt;/strong&gt; 0.784 (must_read)&lt;/p&gt;
&lt;p&gt;Tracking moving vehicles in satellite videos presents several challenges, including complex background interference and the difficulty of detecting small targets. Most existing multiple object tracking (MOT) methods utilize convolutional models to capture local semantics or self-attention mechanisms to address global semantics for moving target detection. However, these methods tend to struggle with small and visually similar targets, making them particularly vulnerable to complex background interference, which often results in a large number of false positives and missed detections. Furthermore, many current approaches rely on the Hungarian matching algorithm or other intricate, unlearnable association optimization methods to achieve effective tracking once relevant information is gathered. This reliance often yields suboptimal outputs from the network models. To tackle these issues, this article presents an end-to-end graph network based on spatiotemporal high-order relation learning and reasoning for vehicle tracking in satellite video. The representation module of spatial high-order relations is designed to capture the spatial high-order relations between moving vehicles and their local environments, as well as global key references. Meanwhile, the temporal semantic reasoning module focuses on analyzing the evolution of these spatial high-order relations over time, thereby constructing the spatiotemporal high-order connections among the targets of interest and ensuring the continuous and stable detection of moving vehicles. Ultimately, a graph network based on spatiotemporal high-order relation reasoning is developed to perform learnable associations of target information across video frames, achieving a globally optimal solution to the tracking problem. Comparative experiments on the SatVideoDT, CGSTL, and ShuangQing-1 satellite video datasets demonstrate that the proposed method effectively enables end-to-end tracking of moving vehicles, attaining state-of-the-art performance across most evaluation metrics. On the SatVideoDT dataset, the model achieves a Multiple Object Tracking Accuracy (MOTA) of 65.1% and an Identity F1 Score (IDF1) of 70.9%. The proposed network model holds significant promise for the automated interpretation of satellite video data. The code is available at https://github.com/zsspo/GHOST-R.&lt;/p&gt;</content:encoded></item><item><title>DCDLNet: A Label-Noise Tolerant Classification Algorithm for PolSAR Images Based on Dual-Band Consistency and Difference</title><link>https://doi.org/10.1016/j.knosys.2025.115120</link><guid>10.1016/j.knosys.2025.115120</guid><pubDate>Sat, 13 Dec 2025 23:34:43 +0000</pubDate><dc:creator>Xinyue Xin</dc:creator><dc:creator>Ming Li</dc:creator><dc:creator>Yan Wu</dc:creator><dc:creator>Peng Zhang</dc:creator><dc:creator>Dazhi Xu</dc:creator><prism:publicationName>Knowledge-Based Systems</prism:publicationName><prism:doi>10.1016/j.knosys.2025.115120</prism:doi><description>With the advancement of technology, PolSAR systems can acquire multiple signals by transmitting and receiving electromagnetic waves in different frequency bands, thereby enabling the collection of richer ground observation information. However, due to the lack of consideration for the concepts of dual-band consistency and dual-band difference, existing fusion methods still encounter problems of incomplete semantic information and low computational efficiency. Moreover, in practice, the process of sample labeling often involves manual intervention, which inevitably introduces labeling errors. To tackle these problems, we propose a novel label-noise tolerant classification framework called DCDLNet: dual-band consistency and difference learning network. Specifically, to extract the rich information contained in dual-band PolSAR data, the DCDLNet comprises two principal parts. The first part is an inter-band difference acquisition module (IDAM), which learns dual-band complementary information based on the concept of dual-band difference. The second part is a spatial-domain and frequency-domain feature extraction (SFFE) module. It acquires more discriminative information by capturing local spatial information in the spatial-domain and global spatial information in the frequency-domain. Furthermore, by integrating the concept of dual-band consistency and the fitting capabilities of neural networks, DCDLNet adopts a cross-band and bidirectional supervised (CBS) strategy to mitigate the impact of label noise during the training process. Experiments on measured PolSAR datasets demonstrate that our method outperforms several existing approaches in terms of dual-band fusion and noisy label processing.
Published: 2025-12-13T23:34:43+00:00
Venue: Knowledge-Based Systems
Score: 0.784 (must_read)</description><content:encoded>&lt;p&gt;&lt;strong&gt;Authors:&lt;/strong&gt; Xinyue Xin; Ming Li; Yan Wu; Peng Zhang; Dazhi Xu&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Journal:&lt;/strong&gt; Knowledge-Based Systems&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;a href="https://doi.org/10.1016/j.knosys.2025.115120"&gt;10.1016/j.knosys.2025.115120&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Relevance:&lt;/strong&gt; 0.784 (must_read)&lt;/p&gt;
&lt;p&gt;With the advancement of technology, PolSAR systems can acquire multiple signals by transmitting and receiving electromagnetic waves in different frequency bands, thereby enabling the collection of richer ground observation information. However, due to the lack of consideration for the concepts of dual-band consistency and dual-band difference, existing fusion methods still encounter problems of incomplete semantic information and low computational efficiency. Moreover, in practice, the process of sample labeling often involves manual intervention, which inevitably introduces labeling errors. To tackle these problems, we propose a novel label-noise tolerant classification framework called DCDLNet: dual-band consistency and difference learning network. Specifically, to extract the rich information contained in dual-band PolSAR data, the DCDLNet comprises two principal parts. The first part is an inter-band difference acquisition module (IDAM), which learns dual-band complementary information based on the concept of dual-band difference. The second part is a spatial-domain and frequency-domain feature extraction (SFFE) module. It acquires more discriminative information by capturing local spatial information in the spatial-domain and global spatial information in the frequency-domain. Furthermore, by integrating the concept of dual-band consistency and the fitting capabilities of neural networks, DCDLNet adopts a cross-band and bidirectional supervised (CBS) strategy to mitigate the impact of label noise during the training process. Experiments on measured PolSAR datasets demonstrate that our method outperforms several existing approaches in terms of dual-band fusion and noisy label processing.&lt;/p&gt;</content:encoded></item><item><title>Do We Need Reformer for Vision? An Experimental Comparison with Vision Transformers</title><link>https://arxiv.org/abs/2512.11260v1</link><guid>http://arxiv.org/abs/2512.11260v1</guid><pubDate>Fri, 12 Dec 2025 03:49:55 +0000</pubDate><dc:creator>Ali El Bellaj</dc:creator><dc:creator>Mohammed-Amine Cheddadi</dc:creator><dc:creator>Rhassan Berber</dc:creator><dc:type>preprint</dc:type><prism:publicationName>arXiv</prism:publicationName><description>Transformers have recently demonstrated strong performance in computer vision, with Vision Transformers (ViTs) leveraging self-attention to capture both low-level and high-level image features. However, standard ViTs remain computationally expensive, since global self-attention scales quadratically with the number of tokens, which limits their practicality for high-resolution inputs and resource-constrained settings.
  In this work, we investigate the Reformer architecture as an alternative vision backbone. By combining patch-based tokenization with locality-sensitive hashing (LSH) attention, our model approximates global self-attention while reducing its theoretical time complexity from $\mathcal{O}(n^2)$ to $\mathcal{O}(n \log n)$ in the sequence length $n$. We evaluate the proposed Reformer-based vision model on CIFAR-10 to assess its behavior on small-scale datasets, on ImageNet-100 to study its accuracy--efficiency trade-off in a more realistic setting, and on a high-resolution medical imaging dataset to evaluate the model under longer token sequences.
  While the Reformer achieves higher accuracy on CIFAR-10 compared to our ViT-style baseline, the ViT model consistently outperforms the Reformer in our experiments in terms of practical efficiency and end-to-end computation time across the larger and higher-resolution settings. These results suggest that, despite the theoretical advantages of LSH-based attention, meaningful computation gains require sequence lengths substantially longer than those produced by typical high-resolution images.
Published: 2025-12-12T03:49:55+00:00
Venue: arXiv
Score: 0.780 (must_read)</description><content:encoded>&lt;p&gt;&lt;strong&gt;Authors:&lt;/strong&gt; Ali El Bellaj; Mohammed-Amine Cheddadi; Rhassan Berber&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Journal:&lt;/strong&gt; arXiv&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Relevance:&lt;/strong&gt; 0.780 (must_read)&lt;/p&gt;
&lt;p&gt;Transformers have recently demonstrated strong performance in computer vision, with Vision Transformers (ViTs) leveraging self-attention to capture both low-level and high-level image features. However, standard ViTs remain computationally expensive, since global self-attention scales quadratically with the number of tokens, which limits their practicality for high-resolution inputs and resource-constrained settings.
  In this work, we investigate the Reformer architecture as an alternative vision backbone. By combining patch-based tokenization with locality-sensitive hashing (LSH) attention, our model approximates global self-attention while reducing its theoretical time complexity from $\mathcal{O}(n^2)$ to $\mathcal{O}(n \log n)$ in the sequence length $n$. We evaluate the proposed Reformer-based vision model on CIFAR-10 to assess its behavior on small-scale datasets, on ImageNet-100 to study its accuracy--efficiency trade-off in a more realistic setting, and on a high-resolution medical imaging dataset to evaluate the model under longer token sequences.
  While the Reformer achieves higher accuracy on CIFAR-10 compared to our ViT-style baseline, the ViT model consistently outperforms the Reformer in our experiments in terms of practical efficiency and end-to-end computation time across the larger and higher-resolution settings. These results suggest that, despite the theoretical advantages of LSH-based attention, meaningful computation gains require sequence lengths substantially longer than those produced by typical high-resolution images.&lt;/p&gt;</content:encoded></item><item><title>Multi-modal Integration with Adversarial Mutual Distribution Matching</title><link>https://doi.org/10.1016/j.patcog.2025.112818</link><guid>10.1016/j.patcog.2025.112818</guid><pubDate>Sun, 14 Dec 2025 23:00:13 +0000</pubDate><dc:creator>Ouhan Huang</dc:creator><dc:creator>Jianyang Shi</dc:creator><dc:creator>Ziwei Li</dc:creator><dc:creator>Siyuan Ye</dc:creator><dc:creator>Chao Shen</dc:creator><dc:creator>Junwen Zhang</dc:creator><dc:creator>Haiwen Cai</dc:creator><dc:creator>Nan Chi</dc:creator><dc:creator>Feng Bao</dc:creator><prism:publicationName>Pattern Recognition</prism:publicationName><prism:doi>10.1016/j.patcog.2025.112818</prism:doi><description>Integrating multimodal data requires learning representations that are invariant and complementary across modalities. Most existing approaches focus on instance-level alignment by explicitly matching paired samples, but they often fail to capture the global structure and are sensitive to noise or missing modalities. This paper presents Adversarial Mutual Distribution Matching (adMDM), a unified framework that jointly enforces sample-level and distribution-level consistency for robust multimodal integration. The proposed method leverages the Wasserstein distance to align latent distributions while maintaining instance-wise correspondence through cosine similarity and reconstruction constraints. A mutual adversarial optimization strategy is introduced to dynamically adapt both modality-specific encoders, achieving symmetric and stable distribution matching. Extensive experiments on synthetic, transformed MNIST, and real-world CITE-seq datasets demonstrate that adMDM not only enhances cross-modal correlation and semantic consistency but also shows superior robustness against data degradation compared with ten state-of-the-art baselines. These results highlight adMDM as a principled and scalable approach to multimodal representation learning under heterogeneous and noisy conditions.
Published: 2025-12-14T23:00:13+00:00
Venue: Pattern Recognition
Score: 0.780 (must_read)</description><content:encoded>&lt;p&gt;&lt;strong&gt;Authors:&lt;/strong&gt; Ouhan Huang; Jianyang Shi; Ziwei Li; Siyuan Ye; Chao Shen; Junwen Zhang; Haiwen Cai; Nan Chi; Feng Bao&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Journal:&lt;/strong&gt; Pattern Recognition&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;a href="https://doi.org/10.1016/j.patcog.2025.112818"&gt;10.1016/j.patcog.2025.112818&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Relevance:&lt;/strong&gt; 0.780 (must_read)&lt;/p&gt;
&lt;p&gt;Integrating multimodal data requires learning representations that are invariant and complementary across modalities. Most existing approaches focus on instance-level alignment by explicitly matching paired samples, but they often fail to capture the global structure and are sensitive to noise or missing modalities. This paper presents Adversarial Mutual Distribution Matching (adMDM), a unified framework that jointly enforces sample-level and distribution-level consistency for robust multimodal integration. The proposed method leverages the Wasserstein distance to align latent distributions while maintaining instance-wise correspondence through cosine similarity and reconstruction constraints. A mutual adversarial optimization strategy is introduced to dynamically adapt both modality-specific encoders, achieving symmetric and stable distribution matching. Extensive experiments on synthetic, transformed MNIST, and real-world CITE-seq datasets demonstrate that adMDM not only enhances cross-modal correlation and semantic consistency but also shows superior robustness against data degradation compared with ten state-of-the-art baselines. These results highlight adMDM as a principled and scalable approach to multimodal representation learning under heterogeneous and noisy conditions.&lt;/p&gt;</content:encoded></item><item><title>TransLocNet: Cross-Modal Attention for Aerial-Ground Vehicle Localization with Contrastive Learning</title><link>https://arxiv.org/abs/2512.10419v1</link><guid>http://arxiv.org/abs/2512.10419v1</guid><pubDate>Thu, 11 Dec 2025 08:34:26 +0000</pubDate><dc:creator>Phu Pham</dc:creator><dc:creator>Damon Conover</dc:creator><dc:creator>Aniket Bera</dc:creator><dc:type>preprint</dc:type><prism:publicationName>arXiv</prism:publicationName><description>Aerial-ground localization is difficult due to large viewpoint and modality gaps between ground-level LiDAR and overhead imagery. We propose TransLocNet, a cross-modal attention framework that fuses LiDAR geometry with aerial semantic context. LiDAR scans are projected into a bird's-eye-view representation and aligned with aerial features through bidirectional attention, followed by a likelihood map decoder that outputs spatial probability distributions over position and orientation. A contrastive learning module enforces a shared embedding space to improve cross-modal alignment. Experiments on CARLA and KITTI show that TransLocNet outperforms state-of-the-art baselines, reducing localization error by up to 63% and achieving sub-meter, sub-degree accuracy. These results demonstrate that TransLocNet provides robust and generalizable aerial-ground localization in both synthetic and real-world settings.
Published: 2025-12-11T08:34:26+00:00
Venue: arXiv
Score: 0.778 (must_read)</description><content:encoded>&lt;p&gt;&lt;strong&gt;Authors:&lt;/strong&gt; Phu Pham; Damon Conover; Aniket Bera&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Journal:&lt;/strong&gt; arXiv&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Relevance:&lt;/strong&gt; 0.778 (must_read)&lt;/p&gt;
&lt;p&gt;Aerial-ground localization is difficult due to large viewpoint and modality gaps between ground-level LiDAR and overhead imagery. We propose TransLocNet, a cross-modal attention framework that fuses LiDAR geometry with aerial semantic context. LiDAR scans are projected into a bird&amp;#x27;s-eye-view representation and aligned with aerial features through bidirectional attention, followed by a likelihood map decoder that outputs spatial probability distributions over position and orientation. A contrastive learning module enforces a shared embedding space to improve cross-modal alignment. Experiments on CARLA and KITTI show that TransLocNet outperforms state-of-the-art baselines, reducing localization error by up to 63% and achieving sub-meter, sub-degree accuracy. These results demonstrate that TransLocNet provides robust and generalizable aerial-ground localization in both synthetic and real-world settings.&lt;/p&gt;</content:encoded></item><item><title>Disentangled Image-Text Classification: Enhancing Visual Representations with MLLM-driven Knowledge Transfer</title><link>https://doi.org/10.1016/j.eswa.2025.130790</link><guid>10.1016/j.eswa.2025.130790</guid><pubDate>Sun, 14 Dec 2025 15:19:32 +0000</pubDate><dc:creator>Qianjun Shuai</dc:creator><dc:creator>Xiaohao Chen</dc:creator><dc:creator>Yongqiang Cheng</dc:creator><dc:creator>Fang Miao</dc:creator><dc:creator>Libiao Jin</dc:creator><prism:publicationName>Expert Systems with Applications</prism:publicationName><prism:doi>10.1016/j.eswa.2025.130790</prism:doi><description>Multimodal image-text classification plays a critical role in applications such as content moderation, news recommendation, and multimedia understanding. Despite recent advances, visual modality faces higher representation learning complexity than textual modality in semantic extraction, which often leads to a semantic gap between visual and textual representations. In addition, conventional fusion strategies introduce cross-modal redundancy, further limiting classification performance. To address these issues, we propose MD-MLLM , a novel image-text classification framework that leverages large multimodal language models (MLLMs) to generate semantically enhanced visual representations. To mitigate redundancy introduced by direct MLLM feature integration, we introduce a hierarchical disentanglement mechanism based on the Hilbert-Schmidt Independence Criterion (HSIC) and orthogonality constraints, which explicitly separates modality-specific and shared representations. Furthermore, a hierarchical fusion strategy combines original unimodal features with disentangled shared semantics, promoting discriminative feature learning and cross-modal complementarity. Extensive experiments on two benchmark datasets, N24News and Food101 , show that MD-MLLM achieves consistently stable improvements in classification accuracy and exhibits competitive performance compared with various representative multimodal baselines. The framework also demonstrates good generalization ability and robustness across different multimodal scenarios. The code is available at https://github.com/xiaohaochen0308/MD-MLLM .
Published: 2025-12-14T15:19:32+00:00
Venue: Expert Systems with Applications
Score: 0.776 (must_read)</description><content:encoded>&lt;p&gt;&lt;strong&gt;Authors:&lt;/strong&gt; Qianjun Shuai; Xiaohao Chen; Yongqiang Cheng; Fang Miao; Libiao Jin&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Journal:&lt;/strong&gt; Expert Systems with Applications&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;a href="https://doi.org/10.1016/j.eswa.2025.130790"&gt;10.1016/j.eswa.2025.130790&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Relevance:&lt;/strong&gt; 0.776 (must_read)&lt;/p&gt;
&lt;p&gt;Multimodal image-text classification plays a critical role in applications such as content moderation, news recommendation, and multimedia understanding. Despite recent advances, visual modality faces higher representation learning complexity than textual modality in semantic extraction, which often leads to a semantic gap between visual and textual representations. In addition, conventional fusion strategies introduce cross-modal redundancy, further limiting classification performance. To address these issues, we propose MD-MLLM , a novel image-text classification framework that leverages large multimodal language models (MLLMs) to generate semantically enhanced visual representations. To mitigate redundancy introduced by direct MLLM feature integration, we introduce a hierarchical disentanglement mechanism based on the Hilbert-Schmidt Independence Criterion (HSIC) and orthogonality constraints, which explicitly separates modality-specific and shared representations. Furthermore, a hierarchical fusion strategy combines original unimodal features with disentangled shared semantics, promoting discriminative feature learning and cross-modal complementarity. Extensive experiments on two benchmark datasets, N24News and Food101 , show that MD-MLLM achieves consistently stable improvements in classification accuracy and exhibits competitive performance compared with various representative multimodal baselines. The framework also demonstrates good generalization ability and robustness across different multimodal scenarios. The code is available at https://github.com/xiaohaochen0308/MD-MLLM .&lt;/p&gt;</content:encoded></item><item><title>Whale Identification and Size Estimation in Satellite Imagery via Intelligent Subtle Perception</title><link>https://doi.org/10.1016/j.eswa.2025.130778</link><guid>10.1016/j.eswa.2025.130778</guid><pubDate>Sat, 13 Dec 2025 23:34:38 +0000</pubDate><dc:creator>Siqi Wang</dc:creator><dc:creator>Baoxiang Huang</dc:creator><dc:creator>Milena Radenkovic</dc:creator><dc:creator>Ge Chen</dc:creator><prism:publicationName>Expert Systems with Applications</prism:publicationName><prism:doi>10.1016/j.eswa.2025.130778</prism:doi><description>Protecting whales is essential for preserving ecological balance because they are essential to marine ecosystems. Conventional detection techniques, such as acoustic technology and visual observation, are expensive, ineffective, and susceptible to environmental influences. Although satellite remote sensing provides a more comprehensive view of whale monitoring, difficulties still exist due to the complexity of the marine environment and the scarcity of datasets. Deep learning, fortunately, is becoming a powerful tool that can accelerate the optimization of subtle perception for a variety of multidisciplinary applications that enable high-precision and accurate real-time ocean observation. Here, a comprehensive methodology is proposed to implement whale detection and size estimation in satellite images, leveraging advanced artificial intelligence and data augmentation strategies to overcome the challenges. First, several techniques, including grayscale processing, Gaussian noise addition, random flipping and cropping, histogram equalization, image overlay, and Clustering Generative Adversarial Networks, are employed to generate synthetic images to augment the whale satellite dataset. Second, a custom detection model is extended with TripletAttention (TA) modules for accurate feature extraction and detection performance in complex marine environments. The expanded dataset is then used to train the model, achieving an mAP 0.5 score of 0.89, indicating high accuracy in whale detection. Next, the spatial resolution of the images is used to estimate the size of the discovered whales, providing valuable data for biological studies. Finally, the proposed method is extended to monitor whale activity in specific regions, such as Hawaii, confirming peak activity levels from December to April. This research supports the effective application of artificial intelligence in the detection of large marine species.
Published: 2025-12-13T23:34:38+00:00
Venue: Expert Systems with Applications
Score: 0.775 (must_read)</description><content:encoded>&lt;p&gt;&lt;strong&gt;Authors:&lt;/strong&gt; Siqi Wang; Baoxiang Huang; Milena Radenkovic; Ge Chen&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Journal:&lt;/strong&gt; Expert Systems with Applications&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;a href="https://doi.org/10.1016/j.eswa.2025.130778"&gt;10.1016/j.eswa.2025.130778&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Relevance:&lt;/strong&gt; 0.775 (must_read)&lt;/p&gt;
&lt;p&gt;Protecting whales is essential for preserving ecological balance because they are essential to marine ecosystems. Conventional detection techniques, such as acoustic technology and visual observation, are expensive, ineffective, and susceptible to environmental influences. Although satellite remote sensing provides a more comprehensive view of whale monitoring, difficulties still exist due to the complexity of the marine environment and the scarcity of datasets. Deep learning, fortunately, is becoming a powerful tool that can accelerate the optimization of subtle perception for a variety of multidisciplinary applications that enable high-precision and accurate real-time ocean observation. Here, a comprehensive methodology is proposed to implement whale detection and size estimation in satellite images, leveraging advanced artificial intelligence and data augmentation strategies to overcome the challenges. First, several techniques, including grayscale processing, Gaussian noise addition, random flipping and cropping, histogram equalization, image overlay, and Clustering Generative Adversarial Networks, are employed to generate synthetic images to augment the whale satellite dataset. Second, a custom detection model is extended with TripletAttention (TA) modules for accurate feature extraction and detection performance in complex marine environments. The expanded dataset is then used to train the model, achieving an mAP 0.5 score of 0.89, indicating high accuracy in whale detection. Next, the spatial resolution of the images is used to estimate the size of the discovered whales, providing valuable data for biological studies. Finally, the proposed method is extended to monitor whale activity in specific regions, such as Hawaii, confirming peak activity levels from December to April. This research supports the effective application of artificial intelligence in the detection of large marine species.&lt;/p&gt;</content:encoded></item><item><title>Reliable Detection of Minute Targets in High-Resolution Aerial Imagery across Temporal Shifts</title><link>https://arxiv.org/abs/2512.11360v1</link><guid>http://arxiv.org/abs/2512.11360v1</guid><pubDate>Fri, 12 Dec 2025 08:20:11 +0000</pubDate><dc:creator>Mohammad Sadegh Gholizadeh</dc:creator><dc:creator>Amir Arsalan Rezapour</dc:creator><dc:creator>Hamidreza Shayegh</dc:creator><dc:creator>Ehsan Pazouki</dc:creator><dc:type>preprint</dc:type><prism:publicationName>arXiv</prism:publicationName><description>Efficient crop detection via Unmanned Aerial Vehicles is critical for scaling precision agriculture, yet it remains challenging due to the small scale of targets and environmental variability. This paper addresses the detection of rice seedlings in paddy fields by leveraging a Faster R-CNN architecture initialized via transfer learning. To overcome the specific difficulties of detecting minute objects in high-resolution aerial imagery, we curate a significant UAV dataset for training and rigorously evaluate the model's generalization capabilities. Specifically, we validate performance across three distinct test sets acquired at different temporal intervals, thereby assessing robustness against varying imaging conditions. Our empirical results demonstrate that transfer learning not only facilitates the rapid convergence of object detection models in agricultural contexts but also yields consistent performance despite domain shifts in image acquisition.
Published: 2025-12-12T08:20:11+00:00
Venue: arXiv
Score: 0.773 (must_read)</description><content:encoded>&lt;p&gt;&lt;strong&gt;Authors:&lt;/strong&gt; Mohammad Sadegh Gholizadeh; Amir Arsalan Rezapour; Hamidreza Shayegh; Ehsan Pazouki&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Journal:&lt;/strong&gt; arXiv&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Relevance:&lt;/strong&gt; 0.773 (must_read)&lt;/p&gt;
&lt;p&gt;Efficient crop detection via Unmanned Aerial Vehicles is critical for scaling precision agriculture, yet it remains challenging due to the small scale of targets and environmental variability. This paper addresses the detection of rice seedlings in paddy fields by leveraging a Faster R-CNN architecture initialized via transfer learning. To overcome the specific difficulties of detecting minute objects in high-resolution aerial imagery, we curate a significant UAV dataset for training and rigorously evaluate the model&amp;#x27;s generalization capabilities. Specifically, we validate performance across three distinct test sets acquired at different temporal intervals, thereby assessing robustness against varying imaging conditions. Our empirical results demonstrate that transfer learning not only facilitates the rapid convergence of object detection models in agricultural contexts but also yields consistent performance despite domain shifts in image acquisition.&lt;/p&gt;</content:encoded></item><item><title>Cross-modal Context-aware Learning for Visual Prompt Guided Multimodal Image Understanding in Remote Sensing</title><link>https://arxiv.org/abs/2512.11680v1</link><guid>http://arxiv.org/abs/2512.11680v1</guid><pubDate>Fri, 12 Dec 2025 15:59:49 +0000</pubDate><dc:creator>Xu Zhang</dc:creator><dc:creator>Jiabin Fang</dc:creator><dc:creator>Zhuoming Ding</dc:creator><dc:creator>Jin Yuan</dc:creator><dc:creator>Xuan Liu</dc:creator><dc:creator>Qianjun Zhang</dc:creator><dc:creator>Zhiyong Li</dc:creator><dc:type>preprint</dc:type><prism:publicationName>arXiv</prism:publicationName><description>Recent advances in image understanding have enabled methods that leverage large language models for multimodal reasoning in remote sensing. However, existing approaches still struggle to steer models to the user-relevant regions when only simple, generic text prompts are available. Moreover, in large-scale aerial imagery many objects exhibit highly similar visual appearances and carry rich inter-object relationships, which further complicates accurate recognition. To address these challenges, we propose Cross-modal Context-aware Learning for Visual Prompt-Guided Multimodal Image Understanding (CLV-Net). CLV-Net lets users supply a simple visual cue, a bounding box, to indicate a region of interest, and uses that cue to guide the model to generate correlated segmentation masks and captions that faithfully reflect user intent. Central to our design is a Context-Aware Mask Decoder that models and integrates inter-object relationships to strengthen target representations and improve mask quality. In addition, we introduce a Semantic and Relationship Alignment module: a Cross-modal Semantic Consistency Loss enhances fine-grained discrimination among visually similar targets, while a Relationship Consistency Loss enforces alignment between textual relations and visual interactions. Comprehensive experiments on two benchmark datasets show that CLV-Net outperforms existing methods and establishes new state-of-the-art results. The model effectively captures user intent and produces precise, intention-aligned multimodal outputs.
Published: 2025-12-12T15:59:49+00:00
Venue: arXiv
Score: 0.773 (must_read)</description><content:encoded>&lt;p&gt;&lt;strong&gt;Authors:&lt;/strong&gt; Xu Zhang; Jiabin Fang; Zhuoming Ding; Jin Yuan; Xuan Liu; Qianjun Zhang; Zhiyong Li&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Journal:&lt;/strong&gt; arXiv&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Relevance:&lt;/strong&gt; 0.773 (must_read)&lt;/p&gt;
&lt;p&gt;Recent advances in image understanding have enabled methods that leverage large language models for multimodal reasoning in remote sensing. However, existing approaches still struggle to steer models to the user-relevant regions when only simple, generic text prompts are available. Moreover, in large-scale aerial imagery many objects exhibit highly similar visual appearances and carry rich inter-object relationships, which further complicates accurate recognition. To address these challenges, we propose Cross-modal Context-aware Learning for Visual Prompt-Guided Multimodal Image Understanding (CLV-Net). CLV-Net lets users supply a simple visual cue, a bounding box, to indicate a region of interest, and uses that cue to guide the model to generate correlated segmentation masks and captions that faithfully reflect user intent. Central to our design is a Context-Aware Mask Decoder that models and integrates inter-object relationships to strengthen target representations and improve mask quality. In addition, we introduce a Semantic and Relationship Alignment module: a Cross-modal Semantic Consistency Loss enhances fine-grained discrimination among visually similar targets, while a Relationship Consistency Loss enforces alignment between textual relations and visual interactions. Comprehensive experiments on two benchmark datasets show that CLV-Net outperforms existing methods and establishes new state-of-the-art results. The model effectively captures user intent and produces precise, intention-aligned multimodal outputs.&lt;/p&gt;</content:encoded></item><item><title>Reusing Source Diffusion Model for Domain Perception: Towards Few-shot Image Generation via Fine-tuning</title><link>https://doi.org/10.1016/j.eswa.2025.130797</link><guid>10.1016/j.eswa.2025.130797</guid><pubDate>Sat, 13 Dec 2025 02:33:33 +0000</pubDate><dc:creator>Yusen Zhang</dc:creator><dc:creator>Min Li</dc:creator><dc:creator>Song Yan</dc:creator><dc:creator>Guanye Xiong</dc:creator><dc:creator>Yujie He</dc:creator><prism:publicationName>Expert Systems with Applications</prism:publicationName><prism:doi>10.1016/j.eswa.2025.130797</prism:doi><description>Training a generative diffusion model with limited images is challenging. Existing methods employ conditional gradient control and pixel-level optimization strategies during model fine-tuning. While these facilitate the inheritance of source domain priors, they struggle to effectively align cross-domain feature distribution discrepancies, thus limiting the improvement of generation quality and diversity. To address these problems, this paper proposes ReSo , a novel classifier-guided diffusion model for limited data scenarios. By Re using the So urce pre-trained diffusion model, we design a domain perception process based on dual time-step sampling. This imposes cross-domain consistency constraints on diffusion latent features rather than at the pixel level, aligning feature differences at any time step, thus avoiding image blurring and distortion caused by pixel-level sensitive Mean Squared Error loss. Furthermore, to enhance both the richness and accuracy of the diffusion sampling process guided by conditional gradient information, we propose a new guidance strategy leveraging a random mask classifier. This encourages the classifier to identify shared features between the target and source domain images, thereby generating richer conditional gradient information to guide cross-domain image generation and improving the diversity of generated results. Theoretical analysis, qualitative and quantitative experiments demonstrate the superiority of our model in cross-domain few-shot image generation tasks compared to prior methods.
Published: 2025-12-13T02:33:33+00:00
Venue: Expert Systems with Applications
Score: 0.773 (must_read)</description><content:encoded>&lt;p&gt;&lt;strong&gt;Authors:&lt;/strong&gt; Yusen Zhang; Min Li; Song Yan; Guanye Xiong; Yujie He&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Journal:&lt;/strong&gt; Expert Systems with Applications&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;a href="https://doi.org/10.1016/j.eswa.2025.130797"&gt;10.1016/j.eswa.2025.130797&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Relevance:&lt;/strong&gt; 0.773 (must_read)&lt;/p&gt;
&lt;p&gt;Training a generative diffusion model with limited images is challenging. Existing methods employ conditional gradient control and pixel-level optimization strategies during model fine-tuning. While these facilitate the inheritance of source domain priors, they struggle to effectively align cross-domain feature distribution discrepancies, thus limiting the improvement of generation quality and diversity. To address these problems, this paper proposes ReSo , a novel classifier-guided diffusion model for limited data scenarios. By Re using the So urce pre-trained diffusion model, we design a domain perception process based on dual time-step sampling. This imposes cross-domain consistency constraints on diffusion latent features rather than at the pixel level, aligning feature differences at any time step, thus avoiding image blurring and distortion caused by pixel-level sensitive Mean Squared Error loss. Furthermore, to enhance both the richness and accuracy of the diffusion sampling process guided by conditional gradient information, we propose a new guidance strategy leveraging a random mask classifier. This encourages the classifier to identify shared features between the target and source domain images, thereby generating richer conditional gradient information to guide cross-domain image generation and improving the diversity of generated results. Theoretical analysis, qualitative and quantitative experiments demonstrate the superiority of our model in cross-domain few-shot image generation tasks compared to prior methods.&lt;/p&gt;</content:encoded></item><item><title>Video Depth Propagation</title><link>https://arxiv.org/abs/2512.10725v1</link><guid>http://arxiv.org/abs/2512.10725v1</guid><pubDate>Thu, 11 Dec 2025 15:08:37 +0000</pubDate><dc:creator>Luigi Piccinelli</dc:creator><dc:creator>Thiemo Wandel</dc:creator><dc:creator>Christos Sakaridis</dc:creator><dc:creator>Wim Abbeloos</dc:creator><dc:creator>Luc Van Gool</dc:creator><dc:type>preprint</dc:type><prism:publicationName>arXiv</prism:publicationName><description>Depth estimation in videos is essential for visual perception in real-world applications. However, existing methods either rely on simple frame-by-frame monocular models, leading to temporal inconsistencies and inaccuracies, or use computationally demanding temporal modeling, unsuitable for real-time applications. These limitations significantly restrict general applicability and performance in practical settings. To address this, we propose VeloDepth, an efficient and robust online video depth estimation pipeline that effectively leverages spatiotemporal priors from previous depth predictions and performs deep feature propagation. Our method introduces a novel Propagation Module that refines and propagates depth features and predictions using flow-based warping coupled with learned residual corrections. In addition, our design structurally enforces temporal consistency, resulting in stable depth predictions across consecutive frames with improved efficiency. Comprehensive zero-shot evaluation on multiple benchmarks demonstrates the state-of-the-art temporal consistency and competitive accuracy of VeloDepth, alongside its significantly faster inference compared to existing video-based depth estimators. VeloDepth thus provides a practical, efficient, and accurate solution for real-time depth estimation suitable for diverse perception tasks. Code and models are available at https://github.com/lpiccinelli-eth/velodepth
Published: 2025-12-11T15:08:37+00:00
Venue: arXiv
Score: 0.772 (must_read)</description><content:encoded>&lt;p&gt;&lt;strong&gt;Authors:&lt;/strong&gt; Luigi Piccinelli; Thiemo Wandel; Christos Sakaridis; Wim Abbeloos; Luc Van Gool&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Journal:&lt;/strong&gt; arXiv&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Relevance:&lt;/strong&gt; 0.772 (must_read)&lt;/p&gt;
&lt;p&gt;Depth estimation in videos is essential for visual perception in real-world applications. However, existing methods either rely on simple frame-by-frame monocular models, leading to temporal inconsistencies and inaccuracies, or use computationally demanding temporal modeling, unsuitable for real-time applications. These limitations significantly restrict general applicability and performance in practical settings. To address this, we propose VeloDepth, an efficient and robust online video depth estimation pipeline that effectively leverages spatiotemporal priors from previous depth predictions and performs deep feature propagation. Our method introduces a novel Propagation Module that refines and propagates depth features and predictions using flow-based warping coupled with learned residual corrections. In addition, our design structurally enforces temporal consistency, resulting in stable depth predictions across consecutive frames with improved efficiency. Comprehensive zero-shot evaluation on multiple benchmarks demonstrates the state-of-the-art temporal consistency and competitive accuracy of VeloDepth, alongside its significantly faster inference compared to existing video-based depth estimators. VeloDepth thus provides a practical, efficient, and accurate solution for real-time depth estimation suitable for diverse perception tasks. Code and models are available at https://github.com/lpiccinelli-eth/velodepth&lt;/p&gt;</content:encoded></item><item><title>Learning complete and explainable visual representations from itemized text supervision</title><link>https://arxiv.org/abs/2512.11141v1</link><guid>http://arxiv.org/abs/2512.11141v1</guid><pubDate>Thu, 11 Dec 2025 22:01:57 +0000</pubDate><dc:creator>Yiwei Lyu</dc:creator><dc:creator>Chenhui Zhao</dc:creator><dc:creator>Soumyanil Banerjee</dc:creator><dc:creator>Shixuan Liu</dc:creator><dc:creator>Akshay Rao</dc:creator><dc:creator>Akhil Kondepudi</dc:creator><dc:creator>Honglak Lee</dc:creator><dc:creator>Todd C. Hollon</dc:creator><dc:type>preprint</dc:type><prism:publicationName>arXiv</prism:publicationName><description>Training vision models with language supervision enables general and transferable representations. However, many visual domains, especially non-object-centric domains such as medical imaging and remote sensing, contain itemized text annotations: multiple text items describing distinct and semantically independent findings within a single image. Such supervision differs from standard multi-caption supervision, where captions are redundant or highly overlapping. Here, we introduce ItemizedCLIP, a framework for learning complete and explainable visual representations from itemized text supervision. ItemizedCLIP employs a cross-attention module to produce text item-conditioned visual embeddings and a set of tailored objectives that jointly enforce item independence (distinct regions for distinct items) and representation completeness (coverage of all items). Across four domains with naturally itemized text supervision (brain MRI, head CT, chest CT, remote sensing) and one additional synthetically itemized dataset, ItemizedCLIP achieves substantial improvements in zero-shot performance and fine-grained interpretability over baselines. The resulting ItemizedCLIP representations are semantically grounded, item-differentiable, complete, and visually interpretable. Our code is available at https://github.com/MLNeurosurg/ItemizedCLIP.
Published: 2025-12-11T22:01:57+00:00
Venue: arXiv
Score: 0.771 (must_read)</description><content:encoded>&lt;p&gt;&lt;strong&gt;Authors:&lt;/strong&gt; Yiwei Lyu; Chenhui Zhao; Soumyanil Banerjee; Shixuan Liu; Akshay Rao; Akhil Kondepudi; Honglak Lee; Todd C. Hollon&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Journal:&lt;/strong&gt; arXiv&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Relevance:&lt;/strong&gt; 0.771 (must_read)&lt;/p&gt;
&lt;p&gt;Training vision models with language supervision enables general and transferable representations. However, many visual domains, especially non-object-centric domains such as medical imaging and remote sensing, contain itemized text annotations: multiple text items describing distinct and semantically independent findings within a single image. Such supervision differs from standard multi-caption supervision, where captions are redundant or highly overlapping. Here, we introduce ItemizedCLIP, a framework for learning complete and explainable visual representations from itemized text supervision. ItemizedCLIP employs a cross-attention module to produce text item-conditioned visual embeddings and a set of tailored objectives that jointly enforce item independence (distinct regions for distinct items) and representation completeness (coverage of all items). Across four domains with naturally itemized text supervision (brain MRI, head CT, chest CT, remote sensing) and one additional synthetically itemized dataset, ItemizedCLIP achieves substantial improvements in zero-shot performance and fine-grained interpretability over baselines. The resulting ItemizedCLIP representations are semantically grounded, item-differentiable, complete, and visually interpretable. Our code is available at https://github.com/MLNeurosurg/ItemizedCLIP.&lt;/p&gt;</content:encoded></item><item><title>Take a Peek: Efficient Encoder Adaptation for Few-Shot Semantic Segmentation via LoRA</title><link>https://arxiv.org/abs/2512.10521v1</link><guid>http://arxiv.org/abs/2512.10521v1</guid><pubDate>Thu, 11 Dec 2025 10:47:01 +0000</pubDate><dc:creator>Pasquale De Marinis</dc:creator><dc:creator>Gennaro Vessio</dc:creator><dc:creator>Giovanna Castellano</dc:creator><dc:type>preprint</dc:type><prism:publicationName>arXiv</prism:publicationName><description>Few-shot semantic segmentation (FSS) aims to segment novel classes in query images using only a small annotated support set. While prior research has mainly focused on improving decoders, the encoder's limited ability to extract meaningful features for unseen classes remains a key bottleneck. In this work, we introduce \textit{Take a Peek} (TaP), a simple yet effective method that enhances encoder adaptability for both FSS and cross-domain FSS (CD-FSS). TaP leverages Low-Rank Adaptation (LoRA) to fine-tune the encoder on the support set with minimal computational overhead, enabling fast adaptation to novel classes while mitigating catastrophic forgetting. Our method is model-agnostic and can be seamlessly integrated into existing FSS pipelines. Extensive experiments across multiple benchmarks--including COCO $20^i$, Pascal $5^i$, and cross-domain datasets such as DeepGlobe, ISIC, and Chest X-ray--demonstrate that TaP consistently improves segmentation performance across diverse models and shot settings. Notably, TaP delivers significant gains in complex multi-class scenarios, highlighting its practical effectiveness in realistic settings. A rank sensitivity analysis also shows that strong performance can be achieved even with low-rank adaptations, ensuring computational efficiency. By addressing a critical limitation in FSS--the encoder's generalization to novel classes--TaP paves the way toward more robust, efficient, and generalizable segmentation systems. The code is available at https://github.com/pasqualedem/TakeAPeek.
Published: 2025-12-11T10:47:01+00:00
Venue: arXiv
Score: 0.771 (must_read)</description><content:encoded>&lt;p&gt;&lt;strong&gt;Authors:&lt;/strong&gt; Pasquale De Marinis; Gennaro Vessio; Giovanna Castellano&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Journal:&lt;/strong&gt; arXiv&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Relevance:&lt;/strong&gt; 0.771 (must_read)&lt;/p&gt;
&lt;p&gt;Few-shot semantic segmentation (FSS) aims to segment novel classes in query images using only a small annotated support set. While prior research has mainly focused on improving decoders, the encoder&amp;#x27;s limited ability to extract meaningful features for unseen classes remains a key bottleneck. In this work, we introduce \textit{Take a Peek} (TaP), a simple yet effective method that enhances encoder adaptability for both FSS and cross-domain FSS (CD-FSS). TaP leverages Low-Rank Adaptation (LoRA) to fine-tune the encoder on the support set with minimal computational overhead, enabling fast adaptation to novel classes while mitigating catastrophic forgetting. Our method is model-agnostic and can be seamlessly integrated into existing FSS pipelines. Extensive experiments across multiple benchmarks--including COCO $20^i$, Pascal $5^i$, and cross-domain datasets such as DeepGlobe, ISIC, and Chest X-ray--demonstrate that TaP consistently improves segmentation performance across diverse models and shot settings. Notably, TaP delivers significant gains in complex multi-class scenarios, highlighting its practical effectiveness in realistic settings. A rank sensitivity analysis also shows that strong performance can be achieved even with low-rank adaptations, ensuring computational efficiency. By addressing a critical limitation in FSS--the encoder&amp;#x27;s generalization to novel classes--TaP paves the way toward more robust, efficient, and generalizable segmentation systems. The code is available at https://github.com/pasqualedem/TakeAPeek.&lt;/p&gt;</content:encoded></item><item><title>RaLiFlow: Scene Flow Estimation with 4D Radar and LiDAR Point Clouds</title><link>https://arxiv.org/abs/2512.10376v1</link><guid>http://arxiv.org/abs/2512.10376v1</guid><pubDate>Thu, 11 Dec 2025 07:41:33 +0000</pubDate><dc:creator>Jingyun Fu</dc:creator><dc:creator>Zhiyu Xiang</dc:creator><dc:creator>Na Zhao</dc:creator><dc:type>preprint</dc:type><prism:publicationName>arXiv</prism:publicationName><description>Recent multimodal fusion methods, integrating images with LiDAR point clouds, have shown promise in scene flow estimation. However, the fusion of 4D millimeter wave radar and LiDAR remains unexplored. Unlike LiDAR, radar is cheaper, more robust in various weather conditions and can detect point-wise velocity, making it a valuable complement to LiDAR. However, radar inputs pose challenges due to noise, low resolution, and sparsity. Moreover, there is currently no dataset that combines LiDAR and radar data specifically for scene flow estimation. To address this gap, we construct a Radar-LiDAR scene flow dataset based on a public real-world automotive dataset. We propose an effective preprocessing strategy for radar denoising and scene flow label generation, deriving more reliable flow ground truth for radar points out of the object boundaries. Additionally, we introduce RaLiFlow, the first joint scene flow learning framework for 4D radar and LiDAR, which achieves effective radar-LiDAR fusion through a novel Dynamic-aware Bidirectional Cross-modal Fusion (DBCF) module and a carefully designed set of loss functions. The DBCF module integrates dynamic cues from radar into the local cross-attention mechanism, enabling the propagation of contextual information across modalities. Meanwhile, the proposed loss functions mitigate the adverse effects of unreliable radar data during training and enhance the instance-level consistency in scene flow predictions from both modalities, particularly for dynamic foreground areas. Extensive experiments on the repurposed scene flow dataset demonstrate that our method outperforms existing LiDAR-based and radar-based single-modal methods by a significant margin.
Published: 2025-12-11T07:41:33+00:00
Venue: arXiv
Score: 0.770 (must_read)</description><content:encoded>&lt;p&gt;&lt;strong&gt;Authors:&lt;/strong&gt; Jingyun Fu; Zhiyu Xiang; Na Zhao&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Journal:&lt;/strong&gt; arXiv&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Relevance:&lt;/strong&gt; 0.770 (must_read)&lt;/p&gt;
&lt;p&gt;Recent multimodal fusion methods, integrating images with LiDAR point clouds, have shown promise in scene flow estimation. However, the fusion of 4D millimeter wave radar and LiDAR remains unexplored. Unlike LiDAR, radar is cheaper, more robust in various weather conditions and can detect point-wise velocity, making it a valuable complement to LiDAR. However, radar inputs pose challenges due to noise, low resolution, and sparsity. Moreover, there is currently no dataset that combines LiDAR and radar data specifically for scene flow estimation. To address this gap, we construct a Radar-LiDAR scene flow dataset based on a public real-world automotive dataset. We propose an effective preprocessing strategy for radar denoising and scene flow label generation, deriving more reliable flow ground truth for radar points out of the object boundaries. Additionally, we introduce RaLiFlow, the first joint scene flow learning framework for 4D radar and LiDAR, which achieves effective radar-LiDAR fusion through a novel Dynamic-aware Bidirectional Cross-modal Fusion (DBCF) module and a carefully designed set of loss functions. The DBCF module integrates dynamic cues from radar into the local cross-attention mechanism, enabling the propagation of contextual information across modalities. Meanwhile, the proposed loss functions mitigate the adverse effects of unreliable radar data during training and enhance the instance-level consistency in scene flow predictions from both modalities, particularly for dynamic foreground areas. Extensive experiments on the repurposed scene flow dataset demonstrate that our method outperforms existing LiDAR-based and radar-based single-modal methods by a significant margin.&lt;/p&gt;</content:encoded></item><item><title>Mul-VMamba: Multimodal semantic segmentation using selection-fusion-based Vision-Mamba</title><link>https://doi.org/10.1016/j.knosys.2025.115119</link><guid>10.1016/j.knosys.2025.115119</guid><pubDate>Sat, 13 Dec 2025 02:33:32 +0000</pubDate><dc:creator>Rongrong Ni</dc:creator><dc:creator>Yuanhui Guo</dc:creator><dc:creator>Biao Yang</dc:creator><dc:creator>Yi Liu</dc:creator><dc:creator>Hai Wang</dc:creator><dc:creator>Chuan Hu</dc:creator><prism:publicationName>Knowledge-Based Systems</prism:publicationName><prism:doi>10.1016/j.knosys.2025.115119</prism:doi><description>For tasks such as autonomous driving and remote sensing, integrating multimodal data (RGB, depth, infrared, and others) can significantly enhance the accuracy and robustness of semantic segmentation under complex environmental conditions, thereby providing precise and reliable information for downstream tasks. However, existing approaches emphasize segmentation accuracy at the expense of efficiency. To address this trade-off, we propose a multimodal semantic segmentation network based on the linear complexity Selective State Space Model (S6, a.k.a Mamba), dubbed Mul-VMamba. Mul-VMamba establishes selection-fusion relationships among multimodal features, enabling semantic segmentation with any input modalities. Specifically, the Mamba Spatial-consistency Selective Module (MSSM) adaptively extracts feature mapping relationships and filters out redundant features at identical spatial locations, preserving the spatial relationships between each modality. Additionally, the Mamba Cross-Fusion Module (MCFM) introduces a Cross Selective State Space Model (Cross-S6), establishing the relationship between S6 and multimodal features, achieving optimal fusion performance. Qualitative and quantitative evaluations on the MCubes and DeLiVER datasets demonstrate the efficacy and efficiency of Mul-VMamba. Notably, Mul-VMamba achieves 54.65% / 68.98% mIoU on Mcubes / DeLiVER datasets using only 55.33M params. The source code of Mul-VMamba is publicly available at https://github.com/Mask0913/Mul-VMamba .
Published: 2025-12-13T02:33:32+00:00
Venue: Knowledge-Based Systems
Score: 0.769 (must_read)</description><content:encoded>&lt;p&gt;&lt;strong&gt;Authors:&lt;/strong&gt; Rongrong Ni; Yuanhui Guo; Biao Yang; Yi Liu; Hai Wang; Chuan Hu&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Journal:&lt;/strong&gt; Knowledge-Based Systems&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;a href="https://doi.org/10.1016/j.knosys.2025.115119"&gt;10.1016/j.knosys.2025.115119&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Relevance:&lt;/strong&gt; 0.769 (must_read)&lt;/p&gt;
&lt;p&gt;For tasks such as autonomous driving and remote sensing, integrating multimodal data (RGB, depth, infrared, and others) can significantly enhance the accuracy and robustness of semantic segmentation under complex environmental conditions, thereby providing precise and reliable information for downstream tasks. However, existing approaches emphasize segmentation accuracy at the expense of efficiency. To address this trade-off, we propose a multimodal semantic segmentation network based on the linear complexity Selective State Space Model (S6, a.k.a Mamba), dubbed Mul-VMamba. Mul-VMamba establishes selection-fusion relationships among multimodal features, enabling semantic segmentation with any input modalities. Specifically, the Mamba Spatial-consistency Selective Module (MSSM) adaptively extracts feature mapping relationships and filters out redundant features at identical spatial locations, preserving the spatial relationships between each modality. Additionally, the Mamba Cross-Fusion Module (MCFM) introduces a Cross Selective State Space Model (Cross-S6), establishing the relationship between S6 and multimodal features, achieving optimal fusion performance. Qualitative and quantitative evaluations on the MCubes and DeLiVER datasets demonstrate the efficacy and efficiency of Mul-VMamba. Notably, Mul-VMamba achieves 54.65% / 68.98% mIoU on Mcubes / DeLiVER datasets using only 55.33M params. The source code of Mul-VMamba is publicly available at https://github.com/Mask0913/Mul-VMamba .&lt;/p&gt;</content:encoded></item><item><title>Parallel Consensus Transformer for Local Feature Matching</title><link>https://doi.org/10.1016/j.patcog.2025.112905</link><guid>10.1016/j.patcog.2025.112905</guid><pubDate>Sat, 13 Dec 2025 23:30:26 +0000</pubDate><dc:creator>Xiaoyong Lu</dc:creator><dc:creator>Yuhan Chen</dc:creator><dc:creator>Bin Kang</dc:creator><dc:creator>Songlin Du</dc:creator><prism:publicationName>Pattern Recognition</prism:publicationName><prism:doi>10.1016/j.patcog.2025.112905</prism:doi><description>Local feature matching establishes correspondences between two sets of image features, a fundamental yet challenging task in computer vision. Existing Transformer-based methods achieve strong global modeling but suffer from high computational costs and limited locality. We propose PCMatcher, a detector-based feature matching framework that leverages parallel consensus attention to address these issues. Parallel consensus attention integrates a local consensus module to incorporate neighborhood information and a parallel attention mechanism to reuse parameters and computations efficiently. Additionally, a multi-scale fusion module combines features from different layers to improve robustness. Extensive experiments indicate that PCMatcher achieves a competitive accuracy-efficiency trade-off across various downstream tasks. The source code will be publicly released upon acceptance.
Published: 2025-12-13T23:30:26+00:00
Venue: Pattern Recognition
Score: 0.769 (must_read)</description><content:encoded>&lt;p&gt;&lt;strong&gt;Authors:&lt;/strong&gt; Xiaoyong Lu; Yuhan Chen; Bin Kang; Songlin Du&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Journal:&lt;/strong&gt; Pattern Recognition&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;a href="https://doi.org/10.1016/j.patcog.2025.112905"&gt;10.1016/j.patcog.2025.112905&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Relevance:&lt;/strong&gt; 0.769 (must_read)&lt;/p&gt;
&lt;p&gt;Local feature matching establishes correspondences between two sets of image features, a fundamental yet challenging task in computer vision. Existing Transformer-based methods achieve strong global modeling but suffer from high computational costs and limited locality. We propose PCMatcher, a detector-based feature matching framework that leverages parallel consensus attention to address these issues. Parallel consensus attention integrates a local consensus module to incorporate neighborhood information and a parallel attention mechanism to reuse parameters and computations efficiently. Additionally, a multi-scale fusion module combines features from different layers to improve robustness. Extensive experiments indicate that PCMatcher achieves a competitive accuracy-efficiency trade-off across various downstream tasks. The source code will be publicly released upon acceptance.&lt;/p&gt;</content:encoded></item><item><title>Quadruplet-Attention Transformer for Scale-Invariant Robot Place Recognition</title><link>https://doi.org/10.1016/j.eswa.2025.130756</link><guid>10.1016/j.eswa.2025.130756</guid><pubDate>Sun, 14 Dec 2025 23:03:28 +0000</pubDate><dc:creator>Zhenyu Li</dc:creator><dc:creator>Pengjie Xu</dc:creator><prism:publicationName>Expert Systems with Applications</prism:publicationName><prism:doi>10.1016/j.eswa.2025.130756</prism:doi><description>Place recognition is a key task in robotics and artificial intelligence, especially for visual localization and navigation in difficult environments such as low-light or dynamic scenes. Many existing methods fail to capture reliable visual cues because of environmental changes and occlusions. To address this issue, we propose the Aggregated Quadruplet Pyramid Transformer (AQPT) for large-scale robot place recognition. AQPT employs a multi-scale attention mechanism to extract robust features at different resolutions. We further enhance these features with masked features, where parts of the image are intentionally hidden during training to simulate occlusions and improve resilience. The model is trained with a quadruplet loss, comparing an anchor with a positive match and two negatives, to achieve better feature separation and generalization. For efficient retrieval, we generate compact binary codes through hash coding and refine candidate matches using a Bayesian re-ranking module. Experiments on benchmark datasets and real-world scenarios show that AQPT outperforms existing methods, offering superior robustness and scalability. Our code is available at https://github.com/CV4RA/AQPT-VPR .
Published: 2025-12-14T23:03:28+00:00
Venue: Expert Systems with Applications
Score: 0.768 (must_read)</description><content:encoded>&lt;p&gt;&lt;strong&gt;Authors:&lt;/strong&gt; Zhenyu Li; Pengjie Xu&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Journal:&lt;/strong&gt; Expert Systems with Applications&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;a href="https://doi.org/10.1016/j.eswa.2025.130756"&gt;10.1016/j.eswa.2025.130756&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Relevance:&lt;/strong&gt; 0.768 (must_read)&lt;/p&gt;
&lt;p&gt;Place recognition is a key task in robotics and artificial intelligence, especially for visual localization and navigation in difficult environments such as low-light or dynamic scenes. Many existing methods fail to capture reliable visual cues because of environmental changes and occlusions. To address this issue, we propose the Aggregated Quadruplet Pyramid Transformer (AQPT) for large-scale robot place recognition. AQPT employs a multi-scale attention mechanism to extract robust features at different resolutions. We further enhance these features with masked features, where parts of the image are intentionally hidden during training to simulate occlusions and improve resilience. The model is trained with a quadruplet loss, comparing an anchor with a positive match and two negatives, to achieve better feature separation and generalization. For efficient retrieval, we generate compact binary codes through hash coding and refine candidate matches using a Bayesian re-ranking module. Experiments on benchmark datasets and real-world scenarios show that AQPT outperforms existing methods, offering superior robustness and scalability. Our code is available at https://github.com/CV4RA/AQPT-VPR .&lt;/p&gt;</content:encoded></item><item><title>VLM2GeoVec: Toward Universal Multimodal Embeddings for Remote Sensing</title><link>https://arxiv.org/abs/2512.11490v1</link><guid>http://arxiv.org/abs/2512.11490v1</guid><pubDate>Fri, 12 Dec 2025 11:39:35 +0000</pubDate><dc:creator>Emanuel Snchez Aimar</dc:creator><dc:creator>Gulnaz Zhambulova</dc:creator><dc:creator>Fahad Shahbaz Khan</dc:creator><dc:creator>Yonghao Xu</dc:creator><dc:creator>Michael Felsberg</dc:creator><dc:type>preprint</dc:type><prism:publicationName>arXiv</prism:publicationName><description>Satellite imagery differs fundamentally from natural images: its aerial viewpoint, very high resolution, diverse scale variations, and abundance of small objects demand both region-level spatial reasoning and holistic scene understanding. Current remote-sensing approaches remain fragmented between dual-encoder retrieval models, which excel at large-scale cross-modal search but cannot interleave modalities, and generative assistants, which support region-level interpretation but lack scalable retrieval capabilities. We propose $\textbf{VLM2GeoVec}$, an instruction-following, single-encoder vision-language model trained contrastively to embed interleaved inputs (images, text, bounding boxes, and geographic coordinates) in a unified vector space. Our single encoder interleaves all inputs into one joint embedding trained with a contrastive loss, eliminating multi-stage pipelines and task-specific modules. To evaluate its versatility, we introduce $\textbf{RSMEB}$, a novel benchmark covering key remote-sensing embedding applications: scene classification; cross-modal search; compositional retrieval; visual-question answering; visual grounding and region-level reasoning; and semantic geospatial retrieval. On RSMEB, it achieves $\textbf{26.6%}$ P@1 on region-caption retrieval (+25 pp vs. dual-encoder baselines), $\textbf{32.5%}$ P@1 on referring-expression retrieval (+19 pp), and $\textbf{17.8%}$ P@1 on semantic geo-localization retrieval (over $3\times$ prior best), while matching or exceeding specialized baselines on conventional tasks such as scene classification and cross-modal retrieval. VLM2GeoVec unifies scalable retrieval with region-level spatial reasoning, enabling cohesive multimodal analysis in remote sensing. We will publicly release the code, checkpoints, and data upon acceptance.
Published: 2025-12-12T11:39:35+00:00
Venue: arXiv
Score: 0.766 (must_read)</description><content:encoded>&lt;p&gt;&lt;strong&gt;Authors:&lt;/strong&gt; Emanuel Snchez Aimar; Gulnaz Zhambulova; Fahad Shahbaz Khan; Yonghao Xu; Michael Felsberg&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Journal:&lt;/strong&gt; arXiv&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Relevance:&lt;/strong&gt; 0.766 (must_read)&lt;/p&gt;
&lt;p&gt;Satellite imagery differs fundamentally from natural images: its aerial viewpoint, very high resolution, diverse scale variations, and abundance of small objects demand both region-level spatial reasoning and holistic scene understanding. Current remote-sensing approaches remain fragmented between dual-encoder retrieval models, which excel at large-scale cross-modal search but cannot interleave modalities, and generative assistants, which support region-level interpretation but lack scalable retrieval capabilities. We propose $\textbf{VLM2GeoVec}$, an instruction-following, single-encoder vision-language model trained contrastively to embed interleaved inputs (images, text, bounding boxes, and geographic coordinates) in a unified vector space. Our single encoder interleaves all inputs into one joint embedding trained with a contrastive loss, eliminating multi-stage pipelines and task-specific modules. To evaluate its versatility, we introduce $\textbf{RSMEB}$, a novel benchmark covering key remote-sensing embedding applications: scene classification; cross-modal search; compositional retrieval; visual-question answering; visual grounding and region-level reasoning; and semantic geospatial retrieval. On RSMEB, it achieves $\textbf{26.6%}$ P@1 on region-caption retrieval (+25 pp vs. dual-encoder baselines), $\textbf{32.5%}$ P@1 on referring-expression retrieval (+19 pp), and $\textbf{17.8%}$ P@1 on semantic geo-localization retrieval (over $3\times$ prior best), while matching or exceeding specialized baselines on conventional tasks such as scene classification and cross-modal retrieval. VLM2GeoVec unifies scalable retrieval with region-level spatial reasoning, enabling cohesive multimodal analysis in remote sensing. We will publicly release the code, checkpoints, and data upon acceptance.&lt;/p&gt;</content:encoded></item><item><title>Knowledge PET3D: An interpretable framework for 3D near-miss detection in thermal traffic video</title><link>https://doi.org/10.1016/j.eswa.2025.130821</link><guid>10.1016/j.eswa.2025.130821</guid><pubDate>Sat, 13 Dec 2025 02:34:06 +0000</pubDate><dc:creator>Arnd Pettirsch</dc:creator><dc:creator>Alvaro Garcia-Hernandez</dc:creator><prism:publicationName>Expert Systems with Applications</prism:publicationName><prism:doi>10.1016/j.eswa.2025.130821</prism:doi><description>Traffic safety engineers often rely on retrospective crash data, limiting their ability to proactively identify systemic risks in road environments. To address this gap, this work presents Knowledge PET3D, a novel, privacy-preserving framework that enables automatic long-term traffic observation, detects safety-relevant interactions, and delivers interpretable video snippets to support informed engineering decisions. The system integrates monocular 3D detection, maneuver-specific rule modeling, and transformer-based anomaly detection to identify near-miss events from thermal video. The system filters non-informative interactions based on rule compliance and behavioral response, enabling interpretable conflict sets suitable for manual review. Compared to conventional PET2D and PET3D baselines, Knowledge PET3D achieves over 4x more true positives and reduces false positives by up to 93%. It delivers high precision across varied urban contexts (22% at a complex signalized intersection and up to 75% at a simpler yield-controlled site), while keeping conflict volumes verifiable by humans. The framework further achieves 87.0% correct classified maneuvers, 92.9% clustering accuracy, and 96.5% correct interpreted traffic rules. Knowledge PET3D advances traffic safety diagnostics by uncovering latent risks while maintaining engineering interpretability and operational scalability.
Published: 2025-12-13T02:34:06+00:00
Venue: Expert Systems with Applications
Score: 0.765 (must_read)</description><content:encoded>&lt;p&gt;&lt;strong&gt;Authors:&lt;/strong&gt; Arnd Pettirsch; Alvaro Garcia-Hernandez&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Journal:&lt;/strong&gt; Expert Systems with Applications&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;a href="https://doi.org/10.1016/j.eswa.2025.130821"&gt;10.1016/j.eswa.2025.130821&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Relevance:&lt;/strong&gt; 0.765 (must_read)&lt;/p&gt;
&lt;p&gt;Traffic safety engineers often rely on retrospective crash data, limiting their ability to proactively identify systemic risks in road environments. To address this gap, this work presents Knowledge PET3D, a novel, privacy-preserving framework that enables automatic long-term traffic observation, detects safety-relevant interactions, and delivers interpretable video snippets to support informed engineering decisions. The system integrates monocular 3D detection, maneuver-specific rule modeling, and transformer-based anomaly detection to identify near-miss events from thermal video. The system filters non-informative interactions based on rule compliance and behavioral response, enabling interpretable conflict sets suitable for manual review. Compared to conventional PET2D and PET3D baselines, Knowledge PET3D achieves over 4x more true positives and reduces false positives by up to 93%. It delivers high precision across varied urban contexts (22% at a complex signalized intersection and up to 75% at a simpler yield-controlled site), while keeping conflict volumes verifiable by humans. The framework further achieves 87.0% correct classified maneuvers, 92.9% clustering accuracy, and 96.5% correct interpreted traffic rules. Knowledge PET3D advances traffic safety diagnostics by uncovering latent risks while maintaining engineering interpretability and operational scalability.&lt;/p&gt;</content:encoded></item><item><title>Assisted Refinement Network Based on Channel Information Interaction for Camouflaged and Salient Object Detection</title><link>https://arxiv.org/abs/2512.11369v1</link><guid>http://arxiv.org/abs/2512.11369v1</guid><pubDate>Fri, 12 Dec 2025 08:29:00 +0000</pubDate><dc:creator>Kuan Wang</dc:creator><dc:creator>Yanjun Qin</dc:creator><dc:creator>Mengge Lu</dc:creator><dc:creator>Liejun Wang</dc:creator><dc:creator>Xiaoming Tao</dc:creator><dc:type>preprint</dc:type><prism:publicationName>arXiv</prism:publicationName><description>Camouflaged Object Detection (COD) stands as a significant challenge in computer vision, dedicated to identifying and segmenting objects visually highly integrated with their backgrounds. Current mainstream methods have made progress in cross-layer feature fusion, but two critical issues persist during the decoding stage. The first is insufficient cross-channel information interaction within the same-layer features, limiting feature expressiveness. The second is the inability to effectively co-model boundary and region information, making it difficult to accurately reconstruct complete regions and sharp boundaries of objects. To address the first issue, we propose the Channel Information Interaction Module (CIIM), which introduces a horizontal-vertical integration mechanism in the channel dimension. This module performs feature reorganization and interaction across channels to effectively capture complementary cross-channel information. To address the second issue, we construct a collaborative decoding architecture guided by prior knowledge. This architecture generates boundary priors and object localization maps through Boundary Extraction (BE) and Region Extraction (RE) modules, then employs hybrid attention to collaboratively calibrate decoded features, effectively overcoming semantic ambiguity and imprecise boundaries. Additionally, the Multi-scale Enhancement (MSE) module enriches contextual feature representations. Extensive experiments on four COD benchmark datasets validate the effectiveness and state-of-the-art performance of the proposed model. We further transferred our model to the Salient Object Detection (SOD) task and demonstrated its adaptability across downstream tasks, including polyp segmentation, transparent object detection, and industrial and road defect detection. Code and experimental results are publicly available at: https://github.com/akuan1234/ARNet-v2.
Published: 2025-12-12T08:29:00+00:00
Venue: arXiv
Score: 0.765 (must_read)</description><content:encoded>&lt;p&gt;&lt;strong&gt;Authors:&lt;/strong&gt; Kuan Wang; Yanjun Qin; Mengge Lu; Liejun Wang; Xiaoming Tao&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Journal:&lt;/strong&gt; arXiv&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Relevance:&lt;/strong&gt; 0.765 (must_read)&lt;/p&gt;
&lt;p&gt;Camouflaged Object Detection (COD) stands as a significant challenge in computer vision, dedicated to identifying and segmenting objects visually highly integrated with their backgrounds. Current mainstream methods have made progress in cross-layer feature fusion, but two critical issues persist during the decoding stage. The first is insufficient cross-channel information interaction within the same-layer features, limiting feature expressiveness. The second is the inability to effectively co-model boundary and region information, making it difficult to accurately reconstruct complete regions and sharp boundaries of objects. To address the first issue, we propose the Channel Information Interaction Module (CIIM), which introduces a horizontal-vertical integration mechanism in the channel dimension. This module performs feature reorganization and interaction across channels to effectively capture complementary cross-channel information. To address the second issue, we construct a collaborative decoding architecture guided by prior knowledge. This architecture generates boundary priors and object localization maps through Boundary Extraction (BE) and Region Extraction (RE) modules, then employs hybrid attention to collaboratively calibrate decoded features, effectively overcoming semantic ambiguity and imprecise boundaries. Additionally, the Multi-scale Enhancement (MSE) module enriches contextual feature representations. Extensive experiments on four COD benchmark datasets validate the effectiveness and state-of-the-art performance of the proposed model. We further transferred our model to the Salient Object Detection (SOD) task and demonstrated its adaptability across downstream tasks, including polyp segmentation, transparent object detection, and industrial and road defect detection. Code and experimental results are publicly available at: https://github.com/akuan1234/ARNet-v2.&lt;/p&gt;</content:encoded></item><item><title>Point2Pose: A Generative Framework for 3D Human Pose Estimation with Multi-View Point Cloud Dataset</title><link>https://arxiv.org/abs/2512.10321v1</link><guid>http://arxiv.org/abs/2512.10321v1</guid><pubDate>Thu, 11 Dec 2025 06:11:24 +0000</pubDate><dc:creator>Hyunsoo Lee</dc:creator><dc:creator>Daeum Jeon</dc:creator><dc:creator>Hyeokjae Oh</dc:creator><dc:type>preprint</dc:type><prism:publicationName>arXiv</prism:publicationName><description>We propose a novel generative approach for 3D human pose estimation. 3D human pose estimation poses several key challenges due to the complex geometry of the human body, self-occluding joints, and the requirement for large-scale real-world motion datasets. To address these challenges, we introduce Point2Pose, a framework that effectively models the distribution of human poses conditioned on sequential point cloud and pose history. Specifically, we employ a spatio-temporal point cloud encoder and a pose feature encoder to extract joint-wise features, followed by an attention-based generative regressor. Additionally, we present a large-scale indoor dataset MVPose3D, which contains multiple modalities, including IMU data of non-trivial human motions, dense multi-view point clouds, and RGB images. Experimental results show that the proposed method outperforms the baseline models, demonstrating its superior performance across various datasets.
Published: 2025-12-11T06:11:24+00:00
Venue: arXiv
Score: 0.764 (must_read)</description><content:encoded>&lt;p&gt;&lt;strong&gt;Authors:&lt;/strong&gt; Hyunsoo Lee; Daeum Jeon; Hyeokjae Oh&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Journal:&lt;/strong&gt; arXiv&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Relevance:&lt;/strong&gt; 0.764 (must_read)&lt;/p&gt;
&lt;p&gt;We propose a novel generative approach for 3D human pose estimation. 3D human pose estimation poses several key challenges due to the complex geometry of the human body, self-occluding joints, and the requirement for large-scale real-world motion datasets. To address these challenges, we introduce Point2Pose, a framework that effectively models the distribution of human poses conditioned on sequential point cloud and pose history. Specifically, we employ a spatio-temporal point cloud encoder and a pose feature encoder to extract joint-wise features, followed by an attention-based generative regressor. Additionally, we present a large-scale indoor dataset MVPose3D, which contains multiple modalities, including IMU data of non-trivial human motions, dense multi-view point clouds, and RGB images. Experimental results show that the proposed method outperforms the baseline models, demonstrating its superior performance across various datasets.&lt;/p&gt;</content:encoded></item><item><title>Learning from a Generative Oracle: Domain Adaptation for Restoration</title><link>https://arxiv.org/abs/2512.11121v1</link><guid>http://arxiv.org/abs/2512.11121v1</guid><pubDate>Thu, 11 Dec 2025 21:04:29 +0000</pubDate><dc:creator>Yuyang Hu</dc:creator><dc:creator>Mojtaba Sahraee-Ardakan</dc:creator><dc:creator>Arpit Bansal</dc:creator><dc:creator>Kangfu Mei</dc:creator><dc:creator>Christian Qi</dc:creator><dc:creator>Peyman Milanfar</dc:creator><dc:creator>Mauricio Delbracio</dc:creator><dc:type>preprint</dc:type><prism:publicationName>arXiv</prism:publicationName><description>Pre-trained image restoration models often fail on real-world, out-of-distribution degradations due to significant domain gaps. Adapting to these unseen domains is challenging, as out-of-distribution data lacks ground truth, and traditional adaptation methods often require complex architectural changes. We propose LEGO (Learning from a Generative Oracle), a practical three-stage framework for post-training domain adaptation without paired data. LEGO converts this unsupervised challenge into a tractable pseudo-supervised one. First, we obtain initial restorations from the pre-trained model. Second, we leverage a frozen, large-scale generative oracle to refine these estimates into high-quality pseudo-ground-truths. Third, we fine-tune the original model using a mixed-supervision strategy combining in-distribution data with these new pseudo-pairs. This approach adapts the model to the new distribution without sacrificing its original robustness or requiring architectural modifications. Experiments demonstrate that LEGO effectively bridges the domain gap, significantly improving performance on diverse real-world benchmarks.
Published: 2025-12-11T21:04:29+00:00
Venue: arXiv
Score: 0.764 (must_read)</description><content:encoded>&lt;p&gt;&lt;strong&gt;Authors:&lt;/strong&gt; Yuyang Hu; Mojtaba Sahraee-Ardakan; Arpit Bansal; Kangfu Mei; Christian Qi; Peyman Milanfar; Mauricio Delbracio&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Journal:&lt;/strong&gt; arXiv&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Relevance:&lt;/strong&gt; 0.764 (must_read)&lt;/p&gt;
&lt;p&gt;Pre-trained image restoration models often fail on real-world, out-of-distribution degradations due to significant domain gaps. Adapting to these unseen domains is challenging, as out-of-distribution data lacks ground truth, and traditional adaptation methods often require complex architectural changes. We propose LEGO (Learning from a Generative Oracle), a practical three-stage framework for post-training domain adaptation without paired data. LEGO converts this unsupervised challenge into a tractable pseudo-supervised one. First, we obtain initial restorations from the pre-trained model. Second, we leverage a frozen, large-scale generative oracle to refine these estimates into high-quality pseudo-ground-truths. Third, we fine-tune the original model using a mixed-supervision strategy combining in-distribution data with these new pseudo-pairs. This approach adapts the model to the new distribution without sacrificing its original robustness or requiring architectural modifications. Experiments demonstrate that LEGO effectively bridges the domain gap, significantly improving performance on diverse real-world benchmarks.&lt;/p&gt;</content:encoded></item><item><title>E-RayZer: Self-supervised 3D Reconstruction as Spatial Visual Pre-training</title><link>https://arxiv.org/abs/2512.10950v1</link><guid>http://arxiv.org/abs/2512.10950v1</guid><pubDate>Thu, 11 Dec 2025 18:59:53 +0000</pubDate><dc:creator>Qitao Zhao</dc:creator><dc:creator>Hao Tan</dc:creator><dc:creator>Qianqian Wang</dc:creator><dc:creator>Sai Bi</dc:creator><dc:creator>Kai Zhang</dc:creator><dc:creator>Kalyan Sunkavalli</dc:creator><dc:creator>Shubham Tulsiani</dc:creator><dc:creator>Hanwen Jiang</dc:creator><dc:type>preprint</dc:type><prism:publicationName>arXiv</prism:publicationName><description>Self-supervised pre-training has revolutionized foundation models for languages, individual 2D images and videos, but remains largely unexplored for learning 3D-aware representations from multi-view images. In this paper, we present E-RayZer, a self-supervised large 3D Vision model that learns truly 3D-aware representations directly from unlabeled images. Unlike prior self-supervised methods such as RayZer that infer 3D indirectly through latent-space view synthesis, E-RayZer operates directly in 3D space, performing self-supervised 3D reconstruction with Explicit geometry. This formulation eliminates shortcut solutions and yields representations that are geometrically grounded. To ensure convergence and scalability, we introduce a novel fine-grained learning curriculum that organizes training from easy to hard samples and harmonizes heterogeneous data sources in an entirely unsupervised manner. Experiments demonstrate that E-RayZer significantly outperforms RayZer on pose estimation, matches or sometimes surpasses fully supervised reconstruction models such as VGGT. Furthermore, its learned representations outperform leading visual pre-training models (e.g., DINOv3, CroCo v2, VideoMAE V2, and RayZer) when transferring to 3D downstream tasks, establishing E-RayZer as a new paradigm for 3D-aware visual pre-training.
Published: 2025-12-11T18:59:53+00:00
Venue: arXiv
Score: 0.764 (must_read)</description><content:encoded>&lt;p&gt;&lt;strong&gt;Authors:&lt;/strong&gt; Qitao Zhao; Hao Tan; Qianqian Wang; Sai Bi; Kai Zhang; Kalyan Sunkavalli; Shubham Tulsiani; Hanwen Jiang&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Journal:&lt;/strong&gt; arXiv&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Relevance:&lt;/strong&gt; 0.764 (must_read)&lt;/p&gt;
&lt;p&gt;Self-supervised pre-training has revolutionized foundation models for languages, individual 2D images and videos, but remains largely unexplored for learning 3D-aware representations from multi-view images. In this paper, we present E-RayZer, a self-supervised large 3D Vision model that learns truly 3D-aware representations directly from unlabeled images. Unlike prior self-supervised methods such as RayZer that infer 3D indirectly through latent-space view synthesis, E-RayZer operates directly in 3D space, performing self-supervised 3D reconstruction with Explicit geometry. This formulation eliminates shortcut solutions and yields representations that are geometrically grounded. To ensure convergence and scalability, we introduce a novel fine-grained learning curriculum that organizes training from easy to hard samples and harmonizes heterogeneous data sources in an entirely unsupervised manner. Experiments demonstrate that E-RayZer significantly outperforms RayZer on pose estimation, matches or sometimes surpasses fully supervised reconstruction models such as VGGT. Furthermore, its learned representations outperform leading visual pre-training models (e.g., DINOv3, CroCo v2, VideoMAE V2, and RayZer) when transferring to 3D downstream tasks, establishing E-RayZer as a new paradigm for 3D-aware visual pre-training.&lt;/p&gt;</content:encoded></item><item><title>Beyond Pixels: A Training-Free, Text-to-Text Framework for Remote Sensing Image Retrieval</title><link>https://arxiv.org/abs/2512.10596v1</link><guid>http://arxiv.org/abs/2512.10596v1</guid><pubDate>Thu, 11 Dec 2025 12:43:41 +0000</pubDate><dc:creator>J. Xiao</dc:creator><dc:creator>Y. Guo</dc:creator><dc:creator>X. Zi</dc:creator><dc:creator>K. Thiyagarajan</dc:creator><dc:creator>C. Moreira</dc:creator><dc:creator>M. Prasad</dc:creator><dc:type>preprint</dc:type><prism:publicationName>arXiv</prism:publicationName><description>Semantic retrieval of remote sensing (RS) images is a critical task fundamentally challenged by the \textquote{semantic gap}, the discrepancy between a model's low-level visual features and high-level human concepts. While large Vision-Language Models (VLMs) offer a promising path to bridge this gap, existing methods often rely on costly, domain-specific training, and there is a lack of benchmarks to evaluate the practical utility of VLM-generated text in a zero-shot retrieval context. To address this research gap, we introduce the Remote Sensing Rich Text (RSRT) dataset, a new benchmark featuring multiple structured captions per image. Based on this dataset, we propose a fully training-free, text-only retrieval reference called TRSLLaVA. Our methodology reformulates cross-modal retrieval as a text-to-text (T2T) matching problem, leveraging rich text descriptions as queries against a database of VLM-generated captions within a unified textual embedding space. This approach completely bypasses model training or fine-tuning. Experiments on the RSITMD and RSICD benchmarks show our training-free method is highly competitive with state-of-the-art supervised models. For instance, on RSITMD, our method achieves a mean Recall of 42.62\%, nearly doubling the 23.86\% of the standard zero-shot CLIP baseline and surpassing several top supervised models. This validates that high-quality semantic representation through structured text provides a powerful and cost-effective paradigm for remote sensing image retrieval.
Published: 2025-12-11T12:43:41+00:00
Venue: arXiv
Score: 0.763 (consider)</description><content:encoded>&lt;p&gt;&lt;strong&gt;Authors:&lt;/strong&gt; J. Xiao; Y. Guo; X. Zi; K. Thiyagarajan; C. Moreira; M. Prasad&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Journal:&lt;/strong&gt; arXiv&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Relevance:&lt;/strong&gt; 0.763 (consider)&lt;/p&gt;
&lt;p&gt;Semantic retrieval of remote sensing (RS) images is a critical task fundamentally challenged by the \textquote{semantic gap}, the discrepancy between a model&amp;#x27;s low-level visual features and high-level human concepts. While large Vision-Language Models (VLMs) offer a promising path to bridge this gap, existing methods often rely on costly, domain-specific training, and there is a lack of benchmarks to evaluate the practical utility of VLM-generated text in a zero-shot retrieval context. To address this research gap, we introduce the Remote Sensing Rich Text (RSRT) dataset, a new benchmark featuring multiple structured captions per image. Based on this dataset, we propose a fully training-free, text-only retrieval reference called TRSLLaVA. Our methodology reformulates cross-modal retrieval as a text-to-text (T2T) matching problem, leveraging rich text descriptions as queries against a database of VLM-generated captions within a unified textual embedding space. This approach completely bypasses model training or fine-tuning. Experiments on the RSITMD and RSICD benchmarks show our training-free method is highly competitive with state-of-the-art supervised models. For instance, on RSITMD, our method achieves a mean Recall of 42.62\%, nearly doubling the 23.86\% of the standard zero-shot CLIP baseline and surpassing several top supervised models. This validates that high-quality semantic representation through structured text provides a powerful and cost-effective paradigm for remote sensing image retrieval.&lt;/p&gt;</content:encoded></item><item><title>Stronger Normalization-Free Transformers</title><link>https://arxiv.org/abs/2512.10938v1</link><guid>http://arxiv.org/abs/2512.10938v1</guid><pubDate>Thu, 11 Dec 2025 18:58:49 +0000</pubDate><dc:creator>Mingzhi Chen</dc:creator><dc:creator>Taiming Lu</dc:creator><dc:creator>Jiachen Zhu</dc:creator><dc:creator>Mingjie Sun</dc:creator><dc:creator>Zhuang Liu</dc:creator><dc:type>preprint</dc:type><prism:publicationName>arXiv</prism:publicationName><description>Although normalization layers have long been viewed as indispensable components of deep learning architectures, the recent introduction of Dynamic Tanh (DyT) has demonstrated that alternatives are possible. The point-wise function DyT constrains extreme values for stable convergence and reaches normalization-level performance; this work seeks further for function designs that can surpass it. We first study how the intrinsic properties of point-wise functions influence training and performance. Building on these findings, we conduct a large-scale search for a more effective function design. Through this exploration, we introduce $\mathrm{Derf}(x) = \mathrm{erf}(x + s)$, where $\mathrm{erf}(x)$ is the rescaled Gaussian cumulative distribution function, and identify it as the most performant design. Derf outperforms LayerNorm, RMSNorm, and DyT across a wide range of domains, including vision (image recognition and generation), speech representation, and DNA sequence modeling. Our findings suggest that the performance gains of Derf largely stem from its improved generalization rather than stronger fitting capacity. Its simplicity and stronger performance make Derf a practical choice for normalization-free Transformer architectures.
Published: 2025-12-11T18:58:49+00:00
Venue: arXiv
Score: 0.763 (consider)</description><content:encoded>&lt;p&gt;&lt;strong&gt;Authors:&lt;/strong&gt; Mingzhi Chen; Taiming Lu; Jiachen Zhu; Mingjie Sun; Zhuang Liu&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Journal:&lt;/strong&gt; arXiv&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Relevance:&lt;/strong&gt; 0.763 (consider)&lt;/p&gt;
&lt;p&gt;Although normalization layers have long been viewed as indispensable components of deep learning architectures, the recent introduction of Dynamic Tanh (DyT) has demonstrated that alternatives are possible. The point-wise function DyT constrains extreme values for stable convergence and reaches normalization-level performance; this work seeks further for function designs that can surpass it. We first study how the intrinsic properties of point-wise functions influence training and performance. Building on these findings, we conduct a large-scale search for a more effective function design. Through this exploration, we introduce $\mathrm{Derf}(x) = \mathrm{erf}(x + s)$, where $\mathrm{erf}(x)$ is the rescaled Gaussian cumulative distribution function, and identify it as the most performant design. Derf outperforms LayerNorm, RMSNorm, and DyT across a wide range of domains, including vision (image recognition and generation), speech representation, and DNA sequence modeling. Our findings suggest that the performance gains of Derf largely stem from its improved generalization rather than stronger fitting capacity. Its simplicity and stronger performance make Derf a practical choice for normalization-free Transformer architectures.&lt;/p&gt;</content:encoded></item><item><title>Blink: Dynamic Visual Token Resolution for Enhanced Multimodal Understanding</title><link>https://arxiv.org/abs/2512.10548v1</link><guid>http://arxiv.org/abs/2512.10548v1</guid><pubDate>Thu, 11 Dec 2025 11:27:25 +0000</pubDate><dc:creator>Yuchen Feng</dc:creator><dc:creator>Zhenyu Zhang</dc:creator><dc:creator>Naibin Gu</dc:creator><dc:creator>Yilong Chen</dc:creator><dc:creator>Peng Fu</dc:creator><dc:creator>Zheng Lin</dc:creator><dc:creator>Shuohuan Wang</dc:creator><dc:creator>Yu Sun</dc:creator><dc:creator>Hua Wu</dc:creator><dc:creator>Weiping Wang</dc:creator><dc:creator>Haifeng Wang</dc:creator><dc:type>preprint</dc:type><prism:publicationName>arXiv</prism:publicationName><description>Multimodal large language models (MLLMs) have achieved remarkable progress on various vision-language tasks, yet their visual perception remains limited. Humans, in comparison, perceive complex scenes efficiently by dynamically scanning and focusing on salient regions in a sequential "blink-like" process. Motivated by this strategy, we first investigate whether MLLMs exhibit similar behavior. Our pilot analysis reveals that MLLMs naturally attend to different visual regions across layers and that selectively allocating more computation to salient tokens can enhance visual perception. Building on this insight, we propose Blink, a dynamic visual token resolution framework that emulates the human-inspired process within a single forward pass. Specifically, Blink includes two modules: saliency-guided scanning and dynamic token resolution. It first estimates the saliency of visual tokens in each layer based on the attention map, and extends important tokens through a plug-and-play token super-resolution (TokenSR) module. In the next layer, it drops the extended tokens when they lose focus. This dynamic mechanism balances broad exploration and fine-grained focus, thereby enhancing visual perception adaptively and efficiently. Extensive experiments validate Blink, demonstrating its effectiveness in enhancing visual perception and multimodal understanding.
Published: 2025-12-11T11:27:25+00:00
Venue: arXiv
Score: 0.762 (consider)</description><content:encoded>&lt;p&gt;&lt;strong&gt;Authors:&lt;/strong&gt; Yuchen Feng; Zhenyu Zhang; Naibin Gu; Yilong Chen; Peng Fu; Zheng Lin; Shuohuan Wang; Yu Sun; Hua Wu; Weiping Wang; Haifeng Wang&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Journal:&lt;/strong&gt; arXiv&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Relevance:&lt;/strong&gt; 0.762 (consider)&lt;/p&gt;
&lt;p&gt;Multimodal large language models (MLLMs) have achieved remarkable progress on various vision-language tasks, yet their visual perception remains limited. Humans, in comparison, perceive complex scenes efficiently by dynamically scanning and focusing on salient regions in a sequential &amp;quot;blink-like&amp;quot; process. Motivated by this strategy, we first investigate whether MLLMs exhibit similar behavior. Our pilot analysis reveals that MLLMs naturally attend to different visual regions across layers and that selectively allocating more computation to salient tokens can enhance visual perception. Building on this insight, we propose Blink, a dynamic visual token resolution framework that emulates the human-inspired process within a single forward pass. Specifically, Blink includes two modules: saliency-guided scanning and dynamic token resolution. It first estimates the saliency of visual tokens in each layer based on the attention map, and extends important tokens through a plug-and-play token super-resolution (TokenSR) module. In the next layer, it drops the extended tokens when they lose focus. This dynamic mechanism balances broad exploration and fine-grained focus, thereby enhancing visual perception adaptively and efficiently. Extensive experiments validate Blink, demonstrating its effectiveness in enhancing visual perception and multimodal understanding.&lt;/p&gt;</content:encoded></item><item><title>Towards Efficient and Effective Multi-Camera Encoding for End-to-End Driving</title><link>https://arxiv.org/abs/2512.10947v1</link><guid>http://arxiv.org/abs/2512.10947v1</guid><pubDate>Thu, 11 Dec 2025 18:59:46 +0000</pubDate><dc:creator>Jiawei Yang</dc:creator><dc:creator>Ziyu Chen</dc:creator><dc:creator>Yurong You</dc:creator><dc:creator>Yan Wang</dc:creator><dc:creator>Yiming Li</dc:creator><dc:creator>Yuxiao Chen</dc:creator><dc:creator>Boyi Li</dc:creator><dc:creator>Boris Ivanovic</dc:creator><dc:creator>Marco Pavone</dc:creator><dc:creator>Yue Wang</dc:creator><dc:type>preprint</dc:type><prism:publicationName>arXiv</prism:publicationName><description>We present Flex, an efficient and effective scene encoder that addresses the computational bottleneck of processing high-volume multi-camera data in end-to-end autonomous driving. Flex employs a small set of learnable scene tokens to jointly encode information from all image tokens across different cameras and timesteps. By design, our approach is geometry-agnostic, learning a compact scene representation directly from data without relying on the explicit 3D inductive biases, such as Bird-Eye-View (BEV), occupancy or tri-plane representations, which are common in prior work. This holistic encoding strategy aggressively compresses the visual input for the downstream Large Language Model (LLM) based policy model. Evaluated on a large-scale proprietary dataset of 20,000 driving hours, our Flex achieves 2.2x greater inference throughput while improving driving performance by a large margin compared to state-of-the-art methods. Furthermore, we show that these compact scene tokens develop an emergent capability for scene decomposition without any explicit supervision. Our findings challenge the prevailing assumption that 3D priors are necessary, demonstrating that a data-driven, joint encoding strategy offers a more scalable, efficient and effective path for future autonomous driving systems.
Published: 2025-12-11T18:59:46+00:00
Venue: arXiv
Score: 0.761 (consider)</description><content:encoded>&lt;p&gt;&lt;strong&gt;Authors:&lt;/strong&gt; Jiawei Yang; Ziyu Chen; Yurong You; Yan Wang; Yiming Li; Yuxiao Chen; Boyi Li; Boris Ivanovic; Marco Pavone; Yue Wang&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Journal:&lt;/strong&gt; arXiv&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Relevance:&lt;/strong&gt; 0.761 (consider)&lt;/p&gt;
&lt;p&gt;We present Flex, an efficient and effective scene encoder that addresses the computational bottleneck of processing high-volume multi-camera data in end-to-end autonomous driving. Flex employs a small set of learnable scene tokens to jointly encode information from all image tokens across different cameras and timesteps. By design, our approach is geometry-agnostic, learning a compact scene representation directly from data without relying on the explicit 3D inductive biases, such as Bird-Eye-View (BEV), occupancy or tri-plane representations, which are common in prior work. This holistic encoding strategy aggressively compresses the visual input for the downstream Large Language Model (LLM) based policy model. Evaluated on a large-scale proprietary dataset of 20,000 driving hours, our Flex achieves 2.2x greater inference throughput while improving driving performance by a large margin compared to state-of-the-art methods. Furthermore, we show that these compact scene tokens develop an emergent capability for scene decomposition without any explicit supervision. Our findings challenge the prevailing assumption that 3D priors are necessary, demonstrating that a data-driven, joint encoding strategy offers a more scalable, efficient and effective path for future autonomous driving systems.&lt;/p&gt;</content:encoded></item><item><title>Task-Aware Multi-Expert Architecture For Lifelong Deep Learning</title><link>https://arxiv.org/abs/2512.11243v1</link><guid>http://arxiv.org/abs/2512.11243v1</guid><pubDate>Fri, 12 Dec 2025 03:05:26 +0000</pubDate><dc:creator>Jianyu Wang</dc:creator><dc:creator>Jacob Nean-Hua Sheikh</dc:creator><dc:creator>Cat P. Le</dc:creator><dc:creator>Hoda Bidkhori</dc:creator><dc:type>preprint</dc:type><prism:publicationName>arXiv</prism:publicationName><description>Lifelong deep learning (LDL) trains neural networks to learn sequentially across tasks while preserving prior knowledge. We propose Task-Aware Multi-Expert (TAME), a continual learning algorithm that leverages task similarity to guide expert selection and knowledge transfer. TAME maintains a pool of pretrained neural networks and activates the most relevant expert for each new task. A shared dense layer integrates features from the chosen expert to generate predictions. To reduce catastrophic forgetting, TAME uses a replay buffer that stores representative samples and embeddings from previous tasks and reuses them during training. An attention mechanism further prioritizes the most relevant stored information for each prediction. Together, these components allow TAME to adapt flexibly while retaining important knowledge across evolving task sequences. Experiments on binary classification tasks derived from CIFAR-100 show that TAME improves accuracy on new tasks while sustaining performance on earlier ones, highlighting its effectiveness in balancing adaptation and retention in lifelong learning settings.
Published: 2025-12-12T03:05:26+00:00
Venue: arXiv
Score: 0.761 (consider)</description><content:encoded>&lt;p&gt;&lt;strong&gt;Authors:&lt;/strong&gt; Jianyu Wang; Jacob Nean-Hua Sheikh; Cat P. Le; Hoda Bidkhori&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Journal:&lt;/strong&gt; arXiv&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Relevance:&lt;/strong&gt; 0.761 (consider)&lt;/p&gt;
&lt;p&gt;Lifelong deep learning (LDL) trains neural networks to learn sequentially across tasks while preserving prior knowledge. We propose Task-Aware Multi-Expert (TAME), a continual learning algorithm that leverages task similarity to guide expert selection and knowledge transfer. TAME maintains a pool of pretrained neural networks and activates the most relevant expert for each new task. A shared dense layer integrates features from the chosen expert to generate predictions. To reduce catastrophic forgetting, TAME uses a replay buffer that stores representative samples and embeddings from previous tasks and reuses them during training. An attention mechanism further prioritizes the most relevant stored information for each prediction. Together, these components allow TAME to adapt flexibly while retaining important knowledge across evolving task sequences. Experiments on binary classification tasks derived from CIFAR-100 show that TAME improves accuracy on new tasks while sustaining performance on earlier ones, highlighting its effectiveness in balancing adaptation and retention in lifelong learning settings.&lt;/p&gt;</content:encoded></item></channel></rss>