<?xml version='1.0' encoding='utf-8'?>
<rss xmlns:content="http://purl.org/rss/1.0/modules/content/" xmlns:dc="http://purl.org/dc/elements/1.1/" xmlns:prism="http://prismstandard.org/namespaces/basic/3.0/" version="2.0"><channel><title>ZotWatch Feed</title><link>https://ehehe.cn/zotwatch/</link><description>AI-assisted literature watch</description><lastBuildDate>Tue, 10 Feb 2026 03:48:48 +0000</lastBuildDate><item><title>SSD-YOLOv12: an improved YOLOv12n model for ship detection using SAR data</title><link>https://doi.org/10.1007/s00521-025-11735-z</link><guid>10.1007/s00521-025-11735-z</guid><pubDate>Mon, 09 Feb 2026 05:25:13 +0000</pubDate><dc:creator>Phat T. Nguyen</dc:creator><dc:creator>Linh V. Cao</dc:creator><prism:publicationName>Neural Computing and Applications</prism:publicationName><prism:doi>10.1007/s00521-025-11735-z</prism:doi><description>Ship detection in Synthetic Aperture Radar (SAR) imagery plays a critical role in maritime surveillance applications, ensuring security and defense, and the management of territorial waters. This task, however, remains challenging due to the complex characteristics of SAR data, including strong background noise, side-lobe effects, and ambiguous target signals. In this context, deep learning methods, particularly real-time object detection architectures like You Only Look Once (YOLO), have shown considerable potential. Nevertheless, their effectiveness on SAR imagery is still limited by suboptimal feature extraction and performance reduction in high-noise environments. This paper proposes an improved version of the YOLOv12n architecture, which integrates an M-MBConvBlock module (an enhanced variant of the MBConvBlock from EfficientNet) into the backbone to enhance representational capacity and adapt to SAR images. Additionally, the loss function is refined by replacing the Complete Intersection over Union (CIoU) with an I-ShapeIoU (improved Shape Intersection over Union) to optimize localization accuracy. Empirical validation demonstrates that the proposed architecture achieves a compelling accuracy of 90.1% mAP@0.5 while maintaining exceptional computational efficiency. Crucially, SSD-YOLOv12 accomplishes this with a mere 1.16 million parameters and a compact 2.8 MB memory footprint, a substantial reduction compared to contemporary YOLO variants such as YOLOv8n (3.01M parameters, 6.3 MB) and the YOLOv12n baseline (2.56M parameters, 5.5 MB). This synergy between high precision and model compactness validates its suitability for real-time ship detection in SAR imagery, particularly on resource-constrained platforms.
Published: 2026-02-09T05:25:13+00:00
Venue: Neural Computing and Applications
Score: 0.829 (must_read)</description><content:encoded>&lt;p&gt;&lt;strong&gt;Authors:&lt;/strong&gt; Phat T. Nguyen; Linh V. Cao&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Journal:&lt;/strong&gt; Neural Computing and Applications&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;a href="https://doi.org/10.1007/s00521-025-11735-z"&gt;10.1007/s00521-025-11735-z&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Relevance:&lt;/strong&gt; 0.829 (must_read)&lt;/p&gt;
&lt;p&gt;Ship detection in Synthetic Aperture Radar (SAR) imagery plays a critical role in maritime surveillance applications, ensuring security and defense, and the management of territorial waters. This task, however, remains challenging due to the complex characteristics of SAR data, including strong background noise, side-lobe effects, and ambiguous target signals. In this context, deep learning methods, particularly real-time object detection architectures like You Only Look Once (YOLO), have shown considerable potential. Nevertheless, their effectiveness on SAR imagery is still limited by suboptimal feature extraction and performance reduction in high-noise environments. This paper proposes an improved version of the YOLOv12n architecture, which integrates an M-MBConvBlock module (an enhanced variant of the MBConvBlock from EfficientNet) into the backbone to enhance representational capacity and adapt to SAR images. Additionally, the loss function is refined by replacing the Complete Intersection over Union (CIoU) with an I-ShapeIoU (improved Shape Intersection over Union) to optimize localization accuracy. Empirical validation demonstrates that the proposed architecture achieves a compelling accuracy of 90.1% mAP@0.5 while maintaining exceptional computational efficiency. Crucially, SSD-YOLOv12 accomplishes this with a mere 1.16 million parameters and a compact 2.8 MB memory footprint, a substantial reduction compared to contemporary YOLO variants such as YOLOv8n (3.01M parameters, 6.3 MB) and the YOLOv12n baseline (2.56M parameters, 5.5 MB). This synergy between high precision and model compactness validates its suitability for real-time ship detection in SAR imagery, particularly on resource-constrained platforms.&lt;/p&gt;</content:encoded></item><item><title>Explainable Visual Question Answering: A Survey on Methods, Datasets and Evaluation</title><link>https://doi.org/10.1016/j.inffus.2026.104215</link><guid>10.1016/j.inffus.2026.104215</guid><pubDate>Sun, 08 Feb 2026 22:55:25 +0000</pubDate><dc:creator>Yaxian Wang</dc:creator><dc:creator>Qikan Lin</dc:creator><dc:creator>Jiangbo Shi</dc:creator><dc:creator>Yisheng An</dc:creator><dc:creator>Jun Liu</dc:creator><dc:creator>Bifan Wei</dc:creator><dc:creator>Xudong Jiang</dc:creator><prism:publicationName>Information Fusion</prism:publicationName><prism:doi>10.1016/j.inffus.2026.104215</prism:doi><description>In recent years, visual question answering has become a significant task at the intersection of computer vision and natural language processing, requiring models to jointly understand images and textual queries. It has emerged as a popular benchmark for evaluating multimodal understanding and reasoning. With advancements in VQA accuracy, there is a growing demand for explainability and transparency for VQA models, which is crucial for improving their trust and applicability in critical domains. This survey explores the emerging field of e X plainable V isual Q uestion A nswering (XVQA), which aims not only to provide the correct answer but also to generate meaningful explanations that justify the predicted answers. Firstly, we systematically review existing methods on XVQA, and propose a three-level taxonomy to organize them. The proposed taxonomy primarily categorizes XVQA methods based on the timing of the rationale generation and the forms of the rationales. Secondly, we review the existing VQA datasets annotated with explanations in different forms, including textual, visual and multimodal rationales. Furthermore, we summarize the evaluation metrics of XVQA for different forms of rationales. Finally, we outline the challenges for XVQA and discuss potential future directions. We aim to organize existing research in this domain and inspire future investigations into the explainability of VQA models.
Published: 2026-02-08T22:55:25+00:00
Venue: Information Fusion
Score: 0.803 (must_read)</description><content:encoded>&lt;p&gt;&lt;strong&gt;Authors:&lt;/strong&gt; Yaxian Wang; Qikan Lin; Jiangbo Shi; Yisheng An; Jun Liu; Bifan Wei; Xudong Jiang&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Journal:&lt;/strong&gt; Information Fusion&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;a href="https://doi.org/10.1016/j.inffus.2026.104215"&gt;10.1016/j.inffus.2026.104215&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Relevance:&lt;/strong&gt; 0.803 (must_read)&lt;/p&gt;
&lt;p&gt;In recent years, visual question answering has become a significant task at the intersection of computer vision and natural language processing, requiring models to jointly understand images and textual queries. It has emerged as a popular benchmark for evaluating multimodal understanding and reasoning. With advancements in VQA accuracy, there is a growing demand for explainability and transparency for VQA models, which is crucial for improving their trust and applicability in critical domains. This survey explores the emerging field of e X plainable V isual Q uestion A nswering (XVQA), which aims not only to provide the correct answer but also to generate meaningful explanations that justify the predicted answers. Firstly, we systematically review existing methods on XVQA, and propose a three-level taxonomy to organize them. The proposed taxonomy primarily categorizes XVQA methods based on the timing of the rationale generation and the forms of the rationales. Secondly, we review the existing VQA datasets annotated with explanations in different forms, including textual, visual and multimodal rationales. Furthermore, we summarize the evaluation metrics of XVQA for different forms of rationales. Finally, we outline the challenges for XVQA and discuss potential future directions. We aim to organize existing research in this domain and inspire future investigations into the explainability of VQA models.&lt;/p&gt;</content:encoded></item><item><title>Multi-Masking Strategies for Self-Supervised Low- and High-Level Text Representation Learning</title><link>https://doi.org/10.1016/j.patcog.2026.113273</link><guid>10.1016/j.patcog.2026.113273</guid><pubDate>Mon, 09 Feb 2026 08:03:22 +0000</pubDate><dc:creator>Zhengmi Tang</dc:creator><dc:creator>Yuto Mitsui</dc:creator><dc:creator>Tomo Miyazaki</dc:creator><dc:creator>Shinichiro Omachi</dc:creator><prism:publicationName>Pattern Recognition</prism:publicationName><prism:doi>10.1016/j.patcog.2026.113273</prism:doi><description>Most existing text recognition methods are trained on large-scale synthetic datasets due to the scarcity of labeled real-world datasets. Synthetic images, however, cannot faithfully reproduce real-world scenarios, such as uneven illumination, irregular layout, occlusion, and degradation, resulting in performance disparities when handling complex real-world images. Recent self-supervised learning techniques, notably contrastive learning and masked image modeling (MIM), narrow this domain gap by exploiting unlabeled real text images. This study first analyzes the original Masked AutoEncoder (MAE) and observes that random patch masking predominantly captures low-level textural features but misses high-level contextual representations. To fully exploit the high-level contextual representations, we introduce random blockwise and span masking in the text recognition task. These strategies can mask the continuous image patches and completely remove some characters, forcing the model to infer relationships among characters within a word. Our Multi-Masking Strategy (MMS) integrates random patch, blockwise, and span masking into the MIM frame, which jointly learns low and high-level text representations. After fine-tuning with real data, MMS outperforms the state-of-the-art self-supervised methods in various text-related tasks, including text recognition, segmentation, and text-image super-resolution.
Published: 2026-02-09T08:03:22+00:00
Venue: Pattern Recognition
Score: 0.789 (consider)</description><content:encoded>&lt;p&gt;&lt;strong&gt;Authors:&lt;/strong&gt; Zhengmi Tang; Yuto Mitsui; Tomo Miyazaki; Shinichiro Omachi&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Journal:&lt;/strong&gt; Pattern Recognition&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;a href="https://doi.org/10.1016/j.patcog.2026.113273"&gt;10.1016/j.patcog.2026.113273&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Relevance:&lt;/strong&gt; 0.789 (consider)&lt;/p&gt;
&lt;p&gt;Most existing text recognition methods are trained on large-scale synthetic datasets due to the scarcity of labeled real-world datasets. Synthetic images, however, cannot faithfully reproduce real-world scenarios, such as uneven illumination, irregular layout, occlusion, and degradation, resulting in performance disparities when handling complex real-world images. Recent self-supervised learning techniques, notably contrastive learning and masked image modeling (MIM), narrow this domain gap by exploiting unlabeled real text images. This study first analyzes the original Masked AutoEncoder (MAE) and observes that random patch masking predominantly captures low-level textural features but misses high-level contextual representations. To fully exploit the high-level contextual representations, we introduce random blockwise and span masking in the text recognition task. These strategies can mask the continuous image patches and completely remove some characters, forcing the model to infer relationships among characters within a word. Our Multi-Masking Strategy (MMS) integrates random patch, blockwise, and span masking into the MIM frame, which jointly learns low and high-level text representations. After fine-tuning with real data, MMS outperforms the state-of-the-art self-supervised methods in various text-related tasks, including text recognition, segmentation, and text-image super-resolution.&lt;/p&gt;</content:encoded></item><item><title>Privacy-Preserving Federated SAR Image Target Recognition with Adaptive Resource Management in Space-Air-Ground Integrated Networks</title><link>https://doi.org/10.1016/j.patcog.2026.113253</link><guid>10.1016/j.patcog.2026.113253</guid><pubDate>Sun, 08 Feb 2026 15:40:28 +0000</pubDate><dc:creator>Yuchao Hou</dc:creator><dc:creator>Bo Yu</dc:creator><dc:creator>Zhiqin Yang</dc:creator><dc:creator>Jie Wang</dc:creator><dc:creator>Wei Xiang</dc:creator><dc:creator>Di Wu</dc:creator><dc:creator>Minghui Liwang</dc:creator><dc:creator>Xiaoyu Xia</dc:creator><dc:creator>Zijian Li</dc:creator><dc:creator>Youliang Tian</dc:creator><dc:creator>Yuzhou Sun</dc:creator><prism:publicationName>Pattern Recognition</prism:publicationName><prism:doi>10.1016/j.patcog.2026.113253</prism:doi><description>Deep learning based synthetic aperture radar (SAR) image target recognition has become an important branch of pattern recognition, especially as space-air-ground integrated networks (SAGINs) demand reliable feature representation and robust recognition across highly distributed sensing platforms. However, current SAR recognition models face privacy risks from centralized data aggregation as well as computation and communication limitations arising from highly heterogeneous sensing platforms. To address these limitations, we design SAR-RAFL, a federated learning (FL) framework with adaptive resource management aimed at enhancing robustness and efficiency for SAR-based pattern recognition in SAGINs. We propose a collaborative computation scheme in which spaceborne, aerial, and ground nodes cooperate to allocate computation and communication resources dynamically for improved operational efficiency. We further introduce a device-aware model assignment strategy that distributes customized sub-models based on each node’s available resources, avoiding fixed model sizes across heterogeneous devices. We also design a dual-objective node selection mechanism that encourages balanced participation and stable convergence in the presence of heterogeneous nodes. A rigorous theoretical analysis provides convergence guarantees, and extensive experiments on the MSTAR and FUSAR-Ship datasets validate the effectiveness of SAR-RAFL in improving recognition accuracy and resource utilization within SAGINs.
Published: 2026-02-08T15:40:28+00:00
Venue: Pattern Recognition
Score: 0.788 (consider)</description><content:encoded>&lt;p&gt;&lt;strong&gt;Authors:&lt;/strong&gt; Yuchao Hou; Bo Yu; Zhiqin Yang; Jie Wang; Wei Xiang; Di Wu; Minghui Liwang; Xiaoyu Xia; Zijian Li; Youliang Tian; Yuzhou Sun&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Journal:&lt;/strong&gt; Pattern Recognition&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;a href="https://doi.org/10.1016/j.patcog.2026.113253"&gt;10.1016/j.patcog.2026.113253&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Relevance:&lt;/strong&gt; 0.788 (consider)&lt;/p&gt;
&lt;p&gt;Deep learning based synthetic aperture radar (SAR) image target recognition has become an important branch of pattern recognition, especially as space-air-ground integrated networks (SAGINs) demand reliable feature representation and robust recognition across highly distributed sensing platforms. However, current SAR recognition models face privacy risks from centralized data aggregation as well as computation and communication limitations arising from highly heterogeneous sensing platforms. To address these limitations, we design SAR-RAFL, a federated learning (FL) framework with adaptive resource management aimed at enhancing robustness and efficiency for SAR-based pattern recognition in SAGINs. We propose a collaborative computation scheme in which spaceborne, aerial, and ground nodes cooperate to allocate computation and communication resources dynamically for improved operational efficiency. We further introduce a device-aware model assignment strategy that distributes customized sub-models based on each node’s available resources, avoiding fixed model sizes across heterogeneous devices. We also design a dual-objective node selection mechanism that encourages balanced participation and stable convergence in the presence of heterogeneous nodes. A rigorous theoretical analysis provides convergence guarantees, and extensive experiments on the MSTAR and FUSAR-Ship datasets validate the effectiveness of SAR-RAFL in improving recognition accuracy and resource utilization within SAGINs.&lt;/p&gt;</content:encoded></item><item><title>Dynamic patch selection and dual-granularity alignment for cross-modal retrieval</title><link>https://doi.org/10.1016/j.neucom.2026.132999</link><guid>10.1016/j.neucom.2026.132999</guid><pubDate>Mon, 09 Feb 2026 08:09:02 +0000</pubDate><dc:creator>Zhenghui Luo</dc:creator><dc:creator>Min Meng</dc:creator><dc:creator>Jigang Wu</dc:creator><prism:publicationName>Neurocomputing</prism:publicationName><prism:doi>10.1016/j.neucom.2026.132999</prism:doi><description>Cross-modal retrieval aims to establish semantic associations between heterogeneous modalities, among which image-text retrieval is a key application scenario that seeks to achieve efficient semantic alignment between images and texts. Existing approaches often rely on fixed patch selection strategies for fine-grained alignment. However, such static strategies struggle to adapt to complex scene variations. Moreover, fine-grained alignment methods tend to fall into local optima by overemphasizing local feature details while neglecting global semantic context. Such limitations significantly hinder both retrieval accuracy and generalization performance. To address these challenges, we propose a Dynamic Patch Selection and Dual-Granularity Alignment (DPSDGA) framework that jointly enhances global semantic consistency and local feature interactions for robust cross-modal alignment. Specifically, we introduce a dynamic sparse module that adaptively adjusts the number of retained visual patches based on scene complexity, effectively filtering redundant information while preserving critical semantic features. Furthermore, we design a dual-granularity alignment mechanism, which combines global contrastive learning with local fine-grained alignment to enhance semantic consistency across modalities. Extensive experiments on two benchmark datasets, Flickr30k and MS-COCO, demonstrate that our method significantly outperforms existing approaches in image-text retrieval.
Published: 2026-02-09T08:09:02+00:00
Venue: Neurocomputing
Score: 0.779 (consider)</description><content:encoded>&lt;p&gt;&lt;strong&gt;Authors:&lt;/strong&gt; Zhenghui Luo; Min Meng; Jigang Wu&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Journal:&lt;/strong&gt; Neurocomputing&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;a href="https://doi.org/10.1016/j.neucom.2026.132999"&gt;10.1016/j.neucom.2026.132999&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Relevance:&lt;/strong&gt; 0.779 (consider)&lt;/p&gt;
&lt;p&gt;Cross-modal retrieval aims to establish semantic associations between heterogeneous modalities, among which image-text retrieval is a key application scenario that seeks to achieve efficient semantic alignment between images and texts. Existing approaches often rely on fixed patch selection strategies for fine-grained alignment. However, such static strategies struggle to adapt to complex scene variations. Moreover, fine-grained alignment methods tend to fall into local optima by overemphasizing local feature details while neglecting global semantic context. Such limitations significantly hinder both retrieval accuracy and generalization performance. To address these challenges, we propose a Dynamic Patch Selection and Dual-Granularity Alignment (DPSDGA) framework that jointly enhances global semantic consistency and local feature interactions for robust cross-modal alignment. Specifically, we introduce a dynamic sparse module that adaptively adjusts the number of retained visual patches based on scene complexity, effectively filtering redundant information while preserving critical semantic features. Furthermore, we design a dual-granularity alignment mechanism, which combines global contrastive learning with local fine-grained alignment to enhance semantic consistency across modalities. Extensive experiments on two benchmark datasets, Flickr30k and MS-COCO, demonstrate that our method significantly outperforms existing approaches in image-text retrieval.&lt;/p&gt;</content:encoded></item><item><title>Dilated Residual and Swin-Neighbor Transformer-Based Methods for Accurate Remote Sensing Image Analysis</title><link>https://doi.org/10.1016/j.knosys.2026.115526</link><guid>10.1016/j.knosys.2026.115526</guid><pubDate>Mon, 09 Feb 2026 08:09:55 +0000</pubDate><dc:creator>Weiye Wang</dc:creator><prism:publicationName>Knowledge-Based Systems</prism:publicationName><prism:doi>10.1016/j.knosys.2026.115526</prism:doi><description>Accurate classification of satellite images is important for applications such as environmental monitoring, urban planning, and land-use mapping. Although conventional transformer-based vision models provide global receptive fields through self-attention, they often exhibit weak local inductive bias and may under-represent fine-grained spatial textures, especially in complex high-resolution remote sensing scenes. To address these challenges, this paper proposes a new satellite image classification architecture that integrates a Dilated Residual Visual Geometry Group (DRVGG-16) network for robust deep feature extraction and a Swin-Neighbor Transformer to hierarchically model local-to-global dependencies. In the transformer, the Window-based Spatial Multi-Head Contextual Attention (W-SaMCA) and Shifted-Window SaMCA (SW-SaMCA) modules adaptively cluster spatial neighborhoods to improve contextual discrimination, and the Neighbor Window Connection (NWC) mechanism allows inter-window communication for boundary continuity. A Kohonen Learning Layer is used to boost class separability through unsupervised clustering. The evaluations were carried out on the Land-Use Scene Classification, WHU-RS19, RSSCN7, and RSI-CB256 datasets using Python 3.10 and the PyTorch framework. The model exhibited a better macro average accuracy of 98.78% for the Land-Use Scene Classification dataset, 97.89% for the WHU-RS19 dataset, 98.90% for the RSSCN7 dataset, and 98.68% for the RSI-CB256 dataset across all datasets. The model's ability to balance the spatial detail and global context modeling offers valuable promise for large-scale land-cover classification tasks and practical geospatial analysis.
Published: 2026-02-09T08:09:55+00:00
Venue: Knowledge-Based Systems
Score: 0.775 (consider)</description><content:encoded>&lt;p&gt;&lt;strong&gt;Authors:&lt;/strong&gt; Weiye Wang&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Journal:&lt;/strong&gt; Knowledge-Based Systems&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;a href="https://doi.org/10.1016/j.knosys.2026.115526"&gt;10.1016/j.knosys.2026.115526&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Relevance:&lt;/strong&gt; 0.775 (consider)&lt;/p&gt;
&lt;p&gt;Accurate classification of satellite images is important for applications such as environmental monitoring, urban planning, and land-use mapping. Although conventional transformer-based vision models provide global receptive fields through self-attention, they often exhibit weak local inductive bias and may under-represent fine-grained spatial textures, especially in complex high-resolution remote sensing scenes. To address these challenges, this paper proposes a new satellite image classification architecture that integrates a Dilated Residual Visual Geometry Group (DRVGG-16) network for robust deep feature extraction and a Swin-Neighbor Transformer to hierarchically model local-to-global dependencies. In the transformer, the Window-based Spatial Multi-Head Contextual Attention (W-SaMCA) and Shifted-Window SaMCA (SW-SaMCA) modules adaptively cluster spatial neighborhoods to improve contextual discrimination, and the Neighbor Window Connection (NWC) mechanism allows inter-window communication for boundary continuity. A Kohonen Learning Layer is used to boost class separability through unsupervised clustering. The evaluations were carried out on the Land-Use Scene Classification, WHU-RS19, RSSCN7, and RSI-CB256 datasets using Python 3.10 and the PyTorch framework. The model exhibited a better macro average accuracy of 98.78% for the Land-Use Scene Classification dataset, 97.89% for the WHU-RS19 dataset, 98.90% for the RSSCN7 dataset, and 98.68% for the RSI-CB256 dataset across all datasets. The model&amp;#x27;s ability to balance the spatial detail and global context modeling offers valuable promise for large-scale land-cover classification tasks and practical geospatial analysis.&lt;/p&gt;</content:encoded></item><item><title>Non-target information also matters: InverseFormer tracker for single object tracking</title><link>https://doi.org/10.1016/j.imavis.2026.105922</link><guid>10.1016/j.imavis.2026.105922</guid><pubDate>Sun, 08 Feb 2026 15:42:06 +0000</pubDate><dc:creator>Qiuhang Gu</dc:creator><dc:creator>Baopeng Zhang</dc:creator><dc:creator>Zhu Teng</dc:creator><dc:creator>Hongwei Xu</dc:creator><prism:publicationName>Image and Vision Computing</prism:publicationName><prism:doi>10.1016/j.imavis.2026.105922</prism:doi><description>Visual object tracking has been significantly improved by Transformer-based methods. However, most existing trackers perform target-oriented inference, which enhances target-relevant features while ignoring non-target features. We argue that non-target information also contains abundant clues that can provide significant guidance for tracking inference. In this work, we propose a novel InverseFormer tracker constructed by stacking multiple InverseFormer blocks. The proposed InverseFormer block consists of a context aggregation unit and an inverse enhancement unit. The former aggregates local context correlation information while boosting tracking efficiency. The latter enhances the template-search image pair by using non-target information in the search region, which significantly suppresses background-relevant features while preserving target details, leading to more accurate tracking. Extensive experiments conducted on seven benchmarks demonstrate that our tracker outperforms state-of-the-art methods at a real-time speed of 45 FPS.
Published: 2026-02-08T15:42:06+00:00
Venue: Image and Vision Computing
Score: 0.757 (consider)</description><content:encoded>&lt;p&gt;&lt;strong&gt;Authors:&lt;/strong&gt; Qiuhang Gu; Baopeng Zhang; Zhu Teng; Hongwei Xu&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Journal:&lt;/strong&gt; Image and Vision Computing&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;a href="https://doi.org/10.1016/j.imavis.2026.105922"&gt;10.1016/j.imavis.2026.105922&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Relevance:&lt;/strong&gt; 0.757 (consider)&lt;/p&gt;
&lt;p&gt;Visual object tracking has been significantly improved by Transformer-based methods. However, most existing trackers perform target-oriented inference, which enhances target-relevant features while ignoring non-target features. We argue that non-target information also contains abundant clues that can provide significant guidance for tracking inference. In this work, we propose a novel InverseFormer tracker constructed by stacking multiple InverseFormer blocks. The proposed InverseFormer block consists of a context aggregation unit and an inverse enhancement unit. The former aggregates local context correlation information while boosting tracking efficiency. The latter enhances the template-search image pair by using non-target information in the search region, which significantly suppresses background-relevant features while preserving target details, leading to more accurate tracking. Extensive experiments conducted on seven benchmarks demonstrate that our tracker outperforms state-of-the-art methods at a real-time speed of 45 FPS.&lt;/p&gt;</content:encoded></item><item><title>Multi-perspective domain-invariant network with energy density-based data augmentation for domain generalization fault diagnosis</title><link>https://doi.org/10.1016/j.eswa.2026.131583</link><guid>10.1016/j.eswa.2026.131583</guid><pubDate>Sun, 08 Feb 2026 15:44:38 +0000</pubDate><dc:creator>Sukeun Hong</dc:creator><dc:creator>Jaewook Lee</dc:creator><dc:creator>Jongsoo Lee</dc:creator><prism:publicationName>Expert Systems with Applications</prism:publicationName><prism:doi>10.1016/j.eswa.2026.131583</prism:doi><description>Existing domain generalization fault diagnosis methods achieve satisfactory interpolation performance but struggle with extrapolation owing to two fundamental limitations: insufficient source domain coverage and the inability to verify whether learned features represent causal fault characteristics or spurious correlations. To address these challenges, this study proposes a multi-perspective domain-invariant network (MPDIN) with energy–density-based data augmentation. MPDIN employs bootstrap aggregation to train multiple feature extractors on strategically defined domain subsets, establishing hierarchical domain invariance by enforcing subset-level invariance through triplet loss and inter-subset consistency via correlation alignment. This multi-perspective framework effectively suppresses subset-specific spurious correlations while preserving genuine fault characteristics. The energy–density-based augmentation leverages the &amp;#x3C9; 2 " role="presentation"&gt; ω 2 ω 2 -proportional relationship between rotational speed and vibration energy to generate realistic extrapolation data beyond source domain boundaries, utilizing raw short-time Fourier transform power spectrograms to preserve absolute energy information essential for physics-based scaling. Experimental validation across four diverse datasets demonstrated substantial improvements in challenging extrapolation scenarios, achieving gains of 19–47%, whereas conventional methods showed significant performance degradation. Manifold analysis confirmed continuity and complete target–source integration, validating the attainment of true domain-invariant learning. Although limitations exist in time-varying scenarios, the proposed methodology provides a principled framework for industrial deployment where targets frequently exceed training envelopes.
Published: 2026-02-08T15:44:38+00:00
Venue: Expert Systems with Applications
Score: 0.753 (consider)</description><content:encoded>&lt;p&gt;&lt;strong&gt;Authors:&lt;/strong&gt; Sukeun Hong; Jaewook Lee; Jongsoo Lee&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Journal:&lt;/strong&gt; Expert Systems with Applications&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;a href="https://doi.org/10.1016/j.eswa.2026.131583"&gt;10.1016/j.eswa.2026.131583&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Relevance:&lt;/strong&gt; 0.753 (consider)&lt;/p&gt;
&lt;p&gt;Existing domain generalization fault diagnosis methods achieve satisfactory interpolation performance but struggle with extrapolation owing to two fundamental limitations: insufficient source domain coverage and the inability to verify whether learned features represent causal fault characteristics or spurious correlations. To address these challenges, this study proposes a multi-perspective domain-invariant network (MPDIN) with energy–density-based data augmentation. MPDIN employs bootstrap aggregation to train multiple feature extractors on strategically defined domain subsets, establishing hierarchical domain invariance by enforcing subset-level invariance through triplet loss and inter-subset consistency via correlation alignment. This multi-perspective framework effectively suppresses subset-specific spurious correlations while preserving genuine fault characteristics. The energy–density-based augmentation leverages the &amp;amp;#x3C9; 2 &amp;quot; role=&amp;quot;presentation&amp;quot;&amp;gt; ω 2 ω 2 -proportional relationship between rotational speed and vibration energy to generate realistic extrapolation data beyond source domain boundaries, utilizing raw short-time Fourier transform power spectrograms to preserve absolute energy information essential for physics-based scaling. Experimental validation across four diverse datasets demonstrated substantial improvements in challenging extrapolation scenarios, achieving gains of 19–47%, whereas conventional methods showed significant performance degradation. Manifold analysis confirmed continuity and complete target–source integration, validating the attainment of true domain-invariant learning. Although limitations exist in time-varying scenarios, the proposed methodology provides a principled framework for industrial deployment where targets frequently exceed training envelopes.&lt;/p&gt;</content:encoded></item><item><title>DSF-Unet: A Dual-Stream Fusion Denoising Diffusion Framework for Imbalanced Wafer Defect Classification</title><link>https://doi.org/10.1016/j.eswa.2026.131471</link><guid>10.1016/j.eswa.2026.131471</guid><pubDate>Sun, 08 Feb 2026 22:53:56 +0000</pubDate><dc:creator>Rongbin Xu</dc:creator><dc:creator>Zhiqiang Xu</dc:creator><dc:creator>Jixiang Wang</dc:creator><dc:creator>Ying Xie</dc:creator><dc:creator>Lijie Wen</dc:creator><dc:creator>Yun Yang</dc:creator><prism:publicationName>Expert Systems with Applications</prism:publicationName><prism:doi>10.1016/j.eswa.2026.131471</prism:doi><description>Wafer defect recognition is crucial for semiconductor manufacturing. However, its accuracy is often limited by severe imbalance among defect categories. To address this challenge, we propose a Dual-Stream Fusion Unet denoising diffusion framework (DSF-Unet), which synthesizes diverse and high-quality wafer samples for minority classes. DSF-Unet builds upon a Unet encoder-decoder backbone and introduces two complementary components. The Bidirectional Mamba (Bi-Mamba) module models long-range spatial dependencies through state-space dynamics, while the Adaptive multi-scale attention (Am-att) module enhances spatial feature representation via cross-channel calibration. By jointly capturing global contextual information and fine-grained local defect patterns, these two modules enable the generation of balanced and representative synthetic samples, thereby effectively alleviating data imbalance. For further performance improvement, an enhanced Channel-Spatial Residual Network (CS-ResNet) is introduced for classification, which embeds a dual channel-spatial attention mechanism into the ResNet backbone to recalibrate feature responses and highlight defect-relevant regions. Experiments on two benchmark datasets demonstrate that DSF-Unet achieves superior performance, reaching 95.12% accuracy on WM-811K and 98.25% on Mixed-WM38.
Published: 2026-02-08T22:53:56+00:00
Venue: Expert Systems with Applications
Score: 0.750 (consider)</description><content:encoded>&lt;p&gt;&lt;strong&gt;Authors:&lt;/strong&gt; Rongbin Xu; Zhiqiang Xu; Jixiang Wang; Ying Xie; Lijie Wen; Yun Yang&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Journal:&lt;/strong&gt; Expert Systems with Applications&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;a href="https://doi.org/10.1016/j.eswa.2026.131471"&gt;10.1016/j.eswa.2026.131471&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Relevance:&lt;/strong&gt; 0.750 (consider)&lt;/p&gt;
&lt;p&gt;Wafer defect recognition is crucial for semiconductor manufacturing. However, its accuracy is often limited by severe imbalance among defect categories. To address this challenge, we propose a Dual-Stream Fusion Unet denoising diffusion framework (DSF-Unet), which synthesizes diverse and high-quality wafer samples for minority classes. DSF-Unet builds upon a Unet encoder-decoder backbone and introduces two complementary components. The Bidirectional Mamba (Bi-Mamba) module models long-range spatial dependencies through state-space dynamics, while the Adaptive multi-scale attention (Am-att) module enhances spatial feature representation via cross-channel calibration. By jointly capturing global contextual information and fine-grained local defect patterns, these two modules enable the generation of balanced and representative synthetic samples, thereby effectively alleviating data imbalance. For further performance improvement, an enhanced Channel-Spatial Residual Network (CS-ResNet) is introduced for classification, which embeds a dual channel-spatial attention mechanism into the ResNet backbone to recalibrate feature responses and highlight defect-relevant regions. Experiments on two benchmark datasets demonstrate that DSF-Unet achieves superior performance, reaching 95.12% accuracy on WM-811K and 98.25% on Mixed-WM38.&lt;/p&gt;</content:encoded></item><item><title>Game-theoretic evaluation of strategic reasoning in large language models: From complete coverage to compositional complexity</title><link>https://doi.org/10.1016/j.neucom.2026.133006</link><guid>10.1016/j.neucom.2026.133006</guid><pubDate>Mon, 09 Feb 2026 08:09:09 +0000</pubDate><dc:creator>Yu Guo</dc:creator><dc:creator>Haochuan Wang</dc:creator><dc:creator>Xiachong Feng</dc:creator><prism:publicationName>Neurocomputing</prism:publicationName><prism:doi>10.1016/j.neucom.2026.133006</prism:doi><description>Game-theoretic evaluation of strategic reasoning in large language models (LLMs) is crucial for advancing artificial intelligence systems, yet faces fundamental challenges: incomplete game coverage, data contamination risks, and inability to assess compositional reasoning complexity. We present TMGBench , a benchmark that progresses from complete coverage to compositional complexity through systematic design. For complete coverage, TMGBench incorporates all 144 canonical game types from the Robinson-Goforth topology, the first benchmark to achieve exhaustive game-theoretic representation, eliminating the sampling bias that undermines existing evaluations. Each game is instantiated through synthetically generated narrative scenarios, rigorously validated to ensure novelty and prevent data leakage. To address compositional complexity, we introduce a hierarchical framework where these atomic games are programmatically composed into sequential, parallel, and nested structures, creating scalable challenges that systematically probe reasoning depth from simple strategic decisions to complex multi-agent interactions. Our evaluation reveals critical limitations in LLM strategic reasoning across this complete-to-complex spectrum. Even state-of-the-art models fail at basic game-theoretic reasoning, exhibiting logical inconsistencies and superficial Theory-of-Mind understanding. Performance degrades catastrophically as compositional complexity increases: models achieving 60% accuracy on isolated games drop below 20% on compositional structures, exposing fundamental architectural limitations in current AI systems’ ability to handle strategic dependencies. These results demonstrate that current LLMs lack the compositional reasoning capabilities required for genuine strategic thinking. TMGBench thus provides both comprehensive diagnostic coverage and a scalable complexity framework essential for advancing artificial intelligence toward human-level game-theoretic reasoning and strategic decision-making capabilities.
Published: 2026-02-09T08:09:09+00:00
Venue: Neurocomputing
Score: 0.743 (consider)</description><content:encoded>&lt;p&gt;&lt;strong&gt;Authors:&lt;/strong&gt; Yu Guo; Haochuan Wang; Xiachong Feng&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Journal:&lt;/strong&gt; Neurocomputing&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;a href="https://doi.org/10.1016/j.neucom.2026.133006"&gt;10.1016/j.neucom.2026.133006&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Relevance:&lt;/strong&gt; 0.743 (consider)&lt;/p&gt;
&lt;p&gt;Game-theoretic evaluation of strategic reasoning in large language models (LLMs) is crucial for advancing artificial intelligence systems, yet faces fundamental challenges: incomplete game coverage, data contamination risks, and inability to assess compositional reasoning complexity. We present TMGBench , a benchmark that progresses from complete coverage to compositional complexity through systematic design. For complete coverage, TMGBench incorporates all 144 canonical game types from the Robinson-Goforth topology, the first benchmark to achieve exhaustive game-theoretic representation, eliminating the sampling bias that undermines existing evaluations. Each game is instantiated through synthetically generated narrative scenarios, rigorously validated to ensure novelty and prevent data leakage. To address compositional complexity, we introduce a hierarchical framework where these atomic games are programmatically composed into sequential, parallel, and nested structures, creating scalable challenges that systematically probe reasoning depth from simple strategic decisions to complex multi-agent interactions. Our evaluation reveals critical limitations in LLM strategic reasoning across this complete-to-complex spectrum. Even state-of-the-art models fail at basic game-theoretic reasoning, exhibiting logical inconsistencies and superficial Theory-of-Mind understanding. Performance degrades catastrophically as compositional complexity increases: models achieving 60% accuracy on isolated games drop below 20% on compositional structures, exposing fundamental architectural limitations in current AI systems’ ability to handle strategic dependencies. These results demonstrate that current LLMs lack the compositional reasoning capabilities required for genuine strategic thinking. TMGBench thus provides both comprehensive diagnostic coverage and a scalable complexity framework essential for advancing artificial intelligence toward human-level game-theoretic reasoning and strategic decision-making capabilities.&lt;/p&gt;</content:encoded></item><item><title>Reconstruction error-based anomaly detection with few outlying examples</title><link>https://doi.org/10.1016/j.neucom.2026.133002</link><guid>10.1016/j.neucom.2026.133002</guid><pubDate>Mon, 09 Feb 2026 08:08:57 +0000</pubDate><dc:creator>Fabrizio Angiulli</dc:creator><dc:creator>Fabio Fassetti</dc:creator><dc:creator>Luca Ferragina</dc:creator><prism:publicationName>Neurocomputing</prism:publicationName><prism:doi>10.1016/j.neucom.2026.133002</prism:doi><description>Reconstruction error-based neural architectures constitute a classical deep learning approach to anomaly detection which has shown great performances. It consists in training an Autoencoder to reconstruct a set of examples deemed to represent the normality and then to point out as anomalies those data that show a sufficiently large reconstruction error. Unfortunately, these architectures often become able to well reconstruct also the anomalies in the data. This phenomenon is more evident when there are anomalies in the training set. In particular, when these anomalies are labeled, a setting called semi-supervised, the best way to train Autoencoders is to ignore anomalies and minimize the reconstruction error on normal data.
When a sufficiently large and representative set of anomalous examples is available, the problem essentially shifts toward a classification task, where standard supervised strategies can be applied effectively. In this work, instead, we focus on the more challenging scenario in which only a limited number of anomalous examples is available, and these examples are not sufficiently representative of the wide variability that anomalies may exhibit.
Published: 2026-02-09T08:08:57+00:00
Venue: Neurocomputing
Score: 0.737 (ignore)</description><content:encoded>&lt;p&gt;&lt;strong&gt;Authors:&lt;/strong&gt; Fabrizio Angiulli; Fabio Fassetti; Luca Ferragina&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Journal:&lt;/strong&gt; Neurocomputing&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;a href="https://doi.org/10.1016/j.neucom.2026.133002"&gt;10.1016/j.neucom.2026.133002&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Relevance:&lt;/strong&gt; 0.737 (ignore)&lt;/p&gt;
&lt;p&gt;Reconstruction error-based neural architectures constitute a classical deep learning approach to anomaly detection which has shown great performances. It consists in training an Autoencoder to reconstruct a set of examples deemed to represent the normality and then to point out as anomalies those data that show a sufficiently large reconstruction error. Unfortunately, these architectures often become able to well reconstruct also the anomalies in the data. This phenomenon is more evident when there are anomalies in the training set. In particular, when these anomalies are labeled, a setting called semi-supervised, the best way to train Autoencoders is to ignore anomalies and minimize the reconstruction error on normal data.
When a sufficiently large and representative set of anomalous examples is available, the problem essentially shifts toward a classification task, where standard supervised strategies can be applied effectively. In this work, instead, we focus on the more challenging scenario in which only a limited number of anomalous examples is available, and these examples are not sufficiently representative of the wide variability that anomalies may exhibit.&lt;/p&gt;</content:encoded></item><item><title>Testing the accuracy and transferability of remotely sensed biomass models across heterogeneous grasslands</title><link>https://doi.org/10.1016/j.rse.2026.115294</link><guid>10.1016/j.rse.2026.115294</guid><pubDate>Mon, 09 Feb 2026 08:04:37 +0000</pubDate><dc:creator>Jan Schweizer</dc:creator><dc:creator>Leon T. Hauser</dc:creator><dc:creator>Hamed Gholizadeh</dc:creator><dc:creator>Anna K. Schweiger</dc:creator><dc:creator>Christian Rossi</dc:creator><prism:publicationName>Remote Sensing of Environment</prism:publicationName><prism:doi>10.1016/j.rse.2026.115294</prism:doi><description>Grassland aboveground biomass provides key insights into ecological processes such as carbon sequestration, animal movement patterns, and agricultural management practices. Different model types have been developed to estimate grassland biomass from satellite imagery. However, differences in model performance across sites with varying management and ecology remain largely understudied. In this study, we compared accuracy and transferability of empirical, physically-based, and hybrid models to estimate grassland biomass from multispectral Sentinel-2 data in an agnostic scenario, i.e., the models were not provided with any site-specific information beyond the spectral data. Based on field data from five study sites in Europe and the United States, we assessed (1)site-level accuracy of biomass estimation models, (2) model transferability between sites (domain shift), (3) the performance of models trained or optimized with data from multiple study sites (domain generalization), and (4) the relationship between epistemic uncertainty and model transferability. Our results showed that (1) all models exhibited comparable performance at the site level, (2) physically-based models showed the highest degree of transferability between sites, (3) no model consistently outperformed all other models when trained or optimized with field data from multiple sites, and (4) epistemic uncertainty was not necessarily a reliable measure of model applicability to unseen data. Our findings demonstrate the challenges associated with grassland biomass models under domain shift. This elucidates limits to agnostic inference in targeting diverse grasslands and highlights that model transferability is an integral part of performance assessment towards scalable satellite-based grassland monitoring systems, especially as the community increasingly deploys models at continental to global scales.
Published: 2026-02-09T08:04:37+00:00
Venue: Remote Sensing of Environment
Score: 0.735 (ignore)</description><content:encoded>&lt;p&gt;&lt;strong&gt;Authors:&lt;/strong&gt; Jan Schweizer; Leon T. Hauser; Hamed Gholizadeh; Anna K. Schweiger; Christian Rossi&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Journal:&lt;/strong&gt; Remote Sensing of Environment&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;a href="https://doi.org/10.1016/j.rse.2026.115294"&gt;10.1016/j.rse.2026.115294&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Relevance:&lt;/strong&gt; 0.735 (ignore)&lt;/p&gt;
&lt;p&gt;Grassland aboveground biomass provides key insights into ecological processes such as carbon sequestration, animal movement patterns, and agricultural management practices. Different model types have been developed to estimate grassland biomass from satellite imagery. However, differences in model performance across sites with varying management and ecology remain largely understudied. In this study, we compared accuracy and transferability of empirical, physically-based, and hybrid models to estimate grassland biomass from multispectral Sentinel-2 data in an agnostic scenario, i.e., the models were not provided with any site-specific information beyond the spectral data. Based on field data from five study sites in Europe and the United States, we assessed (1)site-level accuracy of biomass estimation models, (2) model transferability between sites (domain shift), (3) the performance of models trained or optimized with data from multiple study sites (domain generalization), and (4) the relationship between epistemic uncertainty and model transferability. Our results showed that (1) all models exhibited comparable performance at the site level, (2) physically-based models showed the highest degree of transferability between sites, (3) no model consistently outperformed all other models when trained or optimized with field data from multiple sites, and (4) epistemic uncertainty was not necessarily a reliable measure of model applicability to unseen data. Our findings demonstrate the challenges associated with grassland biomass models under domain shift. This elucidates limits to agnostic inference in targeting diverse grasslands and highlights that model transferability is an integral part of performance assessment towards scalable satellite-based grassland monitoring systems, especially as the community increasingly deploys models at continental to global scales.&lt;/p&gt;</content:encoded></item><item><title>Neural Probabilistic Logic Learning: A Method for Knowledge Graph Reasoning</title><link>https://doi.org/10.1016/j.knosys.2026.115513</link><guid>10.1016/j.knosys.2026.115513</guid><pubDate>Mon, 09 Feb 2026 08:10:02 +0000</pubDate><dc:creator>Fengsong Sun</dc:creator><dc:creator>Xianchao Zhang</dc:creator><dc:creator>Jinyu Wang</dc:creator><dc:creator>Zhiguo Jiang</dc:creator><prism:publicationName>Knowledge-Based Systems</prism:publicationName><prism:doi>10.1016/j.knosys.2026.115513</prism:doi><description>Knowledge graph (KG) reasoning aims to predict missing facts from known data. While rule-based methods achieve high precision, they suffer from scalability limitations in large-scale KGs. Conversely, embedding-based approaches scale efficiently but often compromise precision. To address this trade-off, we propose Neural Probabilistic Logic Learning (NPLL), a novel hybrid framework that simultaneously enhances accuracy and efficiency. NPLL integrates a scoring module to augment the expressive capacity of embedding networks without sacrificing model simplicity or reasoning performance. Furthermore, interpretability is improved through the integration of a Markov Logic Network (MLN) with variational inference. Extensive evaluations on eleven benchmark datasets demonstrate that NPLL consistently outperforms state-of-the-art methods in both accuracy and computational efficiency, yielding substantial improvements in reasoning quality.
Published: 2026-02-09T08:10:02+00:00
Venue: Knowledge-Based Systems
Score: 0.733 (ignore)</description><content:encoded>&lt;p&gt;&lt;strong&gt;Authors:&lt;/strong&gt; Fengsong Sun; Xianchao Zhang; Jinyu Wang; Zhiguo Jiang&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Journal:&lt;/strong&gt; Knowledge-Based Systems&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;a href="https://doi.org/10.1016/j.knosys.2026.115513"&gt;10.1016/j.knosys.2026.115513&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Relevance:&lt;/strong&gt; 0.733 (ignore)&lt;/p&gt;
&lt;p&gt;Knowledge graph (KG) reasoning aims to predict missing facts from known data. While rule-based methods achieve high precision, they suffer from scalability limitations in large-scale KGs. Conversely, embedding-based approaches scale efficiently but often compromise precision. To address this trade-off, we propose Neural Probabilistic Logic Learning (NPLL), a novel hybrid framework that simultaneously enhances accuracy and efficiency. NPLL integrates a scoring module to augment the expressive capacity of embedding networks without sacrificing model simplicity or reasoning performance. Furthermore, interpretability is improved through the integration of a Markov Logic Network (MLN) with variational inference. Extensive evaluations on eleven benchmark datasets demonstrate that NPLL consistently outperforms state-of-the-art methods in both accuracy and computational efficiency, yielding substantial improvements in reasoning quality.&lt;/p&gt;</content:encoded></item><item><title>TF-LLM: Enhanced Time Series Analysis with Time-Frequency Large Language Models</title><link>https://doi.org/10.1016/j.neunet.2026.108687</link><guid>10.1016/j.neunet.2026.108687</guid><pubDate>Mon, 09 Feb 2026 08:08:33 +0000</pubDate><dc:creator>Yuhang Zhang</dc:creator><dc:creator>Zitong Yu</dc:creator><dc:creator>Mingtong Dai</dc:creator><dc:creator>Yue Sun</dc:creator><dc:creator>Tao Tan</dc:creator><prism:publicationName>Neural Networks</prism:publicationName><prism:doi>10.1016/j.neunet.2026.108687</prism:doi><description>In recent years, pre-trained large language models (LLMs) are gradually introduced into time series analysis, and researchers have observed their significant potential for multi-task reasoning, particularly in handling complex symbolic sequences. However, how to effectively and deeply leverage the contextual reasoning capabilities of LLMs in time series data remains a key challenge. To address this challenge, this study proposes the TF-LLM framework to handle various tasks in time series, such as forecasting, classification, imputation, and anomaly detection. We innovatively integrate the strengths of both time and frequency domains: frequency representations simplify data complexity and enhance the capture of global and local periodic patterns, while time modeling addresses fine-grained dependencies, mitigating the effects of non-stationarity. Additionally, to enhance the model’s reasoning capabilities, we introduce prompt learning to enrich the contextual information of inputs and help the LLMs better understand time series data. We conduct extensive multi-task experiments on seven benchmark datasets, covering tasks like forecasting, classification, imputation, and anomaly detection. The results indicate the superior performance of the proposed TF-LLM in handling complex time series tasks, outperforming several existing methods.
Published: 2026-02-09T08:08:33+00:00
Venue: Neural Networks
Score: 0.729 (ignore)</description><content:encoded>&lt;p&gt;&lt;strong&gt;Authors:&lt;/strong&gt; Yuhang Zhang; Zitong Yu; Mingtong Dai; Yue Sun; Tao Tan&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Journal:&lt;/strong&gt; Neural Networks&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;a href="https://doi.org/10.1016/j.neunet.2026.108687"&gt;10.1016/j.neunet.2026.108687&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Relevance:&lt;/strong&gt; 0.729 (ignore)&lt;/p&gt;
&lt;p&gt;In recent years, pre-trained large language models (LLMs) are gradually introduced into time series analysis, and researchers have observed their significant potential for multi-task reasoning, particularly in handling complex symbolic sequences. However, how to effectively and deeply leverage the contextual reasoning capabilities of LLMs in time series data remains a key challenge. To address this challenge, this study proposes the TF-LLM framework to handle various tasks in time series, such as forecasting, classification, imputation, and anomaly detection. We innovatively integrate the strengths of both time and frequency domains: frequency representations simplify data complexity and enhance the capture of global and local periodic patterns, while time modeling addresses fine-grained dependencies, mitigating the effects of non-stationarity. Additionally, to enhance the model’s reasoning capabilities, we introduce prompt learning to enrich the contextual information of inputs and help the LLMs better understand time series data. We conduct extensive multi-task experiments on seven benchmark datasets, covering tasks like forecasting, classification, imputation, and anomaly detection. The results indicate the superior performance of the proposed TF-LLM in handling complex time series tasks, outperforming several existing methods.&lt;/p&gt;</content:encoded></item><item><title>A near-real-time multi-temporal polarimetric InSAR method for landslides monitoring in rapid-decorrelation scenarios</title><link>https://doi.org/10.1016/j.isprsjprs.2026.02.006</link><guid>10.1016/j.isprsjprs.2026.02.006</guid><pubDate>Mon, 09 Feb 2026 08:10:52 +0000</pubDate><dc:creator>Yaogang Chen</dc:creator><dc:creator>Jun Hu</dc:creator><dc:creator>Jordi J. Mallorqui</dc:creator><dc:creator>Haiqiang Fu</dc:creator><dc:creator>Wanji Zheng</dc:creator><dc:creator>Aoqing Guo</dc:creator><prism:publicationName>ISPRS Journal of Photogrammetry and Remote Sensing</prism:publicationName><prism:doi>10.1016/j.isprsjprs.2026.02.006</prism:doi><description>Interferometric synthetic aperture radar (InSAR) technology can measure ground deformation with high precision over wide areas, which is essential for understanding natural hazards and ensuring infrastructure safety. However, in regions with dense vegetation or frequent surface changes, the radar echoes lose stability over time due to temporal decorrelation. This severely limits the reliability and accuracy of InSAR measurements. Many advanced processing methods have been developed to address this issue, and while they work well in stable conditions, their performance degrades sharply when coherence is lost rapidly. To overcome this limitation, this study proposes a near-real-time sequential multi-temporal polarimetric InSAR (MT-PolInSAR) method tailored for such conditions. For each new acquisition, a stack comprising only the latest images is formed, and statistically homogeneous pixels are reselected dynamically to adapt to evolving scattering mechanisms. A sequential polarimetric-temporal phase optimization is then applied within the stack that confines estimation to short, high-coherence windows and avoids coherence loss between stacks, thereby reducing the effect of fast temporal decorrelation. Deformation time series are subsequently updated through a sequential least squares (LS) inversion using only the newly formed interferograms, which eliminates the need to reprocess the whole dataset and enables timely updates. Experiments with simulated data and full-polarization ALOS-2 and dual-polarization Sentinel-1 images over Fengjie, China, demonstrate that the proposed method significantly increases coherent pixel density and improves deformation accuracy in rapid-decorrelation areas, while enabling genuine near-real-time monitoring with a more efficient processing strategy.
Published: 2026-02-09T08:10:52+00:00
Venue: ISPRS Journal of Photogrammetry and Remote Sensing
Score: 0.729 (ignore)</description><content:encoded>&lt;p&gt;&lt;strong&gt;Authors:&lt;/strong&gt; Yaogang Chen; Jun Hu; Jordi J. Mallorqui; Haiqiang Fu; Wanji Zheng; Aoqing Guo&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Journal:&lt;/strong&gt; ISPRS Journal of Photogrammetry and Remote Sensing&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;a href="https://doi.org/10.1016/j.isprsjprs.2026.02.006"&gt;10.1016/j.isprsjprs.2026.02.006&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Relevance:&lt;/strong&gt; 0.729 (ignore)&lt;/p&gt;
&lt;p&gt;Interferometric synthetic aperture radar (InSAR) technology can measure ground deformation with high precision over wide areas, which is essential for understanding natural hazards and ensuring infrastructure safety. However, in regions with dense vegetation or frequent surface changes, the radar echoes lose stability over time due to temporal decorrelation. This severely limits the reliability and accuracy of InSAR measurements. Many advanced processing methods have been developed to address this issue, and while they work well in stable conditions, their performance degrades sharply when coherence is lost rapidly. To overcome this limitation, this study proposes a near-real-time sequential multi-temporal polarimetric InSAR (MT-PolInSAR) method tailored for such conditions. For each new acquisition, a stack comprising only the latest images is formed, and statistically homogeneous pixels are reselected dynamically to adapt to evolving scattering mechanisms. A sequential polarimetric-temporal phase optimization is then applied within the stack that confines estimation to short, high-coherence windows and avoids coherence loss between stacks, thereby reducing the effect of fast temporal decorrelation. Deformation time series are subsequently updated through a sequential least squares (LS) inversion using only the newly formed interferograms, which eliminates the need to reprocess the whole dataset and enables timely updates. Experiments with simulated data and full-polarization ALOS-2 and dual-polarization Sentinel-1 images over Fengjie, China, demonstrate that the proposed method significantly increases coherent pixel density and improves deformation accuracy in rapid-decorrelation areas, while enabling genuine near-real-time monitoring with a more efficient processing strategy.&lt;/p&gt;</content:encoded></item><item><title>Enhancing masked autoencoders with kolmogorov-arnold networks and metric learning for robust hyperspectral analysis</title><link>https://doi.org/10.1080/01431161.2026.2619149</link><guid>10.1080/01431161.2026.2619149</guid><pubDate>Mon, 09 Feb 2026 06:05:20 +0000</pubDate><dc:creator>Shibwabo C. Anyembe</dc:creator><dc:creator>Bin Zou</dc:creator><dc:creator>Jorge Abraham Rios Suarez</dc:creator><prism:publicationName>International Journal of Remote Sensing</prism:publicationName><prism:doi>10.1080/01431161.2026.2619149</prism:doi><description>The accelerating impacts of climate change and human activities on marine and coastal ecosystems demand advanced monitoring strategies that are both accurate and sustainable. Hyperspectral imaging (HSI) offers unparalleled capability to characterize water constituents, benthic habitats, and coastal vegetation, yet its application in aquatic environments is challenged by spectral redundancy, spatial heterogeneity, and severe domain shift. To address these limitations, we propose a KAN-based Masked Autoencoder (KAN-MAE) framework that integrates artificial intelligence and machine learning innovations into ocean remote sensing. The model employs decoupled spectral and spatial encoders with adaptive spectral masking, cross-attention fusion, and KAN-enhanced Transformer blocks for robust representation learning. During pre-training, Brownian distance covariance (BDC) regularization preserves nonlinear inter-band dependencies, while fine-tuning leverages a lightweight KAN classifier optimized with cross-entropy and Gaussian-kernel triplet loss to enhance discriminability under scarce labels. Experiments on Gaofen-5 coastal imagery and benchmark HSI datasets demonstrate improved classification accuracy, resilience to noise, and strong generalization across domains. By advancing AI-driven self-supervised learning for ocean remote sensing, this work supports operational monitoring of coastal ecosystems, sustainable fisheries, and blue carbon management, aligning with the UN Sustainable Development Goals on climate action (SDG 13) and life below water (SDG 14).
Published: 2026-02-09T06:05:20+00:00
Venue: International Journal of Remote Sensing
Score: 0.727 (ignore)</description><content:encoded>&lt;p&gt;&lt;strong&gt;Authors:&lt;/strong&gt; Shibwabo C. Anyembe; Bin Zou; Jorge Abraham Rios Suarez&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Journal:&lt;/strong&gt; International Journal of Remote Sensing&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;a href="https://doi.org/10.1080/01431161.2026.2619149"&gt;10.1080/01431161.2026.2619149&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Relevance:&lt;/strong&gt; 0.727 (ignore)&lt;/p&gt;
&lt;p&gt;The accelerating impacts of climate change and human activities on marine and coastal ecosystems demand advanced monitoring strategies that are both accurate and sustainable. Hyperspectral imaging (HSI) offers unparalleled capability to characterize water constituents, benthic habitats, and coastal vegetation, yet its application in aquatic environments is challenged by spectral redundancy, spatial heterogeneity, and severe domain shift. To address these limitations, we propose a KAN-based Masked Autoencoder (KAN-MAE) framework that integrates artificial intelligence and machine learning innovations into ocean remote sensing. The model employs decoupled spectral and spatial encoders with adaptive spectral masking, cross-attention fusion, and KAN-enhanced Transformer blocks for robust representation learning. During pre-training, Brownian distance covariance (BDC) regularization preserves nonlinear inter-band dependencies, while fine-tuning leverages a lightweight KAN classifier optimized with cross-entropy and Gaussian-kernel triplet loss to enhance discriminability under scarce labels. Experiments on Gaofen-5 coastal imagery and benchmark HSI datasets demonstrate improved classification accuracy, resilience to noise, and strong generalization across domains. By advancing AI-driven self-supervised learning for ocean remote sensing, this work supports operational monitoring of coastal ecosystems, sustainable fisheries, and blue carbon management, aligning with the UN Sustainable Development Goals on climate action (SDG 13) and life below water (SDG 14).&lt;/p&gt;</content:encoded></item><item><title>CDadam: Central difference adam algorithm for physics-informed neural networks</title><link>https://doi.org/10.1016/j.neucom.2026.132969</link><guid>10.1016/j.neucom.2026.132969</guid><pubDate>Mon, 09 Feb 2026 08:08:57 +0000</pubDate><dc:creator>Mengjia Zhao</dc:creator><dc:creator>Yuqiu Shen</dc:creator><dc:creator>Majid Ahmed Khan</dc:creator><dc:creator>Yuanzheng Lou</dc:creator><dc:creator>Fangdan Dai</dc:creator><dc:creator>Jiacheng Weng</dc:creator><dc:creator>Jianhong Wang</dc:creator><prism:publicationName>Neurocomputing</prism:publicationName><prism:doi>10.1016/j.neucom.2026.132969</prism:doi><description>In deep learning, the accuracy of gradient estimation directly affects the convergence behavior of optimizers and the final performance of models. As a representative adaptive optimizer, Adam excels at handling sparse gradients, but its reliance on first-order gradient approximations makes it vulnerable to stochastic noise and one-sided estimation errors. These issues may slow down convergence or distort parameter updates. To address these limitations, we propose central difference Adam algorithm (CDadam), which integrates central differences into Adam’s gradient computation process. We perform a theoretical analysis on it and numerical simulations prove that CDadam not only converges quickly, but also has high accuracy with global convergence ability. Then, the CDadam algorithm is applied to Physics-Informed Neural Networks (PINNs) for solving multiple partial differential equations. The results reveal that the proposed CDadam shows higher accuracy and robustness than other four mainstream optimizers, which proves the effectiveness of CDadam. The code of the CDadam is available at https://github.com/LYZ-NTU/CDadam-algorithm/tree/main .
Published: 2026-02-09T08:08:57+00:00
Venue: Neurocomputing
Score: 0.722 (ignore)</description><content:encoded>&lt;p&gt;&lt;strong&gt;Authors:&lt;/strong&gt; Mengjia Zhao; Yuqiu Shen; Majid Ahmed Khan; Yuanzheng Lou; Fangdan Dai; Jiacheng Weng; Jianhong Wang&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Journal:&lt;/strong&gt; Neurocomputing&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;a href="https://doi.org/10.1016/j.neucom.2026.132969"&gt;10.1016/j.neucom.2026.132969&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Relevance:&lt;/strong&gt; 0.722 (ignore)&lt;/p&gt;
&lt;p&gt;In deep learning, the accuracy of gradient estimation directly affects the convergence behavior of optimizers and the final performance of models. As a representative adaptive optimizer, Adam excels at handling sparse gradients, but its reliance on first-order gradient approximations makes it vulnerable to stochastic noise and one-sided estimation errors. These issues may slow down convergence or distort parameter updates. To address these limitations, we propose central difference Adam algorithm (CDadam), which integrates central differences into Adam’s gradient computation process. We perform a theoretical analysis on it and numerical simulations prove that CDadam not only converges quickly, but also has high accuracy with global convergence ability. Then, the CDadam algorithm is applied to Physics-Informed Neural Networks (PINNs) for solving multiple partial differential equations. The results reveal that the proposed CDadam shows higher accuracy and robustness than other four mainstream optimizers, which proves the effectiveness of CDadam. The code of the CDadam is available at https://github.com/LYZ-NTU/CDadam-algorithm/tree/main .&lt;/p&gt;</content:encoded></item><item><title>ZPD-guided adversarial learning for safety-critical autonomous driving</title><link>https://doi.org/10.1016/j.eswa.2026.131547</link><guid>10.1016/j.eswa.2026.131547</guid><pubDate>Sun, 08 Feb 2026 22:54:02 +0000</pubDate><dc:creator>Wei Wu</dc:creator><dc:creator>Xiaohui Hou</dc:creator><dc:creator>Minggang Gan</dc:creator><dc:creator>Jie Chen</dc:creator><prism:publicationName>Expert Systems with Applications</prism:publicationName><prism:doi>10.1016/j.eswa.2026.131547</prism:doi><description>Ensuring the safety and robustness of autonomous vehicles (AVs) in complex and safety–critical driving scenarios remains a fundamental challenge in the advancement of autonomous driving technology. Traditional training methods often exhibit limitations in coping with uncertainty and rare extreme events encountered in real-world driving environments. To address these challenges, this paper proposes an adversarial learning framework guided by the Zone of Proximal Development (ZPD), aiming to enhance the adaptability and robustness of autonomous driving decision-making policies in complex environments. Specifically, the proposed approach embeds ZPD-inspired guidance into adversarial learning to generate safety–critical traffic interactions that are both extreme and learnable. To regulate adversarial behaviors and maintain a balance between challenge and solvability, the framework incorporates structured constraints based on the Ideal Return Ceiling (IRC) and fine-grained collision severity modeling. Furthermore, a Vehicle Potential Threat Level (VPTL) mechanism is employed to adaptively adjust adversarial training difficulty in accordance with the evolving capability of the ego vehicle, thereby facilitating continuous learning and policy adaptation. Experimental results indicate that, compared with representative baseline methods such as SAC and TD3, the proposed approach reduces the Damage Index by approximately 20–40% across a wide range of evaluation settings, while simultaneously lowering collision severity and maintaining task executability. These results suggest that the proposed framework provides a viable approach for improving safety-oriented learning behavior in complex traffic environments.
Published: 2026-02-08T22:54:02+00:00
Venue: Expert Systems with Applications
Score: 0.721 (ignore)</description><content:encoded>&lt;p&gt;&lt;strong&gt;Authors:&lt;/strong&gt; Wei Wu; Xiaohui Hou; Minggang Gan; Jie Chen&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Journal:&lt;/strong&gt; Expert Systems with Applications&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;a href="https://doi.org/10.1016/j.eswa.2026.131547"&gt;10.1016/j.eswa.2026.131547&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Relevance:&lt;/strong&gt; 0.721 (ignore)&lt;/p&gt;
&lt;p&gt;Ensuring the safety and robustness of autonomous vehicles (AVs) in complex and safety–critical driving scenarios remains a fundamental challenge in the advancement of autonomous driving technology. Traditional training methods often exhibit limitations in coping with uncertainty and rare extreme events encountered in real-world driving environments. To address these challenges, this paper proposes an adversarial learning framework guided by the Zone of Proximal Development (ZPD), aiming to enhance the adaptability and robustness of autonomous driving decision-making policies in complex environments. Specifically, the proposed approach embeds ZPD-inspired guidance into adversarial learning to generate safety–critical traffic interactions that are both extreme and learnable. To regulate adversarial behaviors and maintain a balance between challenge and solvability, the framework incorporates structured constraints based on the Ideal Return Ceiling (IRC) and fine-grained collision severity modeling. Furthermore, a Vehicle Potential Threat Level (VPTL) mechanism is employed to adaptively adjust adversarial training difficulty in accordance with the evolving capability of the ego vehicle, thereby facilitating continuous learning and policy adaptation. Experimental results indicate that, compared with representative baseline methods such as SAC and TD3, the proposed approach reduces the Damage Index by approximately 20–40% across a wide range of evaluation settings, while simultaneously lowering collision severity and maintaining task executability. These results suggest that the proposed framework provides a viable approach for improving safety-oriented learning behavior in complex traffic environments.&lt;/p&gt;</content:encoded></item><item><title>Odin: Oriented dual-module integration for text-rich network representation learning</title><link>https://doi.org/10.1016/j.neucom.2026.133018</link><guid>10.1016/j.neucom.2026.133018</guid><pubDate>Mon, 09 Feb 2026 08:09:22 +0000</pubDate><dc:creator>Kaifeng Hong</dc:creator><dc:creator>Yinglong Zhang</dc:creator><dc:creator>Xiaoying Hong</dc:creator><dc:creator>Xuewen Xia</dc:creator><dc:creator>Xing Xu</dc:creator><prism:publicationName>Neurocomputing</prism:publicationName><prism:doi>10.1016/j.neucom.2026.133018</prism:doi><description>Text-attributed graphs require models to effectively combine strong textual understanding with structurally informed reasoning. Existing approaches either rely on GNNs—limited by over-smoothing and hop-dependent diffusion—or employ Transformers that largely overlook graph topology and treat nodes as isolated sequences. We propose Odin ( O riented d ual-module in tegration), a new architecture that injects graph structure into Transformers at selected depths through an oriented dual-module mechanism. Unlike message-passing GNNs, Odin does not rely on multi-hop diffusion; instead, multi-hop structures are integrated at specific Transformer layers, yielding low-, mid-, and high-level structural abstraction aligned with the model’s semantic hierarchy. Because aggregation operates on node-specific [CLS] representations induced by textual tokens, Odin mitigates over-smoothing by preventing the iterative diffusion of homogeneous hidden states, and decouples structural abstraction from neighborhood size or graph topology. We further establish that Odin’s expressive power strictly contains that of both pure Transformers and GNNs. To make the design efficient in large-scale or low-resource settings, we introduce Light Odin, a lightweight variant that preserves the same layer-aligned structural abstraction for faster training and inference. Experiments on multiple text-rich graph benchmarks show that Odin achieves state-of-the-art accuracy, while Light Odin delivers competitive performance with significantly reduced computational cost. Together, Odin and Light Odin form a unified, hop-free framework for principled structure–text integration. The source code of this model has been released at https://github.com/hongkaifeng/Odin .
Published: 2026-02-09T08:09:22+00:00
Venue: Neurocomputing
Score: 0.716 (ignore)</description><content:encoded>&lt;p&gt;&lt;strong&gt;Authors:&lt;/strong&gt; Kaifeng Hong; Yinglong Zhang; Xiaoying Hong; Xuewen Xia; Xing Xu&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Journal:&lt;/strong&gt; Neurocomputing&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;a href="https://doi.org/10.1016/j.neucom.2026.133018"&gt;10.1016/j.neucom.2026.133018&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Relevance:&lt;/strong&gt; 0.716 (ignore)&lt;/p&gt;
&lt;p&gt;Text-attributed graphs require models to effectively combine strong textual understanding with structurally informed reasoning. Existing approaches either rely on GNNs—limited by over-smoothing and hop-dependent diffusion—or employ Transformers that largely overlook graph topology and treat nodes as isolated sequences. We propose Odin ( O riented d ual-module in tegration), a new architecture that injects graph structure into Transformers at selected depths through an oriented dual-module mechanism. Unlike message-passing GNNs, Odin does not rely on multi-hop diffusion; instead, multi-hop structures are integrated at specific Transformer layers, yielding low-, mid-, and high-level structural abstraction aligned with the model’s semantic hierarchy. Because aggregation operates on node-specific [CLS] representations induced by textual tokens, Odin mitigates over-smoothing by preventing the iterative diffusion of homogeneous hidden states, and decouples structural abstraction from neighborhood size or graph topology. We further establish that Odin’s expressive power strictly contains that of both pure Transformers and GNNs. To make the design efficient in large-scale or low-resource settings, we introduce Light Odin, a lightweight variant that preserves the same layer-aligned structural abstraction for faster training and inference. Experiments on multiple text-rich graph benchmarks show that Odin achieves state-of-the-art accuracy, while Light Odin delivers competitive performance with significantly reduced computational cost. Together, Odin and Light Odin form a unified, hop-free framework for principled structure–text integration. The source code of this model has been released at https://github.com/hongkaifeng/Odin .&lt;/p&gt;</content:encoded></item><item><title>ILrLSUMM+: A NER-Infused Multi-Objective Paradigm to Summarize News in Low-Resource Indian Languages</title><link>https://doi.org/10.1016/j.knosys.2026.115510</link><guid>10.1016/j.knosys.2026.115510</guid><pubDate>Sun, 08 Feb 2026 22:54:01 +0000</pubDate><dc:creator>Jiten Parmar</dc:creator><dc:creator>Naveen Saini</dc:creator><dc:creator>Dhananjoy Dey</dc:creator><dc:creator>Diego Oliva</dc:creator><dc:creator>Omkeshwar</dc:creator><prism:publicationName>Knowledge-Based Systems</prism:publicationName><prism:doi>10.1016/j.knosys.2026.115510</prism:doi><description>Summarizing news articles in low-resource Indian languages presents a unique challenge due to their diverse syntax and semantics. Traditional summarization techniques struggle to adapt, necessitating a smarter, optimization-driven approach to extract concise and meaningful summaries. This paper introduces ILrLSUMM+ , a novel evolutionary-inspired framework for news summarization in Indian low-resource languages, specifically Hindi and Gujarati. By leveraging a multi-objective optimization paradigm powered by the Differential Evolution (DE) algorithm, the framework balances multiple key factors: term frequency-inverse document frequency (TF-IDF) score, sentence-to-title similarity, thematic relevance, and diversity. Furthermore, Named Entity Recognition (NER) is strategically integrated to prioritize essential entities, ensuring summaries retain critical contextual information. This fusion of optimization and linguistic intelligence paves the way for more effective and adaptive summarization techniques in underrepresented languages. Harnessing the power of unsupervised learning, our method utilize 500 articles per language from the M3LS dataset, enabling a robust comparative analysis against single-objective optimization techniques, graph-based models, and transformer-driven large language models (LLMs). Performance evaluation through ROUGE scores reveals a breakthrough in low-resource language summarization–ILrLSUMM+ surpasses the state-of-the-art TGraph model, achieving a 25.17% boost in ROUGE-1 F1 score for Hindi and 27.98% for Gujarati. These results underscore the transformative potential of our multi-objective optimization approach in reshaping text summarization for underrepresented languages. Furthermore, an in-depth qualitative analysis sheds light on the architectural innovations driving this leap forward.
Published: 2026-02-08T22:54:01+00:00
Venue: Knowledge-Based Systems
Score: 0.705 (ignore)</description><content:encoded>&lt;p&gt;&lt;strong&gt;Authors:&lt;/strong&gt; Jiten Parmar; Naveen Saini; Dhananjoy Dey; Diego Oliva; Omkeshwar&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Journal:&lt;/strong&gt; Knowledge-Based Systems&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;a href="https://doi.org/10.1016/j.knosys.2026.115510"&gt;10.1016/j.knosys.2026.115510&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Relevance:&lt;/strong&gt; 0.705 (ignore)&lt;/p&gt;
&lt;p&gt;Summarizing news articles in low-resource Indian languages presents a unique challenge due to their diverse syntax and semantics. Traditional summarization techniques struggle to adapt, necessitating a smarter, optimization-driven approach to extract concise and meaningful summaries. This paper introduces ILrLSUMM+ , a novel evolutionary-inspired framework for news summarization in Indian low-resource languages, specifically Hindi and Gujarati. By leveraging a multi-objective optimization paradigm powered by the Differential Evolution (DE) algorithm, the framework balances multiple key factors: term frequency-inverse document frequency (TF-IDF) score, sentence-to-title similarity, thematic relevance, and diversity. Furthermore, Named Entity Recognition (NER) is strategically integrated to prioritize essential entities, ensuring summaries retain critical contextual information. This fusion of optimization and linguistic intelligence paves the way for more effective and adaptive summarization techniques in underrepresented languages. Harnessing the power of unsupervised learning, our method utilize 500 articles per language from the M3LS dataset, enabling a robust comparative analysis against single-objective optimization techniques, graph-based models, and transformer-driven large language models (LLMs). Performance evaluation through ROUGE scores reveals a breakthrough in low-resource language summarization–ILrLSUMM+ surpasses the state-of-the-art TGraph model, achieving a 25.17% boost in ROUGE-1 F1 score for Hindi and 27.98% for Gujarati. These results underscore the transformative potential of our multi-objective optimization approach in reshaping text summarization for underrepresented languages. Furthermore, an in-depth qualitative analysis sheds light on the architectural innovations driving this leap forward.&lt;/p&gt;</content:encoded></item><item><title>DCM-Net: A Dual-Branch Cross-Scale Guidance Mamba Network for Lithium-Ion Battery State of Health Estimation</title><link>https://doi.org/10.1016/j.eswa.2026.131470</link><guid>10.1016/j.eswa.2026.131470</guid><pubDate>Mon, 09 Feb 2026 08:09:52 +0000</pubDate><dc:creator>Xie Haofei</dc:creator><dc:creator>Li Zhi-hao</dc:creator><dc:creator>Hou Jie</dc:creator><prism:publicationName>Expert Systems with Applications</prism:publicationName><prism:doi>10.1016/j.eswa.2026.131470</prism:doi><description>Accurate state-of-health (SOH) prediction for lithium-ion batteries is critical for electric vehicle safety. Existing data-driven methods, however, struggle with long-range dependency modeling, feature interaction characterization, and multi-scale information decoupling when processing full-lifecycle battery data. To address these challenges, this paper proposes DCM-Net, a novel Dual-Branch Cross-Scale Guided Mamba Network. The framework initially employs feature engineering to extract health factors (HFs) that are highly correlated with battery degradation, then utilizes a convolutional feature extraction module to learn independent and cross features. Each feature representation is decomposed into multiple scales through a learnable wavelet decomposition module and processed via a coarse-to-fine cross-scale guided decoding mechanism. All encoders and decoders within this process are constructed based on the Mamba architecture to efficiently capture long-range temporal dependencies. Finally, the outputs from the dual data streams are integrated by a fusion module to enable collaborative prediction of SOH. Extensive experiments on four public datasets (NASA, CALCE, XJTU, and TJU) demonstrate that DCM-Net achieves state-of-the-art performance across all test scenarios. It demonstrates excellent accuracy and robustness with respect to individual cell differences, unknown dynamic conditions, and generalization across chemical systems.
Published: 2026-02-09T08:09:52+00:00
Venue: Expert Systems with Applications
Score: 0.701 (ignore)</description><content:encoded>&lt;p&gt;&lt;strong&gt;Authors:&lt;/strong&gt; Xie Haofei; Li Zhi-hao; Hou Jie&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Journal:&lt;/strong&gt; Expert Systems with Applications&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;a href="https://doi.org/10.1016/j.eswa.2026.131470"&gt;10.1016/j.eswa.2026.131470&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Relevance:&lt;/strong&gt; 0.701 (ignore)&lt;/p&gt;
&lt;p&gt;Accurate state-of-health (SOH) prediction for lithium-ion batteries is critical for electric vehicle safety. Existing data-driven methods, however, struggle with long-range dependency modeling, feature interaction characterization, and multi-scale information decoupling when processing full-lifecycle battery data. To address these challenges, this paper proposes DCM-Net, a novel Dual-Branch Cross-Scale Guided Mamba Network. The framework initially employs feature engineering to extract health factors (HFs) that are highly correlated with battery degradation, then utilizes a convolutional feature extraction module to learn independent and cross features. Each feature representation is decomposed into multiple scales through a learnable wavelet decomposition module and processed via a coarse-to-fine cross-scale guided decoding mechanism. All encoders and decoders within this process are constructed based on the Mamba architecture to efficiently capture long-range temporal dependencies. Finally, the outputs from the dual data streams are integrated by a fusion module to enable collaborative prediction of SOH. Extensive experiments on four public datasets (NASA, CALCE, XJTU, and TJU) demonstrate that DCM-Net achieves state-of-the-art performance across all test scenarios. It demonstrates excellent accuracy and robustness with respect to individual cell differences, unknown dynamic conditions, and generalization across chemical systems.&lt;/p&gt;</content:encoded></item><item><title>From Apparent to Real: A New Path for Real Personality Recognition in Robot Perception</title><link>https://doi.org/10.1016/j.patcog.2026.113255</link><guid>10.1016/j.patcog.2026.113255</guid><pubDate>Mon, 09 Feb 2026 08:03:15 +0000</pubDate><dc:creator>Yunjia Sun</dc:creator><dc:creator>Shaohui Peng</dc:creator><dc:creator>Tao Wang</dc:creator><prism:publicationName>Pattern Recognition</prism:publicationName><prism:doi>10.1016/j.patcog.2026.113255</prism:doi><description>Accurately recognizing real personality is crucial in robot perception for personalized human-robot interaction. Training high-performance personality recognition models typically requires abundant data. However, real personality can only be obtained through self-reports, collecting such data is challenging. To address this, we present a novel approach that leverages apparent personality data to achieve real personality recognition. Apparent personality is the personality perceived by external observers. It is easier to collect because it does not rely on self-reports and allows a single annotator to label multiple individuals. Specifically, we propose to aggregate apparent personality annotations across multiple videos of a person into a unified personality representation of that individual. This aggregated personality is then used to supervise the training of the personality recognition model. However, direct training may encounter two types of deviations: the label deviation and the sample deviation. Label deviation refers to the gap between the real personality conveyed by each sample and the aggregated personality. Sample deviation indicates that individual samples contribute differently to the aggregated personality. Therefore, we introduce two methods to account for possible deviations of the label and the sample, respectively. For label deviation, we propose to additionally predict the label deviation for each sample in order to correct the original aggregated personality. For sample deviation, we propose applying a weighted loss to reduce the impact of samples whose apparent personality deviates significantly from the aggregated personality. We use the same aggregation strategy to combine predictions from different video clips of a person into a unified real personality result. Experiments on standard benchmarks ELEA demonstrate the effectiveness of our approach, outperforming the model trained directly on apparent personality labels. Our method, without access to real personality labels, achieves only a performance drop of around 3% compared to a fully supervised real personality model, showing the viability of our method in label-scarce environments. The code and data split in the experiment are available at https://github.com/sunyunjia96/App2Real .
Published: 2026-02-09T08:03:15+00:00
Venue: Pattern Recognition
Score: 0.698 (ignore)</description><content:encoded>&lt;p&gt;&lt;strong&gt;Authors:&lt;/strong&gt; Yunjia Sun; Shaohui Peng; Tao Wang&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Journal:&lt;/strong&gt; Pattern Recognition&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;a href="https://doi.org/10.1016/j.patcog.2026.113255"&gt;10.1016/j.patcog.2026.113255&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Relevance:&lt;/strong&gt; 0.698 (ignore)&lt;/p&gt;
&lt;p&gt;Accurately recognizing real personality is crucial in robot perception for personalized human-robot interaction. Training high-performance personality recognition models typically requires abundant data. However, real personality can only be obtained through self-reports, collecting such data is challenging. To address this, we present a novel approach that leverages apparent personality data to achieve real personality recognition. Apparent personality is the personality perceived by external observers. It is easier to collect because it does not rely on self-reports and allows a single annotator to label multiple individuals. Specifically, we propose to aggregate apparent personality annotations across multiple videos of a person into a unified personality representation of that individual. This aggregated personality is then used to supervise the training of the personality recognition model. However, direct training may encounter two types of deviations: the label deviation and the sample deviation. Label deviation refers to the gap between the real personality conveyed by each sample and the aggregated personality. Sample deviation indicates that individual samples contribute differently to the aggregated personality. Therefore, we introduce two methods to account for possible deviations of the label and the sample, respectively. For label deviation, we propose to additionally predict the label deviation for each sample in order to correct the original aggregated personality. For sample deviation, we propose applying a weighted loss to reduce the impact of samples whose apparent personality deviates significantly from the aggregated personality. We use the same aggregation strategy to combine predictions from different video clips of a person into a unified real personality result. Experiments on standard benchmarks ELEA demonstrate the effectiveness of our approach, outperforming the model trained directly on apparent personality labels. Our method, without access to real personality labels, achieves only a performance drop of around 3% compared to a fully supervised real personality model, showing the viability of our method in label-scarce environments. The code and data split in the experiment are available at https://github.com/sunyunjia96/App2Real .&lt;/p&gt;</content:encoded></item><item><title>Smart green Guardian: leveraging transfer learning for early plant disease detection in kitchen gardens</title><link>https://doi.org/10.1007/s00521-025-11796-0</link><guid>10.1007/s00521-025-11796-0</guid><pubDate>Mon, 09 Feb 2026 04:33:11 +0000</pubDate><dc:creator>Megha Singh Chauhan</dc:creator><dc:creator>K. Srinivas</dc:creator><dc:creator>A. Charan Kumari</dc:creator><prism:publicationName>Neural Computing and Applications</prism:publicationName><prism:doi>10.1007/s00521-025-11796-0</prism:doi><description>Early detection of diseases in kitchen garden plants is critical for optimising yields and ensuring plant health. This research investigates the efficacy of state-of-the-art transfer learning models for accurate disease detection across five common plant species: Tomato, Beans, Bell Pepper, Ladyfinger, and Cauliflower. Five Transfer Learning models—DenseNet121, InceptionV3, MobileNetV2, ResNet50V2, and InceptionResNetV2—were evaluated using a curated dataset comprising images of healthy and diseased leaves. The Tomato dataset was sourced from the PlantVillage database, containing 25,336 images across ten classes, including nine disease categories and one healthy class. The Beans dataset, obtained from the Makerere University Beans Image Dataset, consists of 14,129 images with diverse disease manifestations. The Ladyfinger dataset comprises 1949 images from a publicly available source on Kaggle, depicting healthy leaves and those affected by Yellow Vein Mosaic Disease. The Cauliflower dataset includes 7360 images collected from VegNet, featuring images of healthy leaves and those affected by Downy Mildew, Black Rot, and Bacterial Spot Rot. Lastly, the Bell Pepper dataset consists of 2475 images from the PlantVillage database, with images classified as diseased or healthy. The findings indicate that MobileNetV2 outperformed the other models for Beans (96.75%) and Bell Pepper (99.60%), achieving an accuracy of 99.80% on the Tomato dataset. Furthermore, four models, including MobileNetV2, achieved 100% accuracy on the Cauliflower dataset. DenseNet121 demonstrated significant performance on Ladyfinger, tying with ResNet50V2 at 99.49%. However, InceptionResNetV2 exhibited inconsistent results, particularly with a low accuracy of 93.07% for the Beans dataset. This research, thus, focuses on collecting and preprocessing high-quality datasets for the selected plant species, demonstrating effective transfer learning models for leaf disease detection, and providing insights into the suitability of each architecture for specific datasets. Thus, by leveraging transfer learning techniques, this research contributes to the growing knowledge of agricultural technology, addressing a critical need for efficient disease management in kitchen gardens.&amp;nbsp;The implications of this study suggest that integrating accurate plant disease detection models into mobile applications can provide timely information to kitchen gardeners.
Published: 2026-02-09T04:33:11+00:00
Venue: Neural Computing and Applications
Score: 0.698 (ignore)</description><content:encoded>&lt;p&gt;&lt;strong&gt;Authors:&lt;/strong&gt; Megha Singh Chauhan; K. Srinivas; A. Charan Kumari&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Journal:&lt;/strong&gt; Neural Computing and Applications&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;a href="https://doi.org/10.1007/s00521-025-11796-0"&gt;10.1007/s00521-025-11796-0&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Relevance:&lt;/strong&gt; 0.698 (ignore)&lt;/p&gt;
&lt;p&gt;Early detection of diseases in kitchen garden plants is critical for optimising yields and ensuring plant health. This research investigates the efficacy of state-of-the-art transfer learning models for accurate disease detection across five common plant species: Tomato, Beans, Bell Pepper, Ladyfinger, and Cauliflower. Five Transfer Learning models—DenseNet121, InceptionV3, MobileNetV2, ResNet50V2, and InceptionResNetV2—were evaluated using a curated dataset comprising images of healthy and diseased leaves. The Tomato dataset was sourced from the PlantVillage database, containing 25,336 images across ten classes, including nine disease categories and one healthy class. The Beans dataset, obtained from the Makerere University Beans Image Dataset, consists of 14,129 images with diverse disease manifestations. The Ladyfinger dataset comprises 1949 images from a publicly available source on Kaggle, depicting healthy leaves and those affected by Yellow Vein Mosaic Disease. The Cauliflower dataset includes 7360 images collected from VegNet, featuring images of healthy leaves and those affected by Downy Mildew, Black Rot, and Bacterial Spot Rot. Lastly, the Bell Pepper dataset consists of 2475 images from the PlantVillage database, with images classified as diseased or healthy. The findings indicate that MobileNetV2 outperformed the other models for Beans (96.75%) and Bell Pepper (99.60%), achieving an accuracy of 99.80% on the Tomato dataset. Furthermore, four models, including MobileNetV2, achieved 100% accuracy on the Cauliflower dataset. DenseNet121 demonstrated significant performance on Ladyfinger, tying with ResNet50V2 at 99.49%. However, InceptionResNetV2 exhibited inconsistent results, particularly with a low accuracy of 93.07% for the Beans dataset. This research, thus, focuses on collecting and preprocessing high-quality datasets for the selected plant species, demonstrating effective transfer learning models for leaf disease detection, and providing insights into the suitability of each architecture for specific datasets. Thus, by leveraging transfer learning techniques, this research contributes to the growing knowledge of agricultural technology, addressing a critical need for efficient disease management in kitchen gardens.&amp;amp;nbsp;The implications of this study suggest that integrating accurate plant disease detection models into mobile applications can provide timely information to kitchen gardeners.&lt;/p&gt;</content:encoded></item><item><title>Enhancing the Performance of Data-Driven Liquid Loading Severity Grading Models for Shale Gas Wells Using Contrastive Learning</title><link>https://doi.org/10.1016/j.eswa.2026.131580</link><guid>10.1016/j.eswa.2026.131580</guid><pubDate>Mon, 09 Feb 2026 08:10:22 +0000</pubDate><dc:creator>Fanhui Zeng</dc:creator><dc:creator>Peng Chen</dc:creator><dc:creator>Jianchun Guo</dc:creator><dc:creator>Zhangxing John Chen</dc:creator><dc:creator>Yanqiang Wang</dc:creator><dc:creator>Chunyi Yang</dc:creator><prism:publicationName>Expert Systems with Applications</prism:publicationName><prism:doi>10.1016/j.eswa.2026.131580</prism:doi><description>As a low-pollution unconventional energy source, shale gas development relies on fracturing to address inherent low reservoir permeability. However, post-fracturing reservoir/wellbore changes combined with weak gas-liquid carrying capacity cause widespread liquid loading in shale gas wells, increasing backpressure, reducing production, and even leading to reservoir water flooding. Traditional detection methods have limitations: mechanistic models suffer from high errors due to simplified assumptions, existing data-driven models are mostly binary classifiers that fail to distinguish severity, and data-driven liquid level detection is costly. To solve these issues, this study proposes a contrastive learning-enhanced feature fusion classification model (CL-FFCM), consisting of a ”fusion network” for multi-source latent feature extraction and a ”head network” for four-level classification. The SupCon loss function enhances inter-class differences to resolve ambiguous feature boundaries. Experiments on 142 shale gas wells show CL-FFCM achieves 0.94–0.95 accuracy. Contrastive learning stably improves five mainstream models’ accuracy by 6%–9%. Field applications in southern Sichuan indicate an average early warning deviation of 3–8 hours. This model provides a reliable tool for precise liquid loading management, with great significance for efficient shale gas development.
Published: 2026-02-09T08:10:22+00:00
Venue: Expert Systems with Applications
Score: 0.686 (ignore)</description><content:encoded>&lt;p&gt;&lt;strong&gt;Authors:&lt;/strong&gt; Fanhui Zeng; Peng Chen; Jianchun Guo; Zhangxing John Chen; Yanqiang Wang; Chunyi Yang&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Journal:&lt;/strong&gt; Expert Systems with Applications&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;a href="https://doi.org/10.1016/j.eswa.2026.131580"&gt;10.1016/j.eswa.2026.131580&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Relevance:&lt;/strong&gt; 0.686 (ignore)&lt;/p&gt;
&lt;p&gt;As a low-pollution unconventional energy source, shale gas development relies on fracturing to address inherent low reservoir permeability. However, post-fracturing reservoir/wellbore changes combined with weak gas-liquid carrying capacity cause widespread liquid loading in shale gas wells, increasing backpressure, reducing production, and even leading to reservoir water flooding. Traditional detection methods have limitations: mechanistic models suffer from high errors due to simplified assumptions, existing data-driven models are mostly binary classifiers that fail to distinguish severity, and data-driven liquid level detection is costly. To solve these issues, this study proposes a contrastive learning-enhanced feature fusion classification model (CL-FFCM), consisting of a ”fusion network” for multi-source latent feature extraction and a ”head network” for four-level classification. The SupCon loss function enhances inter-class differences to resolve ambiguous feature boundaries. Experiments on 142 shale gas wells show CL-FFCM achieves 0.94–0.95 accuracy. Contrastive learning stably improves five mainstream models’ accuracy by 6%–9%. Field applications in southern Sichuan indicate an average early warning deviation of 3–8 hours. This model provides a reliable tool for precise liquid loading management, with great significance for efficient shale gas development.&lt;/p&gt;</content:encoded></item><item><title>Deep learning based empty shelf detection based on autonomous mobile robot</title><link>https://doi.org/10.1016/j.cviu.2026.104697</link><guid>10.1016/j.cviu.2026.104697</guid><pubDate>Mon, 09 Feb 2026 08:11:45 +0000</pubDate><dc:creator>Giuseppe De Simone</dc:creator><dc:creator>Alessia Saggese</dc:creator><dc:creator>Pasquale Foggia</dc:creator><dc:creator>Mario Vento</dc:creator><prism:publicationName>Computer Vision and Image Understanding</prism:publicationName><prism:doi>10.1016/j.cviu.2026.104697</prism:doi><description>The issue of out-of-stock (OOS) represents a substantial challenge for retailers, often resulting in significant sales losses. To address this problem, this paper introduces an autonomous mobile robotic platform built on the Robot Operating System (ROS) framework, designed to accelerate the restocking process in supermarkets. The platform autonomously detects empty shelves and notifies human operators, streamlining inventory management. Equipped with advanced navigation capabilities, the proposed system employs a deep learning-based, two-stage architecture that identifies shelving areas and subsequently detects empty shelves. To validate the performance of the proposed two-stage artificial vision algorithm, two datasets were used: the first comprises approximately 2000 images (900 of them collected by our team from three different supermarkets), while the second dataset consists of around 5600 manually annotated images extracted from videos recorded in a supermarket by the robotic platform itself. Additionally, in order to validate the entire robotic system, an extensive experimental evaluation was conducted in a supermarket during regular business hours. The results demonstrate that the proposed platform substantially outperforms human operators, identifying OOS items eight times faster than traditional human operator based methods. This advancement provides valuable assistance to supermarket staff, significantly enhancing operational efficiency.
Published: 2026-02-09T08:11:45+00:00
Venue: Computer Vision and Image Understanding
Score: 0.680 (ignore)</description><content:encoded>&lt;p&gt;&lt;strong&gt;Authors:&lt;/strong&gt; Giuseppe De Simone; Alessia Saggese; Pasquale Foggia; Mario Vento&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Journal:&lt;/strong&gt; Computer Vision and Image Understanding&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;a href="https://doi.org/10.1016/j.cviu.2026.104697"&gt;10.1016/j.cviu.2026.104697&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Relevance:&lt;/strong&gt; 0.680 (ignore)&lt;/p&gt;
&lt;p&gt;The issue of out-of-stock (OOS) represents a substantial challenge for retailers, often resulting in significant sales losses. To address this problem, this paper introduces an autonomous mobile robotic platform built on the Robot Operating System (ROS) framework, designed to accelerate the restocking process in supermarkets. The platform autonomously detects empty shelves and notifies human operators, streamlining inventory management. Equipped with advanced navigation capabilities, the proposed system employs a deep learning-based, two-stage architecture that identifies shelving areas and subsequently detects empty shelves. To validate the performance of the proposed two-stage artificial vision algorithm, two datasets were used: the first comprises approximately 2000 images (900 of them collected by our team from three different supermarkets), while the second dataset consists of around 5600 manually annotated images extracted from videos recorded in a supermarket by the robotic platform itself. Additionally, in order to validate the entire robotic system, an extensive experimental evaluation was conducted in a supermarket during regular business hours. The results demonstrate that the proposed platform substantially outperforms human operators, identifying OOS items eight times faster than traditional human operator based methods. This advancement provides valuable assistance to supermarket staff, significantly enhancing operational efficiency.&lt;/p&gt;</content:encoded></item><item><title>ConsistEAE: Enhancing low-resource event argument extraction with linguistically consistent demonstrations</title><link>https://doi.org/10.1016/j.neucom.2026.133005</link><guid>10.1016/j.neucom.2026.133005</guid><pubDate>Mon, 09 Feb 2026 08:09:16 +0000</pubDate><dc:creator>Yikai Guo</dc:creator><dc:creator>Xuemeng Tian</dc:creator><dc:creator>Bin Ge</dc:creator><dc:creator>Yuting Yang</dc:creator><dc:creator>Yao He</dc:creator><dc:creator>Wenjun Ke</dc:creator><dc:creator>Junjie Hu</dc:creator><dc:creator>Yanyang Li</dc:creator><dc:creator>Haoran Luo</dc:creator><prism:publicationName>Neurocomputing</prism:publicationName><prism:doi>10.1016/j.neucom.2026.133005</prism:doi><description>In the low-resource event argument extraction (EAE) task, the scarcity of labeled data restricts the accurate identification of event arguments. Although in-context learning (ICL) has shown promising performance, it fails to ensure fine-grained semantic and syntactic consistency between the selected demonstrations and the target event texts. To address this, we propose ConsistEAE, a method that identifies demonstrations by integrating weighted measures of semantic and syntactic consistency. For semantic consistency, we propose a global-local interactive representation learning approach to capture fine-grained semantic information. For syntactic consistency, we introduce a syntactic alignment approach that constructs syntactic dependency trees and assesses the syntactic consistency between event texts with tree edit distance. Experimental results show that ConsistEAE outperforms existing state-of-the-art baseline on both ACE2005-EN and ACE2005-EN+ datasets, with improvements of 1.63% in Arg-I and 2.15% in Arg-C on the ACE2005-EN dataset, along with 1.27% in Arg-I and 2.29% in Arg-C on the ACE2005-EN+ dataset.
Published: 2026-02-09T08:09:16+00:00
Venue: Neurocomputing
Score: 0.679 (ignore)</description><content:encoded>&lt;p&gt;&lt;strong&gt;Authors:&lt;/strong&gt; Yikai Guo; Xuemeng Tian; Bin Ge; Yuting Yang; Yao He; Wenjun Ke; Junjie Hu; Yanyang Li; Haoran Luo&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Journal:&lt;/strong&gt; Neurocomputing&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;a href="https://doi.org/10.1016/j.neucom.2026.133005"&gt;10.1016/j.neucom.2026.133005&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Relevance:&lt;/strong&gt; 0.679 (ignore)&lt;/p&gt;
&lt;p&gt;In the low-resource event argument extraction (EAE) task, the scarcity of labeled data restricts the accurate identification of event arguments. Although in-context learning (ICL) has shown promising performance, it fails to ensure fine-grained semantic and syntactic consistency between the selected demonstrations and the target event texts. To address this, we propose ConsistEAE, a method that identifies demonstrations by integrating weighted measures of semantic and syntactic consistency. For semantic consistency, we propose a global-local interactive representation learning approach to capture fine-grained semantic information. For syntactic consistency, we introduce a syntactic alignment approach that constructs syntactic dependency trees and assesses the syntactic consistency between event texts with tree edit distance. Experimental results show that ConsistEAE outperforms existing state-of-the-art baseline on both ACE2005-EN and ACE2005-EN+ datasets, with improvements of 1.63% in Arg-I and 2.15% in Arg-C on the ACE2005-EN dataset, along with 1.27% in Arg-I and 2.29% in Arg-C on the ACE2005-EN+ dataset.&lt;/p&gt;</content:encoded></item><item><title>10-minute level and near real-Time wildfire detection: integrating multiple features for Himawari-8/9 satellite</title><link>https://doi.org/10.1080/01431161.2026.2628303</link><guid>10.1080/01431161.2026.2628303</guid><pubDate>Mon, 09 Feb 2026 05:04:22 +0000</pubDate><dc:creator>Baomo Zhang</dc:creator><dc:creator>Qiang Zhang</dc:creator><dc:creator>Wenjing Gao</dc:creator><dc:creator>Bo Liu</dc:creator><dc:creator>Jianwei Wen</dc:creator><prism:publicationName>International Journal of Remote Sensing</prism:publicationName><prism:doi>10.1080/01431161.2026.2628303</prism:doi><description>Wildfires have a great impact on nature, ecology, and human society. To address these challenges, this research integrates multiple features of Himawari-8/9 satellite data to achieve 10-minute-level and near real-time wildfire detection. The proposed method employs the XGBoost method as a wildfire detection model and improves the reliability of wildfire detection by integrating multiple features. This research also divides the detection models into an early detection model and a continuous detection model. In order to detect early fire points, the early model introduces temporal features, which can effectively improve sensitivity to early wildfires. This research utilizes a recursive feature elimination technique to assess the influence of 42 candidate features on model accuracy and picks the highest-ranked multi-type features as input for the model. The suggested approach reduces false detection and missed detection rates of fire points, according to experimental results. In different types of wildfire scenarios, in comparison with the JAXA L2 WLF products, the proposed method can detect early wildfires. Validation using ground-collected forest fire data shows that the proposed method achieves an F1 score of 0.94, which is substantially higher than the 0.68 obtained by the JAXA L2 WLF products. Furthermore, large-scale cross-validation experiments with MODIS fire products further demonstrate the practicality of the proposed method.
Published: 2026-02-09T05:04:22+00:00
Venue: International Journal of Remote Sensing
Score: 0.676 (ignore)</description><content:encoded>&lt;p&gt;&lt;strong&gt;Authors:&lt;/strong&gt; Baomo Zhang; Qiang Zhang; Wenjing Gao; Bo Liu; Jianwei Wen&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Journal:&lt;/strong&gt; International Journal of Remote Sensing&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;a href="https://doi.org/10.1080/01431161.2026.2628303"&gt;10.1080/01431161.2026.2628303&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Relevance:&lt;/strong&gt; 0.676 (ignore)&lt;/p&gt;
&lt;p&gt;Wildfires have a great impact on nature, ecology, and human society. To address these challenges, this research integrates multiple features of Himawari-8/9 satellite data to achieve 10-minute-level and near real-time wildfire detection. The proposed method employs the XGBoost method as a wildfire detection model and improves the reliability of wildfire detection by integrating multiple features. This research also divides the detection models into an early detection model and a continuous detection model. In order to detect early fire points, the early model introduces temporal features, which can effectively improve sensitivity to early wildfires. This research utilizes a recursive feature elimination technique to assess the influence of 42 candidate features on model accuracy and picks the highest-ranked multi-type features as input for the model. The suggested approach reduces false detection and missed detection rates of fire points, according to experimental results. In different types of wildfire scenarios, in comparison with the JAXA L2 WLF products, the proposed method can detect early wildfires. Validation using ground-collected forest fire data shows that the proposed method achieves an F1 score of 0.94, which is substantially higher than the 0.68 obtained by the JAXA L2 WLF products. Furthermore, large-scale cross-validation experiments with MODIS fire products further demonstrate the practicality of the proposed method.&lt;/p&gt;</content:encoded></item><item><title>AI-driven zero trust and blockchain framework for secure electric vehicle infrastructure</title><link>https://doi.org/10.1016/j.eswa.2026.131577</link><guid>10.1016/j.eswa.2026.131577</guid><pubDate>Sun, 08 Feb 2026 07:06:26 +0000</pubDate><dc:creator>Clement Daah</dc:creator><dc:creator>Ysabel Fallot</dc:creator><dc:creator>Amna Qureshi</dc:creator><dc:creator>Irfan Awan</dc:creator><dc:creator>Savas Konur</dc:creator><prism:publicationName>Expert Systems with Applications</prism:publicationName><prism:doi>10.1016/j.eswa.2026.131577</prism:doi><description>Electric vehicle (EV) charging infrastructures are increasingly exposed to sophisticated cyber threats, including replay, spoofing, privilege escalation, and geolocation-based attacks. While standards such as ISO 15118 and OCPP 2.0.1 provide interoperability and cryptographic guarantees, they rely on static policies or isolated detection mechanisms, leaving gaps against adaptive adversaries. This paper presents an AI-driven Zero Trust Blockchain (AI-ZTB) framework whose novelty lies in the system-level integration of identity and access management, AI-based risk assessment, and blockchain-backed decentralized auditability with IPFS-based evidence storage, while operational governance remains centrally managed by the service provider. Unlike prior AI-only or blockchain-only frameworks, AI-ZTB introduces a fully integrated and enforceable Zero Trust control loop in which AI-generated risk scores are operationally bound to access enforcement decisions through smart contracts, enabling adaptive, auditable, and context-aware security governance in real time. The framework was implemented in Python with Solidity smart contracts and evaluated through a large-scale network simulation involving batches of 10,000 EV-charging sessions, trained on a dataset of 50,000 legitimate and adversarial behaviours using Random Forest, Autoencoder, and Isolation Forest models. Results demonstrate that AI-ZTB achieves access-decision accuracy above 95%, reducing false acceptance and rejection rates to approximately 3%. A comparative analysis evaluates AI-ZTB against industry standards (ISO 15,118 and OCPP 2.0.1) as secure communication baselines, and against prior integrated frameworks from the literature, highlighting differences in architectural scope, policy enforceability, and auditability rather than protocol-level performance. Despite modest inference and logging overheads, performance remained within real-time operational tolerances. The framework establishes a robust foundation for securing EV infrastructures, with extensibility to smart grids and other cyber-physical environments
Published: 2026-02-08T07:06:26+00:00
Venue: Expert Systems with Applications
Score: 0.666 (ignore)</description><content:encoded>&lt;p&gt;&lt;strong&gt;Authors:&lt;/strong&gt; Clement Daah; Ysabel Fallot; Amna Qureshi; Irfan Awan; Savas Konur&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Journal:&lt;/strong&gt; Expert Systems with Applications&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;a href="https://doi.org/10.1016/j.eswa.2026.131577"&gt;10.1016/j.eswa.2026.131577&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Relevance:&lt;/strong&gt; 0.666 (ignore)&lt;/p&gt;
&lt;p&gt;Electric vehicle (EV) charging infrastructures are increasingly exposed to sophisticated cyber threats, including replay, spoofing, privilege escalation, and geolocation-based attacks. While standards such as ISO 15118 and OCPP 2.0.1 provide interoperability and cryptographic guarantees, they rely on static policies or isolated detection mechanisms, leaving gaps against adaptive adversaries. This paper presents an AI-driven Zero Trust Blockchain (AI-ZTB) framework whose novelty lies in the system-level integration of identity and access management, AI-based risk assessment, and blockchain-backed decentralized auditability with IPFS-based evidence storage, while operational governance remains centrally managed by the service provider. Unlike prior AI-only or blockchain-only frameworks, AI-ZTB introduces a fully integrated and enforceable Zero Trust control loop in which AI-generated risk scores are operationally bound to access enforcement decisions through smart contracts, enabling adaptive, auditable, and context-aware security governance in real time. The framework was implemented in Python with Solidity smart contracts and evaluated through a large-scale network simulation involving batches of 10,000 EV-charging sessions, trained on a dataset of 50,000 legitimate and adversarial behaviours using Random Forest, Autoencoder, and Isolation Forest models. Results demonstrate that AI-ZTB achieves access-decision accuracy above 95%, reducing false acceptance and rejection rates to approximately 3%. A comparative analysis evaluates AI-ZTB against industry standards (ISO 15,118 and OCPP 2.0.1) as secure communication baselines, and against prior integrated frameworks from the literature, highlighting differences in architectural scope, policy enforceability, and auditability rather than protocol-level performance. Despite modest inference and logging overheads, performance remained within real-time operational tolerances. The framework establishes a robust foundation for securing EV infrastructures, with extensibility to smart grids and other cyber-physical environments&lt;/p&gt;</content:encoded></item><item><title>Analysis of double Beltrami horn surface resistor networks and efficient path planning</title><link>https://doi.org/10.1016/j.knosys.2026.115489</link><guid>10.1016/j.knosys.2026.115489</guid><pubDate>Sun, 08 Feb 2026 22:54:00 +0000</pubDate><dc:creator>Xiaoyu Jiang</dc:creator><dc:creator>Jianwei Daic</dc:creator><dc:creator>Yanpeng Zheng</dc:creator><dc:creator>Zhaolin Jiang</dc:creator><prism:publicationName>Knowledge-Based Systems</prism:publicationName><prism:doi>10.1016/j.knosys.2026.115489</prism:doi><description>Resistor networks, valued for their topological versatility and stable electrical properties, have emerged as a focal point across multiple disciplines. Yet, resistor networks with profound mathematical and physical significance remain largely unexplored. This study presents a detailed investigation of the double Beltrami horn surface resistor network and proposes an interpretable reasoning framework based on graph structures and grounded in physical laws. To improve the efficiency of large scale computation, the seventh type of discrete sine transform and Chebyshev polynomials of the first class are employed to derive the exact potential formula. In addition to generating potential distribution diagrams for various special scenarios, a fast algorithm is developed to significantly enhance the efficiency of potential computation. Furthermore, to expand the application potential of the resistor network, an efficient path planning algorithm based on the exact potential formula is proposed, and its applicability in dynamic environments is validated in preliminary experiments.
Published: 2026-02-08T22:54:00+00:00
Venue: Knowledge-Based Systems
Score: 0.660 (ignore)</description><content:encoded>&lt;p&gt;&lt;strong&gt;Authors:&lt;/strong&gt; Xiaoyu Jiang; Jianwei Daic; Yanpeng Zheng; Zhaolin Jiang&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Journal:&lt;/strong&gt; Knowledge-Based Systems&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;a href="https://doi.org/10.1016/j.knosys.2026.115489"&gt;10.1016/j.knosys.2026.115489&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Relevance:&lt;/strong&gt; 0.660 (ignore)&lt;/p&gt;
&lt;p&gt;Resistor networks, valued for their topological versatility and stable electrical properties, have emerged as a focal point across multiple disciplines. Yet, resistor networks with profound mathematical and physical significance remain largely unexplored. This study presents a detailed investigation of the double Beltrami horn surface resistor network and proposes an interpretable reasoning framework based on graph structures and grounded in physical laws. To improve the efficiency of large scale computation, the seventh type of discrete sine transform and Chebyshev polynomials of the first class are employed to derive the exact potential formula. In addition to generating potential distribution diagrams for various special scenarios, a fast algorithm is developed to significantly enhance the efficiency of potential computation. Furthermore, to expand the application potential of the resistor network, an efficient path planning algorithm based on the exact potential formula is proposed, and its applicability in dynamic environments is validated in preliminary experiments.&lt;/p&gt;</content:encoded></item><item><title>A Moon image point spread function deconvolution framework for satellite Earth observation image restoration</title><link>https://doi.org/10.1080/01431161.2026.2626096</link><guid>10.1080/01431161.2026.2626096</guid><pubDate>Mon, 09 Feb 2026 05:16:19 +0000</pubDate><dc:creator>Han Lin</dc:creator><dc:creator>Ya Su</dc:creator><dc:creator>Binglong Chen</dc:creator><dc:creator>Qunyong Wu</dc:creator><dc:creator>Zhiqing Zhang</dc:creator><prism:publicationName>International Journal of Remote Sensing</prism:publicationName><prism:doi>10.1080/01431161.2026.2626096</prism:doi><description>Accurate restoration of remote sensing imagery is challenged by complex degradation factors, including atmospheric turbulence and sensor imperfections. A major bottleneck lies in the difficulty of disentangling atmospheric effects from the system’s intrinsic Point Spread Function (PSF), which limits the stability and generalization of conventional restoration methods. This study proposes MoonPSF-Decon, which derives an intrinsic, sensor-consistent PSF from high-contrast, atmosphere-free FY-4B/AGRI lunar observations and applies it to Earth imagery for non-blind deconvolution using Wiener (WN) and Lucy – Richardson (LR) deconvolution algorithms. Unlike prior lunar-target studies focused on Modulation Transfer Function (MTF) diagnostics, we demonstrate short-term PSF reusability: a single MoonPSF extracted on 28 September 2023 is reused to restore a month-long sequence of AGRI Earth images (1–31 October 2023), improving operational efficiency. A progressive validation strategy – including simulation, real-scene restoration, and cross-sensor comparison with near-synchronous Himawari-9/AHI high-resolution data – confirms the method’s effectiveness. For AGRI 10.8 µm imagery, peak signal-to-noise ratio (PSNR) increases by 1.8% for both LR and WN, root mean square error (RMSE) decreases by 8.1% (LR) and 8.0% (WN), and structural similarity (SSIM) improves by 0.75% (LR) and 0.8% (WN). The month-long evaluation shows consistently improved PSNR/SSIM and reduced RMSE relative to the AHI quasi-reference, supporting the temporal robustness of the lunar-derived intrinsic PSF. Overall, MoonPSF-Decon provides a stable and physically grounded PSF prior for practical, multi-temporal Earth observation image deblurring.
Published: 2026-02-09T05:16:19+00:00
Venue: International Journal of Remote Sensing
Score: 0.656 (ignore)</description><content:encoded>&lt;p&gt;&lt;strong&gt;Authors:&lt;/strong&gt; Han Lin; Ya Su; Binglong Chen; Qunyong Wu; Zhiqing Zhang&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Journal:&lt;/strong&gt; International Journal of Remote Sensing&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;a href="https://doi.org/10.1080/01431161.2026.2626096"&gt;10.1080/01431161.2026.2626096&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Relevance:&lt;/strong&gt; 0.656 (ignore)&lt;/p&gt;
&lt;p&gt;Accurate restoration of remote sensing imagery is challenged by complex degradation factors, including atmospheric turbulence and sensor imperfections. A major bottleneck lies in the difficulty of disentangling atmospheric effects from the system’s intrinsic Point Spread Function (PSF), which limits the stability and generalization of conventional restoration methods. This study proposes MoonPSF-Decon, which derives an intrinsic, sensor-consistent PSF from high-contrast, atmosphere-free FY-4B/AGRI lunar observations and applies it to Earth imagery for non-blind deconvolution using Wiener (WN) and Lucy – Richardson (LR) deconvolution algorithms. Unlike prior lunar-target studies focused on Modulation Transfer Function (MTF) diagnostics, we demonstrate short-term PSF reusability: a single MoonPSF extracted on 28 September 2023 is reused to restore a month-long sequence of AGRI Earth images (1–31 October 2023), improving operational efficiency. A progressive validation strategy – including simulation, real-scene restoration, and cross-sensor comparison with near-synchronous Himawari-9/AHI high-resolution data – confirms the method’s effectiveness. For AGRI 10.8 µm imagery, peak signal-to-noise ratio (PSNR) increases by 1.8% for both LR and WN, root mean square error (RMSE) decreases by 8.1% (LR) and 8.0% (WN), and structural similarity (SSIM) improves by 0.75% (LR) and 0.8% (WN). The month-long evaluation shows consistently improved PSNR/SSIM and reduced RMSE relative to the AHI quasi-reference, supporting the temporal robustness of the lunar-derived intrinsic PSF. Overall, MoonPSF-Decon provides a stable and physically grounded PSF prior for practical, multi-temporal Earth observation image deblurring.&lt;/p&gt;</content:encoded></item></channel></rss>