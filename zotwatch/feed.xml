<?xml version='1.0' encoding='utf-8'?>
<rss version="2.0"><channel><title>ZotWatch Feed</title><link>https://ehehe.cn/zotwatch/</link><description>AI-assisted literature watch</description><lastBuildDate>Fri, 28 Nov 2025 00:50:27 +0000</lastBuildDate><item><title>Dual-Stream Multi-Modal Fusion with Local-Global Attention for Remote Sensing Object Detection</title><link>https://doi.org/10.1109/jstars.2025.3637891</link><guid>10.1109/jstars.2025.3637891</guid><pubDate>Thu, 27 Nov 2025 18:58:13 +0000</pubDate><category>IEEE Journal of Selected Topics in Applied Earth Observations and Remote Sensing</category><description>Object detection in remote sensing imagery plays a crucial role in providing precise geospatial information for urban planning and environmental monitoring. However, real-world remote sensing scenarios often involve complex conditions such as varying illumination, weather interference, and low signal-to-noise ratios, which significantly degrade the performance of traditional single-modal detection methods. To overcome these limitations, multimodal object detection has developed, demonstrating great potential by integrating complementary information from multiple modalities. Nevertheless, existing multimodal frameworks still face challenges such as insufficient cross-modal interaction, limited learning of complementary features, and high computational costs due to redundant fusion in complex environments. To overcome these challenges, we propose an enhanced multi-modal fusion strategy aimed at maximizing cross-modal feature learning capabilities. Our method employs a dual-backbone architecture to extract mode-specific representations independently, integrating a direction attention (DA) module at an early stage of each backbone to enhance discriminative feature extraction. We then introduce a dual-stream feature fusion network (DSFN) to effectively fuse cross-modal features, generating rich representations for the detection head. Additionally, we embed a local-global channel attention (LGCA) mechanism in the head stage to strengthen feature learning in the channel dimension before generating the final prediction. Extensive experiments on the widely used VEDAI multimodal remote sensing dataset demonstrate that our method achieves state-of-the-art performance, while evaluations on single-modal datasets confirm its exceptional generalization capability.
Published: 2025-11-27T18:58:13+00:00
Venue: IEEE Journal of Selected Topics in Applied Earth Observations and Remote Sensing
Score: 0.892 (must_read)</description></item><item><title>MMT: Multimodal meta-training for few-shot object detection</title><link>https://doi.org/10.1016/j.neucom.2025.132197</link><guid>10.1016/j.neucom.2025.132197</guid><pubDate>Thu, 27 Nov 2025 07:57:02 +0000</pubDate><category>Neurocomputing</category><description>Few-Shot Object Detection (FSOD) aims to detect objects from novel classes using only a few labeled instances per class. Recently, several FSOD approaches have incorporated vision-language models (VLMs) to leverage textual semantics for improving visual representations. However, VLM-based FSOD methods still face two major challenges: (1) the alignment bias between textual and regional features, which leads to unstable or suboptimal performance on novel categories; and (2) the lack of efficient training strategies, as most methods rely on repeatedly fine-tuning models on limited novel samples, which contradicts the few-shot learning paradigm and incurs substantial computational cost. To address these issues, we propose a Multimodal Meta-Training (MMT) framework that enhances both semantic alignment and training efficiency in FSOD. MMT consists of two core components: (1) a Region Feature Enhancement Module (RFEM), which refines visual region representations through cross-modal fusion with textual features to alleviate semantic misalignment; and (2) a Meta-Training Strategy, which adopts an inner–outer loop optimization scheme to improve model generalization and reduce training overhead. Extensive experiments on PASCAL VOC and MS COCO demonstrate that MMT achieves superior detection accuracy on novel classes while significantly reducing training time.
Published: 2025-11-27T07:57:02+00:00
Venue: Neurocomputing
Score: 0.886 (must_read)</description></item><item><title>From Diffusion to One-Step Generation: A Comparative Study of Flow-Based Models with Application to Image Inpainting</title><link>https://arxiv.org/abs/2511.21215v1</link><guid>http://arxiv.org/abs/2511.21215v1</guid><pubDate>Wed, 26 Nov 2025 09:44:51 +0000</pubDate><category>arXiv</category><description>We present a comprehensive comparative study of three generative modeling paradigms: Denoising Diffusion Probabilistic Models (DDPM), Conditional Flow Matching (CFM), and MeanFlow. While DDPM and CFM require iterative sampling, MeanFlow enables direct one-step generation by modeling the average velocity over time intervals. We implement all three methods using a unified TinyUNet architecture (&lt;1.5M parameters) on CIFAR-10, demonstrating that CFM achieves an FID of 24.15 with 50 steps, significantly outperforming DDPM (FID 402.98). MeanFlow achieves FID 29.15 with single-step sampling -- a 50X reduction in inference time. We further extend CFM to image inpainting, implementing mask-guided sampling with four mask types (center, random bbox, irregular, half). Our fine-tuned inpainting model achieves substantial improvements: PSNR increases from 4.95 to 8.57 dB on center masks (+73%), and SSIM improves from 0.289 to 0.418 (+45%), demonstrating the effectiveness of inpainting-aware training.
Published: 2025-11-26T09:44:51+00:00
Venue: arXiv
Score: 0.871 (must_read)</description></item><item><title>ReSAM: Refine, Requery, and Reinforce: Self-Prompting Point-Supervised Segmentation for Remote Sensing Images</title><link>https://arxiv.org/abs/2511.21606v1</link><guid>http://arxiv.org/abs/2511.21606v1</guid><pubDate>Wed, 26 Nov 2025 17:26:00 +0000</pubDate><category>arXiv</category><description>Interactive segmentation models such as the Segment Anything Model (SAM) have demonstrated remarkable generalization on natural images, but perform suboptimally on remote sensing imagery (RSI) due to severe domain shift and the scarcity of dense annotations. To address this, we propose a self-prompting, point-supervised framework that adapts SAM to RSIs using only sparse point annotations. Our method employs a Refine-Requery-Reinforce loop, where coarse pseudo-masks are generated from initial points (Refine), improved with self-constructed box prompts (Requery), and embeddings are aligned across iterations to reduce confirmation bias (Reinforce). Without relying on full-mask supervision, our approach progressively enhances SAM's segmentation quality and domain robustness through self-guided prompt adaptation . We evaluate our proposed method on three RSI benchmark datasets, including WHU, HRSID, and NWPU VHR-10, showing that our method consistently surpasses pretrained SAM and recent point-supervised segmentation methods. Our results demonstrate that self-prompting and semantic alignment provide an efficient path towards scalable, point-level adaptation of foundation segmentation models for remote sensing applications.
Published: 2025-11-26T17:26:00+00:00
Venue: arXiv
Score: 0.852 (must_read)</description></item><item><title>HTTM: Head-wise Temporal Token Merging for Faster VGGT</title><link>https://arxiv.org/abs/2511.21317v1</link><guid>http://arxiv.org/abs/2511.21317v1</guid><pubDate>Wed, 26 Nov 2025 12:04:03 +0000</pubDate><category>arXiv</category><description>The Visual Geometry Grounded Transformer (VGGT) marks a significant leap forward in 3D scene reconstruction, as it is the first model that directly infers all key 3D attributes (camera poses, depths, and dense geometry) jointly in one pass. However, this joint inference mechanism requires global attention layers that perform all-to-all attention computation on tokens from all views. For reconstruction of large scenes with long-sequence inputs, this causes a significant latency bottleneck. In this paper, we propose head-wise temporal merging (HTTM), a training-free 3D token merging method for accelerating VGGT. Existing merging techniques merge tokens uniformly across different attention heads, resulting in identical tokens in the layers' output, which hinders the model's representational ability. HTTM tackles this problem by merging tokens in multi-head granularity, which preserves the uniqueness of feature tokens after head concatenation. Additionally, this enables HTTM to leverage the spatial locality and temporal correspondence observed at the head level to achieve higher merging ratios with lower merging costs compared to existing methods. Thus, HTTM achieves up to 7x acceleration with negligible performance drops in a GPU-based inference.
Published: 2025-11-26T12:04:03+00:00
Venue: arXiv
Score: 0.847 (must_read)</description></item><item><title>SFIFusion: Semantic-Frequency Integration for Task-driven Infrared and Visible Image Fusion</title><link>https://doi.org/10.1016/j.sigpro.2025.110419</link><guid>10.1016/j.sigpro.2025.110419</guid><pubDate>Thu, 27 Nov 2025 16:04:22 +0000</pubDate><category>Signal Processing</category><description>Image fusion integrates complementary features from source images to enhance human and machine vision. Existing methods face two key limitations: (1) prioritizing visual quality over semantic representation, limiting downstream task performance, and (2) relying on spatial domain features, neglecting high-frequency details like textures and edges. To address these, we propose SFIFusion, a task-oriented network for semantic-frequency feature fusion, specifically for infrared and visible images. SFIFusion incorporates a Semantic Enhancement Block (SEB) for deep semantic feature extraction, aligned with visual details via DINOv2 to ensure semantic consistency. The enriched semantic features are subsequently incorporated back into the fusion process, ensuring that final fused image is both visually refined and semantically robust. It also introduces a Frequency Enhancement Block (FEB), using Fourier transform to decompose images into amplitude (texture/style) and phase (structural details), preserving amplitude for visual richness and combining phase for structural integrity. Experiments show SFIFusion outperforms current methods in visual quality, quantitative metrics, and downstream tasks like object detection and semantic segmentation, demonstrating its practical applicability in complex scenarios. The source code will be available at https://github.com/Zzuouo/SFIFusion .
Published: 2025-11-27T16:04:22+00:00
Venue: Signal Processing
Score: 0.845 (must_read)</description></item><item><title>Sawtooth Sampling for Time Series Denoising Diffusion Implicit Models</title><link>https://arxiv.org/abs/2511.21320v1</link><guid>http://arxiv.org/abs/2511.21320v1</guid><pubDate>Wed, 26 Nov 2025 12:05:44 +0000</pubDate><category>arXiv</category><description>Denoising Diffusion Probabilistic Models (DDPMs) can generate synthetic timeseries data to help improve the performance of a classifier, but their sampling process is computationally expensive. We address this by combining implicit diffusion models with a novel Sawtooth Sampler that accelerates the reverse process and can be applied to any pretrained diffusion model. Our approach achieves a 30 times speed-up over the standard baseline while also enhancing the quality of the generated sequences for classification tasks.
Published: 2025-11-26T12:05:44+00:00
Venue: arXiv
Score: 0.843 (must_read)</description></item><item><title>Escaping the Verifier: Learning to Reason via Demonstrations</title><link>https://arxiv.org/abs/2511.21667v1</link><guid>http://arxiv.org/abs/2511.21667v1</guid><pubDate>Wed, 26 Nov 2025 18:42:52 +0000</pubDate><category>arXiv</category><description>Training Large Language Models (LLMs) to reason often relies on Reinforcement Learning (RL) with task-specific verifiers. However, many real-world reasoning-intensive tasks lack verifiers, despite offering abundant expert demonstrations that remain under-utilized for reasoning-focused training. We introduce RARO (Relativistic Adversarial Reasoning Optimization) that learns strong reasoning capabilities from only expert demonstrations via Inverse Reinforcement Learning. Our method sets up an adversarial interaction between a policy (generator) and a relativistic critic (discriminator): the policy learns to mimic expert answers, while the critic learns to compare and distinguish between policy and expert answers. Our method trains both the policy and the critic jointly and continuously via RL, and we identify the key stabilization techniques required for robust learning. Empirically, RARO significantly outperforms strong verifier-free baselines on all of our evaluation tasks -- Countdown, DeepMath, and Poetry Writing -- and enjoys the same robust scaling trends as RL on verifiable tasks. These results demonstrate that our method effectively elicits strong reasoning performance from expert demonstrations alone, enabling robust reasoning learning even when task-specific verifiers are unavailable.
Published: 2025-11-26T18:42:52+00:00
Venue: arXiv
Score: 0.842 (must_read)</description></item><item><title>MBLKNet: a large kernel convolution-driven network with multi-task self-supervised learning for SAR maritime target classification</title><link>https://doi.org/10.1080/10095020.2025.2584937</link><guid>10.1080/10095020.2025.2584937</guid><pubDate>Thu, 27 Nov 2025 15:46:43 +0000</pubDate><category>Geo-spatial Information Science</category><description>Synthetic aperture radar (SAR) maritime target classification serves as a critical component in modern maritime surveillance. While deep learning networks, particularly convolutional neural networks (CNNs), have driven substantial progress in this domain, three key challenges constrain their performance and practical deployment: 1) In SAR maritime images, complex inshore backgrounds and speckle noise are prevalent. Targets such as ships span a wide range of scales due to different imaging resolutions and intrinsic size variability, exacerbating inter-class similarity and intra-class variability, 2) Labeled data for SAR maritime target classification are scarce, and sensor imaging modes differ markedly across platforms, and 3) Existing CNNs that fuse traditional hand-crafted features often explicitly treat hand-crafted feature extraction as a necessary component of the network and primarily focus on classification performance, overlooking the requirement to efficiently leverage their feature extraction capabilities in downstream tasks. To overcome these challenges, this article proposes a novel SAR maritime target classification network (MBLKNet) based on large kernel convolution and multi-task self-supervised learning. In MBLKNet, four improved designs for network structure are proposed to enhance classification accuracy: 1) macro design, 2) multi-branch large kernel convolution module (MBLKCM), 3) lightweight channel-interactive multi-layer perceptron (LCIMLP), and 4) micro design. In addition, a multi-resolution unlabeled SAR maritime target dataset (SL-SARShip) and a masked image modeling framework, HOGSparK, are proposed to enable the pre-training of MBLKNet under joint supervision of pixel and HOG features. Comparison results on OpenSARShip 2.0 and FUSAR-Ship with state-of-the-art networks, as well as experiments on SSDD for SAR downstream target detection and instance segmentation, demonstrate that the proposed MBLKNet achieves superior performance and strong feature extraction ability.
Published: 2025-11-27T15:46:43+00:00
Venue: Geo-spatial Information Science
Score: 0.840 (must_read)</description></item><item><title>Seeing without Pixels: Perception from Camera Trajectories</title><link>https://arxiv.org/abs/2511.21681v1</link><guid>http://arxiv.org/abs/2511.21681v1</guid><pubDate>Wed, 26 Nov 2025 18:57:01 +0000</pubDate><category>arXiv</category><description>Can one perceive a video's content without seeing its pixels, just from the camera trajectory-the path it carves through space? This paper is the first to systematically investigate this seemingly implausible question. Towards this end, we propose a contrastive learning framework to train CamFormer, a dedicated encoder that projects camera pose trajectories into a joint embedding space, aligning them with natural language. We find that, contrary to its apparent simplicity, the camera trajectory is a remarkably informative signal to uncover video content. In other words, "how you move" can indeed reveal "what you are doing" (egocentric) or "observing" (exocentric). We demonstrate the versatility of our learned CamFormer embeddings on a diverse suite of downstream tasks, ranging from cross-modal alignment to classification and temporal analysis. Importantly, our representations are robust across diverse camera pose estimation methods, including both high-fidelity multi-sensored and standard RGB-only estimators. Our findings establish camera trajectory as a lightweight, robust, and versatile modality for perceiving video content.
Published: 2025-11-26T18:57:01+00:00
Venue: arXiv
Score: 0.839 (must_read)</description></item><item><title>OVOD-Agent: A Markov-Bandit Framework for Proactive Visual Reasoning and Self-Evolving Detection</title><link>https://arxiv.org/abs/2511.21064v1</link><guid>http://arxiv.org/abs/2511.21064v1</guid><pubDate>Wed, 26 Nov 2025 05:08:26 +0000</pubDate><category>arXiv</category><description>Open-Vocabulary Object Detection (OVOD) aims to enable detectors to generalize across categories by leveraging semantic information. Although existing methods are pretrained on large vision-language datasets, their inference is still limited to fixed category names, creating a gap between multimodal training and unimodal inference. Previous work has shown that improving textual representation can significantly enhance OVOD performance, indicating that the textual space is still underexplored. To this end, we propose OVOD-Agent, which transforms passive category matching into proactive visual reasoning and self-evolving detection. Inspired by the Chain-of-Thought (CoT) paradigm, OVOD-Agent extends the textual optimization process into an interpretable Visual-CoT with explicit actions. OVOD's lightweight nature makes LLM-based management unsuitable; instead, we model visual context transitions as a Weakly Markovian Decision Process (w-MDP) over eight state spaces, which naturally represents the agent's state, memory, and interaction dynamics. A Bandit module generates exploration signals under limited supervision, helping the agent focus on uncertain regions and adapt its detection policy. We further integrate Markov transition matrices with Bandit trajectories for self-supervised Reward Model (RM) optimization, forming a closed loop from Bandit exploration to RM learning. Experiments on COCO and LVIS show that OVOD-Agent provides consistent improvements across OVOD backbones, particularly on rare categories, confirming the effectiveness of the proposed framework.
Published: 2025-11-26T05:08:26+00:00
Venue: arXiv
Score: 0.839 (must_read)</description></item><item><title>MLPMoE: Zero-Shot Architectural Metamorphosis of Dense LLM MLPs into Static Mixture-of-Experts</title><link>https://arxiv.org/abs/2511.21089v1</link><guid>http://arxiv.org/abs/2511.21089v1</guid><pubDate>Wed, 26 Nov 2025 06:14:26 +0000</pubDate><category>arXiv</category><description>Large Language Models (LLMs) are predominantly deployed as dense transformers, where every parameter in every feed-forward block is activated for every token. While architecturally simple, this is computationally inefficient, since inference costs scale linearly with parameter count. Recent upcycling methods such as MoEfication, CMoE, ToMoE, and MoORE reveal that much of the useful computation lives in sparse, semi-modular substructures inside dense feed-forward networks, but these approaches typically rely on clustering, activation profiling, singular value decomposition, or custom routing that requires calibration data. This paper introduces MLPMoE (MLP Mixture-of-Experts), a training-free, deterministic transformation that restructures the dense MLP in transformer blocks into a static, high-cardinality mixture of experts. The transformation uses simple tensor slicing and summation, reinterpreting the algebra of tensor parallelism as a topological conversion rather than a distributed training pattern. We further introduce Fractal Fade (differential branch sparsity) and Compensated Pruning (variance-preserving branch reduction) as lightweight mechanisms for structured sparsity. On Qwen2.5-0.5B-Instruct and DeepSeek-R1-Distill-Llama-8B, the zero-shot MLPMoE transform changes a proxy perplexity metric by less than 0.05 percent while keeping the parameter count effectively constant. On the 8B model, differential sparsity removes about 20 percent of MLP parameters while keeping perplexity within about 2 percent of the dense baseline. The method operates entirely post hoc on existing checkpoints and does not require gradients, calibration sets, or router training. Code is available at https://gist.github.com/iwallarm/fc2ef1eddf226ca7814f9e5e2ae9bad1
Published: 2025-11-26T06:14:26+00:00
Venue: arXiv
Score: 0.838 (must_read)</description></item><item><title>Breaking the Safety-Capability Tradeoff: Reinforcement Learning with Verifiable Rewards Maintains Safety Guardrails in LLMs</title><link>https://arxiv.org/abs/2511.21050v1</link><guid>http://arxiv.org/abs/2511.21050v1</guid><pubDate>Wed, 26 Nov 2025 04:36:34 +0000</pubDate><category>arXiv</category><description>Fine-tuning large language models (LLMs) for downstream tasks typically exhibit a fundamental safety-capability tradeoff, where improving task performance degrades safety alignment even on benign datasets. This degradation persists across standard approaches including supervised finetuning (SFT) and reinforcement learning from human feedback (RLHF). While reinforcement learning with verifiable rewards (RLVR) has emerged as a promising alternative that optimizes models on objectively measurable tasks, its safety implications remain unexplored. We present the first comprehensive theoretical and empirical analysis of safety properties in RLVR. Theoretically, we derive upper bounds on safety drift under KL-constrained optimization and prove conditions under which safety degradation is eliminated. Empirically, we conduct extensive experiments across five adversarial safety benchmarks, demonstrating that RLVR can simultaneously enhance reasoning capabilities while maintaining or improving safety guardrails. Our comprehensive ablation studies examine the effects of optimization algorithms, model scale, and task domains. Our findings challenge the prevailing assumption of an inevitable safety capability trade-off, and establish that a specific training methodology can achieve both objectives simultaneously, providing insights for the safe deployment of reasoning-capable LLMs.
Published: 2025-11-26T04:36:34+00:00
Venue: arXiv
Score: 0.837 (must_read)</description></item><item><title>Scaling Foundation Models for Radar Scene Understanding</title><link>https://arxiv.org/abs/2511.21105v1</link><guid>http://arxiv.org/abs/2511.21105v1</guid><pubDate>Wed, 26 Nov 2025 06:41:00 +0000</pubDate><category>arXiv</category><description>Radar sensors provide reliable perception across adverse weather, lighting, and long-range conditions. Recent advances in foundation models have transformed visual and language understanding, yet their integration with radar sensing remains largely underexplored. Existing radar approaches are fragmented and task-specific; each downstream task employs distinct architectures and training objectives, preventing transfer across tasks. In this work, we introduce RadarFM: a radar foundation model that learns unified scene-level representations through structured spatial language supervision. We make two key contributions: (1) a structured caption framework that encodes vehicle distributions in native radar coordinates, and (2) a hash-aware contrastive learning objective that quantifies continuous scene similarity rather than binary matching, enabling fine-grained spatial reasoning. Leveraging the CARLA simulator, we generate large-scale, well-annotated radar datasets across diverse driving scenarios. We also propose localization-aware metrics that assess spatial accuracy beyond traditional detection measures.
Published: 2025-11-26T06:41:00+00:00
Venue: arXiv
Score: 0.836 (must_read)</description></item><item><title>The More, the Merrier: Contrastive Fusion for Higher-Order Multimodal Alignment</title><link>https://arxiv.org/abs/2511.21331v1</link><guid>http://arxiv.org/abs/2511.21331v1</guid><pubDate>Wed, 26 Nov 2025 12:25:55 +0000</pubDate><category>arXiv</category><description>Learning joint representations across multiple modalities remains a central challenge in multimodal machine learning. Prevailing approaches predominantly operate in pairwise settings, aligning two modalities at a time. While some recent methods aim to capture higher-order interactions among multiple modalities, they often overlook or insufficiently preserve pairwise relationships, limiting their effectiveness on single-modality tasks. In this work, we introduce Contrastive Fusion (ConFu), a framework that jointly embeds both individual modalities and their fused combinations into a unified representation space, where modalities and their fused counterparts are aligned. ConFu extends traditional pairwise contrastive objectives with an additional fused-modality contrastive term, encouraging the joint embedding of modality pairs with a third modality. This formulation enables ConFu to capture higher-order dependencies, such as XOR-like relationships, that cannot be recovered through pairwise alignment alone, while still maintaining strong pairwise correspondence. We evaluate ConFu on synthetic and real-world multimodal benchmarks, assessing its ability to exploit cross-modal complementarity, capture higher-order dependencies, and scale with increasing multimodal complexity. Across these settings, ConFu demonstrates competitive performance on retrieval and classification tasks, while supporting unified one-to-one and two-to-one retrieval within a single contrastive framework.
Published: 2025-11-26T12:25:55+00:00
Venue: arXiv
Score: 0.835 (must_read)</description></item><item><title>Aligning LLMs Toward Multi-Turn Conversational Outcomes Using Iterative PPO</title><link>https://arxiv.org/abs/2511.21638v1</link><guid>http://arxiv.org/abs/2511.21638v1</guid><pubDate>Wed, 26 Nov 2025 18:12:16 +0000</pubDate><category>arXiv</category><description>Optimizing large language models (LLMs) for multi-turn conversational outcomes remains a significant challenge, especially in goal-oriented settings like AI marketing or sales agents who facilitate transactions via messaging platforms. The difficulty stems from sparse, long-horizon rewards and the discrepancy between response-level planning and token-level generation. In this technical note, we propose a formal reduction of the multi-turn RL problem into a sequence of single-turn RLHF-style problems. This is achieved by setting a learned multi-turn Q-function as the reward model for the single-turn problem. We demonstrate and prove a key insight: solving this single-turn RL problem with standard token-level PPO is equivalent to a policy improvement step within the multi-turn problem. This insight naturally leads to Iterative PPO, a batch online policy iteration algorithm that alternates between fitting Q-functions from logged conversation trajectories and improving the policy. A major practical advantage is that Iterative PPO directly leverages stable, off-the-shelf single-turn RLHF tools, making it straightforward to implement. Our method occupies a middle ground between fully online and fully offline approaches, retaining the adaptability of online updates while gaining the stability benefits of offline training.
Published: 2025-11-26T18:12:16+00:00
Venue: arXiv
Score: 0.835 (must_read)</description></item><item><title>ICPO: Intrinsic Confidence-Driven Group Relative Preference Optimization for Efficient Reinforcement Learning</title><link>https://arxiv.org/abs/2511.21005v1</link><guid>http://arxiv.org/abs/2511.21005v1</guid><pubDate>Wed, 26 Nov 2025 03:10:15 +0000</pubDate><category>arXiv</category><description>Reinforcement Learning with Verifiable Rewards (RLVR) demonstrates significant potential in enhancing the reasoning capabilities of Large Language Models (LLMs). However, existing RLVR methods are often constrained by issues such as coarse-grained rewards, reward noise, and inefficient exploration, which lead to unstable training and entropy collapse. To address this challenge, we propose the Intrinsic Confidence-Driven Group Relative Preference Optimization method (ICPO). The intuition behind it lies in the fact that the probabilities of an LLM generating different responses can inherently and directly reflect its self-assessment of the reasoning process. Inspired by the idea of preference modeling, ICPO calculates a preference advantage score for each response by comparing the relative generation probabilities of multiple responses under the same input prompt, and integrates this score with verifiable rewards to guide the exploration process. We have discovered that the preference advantage score not only alleviates the issues of coarse-grained rewards and reward noise but also effectively curbs overconfident errors, enhances the relative superiority of undervalued high-quality responses, and prevents the model from overfitting to specific strategies, thereby facilitating more thorough exploration. Comprehensive experiments across four general-domain benchmarks and three mathematical benchmarks demonstrate that ICPO steadily boosts reasoning compared to GRPO.
Published: 2025-11-26T03:10:15+00:00
Venue: arXiv
Score: 0.835 (must_read)</description></item><item><title>EM-KD: Distilling Efficient Multimodal Large Language Model with Unbalanced Vision Tokens</title><link>https://arxiv.org/abs/2511.21106v1</link><guid>http://arxiv.org/abs/2511.21106v1</guid><pubDate>Wed, 26 Nov 2025 06:45:59 +0000</pubDate><category>arXiv</category><description>Efficient Multimodal Large Language Models (MLLMs) compress vision tokens to reduce resource consumption, but the loss of visual information can degrade comprehension capabilities. Although some priors introduce Knowledge Distillation to enhance student models, they overlook the fundamental differences in fine-grained vision comprehension caused by unbalanced vision tokens between the efficient student and vanilla teacher. In this paper, we propose EM-KD, a novel paradigm that enhances the Efficient MLLMs with Knowledge Distillation. To overcome the challenge of unbalanced vision tokens, we first calculate the Manhattan distance between the vision logits of teacher and student, and then align them in the spatial dimension with the Hungarian matching algorithm. After alignment, EM-KD introduces two distillation strategies: 1) Vision-Language Affinity Distillation (VLAD) and 2) Vision Semantic Distillation (VSD). Specifically, VLAD calculates the affinity matrix between text tokens and aligned vision tokens, and minimizes the smooth L1 distance of the student and the teacher affinity matrices. Considering the semantic richness of vision logits in the final layer, VSD employs the reverse KL divergence to measure the discrete probability distributions of the aligned vision logits over the vocabulary space. Comprehensive evaluation on diverse benchmarks demonstrates that EM-KD trained model outperforms prior Efficient MLLMs on both accuracy and efficiency with a large margin, validating its effectiveness. Compared with previous distillation methods, which are equipped with our proposed vision token matching strategy for fair comparison, EM-KD also achieves better performance.
Published: 2025-11-26T06:45:59+00:00
Venue: arXiv
Score: 0.832 (must_read)</description></item><item><title>Frequency-Aware Token Reduction for Efficient Vision Transformer</title><link>https://arxiv.org/abs/2511.21477v1</link><guid>http://arxiv.org/abs/2511.21477v1</guid><pubDate>Wed, 26 Nov 2025 15:10:04 +0000</pubDate><category>arXiv</category><description>Vision Transformers have demonstrated exceptional performance across various computer vision tasks, yet their quadratic computational complexity concerning token length remains a significant challenge. To address this, token reduction methods have been widely explored. However, existing approaches often overlook the frequency characteristics of self-attention, such as rank collapsing and over-smoothing phenomenon. In this paper, we propose a frequency-aware token reduction strategy that improves computational efficiency while preserving performance by mitigating rank collapsing. Our method partitions tokens into high-frequency tokens and low-frequency tokens. high-frequency tokens are selectively preserved, while low-frequency tokens are aggregated into a compact direct current token to retain essential low-frequency components. Through extensive experiments and analysis, we demonstrate that our approach significantly improves accuracy while reducing computational overhead and mitigating rank collapsing and over smoothing. Furthermore, we analyze the previous methods, shedding light on their implicit frequency characteristics and limitations.
Published: 2025-11-26T15:10:04+00:00
Venue: arXiv
Score: 0.831 (must_read)</description></item><item><title>Mean Teacher Based on Class Prototype Contrast for Domain Adaptive Object Detection</title><link>https://doi.org/10.1016/j.neunet.2025.108377</link><guid>10.1016/j.neunet.2025.108377</guid><pubDate>Thu, 27 Nov 2025 07:55:50 +0000</pubDate><category>Neural Networks</category><description>Unsupervised domain adaptive object detection (UDAOD) aims to effectively apply the detector trained on a labeled (source domain) and an unlabeled (target domain) dataset to the target domain. The mean teacher framework has demonstrated good applicability and wide application in this task. However, influenced by the difference between the two domains, the teacher model often generates many false positive objects. The pseudo-labels cannot sufficiently include all classes of objects in an image because of single-threshold filtering, causing the model to perform poorly in detection tasks. Therefore, we propose a new student-teacher framework, the mean teacher, which is based on class prototype contrast (PCMT). Utilizing class prototypes to preserve the features that are common in objects of the same class to address the problem of significant feature differences that may exist between these objects. Then, the class prototypes are applied to contrastive learning, so that the model can distinguish various classes more accurately while align the features of the same class across domains. In addition, we design a pseudo-label filtering method based on bounding box localization to retain potentially valid pseudo-labels. Experiments show that PCMT achieves superior performance under different domain adaptive conditions. For the Cityscapes→BDD100K dataset, we obtain the best mean average precision (mAP) of 43.5%, which is 5.0% greater than the state-of-the-art (SOTA).
Published: 2025-11-27T07:55:50+00:00
Venue: Neural Networks
Score: 0.831 (must_read)</description></item><item><title>Knowledge Completes the Vision: A Multimodal Entity-aware Retrieval-Augmented Generation Framework for News Image Captioning</title><link>https://arxiv.org/abs/2511.21002v1</link><guid>http://arxiv.org/abs/2511.21002v1</guid><pubDate>Wed, 26 Nov 2025 03:03:52 +0000</pubDate><category>arXiv</category><description>News image captioning aims to produce journalistically informative descriptions by combining visual content with contextual cues from associated articles. Despite recent advances, existing methods struggle with three key challenges: (1) incomplete information coverage, (2) weak cross-modal alignment, and (3) suboptimal visual-entity grounding. To address these issues, we introduce MERGE, the first Multimodal Entity-aware Retrieval-augmented GEneration framework for news image captioning. MERGE constructs an entity-centric multimodal knowledge base (EMKB) that integrates textual, visual, and structured knowledge, enabling enriched background retrieval. It improves cross-modal alignment through a multistage hypothesis-caption strategy and enhances visual-entity matching via dynamic retrieval guided by image content. Extensive experiments on GoodNews and NYTimes800k show that MERGE significantly outperforms state-of-the-art baselines, with CIDEr gains of +6.84 and +1.16 in caption quality, and F1-score improvements of +4.14 and +2.64 in named entity recognition. Notably, MERGE also generalizes well to the unseen Visual News dataset, achieving +20.17 in CIDEr and +6.22 in F1-score, demonstrating strong robustness and domain adaptability.
Published: 2025-11-26T03:03:52+00:00
Venue: arXiv
Score: 0.826 (must_read)</description></item><item><title>G$^2$VLM: Geometry Grounded Vision Language Model with Unified 3D Reconstruction and Spatial Reasoning</title><link>https://arxiv.org/abs/2511.21688v1</link><guid>http://arxiv.org/abs/2511.21688v1</guid><pubDate>Wed, 26 Nov 2025 18:59:39 +0000</pubDate><category>arXiv</category><description>Vision-Language Models (VLMs) still lack robustness in spatial intelligence, demonstrating poor performance on spatial understanding and reasoning tasks. We attribute this gap to the absence of a visual geometry learning process capable of reconstructing 3D space from 2D images. We present G$^2$VLM, a geometry grounded vision-language model that bridges two fundamental aspects of spatial intelligence: spatial 3D reconstruction and spatial understanding. G$^2$VLM natively leverages learned 3D visual geometry features to directly predict 3D attributes and enhance spatial reasoning tasks via in-context learning and interleaved reasoning. Our unified design is highly scalable for spatial understanding: it trains on abundant multi-view image and video data, while simultaneously leveraging the benefits of 3D visual priors that are typically only derived from hard-to-collect annotations. Experimental results demonstrate G$^2$VLM is proficient in both tasks, achieving comparable results to state-of-the-art feed-forward 3D reconstruction models and achieving better or competitive results across spatial understanding and reasoning tasks. By unifying a semantically strong VLM with low-level 3D vision tasks, we hope G$^2$VLM can serve as a strong baseline for the community and unlock more future applications, such as 3D scene editing.
Published: 2025-11-26T18:59:39+00:00
Venue: arXiv
Score: 0.825 (must_read)</description></item><item><title>From Inpainting to Layer Decomposition: Repurposing Generative Inpainting Models for Image Layer Decomposition</title><link>https://arxiv.org/abs/2511.20996v1</link><guid>http://arxiv.org/abs/2511.20996v1</guid><pubDate>Wed, 26 Nov 2025 02:50:07 +0000</pubDate><category>arXiv</category><description>Images can be viewed as layered compositions, foreground objects over background, with potential occlusions. This layered representation enables independent editing of elements, offering greater flexibility for content creation. Despite the progress in large generative models, decomposing a single image into layers remains challenging due to limited methods and data. We observe a strong connection between layer decomposition and in/outpainting tasks, and propose adapting a diffusion-based inpainting model for layer decomposition using lightweight finetuning. To further preserve detail in the latent space, we introduce a novel multi-modal context fusion module with linear attention complexity. Our model is trained purely on a synthetic dataset constructed from open-source assets and achieves superior performance in object removal and occlusion recovery, unlocking new possibilities in downstream editing and creative applications.
Published: 2025-11-26T02:50:07+00:00
Venue: arXiv
Score: 0.823 (must_read)</description></item><item><title>Leader-Based Multiexpert Neural Network for High-Level Visual Tasks</title><link>https://doi.org/10.1109/tnnls.2025.3631509</link><guid>10.1109/tnnls.2025.3631509</guid><pubDate>Thu, 27 Nov 2025 18:58:39 +0000</pubDate><category>IEEE Transactions on Neural Networks and Learning Systems</category><description>Remarkable progress has been achieved in the detection and segmentation of the baseline; however, for high-level visual tasks in complex scenes (e.g., dense, occlusion, scale diversity, high background noise, etc.), existing frameworks often fail to provide satisfactory performance. To further improve the object recognition ability, this article introduces a leader-based multiexpert mechanism into the detection and segmentation tasks. In this work, we first design a leader-based attention learning layer to fully integrate multilevel features from the backbone network, which can effectively obtain global semantics and assign instructions to detection experts. Then, we propose multiple feature pyramids with dual fusion paths to replace the traditional single pipeline using semantic and spatial allocators. With this strategy, we can further establish deep supervision for multiple experts during training and sufficiently utilize the multiexpert detection results from leaders’ assignments during reasoning, thereby comprehensively improving the performance of the model in complex scenarios. In the experiment, we established ablation studies and performance comparisons on COCO 2017 detection and segmentation tasks. Finally, we demonstrated the model’s performance in three complex application scenarios (remote sensing, autonomous driving, and industrial fields), and the results showed our advantages.
Published: 2025-11-27T18:58:39+00:00
Venue: IEEE Transactions on Neural Networks and Learning Systems
Score: 0.822 (must_read)</description></item><item><title>Staggered Environment Resets Improve Massively Parallel On-Policy Reinforcement Learning</title><link>https://arxiv.org/abs/2511.21011v1</link><guid>http://arxiv.org/abs/2511.21011v1</guid><pubDate>Wed, 26 Nov 2025 03:20:08 +0000</pubDate><category>arXiv</category><description>Massively parallel GPU simulation environments have accelerated reinforcement learning (RL) research by enabling fast data collection for on-policy RL algorithms like Proximal Policy Optimization (PPO). To maximize throughput, it is common to use short rollouts per policy update, increasing the update-to-data (UTD) ra- tio. However, we find that, in this setting, standard synchronous resets introduce harmful nonstationarity, skewing the learning signal and destabilizing training. We introduce staggered resets, a simple yet effective technique where environments are initialized and reset at varied points within the task horizon. This yields training batches with greater temporal diversity, reducing the nonstationarity induced by synchronized rollouts. We characterize dimensions along which RL environments can benefit significantly from staggered resets through illustrative toy environ- ments. We then apply this technique to challenging high-dimensional robotics environments, achieving significantly higher sample efficiency, faster wall-clock convergence, and stronger final performance. Finally, this technique scales better with more parallel environments compared to naive synchronized rollouts.
Published: 2025-11-26T03:20:08+00:00
Venue: arXiv
Score: 0.821 (must_read)</description></item><item><title>Co-Training Vision Language Models for Remote Sensing Multi-task Learning</title><link>https://arxiv.org/abs/2511.21272v1</link><guid>http://arxiv.org/abs/2511.21272v1</guid><pubDate>Wed, 26 Nov 2025 10:55:07 +0000</pubDate><category>arXiv</category><description>With Transformers achieving outstanding performance on individual remote sensing (RS) tasks, we are now approaching the realization of a unified model that excels across multiple tasks through multi-task learning (MTL). Compared to single-task approaches, MTL methods offer improved generalization, enhanced scalability, and greater practical applicability. Recently, vision language models (VLMs) have achieved promising results in RS image understanding, grounding, and ultra-high-resolution (UHR) image reasoning, respectively. Moreover, the unified text-based interface demonstrates significant potential for MTL. Hence, in this work, we present RSCoVLM, a simple yet flexible VLM baseline for RS MTL. Firstly, we create the data curation engine, including data acquisition, offline processing and integrating, as well as online loading and weighting. This data engine effectively addresses complex RS data enviroment and generates flexible vision-language conversations. Furthermore, we propose a unified dynamic-resolution strategy to address the diverse image scales inherent in RS imagery. For UHR images, we introduce the Zoom-in Chain mechanism together with its corresponding dataset, LRS-VQA-Zoom. The strategies are flexible and effectively mitigate the computational burdens. Additionally, we significantly enhance the model's object detection capability and propose a novel evaluation protocol that ensures fair comparison between VLMs and conventional detection models. Extensive experiments demonstrate that RSCoVLM achieves state-of-the-art performance across diverse tasks, outperforming existing RS VLMs and even rivaling specialized expert models. All the training and evaluating tools, model weights, and datasets have been fully open-sourced to support reproducibility. We expect that this baseline will promote further progress toward general-purpose RS models.
Published: 2025-11-26T10:55:07+00:00
Venue: arXiv
Score: 0.820 (must_read)</description></item><item><title>GatedFusion-Net: Per-Pixel Modality Weighting in a Five-Cue Transformer For RGB-D-I-T-UV Fusion</title><link>https://doi.org/10.1016/j.inffus.2025.103986</link><guid>10.1016/j.inffus.2025.103986</guid><pubDate>Thu, 27 Nov 2025 00:18:33 +0000</pubDate><category>Information Fusion</category><description>Highlights • Transformer-based architecture integrating five aligned modalities: RGB, depth, infrared intensity, thermal, and ultraviolet for semantic segmentation.
Published: 2025-11-27T00:18:33+00:00
Venue: Information Fusion
Score: 0.816 (must_read)</description></item><item><title>Monocular 3D Lane Detection with Geometry-Guided Transformation and Contextual Enhancement</title><link>https://doi.org/10.1016/j.patrec.2025.11.041</link><guid>10.1016/j.patrec.2025.11.041</guid><pubDate>Thu, 27 Nov 2025 00:08:14 +0000</pubDate><category>Pattern Recognition Letters</category><description>Monocular 3D lane detection is a critical yet challenging task in autonomous driving, largely due to the lack of depth cues, complex road geometries, and appearance variations in real-world environments. Existing approaches often depend on bird’s-eye-view transformations or rigid geometric assumptions, which may introduce projection artifacts and hinder generalization. In this paper, we present GeoCNet, a BEV-free framework that directly estimates 3D lanes in the perspective domain. The architecture incorporates three key components: a Geometry-Guided Spatial Transformer (GST) for adaptive multi-plane ground modeling, a Perception-Aware Feature Modulation (PFM) module for context-driven feature refinement, and a Structure-Aware Lane Decoder (SALD) that reconstructs lanes as curvature-regularized anchor-aligned sequences. Extensive experiments on the OpenLane dataset demonstrate that GeoCNet achieves competitive performance in overall accuracy and shows clear improvements in challenging conditions such as night scenes and complex intersections. Additional evaluation on the Apollo Synthetic dataset further confirms the robustness and cross-domain generalization of the proposed framework. These results underscore the effectiveness of jointly leveraging geometry and contextual cues for accurate and reliable monocular 3D lane detection. Our code has been released at https://github.com/chunyingsong/GeoCNet .
Published: 2025-11-27T00:08:14+00:00
Venue: Pattern Recognition Letters
Score: 0.815 (must_read)</description></item><item><title>DCTC-Net: Dual-Branch Cross-Fusion Transformer–CNN Architecture for Medical Image Segmentation</title><link>https://doi.org/10.1109/tnnls.2025.3628995</link><guid>10.1109/tnnls.2025.3628995</guid><pubDate>Thu, 27 Nov 2025 18:58:39 +0000</pubDate><category>IEEE Transactions on Neural Networks and Learning Systems</category><description>Hybrid architectures that combine convolutional neural networks (CNNs) with Transformers have emerged as a promising approach for medical image segmentation. However, existing networks based on this hybrid architecture often encounter two challenges. First, while the CNN branch effectively captures local image features through convolution operations, vanilla convolution lacks the ability to achieve adaptive feature extraction. Second, although the Transformer branch can model global image information, conventional self-attention (SA) primarily focuses on spatial relationships, neglecting channel and cross-dimensional attention, leading to suboptimal segmentation results, particularly for medical images with complex backgrounds. To address these limitations, we propose a dual-branch cross-fusion Transformer–CNN architecture for medical image segmentation (DCTC-Net). Our network provides two key advantages. First, a dynamic deformable convolution (DDConv) is integrated into the CNN branch to overcome the limitations of adaptive feature extraction with fixed-size convolution kernels and also eliminate the issue of shared convolution kernel parameters across different inputs, significantly enhancing the feature expression capabilities of the CNN branch. Second, a (shifted)-window adaptive complementary attention module ((S)W-ACAM) and compact convolutional projection are incorporated into the Transformer branch, enabling the network to comprehensively learn cross-dimensional long-range dependencies in medical images. Experimental results demonstrate that the proposed DCTC-Net achieves superior medical image segmentation performance compared to state-of-the-art (SOTA) methods, including CNN and Transformer networks. In addition, our DCTC-Net requires fewer parameters and lower computational costs and does not rely on pretraining.
Published: 2025-11-27T18:58:39+00:00
Venue: IEEE Transactions on Neural Networks and Learning Systems
Score: 0.815 (must_read)</description></item><item><title>Canvas-to-Image: Compositional Image Generation with Multimodal Controls</title><link>https://arxiv.org/abs/2511.21691v1</link><guid>http://arxiv.org/abs/2511.21691v1</guid><pubDate>Wed, 26 Nov 2025 18:59:56 +0000</pubDate><category>arXiv</category><description>While modern diffusion models excel at generating high-quality and diverse images, they still struggle with high-fidelity compositional and multimodal control, particularly when users simultaneously specify text prompts, subject references, spatial arrangements, pose constraints, and layout annotations. We introduce Canvas-to-Image, a unified framework that consolidates these heterogeneous controls into a single canvas interface, enabling users to generate images that faithfully reflect their intent. Our key idea is to encode diverse control signals into a single composite canvas image that the model can directly interpret for integrated visual-spatial reasoning. We further curate a suite of multi-task datasets and propose a Multi-Task Canvas Training strategy that optimizes the diffusion model to jointly understand and integrate heterogeneous controls into text-to-image generation within a unified learning paradigm. This joint training enables Canvas-to-Image to reason across multiple control modalities rather than relying on task-specific heuristics, and it generalizes well to multi-control scenarios during inference. Extensive experiments show that Canvas-to-Image significantly outperforms state-of-the-art methods in identity preservation and control adherence across challenging benchmarks, including multi-person composition, pose-controlled composition, layout-constrained generation, and multi-control generation.
Published: 2025-11-26T18:59:56+00:00
Venue: arXiv
Score: 0.815 (must_read)</description></item></channel></rss>